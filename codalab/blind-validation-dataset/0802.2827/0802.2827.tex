\documentclass[fleqn]{stacs_proc} 
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}

\begin{document}
\title[Design by Measure and Conquer: A Faster Algorithm for Dominating Set]{Design by Measure and Conquer \\ A Faster Exact Algorithm for Dominating Set}
\author{Johan M. M. van Rooij}{Johan M. M. van Rooij}
\author{Hans L. Bodlaender}{Hans L. Bodlaender}
\address{
Institute of Information and Computing Sciences, Utrecht University, \newline
P.O.Box 80.089, 3508 TB Utrecht, The Netherlands
}
\email{jmmrooij@cs.uu.nl}
\email[Hans L. Bodlaender]{hansb@cs.uu.nl}
\urladdr{http://www.cs.uu.nl}

\keywords{exact algorithms, exponential time algorithms, branch and reduce, measure and conquer, dominating set, computer aided algorithm design}
\subjclass{
F.2.2. [Analysis of Algorithms and Problem Complexity]: Non-numerical Algorithms and Problems---computations on discrete structures;  
G.2.2. [Discrete Mathematics]: Graph Theory---graph algorithms;
I.2.2. [Artificial Intelligence]: Automatic Programming---automatic analysis of algorithms}






\begin{abstract}
The {\em measure and conquer} approach has proven to be a powerful tool to
{\em analyse} exact algorithms for combinatorial problems, like {\sc Dominating Set} and
{\sc Independent Set}. In this paper, we propose to use measure and conquer
also as a tool in the {\em design} of algorithms. 
In an iterative process, we can obtain a series of {\em branch and reduce} algorithms.
A mathematical analysis of an algorithm in the series with measure and conquer
results in a quasiconvex programming problem.
The solution by computer
to this problem not only gives a bound on the running time,
but also can give a new reduction rule, thus giving a new,
possibly faster algorithm.
This makes {\em design by measure and conquer} a form of {\em computer aided algorithm design}.

When we apply the methodology to a {\sc Set Cover} modelling of
the {\sc Dominating Set} problem, we obtain the
currently fastest known exact algorithms for {\sc Dominating Set}: an algorithm
that uses  time and polynomial space, and an algorithm
that uses  time.
\end{abstract} 

\maketitle


\stacsheading{2008}{657-668}{Bordeaux}
\firstpageno{657}



\section{Introduction}
The design of fast exponential time algorithms for finding exact solutions to
NP-hard problems such as {\sc Independent Set} and {\sc Dominating Set}
has been a topic for research for over 30 years, see e.g., the results
on {\sc Independent Set} in the 1970s by Tarjan and Trojanowski 
\cite{Tarjan72a,TarjanT77}. A number of different techniques have been used
for these and other exponential time algorithms \cite{FominGK05,Woeginger03,Woeginger04}.

An important paradigm for the design of exact algorithms is {\em branch and reduce},
pioneered in 1960 by Davis and Putnam \cite{DavisP60}. Typically, in a branch
and reduce algorithm, a collection of reduction rules
and branching rules are given.
Each reduction rule simplifies the instance to an equivalent, simpler
instance. If no rule applies, the branching rules generate a collection of two or more instances,
on which the algorithm recurses.

An important recent development in the analysis of branch and reduce algorithms is
{\em measure and conquer}, which has been introduced by Fomin, Grandoni and Kratsch \cite{FominGK05a}.
The measure and conquer approach helps to obtain good upper bounds on the running time
of branch and reduce algorithms, often improving upon the currently best known bounds for
exact algorithms. It has been used successfully on 
{\sc Dominating Set} \cite{FominGK05a}, {\sc Independent Set} \cite{FominGK06},
{\sc Dominating Clique} \cite{KratschL06}, the number of minimal dominating sets
\cite{FominGPS05}, {\sc Connected Dominating Set} \cite{FominGK06a}, {\sc Minimum Independent
Dominating Set} \cite{GaspersL06}, and possibly others.

In this paper, we show that the measure and conquer approach can not only be used
for the {\em analysis} of exact algorithms, but also for the {\em design} of such algorithms.
More specifically, measure and conquer uses a non-standard size measure for instances.
This measure is based on weight vectors, which are computed by solving a quasiconvex programming problem.
Analysis of the solution of this quasiconvex program yields not only an upper bound
to the running time of the algorithm, but also shows where we should improve
the algorithm. This can lead to a new rule, which we add to the branch and
reduce algorithm. 

We apply this {\em design by measure and conquer} methodology to a {\sc Set Cover}
modelling of the {\sc Dominating Set} problem, identical to the setting in which
measure and conquer was first introduced.
If we start with the trivial algorithm, then, we can obtain in a number of steps the original
algorithm of Fomin et al.~\cite{FominGK05a}, but with additional steps, we obtain a faster
algorithm, using  time and polynomial space, with a variant that uses exponential
memory and  time. We also show that at this point we cannot improve this
measure and conquer computed running time, unless we choose a different measure
or change the branching rules.

While for several classic combinatorial problems, the first non-trivial exact algorithms date many years
ago, for the {\sc Dominating Set} problem, the first algorithms with running time 
 with  are from 2004, with three independent papers: by Fomin et
al.~\cite{FominKW04}, by Randerath and Schiermeyer \cite{RanderathS05}, and by
Grandoni \cite{Grandoni06}. The so far fastest algorithm for {\sc Dominating Set} was
found in 2005 by Fomin, Grandoni, and Kratsch \cite{FominGK05a}: this algorithm uses 
 time and polynomial space, or  time and exponential space.






\section{Preliminaries} \label{sec:definitions}
Given a collection of non-empty sets ,
a \emph{set cover} of  is a subset
 such that every element in any of the sets in  occurs in some set in .
In the {\sc Set Cover} problem we are given a collection 
and are asked to compute a set cover of minimum cardinality.

The universe  of a {\sc Set Cover}
problem instance is the set of all elements in any set in ; .
The frequency  of an element  is the number of sets in  in which this element occurs.
Let  be the set of sets in  in which the element  occurs.
We define a connected component  in a {\sc Set Cover} problem instance  in a natural way:
a minimal non-empty subset  for which all elements in the sets in 
occur only in sets contained in .
The dimension  of a {\sc Set Cover} problem instance is the sum
of the number of sets in  and the number of elements in ; .

Let  be an undirected graph.
A subset  of nodes is called a \emph{dominating set}
if every node  is either in  or adjacent to some node in .
The {\sc Dominating Set} problem is to compute for a given graph 
a dominating set of minimum cardinality.

We can reduce the minimum dominating set problem to the
{\sc Set Cover} problem by introducing a set for each node of  which contains
the node itself and its neighbours; .
Thus we can solve a {\sc Dominating Set} problem on a graph of  nodes by a
minimum set cover algorithm running on an instance of dimension .

Both problems are long known to be NP-complete \cite{GareyJ79,Karp72}, which motivates
the search for fast exponential time algorithms.






\section{A Faster Exact Algorithm for Dominating Set} \label{sec:m&calgorithm}
In this section, we give our new algorithm for {\sc Dominating Set}. The
algorithm is an improvement to the algorithm by Fomin et
al.~\cite{FominGK05a}; it is obtained from this algorithm by adding some
additional reduction rules. These rules were derived using the design by
measure and conquer approach, see Section~\ref{section:design}. After
introducing our algorithm, we recall the necessary background of the measure
and conquer method \cite{FominGK05a}. 




\subsection{The Algorithm}
Our algorithm for the {\sc Dominating Set} problem uses the {\sc Set Cover}
modelling of {\sc Dominating Set} shown in Section \ref{sec:definitions}.
It is a branch and reduce algorithm on this modelling consisting of four simple reduction rules,
one base case for the recursion and a branching rule. See Algorithm \ref{algorithm}.

For a given problem instance we first apply the following reduction rules:
\begin{enumerate}
\item {\em Base case}. \label{rule:matching}
	  If all sets in the instance are of size at most two then finding
	  a minimum set cover is equivalent to finding a maximum matching in a graph.
	  Introduce a node for each element and an edge for each set of size two.
	  Now the maximum matching joined with some sets containing the unmatched nodes
	  form a minimum set cover. This matching can be computed in polynomial time \cite{Edmonds65}.
\item {\em Splitting connected components}. \label{rule:component}
		If the instance contains multiple connected components, compute the minimum set cover in each connected component separately.
\item {\em Subset rule}. \label{rule:subset}
	  If the instance contains sets ,  with ,
	  then we remove  from the instance. Namely, in each minimum set
	  cover that contains , we can replace  by  and obtain
	  a minimum set cover without .
\item {\em Subsumption rule}. \label{rule:subsumption}
	  If the set of sets  in which an element  occurs is a subset of the set of sets 
	  in which another element  occurs, we remove the element .
	  For any set cover, covering  also covers .
\item {\em Unique element or singleton rule}.
	  If any set of size one remains in the instance after application of the previous rules, 
	  we add this set to the set cover. For the element in this set must occur uniquely in this set,
	  otherwise it would have been a subset of another set and have been removed by rule \ref{rule:subset}.
\item {\em Avoiding unnecessary branchings based on frequency two elements}. \label{rule:special}
	  For any set  in the problem instance let  be the number of frequency two elements it contains.
	  Let  be the number of elements in the union of sets containing the other occurrences of these
	  frequency two elements, excluding any element already in .
	  If for any set :  then include  in the set cover. \\
	  This rule is correct since if we would branch on  and include it in the set cover we would
	  cover  elements with one set. If the set cover does not use ,
	  it must use 
	  all sets containing the other occurrence of the frequency two elements,
	  since they have become unique elements now. Notice that by Rule \ref{rule:subsumption}
	  all other occurrences of the frequency two elements must be in different sets and
	  thus we would cover  elements with  sets.
	  So if  the first case can be preferred over the second: we can just add  to the
	  cover and have  sets left to cover at least this much elements.
\end{enumerate}
For the branching rule, we select a set of maximum cardinality and
create two subproblems by either including it in the minimum set cover and removing all
newly covered elements from the problem instance or removing it.

\begin{algorithm}
\begin{algorithmic}
\STATE MSC() = \{
\IF{}
	\RETURN minimum set cover of  by computing a matching
\ELSIF{}
	\RETURN MSC() + MSC()
\ELSIF{}
	\RETURN MSC()
\ELSIF{}
	\STATE 
	\RETURN 
\ELSIF{}
	\RETURN )
\ELSIF{}
	\RETURN 
\ELSE
	\STATE Let 
	\STATE 
	\RETURN 
\ENDIF
\STATE \}
\end{algorithmic}
\caption{Algorithm Designed by Measure and Conquer}
\label{algorithm}
\end{algorithm}



\subsection{Running time analysis by Measure and Conquer} \label{sec:m&c}
The basic idea of {\em measure and conquer} is the usage of a non-standard measure 
for the complexity of a problem instance in combination with an extensive subcase analysis.
In the case of {\sc Set Cover}, we give weights to set sizes and element frequencies,
and sum these weights over all items and sets. We enumerate many subcases in which the algorithm
can branch and derive recurrence relations for each of these cases in terms of these weights.
Finally we obtain a large numerical optimisation problem which computes the weights corresponding
to the smallest solution to all recurrence relations, giving an upper bound on the running time of our algorithm.
This analysis is similar to \cite{FominGK05a}.

We let  be the weight of an element of frequency  and a set of size  respectively,
and set our {\em variable measure of complexity}  to:

Sets of different sizes and elements of different frequencies contribute equally to the
dimension of the instance, but now larger sets and higher frequency elements can contribute
more to the measured complexity of the instance.
Furthermore we set  since all frequency one elements
and size one sets are removed by the reduction rules.
For later use we introduce quantities representing the reduction in problem complexity
when the size of a set or the frequency of an element is reduced by one.
For technical reasons, these quantities must be non-negative.


The next step will be to derive recurrence relations representing problem instances the algorithm branches on.
Let  be the number of subproblems generated in order to solve a problem of measured complexity .
And let  (include ) and  (discard ) be the difference in measured complexity
of both subproblems compared to the problem instance we branch on.
Finally let , where  the number of elements in  of frequency~.

If we add  to the set cover,  is removed together with all its elements.
This results in a reduction in size of .
Because of the removal of these elements, other sets are reduced in size;
this leads to an additional complexity reduction of at least .
To keep the formula (and the next) linear, we set 
and keep the formula correctly modelling the algorithm by introducing the following constraints on the weights:

One can show that including these in the numerical optimisation problem
does not change the solution, as it gives the same weights.
The constraints help to considerably speed up this optimisation process.

If we discard , we also remove it from the problem instance, and hence all its elements are reduced in frequency by one.
So we have a complexity reduction of .
Besides this reduction, the sets which contain the second occurrences of any frequency two element
are included in the set cover. Notice that these must be different sets due to reduction Rule \ref{rule:subsumption}.
Because of Rule \ref{rule:special} we know that at least  other elements must be in these sets as well,
and these also must occur somewhere else in the instance, hence even more sets are reduced in size.
Summation leads to an additional size reduction of .
Here we also use Rule \ref{rule:component} to make sure that not all these frequency two elements
occur in the same set, because in that case all considered sets form a connected component of at most five sets
which thus can be solved in  time.

This leads to the following set of recurrence relations: :

where


We make the problem finite by setting for some large enough  all  for ,
and only consider the subcases , where .
Now we have a finite set of recurrences which model our algorithm since
the recurrences for the cases where  are dominated by those where .
The best value for  follows from the optimisation, for if chosen too small the now constant
recurrences (weights equal~1) will dominate all others in the optimum, and if chosen too large
the extra  and  are optimised to 1 (and the optimisation problem was unnecessarily hard).
Here  equals~.

A solution to this set of recurrence relations will be of the form , 
where  is the smallest solution of the set of inequalities:

Since  where  the dimension of the problem, we know that the algorithm
will have a running time of , for any :

From here on we let  be the error in the upward decimal rounding of .

So for any given vector  and 
we can now compute the running time measured with these weights.
As a result we have obtained a numerical optimisation problem: choose
the best weights so that the upper bound on the running time is minimal.

The numerical solution to this problem can be found in the last cell of Table \ref{tab:steps},
resulting in an upper bound on the running time of the algorithm of :



\subsection{Quasiconvex programming}
The sort of numerical optimisation problems arising from
measure and conquer analyses are {\em quasiconvex programs}, named after
the kind of function we are optimising: a {\em quasiconvex function},
which is a function with convex level sets .

To our knowledge there are currently two different techniques in use to solve
these quasiconvex programs: randomised search, and  Eppstein's \emph{smooth
quasiconvex programming} algorithm \cite{Eppstein04}.
We have implemented a variant of the second technique; for details see \cite{vanRooij06}.



\subsection{Results}
As discussed, we have now obtained the following result.
\begin{theorem}
Algorithm \ref{algorithm} solves a {\sc Set Cover}
problem instance of dimension  in  time and polynomial space.
\qed
\end{theorem}
Using the minimum set cover modelling of {\sc Dominating Set} this results in:
\begin{corollary}
There exists an algorithm that solves the {\sc Dominating Set} problem in  time and polynomial space.
\qed
\end{corollary}

We can further reduce the time complexity of the algorithm at the cost of exponential space.
This can be done by dynamic programming; the algorithm keeps track of all solutions to
subproblems solved and if the same subproblem turns up more than ones it is looked up.
Notice that querying and storing the subproblems can be implemented in polynomial time.

We compute the new time complexity based on \cite{FominGK07,Robson86} and obtain:
\begin{theorem} \label{thrm:exptime}
Algorithm \ref{algorithm}, modified as above, solves a {\sc Set Cover}
problem of dimension  in  time and space.
\qed
\end{theorem}
\begin{corollary}
There exists an algorithm that solves the {\sc Dominating Set} problem in  time and space.
\qed
\end{corollary}




\section{Design by Measure and Conquer}
\label{section:design}
The beauty of our algorithm lies in the fact that it has been designed
using a form of {\em computer aided algorithm design} which we call {\em design by measure and conquer}.
Given a variable measure of complexity as in the analysis in Section \ref{sec:m&c}
and a set of branching rules, all polynomial time computable reduction rules
relative to this measure and branching rules follow by the method.
We start with a trivial branch and reduce algorithm, i.e. one without any reduction rules and only
consisting of the branching rule and a trivial base case (if the problem is empty, return ).
Next we exhaustively apply an improvement step,
which comes up with a new reduction rule and hence a possibly faster algorithm.
This changes the algorithm analysis technique measure and conquer into a technique for algorithm design.

Thus, this gives a very nice process, where a human invents 
additional reduction rules, and the computational power of our computer does
the extensive measure and conquer analysis and
points to all possible points of direct improvement.
This combination has proven to be successful
as we see from the results of Section \ref{sec:m&calgorithm}.
While constructing our algorithm, the previously fastest algorithm for {\sc Dominating Set}
by Fomin et al. \cite{FominGK05a} has been obtained as an intermediate step.
It has now been improved up to a point where we need to either
change the branching rule (or add new branching rules) or modify the measure and conquer analysis,
i.e. use a different variable measure or perform a more elaborate subcase analysis.
See Table \ref{tab:steps} for information on the analysis and added rules for
all algorithms, from the starting trivial algorithm without any reduction
rules till we obtain Algorithm \ref{algorithm}.

\subsection{A Single Iteration: improving the previously fastest algorithm}
We now demonstrate how the improvement step works, by giving one such
improvement as elaborate example, namely an improvement we can make when we
start with the algorithm by Fomin et al.~\cite{FominGK05a}.
This step is marked with a star in Table \ref{tab:steps}.
First we perform a measure and conquer analysis on the current algorithm
giving us the optimal instantiation of the variable measure,
and an upper bound on the running time of the algorithm.
Next we examine the quasiconvex function we have just optimised.

The quasiconvex function has the following form:

where  is the set of all possible instances the algorithm can branch on
and  are the differences in measured complexity between
the generated subproblems and branching subcase .

Each one of the functions  is quasiconvex (see \cite{Eppstein04}), i.e. it has convex level sets.
The situation is very similar to finding the point  of minimum maximum distance to
a set of points  in  dimensional space:
only a few points in  have distance to  tight to this maximum,
and moving away from  always results in at least one of these distances to increase.
If one such tight point is moved or removed, this directly influences the optimum  and the minimum maximum distance.



We now consider the eight cases that are tight to the value of the 
quasiconvex function  in the optimum.
These are:

Now, if we can formulate a reduction rule that either further reduces the size of any subproblem generated in these cases,
or removes any of these cases completely, then we lower the value of the corresponding ,
or remove this  respectively, resulting in a new optimum corresponding to a faster running time.

We take the simplest case for improvement;  as small as possible,
and with as low frequency elements as possible.
This corresponds to an instance with:

We emphasize that this is not an entire instance, but just a fragment of an instance containing the set  used for branching 
and the collection of sets in which the elements from  also occur.
In an instance corresponding to this subcase the element 4 can be of frequency two or higher, but all sets are of size three or smaller.

We note that we do not need to branch on this particular subcase:
elements 1 and 2 occur in exactly the same sets, and thus
if a set cover covers one of these, the other is covered as well.
We generalise this and formulate the subsumption rule (Rule
\ref{rule:subsumption} of Algorithm \ref{algorithm}).
Now we have a new algorithm, for which we can adjust the measure and conquer analysis, and repeat this process.


\begin{table} \begin{center}
\begin{tabular}{|ll|}
\hline
\textbf{Latest new reduction rule}		& Running times for {\sc Set Cover} and \\
current formula for 	& \hspace*{5pt} {\sc Dominating Set} \\
subcases considered 					& instance part of the simplest worst case; \\
weights vectors  and 	& \hspace*{5pt}  other occurrences of elements of \\
\hline
\multicolumn{2}{l}{}\\
\hline
\textbf{Trivial algorithm}						&  \\
\multicolumn{2}{|l|}{  } \\

&  \\
 &  \\
\hline
\textbf{Stop when all sets of size one}			&  \\
\multicolumn{2}{|l|}{  }\\

&  \\
 &  \\
\hline
\textbf{Include all frequency one elements}		&  \\
\multicolumn{2}{|l|}{  } \\

&  \\
 &  \\
\hline
\textbf{Subset rule}							&  \\
\multicolumn{2}{|l|}{  } \\

& \\
 &  \\
\hline
\textbf{Compute matching for size two sets} &  \\
\multicolumn{2}{|l|}{  } \\

&  \\
 &  \\
\hline
\textbf{Subsumption rule}						&  \\
\multicolumn{2}{|l|}{  } \\

&  \\
 &  \\
\hline
\textbf{Avoid unnecessary branchings}	&  \\
\multicolumn{2}{|l|}{  } \\

&  \\
\multicolumn{2}{|l|}{  } \\
\multicolumn{2}{|l|}{  } \\
\hline
\textbf{Connected components (final)}	&  \\
\multicolumn{2}{|l|}{  } \\

&  \\
\multicolumn{2}{|l|}{  } \\
\multicolumn{2}{|l|}{  } \\
\hline
\multicolumn{2}{l}{  Algorithm by Fomin, Grandoni and Kratsch \cite{FominGK05a}.}
\end{tabular} \end{center}
\caption{The iterations of the design by measure and conquer process.}
\label{tab:steps}
\end{table}


\subsection{The Process Halts}
Above, we discussed 
how to perform one step of the design by measure and conquer process.
For a complete overview of the construction of Algorithm \ref{algorithm} see
Table \ref{tab:steps}, with the relevant data for each improvement step.
Note from Table \ref{tab:steps}
that after each new step, the example worst case instance part
is no longer a valid worst case for the next step.
As a result, at each step either some subcases are removed by using
a larger smallest set  or by removing small sets or elements (setting  or ),
or the size reduction in the formula for  is increased.
After each step we refactored the reduction rules and removed possible superfluous ones.
We have not included the formula for  in this table,
since it does not change except that  in early stages.

It appears that we must use a different approach to obtain a faster algorithm. 
Considered the following problem:
\begin{problem} \label{NPCproblem}
Given a {\sc Set Cover} instance  and a set  with the properties:
\begin{enumerate}
\item Non of the reduction rules of Algorithm \ref{algorithm} apply to .
\item All sets in  have cardinality at most three; .
\item Every element  has frequency two.
\end{enumerate}
Question: Does there exist a minimum set cover of  containing ?
\end{problem}

\begin{proposition} \label{prop:npc}
Problem \ref{NPCproblem} is NP-complete.
\end{proposition}

Proposition \ref{prop:npc} implies that we cannot formulate a polynomial time reduction rule that removes the
current simplest worst case of our algorithm by deciding on whether  is in a minimum set cover or not, unless .

We can construct similar NP-complete problems for all other worst cases of Algorithm \ref{algorithm}.
Therefore Algorithm \ref{algorithm} is optimal in some sense:
we cannot straightforwardly improve it by performing another iteration.
In order to obtain a faster branch and reduce algorithm using polynomial time reduction rules 
with a smaller measure and conquer proved time bound,
it is necessary to either change the variable measure, the branching rule(s),
or perform a more extensive subcase analysis.

Very recently, we pursued the last option with little result.
We tried to further subdivide the frequency two elements in the branch set
depending on the size of the set containing their second occurence (two or larger) and
if this is a set of size two, on the frequency of the other element in this set.
This resulted in a set of very technical reduction rules
and a small speedup for the case where we use only polynomial space.
This speedup, however, was almost completely lost when using exponential space
because some of the weights involved became almost zero.







\section{Conclusion and Further Research}
In this paper, we have given the currently fastest exact algorithm for the {\sc Dominating Set}
problem. Besides setting the current record for this central graph theoretic problem, we also
have shown that measure and conquer can be used as a tool for the design of algorithms.

We have shown that there exists a strong relation between the chosen variable measure,
the branching rule(s) and the reduction rules of a measure and conquer based algorithm.
We intend to further investigate this relation and 
examine to what point we can deduce not only reduction rules,
but also branching rules from the given measure.

We plan to apply the {\em design by measure and conquer} method to a number of other combinatorial
problems, and hope and expect that in a number of cases, such a computer aided algorithm design
will give further improvements to the best known exact algorithms for these problems.

In this paper, we observe that measure and conquer can be used as a form of
{\em computer aided algorithm design}.
Another intriguing question is whether we can automate some additional steps in the
design process, e.g., can we automatically obtain reduction rules from the 
solution of the quasiconvex program?


\begin{thebibliography}{10}

\bibitem{DavisP60}
M.~David and H.~Putnam.
\newblock A computing procedure for quantification theory.
\newblock {\em J. ACM}, 7:201--215, 1960.

\bibitem{Edmonds65}
J.~Edmonds.
\newblock Paths, trees, and flowers.
\newblock {\em Canad. J. Math.}, 17:445--467, 1965.

\bibitem{Eppstein04}
D.~Eppstein.
\newblock Quasiconvex analysis of backtracking algorithms.
\newblock In {\em Proceedings of the 15th Annual ACM-SIAM Symposium on Discrete
  Algorithms, SODA 2004}, pages 781--790, 2004.

\bibitem{FominGK05a}
F.~V. Fomin, F.~Grandoni, and D.~Kratsch.
\newblock Measure and conquer: Domination --- a case study.
\newblock In {\em Proceedings of the 32nd International Colloquium on Automata,
  Languages and Programming, ICALP 2005}, pages 191--203. Springer Verlag,
  Lecture Notes in Computer Science, vol.\ 3580, 2005.

\bibitem{FominGK05}
F.~V. Fomin, F.~Grandoni, and D.~Kratsch.
\newblock Some new techniques in design and analysis of exact (exponential)
  algorithms.
\newblock {\em Bulletin of the EATCS}, 87:47--77, 2005.

\bibitem{FominGK06}
F.~V. Fomin, F.~Grandoni, and D.~Kratsch.
\newblock Measure and conquer: a simple {} independent set
  algorithm.
\newblock In {\em Proceedings of the 16th Annual ACM-SIAM Symposium on Discrete
  Algorithms, SODA 2006}, pages 18--25, 2006.

\bibitem{FominGK06a}
F.~V. Fomin, F.~Grandoni, and D.~Kratsch.
\newblock Solving connected dominating set faster than .
\newblock In S.~Arun-Kumar and N.~Garg, editors, {\em Proceedings 26th
  International Comference on Foundations of Software Technology and
  Theoretical Computer Science, FSTTCS 2006}, pages 152--163. Springer Verlag,
  Lecture Notes in Computer Science, vol.\ 4337, 2006.

\bibitem{FominGK07}
F.~V. Fomin, F.~Grandoni, and D.~Kratsch.
\newblock A measure \& conquer approach for the analysis of exact algorithms.
\newblock Technical Report 359, Department of Informatics, University of
  Bergen, Bergen, Norway, 2007.

\bibitem{FominGPS05}
F.~V. Fomin, F.~Grandoni, A.~Pyatkin, and A.~A. Stepanov.
\newblock Bounding the number of minimal dominating sets: a measure and conquer
  approach.
\newblock In {\em Proceedings 16th International Symposium on Algorithms and
  Computation, ISAAC 2005}, pages 192--203. Springer Verlag, Lecture Notes in
  Computer Science, vol.\ 3827, 2005.

\bibitem{FominKW04}
F.~V. Fomin, D.~Kratsch, and G.~J. Woeginger.
\newblock Exact (exponential) algorithms for the dominating set problem.
\newblock In J.~Hromkovi\u{c}, M.~Nagl, and B.~Westfechtel, editors, {\em
  Proceedings 30th International Workshop on Graph-Theoretic Concepts in
  Computer Science WG'04}, page 245=256. Springer Verlag, Lecture Notes in
  Computer Science, vol.\ 3353, 2004.

\bibitem{GareyJ79}
M.~R. Garey and D.~S. Johnson.
\newblock {\em Computers and Intractability, A Guide to the Theory of
  {NP}-Completeness}.
\newblock W.H. Freeman and Company, New York, 1979.

\bibitem{GaspersL06}
S.~Gaspers and M.~Liedloff.
\newblock A branch-and-reduce algorithm for finding a minimum independent
  dominating set in graphs.
\newblock In F.~V. Fomin, editor, {\em Proceedings 32nd International Workshop
  on Graph-Theoretic Concepts in Computer Science WG'06}, pages 78--89.
  Springer Verlag, Lecture Notes in Computer Science, vol.\ 4271, 2006.

\bibitem{Grandoni06}
F.~Grandoni.
\newblock A note on the complexity of minimum dominating set.
\newblock {\em J. Disc. Alg.}, 4:209--214, 2006.

\bibitem{Karp72}
R.~M. Karp.
\newblock Reducibility among combinatorial problems.
\newblock In R.~E. Miller and J.~W. Thatcher, editors, {\em Complexity of
  Computer Computations}, pages 85 -- 104. Plenum Press, 1972.

\bibitem{KratschL06}
D.~Kratsch and M.~Liedloff.
\newblock An exact algorithm for the minimum dominating clique problem.
\newblock In H.~L. Bodlaender and M.~A. Langston, editors, {\em Proceedings 2nd
  International Workshop on Parameterized and Exact Computation, IWPEC 2006},
  pages 128--139. Springer Verlag, Lecture Notes in Computer Science, vol.\
  4169, 2006.

\bibitem{RanderathS05}
B.~Randerath and I.~Schiermeyer.
\newblock Exact algorithms for minimum dominating set.
\newblock Technical Report zaik2005-501, Universit\"{a}t zu K\"{o}ln, Cologne,
  Germany, 2005.

\bibitem{Robson86}
J.~M. Robson.
\newblock Algorithms for maximum independent sets.
\newblock {\em J. Algorithms}, 7:425--440, 1986.

\bibitem{Tarjan72a}
R.~E. Tarjan.
\newblock Finding a maximum clique.
\newblock Technical report, Department of Computer Science, Cornell University,
  Ithaca, NY, 1972.

\bibitem{TarjanT77}
R.~E. Tarjan and A.~Trojanowski.
\newblock Finding a maximum independent set.
\newblock {\em SIAM J. Comput.}, 6:537--546, 1977.

\bibitem{vanRooij06}
J.~M.~M. {van Rooij}.
\newblock Design by measure and conquer: An {} algorithm for
  minimum dominating set and similar problems.
\newblock Master's thesis, Institute for Information and Computing Sciences,
  Utrecht University, 2006.

\bibitem{Woeginger03}
G.~J. Woeginger.
\newblock Exact algorithms for {NP}-hard problems: {A} survey.
\newblock In {\em Combinatorial Optimization: ''Eureka, you shrink''}, pages
  185--207, Berlin, 2003. Springer Lecture Notes in Computer Science,
  vol.~2570.

\bibitem{Woeginger04}
G.~J. Woeginger.
\newblock Space and time complexity of exact algorithms: some open problems
  (invited talk).
\newblock In R.~G. Downey and M.~R. Fellows, editors, {\em Proceedings 1st
  International Workshop on Parameterized and Exact Computation, IWPEC 2004},
  pages 281--290, Berlin, 2004. Springer Lecture Notes in Computer Science,
  vol.~3162.

\end{thebibliography}

\newpage
\null

\end{document}