\documentclass{article}
\usepackage{amsmath,amssymb,enumerate,bbm,calc,capt-of,ifthen}
\usepackage{graphicx}\usepackage{epsfig}

\newtheorem{cntr}{do not use}
\newcommand{\tmop}[1]{\operatorname{#1}}
\newtheorem{definition}[cntr]{Definition}
\newtheorem{assumption}{Assumption}
\newcommand{\dueto}[1]{\textup{\textbf{(#1) }}}
\newtheorem{varremark}[cntr]{Remark}
\newenvironment{remark}{\begin{varremark}\em}{\em\end{varremark}}
\newcommand{\nin}{\not\in}
\newcommand{\tmem}[1]{{\em #1\/}}
\newtheorem{varnote}{Note}
\newenvironment{note}{\begin{varnote}\em}{\em\end{varnote}}
\newtheorem{proposition}[cntr]{Proposition}

\newenvironment{proof}{
  \noindent\textbf{Proof.}\ }{\hspace*{\fill}
  \medskip}

\newenvironment{proofof}[1]{
  \noindent\textbf{Proof of #1.}\ }{\hspace*{\fill}
  \medskip}

\newenvironment{proof*}[1]{
  \noindent\textbf{#1\ }}{\hspace*{\fill}
  \medskip}

\newtheorem{lemma}[cntr]{Lemma}
\newtheorem{corollary}[cntr]{Corollary}
\newtheorem{theorem}[cntr]{Theorem}
\newenvironment{enumeratenumeric}{\begin{enumerate}[1.]}{\end{enumerate}}
\newtheorem{algo}{Algorithm}

\numberwithin{cntr}{section}
\numberwithin{equation}{section}

\newcommand{\comment}[1]{}
\newcommand{\abs}[1]{\left| #1 \right|}\newcommand{\absSmall}[1]{| #1 |}\newcommand{\RR}[0]{{\mathbb{R}}}



\newcommand{\vx}[0]{{\vec{x}}}
\newcommand{\vp}[0]{{\vec{p}}}
\newcommand{\vq}[0]{{\vec{q}}}
\newcommand{\vr}[0]{{\vec{r}}}
\newcommand{\vm}[0]{{\vec{m}}}
\newcommand{\vmb}[0]{{\vec{\mathbf{m}}}}
\newcommand{\vv}[0]{{\vec{v}}}
\newcommand{\va}[0]{{\vec{a}}}
\newcommand{\vb}[0]{{\vec{b}}}
\newcommand{\vg}[0]{{\vec{g}}}


\newcommand{\oneto}[1]{{1 \ldots #1}}
\newcommand{\onetoN}[0]{{1 \ldots N}}
\newcommand{\Oto}[1]{{0 \ldots #1-1}}
\newcommand{\OtoN}{{0 \ldots N-1}}

\newcommand{\pointData}{{ \{ \vp_{i} \}_{i=\OtoN} }}
\newcommand{\tanData}{{ \{ \vm_{i} \}_{i=\OtoN} }}
\newcommand{\allData}{{ \{ \vp_{i}, \vm_{i} \}_{i=\OtoN} }}


\newcommand{\curveSet}{{ \{ \gamma_i(t) \}_{\Oto{M}}}}
\newcommand{\poly}{{\Gamma}}

\newcommand{\ball}[2]{ { B_{#1}(#2) } }
\newcommand{\allowed}[2]{ { A_{#1}(#2) } }

\newcommand{\curvemax}{{\kappa_{m}}}
\newcommand{\curvemaxi}{{\curvemax^{-1}}}

\newcommand{\curvesep}{{\delta}}

\newcommand{\pointNoise}{{\zeta}}
\newcommand{\tanNoise}{{\xi}}

\newcommand{\nallowed}[2]{ { A^{\pointNoise, \tanNoise}_{#1}(#2) } }

\newcommand{\densitymax}{{\rho_{m}}}


\begin{document}

\title{Reconstructing Curves from Points and Tangents}

\author{L. Greengard and C. Stucchio}

\maketitle

\begin{abstract}
Reconstructing a finite set of curves from an unordered set of sample points
is a well studied topic. There has been less
effort that considers how much better the reconstruction can be if
\emph{tangential} information is given as well.
We show that if curves are separated from each other by a
distance , then the sampling rate need only be 
for error-free reconstruction.
For the case of point data alone,  sampling is required.
\end{abstract}

\section{Introduction}

In this paper, we consider the problem of reconstructing a
 \emph{figure} -- that is, a family of curves  from a finite
set of data. More precisely, we assume we are given
an unorganized set of points , as well as \emph{unit} tangents to the points . Note that the tangents have no particular orientation; making the change  destroys no information.

\begin{definition}
  \label{def:polygonalization}
  A polygonalization of a figure  is a planar graph
 with the property that each vertex  is a point on some , and each edge connects points which are adjacent samples of some curve .
\end{definition}

Our goal here is to construct an algorithm which reconstructs the
polygonalization of a figure from the data defined above.
An example of a polygonalization is given in Figure \ref{fig:polygonalization}.

\begin{figure}
\setlength{\unitlength}{0.240900pt}
\ifx\plotpoint\undefined\newsavebox{\plotpoint}\fi
\sbox{\plotpoint}{\rule[-0.200pt]{0.400pt}{0.400pt}}\includegraphics[scale=0.5]{polygonalization_example.eps}
\caption{A figure and it's polygonalization, c.f. Definition \ref{def:polygonalization}. }
\label{fig:polygonalization}
\end{figure}

The topic of reconstructing figures solely from point data  has been the subject of considerable attention \cite{amenta98crust,amenta98new,dey99curve,hoppe92surface,amenta02simple, dey01reconstructing, edelsbrunner}. This is actually a more difficult problem, and only weaker results are possible. The main difficulty is the following; if the distance between two separate curves  and  is smaller than the sample spacing, then it is difficult to determine which points are associated to which curve. Thus, sample spacing must be , with  the distance between different curves.

Tangential information makes this task easier; in essence, if two points are nearby (say  and ), but  does not point (roughly) in the direction , then  and  should not be connected. This fact allows us to reduce the sample spacing to , rather than . This is to be expected by analogy to interpolation; knowledge of a function and its derivatives yields quadratic accuracy.

We should mention at this point related work on \emph{Surfels} (short for \emph{Surface Elements}). A surfel is a point, together with information characterizing the tangent plane to a surface at that point (and perhaps other information such as texture). They have become somewhat popular in computer graphics recently, mainly for rendering objects characterized by point clouds
\cite{882320,1103907,598521,1018057,344936,383300}.

In this work, we present an algorithm which allows us to reconstruct a curve from . We make two assumptions, under which the algorithm is provably correct.

\begin{assumption}
  \label{ass:curvature}
  We assume each curve  has bounded curvature:
  
\end{assumption}

This assumption is necessary to prevent the curves from oscillating too much between samples.

\begin{assumption}
  \label{ass:separation}
  We assume the curves  and  are uniformly separated from each other, i.e.:
  
      \label{eq:separationAssumption}
      \inf_{t,t'} \abs{ \gamma_{i}(t) - \gamma_{j}(t')} \geq \curvesep \textrm{~for~} i \neq j
    
      \label{eq:separationAssumptionSameCurve}
      \inf_{\abs{t-t'} > \curvemaxi\pi/2 } \abs{ \gamma_{i}(t) - \gamma_{i}(t')} \geq \curvesep
    
  (assuming the curve  proceeds with unit speed).
\end{assumption}

These assumptions ensure that two distinct curves do not come too close
together (\ref{ass:curvature})  and that separate regions of
the same curve do not come
arbitrarily close (\ref{ass:separation}).
This is illustrated in Figure \ref{fig:separationBetweenCurves}.

\begin{figure}
\setlength{\unitlength}{0.240900pt}
\ifx\plotpoint\undefined\newsavebox{\plotpoint}\fi
\sbox{\plotpoint}{\rule[-0.200pt]{0.400pt}{0.400pt}}\includegraphics[scale=0.5]{assumption_two.eps}

\caption{An illustration of Assumption \ref{ass:separation}. The black arrow illustrates the condition \eqref{eq:separationAssumption}, while the red arrow illustrates the condition \eqref{eq:separationAssumptionSameCurve}.}
\label{fig:separationBetweenCurves}
\end{figure}

\section{The Reconstruction Algorithm}

Before we begin, we require some notation.

\begin{definition}
  \label{def:perp}
  For a vector , let  denote the vector 
rotated clockwise by an angle .
\end{definition}

\begin{definition}
  \label{def:metric}
  Let  denote the usual Euclidean metric, . Let  denote the distance in the  direction between  and , i.e. .
\end{definition}

\begin{definition}
  For a point  and a curve , we say that  if  such that .
\end{definition}

\subsection{The Forbidden Zone}

Before explaining the algorithm which constructs the polygonalization of
a figure (the set of curves ) from discrete data , we
prove a basic lemma which forms the foundation of our method.
We assume for the remainder of this section that the figure
satisfies Assumption 1.

\begin{definition}
  For a point , we refer to the set

as its \emph{forbidden zone},
illustrated in Fig. \ref{fig:forbiddenZone}.
Here,  is the usual ball of radius  about .
\end{definition}

\begin{figure}
\setlength{\unitlength}{0.240900pt}
\ifx\plotpoint\undefined\newsavebox{\plotpoint}\fi
\sbox{\plotpoint}{\rule[-0.200pt]{0.400pt}{0.400pt}}\includegraphics[scale=0.5]{forbidden_zone.eps}
\caption{The forbidden zones, as described in Lemma \ref{lem:forbiddenZone}.
The orange (darker region) is the forbidden zone, and the blue (lighter region)
is the set of points a distance  away from .}
\label{fig:forbiddenZone}
\end{figure}

\begin{lemma}
\label{lem:forbiddenZone}
For every , if  is in the forbidden zone of ,
then  is not an edge in  assuming that the
sample spacing is less than .
\end{lemma}

\begin{proof}
Suppose for simplicity that  and . Now, consider a line  of maximal curvature. The curve of maximal curvature, with  and proceeding at speed  is , while the curve with  is .

By assumption 1, the curve  containing 
must lie between these curves (the near boundaries of the forbidden zone
in Fig \ref{lem:forbiddenZone}). Thus, it is confined to the blue (lighter)
region while its arc length is less than .
If  is in the forbidden zone and
 connects  to , then it must do so after travelling a distance greater than .
\end{proof}

In short, the extra information provided by the tangents
allows us to exclude edges from the polygonalization if they point too far away
from the tangent, resulting in higher fidelity (c.f. Fig. \ref{fig:proximityVsTangentBased}).

\begin{figure}
\setlength{\unitlength}{0.240900pt}
\ifx\plotpoint\undefined\newsavebox{\plotpoint}\fi
\sbox{\plotpoint}{\rule[-0.200pt]{0.400pt}{0.400pt}}\includegraphics[scale=0.5]{tangents_good_for.eps}

\caption{A naive proximity-based reconstruction algorithm (shown),
or even a -crust type algorithm, will introduce edges
between different curves. Knowledge of the forbidden zone allows us to
remove such edges.}
\label{fig:proximityVsTangentBased}
\end{figure}

\begin{definition}
  \label{def:AllowedRegion}
For a point , we define the \emph{allowed zone} or
\emph{allowed region}  by
  
  That is,  is the ball of radius  about  excluding the forbidden zone.
\end{definition}

Clearly, any edge in the polygonalization starting at , with length shorter than , must connect to another point .  We are now ready to describe the polygonalization algorithm.

\vspace{.2in}

\hrule
\begin{algo}
  \label{algo:polygonalization}
  \begin{center} {\bf (Noise-Free Polygonalization)}
  \end{center}

\vspace{.1in}

\hrule

\vspace{.2in}

\noindent { \bf Input: }
[ We assume we are given
the dataset ,
the maximal curvature , and
a parameter  satisfying both
 and .
We assume that adjacent points on a given curve
are less than a distance  apart, i.e. the curve is
-sampled. ]

\vspace{.1in}

  \begin{enumerate}
  \item Compute the graph  with edge set:
    
  \item For each vertex :
    \begin{enumerate}[a.]
    \item Compute the set of vertices
      
    \item Find the nearest tangential neighbors, i.e.
      
    \end{enumerate}
  \item Output the graph  with
    
    This graph is the polygonalization of .
  \end{enumerate}

\end{algo}

\begin{remark}
  As presented, the complexity of Algorithm \ref{algo:polygonalization} is , due to both step 1 and step 2. (Step 2 can be slow if  points are within the allowed region of some particular point). The complexity can be reduced to  using quadtrees if we assume a minimal sampling rate (see Appendix \ref{sec:quadTreeSection}).
\end{remark}

The following theorem guarantees the correctness of
Algorithm \ref{algo:polygonalization}. Its proof is presented
in the next section.

\begin{theorem}
  \label{thm:proofOfAlgo}
  Suppose that:
  
      \label{eq:constraintOnSeparationSampling}
      \curvesep > 2\curvemax \epsilon^{2}
    
      \label{eq:constraintOnkmaxEpsilon}
      \epsilon < \frac{1}{ \curvemax \sqrt{2}}
    
  Suppose also that the distance between adjacent samples in the polygonalization is bounded by , i.e. the curve is -sampled. Then graph  returned by Algorithm \ref{algo:polygonalization} is the polygonalization of .
\end{theorem}

\subsection{Proof of Theorem \ref{thm:proofOfAlgo}}

\begin{lemma}
  \label{lem:separationAllowedRegions}
  Suppose  and that Assumption \ref{ass:separation} holds.
Then for all , if \eqref{eq:separationCondition} holds, then
  
  Similarly, if  and , then \eqref{eq:connectionsBetweenDifferentCurvesNotAllowed} holds.
\end{lemma}
\begin{proof}
  Fix , and define  and . Define  to be the line segment . The boundaries of  are given by

Now, for any  and , the distance between  and  is the normal distance to . This distance is bounded by:

The intermediate value theorem implies  for some ; since  (by \eqref{eq:constraintOnkmaxEpsilon}), we find that:

Substituting this into \eqref{eq:1} yields:

Thus, the \emph{normal} distance between any point in  and  is .

If ,
then clearly 
so we assume . In this case,

for some 
and .
Thus, ,
the normal distance to . By construction, there is a unique
value  such that
.
 then equals .
By the second triangle inequality,

But this implies that , and thus .

The proof when  is identical.
\end{proof}

This result shows that the graph , computed in Step 1 of Algorithm \ref{algo:polygonalization}, separates different  and  from each other, as well as different parts of the same curve. Thus, after Step 1, we are left with a graph  having edges only between points  and  which are on the same curve , and which are separated along  by an arc length no more than .

We now show that  is a superset of the polygonalization .

\begin{proposition}
  \label{prop:polyIncludesNeighboringPoints}
  Suppose the point data  is -sampled, i.e. if two points  and  are adjacent on the curve , then the \emph{arc length} between  and  is bounded by . Then  contains the polygonalization of .
\end{proposition}
\begin{proof}
  If the distance between adjacent points  and  is at most , then . Since the segment of  between  and  has arc length less than ,  is not in the forbidden zone of  (by the same argument as in Lemma \ref{lem:forbiddenZone}. Thus,  (and vice versa), and  is an edge in .
\end{proof}

We have now shown that  separates distinct curves, and that  contains the polygonalization  of . It remains to show that .

\begin{lemma}
  \label{lem:localGraphParameterization}
  A curve  satisfying \eqref{eq:curvatureAssumption} admits the local parameterization
  
  where . The parameterization is valid for . In particular,  where .
\end{lemma}

\begin{proof}
  Taylor's theorem shows the parameterization to be valid on an arbitrarily small ball. All we need to do is show that this parameterization is valid on a region of size .

  The parameterization breaks down when  blows up, so we need to show that this does not happen before . Plugging this parameterization into the curvature bound \eqref{eq:curvatureAssumption} yields:
  
  Assuming  is positive, this is a first order nonlinear differential inequality for . We can integrate both sides (using the hyperbolic trigonometric substitution  for the left side) to obtain:
  
  With  defined as in the statement, then  is singular only at , and is regular before that. Solving \eqref{eq:2} for  shows that:
  
  implying that  is finite for , or .
\end{proof}

\begin{lemma}
  \label{lem:closestTangentPointInAllowedRegionIsCorrect}
  Fix a point . Choose a tangent vector  and fix an orientation, say . Consider the set of points  such that  is an edge in  and . Suppose also that  satisfies \eqref{eq:constraintOnkmaxEpsilon}.
Then, the only edge in the polygonalization of  is the edge for which  is minimal.
\end{lemma}

\begin{proof}
  By Lemma \ref{lem:localGraphParameterization}, the curve  can be locally parameterized as a graph near , i.e. \eqref{eq:localGraphParameterization}. This is valid up to a distance ; by \eqref{eq:constraintOnkmaxEpsilon}, it is valid for all points in the graph  connected to .

The adjacent points on the graph are the ones for which  is minimal. Note that  (simply plug in \eqref{eq:localGraphParameterization}); thus, minimizing  selects the adjacent point on the graph.
\end{proof}

The minimal edge is the edge  as computed in Step (2b) of Algorithm \ref{algo:polygonalization}.
Thus, we have shown that the computed graph  is the polygonalization
 of .

\begin{figure}
\setlength{\unitlength}{0.240900pt}
\ifx\plotpoint\undefined\newsavebox{\plotpoint}\fi
\sbox{\plotpoint}{\rule[-0.200pt]{0.400pt}{0.400pt}}\includegraphics[scale=0.5]{example1.eps}

\caption{Some unordered points/tangents, and the curves reconstructed from them. In this case, ,  and .}
\label{fig:basicExample}
\end{figure}

\section{Reconstruction in the Presence of Noise}

In practice one rarely has perfect data, so it is important to understand the performance of the approach in the presence of errors.
To that end, we consider the polygonalization problem, but with the point data perturbed by noise smaller than  and the tangent data perturbed by noise smaller than .
By this we mean the following; to each point , there exists a point  such that . Similarly, the unit tangent vector  differs from the true tangent  by an angle at most . By a polygonalization of the noisy data, we mean that  is an edge in the noisy polygonalization if  is an edge in the noise-free polygonalization. In what follows,  refers to a given (noisy) point, while  refers to the corresponding true point (and similarly for tangents).

Noise, of course, introduces a lower limit on the features we can resolve. At the very least, the curves must be separated by a distance greater than or equal to , to prevent noise from actually moving a sample from one curve to another. In addition, noise in the tangent data introduces uncertainty which forces us to increase the sampling rate; in particular, we require .

The main idea in extending Algorithm \ref{algo:polygonalization} to the noisy case is to expand the allowed regions to encompass all possible points and tangents. Of course, this imposes new constraints on the separation between curves.

We also require a \emph{maximal} sampling rate in order to ensure that the order of points on the curve is not affected by noise.
For work in the context of reconstruction using point samples only, see \cite{chengnoise,mdnoise}.

\begin{assumption}
  \label{ass:minSamplingRateNoisy}
  We assume that adjacent points  and  on the curve  are separated by a distance greater
  than .
\end{assumption}

To compensate for noise, we expand the allowed region to account
for uncertainty concerning the actual point locations.

\begin{definition}
  \label{def:AllowedRegionNoisy}
The \emph{noisy allowed region} 
is the union of the allowed regions of all points/tangents
near :
  
\end{definition}

\vspace{.2in}

\hrule
\begin{algo}
  \label{algo:polygonalizationNoisy}
  \begin{center} {\bf (Noisy Polygonalization)}
  \end{center}

\vspace{.1in}

\hrule

\vspace{.2in}

\noindent { \bf Input: }
[ We assume we are given the dataset , the maximal curvature
, the noise amplitudes , and a
parameter  satisfying both  and
. We assume that adjacent points on a given curve
are less than a distance  apart, i.e. the curve is
-sampled. ]

\vspace{.1in}

\begin{enumerate}
  \item Compute the graph  with edge set:
    
  \item For each vertex :
    \begin{enumerate}[a.]
    \item Compute the set of vertices
      
    \item Find the nearest tangential neighbors, i.e.
      
    \end{enumerate}
  \item Output the graph  with
    
    This graph is the polygonalization of .
  \end{enumerate}

\end{algo}

The following theorem
guarantees that Algorithm \ref{algo:polygonalizationNoisy} works.
The proof follows that of Theorem \ref{thm:proofOfAlgo}
and is given in Appendix \ref{sec:proofOfNoisyReconstruction}.
An application is shown in Fig. \ref{fig:noisyreconstruction}.

\begin{theorem}
  \label{thm:noisyReconstruction}
  Suppose that Assumptions \ref{ass:curvature}, \ref{ass:separation} and \ref{ass:minSamplingRateNoisy} hold. Suppose also that
  
      \label{eq:noisySeparationDistance}
      \curvesep > 4 \pointNoise + 4 \epsilon \tanNoise + 2.1 \curvemax \epsilon^{2} \, ,
    
      \label{eq:noisyConstraintOnkmaxEpsilon}
      \epsilon < \frac{1}{ \curvemax \sqrt{2}} \, .
    
  Then, Algorithm \ref{algo:polygonalizationNoisy} correctly reconstructs
the figure.
\end{theorem}

\begin{figure}
\setlength{\unitlength}{0.240900pt}
\ifx\plotpoint\undefined\newsavebox{\plotpoint}\fi
\sbox{\plotpoint}{\rule[-0.200pt]{0.400pt}{0.400pt}}\includegraphics[scale=0.50]{noisy_reconstruction.eps}
\caption{Noisy sampled points, and the reconstruction by Algorithm \ref{algo:polygonalizationNoisy}. This example takes , , . }
\label{fig:noisyreconstruction}
\end{figure}

\begin{remark}
Consider a point , which is a noisy sample from some curve in the figure.
All we can say a priori is that  is close to the true
sample , i.e. .
However, given the knowledge that the polygonalization contains
the edges  and , we can obtain further information
on . Not only does  lie in ,
but  and
. In short,

We can therefore improve our approximation to  by minimizing
either the worst case error,

    \vp^{new} = \textrm{argmin}_{\vp}  \sup_{\vx \in A} \abs{\vp - \vx}, ~ A = \ball{\pointNoise}{\vp} \cap \nallowed{\epsilon}{\vq} \cap  \nallowed{\epsilon}{\vr}
  
    \vp^{new} = \textrm{argmin}_{\vp}  \int_{A} \abs{\vp - \vx} d\vx
  
or some application-dependent functional.
Noise in the tangential data can be similarly reduced.
This is a postprocessing matter after polygonalization,
and we will not expanded further on this idea in the present paper.
\end{remark}

\section{Examples}

\subsection{Extracting Topology from MRI images}

In its simplest version, Magnetic Resonance Imaging (MRI) is
used to obtain the
two-dimensional Fourier transform of the proton density in a
planar cross-section through the patient's body.
That is, if  is
is the proton density distribution in the plane , then the MRI device
is able to return the data  at a selection of
points  in the Fourier transform domain (-space).
The number of sample points available, however, is finite and covers
only the low-frequency range in -space well.
Thus, it is desirable to be able to make use of the limited
information in an optimal fashion.
We are currently exploring methods for MRI based on
exploiting the assumption that
 is piecewise smooth (since different tissues have different
densities, and the tissues boundaries tend to be sharp).
Our goal is to carry out reconstruction in three steps.
First, we find the tissue boundaries (the discontinu-
ities). Second, we subtract the influence of the discontonuities from
the measured -space data and third, we reconstruct the remainder which
is now smooth (or smoother). Standard filtered Discrete Fourier Transforms
are easily able to reconstruct the remainder, so the basic
problem is that of reconstructing the edges.

Using directional edge detectors on the -space data, we can extract
a set of point samples from the edges, together with non-oriented normal
directions.  By means of
Algorithm \ref{algo:polygonalizationNoisy}, we can
reconstruct the topology of the edge set and carry out the
procedure sketched out above.
The details of the algorithm are beyond the scope of this article,
and will be reported at a later date,
but Figure \ref{fig:mriExample}
illustrates the idea behind the method. Our work on curve reconstruction was,
in fact, motivated by this application.

\begin{figure}
\setlength{\unitlength}{0.240900pt}
\ifx\plotpoint\undefined\newsavebox{\plotpoint}\fi
\sbox{\plotpoint}{\rule[-0.200pt]{0.400pt}{0.400pt}}\includegraphics[scale=0.5]{mri_edges.eps}
\caption{A simulated MRI image. The original image was two circles, together with some low frequency ``texture''. The noise level is 5\%.}
\label{fig:mriExample}
\end{figure}

\subsection{Figure detection}

A natural problem in various computer vision applications is that of
recognizing sampled objects that are partially obscured by a
complex foreground.
As a model of this problem, we constructed an (oval)
figure, and obscured it by covering it with a sequence of curves.
Algorithm \ref{algo:polygonalization} succesfully reconstructs the
figure, as well as properly connecting points on the
horizontally and vertically oriented covering curves.
The result is shown in
Figure \ref{fig:obscuredExample}. Note that the branches are not
connected to the oval (or each other).

\begin{figure}
\setlength{\unitlength}{0.240900pt}
\ifx\plotpoint\undefined\newsavebox{\plotpoint}\fi
\sbox{\plotpoint}{\rule[-0.200pt]{0.400pt}{0.400pt}}\includegraphics[scale=0.5]{obscured_figure.eps}
\caption{A figure which is partially obscured.
Algorithm \ref{algo:polygonalization} correctly computes its
polygonalization, and distinguishes it from the curves in front of it.
To avoid visual clutter, the tangents are not displayed in this figure.}
\label{fig:obscuredExample}
\end{figure}

\subsection{Filtering spurious points}

The method provided here is relatively robust with regard to the
addition of spurious random data points. This is because spurious data
points are highly unlikely to be connected to any other points in the
polygonalization graph. To see this, note
first that for an incorrect data point to be connected to part of the
polygonalization at all, it would need to be located in
 for some .
This is a region of length  and width .
There are approximately 
such points, for a total volume of . Thus, the probability
that a spurious point is in \emph{some} allowed region is roughly
.

The second reason is that even if a spurious point is in some allowed region,
it is unlikely to point in the correct direction.
If an erroneous point  is inside , it is
still not likely that , since
the tangent at  must point in the direction of 
(with error proportional to , the angular width of
). Thus, the probability that the tangent at
 points towards  is .
Combining these arguments, the probability that any \emph{randomly chosen}
spurious point  is connected to any other point in the
polygonalization is .

\begin{figure}
\setlength{\unitlength}{0.240900pt}
\ifx\plotpoint\undefined\newsavebox{\plotpoint}\fi
\sbox{\plotpoint}{\rule[-0.200pt]{0.400pt}{0.400pt}}\includegraphics[scale=0.5]{noisy_example.eps}
\caption{The same example as in Figure \ref{fig:basicExample}, but with
100 additional points (for a total of ), placed randomly. }
\label{fig:noisyExample}
\end{figure}

\subsubsection{Filtering the data}

The aforementioned criteria suggest that our reconstruction algorithm
has excellent potential for noise removal. It suggests that if we
remove points which do not have edges pointing towards other edges,
then with high probability we are removing spurious edges.

This notion is well supported in practice.
By running Algorithm \ref{algo:polygonalization} on a figure consisting of
 true points, and  randomly placed incorrect points, a nearly
correct polygonalization is calculated (Fig. \ref{fig:noisyExample}).
The original curve is reconstructed with an error at only one point
(the top left corner of the right-hand curve).

Of course, if enough incorrect points are present, some points will
eventually be connected by Algorithm \ref{algo:polygonalization}.
This can be seen in Figure \ref{fig:noisyExample}:
the line segment near  is an edge between two incorrect points.
One hint that an edge is incorrect is that it points to a leaf.
That is, consider a set of vertices 
as well as . Suppose, after approximately computing the
polygonalization, one finds that the graph contains edges
 and . The vertex  is
a leaf, that is it is reachable by only one edge. A polygonalization
of a set of closed curves should not have leaves, suggesting that the
edge  is spurious.
Thus filtering leaves is a very reasonable heuristic for noise filtering.

One final problem with noisy data worth mentioning is that sometimes,
an incorrect point will be present that lies within the allowed
region of a legitimate point, and closer to the legitimate point
than the adjacent points along the curve. This will prevent the
correct edge from being added. This can be remedied by adding not
only  at Step 3 of the algorithm, but also points for
which  whose distance to  is not
much longer than the distance between  and .
With some luck, this procedure combined with filtering out leaves
will approximately reconstruct the correct figure.

\vspace{.2in}

\hrule
\begin{algo}
  \label{algo:noisyPolygonalization}
  \begin{center}
  {\bf (Polygonalization with Noise Removal)}
  \end{center}

\vspace{.1in}

\hrule

\vspace{.2in}

\noindent { \bf Input: }

[ We assume we are given the dataset  (which includes spurious data),
the maximal curvature
, the noise amplitudes , and a
parameter  satisfying both  and
.
We assume that adjacent points on a given curve
are less than a distance  apart, i.e. the curve is
-sampled. We also assume we are given the number of
leaf removal sweeps  and a
threshold . ]

\vspace{.1in}

  \begin{enumerate}
  \item Compute the graph  with edge set:
    
  \item For each vertex :
    \begin{enumerate}[a.]
    \item Compute the set of vertices
      
    \item Find the nearest tangential neighbors, i.e.
      
    \item Find the set of almost-nearest tangential neighbors:
      
    \end{enumerate}
  \item Compute the graph  with
    
  \item Search through  for leaves, and remove edges pointing to the leaves. Repeat this  times.
  \item Output .
  \end{enumerate}
\end{algo}

In practice, we have found that  and
 work reasonably well.
Figure \ref{fig:moreNoisyExample} illustrates the result of Algorithm
\ref{algo:noisyPolygonalization}, both with and without filtering.

\begin{figure}
\setlength{\unitlength}{0.240900pt}
\ifx\plotpoint\undefined\newsavebox{\plotpoint}\fi
\sbox{\plotpoint}{\rule[-0.200pt]{0.400pt}{0.400pt}}\includegraphics[scale=0.5]{more_noisy_example.eps}
\caption{The same example as in Figure \ref{fig:basicExample}, but with
2000 additional random points added (for a total of 2096).
The original curve is no longer completely reconstructed, but the
general shape is still roughly visible, along with many more spurious
points. The middle figure shows the reconstruction
without Step 4 of Algorithm 3. Filtering leaves with  improves
the situation considerably (bottom figure).
\label{fig:moreNoisyExample}}
\end{figure}

\section{Conclusions}

Standard methods for reconstructing a finite set of
curves from sample data are quite general.
By and large, they assume that only point samples are given.
In some applications, however, additional information is available.
In this paper, we have shown that if both sample location and
tangent information are given, significant improvements can be made
in accuracy. We were motivated by a problem in medical imaging,
but believe that
the methods developed here will be of use in a variety of other applications,
including MR tractography and contour line reconstruction in topographic
maps \cite{GORE,TOPO}.

\appendix

\section{Proof of Theorem \ref{thm:noisyReconstruction}}
\label{sec:proofOfNoisyReconstruction}
The proof of Theorem \ref{thm:noisyReconstruction} follows that of
Theorem \ref{thm:proofOfAlgo} closely, with minor modifications
made to account for the noise.
To begin, we need to show that the noisy allowed region is large enough
to separate distinct curves. It is here that we use
\eqref{eq:noisySeparationDistance}.

\begin{proposition}
  Suppose  and assume that
  \eqref{eq:noisySeparationConditions} holds.
  Then  unless  and 
  are samples from the same curve, and are separated by an arc length no
  larger than .
\end{proposition}

\begin{proof}
  For simplicity, suppose that  (since otherwise, , but  is not sampled from the same curve as . Let  denote the curve from which  is sampled. Let  and  to simplify notation.

  Select points  to minimize , where , with the constraint that  and the angle between  and  is smaller than . Let  be the point for which .

  It is shown in the proof of Lemma \ref{lem:separationAllowedRegions} that if , then  (recall \eqref{eq:1}, \eqref{eq:4}). Thus, if  for any , then  for each  and hence . We will show this to be the case.



  By the second triangle inequality, we have the bound:
  
  where  is the point on  closest to . Once we show this is greater than , the proof is complete.

Let  (with  and  being true samples of , approximated by  and ). Then we have the bound:
  
  The bound on  follows since  is a sample from  (recalling \eqref{eq:1}, \eqref{eq:4}).

  Since  (for some ), we can perform the bound:
  
  In \eqref{eq:5}, we assume  and  are oriented the same way. It is easy enough to see that the sup is achieved at the endpoints; we then use the triangle inequality , and similarly for the tangents.
  Thus, we find that:
  
  Plugging this into \eqref{eq:7} shows that:
  
  where the last inequality follows from \eqref{eq:noisySeparationDistance}.
\end{proof}

This shows that the graph  computed in Step 1 separates distinct curves.

The next result parallels Proposition \ref{prop:polyIncludesNeighboringPoints}, and shows that the noisy allowed region contains nearby points on the polygonalization.

\begin{proposition}
  Suppose the figure is sampled at a rate satisfying \eqref{eq:constraintOnkmaxEpsilon}. Then  contains the polygonalization of the figure.
\end{proposition}

\begin{proof}
  The point  and tangent  are close to some point  on the figure ; in particular,  and  . Similarly, there is a point  on the figure a distance no more than  away from . By Proposition \ref{prop:polyIncludesNeighboringPoints}, . Since  and , we find that . Repeating this argument with  and  interchanged shows that \eqref{eq:noisyConditionForCheckingIfEdgeConnectionIsPlausible} holds, and  is an edge of .
\end{proof}

\begin{proposition}
  \label{lem:noisyClosestTangentPointInAllowedRegionIsCorrect}
  Fix a point , and suppose that Assumption \ref{ass:minSamplingRateNoisy} holds. Choose a tangent vector  and fix an orientation. Consider the set of points  such that  is an edge in  (as per Step 1 of Algorithm \ref{algo:polygonalizationNoisy}) and . Suppose also that  satisfies \eqref{eq:noisyConstraintOnkmaxEpsilon}.

  Then the nearest tangential neighbor of  (i.e. the edge for which  is minimal) is the edge in the polygonalization of .
\end{proposition}

\begin{proof}
  The idea of the proof follows that of Lemma \ref{lem:closestTangentPointInAllowedRegionIsCorrect} closely, but we must adjust for our uncertainty as to the point and tangent.

  The curve itself has the parameterization , by Lemma \ref{lem:localGraphParameterization}, and this is valid for . However, we do not know  and , only  and . We wish to find the point  for which  is minimal, and we approximate this by finding the point for which  is minimal.

Using the fact that , we find that

The second and third terms on the right side of \eqref{eq:3} are the error terms. We have the bound:

We wish to find the  for which \eqref{eq:3} is negative for every . If we can show that , we are done.

If we observe that  (using the notation of Lemma \ref{lem:localGraphParameterization}), and similarly , we find then that .

It is here we use the fact that . With  as in Lemma \ref{lem:localGraphParameterization}, we find that:

for some . If  (i.e. \eqref{eq:constraintOnkmaxEpsilon} is satisfied), then  and . Thus:

(the last inequality follows from Assumption \ref{ass:minSamplingRateNoisy}) implying that .
\end{proof}


\section{Speeding it up: an  algorithm}
\label{sec:quadTreeSection}

As remarked earlier, Algorithm \ref{algo:polygonalization} and \ref{algo:polygonalizationNoisy} run in  time as written. The slow step is Step 1 which involves comparing every point/tangent pair to every other such pair. This scaling issue can be remedied by using a spatially adaptive data structure
\cite{quadtrees}

A caveat: there are two different ways of increasing . The first (increasing outward) is by taking larger figures, with the sampling rate held fixed. The second (increasing inward) is by holding the figure size fixed, but increasing the sampling rate. We are interested primarily in the first case, and we will treat this case only. Therefore, we make the following additional assumption:

\begin{assumption}
  \label{ass:sampleDensity}
  We assume that the density of points in the input data is bounded above, i.e.:
  
\end{assumption}

Note that this always holds in the case of noisy data. In this case, Assumption \ref{ass:minSamplingRateNoisy} combined with \eqref{eq:noisySeparationConditions} implies that


In computing Step 1 of Algorithm \ref{algo:polygonalization} or \ref{algo:polygonalizationNoisy}, we must determine whether two points are in each other's allowed region (or a ball of radius  about the noisy allowed region). Note that , so if , then clearly the edge . Similarly, for the noisy case, if , then . We exploit this fact by using quadtrees, which allow us to avoid comparing points more than a distance  apart.

\vspace{.2in}

\hrule
\begin{algo}
  \label{algo:polygonalization}
  \begin{center} {\bf (Fast Computation of the Graph G)}
  \end{center}

\vspace{.1in}

\hrule

\vspace{.2in}

\noindent { \bf Input: }
[ We assume we are given the dataset , the maximal point density  and the sampling . We also take the parameter  (noise free case) or  (noisy case). ]

\vspace{.1in}

  \begin{enumerate}
  \item Compute a quadtree  storing  pairs. The splitting criteria for a node is when the node contains more than  points.
  \item Initialize the graph  with empty edge set.
  \item For each point , iterate over the points  contained in the node containing  and all of its nearest neighbors. If
    
    then add the edge  to the graph .
  \item Return .
  \end{enumerate}
\end{algo}
Initializing the quadtree in step 1 is an  operation. Assumption \ref{ass:sampleDensity} implies that the width of a node will be no smaller than ; thus, a node containing a point  together with it's nearest neighbors contains the allowed region. The comparison at step 3 involves at most  points, regardless of . Thus, the complexity of this algorithm is



\bibliographystyle{hplain}
\begin{thebibliography}{99}

\bibitem{882320}
Bart Adams and Philip Dutr\'{e}.
\newblock Interactive boolean operations on surfel-bounded solids.
\newblock {\em ACM Trans. Graph.}, 22(3):651--656, 2003.

\bibitem{1103907}
Marc Alexa, Markus Gross, Mark Pauly, Hanspeter Pfister, Marc Stamminger, and
  Matthias Zwicker.
\newblock Point-based computer graphics.
\newblock In {\em SIGGRAPH '04: ACM SIGGRAPH 2004 Course Notes}, page~7, New
  York, NY, USA, 2004. ACM.

\bibitem{amenta98crust}
N.~Amenta, M.~Bern, and D.~Eppstein.
\newblock The crust and the -skeleton: Combinatorial curve
  reconstruction.
\newblock {\em Graphical models and image processing: GMIP}, 60(2):125, 1998.

\bibitem{amenta98new}
Nina Amenta, Marshall Bern, and Manolis Kamvysselis.
\newblock A new {Voronoi}-based surface reconstruction algorithm.
\newblock {\em Computer Graphics}, 32({Annual Conference Series}):415--421,
  1998.

\bibitem{amenta02simple}
Nina Amenta, Sunghee Choi, Tamal~K. Dey, and N.~Leekha.
\newblock A simple algorithm for homeomorphic surface reconstruction.
\newblock {\em International Journal of Computational Geometry and
  Applications}, 12(1-2):125--141, 2002.

\bibitem{598521}
Rodrigo~L. Carceroni and Kiriakos~N. Kutulakos.
\newblock Multi-view scene capture by surfel sampling: From video streams to
  non-rigid 3d motion, shape and reflectance.
\newblock {\em Int. J. Comput. Vision}, 49(2-3):175--214, 2002.

\bibitem{chengnoise}
Siu-Wing Chen, Stefan Funke, Mordecai Golin, Piyush Kumar,
Sheung-Hung Poon, and Edgar Ramos.
\newblock Curve reconstruction from noisy samples.
\newblock {\em Comput. Geometry}, 31:63--100, 2005.

\bibitem{TOPO}
Y. Chen, R. Wang, and J. Qian.
\newblock Extracting contour lines from common-conditioned topographic maps.
\newblock {\em IEEE Transactions on Geoscience and Remote Sensing},
44(4):1048--1057, 2006.

\bibitem{dey99curve}
Tamal~K. Dey, Kurt Mehlhorn, and Edgar~A. Ramos.
\newblock Curve reconstruction: Connecting dots with good reason.
\newblock In {\em Symposium on Computational Geometry}, pages 197--206, 1999.

\bibitem{dey01reconstructing}
Tamal~K. Dey and Rephael Wenger.
\newblock Reconstructing curves with sharp corners.
\newblock {\em Computational Geometry}, 19(2--3):89--99, 2001.

\bibitem{edelsbrunner}
Herbert Edelsbrunner.
\newblock Shape reconstruction with Delaunay complex.
\newblock In {\em LATIN '98: Theoretical Informatics}, Lecture Notes
in Computer Science, 1380, Springer-Verlag, 119--32, 1998.

\bibitem{quadtrees}
Raphael Finkel and J.L. Bentley.
\newblock Quad Trees: A Data Structure for Retrieval on Composite Keys
\newblock In {\em Acta Informatica}, 4(1): 1--9, 1974.

\bibitem{GORE}
A. Mishra, Y. Lu, A. S. Choe, A. Aldroubi, J. C. Gore,
A, W. Andersona, Z. Ding.
\newblock An image-processing toolset for diffusion tensor tractography.
\newblock {\em Magnetic Resonance Imaging}, 25: 365--376, 2007.

\bibitem{hoppe92surface}
Hugues Hoppe, Tony DeRose, Tom Duchamp, John McDonald, and Werner Stuetzle.
\newblock Surface reconstruction from unorganized points.
\newblock {\em Computer Graphics}, 26(2):71--78, 1992.

\bibitem{1018057}
Thouis~R. Jones, Fredo Durand, and Matthias Zwicker.
\newblock Normal improvement for point rendering.
\newblock {\em IEEE Comput. Graph. Appl.}, 24(4):53--56, 2004.

\bibitem{mdnoise}
Asisn Mukhopadhyay and Augustus Das.
\newblock Curve reconstruction in the presence of noise.
\newblock In {\em CGIV 2007: 4th International Conference on
Computer Graphics, Imaging and Visualization}, pp. 177--182,
IEEE Compuer Society, 2007.

\bibitem{344936}
Hanspeter Pfister, Matthias Zwicker, Jeroen van Baar, and Markus Gross.
\newblock Surfels: surface elements as rendering primitives.
\newblock In {\em SIGGRAPH '00: Proceedings of the 27th annual conference on
  Computer graphics and interactive techniques}, pages 335--342, New York, NY,
  USA, 2000. ACM Press/Addison-Wesley Publishing Co.

\bibitem{383300}
Matthias Zwicker, Hanspeter Pfister, Jeroen van Baar, and Markus Gross.
\newblock Surface splatting.
\newblock In {\em SIGGRAPH '01: Proceedings of the 28th annual conference on
  Computer graphics and interactive techniques}, pages 371--378, New York, NY,
  USA, 2001. ACM.

\end{thebibliography}
\end{document}
