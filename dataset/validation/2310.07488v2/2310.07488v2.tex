
\documentclass{article} 

\usepackage{iclr2024_conference,times}
\usepackage{CJKutf8}


\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}
\usepackage{inconsolata}
\usepackage{CJKutf8}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{pifont}
\usepackage{tabu}
\usepackage{booktabs}
\usepackage{color}
\usepackage{subcaption}
\usepackage{tablefootnote}
\usepackage{wrapfig}
\usepackage{caption}
\usepackage{threeparttable}
\usepackage{colortbl}
\title{KwaiYiiMath: Technical Report}



\author{Jiayi Fu, Lei Lin, Xiaoyang Gao, Pengli Liu, Zhengzong Chen, Zhirui Yang, Shengnan Zhang, \\
\textbf{Xue Zheng, Yan Li, Yuliang Liu, Xucheng Ye, Yiqiao Liao, Chao Liao, Bin Chen, Chengru Song, } \\
\textbf{Junchen Wan, Zijia Lin, Fuzheng Zhang, Zhongyuan Wang, Di Zhang, Kun Gai} \\
\\ \textbf{Kuaishou Technology} \\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}

\renewcommand{\thefootnote}{}
\footnotetext{Corresponding author: wanjunchen@kuaishou.com.}
\renewcommand{\thefootnote}{\arabic{footnote}}

\maketitle

\begin{abstract}
Recent advancements in large language models (LLMs) have demonstrated remarkable abilities in handling a variety of natural language processing (NLP) downstream tasks, even on mathematical tasks requiring multi-step reasoning. In this report, we introduce the \textbf{KwaiYiiMath} which enhances the mathematical reasoning abilities of KwaiYiiBase\footnote{KwaiYiiBase is a large language model developed by Kuaishou \url{https://github.com/kwai/KwaiYii/}.}, by applying Supervised Fine-Tuning (SFT) and Reinforced Learning from Human Feedback (RLHF), including on both English and Chinese mathematical tasks. Meanwhile, we also constructed a small-scale Chinese primary school mathematics test set (named KMath), consisting of 188 examples to evaluate the correctness of the problem-solving process generated by the models. Empirical studies demonstrate that KwaiYiiMath can achieve state-of-the-art (\textbf{SOTA}) performance on GSM8k, CMath, and KMath compared with the similar size models, respectively.


\end{abstract}

\section{Introduction}

Recent advances in large language models (LLMs) 
have revolutionized the natural language processing (NLP) landscape~\cite{kenton2019bert,brown2020language}, where scaling up model size and the amount of data is one of the key ingredients~\cite{rae2021scaling,chowdhery2022palm,anil2023palm,touvron2023llama,touvron2023llama2}.
State-of-the-art models trained on vast amounts of data with extremely large model sizes, such as ChatGPT~\cite{openai2022chatgpt}, GPT-4~\cite{openai2023gpt4} and PaLM2~\cite{anil2023palm}, have shown unprecedented performance on a wide range of NLP tasks~\cite{brown2020language,rae2021scaling,du2022glam,lewkowycz2022solving,chowdhery2022palm,ouyang2022training,tay2022ul2,openai2022chatgpt,openai2023gpt4,anil2023palm,touvron2023llama2}.






Surprisingly, recent progress suggests that LLMs also have the potential to solve reasoning problems~\cite{clark2020transformers,talmor2020leap,suzgun2022challenging,wei2022chain}. Specifically, LLMs can perform soft deductive reasoning over natural language descriptions with \textit{implicit knowledge} stored in their parameters~\cite{wei2022chain,kojima2022large,fu2022complexity,shi2022language,zhang2022automatic,zhou2022teaching,diao2023active,shum2023automatic} or \textit{explicit knowledge} in external resources~\cite{wang2022iteratively,creswell2022selection,zhou2022least,press2022measuring,dua2022successive,reppert2023iterated}, and perform step-by-step reasoning just with a few demonstrations or instructions via chain-of-thought prompting (CoT)~\cite{wei2022chain}.


In this report, we focus on how to enhance the mathematical reasoning capabilities of LLM through an alignment process that includes supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF).
Specifically, we introduce the KwaiYiiMath which is finetuned with human alignment techniques from KwaiYiiBase to tackle
mathematical problems.

Experimental results show that KwaiYiiMath outperforms many open-source models in similar sizes by a large margin and is approaching GPT-4 on three mathematical benchmarks including both English and Chinese, i.e., GSM8k~\cite{cobbe2021training}, CMath~\cite{wei2023cmath}, and a small-scale in-house dataset KMath.

The structure of this report is as follows: Section 2 provides an overview of related work including LLM and LLMs' reasoning ability.
Section 3 introduces the methodology of KwaiYiiMath including the process of supervised fine-tuning and human preference alignment.
Additionally, it also describes details about the efforts in collecting large amounts of mathematical high-quality training data.
In Section 4, we report the experimental results on two public benchmarks and an in-house dataset.
Section 5 concludes this report and points out the future work of KwaiYiiMath.


\section{Related Work}

\paragraph{Large Language Models}
Nowadays, the advent of LLMs encourages the rethinking of the possibilities of artificial general intelligence (AGI). A recent report has even argued that GPT-4 might be \textit{an early version of AGI system}~\cite{bubeck2023sparks}. The success of LLMs consists of three major aspects, including \textit{pre-training} (how to pre-train a LLM based on large amounts of unlabelled data), \textit{adaption} (how to adapt the LLMs for better interaction with humans) and \textit{utilization} (how to use LLMs for solving various downstream tasks)~\cite{zhao2023survey}. By pre-training on large-scale corpora with language modeling objective, LLMs can acquire essential language understanding and generation abilities~\cite{brown2020language,chowdhery2022palm}.\footnote{We omit most details in \textit{pre-training} since KwaiYii Math only concentrates on how to enhance the mathematical reasoning
abilities of KwaiYiiBase.}

However, obviously, one of the major issues is the gap between the training objective and users' objective: users want the model to ``understand and follow their instructions'' while the LLMs are designed to predict the next token. To bridge the gap, instruction tuning (IT) and alignment tuning (AT) are proposed to enhance the capabilities and controllability of LLMs, where IT mainly aims to unlock the abilities of LLMs while the purpose of AT is to align the behaviors of LLMs with human preferences. It refers to the process of further fine-tuning pre-trained LLMs on a collection of formatted instance pairs (i.e., (\textit{instruction}, \textit{output})), where \textit{instruction} and \textit{output} denote the human instruction and the desired output generated by the LLM that follows the human instruction, respectively. Specifically, we first need to collect or construct (\textit{instruction}, \textit{output}) pairs, including manually constructed formatted instances~\cite{mishra2021cross,victor2022multitask,muennighoff2022crosslingual,wang2022super,longpre2023flan,zhou2023lima,conover2023free,kopf2023openassistant}, and automatically constructed formatted instances~\cite{wei2021finetuned,bach2022promptsource,honovich2022unnatural,wang2022self,xu2023wizardlm,xu2023baize,ji2023towards}. Note that it has been widely shown that the number of tasks, the quality and diversity of instruction instances poses an important impact on the performance of LLMs~\cite{ouyang2022training,victor2022multitask,wei2021finetuned,wang2022super,chung2022scaling,taori2023stanford,zhou2023lima}. Then, we use these formatted instances or carefully selected formatted instances to further fine-tune LLMs in a supervised learning way (also known as SFT) according to different requirements. For example, we can use formatted instances of different modality, domains and applications to obtain different specific LLMs, including multimodal LLMs (InstructPix2Pix~\cite{brooks2023instructpix2pix}, LLaVA~\cite{liu2023visual}, Video-LLaMA~\cite{zhang2023video}, InstructBLIP~\cite{dai2023instructblip}, MultiModal-GPT~\cite{gong2023multimodal}), and domain and application specific LLMs (InstructDial~\cite{gupta2022instructdial}, LINGUIST~\cite{rosenbaum2022linguist}, InstructUIE~\cite{wang2023instructuie}, Writing-Alpaca~\cite{zhang2023multi}, Radiology-GPT~\cite{liu2023radiology}, ChatDoctor~\cite{yunxiang2023chatdoctor}, Goat~\cite{liu2023goat}, WizardCoder~\cite{luo2023wizardcoder}, etc). However, the SFT process seems not to be stable enough due to the small amount of data and large model size. So, some studies focus on how to combine instruction tuning and pre-training, mainly consisting of two directions: a one-stage process (pre-trained from scratch with a mixture of pre-training data and instruction tuning data)~\cite{raffel2020exploring,zeng2022glm} and two-stage process (fine-tuned with a mixture of pre-training data and instruction tuning data)~\cite{iyer2022opt}. AT refers to the process of reinforcement learning from human feedback (RLHF) for better aligning LLMs with human preferences~\cite{ouyang2022training}. RLHF mainly comprises three key components: an LLM after SFT, a reward model learning to reflect human feedback for the text generated by the LLM, and a reinforcement learning (RL) algorithm (e.g., Proximal Policy Optimization (PPO)~\cite{schulman2017proximal}) to align the LLM based on the guidance signals generated by the reward model. 
We train a reward model to predict the human-preferred output based on the collected human feedback data and align the LLM based on the reward signals generated by the reward model using RL algorithm.\footnote{We omit the details of alignment criteria and human feedback collection for brevity.}

After pre-training and adaption, LLMs can serve as a general-purpose language task solver (to some extent) by simply conditioning the models on a few examples (few-shot) or instructions describing the task (zero-shot). The success of LLMs is often attributed to few-shot (in-context) or zero-shot learning (i.e., emergent abilities~\cite{wei2022emergent} that may not be observed in previous smaller language models).\footnote{Note that emergent abilities may not occur in some LLMs.} This leads to the rapid development of the ``prompting'' technique, revolutionizing the way that humans develop and use AI algorithms. Thus, designing prompts has become a hot topic in NLP, including demonstration selection~\cite{liu2021makes,rubin2021learning,xie2021explanation,zhang2022active,kim2022self,lee2022does,levy2022diverse,su2022selective,ye2022complementary}, and demonstration order~\cite{liu2021makes,lu2021fantastically}.

\paragraph{Reasoning with Large Language Models}
Reasoning, the process of making inferences based on existing knowledge, is the core of human intelligence. However, existing LLMs have struggled to achieve high performance on \textit{system-2} tasks requiring slow and multi-step reasoning such as mathematical and commonsense reasoning~\cite{rae2021scaling}. To enhance the reasoning ability of LLMs prompting, there are two major directions: strategy-enhanced reasoning and knowledge-enhanced reasoning~\cite{qiao2022reasoning}. Strategy-enhanced reasoning refers to design a better reasoning strategy, such as the prompt design in single reasoning stage~\cite{wei2022chain,kojima2022large,fu2022complexity,shi2022language,zhang2022automatic,zhou2022teaching,diao2023active,shum2023automatic}, reasoning stage by stage~\cite{wang2022iteratively,creswell2022selection,zhou2022least,press2022measuring,dua2022successive,reppert2023iterated}, natural language rationales optimization~\cite{ye2022unreliability,wang2022self,huang2022large,li2023making,weng2023large,yoran2023answering,shinn2023reflexion,madaan2023self,paul2023refiner}, and external engine augmentation~\cite{liu2022mind,chen2022program,gao2023pal,lyu2023faithful,imani2023mathprompter}. Knowledge enhanced reasoning refers to prompt LLMs with \textit{implicit} knowledge stored in LLM~\cite{li2022explanations,wang2022pinto,magister2022teaching,ho2022large,fu2023specializing} or \textit{explicit} knowledge in external resources~\cite{yang2022logicsolver,su2022selective,lu2022dynamic,he2022rethinking}, such as wiki, the reasoning steps, etc.

\section{Method}
In this section, we introduce the details of KwaiYiiMath.
Figure~\ref{fig:framework} shows the overview of our model.
Specifically, the left part of Figure~\ref{fig:framework} shows the training process overview of KwaiYiiMath, and it mainly consists of two steps, supervised fine-tuning and human preference alignment.
The right part of Figure~\ref{fig:framework} shows the main three components, including SFT data collection, human preference data collection, and human preference alignment training.
We first collect high-quality mathematical instruction data, in the form of \textit{question, answer} pairs, to do the supervised fine-tuning.
Next, for each question, we first generate \emph{K} different answers from SFT models and actor models from reinforcement learning respectively, then classify them into good answers and bad answers with the help of human annotation.
In this way, we can collect a large amount of high-quality data representing human preference and then use this data to train the reward model and do human preference alignment training.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=1.0\linewidth]{figs/framework.pdf}
    \caption{The overview of KwaiYiiMath. The left part is the framework of the model training process. The right part shows the details of three main components, including the SFT data collection, human preference data collection, and human preference alignment training. The data used by RM, PPO, and DPO are only in the same form and collection method, but their training datasets are different.}
    \label{fig:framework}
\end{figure*}


\subsection{Supervised fine-tuning}
Previous work has shown that the diversity and quality of instruction data poses an important impact on the SFT performance~\cite{zhou2023lima,peng2023instruction,chen2023alpagasus}, a conclusion that also holds true in LLMs' mathematical reasoning ability~\cite{yuan2023scaling,luo2023wizardmath,abel,yu2023metamath}. Therefore, we focus on how to collect or construct high-diversity and high-quality instruction data. 

To obtain as much diversity as possible in math data, we first collect math data from a wide range of sources, including different difficulties (e.g., primary school, middle school, and university, etc.), and different fields of math (e.g., algebra, geometry, and probability, etc.). 
Then, we generate intermediate rationales for math questions only with the final answer or without the answer using open-source LLMs and ensure the correctness of intermediate rationales and answers through manual annotation.
We try to construct intermediate rationales for all mathematical instruction data since Chain-of-Thought (CoT)~\cite{wei2022chain} has been proven effective either in prompting or instruction data for fine-tuning~\cite{ho2022large,zhu2022solving,li2023symbolic}. 
In addition to mathematical data, we also sample 300k open-domain conversations from KwaiYiiChat\footnote{KwaiYiiChat is the finetuned model from KwaiYiiBase for open-domain conversations.} training data to maintain the model's ability to handle open-domain questions.
More details of SFT data collection are as follows.











\subsubsection{Data Diversity}
We consider the diversity of instruction data mainly from two aspects: the diversity of instructions and responses, respectively. Figure~\ref{fig:fig1} illustrates the construction process of SFT data with an example. 


\paragraph{Instruction Diversity} Inspired by Evol-Instruct~\cite{xu2023wizardlm, luo2023wizardcoder} which uses LLM instead of humans to generate diverse instructions through a manual designed set of evolutionary actions, we designed Dual-evolving actions in depth and Constrained-mutation evolving actions in breadth for math instruction.
Nearing completion of our work, the authors of Evol-Instruct~\cite{xu2023wizardlm, luo2023wizardcoder} had also adapted their idea to the math LLM and further proposed Reinforcement Learning from Evol-Instruct Feedback~\cite{luo2023wizardmath} which combines Evol-Instruct with reinforcement learning.
However, the Evol-Instruct actions we use are somewhat different. 
\begin{figure*}[!t]
    \centering
    \includegraphics[width=1.0\linewidth]{figs/kuaishou_tech-report-data_diversity.pdf}
\caption{An example of SFT data diversity augmentation. We chose a sample from the train set of GSM8k. The upper part shows the instruction diversity augmentation process. There are two new instructions that are generated through the in-depth evolving and in-breadth evolving, respectively. The bottom part shows the response diversity augmentation process. There are four new responses that are generated through diverse reasoning path construction and one of them is filtered out due to wrong calculation, even the final answer is correct.}
    \label{fig:fig1}
\end{figure*}
Specifically, we consider a logically complex mathematical problem  is constructed by a series of simple sub-problems . 
The original problem  naturally has a CoT relation within sub-problems . 
The math-solving process can be considered as the CoT process of these sub-problems. 
Therefore, how to retain the CoT process in Evol-Instruct is a matter. 
Evol-Instruct mentioned in~\citet{luo2023wizardmath} evolves original instruction  to , which is a one-to-one paradigm no matter Downward evolution or Upward evolution action and will hurt the CoT process of  when making Downward evolution especially. 
We propose Dual-evolving actions in depth to retain the CoT process and expand instruction diversity in the meantime. 
Specifically, Dual-evolving actions include two steps:
\begin{itemize}
    \item First we design prompts to decompose a mathematical problem  into multiple sub-problems . Each sub-problem is individual from each other, and the CoT process consists of the order of sub-problems, which is a one-to-many paradigm of Downward evolution.
    \item Second we design prompts to increasingly enhance the difficulty of sub-problems and further improve the diversity of instructions. Since our method consists of two steps, we call it a Dual-evolving action.
\end{itemize}




In the in-breadth evolving, we use a constrained-evolving action that is inspired by the mutation evolution~\cite{xu2023wizardlm}. 
Specifically, We designed a prompt to evolve new problems based on existing problems within a constrained scope.
The purpose of adding scope constraints is to avoid evolving actions that lead to unsolvable mathematical problems.





\paragraph{Response Diversity}
Previous work~\cite{ho2022large,yuan2023scaling} has shown that diverse reasoning paths can improve the reasoning performance of LLM, including: for a given instruction sample in a training set, using multiple different models or sampling strategies to generate multiple reasoning paths.


Inspired by that, we collect various available LLMs including open-source LLMs such as Llama~\cite{touvron2023llama,touvron2023llama2} of different sizes, etc and different versions of KwaiYiiMath. Then, we use these models to generate multiple reasoning paths given each question. 
Specifically, we fine-tune open-source LLMs on mathematical datasets in a supervised fashion to 
obtain the ability to generate more correct reasoning paths.\footnote{Different versions of KwaiYiiMath are already fine-tuned on mathematical datasets in a supervised fashion.} To further augment such abilities of LLMs and improve the diversity of reasoning paths, we collect diverse CoT prompts,\footnote{https://github.com/FranxYao/chain-of-thought-hub/tree/main/gsm8k/lib\_prompt} and use different prompting strategies such as zero-shot, zero-shot CoT and few-shot CoT. For each question , we generate  candidate reasoning paths and answers ,  with a temperature of 0.7 following~\citet{cobbe2021training} and one of prompting strategies based on different prompts. We first filter out reasoning paths with wrong answers  or wrong calculations where equations are extracted from reasoning paths.\footnote{Note that we only evaluate the correctness of extracted complete equation (e.g., 3  4  7) instead of incomplete equation (e.g.,  4  7).} We retain all reasoning paths with the same equation list as the augmented data unlike~\cite {yuan2023scaling} since we argue that diverse contexts also pose an important impact on the reasoning performance of LLMs.






\subsubsection{Data Quality}
The most important thing for mathematical data is the correctness of the calculation process and the final answers of the response.
We denote  is the mixed datasets, where  are a question and a reasoning path respectively,  denote equation set,  is the final answer, and  denote the ground truth reasoning answer of . LLMs often make calculation or conclusion mistakes in the reasoning path~\cite{gao2023pal,chen2022program} such as  or . 

In order to attain high-quality data, we make an effort to ensure the correctness of both the final answers and calculation processes. Specifically, we first use LLM-generated responses to extract the final answers  and then filter out reasoning paths  with wrong answers . Then, we use the regular expression to extract the equation set  in response and utilize a Python interpreter to evaluate the correctness of the response. As for the single true answer queries, we control the both correctness of the final answer and the calculation process. As for the multiple true answer queries, we only control the correctness of the calculation process. 





\subsection{Human Preferences Alignment}
Despite the significant performance of fine-tuned LLMs on mathematical reasoning abilities, they are still prone to generate content that contains reasoning errors, incorrect answers, or redundant inference processes. We argue that the performance enhancement of LLMs not only derives from supervised fine-tuning but also from human preferences alignment methods. Therefore, we use two scalable alignment frameworks, reinforcement learning from human feedback (RLHF) and DPO~\cite{rafailov2023direct}, both are learning from human preferences for training aligned language models and improving mathematical reasoning abilities and answer correctness.

\subsubsection{Reinforcement Learning From Human Feedback}
RLHF is to apply reinforcement learning directly on LLMs with human preferences as feedback, and developing rapidly in the language models alignment field. Inspired by InstructGPT, we propose a classic RLHF training pipeline that consists of two phases: 1) human preferences comparison data collection and reward model training; 2) reinforcement learning with PPO.

\textbf{Reward Model}: The reward model is used to evaluate the quality of the SFT  generation from the aspect of mathematical result and procedure, as well as human preference.

Given an input, we sample a pair of responses from our SFT and PPO of different versions so that RM can capture diverse data distribution. Some existing open-source preference datasets are also combined to improve the generalization of RM, such as Anthropic Helpful and Harmless~\cite{bai2022training}, OpenAI WebGPT~\cite{nakano2021webgpt}. 
 
The binary ranking loss function we use is consistent with~\cite{ouyang2022training}. We held out 5,000 examples as a test set to evaluate our model. The results are reported in Table~\ref{table:rm}. As a reference point, we also evaluate other publicly available alternative solutions as baselines: the Open Assistant reward model based on DeBERTa V3 Large~\cite{he2020deberta}, and SteamSHP-XL~\cite{ethayarajh2022understanding} based on FLAN-T5-xl.

\textbf{PPO}: PPO is the most famous reinforcement learning method used in RLHF. It utilizes scores from reward models as human feedback signals to fine-tune LLMs. By randomly sampling prompts from SFT datasets and using policy-generated responses, we aim to enable fine-tuned models aligned with human values. We also find it is crucial to whiten reward scores because of the reward hacking issue and increase the training stability.

\subsubsection{Direct Preference Optimization (DPO) }
RLHF methods need a reward model to fit with human preferences, then optimize language models with reinforcement learning to generate high reward score answers. However, they require a large consumption of computational resources and complex large-scale distributed settings. The number of hypermeters in RLHF methods also increases the difficulty of the stability of optimizing results. 

Direct Preference Optimization (DPO) is an RL-free method and easy to implement. By eliminating the reward model, DPO only considers the policy model and reference model probability distributions and uses a simple binary cross-entropy objective to optimize language models from human preferences. DPO training data has a consistent pattern with the reward model data used for training. To effectively improve the mathematical reasoning abilities of KwaiYiiMath, we sample a subset of reward model data and conduct DPO training on the fine-tuned model.

\section{Experiments}
We mainly evaluate KwaiYiiMath on three comprehensive and realistic benchmarks for measuring mathematical reasoning ability, including two public benchmarks (GSM8k~\cite{cobbe2021training} and CMath~\cite{wei2023cmath}), and an in-house dataset (KMath).


\begin{wraptable}{r}{0.6\linewidth}
\centering
\vspace{-1.0em}
\caption{Results on our Chinese and English test set of human preference benchmarks. }
\label{table:rm}
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{@{\hspace{5pt}}ccc}
\toprule
  RM Model & \bf KwaiYiiMath-en & \bf KwaiYiiMath-zh \\
\midrule
  Open Assistant & 63.79 & 65.80 \\
  SteamSHP-XL    & 55.90 & 54.43 \\
\midrule
  KwaiYiiMath-RM & \bf 77.30 (+13.51) & \bf 78.48 (+12.68) \\
\bottomrule
\end{tabular}
}
\vspace{-0.7em}
\end{wraptable}
\subsection{Evaluation Datasets}
GSM8k~\cite{cobbe2021training} contains 7,473 training examples and 1,319 test examples, mainly on grade school-level English math problems. Each question consists of basic arithmetic operations (addition, subtraction, multiplication, and division), and generally requires 2 to 8 reasoning steps to solve. 

CMath~\cite{wei2023cmath} is a Chinese elementary school math word problems dataset that comprises 1.7k\footnote{The CMath data set contains a total of 1.7k data, of which 960 are currently available for download.} elementary school-level math word problems with detailed annotations, sourced from actual Chinese workbooks and exams. 
CMath also has fine-grained annotations, including grade, number of reasoning steps, digits, and distractors.
These annotations can be used to evaluate the LLM's fine-grained mathematical reasoning ability and robustuess mathematical reasoning ability.

KMath is a small-scale in-house mathematical dataset that contains 188 Chinese math questions.
The questions of KMath are mainly on the grade school level, which consists of algebra, calculus, geometry, and probability.

\begin{table*}[t]
  \centering
  \caption{Results of pass@1 (\%) on GSM8k, CMath and KMath. The character  denotes that results are attained from the related works, and the remaining results are attained from our tests. The character  shows our best result from different human preferences alignment experiments.
  }
  \label{table:performance}
  \begin{tabular}{@{}lccccc@{}}
      \toprule
      \bf Model & & \bf \#Params & \bf GSM8K & \bf CMath & \bf KMath \\
      \midrule
      \multicolumn{6}{c}{\textit{Closed-source models}} \\
      \midrule
     
      \multicolumn{2}{@{}l}{GPT-4~\cite{openai2023gpt4}}  & - &  & 86.00 & 75.00 \\

      \arrayrulecolor{lightgray}\midrule
      \multicolumn{2}{@{}l}{ChatGPT~\cite{openai2022chatgpt}}  & - &  & 73.83 & 59.57\\
      


      \arrayrulecolor{lightgray}\midrule
      \multicolumn{2}{@{}l}{\multirow{3}{*}{Minerva~\cite{lewkowycz2022solving}}} & 8B &  & - & - \\ \multicolumn{2}{@{}l}{} & \color[HTML]{9B9B9B}62B & \color[HTML]{9B9B9B} & \color[HTML]{9B9B9B}- & \color[HTML]{9B9B9B}- \\  \multicolumn{2}{@{}l}{} & \color[HTML]{9B9B9B}540B & \color[HTML]{9B9B9B} & \color[HTML]{9B9B9B}- & \color[HTML]{9B9B9B}- \\
      
      \arrayrulecolor{lightgray}\midrule
      \multicolumn{2}{@{}l}{Ernie Bot~\cite{ernie2023}} & - & 56.23 & 84.33 & 72.87\\
      
      \arrayrulecolor{lightgray}\midrule
\multicolumn{2}{@{}l}{\multirow{2}{*}{MATH-QWEN-CHAT~\cite{qwen2023qwen}}} & 7B &  & - & - \\ \multicolumn{2}{@{}l}{} & \color[HTML]{9B9B9B}14B & \color[HTML]{9B9B9B} & \color[HTML]{9B9B9B}- & \color[HTML]{9B9B9B}- \\
      
      \arrayrulecolor{black}\midrule
      \multicolumn{6}{c}{\textit{Open-source models}} \\
      \arrayrulecolor{black}\midrule
      
      
      \multicolumn{2}{@{}l}{\multirow{2}{*}{LLaMA-1~\cite{touvron2023llama}}} & 13B &  & - & - \\ \multicolumn{2}{@{}l}{} & \color[HTML]{9B9B9B}33B & \color[HTML]{9B9B9B} & \color[HTML]{9B9B9B}- & \color[HTML]{9B9B9B}- \\ 
      \arrayrulecolor{lightgray}\midrule
      
      \multicolumn{2}{@{}l}{\multirow{2}{*}{LLaMA-2~\cite{touvron2023llama2}}} & 13B &  & - & - \\ \multicolumn{2}{@{}l}{} & \color[HTML]{9B9B9B}34B & \color[HTML]{9B9B9B} & \color[HTML]{9B9B9B}- & \color[HTML]{9B9B9B}- \\ 
      \arrayrulecolor{lightgray}\midrule
      


      \multicolumn{2}{@{}l}{\multirow{2}{*}{BaiChuan1~\cite{baichuan2023baichuan2}}} & 7B &  & - & - \\ \multicolumn{2}{@{}l}{} & 13B &  & 51.33 & 28.19 \\ 
      \arrayrulecolor{lightgray}\midrule
      
\multicolumn{2}{@{}l}{\multirow{2}{*}{BaiChuan2~\cite{baichuan2023baichuan2}}} & 7B &  & - & - \\ \multicolumn{2}{@{}l}{} & 13B &  & - & - \\ 
      
      \arrayrulecolor{lightgray}\midrule


      \multicolumn{2}{@{}l}{\multirow{2}{*}{WizardMath~\cite{luo2023wizardmath}}}  & 13B &  & 50.83 & 23.40 \\ \multicolumn{2}{@{}l}{} & \color[HTML]{9B9B9B}70B & \color[HTML]{9B9B9B} & \color[HTML]{9B9B9B}- & \color[HTML]{9B9B9B}- \\
      
      \arrayrulecolor{lightgray}\midrule
      \multicolumn{2}{@{}l}{ChatGLM2~\cite{zeng2022glm}} & 6B &  & 68.36 & 50.00\\
      \arrayrulecolor{lightgray}\midrule
      \multicolumn{2}{@{}l}{QWen~\cite{qwen2023qwen}} & 7B &  & 63.16 & 44.15\\
      \arrayrulecolor{lightgray}\midrule


    \multicolumn{2}{@{}l}{\multirow{3}{*}{GAIRMath-Abel~\cite{abel}}}  & 7B &  & - & - \\ \multicolumn{2}{@{}l}{} & 13B &  & - & - \\ \multicolumn{2}{@{}l}{} & \color[HTML]{9B9B9B}70B & \color[HTML]{9B9B9B} & \color[HTML]{9B9B9B}- & \color[HTML]{9B9B9B}- \\ 
      
      \arrayrulecolor{lightgray}\midrule


     \multicolumn{2}{@{}l}{\multirow{3}{*}{MetaMath~\cite{yu2023metamath}}}  & 7B &  & - & - \\ \multicolumn{2}{@{}l}{} & 13B &  & - & - \\ \multicolumn{2}{@{}l}{} & \color[HTML]{9B9B9B}70B & \color[HTML]{9B9B9B} & \color[HTML]{9B9B9B}- & \color[HTML]{9B9B9B}- \\ 
      
     
     \arrayrulecolor{black}\midrule
      \multicolumn{2}{@{}l}{\bf KwaiYiiMath} & 13B & 72.33 & 85.33 & 73.40 \\ 
      \multicolumn{2}{@{}l}{\bf KwaiYiiMath-HPA} & 13B & \textbf{73.31} & \textbf{85.83} & \textbf{74.47} \\ 
      \arrayrulecolor{black}\bottomrule
  \end{tabular}
\end{table*}
\subsection{Baselines}
The baseline models compared in our experiments can be divided into two categories: closed-source models and open-source models.
\paragraph{Closed-source models} Many technology companies have trained LLMs with strong abilities in many downstream tasks, but for some reason do not release their model weights and these models are referred to as closed-source models.
In our experiments, we consider five closed-source LLMs including GPT-4~\cite{openai2023gpt4}, ChatGPT~\cite{openai2022chatgpt}, Ernie Bot\footnote{\url{https://yiyan.baidu.com/}}, Minerva~\cite{zhu2022solving} and MATH-QWEN-CHAT~\cite{qwen2023qwen}. 
\paragraph{Open-source models} There are also many teams that open-source the LLMs they trained, and we can directly download the model weights through open-source communities.
These open-source models also demonstrate excellent capabilities in many downstream tasks.
In our experiments, we select LLaMa1\&2~\cite{touvron2023llama, touvron2023llama2}, BaiChuan1\&2~\cite{baichuan2023baichuan2}, ChatGLM2-6B\footnote{\url{https://github.com/THUDM/ChatGLM2-6B}}, QWen\footnote{\url{https://github.com/QwenLM/Qwen-7B}}, WizardMath\cite{luo2023wizardmath}, GAIRMath-Abel~\cite{abel} ,and MetaMath~\cite{yu2023metamath} as the open-source baselines.

\begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
     \includegraphics[width=\textwidth]{figs/llm_grade2.pdf}
     \caption{}
     \label{subgraph:a}
    \end{subfigure}
\hspace{-2mm}
    \begin{subfigure}[b]{0.45\textwidth}
     \includegraphics[width=\textwidth]{figs/llm_reasoning_step2.pdf}
     \caption{}
     \label{subgraph:b}
    \end{subfigure}

    \begin{subfigure}[b]{0.45\textwidth}
     \includegraphics[width=\textwidth]{figs/llm_num_digit2.pdf}
     \caption{}
     \label{subgraph:c}
    \end{subfigure}
\hspace{-2mm}
    \begin{subfigure}[b]{0.45\textwidth}
     \includegraphics[width=\textwidth]{figs/llm_distractor2.pdf}
     \caption{}
     \label{subgraph:d}
    \end{subfigure}

    \caption{(a) (b) (c): Average test accuracy against one of the problem complexity measures, including 
    grade, number of reasoning steps, and number of digits for each LLM. (d): Average test accuracy against the number of distractors on the distractor dataset. The character  denotes that test accuracy is attained from paper~\cite{wei2023cmath} }
    \label{fig:figperformance}
\end{figure}
\subsection{Training and Evaluation Settings}
\paragraph{Training}
The meta prompt used in training of KwaiYiiMath is the version from Vicuna~\cite{vicuna2023}:
\textit{A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: \{instruction\}. ASSISTANT: \{response\}}.

We follow standard fine-tuning hyperparameters: 3 epochs using AdamW~\cite{loshchilov2017decoupled} with , .
Without warmup steps, we set the initial learning rate to  and use the cosine learning rate decay strategy. 
The global batch size is set to  examples and texts longer than  tokens are trimmed.

\paragraph{Evaluation}
For evaluation on GSM8k~\cite{cobbe2021training}, we generate responses using the greedy decoding strategy.
For each sample in CMath~\cite{wei2023cmath} and KMath, we generate a single response from each baseline model using nucleus sampling~\cite{holtzman2019curious} with  and a temperature of . 
We apply a repetition penalty of previously generated tokens with a hyperparameter of ~\cite{keskar2019ctrl}. 
We limit the maximum token length of output to 2048.

Although all the compared models can generate the intermediate CoT process and final answer, we evaluate all LLMs on GSM8k~\cite{cobbe2021training} using few-shot CoT from~\cite {wei2022chain} for a fair comparison.
We evaluate a solution as correct if the final answer matches the ground truth solution, independent of the quality of the CoT preceding it. 
To evaluate correctness, we parse the final answers and compare them using the SymPy library~\cite{meurer2017sympy}.
For CMath~\cite{wei2023cmath}, we use the code that is released with the data to evaluate the accuracy which only considers the correctness of the final answer.
For KMath, in order to evaluate the model results more comprehensively, we not only evaluate the correctness of the answers but also the correctness of the problem-solving process.
Specifically, we evaluate a solution as correct through human annotation if the answer is correct and the CoT problem-solving process is basically correct.
In order to eliminate the bias of human annotation, the correctness of each sample is first labeled by three different annotators, and then another quality assessment expert checks the labels.
The baseline models are evaluated in a zero-shot way since they have been aligned through SFT both for the CMath~\cite{wei2023cmath} and KMath.

\begin{wrapfigure}{R}{0.5\linewidth}
\begin{center}
    \includegraphics[width=0.8\linewidth]{figs/llm_robust.pdf}
    \end{center}
    \caption{Comparing the accuracy of the origin GSM8k and GSM8k\_Robust.}
    \label{fig:gsm8krobust}
\end{wrapfigure}
\subsection{Main Results}
Results on three datasets are shown in Table~\ref{table:performance}.
We observe that KwaiYiiMath outperforms the same size baseline LLMs on all benchmarks and also surpasses closed-source LLMs including ChatGPT and GPT4 on the KMath dataset, showing that finetuning on diverse and high-quality data is effective.
On the CMath dataset, KwaiYiiMath is close to GPT4 and also achieves a large improvement over other baseline models.
It shows that the KwaiYiiMath is not only effective on English mathematics problems but also on Chinese mathematics problems.
Meanwhile, KwaiYiiMath-HPA achieves improvement over KwaiYiiMath on three benchmarks, showing the effectiveness of the human preference alignment process.




\subsection{Fine-grained and Robustness Results}
In this subsection, we investigate the performance of the LLMs on mathematical problems of varying complexity.


\paragraph{Fine-grained Results}
The CMath~\cite{wei2023cmath} dataset provides the primary school grade corresponding to the question, which can indicate the comprehensive complexity of the question. 
In addition, two dimensions are provided to more intuitively represent the complexity of the question, namely the number of digits that an LLM needs to manipulate, and the number of reasoning steps that an LLM needs to carry out in order to solve a problem.
Intuitively, problems with higher arithmetic complexity or reasoning complexity should be harder to solve, resulting in lower accuracy.

In Figure \ref{subgraph:a}, a distinct downward trend in accuracy is evident, signifying that the performance of all models declines as the complexity increases.
GPT-4 and KwaiYiiMath are the only two models that achieve success (accuracy exceeding 60\%) in math tests across all six elementary school grades and achieve high accuracy (exceeding 80\%) in tests for grades 1 to 4.
The performance of KwaiYiiMath is very close to GPT-4, and even slightly outperforms GPT-4 in some grades.
Following GPT-4 and KwaiYiiMath, ChatGPT, ChatGLM2-6b, and Qwen-7B demonstrate success in tests for grades 1 to 4, but encounter difficulties in grades 5 and 6 (accuracy under 60\%). 

From Figure~\ref{subgraph:b} and~\ref{subgraph:c}, it can be found that all models’ performance declines as either of the problem complexity measures augments.
Judged from the downward slopes of the plots, it is pertinent to say that the reasoning complexity of the problem has generally a larger impact than the arithmetic complexity.

\paragraph{Robustness Results}
The robustness experiment consists of two parts: adversarial evaluation on the GSM8k robust dataset and the CMath distractor dataset. 
The GSM8k robust dataset is a dataset released by~\cite{abel} that was established based on the GSM8k dataset. 
~\cite{abel} randomly modified the numbers within the questions of the GSM8k test set, without altering any other information in the questions, using GPT-4. 
The GSM8k robust dataset can be used to evaluate whether the models overfit the training data, making the models susceptible to out-of-distribution testing samples.
Fiture~\ref{fig:gsm8krobust} shows the performance of KwaiYiiMath on GSM8k robust dataset\footnote{The results of RFT, Abel, and WizardMath are attained from ~\cite{abel}}.
It can be found that there is a slight decrease in the performance of KwaiYiiMath, which demonstrates that the KwaiYiiMath also has strong robustness out-of-distribution testing samples.

The  CMath distractor dataset is a small set released with CMath~\cite{wei2023cmath} for testing the robustness of the model to irrelevant information in the question.
The CMath distractor dataset contains 360 questions: 60 seed questions and 5 noisy versions of each seed question with varying numbers of distractors, from 1 to 5.
The performance of models on the Cmath distractor dataset is plotted in Figure~\ref{subgraph:d}.
From the figure, it can be observed that we observe that the performance of all LLMs, with the exception of GPT-4, drops drastically as the number of distractors increases.
KwaiYiiMath shows a slight ability of robustness, with the overall effect ranking second among all models, and the accuracy on the 5 distractors subset is still greater than 40\%.
However, KwaiYiiMath also suffers an accuracy drop of nearly 50\% for problems augmented with merely three distractors.
We guess the performance decrease is because the training data used by the KwaiYiiMath are relatively clean questions, that is, there is no distractor information in the questions.
Therefore, when distractor information appears in the question, especially information that is very similar to the origin question to be solved, the model will generate many useless steps to solve the problem, resulting in the final answer being wrong.
This is also an important ability that needs to be improved in the future.







\section{Conclusions}
In this report, we introduce the KwaiYiiMath which is fine-tuned from KwaiYiiBase to tackle mathematical problems.
By utilizing a large amount of high-quality mathematical data to perform the human alignment process, including SFT and RLHF, we have greatly enhanced the mathematical reasoning capabilities of KwaiYiiMath.
Experimental results also show that KwaiYiiMath outperforms many open-source models in similar sizes by a large margin and is approaching GPT-4 on three mathematical benchmarks including English and Chinese.
Experimental results on relevant robust data sets also show that the KwaiYiiMath is robust to the disturbed information in question.

Although our work has achieved results close to GPT-4 on relevant benchmarks, in fact, there is still a big gap in a wider range of tasks.
In the future, we hope to further explore the methods to improve the mathematical reasoning capabilities of LLMs and the intrinsic mechanism behind data augmentation for LLMs.


\bibliography{custom}
\bibliographystyle{iclr2024_conference}

\appendix
\section{Appendix}
\subsection{Case Study}










\begin{table}[h!]
\caption{A comparison case on KwaiYiiMath, ChatGPT, and GPT4 of the GSM8k dataset. All three models answer correctly, with GPT4 giving the most concise answer that only included the core calculation steps.}
\begin{tabular}{ c |l } 
 \hline
 \multicolumn{2}{l}{\textbf{Instruction:} While on vacation in Bali, Thea bought a hat from a craftsman worth \20 bills, how much change did she get? }\\

 \hline
 \multicolumn{2}{l}{\textbf{Ground Truth Answer:} 10} \\
 \hline
 KwaiYiiMath & First, we need to find out how much money Thea gave to the craftsman in total.\\
             & Since she gave him four \20 = \70, so to find out how much change she got, \\
             & we simply subtract the cost of the hat from the total amount of money she gave: \\ 
             & \70 = \10 in change. The answer is 10.\\ 
  \hline
  ChatGPT & The total amount Thea paid the craftsman with four \20 = \70, Thea got a change of: \70 = \10. \\
 \hline
 GPT4 & Thea gave the craftsman \80. \\
      & So, she got \70 = \<<350*6=2100>>1,350 (advance payment) + \3,450. \\
      & For Cozy Homes, the total cost of the monthly installments \\
      & is \2,250. So, their total offer's cost is \\
      & \2,250 (installments) = \<<3450-3350=100>>$100. Answer: \boxed{100}. \\
 \hline
\end{tabular}
\end{table}




\begin{table}[h!]
\caption{A comparison case on KwaiYiiMath, ChatGPT, and GPT4 of CMath dataset. KwaiYiiMath and GPT4 answer correctly, while ChatGPT answers incorrectly.}
\begin{tabular}{ c|l } 
 \hline
 \multicolumn{2}{l}{\textbf{Instruction:} \begin{CJK*}{UTF8}{gbsn} 给直径为0.75米的水缸做一个木盖，木盖的直径比缸口直径大5厘米，\end{CJK*}} \\
 \multicolumn{2}{l}{\begin{CJK*}{UTF8}{gbsn}这个木盖的面积是多少平方米？ \end{CJK*}}\\
 \hline
 \multicolumn{2}{l}{\textbf{Ground Truth Answer:} 0.5024} \\
 \hline
 KwaiYiiMath & \begin{CJK*}{UTF8}{gbsn} 首先，将木盖直径与水缸直径相差的5厘米转换为米，即0.05米。\end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 那么，木盖的直径为0.75米+0.05米=0.8米。\end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 接下来，计算木盖的半径，即0.8米÷2=0.4米。\end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 最后，利用圆的面积公式计算木盖的面积：\end{CJK*}\\ 
             & \begin{CJK*}{UTF8}{gbsn} 3.14×(0.4米)²=3.14×0.16=0.5024平方米。\end{CJK*}\\ 
             & \begin{CJK*}{UTF8}{gbsn} 因此，这个木盖的面积是0.5024平方米。\end{CJK*} \\
  \hline
  ChatGPT & \begin{CJK*}{UTF8}{gbsn} 水缸的直径为0.75米，则半径为 0.75m / 2 = 0.375米。\end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 木盖的直径比缸口直径大5厘米，即比缸口半径大0.05米，因此木盖的半径为：\end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 0.375米 + 0.05米 = 0.425米。\end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 那么，木盖的面积为：π × (0.425米)² - π × (0.375米)² ≈ 0.155平方米\end{CJK*}\\ 
             & \begin{CJK*}{UTF8}{gbsn} 因此，这个木盖的面积约为0.155平方米。\end{CJK*}\\ 
 \hline
 GPT4 & \begin{CJK*}{UTF8}{gbsn} 木盖的直径比缸口直径大5厘米，所以木盖的直径是：0.75米 + 0.05米 = 0.8米。\end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 木盖的半径为：0.8米 ÷ 2 = 0.4米。\end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 木盖的面积为圆的面积公式：π × r²，其中r为半径，π ≈ 3.14。\end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 所以木盖的面积为：3.14 × (0.4米)² = 3.14 × 0.16平方米 ≈ 0.5024平方米。 \end{CJK*}\\ 
 \hline
\end{tabular}
\end{table}

\begin{table}[h!]
\caption{A comparison case on KwaiYiiMath, ChatGPT, and GPT4 of CMath dataset. All three models use a series of equations to solve this question. KwaiYiiMath and GPT4 answer correctly, while ChatGPT answers incorrectly.}
\begin{tabular}{ c|l } 
 \hline
 \multicolumn{2}{l}{\textbf{Instruction:} \begin{CJK*}{UTF8}{gbsn} 妈妈和李阿姨一起到超市购物，妈妈买了5千克大米和4千克面粉共付29.2元，\end{CJK*}} \\
 \multicolumn{2}{l}{\begin{CJK*}{UTF8}{gbsn}李阿姨也买了同样的4千克大米和5千克面粉则共付28.4元，每千克大米多少钱？ \end{CJK*}}\\
 \hline
 \multicolumn{2}{l}{\textbf{Ground Truth Answer:} 3.6} \\
 \hline
 KwaiYiiMath & \begin{CJK*}{UTF8}{gbsn} 设每千克大米的价格为x元，每千克面粉的价格为y元。\end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 根据题目信息，我们可以得到以下两个方程：\end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 5x + 4y = 29.2\end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 4x + 5y = 28.4\end{CJK*}\\ 
             & \begin{CJK*}{UTF8}{gbsn} 我们可以用消元法来解这个方程组。\end{CJK*}\\ 
             & \begin{CJK*}{UTF8}{gbsn} 首先，我们将第一个方程乘以4，第二个方程乘以5，得到：\end{CJK*} \\
             & \begin{CJK*}{UTF8}{gbsn} 20x + 16y = 116.8\end{CJK*} \\
             & \begin{CJK*}{UTF8}{gbsn} 20x + 25y = 142\end{CJK*} \\
             & \begin{CJK*}{UTF8}{gbsn} 接下来，我们将第二个方程从第一个方程中减去，得到：\end{CJK*} \\
             & \begin{CJK*}{UTF8}{gbsn} 9y = 25.2\end{CJK*} \\
             & \begin{CJK*}{UTF8}{gbsn} 然后，我们求解y:\end{CJK*} \\
             & \begin{CJK*}{UTF8}{gbsn} y = 25.2 / 9\end{CJK*} \\
             & \begin{CJK*}{UTF8}{gbsn} y = 2.8\end{CJK*} \\
             & \begin{CJK*}{UTF8}{gbsn} 现在我们知道每千克面粉的价格是2.8元，我们可以将y的值代入第一个方程，求解x \end{CJK*} \\
             & \begin{CJK*}{UTF8}{gbsn} 5x + 4(2.8) = 29.2\end{CJK*} \\
             & \begin{CJK*}{UTF8}{gbsn} 5x + 11.2 = 29.2\end{CJK*} \\
             & \begin{CJK*}{UTF8}{gbsn} 5x = 18\end{CJK*} \\
             & \begin{CJK*}{UTF8}{gbsn} x = 18 / 5\end{CJK*} \\
             & \begin{CJK*}{UTF8}{gbsn} x = 3.6\end{CJK*} \\
             & \begin{CJK*}{UTF8}{gbsn} 所以每千克大米的价格是3.6元。\end{CJK*} \\
             
  \hline
  ChatGPT & \begin{CJK*}{UTF8}{gbsn} 设每千克大米的价格为 x 元，则每千克面粉的价格为：\end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 29.2元 - 5千克 × x元/千克 ÷ 4千克 = 28.4元 - 4千克 × x元/千克 ÷ 5千克 \end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 简化可得：7.3元/千克 - 5/4 x元/千克 = 5.68元/千克 - x元/千克 \end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 移项合并同类项可得：2.62元/千克 = 1/4 x元/千克 \end{CJK*}\\ 
             & \begin{CJK*}{UTF8}{gbsn} 即：x元/千克 = 2.62 × 4 = 10.48元/千克 \end{CJK*}\\ 
             & \begin{CJK*}{UTF8}{gbsn} 因此，每千克大米的价格为10.48元。 \end{CJK*}\\
 \hline
 GPT4 & \begin{CJK*}{UTF8}{gbsn} 设每千克大米价格为x元，每千克面粉价格为y元，可以得到以下方程组：\end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 5x + 4y = 29.2（1）\end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 4x + 5y = 28.4（2）\end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 为了求解这个方程组，我们可以将两个方程相加：\end{CJK*}\\ 
             & \begin{CJK*}{UTF8}{gbsn} (5x + 4y) + (4x + 5y) = 29.2 + 28.4 \end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 9x + 9y = 57.6 \end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn}  x + y = 6.4 （3） \end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 然后将方程（1）减去方程（2）： (5x + 4y) - (4x + 5y) = 29.2 - 28.4 x - y = 0.8 （4）\end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 现在我们可以解这个方程组来找到每千克大米的价格： \end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 将方程（3）和（4）相加得到： (x + y) + (x - y) = 6.4 + 0.8 2x = 7.2 \end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 所以每千克大米价格为: x = 7.2 ÷ 2 = 3.6元 \end{CJK*}\\
\hline
\end{tabular}
\end{table}





\begin{table}[h!]
\caption{A comparison case on KwaiYiiMath, ChatGPT, and GPT4 of KMath dataset. ChatGPT and GPT4 answer incorrectly, while KwaiYiiMath answers correctly.}
\begin{tabular}{ c|l } 
 \hline
 \multicolumn{2}{l}{\textbf{Instruction:} \begin{CJK*}{UTF8}{gbsn} 一根电线，第一次用去480米，比第二次用去的1.5倍少60米，第二次用去电线多少米？\end{CJK*}} \\
 \hline
 \multicolumn{2}{l}{\textbf{Ground Truth Answer:} 360} \\
 \hline
 KwaiYiiMath & \begin{CJK*}{UTF8}{gbsn} 设第二次用去的电线长度为x米，根据题意可得方程: \end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 480 = 1.5x - 60。解这个方程，我们可以得到：\end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 1.5x - 60 = 480 \end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 1.5x = 540 \end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} x = 360 \end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 所以，第二次用去的电线长度为360米。 \end{CJK*}\\
  \hline
  ChatGPT & \begin{CJK*}{UTF8}{gbsn} 设第二次用去的电线长度为x，则第一次用去的电线长度为2x-60 \end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} （因为比第二次用去的1.5倍少60米） \end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 根据题意得：第一次用去的电线长度 + 第二次用去的电线长度 = 总电线长度 \end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 即：2x-60 + x = 480 \end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 化简得：3x = 540 \end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 解方程得：x = 180 \end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 所以第二次用去的电线长度为180米。 \end{CJK*}\\
 \hline
 GPT4 & \begin{CJK*}{UTF8}{gbsn} 设第二次用去的电线长度为x米，\end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 则第一次用去的电线长度为1.5x-60米。 \end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 根据题意，两次用去的电线总长度为480米，可以得到方程： \end{CJK*} \\
             & \begin{CJK*}{UTF8}{gbsn} 1.5x - 60 + x = 480 \end{CJK*} \\
             & \begin{CJK*}{UTF8}{gbsn} 解这个方程，我们可以得到：\end{CJK*} \\
             & \begin{CJK*}{UTF8}{gbsn} 2.5x = 540 \end{CJK*} \\
             & \begin{CJK*}{UTF8}{gbsn} x = 216 \end{CJK*} \\
             & \begin{CJK*}{UTF8}{gbsn} 所以，第二次用去的电线长度为216米。 \end{CJK*} \\
 \hline
\end{tabular}
\end{table}

\begin{table}[h!]
\caption{A additional comparison case on KwaiYiiMath, ChatGPT, and GPT4 of calculus field.}
\begin{tabular}{ c|l } 
 \hline
 \multicolumn{2}{l}{\textbf{Instruction:} \begin{CJK*}{UTF8}{gbsn} 求g(x)=exp(x\^{}2+1)对x的微分 \end{CJK*}} \\
 \hline
 KwaiYiiMath & \begin{CJK*}{UTF8}{gbsn} 要计算g(x) = exp(x\^{}2+1)关于x的微分，我们需要使用链式法则。\end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 首先，我们求导exp(x\^{}2+1)关于x的导数。\end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} exp(x\^{}2+1)关于x的导数可以表示为： \end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} d/dx(exp(x\^{}2+1)) = d/dx(exp(x\^{}2+1)) * d/dx(x\^{}2+1) \end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 这里，d/dx(x\^{}2+1) = 2x，因为x\^{}2的导数是2x。 \end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 所以，d/dx(exp(x\^{}2+1)) = exp(x\^{}2+1) * 2x \end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 现在我们求导g(x) = exp(x\^{}2+1)关于x的导数， \end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 就是求导d/dx(exp(x\^{}2+1)) * d/dx(x\^{}2+1)： \end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} dg(x)/gx = exp(x\^{}2+1) * 2x \end{CJK*}\\
              & \begin{CJK*}{UTF8}{gbsn} 这就是g(x)关于x的微分。 \end{CJK*}\\
             
  \hline
  ChatGPT & \begin{CJK*}{UTF8}{gbsn} g'(x) = d/dx(exp(x\^{}2+1)) \end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 使用链式法则，将指数函数部分的导数分离出来并乘以内部函数的导数： \end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} g'(x) = exp(x\^{}2+1) * d/dx(x\^{}2+1) \end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 对于 x\^{}2+1 部分，也使用链式法则： \end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} d/dx(x\^{}2+1) = 2x * 1 \end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 将其代入原式得：g'(x) = exp(x\^{}2+1) * 2x \end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 因此，g(x) 对 x 的微分为：g'(x) = 2x * exp(x\^{}2+1) \end{CJK*}\\
 \hline
 GPT4 & \begin{CJK*}{UTF8}{gbsn} 为了求g(x)=exp(x\^{}2+1)关于x的微分，我们采用链式法则求导。\end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} 链式法则表示为：假设y=f(u)和u=g(x)是可微函数，则复合函数y=f(g(x))的导数为： \end{CJK*}\\
             & \begin{CJK*}{UTF8}{gbsn} (dy/dx) = (dy/du) * (du/dx) \end{CJK*} \\
             & \begin{CJK*}{UTF8}{gbsn} 这里，令u = x\^{}2 + 1，那么g(x) = exp(u)。 \end{CJK*} \\
             & \begin{CJK*}{UTF8}{gbsn} 首先计算du/dx： du/dx = d(x\^{}2 + 1)/dx = 2x \end{CJK*} \\
             & \begin{CJK*}{UTF8}{gbsn} 然后计算dy/du： dy/du = d(exp(u))/du = exp(u) \end{CJK*} \\
             & \begin{CJK*}{UTF8}{gbsn} 现在我们可以计算dy/dx： dy/dx = (dy/du) * (du/dx) = exp(u) * 2x = exp(x\^{}2 + 1) * 2x \end{CJK*} \\
             & \begin{CJK*}{UTF8}{gbsn} 所以，g(x)=exp(x\^2+1)关于x的微分为：2x * exp(x\^{}2 + 1)。 \end{CJK*} \\
 \hline
\end{tabular}
\end{table}


\end{document}
