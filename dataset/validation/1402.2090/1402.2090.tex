\documentclass[11pt]{article}

\setlength{\oddsidemargin}{0.25in}
\setlength{\evensidemargin}{\oddsidemargin}
\setlength{\textwidth}{6in}
\setlength{\textheight}{8in}
\setlength{\topmargin}{-0.0in}


\usepackage[cmex10]{amsmath}
\usepackage{amssymb,amsfonts,textcomp,amsthm}
\usepackage{array}
\usepackage{delarray}
\usepackage{latexsym}
\usepackage[pdftex]{color, graphicx}
\usepackage{subfigure}
\usepackage{mathptmx}
\usepackage{times}
\usepackage{paralist}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{url}
\usepackage{rotating}
\usepackage{cite}
\usepackage[ruled, vlined, noend, linesnumbered, commentsnumbered]{algorithm2e}
\usepackage[font=footnotesize,labelfont=bf]{caption}

\newcommand{\ltot}{{{l_{\mathit{tot}}}}}
\newcommand{\impr}{{{\mathit{impr}}}}
\newcommand{\lmax}{{{l_{\mathit{max}}}}}
\newcommand{\lmaxi}{{{l_{\mathit{max}, i}}}}

\interdisplaylinepenalty=2500
\hyphenation{Da-ta-Ca-ta-log com-pu-ta-tio-na-lly}
\DeclareMathOperator \dir {dir}



\newcommand{\fixme}[1]{{\em\bf{[FIXME: #1]}}}

\newtheorem{theorem}{Theorem}
\newtheorem{example}{Example}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{appendixtheorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{appendixlemma}{Lemma}
\newenvironment{proof-sketch}{\noindent \textit{Sketch of Proof:}}{}


\begin{document}

\title{We Are Impatient: Algorithms for Geographically Distributed Load Balancing with (Almost) Arbitrary Load Functions}

\author{Piotr Skowron \\
   Institute of Informatics \\
   University of Warsaw \\
   Email: p.skowron@mimuw.edu.pl \\
   \and
   Krzysztof Rzadca \\
   Institute of Informatics \\
   University of Warsaw \\
   Email: krzadca@mimuw.edu.pl}

\date{}

\maketitle

\begin{abstract}

In geographically-distributed systems, 
communication latencies are non-negligible.
The perceived processing time of a request is thus composed of the time needed to route the request to the server and the true processing time.
Once a request reaches a target server, the processing time depends on the total load of that server; this dependency is described by a load function.

We consider a broad class of load functions; we just require that they are convex and two times differentiable. In particular our model can be applied to heterogeneous systems in which every server has a different load function. This approach allows us not only to generalize results for queuing theory and for batches of requests, but also to use empirically-derived load functions, measured in a system under stress-testing.

The optimal assignment of requests to servers is communication-balanced, i.e. for any pair of non perfectly-balanced servers, the reduction of processing time resulting from moving a single request from the overloaded to underloaded server is smaller than the additional communication latency. 

We present a centralized and a decentralized algorithm for optimal load balancing.
We prove bounds on the algorithms' convergence. To the best of our knowledge these bounds were not known even for the special cases studied previously (queuing theory and batches of requests).
Both algorithms are any-time algorithms. 
In the decentralized algorithm, each server balances the load with a randomly chosen peer. Such algorithm is very robust to failures. We prove that the decentralized algorithm performs locally-optimal steps. 


Our work extends the currently known results by considering a broad class of load functions and by establishing theoretical bounds on the algorithms' convergence. These results are especially applicable for servers whose characteristics under load cannot be described by a standard mathematical models.




\end{abstract}

\newpage

\section{Introduction}\label{sec::introduction}

We are impatient. 
An ``immediate'' reaction must take less than 100~ms~\cite{card1983psychology};
a Google user is less willing to continue searching if the result page is slowed by just 100-400~ms~\cite{google2009:speed-matters};
and a web page loading faster by just 250~ms attracts more users than the competitor's~\cite{lohr2012impatient}.
Few of us are thus willing to accept the 100-200ms Europe-US round-trip time; even fewer, 300-400ms Europe-Asia.
Internet companies targeting a global audience must thus serve it locally. 
Google builds data centers all over the world; a company that doesn't have Google scale uses a generic content delivery network (CDN)~\cite{pallis2006content, freedman2010experiences}, such as Akamai~\cite{Nygren:2010:ANP:1842733.1842736, akamai2, draftingBehindAkamai}; or spreads its content on multiple Amazon's Web Service regions.

A geographically-distributed system is an abstract model of world-spanning networks. It is a network of interconnected servers processing requests. The system considers both communication (request routing) and computation (request handling). E.g., apart from the communication latencies, a CDN handling complex content can no longer ignore the load imposed by requests on servers. As another example consider computational clouds, which are often distributed across multiple physical locations, thus must consider the network latency in addition to servers' processing times.

Normally, each server handles only the requests issued by local users. For instance, a CDN node responds to queries incoming from the sub-network it is directly connected to (e.g., DNS redirections in Akamai~\cite{draftingBehindAkamai, akamai2, akamaiPatent}). 
However, load varies considerably. 
Typically, a service is more popular during the day than during the night (the daily usage cycle).
Load also spikes during historic events, ranging from football finals to natural disasters.
If a local server is overloaded, some requests might be handled faster on a remote, non-overloaded server.
The users will not notice the redirection if the remote server is ``close'' (the communication latency is small);
but if the remote server is on another continent, the round-trip time may dominate the response time.

In this paper we address the problem of balancing servers' load taking into account the communication latency. 
We model the response time of a single server by a \emph{load function}, i.e., a function that for a given load on a server (the number of requests handled by a server) returns the average processing time of requests. In particular, we continue the work of Liu~et~al.~\cite{Liu:2011:GGL:1993744.1993767}, who showed the convergence of the algorithms for the particular load function that describes requests' handling time in the queuing model~\cite{Gross:2008:FQT:1972549}.
We use a broad class of functions that are continuous, convex and twice-differentiable (Section~\ref{sec:mathematical-model}), 
which allows us to model not only queuing theory-based systems,
but also a particular application with the response time measured empirically in a stress-test.

We assume that the servers are connected by links with high bandwidth. 
Although some models (e.g., routing games \cite{routingGames}) consider limited bandwidth, our aim is to model servers connected by a dense network (such as the Internet), in which there are multiple cost-comparable routing paths between the servers.
The communication time is thus dominated by the latency: 
a request is sent over a long distance with a finite speed. We assume that the latencies are known, as monitoring pairwise latencies is a well-studied problem~\cite{Szymaniak04scalablecooperative, Chan-TinH11}; if the latencies change due to, e.g., network problems, our optimization algorithms can be run again.
On each link, the latency is constant, i.e., it does not vary with the number of sent requests~\cite{Skowron:2013:NDL:2510648.2510769}. This assumption is consistent with the previous works on geographically load balancing~\cite{Liu:2011:GGL:1993744.1993767, Skowron:2013:NDL:2510648.2510769, Cardellini00geographicload, Penmatsa:2006:CLB:1898953.1899089, Grosu:2008:CLB:1455689.1455695, Aote:2009:GMD:1523103.1523153, Grosu:2002:AMD:792762.793282}.

Individual requests are small; rather than an hour-long batch job, a request models, e.g., a single web page hit. Such assumption is often used~\cite{Penmatsa:2006:CLB:1898953.1899089, Liu:2011:GGL:1993744.1993767, Skowron:2013:NDL:2510648.2510769, Grosu:2008:CLB:1455689.1455695, gallet09divisibleload, beaumont2005scheduling, veeravalli2002efficient, drozdowski2008scheduling}. In particular the continuous allocation of requests to servers in our model is analogous to the divisible load model with constant-cost communication (a special case of the affine cost model~\cite{beaumont2005scheduling}) and multiple sources (multiple loads to be handled,~\cite{veeravalli2002efficient, drozdowski2008scheduling}).

The problem of load balancing in geographically distributed systems has been already addressed. Liu~et~al.~\cite{Liu:2011:GGL:1993744.1993767} shows the convergence of the algorithms for a particular load function from the queuing theory. 
Cardellini~et~al.~\cite{Cardellini00geographicload} analyzes the underlying system and network model. Colajanni~et~al.~\cite{Colajanni98dynamicload} presents experimental evaluation of a round-robin-based algorithm for a similar problem.
Minimizing the cost of energy due to the load balancing in geographically distributed systems is a similar problem considered in the literature~\cite{Liu:2013:DCD:2494232.2465740, Liu:2011:GLB:2160803.2160862, Lin:2012:OAG:2410145.2410785}. A different load function is used by Skowron~and~Rzadca~\cite{Skowron:2013:NDL:2510648.2510769} to model the flow time of batch requests.

Some papers analyze the game-theoretical aspects of load balancing in geographically distributed systems~\cite{Penmatsa:2006:CLB:1898953.1899089, Grosu:2008:CLB:1455689.1455695, Aote:2009:GMD:1523103.1523153, Grosu:2002:AMD:792762.793282, Adolphs:2012:DSL:2332432.2332460}. These works use a similar model, but focus on capturing the economic relation between the participating entities.

The majority of the literature ignores the communication costs.
Our distributed algorithm is the extension of the diffusive load balancing~\cite{conf/ipps/AdolphsB12, Ackermann:2009:DAQ:1583991.1584046, Berenbrink:2011:DSL:2133036.2133152}; it incorporates communication latencies into the classical diffusive load balancing algorithms.

Additionally to the problem of effective load balancing we can optimize the choice of the locations for the servers~\cite{staticReplicaPlacement1, staticReplicaPlacement2, journals/cj/JiaLHD01}. The generic formulation of the placement problem, facility location problem~\cite{Chudak:2005:IAA:1047770.1047776} and k-median problem~\cite{Jain:2001:AAM:375827.375845} have been extensively studied in the literature.



\medskip
\textbf{The contributions of this paper are the following.}
\begin{inparaenum}[(i)] 
\item We construct a centralized load-balancing algorithm that optimizes the response time up to a given (arbitrary small) distance to the optimal solution (Section~\ref{sec:approximate-centralized}). 
The algorithm is polynomial in the total load of the system and in the upper bounds of the derivatives of the load function.
\item We show a decentralized load-balancing algorithm (Section~\ref{sec:distr-algor}) in which pairs of servers balance their loads. We prove that the algorithm is optimal (there is no better algorithm that uses only a single pair of servers at each step). We also bound the number of pair-wise exchanges required for convergence.
\item We do not use a particular load function; instead, we only require the load function to be continuous and twice-differentiable (Section~\ref{sec:mathematical-model}); 
thus we are able to model empirical response times of a particular application on a particular machine,
but also to generalize (Section~\ref{sec:specialCases}) Liu~et~al.~\cite{Liu:2011:GGL:1993744.1993767}'s results on queuing model and the results on flow time of batch requests~\cite{Skowron:2013:NDL:2510648.2510769}.
\end{inparaenum}





Our algorithms are suitable for real applications. Both are any-time algorithms which means that we can stop them at any time and get a complete, yet suboptimal, solution.
Furthermore, the distributed algorithm is particularly suitable for distributed systems. It performs only pairwise optimizations (only two servers need to be available to perform a single optimization phase), which means that it is highly resilient to failures. It is also very simple and does not require additional complex protocols.

In this paper we present the theoretical bounds, but we believe that the algorithms will have even better convergence in practice. The experiments have already confirmed this intuition for the case of distributed algorithm used for a batch model~\cite{Skowron:2013:NDL:2510648.2510769}. The experimental evaluation for other load functions is the subject of our future work.

\section{Preliminaries}

In this section we first describe the mathematical model, and next we argue that our model is highly applicable; in particular it generalizes the two problems considered in the literature.

\subsection{Model}\label{sec:mathematical-model}

\noindent
\textbf{Servers, requests, relay fractions, current loads.}\quad
The system consists of a set of  \emph{servers} (processors) connected to the Internet. The -th server has its \emph{local (own) load} of size  
consisting of small \emph{requests}. The local load can be the current number of requests, the average number of requests, or the rate of incoming requests in the queuing model.

The server can relay a part of its load to the other servers. We use a fractional model in which a \emph{relay fraction}  denotes the fraction of the -th server's load that is sent (relayed) to the -th server ( and ). Consequently,  is the part of the -th load that is kept on the -th server. We consider two models. In the \emph{single-hop model} the request can be sent over the network only once. In the \emph{multiple-hop} model the requests can be routed between servers multiple times\footnote{We point the analogy between the multiple-hop model and the Markov chain with the servers corresponding to states and relay fractions  corresponding to the probabilities of changing states.}. Let  denote the size of the load that is sent from the server  to the server . In the single-hop model the requests transferred from  to  come only from the local load of the server , thus:

In the multiple-hop model the requests come both from the local load of the server  and from the loads of other servers that relay their requests to , thus  is a solution of:

The \emph{(current) load} of the server  is the size of the load sent to  by all other servers, including  itself:

\medskip

\noindent
\textbf{Load functions.}\quad
Let  be a \emph{load function} describing the average request's processing time on a server  as a function of 's load  (e.g.: if there are  requests and , then on average it takes  time units to process each request). We assume  is known from a model or experimental evaluation; but each server can have a different characteristics  (heterogeneous servers).
The total processing time of the requests on a server  is equal to  (e.g., it takes  time units to process all requests). 
In most of our results we use  instead of  to be consistent with~\cite{Liu:2011:GGL:1993744.1993767}.



Instead of using a certain load function, we derive all our results for a broad class of load functions (see Section~\ref{sec:specialCases} on how to map existing results to our model). 
Let  be the load that can be effectively handled on a server  (beyond  the server fails due to, e.g., trashing). 
Let . 
Let  be the total load in the system. 
We assume that the total load can be effectively handled,  (otherwise, the system is clearly overloaded). We assume that the values  are chosen so that  are equal to each other (equal to the maximal allowed processing time of the request). 

We assume that the load function  is bounded on the interval  (If  then we follow the convention that ). We assume  is non-decreasing as when the load increases, requests are not processed faster.
We also assume that  is convex and twice-differentiable on the interval  (functions that are not twice-differentiable can be well approximated by twice-differentiable functions). 
We assume that the first derivatives  of all  are upper bounded by  (), 
and that the second derivatives  are upper bounded by  ().
These assumptions are technical---every function that is defined on the closed interval can be upper-bounded by a constant (however the complexity of our algorithms depends on these constants).







\medskip

\noindent
\textbf{Communication delays.}\quad
If the request is sent over the network, the observed handling time is increased by the communication latency on the link.
We denote the communication latency between -th and -th server as  (with ). 
We assume that the requests are small, and so the communication delay of a single request does not depend on the amount of exchanged load (the same assumption was made in the previous works~\cite{Penmatsa:2006:CLB:1898953.1899089, Liu:2011:GGL:1993744.1993767, Skowron:2013:NDL:2510648.2510769, Grosu:2008:CLB:1455689.1455695, gallet09divisibleload, beaumont2005scheduling, veeravalli2002efficient, drozdowski2008scheduling} and it is confirmed by the experiments conducted on PlanetLab~\cite{Skowron:2013:NDL:2510648.2510769}). Thus,  is just a constant instead of a function of the network load.

We assume \emph{efficient -load processing}: for sufficiently small load  the processing time is lower than the communication latency, 
so it is not profitable to send the requests over the network. 
Thus, for any two servers  and  we have:

We use an equivalent formulation of the above assumption (as ):

Since the above must hold for every sufficiently small , we get:



\medskip

\noindent
\textbf{Problem formulation: the total processing time.}\quad
We consider a system in which all requests have the same importance. Thus, 
the optimization goal is to minimize the total processing time of all requests , considering both the communication latencies and the requests' handling times on all servers, i.e., 

\medskip

We formalize our problem in the following definition:
\begin{definition}[Load balancing]
Given  servers with initial loads , load functions  and  communication delays  find , a vector of fractions, that minimizes the total processing time of the requests, .
\end{definition}

We denote the optimal relay fractions by  and the load of the server  in the optimal solution as .
















\subsection{Motivation}\label{sec:specialCases}

Since the assumptions about the load functions are moderate, our analysis is applicable to many systems. In order to apply our solutions one only needs to find the load functions . In particular, our model generalizes the following previous models.

\medskip
\noindent
\textbf{Queuing model.}\quad
Our results generalize the results of Liu~et~al.~\cite{Liu:2011:GGL:1993744.1993767} for the queuing model.
In the queuing model, the initial load  corresponds to the rate of local requests at the -th server. Every server  has a processing rate . According to the queuing theory the dependency between the load  (which is the effective rate of incoming requests) and the service time of the requests is described by ~\cite{Gross:2008:FQT:1972549}.
Its derivative,  is upper bounded by , and its second derivative  is upper bounded by .

\medskip
\noindent
\textbf{Batch model.}\quad
Skowron~and~Rzadca~\cite{Skowron:2013:NDL:2510648.2510769} consider a model inspired by batch processing in high-performance computing. 
The goal is to minimize the flow time of jobs in a single batch.
In this case the function  linearly depends on load  (where  is the speed of the -th server). 
Its derivative is constant, and thus upper bounded by . The second derivative is equal to 0.


\section{Characterization of the problem}
In this section, we show various results that characterize the solutions in both the single-hop and the multiple-hop models. We will use these results in performance proofs in the next sections.

The relation between the single-hop model and the multiple-hop model is given by the proposition followed by the following lemma.

\begin{lemma}\label{lemma:recevAndSend}
If communication delays satisfy the triangle inequality (i.e., for every , and  we have ), then in the optimal solution there is no server  that both sends and a receives the load, i.e. there is no server  such that 
\end{lemma}
\begin{proof}
For the sake of contradiction let us assume that there exist servers  and , such that  and . Then, if we modify the relay fractions: , , and , then the loads  and  remain unchanged, but the communication delay is changed by:

which is, by the triangle inequality, a negative value. This completes the proof.
\end{proof}

\begin{proposition}\label{prop:multiSingleHopEquiv}
If communication delays satisfy the triangle inequality then the single-hop model and the multiple-hop model are equivalent.
\end{proposition}
\begin{proof}
From Lemma~\ref{lemma:recevAndSend} we get that in the optimal solution if , then for every  we have . Thus, .
\end{proof}

We will also use the following simple observation.

\begin{corollary}\label{prop:multiBellerSingleHop}
The total processing time in the multiple-hop model is not higher than in the single-hop model.
\end{corollary}

In the next two statements we recall two results given by Liu~et~al.~\cite{Liu:2011:GGL:1993744.1993767} (these results were formulated for the general load functions). 
First, there exists an optimal solution in which only  relay fractions  are positive. This theorem makes our analysis more practical: the optimal load balancing can be achieved with sparse routing tables. However, we note that most of our results are also applicable to the case when every server is allowed to relay its requests only to a (small) subset of the servers; in such case we need to set the communication delays between the disallowed pairs of servers to infinity. 


\begin{theorem}[Liu~et~al.~\cite{Liu:2011:GGL:1993744.1993767}]
In a single-hop model there exists an optimal solution in which at most  relay fractions  have no-zero values.
\end{theorem}

Second, all optimal solutions are equivalent:
\begin{theorem}[Liu~et~al.~\cite{Liu:2011:GGL:1993744.1993767}]
Every server  has in all optimal solutions the same load .
\end{theorem}

Finally, in the next series of lemmas we characterize the optimal solution, by linear equations. We will use these characterization in the analysis of the central algorithm.

\begin{lemma}\label{lemma:linProg1}
In the multiple hop model, the optimal solution  satisfies the following constraints:

\end{lemma}
\begin{proof}
Inequality~\ref{in:linProgram0} ensures that the completion time of the requests is finite.
Inequalities~\ref{in:linProgram1}~and~\ref{in:linProgram2} state that the values of  are valid relay fractions.
\end{proof}

\begin{lemma}\label{lemma:linProg2}
In the multiple hop model, the optimal solution  satisfies the following constraint:

\end{lemma}
\begin{proof}
For the sake of contradiction let us assume that . Since we assumed that  (see Section~\ref{sec:mathematical-model}), we infer that .

Next, we show that if  and , then the organization  can improve the total processing time of the requests  by relaying some more load to the -th server (which will lead to a contradiction).
Let us consider a function  that quantifies 's and 's contribution to  if  requests are additionally send from  to  (and also takes into account the additional communication latency ):

If , then transferring extra  requests from  to  decreases  (thus leading to a better solution). We compute the derivative of :

Since we assumed that , we get that:

Since  is differentiable, it is continuous; so there exists  such that  is negative on , and thus  is decreasing on . Consequently, , which contradicts the optimality of .
\end{proof}


\begin{lemma}\label{lemma:linProg3}
In the multiple hop model, the optimal solution  satisfies the following constraint:

\end{lemma}
\begin{proof}
If , then in the optimal solution  sends some requests to . There are two possibilities. Either some of the transferred requests of  are processed on , or  sends all of them further to another server . Similarly,  may process some of these requests or send them all further to . Let  be the sequence of servers such that every server from  transfers all received requests of  to the next server in the sequence and  processes some of them on its own.

First, we note that every server from  has non-zero load. Indeed if this is not the case then let  be the last server from the sequence which has load equal to 0. However we assumed that for sufficiently small load, it is faster to process it locally than to send it over the network to the next server  (). This contradicts the optimality of the solution and shows that our observation is true.

Then, we take some requests processed on  and swap them with the same number of requests owned by , processed on . After this swap  processes some requests of ; such a swap does not change . Next, we repeat the same procedure for  and ; then  and ; and so on. As a result,  processes some requests of .

The next part of the proof is similar to the proof of Lemma~\ref{lemma:linProg2}. Let us consider the function  that quantifies 's and 's contribution to  if  requests are \emph{moved back} from  to  (i.e., not sent from  to ):

If , executing  requests on  (and not on ) reduces .

 (see the proof of Lemma~\ref{lemma:linProg2}).
Thus, , and

As  is optimal, , thus , which proves the thesis.
\end{proof}

\begin{lemma}\label{lemma:linProg}
If some solution  satisfies Inequalities~\ref{in:linProgram0},~\ref{in:linProgram1},~\ref{in:linProgram2},~\ref{in:linProgram3},~and~\ref{in:linProgram4} then every server  under  has the same load as in the optimal solution .
\end{lemma}
\begin{proof}
Let  denote the set of servers that in  have greater or equal load than in  (). For the sake of contradiction let us assume that  is non-empty and that it contains at least one server  that in  has strictly greater load than in  ().

Let ; we will show that  in  can receive requests only from the servers from .
By definition of , . Consider a server  that in  relays some of its requests to ; we will show that . Indeed, since , from Inequality~\ref{in:linProgram4} we get that:

Since we assumed that  satisfies Inequality~\ref{in:linProgram3}, we get

By combining these relations we get:

Since  is convex, the function  is non-decreasing (as the sum of two non-decreasing functions); thus .

Similarly, we show that any  in  can send requests only to other  servers. 
Consider a server  that in  receives requests from . 

Thus, .

Let  be the total load sent in  to the servers from  by the servers outside of . Let  be the total load sent by the servers from  in  to the servers outside of . Analogously we define  and  for the state . In the two previous paragraphs we showed that  and that . However, since the total load of the servers from  is in  greater than in , we get that:

From which we get that: , i.e. , which leads to a contradiction as transfers  and  cannot be negative.
\end{proof}


\section{An approximate centralized algorithm}\label{sec:approximate-centralized}

In this section we show a centralized algorithm for the multiple-hop model. As a consequence of Proposition~\ref{prop:multiSingleHopEquiv} the results presented in this section also apply to the single-hop model with the communication delays satisfying the triangle inequality.

For the further analysis we introduce the notion of optimal network flow.

\begin{definition}[Optimal network flow]
The vector of relay fractions  has an optimal network flow if and only if there is no  such that every server in  has the same load as in  and such that the total communication delay of the requests  in  is lower than the total communication delay  in .
\end{definition} 

The problem of finding the optimal network flow reduces to finding a minimum cost flow in uncapacitated network.
Indeed, in the problem of finding a minimum cost flow in uncapacitated network we are given a graph with the cost of the arcs and demands (supplies) of the vertices. For each vertex ,  denotes the demand (if positive) or supply (if negative) of . We look for the flow that satisfies demands and supplies and minimizes the total cost. To transform our problem of finding the optimal network flow to the above form it suffices to set . 
Thus our problem can be solved in time ~\cite{Orlin88afaster}. Other distributed algorithms include the one of Goldberg~et~al.~\cite{minimumCirculation}, and the asynchronous auction-based algorithms~\cite{auctionBasedMinCostFlow}, with e.g., the complexity of .

The following theorem estimates how far is the current solution to the optimal based on the degree to which Inequality~\ref{in:linProgram3} is not satisfied. We use the theorem to prove approximation ratio of the load balancing algorithm.

\begin{theorem}\label{thm:approxBound}
Let  be the vector of relay fractions satisfying Inequalities~\ref{in:linProgram0},~\ref{in:linProgram1},~\ref{in:linProgram2}~and~\ref{in:linProgram4}, and having an optimal network flow. Let  quantify the extent to which Inequality~\ref{in:linProgram3} is not satisfied:

Let .
Let  be the absolute error---the difference between  for solution  and for  , .
For the multiple-hop model and for the single-hop model satisfying the triangle inequality we get the following estimation:

\end{theorem}

\begin{proof}
Let  be the problem instance. Let  be a following instance: initial loads  in  are the same as in ; communication delays  are increased by  (). Let  be the optimal solutions for  in the multiple-hop model.

By Lemma~\ref{lemma:linProg}, loads of servers in  are the same as in , as  satisfies all inequalities for . 
Let  and  denote the total communication delay of  in  and  in , respectively.
First, we show that .

For the sake of contradiction, assume that . We take the solution  in  and modify  by decreasing each latency  by . We obtain instance .
During the process, we decreased (or did not change) communication delay over every link, and so we decreased (or did not change) the total communication delay. Thus, in ,  has smaller communication delay than . This contradicts the thesis assumption that  had in  the optimal network flow. 

As  has the same initial loads and not greater communication delay,

Based on Proposition~\ref{prop:multiSingleHopEquiv}, the same result holds if  is the solution in the single-hop model satisfying the triangle inequality.



We use a similar analysis to bound the processing time.
In the multiple-hop model, if the network flow is optimal, then every request can be relayed at most  times. 
Thus, any solution transfers at most  load. 
Thus, by increasing latencies from  to  we increase the total communication delay of a solution by at most . Taking the optimal solution , we get:


As , by combining the two inequalities we get:

\end{proof}

The above estimations allow us to construct an approximation algorithm (Algorithm~\ref{alg:multipleHopApprox}). The lines~\ref{algline::init1}~to~\ref{algline::init2} initialize the variables. In line~\ref{algline::finiteSol} we build any finite solution (any solution for which the load  on the -th server does not exceed ). Next in the while loop in line~\ref{algline::whileLoop} we iteratively improve the solution. In each iteration we find the pair  with the maximal value of . Next we balance the servers  and  in line~\ref{algline::adjust1}. Afterwards, it might be possible that the current solution does not satisfy Inequality~\ref{in:linProgram4}. In the lines~\ref{algline::topSort}~to~\ref{algline::adjust2} we fix the solution so that Inequality~\ref{in:linProgram4} holds.

\begin{algorithm}[th!]
   \footnotesize
   \SetKwInput{KwNotation}{Notation}
   \SetKwFunction{Improve}{Improve}
   \SetKwFunction{Main}{Main}
   \SetKwFunction{Adjust}{Adjust}
   \SetKwFunction{AdjustBack}{AdjustBack}
   \SetKwFunction{PriorityQueue}{PriorityQueue}
   \SetKwFunction{OptimizeNetwork}{OptimizeNetworkFlow}
   \SetKwFunction{BuildAnyFiniteSolution}{BuildAnyFiniteSolution}
   \SetKwFunction{OptimizeNetworkA}{\textbf{OptimizeNetworkFlow}}
   \SetKwBlock{Block}
   \SetAlCapFnt{\footnotesize}
   \KwNotation{\\
	 --- the required absolute error of the algorithm.\\
	 --- the communication delay between -th and -th server.\\
	 --- the load of the -th server in a current solution.\\
	 --- the number of requests relayed between -th and -th server in a current solution.\\
	\textbf{\OptimizeNetworkA}(, ) --- builds an optimal network flow using algorithm of Orlin~\cite{Orlin88afaster}. 
     }
	\hspace{5mm}

	\Adjust{}:
	\Block{
		\;
		\;
		\;
		\;
	}

	\Improve{}:
	\Block{
		\Adjust()\nllabel{algline::adjust1}\;
		 sort servers topologically according to the order : \nllabel{algline::topSort}\;
		\SetKw{KwTo}{in}
		\For{ \KwTo }
		{
			\SetKw{KwTo}{to}
			\For{ \KwTo }
			{
				\SetKw{KwTo}{and}
				\If{ \KwTo }
				{
					\SetKw{KwTo}{to}
					\AdjustBack()\nllabel{algline::adjust2}\;
				}
			}
		}
	}

	\Main{, , }:
	\Block{
		\For{ \KwTo  \nllabel{algline::init1}}
		{
			\;
			\For{ \KwTo }
			{
				\;
			}
			\nllabel{algline::init2}\;
		}
		\BuildAnyFiniteSolution{}  \nllabel{algline::finiteSol}\;
		\OptimizeNetwork{, }\;
		\;
\While{\nllabel{algline::whileLoop}} 
		{
			\;
			\Improve{, };
		}
		\OptimizeNetwork{, }\;
	}

   \caption{The approximation algorithm for multiple-hop model.}
   \label{alg:multipleHopApprox}
\end{algorithm}

The following Theorem shows that Algorithm~\ref{alg:multipleHopApprox} achieves an arbitrary small absolute error .


\begin{theorem}\label{thm:approxQuality}
Let  be the desired absolute error for Algorithm~\ref{alg:multipleHopApprox}, and let  be the initial error. In the multiple-hop model Algorithm~\ref{alg:multipleHopApprox} decreases the absolute error from  to  in time .
\end{theorem}
\begin{proof}
Let  and  be the loads of the servers  and  before the invocation of the \texttt{Adjust} function in line~\ref{algline::adjust1} of Algorithm~\ref{alg:multipleHopApprox}.
Let  quantify how much Inequality~\ref{in:linProgram3} is not satisfied, .
As in proof of Lemma~\ref{lemma:linProg2}, consider a function  that quantifies 's and 's contribution to  if  requests of are additionally send from  to :

As previously, the derivative of  is:

Thus, .
The second derivative of  is equal to:

The second derivative is bounded by:

For any function  with a derivative  bounded on range  by a constant , the value  is upper-bounded by:

Using this fact, we upper-bound the first derivative by:

We use a particular value of the load difference , getting that for , we have: 


We can use Inequality~\ref{eq:der-upperbound} for a function  to lower-bound the reduction in  for  as :




To conclude that \texttt{Adjust} function invoked in line~\ref{algline::adjust1} reduces the total processing time by at least , we still need ensure that the server  has enough (at least ) load to be transferred to . However we recall that the value of  in  is negative, . This means that after transferring  requests, sending more requests from  to  further reduces .
Thus, if 's load would be lower than , this would contradict the efficient -load processing assumption.



Also, every invocation of \texttt{AdjustBack} decreases the total completion time . Thus,  after invocation of \texttt{Improve} the total completion time  is decreased by at least .

Each invocation of \texttt{Improve} preserves the following invariant: in the current solution Inequalities~\ref{in:linProgram1},~\ref{in:linProgram2}~and~\ref{in:linProgram4} are satisfied. It is easy to see that Inequalities~\ref{in:linProgram1},~\ref{in:linProgram2} are satisfied. We will show that Inequality~\ref{in:linProgram4} holds too. Indeed, this is accomplished by a series of invocations of \texttt{AdjustBack} in line~\ref{algline::adjust2}. Indeed, from the proof of Lemma~\ref{lemma:linProg3}, after invocation of the \texttt{Adjust} function for the servers  and , these servers satisfy Inequality~\ref{in:linProgram4}. 

We also need to prove that the servers can be topologically sorted in line~\ref{algline::topSort}, that is that there is no such sequence of servers  that  and . For the sake of contradiction let us assume that there exists such a sequence. Let us consider the first invocation of \texttt{Adjust} in line~\ref{algline::adjust1} that creates such a sequence. Without loss of generality let us assume that such \texttt{Adjust} was invoked for the servers  and . This means that before this invocation , and so
.
Since the invariant was satisfied before entering \texttt{Adjust} and since , from Inequality~\ref{in:linProgram4} we infer that
, and so that
. Thus, we get contradiction:

Which proves that the invariant is true.

If the algorithm finishes, then . After performing the last step of the algorithm the network flow is optimized and we can use Theorem~\ref{thm:approxBound} to infer that the error is at most .

We estimate the number of iterations to decrease the absolute error from  to . To this end, we estimated the decrease of the error after a single iteration of the while loop in line~\ref{algline::whileLoop}.
The algorithm continues the last loop only when .
Thus, after a single iteration of the loop the error decreases by at least .
Thus, after  iterations the error decreases to 0. Since every iteration of the loop has complexity , we get the thesis.
\end{proof}

Using a bound from Theorem~\ref{thm:approxBound} corresponding to the single-hop model we get the following analogous results.

\begin{corollary}
If the communication delays satisfy the triangle inequality then Algorithm~\ref{alg:multipleHopApprox} for the single-hop model decreases the absolute error from  to  in time .
\end{corollary}

For the relative (to the total load) errors , and , Algorithm~\ref{alg:multipleHopApprox} decreases  to  in time . Thus, we get the shortest runtime if  is large and  is small. If the initial error  is large we can use a modified algorithm that performs \texttt{OptimizeNetworkFlow} in every iteration of the last ``while'' loop (line~\ref{algline::whileLoop}). Using a similar analysis as before we get the following bound.

\begin{theorem}
The modified Algorithm~\ref{alg:multipleHopApprox} that performs \texttt{OptimizeNetworkFlow} in every iteration of the last ``while'' loop (line~\ref{algline::whileLoop}) decreases the relative error  by a multiplicative constant factor in time .
\end{theorem}
\begin{proof}
The analysis is similar as in the proof of Theorem~\ref{thm:approxQuality}. Here however at the beginning of each loop the network flow is optimized. If the absolute error before the loop is equal to , then from Theorem~\ref{thm:approxBound} we infer that . Thus, after a single iteration of the loop the error decreases by , and so by the factor of:

Taking the relative error  as  we get that every iteration decreases the relative error by a constant factor .
Thus, after  iterations the error decreases by a constant factor. Since the complexity of every iteration of the loop is dominated by the algorithm optimizing the network flow (which has complexity ), we get the thesis.
\end{proof}

Algorithm~\ref{alg:multipleHopApprox} is any-time algorithm. We can stop it at any time and get a so-far optimized solution.














\section{Distributed algorithm}\label{sec:distr-algor}

\begin{algorithm}[t!]
  \SetKwInOut{Input}{input}
  \Input{ -- the identifiers of the two servers}
  \KwData{  -- initialized to the number of requests owned by  and relayed to  (  is defined analogously)}
  \KwResult{The new values of  and }
  \ForEach{}{
       ; \; 
  }
   ;  \;

    sort  so that    is before \;
  \ForEach{\nllabel{algline::secLoopStart}}
  {
       \nllabel{algline::argmin}\;
       \;
      \If{}
      {
         ;  \;
         ;  \nllabel{algline::secLoopEnd}\;
      }
  }
  \Return{for each :  and }
  \caption{\textsc{calcBestTransfer}(i, j)}
  \label{alg:exchangingLoads}
\end{algorithm}

\begin{algorithm}[h]
  \SetKwFunction{calcBestTransfer}{calcBestTransfer}
  \SetKwFunction{transfer}{relay}
  \SetKwFunction{random}{random}
  \SetKwInOut{Notation}{Notation}
  partner  \random{m}\;
  \transfer(id, partner, \calcBestTransfer{id, partner})\;
  \caption{Min-Error (MinE) algorithm performed by server id.}
  \label{alg:distributedOptimal}
\end{algorithm}

The centralized algorithm requires the information about the whole network. The size of the input data is . A centralized algorithm has thus the following drawbacks:
\begin{inparaenum}[(i)] 
\item collecting information about the whole network is time-consuming; moreover, loads and latencies may frequently change;
\item the central algorithm is more vulnerable to failures. 
Motivated by these limitations we introduce a distributed algorithm for optimizing the query processing time.
\end{inparaenum}



Each server, , keeps for each server, , information about the number of requests that were relayed to  by . The algorithm iteratively improves the solution -- the -th server in each step communicates with a random partner server --  (Algorithm~\ref{alg:distributedOptimal}). The pair  locally optimizes the current solution by adjusting, for each ,  and  (Algorithm~\ref{alg:exchangingLoads}). 
In the first loop of the Algorithm~\ref{alg:exchangingLoads}, one of the servers , takes all the requests that were previously assigned to  and to . Next, all the servers  are sorted according to the ascending order of . The lower the value of , the less communication delay we need to pay for running requests of  on  rather than on . Then, for each , the loads are balanced between servers  and .  Theorem~\ref{lemma::delegationOptimality} shows that Algorithm~\ref{alg:exchangingLoads} 
optimally balances the loads on the servers  and .



The idea of the algorithm is similar to the diffusive load balancing~\cite{conf/ipps/AdolphsB12, Ackermann:2009:DAQ:1583991.1584046, Berenbrink:2011:DSL:2133036.2133152}; however there are substantial differences related to the fact that the machines are geographically distributed: \begin{inparaenum}[(i)] 
\item In each step no real requests are transferred between the servers; this process can be viewed as a simulation run to calculate the relay fractions . Once the fractions are calculated the requests are transferred and executed at the appropriate server. 
\item Each pair  of servers exchanges not only its own requests but the requests of all servers that relayed their requests either to  or to . Since different servers may have different 
communication delays to  and  the local balancing requires more care (Algorithms~\ref{alg:exchangingLoads}~and ~\ref{alg:distributedOptimal}).
\end{inparaenum}

Algorithm~\ref{alg:distributedOptimal} has the following properties: 
\begin{inparaenum}[(i)] 
\item The size of the input data is  for each server---communication latencies from a server to all other servers (and not for all pairs of servers). It is easy to measure these pair-wise latencies (Section~\ref{sec::introduction}). The algorithm is also applicable to the case when we allow the server to relay its requests only to the certain subset of servers (we set the latencies to the servers outside of this subset to infinity).
\item A single optimization step requires only two servers to be available (thus, it is very robust to failures).
\item Any algorithm that in a single step involves only two servers cannot perform better (Theorem~\ref{lemma::delegationOptimality}).
\item The algorithm does not require any requests to be unnecessarily delegated -- once the relay fractions are calculated the requests are sent over the network.
\item In each step of the algorithm we are able to estimate the distance between the current solution and the optimal one (Proposition~\ref{lemma::convergence}).
\end{inparaenum}


\subsection{Optimality}

The following theorem shows the optimality of Algorithm~\ref{alg:exchangingLoads}.

\begin{theorem}\label{lemma::delegationOptimality}
After execution of Algorithm~\ref{alg:exchangingLoads} for the pair of servers  and ,  cannot be further improved by sending the load of any servers between  and  (by adjusting  and  for any ).
\end{theorem}
\begin{proof}
For the sake of simplicity of the presentation we prove that after performing Algorithm~\ref{alg:exchangingLoads}, for any single server  we cannot improve the processing time  by moving any requests of  from  to  or from  to . Similarly it can be proven that we cannot improve  by moving the requests of any \emph{set} of the servers from  to  or from  to .

Let us consider the total processing time function . Since  is non-decreasing and convex,  is convex. Indeed if , then:

Now, let  be the total load on the servers  and , . Let us consider the function  describing the contribution in  of servers  and  as a function of load  processed on the server  (excluding communication): 

The function  is convex as well. Indeed:


Now, we show that after the second loop (Algorithm~\ref{alg:exchangingLoads}, lines~\ref{algline::secLoopStart}-\ref{algline::secLoopEnd}) transferring any load from  to , would not further decrease the total completion time . 
For the sake of contradiction let us assume that for some server  after the second loop some additional requests of  should be transferred from  to .
The second loop considers the servers in some particular order and in each iteration moves some load (possibly of size equal to 0) from  to .
Let  be the iteration of the second loop in which the algorithm considers the requests owned by  and tries to move some of them from  to .
Let  and  be the loads on the server  immediately after  and after the last iteration of the second loop, respectively.
As no request is moved back from  to , .
We will use a function :

The function  returns the total processing time of  and  assuming the server  after iteration  sent additional  more requests of  to  (including the communication delay of these extra  requests).

Immediately after iteration  the algorithm could not improve the processing time of the requests by moving some requests owned by  from  to . This is the consequence of one of two facts. Either all the requests of  are already on , and so there are no requests of  to be moved (but in such case we know that when the whole loop is finished there are still no such requests, and thus we get a contradiction). Alternatively, the function  is increasing for some interval  (). But then we infer that the function:

is also increasing on . Indeed:

Since  is convex (because  is convex) we get that  is increasing not only on , but also for any positive . Thus, it is not possible to improve the total completion time by sending the requests of  from  to  after the whole loop is finished. This gives a contradiction.

Second, we will show that when the algorithm finishes no requests should be transferred back from  to  either.
Again, for the sake of contradiction let us assume that for some server  after the second loop (Algorithm~\ref{alg:exchangingLoads}, lines~\ref{algline::secLoopStart}-\ref{algline::secLoopEnd}) some requests of  should be transferred back from  to .
Let  be the iteration of the second in which the algorithm considers the requests owned by .
Let us take the last iteration  of the second loop in which the requests of some server  were transferred from  to .
Let  be the load on  after . 
After  no requests of  should be transferred back from  to  ( in line~\ref{algline::argmin}). Thus, for some  the function :

is increasing on . Since the servers are ordered by decreasing latency differences   (increasing latency differences  ), we get , and so that the function: 

is also increasing on . Since  is convex we see that it is increasing or any positive , and thus we get the contradiction. This completes the proof.
\end{proof}


\subsection{Convergence}\label{sec:convergence}
The following analysis bounds the error of the distributed algorithm as a function of the servers' loads. When running the algorithm, this result can be used to assess whether it is still profitable to continue. As the corollary of our analysis we will show the convergence of the distributed algorithm. 

In proofs, we will use an \emph{error graph} that quantifies the difference of loads between the current and the optimal solution.

\begin{definition}[Error graph]
Let  be the snapshot (the current solution) at some moment of execution of the distributed algorithm. 
Let  be the optimal solution (if there are multiple optimal solutions with the same ,  is the closest solution to  in the Manhattan metric).
 is a weighted, directed \emph{error graph} with multiple edges. The vertices in the error graph correspond to the servers;  is a weight of the edge  with a label . The weight indicates the number of requests owned by  that should be executed on  instead of  in order to reach  from .
\end{definition}

The error graphs are not unique. For instance, to move  requests owned by  from  to  we can move them directly, or through some other server . In our analysis we will assume that the total weight of the edges in the error graph  is minimal, that is that there is no , and , such that  and .

Let  denote the set of (immediate) successors of server  in the error graph;  denotes the set of (immediate) predecessors of .

We will also use a notion of \emph{negative cycle}: a sequence of servers in the error graph that essentially redirect some of their requests to one another.
\begin{definition}[Negative cycle]
In the error graph, a \emph{negative cycle} is a sequence of servers  and labels  such that:
\begin{enumerate}
\item ; (the sequence is a cycle)
\item ; (for each pair there is an edge in the error graph)
\item  (the transfer in the circle  decreases communication delay).
\end{enumerate}
\end{definition}
A current solution that results in an error graph without negative cycles has smaller processing time: after dismantling a negative cycle, loads on servers remain the same, but the communication time is reduced. Thus, if the current solution has an optimal network flow, then there are no negative cycles in the error graph. 

Analogously we define \emph{positive cycles}. The only difference is that instead of the third inequality we require . Thus, when an error graph has a positive cycle, the current solution is better than if the cycle would be dismantled.


We start by bounding the load imbalance when there are no negative cycles.

\begin{lemma}\label{lemma::convergence}
Let  be the improvement of the total processing time  after balancing servers  and  by Algorithm~\ref{alg:exchangingLoads}. 
Let  be the load of a server  in the current state; and  be the optimal load. 
If the error graph  has no negative cycles, then for every positive  the following estimation holds:

\end{lemma}

\begin{proof}
First we show that there is no cycle (positive nor negative) in the error graph. By contradiction let us assume that there is a cycle:  (with ) with labels .
Because, we assumed the error graph has no negative cycle, we have: . 
Now, let  be the minimal load on the cycle. If we reduce the number of requests sent on each edge of the cycle:

then the load of the servers  will not change.
Additionally, the latencies decrease by  which is at least equal to 0. Thus, we get a new optimal solution which is closer to  in Manhattan metric, which contradicts that  is optimal.

In the remaining part of the proof, we show how to bound the difference .
Consider a server  for which , and a server . 
We define as  the load that in the current state  should be transferred between  and  so that after this transfer, moving any  more load owned by any  between  to  would be either impossible or would not improve  by more than . 
Intuitively, after moving , we won't be able to further ``significantly'' reduce : further reductions depend on the moved load (), but the rate of the improvement is lower than . This move resembles Algorithm~\ref{alg:exchangingLoads}: i.e., Algorithm~\ref{alg:exchangingLoads} moves  for .


Let  denote a state obtained from  when  moves to  exactly  requests.
Let  and  denote the loads of the servers  and  in , respectively.
We define  as the change of  resulting from moving additional  requests produced by  from  to :


State  satisfies one of the following conditions for each  (consider  as a small, positive number):
\begin{enumerate}
\item The servers  and  are -balanced, thus moving  requests from  to  would not reduce  by more than . In such case we can bound the derivative of :

\item Or, moving  requests from  to  would decrease  by more than , but there are no requests of  on :

\item Or, moving  requests back from  to  would decrease  by more than , but there are no requests of  to be moved back:

\end{enumerate}

In the optimal solution, for any , no 's requests should be moved between  and . We define  similarly to , but for the optimal loads:

By the same reasoning, at least one of the three following inequalities holds:




We consider two 
cases on the sum of weights between  and  in the error graph.
Either (1) in the error graph,  sends to  at most  requests (); or (2) 
. We further analyze (2). Since  is the total load transferred from  to  in  to get , there must exist a  such that  (from  to  more 's requests are moved in the error graph than in  to get ). 
We show that  by contradiction. 
If  (in , 
no 's requests are processed on ), 
then  
(all 's requests were moved to  in  to get ).
As  (the error graph does not transfer more requests than available),
, which contradicts .
As ,
Ineq.~\ref{ineq:convergence1} or~\ref{ineq:convergence3} holds (Ineq.~\ref{ineq:convergence2} does not hold),
thus .
As ,
,
thus, ,
so Ineq.~\ref{ineq:convergence4} or~\ref{ineq:convergence5} holds (Ineq.~\ref{ineq:convergence6} does not hold),
so .


Combining the above inequalities,

Or equivalently:


We can further expand the above inequality for  and its successors (and each expansion is applied on state ), and so on towards the end of the error graph (we proved there are no cycles), until we get that for some  and its successor  the condition of the case (2) does not hold, so (1) must hold (, or equivalently ). 
Analogously to  we define  as the state in which  moves to  load .
Thus, we have:

From the definition of  we have:

Combining Inequalities~\ref{ineq:loadDiff}~and~\ref{ineq:loadDiff1} we get:

We bound the second derivative of :

With the above observations, and using the bound from Inequality~\ref{eq:der-upperbound} we get: 

As , from , we get that:


We can get the same results for the server  such that , by expanding the inequalities towards the predecessors of .

We now relate  with , the result of Algorithm~\ref{alg:exchangingLoads}. Algorithm~\ref{alg:exchangingLoads} stops when no further improvement is possible, thus the load moved by the algorithm is at least  for any . By definition of , moving  load improves  by at least . Thus, 
. 
This completes the proof.
\end{proof}

As the result we get the following corollary.

\begin{corollary}
If the network flow in the current solution  is optimal, then the absolute error  is bounded:

\end{corollary}
\begin{proof}
The value  denotes the average processing time of a request on the -th server. For every server  the average processing time of every request on  in  is by at most  greater than in . Thus, since there are  requests in total, we get the thesis.
\end{proof}

We can use Lemma~\ref{lemma::convergence} directly to estimate the error during the optimization if we run a distributed negative cycle removal algorithm (e.g.~\cite{minimumCirculation, auctionBasedMinCostFlow}).
However, this result is even more powerful  when applied together with the lemmas below, as it will allow to bound the speed of the convergence of the algorithm (even without additional protocols optimizing the network flow).
Now, we show how to bound the impact of the negative cycles.

\begin{lemma}\label{lemma::negativeCyclesElimination}
For every , removing the negative cycles improves the total processing time  of the solution by at most:

\end{lemma}

\begin{proof}
In the analysis we will use a function  defined as  in the proof of Lemma~\ref{lemma::convergence} (here, we will use indices ) 


We will analyze a procedure that removes negative cycles one by one. 

First, we prove that we can remove all the negative cycles by only considering the cycles that satisfy the \emph{one-way transfers} property: if in a cycle there is a transfer , then in no cycle there is a transfer .
Indeed, consider the set of all cycles. We do the following procedure:
\begin{enumerate}
\item If there are two negative cycles  and  with a common edge , such that in the first cycle load  is transferred from  to  and in the second load  is
transferred back from  to  
then we split the first cycle  into two cycles  and  such that  transfers  and  transfers . Next, we merge  and  into one negative cycle that does not contain edge . 
\item If a single cycle transfers load first from  to , and then from  to  then we break the cycle at the edge  into two cycles (without edges between  and ).  \label{enum:cycle-break2}
\end{enumerate}
Let us note that each of the two above steps does not increase the total load transferred on the cycles.
For a given error graph, there are many possible ways (sets of cycles) to express a non-optimal flow as a sum of negative cycles. We will consider a set of cycles with the smallest total load (sum of loads transferred over all edges of all cycles).
In this set, we will remove individual cycles sequentially in an arbitrary order.

In this sequence of cycles with the smallest load, every request is transferred at most once.
Indeed, if this is not the case, a request was transferred through adjacent edges  and . Thus, among the cycles we consider there are two negative cycles, such that the first one contains the edge  and the second one contains . 
(a single request cannot be transferred  in a single cycle, because by sending  we would get a cycle with a smaller load).
Also, between  and  and between  and  requests of the same server  are transferred. Let  be the edge adjacent to  in the second cycle.
If in the first cycle we send requests from  to  and in the second from  to , we would obtain two cycles that transfer an equivalent load (each server has the same requests) and have
smaller total transfer, a contradiction.




Let us consider a state  with a negative cycle , that is the sequence of servers  and labels . Let us assume that in a negative cycle  the load  is carried on. After removing the cycle ,  is improved by :

Let us distribute among the edges the cost of all negative cycles (the cost, i.e., the increase in  resulting from the cycles). For removing a single cycle with load , from the above inequality we assign the cost  to the labeled edge . As every request is sent over a single edge at most once, the total cost assigned to the labeled edge  will be at most . 

In the further part of the proof we will estimate .
First, we bound the second derivative of  as in Eq.~\ref{eq:der2-upperbounded} in the proof of Theorem~\ref{thm:approxQuality}:

Next, we consider two cases: (1) , and (2) . If (1) is the case then the total cost associated with  is at most . We further analyze (2). Let us take . From Inequality~\ref{eq:der-upperbound} we get that: 

Thus (again from Ineq.~\ref{eq:der-upperbound} but applied for ) we get that:

Since  (there are at least  requests of  on ), we infer that Algorithm~\ref{alg:exchangingLoads} would achieve improvement  lower-bounded by:

Further, we consider two sub-cases. If (2a)  then the total cost associated with  is at most . (2b) Otherwise (), we have .
Since , we get that the total cost associated with all edges is at most:

Thus, we get the thesis. 
\end{proof}

Finally we get the following estimation.

\begin{theorem}\label{thm::finalEstimation}
Let  be the improvement of the total processing time  after balancing servers  and  through Algorithm~\ref{alg:exchangingLoads}. Let  be the absolute error in  (the difference between  in the current and the optimal state). For every , we have:

\end{theorem}
\begin{proof}
The error coming from the negative cycles is bounded by Lemma~\ref{lemma::negativeCyclesElimination} by:

The error coming from the processing times is, according to Lemma~\ref{lemma::convergence} bounded by:

The sum of the above errors leads to the thesis.
\end{proof}

And the following theorem.

\begin{theorem}\label{thm::convergence}
Let  and  be the initial and the desired absolute errors. The distributed algorithm reaches the  in expected time complexity:

\end{theorem}
\begin{proof}
In the estimation from Theorem~\ref{thm::finalEstimation} we set  and relax the upper bound by replacing  with :

Thus, either

and the algorithm has already reached the error ; or in every execution step we have:

The expected improvement of the distributed algorithm during every pairwise communication is , and thus it is lower bounded by:

Thus, after, in expectation,  steps the initial error drops to 0.
This completes the proof.
\end{proof}

For the relative errors , and , the complexity of the algorithm is equal to .

\section{Conclusions}

In this paper we considered the problem of balancing the load between geographically distributed servers. In this problem the completion time of a request is sum of the 
communication latency needed to send the request to a server and the servers' processing time. The processing time on a server is described by a load function and depends on the total load on the server. Throughout the paper we considered a broad class of load functions with  the mild assumptions that they are convex and two times differentiable. 

We presented two algorithms---the centralized one and the distributed one. Both algorithms are any-time algorithms that continuously optimize the current solution. We shown that both algorithms converge for almost arbitrary load function. We also presented bounds on speed of their convergence that depend (apart from the standard parameters) on the bounds on the first and second derivatives of the load functions. The centralized algorithm decreases an initial relative error  to a desired value   in time . The distributed algorithm decreases  to  in time . Also, for the large values of initial error  the centralized algorithm decreases the error by half in time .

The distributed algorithm is based on the idea of gossiping. To perform a single optimization step, the algorithm requires just two servers to be available. Thus, the algorithm is robust to transient failures. It also does not require additional protocols. In some sense it is also optimal: we  proved that the local optimization step performed by this algorithm cannot be improved. Finally, at any time moment, during the execution of the distributed algorithm, we are able to assess the current error.

Experimental results were shown for a different algorithm applied to the queuing model~\cite{Liu:2011:GGL:1993744.1993767}; and for a version of our distributed algorithm specialized to the batch model~\cite{Skowron:2013:NDL:2510648.2510769}.
In our future work we plan to experimentally assess the performance of our algorithms on real workloads and several load functions, including the queuing model.









\bibliographystyle{abbrv}
\bibliography{contentDelivery}


\end{document}
