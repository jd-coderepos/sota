

\documentclass[runningheads]{llncs}
\usepackage{graphicx}


\usepackage{tikz}
\usepackage{comment}
\usepackage{amsmath,amssymb} \usepackage{color}

\usepackage[accsupp]{axessibility}  


\usepackage{graphicx}



\graphicspath{{Figures/}} 


\begin{document}
\pagestyle{headings}
\mainmatter
\def\ECCVSubNumber{4368}  

\title{StyleFlow For Content-Fixed Image to Image Translation} 

\begin{comment}
\titlerunning{ECCV-22 submission ID \ECCVSubNumber} 
\authorrunning{ECCV-22 submission ID \ECCVSubNumber} 
\author{Anonymous ECCV submission}
\institute{Paper ID \ECCVSubNumber}
\end{comment}


\author{Weichen Fan\thanks{Equal contribution.} \and
Jinghuan Chen \and Jiabin Ma \and Jun Hou \and Shuai Yi}


\authorrunning{Fan et al.}
\institute{Sensetime Research}
\maketitle

\begin{abstract}
Image-to-image (I2I) translation is a challenging topic in computer vision. We divide this problem into three tasks: strongly constrained translation, normally constrained translation, and weakly constrained translation. The constraint here indicates the extent to which the content or semantic information in the original image is preserved. Although previous approaches have achieved good performance in weakly constrained tasks, they failed to fully preserve the content in both strongly and normally constrained tasks, including photo-realism synthesis, style transfer, and colorization, .etc. To achieve content-preserving transfer in strongly constrained and normally constrained tasks, we propose StyleFlow, a new I2I translation model that consists of normalizing flows and a novel Style-Aware Normalization (SAN) module. With the invertible network structure, StyleFlow first projects input images into deep feature space in the forward pass, while the backward pass utilizes the SAN module to perform content-fixed feature transformation and then projects back to image space. Our model supports both image-guided translation and multi-modal synthesis. We evaluate our model in several I2I translation benchmarks, and the results show that the proposed model has advantages over previous methods in both strongly constrained and normally constrained tasks.
\end{abstract}
\section{Introduction}
\label{sec:intro}
\begin{figure}
\centering
    \includegraphics[width=1.0\linewidth]{Figures/intro_pic_new.pdf}
\caption{Given two images in different visual domains, our model learns to translate from one to the other in various tasks.(1. Summer to Winter\cite{zhu2017unpaired}; 2. Gray to Color\cite{anwar2020image}; 3. Sim to Real\cite{zia2021surgical}; 4. Apple to Orange\cite{zhu2017unpaired}; 5. Dark to Light\cite{wei2018deep}; 6. Photo to Monnet\cite{wikiart}; 7. GTA\cite{richter2016playing} to Cityscapes\cite{cordts2016cityscapes}; 8. Photo to Photo\cite{luan2017deep}; 9. Hazy to Clear\cite{Dense-Haze_2019,NTIRE_Dehazing_2019})}
    \label{fig:various_results}
\end{figure}

Image-to-image (I2I) translation\cite{isola2017image} is a long-standing topic in computer vision, which is required to learn a mapping between two different visual domains while preserving the semantic information (content) of the source domain and obtaining the domain properties (style) of the target domain. Many applications, such as style transfer\cite{gatys2015texture,Gatys_2016_CVPR,huang2017arbitrary,li2017universal}, super-resolution\cite{Ledig_2017_CVPR,Lai_2017_CVPR}, image enhancement\cite{lore2017llnet} and photo-realism synthesis\cite{Chen_2017_ICCV,Wang_2018_CVPR,richter2021enhancing}, can be formulated as I2I translation problems. According to the requirement of content preservation during translation, these problems can be further divided into three subsets: strongly constrained translation, normally constrained translation, and weakly constrained translation.

Recent methods\cite{zhu2017unpaired,huang2018multimodal,lee2018diverse} based on cycle-consistency constraint have shown great progress in I2I translation, by forcing the translated images to be mapped back to the original images during training. Although these methods have achieved impressive visual results in most applications, the cycle-consistency constraint failed to reproduce rich and complex semantic information, thus resulting in different levels of content distortion in translated images, especially in normally and strongly constrained translation settings.

In this work, we propose \textit{StyleFlow} to address the problem of content distortion in I2I translation. Unlike previous methods which follow \textit{encoder-decoder} scheme, StyleFlow utilizes the design of normalizing flow models\cite{dinh2014nice}, which consists of a series of fully invertible basic blocks, to achieve lossless image reconstruction. In addition, a novel \textit{Style-Aware Normalization (SAN)} module is introduced to perform context-fixed feature transformation. Given source image features and target image features, SAN adjusts mean and variance of source features with learnable \textit{content-guided affine parameters} to match the style of target input while preserve the content of source input. Considering that our model only consists of fully reversible transformation during feature extraction and reconstruction, pixel-level reconstruction loss is not required. Following most I2I translation tasks especially in style transfer\cite{gatys2015texture}, content loss and style loss calculated based on a pre-trained VGG encoder\cite{Simonyan2015very} are adopted. We further extend the idea of style loss and introduce its simple extension named \textit{aligned-style loss}, which takes the trade-off between content preservation and stylization into consideration, to further improve translation results especially when unpaired training images are provided.

We apply the proposed framework to a wide range of applications, plausible results (see Figure \ref{fig:various_results}) indicate the significance and effectiveness of the designed modules in our method. We summarize the contributions of this work as below: 1) We divide image-to-image translation tasks into three subsets: strongly, normally and weakly constrained translation, according to the requirement of content preservation. 2) We propose StyleFlow, a novel model based on invertible network structure for content-fixed image-to-image translation, which is capable for unpair, multi-modal and multi-domain translation settings. 3) We design a novel Style-Aware Normalization module for efficient content-fixed feature transformation. 4) We demonstrate that StyleFlow outperforms previous methods with high content preservation and admirable stylization in extensive experiments.
\begin{table}[!h]
  \centering
  \caption{Feature comparison of I2I models.}
  \label{tab:comp}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
    Method &  Unpaired & Multimodal & Multi-domain & Invertible & Parameters\\
    \hline
    \textbf{StyleFlow} &  \checkmark & \checkmark & \checkmark & \checkmark & 16.78 M\\
    \hline
    DRIT++\cite{lee2020drit++} & \checkmark &\checkmark & \checkmark& & 18.58 M\\
    \hline
    MUNIT\cite{huang2018multimodal} & \checkmark &\checkmark & & & 30.05 M\\
    \hline
    UNIT\cite{liu2017unsupervised} & \checkmark & & & & 22.28 M\\
    \hline
    CycleGAN\cite{zhu2017unpaired} & \checkmark & & & & 11.38 M\\
    \hline
    CUT\cite{park2020contrastive} & \checkmark & & & & 11.38 M\\
    \hline
\end{tabular}
\end{table}
\section{Related Work}
\label{sec:related}
\subsection{Image-to-Image Translation}
\noindent\textbf{Generic Image-to-Image Translation}: The previous generic I2I models can be divided into two categories: VAE-based\cite{ma2018exemplar,zhu2017multimodal,lee2018diverse,liu2017unsupervised,huang2018multimodal,wu2019transgaga} and GAN-based\cite{park2020contrastive,zhu2017unpaired,kim2017learning,yi2017dualgan,chen2020reusing}. However, both of them surfer from the problem of content distortion, even though a lot of regularization methods have been proposed to reduce the impact, including cyclic consistency, self-reconstruction, etc. For weakly constrained translations, where the content can be heavily modified, these methods are appropriate, while for strongly constrained and normally constrained translations, they do not perform well. Our proposed model can be applied to both strongly constrained and normally constrained translations without the problem of content distortion.\1ex]
\noindent\textbf{Normally Constrained Translation}: Normally constrained I2I translation contains style transfer\cite{gatys2015texture,huang2017arbitrary,sheng2018avatar,gu2018arbitrary,wang2020diversified,chen2016fast,li2017universal,an2020ultrafast}, season and weather transfer\cite{li2021weather}, raindrop removal\cite{qian2018attentive,shao2021selective}, haze removal\cite{engin2018cycle,anvari2020dehaze,fahim2021single}, and image denoising\cite{guo2019toward,shan2019residual,lehtinen2018noise2noise}. In this setting, the source and target domains usually show different visual effects, such as weather conditions and artistic styles, but share similar structural information, the primary objective is to transfer the overall visual effects of source domains to match those in target domains. Previous work have shown plausible overall visual results in these tasks, while certain level of content distortion can be found when we zoom in to the details of translated images.\
p_X(x) &= p_Y(f(x))|det\frac{\partial f(x)}{\partial x}|\\
  p_Y(x) &= p_X(f(x))|det\frac{\partial f(x)}{\partial x}|^{-1}
1ex]
\noindent\textbf{A. Squeeze Operation} serves as the inter-connection between blocks for feature reorganization. It reduces spatial size of feature map by first splitting input feature into smaller patches along spatial dimension and then concatenating patches along channel dimension.\
      y_{i,j} &= s\odot x_{i,j} + b\\
      x_{i,j} &= (y_{i,j} - b) / s

      y_{1:d} &= x_{1:d}\\
      y_{d+1:D} &= g(x_{d+1:D}, m(x_{1:d}))

      y_{d+1:D} &= x_{d+1:D} - m(x_{1:d})

    x_{1:d} &= y_{1:d}\\
    x_{d+1:D} &= y_{d+1:D} + m(x_{1:d})
1ex]
\noindent \textbf{C. Style-Aware Normalization Module}

We proposed a novel Style-Aware Normalization (SAN) module to achieve content-fixed feature transformation. As the results shown in \cite{huang2017arbitrary}, in I2I translation, content and style are both spatial statistical information of images. To be more specific, content is the information that we would like to preserve during the transformation (e.g. shape, semantic information), while style is what we need to change to make the source image 'similar' to the target image (e.g. color, illumination, clarity). Content-fixed transfer means the content information before and after transformation should be retained.\
    f_c &= \frac{f_x - \mu(f_x)}{\sigma(f_x)} = \frac{E(x) - \mu(E(x))}{\sigma(E(x))}\\
    f_s &= concat(\mu({\phi_1(y)}),..,\mu(\phi_4(y)),\sigma({\phi_1(y)}),..,\sigma(\phi_4(y)))
1ex]
\noindent\textbf{Module Details}: 
Unlike AdaIN\cite{huang2017arbitrary} which directly computes  and  from the target image to transfer source features, our affine parameters are based on both source and target images. We argue that even for the same reference target image, different source images should use different affine parameters to help transferred images retain special content features of source images, we name them \textit{content-guided affine (CGA) parameters}. As shown in Figure \ref{fig:overview}-A, mean () and variance () are obtained from source features and style features through a learnable network.


To summarize, given source image feature , and style feature , we have the transferred image feature :

\noindent\textbf{Proof of Fixed-Content}: 
According to Eq \ref{eq:10}, we have the following equation:

Since content is defined as the normalized term of image features, we have

Therefore, the content of the transferred image feature  is equal to the content of source image feature , which proves that our SAN module performs content-fixed feature transformation.


\subsection{Loss Function}
\label{sec:Loss}
Our loss function can be expressed as:

where  is the content loss,  is our proposed aligned-style loss, and 
 is a weighting factor used to trade-off between content and style.  is used to make the generated images smoother.\
      L_{c} = \left \|norm(\phi(\hat{x})) - norm(\phi(x))\right \|_2
1ex]
\noindent\textbf{Aligned-Style Loss}:
Considering that the semantic information extracted from VGG-19 of the source image and the target image are not exactly matched in unpaired translation, we extend the style loss in \cite{huang2017arbitrary} to be \textit{aligned-style loss} by setting a parameter  to adjust the percentage of extracted tensors that are used for loss computation. We define  as an ascending sort function. Given a source image , a target image , and the transferred image , with an energy function , where  () represents a set of pre-trained VGG-19 layers , we could have the chosen index:

where  is the set of indexes of the sorted tensor ,  denotes its total length, and  is the weighting parameter. Therefore,

where  denotes the  channel of the output tensor of the  layer from the set   of a pre-trained VGG-19 encoder.\
      L_{smooth} = ||\nabla\hat{x} \exp(-\lambda_{smooth}\nabla y)||
1ex]
\noindent\textbf{Dataset}: Following previous art-style transfer works, we use MS-COCO\cite{lin2014microsoft} dataset and Wiki-Art\cite{wikiart} dataset in our experiment. The MS-COCO is used as the source domain, while the Wiki-Art is used as the target domain. For training, the images are resized to 300400. While for evaluation, all synthesized images are resized to 256256.\1ex]
\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.8\linewidth,height=0.46\linewidth]{art_results.pdf}
  \caption{Quantitative results of art-style transfer. SSIM index (higher is better) versus Style Loss (lower is better). Ideal case for art-style transfer is at the top-left corner (\textcolor{green}{green star} â€ ). \textcolor{red}{Red dots} indicate our results.}
  \label{fig:style}
\end{figure}
\noindent{\textbf{Quantitative Evaluation}}:
Following \cite{Hong_2021_ICCV}, we evaluates the stylized images quantitatively using SSIM and style loss, where Structural Similarity Index (SSIM) indicates the performance of content preservation, style loss measures the similarity between the transferred image and the target image. As shown in the Figure \ref{fig:style}, our model with  achieves the best performance. As the  increases, our model could have better style loss score, but would have a trade-off in content preservation.
\subsection{Photo-Realism [Strongly Constrained]}
\label{sim2real}
Simulation environments have been widely used in many fields. Synthesizing realistic images from simulated environments is a very challenging task. Photo-realism synthesis aims to translate simulated images to realistic images. In this task, we select several state-of-the-art generic I2I models and a specific photo-realism model EPE\cite{richter2021enhancing} for comparison.\1ex]
\begin{figure*}
  \centering
  \includegraphics[width=1.0\linewidth]{GTA.pdf}
  \caption{Our results compared with several baselines. As shown in the \textcolor{orange}{orange} and \textcolor{blue}{blue} box, our method greatly preserves the content information. (a) Translation from GTA to Cityscapes. EPE\cite{richter2021enhancing}, CycleGAN\cite{engin2018cycle}, and CUT\cite{park2020contrastive} turn the color of the vehicle from white to black and generate trees in the sky. (b) Translation from GTA to KITTI. Both DRIT++\cite{lee2020drit++} and UNIT\cite{liu2017unsupervised} fail to keep the semantic information. MUNIT\cite{zhu2017multimodal} makes everything blurry.}
  \label{fig:gta}
\end{figure*}
\noindent\textbf{Qualitative Evaluation}: As shown in the Figure \ref{fig:gta}, all previous I2I models fail to retain full content information, different levels of content distortion exist. EPE uses an auxiliary segmentation task to help keep the content. However, it still hallucinates trees on building/sky. Our model achieves content-fixed translation without extra information. (More details are shown in the orange box and blue box in the Figure \ref{fig:gta}).\0.5ex]
\noindent \textbf{Multi-domain Image Translation.} In section 
\ref{sim2real}, our model is trained in a multi-domain image translation manner to perform translation from GTA to Cityscapes and GTA to KITTI at the same time. As shown in  Figure \ref{fig:multi-domain}, our model did learn how to transfer source images to different styles for different domains, while maintaining high content preservation for each output.\0.5ex]
\noindent \textbf{Ablation Study.}
As shown in the Figure \ref{fig:ablation}, we conduct ablation study to verify the effectiveness of our loss functions, using translation from GTA to KITTI as an example. Without smooth loss, the generated images would have gird-like noises. Without the aligned-style loss (), the overall visual effect is reduced, which makes the generated images look unrealistic. Content loss () is used to preserve the semantic information, without this term, the generated images can become distorted and over-stylized.\0.5ex]
\noindent \textbf{Broader Impact.}
Our proposed generative model could be used as a tool to further eliminate the gap between simulation and reality, which can be widely used in self-driving and medical areas. The use of image synthesis would not lead to privacy issues, but might create fake news. More efforts are needed in the future to develop regulations to restrict the usage of synthesized data.
\section{Conclusion}
In this paper, we categorize image-to-image translation problem into three tasks: strongly constrained translation, normally constrained translation and weakly constrained translation. We proposed an invertible network StyleFlow, with Style-Aware Normalization (SAN) module for content-fixed image-to-image translation. We have proved that our method can achieve full content preservation during translation. Qualitative and quantitative results show that our model is more suitable for strongly and normally constrained translation, while the previous work have better performance on weakly constrained translation.


\begin{figure*}
  \centering
  \includegraphics[width=1.0\linewidth,height=1.2\linewidth]{Figures_supp/art_single.pdf}
  \caption{Image-guided style transfer results from MS-COCO\cite{lin2014microsoft} to WikiArt\cite{wikiart}}
  \label{fig:ars}
\end{figure*}

\begin{figure*}
  \centering
  \includegraphics[width=1.0\linewidth,height=1.2\linewidth]{Figures_supp/art_all.pdf}
  \caption{Comparison on style transfer from MS-COCO\cite{lin2014microsoft} to WikiArt\cite{wikiart} between StyleFlow and the previous methods.}
  \label{fig:art_all}
\end{figure*}

\begin{figure*}
  \centering
  \includegraphics[width=1.0\linewidth,height=1.2\linewidth]{Figures_supp/GTA_guide.pdf}
  \caption{Image guided photo-realism on GTA\cite{richter2016playing} to Cityscapes\cite{cordts2016cityscapes} and GTA to KITTI\cite{geiger2015kitti}.}
  \label{fig:gg}
\end{figure*}

\begin{figure*}
  \centering
  \includegraphics[width=1.0\linewidth,height=1.2\linewidth]{Figures_supp/GTA_size.pdf}
  \caption{Results on photo-realism from GTA\cite{richter2016playing} to Cityscapes\cite{cordts2016cityscapes} by sampling-based inference.}
  \label{fig:gs}
\end{figure*}

\begin{figure*}
  \centering
  \includegraphics[width=1.0\linewidth]{Figures/inter.pdf}
  \caption{Continuous outputs from one single model for dark-to-light translation.}
  \label{fig:interpolation}
\end{figure*}

\begin{figure*}
  \centering
  \includegraphics[width=1.0\linewidth]{Figures_supp/edges2shoes_multimodal.pdf}
  \caption{Diverse outputs for edges-to-shoes translation.}
  \label{fig:edge2shoe_multimodal}
\end{figure*}

\begin{figure*}
  \centering
  \includegraphics[width=0.9\linewidth]{Figures_supp/wikiart_multimodal.pdf}
  \caption{Diverse outputs for arbitrary style transfer.}
  \label{fig:wikiart_multimodal}
\end{figure*}

\begin{figure*}
  \centering
  \includegraphics[width=0.8\linewidth]{Figures_supp/edges2shoes.pdf}
  \caption{Translation from edges to shoes\cite{xie15hed},\cite{fine-grained}}
  \label{fig:edge2shoe}
\end{figure*}

\begin{figure*}
  \centering
  \includegraphics[width=1.0\linewidth,height=1.2\linewidth]{Figures_supp/summer.pdf}
  \caption{Translation from summer to winter\cite{zhu2017unpaired}.}
  \label{fig:summer}
\end{figure*}

\begin{figure*}
  \centering
  \includegraphics[width=1.0\linewidth,height=1.2\linewidth]{Figures_supp/haze.pdf}
  \caption{Results on image dehazing.\cite{Dense-Haze_2019}}
  \label{fig:haze}
\end{figure*}

\begin{figure*}
  \centering
  \includegraphics[width=1.0\linewidth,height=1.2\linewidth]{Figures_supp/light.pdf}
  \caption{Results on image enhancement\cite{wei2018deep}.}
  \label{fig:light}
\end{figure*}

\begin{figure*}
  \centering
  \includegraphics[width=1.0\linewidth,height=1.2\linewidth]{Figures_supp/color.pdf}
  \caption{Results on colorization\cite{anwar2020image}.}
  \label{fig:color}
\end{figure*}

\begin{figure*}
  \centering
  \includegraphics[width=1.0\linewidth,height=1.2\linewidth]{Figures_supp/sim.pdf}
  \caption{Results on translation from sim to real\cite{zia2021surgical}.}
  \label{fig:sim}
\end{figure*}

\clearpage

\bibliographystyle{plain}
\bibliography{paper_arxiv}
\end{document}
