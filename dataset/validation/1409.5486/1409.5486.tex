\setlength\abovedisplayskip{3pt}
  \setlength\belowdisplayskip{3pt}
  \setlength\abovedisplayshortskip{3pt}
  \setlength\belowdisplayshortskip{3pt}
\subsection{Proof of Theorem~\ref{thm:1}}
\label{appendix}

\begin{proof}
Suppose  satisfies  with probability , then the set of states of  written as  can be represented as a disjoint union of  transient states and  closed irreducible sets of recurrent classes~\cite{Durrett2012}:



\begin{prop}
\label{prop:rec}
Policy  satisfies  with probability  if and only if there exits  such that  and  for all recurrent classes .
\end{prop}

We omit the proof of Proposition~\ref{prop:rec}; however, it readily follows Definition~\ref{def:accept}.


Let  be the finite set of optimal policies that optimize the expected future utility.  We constructively show that for large enough values of , the discount factor and , the negative reward on non accepting states, all policies  satisfy  with probability .



Suppose  does not satisfy . Then one of the following two cases must be true:

\begin{itemize}
\item {\bf Case 1:} There exists a recurrent class  such that . This means with policy  it is possible to visit  only finitely often.
\item {\bf Case 2:} There exists  such that  is recurrent. That is for some recurrent class of the ,  . This translates to the possibility of visiting a state in  infinitely often.
\end{itemize}

 We let , where  is the set of optimal policies that do not satisfy  by violating {\bf Case 1} ({\bf Case 2}). Notice that this is not a disjoint union. 


In addition, we know that the vector of utilities for any policy  is , where  is the number of states of :



In this equation  and  and  is the transition probability matrix with entries  which are the probability of transitioning from  to  using policy .

We partition the vectors in equation~\eqref{eq:utility} into its transient and recurrent classes:



In equation~\eqref{eq:matrixutility},  is a vector representing the utility of every transient state. Assuming we have  transient states,  is a  probability transition matrix containing the probability of transitioning from one transient state to another. 
Assuming there are  different recurrent classes,  is a zero matrix representing the probability of transitioning from any of the  recurrent classes, each with size  to any of the transient states. This probability is equal to  for all of these entries.


On the other hand,  is a  matrix, where each  is a  matrix whose elements denote the probability of transitioning from any transient state ,  to every state of the th recurrent class .


Finally,  is a block diagonal matrix with  blocks of size  for every recurrent class that states the probabilities of transitioning from one recurrent state to another. It is clear that  is a stochastic matrix since each block of  is a stochastic matrix~\cite{Durrett2012}.
From equation~\eqref{eq:matrixutility}, we can conclude:



Also with some approximations, a lower bound on  can be found:



\noindent {\bf Case 1:}\\
We first consider all policies . These are policies that violate case , thus for  there exists some  such that . We choose any state .
Then we use equation~\eqref{eq:ur} to show that any policy  over state  has a non-positive utility .

In equation~\eqref{eq:up0}, , ,  is the vector that corresponds to transition probabilities from  to any other state in the same recurrent class using policy .   is the vector for the reward values of the recurrent class . Since none of these states are in , we conclude that for all elements . 


We first consider the case that  is in a recurrent class of .
\begin{itemize}
\setlength{\leftmargin}{0pt}
\item If  is in some recurrent class , by proposition~\ref{prop:rec}, . Therefore, there is at least one  such that  and .
In addition, we know that all states in  are in the transient class. Therefore the vector of rewards in this recurrent class  as defined previously contains non-negative elements. That is for all elements  and there exists at least one .



We have shown that for some , and any policy ,  which contradicts the optimality assumption of  for the case where . Thus, we must have that  is in a transient class of . 


\item If  is in a transient class , we first find a lower bound on , and show this lower bound can be greater than any positive number for large enough choice of . 
Note that at minimum all the states in the transient set of  will have utility of , that is , and there will be only one state  that lives in the recurrent class. That is  has a positive reward. 

\begin{prop}
\label{prop:N}
For transient states ,  there exists  such that:

that is, the infinite sum is bounded~\cite{Durrett2012}. 
\end{prop}



We assume  is the number of transient states.

In addition,  is a stochastic matrix with row sum of ~\cite{Durrett2012}.










\begin{prop}
\label{prop:bound}
If  is the probability of returning from a state  to itself in  time steps, there exists a lower bound on .\\ 
First, there exists  such that  is nonzero and bounded. That is  visits itself after  time steps with a nonzero probability. \\
Also we know
. Therefore:

\end{prop}







 Going back to equation~\eqref{eq:lowerbound}, we find a stricter lower bound on the utility of every state  using proposition~\ref{prop:bound}:



 
 Here  and , where  is a block matrix whose nonzero elements are  bounds derived from proposition~\ref{prop:bound}.
 
 



For a fixed , we can select a large enough  so equation~\eqref{eq:case1b} holds for all .
This condition implies equation~\eqref{eq:case1c} which contradicts with optimality of any . Therefore,  cannot be optimal unless it visits  infinitely often.
\end{itemize}

\noindent {\bf Case 2:}\\
Now we consider case , where .  Here for some , . In addition, this state is in the transient class of , .
Using the same procedure as the previous case, we find the following upper bound.







We know that  is in the recurrent class while using policy . So we can use equation~\eqref{eq:ur} to find a bound on the utility. An upper bound assumes that all the other states in the recurrent class have positive reward of .



If the following condition in equation~\eqref{eq:optcondition} holds, we conclude that for a state ,   which violates the optimality of .



We only need to enforce:


Since there are only a finite number of policies in , from all policies , we can find  such that:


Therefore equation~\eqref{eq:optcondition2} can be simplified:




We assumed without loss of generality . For a fixed value of , we choose  small enough so all  satisfy equation~\eqref{eq:optcondition3d} and violate the optimality condition.

As a result, any optimal policy must satisfy case , which is visiting a state in  only finitely often.

For optimal policies , we need to find  and  such that both conditions for case 1 and case 2 are satisfied. That is:



We select a pair of  and  so the system of equations in~\eqref{eq:final} is satisfied. This solution can be found as follows:

First, for a small real number , we select  so:



Then,  is selected so the following holds:


The pair of  satisfy equation~\eqref{eq:final}, and as a result none of the policies  are optimal.
\end{proof}
















