

\documentclass{amsart}
\usepackage{graphicx}

\newcommand{\ZZ}{\mathbf{Z}}
\newcommand{\ep}{\varepsilon}
\newcommand{\ceil}[1]{\left\lceil{#1}\right\rceil}
\newcommand{\floor}[1]{\left\lfloor{#1}\right\rfloor}

\DeclareMathOperator{\GF}{GF}

\newtheorem{thm}{Theorem}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\theoremstyle{definition}
\newtheorem{example}[thm]{Example}
\theoremstyle{remark}
\newtheorem*{rem}{Remark}

\begin{document}

\title[Multipoint Kronecker substitution]{Faster polynomial multiplication via multipoint Kronecker substitution}
\author{David Harvey}
\begin{abstract}
We give several new algorithms for dense polynomial multiplication based on the Kronecker substitution method. For moderately sized input polynomials, the new algorithms improve on the performance of the standard Kronecker substitution by a sizeable constant, both in theory and in empirical tests.
\end{abstract}

\maketitle

\section{Introduction}
\label{sec:intro}

The \emph{Kronecker substitution method} is an algorithm for computing the product of two polynomials. The basic idea was originally suggested by Kronecker \cite{kronecker} as a means of reducing problems concerning multivariate polynomials to univariate polynomials; later Sch\"onhage \cite{schonhage} suggested a similar idea to reduce multiplication in  to multiplication in . The technique is widely used in practice: for example, the Magma computer algebra system uses Kronecker substitution to multiply polynomials in  in some cases \cite{magma}, and Victor Shoup's NTL library \cite{ntl} uses Kronecker substitution to reduce multiplication in  to multiplication in .

The reduction from  to  is best explained by an example. To compute the product
 
one evaluates the polynomials at , and then computes the \emph{integer} product

The coefficients of  may then be read off from the digits of , since the choice of evaluation point ensures that the coefficients do not overlap:
 
Of course, on real hardware, one evaluates at  rather than , since then the packing and unpacking phases are accomplished efficiently (in linear time) by various bit shifting and masking operations.

The main advantage of this algorithm for multiplication in  is that it places the burden of computation on existing highly optimised software libraries for multiprecision integer arithmetic, such as the GMP library \cite{gmp}. This point of view is discussed further in \cite{fateman}. However, the algorithm also has a disadvantage: it introduces unwanted zero-padding. In the example above, if one computes the integer product by hand using the classical algorithm, about \emph{three-quarters} of the digit-by-digit products involve multiplying by zero, because about half of the input digits are zero. Clearly it is desirable to somehow skip these redundant multiplications.

In this paper we give several new algorithms that mitigate this inefficiency, without sacrificing the advantage mentioned above. The basic idea is to evaluate not just at a single point, but at several points; instead of reducing to a single multiplication, we reduce to several smaller multiplications. The evaluation points are chosen carefully so that the packing and unpacking phases are still highly efficient.

The benefit of this approach accrues as follows. Let  denote the time required to multiply -bit integers. Suppose that the standard Kronecker scheme reduces a given polynomial multiplication problem in  to a multiplication of two -bit integers in . The cost is , plus  overhead associated with the packing and unpacking steps. One of our new algorithms (\S\ref{sec:integer-four}) reduces this problem instead to \emph{four} multiplications of size about , so the cost becomes .

The relation between  and  depends on the underlying integer multiplication algorithm, and on the implied constants in the  terms. If  is very large, then one has available FFT-based methods for integer multiplication, and  behaves roughly like , leaving little difference between the two strategies. However, if  is relatively small, then  will typically behave like  for some ; for example, GMP version 4.2.2 implements classical multiplication (), Karatsuba multiplication (), and Toom-Cook 3-way multiplication (). In this situation we expect the new algorithm to win by a factor of about . Under the classical multiplication regime, the theoretical gain is a factor of , which is equivalent to `skipping' all of the redundant multiplications by zero in the example given above. Finally, when  is sufficiently small, we expect the  overhead to dominate, and the usual Kronecker substitution becomes faster, simply because it has a smaller constant in the  term. For the implementation discussed in \S\ref{sec:demo}, we find that the  term already interferes in the region where  is somewhat less than , so we never quite achieve the factor of four speedup suggested by the above theoretical analysis.

Another interesting feature of the new algorithms is that they are trivially parallelisable, since they reduce the original problem to several independent multiplications. For example, using the algorithm described in \S\ref{sec:integer-four}, one can easily split a large multiplication problem in  into four threads, without needing to parallelise any of the internals of the integer multiplication routine.

The organisation of this paper is as follows. In \S\ref{sec:polynomial} we first consider the technically simplest case of reducing multiplication in  to multiplication in . The standard Kronecker substitution evaluates at  for a suitably large . We give three new algorithms. The first algorithm evaluates at  and , where  is about half the size of . The second algorithm evaluates at  and , but only works for rings in which the multiply-by-two map is injective. The third algorithm combines these two algorithms, evaluating at four points , ,  and , where  is about . In \S\ref{sec:integer} we adapt these algorithms to the case of the substitution from  to . The phenomenon of carries in integer arithmetic (the archimedean property of ) makes this slightly more complicated than the bivariate case. Finally in \S\ref{sec:demo} we give some examples of timings of an implementation of the algorithms for the specific problem of multiplication in . We observe that the `four-point' variant is almost twice as fast as the standard Kronecker substitution, over a large range of problem sizes.


\section{The polynomial case}
\label{sec:polynomial}

Let  be a commutative ring with identity. We will regard a polynomial  as a vector of coefficients of a certain known length  (i.e.~ the coefficient of  is permitted to be zero). Similarly a bivariate polynomial  will be treated as a rectangular array of coefficients, with a certain length  with respect to  and a certain length  with respect to . We write such a  as , where each  has length .

Throughout this section we fix two polynomials ; we are interested in computing their product . For simplicity we assume that  and that . We denote these by  and  respectively, and assume that  and . We then have  and . It is not difficult to adapt all of the algorithms below to the case where  and  have different lengths.


\subsection{The standard Kronecker substitution}
\label{sec:polynomial-std}

Let . We evaluate at , that is, we compute
 
as an element of . Since , this evaluation step consists simply of writing down the coefficients of , with  zeroes between  and . The polynomial  has length . We evaluate similarly for , and then multiply in  to obtain . Note that
 
and  for each . Therefore the coefficients of  do not overlap those of  in  for any , so it is easy to read off the coefficients of . We obtain:

\begin{prop}
\label{prop:polynomial-std-ks}
The standard Kronecker substitution reduces the problem of computing  to multiplying two polynomials of length  in .
\end{prop}


\subsection{Reciprocal evaluation points}
\label{sec:polynomial-recip}

In our first variant, we evaluate at  and , where . We obtain

(The normalising factor  ensures that we have an element of  rather than of .)

Since  is exactly , these two evaluations are obtained by concatenating the coefficients  directly, with no zero-padding in between; they differ only in the order of concatenation.

We perform similar evaluations for , and then compute the products

These are both multiplications of polynomials in  of length .

We must now show how to recover the coefficients of  from knowledge of the two polynomials

Note that each  has length , so the coefficients of  generally \emph{do} overlap those of , in both sums. However, there are two exceptions:
\begin{itemize}
\item The lowest  terms of  are precisely the lowest  terms of .
\item The highest  terms of  are precisely the highest  terms of .
\end{itemize}
We therefore completely recover  by gluing together these two halves. Then we subtract  from the appropriate position in both sums, revealing the two halves of . We repeat this procedure for  to completely determine .

We obtain the following:
\begin{prop}
\label{prop:polynomial-recip-ks}
The `reciprocal' Kronecker substitution reduces the problem of computing  to two multiplications of polynomials of length  in , plus  subtractions in .
\end{prop}

\subsection{Negated evaluation points}
\label{sec:polynomial-negate}

The next algorithm only works for rings in which the multiply-by-two map is injective, so for example the algorithm does not work over a field of characteristic two.

As in \S\ref{sec:polynomial-recip}, put . Evaluate at  and , obtaining
 
The first value is obtained by simply concatenating the coefficients of  without any intervening zero-padding. The second value is obtained in the same way, but with the sign alternating on each chunk.

Perform a similar evaluation on , and then multiply to obtain . As in the `reciprocal' algorithm, these are both multiplications of polynomials of length .

Now decompose  into even and odd parts  and  as
  
where ,  and . That is,
 
We find that , and inverting the system we obtain

Since , we are able to read off the coefficients of  from , and those of  from . Therefore we have:
\begin{prop}
\label{prop:polynomial-negate-ks}
The `negated' Kronecker substitution reduces the problem of computing  to two multiplications of polynomials of length  in , plus  additions/subtractions in  and  divisions by  in .
\end{prop}


\subsection{Four evaluation points}
\label{sec:polynomial-four}

We continue to assume that doubling is injective in . The final algorithm we present takes advantage of the fact that the key ideas of the `reciprocal' and `negated' variants are essentially orthogonal, and may be combined.

Let . We evaluate at , ,  and :

A new phenomenon arises here: the coefficients  overlap even in the \emph{evaluation} phase, in all four of the above sums, so some additions and subtractions in  are required to compute them. Each of the four polynomials above has length .

We evaluate similarly for , and then multiply to obtain


As in \S\ref{sec:polynomial-negate}, we decompose  into odd and even parts,
  
and we find that

(Note that , whence the normalising factor .)

Now consider  and . Since , we may use the same strategy as in \S\ref{sec:polynomial-recip}, first reading off the low  coefficients of  from  and the high  coefficients of  from . Iterating, we obtain . Applying the same procedure to  and , we obtain .

Finally we have:
\begin{prop}
\label{prop:polynomial-four-ks}
The `four-point' Kronecker substitution reduces the problem of computing  to four multiplications of polynomials of length  in , plus  additions/subtractions in  and  divisions by  in .
\end{prop}


\subsection{Complex evaluation points} We mention here another variant, which does not at present appear to be competitive with the algorithms above, but may suggest further avenues for research.

Put , and let , where  is a primitive fourth root of unity. We evaluate at ,  and . The value  has `real' and `imaginary' parts, both lying in , so we have four polynomials in  altogether. We compute the pointwise products,

The first two multiplications are ordinary multiplications in ; the last one is a `complex' multiplication, and so requires three multiplications in . One checks that by taking appropriate linear combinations of these three products, the coefficients of  may be reconstructed. Unfortunately, one has committed five multiplications instead of four, so the algorithm appears to be inferior to the algorithm of the previous section.

Further variants are possible. For example, take , where  is a primitive cube root of unity, and let . One may evaluate at  and , reducing the problem to eight multiplications in  that are one-sixth the size of the multiplication generated by the standard Kronecker substitution.



\section{The integer case}
\label{sec:integer}

In this section we adapt the above algorithms to the case of the substitution from  to . This is mostly straightforward; the main complication is the management of carries in the reconstruction phase in the analogue of the `reciprocal' algorithm.

Fix two polynomials , with product . We assume that they have the same length , and put . We write , and similarly for . The \emph{length} of an integer  is defined to be  (the number of bits in the binary representation of ). We assume that  and  have \emph{non-negative} coefficients of length at most  for some integer . It should be possible to handle the case of signed coefficients using essentially the same techniques, but we have not checked the details.

Note that the coefficients of the product  have length at most . In fact, they satisfy the slightly stronger inequality

which we will need in \S\ref{sec:integer-recip} to control the propagation of carries.

We assume that integers of length  may be added and subtracted in time , and that we may divide by a power of two in time . We also assume that we can `pack' and `unpack' binary strings in linear time. More precisely, given a list of integers  satisfying  for some integer , we require that we can construct the sum  in time , and given this packed representation, we require that we can reconstruct the sequence of  in time .

 

\subsection{The standard Kronecker substitution}
\label{sec:integer-std}

Let , so that the coefficients of  have length at most . Evaluate at  to obtain  and , multiply to obtain , and unpack  to obtain the .

\begin{prop}
The standard Kronecker substitution reduces the problem of computing  to multiplying two integers of length , plus packing/unpacking overhead of .
\end{prop}


\subsection{Reciprocal evaluation points}
\label{sec:integer-recip}

Let . We evaluate at  and :

Note that there are only  bits of zero-padding between adjacent coefficients in the above sums. We evaluate similarly for , and then compute the integer products


Now we must show how to recover the  from the two sums

The  overlap in both sums, so to retrieve them we must use a similar strategy to that described in \S\ref{sec:polynomial-recip} for the polynomial case. There are added complications due to the presence of carries, which we resolve as follows.

First we write both sums in base . Note that  and  have length at most , so  has length at most . Therefore we may write

where each digit  satisfies . Similarly  has length at most , so we may write

where . Decompose each  into two digits as
 
where

The latter inequality is equivalent to saying that , which follows from \eqref{eq:h-inequality} since


The various quantities we have introduced satisfy the following relations. From \eqref{eq:overlap-1} and \eqref{eq:digits-1} we have , and

where  and where  is the carry generated by the addition . Similarly, from \eqref{eq:overlap-2} and \eqref{eq:digits-2} we have , and

where  and  is the carry generated by the addition . Note that  since we know that  fits into  digits.

Given the  and , we solve these equations for  and  (and incidentally for  and ) by the following iterative procedure. Start with  and . Now let , and suppose that , ,  and  have been computed. From \eqref{eq:carry-2} we have
 
By \eqref{eq:beta-range}, the left hand side is less than . Therefore,  if , and  otherwise. Taking \eqref{eq:carry-2} modulo  for , we deduce the value of :
 
From \eqref{eq:carry-1} modulo  we obtain ,
 
and then  is obtained directly from \eqref{eq:carry-1}. At this stage we have found , ,  and , and so we may repeat the process, until we have determined  and . Finally we obtain  from \eqref{eq:carry-2} with :
 
The  are then reconstructed as .

\begin{prop}
\label{prop:integer-recip-ks}
The `reciprocal' Kronecker substitution reduces the problem of computing  to two multiplications of integers of length , plus packing/unpacking overhead of .
\end{prop}

\begin{example}
We illustrate (in base ten) using the example from \S\ref{sec:intro}. We have , so the evaluation points are  and . Let

We have

with the vertical bars showing boundaries between base- digits.
The pointwise products are

The low digit  of  is the bottom half  of . Comparing  to , we see that there was no carry, i.e. . Thus the top half  is simply , so . We will not follow through the rest of the algorithm in detail, but we can see that by subtracting  from the appropriate positions in both values, we reveal the top and bottom halves of :


\end{example}


\subsection{Negated evaluation points}
\label{sec:integer-negate}

Put  and evaluate at  and . In , there are  bits of zero-padding between adjacent coefficients; in  the padding alternates between zero-padding and `one-padding'. If one only has available a `packing routine' for non-negative inputs,  may be determined by first computing  and  separately, where  and  are the even and odd parts of , and then taking their difference. Note that  may be negative, if the leading monomial of  has odd exponent.

\begin{prop}
\label{prop:integer-negate-ks}
The `negated' Kronecker substitution reduces the problem of computing  to two multiplications of integers of length , plus packing/unpacking overhead of .
\end{prop}

\begin{example}
We illustrate with our running example. The evaluation points are  and :

The pointwise products are

The even and odd coefficients of  are then read off from

\end{example}


\subsection{Four evaluation points}
\label{sec:integer-four}

We take , and evaluate at , ,  and . The structure of the algorithm is the same as that of \S\ref{sec:polynomial-four}, and we omit the details. The reconstruction algorithm of \S\ref{sec:integer-recip} must be used twice: first on  and  to recover the even-index coefficients of , and then on  and  to recover the odd-index coefficients.

\begin{prop}
\label{prop:integer-four-ks}
The `four-point' Kronecker substitution reduces the problem of computing  to four multiplications of integers of length , plus packing/unpacking overhead of .
\end{prop}

\begin{example}
Continuing with the running example, we put . For  we have

and for  we have

The pointwise products are

Then we obtain

from which we recover the even-index coefficients of , using the reconstruction algorithm from \S\ref{sec:integer-recip}. The odd-index coefficients are similarly found from

\end{example}


\section{Example timings}
\label{sec:demo}

The author implemented the algorithms in C for the case of multiplication in , where  fits into a single machine word. More precisely, the implementation first lifts the input polynomials to , multiplies in  using one of the algorithms of \S\ref{sec:integer}, and then reduces the result modulo . The underlying integer arithmetic is performed by GMP's low-level `mpn' routines. The code is freely available under the GNU General Public License (GPL) from the author's web site, \texttt{http://math.harvard.edu/dmharvey/}.

The timing data shown below were obtained on a 1.8GHz 64-bit AMD Opteron machine, kindly supplied by William Stein (funded by NSF grant No.~0555776). Both our code and GMP 4.2.1 were compiled using gcc 4.1.2, with the \texttt{-O2} optimisation flag. We also used Pierrick Gaudry's AMD patch for GMP, which improves the performance of GMP on the Opteron.

Figure \ref{fig:4-bit} shows the relative performance of the three new algorithms (\S\ref{sec:integer-recip}, \S\ref{sec:integer-negate}, \S\ref{sec:integer-four}) compared to the standard Kronecker substitution (\S\ref{sec:integer-std}), where  is a random 4-bit modulus. Figure \ref{fig:48-bit} is the same, but for a 48-bit modulus. We note several interesting features of the graphs:

\begin{itemize}
\item On both graphs, the four curves converge towards  as the degree grows. This reflects the asymptotically quasilinear running time of the underlying integer multiplication routine, as discussed in \S\ref{sec:intro}.
\item The most impressive region is in Figure \ref{fig:48-bit}, between degrees roughly 100 and 5000. In this range, the four-point variant is almost twice as fast as the standard Kronecker substitution.
\item On both graphs, the negated variant has better performance than the reciprocal variant (although the difference is marginal in the 48-bit case). This is due to the added overhead of the complicated reconstruction algorithm of \S\ref{sec:integer-recip}.
\item The new algorithms gain more over the standard Kronecker substitution in the 48-bit modulus case than in the 4-bit modulus case. This occurs because the packing/unpacking overhead takes up a larger proportion of the total time in the 4-bit case.
\item For sufficiently small degree, the new algorithms are inferior to the standard Kronecker substitution, due to packing/unpacking overhead.
\end{itemize}

For reference, we also compared the performance of our code on the same machine to two well-known systems, Magma (version 2.13-5) and NTL (version 5.4.1). The latter has specialised routines for arithmetic on polynomials with word-sized coefficients (the \texttt{zz\_pX} class). Figure \ref{fig:4-bit-compare} compares our implementation of the \emph{standard} Kronecker substitution against both Magma and NTL for a 4-bit modulus, and Figure \ref{fig:48-bit-compare} is for a 48-bit modulus.


\section*{Acknowledgements}

Many thanks to Paul Zimmermann, Andrew Sutherland and William Hart for their comments on this paper, and for stimulating conversations about these algorithms.


\bibliographystyle{amsplain}
\bibliography{kronecker}

\newpage

\begin{figure}
\begin{center}
\includegraphics[width=0.9\textwidth]{graph4.png}
\caption{Comparison of the four algorithms for a 4-bit modulus}
\label{fig:4-bit}
\end{center}
\end{figure}


\begin{figure}
\begin{center}
\includegraphics[width=0.9\textwidth]{graph48.png}
\caption{Comparison of the four algorithms for a 48-bit modulus}
\label{fig:48-bit}
\end{center}
\end{figure}


\begin{figure}
\begin{center}
\includegraphics[width=0.9\textwidth]{compare4.png}
\caption{Comparison with other systems for a 4-bit modulus}
\label{fig:4-bit-compare}
\end{center}
\end{figure}


\begin{figure}
\begin{center}
\includegraphics[width=0.9\textwidth]{compare48.png}
\caption{Comparison with other systems for a 48-bit modulus}
\label{fig:48-bit-compare}
\end{center}
\end{figure}


\end{document}
