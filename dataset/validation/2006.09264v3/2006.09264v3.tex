\documentclass[10pt,twocolumn,letterpaper]{article}
\pdfoutput=1
\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage[ruled,vlined, noend]{algorithm2e}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{pdfpages}
\pagecolor{white}

\usepackage{enumitem}

\usepackage{graphicx}
\newcommand\sbullet[1][.5]{\mathbin{\vcenter{\hbox{\scalebox{#1}{}}}}}



\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy 

\def\cvprPaperID{****} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\setcounter{page}{1}
\begin{document}

\title{Bonsai-Net: One-Shot Neural Architecture Search via Differentiable Pruners}

\author{Rob Geada, Dennis Prangle, Andrew Stephen McGough  \\
Newcastle University, UK\\
{\tt\small \{r.geada2, dennis.prangle, stephen.mcgough\}@newcastle.ac.uk} \\ \and 
}

\maketitle


\begin{abstract}
	One-shot Neural Architecture Search (NAS) aims to minimize the computational expense of  discovering state-of-the-art models. However, in the past year attention has been drawn to the comparable performance of na{\"i}ve random search across the same search spaces used by leading NAS algorithms. To address this, we explore the effects of drastically relaxing the NAS search space, and we present Bonsai-Net \footnote{See \texttt{https://github.com/RobGeada/bonsai-net-lite} for code and implementation details.}, an efficient one-shot NAS method to explore our relaxed search space. Bonsai-Net is built around a modified differential pruner and can consistently discover state-of-the-art architectures that are significantly better than random search with fewer parameters than other state-of-the-art methods.  Additionally, Bonsai-Net performs simultaneous model search and training, dramatically reducing the total time it takes to generate fully-trained models from scratch.
\end{abstract}
\section{Introduction}
Neural Architecture Search (NAS) is a field which contains many promising methods for developing state-of-the-art architectures. However, recent work by Yu \etal \cite{yu2019} and Li \& Talwalkar \cite{li2019} found the performance of these methods to be barely better, if not worse, than random search. Our hypothesis as to why this occurs mirrors that of Yu \etal; the search space for these methodologies is over-constrained to the extent that \textit{any} model from the search space would perform well. This is in part due to the saturation of the CIFAR-10 problem, in that all the cumulative research on best practices has leaked into the design of the search spaces. This means that the search spaces exclusively contain excellent CIFAR-10 architectures, which both eliminates any potential benefit of NAS as well as limits the generalizability of these spaces to other problem domains. 

To address these issues, we have designed a new search space that significantly relaxes the restrictions commonly seen in other NAS methods, such as to remove their implicit design biases but also explore novel design patterns not seen in other works.  Additionally, we have produced a one-shot NAS method capable of efficiently discovering state-of-the-art models within this new space via differentiable, memory-aware pruners, such as to investigate the efficacy of NAS over broader search spaces. 
 
\section{Search Space}
Our search space is based on the search space of DARTS \cite{liu2019} and PC-DARTS \cite{xu2020}, whose models are composed of stacked cells, each cell a directed graph wherein edges are tensor operations and nodes are tensor aggregations. Cells are classified as either a \textit{normal} cell, meaning that tensor dimensionality remains unchanged throughout, or as a \textit{reduction} cell, meaning that spatial dimensions are halved while the channel dimension is doubled. However, in both of these papers and in most existing NAS models, each cell  is restricted to receiving input from the two previous cells  and , and each reduction or normal cell in the model is necessarily homogenous, that is, identical to every other cell of the same type. In their space, there are roughly  possible 4 node cells, 2 different cells per model, and 1 possible set of connections between these cells.

To relax this search space, we allow each cell to receive input from both the previous cell as well as any combination of previous cells, each node can receive input from any combination of previous nodes in the cell, and each edge can contain any combination of the operations in the operation space. Most importantly, each cell in the network is distinct, meaning each cell has a distinct connectivity and operation set. The operation space is equivalent to that of PC-DARTS: identity, 3x3 average pooling, 3x3 max pooling, 3x3 separable and dilated convolutions, and 5x5 separable and dilated convolutions. Therefore, our search space is a significantly larger superset of the DARTS space; an 8 cell model has around  possible 4 node cells, 8 different cells per model, and  different sets of connections between the cells.

\section{Differentiable Pruners}
The differentiable pruner was introduced by Kim \etal \cite{kim2019} in 2019 as a means of pruning FLOPs from neural operations. The authors describe the problem of gating channels via a gate function, where the gate function has a constant derivative of 0 everywhere it is differentiable, meaning that gradient descent cannot be aware of the effects of the gate. To remedy this, the authors add a saw wave  to the gate function , creating a trainable gate function :

where  is the output tensor of the operation to be pruned, and  is the vector of weights that control the gating effects of the pruner. With large enough ,  is effectively 0 or 1 everywhere while still having a constant derivative of 1 everywhere it is differentiable, as visualized in Figure 1 of the supplementary materials. This lets gradient descent track the effects of gating, and thus allows for gating to be a parameter learned simultaneous with model training. Compression is encouraged by adding a compression term to the loss function, computed as follows:

where  is the compression weight (such as to balance this term with the regression or classification loss of the model),  is the vector of the model's compression, and  is the desired compression level. In the original paper, compression is computed as the ratio of the model's current FLOP count to its original FLOP count.

\subsection{Memory-Aware Pruners}
For our purposes, we adapt the differentiable pruner from a channel-wise operation to a generic binary operation, that can gate any connection within the model. The weight vectors of these adapted pruners are of size 1, meaning that the tensors passed through the pruners are either entirely preserved or entirely pruned.  In addition, we compute the memory size  of each potentially pruned connection in the network, to be used in the computation of our , which is the vector of the compression of each cell in the model:
.5em]
	&=  \frac{\sum\limits_{j=0}^{\# cnx \in C_i}{\left(s_j* (G(w_j) + S(w_j))\right)}}{\sum\limits_{j=0}^{\# cnx \in C_i}{s_j}} 
-1em]
 \multicolumn{1}{c}{} & \multicolumn{2}{c}{Bonsai Search Space} \\
\hline
Bonsai-Net 		& 96.65  0.05\% 	& \textbf{1.45\%} \\
Random	  		& 95.19  0.13\%	& -\\
\end{tabular}
\end{center}
\caption{Bonsai-Net performance versus random search, compared to other NAS algorithms.  Results for other algorithms is taken from Yu \etal  \cite{yu2019}. Level-2 random search is chosen for comparison as it aligns with the random-search methodology of Yu \etal.}
\label{tab:comp_performance}
\end{table}
Table \ref{tab:performance} shows that both random searches performed significantly worse than Bonsai-Net, despite using the same amount of GPU memory and roughly similar parameter counts, which indicates the effectiveness of Bonsai-Net in discovering and training models. Bonsai-Net achieves similar mean performance to other state-of-the-art methods as per table \ref{tab:comp_performance}, while requiring 350K fewer parameters than DARTS, 650K fewer than PC-DARTS, and 1.7M fewer than ENAS. Since the search and train phases of Bonsai-Net are codependent, we report the total time from the commencement of the algorithm to the completion of training, which is typically around 3.3 GPU days on an NVidia 1080Ti, nearly twice as fast as the total runtime of second-order DARTS on the same GPU (6.5 GPU days).  

Additionally, table \ref{tab:comp_performance} demonstrates that de-constricting the search space has indeed removed some of the design bias of the restricted space; the average randomly-selected model in the Bonsai search space performs 1.29 percentage points worse than the average randomly-selected model in the constricted search space of other NAS algorithms. However, despite the de-constricted space, Bonsai-Net still discovers state-of-the-art architectures. While future work is necessary to compare how other NAS methods perform in the Bonsai search space, implementing the other methods into a search space that does not enforce cellular homogeneity may prove to be impossible due to VRAM constraints.

\subsection{Observations} 
While observing the pruning patterns of our models, consistent `choices' became noticeable. These seem to indicate that the architectural decisions being made are deliberate, learned choices rather than the result of random chance:
\begin{itemize}
\setlength\itemsep{0em}
\item Models seem to have a preferential ranking of operations, demonstrated by a consistent occurrence frequency of various operations in different types of cells. In normal cells, the most frequently chosen operation by far is identity operations, followed by separable convolutions, then max pooling. Dilated convolutions are rarely chosen, around a dozen per model, while average pooling has never been chosen in any recorded run. Operation counts are shown in Figures 3 and 4 of the supplementary materials.
\item Average pooling operations are always pruned out of the model entirely. Discovering why this is the case presents an interesting avenue for future investigation.
\item At a higher organizational level, Bonsai-Net tends to build ResNet-esque \cite{he2015} patterns within cells, favoring edges that are the summation of identity and separable convolution operations. This is one of the most common learned designs for edges within Bonsai-Net, and is a configuration that is only possible within our search space. Specific examples are shown in Figure 5 of the  supplementary materials.
\item Cells prefer to receive connections from closer cells rather than further ones. The most frequently chosen input for cell  is , then  and so on. 
\end{itemize}
 
\section{Conclusion}
We have created an expanded Neural Architecture Search space, and have designed a one-shot NAS method to produce state-of-the-art results over this new space. Our search space presents a more realistic starting point for NAS; when approaching a novel problem, the intuition and design experience that is encoded into the restricted search spaces of other NAS algorithms does not exist, and thus techniques must be able to operate on significantly broader search spaces. Our results demonstrate that our search space is significantly less constricted than that of other NAS methods, but over this expanded and more realistic space our one-shot NAS algorithm Bonsai-Net still produces state-of-the-art results, while generating fully-trained models in a much faster time and with fewer parameters than other leading algorithms.
{\small
\bibliographystyle{ieee_fullname}
\bibliography{bib}
}
\newpage

\includepdf[scale=0.8, pages={1}, offset=0 -1cm]{bonsai_supplementary}
\includepdf[pages={2,3,4,5}]{bonsai_supplementary}

\end{document}