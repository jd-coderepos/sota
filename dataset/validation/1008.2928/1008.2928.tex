\documentclass[10pt,a4paper]{article}
\usepackage{charter, amsmath, amsthm, amssymb, graphicx, subfigure, color, enumerate}
\usepackage[lmargin=30mm,rmargin=30mm,tmargin=25mm,bmargin=25mm]{geometry}
\renewcommand{\baselinestretch}{1.10}
\setlength{\footnotesep}{\baselinestretch\footnotesep}

\newcommand{\OPT}{OPT}
\newcommand{\ds}{\displaystyle}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{claim}{Claim}
\newtheorem{corollary}{Corollary}

\title{Minimum Entropy Combinatorial Optimization Problems\footnote{
A preliminary version of the work appeared in \cite{CiE}.
}}
\author{
Jean Cardinal\thanks{Universit\'e Libre de Bruxelles, D\'epartement d'Informatique, c.p.~212, B-1050 Brussels, Belgium, jcardin@ulb.ac.be.}
 \and 
Samuel Fiorini\thanks{Universit\'e Libre de Bruxelles, D\'epartement de Math\'ematique, c.p. 216,  B-1050 Brussels, Belgium,  sfiorini@ulb.ac.be.}
\and 
Gwena\"el Joret\thanks{Universit\'e Libre de Bruxelles, D\'epartement d'Informatique, c.p.~212,  B-1050 Brussels, Belgium, gjoret@ulb.ac.be. G. Joret is a Postdoctoral Researcher of the Fonds National de la Recherche Scientifique (F.R.S. -- FNRS).}
}

\date{}

\begin{document}
\maketitle
\sloppy

\begin{abstract}
We survey recent results on combinatorial optimization problems in which the objective function is the entropy of a discrete distribution.
These include the minimum entropy set cover, minimum entropy orientation, and minimum entropy coloring problems. 
\end{abstract}

\section{Introduction}

Set covering and graph coloring problems are undoubtedly among the most fundamental discrete optimization problems, and countless variants of these have been studied in the last 30 years. We discuss several coloring and covering problems in which the objective function is a quantity of information expressed in bits. More precisely, the objective function is the Shannon entropy of a discrete probability distribution defined by the solution. These problems are motivated by applications as diverse as computational biology, data compression, and sorting algorithms.\medskip

Recall that the entropy of a discrete random variable  with probability distribution , where , is defined as:

Here, logarithms are in base 2, thus entropies are measured in bits; and . From Shannon's theorem~\cite{Shanno48}, the entropy is the minimum average number of bits needed to transmit a random variable on an error-free communication channel.\medskip

\begin{figure}[t]
\begin{center}
\subfigure[\label{fig:exsc}set cover]{\includegraphics[scale=.33]{entropysetcover.pdf}} \hspace{1cm} 
\subfigure[\label{fig:exor}orientation]{\includegraphics[scale=.4]{entropyorientation.pdf}} \hspace{1cm}
\subfigure[\label{fig:excol}coloring]{\includegraphics[scale=.4, angle=90]{entropycoloring.pdf}} \hspace{1cm}
\end{center}
\caption{\label{fig:ex}Instances of the minimum entropy combinatorial optimization problems studied in this paper, together
with feasible solutions. The resulting probability distribution for the given solutions is .}
\end{figure}

We give an overview of the recent hardness and approximability results 
obtained by the authors on three minimum entropy combinatorial optimization problems~\cite{CFJ07,CFJ08b,CFJ08a}. We also provide new approximability results on the minimum entropy orientation and minimum entropy coloring problems (Theorems~\ref{thm:ctapx}, \ref{hardchord}, \ref{thm:greedcol}, and \ref{thm:interv1}). Finally, we present a recent result that quantifies how well the entropy of a perfect graph is approximated by its chromatic entropy, with an application to a sorting problem~\cite{POP_SICOMP}.

\section{Minimum Entropy Set Cover}

In the well-known minimum set cover problem, we are given a ground set  and a collection  of subsets of , and we ask what is the minimum number of subsets from  such that their union is . A famous heuristic for this problem is the greedy algorithm: iteratively choose the subset covering the largest number of remaining elements. The greedy algorithm is known to approximate the minimum set cover problem within a  factor, where . It is also known that this is essentially the best approximation ratio achievable by a polynomial-time algorithm, unless NP has slightly super-polynomial time algorithms~\cite{F98}.\medskip

In the minimum entropy set cover problem, the cardinality measure is replaced by the entropy of a partition of  compatible with a given covering. The function to be minimized is the quantity of information contained in the random variable that assigns to an element of  chosen uniformly at random, the subset that covers it. This is illustrated in Figure~\ref{fig:exsc}.
A formal definition of the minimum entropy set cover problem is as follows~\cite{HK05,CFJ08a}.
\begin{description}
\item[\sc instance:] A ground set  and a collection  of subsets of 
\item[\sc solution:] An assignment  such that  for all 
\item[\sc objective:] Minimize the entropy , where 
\end{description}
Intuitively, we seek a covering of  yielding part sizes that are either large or small, but somehow as nonuniform as possible. Also, an arbitrarily small entropy can be reached using an arbitrarily large number of subsets, making the problem quite distinct from the minimum set cover problem.

\paragraph{Applications.}

The original paper from Halperin and Karp~\cite{HK05} was motivated by applications in computational biology, namely haplotype reconstruction. In an abstract setting, we are given a collection of objects (that is, the set ) and, for each object, a collection of classes to which it may belong (that is, the collection of sets  containing the object). Assuming that the objects are selected at random from a larger population, we wish to assign to each object the most likely class it belongs to.

Consider an assignment , and suppose  is the probability that a random object actually belongs to class . We aim at maximizing the product of the probabilities for the solution :

Now note that if  is an optimal solution (supposing we know the values ), then the value  is a maximum likelihood estimator of the actual probability .
Thus, since the probabilities  are unknown, we may replace  by its estimated value:

Maximizing this function is equivalent to minimizing the entropy , leading to the minimum entropy set cover problem.
In the haplotype phasing problem, objects and classes are genotypes and haplotypes, repectively. In a simplified model, a genotype can be modeled as a string in the alphabet , and a haplotype as a binary string (see Figure~\ref{fig:haplotyping}). A haplotype {\em explains} or is {\em compatible} with a genotype if it matches it on every non-? position, and we aim at finding the maximum likelihood assignment of genotypes to compatible haplotypes. It is therefore a special case of the minimum entropy set cover problem. Experimental results derived from this work were proposed by Bonizzoni {\em et al.}~\cite{BVDM05}, and Gusev, M\u{a}ndoiu, and Pa\c{s}aniuc~\cite{GMIP08}.

\begin{figure}
\begin{center}
\includegraphics[scale=.5]{haplotyping.pdf}
\end{center}
\caption{\label{fig:haplotyping}The haplotype phasing problem (simplified setting).}
\end{figure}

\paragraph{Results.}

We proved that the greedy algorithm performs much better for the minimum entropy version of the set cover problem. Furthermore, we gave a complete characterization of the approximability of the problem under the  hypothesis.

\begin{theorem}[\cite{CFJ08a}]
\label{thm:apxsc}
The greedy algorithm approximates the minimum entropy set cover problem within an additive error of  () bits. Moreover, for every , it is NP-hard to approximate the problem within an additive error of  bits.
\end{theorem}

Our analysis of the greedy algorithm for the minimum entropy set cover problem is an improvement, both in terms of simplicity and approximation error, over the first analysis given by Halperin and Karp~\cite{HK05}. The hardness of approximation is shown by adapting a proof from Feige, Lov\'asz, and Tetali on the minimum sum set cover problem~\cite{FLT04}, which itself derives from the results of Feige on minimum set cover~\cite{F98}.

\paragraph{Analysis of the Greedy Algorithm by Dual Fitting.}

We present another proof that 
the greedy algorithm approximates the minimum entropy set cover 
problem within an additive error of . 
It differs from the proof given in~\cite{CFJ08a} 
in that it uses the dual fitting method.

Let  denote the entropy of an optimum solution.
Let  be the collection of subsets of sets in , that is,
. 
The following linear program gives a lower bound on :
2ex]
\textrm{s.t.} &\ds \sum_{S \in \mathcal{S}^{*}, S \ni v} x_{S} = 1 & \forall v\in U \
Indeed, if we add the requirements that  for every set , 
we obtain a valid integer programming formulation of the minimum entropy set cover problem.

The dual of \eqref{primal} reads:
2ex]
\textrm{s.t.} &\ds \sum_{v \in S} y_{v} \leq -\frac{|S|}{n}\log \frac{|S|}{n} 
& \forall S \in \mathcal{S}^{*}
\end{array}

\label{eq:greedy}
\sum_{i=1}^{\ell} -\frac{|S_{i}|}{n}\log \frac{|S_{i}|}{n} =: g.

\tilde y_{v} := - \frac{1}{n} \log \frac{|S_{i}| \cdot e}{n},

OPT &\geq \sum_{v \in U} \tilde y_{v}  \\
& = \sum_{i=1}^{\ell} -\frac{|S_{i}|}{n}\log \frac{|S_{i}| \cdot e}{n} \\
& = g - \log e,

\label{eq:S}
\prod_{i=1}^{\ell} |S_{i}|^{a_{i}} \geq 
\prod_{i=1}^{\ell}\left(|S| - \sum_{j=1}^{i-1}a_{j}\right)^{a_{i}}.

\label{eq:factorial}
\prod_{i=1}^{\ell}\left(|S| - \sum_{j=1}^{i-1}a_{j}\right)^{a_{i}} \geq |S|!

\sum_{v \in S} \tilde y_{v} &= \sum_{i=1}^{\ell} -\frac{a_{i}}{n}\log \frac{|S_{i}|\cdot e}{n} \\
&= -\frac{1}{n} \sum_{i=1}^{\ell} a_{i}\log\left( |S_{i}|\cdot e\right) + \frac{|S|}{n}\log n \\
&= -\frac{1}{n} \log \prod_{i=1}^{\ell} \left( |S_{i}|\cdot e\right)^{a_{i}} + \frac{|S|}{n}\log n \\
&= -\frac{1}{n} \log \left(e^{|S|} \prod_{i=1}^{\ell} |S_{i}|^{a_{i}} \right) + \frac{|S|}{n}\log n \\
&\leq -\frac{1}{n} \log \left(e^{|S|} |S|! \right) + \frac{|S|}{n}\log n \\
&\leq -\frac{1}{n} \log |S|^{|S|}+ \frac{|S|}{n}\log n \\
&= -\frac{|S|}{n}\log \frac{|S|}{n}.

\OPT(G) \leq H \leq \OPT(G) + (1 + \epsilon).

-\sum_{v\in V} \frac{\rho (v)}{m} \log \frac{\rho (v)}{m} = \log m - \frac 1m \sum_{v\in V} \rho (v) \log \rho (v) \leq \OPT + 1.

E \left[\sum_{i=1}^s \rho(v_i) \log \rho(v_i)\right] = \frac sn \sum_{v\in V} \rho (v) \log \rho (v).

P\left[ \left| \sum_{i=1}^s \rho (v) \log \rho (v) - E\left[ \sum_{i=1}^s \rho (v) \log \rho (v)\right] \right| \geq \epsilon s \right] &  \leq  &
2\exp \left( - \frac{2s^2\epsilon^2}{s(\Delta \log \Delta)^2} \right) \nonumber \\
\Rightarrow
P\left[ \left| \frac n{sm} \sum_{i=1}^s \rho (v) \log \rho (v) - \frac 1m \sum_{v\in V} \rho (v) \log \rho (v) \right| \geq \epsilon \frac nm \right] &  \leq  &
2\exp \left( - \frac{2s\epsilon^2}{(\Delta \log \Delta)^2} \right)\nonumber \\
\Rightarrow
P\left[ \left| H - \OPT \right| \geq 1 + \epsilon \right] \leq
P\left[ \left| H - \OPT \right| \geq 1 + \epsilon \frac nm \right] & \leq & 2\exp \left( - \frac{2s\epsilon^2}{(\Delta \log \Delta)^2} \right). \nonumber

\label{eq-lem-Jk-1}
\sum_{v\in P_{j}} |v| \leq 1

\label{eq-lem-Jk-2}
|P_{1}| + \cdots + |P_{i}| \leq k + (k-1) + \cdots + (k-i+1),

r_i^j := 
\begin{cases}
\big(y_i,-i/k \big) &\text{ if } j=0;\\
\big(-i/k, 1/i \big)  & \text{ if } j=1,\\
\big((j-1)/i, j/i \big) & \text{ otherwise}.\\   
\end{cases}

q^*_{i}:= \left\{
\begin{array}{lll}
\big(n + 2(k-i+1)\big) / |J'| & & \textrm{if } i\in \{1,\dots,k\}; \\
0 & & \textrm{otherwise},
\end{array}
\right.

q^{*, J_{k}} := \big(k/ |J'|, (k-1)/ |J'|, \dots, 1/ |J'|, 0, \dots\big).

q^{*, U}_{i}:= \left\{
\begin{array}{lll}
n / |J'| & & \textrm{if } i\in \{1,\dots,k\}, \\
0 & & \textrm{otherwise}.
\end{array}
\right.

\label{logtoprod}
- \sum_i \frac{|\phi^{-1}(i)|}{n}  \log \frac{|\phi^{-1}(i)|}{n} = -\frac 1n \sum_{v\in V} \log \frac{|\phi^{-1}(\phi(v))|}{n} .

\prod_{\phi} := \prod_{v\in V} |\phi^{-1}(\phi (v))| .

\prod_{\phi_G}\geq \prod_i \frac{|S_i|!}{\beta^{|S_i|}} & \geq & \prod_i \frac{|S_i|^{|S_i|}}{(e\cdot \beta)^{|S_i|}} = \frac{\prod_{\phi_{OPT}}}{(e\cdot \beta)^n},

\log \rho + \log e = \log (\Delta  + 2) + \log e - \log 3 < \log(\Delta  + 2) - 0.1423.

\sum_{i=1}^t |S_i| \geq \sum_{i=1}^t |Q_i|,

\label{eq:minlogp}
- \frac1n \sum_{v\in V} \log p_v.

STAB (G) := \mathrm{conv} \{ \mathbf{1}^S : S \text{ independent set of } G \},

H(G) := \min_{p\in STAB(G)} -\frac1n \sum_{v\in V} \log p_v.

g \leq H(G) + \log (H(G) + 1) + O(1).

\end{theorem}

The proof of Theorem \ref{thm:colapxent} is a dual fitting argument based on the identity , that holds whenever  is perfect (Czisar, K\"orner, Lov\'asz, Marton and Simonyi~\cite{CKLMS90} characterized the perfect graphs as the graphs that ``split entropy'', for all probability distribution on the vertices). In particular, Theorem~\ref{thm:colapxent} implies that the chromatic entropy of a perfect graph never exceeds its entropy by more than . It turns out that this is essentially tight~\cite{POP_SICOMP}.\medskip

Theorem~\ref{thm:colapxent} is the key tool in a recent algorithm for the partial order production problem~\cite{POP_SICOMP}. In this problem, we want to bijectively map a set  of objects coming with an unknown total order to a vector equipped with partial order on its positions, such that the relations of the partial order are satisfied by the mapped elements. The problem admits the selection, multiple selection, sorting, and heap construction problems as special cases. Until recently, it was not known whether there existed a polynomial-time algorithm performing this task using a near-optimal number of comparisons between elements of . By applying (twice) the greedy coloring algorithm and the approximation result of Theorem~\ref{thm:colapxent}, we could reduce, in polynomial time, the problem to a well studied multiple selection problem and solve it with a near-optimal number of comparisons. The reader is referred to \cite{POP_SICOMP} for further details.

\paragraph{Acknowledgment.}
We thank the two anonymous referees for their helpful comments on a previous version of the manuscript.
We are especially grateful to one referee for pointing out an error in a previous
version of the analysis of the greedy algorithm by dual fitting.
This work was supported by the Communaut\'e Fran\c caise de Belgique (projet ARC).

\bibliographystyle{plain}
\bibliography{CiE.bib}

\end{document} 
