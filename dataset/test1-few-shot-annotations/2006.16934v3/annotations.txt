[{'LEADERBOARD': {'Task': 'Visual Question Answering (VQA)', 'Dataset': 'VCR (QA-R) test', 'Metric': 'Accuracy', 'Score': '86.1'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering (VQA)', 'Dataset': 'VCR (Q-AR) test', 'Metric': 'Accuracy', 'Score': '70.5'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering (VQA)', 'Dataset': 'VQA v2 test-std', 'Metric': 'overall', 'Score': '74.93'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering (VQA)', 'Dataset': 'VQA v2 test-std', 'Metric': 'yes/no', 'Score': '90.83'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering (VQA)', 'Dataset': 'VQA v2 test-std', 'Metric': 'number', 'Score': '56.79'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering (VQA)', 'Dataset': 'VQA v2 test-std', 'Metric': 'other', 'Score': '65.24'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering (VQA)', 'Dataset': 'VCR (Q-A) test', 'Metric': 'Accuracy', 'Score': '81.6'}}]
