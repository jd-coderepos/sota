\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subfigure}
\usepackage{multirow}


\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy 

\def\iccvPaperID{****} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\begin{document}
	
\title{Sequential Random Network for Fine-grained Image Classification}
	
	\author{Chaorong Li, Malu Zhang, Wei Huang, Fengqing Qin, Anping Zeng, Yuanyuan Huang\\
		Yibin university,Chongqing University of Technology, University of Electronic Science and Technology,\\Chengdu University of Information Technology\\
		{\tt\small lichaorong88@163.com}
\and 
		{\tt\small }
	}
	\maketitle
\ificcvfinal\thispagestyle{empty}\fi
	
\begin{abstract}
	Deep Convolutional Neural Network (DCNN) and Transformer have achieved remarkable successes in image recognition. However, their performance in fine-grained image recognition is still difficult to meet the requirements of actual needs. This paper proposes a Sequence Random Network (SRN) to enhance the performance of DCNN. The output of DCNN is one-dimensional features. This one-dimensional feature abstractly represents image information, but it does not express well the detailed information of image. To address this issue, we use the proposed SRN, which composed of BiLSTM and several Tanh-Dropout blocks (called BiLSTM-TDN), to further process DCNN one-dimensional features for highlighting  the detail information of image. After the feature transform by BiLSTM-TDN, the recognition performance has been greatly improved. We conducted the experiments on six fine-grained image datasets. Except for FGVC-Aircraft, the accuracy of the proposed methods on the other datasets exceeded 99\%. Experimental results show that BiLSTM-TDN is far superior to the existing state-of-the-art methods. In addition to DCNN, BiLSTM-TDN can also be extended to other models, such as Transformer.
\end{abstract}						
\section{Introduction}
Fine-grained image classification aims to classify specific object categories, such as different airplanes, cars, birds, dogs into more detailed subcategories. Compared with the traditional image classification problem, fine-grained image classification has tiny distinctions between classes. It is also affected by the occlusion, scale, posture, etc., in the image resulting in large differences in the appearance of objects. Therefore, fine-grained image classification is more difficult and more challenging. For fine-grained image classification, we can use general image classification methods, such as deep networks (ResNet\cite{2018Learning} and EfficientNet\cite{2019EfficientNet}) and specific fine-grained image classification methods.

\pmb{General image classification network models}: It has been eight years since the emergence of the deep convolutional network AlexNet, after which various network models have been proposed and have achieved brilliant results in artificial intelligence field. Practice shows that stacking more convolutional layers in a hierarchical manner can provide deep networks with the ability to learn complex representations at different levels. In fact, CNN has made great progress in terms of depth (number of layers of the network) or width (network branches); the number of layers of the network has increased from the initial more than ten layers to the current more than 1000 layers. For example, networks such as WideResNet\cite{WideResNet}, Xception\cite{Xception} improve their performance by simultaneously increasing network depth and width. At present, there are many types of networks used for image classification, but their effectiveness in identifying fine-grained images has much room for improvement.

\pmb{Fine-grained image classification methods}: According to the strength of supervision information used in the train, fine-grained classification models can be divided into two categories: Strong supervision methods\cite{2017Look,Zhang2014Part,2014Bird,2015Augmenting} and Weak supervision methods\cite{2015Bilinear,2020Graph,Kim2020,Ji2020Attention,2019Selective,2020WeaklyCVPR,2020Category,8322428} .

In addition to the category label of the image, the strongly supervised models need extra information to obtain better classification accuracy during model training, such as the object bounding box, part annotation and other additional manual annotation information. Part-based R-CNN\cite{Zhang2014Part} uses R-CNN algorithm to detect object-level (such as birds) and its local areas (head, body, etc.). S. Branson et al.\cite{2014Bird} first computes an estimate of the object’s pose which is used to compute local image features, and in turn, used for classification; the features are computed by applying deep convolutional models to image patches that are located and normalized by the pose.

Although the classification model based on strong supervision information has achieved satisfactory classification accuracy, the acquisition of label information is very expensive, which limits the practical application of this type of algorithm to a certain extent. Therefore, an obvious trend in current fine-grained image classification is that the only image-level annotation information is used during model training, and no additional or Part Annotation information is used to achieve classification accuracy comparable to strongly supervised classification models. This is the fine-grained classification model based on weak supervised information. The weakly supervised fine-grained classification model is similar to the strong-supervised classification model, and it also needs to use global and local information. The difference is that weakly supervised fine-grained classification model hopes to achieve better local information without the help of annotation information. Of course, in terms of classification accuracy, the current best weak-supervised classification model still has a gap with the best strong-supervised classification model.

The Neural Activation Constellations\cite{7410493} is to use the convolutional network features to generate key points and then use the key points to extract local area information. Chowdhury et al.\cite{2015Bilinear} designed an end-to-end network model, called Bilinear CNN(B-CNN), and achieved the best classification accuracy on CUB200-2011 dataset. B-CNN uses two CNNs (CNN A and CNN B) to extract the feature of image, and the outputs of two CNNs at each location are combined employing the matrix outer product and average pooled to obtain the bilinear feature representation.

In recent years, many studies have integrated the attention model into the depth model to identify fine-grained images, and the substantial performance improvement is achieved. The advantage of Attention model is that it can focus on the details parts of image to extract the global features. OPAM\cite{Peng2017Object} is an attention-based model including an object-level attention model which is used to localize object for learning object features, and a part-level attention model which is used to select the discriminative parts for exploiting the subtle and local features. Liu et al.\cite{Liu2020Bidirectional} propose BARM to actualize the bidirectional reinforcement for fine-grained image recognition. BARM consists of one attention agent for discriminate part regions proposing and one recognition agent for feature extraction and recognition.

In weakly supervised classification, Transformer\cite{transformerServey} has shown superior performance compared to DCNN in most applications. Transformer model uses stacked Self-Attention/Multi-head-Attention, point-wise, and fully connected layers to be introduced into the classification task. Ruyi et al. proposed ACNet\cite{Ji2020Attention} for fine-grained image classification, which uses the Self-Attention transformer module to enforce the network to capture discriminative detail features of images. Dosovitskiy et al.\cite{ImageWorth} proposed Vision Transformer (ViT) which uses the Multi-Head Attention to extract the local features of image. Kim et al.\cite{Kim2020} introduced a learnable module, Volumetric Transformer Network (VTN) that predicts channel-wise warping fields, to reconfigure intermediate CNN features spatially and channel-wisely.	
\begin{figure}[t]
	\begin{center}
		\includegraphics[width=1\linewidth]{twopairbirds2.jpg}
	\end{center}
	\caption{Two different types of birds. The birds in same row belong to the same category. The red irregular areas are the parts that distinguishes them.}
	\label{twopairbirds}		 
\end{figure}		
\begin{figure}[t]
	\begin{center}
		\includegraphics[width=1\linewidth]{ourmethod.jpg}
	\end{center}
	\caption{The proposed method for fine-grained image classification. We use Sequential Random Network (SRN) to transform the 1D features of DCNN into a feature vector which can express the different area of two categories.}
	\label{ourmethod}		 
\end{figure}					

No matter it is strong-supervised learning or weak-supervised learning, the biggest problem currently faced by them is that it is difficult to accurately focus on the most distinguishing areas of images because the areas are so irregular that the recognition accuracy is limited. For example, in Fig.\ref{twopairbirds}, if the birds are identified from their approximate motion states, the models are likely to make wrong judgments: the birds in the column direction belong to the same category (in fact, the birds in same row belong to the same category). Strongly supervised models and weakly supervised models both attempt to extract salient features by applying Convolutional Neural Networks (CNN) or matrix transformation (Transformer) on two-dimensional images. Different from these methods, we analyze the one-dimensional features of a Deep CNN (DCNN) to obtain significant new features that can distinguish fine-grained images (see Fig.\ref{ourmethod}). In our method, we proposed Sequential Random Network (SRN) for the one-dimensional features transformation which is very effective, and the recognition performance on the six public fine-grained datasets exceeds the State-Of-The-Art (SOTA) methods. The main contributions of this article are in the following two aspects:	
\begin{itemize}									
	\item We proposed Sequential Random Network (SRN) which consists of two basic modules: BiLSTM and Tanh-Dropout (TD) block. BiLSTM uses the correlation information in the forward and backward directions of the sequence to predict the output. It can make the features produced from the same category become more similar through the correlation of the sequence, and the features produced from different categories become more different. The function of TD blocks is similar to the feature selection function. They can eliminate some unimportant features, and perform feedback learning by randomly discarding some feature elements, making the features distinguishable.	
	\item We proposed feature transformation approach for fine-grained image classification by using the proposed SRN based on the DCNN one-dimensional (1D) features. The transformation regards 1D features as sequences and considers the correlation between these feature sequences. Compared with the strongly supervised model based on two-dimensional images, this method does not need to manually label the regions of image; compared with the weakly supervised deep model, it also does not require extra technique such as Attention mechanism to focus on spatial geometric position information.
\end{itemize} 				
\section{RELATED WORK}
\subsection{Feature processing}
Feature processing can be categorized into three types: Feature extraction, Feature transform, and Feature selection.

Feature extraction: Given an image , the features  are obtained by using the extractor : . The most typical case,  is the one-dimensional feature vector. Traditional methods such as SIFT\cite{2020Integrating} and HOG\cite{2013BFO}, or more advanced DCNN and Transformer can be used as the feature extractors of image. Usually, the features extracted by feature extractors may not be able to meet the application requirements. Therefore, further analysis for the features yield from image is very necessary.	

Feature transform: A set of new features  is obtained from a set of existing features  through mapping , denoted by . Features can be one-dimensional or two-dimensional. For one-dimensional features, Principal Component Analysis (PCA)\cite{Jian2002From}, Linear Discriminant Analysis(LDA)\cite{2005Boosting} and Fully connected network\cite{2018Fully}, are commonly used methods. For two-dimensional features, 2D-PCA\cite{Li2010L1}, 2D-LDA\cite{20052D} and 2D convolution\cite{SurfConv} are commonly used methods.

Feature selection: Choosing a subset from all the features by using a mapping , denoted by , where . Usually, the values of the feature elements have not modified, but the number of feature elements has decreased. Decision trees\cite{2015Learning} and random forests\cite{Marko2016Incremental} belong to feature selection methods.	
\subsection{Activation layer and Dropout layer}	
The main function of the activation function is to perform non-linear transformation of data and solve the problem of insufficient expression and classification capabilities of linear models. Another important function is normalization, mapping the input data to a certain range. It’s advantage of is that it can limit the expansion of data and prevent the risk of overflow caused by excessive data. There are several activation functions for DCNN, such as Sigmoid, ReLU and Tanh which is a double tangent activation function, expressed as follows:
					
The Dropout\cite{2014Dropout} is commonly used in DCNN. During training, a subset of the weight set  in the layer is randomly discarded with dropout probability  (that is the retaining probability is ), which is expressed as follows:

Where  represents the input of this layer; * is element-wise multiplication;  is the output of this layer. When performing classification, the output is multiplied by retaining probability , which is expressed as follows:
			
\subsection{BiLSTM}		
\begin{figure}[htbp]
	\begin{center}
		\includegraphics[width=1\linewidth]{lstmcell.jpg}
	\end{center}
	\caption{LSTM cell}
	\label{lstmcell} 
\end{figure}
In recent years, LSTM network\cite{1997Long,2019Script,UnderstandingLSTM} which is a type of Recurrent Neural Network (RNN), have emerged in image processing domain. LSTM has a chain form of repeated neural network modules designed to solve the long-term dependency problem. In a standard RNN, this repeated structural module is a very simple, such as a Tanh layer, while the repeating module of LSTM is more complex and much more effective. The basic cell of LSTM is shown in Fig.\ref{lstmcell}. The output is a combination of several aspects including the current input , the previous input  (hidden units), and the influence of the cell state . Overall, LSTM consists of three gates. Three gates (forget gate, input gates and output gate) combine these inputs and control the output. The forget gate is expressed as follows:		

There are two important parts in the input gates which will update the cell state, denoted by:

The new cell state  is expressed as 
 
The final output  (or ) is the result of multiplying the output  of the output gate and the current cell state 

Bi-directional LSTM (BiLSTM)\cite{2005Framewise,UnderstandingLSTM} is the improved version of LSTM. It is a combination of forward LSTM and backward LSTM; Fig.\ref{BiLSTM} shows the schematic diagram of BiLSBM. The forward LSTM serves the forward calculation, and the input at time  is the sequence data  multiplied by weight  at time  and the output  multiplied by weight  at time . The backward LSTM serves the reverse calculation, the input  at time  is the sequence data  multiplied by Weight  at time  and the output  multiplied by weight  at time . The final output value at time  depends on the sum of the two directions.
\begin{figure}[htbp]
	\begin{center}
		\includegraphics[width=1\linewidth]{bilstm.jpg}
	\end{center}
	\caption{BiLSTM}
	\label{BiLSTM}		 
\end{figure}					
 
 				

\section{Feature transform with SRN}
The process of our proposed one-dimensional feature selection approach SRN is shown in Fig.\ref{flowchart}. The method uses a pre-trained network, such as ResNet, as the backbone, and then trains the pre-trained network model on a specific image dataset by using transfer learning to obtain the transfer learned network model. By using the transfer learned network model, we can obtain the two new one-dimensional DCNN datasets: train set(1D feature) and test set(1D feature). Finally, feature transform network SRN is trained on train set(1D feature) and evaluate the performance on the test set(1D feature).			
\begin{figure}[htbp]
	\centering
	\includegraphics[width=1\linewidth]{flowchart.jpg}
	\caption{The flowchart of the proposed approach.}\label{flowchart}
\end{figure}	

In this work, the SRN consists of two parts BiLSTM and the combination of Tanh layer and Dropout layer (Therefore, it is also called BiLSTM-TDN), as shown in Fig.\ref{bilstm_tdn}. The first part is the feature transformation component, which is a BiLSTM layer. We regard the one-dimensional DCNN features as a sequence. The feature sequence represents the image sequence, and there are many similarities between image sequences, such as similar backgrounds and similar object shapes. Therefore, there is a strong correlation between the feature sequence. This correlation can be employed with BiLSTM to obtain the transformed features which have the more discriminative capability.	
\begin{figure*}[htbp]
	\begin{center}
		\includegraphics[width=1\linewidth]{bilstm_tdn2.jpg}
	\end{center}
	\caption{The proposed SRN, called BiLSTM-TDN.}
	\label{bilstm_tdn}
\end{figure*}							
The second part is a feature selection component composed of Tanh layer and Dropout layer (Tanh-Dropout block, called TD block). TD block first uses Tanh to perform nonlinear mapping on the output of BiLSTM layer and then uses Dropout layer to discard some features randomly. Several TD blocks can be used in BiLSTM-TDN. The flow of a sequence  being processed is as follows:
	

The two parts of the BiLSTM-TDN are mainly to further improve the discriminability of DCNN as much as possible. The following experiments show that if only BiLSTM layer is used without the feature selection part TD blocks, the model’s performance will be greatly reduced (See the experiment and discussion part for details). Dropouts of TD blocks are used to improve generalizability and reduce overfitting. At the front of BiLSTM-TDN is the Sequence layer, which is used to organize the serialized data, followed by a Dropout layer which is used to randomly discard some features before the DCNN features are fed to the BiLSTM layer.

When the classic PCA and LDA calculate the feature transformation matrix, the correlation between the samples is also considered, but all training samples are required to participate in the calculation at the same time; if the number of samples is large, it is obvious that the computational overhead and memory overhead are very large. Unlike PCA and LDA, when BiLSTM-TDN calculates the correlation between samples, the data sequences are entered one by one, which is similar to a fully connected network. Compared to fully connected network, BiLSTM-TDN can use the correlation of several pieces of data in two directions (forward and backward directions) to jointly update the parameters.
\section{Experiments}
\begin{table}[!htbp]
	\centering
	\caption{Dropout probability setting in BiLSTM-TDN.Dropout-1 refers to the dropout probability (1-p) of the first Dropout layer, TD-1 refers to the dropout probability of the Dropout in the first TD block, and so on} \label{params}
	\setlength{\tabcolsep}{1mm}{
		\begin{tabular}{|c|c|c|c|c|c|}
			\hline
			Dataset&Dropout-1 & TD-1 &TD-2&TD-3&TD-4 \\
			\hline
			Flowers-102& 0.6 & 0.6 &0.6&0.5&0.5 \\
			Other datasets&0.5 & 0.5 &0.5&0.3&0.3 \\
			\hline
	\end{tabular}}
\end{table}	
The experiments were conducted on six fine-trained image datasets. FGVC-Aircraft\cite{aircraft2013} dataset contains 10200 images of aircraft, with 100 images for each of 102 different aircraft model variants; the dataset is divided into training, validation and test subsets. Stanford-Cars dataset\cite{cars196} is an image dataset containing 196 types of cars; it has a total of 16185 images, of which 8144 and 8041 images are training images and test images. CUB200-2011 dataset\cite{WelinderEtal2010} has 11788 bird images, including 200 bird sub-categories; the training dataset has 5994 images, and the test set has 5794 images.  Oxford-IIIT Pets dataset\cite{parkhi12a} is a 37-category pet dataset with roughly 200 images for each class. Food-101 dataset\cite{bossard14} contains 101 food categories image data set, a total of 101,000 images, each category has an average of 250 test images and 750 training images. The Oxford Flowers-102 dataset\cite{Nilsback08} is a consistent of 102 flower categories commonly occurring in the United Kingdom; each class consists of between 40 and 258 images; the training set and validation set each consist of 10 images per class; the test set consists of the remaining 6149 images; due to the small number of training samples, we add the validation set to the training set to train the model.
\begin{table*}[!htbp]
	\begin{center}
		\caption{The recognition results of our proposed method Sequential Random Network (BiLSTM-TDN) on six data sets.}
		\label{overview}
		\begin{tabular}{|l|c|c|c||c|c|c|}
			\hline
			\multirow{2}{2cm}{Dataset} & \multicolumn{3}{|c||}{Transfer learning}& \multicolumn{3}{|c|}{BiLSTM-TDN}\\
			\cline{2-7}
			&ResNet18&ResNet101&EfficientNet-b0&ResNet18&ResNet101&EfficientNet-b0\\
			\hline\hline
			FGVC-Aircraft & \pmb{59.34}&52.77&51.54&96.27&\pmb{96.99}&96.54 \\
			Stanford-Cars & 63.98&64.66&\pmb{65.50}&98.63&\pmb{99.03}&98.84 \\
			CUB200-2011 & 62.72&\pmb{68.63}&66.70&98.56&\pmb{99.06}&98.53 \\
			Oxford-IIIT Pets & 83.18&\pmb{87.78}&84.6&99.83&\pmb{99.91}&99.83 \\
			Food-101 & 71.13&\pmb{77.75}&75.58&99.85&\pmb{99.93}&99.80 \\				
			Flower-102 & 85.1&\pmb{85.93}&83.89&99.70&\pmb{99.71}&98.76 \\
			\hline
		\end{tabular}
	\end{center}			
\end{table*}
\begin{table*}[!htbp]
	\centering
	\caption{BiLSTM-TDN experimental results of the combination of multiple model features. \pmb{Effic} refers to the EfficientNet-b0.} \label{combination}
	\setlength{\tabcolsep}{1mm}{
		\begin{tabular}{|c|c|c|c|c|}
			\hline   
			Dataset & ResNet18+Effic&ResNet101+Effic& ResNet18+ResNet101&ResNet18+ResNet101+Effic\\
			\hline
			\hline
			FGVC-Aircraft & 97.65&97.08&96.78 &\pmb{97.77}\\
			Stanford-Cars &99.20 &99.19&\pmb{99.21}& 99.19\\				
			CUB200-2011 &98.77 &99.15&99.22& \pmb{99.24}\\
			Oxford-IIIT Pets & 99.91&\pmb{99.94}&\pmb{99.94}&99.91 \\
			Food-101 & 99.94&99.94&99.94& \pmb{99.97}\\			
Flower-102 & \pmb{99.79} &99.73&99.77& 99.78\\
			\hline
	\end{tabular}}
\end{table*}

In the experiments, we used three pre-trained networks, including ResNet18, ResNet101 and EfficientNet-b0, which were trained on ImageNet. The experimental hardware environment mainly composed of Intel i9 CPU, 64G memory and 2080ti GPU. All images are three-channel color images, and they are all scaled to the size of . The number of BiLSTM hidden state units is set to 600; the dropout probability of each layer is set as Table \ref{params}. Because the training set of flowers has very few samples to avoid overfitting during training, we appropriately increased the dropout probability .

The experimental results of the three pre-trained networks on six datasets are shown in Table\ref{overview}. We see that the performance of the three network models on fine-grained images is very unsatisfactory. The recognition accuracies of most network models are between 60\%-80\%. The best recognition result is 87.78\% (ResNet101) on Oxford-IIIT Pets. Although ResNet101 shows the best performance, it is difficult to compete with other SOTA methods. After feature transformation by BiLSTM-TDN, the performance of these network models has been amazingly improved. Except for FGVC-Aircraft, the recognition accuracies on the other five databases have exceeded 98\%; the recognition accuracy of ResNet101 has risen steadily to over 99\%, and the best accuracy is 99.93\% on Food-101.	Overall, ResNet101 has the best performance; ResNet18 and EfficientNet-b0 have similar performance. It is necessary to know that ResNet18 has only 71 layers in total, while EfficientNet-b0 has 290 layers. Therefore, after feature transformation through BiLSTM-TDN, actually, the performance of ResNet18 is better than the performance of EfficientNet-b0.	

BiLSTM-TDN can also integrate the features of multiple network models. Table \ref{combination} shows the integrated performance of these network models. It can be seen that compared with a single network model, the effect of integration is also obvious, and it is also very robust. On FGVC-Aircraft, the combination of the three models (ResNet18+ResNet101+Effic) has increased to 97.77\% of recognition accuracy; on Food-101, it has increased to 99.97\%; the performance of the model pairwise combination has also been improved to varying degrees.	

In order to illustrate the superiority of our proposed sequential random network, we the comparisons with the state-of-the-art (SOTA) methods on 6 datasets. For the sake of fairness, we do not choose the different models which yield the best record on the datasets, but choose a fixed model ResNet101 and the combined model ResNet18+ResNet101+Effic (the combination of the three models) for the comparison. 

On FGVC-Aircraft, the best recognition accuracy we can find is 94.7\% (most SOTA levels are between 90\%-93\%), and the recognition accuracy of BiSTM-TDN (ResNet101) is 96.99\%, The recognition accuracy of combined features reached 97.77\%, which significantly exceeded the existing record. On Stanford-Cars, the current highest recognition accuracy is 96.20. In contrast, the recognition accuracies of our BiLSTM-TDN are 99.03\% and 99.19\% respectively, increasing about 3\%.
\begin{table}[!htbp]
	\centering
	\caption{Recent results on FGVC-Aircraft} \label{Aircraft}
	\setlength{\tabcolsep}{1mm}{
		\begin{tabular}{c|c|c}
			\hline  
			DCNN & Backbone & Accuracy\\
			\hline		
			API-Net\cite{Peiqin2020} & DenseNet161 & 90.30\\
			MC-Loss\cite{9005389}&B-CNN&92.90\\
			DF-GMM\cite{9156899}&ResNet50&93.80\\
			DAT\cite{Domain2018}&InceptionV3&94.10\\
			DCL\cite{2019Destruction}&ResNet50&93.00\\
			MMAL-Net\cite{Fan2020}&ResNet50&\pmb{94.70}\\
			ACNet\cite{Ji2020Attention}&ResNet50&91.5\\
			R50D,2020\cite{Compounding2020}&ResNet50&92.4\\
			NAT\cite{9328602}&-&91.68\\
			CIN\cite{2020Channel}&ResNet101&92.8\\
			DB\cite{2020Subtle}&ResNet-50&93.5\\
			FDL\cite{2020Filtration}&ResNet50&93.04\\					
			\hline
			BiSTM-TDN & ResNet101 &\pmb{96.99}\\	
			BiSTM-TDN & Res18+Res101+Effic &\pmb{97.77}\\	
			\hline
	\end{tabular}}
\end{table}	
\begin{table}[!htbp]
	\centering
	\caption{Recent results on Stanford-Cars} \label{Cars}
	\setlength{\tabcolsep}{1mm}{
		\begin{tabular}{c|c|c}
			\hline   DCNN & Backbone & Accuracy\\
			\hline			
			API-Net\cite{Peiqin2020} & DenseNet161 & 95.30\\
			MC-Loss\cite{9005389}&B-CNN&94.40\\
			SpinalNet\cite{SpinalNet}&ResNet101&93.35\\ 
			DAT\cite{Domain2018}&AmoebaNet-B&\pmb{96.20}\\
			MGE-CNN\cite{Learningmixture}&Resnet-50&93.90\\
			DF-GMM\cite{9156899}&ResNet50&94.80\\
			DCL\cite{2019Destruction}&ResNet50&94.50\\
			MMAL-Net\cite{Fan2020}&ResNet50&95.0\\
			ACNet\cite{Ji2020Attention}&ResNet50&94.60\\
			NAT\cite{9328602}&-&95.27\\
			CIN\cite{2020Channel}&ResNet101&94.5\\
			DB\cite{2020Subtle}&ResNet-50&94.9\\
			FDL\cite{2020Filtration}&ResNet50&93.71\\
			DeiT\cite{Training2021}&Transformer&93.9\\
			CAP\cite{2021Context}&Xceptio&95.7\\				
			\hline
			BiSTM-TDN & ResNet101 &\pmb{99.03}\\	
			BiSTM-TDN & Res18+Res101+Effic &\pmb{99.19}\\	
			\hline
	\end{tabular}}
\end{table}
On the most commonly used CUB200-2011, our method has demonstrated unparalleled advancement. The two methods have achieved recognition accuracies of 99.06\% and 99.24\% respectively, while the best recognition accuracy of the SOTA methods is 91.8\%. The identification of Oxford-IIIT Pets is much easier. There are several methods that have achieved 97\% accuracy. Our method also achieved a very high accuracy; both of our methods achieved 99.91\%, which is also the latest record on this dataset.	
\begin{table}[!htbp]
	\centering
	\caption{Recent results on CUB200-2011} \label{CUB200}
	\setlength{\tabcolsep}{0.5mm}{
		\begin{tabular}{c|c|c}
			\hline   DCNN & Backbone & Accuracy\\
			\hline
			API-Net\cite{Peiqin2020} & DenseNet161 & 90.00\\
			MC-Loss\cite{9005389}&B-CNN&86.40\\
			DF-GMM\cite{9156899}&ResNet50&88.80\\
			PC-Softmax\cite{Rethinking} & - &89.73\\
			MGE-CNN\cite{Learningmixture}&Resnet101&89.40\\
			DCL\cite{2019Destruction}&ResNet50&87.80\\
			MMAL-Net\cite{Fan2020}&ResNet50&89.60\\
			ACNet\cite{Ji2020Attention}&ResNet50&88.1\\
			CIN\cite{2020Channel}&ResNet101&88.1\\
			DB\cite{2020Subtle}&ResNet50&88.6\\
			FDL\cite{2020Filtration}&ResNet50&88.35\\
			CAP\cite{2021Context}&Xceptio&\pmb{91.8}\\					 
			\hline
			BiSTM-TDN & ResNet101 &\pmb{99.06}\\	
			BiSTM-TDN & Res18+Res101+Effic &\pmb{99.24}\\			
			\hline	
	\end{tabular}}
\end{table}
\begin{table}[!htbp]
	\centering
	\caption{Recent results on Oxford-IIIT Pets} \label{pets}
	\setlength{\tabcolsep}{0.5mm}{
		\begin{tabular}{c|c|c}
			\hline   DCNN & Backbone & Accuracy\\
			\hline
			ViT-H\cite{ImageWorth}&Transformer&\pmb{97.56}\\
			BiT-L\cite{2019Big}&ResNet&96.62\\
			DAT\cite{Domain2018}&AmoebaNet-B&97.1\\
			R50D\cite{Compounding2020}&ResNet50&94.3\\
			NAT\cite{9328602}&-&96.11\\
			Dom-Ad\cite{Scalable2020}&InceptionV3&97.1\\
			CAP\cite{2021Context}&NASNet-M&97.3\\
			\hline
			BiSTM-TDN & ResNet101 &\pmb{99.91}\\	
			BiSTM-TDN & Res18+Res101+Effic &\pmb{99.91}\\	
			\hline	
	\end{tabular}}
\end{table}

Earlier, the recognition accuracy of Food-101 was around 95\%. The CAP \cite{2021Context} that published in 2021 increased the accuracy to 98.6\%, which is the best record we found. Our methods are not to be outdone, the recognition accuracy of the two methods reached 99.93\% and 99.97\% respectively. Regarding the Flower-102, because there were too few training samples, we augmented the number of samples by scaling and shifting the images for BiLSTM-TDN (ResNet101), called BiLSTM-TDN(AU)(ResNet101). The current SOTA methods have achieved good results on Flowers, and the recognition accuracy has been pushed to the level of 99.74\% by the Transformer-based method ViT-L, which is higher than that of BiLSTM -TDN (ResNet101). However, the accuracy of ResNet18+ResNet101+Effic and the BiLSTM -TDN (AU) (ResNet101) also exceed the accuracy of ViT-L, reaching 99.78\%. 
\begin{table}[!htbp]
	\centering
	\caption{Recent results on Food-101} \label{Food101}
	\setlength{\tabcolsep}{0.5mm}{
		\begin{tabular}{c|c|c}
			\hline   DCNN & Backbone & Accuracy\\			
			\hline
			NAT\cite{9328602}&-&91.11\\
			SOP\cite{PowerSOP}&-&88.4\\
			DAT\cite{Domain2018}&AmoebaNet-B&95.3\\
			R50D\cite{Compounding2020}&ResNet50&92.47\\
			Dom-Ad\cite{Scalable2020}&AmoebaNet-B&95.3\\
			CAP\cite{2021Context}&-&\pmb{98.6}\\
			\hline
			BiSTM-TDN & ResNet101 &\pmb{99.93}\\	
			BiSTM-TDN & Res18+Res101+Effic &\pmb{99.97}\\	
			\hline	
	\end{tabular}}
\end{table}						 
\begin{table}[!htbp]
	\centering
	\caption{Recent results on Flowers} \label{Flowers-102}
	\setlength{\tabcolsep}{0.5mm}{
		\begin{tabular}{c|c|c}
			\hline   DCNN & Backbone & Accuracy\\
			\hline
			ViT-L\cite{ImageWorth}&Transformer&\pmb{99.74}\\
			BiT-L\cite{2019Big}&ResNet&99.63\\
			DAT\cite{Domain2018}&AmoebaNet-B&97.7\\
			MC-Loss\cite{9005389}&B-CNN&97.7\\
			R50D\cite{Compounding2020}&ResNet50&98.9\\
			NAT\cite{9328602}&-&99.63\\
			SOP\cite{PowerSOP}&-&97.62\\
			DeiT\cite{Training2021}&-&98.9\\
			CAP\cite{2021Context}&Xceptio&97.7\\				
			\hline
			BiSTM-TDN & ResNet101 &\pmb{99.71}\\	
			BiSTM-TDN (AU)& ResNet101 &\pmb{99.78}\\
			BiSTM-TDN & Res18+Res101+Effic &\pmb{99.78}\\	
			\hline	
	\end{tabular}}
\end{table}					
\section{Ablaton and Discuss}
The above experiments manifest that the performance improvement of our proposed method BiLSTM-TDN in recognition accuracy is very obvious. Except for FGVC-Aircraft, the experimental results of our method on the other five datasets are all higher than 99\%, which overwhelmingly surpasses the existing SOTA methods. We should also be aware that the performance of BiLSTM-TDN is affected by several factors. The two most important factors are the number of hidden layer units of BiLSTM and the dropout probability of Dropout layers.

With other parameters fixed, we tested the influence of the number of hidden units of BiLSTM on the recognition accuracy. The number of hidden layer units used in the test ranges from 400 to 1200 for the experiments on FGVC-Aircraft and Stanford-Cars (see Fig.\ref{hiddennumber}). It can be seen that there is no obvious rule to follow the impact of the number of hidden units on performance, but we can roughly see that ResNet101 achieves better results when its hidden unit is set to 60; when ResNet18's is set to 800 and EfficientNet-b0's is set to 1000, the recognition effect is better.
\begin{figure}[!htbp]
	\begin{minipage}[t]{0.46\textwidth}			
		\includegraphics[width=1\linewidth]{aircraftcolumn2.jpg}
		\centering	
		\subfigure{(a)}
	\end{minipage}
	\centering	
	\begin{minipage}[t]{0.46\textwidth}			
		\includegraphics[width=1\linewidth]{carscolumn2.jpg}
		\centering
		\subfigure{(b)}
	\end{minipage}		
	\caption{Performance is affected by the number of hidden units in BiLSTM on FGVC-Aircraft(a) and Stanford-Cars (b).}	\label{hiddennumber}
\end{figure}					
The performance of our network is affected by randomness, which comes from the initial parameters, gradient descent or Dropout algorithms, etc. The Dropout maybe is one of the biggest sources of randomness in BiLSMT-TDN. In order to test the randomness of our proposed BiLSTM-TDN and its impact on performance, we fixed the network parameters, then trained and test several times to verify its robustness. The dropout probabilities  in the first Dropout layer and the four TD blocks are set to 0.6, 0.6, 0.6, 0.5, 0.5, respectively. We have conducted 5 experiments on Flowers-102, and the recognition accuracies are: 99.78\%, 99.83\%, 99.71\%, 99.78\% and 99.79\%; the average is 99.78\%; the fluctuation range is about -0.05\% to +0.05\%.	
\begin{figure}[!htbp]
	\centering
	\includegraphics[width=1\linewidth]{netconstructure2.jpg}
	\caption{Ablaton tests on Flowers-102. BiLSTM means that there is only BiLSTM in the model; DP1+TD() means that the model contains BiLSTM, the first Dropout layer and  TD blocks, where  indicates the number of TD blocks. combination indicates the ResNet18+ResNet101+Effic.}\label{netconstructure}
\end{figure}			
We have tested the performance of individual BiLSTM and BiLSTM combined with different numbers of TD blocks. It can be seen from Fig.\ref{netconstructure} that the performance of BiLSTM-TDN gradually increases when the number of TD blocks in BiLSTM increased, and the performance is pretty good when four TD blocks are used. If five TD blocks are used, the number of training iterations for the convergence rapid increases and the performance of model does not increase but decreases.
\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.8\linewidth]{feature100.jpg}
	\caption{The features extracted from the last Dropout layer. The two rows are shown the features of two different flower categories.}
	\label{outputDP}
\end{figure}

In order to further verify the superiority of our proposed sequential random network BiLSTM-TDN in feature transformation, we extracted the output vector of the last Dropout layer from four images in Flower-102 dataset. We intercepted the first 100-dimensional features from the output vector, converted them into matrices(see Fig.\ref{outputDP}). Obviously, the features of two images from the same category are very similar, while the features from two different categories are quite different.
\section{Conclusions}
A simple and effective sequential random network, called BiLSTM-TDN, is proposed in this work. BiLSTM-TDN consists of two parts, BiLSTM unit and quite a few consecutive TD blocks. The experimental results on six publicly available datasets show that our method has an overwhelming advantage. The structure of BiLSTM-TDN may not be optimal; therefore, the performance of BiLSTM-TDN should have room for improvement. In future work, we will continue to study the structure and performance of BiLSTM-TDN, such as the relationship between the number of 1D features and the number of BiLSTM hidden units, as well as the number of TD blocks and the dropout probability of Dropout layer. 
{\small
	\bibliographystyle{ieee_fullname}
	\bibliography{egbib}
}			
\end{document}
