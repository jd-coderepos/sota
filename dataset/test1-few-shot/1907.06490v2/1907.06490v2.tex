









\documentclass[journal]{IEEEtran}























\ifCLASSINFOpdf
   \usepackage[pdftex]{graphicx}
\else
\fi






\usepackage{amsmath}













































\hyphenation{op-tical net-works semi-conduc-tor}

\usepackage{cite}

\begin{document}
\title{DeepSUM: Deep neural network for Super-resolution of Unregistered Multitemporal images}


\author{Andrea~Bordone~Molini,
        Diego~Valsesia,
        Giulia~Fracastoro,
        and~Enrico~Magli\thanks{The authors are with Politecnico di Torino -- Department of Electronics and Telecommunications, Italy. email: \{name.surname\}@polito.it. This research has been funded by the Smart-Data@PoliTO center
for Big Data and Machine Learning technologies.}}






\markboth{}{Bordone Molini \MakeLowercase{\textit{et al.}}: DeepSUM: Deep neural network for Super-resolution of Unregistered Multitemporal images}














\maketitle

\begin{abstract}


Recently, convolutional neural networks (CNN) have been successfully applied to many remote sensing problems. However, deep learning techniques for multi-image super-resolution from multitemporal unregistered imagery have received little attention so far. 
This work proposes a novel CNN-based technique that exploits both spatial and temporal correlations to combine multiple images. This novel framework integrates the spatial registration task directly inside the CNN, and allows to exploit the representation learning capabilities of the network to enhance registration accuracy. The entire super-resolution process relies on a single CNN with three main stages: shared 2D convolutions to extract high-dimensional features from the input images; a subnetwork proposing registration filters derived from the high-dimensional feature representations; 3D convolutions for slow fusion of the features from multiple images. The whole network can be trained end-to-end to recover a single high resolution image from multiple unregistered low resolution images. 
The method presented in this paper is the winner of the PROBA-V super-resolution challenge issued by the European Space Agency.

\end{abstract}

\begin{IEEEkeywords}
Multi-image superresolution, convolutional neural networks, multitemporal images, dynamic filter networks
\end{IEEEkeywords}






\IEEEpeerreviewmaketitle



\section{Introduction}

Super-resolution (SR) techniques reconstruct a high-resolution (HR) image from one or more low-resolution (LR) images. Remote sensing is playing a key role in mapping and monitoring the Earth and increasing the availability of high spatial resolution data is crucial for many applications such as urban mapping, military surveillance, intelligence gathering, disaster and vegetation growth monitoring. The ever increasing spatial and spectral resolution of instruments onboard of satellites generates large amounts of data which challenge compression algorithms \cite{Valsesia2014TGARS,Valsesia2016Universal} to meet the available downlink bandwidth. This often results in reduced availability of HR products. Combining this issue with the high cost of hardware for smaller missions, it is clear that developing a new generation of post-processing techniques to enhance the spatial resolution is a critical objective.
 
The approaches to image super-resolution can be broadly framed into two main categories: single-image SR (SISR) and multi-image SR (MISR). SISR exploits spatial correlation in a single image to recover the HR version. However, the amount of information available in a single image is quite limited as some information has inevitably been lost in the LR image formation process. Certain applications provide multiple LR versions of the same scene to be combined by means of MISR techniques, where the reconstruction of high spatial-frequency details takes full advantage of the complementary information coming from different observations of the same scene. 


For remote sensing problems, multiple images of the same scene can typically be acquired by a spacecraft during multiple orbits, by multiple satellites imaging the same scene at different times, or may be obtained at the same time with different sensors. In this context, developing a successful MISR model hinges on solving important problems such as image registration, invariance to absolute brightness variability, time-varying scene content (e.g., due to the time elapsed between multiple acquisitions), and unreliable data (e.g., due to cloud coverage). Deep learning methods have been proved highly successful in the SISR problem but little work has been done for the MISR problem with remote sensing data.

In this paper we present a deep learning architecture addressing MISR applied to a novel dataset provided by the European Space Agency's Advanced Concepts Team in the context of a challenge \cite{web:kelvins}. The goal of the challenge is to super-resolve images from the PROBA-V satellite. The method presented in this paper won the challenge by achieving the highest fidelity on the reconstructed images.
The unique feature of this dataset is that both LR and HR images have been acquired by the same spacecraft, as opposed to previous works where LR images are artificially down-scaled, degraded and shifted versions of an HR image. The images are not simultaneously acquired so temporal variations exist and have to be handled as well in the super-resolution process.

Our main contribution is DeepSUM, a novel CNN-based architecture to combine multiple unregistered images from the same scene exploiting both spatial and temporal correlations. Our method includes image registration inside the CNN architecture, as a subnetwork named RegNet, which dynamically computes custom filters and applies them to higher dimensional image representations. This is in contrast with the vast majority of deep-learning MISR methods in literature \cite{superdeep} that compensate for the motion as a preprocessing step. This approach allows the registration task to leverage the feature learning capabilities of the network in order to be more accurate and resilient to scene variations, and it also optimizes it in an end-to-end fashion for the final goal of reconstructing a single HR image. The proposed method is blind to the image degradation model as it does not require to explicitly model the blur kernel or the noise statistics, and it is robust to temporal variations in the scene as well as occlusions due to cloud coverage. The only assumption of our model is the translational nature of the shift among LR images.  

A preliminary version of this work \cite{Bordone_Proba-V} addressed MISR for the PROBA-V dataset. With respect to our previous work, in this paper we add the registration as part of the network, we improve the use of the image mask information, we expand the experimental results and comparisons with alternative methods and we discuss how a variable number of LR images can be used while the network is designed to handle a fixed input size.

The remainder of this paper is organized as follows. Section \ref{sec:relatedWork} introduces related works on SISR and MISR. Section \ref{sec:challenge} provides details on the novel PROBA-V dataset. Sections \ref{sec:method} and \ref{sec:training} detail the proposed framework and the training procedure. Section \ref{sec:results} contains results and performance evaluation. Section \ref{sec:conclusions} draws some conclusions.







\section{Related work}
\label{sec:relatedWork}
The literature on SR techniques is extensive, both for SISR and for MISR techniques.
SISR approaches can be classified into three main classes: interpolation-based methods (e.g., Lanczos kernels), optimization-based methods and learning-based methods.
Optimization-based methods explicitly model prior knowledge about natural images to regularize this ill-posed inverse problem, and include low total-variation priors \cite{ng2007total},  gradient-profile prior \cite{articleSun2010,6414620} and non-local similarity \cite{4694003,6241428,Zhang2013SingleIS}. Adding prior knowledge restricts the possible solution space generating higher quality solutions. However, the performance of many optimization-based methods degrades rapidly when the upscaling factor increases, and these methods are usually computationally expensive.

Learning-based methods can be pixel-based or example-based. The latter ones are the most popular and they model the correspondence among LR and HR patches for HR patch prediction. After the early work by Freeman et al. \cite{988747} based on searching -nearest neighbors LR-HR patch pairs of the input LR patch to estimate the HR patch, neighbor embedding \cite{1315043,6166881,Bevilacqua2012LowComplexitySS}, sparse-coding \cite{5466111,6175956,5396341,6392274,10.1007/978-3-642-27413-8_47}, anchored neighborhood regression \cite{6751349}, and random forest \cite{7299003} methods were proposed.
More recently deep convolutional neural networks (CNNs) \cite{DnCnnZhang, liu2018non,10.1007/978-3-319-10593-2_13,kim2016deep,kim2015deep_rec,Shi2016RealTimeSI,Lim2017EnhancedDR,Zhang2018ResidualDN} achieved state-of-the-art results for the SISR task. The deep learning paradigm gained attention due to its natural capability of extracting high-level features from images. This is particularly important in remote sensing scenarios where images are highly detailed and their statistics can be very complex.

While most of the deep learning SISR works are related to traditional natural images, lately CNNs have been exploited for remote sensing imagery.
A deep learning based method has been applied by Ma et al. \cite{8600724} on remote sensing images in the frequency domain. Their CNN takes as input discrete wavelet transformed images and adopts recursive block and residual learning in global and local manners to reconstruct HR wavelet coefficients. Jiang et al. \cite{8677274} proposed a generative adversarial network-based edge-enhancement network for robust satellite image SR reconstruction.


Concerning MISR, the first work was proposed by Tsai and Huang \cite{tsaiHuang1984}, who used a frequency-domain technique to combine multiple under-sampled images with sub-pixel displacements to improve the spatial resolution of Landsat TM acquisitions. 
Due to the drawbacks of the frequency-domain algorithms, like the difficulty to incorporate the prior information about HR images, many spatial-domain MISR techniques were proposed over the years \cite{935034}.
Typical spatial-domain methods include non-uniform interpolation \cite{1176931}, iterative back-projection (IBP) \cite{IRANI1991231}, projection onto convex sets (POCS) \cite{Stark:89,413332}, regularized methods \cite{1331445,4060955,shen2009}, and sparse coding \cite{Kato:2017:DSM:3066426.3066466, KATO201564}.


The iterative back projection (IBP) approach was introduced by Irani and Peleg \cite{IRANI1991231}. IBP aims to improve an initial guess of the super-resolved image by back projecting the difference between simulated LR images and actual LR images to the SR image. The updates are iteratively performed attempting to invert the forward imaging process. Drawbacks come from the inability to deal with  unknown or very difficult to model image degradation processes, as well as the difficulty in including image priors. 

Another class of MISR methods is the projection onto convex sets (POCS) \cite{Stark:89,413332}, where restoration and interpolation problems are simultaneously solved to estimate the SR image, after accurate motion compensation. 
Despite allowing an easy incorporation of a priori knowledge, POCS suffers from slow convergence and high computational cost.



Regularized methods are some of the most effective multi-frame SR reconstruction approaches.
In the past decades, many kinds of regularizers have been proposed to preserve edge information while removing image noise, such as Tikhonov regularizer \cite{Hardie98,913592}, Markov random field regularizer \cite{6096366}, total variation (TV) \cite{661187,Marquina2008,Zhang2014} and  bilateral total variation (BTV) \cite{1331445}.
In particular, a few works have been proposed for remote sensing applications. Shen et al. \cite{shen2009} proposed a maximum-a-posteriori (MAP) SR method with Huber prior for MODIS images captured in different dates. Another multi-temporal SR method was proposed by Li et al.\cite{5308275} for  Landsat-7 PAN images. Instead, other works \cite{6134690,Zhang2014} proposed SR methods for multi-angle remote sensing captures. 

In recent years, sparse coding methods based on dictionary learning have successfully been applied to MISR \cite{Kato:2017:DSM:3066426.3066466, KATO201564}. 

Most of the above SR methods assume a priori knowledge of the motion model, blur kernel and noise level, where both blur identification and image registration are performed as a preprocessing stage before reconstruction. However, there are many applications where knowing the image degradation process or reliably estimating it can be challenging. For this reason, many studies have been carried out on blind SR image reconstruction \cite{941854,He2005ARF}. These blind methods usually work in two stages, namely, (1) image registration from LR images, followed by (2) simultaneous estimation of both the HR image and blurring function. In order to reduce the effect of registration errors, some researchers have developed methods to simultaneously estimate the motion parameters and the reconstruction \cite{650116,Zhang2015}. Hardie et al. \cite{650116} presented a  MAP framework to jointly estimate image registration parameters and the HR image. Along the same lines, Zhang et al. \cite{Zhang2015} also integrated the joint estimation of the blurring function.
Moreover, Kato et al. \cite{Kato:2017:DSM:3066426.3066466} recently proposed a sparse coding method where image registration and sparse coding are performed in a unified framework reducing the image registration error. 

In the last years, deep learning based methods have been proposed to solve similar MISR problems in context of video super-resolution \cite{7444187,DBLP:journals/corr/CaballeroLAATWS16}. Most of these works are composed of two steps: a motion estimation and compensation procedure followed by an upsampling process, heavily relying on the prior motion estimation. Recently, Jo et al. \cite{Jo_2018_CVPR} presented a novel end-to-end residual  CNN  to produce a SR image without explicit motion compensation. A CNN is trained to simultaneously solve motion estimation and HR image reconstruction tasks by producing a set of pixel-dependent filters and a residual correction. A similar idea was developed by Tian et al. \cite{Tian2018TDANTD}. However, little work has been done on deep learning MISR methods in the context of remote sensing, which poses specific challenges. Kawulok et al. \cite{SuperDeep_MISR} propose a MISR method that does not fully exploit the benefit of deep learning, restraining their CNN to solve a SISR problem. The fusion of the upsampled LR images is performed by the median shift-and-add method, generating a SR image that is used as initial guess for a classic regularized method. Their method is not end-to-end trainable in a supervised manner and their CNN is trained against LR images obtained by artificially degrading HR images.
Inspired by the recent video super-resolution works, we aim to tackle the MISR problem on satellite images by jointly registering the input LR images and reconstructing the SR image, all within an end-to-end trainable CNN, where the two tasks are optimized jointly. 


\section{The PROBA-V SR dataset}
\label{sec:challenge}

At present, it is difficult to find a dataset collecting both a set of real-world LR observations and the corresponding HR image for the same scene, as captured from the same platform. Many of the works found in the SR literature are based on simulated data, where LR observations for a specific scene are obtained through a degradation and down-sampling process of the HR images by assuming a sensor imaging model. This is a simplified scenario as it either assumes a non-blind problem, i.e., the degradation model can be characterized to some extent, or has the limitation that a too simple degradation model may not accurately match the real one, especially when in presence of temporal variations in the scene content.

The Advanced Concepts Team of the European Space Agency has issued a competition \cite{2019arXiv190701821M} to perform MISR for the images acquired by the PROBA-V satellite. PROBA-V is an Earth observation satellite designed to map land cover and vegetation growth across the entire globe. It was launched in 2013 into a Sun-synchronous orbit at an altitude of 820km. Its payload provides an almost global coverage with 300m LR images and 100m HR images. However, the HR images are acquired with a higher revisit time, roughly one every 5 days, instead of one per day. The dataset gathers satellite data from 74 regions located around the world from the PROBA-V mission. Images are provided as level 2A products composed of radiometrically and geometrically corrected Top-of-Atmosphere reflectance in Plate Carr√©e projection for the RED and NIR spectral bands. The size of the collected images is  and  for the LR and HR data respectively. The images have a single channel with a bit-depth of 14 bits. Each data point consists of one HR image and several LR images (ranging from a minimum of 9 to a maximum of 30) from the same scene. In total, the dataset contains 1160 scenes, 566 are from NIR spectral band and 594 are from RED band. 
The images of a specific scene are captured at multiple times over a maximum period of 30 days. Weather and changes in the landscape pose a limitation in the similarity of the images. Clouds, cloud shadows, ice, water, missing regions, presence of agricultural activities and, in general, human activity are the main sources of inconsistency across these images, thus posing a major challenge for any image fusion method.
Moreover, each image comes with a mask, indicating which pixels in the image can be reliably used for reconstruction (e.g., they are not covered by clouds).
The geometric disparity among the images can be considered as translational only. Subpixel shifts in the content of the LR images do occur and are indeed important for the MISR task.

The unique nature of this dataset (with real LR and HR images captured by the same platform at multiple times) makes for an interesting case study for SR techniques. Developing SR products from multiple, more frequent LR images could simultaneously provide enhanced resolution and higher temporal availability and is therefore an interesting application of MISR. Moreover, having real images of the same scene for both the low and high resolutions enables data-driven methods such as CNNs to learn the inversion of possibly complex degradation models and the best feature fusion strategy to handle temporal variations.

\begin{figure*}[t]
\centering
\includegraphics[width=7.1in]{fig1.pdf}
\vspace{-0.15cm}
\caption{DeepSUM network. The  input bicubic-upsampled and registered images are independently processed by a SISRNet subnetwork, and their features used by the RegNet to compute registration filters to register the feature maps of the  images to each other. The FusionNet subnetwork merges the features of the images to produce a residual image. The residual image is then added element-wise to the average of the registered input to obtain the SR image.}
\vspace{-0.15cm}
\label{fig:Architecture}
\end{figure*}

 
\section{Proposed method}
\label{sec:method}
The proposed method, called DeepSUM, reconstructs a high-resolution image  given a set of  LR images   representing the same scene:

where  represents the model parameters and  represents the mapping function from LR to HR.
 and  are represented as real-valued tensors with shape  and  respectively, where  and  are the height and the width of the input LR frames,  is the number of channels and  is the scale factor. 
While the LR images roughly represent the same scene as the HR image, there are several factors to be considered:
\begin{itemize}
    \item the LR images are not registered with each other; 
    \item the LR images and the HR image are not registered;
    \item the brightness of the HR image may be different from that of any LR image;
    \item the scene changes over multiple acquisitions;
    \item LR and HR images may be covered by different clouds and cloud shadows patterns or affected by corrupted pixels. \end{itemize}

To tackle this problem we propose to employ a supervised deep learning approach, where a CNN learns the residual between bicubic interpolation and the ground truth.
As a preprocessing step, the LR images are bicubically interpolated to the desired size and then fed into a CNN composed of three main building blocks. An overview of the network is shown in Fig. \ref{fig:Architecture}.

The first block, called SISRNet, is a feature extractor that can be seen as a SISR network without the output projection to a single channel. Each of the  input images is processed independently by a sequence of 2D convolutional layers. The convolutional filters are shared along the temporal dimension, i.e., all the  interpolated LR (ILR) images go through the same set of filters.

The second network block, called RegNet, aims at estimating a set of filters to register the  higher dimensional image representations produced by the SISRNet block to each other at integer-pixel precision (notice that the network is working at the same spatial resolution as the HR image, so integer shifts correspond to sub-pixel shifts in the LR data). RegNet has been devised to align  instances with respect to the first, taken as reference, by operating purely translational shifts. Therefore, the output is a set of  2D filters to be applied spatially to each feature map of the  inputs. 

Finally, the third block, called FusionNet, merges the registered image representations in the feature space in a ``slow'' fashion, i.e., by exploiting a sequence of 3D convolutional operations with small kernels. The output is a single super-resolved image.

In the following, we are going to describe each individual block more in detail.

\subsection{SISRNet Architecture}

The goal of SISRNet is to exploit spatial correlations to improve upon the initial bicubic interpolation. In doing so, the network learns to extract visual features that can be conveniently exploited by the subsequent network blocks. SISRNet has multiple 2D convolutional layers whose weights are shared among the  input images, effectively processing each of them independently. Each convolutional layer is followed by Instance Normalization \cite{vedaldi2016instance}. Instance normalization is used in place of Batch normalization \cite{ioffe2015batch} to make the network training as independent as possible of the contrast and brightness differences among the input images.



\subsection{RegNet Architecture} \label{sec:regnet_arch}

RegNet is composed of two sub-blocks: a CNN, and a global dynamic convolutional layer (GDC).
The CNN processes the higher dimensional image representations  generated by SISRNet block and outputs a set of  filters . Each filter  is subsequently applied in the spatial dimensions to each of the channels of  by the GDC layer by means of a 2D convolution in order to register each feature map of  with respect to the reference one . The filters  have a fixed support equal to  that upper bounds the maximum possible translational shift correction to . Notice that there is an implicit assumption that all feature maps of an image require the same shift to be registered with the reference, so that the computed filter is shared channel-wise. The registered feature maps  of the  images are thus obtained as:


being  the 2D convolution operator. The same filters are also applied to the input ILR images to register them in the residual connection:


The novelty of this network is twofold: firstly the filters are dynamically computed for each input image, and secondly it makes use of the features to compute the per-image optimal registration instead of performing it in image space, like most of motion estimation algorithms do. This allows to leverage the powerful feature space of the network to boost the registration performance by making it robust to scene variations. In addition, it is fully differentiable so that the whole architecture can be trained end-to-end.


More in detail, the operations performed by RegNet are depicted in Fig. \ref{fig:Detailed_Regnet}. SISRNet outputs a tensor  with shape  , where  is the number of features, that is reshaped before being fed to RegNet. The features of the first image  are chosen as a reference and a new tensor of size  is built by replicating the reference   times and interleaving each replica with the other  image representations . This sequence of paired reference/unregistered features is then processed by convolutional layers to produce the filters. 
RegNet has a first 3D convolutional layer and a series of shared 2D convolutional layers. The first layer is the key component of registration and it processes the  image representations in pairs by using a stride equal to 2 along the temporal dimension and filters of shape . This operation allows to correlate the features of each   with respect to the ones of the reference  and compute the shift. Notice that this processing in pairs is necessary to avoid any ordering ambiguity and let the network understand that the output is relative to the reference.
After this 3D convolutional layer the output tensor has shape .

This tensor passes through a series of 2D convolutional layers with shared weights along the temporal dimension. 
The last RegNet 2D convolutional layer applies a number of kernels corresponding to the spatial size of the dynamic filters , obtaining a tensor with shape . Each value over the spatial dimensions can be seen as a local estimate of the desired shift based on the local image representation. Since there is a global translational shift by assumption, the values are averaged over the spatial dimensions to obtain a tensor with shape .

Finally, this tensor is passed through a softmax layer, so that the values over the last dimension () add up to 1. The softmax layer promotes a spiked filter with most elements set to zero \cite{Brabandere2016DynamicFN}.
The final tensor represents the  dynamic filters with shape  to be used to register the  image representations with the GDC operation, as in Fig. \ref{fig:DC}.
\begin{figure}[t]
\centering
\includegraphics[width=3.2in]{fig2.pdf}
\vspace{-0.15cm}
\caption{Visual depiction of the RegNet operations to generate the dynamic registration filters from the image features produced by SISRnet.}
\vspace{-0.15cm}
\label{fig:Detailed_Regnet}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=3.0in]{fig3.pdf}
\vspace{-0.15cm}
\caption{GDC: convolution between the dynamic filters and the image representations to align them with respect to the reference.}
\vspace{-0.15cm}
\label{fig:DC}
\end{figure}


\subsection{Mutual Inpainting}
The registered and interpolated feature maps  have regions with unreliable values due to cloud coverage, shadows, corrupted pixels and so on. 
A per-pixel boolean mask is assumed to be available as side information, with the purpose of mapping pixels that can be reliably used for the fusion task. An example on how to obtain such mask is to run a cloud detection algorithm on the image to segment areas with clouds. This is very important because areas occluded by clouds do not provide any useful information.
In order to prevent FusionNet from combining feature maps from multiple images where some have unreliable intensities, we fill the masked areas with values from the feature maps of other images.
The regions with missing or unreliable values in each feature map of each image are filled with values taken from the corresponding feature map of other images having reliable values in those regions, if any are available. In the case none of the images has feature maps with reliable values, we keep those unreliable regions as they are.
Since after RegNet the masks are not aligned with the corresponding image representations, we shift the masks by an integral shift as close as possible to subpixel shift computed and operated by RegNet.
This procedure is performed on both the residual image representations  and the registered input images  right before averaging them.


\subsection{FusionNet Architecture}

The  registered outputs  are progressively fused by the FusionNet subnetwork. FusionNet is composed of  3D convolutional layers where convolution is performed without any padding in the temporal dimension, so that the temporal depth eventually reduces to 1. This architecture implements a ``slow'' fusion process in the feature space, which allows the network to learn the best space to decouple image features that are relevant to the fusion from irrelevant variations and to construct the best function to exploit spatio-temporal correlations \cite{DBLP:journals/corr/CaballeroLAATWS16}. 
Finally, the proposed architecture employs a global input-output residual connection. The network estimates only the high frequency details necessary to correct the bicubically-upsampled input. This is an established technique for image restoration problems using deep learning \cite{DnCnnZhang}, including SISR. However, with respect to SISR, our proposed network is a many to one mapping, so the residual is actually added element-wise to a basic merge of  in the form of their average. Notice that registration of the input images is performed before averaging by means of the same filters produced by RegNet. Hence, the output is computed as follows:

being  the residual estimated by the CNN.

\subsection{Loss Function}
\label{subsec:loss}
Model parameters are optimized by minimizing a loss function computed as a modified version of the Euclidean distance between the SR image and the HR target. Minimizing the Euclidean distance is optimal in terms of the mean-squared error metric. Some deep learning works on SISR attempted to use an adversarial loss \cite{ledig2017photo}. While this approach produces visually pleasing results, it tends to hallucinate information, resulting in lower MSE scores and less reliable products in the context of remote sensing; hence, the adversarial approach has not been followed in the present work.
As we mentioned in Sec. \ref{sec:challenge}, since the PROBA-V satellite does not capture LR images and HR images of a specific ground scene simultaneously, there are discrepancies coming from different weather conditions, changes in the landscape and variable absolute brightness due to the large interval between scene acquisitions. The LR images could be quite different from one another and from the corresponding HR image as well. For this reason, we must make the training objective as invariant as possible to such conditions. In particular, in order to build invariance to absolute brightness differences between  and , the modified loss function equalizes the intensities of the SR and HR images so that the average pixel brightness is the same on both images.
Moreover, since  and  could be shifted, the loss embeds a shift correction.  is cropped at the center by  pixels, i.e., as many pixels as the maximum expected shift. Then all possible patches  of size  for vertical and horizontal shifts  are extracted from the target . All possible Euclidean distances are computed and the minimum one is taken as loss to optimize.
In summary, our loss is as follows:

where  is the cropped version of  and  represents the brightness correction:



The loss is computed by utilizing only the HR image pixels that are marked as reliable by the mask provided with the dataset and the SR image pixels for which at least one out of  LR images were clear. The reason for this is that a cloud in the HR image can never be predicted from terrain data in the IRLR images, so its pixels should not contribute to the loss function. Viceversa, it is also impossible to predict HR terrain if all the IRLR images have concealed regions. 


\section{Training process}
\label{sec:training}
\subsection{Pre-training}
Training the whole network end-to-end from scratch is hard due to several local minima that do not make SISRNet, RegNet and FusionNet work as expected. For example, the gradients computed during training do not sharply discriminate the RegNet task to generate registration filters from the high-resolution feature learning of SISRNet. 

In order to solve this issue, it is possible to pretrain each block to handle its specific subtask, and then combine all the blocks to be fine-tuned in an end-to-end fashion.

\subsubsection{SISRNet pre-training}

As mentioned in Sec. \ref{sec:method}, SISRNet aims to independently super-resolve each of the  input images, while providing useful higher dimensional image representations. SISRNet is pretrained by setting up a pure SISR problem (i.e., a single input image) where an additional projection layer is added at the end, in order to turn the high-dimensional feature space into a single-channel image. SISRNet with the final projection layer is trained with the same objective function of the final training, where the single image reconstruction is compared with the only HR image available for the scene.
The rationale behind this is to make SISRNet exploit spatial correlations as much as possible to generate the best image features for the SISR task. 
Once the pretraining procedure is completed, the final layer is removed and a dataset of feature maps of the input training images is generated to pretrain RegNet. 

\subsubsection{RegNet pre-training}

The purpose of pre-training RegNet is learning to generate registration filters, i.e., filters that shift the feature maps of the  input images with respect to the reference input. This operation would be quite challenging to learn if the whole network was trained end-to-end, so its pretraining is crucial for the overall network performance.
RegNet is pre-trained by casting registration as a multi-class classification problem. Each dynamic registration filter generated by the network is viewed as a probability distribution over the possible shifts with the objective of estimating the correct shift. The number of classes is  since the filter size is . In case of an ideal shift of an integer number of pixels, the predicted filter should be a delta function centered at the desired shift.

The input data to be used for the pretraining of RegNet are the feature maps produced by the pretrained SISRNet for the images in the training set. As described in Sec. \ref{sec:regnet_arch}, the input to RegNet are  feature maps from images of the same scene. These feature maps are then synthetically shifted with respect to the first one by a random integer amount of pixels. The purpose is to create a balanced dataset where all possible  classes (shifts) are seen by the network. The desired output is a filter with all zeros except for a one in the position corresponding to the chosen shift. A cross-entropy loss between the softmax output and the true filter is used to learn the RegNet weights.

\subsection{Final training}
The proposed network is finally trained as a whole, end-to-end for the MISR task. FusionNet is trained from scratch while SISRNet and RegNet weights are initialized from the pretraining procedures.
The concurrent optimization of all the network blocks allows SISRNet to finetune the image representations to facilitate the RegNet task that in turn finds the best registration to boost the efficiency of FusionNet. 

\subsection{Testing phase} \label{sec:testing}
The network architecture presented in the previous sections has been designed to deal with a fixed number  of LR images for a given scene. However, it might happen that more than  images are available and exploiting them could further boost the SR reconstruction performance. Therefore, during testing, one can perform multiple forward passes by using multiple subsets of the available images. Each subset will produce a different SR estimate and, in the end, all SR estimates are averaged. Notice that the estimates should be registered to each other so it is advisable to always use the same LR image as the reference in the network (e.g., one could choose the image with fewer masked pixels). One method to produce useful subsets when more than  LR images are available is to sort them by increasing number of masked pixels and then use a sliding window over  images to compute SR estimates. It must be remarked that the SR estimate quality degrades with increasing number of masked pixels. Also, the estimates are clearly not independent if some images are reused multiple times, but we found consistent gains on our test set, nevertheless.

Defining the optimal function to merge SR estimates or making the network independent of the number of input images could be studied in future research.

\section{Experimental results and discussions}
\label{sec:results}
In this section we perform an experimental evaluation of DeepSUM, comparing it with several alternative approaches. Code and pretrained models are available online\footnote{https://github.com/diegovalsesia/deepsum}.
We first perform an ablation study to highlight the contribution given by RegNet to the overall network performance.
Then, we assess the performance of alternative approaches.


\subsection{Experimental setting}
In the following experiments, we employ both the NIR and RED band datasets described in Sec. \ref{sec:challenge}. We use 396 scenes for training and 170 for testing from the NIR band dataset and 415 for training and 176 for testing from the RED band dataset. Expanding the training set with more scenes should further improve performance as more variability can be captured by our model. Since DeepSUM is devised to work with a fixed size temporal dimension, we train the network using the minimum number of images available for each scene, i.e.,  images. When more images are available we select the 9 clearest images according to the masks.  As a preprocessing step, all LR images are clipped to  since corrupted pixels with large values occur in the LR images throughout the PROBA-V dataset. 

After the bicubic interpolation, each scene is a data-cube of size , from which we extract a dataset with patches of size . 100 random patches are extracted from each scene, resulting in a total of 38400 samples. The patches are extracted considering the available pixel masks: a patch is accepted only if at least 9 scene images are at least 70\% clear and the HR image in the same coordinates is at least 85\% clear. The amount of unreliable pixels is relaxed to keep as much information as possible from the original images at the cost of training with sub-optimal patches. Separate networks are trained for RED and NIR. The proposed network is trained for around 3000 epochs with a batch size of 8 for both RED and NIR.

The Adam optimization algorithm \cite{kingma2014adam} is employed for training, with momentum parameters , , and . The learning rate  is initialized to  for the whole network. 
We employ the Tensorflow framework to train the proposed network on a PC with 64-GB RAM, an Intel Xeon E5-2609 v3 CPU, and an Nvidia 1080Ti GPU.
The exact number of network layers is shown in Fig. \ref{fig:Architecture} and the number of filters is 64 everywhere except for the RegNet's first layer, which has 128 filters. In order to mitigate border effects, we use reflection padding in all 2D convolutions.
Each layer in the network is followed by Leaky ReLU non-linearity, except for the last layer. Each layer in SISRnet and FusionNet is followed by an Instance Norm layer.
Instance normalization \cite{vedaldi2016instance} is used in place of Batch normalization layer to make the network training as independent as possible of the contrast and brightness differences among the input images.
Finally, since the network produces a residual estimate , we normalize  and  so that their difference gives a unit variance residual , thus avoiding any scaling to be performed by the last layer of the network and improving convergence speed.



\subsection{Quantitative results}


The evaluation metric that we consider is a modified version of the PSNR (mPSNR), from which we derived the loss function described in Sec. \ref{subsec:loss}.

The mPSNR computation is meant only for pixels that are not concealed both in the target HR image and in the reconstructed image. Similarly to the loss function during training, this metric has been devised to cope with the high sensitivity of the PSNR to biases in brightness and with the relative translation that the reconstructed image might have with respect to the target HR image. In this case the maximum mPSNR over all possible shifts is considered for evaluation. Note that, by design of the dataset, the maximum shift in the horizontal and vertical directions is equal to 6 pixels.

We remark that this metric was also used to evaluate submissions to the ESA challenge, where the score was computed as a ratio between the mPSNR of the submission and that of the baseline approach, average over all the held-out test set.


\subsubsection{Ablation study}

First, we want to assess the effectiveness of the sliding window procedure described in Sec.\ref{sec:testing} to account for more than 9 images for a given scene. Fig. \ref{fig:sliding} shows the mPSNR as function of the number of SR estimates used for computing the average. Notice that the mPSNR quickly saturates due to the lower quality of the images in the dataset (e.g., too many masked pixels). Nevertheless, averaging allow to achieve an mPSNR gain up to 0.3 dB over a single SR estimate on the NIR data and up to 0.2 dB on the RED  data. All the following results have been obtained with a sliding factor equal to 5. 

Then, we want to verify the effectiveness of the RegNet component of DeepSUM with respect to external registration of the images by means of cross correlation. This test should highlight the advantage of exploiting the feature space of the end-to-end trained network for the registration task. Hence, we compare two versions of our network: 
 
\begin{itemize}
    \item full network (SISRNet+RegNet+FusionNet);
    \item network without the RegNet block (SISRNet+FusionNet).
We keep the registration filters but they are fixed to be a delta centered at the integer shift determined by maximum cross correlation on the ILR input images.
\end{itemize}
The full network outperforms the one without RegNet by 0.16 dB and 0.13 dB for the NIR and RED test sets, respectively, as shown in Table \ref{table:noregnet}. 
This is a significant margin and it is due to the fact that an inaccurate registration can be an important source of error for the SR reconstruction.


On the other hand, the full network, being trainable end-to-end, is able to exploit the feature space produced by SISRNet to provide a more accurate registration and help FusionNet to perform the feature merging task.
We remark that the full network and the reduced network have been trained independently.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.43\textwidth]{fig4.pdf}
    \caption{Effect of testing sliding window to deal with more than 9 LR images.}
    \label{fig:sliding}
\end{figure}

\begin{table}[t]
\centering
\caption{Average mPSNR (dB) and SSIM - RegNet Performance}
\label{table:noregnet}
\begin{tabular}{lcc}
 & Proposed without RegNet & \textbf{Proposed with RegNet} \\ \hline
NIR & 47.68 / 0.98519 & \textbf{47.84 / 0.98578} \\ \hline
RED & 49.87 / 0.99038 & \textbf{50.00 / 0.99075} \\ \hline
\end{tabular}
\end{table}

\subsubsection{Comparison to State-of-the-Art}

\begin{table*}[t]
\centering
\caption{Average mPSNR (db) and SSIM}
\label{table:baselines}
\begin{tabular}{lcccccccc}
 & Bicubic & Bicubic+Mean & IBP \cite{IRANI1991231} & BTV \cite{1331445} & SISR & SISR+Mean & DUF \cite{Jo_2018_CVPR} & \textbf{DeepSUM} \\ \hline
NIR & 45.05/0.97654 & 45.69/0.97782 & 45.96/0.97960 & 45.93/0.97942 & 45.56/0.97938 & 46.41/0.98166 & 47.06/0.98417 & \textbf{47.84/0.98578} \\ \hline
RED & 47.61/0.98474 & 47.91/0.98507 & 48.21/0.98648 & 48.12/0.98606 & 48.20/0.98704 & 48.71/0.98787 & 49.36/0.98948 & \textbf{50.00/0.99075} \\ \hline
\end{tabular}
\end{table*}

We compare the proposed MISR technique to a number of alternatives based on deep learning and model-based methods:
\begin{enumerate}
    \item single image bicubic interpolation with least masked image (Bicubic);
    \item averaged bicubic interpolated and registered images (Bicubic+Mean);
    \item CNN-based SISR with least masked image;
    \item CNN-based SISR method shared across multiple images followed by registration and averaging (SISR+Mean);
    \item IBP \cite{IRANI1991231};
    \item BTV \cite{1331445};
    \item deep  learning  method  based  on  simultaneous motion compensation and interpolation developed for video (dynamic upsampling filters (DUF) network) \cite{Jo_2018_CVPR}.
\end{enumerate}
Table \ref{table:baselines} reports the results of the comparison. It can be noticed that the proposed method outperforms all the other methods.

For all these methods, we followed the same procedure for the data preparation: bicubic interpolation and registration by phase correlation algorithm, except for DUF that computes its own registration. For MISR methods we averaged the 5 SR estimates produced by the sliding window method to ensure a fair comparison with the proposed technique. 

Our IBP implementation takes as input an initial guess corresponding to our Bicubic+Mean baseline and the precomputed shifts related to the LR images using phase correlation algorithm. At each step, the LR images are estimated through the forward (HR to LR) imaging model and the error with respect to the actual LR images is back projected to the current SR image. We can observe that IBP improves over the Bicubic+Mean baseline but its performance is ultimately limited by its inability to deal with a complex and unknown degradation model.
BTV implementation takes the same initial guess and precomputed shifts as in IBP with the difference that at each iteration the cost function to minimize is a L1 norm plus the bilateral regularization term. 
BTV shows comparable performance with respect to IBP. BTV is slightly worse due to the L1 norm data fidelity that tends to be more robust to outliers but suboptimal with respect to the mPSNR metric.
The deep learning models show marked improvements over the Bicubic+Mean baseline. We consider two deep learning baselines (SISR only and SISR+Mean) that use the SISRNet architecture with the addition of a final layer projecting from the feature space to the image space, a residual connection from the (IRLR) bicubic image(s) and an increased number of parameters to roughly match the number of parameters of the full proposed architecture in order to ensure a fair comparison. The SISR+Mean result has been obtained by averaging 9 SISR images. Notice that SISR+Mean does not train the network by showing the averaged image to the loss function; it just uses the pretrained SISR network on multiple images and averages its outputs. The reason behind this choice is to provide a reference result to reader who might be interested in taking a state-of-the-art off-the-shelf SISR model, apply it to multiple images and then average the results. The comparison between SISR+Mean and the SISR only method is meant to highlight the large gain brought by exploiting both the spatial and temporal correlations, even if the LR images of a specific scene are taken under different conditions and might be wildly different from one another in terms of contrast, brightness and landscape due to temporal variations. Also, notice that SISR only is unable to improve over the simple Bicubic+Mean MISR on the NIR data. Instead, the comparison between DeepSUM and the SISR+Mean method shows the improvement brought by the introduction of FusionNet, which can exploit the slow fusion via 3D convolutions to find the best way to merge the image representations.

Another method chosen for comparison is the recent DUF network \cite{Jo_2018_CVPR}. This is one of the current state-of-the-art methods for video super-resolution. DUF network processes  frames in order to compute local pixel-dependent dynamic filters that are later applied on the central frame to increase its resolution and compensate motion. The network has a residual branch estimating a residual image to increase sharpness of the final SR image.
The DUF network has been trained from scratch, maintaining the original structure and roughly the same number of learnable parameters with respect to our method for fair comparison. The only difference lies in using the loss function stated in Sec. \ref{subsec:loss} instead of the one used in the original paper (Huber loss).
Moreover, we always considered the first one among the 9 input LR images as central frame.
The performance is worse than our proposed method and we can deduce that it highly depends on the LR input image taken to apply the dynamic local filters. We cannot know in advance which is the LR image that is closer to the HR image due to change in brightness, landscape, weather, and clouds. Involving all the LR images for HR estimation is crucial to somehow average the differences across them and try to include as much information as possible in the final SR estimate.

For completeness, we report the score achieved by DeepSUM on the unreleased test set of the PROBA-V challenge. DeepSUM achieved a score equal to 0.9474466476281652, computed as the average ratio between the mPSNR of ESA's baseline and the mPSNR of the submitted images, over both RED and NIR data in the held-out test set.  


\subsection{Qualitative results}
We present a set of qualitative comparisons on the RED and NIR images of our PROBA-V test set. 

First of all, Figs. \ref{fig:many_lr_nir} and \ref{fig:many_lr_red} show the multitemporal variability among the LR images and between the LR set and the HR target for the NIR and RED bands, respectively.

Figs. \ref{fig:zoom_images_nir} and \ref{fig:zoom_images_red} show a visual comparison between the SR images reconstructed by the various methods for the NIR and RED bands, respectively. It can be noticed that our proposed method produces visually more detailed images, recovering finer texture and sharper edges. In order to help visualization, Figs. \ref{fig:diff_nir} and \ref{fig:diff_red} report the absolute difference between the HR target and the SR reconstructions for the various methods after registration and compensation for absolute brightness variations (as in the mPSNR computation).


\begin{figure*}[t]
  \centering
    \begin{minipage}[b]{\textwidth}
        \begin{minipage}[c]{0.15\textwidth}
        \includegraphics[width=\textwidth]{fig5a.png}
        \end{minipage}
        \hfill
        \begin{minipage}[c]{0.15\textwidth}
        \includegraphics[width=\textwidth]{fig5b.png}
        \end{minipage}
        \hfill
        \begin{minipage}[c]{0.15\textwidth}
        \includegraphics[width=\textwidth]{fig5c.png}
        \end{minipage}
        \hfill
        \begin{minipage}[c]{0.15\textwidth}
        \includegraphics[width=\textwidth]{fig5d.png}
        \end{minipage}
        \hfill
        \begin{minipage}[c]{0.15\textwidth}
        \includegraphics[width=\textwidth]{fig5e.png}
        \end{minipage}
        \hfill
        \begin{minipage}[c]{0.15\textwidth}
        \includegraphics[width=\textwidth]{fig5f.png}
        \end{minipage}
        \hfill
        \begin{minipage}[c]{0.041\textwidth}
        \includegraphics[width=\textwidth]{fig5g.png}
        \end{minipage}
        
    \end{minipage}\\
\caption{NIR band images (imgset0708). Left to right: 4 LR images, SR image reconstructed by DeepSUM and HR image.}
  \label{fig:many_lr_nir}
\end{figure*}


\begin{figure*}[t]
  \centering
    \begin{minipage}[b]{\textwidth}
        \begin{minipage}[c]{0.23\textwidth}
        \includegraphics[width=\textwidth]{fig6a.png}
        \end{minipage}
        \hfill
        \begin{minipage}[c]{0.23\textwidth}
        \includegraphics[width=\textwidth]{fig6b.png}
        \end{minipage}
        \hfill
        \begin{minipage}[c]{0.23\textwidth}
        \includegraphics[width=\textwidth]{fig6c.png}
        \end{minipage}
        \hfill
        \begin{minipage}[c]{0.23\textwidth}
        \includegraphics[width=\textwidth]{fig6d.png}
        \end{minipage}
        \hfill
        \hspace{0.05\textwidth}
        
        \vspace{0.20cm}
        \begin{minipage}[c]{0.23\textwidth}
        \includegraphics[width=\textwidth]{fig6e.png}
        \end{minipage}
        \hfill
        \begin{minipage}[c]{0.23\textwidth}
        \includegraphics[width=\textwidth]{fig6f.png}
        \end{minipage}
        \hfill
        \begin{minipage}[c]{0.23\textwidth}
        \includegraphics[width=\textwidth]{fig6g.png}
        \end{minipage}
        \hfill
        \begin{minipage}[c]{0.23\textwidth}
        \includegraphics[width=\textwidth]{fig6h.png}
        \end{minipage}
        \hfill
        \begin{minipage}[c]{0.05\textwidth}
        \includegraphics[width=\textwidth]{fig6i.png}
        \end{minipage}
        
    \end{minipage}\\
    
\caption{NIR band images (imgset0792). Top-Left to bottom-right: one among the LR images, Bicubic+Mean (47.71 dB / 0.98736), IBP (48.46 dB / 0.98919), BTV(48.12 dB / 0.98866), DUF (48.93 dB / 0.99028), proposed method without RegNet (50.71 dB / 0.99303), DeepSUM (50.82 dB / 0.99331), HR image}
  \label{fig:zoom_images_nir}
\end{figure*}



\begin{figure*}[t]
  \centering
    \begin{minipage}[b]{\textwidth}
        \begin{minipage}[c]{0.15\textwidth}
        \includegraphics[width=\textwidth]{fig7a.png}
        \end{minipage}
        \hfill
        \begin{minipage}[c]{0.15\textwidth}
        \includegraphics[width=\textwidth]{fig7b.png}
        \end{minipage}
        \hfill
        \begin{minipage}[c]{0.15\textwidth}
        \includegraphics[width=\textwidth]{fig7c.png}
        \end{minipage}\vspace{0.20cm}
        \hfill
        \begin{minipage}[c]{0.15\textwidth}
        \includegraphics[width=\textwidth]{fig7d.png}
        \end{minipage}
        \hfill
        \begin{minipage}[c]{0.15\textwidth}
        \includegraphics[width=\textwidth]{fig7e.png}
        \end{minipage}
        \hfill
        \begin{minipage}[c]{0.15\textwidth}
        \includegraphics[width=\textwidth]{fig7f.png}
        \end{minipage}
        \hfill
        \begin{minipage}[c]{0.035\textwidth}
        \includegraphics[width=\textwidth]{fig7g.png}
        \end{minipage}
    \end{minipage}\\
\caption{Absolute difference between SR image and HR image (NIR band). Left to right: Bicubic+Mean, IBP, BTV, DUF, proposed method without RegNet, DeepSUM}
  \label{fig:diff_nir}
\end{figure*}


\begin{figure*}[t]
  \centering
    \begin{minipage}[b]{\textwidth}
        \begin{minipage}[c]{0.15\textwidth}
        \includegraphics[width=\textwidth]{fig8a.png}
        \end{minipage}
        \hfill
        \begin{minipage}[c]{0.15\textwidth}
        \includegraphics[width=\textwidth]{fig8b.png}
        \end{minipage}
        \hfill
        \begin{minipage}[c]{0.15\textwidth}
        \includegraphics[width=\textwidth]{fig8c.png}
        \end{minipage}
        \hfill
        \begin{minipage}[c]{0.15\textwidth}
        \includegraphics[width=\textwidth]{fig8d.png}
        \end{minipage}
        \hfill
        \begin{minipage}[c]{0.15\textwidth}
        \includegraphics[width=\textwidth]{fig8e.png}
        \end{minipage}
        \hfill
        \begin{minipage}[c]{0.15\textwidth}
        \includegraphics[width=\textwidth]{fig8f.png}
        \end{minipage}
        \hfill
        \begin{minipage}[c]{0.041\textwidth}
        \includegraphics[width=\textwidth]{fig8g.png}
        \end{minipage}
        
    \end{minipage}\\
\caption{RED band images (imgset0103). Left to right: 4 LR images, SR image reconstructed by DeepSUM and HR image.}
  \label{fig:many_lr_red}
\end{figure*}


\begin{figure*}[t]
  \centering
    \begin{minipage}[b]{\textwidth}
        \begin{minipage}[c]{0.23\textwidth}
        \includegraphics[width=\textwidth]{fig9a.png}
        \end{minipage}
        \hfill
        \begin{minipage}[c]{0.23\textwidth}
        \includegraphics[width=\textwidth]{fig9b.png}
        \end{minipage}
        \hfill
        \begin{minipage}[c]{0.23\textwidth}
        \includegraphics[width=\textwidth]{fig9c.png}
        \end{minipage}
        \hfill
        \begin{minipage}[c]{0.23\textwidth}
        \includegraphics[width=\textwidth]{fig9d.png}
        \end{minipage}
        \hfill
        \hspace{0.05\textwidth}
        
        \vspace{0.20cm}
        \begin{minipage}[c]{0.23\textwidth}
        \includegraphics[width=\textwidth]{fig9e.png}
        \end{minipage}
        \hfill
        \begin{minipage}[c]{0.23\textwidth}
        \includegraphics[width=\textwidth]{fig9f.png}
        \end{minipage}
        \hfill
        \begin{minipage}[c]{0.23\textwidth}
        \includegraphics[width=\textwidth]{fig9g.png}
        \end{minipage}
        \hfill
        \begin{minipage}[c]{0.23\textwidth}
        \includegraphics[width=\textwidth]{fig9h.png}
        \end{minipage}
        \hfill
        \begin{minipage}[c]{0.05\textwidth}
        \includegraphics[width=\textwidth]{fig9i.png}
        \end{minipage}
        
    \end{minipage}\\
\caption{RED band images (imgset0184). Top-Left to bottom-right: one among the LR images, Bicubic+Mean (46.32 dB / ), IBP (46.52 dB / 0.97965), BTV (46.53 dB / 0.97983), DUF (47.64 dB / 0.98468), proposed method without RegNet (49.55 dB / 0.98886), DeepSUM (49.89 dB / 0.99041), HR image.}
  \label{fig:zoom_images_red}
\end{figure*}


\begin{figure*}[t]
  \centering
    \begin{minipage}[b]{\textwidth}       
        \begin{minipage}[c]{0.15\textwidth}
        \includegraphics[width=\textwidth]{fig10a.png}
        \end{minipage}
        \hfill
        \begin{minipage}[c]{0.15\textwidth}
        \includegraphics[width=\textwidth]{fig10b.png}
        \end{minipage}
        \hfill
        \begin{minipage}[c]{0.15\textwidth}
        \includegraphics[width=\textwidth]{fig10c.png}
        \end{minipage}\vspace{0.20cm}
        \hfill
        \begin{minipage}[c]{0.15\textwidth}
        \includegraphics[width=\textwidth]{fig10d.png}
        \end{minipage}
        \hfill
        \begin{minipage}[c]{0.15\textwidth}
        \includegraphics[width=\textwidth]{fig10e.png}
        \end{minipage}
        \hfill
        \begin{minipage}[c]{0.15\textwidth}
        \includegraphics[width=\textwidth]{fig10f.png}
        \end{minipage}
        \hfill
        \begin{minipage}[c]{0.035\textwidth}
        \includegraphics[width=\textwidth]{fig10g.png}
        \end{minipage}
        
    \end{minipage}\\
\caption{Absolute difference between SR image and HR image (RED band). Left to right: Bicubic+Mean, IBP, BTV, DUF, proposed method without RegNet, DeepSUM.}
  \label{fig:diff_red}
\end{figure*}




\section{Conclusion}
\label{sec:conclusions}
In this paper we have introduced DeepSUM, one of the first CNN architectures to deal with super-resolution from multitemporal remote sensing images. We showed that the proposed deep learning framework can successfully deal with complex degradation and temporal variation models and provide state-of-the-art performance, resulting as the best method in the PROBA-V SR challenge.
Future work may focus on integrating non-local features in the network, e.g., by using graph-convolutional architectures \cite{ValsesiaICIP19}, a kind of convolution that draws from ideas in graph signal processing \cite{shuman2013emerging,Valsesia2019Sampling}.







\ifCLASSOPTIONcaptionsoff
  \newpage
\fi




\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}\typeout{** loaded for the language `#1'. Using the pattern for}\typeout{** the default language instead.}\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{Valsesia2014TGARS}
D.~{Valsesia} and E.~{Magli}, ``A novel rate control algorithm for onboard
  predictive coding of multispectral and hyperspectral images,'' \emph{IEEE
  Transactions on Geoscience and Remote Sensing (TGRS)}, vol.~52, no.~10, pp.
  6341--6355, Oct 2014.

\bibitem{Valsesia2016Universal}
D.~{Valsesia} and P.~T. {Boufounos}, ``Universal encoding of multispectral
  images,'' in \emph{IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, March 2016, pp. 4453--4457.

\bibitem{web:kelvins}
{Kelvins - ESA's Advanced Concepts }, ``{PROBA-V Super Resolution},''
  \url{https://kelvins.esa.int/proba-v-super-resolution}.

\bibitem{superdeep}
M.~Kawulok, P.~Benecki, S.~Piechaczek, K.~Hrynczenko, D.~Kostrzewa, and
  J.~Nalepa, ``Deep learning for multiple-image super-resolution,'' \emph{arXiv
  preprint arXiv:1903.00440}, 2019.

\bibitem{Bordone_Proba-V}
A.~Bordone~Molini, D.~Valsesia, G.~Fracastoro, and E.~Magli, ``Deep learning
  for super-resolution of unregistered multi-temporalsatellite images,'' in
  \emph{under review}, 2019.

\bibitem{ng2007total}
M.~K. Ng, H.~Shen, E.~Y. Lam, and L.~Zhang, ``A total variation regularization
  based super-resolution reconstruction algorithm for digital video,''
  \emph{EURASIP Journal on Advances in Signal Processing (JASP)}, vol. 2007,
  no.~1, p. 074585, 2007.

\bibitem{articleSun2010}
J.~Sun, Z.~Xu, and H.-Y. Shum, ``Gradient profile prior and its applications in
  image super-resolution and enhancement,'' \emph{IEEE Transactions on Image
  Processing (TIP)}, vol.~20, pp. 1529--42, 11 2010.

\bibitem{6414620}
L.~{Wang}, S.~{Xiang}, G.~{Meng}, H.~{Wu}, and C.~{Pan}, ``Edge-directed
  single-image super-resolution via adaptive gradient magnitude
  self-interpolation,'' \emph{IEEE Transactions on Circuits and Systems for
  Video Technology (TCSVT)}, vol.~23, no.~8, pp. 1289--1299, Aug 2013.

\bibitem{4694003}
M.~{Protter}, M.~{Elad}, H.~{Takeda}, and P.~{Milanfar}, ``Generalizing the
  nonlocal-means to super-resolution reconstruction,'' \emph{IEEE Transactions
  on Image Processing (TIP)}, vol.~18, no.~1, pp. 36--51, Jan 2009.

\bibitem{6241428}
K.~{Zhang}, X.~{Gao}, D.~{Tao}, and X.~{Li}, ``Single image super-resolution
  with non-local means and steering kernel regression,'' \emph{IEEE
  Transactions on Image Processing (TIP)}, vol.~21, no.~11, pp. 4544--4556, Nov
  2012.

\bibitem{Zhang2013SingleIS}
K.~Zhang, X.~Gao, D.~Tao, and X.~Li, ``Single image super-resolution with
  multiscale similarity learning,'' \emph{IEEE Transactions on Neural Networks
  and Learning Systems (TNNLS)}, vol.~24, pp. 1648--1659, 2013.

\bibitem{988747}
W.~T. {Freeman}, T.~R. {Jones}, and E.~C. {Pasztor}, ``Example-based
  super-resolution,'' \emph{IEEE Computer Graphics and Applications}, vol.~22,
  no.~2, pp. 56--65, March 2002.

\bibitem{1315043}
{Hong Chang}, {Dit-Yan Yeung}, and {Yimin Xiong}, ``Super-resolution through
  neighbor embedding,'' in \emph{Proceedings, IEEE Computer Society Conference
  on Computer Vision and Pattern Recognition (CVPR)}, vol.~1, June 2004, pp.
  I--I.

\bibitem{6166881}
X.~{Gao}, K.~{Zhang}, D.~{Tao}, and X.~{Li}, ``Image super-resolution with
  sparse neighbor embedding,'' \emph{IEEE Transactions on Image Processing
  (TIP)}, vol.~21, no.~7, pp. 3194--3205, July 2012.

\bibitem{Bevilacqua2012LowComplexitySS}
M.~Bevilacqua, A.~Roumy, C.~Guillemot, and M.-L. Alberi-Morel, ``Low-complexity
  single-image super-resolution based on nonnegative neighbor embedding,'' in
  \emph{British Machine Vision Conference (BMVC)}, 2012.

\bibitem{5466111}
J.~{Yang}, J.~{Wright}, T.~S. {Huang}, and Y.~{Ma}, ``Image super-resolution
  via sparse representation,'' \emph{IEEE Transactions on Image Processing
  (TIP)}, vol.~19, no.~11, pp. 2861--2873, Nov 2010.

\bibitem{6175956}
J.~{Yang}, Z.~{Wang}, Z.~{Lin}, S.~{Cohen}, and T.~{Huang}, ``Coupled
  dictionary training for image super-resolution,'' \emph{IEEE Transactions on
  Image Processing (TIP)}, vol.~21, no.~8, pp. 3467--3478, Aug 2012.

\bibitem{5396341}
K.~I. {Kim} and Y.~{Kwon}, ``Single-image super-resolution using sparse
  regression and natural image prior,'' \emph{IEEE Transactions on Pattern
  Analysis and Machine Intelligence (TPAMI)}, vol.~32, no.~6, pp. 1127--1133,
  June 2010.

\bibitem{6392274}
W.~{Dong}, L.~{Zhang}, G.~{Shi}, and X.~{Li}, ``Nonlocally centralized sparse
  representation for image restoration,'' \emph{IEEE Transactions on Image
  Processing (TIP)}, vol.~22, no.~4, pp. 1620--1630, April 2013.

\bibitem{10.1007/978-3-642-27413-8_47}
R.~Zeyde, M.~Elad, and M.~Protter, ``On single image scale-up using
  sparse-representations,'' in \emph{Curves and Surfaces}, J.-D. Boissonnat,
  P.~Chenin, A.~Cohen, C.~Gout, T.~Lyche, M.-L. Mazure, and L.~Schumaker,
  Eds.\hskip 1em plus 0.5em minus 0.4em\relax Berlin, Heidelberg: Springer
  Berlin Heidelberg, 2012, pp. 711--730.

\bibitem{6751349}
R.~{Timofte}, V.~{De}, and L.~V. {Gool}, ``Anchored neighborhood regression for
  fast example-based super-resolution,'' in \emph{IEEE International Conference
  on Computer Vision (ICCV)}, Dec 2013, pp. 1920--1927.

\bibitem{7299003}
S.~{Schulter}, C.~{Leistner}, and H.~{Bischof}, ``Fast and accurate image
  upscaling with super-resolution forests,'' in \emph{IEEE Conference on
  Computer Vision and Pattern Recognition (CVPR)}, June 2015, pp. 3791--3799.

\bibitem{DnCnnZhang}
K.~Zhang, W.~Zuo, Y.~Chen, D.~Meng, and L.~Zhang, ``Beyond a gaussian denoiser:
  Residual learning of deep cnn for image denoising,'' \emph{IEEE Transactions
  on Image Processing (TIP)}, vol.~PP, 08 2016.

\bibitem{liu2018non}
D.~Liu, B.~Wen, Y.~Fan, C.~C. Loy, and T.~S. Huang, ``Non-local recurrent
  network for image restoration,'' in \emph{Advances in Neural Information
  Processing Systems}, 2018, pp. 1673--1682.

\bibitem{10.1007/978-3-319-10593-2_13}
C.~Dong, C.~C. Loy, K.~He, and X.~Tang, ``Learning a deep convolutional network
  for image super-resolution,'' in \emph{European Conference on Computer Vision
  (ECCV)}, D.~Fleet, T.~Pajdla, B.~Schiele, and T.~Tuytelaars, Eds.\hskip 1em
  plus 0.5em minus 0.4em\relax Cham: Springer International Publishing, 2014,
  pp. 184--199.

\bibitem{kim2016deep}
J.~{Kim}, J.~K. {Lee}, and K.~M. {Lee}, ``Accurate image super-resolution using
  very deep convolutional networks,'' in \emph{IEEE Conference on Computer
  Vision and Pattern Recognition (CVPR)}, June 2016, pp. 1646--1654.

\bibitem{kim2015deep_rec}
------, ``Deeply-recursive convolutional network for image super-resolution,''
  in \emph{2016 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, June 2016, pp. 1637--1645.

\bibitem{Shi2016RealTimeSI}
W.~Shi, J.~Caballero, F.~Husz{\'a}r, J.~Totz, A.~P. Aitken, R.~Bishop,
  D.~Rueckert, and Z.~Wang, ``Real-time single image and video super-resolution
  using an efficient sub-pixel convolutional neural network,'' \emph{IEEE
  Conference on Computer Vision and Pattern Recognition (CVPR)}, pp.
  1874--1883, 2016.

\bibitem{Lim2017EnhancedDR}
B.~Lim, S.~Son, H.~Kim, S.~Nah, and K.~M. Lee, ``Enhanced deep residual
  networks for single image super-resolution,'' \emph{IEEE Conference on
  Computer Vision and Pattern Recognition (CVPR)}, pp. 1132--1140, 2017.

\bibitem{Zhang2018ResidualDN}
Y.~Zhang, Y.~Tian, Y.~Kong, B.~Zhong, and Y.~Fu, ``Residual dense network for
  image super-resolution,'' \emph{IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pp. 2472--2481, 2018.

\bibitem{8600724}
W.~{Ma}, Z.~{Pan}, J.~{Guo}, and B.~{Lei}, ``Achieving super-resolution remote
  sensing images via the wavelet transform combined with the recursive
  res-net,'' \emph{IEEE Transactions on Geoscience and Remote Sensing (TGRS)},
  vol.~57, no.~6, pp. 3512--3527, June 2019.

\bibitem{8677274}
K.~{Jiang}, Z.~{Wang}, P.~{Yi}, G.~{Wang}, T.~{Lu}, and J.~{Jiang},
  ``Edge-enhanced gan for remote sensing image superresolution,'' \emph{IEEE
  Transactions on Geoscience and Remote Sensing (TGRS)}, pp. 1--14, 2019.

\bibitem{tsaiHuang1984}
R.~Tsai and T.~Huang, ``Multiframe image restoration and registration,'' in
  \emph{Advances in Computer Vision and Image Processing}, vol.~I.\hskip 1em
  plus 0.5em minus 0.4em\relax JAI Press: Greenwich, CT, USA, 1984, pp.
  317--339.

\bibitem{935034}
M.~{Elad} and Y.~{Hel-Or}, ``A fast super-resolution reconstruction algorithm
  for pure translational motion and common space-invariant blur,'' \emph{IEEE
  Transactions on Image Processing (TIP)}, vol.~10, no.~8, pp. 1187--1193, Aug
  2001.

\bibitem{1176931}
S.~{Lertrattanapanich} and N.~K. {Bose}, ``High resolution image formation from
  low resolution frames using delaunay triangulation,'' \emph{IEEE Transactions
  on Image Processing (TIP)}, vol.~11, no.~12, pp. 1427--1441, Dec 2002.

\bibitem{IRANI1991231}
M.~Irani and S.~Peleg, ``Improving resolution by image registration,''
  \emph{Graphical Models and Image Processing (CVGIP)}, vol.~53, no.~3, pp. 231
  -- 239, 1991.

\bibitem{Stark:89}
H.~Stark and P.~Oskoui, ``High-resolution image recovery from image-plane
  arrays, using convex projections,'' \emph{Journal of the Optical Society of
  America}, vol.~6, no.~11, pp. 1715--1726, Nov 1989.

\bibitem{413332}
A.~J. {Patti}, M.~{Ibrahim Sezan}, and A.~{Murat Tekalp}, ``High-resolution
  image reconstruction from a low-resolution image sequence in the presence of
  time-varying motion blur,'' in \emph{Proceedings of 1st International
  Conference on Image Processing (ICIP)}, vol.~1, Nov 1994, pp. 343--347 vol.1.

\bibitem{1331445}
S.~{Farsiu}, M.~D. {Robinson}, M.~{Elad}, and P.~{Milanfar}, ``Fast and robust
  multiframe super resolution,'' \emph{IEEE Transactions on Image Processing
  (TIP)}, vol.~13, no.~10, pp. 1327--1344, Oct 2004.

\bibitem{4060955}
H.~{Takeda}, S.~{Farsiu}, and P.~{Milanfar}, ``Kernel regression for image
  processing and reconstruction,'' \emph{IEEE Transactions on Image Processing
  (TIP)}, vol.~16, no.~2, pp. 349--366, Feb 2007.

\bibitem{shen2009}
H.~Shen, M.~K.~Ng, P.~Li, and L.~Zhang, ``Super-resolution reconstruction
  algorithm to modis remote sensing images,'' \emph{The Computer Journal},
  vol.~52, pp. 90--100, 01 2009.

\bibitem{Kato:2017:DSM:3066426.3066466}
T.~Kato, H.~Hino, and N.~Murata, ``Double sparsity for multi-frame super
  resolution,'' \emph{Neurocomputing}, vol. 240, no.~C, pp. 115--126, May 2017.

\bibitem{KATO201564}
K.~Toshiyuki, H.~Hideitsu, and M.~Noboru, ``Multi-frame image super resolution
  based on sparse coding,'' \emph{Neural Networks}, vol.~66, pp. 64 -- 78,
  2015.

\bibitem{Hardie98}
R.~Hardie, K.~Barnard, J.~Bognar, E.~Armstrong, and E.~Watson,
  ``High-resolution image reconstruction from a sequence of rotated and
  translated frames and its application to an infrared imaging system,''
  \emph{Optical Engineering}, vol.~37, no.~1, pp. 247 -- 260 -- 14, 1998.

\bibitem{913592}
{Nhat Nguyen}, P.~{Milanfar}, and G.~{Golub}, ``A computationally efficient
  superresolution image reconstruction algorithm,'' \emph{IEEE Transactions on
  Image Processing (TIP)}, vol.~10, no.~4, pp. 573--583, April 2001.

\bibitem{6096366}
J.~{Chen}, J.~{Nunez-Yanez}, and A.~{Achim}, ``Video super-resolution using
  generalized gaussian markov random fields,'' \emph{IEEE Signal Processing
  Letters (SPL}, vol.~19, no.~2, pp. 63--66, Feb 2012.

\bibitem{661187}
T.~F. {Chan} and {Chiu-Kwong Wong}, ``Total variation blind deconvolution,''
  \emph{IEEE Transactions on Image Processing (TIP)}, vol.~7, no.~3, pp.
  370--375, March 1998.

\bibitem{Marquina2008}
A.~Marquina and S.~J. Osher, ``Image super-resolution by tv-regularization and
  bregman iteration,'' \emph{Journal of Scientific Computing}, vol.~37, no.~3,
  pp. 367--382, Dec 2008.

\bibitem{Zhang2014}
H.~Zhang, Z.~Yang, L.~Zhang, and H.~Shen, ``Super-resolution reconstruction for
  multi-angle remote sensing images considering resolution differences,''
  \emph{Remote Sensing}, vol.~6, 12 2013.

\bibitem{5308275}
F.~{Li}, X.~{Jia}, D.~{Fraser}, and A.~{Lambert}, ``Super resolution for remote
  sensing images based on a universal hidden markov tree model,'' \emph{IEEE
  Transactions on Geoscience and Remote Sensing (TGRS)}, vol.~48, no.~3, pp.
  1270--1278, March 2010.

\bibitem{6134690}
J.~{Ma}, J.~{Cheung-Wai Chan}, and F.~{Canters}, ``An operational
  superresolution approach for multi-temporal and multi-angle remotely sensed
  imagery,'' \emph{IEEE Journal of Selected Topics in Applied Earth
  Observations and Remote Sensing (J-STARS)}, vol.~5, no.~1, pp. 110--124, Feb
  2012.

\bibitem{941854}
N.~{Nguyen}, P.~{Milanfar}, and G.~{Golub}, ``Efficient generalized
  cross-validation with applications to parametric image restoration and
  resolution enhancement,'' \emph{IEEE Transactions on Image Processing (TIP)},
  vol.~10, no.~9, pp. 1299--1308, Sep. 2001.

\bibitem{He2005ARF}
H.~He and L.~P. Kondi, ``A regularization framework for joint blur estimation
  and super-resolution of video sequences,'' \emph{IEEE International
  Conference on Image Processing (ICIP)}, vol.~3, pp. III--329, 2005.

\bibitem{650116}
R.~C. {Hardie}, K.~J. {Barnard}, and E.~E. {Armstrong}, ``Joint map
  registration and high-resolution image estimation using a sequence of
  undersampled images,'' \emph{IEEE Transactions on Image Processing (TIP)},
  vol.~6, no.~12, pp. 1621--1633, Dec 1997.

\bibitem{Zhang2015}
H.~Zhang, L.~Zhang, and H.~Shen, ``A blind super-resolution reconstruction
  method considering image registration errors,'' \emph{International Journal
  of Fuzzy Systems (IJFS)}, vol.~17, no.~2, pp. 353--364, Jun 2015.

\bibitem{7444187}
A.~{Kappeler}, S.~{Yoo}, Q.~{Dai}, and A.~K. {Katsaggelos}, ``Video
  super-resolution with convolutional neural networks,'' \emph{IEEE
  Transactions on Computational Imaging (TCI)}, vol.~2, no.~2, pp. 109--122,
  June 2016.

\bibitem{DBLP:journals/corr/CaballeroLAATWS16}
J.~Caballero, C.~Ledig, A.~P. Aitken, A.~Acosta, J.~Totz, Z.~Wang, and W.~Shi,
  ``Real-time video super-resolution with spatio-temporal networks and motion
  compensation,'' \emph{CoRR}, vol. abs/1611.05250, 2016.

\bibitem{Jo_2018_CVPR}
Y.~Jo, S.~Wug~Oh, J.~Kang, and S.~Joo~Kim, ``Deep video super-resolution
  network using dynamic upsampling filters without explicit motion
  compensation,'' in \emph{The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, June 2018.

\bibitem{Tian2018TDANTD}
Y.~Tian, Y.~Zhang, Y.~Fu, and C.~Xu, ``Tdan: Temporally deformable alignment
  network for video super-resolution,'' \emph{CoRR}, vol. abs/1812.02898, 2018.

\bibitem{SuperDeep_MISR}
M.~Kawulok, P.~Benecki, K.~Hrynczenko, D.~Kostrzewa, S.~Piechaczek, J.~Nalepa,
  and B.~Smolka, ``Deep learning for fast super-resolution reconstruction from
  multiple images,'' vol. 10996, 2019.

\bibitem{2019arXiv190701821M}
M.~{M{\"a}rtens}, D.~{Izzo}, A.~{Krzic}, and D.~{Cox}, ``{Super-Resolution of
  PROBA-V Images Using Convolutional Neural Networks},'' \emph{arXiv preprint
  arXiv:1907.01821}, Jul 2019.

\bibitem{vedaldi2016instance}
D.~Ulyanov, A.~Vedaldi, and V.~Lempitsky, ``Instance normalization: The missing
  ingredient for fast stylization,'' \emph{arXiv preprint arXiv:1607.08022},
  2016.

\bibitem{ioffe2015batch}
S.~Ioffe and C.~Szegedy, ``Batch normalization: Accelerating deep network
  training by reducing internal covariate shift,'' \emph{arXiv preprint
  arXiv:1502.03167}, 2015.

\bibitem{Brabandere2016DynamicFN}
B.~D. Brabandere, X.~Jia, T.~Tuytelaars, and L.~V. Gool, ``Dynamic filter
  networks,'' in \emph{Proceedings, International Conference on Neural
  Information Processing Systems (NIPS)}, 2016.

\bibitem{ledig2017photo}
C.~Ledig, L.~Theis, F.~Husz{\'a}r, J.~Caballero, A.~Cunningham, A.~Acosta,
  A.~Aitken, A.~Tejani, J.~Totz \emph{et~al.}, ``Photo-realistic single image
  super-resolution using a generative adversarial network,'' in
  \emph{{Proceedings of the IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}}, 2017, pp. 4681--4690.

\bibitem{kingma2014adam}
D.~P. Kingma and J.~Ba, ``Adam: A method for stochastic optimization,''
  \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{ValsesiaICIP19}
D.~Valsesia, G.~Fracastoro, and E.~Magli, ``Image denoising with
  graph-convolutional neural networks,'' in \emph{IEEE International Conference
  on Image Processing (ICIP)}, 2019.

\bibitem{shuman2013emerging}
D.~Shuman, S.~Narang, P.~Frossard, A.~Ortega, and P.~Vandergheynst, ``The
  emerging field of signal processing on graphs: Extending high-dimensional
  data analysis to networks and other irregular domains,'' \emph{IEEE Signal
  Processing Magazine (SPM)}, vol.~3, no.~30, pp. 83--98, 2013.

\bibitem{Valsesia2019Sampling}
D.~{Valsesia}, G.~{Fracastoro}, and E.~{Magli}, ``Sampling of graph signals via
  randomized local aggregations,'' \emph{IEEE Transactions on Signal and
  Information Processing over Networks (TSIPN)}, vol.~5, no.~2, pp. 348--359,
  June 2019.

\end{thebibliography}



\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Bordone.jpg}}]{Andrea Bordone Molini} received the M.Sc. in computer networks engineering from Politecnico di Torino, Turin, Italy, and a M.Sc. in Communications Systems Security from T\'el\'ecom ParisTech, Paris, France as part of a double degree program in 2016. He joined the SmartData@Polito research center where he is pursuing the Ph.D. degree. His current research interests include deep learning applied to image processing in particular in the fields of super-resolution and denoising. \end{IEEEbiography}


\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Valsesia.jpg}}]{Diego Valsesia} (S'13-M'17) received the Ph.D. degree in Electronic and Communication Engineering from the Politecnico di Torino, Turin, Italy, in 2016 and the M.Sc. degrees in Telecommunications Engineering from Politecnico di Torino and Electrical and Computer Engineering from the University of Illinois at Chicago, Chicago, IL, in 2012 and 2013 respectively. He is currently an Assistant Professor at the Department of Electronics and Telecommunications (DET), Politecnico di Torino. His main research interests include compression of remote sensing images, compressed sensing, and deep learning.\end{IEEEbiography}


\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Fracastoro.jpg}}]{Giulia Fracastoro} (S'13-M'17) Giulia Fracastoro received the Ph.D. degree in Electronic and Telecommunications Engineering from Politecnico di Torino, Turin, Italy, in 2017. During 2016, she was a visiting student at the Signal Processing Laboratory at EPFL, working on graph learning for image compression. She is currently an Assistant Professor with the Department of Electronics and Telecommunications (DET), Politecnico di Torino. Her research interests include graph signal processing, image processing, and deep learning.\end{IEEEbiography}


\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Magli.jpg}}]{Enrico Magli} (S'97-M'01-SM'07-F'17) received the M.Sc. and Ph.D. degrees from the Politecnico di Torino, Torino, Italy, in 1997 and 2001, respectively. He is currently a Full Professor with Politecnico di Torino, Torino, Italy. His research interests include compressive sensing, image and video coding, and vision. He is an Associate Editor of the IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY and the EURASIP Journal on Image and Video Processing, and a former Associate Editor of the IEEE TRANSACTIONS ON MULTIMEDIA. He is a Fellow of the IEEE, and has been an IEEE Distinguished Lecturer from 2015 to 2016. He was the recipient of the IEEE Geoscience and Remote Sensing Society 2011 Transactions Prize Paper Award, the IEEE ICIP 2015 Best Student Paper Award (as senior author), and the 2010 and 2014 Best Associate Editor Award of the IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY.
\end{IEEEbiography}



\end{document}
