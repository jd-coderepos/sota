

\documentclass[runningheads]{eccv2020/llncs}
\usepackage{graphicx}
\usepackage{eccv2020/comment}
\usepackage{amsmath,amssymb} \usepackage{amsmath}
\usepackage{color}
\usepackage{booktabs}
\usepackage{array}

\usepackage[ruled,lined]{algorithm2e}





\newcommand{\fancyname}{SimAug}

\newcommand{\timeto}{\!:\!}
\newcommand{\context}{\mathcal{H}}
\newcommand{\real}{\mathbb{R}}
\DeclareMathOperator*{\argmax}{argmax}


\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}

\newcommand{\lu}[1]{{\color{red}{(Lu\@: #1)}}}
\newcommand{\junwei}[1]{{\color{blue}{(Junwei\@: #1)}}}
\newcommand{\kevin}[1]{{\color{green}{(Kevin\@: #1)}}}



\begin{document}
\pagestyle{headings}
\mainmatter
\def\ECCVSubNumber{1850}  

\title{\textit{\fancyname}: Learning Robust Representations from 3D Simulation for Pedestrian Trajectory Prediction in Unseen Cameras} 

\begin{comment}
\titlerunning{ECCV-20 submission ID \ECCVSubNumber} 
\authorrunning{ECCV-20 submission ID \ECCVSubNumber} 
\author{Anonymous ECCV submission}
\institute{Paper ID \ECCVSubNumber}
\end{comment}
\titlerunning{\textit{\fancyname}}
\author{Junwei Liang\inst{1} \and Lu Jiang\inst{2} \and
Alexander Hauptmann\inst{1}}
\authorrunning{J. Liang et al.}
\institute{Carnegie Mellon University \\
\email{\{junweil,alex\}@cs.cmu.edu}\\ \and
Google Research \\
\email{lujiang@google.com}}
\maketitle



\begin{figure}[ht]
	\vspace{-6mm}
	\centering
		\includegraphics[width=\hsize]{figures/intro.png}
	\caption{Illustration of pedestrian trajectory prediction in unseen cameras. We propose to learn robust representations only from 3D simulation data that could generalize to real-world videos captured by unseen cameras.}
	\label{fig:intro}
	\vspace{-6mm}
\end{figure}

\begin{abstract}
This paper focuses on the problem of predicting future trajectories of people in unseen scenarios and camera views.
We propose a method to efficiently utilize multi-view 3D simulation data for training.
Our approach finds the hardest camera view to mix up with adversarial data from the original camera view in training, thus enabling the model to learn robust representations that can generalize to unseen camera views.
We refer to our method as \textit{\fancyname}. 
We show that \textit{\fancyname} achieves best results on three out-of-domain real-world benchmarks, as well as getting state-of-the-art in the Stanford Drone and the VIRAT/ActEV dataset with in-domain training data.
We will release our models and code.

\keywords{Trajectory Prediction, 3D Simulation, Synthetic Data, Representation Learning, Adversarial Learning}
\end{abstract} 


\section{Introduction}
Future trajectory prediction~\cite{kitani2012activity,alahi2016social,gupta2018social,liang2019peeking,sadeghian2018sophie,lee2017desire,liang2020garden} is a fundamental problem in video analytics, which aims at forecasting a pedestrian's future path in video in the next few seconds. Recent advancements in future trajectory prediction have been promising in a variety of vision applications such as
self-driving vehicles~\cite{bansal2018chauffeurnet,chai2019multipath,chang2019argoverse}, safety monitoring~\cite{liang2019peeking}, robotic planning~\cite{rhinehart2017first,rhinehart2018r2p2}, etc.

A notable bottleneck for existing works is that the current model is closely coupled with the video cameras on which it is trained, and generalizes poorly on new cameras with novel views or scenes. 
For example, prior works have proposed various models to forecast a pedestrian's trajectories in video cameras of different types such as stationary outdoor cameras~\cite{oh2011large,liang2019focal,alahi2016social,gupta2018social,lerner2007crowds,luber2010people}, drone cameras~\cite{sadeghian2018sophie,deo2020trajectory,li2019way}, ground-level egocentric cameras~\cite{yagi2018future,rhinehart2017first,styles2019multiple}, or dash cameras~\cite{mangalam2019disentangling,styles2019forecasting,chang2019argoverse}. 
Models that rely on RGB pixel inputs are especially vulnerable to view changes~\cite{liang2019peeking,liang2020garden,sadeghian2018sophie}.
Previous works~\cite{alahi2016social,gupta2018social,sadeghian2018sophie} have tried to conduct leave-one-scene-out experiments on ETH/UCY~\cite{pellegrini2010improving,lerner2007crowds} dataset under similar camera views.
However, existing models are all separately trained and tested within one or two datasets, and there have been no attempts at successfully generalizing the model to other datasets of novel camera views.
This bottleneck significantly hinders the application whenever there is a new camera because it requires annotating new data to fine-tune the model. Since it takes time to record data in new cameras, such annotation procedure is not only expensive but also can considerably delay the deployment of the model.


An ideal model should be able to disentangle human behavioral dynamics from specific camera views, and produces robust future prediction despite variances in camera views. 
Motivated by this idea, in this work, we learn a robust model for future trajectory prediction that can generalize to unseen video cameras. Different from existing works, our goal is to train a model only on synthetic data but test the model, out of the box, on unseen real-world videos, without further re-training or fine-tuning. 
Following the success of learning from simulation~\cite{ruiz2018learning,de2017procedural,varol2019synthetic,zhang2019rsa,gaidon2016virtual,richter2016playing}, we utilize a 3D simulator CARLA~\cite{dosovitskiy2017carla} to reconstruct static scene and dynamic elements from the VIRAT/ActEV videos~\cite{oh2011large}, and generate multiple views and pixel-precise semantic segmentation labels for training. 
As illustrated in Figure~\ref{fig:intro},
examples of pedestrian trajectory are recorded in multiple camera views to facilitate robust training.
Meanwhile, scene semantic segmentation is used instead of RGB pixels to help alleviate the influence of different lighting conditions, different texture of the scene and subtle noises produced by camera sensors, etc.
Both designs help better model transfer to unseen real-world cameras.

As unseen real-world videos are not available during training in our problem, traditional approaches for domain adaptation~\cite{bousmalis2017unsupervised,ganin2016domain,tzeng2017adversarial} or learning using privileged information~\cite{lambert2018deep,vapnik2015learning,lopez2015unifying} may not be applicable.
To tackle this issue, we propose a new data augmentation method called \emph{SimAug} to augment the representation such that it is robust to the variances in semantic scenes and camera views. 
First, to deal with the gap between real and synthetic semantic scene, we represent each training trajectory by high-level scene semantic segmentation features, and defend our model from adversarial examples generated by white-box attack methods~\cite{goodfellow2014explaining}. 
Second, to overcome the changes in camera views, we generate multiple views for the same trajectory, and encourage the model to focus on the ``hardest'' view to which the model has learned. Following~\cite{jiang2017mentornet,lin2017focal}, the classification loss is adopted and the view with the highest loss is favored during training. 
Finally, the augmented trajectory is computed as a convex combination of the trajectories generated in previous steps. 
Our trajectory prediction model is built on a multi-scale representation in~\cite{liang2020garden} and the final model is trained to minimize the empirical vicinal risk over the distribution of augmented trajectories.
Our method is partially inspired by robust deep learning methods using adversarial training~\cite{madry2017towards,tramer2017ensemble,xie2019feature,cheng2019robust}, Mixup~\cite{zhang2017mixup}, and MentorNet~\cite{jiang2017mentornet}.


We empirically validate our model, which is trained only on simulation data, 
on three real-world benchmarks for future trajectory prediction: VIRAT/ActEV \cite{oh2011large,2018trecvidawad}, Stanford Drone~\cite{robicquet2016learning} and Argoverse~\cite{chang2019argoverse}. 
These three benchmarks represent three different camera views: 45-degree view, top-down view and dashboard camera view with ego-motions.
The results show our method performs favorably against baseline methods including standard augmentation, adversarial learning and imitation learning. 
Notably, our method achieves better results to the state-of-the-art on the VIRAT/ActEV and Stanford Drone benchmark. 
We will release our data, models and code. To summarize, our contribution is threefold:
\begin{itemize}
    \item We study a new setting of future trajectory prediction: the model is trained once and used on any unseen video camera.
    \item We propose a new and effective approach to augment the representation of trajectory prediction models using multi-view simulation data.
    \item Ours is the first work on future trajectory prediction to demonstrate the efficacy of training on 3D simulation data, and establish new state-of-the-art results on three public benchmarks.
\end{itemize}













 
\section{Related Work}
\noindent\textbf{Trajectory prediction.}
Recently there are a large body of work on predicting person future trajectories in a variety of scenarios.
Many works~\cite{alahi2016social,manh2018scene,xue2018ss,zhang2019sr,liang2019peeking,liang2020garden,sadeghian2018sophie} focused on modeling person motions in videos recorded with stationary cameras.
Datasets like VIRAT/ActEV~\cite{oh2011large}, ETH/UCY~\cite{lerner2007crowds,luber2010people} and Stanford Drone~\cite{robicquet2016learning} have been used for evaluating pedestrian trajectory prediction.
Social-LSTM~\cite{alahi2016social} added social pooling to model nearby pedestrian trajectory patterns. 
Social-GAN~\cite{gupta2018social} added adversarial network~\cite{goodfellow2014generative} on Social-LSTM to generate diverse future trajectories. 
Several works focused on learning the effects of the physical scene, e.g., people tend to walk on the sidewalk instead of grass. 
Kitani et al. in ~\cite{kitani2012activity} used Inverse Reinforcement Learning to forecast human trajectory. 
SoPhie~\cite{sadeghian2018sophie} combined deep neural network features from scene semantic segmentation model and generative adversarial network (GAN) using attention to model person trajectory.
Some recent works~\cite{kooij2014context,yagi2018future,ma2017forecasting,liang2019peeking} have attempted to predict person paths by utilizing individuals' visual features instead of considering them as points in the scene.
Recently Liang et al.~\cite{liang2020garden} proposed to use abstract scene semantic segmentation features for better generalization.
Many works~\cite{lee2017desire,sadeghian2018car,bansal2018chauffeurnet,hong2019rules,zhao2019multi,makansi2019overcoming,li2019way,rhinehart2018r2p2} have been proposed for top-down view videos for trajectory prediction.
Notably, the Stanford Drone Dataset (SDD)~\cite{robicquet2016learning} is used in many works~\cite{sadeghian2018sophie,deo2020trajectory,li2019way} for trajectory prediction with drone videos.
Other works have also looked into pedestrian prediction in dashcam videos~\cite{mangalam2019disentangling,styles2019forecasting,kooij2014context,lee2017desire} and first-person videos~\cite{yagi2018future,styles2019multiple}.
Many vehicle trajectory datasets~\cite{caesar2019nuscenes,chang2019argoverse,yu2018bdd100k} have been proposed as a result of self-driving's surging popularity. 

\noindent\textbf{Learning from 3D simulation data.}
As the increasing research focus in 3D computer vision~\cite{zhang2015fast,liang2017event,shah2018airsim,dosovitskiy2017carla,richter2016playing,ros2016synthia,heess2017emergence}, many research works~\cite{qiu2017unrealcv,gaidon2016virtual,de2017procedural,das2018embodied,wu2019revisiting,zhu2017target,sun2019stochastic,liang2020garden,sun2018multi,bak2018domain} have used 3D simulation for training and evaluating real-world tasks.
Many works~\cite{qiu2017unrealcv,gaidon2016virtual,de2017procedural} were proposed to use data generated from 3D simulation for video object detection, tracking and action recognition analysis. 
Sun et al. ~\cite{sun2019stochastic} proposed a forecasting model by using a gaming simulator.
AirSim ~\cite{shah2018airsim} and CARLA ~\cite{dosovitskiy2017carla} were proposed for robotic autonomous controls for drones and vehicles.
Zeng et al.~\cite{zeng2019adversarial} proposed to use 3D simulation for adversarial attacks. RSA~\cite{zhang2019rsa} used randomized simulation data for human action recognition.
The ForkingPaths dataset~\cite{liang2020garden} was proposed for evaluating multi-future trajectory prediction. Human annotators were asked to control agents in a 3D simulator to create a multi-future trajectory dataset. 
Different from the ForkingPaths dataset, we propose an efficient method to reconstruct trajectories in real-world videos and collecting large-scale multi-view data.






\noindent\textbf{Data augmentation and adversarial learning.}
Traditional domain adaptation approaches~\cite{bousmalis2017unsupervised,ganin2016domain,tzeng2017adversarial,kang2019contrastive}, including unsupervised domain adaptation, may not be applicable as our target domain is considered ``unseen'' during training. Methods for learning using privileged information (LUPI)~\cite{lambert2018deep,vapnik2015learning,lopez2015unifying,luo2018graph} is not applicable for a similar reason. Closest to ours is robust deep learning methods. Recent works focused on the following directions: (i) \textit{adversarial training}~\cite{goodfellow2014explaining,madry2017towards,xie2019feature,zeng2019adversarial} defends the adversarial attacks generated on-the-fly during training using gradient-based methods~\cite{madry2017towards,goodfellow2014explaining,tramer2017ensemble}; (ii) data augmentation methods to overcome unknown variances between training and text examples such as \textit{Mixup}~\cite{zhang2017mixup}, \textit{AutoAgument}~\cite{cubuk2019autoaugment}, etc.
Different from prior work, ours uses 3D simulation data as a new way of doing data augmentation. Also, our proposed augmentation method is carefully designed for future trajectory prediction. The key design choices are substantiated by our ablation studies.


 

\begin{figure}[ht]
	\centering
		\includegraphics[width=\textwidth]{figures/method.png}
	\vspace{-8mm}
	\caption{Overview of our method. We efficiently generate multi-view trajectory data as described in Section~\ref{sec:data}. At each training iteration, the hardest camera view of the same trajectory sample is selected into training along with the adversarial sample of the original view.}
	\label{fig:method}
	\vspace{-6mm}
\end{figure}

\begin{figure}[]
	\centering
		\includegraphics[width=\textwidth]{figures/data.png}
	\caption{Visualization of the multi-view 3D simulation data used in \textit{\fancyname} training. Data generation process is described in Section~\ref{sec:data}. We use 4 camera views from 4 scenes defined in~\cite{liang2020garden}. ``0400'' and ``0401'' scene have overlapping views. The views in the left column are the original views from VIRAT/ActEV dataset.}
	\label{fig:data}
\end{figure}

\section{Approach}
\label{sec:approach}
In this section, we describe our approach to learn robust models for future trajectory prediction, which we call \textit{\fancyname}. Our goal is to train a model only on simulation training data that can effectively predict the future trajectory in real-world test videos that are unseen during training.

\subsection{Problem Formulation}
We focus on predicting the locations of a single agent for multiple steps into the future. Given a sequence of historic video frames  of the past  steps and the past agent locations  in training, we learn a probabilistic model on simulation data to estimate  for  steps into the future. At test time, our model takes as input an agent's observable past  in real videos to predict the agent's future locations , where  is the location coordinates. As the test videos are unseen during training, the model is supposed to be invariant to the variances in semantic scenes, camera views, and camera motions. 



\subsection{Training Data Generation From Simulation}
\label{sec:data}
Our model is trained only on simulation data. To ensure high-level realism, the training trajectories are generated by CARLA~\cite{dosovitskiy2017carla}, an open source 3D simulator built on top of the state-of-the-art game engine \textit{Unreal Engine 4}. We use the trajectories from the Forking Paths dataset~\cite{liang2020garden} that are semi-manually recreated from the VIRAT/ActEV benchmark by projecting real-world annotations to the 3D simulation world. Note that it is not our intention to build an exact replica of the real-world scene, nor it is necessary to help train a model for real-world task as suggested in previous works~\cite{gaidon2016virtual,ros2016synthia,liang2020garden,zhang2019rsa}.

With CARLA, we record multiple views of the same trajectory of different camera angles and positions. For a trajectory  in original view, let  denote a set of additional views for the same trajectory.
In our experiments, we use four camera parameters pre-specified in~\cite{liang2020garden}, i.e. three 45-degree views and one top-down view. 
We use a total of 4 scenes. See Fig.~\ref{fig:data}. 
The ground-truth location varies under different camera views and  for .
Note that these camera positions and angles are defined in~\cite{liang2020garden} specifically for VIRAT/ActEV dataset.
The top-down view cameras in Stanford Drone dataset~\cite{robicquet2016learning} are still considered unseen to the model since the scenes and camera positions are quite different.


In simulation, we also collect ground truth scene semantic segmentation from  classes including sidewalk, road, vehicle, pedestrian, etc. At test time, we extract the semantic segmentation feature using a pre-trained model with same number of class labels per pixel. To be more specific, we use the Deeplab model \cite{chen2017deeplab} trained on the ADE20k \cite{zhou2017scene} dataset and keep its weights frozen. To bridge the gap between real and simulated video frames, we represent all trajectory  as a sequence of scene semantic segmentation features.



\subsection{Multi-view Simulation Augmentation (\textit{\fancyname})}
\label{sec:simaug}
In this subsection, we describe \textit{\fancyname} for learning robust models. Given a trajectory in its original view , we generate a set of additional views in  as described in the previous section, where  represents scene semantic features of view  at time .  is a sequence of ground-truth locations for the -th view.

We use the original view as an anchor to search for the ``hardest'' view that is most inconsistent with what the model has learned. Inspired by~\cite{jiang2017mentornet}, we use the classification loss as the criteria and compute:

where  is an -bounded random perturbation applied to the input features.   is the location classification loss which will be discussed in the next subsection.

Then for the original view, we generate an adversarial trajectory feature by the targeted-FGSM attack~\cite{kurakin2016adversarial}:

where  is the hyper-parameter to be chosen. 
The attack forces the adversarial trajectory to be predicted as the true future locations in the selected camera view.
In essence, the resulting adversarial feature is ``warped'' to the ``hardest'' camera view by a small perturbation.
By defending against such adversarial examples, our model learns representations that are robust against changes in camera views.  

Finally, we mix up the trajectory of the selected view and the adversarial trajectory by a convex combination function over their features and one-hot location labels.

where  and the  function projects  coordinates into an one-hot embedding over a predefined grid used in computing the classification loss. Following~\cite{zhang2017mixup},  is drawn from a Beta distribution  controlled by the hyper-parameter .


The detailed algorithm for training with one training step is listed in Algorithm~\ref{alg:simaug}.
To train robust models to various camera views and semantic scenes, we learn representations over augmented training examples to overcome (i) random feature perturbations (ii) targeted adversarial attack, and (iii) the ``hardest'' feature from other views.
By the mix-up step in Eq.~\eqref{eqn:mixup}, 
our model is trained to minimize empirical vicinal risk over a new distribution constituted by the generated augmented trajectories, which is proved to be useful in improving model robustness in CNN training~\cite{zhang2017mixup}.



\vspace{-8mm}
\begin{algorithm}
\SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\LinesNumbered
\Input{Mini-batch of trajectories; hyper-parameters  and }
\Output{Classification loss  computed over augmented trajectories}
\BlankLine
\For{each trajectory  in the mini-batch}{
Generate trajectories from additional views \;
    Compute the loss for each camera view using \;
    Select the view with the largest loss  by Eq.~\eqref{eqn:select_c} \;
    Generate an adversarial trajectory  by Eq.~\eqref{eqn:adv}\;
    Mix up (, ) and (, ) by Eq.~\eqref{eqn:mixup}\;
    Compute  over the augmented trajectory (, ) from Step 6\;
}
\Return  averaged  over the augmented trajectories
\caption{\small Multi-view Simulation Adversarial Augmentation (\textit{\fancyname})}
\label{alg:simaug}
\end{algorithm}
\vspace{-8mm}


\subsection{Trajectory Prediction Model}
\label{sec:multiverse}

We build our backbone on Multiverse~\cite{liang2020garden}, a state-of-the-art multi-future trajectory prediction model. We use \emph{\fancyname} to improve the robustness of Multiverse 
view-invariant representation. Our method is general and flexible to be used in other models.

\noindent\textbf{Input Features.}
The model is given the past locations, , and the scene, . 
Each ground truth location  is encoded by an one-hot vector  representing the nearest cell in a 2D grid of size .
In our experiment, we use a grid scale of .
Each video frame  is encoded to as semantic segmentation feature of size  where  is the total number of class labels.
As discussed in previous section, we use \textit{\fancyname} to generate augmented trajectories  as our input during training.



\noindent\textbf{History Encoder.} 
A convolutional RNN~\cite{xingjian2015convolutional,wang2019eidetic} is used to get the final spatial-temporal feature state , where  is the hidden size.
The context is represented as the last hidden state and the history video frames, .

\noindent\textbf{Location Decoder.}
After getting the context , a coarse location decoder is used to predict locations at the level of grid cells at each time-instant by:

where  is the convolutional recurrent neural network (ConvRNN) with graph attention proposed in ~\cite{liang2020garden} and  is the hidden state of the ConvRNN.
Then a fine location decoder is used to predict a continuous offset in , which specifies a ``delta''
from the center of each grid cell, to get a fine-grained location prediction by:

where  is a separate ConvRNN and  is its hidden state.
To compute the final prediction location, we use

where  is the index of the selected grid cell,  is the center of that cell,
and  is the predicted offset for that cell at time .

\noindent\textbf{Training.} 
We use \textit{\fancyname} (see Section~\ref{sec:simaug}) to generate  as labels for training.
For the coarse decoder, the cross-entropy loss is used:

For the fine decoder, we use the original ground truth label :

where 
 
is the delta between the ground true location and the center of the  grid cell. 
The final loss is then calculated using

where  controls the  regularization (weight decay),
and  is used
to balance the regression and classification losses.






%
 
\section{Experiments}
\label{sec:exp}

In this section, we evaluate various methods, including our \textit{\fancyname} method, on three public video benchmarks of real-world videos captured under different camera views: the VIRAT/ActEV~\cite{2018trecvidawad,oh2011large} dataset, the Stanford Drone dataset~\cite{robicquet2016learning}, and the autonomous driving dataset Argoverse~\cite{chang2019argoverse}.
We demonstrate the efficacy of our method for unseen cameras in Section~\ref{sec:exp_main} and how our method can also improve state-of-the-art when fine-tuned on the real training data in Section~\ref{sec:exp_sdd} and Section~\ref{sec:exp_actev}.

\subsection{Evaluation Metrics}
\label{sec:metrics}
Let  be the true future trajectory for the  test sample, and  be the corresponding  prediction sample, for .

\noindent i) \textit{Minimum Average Displacement Error Given K Predictions} (minADE\textsubscript{K}): for each true trajectory sample ,
we select the closest  predictions,
and then measure its average error:


\noindent ii) \textit{Minimum Final Displacement Error Given K Predictions} (minFDE\textsubscript{K}): similar to minADE\textsubscript{K}, but we only consider the predicted points and the ground truth point at the final prediction time instant:

\noindent iii) \textit{Grid Prediction Accuracy} (Grid\_Acc):
As our base model also predicts coarse grid locations as described in Section~\ref{sec:multiverse}, we also evaluate the accuracy between the predicted grid  and the ground truth grid .


\subsection{Main Results}
\label{sec:exp_main}
\noindent\textbf{Dataset \& Setups.} 
In the following experiments, we compare \textit{\fancyname} with classical data augmentation methods as well as recent adversarial learning methods to train robust representations.
All methods are trained using the same backbone on the same \textbf{simulation training data} described in Section~\ref{sec:data}, and tested on three public benchmarks. All real videos are not allowed to be used during training except in our finetuning experiments. For VIRAT/ActEV, we use the same test split as \cite{liang2019peeking,liang2020garden}. 
For SDD, we utilize the standard test split as \cite{sadeghian2018sophie,deo2020trajectory} and for Argoverse, we use the official validation set from the 3D tracking task as our test set. The videos from the ``ring\_front\_center'' camera are used.


These datasets have different levels of difficulties. VIRAT/ActEV is the easiest one as we have used its training trajectories projected in our simulation training data. SDD is more difficult as its camera positions and scenes are different from our training. Argoverse is the most challenging one with distinct scenes, camera views, and ego-motions.



Following the setting in previous works~\cite{liang2019peeking,alahi2016social,gupta2018social,alahi2016social,gupta2018social,sadeghian2018sophie,makansi2019overcoming,liang2020garden,deo2020trajectory}, 
the models observe 3.2 seconds (8 frames) of every pedestrian and predict the future 4.8 seconds (12 frames) of person trajectory. 
We use the pixel values for the trajectory coordinates as it is done in~\cite{yagi2018future,liang2019peeking,lee2017desire,chai2019multipath,li2019way,makansi2019overcoming,bansal2018chauffeurnet,hong2019rules,deo2020trajectory}. 

\noindent\textbf{Baseline methods.} 
We compare \textit{\fancyname} with the following baseline methods for learning robust representations. All methods are built on the base model and trained using the same simulation training data.
\textbf{\textit{Base Model}} is the trajectory prediction model proposed in~\cite{liang2020garden}.
\textbf{\textit{Standard Aug}} is the base model trained with standard data augmentation techniques including horizontal flipping and random input jittering. 
\textbf{\textit{Fast Gradient Sign Method (FGSM)}} is the base model trained with adversarial examples generated by the targeted-FGSM attack method~\cite{goodfellow2014explaining}. We use random labels for the targeted-FGSM attack. 
\textbf{\textit{Projected Gradient Descent (PGD)}} is learned with a recent iterative adversarial learning method~\cite{madry2017towards,xie2019feature}. The number of iteration is set to 10 and other hyper-parameters follow~\cite{xie2019feature}.


\noindent\textbf{Implementation Details.}
We follow the implementation in~\cite{liang2020garden} and use it as our base model.
To be more specific, we use  for the  distribution in Eq~\eqref{eqn:mixup} and we use  in Eq~\eqref{eqn:adv}.
We use a total of 4 camera views in training, including three 45-degree views and one top-down view. See Section~\ref{sec:data}.
All models are trained using Adadelta optimizer~\cite{zeiler2012adadelta} with an initial learning rate of 0.3 and a weight decay of 0.001.
Other hyper-parameters for the baselines are the same as the ones in~\cite{liang2020garden}.
We evaluate the top  future trajectory prediction of all models.


\noindent\textbf{Quantitative Results.}
Table~\ref{table:main} shows the evaluation results. 
As we see, our method performs favorably against other baseline methods across all three evaluation metrics and all three benchmarks. 
In particular, ``Standard Aug'' seems to be not generalizing well to unseen cameras.
FGSM improves significantly on the ``Grid\_Acc'' metric but fails to translate the improvement to final location predictions. 
\textit{\fancyname} is able to improve the model overall due to the effective use of multi-view data.
All other methods are unable to improve trajectory prediction on Argoverse, whose data characteristics include ego-motions and distinct dashboard-view cameras. 
The results substantiate the efficacy of \textit{\fancyname} for trajectory prediction in unseen cameras.

\noindent\textbf{Qualitative Analysis.}
We visualize outputs of our base model with and without \textit{\fancyname} in Fig.~\ref{fig:qualitative}. We show visualizations on all three datasets. 
In each image, the yellow trajectories are history trajectories and the green ones are ground truth future trajectories. Outputs of the base model without {\fancyname} are colored with blue heatmaps and the yellow-orange heatmaps are from the same model with {\fancyname}.
As we see, the base model with {\fancyname} augmentation yields more accurate trajectories with turnings (Fig.~\ref{fig:qualitative} 1a., 3a.) while without it the model sometimes predicts the wrong turns (Fig.~\ref{fig:qualitative} 1b., 1c., 1d., 2a., 3a., 3c., 3d.).
In addition, the length of {\fancyname} model predictions is more accurate (Fig.~\ref{fig:qualitative} 2b., 2c., 2d., 3b.).

\begin{table}[]
\vspace{-4mm}
\centering
\caption{Comparison to standard data augmentation method and recent adversarial learning methods on three datasets. We report Grid\_Acc()/minADE\textsubscript{1}()/minFDE\textsubscript{1}() metrics. All methods are built on the backbone model in~\cite{liang2020garden} and trained using the same multi-view simulation data described in Section~\ref{sec:data}.}
\setlength\tabcolsep{2mm}
\begin{tabular}{lccc}
\toprule
Method & VIRAT/ActEV & Stanford Drone  & Argoverse    \\ 
\midrule
Base Model~\cite{liang2020garden} & 44.2\%/26.2/49.7 & 31.4\%/21.9/42.8  &  26.6\%/69.1/183.9 \\ 
Standard Aug &   45.5\%/25.8/48.3 & 21.3\%/23.7/47.6  &  28.9\%/70.9/183.4 \\ 
PGD~\cite{madry2017towards,xie2019feature} &  47.5\%/25.1/48.4 & 28.5\%/21.0/42.2 & 25.9\%/72.8/184.0 \\
FGSM~\cite{goodfellow2014explaining} &  48.6\%/25.4/49.3 & 42.3\%/19.3/39.9 & 29.2\%/71.1/185.4\\
SimAug &  \textbf{51.1}\%/\textbf{21.7}/\textbf{42.2} &  \textbf{45.4\%}/\textbf{15.7}/\textbf{30.2} & \textbf{30.9\%}/\textbf{67.9}/\textbf{175.6}\\
\bottomrule
\end{tabular}
\label{table:main}
\vspace{-8mm}
\end{table} 
\begin{figure}[ht]
	\centering
		\includegraphics[width=\textwidth]{figures/qualitative_heatmap.png}
	\caption{Qualitative analysis. Trajectory prediction from different models are colored and overlaid in the same image. See text for details. }
	\label{fig:qualitative}
\end{figure}

\subsection{State-of-the-Art Comparison on Stanford Drone Dataset}
\label{sec:exp_sdd}
In this section, we compare our \textit{\fancyname} model with the state-of-the-art generative models, including Social-LSTM, Social-GAN, DESIRE, and SoPhie~\cite{alahi2016social,gupta2018social,sadeghian2018sophie,lee2017desire}. We also compare with imitation learning model, IDL~\cite{li2019way}, and inverse reinforcement learning model, P2T\textsubscript{IRL}~\cite{deo2020trajectory} for trajectory prediction on the Stanford Drone Dataset. Following previous works, we evaluate our method with minimal errors over 20 predictions.

\noindent\textbf{Results \& Analysis.} The results are shown in Table~\ref{table:sdd}, where \textit{\fancyname} is built on top of the \textit{Multiverse} model. 
As it shows, \textit{\fancyname} model trained only on out-domain simulation data (second to the last row) achieves comparable or even better performance than other state-of-the-art models that are trained on in-domain real videos. By further fine-tuning on the learned representations of \textit{\fancyname}, we achieve the state-of-the-art performance on the Stanford Drone Dataset. 
The promising results demonstrate the efficacy of \textit{\fancyname} for trajectory prediction in unseen cameras.

\begin{table}[]
\vspace{-4mm}
\centering
\caption{State-of-the-art comparison on the Stanford Drone Dataset (SDD). Numbers are minimal errors over 20 predictions and lower the better. Baseline numbers are taken from \cite{sadeghian2018sophie,deo2020trajectory}. ``SimAug'' is trained without using SDD training data and ``SimAug + finetune'' is further finetuned on SDD training data.}
\begin{tabular}{lcc}
\toprule
Method & minADE\textsubscript{20}()   & minFDE\textsubscript{20} ()      \\
\midrule
Social-LSTM~\cite{alahi2016social} & 31.19 & 56.97 \\
Social-GAN~\cite{gupta2018social} & 27.25   &  41.44 \\ 
DESIRE~\cite{lee2017desire} &  19.25 & 34.05 \\ 
SoPhie~\cite{sadeghian2018sophie} &  16.27 & 29.38 \\ 
Multiverse~\cite{liang2020garden} & 14.78 & 27.09 \\
IDL~\cite{li2019way} &  13.93 & 24.40 \\ 
P2T\textsubscript{IRL}~\cite{deo2020trajectory} &  12.58 & 22.07 \\ 
\midrule
SimAug & 12.03  & 23.98 \\
SimAug + finetune & \textbf{10.27}  & \textbf{19.71} \\
\bottomrule
\end{tabular}
\label{table:sdd}
\vspace{-8mm}
\end{table} 
\subsection{State-of-the-Art Comparison on VIRAT/ActEV}
\label{sec:exp_actev}
In this section, we compare our \textit{\fancyname} model with state-of-the-art models on VIRAT/ActEV.
Following the previous work~\cite{liang2020garden}, we evaluate our method with errors in the top 1 prediction. 
Experimental results are shown in Table~\ref{table:actev}, where all models in the top rows are trained on the real-world training videos in VIRAT/ActEV. Our model trained on simulation data achieves competitive performance and with fine-tuning we achieve the best performance on the VIRAT/ActEV benchmark.




\begin{table}[]

\centering
\caption{State-of-the-art comparison on the VIRAT/ActEV dataset. Numbers are minimal errors over 1 predictions and lower the better. }
\begin{tabular}{lcc}
\toprule
Method & minADE\textsubscript{1}()   & minFDE\textsubscript{1} ()      \\ 
\midrule
Social-LSTM~\cite{alahi2016social}  & 23.10 & 44.27 \\
Social-GAN~\cite{gupta2018social}  & 30.42   & 60.70  \\ 
Next~\cite{liang2019peeking}  & 19.78 & 42.43\\
Multiverse~\cite{liang2020garden} & 18.51 & 35.84 \\
\midrule
SimAug & 21.73  & 42.22 \\
SimAug + finetune  & \textbf{17.96}  & \textbf{34.68} \\
\bottomrule
\end{tabular}
\label{table:actev}
\vspace{-5mm}
\end{table} 

\subsection{Ablation Experiments}

We test various ablations of our approach to validate our design decisions.
Results are shown in Table~\ref{table:ablation}, where the top 1 prediction is used in evaluations.
We verify four key design choices by removing each, at a time, from the full model.

(1) \textit{Multi-view data:} Our method is trained on multi-view simulation data and we use 4 camera views in our experiments. We test our method without one of the camera view (top-down view) that is similar to the ones in SDD dataset to see the effects. 
As we see, the performance drops due to fewer number of data and less diverse views, suggesting that we should use more views in \textit{\fancyname} (which is effortless to do in 3D simulator).

(2) \textit{Random perturbation:} We test our model without random perturbation on the original view trajectory samples by setting  (Eq.~\eqref{eqn:select_c}).
As we see, performance drops on all three datasets and particularly on the more difficult Argoverse dataset.

(3) \textit{Adversarial attack:} We test our model without adversarial attack by replacing Eq.~\eqref{eqn:adv} with . The performance drops slightly across all three benchmarks.

(4) \textit{View selection:} We replace Eq.~\eqref{eqn:select_c} with random search to see the effect of view selection. As we see, the significant performance drops on trajectory prediction verify the effectiveness of our design.



\begin{table}[]
\vspace{-4mm}
\centering
\caption{Performance on ablated versions of our method on three benchmarks. We report Grid\_Acc()/minADE\textsubscript{1}()/minFDE\textsubscript{1}() metrics.} 
\begin{tabular}{l C{28mm}C{28mm}C{28mm}}
\toprule
Method & VIRAT/ActEV   & Stanford Drone & Argoverse       \\
\midrule
SimAug full model&  51.1\%/21.7/42.2 &  45.4\%/15.7/30.2 & 30.9\%/67.9/175.6\\
\midrule
- top-down view data &  51.0\%/22.8/43.6 & 44.9\%/18.4/35.6 &  30.9\%/68.4/178.3\\ 
- random perturbation &  49.1\%/23.6/43.8 & 44.2\%/18.7/35.6  & 32.6\%/69.1/180.2 \\ 
- adversarial attack &  49.2\%/23.1/43.8 & 43.1\%/17.4/32.9 & 26.4\%/68.0/177.5 \\ 
- view selection &  52.0\%/23.0/42.9 & 47.4\%/19.6/38.2 & 32.4\%/68.6/177.0\\ 
\bottomrule
\end{tabular}
\label{table:ablation}
\vspace{-8mm}
\end{table} 
 

\section{Conclusion}
\label{sec:concl}
In this paper,
we have introduced \textit{\fancyname}, which utilizes multi-view 3D simulation data to learn robust representations for trajectory prediction.
We have shown that our method achieves competitive performance on three public benchmarks with and without using the real-world training data.
We believe our approach will facilitate future research and applications on robust future prediction using 3D simulation.

 
\clearpage
\bibliographystyle{eccv2020/splncs04}
\bibliography{reference}
\end{document}
