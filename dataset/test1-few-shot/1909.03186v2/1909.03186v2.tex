\def\year{2020}\relax
\documentclass[letterpaper]{article} \usepackage{arXiv_V2_aaai_sty_mods}  \usepackage{times}  \usepackage{helvet} \usepackage{courier}  \usepackage[hyphens]{url}  \usepackage{graphicx} \usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{multirow}
\usepackage{latexsym}
\usepackage{svg}
\usepackage{wrapfig}
\usepackage{rotating}

\usepackage{array}

\usepackage{amsmath}
\usepackage{float}
\usepackage{lipsum}     \usepackage{xargs}      \urlstyle{rm} \def\UrlFont{\rm}  \usepackage{graphicx}  \frenchspacing  \setlength{\pdfpagewidth}{8.5in}  \setlength{\pdfpageheight}{11in}  \pdfinfo{
/Title (AAAI Press Formatting Instructions for Authors Using LaTeX -- A Guide)
/Author (AAAI Press Staff, Pater Patel Schneider, Sunil Issar, J. Scott Penberthy, George Ferguson, Hans Guesgen)
} 



\setcounter{secnumdepth}{0} 

\setlength\titlebox{2.5in} \title{On Extractive and Abstractive Neural Document Summarization \\ with Transformer Language Models}
\author{Sandeep Subramanian\thanks{Equal contribution, order determined by coin flip} , Raymond Li, Jonathan Pilault, Christopher Pal \\
Element AI, Montréal Institute for Learning Algorithms, Université de Montréal, \\École Polytechnique de Montréal, Canada CIFAR AI Chair \\
\texttt{\{jonathan.pilault\}@elementai.com}} \begin{document}

\maketitle

\begin{abstract}
We present a method to produce abstractive summaries of long documents that exceed several thousand words via neural abstractive summarization. We perform a simple extractive step before generating a summary, which is then used to condition the transformer language model on relevant information before being tasked with generating a summary. We show that this extractive step significantly improves summarization results. We also show that this approach produces more abstractive summaries compared to prior work that employs a copy mechanism  while still achieving higher rouge scores.
\textit{Note: The abstract above was not written by the authors, it was generated by one of the models presented in this paper based on an earlier draft of this paper.}
\end{abstract}

\section{Introduction}
\label{sec:intro}
Language models (LMs) are trained to estimate the joint probability of an arbitrary sequence of words or characters using a large corpus of text. They typically factorize the joint distribution of tokens  into a product of conditional probabilities . It is possible to use n-gram based models to estimate these conditional probabilities via counts, relying on Markovian assumptions. However, Markovian assumptions and the curse of dimensionality make it harder for n-gram LMs to model long range dependencies and learn smooth functions that can learn similarities between words in the vocabulary. This has led to a preference for recurrent or feed-forward neural language models \cite{bengio2003neural,mikolov2010recurrent} in recent years due to to their ability to learn expressive conditional probability distributions \cite{radford2019language}.

\begin{figure}[htb]
    \center{\includegraphics[scale=0.35] {images/Pipeline_Fig.pdf}}
    \caption{\label{fig:model} \small Proposed model for abstractive summarization of a scientific article. An older version of this paper is shown as the reference document. First, a sentence pointer network extracts important sentences from the paper. Next, these sentences are provided along with the whole scientific article to be arranged in the following order: Introduction, extracted Sentences, abstract \& the rest of the paper. A transformer language model is trained on articles organized in this format. During inference, the introduction and the extracted sentences are given to the language model as context to generate a summary. In domains like news and patent documents, the introduction is replaced by the entire document. }
\end{figure}

The sequence-to-sequence (seq2seq) paradigm \cite{sutskever2014sequence} uses language models that learn the conditional probability of one sequence given another. Here, a language model serves as a ``decoder'' that is typically conditioned on a representation of an input sequence produced by an encoder neural network. These types of encoder-decoder architectures have been particularly successful when applied to problems such as machine translation \cite{bahdanau2014neural} and abstractive summarization \cite{Rush2015AbsAttn}. The encoder and conditional decoder language models are often parameterized as recurrent neural networks (RNNs). Attention mechanisms \cite{bahdanau2014neural} are used in the decoder to provide more informative conditioning on the representations produced by the encoder and to ease gradient flow into the encoder. RNNs however, are limited by their sequential nature, making them 1) difficult to optimize and learn for long sequences with long range dependencies \cite{hochreiter1998vanishing,pascanu2013difficulty}, and 2) hard to parallelize on modern hardware like GPUs, limiting their scalability.



There has therefore been a recent shift towards feedforward architectures for sequential data, such as convolutional models \cite{kalchbrenner2016neural,van2016wavenet,gehring2017convolutional} or fully attentive models popularized by architectures known as transformers \cite{DBLP:journals/corr/VaswaniSPUJGKP17}. These techniques have a logarithmic or constant path length (as opposed to linear in RNNs) between a network's output and any of its inputs, making gradient flow much easier, thereby opening up the possibility of learning very long term dependencies.

The abstractive summarization of news or scientific papers typically requires encoding and generating hundreds or thousands of words. Recent work by \cite{radford2019language} (GPT-2) has demonstrated that transformers with a large receptive field and trained on a lot of data yield language models that are capable of capturing long range dependencies.

If one is interested in creating a coherent, high-quality summary of long documents, such GPT-like architectures possess many desirable properties. Their results also show that unconditional language models can implicitly learn to perform summarization or machine translation as a consequence of the data on which it is trained. If the data is formatted sequentially into different aspects of a document (introduction, body, summary), each divided by ``tl;dr'', the model can be coaxed to generate one of these aspects. For example, the model can be made to solve a summarization task by presenting it similarly formatted data at test time; i.e. a document's introduction and body followed by ``tl;dr`` that will generate an abstract from a language model conditioned on this context.

In this work, we take this idea a step further by doing away with the sequence-to-sequence paradigm and formatting data for abstractive summarization in a manner that transformer language models can make use of all of the available data present in the documents and their summaries (akin to language model pre-training on mono-lingual data in machine translation \cite{gulcehre2015using}). Specifically, we use a single GPT-like Transformer LM (TLM) trained on documents followed by their summaries. During inference, we generate from the LM, conditioned on the document (see figure \ref{fig:model}). Unlike most previous approaches to neural abstractive summarization, we do not use a seq2seq formulation with an explicit encoder and decoder for word generation. We split the task in two: an extractive step and an abstractive step \cite{chen2018fast,gehrmann2018bottom}. To deal with extremely long documents that exceed several thousand words, we first perform sentence extraction using two different hierarchical document models - one based on pointer networks \cite{vinyals2015pointer}, similar to the variant proposed in \cite{chen2018fast} and the other based on a sentence classifier \cite{nallapati2017summarunner}. This extracts important sentences from the document (described in section \ref{sec:extractive_model}) that can be used to better condition the transformer LM on relevant information before being tasked with generating a summary. We show that this extractive step significantly improves summarization results.

The contributions of this work are two fold:

\begin{itemize}
    \item We demonstrate that transformer language models are surprisingly effective at summarizing long scientific articles and outperform typical seq2seq approaches, even without a copy mechanism.
    \item We show that our approach produces more ``abstractive'' summaries compared to prior work that employs a copy mechanism \cite{asee2017pointer} while still achieving higher ROUGE scores.
\end{itemize}

\section{Related Work}
Automatic summarization systems seek to condense the size of a piece of text while preserving most of its important information content and meaning. The earliest attempts at automatic summarization focused on extractive techniques, which find words or sentences in a document that capture its most salient content.
In the past, various similarity scores based on specific sentence features (keywords, position, length, frequency, linguistic) and metrics (structure-based, vector-based and graph-based) were employed to estimate salience \cite{steinberger2004using,erkan2004lexrank} between a sentence in a document and its reference summary. More recently, with advances in distributed representations of words, phrases and sentences, researchers have proposed to use these to compute similarity scores. Such techniques were further refined by \cite{nallapati2016classify,cheng2016neural,chen2018fast} with encoder-decoder architectures - the representations learned by the encoder are used to choose the most salient sentences.
\cite{cheng2016neural} and \cite{nallapati2016classify} trained encoder-decoder neural networks as a binary classifier to determine if each sentence in a document should belong to the extractive summary or not. \cite{nallapati2016abstractive} also present an alternative that can pick an unordered set of sentences from the source document to assemble an extractive summary. \cite{chen2018fast} use a pointer network \cite{vinyals2015pointer} to sequentially pick sentences from the document that comprise its extractive summary.

Human summarizers have four common characteristics. They are able to (1) interpret a source document, (2) prioritize the most important parts of the input text, (3) paraphrase key concepts into coherent paragraphs and (4) generate diverse output summaries. While extractive methods are arguably well suited for identifying the most relevant information, such techniques may lack the fluency and coherency of human generated summaries. Abstractive summarization has shown the most promise towards addressing points (3) and (4) above. Abstractive generation may produce sentences not seen in the original input document. Motivated by neural network success in machine translation experiments, the attention-based encoder decoder paradigm has recently been widely studied in abstractive summarization \cite{Rush2015AbsAttn,nallapati2016abstractive,chopra2016abstractive}. By dynamically accessing the relevant pieces of information based on the hidden states of the decoder during generation of the output sequence, the model revisits the input and attends to important information. The advantages of extractive, abstractive and attention-based models were first combined in \cite{gu2016copy} with a copy mechanism for out-of-vocabulary words present in the source document. Similarly, \cite{asee2017pointer} used the attention scores to calculate the probability of generating vs copying a word. A coverage mechanism was also added to penalize the attention score of previously attended words, diminishing the model's tendency to repeat itself.

\section{Framework}
Our model comprises two distinct and independently trainable components 1) a hierarchical document representation model that either points to or classifies sentences in a document to build an extractive summary 2) a transformer language model that conditions on the extracted sentences as well as a part of or the entire document.

\subsection{Extractive Models}
\label{sec:extractive_model}
We describe the two neural extractive models used in this work in this section. \\ \\
\textbf{Hierarchical Seq2seq Sentence Pointer}
Our extractive model is similar to the sentence pointer architecture developed by \cite{chen2018fast} with the main difference being the choice of encoder. We use a hierarchical bidirectional LSTM encoder with word and sentence level LSTMs while \cite{chen2018fast} use a convolutional word level encoder for faster training and inference. The decoder is in both cases is an LSTM.

The extractive model considers the document as a list of  sentences , and each sentence as a list of tokens. We are given a ground-truth extracted summary of  sentences , where the  are the indices of the extracted sentences.
The procedure to determine ground-truth extraction targets are identical to previous work - finding two sentences in the document that have the highest ROUGE score with each sentence in the summary.

We use an encoder-decoder architecture for this extractor.
The encoder has a hierarchical structure that combines a token and sentence-level RNN. First, the ``sentence-encoder'' or token-level RNN is a bi-directional LSTM \cite{Hochreiter:1997:LSM:1246443.1246450} encoding each sentence. The last hidden state of the last layer from the two directions produces sentence embeddings: , where  is the number of sentences in the document. The sentence-level LSTM or the ``document encoder'', another bi-directional LSTM, encodes this sequence of sentence embeddings to produce document representations: .

The decoder is an autoregressive LSTM taking the sentence-level LSTM hidden state of the previously extracted sentence as input and predicting the next extracted sentence.
Let  the index of the previous extracted sentence at time step . The input to the decoder is , or a zero vector at time-step .
The decoder's output is computed by an attention mechanism from the decoder's hidden state  over the document representations .
We used the dot product attention method from \cite{luong2015effective}.
The attention weights  produce a context vector , which is then used to compute an attention aware hidden state .
Following the input-feeding approach from \cite{luong2015effective}, the attention aware hidden state  is concatenated to the input in the next time step, giving the following recurrence
, with


The attention weights  are used as output probability distribution over the document sentences, of the choice for the next extracted sentence.
We choose the convention to signal the end of the extraction by putting the same index twice in a row.
Thus, the input to the decoder is the following sequence:

, and the target:

, where  is the length of the ground-truth extracted summary and both sequences have  elements.
The model is trained to minimize the cross-entropy of picking the correct sentence at each decoder time step. At inference, we use beam-search to generate the extracted summary. 

\paragraph{Sentence Classifier}
As with the pointer network, we use a hierarchical LSTM to encode the document and produce a sequence of sentence representations  where  is the number of sentences in the document.
We compute a final document representation as follows:

where  and  are learnable parameters.
Finally, the probability of each sentence belonging to the extractive summary is given by:

where  is the sigmoid activation function. The model is trained to minimize the binary cross-entropy loss with respect to the sentences in the gold-extracted summary.

\paragraph{Model Details} The model uses word embeddings of size . The token-level LSTM (sentence encoder), sentence-level LSTM (document encoder) and decoder each have  layers of  units and a dropout of  is applied at the output of each intermediate layer. We trained it with Adam, a learning rate , a weight decay of , and using batch sizes of . We evaluate the model every  updates, using a patience of . At inference, we decode using beam search with a beam size of  for the pointer model and pick the  most likely sentences from the sentence classifier, where  is the average number of sentences in the summary across the training dataset.

\subsection{Transformer Language Models (TLM)}
\label{sec:gpt}
Instead of formulating abstractive summarization as a seq2seq problem using an encoder-decoder architecture, we only use a single transformer language model that is trained \textit{from scratch}, with appropriately ``formatted'' data (see figure \ref{fig:model}, we also describe the formatting later in this section).

We use a transformer \cite{DBLP:journals/corr/VaswaniSPUJGKP17} language model (TLM) architecture identical to \cite{radford2019language}. Our model has 220M parameters with 20 layers, 768 dimensional embeddings, 3072 dimensional position-wise MLPs and 12 attention heads. The only difference in our architectures (to our knowledge) is that we do not scale weights at initialization. We trained the language model for 5 days on 16 V100 GPUs on a single Nvidia DGX-2 box. We used a linear ramp-up learning rate schedule for the first  updates, to maximum learning rate of  followed by a cosine annealing schedule to 0 over the next  steps with the Adam optimizer. We used mixed-precision training \cite{micikevicius2017mixed} with a batch size of 256 sequences of 1024 tokens each.

In order to get an unconditional language model to do abstractive summarization, we can use the fact that LMs are trained by factorizing the joint distribution over words autoregressively. We organized the training data for the LM such that the ground-truth summary \textit{follows} the information used by the model to generate a system summary. This way, we model the joint distribution of document and summary during training, and sample from the conditional distribution of summary given document at inference. 

When dealing with extremely long documents that may not fit into a single window of tokens seen by a transformer language model, such as an entire scientific article, we use its introduction as a proxy for having enough information to generate an abstract (summary) and use the remainder of the paper as in domain language model training data (Fig \ref{fig:model}). In such cases, we organize the arXiv and PubMed datasets as follows: 1) paper introduction 2) extracted sentences from the sentence pointer model 3) abstract 4) rest of the paper. On other datasets, the paper introduction would be the entire document and there would no rest of the paper. This ensures that at inference, we can provide the language model the paper introduction and the extracted sentences as conditioning to generate its abstract. We found that using the ground truth extracted sentences during training and the model extracted sentences at inference performed better than using the model extracted sentences everywhere.

We use a special token to indicate the start of the summary and use it at test time to signal to the model to start generating the summary. The rest of the article is provided as additional in-domain training data for the LM.
The entire dataset is segmented into non-overlapping examples of  tokens each. We use ``topk'' sampling at inference \cite{fan2018hierarchical,radford2019language}, with  and a softmax temperature of 0.7 to generate summaries.
\begin{figure}[htb]
    \center{\includegraphics[scale=0.43]
    {images/square_fig.png}}
    \caption{\label{fig:abstractiveness} -gram overlaps between the abstracts generated by different models and the input article on the arXiv dataset. We show in detail which part of the input was copied for our TLM conditioned on intro + extract.}
\end{figure}

\section{Results and Analysis}
\subsection{Experimental setup}
\label{sec:experimental_setup}
\paragraph{Datasets}
We experiment with four different large-scale and long document summarization datasets - arXiv, PubMed \cite{discourse/corr/abs-1804-05685}, bigPatent \cite{sharma2019bigpatent} and Newsroom \cite{grusky2018newsroom}. Statistics are reported in Table \ref{tab:dataset}.

\begin{table}[ht]
\centering
\caption{Statistics from \cite{sharma2019bigpatent} for the datasets used in this work - The number of document/summary pairs, the ratio of the number of words in the document to the abstract and the number of words in the summary and document.}
\label{tab:dataset}
\small
\begin{tabular}{|l|c|c|c|c|}
	\hline 
		Dataset & \#Documents & \shortstack{Comp \\ Ratio} & \shortstack{Sum \\ Len} & \shortstack{Doc \\ Len}  \\
		\hline
        arXiv & 215,913 & 39.8 & 292.8 & 6,913.8 \\ 
        PubMed & 133,215 & 16.2 & 214.4 & 3,224.4 \\
        Newsroom & 1,212,726 & 43.0 & 30.4 & 750.9 \\
        BigPatent & 1,341,362 & 36.4 & 116.5 & 3,572.8 \\
    \hline
\end{tabular}
\end{table}

\paragraph{Data preprocessing} Both our extractive and abstractive models use sub-word units computed using \emph{byte pair encoding} \cite{sennrich2015neural} with  replacements. To address memory issues in the sentence pointer network, we only keep  sentences per article, and  tokens per sentence.

\paragraph{Evaluation} We evaluate our method using full-length F-1 ROUGE scores \cite{rouge} and re-used the code from \cite{discourse/corr/abs-1804-05685} for this purpose. All ROUGE numbers reported in this work have a 95\% confidence interval of at most 0.24.

\paragraph{Comparison} We compare our results to several previously proposed extractive and abstractive models. All prior results reported on the arXiv and Pubmed benchmark are obtained from \cite{discourse/corr/abs-1804-05685}. Similarly, prior results for the BigPatent dataset are obtained from \cite{sharma2019bigpatent} and Newsroom from \cite{grusky2018newsroom} and \cite{mendes2019jointly}. These methods include \emph{LexRank} \cite{erkan2004lexrank}, \emph{SumBasic} \cite{vanderwende2007beyond}, \emph{LSA} \cite{steinberger2004using}, \emph{Attention-Seq2Seq} \cite{nallapati2016abstractive,chopra2016abstractive}, \emph{Pointer-Generator Seq2Seq} \cite{asee2017pointer},
\emph{Discourse aware}, which is a hierarchical extension to the pointer generator model, \cite{discourse/corr/abs-1804-05685}, \emph{Sent-rewriting} \cite{chen2018fast}, \emph{RNN-Ext} \cite{chen2018fast}, \emph{Exconsumm} \cite{mendes2019jointly}.

\subsection{Discussion}
We present our main results on summarizing arXiv and PubMed papers in tables \ref{tab:arxiv_abstract}, \ref{tab:pubmed_abstract}. Our extractive models are able to outperform previous extractive baselines on both the arXiv and Pubmed datasets.
Our TLM conditioned on the extractive summary produced by our best extractive model (TLM-I+E (G,M)) outperforms prior abstractive/mixed results on the arXiv, Pubmed and bigPatent datasets, except on ROUGE-L. On Newsroom, we do better than the only other abstractive model (Seq2Seq with attention) by a massive margin and achieve better performance than the pointer generator even on the abstractive and mixed which their model should be better suited to since it has a copy mechanism. The Exconsumm model \cite{mendes2019jointly}  however, which is primarily an extractive model does better on this dataset. We suspect the poor ROUGE-L result is due to the absence of a copy mechanism that makes it hard to get \textit{exact} large n-gram matches. Figure \ref{fig:abstractiveness} further supports this hypothesis, it is evident that a model with a copy mechanism is often able to copy even upto 25-grams from the article. Further, \cite{graham2015re} finds that ROUGE-L is poorly correlated with human judgements when compared to ROUGE-1,2,3. In table \ref{tab:select_papers} and Table \ref{tab:arxiv_random_set}, we present qualitative results of abstracts of notable papers in our field and of our TLM conditioned on the introductions and extracted summaries of a random example from the arXiv test set. Table \ref{tab:qualitative_intro_extract} shows similar qualitative examples on the Newsroom dataset. Tables \ref{tab:arxiv_abstract}, \ref{tab:pubmed_abstract} and \ref{tab:patent_abstract} also provide different train / test settings for our TLM conditioned on extracted sentences. We show a performance upper bound conditioning the Transformer LM on oracle / ground-truth extracted sentences at both train and test time (TLM-I+E (G,G)). We also experiment with using either the ground-truth extracted sentences (TLM-I+E (G,M)) or the model extracted sentences (TLM-I+E (M,M)) during training and find that latter slightly impairs performance. Finally, figure \ref{fig:t-sne}, presents a visualization of the word embeddings learned by our TLM.

\begin{table}[ht]
\centering
\caption{Summarization results on the arXiv dataset. Previous work results from \cite{discourse/corr/abs-1804-05685}. The following lines are a simple baseline Lead-10 extractor and the pointer and classifier models. Our transformer LMs (TLM) are conditioned either on the Introduction (I) or along with extracted sentences (E) either from ground-truth (G) or model (M) extracts.}
\label{tab:arxiv_abstract}
\small
\begin{tabular}{|l|c|cccc|}
	\hline 
		\multirow{2}*{Model} & \multirow{2}*{Type} 
		&  \multicolumn{4}{c|}{ROUGE}  \\
        & & 1 & 2 & 3 & L  \\ \hline
        \multicolumn{6}{|c|}{\textbf{Previous Work}} \\
        \hline
        SumBasic  & Ext & 29.47 & 6.95 & 2.36 & 26.3  \\
        LexRank  & Ext & 33.85 & 10.73 & 4.54 & 28.99  \\
        LSA  & Ext & 29.91 & 7.42 & 3.12 & 25.67  \\
        \hline
        Seq2Seq & Abs & 29.3 & 6.00 & 1.77 & 25.56  \\
        Pointer-gen & Mix & 32.06 & 9.04 & 2.15 & 25.16 \\
        Discourse & Mix & 35.80 & 11.05 & 3.62 & 31.80 \\
        \hline
        \multicolumn{6}{|c|}{\textbf{Our Models}} \\
        \hline
        Lead-10 & Ext & 35.52 & 10.33 & 3.74 & 31.44 \\
        Sent-CLF & Ext & 34.01 & 8.71 & 2.99 & 30.41 \\ 
        Sent-PTR & Ext & \underline{42.32} & \underline{15.63} & \underline{7.49} & \underline{38.06} \\
        \hline
        TLM-I & Abs & 39.65 & 12.15 & 4.40 & 35.76 \\ TLM-I+E (M,M) & Mix & 41.15 & 13.98 & 5.63 & 37.40 \\ TLM-I+E (G,M) & Mix & \textbf{41.62} & \textbf{14.69} & \textbf{6.16} & \textbf{38.03}  \\ \hline
        \multicolumn{6}{|c|}{\textbf{Oracle}} \\
        \hline
        Gold Ext & Oracle & 44.25 & 18.17 & 9.14 & 35.33  \\
        TLM-I+E (G,G) & Oracle & 46.40 & 18.15 & 8.71 & 42.27 \\ \hline
\end{tabular}
\end{table}


\begin{table}[h!]
\caption{\small Qualitative Results - News articles and our model generated summaries on the NewsRoom dataset}
\begin{center}
\scriptsize
\begin{tabular}{|m{7.9cm}|}
\hline
\textbf{Document} --- A new plan from the government of the Philippines would offer free wireless internet to people across the country while also likely eating into the annual revenue of the nations telecoms. Bloomberg reports that the Philippines government plans to roll-out its free Wi-Fi services to roughly half of the countrys municipalities over the next few months and the country has its sights set on nationwide coverage by the end of 2016. The free wireless internet service will be made available in public areas such as schools, hospitals, airports and parks, and is expected to cost the government roughly \n41.33, 14.73, 6.80, 36.34n1020n525\sim$300 most representative words for each category, using TF-IDF scores and plot them.
\begin{figure}[htb]
    \center{\includegraphics[scale=0.43]
    {images/t-sne.png}}
    \caption{\label{fig:t-sne} t-sne visualization of the TLM-learned word embeddings. The model appears to partition the space based on the broad paper categoty in which it frequently occurs.}
\end{figure}

\end{document}
