\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} \usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bbm}
\usepackage{subcaption}
\usepackage{enumitem}

\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}



\usepackage[accepted]{icml2021}

\icmltitlerunning{Contrastive Learning of Musical Representations}

\begin{document}

\twocolumn[
    \icmltitle{Contrastive Learning of Musical Representations}





\icmlsetsymbol{equal}{*}

    \begin{icmlauthorlist}
        \icmlauthor{Janne Spijkervet}{uva}
        \icmlauthor{John Ashley Burgoyne}{uva}
    \end{icmlauthorlist}

    \icmlaffiliation{uva}{University of Amsterdam}
    \icmlcorrespondingauthor{Janne Spijkervet}{janne.spijkervet@gmail.com}

\icmlkeywords{Self-supervised Learning, Contrastive Learning, Music Information Retrieval}

    \vskip 0.3in
]





\printAffiliationsAndNotice{}  


\begin{abstract}
While supervised learning has enabled great advances in many areas of music, labeled music datasets remain especially hard, expensive and time-consuming to create.
In this work, we introduce SimCLR to the music domain and contribute a large chain of audio data augmentations, to form a simple framework for self-supervised learning of raw waveforms of music: \textit{CLMR}.
This approach requires no manual labeling and no preprocessing of music to learn useful representations.
We evaluate CLMR in the downstream task of music classification on the Magna\-Tag\-A\-Tune and Million Song datasets.
A \textit{linear} classifier fine-tuned on representations from a pre-trained CLMR model achieves an average precision of 35.4\% on the Magna\-Tag\-A\-Tune dataset, superseding fully supervised models that currently achieve a score of 34.9\%.
Moreover, we show that CLMR's representations are transferable using out-of-domain datasets, indicating that they capture important musical knowledge.
Lastly, we show that self-supervised pre-training allows us to learn efficiently on smaller labeled datasets: we still achieve a score of 33.1\% despite using only 259 labeled songs during fine-tuning.~\footnote{\label{code}Code available at: \href{https://github.com/spijkervet/CLMR}{https://github.com/spijkervet/clmr}}
\end{abstract} \section{Introduction}\label{sec:introduction}
Music has been both an important application domain and a source of fresh approaches to machine learning for more than two decades, and in recent years, there has been a focus on how deep learning methods can be adapted to work for musical audio.
Supervised, end-to-end learning methods have been widely used in tasks like chord recognition \cite{korzeniowski_fully_2016, chen_harmony_2019}, key detection \cite{korzeniowski_end--end_2017}, beat tracking \cite{bock_joint_2016}, music audio tagging \cite{pons_end--end_2017} and music recommendation \cite{van_den_oord_deep_2013}.
These methods require labeled corpora, which are difficult, expensive and time-consuming to create for music in particular \cite{doi:10.1080/09298215.2019.1613436}, while raw unlabeled music data is available in vast quantities.
Unsupervised alternatives to end-to-end deep learning for music are compelling, especially if these techniques also generalise to smaller datasets.

Despite the importance of unsupervised learning for raw audio signals, unsupervised learning for musical tasks has yet to see breakthroughs comparable to those in supervised learning.
There have been successes with methods like PCA, PMSC's and spherical -means that rely on a transformation pipeline \cite{hamel2011temporal, dieleman_feature_learning}, but learning effective representations of raw audio in an unsupervised manner has remained elusive overall.

\begin{figure}[t]
    \includegraphics[width=\columnwidth]{figs/roc_auc_magnatagatune.pdf}
    \caption{Performance and model complexity comparison of supervised models (grey) and self-supervised models (ours) in music classification of raw audio waveforms on the Magna\-Tag\-A\-Tune dataset to evaluate musical representations.
Supervised models were trained end-to-end, while CLMR and CPC are pre-trained without ground truth: their scores are obtained by training a \textit{linear} classifier on their learned representations but nonetheless perform competitively to the supervised models.}
    \label{fig:music_tagging_overview}
\end{figure}


Self-supervised representation learning is an upcoming unsupervised learning paradigm in many research domains \cite{dosovitskiy2015discriminative, oord_representation_2019, hjelm_learning_2019,chen_simple_2020,grill2020bootstrap}.
Without ground truth, there can be no ordinary loss function for training; self-supervised learning trains by way of a proxy loss function instead.
One way to preserve the amount of useful information during self-supervised learning is to define the proxy loss function with respect to a relatively simple `pretext' task, with the idea that a representation that is good for the pretext task will also be useful for downstream tasks.
Many approaches rely on heuristics to design pretext tasks \cite{doersch_unsupervised_2015,zhang2016colorful}, e.g., by witholding a pitch transformation \cite{spice}.
Alternatively, \emph{contrastive representation learning} formulates the proxy loss directly on the learned representations and relies on contrasting multiple, slightly differing versions of any one example by often using negative sampling strategies \cite{tian2019contrastive,he2019moco,chen_simple_2020} or by bootstrapping the representations \cite{grill2020bootstrap}.


In this paper, we combine the insights of a simple contrastive learning framework for images, SimCLR \cite{chen_simple_2020}, with recent advances in representation learning for audio in the time domain, and contribute a pipeline of data augmentations on musical audio, to form a simple framework for self-supervised, contrastive learning of representations of raw waveforms of music.
To compare the effectiveness of this simple framework compared to a more complex self-supervised learning objective, we also evaluate representations learned by contrastive predictive coding (CPC) \cite{oord_representation_2019}.
The self-supervised models are evaluated on the downstream music tagging task, enabling us to evaluate their versatility: music tags describe many characteristics of music, e.g., genre, instrumentation and dynamics.
Our key contributions are the following.
\begin{itemize}[topsep=0pt, partopsep=0pt, leftmargin=13pt, parsep=0pt, itemsep=4pt]
    \item CLMR achieves strong performance on the music classification task compared to supervised models, despite self-supervised pre-training and fine-tuning on the downstream task using a linear classifier (see Figure~\ref{fig:music_tagging_overview}).
    \item CLMR learns useful, compact representations from high-dimensional, raw signals of musical audio.
    \item CLMR enables efficient classification: when fine-tuning a music classification task, we achieve comparable performance using as few as 1\% of the labeled data.
    \item We show the out-of-domain transferability of representations learned from pre-training CLMR on entirely different corpora of musical audio.
    \item CLMR can learn from \emph{any} dataset of raw music audio, requiring neither transformations nor fine-tuning on the input data; nor do the models require manually annotated labels for pre-training.
    \item We provide an ablation study on the effectiveness of individual audio data augmentations.
\end{itemize}
 \section{Method}
\label{sec:method}
This work builds on SimCLR, a simple contrastive learning framework of visual representations \cite{chen_simple_2020}. Despite a task-agnostic, labelless discriminative pre-training approach, a linear classifier achieved performance comparable to fully supervised models on ImageNet classification.
Its learning objective is to maximise the agreement of latent representations of augmented views of the same image using a contrastive loss.
In Section~\ref{sec:related}, we will continue an overview of contrastive learning.

In CLMR, we adapt this framework to the domain of raw music audio. While most core components of CLMR have appeared in previous work, its ability to model waveforms of music cannot be explained by a single design choice, but by their composition.
We will first elaborate the four core components in the following subsections:
\begin{itemize}[topsep=0pt, partopsep=0pt, leftmargin=13pt, parsep=0pt, itemsep=4pt]
    \item A stochastic composition of data augmentations that produces two correlated, augmented examples of the same audio fragment, the `positive pair', denoted as  and .
    \item An encoder neural network  that encodes the augmented examples to their latent representations.
    \item A projector neural network  that maps the encoded representations to the latent space where the contrastive loss is formulated.
    \item A contrastive loss function, which aims to identify  from the negative examples in the batch  for a given .
\end{itemize}

The complete framework is visualised in Figure~\ref{fig:clmr_model}.

\begin{figure}[t]
    \includegraphics[width=\columnwidth]{figs/clmr_model.png}
    \caption{The complete framework operating on raw audio, in which the contrastive learning objective is directly formulated in the latent space of correlated, augmented examples of pairs of raw audio waveforms of music.}
    \label{fig:clmr_model}
\end{figure}


\subsection{Data Augmentations}
We designed a rich chain of audio augmentations for raw audio waveforms of music to make it harder for the model to identify the correct pair of examples.
Each consecutive augmentation is stochastically applied on  and  independently, i.e., each augmentation has an independent probability  of being applied to the audio.
The order of augmentations applied to audio is carefully considered, e.g., applying a delay effect \textit{after} reverberation empirically gives an entirely different result in music.

\begin{enumerate}[topsep=0pt, partopsep=0pt, leftmargin=13pt, parsep=0pt, itemsep=4pt]
    \item A random fragment of size  is selected from a full piece of audio, without trimming silence (e.g., the intro or outro of a song).
    The independently chosen fragments for  and  could overlap or be very disjoint, allowing the model to infer both local and global structures. This intuition is visualised in Figure~\ref{fig:global_local}.
    \item The polarity of the audio signal is inverted, i.e., the amplitude is multiplied by .
    \item Additive white Gaussian noise is added with a signal-to-noise ratio of 80 decibels to the original signal.
    \item The gain is reduced between  decibels.
    \item A frequency filter is applied to the signal.
    A coin flip determines whether it is a low-pass or a high-pass filter. The cut-off frequencies are drawn from uniform distributions on  or  respectively.
    \item The signal is delayed and added to the original signal with a volume factor of 0.5.
    The delay is randomly sampled between 200-500ms, in 50ms increments.
    \item The signal is pitch shifted.
    The pitch transposition interval is drawn from a uniform distribution of semitones between , i.e., a perfect fourth compared to the original signal's scale.
    \item Reverb is added to alter the signal's acoustics.
    The impulse response's room size, reverbation and damping factor is drawn from a uniform distribution on .
\end{enumerate}

The space of augmentations is not limited to these operations and could be extended to, e.g., randomly applying chorus, distortion and other modulations. Some of these have been shown to improve performance in self-supervised learning for automatic speech recognition in the time-domain as well \cite{pase_plus, wavaugment2020}.



\begin{figure}[t]
    \centering
    \includegraphics[width=0.6\columnwidth]{figs/clmr_local_global_crop.png}
    \caption{During pre-training, a random fragment of size  is selected from a full piece of audio. The independently chosen fragments  (blue) and  (yellow) could overlap or be disjoint, which should allow the  model to infer both local  and global structures.}
    \label{fig:global_local}
\end{figure}



\subsection{Batch Composition}
We sample one song from the batch, augment it into two examples, and treat them as the positive pair.
We treated the remaining  examples in the batch as negative examples, and did not sample the negative examples explicitly.
A larger batch size makes the model's objective harder -- there are simply more negative samples the anchor sample needs to identify the positve samle from -- but it can substantially improve model performance \cite{chen_simple_2020}.
This introduces a practical problem for raw audio when training on a GPU, as the input dimensionality of a raw waveform increases for higher sample rates.
The batch size can be increased more easily when audio is re-sampled at lower sampling rates.

Alternatively, multiple GPU's can be used for training, but this introduces another practical problem: batch normalisation is used in the encoder to stabilise training \cite{batch_normalisation} .
When training in a parallel manner, the batch normalisation statistics are usually aggregated locally per device.
Positive examples are sampled on the same device, leading to potential leakage of batch statistics which improves training loss, but counteracts learning of useful representations.
We used global batch normalisation, which aggregates the batch statistics over all devices during parallel training, to alleviate this issue. We leave the effect of different stabilisation strategies, e.g., layer normalisation \cite{henaff2019data}, for future work.

\subsection{Encoder}
To directly compare a state-of-the-art end-to-end supervised model used in music classification on raw waveforms against a self-supervised model, we use the SampleCNN architecture as our encoder \cite{lee2018samplecnn}.
Similarly, we use a fixed audio input of 59\,049 samples with a sample rate of 22\,050~Hz.
In this configuration, the SampleCNN encoder  consists of 9 1D convolution blocks, each with a with a filter size of 3, batch normalisation, ReLU activation and max pooling with pool size 3.
The fully connected and dropout layers are removed, which yields a 512-dimensional feature vector for every audio input.
The feature vectors from the encoder can be directly used in the learning objective, but formulating the objective on encodings mapped to a different latent space by a parameterised function helps the effectiveness of the representations \cite{chen_simple_2020}.
In our experiments, we use a non-linear layer  with an output dimensionality of 128 as the projection head .
There are 2.5 million trainable parameters in total, which is considerably less than the state-of-the-art supervised model as shown in Figure~\ref{fig:music_tagging_overview}.

We used 96 samples per batch and the aforedescribed encoder configuration to directly compare our self-supervised performance with the equally expressive fully supervised method \cite{lee2018samplecnn}.
We ran experiments with batch sizes of 96 on 2 NVIDIA 1080Ti, while for larger batches up to 4  Titan RTX's were used. With 2 1080Ti's, it takes 5 days to train 1\,000 epochs on our largest dataset.



\subsection{Contrastive Loss Function}
In keeping with recent findings on several objective functions in contrastive learning \cite{chen_simple_2020}, the contrastive loss function used in this model is normalised temperature-scaled cross-entropy loss, commonly denoted as \emph{NT-Xent loss}:



Instead of using a scoring function that preserves the mutual information between vectors, the pairwise similarity is measured using cosine similarity, .
It introduces a new temperature parameter  to help the model learn from hard negatives.
The indicator function  evaluates to  iff .
This loss is computed for all pairs, both  and , for . 




\subsection{Contrastive Predictive Coding}
We adjusted the original CPC encoder  \cite{oord_representation_2019} to a deeper architecture for more direct comparison \cite{lee2018samplecnn}. The encoder  consists of 7 layers with 512 filters each, and filter sizes  and strides .
Instead of relying on max-pooling, the filter sizes and strides are adjusted to parameterise and facilitate downsampling.
We also increased the number of prediction steps  to 20, effectively asking the network to predict 100~ms of audio into the future. The batch size is set to 64 from which 15 negative samples in the contrastive loss are drawn.


\subsection{Evaluation}
\label{sec:evaluation}
The evaluation of representations learned by self-supervised models is commonly done with linear evaluation \cite{oord_representation_2019,hjelm_learning_2019,chen_simple_2020}, which measures how linearly separable the relevant classes are under the learned representations.
We obtain representations  for all datapoints from a frozen CLMR network after pre-training has converged, and train a linear classifier using these self-supervised representations on the downstream task of music classification.
For CPC, the representations are extracted from the autoregressor, yielding a context vector  of size , which is global-average pooled to obtain a single vector of  dimensions.
For CLMR, the last 512-dimensional  from the encoder are used instead of  from the projection head.
We compute the evaluation metrics on a held-out test set, averaged over three runs on the training set using different random seeds.


\subsection{Data Efficient Classification}
When training on a specific task like music classification, only a limited amount of labeled data may be available. In the field of music, data is especially hard and expensive to collect from expert human annotators \cite{doi:10.1080/09298215.2019.1613436}.
Self-supervised approaches have demonstrated the ability to use substantially less labeled data when fine-tuning on a specific task \cite{henaff2019data,chen_simple_2020,chen2020big}.
To test the efficient classification capability of the CLMR model, we fine-tune the linear classifier on consecutive subsets of the labels in the train dataset and report its performance.
During the task-agnostic, self-supervised pre-training phase, 100\% of the data is used.

\subsection{Transfer Learning}
To test the out-of-domain generalisability of the learned representations, we pre-trained CLMR on entirely different music datasets.
After pre-training, we freeze the weights of the network and subsequently perform the linear evaluation procedure outlined in Section~\ref{sec:evaluation}.

\subsection{Optimisers}
We use the Adam optimiser \cite{adam_optimizer} with a learning rate of  and  and  during pre-training and employ Kaiming initialisation for all convolutional layers. The temperature parameter  is set to 0.5, since we observed consistent results regardless of varying batch sizes and temperature .

For linear evaluation, we use the Adam optimiser with a learning rate of , a weight decay of  and backpropagation is only done in the fine-tune head. The pre-trained encoder is frozen during all evaluation procedures, including the efficient classification and transfer learning experiments.
We also employ an early stopping mechanism when the validation scores do not improve for 5 epochs.

 \section{Experimental Results}\label{sec:results}

\begin{table}[t]
    \centering
    \begin{tabular}{@{}llcc@{}}\toprule
        Model & Dataset &  &  \\ \midrule
CLMR (ours) & MTAT & 88.5 (\textbf{89.3}) & \textbf{35.4} (\textbf{35.9}) \\
        Musicnn & MTAT & \textbf{89.0} & 34.9 \\
        SampleCNN & MTAT & 88.6 & 34.4 \\
        CPC (ours) & MTAT & 86.6 (88.0) & 31.0 (33.0) \\
        1D CNN & MTAT & 85.6 & 29.6 \\\midrule
        Musicnn & MSD & 87.4 & \textbf{28.5} \\
        SampleCNN & MSD & \textbf{88.4} & - \\
        CLMR (ours) & MSD & 85.7 & 25.0 \\
        \bottomrule
    \end{tabular}
    \caption{Tag prediction performance on the Magna\-Tag\-A\-Tune (MTAT) dataset and Million Song Dataset (MSD), compared with fully supervised models trained on raw audio waveforms.
    We omit works that operate on audio in the time-frequency domain.
For the supervised models, the tag-wise scores are obtained by end-to-end training.
For the self-supervised models, the scores are obtained by training a \emph{linear}, logistic regression classifier using the representations from self-supervised pre-training.
Scores in parenthesis show performance when adding one hidden layer to the logistic regression classifier (an MLP).}
    \label{tab:results}
\end{table}


\subsection{Datasets}
We evaluated the quality of our models' representations with music classification experiments.
Predicting the top~50 semantic tags in the Magna\-Tag\-A\-Tune and Million Song datasets  \cite{law2009evaluation,Bertin-Mahieux2011} are a popular benchmark for music classification.
These semantic tags are annotated by human listeners, and have a
varying degree of abstraction and describe many facets of music,
including genre, instrumentation and dynamics.
It is a multi-label classification task: each track can have multiple tags, of which we use the 50 most frequently occuring to compare our performance against supervised benchmarks.

The Magna\-Tag\-A\-Tune dataset consists of 25k music clips from 6622 unique songs, of which we use 187k fragments of 2.6 seconds for training, and the same train/test split as previous work \cite{pons_end--end_2017, lee2018samplecnn, dieleman_feature_learning}.
The Million Song Dataset contains a million songs, of which 240k previews of 30 seconds are available and labeled with Last.FM tag annotations.
We use a train, validation and test split of 201\,680 / 11\,774 / 28\,435 songs as used in previous work \cite{pons_end--end_2017, lee2018samplecnn}.
This results in 2.2 million music fragments of 2.6 seconds for training, i.e., almost 1600 hours of music.
The tags for the Million Song Dataset also contain overlapping genre and more semantic tags, e.g., `beautiful', `happy' and `sad', which are arguably harder to separate during the linear evaluation phase.

Like the other music classification studies, we use average tag-wise area under the receiver operating characteristic curve  (ROC-AUC) and average precision (PR-AUC) scores as evaluation metrics, which are global measures indicating how well the classifier ranks music fragments given a tag.
PR-AUC is calculated in addition to ROC-AUC, because ROC-AUC scores can be over-optimistic for imbalanced datasets like Magna\-Tag\-A\-Tune \cite{David-2006}.

\subsection{Quantitative Evaluation}
The most important goal set out in this paper, is to evaluate the difference in performance between an otherwise identical, fully supervised network when learning representations using a self-supervised objective.
CLMR exceeds the supervised benchmark with a PR-AUC of 35.4\%, despite task-agnostic, self-supervised pre-training and a \textit{linear} classifier for fine-tuning.
An additional 0.5\% PR-AUC performance gain is added by adding an extra hidden layer to the classifier.
Evaluation scores of the best-performing CLMR, CPC and other wave-form based models are shown in Table~\ref{tab:results}.
Our best results are obtained after longer pre-training, of which the details are outlined in Section~\ref{sec:epoch_experiments}.

The performance on the larger Million Song Dataset is lower compared to the supervised benchmark, but is still remarkable given the use of a linear classifier.
We attribute the difference to the more semantically complex tags in the Million Song Dataset, e.g., `catchy', `sexy', `happy', or more similar genre tags, e.g., `progressive rock', `classic rock' and `indie rock', which may not be easily linearly separable.

CPC also shows competitive performance with fully supervised models in the music classification task.
Despite CPC's good performance, self-supervised training indeed does not require a memory bank or more complex loss functions, e.g., those incorporating mutual information or more explicit negative sampling strategies, to learn useful representations.


\subsection{Data Augmentations}
\label{sec:data_augmentations}
The CLMR model relies on a pipeline of strong data augmentations to facilitate the learning of representations that are more robust and allow for better generalisation in the downstream task.
In Table~\ref{tab:transformation_study}, we show the linear
evaluation scores obtained by taking a random slice of audio (`random cropping') and performing one additional, individual augmentation.
While all datasets contain songs of variable length, we always sample a random slice of audio of the same size before applying other augmentations.
This makes it harder to assess the individual contribution of each augmentation to the downstream task performance.
We therefore consider an asymmetric data transformation setting: we only apply the augmentation(s) to one branch of the framework, while we settle with an identity function for the other branch (i.e., ) \cite{chen_simple_2020}.
The model is pre-trained from scratch for 1\,000 epochs after which linear evaluation is performed.

When only taking a random slice of audio (i.e., a `crop'), we achieve a PR-AUC score of .
Most individual augmentations show an increase in performance, while adding gain or delay does not impact performance as much.
Adding a filter to the augmentation pipeline increases the downstream performance more significantly.

\begin{table}[t]
\centering
    \footnotesize
    \begin{tabular}{lcccc}
    \toprule
    &\multicolumn{2}{c}{Tag} &\multicolumn{2}{c}{Clip}\\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5}
    Transform &  &  &  &   \\
    \midrule
    Filter & 87.6 & 33.3 & 92.5 & 67.9 \\
    Reverb & 86.5 & 31.7 & 91.8 & 65.8 \\
    Polarity & 86.3 & 31.5 & 91.7 & 65.7 \\
    Noise & 86.1 & 31.5 & 91.5 & 65.5 \\
    Pitch & 86.4 & 31.5 & 91.5 & 65.3 \\
    Gain & 86.2 & 31.1 & 91.5 & 65.1 \\
    Delay & 85.8 & 30.5	& 91.3 & 64.9 \\
    Crop & 85.8 & 30.5 & 91.3 & 64.8  \\
    \bottomrule
    \end{tabular}
    \caption{CLMR music tagging performance using a random crop together with one other audio data augmentation.}
    \label{tab:transformation_study}
\end{table}

Besides evaluating the individual contribution of each augmentation with , we also vary this probability: .
This is done to assess the optimal amount of augmentation to each example, i.e., the contrastive learning task should neither too hard, nor too simple, for learning effective representations for the music classification task.
The linear evaluation PR-AUC score is shown for each augmentation under a different probability  in Figure~\ref{fig:transformation_probabilities}.
For the Polarity and Filter transformations, performing them more often with a probability of  is beneficial.
For the Delay, Pitch and Reverb transformations, a transformation probability of  works better than performing them more aggressively.
Generally, we find that strong data augmentations result in more robust representations and better downstream task performance.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\columnwidth]{figs/transformation_probabilities.pdf}
    \caption{ scores for transformations under different, consecutive probabilities }
    \label{fig:transformation_probabilities}
\end{figure}


\subsection{Data Efficient Classification Experiments}
Figures \ref{fig:perc_train_data_magnatagatune} and~\ref{fig:perc_train_data_msd} show the PR-AUC scores obtained when increasing the amount of labels available during fine-tuning.
For both datasets, self-supervised pre-training greatly improves preformance when less labeled data is available.
Using 100 fewer labels, i.e., only 259 songs, CLMR scores 33.1\% PR-AUC compared to 24.8\% PR-AUC obtained with an equivalent, end-to-end trained supervised model trained on 25k songs.
Pre-training using a self-supervised objective without labels therefore substantially improves efficient classification: only  of the labels are required while maintaining a similar performance.
For the Million Song Dataset, a fully supervised end-to-end trained model exceeds CLMR at  of the labels, which are 24,190 unique songs in total.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\columnwidth]{figs/perc_train_data_magnatagatune.pdf}
    \caption{Percentage of labels used for training vs.
the achieved  score on the Magna\-Tag\-A\-Tune dataset}
    \label{fig:perc_train_data_magnatagatune}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\columnwidth]{figs/perc_train_data_msd.pdf}
    \caption{Percentage of labels used for training vs.
the achieved  score on the Million Song Dataset.}
    \label{fig:perc_train_data_msd}
\end{figure}


\subsection{Transfer Learning Experiments}
Originally made for chord recognition, we use 461 contemporary pop songs recorded between the 1940's and 2000's from the McGill Billboard dataset \cite{burgoyne_billboard}.
The Free Music Archive dataset \cite{fma_dataset} consists of 22\,413 songs for the `medium' version, and the fault-filtered GTZAN dataset \cite{tzanetakis2002musical,sturm2013gtzan} contains 930 fragments of 30 seconds, both popular for music classification.

The results of the transfer learning experiments are shown in Table~\ref{tab:magnatagatune_results}.
Both CPC and CLMR show the ability to learn effective representations from out-of-domain datasets without ground truth, and even exceed accuracy scores of previous, supervised end-to-end systems on raw audio \cite{dieleman2014end}.
Moreover, both models even demonstrate the ability to learn useful representations on the much smaller GTZAN and Billboard datasets.
The CLMR model performs better when it is pre-trained on larger datasets, which is expected as it heavily relies on the number of unique, independent examples that make the contrastive learning task harder, resulting in more robust representations.
When pre-training on smaller datasets, CPC can find more useful representations, especially when adding an extra hidden layer to the fine-tune head.


\begin{table}[t]
\centering
    \footnotesize
    \begin{tabular}{lccc}
    \toprule
    Model & Train Dataset &  &  \\
    \midrule
    CLMR & MSD &  \textbf{86.57} & \textbf{32.04} \\
    CPC & FMA & 86.3 (\underline{87.8}) & 30.7 (\underline{32.5}) \\
    CLMR & FMA & 86.2 (86.6) & 30.6 (31.2) \\
    CPC & Billboard & 85.8 (86.3) & 29.7 (30.2) \\
    CPC & GTZAN & 83.4 (86.0) & 26.9 (29.7) \\
    CLMR & Billboard & 82.7 (84.2) & 26.9 (27.8) \\
    CLMR & GTZAN & 81.9 (85.4) & 26.2 (29.5) \\
    \bottomrule
    \end{tabular}
    \caption{Transfer learning experiments for CLMR and CPC, which are trained on a separate dataset and evaluated on the Magna\-Tag\-A\-Tune dataset.
The reported scores are obtained with a frozen, pre-trained encoder and a linear classifier. Scores in parenthesis are obtained when adding one extra hidden layer to the classifier.}
    \label{tab:magnatagatune_results}
\end{table}



\subsection{Batch Size}\label{sec:exp_mini_batch_size}
The complexity of contrastive learning increases with larger batch sizes, which may result in better representations.
We pre-train from scratch until convergence with varying batch sizes and study its effect on the linear evaluation performance in Table~\ref{tab:mini_batch_ablation}.
While our smallest model already shows competitive performance compared to fully supervised models, the performance increased when using 96 examples per batch.
Our largest model did not perform as expected, and scores consistently lower than our middle-sized model.
We hypothesise that the task of inferring the positive pair of 2.6 second long raw musical audio fragments, in a pool of 910 negative samples, may require even longer training, or is simply too hard for the current encoder.

\begin{table}[t]
\centering
    \footnotesize
    \begin{tabular}{rcccc}
    \toprule
    &\multicolumn{2}{c}{Tag} &\multicolumn{2}{c}{Clip}\\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5}
    Batch Size &  &  &  &   \\
    \midrule
    456 & 88.1 & 34.9 & \textbf{93.0} & 68.9 \\
    96 & \textbf{88.5} & \textbf{35.1} & \textbf{93.0} & \textbf{69.2} \\
    48 & 87.9 & 34.6 & 92.9 & 68.8 \\
    \bottomrule
    \end{tabular}
    \caption{Effect of the batch size used during self-supervised training on the linear music classification performance.}
    \label{tab:mini_batch_ablation}
\end{table}


\subsection{Training Duration}
\label{sec:epoch_experiments}
Contrastive learning techniques benefit from longer training compared to their supervised equivalent \cite{chen_simple_2020}.
While larger batch sizes increase the pretext task complexity, training longer increases the number of natural variations of the data due to the random augmentation scheme.
We pre-train from scratch until convergence and set the batch size to 96.
Table~\ref{tab:epoch_ablation} shows that increasing the self-supervised training duration improves downstream performance.

\begin{table}[t]
\centering
    \footnotesize
    \begin{tabular}{rcccc}
    \toprule
    &\multicolumn{2}{c}{Tag} &\multicolumn{2}{c}{Clip}\\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5}
    Epochs &  &  &  &   \\
    \midrule
    10\,000 & \textbf{88.5} & \textbf{35.4} & \textbf{93.2} & \textbf{69.3} \\
    3\,000 & \textbf{88.5} & 35.1 & 93.1 & 69.2 \\
    1\,000 & 88.3 & 34.4 & 92.9 & 68.6 \\
    300 & 87.1 & 32.7 & 92.0 & 66.6 \\
    100 & 86.4 & 30.9 & 91.3 & 64.1 \\
    \bottomrule
    \end{tabular}
    \caption{Effect of the self-supervised pre-training duration on the downstream, linear music classification performance.}
    \label{tab:epoch_ablation}
\end{table}



\subsection{Sample Rates}
\label{subsection:sample_rates}
We show in Table~\ref{tab:sample_rate_ablation} that there is a marginal penalty to the final scores for the self-supervised models when re-sampling the audio to 8\,000~Hz and 16\,000~Hz respectively, which is in line with previous work \cite{lee2018samplecnn}.
Since re-sampling disturbs the frequency spectrum, we isolate its contribution by disregarding additional augmentations, i.e., only apply random cropping.

\begin{table}[t]
\centering
    \footnotesize
      \begin{tabular}{rcccc}
        \toprule
        &\multicolumn{2}{c}{Tag} &\multicolumn{2}{c}{Clip}\\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5}
        SR &  &  &  &   \\
        \midrule
        22\,050 & \textbf{85.8} & \textbf{30.5} & \textbf{91.3} & \textbf{64.8} \\
        16\,000 & 85.5 & 30.4 & 91.0 & 64.1 \\
        8\,000 & 84.8 & 29.8 & 90.6 & 62.9 \\
        \bottomrule
      \end{tabular}
      \caption{Effect of varying the input audio's sample rate on the linear music classification performance.}
      \label{tab:sample_rate_ablation}
    \end{table}


\subsection{Qualitative Analysis}
For a qualitative view of what the self-supervised models learn from music, we demonstrate in Figure~\ref{fig:filter_visualisation} the magnitude spectrum of the learned filters of the sample-level convolutional layers (layers 1, 4 and 6) for CLMR and CPC, pre-trained on the MagnaTagATune dataset.
In CLMR, the first layer is sensitive to a single, very small band of frequencies around 7500~Hz, while in higher layers the filters spread themselves first linearly and then non-linearly across the full range.
CPC shows a similar pattern in the higher layers, but shows a strong activation of two frequencies that span an octave in the first layer.
Conversely, the filters of the supervised-trained encoder have a non-linearity that is found in frame-level end-to-end learning \cite{dieleman2014end}, as well as in perceptual pitch scales such as mel or Bark scales.
In the supplementary materials, we demonstrate that self-supervised models are also capable of showing this filter behavior.

Additionally, we show how cleanly separable the self-supervised representations are using a -SNE manifold \cite{maaten2008visualizing} in Figure~\ref{fig:tsne_manifold}, and show in the supplementary materials that the difference in performance between self-supervised and supervised models is qualitatively marginal in more detail: there is no single tag performance difference larger than 4\% ROC-AUC.

\begin{figure}[t]
    \centering
    \subcaptionbox{CLMR\label{fig:1a}}{\includegraphics[width=.16\textwidth]{figs/magnatagatune/clmr_spectrum/epoch1490_layer0.png}}\hfill
    \subcaptionbox{CLMR\label{fig:1a}}{\includegraphics[width=.16\textwidth]{figs/magnatagatune/clmr_spectrum/epoch1490_layer3.png}}\hfill
    \subcaptionbox{CLMR\label{fig:1a}}{\includegraphics[width=.16\textwidth]{figs/magnatagatune/clmr_spectrum/epoch1490_layer5.png}}\hfill
    \subcaptionbox{CPC\label{fig:1a}}{\includegraphics[width=.16\textwidth]{figs/magnatagatune/cpc_spectrum/epoch670_layer0.png}}\hfill
    \subcaptionbox{CPC\label{fig:1a}}{\includegraphics[width=.16\textwidth]{figs/magnatagatune/cpc_spectrum/epoch670_layer3.png}}\hfill
    \subcaptionbox{CPC\label{fig:1a}}{\includegraphics[width=.16\textwidth]{figs/magnatagatune/cpc_spectrum/epoch670_layer5.png}}
    \caption{Normalised magnitude spectrum of the filters of the self-supervised models in the sample-level convolution layers, sorted by the frequency of the peak magnitude.
Gradient ascent is performed on a randomly initialised waveform of 729 samples (close to typical frame size) and its magnitude spectrum is calculated subsequently.
Each vertical line in the graph represents the frequency spectrum of a different filter. The first three images are taken from a pre-trained, converged CLMR model, the last three from a CPC model, on the MagnaTagATune dataset.}
    \label{fig:filter_visualisation}
\end{figure}
 
\section{Related Work}
\label{sec:related}


\subsection{Unsupervised Representation Learning}
The goal of representation learning is to identify features that make  prediction tasks easier and more robust to the complex variations of natural data \cite{bengio2013representation}.
In unsupervised representation learning, generative modeling and likelihood-based models typically find useful representations of the data by attempting to reconstruct the observations on the basis of their learned representations \cite{goodfellow2014generative, unsupervised_gan}.
\emph{Self-supervised} representation learning aims to identify the explanatory factors of the data using an objective that is formulated with respect to the learned representations directly \cite{doersch_unsupervised_2015,zhang2016colorful,oord_representation_2019,henaff2019data,grill2020bootstrap}.

\subsection{Contrastive Learning for Music}
Compared to vision, work on self-supervised learning in audio is still very limited.
Contrastive predictive coding is a universal approach to contrastive learning, and has been successful for speaker and phoneme classification using raw audio, among other tasks \cite{oord_representation_2019}. PASE \cite{Pascual2019} introduces several self-supervised workers that solve regression or binary discrimation tasks, that jointly optimise an encoder for speech recognition. To improve the representations for mismatched acoustic conditions and their transferability, they apply augmentations to the input speech signal \cite{pase_plus}.
In music information retrieval, recent advances have been made in self-supervised pitch estimation \cite{spice}, closely matching supervised, state-of-the-art baselines \cite{crepe} despite being trained without ground truth labels.
This work relies on preprocessing with a CQT transform. To the best of our knowledge, we are the first to perform self-supervised learning on raw audio waveforms of musical audio and evaluate them in a music task.












 

\begin{figure}[t]
    \centering
    \includegraphics[width=.6\columnwidth]{figs/tsne-clmr.png}
    \caption{-SNE manifold visualisation from audio representations learned by a CLMR model of a subset of 10 music tracks, with each sixty 2.67 second long fragments.
    Each point represents a music fragment, each belonging to a differently coloured track.}
    \label{fig:tsne_manifold}
\end{figure}

\section{Conclusion}\label{sec:conclusion}
In this paper, we presented CLMR, a self-supervised contrastive learning framework that learns useful representations of raw waveforms of musical audio.
The framework requires no preprocessing of the input audio and is trained without ground truth, which enables simple and straightforward pre-training on music datasets of unprecedented scale.
We tested the learned, task-agnostic representations by fine-tuning a linear classifier on the music classification task on the Magna\-Tag\-A\-Tune and Million Song Dataset, achieving competitive performance compared to fully supervised models.
We also showed that CLMR can achieve comparable performance using 100 fewer labels, and demonstrated the out-of-domain transferability of representations learned from pre-training on entirely different datasets of music.
To foster reproducibility and future research on self-supervised learning in music information retrieval, we publicly release the pre-trained models and the source code of all experiments of this paper (\ref{code}).
The simplicity of training the model without a direct supervised signal and without preprocessing the audio, together with encouraging results obtained with a single linear layer optimised for a challenging music task, are exciting developments towards unsupervised learning on raw musical audio.
\newpage
 \section*{Acknowledgements}
We would like to thank Jordan B.L. Smith for his feedback on the draft. We would also like to extend our gratitude to the University of Amsterdam and SURFsara for giving us access to their Research Capacity Computing Services GPU cluster (Lisa). 
\bibliography{main}
\bibliographystyle{icml2021}

\cleardoublepage
\appendix
\counterwithin{figure}{section}
\counterwithin{table}{section}
\onecolumn

\appendix
\section{Audio Preprocessing}
In this paper, we used raw audio waveform data for training in both the pre-training and linear evaluation phases. The default audio sample rate for all experiments is 22\,050~Hz, except for the sample rate experiment in section \ref{subsection:sample_rates}. The Magna\-Tag\-A\-Tune dataset contains monophonic 30-second audio fragments in MP3 format, sampled at 16\,000~Hz. Some of the audio fragments originate from the same song. We reconstructed  original song by concatenating the fragments into a single file, to avoid occurances of fragments of the same song in the same batch of positive- and negative pairs, thereby ensuring i.i.d. data for training.

The audio files from the Million Song Dataset were obtained from the 7digital service, which provides stereo 30-second audio fragments in MP3 format sampled at 44\,100~Hz.

All files were re-sampled to 22\,000~Hz, 16\,000~Hz and 8\,000~Hz and decoded to the PCM format with , using the following command:
\begin{verbatim}
    ffmpeg -i {input_file}.mp3 -ar {target_sample_rate} {output_file}.wav
\end{verbatim}

This is the only preprocessing step that we performed before training.

\section{Data Augmentation Details}
\label{appendix:data_augmentation}
The default pre-training setting, which we also used for our best models, uses 8 audio data augmentations. Not all augmentations are necessarily applied to all inputs: each independent data augmentation is applied with a probability tuned during hyperparameter gridsearch. The most effective augmentations and their probabilities are presented in Section \ref{sec:data_augmentations}. The implementation details for each augmentation are provided below.

\subsection{Random Crop}
The audio is cropped with a number of samples  for sample rates 8\,000, 16\,000 and 22\,050~Hz respectively. To ensure that every sample in the batch is of the same size, the fragment's window we can crop from with original length  is adjusted to .

\subsection{Polarity inversion}
The polarity of the audio signal is inverted by multiplying the amplitude of the signal by . 

\subsection{Additive White Gaussian Noise}
White Gaussian noise is added to the complete signal with a signal-to-noise ratio (SNR) of 80 decibels.

\subsection{Gain Reduction}
The gain of the audio signal is reduced at random using a value drawn uniformly between -6 and 0 decibels. In our implementation, we use the  interface.

\subsection{Frequency Filter}
A frequency filter is applied to the signal using the  library \cite{essentia}. We process the signal with either the  or  algorithm \cite{zolzer2002dafx}, which is determined by a coin flip.

For the low-pass filter, we draw the cut-off frequency from a uniform distributions between 2200 and 4000~Hz. All frequencies above the drawn cut-off frequency are filtered from the signal.

Similarly for the high-pass filter, we draw the cut-off frequency from a uniform distributions between 200 and 1200~Hz. All frequencies below the cut-off frequency are filtered from the signal.


\subsection{Delay}
The signal is delayed by a value chosen randomly between 200 and 500 milliseconds, in 50ms increments. Subsequently, the delayed signal is added to the original signal with a volume factor of 0.5, i.e., we multiply the signal's amplitude by 0.5. An example implementation of this digital signal processing effect is given below in Python using PyTorch:

\begin{verbatim}
    import random
    import torch
    import numpy as np

    ms = random.choice(
        np.arange(200, 500, 50)
    )

    offset = int(ms * (sample_rate / 1000))
    beginning = torch.zeros(audio.shape[0], offset)
    end = audio[:, :-offset]
    delayed_signal = torch.cat((beginning, end), dim=1)
    delayed_signal = delayed_signal * self.volume_factor
    audio = (audio + delayed_signal) / 2
\end{verbatim}

\subsection{Pitch Shift}
The pitch of the signal is shifted up or down, depending on the pitch interval that is drawn from a uniform distribution between -5 and 5 semitones, i.e., up to a perfect fourth higher or lower than the original signal. We assume 12-tone equal temperament tuning that divides a single octave in 12 semitones.


Pitch shifting is done using the  library, which is interfaced from the  Python library \cite{wavaugment2020}.

\subsection{Reverb}
To alter the original signal's acoustics, we apply a Schroeder reverberation effect \cite{schroeder1962natural}. This is again done using the  library that is interfaced from the  Python library \cite{wavaugment2020}.



\section{Additional Experimental Results}
\label{appendix:additional_results}


\subsection{Additional Hidden Layer}
After pre-training with the self-supervised objective, we performed a linear evaluation to test the expressivity of the representations with a classifier of limited capacity. To further assess the representations' usability, we add a single hidden layer to our classifier and again measure the performance on the downstream task of music classification. The results of this experiment are shown in Table \ref{tab:add_hidden_layer} for linear evaluation (left of the slashes; also shown in the main paper and repeated here for convenience) as well as when a hidden layer is added (right of the slashes), for different pre-training durations measured in epochs.

\begin{table}[h]
\centering
    \footnotesize
    \begin{tabular}{lcccc}
    \toprule
    &\multicolumn{2}{c}{Tag} &\multicolumn{2}{c}{Clip}\\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5}
    Epochs &  &  &  &   \\
    \midrule
    10\,000 & 88.5 / \textbf{89.3} & 35.4 / \textbf{35.9} & 93.2 / \textbf{93.5} & 69.3 / \textbf{70.0} \\
    3\,000 & 88.5 / 88.9 & 35.1 / 35.5 & 93.0 / 93.3 & 69.2 / 69.7 \\
    1\,000 & 88.3 / 88.6 & 34.4 / 34.9 & 92.3 / 93.1 & 68.6 / 69.2 \\ 
    300 & 87.1 / 87.4 & 32.7 / 32.5 & 92.0 / 92.0 & 66.6 / 66.7 \\
    100 & 86.4 / 86.6 & 30.9 / 31.3 & 91.3 / 91.3 & 64.1 / 64.6 \\ 
    \bottomrule
    \end{tabular}
    \caption{Performance difference of a linear classifier and when a single hidden layer is added to the classifier on the downstream music classification performance, for different self-supervised pre-training durations.}
    \label{tab:add_hidden_layer}
\end{table}




\subsection{Additional Qualitative Results}
Figure \ref{fig:tsne} shows -SNE visualisations \cite{maaten2008visualizing} of our best self-supervised models representations  and , for a randomly set of music tracks from the validation set. We show that both self-supervised models can cleanly seperate the classes.

Figure \ref{fig:tag_scores} shows the sorted tag-wise ROC-AUC scores for the top-50 tags in the Magna\-Tag\-A\-Tune dataset, reported for linear evaluation of the trained self-supervised CLMR and CPC models, and the fully end-to-end-trained supervised model. We show that no single tag loses more than 4\% ROC-AUC when trained using self-supervised pre-training and fine-tuned with a linear classifier, as compared to the supervised benchmark.

Figure \ref{fig:filter_visualisation_large} shows the normalised magnitude spectrum of the filters of the self-supervised models CLMR and CPC in the sample-level convolution layers. We perform gradient ascent on a randomly initialised waveform of length 729, i.e., a value that is close to a typical frame size and also interacts conveniently with the convolutional structure of the encoder network, and subsequently calculate the magnitude spectrum. The x-axis plots the filter number, the y-axis the magnitude spectrum for a filter number. Lastly, we sort the plot by the frequency of the peak magnitude. Interestingly, CLMR shows a similar filter structure for the Billboard data set as fully supervised models that were trained on the Magna\-Tag\-A\-Tune dataset \cite{dieleman2014end,lee2018samplecnn}, i.e., filter structures that are also found in mel- and Bark-band filters \cite{Stevens1937ASF, barkbank_1961}.

\begin{figure}[h]
    \centering
    \subcaptionbox{}{\includegraphics[width=.40\textwidth]{figs/tsne-clmr.png}}\hfill
    \subcaptionbox{}{\includegraphics[width=.40\textwidth]{figs/tsne-cpc.png}}\hfill
    \caption{-SNE manifolds of the hidden vectors of music audio from a subset of 10 music tracks, i.e., in this case classes, from the validation set. Each point represents a 2.67 second long music fragment belonging to a music track.}
    \label{fig:tsne}
\end{figure}


\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figs/tag_retrieval.png}
    \caption{Tag-wise ROC-AUC scores for the top-50 tags in the Magna\-Tag\-A\-Tune dataset, reported for linear, logistic regression classifiers trained on representations of self-supervised models CLMR and CPC, and compared to a fully supervised, end-to-end SampleCNN model.}
    \label{fig:tag_scores}
\end{figure*}


\begin{figure*}[h]
    \centering
    \subcaptionbox{CLMR\label{fig:1a}}{\includegraphics[width=.33\textwidth]{figs/magnatagatune/clmr_spectrum/epoch1490_layer0.png}}\hfill
    \subcaptionbox{CLMR\label{fig:1a}}{\includegraphics[width=.33\textwidth]{figs/magnatagatune/clmr_spectrum/epoch1490_layer3.png}}\hfill
    \subcaptionbox{CLMR\label{fig:1a}}{\includegraphics[width=.33\textwidth]{figs/magnatagatune/clmr_spectrum/epoch1490_layer5.png}}\hfill
    \subcaptionbox{CPC\label{fig:1a}}{\includegraphics[width=.33\textwidth]{figs/magnatagatune/cpc_spectrum/epoch670_layer0.png}}\hfill
    \subcaptionbox{CPC\label{fig:1a}}{\includegraphics[width=.33\textwidth]{figs/magnatagatune/cpc_spectrum/epoch670_layer3.png}}\hfill
    \subcaptionbox{CPC\label{fig:1a}}{\includegraphics[width=.33\textwidth]{figs/magnatagatune/cpc_spectrum/epoch670_layer5.png}}

    \subcaptionbox{CLMR\label{fig:1a}}{\includegraphics[width=.33\textwidth]{figs/billboard/clmr_spectrum/epoch1490_layer0.png}}\hfill
    \subcaptionbox{CLMR\label{fig:1a}}{\includegraphics[width=.33\textwidth]{figs/billboard/clmr_spectrum/epoch1490_layer3.png}}\hfill
    \subcaptionbox{CLMR\label{fig:1a}}{\includegraphics[width=.33\textwidth]{figs/billboard/clmr_spectrum/epoch1490_layer5.png}}\hfill
    \subcaptionbox{CPC\label{fig:1a}}{\includegraphics[width=.33\textwidth]{figs/billboard/cpc_spectrum/epoch1490_layer0.png}}\hfill
    \subcaptionbox{CPC\label{fig:1a}}{\includegraphics[width=.33\textwidth]{figs/billboard/cpc_spectrum/epoch1490_layer3.png}}\hfill
    \subcaptionbox{CPC\label{fig:1a}}{\includegraphics[width=.33\textwidth]{figs/billboard/cpc_spectrum/epoch1490_layer5.png}}

    \caption{Normalised magnitude spectrum of the filters of the self-supervised models in the sample-level convolution layers, sorted by the frequency of the peak magnitude.
Gradient ascent is performed on a randomly initialised waveform of 729 samples (close to typical frame size) and its magnitude spectrum is calculated subsequently.
Each vertical line in the graph represents the frequency spectrum of a different filter. The first three images are taken from a pre-trained, converged CLMR model, the last three from a CPC model, on the Magna\-Tag\-A\-Tune or Billboard datasets}
    \label{fig:filter_visualisation_large}
\end{figure*}
 \end{document}