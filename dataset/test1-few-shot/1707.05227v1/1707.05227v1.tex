

\documentclass[11pt,letterpaper]{article}
\usepackage{emnlp2017}
\usepackage{times}
\usepackage{latexsym}

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{amsmath}

\usepackage{color}

\emnlpfinalcopy

\def\emnlppaperid{***}



\newcommand\BibTeX{B{\sc ib}\TeX}


\title{Auxiliary Objectives for Neural Error Detection Models}



\author{\hspace{-0.5cm}Marek Rei\\
	    \hspace{-0.5cm}The ALTA Institute\\
	    \hspace{-0.5cm}Computer Laboratory\\
	    \hspace{-0.5cm}University of Cambridge\\
        \hspace{-0.5cm}United Kingdom\\
	    \hspace{-0.5cm}{\tt marek.rei@cl.cam.ac.uk}
	    \And
	    \hspace{0.5cm}Helen Yannakoudakis\\
	    \hspace{0.5cm}The ALTA Institute\\
	    \hspace{0.5cm}Computer Laboratory\\
	    \hspace{0.5cm}University of Cambridge\\
        \hspace{0.5cm}United Kingdom\\
        \hspace{0.5cm}{\tt helen.yannakoudakis@cl.cam.ac.uk}}

\date{}

\begin{document}

\maketitle

\begin{abstract}
We investigate the utility of different auxiliary objectives and training strategies within a neural sequence labeling approach to error detection in learner writing. 
Auxiliary costs provide the model with additional linguistic information, allowing it to learn general-purpose compositional features that can then be exploited for other objectives.
Our experiments show that a joint learning approach trained with parallel labels on in-domain data improves performance over the previous best error detection system. 
While the resulting model has the same number of parameters, the additional objectives allow it to be optimised more efficiently and achieve better performance.
\end{abstract}


\section{Introduction}



Automatic error detection systems for learner writing need to identify various types of error in text, ranging from incorrect uses of function words, such articles and prepositions, to semantic anomalies in content words, such as adjective--noun combinations.
To tackle the scarcity of error-annotated training data, previous work has investigated the utility of automatically generated ungrammatical data \cite{foster2009generrate,felice2014generating}, as well as explored learning from native well-formed data \cite{rozovskaya2016grammatical,gamon2010using}. 

In this work, we investigate the utility of supplementing error detection frameworks with additional linguistic information that can be extracted from the available error-annotated learner data. 
We construct a neural sequence labeling system for error detection that allows us to learn better representations of language composition and detect errors in context more accurately. 
In addition to predicting the binary error labels, we experiment with also predicting additional information for each token, including token frequency and the specific error type,
which can be extracted from the existing data, as well as part-of-speech (POS) tags and dependency relations, which can be generated automatically using readily available toolkits. 

These auxiliary objectives provide the sequence labeling model with additional linguistic information, allowing it to learn useful compositional features that can then be exploited for error detection. This can be seen as a type of multi-task learning, where the model learns better compositional features via shared representations with related tasks. 
While common approaches to multi-task learning require randomly switching between different tasks and datasets, we demonstrate that a joint learning approach trained on in-domain data with parallel labels substantially improves error detection performance on two different datasets. 
In addition, the auxiliary labels are only required during the training process, resulting in a better model with the same number of parameters. 

In the following sections, we describe our approach to the task, systematically compare the informativeness of various auxiliary loss functions, investigate alternative training strategies, and examine the effect of additional training data. 


\section{Error Detection Model}
\label{sec:errordet}

In addition to the scarcity of errors in the training data (i.e., the majority of tokens are correct), recent research has highlighted the variability in manual correction of
writing errors: re-annotation of the CoNLL 2014 shared task test set
by 10 annotators demonstrated that even humans have great difficulty in agreeing how to correct writing errors \cite{bryantfar}. Given the challenges of the all-errors correction task, previous research has demonstrated that detection models can \textit{detect} more errors than systems focusing on correction \cite{Rei2016}, and therefore provide more extensive feedback to the learner.  

Following \newcite{Rei2016}, we treat error detection as a sequence labeling task -- each token in the input sentence is assigned a label, indicating whether it is correct or incorrect given the current context -- and construct a bidirectional recurrent neural network for detecting writing errors.
The model is given a sequence of tokens as input, which are then mapped to a sequence of distributed word embeddings .
These embeddings are then given as input to a bidirectional LSTM \cite{Hochreiter1997} moving through the sentence in both directions. At each step, the LSTM calculates a new hidden representation based on the current token embedding and the hidden state from the previous step. 




Next, the network includes a {tanh}-activated feedforward layer, using the hidden states from both LSTMs as input, allowing the model to learn more complex higher-level features.
By combining the hidden states from both directions, we are able to have a vector that represents a specific token but also takes into account context on both sides:


\noindent where  and  are fully-connected weight matrices.

The final layer calculates label predictions based on the layer .
The softmax activation function is used to output a normalised probability distribution over all the possible labels for each token:


\noindent where  is a weight matrix and  is a vector with a position for each possible label. In order to find the predicted label, we return the element with the highest predicted value.

The model is optimised using cross entropy, which is equivalent to optimising the negative log-likelihood of the correct labels:



\noindent where  is the predicted probability of token  having label , and  has the value  if the correct label for token  is , and the value  otherwise.

We also make use of the character-level extension described by \newcite{Rei2016a}. Each token is separated into individual characters and mapped to character embeddings. Using a bidirectional LSTM and a hidden feedforward component, the character vectors are composed into a character-based token representation. Finally, a dynamic gating function is used to combine this representation with a regular token embedding, taking advantage of both approaches. This component allows the model to capture useful morphological and character-based patterns, in addition to learning individual token-level vectors of common tokens.


\begin{table*}[t]
\small
\begin{tabular}{rcccccccccccc} \toprule
\textbf{words} & My & husband & was & following & a & course & all & the & week & in & Berne & .\\
\textbf{target} & c & c & c & i & c & c & c & i & c & c & c & c\\
\textbf{freq} & 5 & 3 & 8 & 4 & 8 & 5 & 7 & 9 & 5 & 8 & 0 & 10\\
\textbf{lang} & fr & fr & fr & fr & fr & fr & fr & fr & fr & fr & fr & fr\\
\textbf{error} & c & c & c & RV & c & c & c & UD & c & c & c & c\\
\textbf{POS} & APP\ W_f^{(n)}W_b^{(n)}nd_t^{(n)}W_y^{(n)}y_t^{(n)}nn=2y_{t,k}^{(n)}tkn\widetilde{y}_{t,k}^{(n)}10\alpha_nn\alpha_nn\alpha_n0.1n[0.05, 0.1, 0.2, 0.5, 1.0]F_{0.5}F_{0.5}w\textrm{int}(\textrm{log}(\textrm{freq}_{\textrm{train}}(w))F_{0.5}F_{0.5}F_{0.5}F_{0.5}F_{0.5}2.1\%47.7\%F_{0.5}4.3\%6.6\%2\%F_{0.5}1.8\%1.1\%F_{0.5}F_{0.5}$ \\\midrule
\small{FCE DEV} & 60.7 & 75.1 & 35.1 & \textbf{61.2}\\
\small{FCE TEST} & \textbf{64.3} & 78.4 & 37.0 & 64.1\\
\small{CoNLL TEST1} & 34.3 & 44.7 & 20.5 & \textbf{36.1}\\
\small{CoNLL TEST2} & 44.0 & 63.8 & 20.8 & \textbf{45.1}\\\bottomrule
\end{tabular}}
\caption{Error detection results using auxiliary objectives, trained on additional data.}
\label{tab:moredata}
\end{table}

\section{Previous Work}


\noindent{\bf Error detection:} Early error detection systems were based on manually constructed error grammars and mal-rules (e.g., \citealt{foster2004parsing}). More recent approaches have exploited error-annotated learner corpora and primarily treated the task as a classification problem over vectors of contextual, lexical and syntactic features extracted from a fixed window around the target token. Most work has focused on error-type specific detection models, and in particular on models detecting preposition and article errors, which are among the most frequent ones in non-native English learner writing \cite{chodorow2007,de2008classifier,han2010using,tetreault2010using,han2006, tetreault2008ups,gamon2008using,gamon2010using,kochmar2014detecting,leacock2014automated}. 
Maximum entropy models along with rule-based filters account for a substantial proportion of utilized techniques. Error detection models have also been an integral component of essay scoring systems and writing instruction tools \cite{burstein2004automated,andersen2013developing,attali2006automated}.

The Helping Our Own (HOO) 2011 shared task on error detection and correction focused on a set of different errors \cite{dale2011helping}, though most systems were type specific and targeted closed-class errors. In the following year, the HOO 2012 shared task only focused on correcting preposition and determiner errors \cite{dale2012hoo}. 
The recent CoNLL shared tasks \cite{ng2013conll,Ng2014conll} focused on error correction rather than detection: CoNLL-13 targeted correcting noun number, verb form and subject-verb agreement errors, in addition to preposition and determiner errors made by non-native learners of English, whereas CoNLL-14 expanded to correction of all errors regardless of type. 
Core components of the top two systems across the CoNLL correction shared tasks include Average Perceptrons, L1 error correction priors in Naive Bayes models, and joint inference capturing interactions between errors (e.g., noun number and verb agreement errors) \cite{rozovskaya2014illinois}, as well as phrase-based statistical machine translation, under the hypothesis that incorrect source sentences can be ``translated'' to correct target sentences \cite{felice2014,grundkiewicz2014amu}.

The work that is most closely related to our own is the one by \newcite{Rei2016}, who investigate a number of compositional architectures for error detection, and propose a framework based on bidirectional LSTMs. 
In this work, we used their system architecture as a baseline, compared our model to their results in Sections \ref{sec:results} and \ref{sec:moredata}, and showed that multi-task learning can further improve performance and allow the model to generalise better.



\noindent{\bf Multi-task learning:} Multi-task learning was first proposed by \newcite{Caruana1998} and has since been applied to many language processing tasks and neural network architectures. 
For example, \newcite{Weston2008} constructed a convolutional architecture that shared some weights between tasks such as POS tagging, NER and chunking. 
Whereas their model only shared word embeddings, our approach focuses on learning better compositional features through a shared bidirectional LSTM.

\newcite{Luong2016} explored a multi-task architecture for sequence-to-sequence learning where encoders and decoders in different languages are trained jointly using the same semantic representation space. 
\newcite{Klerke2016} used eye tracking measurements as a secondary task in order to improve a model for sentence compression.
\newcite{Bingel2017} explored beneficial task relationships for training multitask models on different datasets. 
All of these architectures are trained by randomly switching between different tasks and updating parameters based on the corresponding dataset. In contrast, we treat alternative tasks as auxiliary objectives on the same dataset, which is beneficial for error detection  (Section \ref{sec:altstrat}).

There has been some research on using auxiliary training objectives in the context of other tasks. 
\newcite{Cheng2015a} described a system for detecting out-of-vocabulary names by also predicting the next word in the sequence. 
\newcite{Plank2016} predicted the frequency of each word together with the POS, and showed that this can improve tagging accuracy on low-frequency words.
However, we are the first to explore the auxiliary objectives described in Section \ref{sec:auxloss} in the context of error detection. 


\section{Conclusion}

We have described a method for integrating auxiliary loss functions with a neural sequence labeling framework, in order to improve error detection in learner writing. 
While predicting binary error labels, the model also learns to predict additional linguistic information for each token, allowing it to discover compositional features that can be exploited for error detection.
We performed a systematic comparison of possible auxiliary labels, which are either available in existing annotations or can be generated automatically. 
Our experiments showed that POS tags, grammatical relations and error types gave the largest benefit for error detection, and combining them together improved the results further.

We compared this training method to two other multi-task approaches: learning sequence labeling models on related tasks and using them to initialise the error detection model; and training on multiple tasks and datasets by randomly switching between them. Both of these methods were outperformed by our proposed approach using auxiliary labels on the same dataset -- the latter has the benefit of regularising the model with a different task, while also keeping the training data in-domain.

While the main benefits of multi-task learning are expected in scenarios where the available task-specific training data is limited, we found that error detection benefits from additional labels even with large training sets.
Successful error detection systems have to learn about language composition, and introducing an additional objective encourages the model to train more general composition functions and better word representations.
The error detection model, which also learns to predict automatically generated POS tags, achieved improved performance on both CoNLL-14 benchmarks.
A useful direction for future work would be to investigate 
dynamic weighting strategies for auxiliary objectives that allow the network to initially benefit from various available labels, and then specialise to performing the main task.


\bibliography{references}
\bibliographystyle{emnlp_natbib}

\end{document}
