\documentclass{article}
\usepackage{spconf,amsmath,graphicx}
\usepackage{tabu}
\usepackage{amssymb}
\usepackage{subfig}
\usepackage{hyperref}

\usepackage{multirow}

\def\x{{\mathbf x}}
\def\L{{\cal L}}

\title{Puzzle-CAM: Improved localization via matching partial and full features}



\twoauthors
 {Sanghyun Jo\sthanks{Thanks to GYNetworks for funding.}}
	{GYNetworks\\
	\tt\small josanghyeokn@gynetworks.com}
 {In-Jae Yu}
	{Korea Advanced Institute of Science and Technology \\
	School of Computing \\
	\tt\small myhome9830@kaist.ac.kr}
	
\begin{document}
\maketitle


\begin{abstract}








Weakly-supervised semantic segmentation (WSSS) is introduced to narrow the gap for semantic segmentation performance from pixel-level supervision to image-level supervision.
Most advanced approaches are based on class activation maps (CAMs) to generate pseudo-labels to train the segmentation network.
The main limitation of WSSS is that the process of generating pseudo-labels from CAMs that use an image classifier is mainly focused on the most discriminative parts of the objects.
To address this issue, we propose Puzzle-CAM, a process that minimizes differences between the features from separate patches and the whole image.
Our method consists of a puzzle module and two regularization terms to discover the most integrated region in an object.
Puzzle-CAM can activate the overall region of an object using image-level supervision without requiring extra parameters.
In experiments, Puzzle-CAM outperformed previous state-of-the-art methods using the same labels for supervision on the PASCAL VOC 2012 dataset.
Code associated with our experiments is available at \url{https://github.com/OFRIN/PuzzleCAM}.

\end{abstract}
\begin{keywords}
Image segmentation,  Deep learning, Neural Networks, Weakly-supervised semantic segmentation
\end{keywords}





\section{Introduction}
\label{sec:intro}



Semantic segmentation is a fundamental approach using convolutional neural networks (CNNs) with the aim of correctly predicting the pixel-wise classification of an image.
Recently, fully-supervised semantic segmentation (FSSS) has achieved remarkable progress \cite{chen2018encoder, long2015fully, chen2014semantic}.
However, producing large-scale training datasets with precise pixel-level annotations per image is considerably expensive and requires labor-intensive and time-consuming tasks.
To solve this issue, many researchers have focused on weakly supervised semantic segmentation (WSSS), which is used to train networks using image-level annotations, scribbles, bounding boxes, and points.
Image-level supervision can be more easily conducted than other approaches in a group of weak supervision processes.
In this study, we only focused on learning semantic segmentation models using image-level supervision.





\begin{figure}[t]
\centering
\subfloat[]{\includegraphics[width=0.25\linewidth]{images/Figure_1/fig_1_a.png}}  \hspace{2mm}
\subfloat[]{\includegraphics[width=0.25\linewidth]{images/Figure_1/fig_1_b.png}} \hspace{2mm}
\subfloat[]{\includegraphics[width=0.25\linewidth]{images/Figure_1/fig_1_c.png}}
\caption{
CAMs generated from tiled and original images:
(a) conventional CAMs from the original image, (b) generated CAMs from the tiled images, and (c) predicted CAMs by the proposed Puzzle-CAM.
}
\label{fig:intro}
\end{figure}


\begin{figure*}[t]\centering \includegraphics[width=\linewidth]{./images/Figure_2/arch.pdf}
\caption{
The overall architecture of the proposed Puzzle-CAM showing the integration of reconstructing regularization and the puzzle module.
}
\label{fig:full}
\end{figure*}

Most previous methods \cite{ahn2018learning, Wang_2020_CVPR, lee2019ficklenet} using WSSS are based on the class activation maps (CAMs) \cite{zhou2016learning} to achieve good performance.
However, the generated CAMs are usually focused on small parts of the semantic objects to efficiently classify them, which prevents the segmentation models from learning pixel-level semantic knowledge.
Moreover, we can see that the CAMs generated from isolated patches in the tiled image are different from those gained from the original image.
As shown in Fig. \ref{fig:intro}, CAMs of the tiled image comprising tiled patches are significantly inconsistent compared to those of the original image.
The differences are factored in by enlarging the supervision gap between FSSS and WSSS by even more.



The above observations gave us the inspiration to address WSSS issues by using an attention-based feature learning method.
Thus, we propose Puzzle-CAM for WSSS training to detect integrated regions of objects.
Our method applies reconstructing regularization that corresponds to the generated CAMs from the tiled and original images to provide self-supervision.
To improve the network prediction consistency further, we introduced a puzzle module that splits the image and merges CAMs generated from the tiled image.
Puzzle-CAM consists of a Siamese neural network with reconstructing regularization loss that reduces the differences between the original and merged CAMs.
Our experiments yielded both quantitative and qualitative results that demonstrate the superiority of our approach. 



Our main contributions are as follows:
\begin{itemize}
  \item We propose Puzzle-CAM that incorporates reconstructing regularization with a puzzle module, to effectively enhance the quality of CAMs without adding layers.
  \item Puzzle-CAM outperformed existing state-of-the-art methods with the same level of supervision on the PASCAL VOC 2012 dataset. 
\end{itemize}

\section{Related Work}
\label{sec:related}



\subsection{Attention Mechanisms Using CNNs}
\label{ssec:attention}
These provides fine-grained information on the features learned in CNNs.
Simonyan \textit{et al.}~\cite{simonyan2013deep} used the error back-propagation strategy to visualize semantic regions whereas the combined attention model used the global average pooling (GAP) layer in the CNNs to generate the CAMs \cite{zhou2016learning} more efficiently.
Last, a final classifier is used to generate attention maps.
To the best of our knowledge, which attention mechanism is chosen does not have a great effect on achieving high performance with WSSS, and so we based Puzzle-CAM on the combined attention model because it is more manageable than the other attention mechanism.

\subsection{Weakly Supervised Semantic Segmentation}
\label{ssec:wsss}
Unlike FSSS, which requires pixel-wise labels for an image, WSSS employs lower level labeling, such as bounding boxes \cite{khoreva2017simple}, scribbles \cite{lin2016scribblesup}, and image-level classification labels \cite{ahn2018learning, lee2019ficklenet}.
Recently, the performance of WSSS has been significantly boosted by incorporating the CAMs.
Most previous WSSS methods refine the CAMs generated by the image classifier to approximate the segmentation mask \cite{ahn2018learning, ahn2019weakly, huang2018weakly, hou2018self, lee2019ficklenet}.
AffinityNet \cite{ahn2018learning} trains an additional network to learn similarities between the pixels, which often generates a transition matrix and multiplies with CAM to adjust its activation coverage. 
IRNet \cite{ahn2019weakly} generates a transition matrix from the boundary activation map and extends the method to achieve weakly supervised instance segmentation and WSSS. 
SEAM \cite{Wang_2020_CVPR} aims to refine CAMs using a pixel correlation module that captures context appearance information for each pixel and alters the original CAMs by using learned affinity attention maps.


\section{Methodology}
\label{sec:methodology}



\subsection{Motivation}
\label{ssec:motivation}



Most WSSS approaches are based on CAMs to obtain a segmentation mask using image-level supervision. 
Usually, the CAMs are focused on the discriminative region of the object. The well-known reason is that a normal image classifier only uses the classification loss, which induces a partial region that activated during training.
The CAMs generated from a tiled image causes over-activation since the image does not have global-context information.
To address this issue, we propose Puzzle-CAM that improves the network for consistent prediction to match partial and full features.
Puzzle-CAM contains designed loss functions to match the CAMs generated from a tiled image with the original image (Fig. 2).

\subsection{The Employed CAM Method}
\label{ssec:cam}
We first introduce the CAM method for producing the initial attention map.
Given feature extractor  and classifier , we can generate CAMs  that are the collection of CAMs for all of the classes.
After training the classifier by image-level supervision, we apply the weights of the -channel classifier as  on feature map  from input image  to obtain the CAM of class  as follows: 



The generated CAM is normalized by using the maximum value of .
Finally, we obtain the CAMs for all of the classes () by concatenating . 

\subsection{The Puzzle Module}
\label{ssec:module}
When matching partial and full features, the key is to narrow the gap between FSSS and WSSS.
The puzzle module consists of tiling and merging modules.
From input image  of size , the tiling module generates non-overlapping a tiled patches  of size .
Next, we generate  CAMs for each .
Finally, the merging module attaches all  into a single CAMs  that has the same shape as  which is the CAMs of .


















\subsection{The Loss Design for Puzzle-CAM}
\label{ssec:loss}

We employed a GAP layer at the end of the network to incorporate prediction vector  for image classification and to adopt multi-label soft margin loss for the classification task.
For notational convenience, we define  as







The CAMs of the original () and tiled () images are converted using the GAP layer with prediction vectors  and , respectively. 
The classification losses for the original and reconstructed images are respectively calculated as



These two classification losses are used to improve the performance of the image classification. 
To reinforce the CAMs from the original image, we added reconstructing regularization to correspond with the original and reconstructed CAMs. 
The reconstruction loss for the original CAM can be easily defined as



In summary, the final loss of Puzzle-CAM can be written as



where  is the balance of the weights for the different losses.
The classification losses,  and , are used to roughly estimate the region of the object. 
The reconstruction loss, , is used to narrow the gaps between the pixel- and image-level supervision processes. 
We report details of the network training settings and probe into the effectiveness of the proposed module in the experiments section.

\section{Experimental Results}
\label{sec:experiments}
\begin{table}[t]
\caption{
Ablation study of the Puzzle-CAM loss functions using ResNet-50 as the backbone.
}
\footnotesize
\centering
{
\begin{tabu} to \linewidth{X[c,0.8] X[c,0.8] X[c,0.8] | X[c,1.2] } \hline \hline
  &       &  & mIoU (\%)        \\ \hline
\checkmark & & & 47.82  \\
\checkmark &\checkmark & & 47.70   \\ 
\checkmark & & \checkmark & 49.21  \\  
\checkmark & \checkmark & \checkmark & \textbf{51.53}   \\ \hline\hline

\end{tabu}
}
\label{tb:ablation_for_loss}
\end{table}

\subsection{Implementation Details}
\label{ssec:implementation}





We evaluated our method using the PASCAL VOC 2012 dataset \cite{everingham2010pascal}.
The dataset was separated into 1,464 images for training, 1,449 for validation, and 1,456 for testing. 
Following the experimental protocol used in previous methods \cite{ahn2018learning, Wang_2020_CVPR, lee2019ficklenet}, we took additional annotations from the Semantic Boundary Dataset \cite{hariharan2011semantic} to build an augmented training set with 10,582 images. 
The images were randomly re-scaled in the range of [320, 640] and then cropped to  as the network inputs. 
For all experiments, we set  as the maximum and linearly ramped up  to its maximum value by half epochs.
During inference, we utilized classifiers without the puzzle module.
Thus, we adopted multi-scale and horizontal flip to generate pseudo-segmentation labels.
We used four TITAN-RTX GPUs for the model to train the dataset.






\subsection{Ablation Studies}
\label{ssec:ablation}

We conducted ablation studies on the main components of Puzzle-CAM by applying the mean Intersection-Over-Union (mIoU) metric (Table~\ref{tb:ablation_for_loss}), for which the baseline was . 
With the proposed reconstructing regularization () of the  tiled patches, the baseline was boosted to , while the proposed classification loss from the tiled patches () was similar to the baseline. 
Both  and  consistently improved the baseline by a . 



We visualized the CAMs by using combinations of loss functions for each of them (see Eq. \ref{fig:ablation}).
When the classification losses () only are employed, the result will show no marginal differences.
Meanwhile, when the reconstruction loss () only is employed, the result will show improved localization ability for some classes compared to the original but the method will fail to predict several classes.
When both sets of losses are combined, the result will show improved localization without suffering classification losses.








\begin{figure}[t]
\centering
\subfloat[]{\includegraphics[width=0.45\linewidth,height=2.5cm]{images/Figure_3/ablation_part1.pdf}} \hspace{2mm}
\subfloat[]{\includegraphics[width=0.45\linewidth,height=2.5cm]{images/Figure_3/ablation_part2.pdf}}\\ \subfloat[]{\includegraphics[width=0.45\linewidth,height=2.5cm]{images/Figure_3/ablation_part3.pdf}} \hspace{2mm}
\subfloat[]{\includegraphics[width=0.45\linewidth,height=2.5cm]{images/Figure_3/ablation_part4.pdf}}
\caption{
    Visualization of the prediction tags and CAMs by using combinations of loss functions. 
    In (d), the final CAMs not only suppressed over-activation but also expanded the CAMs into complete object activation coverage . 
}
\label{fig:ablation}
\vspace{-2mm}
\end{figure}

\begin{table}[t]
\caption{
Quality of the pseudo semantic segmentation labels in mIoU, evaluated on the PASCAL VOC 2012 training set \cite{everingham2010pascal}. 
RW, random walk with AffinityNet \cite{ahn2018learning}; dCRF, dense conditional random field \cite{krahenbuhl2011efficient}.
}
\footnotesize
\centering
{
\begin{tabu} to \linewidth{X[c,1.6] | X[c,1.5] | X[c,0.5] | X[c,1.0] | X[c,1.3] }  \hline \hline
\multirow{2}{*}{Method} & \multirow{2}{*}{Backbone} & CAM & CAM & CAM+RW \\ 
& & (\%) & +RW (\%) & +dCRF (\%) \\ \hline
AffinityNet \cite{ahn2018learning} & ResNet-50 & 47.82 & 58.10 & 59.70 \\
Puzzle-CAM & ResNet-50 & 51.53 & 64.16  & 64.70 \\ 
Puzzle-CAM & ResNeSt-50 & 57.59 & 69.48  & 69.91 \\ 
Puzzle-CAM & ResNeSt-101 & 61.85 & 71.92 & 72.46 \\ 
Puzzle-CAM & ResNeSt-269 & \textbf{62.45} & \textbf{74.14} & \textbf{74.67} \\ \hline\hline
\end{tabu}
}
\vspace{-4mm}
\label{tb:compare_cam_with_rw}
\end{table}

\begin{table}[t]
\caption{
Comparison of Puzzle-CAM and existing state-of-the-art methods on the PASCAL VOC 2012  and  datasets. 
, image-level labels;  , external saliency models.
}
\footnotesize
\centering
{
\begin{tabu} to \linewidth{X[c,1.2] X[c,1.0] X[c,0.75]  | X[c,0.2]  X[c,0.2] }  \hline \hline
Method  & Backbone & Supervision & val & test \\ \hline
AffinityNet~\cite{ahn2018learning} & Wide-ResNet-38 &  & 61.7 & 63.7 \\
DSRG~\cite{huang2018weakly} & ResNet-101 &  +  & 61.4 & 63.2 \\ 
SeeNet~\cite{hou2018self} & ResNet-101 &  +  & 63.1 & 62.8 \\
IRNet~\cite{ahn2018learning} & ResNet-50 &  & 63.5 & 64.8 \\
FickleNet~\cite{lee2019ficklenet} & ResNet-101 &  +  & 64.9 & 65.3 \\ 
ICD~\cite{fan2020learning} & ResNet-101 &  & 64.1 & 64.3 \\ 
SEAM~\cite{Wang_2020_CVPR} & Wide-ResNet-38 &  & 64.5 & 65.7 \\ \hline
Ours (Puzzle-CAM) & ResNeSt-101 &  & 66.9 & 67.7 \\
Ours (Puzzle-CAM) & ResNeSt-269 &  & \textbf{71.9} & \textbf{72.2} \\

\hline\hline
\end{tabu}
}
\label{tb:final}
\end{table}

\begin{figure}
\begin{minipage}[b]{1.0\linewidth}
  \centering
  \centerline{\includegraphics[width=1.0\linewidth]{images/Figure_4/image.pdf}}
\end{minipage}

\begin{minipage}[b]{1.0\linewidth}
  \centering
  \centerline{\includegraphics[width=1.0\linewidth]{images/Figure_4/gt.pdf}}
\end{minipage}

\begin{minipage}[b]{1.0\linewidth}
  \centering
  \centerline{\includegraphics[width=1.0\linewidth]{images/Figure_4/pred.pdf}}
\end{minipage}

\caption{Qualitative segmentation results on the PASCAL VOC 2012 \textit{val} set. Top: original images. Middle: ground truth. Bottom: prediction of the segmentation model trained using the pseudo-labels from Puzzle-CAM.}
\label{fig:result}
\end{figure}

\subsection{Comparison with Existing State-of-the-Art Methods}
\label{ssec:comparisons}

To further improve the accuracy of pseudo pixel-level annotations, we followed the approach in \cite{ahn2018learning} to train AffinityNet based on Puzzle-CAM. 
We adopted ResNeSt architecture that universally improves the learned feature representations to boost performance across image classification, object detection, instance segmentation and semantic segmentation. 
In Table~\ref{tb:compare_cam_with_rw}, we report the performances with the original CAMs used by the baseline AffinityNet \cite{ahn2018learning} and Puzzle-CAM.

The final synthesized pseudo-labels achieved  mIoU on the PASCAL VOC 2012  set.
Puzzle-CAM was then used to train the segmentation model DeepLabv3+ \cite{chen2018encoder} with the ResNeSt-269 \cite{zhang2020resnest} backbone using the pseudo-labels in full supervision mode to achieve the final segmentation results. 
Table \ref{tb:final} reports a comparison of the mIoU values for the proposed method and the previous approaches. 
Compared to the baseline methods, Puzzle-CAM had remarkably improved performances on both the  and  sets with the same settings for training.
Fig. \ref{fig:result} shows some qualitative results on the  set that illustrate that the proposed method worked well on both large and small objects.








\section{Conclusions}
\label{sec:conclusion}

In this paper, we proposed the Puzzle-CAM algorithm to narrow the supervision gap between FSSS and WSSS using image-level labels. 
To improve the network for generating consistent CAMs, we designed a puzzle module and adopted reconstructing regularization to match partial and full features.
Not only did Puzzle-CAM consistently generate features from local tiled patches but it also fitted the shape of the ground truth masks better. 
The segmentation network trained by our synthesized pixel-level pseudo-labels achieved state-of-the-art performance on the PASCAL VOC 2012 dataset, which proves the effectiveness of our approach. 
We believe that the concepts of Puzzle-CAM as a training module can be generalized and will benefit other weakly- and semi-supervised tasks, such as semantic and instance segmentation.



\bibliographystyle{IEEEbib}
\begin{thebibliography}{10}

\bibitem{chen2018encoder}
Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig
  Adam,
\newblock ``Encoder-decoder with atrous separable convolution for semantic
  image segmentation,''
\newblock in {\em Proceedings of the European conference on computer vision
  (ECCV)}, 2018, pp. 801--818.

\bibitem{long2015fully}
Jonathan Long, Evan Shelhamer, and Trevor Darrell,
\newblock ``Fully convolutional networks for semantic segmentation,''
\newblock in {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, 2015, pp. 3431--3440.

\bibitem{chen2014semantic}
Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan~L
  Yuille,
\newblock ``Semantic image segmentation with deep convolutional nets and fully
  connected crfs,''
\newblock {\em arXiv preprint arXiv:1412.7062}, 2014.

\bibitem{ahn2018learning}
Jiwoon Ahn and Suha Kwak,
\newblock ``Learning pixel-level semantic affinity with image-level supervision
  for weakly supervised semantic segmentation,''
\newblock in {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, 2018, pp. 4981--4990.

\bibitem{Wang_2020_CVPR}
Yude Wang, Jie Zhang, Meina Kan, Shiguang Shan, and Xilin Chen,
\newblock ``Self-supervised equivariant attention mechanism for weakly
  supervised semantic segmentation,''
\newblock in {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR)}, June 2020.

\bibitem{lee2019ficklenet}
Jungbeom Lee, Eunji Kim, Sungmin Lee, Jangho Lee, and Sungroh Yoon,
\newblock ``Ficklenet: Weakly and semi-supervised semantic image segmentation
  using stochastic inference,''
\newblock in {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, 2019, pp. 5267--5276.

\bibitem{zhou2016learning}
Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba,
\newblock ``Learning deep features for discriminative localization,''
\newblock in {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, 2016, pp. 2921--2929.

\bibitem{simonyan2013deep}
Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman,
\newblock ``Deep inside convolutional networks: Visualising image
  classification models and saliency maps,''
\newblock {\em arXiv preprint arXiv:1312.6034}, 2013.

\bibitem{khoreva2017simple}
Anna Khoreva, Rodrigo Benenson, Jan Hosang, Matthias Hein, and Bernt Schiele,
\newblock ``Simple does it: Weakly supervised instance and semantic
  segmentation,''
\newblock in {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, 2017, pp. 876--885.

\bibitem{lin2016scribblesup}
Di~Lin, Jifeng Dai, Jiaya Jia, Kaiming He, and Jian Sun,
\newblock ``Scribblesup: Scribble-supervised convolutional networks for
  semantic segmentation,''
\newblock in {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, 2016, pp. 3159--3167.

\bibitem{ahn2019weakly}
Jiwoon Ahn, Sunghyun Cho, and Suha Kwak,
\newblock ``Weakly supervised learning of instance segmentation with
  inter-pixel relations,''
\newblock in {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, 2019, pp. 2209--2218.

\bibitem{huang2018weakly}
Zilong Huang, Xinggang Wang, Jiasi Wang, Wenyu Liu, and Jingdong Wang,
\newblock ``Weakly-supervised semantic segmentation network with deep seeded
  region growing,''
\newblock in {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, 2018, pp. 7014--7023.

\bibitem{hou2018self}
Qibin Hou, PengTao Jiang, Yunchao Wei, and Ming-Ming Cheng,
\newblock ``Self-erasing network for integral object attention,''
\newblock in {\em Advances in Neural Information Processing Systems}, 2018, pp.
  549--559.

\bibitem{everingham2010pascal}
Mark Everingham, Luc Van~Gool, Christopher~KI Williams, John Winn, and Andrew
  Zisserman,
\newblock ``The pascal visual object classes (voc) challenge,''
\newblock {\em International journal of computer vision}, vol. 88, no. 2, pp.
  303--338, 2010.

\bibitem{hariharan2011semantic}
Bharath Hariharan, Pablo Arbel{\'a}ez, Lubomir Bourdev, Subhransu Maji, and
  Jitendra Malik,
\newblock ``Semantic contours from inverse detectors,''
\newblock in {\em 2011 International Conference on Computer Vision}. IEEE,
  2011, pp. 991--998.

\bibitem{krahenbuhl2011efficient}
Philipp Kr{\"a}henb{\"u}hl and Vladlen Koltun,
\newblock ``Efficient inference in fully connected crfs with gaussian edge
  potentials,''
\newblock {\em Advances in neural information processing systems}, vol. 24, pp.
  109--117, 2011.

\bibitem{fan2020learning}
Junsong Fan, Zhaoxiang Zhang, Chunfeng Song, and Tieniu Tan,
\newblock ``Learning integral objects with intra-class discriminator for
  weakly-supervised semantic segmentation,''
\newblock in {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, 2020, pp. 4283--4292.

\bibitem{zhang2020resnest}
Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi~Zhu, Zhi Zhang, Haibin Lin, Yue
  Sun, Tong He, Jonas Mueller, R~Manmatha, et~al.,
\newblock ``Resnest: Split-attention networks,''
\newblock {\em arXiv preprint arXiv:2004.08955}, 2020.

\end{thebibliography}

\end{document}
