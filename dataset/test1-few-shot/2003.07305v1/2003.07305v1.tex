
\documentclass[jmlr]{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} \usepackage{multicol}
\usepackage[dvipsnames]{xcolor}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage[export]{adjustbox}
\usepackage{jmlr2e}
\usepackage[numbers]{natbib}

\usepackage{xcolor}
\usepackage[bottom=0.85in,top=0.85in,left=0.85in,right=0.85in,footskip=.25in]{geometry}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\small,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{natbib}
\usepackage{enumerate}
\usepackage{tikz}
\usepackage{graphicx}

\newcommand{\cmtt}[1]{{\textcolor{blue}{#1}}}
\newcommand{\ak}[1]{{\textcolor{red}{#1}}}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}\DeclarePairedDelimiter\norm{\lVert}{\rVert}\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\newcommand{\TODO}[1]{\textcolor{red}{TODO: #1}}
\newcommand{\textdiff}[1]{\textcolor{red}{#1}}
\newcommand{\citemissing}{\textcolor{red}{(cite?)}}

\newcommand{\normt}[1]{\left\lVert#1\right\rVert_2}
\newcommand{\normtmu}[1]{\left\lVert#1\right\rVert_{2, \mu}}
\newcommand{\normm}[1]{\left\lVert#1\right\rVert}
\newcommand{\norminf}[1]{\left\lVert#1\right\rVert_\infty}
\newcommand{\normtt}[1]{\left\lVert#1\right\rVert^2_2}

\newcommand{\half}{\frac{1}{2}}
\newcommand{\fourth}{\frac{1}{4}}
\newcommand{\vect}[1]{\overrightafrrow{\textbf{#1}}}
\newcommand{\phat}{\hat{p}}
\newcommand{\KL}[2]{D_{KL}(#1||#2)}
\newcommand{\TV}[2]{D_{TV}(#1||#2)}
\newcommand{\ind}[1]{1[#1]}
\newcommand{\pardiv}[1]{\frac{\partial}{\partial #1}}
\newcommand{\parHess}[1]{\frac{\partial^2}{\partial #1 ^2}}
\newcommand{\Xhat}{{\hat{X}}}
\newcommand{\xhat}{{\hat{x}}}
\newcommand{\defeq}{\mathrel{\stackrel{\makebox[0pt]{\mbox{\normalfont\tiny def}}}{=}}}

\newcommand{\argmax}[1]{\underset{#1}{\textrm{argmax}}\ }
\newcommand{\argmin}[1]{\underset{#1}{\textrm{argmin}}\ }
\newcommand{\grad}[1]{\nabla{#1}}
\newcommand{\innerp}[2]{\langle{#1,#2}\rangle}
\newcommand{\Hess}[1]{\nabla^2{#1}}
\newcommand{\EXP}[1]{\text{exp}\{#1\}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\s}{\mathbf{s}}

\newcommand{\Proj}{\Pi}
\newcommand{\Projmu}{\Pi_\mu}
\newcommand{\trans}{T}
\newcommand{\backup}{\mathcal{T}}
\newcommand{\Qclass}{\mathcal{Q}}
\newcommand{\ReplayBuffer}{\mathcal{B}}
\newcommand{\ltwonorm}{L_2}
\newcommand{\lpnorm}{L_p}
\newcommand{\linfnorm}{L_\infty}

\newcommand{\UniformVec}{U[-1,1]^{32}}
\newcommand{\valuefeedback}{\mathrm{V_{CF}}}
\newcommand{\targetQ}{Q_\bar{\theta}}
\newcommand{\pk}{p_{k}}
\newcommand{\pkplus}{p_{k+1}}
\newcommand{\ponet}{p_{1:T}}
\newcommand{\bellmanopt}{\mathcal{B}^*}
\newcommand{\expec}{\mathbb{E}}
\newcommand{\Deltatilde}{\tilde{\Delta}}
\newcommand{\Regrett}{\mathrm{Regret(T)}}
\newcommand{\Regretk}{\mathrm{Regret(k)}}
\newcommand{\entropy}{\mathcal{H}}
\newcommand{\kldiv}{\mathrm{D_{KL}}}
\newcommand{\tvd}{\mathrm{D_{TV}}}
\newcommand{\diag}{\mathrm{Diag}}
\newcommand{\projectedbellman}{\mathcal{H}}
 \usepackage{semicrunch}
\usepackage{wrapfig,lipsum,booktabs}

\newcommand{\theHalgorithm}{\arabic{algorithm}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{assumption}{Assumption}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}{Lemma}[theorem]
\newtheorem{definition1}{Definition}[section]

\usepackage{sidecap}
\usepackage{minitoc}

 
\usepackage{tikz}
\usetikzlibrary{arrows}

\tikzset{
  treenode/.style = {align=center, inner sep=0pt, text centered,
    font=\sffamily},
  arn_n/.style = {treenode, circle, white, font=\sffamily\bfseries, draw=black,
    fill=black, text width=1.5em},arn_r/.style = {treenode, circle, grey, draw=grey, 
    text width=1.5em, very thick},arn_x/.style = {treenode, rectangle, draw=black,
    minimum width=0.8em, minimum height=0.8em}}



\firstpageno{1}


\author{\name Aviral Kumar\thanks{Corresponding Author; Preprint. Under Review.} \email {aviralk@berkeley.edu}\\
\name Abhishek Gupta \email {abhigupta@berkeley.edu} \\
\name Sergey Levine \email {svlevine@eecs.berkeley.edu} \\
\addr Electrial Engineering and Computer Sciences, University of California, Berkeley}



\begin{document}
\setcitestyle{square}

\title{\textbf{DisCor: Corrective Feedback in Reinforcement Learning via Distribution Correction}}
\editor{}
\maketitle
















\begin{abstract}
Deep reinforcement learning can learn effective policies for a wide range of tasks, but is notoriously difficult to use due to instability and sensitivity to hyperparameters. The reasons for this remain unclear. When using standard supervised methods (e.g., for bandits), on-policy data collection provides ``hard negatives'' that correct the model in precisely those states and actions that the policy is likely to visit. We call this phenomenon ``corrective feedback.'' We show that bootstrapping-based Q-learning algorithms do not necessarily benefit from this corrective feedback, and training on the experience collected by the algorithm is not sufficient to correct errors in the Q-function.
In fact, Q-learning and related methods can exhibit pathological interactions between the distribution of experience collected by the agent and the policy induced by training on that experience, leading to potential instability, sub-optimal convergence, and poor results when learning from noisy, sparse or delayed rewards. We demonstrate the existence of this problem, both theoretically and empirically. We then show that a specific correction to the data distribution can mitigate this issue. Based on these observations, we propose a new algorithm, DisCor, which computes an approximation to this optimal distribution and uses it to re-weight the transitions used for training, resulting in substantial improvements in a range of challenging RL settings, such as multi-task learning and learning from noisy reward signals. \textbf{Blog post presenting a summary of this work is available at:} \url{https://bair.berkeley.edu/blog/2020/03/16/discor/}.
\end{abstract}

\section{Introduction}

Reinforcement learning (RL) algorithms, when combined with high-capacity deep neural net function approximators, have shown promise in domains ranging from robotic manipulation~\citep{kalashnikov18} to recommender systems~\citep{shani2005recommender}. However, current deep RL methods can be difficult to use, due to sensitivity with respect to hyperparameters and inconsistent and unstable convergence. While a number of hypotheses have been proposed to understand these issues~\citep{hassalt10doubleq,Hasselt2018DeepRL,pmlr-v80-fujimoto18a,fu19diagnosing}, and gradual improvements have led to more powerful algorithms in recent years~\citep{Haarnoja18,rainbow}, an effective solution has proven elusive. We hypothesize that a major source of instability in reinforcement learning with function approximation and value function estimation, such as Q-learning~\citep{Watkins92,Riedmiller2005,Mnih2015} and actor-critic algorithms~\citep{Haarnoja2017,konda_ac}, is a pathological interaction between the data distribution induced by the latest policy, and the errors induced in the learned approximate value function as a consequence of training on this distribution, which then exacerbates the issues for the data distribution at the next iteration.


While a number of prior works have provided a theoretical examination of various approximation dynamic programming (ADP) methods -- which encompasses Q-learning and actor-critic algorithms --
prior work has not extensively studied the relationship between the data distribution induced by the latest value function, and the \textit{errors} in the future value functions obtained by training on this data.

When using supervised learning to train a model, as in the case of contextual bandits or model-based RL, using the resulting model to select the most optimal actions results in a kind of ``hard negative'' mining: the model collects precisely those transitions that lead to good outcomes according to the model (potentially erroneously). This results in collecting precisely the data needed to fix those errors and improve. We refer to this as ``corrective feedback.''
We argue that ADP algorithms (e.g., Q-learning and actor-critic), which use bootstrapped targets rather than ground-truth labels, often do not enjoy this sort of corrective feedback. Since they regress onto bootstrapped estimates of the current value function, rather than the true optimal value function (which is unknown), simply visiting states with high error and updating the value function at those states does not necessarily correct those errors, since errors in the target value might be due to upstream errors in other states that are visited less often. This absence of corrective feedback can result in severe detrimental consequences on performance.



We demonstrate that na\"ively training a value function on transitions collected either by the latest policy or a mixture of recent policies (i.e., with replay buffers) may not result in corrective feedback. In fact, in some cases, it can actually lead to increasing accumulation of errors, which can lead to poor performance even on simple tabular MDPs.
We then show how to address this issue by re-weighting the data buffer using a distribution that explicitly optimizes for corrective feedback, which gives rise to our proposed algorithm, \textbf{DisCor}. 


The two main contributions of our work consist of an analysis showing that ADP methods may not benefit from corrective feedback, even with online data collection, as well as a proposed solution to this problem based on estimating target value error and resampling the replay buffer to mitigate error accumulation.
Our method, DisCor, is general and can be used in conjunction with most modern ADP-based deep RL algorithms, such as DQN~\citep{Mnih2015} and SAC~\citep{Haarnoja18}. Our experiments show that DisCor substantially improves performance of standard RL methods, especially in challenging settings, such as multi-task RL and learning from noisy rewards. We evaluate our approach on both continuous control tasks and discrete-action Atari games.
On the multi-task MT10 benchmark~\citep{yu2019meta} and several robotic manipulation tasks, our method learns policies with a final success rate that is 50\% higher than that of SAC.

 \section{Preliminaries}
\label{sec:backrgound}
{The goal in reinforcement learning is to learn a policy that maximizes the expected cumulative discounted reward in a Markov decision process (MDP), which is defined by a tuple} .  represent state and action spaces,  and  represent the dynamics and reward function, and  represents the discount factor.  is the initial state distribution. The infinite-horizon, {discounted marginal state distribution of the policy  is denoted as  and the  corresponding state-action marginal is .}

Approximate dynamic programming (ADP) algorithms, such as Q-learning and actor-critic methods, aim to acquire the optimal policy by modeling the optimal state () and state-action () value functions. These algorithms are based on recursively iterating the Bellman optimality operator, , defined as

The goal is to converge to the optimal value function, , by applying successive Bellman projections.
With function approximation, these algorithms project the values of the Bellman optimality operator  onto a family of Q-function approximators  (e.g., deep neural nets) under a sampling or data distribution , such that  and

We refer to  as the \textit{target value} for the projection step. 
Q-function fitting is usually interleaved with additional data collection, which typically uses a policy derived from the latest value function, augmented with either -greedy~\citep{Mnih2015} or Boltzmann-style~\citep{Haarnoja18} exploration.
To simplify analysis, we mainly consider an underlying RL algorithm (Appendix~\ref{sec:omitted_proofs}, Algorithm~\ref{alg:fqi}) that alternates between fitting the action-value function, , fully with the current data by minimizing Bellman error, and then collecting data with the policy derived from this value function. This corresponds to fitted Q-iteration.

For commonly used ADP methods,  simply corresponds to the on-policy state-action marginal,  (at iteration ) or else a ``replay buffer''~\citep{Haarnoja18,Mnih2015,Lillicrap2015} formed as a mixture distribution over all past policies, such that .
However, as we will show in this paper, the choice of the sampling distribution  is of crucial importance for the stability and efficiency of ADP algorithms, and that many commonly-used choices of this distribution can lead to convergence to sub-optimal solutions, even with online data collection. We analyze this issue in Section~\ref{sec:problem_description},
and then discuss a potential solution to this problem in Section~\ref{sec:method_description}.

\paragraph{Experiment setup for analysis.} For the purposes of the analysis of corrective feedback in Section~\ref{sec:problem_description}, we use tabular MDPs from \cite{fu19diagnosing}, which provide the ability to measure oracle quantities, such as the error against , the ground truth optimal Q-function. We remove other sources of error, such as sampling error, by providing all transitions in the MDP to the ADP algorithm. 
We simulate different data distributions by re-weighting these transitions. We use a two hidden layer feed-forward network to represent the Q-function. More details on this setup are provided in Appendix~\ref{sec:app_exps_gridworld}. \section{Corrective Feedback in Q-Learning}
\label{sec:problem_description}





 





In this paper, we study how the policies induced by value functions learned via ADP result in data distributions that can \emph{fail} to correct systematic errors in those value functions, in contrast to non-bootstrapped methods (e.g., supervised learning of models).
That is, ADP methods lack ``corrective feedback.'' To define corrective feedback intuitively and formally, we start with a contextual bandit example, where the goal is to learn the optimal state-action value function  which, for a bandit, is equal to the reward  for performing action  in state . At iteration , the algorithm minimizes the estimation error of the Q-function: \mbox{}. Using a greedy or Boltzmann policy  to collect data for training  leads the agent to choose actions  with over-optimistic  values at a state , and observe the corresponding true  values as a result. Minimizing this estimation error then leads to the errors being \emph{corrected}, as  is pushed closer to match the true  for actions  with incorrectly high Q-values. This constructive interaction between online data collection and error correction is what we refer to as \textbf{corrective feedback}.


In contrast, as we will observe shortly, ADP methods that employ bootstrapped target values do not necessarily benefit from such corrective feedback, and therefore can converge to suboptimal solutions. Because value functions are trained with target values computed by applying the Bellman backup on the previous \emph{learned} Q-function, rather than the true optimal , errors in the previous Q-function at the states that serve as backup targets can result in incorrect Q-value targets at the current state. In this case, no matter how often the current transition is observed, the error at this transition is not corrected. 
Since ADP algorithms typically use data collected using past Q-functions for training, thus coupling the data distribution to the learned value function, this issue can make them unable to correct target value errors.

As shown in Figure~\ref{fig:visitation_doesnt_correct_eror} (experiment setup described at the end of Section~\ref{sec:backrgound}), state visitation frequencies of the latest policy in an ADP algorithm can often correlate \emph{positively} with increasing error, suggesting that visiting a state can actually \textit{increase} error at that state, in contrast to supervised learning in bandit problems, where this correlation is either negative (i.e., the error decreases) or 0.

\begin{figure}[!h]
\centering
\begin{subfigure}{.47\columnwidth}
  \includegraphics[width=0.7\textwidth,right]{images/problem_description/fig1a_new_again.pdf}
\end{subfigure}~
\begin{subfigure}{.47\columnwidth}
  \includegraphics[width=0.7\textwidth,left]{images/problem_description/fig1b_new_again.pdf}
\end{subfigure}
\caption{\footnotesize{\textbf{Left:} Return attained by with direct supervised learning of values in a bandit problem (``Bandit'') and with bootstrapped targets (``ADP'') on a simple tabular MDP. \textbf{Right:} Cosine similarity between per-iteration error increase  and the policy's state visitation frequency  for the two training runs. The ADP run performs poorly, and exhibits a \textit{positive} cosine similarity for prolonged periods during training, suggesting that the policy visitation at least at some states correlates with an \textbf{increase} in error. With ground truth targets (``Bandit''), this quantity is negative until convergence (around iteration 25), and then fluctuates around , suggesting that this approach does benefit from corrective feedback.}}
\vspace{-15pt}
\label{fig:visitation_doesnt_correct_eror}
\end{figure}

Thus, na\"ively coupling the choice of data distribution  and the  function being optimized, by sampling uniformly from a replay buffer collected by policies corresponding to prior Q-functions~\cite{Mnih2015}, or even just sampling from data collected by the latest policy, can cause an absence of corrective feedback. As we will show in Section~\ref{sec:consequences}, this can lead to several detrimental effects such as sub-optimal convergence, instability, and slow learning progress. To theoretically analyze the lack of corrective feedback, we first define error against the optimal value function as follows:



\begin{definition}
\label{eqn:value_of_feedback}
The value error is defined as the expected absolute error to the optimal Q-function  weighted under the corresponding on-policy () state-action marginal, :

\end{definition}




A smooth decrease in the value error  indicates that the algorithm is effectively correcting errors in the Q-function. If  fluctuates or increases, the algorithm may be making poor learning progress. When the value error  is roughly stagnant at a non-zero value, this may indicate premature, sub-optimal convergence, provided that the function approximator class is able to support lower-error solutions (which is typically the case for large deep networks).
Thus, having the learning process monotonically and quickly bring the value error  down to  is desirable for effective learning.
By means of a simple didactic example, we now discuss some reasons why corrective feedback may be absent in ADP methods. We will then describe several examples that illustrate the negative consequences of absent corrective feedback on learning progress.

\subsection{A Didactic Example and Theoretical Analysis of an Absence of Corrective Feedback}
\label{sec:didactic_example_and_theory}

In this section, we first present a diadctic example which provides intuition for why corrective feedback may be absent in RL, and then we generalize this intuition to a more formal result in Section~\ref{sec:theoretical_analysis}.

\subsubsection{Didactic Example}
\label{sec:didactice_example}
{We first describe our didactic example which is a tree-structured deterministic MDP (Figures \ref{fig:on_policy_example} and \ref{fig:discor_sample}) with 7 states and 2 actions,  and , at each state. The MDP has deterministic transitions, where action  transitions to a node's left child, and  transitions to the right child. At each leaf node, the episode terminates and the agent receives a reward.}

{We illustrate the learning progress of Q-learning (Algorithm~\ref{alg:fqi}) on this tree-MDP in Figure~\ref{fig:on_policy_example} and the learning progress of a method with an optimal distribution in Figure~\ref{fig:discor_sample}. In the on-policy setting, Q-values at states are updated according to the visitation frequency of these states under the current policy.
Since the leaf nodes are the least likely in this distribution, the Bellman backup is slow to correct errors at the leaves. Using these incorrect leaf Q-values as target values for nodes higher in the tree then gives rise to incorrect
values, even if Bellman error is fully minimized for the sampled transitions. Thus, most of the Bellman updates do not actually bring the updated Q-values closer to .
If we carefully order the states, such that the lowest level nodes are updated first before proceeding upwards in the tree, as shown in Figure \ref{fig:discor_sample}, the errors are much lower. We will show in Section~\ref{sec:method_description} that our proposed approach in this paper aims at performing updates in the manner shown in Figure~\ref{fig:discor_sample} by re-weighting transitions collected by on-policy rollouts via importance sampling.}

\begin{figure*}
\centering
\includegraphics[width=0.9\linewidth]{images/on_policy_figure_aliasing_pdf.pdf}
    \caption{\footnotesize{Iterations of Q-learning on a tree-structured MDP. Trajectories are sampled using the current policy, with each trial (shown as dotted borders around states) starting from the root. Function approximation leads to aliasing of box-shaped nodes and circle-shaped nodes, such that updates to one circle-shaped node affect all other circles, and likewise for box-shaped nodes. Due to the training distribution and aliasing, this method often backs up incorrect target values. Due to aliasing, previously correct values at other states may become incorrect due to these erroneous backups, resulting in non-convergence. This issue is due to each of the leaf nodes (which cause the errors) being sampled less often than the nodes higher in the tree (which suffer from these errors), thus disabling  the algorithm from correct erroneous target values.}}
  \vspace{-15pt}
  \label{fig:on_policy_example}
\end{figure*}

\paragraph{Reasons for the absence of corrective feedback.} The above example provides an intuitive explanation for how on-policy or replay buffer distributions may not lead to error correction. Updates on states from such distributions may fail to correct Q-values at states that are the \textit{causes} of the errors. In general, Bellman backups rely on the correctness of the values at the states that are used as targets, which may mean relying on the correctness of states that occur least frequently in any distribution generated by online data collection. While this is sufficient to guarantee convergence in tabular settings, with function approximation this can lead to an absence of corrective feedback, as states with erroneous targets are visited more and more often, while the visitation frequency of the states that \emph{cause} these errors does not increase. 





\begin{figure*}
\centering
\includegraphics[width=0.9\linewidth]{images/discor_func_approx_final_again.pdf}
    \caption{\footnotesize{Iterations of Q-learning on a tree-structured MDP with an optimal training distribution, where states are sampled starting from the leaf nodes, progressing upwards towards the root node in the tree. Note that this method backs up \textit{very few} incorrect values in any iterations, and takes only a few iterations to converge. Our aim will be to approximate such an optimal training distribution.}}
  \vspace{-10pt}
  \label{fig:discor_sample}
\end{figure*}

\subsubsection{Theoretical Analysis} 
\label{sec:theoretical_analysis}
We next present a theoretical result that generalizes the didactic example in a formal result. Proof for this result can be found in Appendix~\ref{sec:omitted_proofs}. 
Our result is a lower-bound on the iteration 
complexity of on-policy Q-learning. We show that, under the on-policy distribution, Q-learning may require exponentially many iterations to learn the optimal Q-function. This will also provide an explanation for the slow and unstable learning as described in Figure~\ref{fig:on_policy_example} and in Section~\ref{sec:consequences}, Figure~\ref{fig:sparse_reward}. We state the result next. An extension to the replay buffer setting is given in Appendix~\ref{sec:omitted_proofs}.

\begin{theorem}[Exponential lower bound for on-policy and replay buffer distributions]
\label{thm:exponential}
There exists a family of MDPs parameterized by , with , , such that even with features,  that can represent the optimal Q-function near-perfectly, i.e., , 
on-policy or replay-buffer Q-learning, i.e. , or  respectively, requires  \textit{exact} Bellman projection steps for convergence to , if at all the algorithm converges to .  
\end{theorem}
Thus, on-policy or replay buffer distributions can induce extremely slow learning in certain environments, even when all transitions are available for training (i.e., without any exploration issues), requiring exponentially many Bellman backups. When function approximation is employed, the smaller the frequency of a transition, the less likely Q-learning is to correct the Q-value of the state-action pair corresponding to this transition. In contrast, we show in Appendix~\ref{thm:discor_suboptimal} that the method we propose in this paper requires only  iterations in this scenario, and it behaves similarly to a method that learns Q-values from the leaf nodes, gradually moving upwards toward the root of the tree. 







\subsection{Consequences of Absent Corrective Feedback}
\label{sec:consequences}
{In this section, we investigate the phenomenon of an absence of corrective feedback in a number of RL tasks.} We first plot the value error  for the Q-function by assuming access to  for analysis, but this is not available while learning from scratch. We first show that an absence of corrective feedback happens in practice. In Figures~\ref{fig:suboptimal_conv}, \ref{fig:instability} and \ref{fig:sparse_reward}, we plot  for on-policy and replay buffer sampling. 
Observe that ADP methods can suffer from prolonged periods where  is increasing or fluctuating, and returns degrade or stagnate (Figure~\ref{fig:instability}).
Next, we empirically analyze a number of pathological outcomes that can occur when corrective feedback is absent.



\begin{enumerate}
\item \textbf{Convergence to suboptimal Q-functions.} We find that on-policy sampling can cause ADP to converge to a suboptimal
solution, even in the absence of sampling error. {This is not an issue with the capacity of the function approximator -- even when the optimal Q-function  can be represented in the function class~\citep{fu19diagnosing}, learning converges to a suboptimal fixed point far from .} 
Figure~\ref{fig:suboptimal_conv} shows that the value error  rapidly decreases initially, and eventually converges to a value significantly greater than , from which the learning process never recovers. 


\item \textbf{Instability in the learning process.}
Q-learning with replay buffers may not converge to sub-optimal solutions as often as on-policy sampling (Figure~\ref{fig:exact_fqi_runs}). However, we observe that even then (Figure~\ref{fig:instability}), ADP with replay buffers can be
unstable. {For instance, the algorithm is prone to degradation even if the latest policy obtains returns that are very close to optimal returns  (Figure~\ref{fig:instability}).} This instability is often correlated with a lack of corrective feedback, and exists even with all transitions present in the buffer controlling for sampling error. 

\item \textbf{Inability to learn with low signal-to-noise ratio.} 
Lack of corrective feedback can also prevent ADP algorithms from learning quickly in scenarios with \textit{low} \textit{signal-to-noise}
ratio, such as tasks with sparse or noisy rewards (Figure~\ref{fig:sparse_reward}). For efficient learning, Q-learning needs to effectively ``exploit'' the reward signal even in the presence of noise and delay, and we find that the learning becomes significantly worse in the presence of these factors. {This is not an exploration issue, since all transitions in the MDP are provided to the algorithm}.
\end{enumerate}

\begin{figure}
    \begin{subfigure}[l]{0.3\linewidth}
    \centering
      \includegraphics[width=0.96\linewidth]{images/problem_description_new/fig2b_new_again_v3.pdf}
      \caption{\footnotesize{Sub-optimal convergence for on-policy distributions: return (dashed) and value error (solid). Note that value error decreases rapidly at the start and finally converges to a nonzero value, leading to sub-optimal return.}}
      \label{fig:suboptimal_conv}
    \end{subfigure}
    ~
    \begin{subfigure}[l]{0.3\linewidth}
    \centering
    \includegraphics[width=0.95\linewidth]{images/problem_description_new/fig3b_new_again_v3.pdf}
    \caption{\footnotesize{Instability for replay buffer distributions: return (dashed) and value error (solid) over training iterations. Note the rapid increase in value error at multiple points, which co-occurs with instabilities in returns.}}
    \label{fig:instability}
    \end{subfigure}
    ~
    \begin{subfigure}[l]{0.3\linewidth}
    \centering
      \includegraphics[width=0.95\linewidth]{images/problem_description_new/fig4b_new_again_v3.pdf}
    \caption{\footnotesize{Error (left) and returns (right) for sparse reward MDP with replay buffer distributions. Note the inability to learn, low return, and highly unstable value error , often increasing sharply, destabilizing the learning process.}}
    \label{fig:sparse_reward}
    \end{subfigure} 
    \caption{\footnotesize{Experiments showing various detrimental consequences of an absence of corrective feedback.}}
\end{figure}














 \section{Optimal Distributions for Optimizing Corrective Feedback}
\label{sec:theoretical_derivation}
In the previous section, we observed that an absence of corrective feedback can occur when ADP algorithms na\"ively use the on-policy or replay buffer distributions for training Q-functions. However, if we can compute an ``optimal'' data distribution that provides maximal corrective feedback, and train Q-functions using this distribution, then we can ensure that the ADP algorithm always enjoys corrective feedback, and hence makes steady learning progress. In this section, we aim to derive a functional form for this optimal data distribution. 

We first present an optimization problem for this optimal data distribution, which we refer to as  (different from , which refers to the data distribution in the replay buffer, or the on-policy distribution), for any iteration  of the ADP algorithm, and then present a solution to this optimization problem. 
Proofs from this section can be found in Appendix~\ref{sec:missing_proof_steps}. We will show in this section that the resulting optimization when approximated practically, yields a very simple and intuitive algorithm. A more intuitive description of the resulting algorithm can be found in Section~\ref{sec:method_description}




Our goal is to minimize the value error  at every iteration , with respect to the the distribution  used for Bellman error minimization at iteration . 
The resulting optimization problem is:

\begin{theorem}
\label{eqn:theorem_solution}
An optimal solution  to optimization problem~\ref{eqn:optimization} satisfies the following relationship:

where , is an optimal Lagrange dual variable that ensures  is a valid distribution in Problem~\ref{eqn:optimization}. 
\end{theorem}
\begin{proof}
\textbf{(Sketch)} A complete proof is provided in Appendix~\ref{sec:missing_proof_steps}. Here we provide a rough sketch of the steps involved for completeness. We first use the Fenchel-Young inequality~\cite{rockafellar-1970a} to obtain an upper bound on the true objective, in terms of the ``soft-min'' of the errors  and then minimize this upper bound. Formally, this is given by the RHS of the equation below,  denotes Shannon entropy.

Then, we solve for  by setting the gradient of the Lagrangian for Problem~\ref{eqn:optimization} to , which requires an addition of constraints  and  to ensure that  is a valid distribution. We then use the implicit function theorem (IFT)~\citep{Krantz2002TheIF} to compute implicit gradients of  with respect to . IFT is required for this step since  is an output of a minimization procedure that uses  as an input. Rearranging the expression gives the above result.    
\end{proof}

Intuitively, , shown in Equation~\ref{eqn:optimal_p}, assigns higher probability to state-action tuples with high Bellman error , but only when the resulting Q-value  is close to . However, the expression for  contains terms that depend on . Since both  and  are observed only \emph{after}  is chosen, we need to estimate these quantities using surrogate quantities which we discuss in the following section. 



\vspace{-5pt}
\subsection{Tractable Approximation to Optimal Distribution}
\label{sec:minimax}
\vspace{-2pt}
The expression for  in Equation~\ref{eqn:optimal_p} contains terms dependent on  and , namely  and . As described previously, since both  and  are observed only \emph{after}  is chosen, in this section, we develop surrogates to estimate these quantities. For error against , we show that the cumulative sum of discounted Bellman errors over the past iterations of training, denoted as , shown in Equation~\ref{eqn:delta_k}, is a valid proxy (equivalent to an upper bound) for . In fact, Theorem~\ref{thm:delta_k_upper_bound} shows that , offset by a state-action independent constant, is a tractable upper bound on  constructed only from prior Q-function iterates, . 
\vspace{-2pt}

The following theorem formally states this result. 
\begin{theorem}
\label{thm:delta_k_upper_bound}
There exists a , such that   and  from Equation~\ref{eqn:delta_k},  satisfies the following inequality, pointwise, for each :
\vspace{-10pt}

\end{theorem}
\begin{proof}
\textbf{(Sketch)} A full proof is provided in Appendix~\ref{app:other_proofs}. The key insight in this argument is to use a recursive inequality, presented in Lemma~\ref{thm:worst_case_estimator}, App.~\ref{app:other_proofs}, to decompose , which allows us to show that  is a solution to the corresponding recursive equality, and hence, an upper bound on . Note that, the initialization  matters only infinitesimally once, , with  being such that , therefore, agnostic to the initialization of , , we note that the statement in the theorem holds true for large-enough . 
\end{proof}

\paragraph{Estimating .} The expression for  in Equation~\ref{eqn:optimal_p} also includes an unobserved Bellman error multiplier term,  as well. With no available information about  -- which will only be observed \textit{after} Bellman error minimization under  -- a viable approximation is to bound this term  between the minimum and maximum Bellman errors obtained at the previous iteration,  and . We can simply restrict the support of state-action pairs  used to compute  and  to come from transitions observed in the replay buffer used for the Q-function update, to ensure that both  and  are finite. 

\paragraph{Re-weighting the replay buffer .} Since it is challenging to directly obtain samples from  via online interaction, a practically viable alternative is to instead perform weighted Bellman updates by re-weighting transitions drawn from the a regular replay buffer  using importance weights given by . However, na\"ive importance sampling often suffers from high-variance of these importance weights, leading to instabilities in learning. To prevent such issues, instead of directly re-weighting to , we re-weight samples from  to a projection of , denoted as , that is still close to  under the KL-divergence metric: , where  is a scalar, .
The equation for weights , in this case, is thus given by:

A derivation is provided in Appendix~\ref{app:other_proofs}.
Having described all approximations, we now discuss how to obtain a tractable and practically usable data distribution for training the Q-function that maximally mitigates error accumulation.

\subsection{Putting it All Together}
\label{sec:putting_all}
We have noted all practical approximations to the expression for optimal  (Equation~\ref{eqn:optimal_p}), including estimating surrogates for  and , and the usage of importance weights to develop a method that can achieve the benefits of the optimal distribution, simply by \textit{re-weighting transitions} in the replay buffer, \textit{rather than altering the exploration strategy}. We also discussed a technique to reduce the variance of weights used for this reweighting. We now put these techniques together to obtain the final, practically tractable expression for the weights used for our practical approach.

We note that the term , appearing inside the exponent in the expression for  in Equation~\ref{eqn:optimal_wk} can be approximated by the tractable upper bound . However, computing  requires the quantity  which also is unknown when  is being chosen. Combining the upper bound on , Theorem~\ref{thm:delta_k_upper_bound} and Equation~\ref{eqn:delta_k}, we obtain the following bound:

Using this bound in the expression for , along with the lower bound, , we obtain the following lower bound on weights :

Finally, we note that using a worst-case lower bound for  (Equation~\ref{eqn:worst_case_weights}) will down-weight some additional transitions which in reality lead to low error accumulation, but this scheme will never up-weight a transition with high error, thus providing for a ``conservative'' distribution. A less conservative expression for getting these weights is a subject of future work.
Simplifying the constants ,  and , the final expression for the practical choice of  is:







 \section{Distribution Correction (DisCor) Algorithm}
\label{sec:method_description}
In this section, we present the resulting algorithm, that uses weights  from Equation~\ref{eqn:importance_weights} to re-weight the Bellman backup in order to induce corrective feedback. We first present an intuitive explanation of our algorithm, and then describe the implementation details. 


\begin{algorithm}[t]
\small
\caption{\textbf{DisCor (Distribution Correction)}}
\label{alg:discor}
\begin{algorithmic}[1]
    \STATE Initialize Q-values , initial distribution , a replay buffer , and an \textdiff{error model} .
    \FOR{step  in \{1, \dots, N\}}
        \STATE Collect  samples using , add them to replay buffer , sample 
        \STATE Evaluate  and  on samples .
        \STATE Compute target values for  and  on samples:\\
        \\ \\
        
        \STATE \textdiff{Compute  using Equation~\ref{eqn:importance_weights}}. 
        \STATE Minimize Bellman error for training  weighted by . \\
        
        \STATE \textdiff{Minimize ADP error for training . \\
        }
    \ENDFOR
\end{algorithmic}
\end{algorithm}




\paragraph{Intuitive Explanation.} Using weights  in Equation~\ref{eqn:importance_weights} for weighting Bellman backups possess a very clear and intuitive explanation.  corresponds to the estimated upper bound on the error of the target values for the current transition, due to the backup operator . Intuitively, this implies that weights  \textit{downweight} those transitions for which the bootstrapped \emph{target} Q-value estimate has a high estimated error to , 
effectively focusing the learning on samples where the supervision (target value) is accurate,
which are precisely the samples that we expect maximally improve the accuracy of the Q function. 
This prevents error accumulation, and hence provides correct feedback. Such a scheme also resembles prior methods for learning with noisy labels by ``abstention'' from training on labels that are likely to be inaccurate~\citep{absention}.


\paragraph{Details.} Pseudocode for our approach, which we call \textbf{DisCor} (\textbf{Dis}tribution \textbf{Cor}rection), is presented in Algorithm~\ref{alg:discor}, with the main differences from standard ADP methods highlighted in red. In addition to a standard Q-function, DisCor trains another parametric model, , to estimate  at each state-action pair. The recursion in Equation~\ref{eqn:delta_k} is used to obtain a simple approximate dynamic programming update rule for the parameters  (Line 8).
The second change is the introduction of a weighted Q-function backup with weights , as shown in Equation~\ref{eqn:importance_weights}, on Line 7. 
We also present a practical implementation of the DisCor algorithm, built on top of standard DQN/SAC algorithm pseudocodes in Algorithm~\ref{alg:practical_alg}, Appendix~\ref{app:exp_details}. 
























 \section{Related Work}
\vspace{-2pt}
Prior work has pointed out a number of issues arising when dynamic programming is used with function approximation. Some work~\citep{munos2005error,farahmand2010error,munos2008finite} focused on analysing error induced from the typically used projected Bellman operator under the assumption of an abstract error model. Further, fully gradient-based objectives for Q-learning~\citep{Sutton09a,Sutton09b,maei09nonlineargtd} gave rise to
convergent ADP algorithms with function approximation. 
In contrast to these works, which mostly focus on ensuring {convergence of the Bellman backup}, we focus on the interaction between the ADP update and the data distribution . 
On the other hand, prior work on fully online Q-learning from a stochastic approximation viewpoint analyzes time-varying , but in the absence of function approximation~\citep{Watkins92,Tsitsiklis1994,devraj2017zap}, or at the granularity of a single time-step in the environment~\citep{Tsitsiklis97ananalysis}, unlike our setting. Our setting involves both a time-varying  depending on the prior and latest Q-functions as well as function approximation.  

A number of prior works have focused on studying non-convergence and generalization effects in ADP methods with deep net function approximators, both theoretically~\citep{Achiam2019TowardsCD} and empirically~\citep{fu19diagnosing,martha2018sparse,kumar19bear}. In this paper, we study a different issue that is distinct from non-convergence and generalization {issues due to the specific choice of deep net function approximators:} the interaction between the data distribution and the fitting error in the value function.


In the fully offline setting, prior works have noted that sampling distributions can affect performance of ADP methods~\citep{fu19diagnosing}. This has been generally aimed at resolving what is typically known as the ``deadly triad''~\citep{suttonrlbook}: {divergence} caused by an interaction between function approximation, bootstrapped updates and off-policy sampling, resulting in the development of batch RL algorithms that choose sampling schemes~\citep{Kolter2011TheFP,sutton16emphatic} {for guaranteed convergence}. However, we show that sampling distributions can have a drastic impact on the learning process even where the algorithm performs online (on-policy) interaction to collects its own data.
\cite{schaul2019ray} studies the interaction between data collection and training for multi-objective policy gradient methods and note the bias towards optimizing only a few components of the objective that arises. 


Our proposed algorithm weights the transition in the buffer based on an estimate of their error to the true optimal value function.
Related to our approach, prioritized sampling has been used previously in ADP methods, such as PER~\citep{Schaul2015}, to prioritize transitions with higher Bellman error. We show in Section~\ref{sec:experiments}, that this choice may fail to perform in many cases. 
Recent work~\citep{du2019distributioncheck} has attempted to use a distribution-checking oracle to control the amount of exploration performed. Our work aims at ensuring corrective feedback, solely by re-weighting the data distribution for Q-learning with bootstrapped backups. 

We further discuss the relationship with other prior works in an extended related work section in  Appendix~\ref{app:related_work_extended}.
 \section{Experimental Evaluation of DisCor}
\label{sec:experiments}
The goal of our empirical evaluation is to study the following questions: 
\begin{enumerate}
    \item Can DisCor ensure continuous corrective feedback in RL tasks, mitigating the issues raised in Section~\ref{sec:consequences}? 
    \item How does DisCor compare to prior methods, including those that also reweight the data in various ways?
    \item Can DisCor attain good performance in challenging settings, such as multi-task RL or noisy reward signals?
    \item How do approximations from Section~\ref{sec:theoretical_derivation} affect the efficacy of DisCor in mitigating error accumulation?
\end{enumerate}
We start by presenting a detailed analysis on tabular MDPs with function approximation, studying each component of DisCor in isolation,
and then study six challenging robotic manipulation tasks, the multi-task MT10 benchmark from MetaWorld~\citep{yu2019meta}, and three Atari games~\citep{bellemare2013ale}.

\begin{figure}
    \centering
    \begin{subfigure}[t]{.99\linewidth}
        \centering
        \includegraphics[width=0.17\linewidth]{images/plot_gridworld_new/plots_discor_vs_discororacle/fig6a_new_again.pdf}
        \includegraphics[width=0.17\linewidth]{images/plot_gridworld_new/plots_discor_vs_discororacle/fig6b_new_again.pdf}
        ~\vline~
        \includegraphics[width=0.17\linewidth]{images/plot_gridworld_new/plots_discor_vs_discororacle/fig6c_new_again.pdf}
        \includegraphics[width=0.17\linewidth]{images/plot_gridworld_new/plots_discor_vs_discororacle/fig6d_new_again.pdf}
        \caption{\footnotesize{Value Error / return for \textit{two} runs of DisCor (blue) and DisCor(oracle) (red) in exact (left) and sampled (right) settings. Observe that (i) DisCor achieves similar performance as DisCor (oracle) generally, (ii) DisCor provides corrective feedback: value error decreases with both DisCor and DisCor(oracle).}}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{.95\linewidth}
        \centering
        \includegraphics[width=0.19\linewidth]{images/plot_gridworld_new/grid16randomobs_ent0_01_exact_new.pdf}
        \includegraphics[width=0.19\linewidth]{images/plot_gridworld_new/grid16randomobs_ent0_01_sampled__new.pdf}
        \includegraphics[width=0.23\linewidth,trim={2cm 0cm 3cm 2.0},clip]{images/plot_gridworld_new/fig10_new_again.pdf}
        \includegraphics[width=0.194\linewidth,trim={3.4cm 1cm 4.0cm 3.5},clip]{images/plot_gridworld_new/fig9_new_again.pdf}
        \includegraphics[width=0.15\linewidth]{images/legend.pdf}
        \caption{\footnotesize{Comparative Performance on tabular domains}}
    \end{subfigure}
    \caption{\footnotesize{Performance of \textbf{DisCor}, \textbf{DisCor (oracle)}, replay buffer, PER, on-policy and uniform sampling averaged across tabular domains with (right) and without (middle) sampling error. Note that: (1) DisCor generally ensures corrective feedback, (2) DisCor is generally comparable to DisCor (oracle), however, DisCor (oracle) outperforms DisCor, as expected, and (3) DisCor (DisCor and DisCor (oracle) generally outperform all distributions. Exact setup for these domains is described in Appendix~\ref{sec:app_exps_gridworld}.}}
    \label{fig:gridworls_samling}
    \vspace{-0.5cm}
\end{figure}

\subsection{Analysis of DisCor on Tabular Environments}
\label{sec:gridworlds}

We first use the tabular domains from Section~\ref{sec:problem_description}, described in detail in Appendix~\ref{sec:app_exps_gridworld}, to analyze the corrective feedback induced by DisCor and evaluate the effect of the approximations used in our method, such as the upper bound estimator . We first study the setting without sampling error, where all transitions in the MDP are added to the training buffer, and then consider the setting with sampling error, where the algorithm also needs to explore the environment and collect transitions.

\paragraph{Results.} In both settings, shown in Figure~\ref{fig:gridworls_samling}, DisCor consistently provides correct feedback (Figure~\ref{fig:gridworls_samling}(a)). An oracle version of the algorithm (labeled DisCor (oracle); Equation~\ref{eqn:optimal_wk}), which uses the true error  in place of , is somewhat better than DisCor (Figure~\ref{fig:gridworls_samling}(b) histograms, red vs blue), but DisCor still outperforms on-policy and replay buffer sampling schemes, which often suffer from an absence of corrective feedback as shown in Section~\ref{sec:problem_description}.
Prioritizing based on high Bellman error struggles on domains with sparse rewards (App.~\ref{sec:app_exps_gridworld}, Fig.~\ref{fig:app_fig_sampled}). 

\paragraph{Analysis.} While DisCor (oracle) consistently performs better than DisCor, as we would expect, the approximate DisCor method still attains better performance than na\"{i}ve uniform weighting and prioritization similar to PER. This shows that the principle behind DisCor is effective when applied exactly, and that even the approximation that we utilize to make the method practical still improves performance considerably.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.4\linewidth]{images/metaworld_results/metaworld_tasks.pdf}
    \caption{\footnotesize{Visual description of the six MetaWorld tasks used in our experiments in Section~\ref{sec:experiments}. Figures taken from \cite{yu2019meta}.}}
    \label{fig:metaworld_tasks_main}
\end{figure}






\subsection{Continuous Control Experiments}
\label{sec:continuous_control}
We next perform a comparative evaluation of DisCor on several continuous control tasks, using six robotic manipulation tasks from the Meta-World suite (pull stick, hammer, insert peg side, push stick, push with wall and turn dial). A pictorial description of these tasks is provided in Figure~\ref{fig:metaworld_tasks_main}. These domains were chosen because they are challenging for state-of-the-art RL methods, such as soft actor-critic (SAC).

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.25\linewidth]{images/metaworld_results/pull_with_stick_True_again.pdf}
    \includegraphics[width=0.25\linewidth]{images/metaworld_results/push_with_wall_True_again.pdf}
    \includegraphics[width=0.25\linewidth]{images/metaworld_results/turn_dial_True_again.pdf} \\
    \includegraphics[width=0.25\linewidth]{images/metaworld_results/hammer_True_again.pdf}
    \includegraphics[width=0.25\linewidth]{images/metaworld_results/push_with_stick_True_again.pdf}
    \includegraphics[width=0.25\linewidth]{images/metaworld_results/insert_peg_side_True_again.pdf}
    \caption{\footnotesize{Evaluation success of DisCor, unweighted SAC and PER on six MetaWorld tasks. From left to right: pull stick, push with wall, push stick, turn dial, hammer and insert peg side. Note that DisCor achieves better final success rates or learns faster on most of the tasks. }}
    \label{fig:success_rates_metaworld}
\end{figure*}

\paragraph{Meta-World.} We applied DisCor to these tasks by modifying the weighting of samples in soft actor-critic (SAC)~\cite{Haarnoja18}. DisCor does not alter any hyperparameter from SAC, and requires minimal tuning.
There is only one additional temperature hyperparameter, which is also automatically chosen across all domains.
More details are presented in Appendix~\ref{sec:app_exp_details}.  





We compare DisCor to standard SAC without weighting, as well as prioritized experience replay (PER)~\cite{Schaul2015}, which uses weights based on the last Bellman error. On these tasks, DisCor often attains better performance, as shown in Figures \ref{fig:success_rates_metaworld} and \ref{fig:app_returns_metaworld}, achieving better success rates than standard SAC on several tasks. 
DisCor is also more efficient, achieving good performance earlier than the other methods on these tasks. PER, on the other hand, performs poorly, as prioritizing high Bellman error states may lead to higher error accumulation if the target values are incorrect. 

\paragraph{Gym.} We also performed comparisons on the conventional OpenAI gym benchmarks, where we see a small but consistent benefit from DisCor reweighting. Since prior methods, such as SAC already solves these tasks easily, and have been tuned well for them, the room for improvement is very small. We include these results in Appendix~\ref{sec:app_mujoco_benchmarks} for completeness. We also modified the gym environments to have \textit{stochastic} rewards, thus lowering the signal-to-noise ratio, and compare different algorithms on these domains. The results, along with a description of the noise added, are shown in Appendix~\ref{sec:app_mujoco_benchmarks}. In these experiments, DisCor generally exhibits better sample efficiency as compared to other methods.

\vspace{-2pt}
\subsection{Multi-Task Reinforcement Learning}
\label{sec:multi-task}
\vspace{-2pt}
Another challenging setting for current RL methods is the multi-task setting. This is known to be difficult, to the point that oftentimes learning completely separate policies for each of the tasks is actually faster, and resulting in better performance, than learning the tasks together~\citep{yu2019meta,schaul2019ray}. 

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{.4\linewidth}
        \centering
        \includegraphics[width=0.74\linewidth]{images/metaworld_results/mt_10_newTrue.pdf}
        \caption{\footnotesize{Average success rate}}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{.4\linewidth}
        \centering
        \includegraphics[width=0.95\linewidth]{images/metaworld_results/discor_mt10_sac_500.png}
        \caption{\footnotesize{Per-task return at 500k environment steps}} 
    \end{subfigure}
    \caption{\footnotesize{Performance of DisCor (blue) and unweighted SAC (green) on the MT10 benchmark. We observe that: (1) DisCor outperforms unweighted SAC by a factor of \textbf{1.5} in terms success rate; (2) DisCor achieves a non-trivial return on \textbf{7/10} tasks after 500k environment steps, as compared to \textbf{3/10} for unweighted SAC.}}
    \label{fig:mt_results}
\end{figure}

\paragraph{MT10 multi-task benchmark.} We evaluate on the MT10 MetaWorld benchmark~\citep{yu2019meta}, which consists of ten robotic manipulation tasks to be learned jointly. We follow the protocol from \cite{yu2019meta}, and append task ID to the state. As shown in Figure~\ref{fig:mt_results}(a), DisCor applied on top of SAC outperforms standard unweighted SAC by a large margin, achieving \textbf{50\%} higher success rates compared to SAC, and a high overall return (Fig \ref{fig:app_mt10}). Figure~\ref{fig:mt_results}(b) shows that DisCor makes progress on \textbf{7/10} tasks, as compared to \textbf{3/10} for SAC. 

\paragraph{MT50 multi-task benchmark.} We further compare the performance of DisCor and SAC on the more challenging MT50 benchmark~\citep{yu2019meta}, which is shown in Figure~\ref{fig:app_mt50}, and observe a similar benefit as compared to MT10, where the standard unweighted SAC algorithm tends to saturate at a suboptimal success rate for about 4M environment steps, whereas DisCor continuously keeps learning, and achieves asymptotic performance faster than SAC. 






\subsection{Arcade Learning Environment}
\label{sec:atari_exps}
Our final experiments were aimed at testing the efficacy of DisCor on stochastic, discrete-action environments. To this end, we evaluated DisCor on three commonly reported tasks from the Atari suite -- Pong, Breakout and Asterix.
We compare to the baseline DQN~\cite{Mnih2015}, all our implementations are built off of Dopamine~\cite{castro18dopamine} and use the evaluation protocol with sticky actions~\cite{machado18sticky}. We build DisCor on top of DQN by simply replacing the standard replay buffer sampling scheme in DQN with the DisCor weighted update. We show in Figure~\ref{fig:atari_results} that DisCor usually outperforms unweighted DQN in learning speed and performance. 
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.25\linewidth]{images/atari_results/Pong_dqn_vs_discor_new_final.pdf}
    \includegraphics[width=0.25\linewidth]{images/atari_results/Breakout_dqn_vs_discor_new_final.pdf}
    \includegraphics[width=0.25\linewidth]{images/atari_results/Asterix_dqn_vs_discor_new_final.pdf}
    \caption{\footnotesize{DQN vs DisCor on Atari. Note that DisCor generally improves learning speed and asymptotic performance on all three tasks that we evaluated.}}
    \label{fig:atari_results}
\end{figure} \section{Discussion, Future Work and Open Problems}
In this work, we show that deep RL algorithms suffer from the absence of a corrective feedback mechanism in scenarios with na\"ive online data collection. This results in a number of problems during learning, including slow convergence and oscillation. We propose a method to compute the optimal data distribution in order to achieve corrective feedback, and design a practical algorithm, DisCor, that applies this distribution correction by re-weighting the transitions in the replay buffer based on an estimate of the accuracy of their target values. DisCor yields improvements across a wide range of RL problems, including challenging robotic manipulation tasks, multi-task reinforcement learning and Atari games and can be easily combined with a variety of ADP algorithms. 

As shown through our analysis and experiments, studying the connection between data distributions, function approximation, and approximate dynamic programming can allow us to devise stable and efficient reinforcement learning algorithms. This suggests several exciting directions for future work. First, a characterization of the learning dynamics and their interaction with corrective feedback and data distributions in ADP algorithms may lead to even better and more stable algorithms, by better identifying the accuracy of target values. Second, our approach is limited by the transitions that are actually collected by the behavior policy, since it only reweights the replay buffer.
An exciting direction of future investigation would involve studying how we might directly modify the exploration policy to change which transitions are collected, so as to more directly alter the training distribution and produce large gains in sample efficiency and asymptotic performance. 
If we can devise RL methods that are guaranteed to enjoy corrective feedback and thus are stable, robust, and effective, then RL algorithms can be reliably scaled to large open-world settings. 



 
 
\section*{Acknowledgements}
We thank Xinyang Geng and Aurick Zhou for helpful discussions. We thank Vitchyr Pong, Greg Kahn, Xinyang Geng, Aurick Zhou, Avi Singh, Nicholas Rhinehart, and Michael Janner for feedback on an earlier version of this paper, and all the members of the RAIL lab for their help and support. We thank Tianhe Yu, Kristian Hartikainen, and Justin Yu for help with debugging and setting up the implementations. This research was supported by: the National Science Foundation, the Office of Naval Research, and the DARPA Assured Autonomy program. We thank Google, Amazon and NVIDIA for providing compute resources.


\bibliography{main}
\bibliographystyle{unsrt}


\clearpage
\appendix
\part*{Appendices}

\section{Detailed Proof of Theorem~\ref{eqn:theorem_solution} (Section~\ref{sec:theoretical_derivation})}
\label{sec:missing_proof_steps}
In this appendix, we present a detailed proofs for the theoretical derivation of DisCor outlined in Section~\ref{sec:theoretical_derivation}. To get started, we mention the optimization problem being solved for convenience.


We break down this derivation in steps marked as relevant paragraphs. The first step is to decompose the objective into a more tractable one via an application of the Fenchel-Young inequality~\cite{rockafellar-1970a}.


\paragraph{Step 1: Fenchel-Young Inequality.} The optimization objective in Problem~\ref{eqn:optimization_app} is the inner product of  and . We can decompose this objective by applying the Fenchel-Young inequality~\cite{rockafellar-1970a}. For any two vectors, , and any convex function  and its Fenchel conjugate , we have that, . We therefore have:
 
Since minimizing an upper bound leads to minimization of the original objective, we can replace the objective in Problem~\ref{eqn:optimization_app} with the upper bound in Equation~\ref{eqn:convex_conjugate_app}. As we will see below, a convenient choice of  is the \emph{soft-min} function:

 in this case is given by the Shannon entropy, which is defined as . Plugging this back in problem~\ref{eqn:optimization_app}, we obtain an objective that dictates minimization of the marginal state-action entropy of the policy . 

In order to make this objective even more convenient and tractable, we upper bound the Shannon entropy,  by the entropy of the uniform distribution over states and actions, . This step ensures that the entropy of the state-action marginal  is not reduced drastically due to the choice of . 
We can now minimize this upper bound, since minimizing an upper bound, leads to a minimization of the original problem, and therefore, we obtain the following new optimization problem shown in Equation~\ref{eqn:final_optimization_app} is:

Another way to interpret this step is to modify the objective in Problem~\ref{eqn:optimization_app} to maximize entropy-augmented :  as is common in a number of prior works, albeit with entropy over different distributions such as \cite{hazan2019maxent,Haarnoja18}. This also increases the smoothness of the loss landscape, which is crucial for performance of RL algorithms~\citep{ahmed19understanding}.

\paragraph{Step 2: Computing the Lagrangian.} In order to solve optimization problem Problem~\ref{eqn:final_optimization_app}, we follow standard procedures for finding solutions to constrained optimization problems. We first write the Lagrangian for this problem, which includes additional constraints to ensure that  is a valid distribution:

with constraints  and  () and their corresponding Lagrange multipliers,  and , respectively, that ensure  is a valid distribution.
An optimal  is obtained by setting the gradient of the Lagrangian with respect to  to 0. This requires computing the gradient of , resulting from Bellman error minimization, i.e. computing the derivative through the solution of another optimization problem,  with respect to the distribution . We use the implicit function theorem (IFT)~\citep{Krantz2002TheIF} to compute this gradient. We next present an application of IFT in our scenario.

\paragraph{Step 3: IFT gradient used in the Lagrangian.} We derive an expression for  which will be used while computing the gradient of the Lagrangian Equation~\ref{eqn:lagrangian} which involves an application of the implicit function theorem. The IFT gradient is given by: 

To get started towards showing Equation~\ref{eqn:ift_gradient}, we consider a non-parametric representation for  (a table), so that we can compute a tractable term without going onto the specific calculations for Jacobian or inverse-Hessian vector products for different parametric models. In this case, the Hessians in the expression for IFT and hence, the implicit gradient are given by:

provided  (which is true, since we operate in a full coverage regime, as there is no exploration bottleneck when all transitions are provided). This quantity is  only if the Bellman residuals  are 0, however, that is rarely the case, hence this gradient is non-zero, and intuitively quantifies the right relationship: Bellman residual errors  should be higher at state-action pairs with low density , indicating a roughly inverse relationship between the two terms -- which is captured by the IFT term. 



\paragraph{Step 4: Computing optimal .} Now that we have the equation for IFT (Equation~\ref{eqn:ift_gradient}) and an expression for the Lagrangian (Equation~\ref{eqn:lagrangian}), we are ready to compute the optimal  via an application of the KKT conditions. At an optimal , we have, 

Now, re-arranging Equation~\ref{eqn:lagrangian_simplify} and plugging in the expression for  from Equation~\ref{eqn:ift_gradient} in this Equation to obtain an expression for , we get:

Provided, each component of  is positive, i.e.  for all , the optimal dual variable , satisfies  by KKT-conditions, and  (since it is a Lagrange dual variable), thus implying that .

Intuitively, the expression in Equation~\ref{eqn:optimal_p_app} assigns higher probability to state-action tuples with high Bellman error , but only when the \textit{post-update} Q-value  is close to . Hence we obtain the required theorem. 
\paragraph{Summary of the derivation.} To summarize, our derivation for the optimal  consists of the following key steps:
\begin{itemize}
    \item Use the Fenchel-Young inequality to get a convenient form for the objective.
    \item Compute the Lagrangian, and use the implicit function theorem to compute gradients of the Q-function  with respect to the distribution .
    \item Compute the expression for optimal  by setting the Lagrangian gradient to . 
\end{itemize}

\section{Proofs for Tractable Approximations in Section~\ref{sec:minimax}}
\label{app:other_proofs}
Here we present the proofs for the arguments behind each of the approximations described in Section~\ref{sec:minimax}.
\paragraph{Computing weights  for re-weighting the buffer distribution, .} 
Since sampling directly from  may not be easy, we instead choose to re-weight samples transitions drawn from a replay buffer , using weights  to make it as close to . How do we obtain the exact expression for ? One option is to apply importance sampling: choose  as the importance ratio, , however this suffers from two problems -- (1) importance weights tend to exhibit high variance, which can be detrimental for stochastic gradient methods; and (2) densities , needed to compute  are unknown. 

In order to circumvent these problems, we solve a different optimization problem, shown in Problem~\ref{ref:eqn_bias_variance_wk} to find the optimal \textit{surrogate} projection distribution , which is closest to , and at the same time closest to  as well, trading off these quantities by a factor .

where  is a temperature hyperparameter that trades of bias and variance. The solution to the above optimization is shown in Equation~\ref{ref:eqn_final_importance_weights},
where the second statement follows by using a tractable approximation of setting  to be equal to , which can be ignored if  is large, hence . The third statement follows by an application of Equation~\ref{eqn:optimal_p_app} and the fourth statement denotes the importance ratio, , as the weights . 


Our next proof justifies the usage of the estimate , which is a worst-case upper bound on  in Equation~\ref{ref:eqn_final_importance_weights}.

\paragraph{Proof of Theorem~\ref{thm:delta_k_upper_bound}.} 
We now present a Lemma~\ref{thm:worst_case_estimator} which proves a recursive inequality for , then show that the corresponding recursive estimator upper bounds  pointwise in Lemma~\ref{thm:intermediate}, and then finally show that our chosen estimator  is equivalent to this recursive estimator in Theorem~\ref{thm:alpha} therefore proving Theorem~\ref{thm:delta_k_upper_bound}.
\begin{lemma}
\label{thm:worst_case_estimator}
For any ,  satisfies the following recursive inequality, pointwise for each :

\end{lemma}
\begin{proof}
Our proof relies on a worst-case expansion of the quantity . The proof follows the following steps. The first few steps follow common expansions/inequalities operated upon in the work on error propagation in Q-learning~\cite{munos2005error}.

where (a) follows from adding and subtracting , (b) follows from an application of triangle inequality, (c) follows from the definition of  applied to two different Q-functions, (d) follows from algebraic manipulation, (e) follows from an application of the triangle inequality, and (f) follows from bounding the maximum difference in transition matrices  by maximum total variation divergence between policy  and , and bounding the maximum possible value of  by .
\end{proof}

We next show that an estimator that satisfies the recursive equality corresponding to Lemma~\ref{thm:worst_case_estimator} is a pointwise upper bound on .

\begin{lemma}
\label{thm:intermediate}
For any , an vector  satisfying

with , and an initialization , pointwise upper bounds  with an offset depending on , i.e. .
\end{lemma}
\begin{proof}
Let  be an estimator satisfying Equation~\ref{eqn:delta_prime_recursion}. In order to show that , we use the principle of mathematical induction. The base case,  is satisfied, since . Now, let us assume that for a given ,  pointwise for each . Now, we need to show that a similar relation holds for , and then we can appeal to the principle of mathematical induction to complete the argument. In order to show this, we note that,

where (\ref{eqn:step1}) follows from the definition of , (\ref{eqn:step2}) follows by rearranging the recursive sum containing , for  alongside , (\ref{eqn:step3}) follows from the inductive hypothesis at , and (\ref{eqn:step4}) follows from Lemma~\ref{thm:worst_case_estimator}.

Thus, by using the principle of mathematical induction, we have shown that  pointwise for each , for every .
\end{proof}

The final piece in this argument is to show, that the estimator  used by the DisCor algorithm (Algorithm~\ref{alg:discor}), which is initialized randomly, i.e. not initialized to , still satisfies the property from Lemma~\ref{thm:intermediate}, possibly for certain .

Therefore, we now show why:  point-wise for a sufficiently large . We restate a slightly modified version of Theorem~\ref{thm:delta_k_upper_bound} for convenience. 
\begin{theorem}[Formal version of Theorem~\ref{thm:delta_k_upper_bound}]
\label{thm:alpha}
For a sufficiently large , the error estimator  pointwise  satisfies:

where 's are scalar constants independent of any state-action pair. \textit{(Note that Theorem~\ref{thm:delta_k_upper_bound} has a typo  instead of , this theorem presents the correct inequality.})
\end{theorem}

\begin{proof}
\paragraph{Main Idea/ Sketch:} As shown in Algorithm~\ref{alg:discor}, the estimator  is initialized randomly, without taking into account . Therefore, in this theorem, we want to show that \textit{irrespective} of the initialization of , a randomly initialized  eventually satisfies the inequality shown in Theorem~\ref{thm:delta_k_upper_bound}. Now, we present the formal proof. \\

Consider  to be the smallest , such that the following inequality is satisfied:

Thus, , assuming  without loss of generality. For a different reward scaling, the bound can be scaled appropriately. To see this, we substitute  as an upper-bound , and  bound  by . 

Let  correspond to the upper-bound estimator as derived in Lemma~\ref{thm:intermediate}. For each , the contribution of the initial error  in  is upper-bounded by , and gradually decreases with a rate  as more backups are performed, i.e., as  increases. Thus we can construct another sequence  which chooses to ignore , and initializes  (or randomly) and the sequences  and  satisfy:

Furthermore, the difference  steadily shrinks to 0, with a linear rate , so the overall contribution of the initialization sub-optimality  drops linearly with a rate of . Hence,  and  converge to the same sequence beyond a fixed . Since  is computed using the RHS of Lemma~\ref{thm:worst_case_estimator}, it is guaranteed to be an upper bound on :

Since, , we get , using \ref{eqn:worst_case_surrogate}, that 

Hence,  for large .
\paragraph{A note on the value of .} For a discounting of , we get that  and for , . In practical instances, an RL algorithm takes a minimum of about  1M gradient steps, so this value of  is easy achieved. Even in the gridworld experiments presented in Section~\ref{sec:gridworlds}, , hence, the effects of initialization stayed significant only untill about  iterations during training, out of a total of 300 or 500 performed, which is a small enough percentage.
\end{proof}

\paragraph{Summary of Proof for Theorem~\ref{thm:delta_k_upper_bound}.}  in DisCor is given by the quantity , is an upper bound for the error , and we can safely initialize the parametric function  using standard neural network initialization, since the value of initial error will matter \textit{only} infinitesimally after a large enough . 

As , the following is true:

Also, note that if  is improving, i.e. , then, we have that , and since limit of a sum is equal to the sum of the limit, and , therefore, the final inequality in Equation~\ref{eqn:limit2} tends to 0 as .

\section{Missing Proofs From Section~\ref{sec:problem_description}}
\label{sec:omitted_proofs}
In this section, we provide omitted proofs from Section~\ref{sec:problem_description} of this paper. Before going into the proofs, we first describe notation and prove some lemmas that will be useful later in the proofs.


We also describe the underlying ADP algorithm we use as an ideal algorithm for the proofs below.

\begin{algorithm}[H]
\small
\caption{Generic ADP algorithm}
\label{alg:fqi}
\begin{algorithmic}[1]
    \STATE Initialize Q-values .
    \FOR{step  in \{1, \dots, N\}}
        \STATE Collect trajectories using 
        \STATE Choose distribution  for projection.
        \STATE  \\ 
    \ENDFOR
\end{algorithmic}
\end{algorithm}

\paragraph{Assumptions.} The assumptions used in the proofs are as follows:
\begin{itemize}
    \item Q-function is linearly represented, i.e. given a set of features,  for each state and action, concisely represented as the matrix , Q-learning aims to learn a d-dimensional feature vector , such that . This is not a limiting factor as we will prove in Assumption~\ref{assumption:optimal_features}, for problems with sufficiently large  and .
\end{itemize}







\subsection{Suboptimal Convergence of On-policy Q-learning}
We first present a result that describes how Q-learning can converge sub-optimally when performed with on-policy distributions, thus justifying our empirical observation of suboptimal convergence with on-policy distributions. 
\begin{theorem}[\cite{farias_fixed_points}]
Projected Bellman optimality operator under the on-policy distribution  with a Boltzmann policy, , where  always has one or more fixed points.
\end{theorem}
\begin{proof}
This statement was proven to be true in \cite{farias_fixed_points}, where it was shown the projection operator  has the same fixed points as another operator,  given by:

where  is a constant. They showed that the operator  is a contraction for small-enough  and used a compact set argument to generalize it to other positive values of . We refer the reader to \cite{farias_fixed_points} for further reference.

They then showed a 2-state MDP example (Example 6.1, \cite{farias_fixed_points}) such that the Bellman operator  has \textbf{2} fixed points, thereby showing the existence of one or more fixed points for the on-policy Bellman backup operator.  
\end{proof}









\subsection{Proof of Theorem~\ref{thm:exponential}}

We now provide an existence proof which highlights the difference in the speeds of learning accurate Q-values from online or on-policy and replay buffer distributions versus a scheme like DisCor. 
We first state an assumption (Assumption~\ref{assumption:optimal_features}) on the linear features parameterization used for the Q-function. This assumption ensures that the optimal Q-function exists in the function class (i.e. linear function class) used to model the Q-function. This assumption has also been used in a number of recent works including \cite{du2020is}. Analogous to \cite{du2020is}, in our proof, we show that this assumption is indeed satisfied for features lying in a space that is logarithmic in the size of the state-space. For this theorem, we present an episodic example, and operate in a finite horizon setting with discounting  and  denotes the horizon length. An episode terminates deterministically as soon as the run reaches a terminal node -- in our case a leaf node of the tree MDP, i.e. a node at level  -- as we will see next.

\begin{assumption}
\label{assumption:optimal_features}
There exists , and , such that for any , the optimal Q-function satisfies: .
\end{assumption}

We first prove an intermediate property of  satisfying the above assumption that will be crucial for the lower bound argument for on-policy distributions.
\begin{corollary}
\label{cor:epsilon_rank_lemma}
There exists a set of features  satisfying assumption~\ref{assumption:optimal_features}, such that the following holds: .
\end{corollary}
\begin{proof}
This proof builds on the existence argument presented in \cite{du2020is}. Using the -rank property of the identity matrix, one can show that there exists a feature set  such that . Thus, we can choose any such , for a sufficiently low threshold . In order to assign features  to a state, we can simply perform an enumeration of nodes in the tree via a standard graph search procedure such as depth first search and assign a node  a feature vector . To begin with, let's show how we can satisfy assumption~\ref{assumption:optimal_features} by choosing a different weight vector  for each level , such that we obtain .
Since for each level  exactly one state satisfies , so we can just let  and thus we are able to satisfy Assumption~\ref{assumption:optimal_features}. This is the extent of the argument used in \cite{du2020is}.

Now we generalize this argument to find a single , unlike different weights  for different levels . In order to do this, we create a new , of size  (note  versus  dimensions for  and ) given any  satisfying the argument in the above paragraph, such that

Essentially, we pad  with zeros, such that for  belonging to a level ,  is equal to  in the th, -sized block.

A choice of a single  for  is given by simply concatenating  found earlier for .

It is easy to see that  satisfies assumption~\ref{assumption:optimal_features}. A fact that will be used in the proof for Theorem~\ref{thm:app_exp_lower_bound}, is that this construction of  also satisfies: .
\end{proof}

We now restate the theorem from Section~\ref{sec:problem_description} and provide a proof below. 
\begin{figure}
    \centering
 \begin{tikzpicture}[->,>=stealth',level/.style={sibling distance = 4cm/#1,
  level distance = 1.5cm}] 
\node [arn_x] {r(s, a) = 0}
    child{ node [arn_x] {r(s) = 0} 
            child{ node [arn_x] {r(s) = 0}} 
            child{ node [arn_x] {r(s) = 0}}
    }
    child{ node [arn_x] {r(s) = 0}
            child{ node [arn_x] {r(s, ) = 1} }
            child{ node [arn_x] {r(s) = 0} }
		}
; 
\end{tikzpicture}
    \caption{\footnotesize{Example element of the tree family of MDPs used to prove the lower bound in Theorem~\ref{thm:app_exp_lower_bound}. Here, the depth of the tree .  implies that executing any action  or , a reward of 0 is obtained as state .  is given by state marked r(s, ) = 1.}}
    \label{fig:tree_fig}
\end{figure}

\begin{theorem}[Exponential lower bound for on-policy distributions]
\label{thm:app_exp_lower_bound}
There exists a family of MDPs parameterized by , with ,   and a set of features satisfying Assumption~\ref{assumption:optimal_features}, such that on-policy sampling distribution, i.e. , requires  \textit{exact} fixed-point iteration steps in the generic algorithm (Algorithm~\ref{alg:fqi}) for convergence, if at all, the algorithm converges to an accurate Q-function. 
\end{theorem}

\begin{proof}
\paragraph{Tree Construction.} Consider the family of tree MDPs like the one shown in Figure~\ref{fig:tree_fig}. Both the transition function  and the reward function  are deterministic, and there are two actions at each state:  and . There are  level of states, thereby forming a full binary tree of depth . Executing action  transitions the state to its left child int he tree and executing action  transitions the state to its right child. There are  states in level . Among the  states in level , there is one state, , such that action  at this state yields a reward of . For other states of the MDP, . This is a typical example of a sparse reward problem, generally used for studying exploration~\cite{du2020is}, however, we re-iterate that in this case, we are primarily interested in the number of iterations needed to learn, and thereby assume that the algorithm is given infinite access to the MDP, and all transitions are observed, and the algorithm just picks a distribution , in this case, the on-policy state-action marginal for performing backups.
 
\paragraph{Main Argument.} Now, we are equipped with a family of the described tree MDPs and a corresponding set of features  which can represent an accurate Q-function. Our aim is to show that on-policy Q-learning takes steps, exponential in the horizon for solving this task.

For any stochastic policy , and  defined as , , the marginal state-action distribution satisfies:
 
Since  is a discounted state-action marginal distribution, another property that it satisfies is that:

where c is a constant . The above is true, since, there are  states in this MDP, and the maximum values of any entry in  can be  since,  is the least eigenvalue of  for any policy , since . 

Now, under an on-policy sampling scheme and a linear representation of the Q-function as assumed, the updates on the weights for each iteration of Algorithm~\ref{alg:fqi} are given by ( represents ): 

Now,  from the property Equation~\ref{eqn:max_dsa}. Hence, the maximum 2-norm of the updated  is given by:

where the first inequality follows by an application of the triangle inequality, the second inequality follows by using the minimum value of the Frobenius norm of the matrix  to be  (using the rank lemma used to satisfy Assumption~\ref{assumption:optimal_features}) in the denominator of the first term, bounding  by Equation~\ref{eqn:max_dsa}, and finally bounding the second term by , since the maximum eigenvalue of the entire matrix in front of  is , as it is a projection matrix with a discount  valued scalar multiplier. The third inequality follows from lower bounding  by  using Equation~\ref{eqn:spectral_bound}.\\

The optimal  is given by the fixed point of the Bellman optimality operator, and in this case satisfies the following via Cauchy-Schwartz inequality,

Thus, in order for  to be equal to , it must satisfy the above condition (Equation~\ref{eqn:neccessary_w_star}). If we choose an initialization  (or a vector sufficiently close to 0), we can compute the minimum number of steps it will take for on-policy ADP to converge in this setting by using \ref{eqn:sufficient_cond} and \ref{eqn:neccessary_w_star}:

for sufficiently small . Hence, the bound follows.

\paragraph{A note on the bound.} Since typically RL problems usually assume discount factors  close to 1, one might wonder the relevance is this bound in practice. We show via an example that this is indeed relevant. In particular, we compute the value of this bound for commonly used  and . For a discount , and a minimum probability of  (as it is common to use entropy bonuses that induce a minimum probability of taking each action), this bound is of the order of 

for commonly used horizon lengths of 1000 (example, on the gym benchmarks).
\end{proof}

\begin{corollary}[Extension to replay buffers]
There exists a family of MDPs parameterized by , with ,  and a set of features  satisfying assumption~\ref{assumption:optimal_features}, such that ADP with replay buffer distribution takes  many steps of exact fixed-point iteration for convergence of ADP, if at all convergence happens to an accurate Q-function.
\end{corollary}
\begin{proof}
For replay buffers, we can prove a similar statement as previously. The steps in this proof follow exactly the steps in the proof for the previous theorem.

With replay buffers, the distribution for the projection at iteration  is given by:

Therefore, we can bound the probability of observing any state-action pair similar to Equation~\ref{eqn:max_dsa} as:

with  as defined previously. Note that this inequality is the same as the previous proof, and doesn't change. We next bound the 2-norm of the state-visitation distribution, in this case, the state-distribution in the buffer.

where . The two main inequalities used are thus the same as the previous proof. Now, we can simply follow the previous proof to prove the result.
\end{proof}

\paragraph{Practical Implications.} In this example, both on-policy and replay buffer Q-learning suffer from the problem of exponentially many samples need to reach the optimal Q-function. Even in our experiments in Section~\ref{sec:problem_description}, we find that on-policy distributions tend to reduce errors very slowly, at a rate that is very small. The above bound extends this result to replay buffers as well. 

In our next result, however, we show that an optimal choice of distribution, including DisCor, can avoid the large iteration complexity in this family of MDPs. Specifically, using the errors against , i.e.  can help provide a signal to improve the Q-function such that this optimal distribution / DisCor will take only  many iterations for convergence.

\begin{corollary}[Optimal distributions / DisCor]
\label{thm:discor_suboptimal}
In the tree MDP family considered in Theorem~\ref{thm:exponential}, with linear function approximation for the Q-function, and with Assumption~\ref{assumption:optimal_features} for the features , DisCor takes  many exact iterations for accurate convergence to the optimal Q-function.
\end{corollary}
\begin{proof}
We finally show that the DisCor algorithm, which prioritizes states based on the error in target values, will take  many steps for convergence. Assume that Q-values are initialized randomly, for example via a normal random variable with standard deviation , i.e., , however,  is very small, but is more than 0 () (this proof is still comparable to the proof for on-policy distributions, since Q-values can also be initialized very close to  even in that case, and the proof of Theorem~\ref{thm:app_exp_lower_bound} still remains valid.). 

Now we reason about a run of DisCor in this case.

\paragraph{Iteration 1.} In the first iteration, among all nodes in the MDP, the leaf nodes (depth -1) have  error at the corresponding target values, since an episode terminates once a rollout reaches a leaf node. Hence, the algorithm will assign equal mass to all leaf node states, and exactly update the Q-values for nodes in this level (upto -accuracy).

\paragraph{Iteration 2.} In the second iteration, the leaf nodes at level  have accurate Q-values, therefore, the algorithm will pick nodes at the level , for which the target values, i.e. Q-values for nodes at level , have 0 error along with nodes at level . The algorithm will update Q-values at these nodes at level , while ensuring that the incurred error at the nodes at level  isn't beyond , since nodes at both levels are chosen. Since, the optimal value function  can be represented upto accuracy, we can satisfy this criterion.

\paragraph{Iteration .} In iteration , the algorithm updates Q-values for nodes at level , while also ensuring Q-values for all nodes at a level higher than  are estimated within the range of allowable error, since all the nodes below level  are updated. This is feasible since,  is expressible with accuracy within the linear function class chosen. 

This iteration process continues, and progress level by level, from the leaves (level ) to the root (level ). At each iteration Q-values for all states at the same level, and below are learned together. Since learning progresses in a ``one level at-a-time'' fashion, with guaranteed correct target values (i.e. target values are equal to the optimal Q-function ) for any update that the algorithm performs, it would take at most  many iterations (for example, multiple passes through the depth of the tree) for -accurate convergence to the optimal -function.
\end{proof}

\section{Extended Related Work}
\label{app:related_work_extended}

\paragraph{Error propagation in ADP.} A number of prior works have analysed error propagation in ADP methods. Most work in this area has been devoted to analysing how errors in Bellman error minimization propagate through the learning process of the ADP algorithm, typically focusing on methods such as fitted Q-iteration (FQI)~\citep{Riedmiller2005} or approximate policy iteration~\citep{perkins2002api}. Prior works in this area assume an abstract error model, and analyze how errors propagate. Typically these prior works only limitedly explore reasons for error propagation or present methods to curb error propagation. \cite{munos2003api} analyze error propagation in approximate policy iteration methods using quadratic norms. \cite{munos2005error} analyze the propagation of error across iterations of approximate value iteration (AVI) for -norm . \cite{munos2008finite} provide finite sample guarantees of AVI using error propagation analysis. Similar ideas have been used to provide error bounds for a number of different methods -- \cite{farahmand2010error,scherrer15a,Lesner2013TightPB,scherrer_comparison} and many more. In this work, we show that ADP algorithms suffer from an absence of corrective feedback, which arises because the data distribution collected by an agent is insufficient to ensure that error propagation is eventually corrected for. We further propose an approach, DisCor, which can be used in conjunction with modern deep RL methods. 

\paragraph{Offline / Batch Reinforcement Learning.} Our work bears similarity to the recent body of literature on batch, or offline reinforcement learning~\citep{kumar19bear,fujimoto19a,wu2019behavior}. All of these works exploit the central idea of constraining the policy to lie in a certain neighborhood of the behavior, data-collection policy. While \cite{kumar19bear} show that this choice can be motivated from the perspective of error propagation, we note that there are clear differences between our work and such prior works in batch RL. First, the problem statement of batch RL requires learning from completely offline experience, however, our method learns online, via on-policy interaction. While error propagation is a reason behind incorrect Q-functions in batch RL, we show that such error accumulation also happens in online reinforcement, which results in a lack of corrective feedback.  

\paragraph{Generalization effects in deep Q-learning.} There are a number of recent works that theoretically analyze and empirically demonstrate that certain design decisions for neural net architectures used for Q-learning, or ADP objectives can prove to be significant in deep Q-learning. For instance, \cite{martha2018sparse} point out that sparse representations may help Q-learning algorithms, which links back to prior literature on state-aliasing and destructive interference. \cite{Achiam2019TowardsCD} uses an objective inspired from the neural tangent kernel (NTK)~\citep{ntk} to ``cancel'' generalization effects in the Q-function induced across state-action pairs to mimic tabular and online Q-learning. Our approach, DisCor, can be interpreted as \textit{only} indirectly affecting generalization via the target Q-values for state-action pairs that will be used as bootstrap targets for the Bellman backup, which are expected to be accurate with DisCor, and this can aid generalization, similar to how generalization can be achieved via abstention from training on noisy labels in supervised learning~\citep{absention}.

\paragraph{Replay Buffers and Generalization.} There are some prior works performing analytical studies on the size of the replay buffer~\citep{zhang2017deeper,liu2018effects}, which propose that larger replay buffers might hurt training with function approximation. Partly this problem goes away if smaller buffers are used, or if the algorithm chooses to replay recent experience more often. Our work indicates an absence of corrective feedback problem -- online data collection might not be able to correct errors in the Q-function -- which is distinct from the size of the replay buffer.   

\section{Experimental Details}
\label{app:exp_details}
In this section, we provided experimental details, such as the DisCor algorithm in practice (Section~\ref{sec:discor_practice}), and the hyperparameter choices (Section~\ref{sec:app_exp_details}).

\subsection{DisCor in Practice}
\label{sec:discor_practice}
In this section, we provide details on the experimental setup and present the pseudo-code for the practical instantiation of our algorithm, DisCor.

The pseudocode for the practical algorithm is provided in Algorithm~\ref{alg:practical_alg}. Like any other ADP algorithm, such as DQN or SAC, our algorithm maintains a pair of Q-functions -- the online Q-network  and a target network . For continuous control domains, we use the clipped double Q-learning trick~\cite{pmlr-v80-fujimoto18a}, which is also referred to as the ``twin-Q'' trick, and it further parametrizes another pair of online and target Q-functions, and uses the minimum Q-value for backup computation. In addition to Q-functions, in a continuous control domain, we parametrize a separate policy network  similar to SAC. In a discrete action domain, the policy is just given by a greedy maximization of the online Q-network. 

DisCor further maintains a model for accumulating errors  parameterized by  and the corresponding target error network . In the setting with two Q-functions, DisCor models two networks, one for modelling error in each Q-function. At every step, a few (depending upon the algorithm) gradient steps are performed on  and , and  -- if it is explicitly modeled, for instance in continuous control domains. This is a modification of generalized ADP Algorithm~\ref{alg:fqi} and the corresponding DisCor version (Algorithm~\ref{alg:discor}), customized to modern deep RL methods.

\begin{algorithm}[t!]
\small
\caption{\textbf{DisCor: Deep RL Version}}
\label{alg:practical_alg}
\begin{algorithmic}[1]
    \STATE Initialize online Q-network , target Q-network, , error network , target error network , initial distribution , a replay buffer  and a policy , number of gradient steps , target network update rate , initial temperature for computing weights , .
    \FOR{step  in \{1, \dots, \}}
        \STATE Collect  samples using , add them to replay buffer , sample 
        \STATE Evaluate  and  on samples .
        \STATE Compute target values for  and  on samples:
        
        
        \STATE {Compute  using Equation~\ref{eqn:importance_weights} with temperature } 
        \STATE Take  gradient steps on the Bellman error for training  weighted by .
        
        \STATE {Tale  gradient steps to minimize unweighted (regular) Bellman error for training .
        }
        \STATE Update the policy  if it is explicitly modeled.
         
        \STATE Update target networks using soft updates (SAC), hard updates (DQN)
            
           
        \STATE Update temperature hyperparameter for DisCor:
        
    \ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Experimental Hyperparameter Choices}
\label{sec:app_exp_details}
We finally specify the hyperparameters we used for our experiments. These are as follows:
\begin{itemize}
    \item \textit{Temperature :} DisCor mainly introduces one hyperparameter, the temperature  used to compute the weights  in Equation~\ref{eqn:importance_weights}. As shown in Line 11 of Algorithm~\ref{alg:practical_alg}, DisCor maintains a moving average of the temperatures and uses this average to perform the weighting. This removes the requirement for tuning the temperature values at all. For initialization, we chose  for all our experiments, irrespective of the domain or task. 
    \item \textit{Architecture for :} For the design of the error network, , we utilize a network with 1 extra hidden layer than the corresponding Q-network. For instance, in metaworld domains, the standard Q-network used was [256, 256, 256] in size, and thus we used an error network of size: [256, 256, 256, 256], and for MT10 tasks we used [160, 160, 160, 160, 160, 160] sized Q-networks~\citep{yu2020gradient} and 1-extra layer error networks .
    \item \textit{Target net updates:} We performed target net updates for  in the same manner as standard Q-functions, in all domains. For instance, in MetaWorld, we update the target network  with a soft update rate of 0.005 at each environment step, as is standard with SAC~\cite{haarnoja2018sacapps}, whereas in DQN~\citep{Mnih2015}, we use hard target resets.
    \item \textit{Learning rates for :} These were chosen to be the same as the corresponding learning rate for the Q-function, which is  for SAC and  for DQN.
    \item \textit{Official Implementation repositories used for our work:} 
        \begin{enumerate}
            \item Soft-Actor-Critic: \url{https://github.com/rail-berkeley/softlearning/}
            \item Dopamine~\citep{castro18dopamine}: Offical DQN implementation \url{https://github.com/google/dopamine}, and the baseline DQN numbers were reported from the logs available at: \url{https://github.com/google/dopamine/tree/master/baselines}
            \item Tabular environments~\citep{fu19diagnosing}: \url{https://github.com/justinjfu/diagnosing_qlearning}
        \end{enumerate}
    \item We perform self-normalized importance sampling across a batch, instead of regular importance sampling, since that gives rise to more stable training, and suffers less from the curse of variance in importance sampling.
    \item \textit{Seeds}: In all our experiments, we implemented our methods on top of the official repositories, ran each experiment for 4 randomly chosen seeds from the interval, , in Meta-World, OpenAI gym and tabular environments. For DQNs on atari, we were only able to run 3 seeds for each game for our method, however, we found similar performances, and less variance across seeds, as is evident from the variance bands in the corresponding results. For baseline DQN, we just used the log files provided by the dopamine repository for our results. 
\end{itemize}  

\section{Additional Experiments}
\label{sec:additional_exps}
We now present some additional experimental results which could not have been presented in Section~\ref{sec:experiments}. 

\subsection{Tabular Environment Analysis}
\label{sec:app_exps_gridworld}
\paragraph{Environment Setup.} We used the suite of tabular environments from from \cite{fu19diagnosing}, which provides a suite of 8 tabular environments and a suite of algorithms based on fitted Q-iteration~\citep{Riedmiller2005}, which forms the basis of modern deep RL algorithms based on ADP. We evaluated performance on different variants of the  gridworld provided, with different reward styles (sparse, dense), different observation functions (one-hot, random features, locally smooth observations), and different amounts of entropy coefficients (0.01, 0.1). We evaluated on five different kinds of environments: grid16randomobs, grid16onehot, grid16smoothobs, grid16smoothsparse, grid16randomsparse -- which cover a wide variety of combinations of feature and reward types. We also evaluated on CliffWalk, Sparsegraph and MountainCar MDPs in Figures~\ref{fig:app_fig_exact} and \ref{fig:app_fig_sampled}. 

\paragraph{Sampling Modes.} We evaluated in two modes -- (1) exact mode, in the absence of sampling error, where an algorithm is provided with all transitions in the MDP and simply chooses a weighting over the states rather than sampling transitions from the environment, and (2) sampled mode, which is the conventional RL paradigm, where the algorithm performs online data collection to collect its own data.  

\begin{figure}
    \centering
\includegraphics[width=0.6\linewidth]{figures/gridworld_summary_no_sampling_all_methods_cropped.png}
    \caption{\footnotesize{Performance of different methods: DisCor (blue), DisCor (oracle) (red), Replay buffer Q-learning (green, on-policy (grey) and prioritized updates (orange), across different environments measured in terms of smooth normalized returns in the \textbf{exact} setting with all transitions. Note that DisCor and DisCor (oracle) generally tend to perform better.}}
    \label{fig:app_fig_exact}
\end{figure}

\paragraph{Setup for Figures~\ref{fig:visitation_doesnt_correct_eror} and \ref{fig:suboptimal_conv}.} For Figures~\ref{fig:visitation_doesnt_correct_eror} and \ref{fig:suboptimal_conv}, we used the grid16randomobs MDP (which is a  gridworld with randomly initialized vectors as observations), with an entropy penalty of 0.01 to the policy. For Figure~\ref{fig:instability} we used the grid16smoothobs MDP with locally smooth observations, with an entropy penalty of 0.01 as well, and for Figure~\ref{fig:sparse_reward}, we used grid16smoothsparse environment, with sparse reward and smooth features. 

\paragraph{Results.} We provide some individual environment performance curves showing the smoothed normalized return achieved at the end of 300 steps of training in both exact (Figure~\ref{fig:app_fig_exact}) and sampled (Figure~\ref{fig:app_fig_sampled}) settings. We also present some individual-environment learning curves for these environments comparing different methods in both exact (Figure~\ref{fig:exact_fqi_runs}) and sampled (Figure~\ref{fig:sampled_fqi_runs}). 

\begin{figure}
    \centering
\includegraphics[width=0.6\linewidth]{figures/gridworld_summary_sampling_all_methods_but_uniform.png}
    \caption{\footnotesize{Performance of different methods: DisCor (blue), DisCor (oracle) (red), Replay buffer Q-learning (green) and prioritized updates (orange). across different environments measured in terms of smooth normalized return with \textbf{sampled} transitions. Note that DisCor and DisCor (oracle) generally tend to perform better.}}
    \label{fig:app_fig_sampled}
\end{figure}

\begin{figure*}
    \centering
    \includegraphics[width=0.19\linewidth]{images/plot_gridworld_new/grid16randomobs_ent0_01_exact_new.pdf}
    \includegraphics[width=0.19\linewidth]{images/plot_gridworld_new/grid16onehot_ent0_01_exact_new.pdf}
     \includegraphics[width=0.19\linewidth]{images/plot_gridworld_new/grid16smoothobs_ent0_01_exact_new.pdf}
     \vline
    \includegraphics[width=0.19\linewidth]{images/plot_gridworld_new/grid16smoothsparse_ent0_01_exact_new.pdf}
    \includegraphics[width=0.19\linewidth]{images/plot_gridworld_new/grid16randomsparse_ent0_01_exact_new.pdf}
    \includegraphics[width=0.75\linewidth]{images/plots_gridworlds/legendforgridworldspng.png}
    \caption{\footnotesize{Learning curves for different algorithms in the \textbf{exact} setting. Note that DisCor (blue) and DisCor (oracle) (red) are generally the best algorithms in these settings. Replay Buffers (green) help over on-policy (pink) distributions. Prioritizing transitions based on high Bellman error (orange) is performant in some cases, but hurts in the other cases -- it is especially slow in cases with sparse rewards, note the speed of learning on grid16randomsparse and grid16smoothsparse (\textbf{right} of the vertical line) environments.}}
    \label{fig:exact_fqi_runs}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.19\linewidth]{images/plots_gridworlds/new_sampled_results/grid16randomobs_ent0_01returns_normalizedsample_512.pdf}
    \includegraphics[width=0.19\linewidth]{images/plots_gridworlds/new_sampled_results/grid16onehot_ent0_01returns_normalizedsample_256.pdf}
    \includegraphics[width=0.19\linewidth]{images/plots_gridworlds/new_sampled_results/grid16smoothobs_ent0_01returns_normalizedsample_256.pdf}
    \vline
    \includegraphics[width=0.19\linewidth]{images/plots_gridworlds/new_sampled_results/grid16randomsparse_ent0_01returns_normalizedsample_512.pdf}
    \includegraphics[width=0.19\linewidth]{images/plots_gridworlds/new_sampled_results/grid16smoothsparse_ent0_01returns_normalizedsample_512.pdf}
    \includegraphics[width=0.5\linewidth]{images/plots_gridworlds/legend_for_gridworlds_sampled.png}
    \caption{\footnotesize{Learning curves for different algorithms in the \textbf{sampled} setting. Note that DisCor and DisCor (oralce) anre generally the best algorithms in these settings. Replay Buffers (green) help over on-policy (gray) distributions, but may the algorithm may still fail to reach optimal return. Prioritizing for high Bellman error (PER) may fail to learn in sparse-reward tasks as is evident from the curves for sparse reward environments (\textbf{right} of the vertical line).}}
    \label{fig:sampled_fqi_runs}
\end{figure*}

\vspace{-10pt}
\subsection{MetaWorld Tasks}
\label{sec:app_exps_metaworld}
In this section, we first provide a pictorial description of the six hard tasks we tested on from meta-world, where SAC usually does not perform very well. Figure \ref{fig:metaworld_tasks} shows these tasks. We provide the trends for average return achieved during evaluation (not the success rate as shown in Figure \ref{fig:success_rates_metaworld} in Section \ref{sec:experiments}) for each of the six tasks. Note that DisCor clearly outperforms both the baseline SAC and the prior method PER in all six cases, achieving nearly \textbf{50\%} more than the returns achieved by SAC. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\linewidth]{images/metaworld_results/metaworld_tasks.pdf}
    \caption{\footnotesize{Visual description of the six MetaWorld tasks used in our experiments in Section~\ref{sec:experiments}. Figures taken from \cite{yu2019meta}.}}
    \label{fig:metaworld_tasks}
\end{figure}

\begin{figure*}
    \centering
    \includegraphics[width=0.25\linewidth]{images/metaworld_results/pull_with_stick_False_again.pdf}
    \includegraphics[width=0.25\linewidth]{images/metaworld_results/push_with_wall_False_again.pdf}
    \includegraphics[width=0.25\linewidth]{images/metaworld_results/turn_dial_False_again.pdf} \\
    \includegraphics[width=0.25\linewidth]{images/metaworld_results/hammer_False_again.pdf}
    \includegraphics[width=0.25\linewidth]{images/metaworld_results/push_with_stick_False_again.pdf}
    \includegraphics[width=0.25\linewidth]{images/metaworld_results/insert_peg_side_False_again.pdf}
    \caption{\footnotesize{Evaluation average return achieved by DisCor (blue), SAC (green) and PER (orange) on six Metaworld benchmarks. From left to right: pull stick, push with wall, push with stick, turn dial, hammer and insert peg side tasks. Note that DisCor clearly achieves better returns or learns faster in most of the tasks.}}
    \label{fig:app_returns_metaworld}
\end{figure*}

\subsection{OpenAI Gym Benchmarks}
\label{sec:app_mujoco_benchmarks}
Here we present an evaluation on the standard OpenAI continuous control gym benchmark environments. Modern ADP algorithms such as SAC can already solve these tasks easily, without any issues, since these algorithms have been tuned on these tasks. A comparison of the three algorithms DisCor, SAC and PER, on three of these benchmark tasks is shown in Figure~\ref{fig:app_mujoco_results}. We note that in this case, all the algorithms are roughly comparable to each other. For instance, DisCor performs better than SAC and PER on Walker2d, however, is outperformed by SAC on Ant. 

\paragraph{Stochastic reward signals.} That said, we also performed an experiment to verify the impact of stochasticity, such as noise in the reward signal, on the DisCor algorithm as compared to other baseline algorithms like SAC and PER. Analogous the diagnostic tabular experiments on low signal-to-noise ratio environments, such as those with sparse reward, we would expect a baseline ADP method to be impacted more due to an absence of corrective feedback in tasks with stochastic reward noise, since a noisy reward effectively reduces the signal-to-noise ratio. We would also expect a method that ensures corrective feedback to perform better. 

In order to test this hypothesis, we created stochastic reward tasks out of the OpenAI gym benchmarks. We modified the reward function  in these gym tasks to be equal to:

and the agent is only provided these noisy rewards during training. However, we only report the deterministic ground-truth reward during evaluation. 
We present the results in Figure~\ref{fig:sac_noisy_results}. Observe that in this scenario, DisCor emerges as the best performing algorithm on these tasks, and outperforms other baselines SAC and PER both in terms of asymptotic performance (example, HalfCheetah) and sample efficiency (example, Ant).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.25\linewidth]{images/mujoco_benchmarks/HalfCheetah-v2_False.pdf}
    \includegraphics[width=0.25\linewidth]{images/mujoco_benchmarks/Ant-v2_False.pdf}
    \includegraphics[width=0.25\linewidth]{images/mujoco_benchmarks/Walker2d-v2_False.pdf}
    \caption{\footnotesize{Peformance of DisCor, SAC and PER on gym benchmarks. On an average, all methods perform roughly similarly on these settings.}}
    \label{fig:app_mujoco_results}
\end{figure}

\begin{figure}[H]
    \centering
        \includegraphics[width=0.25\linewidth]{images/mujoco_benchmarks/new_HalfCheetah-v2_True.pdf}
        \includegraphics[width=0.25\linewidth]{images/mujoco_benchmarks/new_Ant-v2_True.pdf}
        \includegraphics[width=0.25\linewidth]{images/mujoco_benchmarks/new_Walker2d-v2_True.pdf}
    \caption{\footnotesize{Performance of DisCor, SAC and PER on continuous control gym benchmarks \textit{with stochastic reward noise}. Note that DisCor learns slightly faster and performs better than SAC and PER on these stochastic problems.}}
    \label{fig:sac_noisy_results}
\end{figure}

\subsection{MT10 Multi-Task Experiments}
\label{sec:app_multi_task}
In this section, we present the trend of returns, as a learning curve and as a comparative histogram (at 1M environment steps of training) for the multi-task MT10 benchmark, extending the results shown in Section~\ref{sec:multi-task}, Figure~\ref{fig:mt_results}. These plots are shown in Figure~\ref{fig:app_mt10}. Observe that DisCor achieves more than \textbf{30\%} of the return of SAC, and obtains an individually higher value of return on more tasks. 

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{.44\linewidth}
        \centering
        \includegraphics[width=0.65\linewidth]{images/metaworld_results/mt_10_newFalse.pdf}
        \caption{\footnotesize{Average task return}}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{.47\linewidth}
        \centering
        \includegraphics[width=0.7\linewidth]{images/metaworld_results/discor_mt10_sac_1000.png}
        \caption{\footnotesize{Per-task return at 1M steps}} 
    \end{subfigure}
    \caption{\footnotesize{Performance of DisCor and SAC on the MT10 benchmark. Returns for DisCor are higher than SAC by around \textbf{30\%}; (2) DisCor achieves a non-trivial return on \textbf{7/10} tasks after 1000k steps, as compared to \textbf{3/10} for unweighted SAC, similar to the trend at 500k steps shown in Figure~\ref{fig:mt_results}.}}
    \label{fig:app_mt10}
\end{figure}


\subsection{MT50 Multi-Task Experiments}
\label{sec:app_multi_task_ablation}
We further evaluated the performance of DisCor on the multi-task MT50 benchmark~\citep{yu2019meta}. This is an extremely challenging benchmark where the task is to learn a single policy that can solve 50 tasks together, with the same evaluation protocol as previously used in the MT10 experiments (Section~\ref{sec:multi-task} and Appendix~\ref{sec:app_multi_task}). We present the results (average task return and average success rate) in Figures~\ref{fig:app_mt50}. Note that while SAC tends to saturate/plateau in between 4M - 8M steps, accounting for corrective feedback via the DisCor algorithm makes the algorithm continue learning in that scenario too.    

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{.47\linewidth}
        \centering
        \includegraphics[width=0.65\linewidth]{images/metaworld_results/mt_50_newFalse.pdf}
        \caption{\footnotesize{Average task return}}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{.47\linewidth}
        \centering
        \includegraphics[width=0.65\linewidth]{images/metaworld_results/mt_50_newTrue.pdf}
        \caption{\footnotesize{Average success rate}} 
    \end{subfigure}
    \caption{\footnotesize{Performance of DisCor and SAC on the MT50 benchmark. Note that, DisCor clearly keeps learning unlike SAC which tends to plateau for about 3M steps in the middle (the stretch between 4M and 7M steps on the x-axis, where SAC exhibits a small gradient in the learning progress, whereas DisCor continuously keeps learning).}}
    \label{fig:app_mt50}
\end{figure}
\subsection{Comparison with AFM}
\label{sec:afm_comparison}
In this section, we present a comparison of DisCor and AFM~\citep{fu19diagnosing}, a prior method similar to prioritized experience replay on the MuJoCo gym benchmarks. We find that DisCor clearly outperforms AFM in these scenarios. We present these results in Figure~\ref{fig:app_discor_vs_afm} where the top row presents results in the case of regular gym benchmarks, and the bottom row presents results in the case of gym benchmarks with stochastic reward noise.

\subsection{DQN with multi-step returns}
\label{sec:dqn_multi_step}
N-step returns with DQN are hypothesized to stabilize learning since updates to the Q-function now depends on reward values spanning multiple steps, and the coefficient of the bootstrapped Q-value is , which is exponentially smaller than  used conventionally in Bellman backups, implying that the error accumulation process due to incorrect targets is reduced. Thus, we perform a comparison of DisCor and DQN with n-step backups, where  was chosen to be , , in accordance with commonly used multi-step return settings for Atari games. We present the average return obtained by DisCor and DQN (+n-step), with sticky actions, in Table~\ref{table:dqn_discor_nstep}. We clearly observe that DisCor outperforms DQN with 3-step returns in all three games evaluated on. We also observe that n-step returns applied with DisCor also outperform n-step returns applied with DQN, indicating the benefits of using DisCor even when other techniques, such as n-step returns are used.

\begin{table}[t]
\centering
\label{table:atari_multistep}
\begin{tabular}{c r r r}
\hline
\textbf{Game} & \textbf{n-step DQN} & \textbf{DisCor} & \textbf{n-step DisCor}\\
 &  & (Regular) &  \\
\hline
\hline
\textbf{Pong} & 17 & {17} & \colorbox{blue!30}{\textbf{19}} \\
\textbf{Breakout} & 37 & \textbf{175} & \colorbox{blue!30}{47} \\
\hline
\end{tabular}
\caption{\footnotesize{Average Performance of DQN + 3-step returns, DisCor and Discor + 3-step returns on Pong and Breakout at 60M steps into training, rounded off to the nearest integer. Note that DisCor clearly outperforms DQN with multi-step returns. We also find that adding n-step returns to DisCor can hurt, for instance, on Breakout, where the same hurts with DQN as well (for comparison, see Figure~\ref{fig:atari_results} in the main paper), however, we still observe that DisCor, when applied with multi-step returns performs better than DQN with multi-step returns as well, indicating the benefits of DisCor even when methods such as multi-step returns are used.}} 
\label{table:dqn_discor_nstep}
\end{table}

\begin{figure}[H]
    \centering
        \includegraphics[width=0.25\linewidth]{images/mujoco_benchmarks/new_with_afmHalfCheetah-v2_False.pdf}
        \includegraphics[width=0.25\linewidth]{images/mujoco_benchmarks/new_with_afmAnt-v2_False.pdf}
        \includegraphics[width=0.25\linewidth]{images/mujoco_benchmarks/new_with_afmWalker2d-v2_False.pdf}\\
        \vspace{4pt}
        \includegraphics[width=0.25\linewidth]{images/mujoco_benchmarks/new_with_afmHalfCheetah-v2_True.pdf}
        \includegraphics[width=0.25\linewidth]{images/mujoco_benchmarks/new_with_afmAnt-v2_True.pdf}
        \includegraphics[width=0.25\linewidth]{images/mujoco_benchmarks/new_with_afmWalker2d-v2_True.pdf}
    \caption{\footnotesize{Performance of DisCor, SAC, PER and AFM on continuous control gym benchmarks (top row) and gym benchmarks \textit{with stochastic reward noise} (bottom row). Note that DisCor clearly out-performs AFM in both scenarios, on all three benchmarks tested on.}}
    \label{fig:app_discor_vs_afm}
\end{figure}

\begin{figure*}[t!]
\centering
    \begin{lstlisting}[language=Python]
    def _init_critic_update_with_dist(self):
        """Update critic with distribution weighting, 
           and update \delta_\phi using recursive update. """
        next_actions = self._policy.actions([self._next_observations_ph])
        
        ## Compute errors at next state, and an action from the policy
        qf_pred_errs = self._error_fns([self._next_observations_ph, next_actions])
            
        ## error_model_tau_ph: moving mean of the error values over batches
        err_logits = -tf.stop_gradient(
            self._discount * qf_pred_errs / self._error_model_tau_ph)

        Q_target = tf.stop_gradient(self._get_Q_target())
        Q_values = self._Q([self._observations_ph, self._actions_ph])
        
        ## Compute importance sampled loss, also perform self-normalized sampling
        loss, weights = importance_sampled_loss(
            labels=Q_target, predictions=Q_values,
            weights=err_logits, weight_options='self_normalized')
        
        ## Train Q-function
        Q_training_ops = tf.contrib.layers.optimize_loss(loss, learning_rate=self._Q_lr,
            optimizer=self._Q_optimizer, variables=self._Q.trainable_variables)
        training_ops.update({'Q': tf.group(Q_training_ops)})
        
        ## Training the error function
        err_values = self._error_fns([self._observations_ph, self._actions_ph])
        
        ## Mean Bellman error used to compute target values for error
        bellman_errors = tf.abs(Q_values - Q_target)
        err_targets = tf.stop_gradient(self._get_error_target(bellman_errors))
        
        ## This is used to update the moving mean, self._error_model_tau_ph
        self._mean_error_values = tf.reduce_mean(err_values)

        ## Simple mean squared error loss for \delta_\phi
        err_losses =  tf.losses.mean_squared_error(
            labels=err_targets, predictions=err_values, weights=0.5)
        
        ## Update error function: \delta_\phi
        err_training_ops = tf.contrib.layers.optimize_loss(err_losses,
            learning_rate=self._dist_lr, 
            optimizer=self._err_optimizer, variables=self._error_fns.trainable_variables)
        training_ops.update({'Error': tf.group(err_training_ops)})
    \end{lstlisting}
    \caption{\footnotesize{Code for training the error function , and modified training for the Q-function  using  to get weights  for training. Code written in convention with regular Tensorflow guidelines, in the same style as the official SAC implementation~\citep{haarnoja2018sacapps}.}}
\label{fig:code_discor}
\end{figure*}

\subsection{Code for the Method}
\label{sec:code}
The code is shown in Figure~\ref{fig:code_discor}. It is a simplified version of the code from our implementation of DisCor on top of the official SAC repository~\citep{haarnoja2018sacapps}.




 

\end{document}
