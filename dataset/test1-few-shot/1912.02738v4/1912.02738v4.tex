

\documentclass{article}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} 

\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}


\usepackage{url}            \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{color}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{placeins}
\usepackage{threeparttable}
\usepackage{bm}
\usepackage{soul}
\usepackage[normalem]{ulem}
\usepackage{pifont}\usepackage{nicefrac}
\usepackage{cleveref}
\usepackage{paralist}
\usepackage{makecell}
\usepackage{caption}
\usepackage{titling}
\usepackage{authblk}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{soul}
\usepackage{hyperref}
\usepackage[colorinlistoftodos]{todonotes}


\usepackage[inline]{enumitem}

\newtheorem{proposition}{Proposition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\usepackage[accepted]{icml2020}

\icmltitlerunning{MetaFun: Meta-Learning with Iterative Functional Updates}

\usepackage[acronym,smallcaps,nowarn,section,nogroupskip,nonumberlist]{glossaries}

\glsdisablehyper 

\newacronym{CNP}{cnp}{Conditional Neural Process}
\newacronym{NP}{np}{Neural Process}
\newacronym{ANP}{anp}{Attentive Neural Process}
\newacronym{MAML}{maml}{Model Agnostic Meta Learning}
\newacronym{LEO}{leo}{Latent Embedding Optimisation}
\newacronym{RKHS}{rkhs}{Reproducing kernel Hilbert space}
\newacronym{GP}{gp}{Gaussian Process}

\newacronym{KRR}{krr}{Kernel Ridge Regression}
\newcommand{\KRR}[1]{\operatorname{\textsc{krr}}\left(#1\right)}

\newacronym{DP}{dp}{Dot-Product Attention}
\newcommand{\DP}[1]{\operatorname{\textsc{dp}}\left(#1\right)}

\newacronym{KG}{kg}{Kernel Gradient}
\newcommand{\KG}[1]{\operatorname{\textsc{kg}}\left(#1\right)}

\newacronym{DFP}{dfp}{Dot-Product Functional Pooling}
\newcommand{\DFP}[1]{\operatorname{\textsc{dfp}}\left(#1\right)}

\newacronym{KFP}{kfp}{Kernel Functional Pooling}
\newcommand{\KFP}[1]{\operatorname{\textsc{kfp}}\left(#1\right)}

\newacronym{MLP}{mlp}{multi-layer perceptron}
\newcommand{\MLP}[1]{\operatorname{\textsc{mlp}}\left(#1\right)} 
\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\newcommand{\jin}[1]{{\textcolor{cyan}{\textbf{jin}: #1}}}

\begin{document}

\twocolumn[
\icmltitle{MetaFun: Meta-Learning with Iterative Functional Updates}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Jin Xu}{stats}
\icmlauthor{Jean-Francois Ton}{stats}
\icmlauthor{Hyunjik Kim}{dm}
\icmlauthor{Adam R. Kosiorek}{stats,rob}
\icmlauthor{Yee Whye Teh}{stats}
\end{icmlauthorlist}

\icmlaffiliation{stats}{Department of Statistics, University of Oxford, Oxford, United Kingdom}
\icmlaffiliation{dm}{Google DeepMind, London, United Kingdom}
\icmlaffiliation{rob}{Applied AI Lab, Oxford Robotics Institute, University of Oxford, Oxford United Kingdom}

\icmlcorrespondingauthor{Jin Xu}{jin.xu@stats.ox.ac.uk}


\icmlkeywords{Meta-learning, Few-shot, Kernel, Attention, Functional gradient descent}

\vskip 0.3in
]

\printAffiliationsAndNotice{} 

\begin{abstract}
We develop a functional encoder-decoder approach to supervised meta-learning, where labeled data is encoded into an infinite-dimensional functional representation rather than a finite-dimensional one. Furthermore, rather than directly producing the representation, we learn a neural update rule resembling functional gradient descent which iteratively improves the representation. The final representation is used to condition the decoder to make predictions on unlabeled data. Our approach is the first to demonstrates the success of encoder-decoder style meta-learning methods like conditional neural processes on large-scale few-shot classification benchmarks such as miniImageNet and tieredImageNet, where it achieves state-of-the-art performance.
\end{abstract}

\def\xx{\textbf{x}}
\def\yy{\textbf{y}}
\def\zz{\textbf{z}}
\def\rr{\textbf{r}}
\def\ff{\textbf{f}}
\def\uu{\textbf{u}}
\def\hh{\textbf{h}}
\def\EE{\mathbb{E}}
\def\CC{\mathbf{C}}
\def\TT{\mathbf{T}}
\def\loss{\mathcal{L}}
\def\reg{\mathcal{R}}
\def\KL{\mathsf{KL}}
\def\ptrain{p_\text{train}}


\section{Introduction} \label{sec:introduction}

The goal of meta-learning is to be able to generalise to new tasks from the same task distribution as the training tasks. In supervised meta-learning, a task can be described as making predictions on a set of unlabelled data points (\emph{target}) by effectively learning from a set of data points with labels (\emph{context}).
Various ideas have been proposed to tackle supervised meta-learning from different perspectives \citep{andrychowicz2016learning,ravi2016optimization,finn2017model,koch2015siamese,snell2017prototypical,vinyals2016matching,santoro2016meta,rusu2018meta}. In this work, we are particularly interested in a family of meta-learning models that use an encoder-decoder pipeline, such as Neural Processes \citep{garnelo2018conditional,garnelo2018neural}. The encoder is a permutation-invariant function on the context that summarises the context into a task representation, while the decoder produces a predictive model for the targets, conditioned on the task representation. The objective of meta-learning is then to learn the encoder and the decoder such that the produced predictive model generalises well to the targets of new tasks. 


Previous works in this category such as the \gls{CNP} and the \gls{NP} \citep{garnelo2018conditional,garnelo2018neural} use sum-pooling operations to produce finite-dimensional, vectorial, task representations. In this work, we investigate the idea of summarising tasks with infinite-dimensional functional representations.
Although there is a theoretical guarantee that sum-pooling of instance-wise representations can express any set function (\textit{universality}) \citep{zaheer2017deep,bloem2019probabilistic}, in practice \gls{CNP} and \gls{NP} tend to underfit the context \citep{kim2019attentive}. 
This observation is in line with the theoretical finding that for universality, the dimension of the task representation should be at least as large as the cardinality of the context set, if the encoder is a smooth function \citet{wagstaff2019limitations}. 
We develop a method that explicitly uses functional representations. Here the effective dimensionality of the task representation grows with the number of context points, which addresses the aforementioned issues of fixed-dimensional representations.
Moreover, in practice it is difficult to model interactions between data points with only sum-pooling operations. This issue can be partially addressed by inserting modules such as relation networks \citep{sung2018learning,rusu2018meta} or set transformers \citep{lee2019set} before sum-pooling. However, only within-context but not context-target interactions can be modelled by these modules.
The construction of our functional representation involves measuring similarities between all data points, which naturally contains information regarding interactions between elements in either the context or the target.



Furthermore, rather than producing the functional representation in a single pass, we develop an approach that \textit{learns} iterative updates to encode the context into the task representation.
In general, learning via iterative updates is often easier than directly learning the final representation, because of the error-correcting opportunity at each iteration. 
For example, an iterative parameterisation of the encoder in Variational Autoencoders (VAEs) has been demonstrated to be effective in reducing the amortisation gap \citep{marino2018iterative}, 
while in meta-learning, both learning to learn methods \citep{andrychowicz2016learning,ravi2016optimization} and \gls{MAML} \citep{finn2017model} use iterative updating procedures to adapt to new tasks, although these update rules operate in parameter space rather than function space.
Therefore, it is reasonable to conjecture that iterative structures are favourable inductive biases for the task encoding process.


In summary,
the primary contribution of this work is a meta-learning approach that learns to summarise a task using a functional representation constructed via iterative updates. 
We apply our approach to solve  meta-learning problems on both regression and classification tasks, and achieve state-of-the-art performance on heavily benchmarked datasets such as miniImageNet \citep{vinyals2016matching} and tieredImageNet \citep{ren2018meta}, which has never been demonstrated with encoder-decoder meta-learning methods without \gls{MAML}-style gradient updates.
We also conducted an ablation study to understand the effects of the different model components.

\section{MetaFun} \label{sec:metafun}

\begin{figure}
  \centering
    \includegraphics[width=0.99\linewidth]{metafun-cartoon.png}
\caption{To illustrate the iterative procedure in MetaFun, we consider a simpler case where our functional representation is just a predictor for the task. (A) The figure depicts a 1D regression task with the current predictor. (B) Local updates are computed by evaluating the functional representation (the current predictor) on the context inputs, and comparing it to the corresponding context outputs. Here we simply measure differences between evaluations (predictions) and outputs. (C) We apply functional pooling to aggregate local updates into a global functional update, which generalises the local updates to the whole input domain. (D) The functional update is applied to the current functional representation with a learning rate .}
\label{fig:cartoon}
\end{figure}


Meta-learning, or learning to learn, leverages past experiences to quickly adapt to tasks  drawn iid from some task distribution.
In supervised meta-learning, a task  takes the form of , where  are inputs,  outputs,  is the loss function to be minimised,  is the context, and  is the target. We consider the process of learning as constructing a predictive model using the task context and refer to the mapping from context  to a predictive model  as the \emph{learning model} parameterised by . In our formulation, the objective of meta-learning is to optimise the learning model such that the expected loss on the target under  is minimised, formally written as:

where both ,  come from task .

\begin{figure*}
  \centering
    \includegraphics[width=0.99\linewidth]{metafun-main.png}
\caption{This figure illustrates the iterative computation of functional representation in MetaFun. At each iteration, we first evaluate the current functional representation at both context and target points. Then the shared local update function  takes in each context point and the corresponding evaluation as inputs, and produces local update . Next, we apply (kernel-based or attention-based) functional pooling  to aggregate local updates  into a functional update , which for each query is a linear combination of local updates  weighted by similarities between this query and all keys. Finally, the functional updates are evaluated for both the context and the target, and are applied to the corresponding evaluations of functional representation with a learning rate .}
\label{fig:metafun}
\end{figure*}


\subsection{Learning Functional Task Representation} \label{sub:learning task representation}

Like previous works such as \gls{CNP} and \gls{NP}, we construct the learning model using an encoder-decoder pipeline, where the encoder  is a permutation-invariant function of the context producing a task representation.
In past works, pooling operations are usually used to enforce permutation-invariance. \gls{CNP} and \gls{NP} use sum-pooling: , where  is a representation for context pair , and  is a fixed-dimensional task representation. Instead, we introduce functional-pooling operations, which also enforce permutation-invariance but output a function that can loosely be interpreted as an infinite-dimensional representation.

\theoremstyle{definition}
\begin{definition}[Functional pooling] Let  be a real-valued similarity measure, and  be a set of key-value pairs with , . \emph{Functional pooling} is a mapping  defined as

where the output is a function  and  is a space of such functions. 
\end{definition}

In practice, we only need to evaluate this function on a finite query set  (consisting of both contexts and targets; see below). That is, we only need to compute , which can be easily implemented using matrix operations. 
We consider two types of \textsc{FunPooling} here, though others are possible. The kernel-based \textsc{FunPooling} reads as,

where  is the RBF kernel,  is a matrix whose rows are queries,   is a  transformation mapping inputs into features,   a matrix whose rows are keys, and  a matrix whose rows are values (using terminology from the attention literature). Parameterising input transformation  with a deep neural network can be seen as using deep kernels \citep{wilson2016deep} as the similarity measure.
The second type of \textsc{FunPooling} is given by dot-product attention,

where  is the dimension of the query/key vectors.

Our second core idea is that rather than producing the task representation in a single pass (like  previous encoder-decoder meta-learning approaches), we start from an initial representation , and iteratively produce improved representations . 
At each step, a parameterised local update rule  compares  to the context input/output pairs, producing local update values  for each . These can then be aggregated into a global update to the task representation using functional pooling,

where  is the step size. 
Once the local update function  and the functional pooling operations are parameterised by neural networks, \Cref{eq:r-iter} defines a  neural update rule operating directly in function space. The functional update  depends on the current representation  and the context . \Cref{fig:cartoon} illustrates our iterative procedure in a simplified setting.

The final task representation can then be decoded into a predictor   . 
The specific parametric forms of the decoder take different forms for regression and classification, and are described in \Cref{sub:regression-and-classification}. The decoder requires the evaluation of functional representation  at  only for predicting . Therefore, it is unnecessary to compute the functional representations  (including their functional updates) on all input points. Instead, we compute them only on the context  and target inputs . We use  to denote a matrix where each row is  evaluated on either context or target inputs, and let .
\Cref{eq:r-iter} can be implemented using matrix computations as follows,

where  denotes the -th row of .

To obtain a prediction  for the target , we decode the final representation for this target point: , and the overall training loss can be written as:

where the predictions  depend on  and .

Assuming the width and depth of all our neural network components are bounded by  and  respectively, and the output dimension of  is also less than , the time complexity of our approach is , and the space complexity is . For few-shot problems,  and  are typically small, and  in all our experiments.


\subsection{MetaFun for Regression and Classification} \label{sub:regression-and-classification}


While the proposed framework can be applied to any supervised learning task, the specific parameterisation of learnable components can affect the model performance.
In this section, we specify the parametric forms of our model that work well on regression and classification tasks.
\paragraph{Regression} For regression tasks, we parameterise the local update function  using a multi-layer perceptron\glsunset{MLP} as , , where  is concatenation.
We also use an \gls{MLP} to parametrise the input transformation  in the functional pooling.
The decoder in this case is given by , another \gls{MLP}\footnote{
It might be desirable to use other parameterisations of the input transformation , and the decoder , e.g., , or feeding  to each layer of the \gls{MLP}.
} that outputs , which then parameterises the predictive model .

Note that our model can easily be modified to incorporate Gaussian uncertainty by adding an extra output vector for the predictive standard deviation: . For further architecture details, see Appendix.

\paragraph{Classification} For -way classification, we divide the latent functional representation  into  parts , where  corresponds to the class . 
Consequently, the local update function  also has  parts, that is, . In this case,  is the class label expressed as a one-hot vector; the  is defined as follows,

where  summarises representations of all classes, and , ,  are parameterised by separate \gls{MLP}s.
With this formulation, we update the class representations using either  (when the label matches ) or  (when the label is different to ), so that labels are not concatenated to the inputs, but directly used to activate different model components, which is crucial for model performance. Furthermore, interactions between data points in classification problems include both within-class and between-class interactions. Our approach is able to integrate two types of interactions by having separate functional representation for each class and computing local updates for each class differently based on class membership of each data point.
In fact, this formulation resembles the structure of the local update rule in functional gradient descent for classification tasks, which is a special case of our approach (see \Cref{sec:related_work}).
Same as in regression tasks, the input transformation  in the functional pooling is still an \gls{MLP}.
The parametric form of the decoder is the same as in \gls{LEO} \citep{rusu2018meta}.
The class representation  generates weights  where  and  are \gls{MLP}s or just linear functions,
and the final prediction is given by

where , .
Hyperparameters of all components are described in Appendix.


\section{Related Work} \label{sec:related_work}

\paragraph{Functional Gradient Descent} Functional gradient descent \citep{mason1999functional,guo2001norm} is an optimisation algorithm used to minimise the objective function by moving in the direction of the negative gradient in function space. 
To ensure smoothness, we may work with functions in a \gls{RKHS} \citep{aronszajn1950theory,berlinet2011reproducing} defined by a kernel .
Given a function  in the \gls{RKHS}, we are interested in minimising the supervised loss  with respect to . We can do so by computing the functional derivative and use it to iteratively update  (see Appendix for more details),

with step size , and  denotes gradient w.r.t. to predictions in the loss function .


The update rule in \Cref{eq:r-iter} becomes that of functional gradient descent in \Cref{eq:functionupdate} when 
\begin{enumerate}[label={\upshape(\roman*)}]
    \item A trivial decoder  is used, so the functional representation  is the same as the predictive model .
    \item Kernel functional pooling  is used and the kernel function is fixed.
    \item Using gradient-based local update function .
\end{enumerate}

Furthermore, for a -way classification problem, we predict -dimensional logits , and use cross entropy loss as follows:

where  is the one-hot label for .

The gradient-based local update function is now  where  is partial derivative w.r.t. each predictive logit:

Here  is analogous to  in \Cref{eq:value-function-classification}, which is the local update function for class . 

If  in \Cref{eq:value-function-classification} are specified rather than being learned, more specifically:

\Cref{eq:fgd-classification} can be rewritten as:

which has a similar form as \Cref{eq:value-function-classification}.




Therefore, our approach can be seen as an extension of functional gradient descent, with an additional learning capacity due learnable neural modules which afford more flexibility. From this perspective, our approach tackles supervised meta-learning problems by learning an optimiser in function space.

\paragraph{Supervised Meta-Learning} Various ideas have been proposed to solve the problem of supervised meta-learning. \citet{andrychowicz2016learning,ravi2016optimization} learn the neural optimisers from previous tasks which can be used to optimise models for new tasks. However, these learned optimisers operate in parameter space rather than function space as we do.
\gls{MAML} \citep{finn2017model} learns the initialisation from which models are further adapted for a new task by a few gradient descent steps.
\citet{koch2015siamese,snell2017prototypical,vinyals2016matching} explore the idea of learning a metric space from previous tasks in which data points are compared to each other to make predictions at test time.
\citet{santoro2016meta} demonstrate that Memory-Augmented Neural Networks (MANN) can rapidly integrate the data for a new task into memory, and utilise this stored information to make predictions.

Our approach, in line with previous works such as \gls{CNP} and \gls{NP}, adopt an encoder-decoder pipeline to tackle supervised meta-learning.
The encoder in \gls{CNP} corresponds to a summation of instance-level representations produced by a shared instance encoder.
\Gls{NP}s, on the other hand, use a probabilistic encoder with the same parametric form as \gls{CNP}, but producing a distribution of stochastic representation.
The \gls{ANP} \citep{kim2019attentive} adds a deterministic path in addition to the stochastic path in \gls{NP}. The deterministic path produces a target-specific representation, which can be interpreted as
applying functional pooling (implemented with multihead attention \citep{vaswani2017attention}) to instance-wise representation. However, the representation is directly produced in a single pass rather than iteratively improved as we do, and only regression applications are explored as opposed to few-shot image classification. 
In fact, to achieve high performance for classification tasks, it is crucial for \gls{CNP} to only apply sum-pooling within each class \citep{garnelo2018conditional}, and it is unclear how to follow similar practices in \gls{ANP} with both within-class and between-class interactions still being modelled.
Recently, \citet{gordon2019convolutional} have also extended \gls{CNP} to use functional representations, but for the purpose of incorporating translation equivariance in the inputs as an inductive bias rather than increasing representational capacity as we do. Their approach uses convnets to impose translation equivariance and does not learn a flexible iterative encoder.

Pooling operations are usually used in encoder-decoder meta-learning to enforce permutation invariance in the encoder.
As an example, encoders in both \gls{CNP} and \gls{NP} use simple \emph{sum-pooling} operations. 
More expressive pooling operations have been proposed to model interactions between data points.
\citet{murphy2019janossy} introduces \emph{Janossy pooling} which applies permutation-sensitive functions to all reorderings and averages the outputs, while \citet{lee2019set} use \emph{pooling by multihead attention} (\textsc{pma}), which uses a finite query set to attend to the processed key-value pairs. Loosely speaking, attention-based functional pooling can be seen as having the whole input domain  as the query set in \textsc{pma}.

\paragraph{Gradient-Based Meta-Learning}
Interestingly, many gradient-based meta-learning methods such as \gls{MAML} can also be cast into an encoder-decoder formulation, because a gradient descent step is a valid permutation-invariant function. For a model  parameterised by , one gradient descent step on the context loss has the following form,

where  is the loss function,  is the learning rate, and  are the model parameters after  gradient steps.
This corresponds to a special case of permutation-invariant functions where we take the instance-wise encoder to be  and apply sum-pooling . 
Multiple gradient-descent steps also result in a permutation-invariant function, which can be proved by induction. We refer to this as a gradient-based encoder.
What follows is that popular meta-learning methods such as \gls{MAML} can be seen as part of the encoder-decoder formulation.
More specifically, in \gls{MAML}, we learn an initialisation of the model parameters  from training tasks, and adapt to new tasks by running  gradient steps from the learned initialisation. Therefore,  can be seen as the task representation (albeit very high-dimensional) produced by a gradient-based encoder.
The success of \gls{MAML} on a variety of tasks can be partially explained by the high-dimensional representation and the iterative adaptation by gradient descent, supporting our usage of a functional ('infinite-dimensional') representation and iterative updating procedure. Note, however, that the update rule in \gls{MAML} operates in parameter space rather than function space as in our case. 

Under the same encoder-decoder formulation, a comparison regarding \gls{MAML} and MetaFun can be made, which partially explains why MetaFun can be desirable: Firstly, the updates in \gls{MAML} must lie in its parametric space, while there is no parametric constraint in MetaFun, which is better illustrated in \Cref{fig:sinusoid}. Secondly, \gls{MAML} uses gradient-based updates, while MetaFun uses learned local updates, which potentially contains more information than gradient. Finally, \gls{MAML} does not explicitly consider interactions between data points, while both within-context and context-target interactions are modelled in MetaFun.

\section{Experiments} \label{sec:experiments}


We evaluate our proposed model on both few-shot regression and classification tasks. 
In all experiments that follow, we partition the data into training, validation and test meta-sets, each containing data from disjoint tasks. 
For quantitative results, we train each model with  different random seeds and report the mean and the standard deviation of the test accuracy. For further details on hyperparameter tuning, see the Appendix. All experiments are performed using TensorFlow \citep{tensorflow2015-whitepaper}, and the code is available online \footnote{A tensorflow implementation of our model is available at \url{github.com/jinxu06/metafun-tensorflow}}.

\subsection{1-D Function Regression}

\begin{figure*} 
\begin{minipage}{0.49\textwidth}
    \includegraphics[width=1.0\linewidth]{sinusoid.pdf}
    \caption{MetaFun is able to learn smooth updates, and recover the ground truth function almost perfectly. While the updates given by \gls{MAML}s are relatively not smooth, especially for \gls{MAML} with less parameters.}
    \label{fig:sinusoid}
\end{minipage} 
\hspace{0.5cm}
\begin{minipage}{0.49\textwidth}
    \includegraphics[width=1.0\linewidth]{gp-uncertainty.pdf}
\caption{Predictive uncertainties for MetaFun matches those for the oracle \gls{GP} very closely in both 5-shot and 15-shot cases. The model is trained on random context size ranging from  to .}
\label{fig:gp}
\end{minipage}
\end{figure*}


We first explore a 1D sinusoid regression task where we visualise the updating procedure in function space, providing intuition for the learned functional updates. Then we incorporate Gaussian uncertainty into the model, and compare our predictive uncertainty against that of a \gls{GP} which generates the data.

\begin{table}[!ht]  
  \centering
  \footnotesize
  \caption{Few-shot regression on sinusoid. \gls{MAML} can beneift from more parameters, but MetaFun still outperforms all \gls{MAML}s despite less parameters being used compared to large \gls{MAML}. We report mean and standard deviation of  independent runs.}
  \begin{tabular}{l|cc}
    \toprule
    Model &  -shot MSE & -shot MSE \\ 
    \midrule
    Original \gls{MAML} &  &  \\
    Large \gls{MAML} &  &  \\
    Very Wide \gls{MAML}  &  &  \\
    MetaFun &  &  \\
    \bottomrule
  \end{tabular}
  \label{tb:sinusoid}
\end{table}

\paragraph{Visualisation of functional updates} We train a -step MetaFun with dot-product functional pooling, on a simple sinusoid regression task from \citet{finn2017model}, where each task uses data points of a sine wave. The amplitude  and phase  of the sinusoid varies across tasks and are randomly sampled during training and test time, with  and . The x-coordinates are uniformly sampled from .
\Cref{fig:sinusoid} shows that our proposed algorithm learns a smooth transition from the initial state to the final prediction at . Note that although only  context points on a single phase of the sinusoid are given at test time, the final iteration makes predictions close to the ground truth across the whole period.
As a comparison, we use \gls{MAML} as an example of updating in parameter space. The original \gls{MAML} ( units   hidden layers) can fit the sinusoid quite well after several iterations from the learned initialisation. However the prediction is not as good, particularly on the left side where there are no context points (see \Cref{fig:sinusoid} B). As we increase the model size to large \gls{MAML} ( units   hidden layers), updates become much smoother (\Cref{fig:sinusoid} C) and the predictions are closer to the ground truth. 
We further conduct experiments with a very wide \gls{MAML} ( units   hidden layers), but the performance cannot be further improved (\Cref{fig:sinusoid} D). In \Cref{tb:sinusoid}, we compare the mean squared error averaged across tasks. MetaFun performs much better than all \gls{MAML}s, even though less parameters ( parameters) are used compared to large \gls{MAML} ( parameters).


\paragraph{Predictive uncertainties} As another simple regression example, we demonstrate that MetaFun, like \gls{CNP}, can produce good predictive uncertainties. We use synthetic data generated using a \gls{GP} with an RBF kernel and Gaussian observation noise (), and our decoder produces both predictive means and variances. 
As in \citet{kim2019attentive}, we found that MetaFun-DFP can produce somewhat piece-wise constant mean predictions which is less appealing in this situation. On the other hand, MetaFun-KFP (with deep kernels) performed much better, as can be seen in \Cref{fig:gp}. We consider the cases of  or  context points, and compare our predictions to those for the oracle \gls{GP}. In both cases, our model gave very good predictions. 

\begin{table*}[!htb]  
  \centering
  \footnotesize
  \begin{threeparttable}[]
  \caption{Few-shot Classification Test Accuracy}
  \begin{tabular}{l|cc}
    \toprule
         & \textbf{miniImageNet 5-way}     & \textbf{miniImageNet 5-way}  \\
    Models    & \textbf{1-shot}     & \textbf{5-shot} \\
    \midrule
    \emph{(Without deep residual networks feature extraction):} & & \\
    Matching networks \citep{vinyals2016matching} &  &  \\
    Meta-learner LSTM \citep{ravi2016optimization}  &  &  \\
    MAML \citep{finn2017model} &  &  \\
    LLAMA \citep{grant2018recasting}  &  & - \\
    REPTILE \citep{nichol2018first}  &  &  \\
    PLATIPUS \citep{finn2018probabilistic}  &  & - \\
    \midrule
    \emph{(Without data augmentation):} & & \\
    Meta-SGD \citep{li2017meta} &  &   \\
    SNAIL \citep{mishra2017simple} &  &  \\
    \citet{bauer2017discriminative} &  &  \\
    \citet{munkhdalai2018rapid} &  &  \\
    TADAM \citep{oreshkin2018tadam} &  &   \\
    \citet{qiao2018few} &  &   \\
    LEO &  &   \\
MetaFun-DFP &  &   \\
    MetaFun-KFP &  &   \\
    
    
    \midrule
    \emph{(With data augmentation):} & & \\
    \citet{qiao2018few} &  &   \\
    LEO &  &   \\
    MetaOptNet-SVM \citep{lee2019meta}\tnote{1} &  &   \\
    MetaFun-DFP &   & 
      \\
    MetaFun-KFP &   & 
      \\
\bottomrule
  \end{tabular}
  
  \begin{tabular}{l|cc}
    \toprule
     & \textbf{tieredImageNet 5-way}     & \textbf{tieredImageNet 5-way}  \\
    Models    & \textbf{1-shot}     & \textbf{5-shot} \\
    \midrule
    \emph{(Without deep residual networks feature extraction):} & & \\
    MAML \citep{finn2017model} &  &  \\
    Prototypical Nets \citep{snell2017prototypical} &  &  \\
    Relation Net [in \citet{liu2019learning}] &  &  \\
    Transductive Prop. Nets \citep{liu2019learning}\hspace{1cm} &  &  \\
    \midrule
    \emph{(With deep residual networks feature extraction):} & & \\
    Meta-SGD &  &   \\
    LEO &  &   \\
    MetaOptNet-SVM &  &   \\
    MetaFun-DFP &  &   \\
    MetaFun-KFP &  &   \\
    \bottomrule
  \end{tabular}
  \label{tb:classification}
  \end{threeparttable}
\end{table*}


\subsection{Classification: miniImageNet and tieredImageNet}\label{sub:imagenet}

The \emph{miniImageNet} dataset \citep{vinyals2016matching} consists of 100 classes selected randomly from
the ILSVRC-12 dataset \citep{russakovsky2015imagenet}, and each class contains 600 randomly sampled images. We follow the split in \citet{ravi2016optimization}, where the dataset is divided into training ( classes), validation ( classes), and test ( classes) meta-sets. 
The \emph{tieredImageNet} dataset \citep{ren2018meta} contains a larger subset of the ILSVRC-12 dataset. These classes are further grouped into  higher-level nodes. These nodes are then divided into training ( nodes), validation ( nodes), and test ( nodes) meta-sets. This dataset is considered more challenging because the split is near the root of the ImageNet hierarchy \citep{ren2018meta}. For both datasets, we use the pre-trained features provided by \citet{rusu2018meta}. 

Following the commonly used experimental setting, each few-shot classification task consists of  randomly sampled classes from a meta-set. Within each class, we have either  example (-shot) or  examples (-shot) as context, and  examples as target. For all experiments, hyperparameters are chosen by training on the training meta-set, and comparing target accuracy on the validation meta-set. We conduct randomised hyperparameters search \citep{bergstra2012random}, and the search space is given in Appendix. Then with the model configured by the chosen hyperparameters, we train on the union of the training and validation meta-sets, and report final target accuracy on the test meta-set. 

In \Cref{tb:classification} we compare our approach to other meta-learning methods. The numbers presented are the mean and standard deviation of  independent runs. The table demonstrates that our model outperforms previous state-of-the-art on 1-shot and 5-shot classification tasks for the more challenging tieredImageNet. 
As for miniImageNet, we note that previous work, such as MetaOptNet-SVM \citep{lee2019meta}, used significant data augmentation to regularise their model and hence achieved superior results. For a fair comparison, we also equipped each model with data augmentation and reported accuracy with/without data augmentation. 
However, MetaOptNet-SVM \citep{lee2019meta} uses a different data augmentation scheme involving horizontal flip, random crop, and color (brightness, contrast, and saturation) jitter. On the other hand, MetaFun, \citet{qiao2018few} and LEO \citep{rusu2018meta}, only use image features averaging representation of different crops and their horizontal mirrored versions.
In 1-shot cases, MetaFun matches previous state-of-the-art performance, while in 5-shot cases, we get significantly better results. In \Cref{tb:classification}, results for both MetaFun-DFP (using dot-product attention) and MetaFun-KFP (using deep kernels) are reported. Although both of them demonstrate state-of-the-art performance, MetaFun-KFP generally outperforms MetaFun-DFP for 5-shot problems, but performs slightly worse for 1-shot problems.



\subsection{Ablation Study}

\begin{table*}[!htb] 
  \centering
  \footnotesize
  \begin{threeparttable}[]
  \caption{Ablation Study. We conduct independent randomised hyperparameter search for each number presented, and reported means and standard deviations over 5 independent runs for each.}
  \label{table:few_shot}
  \begin{tabular}{ccc|cc|cc}
    \toprule
    Functional &  Local update &  \multirow{2}{*}{Decoder} &  \multicolumn{2}{c|}{\textbf{MiniImageNet}} & \multicolumn{2}{c}{\textbf{tieredImageNet}} \\ 
     pooling &  function &  & 1-shot   & 5-shot \\
    \midrule
    Attention & NN & \cmark & \bm{} &  & \bm{} &   \\
    Deep Kernel & NN& \cmark &  & \bm{} &  & \bm{} \\
    Attention & Gradient & \cmark &  &   &  &  \\
    Deep Kernel & Gradient & \cmark &  &  &  &  \\
    \midrule
    SE Kernel & NN & \cmark &  &   &  &  \\
    Deep Kernel & Gradient & \xmark &  &  &  &  \\
    \bottomrule
  \end{tabular}
  \label{tb:ablation}
  \end{threeparttable}
\end{table*}

\begin{figure*}
    \includegraphics[width=0.99\linewidth]{iterations.pdf}
    \caption{This figure illustrates the accuracy of our approach for varying number of iterations , over different few-shot learning problems. For each problem, we use the same configuration of hyperparameters except for the number of iterations and the choice between attention and deep kernels. Error bars (standard deviations) are given by training the same model  times with different random seeds.}
\label{fig:iteration}
\end{figure*}


As stated in \Cref{sub:regression-and-classification}, our model has three learnable components: the local update function, the functional pooling, and the decoder. In this section we explore the effects of using different versions of these components. We also investigate how the model performance would change with different numbers of iterations.

\Cref{tb:ablation} demonstrates that neural network parameterised local update functions, described in \Cref{sub:learning task representation}, consistently outperforms gradient-based local update function, despite the latter having build-in inductive biases. 
Interestingly, the choice between dot-product attention and deep kernel in functional pooling is problem dependent. We found that MetaFun with deep kernels usually perform better than MetaFun with dot product attention on -shot classification tasks, but worse on -shot tasks. We conjecture that the deep kernel is better able to fuse the information across the 5 images per class compared to attention. In the comparative experiments in \Cref{sub:imagenet} we reported results on both.



In addition, we investigate how a simple Squared Exponential (SE) kernel would perform on these few-shot classification tasks. This corresponds to using an identity input transformation function  in deep kernels. \Cref{tb:ablation} shows that using SE kernel is consistently worse than using deep kernels, showing that the heavily parameterised deep kernel is necessary for these problems.

Next, we looked into directly applying functional gradient descent with parameterised deep kernel to these tasks. This corresponds to removing the decoder and using deep kernels and gradient-based local update function (see \Cref{sec:related_work}). Unsurprisingly, this did not fare as well, given as it only has one trainable component (the deep kernel) and the updates are directly applied to the predictions rather than a latent functional representation. 

Finally, \Cref{fig:iteration} illustrates the effects of using different numbers of iterations . On all few-shot classification tasks, we can see that using multiple iterations (two is often good enough) always significantly outperform one iteration. We also note that this performance gain diminishes as we add more iterations. In \Cref{sub:imagenet} we treated the number of iterations as one of the hyperparameters.

\section{Conclusions and Future Work}

In this paper, we propose a novel functional approach for meta-learning called MetaFun. The proposed approach learns to generate a functional task representation and an associated functional update rule, which allows to iteratively update the task representation directly in the function space.
We evaluate MetaFun on both few-shot regression and classification tasks, and
demonstrate that it matches or exceeds previous state-of-the-art results on miniImageNet and tieredImageNet few-shot classification tasks. 

Interesting future research directions include
a) exploring a stochastic encoder and hence working with stochastic functional representations, akin to the \glsreset{NP}\gls{NP},
and b) using local update functions and the functional pooling components whose parameters change with iterations instead of sharing them across iterations, where the added flexibility could lead to further performance gains.

\section*{Acknowledgements}

We would like to thank Jonathan Schwarz for valuable discussion, and the anonymous reviewers for their feedback. Jin Xu and Yee Whye Teh acknowledge funding from Tencent AI Lab through the Oxford-Tencent Collaboration on Large Scale Machine Learning project. Jean-Francois Ton is supported by the EPSRC and MRC through the OxWaSP CDT programme (EP/L016710/1). 

\clearpage
\newpage


\bibliography{metafun}
\bibliographystyle{icml2020}


\onecolumn
\icmltitle{Appendix for \\ MetaFun: Meta-Learning with Iterative Functional Updates}

\appendix 

\section{Functional Gradient Descent}

Functional gradient descent \citep{mason1999functional,guo2001norm} is an iterative optimisation algorithm for finding the minimum of a function. However, the function to be minimised is now a function on functions (\emph{functional}). Formally, a functional  is a mapping from a function space  to a  Euclidean space .
Just like gradient descent in parameter space which takes steps proportional to the negative of the gradient, functional gradient descent updates  following the gradient in function space. In this work, we only consider a special function space called \gls{RKHS} (\Cref{sub:rkhs}), and calculate functional gradients in \gls{RKHS} (\Cref{sub:fg}). The algorithm is further detailed in \Cref{sub:fgd-appendix}.

\subsection{Reproducing Kernel Hilbert Space} \label{sub:rkhs}

A Hilbert space  extends the notion of Euclidean space by introducing inner product  which describes the concept of distance or similarity in this space. A \gls{RKHS}  is a Hilbert space of real-valued functions on  with the reproducing property that for all  there exists a unique  such that the \emph{evaluation functional}  can be represented by taking the inner product of this element  and , formally as:


Since  for any , we can define a kernel function  by letting

Using properties of inner product, it is easy to show that the kernel function  is symmetric and positive definite, and we call it the \emph{reproducing kernel} of the Hilbert space .


\subsection{Functional Gradients} \label{sub:fg}

\emph{Functional derivative} can be thought of as describing the rate of change of the output with respect to the input in a functional. Formally, functional derivative at point  in the direction of  is defined as:

which is a function of . This is known as \emph{Fr√©chet derivative} in a Banach space, of which the Hilbert space is a special case.

\emph{Functional gradient}, denoted as , is related to functional derivative by the following equation:


Thanks to the reproducing property, it is straightforward to calculate functional derivative of an evaluation functional in \gls{RKHS}:

Therefore, the functional gradient of an evaluation functional is:


For a learning task with loss function  and a context set , the overall supervised loss on the context can be written as:

In this case, the functional gradient of  can be easily calculated by applying the chain rule:


\subsection{Functional Gradient Descent} \label{sub:fgd-appendix}


To optimise the overall loss on the entire context in \Cref{eq:loss-appendix}, we choose a suitable learning rate , and iteratively update  with:



In order to evaluate the final model  at iteration , we only need to compute

which does not depend on function values outside the context from previous iterations .


\newcommand{\randint}{\fontfamily{pcr}\selectfont randint}
\newcommand{\uniform}{\fontfamily{pcr}\selectfont uniform}
\newcommand{\numpy}{\fontfamily{pcr}\selectfont numpy.random}

\begin{table}[!htb] 
  \centering
  \begin{threeparttable}[]
  \caption{Considered Range of Hyperparameters. The random generators such as {\randint} or {\uniform} use {\numpy} syntax, so the first argument is inclusive while the second argument is exclusive. Whenever a list is given, it means uniformly sampling from the list.  and  will be followed by a linear transformation with an output dimension of \emph{dim-reprs}.}
  \begin{tabular}{l|c}
    \toprule
    Components &         Architecture \\ 
    \midrule
    Shared MLP  & \emph{nn-sizes}  \emph{nn-layers} \\
    MLP for positive labels  & \emph{nn-sizes}  \emph{nn-layers} \\
    MLP for negative labels  & \emph{nn-sizes}  \emph{nn-layers} \\
    Key/query transformation MLP  & \emph{dim}  \emph{embedding-layers} \\
    Decoder & linear with output dimension \emph{dim} \\
    \bottomrule
  \end{tabular}
  \begin{tabular}{l|c}
    \toprule
    Hyperparameters &         Considered Range \\ 
    \midrule
    \emph{num-iters} & \randint(2, 7)  \\
    \emph{nn-layers} & \randint(2, 4)\\
    \emph{embedding-layers} & \randint(1, 3)\\
    \emph{nn-sizes} &  \\
    \emph{dim-reprs} &  \emph{nn-sizes} \\
    Initial representation  \;\;\;\;\;\;\;\;\;\;\;\;\;\;\; & [zero, constant, parametric] \\ 
    \midrule 
    Outer learning rate &   \\ 
    Initial inner learning rate &  \\ 
    Dropout rate & \uniform(0.0, 0.5) \\ 
    Orthogonality penalty weight &  \\ 
    L2 penalty weight &  \\ 
    Label smoothing &  \\ 
    \bottomrule
  \end{tabular}
  \label{tb:hyperparameters-considered}
  \end{threeparttable}
\end{table}

\section{Experimental Details} \label{sec:experimental-details}

We run experiments on Nvidia's GeForce GTX 1080 Ti, and it typically takes about -- minutes to train a few-shot model on a single GPU card until early-stopping is triggered (after seeing -- tasks).
For miniImageNet and tieredImageNet, we conduct randomised hyperparameters search \citep{bergstra2012random} for hyperparameters tunning. Typically,  configurations of hyperparameters are sampled for each problem, and the best configuration is chosen by comparing accuracy on the validation set. The considered range of hyperparameters is given in \Cref{tb:hyperparameters-considered}, and the chosen hyperparameters are shown in \Cref{tb:hyperparameters-chosen}. For regression tasks, we simply use hyperparameters listed in \Cref{tb:hyperparameters-regression} for both MetaFun-DFP and MetaFun-KFP.


\begin{table*}[!htb] 
  \centering
  \footnotesize
  \begin{threeparttable}[]
  \caption{Results of randomised hyperparameters search. Hyperparameters shown in this table are not guaranteed to be optimal within the considered range, because we conduct randomised hyperparameters search. However, models configured with these hyperparameters perform reasonably well, and we used them to report final results comparing to other methods. Furthermore, dropout is only applied to the inputs. Orthogonality penalty weight and L2 penalty weight are used in exactly the same way as in \citet{rusu2018meta}. Inner learning rate  is trainable so only an initial inner learning rate is given in the table.}
  \begin{tabular}{l|cc|cc}
  \toprule
     & \multicolumn{2}{c|}{\textbf{miniImageNet}} &  \multicolumn{2}{c}{\textbf{tieredImageNet}}\\ 
    \toprule
    Hyperparameters (for MetaFun-DFP)  &    -shot & -shot & -shot & -shot \\ 
    \midrule
    \emph{num-iters} &  &  &  &  \\
    \emph{nn-layers}  &  &  &  &  \\
    \emph{embedding-layers} &  &  &  &  \\
    \emph{nn-sizes} &  &  &  &  \\
    Initial state & zero & constant & constant & constant \\ 
    \midrule 
    Outer learning rate &  &  &  &  \\ 
    Initial inner learning rate &  &  &  &  \\ 
    Dropout rate &  &  &  &  \\ 
    Orthogonality penalty weight &  &  &   &  \\ 
    L2 penalty weight &  &  &  &  \\ 
    Label smoothing &  &  &  &  \\ 
    \bottomrule
  \end{tabular}
  
  \begin{tabular}{l|cc|cc}
  \toprule
    &   \multicolumn{2}{c|}{\textbf{miniImageNet}} &  \multicolumn{2}{c}{\textbf{tieredImageNet}}\\ 
    \toprule
    Hyperparameters (for MetaFun-KFP)\;\;\;\;\;  &   -shot & -shot & -shot & -shot \\ 
    \midrule
    \emph{num-iters} &  &  &  &  \\
    \emph{nn-layers} &  &  &  &  \\
    \emph{embedding-layers} &  &  &  &  \\
    \emph{nn-sizes} &  &  &  &  \\
    Initial state & zero & parametric & parametric & zero \\ 
    \midrule 
    Outer learning rate &  &  &  &  \\ 
    Initial inner learning rate &  &  &  &  \\ 
    Dropout rate &  &  &  &  \\ 
    Orthogonality penalty weight &  &  &   &  \\ 
    L2 penalty weight &  &  &  &  \\ 
    Label smoothing &  &  &  &  \\ 
    \bottomrule
  \end{tabular}
  \label{tb:hyperparameters-chosen}
  \end{threeparttable}
\end{table*}


\begin{table*}[!htb] 
  \centering
\begin{threeparttable}[]
  \caption{Hyperparameters for regression tasks. Local update function and the predictive model will be followed by linear transformations with output dimension of \emph{dim-reprs} and \emph{dim}(\yy) accordingly.}
  \begin{tabular}{l|c}
    \toprule
    Components &         Architecture \\ 
    \midrule
    Local update function & \emph{nn-sizes}  \emph{nn-layers} \\
    Key/query transformation MLP  & \emph{nn-sizes}  \emph{embedding-layers} \\
    Decoder & \emph{nn-sizes}  \emph{nn-layers} \\
    Predictive model & \emph{nn-sizes}  (\emph{nn-layers}-1) \\
    \bottomrule
  \end{tabular}
  \begin{tabular}{l|c}
    \toprule
    Hyperparameters &         Considered Range \\ 
    \midrule
    \emph{num-iters} &  \\
    \emph{nn-layers} & \\
    \emph{embedding-layers} &  \\
    \emph{nn-sizes} &  \\
    \emph{dim-reprs} &  \emph{nn-sizes} \\
    Initial representation  \;\;\;\;\;\;\;\;\;\;\;\;\;\;\; & zero \\ 
    \midrule 
    Outer learning rate &   \\ 
    Initial inner learning rate &  \\ 
    Dropout rate &  \\ 
    Orthogonality penalty weight &  \\ 
    L2 penalty weight &  \\ 
    \bottomrule
  \end{tabular}
  \label{tb:hyperparameters-regression}
  \end{threeparttable}
\end{table*}

\end{document}
