

\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{amsmath,amssymb} \usepackage{color}
\usepackage{array,multirow,booktabs}
\usepackage{subcaption}
\captionsetup{compatibility=false}
\usepackage{caption}
\graphicspath{{./pics/}}
\usepackage[outdir=./pics/]{epstopdf}
\usepackage{pifont}\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
urlcolor=magenta,
}
\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\newcommand{\dataset}{YouTube-VOS}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{xspace}
\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}
\makeatother


\begin{document}




\title{~\dataset: Sequence-to-Sequence Video Object Segmentation} 

\titlerunning{~\dataset}

\author{Ning Xu\inst{1} \and
Linjie Yang\inst{2} \and
Yuchen Fan\inst{3}\and
Jianchao Yang\inst{2}\and
Dingcheng Yue\inst{3} \and
Yuchen Liang\inst{3} \and
Brian Price\inst{1} \and
Scott Cohen\inst{1} \and
Thomas Huang\inst{3}}

\authorrunning{N. Xu~\etal}





\institute{Adobe Research, USA \\
\email{{\{nxu,bprice,scohen\}}}@adobe.com \and
Snapchat Research, USA \\
\email{\{linjie.yang,jianchao.yang\}@snap.com}\\ \and
University of Illinois at Urbana-Champaign, USA\\
\email{\{yuchenf4,dyue2,yliang35,t-huang1\}@illinois.edu}}


\maketitle

\begin{abstract}
Learning long-term spatial-temporal features are critical for many video analysis tasks. However, existing video segmentation methods predominantly rely on static image segmentation techniques, and methods capturing temporal dependency for segmentation have to depend on pretrained optical flow models, leading to suboptimal solutions for the problem. End-to-end sequential learning to explore spatial-temporal features for video segmentation is largely limited by the scale of available video segmentation datasets, i.e., even the largest video segmentation dataset only contains 90 short video clips. To solve this problem, we build a new large-scale video object segmentation dataset called YouTube Video Object Segmentation dataset (\dataset). Our dataset contains 3,252 YouTube video clips and 78 categories including common objects and human activities\footnote{This is the statistics when we submit this paper, see updated statistics on our website.}. This is by far the largest video object segmentation dataset to our knowledge and we have released it at \href{https://youtube-vos.org}{https://youtube-vos.org}. Based on this dataset, we propose a novel sequence-to-sequence network to fully exploit long-term spatial-temporal information in videos for segmentation. We demonstrate that our method is able to achieve the best results on our \dataset~test set and comparable results on DAVIS 2016 compared to the current state-of-the-art methods. Experiments show that the large scale dataset is indeed a key factor to the success of our model.

\keywords{Video Object Segmentation, Large-scale Dataset, Spatial-Temporal Information.}
\end{abstract}


\section{Introduction}

Learning effective spatial-temporal features has been demonstrated to be very important for many video analysis tasks. For example, Donahue \etal~\cite{donahue2015rcn} propose long-term recurrent convolution network for activity recognition and video captioning. Srivastava \etal\cite{srivastava2015unsupervisedlstm} propose unsupervised learning of video representation with a LSTM autoencoder. Tran \etal~\cite{tran2015c3d} develop a 3D convolutional network to extract spatial and temporal information jointly from a video. Other works include learning spatial-temporal information for precipitation prediction~\cite{xingjian2015convolutional}, physical interaction~\cite{finn2016videopred}, and autonomous driving~\cite{xu2016driving}. 

Video segmentation plays an important role in video understanding, which fosters many applications, such as accurate object segmentation and tracking, interactive video editing and augmented reality. Video object segmentation, which targets at segmenting a particular object instance throughout the entire video sequence given only the object mask on the first frame, has attracted much attention from the vision community recently~\cite{Caelles2017osvos,Perazzi2017masktrack,Yang2018osmn,Cheng2017segflow,Jain_2017_CVPR,Jampani2017vpn,Tokmakov2017memory,Hu2017Maskrnn,voigtlaender2017online}. However, existing state-of-the-art video object segmentation approaches primarily rely on single image segmentation frameworks~\cite{Caelles2017osvos,Perazzi2017masktrack,Yang2018osmn,voigtlaender2017online}. For example, Caelles \etal~\cite{Caelles2017osvos} propose to train an object segmentation network on static images and then fine-tune the model on the first frame of a test video over hundreds of iterations, so that it remembers the object appearance. The fine-tuned model is then applied to all following individual frames to segment the object without using any temporal information. Even though simple, such an online learning or one-shot learning scheme achieves top performance on video object segmentation benchmarks~\cite{Perazzi2016davis,Jain2014youtubeobjects}. Although some recent approaches~\cite{Jain_2017_CVPR,Cheng2017segflow,Tokmakov2017memory} have been proposed to leverage temporal consistency, they depend on models pretrained on other tasks such as optical flow~\cite{Ilg2017flownet,Revaud2015epicflow} or motion segmentation~\cite{Tokmakov2017mpnet}, to extract temporal information. These pretrained models are learned from separate tasks, and therefore are suboptimal for the video segmentation problem.







Learning long-term spatial-temporal features directly for video object segmentation task is, however, largely limited by the scale of existing video object segmentation datasets. For example, the popular benchmark dataset DAVIS~\cite{Pont-Tuset2017davis} has only 90 short video clips, which is barely sufficient to learn an end-to-end model from scratch like other video analysis tasks. Even if we combine all the videos from available datasets~\cite{Jain2014youtubeobjects,jumpcut,Fli2013segtrack,brox2010BMS,ochs2014FBMS,galasso2013VSB100}, its scale is still far smaller than other video analysis datasets such as YouTube-8M~\cite{abu2016youtube} and ActivityNet~\cite{heilbron2015activitynet}. To solve this problem, we present the first large-scale video object segmentation dataset called~\dataset~(YouTube Video Object Segmentation dataset) in this work. Our dataset contains 3,252 YouTube video clips featuring 78 categories covering common animals, vehicles, accessories and human activities. Each video clip is about 36 seconds long and often contains multiple objects, which are manually segmented by professional annotators. Compared to existing datasets, our dataset contains a lot more videos, object categories, object instances and annotations, and a much longer duration of total annotated videos. Table~\ref{tab:dataset-cmp} provides quantitative scale comparisons of our new dataset against existing datasets. We retrain existing algorithms on~\dataset~and benchmark their performance on our test set which contains 322 videos. In addition, our test set contains 10 categories unseen in the training set and are used to evaluate the generalization ability of existing approaches. 



















Based on Youtube-VOS, we propose a new sequence-to-sequence learning algorithm to explore spatial-temporal modeling for video object segmentation. We utilize a convolutional LSTM~\cite{xingjian2015convolutional} to learn long-term spatial-temporal information for segmentation. At each time step, the convolutional LSTM accepts last hidden states and an encoded image frame, it then outputs encoded spatial-temporal features which are decoded into a segmentation mask. Our algorithm is different from existing approaches in that it fully exploits the long-term spatial-temporal information in an end-to-end manner and does not depend on existing optical flow or motion segmentation models. We evaluate our algorithm on both~\dataset~and DAVIS 2016 and it achieves better or comparable results compared to the current state of the arts.

The rest of our paper is organized as follows. In Section~\ref{sec:related} we briefly introduce the related works. In Section~\ref{sec:dataset} and~\ref{sec:algorithm} we describe our \dataset~dataset and the proposed algorithm in detail. Experimental results are presented in Section~\ref{sec:exp}. Finally we conclude the paper in Section~\ref{sec:conclusion}. 

\begingroup
\setlength{\tabcolsep}{1.4pt}
\begin{table}[t]
\footnotesize
\centering
\caption{ Scale comparison between~\dataset~and existing datasets. ``Annotations'' denotes the total number of object annotations. ``Duration'' denotes the total duration (in minutes) of the annotated videos. }
\label{tab:dataset-cmp}
\begin{tabular}{|l|c|c|c|c|c|c|c|}
\hline
Scale & \begin{tabular}[x]{@{}c@{}}{JC}\\\cite{jumpcut}\end{tabular} & \begin{tabular}[x]{@{}c@{}}{ST}\\\cite{Fli2013segtrack}\end{tabular}   & \begin{tabular}[x]{@{}c@{}}{YTO}\\\cite{Jain2014youtubeobjects}\end{tabular} &  \begin{tabular}[x]{@{}c@{}}{FBMS}\\\cite{ochs2014FBMS}\end{tabular} & \multicolumn{2}{c|}{\begin{tabular}[x]{@{}c@{}}\multicolumn{1}{c}{DAVIS}\\\cite{Perazzi2016davis}~~\cite{Pont-Tuset2017davis}\end{tabular} }  & \begin{tabular}[x]{@{}c@{}}{\textbf{~\dataset}}\
\mathbf{c}_0, \mathbf{h}_0 &= Initializer(\mathbf{x}_0,\mathbf{y}_0) \\
\mathbf{\tilde x}_t &= Encoder(\mathbf{x}_t) \\
\mathbf{c}_t, \mathbf{ h}_t  &= ConvLSTM(\mathbf{\tilde x}_t, \mathbf{c}_{t-1}, \mathbf{h}_{t-1}),\\
\mathbf{\hat y}_t &= Decoder(\mathbf{h}_t) \\
\mathcal{L} &= -(\mathbf{y_t}\log(\mathbf{\hat y_t})) + ((1-\mathbf{y_t})\log(1-\mathbf{\hat y_t})) 
\times\mathcal{J}\mathcal{F} as the evaluation metrics as in~\cite{Perazzi2016davis}.

\subsection{\dataset}\label{sec:exp_new_data}

For fair comparison, we re-train previous methods (\ie SegFlow~\cite{Cheng2017segflow}, OSMN~\cite{Yang2018osmn}, MaskTrack~\cite{Perazzi2017masktrack}, OSVOS\cite{Caelles2017osvos} and OnAVOS~\cite{voigtlaender2017online}) on our training set with the same settings as our algorithm. One difference is that other methods leverage post-processing steps to achieve additional gains while our models do not.

The results are presented in Table~\ref{tab:res_2017}. All the comparison methods use static image segmentation models and four of them (\ie SegFlow, MaskTrack, OSVOS and OnAVOS) require online learning. Our algorithm leverages long-term spatial-temporal characteristics and achieves better performance even without online learinng (the second last row in Table~\ref{tab:res_2017}), which effectively demonstrates the importance of long-term spatial-temporal information for video object segmentation. With online learning, our model is further improved and achieves around  absolute improvement over the best previous method OSVOS on  mean. Our method also outperforms previous methods on contour accuracy and decay rate with a large margin. Surprisingly, OnAVOS which is the best performing method on DAVIS does not achieve good results on our dataset. We believe the drastic appearance changes and complex motion patterns in our dataset makes the online adaptation fail in many cases.
Figure~\ref{fig:j_over_time} visualizes the changes of  mean over the duration of video sequences. Without online learning, our method is worse than online learning methods such as OSVOS at the first few frames since the object appearance usually has not changed too much from the initial frame and online learning is effective under such scenario. However, our method degrades slower than the other methods and starts to outperform OSVOS at around 25\% of the videos, which demonstrates that our method indeed propagates object segmentations more accurately over time than previous methods. With the help of online learning, our method outperforms previous methods in most parts of the video sequences, while maintaining a small decay rate.


\begin{table*}[t]
\centering
\caption{Comparisons of our approach and other methods on~\dataset~test set. The results in each cell show the test results for seen/unseen categories. ``OL'' denotes online learning. The best results are highlighted in bold.}
\label{tab:res_2017}
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
Method       &  mean &  recall &  decay &  mean &   recall &   decay \\ \hline
SegFlow~\cite{Cheng2017segflow}   & 40.4/38.5 & 45.4/41.7 & \textbf{7.2}/\textbf{8.4} & 35.0/32.7 & 35.3/32.1 & \textbf{6.9}/\textbf{9.1} \\ \hline
OSVOS~\cite{Caelles2017osvos}  & 59.1/58.8  & 66.2/64.5   & 17.9/19.5  & 63.7/63.9 & 69.0/67.9    &  20.6/23.0 \\ \hline
MaskTrack~\cite{Perazzi2017masktrack} &56.9/60.7     & 64.4/69.6   & 13.4/16.4     & 59.3/63.7   & 66.4/73.4   & 16.8/19.8    \\ \hline
OSMN~\cite{Yang2018osmn} & 54.9/52.9    & 59.7/57.6    & 10.2/14.6     & 57.3/55.2  & 60.8/58.0  & 10.4/13.8     \\ \hline
OnAVOS~\cite{voigtlaender2017online} & 55.7/56.8    & 61.6/61.5    & 10.3/{9.4}     & 61.3/62.3  & 66.0/67.3  & 13.1/12.8     \\ \hline
\textbf{Ours} (w/o OL)        &  60.9/60.1  &   70.3/71.2   & 7.9/12.9  & 64.2/62.3  &  73.0/71.4    &  9.3/14.5  \\ \hline
\textbf{Ours} (with OL)    &  \textbf{66.9}\textbf{/66.8}   &   \textbf{78.7}\textbf{/76.5}   &  10.2/9.5 & \textbf{74.1}\textbf{/72.3}  &   \textbf{82.8}\textbf{/80.5}   &  12.6/13.4  \\ \hline
\end{tabular}
\end{table*}

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
       \includegraphics[width=1\textwidth]{j_over_time_seen.png}
        \caption{Seen categories}
    \end{subfigure}~ 
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{j_over_time_unseen.png}
        \caption{Unseen categories}
    \end{subfigure}
       
    \caption{The changes of  mean values over the length of video sequences.}
    \label{fig:j_over_time}
\end{figure}


Next we compare the generalization ability of existing methods on unseen categories in Table~\ref{tab:res_2017}. Most methods have better performance on seen categories than unseen categories, which is expected. But the differences are not obvious, \eg usually within  absolute differences on each metric. On one hand, it suggests that existing methods are able to alleviate the mismatch issue between training and test categories by approaches such as online learning. On the other hand, it also demonstrates the diverse training categories in~\dataset~helps different methods to generalize to new categories. Experiments on dataset scale in Section~\ref{sec:ablation} further suggests the power of data scale on our model. Compared to other single-frame based methods, OSMN has a more obvious degradation on unseen categories since it does not use online learning. Our method without online learning does not have the issue since it leverages spatial-temporal information which is more robust to unseen categories.    MaskTrack and OnAVOS have better performance on unseen than seen categories. We believe that they benefit from the guidance of previous segmentation or online adaption, which have advantages to deal with videos with slow motion. There are indeed several objects with slow motion in the unseen categories such as \textit{snail} and \textit{chameleon}.



\begingroup
\setlength{\tabcolsep}{1pt}
\renewcommand{\arraystretch}{1}
\begin{figure*}[t]
\centering
 \begin{tabular}{@{}ccccc@{}}
\includegraphics[width=.19\textwidth]{2_MpmA6Na6Y_5_0.jpg}  &
\includegraphics[width=.19\textwidth]{2_MpmA6Na6Y_5_60.jpg}  &
\includegraphics[width=.19\textwidth]{2_MpmA6Na6Y_5_100.jpg}  &
 \includegraphics[width=.19\textwidth]{2_MpmA6Na6Y_5_120.jpg}  &
 \includegraphics[width=.19\textwidth]{2_MpmA6Na6Y_5_170.jpg}  \\
 \includegraphics[width=.19\textwidth]{vid_seg_02023_00110.jpg}  &
\includegraphics[width=.19\textwidth]{vid_seg_02023_00140.jpg}  &
\includegraphics[width=.19\textwidth]{vid_seg_02023_00165.jpg}  &
 \includegraphics[width=.19\textwidth]{vid_seg_02023_00185.jpg}  &
 \includegraphics[width=.19\textwidth]{vid_seg_02023_00200.jpg}  \\
 \includegraphics[width=.19\textwidth]{qTkdU4C3Vt4_3_0.jpg}  &
\includegraphics[width=.19\textwidth]{qTkdU4C3Vt4_3_60.jpg}  &
\includegraphics[width=.19\textwidth]{qTkdU4C3Vt4_3_85.jpg}  &
 \includegraphics[width=.19\textwidth]{qTkdU4C3Vt4_3_140.jpg}  &
 \includegraphics[width=.19\textwidth]{qTkdU4C3Vt4_3_160.jpg}  \\
 \includegraphics[width=.19\textwidth]{UCTfEaGcw5o_4_0.jpg}  &
\includegraphics[width=.19\textwidth]{UCTfEaGcw5o_4_45.jpg}  &
\includegraphics[width=.19\textwidth]{UCTfEaGcw5o_4_80.jpg}  &
 \includegraphics[width=.19\textwidth]{UCTfEaGcw5o_4_125.jpg}  &
 \includegraphics[width=.19\textwidth]{UCTfEaGcw5o_4_170.jpg}  
 \end{tabular}
 \caption{Some visual results produced by our model without online learning on the~\dataset~test set. The first column shows the initial ground truth object segmentation (green color) while the second to the last column are predictions.  }
\label{fig:youtube-vos-res}
\end{figure*}
\setlength{\tabcolsep}{1.4pt}
\endgroup

Some test results produced by our model without online learning are visualized in Figure~\ref{fig:youtube-vos-res}. The first two rows are from seen categories while the last two rows are from unseen categories. In addition, each example represents some challenging cases in video object segmentation. For example, the person in the first example has large changes in appearance and illumination. The second and third examples both have multiple similar objects and heavy occlusions. The last example has strong camera motion and the penguin changes its pose frequently. Our model obtains accurate results on all the examples, which demonstrates the effectiveness of spatial-temporal features learned from large-scale training data.


\subsection{DAVIS 2016}

DAVIS 2016 is a popular prior benchmark dataset for video object segmentation. To evaluate our algorithm, we first fine-tune our pretrained model in 200 epochs on the DAVIS training set which contains 30 videos. The comparison results between our fine-tuned models and previous methods are shown in Table~\ref{tab:res_davis_2016}.  

\begin{table*}[t]
\centering
\caption{Comparisons of our approach and previous methods on the DAVIS 2016 dataset. Different components used in each algorithm are marked. ``OL" denotes online learning. ``PP'' denotes post processing by CRF~\cite{Krahenbuhl2011crf} or Boundary Snapping~\cite{Caelles2017osvos}. ``OF'' denotes optical flows. ``RNN'' denotes RNN and its variants. }
\label{tab:res_davis_2016}
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
Method & OL & PP & OF & RNN & mean IoU() &  Speed(s) \\ \hline
BVS~\cite{Tsai2016objflow}&  -   &  \xmark & \xmark &  -    &   60.0 & 0.37  \\ \hline
OFL~\cite{Marki2016bilateral}  &  -   & \cmark  & \cmark &  -    &   68.0  & 42.2 \\ \hline
SegFlow~\cite{Cheng2017segflow}   &  \cmark   & \cmark   & \cmark &  \xmark    &  76.1  & 7.9  \\ \hline
MaskTrack~\cite{Perazzi2017masktrack} &  \cmark    &  \cmark  &  \xmark    &  \xmark  & 79.7  &  12   \\ \hline
OSVOS~\cite{Caelles2017osvos}  &  \cmark & \cmark  & \xmark  & \xmark  & 79.8 &  10    \\ \hline
OnAVOS~\cite{voigtlaender2017online} &  \cmark & \cmark  & \xmark  & \xmark  & \textbf{85.7} &  13    \\ \hline
OSMN~\cite{Yang2018osmn}       &  \xmark  &  \xmark    &  \xmark & \xmark &  74.0    &  \textbf{0.14}  \\ \hline
VPN~\cite{Jampani2017vpn} &  \xmark  &   \xmark   & \xmark  & \xmark  & 70.2  &   0.63   \\ \hline
ConvGRU~\cite{Tokmakov2017memory}   &  \xmark   & \cmark   & \cmark &  \cmark    &   75.9  & 20 \\ \hline
\textbf{Ours}       &  \xmark  &   \xmark   & \xmark  & \cmark  & 76.5 & 0.16 \\ \hline
\textbf{Ours}    &  \cmark  &  \xmark    & \xmark  & \cmark &   {79.1}   &  9  \\ \hline
\end{tabular}
\end{table*}

BVS and OFL are based on hand-crafted features and graphical models, while the rest are all deep learning based methods. Among the methods~\cite{Perazzi2017masktrack,Caelles2017osvos,voigtlaender2017online,Yang2018osmn} using image segmentation frameworks, OnAVOS achieves the best performance. However, its online adaption process makes the inference pretty slow (13s per frame). Our model without online learning (the second last row) achieves comparable results to other online learning methods without post-processing (\eg MaskTrack 69.8\% and OSVOS 77.4\%), but with a significant speed-up (60 times faster). Previous methods using spatial-temporal information including SegFlow, VPN and ConvGRU get inferior results compared to ours. Among them, ConvGRU is most related to ours since it also incorporates RNN memory cells in its model. However, it is an unsupervised methods to only segment moving foreground, while our method can segment arbitrary objects given the mask supervision.
Finally, online learning helps our model segment object boundary more accurately. Figure~\ref{fig:davis-res} shows such an example. 

\begingroup
\setlength{\tabcolsep}{1pt}
\renewcommand{\arraystretch}{1}
\begin{figure*}[t]
\centering
 \begin{tabular}{@{}ccccc@{}}
 \includegraphics[width=.19\textwidth]{dance-twirl_00002.jpg}  &
\includegraphics[width=.19\textwidth]{dance-twirl_00017.jpg}  &
\includegraphics[width=.19\textwidth]{dance-twirl_00031.jpg}  &
 \includegraphics[width=.19\textwidth]{dance-twirl_00068.jpg}  &
 \includegraphics[width=.19\textwidth]{dance-twirl_00085.jpg}  \\
 \includegraphics[width=.19\textwidth]{dance-twirl_online_00002.jpg}  &
\includegraphics[width=.19\textwidth]{dance-twirl_online_00017.jpg}  &
\includegraphics[width=.19\textwidth]{dance-twirl_online_00031.jpg}  &
 \includegraphics[width=.19\textwidth]{dance-twirl_online_00068.jpg}  &
 \includegraphics[width=.19\textwidth]{dance-twirl_online_00085.jpg}  
 \end{tabular}
 \caption{The comparison results between our model without online learning (upper row) and with online learning (bottom row). Each column shows predictions of the two models at the same frame. }
\label{fig:davis-res}
\end{figure*}
\setlength{\tabcolsep}{1.4pt}
\endgroup


To demonstrate the scale limitation of existing datasets, we train our models on three different settings and evaluate on DAVIS 2016. 
\begin{itemize}
\item Setting 1: We train our model from scratch on the 30 training videos. 
\item Setting 2: We train our model from scratch on the 30 training videos, plus all the videos from the SegTrackv2, JumpCut and YoutubeObjects datasets, which results in a total of 192 training videos.
\item Setting 3: Following the idea of ConvGRU, we use a pretrained object segmentation model DeepLab ~\cite{Chen2016Deeplab} as our \textit{Encoder} and train the other parts of our model on the 30 training videos.
\end{itemize}
Our models trained on setting 1 and 2 only get  and  mean IoU, which suggests that existing video object segmentation datasets do not have sufficient data to train our models. Therefore our \dataset~dataset is one of the key elements for the success of our algorithm. In addition, there is only little improvement by adding videos from the SegTrackv2, JumpCut and YoutubeObjects datasets, 
which suggests that the small scale is not the only problem for previous datasets. For example, videos in the three datasets usually only have one main foreground. SegTrackv2 has low-resolution videos. The annotation of YoutubeObjects videos is not accurate along object boundaries, \etc   However, our \dataset~dataset is carefully created to avoid all these problems. 
Setting 3 is a common detour for existing methods to bypass the data-insufficiency issue, \ie using pre-trained models on other large-scale datasets to reduce the parameters to be learned for their models. However, our model using this strategy gets even worse results () than training from scratch, which suggests that spatial-temporal features cannot be trivially transfered from representations learned from static images. Thus large scale training data such as our dataset is essential to learn spatial-temporal representation for video object segmentation.



\subsection{Ablation study} \label{sec:ablation}

In this subsection, we perform an ablation study on the~\dataset~dataset to evaluate different variants of our algorithm.

\begin{table*}[t]
\centering
\caption{The effect of data scale on our algorithm. We use different portions of training data to train our models and evaluate on the~\dataset~test set.}
\label{tab:ablation}
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
Scale      &  mean &  recall &  decay &  mean &   recall &   decay \\ \hline
25 &  46.7/40.1    &  53.5/45.6  & 8.3/13.6  &   46.7/40.0   &  52.2/41.6   &  8.5/13.2  \\ \hline
50 &  51.5/50.3   &  59.2/58.8    &  10.3/13.1  & 51.8/50.2  &  59.5/55.8  &  11.1/13.3    \\ \hline
75 &   56.8/56.0  &  65.7/67.1   &   7.6/10.0   & 59.6/56.3  & 68.8/64.1  &  8.5/11.1    \\ \hline
100  &  60.9/60.1  &   70.3/71.2   & 7.9/12.9  & 64.2/62.3  &  73.0/71.4    &  9.3/14.5  \\ \hline
\end{tabular}
\end{table*}


\paragraph{\textbf{Dataset scale.}}
Since the dataset scale is very important to our models, we train several models on different portions of the training set of~\dataset~to explore the effect of data. Specifically, we randomly select ,  and  of the training set and retrain our models from scratch. The results are listed in Table~\ref{tab:ablation}. It can be seen that using only  of the training videos (700 videos) drops the performance almost  from the original model. In addition, the performance of the model on unseen categories are much worse than its performance on seen categories, which suggests that the model fails to capture general features for objectness. Since the scale of adding all the videos from all existing datasets is still much less than 700 videos, there is no doubt that existing datasets cannot satisfy the needs of our algorithm. With more and more training videos, our algorithm is improved rapidly, which well demonstrates the importance of large-scale data on our algorithm. We can see the trend of accuracies for  data still have not reached a plateau. We are working on collecting more data to explore the impact of data on the algorithm further.

\paragraph{\textbf{\textit{Initializer} variants.}}
The \textit{Initializer} in our original model is a VGG-16 network which encodes a RGB frame and an object mask and outputs initial hidden states of \emph{ConvLSTM}. We would like to explore using the object mask directly as the initial hidden states of \emph{ConvLSTM}. We train an alternative model by removing the \textit{Initializer} and directly using the object mask as the hidden states, \ie the object mask is reshaped to match the size of the hidden states. The  mean of the adapted model are  on the seen categories and  on the unseen categories. This suggests that the object mask alone does not have enough information for localizing the object.

\paragraph{\textbf{\textit{Encoder} variants.}}
The \textit{Encoder} in our original model receives a RGB frame as input at each time step. Alternatively, we can use the segmentation mask of the previous step as additional inputs to explicitly provide extra information to the model, similar to MaskTrack~\cite{Perazzi2017masktrack}. In this way, our \textit{Initializer} and \textit{Encoder} can be replaced with a single VGG-16 network since the inputs at every time step have same dimensions and similar meaning. However, such a framework potentially has the error-drifting issue since segmentation mistakes made at previous steps will be propagated to the current step. 

In the early stage of training, the model is unable to predict good segmentation results. Therefore we use the ground truth annotation of the previous step as the input. Such strategy is known as teacher forcing~\cite{williams1989learning} which can make the training faster. After the training losses become stable, we replace the ground truth annotation with the model's prediction of the previous step so that the model is forced to correct its own mistakes. Such a strategy is known as curriculum learning~\cite{bengio2015scheduled}. Empirically we find that both the two strategies are important to make the model to work well. The  mean results of the model are  on the seen categories and  on the unseen categories, which is similar to our original model.

\section{Conclusion} \label{sec:conclusion}
In this work, we introduce the largest video object segmentation dataset (\dataset) to date. The new dataset, much larger than existing datasets in terms of number of videos and annotations, allows us to design a new deep learning algorithm to explicitly model long-term spatial-temporal dependency from videos for segmentation in an end-to-end learning framework. Thanks to the large scale dataset, our new algorithm achieves better or comparable results compared to existing state-of-the-art approaches. We believe the new dataset will foster research on video-based computer vision in general.

\section{Acknowledgement}
This research was partially supported by a gift funding from Snap Inc. and UIUC Andrew T. Yang Research and Entrepreneurship Award to Beckman Institute for Advanced Science \& Technology, UIUC. 



\bibliographystyle{splncs04}
\bibliography{egbib}
\end{document}
