

\documentclass{article}



\usepackage{booktabs} 

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} 

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{color}
\usepackage{wasysym}
\usepackage{commath}
\usepackage{mathtools}
\usepackage{pdfpages}

\let\oldnorm\norm   \let\norm\undefined \DeclarePairedDelimiter\norm{\lVert}{\rVert}
\renewcommand\thesection{\Alph{section}}





\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}



\usepackage[accepted]{icml2019}

\icmltitlerunning{TapNet: Neural Network Augmented with Task-Adaptive Projection for Few-Shot Learning}

\begin{document}

\twocolumn[
\icmltitle{TapNet: Neural Network Augmented with Task-Adaptive Projection for Few-Shot Learning }





\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Sung Whan Yoon}{kaist}
\icmlauthor{Jun Seo}{kaist}
\icmlauthor{Jaekyun Moon}{kaist}

\end{icmlauthorlist}

\icmlaffiliation{kaist}{School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Korea}


\icmlcorrespondingauthor{Sung Whan Yoon}{shyoon8@kaist.ac.kr}


\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]
{\large }




\printAffiliationsAndNotice{}  

\begin{abstract}
	
Handling previously unseen tasks after given only a few training examples continues to be a tough challenge in machine learning. We propose TapNets, neural networks augmented with task-adaptive projection for improved few-shot learning. Here, employing a meta-learning strategy with episode-based training, a network and a set of per-class reference vectors are learned across widely varying tasks. At the same time, for every episode, features in the embedding space are linearly projected into a new space as a form of quick task-specific conditioning. The training loss is obtained based on a distance metric between the query and the reference vectors in the projection space. Excellent generalization results in this way. When tested on the Omniglot, \textit{mini}ImageNet and \textit{tiered}ImageNet datasets, we obtain state of the art classification accuracies under various few-shot scenarios.
	
\end{abstract}

\section{Introduction}
\label{intro}

Few-shot learning promises to allow machines to carry out tasks that are previously unencountered, using only a small number of relevant examples. As such, few-shot learning finds wide applications, where labeled data are scarce or expensive, which is far more often the case than not. Unfortunately, despite immense interest and active research in recent years, few-shot learning remains an elusive challenge to machine learning community. For example, while deep networks now routinely offer near-perfect classification scores on standard image test datasets given ample training, reported results on few-shot learning still fall well below the levels that would be considered reliable in crucial real world settings. 

One popular way of developing few-shot learning strategies is to take a meta-learning perspective coupled with episodic training \cite{MN,Ravi, survey2}. Meta-learning seems to convey somewhat different meanings to different people, but none would disagree that it is about learning a general strategy to learn new tasks \cite{survey1}. Episodic training refers to a training method in which widely varying tasks (or episodes) are presented to the learning model one by one, with each episode containing only a few labeled examples. The repetitive exposure to previously unseen tasks, each time with low samples, during this initial learning or meta-training stage seems to provide a viable option for preparing the learner for quick adaptation to new data \cite{MN}. 

Among the well-known approaches in this direction are the metric-based learners like Matching Networks \cite{MN} and Prototypical Networks \cite{PN}. These methods all incorporate non-parametric, distance-based learning, where embedding space is trained to 
minimize a relevant distance metric across episodes before stabilizing to perform actual few-shot classification. 
Matching Networks train separate networks to process labeled samples and query samples, and utilize each labeled sample in the embedding space as reference points in classifying the query samples. 
Prototypical Networks employ only one embedding network with its per-class centroids used as classification references 
in the embedding space. 
Based on learning only a single feed-forward feature extractor, Prototypical Networks offer a surprisingly good ability to generalize to new tasks, as an inductive bias seems to settle in somehow via episodic training.  







We are also interested in distance-based learning with no fine-tuning of parameters beyond the episodic meta-training stage. 
Relative to prior work, the unique characteristic of our method is in explicit task-dependent conditioning via linear projection of embedded features. Once the neural network outputs are projected into a new space, classification is done there based on distances from per-class reference vectors. Both the neural network and the reference vectors are learned across the sequence of episodes reflecting widely varying tasks, while the projected classification space is constructed anew specific to each episode. The projection to an alternative classification space is done via linear nulling of errors between the embedded features and the per-class references. Unlike in \cite{MN} and \cite{PN}, class-representing vectors in our scheme are not the outputs of an embedding function. Rather, the references in our method are a simple set of stand-alone vectors not directly coupled to input images, although they are updated for each episode based on distances from the embedded query images projected in the classification space.   







The combination of across-task learning of the network and per-class reference vectors with a quick task-adaptive conditioning of classification space allows excellent generalization. Extensive testing on the Omniglot, \textit{mini}ImageNet and \textit{tiered}ImageNet datasets show that the proposed network augmented with task-adaptive projection (TapNet) yields state of the art few-shot classification accuracies. 



















\begin{figure*}
\centering
\includegraphics[width=0.90\textwidth]{figure2b_rev.pdf}
\caption{TapNet learning process}
\label{fig:opt}
\end{figure*}

\section{Task-Adaptive Projection Network}
\subsection{Model Description}
TapNet consists of three key elements: an embedding network , a set of per-class reference vectors , and the task-dependent adaptive projection or mapping  of embedded features to a new classification space.  is a matrix whose  row is the per-class reference (row) vector .  denotes projection or mapping, but sometimes would mean the projection space itself. 
See Fig. \ref{fig:opt}, where a new episode is being presented to the model during the sequential episodic training process. An episode consists of a support set of images/labels  as well as a query set . For clear illustration, there is only one image/label pair for each of  given classes, in either set here.  

Given the new support set, as well as  and  learned 
through the last episode stage, a projection space  is first constructed such that the embedded feature vectors 's and the class reference vectors 's with matching labels align closely when projected into . The details of projection space construction will be given shortly.  

The network   and the reference set  are in turn updated according to softmax based on Euclidean distance 
 between the mapped query image and reference vectors: 

which is averaged over all classes . 
Here,  denotes projection of row vector , and all vectors including the embedded feature vector  of input  are assumed to be row vectors, unless specified otherwise. 
The updated   and  are passed to the next episode processing stage. 
The projection and parameter updates continue for each episode until all given episodes are exhausted. 

Come the few-shot test stage, again a projection space is computed to align the embedded features of the presented shots appearing at the output of the network with the references, both of which are now fixed after having learned throughout the episodic meta-training process. The query image is finally compared with the references in the projection space for final classification. 


In summary, the embedder   and the per-class reference vectors  are learned across varying tasks (episodes)
while the projection space   is built specific to the given task, providing a quick task-dependent conditioning. This combination results in an excellent ability to generalize to new data, as extensive experimental results will verify shortly.


\subsection{Construction of Task-Adaptive Projection Space}
Finding the mapping function or projection space  is based on removing misalignment between the task-embedded features and the references. 
To handle general cases with multiple example images per class, let  be the per-class average of the embedded features for class  corresponding to the images out of the support set. 

We wish to find a mapper  such that  and the matching reference vector  are highly aligned in the mapped space. At the same time, it would be beneficial to make  and the non-matching weights  for all  well-separated in the same space. It turns out that a simple linear projection that does not require any learning offers an effective solution. The idea is to find a projection space where  aligns with a modified vector

where the factor  provides natural normalization reflecting the number of non-matching vectors. This is to say that given the error vector defined as 

 can be found such that the projected error vector  is zero for every , where
 is now a matrix whose columns span the projection space. In other words,  is found by a linear nulling of 
errors .  and  are normalized to remove power imbalance between them.
Formally, we express

where  is the column dimension of .  
A well-known solution is through the singular value decomposition (SVD) of the matrix , namely, by taking  of the right singular vectors of the matrix, from index  through . With  being the length of , if , then the projection space  with dimension  always exists.

Note that  can be set less than , indicating a possibility of significant dimension reduction. Our empirical observation suggests that a 
dimension reduction sometimes actually improves few-shot classification accuracies. Note that for SVD of an  matrix with , computational complexity is . The required SVD computational complexity for obtaining our projection is thus , which is small compared to typical model complexity.

We also remark that because the solution to linear nulling as formulated above exists irrespective of particular labeling of 's, we do not need to relabel the reference vectors in every episode; the same label sticks to each reference vector throughout the episodic training phase.  




\subsection{Training}
As mentioned, training of the embedding network  and the reference vectors  is done via episodic training, following \cite{MN}.
The detailed steps of learning TapNets is provided in Algorithm \ref{alg}. 

For each training episode,  classes are randomly chosen from the training set of a given dataset. 
Then, for each class,  labeled samples are randomly chosen as the support set , and   labeled samples are chosen as the query set , without any overlapping samples between the two sets.
With the support set , the average network output vector  is obtained for each class (in line 5). Based on the per-class average network output vectors, error vectors are obtained for all classes (in line 6)
without any relabeling on the reference vectors.
Then the projection space  is computed as a null-space of the error signals, as explained in the prior subsection.
For each query input, the Euclidean distances to the reference vectors in the projection space  are measured, and the training loss is computed using these distances.
The average training loss is obtained over all  query inputs for each of  given classes (in line 11 to 14).
The learnable parameters  of the embedding network and the references  are now updated based on the average training loss (in line 16). This process gets repeated for every remaining episode with new classes of images and queries. 

Following \cite{PN}, we also use a larger number of classes than  during episodic training, for improved performance. For example, 20-way classification is used during episodic learning or meta-training whereas 5-way classification is done in the final few-shot learning and testing. In this case, a question remains as to how 
5 reference vectors are chosen in the few-shot stage for linear projection out of 20 references that have been trained. For this, we first obtain 
the average vectors  for the 5 given classes and then select the 5 reference vectors closest to the 's and also relabel them accordingly before the linear projection is carried out. Notice that with the higher-way training employed, TapNets can easily handle cases where the number of classes for actual few-shot classification is not known in advance.





\begin{algorithm*}[h]
	\caption{Episodic learning is done by  episodes. Each episode  consists of  (image, label) pairs. These  shots are composed of  support images/labels and  queries from each of  given classes and .  is the loss for training learnable parameters. The Euclidean distance between two vectors is denoted as .  is the dimensionality of projection space .}
	\label{alg}
	\textbf{Input}: Training set  where  is an episode of  image/label pairs over  classes.  is a subset of  corresponding to label .
	\begin{algorithmic}[1]
		\FOR{ in }
		\STATE 
		
		\FOR{ in }
\STATE 
		\STATE 
		
		\ENDFOR
		\STATE 
		\FOR{ in }
\FOR{ in }
\STATE 
		\ENDFOR
		\ENDFOR
		\STATE Update  minimizing  via optimizer
		\ENDFOR
	\end{algorithmic}
\end{algorithm*}


\section{Related Work}


\subsection{Relation to Metric-Based Meta-Learners}
Two well-known metric-based few-shot learning algorithms are Matching Networks of \cite{MN} and Prototypical Networks of \cite{PN}. Matching Networks yield decisions based on matching the output of a network driven by a query sample to the output of another network fed by labeled samples. In Matching Networks, the similarities are measured between the query output and the labeled sample outputs on two separate embeddings. The labeled sample outputs are provided as reference points for estimating the label of the query. On the other hand, Prototypical Networks are trained to minimize the distance metric between the per-class average outputs and the query output from the single embedding network.
The per-class average outputs on the embedding space work as references 
and the embedding network is trained to make a given query output stay close to the correct reference while pushing it 
away from the incorrect reference points.

Our TapNet also learns to minimize distance between the projected query and per-class references. Unlike Matching Networks and Prototypical Networks, however, there is explicit task-dependent conditioning in TapNets in the form of projection into a new classification space. Assuming one-shot support and query samples for clear illustration, the distance functions utilized by three methods are compared as:

where  and  are the one-shot query and support samples, respectively, for class . Matching Networks
use two separate embedding functions  and , while Prototypical Networks rely on a single embedding function. TapNets employ one embedding network but there is a learnable reference vector set 's as well as task-conditioning linear projection . Also note that the class references in Matching Networks and Prototypical Networks are embedded features themselves, but
those in TapNets are not; rather, the per-class references in TapNets are stand-alone vectors that are not directly coupled to the input images.   

Built upon a base Prototypical Network model, TADAM of \cite{TADAM} employs learned metric-scaling and additional task-dependent conditioning in the form of element-wise scaling and shifts for the feature vectors of component convolutional layers, similar to \cite{FILM}. But this conditioning requires learning of extra fully-connected networks, whereas the task-conditioned  in TapNets is computed directly from the embedded features of the new task and the up-to-date references. A metric-based learner utilizing a nonlinear distance metric was also suggested in \cite{Yang}, where the distance metric itself is learned together with the embedding network.  

\subsection{Relation to Memory-Augmented Neural Network}

While our TapNet is close in spirit to the metric-based learners in the form of Matching Networks and Prototypical Networks as described above, it also bears a surprisingly close connection to the memory-augmented neural network (MANN) of \cite{MANN}. MANN 
utilizes an external memory module with its contents rapidly adapting to new samples while the read and write weights learned across episodes. 
The explicit form of the memory module is an external matrix array  designed to store information extracted by the controller network for a given episode. The memory read output vector  is seen as a weighted linear combination of the columns of : 
. The read weight is proportional to the cosine similarity of the column  of the memory with the controller network output or key vector :  

where  is the cosine correlation of two vectors. Approximating the exponentiation by a linear function, i.e., , and further dropping the class-independent constant,  
we can write 

where  is a column vector of the read weights. 
Eventually, we can approximate the read output  as

In arriving at the final decision, this memory read output is multiplied by the matrix  whose row vectors are learnable per-class weights:

While the inference from the direct branch of the hidden state of the long short-term memory (LSTM) is also used in the MANN, we consider only the inference from the read vector  which utilizes information from the external memory.
The resulting vector of (\ref{eq:mann}) is the inner-product similarities between the read output  and weights . 

Going back to our TapNet, if we were to use the cosine similarity instead of Euclidean distance, the distance profile between the query and the per-class references would be  
 , which is essentially the same as (\ref{eq:mann}) of MANN, with 
   playing the same role as , given that the key vector  of MANN is the same as the embedded feature of the input image for TapNets. Very interestingly, an important implication here is that the external memory array  of MANN can be interpreted as a kind of task-adaptive projection space where the similarities between the query key  and the weights  are measured. 
  
  \subsection{Other Types of Optimizers}
  
  There are other types of approaches often referred to as the optimization-based meta-learners. 
  They aim to optimize the embedding network quickly so that the fine-tuned network successfully adapts to the given task. 
  The meta-learner LSTM of \cite{Ravi} is one such approach, where an LSTM  \cite{LSTM} is trained to optimize another learner, which performs actual classification. There, the parameters of the few-shot learner are first set to the memory state of the LSTM, and then quickly learned based on the memory update rule of the LSTM, effectively capturing the knowledge from small samples. 


  The model-agnostic meta-learner (MAML) of \cite{MAML} sets up the model for easy fine-tuning so that a small number of gradient-based updates allows the model to yield a good generalization to new tasks. The fine-tuning method of MAML has been incorporated into many other schemes, such as Reptile of \cite{REPTILE} and Platipus of \cite{PMAML}. 
  Very recently, meta-learners with latent embedding optimization (LEO) of \cite{LEO} have been introduced that attempt at task-dependent 
  initialization of the model parameters with additional fine-tuning of the parameters in a low-dimensional latent space. Also, separate pre-training is required over the same training dataset in order for LEO to work properly.
  There also exist other meta-learners employing different forms of task-conditioning \cite{adares}. 
  A method dubbed the simple neural attentive meta-learner (SNAIL) combines an embedding network with temporal convolution and soft attention to draw from past experience while attempting precision access at the same time 
  \cite{SNAIL}.  
  
\section{Experiment Results}

\begin{table*}[h]
  \caption{Few-shot classification accuracies for 20-way Omniglot and 5-way \textit{mini}ImageNet}
  \label{acc_table1}
  \centering
  \begin{tabular}{l||cc||cc}
    \toprule  
    & \multicolumn{2}{c}{\textbf{20-way Omniglot}} & \multicolumn{2}{c}{\textbf{5-way \textit{mini}ImageNet}} \\
    \cmidrule{2-5}
    \textbf{Methods}    & 1-shot & 5-shot	& 1-shot & 5-shot  \\
    \midrule
    \textbf{Matching Nets} \cite{MN}  & 88.2\%	& 97.0\% & 43.56  0.84\%  & 55.31  0.73\%   \\
    \textbf{MAML} \cite{MAML} & 95.8\%	& 98.9\% & 48.70  1.84\%  & 63.15  0.91\%	 \\
    \textbf{Prototypical Nets} \cite{PN} & 96.0\%	& 98.9\% & 49.42  0.78\%  & 68.20  0.66\%	  \\
    \textbf{SNAIL} \cite{SNAIL}  & 97.64\% & 99.36\% & 55.71  0.99\%  & 68.88  0.92\%  \\
    \textbf{adaResNet} \cite{adares} & 96.12\% & 98.48\% & 56.88  0.62\% & 71.94  0.57\% \\
    \textbf{Transductive Propagation Nets} \cite{Liu} & - & - & 55.51  0.86\% & 69.86  0.65\% \\
    \textbf{TADAM-} \cite{TADAM} & - & - &56.8  0.3\% & 75.7  0.2\% \\ 
    \textbf{TADAM-TC} \cite{TADAM}  & - & - &58.5  0.3\% & \textbf{76.7}  \textbf{0.3}\% \\
    \textbf{TapNet} (Ours)  & \textbf{98.07}\% & \textbf{99.49}\%  &\textbf{61.65}  \textbf{0.15}\%  & \textbf{76.36}  \textbf{0.10}\%\\
    \bottomrule
  \end{tabular}
\end{table*}


\iffalse
\begin{table*}[h]
  \caption{Few-shot classification accuracies for 5-way \textit{mini}ImageNet}
  \label{acc_table2}
  \centering
  \begin{tabular}{l||cc}
    \toprule  
    & \multicolumn{2}{c}{\textbf{5-way \textit{mini}ImageNet}} \\
    \cmidrule{2-3}
    \textbf{Methods}    & 1-shot & 5-shot  \\
    \midrule
    \textbf{Matching Nets} \cite{MN}  & 43.56  0.84\%  & 55.31  0.73\%   \\
    \textbf{MAML} \cite{MAML} & 48.70  1.84\%  & 63.15  0.91\%	 \\
    \textbf{Prototypical Nets} \cite{PN} & 49.42  0.78\%  & 68.20  0.66\%	  \\
    \textbf{TapNet} (Ours, Conv4)  &\textbf{50.68}  \textbf{0.11}\%  & \textbf{69.00}  \textbf{0.09}\% \\
    \midrule
    \textbf{Transductive Propagation Nets} \cite{Liu} & 55.51 \% & 69.86\% \\
    \midrule
    \textbf{SNAIL} (ResNet-12) \cite{SNAIL}  & 55.71  0.99\%  & 68.88  0.92\%  \\
    \textbf{adaResNet} (ResNet-12 SNAIL-like) \cite{adares} & 56.88  0.62\% & 71.94  0.57\% \\
    \textbf{TapNet} (Ours, ResNet-12 SNAIL-like)   &\textbf{59.47}  \textbf{0.12}\%  & \textbf{72.79}  \textbf{0.10}\%\\
    \midrule
    \textbf{TADAM-} \cite{TADAM}  &56.8  0.3\% & 75.7  0.2\% \\ 
    \textbf{TADAM-TC} \cite{TADAM}   &58.5  0.3\% & \textbf{76.7}  \textbf{0.3}\% \\
    \textbf{TapNet} (Ours, ResNet-12 TADAM-like)   &\textbf{61.65}  \textbf{0.15}\%  & 76.36  0.10\%\\
    \bottomrule
  \end{tabular}
\end{table*}
\fi

\begin{table*}[h]
  \caption{Few-shot classification accuracies for 5-way \textit{tiered}ImageNet}
  \label{acc_table2}
  \centering
  \begin{tabular}{l||cc}
    \toprule  
    &  \multicolumn{2}{c}{\textbf{5-way \textit{tiered}ImageNet}} \\
    \cmidrule{2-3}
    \textbf{Methods} 	& 1-shot & 5-shot  \\
    \midrule
    \textbf{MAML} (as evaluated in \cite{Liu})  & 51.67  1.81\%  & 70.30  1.75\%   \\
    \textbf{Prototypical Nets} (as evaluated in \cite{Liu}) & 53.31  0.89\%  & 72.69  0.74\%	 \\
    \textbf{Relation Nets} (as evaluated in \cite{Liu}) & 54.48  0.93\%  & 71.31  0.78\%	  \\
    \textbf{Transductive Propagation Nets} \cite{Liu}  & 59.91  0.94\%  & 73.30  0.75\%  \\
    \textbf{TapNet} (Ours)   &\textbf{63.08}  \textbf{0.15}\%  & \textbf{80.26}  \textbf{0.12}\%\\
    \bottomrule
  \end{tabular}
\end{table*}

\subsection{Datasets}
\textbf{Omniglot} \cite{Omniglot} is a set of images of 1623 handwritten characters from 50 alphabets with 20 examples for each class. We have used 2828 downsized grayscale images and introduced class-level data augmentation by random angle rotation of images in multiples of 90 degrees, as done in prior works \cite{MANN,PN,MN}. 1200 characters are used for training and test is done with the remaining characters.

\textbf{\textit{mini}ImageNet} \cite{MN} is a dataset suggested by Vinyals et al. for few-shot classification of colored images. It is a subset of the ILSVRC-12 ImageNet dataset \cite{Imagenet} with 100 classes and 600 images per class. We have used the splits introduced by Ravi and Larochelle \cite{Ravi}. For our experiment, we have used 8484 downsized color images with a split of 64 training classes, 16 validation classes and 20 test classes.

\textbf{\textit{tiered}ImageNet} \cite{MRen} is a dataset suggested by Ren et al. It is a larger subset of the ILSVRC-12 ImageNet dataset with 608 classes and 779,165 images in total. The classes in \textit{tiered}ImageNet are grouped into 34 categories corresponding to higher-level nodes in the ImageNet hierarchy curated by human \cite{Deng}. These categories are split into 20 training, 6 validation and 8 test categories, and the training, validation and test sets contain 351, 97 and 160 classes, respectively. The split of \textit{tiered}ImageNet ensures that the classes in the training set are distinct from those in the test set, possibly resulting in more realistic classification scenarios.

\begin{figure*}[!htbp]
	\centering
	\includegraphics[width=120mm]{tSNE_tmp3.pdf}
	\caption{t-SNE visualization of the network embedding space (left) and projection space (right) }
	\label{fig:tSNE_diagram}
\end{figure*}

\subsection{Experimental Settings}



For all our experiments here, we employ ResNet-12 \cite{Resnet} as the embedding network (results with smaller networks are also available as discussed in Supplementary Material).
 ResNet-12 has four residual blocks, each of which contains three 33 convolution layers and convolutional shortcut connection. Each convolution layer is followed by a batch normalization layer and ReLU activation. A 22 max-pooling is applied at the end of each residual block. At the top of the stack of residual blocks, we also apply a global average-pooling to reduce feature dimensionality. We use different numbers of channels for the three datasets. For 5-way \textit{mini}ImageNet and 5-way \textit{tiered}ImageNet, the number of channels starts with 64, and doubles after the max-pooling is applied. For 20-way Omniglot classification, the number of channels starts with 64 and then increases for the subsequent blocks, although less channels are employed at later blocks than the \textit{mini}ImageNet and \textit{tiered}ImageNet cases.

The Adam optimizer \cite{Adam} with an optimized learning-rate decay is employed. 
For all experiments, the initial learning rate is . In the 20-way Omniglot experiment, the learning rate is reduced by half at every  episodes, but for 5-way \textit{mini}ImageNet and 5-way \textit{tiered}ImageNet classification, we cut the learning rate by a factor of 10 at every  and  episodes, respectively,  for 1-shot experiments and every  and  episodes, respectively, for 5-shot experiments.

\iffalse
For Omniglot and \textit{mini}ImageNet experiment, we employ the same embedding convolutional neural network (CNN) widely used in prior works. It is based on four convolutional blocks, each of which consists of a 33 2D convolution layer with 64 filters, stride 1 and padding, a batch normalization layer \cite{BN}, a ReLU activation and a 22 max-pooling, as done in \cite{PN,MN}. This CNN is denoted as Conv4 in the following section and the table. The Adam optimizer \cite{Adam} is used for optimization.

For \textit{mini}ImageNet and \textit{tiered}ImageNet experiment, we employ ResNet-12 \cite{Resnet} as the embedding network. ResNet-12 has four residual blocks with three 3 2D convolution layers and convolutional shortcut connection. Each convolution layer is followed by a batch normalization layer and ReLU activation. The 22 max-pooling is applied at the end of each residual block. On the top of stack of residual blocks, we applied the global average-pooling to reduce the dimensionality of feature. The number of channel starts with 64, and increased after the max-pooling is applied. The Adam optimizer \cite{Adam} with optimized learning-rate decay is employed for optimization.
\fi


For meta-training of the network, we adopt the higher-way training of Prototypical Networks of \cite{PN}. We used 60-way episodes for 20-way Omniglot classification, and 20-way episodes for 5-way \textit{mini}ImageNet and \textit{tiered}ImageNet classification for training. In the test phase, we have to choose 20 and 5 references among 60 and 20 vectors, respectively. In selecting only a subset of reference vectors for testing purposes, relabeling is done. For each average network output chosen in arbitrary order, the closest vector among the remaining ones in  is tagged with the matching label.
The closeness measure is the Euclidean distance in our experiments. 
After choosing the closest reference vectors, the projection space  is obtained for few-shot classification. The experimental results of our meta-learner in Tables \ref{acc_table1} and \ref{acc_table2} are based on 60-way initial learning for 20-way Omniglot and 20-way initial learning for 5-way \textit{mini}ImageNet and 5-way \textit{tiered}ImageNet classification.

For 20-way Omniglot classification, we used  training episodes with 15 query samples per class. For both 5-way \textit{mini}ImageNet and 5-way \textit{tiered}ImageNet settings,  training episodes with 8 query samples per class are used. For all experiments, we pick the best model with the highest validation accuracy during meta-training. Additional parameter settings are considered in Supplementary Material.
\subsection{Results}



In Tables \ref{acc_table1} and \ref{acc_table2}, few-shot classification accuracies on the Omniglot, \textit{mini}ImageNet and \textit{tiered}ImageNet datasets are compared\footnote{Codes are available on https://github.com/istarjun/TapNet}. The performance in the 20-way Omniglot experiment is evaluated by the average accuracy over randomly chosen  test episodes with 5 query images for each class. On the other hand, the performance in 5-way \textit{mini}ImageNet and \textit{tiered}ImageNet is evaluated by the average accuracy and a 95\% confidence interval over randomly chosen  test episodes with 15 query images for each class. 

For the 20-way Omniglot results in Table \ref{acc_table1}, TapNet shows the best performance for both 1-shot and 5-shot cases. For the 5-way \textit{mini}ImageNet results, our TapNet shows the best 1-shot accuracy and a 5-shot accuracy comparable to the best,
that of TADAM-TC, in the sense that the confidence intervals overlap. 
TADAM- is a model with metric scaling but without task-conditioning, built upon Prototypical Networks with the same ResNet-12 base architecture we use. TapNet achieves a higher accuracy than TADAM-. TADAM-TC requires additional fully-connected layers for task-conditioning.



In Table \ref{acc_table2}, the results for \textit{tiered}ImageNet experiments are presented. We compared our method with prior work as evaluated in \cite{Liu}. Our TapNet also achieves the best performance for both 1-shot and 5-shot classification tasks with considerable margins.

We remark that although better results have been reported recently in \cite{LEO}, the base feature extractor used there is Wide ResNet-28-10, which is considerably larger - more than twice as deep and also much wider - than our base model, ResNet-12. In addition, for the method of \cite{LEO}, a separate round of pre-training is necessary over the same training set. At this point, we do not make direct performance comparison with \cite{LEO}.
Additional experimental results with varying network sizes are presented in Supplementary Material.

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.40\textwidth]{dist_phi.pdf}
	\caption{Minimum distance between the normalized references}
	\label{fig:dist_phi}
\end{figure}



\begin{figure*}[h]
	\centering
	\begin{subfigure}[b]{0.38\textwidth}
		\includegraphics[width=\textwidth]{tsne_phi.pdf}
		\subcaption{t-SNE visualization for the trajectories of 's}
		\label{fig:tsne_phi}
	\end{subfigure}	
	\label{fig:PhiTrain}
	\begin{subfigure}[b]{0.38\textwidth}
		\includegraphics[width=\textwidth]{tsne_phi_tmp.pdf}
		\subcaption{t-SNE visualization of four selected references}
		\label{fig:tsne_phi3}
	\end{subfigure}	
	\caption{Trajectories of  in episodic learning}
	\label{fig:tsne_phi_full}
\end{figure*} 

\textbf{t-SNE plot.}
Fig. \ref{fig:tSNE_diagram} is a t-SNE visualization of network embedding space and projection space for TapNets trained with the Omniglot dataset. The reference vector 's are marked as the alphabet images in the circles. We can observe that the extracted images and the corresponding reference vectors are not located closely in the embedding space, while they lie close in the projection space. Moreover, note that the projected images are not only closely located to the matching references, but also tend to be away from non-matching references. This is due to the inclusion of the non-matching references in the modified reference vector
,
utilized in defining the error vector to null in the linear projection process.

\textbf{Learning trend of references .} In Figs. \ref{fig:dist_phi} and \ref{fig:tsne_phi_full}, we visualize how the labeled references  are being optimized during the meta-training phase. 
First, Fig. \ref{fig:dist_phi} shows a plot of the Euclidean minimum distance between the normalized 's.
The minimum distance escalates quickly during the first  episodes, and then increases slowly until the learning rate decay after  episodes. This implies that the references tend to grow apart, showing improving separation over time. The solid dot indicates the moment where the model yields the highest validation accuracy.
We can develop further insights by exploring the trajectories of the vector tips of 's. Fig. \ref{fig:tsne_phi} represents a t-SNE visualization of the trajectories of 20 references.
From the random initial points which are not well-separated, the references spread out for better separation, consistent with the observation from the minimum distance plot.
In Fig. \ref{fig:tsne_phi3}, only 4 references are selected for a clearer illustration. For a given reference vector, the numbers shown by the pointing arrows represent time stamps as indicated by the number of episodes processed. As seen, there appears to be a sudden jump at some point as the vector tip grows radially, and as it reaches around the optimal point it no longer seems to move outward, tending to settle into a place.

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.39\textwidth]{Dimension2.pdf}
	\caption{Test accuracies for 5-way 5-shot \textit{mini}ImageNet classification with different projection space dimensionalities}
	\label{fig:dimension}
\end{figure}

\textbf{Dimensionality of projection space.}
We study the effects of the dimensionality of projection space  on generalization performance. 
For the 5-way \textit{mini}ImageNet experiments, the full dimensionality  of  is . This means that   is 492 for training and 507 for testing.
We carry out the 5-way 5-shot \textit{mini}ImageNet classification test with projection using various values of .  
In Fig. \ref{fig:dimension}, we  observe that the test accuracy improves rapidly as dimensionality reaches around 50 and then settles down with a slight peak around 200. In our experiments we typically use full dimension with an exception of 
5-way, 5-shot \textit{mini}ImageNet classification, where  is used, as taught by Fig. \ref{fig:dimension}. 

\section{Conclusions}
In this work, we proposed a few-shot learning algorithm aided by a linear transformer that 
performed task-specific null-space projection of the network output.
The original feature space is linearly
projected into another space where actual classification takes place.
Both the embedding network and the per-class references are learned over the entire episodes while 
projection is specific to the given episode. The resulting combination shows an excellent generalization capability to new tasks. 
State of the art few-shot classification accuracies have been observed on standard image datasets. Relationships to other metric-based meta-learners 
as well as memory-augmented networks have been explored. 



\nocite{langley00}

\section*{Acknowledgements}
This work is supported in part by the ICT R\&D program of Institute for Information \& Communications Technology Promotion (IITP) grant funded by the Korea government (MSIP) [2016-0-005630031001, Research on Adaptive Machine Learning Technology Development for Intelligent Autonomous Digital Companion].

\bibliography{ICML2019}
\bibliographystyle{icml2019}
\includepdf[pages=-]{supp.pdf}






\iffalse
\appendix
\section{Do \emph{not} have an appendix here}

\textbf{\emph{Do not put content after the references.}}
Put anything that you might normally include after the references in a separate
supplementary file.

We recommend that you build supplementary material in a separate document.
If you must create one PDF and cut it up, please be careful to use a tool that
doesn't alter the margins, and that doesn't aggressively rewrite the PDF file.
pdftk usually works fine. 

\textbf{Please do not use Apple's preview to cut off supplementary material.} In
previous years it has altered margins, and created headaches at the camera-ready
stage. 






\fi
\end{document}
