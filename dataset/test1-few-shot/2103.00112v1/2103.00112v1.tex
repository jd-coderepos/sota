\documentclass{article}




\usepackage[preprint,nonatbib]{neurips_2020}





\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{graphicx}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{bbding}
\usepackage{wrapfig}

\title{Transformer in Transformer}



\author{Kai Han~~An Xiao~~Enhua Wu~~Jianyuan Guo~~Chunjing Xu~~Yunhe Wang \\
  Noah's Ark Lab, Huawei Technologies\\
  State Key Lab of Computer Science, ISCAS \& UCAS\\
  \texttt{\small \{kai.han,an.xiao,jianyuan.guo,xuchunjing,yunhe.wang\}@huawei.com,\ ehwu@um.edu.mo} \\
}

\begin{document}

\maketitle

\begin{abstract}
	Transformer is a type of self-attention-based neural networks originally applied for NLP tasks. Recently, pure transformer-based models are proposed to solve computer vision problems. These visual transformers usually view an image as a sequence of patches while they ignore the intrinsic structure information inside each patch. In this paper, we propose a novel Transformer-iN-Transformer (TNT) model for modeling both patch-level and pixel-level representation. In each TNT block, an outer transformer block is utilized to process patch embeddings, and an inner transformer block extracts local features from pixel embeddings. The pixel-level feature is projected to the space of patch embedding by a linear transformation layer and then added into the patch. By stacking the TNT blocks, we build the TNT model for image recognition. Experiments on ImageNet benchmark and downstream tasks demonstrate the superiority and efficiency of the proposed TNT architecture. For example, our TNT achieves  top-1 accuracy on ImageNet which is  higher than that of DeiT with similar computational cost. The code will be available at \url{https://github.com/huawei-noah/noah-research/tree/master/TNT}.
\end{abstract}

\section{Introduction}
\label{sec:intro}
Transformer is a type of neural network mainly based on self-attention mechanism~\cite{Att}. Transformer is widely used in the field of natural language processing (NLP), \emph{e.g.}, the famous BERT~\cite{bert} and GPT-3~\cite{gpt3} models. Inspired by the breakthrough of transformer in NLP, researchers have recently applied transformer to computer vision (CV) tasks such as image recognition~\cite{vit,deit}, object detection~\cite{detr,ddetr}, and image processing~\cite{ipt}. For example, DETR~\cite{detr} treats object detection as a direct set prediction problem and solve it using a transformer encoder-decoder architecture. IPT~\cite{ipt} utilizes transformer for handling multiple low-level vision tasks in a single model. Compared to the mainstream CNN models~\cite{nin,resnet,efficientnet}, these transformer-based models have also shown promising performance on visual tasks~\cite{han2020survey}.

CV models purely based on transformer are attractive because they provide an computing paradigm without the image-specific inductive bias, which is completely different from convolutional neural networks (CNNs). Chen \emph{et.al} proposed iGPT~\cite{igpt}, the pioneering work applying pure transformer model on image recognition by self-supervised pre-training. ViT~\cite{vit} views an image as a sequence of patches and perform classification with a transformer encoder. DeiT~\cite{deit} further explores the data-efficient training and distillation of ViT. Compared to CNNs, transformer-based models can also achieve competitive accuracy without inductive bias, \emph{e.g.}, DeiT trained from scratch has an 81.8\% ImageNet top-1 accuracy by using 86.4M parameters and 17.6B FLOPs.

In ViT~\cite{vit}, an image is split into a sequence of patches and each patch is simply transformed into a vector (embedding). The embeddings are processed by vanilla transformer block. By doing so, ViT can process images by a standard transformer with few modifications. This design also affects the subsequent works including DeiT~\cite{deit}, ViT-FRCNN~\cite{beal2020toward}, IPT~\cite{ipt} and SETR~\cite{setr}. However, these visual transformers ignore the local relation and structure information inside the patch which is important for visual recognition~\cite{sift,bagnet}. By projecting the patch into a vector, the spatial structure is corrupted and hard to learn.

In this paper, we propose a novel Transformer-iN-Transformer (TNT) architecture for visual recognition as shown in Fig.~\ref{Fig:tnt}. Specifically, an image is split into a sequence of patches and each patch is reshaped to (super) pixel sequence. The patch embeddings and pixel embeddings are obtained by a linear transformation from the patches and pixels respectively, and then they are fed into a stack of TNT blocks for representation learning. In the TNT block, there are two transformer blocks where the outer transformer block models the global relation among patch embeddings, and the inner one extracts local structure information of pixel embeddings. The local information is added on the patch embedding by linearly projecting the pixel embeddings into the space of patch embedding. Patch-level and pixel-level position embeddings are introduced in order to retain spatial information. Finally, the class token is used for classification via a MLP head. Through the proposed TNT model, we can model both global and local structure information of the images and improve the representation ability of the feature. Experiments on ImageNet benchmark and downstream tasks demonstrate the superiority of our method in terms of accuracy and FLOPs. For example, our TNT-S achieves 81.3\% ImageNet top-1 with only 5.2B FLOPs, which is 1.5\% higher than that of DeiT with similar computational cost.




\begin{figure*}[t]
	\centering	
	\setlength{\tabcolsep}{2pt}{
	\begin{tabular}{cc}
		\includegraphics[width=0.65\linewidth]{fig/tnt-net.png} \hspace{0.1cm} &\includegraphics[width=0.3\linewidth]{fig/tnt1.png} \hspace{0.1cm}
		\\
		\small (a) TNT framework. & \small (b) TNT block.
	\end{tabular}
	}
	\caption{Illustration of the proposed Transformer-iN-Transformer (TNT) framework. The position embedding is not drawn for neatness. T-Block denotes transformer block.}
	\label{Fig:tnt}
\end{figure*}


\section{Approach}
\label{sec:method}
In this section, we describe the proposed transformer-in-transformer architecture and analyze the complexity in details.

\subsection{Preliminaries}
We first briefly describe the basic components in transformer~\cite{Att}, including MSA (Multi-head Self-Attention), MLP (Multi-Layer Perceptron) and LN (Layer Normalization).

\paragraph{MSA.}
In the self-attention module, the inputs  are linearly transformed to three parts, \emph{i.e.}, queries , keys  and values  where  is the sequence length, , ,  are the dimensions of inputs, queries (keys) and values, respectively. The scaled dot-product attention is applied on :

Finally, a linear layer is used to produce the output. Multi-head self-attention splits the queries, keys and values for  times and perform the attention function in parallel, and then the output values of each head are concatenated and linearly projected to form the final output.

\paragraph{MLP.}
The MLP is applied between self-attention layers for feature transformation and non-linearity:

where  and  are weights of the two fully-connected layers respectively,  and  are the bias terms, and  is the activation function such as GELU~\cite{gelu}.

\paragraph{LN.}
Layer normalization~\cite{ba2016layer} is a key part in transformer for stable training and faster convergence. LN is applied over each sample  as follows:

where ,  are the mean and standard deviation of the feature respectively,  is the element-wise dot, and ,  are learnable affine transform parameters.

\subsection{Transformer in Transformer}
Given a 2D image, we uniformly split it into  patches , where  is the resolution of each image patch. ViT~\cite{vit} just utilizes a standard transformer to process the sequence of patches which corrupts the local structure of a patch, as shown in Fig.~\ref{Fig:tnt}(a). Instead, we propose Transformer-iN-Transformer (TNT) architecture to learn both global and local information in an image. In TNT, each patch is further transformed into the target size  with pixel unfold~\cite{pytorch}, and with a linear projection, the sequence of patch tensors is formed as

where , , and  is the number of channels. In particular, we view each patch tensor  as a sequence of pixel embeddings:

where , and , .

In TNT, we have two data flows in which one flow operates across the patch and the other processes the pixels inside each patch. For the pixel embeddings, we utilize a transformer block to explore the relation between pixels:

where  is index of the -th layer, and  is the total number of layers. All patch tensors after transformation are . This can be viewed as an inner transformer block, denoted as .  This process builds the relationship among pixels by computing interactions between any two pixels. For example, in a patch of human face, a pixel belonging to the eye is more related to other pixels of eyes while interacts less with forehead pixels.

For the patch level, we create the patch embedding memories to store the sequence of patch-level representations:  where  is the classification token similar to ViT~\cite{vit}, and all of them are initialized as zero. In each layer, the patch tensors are transformed into the domain of patch embeddings by linear projection and added into the patch embeddings:

where ,  flattens the input to a vector, and  and  are the weights and bias respectively.  We use the standard transformer block for transforming the patch embeddings:

This outer transformer block  is used for modeling relationship among patch embeddings.

In summary, the inputs and outputs of the TNT block include the pixel embeddings and patch embeddings as shown in Fig.~\ref{Fig:tnt}(b), so the TNT can be formulated as

In our TNT block, the inner transformer block is used to model the relationship between pixels for local feature extraction, and the outer transformer block captures the intrinsic information from the sequence of patches. By stacking the TNT blocks for  times, we build the transformer-in-transformer network. Finally, the classification token serves as the image representation and a fully-connected layer is applied for classification.

\begin{wrapfigure}{r}{0.5\textwidth}
	\vspace{-1.5em}
	\begin{center}
		\includegraphics[width=0.5\textwidth]{./fig/position.png}
	\end{center}
	\vspace{-1em}
	\caption{Patch-level and pixel-level position encodings.}\label{Fig:position}
	\vspace{-0em}
\end{wrapfigure}


\paragraph{Position encoding.}
Spatial information is an important factor in image recognition. For patch embeddings and pixels embeddings, we both add the corresponding position encodings to retain spatial information as shown in Fig.~\ref{Fig:position}. The standard learnable 1D position encodings are utilized here. Specifically, each patch is assigned with a position encodings:

where  are the patch position encodings. As for the pixels in a patch, a pixel position encoding is added to each pixel embedding:

where  are the pixel position encodings which are shared across patches. In this way, patch position encoding can maintain the global spatial information, while pixel position encoding is used for preserving the local relative position.


\subsection{Complexity Analysis}
A standard transformer block includes two parts, \emph{i.e.}, the multi-head self-attention and multi-layer perceptron. The FLOPs of MSA are , and the FLOPs of MLP are  where  is the dimension expansion ratio of hidden layer in MLP. Overall, the FLOPs of a standard transformer block are

Since  is usually set as 4, and the dimensions of input, key (query) and value are usually set as the same, the FLOPs calculation can be simplified as

The number of parameters can be obtained as


Our TNT block consists of three parts: an inner transformer block , an outer transformer block  and a linear layer. The computation complexity of  and  are  and  respectively. The linear layer has FLOPs of . In total, the FLOPs of TNT block are

Similarly, the parameter complexity of TNT block is calculated as


Although we add two more components in our TNT block, the increase of FLOPs is small since  and  in practice. For example, in the ViT-B/16 configuration, we have  and . We set  and  in our structure of TNT-B correspondingly. From Eq.~\ref{eq:flops_t} and Eq.~\ref{eq:flops_tnt}, we can obtain that  and . The FLOPs ratio of TNT block over standard transformer block is about . Similarly, the parameters ratio is about . With a small increase of computation and memory cost, our TNT block can efficiently model the local structure information and achieve a much better trade-off between accuracy and complexity as demonstrated in the experiments.


\subsection{Network Architecture}
We build our TNT architectures by following the basic configuration of ViT~\cite{vit} and DeiT~\cite{deit}. The patch size is set as 1616. The unfolded patch size  is set as 4 by default, and other size values are evaluated in the ablation studies. As shown in Table~\ref{tab:arch}, there are two variants of TNT networks with different model size, namely, TNT-S and TNT-B. They consist of 23.8M and 65.6M parameters respectively. The corresponding FLOPs for processing a 224224 image are 5.2B and 14.1B respectively.

\paragraph{Operational optimizations.}
Inspired by squeeze-and-excitation (SE) network for CNNs~\cite{senet}, we propose to explore channel-wise attention for transformers. We first average all the patch (pixel) embeddings and use a two-layer MLP to calculate the attention values. The attention is multiplied to all the embeddings. The SE module only brings in a few extra parameters but is able to perform dimension-wise attention for feature enhancement.



\begin{table}[htp]
	\vspace{-0.5em}
	\small 
	\centering
	\caption{Variants of our TNT architecture. `S' means small, `B' means base. The FLOPs are calculated for images at resolution 224224.}\label{tab:arch}
	\renewcommand{\arraystretch}{1.05}
	\setlength{\tabcolsep}{5pt}{
	\begin{tabular}{l|c|p{0.9cm}<{\centering}|p{0.9cm}<{\centering}|p{0.9cm}<{\centering}|p{0.9cm}<{\centering}|p{0.9cm}<{\centering}|p{0.9cm}<{\centering}|c|c}
		\toprule[1.5pt]
		\multirow{2}{*}{Model}    & \multirow{2}{*}{Depth}   & \multicolumn{3}{c|}{Inner transformer} & \multicolumn{3}{c|}{Outer transformer} & Params & FLOPs \\
		& & dim  & \#heads & MLP  & dim  & \#heads & MLP  & (M) & (B) \\
		\midrule
TNT-S (ours) & 12 & 24 & 4 & 4 & 384 & 6 & 4 & 23.8 & 5.2 \\
		TNT-B (ours) & 12 & 40 & 4 & 4 & 640 & 10 & 4 & 65.6 & 14.1 \\
\bottomrule[1pt]
	\end{tabular}
	}
	\vspace{-1.0em}
\end{table}


\section{Experiments}
In this section, we conduct extensive experiments on visual benchmarks to evaluate the effectiveness of the proposed TNT architecture.

\subsection{Datasets and Experimental Settings}
\paragraph{Datasets.}
ImageNet ILSVRC 2012~\cite{imagenet} is an image classification benchmark consisting of 1.2M training images belonging to 1000 classes, and 50K validation images with 50 images per class. We adopt the same data augmentation strategy as that in DeiT~\cite{deit} including random crop, random clip, Rand-Augment~\cite{randaugment}, Mixup~\cite{mixup} and CutMix~\cite{cutmix}.

In addition to ImageNet, we also test on the downstream tasks with transfer learning to evaluate the generalization of TNT. The details of used visual datasets are listed in Table~\ref{tab:data}. The data augmentation strategy are the same as that of ImageNet.
\begin{table}[htp]
	\small 
	\centering
	\caption{Details of used visual datasets.}\label{tab:data}
	\renewcommand{\arraystretch}{1.0}
	\setlength{\tabcolsep}{6pt}{
		\begin{tabular}{l|c|c|c}
			\toprule[1.5pt]
			Dataset  & Train size & Test size  & Classes \\
			\midrule
			ImageNet~\cite{imagenet} & 1,281,167 & 50,000 & 1000 \\
			\midrule
			\multicolumn{4}{l}{\emph{Fine-grained}} \\
			Oxford 102 Flowers~\cite{flower} & 2,040 & 6,149 & 102\\
			Oxford-IIIT Pets~\cite{pets} & 3,680 & 3,669 & 37 \\
			\midrule
			\multicolumn{4}{l}{\emph{Superordinate-level}} \\
			CIFAR-10~\cite{cifar} & 50,000 & 10,000 & 10 \\
			CIFAR-100~\cite{cifar}  & 50,000 & 10,000 & 100 \\
			\bottomrule[1pt]
		\end{tabular}
	}
	\vspace{-1.0em}
\end{table}


\paragraph{Implementation Details.}
We utilize the training strategy provided in DeiT~\cite{deit}. The main advanced technologies apart from common setting~\cite{resnet} include AdamW~\cite{adamw}, label smoothing~\cite{label-smooth}, DropPath~\cite{larsson2016fractalnet}, repeated augmentation~\cite{hoffer2020augment}.
We list the hyper-parameters in Table~\ref{tab:hyperparameters} for better understanding. All the models are implemented using PyTorch~\cite{pytorch} and trained on NVIDIA Tesla V100 GPUs.

\begin{table}[htp]
	\vspace{-0em}
	\small 
	\centering
	\caption{Default training hyper-parameters used in our method, unless stated otherwise.}\label{tab:hyperparameters}
	\renewcommand{\arraystretch}{1.05}
	\setlength{\tabcolsep}{5.8pt}{
		\begin{tabular}{c|c|c|c|c|c|c|c|c|c}
			\toprule[1.5pt]
			\multirow{2}{*}{Epochs} & \multirow{2}{*}{Optimizer} & Batch & Learning & LR & Weight & Warmup & Label & Drop & Repeated \\
			 & & size & rate & decay & decay & epochs & smooth & path & Aug \\
			\midrule
			300 & AdamW & 1024 & 1e-3 & cosine & 0.05 & 5 & 0.1 & 0.1 &  \\
			\bottomrule[1pt]
		\end{tabular}
	}
	\vspace{-1.0em}
\end{table}

\subsection{TNT on ImageNet}
We train our TNT models with the same training setting as that of DeiT~\cite{deit}. The recent transformer-based models like ViT~\cite{vit} and DeiT~\cite{deit} are compared. To have a better understanding of current progress of visual transformers, we also include the representative CNN-based models such as ResNet~\cite{resnet}, RegNet~\cite{regnet} and EfficientNet~\cite{efficientnet}. The results are shown in Table~\ref{tab:sota}. We can see that our transformer-based model, \emph{i.e.}, TNT outperforms all the other visual transformer models. In particular, TNT-S achieves 81.3\% top-1 accuracy which is 1.5\% higher than the baseline model DeiT-S, indicating the benefit of the introduced TNT framework to preserve local structure information inside the patch. By adding SE module, TNT-S model can be further improved to obtain 81.6\% top-1. Compared to CNNs, TNT can outperform the widely-used ResNet and RegNet. Note that all the transformer-based models are still inferior to EfficientNet which utilizes special depth-wise convolutions, so it is yet a challenge of how to beat EfficientNet using pure transformer.

\begin{table}[htp]
	\vspace{-0em}
	\small 
	\centering
	\caption{Results of TNT and other networks on ImageNet.}\label{tab:sota}
	\renewcommand{\arraystretch}{1.0}
	\setlength{\tabcolsep}{8pt}{
		\begin{tabular}{l|c|c|c|c|c}
			\toprule[1.5pt]
			Model    &  Resolution  & Params (M) & FLOPs (B) & Top-1 (\%) & Top-5 (\%) \\
			\midrule
			\multicolumn{6}{l}{\textbf{CNN-based}} \\
			ResNet-50~\cite{resnet} & 224224 & 25.6 & 4.1 & 76.2 & 92.9 \\
			ResNet-152~\cite{resnet} & 224224 & 60.2 & 11.5 & 78.3 & 94.1 \\
			RegNetY-8GF~\cite{regnet} & 224224 & 39.2 & 8.0 & 79.9 & - \\
			RegNetY-16GF~\cite{regnet} & 224224 & 83.6 & 15.9 & 80.4 & - \\
			GhostNet-A~\cite{ghostnet,tinynet} & 240240 & 11.9 & 0.6 & 79.4 & 94.5 \\
			EfficientNet-B3~\cite{efficientnet} & 300300 & 12.0 & 1.8 & 81.6 & 94.9 \\
			EfficientNet-B4~\cite{efficientnet} & 380380 & 19.0 & 4.2 & 82.9 & 96.4 \\
			\midrule[1pt]
			\multicolumn{6}{l}{\textbf{Transformer-based}} \\
			DeiT-S~\cite{deit}  & 224224 & 22.1 & 4.6 & 79.8 & - \\
			T2T-ViT\_{t}-14~\cite{yuan2021tokenstotoken} & 224224 & 21.5 & 5.2 & 80.7 & - \\
			TNT-S (ours)  & 224224 & 23.8 & 5.2 & 81.3 & 95.6 \\
			TNT-S + SE (ours)  & 224224 & 24.7 & 5.2 & 81.6 & 95.7 \\
			\midrule
			ViT-B/16~\cite{vit}  & 384384 & 86.4 & 55.5 &  77.9 & - \\
DeiT-B~\cite{deit}   & 224224 & 86.4 & 17.6 & 81.8 & - \\
TNT-B (ours)  & 224224 & 65.6 & 14.1 & 82.8 & 96.3 \\
\bottomrule[1pt]
		\end{tabular}
	}
	\vspace{-0.0em}
\end{table}


We also plot the accuracy-parameters and accuracy-FLOPs line charts in Fig.~\ref{Fig:flops} to have an intuitive comparison of these models. Our TNT models consistently outperform other transformer-based models by a significant margin.


\begin{table}[htp]
	\vspace{-0em}
	\small 
	\centering
	\caption{Effect of position encoding.}\label{tab:position}
	\renewcommand{\arraystretch}{1.0}
	\setlength{\tabcolsep}{6pt}{
		\begin{tabular}{c|c|c|c}
			\toprule[1.5pt]
			Model & Patch position encoding & Pixel position encoding & Top-1 (\%) \\
			\midrule
			\multirow{4}{*}{TNT-S} & \XSolidBrush & \XSolidBrush & 80.5 \\
			& \CheckmarkBold & \XSolidBrush & 80.8 \\
			& \XSolidBrush & \CheckmarkBold & 80.7 \\
			& \CheckmarkBold & \CheckmarkBold & 81.3 \\
			\bottomrule[1pt]
		\end{tabular}
	}
	\vspace{-1.0em}
\end{table}




\subsection{Ablation Studies}

\paragraph{Effect of position encodings.}
Position information is important for image recognition. In TNT structure, patch position encoding is for maintaining global spatial information, and pixel position encoding is used to preserve local relative position. We verify their effect by removing them separately. As shown in Table~\ref{tab:position}, we can see that TNT-S with both patch position encoding and pixel position encoding performs the best by achieving 81.3\% top-1 accuracy. Removing patch/pixel position encoding brings in a 0.6\%/0.5\% accuracy drop respectively, and removing all position encodings heavily decrease the accuracy by 0.8\%.

\begin{figure*}[tp]
	\centering	
	\setlength{\tabcolsep}{4pt}{
		\begin{tabular}{cc}
			\makecell*[c]{\includegraphics[width=0.45\linewidth]{fig/acc-params.pdf}}  & \makecell*[c]{\includegraphics[width=0.45\linewidth]{fig/acc-flops.pdf}} 
			\\
			\small (a) Acc v.s. Params & \small (b) Acc v.s. FLOPs
		\end{tabular}
	}
	\caption{Performance comparison of the representative visual backbone networks on ImageNet.}
	\label{Fig:flops}
	\vspace{-1.0em}
\end{figure*}

\paragraph{Number of \#heads.} The effect of \#heads in standard transformer has been investigated in multiple works~\cite{sixteen-head,Att} and a head width of 64 is recommended for visual tasks~\cite{vit,deit}. We adopt the head width of 64 in outer transformer block in our model. The number of heads in inner transformer block is another hyper-parameter for investigation. We evaluate the effect of \#heads in inner transformer block (Table~\ref{tab:heads}). We can see that a proper number of heads (\emph{e.g.}, 2 or 4) achieve the best performance.

\begin{table}[htp]
	\vspace{-0em}
	\small 
	\centering
	\caption{Effect of \#heads in inner transformer block in TNT-S.}\label{tab:heads}
	\renewcommand{\arraystretch}{1.0}
	\setlength{\tabcolsep}{8pt}{
		\begin{tabular}{c|c|c|c|c}
			\toprule[1.5pt]
			\#heads & 1 & 2 & 4 & 8 \\
			\midrule
			Top-1 & 81.0 & 81.3  & 81.3 & 81.1 \\
			\bottomrule[1pt]
		\end{tabular}
	}
	\vspace{-1.0em}
\end{table}

\begin{wraptable}{r}{0.4\textwidth}
	\vspace{-2em}
	\small 
	\centering
	\caption{Effect of .}\label{tab:p'}
	\renewcommand{\arraystretch}{1.0}
	\setlength{\tabcolsep}{6pt}{
		\begin{tabular}{l|c|c|c}
			\toprule[1.5pt]
			 & Params & FLOPs & Top-1 \\
			\midrule
			2 & 23.8M & 5.1B & 81.0\% \\
			4 & 23.8M & 5.2B & 81.3\% \\
			8 & 25.1M & 6.0B & 81.1\% \\
			\bottomrule[1pt]
		\end{tabular}
	}
	\vspace{-1.0em}
\end{wraptable}
\paragraph{Transformed patch size .}
In TNT, the input image is split into a number of 1616 patches and each patch is further transformed into size  for computational efficiency. Here we test the effect of hyper-parameter  on TNT-S architecture. As shown in Table~\ref{tab:p'}, we can see that the value of  has slight influence on the performance, and we use  by default for its efficiency, unless stated otherwise.


\begin{figure*}[htp]
	\vspace{-0.5em}
	\centering
	\setlength{\tabcolsep}{2pt}{
		\begin{tabular}{cc}
			\makecell*[c]{\includegraphics[width=0.69\linewidth]{fig/vis.jpg}}  & \makecell*[c]{\includegraphics[width=0.29\linewidth]{fig/map-tsne.pdf}}
			\\
			\small (a) Feature maps in Block-1/6/12. & \small (b) T-SNE of Block-12.
		\end{tabular}
	}
	\caption{Visualization of the features of DeiT-S and TNT-S.}
	\label{Fig:vis}
	\vspace{-1.0em}
\end{figure*}

\subsection{Visualization}
We visualize the learned features of DeiT and TNT to further understand the effect of the proposed method. For better visualization, the input image is resized to 10241024. The feature maps are formed by reshaping the patch embeddings according to their spatial positions. The feature maps in the 1-st, 6-th and 12-th blocks are shown in Fig.~\ref{Fig:vis}(a) where 12 feature maps are randomly sampled for these blocks each. In TNT, the local information are better preserved compared to DeiT. We also visualize all the 384 feature maps in the 12-th block using t-SNE~\cite{tsne} (Fig.~\ref{Fig:vis}(b)). We can see that the features of TNT are more diverse and contain richer information than those of DeiT. These benefits owe to the introduction of inner transformer block for modeling local features.



In addition to the patch-level features, we also visualize the pixel embeddings of TNT in Fig.~\ref{Fig:pixel-vis}. For each patch, we reshape the pixel embeddings according to their spatial positions to form the feature maps and then average these feature maps by the channel dimension. The averaged feature maps corresponding to the 1414 patches are shown in Fig.~\ref{Fig:pixel-vis}. We can see that the local information is well preserved in the shallow layers, and the representations become more abstract gradually as the network goes deeper.

\begin{figure}[htp]
	\vspace{-0.5em}
	\centering
	\includegraphics[width=0.85\linewidth]{./fig/pixel-vis.jpg}
	\vspace{-0.5em}
	\caption{Visualization of the averaged pixel embeddings of TNT-S.}
	\label{Fig:pixel-vis}
	\vspace{-1.0em}
\end{figure}






\subsection{Transfer Learning}
To demonstrate the strong generalization ability of TNT, we tranfer TNT-S, TNT-B models trained on ImageNet to other benchmark datasets. More specifically, we evaluate our models on 4 image classification datasets with traning set size ranging from 2,040 to 50,000 images. These datasets include superordinate-level object classification (CIFAR-10~\cite{cifar}, CIFAR-100~\cite{cifar}) and fine-grained object classification (Oxford-IIIT Pets~\cite{pets}, Oxford 102 Flowers~\cite{flower}), shown in Table~\ref{tab:data}. All models are fine-tuned with an image resolution of 384384. Table~\ref{tab:transfer} compares the transfer learing results of TNT to those of VIT, DeiT and other convlutional networks. We find that TNT outperforms DeiT in most datasets with less parameters, which shows the supreority of modeling pixel-level relations to get better feature resprentation.

\paragraph{Fine-tuning Details.}
We adopt the same training settings as those at the pre-training stage by perserving all data augmentation strategies. In order to fine-tune in a different resolution, we also interpolate the position embeddings of new patches. For CIFAR-10 and CIFAR-100, we fine-tune the models for 64 epochs, and for fine-grained datasets, we fine-tune the models for 300 epochs. 

\begin{table}[htp]
	\small 
	\centering
	\caption{Results on downstream tasks with ImageNet pre-training.  denotes fine-tuning with 384384 resolution.}\label{tab:transfer}
	\renewcommand{\arraystretch}{1.0}
	\setlength{\tabcolsep}{6pt}{
		\begin{tabular}{l|c|c|c|c|c|c}
			\toprule[1.5pt]
			Model       & Params (M) & ImageNet & CIFAR10 & CIFAR100 & Flowers & Pets \\
			\midrule
			\multicolumn{6}{l}{\textbf{CNN-based}} \\
			Grafit ResNet-50~\cite{touvron2020grafit} & 25.6 & 79.6 & - & - & 98.2 & - \\
			Grafit RegNetY-8GF~\cite{touvron2020grafit} & 39.2 & - & - & - & 99.1 & - \\
			EfficientNet-B5~\cite{efficientnet} & 30 & 83.6 & 98.7 & 91.1 & 98.5 & - \\
			\midrule
			\multicolumn{6}{l}{\textbf{Transformer-based}} \\
			ViT-B/16 & 86.4 & 77.9 & 98.1 & 87.1 & 89.5 & 93.8 \\
			DeiT-B  & 86.4 & 83.1 & 99.1 & 90.8 & 98.4 & - \\
			TNT-S (ours) & 23.8 & 83.1 & 98.7 & 90.1 & 98.8 & 94.7 \\
			TNT-B (ours) & 65.6 & 83.9 & 99.1 & 91.1 & 99.0 & 95.0 \\
			\bottomrule[1pt]
		\end{tabular}
	}
	\vspace{-1.0em}
\end{table}


\section{Conclusion}
In this paper, we propose a novel Transformer-iN-Transformer (TNT) network architecture for visual recognition. In particular, we uniformly split the image into a sequence of patches and view each patch as a sequence of pixels. We introduce a TNT block in which an outer transformer block is utilized for processing the patch embeddings and an inner transformer block is used to model the relation among pixel embeddings. The information of pixel embeddings is added on the patch embedding after the projection of a linear layer. We build our TNT architecture by stacking the TNT blocks. Compared to the conventional vision transformers (ViT) which corrupts the local structure of the patch, our TNT can better preserve and model the local information for visual recognition. Extensive experiments on ImageNet and downstream tasks have demonstrate the effectiveness of the proposed TNT architecture.

\bibliography{ref}
\bibliographystyle{plain}

\end{document}
