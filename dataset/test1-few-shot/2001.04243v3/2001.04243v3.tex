\documentclass[]{article} \usepackage{geometry}
\geometry{textwidth=400pt,centering}
\usepackage{url}


\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{lmodern}
\usepackage{booktabs}       \usepackage{nicefrac}       \usepackage{microtype}      

\usepackage{setspace}
\usepackage{makecell}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{bm}
\usepackage{comment}
\usepackage{tablefootnote}
\usepackage{multirow}
\usepackage{hhline}\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{algorithm,algorithmic}
\usepackage[font=footnotesize,labelfont=bf]{caption}
\usepackage{url}
\usepackage{authblk}
\usepackage{hyperref}
\usepackage{hhline}
\newcommand{\bc}{\mbox{\textit{c}}}
\newcommand{\bbc}{\mbox{\textit{\large c}}}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{remark}{Remark}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\title{Multi-Complementary and Unlabeled Learning for Arbitrary Losses and Models}

\author[1]{Yuzhou Cao}
\author[1]{Shuqi Liu}
\author[1]{Yitian Xu}
\affil[1]{College of Science, China Agricultural University}


\date{}

\begin{document}
\maketitle
\begin{abstract}
A weakly-supervised learning framework named as \textit{complementary-label learning} has been proposed recently, where each sample is equipped with a single complementary label that denotes one of the classes the sample does \textit{not} belong to. However, the existing complementary-label learning methods cannot learn from the easily accessible unlabeled samples and samples with multiple complementary labels, which are more informative. In this paper, to remove these limitations, we propose the novel \textit{multi-complementary and unlabeled learning} framework that allows unbiased estimation of classification risk from samples with any number of complementary labels and unlabeled samples, for arbitrary loss functions and models. We first give an unbiased estimator of the classification risk from samples with multiple complementary labels, and then further improve the estimator by incorporating unlabeled samples into the risk formulation. The estimation error bounds show that the proposed methods are in the optimal parametric convergence rate. Finally, the experiments on both linear and deep models show the effectiveness of our methods.
\end{abstract}

\section{Introduction}
The ordinary supervised classification problems require that each training sample should be equipped with an exact label that denotes the class the sample belongs to. However, the preparation of massive exactly labeled data is usually laborious and unrealistic in practical. Therefore, a lot of studies on learning from weak supervision have been made to tackle this problem in different scenarios, e.g. semi-supervised learning \cite{DBLP:books/mit/06/CSZ2006,DBLP:conf/icml/SakaiPNS17,DBLP:conf/ijcai/ZhangZ18,SSL1,SSL2}, partial label learning \cite{DBLP:journals/jmlr/CourST11,DBLP:journals/tkde/ZhangYT17}, and positive-unlabeled learning \cite{DBLP:conf/kdd/ElkanN08,DBLP:conf/icml/PlessisNS15,DBLP:journals/pami/SansoneNZ19,DBLP:conf/icml/HsiehNS19}. Recently, another weakly-supervised learning scenario called \textit{complementary-label learning} (CLL) has been proposed. In the CLL setting, each ordinary label is substituted with the complementary label, which denotes one of the classes that a training sample does \textit{not} belong to. It is obvious that the preparation of complementarily labeled data is much more labor-saving than that of ordinarily labeled data.

The complementary-label learning problem has been investigated in previous studies \cite{DBLP:conf/nips/IshidaNHS17,DBLP:conf/eccv/YuLGT18,DBLP:conf/icml/IshidaNMS19}. In these works, different risk estimators were proposed to recover classification risk only from complementarily labeled data under the empirical risk minimization (ERM) framework. In \cite{DBLP:conf/nips/IshidaNHS17} and \cite{DBLP:conf/eccv/YuLGT18}, the proposed risk estimators had restrictions on loss functions and unbiasedness respectively. \cite{DBLP:conf/icml/IshidaNMS19} overcame the shortcomings by giving an unbiased risk estimator without any restriction on models and loss functions while guaranteeing the superior performance in terms of classification accuracy over the previous two methods.

It is noticeable that in these works, each training sample was given only a single complementary label. However, in quite a few cases, the training samples can be multi-complementarily labeled, namely each training sample is equipped with multiple complementary labels. For example, in the stage of data annotation, an annotator who has no idea of a training sample's exact label may be able to recognize multiple classes that the sample does not belong to, which results in a sample with multiple complementary labels. In crowdsourcing scenario \cite{Crowdsourcing,CS}, the quality of crowdsourcing label is especially crucial \cite{DBLP:journals/chinaf/WangZ15a}. Instead of being ordinarily labeled, a sample can be complementarily labeled to alleviate the effect of low-quality noisy crowdsourcing labels. Since a sample can be complementarily labeled by different crowdworkers, each training sample may have more than one complementary label. Moreover, compared with the single-complementary-label setting in previous CLL studies, the samples with multiple complementary labels are more informative. To sum up, a framework for learning from data with arbitrary\footnote{ `Arbitrary' means the samples can be equipped with different numbers of complementary labels.} number of complementary labels is in demand.

Furthermore, the information concealed in the easily accessible unlabeled data proved to be helpful in many other weakly-supervised learning scenario both theoretically and practically \cite{shai,incomplete,partial_unlabeled}. Therefore, it is promising to further enhance the capability of CLL framework by incorporating the unlabeled data.

In this paper, we study the \textit{multi-complementary label and unlabeled learning} (MCUL) problem, where both multi-complementarily labeled data and unlabeled data are leveraged to obtain better classifiers. In our method, we propose a novel unbiased risk estimator for MCUL problem with no limitation on loss functions and models. By using a mild assumption, we first derive the risk estimator for \textit{multi-complementary label learning} (MCL) problem. Then we further utilize the unlabeled data to construct the risk estimator for MCUL problem. With no more assumption on loss functions and models, we show that the estimation error bounds of MCL and MCUL are in optimal parametric convergence rate \cite{vapnik}. The effectiveness of the proposed MCUL is demonstrated through experiments on both linear and deep models.

The main contributions are summarized as follows:
\begin{itemize}
\item We propose the novel MCL framework that allows unbiased estimation of the classification risk only from samples with arbitrary number of complementary labels and can be applied on \textit{arbitrary losses and models}.

\item We further propose the MCUL framework to utilize the unlabeled samples, which are neglected in previous studies on CLL\cite{DBLP:conf/nips/IshidaNHS17,DBLP:conf/eccv/YuLGT18,DBLP:conf/icml/IshidaNMS19} and validate the benefits of the incorporation of unlabeled samples both experimentally and theoretically.

\item The previous CLL framework and ordinary classification problems are proven to be special cases of the MCUL framework, which  shows the comprehensiveness of the MCUL framework as a weakly-supervised leaning framework.
\end{itemize}

This rest of this paper is organized as follows. We give the review of complementary-label learning in Section 2. The MCL and MCUL frameworks are proposed in Section 3. Moreover, we analyse the estimation error bounds of the proposed methods in Section 4 and discuss the helpfulness of integrating the class-prior information in Section 5. Finally, we give the experimental results of our frameworks on both linear and deep models in Section 6 and conclude the paper in Section 7. The detailed proof is shown in the appendix.
\section{Review of Complementary-Label Learning}
To begin with, we first show the classification risk of learning from ordinary labels and then review how the previous risk estimators of learning from complementarily labeled samples recover the classification risk under the ERM framework.
\subsection{Ordinary Classification Problem}
Let's denote the feature space with  and  is the label space. The training samples are drawn independently and identically from the unknown distribution , which is the joint distribution over  with density . Then the critical work is to find a decision function  that minimizes the classification risk with loss function :


Since the density  is unknown, the classification risk is approximated by the empirical risk:

\subsection{Complementary-Label Learning}
In the CLL setting, each sample is equipped with a complementary label. The complementarily labeled data  are sampled independently and identically from a joint distribution with density .

In \cite{DBLP:conf/nips/IshidaNHS17}, an assumption on density  was made:

Under this assumption, \cite{DBLP:conf/nips/IshidaNHS17} proved that classification risk (\ref{E1}) can be recovered by an unbiased estimator only from complementarily labeled data. However, the loss functions are restricted to one-versus-all and pairwise comparison multi-class loss functions \cite{TZhang}. Moreover, the binary loss functions  used in the two multi-class loss functions are required to fulfill symmetric condition: . Obviously, the popular softmax cross-entropy loss and all the other convex loss functions do not meet these conditions. Since the softmax cross-entropy loss is widely used in deep learning, this requirement will be a serious limitation for the application of state-of-the-art deep models.

To make deep models available, \cite{DBLP:conf/eccv/YuLGT18} proposed another risk estimator limited to softmax cross-entropy loss. Though the risk estimator is not necessarily unbiased, the method is ensured to identify the optimal classifier that minimizes classification risk (\ref{E1}) by minimizing its learning object. The method also introduces bias into the choice of complementary labels. However, in the stages of bias estimation, ordinarily labeled data are required. The severe requirement might not align with the motivation of complementary-label learning.

The limitations above were removed in \cite{DBLP:conf/icml/IshidaNMS19}. An unbiased risk estimator with only complementarily labeled data was deduced by taking a different approach than \cite{DBLP:conf/nips/IshidaNHS17}. With the same assumption (\ref{E2}) adopted, the risk formulation is valid for arbitrary losses and models. Experiments on both linear and deep models showed the superiority of the estimator in \cite{DBLP:conf/icml/IshidaNMS19} than those in previous works \cite{DBLP:conf/nips/IshidaNHS17,DBLP:conf/eccv/YuLGT18}. Nevertheless, the estimator is still confined within single-complementary-label setting, where each sample is given merely one complementary label. The unlabeled data are also neglected in previous CLL studies, which prevents the CLL from being a more general framework.

\begin{figure*}[t]
    \centerline{\includegraphics[width = 20cm, height = 7.5cm,trim=10 140 160 80,clip]{comp.pdf}}
    \caption{The demonstration of previous CCL setting and the proposed MCL\&MCUL settings.}
    \label{Comp}
\end{figure*}

The MCUL framework proposed in this paper further enables learning from both multi-complementarily labeled samples and unlabeled samples. Figure \ref{Comp} describes the differences between the previous CLL setting and the proposed MCUL setting.
\section{Proposed Frameworks}
In this section, we propose our framework to enable unbiased estimation of classification risk from both multi-complementarily labeled data and unlabeled data.

We first prove that the classification risk can be recovered from multi-complementarily labeled data under a mild assumption by employing the risk rewrite technique \cite{UU}. Then we further present the risk formulation of MCUL and show the estimation error bounds of the two methods.

{\em \textbf{Notations and Settings:}} Denote by  the complementary label space.  is the collection of all the possible combinations of  different complementary labels, e.g. .  is referred to as complementary-label set in the following sections. Suppose the training samples are sampled as follows:

where  is the marginal density and  is the density on .  are the unlabeled data and  are the multi-complementarily labeled data with complementary-label sets of size . The size of complementary-label set  is denoted by  and .
\subsection{Multi-Complementary Label Learning (MCL)}
In this section, we give an account of the risk minimization framework of multi-complementary label learning.

As in the previous works, we first make assumptions on the relation between density  and .


The assumption implies each combination of  complementary labels are selected uniformly, which is a mild generalized assumption of those in the previous works \cite{DBLP:conf/nips/IshidaNHS17,DBLP:conf/eccv/YuLGT18,DBLP:conf/icml/IshidaNMS19}. Under this assumption, we prove that the \textit{multi-complementary loss} allows unbiased estimation of classification risk (\ref{E1}) from samples with complementary-label sets:
\begin{lemma}
\label{T1}Suppose the density  and  follow the assumption (\ref{E3}). For \textbf{any} loss function  and decision function , the classification risk (\ref{E1}) is equal to the risk formulation below:

where  is the multi-complementary loss:

\end{lemma}
The proof can be found in the Appendix \ref{AA}. For ease of notation, we the following notation for cumulative loss :

Due to the notation, we can further rewrite the multi-complementary loss into the form below:

Notice that the cumulative loss  is obtained by summing up the loss of the prediction  \textit{w.r.t.} all the potential labels , so it only relies on the sample  and the classifier . As a result, the label information used in the calculation of multi-complementary loss (\ref{E5}) is only complementary-label set . Therefore, the multi-complementary label learning setting totally gets rid of the dependence on true labels.

The risk formulation in Lemma \ref{T1} shows that the classification risk can be recovered only from samples with complementary-label sets of fixed size . However, the complementary-label sets of samples are \textit{not} necessarily limited to a certain size in reality. To completely remove the limitation on the size of complementary-label set, we consider the convex combination of  called \textit{multi-complementary risk}.
\begin{definition}\label{D1}(Multi-Complementary Risk)~For any decision function , its MCL risk is defined as:

where  is any vector in .
\end{definition}
\begin{theorem}
The MCL risk is equal to classification risk (\ref{E1}):

\end{theorem}
\begin{proof}
Due to Lemma \ref{T1}, we can get . Then the following equations holds:
\end{proof}
The empirical MCL risk is as below:

Then the following work is to find the minimizer  of empirical MCL risk:

where  is a real function class and  is a -dimensional function class.

In (\ref{unbiased}), all the samples are taken into consideration regardless of the size of their complementary-label sets. Since there is no restriction on loss function  and classifier , any loss and model is available for the multi-complementary learning framework.

\begin{remark}{\rm
There are some special cases in the multi-complementary label learning setting. If , the proposed estimator will reduce to the estimator in single-complementary-label setting \cite{DBLP:conf/icml/IshidaNMS19}. If , the proposed estimator will be the same with that in ordinary classification problem (\ref{ordinary}). According to the special cases, the proposed MCL proved to be a comprehensive weakly-supervised learning framework.}
\end{remark}
\subsection{Multi-Complementary and Unlabeled Learning (MCUL)}
To utilize both multi-complementarily labeled data and unlabeled data, we further rewrite the risk formulation and propose the MCUL framework. Based on Lemma \ref{T1}, we can incorporate the unlabeled data to construct an unbiased estimator of classification risk (\ref{E1}):
\begin{lemma}
\label{T2}
The classification risk (\ref{E1}) is equal to the risk formulation below:

where  is the trade-off coefficient.
\end{lemma}
\begin{proof}
The cumulative loss  is independent of , and thus:

According to the equation above and Lemma \ref{T1}, we can obtain:
\end{proof}
In the same manner as in the derivation of (\ref{tot1}), we can derive the \textit{multi-complementary and unlabeled risk}:
\begin{definition}(Multi-Complementary\&Unlabeled Risk)~For any decision function , its MCUL risk is defined as:
\label{D2}

where  is any vector in .
\end{definition}
When the trade-off coefficient  is set to 0, the MCUL risk is the same with MCL risk (\ref{tot1}). The following Theorem allows unbiased estimation with both unlabeled data and multi-complementarily labeled data.
\begin{theorem}
The MCUL risk is equal to classification risk (\ref{E1}):
    
\end{theorem}
The Theorem can be proven in the same way as in Theorem \ref{tot1}.
We can approximate the MCUL risk by the empirical MCUL risk below:

Notice that the unlabeled data are used for construct the estimator of cumulative loss since the calculation of  does not need any label information. Theorem \ref{T2} shows that this incorporation can still yields an unbiased estimator of classification risk (\ref{E1}).

Then the following work is to find the minimizer  of empirical MCUL risk:


Compared with the empirical MCL risk (\ref{unbiased}), the empirical MCUL risk (\ref{MCUL}) further incorporates the unlabeled data into the risk formulation. With the incorporation of easily accessible unlabeled data, the estimation error bound will be tighter, which indicates a better decision function . The claim is further validated in the following sections.
\section{Estimation Error Bounds of MCL and MCUL}
In this section, we give the estimation error bounds of the proposed MCL and MCUL frameworks.

Suppose the non-negative loss function  does not exceed  on feature space  and let  be the Lipschitz constant of .  is the Rademacher complexity \cite{foundation} of function class  with sample size of  from  and we suppose it decays in the rate of . We have the following estimation error bounds, which show the convergence of  and  to the optimal decision function :
\begin{theorem}(Estimation error bound of MCL) For any , with probability at least :
\label{MCL_bound}

\end{theorem}
\begin{theorem}(Estimation error bound of MCUL) For any , with probability at least :
\label{MCUL_bound}

\end{theorem}
The proof of the theorems above can be found in the Appendix \ref{AB}.
\begin{remark}{\rm From Theorems \ref{MCL_bound} and \ref{MCUL_bound}, we can learn the estimation error bounds of the proposed methods are in the optimal convergence rate without any additional assumption \cite{vapnik}. Moreover, with increasing number of unlabeled data, the error bound of MCUL will get tighter, which implies the helpfulness of utilizing unlabeled data.}\end{remark}
\section{Integration of Class-Prior Information}
In the previous sections, a sample with  complementary labels is considered to be sampled from the distribution with density , which is independent from the class-prior probability . In practical situations, however, the class-prior may be accessible. For example, in \cite{CR}, the class-prior can be estimated from the given data; the prior is simply approximated by the relative frequency in \cite{DBLP:conf/icml/IshidaNMS19}. \cite{PUPN} proves the helpfulness of integrating the class-prior into learning algorithm. As can be seen, it is promising to further enhance the capability of proposed MCL and MCUL framework by utilizing the class-prior information.

Notice that  is the conditional density  in essence. Then the following equation holds:

where  and .
Due to the equation (\ref{cond}), we can integrate the class-prior information into the risk formulations of MCL and MCUL as follows:
\begin{theorem}(MCL risk and MCUL risk)
\label{CPI}

where  and  is a trade-off parameter. (\ref{tot2}) and (\ref{tot3}) are called MCL risk and MCUL risk respectively.
\end{theorem}
The proof can be found in the Appendix \ref{AC}.

From the Theorem \ref{CPI}, the coefficient  in (\ref{tot1}) and (\ref{MCUL}) is substituted by the class-prior . Compared with (\ref{tot1}) that converges in the rate of , MCL risk (\ref{tot2}) converges in , which often indicates a faster convergence rate. We will experimentally evaluate the helpfulness of integrating class-prior information in the next section.
\section{Experiments}




In this section, we experimentally evaluate the proposed methods on nine benchmark datasets including: PENDIGITS, LETTER, SATIMAGE, USPS, MNIST \cite{MNIST}, Fashion-MNIST \cite{FMNIST}, Kuzushi-MNIST \cite{kmnist}, EMNIST-balanced \cite{EMNIST} and SVHN \cite{SVHN}. The first three datasets can be downloaded from the \textit{UCI machine learning repository} and all the other datasets are available on public websites. We compare three complementary-label learning baseline methods: Pairwise Comparison(\textit{PC}) with sigmoid loss from \cite{DBLP:conf/nips/IshidaNHS17}, Forward Correction(\textit{Fwd}) from \cite{DBLP:conf/eccv/YuLGT18} and Gradient Ascent(\textit{GA}) from \cite{DBLP:conf/icml/IshidaNMS19}.

The details of the datasets are shown in the following sections. The implementation is based on Pytorch.
\subsection{Experimental Setup}
In the experiments, the empirical risk minimization of MCL and MCUL is conducted by minimizing the risk formulation (\ref{unbiased}) and (\ref{EMMCUL}) \textit{w.r.t.} softmax cross-entropy loss. \textit{GA}, and \textit{Fwd} follow the setting above and \textit{PC} is trained with pairwise-comparison loss. \textit{Adam} \cite{Adam} is applied for optimization. All the datasets are split into training/testing sets with a 9:1 ratio and the training sets are further divided into training/validation sets with the same ratio.

To ensure that the assumption (\ref{E3}) is satisfied, each complementary-label set of size \textit{c} is generated by randomly choosing \textit{c} labels from the candidate labels other than the true label. Though a sample with \textit{c} complementary labels can  be simply split into \textit{c} samples with one complementary label each, it's obvious that the \textit{c} samples are not independent of each other. Therefore this approach will lead to serious violation of the fundamental \textit{i.i.d.} assumption. For fair comparison in these experiments, the complementary labels are generated in the same way as in \cite{DBLP:conf/icml/IshidaNMS19}.

For PENDIGITS, LETTER, SATIMAGE, USPS, and MNIST, a linear-in-input model with a bias term is used. For MNIST, the learning rate is fixed to 1e-4; weight decay 1e-4; maximum iterations 60000; and batch size is set to 100. For the rest datasets, the learning rate is selected from 1e-1, 1e-2, 1e-3, 1e-4 and the number of maximum iterations is changed to 5000.

For Fashion-MNIST, Kuzushi-MNIST and EMNIST-balanced, a MLP model(\textit{d}-500-\textit{K}) is trained for 300 epochs. The learning rate and weight decay are fixed to 1e-4 and the batch size is 256. For SVHN, Resnet-18 \cite{resnet} is deployed and trained for 120 epochs. The learning rate is selected from 1e-2, 1e-3, 1e-4 and weight decay is fixed to 5e-4. To alleviate overfitting by forcing the non-negativity of loss functions \cite{nn}, in the experiments on flexible models, MCL and MCUL losses are replaced by their absolute values.

Experiments on datasets with or without unlabeled samples are both conducted. When the unlabeled samples are incorporated, we randomly set 99\% of training samples to be unlabeled for datasets with less than 50000 samples, which is a common setting in previous studies of weakly-supervised learning \cite{DBLP:conf/icml/SakaiPNS17,nn}. The fraction is further increased to 99.5\% for datasets with more than 50000 samples.

In respect of parameter setting, the setting of baseline methods follow the previous work \cite{DBLP:conf/icml/IshidaNMS19}. For MCUL, we use . The parameter  is set according to the equations below:


In our methods, we make no assumption on the distribution of the size of complementary-label sets. To generate the multi-complementarily labeled samples as close to the reality, we suppose that samples with too few or too many complementary labels are less likely to appear. Then the  follows the equation below:
 In the experiments,  is used.
\subsection{Experiments on Linear Model and Flexible Models}
The experimental results of linear and flexible models are summarized in Table \ref{TB1} and Table \ref{TB22} respectively. The experimental results under the presence of unlabeled samples are shown in the second row corresponding to each dataset.

\begin{table*}[htbp]
\caption{\footnotesize Test mean and standard deviation of the classification accuracy of linear model for 10 trials. The best one is emphasized in bold. \#n, \#f and \#c denote the number of samples, features and classes of each dataset. The results of experiments under the presence of unlabeled samples are shown in the second row corresponding to each dataset.}
\footnotesize
\centering
\begin{tabular}{ccccccccc}\hline
\label{TB1}
Datasets&\#n&\#f&\#c&\textit{PC}&\textit{Fwd}&\textit{GA}&\textit{MCL}&\textit{MCUL}\\\hline
\rule{0pt}{10pt}
\multirow{2}*{PENDIGITS}&\multirow{2}*{10092}&\multirow{2}*{16}&\multirow{2}*{10}&62.985.09&77.912.83&15.016.71&\textbf{84.321.09}&--~~~~--\\&&&&5.984.24&8.091.71&11.055.05&11.445.68&\textbf{28.038.10}\\\rule{0pt}{15pt}
\multirow{2}*{LETTER}&\multirow{2}*{20000}&\multirow{2}*{16}&\multirow{2}*{26}&9.172.44&9.372.52&4.680.81&\textbf{45.752.31}&--~~~~--\\&&&&4.662.78&3.990.88&5.122.01&5.330.76&\textbf{8.411.86}\\\rule{0pt}{15pt}
\multirow{2}*{SATIMAGE}&\multirow{2}*{6435}&\multirow{2}*{36}&\multirow{2}*{6}&74.054.65&77.455.43&38.321.82&\textbf{82.101.82}&--~~~~--\\&&&&15.926.67&17.7710.98&17.6812.08&21.137.57&\textbf{51.357.94}\\\rule{0pt}{15pt}
\multirow{2}*{USPS}&\multirow{2}*{9298}&\multirow{2}*{256}&\multirow{2}*{10}&41.755.45&46.1513.10&14.196.07&\textbf{82.312.32}&--~~~~--\\&&&&9.664.72&11.567.23&8.573.38&9.666.62&\textbf{26.203.98}\\\rule{0pt}{15pt}
\multirow{2}*{MNIST}&\multirow{2}*{70000}&\multirow{2}*{784}&\multirow{2}*{10}&51.214.87&52.174.88&67.801.89&\textbf{77.361.27}&--~~~~--\\&&&&13.784.16&13.582.11&14.861.77&25.863.49&\textbf{38.215.24}\\\hline
\end{tabular}
\end{table*}



\begin{table*}[htbp]
\footnotesize
\caption{\footnotesize Test mean and standard deviation of the classification accuracy of flexible models for 4 trials. The best one is emphasized in bold. \#n, \#f and \#c denote the number of samples, features and classes of each dataset. The results of experiments under the presence of unlabeled samples are shown in the second row corresponding to each dataset.}
\centering
\begin{tabular}{ccccccccc}\hline
\label{TB22}
Datasets&\#n&\#f&\#c&\textit{PC}&\textit{Fwd}&\textit{GA}&\textit{MCL}&\textit{MCUL}\\\hline
\rule{0pt}{10pt}
\multirow{2}*{Fashion-MNIST}&\multirow{2}*{70000}&\multirow{2}*{784}&\multirow{2}*{10}&77.340.88&83.490.18&81.730.25&\textbf{84.970.09}&--~~~~--\\&&&&25.754.39&23.363.05&22.343.18&49.814.42&\textbf{56.931.66}\\\rule{0pt}{15pt}
\multirow{2}*{Kuzushi-MNIST}&\multirow{2}*{70000}&\multirow{2}*{784}&\multirow{2}*{10}&59.311.07&66.460.17&70.680.88&\textbf{79.250.28}&--~~~~--\\&&&&16.354.92&12.942.72&15.132.42&23.421.15&\textbf{31.041.89}\\\rule{0pt}{15pt}
\multirow{2}*{EMNIST-balanced}&\multirow{2}*{131600}&\multirow{2}*{784}&\multirow{2}*{47}&14.281.18&18.212.93&4.250.71&\textbf{65.410.22}&--~~~~--\\&&&&2.360.36&2.680.08&2.540.36&4.141.13&\textbf{6.770.51}\\\rule{0pt}{15pt}
\multirow{2}*{SVHN}&\multirow{2}*{99289}&\multirow{2}*{1024}&\multirow{2}*{10}&20.742.61&76.272.07&6.870.41&\textbf{83.270.55}&--~~~~--\\&&&&14.771.27&12.950.75&17.561.54&19.050.44&\textbf{19.470.17}\\\hline\end{tabular}
\end{table*}

\begin{figure*}[htbp]
\subfigure[FMNIST]{\includegraphics[keepaspectratio,width=8.5cm]{Fmnist.pdf}}
\subfigure[KMNIST]{\includegraphics[keepaspectratio,width=8.5cm]{Kmnist.pdf}}\\
\subfigure[EMNIST-balanced]{\includegraphics[keepaspectratio,width=8.5cm]{Emnist.pdf}}
\subfigure[SVHN]{\includegraphics[keepaspectratio,width=8.5cm]{SVHN.pdf}}
\caption{The training curves of baseline methods and MCL on flexible models.}

\label{F1}
\end{figure*}
{\em Results of MCL:} First we compare the proposed MCL framework with the three baseline methods. From the experimental results, we can see that MCL framework outperforms the baseline methods on all the datasets regardless of which model is applied. The superiority of MCL is especially apparent when the datasets have a large number of classes. Due to the experimental results on LETTER and EMNIST-balanced, it can be seen that the baseline methods can hardly generate effective classifiers. Furthermore, the training curves in Figure \ref{F1} show that in most cases, MCL can generate better classifiers and converge in a faster rate. Compared with the baseline methods, MCL remains valid owing to the capability of utilizing multi-complementarily labeled samples.

{\em Results of MCUL:} From Table \ref{T1}, as can be seen, under the presence of a great percentage of unlabeled samples, the baseline methods suffer from the lack of complementarily labeled samples, while MCUL can still enhance its performance by incorporating unlabeled samples. Moreover, in the cases that only a small number of complementarily labeled samples are available, the performance of baseline methods \textit{GA} is seriously degraded due to the imprecise estimation of class prior, which is one of the reasons that the performance of \textit{GA} is relatively weak with a small fraction of complementarily labeled samples.
\subsection{Experiments with Accessible Class-Prior Probability}
In this section, we further show the benefits of integrating the class-prior information. We compare the performance of MCL and MCUL with MCL and MCUL on linear model. The experimental results are summarized in Table \ref{TB2} and the setup is consistent with that in the previous experiments.

From the results, we can see that by integrating the class-prior probability, MCL outperforms MCL on most datasets and MCUL performs better than MCUL on all the datasets, which shows the helpfulness of utilizing the class-prior information when it is accessible. Furthermore, under the presence of a great percentage of unlabeled samples, the integration of class-prior information can always boost the performance of models. A rational explanation is that when supervision information is inadequate, the benefits of incorporating class-prior information is more conspicuous.

\begin{table}[tbph]
\caption{\footnotesize Test mean and standard deviation of the classification accuracy of linear model for 10 trials. The best one is emphasized in bold. The results of experiments under the presence of unlabeled samples are shown in the second row corresponding to each dataset.}
\footnotesize
\centering
\begin{tabular}{ccccc}\hline
\label{TB2}
Datasets&\textit{MCL}&\textit{MCUL}&\textit{MCL}&\textit{MCUL}\\\hline\rule{0pt}{10pt}
\multirow{2}*{PENDIGITS}&\textbf{84.321.09}&--~~~~--&84.270.26&--~~~~--\\&11.445.68&28.038.10&13.822.39&\textbf{38.475.68}\\\rule{0pt}{15pt}
\multirow{2}*{LETTER}&45.752.31&--~~~~--&\textbf{53.295.31}&--~~~~--\\&5.330.76&8.411.86&17.321.84&\textbf{18.540.92}\\\rule{0pt}{15pt}
\multirow{2}*{SATIMAGE}&82.101.82&--~~~~--&\textbf{83.531.19}&--~~~~--\\&21.137.57&51.357.94&39.619.57&\textbf{54.236.14}\\\rule{0pt}{15pt}
\multirow{2}*{USPS}&\textbf{82.312.32}&--~~~~--&81.071.58&--~~~~--\\&9.666.62&26.203.98&24.294.72&\textbf{39.582.99}\\\rule{0pt}{15pt}
\multirow{2}*{MNIST}&77.361.27&--~~~~--&\textbf{81.472.46}&--~~~~--\\&25.863.49&38.215.24&27.542.71&\textbf{41.585.27}\\\hline
\end{tabular}
\end{table}

\section{Conclusion}
We first derive the MCL framework to learn from samples with any number of complementary labels for arbitrary losses and models. Then we incorporate unlabeled data into the risk formulation and propose the MCUL framework to enhance the performance of MCL by learning from multi-complementarily labeled data and unlabeled data simultaneously. We further show that the estimation error bounds of the proposed methods are in the optimal parametric convergence rate. Finally, we conduct experiments and show our methods outperform the current state-of-the-art methods on both linear and deep model. A promising direction is applying our methods on crowdsourcing and other weakly-supervised classification scenarios, which is our future work.
\section*{Acknowledgement}
This work was supported in part by the National Natural Science Foundation of China under Grant 11671010, and Beijing Natural Science Foundation under Grant 4172035.

\begin{thebibliography}{10}

\bibitem{DBLP:books/mit/06/CSZ2006}
Olivier Chapelle, Bernhard Sch{\"{o}}lkopf, and Alexander Zien, editors.
\newblock {\em Semi-Supervised Learning}.
\newblock The {MIT} Press, 2006.

\bibitem{DBLP:conf/icml/SakaiPNS17}
Tomoya Sakai, Marthinus~Christoffel du~Plessis, Gang Niu, and Masashi Sugiyama.
\newblock Semi-supervised classification based on classification from positive
  and unlabeled data.
\newblock In {\em {ICML}}, volume~70 of {\em Proceedings of Machine Learning
  Research}, pages 2998--3006. {PMLR}, 2017.

\bibitem{DBLP:conf/ijcai/ZhangZ18}
Teng Zhang and Zhi{-}Hua Zhou.
\newblock Semi-supervised optimal margin distribution machines.
\newblock In {\em {IJCAI}}, pages 3104--3110. ijcai.org, 2018.

\bibitem{SSL1}
Joris Tavernier, Jaak Simm, Karl Meerbergen, J{\"{o}}rg~Kurt Wegner, Hugo
  Ceulemans, and Yves Moreau.
\newblock Fast semi-supervised discriminant analysis for binary classification
  of large data sets.
\newblock {\em Pattern Recognit.}, 91:86--99, 2019.

\bibitem{SSL2}
Jonathan Gordon and Jos{\'{e}}~Miguel Hern{\'{a}}ndez{-}Lobato.
\newblock Combining deep generative and discriminative models for bayesian
  semi-supervised learning.
\newblock {\em Pattern Recognit.}, 100:107156, 2020.

\bibitem{DBLP:journals/jmlr/CourST11}
Timoth{\'{e}}e Cour, Benjamin Sapp, and Ben Taskar.
\newblock Learning from partial labels.
\newblock {\em J. Mach. Learn. Res.}, 12:1501--1536, 2011.

\bibitem{DBLP:journals/tkde/ZhangYT17}
Min{-}Ling Zhang, Fei Yu, and Cai{-}Zhi Tang.
\newblock Disambiguation-free partial label learning.
\newblock {\em {IEEE} Trans. Knowl. Data Eng.}, 29(10):2155--2167, 2017.

\bibitem{DBLP:conf/kdd/ElkanN08}
Charles Elkan and Keith Noto.
\newblock Learning classifiers from only positive and unlabeled data.
\newblock In {\em {KDD}}, pages 213--220. {ACM}, 2008.

\bibitem{DBLP:conf/icml/PlessisNS15}
Marthinus~Christoffel du~Plessis, Gang Niu, and Masashi Sugiyama.
\newblock Convex formulation for learning from positive and unlabeled data.
\newblock In {\em {ICML}}, volume~37 of {\em {JMLR} Workshop and Conference
  Proceedings}, pages 1386--1394. JMLR.org, 2015.

\bibitem{DBLP:journals/pami/SansoneNZ19}
Emanuele Sansone, Francesco G. B.~De Natale, and Zhi{-}Hua Zhou.
\newblock Efficient training for positive unlabeled learning.
\newblock {\em {IEEE} Trans. Pattern Anal. Mach. Intell.}, 41(11):2584--2598,
  2019.

\bibitem{DBLP:conf/icml/HsiehNS19}
Yu{-}Guan Hsieh, Gang Niu, and Masashi Sugiyama.
\newblock Classification from positive, unlabeled and biased negative data.
\newblock In {\em {ICML}}, volume~97 of {\em Proceedings of Machine Learning
  Research}, pages 2820--2829. {PMLR}, 2019.

\bibitem{DBLP:conf/nips/IshidaNHS17}
Takashi Ishida, Gang Niu, Weihua Hu, and Masashi Sugiyama.
\newblock Learning from complementary labels.
\newblock In {\em {NIPS}}, pages 5639--5649, 2017.

\bibitem{DBLP:conf/eccv/YuLGT18}
Xiyu Yu, Tongliang Liu, Mingming Gong, and Dacheng Tao.
\newblock Learning with biased complementary labels.
\newblock In {\em {ECCV} {(1)}}, volume 11205 of {\em Lecture Notes in Computer
  Science}, pages 69--85. Springer, 2018.

\bibitem{DBLP:conf/icml/IshidaNMS19}
Takashi Ishida, Gang Niu, Aditya~Krishna Menon, and Masashi Sugiyama.
\newblock Complementary-label learning for arbitrary losses and models.
\newblock In {\em {ICML}}, volume~97 of {\em Proceedings of Machine Learning
  Research}, pages 2971--2980. {PMLR}, 2019.

\bibitem{Crowdsourcing}
J.~Howe.
\newblock {\em Crowdsourcing: Why the Power of the Crowd Is Driving the Future
  of Business}.
\newblock Crwon Publishing Group, 2009.

\bibitem{CS}
Alba Pag{\`{e}}s{-}Zamora, Margarita Cabrera{-}Bean, and Carles Diaz{-}Vilor.
\newblock Unsupervised online clustering and detection algorithms using
  crowdsourced data for malaria diagnosis.
\newblock {\em Pattern Recognit.}, 86:209--223, 2019.

\bibitem{DBLP:journals/chinaf/WangZ15a}
Wei Wang and Zhi{-}Hua Zhou.
\newblock Crowdsourcing label quality: a theoretical analysis.
\newblock {\em {SCIENCE} {CHINA} Information Sciences}, 58(11):1--12, 2015.

\bibitem{shai}
Christina G{\"{o}}pfert, Shai Ben{-}David, Olivier Bousquet, Sylvain Gelly,
  Ilya~O. Tolstikhin, and Ruth Urner.
\newblock When can unlabeled data improve the learning rate?
\newblock In {\em {COLT}}, volume~99 of {\em Proceedings of Machine Learning
  Research}, pages 1500--1518. {PMLR}, 2019.

\bibitem{incomplete}
Zhen{-}yu Zhang, Peng Zhao, Yuan Jiang, and Zhi{-}Hua Zhou.
\newblock Learning from incomplete and inaccurate supervision.
\newblock In {\em {KDD}}, pages 1017--1025. {ACM}, 2019.

\bibitem{partial_unlabeled}
Qian{-}Wei Wang, Yu{-}Feng Li, and Zhi{-}Hua Zhou.
\newblock Partial label learning with unlabeled data.
\newblock In {\em {IJCAI}}, 2019.

\bibitem{vapnik}
Vladimir Vapnik.
\newblock An overview of statistical learning theory.
\newblock {\em {IEEE} Trans. Neural Networks}, 10(5):988--999, 1999.

\bibitem{TZhang}
Tong Zhang.
\newblock Statistical analysis of some multi-category large margin
  classification methods.
\newblock {\em J. Mach. Learn. Res.}, 5:1225--1251, 2004.

\bibitem{UU}
Nan Lu, Gang Niu, Aditya~Krishna Menon, and Masashi Sugiyama.
\newblock On the minimal supervision for training any binary classifier from
  only unlabeled data.
\newblock In {\em 7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}, 2019.

\bibitem{foundation}
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar.
\newblock {\em Foundations of Machine Learning}.
\newblock Adaptive computation and machine learning. {MIT} Press, 2012.

\bibitem{CR}
Marthinus~Christoffel du~Plessis, Gang Niu, and Masashi Sugiyama.
\newblock Class-prior estimation for learning from positive and unlabeled data.
\newblock {\em Mach. Learn.}, 106(4):463--492, 2017.

\bibitem{PUPN}
Gang Niu, Marthinus~Christoffel du~Plessis, Tomoya Sakai, Yao Ma, and Masashi
  Sugiyama.
\newblock Theoretical comparisons of positive-unlabeled learning against
  positive-negative learning.
\newblock In {\em Advances in Neural Information Processing Systems 29: Annual
  Conference on Neural Information Processing Systems 2016, December 5-10,
  2016, Barcelona, Spain}, pages 1199--1207, 2016.

\bibitem{MNIST}
Y.~{Lecun}, L.~{Bottou}, Y.~{Bengio}, and P.~{Haffner}.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, Nov 1998.

\bibitem{FMNIST}
Han Xiao, Kashif Rasul, and Roland Vollgraf.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock {\em CoRR}, abs/1708.07747, 2017.

\bibitem{kmnist}
Tarin Clanuwat, Mikel Bober{-}Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki
  Yamamoto, and David Ha.
\newblock Deep learning for classical japanese literature.
\newblock {\em CoRR}, abs/1812.01718, 2018.

\bibitem{EMNIST}
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andr{\'{e}} van Schaik.
\newblock {EMNIST:} extending {MNIST} to handwritten letters.
\newblock In {\em {IJCNN}}, 2017.

\bibitem{SVHN}
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo~Wu, and Andrew Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock {\em NIPS}, 2011.

\bibitem{Adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In {\em 3rd International Conference on Learning Representations,
  {ICLR} 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track
  Proceedings}, 2015.

\bibitem{resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em 2016 {IEEE} Conference on Computer Vision and Pattern
  Recognition, {CVPR} 2016, Las Vegas, NV, USA, June 27-30, 2016}, pages
  770--778, 2016.

\bibitem{nn}
Ryuichi Kiryo, Gang Niu, Marthinus~Christoffel du~Plessis, and Masashi
  Sugiyama.
\newblock Positive-unlabeled learning with non-negative risk estimator.
\newblock In {\em {NIPS}}, pages 1675--1685, 2017.

\bibitem{tala}
Michel Ledoux and Michel Talagrand.
\newblock {\em Probability in Banach Spaces: isoperimetry and processes}.
\newblock Springer Science \& Business Media, 2013.

\bibitem{Mc}
Colin McDiarmid.
\newblock On the method of bounded differences.
\newblock {\em Surveys in combinatorics}, 141(1):148--188, 1989.

\end{thebibliography}


\newpage
\section*{Appendix}
\subsection*{A. Proof of Lemma \ref{T1}}
\label{AA}

\begin{proof}
Suppose , then we can obtain . Denote  with . Due to assumption (\ref{E3}), the equations below hold:

We can get the following equation by substituting the second equation above into the first equation:

Then we can rewrite the classification risk (\ref{E1}) due to the equation above:

By exchanging the order of summation, we can get an equal version of the last equation above and finally conclude the proof:

\end{proof}



\subsection*{B. Proof of Theorem \ref{MCUL_bound}}
\label{AB}
The proof of Theorem \ref{MCL_bound} is omitted since it is the special case of Theorem \ref{MCUL_bound} by setting  to 0. 

First we introduce the Talagrand's contraction lemma \cite{tala}:
\begin{lemma}
\label{L7}
Let  be a class of real functions and  be a K-dimensional function class and  a Lipschitz function with constant  and  . Then .
\end{lemma}



To apply Talagrand's contraction lemma, we use the shifted loss  instead of . Then we abbreviate some complex terms in these forms:



We give the following conclusions:
\begin{lemma} Denote  with :

\end{lemma}
\begin{proof}
The first inequality can be deduced from Lemma \ref{L7} directly. By definition and the sub-additivity of supremum: 

The  is a fixed loss function and the first equation holds. Since  is independent of , we can get:

Let  be the indicator function and . Then we have the conclusion below:

Then the inequalities hold:

\end{proof}


We can bound  and  using Mcdiarmid's inequality \cite{Mc}:
\begin{lemma}\label{L9}For a certain c, the inequalities below hold with probability at least :

\end{lemma}
\begin{proof}
We are going to prove the first inequality and the second can be proved in a similar way. Firstly, we consider the single direction . The  will not exceed  due to the definition, then
the change of  will not exceed  when we replace a single  with . Due to the Mcdiarmid's inequality, the inequality below holds with probability at least :

Due to the symmetrization inequality \cite{foundation}, we can obtain that:

The other direction is similar.
\end{proof}
~\\

Now we can prove the Theorem \ref{MCUL_bound}:
\begin{proof}
Notice that . Due to Lemma \ref{T2}, we can get:

 According to the sub-additivity of supremum, we can get the inequality below:


Due to the union bound and Lemma \ref{L9}, the inequality below holds with probability at least : 

which concludes the proof.
\end{proof}

\subsection*{{Proof of Theorem \ref{CPI}}}
\label{AC}
\begin{proof}
\rm Denote  with . Due to the equation (\ref{cond}), the equations following hold:

By substituting the second equation into the first one, we can get:

By denoting  with , the following equations hold:

which concludes the proof of equation (\ref{tot2}). (\ref{tot3}) can be proved in the same way as in the proof of Lemma \ref{Eu}.
\end{proof}
\end{document}