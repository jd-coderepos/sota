[{'LEADERBOARD': {'Task': 'Multimodal Intent Recognition', 'Dataset': 'PhotoChat', 'Metric': 'F1', 'Score': '52.2'}}, {'LEADERBOARD': {'Task': 'Multimodal Intent Recognition', 'Dataset': 'PhotoChat', 'Metric': 'Precision', 'Score': '44.8'}}, {'LEADERBOARD': {'Task': 'Multimodal Intent Recognition', 'Dataset': 'PhotoChat', 'Metric': 'Recall', 'Score': '62.7'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Humanities', 'Score': '27.2'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Average (%)', 'Score': '27.1'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Parameters (Billions)', 'Score': '0.031'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'STEM', 'Score': '27.7'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Social Sciences', 'Score': '25.7'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Other', 'Score': '27.9'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'Quora Question Pairs', 'Metric': 'Accuracy', 'Score': '90.5%'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'SQuAD2.0', 'Metric': 'EM', 'Score': '89.731'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'SQuAD2.0', 'Metric': 'F1', 'Score': '92.215'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'SQuAD2.0', 'Metric': 'EM', 'Score': '88.107'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'SQuAD2.0', 'Metric': 'F1', 'Score': '90.902'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'SQuAD2.0 dev', 'Metric': 'F1', 'Score': '88.1'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'SQuAD2.0 dev', 'Metric': 'EM', 'Score': '85.1'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'SQuAD2.0 dev', 'Metric': 'F1', 'Score': '85.9'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'SQuAD2.0 dev', 'Metric': 'EM', 'Score': '83.1'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'SQuAD2.0 dev', 'Metric': 'F1', 'Score': '82.1'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'SQuAD2.0 dev', 'Metric': 'EM', 'Score': '79.0'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'SQuAD2.0 dev', 'Metric': 'F1', 'Score': '79.1'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'SQuAD2.0 dev', 'Metric': 'EM', 'Score': '76.1'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'CommonsenseQA', 'Metric': 'Accuracy', 'Score': '76.5'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'QNLI', 'Metric': 'Accuracy', 'Score': '99.2%'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'WNLI', 'Metric': 'Accuracy', 'Score': '91.8%'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'RTE', 'Metric': 'Accuracy', 'Score': '89.2%'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'MultiNLI', 'Metric': 'Matched', 'Score': '91.3'}}, {'LEADERBOARD': {'Task': 'Semantic Textual Similarity', 'Dataset': 'MRPC', 'Metric': 'Accuracy', 'Score': '93.4%'}}, {'LEADERBOARD': {'Task': 'Semantic Textual Similarity', 'Dataset': 'STS Benchmark', 'Metric': 'Pearson Correlation', 'Score': '0.925'}}, {'LEADERBOARD': {'Task': 'Sentiment Analysis', 'Dataset': 'SST-2 Binary classification', 'Metric': 'Accuracy', 'Score': '97.1'}}, {'LEADERBOARD': {'Task': 'Linguistic Acceptability', 'Dataset': 'CoLA', 'Metric': 'Accuracy', 'Score': '69.1%'}}]
