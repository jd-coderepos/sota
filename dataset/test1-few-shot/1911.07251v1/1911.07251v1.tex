\def\year{2020}\relax
\documentclass[letterpaper]{article} \usepackage{aaai20}  \usepackage{times}  \usepackage{helvet} \usepackage{courier}  \usepackage[hyphens]{url}  \usepackage{graphicx} \urlstyle{rm} \def\UrlFont{\rm}  \usepackage{graphicx}  \frenchspacing  \setlength{\pdfpagewidth}{8.5in}  \setlength{\pdfpageheight}{11in}  \usepackage{mathrsfs} \usepackage{amsfonts,amssymb}  \usepackage{tabularx} \usepackage{url}

\usepackage{array}
\newcommand{\PreserveBackslash}[1]{\let\temp=\\#1\let\\=\temp}
\newcolumntype{C}[1]{>{\PreserveBackslash\centering}p{#1}}
\newcolumntype{R}[1]{>{\PreserveBackslash\raggedleft}p{#1}}
\newcolumntype{L}[1]{>{\PreserveBackslash\raggedright}p{#1}}


\usepackage[colorlinks,
            linkcolor=black,
            anchorcolor=black,
            citecolor=black,
            urlcolor=black]{hyperref}


\pdfinfo{
/Title (DualVD: An Adaptive Dual Encoding Model for Deep Visual Understanding in Visual Dialogue)
/Author (AAAI Press Staff, Pater Patel Schneider, Sunil Issar, J. Scott Penberthy, George Ferguson, Hans Guesgen)
} 



\setcounter{secnumdepth}{0} 

\setlength\titlebox{2.5in} \title{DualVD: An Adaptive Dual Encoding Model for Deep Visual Understanding in Visual Dialogue}
\author{Xiaoze Jiang\textsuperscript{\rm 1,2} Jing Yu \textsuperscript{\rm 1}\thanks{Corresponding authors: Jing Yu and Zengchang Qin.}  Zengchang Qin\textsuperscript{\rm 2*} Yingying Zhuang\textsuperscript{\rm 1,2} Xingxing Zhang\textsuperscript{\rm 3} Yue Hu\textsuperscript{\rm 1} Qi Wu\textsuperscript{\rm 4} \\
  \textsuperscript{\rm 1}Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China\\ \textsuperscript{\rm 2} Intelligent Computing and Machine Learning Lab, School of ASEE, Beihang University, Beijing, China\\  \textsuperscript{\rm 3} Microsoft Research Asia, Beijing, China\\
   \textsuperscript{\rm 4}  University of Adelaide, Australia \\
   \{yujing02, huyue\}@iie.ac.cn,   \{xzjiang,zcqin\}@buaa.edu.cn, xizhang@microsoft.com, qi.wu01@adelaide.edu.au
}



 \begin{document}

\maketitle

\begin{abstract}
Different from Visual Question Answering task that requires to answer only one question about an image, Visual Dialogue involves multiple questions which cover a broad range of visual content that could be related to any objects, relationships or semantics. The key challenge in Visual Dialogue task is thus to learn a more comprehensive and semantic-rich image representation which may have adaptive attentions on the image for variant questions. In this research, we propose a novel model to depict an image from both visual and semantic perspectives. Specifically, the visual view helps capture the appearance-level information, including objects and their relationships, while the semantic view enables the agent to understand high-level visual semantics from the whole image to the local regions. Futhermore, on top of such multi-view image features, we propose a feature selection framework which is able to adaptively capture question-relevant information hierarchically in fine-grained level. The proposed method achieved state-of-the-art results on benchmark Visual Dialogue datasets. More importantly, we can tell which modality (visual or semantic) has more contribution in answering the current question by visualizing the gate values. It gives us insights in understanding of human cognition in Visual Dialogue.
\end{abstract}

\section{Introduction}

\begin{figure}[t]
\centering
\includegraphics[width=8cm]{2236_basicIdea.pdf}
\caption{An illustration of DualVD. Left: the input of the dialogue system. Right: visual and semantic modules designed to adaptively understand the visual content like humans. The answer is inferred depending on multi-modal evidence.}
\label{basicIdea}
\end{figure}

To understand the real world by analyzing vision and language together is a priority for AI to achieve human-like abilities, which enables the development of diverse applications, such as Visual Question Answering (VQA) \cite{Agrawal2017VQA}, Referring Expressions \cite{Wang2019Neighbourhood}, Image Captioning \cite{johnson2016densecap}, \emph{etc}. To move a step further, this work focuses on the Visual Dialogue \cite{Das2017Visual} problem, which requires the agent to answer a series of questions in natural language regarding an image. It is more challenging because it demands the agent to adaptively focus on diverse visual content with respect to the current question, while other vision-language problems mostly attend to some specific objects or regions. Considering the dialogue in Figure \ref{basicIdea}: Given `` \emph{Q1: Is the man on the skateboard?}'', the agent should be aware of the foreground visual content, i.e. \emph{the man, the skateboard}, while `` \emph{Q5: Is there sky in the picture?}'' changes the attention of the agent to the background of \emph{sky}. Besides appearance-level questions like \emph{Q1} and \emph{Q5}, `` \emph{Q4: Is he young or older?}'' requires the agent to reason about the visual content for higher-level semantics. How to adaptively capture the desired visual content through dialogue becomes one of the most critical challenges in visual dialogue.  


The typical solution for visual dialogue is to firstly fuse visual (\emph{i.e.} image) features and textual (\emph{i.e.} dialogue history, current question) features together and then to infer the correct answer. Most approaches focus on enhancing the textural representations by recovering the dialogue relational structure \cite{Zheng2019Reasoning}, imperfect dialogue history \cite{yang2019making}, and dialogue consistency \cite{wu2018areyou}. However, the role of visual information is at present less studied. Existing models simply use CNN \cite{Simonyan2014Very} or R-CNN \cite{Ren2017Faster} to extract visual features and focus on the question-relevant content. Such visual features have limited expressive ability due to the monolithic representations \cite{Wang2019Neighbourhood}. On one hand, questions in a visual dialogue refer to a wide range of visual content, including objects, relationships and high-level semantics, which can not be covered by monolithic features. On the other hand, the referred visual content may change remarkably from visual appearance to high-level semantics through the dialogue, which is difficult for monolithic features to capture. 



Our work is inspired by the Dual-coding theory \cite{paivio1971imagery} of human cognition process. \textit{Dual-coding theory}  postulates that our brain encodes information in two ways: \textit{visual imagery} and \textit{textual associations}.  When asked to act upon a concept, our brain retrieves either images or words, or both simultaneously. The ability to encode a concept by two different ways strengthens the capacity of memory and understanding. Inspired by the cognitive process, we first propose a novel scheme to comprehensively depict an image from  both  visual  and  semantic  perspectives, where the major objects and their relationships are kept in the visual view while the higher-level abstraction is provided in the semantic view. We propose a model called \textit{Dual Encoding Visual Dialogue} (\textit{DualVD}) to adaptively select question-relevant information from the image in a hierarchical mode: intra-modal selection first captures the visual and semantic information individually from the object-relational visual features and global-local semantic features; then inter-modal selection obtains the joint visual-semantic knowledge by correlating vision and semantics. This hierarchical framework imitates human cognition process to capture targeted visual clues from multiple perceptual views and semantic levels. 




The main contributions are summarized as follows: (1) We exploit the possibility of cognition in visual dialogue by depicting an image from both visual and semantic views, which covers a broad range of visual content referred by most of questions in the visual dialogue task; (2) We propose a hierarchical visual information selection model, which is able to progressively select question-adaptive clues from intra-modal and inter-modal information for answering diverse questions. 
It supports explicit visualization in visual-semantic knowledge selection and reveals which modality has more contribution to answer the question; (3) The proposed model outperforms state-of-the-art approaches on benchmark visual dialogue datasets, which demonstrates the feasibility and effectiveness of the proposed model. The code is available at \url{https://github.com/JXZe/DualVD}.












\section{Related Work}

\subsubsection{Visual Question Answering (VQA)} focuses on answering arbitrary natural language questions conditioned on an image. The typical solutions in VQA build multi-modal representations upon CNN-RNN architecture \cite{ren2015exploring,wu2017image}. Existing approaches incorporate context-aware visual features. 
For example, \cite{ren2015exploring} applies CNN features of the whole image as global context, \cite{xu2016ask,anderson2018bottom-up} adopt patches and salient objects learned by attention mechanism as the region context, and \cite{gao2018question,li2019relation} exploits inter-object relationships via graph attention networks or convolutional networks to model the relational context. However, how to leverage the external visual-semantic knowledge to learn more informative relational representations for better semantic understanding has not been well exploited yet.   
Another emerging line of work represents visual content explicitly by natural language and solves VQA as a reading comprehension problem. In \cite{Li2019Visual}, the image is wholly converted into descriptive captions, which preserves information at semantic-level in textual domain. 
However, this kind of approaches use the generated captions, which could not be correct as we desired, and that they fully abandon the informative and subtle visual features.
Besides the specific tasks, our model has notable progress compared to the above approaches. We adopt dual encoding mechanism to provide both appearance-level and semantic-level visual information, so that it incorporates the strong points of the above two kinds of approaches. 





\subsubsection{Visual Dialogue} aims to answer a current question conditioned on an image and dialogue history. 
Most existing works are based on late fusion framework and focused on modeling the dialogue history. Sequential co-attention mechanism \cite{wu2018areyou} enables the model to identify question-relevant image regions and dialogue history to keep the dialogue consistency. \cite{yang2019making} introduces false response in dialogue history for an adverse critic on the historic error.  \cite{Zheng2019Reasoning} introduces an Expectation Maximization algorithm to infer the dialogue structure and the answers via graph neural networks. 
By contrast to extensive study on modeling dialogue history, the image content has been less studied. Although some works devise attention mechanism to focus on the essential visual features most relevant to the question and dialogue history, such monolithic visual representations still have limited expressive abilities. In this work, we exploit the role of visual information in visual dialogue. Different from existing works merely modeling the  appearance, our model is able to adaptively capture visual and semantic information in a hierarchical mode inspired by the Dual-coding theory of human cognition process to provide adequate visual clues for diverse questions in visual dialogue.











\begin{figure*}[t]
\centering
\includegraphics[width=17.5cm]{2236_model.pdf}
\caption{Overview structure of the DualVD model for visual dialogue. The model mainly contains two parts: Visual Module and Semantic Module, where ``G'' represents gate operation given inputs.}
\label{model_pic}
\end{figure*}

\section{Methodology}
The visual dialogue task can be described as follows: given an image  and its caption , a dialogue history till round -, , and the current question , 
the task is to rank a list of 100 candidate answers  and return the best answer  to . In this section, we first introduce the idea of  depicting an image from both visual and semantic perspectives. It covers a broad range of visual content like objects, relationships, global semantics and local semantics. Then we introduce a hierarchical feature selection approach to adaptively capture question-relevant visual-semantic information. Our model is based on the late fusion (LF) framework \cite{Das2017Visual}, which will be described at the end of this section. 




\subsection{Visual-Semantic Dual Encoding}
\label{DualEncoding}

In visual dialogue, two types of information play the primary role to depict an image and answer the diverse questions: visual information and semantic information (Figure \ref{model_pic}). For visual information, the major objects and relationships should be kept. In semantic information, higher-level abstraction of the image content should be provided, which involves prior knowledge and complex cognition. In this section, we introduce a dual encoding scheme to generate both visual and semantic representations to depict an image. A scene graph is proposed to represent the visual information while multi-level captions in natural language are leveraged to represent the semantic information. These representations are served as the input of our DualVD model. 

\subsubsection{Scene Graph Construction} Each image is represented as a scene graph. Let  denotes its nodes, which represents objects detected by a pre-trained object detector and let  denotes its edges, which represents the semantic visual relationships embedded by our visual relationship encoder. 
We use a pre-trained Faster-RCNN \cite{Ren2017Faster} to detect  objects in an image and describe the object  as a -dimensional vector, denoted by . The visual relationship encoder \cite{zhang2019large}, which is pre-trained on a visual relationship benchmark, i.e. \textit{GQA} \cite{hudson2019GQA}, encodes relationships between the subject  and object  as a -dimensional relation embedding, denoted as . We assume that certain relationship exists between any pair of objects by considering ``unknown-relationship'' as a special kind of relationship. Therefore, the scene graph we constructed is fully-connected.

The visual relationship encoder embeds the relationships between objects into a semantic space which is aligned with their corresponding descriptions in natural language. Such continuous representations instead of discrete labels can preserve the discriminative capability and contextual awareness. 
Inspired by recent work \cite{zhang2019large}, our encoder consists of a visual part and a textual part. The visual part takes three CNN feature maps corresponding to the visual regions of subject, object and their union region as input and outputs the three encoded embeddings ,  and . The textual part uses a shared GRU to encode the annotations and yield textual embeddings. The loss function is designed to minimize the cosine similarity between the embeddings of positive visual-textual pairs and alienate negative pairs.  The union embedding  is served as the visual relationship representation  between  and . 


\subsubsection{Multi-level Image Captions} The advantages of captions compared to visual features lie in that captions are represented by natural language with high-level semantics, which can provide straightforward clues for the questions without ``heterogeneous gap''. Global image caption  (provided by the dataset) is beneficial to response to questions exploring the scene. Meanwhile, dense captions \cite{johnson2016densecap}, denoted as  ( is the number of dense captions), provide a set of local-level semantics, including the object properties (position, color, shape, ), the prior knowledge related to the objects (weather, species, emotion, ), and the relationships between objects (interactions, spatial positions, comparison, ). The words in both  and  are represented by concatenated GloVe \cite{Pennington2014Glove} and ELMo \cite{Peters2018Deep} word embeddings. Then  and  are separately encoded with two different LSTMs, denoted as  and , respectively.

\subsection{Adaptive Visual-Semantic Knowledge Selection}
\label{selection}

On top of the visual and semantic image representations, we propose a novel feature selection framework to adaptively select question-relevant information from the image. Under the guidance of the current question, the feature selection process is devised in a hierarchical mode: intra-modal selection first captures the visual and semantic information respectively from the \textit{visual module} and \textit{semantic module}; then inter-modal selection obtains the desired visual knowledge from both the visual module and semantic module via \textit{selective visual-semantic fusion}. The advantages of such hierarchical framework is that it can explicitly reveal the progressive feature selection mode and preserve fine-grained information as much as possible.





\subsubsection{Visual Module}

This module is presented on the top of Figure \ref{model_pic}. Based on the constructed scene graph introduced in \textit{Scene Graph Construction}, we aim to select question-relevant relation information and object information. For relation information, we propose a relation-based graph attention network to enrich the object representations with question-aware relationships. It mainly consists of two units: \textit{Question-Guided Relation Attention} highlights the critical relationships and \textit{Question-Guided Graph Convolution} enriches the object features by its relation-critical neighbors. 
For object information, we highlight the most informative objects to answer the question. Finally, the clues of objects and relationships are further fused in \textit{Object-Relation Information Fusion} to obtain the question-relevant visual content. 

\textit{Question-Guided Relation Attention}: The question-guided relation attention examines all the relationships to highlight the ones most relevant to the question. First, we select question-relevant information from the dialogue history to merge into the question representation via a gate operation, which is defined as:



where ``'' denotes concatenation, ``'' denotes the element-wise product. Each word is represented by concatenating the hidden states extracted from pre-trained GloVe and ELMo models. Then dialogue history  and the current question  are separately encoded with two different LSTMs, denoted as  and , respectively.  is a vector of  gate values over  and ,  (as well as  mentioned below) is the linear transformation layer and  is the encoded history-aware question features.

The attention weights  of all the visual relationships are calculated under the guidance of the question :


Each relation embedding is updated based on the attention importance. Formally defined  as:

where  is the question-guided relation embedding.

\textit{Question-Guided Graph Convolution}: This module further updates each object's representation under the guidance of questions by aggregating information from its neighborhood and the corresponding relationships. Given the feature  of object  and its relation embedding , the attention value of  w.r.t.  is calculated as:





The obtained attention values for all the neighbors of  are used to compute a linear combination of their features, which serves as the updated representation  for :



Since the scene graph is a fully connected graph, the number of neighbors  for each object is equal to the number of objects detected in each image. 

\textit{Object-Relation Information Fusion}: In visual dialogue, the object appearance and the visual relationships will contribute to infer the answer, but with different contributions. In this module, we adaptively fuse question-relevant object features  from both original object feature  and relation-aware object feature  again by a gate, 
which is defined by:


where  is the updated representation of object . The whole image representation  is obtained as the weighted sum of the object representations.  In order to strengthen the influence of the current question  and the original object features on the retrieved visual clues, we calculate the attention value  for  under the guidance of : 
Then the the whole representation of the image  can be updated by:



\subsubsection{Semantic Module}

This module aims to select and merge question-relevant semantic information from global and local captions with a \textit{Question-Guided Semantic Attention} module and a \textit{Global-Local Information Fusion} module. The semantic module is located in the middle of Figure \ref{model_pic}. 


\textit{Question-Guided Semantic Attention}: The semantic attention mechanism highlights relevant captions at both global-level and local-level. This type of attention is guided by the current question which is enhanced with corresponding information from the dialogue history (as introduced above). According to the attention distribution, we enrich the caption representations in order to better adapt to the question. The attention value for each caption in  is calculated as follows:


The caption representation for  and   will be updated to  and :



\textit{Global-Local Information Fusion}: Some questions are global-related while others are local-related. This step adaptively selects the information from the global caption  and local caption  via a gate as described above:


where  is the textural representations for the abstract visual semantics.


\subsubsection{Selective Visual-Semantic Fusion}

When asked to answer a question, the agent will retrieve either the visual information or the semantic information individually, or both simultaneously.
In this module, we design a gate operation to decide the contributions of the two modalities on the answer prediction. The gate operation and the final visual knowledge representation  are calculated as:



\subsection{Late Fusion and Discriminative Decoder}
\label{LateFusion}
The full model consists of late fusion encoder and discriminative (softmax) decoder. The encoder first embeds each part in a dialogue tuple . Then we concatenate   and  with the visual knowledge representation  into a joint input embedding for answer prediction. 
The decoder ranks all the answers from a set of 100 candidates . It first encodes each candidate via a common LSTM. Then a dot product followed by softmax operation is calculated between the joint input embedding and candidates to get the posterior probability over each candidate. We obtain the correct answer by ranking the candidates based on their posterior probabilities. Our model can also be applied to more complex decoders and fusion strategies, such as memory network, co-attention, adversarial network, \emph{etc}. In this paper, we utilize the simple late fusion and  discriminative decoder to highlight the advantages of our visual encoder.




\iffalse
\subsection{Visual Module}
\label{VisualModule}

Considering that a majority of questions are referring to the visual objects and their semantic relationships, in this module, we exploit to effectively represent the object appearance and their relational semantics in an image.The architecture of the visual module is presented on the top of  Figure \ref{model_pic}. Each image is represented as a scene graph with each node denoting an object and each directed edge denoting the relationship between objects. On top of the constructed scene graph, we propose an upgraded Graph Attention Network to enrich the region features with relational semantics. It mainly consists of two units: \textit{question-guided relation attention} and \textit{Relation-guided graph convolution}. Finally, the information of object appearance and relationships are further fused to adaptively highlight the question-relevant visual semantics. 


\textbf{Scene Graph Construction}

Each image is represented as a scene graph where the nodes denoted as , represent objects detected by a pre-trained object detector while edges, denoted as , represent the semantic visual relationships embedded by our relationship encoder. Note that the edges are directed. We use a pre-trained Faster-RCNN \cite{Ren2017Faster} to detect  objects in an image and describe the object  as a -dimensional vector, denoted as . The relationship encoder, pre-trained on a visual relationship benchmark, i.e. \textit{GQA} \cite{hudson2019GQA}, encodes relationships between the object  and object  as a -dimensional relation embedding, denoted as . We assume that certain relationship exists between any pair of objects by considering ``unknown-relationship'' as a kind of relationship. Therefore, the graph we constructed is fully-connected.










The visual relationship encoder embeds the relationships between objects into a semantic space which is aligned with their corresponding descriptions in natural language. Such soft and continuous representations for the relationships instead of rigid and discrete labels can preserve the discriminative capability and contextual awareness. 
Inspired by the most recent work \cite{zhang2019large}, our encoder consists of a visual module and a semantic module. The visual module takes three CNN feature maps with respect to the visual regions of subject, relationship and object as input and output the corresponding three encoded embeddings ,  and . The semantic module uses a shared GRU to encode the textual annotations and yield textual embeddings ,  and . The loss function is designed to minimize the cosine similarity between the embeddings of positive visual-semantic pairs and alienate negative pairs. The detailed structure of the two modules as well as the design of the loss function is referring to \cite{zhang2019large} and presented in the supplementary material. Finally, the learned output embedding  from the visual module is served as the visual relationship representation, which is denoted as  between  and . 




\textbf{Question-Guided Relation Attention}

The question guided relation attention examines all the relationships to highlight the ones most relevant to the question. First, we adaptively select question-relevant information from the dialogue history to merge into the question representation via a gate operation, which is defined as:



where ``'' denotes concatenation, ``'' denotes the element-wise product,  is a vector of  gate values over  and ,  (and  mentioned below) is the weight of dimension transformation layer which is convenient for vector calculation and  is the encoded history-aware question features.

The attention weights  of all the visual relationships are calculated under the guidance of the question:


Each of the relation embedding is updated based on the attention importance. Formally defined  as:

where  is the question-guided relation embedding.

\textbf{Question-Guided Graph Convolution}

This module further updates each object's representation under the guidance of questions by aggregating information from its neighborhood and the corresponding relationships. Given the feature  of object  and its relation embedding , the attention value of  with regard to  is calculated as:





The obtained attention values for all the neighbors of  are used to compute a linear combination of their features, which serves as the final representation  for :



Since the scene graph is a fully connected graph, the number of neighbors  for each object is equal to the number of objects detected in each image. 

\textbf{Object-Relation Feature Fusion}

In visual dialogue, both the object appearance and the visual relationships will contribute to the answer inference, but in varying degrees. In this module, we adaptively fuse question-relevant object features  from both original object feature  and relation-aware object feature  by a gate mechanism. 
which is defined as:


where  is the updated representation of object . The whole image representation  is obtained as the weighted sum of the object representations.  In order to strength the influence of the current question  and the original object features on the retrieved visual clues, we calculate the attention value  for  under the guidance of : 
Then the the whole representation of the image  can be updated by:



\subsection{Semantic Module}
\label{SemanticModule}

Among all the modalities, natural language is a flexible one to describe the high-level semantic visual information via complex cognitive process from the perceived low-level visual features. A diverse range of prior knowledge and reasoning ability is involved in the transformation from vision to language. We attempt to convert the image into hierarchical descriptions in natural language, so as to implicitly introduced prior knowledge and human inference for deep visual understanding. The architecture of our proposed semantic module is presented in the middle of Figure \ref{model_pic}. 

\textbf{Hierarchical Image Description}

We propose to represent an image by both global-level abstraction covering the full image and the local-level dense descriptions for salient regions. Global image caption  (provided by the dataset) is beneficial to response the questions exploring the scene. Meanwhile, dense caption \cite{johnson2016densecap}, denoted as  ( is the number of dense captions), provides a set of local-level semantics, including the object properties (position, color, shape, ), the relationships between objects (interactions, spatial positions, comparison, ) and the prior knowledge related to the objects (weather, species, emotion, ).The words in both  and  are represented by concatenated glove and elmo word embeddings. Then  and  are separately encoded with 2 different LSTMs, denoted as  and  respectively.

\textbf{Question-guided Semantic Attention}

The semantic attention mechanism highlights relevant captions at both global-level and local-level. Normally, this type of attention is guided by the current question which is enhanced with corresponding information from the dialogue history (as introduced above). According to the attention importance, we enrich the caption representations in order to better adapt to the question. The attention value for each caption in  is calculated as:


The caption representation for  and   will be updated to  and :



\textbf{Global-local Feature Fusion}

Actually, some questions are global-related while others are local-related. This step adaptively selects the information from global caption  and local caption  via a gate as described above:


where  is the textural representations for the abstract visual semantics.



\subsection{Selective Visual-Semantic Fusion}

Different questions in a dialogue require different amount of appearance-level or semantic-level visual information, based on which the agent can generate more diverse and human-like responses. 

According to the cognitive process of human, people not only use visual information but also use textual information to understand the world. The Visual Model and Semantic Model are two different representations of visual information, and we design a gate to adaptively select which type of information to use when answering the question. The whole visual representation  can be got as:


\fi


\section{Experiments}
\label{sec:experiments}



\textbf{Datasets}: We conduct extensive experiments on datasets \cite{Das2017Visual}: VisDial v0.9 and VisDial v1.0. For both datasets, the examples are split into  ``train'', ``val'' and ``test'' and 
each dialogue contains 10 rounds of question-answer pairs. 
VisDial v1.0 is an upgraded version of VisDial v0.9. For VisDial v0.9, all the splits are built on MSCOCO images. 
For VisDial v1.0, all the splits of VisDial v0.9 serve as ``train'' (120k), while ``val'' (2k) and ``test'' (8k)  consist of dialogues on extra 10k COCO-like images from Flickr. 

\textbf{Evaluation Metrics}: We follow the metrics in \cite{Das2017Visual} to evaluate the response performance. In the test stage, the model is asked to rank 100 candidate answer options and evaluated by Mean Reciprocal Rank (MRR), Recall\textsl{@} and Mean Rank of human response (Mean) on both datasets. 
For VisDial v1.0, Normalized Discounted Cumulative Gain (NDCG) is added as an extra metric for more comprehensive analysis. Lower value for Mean and higher value for other metrics are desired.


\textbf{Implementation Details}: For the textual part, the maximum sentence length of the dialogue history,  dense captions and the current question is all set to 20. The hidden state size of all the LSTM blocks is set to 512.  We use Faster-RCNN with the ResNet-101 to detect object regions and extract the 2048-dimensional region features.
Since some captions with low confidence are likely to introduce unexpected noise and too many captions will decrease the computation efficiency, we select the top 6 (the mean value of the caption distribution) dense captions in our model. We train all of our models by Adam optimizer with 16 epochs, where the mini-batch size is 15 and the dropout ratio is  0.5. For the strategy of learning rate, we first apply warm up strategy for 2 epoches with initial learning rate   and warm-up factor 0.2. Then we adopt cosine annealing learning strategy with initial learning rate = and termination learning rate = for the rest epoches. 










\iffalse
\indent\indent\textbf{Dataset} VisDial v1.0 is the latest dataset in Visual dialogue, which was collected based on MS COCO images and captions. VisDial v1.0 is divided into training set, validation set and testing set. The training set consists of dialogues on 120k images from COCO-trainval, while the validation and test sets consist of dialogues on an additional 10k COCO-like images from Flickr.  What's more, in training and validation sets each image equipped with 10 rounds dialogue, and in testing set the dialogue for each image has a random length within 10 rounds. And we also trained our model on VisDial v0.9, which is the previous version of VisDial v1.0. VisDial v0.9 contained 83k dialogues on COCO-train and 40k on COCO-val images, following \cite{Das2017Visual} we split 80k as training set, 3k for validation set and 40k as testing set. It should be noted that training set data in VisDial v1.0 is fully composed of the data from VisDial v1.0.


\textbf{Metrics}
Following \cite{Das2017Visual}, we choose retrieval metrics: MRR, R\textsl{@}1, R\textsl{@}5, R\textsl{@}10, Mean and NDCG for evaluation. 
\begin{itemize}
\item MRR: 
mean reciprocal rank (higher is better).
\item R\textsl{@}:
existence of the human response in top-k ranked responses (higher is better).
\item Mean: rank of human response
mean rank of human response (lower is better).
\item NDCG: 
normalized discounted cumulative gain (higher is better). 

As some of the candidate options may be semantically identical (e.g. 'hi' and 'hello'), Visual dialogue Team has had four human annotators indicate whether each of the 100 candidate answers is correct for each val and test phase instance. For evaluation, we report NDCG over the top K ranked options, where K is the number of answers marked as correct by at least one annotator. NDCG can be calculated as follows:


\end{itemize}


\textbf{Details}
We minimized the cross-entropy loss for training, and we used Adam \cite{Kingma2014Adam}. For the strategy of learning rate we connect Warm-Up and Cosine Annealing learning strategy. In detail, first warm up for 2 epoches with Warm-Up factor of 0.2 to initial learning rate which is . Then we adopt Cosine Annealing learning strategy and initial learning rate =, termination learning rate =, the transformation formula is as follows:

where  represents maximum number of epoch,  represents current epoch.

The LSTM hidden size is all set to 512, the dropout ratio is set to 0.5 and max sentence length of question, history and caption are all set to 20, and the maximum number of epoch is set to 16.



As shown in Figure \ref{caption_st}, -axis represents the number of Dense Caption of each image, -axis represents the number of images. Whether in training, validation or testing sets, the distribution of each self-extracted caption`s number approximates a normal distribution as well as in the whole dataset VisDial v1.0 (or VisDial v0.9), and the mean of the normal distribution is about 6. For the convenience of training, but also to not introduce too much noise, we set the the number of Dense Caption after selected by confidence  6.
\fi

\iffalse
\begin{table}[t]
\caption{Quantiative evaluation of discriminative model on validation split of VisDial v0.9.}
\label{v9}
\begin{tabular}{L{2.42cm}C{0.68cm}C{0.68cm}C{0.68cm}C{0.8cm}C{0.68cm}}\hline                       
Model & MRR & R\textsl{@}1 & R\textsl{@}5 & R\textsl{@}10 & Mean \\
\hline  
LF &58.07&43.82&74.68&84.07&5.78  \\
HRE &58.46 &44.67&74.50&84.22&5.72\\  
HREA &58.68&44.82&74.81&84.36&5.66 \\  
MN & 59.65 & 45.55 & 76.22&85.37& 5.46 \\ 
SAN-QI &57.64&43.44&74.26&83.72&5.88\\
HieCoAtt-QI &57.88&43.51&74.49&83.96&5.84\\
AMEM &61.60&47.74&78.04&86.84&4.99\\
HCIAE &62.22&48.48&78.75&87.59&4.81\\
SF &62.42&48.55&78.96&87.75&4.70\\
CoAtt &63.98&50.29&80.71&88.81&4.47\\
CorefMN &\textbf{64.10}&\textbf{50.92}&80.18&88.81&4.45\\
VGNN &62.85&48.95&79.65&88.36&4.57\\
FGA\cite{schwartz2019factor} & \textbf{65.25} & \textbf{51.43} & \textbf{82.08} & 89.56 & 4.35 \\
\hline
\textbf{DualVD}  & 62.94 & 48.64 & 80.89 & 89.94 & 4.17 \\
\textbf{DualVD-MN}  & 63.03 & 48.75 & 81.09 & \textbf{90.27} & \textbf{4.12} \\
\hline  
\end{tabular}  
\end{table}  

\begin{table}[t] 
\caption{Quantitative evaluation of discriminative model on test-standard split of VisDial v1.0.}
\label{v1}
\begin{tabular}{L{1.66cm}C{0.6cm}C{0.6cm}C{0.6cm}C{0.69cm}C{0.6cm}C{0.77cm}}
\hline                       
Model & MRR & R\textsl{@}1 & R\textsl{@}5 & R\textsl{@}10 & Mean & NDCG  \\
\hline  
LF &55.42 & 40.95 & 72.45 & 82.83 & 5.95 & 45.31 \\
HRE & 54.16 & 39.93 & 70.47 & 81.50 & 6.41& 45.46 \\  
MN & 55.49 & 40.98 & 72.30 & 83.30 & 5.92 &47.50\\  
LF-Att &57.07&42.08&74.82&85.05&5.41&40.76\\
MN-Att & 56.90 & 42.43 & 74.00 & 84.35 & 5.59 & 49.58\\
CorefMN & 61.50 & 47.55 & 78.10 & 88.80 & 4.40 &54.70\\
VGNN & 61.37 &47.33 &77.98&87.83&4.57&52.82\\
\hline
\textbf{DualVD} & 63.23 & 49.25 & 80.23 & 89.70 & \textbf{4.11} & 56.32\\
\textbf{DualVD-MN} & 63.64 & \textbf{49.90} & \textbf{81.08} & \textbf{89.98} & 4.14 &56.12\\
\hline  
\end{tabular}  
\end{table}

\fi
\begin{table}[t]
\caption{Comparison on validation split of VisDial v0.9.}
\label{v9}
\resizebox{.95\columnwidth}{!}{
\begin{tabular}{L{2.42cm}C{0.68cm}C{0.68cm}C{0.68cm}C{0.68cm}C{0.68cm}}\hline                       
Model & MRR & R\textsl{@}1 & R\textsl{@}5 & R\textsl{@}10 & Mean \\
\hline  
LF &58.07&43.82&74.68&84.07&5.78  \\
HRE &58.46 &44.67&74.50&84.22&5.72\\  
MN & 59.65 & 45.55 & 76.22&85.37& 5.46 \\ 
SAN-QI &57.64&43.44&74.26&83.72&5.88\\
HieCoAtt-QI &57.88&43.51&74.49&83.96&5.84\\
AMEM &61.60&47.74&78.04&86.84&4.99\\
HCIAE &62.22&48.48&78.75&87.59&4.81\\
SF &62.42&48.55&78.96&87.75&4.70\\
CoAtt &63.98&50.29&80.71&88.81&4.47\\
CorefMN &\textbf{64.10} & \textbf{50.92} &80.18&88.81&4.45\\
VGNN &62.85&48.95&79.65&88.36&4.57\\
\hline
\textbf{DualVD}  & 62.94 & 48.64 & \textbf{80.89} & \textbf{89.94} & \textbf{4.17} \\
\hline  
\end{tabular}  
}
\end{table}  



\subsection{Overall Results}

In Table \ref{v9} and Table \ref{v1}, we compare DualVD with state-of-the-art discriminative models, namely  LF \cite{Das2017Visual}, HRE \cite{Das2017Visual}, 
MN \cite{Das2017Visual}, SAN-QI \cite{YangZi2016Stacked}, HieCoAtt-QI \cite{Lu2016Hierarchical}, AMEM \cite{seo2017visual},  HCIAE \cite{lu2017best}, SF \cite{Jain2018Two}, CoAtt \cite{wu2018areyou}, CorefMN \cite{Kottur2018Visual}, VGNN \cite{Zheng2019Reasoning}, LF-Att \cite{Das2017Visual}, MN-Att \cite{Das2017Visual}, 
RvA\cite{Niu2018Recursive} and DL-61\cite{guo2019image}. Our model consistently outperforms all the approaches on most metrics, which highlights the importance of visual understanding from visual and semantic modules in visual dialogue. CoAtt and HeiCoAtt-QI are relevant to our model in the sense that they leverage attention mechanism to identify question-relevant visual features. However, they ignore the semantic-rich relationships and language priors. It should be noted that our model and the compared approaches all belong to single-step models. With the success of multi-step reasoning, ReDAN \cite{Gan2019Multi} achieves 1\% boost over our model on most metrics. We believe that stacking our visual encoder to achieve multi-step visual understanding is a promising future work. DL-61 \cite{guo2019image} is a two-stage network for candidate selection and re-ranking while FGA \cite{schwartz2019factor} conducts attention across all the data parts, which gain relatively high performance on some metrics compared with our model. We believe that our model for the visual part and existing works for the dialogue or answer parts have complementary advantages. 










\begin{table}[t] 
\caption{Comparison on test-standard split of VisDial v1.0.}
\label{v1}
\resizebox{.95\columnwidth}{!}{
\begin{tabular}{L{1.66cm}C{0.6cm}C{0.6cm}C{0.6cm}C{0.69cm}C{0.6cm}C{0.77cm}}
\hline                       
Model & MRR & R\textsl{@}1 & R\textsl{@}5 & R\textsl{@}10 & Mean & NDCG  \\
\hline  
LF &55.42 & 40.95 & 72.45 & 82.83 & 5.95 & 45.31 \\
HRE & 54.16 & 39.93 & 70.47 & 81.50 & 6.41& 45.46 \\  
MN & 55.49 & 40.98 & 72.30 & 83.30 & 5.92 &47.50\\  
LF-Att &57.07&42.08&74.82&85.05&5.41&40.76\\
MN-Att & 56.90 & 42.43 & 74.00 & 84.35 & 5.59 & 49.58\\
CorefMN & 61.50 & 47.55 & 78.10 & 88.80 & 4.40 &54.70\\
VGNN & 61.37 &47.33 &77.98&87.83&4.57&52.82\\
RvA &63.03&49.03&80.40&89.83&4.18&55.59\\
DL-61 &62.20&47.90&\textbf{80.43}&\textbf{89.95}&4.17&\textbf{57.32}\\
\hline
\textbf{DualVD} & \textbf{63.23} & \textbf{49.25} & 80.23 & 89.70 & \textbf{4.11} & 56.32\\
\hline  
\end{tabular}  
}
\end{table}



\subsection{Ablation Study}

Ablation study on VisDial v1.0 validation set exploits the influence of the essential components of DualVD. We use the same discriminative decoder for all the following variations:

\textbf{Object Representation (ObjRep)}: this model uses the averaged object features to represent an image. Object representations are enhanced by question-driven attention.



\textbf{Relation Representation (RelRep)}: this model applies averaged relation-aware object representations via \emph{question-guided relation attention} and \emph{question-guided graph convolution} as the image representation. 

\textbf{Visual Module without Relationships (VisNoRel)}: this is our full visual module except that the relation embeddings are replaced by unlabeled edges and the convolution is conducted via the intra-modal attention \cite{gao2019dynamic}.

\textbf{Visual Module (VisMod)}: this is our full visual module, which fuses objects and relation features. 

\textbf{Global Caption (GlCap)}: this model uses LSTM to encode the global caption to represent the image. 

\textbf{Local Caption (LoCap)}: this model uses LSTM to encode the local captions to represent the image. 

\textbf{Semantic Module (SemMod)}: this is our full semantic module, which fuses global and local features. 

\textbf{DualVD (full model)}: this is our full model, which incorporates both the visual module and semantic module.









\begin{figure*}[t]
\centering
\includegraphics[width=17.6cm]{2236_result_visual.pdf} \caption{Visualization for DualVD. Visual module highlights the most relevant subject (red box) according to attention weights of each object ( in Eq. \ref{objectAtt}) and the objects (orange and blue boxes) with the top two attended relationships ( in Eq. \ref{relationAtt}). Semantic module shows the attention distribution ( in Eq. \ref{languageAtt}) over the global caption (first row) and the local captions (rest rows), where darker green color indicates bigger attention weight. The yellow thermogram on the top
visualizes the gate values ( in Eq. \ref{fussAtt}) of the visual embedding (left) and the caption embedding (right) in visual-semantic fusion. The ratio of gate values for the visual module and semantic module is computed from Eq. \ref{fussAtt}.}


\label{result_visual}
\end{figure*}


\iffalse
\begin{table}[t] 
\caption{Ablation study on ELMo of DualVD on VisDial v1.0 validation set (the first block) and test-standard set (the second block). }
\label{elmo_ab}
\begin{tabular}{L{1.8cm}C{0.6cm}C{0.6cm}C{0.6cm}C{0.69cm}C{0.6cm}C{0.77cm}}\hline                       
Model & MRR & R\textsl{@}1 & R\textsl{@}5 & R\textsl{@}10 & Mean & NDCG  \\
\hline  
w/o ELMo &63.67&49.89&80.44&89.84&4.14&56.41\\
DualVD& 64.64 & 50.74 & 82.10 & 91.00 & 3.91 & 57.30\\
\hline 
w/o ELMo  &62.84 & 49.13 & 79.93 & 89.15 & 4.39& 55.51\\
DualVD & 63.23 & 49.25 & 80.23 & 89.70 & 4.11 & 56.32\\

\hline  
\end{tabular}  
\end{table} 

\fi



\begin{table}[t] 
\caption{Ablation study of DualVD on VisDial v1.0.}
\label{ablation}
\resizebox{.95\columnwidth}{!}{
\begin{tabular}{L{1.66cm}C{0.6cm}C{0.6cm}C{0.6cm}C{0.69cm}C{0.6cm}C{0.77cm}}\hline                       
Model & MRR & R\textsl{@}1 & R\textsl{@}5 & R\textsl{@}10 & Mean & NDCG  \\
\hline  
ObjRep &63.84&49.83&81.27&90.29&4.07&55.48\\
RelRep &63.63 & 49.25 & 81.01 & 90.34 & 4.07& 55.12 \\
VisNoRel & 63.97 & 49.87 & 81.74 & 90.60 & 4.00& 56.73 \\ 
VisMod &64.11& 50.04&  81.78  &   90.52 &  3.99   &56.67 \\
\hline 
GlCap & 60.02 & 45.34 & 77.66 & 87.27 & 4.78 & 50.04\\
LoCap & 60.95 &  46.43 & 78.45 & 88.17 & 4.62 &51.72\\
SemMod & 61.07 & 46.69 & 78.56 & 88.09 & 4.59 & 51.10\\
\hline 
w/o ELMo &63.67&49.89&80.44&89.84&4.14&56.41\\
\textbf{DualVD} & \textbf{64.64} & \textbf{50.74} & \textbf{82.10} & \textbf{91.00} & \textbf{3.91} & \textbf{57.30}\\

\hline  
\end{tabular} 
}
\end{table} 



In Table \ref{ablation}, models in the first block are designed to evaluate the influence of key components in the visual module.  \textbf{ObjRep} only considers isolated objects and ignores the relational information, which achieves worse performance compared with VisMod. \textbf{RelRep} considers the relationships by introducing relation embedding. However, empirical study indicates that enhancing visual relationships while weakening object appearance is still not sufficient for better performance. \textbf{VisNoRel} fuses the information from both object appearance and neighborhoods without relational semantics, which achieves slight improvement compared to ObjRep. On top of VisNoRel, \textbf{VisMod} moves a step further by aggregating all the neighborhood features with relational information, which achieves the best performance compared to above three models. 

Orthogonal to visual part, models in the second block evaluate the influence of key components in the semantic part. The overall performance of either \textbf{GlCap} or \textbf{LoCap} decreases by 1\% and 0.15\% respectively, compared to their integrated version \textbf{SemMod}, which adaptively selects and fuses the task-specific descriptive clues from both global-level and local-level captions. 

\textbf{DualVD} results in a great boost compared to SemMod and a relatively slight boost compared to VisMod. This unbalanced boost indicates that visual module provides comparatively richer clues than semantic module. Combining the two modules together gains an extra boost because of their complementary information. The performance of DualVD without ELMo embedding decrease slightly, which proves that the improvement of DualVD mainly comes from the contribution of the novel visual representation.



\subsection{Interpretability}

A critical advantage of DualVD lies in its interpretability: DualVD is capable to predict the attention weights in the visual module, semantic module and the gate values in visual-semantic fusion. It supports explicit visualization and can reveal DualVD's mode in information selection. Figure \ref{result_visual} shows three examples with variant dependence on visual and semantic modules. The third example (third and fourth rows in Figure \ref{result_visual}) shows three round of dialogues about an image. In each round of dialogue, DualVD is capable to capture the most relevant visual and semantic information regarding the current question. In the first question, the visual module highlights the face of a boy and the relationships to his body and the other boy, while the semantic module puts more attention on the captions describing the two boys, which all provide useful clues to infer the correct answer. In the second and third round of dialogues, DualVD respectively attends to the whole grass and the discs. In this example, the attended information is adaptively changed through the dialogue and this explains why the correct answer is selected.

We further show another two examples with a current question and the dialogue history (first two rows in Figure \ref{result_visual}) to reveal DualVD's mode in information selection. We observe that the amount of information derived from each module highly depends on the complexity of the question and the relevance of the content. More information will come from the semantic module when the question involves complex relationships or the semantic module explicitly contains question-relevant clues. In Figure \ref{result_visual}, \textit{ratio of total gate values} reveals the amount of information derived from each module.  In the first example, more visual information is required. Similar observation exists for the second question in the third example. Such questions referring to object appearance depend more clues from the visual module. In the second example, the current question is about the relationship between the girl and the hair. The amount of semantic information remarkably increases since there exists explicit evidence ``\emph{The girl has long hair}''. This observation holds for the third question in the third example. Since language is a higher-level encoding of the visual content after complex reasoning involved with prior knowledge, it provides more useful clues for semantic-level questions.







\section{Conclusion}



In this paper, inspired by the dual-coding theory in cognitive science, we propose a novel DualVD model for visual dialogue. DualVD mainly consists of a visual module and a semantic module, which encodes image information at appearance-level and semantic-level, respectively. Desired clues for answer inference are adaptively selected from the two modules via gate mechanism. Results from extensive experiments on benchmarks demonstrate that deriving visual information from visual-semantic representations can achieve superior performance compared to other state-of-the-art approaches. Another major advantage of DualVD is its interpretability via progressive visualization. It can give us insight of how information from different modalities is used for inferring answers. 

\section{Acknowledgement}
This work is supported by the National Key Research and Development Program (Grant No.2017YFB0803301).







\bibliographystyle{aaai}
\bibliography{essay.bbl}






\end{document}
