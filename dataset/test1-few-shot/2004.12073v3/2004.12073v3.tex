\documentclass{article}





\usepackage[numbers]{natbib}
\usepackage[final]{neurips_2020}





\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{graphicx}
\usepackage{amsmath}
\usepackage{subfigure}


\title{All Word Embeddings from One Embedding}



\author{Sho Takase \\
  Tokyo Institute of Technology \\
  \texttt{sho.takase@nlp.c.titech.ac.jp} \\
\And
  Sosuke Kobayashi \\
  Tohoku University \hspace{4mm} Preferred Networks, Inc. \\
  \texttt{sosk@preferred.jp} \\
}

\begin{document}

\maketitle

\begin{abstract}
In neural network-based models for natural language processing (NLP), the largest part of the parameters often consists of word embeddings.
Conventional models prepare a large embedding matrix whose size depends on the vocabulary size.
Therefore, storing these models in memory and disk storage is costly.
In this study, to reduce the total number of parameters, the embeddings for all words are represented by transforming a shared embedding.
The proposed method, \textbf{ALONE} (\textbf{al}l word embeddings from \textbf{one}), constructs the embedding of a word by modifying the shared embedding with a filter vector, which is word-specific but non-trainable.
Then, we input the constructed embedding into a feed-forward neural network to increase its expressiveness.
Naively, the filter vectors occupy the same memory size as the conventional embedding matrix, which depends on the vocabulary size.
To solve this issue, we also introduce a memory-efficient filter construction approach.
We indicate our ALONE can be used as word representation sufficiently through an experiment on the reconstruction of pre-trained word embeddings.
In addition, we also conduct experiments on NLP application tasks: machine translation and summarization.
We combined ALONE with the current state-of-the-art encoder-decoder model, the Transformer~\cite{NIPS2017_7181}, and achieved comparable scores on WMT 2014 English-to-German translation and DUC 2004 very short summarization with less parameters\footnote{The code is publicly available at \href{https://github.com/takase/alone_seq2seq}{https://github.com/takase/alone\_seq2seq}}.
\end{abstract}

\section{Introduction}
Word embeddings have played a crucial role in the recent progress in the area of natural language processing (NLP)~\cite{Collobert:2011:NLP:2078183.2078186,NIPS2013_5021,pennington2014glove,peters-etal-2018-deep}.
In particular, word embeddings are necessary to convert discrete input representations into vector representations in neural network-based NLP methods~\cite{Collobert:2011:NLP:2078183.2078186}.
To convert an input word  into a vector representation in a conventional way, we prepare a one-hot vector  whose dimension size is equal to the vocabulary size  and an embedding matrix  whose shape is  ( represents a word embedding size).
Then, we multiply  and  to obtain a word embedding .


NLP researchers have used word embeddings as input in their models for several applications such as language modeling~\cite{DBLP:journals/corr/ZarembaSV14}, machine translation~\cite{Sutskever:2014:SSL:2969033.2969173}, and summarization~\cite{rush-chopra-weston:2015:EMNLP}.
However, in these methods, the embedding matrix forms the largest part of the total parameters, because  is much larger than the dimension sizes of other weight matrices.
For example, the embedding matrix makes up one-fourth of the parameters of the Transformer, a state-of-the-art neural encoder-decoder model, on WMT English-to-German translation~\cite{NIPS2017_7181}.
Thus, if we can represent each word with fewer parameters without significant compromises on performance, we can reduce the model size to conserve memory space.
If we save the memory space for embeddings, we can also train a bigger model to improve the performance.


To reduce the size of neural network models, some studies have proposed the pruning of unimportant connections from a trained network~\cite{NIPS1989_250,NIPS2015_5784,zhang-etal-2017-towards}.
However, their approaches require twice or much computational cost, because we have to train the network before and after pruning.
Another approach~\cite{Suzuki:Compress:Embed,Shu:coding,pmlr-v80-chen18g} limited the number of embeddings by utilizing the composition of shared embeddings.
These methods represent an embedding by combining several primitive embeddings, whose number is less than the vocabulary.
However, since they require learning the combination of assignments of embeddings to each word, they need additional parameters during the training phase.
Thus, prior approaches require multiple training steps and/or additional parameters that are necessary only during the training phase.


To address the above issues, we propose a novel method: \textbf{ALONE} (\textbf{al}l word embeddings from \textbf{one}), which can be used as a word embedding set without multiple training steps and additional trainable parameters to assign each word to a unique vector.
ALONE computes an embedding for each word (as a replacement for ) by transforming a shared base embedding with a word-specific filter vector and a shared feed-forward network.
We represent the filter vectors with combinations of primitive random vectors, whose total is significantly smaller than the vocabulary size.
In addition, it is unnecessary to train the assignments, because we assign the random vectors to each word randomly.
Therefore, while ALONE retains its expressiveness, its total parameter size is much smaller than the conventional embedding size, which depends on the vocabulary size due to .


Through experiments, we demonstrate ALONE can be used for NLP with comparable performances to the conventional embeddings but fewer parameters.
We first indicate ALONE has enough expressiveness to represent the existing word embedding (GloVe~\cite{pennington2014glove}) through a reconstruction experiment.
In addition, on two NLP applications of machine translation and summarization,
we demonstrate ALONE with the state-of-the-art encoder-decoder model trained in an end-to-end manner achieved comparable scores with fewer parameters than models with the conventional word embeddings.


\section{Proposed Method: ALONE}
\begin{figure}[!t]
  \centering
  \includegraphics[width=11cm]{./overview.pdf}
   \caption{Word embedding constructions by the conventional way and our proposed ALONE. ALONE represents each word with an embedding  and the filter vector . We fix source matrices with random initialized values, and thus it is unnecessary to prepare trainable parameters for construction of . To increase the expressiveness, we input the embedding into a feed-forward neural network.}
   \label{fig:overview}
\end{figure}


Figure~\ref{fig:overview} shows an overview of the conventional and proposed word embedding construction approaches.
As shown in this figure, the conventional way requires a large embedding matrix , where each column vector is assigned to a word, and we pick a word embedding with a one-hot vector .
In other words, each word has a word-specific vector, whose size is , and the total size summed over the vocabulary is .
To reduce the size, ALONE make words share some type of vector element with each other.
ALONE has a base embedding , which is shared by all words, and represents each word by transforming the base embedding.
To concretely obtain a word representation for , ALONE computes an element-wise product of  and a filter vector , and then applies a feed-forward network to increase its expressiveness.
The two components are described as follows.


\subsection{Filter Construction}\label{sec:mask_construction}
To make the result of the element-wise product  unique to each word, we have to prepare different filter vectors from each other.
In the simplest way, we sample values from a distribution such as Gaussian  times, and then we construct the matrix whose size is  with the sampled values.
However, the above method requires similar memory space to  in the conventional word embeddings.
Thus, we introduce a more memory-efficient way herein.


Our filter construction approach does not prepare the filter vector for each word explicitly.
Instead, we construct a filter vector by combining multiple vectors, as shown in Figure~\ref{fig:mask_construction}.
In the first step, we prepare  source matrices such as codebooks, each of which is .
Then, we assign one column vector of each matrix to each word randomly.
Thus, each word is tied to  (column) vectors.
In this step, since the probability of collision between two combinations is much small ( based on the birthday problem), each word is probably assigned to the unique set of  vectors.
Moreover, the required memory space, i.e.,  is smaller than  when we use .


To construct the filter vector , we pick assigned column vectors from each matrix, and compute the sum of the vectors.
Then, we apply a function  to the result.
Formally, let  be column vectors assigned to , we compute the following equation: 

In this paper, we use two types of filter vectors: a binary mask and a real number vector based on the following distribution and function.


\paragraph{Binary Mask}
A binary mask is a binary vector whose elements are  or , such as the dropout mask~\cite{JMLR:v15:srivastava14a}.
Thus, we ignore some elements of  based on the binary mask for each word.
For the binary mask construction, we use the Bernoulli distribution.
To make the binary mask containing  with probability , we construct the source matrices with sampling from the .


Moreover, we use the following \footnote{Equation (\ref{eq:clipping}) represents the operation for a scalar value. If an input is a vector such as Equation (\ref{eq:mask_construction}), we apply Equation (\ref{eq:clipping}) to all elements in the input vector.} as the function  to trim  or more to  in the filter vectors.

In other words, in binary mask construction,  is computed from the element-wise logical OR operation over all .


\paragraph{Real Number Vector}
In addition to the binary mask, we use the filter vector, which consists of real numbers.
We use the Gaussian distribution to construct source matrices, and the identity transformation as the function .
In other words, we use the sum of vectors from source matrices without any transformation as the filter vector.


\begin{figure}[!t]
  \centering
  \includegraphics[width=11cm]{./mask_construction.pdf}
   \caption{Construction of the filter vector . We pick several vectors corresponding to the word  from source matrices and combine them to construct .}
   \label{fig:mask_construction}
\end{figure}


\subsection{Feed-Forward Network}
We obtain the unique vector to each word by computing the element-wise product of  and .
However, in this situation, words share the same value in several elements.
Thus, we increase the expressiveness by applying a feed-forward network  to the result of :

where  and  are weight matrices.
In short, the feed-forward network in this paper consists of two linear transformations with a ReLU activation function.
We use the output of  as the word embedding for .


\subsection{Discussion on the Number of Parameters and Memory Footprint}

\begin{table}[!t]
  \centering
  \caption{Memory space required by each method for word representation. ALONE (naive) represents the case where we prepare filter vectors explicitly.\label{tab:param_num}}
  \begin{tabular}{ l c } \hline
   \multicolumn{1}{c}{Method} & Memory usage \\ \hline
  Conventional way &  \\
  ALONE (naive)    &  \\
  ALONE (proposed) &  \\
  ALONE (proposed (volatile)) &  \\ \hline
  \end{tabular}
\end{table}

Table \ref{tab:param_num} summarizes the memory space required by each method.
In this table, ALONE (naive) ignores the filter construction approach introduced in Section \ref{sec:mask_construction}.
As described previously, the conventional way requires a large amount of memory space because  is exceptionally large () in most cases.
In contrast, ALONE (proposed) drastically reduces the number of parameters due to , and thus, we can reduce the memory footprint when we adopt the introduced filter construction way.
Moreover, since ALONE uses random initialized vectors as filter vectors without any training, we can reconstruct them again and again if we store a random seed.
Thus, we can ignore the memory space for filter vectors such as ALONE (proposed (volatile)).


As an example, consider word embeddings on WMT 2014 English-to-German translation in the setting of the original Transformer~\cite{NIPS2017_7181}.
In this setting, the conventional way requires 19M as the memory footprint due to  and .
In contrast, ALONE compresses the parameter size to 4M, which is less than a quarter of that footprint, when we set  and .
These values are used in the following experiment on machine translation.
For filter vectors, the naive filter construction way requires an additional 19M for the memory footprint, but our introduced approach requires only 262k more when we set  and .
These values are also used in our experiments.
Thus, the proposed ALONE reduces the memory footprint as compared to that of the conventional word embeddings.


\section{Experiments}
In this section, we investigate whether the proposed method, ALONE, can be an alternation of the conventional word embeddings.
We first conduct an experiment on the reconstruction of pre-trained word embeddings to investigate whether ALONE is capable of mimicking the conventional word embeddings.
Then, we conduct experiments on real applications: machine translation and summarization.
We train the Transformer, which is the current state-of-the-art neural encoder-decoder model, combined with the ALONE in an end-to-end manner.


\subsection{Word Embedding Reconstruction}\label{sec:word_reconstruction}
In this experiment, we investigate whether ALONE has a similar expressiveness to the conventional word embeddings.
We used the pre-trained  dimensional GloVe\footnote{\href{https://nlp.stanford.edu/projects/glove/}{https://nlp.stanford.edu/projects/glove/}}~\cite{pennington2014glove} as source word embeddings and reconstructed them with ALONE.


\paragraph{Training details:}
The training objective is to mimic the GloVe embeddings with ALONE.
Let  be the conventional word embedding (GloVe in this experiment), we minimize the following objective function:


We optimized the above objective function with Adam~\cite{DBLP:journals/corr/KingmaB14} whose hyper-parameters are default settings in PyTorch~\cite{NEURIPS2019_9015}.
We set mini-batch size  and the number of epochs .
We constructed each mini-batch with uniformly sampling from vocabulary and regard training on the whole vocabulary as one epoch.
For , , and  in the binary mask, we set 64, 8, and 0.5 respectively\footnote{We tried several values, but we cannot observe any significant differences among the results.}.
We used the same dimension size as GloVe () for  and conducted experiments with varying  in .
In each setting, the total number of parameters is 0.4M, 0.7M, 1.1M, and 1.4M.
We selected the top 5k words based on the frequency in English Wikipedia as target words\footnote{We can use the whole vocabulary of pre-trained GloVe embeddings but we restricted vocabulary size to shorten the training time.}.
We used five random seeds to initialize ALONE, and report the average of five scores.


\paragraph{Test data:}
We used the word similarity task, which is widely used to evaluate the quality of word embeddings~\cite{pennington2014glove,faruqui-etal-2015-retrofitting,wieting-etal-2016-charagram}.
The task investigates whether similarity based on trained word embeddings corresponds to the human-annotated similarity.
In this paper, we used three test sets: SimLex-999~\cite{hill-etal-2015-simlex}, WordSim-353~\cite{WordSim353:Finkelstein:WWW:2001}, and RG-65~\cite{rubenstein1965contextual}.
We computed Spearman's rank correlation () between the cosine similarities of word embeddings and the human annotations as in previous studies.


\paragraph{Results:}

\begin{figure}[!t]
  \centering
  \includegraphics[width=13.5cm]{./wordsim_result.pdf}
   \caption{Results on word embedding reconstruction. The dashed line indicates the Spearman's rank correlation coefficient of GloVe.}
   \label{fig:word_sim_result}
\end{figure}


Figure \ref{fig:word_sim_result} shows the Spearman's  of ALONE in both filter vectors.
This figure also shows the Spearman's  without training  (Fix ).
The dashed line indicates the Spearman's  of GloVe, i.e., the upper bound of word embedding reconstruction.
Figure \ref{fig:word_sim_result} indicates that ALONE achieved comparable scores to GloVe on all datasets in .
These results indicate that ALONE has the expressive power to represent the conventional word embeddings.

In the comparison between filter vectors, we cannot find the significant difference in , but the real number vectors slightly outperformed binary masks in small .
Thus, we should use real number vectors as the filter vectors if the number of parameters is strongly restricted.

Figure \ref{fig:word_sim_result} shows that the setting without training  is defeated against training  in most cases.
Therefore, it is better to train  to obtain superior representations.


\subsection{Machine Translation}\label{sec:mt}
Section \ref{sec:word_reconstruction} indicates ALONE can mimic pre-trained word embeddings, but there remain two questions:
\begin{enumerate}
  \item Can we train ALONE in an end-to-end manner?
  \item In the realistic situation, can ALONE reduce the number of parameters related to word embeddings while maintaining performance?
\end{enumerate}
To answer these questions, we conduct experiments on machine translation and summarization.


\paragraph{Training details:}
In machine translation, we used ALONE as word embeddings instead of the conventional embedding matrix  in the Transformer~\cite{NIPS2017_7181}.
We adopted the base model setting and thus shared embeddings with the pre-softmax linear transformation matrix.
We used the fairseq implementation~\cite{ott-etal-2019-fairseq} and followed the training procedure described in its documentation~\footnote{\href{https://github.com/pytorch/fairseq/tree/master/examples/scaling_nmt}{https://github.com/pytorch/fairseq/tree/master/examples/scaling\_nmt}}.
We set  the same number as the dimension of each layer in the Transformer (, i.e., ) and varied .
For other hyper-parameters, we set as follows: , , and .
Moreover, we applied the dropout after the ReLU activation function in Equation (\ref{eq:ffn}).

\paragraph{Dataset:}
We used WMT En-De dataset since it is widely used to evaluate the performance of machine translation~\cite{pmlr-v70-gehring17a,NIPS2017_7181,ott-etal-2018-scaling}.
Following previous studies~\cite{NIPS2017_7181,ott-etal-2018-scaling}, we used WMT 2016 training data, which contains 4.5M sentence pairs, newstest2013, newstest2014 for training, validation, and test respectively.
We constructed a vocabulary with the byte pair encoding~\cite{sennrich-etal-2016-neural} whose merge operation is 32K in sharing source and target.
We measured case-sensitive BLEU with SacreBLEU~\cite{post-2018-call}.


\paragraph{Result:}
\begin{table}[!t]
  \centering
  \caption{Results of WMT En-De translation.\label{tab:translation}}
  \begin{tabular}{| l | r | r | r |} \hline
  Method &  & Embed & BLEU \\ \hline
  \multicolumn{2}{|l|}{ConvS2S~\cite{pmlr-v70-gehring17a}} & 66.0M & 25.2 \ \ \\
  \multicolumn{2}{|l|}{Transformer~\cite{NIPS2017_7181}} & 16.8M & 27.3 \ \  \\
  \multicolumn{2}{|l|}{Transformer+DeFINE~\cite{factorizationInput}} & - & 27.01 \\ \hline
  \multicolumn{2}{|l|}{Transformer (conventional word embeddings)} & 16.8M & 27.12 \\
  \multicolumn{2}{|l|}{Transformer (factorized embed)} & 4.3M & 26.43 \\
  \multicolumn{2}{|l|}{Transformer (factorized embed)} & 8.5M & 26.56 \\ \hline
  Transformer+ALONE (Binary)      & 4096 & 4.2M & 26.97 \\
  Transformer+ALONE (Real number) & 4096 & 4.2M & 26.93 \\
  Transformer+ALONE (Binary)      & 8192 & 8.4M &\textbf{27.55} \\
  Transformer+ALONE (Real number) & 8192 & 8.4M &\textbf{27.61} \\ \hline
  \multicolumn{4}{|l|}{ALONE without training } \\ \hline
  Transformer+ALONE (Binary)      & 4096 & 4.2M & 26.75 \\
  Transformer+ALONE (Real number) & 4096 & 4.2M & 26.85 \\
  Transformer+ALONE (Binary)      & 8192 & 8.4M & 26.90 \\
  Transformer+ALONE (Real number) & 8192 & 8.4M & 26.95 \\ \hline  
  \end{tabular}
\end{table}



Table \ref{tab:translation} shows BLEU scores of the Transformer with ALONE in the case .
This table also shows BLEU scores of previous studies~\cite{pmlr-v70-gehring17a,NIPS2017_7181,factorizationInput} and the Transformer trained in our environment with the same hyper-parameters as the original one~\cite{NIPS2017_7181}\footnote{The number of parameters for embeddings in the Transformer is different from that in the original one~\cite{NIPS2017_7181} owing to the difference of vocabulary size.}.
DeFINE~\cite{factorizationInput} uses a factorization approach, which constructs embeddings from small matrices instead of one large embedding matrix, to reduce the number of parameters\footnote{Table \ref{tab:translation} shows the reported score. We cannot demonstrate the embedding parameter size for Transformer+DeFINE because its vocabulary size is unreported but the parameter size of Transformer+DeFINE is 68M, which is larger than that of the original Transformer (60.9M). We consider that the original Transformer (and our experiments) save the total parameter size by sharing embeddings with the pre-softmax linear transformation matrix.}.
In addition, we trained the Transformer with a simple factorization approach as a baseline.
In the simple factorization approach, we construct word embeddings from one small embedding matrix and weight matrix to expand a small embedding to one with the larger dimensionality .
We modified the dimension size to make the number of parameters related to embeddings almost equal to those in ALONE.


Table \ref{tab:translation} shows the Transformer with ALONE in  outperformed other methods even though the embedding parameter size is half that of the Transformer with the conventional embeddings.
This result indicates that our ALONE can reduce the number of parameters related to embeddings without negatively affecting the performance on machine translation.


In comparison with the factorized embedding approaches, Transformer with ALONE () achieved superior BLEU scores as compared to Transformer+DeFINE~\cite{factorizationInput}, while the total parameter size of Transformer with ALONE, 52.5M, is smaller than it (68M).
In other words, Transformer+ALONE achieved better performance even though it had a disadvantage in the parameter size.
Moreover, Transformer+ALONE outperformed Transformer (factorized embed) despite the number of parameters related to embeddings in both being almost equal.
These results imply that ALONE is superior to a simple approach using a small embedding matrix and expansion with a weight matrix.

Table \ref{tab:translation} also shows BLEU scores of the Transformer with ALONE in the case without training the base embedding .
The setting without training  didn't achieve BLEU scores as high as training .
Therefore, we also have to train  in the training ALONE with an end-to-end manner.


\subsection{Summarization}\label{sec:summarization}
\paragraph{Training details:}
We also conduct an experiment on the headline generation task, which is one of the abstractive summarization tasks~\cite{rush-chopra-weston:2015:EMNLP,takase-okazaki-2019-positional}.
The purpose of this task is to generate a headline of a given document with the desired length.
Thus, we introduced ALONE into the Transformer with the length control method~\cite{takase-okazaki-2019-positional}.
For other model details, we used the same as the experiment on machine translation.
We used the publicly available implementation\footnote{https://github.com/takase/control-length} and followed their training procedure.
As the length control method, we used the combination of LRPE and PE~\cite{takase-okazaki-2019-positional}.
Moreover, we re-ranked decoded candidates based on the content words following the previous study~\cite{takase-okazaki-2019-positional}.


\paragraph{Dataset:}
As in previous studies~\cite{rush-chopra-weston:2015:EMNLP,takase-okazaki-2019-positional}, we used pairs of the first sentence and headline extracted from the annotated English Gigaword with the same pre-processing script provided by \citet{rush-chopra-weston:2015:EMNLP}\footnote{https://github.com/facebookarchive/NAMAS} as the training data.
The training set contains about 3.8M pairs.
For the source-side, we used the byte pair encoding~\cite{sennrich-etal-2016-neural} to construct a vocabulary.
We set the hyper-parameter to fit the vocabulary size 16K.
For the target-side, we used a set of characters as a vocabulary to control the number of output characters.
We shared the source-side and target-side vocabulary.

We used the DUC 2004 task 1~\cite{Over:2007:DC:1284916.1285157} as the test set.
Following the evaluation protocol~\cite{Over:2007:DC:1284916.1285157}, we truncated the characters of the output headline to 75 bytes and computed the recall-based ROUGE score.

\paragraph{Result:}
\begin{table}[!t]
  \centering
  \caption{Results on DUC 2004 task 1. The scores in bold are superior to the previous top score.\label{tab:headline}}
  \begin{tabular}{| l | r | r | r r r |} \hline
  Method &  & Embed & \multicolumn{1}{|c}{R-1} & \multicolumn{1}{c}{R-2} & \multicolumn{1}{c|}{R-L} \\ \hline
  \multicolumn{2}{|l|}{ABS~\cite{rush-chopra-weston:2015:EMNLP}} & 42.1M & 28.18 & 8.49 & 23.81 \\
  \multicolumn{2}{|l|}{LSTM EncDec+WFE~\cite{suzuki-nagata-2017-cutting}} & 37.7M & 32.28 & 10.54 & 27.80 \\
  \multicolumn{2}{|l|}{Transformer+LRPE+PE~\cite{takase-okazaki-2019-positional}} & 8.3M & 32.29 & 11.49 & 28.03 \\ \hline
  +ALONE (Binary)      & 512 & 0.5M & 31.60 & 11.12 & 27.25 \\
  +ALONE (Real number) & 512 & 0.5M & 31.96 & \textbf{11.50} & 27.74 \\
  +ALONE (Binary)      & 1024 & 1.0M & \textbf{32.51} & 11.48 & \textbf{28.08} \\
  +ALONE (Real number) & 1024 & 1.0M & \textbf{32.57} & \textbf{11.63} & \textbf{28.24} \\ \hline
  \end{tabular}
\end{table}

\begin{figure}
      \subfigure[Machine Translation]{\includegraphics[clip, width=0.5\columnwidth]{./mt_valid_nll_curve.pdf}}\subfigure[Summarization]{\includegraphics[clip, width=0.5\columnwidth]{./headline_valid_nll_curve.pdf}}\caption{Validation loss of each method.}
    \label{fig:valid_curve}
\end{figure}


Table \ref{tab:headline} shows the ROUGE scores of the previous best method (Transformer+LRPE+PE~\cite{takase-okazaki-2019-positional}) with ALONE in the case \footnote{In the abstractive summarization task, the vocabulary size is much smaller than that in the machine translation experiment because we used characters for the target-side vocabulary, and the source-side vocabulary size is also small. Thus, we set a smaller .}.
This table indicates that ALONE (Real number) achieved the comparable ROUGE-2 score to the previous best in .
In addition, Transformer+LRPE+PE+ALONE in  outperformed the previous best score except for ROUGE-2 in ALONE (Binary) despite the embedding parameter size being one-eighth of that of the original Transformer+LRPE+PE.


Figure \ref{fig:valid_curve} shows loss (negative log likelihood) values on validation sets of machine translation and summarization (newstest2013 and a set sampled from English Gigaword, respectively) for each word embedding method.
This figure indicates that the convergence of the Transformer with ALONE is slower than the conventional embeddings.
In particular, the convergence of Binary (512) is much slower than other methods in summarization.
We consider this slow convergence harmed ROUGE scores of ALONE (Binary) in DUC 2004.
Meanwhile, the Transformer+ALONE in , which outperformed the original Transformer in machine translation, achieved superior validation loss values as compared to the conventional embeddings.

In conclusion, based on the machine translation and summarization results, the answers for the previously mentioned two questions are as follows; 1. Yes, we can train ALONE with a neural method from scratch. 2. Yes, our ALONE can reduce the parameter size for embeddings without sacrificing performance.


\section{Related Work}
Researchers have proposed several strategies to compress neural networks such as pruning~\cite{NIPS1989_250,NIPS2015_5784,zhang-etal-2017-towards}, knowledge distillation~\cite{44873,kim-rush-2016-sequence}, and quantization~\cite{DBLP:journals/corr/GongLYB14,Hashednet,Shu:coding,pmlr-v80-chen18g,Suzuki:Compress:Embed,NIPS2017_7078}.
\citet{NIPS2015_5784} proposed iterative pruning, which consists of the following three steps: (1) train a neural network to find important connections, (2) prune the unimportant connections, and (3) re-train the neural network to tune the remaining connections.
\citet{zhang-etal-2017-towards} iteratively performed steps (2) and (3) to compress a neural machine translation model.

Knowledge distillation approaches train a small network to mimic a pre-trained network by minimizing the difference between the outputs of the small network and pre-trained original network~\cite{44873,kim-rush-2016-sequence}.
\citet{kim-rush-2016-sequence} applied knowledge distillation to a neural machine translation model and obtained a smaller model that achieves comparable scores to the original network.

These approaches require additional computational costs to acquire a compressed model because they need to train a base network before compression.
In contrast, our proposed ALONE does not require a pre-trained network because we can train it with an end-to-end manner in the same way as the conventional word embeddings.
In addition, we can also apply the above approaches to compress ALONE because the approaches are orthogonal to it.

\citet{Hashednet} proposed HashedNet, which constructs the weight matrix from a few parameters.
HashedNet decides the assignment of trainable parameters to an element of the weight matrix based on a hash function.
Some studies applied such a parameter assignment approach to word embeddings~\cite{Suzuki:Compress:Embed,Shu:coding,pmlr-v80-chen18g}.
For example, \citet{Suzuki:Compress:Embed} constructs word embeddings with the concatenation of several sub-vectors (trainable parameters).
Their method optimizes the parameters and assignments of sub-vectors through training.
While those methods can represent each word with small parameters after training, they require additional parameters during the training phase.
In fact, \citet{Suzuki:Compress:Embed} uses conventional word embeddings during training.
In contrast, such additional parameters are unnecessary in ALONE.

\citet{NIPS2017_7078} proposed the method to assign each word to several vectors using a hash function.
Their method also has no additional parameters during training, but it requires weight vectors for each word.
Thus, its parameter size depends on the vocabulary size .
In contrast, since the parameters of ALONE are independent from , ALONE can represent each word with fewer parameters.

To reduce the number of parameters related to word embeddings, some studies have reduced the vocabulary size ~\cite{Kim:2016:CNL:3016100.3016285,wieting-etal-2016-charagram,Takase:chara:ngram}.
In these days, it is common to construct the vocabulary with sub-words~\cite{sennrich-etal-2016-neural,kudo-2018-subword} for generation tasks such as machine translation.
Furthermore, some studies use characters (or character n-grams) as their vocabulary and construct word embeddings from character embeddings~\cite{Kim:2016:CNL:3016100.3016285,wieting-etal-2016-charagram,Takase:chara:ngram}.
This study does not conflict with these studies because ALONE is used to represent the word, sub-word, and character embeddings in our experiments.



\section{Conclusion}
This paper proposes ALONE, a novel method to reduce the number of parameters related to word embeddings.
ALONE constructs embeddings for each word from one embedding in contrast to the conventional way that prepares a large embedding matrix whose size depends on the vocabulary size .
Through experiments, we indicated that ALONE can represent each word while maintaining the similarity of pre-trained word embeddings.
In addition, we can train ALONE in an end-to-end manner on real NLP applications.
We combined ALONE with the strong neural encoder-decoder method, Transformer~\cite{NIPS2017_7181}, and achieved comparable scores on WMT 2014 English-to-German translation and DUC 2004 very short summarization with fewer parameters.

\section*{Broader Impact}
This study addresses the reduction of trainable parameters for word embeddings.
Word embeddings are fundamental component of various neural network-based NLP methods because we need them to convert a symbolic input into vector representations.
Thus, the proposed method, ALONE, has potential to reduce the parameter size of existing neural network-based NLP methods.
In this paper, we combined ALONE with a neural encoder-decoder model but we expect that it also has a positive effect on other methods such as large-scale neural language models.

\begin{ack}
We thank Hiroshi Noji, Hitomi Yanaka, Koki Washio, and Saku Sugawara for constructive discussions.
We thank anonymous reviewers for their useful suggestions.
This work was supported by JSPS KAKENHI Grant Number JP18K18119.
The first author is supported by Microsoft Research Asia (MSRA) Collaborative Research Program.
\end{ack}


\bibliographystyle{plainnat}
\bibliography{neurips_2020.bbl}

\end{document}
