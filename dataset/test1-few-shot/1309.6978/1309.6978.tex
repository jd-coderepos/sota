\documentclass[oribibl, 11pt]{llncs}
\usepackage{fullpage}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{color}
\usepackage{hyperref}

\usepackage{fancyhdr}
\setlength{\headheight}{15.2pt}
\pagestyle{fancy}

\fancyhf{} 
\renewcommand{\headrulewidth}{0pt}
\cfoot{\thepage}

\setcounter{secnumdepth}{3}

\makeatletter
\def\blfootnote{\xdef\@thefnmark{}\@footnotetext}
\makeatother

\newcommand{\removed}[1]{}

\newcommand{\rem}{\mathbf}

\newcommand{\cm}{\mathcal{M}}
\newcommand{\cn}{\mathcal{N}}
\newcommand{\co}{\mathcal{O}}
\newcommand{\ce}{\mathcal{E}}
\newcommand{\ch}{\mathcal{H}}
\newcommand{\cs}{\mathcal{S}}
\newcommand{\cu}{\mathcal{U}}
\newcommand{\cp}{\mathcal{P}}
\newcommand{\ca}{\mathcal{A}}
\newcommand{\cb}{\mathcal{B}}
\newcommand{\cc}{\mathcal{C}}
\newcommand{\cd}{\mathcal{D}}
\newcommand{\cg}{\mathcal{G}}
\newcommand{\cx}{\mathcal{X}}
\newcommand{\cl}{\mathcal{L}}
\newcommand{\cj}{\mathcal{J}}
\newcommand{\calt}{\mathcal{T}}
\newcommand{\calr}{\mathcal{R}}

\newcommand{\ra}{\rightarrow}
\newcommand{\rsa}{\rightsquigarrow}
\newcommand{\la}{\longrightarrow}
\newcommand{\lra}{\Leftrightarrow}
\newcommand{\coleq}{\mathrel{\mathop:}=}
\newcommand{\bs}{\backslash}
\newcommand{\past}{\mathrm{past}}
\newcommand{\psum}{\mathrm{psum}}
\newcommand{\future}{\mathrm{future}}
\newcommand{\fsum}{\mathrm{fsum}}

\newcommand{\dprime}{{\prime\prime}}
\newcommand{\tprime}{{\prime\prime\prime}}

\newcommand{\gfs}{\cg_{star}}
\newcommand{\gfc}{\cg_{com}}
\newcommand{\gfu}{\cg_{unr}}
\newcommand{\srarrow}{\stackrel{*}\rightarrow}
\newcommand{\lan}{\langle}
\newcommand{\ran}{\rangle}

\DeclareMathOperator{\lcm}{\textup{lcm}}

\renewcommand{\P}{\textup{P}}
\newcommand{\E}{\textup{E}}
\newcommand{\Var}{\textup{Var}}

\newcommand{\tsf}{\textsf}
\newcommand{\ap}{\text{\emph{all paths}}}
\newcommand{\shp}{\text{\emph{shortest paths}}}
\newcommand{\reach}{reach}
\newcommand{\ALG}{\text{ALG}}
\newcommand{\OPT}{\text{OPT}}

\sloppy

\newtheorem{observation}{Observation}

\title{Simple and Efficient Local Codes for Distributed Stable Network Construction\thanks{Supported in part by the project ``Foundations of Dynamic Distributed Computing Systems'' (\textsf{FOCUS}) which is implemented under the ``ARISTEIA'' Action of the  Operational Programme ``Education and Lifelong Learning'' and is co-funded by the European Union (European Social Fund) and Greek National Resources.}}

\author{Othon Michail\inst{1} \and Paul G. Spirakis\inst{1,2}}
\institute{Computer Technology Institute \& Press ``Diophantus'' (CTI), Patras, Greece \and Department of Computer Science, University of Liverpool, UK\\
Email:\email{ michailo@cti.gr, P.Spirakis@liverpool.ac.uk}}

\begin{document}

\maketitle

\begin{abstract}
In this work, we study protocols (i.e. distributed algorithms) so that populations of distributed processes can \emph{construct networks}. In order to highlight the basic principles of distributed network construction we keep the model minimal in all respects. In particular, we assume \emph{finite-state processes} that all begin from the same initial state and all execute the same protocol (i.e. the system is \emph{homogeneous}). Moreover, we assume \emph{pairwise interactions} between the processes that are scheduled by an adversary. The only constraint on the \emph{adversary scheduler} is that it must be \emph{fair}, intuitively meaning that it must assign to every reachable configuration of the system a non-zero probability to occur. In order to allow processes to construct networks, we let them \emph{activate} and \emph{deactivate} their pairwise connections. When two processes interact, the protocol takes as input the states of the processes and the state of their connection and updates all of them. In particular, in every interaction, the protocol may activate an inactive connection, deactivate an active one, or leave the state of a connection unchanged. Initially all connections are inactive and the goal is for the processes, after interacting and activating/deactivating connections for a while, to end up with a desired \emph{stable network} (i.e. one that does not change any more). We give protocols (optimal in some cases) and lower bounds for several basic network construction problems such as \emph{spanning line}, \emph{spanning ring}, \emph{spanning star}, and \emph{regular network}. We provide proofs of correctness for all of our protocols and analyze the \emph{expected time to convergence} of most of them under a \emph{uniform random scheduler} that selects the next pair of interacting processes uniformly at random from all such pairs. Finally, we prove several \emph{universality} results by presenting generic protocols that are capable of simulating a Turing Machine (TM) and exploiting it in order to construct a large class of networks. Our universality protocols use a subset of the population (\emph{waste}) in order to distributedly construct there a TM able to decide a graph class in some given space. Then, the protocols repeatedly construct in the rest of the population (\emph{useful space}) a graph equiprobably drawn from all possible graphs. The TM works on this and accepts if the presented graph is in the class. We additionally show how to partition the population into  \emph{supernodes}, each being a line of  nodes, for the largest such . This amount of local memory is sufficient for the supernodes to obtain unique names and exploit their names and their memory to realize nontrivial constructions. Delicate composition and reinitialization issues have to be solved for these general constructions to work.
\end{abstract}

\noindent
\textbf{Keywords:} distributed network construction, stabilization, homogeneous population, distributed protocol, interacting automata, fairness, random schedule, structure formation, self-organization

\section{Introduction}
\label{sec:intro}

\subsection{Motivation}

Suppose a set of tiny computational devices (possibly at the nanoscale) is injected into a human circulatory system for the purpose of monitoring or even treating a disease. The devices are incapable of controlling their mobility. The mobility of the devices, and consequently the interactions between them, stems solely from the dynamicity of the environment, the blood flow inside the circulatory system in this case. Additionally, each device alone is incapable of performing any useful computation, as the small scale of the device highly constrains its computational capabilities. The goal is for the devices to accomplish their task via cooperation. To this end, the devices are equipped with a mechanism that allows them to create bonds with other devices (mimicking nature's ability to do so). So, whenever two devices come sufficiently close to each other and interact, apart from updating their local states, they may also become connected by establishing a physical connection between them. Moreover, two connected devices may at some point choose to drop their connection. In this manner, the devices can organize themselves into a desired global structure. This network-constructing self-assembly capability allows the artificial population of devices to evolve greater complexity, better storage capacity, and to adapt and optimize its performance to the needs of the specific task to be accomplished.

\subsection{Our Approach}

In this work, we study the fundamental problem of \emph{network construction} by a distributed computing system. The system consists of a set of processes that are capable of performing local computation (via pairwise interactions) and of forming and deleting connections between them. Connections between processes can be either \emph{physical} or \emph{virtual} depending on the application. In the most general case, a connection between two processes can be in one of a finite number of possible states. For example, state 0 could mean that the connection does not exist while state , for some finite , that the connection exists and has strength . We consider here the simplest case, which we call the \emph{on/off} case, in which, at any time, a connection can either exist or not exist, that is there are just two states for the connections. If a connection exists we also say that it is \emph{active} and if it does not exist we say that it is \emph{inactive}. Initially all connections are inactive and the goal is for the processes, after interacting and activating/deactivating connections for a while, to end up with a desired \emph{stable network}. In the simplest case, the output-network is the one induced by the active connections and it is stable when no connection changes state any more.

Our aim in this work is to initiate this study by proposing and studying a very \emph{simple}, yet sufficiently generic, model for distributed network construction. To this end, we assume the computationally weakest type of processes. In particular, the processes are finite automata that all begin from the same initial state and all execute the same finite program which is stored in their memory (i.e. the system is \emph{homogeneous}). The communication model that we consider is also very minimal. In particular, we consider processes that are inhabitants of an \emph{adversarial environment} that has total control over the inter-process interactions. We model such an environment by an adversary scheduler that operates in discrete steps selecting in every step a pair of processes which then interact according to the common program. This represents very well systems of (not necessarily computational) entities that interact in pairs whenever two of them come sufficiently close to each other. When two processes interact, the program takes as input the states of the interacting processes and the state of their connection and outputs a new state for each process and a new state for the connection. The only restriction that we impose on the scheduler in order to study the constructive power of the model is that it is \emph{fair}, by which we mean the weak requirement that, at every step, it assigns to every reachable configuration of the system a non-zero probability to occur. In other words, a fair scheduler cannot forever conceal an always reachable configuration of the system. Note that such a generic scheduler gives no information about the running time of our constructors. Thus, to estimate the efficiency of our solutions we assume a \emph{uniform random scheduler}, one of the simplest fair probabilistic schedulers. The uniform random scheduler selects in every step independently and uniformly at random a pair of processes to interact from all such pairs. What renders this model interesting is its ability to achieve complex global behavior via a set of notably simple, uniform (i.e. with codes that are independent of the size of the system), homogeneous, and cooperative entities.

We now give a simple illustration of the above. Assume a set of  very weak processes that can only be in one of two states, ``black'' or ``red''. Initially, all processes are black. We can think of the processes as small particles that move randomly in a fair solution. The particles are capable of forming and deleting physical connections between them, by which we mean that, whenever two particles interact, they can read and write the state of their connection. Moreover, for simplicity of the model, we assume that fairness of the solution is independent of the states of the connections. This is in contrast to schedulers that would take into account the geometry of the active connections and would, for example, forbid two non-neighboring particles of the same component to interact with each other. In particular, we assume that throughout the execution every pair of processes may be selected for interaction. Consider now the following simple problem. We want to identically program the initially disorganized particles so that they become self-organized into a \emph{spanning star}. In particular, we want to end up with a unique black particle  connected (via active connections) to  red particles and all other connections (between red particles) being inactive. Equivalently, given a (possibly physical) system that tends to form a spanning star we would like to unveil the code behind this behavior. Consider the following program. When two black particles that are not connected interact, they become connected and one of them becomes red. When two connected red particles interact they become disconnected (i.e. reds repel). Finally, when a black and a red that are not connected interact they become connected (i.e. blacks and reds attract). The protocol forms a spanning star as follows. As whenever two blacks interact only one survives and the other becomes red, eventually a unique black will remain and all other particles will be red (we say ``eventually'', meaning ``in finite time'', because we do not know how much time it will take for all blacks to meet each other but from fairness we know that this has to occur in a finite number of steps). As blacks and reds attract while reds repel, it is clear that eventually the unique black will be connected to all reds while every pair of reds will be disconnected. Moreover, no rule of the program can modify such a configuration thus the constructed spanning star is stable (see Figure \ref{fig:global-star}). It is worth noting that this very simple protocol is optimal both w.r.t. to the number of states that it uses and w.r.t. to the time it takes to construct a stable spanning star under the uniform random scheduler.

\begin{figure}[!hbtp]
   \centering{
        \subfigure[]{
        \includegraphics[width=0.27\textwidth]{figures/global-star1.pdf}
        \label{fig:gs1}}
	\hspace{1cm}
        \subfigure[]{
        \includegraphics[width=0.28\textwidth]{figures/global-star2-bg.pdf}
        \label{fig:gs2}}
	\hspace{1cm}
        \subfigure[]{
        \includegraphics[width=0.23\textwidth]{figures/global-star3-bg.pdf}
        \label{fig:gs3}}
        }
   \caption{(a) Initially all particles are black and no active connections exist. (b) After a while, only 3 black particles have survived each having a set of red neighbors (red particles appear as gray here). Note that some red particles are also connected to red particles. The tendency is for the red particles to repel red particles and attract black particles. (c) A unique black has survived, it has attracted all red particles, and all connections between red particles have been deactivated. The construction is a stable spanning star.} \label{fig:global-star}
\end{figure}

Our model for network construction is strongly inspired by the Population Protocol model \cite{AADFP06} and the Mediated Population Protocol model \cite{MCS11-2}. In the former, connections do not have states. States on the connections were first introduced in the latter. The main difference to our model is that \emph{in those models the focus was on the computation of functions of some input values and not on network construction}. Another important difference is that we allow the edges to choose between \emph{only two possible states} which was not the case in \cite{MCS11-2}. Interestingly, when operating under a uniform random scheduler, population protocols are formally equivalent to \emph{chemical reaction networks} (CRNs) which model chemistry in a \emph{well-mixed solution} \cite{Do14}. CRNs are widely used to describe information processing occurring in natural cellular regulatory networks, and with upcoming advances in synthetic biology, CRNs are a promising programming language for the design of artificial molecular control circuitry. However, CRNs and population protocols can only capture the dynamics of molecular counts and not of structure formation. Our model then may also be viewed as an extension of population protocols and CRNs aiming to capture the stable structures that may occur in a well-mixed solution. From this perspective, our goal is to determine what stable structures can result in such systems (natural or artificial), how fast, and under what conditions (e.g. by what underlying codes/reaction-rules). Most computability issues in the area of population protocols have now been resolved. Finite-state processes on a complete interaction network, i.e. one in which every pair of processes may interact, (and several variations) compute the \emph{semilinear predicates} \cite{AAER07}. Semilinearity persists up to  local space but not more than this \cite{MNPS11}.  If additionally the connections between processes can hold a state from a finite domain (note that this is a stronger requirement than the on/off that the present work assumes) then the computational power dramatically increases to the commutative subclass of  \cite{MCS11-2}. Other important works include \cite{GR09} which equipped the nodes of population protocols with unique ids and \cite{BBCK10} which introduced a (weak) notion of speed of the nodes that allowed the design of fast converging protocols with only weak requirements. For a very recent introductory text see \cite{MCS11}.

The paper essentially consists of two parts. In the first part, we give simple (i.e. small) and efficient (i.e. polynomial-time) protocols for the construction of several fundamental networks. In particular, we give protocols for spanning lines, spanning rings, cycle-covers, partitioning into cliques, and regular networks (formal definitions of all problems considered can be found in Section \ref{sec:problems}). We remark that the spanning line problem is of outstanding importance because it constitutes a basic ingredient of universal constructors. We give three different protocols for this problem each improving on the running time but using more states to this end. Additionally, we establish a  generic lower bound on the expected running time of all constructors that construct a spanning network and a  lower bound for the spanning line, where  throughout this work denotes the number of processes. Our fastest protocol for the problem runs in  expected time and uses 9 states while our simplest uses only 5 states but pays in an expected time which is between  and . In the second part, we investigate the more generic question of \emph{what is in principle constructible by our model}. We arrive there at several satisfactory characterizations establishing some sort of universality of the model. The main idea is as follows. To construct a decidable graph-language  we (i) construct on  of the processes (called the \emph{waste}) a network  capable of simulating a Turing Machine (abbreviated ``TM'' throughout the paper) and of constructing a random network on the remaining  processes (called the \emph{useful space}), (ii) use  to construct a random network  on the remaining  processes, (iii) execute on  the TM that decides  with  as input. If the TM accepts, then we output  (note that this is not a terminating step - the reason why will become clear in Section \ref{sec:gencon}; the protocol just freezes and its output forever remains ), otherwise we go back to (ii) and repeat. Using this core idea we prove several universality results for our model. Additionally, we show how to organize the population into a distributed system with names and logarithmic local memories.

In Section \ref{sec:rw}, we discuss further related literature. Section \ref{sec:prel} brings together all definitions and basic facts that are used throughout the paper. In particular, in Section \ref{sec:model} we formally define the model of network constructors, Section \ref{sec:problems} formally defines all network construction problems that are considered in this work, and in Section \ref{sec:basic-processes} we identify and analyze a set of basic probabilistic processes that are recurrent in the analysis of the running times of network constructors. In Section \ref{sec:global-line}, we study the spanning line problem. In Section \ref{sec:basic-con}, we provide direct constructors for all the other basic network construction problems. Section \ref{sec:gencon} presents our \emph{universality} results. Finally, in Section \ref{sec:conclusions} we conclude and give further research directions that are opened by our work.

\section{Further Related Work}
\label{sec:rw}

\noindent\textbf{Algorithmic Self-Assembly.} There are already several models trying to capture the self-assembly capability of natural processes with the purpose of engineering systems and developing algorithms inspired by such processes. For example, \cite{Do12} proposes to learn how to program molecules to manipulate themselves, grow into machines and at the same time control their own growth. The research area of ``algorithmic self-assembly'' belongs to the field of ``molecular computing''. The latter was initiated by Adleman \cite{Ad94}, who designed interacting DNA molecules to solve an instance of the Hamiltonian path problem. The model guiding the study in algorithmic self-assembly is the Abstract Tile Assembly Model (aTAM) \cite{Wi98,RW00} and variations. In contrast to those models that try to incorporate the exact molecular mechanisms (like e.g. temperature, energy, and bounded degree), we propose a very abstract combinatorial rule-based model, free of specific application-driven assumptions, with the aim of revealing the fundamental laws governing the distributed (algorithmic) generation of networks. Our model may serve as a common substructure to more applied models (like assembly models or models with geometry restrictions) that may be obtained from our model by imposing restrictions on the scheduler, the degree, and the number of local states (see Section \ref{sec:conclusions} for several interesting variations of our model).\\

\noindent\textbf{Distributed Network Construction.} To the best of our knowledge, classical distributed computing has not considered the problem of constructing an actual communication network from scratch. From the seminal work of Angluin \cite{An80} that initiated the theoretical study of distributed computing systems up to now, the focus has been more on assuming a given communication topology and constructing a virtual network over it, e.g. a spanning tree for the purpose of fast dissemination of information. Moreover, these models assume most of the time unique identities, unbounded memories, and message-passing communication. Additionally, a process always communicates with its neighboring processes (see \cite{Ly96} for all the details). An exception is the area of geometric pattern formation by mobile robots (cf. \cite{SY99,DFSY10} and references therein). A great difference, though, to our model is that in mobile robotics the computational entities have complete control over their mobility and thus over their future interactions. That is, the goal of a protocol is to result in a desired interaction pattern while in our model the goal of a protocol is to construct a network while operating under a totally unpredictable interaction pattern. Very recently, a model inspired by the behavior of ameba that allows algorithmic research on self-organizing particle systems was proposed \cite{DGRS13}. The goal is for the particles to self-organize in order to adapt to a desired shape without any central control, which is quite similar to our objective, however the two models seem two have little in common. In the same work, the authors observe that, in contrast to the considerable work that has been performed w.r.t. to systems (e.g. self-reconfigurable robotic systems), only very little theoretical work has been done in this area. This further supports the importance of introducing a simple yet sufficiently generic model for distributed network construction, as we do in this work.\\

\noindent\textbf{Cellular Automata.} A cellular automaton (cf. e.g. \cite{Sc11}) consists of a grid of cells each cell being a finite automaton. A cell updates its own state by reading the states of its neighboring cells (e.g. 2 in the 1-dimensional case and 4 in the 2-dimensional case). All cells may perform the updates in discrete synchronous steps or updates may occur asynchronously. Cellular automata have been used as models for self-replication, for modeling several physical systems (e.g. neural activity, bacterial growth, pattern formation in nature), and for understanding emergence, complexity, and self-organization issues. Though there are some similarities there are also significant differences between our model and cellular automata. One is that in our model the interaction pattern is nondeterministic as it depends on the scheduler and a process may interact with any other process of the system and not just with some predefined neighbors. Moreover, our model has a direct capability of forming networks whereas cellular automata can form networks only indirectly (an edge between two cells  and  has to be represented as a line of cells beginning at , ending at  and all cells on the line being in a special edge-state). In fact, cellular automata are more suitable for studying the formation of patterns on e.g. a discrete surface of static cells while our model is more suitable for studying how a totally dynamic (e.g. mobile) and initially disordered collection of entities can self-organize into a network.\\

\noindent\textbf{Social Networks.} There is a great amount of work dealing with networks formed by a group of interacting individuals. Individuals, also called players, which may, for example, be people, animals, or companies, depending on the application, usually have incentives and connections between individuals indicate some social relationship, like for example friendship. The network is formed by allowing the individuals to form or delete connections usually selfishly by trying to maximize their own utility. The usual goal there is to study how the whole network affects the outcome of a specific interaction, to predict the network that will be formed by a set of selfish individuals, and to characterize the quality of the network formed (e.g. its efficiency). See e.g. \cite{Ja05,BEK13}. This is a game-theoretic setting which is very different from the setting considered here as the latter does not include incentives and utilities. Another important line of research considers random social networks in which new links are formed according to some probability distribution. For example, in \cite{BA99} it was shown that growth and preferential attachment that characterize a great majority of social networks (like, for example, the Internet) results in scale-free properties that are not predicted by the Erd\"{o}s-R\'{e}nyi random graph model \cite{ER59,Bo01}. Though, in principle, we allow processes to perform a coin tossing during an interaction, our focus is not on the formation of a random network but on cooperative (algorithmic) construction according to a common set of rules. In summary, our model looks more like a standard dynamic distributed computing system in which the interacting entities are computing processes that all execute the same program.\\

\noindent\textbf{Network Formation in Nature.} Nature has an intrinsic ability to form complex structures and networks via a process known as \emph{self-assembly}. By self-assembly, small components (like e.g. molecules) automatically assemble into large, and usually complex structures (like e.g. a crystal). There is an abundance of such examples in the physical world. Lipid molecules form a cell's membrane, ribosomal proteins and RNA coalesce into functional ribosomes, and bacteriophage virus proteins self-assemble a capsid that allows the virus to invade bacteria \cite{Do12}. Mixtures of RNA fragments that self-assemble into self-replicating ribozymes spontaneously form cooperative catalytic cycles and networks. Such cooperative networks grow faster than selfish autocatalytic cycles indicating an intrinsic ability of RNA populations to evolve greater complexity through cooperation \cite{VMC12}. Through billions of years of prebiotic molecular selection and evolution, nature has produced a basic set of molecules. By combining these simple elements, natural processes are capable of fashioning an enormously diverse range of fabrication units, which can further self-organize into refined structures, materials and molecular machines that not only have high precision, flexibility and error-correction capacity, but are also self-sustaining and evolving. In fact, nature shows a strong preference for bottom-up design.

Systems and solutions inspired by nature have often turned out to be extremely practical and efficient. For example, the bottom-up approach of nature inspires the fabrication of biomaterials by attempting to mimic these phenomena with the aim of creating new and varied structures with novel utilities well beyond the gifts of nature \cite{Zh03}. Moreover, there is already a remarkable amount of work envisioning our future ability to engineer computing and robotic systems by manipulating molecules with nanoscale precision. Ambitious long-term applications include molecular computers \cite{BPSPF10} and miniature (nano)robots for surgical instrumentation, diagnosis and drug delivery in medical applications (e.g. it has very recently been reported that DNA nanorobots could even kill cancer cells \cite{DBC12}) and monitoring in extreme conditions (e.g. in toxic environments). However, the road towards this vision passes first through our ability to discover \emph{the laws governing the capability of distributed systems to construct networks}. The gain of developing such a theory will be twofold: It will give some insight to the role (and the mechanisms) of network formation in the complexity of natural processes and it will allow us engineer artificial systems that achieve this complexity.

\section{Preliminaries}
\label{sec:prel}

\subsection{A Model of Network Constructors}
\label{sec:model}

\begin{definition}
A \emph{Network Constructor} (NET) is a distributed protocol defined by a 4-tuple , where
 is a finite set of \emph{node-states},  is the \emph{initial node-state},  is the set of \emph{output node-states}, and  is the \emph{transition function}.
\end{definition}

If , we call  a \emph{transition} (or \emph{rule}) and we define , , and . A transition  is called \emph{effective} if  for at least one  and \emph{ineffective} otherwise. When we present the transition function of a protocol we only present the effective transitions. Additionally, we agree that the \emph{size} of a protocol is the number of its states, i.e. .

The system consists of a population  of  distributed \emph{processes} (also called \emph{nodes} when clear from context). In the generic case, there is an underlying \emph{interaction graph}  specifying the permissible interactions between the nodes. Interactions in this model are always pairwise. In this work,  is a \emph{complete undirected interaction graph}, i.e. , where . Initially, all nodes in  are in the initial node-state . 

A central assumption of the model is that edges have binary states. An edge in state 0 is said to be \emph{inactive} while an edge in state 1 is said to be \emph{active}. All edges are initially inactive.

Execution of the protocol proceeds in discrete steps. In every step, a pair of nodes  from  is selected by an \emph{adversary scheduler} and these nodes interact and update their states and the state of the edge joining them according to the transition function . In particular, we assume that, for all distinct node-states  and for all edge-states ,  specifies either  or . So, if , , and  are the states of nodes , , and edge , respectively, then the unique rule corresponding to these states, let it be , is applied, the edge that was in state  updates its state to  and if , then  updates its state to  and  updates its state to , if  and , then both nodes update their states to , and if  and , then the node that gets  is drawn equiprobably from the two interacting nodes and the other node gets . 

A \emph{configuration} is a mapping  specifying the state of each node and each edge of the interaction graph. Let  and  be configurations, and let ,  be distinct nodes. We say that \emph{ goes to  via encounter }, denoted , if      . We say that \emph{ is reachable in one step from }, denoted , if  for some encounter . We say that  is \emph{reachable} from  and write , if there is a sequence of configurations , such that  for all , .

An \emph{execution} is a finite or infinite sequence of configurations  , where  is an initial configuration and , for all . A \emph{fairness condition} is imposed on the adversary to ensure the protocol makes progress. An infinite execution is \emph{fair} if for every pair of configurations  and  such that , if  occurs infinitely often in the execution then so does . In what follows, every execution of a NET will by definition considered to be fair.

We define the \emph{output of a configuration}  as the graph  where  and  , and . In words, the output-graph of a configuration consists of those nodes that are in output states and those edges between them that are active, i.e. the active subgraph induced by the nodes that are in output states. The output of an execution  is said to \emph{stabilize} (or \emph{converge}) to a graph  if there exists some step  s.t.  for all , i.e. from step  and onwards the output-graph remains unchanged. Every such configuration , for , is called \emph{output-stable}. The \emph{running time} (or \emph{time to convergence}) of an execution is defined as the minimum such  (or  if no such  exists). Throughout the paper, whenever we study the running time of a NET, we assume that interactions are chosen by a \emph{uniform random scheduler} which, in every step, selects independently and uniformly at random one of the  possible interactions. In this case, the running time becomes a random variable (abbreviated ``r.v.'')  and our goal is to obtain bounds on the expectation  of . Note that the uniform random scheduler is fair with probability 1.

\begin{definition}
We say that an execution of a NET on  processes \emph{constructs a graph} (or \emph{network}) , if its output stabilizes to a graph isomorphic to .
\end{definition}

\begin{definition}
We say that a NET  \emph{constructs a graph language  with useful space }, if  is the greatest function for which: (i) for all , every execution of  on  processes constructs a  of order at least  (provided that such a  exists) and, additionally, (ii) for all  there is an execution of  on  processes, for some  satisfying , that constructs . Equivalently, we say that \emph{ constructs  with waste }.
\end{definition}

\begin{definition}
Define  to be the class of all graph languages that are constructible with useful space  by a NET. We call  the \emph{relation} or \emph{on/off} class. 
\end{definition}

Also define  in precisely the same way as  but in the extension of the above model in which every pair of processes is capable of tossing an unbiased coin during an interaction between them. In particular, in the weakest probabilistic version of the model, we allow transitions that with probability  give one outcome and with probability  another. Additionally, we require that all graphs have the same probability to be constructed by the protocol.

We denote by  (for ``Deterministic Graph Space'') the class of all graph languages that are decidable by a TM of (\emph{binary}) space , where  is the length of the adjacency matrix encoding of the input graph.

\subsection{Problem Definitions}
\label{sec:problems}

We here provide formal definitions of all the network construction problems that are considered in this work. Protocols and bounds for these problems are presented in Sections \ref{sec:global-line} and \ref{sec:basic-con}.\\

\noindent\textbf{Global line.} The goal is for the  distributed processes to construct a spanning line, i.e. a connected graph in which 2 nodes have degree 1 and  nodes have degree 2.\\

\noindent\textbf{Cycle cover.} Every process in  must eventually have degree 2. The result is a collection of node-disjoint cycles spanning .\\

\noindent\textbf{Global star.} The processes must construct a spanning star, i.e. a connected graph in which 1 node, called the \emph{center}, has degree  and  nodes, called the \emph{peripheral nodes}, have degree 1.\\

\noindent\textbf{Global ring.} The processes must construct a spanning ring, i.e. a connected graph in which every node has degree 2.\\

\noindent\textbf{-regular connected.} The generalization of global ring in which every node has degree  (note that  is a constant and a protocol for the problem must run correctly on any number  of processes).\\

\noindent\textbf{-cliques.} The processes must partition themselves into  cliques of order  each (again  is a constant).\\

\noindent\textbf{Replication.} The protocol is given an input graph  on a subset  of the processes (). The processes in  are initially in state  and the edges of  are the active edges between them. All other edges in  are initially inactive. The processes in  are initially in state . The goal is to create a \emph{replica} of  on , provided that . Formally, we want, in every execution, the output induced by the active edges between the nodes of  to stabilize to a graph isomorphic to .

\subsection{Basic Probabilistic Processes}
\label{sec:basic-processes}
 
We now present a set of very fundamental probabilistic processes that are recurrent in the analysis of the running times of network constructors. All these processes assume a uniform random scheduler and are applications of the standard coupon collector problem. In most of these processes, we ignore the states of the edges and focus only on the dynamics of the node-states, that is we consider rules of the form . Throughout this section, we call a step a \emph{success} if an effective rule applies on the interacting nodes and we denote by  the r.v. of the running time of the processes.\\

\noindent\textbf{One-way epidemic.} Consider the protocol in which the only effective transition is . Initially, there is a single  and  s and we want to estimate the expected number of steps until all nodes become s.

\begin{proposition} \label{pro:one-way-ep}
The expected time to convergence of a one-way epidemic (under the uniform random scheduler) is .
\end{proposition}
\begin{proof}
Let  be a r.v. defined to be the number of steps until all  nodes are in state . Call a step a success if an effective rule applies and a new  appears on some node. Divide the steps of the protocol into \emph{epochs}, where epoch  begins with the step following the st success and ends with the step at which the th success occurs. Let also the r.v. ,  be the number of steps in the -th epoch. Let  be the probability of success at any step during the -th epoch. We have , where  denotes the total number of possible interactions and . By linearity of expectation we have 

\qed
\end{proof}
where  denotes the th Harmonic number.\\

\noindent\textbf{One-to-one elimination.} All nodes are initially in state . The only effective transition of the protocol is . We are now interested in the expected time until a single  remains. We call the process one-to-one elimination because s are only eliminated with themselves. A straightforward application is in protocols that elect a unique leader by beginning with all nodes in the leader state and eliminating a leader whenever two leaders interact.

\begin{proposition} \label{pro:one-to-one}
The expected time to convergence of a one-to-one elimination is .
\end{proposition}
\begin{proof}
Epoch  begins with the step following the th success and ends with the step at which the st success occurs. The probability of success during the th epoch, for , is  and

The above uses the fact that  is bounded from above by . This holds because .

Now, for the lower bound, observe that the last two s need on average  steps to meet each other. As , we conclude that .
\qed
\end{proof}

A slight variation of the one-to-one elimination protocol constructs a \emph{maximum matching}, i.e. a matching of cardinality  (which is a perfect matching in case  is even). The variation is  and the running time of a one-to-one elimination, i.e. , is an upper bound on this variation. For the lower bound, notice that when only two (or three) s remain the expected number of steps for a success is  (, respectively), that is the running time is also . We conclude that there is a protocol that constructs a maximum matching in an expected number of  steps.\\

\noindent\textbf{One-to-all elimination.} All nodes are initially in state . The effective rules of the protocol are  and . We are now interested in the expected time until no  remains. The process is called one-to-all elimination because s are eliminated not only when they interact with s but also when they interact with s. At a first sight, it seems to run faster than a one-way epidemic as s still propagate towards s as in a one-way epidemic but now s are also created when two s interact. We show that this is not the case.

\begin{proposition} \label{pro:one-to-all}
The expected time to convergence of a one-to-all elimination is .
\end{proposition}
\begin{proof} 
The probability of success during the th epoch, for , is  and


For the upper bound, we have 


For the lower bound, we have

We conclude that .
\qed
\end{proof}

\noindent\textbf{Meet everybody.} A single node  is initially in state  and all other nodes are in state . The only effective transition is . We study the time until all s become s which is equal to the time needed for  to interact with every other node.

\begin{proposition} \label{pro:meet-ever}
The expected time to convergence of a meet everybody is .
\end{proposition}
\begin{proof}
Assume that in every step  participates in an interaction. Then  must collect the  coupons which are  different nodes that it must interact with. Clearly, in every step, every node has the same probability to interact with , i.e. , and this is the classical coupon collector problem that takes average time . But on average  needs  steps to participate in an interaction, thus the total time is . 
\qed
\end{proof} 

\noindent\textbf{Node cover.} All nodes are initially in state . The only effective transitions are , . We are interested in the number of steps until all nodes become s, i.e. the time needed for every node to interact at least once.

\begin{proposition} \label{pro:node-cover}
The expected time to convergence of a node cover is .
\end{proposition}
\begin{proof}
For the upper bound, simply observe that the running time of a one-to-all elimination, i.e. , is an upper bound on the running time of a node cover. The reason is that a node cover is a one-to-all elimination in which in some cases we may get two new bs by one effective transition (namely ) while in one-to-all elimination all effective transitions result in at most one new .

For the lower bound, if  is the number of s then the probability of success is . Observe now that a node cover process is slower than the artificial variation in which whenever rule  applies we pick another  and make it a . This is because, given  s, this artificial process has the same probability of success as a node cover but additionally in every success the artificial process is guaranteed to produce two new s while a node cover may in some cases produce only one new . Define . Then, taking into account what we already proved in the lower bound of one-to-all elimination (see Proposition \ref{pro:one-to-all}), we have

We conclude that . 
\qed
\end{proof}

\noindent\textbf{Edge cover.} All nodes are in state  throughout the execution of the protocol. The only effective transition is  (we now focus on edge-state updates), i.e. whenever an edge is found inactive it is activated (recall that initially all edges are inactive). We study the number of steps until all edges in  become activated, which is equal to the time needed for all possible interactions to occur.

\begin{proposition} \label{pro:edge-cover}
The expected time to convergence of an edge cover is .
\end{proposition}
\begin{proof}
Given that  and given that  successes (i.e.  distinct interactions) have occurred the corresponding probability for the coupon collector argument is  and the expected number of steps is . Another way to see this is to observe that it is a classical coupon collector problem with  coupons each selected in every step with probability , thus .
\qed
\end{proof}

Table \ref{tab:basic-proc-full} summarizes the expected time to convergence of each of the above fundamental probabilistic processes.

\begin{table}[h]
\normalsize
\setlength{\tabcolsep}{15pt}
\begin{center}
\begin{tabular}{  l  c  }
  \hline
  Protocol & Expected Time \\ \hline
  \emph{One-way epidemic} &  \\ 
  \emph{One-to-one elimination} &  \\ 
  \emph{One-to-all elimination} &  \\ 
  \emph{Meet everybody} &  \\
  \emph{Node Cover} &  \\ 
  \emph{Edge cover} &  \\ \hline
\end{tabular}
\end{center}
\caption{Our results for the expected time to convergence of several fundamental probabilistic processes.} \label{tab:basic-proc-full}
\end{table}

\section{Constructing a Global Line}
\label{sec:global-line}

In this section, we study probably the most fundamental network-construction problem, which is the problem of constructing a spanning line. Its importance lies in the fact that a spanning line provides an ordering on the processes which can then be exploited (as shown in Section \ref{sec:gencon}) to simulate a TM and thus to establish universality of our model. We give three different protocols for the spanning line problem each improving on the running time but using more states to this end.

We begin with a generic lower bound holding for all protocols that construct a spanning network.  

\begin{theorem} [Generic Lower Bound]
The expected time to convergence of any protocol that constructs a spanning network, i.e. one in which every node has at least one active edge incident to it, is . Moreover, this is the best lower bound for general spanning networks that we can hope for, as there is a protocol that constructs a spanning network in  expected time.
\end{theorem}
\begin{proof}
Consider the time at which the last edge is activated. Clearly, by that time, all nodes must have some active edge incident to them which implies that every node must have interacted at least once. Thus the running time is lower bounded by a node cover, which by Proposition \ref{pro:node-cover} takes an expected number of  steps.

Now consider the variation of node cover which in every transition that is effective w.r.t. node-states additionally activates the corresponding edge. In particular, the protocol consists of the rules  and . Clearly, when every node has interacted at least once, or equivalently when all s have become s, every node has an active edge incident to it, and thus the resulting stable network is spanning. The reason is that all nodes are s in the beginning, every node at some point is converted to , and every such conversion results in an activation of the corresponding edge. As a node-cover completes in  steps, the above protocol takes  steps to construct a spanning network.
\qed
\end{proof}

We now give an improved lower bound for the particular case of constructing a spanning line.

\begin{theorem} [Line Lower Bound]
The expected time to convergence of any protocol that constructs a spanning line is .
\end{theorem}
\begin{proof}
Take any protocol  that constructs a spanning line and any execution of  on  nodes. Consider the step  at which  performed the last modification of an edge. Observe that the construction after step  must be a spanning line. We distinguish two cases. 

(i) The last modification was an activation. In this case, the construction just before step  was either a line on  nodes and an isolated node or two disjoint lines spanning all nodes. To see this, observe that these are the only constructions that can be turned into a line by a single additional activation. In the first case, the probability of obtaining an interaction between the isolated node and one of the endpoints of the line is  and in the second the probability of obtaining an interaction between an endpoint of one line and an endpoint of the other line is . In both cases, the expected number of steps until the last edge becomes activated is . 

(ii) The last modification was a deactivation. This implies that the construction just before step  was a spanning line with an additional active edge between two nodes,  and , that are not neighbors on the line. If one of these nodes, say , is an internal node, then  has degree 3 and we can only obtain a line by deactivating one of the edges incident to . Clearly, the probability of getting one of these edges is  and it is even smaller if both nodes are internal. Thus, if at least one of  and  is internal, the expected number of steps is . It remains to consider the case in which the construction just before step  was a spanning ring, i.e. the case in which  and  are the endpoints of the spanning line. In this case, consider the step  of the last modification of an edge that resulted in the ring. To this end notice that all nodes of a ring have degree 2. If  was an activation then exactly two nodes had degree 1 and if  was a deactivation then two nodes had degree 3. In both cases, there is a single interaction that results in a ring, the probability of success is  and the expectation is again . 
\qed
\end{proof}

We proceed by presenting protocols for the spanning line problem.

\subsection{1st Protocol}
\label{subsec:simple-global-line}

We present now our simplest protocol for the spanning line problem.

\floatname{algorithm}{Protocol}
\renewcommand{\algorithmiccomment}[1]{// #1}
\begin{algorithm}[!h]
  \caption{\emph{Simple-Global-Line}}\label{prot:gline}
  \begin{algorithmic}
    \medskip
    \State 
    \State : 
    
    \State \Comment {All transitions that do not appear have no effect}
  \end{algorithmic}
\end{algorithm}

\noindent\emph{Protocol Simple-Global-Line:} , , , , 

\begin{theorem} \label{the:gline}
Protocol \emph{Simple-Global-Line} constructs a spanning line. It uses 5 states and its expected running time is  and .
\end{theorem}
\begin{proof}
We begin by proving that, for any number of processes , the protocol correctly constructs a spanning line under any fair scheduler. Then we study the running time of the protocol under the uniform random scheduler.

\emph{Correctness.} In the initial configuration , all nodes are in state  and all edges are inactive, i.e in state 0. Every configuration  that is reachable from  consists of a collection of active lines and isolated nodes. Additionally, every active line has a unique leader which either occupies an endpoint and is in state  or occupies an internal node, is in state , and moves along the line. Whenever the leader lies on an endpoint of its line, its state is  and whenever it lies on an internal node, its state is . Lines can expand towards isolated nodes and two lines can connect their endpoints to get merged into a single line (with total length equal to the sum of the lengths of the merged lines plus one). Both of these operations only take place when the corresponding endpoint of every line that takes part in the operation is in state . 

We have to prove two things: (i) there is a set  of output-stable configurations whose active network is a spanning line, (ii) for every reachable configuration  (i.e. ) it holds that  for some . For (i), consider a spanning line, in which the non-leader endpoints are in state , the non-leader internal nodes in , and there is a unique leader either in state  if it occupies an endpoint or in state  if it occupies an internal node. For (ii), note that any reachable configuration  is a collection of active lines with unique leaders and isolated nodes. We present a (finite) sequence of transitions that converts  to a . If there are isolated nodes, take any line and if its leader is internal make it reach one of the endpoints by selecting the appropriate interactions. Then successively apply the rule  to expand the line towards all isolated nodes. Thus we may now w.l.o.g. consider a collection of lines without isolated nodes. By successively applying the rule  to pairs of lines while always moving the internal leaders that appear towards an endpoint it is not hard to see that the process results in an output-stable configuration from , i.e. one whose active network is a spanning line. 

\emph{Running Time Upper Bound.} For the running time upper bound, we have an expected number of  steps until another progress is made (i.e. for another merging to occur given that at least two -leaders exist) and  steps for the resulting random walk (walk of state  until it reaches one endpoint of the line) to finish and to have again the system ready for progress.  follows because we have a random walk on a line with two absorbing barriers (see e.g. \cite{Fe68} pages 348-349) delayed on average by a factor of . As progress must be made  times, we conclude that the expected running time of the protocol is bounded from above by .

We next prove that we cannot hope to improve the upper bound on the expected running time by a better analysis by more than a factor of . For this we first prove that the protocol w.h.p. constructs  different lines of length 1 during its course. A set of  disjoint lines implies that  distinct merging processes have to be executed in order to merge them all in a common line and each single merging results in the execution of another random walk. We exploit all these to prove the desired  lower bound.

Recall that initially all nodes are in . Every interaction between two -nodes constructs another line of length 1. Call the random interaction of step  a \emph{success} if both participants are in . Let  be the r.v. of the number of nodes in state ; i.e. initially . Note that, at every step,  decreases by at most 2, which happens only in a success (it may also remain unchanged, or decrease by 1 if a leader expands towards a ). Let the r.v.  be the number of successes up to step  and  be the total number of successes throughout the course of the protocol (e.g. until no further successes are possible or until stabilization). Our goal is to calculate the expectation of  as this is equal to the number of distinct lines of length 1 that the protocol is expected to form throughout its execution (note that these lines do not necessarily have to coexist). Given , the probability of success at the current step is . As long as  it holds that . Moreover, as  decreases by at most 2 in every step, there are at least  steps until  becomes less or equal to . Thus, our process \emph{dominates} a Bernoulli process  with  trials and probability of success  in each trial. For this process we have .

We now exploit the following Chernoff bound (cf. \cite{MR95}, page 70) establishing that w.h.p.  does not deviate much below its mean :\\
\emph{
Chernoff Bound. Let  be independent Poisson trials such that, for , , where . Then, for , , and ,}


Additionally, it holds that . Thus  implies    


So, for all ,

and as  dominates , we have . In words, w.h.p. we expect  lines of length 1 to be constructed by the protocol.

Now, given that , we distinguish two cases: (i) At some point during the course of the protocol two lines both of length  get merged. In this case, the corresponding random walk takes on average  transitions involving the leader and on average the leader is selected every  steps to interact with one of the 2 active edges incident to it. That is, the expected number of steps for the completion of such a random walk is  and the expected running time of the protocol is  (ii) In every merging process, at least one of the two participating lines has length at most . We have already shown that the protocol w.h.p. forms  distinct lines of length 1. Consider now the interval . As  for all , only a single line can ever have length  and one, call it , will necessarily fall in this interval due to the fact that the length of  will increase by at most  in every merging until it becomes . Consider now the time at which  has length . As the total length due to lines of length 1 (ever to appear) is  and the length of  is  there is still a remaining length of at least  to be merged to . As the maximum length of any line different than  is ,  will get merged to the  remaining length via at least  distinct mergings with lines of length at most . These mergings, and thus also the resulting random walks, cannot occur in parallel as all of them share  as a common participant (and a line can only participate in one merging at a time). Let  denote the length of the -th line merged to , for . If  has length  just before the -th merging, then the expected duration of the resulting random walk is  and the new  resulting from merging will have length . Let  denote the duration of all random walks, and , , the duration of the -th random walk. In total, the expected duration of all random walks resulting from the  mergings of  is

The fifth equality follows from the fact that . We conclude that the expected running time of the protocol is also in this case .

Now if we define the r.v.  to be the total running time of the protocol (until convergence), by the law of total probability and for every constant , we have that:

Thus, the expected running time of the protocol is .
\qed
\end{proof}

\subsection{2nd Protocol}
\label{subsec:gline2}

The random walk approach followed in Protocol \ref{prot:gline} takes time, thus a straightforward attempt for improvement is to replace the random walk merging process with some more ``deterministic'' merging. In Protocol \ref{prot:gline2}, the random walk rules 3-5 of Protocol \ref{prot:gline} have been replaced by a more ``deterministic'' procedure.  

\floatname{algorithm}{Protocol}
\renewcommand{\algorithmiccomment}[1]{// #1}
\begin{algorithm}[!h]
  \caption{\emph{Intermediate-Global-Line}}\label{prot:gline2}
  \begin{algorithmic}
    \medskip
    \State 
    \State : 
    
  \end{algorithmic}
\end{algorithm}

\begin{theorem} \label{the:gline2}
Protocol \emph{Intermediate-Global-Line} constructs a spanning line. It uses 8 states and its expected running time under the uniform random scheduler is  and .
\end{theorem}
\begin{proof}
The proof idea is precisely the same as that of Theorem \ref{the:gline}. The only difference is that now merging two lines of lengths  and  takes time  (asymptotically) instead of the  of the random walk. Thus, for the upper bound we need  mergings each taking an average of  to complete in the worst case.  holds because  steps are performed by the merging process in the worst-case and the process must wait an average of  until its leader is selected to interact over one of its active edges. Thus the dominating factor is now .

For the lower bound we again have w.h.p.  lines of length 1 and for cases (i), (ii) as above we have: (i) merging two lines both of length  takes time . (ii)

\qed
\end{proof}

\subsection{3rd Protocol}
\label{subsec:third-line}

We now give our fastest protocol (Protocol \ref{prot:gline3}) for the global line construction. The main difference between this and the previous protocols is that we now totally avoid mergings as they seem to consume much time. As shown above, even a merging in which a linear number of steps must be performed needs  time as every step takes an average of  time. Then a linear number of mergings naturally require an average of  time, which is quite big. The intuition behind the following improvement is that when two disjoint lines interact, instead of merging, the corresponding leaders play a game in which only one survives. The winner grows by one towards the other line and the loser sleeps. A sleeping line cannot increase any more and only loses nodes by lines that are still awake. A single leader is guaranteed to always win and this occurs quite fast. Then the leader makes progress (by one) in most interactions and every such progress is in turn quite fast.

\floatname{algorithm}{Protocol}
\renewcommand{\algorithmiccomment}[1]{// #1}
\begin{algorithm}[!h]
  \caption{\emph{Fast-Global-Line}}\label{prot:gline3}
  \begin{algorithmic}
    \medskip
    \State 
    \State : 
    
  \end{algorithmic}
\end{algorithm}

\begin{theorem} \label{the:gline3}
Protocol \emph{Fast-Global-Line} constructs a spanning line. It uses 9 states and its expected running time under the uniform random scheduler is .
\end{theorem}
\begin{proof}
Observe first that in  steps all s become something else. To see this let  be the r.v. of the total number of steps until all s disappear and let  be the r.v. of the number of steps between the th and the st interaction between two nodes in state  (assume no other interactions can change the state of a ). Let  be the probability that such an interaction occurs. Then  and . The last equation follows from the fact that , i.e. it is bounded. Finally, observe that s that become leaders can also turn other s to something else thus the actual expectation is in fact  (i.e. what we have ignored can only help the process end faster).

Now notice that after this  time we have a set of at most  leaders and no new leader can ever appear. Moreover, in every interaction between two leaders only one survives and the other becomes a follower. Clearly, a single leader must win all the pairwise games in which it will participate. Consider that leader and observe that it takes it an average of  steps to participate to another game in the worst case and another  steps to win it. Clearly, in  steps on average there is a unique leader and every other node is either isolated in state  or part of a line that has a unique follower . Every interaction of a leader with a follower increases the length of the leader's line by 1 in  steps. Thus an increment occurs every  steps as the leader needs  steps to meet a follower and then  steps to increase by 1 towards that follower. As the leader needs to make at most  increments to make its own line global, we conclude that the expected time for this to occur is .
\qed
\end{proof}

\section{Other Basic Constructors}
\label{sec:basic-con}

\noindent\textbf{Cycle Cover}

\floatname{algorithm}{Protocol}
\renewcommand{\algorithmiccomment}[1]{// #1}
\begin{algorithm}[!h]
  \caption{\emph{Cycle-Cover}}\label{prot:cycle-cover}
  \begin{algorithmic}
    \medskip
    \State 
    \State : 
    
  \end{algorithmic}
\end{algorithm}

\begin{theorem} \label{the:cycle-cover}
Protocol \emph{Cycle-Cover} constructs a cycle cover with waste 2 (i.e. a cycle cover on a subset of  of  nodes). It uses 3 states, its expected running time under the uniform random scheduler is , and it is optimal w.r.t. time.
\end{theorem}
\begin{proof}
Note that the protocol stabilizes when all nodes have become . In  time all s have become  and in another  steps (by dominating a one-to-one elimination) all s have become s. For the lower bound consider the last edge modification that ever occurs. Due to the symmetry of cycle cover, both if it was an activation or a deactivation only a single edge satisfies the fact that after its activation or deactivation we get a cycle cover, which requires  rounds (this is a lower bound for any protocol that constructs a cycle cover). That the waste is 2 follows from the fact that some executions may construct a cycle cover on  nodes and leave the remaining two nodes connected and in state  forever.
\qed
\end{proof}

\noindent\textbf{Global Star}

\begin{theorem} [Star Lower Bound]
Any protocol that constructs a spanning star has at least 2 states and its expected time to convergence is .
\end{theorem}
\begin{proof}
Clearly, with a single state we cannot make the necessary distinction of a center and a peripheral node. More formally, if there is a single state  then  must necessarily activate the edge (otherwise no edges will be ever activated) which implies that eventually all edges will become activated, i.e. instead of a star we will end up with a global clique. So every protocol that constructs a global star must have at least 2 states.

For the lower bound on the expected running time we argue as follows. Take any execution of a protocol that constructs a global star. Consider the node  that will become the center in that execution. When the execution stabilizes,  must be connected to every other node by an active edge. This implies that  must have interacted to every other node. Clearly, the time it takes for the eventually unique center,  in this case, to meet every other node is a lower bound on the total running time. This is a meet everybody that, as proved in Proposition \ref{pro:meet-ever}, takes  time.   
\qed
\end{proof}


\floatname{algorithm}{Protocol}
\renewcommand{\algorithmiccomment}[1]{// #1}
\begin{algorithm}[!h]
  \caption{\emph{Global-Star}}\label{prot:gstar}
  \begin{algorithmic}
    \medskip
    \State , 
    \State : 
    
  \end{algorithmic}
\end{algorithm}

\begin{theorem}
Protocol \emph{Global-Star} constructs a spanning star. It uses 2 states and its expected running time under the uniform random scheduler is , that is it is optimal both w.r.t. size and time.
\end{theorem}
\begin{proof}
\emph{Correctness.} Each node may play one of the following two roles during an execution of the protocol: a \emph{center} (state ) or a \emph{peripheral} (state ). The unique output-stable configuration  whose active network is a spanning star, has one center and  peripheral nodes, and a  edge is active iff one of  is the center. Initially all nodes are centers. When two centers interact one of them remains a center and the other becomes a peripheral. No other interactions eliminate a center, which implies that not all centers can be eliminated, and once a center becomes a peripheral it can never become a center again. Due to fairness, eventually all pairs of centers will interact and, as no new centers appear, eventually a single center will remain. Thus from some point on there is a single center and  peripheral nodes. The idea from now on is that - attract while - repel. In particular, rule  guarantees that any inactive edges joining the center to the peripherals will become activated and rule  guarantees that any active edges joining two peripherals will become deactivated. At the same time active edges between the center and the peripherals remain active and inactive edges between two peripherals remain inactive. This clearly leads to the construction of a spanning star.

\emph{Running Time.} Forget for a while the edge updates and consider the rule , which is the only effective interaction of the protocol w.r.t. the states of the nodes. We are interested in the time needed for a single  to remain. This is clearly an original application of one-to-one elimination and as proved in Proposition \ref{pro:one-to-one} it takes  time.

Notice now that once the states of the nodes have stabilized, the constructed network will for sure stabilize to a global star after all -nodes have interacted with each other in order to deactivate any active edges between them and after the  has interacted with all s in order to activate any inactive edges, i.e. after all pairs of interactions have occurred. This is an edge cover that, as proved in Proposition \ref{pro:edge-cover}, takes  time. Thus the total expected running time is at most .
\qed
\end{proof}

\noindent\textbf{Global Ring}

\begin{theorem} [Ring Lower Bound]
The expected time to convergence of any protocol that constructs a spanning ring is .
\end{theorem}
\begin{proof}
Take any protocol  that constructs a spanning ring and any execution of  on  nodes. Consider the step  at which  performed the last modification of an edge. Observe that the construction after step  must be a spanning ring. We distinguish two cases. 

(i) The last modification was an activation. It follows that the previous active network should be a spanning line . But the only activation that can convert this spanning line into a spanning ring is  which occurs with probability , i.e. in an expected number of  steps.  

(ii) The last modification was a deactivation. It follows that the previous active network should be a spanning ring  with an additional active edge  for  and  (i.e. a chord). Clearly, the only interaction that can convert such an active network into a spanning ring is  which takes an expected number of  steps to occur.
\qed
\end{proof}

\floatname{algorithm}{Protocol}
\renewcommand{\algorithmiccomment}[1]{// #1}
\begin{algorithm}[!h]
  \caption{\emph{Global-Ring}}\label{prot:gring}
  \begin{algorithmic}
    \medskip
    \State 
    \State : 
    
    \State \Comment {The first 5 rules are the same as in the \emph{Simple-Global-Line} protocol (Protocol \ref{prot:gline})} 
  \end{algorithmic}
\end{algorithm}

\begin{theorem}
Protocol \emph{Global-Ring} constructs a spanning ring.
\end{theorem}
\begin{proof}
The protocol is essentially the same as the \emph{Simple-Global-Line} protocol (Protocol \ref{prot:gline}) but additionally we allow the endpoints of a line to become connected. This occurs whenever one endpoint is in state  and the other is in state  and the two endpoints interact. In this case, rule  applies and the two endpoints become blocked. If any of the two endpoints detects the existence of another component, then, in the next interaction between them, the two endpoints backtrack, by which we mean that they deactivate the connection between them and both become unblocked again by returning to their original states. The existence of another component can be eventually detected due to the fact that every component is either an isolated node in state  or has at least one leader. Now take an arbitrary reachable configuration  with at least 2 components. We may w.l.o.g. assume that  has no blocked nodes, as if it has there is a sequence of interactions that unblocks them all. Thus, as in the \emph{Simple-Global-Line} protocol we have a collection of lines and isolated nodes. This may very well lead to the formation of a spanning line with a single leader. It is now clear that at some point the leader will occupy one endpoint of the line, will interact with the other endpoint, the spanning line will close to form a spanning ring and the previous endpoints will become blocked. As there is a single component in the network, these two nodes will remain blocked forever and therefore the constructed active ring is stable. 
\qed
\end{proof}

\noindent\textbf{Global Ring: A Generic Approach}

\floatname{algorithm}{Protocol}
\renewcommand{\algorithmiccomment}[1]{// #1}
\begin{algorithm}[!h]
  \caption{\emph{2RC}}\label{prot:2rc}
  \begin{algorithmic}
    \medskip
    \State 
    \State : 
     
  \end{algorithmic}
\end{algorithm}

\begin{theorem} \label{the:2rc}
Protocol \emph{2RC} constructs a connected spanning 2-regular network (i.e. a spanning ring).
\end{theorem}
\begin{proof}
The set  of output-stable configurations whose active network is a spanning ring consists of those configurations that have one node in state  and all other nodes in state . The index of a state indicates the number of active neighbors of a node. A first goal is for all nodes to have degree 2 which implies a cycle cover, i.e. a partitioning of the nodes into disjoint cycles. The protocol achieves this by allowing every node with degree smaller than 2 to increase its degree. The final goal is to end up with a unique spanning ring. To achieve this, the protocol allows nodes with degree 2 to drop an existing neighbor and pick a new one provided that there are at least 2 components in the network. Clearly, this implies that any closed cycle coexisting with other components, which are cycles, lines, or isolated nodes, may open to form a line. As any collection of lines and isolated nodes can always be merged to a global line and any global line can close to form a global ring, the theorem follows. 
\qed
\end{proof}

\noindent\textbf{Generalizing to -Regular Connected}

\floatname{algorithm}{Protocol}
\renewcommand{\algorithmiccomment}[1]{// #1}
\begin{algorithm}[!h]
  \caption{\emph{RC}}\label{prot:krc}
  \begin{algorithmic}
    \medskip
    \State , i.e. 
    \State : 
     
  \end{algorithmic}
\end{algorithm}

Using almost the same ideas as in the proof of Theorem \ref{the:2rc}, one can prove the following. 

\begin{theorem}
For every fixed integer  and population of size , Protocol \emph{RC} (see Protocol \ref{prot:krc}) constructs a connected spanning network in which at least  nodes have degree  and each of the remaining  nodes has degree at least  and at most .
\end{theorem}

It is interesting to point out that the number of states can be substantially reduced in some cases by relying on the computability of the target-degree . For an example, we show that we can make a node  obtain  neighbors by using only  states, for all fixed integers . Node  is initially in state  and all other nodes are in state . The protocol is , , ,  for all , . Note that  initially collects 2 neighbors (by activating edges) which go to state . Then for every  neighbor that it encounters it makes it an  and collects another neighbor which goes to state . Eventually both  neighbors will become  and there will be another  neighbors in state , so in total 4  neighbors. This process is repeated  times (the 4 s will become 8 , and so on), each time doubling the number of neighbors, thus eventually  will have obtained  neighbors. The protocol uses only  states for the indices of the s and the s and another 2 states, namely  and . Clearly, it follows that the target-degree of the nodes is not a lower bound on the size of the protocol.\\

\noindent\textbf{Many Small Components}\\

We show here how to partition the population into small active cliques. This construction is of special value as such a partitioning may serve as a means of maintaining non-interfering clusters. In particular, given such a partitioning, we can easily have a node  perform effective interactions only with nodes belonging to the same component as . This can be easily determined by the state of the connection between the interacting nodes. 

\floatname{algorithm}{Protocol}
\renewcommand{\algorithmiccomment}[1]{// #1}
\begin{algorithm}[!h]
  \caption{\emph{-Cliques}}\label{prot:cliques}
  \begin{algorithmic}
    \medskip
    \State 
    \State : 
    
  \end{algorithmic}
\end{algorithm}

\begin{theorem}
For every fixed positive integer , Protocol \emph{-Cliques} (see Protocol \ref{prot:cliques}) constructs  cliques of order  each.
\end{theorem}
\begin{proof}
The protocol first constructs  components of order , each having a unique leader (state `') directly connected to  followers (state ). Each follower then tries to become connected to the other  followers of the component. To do this, it becomes connected one after the other to  other followers. As it cannot distinguish the followers of its component from the followers of other components several of these connections may be wrong. It suffices to prove that the protocol recognizes wrong connections and deactivates them. Then, as followers always try to make their degree  when it is still less than  and as wrong connections between different components are always corrected, it follows (by fairness) that eventually each component will become a clique (having only correct connections). At that time, no new connections may be created and no existing connection can be deactivated (as they are all correct), and the correctness of the protocol follows. To recognize erroneous connections, the leader of a component constantly visits the followers of its component and checks any active connections that it may encounter during its stay (the duration of its stay is nondeterministic as it depends on the chosen interactions). If it ever encounters another leader over an active connection, then this is clearly a connection between different components and the leaders deactivate that connection and decrease the counters of the corresponding followers. Clearly, by fairness, every wrong connection will eventually be selected for interaction while having a leader in each of its endpoints. Finally, note that correct connections (between nodes of the same component) are never deactivated as at any time at most one of their endpoints may be occupied by a leader.
\qed
\end{proof}

\noindent\textbf{Replication}\\

We now study the related problem of replicating a given input graph . Let  be the set of the remaining nodes. The protocols use the nodes of  to construct a replica  of , thus it must hold that . In most cases, it suffices that . Throughout the section we assume that nodes in  are in different initial states than nodes in . We usually use  and  as the initial states of nodes in  and , respectively. Additionally,  is defined by the active edges between nodes in . We assume that  is connected.

We present a very simple protocol (Protocol \ref{prot:lreplic}) that exploits the election of a unique leader to copy  on  free nodes.

\floatname{algorithm}{Protocol}
\renewcommand{\algorithmiccomment}[1]{// #1}
\begin{algorithm}[!h]
  \caption{\emph{Leader-Replication}}\label{prot:lreplic}
  \begin{algorithmic}
    \medskip
    \State 
    \State : 
    
  \end{algorithmic}
\end{algorithm}

\begin{theorem}
Protocol \emph{Leader-Replication} constructs a copy of any connected input graph  with no waste. It uses 12 states and its expected running time under the uniform random scheduler is .
\end{theorem}
\begin{proof}
Initially, all nodes of  are in  and all nodes of  are in . The protocol first creates a matching between  and . Then it starts pairwise eliminations between leaders, that is when two leaders (nodes in state `') interact one of them survives (i.e. remains ) and the other becomes a follower (state `'). Eventually the protocol ends up with a unique leader and  followers. Moreover, when a leader and a follower meet they swap their states with probability . With the remaining  probability they become either  or  depending on whether the edge joining them was active or inactive, respectively. In both cases they mark their matched nodes from  to either activate or deactivate the edge between them in  accordingly. Once there is a unique leader, the leader moves nondeterministically over the nodes of  and again nondeterministically applies this copying process on the edges of . Thus it will eventually apply this copying process to all edges of  and as there are no conflicts with other activations/deactivations (as no other leaders exist)  eventually becomes equal to . Finally, note that the active edges of the matching between  and  are never deactivated but this is not a problem provided that  as each such edge  has  in a state from  and  in a state from , that is it is not considered as part of the output.

Now for the running time we consider three phases: the \emph{matching formation}, the \emph{leader election}, and the \emph{unique-leader replication}. The matching formation phase begins from step 1 and ends when the last  becomes , i.e. when all nodes in  have been matched to the nodes of . It is not hard to see that the probability of the th edge of the matching to be established (given  established matches) is  and the corresponding expectation is . Then similarly to the coupon collector's application in the running time of Protocol \emph{Fast-Global-Line} in Theorem \ref{the:gline3} we have that the expected running time of this phase is . An almost identical analysis yields that the expected running time of the leader election phase is also . Thus it remains to estimate the time it takes for the unique leader to copy every edge of . Given that the leader has marked the endpoints of a particular edge of  then copying and restoring the state of the leader takes on average  time (as a constant number of particular interactions must occur and each one occurs with probability ). Now we consider the time for copying as constant and try to estimate the time it takes for the leader to ``collect'' (i.e. visit and mark) all edges of . Assume also that the leader is selected in every step to interact with one of its neighbors (the truth is that it is selected every  steps on average). If  is the probability that a specific edge  is selected after two subsequent interactions then , where  is the probability that the leader interacts with and decides to move on one endpoint of  and  the probability that it then interacts with and decides to mark the other endpoint of . Let  be the r.v. of the number of steps between the th and th edge collected and  be the probability of a success in two consecutive steps of the th epoch. Clearly, , , and . Thus, provided that the leader always interacts and that every copying that it performs takes constant time, the expected time until the unique-leader replication phase ends is . Now, notice that on average it takes  steps for the leader to interact and that in half of its interactions the leader performs a copying that takes  steps to complete. That is, each of the above  steps is charged on average by  and half of them are charged by , i.e. half of the steps are charged by  and the other half are charged by . We conclude that the expected running time of the unique-leader replication phase is . This is clearly the dominating factor of the total running time of the protocol.
\qed
\end{proof}

Table \ref{tab:ulb} summarizes all upper and lower bounds that we established in Sections \ref{sec:global-line} and \ref{sec:basic-con}.

\begin{table}[h]
\normalsize
\setlength{\tabcolsep}{15pt}
\begin{center}
\begin{tabular}{  l  c  c c  }
  \hline
  Protocol & \# states & Expected Time & Lower Bound \\ \hline
  \emph{Simple-Global-Line} & 5 &  and  &  \\
  \emph{Intermediate-Global-Line} & 8 &  and  &  \\ 
  \emph{Fast-Global-Line} & 9 &  &  \\
  \emph{Cycle-Cover} & 3 &  (optimal) &  \\
  \emph{Global-Star} & 2 (optimal) &  (optimal) &  \\ 
  \emph{Global-Ring} & 9 & &  \\ 
  \emph{2RC} & 6 & &  \\ 
  \emph{RC} &  &  &  \\ 
  \emph{-Cliques} &  &  &  \\
  \emph{Leader-Replication} & 12 &  & \\ \hline
\end{tabular}
\end{center}
\caption{All upper and lower bounds established in Sections \ref{sec:global-line} and \ref{sec:basic-con}. \emph{Leader-Replication} is a randomized protocol thus it concerns class , while all other protocols do not rely on randomization thus they concern .}
\label{tab:ulb}
\end{table}

\section{Generic Constructors}
\label{sec:gencon}

In this section, we ask whether there is a generic constructor capable of constructing a large class of networks. We answer this in the affirmative by presenting (i) constructors that simulate a Turing Machine (TM) and (ii) a constructor that simulates a distributed system with names and logarithmic local memories. Denote by  the binary length of the input of a TM and by  the size of a population. Note that Theorems \ref{the:gencon-half} to \ref{the:no-waste} construct a random graph in the useful space and that graph constitutes the input to a TM. Thus, if the useful space consists of  nodes, the input to the TM has size  and, as all of our constructors have , in what follows it holds that . Moreover, the TM can use space at most , where  is the waste.

We now briefly describe the main idea behind all of our generic constructors that simulate a TM (see also Figure \ref{fig:main-loop}). Assume that we are given a decidable graph-language  and we are asked to provide a NET that constructs . The NET that we give works as follows:

\begin{enumerate}
 \item It constructs on  of the nodes a network  capable of simulating a TM and of constructing a random network on the remaining  nodes. Let  be the set of the  nodes and  the set of the remaining  nodes.  is usually a sufficiently long line or a bounded degree network as these networks can be operated as TMs. A line also serves as a measure of order as we can match a line of length  with  other nodes and by exploiting the ordering of the line we may achieve an ordering of the other nodes. 
 \item The NET exploits  to construct a random network on . The idea is to exploit the structure of  so that it can perform a random coin tossing on each edge between nodes of  exactly once. In this manner, it constructs a random network  from  on the nodes of . It is worth noting that all networks of  have an equal probability to occur and this results in an equiprobable constructor.
 \item The NET simulates on  the TM that decides  with  as its input. The only constraint is that the space used by the TM should be at most the space that the constructor can allocate in . If the TM rejects, then the protocol goes back to 2, that is it draws another random network and starts a new simulation. Otherwise, its output stabilizes to .
\end{enumerate}


\begin{figure}[!hbtp]
\centering{
\includegraphics[width=0.53\textwidth]{figures/main-loop.pdf}
}
\caption{The main mechanism used by all generic constructors in this section. The loop repeats until the TM accepts for the first time. When this occurs, the random graph  constructed belongs to  and thus the protocol may output . Note that this is not a terminating step. The protocol just does not repeat the loop and thus its output forever remains stable to .} \label{fig:main-loop}
\end{figure} 

\subsection{Linear Waste} 

\begin{theorem} [Linear Waste-Half] \label{the:gencon-half}
. In words, for every graph language  that is decidable by a -space TM, there is a protocol that constructs  equiprobably with useful space .
\end{theorem}
\begin{proof}
We give a high-level description of the protocol, call it . Let us begin by briefly presenting the main idea. Given a population of size ,  partitions the population (apart from one node when  is odd) into two equal sets  and  such that all nodes in  are in state , all nodes in  are in state  and each  is matched via an active edge to a , i.e. there is an active perfect matching between  and  (see Figure \ref{fig:gencon-overview}). By using the \emph{Simple-Global-Line} protocol (see Protocol \ref{prot:gline} in Section \ref{subsec:simple-global-line}) on the nodes of set ,  constructs a spanning line in  which has the endpoints in state , the internal nodes in state , and has additionally a unique leader on some node. We should mention that, though we use protocol Simple-Global-Line here as our reference, any protocol that constructs a spanning line would work. Given such a construction,  organizes the line into a TM. The goal is for the TM to compute a graph from  and construct it on the nodes of set . To achieve this, the TM implements a binary counter ( bits long) in its memory and uses it in order to uniquely identify the nodes of set  according to their distance from one endpoint, say e.g. the left one. Whenever it wants to modify the state of edge  of the network to be constructed, it marks by a special activating or deactivating state the -nodes at distances  and  from the left endpoint, respectively. Then an interaction between two such marked -nodes activates or deactivates, respectively, the edge between them. To compute a graph from  equiprobably, the TM performs the following random experiment. It activates or deactivates each edge of  equiprobably (i.e. each edge becomes active/inactive with probability ) and independently of the other edges. In this manner, it constructs a random graph  in  and all possible graphs have the same probability to occur. Then it simulates on input  the TM that decides  in  space to determine whether . Notice that the  space of the simulator is sufficient to decide on an input graph encoded by an adjacency matrix of  binary cells (which are the edges of ). If the TM rejects, then  and the protocol repeats the random experiment to produce a new random graph  and starts another simulation on input  this time. When the TM accepts for the first time, the constructed random network belongs to  and the protocol releases the constructed network by deactivating one after the other the active  edges and at the same time updates the state of each -node to a special  state. Finally, we should point out that, whenever the global line protocol makes progress, all edges in  are deactivated and the TM-configuration is \emph{reinitialized} to ensure that, when the final progress is made (resulting in the final line spanning ) the TM will be executed from the beginning on a correct configuration (free of residues from previous partial simulations).

\begin{figure}[!hbtp]
\centering{
\includegraphics[width=0.75\textwidth]{figures/gencon-overview.pdf}
}
\caption{The population partitioned into sets  and . The vertical active edges (solid) match the nodes of the two sets. The horizontal active edges between nodes in  form a spanning line that is used to simulate a TM. The TM will construct the desired network on the nodes of set  by activating the appropriate edges between them (dashed edges that are initially inactive).} \label{fig:gencon-overview}
\end{figure}  

We now proceed with a more detailed presentation of the various subroutines of the protocol.

\noindent\emph{Simulating the direction of the TM's head.} We begin by assuming that the spanning line has been constructed somehow (we defer for the end of the proof the actual mechanism of this construction), as in Figure \ref{fig:gencon-overview}, and that each node has three components  in its state.  is used to store the head of the TM, i.e. the actual state of the control of the TM; assume that initially the head lies on an arbitrary node, e.g. on the second one from the left as in Figure \ref{fig:gencon-overview}.  is used to store the symbol written on each cell of the TM.  is , ,  for ``left'', ``right'', and ``temporal'' respectively, or  (for ``empty'') and we assume that initially the left endpoint is , the right endpoint is , and all internal nodes are . As initially the head cannot have any sense of direction, it moves towards an arbitrary neighbor, say w.l.o.g. the right one, and leaves a  on its previous position. The  mark gives to the head a sense of direction on the line. The head can make progress towards one endpoint by just moving only towards the unmarked neighbor (avoiding the one marked by ). Once the head reaches the right endpoint for the first time, it starts moving towards the left endpoint by leaving  marks on the way. Once it reaches the left endpoint it is ready to begin working as a TM. Now every time it wants to move to the right it moves onto the neighbor that is marked by  while leaving an  mark on its previous position. Similarly, to move to the left, it moves onto the  neighbor and leaves an  mark on its previous position. In this way, no matter what the position of the head will be, there will be always  marks to its left and  marks to its right, as in Figure \ref{fig:gencon-direction}, and the head can exploit them to move correctly. Additionally, we ensure that the endpoints are in special states, e.g.  and , to ensure that the head recognizes them in order to start moving to the opposite direction.  

\begin{figure}[!hbtp]
\centering{
\includegraphics[width=0.75\textwidth]{figures/gencon-direction.pdf}
}
\caption{The main idea of using  and  marks to simulate the movement of the head of a TM. The first three snapshots present the phase of the initialization of the marks where a temporal  mark is used to move for the first time towards an endpoint. In the fourth snapshot, after the head has visited both endpoints, the  marks have been removed and all nodes to the left of the head are marked  while all nodes to the right are marked . Additionally, the endpoints have special marks to ensure that the head recognizes them.} \label{fig:gencon-direction}
\end{figure}  
 
\noindent\emph{Reading and Writing on the edges of set .} We now present the mechanism via which the TM reads or writes the state of an edge joining two -nodes. The TM uniquely identifies a -node by its distance from the left endpoint. To do this, it implements a binary counter on  cells of its memory. Whenever it wants to read (write, resp.) the state of the edge joining the -nodes  and , it sets the counter to , places a special mark on the left endpoint, and repeatedly moves the mark one position to the right while decrementing the counter by one. When the counter becomes 0, it knows that the mark is over the -th -node. Now by exploiting the corresponding active vertical edge it may assign a special mark to the -th -node (Figure \ref{fig:gencon-rw-edges} provides an illustration). By setting the counter to  and repeating the same process, another special mark may be assigned to the -th -node. Now the TM waits for an interaction to occur between the marked -nodes  and . During that interaction edge  is read (written, resp.) by the corresponding endpoints. Then, in case of a read (and similarly for a write), the TM reads the value of the edge that the endpoints detected, and in both cases unmarks both endpoints resetting them to their original states.

\begin{figure}[!hbtp]
\centering{
\includegraphics[width=0.75\textwidth]{figures/gencon-rw-edges.pdf}
}
\caption{By exploiting the implemented binary counter, the TM has managed to mark the desired nodes from set , in this case the 2nd and the 4th ones counting from left, which are now in a special ``reading'' state . An interaction between them will read the state of the edge joining them, which here happens to be an active one. Then the TM will read that value from one of these two nodes, in this case from the 2nd one. A write is implemented similarly.} \label{fig:gencon-rw-edges}
\end{figure}

\noindent\emph{Creating the input of the TM.} We now describe how the network construction works. As already stated, to simplify the description and in order to present an equiprobable constructor we have allowed nodes to toss a fair coin during their interaction. In particular, we allow transitions that with probability  give one outcome and with probability  another. Now before executing the simulation, the simulating protocol does the following. It visits one after the other the edges of set  and on each one of them performs the following random experiment: with probability  it activates the edge and with probability  it deactivates it. The result of this random process is an equiprobable construction of a random graph. In particular, all possible graphs have the same probability to occur. Note that the protocol can detect when all random experiments have been performed because it can detect the endpoints of the spanning line. For example, to visit all edges one after the other we may: (i) place two marks on the left endpoint, let  be their positions on the line, (ii) for all , perform random experiments on all  by starting the second mark from position  and moving it each time one position to the right, (iii) the process stops when  becomes , i.e. when the first mark occupies the right endpoint (which can be detected). Thus we can safely compose the process that draws the random graph to the process that simulates the TM. Once the random graph has been drawn, the protocol starts the simulation of the TM. Notice that the input to the TM is the random graph that has been drawn on the edges of  which provide an encoding equivalent to an adjacency matrix. There are  edges and the simulator has available space , which is sufficient for the simulation of a -space TM. We now distinguish two cases, one for each possible outcome of the simulation.
\begin{enumerate}
 \item The TM rejects: In this case, the constructed random graph does not belong to . The protocol repeats the random experiment, i.e. draws another random graph, and starts over the simulation on the new input.
 \item The TM accepts: The constructed graph belongs to  and the protocol enters the Releasing phase (see below).
\end{enumerate}

\noindent\emph{Releasing.} When the TM accepts for the first time, the simulating protocol updates the head to a special finalizing state . Now the head moves to the left endpoint and starts releasing one after the other the nodes of set  by deactivating the vertical edges and updating the states of the released -nodes to . Now the network constructed over the nodes of set  is free to move in the ``solution''.

It remains to resolve the following issue. In the beginning, we made the assumptions that the population has been partitioned into sets  and  and that a spanning line in  has been constructed somehow. Though it is clear that the rule  can achieve the partitioning and that the \emph{Simple-Global-Line} protocol can construct a spanning line in , it is not yet clear whether these processes can be safely composed to the simulating process. To get a feeling of the subtlety, consider the following situation. It may happen that a small subset  of the nodes has been partitioned into sets  and  and that  has been organized into a line spanning its nodes. If the nodes in  do not communicate for a while to the rest of the network, then it is possible that a graph is constructed in , which on one hand belongs to  but on the other hand its order is much smaller than the desired . To resolve this we introduce a reinitialization phase.\\

\vspace{-3pt}
\noindent\emph{Reinitialization.} A reinitialization phase is executed whenever a line on -nodes expands (either by attracting free nodes or by merging with another line). At that point, the protocol ``makes the assumption'' that no further expansions will occur, restores the components of the simulation to their original values, ensures that each node in the updated set  has a -neighbor (as it is possible that some of them have released their neighbors), and initiates the drawing of a new random graph on the new set . Though the assumption of the protocol may be wrong as long as further expansions of the line may occur, at some point the last expansion will occur and the assumption of the protocol will be correct. From that point on, the simulation will be reinitialized and executed for the last time on the correct sets  and . A final point that we should make clear is the following. During reinitialization we have two options: (i) block the line from further expansions until all components have been restored correctly and then unblock it again or (ii) leave it unblocked from the beginning. In the latter case, if another expansion occurs before completion of the previous reinitialization then another reinitialization will be triggered. However, if the two reinitialization processes ever meet then we can always kill one of them and restart a new single reinitialization process. Both options are correct and equivalent for our purposes. 
\qed
\end{proof}

We now show an interesting trade-off between the space of the simulated TM and the order of the constructed network. In particular, we prove that if the constructed network is required to occupy  instead of half of the nodes, then the available space of the TM-constructor dramatically increases to  from .

\begin{theorem} [Linear Waste-Two Thirds]
. In words, for every graph language  that is decidable by a -space TM, there is a protocol that constructs  equiprobably with useful space .
\end{theorem}
\begin{proof}
The idea is to partition the population into three equal sets , , and  instead of the two sets of Theorem \ref{the:gencon-half}. The purpose of sets  and  is more or less as in Theorem \ref{the:gencon-half}. The purpose of the additional set  is to constitute a  memory for the TM to be simulated. The goal is to exploit the  edges of set  as the binary cells of the simulated TM (see Figure \ref{fig:gencon-one-third}). The set  now, instead of executing the simulation on its own nodes, uses for that purpose the edges of set . Reading and writing on the edges of set  is performed in precisely the same way as reading/writing the edges of set  (described in Theorem \ref{the:gencon-half}).

\begin{figure}[!hbtp]
\centering{
\includegraphics[width=0.75\textwidth]{figures/gencon-one-third.pdf}
}
\caption{A partitioning into three equal sets , , and . The line of set  plays the role of an ordering that will be exploited both by the random graph drawing process and by the TM-simulation. The line of set  instead of using its  memory as the memory of the TM it now uses the  memory of set  for this purpose. Set  is again the useful space on which the output-network will be constructed. Sets  and  constitute the waste.} \label{fig:gencon-one-third}
\end{figure}

As everything works in precisely the same way as in Theorem \ref{the:gencon-half}, we only present the subroutine that constructs the  partitioning.

\noindent\emph{Constructing the  partitioning.} The rules that guarantee the desired partitioning into the three sets are:

The idea is to consider a -node as unsatisfied as long as it has not managed to obtain a  neighbor. The unsatisfied state of a -node is . If a  meets a  then it makes that  its  neighbor and becomes satisfied. Note that it is possible that at some point the population may only consist of  nodes matched to -nodes which is not a desired outcome. For this reason, we have allowed  nodes to be capable of making other  nodes their  neighbors. That is, when two  nodes interact, one of them becomes satisfied, the other becomes , and the edge joining them becomes active. A  just waits to meet its active connection to a -node, deactivates it, isolates the -node by making it  again, and becomes . For an illustration, see Figure \ref{fig:gencon-three-sets}. Then, for the construction of the line spanning , we only allow satisfied -nodes to participate to the construction. As a satisfied -node never becomes unsatisfied again, this choice is safe.

\begin{figure}[!hbtp]
\centering{
\includegraphics[width=0.85\textwidth]{figures/gencon-three-sets-hor.pdf}
}
\caption{An example construction of a  partitioning.} \label{fig:gencon-three-sets}
\end{figure}
\qed
\end{proof}

\subsection{Logarithmic Waste}

We now relax our requirement for simulation space in order to reduce the waste (which, in both of the previous two theorems, was of the order of ).

\begin{theorem} [Logarithmic Waste] \label{the:logar}
. In words, for every graph language  that is decidable in logarithmic space, there is a protocol that constructs  equiprobably with useful space .
\end{theorem}
\begin{proof}
We give the main idea. The protocol first constructs a spanning line. Let us for now assume that the spanning line has been somehow constructed by the protocol. Then the protocol exploits the line to count the number of nodes in the network. We may assume that counting is performed in the rightmost cells of the line. The head visits one after the other the nodes from left to right and for each next move it increments the binary counter by one. When the head reaches the right endpoint, counting stops and the binary counter will have occupied approximately  nodes (in fact, the rightmost  nodes). Now the protocol releases the counter without altering its line structure and additionally makes all remaining  nodes isolated by resetting their states and deactivating the edges between them. From now on, we may assume w.l.o.g. that there is a line of  nodes with a unique leader and with a distributed variable containing a very good estimate of the number of isolated nodes (for this, we just compute in the logarithmic memory , where  was already stored in binary and  is the number of cells of the memory; another way to achieve this is to stop counting when the head - moving from left to right - reaches the first, i.e. leftmost, cell occupied by the counter). All nodes of the memory are in a special  state while all remaining nodes are in some other state, e.g. , so the two sets are distinguishable. Next the leader starts a random experiment in order to construct a random graph on the free nodes as follows. It picks the first free node that it sees, call it , activates the edge between them and informs it to start tossing coins on each one of the edges joining it to other free nodes. Whenever  tosses a coin on a new edge, it marks the corresponding node to avoid it in the future and informs the leader to decrement its ()-counter by 1. When the counter becomes 0,  has tossed coins on all its edges, by a similar counting process it removes all marks from the other free nodes, and remains marked so that the leader avoids picking it again in the future. Then the leader moves to some other free node , repeating more or less the same process. At the same time the leader decrements another ()-counter by one to know when all free s have been picked. In this manner, a random graph is drawn equiprobably on the set of free nodes. Next, the leader simulates a logarithmic TM in its memory trying to decide whether the random graph belongs or not to a given language . If yes, then we are done. If not, then the TM just repeats the random experiment and restarts the simulation.\\

\vspace{-3pt}
\noindent\emph{Reinitialization.} Clearly, the protocol cannot know when the line that it was initially trying to construct has become spanning. Due to this, after every expansion of the line it assumes that the line has become spanning and starts counting. It is clear that every counting process leads to the formation of a small line with a leader (of length logarithmic in the length of the original line) and several free nodes. The small line and its leader are kept forever by the simulation process. This implies that if there are more than one such lines, they will eventually interact and detect that their original line was not spanning. At that point, the interacting lines may merge to form a new line. It is clear that the only stable case is the one in which the original line was spanning and this will eventually occur. 
\qed
\end{proof}

\subsection{No Waste}

Going one step further we prove that a large class of graph-families can be constructed with no waste.

\begin{theorem} [No Waste] \label{the:no-waste}
Let  be a graph language such that:  (i) there exists a natural number  s.t. for all  there is a subgraph  of  of logarithmic order s.t. either  or its complement is connected and has degree upper bounded by  and (ii)  is decidable in logarithmic space. Then , i.e. there is a protocol that constructs  equiprobably with useful space .
\end{theorem}
\begin{proof}
We give again the main idea. As in Theorem \ref{the:logar}, the protocol first constructs a spanning line used to separate a subpopulation  of  of size approximately . Before deactivating the line of  of length  the protocol first exploits it to construct a random graph in  of active or inactive degree (choosing randomly between these) upper bounded by  (note that  is finite and thus it is known in advance by the protocol). Then the line of  organizes the bounded-degree graph of  into a TM  (which is feasible due to the fact that the degree is bounded; see Theorem 7 of \cite{AACFJP05}) of logarithmic space with a unique leader on some node. Next  draws (more or less as in Theorem \ref{the:logar}) a random graph on the edges of , i.e. on all edges apart from those between the nodes of  (to prevent destroying the structure of the TM). Note that, in order for the TM to be able to distinguish the nodes of , the protocol has all these nodes in a special state that is not present in . Observe now that, in this manner, the protocol has constructed on  a random graph from those having a connected subgraph of logarithmic order and degree upper bounded by . It remains to verify whether the one constructed indeed belongs to . To do this,  simulates the TM  that decides  in logarithmic space. If  rejects then  builds another line in  that repeats the whole process, i.e. draws a new random graph in  and so on. When  first accepts, the protocol sleeps (in the sense that it does not terminate but does not alter edges anymore either).  
\qed
\end{proof}

\begin{remark}
If the graph-property  (in any of the above results) happens to occur with probability at least , where  is polynomial on , in the  random graph model, then its corresponding generic constructor runs in polynomial expected time. Connectivity is such an example as every  is almost surely connected and the same holds for every  (hamiltonicity is another example).
\end{remark}

\begin{remark}
All the above generic results have been proved for . Note that in  we can again construct a sufficiently long line (as our protocols for global line are in ) and exploit it as a space-bounded TM of the following sort: on input  (i.e. the useful space) the TM outputs a graph of order . By exploiting such graph-constructing TMs we can again construct a possibly large class of networks without giving to our protocols access to randomization.
\end{remark}

\subsection{Constructing and Simulating Supernodes with Logarithmic Memories}

We now show that a population consisting of  nodes can be partitioned into  \emph{supernodes} each consisting of  nodes, for the largest such . The internal structure of each supernode is a line, thus it can be operated as a TM of memory logarithmic in the total number of supernodes. This amount of storage is sufficient for the supernodes to obtain unique names and exploit their names and their internal storage to realize nontrivial constructions. We are interested in the networks that can be constructed at the supernode abstraction layer. The following theorem establishes that such a construction is feasible and presents a network constructor that achieves it.  

\begin{theorem} [Partitioning into Supernodes] \label{the:supernodes}
For every network  that can be constructed by  nodes having local memories  and unique names there is a NET that constructs  on  nodes.  
\end{theorem}
\begin{proof}
We present a NET  that when executed on  nodes it is guaranteed to organize the nodes into  lines of length  each for the maximum  for which . We assume a unique pre-elected leader in the initial configuration of the system and we will soon show how to drop this requirement. Assume also for simplicity that  (this is again not necessary). The protocol operates in phases. Variable  denotes the current phase number,  denotes the number of new lines that should be constructed in the current phase, and  is a line counter. We assume that the leader has somehow already created 4 lines of length 2 each (note that here we count the length of a line in terms of its nodes). One of them is the leader's line. Also the left endpoint of the leader's line is directly connected  to the left endpoints of the other 3 lines. In fact, all these assumptions are trivial to achieve. Initially . All variables are stored by the leader in the distributed memory of its line.
\begin{itemize}
 \item A new phase starts when the leader manages to increase by one the length of its line by attaching an isolated node its right endpoint. When this occurs, the leader sets , , and . A phase is divided into two subphases: the \emph{Increment existing lines} subphase and the \emph{Create new lines} subphase.
  \begin{itemize}
  \item \emph{Increment existing lines}: Initially, all existing lines, excluding the leader's line, are marked as \emph{unvisited}. While  the leader visits an unvisited line and tries to increment its length by one by attaching an isolated node to its right endpoint. When it succeeds, it marks the line as \emph{visited}, sets  and returns to its own line. When this subphase ends all existing lines have length . Then the leader sets  and the \emph{Create new lines} subphase begins.
  \item \emph{Create new lines}: While  the leader becomes connected to an isolated node, it marks that node as the left endpoint of the new line and then starts creating the new line node-by-node, by attaching isolated nodes to its right. It stops increasing the length of the new line when it becomes equal to the length of its own line. This can be easily implemented by a mark on the leader's line that moves one step to the right every time the length of the new line increases by one. The new line has the right length when the mark reaches the right endpoint of the leader's line. When this subphase ends there is a total of  lines of length  each and the leader is directly connected to the left endpoint of each one of them. Then the leader waits again to increase its own length by one and when this occurs a new phase begins. 
  \end{itemize}
\end{itemize}

\emph{Naming.} We now show that it is not hard to keep the constructed lines named (in fact there are various strategies for achieving this). Initially, the leader has 4 lines of length 2 each and we may assume that these are uniquely named  in binary, that is every line has its name stored in its own memory. During a phase, the leader keeps a variable \emph{cname} storing the current name to be assigned, initially 0. Whenever the leader increases the length of an existing line (during the \emph{increment} subphase) or creates a new line (during the \emph{create} subphase) it assigns to it \emph{cname} in binary and sets . Clearly, at the end of phase  the lines are uniquely named .\\

\vspace{-3pt}
\emph{Electing the Leader.} We now show how to circumvent the problem of not having initially a unique pre-lected leader. In fact, as we soon discuss, the solution we develop may serve as a generic technique for simulating protocols that assume a pre-elected leader. Initially all nodes are leaders in state . Rule  eliminates one leader when two of them interact. These leaders start executing the above protocol by attaching  nodes to their construction. By the time a  leader attaches the first isolated node to its construction it remains leader but changes its state to . Each leader executes the protocol on its own constructed component until it meets another leader. One this occurs, one of the two s becomes . The goal of a  leader is to revert its whole component to a set of isolated nodes in a special sleeping state  (itself inclusive). A sleeping node can like a  node be attached to lines by leaders but unlike a  node it cannot participate in the creation of another leader. First of all note that it can easily revert a single line by beginning from the right endpoint and releasing one after the other the nodes until it reaches the left endpoint. In fact, the generic idea (that works for other constructions as well) is that in order to release a node it suffices to know its degree. Then the only possible difficulty in our case is the fact that the left endpoint of the leader's line may be connected to a non-constant number of other endpoints. To resolve this, the leader exploits the fact that it can count in its line's memory the number of lines. When the reversion process begins, the leader knows the number of lines, that is it knows also the degree of the left endpoint of its line. Whenever it reverts another line it decreases the counter by one. So, when the counter becomes equal to 1, it knows that the only remaining line is its own line, thus it knows that when it comes to release the last two nodes of its own line (i.e. during the interaction between the left endpoint and the other remaining node of the line) it should make both sleeping as there is no other reversion to be performed. This is quite important as it guarantees that reverting does not introduce waste. Note that if the reversion process could not determine its completion then every such reversion would result in a node remaining forever in state . Such zombie s cannot be exploited by other leaders in their constructions as allowing a leader to attach a  would introduce conflicts between constructing and reverting processes.\\

\vspace{-3pt}
\emph{Reinitialization.} Note that the simulated protocol that constructs  assuming memories and names must be executed from the beginning because protocol  that gives the organization into lines is not terminating thus the two protocols must be composed in parallel. It suffices to have every line remember the number of active edges that it has to other lines. Then, whenever a new phase begins (implying that what has been constructed so far by the simulated protocol is not valid), each line deactivates one after the other all those edges and starts over the simulation.\\

\vspace{-3pt}
The only drawback is that the above protocol retains forever the connections between the left endpoint of the leader's line and the left endpoints of the other lines. However, if we agree that the output-network of the protocol is the one induced by the active edges joining the right endpoints of lines then this is not an issue. Additionally, it should not be that hard to circumvent this subtlety by having the leader periodically release the constructed lines and reattracting them only in case it manages to increase the length of one of them.
\qed
\end{proof}

Many network construction problems are substantially simplified given the supernodes with names and memories. For a simple example, consider the problem of partitioning the nodes into triangles. This construction is quite hard to achieve in the original setting without a leader, however, given the supernodes it becomes trivial. Each supernode with id  checks whether its id is a multiple of 3 and, if it is, it connects to id , otherwise it connects to id . This is a totally parallel and thus a very efficient solution.

Finally, the above approach introduces the idea of constructing disjoint stable structures and then looking at those structures from a higher level and considering them as units (supernodes). It is then challenging, interesting, and valuable to understand how these units behave, what is the dependence of their behavior to their internal structure and configuration, what is the outcome of an interaction between two such units, and what are their constructive capabilities. In fact, one can imagine a whole such hierarchy of layers were nodes self-assemble into supernodes, supernodes self-assemble into supersupernodes, and so on. Formalizing this hierarchy is a very promising and totaly open research direction.

\section{Conclusions and Further Research}
\label{sec:conclusions}

There are many open problems related to the findings of the present work. Though our universal constructors show that a large class of networks is in principle constructible, they do not imply neither the simplest nor the most efficient protocol for each single network in the class. To this end, we have provided direct constructors for some of the most basic networks, but there are still many other constructions to be investigated like grids or planar graphs. Moreover, a look at Table \ref{tab:ulb} makes it evident that there is even more work to be done towards the probabilistic analysis of protocols and in particular towards the establishment of tight bounds. Of special interest is the spanning line problem as it is a key component of universal construction. All of our attempts to give a protocol asymptotically faster than  have failed. Observe that with a preelected leader in state  and all edges initially inactive, the straightforward protocol  produces a stable spanning line in an expected number of  steps (follows from the meet everybody fundamental process). Moreover, by a one-to-one elimination we can elect a unique leader in an expected number of  steps. If we could safely compose these two protocols, then we would obtain a  constructor which is almost optimal as our present best lower bound for the spanning line is . The problem is that the protocol cannot detect when the leader-election phase has completed, thus it has to activate edges while still having more than one leaders but this gives an overhead for either merging the constructed disjoint lines or deactivating some wrong connections. A possible solution could be to consider Monte Carlo protocols that may err with some small probability, e.g. a protocol that would try somehow to estimate when w.h.p. the leader-election phase completes and only then start the line construction phase. 

One of the problems that we considered in this work, was the problem of constructing any -regular network. Note that this is a quite different problem than the problem of constructing a specific -regular network. For example, given a population of 10 processes is there a protocol that stabilizes to the Petersen graph? In general, it is worth considering non-uniform protocols that when executed on the correct number of nodes are required to construct a unique network like e.g. the cubical graph or the Wagner graph on 8 processes. 

Another very intriguing issue has to do with the size of network constructors. In particular, we would like to know whether there is some generic lower bound on the size of all constructors, to give problem-specific lower bounds, and to formalize the apparent relationship between the size and the running time of a protocol. Is there some sort of hierarchy showing that with more states we can produce faster protocols (until optimality is obtained)? To this end observe that neither the maximum degree nor the number of different degrees of the target-network are lower bounds on the number of states required to construct the network. For the former, it is not hard to show that  states suffice to make a node obtain  neighbors (stably). The idea is to have a node initially obtain 2 neighbors and then repeatedly double their number. For the latter, one can show that  states suffice to have  nodes with different degrees (stably) and in particular for all  we obtain a node with degree . The idea is to mark a set of  nodes as before and construct a line spanning these nodes. Then the protocol assigns to the th node of the line, counting e.g. from the left endpoint,  neighbors. This can be done by using only a constant number of states. The head begins from the left endpoint and moves step-by-step on the line towards . For every step it takes it assigns to  a new neighbor and stops when it reaches . In this manner, it assigns to  a number of neighbors equal to its distance from the endpoint without having to explicitly count the distance. Is there some other property of the target-network that determines the number of states that have to be used?

All of our generic constructors, produce a random graph in the useful space and then simulate a space bounded TM in the waste to determine whether the random graph belongs to the language and therefore whether it should be accepted. An interesting open problem is to characterize the class  in which protocols do not have access to (internal) randomness. In this case, it seems useless to simulate deciding TMs and the focus should be on \emph{graph-constructing TMs} which are much less understood. Such a characterization would also help us appreciate whether randomization increases the constructive power of the model. It is worth noting that our results on universal construction indicate that the constructive power increases as a function of the available waste. A complete characterization of this dependence would be of special value. 

There is also a practically unlimited set of variations of the proposed model that is worth considering. We mention a few of them. As already discussed, in this work we have considered a model of network construction with as minimal assumptions as possible to serve as a simple and clear starting point for more applied models to be defined. We now introduce such a model which seems to be of particular interest. Assume that every node is equipped with a predefined number of ports at specific positions of its ``body''. For example, in the 2-dimensional case these could be ``North'', ``South'', ``East'', ``West'' having the obvious angles between them. Nodes interact via their ports and they can detect which of their ports are used in an interaction. Moreover, when a connection is activated, it is always activated at a predetermined distance (i.e. all connections have the same length ) and it is always a straight line respecting the angles between itself and the (potentially active) lines of the other ports of the same node. Such a model (and possible variations of it, depending on the assumed hardware) seems particularly suitable for studying/designing very simple and local distributed protocols that are capable of constructing stable geometric objects (even in three dimensions), like squares, cubes, or more complex polyhedra, without any mobility-control mechanism. Moreover, an immediate extension of our model is to allow the connections to have more than just the two states that we considered in this work. Recall that, whenever we had to analyze the running time of a protocol, we did it under the uniform random scheduler, mainly because we wanted to keep this first model of network construction as simple as possible and because of its correspondence to a well-mixed solution. However, there are many other natural probabilistic scheduling models to be considered which would probably require different algorithmic developments and techniques to achieve efficiency. It is also natural to consider a variant in which connected nodes communicate much faster (even in synchronous rounds) than disconnected nodes. Moreover, it would be interesting to consider a model of network construction in which the behavior of a node depends on some input from the environment (this would allow the consideration of codes that exhibit different behaviors in different environments). Interesting and natural seems also the model in which a connected component has access to a \emph{self-bit} indicating whether a given interaction involves two nodes of the same component or not. It is not yet clear whether this extra assumption increases the constructive power of the model but it is clear that it substantially simplifies the description of several protocols. Also, of its own value would be to depart from cooperative models and consider an antagonistic scenario in which different sets of nodes try to construct different networks (by deterministic codes and not game-theoretic assumptions involving incentives). It would be interesting to discover cases in which the antagonism leads to unexpected stable formations. 

Finally, a very valuable and challenging interdisciplinary goal is to further investigate and formalize the apparent applicability of the model proposed here (and potential variations of it) in physical and chemical (possibly biological) processes. As already stated, we envision that a potential usefulness of such models is to unveil the algorithmic properties underlying the structure/network formation capabilities of natural processes.\\

\noindent \textbf{Acknowledgements.} We would like to thank Leslie Ann Goldberg for bringing to our attention the importance of constructing regular networks and also the reviewers of this work and some previous versions of it whose comments have helped us to improve our work substantially.

\bibliographystyle{alpha-abr}
\bibliography{podc14-full}

\end{document}
