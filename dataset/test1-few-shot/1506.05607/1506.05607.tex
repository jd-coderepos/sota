\documentclass{IEEEtran}


\renewcommand{\baselinestretch}{1.25}
\usepackage[includehead,hmargin={3.6cm, 2.6cm}, vmargin={2.5cm,2.7cm}, headsep=.8cm,footskip=1.2cm]{geometry}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{paralist}
\usepackage{latexsym}
\usepackage{txfonts}
\usepackage{dsfont}
\usepackage{bbold}
\usepackage{cite}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{sidecap}
\usepackage{microtype}
\usepackage{pifont}
\usepackage{mathtools}
\usepackage{algorithm,algpseudocode}

\usetikzlibrary{positioning, automata, shapes.arrows, calc, shapes, arrows}

\newcommand{\comment}[1]{#1}
\newcommand{\hide}[1]{}
\newcommand{\pponly}[1]{{}}
\newcommand{\jrronly}[1]{{}}
\newcommand{\rronly}[1]{{#1}}
\newcommand{\inputsynth}[1]{{}}
\newcommand{\controllersynth}[1]{{#1}}

\newcommand{\dccmt}[1]{\textcolor{green}{[\comment{#1}]}}
\newcommand{\pscmt}[1]{{\color{red} [\comment{#1}]}}
\newcommand{\danielscomment}[1]{\textcolor{blue}{[\comment{#1}]}}
\newcommand{\aacmt}[1]{\textcolor{magenta}{[\comment{#1}]}}

\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\cvec}[2]{\left[\begin{array}{@{}c@{}}{#1}\\{#2}\end{array}\right]}
\newcommand{\qmat}[4]{\left[\begin{array}{@{}cc@{}}{#1}&{#2}\\{#3}&{#4}\end{array}\right]}
\newcommand{\mat}[1]{\boldsymbol{#1}}
\newcommand{\intvec}[1]{\mathbb{#1}}
\newcommand{\intmat}[1]{\mathbb{#1}}
\newcommand{\res}{\mathit{res}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\xmark}{\ding{55}}
\newcommand{\thsup}{^{\mbox{\scriptsize th}}}
\DeclarePairedDelimiter\norm{\lVert}{\rVert}
\DeclarePairedDelimiter\abs{|}{|}


\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}

\title{{Unbounded-Time Analysis of\jrronly{ Continuous} Guarded LTI Systems with Inputs by\jrronly{ Counterexample Guided} Abstract Acceleration }}
  
\author{Dario Cattaruzza \and Alessandro Abate \and Peter Schrammel \and Daniel Kroening}



\begin{document}
\maketitle

\begin{abstract}
Reachability analysis of continuous and discrete time systems is a hard problem that has seen much progress in the last decades.
In many cases the problem has been reduced to bisimulations with a number of limitations in the nature of the dynamics, soundness, or time horizon. In this article we focus on sound safety verification of Unbounded-Time Linear Time-Invariant (LTI) systems with inputs \jrronly{for both continuous and discrete time} using reachability analysis. We achieve this by using \jrronly{Counterexample-Guided} Abstract Acceleration, which over-approximates the reach tube of a system over unbounded time by using abstraction \jrronly{and finding concrete counterexamples for abstraction refinement based on the safety specification}. The technique is applied to a number of \jrronly{discrete and continuous} models and the results show good performance when compared to state-of-the-art tools.
\end{abstract}

\section{Introduction} \label{sec:intro}


Linear loops are an ubiquitous programming pattern. Linear loops iterate
over continuous variables, which are updated with a linear transformation. 
Linear loops may be guarded, i.e., terminate if a given linear condition
holds.  Inputs from the environment can be modelled by means of
non-deterministic choices within the loop.  These features make linear loops
expressive enough to capture the dynamics of many hybrid dynamical models. 
The usage of such models in safety-critical embedded systems makes linear
loops a fundamental target for formal methods.

Many high-level requirements for embedded control systems can be modelled as
safety properties, \emph{i.e.} deciding reachability of certain \emph{bad
states}, in which the system exhibits unsafe behaviour.  Bad states may, in
linear loops, be encompassed by guard assertions.  

Reachability in linear programs, however, is a formidable challenge for automatic analysers:
the problem is undecidable despite the restriction to linear transformations
(i.e., linear dynamics) and linear guards.  

The goal of this article is to push the frontiers of unbounded-time
reachability analysis: we aim at devising a method that is able to
reason soundly about unbounded trajectories.  We present a new
approach for performing \emph{abstract acceleration}.  Abstract
acceleration~\cite{GH06,JSS14,GS14} approximates the effect of an
arbitrary number of loop iterations (up to infinity) with a single,
non-iterative transfer function that is applied to the entry state of
the loop (i.e., to the set of initial conditions of the linear
dynamics).  This article extends the work in~\cite{JSS14} to systems
with non-deterministic inputs \jrronly{and to continuous time models,}
elaborating the details omitted in~\cite{Sch15}.

The key contributions of this article are:

\begin{enumerate}
\item We present a new technique to include inputs (non-determinism) in the
abstract acceleration of general linear loops.
\item We introduce the use of support functions in complex spaces, in order
to increase the precision of previous abstract acceleration methods.
\jrronly{
\item We develop a counterexample-guided refinement for Abstract Acceleration for safety verification, 
maximising speed when precision is not necessary, thus allowing for optimal analysis within a safe region.
\item We extend Abstract Acceleration to the continuous time case.
}
\end{enumerate}


\section{Preliminaries} 


\subsection{Linear Loops with Inputs}\label{sec:linear_loops}


Simple linear loops are programs expressed in the form:

where  is a valuation on the state variables, 
 is a linear constraint on the states (with
 and ), 
 is a non-deterministic input, 
and  and  are linear
transformations characterising the dynamics of the system. 
In particular, the special instance where  (i.e., ``while
true'') represents a time-unbounded loop with no guards, for which the
discovery of a suitable invariant (when existing) is paramount.
As evident at a semantical level, this syntax can be
interpreted as the dynamics of a discrete-time LTI model with inputs, under
the presence of a guard set which, for ease of notation, we denote as .
In the remaining of this work we will also use the notation 
to represent the rows of a matrix and  its columns.

\subsection{Model Semantics}\label{sec:model_semantics}


The traces of the model starting from an initial set , 
with inputs restricted to , are sequences 
, 
where  and , 
where 


We extend the notation above to convex sets of states and inputs ( and ), 
and denote the set of states reached from  by  in one step:

We furthermore denote the set of states reached from
 via  in  steps (\emph{-reach set}), for :

Since the transformations  and  are linear, and
vector sums preserve convexity, the sets  are also
convex.

We define the \emph{-reach tube} 

as the union of the reachable sets over  iterations.
Moreover,  
extends the previous notion over an 
unbounded time horizon.

\subsection{Support Functions} \label{sec:support}


\subsubsection{Support Function Definition} \label{sec:support_def}


A support function is a convex function on  which describes the distance of a supporting
hyperplane for a given geometrical set in . 

Support functions may be used to describe a set by defining the distance of
its convex hull with respect to the origin, given a number of directions. 
More specifically, the distance from the origin to the hyperplane that is orthogonal to the given
direction and that touches its convex hull at its farthest. Finitely sampled 
support functions are template polyhedra in which the directions are not fixed, 
which helps avoiding wrapping effects \cite{GLM06}.
The larger the number of directions provided, the more precisely represented the set will be. 

In more detail, given a direction ,
the support function of a non-empty set  in the direction of  is defined as 

where  is the dot product of the two vectors.

Support functions do not exclusively apply to convex polyhedra, 
but in fact to any set  represented by a general assertion . 
We will restrict ourselves to the use of 
convex polyhedra, in which case the support function definition translates to solving the linear program

\\
\subsubsection{Support Functions Properties}\label{sec:support_properties}


Several properties of support functions allow us to reduce operational
complexity.  The most significant
are~\cite{DBLP:dblp_journals/cviu/GhoshK98}:

As can be seen by their structure, some of these properties reduce
complexity to lower-order polynomial 
or even to constant time, by turning matrix-matrix multiplications () into matrix-vector (), 
or into scalar multiplications.

\subsubsection{Support Functions in Complex Spaces}\label{sec:support_complex}


The literature does not state, as far as we found any description of the use of support functions in complex spaces. 
Since this is relevant to using our technique, we extend the definition of support functions to encompass their operation
on complex spaces.\\
\\
A support function in a complex vector field is a transformation:

The dot product used here is the Euclidean Internal Product of the vectors,
which is commonly defined in the complex space as:

We are interested in the norm of the complex value, which is a 1-norm given our definition of dot product:


 Returning to our support function properties, we now have:

which is consistent with the real case when .  
The reason why  cannot be extracted out is because it is
a rotation, and therefore follows the same rules as a matrix
multiplication,  

Since matrices using pseudo-eigenvalues are real, all other properties remain the same. 
An important note is that when using pseudo-eigenvalues, 
conjugate eigenvector pairs must be also converted into two separate real eigenvectors, 
corresponding to the real and the imaginary parts of the pair. 



\section{The Polyhedral Abstract Domain} \label{sec:polyhedra}


\subsection{Convex Polyhedra} \label{sec:convex_polyhedra}


A polyhedron is a topological element in  with flat polygonal (2-dimensional) faces. Each 
face corresponds to a hyperplane that creates a halfspace, and the intersections of these hyperplanes are the edges of the polyhedron.
A polyhedron is said to be convex if its surface does not intersect itself and a line segment joining any two 
points of its surface is contained in the interior of the polyhedron. Convex polyhedra are better suited than
general polyhedra as an abstract domain, mainly because they have a simpler representation and operations
over convex polyhedra are in general easier than for general polyhedra.
There are a number of properties of convex polyhedra that make them ideal for abstract interpretation of
continuous spaces, including their ability to reduce an uncountable set of real points into a countable set
of faces, edges and vertices.
Convex polyhedra retain their convexity across linear transformations, and are functional across a number
of operations because they have a dual representation~\cite{fukuda1996double}. The mechanism to switch between these two
representations is given in section \ref{sec:vertex}

\subsubsection{Vertex Representation} \label{sec:vertex_representation}
Since every edge in the polyhedron corresponds to a line between two vertices and every face corresponds to
the area enclosed by a set of co-planar edges, a full description of the polyhedron is obtain by simply listing its 
vertices.
Since linear operations retain the topological properties of the polyhedron, performing these operations on the
vertices is sufficient to obtain a complete description of the transformed polyhedron (defined by the transformed
vertices).
Formally, a polyhedron is a set  such that  is a vertex of the polyhedron. 

\subsubsection{Inequality Representation} \label{sec:ine_representation}


The dual of the Vertex representation is the face representation. Each face corresponds to a bounding hyperplane
of the polyhedron (with the edges being the intersection of two hyperplanes and the vertices the intersection of 3 or
more), and described mathematically as a function of the vector normal to the hyperplane.
If we examine this description closely, we can see that it corresponds to the support function of the vector normal
to the hyperplane.
Given this description we formalise the following:
A convex polyhedron is a topological region in  described by the set 

where the rows  for  correspond to the transposed vectors normal to the faces of the polyhedron and
 for  their support functions.
For simplicity of expression, we will extend the use of the support function operator as follows:


\subsection{Operations on Convex Polyhedra} \label{sec:convex_polyhedra_ops}


Several operations of interest can be performed on convex polyhedra
\subsubsection{Translation} \label{sec:convex_translation}
Given a vertex representation  and a translation vector , the transformed polyhedron is

Given an inequality representation  and a translation vector , the transformed polyhedron corresponds to


\subsubsection{Linear Transformation} \label{sec:convex_transform}
Given a vertex representation  and a linear transformation , the transformed polyhedron is

Given an inequality representation  and a linear transformation ,
the transformed polyhedron corresponds to

where  represents the pseudo-inverse of . In the case when the inverse  exists, then

From this we can conclude that linear transformations are better handled by a vertex representation, except when the inverse of the transformation exists and is know a-priori. This works makes use of this last case to avoid continuous swapping in representations.

\subsubsection{Set Sums} \label{sec:convex_sum}
The addition of two polyhedra is a slightly more complex matter. The resulting set is one such that for all possible combinations of points inside both original polyhedra, the sum is contained in the result. This operation is commonly known as the Minkowski sum, namely

Given two vertex representations  and  the resulting polyhedron

where  is the convex hull of the set of vertices contained in the Minkowski sum.\\
Let

be two sets, then

where

Because these sets correspond to systems of inequalities, they may be reduced removing redundant constraints. Note that if  then


\subsubsection{Set Hadamard Product} \label{sec:convex_prod}
\begin{lemma}
Given two vertex representations  and  the resulting polyhedron

where  represents the Hadamard (coefficient-wise) product of the vectors, contains all
possible combinations of products between elements of each set.
\end{lemma}
\begin{proof}
Given a convex set , we have:

Given , , 

This equation proves that given ,  and ,

\end{proof}

\subsubsection{Vertex Enumeration} \label{sec:vertex}


The vertex enumeration problem corresponds to the algorithm required to obtain a list of all vertices of a
polyhedron given an inequality description of its bounding hyperplanes. Given the duality of the problem, it is
also possible to find the bounding hyperplanes given a vertex description if the chosen algorithm exploits this duality.
In this case the description of V is given in the forms of a matrix inequality  with . Similarly,  can be described as a set containing each of its rows. 
At the time of writing, there are two algorithms that efficiently solve the  vertex enumeration problem. lrs is
a reverse search algorithm, while cdd follows the double description method. 
In this work we use the cdd algorithm for convenience in implementation (the original cdd was developed 
for floats, whereas lrs uses rationals). The techniques presented here can be applied to either.

Let 

be the polyhedral cone represented by . 
The pair  is said to be a double description pair if

 is called the generator of .
Each element in  lies in the cone of , and its minimal form (smallest ) has a one-to-one
correspondence with the extreme rays of X if the cone is pointed (i.e., it has a vertex at the origin).
This last can be ensured by translating a polyhedral description so that it includes the origin, and then 
translating the vertices back once they have been discovered (see section \ref{sec:convex_polyhedra_ops}).

We will also point out that


The vertex enumeration algorithm starts by finding a base  which contains a number of
vertices of the polyhedron. This can be done by pivoting over a number of different rows in  and selecting the 
feasible visited points, which are known to be vertices of the polyhedron (pivoting  times will ensure at least one
vertex is visited if the polyhedron is non-empty).  is represented by   which contains the rows 
used for the pivots.
The base  is then iteratively expanded to  by exploring the  row of
 until . The corresponding pairs  are
constructed using the information from  as follows:\\
Let , , ,

be the spaces outside inside and on the  hyperplane and 

the existing vertices lying on each of these spaces.\\
Then 

For the proof see \cite{fukuda1996double}.

\section{Abstract Matrices in abstract acceleration}\label{sec:absacc}


\subsection{Acceleration Techniques}\label{sec:accel}


Acceleration of a transition system is a method that seeks to precisely describe the transition relations over
a number of steps using a concise description of the overall transition between the first and final step. Namely,
it looks for a direct formula to calculate the postimage of a loop from the initial states of the loop.
\jrronly{It may be applied to both continuous and discrete systems using different paradigms.}
Formally, given the dynamics in equation \eqref{equ:reachtraj} an acceleration formula aims at computing
the reachset \eqref{equ:reachset} using a function  such that . In the case
of systems without inputs, this equation is . We will use this property
and others derived from it to calculate our abstract matrices.\\

\subsection{Overview of the Algorithm}\label{sec:absaccel_overview}


The basic steps required to evaluate a reach tube using abstract acceleration
can be seen in figure~\ref{fig:procedure}. 
\begin{enumerate}
\item The process starts by doing
eigendecomposition of the dynamics () in order to transform the
problem into a simpler one. 
\item A variety of off-the-shelf tools may be used, but
since larger problems require numerical algorithms for scalability, a second
step involves upper-bounding the error in order to obtain sound results. 
In such cases, all subsequent steps must be performed using interval arithmetic.
\item The inverse of the generalised eigenvectors must be calculated soundly.
\item The problem gets transformed into canonical form by multiplying both sides of the equation by :

\item We calculate the number of iterations as explained in section~\ref{sec:guards}. If there are no guards, we use . It is worth noting that this number need not be exact: if we overapproximate the number of iterations, the resulting reachtube will overapproximate the desired one.
\item we overapproximate the dynamics of the variable inputs (for parametric or no inputs this step will be ignored) using the techniques described in section~\ref{sec:var_inpur_accel}
\item we calculate the abstract dynamics using the techniques described in section~\ref{sec:support_aa}
\item we evaluate the vertices of the combined input-initial eigenspace to be used as source for the reachtube calculation
\item we use a sound simplex algorithm to evaluate the convex set product of the abstract dynamics (used as the tableau) and the initial set (whose vertices are used as the obejctive functions alongside a set of template directions for the result).
\item since we have calculated our result in the eigenspace, we transform the reachtube back into the normal space by multiplying by  .
\end{enumerate}
\begin{figure}
\centering
{\scriptsize
\begin{tikzpicture}[scale=0.3,->,>=stealth',shorten >=.2pt,auto, semithick, ampersand replacement=\&,]
  \matrix[nodes={draw, fill=none, shape=rectangle, minimum height=.2cm, minimum width=1.5cm, align=center},row sep=.7cm, column sep=.5cm] {
   \& \node[fill=white!20,align=center] (eigen) {{\sc 1. Calculate}\\{\sc Eigenspace}};
   \& \node[fill=blue!20,align=center] (sound_eigen) {{\sc 2. Restore}\\{\sc Soundness}};
   \& \node[fill=blue!20,align=center] (inverse) {{\sc 3. Find}\\{\sc Inverse}};\\
   \& \coordinate (aux1);
   \& \node[fill=orange!20,align=center] (iters) {{\sc 5. Find Number}\\{\sc of  Iterations}};
   \& \node[fill=purple!20,align=center] (init) {{\sc 4. Transform}\\{\sc problem into}\\{\sc Eigenspace}};\\
   \& \coordinate (aux);
   \& \node[fill=orange!20,align=center] (aaj) {{\sc 7. Get Abstract}\\{\sc Dynamics}};
   \& \node[fill=orange!20,align=center] (sphere) {{\sc 6. Semispherical}\\{\sc Approximation}};\\
   \& \node[fill=purple!20,align=center] (reach) {{\sc 10. Get}\\{\sc Reach Tube}};
   \& \node[fill=green!20,align=center] (eigenreach) {{\sc 9. Find Eigen}\\{\sc Reach Tube}};
   \& \node[fill=green!20,align=center] (vert) {{\sc 8. Find}\\{\sc Vertices}};\\
  };
  \path
     ([xshift=-2cm]eigen.west) edge node[align=center] {} (eigen.west)
    (eigen.east) edge node[align=center] {, } (sound_eigen.west)
    (sound_eigen.east) edge node[align=center] {} (inverse.west);
  \path
    (sound_eigen.south) edge node[align=center] {} (iters.north)
    (inverse.south) edge node[align=center] {} (init.north);
  \path
    (init.west) edge node[align=center, yshift=.38cm] {{}\\{}\\{}} (iters.east);
  \path
    (iters.south) edge node[align=center] {, } (aaj.north)
    (init.south) edge node[align=center] {{, }} (sphere.north);
  \path 
    (sphere.south) edge node[align=center] {{, }} (vert.north)
    (sphere.west) edge node[align=center] {{}} (aaj.east)
    (aaj.south) edge node[align=center] {} (eigenreach.north);
   \path
    ([xshift=2cm]init.east) edge node[align=center, yshift=.38cm] {{}\\{}\\{}} (init.east)
    (vert.west) edge node[align=center] {{}} (eigenreach.east)
    (eigenreach.west) edge node[align=center] {{}} (reach.east)
    (aux) edge node[align=center] {}  (reach.north)
    (reach.west) edge node[align=center] {} ([xshift=-2cm]reach.west);
\end{tikzpicture}
}
\caption{Block diagram describing the different steps used to calculate the abstract reach tube of a system.}
\label{fig:procedure}
\end{figure}
\subsection{Computation of Abstract Matrices}\label{sec:absmat_real}


We define the abstract matrix  as an over-approximation
of the union of the powers of the matrix  such that  and its application to the initial set 

Next we explain how to compute such an abstract matrix.
For simplicity, we first describe this computation for matrices  with real eigenvalues, 
whereas the extension to the complex case will be addressed in Section~\ref{sec:absmat_complex}. 
Similar to \cite{JSS14}, we first have to compute the Jordan normal form of .
Let  where  is the normal Jordan form of , 
and  is made up by the corresponding eigenvectors. 
We can then easily compute , where 

The abstract matrix  is computed as an abstraction over 
a set of vectors  of entries of . 

Let . 
The vector  is obtained by the transformation :

such that 
. 

If  is diagonal \cite{JSS14}, then  equals the
vector of powers of eigenvalues .
An interval abstraction can thus be simply obtained by computing the
intervals
 .
We observe that the spectrum of the interval matrix  (defined as intuitively) 
is an over-approximation of .

In the case of the  Jordan block  with geometric non-trivial
multiplicity  (), observe that
the first row of  contains all (possibly) distinct
entries of .  Hence, in general, the vector section  is
the concatenation of the (transposed) first row vectors
 of .

Since the transformation  transforms the vector  into the shape of \eqref{jord:pow} of , it is called a \emph{matrix shape} \cite{JSS14}. 
We then define the abstract matrix as
 
where the constraint  is synthesised
from intervals associated to the individual eigenvalues and to their
combinations.  More precisely, we compute polyhedral relations: 
for any pair of eigenvalues (or binomials) within , we find
an over-approximation of the convex hull containing the points 



\subsection{Abstract Matrices in Complex Spaces}\label{sec:absmat_complex}


To deal with complex numbers in eigenvalues and eigenvectors, 
\cite{JSS14} employs the real Jordan form for conjugate eigenvalues 
 and  (), so that 

Although this equivalence will be of use once we evaluate the progression of the system, 
calculating powers under this notations is often more difficult than handling directly the original matrices with complex values.  

In Section~\ref{sec:absmat_real}, 
in the case of real eigenvalues we have abstracted the entries in the power matrix  by ranges of eigenvalues
.  
In the complex case we can do something similar by rewriting
eigenvalues into polar form  and abstracting
by .

\section{General Abstract Acceleration with Inputs} \label{sec:forward_aa}


\subsection{Using Support Functions on Abstract Acceleration}\label{sec:support_aa}


\begin{figure}[]
\centering
\begin{tikzpicture}[scale=0.8]
\begin{axis}[ylabel={},xlabel={}]\addplot[color=blue,mark=*] table{
2 3
4 9
8 27
16 81
32 243
};
\addplot[dash pattern=on 1pt off 3pt , color=gray] coordinates {(5.5,0) (32,0)};
\addplot[dash pattern=on 1pt off 3pt , color=gray] coordinates {(32.5,0) (32.5,33)};
\addplot[dash pattern=on 1pt off 3pt , color=gray] coordinates {(1.5,213) (1.5,247)};
\addplot[dash pattern=on 1pt off 3pt , color=gray] coordinates {(1.5,247) (32.5,247)};
\addplot[dashed, color=red] coordinates {(1.5,0) (1.5,213)};
\addplot[dashed, color=red] coordinates {(1.5,213) (32,247)};
\addplot[dashed, color=red] coordinates {(2,0) (32,30)};
\addplot[dashed, color=red] coordinates {(32.5,33) (32.5,187)};
\addplot[color=purple, line width=1.5pt] coordinates {(2,7) (32,247)};
\addplot[color=purple, line width=1.5pt] coordinates {(5,0) (32.5,187)};
\addplot[color=purple, line width=1.5pt] coordinates {(1.5,0) (5,0)};
\addplot[color=purple, line width=1.5pt] coordinates {(32.5,187) (32.5,247)};
\node at (axis cs:2,3) [pin={+90:},inner sep=0pt] {};
\node at (axis cs:4,9) [pin={-10:},inner sep=0pt] {};
\node at (axis cs:8,27) [pin={-10:},inner sep=0pt] {};
\node at (axis cs:16,81) [pin={-10:},inner sep=0pt] {};
\node at (axis cs:32,243) [pin={-170:},inner sep=0pt] {};
\end{axis}
\end{tikzpicture}
\caption{Polyhedral faces from an  subspace, where 
   so that . 
  Bold purple lines represent supports found by this article. The dotted grey and dashed
  red polytopes show logahedral approximations (box and octagon)
  used in \cite{JSS14}. Note the scales (sloped dashed lines are
  parallel to the x=y line, and dashed red polytope hides two small faces yielding an octagon).}
\label{aa:supports}
\end{figure}

As an improvement over \cite{JSS14}, the rows in  and  (see \eqref{abs:mat}) are synthesised by discovering
support functions in these sets. The freedom of directions provided by these support functions results in an improvement over the logahedral abstractions used in previous works (see Figures~\ref{aa:supports} - \ref{aa:mixed_supports}).
The mechanism by which this works follows the convex properties of the exponential progression. 
There are four cases cases to consider \footnote{these explain in detail the procedure alluded in \cite{cattaruzza2015unbounded}}:
\begin{enumerate}
\item Positive Real Eigenvalues\\
The exponential curve is cut along the diagonal between the maximum and minimum eigenvalues to create a support function for the corresponding hyperplane. A third point taken from the curve is used to test the direction of the corresponding template vector. An arbitrary number of additional hyperplanes are selected by picking pairs of adjacent points in the curve and creating the corresponding support function as shown in Figure~\ref{aa:supports}.
\item Complex Conjugate Eigenvalue pairs\\
In the case of complex conjugate pairs, the eigenvalue map corresponds to a logarithmic spiral. In this case, we must first extract the number of iterations required for a full cycle. For convergent eigenvalues, only the first  iterations have an effect on the support functions, while in the divergent case only the last  iterations are considered. Support functions are found for adjacent pairs, checking for the location of the origin point (first point for convergent eigenvalues, last for divergent eigenvalues). If the origin falls outside the support function, we look for an interpolant point that closes the spiral tangent to the origin. This last check is performed as a binary search over the remaining points in the circle (whose supporting planes would exclude the origin) to achieve maximum tightness (see Figure~\ref{aa:complex_supports}).

\begin{figure}[]
\centering
\begin{tikzpicture}[scale=0.8]
\begin{axis}[ylabel={},xlabel={}]\addplot[color=blue,mark=*] table{
0.8 0.4
0.48 0.64
0.128 0.704
-0.1792 0.6144
-0.3891 0.4198
-0.4792 0.1802
-0.4554 -0.0475
-0.3454 -0.2202
-0.1882 -0.3143
-0.0249 -0.3267
0.1108 -0.2713
0.1972 -0.1727
0.2268 -0.0593
0.2052 0.04328
};
\addplot[dash pattern=on 1pt off 3pt , color=gray] coordinates {(-0.48,.72) (0.82,.72)};
\addplot[dash pattern=on 1pt off 3pt , color=gray] coordinates {(-.48,-.33) (0.8,-.33)};
\addplot[dash pattern=on 1pt off 3pt , color=gray] coordinates {(0.82,-.33) (0.82,.72)};
\addplot[dash pattern=on 1pt off 3pt , color=gray] coordinates {(-0.48,-.33) (-0.48,.72)};
\addplot[dash pattern=on 1pt off 3pt, line width=1.5pt , color=blue] coordinates {(0.08,-.33) (0.82,.5)};
\addplot[color=purple, line width=1.5pt] coordinates {(0.82,0.415) (0.82,0.385)};
\addplot[color=purple, line width=1.5pt] coordinates {(0.82,0.415) (.42,.72)};
\addplot[color=purple, line width=1.5pt] coordinates {(-.1,.72) (.42,.72)};
\addplot[color=purple, line width=1.5pt] coordinates {(-.1,.72) (-.48,0.37)};
\addplot[color=purple, line width=1.5pt] coordinates {(-.48,-.05) (-.48,0.37)};
\addplot[color=purple, line width=1.5pt] coordinates {(-.48,-.05) (-.3,-.33)};
\addplot[color=purple, line width=1.5pt] coordinates {(-.3,-.33) (0.08,-0.33)};
\addplot[color=purple, line width=1.5pt] coordinates {(0.82,0.385) (0.08,-0.33)};
\node at (axis cs:0.8,0.4) [pin={+200:},inner sep=0pt] {};
\node at (axis cs:0.48,0.64) [pin={-90:},inner sep=0pt] {};
\node at (axis cs:.128,.704) [pin={-90:},inner sep=0pt] {};
\end{axis}
\end{tikzpicture}
\caption{Polyhedral faces from an  complex conjugate subspace, where 
   so that . 
  Bold purple lines represent supports found by this article. The blue dotted line shows the support function that excludes the origin (n=1), which is replaced by the support function projecting from said origin.}
\label{aa:complex_supports}
\end{figure}

\item Equal Eigenvalues\\
When two eigenvalues are the same, the resulting support functions are those orthogonal to the  plane, intersecting the square created by the maximum and minimum values.
\item Jordan Blocks of size \\
In the case of eigenvalues with geometric multiplicities , the shape of the function is similar to its corresponding unit size block. In the convergent case, since the convexity can be sharp, it is important to find the apex of the upper diagonals in order to minimise the over-approximation. See Figure~\ref{aa:jordan_supports}.

\begin{figure}[]
\centering
\begin{tikzpicture}[scale=0.8]
\begin{axis}[ylabel={},xlabel={}]\addplot[color=blue,mark=*] table{
0.8 1
0.64 1.6
0.512 1.92
0.4096 2.048
0.3277 2.050
0.2621 1.966
0.2097 1.835
.1678 1.678
.1342 1.510
.1074 1.342
.0859 1.181
.0687 1.031
.0550 0.893
.0440 0.770
.0352 0.660
};
\addplot[dash pattern=on 1pt off 3pt , color=gray] coordinates {(0.01,0.62) (0.82,0.62)};
\addplot[dash pattern=on 1pt off 3pt , color=gray] coordinates {(0.01,2.07) (0.82,2.07)};
\addplot[dash pattern=on 1pt off 3pt , color=gray] coordinates {(0.82,0.62) (0.82,2.07)};
\addplot[dash pattern=on 1pt off 3pt , color=gray] coordinates {(0.01,0.62) (0.01,2.07)};
\addplot[color=purple, line width=1.5pt] coordinates {(0.82,1.02) (0.55,2.07)};
\addplot[color=purple, line width=1.5pt] coordinates {(0.01,.62) (.1,1.5)};
\addplot[color=purple, line width=1.5pt] coordinates {(.1,1.5) (.25,2.07)};
\addplot[color=purple, line width=1.5pt] coordinates {(.25,2.07) (.55,2.07)};
\addplot[color=purple, line width=1.5pt] coordinates {(0.82,1.02) (0.82,0.96)};
\addplot[color=purple, line width=1.5pt] coordinates {(0.02,0.6) (0.82,0.96)};
\addplot[color=purple, line width=1.5pt] coordinates {(0.02,0.6) (0.01,0.62)};
\node at (axis cs:0.8,1) [pin={+180:},inner sep=0pt] {};
\node at (axis cs:0.64,1.6) [pin={190:},inner sep=0pt] {};
\node at (axis cs:.512,1.92) [pin={-120:},inner sep=0pt] {};
\node at (axis cs:.3277,2.05) [pin={-90:},inner sep=0pt] {};
\end{axis}
\end{tikzpicture}
\caption{Polyhedral faces from an  Jordan block subspace, where 
   so that . 
  Bold purple lines represent supports found by this article. The blue dotted line shows the support function that excludes the origin (n=1), which is replaced by the support function projecting from said origin.}
\label{aa:jordan_supports}
\end{figure}

\item Negative Eigenvalues and mixed types\\
When mapping a positive real eigenvalue to a complex conjugate or negative one, we must account for both sides of the axis on the latter. 
These form mirror images that are merged during the abstraction. To make matters simple, we use the magnitude of a complex eigenvalue and evaluate whether the dynamics are concave or convex with respect to the mirroring plane. See Figure~\ref{aa:mixed_supports}.

Note that if both eigenvalues are negative and/or conjugate pairs from a different pair, the mirror image would be taken on both axes, resulting in a hyperrectangle. For a tighter bound in the purely convergent case, we find the convex hull of a point cloud for a small time horizon and merge it with the hyperrectangle for the infinite time horizon thereon.
\end{enumerate}



\begin{figure}[]
\centering
\begin{tikzpicture}[scale=0.8]
\begin{axis}[ylabel={},xlabel={}]\addplot[color=blue,mark=*] table{
1.41 1.76
2 3
2.82 5.28
4 9
5.64 15.84
};
\addplot[color=blue,mark=*] table{
-1.41 1.76
-2 3
-2.82 5.28
-4 9
-5.64 15.84
};
\addplot[color=green,mark=*] table{
1.76 1.41
3 2
5.28 2.82
9 4
15.84 5.64
};
\addplot[color=green,mark=*] table{
-1.76 1.41
-3 2
-5.28 2.82
-9 4
-15.84 5.64 
};
\addplot[color=orange,mark=*] table{
1.76 1.76
3 3
5.28 5.28
9 9
};
\addplot[color=orange,mark=*] table{
-1.76 1.76
-3 3
-5.28 5.28
-9 9
};
\addplot[color=brown,mark=*] table{
1.76 .707
3 .5
5.28 .35
9 .25
15.84 .176
};
\addplot[color=brown,mark=*] table{
-1.76 .707
-3 .5
-5.28 .35
-9 .25
-15.84 .176 
};
\addplot[dash pattern=on 1pt off 3pt, color=purple, line width=1.5pt] coordinates {(-1.45,1.65) (1.45,1.65)};
\addplot[dash pattern=on 1pt off 3pt, color=purple, line width=1.5pt] coordinates {(-6.2,16.1) (6.2,16.1)};
\addplot[dash pattern=on 1pt off 3pt, color=purple, line width=1.5pt] coordinates {(-6.2,16.1) (-6.2,14.1)};
\addplot[dash pattern=on 1pt off 3pt, color=purple, line width=1.5pt] coordinates {(-1.8,1.65) (-6.2,14.1)};
\addplot[dash pattern=on 1pt off 3pt, color=purple, line width=1.5pt] coordinates {(6.2,16.1) (6.2,14.1)};
\addplot[dash pattern=on 1pt off 3pt, color=purple, line width=1.5pt] coordinates {(1.8,1.65) (6.2,14.1)};
\addplot[dash pattern=on 1pt off 3pt, color=purple, line width=1.5pt] coordinates {(-1.85,1.25) (1.85,1.25)};
\addplot[dash pattern=on 1pt off 3pt, color=purple, line width=1.5pt] coordinates {(-16.5,5.8) (16.5,5.8)};
\addplot[dash pattern=on 1pt off 3pt, color=purple, line width=1.5pt] coordinates {(1.85,1.25) (16.5,5.5)};
\addplot[dash pattern=on 1pt off 3pt, color=purple, line width=1.5pt] coordinates {(16.5,5.5) (16.5,5.8)};
\addplot[dash pattern=on 1pt off 3pt, color=purple, line width=1.5pt] coordinates {(-1.85,1.25) (-16.5,5.5)};
\addplot[dash pattern=on 1pt off 3pt, color=purple, line width=1.5pt] coordinates {(-16.5,5.5) (-16.5,5.8)};
\addplot[dash pattern=on 1pt off 3pt, color=purple, line width=1.5pt] coordinates {(-1.85,.8) (1.85,.8)};
\addplot[dash pattern=on 1pt off 3pt, color=purple, line width=1.5pt] coordinates {(-16.5,0) (16.5,0)};
\addplot[dash pattern=on 1pt off 3pt, color=purple, line width=1.5pt] coordinates {(1.85,.8) (16.5,.3)};
\addplot[dash pattern=on 1pt off 3pt, color=purple, line width=1.5pt] coordinates {(16.5,0) (16.5,.3)};
\addplot[dash pattern=on 1pt off 3pt, color=purple, line width=1.5pt] coordinates {(-1.85,.8) (-16.5,.3)};
\addplot[dash pattern=on 1pt off 3pt, color=purple, line width=1.5pt] coordinates {(-16.5,0) (-16.5,.3)};
\addplot[dash pattern=on 1pt off 3pt, color=purple, line width=1.5pt] coordinates {(-9.2,9.2) (9.2,9.2)};
\addplot[dash pattern=on 1pt off 3pt, color=purple, line width=1.5pt] coordinates {(-9.5,9.2) (-2.1,1.8)};
\addplot[dash pattern=on 1pt off 3pt, color=purple, line width=1.5pt] coordinates {(9.5,9.2) (2.1,1.8)};
\node at (axis cs:9,4) [pin={+90:\small },inner sep=0pt] {};
\node at (axis cs:4,9) [pin={+30:\small },inner sep=0pt] {};
\node at (axis cs:9,9) [pin={+0:\small },inner sep=0pt] {};
\node at (axis cs:-9,.5) [pin={+98:\small },inner sep=0pt] {};
\end{axis}
\end{tikzpicture}
\caption{Polyhedral faces from an  subspace, with different convexities  
(note that the blue and orange plots are convex w.r.t. the -axis, whereas the green and brown are concave).
Dotted purple lines represent supports for some of these layouts.}
\label{aa:mixed_supports}
\end{figure}


An additional drawback of \cite{JSS14} is that calculating the exact
Jordan form of any matrix is computationally expensive and hard to
achieve for large-dimensional matrices.  
We will instead use numerical algorithms in
order to get an approximation of the Jordan normal form
and account for numerical errors. In
particular, if we examine the nature of
\eqref{eq:aa_reach_tube}, we find out that the
numerical operations are not iterative, therefore the errors do not
accumulate with time.  We use properties of
eigenvalues to relax  by finding the maximum error in the
calculations that can be determined by computing the norm
,
where  and  are the
numerically calculated eigenvalues and eigenvectors of .
The notation above is used to represent the matrices as interval
matrices and all operations are performed using interval arithmetic
with outward rounding in order to ensure soundness. In the following
we will presume exact results and use the regular notation to describe the algorithms. 
The constraints  are then computed by
considering the ranges of eigenvalues  
(represented in Fig.~\ref{aa:supports} as the diameter of the blue dots).
The outward relaxation of the support functions (), which follows
a principle similar to that introduced in~\cite{gao2012delta}, reduces the
tightness of the over-approximation, but ensures the soundness of the
abstract matrix  obtained. It is also worth noting that
the transformation matrices into and from the eigenspace will also
introduce over-approximations due to the intervals.
One can still use exact arithmetic with a noticeable improvement over
previous work; however, for larger-scale systems the option of using
floating-point arithmetic, while taking into account errors and meticulously
setting rounding modes, provides a -fold plus improvement,
which can make a difference towards rendering verification practically feasible.
For a full description on the numerical processes described here see \cite{cattaruzza2017sound}

\subsection{Abstract Matrices in Support Functions}\label{sec:absmatinsupfunc}


Since we are describing operations using abstract matrices and support
functions, we briefly review the nature of these operations and
the properties that the support functions retain within this domain.  Let  be a space and 
an abstract matrix for the same space.  From the definition we have

which leads to 

where

and

Here,  is the Hadamard product, where , and  is the reverse operation of  in order to align the elements on  with the elements in . In the case of conjugate pairs this is equivalent to multiplying the vector section by , and in the case of a Jordan block by an upper triangular matrix of all ones.\\

We may also define

In order to simplify the nomenclature we write


\subsection{Acceleration of Parametric Inputs}


Let us now consider the following over-approximation for  on sets:

Unfolding~\eqref{equ:reachset} (ignoring the presence of the guard set  for the time being), we obtain 
What is left to do is to further simplify the sum .
We can exploit the following simple results from linear algebra. 
\begin{lemma}
\label{eq::accel}
If  is invertible, then 
.  
If furthermore , then 
. 
\end{lemma}

This lemma presents a difficulty in the nature of . The inverse ,
does not exist for eigenvalues of , i.e. we need , 
where  is the spectrum (the set of all the eigenvalues) of matrix .
In order to overcome this problem, we introduce the eigen-decomposition of , 
(and trivially ), and by the distributive and transitive properties we obtain 

This allows us to accelerate the eigenvalues individually, using the property  for eigenvalues of .
Using the properties above, and translating the problem into the generalised eigenspace to accounting for
unit eigenvalues, we obtain the following representation:

where  is the geometric multiplicity of the given eigenvalue.

\subsection{Acceleration of Variable Inputs}\label{sec:var_inpur_accel}


The result in the previous section can be only directly applied under restricted conditions in the case of variable inputs. For
instance whenever . 
In order to generalise it (in particular to non-constant inputs), we will
over-approximate  over the eigenspace by a semi-spherical enclosure with
centre  and radius .  To this end, we first rewrite

where  is the centre of the interval hull of :

We then over-approximate  via , by the maximum radius in
the directions of the complex eigenvalues as (cf. illustration in Figure~\ref{aa:varinputs}). Let


and red() is a function that reduces the dimension of a vector by removing the elements where .
Extending this to matrices we have

Finally


Since the description of  is no longer polyhedral in ,
we will also create a semi-spherical over-approximation  of
 in the directions of the complex eigenvectors, in a
similar way as we generated  for . 
More precisely,

where  is the geometric multiplicity of the Jordan block.
\begin{definition}
Given a matrix  and a vector , we define the following operations:

\end{definition}
Finally, we refer to the accelerated sets 


Returning to our original equation for the -reach set, we obtain\footnote{Note that  so that  and .  
Hence, this inclusion is also valid in the original state space. }



\begin{figure}
\centering
\begin{tikzpicture}[scale=0.9]
\begin{axis}[ylabel={},xlabel={},xmin=-50,xmax=50,ymin=-40,ymax=40]
\addplot[color=blue,mark=*] table{
40.6 23
37.1 24.75
5.95 24.75
4.95 22.75
-4.95 -0.25
8.45 -8
39.6 -2
40.6 0
40.6 23
};
\addplot[dashed, color=blue,mark=*] table{
22.7 11.63
19.2 13.38
-12.05 13.38
-13.05 11.38
-22.85 -11.62
-10.35 -19.37
21.7 -13.37
22.7 -11.37
22.7 11.63
};
\addplot[color=purple,mark=*] table{
17.9 11.37
0 0
};
\draw[color=red] (axis cs:17.9,11.37) circle[radius=260];
\draw[dashed,color=orange] (axis cs:0,0) circle[radius=260];
\node at (axis cs:17.9,11.37) [pin={-10:},inner sep=0pt] {};
\node at (axis cs:0,0) [pin={-10:},inner sep=0pt] {};
\node at (axis cs:12.1,-23.37) [pin={-10:},inner sep=0pt] {};
\node at (axis cs:8,13) [pin={+10:},inner sep=0pt] {};
\node at (axis cs:32,-4) [pin={-10:},inner sep=0pt] {};
\end{axis}
\end{tikzpicture}
\caption{Relaxation of an input set within a complex subspace, 
in order to make it invariant to matrix rotations.  
Dashed lines and curves denote translated quantities onto the origin. }
\label{aa:varinputs}
\end{figure}

Shifting our attention from reach sets to tubes, 
we can now over-approximate the {\it reach tube} by abstract acceleration of the three summands in (\ref{equ:accinput2}), as follows. 
\begin{theorem}
The abstract acceleration

is an over-appro\-xi\-mation of the -reach tube, 
namely .
\end{theorem}
\begin{proof}
The proof is derived from that in~\cite{JSS14} for , 
and extends it as in the developments presented above.
\end{proof}

\subsection{Combining Abstract Matrices}\label{sec:comb_matrices}


One important property of the abstract matrices ,
 and  is that they are correlated.  In~the case of parametric
inputs, this correlation is linear and described by the acceleration defined
in Lemma \eqref{eq::accel}.  In the case of  this
relationship is not linear (see Eq.~\ref{eq:overball}).  However, we can
still find a linear over-approximation of the correlation between
 and  based on the time steps .  Given
two orthonormal spaces  and a
transition equation

which is related to 

we define a space 

so that 

with 

Accelerating , we obtain 

with 

in the case of parametric inputs.  More generally, the diagonal elements of
 correspond to the diagonal elements of  and
,  which means we can construct

We can then apply this abstraction to (\ref{eq:overball}) and obtain:

with  defined by \eqref{eq:round_jordan}. 
This model provides a tighter over-approximation than \eqref{equ:absaccinput} since the accelerated dynamics of the inputs are now constrained by the accelerated dynamics of the system.

\section{Abstract Acceleration with Guards: Estimation of the number of Iterations} \label{sec:guards}


The most important task remaining is how to calculate the number of iterations dealing with the presence of the guard set . 

Given a convex polyhedral guard expressed as the assertion , 
we define  as the  row of  and  as the corresponding
element of . 
We denote the normal vector to the  face of the guard as .  
The distance of the guard to the origin is thus .

Given a convex set , we may now describe its position with respect to
each face of the guard through the use of its support function alongside the normal
vector of the hyperplane (for clarity, we assume the origin to be inside set ): 

Applying this to equation \eqref{equ:accinput2} we  obtain:


From the inequalities above we can determine up to which number of iterations  the reach tube remains inside the corresponding hyperplane, 
and starting from which iteration  the corresponding reach set goes beyond the guard:


In order for a reach set to be inside the guard it must therefore be inside
all of its faces, and we can ensure it is fully outside of the guard set
when it is fully beyond any of them.  Thus, we have , and .

We have not however discussed why these two cases are important. Looking at the transition in equation \eqref{equ:reachtraj}, we can easily derive that if  the postimage of all subsequent iterations is empty. Therefore, any overapproximation henceforth will only add imprecision. We will use the bounds  and  to create a tighter overapproximation.
Let 

This double step prevents the set  to be included in further projections, thus reducing the size of the overapproximation. 

\medskip 
Computing the maximum  such that \eqref{equ:underiter} is satisfied is not easy 
because the unknown  occurs in the exponent of the equation.
However, since an intersection with the guard set will always return a
sound over-approximation, we do not need a precise value. We can over-approximate it by decomposing  into the generalised eigenspace of .
Let , 
where  are row vectors of  or  such that , 
and  is the component of  that lies outside the range of .  
Notice that since  has an inverse, it is full rank and therefore
 and subsequently not relevant.  It is also
important to note that  is the matrix of generalised eigenvectors
of  and therefore we are expressing our guard in the generalised
eigenspace of .


\subsection{Overestimating the Iterations of a loop without inputs} \label{sec:guards_noinputs}
We start by looking into the approximation of the inside bound (i.e. the iterations for which the
reachtube remains fully inside the guard). Since rotating dynamics and Jordan shapes will have
a complex effect on the behaviour of the equation, we seek to transform the Jordan form into a
real positive diagonal. In such a case, the progression of the support function in each direction
is monotonically increasing (or decreasing) and it is therefore very easy to find a bound for its 
progression. We note that the envelope of rotating dynamics will always contain the true
dynamics and is therefore a sound overapproximation. We will initially assume that 
 is positive and then extend to the general case.

Let  such that




Let


and red() is a function that reduces the dimension of a vector by removing the elements where . This reduction is not extrictly necessary, but it enables a faster implementation by reducing dimensionality.
Correspondingly, given 

where  is the maximum singular value (hence the induced norm ~\cite{LT84}) of the Jordan block .


Finally, let

and .

Using eigenvalue and singular value properties, we
obtain , and
therefore:


Since we have no inputs, , hence we may solve for :


To separate the divergent element of the dynamics from the
convergent one, let us define

This step will allow us to track effectively which trajectories are
likely to hit the guard and when, since it is only the divergent
element of the dynamics that can increase the reach tube in a given
direction.

Replacing \eqref{eq:low_n}, we obtain

which allows to finally formulate an iteration scheme for approximating .

\begin{proposition}\label{prop:real_under_n}
  An iterative under-approximation of the number of iterations  can
  be computed by starting with  and iterating over

substituting  on the right-hand side until we meet the inequality. 
\end{proposition}
\begin{proof}
This follows from the developments unfolded above. 
Notice that the sequence  is monotonically increasing, before it breaks the inequality. 
As such any local minimum represents a sound under-approximation of the number of loop iterations. 
Note that in the case where  we must first translate the system coordinates such that . 
This is simply done by replacing  and operating over the resulting system where  .

Mathematically this is achieved as follows: first we get  by finding the center of the interval hull of  (if  is open in a given direction we may pick any number in that direction for the corresponding row of ). Next we transform the dynamics into

where 

\end{proof}

\subsection{Underestimating the Iterations of a loop without inputs} \label{sec:guards_under}
In order to apply a similar techniques to \eqref{equ:overiter} we must find an equivalent under-approximation. In the case of equation \eqref{eq:low_n}, the  esure that the equation diverges faster than the real dynamics, hence the iteration found is an upper bound to the desired iteration. In this case we want the opposite, hence we look for a model where the dynamics diverge slower. In this case it is easy to demonstrate that  represents these slower dynamics.

which reduces to

where


An additional consideration must also be made regarding the rotational nature of the dynamics. In the previous case we did not care about the rotational alignment of the set  with respect to the vector , because any rotation would move the set inside the guard. In this case, although the magnitude of the resulting vector is greater than the required one, the rotation may cause it to be at an angle that keeps the set inside the guard. 
We must therefore account for the rotating dynamics in order to find the point where the angles align with the guard. In order to do this, let us first fix the magnitudes of the powered eigenvalues, in the case of convergent dynamics we will assume they have converged a full rotation in  to make our equation strictly divergent. Let , where  are the angles of the complex conjugate eigenvalues. Let  be the maximum number of iterations needed for any of the dynamics to complete a full turn. Then at any given turn .
This means that any bound we find on the iterations will be necessarily smaller than the true value. Our problem becomes the solution to:

The problem is simplified by underapproximating the cosines and removing the constants:

The solution to this equation is 

The second part of the equation is expected to be a positive value. When this is not the case, the dominating dynamics will have a rotation . In such cases we must explicitly evaluate the set of up to  iterations after .
If the resulting bound does not satisfy the original inequality: , we replace  until it does \footnote{this is a tighter value than work shown on previous versions of this paper where we overapproximated using , where  is the number of conjugate pairs.}.

\begin{proposition}\label{prop:real_under_n}
  An iterative under-approximation of the number of iterations  can
  be computed by starting with  and iterating over

where  is the result of equation \eqref{eq:circ_iters}.
we substitute for  on the right-hand side until we break the inequality, and then find  such that the second inequality holds.
\end{proposition}
Since we are explicitly verifying the inequality, there is no further proof required.
\subsection{Estimating the Iterations of a loop with inputs} \label{sec:guards_inputs}
For the case with inputs, we will use the same paradigm explained in the previous section after performing a mutation that transforms the system with inputs into an over-approximating system without inputs.

Let  be the corresponding sets of initial states and inputs obtained by applying equation \eqref{eq:overball_init} to  and , and let . The accelerated resulting system may be represented by the equations

Let us now define  which allows us to translate the system into

which has the same shape as the equations in the previous section. We may now apply the techniques described above to find the bounds on the iterations.

\rronly{
\subsection{Narrowing the estimation of the iterations} \label{sec:guards_tight}
The estimations above are very conservative, but we may use further techniques to obtain tighter bounds on the number of iterations.
In the first instance we note that we have eliminated all negative terms in the sums in equation \eqref{eq:est_low_n}. Reinstating these terms can cause us to lose monotonicity, but we may still create an iterative approach by fixing the negative value at intermediate stages.
Let  be our existing bound for the time horizon before reaching a guard, and ,   the corresponding negative and positive terms of the equation.
We may now find upper and lower bounds for  by replacing the equation 

where  is the bound found in the previous stage. Some stages of this process will provide an unsound result, but they will also provide an upper bound to our number of iterations. 
In fact, every second stage will provide a monotonically increasing sound bound which will be tighter than the one in equation \eqref{eq:est_low_n}.
\begin{proof}
Since the elements of the sums are convergent, we have

which means that  in equation \eqref{eq:refine_iters} is smaller than or  in equation \eqref{eq:est_low_n} (.
\end{proof}

In the case of equation \eqref{eq:est_high_n}, the explicit evaluation of the guard at each cycle executes the behaviour described here.
}
\subsection{Maintaining Geometric Multiplicity} \label{sec:guards_geometric}
A second step in optimising the number of iterations comes from adding granularity to the bounding semi-spherical abstraction by retaining the geometric multiplicity using the matrix .

\begin{lemma}
\label{lemma:jordan_guards}
Given a matrix  with eigenvalues , where each eigenvalue  has a geometric multiplicity  and corresponding generalised eigenvectors ,
  
\end{lemma}
\begin{proof}
By definition, given an eigenvector  of , then ~\cite{horn2012matrix}. Similarly a generalised eigenvector  of  satisfies the equation  and  hence 

From here we recursively expand the formula for  and obtain:

\end{proof}

Let  denote the position of  within the block  it belongs to, such that its corresponding generalised eigenvector is identified as .
Then


I order to manage the product on the right hand side we use slightly different techniques for over- and under-approximations.
For  we first find an upper bound  using equation \eqref{eq:est_low_n} and 
and then do a second iteration using  which ensures the true value is under the approximation.
In the case of , we also start with  and update it during the iterative process.

Let us look at the following example:




The progression of the system along the support function and corresponding bounds as described in the previous section are shown in figure~\ref{fig:iters}


\begin{figure}[]
\centering
\begin{tikzpicture}[scale=0.8]
\begin{semilogyaxis}[ylabel={},xlabel={iteration}, log ticks with fixed point]\addplot[color=blue,mark=*] table{
1 6
2 2
3 29
4 43
5 169
6 331
7 1213
8 2603
};
\addplot[dash pattern=on 2pt off 2pt, color=green!50!black,mark=.] table{
1 12
2 22.8
3 49
4 160.6
5 308
6 840.2
7 2371
8 6895.6
};
\addplot[dash pattern=on 2pt off 2pt, color=yellow!50!black,mark=.] table{
3 12
3 20.8
4 39
5 78.6
6 173
7 412.6
8 1041
};
\addplot[color=purple,mark=.] table{
1 12
2 21.8
3 43
4 90.6
5 205
6 472.6
7 1425
8 3225.6
};\addplot[color=purple,mark=.] table{
3 12
3 21.8
4 43
5 90.6
6 205
7 472.6
8 1425
};]
\addplot[dash pattern=on 1pt off 3pt, color=red, line width=1.5pt] coordinates {(0,300) (10,300)};
\node at (axis cs:4,160) [pin={+90:\small },inner sep=0pt] {};
\node at (axis cs:7,412) [pin={-45:\small },inner sep=0pt] {};
\node at (axis cs:5,205) [pin={-45:\small },inner sep=0pt] {};
\node at (axis cs:7,473) [pin={0:\small },inner sep=0pt] {};
\end{semilogyaxis}
\end{tikzpicture}
\caption{Progression of the support function of a system for a given guard. Blue dots are real values.
The dashed green line overapproximates the progression using singular values (sec~\ref{sec:guards_noinputs}), 
the dashed yellow line underapproximates them using eigenvalue norms (sec~\ref{sec:guards_under}), 
whereas the continuous purple lines represent the tighter overapproximation maintaining the gemoetric 
multiplicity (sec~\ref{sec:guards_geometric}). We can see how the purple line finds a
better bound for , while the  bound is conservative for both approaches. Mind the logarithmic scale.}
\label{fig:iters}
\end{figure}

Changing the eigenvalues to:


we get the results in figure~\ref{fig:round_iters}. In this case we can see that the rotational dynamics force an increase of the initially calculated iteration to account for the effects of the rotation.

\begin{figure}[]
\centering
\begin{tikzpicture}[scale=0.8]
\begin{semilogyaxis}[ylabel={},xlabel={iteration}, log ticks with fixed point]\addplot[color=blue,mark=*] table{
1 9.43
2 5.08
3 0.67
4 27.72
5 57.58
};
\addplot[color=blue,mark=*] table{
6 -7.79
7 -171.74
8 -197.26
};
\addplot[color=blue,mark=*] table{
9 120.35
};
\addplot[color=blue,mark=*] table{
10 -20.24
11 -2778.33
12 -8156.45
13 -8912.76
};
\addplot[color=blue,mark=*] table{
14 51.58
};
\addplot[color=blue,mark=*] table{
15 -8545.78
16 -86240.20
17 -175680.75
};
\addplot[color=blue,mark=*] table{
18 487.48
19 542552.47
20 591848.64
};
\addplot[color=purple,mark=.] table{
1 15.94
2 22.47
3 33.80
4 54.25
5 92.24
6 164.40
7 303.58
8 575.00
9 1108.38
10 2162.15
11 4251.72
12 8405.95
13 16679.66
14 33178.53
15 66108.22
16 131872.05
17 263265.32
18 525862.65
19 1050790.65
20 2100270.50
};
\addplot[color=purple,mark=.] table{
4 15.94
4 22.47
5 33.80
5 54.25
9 92.24
19 164.40
19 303.58
19 575.00
19 1108.38
19 2162.15
19 4251.72
19 8405.95
19 16679.66
19 33178.53
19 66108.22
19 131872.05
19 263265.32
19 525862.65
};
\addplot[dash pattern=on 1pt off 3pt, color=red, line width=1.5pt] coordinates {(0,300) (20,300)};
\node at (axis cs:6,164) [pin={-45:\small },inner sep=0pt] {};
\node at (axis cs:19,303) [pin={0:\small },inner sep=0pt] {};
\end{semilogyaxis}
\end{tikzpicture}
\caption{Progression of the support function of a rotational system for a given guard. Blue dots are real values 
(negative values are missing due to the log scale).
Continuous purple lines represent the overapproximation. The steep vertical line at 19 is due to the alignment of the rotations with the guard at this point. The point at iteration 14 appears below the line because of the higher point at iteration 9. The model will either find that this boundary was met at iteration 9 or push it forward to 19.}
\label{fig:round_iters}
\end{figure}

\subsection{Case Study}
We have selected a known benchmark to illustrate the discussed procedure:  
the room temperature control problem \cite{Fehnker04benchmarksfor}. 
The temperature (variable \texttt{temp}) of a room is controlled to a user-defined set point (\texttt{set}), 
which can be changed at any time through a heating (\texttt{heat}) element, 
and is affected by ambient temperature (\texttt{amb}) that is out of the control of the system. 

We formalise the description of such a system both via a linear loop and via hybrid dynamics.  
Observe that since such a system may be software controlled, 
we assume that part of the system is coded, 
and further assume that it is possible to discretise the physical environment for simulation. 
Algorithm \ref{alg:temp_code} shows a pseudo-code fragment for the temperature control problem.
\begin{algorithm}
\caption{Temperature Control Loop}
\textbf{States:} temp=temperature, heat=heat output.\\
\textbf{Inputs:} set=set-point, amb=ambient temperature.
\begin{algorithmic}[1]
\State temp=5+read(35);
\State heat=read(1);
\State while(temp  heat)
\State \{
\State     \hspace{.5cm} amb=5+read(35);
\State     \hspace{.5cm} set=read(300);
\State     \hspace{.5cm} temp=.97 temp + .02 amb + .1 heat;
\State     \hspace{.5cm} heat=heat + .05 set; 
\State \}
\end{algorithmic}
\label{alg:temp_code}
\end{algorithm}
We use the \texttt{read} function to represent non-deterministic values
between 0 and the maximum given as argument.  Alternatively, this loop
corresponds to the following hybrid dynamical model:

with initial condition

non-deterministic inputs

and guard set


In this model the variables are continuous and take values over the real line, 
whereas within the code they are represented as long double precision floating-point values, with precision of , 
moreover the error of the approximate Jordan form computation results in . 
Henceforth we focus on the latter description, as in the main text of this work.
The eigen-decomposition of the dynamics is (the values are rounded to three decimal places):

The discussed over-approximations of the reach-sets indicate that the
temperature variable intersects the guard at iteration . 
Considering the pseudo-eigenvalue matrix (described in the extended version for the case of complex eigenvalues)
along these iterations, we
use Equation~\eqref{abs:mat} to find that the corresponding complex pair
remains within the following boundaries:
 

The reach tube is calculated by multiplying these abstract matrices with the
initial sets of states and inputs, as described in
Equation~\eqref{equ:absaccinput}, by the following inequalities:

The negative values represent the lack of restriction in the code on the lower side and correspond to system cooling (negative heating). 
The set is displayed in Figure~\ref{fig:abs}, where for the sake of clarity we display only 8 directions of the 16 constraints. 
This results in a rather tight over-approximation that is not much looser than the convex hull of all reach sets obtained by \cite{FLD+11} using the given directions. 
In Figure~\ref{fig:abs}, we can see the initial set in black colour, 
the collection of reach sets in white, 
the convex hull of all reach sets in dark blue (as computed by \cite{FLD+11}), 
and finally the abstractly accelerated set in light yellow (dashed lines). 
The outer lines represent the guards. 

\begin{figure}[]
\centering
\begin{tikzpicture}[scale=0.7]
\begin{axis}[
height=0.55\textwidth,
ylabel={},
xlabel={},]
\addplot[color=red,domain = -20:420] {300};
\node at (axis cs:0,300) [pin={-10:},inner sep=0pt] {};
\addplot[color=red] coordinates {(400,-20) (400,310)};
\addplot[dashed,color=gray,fill=yellow] table{
393.548 235.724
384.908 253.004
384.908 253.004
384.908 253.004
223.005 253.004
110.38 196.691
4.42517 90.7365
-26.1104 29.6655
-26.1104 -13.1998
-19.9766 -25.4673
-17.9272 -27.5167
-12.3942 -30.2832
58.4534 -30.2832
246.894 63.9373
384.232 201.275
393.548 219.906
393.548 235.724
};
\addplot[color=blue,fill=blue] table{
390.191 219.721
387.012 226.079
384.546 228.545
361.243 240.197
227.71 240.197
103.086 177.885
39.4593 114.258
-19.2963 -3.253
-19.2963 -17.3902
-16.1173 -23.7483
-13.6509 -26.2146
-7.92582 -29.0772
10.0934 -29.0772
272.694 102.223
390.191 219.721
390.191 219.721
390.191 219.721
};
\addplot[red] table{./Reach32.dat};
\addplot[blue,fill=white] table{./Reach31.dat};
\addplot[blue,fill=white] table{./Reach30.dat};
\addplot[blue,fill=white] table{./Reach29.dat};
\addplot[blue,fill=white] table{./Reach28.dat};
\addplot[blue,fill=white] table{./Reach27.dat};
\addplot[blue,fill=white] table{./Reach26.dat};
\addplot[blue,fill=white] table{./Reach25.dat};
\addplot[blue,fill=white] table{./Reach24.dat};
\addplot[blue,fill=white] table{./Reach23.dat};
\addplot[blue,fill=white] table{./Reach22.dat};
\addplot[blue,fill=white] table{./Reach21.dat};
\addplot[blue,fill=white] table{./Reach20.dat};
\addplot[blue,fill=white] table{./Reach19.dat};
\addplot[blue,fill=white] table{./Reach18.dat};
\addplot[blue,fill=white] table{./Reach17.dat};
\addplot[blue,fill=white] table{./Reach16.dat};
\addplot[blue,fill=white] table{./Reach15.dat};
\addplot[blue,fill=white] table{./Reach14.dat};
\addplot[blue,fill=white] table{./Reach13.dat};
\addplot[blue,fill=white] table{./Reach12.dat};
\addplot[blue,fill=white] table{./Reach11.dat};
\addplot[blue,fill=white] table{./Reach10.dat};
\addplot[blue,fill=white] table{./Reach9.dat};
\addplot[blue,fill=white] table{./Reach8.dat};
\addplot[blue,fill=white] table{./Reach7.dat};
\addplot[blue,fill=white] table{./Reach6.dat};
\addplot[blue,fill=white] table{./Reach5.dat};
\addplot[blue,fill=white] table{./Reach4.dat};
\addplot[blue,fill=white] table{./Reach3.dat};
\addplot[blue,fill=white] table{./Reach2.dat};
\addplot[blue,fill=white] table{./Reach1.dat};
\addplot[blue,fill=black] table{./Reach0.dat};
\end{axis}
\end{tikzpicture}
\caption{The abstractly accelerated tube (yellow, dashed boundary),
  representing an over-approximation of the thermostat reach tube
  (dark blue).  The set of initial conditions is shown in black, whereas
  successive reach sets are shown in white.  The guards and the
  reach set that crosses them are close to the boundary in red.
} \label{fig:abs}
\end{figure}

\jrronly{
\section{Applying Abstraction Refinements to Abstract Acceleration}\label{sec:aa_CEGAR}


One of the main limitations of Abstract Acceleration is that despite being very fast, it leverages
on finding an over-approximation of the actual reach-tube for verification. While in many cases this 
over-approximation is suitable, it can occasionally be too conservative for the proof of the properties being sought.
This section deals with methods for refining this over-approximation while remaining fast. In all cases, 
the refinement is based on a counterexample of a support not meeting the specification. This means that
our approach is a Counterexample- Guided Abstraction Refinement (CEGAR) loop for which we use different 
models to obtain the refinements.

\subsection{Finding Counterexample Iterations}\label{sec:cegar_iters}
In all cases, because the objective is to refine the abstract dynamics, we need to find the corresponding 
iteration to the counterexample which will allow us to reduce the polyhedron in the right direction. Since the Abstract 
Dynamics are built over pairs of eigenvalues, it is possible that multiple iterations are found for a given 
refinement, in which case all of them are used.

Let a verification step explore the solution , where 
 is the direction we are examining,  is its corresponding support function, and 
the safety specification. If  the specification will not be met and we need a refinement.
Let  be the vertex at which the maximum was found, i.e., .
We can discover a set of possible iterations reaching  by analysing its location with respect to
the dynamics in the following form:
\begin{enumerate}
\item Conjugate eigenvalues\\
Since the trajectories along these are circular and centred at the origin, we 
can find the angle that  forms with these axes and use it to calculate the right iteration. Let 
 be the angle of the conjugate eigenvalue pair and  the angle formed
by  in the  plane (since we are using pseudo-eigenspaces, this is equivalent to 
. 
The corresponding iteration will depend on whether the eigenvalue is convergent or divergent. In the former
case, it will be , and in the latter it will be ,
where  is the modulus operation over the reals.
\item Real Eigenvalues\\
In the case of reals, the solution relies on the direct relation between the given
eigenvalue and the target counterexample. Since . 
If the logarithm does not exist, then we presume we cannot further refine using this method.
\item Jordan Blocks with non-unitary gemoetric multiplicity\\
In the case of larger Jordan blocks we need to examine the nature of the dynamics. Let us look at the equation representing the contribution of a Jordan block to the support:

In this case we must use an iterative approximation as described in section \ref{sec:guards_geometric} to find the closest iteration to the breaking guard. Although this processis more costly than the ones described above, it is also more precise, thus providing a much better refinement. Note as well, that the technique can be applied to the full set of eigenvalues or to any subset of Jordan blocks. This choice is a compromise between precision and speed. We also note that when the refinement process is done in the eigenspace, the new eigenvectors are now the identity set, which makes the problem more tractable. 
\end{enumerate}
Since the exclusion of an unsafe vertex from the Abstract Dynamics does not ensure the tightest over-approximation, 
we must perform this step iteratively until either we run out of new refinements or have a user-defined timeout.
Once the candidate iterations are found, it suffices to add further constraints to the abstract matrix for these iterations as described in figure~\ref{fig:cegar_jordan_supports}.
We may also note that given the above procedure, it is often faster and more beneficial to begin by 
performing the refinement over the complex eigenvalues by directly examining the directions vector  
in the corresponding sub-spaces.
\begin{figure}[]
\centering
\begin{tikzpicture}
\begin{axis}[ylabel={},xlabel={},scale=.9]
\addplot[color=purple, fill=yellow, line width=1pt] coordinates{
(0.01,.62)
(0.1,1.5)
(0.25,2.07)
(0.42,2.07)
(0.61,1.84)
(0.82,1.02)
(0.82,0.96)
(0.02,0.6)
(0.01,0.62)
};
\addplot[color=blue,mark=*] coordinates{
(0.8,1)
(0.64,1.6)
(0.512,1.92)
(0.4096,2.048)
(0.3277,2.050)
(0.2621,1.966)
(0.2097,1.835)
(.1678,1.678)
(.1342,1.510)
(.1074,1.342)
(.0859,1.181)
(.0687,1.031)
(.0550,0.893)
(.0440,0.770)
(.0352,0.660)
};
\addplot[color=purple, line width=1pt] coordinates{
(0.01,.62)
(.1,1.5)
(.25,2.07)
(.55,2.07)
(0.82,1.02)
(0.82,0.96)
(0.02,0.6)
(0.01,0.62)
};
\addplot[dash pattern=on 1pt off 3pt, line width=1.5pt , color=blue] coordinates {(0.45,2.1) (0.82,1.75)};
\addplot[color=red,mark=*,only marks] coordinates {
(0.55,2.07)
};
\addplot[color=green, line width=1.5pt] coordinates {(0.42,2.07) (0.61,1.84)};
\node at (axis cs:0.8,1) [pin={+180:},inner sep=0pt] {};
\node at (axis cs:0.64,1.6) [pin={190:},inner sep=0pt] {};
\node at (axis cs:.512,1.92) [pin={-120:},inner sep=0pt] {};
\node at (axis cs:.512,1.92) [color=red,pin={-120:},inner sep=0pt] {};
\node at (axis cs:.3277,2.05) [pin={-90:},inner sep=0pt] {};
\node at (axis cs:0.55,2.07) [pin={0:},inner sep=0pt] {};
\node at (axis cs:0.78,1.78) [pin={-90:},inner sep=0pt] {};
\end{axis}
\end{tikzpicture}
\caption{Polyhedral faces from an  Jordan block subspace, where 
   so that .
The red dot specifies an abstract vertex violating the safety specification (dashed blue line).
The closest iteration to the violating vertex is n=3.
A new support function (green) based on n=3 eliminates the violating vertex.
The new abstract polyhedra meets the safety specification (yellow).}
\label{fig:cegar_jordan_supports}
\end{figure}
}
\jrronly{
\subsection{Using Bounded Concrete Runs}\label{sec:cegar_concrete}
Due to the complex interactions in the dynamics, the above procedure may not always find the correct iterations 
for refinement, or at least not optimal ones. For this reason, a second method is proposed that in most cases 
will be more efficient and precise when the dynamics are strictly convergent.
This second approach relies on the direct calculation of the initial  iterations. Since we operate
over the eigenvalues and we limit  to a conservative bound, this is a relatively inexpensive calculation.
The approach leverages on the idea that for convergent dynamics, counterexamples are often found in the initial runs.
The first step is to directly calculate the trajectory of the counterexample for the first  iterations, and its
corresponding support function in the direction of . Once again, because this is a single point and a bounded 
time, this operation is comparatively inexpensive. 
The second step consists of finding an upper bound for all subsequent iterations, which we can do by using the norms
of the eigenvalues and the peaks of each geometrical multiple of a Jordan Block (which relate to these norms).
By selecting the larger between these two supports, we ensure soundness over the infinite time horizon.
This is equivalent to evaluating the reach tube as .

Since the above result is know to be an upper bound for the support in the direction of  we can directly add it to the inequalities of . 

\rronly{
\subsection{Case Study}


We have taken an industrial benchmark `CAFF problem Instance: Building' from
the competition forum (\footnote{\url{http://cps-vo.org/node/30277}}).  The
benchmark consists of a continuous model with 48 state variables and one
input.  Furthermore, there is an initial state corresponding to a
10-dimensional hyperrectangle.  The model is discretised using a 5\,ms sample
time to give us a discrete time model for verification.  Notice that the
choice of sample time has very little effect on abstract acceleration.  It
mainly affects the requirement for floating point precision (as very small
angles may require higher precision), and may have an effect on
counterexample generation which can either decrease precision or increase
time-cost based on some algorithmic choices.  The provided model requires an
analysis on the  variable, with a safety specification requiring it
to remain below .  The problem has been verified using
SpaceEx\footnote{\url{http://spaceex.imag.fr}} in under 60 seconds.

The tool was run on this benchmark using different parameters.  We used an
Intel 2.6\,GHz I7 processor with 8\,GB of RAM running on Linux.  Although
the algorithm lends itself to concurrency, the tool currently supports only
single threading.  The process itself uses 82\,MB on this particular
benchmark.  The results are summarised in Table~\ref{table:results}.  It is
worth noting that for precisions under 1024 bits the tool returns soundness
errors when using sound arithmetic.



\begin{table*}[t!]
\centering
\footnotesize
\begin{tabular}{|l|*{5}{@{\;\;}c@{\;}}|}
\hline
Parameters &Sound&Inputs&Bits&Time&Bound \\  \hline \hline
-mp 128 -params "p=48,q=1,l=2" -templates "" & No &P & 128 &  & \\  \hline
-mp 128 -params "p=48,v=1:1,l=2" -templates "" & No  & V & 128 &  & \\  \hline \hline
-mp 128 -params "p=48,q=1,l=2" -sguard "" & No  & P & 128 &  & \\  \hline
-mp 128 -params "p=48,v=1:1,l=2" -sguard "" & No  & V & 128 &  & \\  \hline
-mp 1024 -params "p=48,q=1,l=2" -sguard "" & No  & P & 1024 &  & \\  \hline
-mp 1024 -params "p=48,v=1:1,l=2" -sguard "" & No  & V & 1024 &  & \\  \hline \hline
-mpi 1024 -params "p=48,q=1,l=2" -sguard "" & Yes  & P & 1024 &  & \\  \hline
-mpi 1024 -params "p=48,v=1:1,l=2" -sguard ""& Yes  & V & 1024 &  & \\  \hline
\end{tabular}~\
\dot{x}(t)=\mat{A}_o\vec{x}(t)+\mat{B}_o\vec{u}(t)
\label{eq:dynamical}

\vec{y}(t)=\mat{C}_o\vec{x}(t)+\mat{D}_o\vec{u}(t)

\label{eq:discretization}
\vec{x}_{k+1} &= \mat{A}_d\vec{x}_k+\mat{B}_d\vec{u}_k\\
y_k &= \mat{C}_d \vec{x}_ k + \mat{D}_d \vec{u}_ k 

\label{eq:discretize}
\mat{A}_d &= e^{\mat{A}_o T_s} = \mathcal{L}^{-1} { ( s \mat{I} - \mat{A}_o )^{-1} }_{t = T_s}\\
\mat{B}_d &= \int_{t = 0}^{T_s} e^{\mat{A}_o t} dt\ \mat{B}_o = \mat{A}_o^{-1} ( \mat{A}_d - \mat{I} ) \mat{B}_o\\
\mat{C}_d &= \mat{C}_o\\
\mat{D}_d &= \mat{D}_o

x(kT)=x_k \text{ and } y(kT) = y_k, \forall k
G_d(z)=G(z)|_{z=e^{sT}} : g_d(k)=\mathcal{G}(t,g(t),T) \wedge T < \frac{1}{0.5f_s}
 \vec{x}_T&=\vec{x}(t=T)=\mat{A}_{T}\vec{x}_0\\
 \mat{A}_{T}&= \mat{S}
 \left [ \begin{array}{cccc}
 e^{T\lambda_1}  & s_1\frac{T^{1}e^{T\lambda_i}}{(2)!} & \hdots  & s_i\frac{T^{p-1}e^{T\lambda_i}}{(p-i)!} \\
0 & e^{T\lambda_i}  & s_i\frac{T^{j-i}e^{T\lambda_i}}{(j-i)!} & \vdots \\
\vdots & & \ddots & \vdots \\
0 & \cdots & 0  &e^{T\lambda_i} \\
\end{array} \right ]
 \mat{S}^{-1}
 \label{eq:continuous_tube_dyn}\\
 &\text{where } s_i=\left\{\begin{array}{cc}1&gm(\lambda_i)>1\\0&gm(\lambda_i)=1\end{array}\right.,\nonumber
 
 e^{\mat{A}}&=\sum_{k=0}^\infty \frac{1}{k!}\mat{A}^k
 
 e^{\mat{S}\mat{J}\mat{S}^{-1}}&=\sum_{k=0}^\infty \frac{1}{k!}\left(\mat{S}\mat{J}\mat{S}^{-1}\right)^k
 =\mat{S} \left (\sum_{k=0}^\infty \frac{1}{k!}\mat{J}^k\right) \mat{S}^{-1}\nonumber\\
 &=\mat{S}e^{\mat{J}}\mat{S}^{-1}

\mat{A}=\mat{S}\mat{J}\mat{S}^{-1} \Rightarrow \vec{A}_1 = \mat{S}\mat{J}_1\mat{S}^{-1}= \mat{S}e^{\mat{J} T_1}\mat{S}^{-1}.
 {\lambda_1}_i=e^{\lambda_i T_1}, \forall \lambda_i \in \mat{J}.{\lambda_2}_i=e^{\lambda_i T_2}, \forall \lambda_i \in \mat{J}.
 {\lambda_2}_i=e^{\lambda_i T_1 \frac{T_2}{T_1}}={\lambda_1}_i^{\frac{T_2}{T_1}} \Rightarrow A_2=A_1^{\frac{T_2}{T_1}}.
 
 \mat{J}_1&=\sum_{k=0}^\infty \frac{1}{k!}\left(\mat{J}T_1\right)^k\nonumber\\
 &=\sum_{k=0}^\infty \frac{1}{k!} \left [ \begin{array}{cccc}
 \lambda_i^k  & \binom{k}{1}  \lambda^{k-1} & \hdots  & \binom{k}{p-1} \lambda_i^{k-p+1} \\
& \lambda_i^k  & \binom{k}{1}  \lambda_i^{k-1} & \vdots \\
\vdots & \ddots & \ddots & \vdots \\
& &  &\lambda_i^k \\
\end{array} \right ] T_1^k

\forall j>i, c_{ij}&=\sum_{k=j-i}^\infty \frac{1}{k!}\binom{k}{j-i} \lambda_i^{k-(j-i)}T_1^k\\
&=\frac{1}{(j-i)!}\sum_{k=j-i}^\infty \frac{1}{(k-(j-i))!} \lambda_i^{k-(j-i)}T_1^k\nonumber\\
&=\frac{T_1^{j-i}e^{\lambda_i T_1}}{(j-i)!}\nonumber

 \vec{x}_T&=\vec{x}(t=T)=\mat{A}^{-1}\vec{x}_T'\nonumber\\
\vec{x}_T'&=\mat{A}\mat{A}_T\vec{x}_0 + (\mat{I}-\mat{A}_T)\mat{B}\vec{u} : \forall t \leq T,\ \vec{u}(t)=\vec{u} 
 
 \vec{x}_T&=\mat{A}^{-1}\vec{x}_T'\nonumber\\
 &=\mat{A}_T\vec{x}_0 + \mat{A}^{-1}(\mat{I}-\mat{A}_T)\mat{B}\vec{u}\nonumber\\
 &= \mat{A}_T\vec{x}_0 + \mat{B}_T\vec{u}
 
\label{eq:contu_reachset}
X_T^\sharp &=\mat{A}^{-1}X_T'\supseteq X_T \text{ where }\\
X_T'&=\mat{A}\mat{A}_TX_0 \oplus F_b^*\left((\mat{I}-\mat{A}_T),U_d\right) \oplus (\mat{I}-\mat{A}_T)U_c \nonumber

 X_k\subseteq A_d^kX_0 &\oplus F_b^*\left((\mat{I}-\mat{A}_d^k)F_b^*\left((\mat{I}-\mat{A}_d)^{-1},U_d\right)\right)\nonumber\\
 &\oplus (\mat{I}-\mat{A}_d^k)(\mat{I}-\mat{A}_d)^{-1}U_c
 
\vec{x}_T=e^AT\vec{x}_0+\int_{t = 0}^{T_s} e^{\mat{A} t} \mat{B} \vec{u}(t) dt

\vec{x}_T&=e^{AT}\vec{x}_0+\int_{t = 0}^{T} e^{\mat{A} t} \mat{B} \vec{u}_c dt +\int_{t = 0}^{T_s} e^{\mat{A} t} \mat{B} (\vec{u}(t)-\vec{u}_c) dt\\
&=A_T\vec{x}_0+B_T\vec{u}_c+\int_{t = 0}^{T} \sum_{k=0}^\infty \frac{1}{k!} \mat{A}^k t^k \mat{B}(\vec{u}(t)-\vec{u}_c) dt

X_T \subseteq &A_TX_0+B_T\vec{u}_c\\
&+\int_{t = 0}^{T} \sum_{k=0}^\infty \frac{1}{k!}t^k F_b^*\left(\mat{A}^k, \mat{B}(U-\vec{u}_c)\right) dt\\
\subseteq& A_TX_0+B_T\vec{u}_c+\int_{t = 0}^{T_s} F_b^*\left(e^{\mat{A} t},\mat{B}(U-\vec{u}_c)\right) dt\\
\subseteq& A_TX_0+B_T\vec{u}_c+ \mat{A}^{-1}F_b^*\left((\mat{I}-\mat{A}_{T}),\mat{B}(U-\vec{u}_c)\right)

\forall t \geq 0, \mat{A}_t\vec{v}_{s}^{i}&=e^{\lambda_{s}}\vec{v}_{s,i}+\sum_{j=1}^{i-1} t^je^{\lambda_{s}}\vec{v}_{s,i-j}\nonumber\\
&=e^{\lambda_{s}}\left( \vec{v}_{s,i} + \sum_{j=1}^{i-1} t^j\vec{v}_{s,i-j}\right)
\label{eq:cont_jordan_iters}

e^{\mat{A}t}\vec{v}_{s,i}&=\sum_{k=0}^\infty \mat{A}^k\frac{t^k}{k!}\vec{v}_{s,i}=\sum_{k=0}^\infty \frac{t^k}{k!}\left( \lambda_s^k\vec{v}_{s,i}+k\lambda_s^{k-1}\vec{v}_{s,i-1}\right)\nonumber\\
&=\sum_{k=0}^\infty \frac{t^k}{k!}\lambda_s^k\vec{v}_{s,i}+\sum_{k=0}^\infty \frac{t^k}{k!}k\lambda_s^{k-1}\vec{v}_{s,i-1}\nonumber\\
&=e^{\lambda_it}(\vec{v}_{s,i}+t\vec{v}_{s,i-1})
0.7ex]
\textbf{type}: \textbf{} -- stable loop, \textbf{} -- complex eigenvalues, \textbf{} -- loops with guard;
\textbf{dim}: system dimension (variables); \textbf{bounds}: nb. of half-planes defining the polyhedral set; \\
\textbf{IProc} is \cite{jeannet2010interproc}; \textbf{Sti} is \cite{colon2003linear}; \textbf{J+I} is this work; \\
\textbf{improved}: number of bounds newly detected by J+I over the existing tools~(IProc,~Sti)
\caption{Experimental comparison of unbounded-time analysis tools with inputs}
\label{tab:exp1}
\end{table*}


\subsection{Comparison with other unbounded-time approaches.} 


\begin{table*}[t]
\centering
\footnotesize
\begin{tabular}{|l|lcc|rr|r@{}lr@{}l|@{\;\;}r@{}lr@{}l|@{\;\;}r@{}l|@{\;\;}r@{}l|}
\hline
& \multicolumn{3}{c|}{characteristics}
& \multicolumn{2}{c|}{improved}
& \multicolumn{12}{c|}{analysis time (sec)} \\
name            & type                      & dim & bounds & tighter& looser& \multicolumn{4}{c}{\quad J\quad (jcf)} &\multicolumn{4}{l}{mpfr+(jcf) }
& \multicolumn{2}{l}{mpfr} & \multicolumn{2}{c|}{ld}\\ \hline
parabola\_i1& ,,  & 3 & 80 &+4(5\%) & 0(0\%) & 2&.51 &( 2&.49) & 0&.16&(0&.06) & 0&.097 & 0&.007 \\
parabola\_i2& ,,  & 3 & 80 &+4(5\%) & 0(0\%) & 2&.51 &( 2&.49) & 0&.26&(0&.06)& 0&.101 & 0&.008 \\
cubic\_i1& ,,        & 4 & 120 & 0(0\%) & 0(0\%) & 2&.47 &( 2&.39) & 0&.27 &(0&.20) & 0&.110 & 0&.013 \\
cubic\_i2& ,,        & 4 & 120 & 0(0\%) & 0(0\%) & 2&.49 &( 2&.39) & 0&.32&(0&.20) & 0&.124 & 0&.014 \\
oscillator\_i0& ,,          & 2 &  56 & 0(0\%) & -1(2\%) & 2&.53 &( 2&.52) & 0&.12 &(0&.06)& 0&.063 & 0&.007 \\
oscillator\_i1& ,,          & 2 &  56 & 0(0\%) & -1(2\%) & 2&.53 &( 2&.52) & 0 &.12 &(0&.06)& 0&.078 & 0&.008 \\
inv\_pendulum& ,, & 4 & 12 & +8(50\%) & 0(0\%) & 65&.78 &(65&.24) & 0&.24 & (0&.13) & 0&.103 & 0&.012 \\
convoyCar2\_i0 & ,, & 5 & 12 & +9(45\%) & 0(0\%) & 5&.46 &( 4&.69) & 3&.58&(0&.22) & 0&.258 & 0&.005 \\
convoyCar3\_i0 & ,, & 8 & 24 & +10(31\%) & -2(6\%) & 24&.62 & (11&.98) & 3&.11&(1&.01) & 0&.552 & 0&.051 \\
convoyCar3\_i1 & ,, & 8 & 24 & +10(31\%) & -2(6\%)& 23&.92 & (11&.98) & 4&.94&(1&.01) & 0&.890 & 0&.121 \\
convoyCar3\_i2 & ,, & 8 & 24 & +10(31\%) & -2(6\%) & 1717&.00 & (11&.98) & 6&.81&(1&.01) & 1&.190 & 0&.234 \\
convoyCar3\_i3 & ,, & 8 & 24 & +10(31\%) & -2(6\%)  & 1569&.00 & (11&.98) & 8&.67&(1&.01) & 1&.520 & 0&.377 \\ \hline
\end{tabular}\0.5ex]}
\caption{Experimental comparison with previous work}
\label{tab:exp2}
\end{table*}

In a first experiment we have benchmarked our implementation against the
tools~\textsc{InterProc}~\cite{jeannet2010interproc} and
\textsc{Sting}~\cite{colon2003linear}.  
We have tested these tools on
different scenarios, including guarded/unguarded, stable/un\-stable and
complex/real loops with inputs (details in Table~\ref{tab:exp1}).\footnote{The tool and the benchmarks are available from
\url{http://www.cprover.org/LTI/}.}
It is important to note that in many instances,  \textsc{InterProc}
\rronly{(due to the limitations of widening)} and \textsc{Sting}
\rronly{(due to the inexistence of tight polyhedral, inductive invariants)}
are unable to infer finite bounds at all.

Table~\ref{tab:exp2} gives the comparison 
of our implementation using different levels of precision (long
double, 256 bit, and 1024 bit floating-point precision) with
the original abstract acceleration for linear
loops without inputs (J)~\cite{JSS14} (where inputs are fixed to
constants). 
This shows that our implementation gives tighter over-approximations on most
benchmarks (column `improved').  While on a limited number of instances the
current implementation is less precise (Fig.~\ref{aa:supports} gives a hint
why this is happening), the overall increased precision is owed to lifting
the limitation on directions caused by the use of logahedral abstractions.

At the same time, our implementation is faster -- even when used with 1024
bit floating-point precision -- than the original abstract acceleration
(using rationals).  The fact that many bounds have improved with the new
approach, while speed has increased by several orders of magnitude, provides
evidence of the advantages of the new approach.

\rronly{
The speed-up is due to the faster Jordan form computation, which takes
between 2 and 65 seconds for~\cite{JSS14} (using the ATLAS package), whereas
our implementation requires at most one second.
For the last two benchmarks, the polyhedral computations blow up
in~\cite{JSS14}, whereas our support function approach shows only moderately
increasing runtimes.  The increase of speed is owed to multiple factors, as
detailed in Table~\ref{tab:speed}.  The difference of using long double
precision floating-point vs.~arbitrary precision arithmetic is negligible,
as all results in the given examples match exactly to 9 decimal places. 
Note that, as explained above, soundness can be ensured by appropriate
rounding in the floating-point computations.

\begin{table}[b!]
\centering
\footnotesize
\begin{tabular}{|l|*{1}{@{\;\;}c@{\;}}|}
\hline
Optimisation & Speed-up \\  \hline \hline
Eigen vs.~ATLAS \footnote{http://eigen.tuxfamily.org/index.php?title=Benchmark} & -- \\  \hline
Support functions vs.~generators
& -- \\ \hline
long double vs.~multiple precision arithmetic & -- \\ \hline
interval vs.~regular arithmetic & -- \\ \hline \hline
Total & -- \\ \hline
\end{tabular}~\0.5ex]
\caption{Comparison on convoyCar2 benchmark, between this work and
the LGG algorithm~\cite{LG09}}
\label{tab:spaceX}
\end{table*}

\jrronly{
\subsection{Comparison with other Abstract Acceleration techniques.}
Table \ref{tab:leguernic} shows a comparison between our approach and \cite{leguernic:hal-01550767}. 
As can be seen by the results, our approach is not only faster but much more precise. The reasons for
this are many-fold. In terms of speed the fact that they require a matrix twice the size and that the 
algorithm is O() alredy makes a difference of an order of magnitude. Even when using sparse matrices
their larger number of coefficients will always result in a slower approach.
In terms of precision, the choice to separate the center of the inputs makes a considerable difference, 
which is increased by the fact that our circular overapproximations are contained within their interval hulls
and are therefore up to  times smaller in volume. Since these are accelerated the encompassing increase 
in the size of the will be considearble.
Finally, we note that in the case below, where all original eigenvalues are convergent, the interval hull 
approach creates one divergent eigenvalue which causes a critical change in the behaviour of the dynamics
that leads to unbounded results.

\begin{table*}[t]
\centering
\footnotesize
\begin{tabular}{|l|*{2}{@{\;\;\;}c@{\;}}|*{2}{c@{\;\;\;}}|}
\hline
& \multicolumn{2}{c|}{this article} & \multicolumn{2}{c|}{Interval Hulls}\\
name            & 100 iterations                     &\;\;\; unbounded \;\;\;& 100 iterations& unbounded \\ \hline
run time &  166\,ms & 166\,ms & 155085\,ms & 155085\,ms \\
car acceleration & [-0.820 1.31] & [-1.262 1.31] &[-24.24 23.9] & [-  ] \\
car speed & [-1.013 5.11] & [-4.515 6.15] &[-97.2 86.7] & [-  ] \\
car position & [43.7  83.4] & [40.86  91.9] & [-319  343.4] & [-  ]\\ \hline
\end{tabular}~\-0.5ex]
\end{center}


\section{Related Work} 


There are several approaches that solve the safety problem for the linear and other cases such as hybrid systems. 
They are broadly divided into two categories due to the inherent nature of these. Namely the time bounded analysis is in most cases
unsound since it cannot reason about the unbounded time case (we not that a proof of the existence of a fix-point for the given horizon would restore such soundness by many tools do not attempt to find such proof which is left to the user). Unbounded-time solutions are therefore preferred when such soundness is required, although they are often either less precise or slower than their bounded counterparts.
\subsection{Time-Bounded Reachability Analysis}\label{sec:bounded_RW}
The first approach is to surrender exhaustive analysis over the infinite
time horizon, and to restrict the exploration to system dynamics up to some
given finite time bound.  Bounded-time reachability is decidable, and
decision procedures for the resulting satisfiability problem have made much
progress in the past decade.  The precision related to the bounded analysis
is offset by the price of uncertainty: behaviours beyond the given time
bound are not considered, and may thus violate a safety requirement.
Representatives are STRONG~\cite{DRJ13}, HySon~\cite{bouissou2012hyson}, 
CORA~\cite{althoff2015introduction}, HYLAA~\cite{DBLP:conf/hybrid/BakD17}
and SpaceEx~\cite{FLD+11}.

Set-based simulation methods generalise guaranteed integration~\cite{Loe88,Bou08}from enclosing intervals to relational domains.  They use precise
abstractions with low computational cost to over-approximate sets
of reachable states up to a given time horizon.
Early tools used polyhedral sets (\textsc{HyTech}~\cite{HHW97} and
\textsc{PHAVer}~\cite{Fre05}), 
polyhedral flow-pipes~\cite{CK98}, 
ellipsoids~\cite{BT00} and zonotopes~\cite{Gir05}. 
A breakthrough was been achieved by~\cite{GLM06,LG09}, with the
representation of convex sets using template polyhedra and support
functions.  This method is implemented in the
tool~\textsc{SpaceEx}~\cite{FLD+11}, which can handle dynamical systems with
hundreds of variables. Although it may use exact arithmetic to maintain soundness,
it performs computations using floating-point numbers: this is a deliberate choice to
boost performance, which, although quite reasonable, its implementation is numerically
unsound and therefore does not provide genuine formal guarantees.
In fact, most tools using eigendecomposition over a large number of variables (more than 10)
are numerically unsound due to the use of unchecked floating-point arithmetic.
Another breakthrough in performance was done by HYLAA~\cite{DBLP:conf/hybrid/BakD17} which
was the first tool to solve all high order problems of hundreds and thousands dimensions.
Other approaches use specialised constraint solvers (HySAT~\cite{FH07},
iSAT~\cite{EFH08}), or SMT encodings~\cite{CMT12,GT08} for bounded model
checking of hybrid automata.

\subsection{Unbounded Reachability Analysis}\label{sec:unbounded_RW}


The second approach, epitomised in static analysis methods~\cite{HRP94},
explores unbounded-time horizons.  It employs conservative
over-approximations to achieve completeness and decidability over infinite
time horizons.

Unbounded techniques attempt to infer a \emph{loop invariant}, i.e., an
inductive set of states that includes all reachable states.  If the computed
invariant is disjoint from the set of bad states, this proves that the
latter are unreachable and hence that the loop is safe.  However, analysers
frequently struggle to obtain an invariant that is precise enough with
acceptable computational cost.  The problem is evidently exacerbated by 
non-determinism in the loop, which corresponds to the case of
open systems.  Prominent representatives of this analysis approach include
Passel~\cite{johnsonpassel}, Sting~\cite{colon2003linear}, and abstract
interpreters such as \textsc{Astr\'ee} \cite{BCC+03} and
InterProc~\cite{jeannet2010interproc}.
Early work in this area has used implementations of abstract interpretation
and widening~\cite{CC77}, which are still the foundations of most modern tools.
The work in~\cite{HRP94} uses abstract interpretation with convex polyhedra over piecewise-constant
differential inclusions.
Dang and Gawlitza~\cite{DG11a} employ optimi\-sation-based (max-strategy
iteration) with linear templates for hybrid systems with linear dynamics.
Relational abstractions \cite{ST11} use ad-hoc ``loop summarisation''
of flow relations, while abstract acceleration focuses on linear relations
analysis~\cite{GH06,GS14}, which is common in program analysis.  

\subsection{Abstract Acceleration}\label{sec:AA_RW}


Abstract acceleration~\cite{GH06,JSS14,GS14} captures the effect of an arbitrary
number of loop iterations with a single, non-iterative transfer function
that is applied to the entry state of the loop (i.e., to the set of initial
conditions of the linear dynamics).
Abstract acceleration has been extended from its original version to encompass inputs over
reactive systems~\cite{SJ12} but restricted to subclasses of linear
loops, and later to general linear loops but without inputs~\cite{JSS14}. 

The work presented in this article lifts these limitations by presenting 
abstract acceleration for \emph{general} linear loops \emph{with} inputs \cite{cattaruzza2015unbounded},
developing numeric techniques for scalability and extending the domain to continuous time systems.

\jrronly{
The work in \cite{leguernic:hal-01550767} claims the unsoundness of \cite{cattaruzza2015unbounded}. 
The sources of unsoundness referred to in that paper had been address before its publication in
\cite{extended-version} for algorithmic soundness and \cite{cattaruzza2017sound} regarding numerical soundness.
The paper proposes an alternative approach to Abstract Acceleration with inputs which relies on the expansion of
the dynamical equation to a 4x-dimensional model. The model presented there is slower and more imprecise than
the one presented in \cite{cattaruzza2015unbounded}. However, the handling of the guards with respect to calculating
the exact number of iterations would prove to be more precise in most cases. 
}
\section{Conclusions and Future Work} \label{sec:concl}


We have presented an extension of the Abstract Acceleration paradigm to
guarded LTI systems (linear loops) with inputs, overcoming the limitations
of existing work dealing with closed systems.  We have decisively
shown the new approach to over-compete state-of-the-art tools for
unbounded-time reachability analysis in both precision and scalability.  The
new approach is capable of handling general unbounded-time safety analysis
for large scale open systems with reasonable precision and fast computation
times. Conditionals inside loops and nested loops are out of the scope of
this paper.

Work to be done is extending the approach to non-linear
dynamics, which we believe can be explored via hybridisation
techniques~\cite{ADG07}, and to formalise the framework for general hybrid
models with multiple guards and location-dependent dynamics, with the aim to
accelerate transitions across guards rather than integrate individual
accelerations on either side of the guards.

\paragraph{\textsc{Acknowledgments}.} 
We would like to thank Colas Le Guernic  for his
constructive suggestions and comments on the paper.

\newpage
\bibliographystyle{abbrv} 
\bibliography{mybibliography}

\end{document}
