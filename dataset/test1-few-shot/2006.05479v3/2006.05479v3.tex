\documentclass{article}





\usepackage[preprint]{neurips_2020}





\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      

\usepackage[ruled,vlined]{algorithm2e}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{ bbold }
\graphicspath{ {./figures/} }
\usepackage{wrapfig}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\Ima}{Im}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\tr}{tr}
\usepackage{url}
\def\UrlBreaks{\do\/\do-}
\def\bi#1{\hbox{\boldmath{}}}
\usepackage{xcolor}
\newcommand{\todo}[1]{{\color{red}todo: #1}}


\title{Probabilistic Auto-Encoder}



\author{Vanessa B\"ohm\\Berkeley Center for Cosmological Physics\\
    Department of Physics\\
    University of California\\
    Berkeley, CA, USA\\
    Lawrence Berkeley National Laboratory\\
\texttt{vboehm@berkeley.edu}
\And
  Uro\v s Seljak\\
  Berkeley Center for Cosmological Physics\\
  Department of Physics\\
University of California\\
  Berkeley, California, USA\\
  Lawrence Berkeley National Laboratory\\
\texttt{useljak@berkeley.edu} \\
}

\begin{document}

\maketitle

\begin{abstract}
We introduce the probabilistic auto-encoder (PAE), a generative model with a lower dimensional latent space that is based on an auto-encoder which is interpreted probabilistically after training using a normalizing flow. The PAE is fast and easy to train, achieves small reconstruction errors, high sample quality and good performance in downstream tasks. Compared to a VAE and its common variants, the PAE trains faster, reaches a lower reconstruction error and produces state of the art sample quality without requiring special tuning parameters or training procedures. We further demonstrate that the PAE is a powerful model for performing the downstream tasks of outlier detection and probabilistic image reconstruction: 1) We identify a PAE-based outlier detection metric which achieves state of the art results and outperforms other likelihood based estimators. 2) We perform high dimensional data inpainting and denoising with uncertainty quantification by means of posterior analysis in the PAE latent space. Most generative models are specifically tuned to excel in one or two applications. With the PAE we introduce an easy-to-train, simple, but at the same time powerful model that performs well and reliably in many tasks without requiring special fine-tuning or training procedures. We make all PAE codes publicly available.\footnote{\url{https://github.com/VMBoehm/PAE}}
\end{abstract}
\section{Introduction}
Deep generative models are powerful machine learning models designed to perform tasks such as realistic artificial data generation and  data likelihood evaluation. 
Many generative models are laid out for one specific task. For example, generative adversarial networks~\citep{GANs} produce high quality samples, but do not allow for a direct evaluation of the data likelihood. Their samples often fail to cover the entire data space. 
A probabilistic model that covers the data space are 
variational auto-encoders (VAEs)~\citep{ KingmaWelling13,RezendeMW14}, which are trained to maximize a lower bound to the data likelihood (ELBO). VAEs allow for expressive architectures but require fine-tuning additional parameters~\citep{beta-VAE}, architectural tweaks or special training procedures to reach good sample quality~\citep{FixElbo, RezendeMohamed15, VAEGAN}. Both GANs and VAEs use lower dimensional latent spaces. 
Normalizing flows (NFs)~\citep{RippelAdams13,DinhKB14,DinhSB16,KingmaD18,ffjord18,glow, nsf,NF_review} model the probability density  of high-dimensional data directly. They introduce a bijective mapping from  to an underlying latent representation . The latter is enforced to follow a tractable prior distribution . NFs do not reduce the dimensionality and achieve high data likelihoods on many standard data sets~\citep{GritsenkoSnoekEtAl19}. The GLOW model~\citep{glow} achieves visually impressive results in artificial image generation, but requires the user to make a trade-off between image qualities and diversities. This trade-off is chosen by setting a temperature parameter. In addition, the data likelihood of NFs has been shown to be a poor out-of-distribution (OoD) metric~\citep{Nalisnick2019}. This finding similarly applies to the ELBO of VAEs~\citep{Nalisnick2019}.

The failure of VAEs and NFs in OoD detection shows that 
maximizing the data likelihood does not necessarily correspond to optimizing a model for a specific downstream task. For example, a phenomenon which is well known in linear methods such as probabilistic PCA \citep{TippingBishop1999} are singular latent space variables. They have vanishing effect on the data distribution, yet dominate the likelihood: the contribution of the smallest eigenvalues, , to the log likelihood of a Gaussian, linear model is , which diverges for . This suggests that maximizing the likelihood under a model can be dominated by these eigenvalues, while they may have no relevance for tasks such as generative sampling or outlier detection. In linear methods this problem is addressed with shrinkage regularization \citep{LedoitWolf2004,LedoitWolf15} or dimensionality reduction (low rank PCA)~\citep{TippingBishop1999}. 

Here we propose a new composite generative model, the Probabilistic auto-encoder (PAE), which is not trained to maximize the data likelihood, but achieves state of the art results in many downstream tasks. Specifically, we show that the PAE produces high quality samples, that a PAE-based density estimator outperforms other generative model-based outlier detectors, and that the PAE can be used for probabilistic data denoising and inpainting. The PAE maps the data to a lower dimensional latent space with an auto-encoder (AE) and performs density estimation in the latent space of the AE with a normalizing flow. In the limit of no dimensionality reduction, the PAE becomes a normalizing flow. The PAE can thus be interpreted as a regularized normalizing flow, in which the effect of singular dimensions is suppressed by the AE compression, in analogy to a low rank probabilistic PCA regularizing the density estimation in the linear case. 
An advantage of the PAE over many other generative models is that it requires little hyperparameter tuning, no special training procedures or architectural tweaks to achieve good results. It is thus particularly useful for non-ML experts, in particular scientists, who are looking for an easy-to-use and reliable generative model that also performs well in downstream tasks.  
\section{PAE training, sampling and density estimation}
To construct the PAE we start by training a standard auto-encoder on the -dimensional data . The latent space  is of dimensionality . Encoder  and decoder  are deep neural networks with trainable parameters  and , respectively.

The training objective of the AE is the reconstruction error

The auto-encoder by itself is not a probabilistic model. To construct the PAE, we interpret it probabilistically \textit{after} training.
The latent space prior, , of the PAE is found by performing a density estimation on the AE-encoded training data. Formally, this can be be derived from

where in the last step we have identified the AE posterior, , as a Dirac delta distribution. In our experiments we learn  using a normalizing flow (NF), but other density estimators could be used instead. Note that the NF training is performed \textit{after we have trained the auto-encoder}. We do not retrain the generator or decoder of the AE in this step. The AE latent space is of relatively low dimensionality, , which allows for computationally tractable density estimation: the NF models require little tuning in terms of architecture and are fast to train. The performance of different density estimators can be compared at this step.

The NF is a bijective mapping from the latent space of the AE-encoder, , to the latent space of the NF, , which is parametrized by deep networks with parameters, ,

The training objective of the NF is the negative log likelihood of the encoded samples,

where the NF prior is  and the determinant of the NF Jacobian is given by

\begin{figure}
\centering{
\includegraphics[width=0.8\linewidth,keepaspectratio]{figures/model_illustration_new.pdf}}
\caption{\label{fig:illu} Schematic diagram of the PAE (left panel) and an illustration of the sampling procedure from the PAE (right panel). The auto-encoder networks are depicted as gray trapezia, the normalizing flow is represented by black arrows and the latent spaces of the auto-encoder and normalizing flow are shown in red and blue, respectively.}
\end{figure}
\subsection{Sampling from the PAE}
\label{sec:sampling}
To sample from  we draw a sample, , from the NF latent distribution and pass it through both the NF and AE generators (Figure~\ref{fig:illu}), 



It is often argued that AEs are problematic because they can map 
very different images into very nearby latent space points. 
This is cured by the NF, which maps well separated samples, , to nearby samples in the AE latent space . It makes sure that many samples land in regions of high  and that the entire AE latent space is covered. This approach achieves an excellent sample quality and diversity, as well as continuous interpolations between data points. 
We note that a lower dimensional latent space is usually not a hindrance to high sample quality, as long as its dimensionality is chosen high enough to achieve reconstructions with the desired quality. 


The PAE is trained to first achieve optimal reconstruction quality. The NF then allows to draw samples of that same quality. This is different from a VAE which needs to balance the reconstruction error (the likelihood term) with the sample quality (the KL term). If the former dominates the loss during training, the encoded distribution and prior do not match well. In this case, samples from the prior can land outside of the encoded domain resulting in a low sample quality. If the KL term dominates, some latent variables will not encode sufficient information and the reconstruction quality is low (a problem known as posterior collapse). A number of solutions have been proposed to overcome these problems. They generally add complexity in terms of model design and training procedure to the VAE. The differences between PAE and modified VAEs are discussed in more detail in Section~\ref{sec:rel_work}.
\subsection{A PAE-based density estimator for outlier detection}
\label{sec:ood-detector}
The PAE is not trained to maximize the data likelihood . While we have shown in Section~\ref{sec:sampling} that this is not required to obtain a probabilistic generative model, an estimate of  can be useful for certain downstream tasks such as out-of-distribution detection.

In latent space models a  estimate is obtained by marginalization over the latent space

The prior, , is modelled by the NF (Eq.~\ref{loss_NF}), but Eq.~\ref{px} further requires defining an implicit likelihood, .
For this we make the Gaussian ansatz

where  is the mean reconstruction error of the AE in dimension , which we measure on the validation data. We denote the noise covariance of the implicit likelihood, which is the diagonal matrix with values , as . 


The integral in~Eq.\ref{px} is generally not tractable and needs to be solved approximately. In VAEs it is approximated with variational inference (VI). Here, we follow a different approach and approximate it with Laplace's method: We approximate the exponent in the integrand of~Eq.\ref{px} by its expansion to second order around the maximum and perform the remaining Gaussian integral. For a function  with unique maximum  the Laplace approximation to the integral  is 

with the covariance given by the inverse Hessian at the maximum , .
The PAE density estimate for a data point under the Laplace approximation is thus,

Evaluating Eq.~\ref{eq:laplace} proceeds in two steps:
\begin{enumerate}
\item Find the maximum of the posterior (MAP), , by minimizing

The encoded value, , can be used as initial value. The minimization is generally computationally tractable and fast since it is performed in the lower dimensional latent space. We have found that using the amortized  in lieu of the MAP already achieves excellent outlier detection accuracy. 

\item Obtain the Hessian at the MAP value

where we have used the Gauss-Newton approximation
to the Hessian, meaning that we have dropped all terms with higher derivatives. 
\end{enumerate}
Given the MAP position and the Hessian we can then estimate  under the Laplace approximation,

We found in our experiments that the Hessian tends to suffer from numerical noise, which decreases the accuracy of the density estimation.
To obtain a more reliable OoD metric, we drop the Hessian term,

The metric in Eq.~\ref{px_tilde} has been derived from a normalized probability density estimator but can no longer be interpreted as one. Intuitively, it can be interpreted as the conjunction of two outlier metrics: a (weighted) reconstruction error (first term of Eq.~\ref{px_tilde}) and a density estimator in the latent space of the auto-encoder (second and third term of Eq.~\ref{px_tilde}).

We will show that in high latent space dimensions we can also drop the 
reconstruction error (first term) from Eq.~\ref{px_tilde}: in this case the density estimator becomes a regularized NF density estimator, where regularization has been achieved by dimensionality reduction. 
We find that Eq.~\ref{px_tilde} is an excellent outlier detection metric for high-dimensional data , outperforming AE-based outlier detection (AE-based outlier detection uses the reconstruction error, which corresponds to the first term of Eq.~\ref{px_tilde} if all  are equal.), the ELBO, IWAE~\citep{IWAE}, and the data likelihood of normalizing flows. 

\subsection{PAE-based denoising and inpainting with uncertainty quantification}
\label{sec:recon}
Another important downstream task of generative models is image denoising and inpainting. This is often performed with point estimators and the important task of uncertainty quantification (UQ) is neglected. The probabilistic PAE framework can be used to address both with posterior inference~\citep{DeepUQ}. 
For Gaussian noise, , with noise 
covariance matrix 
(typically a diagonal matrix) and a pixel-wise mask , the log posterior of a corrupted data point, , under the PAE is

The prior is  and the implicit likelihood is given by

The covariance of this Gaussian likelihood is composed of the PAE reconstruction error,, and the noise level in the corrupted data, . For sufficiently high latent space dimensionalities the latter dominates, , ensuring that the likelihood is well approximated by a Gaussian. Note that we have replaced  by its generative process, , thus bringing the inference problem to the low dimensional latent space of the PAE. Posterior analysis is computationally more tractable in this lower dimensional space.

To denoise and inpaint a corrupted image we perform latent space posterior analysis. A point estimate is given by the MAP, , the maximum of Eq.~\ref{eq:recon_posterior}, which forward modeled into data space, , yields the most likely underlying image. For a full posterior analysis any technique, such as MAP+Lapace, VI or MCMC sampling could be used. Given the multi-modal posterior of some of our examples we fit a full rank Gaussian mixture model to Eq.~\ref{eq:recon_posterior} following~\cite{SeljakYu19}. We can then sample from this model to obtain other solutions that are compatible with the data, thereby obtaining an uncertainty estimate of the reconstruction.
In Sec.~\ref{sec:exp} we show that this procedure recovers high quality images from badly corrupted inputs.
\section{Related Work}
\label{sec:rel_work}
The PAE can be viewed as a generalization of probabilistic PCA~\citep{TippingBishop1999} and reduces to it for linear, Gaussian models. The PAE replaces the low rank PCA components by the low dimensional AE latent space. Both approaches treat the reconstruction error that arises from the compression as Gaussian noise (Eq.~\ref{il}). The low rank PCA representation removes components with very small eigenvalues that would otherwise dominate the likelihood. A Gaussian prior is imposed on the PCA weights to make the PCA probabilistic. The PAE imposes a normalizing flow prior on the latent space. 

The PAE is also a form of regularized normalizing flow, since an NF can be viewed as a nonlinear generalization of a full rank probabilistic PCA. The PAE regularizes the NF by reducing its dimensionality. A similar reinterpretation of an NF as a VAE has been discussed in~\cite{GritsenkoSnoekEtAl19}. The PAE uses an AE as a dimensionality reduction tool, which for powerful architectures and high latent dimensions has a small reconstruction error, yet regularizes the sensitivity of the NF to low variance components. 

The PAE uses an NF prior, which differs from a VAE with an NF posterior \citep{RezendeMohamed15}: for the latter the training relies on the standard ELBO optimization. 
The PAE uses an NF to learn the prior after the AE training is complete. VI and ELBO are never used. This separation allows for fast, easy and stable training of both the AE and the NF. 


Other approaches that try to improve the VAE sample quality include -VAEs~\citep {beta-VAE} and
2-Stage-VAEs~\citep{2StageVAE}. 
-VAEs need to balance reconstruction error and sample quality and require an additional costly hyperparameter tuning (Eq.~\ref{beta_obj}).  
2-stage-VAEs combine two consecutive VAE's, one for the purpose of data compression, where the KL term in the ELBO is suppressed (small  in Eq.~\ref{beta_obj}), and a second one for latent space density estimation. 
This is similar to the PAE, and 
achieves similarly high quality samples. Our approach shows 
that VI and ELBO are not essential to obtain these results: in the first stage the KL 
term of the ELBO (Eq.~\ref{beta_obj}) can be completely dropped and no sampling from  is needed (the VAE becomes an AE), and in the second stage the VAE can be replaced by a powerful NF, which is useful for downstream tasks such as OoD detection. 

\cite{GLF} propose a similar conjunction of AE and normalizing flow (the generative latent flow) and achieve high sample quality, but do 
not explore the probabilistic interpretation for downstream tasks. Generative moment matching networks~\citep{GMMN} have also been proposed to be used in a 2-stage PAE-like set up, consisting of an auto-encoder and a mapping of a Gaussian to the encoded distribution. The second stage is non-invertible and trained with a moment-matching objective. GMMNs have been shown to produce high quality samples on MNIST and the Toronto face dataset. Wasserstein auto-encoders~\citep{WAE} employ yet another training objective to match the encoded distribution to a given prior. WAEs achieve high sample quality with comparable FID scores to the PAE, but do not provide a density estimate that is required for other tasks.

Other approaches that can be interpreted as regularized normalizing flows and that address cases where the data is confined to a lower dimensional manifold include -flows~\citep{Brehmer2020} and relaxed injective probability flows~\citep{Kumar2020}. Instead of separating the tasks of compression and density estimation of the manifold, these models regularize the flow itself. 

Out-of-distribution detection with generative models has recently attracted a lot of attention, triggered by the finding that state-of-the art generative models such as VAE, GLOW and MAF fail in this task on a number of standard datasets~\citep{Nalisnick2019}. \citep{Nalisnick2019} proposed a fix that uses likelihood ratios as an OoD metric instead of the likelihood itself. While achieving good OoD accuracy, their method requires fine-tuning a noise parameter, which in turn requires some kind of correctly labeled in- and out-of-distribution data sets. We demonstrate in our experiments (Section~\ref{sec:down}) that the PAE density estimate achieves high OoD accuracy without fine-tuning or requiring a labeled outlier training set.

There is a long list of works that have addressed the problem of image inputation and denoising with generative models, ~\citep[]{RezendeMW14, MIWAE, Mattei2018, DongLHT15, JinMcCann2016, RIMPutzky, DeepImagePrior, AmbientGAN}. Most of them, however, do not address the crucial task of uncertainty estimation. A single point estimate is useless in many applications if its precision cannot be quantified. Our approach has two further advantages that many other methods do not share: 1) It only requires a single training of the generative model and can be applied to different corruption types without retraining. 2) It uses a robust Bayesian framework that comes with well established theoretical guarantees and well known behavior in reconstruction and uncertainty estimation, while it limits the use of the generative model to the task of dimensionality reduction, where its performance can be easily tested and verified.  
\section{Experiments}
\label{sec:exp}
\begin{table}
  \caption{FID scores and reconstruction errors at latent space dimensionality  measured on 10000 samples/test data. The errors on the FIDs are smaller than the differences (order 1).}
  \label{tab:FID}
  \centering
  \begin{tabular}{lllll}
    Model            & AE  & VAE & -VAE & PAE \\
    \midrule
    sample FID score  &    102.9 &  102.4   &   56.3      & \textbf{35.4}\\
    reconstruction FID score  &     \textbf{31.5} &   32.3   &     51.3    & \textbf{31.5}\\
    mean reconstruction error  &    \textbf{0.077} &   0.084  &  0.114 & \textbf{0.077}\\
    \bottomrule
  \end{tabular}
\end{table}
\begin{figure}
\includegraphics[width=\columnwidth/100*19,keepaspectratio]{figures/raw_samples_RNF.pdf}
\includegraphics[width=\columnwidth/100*19,keepaspectratio]{figures/raw_samples_VAE.pdf}
\includegraphics[width=\columnwidth/100*19,keepaspectratio]{figures/raw_samples_VAE32.pdf}
\includegraphics[width=\columnwidth/100*19,keepaspectratio]{figures/nvp_samples_RNF.pdf}
\includegraphics[width=\columnwidth/100*19,keepaspectratio]{figures/rec_valid_set.pdf}
\caption{Sample qualities at : From left to right we show samples from an AE, samples from an equivalent VAE without parameter-tuning, samples from a -VAE with tuned parameters , samples from the PAE and reconstructions of test data. Corresponding FID scores are listed in Table~\ref{tab:FID}: the PAE reaches better FIDs than the -VAE without requiring parameter-tuning.}
\label{fig:fmnist32}
\end{figure}
We train PAEs on the MNIST~\citep{LecunMNIST}, Fashion-MNIST~\citep{f-mnist} and Celeb-A~\citep{celeba} training data sets. We preprocess these data sets with standard procedures: we dequantize and rescale pixel values to the interval [-0.5,0.5]. The celeb-A samples are cropped to the central 128x128 pixels and then downsampled to 64x64 pixels. 
The first building block of the PAE is an auto-encoder: The AE encoder and decoder networks are based on the infoGAN architecture~\citep{infoGAN}. We train the AEs until convergence (at least 500 epochs) using an initial learning rate of 1e-3 which we reduce to 1e-4 after 400 epochs.
The second building block of the PAE is a normalizing flow. This NF is trained on the AE latent space (Eq.~\ref{nf}). Depending on the latent space dimensionality we use different architectures for the NF. For , we find that a realNVP architecture~\citep{DinhSB16} with random permutations is sufficient to achieve samples of the same quality as the reconstructions. For higher , we switch to more expressive neural spline flows~\citep{nsf} and trainable permutations~\citep{KingmaD18}. All NFs are trained for at least 100 epochs until convergence. 
For comparison we also train ()-VAEs using the same architecture and training procedure as for the AE. The VAEs are trained with a Gaussian likelihood (allowing for a direct comparison with the PAE), a standard normal distribution as prior and using the mean field approximation for the posterior. 
In -VAEs~\citep{beta-VAE} the training objective is modified by two tunable parameters,  and ,

We iterate over several parameter combinations until we achieve optimal sample quality.
\subsection{Sample and reconstruction quality}
We compare the sample and reconstruction quality of the PAE with an equivalent VAE. We quantify sample qualities with the Frechet-Inception Distance (FID)~\citep{FID_Heusel} to the test data. We also quote FIDs for reconstructions, not only because they quantify reconstruction quality, but also because they constitute a lower bound to the sample FID that is achievable with a model. The reconstruction quality is further quantified with the mean reconstruction error on the test data (averaged over all pixels).

In Fig.~\ref{fig:fmnist32} we show F-MNIST samples of different models at latent space dimensionality . The samples of the VAE without parameter-tuning are very similar to samples from an AE (to sample from the AE, we draw latent space points from  and pass them through the AE generator). The low VAE sample quality is a consequence of the likelihood term dominating the ELBO in this setting. This can be cured by fine-tuning the relative weighting of likelihood and KL terms (Eq.~\ref{beta_obj}). We run 5 different parameter combinations (changing the parameters based on the result of the previous combination) and show results for the combination with the highest FID score. In contrast to the -VAE, the PAE does not require any iterative parameter search to achieve high quality samples. In Table~\ref{tab:FID} we list the corresponding FID scores.

While the -VAE balances a trade-off between reconstruction error and sample quality, the PAE is trained to reach an optimal reconstruction error. Matching the latent space distribution of the AE with an NF then allows to achieve samples with similar quality as the reconstructions. Because of this, the PAE reaches lower reconstruction errors (Table~\ref{tab:FID}) and higher sample quality than the -VAE. At the same time it does so at lower computational (and human) cost: Several -VAE models have to be trained to find the optimal parameter combination, while the PAE only needs to be trained once.
We show PAE samples for F-MNIST and PAE samples and interpolations for celeb-A in Figs.~\ref{fig:fmnist64_128} and~\ref{fig:celeba_samples}. The interpolations are obtained by mapping two random images from the test data set into the latent space, fitting a linear interpolation between the latent points and mapping equidistant points along this line back into the image space.
\begin{figure}
\centering{
\includegraphics[width=\columnwidth/100*25,keepaspectratio]{figures/nvp_samples_64_new.pdf}
\includegraphics[width=\columnwidth/100*25,keepaspectratio]{figures/nvp_samples_128new.pdf}
\caption{F-MNIST samples from the PAE at  with  (left) and  with  (right).}
\label{fig:fmnist64_128}}
\end{figure}
\begin{figure}
\begin{subfigure}{0.4\textwidth}
\centering{
\includegraphics[width=\columnwidth/100*85,keepaspectratio]{figures/nvp_samples_64_celeba.pdf}}
\end{subfigure}
\begin{subfigure}{0.59\textwidth}
\centering{
\includegraphics[width=\columnwidth/100*98,keepaspectratio,trim=0 7 0 7, clip]{figures/interp11.pdf}
\includegraphics[width=\columnwidth/100*98,keepaspectratio, trim=0 7 0 7, clip]{figures/interp10.pdf}
\includegraphics[width=\columnwidth/100*98,keepaspectratio, trim=0 7 0 7, clip]{figures/interp0.pdf}
\includegraphics[width=\columnwidth/100*98,keepaspectratio, trim=0 7 0 7, clip]{figures/interp6.pdf}
\includegraphics[width=\columnwidth/100*98,keepaspectratio, trim=0 7 0 7, clip]{figures/interp8.pdf}}
\end{subfigure}
\caption{PAE performance on Celeb-A at . Samples (left) reach  (reconstructions ). Right: Interpolations between samples from the test set.}
\label{fig:celeba_samples}
\end{figure}
\subsection{Downstream tasks: Out of Distribution (OoD) detection, denoising, inpainting}
\label{sec:down}
In Section~\ref{sec:ood-detector} we derived an approximate, PAE-based density estimator which we apply as an outlier detector. OoD detection with generative models has recently attracted a lot of attention, since their probability estimates have been shown to be poor outlier-detectors:~\cite{Nalisnick2019} found that different generative models, including VAEs and NFs, can assign consistently higher log probabilities to OoD data than training data. One particular combination of data sets for which this has been observed is Fashion-MNIST and MNIST, where a model trained on the former assigns higher probability to the latter. We use this pair to demonstrate the discriminating power of the PAE outlier detector. We compare our results to other recently suggested outlier detectors:~\cite{Choi2018} propose the use of the Watanabe-Akaike information criterion (WAIC) and compare it to several other techniques. Here, we also quote their results using the variational information bottleneck OoD detector (VIB)~\citep{Alemi2018}, since it performed best in their experiments on the data pairs we consider here, and the importance weighted auto-encoder (IWAE)~\citep{IWAE}. Different to our PAE-based detector, the VIB OoD-detector requires a labeled training data set.
\cite{LikelihoodRatioAI} propose to use the likelihood ratio between two models: one trained on in-distribution data, the other trained on perturbed in-distribution data. Their method requires hyper-parameter tuning (the amount of perturbation) for which some OoD data needs to be used.

The PAE-based outlier detector, Eq.~\ref{px_tilde} combines two terms, a likelihood term measuring the reconstruction error, and a latent space density term measuring the probability of the encoded data point in latent space. 
We find that the reconstruction error is more informative about outliers at low latent space dimensionalities (), while it quickly becomes irrelevant for . At higher , the reconstruction error becomes very small (as illustrated in Fig.~\ref{fig:recons_k}), while the latent space density becomes very informative about outliers. If we allowed for an additional hyper-parameter, we could find an optimal weighting between likelihood and latent space term for each . However, the aim of the PAE is to omit hyper-parameter tuning, and so we quote results for the two extreme settings: 1) OoD detection with only the latent space density at . 2) OoD detection with ony the AE reconstruction error at . We compare our results for these settings to other methods in terms of the Area Under Receiver Operating Characteristic (AUROC) curve in Table~\ref{tab:auroc}. As OoD data we use MNIST, OMNIGLOT~\citep{OMNIGLOT} as well as vertically (vflip) and horizontaly (hflip) flipped F-MNIST test data. The in distribution data is the F-MNIST test set.
We find that the PAE outlier detector yields consistently high AUROC values without any parameter tuning, outperforming the other methods in MNIST, OMNIGLOT and hflip.
The good performance of the PAE-detector is owed to the regularization imposed by dimensionality reduction, as it eliminates the low variance 
components that dominate the likelihood but 
are not informative about outliers.
The likelihood ratio \citep{LikelihoodRatioAI} similarly upweights the more informative part of the data by adding noise, and can be viewed as a form of regularization since it also eliminates information from low variance components. However, it requires fine-tuning the type and amount of perturbation on some additional OoD data set. We achieve competitive results without parameter tuning and without requiring any kind of OoD data for calibration.
\begin{table}
  \caption{OoD detection accuracy quantified by the AUROC () for models trained on F-MNIST.  }
  \label{tab:auroc}
  \centering
  \begin{tabular}{lllll}
    Outlier          & MNIST  & OMNIGLOT & FMNIST-hflip & FMNIST-vflip \\
    \midrule
    PAE density (this work)               &  \textbf{0.997} & \textbf{0.981}  &  \textbf{0.698} &   0.891        \\
    AE reconstruction error (this work)   & 0.986    &  0.916  &  0.689   &    0.880      \\
likelihood ratios (Ren et al. 2019)   &  0.996   & -       &   -      & -              \\
    VIB (Choi et al. 2018)                &  0.941   & 0.943   &  0.667   & \textbf{0.902} \\
    WAIC (Choi et al. 2018)               &  0.766   & 0.796   &  0.624   & 0.704           \\
    IWAE (Choi et al. 2018)               &  0.423   & 0.568   &  0.594   & 0.668           \\
    \bottomrule
  \end{tabular}
\end{table}
\begin{figure}
\begin{subfigure}{0.65\textwidth}
\centering{
\includegraphics[width=\linewidth/100*19,keepaspectratio,trim=4 0 4 0 clip]{figures/ood_inputs.pdf}
\includegraphics[width=\linewidth/100*19,keepaspectratio,trim=4 0 4 0 clip]{figures/ood_recons_K4.pdf}
\includegraphics[width=\linewidth/100*19,keepaspectratio,trim=4 0 4 0 clip]{figures/ood_recons_K8.pdf}
\includegraphics[width=\linewidth/100*19,keepaspectratio,trim=4 0 4 0 clip]{figures/ood_recons_K16.pdf}
\includegraphics[width=\linewidth/100*19,keepaspectratio,trim=4 0 4 0 clip]{figures/ood_recons_K32.pdf}}
\end{subfigure}
\begin{subfigure}{0.34\textwidth}
\includegraphics[width=\linewidth/100*99,keepaspectratio]{figures/prior_versus_likleihood.pdf}
\end{subfigure}
\caption{MNIST input images (outermost left plot) and their reconstructions with with AEs trained on F-MNIST. As  increases from left to right (4,8,16,32), the reconstructions transition from being visually close to F-MNIST (high reconstruction error) to visually close to the actual input (low reconstruction error). On the right we compare OoD detection based on reconstruction error (yellow) to latent space density (blue) as a function of  in terms of the ratio of True Positive (TP) over False Positive (FP) rate at a False Negative rate of . At low  the reconstruction error is a better OoD detector, at higher  the latent space density outperforms it.}
\label{fig:recons_k}
\end{figure}
\begin{figure}
\begin{center}
\includegraphics[width=\linewidth/100*70,keepaspectratio]{figures/recon_neurips_noise2.pdf}
\end{center}
\caption{Left panel: corrupted input data with masks shown in gray (left column), MAP 
reconstructions (middle column) and underlying truth (right column). Middle panel: posterior density of two latent space variables (out of 8) for the first example of the masked 3. Right panel: Samples drawn from the posterior, the bottom rows show the same samples with the mask.}
\label{fig:inpainting}
\end{figure}


Another downstream task that deep probabilistic 
models are useful for is posterior analysis of incomplete or 
corrupted data. We address this task with the PAE following Section~\ref{sec:recon}. In the left panel of Figure~\ref{fig:inpainting} we show 3 examples of corrupted MNIST test data (left column), reconstructions at the peak of the posterior (MAP) (middle) and and true images (right column). The first example of the masked 3 illustrates why uncertainty quantification in terms of posterior analysis is important: The corrupted data is compatible not only with a 3, but also a 5. We see in the middle panel\footnote{plot created with the mcmcplot package~\citep{mcmcplot}} that the corresponding posterior is 
complicated, possibly multi-modal. Drawing from this posterior results in the 
samples at the top of the right panel. The masked samples in the bottom show that they are all compatible with the input data. The samples are mostly 3s, 
with occasional occurrence of 5 (possibly associated 
with the secondary peak in the latent space
posterior). While the digit 5 can be consistent with the 
data, this is true for only a small fraction of the prior and the posterior reflects this. \section{Discussion and conclusion}
We show that an auto-encoder combined with a normalizing 
flow is a powerful, easy and fast to train probabilistic model (PAE). The PAE can be used for sample generation, outlier detection and probabilistic image inpainting and denoising. When compared to baselines such as (-)VAEs it outperforms these in terms of reconstruction error, sample quality, computational time, and downstream tasks such as outlier detection, without the need of additional parameter-tuning. 

The PAE can be viewed either as a nonlinear generalization of a low rank probabilistic PCA or as a regularized form of an normalizing flow. The regularization through dimensionality reduction avoids the issues that plague normalizing flows, such as zero or 
near zero variance pixels, which can flaw density estimation based outlier detection. This allows the PAE to achieve state of the art results in out-of-distribution problems where normalizing flows are known to fail. 

\newpage
{\bf Acknowledgements}

We thank Fran\c{c}ois Lanusse for useful discussions. 
This material is based upon work supported by the National Science Foundation under Grant Numbers 1814370 and NSF 1839217, and by NASA under Grant Number 80NSSC18K1274. This research used resources of the National Energy Research Scientific Computing Center (NERSC), a U.S. Department of Energy Office of Science User Facility operated under Contract No. DE-AC02-05CH11231.
\bibliography{cosmo,ML,vae_sample_improvements}  
\bibliographystyle{icml2020}


\end{document}
