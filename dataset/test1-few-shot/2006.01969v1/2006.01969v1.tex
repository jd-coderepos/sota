\documentclass[sigconf]{acmart}

\usepackage[T1]{fontenc}

\usepackage{multirow}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{enumitem}





\colorlet{punct}{red!60!black}
\definecolor{background}{HTML}{EEEEEE}
\definecolor{delim}{RGB}{20,105,176}
\colorlet{numb}{magenta!60!black}

\lstdefinelanguage{json}{
    basicstyle=\normalfont\ttfamily,
    numberstyle=\scriptsize,
    breaklines=true,
    stepnumber=1,
    numbersep=8pt,
    showstringspaces=false,
    frame=lines,
    backgroundcolor=\color{background},
    literate=
     *{0}{{{\color{numb}0}}}{1}
      {1}{{{\color{numb}1}}}{1}
      {2}{{{\color{numb}2}}}{1}
      {3}{{{\color{numb}3}}}{1}
      {4}{{{\color{numb}4}}}{1}
      {5}{{{\color{numb}5}}}{1}
      {6}{{{\color{numb}6}}}{1}
      {7}{{{\color{numb}7}}}{1}
      {8}{{{\color{numb}8}}}{1}
      {9}{{{\color{numb}9}}}{1}
      {:}{{{\color{punct}{:}}}}{1}
      {,}{{{\color{punct}{,}}}}{1}
      {\{}{{{\color{delim}{\{}}}}{1}
      {\}}{{{\color{delim}{\}}}}}{1}
      {[}{{{\color{delim}{[}}}}{1}
      {]}{{{\color{delim}{]}}}}{1},
}


\DeclareMathOperator*{\argmax}{argmax} 

\newcommand{\miniskip}{\vspace*{-.5\baselineskip}}
\newcommand{\shrink}{\vspace*{-.9\baselineskip}}




\newcommand{\todo}[1]{\textcolor{red}{#1}}
\newcommand{\mi}[1]{\textcolor{blue}{#1}}
\newcommand{\fa}[1]{\textcolor{olive}{#1}}



\copyrightyear{2020}
\acmYear{2020}
\setcopyright{acmcopyright}
\acmConference[SIGIR '20]{Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval}{July 25--30, 2020}{Virtual Event, China}
\acmPrice{15.00}
\acmDOI{10.1145/3397271.3401416}
\acmISBN{978-1-4503-8016-4/20/07}



\begin{document}

\title{REL: An Entity Linker Standing on the Shoulders of Giants} 

\author{Johannes M. van Hulst}
\affiliation{Radboud University}
\email{mick.vanhulst@gmail.com}

\author{Faegheh Hasibi}
\affiliation{Radboud University}
\email{f.hasibi@cs.ru.nl}

\author{Koen Dercksen}
\affiliation{Radboud University}
\email{koen.dercksen@ru.nl}

\author{Krisztian Balog}
\affiliation{University of Stavanger}
\email{krisztian.balog@uis.no}

\author{Arjen P. de Vries}
\affiliation{Radboud University}
\email{a.devries@cs.ru.nl}

\renewcommand{\shortauthors}{J.M. van Hulst, et al.}


\begin{abstract}
Entity linking is a standard component in modern retrieval system that is often performed by third-party toolkits.  Despite the plethora of open source options, it is difficult to find a single system that has a modular architecture where certain components may be replaced, does not depend on external sources, can easily be updated to newer Wikipedia versions, and, most important of all, has state-of-the-art performance.
The REL system presented in this paper aims to fill that gap.
Building on state-of-the-art neural components from natural language processing research, it is provided as a Python package as well as a web API.  We also report on an experimental comparison against both well-established systems and the current state-of-the-art on standard entity linking benchmarks.
\end{abstract}




\keywords{Entity Linking; Toolkit; Entity Disambiguation; NER}

\maketitle


\miniskip
\section{Introduction}
\label{sec:intro}



Entity linking (EL) refers to the task of recognizing mentions of specific entities in text and assigning unique identifiers to them from an underlying knowledge repository~\citep{Balog:2018:EOS}.  The problems of entity recognition and disambiguation have traditionally been studied in the natural language processing (NLP) community.  It was also them who first recognized the utility of Wikipedia as a large-scale knowledge repository to disambiguate against~\citep{Bunescu:2006:UEK,Cucerzan:2007:LSN}.
This line of work has been quickly followed up by information retrieval (IR) researchers~\citep{Milne:2008:LLW,Mihalcea:2007:WLD}.
Over the past years, entity linking has become a standard component in modern retrieval systems, and has been leveraged in a range of tasks, including document ranking~\citep{Xiong:2017:WDR}, entity retrieval~\citep{Hasibi:2016:EEL}, knowledge base population~\citep{Balog:2013:MCA}, and query recommendation~\citep{Reinanda:2015:MRR}.  Since entity linking is not the main focus of these works, it is commonly performed by some third-party toolkit, with the resulting annotations being utilized in downstream processing.  
Some of the most prominent toolkits used for this purpose include DBpedia Spotlight~\citep{Mendes:2011:DSS}, TAGME~\citep{Ferragina:2010:TOA}, WAT~\cite{Piccinno:2014:WAT}, and FEL~\cite{Pappu:2017:LME}.



Existing toolkits fall short in a number of areas.  Some are unmaintained~\cite{Pappu:2017:LME}; others are meant for short text and inefficient for long text~\cite{Hasibi:2017:NTE}; some rely on external sources like web search engines~\cite{Cornolti:2018:APA}. Typically, they are shipped with a specific Wikipedia version that has become dated, causing difficulties when attempting to update to a recent Wikipedia~\cite{Piccinno:2014:WAT, Cornolti:2018:APA}. An issue that is often not addressed is the lack of speed (throughput). Most importantly, none of the default open source entity linkers incorporate recent progress made in the NLP community on neural network-based approaches~\cite{Kolitsas:2018:ENE}. With this work, we aim to close that gap and remedy all of these problems by introducing an efficient, up-to-date entity linker that has a modular architecture to ease, e.g., updates of external resources like Wikipedia.



We present REL\footnote{REL in Dutch means mayhem, interference, or
  disturbance; and, it is easily recognized to abbreviates `relatie' (relation in English).}
 (which stands for Radboud Entity Linker), an open source toolkit for entity linking. 
REL stands on the shoulders of giants and is an ensemble of multiple methods and packages from the state-of-the-art natural language processing research. REL has been developed with the following design considerations:
\begin{itemize}[leftmargin=3mm]
	\item Use \emph{state-of-the-art} approaches for entity disambiguation (ED)~\cite{Le:2018:IEL, Ganea:2017:DJE} and named entity recognition (NER)~\cite{Akbik:2018:flair}, ensuring it is on par with the state-of-the-art on end-to-end entity linking~\citep{Kolitsas:2018:ENE}.
	\item Use a \emph{modular architecture} with mention detection (using a NER approach) and entity disambiguation components. Specifically, separating mention detection from entity disambiguation enables us to choose an NER method appropriate for the context in which entity linking is employed (i.e., optimizing for recall vs. throughput). \item Design for sufficient \emph{throughput}; reporting 700 ms for an average document of 300 words.  Notably, most of the time is used for NER, which could be changed to a more efficient option.\item Develop a \emph{lightweight} solution that can be deployed on an average laptop/desktop machine; it does not need much RAM, and, importantly, it does not need a GPU.  \item Train on a recent Wikipedia dump (2019-07) and ensure easy \emph{updates} to new Wikipedia versions (all necessary scripts included).  \end{itemize}
REL is available at \url{http://tiny.cc/RadboudEL} under a MIT license, can be deployed as a Python package, or used via a restful API.

 \section{Entity linking in REL} 
\label{sec:approach}
In this section, we present the entity linking method underlying REL.  We follow a standard entity linking pipeline architecture~\citep{Balog:2018:EOS}, consisting of three components: (i) mention detection, (ii) candidate selection, and (iii) entity disambiguation.

\miniskip
\subsection{Mention Detection}
\label{sec:approach:md} 
In the mention detection step, we aim to detect all text spans that can be linked to entities. These text spans, referred to as \emph{mentions}, are obtained by employing a Named Entity Recognition (NER) tool. NER taggers detect entity mentions in text and annotate them with (coarse-grained) entity types~\cite{Balog:2018:EOS}.  We employ Flair~\cite{Akbik:2018:flair}, a state-of-the-art NER based on contextualized word embeddings.  Flair takes the input to be a sequence of characters and passes it to a bidirectional character-level neural language model to generate a contextual string embedding for each word.  These embeddings are then utilized in a sequence labeling module to generate tags for NER.
 
Using a NER method for mention detection enables us to strike a balance between precision and recall. Another approach, which may result in high recall,
is matching all n-grams (up to a certain ) in the input text against a rich dictionary of entity names~\cite{Hasibi:2015:ELQ,Balog:2018:EOS}.  In REL, the mention detection component can easily be replaced by another NER tagger such as spaCy\footnote{\url{https://spacy.io/}} or by a dictionary-based approach. 

 \miniskip
 \subsection{Candidate Selection}
 \label{sec:approach:cs} 
 For each text span detected as a mention, we select up to  (=7) candidate entities (following~\cite{Ganea:2017:DJE}). The  (=4) candidate entities are selected from the top ranked entities based on the mention-entity prior , for a given entity  and a mention . To compute this prior, we sum up hyperlink counts from Wikipedia and from the CrossWikis corpus~\cite{Spitkovsky:2012:CDE} to estimate probability . A uniform probability  is also extracted from YAGO dictionary~\cite{Hoffart:2011:RDN}. These two probabilities are combined into the final  prior as ~\cite{Ganea:2017:DJE}.
 
 The other  (=3) candidate entities are chosen based on their similarity to the context of the mention. This similarity score is obtained by , where  is n-word () context surrounding mention  and  and  are entity and word embedding vectors. This score is computed for  (=30) entities with the highest  prior and the top- entities are added to the list of candidate entities~\cite{Ganea:2017:DJE}.



In REL, we use Wikipedia2Vec word and entity embeddings~\cite{Yamada:2016:JLE} to estimate the similarity between an entity and a mention's local context. Wikipedia2Vec jointly learns word and entity embeddings from Wikipedia text and link structure, and is available as an open source library~\cite{Yamada:2018:WOT}. The hyper-parameters , , , and  are set based on the recommended values in~\cite{Le:2018:IEL, Ganea:2017:DJE}.


\miniskip
 \subsection{Entity Disambiguation}
 \label{sec:approach:ed} 
In the entity disambiguation step, we link mentions to their corresponding entities in the knowledge graph (here: Wikipedia). Entity disambiguation in REL is based on the Ment-norm method proposed by \citet{Le:2018:IEL}. Given an input document , the entity linking decisions are made by combining local compatibility (which includes prior importance and contextual similarity) and coherence with the other entity linking decisions in the document:

where  denotes the set of candidate entities for mention  and . The coherence score between entity  and its local context  is computed by the function  as defined in~\citep{Ganea:2017:DJE}, and the coherence between all entity linking decisions is captured by the function . \citet{Le:2018:IEL} compute the  function by incorporating relations between mentions of a document. Assuming  latent relations,  is calculated as:

where  are the embeddings of entities  (using the same embeddings as in the candidate selection step),  is a diagonal matrix, and  is a normalized score defined as: 

where  is a diagonal matrix, and function  is a single-layer neural network that maps mention  and its context  to a -dimensional vector.  is a normalization factor over  and is computed as:

The optimization of Eq.~\eqref{eq:local_golobal} is performed using max-product loopy belief propagation (LBP), and the final score for an entity of a mention is obtained by a two-layer neural network that combines  with max-marginal probability of an entity for a given document.
The training of the model, referred to as the \emph{ED model} henceforth, is performed using max-margin loss.
To estimate posterior probabilities of the linked entities, we fit a logistic function over the final scores obtained by the neural model~\cite{Platt:2000:PSVM}.
 \section{Implementation and Usage}
\label{sec:Imp}
\vspace*{-0.25\baselineskip}
Next, we describe the implementation details and usage of REL.

\vspace*{-0.25\baselineskip}
\subsection{Implementation Details}

\noindent
\emph{\textbf{Memory and GPU usage.}} One of the design requirements of REL is being lightweight, such that it can be deployed on an average machine. To minimize memory requirements, we store Wikipedia2Vec entity and word embeddings, GloVe embeddings, and an index of pre-computed  values (i.e., a surface form dictionary) in a SQLite3\footnote{\url{https://www.sqlite.org/index.html}} database. Using SQLite, we are able to minimize memory usage for our API to 1.8GB if the user chooses to not preload embeddings. REL also does not require GPU during inference. The neural model used for entity disambiguation is a feed-forward network and does not require heavy CPU/GPU usage. Training of Wikipedia2Vec embeddings, however, requires high memory and is done more efficiently using a GPU.
\\



\noindent
\emph{\textbf{REL components.}} REL has a modular architecture, with separate components for mention detection, entity disambiguation, and the generation of the  index. The mention detection component is based on the Flair package\footnote{\url{https://github.com/flairNLP/flair}} and can be easily replaced by another mention detection approach. The disambiguation component is implemented using PyTorch and based on the source code of~\citep{Le:2018:IEL}.\footnote{\url{https://github.com/lephong/mulrel-nel}} The generation of the  index is based on the source code of~\cite{Ganea:2017:DJE} and involves the parsing of Wikipedia, the CrossWikis corpus, and YAGO. Any of these may be either removed completely, or replaced by different corpora; using the resulting  index in the package instead. \\

\noindent
\emph{\textbf{ED Training.}} For the entity disambiguation method, we used the AIDA-train dataset for training and AIDA-A for validation. We use the Adam optimizer and reduce the learning rate from  to  once the F1-score of the validation set reaches  (following~\cite{Le:2018:IEL}).
\\

\noindent
\emph{\textbf{Embeddings.}} The entity and word embeddings used for selecting candidate entities are trained on a Wikipedia 2019-07 dump using the Wikipedia2Vec package.\footnote{\url{https://wikipedia2vec.github.io/wikipedia2vec}} Following~\cite{Gerritse:2020:GEE}, we set the \emph{min-entity-count} parameter to zero and used the Wikipedia link graph during training. For the entity disambiguation model, we used GloVe embeddings~\cite{Pennington:2014:glove} as suggested in~\cite{Le:2018:IEL}. 







\begin{figure}[t]
\caption{Example API input and output for entity linking.}
\label{fig:API}
\scriptsize
\begin{lstlisting}[language=json,firstnumber=1]
INPUT: 
 {"text": "Belgrade 1996-08-30 Result in an international basketball tournament on Friday: Red Star ( Yugoslavia ) beat Dinamo ( Russia) 92-90 ( halftime 47-47 )."}
\end{lstlisting}	

\begin{lstlisting}[language=json,firstnumber=1]
OUTPUT:
[
[0, 8, 'Belgrade', 'Belgrade', 0.91, 0.98, 'LOC', ], 
[80, 8, 'Red Star', 'KK_Crvena_zvezda', 0.36, 0.99, 'ORG'], 
[91, 10, 'Yugoslavia', 'Yugoslavia', 0.8, 0.99, 'LOC'], 
[109, 6, 'Dinamo', 'FC_Dinamo_Bucuresti', 0.7, 0.99, 'ORG'], 
[118, 6, 'Russia', 'Russia', 0.85, 0.99, 'LOC']
]
\end{lstlisting}
\shrink	
\end{figure}


\subsection{Usage}
REL can be used as a Python package deployed on a local machine, or as a service, via a restful API.

To use REL as a package, our GitHub repository contains step-by-step tutorials on how to perform end-to-end entity linking, and on how to (re-)train the ED model. We provide scripts and instructions for deploying REL using a new Wikipedia dump; this helps REL users to keep up-to-date with emerging entities in Wikipedia, and enables researchers to deploy REL for any specific Wikipedia version that is required for a downstream task.

The API is publicly available. Given an input text, depicted in Fig.~\ref{fig:API} (Top), the API returns a list of mentions, each with (i) the start position and length of the mention, (ii) the mention itself, (iii) the linked entity, (iv) the confidence score of ED, and (vi) the confidence score and type of entity from the mention detection step (if available); see Fig.~\ref{fig:API} (Bottom).  Alternatively, a user can use the API for entity disambiguation only, by submitting an input text and a list of spans (specified with start position and length). 

 \section{Evaluation}
\label{sec:eval}

\begin{table}[t]
\centering
\caption{EL strong matching results on the GERBIL platform.}
\label{tbl:el-gerbil}
\small
\begin{tabular}{l|@{~}l|@{~}l|@{~}l|@{~}l|@{~}l|@{~}l|@{~}l|@{~}l}
& \rotatebox[origin=l]{90}{AIDA-B} & 
\rotatebox[origin=l]{90}{MSNBC} & 
\rotatebox[origin=l]{90}{OKE-2015} & 
\rotatebox[origin=l]{90}{OKE-2016} & 
\rotatebox[origin=l]{90}{N3-Reuters-128} & 
\rotatebox[origin=l]{90}{N3-RSS-500} & 
\rotatebox[origin=l]{90}{Derczynski} & 
\rotatebox[origin=l]{90}{KORE50}
\\
\textbf{Macro F1} &&&&&&&& \\
\textbf{Micro F1} &&&&&&&&\\
\hline
DBpedia  & 52.0 & 42.4 & 42.0 & 41.4 & 21.5 & 26.7 & 33.7 & 29.4   \\
Spotlight & 57.8 & 40.6 & 44.4 & 43.1 & 24.8 & 27.2 & 32.2 & 34.9  \\
\hline
\multirow{2}*{WAT} & 70.8 & 62.6 & 53.2 & 51.8 & 45.0 & \textbf{45.3} & \textbf{44.4} & 37.3   \\
 & 73.0 & 64.5 & 56.4 & 53.9 & 49.2 & \textbf{42.3} & 38.0 & 49.6  \\
\hline
\multirow{2}*{SOTA NLP}  & \textbf{82.6} & 73.0 & 56.6 & 47.8 & 45.4 & 43.8 & 43.2 & 26.2   \\
  & 82.4 & 72.4 & 61.9 & 52.7 & \textbf{50.3} & 38.2 & 34.1 & 35.2   \\
\hline
\multirow{2}*{REL (2014)}  & 81.3 & \textbf{73.2} & 61.5 & \textbf{57.5} & \textbf{46.8} & 35.9 & 38.1 & \textbf{60.1}  \\
 & \textbf{83.3} & \textbf{74.4} & \textbf{64.8} & \textbf{58.8} & 49.7 & 34.3 & \textbf{41.2} & \textbf{61.6}  \\
\hline
\multirow{2}*{REL (2019)} & 78.6 & 71.1 & \textbf{61.8} & 57.4 & 45.7 & 36.2 & 38.0 & 50.1 \\
 & 80.5 & 72.4 & 63.1 & 58.3 & 49.9 & 35.0 & 41.1 & 50.7   \\
\hline
\end{tabular}
\miniskip
\end{table}

\begin{table}[t]
\centering
\caption{ED results on the GERBIL platform.}
\label{tbl:ed-gerbil}
\small
\begin{tabular}{l|@{~}l|@{~}l|@{~}l|@{~}l|@{~}l|@{~}l|@{~}l|@{~}l}
& \rotatebox[origin=l]{90}{AIDA-B} & 
\rotatebox[origin=l]{90}{MSNBC} & 
\rotatebox[origin=l]{90}{OKE-2015} & 
\rotatebox[origin=l]{90}{OKE-2016} & 
\rotatebox[origin=l]{90}{N3-Reuters-128} & 
\rotatebox[origin=l]{90}{N3-RSS-500} &
\rotatebox[origin=l]{90}{Derczynski} & 
\rotatebox[origin=l]{90}{KORE50}
\\
\textbf{Macro F1} &&&&&&&\\
\textbf{Micro F1} &&&&&&&\\
\hline
DBpedia & 53.7 & 43.6 & 30.4 & 43.0 & 41.8 & 42.6 & 50.3 & 48.7  \\
Spotlight & 56.1 & 42.1 & 35.8 & 43.1 & 43.4 & 34.6 & 43.3 & 52.3 \\
\hline
\multirow{2}*{WAT} & 79.8 & 79.7 & 62.2 & 0.0 & 59.2 & 62.8 & 70.4 & 52.4 \\
 & 80.5 & 78.8 & 64.9 & 0.0 & 63.1 & 63.9 & 69.5 & 62.2  \\
\hline
\multirow{2}*{SOTA NLP} & 83.8 & 88.5 & \textbf{73.2} & \textbf{76.7} & \textbf{63.4} & \textbf{66.6} & \textbf{65.3} & 52.4 \\
 & 83.0 & 86.2 & \textbf{74.0} & \textbf{78.1} & \textbf{67.3} & \textbf{68.6} & \textbf{65.4} & 60.8  \\
\hline
 \multirow{2}*{REL (2014)} & \textbf{85.5} & \textbf{89.6} & 65.5 & 72.0 & 59.8 & 61.0 & 61.9 & \textbf{61.9}\\
  & \textbf{86.6} & \textbf{88.5} & 65.8 & 72.2 & 64.9 & 62.8 & 62.1 & \textbf{64.6}\\
 \hline
\multirow{2}*{REL (2019)} & 82.9 & 86.3 & 64.0 & 67.0 & 58.2 & 61.7 & 62.3 & 54.4 \\
 & 84.0 & 85.8 & 64.3 & 67.3 & 64.9 & 64.1 & 62.0 & 54.0  \\
\hline
\end{tabular}
\miniskip
\end{table}

We compare REL with a state-of-the-art end-to-end entity linking~\cite{Kolitsas:2018:ENE}, referred to as SOTA NLP, and two popular well-established entity linking systems: (i) DBpedia-spotlight~\cite{Mendes:2011:DSS} and (ii) WAT~\cite{Piccinno:2014:WAT}, the updated version of \textsc{TagMe}~\cite{Ferragina:2010:TOA}.
We report the results for two versions of our system. The first one, denoted as \emph{REL (2014)}, is based on the original implementation of~\cite{Le:2018:IEL} for ED.  It uses Wikipedia 2014 as the reference knowledge base and employs entity embeddings provided by~\cite{Ganea:2017:DJE} for candidate selection. The second version of our system, denoted as \emph{REL (2019)}, is based on Wikipedia 2019-07 and uses Wikipedia2Vec embeddings; cf. Section~\ref{sec:Imp}. 


We use the GERBIL platform~\cite{Roder:2018:Gerbil} for evaluation, and report on micro and macro InKB F1 scores for both EL and ED. Table~\ref{tbl:el-gerbil} shows the strong matching results for EL, where strong refers to the requirement of  exactly predicting the gold mention boundaries. We first note that REL outperforms the well-established entity linking toolkits (DBpedia Spotlight and WAT) by a large margin. Comparing with SOTA NLP, we observe that REL (2019) outperforms (or performs on par with) SOTA NLP on half of the datasets. 
The ED results in Table~\ref{tbl:ed-gerbil} also show consistent and significant improvements of REL over the two well-established toolkits. SOTA NLP, however, obtains better results than REL for all, except three datasets.
For both EL and ED results, we observe that REL (2014) achieves better results compared to REL (2019). This can be attributed to the different embeddings used for candidate selection: the recall of candidate entities chosen by their similarity to the context of the mentions is lower in REL (2019) when compared to REL (2014).

For a reference comparison, we also report the results of the ED method (referred to as MulRel-NEL) as reported in~\cite{Le:2018:IEL}; see Table~\ref{tbl:el_local}. The micro F1 score reported in this table is computed locally and by matching ED results against the original datasets. The results show that REL (2014) and MulRel-NEL scores are almost identical, which attests to the repeatability of~\cite{Le:2018:IEL}. Again, we observe a decrease in performance when comparing REL (2019) to REL (2014), just like in Table~\ref{tbl:ed-gerbil}.

Finally, we report on the runtime efficiency of REL in Table~\ref{tbl:efficiency}.  Specifically, we measure efficiency on a random sample of 50 documents (with a minimum length of 200 words) taken from AIDA-B.  
The experiments were run on a laptop with Intel i7 CPU (2.80GHz), 16GB RAM, and an NVIDIA Geforce GTX 1050 (4GB) GPU. The results show that detecting the mentions takes considerably more time than ED, and is done more efficiently using GPU. The ED time, however, is less affected by the GPU usage. This indicates that the overall efficiency of REL can be improved by replacing MD with a more efficient NER approach.


\begin{table}[t]
\centering
\caption{Local ED results as reported in \cite{Le:2018:IEL}}
\label{tbl:el_local}
\small
\begin{tabular}{l|l|l|l|l|l|l}
& \rotatebox[origin=l]{90}{AIDA-B} &
\rotatebox[origin=l]{90}{ACE2004} &
\rotatebox[origin=l]{90}{Aquaint} &
\rotatebox[origin=l]{90}{CLUEWEB} &
\rotatebox[origin=l]{90}{MSNBC} &
\rotatebox[origin=l]{90}{Wikipedia} \\
\textbf{Micro F1} &&&&&& \\
\hline
MulRel-NEL~\cite{Le:2018:IEL} & 93.1 & 89.9 & 88.3 & 77.5 & 93.9 & 78.0
\\
\hline
REL (2014) & 92.8 & 89.7 & 87.4 & 77.6 & 93.5 & 78.7 \\
\hline
REL (2019) & 89.4 & 85.3 & 84.1 & 71.9 & 90.7 & 73.1 \\
\hline
\end{tabular}
\miniskip
\end{table}




\begin{table}[t]
\centering
\caption{Efficiency of REL (in seconds) for 50 documents from AIDA-B with  200 words, which is 323 ( 105) words and 42 ( 19) mentions per document.}
\label{tbl:efficiency}
\small
\begin{tabular}{l|ll}
\hline
& \textbf{Time MD} & \textbf{Time ED} \\
\hline
With GPU & 0.440.22 & 0.240.08   \\
Without GPU & 2.411.24 & 0.180.09 \\
\hline
\end{tabular}
\miniskip
\end{table}

 \section{Conclusion}
We have introduced the Radboud Entity Linker (REL), an open source toolkit for entity linking. REL builds on state-of-the-art neural components from natural language processing research, and is provided as a Python package and as a web API. Currently, REL is optimized for annotating documents and short texts. In the future,  we plan to train REL on a large corpus of annotated queries and make it available for the task of entity linking in queries as well.



 
\bibliographystyle{ACM-Reference-Format}
\bibliography{ELdemo}

\end{document}  
