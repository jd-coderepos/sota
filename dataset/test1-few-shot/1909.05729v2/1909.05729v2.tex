\documentclass{article} 
\usepackage{iclr2020_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak


\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{amsthm}
\usepackage{natbib}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{caption}


\newtheorem{definition}{\textsc{Definition}}
\newtheorem{theo}{\textsc{Theorem}}
\newtheorem{lemma}{\textsc{Lemma}}
\newtheorem{corollary}{\textsc{Corollary}}
\newtheorem{assumption}{\textsc{Assumption}}

\newtheorem{appendix_theo}{\textsc{Theorem}}
\newtheorem{appendix_lemma}{\textsc{Lemma}}
\newtheorem{appendix_corollary}{\textsc{Corollary}}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\newcommand{\mb}{\mathbf}
\newcommand{\mc}{\mathcal}
\newcommand{\bs}{\boldsymbol}

\newcommand{\gresnet}{\textsc{GResNet}}
\newcommand{\res}{\textsc{ResNet}}
\newcommand{\gcn}{\textsc{GCN}}
\newcommand{\gat}{\textsc{GAT}}
\newcommand{\dif}{\textsc{DifNN}}
\newcommand{\loopy}{\textsc{LoopyNet}}


\title{{\gresnet}: Graph Residual Network for Reviving Deep GNNs from Suspended Animation}

\author{Jiawei Zhang\thanks{
IFM Lab\\
Florida State University\\
\texttt{\{jiawei@ifmlab.org}
}
}

\begin{document}

\maketitle

\textbf{Jiawei Zhang} and \textbf{Lin Meng} \\
IFM Lab\\
Florida State University\\
\texttt{jiawei@ifmlab.org, lin@ifmlab.org}\\

\begin{abstract}

The existing graph neural networks (GNNs) based on the spectral graph convolutional operator have been criticized for its performance degradation, which is especially common for the models with deep architectures. In this paper, we further identify the suspended animation problem with the existing GNNs. Such a problem happens when the model depth reaches the suspended animation limit, and the model will not respond to the training data any more and become not learnable. Analysis about the causes of the suspended animation problem with existing GNNs will be provided in this paper, whereas several other peripheral factors that will impact the problem will be reported as well. To resolve the problem, we introduce the {\gresnet} (Graph Residual Network) framework in this paper, which creates extensively connected highways to involve nodes' raw features or intermediate representations throughout the graph for all the model layers. Different from the other learning settings, the extensive connections in the graph data will render the existing simple residual learning methods fail to work. We prove the effectiveness of the introduced new graph residual terms from the norm preservation perspective, which will help avoid dramatic changes to the node's representations between sequential layers. Detailed studies about the {\gresnet} framework for many existing GNNs, including {\gcn}, {\gat} and {\loopy}, will be reported in the paper with extensive empirical experiments on real-world benchmark datasets.

\end{abstract}
\vspace{-15pt}
\section{Introduction}\label{sec:introduction}
\vspace{-8pt}

Graph neural networks (GNN), e.g., \textit{graph convolutional network} ({\gcn}) \cite{KW16} and \textit{graph attention network} ({\gat}) \cite{PGAAPY18}, based on the approximated spectral graph convolutional operator \cite{HVG11}, can learn the representations of the graph data effectively. Meanwhile, such GNNs have also received lots of criticism, since as these GNNs' architectures go deep, the models' performance will get degraded, which is similar to observations on other deep models (e.g., convolutional neural network) as reported in \cite{HZRS15}. Meanwhile, different from the existing deep models, when the GNN model depth reaches a certain limit (e.g., depth  for {\gcn} with the bias term disabled or depth  for {\gcn} with the bias term enabled on the Cora dataset), the model will not respond to the training data any more and become not learnable. Formally, we name such an observation as the GNNs' \textit{suspended animation problem}, whereas the corresponding model depth is named as the \textit{suspended animation limit} of GNNs. Here, we need to add a remark: to simplify the presentations in this paper, we will first take vanilla {\gcn} as the base model example to illustrate our discoveries and proposed solutions in the method sections. Meanwhile, empirical tests on several other existing GNNs, e.g., {\gat} \cite{PGAAPY18} and {\loopy} \cite{difnet}, will also be studied in the experiment section of this paper.

As illustrated in Figure~\ref{fig:gcn_acc_analysis}, we provide the learning performance of the {\gcn} model on the Cora dataset, where the learning settings (including train/validation/test sets partition, algorithm implementation and fine-tuned hyper-parameters) are identical to those introduced in \cite{KW16}. The {\gcn} model with the bias term disable of seven different depths, i.e., {\gcn}(1-layer)-{\gcn}(7-layer), are compared. Here, the layer number denotes the sum of hidden and output layers, which is also equal to the number of spectral graph convolutional layers involved in the model. For instance, besides the input layer, {\gcn}(7-layer) has 6 hidden layer and 1 output layer, both of which involve the spectral graph convolutional operations. According to the plots, {\gcn}(2-layer) and {\gcn}(3-layer) have comparable performance, which both outperform {\gcn}(1-layer). Meanwhile, as the model depth increases from  to , its learning performance on both the training set and the testing set degrades greatly. It is easy to identify that such degradation is not caused by overfitting the training data. What's more, much more surprisingly, as the model depth goes deeper to  or more, it will suffer from the \textit{suspended animation problem} and does not respond to the training data anymore. (Similar phenomena can be observed for {\gcn} (bias enabled) and {\gat} as illustrated by Figures~\ref{fig:gcn_acc_analysis_bias_enabled} and \ref{fig:gresnet_gat_analysis} in the appendix of this paper, whose suspended animation limits are  and , respectively. Meanwhile, on {\loopy}, we didn't observe such a problem as shown in Figure~\ref{fig:gresnet_loopy_analysis} in the appendix, and we will state the reasons in Section~\ref{sec:experiment} in detail.)

\begin{figure}
\vspace{-40pt}
    \centering
    \begin{subfigure}[b]{.45\textwidth}
    	\includegraphics[width=\linewidth]{./train_acc.png}
    	\caption{Training Accuracy}\label{fig:gcn_acc_train}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{.45\textwidth}
    	\includegraphics[width=\linewidth]{./test_acc.png}
    	\caption{Testing Accuracy}\label{fig:gcn_acc_test}
    \end{subfigure}\vspace{-5pt}
    \caption{The learning performance of {\gcn} (bias disabled) with 1-layer, 2-layer, , 7-layer on the Cora dataset. The x axis denotes the iterations over the whole training set. The y axis of the left plot denotes the training accuracy, and that of the right plot denotes the testing accuracy.}\label{fig:gcn_acc_analysis}
\vspace{-15pt}
\end{figure}


In this paper, we will investigate the causes of the GNNs' \textit{suspended animation problem}, and analyze if such a problem also exists in all other GNN models or not. GNNs are very different from the traditional deep learning models, since the extensive connections among the nodes render their learning process no longer independent but strongly correlated. Therefore, the existing solutions proposed to resolve such problems, e.g., residual learning methods used in ResNet for CNN \cite{HZRS15}, cannot work well for GNNs actually. In this paper, several different novel \textit{graph residual terms} will be studied for GNNs specially. Equipped with the new \textit{graph residual terms}, we will further introduce a new graph neural network architecture, namely \textit{graph residual neural network} ({\gresnet}), to resolve the observed problem. Instead of merely stacking the spectral graph convolution layers on each other, the extensively connected high-ways created in {\gresnet} allow the raw features or intermediate representations of the nodes to be fed into each layer of the model. We will study the effectiveness of the {\gresnet} architecture and those different \textit{graph residuals} for several existing vanilla GNNs. In addition, theoretic analyses on {\gresnet} will be provided in this paper as well to demonstrate its effectiveness from the norm-preservation perspective.

The remaining parts of this paper are organized as follows. In Section~\ref{sec:related_work}, we will introduce the related work of this paper. The suspended animation problem with the spectral graph convolutional operator will be discussed in Section~\ref{sec:analysis}, and the suspended animation limit will be analyzed in Section~\ref{sec:limited_analysis}. Graph residual learning will be introduced in Section~\ref{sec:method}, whose effectiveness will be tested in Section~\ref{sec:experiment}. Finally, we will conclude this paper in Section~\ref{sec:conclusion}.

\vspace{-8pt}
\section{Related Work}\label{sec:related_work}
\vspace{-8pt}

\noindent \textbf{Graph Neural Network}: Graph neural networks \cite{MBMRSB17, AT16, MBBV15, KW16, BHBSZMTRSF18, BDSW18, SGTHM09, ZCZYLS18, NAK16} have become a popular research topic in recent years. Traditional deep models cannot be directly applied to graph data due to the graph inter-connected structures. Many efforts have been devoted to extend deep neural networks on graphs for representation learning. GCN proposed in \cite{KW16} feeds the generalized spectral features into the convolutional layer for representation learning. Similar to {\gcn}, deep loopy graph neural network \cite{loopynet} proposes to update the node states in a synchronous manner, and it introduces a spanning tree based learning algorithm for training the model. {\loopy} accepts nodes' raw features into each layer of the model, and it can effectively fight against the suspended animation problem according to the studied in this paper. GAT \cite{PGAAPY18}  leverages masked self-attentional layers to address the shortcomings of GCN. In this year, we have also witnessed some preliminary works on heterogeneous graph neural networks \cite{WJSWYCY19, LCYZLS18}. Similar to GCN, GEM \cite{LCYZLS18} utilizes one single layer of attention to capture the impacts of both neighbors and network heterogeneity, which cannot work well on real-world complex networks. Based on GAT, HAN \cite{WJSWYCY19} learns the attention coefficients between the neighbors based on a set of manually crafted meta paths \cite{SBGAH11}, which may require heavy human involvements. {\dif} \cite{difnet} introduce a diffusive neural network for the graph structured data specifically, which doesn't suffer from the oversmoothing problem due to the involvement of the neural gates and residual inputs for all the layers. Due to the limited space, we can only name a few number of the representative graph neural network here. The readers are also suggested to refer to page\footnote{https://paperswithcode.com/task/node-classification}, which provides a summary of the latest graph neural network research papers with code on the node classification problem. 

\noindent \textbf{Residual Network}: Residual learning \cite{SGS15, HZRS15, BYY16, HKK16, GRUG17, TYL17, YKF17, AKS18, LFMZ18, BGCDJ19} has been utilized to improve the learning performance, especially for the neural network models with very deep architectures. To ease gradient-based training of very deep networks, \cite{SGS15} introduces the highway network to allow unimpeded information flow across several layers. Innovated by the high-way structure, \cite{HZRS15} introduces the residual network to simplify highway network by removing the fusion gates. After that, residual learning has been widely adopted for deep model training and optimization. \cite{BYY16} introduces residual learning for image restoration via persistent homology-guided manifold simplification; \cite{TYL17, LFMZ18} introduces a recursive residual network for image resolution adjustment. Some improvement of residual network has also been proposed in recent years. A reversible residual network is introduced in \cite{GRUG17}, where each layer's activations can be reconstructed exactly from the next layer's; \cite{HKK16} improves the conventional model shape with a pyramidal residual network instead; \cite{YKF17} introduce the dilated residual network to increases the resolution of output feature maps without reducing the receptive field of individual neurons; and \cite{AKS18} studies the cascading residual network as an accurate and lightweight deep network for image super-resolution. Readers can also refer to \cite{residual_tutorial} for a detailed tutorial on residual learning and applications in neural network studies.
\vspace{-8pt}
\section{Suspended Animation Problem with {\gcn} Model}\label{sec:analysis}
\vspace{-8pt}

In this part, we will provide an analysis about the \textit{suspended animation} problem of the spectral graph convolutional operator used in {\gcn} to interpret the causes of the observations illustrated in Figure~\ref{fig:gcn_acc_analysis}. In addition, given an input network data, we will provide the theoretic bound of the \textit{suspended animation limit} for the {\gcn} model.

\vspace{-8pt}
\subsection{Vanilla Graph Convolutional Network Revisit}
\vspace{-8pt}

To make this paper self-contained, we will provide a brief revisit of the vanilla {\gcn} model in this part. Formally, given an input network , its network structure information can be denoted as an adjacency matrix  (where ). {\gcn} defines the normalized adjacency matrix of the input network as , where  and  is the diagonal matrix of  with entry . Given all the nodes in  together with their raw feature inputs  ( denotes the node raw feature length), {\gcn} defines the \textit{spectral graph convolutional} operator to learn the nodes' representations as follows:

where  is the variable involved in the operator.

Furthermore, {\gcn} can involve a deep architecture by stacking multiple \textit{spectral graph convolutional} layers on each other, which will be able to learn very complex high-level representations of the nodes. Here, let's assume the model depth to be  (i.e., the number of hidden layers and output layer), and the corresponding node representation updating equations can be denoted as:


\vspace{-8pt}
\subsection{Suspended Animation Problem with {\gcn}}
\vspace{-8pt}

By investigating the \textit{spectral graph convolutional} operator defined above, we observe that it actually involves two sequential steps:

where the first term on the right-hand-side defines a 1-step Markov chain (MC or a random walk) based on the graph and the second term is a fully-connected (FC) layer parameterized by variable . Similar observations have been reported in \cite{LHW18} as well, but it interprets the \textit{spectral graph convolutional} operator in a different way as the \textit{Laplacian smoothing} operator used in mesh smoothing in graphics instead.

Therefore, stacking multiple \textit{spectral graph convolutional} layers on top of each other is equivalent to the stacking of multiple 1-step Markov chain layers and fully-connected layers in a crosswise way. Considering that the variables  for the vector dimension adjustment are shared among all the nodes, given two nodes with identical representations, the fully-connected layers (parameterized by  will still generate identical representations as well. In other words, the fully-connected layers with shared variables for all the nodes will not have significant impacts on the convergence of Markov chain layers actually. Therefore, in the following analysis, we will simplify the model structure by assuming the mapping defined by fully-connected layers to the identity mapping. We will investigate the Markov chain layers closely by picking them out of the model to compose the Markov chain of multiple steps:

Meanwhile, the Markov chain layers may converge with  layers iff , i.e., the representations before and after the updating are identical (or very close), which is highly dependent on the input network structure, i.e., matrix , actually.

\begin{definition}
(Irreducible and Aperiodic Network): Given an input network ,  is \textit{irreducible} iff for any two nodes , node  is accessible to . Meanwhile,  is \textit{aperiodic} iff  is not bipartite.
\end{definition}

\begin{lemma}\label{lemma:random_walk}
Given an unweighted input graph , which is \textit{irreducible}, \textit{finite} and \textit{aperiodic}, if its corresponding matrix is asymmetric, starting from any initial distribution vector  ( and ), the Markov chain operating on the graph has one unique stationary distribution vector  such that , where . Meanwhile, if matrix  is symmetric, the stationary distribution vector  will be a uniform distribution over the nodes, i.e., .
\end{lemma}

Based on the above Lemma~\ref{lemma:random_walk}, we can derive similar results for the multiple Markov chain layers in the {\gcn} model based on the nodes' feature inputs, which will reduce the learned nodes' representations to the stationary representation matrix.

\begin{theo}\label{theo:convergence}
Given a input network , which is \textit{unweighted}, \textit{irreducible}, \textit{finite} and \textit{aperiodic}, if there exist enough nested Markov chain layers in the {\gcn} model, it will reduce the nodes' representations from the column-normalized feature matrix  to the stationary representation . Furthermore, if  is undirected, then the stationary representation will become .
\end{theo}

Theorem~\ref{theo:convergence} illustrates the causes of the GNNs' suspended animation problem. Proofs of Lemma~\ref{lemma:random_walk} and Theorem~\ref{theo:convergence} are provided in the appendix attached to this paper at the end.

\vspace{-8pt}
\section{Suspended Animation Limit Analysis}\label{sec:limited_analysis}
\vspace{-8pt}

Here, we will study the \textit{suspended animation limit} of the {\gcn} model based on its spectral convolutional operator analysis, especially the Markov chain layers. 

\vspace{-8pt}
\subsection{Suspended Animation Limit based Input Network Structure}
\vspace{-8pt}

Formally, we define the \textit{suspended animation limit} of {\gcn} as follows:

\begin{definition}
(Suspended Animation Limit): The \textit{suspended animation limit} of {\gcn} on network  is defined as the smallest model depth  such that for any nodes' column-normalized featured matrix input  in the network  the following inequality holds:

For representation convenience, we can also denote the \textit{suspended animation limit} of {\gcn} defined on network  as  (or  for simplicity if there is no ambiguity problems).
\end{definition}

Based on the above definition, for {\gcn} with identity FC mappings, there exists a tight bound of the \textit{suspended animation limit} for the input network.

\begin{theo}\label{theo:limit}
Let  be the eigen-values of matrix  defined based on network , then the corresponding \textit{suspended animation limit} of the {\gcn} model on  is bounded

In the case that the network  is a \textit{d-regular}, then the \textit{suspended animation limit} of the {\gcn} model on  can be simplified as

\end{theo}

The \textit{suspended animation limit} bound derived in the above theorem generally indicates that the network structure  determines the maximum allows depth of {\gcn}. Among all the eigen-values of  defined on network ,  measures how far  is from being disconnected; and  measures how far  is from being bipartite. In the case that  is \textit{reducible} (i.e., ) or \textit{bipartite} (i.e., ), we have  and the model will not suffer from the \textit{suspended animation problem}.

In the appendix of \cite{KW16}, the authors introduce a naive-residual based variant of {\gcn}, and the \textit{sepctral graph convolutional} operator is changed as follows (the activation function is also changed to sigmoid function instead):


For the identity fully-connected layer mapping, the above equation can be reduced to the following \textit{lazy Markov chain} based layers

Such a residual term will not help resolve the problem, and it will still suffer from the suspended animation problem with the following suspended animation limit bound:

\begin{corollary}\label{coro:limit}
Let  be the eigen-values of matrix  defined based on network , then the corresponding \textit{suspended animation limit} of the {\gcn} model (with \textit{lazy Markov chain} based layers) on  is bounded

\end{corollary}

Proofs to Theorem~\ref{theo:limit} and Corollary~\ref{coro:limit} will be provided in the appendix as well.

\vspace{-8pt}
\subsection{Other Practical Factors Impacting the Suspended Animation Limit}
\vspace{-8pt}

According to our preliminary experimental studies, {\gcn} is quite a sensitive model. Besides the impact of input network explicit structures (e.g., network size , directed vs undirected links, and network eigenvalues) as indicated in the bound equation in Theorem~\ref{theo:limit}, many other factors can also influence the performance of {\gcn} model a lot, which are summarized as follows:
\begin{itemize}

\item \textbf{Network Degree Distribution}: According to Theorem~\ref{theo:convergence}, if the input network  is directed and unweighted, the Markov chain layers at convergence will project the features to , where  is determined by the degree of node  in the network. For any two nodes , the differences of their learned representation can be denoted as 

According to \cite{power_law}, the node degree distributions in most of the networks follow the power-law, i.e., majority of the nodes are of very small degrees. Therefore, for massive nodes in the input network with the same (or close) degrees, the differences between their learned representations will become not distinguishable.

\item \textbf{Raw Feature Coding}: Besides the input network, the raw feature coding can also affect the learning performance greatly. Here, we can take the {\gcn} with one single Markov chain layer and one identity mapping layer. For any two nodes  with raw feature vectors  and , we can denote the differences between their learned representations as follows:

For the one-hot feature coding used in the source code of {\gcn} \cite{KW16} and other GNNs, matrix  can be also very sparse as well. Meanwhile, vector  is also a sparse vector, which renders the right-hand-side term to be a very small value.

\item \textbf{Training Set Size}: Actually, the nodes have identical representation and the same labels will not degrade the learning performance of the model. However, if such node instances actually belong to different classes, it will become a great challenge for both the training and test stages of the {\gcn} model. 

\item \textbf{Gradient Vanishing/Exploding}: Similar to the existing deep models, deep GNNs will also suffer from the gradient vanishing/exploding problems \cite{PMB12}, which will also greatly affect the learning performance of the models.

\end{itemize}

Although these factors mentioned above are not involved in the \textit{suspended animation limit} bound representation, but they do have great impacts on the {\gcn} model in practical applications. In the following section, we will introduce \textit{graph residual network} {\gresnet}, which can be useful for resolving such a problem for {\gcn}.














\vspace{-8pt}
\section{Graph Residual Network}\label{sec:method}
\vspace{-8pt}

Different from the residual learning in other areas, e.g., computer vision \cite{HZRS15}, where the objective data instances are independent with each other, in the inter-connected network learning setting, the residual learning of the nodes in the network are extensively connected instead. It renders the existing residual learning strategy less effective for improving the performance of {\gcn}.

\begin{table}[t]
\vspace{-35pt}
\caption{A Summary of Graph Residual Terms and Physical Meanings.} \label{tab:residual_summary}
\vspace{-5pt}
\hspace{-20pt}
\centering
\setlength{\tabcolsep}{0.2em}
\begin{tabular}{p{1.7cm} p{4.5cm} p{8.0cm}}
\hline
Name & Residual Term    & Description\\
\hline
\hline
naive residual & \vspace{1pt}\vspace{1pt}       & Node residual terms are assumed to be independent and determined by the current state only.\\
\hline
graph-naive residual  &\vspace{1pt}       & Node residual terms are correlated based on network structure, and can be determined by the current state. \\
\hline
raw residual  &\vspace{1pt}       & Node residual terms are assumed to be independent and determined by the raw input features only. \\
\hline
graph-raw residual  &\vspace{1pt}       & Node residual terms are correlated based on network structure, and are determined by the raw input features. \\
\hline
\end{tabular}
\vspace{-10pt}
\end{table}


\vspace{-8pt}
\subsection{Graph Residual Learning}
\vspace{-8pt}

Residual learning initially introduced in \cite{HZRS15} divides the objective mapping into two parts: the inputs and the residual function. For instance, let  be the objective mapping which projects input  to the desired domain. The ResNet introduced in \cite{HZRS15} divides  as  (where  is used in \cite{HZRS15}). This reformulation is motivated by the counterintuitive phenomena about the degradation problem observed on the deep CNN. Different from the learning settings of CNN, where the data instances are assumed to be independent, the nodes inside the input network studied in {\gcn} are closely correlated. Viewed in such a perspective, new residual learning mechanism should be introduced for {\gcn} specifically. 

\underline{\textbf{Here, we need to add a remark}}: For the presentation simplicity in this paper, given the objective  , we will misuse the terminologies here: we will name  as the approximated mapping of , and call  as the \textit{graph residual term}. Formally, by incorporating the {residual learning} mechanism into the {\gcn} model, the node representation updating equations (i.e., Equation~\ref{equ:gcn_update}) can be rewritten as follows:


The \textit{graph residual term}  can be defined in different ways. We have also examined to put  outside of the ReLU() function for the hidden layers (i.e., ), whose performance is not as good as what we show above. In the appendix of \cite{KW16}, by following the ResNet (CNN) \cite{HZRS15}, the \textit{graph residual term}  is simply defined as , which is named as the \textit{naive residual term} in this paper (Here, term ``naive'' has no disparaging meanings). However, according to the studies, such a simple and independent residual term for the nodes fail to capture information in the inter-connected graph learning settings.

\vspace{-8pt}
\subsection{{\gresnet} Architecture}
\vspace{-8pt}


\begin{figure}[t]
\vspace{-40pt}
    \begin{minipage}{\textwidth}
    \centering
    	\includegraphics[width=1.0\linewidth]{./architecture.pdf}
	\vspace{-25pt}
    	\caption{A comparison of vanilla {\gcn} and {\gresnet} (\gcn) with different graph residual terms. The vanilla {\gcn} has two layers, and the {\gresnet} models have a deep architecture which involves seven layers of SGC operators. The intermediate ReLU activation functions used between sequential layers in {\gresnet} are omitted for simplicity. Notation {\tiny \fbox{}} denotes the normalized adjacency matrix of input network , which indicates the correlated graph residual terms.}
    	\label{fig:architecture}
    \end{minipage}\vspace{-15pt}
\end{figure}


In this paper, we introduce several other different representation of the \textit{graph residual terms}, which are summarized in Table~\ref{tab:residual_summary}. If feature dimension adjustment is needed, e.g., for raw residual term, an extra variable matrix  can be added to redefine the terms (which are not shown in this paper). For the graph residual term representations in Table~\ref{tab:residual_summary}, \textit{naive residual} and \textit{raw residual} are based on the assumption that node residuals are independent and determined by either the current state or the raw features. Meanwhile, the \textit{graph naive residual} and \textit{graph raw residual} assume the residual terms of different nodes are correlated instead, which can be computed with the current state or raw features. We also illustrate the architectures of vanilla 2-layer {\gcn} and the 7-layer {\gresnet}s (taking {\gcn} as the base model) with different \textit{graph residual terms} in Figure~\ref{fig:architecture}.

\textbf{Vanilla {\gcn}}: For the vanilla {\gcn} network used in \cite{KW16}, i.e., the left plot of Figure~\ref{fig:architecture}, given the inputs  and , it employs two SGC layers to project the input to the objective labels. For the intermediate representations, the hidden layer length is , and ReLU is used as the activation function, whereas softmax is used for output label normalization.

\textbf{{\gresnet} Network}: For the {\gresnet} network, i.e., the right four models in Figure~\ref{fig:architecture}, they accept the identical inputs as vanilla {\gcn} but will create \textit{graph residual terms} to be added to the intermediate representations. Depending on the specific residual term representations adopted, the corresponding high-way connections can be different. For the hidden layers involved in the models, their length is also , and ReLU is used as the activation function for the intermediate layers.

By comparing the output  learned by the models against the ground-truth  of the training instances, all the variables involved in the model, i.e., , can be effectively learned with the back-propagation algorithm to minimize the loss functions   (or  for simplicity). In the following part, we will demonstrate that for the {\gresnet} model added with graph residual terms. It can effectively avoid dramatic changes to the nodes' representations between sequential layers.

\begin{figure}
\vspace{-30pt}
    \centering
\begin{subfigure}[b]{.24\textwidth}
    	\includegraphics[width=\linewidth]{./gresnet_gcn_naive_train_acc.png}
	\vspace{-10pt}
	\captionsetup{justification=centering}
    	\caption{\small Training Accuracy {\gresnet}(naive)}\label{fig:gresnet_naive_acc_train}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{.24\textwidth}
    	\includegraphics[width=\linewidth]{./gresnet_gcn_naive_test_acc.png}
	\vspace{-10pt}
	\captionsetup{justification=centering}
    	\caption{Testing Accuracy {\gresnet}(naive)}\label{fig:gresnet_naive_acc_test}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{.24\textwidth}
    	\includegraphics[width=\linewidth]{./gresnet_gcn_graph_naive_train_acc.png}
	\vspace{-10pt}
	\captionsetup{justification=centering}
    	\caption{Training Accuracy {\gresnet}(graph-naive)}\label{fig:gresnet_graph_naive_acc_train}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{.24\textwidth}
    	\includegraphics[width=\linewidth]{./gresnet_gcn_graph_naive_test_acc.png}
	\vspace{-10pt}
	\captionsetup{justification=centering}
    	\caption{Testing Accuracy {\gresnet}(graph-naive)}\label{fig:gresnet_graph_naive_acc_test}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{.24\textwidth}
    	\includegraphics[width=\linewidth]{./gresnet_gcn_raw_train_acc.png}
	\vspace{-10pt}
	\captionsetup{justification=centering}
    	\caption{Training Accuracy {\gresnet}(raw)}\label{fig:gresnet_raw_acc_train}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{.24\textwidth}
    	\includegraphics[width=\linewidth]{./gresnet_gcn_raw_test_acc.png}
	\vspace{-10pt}
	\captionsetup{justification=centering}
    	\caption{Testing Accuracy {\gresnet}(raw)}\label{fig:gresnet_raw_acc_test}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{.24\textwidth}
    	\includegraphics[width=\linewidth]{./gresnet_gcn_graph_raw_train_acc.png}
	\vspace{-10pt}
	\captionsetup{justification=centering}
    	\caption{Training Accuracy {\gresnet}(graph-raw)}\label{fig:gresnet_graph_raw_acc_train}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{.24\textwidth}
    	\includegraphics[width=\linewidth]{./gresnet_gcn_graph_raw_test_acc.png}
	\vspace{-10pt}
	\captionsetup{justification=centering}
    	\caption{Testing Accuracy {\gresnet}(graph-raw)}\label{fig:gresnet_graph_raw_acc_test}
    \end{subfigure}\vspace{-7pt}
    \caption{The learning performance of {\gresnet} with {\gcn} as the base model and different graph residual terms on the Cora dataset: (a)-(b) {\gresnet}({\gcn}, naive); (c)-(d) {\gresnet}({\gcn}, graph-naive); (e)-(f) {\gresnet}({\gcn}, raw); (g)-(h) {\gresnet}({\gcn}, graph-raw). For plot (b), the curves corresponding to the 5-layer, 6-layer and 7-layer models are hidden by the legend. For plot (c) and (d), the curve of 7-layer model is hidden by the legend.}\label{fig:gresnet_gcn_analysis}
     \vspace{-12pt}
\end{figure}


\vspace{-8pt}
\subsection{Graph Residual Learning Effectiveness Analysis}
\vspace{-8pt}

In this part, we will illustrate why the inclusion of the \textit{graph residual learning} can be effective for learning deep graph neural networks. Here, we assume the ultimate model that we want to learn as , where  and  denote the feature and label spaces, respectively. For analysis simplicity, we have some assumptions about the function  \cite{ZRS18}.
\begin{assumption}\label{assumption:conditions}
Function  is differentiable, invertible and satisfies the following conditions:
\begin{itemize}
\vspace{-7pt}
\item  with bounded norm, , ,
\vspace{-6pt}
\item  with bounded norm, , ,
\vspace{-6pt}
\item  with bounded norm such that .
\vspace{-7pt}
\end{itemize}
In the above conditions, terms  and  are constants.
\end{assumption}

To model the function , the {\gresnet} actually defines a sequence of  sequential mappings with these  layers:

where  and  denote the intermediate node representations serving as input and output of the  layer.  and  denote the function approximation and residual term learned by the  layer of the model. When training these  sequential mappings, we have the following theorem hold for the representation gradients in the learning process.

\begin{theo}\label{theo:norm_preservation}
Let  denote the objective function that we want to model, which satisfies Assumption~\ref{assumption:conditions}, in learning the -layer {\gresnet} model, we have the following inequality hold:

where  and  for some .
\end{theo}

Proof of Theorem~\ref{theo:norm_preservation} will be provided in the appendix. The above theorem indicates that in the learning process, the norm of loss function against the intermediate representations doesn't change significantly between sequential layers. In other words, {\gresnet} can maintain effective representations for the inputs and overcome the \textit{suspended animation problem}. In addition, we observe that the bound of the gap term , i.e., , decreases as  increases (when ). Therefore, for deeper {\gresnet}, the model will lead to much tighter gradient norm changes, which is a desired property. In the following section, we will provide the experimental results of {\gresnet} compared against their vanilla models on several graph benchmark datasets.































































































\begin{table}[t]
 \vspace{-35pt}
\caption{Best performance (accuracy) and model depth summarization of {\gresnet} with different residual terms on the benchmark datasets (we take {\gcn}, {\gat} and {\loopy} as the base models). }\label{tab:performance_summary}
 \vspace{-5pt}
 \small
\centering
\setlength{\tabcolsep}{3.5pt}
\begin{tabular}{|c|l|p{0.9cm}|p{1.2cm}|p{0.9cm}|p{1.2cm}|p{0.9cm}|p{1.2cm}| }
\hline
\multicolumn{2}{|c}{Methods } & \multicolumn{6}{|c|}{Datasets (Accuracy \& Model Depth)} \\
\hline
Base Models & Residuals & \multicolumn{2}{|c}{\textbf{Cora}} & \multicolumn{2}{|c}{\textbf{Citeseer}} & \multicolumn{2}{|c|}{\textbf{Pubmed}} \\
\hline 
\hline 
\multicolumn{2}{|c|}{vanilla {\gcn} (\cite{KW16})} &0.815 &2-layer  &0.703 &2-layer &0.790 &2-layer \\
\hline 
\multirow{4}{*}{{\gcn}}
&naive&0.814 &3-layer  &0.710 &3-layer &0.814 &3-layer \\
\cline{2-8}
&graph-naive&0.833 &2-layer  &0.715 &3-layer &0.811 &2-layer \\
\cline{2-8}
&raw&0.826 &4-layer  &\textbf{0.727} &\textbf{4-layer} &0.810 &3-layer \\
\cline{2-8}
&graph-raw&\textbf{0.843} &\textbf{5-layer}  &0.722 &4-layer &\textbf{0.817} &\textbf{7-layer} \\
\hline
\hline
\multicolumn{2}{|c|}{vanilla {\gat} (\cite{PGAAPY18})} &0.830 &2-layer  &0.725 &2-layer &0.790 &2-layer \\
\hline
\multirow{4}{*}{{\gat}}
&naive& 0.844 &5-layer  &\textbf{0.735} &\textbf{5-layer} &0.809 &3-layer \\
\cline{2-8}
&graph-naive&\textbf{0.855} &\textbf{3-layer}  &0.732 &4-layer &0.815 &5-layer \\
\cline{2-8}
&raw&0.842  &3-layer  &0.733 &3-layer &0.814 &4-layer \\
\cline{2-8}
&graph-raw&{0.847} &{3-layer}  &0.729 &5-layer &\textbf{0.822} &\textbf{4-layer} \\
\hline
\hline
\multicolumn{2}{|c|}{vanilla {\loopy} (\cite{loopynet})} &0.826 &2-layer  &0.716 &2-layer &0.812 &2-layer \\
\hline
\multirow{4}{*}{{\loopy}}
&naive&0.833 &2-layer  &0.728 &3-layer &\textbf{0.830} &\textbf{4-layer} \\
\cline{2-8}
&graph-naive&0.832 &2-layer  &0.728 &3-layer &0.819 &2-layer \\
\cline{2-8}
&raw& 0.836 &2-layer  &0.730 &5-layer &0.828 &4-layer \\
\cline{2-8}
&graph-raw&\textbf{0.839} &\textbf{4-layer}  &\textbf{0.737} &\textbf{5-layer} &0.814 &4-layer \\
\hline
\end{tabular}
\vspace{-8pt}
\end{table}


\vspace{-8pt}
\section{Experiments}\label{sec:experiment}
\vspace{-8pt}

To demonstrate the effectiveness of {\gresnet} in improving the learning performance for graph neural networks with deep architectures, extensive experiments will be done on several graph benchmark datasets. Similar to the previous works on node classification \cite{KW16, PGAAPY18}, the graph benchmark datasets used in the experiments include Cora, Citeseer and Pubmed from \cite{SNBGGE08}. For fair comparison, we follow exactly the same experimental settings as \cite{KW16} on these datasets.

In this paper, we aim at studying the suspended animation problems with the existing graph neural networks, e.g., {\gcn} \cite{KW16}, {\gat} \cite{PGAAPY18} and {\loopy} \cite{loopynet}, where {\loopy} is not based on the spectral graph convolutional operator. We also aim to investigate the effectiveness of these proposed graph residual terms in improving their learning performance, especially for the models with deep architectures. In addition, to make the experiments self-contained, we also provide the latest performance of the other baseline methods on the same datasets in this paper, which include state-of-the-art graph neural networks, e.g., APPNP \cite{KBG19}, GOCN \cite{JZTL19} and GraphNAS \cite{GYZZH19}, existing graph embedding models, like DeepWalk \cite{PAS14}, Planetoid \cite{YCS16} and MoNet \cite{MBMRSB16} and representation learning approaches, like ManiReg \cite{BNS06}, SemiEmb \cite{WRC08}, LP \cite{ZGL03} and ICA \cite{LG03}. 

\noindent \textbf{Reproducibility}: Both the datasets and source code used in this paper can be accessed via link\footnote{https://github.com/anonymous-sourcecode/GResNet}. Detailed information about the server used to run the model can be found at the footnote\footnote{GPU Server: ASUS X99-E WS motherboard, Intel Core i7 CPU 6850K@3.6GHz (6 cores), 3 Nvidia GeForce GTX 1080 Ti GPU (11 GB buffer each), 128 GB DDR4 memory and 128 GB SSD swap. For the deep models which cannot fit in the GPU memory, we run them with CPU instead.}.


\begin{table}[t]
\vspace{-35pt}
\caption{Learning result accuracy of node classification methods. In the table, `-' denotes the results of the methods on these datasets are not reported in the existing works. Performance of {\gcn}, {\gat} and {\loopy} shown in Table~\ref{tab:performance_summary} are not provided here to avoid reporting duplicated results.}\label{tab:performance_comparison}
 \vspace{-5pt}
\centering
\small
\begin{tabular}{l c c c c }
\toprule
 \multirow{2}{*}{Methods}  & \multicolumn{3}{c}{Datasets (Accuracy)} \\
 \addlinespace[0.05cm]
\cline{2-4}
\addlinespace[0.05cm]
& \textbf{Cora} & \textbf{Citeseer} & \textbf{Pubmed} \\
\addlinespace[0.05cm]
\hline
\addlinespace[0.05cm]

{LP (\cite{ZGL03}) } &0.680 &0.453 &0.630  \\
{ICA (\cite{LG03})} &0.751  &0.691  &0.739   \\
{ManiReg (\cite{BNS06})} &0.595  &0.601  &0.707   \\
{SemiEmb (\cite{WRC08})} &0.590  &0.596  &0.711  \\

\addlinespace[0.05cm]

\hline

\addlinespace[0.05cm]

{DeepWalk (\cite{PAS14})} &0.672  &0.432  &0.653   \\
{Planetoid (\cite{YCS16})} &0.757  &0.647  &0.772  \\
{MoNet (\cite{MBMRSB16})} &0.817  &-  &0.788  \\

\addlinespace[0.05cm]

\hline

\addlinespace[0.05cm]

{APPNP (\cite{KBG19})} &\textbf{0.851}  &\textbf{0.757}  &0.797   \\
{GOCN (\cite{JZTL19})} &\textbf{0.848}  &0.718  &0.797  \\
{GraphNAS (\cite{GYZZH19})} &-  &0.731  &0.769  \\

\addlinespace[0.05cm]



\bottomrule

\addlinespace[0.05cm]

{{\gresnet}({\gcn})} &{0.843}  &0.727  &\textbf{0.817}  \\
{{\gresnet}({\gat})} &\textbf{0.855}  &\textbf{0.735}  &  \textbf{0.822} \\
{{\gresnet}({\loopy})} &{0.839}  &\textbf{0.737}  &\textbf{0.830}  \\
\addlinespace[0.05cm]

\bottomrule
\end{tabular}
\vspace{-10pt}
\end{table}


\vspace{-8pt}
\subsection{Effectiveness of the Graph Residual Terms}
\vspace{-8pt}

In addition to Figure~\ref{fig:gcn_acc_analysis} for {\gcn} (bias disabled) on the Cora dataset, as shown in Figures~\ref{fig:gcn_acc_analysis_bias_enabled}-\ref{fig:gresnet_loopy_analysis} in the appendix, for the {\gcn} (bias enabled) and {\gat} with deep architectures, we have observed similar suspended animation problems. Meanwhile, the performance of {\loopy} is different. Since {\loopy} is not based on the spectral graph convolution operator, which accepts nodes' raw features in all the layer (it is quite similar to the \textit{raw} residual term introduced in this paper). As the model depth increase, performance of {\loopy} remains very close but converge much more slowly. By taking {\gcn} as the base model, we also show the performance of {\gresnet} with different residual terms in Figure~\ref{fig:gresnet_gcn_analysis}. By comparing these plots with Figure~\ref{fig:gcn_acc_analysis}, both \textit{naive} and \textit{graph-naive} residual terms help stabilize the performance of deep {\gresnet}({\gcn})s. Meanwhile, for the \textit{raw} and \textit{graph-raw} residual terms, their contributions are exceptional. With these two residual terms, deep {\gresnet}({\gcn})s can achieve even better performance than the shallow vanilla {\gcn}s.

Besides the results on the Cora dataset, in Table~\ref{tab:performance_summary}, we illustrate the best observed performance by {\gresnet} with different residual terms based on {\gcn}, {\gat} and {\loopy} base models respectively on all the datasets. Both the best accuracy score and the achieved model depth are provided. According to the results, for the vanilla models, {\gcn}, {\gat} and {\loopy} can all obtain the best performance with shallow architectures. For instance on Cora, {\gcn}(2-layer) obtains 0.815; {\gat}(2-layer) gets 0.830; and {\loopy}(2-layer) achieves 0.826, respectively. Added with the residual terms, the performance of all these models will get improved. In addition, deep {\gresnet}({\gcn}), {\gresnet}({\gat}) and {\gresnet}({\loopy}) will be able to achieve much better results than the shallow vanilla models, especially the ones with the \textit{graph-raw residual} terms. The best scores and the model depth for each base model on these datasets are also highlighted. The time costs of learning the {\gresnet} model is almost identical to the required time costs of learning the vanilla models with the same depth, which are not reported in this paper. 

\vspace{-8pt}
\subsection{A Complete Comparison with Existing Node Classification Methods}
\vspace{-8pt}

Besides the comparison with {\gcn}, {\gat} and {\loopy} shown in Table~\ref{tab:performance_summary}, to make the experimental studies more complete, we also compare {\gresnet}({\gcn}), {\gresnet}({\gat}) and {\gresnet}({\loopy}) with both the classic and the state-of-the-art models, whose results are provided in Table~\ref{tab:performance_comparison}. In the table, we didn't indicate the depth of the {\gresnet} models and results of {\gcn}, {\gat} and {\loopy} (shown in Table~\ref{tab:performance_summary} already) are not included. According to the results, compared against these baseline methods, {\gresnet}s can also outperform them with great advantages. Without the complex model architecture extension or optimization techniques used by the latest methods APPNP \cite{KBG19}, GOCN \cite{JZTL19} and GraphNAS \cite{GYZZH19}, adding the simple graph residual terms into the base models along can already improve the learning performance greatly.


















\vspace{-8pt}
\section{Conclusion}\label{sec:conclusion}
\vspace{-8pt}

In this paper, we focus on studying the suspended animation problem with the existing graph neural network models, especially the spectral graph convolutional operator. We provide a theoretic analysis about the causes of the suspended animation problem and derive the bound for the maximum allowed graph neural network depth, i.e., the suspended animation limit. To resolve such a problem, we introduce a novel framework {\gresnet}, which works well for learning deep representations from the graph data. Assisted with these new graph residual terms, we demonstrate that {\gresnet} can effectively resolve the suspended animation problem with both theoretic analysis and empirical experiments on several benchmark node classification datasets.

\newpage
\bibliography{reference}
\bibliographystyle{iclr2020_conference}


\newpage
\section{Appendix}\label{sec:appendix}

\subsection{Extra Experimental Results}


\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{.45\textwidth}
    	\includegraphics[width=\linewidth]{./gcn_train_bias_acc.png}
    	\caption{Training Accuracy}\label{fig:gcn_bias_enabled_acc_train}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{.465\textwidth}
    	\includegraphics[width=\linewidth]{./gcn_test_bias_acc.png}
    	\caption{Testing Accuracy}\label{fig:gcn_bias_enabled_acc_test}
    \end{subfigure}\caption{The learning performance of {\gcn} (bias enabled) on the Cora dataset.}\label{fig:gcn_acc_analysis_bias_enabled}
\end{figure}


\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{.4\textwidth}
    	\includegraphics[width=\linewidth]{./gat_train_acc.png}
    	\caption{Training Accuracy ({\gat})}\label{fig:gat_acc_train}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{.4\textwidth}
    	\includegraphics[width=\linewidth]{./gat_test_acc.png}
    	\caption{Testing Accuracy ({\gat})}\label{fig:gat_acc_test}
    \end{subfigure}\caption{The learning performance of {\gat} on the Cora dataset}\label{fig:gresnet_gat_analysis}
\end{figure}


\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{.4\textwidth}
    	\includegraphics[width=\linewidth]{./LoopyNet_train_acc.png}
    	\caption{Training Accuracy ({\loopy})}\label{fig:loopy_acc_train}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{.4\textwidth}
    	\includegraphics[width=\linewidth]{./LoopyNet_test_acc.png}
    	\caption{Testing Accuracy ({\loopy})}\label{fig:loopy_acc_test}
    \end{subfigure}\caption{The learning performance of {\loopy} on the Cora dataset}\label{fig:gresnet_loopy_analysis}
\end{figure}



\newpage

\subsection{Proofs of Theorem and Lemma}

\subsubsection{Proof of Lemma 1}

\begin{appendix_lemma}\label{lemma:appendix_stationary}
Given an \textit{irreducible}, \textit{finite} and \textit{aperiodic} graph , starting from any initial distribution vector  ( and ), the Markov chain operating on the graph has one unique stationary distribution vector  such that , where . If matrix  is symmetric (i.e.,  is undirected),  will be a uniform distribution over the nodes, i.e., .
\end{appendix_lemma}

\begin{proof}
The stationary distribution vector existence and uniqueness has been proved in \cite{N98}. Here, we need to identify on vector  at convergence such that , i.e.,


According to the definition of , it is easy to have

where  denotes the initial connection weight between  and . For the unweighted network,  will be in  indicating if  and  are connected or not. Notation  denotes the rough degree of node  in the network subject to the weight , which sums the weight of edges connected to the nodes in the network. So, it is enough to have . More precisely, we can set 


In this case,


Meanwhile, for the symmetric and normalized adjacency matrix , we can prove the stationary distribution  in a similar way,
which concludes the proof.
\end{proof}

\subsubsection{Proof of Theorem 1}

\begin{appendix_theo}
Given a input network , which is \textit{unweighted}, \textit{irreducible}, \textit{finite} and \textit{aperiodic}, if there exist enough nested Markov chain layers in the {\gcn} model, it will reduce the nodes' representations from the column-normalized feature matrix  to the stationary representation . Furthermore, if  is undirected, then the stationary representation will become .
\end{appendix_theo}

\begin{proof}
This theorem can be proved based on Lemma 1. For any initial state distribution vector , for the Markov chain at convergence, we have

We misuse the notation . In this case,
 
which together with Lemma~\ref{lemma:appendix_stationary} conclude the proof.
\end{proof}

\subsubsection{Proof of Theorem 2}

Prior to introducing the proof of Theorem 2, we will introduce the following lemma first.
\begin{appendix_lemma}\label{lemma:help_lemma}
For any vector , the following inequality holds:

\end{appendix_lemma}


\begin{proof}
According to H\"older's inequality \cite{H89}, for  and ,

Let ,  and , 

Therefore,

\end{proof}

\begin{appendix_theo}\label{theo:limit}
Let  be the eigen-values of matrix  defined based on network , then the corresponding \textit{suspended animation limit} of the {\gcn} model on  is tightly bounded

In the case that the network  is a \textit{d-regular}, then the \textit{suspended animation limit} of the {\gcn} model on  can be simplified as

\end{appendix_theo}

\begin{proof}
Instead of proving the above inequality directly, we propose to prove that  is \textit{suspended animation limit} by the following inequality instead

which can derive the following inequality according to Lemma~\ref{lemma:help_lemma}:


Let  be the eigenvectors of  and 

where .

Therefore, we have

\end{proof}

Considering that  and , then

and

where 

Therefore, we have

where .

Therefore, to ensure 

we can have


\subsubsection{Proof of Corollary 1}

\begin{appendix_corollary}
Let  be the eigen-values of matrix  defined based on network , then the corresponding \textit{suspended animation limit} of the {\gcn} model (with \textit{lazy Markov chain} based layers) on  is tightly bounded

\end{appendix_corollary}

\begin{proof}
For the \textit{lazy Markov chain} layer, we have its updating equation as follows

It is easy to show that  is positive definite and we have its eigen-values . Therefore,

which together with Theorem~\ref{theo:limit} conclude the proof.
\end{proof}

\subsection{Proof of Theorem 3}

Prior to introducing the proof of Theorem~\ref{theo:norm_preservation}, we first introduce the following lemma.

\begin{appendix_lemma}
For any non-singular matrix , we have

where  and  denote the maximum and minimum singular values of the input matrix, respectively.
\end{appendix_lemma}
\begin{proof}
Due to the triangle inequality, the upper bound is easy to prove:


In the case that , the lower bound is trivial to prove since  is non-singular, we have

Meanwhile, in the case that , it is easy to know that , where  denotes the latest eigenvalue of the input matrix.

which concludes the proof for the lower bound.
\end{proof}

\begin{appendix_theo}
Let  denote the objective function that we want to model, which satisfies Assumption~\ref{assumption:conditions}, in learning the -layer {\gresnet} model, we have the following inequality hold:

where  and  for some .
\end{appendix_theo}

\begin{proof}
We can represent the Jacobian matrix  of  with . Therefore, we have

Matrix  can be rewritten as , where


Meanwhile, it is easy to know that

\end{proof}

Based on the above lemma, we have

where .

Furthermore, we know that

where  denotes the Lipschitz seminorm of the input function and it is defined as


Meanwhile, according to the Theorem 1 in \cite{BEL18} (whose proof will not be introduced here), we know that 

which concludes the proof. In the above equation,  denotes the layer depth of the model.




























\end{document}
