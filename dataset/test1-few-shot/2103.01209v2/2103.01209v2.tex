\documentclass{article}
\usepackage{pagecolor,lipsum}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} \usepackage{amsmath} \usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{colortbl}
\usepackage{subfig}
\usepackage[numbers,sort&compress]{natbib}

\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}



\usepackage[accepted]{icml2021}



\definecolor{Blue}{RGB}{3, 31, 97}
\definecolor{Blue1}{RGB}{214, 235, 245}
\definecolor{Blue2}{RGB}{235, 245, 250}
\definecolor{Gray}{RGB}{247, 252, 255}

\begin{document}

\icmltitlerunning{Generative Adversarial Transformers}

\twocolumn[
\title{Generative Adversarial Transformers}

\author{Drew A. Hudson \\
Department of Computer Science\\
Stanford University\\
\texttt{dorarad@cs.stanford.edu}\\
\And
C. Lawrence Zitnick \\
Facebook AI Research\\
Facebook, Inc.\\
\texttt{zitnick@fb.com}\\
}

\icmlkeywords{Machine Learning, ICML, generative models, GANs, transformers, image synthesis, attention, compositionality}

\maketitle

]





\printAffiliationsAndNotice{} 

\begin{abstract}
We introduce the GANsformer, a novel and efficient type of transformer, and explore it for the task of visual generative modeling. The network employs a bipartite structure that enables long-range interactions across the image, while maintaining computation of linearly efficiency, that can readily scale to high-resolution synthesis. It iteratively propagates information from a set of latent variables to the evolving visual features and vice versa, to support the refinement of each in light of the other and encourage the emergence of compositional representations of objects and scenes. In contrast to the classic transformer architecture, it utilizes multiplicative integration that allows flexible region-based modulation, and can thus be seen as a generalization of the successful StyleGAN network. We demonstrate the model's strength and robustness through a careful evaluation over a range of datasets, from simulated multi-object environments to rich real-world indoor and outdoor scenes, showing it achieves state-of-the-art results in terms of image quality and diversity, while enjoying fast learning and better data-efficiency. Further qualitative and quantitative experiments offer us an insight into the model's inner workings, revealing improved interpretability and stronger disentanglement, and illustrating the benefits and efficacy of our approach.  An implementation of the model is available at \url{https://github.com/dorarad/gansformer}.
\end{abstract}
 \begin{figure}[t]
\centering
\subfloat{\includegraphics[width=0.33\linewidth]{images/69a.png}}
\hfill
\subfloat{\includegraphics[width=0.33\linewidth]{images/69b.png}}
\hfill
\subfloat{\includegraphics[width=0.33\linewidth]{images/69c.png}}
\vspace*{-9pt}
\subfloat{\includegraphics[width=0.33\linewidth]{images/final/nl/duplex/fakes00655208.png}}
\hfill
\subfloat{\includegraphics[width=0.33\linewidth]{images/final/nl/duplex/maps_006552_l0_h008_2.png}}
\hfill
\subfloat{\includegraphics[width=0.33\linewidth]{images/final/nl/duplex/maps_006552_l0_h008_3.png}}
\vspace*{-9pt}
\subfloat{\includegraphics[width=0.33\linewidth]{images/98a.png}}
\hfill
\subfloat{\includegraphics[width=0.33\linewidth]{images/98b.png}}
\hfill
\subfloat{\includegraphics[width=0.33\linewidth]{images/98c.png}}
\vspace*{-4pt}
\caption{Sample images generated by the GANsformer, along with a visualization of the model attention maps.}
\vspace*{-14pt}
\label{teaser}
\end{figure} 


\vspace*{-18pt}
\section{Introduction}
\label{intro}
The cognitive science literature speaks of two reciprocal mechanisms that underlie human perception: the \textbf{\textit{bottom-up}} processing, proceeding from the retina up to the cortex, as local elements and salient stimuli hierarchically group together to form the whole \citep{gibson1,gibson2}, and the \textbf{\textit{top-down}} processing, where surrounding context, selective attention and prior knowledge inform the interpretation of the particular \citep{gregory1,gregory2}. While their respective roles and dynamics are being actively studied, researchers agree that it is the interplay between these two complementary processes that enables the formation of our rich internal representations, allowing us to perceive the world around in its fullest and create vivid imageries in our mind's eye \citep{vision1,vision2,vision3,vision4}.


Nevertheless, the very mainstay and foundation of computer vision over the last decade -- the Convolutional Neural Network, surprisingly does not reflect this bidirectional nature that so characterizes the human visual system, and rather displays a one-way feed-forward progression from raw sensory signals to higher representations. Their local receptive field and rigid computation reduce their ability to model long-range dependencies or develop holistic understanding of global shapes and structures that goes beyond the brittle reliance on texture \citep{texture}, and in the generative domain especially, they are linked to considerable optimization and stability issues \citep{resnet,sagan} due to their fundamental difficulty in coordinating between fine details across a generated scene. These concerns, along with the inevitable comparison to cognitive visual processes, beg the question of whether convolution alone provides a complete solution, or some key ingredients are still missing. 

\begin{figure*}[ht]
\centering
\subfloat{\includegraphics[width=0.82\linewidth]{images/overview.png}}
\caption{We introduce the GANsformer network, that leverages a bipartite structure to allow long-range interactions, while evading the quadratic complexity standard transformers suffer from. We present two novel attention operations over the bipartite graph: \textit{simplex} and \textit{duplex}, the former permits communication in one direction, in the generative context -- from the latents to the image features, while the latter enables both top-down and bottom up connections between these two variable groups.}
\label{plots}
\vspace*{-8pt}
\end{figure*}

Meanwhile, the NLP community has witnessed a major revolution with the advent of the Transformer network \citep{transformer}, a highly-adaptive architecture centered around relational attention and dynamic interaction. In response, several attempts have been made to integrate the transformer into computer vision models, but so far they have met only limited success due to scalabillity limitations stemming from their quadratic mode of operation.

Motivated to address these shortcomings and unlock the full potential of this promising network for the field of computer vision, we introduce the Generative Adversarial Transformer, or GANsformer for short, a simple yet effective generalization of the vanilla transformer, explored here for the task of visual synthesis. The model features a biparatite construction for computing soft attention, that iteratively aggregates and disseminates information between the generated image features and a compact set of latent variables to enable bidirectional interaction between these dual representations. This proposed design achieves a favorable balance, being capable of flexibly modeling global phenomena and long-range interactions on the one hand, while featuring an efficient setup that still scales linearly with the input size on the other. As such, the GANsformer can sidestep the computational costs and applicability constraints incurred by prior work, due to their dense and potentially excessive pairwise connectivity of the standard transformer \citep{sagan,biggan}, and successfully advance generative modeling of images and scenes. 

We study the model quantitative and qualitative behavior through a series of experiments, where it achieves state-of-the-art performance over a wide selection of datasets, of both simulated as well as real-world kinds, obtaining particularly impressive gains in generating highly-compositional multi-object scenes. The analysis we conduct  indicates that the GANsformer requires less training steps and fewer samples than competing approaches to successfully synthesize images of high quality and diversity. Further evaluation provides robust evidence for the network's enhanced transparency and compositionality, while ablation studies empirically validate the value and effectiveness of our approach. We then present visualizations of the model's produced attention maps to shed more light upon its internal representations and visual generation process. All in all, as we will see through the rest of the paper, by bringing the renowned GANs and Transformer architectures together under one roof, we can integrate their complementary strengths, to create a strong, compositional and efficient network for visual generative modeling. \section{Related Work}
\label{related}

Generative Adversarial Networks (GANs), originally introduced in 2014 \citep{gan}, have made remarkable progress over the past few years, with significant advances in training stability and dramatic improvements in image quality and diversity, turning them to be nowadays a leading paradigm in visual synthesis \citep{dcgan,biggan,stylegan}. In turn, GANs have been widely adopted for a rich variety of tasks, including image-to-image translation\citep{pix2pix,cyclegan}, super-resolution \citep{superres}, style transfer \citep{stargan}, and representation learning \citep{bigan}, to name a few. But while automatically produced images for faces, single objects or natural scenery have reached astonishing fidelity, becoming nearly indistinguishable from real samples, the unconditional synthesis of more structured or compositional scenes is still lagging behind, suffering from inferior coherence, reduced geometric consistency and, at times, lack of global coordination \citep{johnson,sbgan,iclrscenes,sagan}. As of now, faithful generation of structured scenes is thus yet to be reached. 

Concurrently, the last couple of years saw impressive progress in the field of NLP, driven by the innovative architecture called Transformer \citep{transformer}, which has attained substantial gains within the language domain and consequently sparked considerable interest across the deep learning community \citep{transformer,bert}. In response, several attempts have been made to incorporate self-attention constructions into vision models, most commonly for image recognition, but also in segmentation \citep{attsgm}, detection \citep{detr}, and synthesis \citep{sagan}. From structural perspective, they can be roughly divided into two streams: those that apply local attention operations, failing to capture global interactions \citep{localtrns1,localtrns2,localtrns3,localtrns4,imgtrns}, and others that borrow the original transformer structure as-is and perform attention globally, across the entire image, resulting in prohibitive computation due to its quadratic complexity, which fundamentally hinders its applicability to low-resolution layers only \citep{sagan,biggan,glbltrns1,glbltrns2,taming,vistrns,transgan}. Few other works proposed sparse, discrete or approximated variations of self-attention, either within the adversarial or autoregressive contexts, but they still fall short of reducing memory footprint and computation costs to a sufficient degree \citep{gsa,axial,criscros,taming,sparsetrns}. 

Compared to these prior works, the GANsformer stands out as it manages to avoid the high costs ensued by \textbf{\textit{self attention}}, employing instead \textbf{\textit{bipartite attention}} between the image features and a small collection of latent variables. Its design fits naturally with the generative goal of transforming source latents into an image, facilitating long-range interaction without sacrificing computational efficiency. Rather, the network maintains a scalable linear efficiency across all layers, realizing the transformer full potential. In doing so, we seek to take a step forward in tackling the challenging task of compositional scene generation. Intuitively, and as is later corroborated by our findings, holding multiple latent variables that interact through attention with the evolving generated image, may serve as a structural prior that promotes the formation of compact and compositional scene representations, as the different latents may specialize for certain objects or regions of interest. Indeed, as demonstrated in section \ref{exps}, the Generative Adversarial Transformer achieves state-of-the-art performance in synthesizing both controlled and real-world indoor and outdoor scenes, while showing indications for semantic compositional disposition along the way. 


In designing our model, we drew inspiration from multiple lines of research on generative modeling, compositionality and scene understanding, including techniques for scene decomposition, object discovery and representation learning. Several approaches, such as \citep{monet,iodine,air,genesis}, perform iterative variational inference to encode scenes into multiple slots, but are mostly applied in the contexts of synthetic and oftentimes fairly rudimentary 2D settings. Works such as Capsule networks  \citep{capsules1} leverage ideas from psychology about Gestalt principles \citep{gestalt1,gestalt2}, perceptual grouping \citep{gestalt3} or analysis-by-synthesis \cite{rbc1}, and like us, introduce ways to piece together visual elements to discover compound entities or, in the cases of Set Transformers \citep{settrns} and -Nets \citep{doubleatt}, group local information into global aggregators, which proves useful for a broad specturm of tasks, spanning unsupervised segmentation \citep{nem,slotatt}, clustering \citep{settrns}, image recognition \citep{doubleatt}, NLP \citep{etc} and viewpoint generalization \citep{capsules3}. However, our work stands out incorporating new ways to integrate information between nodes, as well as novel forms of attention (\textit{Simplex} and \textit{Duplex}) that iteratively update and refine the assignments between image features and latents, and is the first to explore these techniques in the context of high-resolution generative modeling.

Most related to our work are certain GAN models for conditional and unconditional visual synthesis: A few methods \citep{kgan,mganprior,blockgan,relate} utilize multiple replicas of a generator to produce a set of image layers, that are then combined through alpha-composition. As a result, these models make quite strong assumptions about the independence between the components depicted in each layer. In contrast, our model generates one unified image through a cooperative process, coordinating between the different latents through the use of soft attention. Other works, such as SPADE \citep{spade,sean}, employ region-based feature modulation for the task of layout-to-image translation, but, contrary to us, use fixed segmentation maps or static class embeddings to control the visual features. Of particular relevance is the prominent StyleGAN model \citep{stylegan,stylegan2}, which utilizes a single global style vector to consistently modulate the features of each layer. The GANsformer generalizes this design, as multiple style vectors impact different regions in the image concurrently, allowing for a spatially finer control over the generation process. Finally, while StyleGAN broadcasts information in one direction from the \textit{global} latent to the \textit{local} image features, our model propagates information both from latents to features and vise versa, enabling top-down and bottom-up reasoning to occur simultaneously\footnote{Note however that our model certainly does not claim to serve as a biologically-accurate reflection of cognitive top-down processing. Rather, this analogy played as a conceptual source of inspiration that aided us through the idea development.}. \section{The Generative Adversarial Transformer}
\label{model}

\begin{figure*}[t]
\centering
\subfloat{\includegraphics[width=0.14\linewidth]{images/69a.png}}
\hfill
\subfloat{\includegraphics[width=0.14\linewidth]{images/69b.png}}
\hfill
\subfloat{\includegraphics[width=0.14\linewidth]{images/69c.png}}
\hfill
\subfloat{\includegraphics[width=0.193\linewidth]{images/79a.png}}
\hfill
\subfloat{\includegraphics[width=0.193\linewidth]{images/79b.png}}
\hfill
\subfloat{\includegraphics[width=0.193\linewidth]{images/79c.png}}
\vspace*{-10pt}
\centering
\subfloat{\includegraphics[width=0.166\linewidth]{images/146a.png}}
\hfill
\subfloat{\includegraphics[width=0.166\linewidth]{images/146b.png}}
\hfill
\subfloat{\includegraphics[width=0.166\linewidth]{images/146c.png}}
\hfill
\subfloat{\includegraphics[width=0.166\linewidth]{images/97a.png}}
\hfill
\subfloat{\includegraphics[width=0.166\linewidth]{images/97b.png}}
\hfill
\subfloat{\includegraphics[width=0.166\linewidth]{images/97c.png}}
\vspace*{-4pt}
\caption{Samples of images generated by the GANsformer for the CLEVR, Bedroom and Cityscapes datasets, and a visualization of the produced attention maps. The different colors correspond to the latents that attend to each region.}
\label{imgs}
\vspace*{-8pt}
\end{figure*} 

The Generative Adversarial Transformer is a type of Generative Adversarial Network, which involves a \textit{generator} network (G) that maps a sample from the \textit{latent} space to the output space (e.g. an image), and a \textit{discriminator} network (D) which seeks to discern between real and fake samples \citep{gan}. The two networks compete with each other through a minimax game until reaching an equilibrium. Typically, each of these networks consists of multiple layers of convolution, but in the GANsformer case, we instead construct them using a novel architecture, called 
\textbf{\textit{Bipartite Transformer}}, formally defined below. 

The section is structured as follows: we first present a formulation of the Bipartite Transformer, a general-purpose generalization of the Transformer\footnote{By \textit{transformer}, we precisely mean a multi-layer bidirectional transformer encoder, as described in \citep{bert}, which interleaves self-attention and feed-forward layers.} (section \ref{definition}). Then, we provide an overview of how the transformer is incorporated into the generator and discriminator framework (section \ref{gan}). We conclude by discussing the merits and distinctive properties of the GANsformer, that set it apart from the traditional GAN and transformer networks (section \ref{discussion}).


\subsection{The Bipartite Transformer}
\label{definition}

\begin{figure*}[ht]
\centering
\subfloat{\includegraphics[width=0.142\linewidth]{images/c1.png}}
\hfill
\subfloat{\includegraphics[width=0.142\linewidth]{images/c2.png}}
\hfill
\subfloat{\includegraphics[width=0.142\linewidth]{images/c3.png}}
\hfill
\subfloat{\includegraphics[width=0.142\linewidth]{images/c4.png}}
\hfill
\subfloat{\includegraphics[width=0.142\linewidth]{images/c5.png}}
\hfill
\subfloat{\includegraphics[width=0.142\linewidth]{images/c6.png}}
\hfill
\subfloat{\includegraphics[width=0.142\linewidth]{images/c7.png}}
\vspace*{-5pt}
\caption{\footnotesize A visualization of the GANsformer attention maps for bedrooms.}
\label{attmaps}
\vspace*{-12pt}
\end{figure*}

The standard \textit{transformer} network consists of alternating multi-head self-attention and feed-forward layers. We refer to each pair of self-attention and feed-forward layers as a transformer layer, such that a Transformer is considered to be a stack composed of several such layers. The Self-Attention operator considers all pairwise relations among the input elements, so to update each single element by attending to all the others. The Bipartite Transformer generalizes this formulation, featuring instead a bipartite graph between two groups of variables (in the GAN case, latents and image features). In the following, we consider two forms of attention that could be computed over the bipartite graph -- Simplex attention, and Duplex attention, depending on the direction in which information propagates\footnote{In computer networks, \textit{simplex} refers to communication in a single direction, while \textit{duplex} refers to communication in both ways.} -- either in one way only, or both in top-down and bottom-up ways. While for clarity purposes we present the technique here in its one-head version, in practice we make use of a multi-head variant, in accordance with \citep{transformer}. 

\subsubsection{Simplex Attention}
We begin by introducing the \textbf{\textit{simplex attention}}, which distributes information in a single direction over the Bipartite Transformer. Formally, let  denote an input set of  vectors of dimension  (where, for the image case, ), and  denote a set of  aggregator variables (the latents, in the case of the generator). We can then compute attention over the derived bipartite graph between these two groups of elements. Specifically, we then define:

Where  stands for , and  are functions that respectively map elements into queries, keys, and values, all maintaining the same dimensionality . We also provide the query and key mappings with respective positional encoding inputs, to reflect the distinct position of each element in the set (e.g. in the image) (further details on the specifics of the positional encoding scheme in section \ref{gan}).

We can then combine the attended information with the input elements , but whereas the standard transformer implements an additive update rule of the form:  (where  in the standard self-attention case) we instead use the retrieved information to control both the \textit{scale} as well as the \textit{bias} of the elements in , in line with the practices promoted by the StyleGAN model \citep{stylegan}. As our  experiments indicate, such multiplicative integration enables significant gains in the model performance. Formally:

Where  are mappings that compute multiplicative and additive styles (gain and bias), maintaining a dimension of , and  normalizes each element, with respect to the other features\footnote{The statistics are computed with respect to the other elements in the case of instance normalization, or among element channels in the case of layer normalization. We have experimented with both forms and found that for our model layer normalization performs a little better, matching reports by \citep{gannorm}.}. This update rule fuses together the normalization and information propagation from  to , by essentially letting  control  statistical tendencies  of , which for instance can be useful in the case of visual synthesis for generating particular objects or entities. 

\subsubsection{Duplex Attention}
We can go further and consider the variables  to poses a key-value structure of their own \citep{kvmn}: , where the values store the \textit{content} of the  variables (e.g. the randomly sampled latents for the case of GANs) while the keys track the centroids of the attention-based assignments from  to , which can be computed as: . Consequently, we can define a new update rule, that is later empirically shown to work more effectively than the simplex attention:

This update compounds two attention operations on top of each other, where we first compute soft attention assignments between  and , and then refine the assignments by considering their centroids, analogously to the k-means algorithm \citep{kmeans,slotatt}.

Finally, to support bidirectional interaction between the elements, we can chain two reciprocal simplex attentions from  to  and from  to , obtaining the \textbf{\textit{duplex attention}}, which  alternates computing  and , such that each representation is refined in light of its interaction with the other, integrating together bottom-up and top-down interactions.

\subsubsection{Overall Architecture Structure}
\paragraph{Vision-Specific adaptations.} In the standard Transformer used for NLP, each self-attention layer is followed by a Feed-Forward FC layer that processes each element independently (which can be deemed a  convolution). Since our case pertains to images, we use instead a kernel size of  after each application of attention. We also apply a Leaky ReLU nonlinearity after each convolution \citep{lrelu} and then upsample or downsmaple the features , as part of the generator or discriminator respectively, following e.g. StyleGAN2 \citep{stylegan2}. To account for the features location within the image, we use a sinusoidal positional encoding along the horizontal and vertical dimensions for the visual features  \citep{transformer}, and a trained embedding for the set of latent variables .

Overall, the Bipartite Transformer is thus composed of a stack that alternates attention (simplex or duplex) and convolution layers, starting from a  grid up to the desirable resolution. Conceptually, this structure fosters an interesting communication flow: rather than densely modeling interactions among all the pairs of pixels in the images, it supports adaptive long-range interaction between far away pixels in a moderated manner, passing through through a compact and global bottleneck that selectively gathers information from the entire input and distribute it to relevant regions. Intuitively, this form can be viewed as analogous to the top-down notions discussed in section \ref{intro}, as information is propagated in the two directions, both from the local pixel to the global high-level representation and vise versa.

We note that both the simplex and the duplex attention operations enjoy a bilinear efficiency of  thanks to the network's bipartite structure that considers all pairs of corresponding elements from  and . Since, as we see below, we maintain  to be of a fairly small size, choosing  in the range of 8--32, this compares favorably to the potentially prohibitive  complexity of the self-attention, that impedes its applicability to high-resolution images. 

\subsection{The Generator and Discriminator networks}
\label{gan}
We use the celebrated StyleGAN model as a starting point for our GAN design. Commonly, a generator network consists of a multi-layer CNN that receives a randomly sampled vector  and transforms it into an image. The StyleGAN approach departs from this design and, instead, introduces a feed-forward \textit{mapping network} that outputs an intermediate vector , which in turn interacts directly with each convolution through the \textit{synthesis network}, globally controlling the feature maps statistics of every layer. 

Effectively, this approach attains \textit{layer-wise decomposition} of visual properties, allowing the model to control particular global aspects of the image such as pose, lighting conditions or color schemes, in a coherent manner over the entire image. But while StyleGAN successfully disentangles global properties, it is potentially limited in its ability to perform \textit{spatial decomposition}, as it provides no means to control the style of a localized regions within the generated image. 

Luckily, the Bipartite Transformer offers a solution to meet this goal. Instead of controlling the style of all features globally, we  use instead our new attention layer to perform adaptive and local region-wise modulation. We split the latent vector  into  components,  and, as in StyleGAN, pass each of them through a shared mapping network, obtaining a corresponding set of intermediate latent variables . Then, during synthesis, after each CNN layer in the generator, we let the feature map 

\clearpage

\begin{figure*}[ht]
\centering
\subfloat{\includegraphics[width=0.2\linewidth]{images/final/nc/att/000379.png}}
\hfill
\subfloat{\includegraphics[width=0.2\linewidth]{images/final/nc/att/000363.png}}
\hfill
\subfloat{\includegraphics[width=0.2\linewidth]{images/final/nc/att/000028.png}}
\hfill
\subfloat{\includegraphics[width=0.2\linewidth]{images/final/nc/att/000024.png}}
\hfill
\subfloat{\includegraphics[width=0.2\linewidth]{images/final/nc/att/000013.png}}
\vspace*{-8pt}
\subfloat{\includegraphics[width=0.2\linewidth]{images/final/nc/att/379a.png}}
\hfill
\subfloat{\includegraphics[width=0.2\linewidth]{images/final/nc/att/363a.png}}
\hfill
\subfloat{\includegraphics[width=0.2\linewidth]{images/final/nc/att/28a.png}}
\hfill
\subfloat{\includegraphics[width=0.2\linewidth]{images/final/nc/att/24a.png}}
\hfill
\subfloat{\includegraphics[width=0.2\linewidth]{images/final/nc/att/13a.png}}
\vspace*{-8pt}
\subfloat{\includegraphics[width=0.2\linewidth]{images/final/nc/att/b_000379.png}}
\hfill
\subfloat{\includegraphics[width=0.2\linewidth]{images/final/nc/att/b_000363.png}}
\hfill
\subfloat{\includegraphics[width=0.2\linewidth]{images/final/nc/att/b_000028.png}}
\hfill
\subfloat{\includegraphics[width=0.2\linewidth]{images/final/nc/att/b_000024.png}}
\hfill
\subfloat{\includegraphics[width=0.2\linewidth]{images/final/nc/att/b_000013.png}}
\vspace*{5pt}

\subfloat{\includegraphics[width=0.2\linewidth]{images/final/nb/duplex_real/s_fakes01564827.png}}
\hfill
\subfloat{\includegraphics[width=0.2\linewidth]{images/final/nb/duplex_real/s_fakes01618025.png}}
\hfill
\subfloat{\includegraphics[width=0.2\linewidth]{images/final/nb/duplex_real/s_fakes01564823.png}}
\hfill
\subfloat{\includegraphics[width=0.2\linewidth]{images/final/nb/duplex_real/s_fakes01564817.png}}
\hfill
\subfloat{\includegraphics[width=0.2\linewidth]{images/final/nb/duplex_real/s_fakes01521213.png}}
\vspace*{-8pt}
\subfloat{\includegraphics[width=0.2\linewidth]{images/final/nb/duplex_real/maps_017583_l0_h027.png}}
\hfill
\subfloat{\includegraphics[width=0.2\linewidth]{images/final/nb/duplex_real/maps_017583_l0_h025.png}}
\hfill
\subfloat{\includegraphics[width=0.2\linewidth]{images/final/nb/duplex_real/maps_017583_l0_h023.png}}
\hfill
\subfloat{\includegraphics[width=0.2\linewidth]{images/final/nb/duplex_real/maps_017583_l0_h017.png}}
\hfill
\subfloat{\includegraphics[width=0.2\linewidth]{images/final/nb/duplex_real/maps_017583_l0_h013.png}}
\vspace*{5pt}

\subfloat{\includegraphics[width=0.2\linewidth]{images/final/nl/duplex/fakes00655218.png}}
\hfill
\subfloat{\includegraphics[width=0.2\linewidth]{images/final/nl/duplex/fakes00655216.png}}
\hfill
\subfloat{\includegraphics[width=0.2\linewidth]{images/final/nl/duplex/fakes00655208.png}}
\hfill
\subfloat{\includegraphics[width=0.2\linewidth]{images/final/nl/duplex/fakes00655203.png}}
\hfill
\subfloat{\includegraphics[width=0.2\linewidth]{images/final/nl/duplex/fakes00655201.png}}
\vspace*{-8pt}
\subfloat{\includegraphics[width=0.2\linewidth]{images/final/nl/duplex/maps_006552_l0_h018.png}}
\hfill
\subfloat{\includegraphics[width=0.2\linewidth]{images/final/nl/duplex/maps_006552_l0_h016.png}}
\hfill
\subfloat{\includegraphics[width=0.2\linewidth]{images/final/nl/duplex/maps_006552_l0_h008.png}}
\hfill
\subfloat{\includegraphics[width=0.2\linewidth]{images/final/nl/duplex/maps_006552_l0_h003.png}}
\hfill
\subfloat{\includegraphics[width=0.2\linewidth]{images/final/nl/duplex/maps_006552_l0_h001.png}}
\vspace*{-4pt}
\caption{\textbf{Sampled Images and Attention maps.} Samples of images generated by the GANsformer for the CLEVR, LSUN-Bedroom and Cityscapes datasets, and a visualization of the produced attention maps. The different colors correspond to the latent variables that attend to each region. For the CLEVR dataset we should multiple attention maps in different layers of the model, revealing how the latent variables roles change over the different layers -- while they correspond to different objects as the layout of the scene is being formed in early layers, they behave similarly to a surface normal in the final layers of the generator.}
\label{layer_attmaps}
\vspace*{-8pt}
\end{figure*} 

\clearpage

and latents  to play the roles of the two element groups, mediate their interaction through our new attention layer (either simplex or duplex). This setting thus allows for a flexible and dynamic style modulation at the region level. Since soft attention tends to group elements based on their proximity and content similarity \citep{transformer}, we see how the transformer architecture naturally fits into the generative task and proves useful in the visual domain, allowing the network to exercise finer control in modulating semantic regions. As we see in section \ref{exps}, this capability turns to be especially useful in modeling compositional scenes. 
 
 \begin{table*}[ht]
\caption{Comparison between the GANsformer and competing methods for image synthesis. We evaluate the models along commonly used metrics such as FID, Inception, and Precision \& Recall scores. FID is considered to be the most well-received as a reliable indication of images fidelity and diversity. We compute each metric 10 times over 50k samples with different random seeds and report their average.}
\vspace*{5.5pt}
\label{table1}
\centering
\scriptsize
\begin{tabular}{lcccccccc}
\rowcolor{Blue1}
\scriptsize & {\color{Blue} \textbf{CLEVR}} &  &  &  & {\color{Blue} \textbf{LSUN-Bedroom}} &  &  &  \\ 
\rowcolor{Blue1}
\textbf{Model} & FID  & IS  & Precision  & Recall  & FID  & IS  & Precision  & Recall  \\
\scriptsize GAN & 25.0244 & 2.1719 & 21.77 & 16.76 & 12.1567 & 2.6613 & 52.17 & 13.63 \\
\scriptsize k-GAN & 28.2900 & 2.2097 & 22.93 & 18.43 & 69.9014 & 2.4114 & 28.71 & 3.45 \\
\scriptsize SAGAN & 26.0433 & 2.1742 & 30.09 & 15.16 & 14.0595 & 2.6946 & 54.82 & 7.26 \\
\scriptsize StyleGAN2 & 16.0534 & 2.1472 & 28.41 & 23.22 & 11.5255 & \textbf{2.7933} & 51.69 & 19.42 \\
\scriptsize VQGAN & 32.6031 & 2.0324 & 46.55 & 63.33 & 59.6333 & 1.9319 & 55.24 & 28.00 \\
\rowcolor{Blue2}
\scriptsize \textbf{GANsformer} & 10.2585 & \textbf{2.4555} & 38.47 & 37.76 & 8.5551 & 2.6896 & 55.52 & 22.89 \\
\rowcolor{Blue2}
\scriptsize \textbf{GANsformer} & \textbf{9.1679} & 2.3654 & \textbf{47.55} & \textbf{66.63} & \textbf{6.5085} & 2.6694 & \textbf{57.41} & \textbf{29.71} \\
\rowcolor{Gray}
\tiny  &  &  &  &  &  &  &  &  \\
\rowcolor{Blue1}
\scriptsize & {\color{Blue} \textbf{FFHQ}} &  &  &  & {\color{Blue} \textbf{Cityscapes}} &  &  &  \\ 
\rowcolor{Blue1}
\textbf{Model} & FID  & IS  & Precision  & Recall  & FID  & IS  & Precision  & Recall  \\
\scriptsize GAN & 13.1844 & 4.2966 & 67.15 & 17.64 & 11.5652 & 1.6295 & 61.09 & 15.30  \\
\scriptsize k-GAN & 61.1426 & 3.9990 & 50.51 & 0.49 & 51.0804 & 1.6599 & 18.80 & 1.73 \\
\scriptsize SAGAN & 16.2069 & 4.2570 & 64.84 & 12.26 & 12.8077 & 1.6837 & 43.48 & 7.97 \\
\scriptsize StyleGAN2 & \textbf{10.8309} & 4.3294 & 68.61 & 25.45 & 8.3500 & 1.6960 & 59.35 & 27.82 \\
\scriptsize VQGAN & 63.1165 & 2.2306 & 67.01 & \textbf{29.67} & 173.7971 & \textbf{2.8155} & 30.74 & \textbf{43.00} \\
\rowcolor{Blue2}
\scriptsize \textbf{GANsformer} & 13.2861 & \textbf{4.4591} & \textbf{68.94} & 10.14 & 14.2315 & 1.6720 & \textbf{64.12} & 2.03 \\
\rowcolor{Blue2}
\scriptsize \textbf{GANsformer} & 12.8478 & 4.4079 & 68.77 & 5.7589 & \textbf{5.7589} & 1.6927 & 48.06 & 33.65 \\
\rowcolor{Blue2}
\end{tabular}
\vspace*{-11pt}
\end{table*}

For the discriminator, we similarly apply attention after every convolution, in this case using trained embeddings to initialize the aggregator variables , which may intuitively represent background knowledge the model learns about the scenes. At the last layer, we concatenate these variables to the final feature map to make a prediction about the identity of the image source. We note that this construction holds some resemblance to the PatchGAN discriminator introduced by \citep{pix2pix}, but whereas PatchGAN pools features according to a fixed predetermined scheme, the GANsformer can gather the information in a more adaptive and selective manner. Overall, using this structure endows the discriminator with the capacity to likewise model long-range dependencies, which can aid the discriminator in its assessment of the image fidelity, allowing it to acquire a more holistic understanding of the visual modality.

In terms of the loss function, optimization and training configuration, we adopt the settings and techniques used in the StyleGAN and StyleGAN2 models \citep{stylegan,stylegan2}, including in particular style mixing, stochastic variation, exponential moving average for weights, and a non-saturating logistic loss with lazy R1 regularization. 

\subsection{Summary} \label{discussion}

To recapitulate the discussion above, the GANsformer successfully unifies the GANs and Transformer for the task of scene generation. Compared to the traditional GANs and transformers, it introduces multiple key innovations:
\begin{itemize}
  \item Featuring a bipartite structure the reaching a sweet spot that balances between expressiveness and efficiency, being able to model long-range dependencies while maintaining linear computational costs.
  \item Introducing a compositional structure with multiple latent variables that coordinate through attention to produce the image cooperatively, in a manner that matches the inherent compositionality of natural scenes.
  \item Supporting bidirectional interaction between the latents and the visual features which allows the refinement and interpretation of each in light of the other.
  \item Employing a multiplicative update rule to affect feature styles, akin to StyleGAN but in contrast to the transformer architecture.
\end{itemize}
As we see in the following section, the combination of these design choices yields a strong architecture that demonstrates high efficiency, improved latent space disentanglement, and enhanced transparency of its generation process.

%
 \section{Experiments}
\label{exps}

\begin{figure}[t]
\centering
\subfloat{\includegraphics[width=0.5\linewidth]{images/att_first.pdf}}
\hfill
\subfloat{\includegraphics[width=0.5\linewidth]{images/att_last.pdf}}
\vspace*{-2mm}
\caption{Performance of the GANsformer and competing approaches for the CLEVR and Bedroom datasets. }
\label{aplots}
\vspace*{-12pt}
\end{figure}

We investigate the GANsformer through a suite of experiments to study its quantitative performance and qualitative behavior. As detailed in the sections below, the GANsformer achieves state-of-the-art results, successfully producing high-quality images for a varied assortment of datasets: FFHQ for human faces \citep{stylegan}, the CLEVR dataset for multi-object scenes \citep{clevr}, and the LSUN-Bedroom \citep{lsun} and Cityscapes \citep{cityscapes} datasets for challenging indoor and outdoor scenes. The use of these datasets and their reproduced images are only for the purpose of scientific communication. Further analysis we then conduct in section \ref{data}, \ref{comp} and \ref{dis} provide evidence for several favorable properties that the GANsformer posses, including better data-efficiency, enhanced transparency, and stronger disentanglement compared to prior approaches. Section \ref{dvrs} then quantitatively assesses the network semantic coverage of the natural image distribution for the CLEVR dataset, while ablation studies \ref{ablt} empirically validate the relative importance of each of the model's design choices. Taken altogether, our evaluation offers solid evidence for the GANsformer effectiveness and efficacy in modeling compsitional images and scenes.

We compare our network with multiple related approaches including both baselines as well as leading models for image synthesis: (1) A baseline GAN \citep{gan}: a standard GAN that follows the typical convolutional architecture.\footnote{We specifically use a default configuration from StyleGAN2 codebase, but with the noise being inputted through the network stem instead of through weight demodulation.} (2) StyleGAN2 \citep{stylegan2}, where a single global latent interacts with the evolving image by modulating its style in each layer. (3) SAGAN \citep{sagan}, a model that performs self-attention across all pixel pairs in the low-resolution layers of the generator and discriminator. (4) k-GAN \citep{kgan} that produces  separated images, which are then blended through alpha-composition. (5) VQGAN \citep{taming} that has been proposed recently and utilizes transformers for discrete autoregessive auto-encoding.

To evaluate all models under comparable conditions of training scheme, model size, and optimization details, we implement them all within the codebase introduced by the StyleGAN authors. All models have been trained with images of  resolution and for the same number of training steps, roughly spanning a week on 2 NVIDIA V100 GPUs per model (or equivalently 3-4 days using 4 GPUs). For the GANsformer, we select  -- the number of latent variables, from the range of 8--32. Note that increasing the value of  does not translate to increased overall latent dimension, and we rather kept the overall latent equal across models. See supplementary material \ref{impl} for further implementation details, hyperparameter settings and training configuration. 

\begin{figure*}[t]
\centering
\subfloat{\includegraphics[width=0.33\linewidth]{images/clevr.pdf}}
\hfill
\subfloat{\includegraphics[width=0.33\linewidth]{images/bedroom.pdf}}
\hfill
\subfloat{\includegraphics[width=0.33\linewidth]{images/data.pdf}}
\vspace*{-2mm}
\caption{From left to right: \textbf{(1-2)} Performance as a function of start and final layers the attention is applied to. \textbf{(3)}: data-efficiency experiments for CLEVR.}
\label{plots}
\vspace*{-12pt}
\end{figure*}

We can see that the GANsformer matches or outperforms the performance of prior approaches, with the least benefits for the non-compositional FFHQ dataset for human faces, and largest gains for the highly-compositional CLEVR dataset. 

As shown in table \ref{table1}, our model matches or outperforms prior work, achieving substantial gains in terms of FID score which correlates with image quality and diversity \citep{fid}, as well as other commonly used metrics such as Inception score (IS) and Precision and Recall (P{\&}R).\footnote{Note that while the StyleGAN paper \citep{stylegan2} reports lower FID scores in the FFHQ and Bedroom cases, they obtain them by training their model for 5-7 times longer than our experiments (StyleGAN models are trained for up to 17.5 million steps, producing 70M samples and demanding over 90 GPU-days). To comply with a reasonable compute budget, in our evaluation we equally reduced the training duration for all models, maintaining the same number of steps.} As could be expected, we obtain the least gains for the FFHQ human faces dataset, where naturally there is relatively lower diversity in images layout. On the flip side, most notable are the significant improvements in the performance for the CLEVR case, where our approach successfully lowers FID scores from 16.05 to 9.16, as well as the Bedroom dataset where the GANsformer nearly halves the FID score from 11.32 to 6.5, being trained for equal number of steps. These findings suggest that the GANsformer is particularly adept in modeling scenes of high compositionality (CLEVR) or layout diversity (Bedroom). Comparing between the Simplex and Duplex Attention versions further reveals the strong benefits of integrating the reciprocal bottom-up and top-down processes together.

\subsection{Data and Learning Efficiency}
\label{data} 
We examine the learning curves of our and competing models (figure \ref{plots}, middle) and inspect samples of generated image at different stages of the training (figure \ref{training} supplementary). These results both reveal that our model learns significantly faster than competing approaches, in the case of CLEVR producing high-quality images in approximately 3-times less training steps than the second-best approach. To explore the GANsformer learning aptitude further, we have performed experiments where we reduced the size of the dataset that each model (and specifically, its discriminator) is exposed to during the training (figure \ref{plots}, rightmost) to varied degrees. These results similarly validate the model superior data-efficiency, especially when as little as 1k images are given to the model.  





\subsection{Transparency \& Compositionality} \label{comp} 
To gain more insight into the model's internal representation and its underlying generative process, we visualize the attention distributions produced by the GANsformer as it synthesizes new images. Recall that at each layer of the generator, it casts attention between the  latent variables and the evolving spatial features of the generated image. 

From the samples in figures \ref{imgs} and \ref{attmaps}, we can see that particular latent variables tend to attend to coherent regions within the image in terms of content similarity and proximity. Figure \ref{layer_attmaps} shows further visualizations of the attention computed by the model in various layers, showing how it behaves distinctively in different stages of the synthesis process. These visualizations imply that the latents carry a semantic sense, capturing objects, visual entities or constituent components of the synthesized scene. These findings can thereby attest for an enhanced compositionality that our model acquires through its multi-latent structure. Whereas models such as StyleGAN use a single monolithic latent vector to account for the whole scene and modulate features only at the global scale, our design lets the GANsformer exercise finer control in impacting features at the object granularity, while leveraging the use of attention to make its internal representations more explicit and transparent.  

To quantify the compositionality level exhibited by the model, we use a pre-trained segmentor \citep{detectron2} to produce semantic segmentations for a sample set of generated scenes, and use them to measure the correlation between the attention cast by latent variables and various semantic classes. In figure \ref{corr} in the supplementary, we present the classes that on average have shown the highest correlation with respect to latent variables in the model, indicating that the model coherently attend to semantic concepts such as windows, pillows, sidewalks and cars, as well as coherent background regions like carpets, ceiling, and walls.
  


\begin{table}[t]
\caption{\textbf{Chi-Square statistics} of the output image distribution for the CLEVR dataset, based on 1k samples that have been processed by a pre-trained object detector to identify the objects and semantic properties within the sample generated images.}
\vspace*{8pt}
\label{dist}
\centering
\scriptsize
\begin{tabular}{lcccc}
\rowcolor{Blue1}
& \textbf{GAN} & \textbf{StyleGAN} & \textbf{GANsformer} & \textbf{GANsformer} \\
\scriptsize \textbf{Object Area} & 0.038 & 0.035 & 0.045 & \textbf{0.068} \\
\rowcolor{Blue2}
\scriptsize \textbf{Object Number} & 2.378 & 1.622 & 2.142 & \textbf{2.825} \\
\scriptsize \textbf{Co-occurrence} & \textbf{13.532} & 9.177 & 9.506 & 13.020 \\
\rowcolor{Blue2}
\scriptsize \textbf{Shape} & 1.334 & 0.643 & 1.856 & \textbf{2.815} \\
\scriptsize \textbf{Size} & 0.256 & 0.066 & 0.393 & \textbf{0.427} \\
\rowcolor{Blue2}
\scriptsize \textbf{Material} & 0.108 & 0.322 & 1.573 & \textbf{2.887} \\
\scriptsize \textbf{Color} & 1.011 & 1.402 & 1.519 & \textbf{3.189} \\
\rowcolor{Blue2}
\scriptsize \textbf{Class} & 6.435 & 4.571 & 5.315 & \textbf{16.742} \\
\end{tabular}
\vspace*{-14pt}
\end{table}

\begin{table}[t]
\caption{\textbf{Disentanglement metrics (DCI)}, which asses the Disentanglement, Completeness and Informativeness of latent representations, computed over 1k CLEVR images. The GANsformer achieves the strongest results compared to competing approaches.}
\vspace*{8pt}
\label{disen}
\centering
\scriptsize
\begin{tabular}{lcccc}
\rowcolor{Blue1}
\scriptsize & \textbf{GAN} & \textbf{StyleGAN} & \textbf{GANsformer} & \textbf{GANsformer} \\
\rowcolor{Blue2}
\scriptsize \textbf{Disentanglement} & 0.126 & 0.208 & 0.556 & \textbf{0.768} \\
\scriptsize \textbf{Modularity} & 0.631 & 0.703 & 0.891 & \textbf{0.952} \\
\rowcolor{Blue2}
\scriptsize \textbf{Completeness} & 0.071 & 0.124 & 0.195 & \textbf{0.270} \\
\scriptsize \textbf{Informativeness} & 0.583 & 0.685 & 0.899 & \textbf{0.971625} \\
\rowcolor{Blue2}
\scriptsize \textbf{Informativeness'} & 0.4345 & 0.332 & 0.848 & \textbf{0.963} \\
\end{tabular}
\vspace*{-13pt}
\end{table}

\subsection{Disentanglement}
\label{dis}
We consider the DCI metrics commonly used in the disentanglement literature \citep{dci}, to provide more evidence for the beneficial impact our architecture has on the model's internal representations. These metrics asses the Disentanglement, Completeness and Informativeness of a given representation, essentially evaluating the degree to which there is 1-to-1 correspondence between latent factors and global image attributes. To obtain the attributes, we consider the area size of each semantic class (bed, carpet, pillows) obtained through a pre-trained segmentor and use them as the output response features for measuring the latent space disentanglement, computed over 1k images. We follow the protocol proposed by \citep{stylespace} and present the results in table \ref{disen}. This analysis confirms that the GANSformer latent representations enjoy higher disentanglement when compared to the baseline StyleGAN approach. 


\subsection{Image Diversity}
\label{dvrs} 
One of the advantages of compositional representations is that they can support combinatorial generalization -- one of the key foundations of human intelligence \cite{relational}. Inspired by this observation, we perform an experiment to measure that property in the context of visual synthesis of multi-object scenes. We use a pre-trained detector on the generated CLEVR scenes to extract the objects and properties within each sample. Considering a large number of samples, we then compute Chi-Square statistics to determine the degree to which each model manages to covers the natural uniform distribution of CLEVR images. Table \ref{dist} summarizes the results, where we can see that our model obtains better scores across almost all the semantic properties of the image distribution. These metrics complement the common FID and IS scores as they emphasize structure over texture, focusing on object existence, arrangement and local properties, and thereby substantiating further the model compositionality.

\subsection{Ablation and Variation Studies}
\label{ablt} 
To validate the usefulness of our approach and obtain a better assessment of the relative contribution of each design choice, we conduct multiple ablation  studies, where we test our model under varying conditions, specifically studying the impact of: latent dimension, attention heads, number of layers attention is incorporated into, simplex vs. duplex, generator vs. discriminator attention, and multiplicative vs. additive integration. While most results appear in the supplementary, we wish to focus on two variations in particular, where we incorporate attention in different layers across the generator. As we can see in figure \ref{aplots}, the earlier in the stack the attention is applied (low-resolutions), the better the model's performance and the faster it learns. The same goes for the final layer to apply attention to -- as attention can especially contribute in high-resolutions that can benefit the most from long-range interactions. These studies provide a validation for the effectiveness of our approach in enhancing generative scene modeling.
 \section{Conclusion}
\label{conclusion}
We have introduced the GANsformer, a novel and efficient bipartite transformer that combines top-down and bottom-up interactions, and explored it for the task of generative modeling, achieving strong quantitative and qualitative results that attest for the model robustness and efficacy. The GANsformer fits within the general philosophy that aims to incorporate stronger inductive biases into neural networks to encourage desirable properties such as transparency, data-efficiency and compositionality -- properties which are at the core of human intelligence, and serve as the basis for our capacity to reason, plan, learn, and imagine. While our work focuses on visual synthesis, we note that the Bipartite Transformer is a general-purpose model, and expect it may be found useful for other tasks in both vision and language. Overall, we hope that our work will help taking us a little closer in our collective search to bridge the gap between the intelligence of humans and machines.


 

\bibliography{example_paper}
\bibliographystyle{neurips_2019}

\cleardoublepage
\newpage

\appendix

\section*{Supplementary Material}

In the following we provide additional quantitative experiments and visualizations for the GANsformer model. Section \ref{exps} discusses multiple experiments we have conducted to measure the model's latent space disentanglement (Section \ref{disen}), image diversity (Section \ref{dvrs}), and spatial compositionality (Section \ref{comp}). For the last of which we describe the experiment in the main text and provide here the numerical results: See figure \ref{corr} for results of the spatial compositionality experiment that sheds light upon the roles of the different latent variables. To complement the numerical evaluations with qualitative results, we present in figures \ref{training} and \ref{sota} a comparison of sample images produced by the GANsformer and a set of baseline models, over the course the training and after convergence respectively, while section \ref{impl} specifies the implementation details, optimization scheme and training configuration of the model. Finally, section \ref{abltsec} details additional ablation studies to measure the relative contribution of different model's design choices. 



\section{Implementation and Training details}
\label{impl}
To evaluate all models under comparable conditions of training scheme, model size, and optimization details, we implement them all within the TensorFlow codebase introduced by the StyleGAN authors \citep{stylegan}. See tables \ref{hyperparams} for particular settings of the GANsformer and table \ref{paramsnum} for comparison of models' sizes. In terms of the loss function, optimization and training configuration, we adopt the settings and techniques used in the StyleGAN and StyleGAN2 models \citep{stylegan,stylegan2}, including in particular style mixing, Xavier Initialization, stochastic variation, exponential moving average for weights, and a non-saturating logistic loss with lazy R1 regularization. We use Adam optimizer with batch size of 32 (4 times 8 using gradient accumulation), equalized learning rate of ,  and  as well as leaky ReLU activations with , bilinear filtering in all up/downsampling layers and minibatch standard deviation layer at the end of
the discriminator. The mapping layer of the generator consists of 8 layers, and resnet connections are used throughout the model, for the mapping network synthesis network and discriminator. We train all models on images of  resolution, padded as necessary. The CLEVR dataset consists of 100k images, the FFHQ has 70k images, Cityscapes has overall about 25k images and the LSUN-Bedroom has 3M images. The images in the Cityscapes and FFHQ datasets are mirror-augmented to increase the effective training set size. All models have been trained for the same number of training steps, roughly spanning a week on 2 NVIDIA V100 GPUs per model. 

\section{Spatial Compositionality}

\begin{figure}[ht]

\centering
\subfloat{\includegraphics[width=0.45\linewidth]{images/msegs07.jpg}}
\hfill
\subfloat{\includegraphics[width=0.55\linewidth]{images/bed_corr.pdf}}

\subfloat{\includegraphics[width=0.45\linewidth]{images/msegs12.jpg}}
\hfill
\subfloat{\includegraphics[width=0.55\linewidth]{images/city_corr.pdf}}
\vspace*{-2mm}
\caption{\textbf{Attention spatial compositionality experiments.} Correlation between attention heads and semantic segments, computed over 1k sample images. Results presented for the Bedroom and Cityscapes datasets.}
\label{corr}
\end{figure}

To quantify the compositionality level exhibited by the model, we employ a pre-trained segmentor to produce semantic segmentations for the synthesized scenes, and use them to measure the correlation between the attention cast by the latent variables and the various semantic classes. We derive the correlation by computing the maximum intersection-over-union between a class segment and the attention segments produced by the model in the different layers. The mean of these scores is then taken over a set of 1k images. Results presented in figure \ref{corr} for the Bedroom and Cityscapes datasets, showing semantic classes which have high correlation with the model attention, indicating it decomposes the image into semantically-meaningful segments of objects and entities.

\begin{table}[ht]
\caption{\textbf{Hyperparameter choices} for the GANsformer and baseline models. The number of latent variables (each variable can be multidimensional) is chosen based on performance among . The overall latent dimension (a sum over the dimensions of all the latents variables) is chosen among  and is then used both for the GANsformer and the baseline models. The R1 regularization factor  is chosen among .}
\vspace*{8pt}
\label{hyperparams}
\centering
\scriptsize
\begin{tabular}{lcccc}
\rowcolor{Blue1}
& \textbf{FFHQ} & \textbf{CLEVR} & \textbf{Cityscapes} & \textbf{Bedroom} \\
\scriptsize \# \textbf{Latent vars} & 8 & 16 & 16 & 16 \\
\rowcolor{Blue2}
\scriptsize \textbf{Latent var dim} & 16 & 32 & 32 & 32 \\
\scriptsize \textbf{Latent overall dim} & 128 & 512 & 512 & 512 \\
\rowcolor{Blue2}
\scriptsize \textbf{R1 reg weight} () & 10 & 40 & 20 & 100 \\
\end{tabular}
\end{table}

\begin{table}[ht]
\caption{\textbf{Model size} for the GANsformer and competing approaches, computed given 16 latent variables and an overall latent dimension of 512. All models have comparable size.}
\vspace*{8pt}
\label{paramsnum}
\centering
\scriptsize
\begin{tabular}{lcc}
\rowcolor{Blue1}
 & \# \textbf{G Params} & \# \textbf{D Params} \\
\scriptsize \textbf{GAN} & 34M & 29M \\
\rowcolor{Blue2}
\scriptsize \textbf{StyleGAN2} & 35M  & 29M \\
\scriptsize \textbf{k-GAN} & 34M & 29M \\
\rowcolor{Blue2}
\scriptsize \textbf{SAGAN} & 38M & 29M \\
\scriptsize \textbf{GANsformer} & 36M & 29M \\
\rowcolor{Blue2}
\scriptsize \textbf{GANsformer} & 36M & 29M \\
\end{tabular}
\end{table}

\section{Ablation and Variation Studies}
\label{abltsec}
To validate the usefulness of our approach and obtain a better assessment of the relative contribution of each design choice, we conduct multiple ablation and variation studies (table will be updated soon), where we test our model under varying conditions, specifically studying the impact of: latent dimension, attention heads, number of layers attention is incorporated into, simplex vs. duplex, generator vs. discriminator attention, and multiplicative vs. additive integration, all performed over the diagnostic CLEVR dataset \citep{clevr}. We see that having the bottom-up relations introduced by the duplex attention help the model significantly, and likewise conducive are the multiple distinct latent variables (up to 8 for CLEVR) and the use of multiplicative integration. Note that additional variation results appear in the main text, showing the GANsformer's performance throughout the training as a function of the number of attention layers used, either how early or up to what layer they are introduced, demonstrating that our biprartite structure and both duplex attention lead to substantial contributions in the model generative performance.









\begin{figure*}[t]
\centering
\setlength{\tabcolsep}{0pt} \renewcommand{\arraystretch}{0} \begin{tabular}{c c c c c c}
\rowcolor{white}

\textbf{{GAN  }} & \includegraphics[width=0.165\linewidth]{images/final/nc/gan/000069.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nc/gan/000043.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nc/gan/000038.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nc/gan/000012.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nc/gan/000009.jpg} \\

 & \includegraphics[width=0.165\linewidth]{images/final/nb/gan/000031.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nb/gan/000004.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nb/gan/000003.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nb/gan/000001.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nb/gan/000000.jpg} \\

\vspace*{12pt}

 & \includegraphics[width=0.165\linewidth]{images/final/nl/gan/fakes00268327.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nl/gan/fakes00268326.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nl/gan/fakes00268324.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nl/gan/fakes00268323.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nl/gan/fakes00268322.jpg} \\

\textbf{{StyleGAN2  }} & \includegraphics[width=0.165\linewidth]{images/final/nc/stylegan/000017.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nc/stylegan/000014.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nc/stylegan/000009.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nc/stylegan/000004.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nc/stylegan/000002.jpg} \\

 & \includegraphics[width=0.165\linewidth]{images/final/nb/stylegan/000008.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nb/stylegan/000006.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nb/stylegan/000005.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nb/stylegan/000004.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nb/stylegan/000002.jpg} \\

\vspace*{12pt}

 & \includegraphics[width=0.165\linewidth]{images/final/nl/stylegan/fakes00220127.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nl/stylegan/fakes00220126.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nl/stylegan/fakes00220123.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nl/stylegan/fakes00220120.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nl/stylegan/fakes00220116.jpg} \\

\textbf{{k-GAN  }} & \includegraphics[width=0.165\linewidth]{images/final/nc/kgan/s_26.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nc/kgan/s_24.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nc/kgan/s_21.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nc/kgan/s_20.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nc/kgan/s_08.jpg} \\

 & \includegraphics[width=0.165\linewidth]{images/final/nb/kgan/000005.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nb/kgan/000004.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nb/kgan/000002.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nb/kgan/000001.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nb/kgan/000000.jpg} \\

 & \includegraphics[width=0.165\linewidth]{images/final/nl/kgan/000001.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nl/kgan/000003.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nl/kgan/000005.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nl/kgan/000009.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nl/kgan/000010.jpg} \\

\end{tabular}
\caption{\textbf{State-of-the-art Comparison.} A comparison of models' sampled images for the CLEVR, LSUN-Bedroom and Cityscapes datasets. All models have been trained for the same number of steps, which ranges between 5k to 15k samples. Note that the original StyleGAN2 model has been trained by its authors for up to generate 70k samples, which is expected to take over 90 GPU-days for a single model. See next page for image samples by further models. These images show that given the same training length the GANsformer model's sampled images enjoy high quality and diversity compared to the prior works, demonstrating the efficacy of our approach.}
\label{sota}
\end{figure*}

\begin{figure*}[t]
\centering
\setlength{\tabcolsep}{0pt} \renewcommand{\arraystretch}{0} \begin{tabular}{c c c c c c}
\rowcolor{white}

\textbf{{SAGAN  }} & \includegraphics[width=0.165\linewidth]{images/final/nc/sagan/000022.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nc/sagan/000012.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nc/sagan/000005.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nc/sagan/000002.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nc/sagan/000000.jpg} \\

& \includegraphics[width=0.165\linewidth]{images/final/nb/sagan/000018.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nb/sagan/000016.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nb/sagan/000006.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nb/sagan/000005.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nb/sagan/000002.jpg} \\

\vspace*{12pt}

 & \includegraphics[width=0.165\linewidth]{images/final/nl/sagan/000099.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nl/sagan/000092.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nl/sagan/000091.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nl/sagan/000078.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nl/sagan/000066.jpg} \\

\textbf{{VQGAN  }} & \includegraphics[width=0.165\linewidth]{images/final/nc/taming/s_samples_nopix_gs-139999_e-000013_b-00450002.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nc/taming/s_samples_nopix_gs-139999_e-000013_b-00450001.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nc/taming/s_samples_nopix_gs-139999_e-000013_b-00450000.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nc/taming/s_samples_nopix_gs-139999_e-000013_b-00375000.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nc/taming/s_samples_nopix_gs-139999_e-000013_b-00300001.jpg} \\

 & \includegraphics[width=0.165\linewidth]{images/final/nb/taming/s_samples_nopix_gs-149999_e-000009_b-00675003.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nb/taming/s_samples_nopix_gs-149999_e-000009_b-00675002.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nb/taming/s_samples_nopix_gs-149999_e-000009_b-00675001.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nb/taming/s_samples_nopix_gs-149999_e-000009_b-00675000.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nb/taming/s_samples_nopix_gs-149999_e-000009_b-00600001.jpg} \\

\vspace*{12pt}

 & \includegraphics[width=0.165\linewidth]{images/final/nl/taming/s_samples_nopix_gs-077499_e-000030_b-00000003.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nl/taming/s_samples_nopix_gs-077499_e-000030_b-00000002.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nl/taming/s_samples_nopix_gs-077499_e-000030_b-00000001.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nl/taming/s_samples_nopix_gs-077499_e-000030_b-00000000.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nl/taming/s_samples_nopix_gs-074999_e-000029_b-00075003.jpg} \\

\textbf{{GANsformer  }} & \includegraphics[width=0.165\linewidth]{images/final/nc/simplex/000390.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nc/simplex/000061.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nc/simplex/000031.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nc/simplex/000027.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nc/simplex/000011.jpg} \\

 & \includegraphics[width=0.165\linewidth]{images/final/nb/simplex/000025.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nb/simplex/000024.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nb/simplex/000022.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nb/simplex/000019.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nb/simplex/000008.jpg} \\

 & \includegraphics[width=0.165\linewidth]{images/final/nl/simplex/fakes00234614.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nl/simplex/fakes00234612.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nl/simplex/fakes00234604.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nl/simplex/fakes00234602.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nl/simplex/fakes00234601.jpg} \\

\end{tabular}
\caption{A comparison of models' sampled images for the CLEVR, LSUN-Bedroom and Cityscapes datasets. See figure \ref{sota} for further description.}
\end{figure*}

\begin{figure*}[t]
\centering
\setlength{\tabcolsep}{0pt} \renewcommand{\arraystretch}{0} \begin{tabular}{c c c c c c}
\rowcolor{white}

\textbf{{GANsformer  }} & \includegraphics[width=0.165\linewidth]{images/final/nc/duplex/000030.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nc/duplex/000052.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nc/duplex/000055.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nc/duplex/000059.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nc/duplex/000067.jpg} \\

  & \includegraphics[width=0.165\linewidth]{images/final/nc/duplex/000069.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nc/duplex/000196.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nc/duplex/000930.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nc/duplex/000937.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nc/duplex/000942.jpg} \\

 & \includegraphics[width=0.165\linewidth]{images/final/nc/duplex/000971.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nc/duplex/000973.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nc/duplex/000974.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nc/duplex/000981.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nc/duplex/000990.jpg} \\
\vspace*{8pt}

 & \includegraphics[width=0.165\linewidth]{images/final/nc/duplex/000991.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nc/duplex/000992.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nc/duplex/000993.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nc/duplex/000994.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nc/duplex/000999.jpg} \\

 & \includegraphics[width=0.165\linewidth]{images/final/nb/duplex/000041.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nb/duplex/000029.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nb/duplex/000027.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nb/duplex/000010.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nb/duplex/000007.jpg} \\

 & \includegraphics[width=0.165\linewidth]{images/final/nb/duplex/000246.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nb/duplex/000235.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nb/duplex/000234.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nb/duplex/000220.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nb/duplex/000202.jpg} \\

 & \includegraphics[width=0.165\linewidth]{images/final/nb/duplex/000281.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nb/simplex/s_fakes01057021.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nb/duplex/000255.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nb/duplex/000251.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nb/duplex/000248.jpg} \\
\vspace*{8pt}

 & \includegraphics[width=0.165\linewidth]{images/final/nb/duplex/000298.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nb/duplex/000294.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nb/duplex/000292.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nb/duplex/000284.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nb/duplex/000282.jpg} \\

 & \includegraphics[width=0.165\linewidth]{images/final/nl/duplex/fakes00655219.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nl/duplex/fakes00655214.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nl/duplex/fakes00655211.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nl/duplex/fakes00655209.jpg} & \includegraphics[width=0.165\linewidth]{images/final/nl/duplex/fakes00655205.jpg} \\

\end{tabular}
\caption{A comparison of models' sampled images for the CLEVR, LSUN-Bedroom and Cityscapes datasets. See figure \ref{sota} for further description.}
\end{figure*}

\begin{figure*}[t]
\centering
\setlength{\tabcolsep}{0pt} \renewcommand{\arraystretch}{0} \begin{tabular}{c c c c c c}
\rowcolor{white}

\textbf{{GAN  }} & \includegraphics[width=0.165\linewidth]{images/training/nc/gan/s_fakes00012110.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nc/gan/s_fakes00021710.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nc/gan/s_fakes00050810.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nc/gan/s_fakes00101610.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nc/gan/s_fakes00200710.jpg} \\

 & \includegraphics[width=0.165\linewidth]{images/training/nb/gan/s_fakes00050802.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nb/gan/s_fakes00101602.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nb/gan/s_fakes00200802.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nb/gan/s_fakes00270902.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nb/gan/s_fakes00367702.jpg} \\

\vspace*{12pt}

 & \includegraphics[width=0.165\linewidth]{images/training/nl/gan/s_fakes00012101.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nl/gan/s_fakes00021701.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nl/gan/s_fakes00050801.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nl/gan/s_fakes00104001.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nl/gan/s_fakes00208001.jpg} \\

\textbf{{StyleGAN  }} & \includegraphics[width=0.165\linewidth]{images/training/nc/stylegan/s_fakes00012101.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nc/stylegan/s_fakes00021701.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nc/stylegan/s_fakes00050801.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nc/stylegan/s_fakes00101601.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nc/stylegan/s_fakes00207901.jpg} \\

 & \includegraphics[width=0.165\linewidth]{images/training/nb/stylegan/s_fakes00045907.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nb/stylegan/s_fakes00091907.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nb/stylegan/s_fakes00183807.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nb/stylegan/s_fakes00261207.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nb/stylegan/s_fakes00345907.jpg} \\

\vspace*{12pt}

 & \includegraphics[width=0.165\linewidth]{images/training/nl/stylegan/s_fakes00012100.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nl/stylegan/s_fakes00021700.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nl/stylegan/s_fakes00053200.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nl/stylegan/s_fakes00101600.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nl/stylegan/s_fakes00203100.jpg} \\

\textbf{{k-GAN  }} & \includegraphics[width=0.165\linewidth]{images/training/nc/kgan/s_fakes00012009.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nc/kgan/s_fakes00021709.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nc/kgan/s_fakes00050709.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nc/kgan/s_fakes00104009.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nc/kgan/s_fakes00205509.jpg} \\

 & \includegraphics[width=0.165\linewidth]{images/training/nb/kgan/s_fakes00050800.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nb/kgan/s_fakes00108800.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nb/kgan/s_fakes00203200.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nb/kgan/s_fakes00283000.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nb/kgan/s_fakes00365300.jpg} \\

 & \includegraphics[width=0.165\linewidth]{images/training/nl/kgan/s_fakes00012100.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nl/kgan/s_fakes00021700.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nl/kgan/s_fakes00050800.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nl/kgan/s_fakes00101600.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nl/kgan/s_fakes00208000.jpg} \\

\end{tabular}
\caption{\textbf{State-of-the-art Comparison over training.} A comparison of models' sampled images for the CLEVR, LSUN-Bedroom and Cityscapes datasets, generated at different stages throughout the training. Sampled image from different points in training of based on the same sampled latents, thereby showing how the image evolves during the training. For CLEVR and Cityscapes, we present results after training to generate 100k, 200k, 500k, 1m, and 2m samples. For the Bedroom case, we present results after 500k, 1m, 2m, 5m and 10m generated samples while training. These results show how the GANsformer, and especially when using duplex attention, manages learn a lot faster than the competing approaches, generating impressive images very early in the training.}
\label{training}
\end{figure*}

\begin{figure*}[t]
\centering
\setlength{\tabcolsep}{0pt} \renewcommand{\arraystretch}{0} \begin{tabular}{c c c c c c}
\rowcolor{white}

\textbf{{SAGAN  }} & \includegraphics[width=0.165\linewidth]{images/training/nc/sagan/s_fakes00012125.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nc/sagan/s_fakes00021725.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nc/sagan/s_fakes00050825.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nc/sagan/s_fakes00104025.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nc/sagan/s_fakes00200825.jpg} \\

& \includegraphics[width=0.165\linewidth]{images/training/nb/sagan/s_fakes00043510.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nb/sagan/s_fakes00087110.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nb/sagan/s_fakes00164510.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nb/sagan/s_fakes00222510.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nb/sagan/s_fakes00309610.jpg} \\

\vspace*{12pt}

 & \includegraphics[width=0.165\linewidth]{images/training/nl/sagan/s_fakes00012101.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nl/sagan/s_fakes00021701.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nl/sagan/s_fakes00050801.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nl/sagan/s_fakes00106401.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nl/sagan/s_fakes00200801.jpg} \\

\textbf{{VQGAN  }} & \includegraphics[width=0.165\linewidth]{images/training/nc/taming/s_samples_nopix_gs-019999_e-000001_b-00300001.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nc/taming/s_samples_nopix_gs-029999_e-000002_b-00225000.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nc/taming/s_samples_nopix_gs-049999_e-000004_b-00000003.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nc/taming/s_samples_nopix_gs-099999_e-000009_b-00150001.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nc/taming/s_samples_nopix_gs-139999_e-000013_b-00150003.jpg} \\

 & \includegraphics[width=0.165\linewidth]{images/training/nb/taming/s_samples_nopix_gs-029999_e-000001_b-00000001.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nb/taming/s_samples_nopix_gs-044999_e-000002_b-00675003.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nb/taming/s_samples_nopix_gs-104999_e-000006_b-00300001.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nb/taming/s_samples_nopix_gs-134999_e-000008_b-00450003.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nb/taming/s_samples_nopix_gs-149999_e-000009_b-00600003.jpg} \\

\vspace*{12pt}

 & \includegraphics[width=0.165\linewidth]{images/training/nl/taming/s_samples_nopix_gs-002499_e-000000_b-00000001.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nl/taming/s_samples_nopix_gs-004999_e-000001_b-00075001.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nl/taming/s_samples_nopix_gs-012499_e-000004_b-00075001.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nl/taming/s_samples_nopix_gs-024999_e-000009_b-00075001.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nl/taming/s_samples_nopix_gs-054999_e-000021_b-00000001.jpg} \\

\textbf{{GANsformer  }} & \includegraphics[width=0.165\linewidth]{images/training/nc/simplex/s_fakes00012101.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nc/simplex/s_fakes00021701.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nc/simplex/s_fakes00053201.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nc/simplex/s_fakes00113601.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nc/simplex/s_fakes00227301.jpg} \\

 & \includegraphics[width=0.165\linewidth]{images/training/nc/simplex/s_fakes00012124.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nc/simplex/s_fakes00021724.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nc/simplex/s_fakes00053224.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nc/simplex/s_fakes00113624.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nc/simplex/s_fakes00227324.jpg} \\

 & \includegraphics[width=0.165\linewidth]{images/training/nb/simplex/s_fakes00050805.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nb/simplex/s_fakes00104005.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nb/simplex/s_fakes00205605.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nb/simplex/s_fakes00520005.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nb/simplex/s_fakes01057005.jpg} \\

 & \includegraphics[width=0.165\linewidth]{images/training/nl/simplex/s_fakes00012121.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nl/simplex/s_fakes00024221.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nl/simplex/s_fakes00053221.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nl/simplex/s_fakes00104021.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nl/simplex/s_fakes00205621.jpg} \\

 & \includegraphics[width=0.165\linewidth]{images/training/nl/simplex/s_fakes00012109.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nl/simplex/s_fakes00024209.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nl/simplex/s_fakes00053209.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nl/simplex/s_fakes00104009.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nl/simplex/s_fakes00205609.jpg} \\

\end{tabular}
\caption{A comparison of models' sampled images for the CLEVR, LSUN-Bedroom and Cityscapes datasets throughout the training. See figure \ref{training} for further description.}
\end{figure*}

\begin{figure*}[t]
\centering
\setlength{\tabcolsep}{0pt} \renewcommand{\arraystretch}{0} \begin{tabular}{c c c c c c}
\rowcolor{white}

\textbf{{GANsformer  }} & \includegraphics[width=0.165\linewidth]{images/training/nc/duplex/s_fakes00012114.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nc/duplex/s_fakes00024214.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nc/duplex/s_fakes00053214.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nc/duplex/s_fakes00113714.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nc/duplex/s_fakes00224914.jpg} \\
\vspace*{8pt}

  & \includegraphics[width=0.165\linewidth]{images/training/nc/duplex/s_fakes00012121.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nc/duplex/s_fakes00024221.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nc/duplex/s_fakes00053221.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nc/duplex/s_fakes00113721.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nc/duplex/s_fakes00224921.jpg} \\

 & \includegraphics[width=0.165\linewidth]{images/training/nb/duplex/s_fakes00058025.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nb/duplex/s_fakes00118425.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nb/duplex/s_fakes00229725.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nb/duplex/s_fakes00522425.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nb/duplex/s_fakes01100525.jpg} \\
 \vspace*{8pt}
 
 & \includegraphics[width=0.165\linewidth]{images/training/nb/duplex/s_fakes00058017.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nb/duplex/s_fakes00118417.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nb/duplex/s_fakes00229717.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nb/duplex/s_fakes00522417.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nb/duplex/s_fakes01100517.jpg} \\

 & \includegraphics[width=0.165\linewidth]{images/training/nl/duplex/s_fakes00014527.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nl/duplex/s_fakes00024227.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nl/duplex/s_fakes00053227.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nl/duplex/s_fakes00108827.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nl/duplex/s_fakes00222527.jpg} \\

 & \includegraphics[width=0.165\linewidth]{images/training/nl/duplex/s_fakes00014500.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nl/duplex/s_fakes00024200.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nl/duplex/s_fakes00053200.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nl/duplex/s_fakes00108800.jpg} & \includegraphics[width=0.165\linewidth]{images/training/nl/duplex/s_fakes00222500.jpg} \\

\end{tabular}
\caption{A comparison of models' sampled images for the CLEVR, LSUN-Bedroom and Cityscapes datasets throughout the training. See figure \ref{training} for further description.}
\end{figure*}











%
 
\end{document}
