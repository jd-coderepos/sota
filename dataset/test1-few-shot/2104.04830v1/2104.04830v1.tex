\listfiles
\documentclass[3p]{elsarticle}

\usepackage{lineno,hyperref}
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage[table]{xcolor}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{arydshln}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{sfmath}
\usetikzlibrary{patterns}
\usepackage{soul}
\usepackage{float}
\usepackage{subfloat}
\usepackage{subfig}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{rotating}
\usepackage{pdflscape}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{tcolorbox}

\modulolinenumbers[5]

\journal{ArXiv}



\bibliographystyle{model1-num-names}















\begin{document}

\begin{frontmatter}

\title{
FRAKE: Fusional Real-time Automatic Keyword Extraction\\
}


\author[cominsys]{Aidin Zehtab-Salmasi\corref{cor}}\ead{a.zehtab97@ms.tabrizu.ac.ir}
\author[cominsys]{Mohammad-Reza Feizi-Derakhshi}\ead{mfeizi@tabrizu.ac.ir}

\author[UT]{Mohamad-Ali Balafar}\ead{balafarila@tabrizu.ac.ir}

\cortext[cor]{a.zehtab97@ms.tabrizu.ac.ir}

\address[cominsys]{Computerized Intelligence Systems Laboratory, Department of Computer Engineering, University of Tabriz, Tabriz, IRAN.}
\address[UT]{Department of Computer Engineering, University of Tabriz, Tabriz, IRAN.}

\begin{abstract}
Keyword extraction is called identifying words or phrases that express the main concepts of texts in best. There is a huge amount of texts that are created every day and at all times through electronic infrastructure. So, it is practically impossible for humans to study and manage this volume of documents. However, the need for efficient and effective access to these documents is evident in various purposes. Weblogs, News, and technical notes are almost long texts, while the reader seeks to understand the concepts by topics or keywords to decide for reading the full text. To this aim, we use a combined approach that consists of two models of graph centrality features and textural features. In the following, graph centralities, such as degree, betweenness, eigenvector, and closeness centrality, have been used to optimally combine them to extract the best keyword among the candidate keywords extracted by the proposed method. Also, another approach has been introduced to distinguishing keywords among candidate phrases and considering them as a separate keyword. To evaluate the proposed method, seven datasets named, Semeval2010, SemEval2017, Inspec, fao30, Thesis100, pak2018 and WikiNews have been used, and results reported Precision, Recall, and F- measure.

\end{abstract}

\begin{keyword}
Keyword extraction, Key-phrase extraction, Natural language processing
\end{keyword}

\end{frontmatter}



\section{Introduction} \label{sec:Introduction}
Nowadays, people need to find the favourite documents among the vast volume of daily creates. Typically, millions of web pages and text files are stored daily. Without indexing, it is impossible to analyse them, so if the documents index with expressions makes analysing them highly convenient. In fact, an expression makes understanding a document more convenient. An expression could be one or a couple of words or also phrases called keyword(s) or key-phrases. Keyword or key-phrase extraction is a solution that identifies a set of terms that conclude the main idea of a document \cite{berry_kogan_2010}.

Key word or phrase extraction is a hot topic in natural language processing, and researchers take almost three different approaches, namely, text approach, graph approach, and hybrid of these two. Text-based approaches viewed in the aspect of textual features could be extracted from documents using such as Part 0f Speech (POS), mean TF, etc. In the approach of graph, document model to graph by co-occurrence or N-grams thus that the words represented by nodes and relations represented by edges. After graph creation, statistical analysis of graphs deploys to scoring nodes intending to select the top words as key-words(phrases). As the last approach, hybrid models try to use both of graph and text methods to extract key-words(phrases).



The rest of this study is organized in the following manner: the second section contains a literature review of mobile price prediction. Section \ref{sec:proposed_method} presents the proposed methods. The experimental results are given and discussed in section \ref{sec:experiments_results}. Some conclusions are drawn in the final section, and the areas for further researches are identified, as well.
\section{Related Works} \label{sec:related_works}
Studies related to keywords can fall into two major approaches: Extraction \& Generation. The approach of this paper is keyword extraction and mean that keyword should appear in the document to be selected as keywords. As the second stage of the division of keywords extraction methods, related words can be ordered into four classes: statistical, textual, graph-based, and hybrid. Statistical methods deployed an arithmetical approach to scoring candidate words and selecting top-scored words as keywords of the document. TF-IDF \cite{Lott2012} and co-occurrence \cite{matsuo2004keyword} can be listed as the most well-known statistical keywords extractor methods.

Textual-based methods pay attention only to linguistic features such as lexical, syntactic, and semantic of words in a document. Part of speech tags of words of a document was one of the text-based keyword extraction methods that ignored stop words \cite{10.3115/1119355.1119383}. Morphology is a subset of f linguistic analysis that was used in keyword extraction. Morphology-based approaches \cite{li2015keyphrase} need a thesaurus, and WordNet \cite{miller1998wordnet} is the most famous one. However, language dependency is the main limitation of this approach. Moreover, some of the linguistic features in textual-based keyword extraction methods could be listed as "Noun phrase chunking" \cite{hulth2003improved}, Ontology \cite{shamsfard2008} or "Lexical chains" \cite{enss2006investigation}.

Graph-based methods idea is to construct a graph from documents elements. The co-occurrence network is the most used one that shows the interactions of words in a document. In the co-occurrence graph, nodes represent words, and there is an edge between two nodes if the relevant words co-occur within a window. Moreover, mostly some centrality measures such as degree, closeness, betweenness and eigenvector are deployed to scoring nodes, and the most central nodes are identified as candidate keywords. Graph-based methods have gained good results in this area, and there are vast works; a number of methods can be listed as Text Rank \cite{Mihalcea2004}, Single Rank \cite{Wan2008}, Topic page Rank \cite{Sterckx2015}, Position Rank \cite{Florescu2017}, Multipartite Rank \cite{Boudin2018}, Expended Rank \cite{Wan2008}.

The hybrid models attempt to combine two previously mentioned categories, textual-based and graph-based. The overall structure of hybrid models is to calculate each text and graph based features separately and combine them in a technique. The way of combining and scoring is the main contribution of works. Mike \cite{10.1145/3132847.3132956}, Sgrank \cite{sgrank-2015}, Key2Vec \cite{mahata-key2vec}, RaKUn \cite{rakun2019} are examples of hybrid methods.
\section{FRAKE} \label{sec:proposed_method}
The proposed method, called FREAK, is a fusion of two parallel keyword extraction approaches, graph features and textural features, and each of the techniques has its advantages. Illustration of the proposed method is shown in fig. \ref{fig:diagram}. The proposed method consists of 5 steps; pre-processing, feature extraction, scores computation, key-phrase generation, and ranking, respectively, with the aim of extracting keywords. In the following sub-sections, every step is expanded. To make each step sensible, the output of those steps are shown with an example. The example document is: \newline
\begin{tcolorbox}
\small{"fuzzy systems overlapping gaussian concepts approximation properties sobolev norms in paper approximating capabilities fuzzy systems overlapping gaussian concepts considered the target function assumed sampled either regular gird according uniform probability density by exploiting connection radial basis functions approximators new method computation system coefficients provided showing guarantees uniform approximation derivatives target function."}
\end{tcolorbox}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Diagram.pdf}
    \caption{Diagram}
    \label{fig:diagram}
\end{figure}

















\subsection{Pre-processing}
As discussed at the beginning of the section, our method consists of two parallel sections in the main stage that need to prepare the input pre-processing of both stages. For the step of extracting keywords from the graph criterion, we need the words and the relationships of these words with our neighbors, in which all the text words are stored in a token in their lists. But unlike the step-by-step extraction of keywords from the local benchmark, we need to separate the sentences. In this step, after separating the words and sentences and deleting the stop words from the input data, the prepared data are sent to each of the steps.

\begin{algorithm}
\SetAlgoLined

 \textbf{Input:} text,alpha,Lang \\
 sentences = split text into sentences\\
 \For{\textbf{each}  sentence \textbf{in} sentences  }{
    words = split sentence into words\\
    \For{\textbf{each}  word \textbf{in} words }{
        tag (word), to-lowercase (word), is-stop-word (word, Lang)\\
    }
 }
\textbf{Output:} List of annotated sentences, words\\

\caption{pre-processing}
\label{alg:pre-processing}
\end{algorithm}

\begin{tcolorbox}
    \small{
        The sentence segmentation and tokenization of example document:
        \begin{itemize}
            \item \textit{Sentence 1}: “fuzzy systems overlapping gaussian concepts approximation properties sobolev norms”.
            \item \textit{Sentence 2}: “in paper approximating capabilities fuzzy systems overlapping gaussian concepts considered".
            \item \textit{Sentence 3}: “the target function assumed sampled either regular gird according uniform probability density by exploiting connection radial basis functions approximators new method computation system coefficients provided showing guarantees uniform approximation derivatives target function”.
            \item \textit{Words}: 'fuzzy', 'systems', 'overlapping', 'gaussian', 'concepts', 'approximation', 'properties', 'sobolev', 'norms', in, 'paper', 'approximating', 'capabilities', 'fuzzy', 'systems', 'overlapping', 'gaussian', 'concepts', 'considered', 'the', 'target', 'function', 'assumed', 'sampled', 'either', 'regular', 'gird', 'according', 'uniform', 'probability', 'density', 'by', 'exploiting', 'connection', 'radial', 'basis', 'functions', 'approximators', 'new', 'method', 'computation', 'system', 'coefficients', 'provided', 'showing', 'guarantees', 'uniform', 'approximation', 'derivatives', 'target', 'function'.
        \end{itemize}
    }
\end{tcolorbox}
\subsection{Feature extracting}
Feature extraction consists of three steps: graph features extraction, textural features extraction and scoring these features. graph features and textural features extraction steps are parallel and extract features simultaneously. Each of the features represents different sight of the document, and the novelty of this paper is to fusion them to get an overview.

\subsubsection{Graph features}
Graphs deploy to represent relations and make them easy to understand. Also, provide more computational results. Graphs consist of nodes and vertices, , and nodes of the created unweighted and undirected graph are words, and vertices are 3-gram (trigram) of words. Figure \ref{fig:graph_example} shows an example of a document and its graph. Once the graph is created, centralities deployed to scoring each node. Centralities are measurements that score on nodes of a graph from a different point of view. Centralities are deployed in this paper are degree centrality, closeness centrality, betweenness centrality, eigenvector centrality, structural holes, page rank, clustering coefficient, Eccentricity. These eight centralities are the most used in NLP and its graph representation scoring.




\begin{tcolorbox}
    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=0.5\textwidth]{graph_example.pdf}
        \captionof{figure}{The generated graph of example document.}
        \label{fig:graph_example}
    \end{minipage}
\end{tcolorbox}


After calculating centralities, every node of the graph gets a score from each centrality, so eight scores for each node. Furthermore, this step should return one score for every word; to this aim, PCA is deployed. PCA is a technique for reducing the dimensionality of such datasets, increasing interpretability and minimizing information loss \cite{jolliffe_cadima_2016}. Nowadays, different versions of PCA with different aims are developed. PC1 is the first principal component used in NLP, and keyword extraction task \cite{vega2019multi} and we employed it to achieve one score per node (word). This progress is shown in algorithm \ref{alg:graph-features}.

\begin{algorithm}
 \SetAlgoLined
  \textbf{Input:} words \\
\# build graph\\
\For {\textbf{each}  word \textbf{in} words}{
    G.node = word\\
    G.edge = relation 2-gram of words
  }  
\# compute centrality measures\\
set-Centrality = \{DE, BE, CL, PR, EV, KC, EC, SH, CT\}

 \For {\textbf{each} word \textbf{in} words} {
    \For {\textbf{each} cnt \textbf{in} set-Centrality} {

    word.centrality  =  CALCCENTRALITY(Word, cnt)
}

word.graph-Score = word.centrality * PC1(word.centrality)
}

 \textbf{Output:} graph scores \\
 
 \caption{build graph and compute centrality measures Score}
 \label{alg:graph-features}
 \end{algorithm}
\subsubsection{textural features}
Word itself and its position in a document is essential and could make sense in understanding meaning. The features that have been utilized in this paper are capital letters, the position of the word, frequency of word in sentences of document, term frequency of word, and part of speech (POS) tag of word. Algorithm \ref{alg:local-features} details the features. Each of the features calculates a score. Finally, the textural feature score calculates in algorithm \ref{alg:local-features}. 

\begin{algorithm}
    \SetAlgoLined
    \textbf{Input:} textural, words \\
    
    \For{\textbf{each} word \textbf{in} words}{
        word.TCase = max (word.TF-a, word.TF-U) / (1 + ln (word.TF))\\
        word.TPos = ln (3 + mean (word.offsets-sentences))\\
        validTFs = [word.TF for word in words if not word.stop-word]\\
        avgTF = mean (validTFs)\\
        stdTF = stf (validTFs )\\
        word.TFNorm = word.TF / ( avgTF + stdTF)\\
        word.TSent =  word.offsets-sentences  /  sentences \\
        word.Pos = part-of-speech(word)  'NN' = 1 , 'Adj' = 0.5 , 'V' = 0.25 \\
    
        word.sentence-Score = (word.TCase + word.TSent + word.TFNorm + word.Pos) / word.TPos\\
    }
    \textbf{Output:} textural scores\\
  
    \caption{Feature extraction and Compute textural measures scores}
    \label{alg:local-features}
\end{algorithm}
\subsection{Scores computation}
In this part, calculated scores of words by graph features and textural features are merged to claim one score per word. Multiply is used to combine two scores, as can be seen in algorithm \ref{alg:scores-computation}.

\begin{algorithm}
  \SetAlgoLined
  \textbf{Input:} graph-Score , textural-Score \\
  
  \For {\textbf{each} word \textbf{in} words} {

    word.final-score = word.graph-Score × word.textural-Score\\
}

Sorting(words)\\
\textbf{Output:} final score
 
 \caption{compute final score from graph scores and Sentence scores}
 \label{alg:scores-computation}
\end{algorithm}

\begin{tcolorbox}
\begin{center}
        \captionof{table}{Candidate keywords in couple with graph-based score and textual-based score and Final score of the example document.}
        \begin{tabular}{lccc}
            \hline
            \textbf{Words}  &\textbf{Graph-based score} &\textbf{Texture score}  &\textbf{Final score} \\
            \hline
            uniform                & 1.77   & 3.92    & 6.95  \\
            concepts               & 1.46   & 4.71    & 6.87  \\
            target                 & 1.61   & 3.93    & 6.34  \\
            systems                & 1.13   & 4.86    & 5.49  \\
            fuzzy                  & 1.11   & 4.94    & 5.48  \\
            overlapping            & 1.15   & 4.21    & 4.85  \\
            function               & 1.19   & 3.93    & 4.66  \\
            gaussian               & 1.14   & 3.91    & 4.45  \\
            properties             & 1.18   & 3.58    & 4.22  \\
            approximation          & 1.12   & 3.68    & 4.1   \\
            density                & 1.26   & 3.14    & 3.94  \\
            sobolev                & 1.12   & 3.51    & 3.92  \\
            norms                  & 1.1    & 3.46    & 3.81  \\
            paper                  & 1.1    & 3.41    & 3.75  \\
            capabilities           & 1.11   & 3.38    & 3.74  \\
            gird                   & 1.18   & 3.17    & 3.73  \\
            probability            & 1.18   & 3.14    & 3.7   \\
            guarantees             & 1.2    & 3.08    & 3.68  \\
            connection             & 1.15   & 3.12    & 3.6   \\
            sampledeither          & 1.12   & 3.19    & 3.57  \\
            coefficients           & 1.15   & 3.09    & 3.56  \\
            derivatives            & 1.16   & 3.07    & 3.55  \\
            radial                 & 1.11   & 3.12    & 3.48  \\
            computation            & 1.12   & 3.09    & 3.46  \\
            functionsapproximators & 1.11   & 3.11    & 3.44  \\
            basis                  & 1.09   & 3.11    & 3.4   \\
            method                 & 1.09   & 3.1     & 3.39  \\
            showing                & 1.25   & 2.7     & 3.38  \\
            considered             & 1.17   & 2.78    & 3.26  \\
            assumed                & 1.15   & 2.76    & 3.18  \\
            according              & 1.15   & 2.74    & 3.16  \\
            exploiting             & 1.16   & 2.72    & 3.16  \\
            provided               & 1.17   & 2.7     & 3.15  \\
            regular                & 1.12   & 2.53    & 2.84  \\
            new                    & 1.11   & 2.51    & 2.78 \\
            \hline
        \end{tabular}
\end{center}
\end{tcolorbox}
\subsection{Key-phrase generation}
Key-phrase is a word or a couple of words (phrase) that convey the document's meaning. In the last part, keywords and scores are extracted. In this part, Key-phrases generate based on extracted keywords. N-grams is a general technique combining words to generate phrases, and determining N (length of N-gram) is one of the most issues. In this paper, HUPM\footnote{High Utility Pattern Mining} \cite{huang2015topic}, which core is FP-Growth, is applied to sentences to generate key-phrases. In the calculation of phrase score, keywords and calculated scores which are in the phrase are summed to achieve the phrase's score. The proposed algorithm is shown in algorithm \ref{alg:Key-phrase-generation}.

\begin{algorithm}
  \SetAlgoLined
    \textbf{Input:} final-score ,sentences, alpha \\
    \textbf{Initialize:} P ← , P U ← , delete pattern set ← \\
    Fp-growth = FP-Growth(sentences)\\
    
    \For {\textbf{pattern q} in  \textbf{Fp-growth}}{
    \eIf { }{
        
    Calculate the pattern utility u(q)\\
    Sort the pattern in P with utility in descending order\\

    Count = 0\\
    
    \For{each \textbf{pattern p} in \textbf{P}}{
    Calculate overlap-degree o(p, q) \\
    \eIf {o(p, q)  alpha then}{

                \eIf {u(q)  u(p) then}{

                    Add p into delete pattern set;

                Count+ = 1\\
            
            }
            {
            break
            }
            
    }
    {
    Count+ = 1;
    }
    
    }
    \If {Count == P  }{
        Add pattern q into P ;\\
        Add pattern utility u(q) into P U ;\\
        }
        
        Remove patterns in delete pattern set from P
        and its corresponding pattern utilities from P U ;\\
        Set delete pattern set ;\\
    }{break}
}

word patterns  = P

\For {each \textbf{keyphrase} in \textbf{keyphrases} }{
    keyphrase.score = sum(final score(words in keyphrase)) 
}
keywords condidate = concatinate(keyphrases,keywords)\\
\textbf{output:} keywords condidate
  
  \caption{Key-phrase generation}
  \label{alg:Key-phrase-generation}
\end{algorithm}

\subsection{Keywords ranking}
The last step is to select key-phrases between candidates, which are generated from the previous parts. To this aim, words and phrases sort descending by scores of each one, and top K scored ones picked as key-phrases of the document.

\begin{tcolorbox}
    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width = 0.8 \textwidth]{Keywords-cloud.png}
        \captionof{figure}{The word cloud illustration of keywords and key-phrases.}
        \label{fig:word-cloud}
    \end{minipage}
\end{tcolorbox}

\section{Experiments and Results}   \label{sec:experiments_results}
In this section, the evaluation of the proposed methods and comparison are detailed. Firstly, datasets that used as benchmarks introduced and then results in the metrics are demonstrated.



















\subsection{Datasets}   \label{subsec:dataset}
Evaluation of the proposed methods has been done on 7 benchmark datasets. Summary of datasets is shown on table \ref{tbl:dataset_summary}. As shown in table \ref{tbl:dataset_summary}, various inputs and topics are selected for evaluation. To proving language-independency of the proposed method, two datasets are non-English.

\begin{table}[]
    \centering
    \caption{Summary of datasets}
    \label{tbl:dataset_summary}
    \begin{tabular}{cccccm{2cm}m{2cm}}
        \hline
        \textbf{Dataset}    &\textbf{Lang.}    &\textbf{Input} &\textbf{Topic} &\textbf{\# documents}    &\textbf{Avg \# words in documents}   &\textbf{\# keywords (percent)} \\
        \hline
        SemEval2010 \cite{Kim2013}  &en &Article    &Computer science   &243    &8332.34    &4002 (16.47)   \\
        \rowcolor{gray!10} SemEval2017 \cite{Augenstein2018}    &en &Paragraph  &Scientific &493    &178.22 &8969 (18.19)   \\
        Inspec \cite{Hulth2003} &en &Abstract   &Computer science   &2000   &128.20 &29230 (14.64) \\
        \rowcolor{gray!10} Fao30 \cite{Medelyan2006}    &en &Article    &Agricultural   &30 &4777.40    &997 (33.23)    \\
Thesis100 \cite{medelyan_2015}  &en &Thesis &Scientific &100    &4728.86    &767 (7.67)\\
        \rowcolor{gray!10} pak2018 \cite{Campos2020}    &pl &Abstract   &Misc   &50 &97.36  &232 (4.64)  \\
        WikiNews \cite{Bougouin2013}    &fr &News   &Misc   &100   &293.52   &1177 (11.77)   \\
\hline
    \end{tabular}
\end{table}

\subsection*{semeval2010}
Semival2010 consists of 244 articles indexed by ACM in four computer science area, namely; distributed systems, information retrieval, distributed artificial intelligence, and social science, introduced by \citet{Kim2013}. The input type is articles in the length of 6 till 8 pages and keywords are labelled by authors and expert. It is worthwhile to say that keywords may not be in the text. Summery of the dataset is shown in table \ref{tbl:dataset_summary}.

\subsection*{SemEval2017}
Semival2017 contains 500 articles abstract indexed by ScienceDirect equally divided in area of computer science, material engineering, and physics. Experts label the keywords. The dataset introduced fir the first time by \citet{Augenstein2018}. Summery of the dataset is shown in table \ref{tbl:dataset_summary}.

\subsection*{Inspec}
Inspec \cite{Hulth2003} includes 2000 articles abstracts in computer science collected between the years 1998 and 2002. Each document has two sets of keywords: the controlled keywords, which are manually controlled assigned keywords that appear in the Inspec thesaurus but may not appear in the document, and the uncontrolled keywords, which are freely assigned by the editors (that is, they are not restricted to the thesaurus or the document). In our experiments, we consider a union of both sets as the keywords. In table \ref{tbl:dataset_summary} summary of Inspec has been shown.

\subsection*{fao30}
fao30 is a collection of 30 agricultural documents from the Food and Agriculture Organization (FAO) of the United Nations. The fao30 is introduced in \cite{Medelyan2006}, and a summary is shown in table \ref{tbl:dataset_summary}. Six experts annotated foa30. fao30 is deployed to evaluate methods in long and non-scientific documents.

\subsection*{Thesis100}
Thesis100 \cite{medelyan_2015} has included 100 master and PhD thesis of the University of Waikato, New Zeland in English. The domain of the thesis100 made available is quite different ranging from chemistry, computer science, philosophy, history, and others. Such as fao30, the Thesis100 is used to evaluating methods in long documents (more than 10 pages).

\subsection*{pak2018}
pak2018 is a dataset in Polish set of 50 abstracts of journals on technical topics collected from Measurement Automation and Monitoring\footnote{\url{http://pak.info.pl/}} and introduced by \citet{Campos2020}. The keywords are author-assigned, and a summary of the dataset is displayed in table \ref{tbl:dataset_summary}.

\subsection*{WikiNews}
WikiNews \cite{Bougouin2013} is a French corpus created from the French version of WikiNews\footnote{\url{https://www.wikinews.org/}} that contains 100 news articles published between May 2012 and December 2012 and manually annotated by at least three students. More details are given in table \ref{tbl:dataset_summary}.


\subsection{Results}    \label{subsec:results}
The first step on the calculating results is defining metrics. In the following paragraph, metrics defined and then a discussion on the results of the proposed method compared with state-of-the-art methods are formed.

\begin{table}[]
    \centering
    \caption{(Textual column should be checked) Comparison of features of the state-of-the-art methods and the proposed method.}
    \label{tbl:features-summary}
    \begin{tabular}{c|ccc|ccc}
        \hline
        \multirow{2}{*}{\textbf{Method}} &\multicolumn{3}{c|}{\textbf{Unsupervised}}   &\multicolumn{3}{c}{\textbf{Language dependence}}  \\
        &\textbf{Statistical}   &\textbf{Textual}   &\textbf{Graph-based}   &\textbf{Stop words}    &\textbf{POS tag}   &\textbf{Stream data}    \\
        \hline
        Proposed method & &\checkmark   &\checkmark   &\checkmark   &\checkmark   &   \\
        \rowcolor{gray!10} TF-IDF \cite{Lott2012}   &\checkmark &   &   &\checkmark &   &   \\
        KP-Miner \cite{El-Beltagy2009}  &\checkmark &   &   &\checkmark &   &   \\
\rowcolor{gray!10} YAKE \cite{Campos2020}  &    &\checkmark    &   &\checkmark &   &   \\
        RaKUn \cite{rakun2019}   &   &\checkmark &   &   &   &   \\
        \rowcolor{gray!10} Text Rank \cite{Mihalcea2004}    &  &   &\checkmark &   &\checkmark & \\
        Single Rank \cite{Wan2008}  &    &   &\checkmark &\checkmark &\checkmark &   \\
        \rowcolor{gray!10} Topic Rank \cite{Bougouin2013}   &  &   &\checkmark &\checkmark &\checkmark &   \\
        Topical Page Rank \cite{Sterckx2015}    &    &   &\checkmark &\checkmark  &\checkmark &\checkmark   \\
        \rowcolor{gray!10} Posotion Rank \cite{Florescu2017}    &  &   &\checkmark &\checkmark  &\checkmark &   \\
        Multiparted Rank \cite{Boudin2018}  &    &   &\checkmark &\checkmark &\checkmark &\checkmark\\
        \rowcolor{gray!10} Expended Rank \cite{Wan2008} &  &   &\checkmark &   &\checkmark &   \\
\hline
    \end{tabular}
\end{table}

Three primary metrics are used in evaluating keyword extraction methods are precision, recall, and F1-score. To calculating these metrics, 4 concepts, TP, TN, FP, FN, should be defined. TP denotes to the phrase correctly defined as a keyword. TN denotes to the phrase correctly define as a not keyword. FP denotes the phrase incorrectly define as a keyword, and FN denotes the phrase does not define as a keyword incorrectly. Respectively, precision and recall calculate by equation \ref{eq:precision} and equation \ref{eq:recall}. F1-score means the harmonic mean of the precision and recall, as equation \ref{eq:f1}.Traditionally, keyword extraction is a ranking problem. Based on this, we opted to calculate Precision at k (Precision@k), Recall at k (Recall@k) and F1-score at k (F1-score@k) to determine the effectiveness of the proposed method.











The features of the state-of-the-art methods used as baselines compared with the proposed method have been shown in table \ref{tbl:features-summary}. As you see, two models of unsupervised, statistical and graph-based, methods with approaches on pre-processings detailed. To evaluation the state-of-the-art methods, implementation obtained from pke\footnote{python keyphrase extraction} \{\url{https://github.com/boudinfl/pke}\} developed by Boudin. Table \ref{tbl:result-precision}, \ref{tbl:result-recall}, \ref{tbl:result-f1} are the results of the methods in each of metrics.

Compared to the proposed method with other state-of-the-art methods, The proposed method achieves the best performance in all metrics and datasets except Thesis100. Thiesis100 is consists of very long texts, MSc thesis, with a low ratio of keywords to documents (7.67\% based on table \ref{tbl:dataset_summary}). Nevertheless, the proposed method obtains third place compared to other methods in all of the metrics. As seen, the graph-based methods outperformed this dataset. It is worthwhile to say that the comparisons have been made in Polish and France documents and the proposed method reaches the best results.

\begin{table}[]
    \centering
    \caption{Comparison of methods on Precision @10. Boldface values indicates the best values on each dataset.}
    \label{tbl:result-precision}
    \begin{tabular}{cc|cccccccc}
        \hline
        \multicolumn{2}{c|}{\multirow{2}{*}{Methods}}    &\multicolumn{8}{c}{Dataset}   \\
        \cline{3-9}
        &   &SemEval10    &SemEval17    &Inspec &Fao30  &Thesis100 &pak18   &WikiNews   \\ \hline
        \multicolumn{2}{c|}{Proposed Method (FRAKE)} &\textbf{0.415}   &\textbf{0.536} &\textbf{0.572}  &\textbf{0.294}  &0.24   &\textbf{0.126}    &\textbf{0.537} \\\cdashline{1-10}
&TF-IDF \cite{Lott2012} &0.316  &0.488  &0.475  &0.251  &0.28   &0.104  &0.441   \\&KP-Miner \cite{El-Beltagy2009} &0.347  &0.398  &0.349  &0.222  &\textbf{0.291} &0.1    &0.452 \\&YAKE \cite{Campos2020}   &0.345    &0.334    &0.329    &0.1    &0.062  &0.054  &0.151  \\ &Text Rank \cite{Mihalcea2004} &0.019  &0.335  &0.345  &0  &0.003  &0.008  &0.098 \\&Single Rank \cite{Wan2008}    &0.028   &0.151  &0.293  &0.007  &0.008  &0.027  &0.269 \\ &Topic Rank \cite{Bougouin2013} &0.237  &0.436  &0.465 &0.129   &0.168  &0.04   &0.463 \\ &Topical Page Rank \cite{Sterckx2015}  &0.027 &0.401 &0.42  &0.007 &0.11 &0.012  &0.331 \\&Position Rank \cite{Florescu2017}  &0.076 &0.438 &0.473 &0.025 &0.031 &0.035  &0.443 \\ &MultiPartite Rank \cite{Boudin2018} &0.268 &0.458 &0.497 &0.159 &0.194 &0.037 &0.442 \\ &Expanded Rank \cite{Wan2008}  & 0.027 & 0.396 & 0.413 & 0.007  &0.008 & 0.013  & 0.273 \\\hline
\end{tabular}
\end{table}


\begin{table}[]
    \centering
    \caption{Comparison of methods on Recall @10. Boldface values indicates the best values on each dataset.}
    \label{tbl:result-recall}
    \begin{tabular}{cc|cccccccc}
        \hline
        \multicolumn{2}{c|}{\multirow{2}{*}{Methods}}    &\multicolumn{8}{c}{Dataset}   \\
        \cline{3-9}
        &   &SemEval10    &SemEval17    &Inspec &Fao30  &Thesis100 &pak18   &WikiNews   \\\hline
        \multicolumn{2}{c|}{Proposed Method (FRAKE)} &\textbf{0.343}   &\textbf{0.544} &\textbf{0.607}  &\textbf{0.287}  &0.316   &\textbf{0.296} &\textbf{0.564} \\\cdashline{1-10}
&TF-IDF \cite{Lott2012} &0.285  &0.5 &0.45   &0.226  &0.347  &0.247  &0.468 \\&KP-Miner \cite{El-Beltagy2009} &0.313  &0.405  &0.359  &0.2    &\textbf{0.354} &0.185  &0.464 \\&YAKE \cite{Campos2020}   &0.311    &0.342  &0.345  &0.1    &0.079  &0.123  &0.155  \\ &Text Rank \cite{Mihalcea2004} &0.017  &0.31  &0.326 &0  &0.003 &0.17  &0.098 \\&Single Rank \cite{Wan2008}    &0.025   &0.141  &0.551 &0.006  &0.009  &0.013  &0.256 \\&Topic Rank \cite{Bougouin2013} &0.213  &0.400  &0.431  &0.116  &0.21   &0.08   &0.438  \\&Topical Page Rank \cite{Sterckx2015}  &0.024 &0.37 &0.395  &0.006  &0.012   &0.022 &0.315 \\&Position Rank \cite{Florescu2017}  &0.068 &0.405 &0.444 &0.023 &0.045  &0.073 &0.419 \\&MultiPartite Rank \cite{Boudin2018} &0.242 &0.421  &0.463  &0.143  &0.242  &0.074  &0.462  \\&Expanded Rank \cite{Wan2008}  &0.024   &0.366  &0.399  &0.006  &0.009  &0.027  &0.26 \\ \hline
\end{tabular}
\end{table}


\begin{table}[]
    \centering
    \caption{Comparison of methods on F1-score @10. Boldface values indicates the best values on each dataset.}
    \label{tbl:result-f1}
    \begin{tabular}{cc|cccccccc}
        \hline
        \multicolumn{2}{c|}{\multirow{2}{*}{Methods}}    &\multicolumn{8}{c}{Dataset}   \\
        \cline{3-9}
        &   &SemEval10    &SemEval17    &Inspec &Fao30  &Thesis100 &pak18   &WikiNews   \\\hline
        \multicolumn{2}{c|}{Proposed Method (FRAKE)} &\textbf{0.375}   &\textbf{0.54} &\textbf{0.589}  &\textbf{0.29}  &0.272 &\textbf{0.177} &\textbf{0.55} \\\cdashline{1-10}
&TF-IDF \cite{Lott2012} & 0.3 & 0.493 & 0.462 & 0.238 & 0.315&  0.146 & 0.454 \\&KP-Miner \cite{El-Beltagy2009} &0.329 &0.402 &0.354 & 0.21 &\textbf{0.319} &0.129  &0.457 \\&YAKE \cite{Campos2020}   &0.327    &0.338  &0.337  &0.1    &0.07   &0.075  &0.153   \\ &Text Rank \cite{Mihalcea2004} &0.018   &0.322  &0.33 &0  &0.003    &0.011 &0.098 \\&Single Rank \cite{Wan2008}    &0.026   &0.145  &0.381  &0.007  &0.009  &0.017  &0.263 \\ &Topic Rank \cite{Bougouin2013} &0.224  &0.417  &0.448  &0.122  &0.187  &0.053  &0.45 \\
        &Topical Page Rank \cite{Sterckx2015}   &0.026  &0.385  &0.407  &0.007  &0.012  &0.015 &0.323 \\&Position Rank \cite{Florescu2017}  &0.072  &0.421  &0.458  &0.024  &0.037  &0.047  &0.43  \\&MultiPartite Rank \cite{Boudin2018} &0.254 &0.439  &0.48   &0.15   &0.215  &0.05   &0.452 \\&Expanded Rank \cite{Wan2008}  &0.025   &0.381  &0.401  &0.007  &0.009  &0.017  &0.266 \\\hline
\end{tabular}
\end{table}
\section{Conclusion}
In this paper, a fusion method for keyword extraction called FRAKE is proposed. Two approaches, graph and textural features, are extracted from text and FRAKE fusion them to extract keywords and key phrases, also. Five steps constitute the proposed method, pre-processing, graph feature extraction, textural feature extraction, Scores computation, Key-phrase generation, and Keywords ranking, respectively. The results are shown the best performance of the proposed method in English, Polish, and France texts.
\section*{Declarations}
\subsection*{Funding}
No funding was received to assist with the preparation of this manuscript.
\subsection*{Conflicts of interests}
The authors have no conflicts of interest to declare that are relevant to the content of this article.
\subsection*{Ethical approval}
This article does not contain any studies with human participants or animals performed by any of the authors.

\bibliography{mybibfile.bib}

\end{document}