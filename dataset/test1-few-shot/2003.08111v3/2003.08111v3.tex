









\documentclass[a4paper,conference]{IEEEtran}



\usepackage{graphicx}
\usepackage{comment}
\usepackage{amsmath,amssymb} \usepackage{color}
\usepackage{booktabs}
\usepackage{stfloats}

\usepackage[citecolor=green]{hyperref}


\newcommand{\blu}[1]{{\emph{\color{blue} {#1}}}}
\newcommand{\tbf}[1]{{\textbf{#1}}}





















\ifCLASSINFOpdf
\else
\fi














































\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
\title{Transformer Networks for Trajectory Forecasting}






\author{\IEEEauthorblockN{Francesco Giuliari}
\IEEEauthorblockA{University of Verona\\
}

\and
\IEEEauthorblockN{Irtiza Hasan}
\IEEEauthorblockA{Inception Institute of Artificial Intelligence\\
}
\and
\IEEEauthorblockN{Marco Cristani}
\IEEEauthorblockA{University of Verona\\
}
\and
\IEEEauthorblockN{Fabio Galasso}
\IEEEauthorblockA{Sapienza University of Rome\\
}}

\iffalse
\author{\IEEEauthorblockN{Francesco Giuliari\IEEEauthorrefmark{1},
Irtiza Hasan\IEEEauthorrefmark{2},
Marco Cristani\IEEEauthorrefmark{1},
Fabio Galasso\IEEEauthorrefmark{3}
}
\IEEEauthorblockA{\IEEEauthorrefmark{1}University of Verona, Italy, \{francesco.giuliari, marco.cristani\}@univr.it\\
\IEEEauthorrefmark{2}Inception Institute of Artificial Intelligence, irtiza.hasan@inceptioniai.org\\
\IEEEauthorrefmark{3}Sapienza University of Rome, Italy, galasso@di.uniroma1.it}
}
\fi














\maketitle









\IEEEpeerreviewmaketitle
\begin{abstract}
    

Most recent successes on forecasting the people motion are based on LSTM models and \emph{all} most recent progress has been achieved by modelling the social interaction among people and the people interaction with the scene.
We question the use of the LSTM models and propose the novel use of Transformer Networks for trajectory forecasting. This is a fundamental switch from the sequential step-by-step processing of LSTMs to the only-attention-based memory mechanisms of Transformers. In particular, we consider both the original Transformer Network (TF) and the larger Bidirectional Transformer (BERT), state-of-the-art on all natural language processing tasks.
Our proposed Transformers predict the trajectories of the individual people in the scene. These are ``simple'' models because each person is modelled separately without any complex human-human nor scene interaction terms. In particular, the TF model \emph{without bells and whistles} yields the best score on the largest and most challenging trajectory forecasting benchmark of TrajNet~\cite{sadeghiankosaraju2018trajnet}. Additionally, its extension which predicts multiple plausible future trajectories performs on par with more engineered techniques on the 5 datasets of ETH~\cite{pellegrini2009iccv}+UCY~\cite{lerner2007crowds}. Finally, we show that Transformers may deal with missing observations, as it may be the case with real sensor data.
Code is available at \texttt{github.com/FGiuliari/Trajectory-Transformer}.

\end{abstract}


\iffalse
Forecasting the people motion is an important but complex task, which has been addressed so far with a variety of temporal and spatio-temporal models. Most recent successes are based on LSTM models and \emph{all} most recent progress has been achieved by modelling the social interaction among people and the people interaction with the scene.
We question the use of the LSTM models and propose the novel use of Transformer Networks for trajectory forecasting. This is a fundamental switch from the sequential step-by-step processing of LSTMs to the only-attention-based memory mechanisms of Transformers. In particular, we consider both the original Transformer Network (TF) and the larger Bidirectional Transformer (BERT), state-of-the-art on all natural language processing tasks.
Our proposed Transformers predict the trajectories of the individual people in the scene. These are ``simple'' model because each person is modelled separately without any complex human-human nor scene interaction terms. In particular, the TF model \emph{without bells and whistles} yields the best score on the largest and most challenging trajectory forecasting benchmark of TrajNet~\cite{sadeghiankosaraju2018trajnet}. Additionally, its extension which predicts multiple plausible future trajectories performs on par with more engineered techniques on the 5 datasets of ETH~\cite{pellegrini2009iccv}+UCY~\cite{lerner2007crowds}. Finally, we show that Transformers may deal with missing observations, as it may be the case with real sensor data.
\fi






 \section{Introduction}\label{sec:intro}

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{./Figures/Teaser_F_2.png}
\caption{People trajectory forecasting stands for predicting the future motion of people (\textit{green ground-truth dots}), given an observation interval (\textit{blue dots}). LSTM (left) sequentially processes the observations before starting to predict, while TF analyses in one shot all available observations.}
\label{fig:teaser}
\end{figure*}



Pedestrian forecasting, the goal of predicting future people motion given their past trajectories, has been steadily growing in attention by the research community. Further to being a crucial compound of trackers, especially for the cases of large motion and/or missing observations, the topic serves early action recognition, surveillance and automotive systems. 


Starting from \cite{alahi2016cvpr}, Long Short-Term Memory (LSTM) networks have been the workhorse for forecasting and progress has been achieved by devising social pooling mechanisms to model the people social interaction~\cite{alahi2016cvpr,gupta2018social}. The LSTM is based on sequentially processing sequences and storing hidden states to represent knowledge about the people, e.g.\ its speed, direction and motion pattern. Most modern approaches have challenged each other on the social interaction of pedestrians, each modelled with a separate LSTM and exchanging information by means of social pooling mechanisms~\cite{alahi2016cvpr,gupta2018social}. In fact best performing approaches additionally include the semantics of the scene into the LSTMs~\cite{kosaraju2019social,salzmann2020trajectron++,ivanovic2019trajectron,sadeghian2019sophie}. However LSTMs have also been target of criticism: their memory mechanism has been criticised~\cite{bai2018empirical,Luo18} and, most recently, also their capability of modelling social interaction~\cite{scholler2020constant,becker2018red,becker2018evaluation}. An in-depth understanding of such mechanisms has not been supported by the adopted datasets, such as the 5 datasets of ETH~\cite{pellegrini2009iccv} and UCY~\cite{lerner2007crowds}, where performance measures are close to saturation, since leading techniques only report average forecasting errors of 20cm across 200m-long pavements. 

In this work we side-step social and map mechanisms, and propose to model the trajectories of individual people by Transformer Networks~\cite{TransformersNIPS17}, for the first time. Transformer networks have been proposed for Natural Language Processing to model word sequences. These use attention instead of sequential processing. In other words, these estimate which part of the input sentence to focus on, when needing to translate, answer a question or complete the sentence~\cite{BERT19,SparseTF}. Here we consider for trajectory forecasting the original Transformer Network (TF) and the Bidirectional Transformer (BERT) models, on which state-of-the-art NLP algorithms are based. Fig.~\ref{fig:teaser} illustrates the fundamental difference between TF and LSTM: LSTM sequentially processes the observations before starting to predict auto-regressively, while TF ``looks'' at all available observations, weighting them according to an attention mechanism.





We assess the performance of TF and BERT on the TrajNet benchmark~\cite{sadeghiankosaraju2018trajnet}, in order to have a clean evaluation (TrajNet uses a unified evaluation system with a dedicated server) against 42 forecasting approaches, on a large selection of datasets.
Our TF outperforms all other techniques, also those including social mechanisms. TF compares favorably also on the ETH+UCY datasets, in particular beating all of the approaches that consider the individual trajectories only. Finally we conduct an ablation to highlight the potential of the Transformers, quantitatively and qualitatively. Of particular interest is the ability of TF to still predict from inputs with missing observation data, thanks to its attention mechanism, which the LSTM cannot do.










\vspace{-0.2cm}



 \section{Related work}
\label{sec:prev}





Forecasting people trajectories has been studied for over two decades and relevant literature has been surveyed by the work of \cite{becker2018evaluation,morris2008survey}.
For the purpose of this paper, we distinguish two main trends of related work: a first which has focused on progressing sequence modelling and a second which has modelled the interactions between the people and between the people and the scene.

\noindent\textit{Sequence modelling:}
Trajectory forecasting has experienced a steady progress from hand-crafted energy-based optimization approaches to data-driven ones.
Early work on human path prediction have adopted linear~\cite{mccullagh1989generalized} or Gaussian regression models ~\cite{quinonero2005unifying,williams1998prediction}, time-series analysis~\cite{priestley1981spectral} and auto-regressive models~\cite{akaike1969fitting}, optimizing for hand-crafted energy functions. By contrast, later models have been most successful by the adoption of LSTM~\cite{hochreiter1997long} and RNN models, trained with copious amounts of data.
In particular, LSTM can be employed to regress directly the predicted values~\cite{becker2018red,gupta2018social,salzmann2020trajectron++}, or to produce mean and (diagonal) covariance over the  coordinates in order to express the uncertainty associated to the prediction~\cite{alahi2016cvpr}. In the latter case, we refer to this model as \emph{Gaussian LSTM}.
Here we argue that Transformer Networks are most suitable to sequence modelling and to forecast trajectories, thanks to their better capability to learn non-linear patterns, especially emerging when large amounts of data is available.



\noindent\textit{Social models and context:}
Enabled by the flexibility of the LSTM machinery, best performance has been recently achieved by modelling the social interaction~\cite{alahi2016cvpr,gupta2018social,vemula2018social} among people and the scene context~\cite{sadeghian2019sophie,salzmann2020trajectron++,kosaraju2019social}, aided by tracking dynamics~\cite{sadeghian2017tracking} and the spatio-temporal relations among neighboring people~\cite{ijcai2017-386,su2016crowd}.
Much literature has recently criticised the capability of LSTM to model the human-human interaction~\cite{scholler2020constant,becker2018red,becker2018evaluation}, maintaining that this limits the model generalization capability~\cite{scholler2020constant}. Our work side-steps social and environmental interactions and focuses on the prediction of the motion of each person individually. Somehow surprisingly, our ``simple'' approach achieves best performance on the most challenging benchmark of TrajNet.

In this work, we leverage findings and state-of-the-art techniques developed within the NLP field to model word sequences. In particular, we consider here for trajectory forecasting the original Transformer Networks~\cite{TransformersNIPS17}, first to model sequences merely by attention mechanisms. Aside TF, we consider the Bidirectional Transformers BERT~\cite{BERT19}, which forms the basis for the current performer on most NLP tasks~\cite{RoBERTa19}. To the best of our knowledge, this is the first work adopting NLP technique for trajectory forecasting.




























 


\section{The Transformer Model}\label{sec:method}

\begin{figure*}[!t]
\centering
  \includegraphics[width=0.40\textwidth, height = 4.5cm]{Figures/LSTM__fix.png}
  \includegraphics[width=0.59 \textwidth, height = 4.5cm]{Figures/TF__.png}
\vspace{-0.5cm}
\caption{Model illustration of LSTM~(\textit{left}) and TF~(\textit{right}). At each time step, LSTM leverages the current-frame information and its hidden state. By contrast, TF leverages the encoder representation of the observed input positions and the previously predicted outputs. In purple and grey are the self-attention and encoder-decoder attention modules, that allow TF to learn on which past position it needs to focus to predict a correct trajectory.
}
\label{fig:lstm_tf}
\vspace{-0.2cm}
\end{figure*}

















We propose a multi-agent framework where each person is modelled by an instance of our transformer network. Each Transformer Network predicts the future motion of the person as a result of its previous motion.

We describe in this section the model input and output (Sec.~\ref{sec:inout}), the encoder-decoder Transformer Network (TF) (Sec.~\ref{sec:encdec}) and the just-encoder BERT model (Sec.~\ref{sec:bert}) and the implementation details (Sec.~\ref{sec:modtrain}). 



\subsection{Model input and output}\label{sec:inout}

For each person, the transformer network outputs the predicted future positions by processing their current and prior positions (observations or motion history). We detail here each of the input and output information and parallel those with the established LSTM, with reference to Fig.~\ref{fig:lstm_tf}.




\paragraph{Observed and predicted trajectories}
In formal terms, for person , we are provided a set  of  observed current and prior positions in Cartesian coordinates , and we are required to predict a set  of  predicted positions. In order to let the transformer deal with the input, this is embedded onto a higher -dimensional space by means of a linear projection with a matrix of weights , i.e., .

In the same way, the output of our transformer model for person  at time  is the -dimensional vector , which is back-projected to the Cartesian person coordinates . LSTM and TF share this aspect.













\paragraph{Positional encoding}
The transformer encodes time for each past and future time instant  with a ``positional encoding''. In other words, each input embedding  is time-stamped with its time . The same encoding is used to prompt the model to predict into future instants, as we detail in the next section.

More formally, the input embedding  is time-stamped at time  by adding a positional encoding vector , of the same dimensionality :



We use sine/cosine functions to define  as in~\cite{TransformersNIPS17}:

In other words, each dimension of the positional encoding varies in time according to a sinusoid of different frequency, from  to . This ensures a unique time stamp for sequences of up to 10000 elements and extends unseen lengths of sequences. 

In this aspect, TF differs greatly from LSTM, cf.\ Fig.~\ref{fig:lstm_tf}. LSTM processes the input sequentially and the order of input positions determine the flow of time. It does not therefore need a positional encoding. However, LSTM needs to ``unroll'' at training time, i.e.\ back-propagate the signal sequentially across the LSTM blocks processing the observations. By contrast, the training of TF is parallelizable.


Notably, thanks to the positional encoding which time-stamps the input, TF may deal with missing observations.  Missing data is just neglected, but the model is aware of the relative time-stamps of the presented observations. In Sec.~\ref{Sec:exp}, we experiment on this unique feature, important when dealing with real sensor data.

\paragraph{Regression Vs.\ classification}

Regression Vs.\ classification is a recurrent question in trajectory forecasting. Regression techniques, predicting the (,) coordinates directly, generally outperform classification-based approaches, where the inputs are quantized into classes and the input data represented as one-hot-vectors. We test both approaches and confirm the better performance of regression. However, a classification approach, which we dub TF, provides a probabilistic output across the quantized motions.

We leverage TF to sample multiple future predictions, which we assess both quantitatively and qualitative. The predictions of TF are multi-modal, as we illustrate in Sec.~\ref{Sec:exp}.










\subsection{Encoder-decoder Transformer (TF)}\label{sec:encdec}

As illustrated in Fig.~\ref{fig:lstm_tf}, TF is a modular architecture, where both the encoder and the decoder are composed of 6 layers, each containing three building blocks: \textbf{i.}\ an attention module, \textbf{ii.}\ a feed-forward fully-connected module, and \textbf{iii.}\ two residual connections after each of the previous blocks.

The capability of the network to capture sequence non-linearities lies mainly in the attention modules. Within each attention module, an entry of a sequence, named ``query'' (Q), is compared to all other sequence entries, named ``keys'' (K) by a scaled dot product, scaled by the equal query and key  embedding dimensionality. The output is then used to weight the same sequence entries, named now ``values'' (V). Attention is therefore given by the equation:


The goal of the encoding stage is to create a representation for the observation sequence, which makes the model \emph{memory}. To this goal, after the encoding of the  input embeddings , the network outputs two vectors of keys  and values  which would be passed on to the decoder stage.

The decoder predicts auto-regressively the future track positions. At each new prediction step, a new decoder query  is compared against the encoder keys  and values  according to Eq.~\eqref{eq:att} (encoder-decoder attention) and against the previous decoder prediction (self-attention).


Note the important difference w.r.t.\ LSTM: TF maintains the encoding output (memory) separate from the decoded sequence, while LSTM accumulates both into its hidden state, steering what to memorize or forget at each time. We believe this may contribute to explain how TF outperforms LSTM in long-term horizon predictions, cf.\ Sec.~\ref{Sec:exp}.







\subsection{BERT}\label{sec:bert}

We consider for trajectory forecasting a second Transformer model, BERT~\cite{BERT19}.
Differently from TF, BERT is only composed of an encoder and it trains and infers thanks to a masking mechanism. In other words, the model hides (masks) from the self-attention the output positions which it targets for prediction as the TF decoder also does.
During training the model learns to predict masked positions. At inference, the model output predictions for the masked outputs.

BERT is the \textit{de-facto} reference model for state-of-the-art NLP methods, but larger than TF (2.2 times larger). As we would illustrate in Sec.~\ref{Sec:exp}, training BERT on the current largest trajectory forecasting benchmarks does not keep up to the expectations. We draw inspiration from transfer learning and test therefore also how a BERT pre-trained on an NLP task performs on the target task. In particular, we take the lower-cased English text using Whole-Word-Masking; we substitute for the word embedding from dictionary keys with similar linear modules encoding (,) positions; and then we similarly convert also the output into (,) positions.





\subsection{Implementation details}\label{sec:modtrain}

Our TF implementations adopts the parameters of the original Transformer Networks~\cite{TransformersNIPS17}, namely , 6 layers and 8 attention heads. We adopt an L2-loss between the predicted and annotated pedestrian positions and train the network via backpropagation with the Adam optimizer, linear warm-up phase for the first 5 epoch and a decaying learning rate afterward; dropout value of 0.1.
The normalization of the network input influences its performance, as also maintained in \cite{zhang2019sr,graves2013generating}. So we normalize the people speeds by subtracting the mean and dividing by the standard deviation of the train set.
For the TF, we quantize the people motion by clustering speeds into 1000 joint (,) bins, then encode the position by 1000-way one-hot vectors. In order to get a good cluster granularity, we augment the training data by random scaling uniformly with scale .












 \section{Experimental Evaluation}\label{Sec:exp}


We show the capabilities of the proposed Transformer networks for trajectory forecasting on two recent and large datasets: the TrajNet Challenge~\cite{sadeghiankosaraju2018trajnet} dataset and the ETH+UCY dataset~\cite{pellegrini2009iccv,lerner2007crowds}. Additionally, we perform an ablation study to quantify the model robustness, also in comparison with the widely-adopted LSTM. This includes varying the observation horizon and testing the model on missing data, the latter occurring when some observation samples are missing due to frame-rate drops or excessive uncertainty in the tracking data.




\subsection{The Trajnet Challenge}\label{sec:trajnet}

\noindent\textbf{The TrajNet Dataset:}
At the moment of writing, the TrajNet Challenge\footnote{\url{http://trajnet.stanford.edu/}} ~\cite{sadeghiankosaraju2018trajnet} does represent the largest multi-scenario forecasting benchmark~\cite{rudenko2019human}; the challenge requests to predict 3161 human trajectories, observing for each trajectory 8 consecutive ground-truth values (3.2 seconds) \emph{i.e.}, , in world plane coordinates (the so-called \emph{world plane Human-Human} protocol) and fo\-re\-cas\-ting the following 12 (4.8 seconds), \emph{i.e.}, .
In fact, TrajNet is a superset of diverse datasets that requires to train on four families of trajectories, namely 1) BIWI Hotel~\cite{pellegrini2009iccv} (orthogonal bird's eye flight view, moving people), 2) Crowds UCY \cite{lerner2007crowds} (3 datasets, tilted bird's eye view, camera mounted on building or utility poles, moving people), 3) MOT PETS~\cite{PETS2009} (multi\-sensor, different human activities) and 4) Stanford Drone Dataset~\cite{robicquet2016learning} (8 scenes, high orthogonal bird's eye flight view, different agents as people, cars etc.), for a total of 11448 trajectories. Testing is requested on diverse partitions of BIWI Hotel, Crowds UCY, Stanford Drone Dataset, and is evaluated by a specific server (ground-truth testing data is unavailable for applicants). As a proof of its toughness, it is worth noting that many recent studies restrict on subsets of TrajNet~\cite{deo2020trajectory,haddad2020self,ridel2020scene}, adopting their train/test splits~\cite{van2019safecritic}.  We instead consider the whole TrajNet dataset for our experiments. TrajNet allows to consider concurrent trajectories, so that it is compliant with ``social'' approaches, that can apply. Conversely, it does not allow use raw images, so that approaches which infer on maps as ~\cite{sadeghian2019sophie,ivanovic2019trajectron,salzmann2020trajectron++} cannot apply.   
\vspace{0.2cm}

\noindent\textbf{Metrics:} In agreement with most literature on people trajectory forecasting, the TrajNet performance is measured in terms of: Mean Average Displacement (MAD, equivalently  Average Displacement Error ADE~\cite{ivanovic2019trajectron}), measuring the general fit of the prediction w.r.t. the ground truth, averaging the discrepancy at each time step; Final Average Displacement (FAD, equivalently Final Displacement Error FDE~\cite{ivanovic2019trajectron}), to check the goodness of the prediction at the last time step.
The average of MAD and FAD is used to rank the approaches.
\vspace{0.2cm}

\noindent\textbf{Results on TrajNet:}
We report in Table~\ref{Tab:TrajNet} the complete list of 22 \emph{published} comparative approaches, for a total of 39 approaches at the moment of writing; we omit the 18 unpublished results, all of which, apart from one, nonetheless had lower performance than the previous published top-scoring approach REDv3~\cite{becker2018evaluation}.
In the table, ``Rank'' indicates the absolute ranking over all the approaches, including the unpublished ones; ``Year'' the year of publication of the method; ``Context'' indicates whether the additional social context (the trajectories of the other co-occurring people) is taken into account (``s'') or not (``/'').


The scores in \textcolor{blue}{\textit{blue italic}} refer to the methods proposed in this work (TF, TF, BERT, BERT\_NLP\_pretrained). 
Surprisingly, the TF model is the new best, with an advantage in terms of both MAD and FAD w.r.t.\ the second REDv3~\cite{becker2018evaluation} and reducing the total error across the 3161 test tracks by 145 meters. 

It is of interest that the top four approaches (including ours) are individual ones, so no social context is taken into account. These results undoubtedly suggest that in 3 seconds of individual observation of an individual, much information about his future can be extracted, and TF is the most successful in doing it. 
In fact,  social approaches appear at lower ranks: the first among them is the SR-LSTM~\cite{zhang2019sr}, then the highly-cited Social Forces~\cite{helbing1995social} (rank 9 and 27), the  MX-LSTM~\cite{Hasan18} and Social GAN~\cite{gupta2018social}. 
The quantized TF ranks 16th, very probably due to quantization errors.
For the trajnet challenge the TF was used in its deterministic mode, i.e.\ the class with highest confidence was selected for the 12 predictions. This is done so because TrajNet is not set up to evaluate best-of-N metric and only a single prediction can be evaluated by the server.

BERT trained from scratch on trajectories ranks 25th; its  NLP-pretrained version, fine-tuned on TrajNet, follows immediately. The BERT performance may indicate that the model does require a way larger amount of training data, which at the present moment is absolutely not comparable to the size of an NLP dataset. For this reason, in the rest of the experiments we will concentrate on the TF. 



\iffalse  \begin{table}[t]
\begin{center}
\small
\caption{TrajNet Challenge results (world plane Human-Human TrajNet challenge, websites accessed on 26/07/2020). \blu{Blue italic indicates approaches proposed in this work.} \vspace{-0.2cm} \label{Tab:TrajNet}}
\resizebox{1\linewidth}{!}{
\begin{tabular}{cllllccl} \toprule
    \tbf{Rank} & \tbf{Method} & \tbf{Avg} & \tbf{FAD} & \tbf{MAD}  & \tbf{Context}& \tbf{Cit.}& \tbf{Year} \\ \midrule
2  & \blu{TF} & \blu{\tbf{0.776}} & \blu{\tbf{1.197}} & \blu{\tbf{0.356}}  & \blu{/} && \blu{2020}  \\
    
    3  & REDv3 & 0.781 & 1.201  & 0.360 & /&\cite{becker2018evaluation}& 2019  \\
4  & REDv2 & 0.783 & 1.207  & 0.359   & / &\cite{becker2018evaluation}& 2019 \\
6  & RED & 0.798 & 1.229  & 0.366   & / &\cite{becker2018evaluation}& 2018\\
    7  & SR-LSTM  & 0.816 & 1.261 & 0.37 & s &\cite{zhang2019sr}& 2019\\ 
9  & S.Forces (EWAP)  & 0.819  & 1.266 & 0.371& s &\cite{helbing1995social}& 1995 \\
12  & N-Lin. RNN-Enc-MLP &  0.827&  1.276&  0.377 & /&\cite{becker2018evaluation}& 2018\\
13  & N-Lin. RNN &  0.841&  1.300 & 0.381& /&\cite{becker2018evaluation}&  2018 \\
15  & Temp. ConvNet (TCN) & 0.841 &  1.301 & 0.381& /&\cite{bai2018empirical}&  2018 \\
16 & \blu{TF} &	\blu{0.858} & \blu{1.300} & \blu{0.416}&\blu{/}&&  \blu{2020}  \\
17  & N-Linear Seq2Seq &  0.860 & 1.331 & 0.390& /&\cite{becker2018evaluation}&  2018 \\
18  & MX-LSTM  & 0.887           & 1.374 & 0.399& s &\cite{Hasan18}& 2018\\
21  & Lin. RNN-Enc.-MLP & 0.892 & 1.381 & 0.404& /&\cite{becker2018evaluation}&  2018 \\
22  & Lin. Interpolation & 0.894 & 1.359 & 0.429& /&\cite{becker2018evaluation}&  2018 \\
24  & Lin. MLP (Off) & 0.896 & 1.384 & 0.407& /&\cite{becker2018evaluation}&  2018 \\
25 & \blu{BERT} &	\blu{0.897}	& \blu{1.354} &	\blu{0.440}& \blu{/}&\cite{BERT19}&  \blu{2020} \\
26 & \blu{BERT\_NLP\_pretrained} &	\blu{0.902} & \blu{1.357} & \blu{0.447}&\blu{/}&&  \blu{2020} \\
    27  & S.Forces (ATTR)  & 0.904  & 1.395 & 0.412& s &\cite{helbing1995social}& 1995 \\
29  & Lin. Seq2Seq & 0.923 & 1.429 & 0.418& /&\cite{becker2018evaluation}&  2018 \\
30  & Gated TCN & 0.947 & 1.468 & 0.426& /&\cite{bai2018empirical}&  2018 \\
31  & Lin. RNN & 0.951 & 1.482 & 0.420& /&\cite{becker2018evaluation}&  2018 \\
32  & Lin. MLP (Pos) & 1.041 & 1.592 & 0.491& /&\cite{becker2018evaluation}&  2018 \\
34 & LSTM &	1.140 & 1.793 & 0.491& /&\cite{LSTM_MATLAB} & 2018 \\
36  & S-GAN & 1.334  & 2.107 & 0.561& s &~\cite{gupta2018social}& 2018   \\
40  & Gauss. Process  & 1.642           & 1.038 & 2.245& /&\cite{trautman2010iros}&  2010 \\
42  & N-Linear MLP (Off) & 2.103 & 3.181 & 1.024& /&~\cite{becker2018evaluation}&  2018  \\
\bottomrule
\end{tabular}
}
\end{center}
\vspace{-0.5cm}

\end{table}

\fi


\begin{table}[t]
\begin{center}
\small
\caption{TrajNet Challenge results (world plane Human-Human TrajNet challenge, websites accessed on 26/07/2020). \blu{Blue italic indicates approaches proposed in this work.} \vspace{-0.2cm} \label{Tab:TrajNet}}
\resizebox{1\linewidth}{!}{
\begin{tabular}{cllllccl} \toprule
    \tbf{Rank} & \tbf{Method} & \tbf{Avg} & \tbf{FAD} & \tbf{MAD}  & \tbf{Context}& \tbf{Cit.}& \tbf{Year} \\ \midrule
2  & \blu{TF} & \blu{\tbf{0.776}} & \blu{\tbf{1.197}} & \blu{\tbf{0.356}}  & \blu{/} && \blu{2020}  \\
    
    3  & REDv3 & 0.781 & 1.201  & 0.360 & /&\cite{becker2018evaluation}& 2019  \\
4  & REDv2 & 0.783 & 1.207  & 0.359   & / &\cite{becker2018evaluation}& 2019 \\
6  & RED & 0.798 & 1.229  & 0.366   & / &\cite{becker2018evaluation}& 2018\\
    7  & SR-LSTM  & 0.816 & 1.261 & 0.37 & s &\cite{zhang2019sr}& 2019\\ 
9  & S.Forces (EWAP)  & 0.819  & 1.266 & 0.371& s &\cite{helbing1995social}& 1995 \\
12  & N-Lin. RNN-Enc-MLP &  0.827&  1.276&  0.377 & /&\cite{becker2018evaluation}& 2018\\
13  & N-Lin. RNN &  0.841&  1.300 & 0.381& /&\cite{becker2018evaluation}&  2018 \\
15  & Temp. ConvNet (TCN) & 0.841 &  1.301 & 0.381& /&\cite{bai2018empirical}&  2018 \\
16 & \blu{TF} &	\blu{0.858} & \blu{1.300} & \blu{0.416}&\blu{/}&&  \blu{2020}  \\
17  & N-Linear Seq2Seq &  0.860 & 1.331 & 0.390& /&\cite{becker2018evaluation}&  2018 \\
18  & MX-LSTM  & 0.887           & 1.374 & 0.399& s &\cite{Hasan18}& 2018\\
21  & Lin. RNN-Enc.-MLP & 0.892 & 1.381 & 0.404& /&\cite{becker2018evaluation}&  2018 \\
22  & Lin. Interpolation & 0.894 & 1.359 & 0.429& /&\cite{becker2018evaluation}&  2018 \\
24  & Lin. MLP (Off) & 0.896 & 1.384 & 0.407& /&\cite{becker2018evaluation}&  2018 \\
25 & \blu{BERT} &	\blu{0.897}	& \blu{1.354} &	\blu{0.440}& \blu{/}&\cite{BERT19}&  \blu{2020} \\
26 & \blu{BERT\_NLP\_pretrained} &	\blu{0.902} & \blu{1.357} & \blu{0.447}&\blu{/}&&  \blu{2020} \\
    27  & S.Forces (ATTR)  & 0.904  & 1.395 & 0.412& s &\cite{helbing1995social}& 1995 \\
29  & Lin. Seq2Seq & 0.923 & 1.429 & 0.418& /&\cite{becker2018evaluation}&  2018 \\
30  & Gated TCN & 0.947 & 1.468 & 0.426& /&\cite{bai2018empirical}&  2018 \\
31  & Lin. RNN & 0.951 & 1.482 & 0.420& /&\cite{becker2018evaluation}&  2018 \\
32  & Lin. MLP (Pos) & 1.041 & 1.592 & 0.491& /&\cite{becker2018evaluation}&  2018 \\
34 & LSTM &	1.140 & 1.793 & 0.491& /&\cite{LSTM_MATLAB} & 2018 \\
36  & S-GAN & 1.334  & 2.107 & 0.561& s &~\cite{gupta2018social}& 2018   \\
40  & Gauss. Process  & 1.642           & 1.038 & 2.245& /&\cite{trautman2010iros}&  2010 \\
42  & N-Linear MLP (Off) & 2.103 & 3.181 & 1.024& /&~\cite{becker2018evaluation}&  2018  \\
\bottomrule
\end{tabular}
}
\end{center}
\vspace{-0.5cm}

\end{table}



\subsection{The ETH+UCY Benchmark}\label{sec:ethucy}

Prior to TrajNet, most literature have benchmarked forecasting performance on a set of 5 datasets, namely the ETH-univ and ETH-hotel~\cite{pellegrini2009iccv} video sequences and the UCY-zara01, UCY-zara02 and UCY-univ~\cite{lerner2007crowds} videos.

\noindent\textbf{Datasets and metrics:} The ETH+UCY datasets consist overall of 5 videos taken from 4 different scenes (Zara1 and Zara2 are taken from the same camera but at a different time). Following the evaluation protocol of \cite{alahi2016cvpr} we sample from the data each 0.4 seconds to get the trajectories. We observe each pedestrian for 3.2 seconds (8 frames) and get ground-truth data for the next 4.8 seconds (12 frames) to evaluate the predictions. 
The pedestrian positions are converted to world coordinates in meters from the original pixel locations using homography matrices released by the authors.
The evaluation is done with a LOO approach training for 4 dataset and testing on the remaining one.
Recent works brought up some issues with the ETH+UCY dataset, \cite{scholler2020constant} showed that Hotel contains trajectory that go in a different direction than most of the ones in the other 4 dataset, so learning an environmental prior can be difficult without data augmentation like rotation; \cite{zhang2019sr} bring up the issue that ETH is an accelerated video and so by using a sampling rate of 0.4 seconds the trajectory behave in a different way than the ones in the other 4 datasets, they showed how by reducing the sampling rate they were able to improve their results.
We do not take any measure to fix these issues, in order to have a fair comparison against all the other methods that use these dataset using the standard protocol, but during our internal testing we noticed similar improvement when using their sampling rate for ETH.
Performance is evaluated using MAD and FAD, in meters.


\noindent\textbf{Results:} In Table~\ref{Tab:ETHUCY}, we compare on the ETH+UCY against the most recent and best performing approaches: S-GAN~\cite{gupta2018social}, Social-BIGAT~\cite{kosaraju2019social} and Trajectron++~\cite{salzmann2020trajectron++}. Additionally we include the ``individual'' version of S-GAN~\cite{gupta2018social}, which does not leverage the social information. Note in the Table the trend to include and model as much information as possible. The three leading techniques of S-GAN and Trajectron are in fact ``social'', and one of the best performing ones, Social-BIGAT, additionally ingests the semantic map of the environment (``+map''). Additionally, note that best results are obtained by sampling 20 multiple plausible futures and selecting the best one according to best test performance. We dub this here the best-of-20 protocol, which any technique in Table~\ref{Tab:ETHUCY} adopts.

The rightmost column in Table~\ref{Tab:ETHUCY} shows our proposed TF model, the only which allow to sample distributions of trajectories. 
TF achieves the second best performance, only 0.10 behind in terms of MAD and 0.10 in terms of FAD. 


Consistently with the TrajNet challenge, an individual forecasting TF technique yields a performance surprisingly ahead or comparable with the best social techniques, even if enclosing additional map information. And trend is also reflected by S-GAN~\cite{gupta2018social}, slightly under-performing its individual counterpart.\\

\begin{table}[t]
\begin{center}
\fontsize{9}{10}\selectfont
\caption{Comparison against SoA models following the best-of-20 protocol. The entirety of SoA approaches is rooted on LSTM, and leverages additional information (social, segmented maps). The mere quantized Transformer TF is superior to all the social approaches, second only to Trajectron++. Actually, only S-GAN-ind~\cite{gupta2018social} and TF have the same input and are directly comparable; all of the other performances are reported as reference, written in cursive.} 
\label{Tab:ETHUCY}
\vspace{-0.5cm}
\resizebox{1\linewidth}{!}{
\begin{tabular}{cccccc} \toprule
     &\multicolumn{4}{c}{LSTM-based}& TF-based \\
     \cmidrule(lr){2-5} \cmidrule(lr){6-6}
     & Individual &\multicolumn{2}{c}{Social} &Soc.+ map&Ind.\\
     \cmidrule(lr){2-2} \cmidrule(lr){3-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6}
    &  S-GAN-ind  & S-GAN      & Trajectron++  & Soc-BIGAT & TF \\
    &~\cite{gupta2018social}&~\cite{gupta2018social}&~\cite{salzmann2020trajectron++}&~\cite{kosaraju2019social}&\\
    \midrule
   ETH &0.81/1.52       & \emph{0.87/1.62}      &  \emph{0.35/0.77}   &\emph{0.69/1.29}& \tbf{0.61 / 1.12}                    \\
   Hotel             & 0.72/1.61       & \emph{0.67/1.37}       & \emph{0.18/0.38} & \emph{0.49/1.01} & \tbf{0.18 / 0.30}                     \\
UCY               & 0.60/1.26       & \emph{0.76/1.52}      & \emph{0.22/0.48}  & \emph{0.55/1.32} & \tbf{0.35 / 0.65}                     \\
Zara1              & 0.34/0.69       & \emph{0.35/0.68}       & \emph{0.14/0.28}& \emph{0.30/0.62}  & \tbf{0.22 / 0.38}                     \\
Zara2                & 0.42/0.84       & \emph{0.42/0.84}       & \emph{0.14/0.30}  & \emph{0.36/0.75}& \tbf{0.17 / 0.32}                     \\
\midrule
\emph{Avg}                  & 0.58/1.18       & \emph{0.61/1.21}   & \emph{0.21/0.45}   & \emph{0.48/1.00}   & \tbf{0.31 / 0.55}     \\               
\bottomrule
\end{tabular}
}
\end{center}
\vspace{-0.7cm}

\end{table}



Note that the best-of-20 protocol is a sort of upper-bound reachable by sampling-based approaches; therefore, we analyze the behavior of our Transformer-based predictors TF in the single-trajectory deterministic regime as in~\cite{salzmann2020trajectron++}, where each method gives a single prediction. Results are reported in Table~\ref{Tab:ETHUCYdet}.
\begin{table*}[t]
\begin{center}
\fontsize{8}{8}\selectfont
\caption{Comparison against SoA models following the single trajectory deterministic protocol (numbers of other approaches are taken from~\cite{salzmann2020trajectron++} ). Regular font indicates approaches which are comparable with our Transformer-based predictors, since they use a single individual observed trajectory as input. The other approaches have performance in italic, and are displayed as a reference.} 
\label{Tab:ETHUCYdet}
\resizebox{0.9\linewidth}{!}{
\begin{tabular}{cccccccc} \toprule
     &Linear&\multicolumn{5}{c}{LSTM-based}& {TF-based}\\
     \cmidrule(lr){2-2}\cmidrule(lr){3-7} \cmidrule(lr){8-8}
     &Individual & \multicolumn{2}{c}{Individual} &\multicolumn{2}{c}{Social} &Soc.+ map&Individual\\
      \cmidrule(lr){2-2}\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-7}\cmidrule(lr){8-8}
     &Interpolat.&LSTM&  S-GAN-ind  & Social      &  Soc.  & Trajectron++& Trasformer\\
     & & \cite{gupta2018social}&  \cite{gupta2018social}  & LSTM~\cite{gupta2018social}      &   Att.~\cite{kosaraju2019social} & \cite{salzmann2020trajectron++} & TF (ours)\\
     \midrule
  ETH &1.33/2.94       & 1.09/2.94      & 1.13/2.21   & \emph{1.09/2.35}&\emph{0.39/3.74}& \emph{0.50/1.19}& \textbf{1.03/2.10}  \\
  Hotel &0.39/0.72       & 0.86/1.91      & 1.01/2.18   & \emph{0.79/1.76}&\emph{0.29/2.64}& \emph{0.24/0.59}& \textbf{0.36/0.71}  \\
 UCY &0.82/1.59       & 0.61/1.31      & 0.60/\textbf{1.28}   & \emph{0.67/1.40}&\emph{0.20/0.52}& \emph{0.36/0.89}& \textbf{0.53}/1.32  \\
  Zara1 &0.62/1.21       & \textbf{0.41/0.88}      & 0.42/0.91   & \emph{0.47/1.00}&\emph{0.30/2.13}& \emph{0.29/0.72}& 0.44/1.00\\
  Zara2 &0.77/1.48       & 0.52/1.11      & 0.52/1.11   & \emph{0.56/1.17}&\emph{0.33/3.92}& \emph{0.27/0.67}& \textbf{0.34/0.76} \\
  \midrule
    avg &0.79/1.59       & 0.70/1.52      & 0.74/1.54   & \emph{0.72/1.54}&\emph{0.30/2.59}& \emph{0.34/0.84}& \textbf{0.54/1.17}  \\
\bottomrule
\end{tabular}
}
\end{center}
\vspace{-0.3cm}

\end{table*}


The message is clear: when it comes to individual approaches, the transformer predictor is better than any individual LSTM-based approach. Notably, TF is better than the Social-LSTM~\cite{gupta2018social}, and it outperforms the Social Attention~\cite{kosaraju2019social} in terms of FAD too, by a large margin. Notably, the only case in which LSTM compares favorably with TF is on Zara1, which is the less structured of the datasets of the benchmark, mostly containing straight lines.











\subsection{Ablation study and qualitative results}\label{sec:abl}

Here we conduct an ablation study on the proposed TF model for forecasting, compare it with the LSTM, and finally illustrate qualitative results.


\subsubsection{Changing the Prediction Lengths \label{Sec:var_length}}

As a first study case, we compare the stability of the TF and LSTM models when predicting longer temporal horizons. Unfortunately, TrajNet does not allow to set the prediction horizon. We set therefore to pursue a test-time experiment of models trained on the large and complex TrajNet on longer video dataset. We collect these from the 5 datasets of ETH+UCY, by selecting those datasets which are not part of the TrajNet training set, namely ETH and Zara01.
In Table~\ref{Tab:var_length}, we vary the observation sequence, from 12 frames (4.8s) to 32 frames (12.8) at a step of 1.8s. Both TF and LSTM have been trained one-dataset-out with training sequences of 8 samples and 12 for the prediction. 

\begin{table}[h] 
\begin{center}
\footnotesize
\caption{MAD and FAD errors when letting the  and the LSTM models predict longer horizons, i.e.\ from 12 to 32 time steps. Both models were trained on the TrajNet train set, while errors are reported over the union of ETH and Zara1 sequences (not part of the TrajNet train set).}
 \begin{tabular}{ccc}
  \toprule
    {Pred.} & {\textbf{TF (ours)}} & {LSTM~\cite{LSTM_MATLAB}}  \\ 
    {} & {MAD / FAD} & {MAD / FAD}\\\midrule
12      & \textbf{0.71/1.56} & 0.78/1.70 \\
16      & \textbf{0.95/2.15} & 1.15/2.72 \\
20      & \textbf{1.27/2.90} & 1.64/3.99 \\
24      & \textbf{1.66/3.76} & 2.29/5.55 \\
28      & \textbf{2.27/5.09} & 3.07/7.46 \\
32      & \textbf{2.98/4.52} & 4.13/9.96\\
\bottomrule \label{tab:horizon}
\end{tabular}
\end{center}
\label{Tab:var_length}\vspace{-0.5cm}

\end{table}
On Table~\ref{tab:horizon} are reported the average MAD and FAD values over the ETH-univ and UCY-zara1. Obviously, performances are generally decreasing. TF has a consistent advantage at every horizon Vs.\ LSTM and the decrease with the horizon of LSTM is approximately 25\% worse, as LSTM degrades from 0.78 to 4.13 MAD, while TF degrades from 0.71 to 2.98 MAD.
\begin{figure*}[!b]
\includegraphics[width= 1.0\linewidth]{./Figures/qualitative.pdf}
    \caption{Qualitative results: a) and b) showcasing failures of LSTM, c) and d) illustrating the trajectory distributions learned by TF. Best viewed in colors.}
\label{fig:trajnet-qualit}
\end{figure*}{}

\subsubsection{Missing and noisy data \label{Sec:missing}}
To the best of our knowledge, the problem of having missing coordinates in \emph{coordinate-based} long-term forecasting\footnote{\emph{Coordinate-based} forecasting takes as input floor coordinates of people, and is different to \emph{image-based} forecasting, where images are processed to extract bounding boxes locations on the image plane such as~\cite{kitani2012activity}.}  has been never taken into account. On the contrary, the problem of missing data is common in short term-forecasting (\emph{i.e.} tracking \cite{bae2017confidence}), or forecasting of heterogeneous data \cite{anava2015online,chen2001study,rodrigues2013spectral,ghazi2018robust,golyandina2007caterpillar}, where in general is treated by designing ad-hoc extensions for filling properly the missing entries (the so called \emph{hindsighting}~\cite{anava2015online}). 
Compared to these techniques, our transformer architecture represents a novel view, since \emph{it does not need to fill missing data}; instead, it exploits the remaining samples knowing when they have been observed thanks to the positional encoding. For example, supposing the th sample being missed in the observation sequence, the transformer will use the remaining , with  to perform the prediction of . This structural ability is absent in LSTM and RNN in general (they cannot work with missing data), and in this sense the Transformer is superior. If replacements of missing values can be computed, we found that simple linear interpolation gives slight improvements to the results.

Having witnessed the superiority of the transformer over LSTM in absolute sense (on TrajNet, and see Tab.~\ref{Tab:TrajNet}) and varying the forecasting horizons (Sec.~\ref{Sec:var_length}), we continue this analysis focusing on our proposed model on the same TrajNet dataset. The idea is to systematically drop one element at observation time, at a fixed position, from the most recent (time , indicated also as the \emph{current frame}, after that it starts the prediction) to the furthest ().
Results are reported in Tab.~\ref{Tab:MissingNew}.

\begin{table}[t]
\begin{center}
\small
\caption{Evaluation of missing data results for TF on TrajNet. We experiment dropping a varying number of most recent observed samples, either including or excluding the current frame. For example, in the case of dropping 3 frames, we drop  and   respectively.
}
\label{Tab:MissingNew}
\resizebox{1\linewidth}{!}{
\begin{tabular}{ccc} \toprule
    {\# most recent } & {Drop most recent obs.} & {Drop most recent obs.}\\ frames dropped & \emph{including} current frame & \emph{excluding}\ current frame\\ & ({FAD}/{MAD})& ({FAD}/{MAD})\\
    \midrule
0        & 1.197 / 0.356        & 1.197 / 0.356        \\
    \midrule
1        & 1.305/ 0.389         & 1.267 / 0.373        \\
2        & 1.409 / 0.429        & 1.29 / 0.38          \\
3        & 1.602 / 0.495        & 1.303 / 0.384        \\
4        & 1.787 / 0.557        & 1.313 / 0.387        \\
5        & 1.897/ 0.593         & 1.327 / 0.329        \\
6        & 2.128 / 0.669        & 1.377 / 0.406       \\
\bottomrule
\end{tabular}
}
\end{center}
\vspace{-0.5cm}

\end{table}
The results show that, in a complex scenario such as TrajNet, dropping input frames impact the prediction performance, matching the intuition: the more dropped frames, the larger the performance decrease. Interestingly, the current frame plays a key importance, as it is the most recent observed input, from which future predictions start. In fact, dropping the current frame together with the most recent 6 nearly doubles the error, i.e.\ degrades performance by 91\%, from 0.356 to 0.669 MAD. By contrast dropping 6 observed frames but keeping the current one only degrades the performance by 16\% (from 0.356 to 0.406 MAD), although the TF may now only leverage 2 observations (farther and closest in time).



\subsubsection{Qualitative results \label{Sec:qualitative}}
Qualitative results can further motivate the numerical results presented so far. In Fig.~\ref{fig:trajnet-qualit}, we report two predictions assessed on TrajNet, built by using the official visualizer of the benchmark. In particular, we artificially superpose the predicted trajectories of LSTM and TF to highlight their different behavior. In Fig.~\ref{fig:trajnet-qualit} a), the subject is going south, with a minimal acceleration (not immediately visible by the figure, but numerically present); LSTM takes this gentle acceleration, predicting a uniform acceleration toward south. TF captures better the dynamics, despite at the very end the final direction is not correct. 

In Fig.~\ref{fig:trajnet-qualit} b) a similar behavior caused LSTM to predict a faster straight trajectory, while TF followed in this case the bending of the GT more precisely.  

In general, we observed that LSTM generates trajectories way more regular than those predicted by TF, and this is certainly motivated by its unrolling, opposed to the encoder+decoder architecture of TF. This is also the reason why LSTM is so effective on Zara1, consisting essentially in straight trajectories, and so scarce on Hotel (and in general on TrajNet) if compared to TF. 

To further motivate this, in Fig.~\ref{fig:trajnet-qualit} c) and d), we show 100 sampled trajectories by TF on Zara1, for two different cases. Fig.~\ref{fig:trajnet-qualit} c)  presents essentially a monomodal distribution, with the samples concentrated around the GT, enriched by few articulated trajectories, that have low probability (they are few), but are still plausible.  Fig.~\ref{fig:trajnet-qualit} d) shows that TF has learnt a multimodal distribution, which has at least three modes, one turning north, the other going diagonal, the third (with larger number of trajectories) going east.































































































\iffalse
 \begin{figure*}[]
	\begin{center}
	    \includegraphics[width=1\linewidth]{./Figures/traj-s1.pdf}
\end{center}
\caption{Trajnet sample. Fabio please chck}\label{fig:prop-appro}
\end{figure*}


\begin{figure*}[h]
    \centering
    \includegraphics[width= \linewidth]{./Figures/zara1_56_zoom_2.png}
\caption{Example of five sampled prediction extracted using our Quantized-TF}
    \label{fig:Qual_zara1_56}
\end{figure*}{}

\begin{figure*}[h]
    \centering
    \includegraphics[width= 0.3\linewidth]{./Figures/zara1_56_zoom_2.png}
    \includegraphics[width= 0.3\linewidth]{./Figures/zara1_56_zoom_3.png}
    \includegraphics[width= 0.2\linewidth]{./Figures/zara1_56_zoom_3.png}
\caption{Example of five sampled prediction extracted using our Quantized-TF}
    \label{fig:Qual_zara1_56}
\end{figure*}{}
\fi










 \section{Conclusions}
We have proposed the use of Transformers Networks, based on attention mechanisms, to predict people future trajectories. The Transformers, state-of-the-art on all NLP tasks, also perform best on trajectory forecasting. We believe that this questions the widespread use of LSTMs for modelling people motion and that this questions the current formulation of complex social and environmental interactions, which our model does not need for best performance.

In addition to achieving the best performance on people forecasting datasets, the proposed Transfomers have shown better long-term prediction behavior, the capability to predict sensible multiple future trajectories and the unique feature of coping with missing input observations, as it may happen when dealing with real sensor data. Equipped with the better temporal models, we envisage potential to address even larger datasets of long-term sequences, where the importance of social terms may play more crucial roles.










 \section{Acknowledgments}
\vspace{-0.1cm}
This work is partially supported by the Italian MIUR through PRIN 2017 - Project Grant 20172BH297: I-MALL - improving the customer experience in stores by intelligent computer vision, and  by  the  project  of  the  Italian  Ministry  of  Education,  Universities  and  Research  (MIUR)  ”Dipartimenti  di  Eccellenza  2018-2022”. 









\bibliographystyle{IEEEtran}
\bibliography{main_arxiv}



\end{document}
