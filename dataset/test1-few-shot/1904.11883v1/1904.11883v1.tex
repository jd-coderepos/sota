\documentclass{article}

\PassOptionsToPackage{numbers, compress}{natbib}


\usepackage[preprint]{nips_2018}



\usepackage[T1]{fontenc}    \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{tabularx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{subfigure}

\newtheorem{theorem}{\textbf{Theorem}}
\newtheorem{proposition}{\textbf{Proposition}}
\newtheorem{lemma}[theorem]{\textbf{Lemma}}

\def\yt{{\tilde y}}
\def\Yt{{\tilde Y}}
\def\Xt{{\tilde X}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\mean}{mean}
\DeclareMathOperator*{\mat}{mat}
\DeclareMathOperator*{\diag}{diag}
\DeclareMathOperator*{\VEC}{vec}
\DeclareMathOperator*{\Tr}{Tr}
\DeclareMathOperator*{\T}{T}
\DeclareMathOperator*{\Orthogonality}{Orthogonality}
\DeclareMathOperator*{\Sparsity}{Sparsity}

\def\yt{{\tilde y}}
\def\xt{{\tilde x}}
\def\Xb{{\textbf{X}}}
\def\Zb{{\textbf{Z}}}
\def\Wb{{\textbf{W}}}
\def\Mb{{\textbf{M}}}
\def\Kb{{\textbf{K}}}
\def\Ib{{\textbf{I}}}

\title{Graph Optimized Convolutional Networks}


\author{
  Bo Jiang, Ziyan Zhang, Jin Tang and Bin Luo\\
  School of Computer Science and Technology, Anhui University\\
  \texttt{jiangbo@ahu.edu.cn} \\
}

\begin{document}

\maketitle

\begin{abstract}
Graph Convolutional Networks (GCNs) have been widely studied for graph data representation and learning tasks.
Existing GCNs generally  use a fixed single graph which may lead to weak suboptimal for data representation/learning and are also hard to deal with multiple graphs.
To address these issues, we propose a novel Graph Optimized Convolutional Network (GOCN) for graph data representation and learning.
Our GOCN is motivated based on our re-interpretation of graph convolution from a regularization/optimization framework.
The core idea of GOCN is to formulate  graph optimization and graph convolutional representation into a
unified framework and thus conducts both of them cooperatively to boost their respective performance in GCN  learning scheme.
Moreover, based on the proposed unified graph optimization-convolution framework, we propose  a novel Multiple Graph Optimized Convolutional Network (M-GOCN) to naturally address the data with multiple graphs. 
Experimental results  demonstrate the effectiveness and benefit of the proposed GOCN and M-GOCN.
\end{abstract}

\section{Introduction}


Convolutional Neural Networks (CNNs) have been widely applied for grid-like structure data representation and learning in computer vision and machine learning area. However, in many real applications, data are not coming with grid-like structure but instead have some irregular structures which are usually represented as structured graphs.
Traditional CNNs generally fail to  address graph-structured data. 
Recently, Graph Convolutional Networks (GCNs) have been widely studied  to deal with arbitrary graph-structured data representation and learning~\cite{defferrard2016convolutional,kipf2016semi,monti2017geometric,hamilton2017inductive,velickovic2017graph,DGI,CGNN}.
The aim of GCNs is trying to define some reasonable convolution operations  on arbitrary structured graphs.
For example,
Bruna et al.~\cite{bruna2014spectral} propose to define a  graph convolution by using the eigen-decomposition of graph Laplacian matrix.
Henaff et al.~\cite{henaff2015deep} further introduce a spatially constrained spectral filters to define graph convolution. Kipf et al.~\cite{kipf2016semi} propose to explore the first-order approximation of spectral filters and present a simple Graph Convolutional Network (GCN) for semi-supervised learning. Li et al.~\cite{velickovic2017graph} present an adaptive graph CNNs, in which the graph is learned adaptively by employing a metric learning method.
Hamilton et al.~\cite{hamilton2017inductive} present  a general inductive representation and learning framework to generate embeddings for the unseen nodes.
Velickovic et al.~\cite{velickovic2017graph} propose Graph Attention Networks (GAT) for graph based semi-supervised learning. Klicpera et al.~\cite{APPNN} propose to combine GCN and PageRank together to derive an improved propagation scheme in layer-wise propagation.
Some recent works also explore specific graph neural networks for computer vision tasks~\cite{GCN_CV3,GCN_CV1,GCN_CV2,te2018rgcnn}.







The above GCNs have been widely used for graph data representation and learning. One important aspect of GCNs is the graph structure representation of data. In general, the graph data we feed to existing GCNs should be a single graph which is obtained
from either domain knowledge ({e.g.}, social network) or human establishment, such as -NN graph.
However, there are three main issues.
First, traditional human established graphs generally use fixed parameters to determine the graph structure and thus are usually sensitive to the local noises and errors.
Second, the graphs obtained from domain knowledge or established by human are generally independent of graph convolutional learning, which thus are not guaranteed to be optimal for graph convolutional representation/learning in GCNs.
Third, existing GCNs are generally hard to deal with multiple graphs, although some heuristic fusion strategies can be utilized for multiple graph convolutional network learning~\cite{simonovsky2017dynamic,schlichtkrull2018modeling,zhuang2018dual}.
It is known that the convolution operation on multiple graphs is not as well-defined as on single graph.


To address these issues, in this paper, we propose a novel Graph Optimized Convolutional Network (GOCN) for data representation and learning problem.
Our GOCN is motivated based on our new re-interpretation of graph convolution from a regularization/optimization framework.
The core idea of GOCN is to formulate  graph optimization and graph convolutional representation into a
unified framework and thus conducts both of them cooperatively to boost their respective performance in GCN learning scheme.
The main advantage of GOCN is that the learned representation of  data
can provide useful ``weakly" supervised information for
learning a better graph which simultaneously facilitates graph convolutional representation and learning.
Furthermore, based on the proposed unified graph optimization-convolution framework, we propose  a novel Multiple Graph Optimized Convolutional Network (M-GOCN) to naturally address the data with multiple graphs.


Overall, the main contributions of this paper are summarized as follows:
\begin{itemize}
  \item  We propose to reformulate graph convolution learning as a regularization framework, based on which a unified graph optimization-convolution (GOC) framework is derived to learn an optimal graph for graph convolutional representation and learning.
 \item  Based on the proposed unified GOC model, we propose a novel Graph Optimized Convolutional Network (GOCN) which conducts both graph construction and graph  convolution simultaneously in GCN scheme for data representation and semi-supervised learning.
  \item We extend GOC to deal with multiple graphs and provide a novel Multiple Graph Optimized Convolutional Network (M-GOCN) for multi-graph data representation and learning.
\end{itemize}
Experimental results on several datasets demonstrate the effectiveness of the proposed GOCN and M-GOCN methods. 


\section{Revisiting GCN}
Recently, Graph Convolutional Networks (GCNs) have been widely studied for graph data representation and learning~\cite{defferrard2016convolutional,kipf2016semi,henaff2015deep,velickovic2017graph}.
The core aspect of GCNs is the specific definition of graph convolution in layer-wise propagation.
Here, we briefly review the widely used GCN model proposed in work~\cite{kipf2016semi}.
Given an input feature matrix  and graph  with , GCN defines the layer-wise propagation  as~\cite{kipf2016semi}, 
where  and  denotes an identity matrix, and  is a diagonal degree matrix.  is a layer-specific trainable weight matrix, and  denotes an activation function. 
The last layer of GCN outputs the final representation  of graph nodes, which can be used for many learning tasks, such as clustering, visualization and (semi-supervised) classification etc.
In this paper, we focus on semi-supervised classification.
For this task, a softmax activation function is further used to output the label prediction  for graph nodes, where  denotes class number. The weight matrices of GCN network  are optimized by minimizing the cross-entropy loss as~\cite{kipf2016semi},

where  indicates the set of labeled nodes and each row  of  denotes the corresponding label indication vector for the -th labeled node.



\section{Graph Optimized Convolutional Network}

In this section, we present our Graph Optimized Convolutional Network (GOCN) model.
GOCN is motivated based on our re-interpretation of GCN by using a regularization framework.
In the following, we first present our regularization reformulation of graph convolution in \S 3.1.
Based on it, we then derive a  unified framework of graph optimization-convolution operation in \S 3.2.
Finally, we present our GOCN architecture in \S 3.3.






\subsection{Regularization framework of GCN}


The propagation rule Eq.(\ref{EQ:layer_gcn0}) in GCN can be decomposed into two operations, i.e.,  
where Eq.(\ref{EQ:layer00}) defines a kind of \emph{feature aggregation} for features  on graph and Eq.(4) gives a non-linear feature transformation via projection  and non-linear activation  .
For simplicity, we rewrite Eq.(\ref{EQ:layer00}) as

where  and  (because ).
From Eq.(\ref{EQ:gcn_diffusion}), we can note that GCN employs an one-step feature aggregation on normalized graph  (biased by feature itself) to obtain contextual feature representation in layer-wise propagation.

First, here one can also explore a more flexible -step aggregation in GCN layer-wise propagation as

where   and . Parameter  denotes the fraction of feature information that node  receives from its neighbors on normalized graph .
Note that, when  and , the aggregation (Eq.(\ref{EQ:propagation_rwr})) degenerates to GCN update Eq.(\ref{EQ:gcn_diffusion}).
When , this aggregation converges to an equilibrium solution  as~\cite{LCL,LCL2},


Second, one important property of the update Eq.(\ref{EQ:propagation_rwr}) is that it can be theoretically explained by using an optimization framework~\cite{LCL}. Specifically, the converged solution of Eq.(\ref{EQ:propagation_rwr0}) is the optimal solution
that minimizes the following optimization problem,

where  is a replacement parameter of  to balance two terms.  denotes the trace function and  denotes the Frobenious norm.
Moreover, the update Eq.(\ref{EQ:propagation_rwr}) (or Eq.(7)) also provides an approximate solution to this problem by using a -step power iteration algorithm.

In particular, we can note that \emph{GCN update (Eq.(\ref{EQ:gcn_diffusion})) provides a very approximate solution for the problem  by using an one-step power algorithm.}
 This provides a regularization/optimizaton framework interpretation for GCN update rule, which motivates us to develop  some more effective graph convolution (feature aggregation) variants in GCN layer-wise propagation. In this paper, we focus on  graph construction  and aim to learn a more effective graph for GCN representation/learning.
 To do so, in the following we first derive a  unified framework of graph optimization and convolution, followed by a simple power iteration implementation. Based on this unified framework, we then develop our GOCN architecture in \S 3.3.





\subsection{Unified graph optimization-convolution model}

One important aspect of the above feature aggregation is the graph construction .
Constructing a good graph to represent data relationship is generally important for data representation and (semi-supervised) learning tasks.
Existing GCNs generally  use a fixed graph which may lead to weak suboptimal for data representation and learning. Our aim in this section is to propose a unified graph optimization-convolution model that aims to learn an optimal graph adaptively for feature aggregation in GCN  scheme.

Formally, given an initial (normalized) graph , we propose to learn an optimal graph  that best serves the above feature aggregation Eq.(\ref{EQ:propagation_opt}) while preserving the structure information encoded in .
This can be achieved by optimizing the following unified framework, 
where   denotes the graph learning functions and  is used to balance two terms.
In this paper, we use the simple Frobenious norm and propose a graph optimized feature aggregation as,

The optimal  and  can be obtained via a simple algorithm which alternatively conducts the following {step 1} and {step 2} until convergence.

\noindent \textbf{Step 1}. Solving  while fixing , the problem becomes

It has a simple closed-form solution which is given as 

\noindent \textbf{Step 2}. Solving  while fixing , the problem becomes
to Eq.(\ref{EQ:propagation_opt}). The optimal solution is

and an approximate solution can be obtained by


\noindent \textbf{Remark.} Here, to avoid the inversion computation in Eq.(\ref{EQ:solveZ}), we can instead use a -step power iteration and obtain Eq.(\ref{EQ:solveZ_app}) to compute the optimal  approximately~\cite{LCL}. 



\subsection{GOCN architecture}


Using the proposed unified model , we present our Graph Optimized Convolutional Network (GOCN).
Similar to the architecture of standard GCN~\cite{kipf2016semi}, our GOCN contains one input layer, several hidden propagation layers and one final perceptron layer, as introduced in the following.

\subsubsection{Hidden Propagation Layer}
For hidden propagation layer, it takes features  and an initial graph  as the input and
outputs feature map . Let  be the optimal -solution of unified model  (Eq.(\ref{EQ:propagation_unified0})), then GOCN conducts layer-wise propagation as

where  and  denotes the layer-specific trainable weight matrix.  denotes an activation function. 

 \textbf{Efficient computation of }.
 Exactly calculating  is time consuming due to (i) inversion operation (Eq.(\ref{EQ:solveZ})) and (ii) alternative computation of {step 1} and {step 2} until convergence.
To overcome this issue, we first use  update Eq.(\ref{EQ:solveZ_app}) to compute the optimal  in Eq.(\ref{EQ:solveZ}) approximately.
Second, we use a -step alternative iteration to optimize  and  approximately.
The detail propagation rule in GOCN hidden layer is summarized in Algorithm 1. In our experiments, we set  and .


\begin{algorithm}[h]
\caption{GOCN  layer-wise propagation}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Feature matrix , initial graph  and weight parameter , parameters ,  maximum iteration 
\STATE \textbf{Output:} Feature map 
\STATE Initialize 
\FOR {}
\STATE Compute  as\\

\STATE Compute  as\\

\ENDFOR \label{code:recentEnd}
\STATE Return

\emph{}
\end{algorithmic}
\end{algorithm}




\subsubsection{Final Perceptron Layer}

The last layer of GOCN outputs the final label prediction  for graph nodes, where  denotes the number of class.
Similar to work~\cite{kipf2016semi}, the optimal network weight parameters  of GOCN are obtained by minimizing the following cross-entropy loss function over all the labeled nodes , i.e.,

where  indicates the set of labeled nodes and  denotes the corresponding label indication vector for the -th labeled node. 



\section{GOCN on Multiple Graphs}

One desired property of the proposed GOCN is that it  can be naturally adapted to address  the data with multiple graph representations.
This is because we can extend the unified model  (Eqs.(\ref{EQ:propagation_unified00},\ref{EQ:propagation_unified0})) to deal with multiple graphs.

\subsection{Multi-graph convolution model}

Given an input feature matrix  with multiple graphs , we can obtain an optimal feature aggregation  on graph set  by optimizing,

In particular, we use Frobenious norm and propose our multiple graph optimized feature aggregation as

where 
denote the important weights of different graphs which are learned adaptively.
The parameter  is used to control the weight distribution, as
suggested in previous work~\cite{xia2010multiview}.
The optimal ,  and  can be obtained by alternatively conducting the following {Step 13} until convergence.

\noindent \textbf{Step 1}. Solving  while fixing , the problem becomes

It has a simple closed-form solution  as 

\noindent \textbf{Step 2}. Solving  while fixing , the problem becomes to Eq.(\ref{EQ:propagation_opt}).
The optimal solution is given as Eq.(\ref{EQ:solveZ}) exactly or Eq.(\ref{EQ:solveZ_app}) approximately.

\noindent \textbf{Step 3}. Solving  while fixing , the problem becomes
\label{EQ:solveZ0}
The Lagrangian function is

where   is the Lagrangian multiplier.
With some simple algebraic manipulations, the optimal  is derived as


\subsection{M-GOCN architecture}

Let  and  be the optimal solution of problem Eq.(\ref{EQ:propagation_unified_multi0}), then our Multiple GOCN (M-GOCN) conducts the layer-wise propagation as,

where  and  denotes the layer-specific trainable weight matrix.
 denotes an activation function, such as .


\noindent \textbf{Remark.} Similar to GOCN, here we can instead use a -step power iteration to compute the optimal  approximately to avoid the inversion computation.
The complete algorithm to compute  in M-GOCN hidden propagation is summarized in Algorithm 2. In our experiments, we set  and  for M-GOCN. 
\begin{algorithm}[h]
\caption{M-GOCN layer-wise propagation}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Feature matrix , initial multiple graphs  and weight parameter , parameters ,  maximum iteration 
\STATE \textbf{Output:} Feature map 
\STATE Initialize ; 
\FOR {}
\STATE Compute  as\\

\STATE Compute  as\\

\STATE Update  as\\

\ENDFOR \label{code:recentEnd}
\STATE Return

\emph{}
\end{algorithmic}
\end{algorithm}
The perceptron layer of M-GOCN  outputs the final label prediction  where  denotes the number of class, and the loss function is designed as a cross-entropy loss defined over labeled data which is the same as GOCN, as discussed in \S 3.3.2. 


\section{Experiments}

In order to evaluate the effectiveness of the proposed GOCN and M-GOCN, we test them on several datasets and compare them with some other baseline models. 
\subsection{Datasets}

In single graph learning experiments, we test GOCN on six datasets including three standard network datasets (Citeseer, Cora and Pubmed~\cite{sen2008collective}) and three image datasets (CIFAR10~\cite{krizhevsky2009learning}, SVHN~\cite{netzer2011reading} and Scene15~\cite{jiang2013label}). Their usages in our experiments are introduced below.

\noindent\textbf{Citeseer} contains 3327 nodes and 4732 edges whose nodes denote documents and edges encode the citation relationships between
documents.
Each node is represented by a 3703 dimension feature descriptor and all nodes are classified into six classes.
\\
\noindent\textbf{Cora} contains 2708 nodes and 5429 edges.
Each node is represented by a 1433 dimension feature descriptor and all the nodes are falling into six classes. \\
\noindent\textbf{Pubmed} contains 19717 nodes and 44338 edges.
Each node is represented by a 500 dimension feature descriptor and
all the nodes are falling into three classes.
\\
\noindent\textbf{Scene15} dataset consists of 4485 scene images with 15 different categories~\cite{jiang2013label}.
For each image, we use the feature descriptor provided by work~\cite{jiang2013label}.
\\
\noindent\textbf{CIFAR10} dataset  contains  50000 natural RGB color images and all images are falling into 10 classes~\cite{krizhevsky2009learning}.
In our experiments, we select 10000 images in all with 1000 images per class for
 evaluation.
We have not use all of images because large storage and high computational complexity are required for graph convolution operation in our GOCN and other comparing GCN based methods.
For each image, a CNN feature descriptor is extracted to represent it.
\\
\noindent\textbf{SVHN} dataset contains 73257 training and 26032 test RGB images~\cite{netzer2011reading}.
Similarly, we select 10000 images in all with 1000 images for each class in our evaluation.
For each image, a CNN feature descriptor is extracted.

In our multi-graph learning experiments, we evaluate M-GOCN on three datasets including MSRC-v1~\cite{winn2005locus}, Caltech101-7~\cite{li2015large,nie2016parameter} and Handwritten numerals~\cite{AsuncionNewman2007}.
Their usages in our experiments are introduced below.
\\
\noindent\textbf{MSRC-v1}~\cite{winn2005locus} contains 8 classes of 240 images.
Each class contains 30 images.
Similar to the setup in work~\cite{nie2016parameter},
we obtain five graphs by using five different kinds of visual descriptors and
the final input feature of each graph node is constructed by  concatenating different descriptors together. \\
\noindent\textbf{Caltech101-7}~\cite{li2015large} contains 101 categories of images. Following the setup of work~\cite{nie2017multi}, we select the widely used 7 classes  and obtain 1474 images in all in our experiments.
We construct six graphs  by using six different feature descriptors and obtain the input feature of each graph node by concatenating these
descriptors together~\cite{nie2017multi}.
\\
\noindent\textbf{Handwritten numerals}~\cite{AsuncionNewman2007} contains 2000 digits from `0' to `9' and each  digit class has 200 data samples.
We construct six graphs by using six published feature descriptors and obtain the final input feature of each graph node by concatenating them together.



\subsection{Experimental setup}

\subsubsection{Parameter setting}
Similar to experimental setting of GCN~\cite{kipf2016semi}, we use a two-layer graph convolutional network and the number of units in hidden layer is set to 16. We train our GOCN using an ADAM algorithm~\cite{Adam}  with 10000 maximum epochs and learning rate of 0.01.
We stop training if the validation loss does not decrease for 100 consecutive
epochs, as suggested in \cite{kipf2016semi}.
All the network weights are initialized using Glorot initialization~\cite{glorot2010understanding}.
The parameters  are set to 0.9 and 20 respectively. Note that, GOCN is not insensitive to these parameters. We provide additional experiments across different settings of ,   and hidden layer number in \S 5.4.


\subsubsection{Data setting}

For network datasets (Citeseer, Cora and Pubmed), we utilize the similar data setting used in previous works~\cite{kipf2016semi,velickovic2017graph}. For each class, we select 20 nodes as labeled data and 300 nodes as validation data, and then evaluate the performance of label prediction on the remaining 1000 test nodes.
For image dataset CIFAR10 and SVHN,
we randomly select 1000, 2000 and 3000 images as labeled samples and use the
remaining data as unlabeled samples.
For unlabeled samples, we select 1000 images for validation purpose and
use the remaining 8000, 7000 and 6000 images as test samples, respectively.
For image dataset Scene15~\cite{jiang2013label}, we randomly select 500, 750 and 1000 images as labeled data and select 500 images for validation. The remaining images are used for testing.
All the reported results are averaged over five runs  with different  data splits of training, validation and testing.
In our multi-graph learning experiments, for all datasets (MSRC-v1, Caltech101-7 and Handwritten numerals), we select 10\%, 20\% and 30\% nodes as labeled samples and use the remaining data as unlabeled samples.
For unlabeled samples, we also use 5\% nodes for validation purpose to determine the convergence criterion, and use the remaining 85\%, 75\% and 65\% nodes respectively as test samples.
All the reported results are averaged over five runs  with different data splits of training, validation and testing.

\begin{table}[!htp]
\centering
\caption{Comparison results of different methods on  dataset Citeseer, Cora and Pubmed. The best results are marked by bold.}
\centering
\begin{tabular}{c|c|c|c}
  \hline
  \hline
Methond & Citeseer & Cora & Pubmed\\
  \hline
  ManiReg & 60.1\% & 59.5\% & 70.7\%\\
  LP & 59.6\% & 59.0\% & 71.1\%\\
  DeepWalk & 43.2\% &67.2\% & 65.3\%\\
  DGI &71.5\% & 76.8\% & 77.2\%\\
  GCN & 68.9\% & 82.9\% & 77.9\%\\
  GAT & 71.0\% & 83.2\% & 78.0\%\\
  \hline
  GOCN & \textbf{71.8\%} & \textbf{84.8\%} & \textbf{79.7\%}\\
  \hline
  \hline
\end{tabular}
\end{table}
\begin{table}
\centering
\caption{\upshape Comparison results of different methods on Scene15 dataset. The best results are marked by bold.}
\begin{tabular}{c||c|c|c}
  \hline
  \hline
   Dataset& \multicolumn{ 3}{c}{Scene15}\\
  \hline
  No. of label & 500 & 750 & 1000 \\
  \hline
   ManiReg  & 81.293.35 & 86.451.91 & 89.860.71\\
   LP        & 89.404.74 & 92.122.87 & 92.982.45\\
   Deep Walk & 95.640.24 & 96.010.24 & 96.530.37\\
   DGI      & 92.941.61 & 94.210.64 & 94.270.94\\
   GCN      & 91.422.07 & 94.410.92 & 95.440.89\\
   GAT      &  93.980.75 & 94.640.41 & 95.030.46\\
   \hline
   GOCN     & \textbf{95.870.56} & \textbf{97.400.34} & \textbf{98.000.29}\\
  \hline
  \hline
\end{tabular}
\end{table}
\begin{table*}\small
\centering
\caption{\upshape Comparison results of different methods on dataset SVHN and CIFAR10. The best results are marked by bold.}
\begin{tabular}{c||c|c|c||c|c|c}
  \hline
  \hline
Dataset& \multicolumn{ 3}{c||}{SVHN} & \multicolumn{ 3}{c}{CIFAR10}\\
  \hline
  No. of label & 1000 & 2000 & 3000 & 1000 & 2000 & 3000 \\
  \hline
   ManiReg  & 69.440.69 & 72.730.44 & 74.630.45 & 52.300.66 & 57.080.80 & 59.690.71\\
   LP       & 69.680.84 & 70.351.73 & 69.472.96 & 57.520.67 & 59.220.67 & 60.380.51\\
   Deep Walk& 74.640.23 & 76.210.23 & 77.040.42 & 56.160.54 & 59.730.35 & 61.260.32\\
   DGI      & 70.821.22 & 72.830.79 & 73.161.20 & 58.970.61 & 60.260.56 & 60.560.36\\
   GCN      & 71.331.48 & 73.430.46 & 73.630.52 & 60.430.56 & 60.910.50 & 60.990.49\\
   GAT      & 73.870.32 & 74.850.55 & 75.170.43 & 63.250.50 & 65.550.58 & 66.560.58\\
   \hline
   GOCN     & \textbf{80.720.35} & \textbf{82.670.25} & \textbf{83.630.37} & \textbf{68.130.58} & \textbf{71.830.37} & \textbf{73.660.52}\\
  \hline
  \hline
\end{tabular}
\end{table*}
\subsection{Comparison with state-of-the-art methods}

\subsubsection{Evaluation on single graph}
We first compare our GOCN with the baseline model GCN~\cite{kipf2016semi} to demonstrate the benefit of graph optimization.
Also, we compare our method against some other  graph neural network based semi-supervised learning methods which contain two traditional graph based
semi-supervised learning methods including  Label Propagation (LP)~\cite{zhu2003semi}, Manifold Regularization (ManiReg)~\cite{belkin2006manifold},
and four graph neural network methods including DeepWalk~\cite{perozzi2014deepwalk}, Graph Convolutional Network (GCN)~\cite{kipf2016semi},
Graph Attention Networks (GAT)~\cite{velickovic2017graph} and Deep Graph Informax (DGI)~\cite{DGI}. Table 1 shows the comparison results on three citation network benchmark datasets. Table 2 and 3 summarize the comparison results on three image datasets. The best results are marked as bold.
We can note that
(1) Comparing with the baseline model GCN~\cite{kipf2016semi}, GOCN obtains obviously better learning results on all datasets, especially on image datasets.
This  demonstrates the higher predictive ability  of GOCN on semi-supervised classification  by incorporating graph optimization, which indicates that GOCN conducts data representation and semi-supervised learning more optimal than GCN.
(2) GOCN performs better than recent graph network GAT ~\cite{velickovic2017graph} and DGI~\cite{DGI}, which demonstrates the benefit of GOCN on  data representation and learning.
(3) GOCN generally performs better than other graph based semi-supervised method LP~\cite{zhu2003semi}, ManiReg~\cite{belkin2006manifold}
and DeepWalk~\cite{perozzi2014deepwalk}, which further indicates the effectiveness of GOCN on conducting semi-supervised classification tasks.
\begin{table*}[htbp]\small
\centering
\caption{\upshape Comparison results of different multi-graph learning methods on dataset Caltech101-7, MSRC-v1 and Handwritten numerals, respectively. The best results are marked by bold.}
\begin{tabular}{c||c|c||c|c||c|c}
  \hline
  \hline
Dataset& \multicolumn{ 2}{c||}{Caltech101-7} & \multicolumn{ 2}{c||}{MSRC-v1} & \multicolumn{ 2}{c}{Handwritten numerals}\\
  \hline
  Ratio of label & 10\% & 20\%  & 10\% & 20\%  & 10\% & 20\% \\
  \hline
  GCN(1)   & 82.620.78 & 84.711.38  & 62.086.74 & 66.215.42 & 90.920.22 & 91.330.45\\
  GCN(2)  & 85.001.90 & 86.341.73  & 81.971.61 & 85.842.90 & 94.720.22 & 95.390.77\\
  GCN(3)   & 86.991.42 & 88.370.82  & 84.943.36 & 88.071.54 & 95.590.61 & 96.160.57\\
  GCN(4)   & 93.180.98 & 93.440.55  & 77.473.20 & 83.101.54 & 95.780.31 & 96.450.68\\
  GCN(5)   & 92.350.62 & 92.490.47  & 80.002.60 & 84.723.46 & 88.530.53 & 89.170.63\\
  GCN(6)   & 92.050.78 & 92.450.86  & - & - & 82.340.67 & 83.110.25\\
  GCN-M    & 90.392.02 & 91.910.33  & 86.813.60 & 90.311.87 & 96.420.47 & 97.080.48\\
  Multi-GCN   & 95.090.62 & 96.080.58  & 87.142.13 & 90.431.45 & 97.140.23 & 97.880.30\\
  MLAN  & 93.450.36 & 94.860.29  & 83.422.10 & 87.271.66 & 97.230.26 & 97.460.52\\
  \hline
  M-GOCN  & \textbf{95.970.36} & \textbf{97.230.47}  & \textbf{90.221.06} & \textbf{91.681.50}& \textbf{97.760.42} & \textbf{97.920.41} \\
  \hline
  \hline
\end{tabular}
\end{table*}
\subsubsection{Evaluation on multiple graphs}

For multi-graph learning tasks, we compare our M-GOCN against the state-of-the art baseline methods which contain
{GCN(v)} that conducts traditional GCN~\cite{kipf2016semi} on the -th singe graph  and node content features ;
{GCN-M} that conducts traditional GCN~\cite{kipf2016semi} on the averaged graph representation  and node features ;
{Multi-GCN} that first learns representations for multiple graphs  by using/sharing the common parameters (as suggested in work~\cite{duvenaud2015convolutional}), and then select the final representation with the lowest training loss function for multi-graph representation;
{MLAN}~\cite{nie2017multi} that is a multi-view learning model for graph learning and semi-supervised classification.
Table 4 summarizes the comparison results of semi-supervised classification on these datasets.
Here, we can note that
(1) The proposed M-GOCN performs obviously better than traditional GCN~\cite{kipf2016semi} model conduced on each individual graph . This clearly demonstrates the effectiveness of the proposed M-GOCN on learning a compact representation for multiple graphs by intergrading the information of multiple graphs together.
(2) M-GOCN performs better than the baseline method GCN-M, Multi-GCN~\cite{duvenaud2015convolutional} and {MLAN}~\cite{nie2017multi}, which indicates the effectiveness of M-GOCN architecture on conduct multiple graph representation and semi-supervised learning tasks.


\section{Conclusion and Future Works}

This paper proposes a novel Graph Optimized Convolutional Network (GOCN) for graph data representation and semi-supervised learning.
GOCN is inspired based on the re-formulation of graph convolution as a regularization framework.
GOCN integrates graph optimization and convolution in a unified scheme and thus can boost their respectively performance in graph neural network learning.
Also, GOCN can  be naturally extended to M-GOCN to deal with multiple graphs.
Experimental results on several benchmarks demonstrate
the effectiveness and benefits of the proposed GOCN and M-GOCN on semi-supervised learning tasks.
In the future, we will explore GOCN and M-GOCN methods on some other learning tasks, such as graph clustering, embedding etc.
 Also, we will adapt them on some more computer vision tasks, such as object detection, image co-segmentation and multiple object tracking etc. 

{\small
\bibliographystyle{ieee}
\bibliography{nmfgm-GOCN}
}



\end{document}
