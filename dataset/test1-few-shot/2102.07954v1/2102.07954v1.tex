\documentclass{article} 



\usepackage[preprint]{icml2019}

\usepackage{times}
\usepackage{multirow}
\usepackage{url}
\usepackage{qiangstyle}
\usepackage{mathrsfs}
\usepackage{amsmath}

\usepackage{multirow}
\usepackage{booktabs}

\usepackage{lipsum}  

\icmltitlerunning{AlphaNet: Improved Training of Supernet with Alpha-Divergence}

\renewcommand{\tt}{{p}}
\renewcommand{\ss}{{q}}
\newcommand{\amin}{\alpha_{-}}
\newcommand{\amax}{\alpha_{+}}
\renewcommand{\KD}{\mathrm{KD}}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\qq}[1]{{\small\red{[QL:~#1]}}}
\newcommand{\dw}[1]{{\small\red{[DW:~#1]}}}
\newcommand{\erm}{\mathrm{ERM}}
\renewcommand{\D}{\mathrm{D}}

\begin{document}

\twocolumn[
\icmltitle{AlphaNet: Improved Training of Supernet with Alpha-Divergence}









\begin{icmlauthorlist}
\icmlauthor{Dilin Wang}{fb}
\icmlauthor{Chengyue Gong*}{austin}
\icmlauthor{Meng Li*}{fb}
\icmlauthor{Qiang Liu}{austin}
\icmlauthor{Vikas Chandra}{fb}
\end{icmlauthorlist}

\icmlaffiliation{austin}{Department of Computer Science, The University of Texas at Austin}
\icmlaffiliation{fb}{Facebook}

\icmlcorrespondingauthor{Dilin Wang}{wdilin@fb.com}
\vskip 0.3in
]

\printAffiliationsAndNotice{\icmlEqualContribution}

\begin{abstract}

Weight-sharing neural architecture search (NAS) is an effective technique for automating efficient neural architecture design. 
Weight-sharing NAS builds a supernet that assembles all the architectures as its sub-networks and jointly trains the supernet with the sub-networks. The success of weight-sharing NAS heavily relies on distilling the knowledge of the supernet to the sub-networks. However, we find that the widely used distillation divergence, i.e., KL divergence, may lead to student sub-networks  that over-estimate or under-estimate the uncertainty of the teacher supernet, leading to inferior performance of the sub-networks. In this work, we propose to improve the supernet training with a more generalized -divergence.
By adaptively selecting the -divergence,
we simultaneously prevent the over-estimation or under-estimation of the uncertainty of the teacher model.  
We apply the proposed -divergence based supernet training to both slimmable neural networks and weight-sharing NAS, and demonstrate significant improvements. Specifically, our discovered model family, AlphaNet, outperforms prior-art models on a wide range of FLOPs regimes, including BigNAS, Once-for-All networks, FBNetV3, and AttentiveNAS. We achieve ImageNet top-1 accuracy of 80.0\% with only 444 MFLOPs.

\end{abstract}

\section{Introduction}

Designing accurate and computationally efficient neural network architectures is an important but challenging task. Neural architecture search (NAS) automates the neural network design by exploring an enormous architecture space and achieves state-of-the-art (SOTA) performance on various applications including image classification~\citep{zoph2016neural, zoph2018learning}, object detection~\citep{ghiasi2019fpn}, semantic segmentation~\citep{zhang2019customizable} 
and natural language processing \citep{wang2020hat}. 

Conventional NAS approaches can be prohibitively expensive as hundreds of candidate architectures need to be trained from scratch and evaluated~\citep[e.g.,][]{tan2019mnasnet, zoph2018learning}. Supernet based approach has recently emerged to be a promising approach for efficient NAS. A supernet assembles all candidate architectures into a weight sharing network with each architecture corresponding to one sub-network. By training the sub-networks simultaneously with the supernet, different architectures can directly inherit the weights from the supernet for evaluation and deployment, which eliminates the huge cost of training or fine-tuning each architecture individually.

Though promising, supernet training is highly challenging in order to ensure that all the sub-networks are well trained \citep[e.g.,][]{yu2020bignas, cai2019once, wang2020attentivenas}. To stabilize the supernet training and improve the performance of sub-networks, one widely used approach is in-place knowledge distillation (KD) \citep{yu2019universally}. Inplace KD leverages the soft labels predicted by the largest architecture in the supernet to supervise all the other sub-networks. By distilling the knowledge of the teacher model, the performance of the sub-networks can be improved significantly \citep{yu2019universally, yu2020bignas}. 

Standard knowledge distillation uses KL divergence to measure the discrepancy between the teacher and student networks. However, KL divergence penalizes the student model much more when it fails to cover one or more local modes of the teacher model~\citep{murphy2012machine}. Hence, the student model tends to over-estimate the uncertainty of the teacher model and suffers from inaccurate approximation of the most important mode, i.e., the correct prediction of the teacher model. 


To further enhance the supernet training, we propose to replace the KL divergence with a more generalized -divergence~\citep{amari1985differential, minka2005divergence}. 
Specifically, we show that by adaptively controlling  in the proposed divergence metric, we can penalize both the under-estimation and over-estimation of the teacher model uncertainty to encourage a more accurate approximation for the student models. 
While directly optimizing the proposed adaptive -divergence may suffer from a high variance of the gradients, we further propose a simple technique to clip the gradients of our adaptive -divergence to stabilize the training process.
We show the clipped gradients still define a valid divergence metric implicitly and hence, yielding a proper optimization objective for KD. 

We empirically verify the proposed adaptive -divergence in two notable applications of supernet - slimmable networks \citep{yu2019universally, yu2018slimmable} and weight-sharing NAS \citep{yu2020bignas, wang2020attentivenas} on ImageNet. For weight-sharing NAS, we train a supernet containing both small (200M FLOPs) and large (2G FLOPs) sub-networks following \citet{wang2020attentivenas}. With the proposed adaptive -divergence, we are able to train high-quality sub-networks, called AlphaNet, that surpass all prior state-of-the-art models in the range of 200 to 800 MFLOPs, like EfficientNets \citep{tan2019efficientnet}, OFANets~\citep{cai2019once}, FBNetV3~\citep{dai2020fbnetv3}, and BigNas~\citep{yu2020bignas}. 
Specifically, AlphaNet-A4 achieves 80.0\% accuracy with only 444 MFLOPs.



 
\section{Background}


Training high-quality supernet is fundamental for weight-sharing NAS but non-trivial~\citep{benyahia2019overcoming}. 
Recently, in-place KD is shown to be an effective mechanism that significantly improves the supernet performance \citep{yu2019universally, yu2020bignas}. 

To formalize the supernet training and in-place KD, consider a supernet with trainable parameter .
Let  denote the collection of all sub-networks contained in the supernet. 
The goal of training a supernet is to learn  such that all the sub-networks in  can be optimized simultaneously to achieve good accuracy.

The supernet training process with the in-place KD is illustrated in Figure~\ref{fig:supernet_kd}. At each training step, given a mini-batch of data, the supernet as well as several sub-networks are sampled. While the supernet is trained with the real labels, all the sampled sub-networks are supervised with the soft labels predicted by the supernet. Then, the gradients from all the sampled networks are aggregated before the supernet parameters are updated. More formally, at the training step , the supernet parameters  are updated by

where  is the step size, and

Here,  is the standard cross entropy loss of the supernet on a training dataset ,  is the weight coefficient, and  is the KD loss for distilling the supernet into a randomly sampled sub-network , for which KL divergence has been widely used~\citep[e.g.,][]{yu2020bignas}.

Let  and  denote the output probability of the supernet and the sub-network  given input , then, we have

where . Note that the gradient on  in the KD loss is stopped as~\eqref{eq:kdloss} indicated. For notation simplicity, 
we denote  as our teacher model and  (or ) as student models in the following. 

Additionally, note that the way KD is used in supernet training is different from the standard settings such as \citet[e.g.,][]{hinton2015distilling}, where the teacher network is pre-trained and fixed.



\iffalse
Given the weight-sharing parameter  and the architecture space , we denote  as the parameter for a sub-network  and define  and  as the largest and smallest sub-network in . Mark the training set as ,
the training objective is

\qq{does the algorithm actually optimize a single object, or it is actually an iterative algorithm that does not optimize any object? Otherwise there will be stop gradient issues on the KD loss.}
\qq{ is strange. Write  where  is a fixed sampling distribution on ?}
where  is a coefficient,  is a uniform sampler for all candidate  in ,  is the output of sub-network ,  denotes supervised loss while  denotes the knowledge distillation loss.
Here,  is trained with  and under the supervision of the label  from .
The other architectures  (including ) is trained with , the soft label provided by the biggest sub-network . 
Overall, the objective contains two parts.
The first part is the expectation of the KD loss over all the architecture . The second term is called as sandwich rule, which introduces the supervised loss for the biggest and KD loss for smallest sub-network.
In practice, equation \eqref{eq:loss} is optimized under a stochastic version with gradient descent, where random samples of  and  is drawn for each gradient descent step.
\fi




\begin{figure}[t]
\centering
\begin{tabular}{c}
\includegraphics[width=0.44\textwidth]{figures/toy/kd.pdf} \\
\end{tabular}
\caption{An illustration of training supernet with KD. Sub-networks are part of the supernet with weight-sharing. }
\label{fig:supernet_kd}
\end{figure}
 
\section{Supernet training with -divergence}

In this section, we analyze the limitations of using KL divergence in KD and propose to replace KL divergence with a more generalized -divergence. We study the impact of different choices of  in the proposed divergence metric and further propose an adaptive algorithm to select  during the supernet training. Meanwhile, we also show that while directly optimizing -divergence is challenging due to large gradient variances, a simple clipping strategy on -divergence can be very effective to stabilize the training.


\begin{figure*}[t]
\centering
\setlength{\tabcolsep}{6pt}
\begin{tabular}{ccc}
\raisebox{2.5em}{\rotatebox{90}{\small Prediction}}
\includegraphics[height=0.19\textwidth]{figures/toy/case0.pdf} & 
\raisebox{2.5em}{\rotatebox{90}{\small Prediction}}
\includegraphics[height=0.19\textwidth]{figures/toy/case1.pdf} & 
\raisebox{2.0em}{\rotatebox{90}{\small -divergence}}
\includegraphics[height=0.19\textwidth]{figures/toy/alpha_discrete.pdf} \\ 
\small (a) Example 1  \emph{(under-estimation)}  & \small  (b) Example 2 \emph{(over-estimation)} &\small  (c) choices of  \\
\end{tabular}
\caption{
(a) \emph{Example 1 - uncertainty under-estimation}.
The student network under-estimates the uncertainty of the teacher model and misses important local modes of the teacher model. 
 (b) \emph{Example 2 - Uncertainty over-estimation}. In this case, 
the student network over-estimates the uncertainty of the teacher model and mis-classify the most dominant mode of the teacher model. (c) plots the corresponding -divergences between the student model and the teacher model for \emph{Examples 1} and \emph{2}. Note that  is a special case of  divergence.}
\label{fig:overview_alpha}
\end{figure*}


\subsection{Classic KL based KD and its limitations}
\label{sec:alpha_div}

KL divergence has been widely used to measure the discrepancy in output probabilities between the teacher and student models in KD. 
One main drawback with KL divergence is that it cannot sufficiently penalize the student model when it over-estimates the uncertainty of the teach model.
Let  and  denote the output probability of the teacher and student models, respectively. The KL divergence between the teacher and student models is calculated by . When , to ensure  remains finite, we must have . This is the \emph{zero avoiding} property of . In contrast, when ,  does not get penalized. 
For example, as shown in Figure \ref{fig:overview_alpha} (b) and (c), even though the student model over-estimates the uncertainty of the teacher model and predicts the wrong class ("class 4"), the KL divergence is still small.


The aforementioned \emph{over-estimation} in Example 2 would be penalized
at a larger magnitude when using other types of divergences, e.g., reverse KL divergence . 
For reverse KL divergence, 
 is infinite if  and . 
Hence if  we must ensure , this is known as the \emph{zero forcing} property~\citep{murphy2012machine}. 
Therefore, minimizing reverse KL divergence encourages the student model  to avoid low probability modes of  while focusing on the modes with high probabilities, and thus, may \emph{under-estimate} the uncertainty of the teacher model, as shown in Example 1 in Figure~\ref{fig:overview_alpha}. 

Hence, a natural question is whether it is possible to generalize the KL divergence to simultaneously suppress both the under-estimation and over-estimation of the teacher model uncertainty during the supernet training.

\subsection{KD with adaptive -divergence}
Our observations shown in Figure~\ref{fig:overview_alpha} motivate us to design a new KD objective that 
simultaneously penalize both over-estimation and under-estimation of the teacher model uncertainty. 
We first generalize the typical  divergence with a more flexible -divergence~\citep{minka2005divergence}. Consider , 
the -divergence is defined as 

where  and  are two discrete distributions on  categories. 
The -divergence includes a large spectrum of classic divergence measures. In particular, 
 the KL divergence    is the limit of  with  while the reverse KL divergence  is the limit of  with .

A key feature of -divergence is that we can decide to focus on penalizing different types of discrepancies (under-estimation or over-estimation) by choosing different  values. 
For example, 
as shown in Figure \ref{fig:overview_alpha} (c), 
when  is negative, 
 is large when  is more widely spread than 
(when  \emph{over-estimates} the uncertainty in ), 
and is small when  is more concentrated than  
(when  \emph{under-estimates} the uncertainty in ). 
The trend is opposite when  is positive: 
under-estimation would be more heavily penalized than over-estimation. 



To simultaneously alleviate the over-estimation and under-estimation problem when training the supernet, we consider a positive  and a negative , and propose to
use the maximum of 
and  in the KD loss function: 


Our KL loss now changes from  Eq~\eqref{eq:kdloss} to 

We denote this KD strategy that always chooses the maximum of  and  to optimize as \emph{Adaptive-KD}. 



\subsection{Stabilizing -divergence KD}
\label{sec:stablize}


One would prefer to set both  and  to be large to ensure the student model is sufficiently penalized when it either under-estimates or over-estimates the uncertainty the teacher model.
However, 
directly optimizing the -divergence with large  is often challenging in practice. 
Consider the gradient of -divergence:

If  is large, then the powered term  can be quite significant and cause the training process to be unstable.
To enhance the training stability, we clamp the maximum value of  to be , and obtain 

where
 . 


Eqn.~\eqref{eq:grad_stablize_alpha_div} is a simple yet effective heuristic approximation of . It is important to note that Eqn.~\eqref{eq:grad_stablize_alpha_div} equals the \emph{exact} gradient of a special  divergence between  and . Hence, our updates still amount to minimizing a valid divergence. 
Note that the clipping function  is only partially differentiable. 
So naively clipping on  in Eqn.~\eqref{equ:alpha_divergence} may stop gradients back-propagating from the density ratio terms, 
hence yielding gradients that are not from a valid divergence. 

To show that we are still optimizing a valid divergence with Eqn.~\eqref{eq:grad_stablize_alpha_div}, note that, for a convex function , 
the -divergence between  and  is defined as 

Its gradient w.r.t.  is 

where  (\citet{wang2018variational}). 
Note that -divergence is a special case of -divergence when 
. 

\begin{pro}
There exists a convex function , such that 
 in~\eqref{eq:grad_stablize_alpha_div} is the exact gradient of  that is, 
. 
\end{pro}
\begin{proof}
Let . 
We just need to find an  such that 

Taking derivation on both sides, we get 

This gives  
and hence ,
where   denotes second-order antiderivative (or indefinite integral). 
Because  is non-decreasing, 
we have  for , and hence  is convex on . 
\end{proof}

In practice, we apply equation \eqref{eq:grad_stablize_alpha_div} to the -divergence used in equation \eqref{eq:adaptive-alpha-divergence}.
By clipping the value of importance weights, what we optimize is still a divergence metric but is more friendly to gradient-based optimization. 

 

\begin{table*}[ht]
\centering
\setlength{\tabcolsep}{4pt}
\begin{tabular}{c|l|ccccccccccc}
\hline
Model & Method & 0.25 &  0.3  &  0.35  &  0.4  &  0.45  & 0.5  &  0.55   &  0.6  & 0.65 & 0.7 & 0.75\\
\hline
\hline
\multirow{3}{*}{MbV1} & w/o KD & 53.9 & 55.3 & 57.1 & 59.1 & 61.1 & 62.9 & 64.0 & 65.8 & 66.9 & 67.9 & 68.8 \\  
&w/ KL-KD  &  \bf 56.4  & 57.8 & 59.5 & 61.0 & 63.0 & 64.4 & 65.5 & 67.1 & 68.3 & 69.1 & 69.8 \\
& \bf w/ Adaptive-KD  (ours) & \bf 56.4 & \bf 57.9 & \bf  59.7 &  \bf 61.7 & \bf  63.4 &  \bf 65.0 &  \bf 66.2 &  \bf 67.7 &  \bf 68.8 &  \bf 69.5 &  \bf 70.1\\ \hline \hline 
\multirow{3}{*}{MbV2} &  w/o KD & - & - & 61.9 & 62.8 & 63.7 & 64.5 & 65.1 & 67.2 & 67.7 & 68.3 & 69.0 \\
& w/ KL-KD  &  - & - &63.2 & 64.4 & 65.1 & 66.0 & 66.5 & 68.4 & 69.2 & 69.5 & 70.1  \\
& \bf  w/ Adaptive-KD  (ours) &  - & - & \bf 63.7 & \bf64.6 &\bf 65.6 & \bf66.3 &\bf 66.9 & \bf68.7 &\bf 69.3 &\bf 69.9 & \bf70.5 \\ \hline 
\end{tabular}
\caption{Top-1 validation accuracy on ImageNet for Slimmable MobileNetV1 networks (denoted by MbV1) and Slimmable MobileNetV2 networks (denoted by MbV2) 
trained with different KD strategies.}
\label{tab:slimmbale}
\end{table*}


\begin{table*}[ht]
\centering
\setlength{\tabcolsep}{4pt}
\begin{tabular}{c|l|ccccccccccc}
\hline
Model & Method & 0.25 &  0.3  &  0.35  &  0.4  &  0.45  &  0.5  &  0.55   &  0.6  & 0.65 & 0.7 & 0.75\\
\hline \hline 
\multirow{3}{*}{MbV1} & w/ KL-KD (T=0.5)~~~~
& 55.1  & 56.0 & 57.6& 59.1  & 61.4 & 62.5  & 64.0 & 65.6  & 66.9  & 67.9  & 68.7  \\
& T=2.0 & 55.4& 57.0 & 58.8 & 60.7 & 62.6 & 64.1& 65.3 & 66.6  & 67.9 & 68.7& 69.5\\
&  T=4.0 & 48.7  & 50.7  & 53.1 & 55.9  & 58.8 & 60.9& 62.7 & 64.6 & 66.0 & 67.4 & 68.3\\
\hline \hline 
\multirow{3}{*}{MbV2} & w/ KL-KD (T=0.5)~~~& - & - &  61.7 & 62.9 & 63.8 & 64.6 & 65.0 & 67.4 & 68.4 & 68.8 & 69.8  \\
 & T=2.0 & - & - & 62.6 & 63.9 & 64.8 & 65.6 & 66.4 & 68.1 & 68.6 & 69.1 & 70.0 \\ 
 & T=4.0 & - & - & 59.3 & 60.9 & 62.2 & 63.1 & 64.0 & 66.3 & 67.1 & 67.7 & 68.8 \\ \hline 
\end{tabular}
\caption{Comparison to KL based KD with different temperature (T) settings.  
We report top-1 validation accuracy on ImageNet for slimmable MobileNetV1 and MobileNetV2  networks, denoted by MbV1 and MbV2, respectively.}
\label{tab:slimmable_abalation}
\end{table*}

\section{Experiments}
We apply our Adaptive-KD to improve notable supernet-based applications, including  slimmable neural networks \citep{yu2019universally}
and weight-sharing NAS  \citep[e.g.,][]{cai2019once, yu2020bignas, wang2020attentivenas}. 
We provide an overview of our algorithm for training supernet in Algorithm~\ref{alg:main}.

\begin{algorithm}[t]
\caption{Training supernet with -divergence}
\label{alg:main}
\begin{algorithmic}[1]
\STATE {\bf Input}: Adaptive -divergence range given by  and , a clipping factor , a supernet with parameter , search space .
\WHILE{not converging}
\STATE Sample a mini-batch of data .
\STATE Train the supernet with true labels from 
\STATE Draw  subnetworks  from ; train sub-networks to mimic the supernet on the mini-batch data  with the KD loss defined in Eqn.~\eqref{eq:adaptive-alpha-divergence} using clipped gradients in Eqn.~\eqref{eq:grad_stablize_alpha_div}.
\ENDWHILE
\end{algorithmic}
\end{algorithm}


\paragraph{Adaptive-KD settings}
In our algorithm,  and  control the magnitude of penalizing on \emph{over-estimation} and \emph{under-estimation}, respectively. And,  controls the range of density ratios between the teacher model and the student model.
We find our method performs robustly w.r.t. a wide of range of choices of  and , yielding consistent improvements over the KL based KD baseline.  
Throughout the experimental section, we set ,  and  as default for our method. We provide detailed ablation studies on these hyper-parameters in section~\ref{sec:exp_ablation}. 

\subsection{Slimmable  Neural Networks}
\label{sec:exp_slimmable}
Slimmable neural networks~\citep{yu2018slimmable, yu2019universally} are examples of supernet that support a wide range of channel width configurations. The search space  of slimmable networks contains networks with different width and all the other architecture configurations (e.g. depth, convolution type, kernel size) are the same.
This way, 
slimmable networks allow different devices or applications to adaptively adjust the model width on the fly according to on-device resource constraints to achieve the optimal accuracy vs. energy efficiency trade-off. 


\paragraph{Settings}
We closely follow the training recipe provided in \citet{yu2019universally}, 
and use slimmable MobileNetV1 \citep{howard2017mobilenets} 
and slimmable MobileNetV2 \citep{sandler2018mobilenetv2} as our testbed.
Specifically, we train slimmable MobileNetV1 to support arbitrary dynamic width in the range of , and train slimmable MobileNetV2 to support 
dynamic widths of . 

We adopt the sandwich rule sampling proposed in~\citet{yu2019universally} for training. At each training iteration, 
we sample the supernet with the largest channel width, 
the smallest sub-network with the smallest channel width and two random sub-networks to accumulate the gradients.
We train the supernet with ground truth labels and 
train all subsampled sub-networks with KD following~\eqref{eq:iterative-update}. 
For our baseline KD strategy, we set the KD coefficient  to be the number of sub-networks sampled, i.e., , as default  following~\citet{yu2019universally}.
To evaluate the effectiveness of our method, 
we simply replace the baseline KL-based KD loss used in \citet{yu2019universally} with our adaptive KD loss in~\eqref{eq:adaptive-alpha-divergence}. 

Additionally, we train all models for 360 epochs using SGD optimizer with momentum as 0.9, weight decay as  and dropout as 0.2. 
We use cosine learning rate decay, with an initial learning rate of 0.8, and batch size of 2048 on 16 GPUs.  
Following~\citet{yu2019universally}, we evaluate on ImageNet~\citep{deng2009imagenet}. 
We note that the baseline models trained with our hyper-parameter settings 
outperform those reported in \citet{yu2019universally}. 


\paragraph{Results} 
We summarize our results in Table~\ref{tab:slimmbale}. 
We report the top-1 accuracy on the ImageNet.
Here, \emph{w/o KD} denotes the training strategy that excludes the effect of KD.  All such sub-networks are trained with ground truth labels via cross entropy. 

As we can see from Table~\ref{tab:slimmbale}, 
both baseline KL based KD (denoted as \emph{w/ KL-KD}) and our adaptive KD (denoted as \emph{w/ Adaptive-KD}) yield significant performance improvements compared to \emph{w/o KD}. 
Our results confirm the importance of KD for training Slimmable networks. 
Meanwhile, our Adaptive-KD further improves on KL based KD for all the channel width configurations evaluated for both Slimmable MobileNetV1 (denoted by MbV1) and Slimmable MobileNetV2 (denoted by MbV2). 

\paragraph{Comparison to KD with different temperature coefficients}
As discussed in~\citet{hinton2015distilling}, 
for standard KL based KD, one can soften (or sharpen) the probabilities of the teacher and the student model by applying a temperature in their softmax layers. The best distillation performance might be achieved with a different temperature other than the normally used temperature of . 

To ensure a fair comparison,  
we further evaluate the baseline KL based KD under different temperature () settings following the approach in~\citet{hinton2015distilling}. We refer the reader to  Appendix~\ref{app:kd} for detailed discussion on this topic.
In particular, we test a number of temperatures - 0.5, 2, 4. 
We summarize our results in Table~\ref{tab:slimmable_abalation}.
We find all these settings to systematically perform worse 
than the simple KD strategy without temperature scaling, i.e., . 
Additionally, the models trained via our method yield the best performance. 


\begin{figure*}[t]
\centering
\setlength{\tabcolsep}{2pt}
\begin{tabular}{ccc}
\raisebox{1.5em}{\rotatebox{90}{\small Top-1 validation accuracy}}
\includegraphics[width=0.3\textwidth]{figures/supernet/imnet_pareto.pdf} &
\includegraphics[width=0.3\textwidth]{figures/supernet/imnet_bignas_min_net.pdf} &
\includegraphics[width=0.3\textwidth]{figures/supernet/imnet_bignas_max_net.pdf} \\
\small  MFlops & \small Training Epoch & \small  Training Epoch \\
\small (a) Performance Pareto front & \small (b) Training curve of the smallest sub-network & \small (c) Training curve of the supernet \\ 
\vspace{-1.0em}
\end{tabular}
\caption{(a) Comparison of Pareto-set performance of the supernet trained via KL based KD and our adaptive KD, respectively. Each dot represents a sub-network evaluated during the evolutionary search step. 
(b-c) Training curves of the smallest sub-network and the largest sub-network (i.e., the supernet). }
\vspace{-1em}
\label{fig:supernet_pareto_and_training}
\end{figure*}

\begin{figure*}[t]
\vspace{1em}
\begin{tabular}{c}
\raisebox{3em}{\rotatebox{90}{\small Validation Accuracy}}
\includegraphics[width=0.93\textwidth]{figures/supernet/super_box.pdf}  \\
\end{tabular}
\caption{Top-1 accuracy on ImageNet from weight-sharing NAS with KL-based KD and adaptive-KD. Each box plot shows the performance of sampled sub-networks within each FLOPs regime. From bottom to top, each horizontal bar represents the minimum accuracy, the first quartile, the median, the third quartile and the maximum accuracy, respectively. }
\label{fig:supernet_boxplot}
\end{figure*}

\begin{table*}[ht]
    \centering
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{l|ccccccc}
    \hline 
     & A0 {\scriptsize (203M)}   & A1{\scriptsize (279M)}  & A2{\scriptsize (317M)}  & A3{\scriptsize (357M)} & A4{\scriptsize (444M)}  & A5 {\scriptsize (491M)} & A6 {\scriptsize (709M)}   \\ \hline 
    w/o KD & 73.8 & 75.4 & 75.6 & 76.0 & 76.8 &77.1 &  77.9  \\
    w/ KL-KD & 77.0 & 78.2 & 78.5 &78.8 &79.3 &79.6 & 80.1 \\
    w/ KL-KD + Attentive Sampling \textsuperscript{} & 77.3 & 78.4 & 78.8 & 79.1 & 79.8 & 80.1 & 80.7 \\ \hline 
    \bf  w/ Adaptive-KD (ours - AlphaNet) &   \bf77.8 & \bf 78.9  & \bf79.1 & \bf 79.4 &\bf 80.0 & \bf 80.3 & \bf 80.8  \\
    \hline  
    \end{tabular}
    \caption{
    Performance on the discovered networks in \citet{wang2020attentivenas}. 
    Each (\#M) denotes the MFLOPs of the corresponding model. 
    \textsuperscript{} uses additional attentive sampling \citep{wang2020attentivenas} for training supernet.
    We denote our models as AlphaNet. 
    }
    \label{tab:attnas}
\end{table*}

\subsection{Weight-sharing NAS}
\label{sec:exp_supernet}
We apply our Adaptive-KD to improve supernet for weight-sharing NAS~\citep{cai2019once, yu2020bignas, wang2020attentivenas}. Please see Appendix~\ref{app:one_shot_nas} for a brief introduction on weight-sharing NAS. 
Note that one main procedure of weight-sharing NAS is to simultaneously train all sub-networks specified in the search space to converge. 
Similar to training Slimmable neural networks, 
this is often achieved by enforcing all sub-networks to learn from the supernet with KL based KD, ~\citep[e.g.,][]{yu2020bignas}. 

\paragraph{Training.} 
Our training recipe follows~\citet{wang2020attentivenas} except we use uniform sampling for simplicity. We pursue minimum code modifications to ablate the effectiveness of our KD strategy. 
We evaluate on the ImageNet dataset~\citep{deng2009imagenet}. 
All training details and the search space we used are discussed in Appendix~\ref{app:supernet}.


We use the update rule defined in \eqref{eq:iterative-update} to train the supernet.
Following~\citet{wang2020attentivenas} and \citet{yu2020bignas}, 
at each iteration, 
we train the supernet with ground truth labels and simultaneously 
we train the smallest sub-network and two random sub-networks with KD. 
In this way, a total of  networks are trained at each iteration.

\begin{figure}[h]
\centering
\begin{tabular}{c}
\raisebox{2.5em}{\rotatebox{90}{Top-1 validation accuracy}}
\includegraphics[width=0.42\textwidth]{figures/supernet/supernet_with_baseline.pdf}
\\
\small MFLOPs\\
\end{tabular}
\caption{Comparison with prior art NAS approaches on ImageNet. 
\#75ep denotes the models are further finetuned for 75 epochs with weights inherited from the corresponding supernet.}
\label{fig:compare_with_sota}
\end{figure}

\paragraph{Evaluation}
We compare the accuracy vs. FLOPs Pareto formed by the supernet learned by different KD strategies. 
To estimate the performance Pareto, we proceed as follows: 
1) we first randomly sample 512 sub-networks from the supernet and estimate their accuracy on the ImageNet validation set;  
2) we apply crossover and random mutation on the best performing 128 sub-networks following~\citet{wang2020attentivenas}. 
We fix both the crossover size and mutation size to be 128, yielding 256 new sub-networks. We then evaluate the performance of these sub-networks; 
3) We repeat the second step 20 times. 
The total number of sub-networks thus evaluated is . 


\paragraph{Results}
As we can see from Figure~\ref{fig:supernet_pareto_and_training}(a), 
\emph{Adaptive-KD} achieves a significantly better Pareto frontier compared to the KL-based KD baseline (denoted as \emph{w/ KL-KD}) and 
the simple training strategy without KD (denoted as \emph{w/o KD}).
Figures~\ref{fig:supernet_pareto_and_training}(b) and (c) plot the convergence curve of the smallest sub-network and the supernet, respectively. 
Our method adaptively optimizes a more difficult KD loss between the supernet and the sub-networks, yielding slightly slower convergence in the early stage of the training but better performance towards the end of the training. 

In Figure~\ref{fig:supernet_boxplot}, we group sub-networks according to their FLOPs and visualize five statistics for each group of sub-networks, including the minimum, the first quantile, the median, the third quantile and the maximum accuracy. Our method learns significantly better sub-networks in a quantitative way.


\paragraph{Improvement on SOTA}
As we use the same search space as in~\citet{wang2020attentivenas},
we further evaluate the discovered AttentiveNAS models (from A0 to A6)
with the supernet weights learned by our adaptive KD.
We call our models as AlphaNet. 

As we can see from  Table~\ref{tab:attnas}, 
our Adaptive-KD significantly improves on classic KL based KD, yielding 
an average of  improvements in the top-1 accuracy from A0 to A6. 
Also, our AlphaNet outperform all corresponding AttentiveNAS models~\citep{wang2020attentivenas}, which requires building Pareto-aware sampling distributions with additional computational overhead.  

We further compare our AlphaNet against prior art NAS baselines, including  EfficientNet~\citep{tan2019efficientnet}, FBNetV3~\citep{dai2020fbnetv3}, BigNAS~\citep{yu2020bignas}, OFA~\citep{cai2019once}, MobileNetV3~\citep{howard2019searching}, FairNAS~\citep{chu2019fairnas} and  MNasNet~\citep{tan2019mnasnet}, in Figure~\ref{fig:compare_with_sota}. 
Our method outperforms all the baselines evaluated, establishing new SOTA accuracy vs. FLOPs trade-offs on ImageNet. 
For example, our model achieves 77.8\% top-1 accuracy with only 203 MFLOPs. Under similar FLOPs constraint, the  corresponding top-1 accuracy is 75.2\%  with 219 MFLOPs for MobileNetV3, 76.5\% top-1 accuracy with 242 MFLOPs for BigNAS. 
Compared to OFA, our model achieves the same 80.0\% top-1 accuracy with 35\% fewer FLOPs (444M \emph{v.s.} 595M) and the same 79.1\% top-1 accuracy with  26\% fewer FLOPs (317M \emph{v.s.} 400M).


\begin{table}[t]
    \centering
    \setlength{\tabcolsep}{2pt}
    \begin{tabular}{l|cc|cc}
    \hline 
    Dataset &  Eff-B0 & Alp-A0 & Eff-B1 & Alp-A6 \\ \hline 
    Oxford Flowers & 97.2 & \bf{97.7} & 97.8 & \bf 98.7 \\
    Oxford-IIIT Pets& 91.2 & \bf{91.5} & 92.4 & \bf 92.9\\
    Food-101 & 87.6  & \bf{88.3} & 89.0 & \bf 89.6\\
    Stanford Cars & 91.0& \bf{91.5} & 92.2 & \bf 92.6\\
    FGVC Aircraft & 88.1  & \bf{88.5} & 88.7 & \bf 89.1\\
    \hline 
    \end{tabular}
    \caption{
    Comparison of transfer learning accuracy.
    `Eff' and `Alp' denotes EfficientNet and AlphaNet, respectively.
    All the networks are pretrained on ImageNet and then finetuned on transfer learning datasets. 
    EfficientNet-B0 and B1 has a model size of 390 MFLOPs and 700 MFLOPs, respectively. AlphaNet-A0 and A6 use 203 MFLOPs  and 709 MFLOPs, respectively. 
    }
    \label{tab:transfer_learning}
\end{table}



\begin{figure*}[ht]
\centering
\begin{tabular}{cc}
\raisebox{2.5em}{\rotatebox{90}{\small Relative Accuracy}}
\includegraphics[height=0.27\textwidth]{figures/supernet/ablation_tau.pdf} &
\raisebox{2.5em}{\rotatebox{90}{\small Relative Accuracy}}
\includegraphics[height=0.27\textwidth]{figures/supernet/ablation_alpha.pdf} \\
\small MFLOPs  & \small MFLOPs \\
\small (a) Ablation study of  & \small (b) Ablation study of  and \\
\end{tabular}
\vspace{-0.5em}
\caption{Relative accuracy compared to the results of KL based KD. 
Figure (a): we fix  and study the effect of our clipping factor . 
Figure (b): we set  as default and study the impact of  and . 
}
\label{fig:abla_alpha}
\end{figure*}

\begin{table*}[ht]
    \centering
    \begin{tabular}{l|c|ccc|c}
    \hline 
      Model  & w/o KD &  w/ KL-KD (T=1) & T=2  &T=4  & Adaptive-KD \\ \hline
      MobileNetV3  & 73.3 &\bf 73.9 & 72.2 & 70.8 &\bf 73.9  \\ 
      MobileNetV3  & 69.6 & 69.8 & 65.4 & 63.6 & \bf 70.0 \\ \hline
    \end{tabular}
    \caption{
    Comparison to KL based KD with a fixed teacher model.
    Here  denotes the temperature used in classic KL based KD (see Appendix~\ref{app:kd}). 
    We use MobileNetV3  as our teacher model, which yields  top-1 validation accuracy on ImageNet. }
    \label{tab:mbv3}
\end{table*}

\subsection{Transfer learning}
\label{sec:exp_transfer}

We take our AlphaNet-A0 and AlphaNet-A6 models that are pretrained on ImageNet and fine-tune them on a number of transfer learning benchmarks.
We closely follow the training settings in EfficientNet~\citep{tan2019efficientnet} and GPipe~\citep{huang2018gpipe}. 
Specifically, we use SGD with momentum of 0.9, label smoothing of 0.1 and dropout of 0.5. 
All models are fine-tuned for 150 epochs with batch size of 64. 
Following \citet{huang2018gpipe}, we search the best learning rate and weight decay on a hold-out subset (20\%) of the training data. 

    
\paragraph{Transfer learning results} 
We evaluated on five transfer learning datasets, including Oxford Flowers \citep{nilsback2008automated}, Oxford Pets \citep{parkhi2012cats}, Food-101~\citep{bossard2014food}, Stanford Cars~\citep{krause2013collecting} and Aircraft~\citep{maji2013fine}.
As we can see from Table~\ref{tab:transfer_learning}, 
our AlphaNet-A0 and  AlphaNet-A6 models lead to significant better transfer learning accuracy compared to those from EfficientNet-B0 and EfficientNet-B1 models. 


\subsection{Additional results}
\label{sec:exp_ablation}


\paragraph{Robustness w.r.t. clipping factor }
We follow the training and evaluation settings in section~\ref{sec:exp_supernet} and study the effect of .
In Figure~\ref{fig:abla_alpha} (a), we group sub-networks according to their FLOPs, and report the relative top-1 accuracy improvements of the maximum top-1 accuracy of each FLOPs group over the result from the KL based KD baseline.   
As shown in Figure~\ref{fig:abla_alpha}(a),  our algorithm is robust to the choice of . Our algorithm works with a large range of , from 1 to 10, yielding consistent improvements over the classic KL based KD baseline. 
And our default setting  achieves best performance on all FLOPs regimes evaluated.

\paragraph{Robustness w.r.t. }
We ablate the impact of both  and  under the same settings as in section~\ref{sec:exp_supernet}.
In this case,  we fix .
We present our findings in Figure~ \ref{fig:abla_alpha}(b). 
Firstly,  we test with , with  fixed as . 
With a more negative  (e.g., ), this defines a more difficult objective that brings optimization challenges. With a large  (e.g., ), the resulting KD loss is less discriminative regarding {uncertainty over-estimation}. Overall,  achieves a good balance between optimization difficulty and over-estimation penalization, yielding the best performance; 
Secondly, we vary  from  to , with  fixed as . 
Similarly, we find that large  (e.g, ) yields the best performance; 
Lastly, we set both . In this case, 
we still achieve better performance compared to the results of our KL based KD baseline, indicating the importance of penalizing over-estimation in training sub-networks. 
Also, our adaptive KD that regularizes on both over-estimation and over-estimation achieves better performance in general. 



\paragraph{Improvement on single network training}
Additionally, we apply our Adaptive-KD to train 
a single neural network with a pretrained teacher model, 
as in convectional KD setup (See Appendix~\ref{app:kd}). 
Specifically, 
we use MobileNetV3 ~\citep{howard2019searching} as our teacher model and 
train MobileNetV3  and   as our student models.  
We summarize the top-1 validation accuracy  on ImageNet from the models trained with different KD strategies in Table~\ref{tab:mbv3}. 
The student models trained via our method yield the best accuracy.   

 

\section{Conclusion}
In this work, we propose a method to improve the training of supernet with -divergence based knowledge distillation. 
By adaptively selecting an -divergence to optimize, 
our method simultaneously penalizes \emph{over-estimation} and \emph{under-estimation} in KD. 
Applying our method for neural architecture search, the searched AlphaNet models establish new state-of-the-art accuracy vs. FLOPs trade-offs on the ImageNet dataset.
 
\bibliographystyle{iclr2019_conference}
\bibliography{supernet}

\newpage
\onecolumn
\appendix

\section{Weight-sharing NAS}
\label{app:one_shot_nas}

Most  RL-based NAS~\citep[e.g.,][]{zoph2016neural} and differentiable NAS~\citep{liu2018darts, cai2018proxylessnas} 
consist of the following two stages as shown in Figure~\ref{fig:nas_sample}: 
\begin{enumerate}
\item[1)]  Stage 1 (architecture searching) - 
search potential architectures following a single resource constraint by using blackbox optimization techniques~\citep[e.g.,][]{zoph2016neural} or differentiable weight-sharing based approaches~\citep[e.g.,][]{liu2018darts, cai2018proxylessnas}; 
 \item[2)] Stage 2 (retraining) - retrain deep neural networks (DNNs) found in step 1) from scratch for best accuracy and final deployment.
\end{enumerate}

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/one_shot_nas/nas_simple.pdf}
    \caption{An overview of convectional NAS pipeline.}
    \label{fig:nas_sample}
\end{figure*}


Though promising results have been demonstrated, these NAS methods usually suffer from the following disadvantages: 
1) need to re-do the NAS search for different hardware resource constraints; 
2) require training the selected candidate from scratch to achieve desirable accuracy; 
3) 1) especially for RL-based NAS that uses black-box optimization techniques, it requires training a large number of neural networks from scratch or on proxy tasks; 
These disadvantages significantly increase the computational cost of NAS and make the NAS search computationally expensive.

\paragraph{Supernet-based Weight-sharing NAS}
To alleviate the aforementioned issues, supernet-based  weight-sharing NAS transforms the previous NAS training and search procedures as follows; see Figure~\ref{fig:fastnas}.
\begin{enumerate}
\item[1)] Stage 1  (supernet pretraining): jointly optimize the supernet and all possible sub-networks specified in the search space, such that all searchable networks simultaneously achieve good performance at the end of the training phase.

\item[2)]  Stage 2 (searching \& deployment): 
After stage 1 training, all the sub-networks are optimized simultaneously. 
One could then use typical searching algorithms, like evolutionary algorithms, to search the best model of interest. The model weights of each sub-network are directly inherited from the pre-trained supernet without any further re-training or fine-tuning.
\end{enumerate}

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.65\textwidth]{figures/one_shot_nas/fastnas_simple.pdf}
    \caption{An overview of supernet-based weight-sharing NAS.}
    \label{fig:fastnas}
\end{figure*}


Compared to RL-based NAS and differentiable NAS algorithms, the key advantages of the supernet-based weight-sharing NAS pipeline are: 1) one needs to only perform the computationally expensive supernet training for once. All sub-networks defined in the search space are ready to use after stage 1 is fully optimized. No retraining or fine-tuning is required;
2) all sub-networks of various model sizes are jointly optimized in stage 1, finding a set of Pareto optimal models that naturally supports various resource considerations. 

Notable examples of supernet-based weights-sharing NAS include
BigNAS~\citep{yu2020bignas}, OFA~\citep{cai2019once}, AttentiveNAS~\citep{wang2020attentivenas} and HAT~\citep{wang2020hat}.


\section{Weights-sharing NAS training settings}
\label{app:supernet}
We exactly follow the training settings in~\citet{wang2020attentivenas}~\footnote{\url{https://github.com/facebookresearch/AttentiveNAS}}. 
Specifically, 
we train our supernets for 360 epochs with cosine learning rate decay. 
We adopt SGD training on 64 GPUs. The mini-batch size is 32 per GPU. 
We use momeutm of 0.9, weight decay of , dropout of , stochastic layer dropout of . 
The base learning rate is set as 0.1 and is linearly scaled up for every 256 training samples. 
We use  AutoAugment \cite{cubuk2018autoaugment}  for data augmentation and set label smoothing coefficient to .

We use the same search space provided in~\citet{wang2020attentivenas}, see Table~\ref{tab:fbnet_reduced_se}. 
Here Conv denotes regular convolutional layers and 
MBConv refers to inverted residual block proposed by~ \citet{sandler2018mobilenetv2}. 
We use swish activation. 
Channel width represents the number of output channels of the block. 
MBPool denotes the efficient last stage in~ \citet{howard2019searching}. 
SE represents the squeeze and excite layer ~\citep{hu2018squeeze}. 
\emph{Input resolution} denotes the candidate resolutions. 
To simplify the data loading procedure, 
we always pre-fetch training patches of a fixed size, e.g., 224x224 on ImageNet, and then rescale them to our target resolution with bicubic interpolation following \citep{yu2020bignas}.


\begin{table*}[ht]
    \centering
    \setlength\tabcolsep{10pt}
    \begin{tabular}{c|ccccc}
    \hline
     Block name & Channel width & Depth  & Kernel size  & Expansion ratio & SE \\ \hline
     Conv &  \{16, 24\} & - & 3 & - & - \\
     MBConv-1 &  \{16, 24\} &  \{1,2\} & \{3, 5\} & 1 & N\\
     MBConv-2 &  \{24, 32\} & \{3, 4, 5\} & \{3, 5\} &  \{4, 5, 6\} & N \\
     MBConv-3 &  \{32, 40\} & \{3, 4, 5, 6\} &\{3, 5\} & \{4, 5, 6\} & Y\\
     MBConv-4 &  \{64, 72\} & \{3, 4, 5, 6\} &\{3, 5\} & \{4, 5, 6\} & N\\
     MBConv-5 &  \{112,128\} & \{3, 4, 5, 6, 7, 8\} & \{3, 5\} & \{4, 5, 6\} & Y\\
     MBConv-6 &  \{192, 200, 208, 216\} & \{3, 4, 5, 6, 7, 8\}  &\{3, 5\} & 6 & Y\\
     MBConv-7 &  \{216, 224\} & \{1, 2\}  &\{3, 5\} & 6 & Y\\
     MBPool  & \{1792, 1984\} & - & 1 & 6 & - \\
\hline
     Input resolution &   \multicolumn{4} {c}{\{192, 224, 256, 288\}} \\
    \hline
    \end{tabular}
    \caption{An illustration of our search space. Every row denotes a block group. 
    }
    \label{tab:fbnet_reduced_se}
\end{table*}


\section{Knowledge distillation}
\label{app:kd}
Consider the image classification task over a set of classes ,
where we have a collection of training images and one-hot labels 
 with 
and . 
We are interested in designing a deep neural network  that captures the relationship between  and . 
Here  is the network parameters of interest. 


KD provides an effective way to train  by distilling knowledge from a teacher model in addition to the one-hot labels. 
The teacher network is often a relative larger network with better performance. 
Specifically, let  be the teacher network, KD enforces  to mimic the output of  by minimizing the closeness between  and , which is often specified by the KL divergence , 
yielding the following loss function, 

Here  represents the empirical loss,  e.g., the typical cross entropy loss  with  be the -class probability produced by . And . Furthermore,  is the distilling weight that balances of the empirical loss and KD loss.

One could also apply a temperature  to soften (or sharpen) the outputs the teacher model and the student model in KD.  More precisely, given an input , we assume  and  the logit for the -th class produced by  and , respectively. Then the corresponding predictions of  and  after temperature scaling are as follows,  

with . 
In this way, the previous KD objective~\eqref{eq:kd} 
is now adapted to the following, 
 
Here  is introduced to ensure the gradients from the KD loss is at the same scale w.r.t the gradients from the empirical loss, see~\citep[e.g.,][]{hinton2015distilling}. 
We set  as default. 


\section{Related work}

\paragraph{Neural architecture search (NAS)}
NAS offer a powerful tool to automate the design of neural architectures for challenging machine learning tasks.
Early NAS solutions usually build upon black-box optimization, e.g. reinforcement learning \citep[e.g.,][]{zoph2016neural}, Bayesian optimisation \citep[e.g.,][]{kandasamy2018neural}, evolutionary algorithms \citep[e.g.,][]{real2019regularized}. These methods find good networks but are extremely computationally expensive in practice.
More recent NAS approaches have adopted weight-sharing \citep{pham2018efficient} to improve search efficiency. 
Weight-sharing based approaches often frame NAS as a constrained optimization and solve with continuous relaxation~\citep[e.g.,][]{liu2018darts, cai2018proxylessnas}. 
However, these methods require to run NAS for each deployment consideration, 
 e.g. a specific latency constraint for a particular mobile device, 
the total search cost grows linearly with the number of deployment considerations  \citep{cai2019once}. 
To further alleviate the aforementioned limitations, 
one-shot supernet-based NAS~\citep[e.g.,][]{cai2019once, yu2020bignas, wang2020attentivenas} proposes to first jointly train 
 all candidate sub-networks specified in the weight-sharing graph such that all sub-networks reach good performance at the end of training; 
 then one can apply typical search algorithms, e.g., genetic search, to find a set of Pareto optimal networks for various deployment scenarios. Overall, one-shot supernet-based methods provide a highly flexible and efficient NAS framework, yielding state-of-the-art empirical NAS performance on various challenging applications~\citep[e.g.,][]{cai2019once, wang2020hat}.

\paragraph{Knowledge Distillation} 
Our knowledge distillation works by forcing the student model to mimic the predictions of the teacher model.
As shown in the literature, the feature activations of intermediate layers of the teacher model can also be used as knowledge to supervise the training of the student model, 
notable examples include \citep[e.g.,]{romero2014fitnets,  huang2017like, ahn2019variational, jang2019learning, passalis2018learning, li2019hint}. 
Furthermore, correlations between different training examples (e.g. similarity) learned by the teacher model also provide rich information, which could be distilled to the student model \citep{park2019relational, yim2017gift}.
However, in our work, our KD involves training a large amount of sub-networks (student models) with different architecture configurations, e.g., different network depth, channel width, etc. 
It is less clear on how to define a good matching in the latent feature space between the teacher supernet and student sub-networks in a consistent way. 
While our  method offers a simple distillation mechanism that is easy to use in practice and in the meantime, leads to significant empirical improvements. 


\section{Additional results on ablation studies}
Following the settings in section~\ref{sec:exp_supernet}, 
we provide further analyses on the performance of sub-networks learned under 
different  and  settings. 

\begin{figure*}[ht]
\begin{tabular}{c}
\raisebox{3em}{\rotatebox{90}{\small Validation Accuracy}}
\includegraphics[width=0.93\textwidth]{figures/ablation/min_-1_max_1_beta_1.pdf}  \\
\raisebox{3em}{\rotatebox{90}{\small Validation Accuracy}}
\includegraphics[width=0.93\textwidth]{figures/ablation/min_-1_max_1_beta_10.pdf}  \\
\raisebox{3em}{\rotatebox{90}{\small Validation Accuracy}}
\includegraphics[width=0.93\textwidth]{figures/ablation/min_-2_max_1_beta_5.pdf}  \\
\raisebox{3em}{\rotatebox{90}{\small Validation Accuracy}}
\includegraphics[width=0.93\textwidth]{figures/ablation/min_0_max_1_beta_5.pdf}  \\
\end{tabular}
\label{fig:app_abalate_supernet_boxplot}
\caption{Additional results on ablation studies. Each box plot shows the performance of sampled sub-networks within each FLOPs regime. From bottom to top, each horizontal bar represents the minimum accuracy, the first quartile, the median, the third quartile and the maximum accuracy, respectively. }
\end{figure*}


\begin{figure*}[ht]
\begin{tabular}{c}
\raisebox{3em}{\rotatebox{90}{\small Validation Accuracy}}
\includegraphics[width=0.93\textwidth]{figures/ablation/min_-1_max_0.5_beta_5.pdf}  \\
\raisebox{3em}{\rotatebox{90}{\small Validation Accuracy}}
\includegraphics[width=0.93\textwidth]{figures/ablation/min_-1_max_2_beta_5.pdf}  \\
\raisebox{3em}{\rotatebox{90}{\small Validation Accuracy}}
\includegraphics[width=0.93\textwidth]{figures/ablation/min_-1_max_-1_beta_5.pdf}  \\
\end{tabular}
\label{fig:app_abalate_supernet_boxplot_cont}
\caption{Additional results on ablation studies. Each box plot shows the performance of sampled sub-networks within each FLOPs regime. From bottom to top, each horizontal bar represents the minimum accuracy, the first quartile, the median, the third quartile and the maximum accuracy, respectively. }
\end{figure*}
 
\end{document}
