\documentclass{article}





\usepackage[preprint]{neurips_2020}

\usepackage{neurips_2020}



\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      

\usepackage{subfigure}
\usepackage{booktabs} \usepackage{graphicx}
\usepackage{boldline}
\usepackage{amsthm}
\usepackage[cmex10]{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{natbib}
\usepackage{boldline}
\usepackage{siunitx}
\usepackage{bbm}


\usepackage{algorithm}
\usepackage{algorithmic}


\usepackage{makecell}
\renewcommand\theadalign{bc}
\renewcommand\theadfont{\bfseries}
\renewcommand\theadgape{\Gape[4pt]}	
\renewcommand\cellgape{\Gape[4pt]}
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newenvironment{proof1}{\paragraph{Proof:}}{\hfill}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 


\title{Learning Generative Models using Denoising Density Estimators}



\author{Siavash A. Bigdeli \\
  CSEM, Neuchâtel\\ Switzerland\\
  \\
	\And
	Geng Lin \\
  University of Maryland\\ College Park, USA \\
		\AND
	Tiziano Portenier \\
  ETHZ, Zurich\\ Switzerland \\
	  \And
	L. Andrea Dunbar \\
  CSEM, Neuchâtel\\ Switzerland \\
		\And
	Matthias Zwicker \\
  University of Maryland\\ College Park, USA \\
}

\begin{document}

\maketitle

\begin{abstract}

Learning probabilistic models that can estimate the density of a given set of samples, and generate samples from that density, is one of the fundamental challenges in unsupervised machine learning. We introduce a new generative model based on denoising density estimators (DDEs), which are scalar functions parameterized by neural networks, that are efficiently trained to represent kernel density estimators of the data. Leveraging DDEs, our main contribution is a novel technique to obtain generative models by minimizing the KL-divergence directly. We prove that our algorithm for obtaining generative models is guaranteed to converge to the correct solution. Our approach does not require specific network architecture as in normalizing flows, nor use ordinary differential equation solvers as in continuous normalizing flows. Experimental results demonstrate substantial improvement in density estimation and competitive performance in generative model training.

\end{abstract}

\section{Introduction}
Learning generative probabilistic models from raw data is one of the fundamental problems in unsupervised machine learning. These models enable sampling from the probability density represented by the input data, or also performing density estimation and inference of latent variables. Recently, the use of deep neural networks has led to significant advances in this area. For example, generative adversarial networks~\citep{Goodfellow2014GAN} can be trained to sample very high dimensional densities, but they do not provide density estimation or inference. Inference in Boltzman machines~\citep{salakhutdinov2009deep} is tractable only under approximations~\citep{WELLING200319}. Variational autoencoders~\citep{Kingma2014VAE} provide functionality for both (approximate) inference and sampling. Finally, normalizing flows~\citep{Dinh2014NICE} perform all three operations (sampling, density estimation, inference) efficiently.

In this paper we introduce a novel type of generative model based on what we call denoising density estimators (DDEs), which supports efficient sampling and density estimation. Our approach to construct a sampler is straightforward: assuming we have a density estimator that can be efficiently trained and evaluated, we learn a sampler by forcing its generated density to be the same as the input data density via minimizing their Kullback-Leibler (KL) divergence. In particular, we use the {\em reverse} KL divergence, which avoids saddle points when the two distributions are non-overlapping. In our approach, the density estimator is derived from the theory of denoising autoencoders, hence our term {\em denoising density estimator}. Compared to normalizing flows, a key advantage of our theory is that it does not require any specific network architecture, except differentiability, and we do not need to solve ordinary differential equations (ODE) like in continuous normalizing flows. In summary, our main contribution is a novel approach to obtain a generative model by explicitly estimating the energy (un-normalized density) of the generated and true data distributions and minimizing the statistical divergence of these densities. 

\section{Related Work}
Generative adversarial networks~\citep{Goodfellow2014GAN} are currently the most widely studied type of generative probabilistic models for very high dimensional data.
GANs are often difficult to train, however, and they can suffer from mode-collapse, sparking renewed interest in alternatives. A common approach is to formulate these models as mappings between a latent space and the data domain, and one way to categorize them is to consider the constraints on this mapping. In normalizing flows~\citep{Dinh2014NICE,Rezend2015NFl} the mapping is invertible and differentiable, such that the data density can be estimated using the determinant of its Jacobian, and inference can be perfomed via the inverse mapping. Normalizing flows can be trained simply using maximum likelihood estimation (MLE)~\citep{Dinh2017NVP}. The challenge, however, is to design efficient computational structures for the required operations~\citep{Huang2018NAF,Kingma2018GLOW}. \citet{Chen2018NODE} and \citet{grathwohl2019ffjord} derive continuous normalizing flows by parameterizing the dynamics (the time derivative) of an ODE using a neural network,
but it comes at the cost of solving ODEs to produce outputs.
In contrast, in variational techniques the relation between the latent variables and data is probabilistic, usually expressed as a Gaussian likelihood function. Hence computing the marginal likelihood requires integration over latent space. To make this tractable, it is common to bound the marginal likelihood using the evidence lower bound~\citep{Kingma2014VAE}. 
Recently,~\citet{Li2018IMLE} proposed an approximate form of MLE, which they call implicit MLE (IMLE), that can also be performed without requiring invertible mappings. As a disadvantage, IMLE requires nearest neighbor queries in (high dimensional) data space.

Not all generative models include a latent space, for example autoregressive models~\citep{Oord2016PCNN} or denoising autoencoders (DAEs)~\citep{JMLR:v15:alain14a}. In particular,~\citet{JMLR:v15:alain14a} and~\citet{Saremi2019NeuralEB} use the well known relation between DAEs and the score of the corresponding data distributions~\citep{Vincent:2011:CSM,Raphan:2011:LSE} to construct an approximate Markov Chain sampling procedure.
Similarly, \citet{bigdeli2017image} and~\citet{bigdeli2017deep} use DAEs to learn the gradient of image densities for optimizing maximum a-posteriori problems in image restoration.
We build on DAEs, but formulate an estimator for the un-normalized, scalar density, rather than for the score (a vector field). This is crucial to allow us to train a generator instead of requiring Markov chain sampling, which has the disadvantages of requiring sequential sampling and producing correlated samples.

Instead of using a denoising objective, score-matching can also be achieved by minimizing Stein's loss for the true and estimated density gradients.
\citet{kingma2010regularized} use a regularized version of the loss to parametrize a product-of-experts model for images, and
\citet{li2019learning} train deep density estimators based on exponential family kernels.
These techniques require computation of third order derivatives, however, limiting the dimensionality of their models.
\citet{Song2019GMG} extend this approach by introducing a sliced score-matching objective that leads to more efficient training.
Unlike these techniques, DDEs are optimized using a denoising objective, hence they can be optimized without approximations, nor higher order derivatives. This allows us to efficiently train an exact generator that scales well with the data dimensionality.
In addition, \citet{Song2019GMG} formulate a generative model using Langevin dynamics,
which requires an iterative sampling procedure that provides exact sampling only asymptotically.
Similarly, \citet{dai2019exponential} use adversarial training to learn dynamics for generating samples.
Unlike these approaches, we do not require an iterative sampling scheme and our generator produces samples in single forward passes.

Other energy-based techniques for generative models include the work by~\citet{kim2016deep}, who use directed graphs to learn densities in latent space and to train their generator.
The approximation in this approach limits their generalization to complex and higher dimension datasets.
Using kernel exponential families, \citet{dai2019kernel} train a density estimator at the same time as their dual generator.
Similar to other score-matching optimizations, their approach requires quadratic computations with respect to the input dimensions at each gradient calculation.
Moreover, they only report generated results on 2D toy examples.
Table~\ref{tbl:comparison} summarizes the differences of our approach to GANs, Score-Matching, and Normalizing Flows.




\begin{table}[t]
\bgroup
\setlength{\tabcolsep}{3.5pt}
\begin{center}
\begin{tabular}[c]{l c c c c }
\hlineB{3}
Property & GAN & Score-Matching & Normalizing Flows & Ours \\
\hline
Provides density & (-) & - & \checkmark & \checkmark \\
Forward sampling model & \checkmark & iterative & \checkmark & \checkmark \\
Exact sampling & \checkmark & asymptotic & \checkmark & \checkmark \\
Free net architecture & \checkmark & \checkmark & - & \checkmark \\
\hlineB{3}
\end{tabular}
\end{center}
\egroup
\caption
{
Comparison of different deep generative approaches based on GANs, Score-Matching, Normalizing Flows, and our proposed technique. Adversarial density estimation can be achieved using the approach by~\citet{abbasnejad2019generative} using a suitable training objective. 
}
\label{tbl:comparison}
\end{table}

\section{Denoising Density Estimators (DDEs)}
\label{sec:dde}

First we describe how to estimate a density using a variant of denoising autoencoders (DAEs). More precisely, this approach allows us to obtain the density smoothed by a Gaussian kernel, which is equivalent to kernel density estimation~\citep{parzen1962}, up to a normalizing factor. Originally, the optimal DAE ~\citep{Vincent:2011:CSM,JMLR:v15:alain14a} is defined as the function minimizing the following denoising loss, 

where the data  is distributed according to a density  over , and  represents -dimensional, isotropic additive Gaussian noise with variance . It has been shown~\citep{robbins1956,Raphan:2011:LSE,bigdeli2017image} that the optimal DAE  minimizing  can be expressed as follows, which is also known as Tweedie's formula,

where  is the gradient with respect to the input ,  denotes the convolution between the data and noise distributions , and . 
Inspired by this result, we reformulate the DAE-loss as a noise estimation loss,

where  is a vector field that estimates the noise vector .
Similar to \citet{Vincent:2011:CSM} and \citet{JMLR:v15:alain14a}, we formulate the following proposition and provide the proof in the supplementary material:

\begin{proposition}
There is a unique minimizer  that satisfies

That is, the optimal estimator corresponds to the gradient of the logarithm of the Gaussian smoothed density , that is, the score of the density.
\label{prop:nes}
\end{proposition}

A key observation is that the desired vector-field  is the gradient of a scalar function and conservative. Hence we can write the noise estimation loss in terms of a scalar function  instead of the vector field , which we call the denoising density estimation loss, 

A similar formulation has recently been proposed by~\citet{Saremi2019NeuralEB}. Our terminology is motivated by the following corollary:
\begin{corollary} The minimizer  satisfies

with some constant .
\label{cor:dde}
\end{corollary}
\begin{proof}
From Proposition~\ref{prop:nes} and the definition of  we know that , which leads immediately to the corollary.
\end{proof}
In summary, we have shown how modifying the denoising autoencoder loss (Eq.~\ref{eq:denoisingloss}) into a noise estimation loss based on the gradients of a scalar function (Eq.~\ref{eq:ddeloss}) allows us to derive a density estimator (Corollary~\ref{cor:dde}), which we call the denoising density estimator (DDE). In practice, we approximate the DDE using a neural network .
For illustration, Figure~\ref{fig:2Ddensity} shows 2D distribution examples, which we approximate using a DDE implemented as a multi-layer perceptron.


\section{Learning Generative Models using DDEs}
\label{sec:generativemodels} 

By leveraging DDEs, our key contribution is to formulate a novel training algorithm to obtain generators for given densities, which can be represented by a set of samples or as a continuous function. In either case, we denote the smoothed data density , which is obtained by training a DDE in case the input is given as a set of samples as described in Section~\ref{sec:dde}. We express our samplers using mappings , where , and  (usually ) is a latent variable, which typically has a standard normal distribution. In contrast to normalizing flows,  does not need to be invertible. Let us denote the distribution of  induced by the generator as , that is , and also its Gaussian smoothed version . 

We obtain the generator by minimizing the KL divergence  between the density induced by the generator  and the data density . Our algorithm is based on the following observation:

\begin{proposition}
Given a scalar function  that satisfies the following conditions:

then  for small enough .
\label{pro:klupdate}
\end{proposition}

\begin{proof}
We will use the first order approximation , where the division is pointwise. Using  to denote the inner product, we can write

This means

because the first term on the right hand side is negative (first assumption in Equation~\ref{eq:assumption1}), the second term is zero (second assumption in Equation~\ref{eq:assumption2}), and the third and fourth terms are quadratic in  and can be ignored for  when  is small enough (third assumption in Equation~\ref{eq:assumption3}).
\end{proof}
Based on the above observation, Algorithm~\ref{algo:generator} minimizes  by iteratively computing updated densities  that satisfy the conditions from Proposition~\ref{pro:klupdate}, hence . This is guaranteed to converge to a global minimum, because  is convex in . 

At the beginning of each iteration in Algorithm~\ref{algo:generator} (Line 3), by definition  is the density obtained by sampling our generator  (-dimensional standard normal distribution), and the generator is a neural network with parameters . In addition,  is defined as the density obtained by sampling . Finally, the DDE  correctly estimates , that is . 
In each iteration, we update the generator such that its density is changed by a small  that satisfies the conditions from Proposition~\ref{pro:klupdate}. We achieve this by computing a gradient descent step of  with respect to the generator parameters  (Line 4). The constant  can be ignored since we only need the gradient ( always integrate to one after any generator update). A small enough learning rate guarantees that condition one (Equation~\ref{eq:assumption1}) in Proposition~\ref{pro:klupdate} is satisfied. The second condition (Equation~\ref{eq:assumption2}) is satisfied because we update the distribution by updating its generator, and the third condition (Equation~\ref{eq:assumption3}) is also satisfied under a small enough learning rate (and assuming the generator network is Lipschitz continuous). After updating the generator, we update the DDE to correctly estimate the new density produced by the updated generator (Line 6). Note that in practice, we perform fixed number of iterations (5-10 steps similar to GANs) to optimize the DDE, which did not lead to any instabilities.

Note that it is crucial in the first step in the iteration in Algorithm~\ref{algo:generator} that we sample using  and not . This allows us, in the second step, to use the updated  to train a DDE  that exactly (up to a constant) matches the density generated by . Even though in this approach we only minimize the KL divergence with the ``noisy'' input density , the sampler  still converges to a sampler of the underlying density  in theory (exact sampling).

\begin{algorithm}
\caption{Training steps for the generator. The input to the algorithm is a pre-trained optimal DDE on input data  and a learning rate .}
	\label{algo:generator}
	\begin{algorithmic}[1]
		\STATE Initialize generator parameters  
		\STATE Initialize DDE  with 
		\WHILE{not converged}
			\STATE , with 
			\STATE //  now indicates the updated density using the updated 
			\STATE  // In practice, we only take few optimization steps
			\STATE //  is now the density (up to a constant) of 
 		\ENDWHILE
	\end{algorithmic}
\end{algorithm}





\paragraph{Exact Sampling.}

Our objective involves reducing the KL divergence between the Gaussian smoothed generated density  and the data density . This also implies that the density  obtained from sampling the generator  is identical with the data density , without Gaussian smoothing, which can be expressed as the following corollary:

\begin{corollary}
Let  and  be related to densities  and , respectively, via convolutions using a Gaussian , that is . Then the smoothed densities  and  are the same if and only if the data density  and the generated density  are the same.
\end{corollary}

This follows immediately from the convolution theorem and the fact that the Fourier transform of Gaussian functions is non-zero everywhere, that is, Gaussian blur is invertible.

\paragraph{Relation to GANs.}

In the original GANs~\citep{Goodfellow2014GAN}, the generator is trained to minimize the Jensen-Shannon divergence between generated and real data distributions.
Our model is optimized to minimize the KL-divergence instead, which has been shown to achieve better likelihood scores compared to GANs~\citep{nowozin2016f}.
Moreover, we use the reverse KL-divergence loss in our training, which unlike forward KL-divergence, avoids saddle points when the two distributions are non-overlapping. This is because minimizing the reverse KL divergence can be reformulated as

which includes a term that attempts to maximize the entropy  of the generated distribution .
Wasserstein-GANs address the same issue by using the Wasserstein distance between the two distributions to formulate their loss.
These models, however, require the discriminator network to guarantee Lipschitz continuity, which is imposed either by weight-clipping~\cite{arjovsky2017wasserstein} or gradient penalty methods~\citep{gulrajani2017improved}.
Our DDEs explicitly impose Gaussian-smoothness on the data distribution, which guarantees that the density is non-zero everywhere.
Additionally, the DDEs are trained to exactly constrain their gradients with respect to their inputs (Equation~\ref{eq:ddeloss}), without requiring additional techniques to control gradient magnitudes or weight clipping.











\section{Experiments}

\paragraph{2D Comparisons.}



Similar to~\citet{grathwohl2019ffjord}, we perform experiments for 2D density estimation and visualization over three datasets.
Additionally, we learn generative models. For our DDE networks, we used multi-layer perceptrons with residual connections. All networks have 25 layers, each with 32 channels and Softplus activation. For training we use 2048 samples per iteration to estimate the expected values.
Figure~\ref{fig:2Ddensity} shows the comparison of our method with Glow~\citep{Kingma2018GLOW}, BNAF~\citep{de2019block}, and FFJORD~\citep{grathwohl2019ffjord}.
Our DDEs can estimate the density accurately and capture the underlying complexities of each density.
Due to inherent smoothing as in kernel density estimation (KDE), our method induces a small blur to the density compared to BNAF. To demonstrate this effect, we show DDEs trained with both small and large noise standard deviations  and .
However, our DDE can estimate the density coherently through the data domain, whereas BNAF produces noisy approximation across the data.

Generator training and sampling is demonstrated in Figure~\ref{fig:2Ddensity} on the right. The sharp edges of the checkerboard samples imply that, due to invertibility of a small Gaussian blur, the generator learns to sample from the sharp target density even though the DDEs estimate noisy densities. While the generator update in theory requires DDE networks to be optimal at each gradient step, we take a limited number of 10 DDE gradient descent steps for each generator update to accelerate convergence. We summarize the training parameters used in these experiments the supplementary material.

\begin{figure*}[t]
\centering
\small
\setlength\tabcolsep{1.5pt}
\begin{tabular} {cccccccc}
& Real samples & Glow & BNAF & FFJORD & Ours \scriptsize{()} & Ours \scriptsize{()} & Ours generated \\
\raisebox{3.0\normalbaselineskip}[0pt][0pt]{\rotatebox[origin=c]{90}{Two Spirals}} &
\includegraphics[width=.13\textwidth]{images/toys/ts-gt.png} &
\includegraphics[width=.13\textwidth]{images/bnaf/ts-glow.jpg} &
\includegraphics[width=.13\textwidth]{images/bnaf/ts-bnaf.jpg} &
\includegraphics[width=.13\textwidth]{images/bnaf/ts-ffjord.png} &
\includegraphics[width=.13\textwidth]{images/toys/ts-dde-nogen-s5e-2-dde-real.png} &
\includegraphics[width=.13\textwidth]{images/toys/ts-dde-nogen-s2e-1-dde-real.png} &
\includegraphics[width=.13\textwidth]{images/toys/ts-sigmacmp-s2e-1-gen.png} \\
\raisebox{3.0\normalbaselineskip}[0pt][0pt]{\rotatebox[origin=c]{90}{Checkerboard}} &
\includegraphics[width=.13\textwidth]{images/toys/cb-gt.png} &
\includegraphics[width=.13\textwidth]{images/bnaf/cb-glow.jpg} &
\includegraphics[width=.13\textwidth]{images/bnaf/cb-bnaf.jpg} &
\includegraphics[width=.13\textwidth]{images/bnaf/cb-ffjord.png} &
\includegraphics[width=.13\textwidth]{images/toys/cb-dde-nogen-s5e-2-dde-real.png} &
\includegraphics[width=.13\textwidth]{images/toys/cb-dde-nogen-s2e-1-dde-real.png} &
\includegraphics[width=.13\textwidth]{images/toys/cb-dde-s2e-1-gen.png}
\end{tabular}
\caption{Density estimation in 2D, showing that we can accurately capture these densities with few visual artifacts.
The rightmost column shows samples generated using our generative model training.}
\label{fig:2Ddensity}
\end{figure*}











\paragraph{MNIST.} Figure~\ref{fig:mnist} illustrates our generative training on MNIST~\citep{lecun1998mnist} using Algorithm~\ref{algo:generator}. We use a dense block architecture with fully connected layers here and refer to the supplementary material for the network and training details, including additional results for Fashion-MNIST~\citep{xiao2017fashion}. Figure~\ref{fig:mnist} shows qualitatively that our generator is able to replicate the underlying distributions. In addition, latent-space interpolation demonstrates that the network learns an intuitive and interpretable mapping from normally distributed latent variables to samples of the data distribution. 

 \begin{figure*}[t]
\centering
\begin{tabular} {cc}
(a) Generated samples & (b) Real samples\\
\includegraphics[width=.47\textwidth]{images/mnist/mnist_generated_cropped.jpg} &
\includegraphics[width=.47\textwidth]{images/mnist/mnist_real_cropped.jpg} 
\end{tabular}  \\
(c) Interpolated samples using our model
\includegraphics[width=.98\textwidth]{images/mnist/mnist_interp_cropped.png} 
\caption[MNIST generated results.]
{
Generated MNIST (a), from the dataset (b), and latent space interpolation (c).
}
\label{fig:mnist}
\end{figure*}

\paragraph{CelebA.} Figure~\ref{fig:celebA} shows additional experiments on the CelebA dataset~\citep{liu2015deep}. 
The images in the dataset have  dimensions and we normalize the pixel values to be in range .
To show the flexibility of our algorithm with respect to neural network architectures, here we use a style-based generator~\citep{karras2019style} architecture for our generator network. Please refer to the supplementary material for network and training details. Figure~\ref{fig:celebA} shows that our approach can produce natural-looking images, and the model has learned to replicate the global distribution with a diverse set of images and different characteristics.

\begin{figure*}[t]
\centering
\begin{tabular} {cc}
(a) Generated samples & (b) Real samples\\
\includegraphics[width=.47\textwidth, trim={0 128px 0 0},clip]{images/celebA_32.png} &
\includegraphics[width=.47\textwidth, trim={0 128px 0 0},clip]{images/celebA_real_32.png} 
\end{tabular} 
\caption[celebA generated results.]
{
Generated results on  images from the celebA dataset~\citep{liu2015deep}.
}
\label{fig:celebA}
\end{figure*}


\paragraph{Quantitative Evaluation with Stacked-MNIST.} We perform a quantitative evaluation of our approach based on the synthetic Stacked-MNIST~\citep{metz2016unrolled} dataset, which was designed to analyse mode-collapse in generative models. The dataset is constructed by stacking three randomly chosen digit images from MNIST to generate samples of size .
This augments the number of classes to , which are considered as distinct modes of the dataset. Mode-collapse can be quantified by counting the number of nodes generated by a model. Additionally, the quality of the distribution can be measured by computing the KL-divergence between the generated class distribution and the original dataset, which has a uniform distribution in terms of class labels. Similar to prior work~\citep{metz2016unrolled}, we use an external classifier to measure the number of classes that each generator produces by separately inferring the class of each channel of the images. 

Figure~\ref{fig:stackedmnist} reports the quantitative results for this experiment by comparing our method with well-tuned GAN models. DCGAN~\citep{radford2015unsupervised} implements a basic GAN training strategy using a stable architecture. WGAN uses the Wasserstein distance~\citep{arjovsky2017wasserstein}, and WGAN+GP includes a gradient penalty to regularize the discriminator~\citep{gulrajani2017improved}. For a fair comparison, all methods use the DCGAN network architecture. Since our method requires two DDE networks, we have used fewer parameters in the DDEs so that in total we preserve the same number of parameters and capacity as the other methods.
For each method, we generate batches of 512 samples per training iteration and count the number of classes within each batch (that is, the maximum number of different labels in each batch is 512). We also plot the reverse KL-divergence to the uniform ground truth class distribution. Using the two measurements we can see how well each method replicates the distribution in terms of diversity and balance. Without fine-tuning and changing the capacity of our network models, our approach is comparable to modern GANs such as WGAN and WGAN+GP, which outperform DCGAN by a large margin in this experiment. 

We also report results for sampling techniques based on Score-Matching. We trained a Noise Conditional Score Network (NCSN) parametrized with a UNET architecture~\citep{ronneberger2015u}, which is then followed by a sampling algorithm using the Annealed Langevin Dynamics (ALD) as described by~\citet{Song2019GMG}. We refer to this method as UNET+ALD. We also implemented a model based on our approach called DDE+ALD, where we used our DDE network in combination with iterative Langevin sampling. While our training loss is equivalent to the score-matching objective, the DDE network outputs a scalar and explicitly enforces the score to be a conservative vector field by computing it as the gradient of its scalar output. DDE+ALD uses the spatial gradient of the DDE for iterative sampling with ALD~\citep{Song2019GMG}, instead of our proposed direct, one-step generator as described in Section~\ref{sec:generativemodels}. We observe that DDE+ALD is more stable compared to the UNET+ALD baseline, even though the UNET achieves a lower loss during training. We believe that this is because DDEs guarantee conservativeness of the distribution gradients (i.e. scores), which leads to more diverse and stable data generation as we see in Figure~\ref{fig:stackedmnist}. Furthermore, our approach with direct sampling outperforms both UNET+ALD and DDE+ALD.

\begin{figure*}[t]
\centering
\begin{tabular} {cc}
(a) Generated modes per batch & (b) KL-divergence\\
\includegraphics[width=.4\textwidth]{images/stackedMNIST/modes.png} &
\includegraphics[width=.4\textwidth]{images/stackedMNIST/kl.png} \end{tabular} 
\caption[Stacked-MNIST dataset results.]
{
Mode-collapse experiment results on Stacked-MNIST as a function of training iterations (for discriminator or DDE).
(a) Number of generated modes per batch of size 512.
(b) Reverse KL-divergence between the generated and the data distribution in the logarithmic domain.
}
\label{fig:stackedmnist}
\end{figure*}

\paragraph{Real Data Density Estimation.}

We follow the experiments in BNAF~\citep{de2019block} for density estimation, which includes the POWER, GAS, HEPMASS, and MINIBOON datasets~\citep{asuncion2007uci}. Since DDEs estimate densities up to their normalizing constant, we approximate the constant using Monte Carlo estimation here.
Similarly, \citet{li2019learning} use sampling to estimate the normalizing constant.
We show average log-likelihoods over test sets and compare to state-of-the-art methods for normalized density estimation in Table~\ref{tbl:densityestimation}.
We have omitted the results of the BSDS300 dataset~\citep{martin2001database}, since we could not estimate the normalizing constant reliably (due to high dimensionality of the data).
To train our DDEs, we used Multi-Layer Perceptrons (MLP) with residual connections between each layer.
All networks have 25 layers, with 64 channels and Softplus activations, except for GAS and HEPMASS, which employ 128 channels.
We trained the models for 400 epochs using learning rate of \num{2.5e-4} with linear decay with scale of  every 100 epochs.
Similarly, we started the training by using noise standard deviation  and decreased it linearly with the scale of  up to a dataset specific value, which we set to \num{5e-2} for POWER, \num{4e-2} for GAS, \num{2e-2} for HEPMASS, and  for MINIBOON.
We estimate the normalizing constant via importance sampling using a Gaussian distribution with the mean and variance of the DDE input distribution.
We average 5 estimations using 51200 samples each (we used 10 times more samples for GAS), and we indicate the variance of this average in Table~\ref{tbl:densityestimation}. 

\newcommand{\addvar}[1]{{\tiny{}}}
\newcommand{\descrcell}[2]{\scriptsize
  \begin{tabular}[t]{@{}c@{}}\normalsize#1\\ \tiny{#2}\end{tabular}}




\begin{table*}[t]
\bgroup
\begin{center}
\begin{tabular}[c]{l c c c c }
\hlineB{3}
\raisebox{-0.35\normalbaselineskip}[0pt][0pt]{Model} & \descrcell{POWER}{} & \descrcell{GAS}{ } & \descrcell{HEPMASS}{} & \descrcell{MINIBOON}{ } \\\hline
\cite{Dinh2017NVP} &  \addvar{.01} &  \addvar{.14} &  \addvar{.02} &  \addvar{.49} \\\cite{Kingma2018GLOW} &  \addvar{.01} &  \addvar{.40} &  \addvar{.08} &  \addvar{.07} \\\cite{germain2015made}* &  \addvar{.01} &  \addvar{.02} &  \addvar{.02} &  \addvar{.47} \\\cite{papamakarios2017masked} &  \addvar{.01} &  \addvar{.02} &  \addvar{.02} &  \addvar{.45} \\\cite{papamakarios2017masked}* &  \addvar{.01} &  \addvar{.02} &  \addvar{.02} &  \addvar{.44} \\\cite{grathwohl2019ffjord} &  \addvar{.01} &  \addvar{.12} &  \addvar{.08} &  \addvar{.04} \\\cite{Huang2018NAF} &  \addvar{.01} &  \addvar{.33} &  \addvar{.40} &  \addvar{.15} \\\cite{oliva2018transformation} &  \addvar{.01} &  \addvar{.02} &  \addvar{.02} &  \addvar{.48} \\\cite{de2019block} &  \addvar{.01} &  \addvar{.09} &  \addvar{.38} &  \addvar{.07} \\\hline
\citet{li2019learning} & - & - &   &  \\\textbf{Ours} &  \addvar{.18} &  \addvar{1.14} &  \addvar{.16} &  \addvar{1.81} \\\hlineB{3}
\end{tabular}
\end{center}
\egroup
\caption{Average log-likelihood comparison in four datasets~\citep{asuncion2007uci}.
The top rows includes dataset size and dimensionality, bottom rows are normalized by sampling.
The upper section includes methods that estimate normalized densities.
Results of~\citet{li2019learning} are read from the bar plots reported in their article.
*Mixture of Gaussions. Best performances are in bold.}
\label{tbl:densityestimation}
\end{table*}





\paragraph{Discussion.}

Our approach relies on a key hyperparameter  that determines the training noise for the DDE, which we currently set manually. In the future we will investigate strategies to determine this parameter in a data-dependent manner. Another challenge is to obtain high-quality results using complex, high-dimensional data such CIFAR or high-resolution images. In practice, one strategy is to combine our approach with latent embedding learning methods~\citep{pmlr-v80-bojanowski18a}, in a similar fashion as proposed by~\citet{Hoshen_2019_CVPR}. The robustness of our technique with very high-dimensional data could potentially also be improved by leveraging slicing techniques~\citep{Song2019SSM,Wu_2019_CVPR}. Finally, we uses three networks to learn a generator (a DDE each for the input and generated data, and the generator). Our generator training approach, however, is independent of the type of density estimator, and techniques other than DDEs could also be used. 


\section{Conclusions}

We presented a novel approach to learn generative models using denoising density estimators (DDE), and our theoretical analysis proves that our training algorithm converges to a unique optimum.
Further, our technique 
does not require specific neural network architectures or ODE integration. We achieve state of the art results on a standard log-likelihood evaluation benchmark compared to recent techniques based on normalizing flows, continuous flows, and autoregressive models, and we demonstrate successful generators on diverse image data sets. Finally, a quantitative evaluation using the stacked MNIST data set shows that our approach avoids mode collapse similarly as state of the art Wasserstein GANs.



\section*{Broader Impact}

We propose a novel generative model that also provides a density estimate, which has potentially very broad applicability with many types of data. Generative models could be used to easily author visual media, for example, which would allow individuals or small teams to create content for education, training, or entertainment very easily. The density estimate could be used to leverage the generative model as a prior in highly underconstrained inverse problems, such as image or video restoration and various computational imaging techniques. Nefarious applications include deepfakes that attempt to mislead the consumers of the generated content. Generative models also replicate any biases that are inherent in the training data.

\bibliographystyle{apalike}
{\small
\bibliography{DDE}}

\end{document}
