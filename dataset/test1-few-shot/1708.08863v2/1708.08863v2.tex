\documentclass{article}





\usepackage[preprint]{nips_2018}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{graphicx}
\usepackage{psfrag}
\usepackage[usenames, dvipsnames]{color}
\usepackage{amsmath}
\usepackage{amsbsy}
\usepackage{color}
\usepackage{filecontents}
\usepackage[export]{adjustbox}
\usepackage{subcaption}

\usepackage{theorem}
\setlength{\theorempreskipamount}{5pt}
\setlength{\theorempostskipamount}{5pt}
\theoremheaderfont{\upshape\bfseries} \theoremstyle{plain}

\theorembodyfont{\rmfamily}

\newtheorem{test}{Test case} \def\testref#1{Test Case~\ref{#1}}

\theoremheaderfont{\itshape}
\newtheorem{remark}{Remark} \def\remref#1{Remark~\ref{#1}}
\newtheorem{example}{Example}
\newtheorem{exercise}{Exercise}
\newtheorem{property}{Property}
\newtheorem{definition}{Definition}

\newtheorem{theorem}{Theorem}
\newtheorem{proof}{Proof} 
\renewcommand\theproof{\unskip}
\newtheorem{proposition}{Proposition}
\newtheorem{comment}{Comment}
\newtheorem{lemma}{Lemma} \def\lemref#1{Lemma~\ref{#1}}
\newtheorem{corollary}{Corollary}


\newcommand{\comm}[1]{}
\renewcommand*{\thefootnote}{\fnsymbol{footnote}}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\title{Gradual Learning of Recurrent Neural Networks}



\author{
  Ziv Aharoni \\
  Department of Electrical Engineering\\
  Ben-Gurion University\\
  Beer-Sheva, Israel 8410501\\
  \texttt{zivah@post.bgu.ac.il} \\
\And
   Gal Rattner \\
   Department of Electrical Engineering \\
   Ben-Gurion University\\
   Beer-Sheva, Israel 8410501\\
   \texttt{rattner@post.bgu.ac.il} \\
   \AND
   Haim Permuter \\
   Department of Electrical Engineering \\
   Ben-Gurion University\\
   Beer-Sheva, Israel 8410501\\
   \texttt{haimp@bgu.ac.il} \\
}

\begin{document}

\maketitle

\begin{abstract}
Recurrent Neural Networks (RNNs) achieve state-of-the-art results in many sequence-to-sequence modeling tasks. 
However, RNNs are difficult to train and tend to suffer from overfitting. 
Motivated by the Data Processing Inequality (DPI), we formulate the multi-layered network as a Markov chain, introducing a training method that comprises training the network gradually and using layer-wise gradient clipping.
We found that applying our methods, combined with previously introduced regularization and optimization methods, resulted in improvements in state-of-the-art architectures operating in language modeling tasks.

\end{abstract}

\section{Introduction}
Several forms of Recurrent Neural Network (RNN) architecture, such as LSTM \cite{HOCHREITER} and GRU \cite{cho2014enc}, have achieved state-of-the-art results in many sequential classification tasks \cite{CHO,Hypernetworks,Resnets,Dropin,InitAndMomentum,RHN} during the past few years. 
The number of stacked RNN layers, i.e. the network depth, has key importance in extending the ability of the architecture to express more complex dynamic systems \cite{Bianchini, Montufar}. However, training deeper networks poses problems that are yet to be solved.
\par Training a deep RNN network to exploit its performance potential can be a very difficult task. 
One of the problems in training deep networks (not recurrent necessarily) is the degradation problem, as studied in \cite{Resnets}. 
Moreover, RNNs are exposed to exponential vanishing or exploding gradients through the Back-Propagation-Through-Time (BPTT) algorithm. 
Many studies have attempted to address those problems by regularizing the network \cite{RNNBatchNorm,VariationalDropout,Zaremba}, using different layer initialization methods \cite{GreedyLayerwiseInit, hinton2006fast}, or using shortcut connections between layers \cite{Resnets,Dropin,RHN}.

 An additional problem in training a deep architecture is the \textit{covariate shift}. Previous studies \cite{RNNBatchNorm, IofeSzegedy, Shimodaira} have shown that the \textit{covariate shift} has a negative effect on the training process among deep neural architectures. Covariate shift is the change in a layer's input distribution during training, also manifested as \textit{internal covariate shift}, when a network internal layer input distribution is changed due to a shallower layer training process.

\par In this paper, we revisit the constructive/incremental approach that breaks the optimization process into several learning phases. Each learning phase includes training an increasingly deeper architecture than the previous ones. 
We show that given a specific architecture is capable of approximating a specific function/dynamic system, one can train it gradually to obtain the same performance.
In this way, one can train the network gradually, reducing the deleterious effects of degradation and backpropagation problems. Additionally, we suggest that by adjusting the gradient clipping norms in a layerwise manner, we are able to improve the network performance even further by reducing the covariate shift.

In order to evaluate our method's performance, we conducted experiments over the word-level Penn Treebank (PTB) and the Wikitext-2 datasets, which are commonly used in evaluating language modeling tasks from the field of natural language processing. 
We demonstrated that combining the Gradual Learning (GL) method and layerwise regularization adjustments can significantly improve the results over the traditional training method of LSTM.  

\section{Related Research}


In the past years, the idea of incremental/constructive learning has been explored in many various previous works.
\citet{Dropin} introduced the method of gradually adding layers to the network by a \textit{Dropin} layer. 
\citet{net2net} introduced a method for expanding a feedforward network using a function preserving transformation. 
\citet{TrainingAnalysingDRNN} studied training deep RNNs and their ability to process the data at multiple timescales. 
They proposed two configurations, one of which (DRNN-1O) comprises the same training scheme as that we present in our work. 
The main difference from the preceding works is that we do not intend to find the best incremental training scheme for training a RNN (or DNN). Instead, we seek to claim that using these methods could yield optimal models, and due to the difficulties of training deep networks, using these methods could yield improved results.

\citet{GreedyLayerwiseInit} proposed that a proper initialization of the network parameters by a layer-by-layer greedy unsupervised initialization process could recognize features in the input structure that would be valuable to the classifier at the network's output. This process is done by a reconstruction objective that encourages the network states to preserve all the information of the input. This constitutes the main difference from our method that seeks to preserve only the information of the inputs that is relevant for estimating the labels.

\citet{IofeSzegedy} and \citet{RNNBatchNorm} introduced and addressed the covariate shift problem in deep architectures, using batch normalization as a straightforward solution by fixing the layer activation distribution. 
While this method may be effective in overcoming the covariate shift phenomena and claimed to accelerate the training process, it poses restrictions such as large batch size. 
Our work approaches this issue from a different perspective and suggests a way to reduce the covariate shift by clipping the norm of the update step per layer while training the network gradually. 
Our proposed methods avoid the addition of excessive normalization layers and allow training using smaller batches, such that the training process is slower yet ends up with improved performance.

\section{Notation}
Let us represent a network with  layers as a mapping from an input sequence  to an output sequence  by 

where  represents the composition of  over . 
The term  denotes the network parameters, such that  is the parameters of the  layer, that is . The term  denotes the parameters of the softmax mapping  that is applied to a network with  layers. 
For convenience, we denote .
We also define the  layer \textit{state sequence} by ,
where , and we define the  layer cost function by .
Next, we define the gradient vector with respect to  by , and the gradient vector of the  layer parameters with respect to  by .


\section{Gradual Learning}
In this section, we discuss a theoretical motivation that explains why a greedy training scheme is a reasonable approach for training  a deep  neural network. 
Then, we elaborate on the implementation of the method.

\subsection{Theoretical motivation}

The structure of a neural network comprises a sequential processing scheme of its inputs. This structure constitutes the Markov chain
 
This Markov chain has an elementary and well-known property that is used in our analysis, hence it is given without a proof.
\begin{property} \label{prop_markov}
Given a Markov chain , for any ordered triplet , such that , the Markov chain  holds.
\end{property}
Hence, for every  we can consider the Markov chain  that induces the conditional probabilities  and . 
Note that within the scope of training a neural network,  can be factorized by
 
where the terms  are induced entirely from the underlying distribution that generated the data, namely . Yet, the term  is determined by the network parameters. Hence, any modification of the joint distribution  could be achieved only by modifying . In particular, this means that  is also affected by the network through the term , as shown below: 
 
Next, we estimate  by the Maximum Likelihood Estimator (MLE) or, equivalently, by the negative log loss function.
Given a training set of  examples  drawn i.i.d from an unknown distribution , we want to estimate  by a -layered neural network that is parameterized by . Let us denote the estimator by , where . Thus, the MLE is given by

where  denotes . Next, we optimize the maximum likelihood criteria. 
The following theorem is well-known, but for completeness we present a proof which is helpful for understanding the subsequent arguments.
\begin{theorem}[MLE and minimal negative log-likelihood] \label{opt_ll}
Given a training set of  examples  drawn i.i.d from an unknown distribution , the MLE is given by  and the optimal value of the  criteria is .
\end{theorem}
\begin{proof}
By the law of large numbers, the maximum likelihood criteria in \eqref{mle_arg} converges to

where,  denotes the conditional entropy of  given  and  denotes the Kullback–Leibler (KL) divergence between  and . Due the non-negativity of the KL divergence, and since only  depends on , the negative log likelihood is minimized when , which happens if and only if . 
Hence, the MLE is  and the optimal bound for the negative log-likelihood loss is . 
\end{proof}
Thus, under the optimality conditions of Theorem \eqref{opt_ll} we obtain that the estimate for , that is , satisfies .
We proceed by using the data processing inequality, which is is given as follows.
\begin{theorem}[Data-processing inequality {\cite[section 2.8]{cover2012elements}}]
For  random variables, if the Markov relation  holds, then , where  denotes the mutual-information between  and .
\end{theorem}

By the Data Processing Inequality (DPI) and Property \eqref{prop_markov}, we can claim that for all  the following statement hold

This means that by processing the inputs , one cannot increase the information between  and .

Next, we show that under the conditions of Theorem \eqref{opt_ll}, Equation \eqref{transform_info} holds with equality.
\begin{theorem} \label{thm:gl}
    If  satisfies the optimality conditions of Theorem \eqref{opt_ll}, then .
\end{theorem}
\begin{proof}
By the Markov relation of the network \eqref{nn-markov} and Property \eqref{prop_markov} we can say that

and hence, by definition,
 
By the optimatilty condition of Theorem \eqref{opt_ll} we can say that 
 
Combining \eqref{y-x-t} and \eqref{y|x=y|t} we get
 
which shows by definition that the following Markov chain holds:
 
According to the Markov relations \eqref{mark:y-x-t} and \eqref{mark:y-t-x} the following hold:

Therefore, by definition,  and . Hence, 

where the equalities follow the chain rule for mutual information as given in \cite[Section~2.5.2]{cover2012elements}. 
Thus we concluded that . According to the DPI we can claim directly that  , as desired. 
\end{proof}

We showed that a necessary condition to achieve the MLE is that the network states, namely , will satisfy . 
This means that an optimal model with  layers will satisfy that all the relevant information about  within  must pass through every layer of the network, in particular layer 1. 
Due to the fact that shallow networks are easier to train, and since minimizing cross-entropy "drives" the network states to contain all the relevant information about  (Theorem \eqref{thm:gl}), we propose a greedy training scheme. 
In this way we can overcome the difficulties of training a deep architecture and simultaneously assure that an optimal model could be achieved.

\begin{comment}
    We cannot guarantee that after training a layer, say layer , the negative log likelihood is minimized, either because of the model limitation or because the optimization yielded a local minimum and not a global one. 
    Hence, we cannot claim  that we maximized the mutual information between  and . 
    But since this is a necessary condition to achieve the optimal condition of Theorem \eqref{opt_ll} (and not a sufficient condition), we assume that by the end of training, when the negative log-likelihood is small,  is maximized (or relatively close to it).
\end{comment}

\begin{figure}[!ht]
  \centering
  \psfrag{A}[c][][.65]{layer 1}
  \psfrag{B}[c][][.65]{layer 2}
  \psfrag{C}[c][][.65]{layer 3}
  \psfrag{D}[c][][.65]{softmax 1}
  \psfrag{E}[c][][.65]{softmax 2}             
  \psfrag{F}[c][][.65]{softmax 3}             
  \psfrag{G}[c][][.65]{cost 1}              
  \psfrag{H}[c][][.65]{cost 2}    
  \psfrag{I}[c][][.65]{cost 3}             
  \psfrag{a}[c][][.65]{}    
  \psfrag{b}[c][][.65]{}      
  \psfrag{c}[c][][.65]{}        
  \psfrag{d}[c][][.65]{}        
  \psfrag{e}[c][][.65]{}        
  \psfrag{f}[c][][.65]{}        
  \psfrag{g}[c][][.65]{}        
  \psfrag{h}[c][][.65]{}        
  \psfrag{i}[c][][.65]{initialize}        
  \psfrag{x}[c][][.8]{phase 1}          
  \psfrag{y}[c][][.8]{phase 2}          
  \psfrag{z}[c][][.8]{phase 3}                
  \includegraphics[scale=0.4]{GL.eps}
  \caption{\textbf{Depiction of our training scheme for a 3 layered network.} At phase 1 we optimize the parameters of layer 1 according to cost 1. At phase 2, we add layer 2 to the network, and then we optimize the parameters of layers 1,2, when layer 1 is copied from phase 1 and layer 2 is initialized randomly.  At phase 3, we add layer 3 to the network, and then we optimize all of the network's parameters, when layers 1,2 are copied from phase 2 and layer 3 is initialized randomly.}
  \label{learning_scheme}
\end{figure}

\subsection{Implementation}

Now, motivated by the conclusions of the preceding section, we propose to break up the optimization process into  phases, as the number of layers, optimizing  sequentially as  increases from 1 to L.
In each phase , the network is optimized with respect to  until convergence. 
Once the training of phase  is done, phase  is initiated with layers  initialized with weights  learned in the previous phase, and  is initialized randomly. Initialization of the softmax layer's weights  can be done either randomly or by inheritance of  from the preceding training phase. An example for a training scheme is depicted in Figure \ref{learning_scheme}.




\section{Layer-wise Regularization Adjustments}
Driven by the Markov chain realtion of the neural network and the DPI, we conclude that exploiting the training procedure at any of the training phases will be most beneficial. Adjusting hyper-parameters for each training phase separately, such that it improves the minimization of the loss function is a necessary step to ensure the quality of our method implementation. However, adjusting the hyper-parameters might increase the robustness of the experiments and add variance to the experiments results as suggested in \cite{melis2017state}.

\subsection{Layer-wise Gradient Clipping (LWGC)} \label{lwgc}
Performing gradient clipping over the norm of the entire gradient vector , as suggested in \cite{pascanu}, may cause a contraction of the relatively small elements in the gradient vector on each update step, due to presence of much larger gradient elements in the vector that are much more dominant in the squared elements sum of the global norm . Due to this phenomenon, global gradient clipping, although proven to be useful and widely used in many architectures \cite{Regularizing_Optimizing_LSTM_LM, mos,Zaremba}, may have negative impact on the update step size for some weights elements in the network.
Clipping the gradients of each layer separately will eliminate the influence of gradient contraction between the different layers of the network.
 Global gradient norm clipping of vector  is formulated as , where  is the fixed maximum gradient norm hyper-parameter.
  


  
\begin{figure}[!ht]
  \centering
  \includegraphics[scale=0.35, center]{latex_fig.eps}
  \caption{Depiction of the \textit{covariate shift} reduction at the initial stage of the training process, offering a comparison of the histograms of the third layer activations during the first 500 updates on training. Figure (a) shows the histogram of an AWD-MoS-LSTM network on traditional training. Figure (b) shows the histogram of the AWD-MoS-LSTM network trained using GL-LWGC.}
  \label{covariate-shift}
\end{figure}


When training layers gradually, at the beginning of each training phase the randomly initialized newly added layer tends to have a significantly larger gradient norm compared to the shallower pre-trained layers. Considering the differences in the layers' gradient norms, we suggest that treating each layer weights' gradient vector individually and clipping the gradients vector layerwise can reduce internal covariate shift significantly, as depicted in Figure \ref{covariate-shift}. 
In our experiments, we clipped each layer's gradient separately, increasing the clipping norm as the layer is deeper in the network. Moreover, we used a strictly low gradient clipping norm on the encoder matrix to restrict the entire network's covariate shift during training.

The formulation of the LWGC method for a network with  layers is given by 
 where  is the gradient vector w.r.t the weights of layer , and  is a fixed maximum gradient hyper-parameter of layer .

\section{Experiments}
We present results on two datasets from the field of natural language processing, the word-level Penn Tree-Bank (PTB) and Wikitext-2 (WT2). We conducted most of the experiments on the PTB dataset and used the best configuration (except for minor modifications) to evaluate the WT2 dataset.

Following previous work \cite{melis2017state}, we established our experiments based on the pytorch implementation of \citet{mos}.
We have added our methods (GL, LWGC) to the implementation, closely following the hyper-parameter settings, for fair comparison. 
This implementation includes variational dropout as proposed by \cite{VariationalDropout}, Weight Tying (WT) method as proposed in \cite{WT}, the regularization and optimization techniques as proposed in \cite{Regularizing_Optimizing_LSTM_LM} and the mixture of softmaxes as proposed by \cite{mos}. 
Dynamic evaluation \cite{dynamicEval} was applied on the trained model to evaluate the performance of our model compared to previous state-of-the-art results.

We conducted two models in our experiments, a \textit{reference} model and a \textit{GL-LWGC LSTM} model that was used to check the performance of our methods. The \textit{reference} model is 3 layered LSTM optimized and regularized with the properties described in \cite{mos}\footnote[3]{https://github.com/zihangdai/mos} with a slight change in the form of enlarging the third layer from 620 to 960 cells, in order to even the total number of parameters. The model was trained for 1000 epochs until the validation score stopped improving. Our reference model failed to improve the previous results presented by \cite{mos}. 

\begin{table}[!ht] 
  \caption{Single model test perplexity of the PTB dataset}
  \centering
  \begin{tabular}{lllll}
    \toprule
    \textbf{Model}	& \textbf{Size} 	& \textbf{Valid} & \textbf{Test} \\
    \toprule
    
    \citet{RHN} - Variational RHN + WT 		 						    & 23M & 67.9 & 65.4 \\
    \citet{NAS} - NAS & 25M & - & 64.0 \\
    \citet{melis2017state} - 2-layer skip connection LSTM & 24M &  60.9 & 58.3 \\
    \citet{Regularizing_Optimizing_LSTM_LM} - AWD-LSTM                  & 24M & 60.0 & 57.3 \\
    \citet{mos} - AWD-LSTM-MoS & 22M & 58.08 & 55.97\\
    \citet{mos} - AWD-LSTM-MoS + finetune & 22M & 56.54 & 54.44\\
    \midrule
    Ours - Reference Model         & 26M &  57.41       & 55.58 \\
Ours - 2-layers GL-LWGC-AWD-MoS-LSTM + finetune & 19M & 55.18 & 53.54\\
    Ours - GL-LWGC-AWD-MoS-LSTM & 26M & 54.57 & 52.95 \\
    Ours - GL-LWGC-AWD-MoS-LSTM + finetune & 26M & \textbf{54.24} & \textbf{52.57} \\
    \midrule
    \citet{dynamicEval} AWD-LSTM + dynamic evaluation\footnote[2] & 24M & 51.6 & 51.1 \\
    \citet{mos} AWD-LSTM-MoS + dynamic evaluation\footnote[2] & 22M & 48.33 & 47.69 \\
    Ours - GL-LWGC-AWD-MoS-LSTM + dynamic evaluation\footnote[2] & 26M & \textbf{46.64} & \textbf{46.34} \\
    \bottomrule
  \end{tabular}
  \label{res_table}
\end{table}

\begin{table}[!ht] 
  \caption{Single model perplexity of the WikiText-2 dataset}
  \centering
  \begin{tabular}{lllll}
    \toprule
    \textbf{Model}	& \textbf{Size} 	& \textbf{Valid} & \textbf{Test} \\
    \toprule
    \citet{melis2017state} - 2-layer skip connection LSTM & 24M &  69.1 & 65.9 \\
    \citet{Regularizing_Optimizing_LSTM_LM} - AWD-LSTM + finetune                 & 33M & 68.6 & 65.8 \\
    \citet{mos} - AWD-LSTM-MoS + finetune & 35M & 63.88 & 61.45\\
    \midrule
    Ours - GL-LWGC-AWD-MoS-LSTM & 35M & 63.59 & 61.27 \\
    Ours - GL-LWGC-AWD-MoS-LSTM + finetune & 38M & \textbf{62.79} & \textbf{60.54} \\
    \midrule
    \citet{dynamicEval} AWD-LSTM + dynamic evaluation\footnote[2] & 33M & 46.4 & 44.3 \\
    \citet{mos} AWD-LSTM-MoS + dynamic evaluation\footnote[2] & 35M & 42.41 & 40.68 \\
    Ours - GL-LWGC-AWD-MoS-LSTM + dynamic evaluation\footnote[2] & 38M & \textbf{42.19} & \textbf{40.46} \\
    \bottomrule
  \end{tabular}
  \label{wiki_res_table}
\end{table}
In our \textit{GL-LSTM} model we applied  the GL method with 3 training phases, at each we added a single layer of 960 cells and applied LWGC with an increasing maximum gradient norm at deeper layers. The LWGC max gradient norm for the LSTM and softmax layers was set in the range of 0.12-0.17, and for the embedding matrix a maximum gradient norm of 0.035-0.05 was set in order to lower the covariate shift along the training process. Other than that, regularization methods and hyper-parameter settings similar to those of the \textit{reference} model were used. Our GL-LSTM model overcame the state-of-the-art results with only two layers and 19M parameters, and further improved the state-of-the-art results with the third layer phase. Results of the \textit{reference} model and \textit{GL-LWGC LSTM} model are shown in Table \ref{res_table}. 

Experiments on the WT2 database were conducted with the same parameters as were used with the PTB model except for enlarging the embedding size to 300 units and changing the LSTM hidden size to 1050 units. Results of the WT2 model are shown in Table \ref{wiki_res_table}.

\section{Ablation Analysis}
In order to evaluate the benefit gained by each of our proposed methods, we measured the performance of our best-performing model removing one of the methods each time. Other than that, in order to provide a fair comparison with the previously suggested state-of-the-art methods \cite{mos} we evaluated a reference AWD-LSTM-MoS model with an enlarged third layer, to make the comparison with the same number of parameters as in our models. Table \ref{ablation_table} shows validation and test results for the ablated models.

\footnotetext[2]{Marking dynamic eval methods that update the model while evaluating, to distinguish from static evaluation models.}

\begin{table}[!ht] 
  \label{table:ablation}
  \centering
  \begin{tabular}{llll}
  \toprule
    & \quad \quad \quad PTB &  \\
    \textbf{Model} 	& \textbf{Valid} & \textbf{Test} \\
    \midrule
    \midrule
    GL-LWGC-AWD-MoS-LSTM    &  54.24     & 52.57 \\
    \toprule
    w/o fine-tuning            &  54.57    & 52.95 \\
    w/o LWGC                  &  56.32    & 54.09 \\
    w/o GL                    &  55.43    & 53.70 \\

    \bottomrule
    \hfill
  \end{tabular}
  \caption{Ablation analysis of the best-performing LSTM models over the Penn Treebank dataset. All models with 26M parameters.}
  \label{ablation_table}
\end{table}

Performance measurement of the GL ablated model, required setting the hyper-parameters ahead of initializing the training process. In this case we set the hyper-parameters as in the last phase of training of our best-performing GL-LWGC model, yet allowing a larger number of epochs to exploit the convergence. The ablated LWGC model was trained in three phases, each equivalent in length and hyper-parameter settings to the parallel phase in the best-performing GL-LWGC model, except for the global gradient norm clipping. While training the ablated LWGC model we set the maximum global gradient norm to be 
   ,  
where  is the maximum gradient norm for element  in phase  using LWGC, and  is the maximum gradient norm for the non-LWGC case in phase .

Each of the ablated GL and ablated LWGC models outperformed the previous state-of-the-art results, Yet combining both of the methods showed improved results.
The ablation analysis shows that the LWGC method has a stronger impact on the final results, while the GL method reduces the effectiveness of premature fine-tuning. Applying fine-tuning on the ablated LWGC model was not effective.  

We tested several hyper-parameters settings for our reference model, yet failed to improve the results presented by \cite{mos}. This result empowers our conclusions that the GL method decreases the degradation problem caused by increasing the depth or layer size of a model. 

\section{Conclusions}
We presented an effective technique to train RNNs. GL increases the network depth gradually as training progresses, and LWGC adjusts a gradient clipping norm in a layerwise manner at every learning phase of the training. Our techniques are implemented easily and do not involve increasing the amount of parameters of a network. We demonstrated the effectiveness of our techniques on the PTB and WikiText-2 datasets. We believe that our techniques would be useful for additional neural network architectures, such as GRUs and stacked RHNs, and in additional settings, such as in reinforcement learning.

\medskip

\small


\bibliographystyle{plainnat}
\bibliography{main}

\end{document}
