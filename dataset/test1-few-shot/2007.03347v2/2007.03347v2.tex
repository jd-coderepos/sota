
\documentclass[journal]{IEEEtran}
\usepackage{blindtext, graphicx}
\usepackage{multirow}
\usepackage{cite}
\usepackage[ruled,vlined,commentsnumbered,linesnumbered,lined,boxed]{algorithm2e}
\usepackage{algorithmic}
\usepackage{amsmath}


\newcommand\tab[1][1cm]{\hspace*{#1}}














\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}

\title{SpinalNet: Deep Neural Network with \\Gradual Input}
\author{H M Dipu Kabir, Moloud Abdar, Seyed Mohammad Jafar Jalali, \emph{Student Member, IEEE};\\ Abbas Khosravi, \emph{Member, IEEE}; Amir F Atiya, \emph{Senior Member, IEEE};\\ Saeid Nahavandi, \emph{Fellow, IEEE};  Dipti Srinivasan, \emph{Fellow, IEEE}.

\thanks{H M Dipu Kabir, Moloud Abdar, Seyed Mohammad Jafar Jalali, Abbas Khosravi, and Saeid Nahavandi are with Institute for Intelligent Systems Research and Innovation (IISRI), Deakin University, Australia. (Email: \{hussain.kabir, mabdar, sjalali, abbas.khosravi, saeid.nahavandi\}@deakin.edu.au)}

\thanks{Amir F Atiya in with Cairo University. Email: amir@alumni.caltech.edu}

\thanks{Dipti Srinivasan is with National University of Singapore. Email: dipti@nus.edu.sg}


\thanks{Manuscript received  --, 2020;}
\thanks{accepted-------.}
}













\maketitle


\begin{abstract}
Over the past few years, deep neural networks (DNNs) have garnered remarkable success in a diverse range of real-world applications. However, DNNs consider a large number of inputs and consist of a large number of parameters, resulting in high computational demand. We study the human somatosensory system and propose the SpinalNet to achieve higher accuracy with less computational resources. In a typical neural network (NN) architecture, the hidden layers receive inputs in the first layer and then transfer the intermediate outcomes to the next layer. In the proposed SpinalNet, the structure of hidden layers allocates to three sectors: 1) Input row, 2) Intermediate row, and 3) output row. The intermediate row of the SpinalNet contains a few neurons. The role of input segmentation is in enabling each hidden layer to receive a part of the inputs and outputs of the previous layer. Therefore, the number of incoming weights in a hidden layer is significantly lower than traditional DNNs. As all layers of the SpinalNet directly contributes to the output row, the vanishing gradient problem does not exist. We also investigate the SpinalNet fully-connected layer to several well-known DNN models and perform traditional learning and transfer learning. We observe significant error reductions with lower computational costs in most of the DNNs. We have also obtained the state-of-the-art (SOTA) performance for QMNIST, Kuzushiji-MNIST, EMNIST (Letters, Digits, and Balanced), STL-10, Bird225, Fruits 360, and Caltech-101 datasets. The scripts of the proposed SpinalNet are available with the following link: https://github.com/dipuk0506/SpinalNet 


 
\end{abstract}

\begin{IEEEkeywords}
DNN, CNN, AdaNet, ResNet, VGG, Transfer Learning.
\end{IEEEkeywords}




\IEEEpeerreviewmaketitle



\section{Introduction}


Deep Neural Networks (DNNs) have brought the state of the art performance in various scientific and engineering fields \cite{byerly2020branching, ciregan2012multi, bengio2013representation, kowsari2018rmdl}. DNNs usually have a large number of input features, as the consideration of more parameters usually improves the accuracy of the prediction. The size of the first hidden layer is critical. A small first hidden layer fails to propagate all input features properly while a large first hidden layer increases the number of weights drastically. Another limitation of the traditional DNNs is the vanishing gradient. When the number of layers is large, the gradient is high at parameters near output, and the gradient becomes negligible at parameters near inputs. DNN training becomes difficult due to the vanishing gradient problem.  

\begin{figure}
  \centering
  \includegraphics[width=3.4in,angle=0]{Human_sensory.png}
  \caption{ We develop SpinalNet by mimicking the human somatosensory system to receive large input efficiently and to achieve better performance. (a) Half part of the human somatosensory system, presenting how our spinal cord receives sensory signals from our body. (b) Structure of the proposed SpinalNet. The proposed NN consists of the input row, the intermediate row, and the output row. The intermediate row contains multiple hidden layers. Each hidden layer receives a portion of the input. All layers except the first layer also receive outputs of the previous layer. The output layer adds the weighted outputs of all hidden neurons of the intermediate row. The user can also construct and train a SpinalNet for any arbitrary number of inputs, intermediate neurons, and outputs.   }
  \label{Human_SN}
\end{figure}

The human brain also receives a lot of information from our skin. Numerous tactile sensory neurons send the sense of touch, heat, vibration, etc. from different parts of our body. They can sense pressure, heat, vibrations, complex-textures, hardness, state of matter, etc. \cite{okamoto2012psychophysical}. Humans can have different touch sensitivity over time. Although the exact mechanism is unknown to humans, the current knowledge base states a tremendous function of our spinal cord neurons. The human spinal cord receives senses of touch from different locations in different parts of it. Multiple vertebrae can be connected to one internal organ too. Fig. \ref{Human_SN}(a) presents simplified rough connections between human touch-sensors and the spinal cord.  Researchers develop convolutional neural networks (CNN) by mimicking the functionality of the cats' visual cortex, and that brings a significant improvement in the accuracy of NNs \cite{hubel1962receptive}. The miraculous spinal architecture of humans and the recent success of CNNs motivate us to develop a neural network with gradual inputs. 





A well-known approach to reducing computation is pooling \cite{lawrence1997face}. However, pooling causes loss of information. 
Popular solutions to the vanishing gradient problem are ResNet and DenseNet. They allow shortcut connections over different layers. Therefore, the gradient remains high at neurons near the input \cite{he2016deep}. ResNets always provide better performance with increasing depth and can be as deep as thousands of layers. However, there is a slight marginal improvement in ResNet with increased depth. Moreover, very deep ResNets have a problem of diminishing feature reuse. Therefore, Sergey et al. proposed wide residual networks \cite{zagoruyko2016wide } and achieved superior performance.  Zifeng et al. also received superior performance with shallow and wide NNs \cite{wu2019wider}. Gao et al. propose DenseNet, where all layers are connected \cite{huang2017densely}. DenseNet training is faster and provides better performance in most situations due to two reasons i) all layers of DenseNet are connected, ii) they made dense-net narrower than the ResNet. When all layers are connected, the gradient and feature-reuse do not vanish over layers.  However, as all layers are connected, an increment of the network size by one layer needs connections to that layer from all existing layers. Therefore, deep DenseNets are computationally expensive. Adaptive Structural Learning of Neural Networks(AdaNet) performs both connecting neurons and optimizing weights during the training. Consideration of all possible connections of a DNN is computationally intensive. The inauguration of a new neuron requires the consideration of connecting the neuron to other neurons. Therefore, AdaNet is suitable for shallow NNs \cite{cortes2017adanet, kabir2019partial}. 


Although DNNs have achieved SOTA performances in several fields, DNNs still suffers from a large computational overhead during training and execution \cite{jalali2019optimal}. This paper proposes the SpinalNet, presented in Fig. \ref{Human_SN}(b) to improve performance with smaller computational overhead. The proposed structure with gradual and repetitive input capabilities enables NNs to achieve promising results with fewer parameters. We investigate the proposed SpinalNet as the fully connected layer of the VGG-5 network and receive SOTA performance in four MNIST datasets. We apply the transfer learning with SpinalNet fully connected layer and receive SOTA performance in STL-10, Bird225, Fruits 360, and Caltech-101 datasets. We investigate SpinalNet and its variations in more than seventeen different datasets. We also receive promising results in other datasets with at least one variant of SpinalNet.

We organize the rest of the paper as follows: Section II presents the theoretical background of SpinalNet. The section discusses the similarity between the human spinal cord and the proposed SpinalNet, proves the universal approximation of the SpinalNet, and discuss the transfer learning. Section III reports the experimental results of SpinalNet with other competitors for solving regression and classification problems, Section IV presents the prospects of SpinalNet, and Section V is the concluding section.

\section{Theoretical Background}
Generally, there are two types of DNNs: convolutional and non-convolutional. The structure of non-convolutional NNs consist of fully connected input, hidden, and output layers. Deep CNNs contain convolutional layers and fully-connected layers. The number of parameters increases drastically due to the convolution and flattening procedures. The task of pooling is in reducing the number of network parameters \cite{wiatowski2017mathematical}. According to Vogt's work \cite{vogt2019overview}, there are two main points to the importance of pooling layers in DNN methods. Firstly, pooling declines the size of input for the next layer but it allows learning more mappings. Second, pooling assists in combining the output obtained from the prior layer on a large scale. However, pooling also causes the loss of information, as it considers only the aggregation of nearby values \cite{xie2015simple, vogt2019overview}. The information gathered from pooling layer is converted to a one-dimensional vector through flattening. At the end, fully-connected layers compute the output from the one-dimensional vector.

\begin{figure}
  \centering
  \includegraphics[width=2.7in,angle=0]{Brachial_plexus.png}
  \caption{A portion of the human nerve plexus; known as the brachial plexus. The information of any touch or pain reaches the brain through the nerve plexus and the spinal cord. Nerve plexus is a network of intersecting nerves. Our spinal cord receives information gradually. Here, C5-C8 and T1 are vertebrae in the human skeleton \cite{chang2019vascularized}.}
  \label{Hsensory}
\end{figure}

\subsection{Human Somatosensory System and the Spinal Cord}
Although the exact mechanism of the human somatosensory system is not well understood, we find several similarities between the human spinal cord and the proposed neural network \cite{d2008spinal}. Features we aimed to mimic are as follows:
\begin{enumerate}
	\item Gradual input and nerve plexus. 
	\item Voluntary and involuntary actions.
	\item Attention to pain intensity. 
\end{enumerate}


Sensory neurons reach the spinal cord through a complex network, known as nerve plexus. Fig. \ref{Hsensory} represents a portion of nerve plexus. A single vertebra does not receive all of the information. The tactile sensory network covers millions of sensors. Furthermore, the human tactile system is more stable compared to the vision or the auditory systems, as there are a fewer number of touch-blind patients, compared to the number of blind patients. The nerve plexus network sends all tactile signals to the spinal cord gradually. Different locations of a spinal cord receive the pain of leg and the pain of hand \cite{d2008spinal}. 
The neurons existing in the vertebral column are responsible for transferring the sense of touch to the brain and may take some actions. Our brain can control the spinal neurons to increase or decrease the pain intensity \cite{sprenger2012attention}. Sensory neurons may also convey information to the lower-motor before getting instruction from the brain. This miraculous procedure is called involuntary or reflex movements.






\subsection{Proposed SpinalNet}

The proposed SpinalNet has the following similarities with the above-mentioned features of the human spinal cord.
\begin{enumerate}
    \item Gradual input 
    \item Local output and probable global influence
    \item Weights reconfigured during training
\end{enumerate}
Similar to our spinal cord, the proposed SpinalNet takes inputs gradually and repetitively. Each layer of the SpinalNet contributes towards the local output (reflex). The SpinalNet also sends a modulated version of inputs towards the global-output (brain). The NN training process configures weights based on the training data, similar to our brain, configuring the spinal neurons for tuning the pain sensitivity of different sensories of our body.





Fig. \ref{Human_SN} demonstrates the structure of the proposed SpinalNet. The network structure consists of an input row, an intermediate row, and an output row. The input is split and sent to the intermediate row of multiple hidden layers. In Fig. \ref{Human_SN}, the intermediate row, and the output row contain two neurons per each hidden layer. The number of output neurons per each hidden layer is equal to the number of outputs. The number of intermediate neurons can be changed according to the user. However, both the number of intermediate neurons and the number of inputs per layer is regularly kept small to reduce the number of multiplication. As typically the number of inputs and the intermediate hidden neurons per layer allocates a small amount, the network may have an under-fit shape. As a consequence, each layer receives inputs from the previous layer. Since the input is repeated, if one important feature of input does not impact on the output in one hidden layer, the feature may impact the output in another hidden layer. The intermediate row contains a nonlinear activation function and the output row contains the linear activation function. 
In Fig. \ref{Human_SN}(b), the input values are split into three rows. These rows are assigned to the different hidden layers in a repeated manner. 

\subsection{Universal Approximation of the Proposed SpinalNet}
\begin{figure}
  \centering
  \includegraphics[width=3.5in,angle=0]{UA_one_layer.png}
  \caption{The visual proof of the universal approximation of the SpinalNet. A simplified version of SpinalNet in (a) can act as a NN of a single hidden layer, drawn in (b). Similarly, a 4 layer SpinalNet in (d) can be equivalent to a NN of one hidden layer (HL), containing four neurons, shown in (c). }
  \label{UApprox}
\end{figure}
Proposing a new NN architecture raises a question about its universal approximability \cite{lin2018resnet,fan2020universal}. Therefore, we prove the universal approximation. The traditional mathematical proof of the universal approximation theorem contains scholarly and esoteric equations. However, we aim to make the paper equation-free to attract general audiences. Therefore, we prove the universal approximation with the following approach.
\begin{enumerate}
  \item Single hidden layer of a NN with large width is a universal approximator \cite{csaji2001approximation}.
  \item If we can prove that SpinalNet of a large number of layers is equivalent to the single hidden layer of a large width NN, the universal approximation is proved.
\end{enumerate}  



Fig. \ref{UApprox} presents how a simpler version of SpinalNet can be equivalent to a single hidden layer NN. In Fig. \ref{UApprox}(a), a SpinalNet with two hidden layers (HLs), each layer containing two neurons is simplified. The neurons of the first layer contain the purely linear function. Therefore, the first layer gives only the weighted sum of  to  inputs. Outputs of each hidden neuron for the first hidden layer only go to a similar neuron of the second hidden layer. Cross connections and connections from the first hidden layer to output are disconnected by assigning zero weights. The second hidden layer receives the weighted summation from  to . It also receives the weighted summation from   to  of the previous layer. Then, the neurons of this layer apply an activation function to the weighted sum of  to . Therefore, these two layers are equivalent to a neural network of single HL, containing two hidden neurons, shown in Fig. \ref{UApprox}(b). A simplified version of SpinalNet with four HLs, containing two neurons in each layer is shown in Fig. \ref{UApprox}(d). Similarly, the SpinalNet in Fig. \ref{UApprox}(d) is also equivalent to a NN with one HL, containing four neurons in Fig. \ref{UApprox}(c). 

Similarly, a deep SpinalNet can be equivalent to a NN of a single hidden layer, containing a large number of neurons.  A NN with a single hidden layer and a large number of neurons achieves the universal approximation. Therefore, a deep SpinalNet also has universal approximability.

\subsection{Transfer Learning}
Transfer of learning is an efficient technique of using previously acquired knowledge and skills in novel problems. It is also similar to educating humans with a much broader syllabus to achieve competencies for an unpredictable future. One of the most efficient ideas of machine learning is the transfer learning(TL), which is similar to the transfer of learning in humans \cite{wolf2019transfertransfo}. DNNs requires adequate training samples for proper training. Insufficient training samples may result in poor performance. Transfer learning (TL) is an efficient DNN training technique where initial layers of DNN are pre-trained with a large dataset. The corresponding train dataset trains only a few final layers. As a result, the user can get a well-trained NN for the specific dataset of a small sample number, with lower computational overhead \cite{shao2018starcraft}.  The TL is gaining huge popularity these days due to exceptional performance and usability. Many researchers expect TL as the next driver of the commercial success of machine learning \cite{ng2016nuts}.


Several standard datasets contain insufficient training samples. CIFAR-100, Caltech-101, STL-10, etc. datasets are examples of such datasets. The traditional DNN training trains the entire DNN. According to the reported results, the traditional DNN training technique cannot achieve more than 90\% accuracy on the CIFAR-100 dataset, where TL achieves more than 93\% accuracy. 


\section{Results}
In this section, we aim to verify the effectiveness of SpinalNet for regression and classification problems. We apply different variants of the SpinalNet and receive SOTA performance in several datasets. The training procedure follows stochastic gradient descent (SGD) or Adam training technique. We upload training scripts to GitHub to help future researchers.   


\subsection{Regression Dataset}
Regression is a less popular topic among the researchers of NN compared to classification. There exist a large number of datasets and organized competition among various algorithms for the classification problem. Regregression lacks popular standard datasets and competitions. Therefore, we compare our SpinalNet with the PyTorch regression example, developed by Ben Phillips \cite{Regression}. The example considers a single input and a single output. Following the example, we apply the Adam algorithm \cite{kingma2014adam} for the optimization purpose. The loss function used in the experiments is the mean square error (MSE), the learning rate is equal to 0.01, and the number of the epoch is equal to 200. We changed the problem to eight variables and tried different combinations of variables with the same level of noise. Combinations are 1) summation of variables (), 2) sine of summation of variables (), and 3) product of variables (), and 4) sine of product of variables ().  We record the MSE at 100 and 200 epochs. The default code \cite{Regression} shows the MSE of the last epoch, but our code shows the minimum MSE among the current and previous epochs. We segment inputs into two groups, each containing four inputs.  

The number of hidden neurons in traditional NN is 300 \cite{Regression}. The number of hidden neurons in SpinalNet is also 300. The number of multiplication in traditional NN is 21700, and the number of multiplications in SpinalNet is 14000. The SpinalNet achieves a 35.5\% reduction in the number of multiplications. The number of parameters in the traditional NN is 22.00k, and the number of parameters in SpinalNet is 14.30k. 
There are eight combinations for MSE comparisons, as shown in Table \ref{Regression1}. Superior performances are highlighted as bold characters. The SpinalNet performs better in six out of eight combinations. 
 

\begin{table}
\centering
\caption{Comparison between Traditional Feed-forward NN and SpinalNet for Regression datasets. }
\label{Regression1}
\begin{tabular}{|c|c|c|c|}
\hline
 Neural Network & Data & \multicolumn{2}{|c|}{MSE ( Unit)}    \\ \cline{3-4}
                &      & 100 Epoch & 200 Epoch    \\ \hline 
 Feed-forward NN & 8 Var.  & 1.178&0.887    \\ \cline{2-4} 
 Two Hidden Layers& 8 Var.  &1.918&\bf{1.086}    \\ \cline{2-4}  
 200, 100 Neurons & 8 Var.  & \bf{3.875} &3.875    \\ \cline{2-4} 
 \cite{Regression} & 8 Var.  &3.403 &1.554    \\ \hline 
 
 SpinalNet & 8 Var.  &\bf{1.007}& \bf{0.855}    \\ \cline{2-4} 
 6 Hidden Layers& 8 Var.  & \bf{1.912}&1.219    \\ \cline{2-4}  
 50 Neurons Each Layer & 8 Var.  &3.966 &\bf{2.217}    \\ \cline{2-4} 
 Half Input Each Layer & 8 Var.  & \bf{0.910} &\bf{0.910}    \\ \hline 
 
\end{tabular}
\end{table}

\subsection{Classification: Learning from Scratch}
We train several existing networks and different variations of SpinalNet with random initialization on MNIST, Fashion-MNIST, KMNIST, QMNIST, EMNIST, CIFAR-10, and CIFAR-100  image classification datasets.


\subsubsection{MNIST}
The MNIST dataset is one of the most popular datasets for investigating image classification algorithms due to its simplicity and small size. We compare our SpinalNet with PyTorch CNN \cite{MNIST_CNN}. A hidden layer of fifty neurons joins them with the output. The default CNN code provides 98.17\% accuracy. We investigate the same NN with a SpinalNet fully-connected (FC) layer. The FC layer consists of six sub-hidden layers, each layer contains eight neurons. That CNN with Spinal FC provides 98.44\% accuracy. That structure brings more than a 48.5\% reduction in multiplication and a 4\% reduction in the activation functions on the fully-connected layer. The first segment in Table \ref{Perform_tab} presents results on the MNIST dataset. The SpinalNet reduces the overall number of parameters and increases performance significantly. 

As VGG models perform very well with the MNIST datasets, we incorporate the SpinalNet with the VGG-5 network \cite{VGG5}. VGG-5 with the Spinal fully connected layer provides a near state-of-the-art performance. We receive 99.72\% accuracy with VGG-5 (Spinal FC). According to our literature search, it is one of the top twenty reported accuracies. We perform the random rotation of 10 degrees and the random perspective PyTorch library functions to enhance data. 


\subsubsection{Fashion-MNIST}
The Fashion-MNIST data is quite similar to MNIST. It also contains 28  28 grayscale images and the output contains 10 classes. Therefore, MNIST codes are executable to the Fashion-MNIST data without any modification. The same NN is applied to compute CNN and CNN (Spinal FC).  

We receive 94.68\% accuracy with VGG-5 (Spinal FC). The default VGG-5 provides 94.63\% accuracy. Therefore, the Spinal FC provides better performance with a lower number of multiplications. We apply random rotation and random resized crops to enhance the data. According to our literature search, it is one of the top five reported results.



\subsubsection{Kuzushiji-MNIST}
Kuzushiji-MNIST or shortly, KMNIST is a Japanese character recognition dataset. The data format of KMNIST is also the same as the MNIST data, and the same codes can be applied. Table \ref{Perform_tab} presents the performance of codes. We receive superior performance by using the Spinal FC layer with CNN.

We receive 99.15\% accuracy with VGG-5 (Spinal FC). 
It is a new SOTA for the Kuzushiji-MNIST dataset \cite{nokland2019training,tissera2019context}. We apply the random perspective and the random rotation to enhance the data.
The data augmentation for VGG-5 and VGG-5  (Spinal FC) are the same. The default VGG-5 provides 98.94\% accuracy. Therefore, the Spinal FC provides better performance with a lower number of multiplications. 


\subsubsection{QMNIST}
QMNIST is a recently published English digit recognition dataset. QMNIST dataset contains fifty thousand test images. The dimensions of inputs and outputs are the same as the dimension of inputs and outputs of the MNIST dataset. Therefore, the same code is executable for the QMNIST dataset. The fourth segment of table \ref{Perform_tab} presents QMNIST results. The default PyTorch CNN provides 97.82\% accuracy on the QMNIST data. CNN(Spinal FC) provides 97.97\% and 98.07\% accuracy for the spinal-layer size of eight and ten respectively. 

The VGG-5 receives 99.66\% accuracy. The VGG-5 (Spinal FC) receives 99.68\% accuracy. According to our literature search, we have received SOTA performance for the QMNIST dataset. We apply the random perspective and random rotation functions to achieve these results.


\subsubsection{EMNIST}
The EMNIST dataset contains several hand-written character datasets. These datasets are derived from the NIST database and converted to a 2828 pixel grey-scale image format. The EMNIST(digits) dataset also has ten classes. The accuracy of NNs on the EMNIST (digit) data are available in the fifth segment of table \ref{Perform_tab}. The default PyTorch CNN provides 98.89\% accuracy. 
The VGG-5 provides 99.81\% accuracy and VGG-5 (Spinal FC) provides 99.82\% accuracy. According to our literature search, we have received SOTA performance for the EMNIST (digits) dataset.

The default PyTorch CNN provides 87.57\% accuracy for EMNIST (letters) dataset. 
The VGG-5 provides 95.86\% accuracy and VGG-5 (Spinal FC) provides 95.88\% accuracy. According to our literature search, we have received SOTA performance for the EMNIST (letters) dataset.

The default PyTorch CNN provides 79.61\% accuracy for EMNIST (balanced) dataset. 
The VGG-5 provides 91.04\% accuracy. VGG-5 (Spinal FC) provides 91.05\% accuracy. According to our literature search, we have received SOTA performance for the EMNIST (balanced) dataset. We apply the random perspective and the random rotation functions to enhance all EMNIST datasets.

\subsubsection{CIFAR-10 Dataset}
The CIFAR-10 dataset is less predictable compared to the original MINST dataset \cite{huang2019gpipe}. The PyTorch CNN example \cite{CR_CNN2} classifies the CIFAAR-10 data with 60\% accuracy. A SpinalNet of 6 hidden layers, each layer containing 20 neurons achieves 62\% accuracy for the same number of epochs. 5\% lower error is achieved with a 4.8\% multiplication reduction at the fully-connected layer.

The DenseNet code \cite{DenseNet_code} shared by Hasan et al. provides 77.79\% accuracy after 35 epochs. The DenseNet fully connected layer has a 512 neuron hidden layer. We train a SpinalNet of 8 hidden layers, each containing 16 neurons. The number of hidden neurons becomes one-fourth and the number of multiplication is reduced by 87\%. Still, we observe a 15\% error reduction.  Another SpinalNet has 8 hidden layers, each containing 64 neurons. The number of hidden neurons remains the same and the number of multiplication is reduced by 44.1\%. The error is reduced by 18.7\% compared to the previous DenseNet code \cite{DenseNet_code}.

The ResNet \cite{ResNet_code} example of PyTorch provides 88.35\% accuracy after 80 cycles. We achieve 88.65\% accuracy after 80 cycles and 88.93\% accuracy after 140 cycles with a SpinalNet of 4 hidden layers, each layer contains 16 neurons. However, we reduce the pooling and increase multiplication at the fully-connected layer at the default ResNet code \cite{ResNet_code}. However, the accuracy is lower with the standard ResNet blocks. The spinal fully-connected layer degrades the performance of ResNet-18 and ResNet-34 slightly. 

A consistent improvement in the NN size and performance is achieved with the VGG network. As the VGG NN contains a large fully-connected layer, the SpinalNet can act as an optimized fully-connected layer. We investigate VGG-11, VGG-13, VGG-16, and VGG-19. VGG-19 provides the highest accuracy (91.40\%) amongst the VGG networks and VGG SpinalNets. However, the ResNet-18 provides 91.98\% accuracy, and adding a Spinal fully connected layer degrades the accuracy.

According to our results, the SpinalNet brings significant improvement on NNs, containing activation functions in the fully connected layer. ResNet has no activation on the fully connected layer. Therefore, the improvement of ResNet with a SpinalNet FC layer is not significant. 




\begin{table*}
\centering
\caption{Performance of the SpinalNet and several popular Nets on Different Classification Datasets}
\label{Perform_tab}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
 Data  & Model & Size of Fully Connected Layer & Epoch & Test Accuracy & Error Reduction & Parameters  \\ \hline 

      & CNN\cite{MNIST_CNN}& 1HL, 50 Neurons &8 &98.17\%&- & 21.84k \\ \cline{2-7}
 MNIST  & CNN (Spinal FC) & 6HL, 8 Neurons Per Layer &8 &98.44\%  & 14.8\% &13.82k\\ \cline{2-7}
 \cite{deng2012mnist} & CNN(Spinal FC) & 6HL, 10 Neurons Per Layer &8 &98.48\% &16.9\%&16.05k\\ \cline{2-7}
      
      & VGG-5\cite{VGG5} & 1HL, 512 Neurons &100&99.72\% &- &3.646M\\ \cline{2-7}
      & VGG-5 (Spinal FC) & 4HL, 128 Neurons Per Layer &100&99.72\% &0.0\% &3.630M\\ \hline \hline 
      
        & CNN\cite{MNIST_CNN}& 1HL, 50 Neurons &8 &84.10\%&-& 21.84k \\ \cline{2-7}
 Fashion-MNIST  & CNN (Spinal FC) & 6HL, 8 Neurons Per Layer  &8 &85.98\%  & 11.8\%&13.82k\\ \cline{2-7}
  \cite{xiao2017fashion}    & CNN(Spinal FC) & 6HL, 10 Neurons Per Layer &8 &86.61\% &15.8\%&16.05k\\ \cline{2-7}
     
      & VGG-5\cite{VGG5} & 1HL, 512 Neurons &100&94.63\% &- &3.646M\\ \cline{2-7}
      & VGG-5 (Spinal FC) & 4HL, 128 Neurons Per Layer &100&94.68\% &0.9\% &3.630M\\ \hline  \hline 
      
        & CNN\cite{MNIST_CNN}& 1HL, 50 Neurons &8 &84.48\%&- & 21.84k\\ \cline{2-7}
 Kuzushiji-MNIST  & CNN (Spinal FC) & 6HL, 8 Neurons Per Layer &8 &87.94\%  & 22.3\%&13.82k\\ \cline{2-7}
     (KMNIST) \cite{clanuwat2018deep} & CNN (Spinal FC) & 6HL, 10 Neurons Per Layer &8 &88.25\% &24.3\%&16.05k\\ \cline{2-7}
      
      & VGG-5\cite{VGG5} & 1HL, 512 Neurons &200 &98.94\%&- &3.646M\\ \cline{2-7}
      & VGG-5 (Spinal FC) & 4HL, 128 Neurons Per Layer &200 &99.15\%&19.8\% &3.630M\\ \hline \hline 



        & CNN\cite{MNIST_CNN}& 1HL, 50 Neurons &8 &97.82\%&-& 21.84k \\ \cline{2-7}
 QMNIST  & CNN (Spinal FC) & 6HL, 8 Neurons Per Layer &8 &97.97\%  & 6.9\%&13.82k\\ \cline{2-7}
   \cite{yadav2019cold}   & CNN(Spinal FC) & 6HL, 10 Neurons Per Layer &8 &98.07\% &11.5\%&16.05k\\ \cline{2-7}
     
      & VGG-5\cite{VGG5} & 1HL, 512 Neurons &100 &99.66\%&- &3.646M\\ \cline{2-7}
      & VGG-5 (Spinal FC) & 4HL, 128 Neurons Per Layer &100 &99.68\%&5.9\% &3.630M\\ \hline \hline 
      
              & CNN\cite{MNIST_CNN}& 1HL, 50 Neurons &8 &98.89\%&- & 21.84k\\ \cline{2-7}
 EMNIST  & CNN (Spinal FC) & 6HL, 8 Neurons Per Layer &8 &99.12\%  & 20.7\%&13.82k\\ \cline{2-7}
  (Digits) \cite{cohen2017emnist}   & CNN(Spinal FC) & 6HL, 10 Neurons Per Layer &8 &99.16\% &24.3\%&16.05k\\ \cline{2-7}
      
      & VGG-5\cite{VGG5} & 1HL, 512 Neurons &50 &99.81\%&- &3.646M\\ \cline{2-7}
      & VGG-5 (Spinal FC) & 4HL, 128 Neurons Per Layer & 50 &99.82\% & 5.3\% &3.630M\\ \hline \hline 
      
    & CNN\cite{MNIST_CNN}& 1HL, 50 Neurons &8 &87.57\%&- & 21.84k\\ \cline{2-7}
 EMNIST  & CNN (Spinal FC) & 6HL, 8 Neurons Per Layer &8 &90.07\%  & 20.11\%&13.82k\\ \cline{2-7}
  (Letters) \cite{cohen2017emnist}   & CNN(Spinal FC) & 6HL, 10 Neurons Per Layer &8 &90.23\% &21.4\%&16.05k\\ \cline{2-7}
     
      & VGG-5\cite{VGG5} & 1HL, 512 Neurons &200 &95.86\%&- &3.646M\\ \cline{2-7}
      & VGG-5 (Spinal FC) & 4HL, 128 Neurons Per Layer & 200 &95.88\% & 0.5\% &3.630M\\ \hline \hline 
      
    & CNN\cite{MNIST_CNN}& 1HL, 50 Neurons &8 &79.61\%&- & 21.84k\\ \cline{2-7}
 EMNIST  & CNN (Spinal FC) & 6HL, 8 Neurons Per Layer &8 &82.77\%  & 15.50\%&13.82k\\ \cline{2-7}
  (Balanced) \cite{cohen2017emnist}   & CNN(Spinal FC) & 6HL, 10 Neurons Per Layer &8 &83.21\% &17.66\% &16.05k\\ \cline{2-7}
      
      & VGG-5\cite{VGG5} & 1HL, 512 Neurons &200 &91.04\%&- &3.646M\\ \cline{2-7}
      & VGG-5 (Spinal FC) & 4HL, 128 Neurons Per Layer & 200 &91.05\% & 0.1\% &3.630M\\ \hline \hline
      
      
      
      
      
      
      
      
      
      
      & CNN\cite{CR_CNN2}& 2HL, 120 and 84 Neurons & 8 &60.65\% &- &62.01k\\ \cline{2-7}
CIFAR-10      & CNN (Spinal FC) & 6HL, 20 Neurons Per Layer& 8 &62.37\%&4.4\% &30.20k\\ \cline{2-7}
      
 \cite{krizhevsky2009learning}  & DenseNet\cite{DenseNet_code} & 1HL, 512 Neurons& 35 &77.79\%&- & 1.07M \\ \cline{2-7}  
      & DenseNet (Spinal FC) &8HL, 16 Neurons Per Layer& 35 &81.13\%&15.0\% &1.13M\\ \cline{2-7}  
 & DenseNet (Spinal FC) &8HL, 64 Neurons Per Layer& 35 &81.95\%&18.7\% & 1.36M\\ \cline{2-7} 
      
 
   & ResNet-18 \cite{he2016deep}&Only Output Layer&150 &91.98\%&- &3.14M\\ \cline{2-7}
 & ResNet-18 (Spinal FC) & 4HL, 20 Neurons Per Layer &150 &91.42\%  & -7.0\% &3.22M\\ \cline{2-7}
    & ResNet-34 \cite{he2016deep}&Only Output Layer&150 &89.56\%&- &5.67M \\ \cline{2-7}
 & ResNet-34 (Spinal FC) & 4HL, 256 Neurons Per Layer &150 &89.88\%  & 3.1\% &5.75M\\ \cline{2-7}
& VGG-11\cite{simonyan2014very} & 2HL, 4096 Neurons Each &35 &86.68\% &- &28.15M\\ \cline{2-7} 
    & VGG-11 (Spinal FC) &4HL, 1024 Neurons Each &35 &87.08\%&3.0\% &9.35M\\ \cline{2-7} 

    & VGG-13\cite{simonyan2014very} & 2HL, 4096 Neurons Each &35 &87.79\% &- &28.33M\\ \cline{2-7} 
    & VGG-13 (Spinal FC) &4HL, 1024 Neurons Each &35 &89.16\%&11.2\% &95.31M \\ \cline{2-7}      
      & VGG-19\cite{simonyan2014very} & 2HL, 4096 Neurons Each &200 &90.75\% &-&38.96M\\ \cline{2-7} 
    & VGG-19 (Spinal FC) &4HL, 512 Neurons Each &200 &91.40\%&7.0\% & 20.16M\\ \hline \hline 
      
      


  & ResNet-18 \cite{he2016deep}&Only Output Layer&30 &65.04\%&- &3.16M\\ \cline{2-7}
   CIFAR-100 & ResNet-18 (Spinal FC) & 4HL, 128 Neurons Per Layer &30 &63.60\%  & -4.1\% &4.66M\\ \cline{2-7}
 
    \cite{krizhevsky2009learning} & ResNet-34 \cite{he2016deep}&Only Output Layer&30 &65.51\%&- &5.69M \\ \cline{2-7}
 & ResNet-34 (Spinal FC) & 4HL, 128 Neurons Per Layer &30 &63.32\%  & -6.3\% &7.19M\\ \cline{2-7}
 
     & VGG-11\cite{simonyan2014very} & 2HL, 4096 Neurons Each &40 &55.60\% &- &28.52M\\ \cline{2-7}
      & VGG-11 (Spinal FC) &4HL, 1024 Neurons Each &40 &60.48\%&11.0\% &9.39M\\ \cline{2-7}
      & VGG-13\cite{simonyan2014very} & 2HL, 4096 Neurons Each &50 &60.75\% &-&28.70M\\ \cline{2-7}
      & VGG-13 (Spinal FC) &4HL, 1024 Neurons Each &50 &62.46\%&4.4\% &9.58M \\ \cline{2-7} 
      & VGG-16\cite{simonyan2014very} & 2HL, 4096 Neurons Each &150 &63.20\% &-&34.02M\\ \cline{2-7}
      & VGG-16 (Spinal FC) &4HL, 512 Neurons Each &150 &64.99\%&4.9\% &14.89M \\ \cline{2-7}
      & VGG-19\cite{simonyan2014very} & 2HL, 4096 Neurons Each &150 &62.05\% &- &39.33M\\ \cline{2-7}
      & VGG-19 (Spinal FC) &4HL, 512 Neurons Each &150 &64.77\%&7.2\% &20.20M \\ \hline 

\end{tabular}


\end{table*}

\subsubsection{CIFAR-100 Dataset}
ResNet and VGG networks bring promising results with CIFAR-10 data. Therefore, we apply them to the CIFAR-100 dataset. We receive significant improvement with the Spinal FC layer for different VGG networks. Among the VGG networks, VGG-16 provides the best performance. VGG-16 provides 63.20\% accuracy alone, and 64.99\% accuracy with the SpinalNet after 150 epoch of training. Moreover, the number of neurons in the fully connected layer is reduced to half with the Spinal FC layer. Moreover, the number of multiplication in the FC layer is reduced to 7\%. The VGG-16 has two hidden layers of size 4096. Replacing them with four spinal layers of 512 size reduces the number of multiplication significantly.
However, the Spinal fully-connected layer does not improve the performance of ResNet. Although the number of hidden neurons in the fully connected layer is increasing for ResNet, we are getting a lower performance. The probable reason can be a decrease in the gradient is the initial layers of ResNet due to additional layers in the SpinalNet.


\begin{table*}
\centering
\caption{Transfer Learning Performance of the SpinalNet and several popular Nets on Different Classification Datasets}
\label{Perf_TL}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
 Data  & Model & Size of Fully Connected Layer & Epoch & Test Accuracy & Error Reduction & Parameters  \\ \hline 
 
   & VGG-19\_bn & 2 HL, 4096 Neurons Per Layer & 25 & 95.91\%  & - &263.27M\\  \cline{2-7}
  CIFAR-10    & VGG-19\_bn (Spinal FC) &  4HL, 1024 Neurons Per Layer & 25 & 96.00\%  & 2.2\% &198.26M\\  \cline{2-7}
 \cite{krizhevsky2009learning} & Wide\_ResNet-101\_2 & 0 Neurons & 50 & 98.22\%  & - &124.86M\\  \cline{2-7}
 & Wide\_ResNet-101\_2 (Spinal FC) & 4HL, 20 Neurons Per Layer &50 &98.12\% & -5.6\% &124.92M\\ \hline  \hline

   & VGG-19\_bn & 2 HL, 4096 Neurons Per Layer & 25 & 79.22\%  & - &263.63M\\  \cline{2-7}
 CIFAR-100     & VGG-19\_bn (Spinal FC) &  4HL, 1024 Neurons Per Layer & 25 & 79.56\%  & 1.6\% &198.63M\\  \cline{2-7}
  \cite{krizhevsky2009learning} & Wide\_ResNet-101\_2 & 0 Neurons & 50 & 87.15\%  & - &125.04M\\  \cline{2-7}
 & Wide\_ResNet-101\_2 (Spinal FC) &  4HL, 512 Neurons Per Layer &50 &88.34\% & 9.26\% &132.59M\\ \hline \hline

   & VGG-19\_bn & 2 HL, 4096 Neurons Per Layer & 10 & 92.98\%  & - &263.64M\\  \cline{2-7}
  Caltech-101     & VGG-19\_bn (Spinal FC) &  4HL, 1024 Neurons Per Layer & 10 & 93.16\%  & 2.6\% &198.63M\\  \cline{2-7}
 \cite{li2004caltech} & Wide\_ResNet-101\_2 & 0 Neurons & 10 & 97.11\%  & - &125.05M\\  \cline{2-7}
 & Wide\_ResNet-101\_2 (Spinal FC) & 4HL, 101 Neurons Per Layer &10 &97.32\% & 7.27\% &132.60M\\ \hline \hline

  & VGG-19\_bn & 2 HL, 4096 Neurons Per Layer & 25 & 98.67\%  & - &264.15M\\  \cline{2-7}
 Bird225 & VGG-19\_bn (Spinal FC) & 4HL, 1024 Neurons Per Layer &25 &99.02\% & 26.3\% &199.14M\\ \cline{2-7}
 \cite{bird225} & Wide\_ResNet-101\_2 & 0 Neurons & 25 & 99.38\%  & - &125.30M\\  \cline{2-7} 
 & Wide\_ResNet-101\_2 (Spinal FC) & 4HL, 225 Neurons Per Layer &25 &99.56\% & 11.1\% &126.11M\\ \hline \hline

  & VGG-19\_bn & 2 HL, 4096 Neurons Per Layer & 25 & 87.21\%  & - &264.03M\\  \cline{2-7}
 Stanford Cars & VGG-19\_bn (Spinal FC) &  4HL, 1024 Neurons Per Layer &25 &88.72\% & 13.4\% &153.78M\\ \cline{2-7}
 \cite{Stcar} & Wide\_ResNet-101\_2 & 0 Neurons & 25 & 93.35\%  & - &125.24M\\  \cline{2-7}
 & Wide\_ResNet-101\_2 (Spinal FC) &  4HL, 196 Neurons Per Layer &25 &93.35\% & 0\% &132.98M\\ \hline \hline

  & VGG-19\_bn & 2 HL, 4096 Neurons Per Layer & 25 & 94.80\%  & - & 263.27M\\  \cline{2-7}
 SVHN & VGG-19\_bn (Spinal FC) &  4HL, 1024 Neurons Per Layer &25 &95.26\% & 18.28\% &198.26M\\ \cline{2-7}
 \cite{SVHN_data} & Wide\_ResNet-101\_2 & 0 Neurons & 25 & 97.80\%  & - &124.86M\\  \cline{2-7}
 & Wide\_ResNet-101\_2 (Spinal FC) & 4HL, 20 Neurons Per Layer &25 &97.87\% & 3.18\% &124.92M\\ \hline \hline

   & VGG-19\_bn & 2 HL, 4096 Neurons Per Layer & 25 & 90.28\%  & - & 263.27M\\  \cline{2-7}
  CINIC-10    & VGG-19\_bn (Spinal FC) &  4HL, 1024 Neurons Per Layer & 25 & 91.00\%  & 8.00\% &198.26M\\  \cline{2-7}
 \cite{darlow2018cinic} & Wide\_ResNet-101\_2 & 0 Neurons & 50 & 92.15\%  & - &124.86M\\  \cline{2-7}
 & Wide\_ResNet-101\_2 (Spinal FC) & 4HL, 20 Neurons Per Layer &50 & 93.60\% & 18.47\% &124.92M\\ \hline \hline

   & VGG-19\_bn & 2 HL, 4096 Neurons Per Layer & 10 & 95.44\%  & - & 263.27M\\  \cline{2-7}
  STL-10    & VGG-19\_bn (Spinal FC) &  4HL, 1024 Neurons Per Layer & 10 & 95.57\%  & 2.9\% &198.26M\\  \cline{2-7}
 \cite{coates2011analysis} & Wide\_ResNet-101\_2 & 0 Neurons & 10 & 98.40\%  & - &124.86M\\  \cline{2-7}
 & Wide\_ResNet-101\_2 (Spinal FC) & 4HL, 20 Neurons Per Layer &10 &98.66\% & 16.3\% &124.92M\\ \hline \hline

   & VGG-19\_bn & 2 HL, 4096 Neurons Per Layer & 25 & 95.20\%  & - &263.64M\\  \cline{2-7}
  Oxford 102    & VGG-19\_bn (Spinal FC) & 4HL, 1024 Neurons Per Layer & 25 & 95.46\%  & 32.5\% &198.63M\\  \cline{2-7}
 Flower \cite{nilsback2008automated} & Wide\_ResNet-101\_2 & 0 Neurons & 50 & 99.39\%  & - & 125.05M\\  \cline{2-7}
 & Wide\_ResNet-101\_2 (Spinal FC) &  4HL, 101 Neurons Per Layer &50 &99.30\% & -14.7\% &125.32M\\ \hline \hline 

   & VGG-19\_bn & 2 HL, 4096 Neurons Per Layer & 10 & 99.90\%  & - &263.76M\\  \cline{2-7}
  Fruits 360    & VGG-19\_bn (Spinal FC) & 4HL, 1024 Neurons Per Layer & 10 & 99.96\%  & 60\% &198.75M\\  \cline{2-7}
  \cite{Fruits360} & Wide\_ResNet-101\_2 & 0 Neurons & 10 & 99.96\%  & - & 125.11M\\  \cline{2-7}
 & Wide\_ResNet-101\_2 (Spinal FC) &  4HL, 131 Neurons Per Layer &10 &100\% & 100\% &125.50M\\ \hline 


\end{tabular}


\end{table*}


\begin{table}
\centering
\caption{SOTA Performances of Investigated Datasets in June 2020*}
\label{SOTA_TAB}
\begin{tabular}{|l|l|c|}
\hline
 Data&Model Name  & Accuracy    \\ \hline 
 MNIST&BranchingMerging CNN  \cite{byerly2020branching} & 99.84\%   \\ \hline 
 
 Fashion-MNIST&PreAct-ResNet18 + FMix  \cite{harris2020understanding} & 96.36\%   \\ \hline 
 Kuzushiji-MNIST**&	CAMNet3 \cite{tissera2019context}   & 99.05\%    \\ \hline 
 QMNIST**&Deep Regularization  \cite{yoo2020deep} & 99.67\%    \\ \hline 
 EMNIST (Digits)** & DWT-DCT with KNN  \cite{ghadekar2018handwritten} & 97.74\%    \\ \hline 
 EMNIST (Letters)**& TextCaps  \cite{jayasundara2019textcaps} & 95.39\%    \\ \hline 
 EMNIST (Balanced)**& TextCaps  \cite{jayasundara2019textcaps} & 90.46\%    \\ \hline 
 CIFAR-10& 	BiT-L (ResNet) \cite{kolesnikov2019large}  & 99.00\%  \\ \hline 
  CIFAR-100& BiT-L (ResNet) \cite{kolesnikov2019large} & 93.51\%  \\ \hline 
  Caltech-101**& UL-Hopfield \cite{liu2018unsupervised} & 91.00\%  \\ \hline 
  Stanford Cars& DAT \cite{Ngiam2018unsupervised} & 96.20\%  \\ \hline 
  Oxford 102 Flowers& BiT-L (ResNet) \cite{kolesnikov2019large} & 99.63\%  \\ \hline
  STL-10** & NAT-M4 \cite{Lu2020neural} & 97.90\% \\ \hline
  CINIC-10 & NAT-M4 \cite{Lu2020neural} & 94.80\% \\ \hline
  SVHN & WideResNet-28-10 \cite{Cubuk2019randaugment}  & 99.00\% \\ \hline
  225 Bird Species** & Vgg-16 \cite{bird_vgg16} & 99.10\% \\ \hline
  Fruits 360** & EfficientNet-B1  \cite{duong2020automated} & 100\% \\ \hline
\end{tabular}
\\  According to the following website and our literature search:  \emph{www.paperswithcode.com/task/image-classification}  \\  
 After the online appearance of the current paper, the performance of SpinalNet will be SOTA performance; unless someone reports a better performance earlier.

\end{table}



\subsection{Classification: Transfer Learning}
While learning from the scratch, we observe that, although the number of hidden neurons in the fully connected layer is increasing, the ResNet with Spinal FC is getting a lower performance compared to the similar ResNet. The probable reason can be a decrease with the gradient in the initial layers of ResNet due to additional layers. Therefore, we apply the transfer learning technique to the pre-trained ResNet and VGG networks. We download pre-trained VGG-19\_bn and Wide-Resnet-101 models from the Torchvision. These models are pre-trained on the Imagenet dataset. We apply these two models to the following datasets.

\subsubsection{CIFAR-10}
We train pre-trained VGG-19\_bn and Wide-Resnet-101 models to the CIFAR-10 data. We investigate both of the traditional FC layers and Spinal FC layers. The SpinalNet achieves a significant improvement in the VGG network in terms of accuracy and the number of parameters. However, there is a slightly lower performance in the wide ResNet with the Spinal FC, compared to the original Wide-ResNet. 

\subsubsection{CIFAR-100}
As usual, the SpinalNet performs well with the VGG network on the CIFAR-100 dataset. We receive a superior accuracy of 88.34\% with a wide Spinal FC of 512 neurons with dropout on the Wide-ResNet-101. It is one of the top ten reported results on the CIFAR-100 dataset. Results are presented in the second segment of table \ref{Perf_TL}.

\subsubsection{Caltech-101}
The SpinalNet performs well with both of the VGG and the Wide-ResNet network on the Caltech-101 dataset. We receive SOTA performance for the Caltech-101 dataset.

\subsubsection{Bird225}
The SpinalNet performs well with both of the VGG and the Wide-ResNet network on the Bird225 dataset. We receive SOTA performance for the Bird225 dataset. Results are available as the fourth segment of table \ref{Perf_TL}.

\subsubsection{Stanford Cars}
Although SpinalNet performs well with the VGG model, the accuracy with the Wide-ResNet model is equal. Moreover, our results are not one of the top 20 results in the Stanford Car leaderboard. The probable reason for the poor performance is the high resolution of the image. Several other researchers are concentrating on certain parts of the image to get high accuracy.  

\subsubsection{SVHN}
Although we receive a slightly better result with the Spinal FC for both networks, our performance is not one of the top twenty performance. The probable reason for getting such poor performance is the training with a different kind of data. The most classes in the Imagenet dataset are not digits. Pre-training with different types of data may limit the performance. 

\subsubsection{CINIC-10}
We receive significant improvements in terms of accuracy for both VGG and Wide-ResNet neural networks. Our result with Wide-ResNet Spinal FC is one of the top five reported results. Detailed procedures of data augmentation and simulations are available in the GitHub code.

\subsubsection{STL-10}
The SpinalNet performs well with both of the VGG and the Wide-ResNet network on the STL-10 dataset. We receive SOTA performance for the STL-10 dataset.

\subsubsection{Oxford 102 Flower}
We trained pre-trained VGG-19\_bn and Wide-Resnet-101 to the Oxford 102 Flower data. We investigate both of the traditional FC layers and Spinal FC layers. The SpinalNet achieves a significant improvement in the VGG network in terms of accuracy and the number of parameters. There is a slightly lower performance in the wide ResNet. Our results with the Wide ResNet networks are among the top 5 accuracies.

\subsubsection{Fruits 360}
Fruits 360 dataset is one of the easiest machine learning problems. Several programmers have released codes with more than 99.9\% efficiency in Kaggle for that dataset. We achieve promising performance with the SpinalNet on the Fruit 360 dataset with both the VGG and the Wide-ResNet networks. The Wide-ResNet-101 with Spinal FC provides 100\% accuracy. However, another paper has reported the same efficiency this year.

\subsubsection{Other Investigated Datasets}
We also investigate the Wide-ResNet101 with SpinalNet fully connected layers in `Intel Image Classification' and `10 Monkey Species' datasets in Kaggle and receive accuracies of 93.77\% and 99.26\% respectively. 

\subsection{The Highest Accuracy}
Table \ref{SOTA_TAB} presents SOTA performances for our investigated datasets. Combining SpinalNet with VGG-5 provides near SOTA or SOTA performance in MNIST datasets. MNIST SOTA models are usually a combined model and have the best performance for a specific dataset. We also obtain SOTA performance in five MNIST datasets. The prime reason for obtaining SOTA is not the proposed network. As these datasets are new, very few researchers investigated these data, and our result is the best among the reported results. We also apply transfer learning with a spinal fully connected layer and achieve SOTA performance in several color image datasets.



\section{Prospects of SpinalNet}
There are many rooms for further investigation and improvement of SpinalNet which can be mentioned in the rest of this section.

\subsection{Auto Dimension Reduction}
Dimension reduction is a popular technique of reducing the number of inputs to a neural network without facing noticeable performance degradation \cite{vohra2019active}. The input combination of the NN network may contain a large number of inter-related or irrelevant data. As the proposed SpinalNet takes input in every layer and there are fewer neurons per hidden layer than the total number of inputs, the SpinalNet may automatically discard irrelevant data. Moreover, the necessity for dimension reduction may decrease, as a large number of input features do not increase the computation greatly.    




\subsection{Very Deep NN}
The computation performed inside the proposed SpinalNet increases linearly with the increase in depth. The SpinalNet has outputs at every layer. Moreover, gradual training may enable us to increase SpinalNet depth gradually. 



\subsection{Spinal Hidden Layer}
This paper presents the SpinalNet as an independent network and as the fully connected layer of a CNN. The SpinalNet can also replace a wide hidden layer of a traditional NN. Fig. \ref{SpinalHL} presents how a SpinalNet can replace a traditional hidden layer. The figure shows two inputs and one neuron per sub-layer. The number of inputs can be large and the input can be segmented into more than two segments. Similarly, one sub-layer may hold more than one neuron.  The number of sub-layer can be more than or less than four.  

\begin{figure}
  \centering
  \includegraphics[width=3.2in,angle=0]{SpinalHL.png}
  \caption{Any traditional hidden layer can be converted to a spinal hidden layer. The traditional hidden layer in (a) is converted to a spinal hidden layer in (b). A spinal hidden layer has the structure of the proposed SpinalNet.}
  \label{SpinalHL}
\end{figure}

\subsection{Better Accuracy and New Datasets}
The SpinalNet may achieve higher accuracy with augmented datasets and its different structural variants. We apply SpinalNet as fully connected layers on several other networks to achieve higher accuracies. Researchers may apply SpinalNet for different datasets, new applications \cite{kabir2018neural,rahman2018unified, neven2018towards}, and combining with other networks in the future. 

\subsection{NN Ensemble and Voting}
Recently Mo Kweon et al. perform ensemble and voting from two different VGG networks and ResNet to achieve better performance \cite{VGG5}. Researchers may use different NN along within SpinalNet to get better performance.


\section{Conclusion}
This paper aims to present the concept and structure of a novel DNN model called SpinalNet. The chordate nervous system has a unique way of connecting a large number of sensing information and taking local decisions. The main drawback of the recent proposed DNNs is in their computational intensiveness due to a large number of network inputs. Therefore, taking inputs gradually and considering local decisions such as similar to our spinal cord decreases computations. In this paper, we also present the effectiveness of SpinalNet on several well-known benchmark datasets leading to the improvement of the classification accuracy. Moreover, the SpinalNet is usually less computation extensive than its counterpart. Combining with VGG-5 and transfer learning, the SpinalNet has achieved SOTA performance in four datasets. As the future research direction, we will try to improve the accuracy of the proposed SpinalNet and then apply the improved SpinalNet to a wide range of real-world scenarios.

\bibliographystyle{IEEEtran}
\bibliography{Ref}

\end{document}
