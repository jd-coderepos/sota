\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subfigure}
\usepackage{stmaryrd}

\usepackage{empheq}
\usepackage{bm}
\usepackage{centernot}
\usepackage{soul}
\usepackage[subtle, title=normal, bibliography=normal]{savetrees24}

\newcommand{\bb}[1]{\textbf{#1}}
\newcommand{\mbb}[1]{\mathbb{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\Perp}{\perp\!\!\!\perp}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\PP}{\partial}
\usepackage{cancel}
\newcommand{\NN}{\mathcal{N}}
\newcommand{\lra}{\longrightarrow}
\newtheorem{mythm}{Theorem}
\let\emptyset\varnothing
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\dP}[1]{\frac{\partial }{\partial #1}}
\newcommand{\dPP}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\KL}[2]{D_\mc{KL}\left[#1||#2\right]}
\newcommand{\VV}{\mathbb{V}}
\newcommand{\msf}[1]{\mathsf{#1}}
\newcommand{\lta}{\overset{\mc{T}}{\lra}}
\newcommand{\la}{\left\langle}
\newcommand{\ra}{\right\rangle}
\newcommand{\lb}{\left [}
\newcommand{\rb}{\right ]}
\renewcommand{\det}{\text{det }}
\newcommand{\conj}[1]{\overline{#1}}
\newcommand{\ff}[1]{\mathsf{#1}}
\newcommand{\mf}[1]{\mathfrak{#1}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}


\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy 

\def\cvprPaperID{2077} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

\title{Harmonic Networks: Deep Translation and Rotation Equivariance}

\author{Daniel E.~Worrall, Stephan J.~Garbin, Daniyar Turmukhambetov and Gabriel J.~Brostow \\
{\tt \small \{{d.worrall}, {s.garbin}, {d.turmukhambetov}, {g.brostow}\}{@cs.ucl.ac.uk}}\\
University College London\thanks{\texttt{http://visual.cs.ucl.ac.uk/pubs/harmonicNets/}}}


\maketitle


\begin{abstract}
	Translating or rotating an input image should not affect the results 
    of many computer vision tasks. Convolutional neural networks (CNNs) 
    are already translation equivariant: input image translations produce 
    proportionate feature map translations. This is not the case for 
    rotations. Global rotation equivariance is typically sought through 
    data augmentation, but patch-wise equivariance is more difficult. 
    We present Harmonic Networks or H-Nets, a CNN exhibiting equivariance 
    to patch-wise translation and 360-rotation. We achieve this by replacing 
    regular CNN filters with circular harmonics, returning a maximal 
    response and orientation for every receptive field patch.
    
    H-Nets use a rich, parameter-efficient and fixed computational complexity 
    representation, and we show that deep feature maps within the network 
    encode complicated rotational invariants. We demonstrate that our layers 
    are general enough to be used in conjunction with the latest architectures 
    and techniques, such as deep supervision and batch normalization. We also 
    achieve state-of-the-art classification on rotated-MNIST, and competitive 
    results on other benchmark challenges.
\end{abstract}



\section{Introduction}
We tackle the challenge of representing -rotations in convolutional neural networks (CNNs) \cite{lecun1995cnn}. Currently, convolutional layers are constrained by design to map an image to a feature vector, and \emph{translated} versions of the image map to proportionally-translated versions of the same feature vector \cite{lenc2015equivariance} (ignoring edge effects)---see Figure~\ref{fig:Z_equivariance}. However, until now, if one \emph{rotates} the CNN input, then the feature vectors do not necessarily rotate in a meaningful or easy to predict manner. The sought-after property, directly relating input transformations to feature vector transformations, is called \emph{equivariance}.

A special case of equivariance is invariance, where feature vectors remain constant under all transformations of the input. This can be a desirable property globally for a model, such as a classifier, but we should be careful not to restrict all intermediate levels of processing to be transformation invariant. For example, consider detecting a deformable object, such as a butterfly. The pose of the wings is limited in range, and so there are only certain poses our detector should normally see. A transformation invariant detector, good at detecting wings, would detect them whether they were bigger, further apart, rotated, etc., and it would encode all these cases with the same representation. It would fail to notice nonsense situations, however, such as a butterfly with wings rotated past the usual range, because it has thrown that extra pose information away. An equivariant detector, on the other hand, does not dispose of local pose information, and so it hands on a richer and more useful representation to downstream processes.
\begin{figure}[t]
	\includegraphics[width=\linewidth]{./images/figure1_translationalEquivariance2}
    \caption{Patch-wise translation equivariance in CNNs arises from 
    translational weight tying, so that a translation  of the input image 
    , leads to a corresponding translation  of the feature 
    maps , where  in general, due to pooling 
    effects. However, for rotations, CNNs do not yet have a feature space transformation
     `hard-baked' into their structure, and it is complicated to discover 
    what  may be, if it exists at all. Harmonic Networks have
    a hard-baked representation, which allows for easier interpretation of
    feature maps---see Figure \ref{fig:commutativity}.}
    \label{fig:Z_equivariance}
\vspace{-1em}
\end{figure}
Equivariance conveys more information about an input to downstream processes, it also constrains the space of possible learned models to those that are valid under the rules of natural image formation \cite{soatto2009actionable}. This makes learning more reliable and helps with generalization. For instance, consider CNNs. The key insight is that the statistics of 
natural images, embodied in the correlations between pixels, are 
a) invariant to translation, and b) highly localized. Thus 
features at every layer in a CNN are computed on local receptive 
fields, where weights are shared across translated receptive fields. 
This weight-tying serves both as a constraint on the translational 
structure of image statistics, and as an effective technique to reduce 
the number of learnable parameters---see Figure \ref{fig:Z_equivariance}.
In essence, translational equivariance has been `baked' into the
architecture of existing CNN models. We do the same for rotation and 
refer to it as \emph{hard-baking}.

The current widely accepted practice to cope with rotation is to train with aggressive data augmentation \cite{krizhevsky2012imagenet}. This certainly improves generalization, but is not exact, fails to capture 
local equivariances, and does not ensure equivariance at every
layer within a network. How to maintain the richness of local rotation information, is what we present in this paper. Another disadvantage of data augmentation is that it leads to 
the so-called \emph{black-box} problem, where there is a lack 
of feature map interpretability. Indeed, close inspection of
first-layer weights in a CNN reveals that many of them are rotated,
scaled, and translated copies of one another \cite{zeiler2014understanding}. 
Why waste computation learning all of these redundant weights?





In this paper, we present \emph{Harmonic Networks}, or \emph{H-Nets}.
They design patch-wise -rotational equivariance into 
deep image representations, by constraining the filters to the family 
of \emph{circular harmonics}. The circular harmonics are \emph{steerable 
filters} \cite{freeman1991design}, which means that we can represent 
all rotated versions of a filter, using just a finite, linear combination 
of \emph{steering bases}. This overcomes the issue of learning multiple 
filter copies in CNNs, guarantees rotational equivariance, and produces
feature maps that transform predictably under input rotation.

\section{Related Work}
Multiple existing approaches seek to encode rotational equivariance into
CNNs. Many of these follow a broad approach of introducing filter or
feature map copies at different rotations. None has dominated as standard practice.

\textbf{Steerable filters}
At the root of H-Nets lies the property of \emph{filter steerability} 
\cite{freeman1991design}. Filters exhibiting steerability can be 
constructed at any rotation as a finite, linear combination of base 
filters. This removes the need to learn multiple filters at different 
rotations, and has the bonus of constant memory requirements. As such,
H-Nets could be thought of as using an infinite bank of rotated filter
copies. A work, which combines steerable filters with learning is
\cite{liu2012equi}. They build shallow features from steerable
filters, which are fed into a kernel SVM for object detection and rigid 
pose regression. H-Nets use the same filters with an added rotation offset
term, so that filters in different layers can have orientation-selectivity 
relative to one another. 

\textbf{Hard-baked transformations in CNNs}
While H-Nets hard-bake patch-wise -rotation into the feature 
representation, numerous related works have encoded equivariance to 
discrete rotations. The following works can be grouped into those,
which encode global equivariance versus patch-wise equivariance, and 
those which rotate filters versus feature maps.

\cite{cohen2016group} introduce equivariance to -rotations and dihedral flips in CNNs by copying the transformed filters at different rotation--flip combinations. More recently they generalized this theory to all group-structured transformations in \cite{cohen2016steer}, but they only demonstrated applications on finite groups---an extension to continuous transformations would require a treatment on anti-aliasing and bandlimiting. \cite{marcos2016learning} use a larger number of rotations for texture classification and \cite{oyallon2015scattering} also use many rotated handcrafted filter copies, opting not to learn the filters. To achieve equivariance to a greater number of rotations, these methods would need an infinite amount of computation. H-Nets achieve equivariance to all rotations, but with finite computation. 

\cite{fasel2006neocognitron} feed in multiple rotated copies of the CNN input and fuse the output predictions. \cite{laptev2016tipooling} do the same for a broader class of global image transformations, and propose a novel per-pixel pooling technique for output fusion. As discussed, these techniques lead to global equivariances only and do not produce interpretable feature maps. \cite{dieleman2016exploiting} go one step further and copy each feature map at four -rotations. They propose 4 different equivariance preserving feature map transformations. Their CNN is similar to \cite{cohen2016group} in terms of what is being computed, but rotating feature maps instead of filters. A downside of this is that all inputs and feature maps have to be square; whereas, we can use any sized input.

\textbf{Learning generalized transformations}
Others have tried to learn the transformations directly from the data. While this is an appealing idea, as we have said, for certain transformations it makes more sense to hard-bake these in for interpretability and reliability. \cite{memisevic2010hobm} construct a higher-order Boltzmann machine, which learns tuples of transformed linear filters in input--output pairs. Although powerful, they have only shown this to work on shallow architectures. \cite{hinton2011transforming} introduced \emph{capsules}, units of neurons designed to mimic the action of cortical columns. Capsules are designed to be invariant to complicated transformations of the input. Their outputs are merged at the deepest layer, and so are only invariant to global transformation. \cite{lenc2016covariant} present a method to regress equivariant feature detectors using an objective, which penalizes representations, which lie far from the equivariant manifold. Again, this only encourages global equivariance; although, this work could be adapted to encourage equivariance at every layer of a deep pipeline.

\section{Problem analysis}
Many computer vision systems strive to be view independent, such as object 
recognition, which is invariant to affine transformations, or boundary detection, 
which is equivariant to non-rigid deformations. H-Nets hard-bake -rotation
equivariance into their feature representation, by constraining the convolutional 
filters of a CNN to be from the family of circular harmonics. Below, we outline 
the formal definition of equivariance (Section \ref{sec:equivariance}), how the 
circular harmonics exhibit rotational equivariance (Section \ref{sec:circular_harmonics}) 
and some properties of the circular harmonics, which we must heed for successful
integration into the CNN framework (Section \ref{sec:properties}).

\textbf{Continuous domain feature maps}
In deep learning we use feature maps, which live in a discrete domain.
We shall instead use continuous spaces, because the analysis
is easier. Later on in Section \ref{sec:sampling} we shall demonstrate
how to convert back to the discrete domain for practical implementation,
but for now we work entirely in continuous Euclidean space.

\subsection{Equivariance}
\label{sec:equivariance}
Equivariance is a useful property to have because transformations 
 of the input produce predictable transformations  of 
the features, which are interpretable and can make learning easier.
Formally, we say that feature mapping  is \emph{equivariant} 
to a group of transformations if we can associate every transformation
 of the input  with a transformation 
 of the features; that is,
 
This means that the order, in which we apply the feature mapping and the 
transformation is unimportant---they \emph{commute}. An example is depicted 
in Figure \ref{fig:Z_equivariance}, which shows that in CNNs the order of 
application of integer pixel-translations and the feature map are interchangeable.
An important point of note is that  in general, so if we 
seek for  to be rotations in the image domain, we do not require 
to find the set of , such that  ``looks like'' a rotation in 
feature space, rather we are searching for the set of , such that there 
exists an \emph{equivalent} class of transformations  in feature 
space. A special case of equivariance is \emph{invariance}, when 
, the identity.
\begin{figure}[t]
	\includegraphics[width=\linewidth]{./images/filters}
    \caption{Real and imaginary parts of the complex Gaussian filter 
    , for
    some rotation orders. As a simple example, we have set 
    and , but in general we learn these quantities. 
    Cross-correlation, of a feature map 
    of rotation order  with one of these filters of rotation order 
    , results in a feature map of rotation order . Note the negative rotation order filters have flipped imaginary parts compared to the positive orders.}
    \label{fig:circular_harmonics}
\vspace{-1em}
\end{figure}
\subsection{The Complex Circular Harmonics}
\label{sec:circular_harmonics}
With data augmentation CNNs may learn some rotation equivariance, but this is difficult to quantify \cite{lenc2015equivariance}. H-Nets take the simpler approach of hard-baking this structure in. If  is the feature mapping of a standard convolutional layer, then -rotational equivariance can be hard-baked in by restricting the filters to be of the from the circular harmonic family (proof in Supplementary Material)

Here  are the spatial coordinates of image/feature maps, expressed in polar form,  is known as the \emph{rotation order},  is a function, called the \emph{radial profile}, which controls the overall shape of the filter, and  is a \emph{phase offset} term, which gives the filter orientation-selectivity. During training, we learn the radial profile and phase offset terms. Examples of the real component of  for a `Gaussian envelope' and different rotation orders are shown in Figure \ref{fig:circular_harmonics}. Since we are dealing with complex-valued filters, all filter responses are complex-valued, and we assume from now on that the reader understands that all feature maps are complex-valued, unless otherwise specified. Note that there are other works (e.g.,\ \cite{tygert2016complex}), which use complex filters, but our treatment differs in that the complex phase of the response is explicitly tied to rotation angle.
\begin{figure}[t]
	\includegraphics[width=\linewidth]{./images/patch_rotation}
    \caption{\textsc{Down}: Cross-correlation of the input patch with  yields a scalar complex-valued response. 
    \textsc{Across-then-down}: Cross-correlation with the -rotated 
    image yields another complex-valued response. \textsc{Bottom}: 
    We transform from the unrotated response to the rotated response, through
    multiplication by .}
    \label{fig:commutativity}
\vspace{-1em}
\end{figure}

\textbf{Rotational Equivariance of the Circular Harmonics}
Some deep learning libraries implement cross-correlation  rather than convolution , and since the understanding is slightly easier to follow, we consider correlation. Strictly, cross-correlation with complex functions requires that one of the arguments is conjugated, but we do not do this in our model/implementation, so

Consider correlating a circular harmonic of order  with a rotated image patch. We assume that the image patch is only able to rotate locally about the origin of the filter. This means that the cross-correlation response is a scalar function of input image patch rotation . Using the notation from Equation \ref{eq:equivariance}, and recalling that we are working in polar coordinates , counter-clockwise rotation of an image  about the origin by an angle  is . As a shorthand we denote . It is a well-known result \cite{liu2012equi,freeman1991design} (proof in Supplementary Material) that

where we have written  in place of  for brevity. We see that the response to a -rotated image  with a circular harmonic of order  is equivalent to the cross-correlation of the unrotated image  with the harmonic, followed by multiplication by . While the rotation is done in input space,  multiplication by  is performed in feature space, and so, using the notation from Equation 1, . This process is shown in Figure \ref{fig:commutativity}. Note that we have included a subscript  on the feature space transformation. This is important, because the kind of feature space transformation we apply is dependent on the rotation order of the harmonic. Because the phase of the response rotates with the input at frequency , we say that the response is an \emph{-equivariant feature map}. By thinking of an input image as a complex-valued feature map with zero imaginary part, we could think of it as -equivariant. 

The rotation order of a filter defines its response properties to input rotation. In particular, rotation order  defines invariance and  defines linear equivariance. For  this is because, denoting , then , which is independent of . For , ---as the input rotates,  is a complex-valued number of constant magnitude , spinning round with a phase equal to . Naturally, we are not constrained to using rotation orders 0 or 1 only, and we make use of higher and negative orders in our work. 

\textbf{Arithmetic and the Equivariance Condition}
\label{sec:properties}
Further important properties of the circular harmonics, which are proven 
in the Supplementary Material, are: 1) Chained cross-correlation of rotation 
orders  and  lead to a new response with rotation order .
2) Point-wise nonlinearities , acting solely on the 
magnitudes maintain rotational equivariance, so we can interleave 
cross-correlations with typical CNN nonlinearities adapted to the complex 
domain. 3) The summation of two responses of the same order  remains of 
order . Thus to construct a CNN where the output is -equivariant 
to the input rotation, we require that the sum of rotation orders along 
any path equals , so

This is the fundamental condition underpinning the equivariance properties
of H-Net, so we call it the \emph{equivariance condition}.

We note here that for our purposes, our filter  (the 
complex conjugate), which saves on parameters, but this does not necessarily 
imply conjugacy of the responses unless  is real, which is only true 
at the input.

\section{Method}
We have considered the -rotational equivariance of feature maps
arising from cross-correlation with the circular harmonics, and we determined 
that the rotation orders of chained cross-correlations sum. Next, we use these 
results to construct a deep architecture, which can leverage the equivariance 
properties of circular harmonics.

\subsection{Harmonic Networks}
\begin{figure}[t]
	\includegraphics[width=\linewidth]{./images/network}
    \caption{An example of a 2 hidden layer H-Net with  output, 
    input--output left-to-right. Each horizontal stream represents a series 
    of feature maps (circles) of constant rotation order. The edges 
    represent cross-correlations and are numbered with the rotation 
    order of the corresponding filter. The sum of rotation orders along
    any path of consecutive edges through the network must equal ,
    to maintain disentanglement of rotation orders.}
    \label{fig:H-net_example}
\vspace{-1em}
\end{figure}
The rotation order of feature maps and filters sum upon cross-correlation, so to achieve a given output rotation order, we must obey the equivariance condition. In fact, at every feature map, the equivariance condition must be met, otherwise, it should be possible to arrive at the same feature map along two different paths, with different summed rotation orders. The problem is that combining complex features, with phases, which rotate at different frequencies, leads to \emph{entanglement} of the responses. The resultant feature map is no longer equivariant to a single rotation order, making it difficult to work with. We resolve this by enforcing the equivariance condition at every feature map.

Our solution is to create separate streams of constant rotation order responses running through the network---see Figure \ref{fig:H-net_example}. These streams contain multiple layers of feature maps, separated by rotation order zero cross-correlations and nonlinearities. Moving between streams, we use cross-correlations of rotation order equal to the difference between those two streams. It is very easy to check that the equivariance condition holds in these networks.

When multiple responses converge at a feature map, we have multiple choices of how to combine them. We could stack them, we could pool across them, or we could sum them \cite{dieleman2016exploiting}. To save on memory, we chose to sum responses of the same rotation order

 is then fed into the next layer. Usually in our experiments, 
we use streams of orders 0 and 1, which we found to work well and is justified by the fact that CNN filters tend to contain very little high frequency information \cite{jacobsen2016structure}.
\begin{figure}[t]
	\centering
	\includegraphics[width=0.7\linewidth]{./images/pixelated}
    \caption{H-Nets operate in a continuous spatial domain, but we can
    implement them on pixel-domain data because sampling and 
    cross-correlation commute. The schematic shows an example of 
    a layer of an H-Net (magnitudes only). The solid arrows follow 
    the path of the implementation, while the dashed arrows follow 
    the possible alternative, which is easier to analyze, but 
    computationally infeasible. The introduction
    of the sampling defines \emph{centers of equivariance} at 
    pixel centers (yellow dots), about which a feature map is 
    rotationally equivariant. }
    \label{fig:sampling_grid}
\vspace{-1em}
\end{figure}

Above, we see that the structure of the Harmonic Network is very
simple. We replaced regular CNN filters with radially reweighted and
phase shifted circular harmonics. This causes each filter response to be
equivariant to input rotations with order . To prevent responses of 
different rotation order from entangling upon summation, we separated
filter responses into streams of equal rotation order.

\textbf{Complex nonlinearities}
\label{sec:complex_nonlinearities}
Between cross-correlations, we use complex nonlinearities, which act on the 
magnitudes of the complex feature maps only, to preserve rotational equivariance.
An example is a complex version of the ReLU

We can provide similar analogues for other nonlinearities and for Batch 
Normalization \cite{ioffe2015bn}, which we use in our experiments.

We have thus far presented the Harmonic Network. Each layer is a collection
of feature maps of different rotation orders, which transform predictably
under rotation of the input to the network and the -rotation 
equivariance is achieved with finite computation. Next we show how to 
implement this in practice.

\subsection{Implementation: Discrete cross-correlations}
\label{sec:sampling}
Until now, we have operated on a domain with continuous spatial dimensions . However, the H-Net needs to operate on real-world images, which are sampled on a 2D-grid, thus we need to anti-alias the input to each discretized layer. We do this with a simple Gaussian blur. We can then use a regular CNN architecture without any problems. This works on the fact that the order of bandlimited sampling and cross-correlation is interchangeable~\cite{freeman1991design}; so either we correlate in continuous space, then downsample, or downsample then correlate in the discrete space. Since point-wise nonlinearities and sampling also commute, the entire H-Net, seen as a deep feature-mapping, commutes with sampling. This could allow us to implement the H-Net on non-regular grids; although, we did not explore this. 

Viewing cross-correlation on discrete domains sheds some insight into how the equivariance properties behave. In Figure \ref{fig:sampling_grid}, we see that the sampling strategy introduces multiple origins, one for each feature map patch. We call these, \emph{centers of equivariance}, because a feature map will exhibit local rotation equivariance about each of these points. If we move to using more exotic sampling strategies, such as strided cross-correlation or average pooling, then the centers of equivariance are ablated or shifted. If we were to use max-pooling, then the center of equivariance would be a complicated nonlinear function of the input image and harmonic weights. For this reason we have not used max-pooling in our experiments.

\textbf{Complex cross-correlations}
\begin{figure}[t]
\begin{center}
	\includegraphics[width=\linewidth]{./images/weights_pdf}
\end{center}
\vspace{-1em}
\caption{Images are sampled on a rectangular grid but our filters are defined in the polar domain, so we bandlimit and resample the data before cross-correlation via Gaussian resampling.}
\label{fig:weights}
\vspace{-1em}
\end{figure}
On a practical note, it is worth mentioning, that complex cross-correlation can be implemented efficiently using 4 real cross-correlations

So circular harmonics can be implemented in current deep learning frameworks, with minor engineering. We implement a grid-resampled version of the filters , with  (see Figure \ref{fig:weights}). The polar representation  can be mapped from the components  by . If we stack all the polar filter samples into a matrix we can write each point as the outer product of a radial tensor  and trigonometric angular tensor . The phase offset  can be separated out by noting that

where the complex exponential and trigonometric terms are element-wise, 
and  is the identity matrix. This is just a reweighting of the 
ring elements. In full generality, we could also use a per-radius phase 
, which would allow for spiral-like left- and right-handed 
features, but we did not investigate this.

\subsection{Computational cost}
We have increased the computational cost of cross-correlation, in return for continuous rotational equivariance. Here we analyze the computational cost in terms of number of multiplications.
\begin{figure}[t]
	\includegraphics[width=\linewidth]{./images/designs}
    \caption{Networks used in our experiments. \textsc{Left}:
    MNIST networks, as per \cite{cohen2016group}. \textsc{Right} 
    deeply-supervised networks (DSN) \cite{lee2015deeply} 
    for boundary segmentation, as per \cite{xie2015hed}. Red
    boxes denote feature maps. Blue boxes are pooling (max for CNNs and
    average for H-Nets). Green boxes are side feature maps
    as per \cite{xie2015hed}; these are connected to the DSN with dashed
    lines for ease of viewing. All main cross-correlations are ,
    unless otherwise stated in the experiments section.}
    \label{fig:networks}
\vspace{-1em}
\end{figure}
In the standard cross-correlation, for an input of size , (height, width, input channels) and filters of size  (height, width, output channels), the number of multiplications to form a feature map of the same size as the input is . In the H-Net, we have  rotation orders on the input and  rotation orders on the output, so perform  complex cross-correlations. Each complex cross-correlation can be formed from 4 real cross-correlations, so the number of multiplications is , where  and  are the number of input and output channels, respectively. Thus for similar computational cost we equate the two to yield . Rearranging; setting ,  and ; and taking the square root of both sides, we arrive at a simple rule of thumb for network design,
.
For example, if we want to build an H-Net with similar computational cost to a regular CNN with 64 channels per layer, then if we use 2 rotation orders , then the number of H-Net channels is .

\section{Experiments}
We validate our rotation equivariant formulation below, performing
some introspective investigations, and measuring against relevant
baselines for classification on the rotated-MNIST dataset~\cite{larochelle2007deep} 
and boundary detection on the Berkeley Segmentation Dataset~\cite{arbelaez2011bsd}.
We selected our baselines as representative examples of the current 
state-of-the-art and to demonstrate that H-Nets can be used on different 
architectures for different tasks. The networks we used are in Figure \ref{fig:networks}.

\subsection{Benchmarks}
Here we compare H-Nets for classification and boundary detection. 
Classification is a typical rotation invariant task, and should 
suit H-Nets very well. In contrast, boundary detection is a rotation 
equivariant task. The key to the success of the H-Net is that it can achieve
global equivariance, without sacrificing local equivariance of features.

\textbf{MNIST}
Of course, this is a small dataset, with simple visual structures, but it is a 
good indication of how introducing the right equivariances into CNNs can aid 
inference.
\begin{table}[t]
  \begin{center}
    {
    \begin{tabular}{|l|c|c|}
    \hline
    Method & Test error (\%) & \# params\\
    \hline\hline
    SVM \cite{larochelle2007deep}						& 		& -\\
    Transformation RBM \cite{sohn2012local}				&  		& -\\
    Conv-RBM \cite{schmidt2012priors} 					&  		& -\\
    CNN \cite{cohen2016group}							&  		& 22k\\
    CNN \cite{cohen2016group} + data aug*				&  		& 22k\\
    CNN rotation pooling \cite{cohen2016group}		& 		& 25k\\
    CNN \cite{cohen2016group} 						& 		& 25k\\
    H-Net (Ours)										&  	& 33k\\
    \hline
    \end{tabular}
    }
  \end{center}
  \caption{Results. Our model sets a new state-of-the-art on the 
  rotated MNIST dataset, reducing the test error by 26\%. * Our
  reimplementation}
  \label{tab:MNIST}
\end{table}
We investigate classification on the rotated MNIST dataset (new version) \cite{larochelle2007deep} 
as a baseline. This has 10000 training images, 2000 validation images, and
50000 test images. The -rotations and small training set
size make this a difficult task for classical CNNs. We compare against 
a collection of previous state-of-the-art papers and 
\cite{cohen2016group}, who build a deep CNN with filter copies at 
-rotations. We try to mimic their network architecture for
H-Nets as best as we can, using 2 rotation order streams with  
through to the deepest layer, and complex-valued versions of ReLU 
nonlinearities and Batch Normalization (see Method). We also replace
max-pooling with mean-pooling layers, as shown in Figure \ref{fig:networks}. 
We perform stochastic gradient descent on a cross-entropy loss using 
Adam \cite{kingma2014adam} and an adaptive learning rate, which we divide by 10 if there has 
been no improvement in validation accuracy in the last 10 epochs. We 
train multiple models with randomly chosen hyperparameters, and report 
the test error of the model that performed best on the validation set, 
training on a combined training and validation set Table~\ref{tab:MNIST} 
lists our results. This model actually has 33k parameters, which is about 50\%
larger than the standard CNN and \cite{cohen2016group}, which have 22k. 
This is because it uses  convolutions instead of .
Interestingly, it does not overfit on such a small dataset and it still
outperforms the standard CNN trained with rotation augmentations, which
we do not use. We set the new state-of-the-art, with a 26\% 
improvement on the previous best model.

\begin{table}[t]
  \begin{center}
  {
    \begin{tabular}{|l|c|c|c|}
    \hline
    Method & ODS & OIS & \# params\\
    \hline\hline
    HED, \cite{xie2015hed}*							& 0.640 		& 0.650			& 2346k \\
    HED, low \# params \cite{xie2015hed}*				& 0.697 		& 0.709			& 115k \\
    Kivinen et al. \cite{kivinen2014boundary}		& 0.702			& 0.715			& -\\
    H-Net (Ours)									& 	& 	& 116k\\
    \hline
    CSCNN\dag, \cite{hwang2015bsd}					& 0.741			& 0.759 & \\
    DeepEdge\dag, \cite{bertasius2015bsd}			& 0.753			& 0.772 & \\
    -Fields\dag, \cite{ganin2014bsd}			& 0.753			& 0.769 & \\
    DeepContour\dag, \cite{shen2015bsd}				& 0.756			& 0.773	& \\
    HED\dag,  \cite{xie2015hed}				& 0.782 		& 0.804			& 2346k \\
    DCNN + sPb\dag, \cite{kokkinos2015bsd}			& 	&  & \\
    \hline
    \end{tabular}
    }
  \end{center}
  \caption{Our model beats the non-pretrained neural networks baselines on BSD500~\cite{arbelaez2011bsd}. *Our implementation. \dag ImageNet pretrained}
  \label{tab:bsd}
\vspace{-1em}
\end{table}

\textbf{Deep Boundary Detection} Boundary detection is equivariant to non-rigid transformations;
although edge presence is locally invariant to orientation. The
current state-of-the-art depends on fine-tuning ImageNet-pretrained
networks to regress boundary probabilities on a per-patch basis.
To demonstrate that hard-baked rotation equivariance serves as a
strong generalization tool, we compared against a previous state-of-the-art
architecture \cite{xie2015hed}, \emph{without pretraining}. We tried 
to mimic \cite{xie2015hed} as closely as possible, as shown in Figure \ref{fig:networks}. The main difference is
that we divide the number of all feature maps by 2, for faster,
more stable training. They use a VGG network~\cite{simonyan2014vgg} 
extended with deeply supervised network (DSN)~\cite{lee2015deeply} 
\emph{side-connections}. These are -convolutions, which 
perform weighted averages across all relevant feature maps, resized 
to match the input. A binary cross-entropy loss is applied to each 
side connection, to stabilize learning. A final `fusion' layer is 
created by taking a weighted linear combination of the side-connections, 
this is the final output. We adapt side-connections to H-Nets, by
using the complex magnitude of feature maps before taking a weighted
average. This means that the resultant boundary predictions are
locally invariant to rotation. We added a small sparsity regularizer
to our cost function, because we found it improved results slightly.
We call the Harmonic variant of the DSN, an \emph{H-DSN}. We also compare against \cite{xie2015hed} with the number of parameters matched to H-DSN (the first layer has 7 features, instead of 16, and so on).

We also compared with \cite{kivinen2014boundary}, who use a 
mean-and-covariance-RBM. Their technique has five main contributions:
1) zero-mean, unit variance normalization of inputs, 2) sparsity
regularization of hidden units, 3) averaged ground truth edge 
annotations, 4) average outputs to 16 input rotations, 5)
non-maximum suppression of results by the Canny method.
We perform the first 2 methods, but leave the last 3 alone.
In particular, they do not pretrain on ImageNet, and attempt some
kind of rotation averaging for global equivariance, so are a good baseline
to measure against. We tested on the Berkeley Segmentation
Dataset (BSD500)~\cite{arbelaez2011bsd}. As shown in Table~\ref{tab:bsd} 
for non-pretrained models, H-Nets deliver superior performance over 
current state-of-the-art architectures, including \cite{kivinen2014boundary}, 
who also encode rotation equivariance. Most noticeable of all is
that we only use 5\% of the parameters of \cite{xie2015hed}, showing
how by restricting the search space of learnable models through 
hard-baking local rotation equivariance, we need not learn so many
parameters.

\subsection{Model Insight}
Here we investigate some of the properties of the H-Net implementation,
making sure that the motivations behind H-Net design are achieved by
the implementation.

\textbf{Rotational stability}
As a sanity check, we measured the invariance of the magnitude response to rotation for . We show the result of rotating a random input to an H-Net layer in Figure \ref{fig:stability}. The response is very flat, with periodic small fluctuations due to the inexactness of anti-aliasing.
\begin{figure}[t]
\begin{center}
	\includegraphics[width=0.8\linewidth]{./images/rotation_stability.png}
\end{center}
\vspace{-1em}
\caption{Stability of the response magnitude to input rotation angle. Black , blue , green .}
\label{fig:stability}
\vspace{-1em}
\end{figure}

\textbf{Filter Visualization}
\begin{figure}[b]
	\includegraphics[width=\linewidth]{./images/phases_filt}
    \caption{Randomly selected filters and phase histograms from the 
    BSDS500 trained H-DSN. Filter are aligned at ; and the 
    oriented circles represent phase. We see few filter copies and no
    blank filters, as usually seen in CNNs. We also see a balanced 
    distribution over phases, indicating that boundaries, and their 
    deep feature representations, are uniformly distributed in orientation.}
    \label{fig:layer1}
\vspace{-1em}
\end{figure}
The real parts of the filters, from the first layer of the 
boundary-detection-trained H-Net, are shown in Figure~\ref{fig:layer1}. 
They are aligned at zero phase () for ease of viewing. 
Since the network is trained on zero-mean, unit variance, normalized 
color images, the weights do not have the natural colors we would see 
in real-world images. Nonetheless, there is useful information we can 
glean from inspecting these. Most 1\textsuperscript{st} layer filters
detect color boundaries, there are no blank filters as one usually sees 
in CNNs, and there are few reoriented copies. We also see from the
phase histograms that all phases are utilized by filters throughout 
the network, indicating full use of the phase information. This is
interesting, because it means that the model's parameters are being
used fully, with low redundancy, which we surmise comes from easier
optimization on the equivariant manifold.
\begin{figure}[b]
\vspace{-1em}
	\includegraphics[width=\linewidth]{./images/ablation}
    \caption{Data ablation study. On the rotated MNIST dataset, we experiment
    with test accuracy for varying sizes of the training set. We normalize the
    maximum test accuracy of each method to 1, for direct comparison of the falloff with
    training size. Clearly H-Nets are more data-efficient than regular CNNs,
    which need more data to discover rotational equivariance unaided.}
    \label{fig:DataAblation}
\end{figure}

\textbf{Data ablation}
Here we investigate H-nets data-efficiency. CNNs are massively data-hungry. Krizhevsky's landmark paper \cite{krizhevsky2012imagenet} used 60 million parameters, trained on 1.2 million  RGB images quantized to 256 bits and split between 1000 classes, for a total of 10 bits of information per weight. Even this vast amount of data was insufficient for training, and data augmentation was needed to improve results. We ran an experiment on the rotated MNIST dataset to show that with hard-baked rotation equivariance, we require less data than competing methods, which is indeed the case (see Figure~\ref{fig:DataAblation}). Interestingly, and predictably, regular CNNs trained with data augmentation still perform worse than H-Nets, because they only learn global invariance to rotation, rather than local equivariances at each layer.

\textbf{Feature maps}
\begin{figure}[t]
\begin{center}
	\includegraphics[width=0.8\linewidth]{./images/eight}
\end{center}
\caption{Feature maps from the MNIST network. The arrows display phase, and the colors display magnitude information (jet color scheme). There are diverse features encoding edges, corners, whole objects, negative spaces, and outlines.}
\label{fig:feature_maps}
\vspace{-1em}
\end{figure}
We visualize feature maps in the lower layers of an MNIST trained 
H-Net (see Figure~\ref{fig:feature_maps}). 
For given input, we see the feature maps encode very 
complicated structures. Left to right, we see the H-Net 
learns to detect edges, corners, object presence, negative space, 
and outlines of objects. We perform this for the BSD500 trained
H-DSN (see Figure~\ref{fig:flow}). It shows equivariance
is preserved through to the deepest feature maps.
It also highlights the compact representation of feature presence and pose,
which regular CNNs cannot do.
\begin{figure}[b]
\vspace{-1em}
	\includegraphics[width=\linewidth]{./images/flow}
    \caption{View best in color. 
    Orientated feature maps for the H-DSN. The color wheel shows
    orientation coding. Note that between layers boundary orientations
    are colored differently because each feature has a different . 
    This visualization demonstrates the consistency
    of orientation within a feature map and across multiple layers.
    The images are taken from layers 2, 4, 6, 8, and 10 in a clockwise
    order from largest to smallest.}
    \label{fig:flow}
\end{figure}
\section{Conclusions}
We presented a convolutional neural network that is locally equivariant to patch-wise translation and, for the first time, to continuous -rotation. We achieved this by restricting the filters to circular harmonics, essentially hard-baking rotation into the architecture. This can be implanted onto other architectures too. The use of circular harmonics pays dividends in that we receive full rotational equivariance using few parameters. This leads to good generalization, even with less (or less augmented) training data. The only disadvantage we've seen so far is the higher per-filter computational cost, but our guidance for network design balances that cost against the more expressive representation. The better interpretability of the feature maps is a bonus, because we know how they transform under input image rotations. We applied our network to the problem of classifying rotated-MNIST, setting a new state-of-the-art. We also applied our network to boundary detection, again achieving state-of-the-art results, for non-pretrained networks. We have shown that -rotational equivariance is both possible and useful. Our TensorFlow\texttrademark   implementation is available on the project website. 

\textbf{Future work}
Extension of this work could involve hard-baking yet more transformations into the equivariance properties of the Harmonic Network, possibly extending to 3D. This will allow yet more expressibility in network representations, extending the benefits we have seen afforded by rotation equivariance to a larger class of models and applications.

\textbf{Acknowledgements}
Support is from Fight for Sight UK, a Microsoft Research PhD Scholarship, EPSRC Nature Smart Cities EP/K503745/1 and ENGAGE EP/K015664/1.

{\small
\bibliographystyle{ieee}
\bibliography{SteerBib}
}

\newpage
\onecolumn
\appendix
\begin{center}
\Large
\textbf{Supplementary Material}
\end{center}


\begin{abstract}
	We include some proofs and derivations of the rotational equivariance
    properties of the circular harmonics, along with a demonstration of how
    we calculate the number of parameters for various network architectures.
\end{abstract}



\section{Equivariance properties}
In Section 3.2 we mentioned that cross-correlation with the circular harmonics
is a -rotation equivariant feature transform. Here we provide the proof,
and some of the properties mentioned in \bb{Arithmetic and Equivariance Condition}.

\subsection{Equivariance of the Circular Harmonics}
We are interested in proving that there exists a filter , such that cross-correlation of  with  yields a rotationally equivariant feature map. The proof requires us to introduce two different kinds of transformation: rotation  and translation . To simplify the math, we use vector notation, so the spatial domain of the filter/image is . We write the filter as  and image as  for . We define the transformation operators  and , such that  and , where  is a 2D rotation matrix for a  counter-clockwise rotation. We introduce rotational cross-correlation . This is defined as

where we have used the decomposition , with  and . The rotational cross-correlation is performed about the origin of the image. If we rotate the image, then we have

If we define , where , then

And so rotational cross-correlation is rotationally equivariant about the origin of rotation. In the next part, we build up to a result needed for proving the chained cross-correlation result.
\paragraph{Cross-correlation about }
To perform the rotational cross-correlation about another point , we first have to translate the image such that  is the new origin, so , then perform the rotational cross-correlation, so

\paragraph{Cross-correlation about  with rotated  about }
In general, for every  this expression returns a different value. The response of a -rotated image about  is then

\paragraph{Cross-correlation about  with rotated  about origin}
Say we wish to perform the rotational cross-correlation about a point , when the image has been rotated about the origin. Denoting , then the response is



Thus we see that cross-correlation of the rotated signal  with the circular harmonic filter  is equal to the response at zero rotation , multiplied by a complex phase shift . In the notation of the paper, we denote this multiplication by  as . Thus cross-correlation with  yields a rotationally equivariant feature mapping.

\subsection{Properties}
\subsubsection{Chained cross-correlation}
We claimed in \bb{Arithmetic and Equivariance Condition}, that the rotation order of a feature map resulting from chained cross-correlations is equal to the sum of the the rotation orders of the filters in the chain. We prove this for a chain of two filters, and the rest follows by induction. Consider taking a -rotated image  about the origin, then cross-correlating it with a filter  as every point in the image plane , followed by cross-correlation with  as a point . We already know that the response to the rotation is , for all rotations  of the input and all points  in the response plane, so we can write the chained convolution as 

We have used the property that the cross-correlation is linear and that we may pull the scalar factor  outside. If we write  then , so

Thus we see that the chained cross-correlation results in a summation of the rotation orders of the individual filters  and . Setting , such that we evaluate the cross-correlation at the center of rotation, we regain an equation similar to \ref{eq:equi}.

\subsubsection{Magnitude nonlinearities}
Point-wise nonlinearities acting on the magnitude of a feature map maintain rotational 
equivariance. Consider that we have a point on a feature map of rotation order , which 
we can write as , where  is the magnitude of the feature map and 
 is the phase component. The output of the nonlinearity  
is

since  only acts on magnitudes. Since for fixed  the output is a function of 
and  only, the point-wise magnitude-acting nonlinearity preserves rotational
equivariance.

\subsubsection{Summation of feature maps}
The summation of feature maps of the same rotation order is a new feature map of the
same rotation order. Consider two feature maps  and  of rotation
order . Summation is a pointwise operation, so we only consider two corresponding 
points in the feature maps, which we denote  and 
, where  and  are phase offsets. The
sum is

which for fixed  is a function of  and  only 
and so also rotationally equivariant with order .

\newpage
\section{Number of parameters}
Here we list a break down of how we computed the number of parameters for the 
various network architectures in the experiments section. The networks architectures
used are in Figure \ref{fig:networks}. Red boxes are cross-correlations, blue
boxes are pooling (average for H-Nets, max for regular CNNs), green boxes are 
-cross-correlations.
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.5\linewidth]{./images/designs}
    \caption{Networks used}
    \label{fig:networks}
\end{figure}

\subsection{Standard CNN}
For a standard CNN layer with  input channels and  output channels, and
 sized weights, the number of learnable parameters is .
Since there is one bias per output layer, this increases to . If using
batch normalization, then there is an extra per-channel scaling factor, which 
increases the number of learnable parameters to . The standard CNN
for the rotated MNIST experiments has 6 layers of  cross-correlations, and 
1 layer of -cross-correlations, with 20 feature maps per layer and 3 batch 
normalization layers so the number of learnable parameters is 21570. The calculations
are shown in Table \ref{tab:cnn}.
\begin{table}[h]
  \begin{center}
    \begin{tabular}{|l|c|c|c|c|}
    \hline
    Layer	& Weights		& Batch Norm/Bias	& \#Params\\
    \hline\hline
    1 	& 	& 			&	200\\
    2 	& 	& 	& 	3640\\
    3 	& 	& 			&	3620\\
    4 	& 	& 	&	3640\\
    5 	& 	& 			&	3620\\
    6 	& 	& 	&	3640\\
    7 	& 	& 			&	3210\\
    \hline
    \text{Total}	&				&				&	21570\\
    \hline
    \end{tabular}
  \end{center}
  \caption{Number of parameters for a regular CNN.}
  \label{tab:cnn}
\end{table}

\subsection{Harmonic networks}
The learnable parameters of a Harmonic Network are the radial profile and the
the per-filter phase offset. For a  filter, the number of radial
profile elements is equal to the number of rings of equal distance from the
center of the filter. For example, consider the Figure \ref{fig:radial_profile},
which is an excerpt from the main paper. This is a  filter, with
6 rings of equal distance from the center of the filter (the smallest ring 
is just a single point). So this filter has 6 radial profile terms and 1 
phase offset to learn. This contrasts with a regular filter, which would have
25 learnable parameters. Note, that for filters with rotation order ,
the center pixel of the filter is in fact always zero, and so for a 
rotation order  filter, the number of radial profile terms is .
So for the H-Net in the main paper with  filters and batch normalization
in layers 2, 4, \& 6, the number of learnable parameters is 33347. The calculations
are in Table \ref{tab:h_net}. Note that the final layer contains just one set 
of biases and no phase offsets. A similar set of calculations can be performed 
for the deeply supervised networks.
\begin{figure}[t]
\centering
	\includegraphics[width=0.7\linewidth]{./images/weights_pdf}
    \caption{Each radius has a single learnable weight. Then there is a bias for the whole filter.}
    \label{fig:radial_profile}
\end{figure}


\begin{table}[h]
  \begin{center}
    \begin{tabular}{|l|c|c|c|c|}
    \hline
    Layer	& 		& 		& Batch Norm/Bias	& \#Params\\
    \hline\hline
    1 	& 		& 		& 	&	120\\
    2 	& 		& 		& 	& 	864\\
    3 	& 	& 	& 	&	1696\\
    4 	& 	& 	& 	&	3392\\
    5 	& 	& 	& 	&	7350\\
    6 	& 	& 	& 	&	16065\\
    7 	& 				& 				& 			&	3860\\
    \hline
    \text{Total}	&					&								&				&	33347\\
    \hline
    \end{tabular}
  \end{center}
  \caption{Number of parameters for H-Net.}
  \label{tab:h_net}
\end{table}


\end{document}