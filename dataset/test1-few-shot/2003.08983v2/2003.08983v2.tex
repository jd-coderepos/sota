

\documentclass[runningheads]{llncs}


\usepackage{amsfonts} 
\usepackage{amsmath}
\usepackage{amssymb}
\let\proof\relax
\let\endproof\relax
\usepackage{amsthm}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{cite}
\usepackage{color, colortbl}
\usepackage[inline]{enumitem}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage{float}
\usepackage[symbol]{footmisc}
\usepackage{latexsym}
\usepackage{mathtools}
\usepackage{microtype}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{tabularx}
\usepackage[hyphens]{url}
\usepackage{xspace}
\usepackage[colorlinks,breaklinks=true,bookmarks=false]{hyperref}
\hypersetup{linkcolor=black}
\DeclareMathAlphabet{\mathalphm}{OMS}{cmsy}{m}{n}
\DeclareMathAlphabet{\mathalphb}{OMS}{cmsy}{b}{n}

\newcommand{\focus}[1]{{\color{black} #1}}

\newcommand{\J}{\mathbf{J}}
\newcommand{\w}{\bm{\theta}}
\newcommand{\x}{\bm{x}} \newcommand{\cc}{\bm{c}} \newcommand{\y}{\bm{y}} \newcommand{\z}{\bm{z}} \newcommand{\xvec}{\bm{x}\xspace} \newcommand{\yvec}{\bm{y}\xspace} \newcommand{\N}{\mathbb{N}\xspace}
\newcommand{\E}{\mathop{\mathbb{E} \xspace}}
\newcommand{\ind}{1 \xspace}
\newcommand{\Dcos}{D^{\text{cos}}}
\newcommand{\F}{\mathalphm{F}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\uceq}{\underset{u}{\stackrel{\mathclap{\normalfont\mbox{c}}}{=}}}
\newcommand{\ceq}{\stackrel{\mathclap{\normalfont\mbox{c}}}{=}}
\newcommand{\capprox}{\stackrel{\mathclap{\normalfont\mbox{c}}}{\approx}}
\newcommand{\cleq}{\stackrel{\mathclap{\normalfont\mbox{c}}}{\leq}}
\newcommand{\cgeq}{\stackrel{\mathclap{\normalfont\mbox{c}}}{\geq}}
\newcommand{\Z}{\mathalphm{Z}}
\newcommand{\ent}{\mathcal{H}}
\newcommand{\I}{\mathcal{I}}

\newcommand{\tran}{^\mathsf{T}}

\DeclareMathOperator*{\Prob}{Pr}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\softmax}{softmax}
\DeclareMathOperator{\sigm}{sigm}
\DeclareMathOperator{\relu}{ReLU}
\DeclareMathOperator{\maxout}{maxout}
\DeclareMathOperator{\diag}{diag}
\def\vx{{\bm{x}}}
\def\sign{{\text{sign}}}
\def\vtheta{{\bm{\theta}}}
\def\eps{{\epsilon}}


\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}
\newcommand{\specialcell}[2][c]{\begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}


\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}
\makeatother

\newcommand{\bmsf}[1]{\bm{\mathsf{#1}}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\data}{\mathcal{D} \xspace}
\newcommand{\m}{\mathcal{M} \xspace}
\newcommand{\cost}{\mathcal{C} \xspace}

\newcommand{\noise}{f \xspace}

\newcommand{\nolabel}{\text{\textemdash}}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclarePairedDelimiter\abs{\lvert}{\rvert}\newcommand{\magnitude}[1]{\left\lvert#1\right\rvert}

\newcommand{\mydelta}{\mathbf{\Delta} \xspace}
\newcommand{\W}{\mathbf{W} \xspace}
\newcommand{\h}{\mathbf{h} \xspace}
\newcommand{\g}{\mathbf{g} \xspace}
\newcommand{\base}{\mathbf{b} \xspace}
\newcommand{\xTilde}{\mathbf{\tilde{\xvec}} \xspace}
\newcommand{\yTilde}{\mathbf{\tilde{\yvec}} \xspace}
\newcommand{\xHat}{\mathbf{\hat{\xvec}} \xspace}
\newcommand{\yHat}{\mathbf{\hat{\yvec}} \xspace}
\newcommand{\mapSup}{\mathcal{M} \xspace}
\newcommand{\mapIn}{\mathcal{R}_{in} \xspace}
\newcommand{\mapOut}{\mathcal{R}_{out} \xspace}
\newcommand{\mapLink}{\mathcal{R}_{link} \xspace}
\newcommand{\costSup}{\mathbf{\ell_{sup}} \xspace}
\newcommand{\costIn}{\mathbf{\ell_{in}} \xspace}
\newcommand{\costOut}{\mathbf{\ell_{out}} \xspace}
\newcommand{\costLink}{\mathbf{\ell_{link}} \xspace}
\newcommand{\spaceXTilde}{\mathcal{{\tilde{X}}}}
\newcommand{\spaceYTilde}{\mathcal{{\tilde{Y}}}}
\newcommand{\D}{\mathbf{\mathcal{D}} \xspace}
\newcommand{\thetaModel}{\mathbf{\theta} \xspace}
\newcommand{\thetaLink}{\mathbf{\theta}_{link} \xspace}
\newcommand{\thetaIn}{\mathbf{\theta}_{in} \xspace}
\newcommand{\thetaOut}{\mathbf{\theta}_{out} \xspace}
\newcommand{\px}{p(\x) \xspace}
\newcommand{\py}{p(\y) \xspace}
\newcommand{\pygivenx}{p(\y|\x) \xspace}
\newcommand{\spaceX}{\mathcal{X} \xspace}
\newcommand{\spaceY}{\mathcal{Y} \xspace}
\newcommand{\DataN}{D_{n}=\{\x_i,\mathbf{y}_i\}^n_{i=1} \xspace}
\newcommand{\scoreFunc}{f(\x_i, \mathbf{y}_i, \mathbf{w}) \xspace}
\newcommand{\hingeLossDelta}{\mathcal{L}(\x_i, \y_i, \w) = \max\limits_y [f(\x_i, \y, \w) + \mydelta(\y, \y_i)]-f(\x_i, y_i, w) \xspace}
\newcommand{\costTotal}{\mathfrak{L} \xspace}
\newcommand{\encIn}{{f_{enc}}}
\newcommand{\decIn}{{f_{dec}}}
\newcommand{\encOut}{{g_{enc}}}
\newcommand{\decOut}{{g_{dec}}}
\newcommand{\hLink}{{h_{link}}}
\newcommand{\hSup}{{\mathbf{{H}_{sup}}}}
\newcommand{\lamin}{{\mathbf{\lambda}_{in}}}
\newcommand{\lamout}{{\mathbf{\lambda}_{out}}}
\newcommand{\lamsup}{{\mathbf{\lambda}_{sup}}}
\newcommand{\sS}{{\mathcal{S}}}
\newcommand{\sU}{{\mathcal{U}}}
\newcommand{\sL}{{\mathcal{L}}}
\newcommand{\sF}{{\mathcal{F}}}
\newcommand{\costTot}{\mathbf{\ell} \xspace}


\newcommand{\xest}{\hat{\xvec}}
\newcommand{\yest}{\hat{\yvec}}
\newcommand{\xproj}{\tilde{\xvec}}
\newcommand{\yproj}{\tilde{\yvec}}



\DeclareMathOperator{\card}{card}

\makeatletter

\newcommand\Autoref[1]{\@first@ref#1,@}
\def\@throw@dot#1.#2@{#1}\def\@set@refname#1{\edef\@tmp{\getrefbykeydefault{#1}{anchor}{}}\xdef\@tmp{\expandafter\@throw@dot\@tmp.@}\ltx@IfUndefined{\@tmp autorefnameplural}{\def\@refname{\@nameuse{\@tmp autorefname}s}}{\def\@refname{\@nameuse{\@tmp autorefnameplural}}}}
\def\@first@ref#1,#2{\ifx#2@\autoref{#1}\let\@nextref\@gobble \else \@set@refname{#1}\@refname~\ref{#1}\let\@nextref\@next@ref \fi \@nextref#2}
\def\@next@ref#1,#2{\ifx#2@ and~\ref{#1}\let\@nextref\@gobble \else, \ref{#1}\fi \@nextref#2}

\makeatother
 \usepackage{authblk}

\definecolor{Gray}{gray}{0.97}

\def\equationautorefname{Eq.}
\def\figureautorefname{Fig.}
\def\sectionautorefname{Section}
\def\subsectionautorefname{Section}
\def\propositionautorefname{Proposition}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\makeatletter \newcommand{\printfnsymbol}[1]{\textsuperscript{\@fnsymbol{#1}}}
\makeatother

\begin{document}
\pagestyle{headings}
\mainmatter
\def\ECCVSubNumber{6496}  

\title{A unifying mutual information view of \\ metric learning:  cross-entropy vs. pairwise losses}
\titlerunning{Metric learning: cross-entropy vs. pairwise losses}

\author{Malik~Boudiaf\thanks{Equal contributions}\inst{1}\and
Jérôme~Rony\printfnsymbol{1}\inst{1}\and
Imtiaz~Masud~Ziko\printfnsymbol{1}\inst{1}\and
Eric~Granger\inst{1}\and
Marco~Pedersoli\inst{1}\and
Pablo~Piantanida\inst{2}\and
Ismail~Ben~Ayed\inst{1}
}
\authorrunning{M. Boudiaf et al.}
\institute{Laboratoire d'Imagerie, de Vision et d'Intelligence Artificielle (LIVIA),\\ ÉTS Montreal, Canada\\ \email{\{malik.boudiaf.1, jerome.rony.1, imtiaz-masud.ziko.1\}@etsmtl.net}\and
Laboratoire des Signaux et Systèmes (L2S),\\ CentraleSupelec-CNRS-Université Paris-Saclay, France}
\maketitle

\begin{abstract}


Recently, substantial research efforts in Deep Metric Learning (DML) focused on designing complex pairwise-distance losses, which require convoluted schemes to ease optimization, such as sample mining or pair weighting.
The standard cross-entropy loss for classification has been largely overlooked in DML.
On the surface, the cross-entropy may seem unrelated and irrelevant to metric learning as it does not explicitly involve pairwise distances.
However, we provide a theoretical analysis that links the cross-entropy to several well-known and recent pairwise losses.
Our connections are drawn from two different perspectives: one based on an explicit optimization insight; the other on discriminative and generative views of the mutual information between the labels and the learned features.
First, we explicitly demonstrate that the cross-entropy is an upper bound on a new pairwise loss, which has a structure similar to various pairwise losses: it minimizes intra-class distances while maximizing inter-class distances.
As a result, minimizing the cross-entropy can be seen as an approximate \emph{bound-optimization} (or \emph{Majorize-Minimize}) algorithm for minimizing this pairwise loss.
Second, we show that, more generally, minimizing the cross-entropy is actually equivalent to maximizing the mutual information, to which we connect several well-known pairwise losses. 
Furthermore, we show that various standard pairwise losses can be explicitly related to one another via bound relationships.
Our findings indicate that the cross-entropy represents a proxy for maximizing the mutual information -- as pairwise losses do -- without the need for convoluted sample-mining heuristics. Our experiments\footnote[2]{Code available at: \url{https://github.com/jeromerony/dml_cross_entropy}} over four standard DML benchmarks 
strongly support our findings. We obtain state-of-the-art results, outperforming recent and complex DML methods. \keywords{Metric Learning, Deep Learning, Information Theory}
\end{abstract}
 \section{Introduction}
The core task of metric learning consists in learning a metric from high-dimensional data, such that the distance between two points, as measured by this metric, reflects their semantic similarity. Applications of metric learning include image retrieval, zero-shot learning or person re-identification, among others. Initial attempts to tackle this problem tried to learn metrics directly on the input space \cite{lowe1995similarity}. Later, the idea of learning suitable embedding was introduced, with the goal of learning Mahalanobis distances \cite{xing2003distance, schultz2004learning, goldberger2005neighbourhood, weinberger2009distance, davis2007information}, which corresponds to learning the best linear projection of the input space onto a lower-dimensional manifold, and using the Euclidean distance as a metric. Building on the embedding-learning ideas, several papers proposed to learn more complex mappings, either by kernelization of already existing linear algorithms \cite{davis2007information}, or by using a more complex hypothesis such as linear combinations of gradient boosted regressions trees \cite{kedem2012non}.

The recent success of deep neural networks at learning complex,  nonlinear mappings of high-dimensional data aligns with the problem of learning a suitable embedding. Following works on Mahalanobis distance learning, most Deep Metric Learning (DML) approaches are based on pairwise distances. Specifically, the current paradigm is to learn a deep encoder 
that maps points with high semantic similarity
close to each other in the embedded space (\wrt pairwise Euclidean or cosine distances). 
This paradigm concretely translates into {\em pairwise losses} that encourage 
small distances for pairs of samples from the same class
and large distances for pairs of samples from different classes.
While such formulations seem intuitive, the practical implementations and optimization schemes for pairwise losses may become cumbersome, and randomly assembling pairs of samples typically results in slow convergence or degenerate solutions \cite{hermans2017defense}. Hence, research in DML focused on finding efficient ways to reformulate, generalize and/or improve sample mining and/or sample weighting strategies over the existing pairwise losses. Popular pairwise losses include triplet loss and its derivatives \cite{hermans2017defense, sohn2016improved, song2016deep, zheng2019hardness, ge2018deep}, contrastive loss and its derivatives \cite{hadsell2006dimensionality, wang2019multi}, Neighborhood Component Analysis and its derivatives \cite{goldberger2005neighbourhood, movshovitz2017no, wu2018improving}, among others. However, such modifications are often heuristic-based, and come at the price of increased complexity and additional hyper-parameters, reducing the potential of these methods in real-world applications. Furthermore, the recent experimental study in \cite{reality_check} showed that the improvement brought by an abundant metric learning literature in the last 15 years is at best marginal when the methods are compared fairly.

Admittedly, the objective of learning a useful embedding of data points intuitively aligns with the idea of directly acting on the distances between pairs of points in the embedded space. Therefore, the standard cross-entropy loss, widely used in classification tasks, has been largely overlooked by the DML community, most likely due to its apparent irrelevance for Metric Learning \cite{center_loss}. As a matter of fact, why would anyone use a point-wise prediction loss to enforce pairwise-distance properties on the embedding space? Even though the cross-entropy was shown to be competitive for face recognition applications \cite{deephypersphere, wang2018cosface, wang2018additivesoftmax}, to the best of our knowledge, only one paper empirically observed competitive results of a normalized, temperature-weighted version of the cross-entropy in the context of deep metric learning \cite{zhai2018classification}. However, the authors did not provide any theoretical insights for these results. 


On the surface, the standard cross-entropy loss may seem unrelated to the pairwise losses used in DML. Here, we provide theoretical justifications that connect directly the cross-entropy to several well-known and recent pairwise losses. Our connections are drawn from two different perspectives; one based on an explicit optimization insight and the other on mutual-information arguments. We show that four of the most prominent pairwise metric-learning losses, as well as the standard cross-entropy, are maximizing a common underlying objective: the Mutual Information (MI) between the learned embeddings and the corresponding samples' labels. As sketched in \autoref{sec:two_views_on_mi}, this connection can be intuitively understood by writing this MI in two different, but equivalent ways. Specifically, we establish tight links between pairwise losses and the \textit{generative} view of this MI. We study the particular case of contrastive loss \cite{hadsell2006dimensionality}, explicitly showing its relation to this MI. We further generalize this reasoning to other DML losses by uncovering tight relations with contrastive loss.
As for the cross-entropy, we demonstrate that the cross-entropy is an upper bound on an underlying pairwise loss -- on which the previous reasoning can be applied -- which has a structure similar to various existing pairwise losses. As a result, minimizing the cross-entropy can be seen as an approximate \emph{bound-optimization} (or \emph{Majorize-Minimize}) algorithm for minimizing this pairwise loss, implicitly minimizing intra-class distances and maximizing inter-class distances.  We also show that, more generally, minimizing the cross-entropy is equivalent to maximizing the \textit{discriminative} view of the mutual information. Our findings indicate that the cross-entropy represents a proxy for maximizing the mutual information, as pairwise losses do, without the need for complex sample-mining and optimization schemes. Our comprehensive experiments over four standard DML benchmarks (CUB200, Cars-196, Stanford Online Product and In-Shop) strongly support our findings. We consistently obtained state-of-the-art results, outperforming many recent and complex DML methods.

\subsubsection{Summary of contributions}
\begin{enumerate}
    \item Establishing relations between several pairwise DML losses and a generative view of the mutual information between the learned features and labels;
    \item Proving explicitly that optimizing the standard cross-entropy corresponds to an approximate bound-optimizer of an underlying pairwise loss;
    \item More generally, showing that minimizing the standard cross-entropy loss is equivalent to maximizing a discriminative view of the mutual information between the features and labels. 
    \item Demonstrating state-of-the-art results with cross-entropy on several DML benchmark datasets.
\end{enumerate}

%
 \section{On the two views of the mutual information}\label{sec:two_views_on_mi}

\begin{table}
    \caption{Definition of the random variables and information measures used in this paper.}
    \label{table:notations}
    \centering
    \begin{tabular}{lc}
    \multicolumn{2}{c}{General} \\
    \toprule
    Labeled dataset &  \\
    \midrule
    Input feature space &  \\
    \midrule
    Embedded feature space &  \\
    \midrule
    Label/Prediction space &  \\
    \midrule
    Euclidean distance &  \\
    \midrule
    Cosine distance &  \\
    \bottomrule
    \end{tabular}
    \qquad
    \begin{tabular}{lc}
    \multicolumn{2}{c}{Model} \\
    \toprule
    Encoder &  \\
    \midrule
    Soft-classifier &  \\
    \bottomrule \\
    \multicolumn{2}{c}{Random variables (RVs)} \\
    \toprule
    Data &  ,  \\
    \midrule
    Embedding &   \\
    \midrule 
    Prediction &   \\
    \bottomrule
    \end{tabular}
    \begin{tabular}{lc}
    \multicolumn{2}{c}{\rule{0pt}{4ex}Information measures} \\
    \toprule
    Entropy of  &  \\
    \midrule
    Conditional entropy of  given   &  \\
    \midrule
    Cross entropy (CE) between  and   &  \\
    \midrule
    Conditional CE given   &  \\
    \midrule
    Mutual information between  and  &   \\
    \bottomrule
    \end{tabular}
\end{table}


The Mutual Information (MI) is a well known-measure designed to quantify the amount of information shared by two random variables. Its formal definition is presented in \autoref{table:notations}. Throughout this work, we will be particularly interested in  which represents the MI between learned 
features  and labels . 
Due to its symmetry property, the MI can be written in two ways, which we will refer to as the \textit{discriminative view} and \textit{generative view} of MI:

While being analytically equivalent, these two views present two different, complementary interpretations. In order to maximize , the discriminative view conveys that the labels should be balanced (out of our control) and 
easily identified from the features. On the other hand, the generative view conveys that the features learned should spread as much as possible in the feature space, while keeping samples sharing the same class close to each other. Hence, the discriminative view is more focused on label identification, while the generative view focuses on more explicitly shaping the distribution of the features learned by the model. Therefore, the MI enables us to draw links between classification losses (\eg cross-entropy) and feature-shaping losses (including all the well-known pairwise metric learning losses). \section{Pairwise losses and the generative view of the MI}\label{sec:pairwise_losses}

In this section, we study four pairwise losses used in the DML community: center loss \cite{center_loss}, contrastive loss \cite{hadsell2006dimensionality}, Scalable Neighbor Component Analysis (SNCA) loss \cite{wu2018improving} and Multi-Similarity (MS) loss \cite{wang2019multi}. We show that these losses can be interpreted as proxies for maximizing the generative view of mutual information . We begin by analyzing the specific example of contrastive loss, establishing its tight link to the MI, and further generalize our analysis to the other pairwise losses (see \autoref{table:losses}). Furthermore, we show that these pairwise metric-learning losses can be explicitly linked to one another via bound relationships. 

\subsection{The example of contrastive loss}\label{subsec:contrastive_example}

We start by analyzing the representative example of contrastive loss \cite{hadsell2006dimensionality}. For a given margin , this loss is formulated as:

where . This loss naturally breaks down into two terms: a \textit{tightness} part  and a \textit{contrastive} part . The tightness part encourages samples from the same class to be close to each other and form \textit{tight} clusters. As for the \textit{contrastive} part, it forces samples from different classes to stand far apart from one another in the embedded feature space. Let us analyze these two terms from a mutual-information perspective.

As shown in the next subsection, the tightness part of contrastive loss is equivalent to the tightness part of the center loss \cite{center_loss}: , where  denotes the mean of feature points from class  in embedding space  and symbol  denotes equality up to a multiplicative and/or additive constant. Written in this way, we can interpret  as a conditional cross entropy between  and another random variable , whose conditional distribution given  is a standard Gaussian centered around : :

As such,  is an upper bound on the conditional entropy that appears in the mutual information: 

This bound is tight when . Hence, minimizing  can be seen as minimizing , which exactly encourages the encoder  to produce low-entropy (=compact) clusters in the feature space for each given class. Notice that using this term only will inevitably lead to a trivial encoder that maps all data points in  to a single point in the embedded space , hence achieving a global optimum.

To prevent such a trivial solution, a second term needs to be added. This second term -- that we refer to as the \textit{contrastive} term -- is designed to push each point away from points that have a different label. In this term, only pairs such that  produce a cost. Given a pair , let us define . Given that , one can show the following: . Using linear approximation  (with error at most ), we obtain:

While the second term in \autoref{eq:contrastive_contrast} is redundant with the tightness objective, the first term is close to the differential entropy estimator proposed in \cite{wang2011information}:

Both terms measure the spread of , even though they present different gradient dynamics.
All in all, minimizing the whole contrastive loss can be seen as a proxy for maximizing the MI between the labels  and the embedded features :


\subsection{Generalizing to other pairwise losses}

\begin{table}
\caption{
Several well-known and/or recent DML losses broken into a \textit{tightness} term and a \textit{contrastive} term. Minimizing the cross-entropy corresponds to an approximate bound optimization of PCE. 
}
\label{table:losses}
\begin{tabular}{lcc}
\textbf{Loss} & \textbf{Tightness part}  & \textbf{Contrastive part}  \\ 
\toprule
Center \cite{center_loss} 
& 
& 
\\
\midrule
Contrast \cite{hadsell2006dimensionality} 
&  
&  \\
\midrule
SNCA \cite{wu2018improving} 
&  
& \\
\midrule
MS \cite{wang2019multi} 
& 
&  \\
\midrule
\begin{tabular}{l}
     PCE\\
     Prop. \ref{prop:cross_entropy_lower_bound}
\end{tabular}
&  
& \begin{tabular}{c}
 \\
\end{tabular} \\
\bottomrule
\end{tabular}
\end{table}

A similar analysis can be carried out on other, more recent metric learning losses. More specifically, they can also be broken down into two parts: a \textit{tightness} part that minimizes intra-class distances to form compact clusters, which is related to the \emph{conditional entropy} , and a second \emph{contrastive} part that prevents trivial solutions by maximizing inter-class distances, which is related to the \emph{entropy} of features . Note that, in some pairwise losses, there might be some redundancy between the two terms, \ie, the tightness term also contains some contrastive subterm, and vice-versa. For instance, the cross-entropy loss is used as the contrastive part of the center-loss but, as we show in \autoref{sec:cross_entropy_mi}, the cross-entropy, used alone, already contains both tightness (conditional entropy) and contrastive (entropy) parts. \autoref{table:losses} presents the split for four DML losses. The rest of the section is devoted to exhibiting the close relationships between several pairwise losses and the tightness and contrastive terms (\ie,  and  ).

\textbf{Links between losses:}
In this section, we show that the tightness and contrastive parts of the pairwise losses in \autoref{table:losses}, even though different at first sight, can actually be related to one another.\\

\begin{lemma}
\label{prop:tightness_terms}
Let  denote the tightness part of the loss from method A. Assuming  that features are -normalized, and that classes are balanced, the following relations between Center \cite{center_loss}, Contrastive \cite{hadsell2006dimensionality}, SNCA \cite{wu2018improving} and MS \cite{wang2019multi} losses hold:

Where  stands for lower than, up to a multiplicative and an additive constant, and  stands for equal to, up to a multiplicative and an additive constant.
\end{lemma}

The detailed proof of \autoref{prop:tightness_terms} is deferred to the supplemental material.
As for the contrastive parts, we show in the supplemental material that both  and  are lower bounded by a common contrastive term that is directly related to . We do not mention the \textit{contrastive} term of center-loss, as it represents the cross-entropy loss, which is exhaustively studied in \autoref{sec:ce}. \section{Cross-entropy does it all}\label{sec:ce}

We now completely change gear to focus on the widely used {\em unary} classification loss: cross-entropy. On the surface, the cross-entropy may seem unrelated to metric-learning losses as it does not involve pairwise distances. We show that a close relationship exists between these pairwise losses widely used in deep metric learning and the cross-entropy classification loss. This link can be drawn from two different perspectives, one is based on an explicit optimization insight and the other is based on a discriminative view of the mutual information. First, we explicitly demonstrate that the cross-entropy is an upper bound on a new pairwise loss, which has a structure similar to all the metric-learning losses listed in  \autoref{table:losses}, \ie, it contains a tightness term and a contrastive term. Hence, minimizing the cross-entropy can be seen as an approximate {\em bound-optimization (or Majorize-Minimize)} algorithm for minimizing this pairwise loss. Second, we show that, more generally, minimization of the cross-entropy is actually equivalent to maximization of the mutual information, to which we connected various DML losses. These findings indicate that the cross-entropy represents a proxy for maximizing , just like pairwise losses, without the need for dealing with the complex sample mining and optimization schemes associated to the latter.

\subsection{The pairwise loss behind unary cross-entropy}\label{sec:cross_entropy_explicit}

\textbf{Bound optimization:}
Given a function  that is either intractable or hard to optimize, bound optimizers are iterative algorithms that instead optimize
auxiliary functions (upper bounds on ). These auxiliary functions are usually more tractable than the
original function . Let  be the current iteration index, then  is an auxiliary function if:

A bound optimizer follows a two-step procedure: first an auxiliary function  is computed, then  is minimized, such that:

This iterative procedure is guaranteed to decrease the original function :

Note that bound optimizers are widely used in machine learning. Examples of well-known bound optimizers include the concave-convex procedure (CCCP) \cite{cccp}, expectation maximization (EM) algorithms or submodular-supermodular procedures (SSP) \cite{ssp}. Such optimizers are particularly used in clustering \cite{tang2015kernel} and, more generally, in problems involving latent-variable optimization.
 
\textbf{Pairwise Cross-Entropy:} We now prove that  minimizing cross-entropy can be viewed as an approximate bound optimization of a more complex pairwise loss. 
\begin{proposition}\label{prop:cross_entropy_lower_bound}
Alternately minimizing the cross-entropy loss  with respect to the encoder's parameters  and the classifier's weights  can be viewed as an approximate bound-optimization of a Pairwise Cross-Entropy (PCE) loss, which we define as follows:

Where  represents the soft-mean of class ,  represents the softmax probability of point  belonging to class k, and  depends on the encoder .
\end{proposition}



The full proof of \autoref{prop:cross_entropy_lower_bound} is provided in the supplemental material. We hereby provide a quick sketch. 
Considering the usual softmax parametrization for our model's predictions , the idea is to break the cross-entropy loss in two terms, and artificially add and remove the regularization term :

By properly choosing  in Eq. \eqref{eq:ce_split}, both  and  become convex functions of . For any class , we then show that the optimal values of  for  and  are proportional to, respectively, the hard mean  and the soft mean  of class . By plugging-in those optimal values, we can lower bound  and  individually in \autoref{eq:ce_split} and get the result.


\autoref{prop:cross_entropy_lower_bound} casts a new light on the cross-entropy loss by explicitly relating it to a new pairwise loss (PCE), following the intuition that the optimal weights  of the final layer, \ie, the linear classifier, are related to the centroids of each class in the embedded feature space . Specifically, finding the optimal classifier's weight  for cross-entropy can be interpreted as building an auxiliary function  on . Subsequently minimizing cross-entropy \wrt the encoder's weights  can be interpreted as the second step of bound optimization on . 
Similarly to other metric learning losses, PCE contains a  part that encourages samples from the same classes to align with one another. In echo to \autoref{prop:tightness_terms}, this tightness term, noted , is equivalent, up to multiplicative and additive constants, to  and , when the features are assumed to be normalized:

 PCE also contains a  part, divided into two  terms. The first pushes all samples away from one another, while the second term forces soft means  far from the origin. Hence, minimizing the cross-entropy can be interpreted as implicitly minimizing a pairwise loss whose structure appears similar to the well-established metric-learning losses in \autoref{table:losses}. \\
 
 \textbf{Simplified Pairwise Cross-Entropy:} While PCE brings interesting theoretical insights, the computation of the parameter  at every iteration requires computating the eigenvalues of a  matrix at every iteration (cf. full proof in supplemental material), which makes the implementation of PCE difficult in practice.
In order to remove the dependence upon , one can plug in the same  for both  and  in \autoref{eq:ce_split}. We choose to use . This yields a simplified version of PCE, that we call SPCE:
\newcommand{\lspce}{\mathcal{L}_{SPCE} = \underbrace{-\frac{1}{n^2} \sum_{i=1}^n\sum_{j:y_j=y_i} \z_i\tran \z_j}_{\textsc{tightness}} + \underbrace{\frac{1}{n} \sum_{i=1}^n \log\sum_{k=1}^K \exp \Big(\frac{1}{n} \sum\limits_{j:y_j=k}\z_i\tran \z_j\Big)}_{\textsc{contrastive}}}

SPCE and PCE are similar (the difference is that PCE was derived after plugging in the soft means instead of hard means in ). Contrary to PCE, however, SPCE is easily computable, and the preliminary experiments we provide in the supplementary material indicate that CE and SPCE exhibit similar behaviors at training time. 
Interestingly, our derived SPCE loss has a form similar to contrastive learning losses in unsupervised representation learning \cite{oord2018representation, tschannen2019mutual, chen2020simple}.



 


\subsection{A discriminative view of mutual information}\label{sec:cross_entropy_mi}

\begin{lemma}\label{prop:cross_ent_and_mi}
Minimizing the conditional cross-entropy loss, denoted by  , is equivalent to maximizing the mutual information .
\end{lemma}




The proof of \autoref{prop:cross_ent_and_mi} is provided in the supplementary material. Such result is compelling. Using the discriminative view of mutual information allows to show that minimizing cross-entropy loss is equivalent to maximizing the mutual information . This information theoretic argument reinforces our conclusion from \autoref{prop:cross_entropy_lower_bound} that cross-entropy and the previously described metric learning losses are essentially doing the same job.

\subsection{Then why would cross-entropy work better?}

We showed that cross-entropy essentially optimizes the same underlying mutual information  as other DML losses. This fact alone is not enough to explain why the cross-entropy is able to consistently achieve better results than DML losses as shown in \autoref{sec:exps}. We argue that the difference is in the optimization process. On the one hand, pairwise losses require careful sample mining and weighting strategies to obtain the most informative pairs, especially when considering mini-batches, in order to achieve convergence in a reasonable amount of time, using a reasonable amount of memory. On the other hand, optimizing cross-entropy is substantially easier as it only implies minimization of unary terms. 
Essentially, cross-entropy does it all without dealing with the difficulties of pairwise terms.
Not only it makes optimization easier, but also it simplifies the implementation, thus increasing its potential applicability in real-world problems.






%
 \section{Experiments}
\label{sec:exps}

\subsection{Metric}
The most common metric used in DML is the recall.
Most methods, especially recent ones, use the cosine distance to compute the recall for the evaluation. They include  normalization of the features in the model \cite{oh2017deep, movshovitz2017no, wang2017deep, opitz2017bier, ge2018deep, yuan2017hard, xuan2020improved, zhai2018classification, wang2019multi, sanakoyeu2019divide, xuan2018deep}, which makes cosine and Euclidean distances equivalent. Computing cosine similarity is also more memory efficient and typically leads to better results \cite{schroff2015facenet}. For these reasons, the Euclidean distance on non normalized features has rarely been used for both training and evaluation.
In our experiments, -normalization of the features during training actually hindered the final performance, which might be explained by the fact that we add a classification layer on top of the feature extractor. Thus, we did not -normalize the features during training and reported the recall with both Euclidean and cosine distances.

\subsection{Datasets}

\setlength{\tabcolsep}{5pt}
\begin{table}[t]
\centering
\caption{Summary of the datasets used for evaluation in metric learning.}
\label{table:datasets}
\begin{tabularx}{\textwidth}{lccc}
Name & Objects & Categories & Images \\ 
\toprule
Caltech-UCSD Birds-200-2011 (CUB)\cite{WahCUB_200_2011} & Birds & 200 & 11\,788 \\
Cars Dataset \cite{krause20133d} & Cars & 196 & 16\,185 \\
Stanford Online Products (SOP) \cite{song2016deep} & House furniture & 22\,634 & 120\,053 \\
In-shop Clothes Retrieval \cite{liu2016deepfashion} & Clothes & 7\,982 & 52\,712 \\
\bottomrule
\end{tabularx}
\end{table}
Four datasets are commonly used in metric learning to evaluate the performances. These datasets are summarized in \autoref{table:datasets}. CUB \cite{WahCUB_200_2011}, Cars \cite{krause20133d} and SOP \cite{song2016deep} datasets are divided into train and evaluation splits. For the evaluation, the recall is computed between each sample of the evaluation set and the rest of the set. In-Shop \cite{liu2016deepfashion} is divided into a query and a gallery set. The recall is computed between each sample of the query set and the whole gallery set.

\subsection{Training specifics}

\textbf{Model architecture and pre-training:}
In the metric learning literature, several architectures have been used, which historically correspond to the state-of-the-art image classification architectures on ImageNet \cite{deng2009imagenet}, with an additional constraint on model size (\ie, the ability to train on one or two GPUs in a reasonable time). These include GoogLeNet \cite{szegedy2015going} as in \cite{kim2018attention}, BatchNorm-Inception \cite{szegedy2016rethinking} as in \cite{wang2019multi} and ResNet-50 \cite{he2016identity} as in \cite{xuan2020improved}. They have large differences in classification performances on ImageNet, but the impact on performances over DML benchmarks has rarely been studied in controlled experiments. As this is not the focus of our paper, we use ResNet-50 for our experiments. We concede that one may obtain better performances by modifying the architecture (\eg, reducing model stride and performing multi-level fusion of features). Here, we limit our comparison to standard architectures. Our implementation uses the PyTorch \cite{NIPS2019_9015} library, and initializes the ResNet-50 model with  weights pre-trained on ImageNet.



\begin{table}
\centering
\caption{Performance on CUB200, Cars-196, SOP and In-Shop datasets.  refers to the distance used to compute the recall when evaluating.}
\label{table:results}
\resizebox{\textwidth}{!}{
\begin{tabular}{clccc}
& Method &  & Architecture & Recall at \\
\toprule
\multirow{13}{*}{\begin{turn}{90}Caltech-UCSD Birds-200-2011\end{turn}} & & & & 
\multirow{13}{*}{\begin{tabular}{cccccc} 1 & 2 & 4 & 8 & 16 & 32\\
47.2 & 58.9 & 70.2 & 80.2 & 89.3 & 93.2 \\
49.2 & 61.9 & 67.9 & 81.9 & - & - \\
57.1 & 68.8 & 78.7 & 86.5 & 92.5 & 95.5 \\
60.6 & 71.5 & 79.8 & 87.4 & -- & -- \\
60.7 & 72.4 & 81.9 & 89.2 & 93.7 & 96.8 \\
63.9 & 75.0 & 83.1 & 89.7 & -- & -- \\
64.9 & 75.3 & 83.5 & -- & -- & --\\
65.3 & 76.7 & 85.4 & 91.8 & -- & -- \\
65.7 & 77.0 & 86.6 & 91.2 & 95.0 & 97.3 \\
65.9 & 76.6 & 84.4 & 90.6 & -- & --\\
67.6 & 78.1 & 85.6 & 91.1 & 94.7 & 97.2 \\
69.2 & 79.2 & 86.9 & 91.6 & 95.0 & 97.3 \\
\end{tabular}}\\
\cline{5-5}
& Lifted Structure \cite{song2016deep} &  & GoogLeNet & \\
& Proxy-NCA \cite{movshovitz2017no} & cos & BN-Inception & \\
& HTL \cite{ge2018deep} & cos & GoogLeNet & \\
& ABE \cite{kim2018attention} & cos & GoogLeNet & \\
& HDC \cite{yuan2017hard} & cos & GoogLeNet & \\
& DREML \cite{xuan2018deep} & cos & ResNet-18 & \\
& EPSHN \cite{xuan2020improved} & cos & ResNet-50 & \\
& NormSoftmax \cite{zhai2018classification} & cos & ResNet-50 & \\
& Multi-Similarity \cite{wang2019multi} & cos & BN-Inception& \\
& D\&C \cite{sanakoyeu2019divide} & cos & ResNet-50 & \\
\cline{2-5}
& \multirow{2}{*}{Cross-Entropy} &  & \multirow{2}{*}{ResNet-50} & \\
& & cos & & \\
\midrule[0.75pt]
\multirow{13}{*}{\begin{turn}{90}Stanford Cars\end{turn}} & & & & 
\multirow{13}{*}{\begin{tabular}{cccccc} 1 & 2 & 4 & 8 & 16 & 32\\
49.0 & 60.3 & 72.1 & 81.5 & 89.2 & 92.8 \\
73.2 & 82.4 & 86.4 & 88.7 & -- & -- \\
81.4 & 88.0 & 92.7 & 95.7 & 97.4 & 99.0 \\
82.7 & 89.3 & 93.0 & -- & -- & --\\
83.8 & 89.8 & 93.6 & 96.2 & 97.8 & 98.9 \\
84.1 & 90.4 & 94.0 & 96.5 & 98.0 & 98.9 \\
84.6 & 90.7 & 94.1 & 96.5 & -- & -- \\
85.2 & 90.5 & 94.0 & 96.1 & -- & -- \\
86.0 & 91.7 & 95.0 & 97.2 & -- & -- \\
89.3 & 94.1 & 96.4 & 98.0 & -- & -- \\
89.1 & 93.7 & 96.5 & 98.1 & 99.0 & 99.4 \\
89.3 & 93.9 & 96.6 & 98.4 & 99.3 & 99.7 \\
\end{tabular}}\\
\cline{5-5}
& Lifted Structure \cite{song2016deep} &  & GoogLeNet & \\
& Proxy-NCA \cite{movshovitz2017no} & cos & BN-Inception \\
& HTL \cite{yuan2017hard}  & cos & GoogLeNet & \\
& EPSHN \cite{xuan2020improved} & cos & ResNet-50 & \\
& HDC \cite{yuan2017hard} & cos & GoogLeNet & \\
& Multi-Similarity \cite{wang2019multi} & cos & BN-Inception & \\
& D\&C \cite{sanakoyeu2019divide} & cos & ResNet-50 & \\
& ABE \cite{kim2018attention} & cos & GoogLeNet & \\
& DREML \cite{xuan2018deep} & cos & ResNet-18 & \\
& NormSoftmax \cite{zhai2018classification} & cos & ResNet-50 &  \\
\cline{2-5}
& \multirow{2}{*}{Cross-Entropy} &  & \multirow{2}{*}{ResNet-50} & \\
& & cos & & \\
\midrule[0.75pt]
\multirow{11}{*}{\begin{turn}{90}Stanford Online Product\end{turn}} & & & & 
\multirow{11}{*}{\setlength{\tabcolsep}{13.5pt}\begin{tabular}{cccc} 1 & 10 & 100 & 1000\\
62.1 & 79.8 & 91.3 & 97.4 \\
70.1 & 84.9 & 93.2 & 97.8 \\
74.8 & 88.3 & 94.8 & 98.4 \\
75.9 & 88.4 & 94.9 & 98.1 \\
76.3 & 88.4 & 94.8 & 98.2 \\
78.2 & 90.5 & 96.0 & 98.7 \\
78.3 & 90.7 & 96.3 & -- \\
79.5 & 91.5 & 96.7 & -- \\
80.8 & 91.2 & 95.7 & 98.1 \\
81.1 & 91.7 & 96.3 & 98.8 \\
\end{tabular}}
\setlength{\tabcolsep}{1pt}\\
\cline{5-5}
& Lifted Structure \cite{song2016deep} &  & GoogLeNet & \\
& HDC \cite{yuan2017hard} & cos & GoogLeNet & \\
& HTL \cite{ge2018deep} & cos & GoogLeNet & \\
& D\&C \cite{sanakoyeu2019divide} & cos & ResNet-50 & \\
& ABE \cite{kim2018attention} & cos & GoogLeNet & \\
& Multi-Similarity \cite{wang2019multi} & cos & BN-Inception &\\
& EPSHN \cite{xuan2020improved} & cos & ResNet-50 & \\
& NormSoftmax \cite{zhai2018classification} & cos & ResNet-50 & \\
\cline{2-5}
& \multirow{2}{*}{Cross-Entropy} &  & \multirow{2}{*}{ResNet-50} & \\
& & cos & & \\
\midrule[0.75pt]
\multirow{11}{*}{\begin{turn}{90}In-Shop Clothes Retrieval\end{turn}} & & & & 
\multirow{11}{*}{\begin{tabular}{cccccc} 1 & 10 & 20 & 30 & 40 & 50\\
62.1 & 84.9 & 89.0 & 91.2 & 92.3 & 93.1 \\
78.4 & 93.7 & 95.8 & 96.7 & -- & -- \\
80.9 & 94.3 & 95.8 & 97.2 & 97.4 & 97.8 \\
85.7 & 95.5 & 96.9 & 97.5 & -- & 98.0 \\
87.3 & 96.7 & 97.9 & 98.2 & 98.5 & 98.7 \\
87.8 & 95.7 & 96.8 & -- & -- & -- \\
89.4 & 97.8 & 98.7 & 99.0 & -- & -- \\
89.7 & 97.9 & 98.5 & 98.8 & 99.1 & 99.2 \\
90.6 & 97.8 & 98.5 & 98.8 & 98.9 & 99.0 \\
90.6 & 98.0 & 98.6 & 98.9 & 99.1 & 99.2 \\
\end{tabular}}\\
\cline{5-5}
& HDC \cite{yuan2017hard} & cos & GoogLeNet & \\
& DREML \cite{xuan2018deep} & cos & ResNet-18 & \\
& HTL \cite{ge2018deep} & cos & GoogLeNet & \\
& D\&C \cite{sanakoyeu2019divide} & cos & ResNet-50 & \\
& ABE \cite{kim2018attention} & cos & GoogLeNet & \\
& EPSHN \cite{xuan2020improved} & cos & ResNet-50 & \\
& NormSoftmax \cite{zhai2018classification} & cos & ResNet-50 & \\
& Multi-Similarity \cite{wang2019multi} & cos & BN-Inception & \\
\cline{2-5}
& \multirow{2}{*}{Cross-Entropy} &  & \multirow{2}{*}{ResNet-50} & \\
& & cos & & \\
\bottomrule
\end{tabular}
}
\end{table}
 \textbf{Sampling:} To the best of our knowledge, all DML papers -- including \cite{zhai2018classification} -- use a form of pairwise sampling to ensure that, during training, each mini-batch contains a fixed number of classes and samples per class (\eg mini-batch size of 75 with 3 classes and 25 samples per class in \cite{zhai2018classification}). Deviating from that, we use the common random sampling among all samples (as in most classification training schemes) and set the mini-batch size to 128 in all experiments (contrary to \cite{wang2019multi} in which the authors use a mini-batch size of 80 for CUB, 1\,000 for SOP and did not report for Cars and In-Shop).

\textbf{Data Augmentation:} As is common in training deep learning models, data augmentation improves the final performances of the methods. 
For CUB, the images are first resized so that their smallest side has a length of 256 (\ie, keeping the aspect ratio) while for Cars, SOP and In-Shop, the images are resized to . Then a patch is extracted at a random location and size, and resized to . For CUB and Cars, we found that random jittering of the brightness, contrast and saturation slightly improves the results. All of the implementation details can be found in the publicly available code.





\textbf{Cross-entropy:} The focus of our experiments is to show that, with careful tuning, it is possible to obtain similar or better performance than most recent DML methods, while using only the cross-entropy loss. To train with the cross-entropy loss, we add a linear classification layer (with bias) on top of the feature extraction -- similar to many classification models -- which produces logits for all the classes present in the training set. Both the weights and biases of this classification layer are initialized to . We also add dropout with a probability of  before this classification layer. To further reduce overfitting, we use label smoothing for the target probabilities of the cross-entropy. We set the probability of the true class to  and the probabilities of the other classes to  with  in all our experiments.

\textbf{Optimizer:} In most DML papers, the hyper-parameters of the optimizer are the same for Cars, SOP and In-Shop whereas, for CUB, the methods typically use a smaller learning rate. In our experiments, we found that the best results were obtained by tuning the learning rate on a per dataset basis. In all experiments, the models are trained with SGD with Nesterov acceleration and a weight decay of , which is applied to convolution and fully-connected layers' weights (but not to biases) as in \cite{jia2018highly}. For CUB and Cars, the learning rate is set to 0.02 and 0.05 respectively, with 0 momentum. For both SOP and In-Shop, the learning rate is set to 0.003 with a momentum of 0.99.

\textbf{Batch normalization:} Following \cite{wang2019multi}, we freeze all the batch normalization layers in the feature extractor. For Cars, SOP and In-Shop, we found that adding batch normalization -- without scaling and bias -- on top of the feature extractor improves our final performance and reduces the gap between  and cosine distances when computing the recall. On CUB, however, we obtained the best recall without this batch normalization.



\subsection{Results}

Results for the experiments are reported in \autoref{table:results}. We also report the architecture used in the experiments as well as the distance used in the evaluation to compute the recall. \emph{} refers to the Euclidean distance on non normalized features while \emph{cos} refers to either the cosine distance or the Euclidean distance on -normalized features, both of which are equivalent.

On all datasets, we report state-of-the-art results except on Cars, where the only method achieving similar recall uses cross-entropy for training. 
We also notice that, contrary to common beliefs, using Euclidean distance can actually be competitive as it also achieves near state-of-the-art results on all four datasets. These results clearly highlight the potential of cross-entropy for metric learning, and confirm that this loss can achieve the same objective as pairwise losses.
 \section{Conclusion}

Throughout this paper, we revealed non-obvious relations between the cross-entropy loss, widely adopted in classification tasks, and pairwise losses commonly used in DML. These relations were drawn under two different perspectives. First, cross-entropy minimization was shown equivalent to an approximate bound-optimization of a pairwise loss, introduced as Pairwise Cross-Entropy (PCE), which appears similar in structure to already existing DML losses. Second, adopting a more general information theoretic view of DML, we showed that both pairwise losses and cross-entropy were, in essence, maximizing a common mutual information  between the embedded features and the labels. This connection becomes particularly apparent when writing mutual information in both its \textit{generative} and \textit{discriminative} views. Hence, we argue that most of the differences in performance observed in previous works come from the optimization process during training. Cross-entropy contains only unary terms, while traditional DML losses are based on pairwise-term optimization, which requires substantially more tuning (\eg mini-batch size, sampling strategy, pair weighting).
While we acknowledge that some losses have better properties than others regarding optimization, we empirically showed that the cross-entropy loss was also able to achieve state-of-the-art results when fairly tuned, highlighting the fact that most improvements have come from enhanced training schemes (\eg data augmentation, learning rate policies, batch normalization freeze) rather than the intrinsic properties of pairwise losses.
We strongly advocate that cross-entropy should be carefully tuned to be compared against as a baseline in future works. 
%
 
\clearpage
\bibliographystyle{splncs04}
\bibliography{biblio}

\clearpage
\appendix
\section{Proofs}

    \subsection{\autoref{prop:tightness_terms}}\label{appendix:proof_tightness_terms}
        \begin{proof}
            Throughout the following proofs, we will use the fact that classes are assumed to be balanced in order to consider , for any class , as a constant . We will also use the feature normalization assumption to connect cosine and Euclidean distances. On the unit-hypersphere, we will use that: .\\
            
            \subsubsection{Tightness terms:}
            Let us start by linking center loss to contrastive loss. For any specific class , let   denotes the hard mean. We can write:
            
            Summing over all classes , we get the desired equivalence. Note that, in the context of K-means clustering, where the setting is different\footnote{In clustering, the optimization is performed over assignment variables, as opposed to DML, where assignments are already known and optimization is carried out over the embedding.}, a technically similar result could be established \cite{tang2015kernel}, linking K-means to pairwise graph clusteirng objectives. \\
            
            \noindent
            Now we link contrastive loss to SNCA loss. For any class , we can write:
            
            where we used the convexity of  and Jenson's inequality. The proof can be finished by summing over all classes . \\
            
            Finally, we link MS loss \cite{wang2019multi} to contrastive loss:
            
            where we used the concavity of  and  Jenson's inequality.
            
            \subsubsection{Contrastive terms:} In this part, we first show that the contrastive terms  and  represent upper bounds on :
            
            where, again, we used Jenson's inequality in the second line above. The link between SNCA and contrastive loss can be established quite similarly:
            
            
            Now, similarly to the reasoning carried out in \autoref{subsec:contrastive_example}, we can write:
            
            Where the redundant tightness term is very similar to the tightness term in contrastive loss  treated in details in \autoref{subsec:contrastive_example}. As for the truly contrastive part of , it can also be related to the differential entropy estimator used in \cite{wang2011information}:
            
            In summary, we just proved that the contrastive parts of MS and SNCA losses are upper bounds on the contrastive term . The latter term is composed of a proxy for the entropy of features , as well 
            as a tightness sub-term.
        \end{proof}
    
    
    \subsection{\autoref{prop:cross_entropy_lower_bound}}\label{appendix:proof_cross_entropy_lower_bound}
        \begin{proof}
        First, let us show that . 
        Consider the usual softmax parametrization of point  belonging to class : , where . We can explicitly write the cross-entropy loss:
        
        Where we introduced . How to specifically set  will soon become clear. Let us now write the gradients of  and  in \autoref{eq:cross_entropy_explicit} with respect to :
        
        Notice that  is a convex function of , regardless of . As for , we set  such that  becomes a convex function of . Specifically, by setting:
        
        where  and  represents the  eigenvalue of , we make sure that the hessian of  is semi-definite positive. Therefore, we can look for the minima of  and .\\
        
        Setting gradients in \autoref{eq:gradients_1} and \autoref{eq:gradients_2} to 0, we obtain that for all , the optimal  for  is, up to a multiplicative constant, the hard mean of features from class : , while the optimal  for  is, up to a multiplicative constant, the soft mean of features: . Therefore, we can 
        write:
        
        And 
        
        Putting it all together, we can obtain the desired result:
        
        where  represents the soft mean of class k.  \\
        
        Let us now justify that minimizing cross-entropy can be seen as an approximate bound optimization on . At every iteration  of the training, cross-entropy represents an upper bound on Pairwise Cross-entropy.
        
        
        When optimizing w.r.t , the bound almost becomes tight. The approximation comes from the fact that  and  are quite dissimilar in early training, but become very similar as training progresses and the model's softmax probabilities align with the labels. Therefore, using the notation:
        
        We can write:
        
        Then, minimizing  and  w.r.t  becomes approximately equivalent.
        \end{proof}
    
    \subsection{\autoref{prop:cross_ent_and_mi}}\label{appendix:cross_ent_and_mi}
        \begin{proof}
            Using the discriminative view of MI, we can write:
            
            The entropy of labels  is a constant and, therefore, can be ignored. From this view of MI, maximization of  can only be achieved through a minimization of , which depends on our embeddings . We can relate this term to our cross-entropy loss using the following relation:
            
            Therefore, while minimizing cross-entropy, we are implicitly both minimizing  as well as .   In fact, following \autoref{eq:penalty}, optimization could naturally be decoupled in 2 steps, in a \textit{Maximize-Minimize} fashion. One step would consist in fixing the encoder's weights  and only minimizing \autoref{eq:penalty} w.r.t to the classifier's weights . At this step,  would be fixed while  would be adjusted to minimize . Ideally, the KL term would vanish at the end of this step. In the following step, we would minimize \autoref{eq:penalty} w.r.t to the encoder's weights , while keeping the classifier fixed.
        \end{proof}
        
        
\section{Preliminary results with SPCE}\label{appendix:spce_experiment}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.45]{figures/losses.pdf}
    \caption{Evolution of the cross-entropy loss (CE) and the simplified pairwise cross-entropy (SPCE) during training on MNIST, as well as the validation accuracy for both losses.}
    \label{fig:CE_vs_PCE}
\end{figure}


In \autoref{fig:CE_vs_PCE}, we track the evolution of both loss functions and validation accuracy when training with  and  on MNIST dataset. We use a small CNN composed of four convolutional layers. The optimizer used is Adam. Batch size is set to 128, learning rate to  with cosine annealing, weight decay to  and feature dimension to . \autoref{fig:CE_vs_PCE} supports the theoretical links that were drawn between Cross-Entropy and its simplied pairwise version SPCE. Particularly, this preliminary result demonstrates that SPCE is indeed employable as a loss, and exhibits a very similar behavior to the original cross-entropy. Both losses remain very close to each other throughout the training, and so remain the validation accuracies. 



\section{Analysis of ranking losses for Deep Metric Learning}

Some recent works \cite{cakir2019deep,wang2019ranked,rolinek2020optimizing} tackle the problem of deep metric learning using a rank-based approach. In other words, given a point in feature space , the pairwise losses studied throughout this work try to impose manual margins , so that the distance between  and any negative point  is at least . Rank-based losses rather encourage that all points are well ranked, distance-wise, such that  for any positive and negative points  and . We show that our tightness/contrastive analysis also holds for such ranking losses. In particular, we analyse the loss proposed in \cite{cakir2019deep}. For any given query embedded point , let us call  the random variable associated to the distance between  and all other points in the embedded space, defined over all possible (discretized) distances . Furthermore, let us call  the binary random variable that describes the relation to the current query point ( and  describe respectively a positive and negative relationship to ). The loss maximized in \cite{cakir2019deep} reads:

Taking the logarithm, and using Jensen's inequality, we can lower bound this loss:

To intuitively understand what those two terms are doing, let us imagine we approximate each of the expectations with a single point Monte-Carlo approximation. In other words, we sample a positive point , take its associated distance to , which we call , then we approximate the tightness term as:

Maximizing  has a clear interpretation: it encourages all positive points to lie inside the hypersphere of radius  around query point . Similarly:

Maximizing  also has a clear interpretation: it encourages all points (both positive and negative ones) to lie outside the hypersphere of radius  around query point . Now, \autoref{eq:log_fastap} is nothing more than an expectation over all positive distance  one could sample. Therefore, such loss can be analyzed through the same lens as other DML losses, i.e., one tightness term that encourages all points from the same class as  to lie close to it in the embedded space, and one contrastive term that oppositely refrains all points from approaching  closer than its current positive points.


\section{On the limitations of cross-entropy}

While we demonstrated that the cross-entropy loss could be competitive in comparison to pairwise losses, while being easier to optimize, there still exist scenarios for which a straightforward use of the CE loss becomes prohibitive. Hereafter, we describe two such scenarios. \\

\textbf{Case of relative labels: } The current setting assumes that absolute labels are given for each sample, \ie, each sample  belongs to a single absolute class . However, DML can be applied to more general problems where the absolute class labels are not available. Instead, one has access to relative labels that only describe the relationships between points (\eg, a pair is similar or dissimilar). From these relative labels, one could still define absolute classes as sets of samples inside which every pair has a positive relationship. Note that with this definition, each sample may belong to multiple classes simultaneously, which makes the use of standard cross-entropy difficult.
However, with such re-formulation, our Simplified Pairwise Cross-Entropy (SPCE), which we hereby remind:

can handle such problems, just like any other pairwise loss. \\

\textbf{Case of large number of classes: } In some problems, the total number of classes K can grow to several millions. In such cases, even simply storing the weight matrix  of the final classifier required by cross-entropy becomes prohibitive. Note that there exist heuristics to handle such problems with standard cross-entropy, such as sampling subsets of classes and solving those sub-problems instead, as was done in \cite{zhai2018classification}. However, we would be introducing new training heuristics (e.g., class sampling), which defeats the initial objective of using the cross-entropy loss. Again, the SPCE loss underlying the unary cross-entropy could again handle such cases, similarly to other pairwise losses, given that it doesn't require storing such weight matrix. 
\end{document}