\documentclass{article}






\usepackage[preprint,nonatbib]{neurips_2020}
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage[pdftex]{graphicx}
\usepackage{wrapfig}
\usepackage{subfigure}
\usepackage{amsmath,amssymb}
\usepackage{multirow}
\usepackage{array}
\usepackage{comment}
\usepackage{color}
\usepackage{bm}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\newcommand{\PreserveBackslash}[1]{\let\temp=\\#1\let\\=\temp}
\newcolumntype{C}[1]{>{\PreserveBackslash\centering}p{#1}}
\newcolumntype{R}[1]{>{\PreserveBackslash\raggedleft}p{#1}}
\newcolumntype{L}[1]{>{\PreserveBackslash\raggedright}p{#1}}



\title{Adaptive Graph Convolutional Recurrent Network for Traffic Forecasting}



\author{
  Lei Bai \\
  UNSW, Sydney\\
  \texttt{baisanshi@gmail.com} \\
  \And
  Lina Yao \\
  UNSW, Sydney \\
  \texttt{lina.yao@unsw.edu.au} \\
  \And
  Can Li \\
  UNSW, Sydney \\
  \texttt{can.li4@student.unsw.edu.au} \\
  \And
  Xianzhi Wang \\
  University of Technology Sydney \\
  \texttt{xianzhi.wang@uts.edu.au} \\
  \And
  Can Wang \\
  Griffith University \\
  \texttt{can.wang@griffith.edu.au} \\
}

\begin{document}

\maketitle

\begin{abstract}
Modeling complex spatial and temporal correlations in the correlated time series data is indispensable for understanding the traffic dynamics and predicting the future status of an evolving traffic system.
Recent works focus on designing complicated graph neural network architectures to capture shared patterns with the help of pre-defined graphs. In this paper, we argue that learning node-specific patterns is essential for traffic forecasting while the pre-defined graph is avoidable.
To this end, we propose two adaptive modules for enhancing Graph Convolutional Network (GCN) with new capabilities:
1) a Node Adaptive Parameter Learning (NAPL) module to capture node-specific patterns; 2) a Data Adaptive Graph Generation (DAGG) module to infer the inter-dependencies among different traffic series automatically.
We further propose an Adaptive Graph Convolutional Recurrent Network (AGCRN) to capture fine-grained spatial and temporal correlations in traffic series automatically based on the two modules and recurrent networks.
Our experiments \footnote{Code available at: https://github.com/LeiBAI/AGCRN} on two real-world traffic datasets show AGCRN outperforms state-of-the-art by a significant margin without pre-defined graphs about spatial connections. 



\end{abstract}

\section{Introduction}
The fast urbanization introduces growing populations in cities and presents significant mobility and sustainability challenges. Among those challenges, Intelligent Transportation Systems (ITS) has become an active research area \cite{streets-nips2019}, given its potential to promote system efficiency and decision-making. As an essential step towards the ITS, traffic forecasting aims at predicting the future status (e.g., traffic flow and speed, and passenger demand) of urban traffic systems. It plays a vital role in traffic scheduling and management and has attracted tremendous attention from the machine learning research community in recent years \cite{DCRNN,lei-ijcai2019,stgcn,graphwavenet,astgcn}. 

Traffic forecasting is challenging due to the complex intra-dependencies (i.e., temporal correlations within one traffic series) and inter-dependencies (i.e., spatial correlations among multitudinous correlated traffic series) \cite{lei-ijcai2019} generated from different sources, e.g., different loop detectors/intersections for traffic flow \& traffic speed prediction, and various stations/regions for passenger demand prediction. Traditional methods simply deploy time series models, e.g., Auto-Regressive Integrated Moving Average (ARIMA) and Vector Auto-Regression (VAR), for traffic forecasting. They cannot capture the nonlinear correlations nor intricate spatial-temporal patterns among large scale traffic data. Recently, researchers shift to deep-learning-based methods and focus on designing new neural network architectures to capture prominent spatial-temporal patterns shared by all traffic series. 
They typically model temporal dependencies with recurrent neural networks \cite{lei-pakdd2019,xianfeng-aaai2020,dmvst,deepar-2019} (e.g., Long-Short Term Memory and Gated Recurrent Unit) or temporal convolution modules \cite{lei-ijcai2019,stgcn}. Regarding spatial correlations, they commonly use GCN-based methods \cite{DCRNN,stgcn,lei-ijcai2019,astgcn,stsgcn-aaai2020,graphwavenet,gman-aaai2020} to model unstructured traffic series and their inter-dependencies.

\begin{wrapfigure}{R}{0.5\linewidth}
\includegraphics[width=7.0cm]{./figs/flow.png}
\caption{Examples of traffic flow with diverse patterns. The traffic flow of road 3 is steady in the day time. As a contrast, the traffic flows of road 1, 2 and 4 have obvious evening peak, morning peak, and both peaks, respectively.}
\label{flow}
\end{wrapfigure}

While recent deep-learning-based methods achieve promising results, they are biased to the prominent and shared patterns among all traffic series---the shared parameter space makes current methods inferior in capturing fine-grained data-source specific patterns accurately. In fact, traffic series exhibit diversified patterns (as shown in Fig. \ref{flow}), they may appear similar, dissimilar, and even contradictory owning to the distinct attributes across a variety of data sources \cite{lei-pakdd2019,zheyi-kdd2019}.
Moreover, existing GCN-based methods require pre-defining an inter-connection graph by similarity or distance measures \cite{multigraph-aaai2019} to capture the spatial correlations. That further requires substantial domain knowledge and is sensitive to the graph quality. The graphs generated in this manner are normally intuitive, incomplete, and not directly specific to the prediction tasks; they may contain biases and not adaptable to domains without appropriate knowledge. 

Instead of designing more complicated network architectures, we propose two concise yet effective mechanisms by revising the basic building block of current methods (i.e., GCN) to solve the above problems separately. Specifically, we propose to enhance GCN with two adaptive modules for traffic forecasting tasks: 
1) a Node Adaptive Parameter Learning (NAPL) module to learn node-specific patterns for each traffic series---NAPL factorizes the parameters in traditional GCN and generates node-specific parameters from a weights pool and bias pool shared by all nodes according to the node embedding; 
2) a Data Adaptive Graph Generation (DAGG) module to infer the node embedding (attributes) from data and to generate the graph during training.
NAPL and DAGG are independent and can be adapted to existing GCN-based traffic forecasting models both separately and jointly.
All the parameters in the modules can be easily learned in an end-to-end manner.
Furthermore, we combine NAPL and DAGG with recurrent networks and propose a unified traffic forecasting model - Adaptive Graph Convolutional Recurrent Network (AGCRN).
AGCRN can capture fine-grained node-specific spatial and temporal correlations in the traffic series and unify the nodes embeddings in the revised GCNs with the embedding in DAGG. As such, training AGCRN can result in a meaningful node representation vector for each traffic series source (e.g., roads for traffic speed/flow, stations/regions for passenger demand). The learned node representation contains valuable information about the road/region and can be potentially applied to other tasks~\cite{embedding-cikm2019}. 

We evaluate AGCRN on two real-world datasets for the multi-step traffic prediction task and compare it with several representative traffic forecasting models. The experimental results show that AGCRN outperforms state-of-the-art with a significant margin. We also conduct ablation studies and demonstrate the effectiveness of both NAPL and DAGG. 



\section{Related Work}
\textbf{Correlated time series prediction} Traffic forecasting belongs to correlated time series analysis (or multivariate time series analysis) and has been studied for decades. In recent years, deep learning has dominated the correlated time series prediction due to its superior ability in modeling complex functions and learning correlations from data automatically. A majority of such studies \cite{can2020cikm,vrnn-2015nips,deepar-2019,xianfeng-aaai2020,imputation-nips2018,deepstate-2018nips,brits-nips2018,lstnet} rely on LSTM or GRU to model the temporal dynamics in the time series data.
Some efforts employ temporal convolutional networks \cite{convtimeseries-2017,convtimeseries-2018,tconv-nips2019} to enable the model process very long sequence with fewer time. However, these studies do not explicitly model the inter-dependencies among different time series.
A very recent work \cite{sttransformer-2020} uses transformers for correlated time series prediction. Such work normally requires massive training samples due to tremendous trainable parameters \cite{transformer-timeseries2019}.

\textbf{GCN based Traffic forecasting} Different with general correlated time series prediction, traffic forecasting researches also pay more attention to spatial correlations among the traffic series from different sources (spaces/regions/sensors) except for the temporal correlations. A part of these studies \cite{deepst,resstnet,lei-pakdd2019,dmvst} utilize CNN to capture spatial correlations among near regions based on the assumption that traffic series are generated from grid-partitioned cities \cite{resstnet}, which does not always hold. To develop more general and widely-used traffic forecasting methods, researchers are shifting to GCN-based models in recent years. These efforts \cite{stgcn,lei-ijcai2019,lei2019cikm,graphwavenet,astgcn,stsgcn-aaai2020,multigraph-aaai2019,gcn-traffic-survey,traffic-survey} formulate the traffic forecasting problem on graph and utilize the spectral GCN developed in \cite{gcn-defferrard,kipf-gcn} for capturing the prominent spatial interactions among different traffic series. DCRNN \cite{DCRNN} re-formulates the spatial dependency of traffic as a diffusion process and extends the previous GCN \cite{gcn-defferrard,kipf-gcn} to a directed graph. 
Following DCRNN, Graph Wavenet \cite{graphwavenet} combines GCN with dilated causal convolution networks for saving computation cost in handling long sequence and propose a self-adaptive adaptive adjacency matrix as a complement for the pre-defined adjacent matrix to capture spatial correlations.
More recent works such as ASTGCN \cite{astgcn}, STSGCN \cite{stsgcn-aaai2020} and GMAN \cite{gman-aaai2020} further add more complicated spatial and temporal attention mechanisms with GCN to capture the dynamic spatial and temporal correlations. However, these methods can only capture shared patterns among all traffic series and still rely on the pre-defined spatial connection graph. 

\textbf{Graph Convolutional Networks} GCN \cite{gcn-defferrard,kipf-gcn} is a special kind of CNN generalized for graph-structured data, which is widely used in node classification, link prediction, and graph classification \cite{wu2020gcnsurvey}. Most of these works focus on graph representation, which learns node embedding by integrating the features from node's local neighbours based on the given graph structure. To manipulate neighbours' information more accurately, GAT \cite{velivckovic2017gat} learns to weight the information from different neighbours with attention scores learned by multi-head self-attention mechanism. DIFFPOOL \cite{ying2018diffpool} enhances GCN with node clustering to generate hierarchical graph representations. Different from these works dealing with static features, our work deals with dynamically evolving streams and operates on both spatial and temporal dimensions without the given graph structure.


\section{Methodology}


\subsection{Problem Definition}
We target on the multi-step traffic forecasting problem. Consider  multitudinous traffic series that contains  correlated univariate time series represented as , where  is the recording of  sources at time step , our target is to predict the future values of the correlated traffic series based on the observed historical values.  Following in the practice in the time series prediction, we formulate the problem as finding a function  to forecast the next  steps data based on the past  steps historical data:

where  denotes all the learnable parameters in the model. In order to accurately manipulate the spatial correlations between different traffic series, the problem is  further formulated on graph , where  is a set of nodes represent the sources of traffic series and ,  is a set of edges, and  is the adjacent matrix of the graph representing the proximity between nodes or traffic series (e.g., a function of traffic network distance or traffic series similarity). Thus, the problem is modified as:


\subsection{Node Adaptive Parameter Learning}

Most recent work in traffic forecasting deploys GCN to capture the spatial correlations among traffic series and follows the calculations proposed in the spectral domain \cite{gcn-defferrard,kipf-gcn}. According to \cite{kipf-gcn}, the graph convolution operation can be well-approximated by  order Chebyshev polynomial expansion and generalized to high-dimensional GCN as:

where  is the adjacent matrix of the graph,  is the degree matrix,  and  are input and output of the GCN layer,  and  denote the learnable weights and bias, separately.
From the view of one node (e.g., node ), the GCN operation can be regarded as transforming the features of node  to  with the shared  and  among all nodes. While sharing parameters may be useful to learn the most prominent patterns among all nodes in many problems and can significantly reduce the parameter numbers, we find its sub-optimal for traffic forecasting problems. 
Except for the close spatial correlations between close related traffic series, there also exist diverse patterns among different traffic series due to the dynamic propriety of time series data and various factors of the node that could influence traffic.
On the one hand, the traffic streams from two adjacent nodes may also present dissimilar patterns at some particular period because of their specific attributes (e.g., PoI, weather). On the other hand, the traffic series from two disjoint nodes may even show reverse patterns.
As a result, only capturing shared patterns among all nodes is not enough for accurate traffic forecasting, and it is essential to maintain a unique parameter space for each node to learn node-specific patterns. 

However, assigning parameters for each node will result in , which is too huge to optimize and would lead to over-fitting problem, especially when  is big. To solve the issue, we propose to enhance traditional GCN with a Node Adaptive Parameter Learning module, which draws insights from the matrix factorization. Instead of directly learning , NAPL learns two smaller parameter matrix: 1) a node-embedding matrix , where  is the embedding dimension, and ; 2) a weight pool . Then,  can be generated by . From the view of one node (e.g., node ), this process extracts parameters  for  from a large shared weight pool  according to the node embedding , which can be interpreted as learning node specific patterns from a set of candidate patterns discovered from all traffic series. The same operation can also be used for . Finally, the NAPL enhanced GCN (i.e., NAPL-GCN) can be formulaed as:


\subsection{Data Adaptive Graph Generation}
Another problem lies in existing GCN-based traffic forecasting models, which require a pre-defined adjacent matrix  for the graph convolution operation. 
Existing work mainly utilizes distance function or similarity metrics to calculate the graph in advance.
There are mainly two approaches for defining : 1) distance function, which defines the graph according to the geographic distance among different nodes\cite{DCRNN,stgcn}; 2) similarity function, which defines the node proximity by measuring the similarity of the node attributes (e.g., PoI information) \cite{lei-pakdd2019,multigraph-aaai2019} or traffic series itself \cite{lei-ijcai2019}. However, these approaches are quite intuitive. The pre-defined graph cannot contain complete information about spatial dependency and is not directly related to prediction tasks, which may result in considerable biases. Besides, these approaches cannot be adapted to other domains without appropriate knowledge, making existing GCN-based models ineffective.

To solve the issue, we propose a Data Adaptive Graph Generation (DAGG) module to infer the hidden inter-dependencies from data automatically. 
The DAGG module first randomly initialize a learnable node embedding dictionaries  for all nodes, where each row of  represents the embedding of a node and  denotes the dimension of node embedding. Then, similar as defining the graph by nodes similarity, we can infer the spatial dependencies between each pair of nodes by multiplying  and :


where  function is used to normalize the adaptive matrix.
Here, instead of generating  and calculating a Laplacian matrix, we directly generate  to avoid unnecessary and repeated calculations in the iterative training process. During training,  will be updated automatically to learn the hidden dependencies among different traffic series and get the adaptive matrix for graph convolutions.
Comparing with the self-adaptive adjacent matrix in \cite{graphwavenet}, DAGG module is simpler and the learned  has better interpret-ability.
Finally, the DAGG enhanced GCN can be formulated as:

When dealing with extremely large graphs (i.e.,  is huge), DAGG may require heavy computation cost. Graph partition and sub-graph training methods \cite{gman-aaai2020,mallick2019graph} could be applied to address the problem.


\subsection{Adaptive Graph Convolutional Recurrent Network}
Except for the spatial correlations, traffic forecasting also involves complex temporal correlations. In this part, we introduce an Adaptive Graph Convolutional Recurrent Network (AGCRN), which integrates NAPL-GCN, DAGG, and Gated Recurrent Units (GRU) to capture both node-specific spatial and temporal correlations in traffic series. AGCRN replaces the MLP layers in GRU with our NAPL-GCN to learn node-specific patterns. Besides, it discoveries spatial dependencies automatically with the DAGG module. 
Formally: 


where  and  are input and output at time step ,  denotes the concate operation,  and  are reset gate and update gate, respectively. , , , , , , and  are learnable parameters in AGCRN. Similar to GRU, all the parameters in AGCRN can be trained end-to-end with back-propagation through time.
As can be observed from the equation, AGCRN unifies all the embedding matrix to be  instead of learning separate node embedding matrix in different NAPL-GCN layers and DAGG. This gives a strong regularizer to ensure the nodes embedding consistent among all GCN blocks and gives our model better interpretability. 

\subsection{Multi-step traffic prediction}
To achieve multi-step traffic prediction, we stack several AGCRN layers as an encoder to capture the node-specific spatial-temporal patterns and represents the input (i.e., historical data) as .  Then, we can directly obtain the traffic prediction for the next  steps of all nodes by applying a linear transformation to project the representation from  to . Here, we do not generate the output in the sequential manner as it would increase the time consumption significantly. 

We choose L1 loss as our training objective and optimize the loss for multi-step prediction together. Thus, the loss function of AGCRN for multi-step traffic
prediction can be formulated as:

where  represents all the learnable parameters in the network,  is the ground truth, and  is the prediction of all nodes at time step . The problem can be solved via back-propagation and Adam optimizer.



\section{Experiments}
\subsection{Datasets}
To evaluate the performance of our work, we conduct experiments on two public real-world traffic datasets: PeMSD4 and PeMSD8 \cite{astgcn,stsgcn-aaai2020}. PeMS means Caltrans Performance Measure System (PeMS) \cite{pems}, which measures the highway traffic of California in real-time every 30 seconds.

\textbf{PeMSD4}: The PeMSD4 dataset refers to the traffic flow data in the San Francisco Bay Area. There are 307 loop detectors selected within the period from 1/Jan/2018 to 28/Feb/2018. 

\textbf{PeMSD8}: The PeMSD8 dataset contains traffic flow information collected from 170 loop detectors on the San Bernardino area from 1/Jul/2016 - 31/Aug/2016. 

\textbf{Data Preprocess:} The missing values in the datasets are filled by linear interpolation. Then, both datasets are aggregated into 5-minute windows, resulting in 288 data points per day. Besides, we normalize the dataset by standard normalization method to make the training process more stable. For multi-step traffic forecasting, we use one-hour historical data to predict the next hour's data, i.e., we organize 12 steps' historical data as input and the following 12 steps data as output. We split the datasets into training sets, validation sets, and test sets according to the chronological order. The split ratio is 6:2:2 for both datasets.
Although our method does not need a pre-defined graph, we use the pre-defined graph for our baselines. Detailed dataset statistics are provided in the appendix.

\subsection{Experimental Settings}
 
To evaluate the overall performance of our work, we compare AGCRN with widely used baselines and state-of-the-art models, including 1) Historical Average (HA): which models the traffic as a seasonal process and uses the average of previous seasons (e.g., the same time slot of previous days) as the prediction; 2) Vector Auto-Regression (VAR) \cite{var}: a time series model that captures spatial correlations among all traffic series; 3) GRU-ED: an GRU-based baseline and utilize the encoder-decoder framework \cite{encoder-decoder} for multi-step time series prediction; 
4) DSANet \cite{dsanet}: a correlated time series prediction model using CNN networks for capturing temporal correlations with one time-series and self-attention mechanism for spatial correlations;
5) DCRNN \cite{DCRNN}: diffusion convolution recurrent neural network, which formulates the graph convolution with the diffusion process and combines GCN with recurrent models in an
encoder-decoder manner for multi-step prediction; 
6) STGCN \cite{stgcn}: a spatio-temporal graph convolutional network that deploys GCN and temporal convolution to capture spatial and temporal correlations, respectively; 
7) ASTGCN \cite{astgcn}: attention-based spatio-temporal graph convolutional network, which further integrates spatial and temporal attention mechanisms to STGCN for capturing dynamic spatial and temporal patterns. We take its recent components to ensure the fairness of comparison;
8) STSGCN \cite{stsgcn-aaai2020}: Spatial-Temporal Synchronous Graph Convolutional Network that captures spatial-temporal correlations by stacking multiple localized GCN layers with adjacent matrix over the time axis. 

All the deep-learning-based models, including our AGCRN, are implemented in Python with Pytorch 1.3.1 and executed on a server with one NVIDIA Titan X GPU card. We optimize all the models by Adam optimizer for a maximum of 100 epochs and use an early stop strategy with the patience of 15. The best parameters for all deep learning models are chosen through a carefully parameter-tuning process on the validation set.

\subsection{Overall Comparison}
We deploy three widely used metrics - Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and Mean Absolute Percentage Error (MAPE) to measure the performance of predictive models. Table \ref{overall-comparison} presents the overall prediction performances, which are the averaged MAE, RMSE and MAPE over 12 prediction horizons, of our AGCRN and eight representative comparison methods.  We can observe that: 1) GCN-based methods outperform baselines and self-attention-based DSANet, demonstrating the importance of modeling spatial correlations explicitly and the effectiveness of GCN in traffic forecasting; 2) our method further improves GCN-based methods with a significant margin. AGCRN brings more than 5\% relative improvements to the existing best results in MAE and MAPE for both PeMSD4 and PeMSD8 dataset. Fig. \ref{pemsd4} further shows the prediction performance at each horizon in the PeMSD4 dataset. AGCRN balances short-term and long-term prediction well and achieves the best performance for almost all horizons (except for the first step). Besides, the performance of AGCRN deteriorate much slower than other GCN-based models (see appendix for similar results in the PeMSD8 dataset). 

Overall, the results demonstrate that AGCRN can accurately capture the spatial and temporal correlations in the correlated traffic series and achieve promising predictions.



\begin{table}[]
\centering
\caption{Overall prediction performance of different methods on the PeMSD4 dataset and PeMSD8 dataset, results with * are reported performance in the paper used the same datasets and results with \_\_ are the best performance achieved by baselines. (smaller value means better performance)}
\label{overall-comparison}
\begin{tabular}{@{}cccccccc@{}}
\toprule
\multirow{2}{*}{Model} & Dataset & \multicolumn{3}{c}{PeMSD4}                & \multicolumn{3}{c}{PeMSD8}               \\ \cmidrule(l){2-8} 
                       & Metrics & MAE         & RMSE        & MAPE          & MAE         & RMSE        & MAPE          \\ \midrule
\multicolumn{2}{c}{HA}           & 38.03       & 59.24       & 27.88\%       & 34.86       & 52.04       & 24.07\%       \\ \midrule
\multicolumn{2}{c}{VAR}          & 24.54       & 38.61       & 17.24\%       & 19.19       & 29.81       & 13.10\%       \\ \midrule
\multicolumn{2}{c}{GRU-ED}          & 23.68       & 39.27       & 16.44\%       & 22.00       & 36.23       & 13.33\%       \\ \midrule
\multicolumn{2}{c}{DSANet \cite{dsanet}}       & 22.79       & 35.77       & 16.03\%       & 17.14       & 26.96       & 11.32\%       \\ \midrule
\multicolumn{2}{c}{DCRNN \cite{DCRNN}}        & 21.22       & {\ul 33.44} & 14.17\%       & {\ul 16.82} & {\ul 26.36} & {\ul 10.92\%} \\ \midrule
\multicolumn{2}{c}{STGCN \cite{stgcn}}        & {\ul 21.16} & 34.89       & {\ul 13.83\%} & 17.50       & 27.09       & 11.29\%       \\ \midrule
\multicolumn{2}{c}{ASTGCN \cite{astgcn}}       & 22.93       & 35.22       & 16.56\%       & 18.25       & 28.06       & 11.64\%       \\ \midrule
\multicolumn{2}{c}{STSGCN \cite{stsgcn-aaai2020}}       & 21.19*       & 33.65*       & 13.90\%*       & 17.13*       & 26.86*       & 10.96\%*       \\ \midrule
\multicolumn{2}{c}{AGCRN (ours)}        & 19.83       & 32.26       & 12.97\%       & 15.95       & 25.22       & 10.09\%       \\ \midrule
\multicolumn{2}{c}{Improvements} & +6.29\%     & +3.52\%     & +6.22\%       & +5.17\%     & +4.32\%     & +7.60\%       \\ \bottomrule
\end{tabular}
\end{table}


\begin{figure}[h]
\centering
\subfigure[MAE]{
\includegraphics[width=.32\linewidth]{figs/pems_mae.png}}
\subfigure[RMSE]{
\includegraphics[width=.32\linewidth]{figs/pems_rmse.png}}
\subfigure[MAPE]{
\includegraphics[width=.32\linewidth]{figs/pems_mape.png}}
\caption{Prediction performance comparison at each horizon on the PeMSD4 dataset.}
\label{pemsd4} 
\end{figure}

\subsection{Ablation Study}
To better evaluate the performance of NAPL and DAGG, we conduct a comprehensive ablation study. The baseline for our ablation study is GCGRU, which integrates traditional GCN with GRU to capture spatial and temporal correlations. We construct NAPL-GCGRU by replacing traditional GCN with our NAPL-GCN and DAGG-GCGRU by replacing the pre-defined graph with the DAGG module. AGCCRN-I is the variant of our AGCRN, which does not unify the node embeddings but employs an independent node embedding matrix among different NAPL-GCN layers and DAGG. The experiments on the PeMSD4 dataset are illustrated in Fig. \ref{ablation}. We can observe that: 1) NAPL-GCGRU generally outperforms GCGRU and AGCRN-I outperforms DAGG-GCGRU, demonstrating the necessity of capturing node-specific patterns. Moreover, NAPL mainly enhances the long-term (e.g., 30Min and 60 Min) prediction but slightly harms the short-term (e.g., 5Min and 15 Min) prediction. We conjecture the reason is that long-term prediction lacks enough useful information from historical observations and thus benefits from the specific node embedding learned by the NAPL module to deduce future patters. At the same time, short-term prediction can obtain enough information from historical observations.
2) DAGG-GCGRU improves GCGRU, and AGCRN-I beats NAPL-GCGRU. Both demonstrate the superiority of DAGG in inferring spatial correlations. The results also indicate that GCN-based methods can potentially be applied to more general correlated time series forecasting tasks with the help of our DAGG module, and pre-defining an adjacent matrix is not necessary; 3) AGCRN achieves the best performance, demonstrating that we can share the node embedding among all the modules and learn a unified node embedding for each node from the data.

Overall, our NAPL and DAGG modules can be deployed either separately and jointly, and they consistently boost the prediction performance.

\begin{figure}[h]
    \begin{minipage}[t]{0.65\linewidth}
    \vspace{0pt}
    \centering
    \subfigure[MAE]{
    \includegraphics[width=.48\linewidth]{figs/ablation_mae.png}}
    \subfigure[MAPE]{
    \includegraphics[width=.48\linewidth]{figs/ablation_mape.png}}
    \caption{Ablation study on the PeMSD4 dataset.}
    \label{ablation} 
    \end{minipage}
    \begin{minipage}[t]{0.34\linewidth}
    \vspace{6pt}
    \centering
    \includegraphics[width=0.99\linewidth]{figs/dimension.png}
    \vspace{-0.5cm}
    \caption{Influence of the embedding dimension.}
    \label{dimension}
    \end{minipage}
\end{figure}

\subsection{Model Analysis} \label{model_analysis}
\textbf{Graph Generation} To further investigate DAGG, we compare it with two variants: 1) DAGG-r, which removes the identity matrix in Eq. \ref{dagg}; 2) DAGG-2 which mimics the second-order Chebyshev polynomial expansion in GCN \cite{stgcn,kipf-gcn} with our learned . The backbone network is AGCRN-I, which does not share the embedding matrix among NAPL-GCN and DAGG to avoid the constraints from the NAPL module. As shown in Table~\ref{graph-generation} (where DAGG-1 follows Eq. \ref{dagg}), removing the identity matrix from DAGG significantly harms the prediction performance, which presents the importance of highlighting the self-information manually in prediction. Besides, DAGG-2 achieves similar performance with DAGG-1, which is consistent with the existing works \cite{kipf-gcn,stgcn,DCRNN} using pre-defined graphs. The results reveal that the generated graph Laplacian matrix  shares similar property as the pre-defined graph in Chebyshev polynomial expansion.


\begin{table}[h]
\centering
\caption{Analysis of graph generation process on the PeMSD4 dataset. }
\label{graph-generation}
\begin{tabular}{@{}cccccccccc@{}}
\toprule
\multirow{2}{*}{Model} & \multicolumn{3}{c}{15 Min} & \multicolumn{3}{c}{60 Min} & \multicolumn{3}{c}{Average} \\ \cmidrule(l){2-10} 
                       & MAE    & RMSE   & MAPE     & MAE    & RMSE   & MAPE     & MAE     & RMSE   & MAPE     \\ \midrule
DAGG-r                 & 21.85  & 35.03  & 14.96\%  & 26.54  & 41.07  & 17.91\%  & 23.35   & 37.07  & 15.82\%  \\ \midrule
DAGG-1                 & 19.15  & 30.65  & 13.15\%  & 21.98  & 34.91  & 14.82\%  & 20.18   & 32.30  & 13.70\%  \\ \midrule
DAGG-2                 & 19.26  & 31.20  & 13.06\%  & 21.58  & 34.73  & 14.49\%  & 20.11   & 32.56  & 13.58\%  \\ \bottomrule
\end{tabular}
\end{table}

\textbf{Embedding Dimension} One key parameter in AGCRN is the dimensions of the node embedding, which not only influences the quality of the learned graph but also decides the parameter diversity in NAPL-GCN layers. Fig. \ref{dimension} shows the effects of different embedding dimensions to AGCRN on the PeMSD4 dataset. AGCRN obtains relatively good performance for all the tested embedding dimensions, which shows the robustness of our methods. Besides, AGCRN achieves the best performance when the embedding dimension is set to 10. Both an excessively small and large node embedding dimension will lead to weaker performance. On the one hand, node embedding with a larger dimension can contain more information and thus help our DAGG module to deduce more accurate spatial correlations. On the other hand, a larger node embedding dimension will significantly increase the parameter numbers in the NAPL module, making the model harder to optimize and causing over-fitting. Overall, it would be a  good practice for AGCRN to find a suitable node embedding dimension and balance the model's performance and complexity.

\textbf{Computation Cost}
To evaluate the computation cost, we compare the parameter numbers and training time of AGCRN with DCRNN, STGCN, and ASTGCN on the PeMSD4 dataset in Table~\ref{cost}. When he node embedding dimension is set to 10, AGCRN has five times more parameters than the DCRNN model as a sacrifice for learning node-specific patterns. In terms of the training time, AGCRN runs slightly faster than DCRNN as we generate all predictions directly instead of the iterative manner in DCRNN. STGCN is the fastest thanks to the temporal convolution structure. However, it will require more parameters and training time to add spatial and temporal attention mechanisms to STGCN for learning more accurate spatial-temporal patterns (e.g., ASTGCN). Considering the significant performance improvement (as shown in Table~\ref{overall-comparison}), the computation cost of AGCRN is moderate. 

\begin{table}[h]
\caption{The computation cost on the PeMSD4 dataset, "dim" means the dimension of \bm{}.}
\label{cost}
\centering
\begin{tabular}{@{}ccc@{}}
\toprule
Model          & \# Parameters & Training Time (epoch) \\ \midrule
DCRNN          & 149057        & 36.39 s               \\ \midrule
STGCN          & 211596        & 16.36 s               \\ \midrule
ASTGCN         & 450031        & 49.47 s               \\ \midrule
AGCRN (dim=2)  & 150386        & 33.88 s               \\ \midrule
AGCRN (dim=10) & 748810        & 35.56 s               \\ \bottomrule
\end{tabular}
\end{table}

\section{Discussion}
Multivariate/correlated time series prediction is a fundamental task for many applications, such as epidemic transmission forecasting \cite{epideep-2019kdd}, meteorology (e.g., air quality, rainfall) prediction \cite{airquality}, stock forecasting \cite{stock-tao2019}, and sale prediction \cite{sales-cikm2019}.
While our work is motivated by the traffic forecasting task, the proposed two adaptive modules and our AGCRN model may also be adapted to a wide variety of multivariate/correlated time series predictive tasks separately or jointly. 
It is possible to automatically discover the inter-dependency among different correlated series from data, which bridges the gap between graph-based prediction models and general correlated time series forecasting problems that cannot pre-define the graph easily. 
Our future work will focus on examining the scale-ability of our work from two perspectives: 1) data perspective - validating the performance of AGCRN on more time series prediction tasks; 2) model perspective - adapting NAPL and DAGG to more GCN-based traffic forecasting models. 


\section{Conclusion}
In this paper, we propose to enhance the traditional graph convolutional network with node adaptive parameter learning and data-adaptive graph generation modules for learning node-specific patterns and discovering spatial correlations from data, separately. Based on the two modules, we further propose the Adaptive Graph Convolutional Recurrent Network, which can capture node-specific spatial and temporal correlations in time-series data automatically without a pre-defined graph. Extensive experiments on multi-step traffic forecasting tasks demonstrate the effectiveness of both AGCRN and the proposed adaptive modules. This work sheds light on applying GCN-based models in correlated time series forecasting by inferring the inter-dependency from data and reveals that learning node-specific patterns is essential for understanding correlated time series data. 

\section*{Broader Impact}
In general, this work enables more accurate traffic forecasting, which facilities the higher-lever traffic scheduling such as taxi dispatch and route planing. In this way, our work can help save time for travelers, improve efficiency and income for transport operators, and save energy consumption. In a broad sense, adaptability is desirable in correlated time series analysis for broad social and business applications in the era of big data.
The proposed adaptive modules enable elevated robustness of data analysis and relevant applications based on dynamic, interdependent, time-series data.
This research generally supports better modeling and analysis of multiple channels of data based on graph structures with complex explicit and implicit correlations. It has implications and potentially accelerates the research progress in address many world-scale economic and societal issues that rely on complex times series data, such as predictions of influenza outbreak, economic growth, and climate change. 
A potential negative impact of this work is the fairness problem in the ride-sharing platforms. In the case that cabs supply cannot guarantee demand, platforms may emphasize the predicted high-demand areas too much, which would increase the waiting time of travelers in the low-demand areas.






\bibliographystyle{unsrt} 
\bibliography{ref}

\newpage
\appendix
\section{Appendix}
To support reproducibility of the results in this paper, we have submitted our code and datasets as the supplementary information. Here, we will present the datasets statistics, evaluation metrics, implementation details, and more results.
\subsection{Datasets Statistics}
The dataset used in our experiments (namely PeMSD4 dataset and PeMSD8 dataset) contain the traffic flow data measured by road traffic sensors. As introduced in Section 3.1, we formulate the traffic forecasting problem on a graph where each node corresponds to a traffic sensor. 
Our ASTGCN can infer spatial proximity from data by DAGG module automatically. Thus is does not require pre-defining the adjacent matrix. For graph-based baselines, we reuse the pre-defined graph given in \cite{stsgcn-aaai2020} to capture spatial correlations. The connectivity between different nodes is determined by the actual road network. If two monitors are on the same road, then they are considered connected. The statistics about the two datasets are shown in Table \ref{dataset}.
\begin{table}[h]
\centering
\caption{Summary statistics of the PeMSD4 and PeMSD8 dataset}
\label{dataset}
\begin{tabular}{@{}ccccccc@{}}
\toprule
Dataset & Time Span                & \#Nodes & \#Edges & \#Samples & Data Range   & Median \\ \midrule
PeMSD4  & 1/Jan/2018 - 28/Feb/2018 & 307     & 340     & 16992     & 0 919  & 180    \\ \midrule
PeMSD8  & 1/Jul/2016 - 31/Aug/2016 & 170     & 277     & 17856     & 0 1147 & 215    \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Evaluation Metrics}
We use three evaluation metrics to measure the performance of predictive models. Let  be the ground truth traffic of all nodes at time step ,  be the predicted values, and  be indices of observed samples. The metrics are defined as follows.

Mean Absolute Error (MAE)


Root Mean Square Error (RMSE)


Mean Absolute Percentage Error (MAPE)


\subsection{Implementation Details}
The details of the baselines are as follows:
\begin{itemize}
    \item HA: the historical average model operates on each traffic series separately, and it averages all the historical traffic at the same time slot to predict current traffic. Historical Average does not depend on recent data and thus the performance is invariant for 12 forecasting horizons. 
    \item VAR: we implement the VAR model based on \textit{statsmodel} python package and search the number of lags among \{1, 3, 6, 9, 12\}. The number of lags is set to 12 for both PeMSD4 and PeMSD8 datasets.
    \item GRU-ED: we implement an encoder-decoder model based on GRU with Pytorch. GRU-ED contains two layers of GUR for both encoder and decoder; each layer has 128 hidden units. A fully-connected layer projects the output of the decoder at each time step to a prediction. We set the batch size to 64, learning rate to 0.001, and the loss function to L1 when training the model.
    \item DSANet: we reuse the code released in the original paper and tune the parameters carefully for our dataset according to the validation error. We set the CNN filter size to 3, number of CNN kernels to 64, number of attention blocks to 3, dropout probability to 0.1, and the learning rate to 0.001.
    \item DCRNN: similar to GRU-ED, the DCRNN model also deploys the ecoder-decoder framework for multi-step traffic forecasting. It contains two-layers DCGRU for both encoder and decoder. We set the number of GRU hidden units to 64, the maximum step of randoms walks to 3, the initial learning rate to 0.01. We decrease the learning rate tby  every 20 epochs starting from  epochs.
    \item STGCN: STGCN contains two spatial-temporal convlutional blocks, one temporal convolutional layer and one output layer. Different from the original STGCN, we implement the output layer to generate prediction for all horizons at one time (instead of one step per time). Following the practice of STGCN, we set the size of temporal kernel to 2, the order of Chebyshev polynomials to 1, and the filter number to 64 for both CNN and GCN. Besides, We set the learning rate to 0.003 for the PeMSD4 dataset and 0.001 for the PeMDS8 dataset.
    \item ASTGCN: The orginal ASTGCN model ensembles three bolocks to process the recent, daily-periodic, and weekly-periodic segments for capturing multi-scale temporal correlations. We take its recent component that only uses recent input segments for a fair comparison. For implementation, we reuse the code and parameters released in the original paper and train the model with a L1 loss function. 
    \item STSGCN: We reuse the results reported in the original paper directly for our overall comparison as it conducts experiments on the PeMSD4 and PeMSD8 datasets with the same evaluation metrics.
\end{itemize}


\textbf{AGCRN}: Our model stacks two layers AGCRN to capture the node-specific spatial and temporal dynamics. The output at the last step is used as the representation of the historical traffic series, which is directly mapped to the predictions for all horizons by linear transformation . For the hype-parameters, we set the hidden unit to 64 for all the AGCRN cells and the batch size also to 64. We search the learning rate among \{0.0007, 0.001, 0.003, 0.005, 0.009\}, the embedding dimension among \{1, 3, 5, 10, 15, 20, 30\} for the PeMSD4 dataset and among \{1, 2, 3, 5, 8, 10, 15\} for the PeMSD8 dataset. Finally, the learning rate is set to 0.003 for both datasets, and the embedding dimension is to 10 for the PeMSD4 dataset and 2 for the PeMSD8 dataset. Besides, we choose L1 Loss as the loss function and do not use any non-mentioned optimization tricks such as learning rate decay, weights decay, or gradient normalization when training our model.

For all the deep learning models, we optimize them with the Adam optimizer for 100 epochs and use an early stop strategy with the patience of 15 by monitoring the loss in the validation set.



\subsection{Multi-step Prediction on PeMSD8}
\begin{figure}[h]
\centering
\subfigure[MAE]{
\includegraphics[width=.32\linewidth]{figs/pems08_mae.png}}
\subfigure[RMSE]{
\includegraphics[width=.32\linewidth]{figs/pems08_rmse.png}}
\subfigure[MAPE]{
\includegraphics[width=.32\linewidth]{figs/pems08_mape.png}}
\caption{Prediction performance comparison at each horizon on the PeMSD8 dataset.}
\label{pemsd8} 
\end{figure}
Fig. \ref{pemsd8} presents the prediction performance of our AGCRN and baselines at each horizon on the PeMSD8 dataset. STSGCN is not included because the step-wise results of it are not reported in \cite{stsgcn-aaai2020}. Besides, we omit HA as it's performance is consistent for all 12 horizons. Our AGCRN model outperforms existing baselines with a significant margin, especially for long-term predictions. Besides, the performance of AGCRN deteriorates much slower than the other GCN-based models. The observations are similar on the PeMSD4 dataset.

\subsection{Prediction Visualization}
\begin{figure}[h]
\centering
\subfigure[]{
\includegraphics[width=\linewidth]{figs/15.png}}
\subfigure[]{
\includegraphics[width=\linewidth]{figs/17.png}}
\subfigure[]{
\includegraphics[width=\linewidth]{figs/23.png}}
\caption{Traffic forecasting visualization.}
\label{visualization1} 
\end{figure}

\begin{figure}[h]
\centering
\subfigure[]{
\includegraphics[width=\linewidth]{figs/63.png}}
\subfigure[]{
\includegraphics[width=\linewidth]{figs/78.png}}
\subfigure[]{
\includegraphics[width=\linewidth]{figs/89.png}}
\caption{Traffic forecasting visualization.}
\label{visualization2} 
\end{figure}


\begin{comment}
\section{Graph Convolutional Network}
Our work is based on the widely-used spectral Graph Convolutional Network \cite{gcn-defferrard,kipf-gcn}, which is formulated in the Fourier domain. In this part, we introduce the knowledge about  Chebyshev polynomial approximation of GCN that we mentioned in Eq. \ref{gcn_formulation} and Section \ref{model_analysis}.
\end{comment}

\end{document}
