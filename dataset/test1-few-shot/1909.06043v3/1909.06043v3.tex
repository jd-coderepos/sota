\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{bm, multirow}
\usepackage{enumitem}

\usepackage{color,soul}

\usepackage{tabularx} \usepackage{rotating}
\newcolumntype{L}[1]{>{\hsize=#1\hsize\raggedright\arraybackslash}X}\newcolumntype{R}[1]{>{\hsize=#1\hsize\raggedleft\arraybackslash}X}\newcolumntype{C}[1]{>{\hsize=#1\hsize\centering\arraybackslash}X}\newcommand\RotText[1]{\rotatebox{90}{\parbox{2cm}{\centering#1}}}
\newcommand{\hlcyan}[1]{{\sethlcolor{cyan}\hl{#1}}}



\newcommand{\bK}{\mathbf{K}}
\newcommand{\bx}{\bm{x}}
\newcommand{\by}{\bm{y}}
\newcommand{\bz}{\bm{z}}
\newtheorem{tm}{Theorem}
\DeclareMathOperator*{\argmin}{arg\,min}
\graphicspath{{figs/}}





\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy 

\def\cvprPaperID{6840} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ifcvprfinal\pagestyle{empty}\fi

\begin{document}

\title{End-to-End Learnable Geometric Vision by Backpropagating PnP Optimization}

\author{Bo Chen \quad \'{A}lvaro Parra \quad  Jiewei Cao \quad Nan Li \quad Tat-Jun Chin\\
The University of Adelaide \quad Shenzhen University\\
{\tt\small \{bo.chen, alvaro.parrabustos,  jiewei.cao, tat-jun.chin\}@adelaide.edu.au \quad nan.li@szu.edu.cn}
}

\maketitle


\begin{abstract}
Deep networks excel in learning patterns from large amounts of data. On the other hand, many geometric vision tasks are specified as optimization problems. To seamlessly combine deep learning and geometric vision, it is vital to perform learning and geometric optimization end-to-end. Towards this aim, we present BPnP, a novel network module that backpropagates gradients through a Perspective-n-Points (PnP) solver to guide parameter updates of a neural network. Based on implicit differentiation, we show that the gradients of a ``self-contained" PnP solver can be derived accurately and efficiently, as if the optimizer block were a differentiable function. We validate BPnP by incorporating it in a deep model that can learn camera intrinsics, camera extrinsics (poses) and 3D structure from training datasets. Further, we develop an end-to-end trainable pipeline for object pose estimation, which achieves greater accuracy by combining feature-based heatmap losses with 2D-3D reprojection errors. Since our approach can be extended to other optimization problems, our work helps to pave the way to perform learnable geometric vision in a principled manner. Our PyTorch implementation of BPnP is available on \url{http://github.com/BoChenYS/BPnP}.
\end{abstract}

\section{Introduction}

The success of deep learning is due in large part to its ability to learn patterns from vast amounts of training data. Applications that have benefited from this ability include object detection and image segmentation~\cite{krizhevsky12,he17}. Fundamentally, such problems can often be formulated as classification/regression problems, which facilitates suitable objective functions for backpropagation learning~\cite{lecun15}.

On the other hand, there are many important computer vision tasks that are traditionally formulated as geometric optimization problems, e.g., camera localization/pose estimation, 3D reconstruction, point set registration. A common property in these optimization problems is the minimization of a residual function (e.g., sum of squared reprojection errors) defined over geometric quantities (e.g., 6DOF camera poses), which are not immediately amenable to backpropagation learning. This limits the potential of geometric vision tasks to leverage large datasets.

A straightforward solution towards ``learnable" geometric vision is to replace the ``front end" modules (e.g., image feature detection and matching) using a deep learning alternative~\cite{Thewlis2016fully,Yi2016lift,Suwajanakorn2018discovery}. However, this does not allow the ``back end" steps (e.g., searching for optimal geometric quantities) to influence the training of the neural network parameters.

On the other extreme, end-to-end methods have been devised~\cite{kendall2015posenet,Kendall2016modelling,kendall2017geometric,brahmbhatt2018geometry,naseer2017deep,walch2017image,wu2017delving,Cai2018hybrid} that bypass geometric optimization, by using fully connected layers to compute the geometric quantity (e.g., 6DOF camera pose) from a feature map derived from previous layers. However, it has been observed that these methods are equivalent to performing image retrieval~\cite{Sattler2019understanding}, which raises questions on their ability to generalize. Also, such end-to-end methods do not explicitly exploit established methods from geometric vision~\cite{hartley2003multiple}, such as solvers for various well-defined tasks.

To benefit from the strengths of deep learning \emph{and} geometry, it is vital to combine them in a mutually reinforcing manner. One approach is to incorporate a geometric optimization solver in a deep learning architecture, and allow the geometric solver to participate in guiding the updates of the neural network parameters, thereby realising end-to-end learnable geometric vision. The key question is how to compute gradients from a ``self-contained" optimizer.

A recent work towards the above goal is differentiable RANSAC~\cite{Brachmann2017dsac, brachmann2019expert, brachmann2019neural}, which was targeting at the camera localization task. A perspective-n-point (PnP) module was incorporated in a deep architecture, and the derivatives of the PnP solver are calculated using central differences~\cite{richardson1954introduction} to enable parameter updates in the rest of the pipeline. However, such an approach to compute gradients is inexact and time consuming because, in order to obtain each partial derivative, it requires solving PnP at values that lie to the left and right of the input.

Other approaches to derive gradients from an independent optimization block for backpropagation learning~\cite{gould2016differentiating,amos2017optnet} conduct implicit differentiation~\cite[Chap.~8]{binmore83}. Briefly, in the context of end-to-end learning, the gradient of the optimization routine with respect to the input variables can be computed via partial derivatives of the stationary constraints of the optimization problem (more details in Sec.~\ref{sec:bpnp}). The gradient can then be backpropagated to the previous layers for parameter updates. A number of motivating examples and applications were explored in~\cite{gould2016differentiating,amos2017optnet}. However, larger-scale experiments in the context of specific geometric vision problems, and benchmarking against other end-to-end learning alternatives, were unavailable in~\cite{gould2016differentiating,amos2017optnet}. It is worth noting that implicit differentiation of optimization subroutines has been explored previously in several computer vision applications~\cite{tappen07,eriksson10,schmidt14} (also earlier in~\cite[Chap.~5]{faugeras93}).

\paragraph{Contributions}

Our main contribution is a novel network module called \emph{BPnP} that incorporates a PnP solver. BPnP backpropagates the gradients through the PnP ``layer" to guide the updates of the neural network weights, thereby achieving end-to-end learning using an established objective function (sum of squared 2D-3D reprojection errors) and solver from a geometric vision problem. Despite incorporating only a PnP solver, we show how BPnP can be used to learn effective deep feature representations for multiple geometric vision tasks (pose estimation, structure-from-motion, camera calibration). We also compare our method against state-of-the-art methods for geometric vision tasks. Fundamentally, our method is based on implicit differentiation; thus our work can be seen as an application of~\cite{gould2016differentiating,amos2017optnet} to geometric vision learning.




\section{Related works}


\paragraph{Backpropagating optimization problems}



As alluded to above, there are several works that incorporate optimizer blocks in deep neural network architectures, and perform differentiation of the optimization routines for backpropagation learning. A subset of these works address the chellange of incorporating RANSAC in an end-to-end trainable pipeline, such as DSAC~\cite{Brachmann2017dsac}, ESAC~\cite{brachmann2019expert}, and NG-DSAC~\cite{brachmann2019neural}. In fact, since these works aim to solve camera localization, they also incorporate a PnP solver in their pipeline. To backpropagate through the PnP solver, they use central differences to compute the partial derivatives. In effect, if the input dimension is , it requires solving PnP  times in order to obtain the full Jacobian. Another group of methods applies implicit differentiation~\cite{gould2016differentiating,amos2017optnet}, which provides an exact and efficient solution for backpropagating through an optimization process. We will describe implicit differentiation in detail later.

\paragraph{Pose estimation from images}

A target application of our BPnP is pose estimation. Existing works on end-to-end pose estimation~\cite{kendall2015posenet,Kendall2016modelling,kendall2017geometric,brahmbhatt2018geometry,naseer2017deep,walch2017image,wu2017delving} usually employ fully connected layers to compute the target output (pose) using feature maps from previous layers. The output loss function is typically defined using pose metrics (e.g., chordal distance), which are backpropagated using standard differentiation. A recent analysis~\cite{Sattler2019understanding} suggests that what is being performed by these end-to-end networks is akin to learning a set of base poses from the training images, computing a set of weights for the testing image, then predicting the pose as a weighted combination of the base poses. It was further shown that such methods were more related to image retrieval than intrinsically learning to predict pose, hence they may not outperform an image retrieval baseline~\cite{Sattler2019understanding}.

Other pose estimation approaches that combine deep learning with geometric optimization (PnP solver)~\cite{pavlakos20176, rad2017bb8, tekin2018real, Peng2019pvnet, Chen2019satellite} adopt a two-stage strategy: first learn to predict the 2D landmarks or fiducial points from the input image, then perform pose estimation by solving PnP on the 2D-3D correspondences. While the first stage can  benefit from the regularities existing in a training dataset, the second stage (PnP solving) which encodes the fundamental geometric properties of the problem do not influence the learning in the first stage. Contrast this to our BPnP which seamlessly connects both stages, and allows the PnP optimizer to guide the weight updates in the first stage (in addition to standard keypoint or landmark regression losses).

\paragraph{Depth estimation and 3D reconstruction} There exist many works that employ deep networks to learn to predict depth or 3D structure from input images in an end-to-end fashion. Some of these works~\cite{Handa2016gvnn, Clark2018learning, Kendall2017end} can only impose constraints on pairs of images, while others~\cite{Zhou2017unsupervised, Ummenhofer2017demon} learn the structure and the motion in different network branches and do not impose explicit geometric constraints. Also, many of such works~\cite{Eigen2014depth, Ladicky2014pulling, Liu2016learning, Liu2015deep, Laina2016deeper, Xu2017multi, Wang2015towards} require training datasets with ground truth depth labels, which can be expensive to obtain. The proposed BPnP may help to alleviate this shortcoming; as we will show in Sec.~\ref{sec:sfm}, a simple structure-from-motion (SfM) framework that utilizes BPnP can jointly optimize using multiple views (not just two), explicitly impose geometric constraints, and learn structure and motion in an unsupervised fashion without depth labels or ground truth 3D structures.






\section{Backpropagating a PnP solver (BPnP)}\label{sec:bpnp}

Let  denote a PnP solver in the form of a ``function"

which returns the 6DOF pose  of a camera with intrinsic matrix  from  2D-3D correspondences

where  is the -th correspondence. Let  be a projective transformation of 3D points onto the image plane with pose  and camera intrinsics . Intrinsically, the ``evaluation" of  requires solving the optimization problem

where 

is the reprojection error of the -th correspondence and

is the projection of 3D point  on the image plane. We introduce the shorthand

thus~\eqref{eq:pnp0} can be rewritten as

The choice of formulation~\eqref{eq:pnp} will be justified in Sec.~\ref{sec:formulation}.

Our ultimate goal is to incorporate  in a learnable model, where ,  and  can be the (intermediate) outputs of a deep network. Moreover, the solver for~\eqref{eq:pnp} should be used to participate in the learning of the network parameters. To this end, we need to treat  as if it were a differentiable function, such that its ``gradients" can be backpropagated to the rest of the network. In this section, we show how this can be achieved via implicit differentiation.

\subsection{The Implicit Function Theorem (IFT)}
 
\begin{tm}[\cite{Krantz2012implicit}]\label{tm:ift}
Let  be a continuously differentiable function with input . If a point  satisfies

and the Jacobian matrix  is invertible, then there exists an open set  such that  and an unique continuously differentiable function  such that  and 


Moreover, for all , the Jacobian matrix  is given by
  
\end{tm}

The IFT allows computing the derivatives of a function  with respect to its input  without an explicit form of the function, but with a function  constraining  and . 



\subsection{Constructing the constraint function }\label{sec:f}



To invoke the IFT for implicit differentiation, we first need to define the constraint function  such that Eq.~\eqref{eq:f_con} is upheld. For our problem, we use all four variables , ,  and  to construct . But we treat  as a two variables function , in which  takes values in   - depending on which partial derivative to obtain - and   (\ie, the output pose of ).

To uphold Eq.~\eqref{eq:f_con}, we exploit the stationary constraint of the optimization process. Denote the objective function of the PnP solver  as

Since the output pose  of a PnP solver is a local optimum for the objective function, a stationary constraint can be established by taking the first order derivative of the objective with respect to , \ie,


Given an output pose from a PnP solver , we construct  based on Eq.~\eqref{eq:f0}, which can be written as

where for all , 

with 



\subsection{Forward and backward pass}\label{sec:formulation}
Our PnP formulation~\eqref{eq:pnp} for  essentially performs least squares (LS) estimation, which is not robust towards outliers (egregious errors in ,  and ). Alternatively, we could apply a more robust objective, such as incorporating an M-estimator~\cite{Zhang1997parameter} or maximizing the number of inliers~\cite{Fischler1981random}. However, our results suggest that LS is actually more appropriate, since its sensitivity to errors in the input measurements encourages the learning to quickly converge to parameters that do not yield outliers in ,  and . In contrast, a robust objective would block the error signals of the outliers, causing the learning process to be unstable. 

Given~\eqref{eq:pnp}, the choice of the solver remains. To conduct implicit differentiation, we need not solve~\eqref{eq:pnp} exactly, since~\eqref{eq:f0} is simply the stationary condition of~\eqref{eq:pnp}, which is satisfied by any local minimum. To this end, we apply the Levenberg-Marquardt (LM) algorithm (as implemented in the \texttt{SOLVEPNP\_ITERATIVE} method in OpenCV~\cite{Opencv2000}), which guarantees local convergence. As an iterative algorithm, LM requires initialization  in solving~\eqref{eq:pnp}. We make explicit this dependence by rewriting~\eqref{eq:g} as

We obtain the initial pose  with RANSAC if it is not provided.


In the backward pass, we first construct  as described in Sec.~\ref{sec:f} to then obtain the Jacobians of  with respect to each of its inputs as

Given the output gradient , BPnP returns the input gradients 















\subsection{Implementation notes}

The number of dimensions of , i.e., , is dependant on the parameterization of  within the pose. For example,  for the axis-angle representation,  for the quaternion representation, and  for the rotation matrix representation. Experimentally we found the axis-angle representation leads to the best result, possibly since then  is equal to the degrees of freedom.





We compute the partial derivatives in Eqs.~\eqref{eq:cij}, \eqref{eq:ff1}, \eqref{eq:ff2}, and \eqref{eq:ff3} using the Pytorch autograd package~\cite{Paszke2017automatic}. 




\section{End-to-end learning with BPnP}

BPnP enables important geometric vision tasks to be solved using deep networks and PnP optimization in an end-to-end manner. Here, we explore BPnP for pose estimation, SfM and camera calibration, and report encouraging initial results. These results empirically validate the correctness of the Jacobians ,  and  of the PnP solver  obtained using implicit differentiation in Sec.~\ref{sec:f}.



This section is intended mainly to be illustrative; in Sec.~\ref{sec:objectpose}, we will develop a state-of-the-art object pose estimation method based on BPnP and also report more comprehensive experiments and benchmarks.

\subsection{Pose estimation}\label{sec:pose_reg}



Given a known sparse 3D object structure  and known camera intrinsics , a function  (a deep network, e.g., CNN, with trainable parameters ) maps input image  to a set of 2D image coordinates  corresponding to , before  is invoked to calculate the object pose . Our goal is to train  to accomplish this task. Since the main purpose of this section is to validate BPnP, it is sufficient to consider a ``fixed input" scenario where there is only one training image  with ground truth pose .

Algorithm~\ref{alg:pose_reg} describes the algorithm for this task. The loss function  has the form

which is the sum of squared errors between the projection of  using the ground truth pose  and current pose  from PnP (which in turn depends on ), plus a regularization term

The regularization ensures convergence of the estimated image coordinates  to the desired positions (note that the first error component does not impose constraints on ).

\begin{algorithm}[t]
\caption{Pose estimation.}
\label{alg:pose_reg}
\begin{algorithmic}[1]
\STATE  Identity pose.
\STATE Randomly initialize 
\WHILE{loss  has not converged}
\STATE .
\STATE .
\STATE .
\STATE . \quad (Backpropagate through PnP)   \ENDWHILE
\end{algorithmic}
\end{algorithm}


A main distinguishing feature of Algorithm~\ref{alg:pose_reg} is that one of the gradient flow of  is calculated w.r.t.~ before the gradient of  is computed w.r.t.~to  which is then backpropagated to update :

The implicit differentiation of  follows Eq.~\eqref{eq:ff1}.



Figs.~\ref{fig:pose_reg1} and \ref{fig:pose_reg2} illustrate Algorithm~\ref{alg:pose_reg} on a synthetic example with  landmarks, respectively for the cases
\begin{itemize}[leftmargin=1em,topsep=0.5em,parsep=0.5em]
\item , i.e., the parameters  are directly output as the predicted 2D keypoint coordinates ; and
\item  is a modified VGG-11~\cite{Simonyan2015very} network that outputs the 2D keypoints  for .
\end{itemize}
The experiments show that the loss  is successfully minimized and the output pose  converges to the target pose ---this is a clear indication of the validity of~\eqref{eq:ff1}.

The experiments also demonstrate the usefulness of the regularization term. While the output pose  will converge to  with or without , the output of  (the predicted keypoints ) can converge away from the desired positions  without regularization.






\begin{figure}[t]\centering
    \centering
    \includegraphics[width=\linewidth]{aaa1}
    \includegraphics[width=\linewidth]{aaa2}
    \caption{Sample run of Algorithm~\ref{alg:pose_reg} on synthetic data with  landmarks and , where . The first and second row has  and  respectively. Left column: loss curve. Middle column: evolution of  presented as . Right column: the evolution of predicted keypoints . Red square markers represent the target locations .}
    \label{fig:pose_reg1}
\end{figure}

\begin{figure}[t]\centering
    \centering
    \includegraphics[width=\linewidth]{bbb1}
    \includegraphics[width=\linewidth]{bbb2}
    \caption{Same experiment as in Fig.~\ref{fig:pose_reg1} except that  is a modified VGG-11~\cite{Simonyan2015very} network which outputs the 2D keypoints .}
    \label{fig:pose_reg2}
\end{figure}


\subsection{SfM with calibrated cameras}\label{sec:sfm}

Let  indicate a set of 2D image features corresponding to  3D points  associated/tracked across  frames . Following~\eqref{eq:points}, each  is a vector of 2D coordinates; however,  may not be fully observed in , thus  could contain fewer than  2D coordinates. Let

indicate the selection of the 3D points  that are seen in . Given  and the camera intrinsics for each frame (assumed to be constant  without loss of generality), we aim to estimate the 3D structure  and camera poses  corresponding to the  frames.

\begin{algorithm}[t]
\caption{SfM with calibrated cameras.}
\label{alg:sfm}
\begin{algorithmic}[1]
\STATE  Identity pose for .
\STATE Randomly initialize 
\WHILE{loss  has not converged}
\STATE .
\STATE , for 
\STATE , for .
\STATE .
\STATE . \quad (Backpropagate through PnP) 
\ENDWHILE
\end{algorithmic}
\end{algorithm}

Our end-to-end method estimates the 3D structure

using a deep network  (a modified VGG-11~\cite{Simonyan2015very}) with the input fixed to a -tensor (more on this below); see Algorithm~\ref{alg:sfm}. Note that the algorithm makes use of the PnP subroutine to estimate each camera pose given the current  estimate. The loss function  has the form

which is simply the sum of squared reprojection errors across all frames. Again, a unique feature of our pipeline is the backpropagation of the loss through the PnP solver to update network parameters .

The implicit differentiation of  follows Eq.~\eqref{eq:ff2}.










Fig.~\ref{fig:sfm_duck} illustrates the results of Algorithm~\ref{alg:sfm} on a synthetic dataset with  points on a 3D object seen in  images (about half of the 3D points are seen in each image). Starting from a random initialization of  (which leads to a poor initial ), the method is able to successfully reduce the loss and recover the 3D structure and camera poses. Fig~\ref{fig:sfm_driller} shows the result from another dataset.

Effectively, our tests show that a generic deep model (VGG-11 with fixed input~\eqref{eq:vggtensor}) is able to ``encode" the 3D structure  of the object in the network weights, even though the network is not designed using principles from multiple view geometry. Again, our aim in this section is mainly illustrative, and Algorithm~\ref{alg:sfm} is not intended to replace established SfM algorithms, e.g.,~\cite{schoenberger2016pixelwise, schoenberger2016structure}. However, the results again indicate the correctness of the steps in Sec.~\ref{sec:bpnp}.

\begin{figure}[t]
    \centering
    \includegraphics[width = \linewidth]{sfm_duck2.png}
    \caption{SfM result with Algorithm~\ref{alg:sfm}. The mesh of the object has  points , which were projected to  different views to obtain  (about half of the 3D points are seen in each view). The function  is a modified VGG-11 network~\cite{Simonyan2015very} which outputs the 3D structure  from a fixed input of -tensor. We depict the output structure  at various steps. A movie of this reconstruction is provided in the supplementary material.}
    \label{fig:sfm_duck}
\end{figure}

\begin{figure}\centering
    \includegraphics[width = \linewidth]{sfm_driller.png}
    \caption{SfM result with a different object which has  points . All settings are the same as in Fig.~\ref{fig:sfm_duck}. A movie of this reconstruction is provided in the supplementary material.}
    \label{fig:sfm_driller}
\end{figure}


\subsection{Camera calibration}

In the previous examples, the intrinsic matrix  is assumed known and only  and/or  are estimated. Here in our final example, given  and  (2D-3D correspondences), our aim is to estimate  of the form

where  and  define the focal length, and  and  locate the principal point of the image.

\begin{algorithm}[t]
\centering
\caption{Camera calibration.}
\label{alg:CamCali}
\begin{algorithmic}[1]
\STATE  Identity pose.
\STATE Randomly initialize 
\WHILE{loss  has not converged}
\STATE .
\STATE .
\STATE .
\STATE .
\STATE . \quad (Backpropagate through PnP) 
\ENDWHILE
\end{algorithmic}
\end{algorithm}

We assume . Under our BPnP approach, we train a simple neural network

to learn the parameters from correspondences  and , where . Algorithm~\ref{alg:CamCali} summarizes a BPnP approach to learn the parameters  of . The loss function is simply the sum of squared reprojection errors

which is backpropagated through the PnP solver via

The implicit differentiation of  follows Eq.~\eqref{eq:ff3}.


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{CamCali}
    \caption{Camera calibration using Algorithm~\ref{alg:CamCali}, based on the ground truth correspondences ,  in Fig.~\ref{fig:pose_reg1}. Left: loss curve. Right: the intrinsic parameters which converge to the ground truth. }
    \label{fig:CamCali}
\end{figure}

Fig.~\ref{fig:CamCali} illustrates the result of Algorithm~\ref{alg:CamCali} using the ground truth correspondences ,  in Fig.~\ref{fig:pose_reg1} as input. The correct intrinsic parameters are , which the algorithm can clearly achieve as the loss converges to . 










\section{Object pose estimation with BPnP}\label{sec:objectpose}




We apply BPnP to build a model that regresses the object pose directly from an input image, not through fully connected layers, but through projective geometric optimization while remaining end-to-end trainable. Our model is unique in that it simultaneously learns from feature-based loss and geometric constraints in a seamless pipeline. 

The pipeline of the proposed model is depicted in Fig.~\ref{fig:obpose_pipeline}. We use HRNet~\cite{Sun2019deep} as the backbone to predict the landmark heatmaps . We then use the Differentiable Spatial to Numerical Transform (DSNT)~\cite{Nibali2018numerical} to convert heatmaps  to 2D landmark coordinates . Finally, BPnP obtains the pose  from the 2D landmarks  and the 3D structural landmarks of the object .


\begin{figure} \centering
    \includegraphics[width=\linewidth]{arch2}
    \caption{The pipeline of our object pose estimation network. }
    \label{fig:obpose_pipeline}
\end{figure}

Let  denote the ground truth heatmaps constructed with the ground truth 2D landmarks . We define a heatmap loss

where MSE is the mean squared error function; a pose loss

and a mixture loss

for training the model respectively. The regularization term  is defined in Eq.~\eqref{eq:Rxy}. Note that in the mixture loss  is unnecessary because the heatmap loss  acts as a regularization term. We set the balancing weight  to  in the experiments.

We apply our pipeline on the LINEMOD~\cite{Hinterstoisser2012model} dataset. For each object we
\begin{itemize}[leftmargin=1em,topsep=0.3em,parsep=0.3em]
    \item obtain a 3D model representation consisting of 15 landmarks by using the Farthest Point Sampling (FPS)~\cite{Peng2019pvnet} over the original object mesh, 
    \item randomly reserve  images as the test set and set the remaining (about , depending on the object) as the training set, and
    \item train a model to predict the 6DOF object pose from the input image.
\end{itemize}
We train each model with three different losses (, , and ), for 120 epochs each. To assist convergence, when training the model with  and , we first train with  for the first 10 epochs leaving the remaining  epochs to train the target loss. 






We evaluate our method with the following two metrics. 
\begin{description}
    \item[Average 3D distance of model points (ADD)~\cite{Hinterstoisser2012model}] This is the percentage of accurately predicted poses in the test set. We consider a predicted pose as accurate if the average distance between the 3D model points expressed in the predicted coordinate system and that expressed in the ground truth coordinate system is less than 10\% of the model diameter. For symmetric objects we use the  ADD-S~\cite{Xiang2018posecnn} metric instead which is based on the closest point distance.
    
    


    \item [2D projection~\cite{Brachmann2016Uncertainty}.] Mean distance between 2D keypoints projected with the estimated pose and those projected with ground truth pose. An estimated pose is considered correct if this distance is less than a threshold . 
\end{description}









\begin{table*} \begin{center}
    \begin{tabular}{|c|cccc|cccc|ccc|}
   \hline
    \multicolumn{1}{|c|}{\multirow{3}{*}{Model} }
   &\multicolumn{4}{c|}{ADD(-S)}
   &\multicolumn{4}{c|}{2D projection with }
   &\multicolumn{3}{c|}{2D projection with  }
   \\
   \cline{2-12}
  &\multicolumn{3}{c|}{Ours}
  &\multicolumn{1}{c|}{\multirow{2}{*}{PVNet}}
  &\multicolumn{3}{c|}{Ours}
  &\multicolumn{1}{c|}{\multirow{2}{*}{PVNet}}
  &\multicolumn{3}{c|}{Ours}
  \\
  \cline{2-4}
  \cline{6-8}
  \cline{10-12}
  & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c|}{} &
  & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c|}{} &
  & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c|}{} 
  \\
  \hline
ape&74.00&56.75&\textbf{74.75}&43.62&\textbf{99.50}&\textbf{99.50}&\textbf{99.50}&99.23 & 90.00 & 86.75 & \textbf{93.75} \\
benchwise&98.50&98.00&99.00&\textbf{99.90}&99.25&98.75&99.25&\textbf{99.81}&\textbf{86.00} & 82.75 & \textbf{86.00} \\
cam&\textbf{96.25}&83.75&\textbf{96.25}&86.86&98.75&98.50&\textbf{99.25}&99.21& \textbf{91.50} & 81.50 & 90.50 \\
can&97.00&94.75&\textbf{98.00}&95.47&99.50&99.50&99.75&\textbf{99.90}& \textbf{93.25} & 89.00 & 92.75 \\
cat&93.00&85.25&\textbf{94.25}&79.34&\textbf{99.50}&\textbf{99.50}&\textbf{99.50}&99.30& \textbf{96.75} & 95.25 & \textbf{96.75} \\
driller&98.50&98.00&\textbf{99.25}&96.43&\textbf{99.00}&98.50&98.50&96.92& 83.75 & 81.50 &\textbf{84.50} 
\\
duck&76.25&49.25&\textbf{78.50}&52.58
&99.00&\textbf{99.25}&99.00&98.02
& 88.50 & 84.00 & \textbf{91.50} 
\\
eggbox&95.75&93.25&96.50&\textbf{99.15}
&\textbf{99.50}&99.25&\textbf{99.50}&99.34
& 92.75 & \textbf{93.25} & 92.50 
\\
glue&87.50&76.25&90.00&\textbf{95.66}
&\textbf{99.50}&\textbf{99.50}&\textbf{99.50}&98.45
& 93.50 & 90.00 & \textbf{94.00} 
\\
holepuncher&89.50&80.25&\textbf{91.50}&81.92
&99.75&99.50&99.75&\textbf{100.00}
& \textbf{92.25} & 90.25 & 91.75 
\\
iron&97.75&96.50&97.75&\textbf{98.88}
&98.50&98.25&98.50&\textbf{99.18}
& 86.75 & 79.75 & \textbf{87.25} 
\\
lamp&\textbf{99.75}&98.50&\textbf{99.75}&99.33
&\textbf{98.75}&98.00&98.50&98.27
& 85.00 & 83.25 & \textbf{86.75} 
\\
phone&95.75&96.00&\textbf{97.00}&92.41
&99.25&99.00&99.25&\textbf{99.42}
& 91.50 & 88.00 & \textbf{92.25} \\
\hline
average&92.27&85.12&\textbf{93.27}&86.27
&\textbf{99.21}&99.00&\textbf{99.21}&99.00
& 90.12 & 86.56 & \textbf{90.79} 
\\
\hline
 \end{tabular}
\end{center}
\caption{
Test accuracy on the LINEMOD dataset in terms of the ADD(-S) metric (columns 2-5) and the 2D projection metric with  pixels (columns 6-9) and  pixels (columns 10-12). Objects eggbox and glue are considered as symmetric objects and the ADD-S metric is used. }
\label{tab:result}
\end{table*}

\begin{figure*}\centering
    \includegraphics[width=0.98\linewidth]{results}
    \caption{Random sample of test results of the proposed model trained with the mixture loss . The first row are the test images for predicting the pose of the central object. The second row shows the regressed alignment for each query object. }
\label{fig:linemod}
\end{figure*}

Table~\ref{tab:result} summarizes the results of our experiments. In terms of the ADD(-S) metric, the model trained with  performs considerably better than the one with . As expected, heatmaps can exploit richer spatial features than coordinates. However, the mixture loss achieves the highest accuracy, which suggests that heatmap loss benefits from additional correction signals from the pose loss. 





In terms of the 2D projection metric, all methods perform similarly, with an average accuracy of at least 99\%.  To better distinguish the performances amongst different loss functions, we tighten the positive threshold  from the standard 5 pixels to 2 pixels. Consistent with the ADD(-S) result,  the mixture loss outperformed pure heatmap loss on training an object pose estimation model. Visualization of a random subset from the test results is shown in Fig.~\ref{fig:linemod}.



We provide the result of the current state-of-the-art PVNet~\cite{Peng2019pvnet} as a reference. Overall, models trained with  and  have higher average test accuracy than PVNet, in terms of both the ADD(-S) metric and the 2D projection metric. We remind the reader to be aware of several factors while comparing the performances: we use a different backbone from PVNet; our train-test numbers of images are about 800-400 while about 20200-1000 in PVNet. Because we require the ground truth pose label for training, we did not use any data augmentation such as cropping, rotation or affine transformation. 






\section{Conclusions}


We present BPnP, a novel approach for performing backpropagation through a PnP solver. BPnP leverages on implicit differentiation to address computing the non-explicit gradient of this geometric optimization process. We validate our approach in three fundamental geometric optimization problems (pose estimation, structure from motion, and camera calibration). Furthermore, we developed an end-to-end trainable object pose estimation pipeline with BPnP, which outperforms the current state-of-the-art. Our experiments show that exploiting 2D-3D geometry constraints improves the performance of a feature-based training scheme.

The proposed BPnP opens a door to vast possibilities for designing new models. We believe the ability to incorporate geometric optimization in end-to-end pipelines will further boost the learning power and promote innovations in various computer vision tasks. 



\subsection*{Acknowledgement} This work was supported by ARC LP160100495 and the Australian Institute for Machine Learning. Nan Li is sponsored by NSF China (11601378) and Tencent-SZU Fund.



\clearpage

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
