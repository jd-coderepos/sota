

\typeout{IJCAI--21 Instructions for Authors}



\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in
\usepackage{ijcai21}

\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage{amssymb}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\urlstyle{same}
\usepackage{booktabs}
\usepackage{threeparttable}


\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}







\pdfinfo{
/TemplateVersion (IJCAI.2021.0)
}

\title{Gumbel-Attention for Multi-modal Machine Translation}

\author{
}

\author{
Pengbo Liu
\and
Hailong Cao\and
Tiejun Zhao\and
\affiliations
Harbin Institute of Technology, Harbin, China\\
\emails
liupengbo.work@gmail.com ,
\{hailong,tjzhao\}@mtlab.hit.edu.com
}

\begin{document}

\maketitle

\begin{abstract}
Multi-modal machine translation (MMT) improves translation quality by introducing visual information. However, the existing MMT model ignores the problem that the image will bring information irrelevant to the text, causing much noise to the model and affecting the translation quality. In this paper, we propose a novel Gumbel-Attention for multi-modal machine translation, which selects the text-related parts of the image features. Specifically, different from the previous attention-based method, we first use a differentiable method to select the image information and automatically remove the useless parts of the image features. Through the score matrix of Gumbel-Attention and image features, the image-aware text representation is generated. And then, we independently encode the text representation and the image-aware text representation with the multi-modal encoder. Finally, the final output of the encoder is obtained through multi-modal gated fusion.
Experiments and case analysis prove that our method retains the image features related to the text, and the remaining parts help the MMT model generates better translations.

\end{abstract}

\section{Introduction}
Multi-modal machine translation (MMT) is a novel research field of machine translation, which not only considers text information but also uses other modal information (mostly visual modal information) to improve translation effects. Under the influence of visual modal information, the translation result will be more accurate, since the contextual representation of the fusion of visual information will reduce ambiguity. 

Recent research explores various methods based on the seq2seq network for MMT. 
~\cite{DBLP:conf/wmt/HuangLSOD16}transform and make the image features as one of the steps in the encoder as text, in order to make it possible to attend to both the text and the image while decoding.
\cite{DBLP:conf/emnlp/CalixtoL17} uses global image features to initialize the hidden state of the encoder/decoder.
~\cite{DBLP:conf/emnlp/ZhouCLY18} adopts the mechanism of visual attention to jointly optimize the shared visual language embedding and model, which links visual semantics with corresponding text semantics. 
In Zero-resource Machine Translation, images are used to generate bilingual descriptions to complete the translation model, thereby solving the problem of lack of data ~\cite{DBLP:conf/ijcai/ChenJF19}.


Transformer~\cite{DBLP:conf/nips/VaswaniSPUJGKP17} introduces a multi-head self-attention mechanism, which can capture relationships between words in a sentence.
There have been many improved MMT models based on the Transformer recently.
~\cite{DBLP:conf/acl/IveMS19} propose a translate-and-refine method based on deliberation networks and structured visual information on Transformer, where images are only used by a second stage decoder. 
To solve the problem of relative importance among different modalities,  ~\cite{DBLP:conf/acl/YaoW20} models text modal and vision modal from the perspective of a graph network.
\begin{figure}[t]
\centering\includegraphics[width=0.48\textwidth]{example.pdf}
	\footnotesize\caption{An example of useless information in the Multi30k dataset. The above one is the original picture, and the bottom is the remaining part after masking the parts irrelevant to the text in the picture.} 
	\label{fig:01}
\end{figure}

The above methods integrate multi-modal information in multi-modal machine translation, and has made great progress. 
Despite their success, the current studies did not exploit how to automatically select the information that is valuable to the text in the image mixed with noise.
The noise in the image will not help understand the context and affect the performance of the model.
As shown in Figure 1, more than two-thirds of the original image is masked, and the remaining part can still fully express the semantics of the text. Even if we only keep the area around the entities in the picture (such as ``three people", ``kites"), we can explain all the content in the text. 
Namely, most of the content in the image is not related to the text in some scenarios.                                          
We believe that selecting the necessary parts of the image and reducing the noise of visual information during the training process will help the model to capture image information related to the text. That is to say, a selecting method is essential.

In this paper, we propose a novel attention mechanism using  Gumbel-Sigmoid~\cite{DBLP:conf/acl/GengWWQLT20} to automatically select meaningful information in the image, called Gumebl-Attention. The selected part of the image can help the model understand the context.
Through this method, an image-aware text representation can be obtained, and then we use both image-aware text representation and text representation in the encoder.
In summary, our major contributions are as follows:

\begin{itemize}
\item We propose a novel Gumbel-Attention MMT model. Different from the conventional attention mechanism, our Gumbel-Attention is a selecting method in multi-modal scenarios. To the best of our knowledge, this is the first attempt to perform denoising with sampling method during training among multimodal machine translation models.
\item We also design a loss function to constrain the visual representation, expecting the image-aware text representation to be semantically similar to the text representation. It further ensures that parts irrelevant to the text in the image are removed.
\item Our method achieves or nears the state-of-the-art performance on the three test datasets.
\end{itemize}

\section{Approach}


Our model is based on the structure of Transformer and incorporates visual modal information. 
In this section, we will elaborate our proposed Gumbel-Attention MMT model.
The visual information related to text is selected and integrated into the model by Gumbel-Attention.
\subsection{Text-only Transformer Model}
Given the source language sentence , we firstly use the encoder to get the  representation of . Next, the decoder will generate the target language sentence  based on the representation of source language. 

\textbf{Input.} We convert the input sentence into the combination of the position encoding and word embedding of each word. The word embedding is obtained by random initialization, and position encoding is calculated by sine and cosine functions. The sum  of word embedding and position encoding will be input of the model.

\textbf{Encoder.} The encoder consists of  layers, and each layer contains two sub-layers: multi-head self-attention and position-wise fully connected feed-forward network. We employ residual connection and layer normalization between each layer. The output of the encoder  is contextual representation of the source language, and the dimensions of  are the same as the input  of the encoder.

\textbf{Decoder.} Similarly, the decoder consists of  layers. In addition to the two sub-layers included in the encoder, the decoder has a third sub-layer that performs multi-head attention on the output of the encoder. After obtaining the output  of the decoder, we can generate words through full connection layer and softmax operations. 

In our work, the main improvements are concentrated in the encoder of text-only Transformer, and we will keep the original decoder.


\subsection{Gumbel-Attention}
Compared with Text-only Transformer Model, the main improvement of our model is to propose the Gumbel-Attention mechanism that can denoise image information.  

\subsubsection{Vanilla Multi-head Attention}
Multi-head Attention mainly consists of scaled dot-product attention. , ,  represent the query, key and value, respectively. The output is calculated as:



Besides, the multi-head attention mechanism concatenates a series of basic attention with different parameters to enhance the performance of the model.



where , ,  and  are parameter matrices.

\begin{figure}[t]
\centering\includegraphics[width=0.45\textwidth]{model.pdf}
	\footnotesize\caption{Multi-modal Gumbel-Attention} 
	\label{fig:02}
\end{figure}

\subsubsection{Gumbel-Attention}
In the encoder block of multi-modal machine translation, we propose Gumbel-Attention to reduce the interference of irrelevant information in the image and map image and text to the same semantic space. More specifically, our main purpose is to select the regions in the image that are really helpful for the text semantics, instead of directly using the entire picture as in the previous work. 

\textbf{Softmax and Selecting.} Selecting relevant content in the picture is a question of selecting a few elements in some candidate sets. The usual approach is to normalize them using the softmax function first and then select the candidate elements according to the probability. This is also a common method for classification tasks. However, the selecting of the image is an intermediate step of the entire model in this task, which should be differentiable and can be backpropagated. 

\textbf{Gumbel-Max trick and Gumbel-Softmax.}  To draw samples  from a categorical distribution with class probabilities , \cite{gumbel1954statistical} provides Gumbel-Max method:



where , called Gumbel(0, 1) distribution. This method is equivalent to sampling by probability, but it is still not differentiable. 

~\cite{DBLP:conf/iclr/JangGP17} approximates argmax with softmax and introduces Gumbel-Softmax:



where  is a hyperparameter controlling the distribution tendency of sampling results. When  is smaller(such as 0.1), the sampling result tends to be closer to a real one-hot vector. In contrast, the sampling result will be more similar in each dimension when  becomes larger. 

\textbf{Gumbel-Sigmoid.} ~\cite{DBLP:conf/acl/GengWWQLT20} proposed Gumbel-Sigmoid to select a subset of elements from all elements helping contribute to the meaning of the sentence by paying more attention to content words. The implement of Gumebl-Sigmoid is similar to Gumbel-Softmax by adding Gumbel noise in the sigmoid function:


where the input  is a matrix, and ,  are two independent noises called Gumbel noise. we can obtain differentiable sample by Gumbel-Sigmoid. Similarly, in the local features of the image, a similar method can also be used to select whether each feature is retained.

\begin{figure}[t]
\centering\includegraphics[width=0.45\textwidth]{gumbel-attenion.pdf}
	\footnotesize\caption{A simple description of Gumbel-Attention. In our attention mechanism, only the region associated with the current word in the image is selected to participate in the score matrix calculation.} 
	\label{fig:}
\end{figure}

\textbf{Gumbel-Attention.} To map the vision information into the semantic space of the text, a simple method is using attention mechanism. In our work, we introduce an improved method for attention mechanism using Gumbel-Sigmoid, called Gumbel-Attention. The Gumbel-Attention mechanism can be regarded as a selecting of the part of the image that is related to the text, which is a differentiable, discretely distributed approximate sampling. The selecting for each element in the score matrix in the attention is as follows:

where  is the  th text features,  is the  th image spatial features,  is dimension of model, and ,  is trainable parameter matrices. The score-matrix consists of the sampling results of each word and all regional features of the image. Then, we will get image-aware text representation using score matrix:


 is  image-aware text representation which calculate the weighted sum and only use the image features related to the current word. Figure 3 provides an easy-to-understand explanation of the principle of this novel attention mechanism. Gumbel-Attention in matrix form is as follows:




\begin{table*}[tp]

  \centering
  \begin{threeparttable}
  
  \label{tab:performance_comparison}
    \begin{tabular}{lcccccc}
    \toprule
    \multirow{2}{*}{Model}&
    \multicolumn{2}{c}{Test2016}&\multicolumn{2}{c}{Test2017}&\multicolumn{2}{c}{MSCOCO}\cr
    \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
    &BLEU&METEOR&BLEU&METEOR&BLEU&METEOR\cr
    \midrule
    Text-only Transformer~\cite{DBLP:conf/nips/VaswaniSPUJGKP17}&37.8&55.3&29.1&48.6&25.2&44.1\cr
    Doubly-attention~\cite{DBLP:conf/acl/CalixtoLC17}&36.5&55.0&-&-&-&-\cr
    Fusion-conv~\cite{DBLP:conf/wmt/CaglayanABGBBMH17}&37.0&57.0&29.8&51.2&25.1&46.0\cr
    Trg-mul~\cite{DBLP:conf/wmt/CaglayanABGBBMH17}&37.8&55.7&30.7&{\bf 52.2}&26.4&{\bf 47.4}\cr
    Latent Variable MMT~\cite{DBLP:conf/acl/CalixtoRA19}&37.7&56.0&30.1&49.9&25.5&44.8\cr
    Deliberation networks~\cite{DBLP:conf/acl/IveMS19}&38.0&55.6&-&-&-&-\cr
    Multi-modal Transformer~\cite{DBLP:conf/acl/YaoW20}&38.7&55.7&-&-&-&-\cr
    \midrule
    Gumbel-Attention MMT&{\bf 39.2}&{\bf 57.8}&{\bf 31.4}&{ 51.2}&{\bf 26.9}&{ 46.0}\cr
    \bottomrule
    \end{tabular}
    \end{threeparttable}
    \caption{Experimental results on the Multi30k test set. Best results are highlighted in bold.}
\end{table*}


To enhance the selecting accuracy of Gumbel-Attention, we also use multiple heads to improve ability of Gumbel-Attention to filter image features, just like the attention in vanilla transformer:



Besides, similar to Gumbel-Sigmoid, regions with a probability higher than the threshold are selected as output in the inference stage.





\subsection{Multi-modal Encoder and Gated Fusion}
\textbf{Multi-modal Gated Encoder.} We use two independent transformer encoder to encode text representation and image-aware text representation respectively, and fuse them together after obtaining their respective outputs:

\textbf{Multi-modal Gated Fusion.} To merge the output of the two modals, we introduce a gating mechanism to control the fusion of  and  refering to recent work~\cite{DBLP:conf/iclr/0001C0USLZ20}:


where  and  are trainable parameters. The final output  is directly fed to the vanilla transformer decoder.





\subsection{Multi-modal Similarity Loss}
We propose a loss function to make the text and image expression more similar, which is also a supplement to image denoising:

where  is a hyperparameter that controls the degree of similarity slack\footnote{ should be a number from -1 to 1, 0-0.5 is suggested. In our experiment, we choose 0.3 as the value of margin.}, and we use cosine similarity to measure the similarity of two vector. The loss function of the entire model is as follows:


where ,  represent source language sentence and target language sentence respectively,  is a trainable parameter to control the proportion of , and  is the corpus.





\section{Experiment}





\subsection{Datasets}
Following previous work, we conduct experiments on the widely used  Multil30k~\cite{DBLP:conf/acl/ElliottFSS16} dataset to investigate the effectiveness of our proposed model, which is the largest existing manual labeling dataset for multi-modal machine translation. 
Each picture in the Multil30k dataset is equipped with an English description, and the description is manually translated into German and French. 
The dataset contains 29,000 instances for training, 1,014 instances for validation, and 1,000 instances for testing(Test2016). We also evaluate our model on the WMT17 test set(Test2017) and the MSCOCO test, which contain 1,000 and 461 instances respectively.

\subsection{Setup}
Our model is implemented based on OpenNMT-py toolbox~\cite{DBLP:conf/acl/KleinKDSR17}. Due to the small size of the training corpus, our encoder and decoder only use 4 layers, the number of attention heads is 4, and the input and output dimensions are both 128. We adopt Adam with ,  to optimeze our model.


We used official script\footnote{https://github.com/Multi30k/dataset/tree/master/scripts} of Multi30k to preprocess the data, containing tokenization and the byte pair encoding(BPE) ~\cite{DBLP:conf/acl/SennrichHB16a}. Spatial features are extracted from VGG19 network as the visual representation. The feature dimension is  , which represents the local information in the image. The text representation is the randomly initialized word embedding
. Finally, we evaluate the translation quality using BLEU~\cite{DBLP:conf/acl/PapineniRWZ02} and METEOR~\cite{DBLP:conf/wmt/DenkowskiL11}.







\subsection{Results}

Table 1 shows the results of all method on three test set. Our Gumbel-Attention MMT model is better than most existing models, except for Trg-mul~\cite{DBLP:conf/wmt/CaglayanABGBBMH17} in meteor. One possible reason is that the result of Trg-mul comes from the system on the latest technology WMT2017 test set, which has been selected based on METEOR. An obvious finding is that our model surpasses the text-only Transformer by above 1.5 bleu points. In fact, Gumbel-Attention based method has only a minor modifications on the vanilla Transformer, which proves the effectiveness of our model. Moreover, we draw two important conclusions:



First, compared with Multi-modal Transformer~\cite{DBLP:conf/acl/YaoW20}, the main improvement of our method is the Gumbel-Sigmoid operation in the score matrix and the addition of the loss function. The results of Test2016 show that this method of information selection based on Gumbel-Sigmoid is indeed effective.  


Second, our method is simpler while achieving better results compared with the transformer-based two-stage model Deliberation networks~\cite{DBLP:conf/acl/IveMS19}. This shows that our method is not only effective, but the model structure is also as simple as possible compared to other methods, which is conducive to reproducibility and easy to apply to other tasks.

Overall, our model maintains the best or near best performance on the three test sets. Therefore, we reconfirmed the effectiveness and universality of Gumbel-Attention.




\subsection{Which Layer Applies Gumbel-Attention?}
To explore which layer of the encoder is more suitable for applying Gumbel-Attention, we apply Gumbel-Attention before the 1st to 4th layer respectively. For instance, if Gumbel-Attention is applied on the 3rd layer, in the first two layers we use a transformer encoder to encode image features, and then interact with the text representation through  Gumbel-Attention to get image-aware text representation. Then in the last two layers, we continue to encode the obtained image-aware text representation to get the final output. The comparison results are shown in Figure 4. 


\begin{figure}[t]
\centering\includegraphics[width=0.45\textwidth]{table-where.pdf}
	\footnotesize\caption{The result of applying Gumbel-Attention before different layers, ``after layer 4" means to use Gumbel-Attention to get image-aware text representation after the respective encoding of the image and text.} 
	\label{fig:01}
\end{figure}


The conclusion we got is similar to that of ~\cite{DBLP:conf/acl/GengWWQLT20}: 
applying the Gumbel-Attention before 1st layer achieves the best performance.
This conclusion is intuitive: we need to select and encode valuable visual information as early as possible.
This also shows that the multi-modal Gumbel-Sigmoid selecting method is very similar to the pure text Gumbel-Sigmoid selecting method.



\subsection{Ablation Study}

\begin{table}
\centering
\begin{tabular}{lll}
\hline
  &BLEU  & METEOR \\
\hline
Gumbel-Attention Model       & 39.2  & 57.8     \\
Text-only Transformer       & 37.8  & 55.3     \\
\midrule 
-replace with vanilla-attention       & 38.6  & 55.9      \\
-replace with random images   & 38.3  & 55.6     \\
-shared parameters  & 38.5  & 56.0     \\
-w/o multi-modal gated fusion   & 38.9  & 56.2     \\
-w/o multi-modal similarity loss   & 39.0  & 57.1     \\
\hline
\end{tabular}
\caption{Ablation study of our model on Test2016}
\label{tab:plain}
\end{table}


To investigate the effectiveness of different modules in Gumbel-Attention MMT, we further compare our method with the variants in Table 2.

\textbf{replace with vanilla-attention.} 
In this variant, we replace Gumebl-Attention with vanilla-attention, which performs a weighted sum of similarity between text and image information instead of selecting. This method can also make reasonable use of picture information, so the performance is improved compared with text-only baseline. Additionally, Gumbel-Attention model and vanilla-attention-based model have a gap of more than 1 bleu on Test2016, which demonstrates the influence of image noise on the MMT task.




\textbf{replace with random images.}
With reference to Multi-modal Transformer~\cite{DBLP:conf/acl/YaoW20}, we conducted random images replacement experiments. The result of ~\cite{DBLP:conf/acl/YaoW20} indicates the model with random images performs even worse than the text-only model. However, different from the previous conclusion, the result of row 4 shows that using random images to replace the original images in the Gumbel-Attention model is still better than text-only. This suggests that the random image as a regularization item improves the model effect, similar to the effect of random noise on the model~\cite{DBLP:journals/neco/Bishop95}.



\textbf{shared parameters.}
In this variant, we encode text representation and image-aware text representation with the shared encoder, which can reduce a large number of parameters in the model. Although there is a decline compared with the original model, it is still an improvement over the text-only Transformer. Through the method of sharing parameters, we can obtain the gain brought by the image information with a smaller training cost.

\textbf{w/o multi-modal gated fusion.}
Instead of multi-modal gated fusion, we directly sum the outputs of two independent encoders. The result in row 6 shows the multi-modal features fusion method based on the gate is a more suitable method than direct addition.  



\textbf{w/o multi-modal similarity loss}
We delete the multi-modal similarity loss in the loss function. The results of row 7 show that compared with the original model, the performance does not drop much. This shows that image-aware representation is semantically close to text representation even without additional loss function constraints.


\section{Case Study and Error Analysis}

\begin{figure*}[t]
\centering\includegraphics[width=1\textwidth]{case.pdf}
	\footnotesize\caption{Translation cases of different models, red indicates parts that are seriously inconsistent with semantics, and blue indicates parts that are consistent with semantics} 
	\label{fig:05}
\end{figure*}

In this section, we discuss an error in translation and analyze the advantages of our method over other methods. we display the best translations of the four cases generated by different models(including text-only Transformer, multi-modal Transformer, vanilla-attention-based model and our Gumbel-Attention model) as shown in Figure 5.

Because the word ``vorbeifahren" in the reference translation only appears twice in the training set, and the training set is much maller than other machine translation datasets, it is difficult to translate the accurate meaning. It can be seen from Figure 5 that none of these four models accurately translate all meanings of the source text, including our method. But in terms of the degree of error and the severity of the error, the gap between the four models is very large.

Different from some images in the Multi30k dataset, the objects in the image are very cluttered. The taxi and the boy are in the middle position, and the upper part of the picture is the shops, passing vehicles and pedestrians. Intuitively, the extra objects in the image will bring noise to the model and cause erroneous translation results. 

In the translation results of the multi-modal Transformer model, the word ``sanitärcontainer" appeared, which is not related to the semantic information of text and pictures, and even ``sanitärcontainer" is not in the original Multi30k training vocabulary. The fundamental reason is that ~\cite{DBLP:conf/acl/YaoW20} use the back-translation method to expand the data set, which brings in vocabulary outside the domain. However, if the back-translation method is not used, the result of 
this model is ``ein junge hängt aus dem fenster eines busses und fährt aus dem fenster.", which is farther away from the correct result.

The word ``geschäft" appeared in the results of the vanilla-attention model. This word caused the translation error of the whole sentence. In the text-only machine translation model (such as the text-only transformer in the line 3), it is almost difficult to make this error, because ``geschäft" and the source language text have a large semantic difference. The underlying reason is that the shop in the picture affected the image-aware text representation, which in turn mislead the model.

In the translation of our Gumbel-Attention model, not all words are completely translated. However, due to the noise reduction capability of Gumbel-Attention, compared with other multi-modal translation models, our translation results do not introduce noisy words and contains more information than the translation result of the text-only model.

This case shows that Gumbel-Attention can filter out irrelevant parts of the image and provide visual-modal auxiliary information for the text.


\section{Conclusions and Future work}
In this paper, we have proposed Gumbel-Attention for multi-modal machine translation, which aims to reduce irrelevant noise in images by differentiable method to select the image information. Our experiment results shows that we have achieved state-of-the-art results on Multi30k test set, and it demonstrates the effectiveness of our model. We also introduced multi-modal similarity loss to further restrict image representation and text representation to be more similar. In the ablation study, we proved that even if the original picture is replaced with a random picture, the translation result will be improved due to the effect of regularization. This is different from the previous conclusion ~\cite{DBLP:conf/acl/YaoW20}.   Moreover, we give an example of a translation that does not perform well in all models. The example proves that our method can improve the quality of translation without introducing noise from irrelevant objects in the image.

In future work, we will study the noise filtering of image information in the decoder. We are also very interested in the principle of the association mechanism between these two modals, and look forward to doing some explanatory work to analyze which stage of the image plays a role in multi-modal machine translation.



\clearpage

\begin{thebibliography}{}

\bibitem[\protect\citeauthoryear{Bishop}{1995}]{DBLP:journals/neco/Bishop95}
Christopher~M. Bishop.
\newblock Training with noise is equivalent to tikhonov regularization.
\newblock {\em Neural Comput.}, 7(1):108--116, 1995.

\bibitem[\protect\citeauthoryear{Caglayan \bgroup \em et al.\egroup
  }{2017}]{DBLP:conf/wmt/CaglayanABGBBMH17}
Ozan Caglayan, Walid Aransa, Adrien Bardet, Mercedes
  Garc{\'{\i}}a{-}Mart{\'{\i}}nez, Fethi Bougares, Lo{\"{\i}}c Barrault, Marc
  Masana, Luis Herranz, and Joost van~de Weijer.
\newblock {LIUM-CVC} submissions for {WMT17} multimodal translation task.
\newblock pages 432--439. Association for Computational Linguistics, 2017.

\bibitem[\protect\citeauthoryear{Calixto and
  Liu}{2017}]{DBLP:conf/emnlp/CalixtoL17}
Iacer Calixto and Qun Liu.
\newblock Incorporating global visual features into attention-based neural
  machine translation.
\newblock In {\em Proceedings of the 2017 Conference on Empirical Methods in
  Natural Language Processing}, pages 992--1003. Association for Computational
  Linguistics, 2017.

\bibitem[\protect\citeauthoryear{Calixto \bgroup \em et al.\egroup
  }{2017}]{DBLP:conf/acl/CalixtoLC17}
Iacer Calixto, Qun Liu, and Nick Campbell.
\newblock Doubly-attentive decoder for multi-modal neural machine translation.
\newblock In Regina Barzilay and Min{-}Yen Kan, editors, {\em Proceedings of
  the 55th Annual Meeting of the Association for Computational Linguistics},
  pages 1913--1924. Association for Computational Linguistics, 2017.

\bibitem[\protect\citeauthoryear{Calixto \bgroup \em et al.\egroup
  }{2019}]{DBLP:conf/acl/CalixtoRA19}
Iacer Calixto, Miguel Rios, and Wilker Aziz.
\newblock Latent variable model for multi-modal translation.
\newblock In Anna Korhonen, David~R. Traum, and Llu{\'{\i}}s M{\`{a}}rquez,
  editors, {\em Proceedings of the 57th Conference of the Association for
  Computational Linguistics}, pages 6392--6405. Association for Computational
  Linguistics, 2019.

\bibitem[\protect\citeauthoryear{Chen \bgroup \em et al.\egroup
  }{2019}]{DBLP:conf/ijcai/ChenJF19}
Shizhe Chen, Qin Jin, and Jianlong Fu.
\newblock From words to sentences: {A} progressive learning approach for
  zero-resource machine translation with visual pivots.
\newblock In Sarit Kraus, editor, {\em Proceedings of the Twenty-Eighth
  International Joint Conference on Artificial Intelligence, {IJCAI}}, pages
  4932--4938, 2019.

\bibitem[\protect\citeauthoryear{Denkowski and
  Lavie}{2011}]{DBLP:conf/wmt/DenkowskiL11}
Michael~J. Denkowski and Alon Lavie.
\newblock Meteor 1.3: Automatic metric for reliable optimization and evaluation
  of machine translation systems.
\newblock In Chris Callison{-}Burch, Philipp Koehn, Christof Monz, and Omar
  Zaidan, editors, {\em Proceedings of the Sixth Workshop on Statistical
  Machine Translation}, pages 85--91. Association for Computational
  Linguistics, 2011.

\bibitem[\protect\citeauthoryear{Elliott \bgroup \em et al.\egroup
  }{2016}]{DBLP:conf/acl/ElliottFSS16}
Desmond Elliott, Stella Frank, Khalil Sima'an, and Lucia Specia.
\newblock Multi30k: Multilingual english-german image descriptions.
\newblock In {\em Proceedings of the 5th Workshop on Vision and Language}.
  Association for Computer Linguistics, 2016.

\bibitem[\protect\citeauthoryear{Geng \bgroup \em et al.\egroup
  }{2020}]{DBLP:conf/acl/GengWWQLT20}
Xinwei Geng, Longyue Wang, Xing Wang, Bing Qin, Ting Liu, and Zhaopeng Tu.
\newblock How does selective mechanism improve self-attention networks?
\newblock In {\em Proceedings of the 58th Annual Meeting of the Association for
  Computational Linguistics}, pages 2986--2995. Association for Computational
  Linguistics, 2020.

\bibitem[\protect\citeauthoryear{Gumbel}{1954}]{gumbel1954statistical}
Emil~Julius Gumbel.
\newblock Statistical theory of extreme values and some practical applications.
\newblock {\em NBS Applied Mathematics Series}, 33, 1954.

\bibitem[\protect\citeauthoryear{Huang \bgroup \em et al.\egroup
  }{2016}]{DBLP:conf/wmt/HuangLSOD16}
Po{-}Yao Huang, Frederick Liu, Sz{-}Rung Shiang, Jean Oh, and Chris Dyer.
\newblock Attention-based multimodal neural machine translation.
\newblock In {\em Proceedings of the First Conference on Machine Translation},
  pages 639--645. Association for Computer Linguistics, 2016.

\bibitem[\protect\citeauthoryear{Ive \bgroup \em et al.\egroup
  }{2019}]{DBLP:conf/acl/IveMS19}
Julia Ive, Pranava Madhyastha, and Lucia Specia.
\newblock Distilling translations with visual awareness.
\newblock In {\em Proceedings of the 57th Conference of the Association for
  Computational Linguistics}, pages 6525--6538. Association for Computational
  Linguistics, 2019.

\bibitem[\protect\citeauthoryear{Jang \bgroup \em et al.\egroup
  }{2017}]{DBLP:conf/iclr/JangGP17}
Eric Jang, Shixiang Gu, and Ben Poole.
\newblock Categorical reparameterization with gumbel-softmax.
\newblock In {\em 5th International Conference on Learning Representations},
  2017.

\bibitem[\protect\citeauthoryear{Klein \bgroup \em et al.\egroup
  }{2017}]{DBLP:conf/acl/KleinKDSR17}
Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander~M. Rush.
\newblock Opennmt: Open-source toolkit for neural machine translation.
\newblock In {\em Proceedings of the 55th Annual Meeting of the Association for
  Computational Linguistics}, pages 67--72. Association for Computational
  Linguistics, 2017.

\bibitem[\protect\citeauthoryear{Papineni \bgroup \em et al.\egroup
  }{2002}]{DBLP:conf/acl/PapineniRWZ02}
Kishore Papineni, Salim Roukos, Todd Ward, and Wei{-}Jing Zhu.
\newblock Bleu: a method for automatic evaluation of machine translation.
\newblock In {\em Proceedings of the 40th Annual Meeting of the Association for
  Computational Linguistics}, pages 311--318. Association for Computational
  Linguistics, 2002.

\bibitem[\protect\citeauthoryear{Sennrich \bgroup \em et al.\egroup
  }{2016}]{DBLP:conf/acl/SennrichHB16a}
Rico Sennrich, Barry Haddow, and Alexandra Birch.
\newblock Neural machine translation of rare words with subword units.
\newblock In {\em Proceedings of the 54th Annual Meeting of the Association for
  Computational Linguistics}. Association for Computer Linguistics, 2016.

\bibitem[\protect\citeauthoryear{Vaswani \bgroup \em et al.\egroup
  }{2017}]{DBLP:conf/nips/VaswaniSPUJGKP17}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5998--6008, 2017.

\bibitem[\protect\citeauthoryear{Yao and Wan}{2020}]{DBLP:conf/acl/YaoW20}
Shaowei Yao and Xiaojun Wan.
\newblock Multimodal transformer for multimodal machine translation.
\newblock In {\em Proceedings of the 58th Annual Meeting of the Association for
  Computational Linguistics}, pages 4346--4350. Association for Computational
  Linguistics, 2020.

\bibitem[\protect\citeauthoryear{Zhang \bgroup \em et al.\egroup
  }{2020}]{DBLP:conf/iclr/0001C0USLZ20}
Zhuosheng Zhang, Kehai Chen, Rui Wang, Masao Utiyama, Eiichiro Sumita, Zuchao
  Li, and Hai Zhao.
\newblock Neural machine translation with universal visual representation.
\newblock In {\em 8th International Conference on Learning Representations}.
  OpenReview.net, 2020.

\bibitem[\protect\citeauthoryear{Zhou \bgroup \em et al.\egroup
  }{2018}]{DBLP:conf/emnlp/ZhouCLY18}
Mingyang Zhou, Runxiang Cheng, Yong~Jae Lee, and Zhou Yu.
\newblock A visual attention grounding neural model for multimodal machine
  translation.
\newblock In {\em Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing, Brussels}, pages 3643--3653. Association for
  Computational Linguistics, 2018.

\end{thebibliography}
 \bibliographystyle{named}
\bibliography{ijcai21}

\end{document}
