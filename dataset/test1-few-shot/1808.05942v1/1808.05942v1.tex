

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{3dv}
\usepackage{times}
\usepackage{array}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath,amssymb} \usepackage{color}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{floatrow}
\usepackage[resetlabels]{multibib}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\usepackage{paralist}
\usepackage{wrapfig,lipsum,booktabs}
\usepackage{makecell, pict2e}
\usepackage{pdfpages}

\newcommand{\LSP}{LSP}
\newcommand{\LSPextended}{LSP-extended}
\newcommand{\MPII}{MPII-HumanPose}
\newcommand{\HumanEva}{HumanEva}
\newcommand{\HumanM}{Human3.6M}
\newcommand{\FP}{FashionPose}
\newcommand{\SMPLPose}{UP}
\newcommand{\smpl}[0]{M}
\newcommand{\posefun}[0]{T}
\newcommand{\blendfun}[0]{W}
\newcommand{\offsetfun}[0]{B}
\newcommand{\jointfun}[0]{J}
\newcommand{\jointtransfun}[0]{G}

\newcommand{\pose}[0]{\boldsymbol{\theta}}
\newcommand{\shape}[0]{\boldsymbol{\beta}}
\newcommand{\trans}[0]{\vec{t}}
\newcommand{\joints}[0]{\mathbf{J}}
\newcommand{\blendweight}[0]{w}
\newcommand{\blendweights}[0]{\mathbf{W}}
\newcommand{\template}[0]{\mathbf{T}}
\newcommand{\templatepersonal}[0]{\mathbf{T}_P}
\newcommand{\shapefixed}[0]{\boldsymbol{\offsetfun_\beta}}
\newcommand{\jointsshapefixed}[0]{\boldsymbol{\jointfun_\beta}}
\newcommand{\templatetpose}[0]{\mathbf{T}_\mu}
\newcommand{\personalshape}[0]{\boldsymbol{P}}

\newcommand{\errjoints}[0]{err_{joints3D}}
\newcommand{\pckh}[0]{PCKh}
\newcommand{\errmesh}[0]{err_{mesh}}
\newcommand{\errquat}[0]{err_{quat}}

\newcommand{\losslat}[0]{L_{lat}}
\newcommand{\lossthreed}[0]{L_{3D}}
\newcommand{\losstwod}[0]{L_{2D}}

\newcommand{\MO}[1]{{\textcolor{magenta}{[\textbf{MO:} #1]}}}
\newcommand{\GPM}[1]{{\textcolor{blue}{[\textbf{GPM:} #1]}}}
\newcommand{\CL}[1]{{\textcolor{green}{[\textbf{CL:} #1]}}}
\newcommand{\PG}[1]{{\textcolor{red}{[\textbf{PG:} #1]}}}
\newcommand{\bernt}[1]{{\textcolor[rgb]{0,0.5,0}{[\textbf{BS:} #1]}}}

\newcolumntype{x}[1]{>{\centering\arraybackslash\hspace{-9pt}}p{#1}}
\newcolumntype{y}[1]{>{\centering\arraybackslash\hspace{-6pt}}p{#1}}
\newfloatcommand{capbtabbox}{table}[][\FBwidth]

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ifthreedvfinal\pagestyle{empty}\fi


\newcites{apndx}{References}

\begin{document}	
\title{Neural Body Fitting: Unifying Deep Learning and Model-Based Human Pose and Shape Estimation}

\author{Mohamed Omran\\
\and Christoph Lassner\footnotemark[1]\\
\and Gerard Pons-Moll\\
\and Peter V. Gehler\footnotemark[1]\\
\and Bernt Schiele\\Max Planck Institute for Informatics, Saarland Informatics Campus, Saarbr\"ucken, Germany\\
Amazon, T\"ubingen, Germany\\
{\tt\small \{mohomran, gpons, schiele\}@mpi-inf.mpg.de}, {\tt\small \{classner, pgehler\}@amazon.com}\thanks{This work was done while Christoph Lassner and Peter V. Gehler were with the MPI for Intelligent Systems and the University of T\"ubingen, and additionally while Peter was with the University of W\"urzburg.}\\
}

\maketitle

\begin{figure*}[t]
\caption{Given a single 2D image of a person, we predict a semantic body part segmentation. This part segmentation is represented as a color-coded map and used to predict the parameters of a 3D body model.}
\includegraphics[width=\textwidth]{figures/teaser_prepress.pdf}
\end{figure*}

\begin{abstract}
Direct prediction of 3D body pose and shape remains a challenge even for
highly parameterized deep learning models. Mapping from the 2D image space to 
the prediction space is difficult: perspective ambiguities make the loss
function noisy and training data is scarce. In this paper, we propose a novel 
approach (\emph{Neural Body Fitting} (NBF)).
It integrates a statistical body model within a CNN, leveraging reliable 
bottom-up semantic body part segmentation and robust top-down body model constraints. 
NBF is fully differentiable and can be trained using 2D and 3D annotations. 
In detailed experiments, we analyze how the components of our model affect performance, especially
the use of part segmentations as an explicit intermediate representation, and present a robust, efficiently trainable 
framework for 3D human pose estimation from 2D images with competitive results on standard benchmarks. 
Code will be made available at 
\url{http://github.com/mohomran/neural_body_fitting}
\end{abstract}


\vspace{-1em}

\section{Introduction}

Much research effort has been successfully directed towards predicting 
3D keypoints and stick-figure representations from images of people. 
Here, we consider the more challenging problem of estimating the parameters 
of a detailed statistical human body model from a single image. 

We tackle this problem by incorporating a model of the human body into 
a deep learning architecture, which has several advantages. First, the model 
incorporates limb orientations and shape, which are required for many 
applications such as character animation, biomechanics and virtual 
reality. Second, anthropomorphic constraints are automatically satisfied -- for example 
limb proportions and symmetry. Third, the 3D model output is one step closer to 
a faithful 3D reconstruction of people in images. 

Traditional \emph{model-based} approaches typically optimize an objective function that 
measures how well the model fits the image observations -- for example, 2D 
keypoints~\cite{bogo_smpl_eccv16,Lassner:UP:2017}. These methods do not require 
paired 3D training data (images with 3D pose), but only work well when 
initialized close to the solution. By contrast, initialization is not required 
in forward prediction models, such as CNNs that directly predict 3D keypoints.
However many  images with 3D pose annotations are required, which are difficult to 
obtain, unlike images with 2D pose annotations. 

Therefore, like us, a few recent works have proposed hybrid CNN architectures 
that are trained using model-based loss functions~\cite{tan2017indirect,tung2017self,hmrKanazawa17,pavlakos2018humanshape}.
Specifically, from an image, a CNN predicts the parameters of the SMPL 3D 
body model~\cite{smpl2015loper}, and the model is re-projected onto the 
image to evaluate the loss function in 2D space. Consequently, 2D pose 
annotations can be used to train such architectures. While these hybrid approaches 
share similarities, they all differ in essential design choices, such as the amount 
of 3D vs 2D annotations for supervision and the input representation used to lift to 3D.

To analyze the importance of such components, we introduce Neural Body Fitting
(NBF), a framework designed to provide fine-grained control over all
parts of the body fitting process. NBF is a hybrid architecture that
integrates a statistical body model within a CNN. From an RGB image
or a semantic segmentation of the image, NBF directly predicts the parameters of
the model; those parameters are passed to SMPL to produce a 3D mesh; the joints
of the 3D mesh are then projected to the image closing the loop. Hence, NBF
admits both full 3D supervision (in the model or 3D Euclidean space) and weak 2D
supervision (if images with only 2D annotations are available). NBF combines the advantages 
of direct bottom-up methods and top-down methods. It requires neither 
initialization nor large amounts of 3D training data.

One key question we address with our study is whether to use an intermediate 
representation rather than directly lifting to 3D from the raw RGB image.
Images of humans can vary due to factors such as illumination, clothing,
and background clutter. Those effects do not necessarily correlate with pose and shape, 
thus we investigate whether a simplification of the RGB image into a
semantic segmentation of body parts improves 3D inference. We also consider
the granularity of the body part segmentation as well as segmentation quality,
and find that: \begin{inparaenum}[(i)]
       \item a color-coded 12-body-part segmentation contains sufficient information for predicting shape and pose,
       \item the use of such an intermediate representation results in competitive performance and easier, more 
       data-efficient training compared to similar methods that predict pose and shape parameters from raw RGB images,
       \item segmentation quality is a strong predictor of fit quality.
     \end{inparaenum}

We also demonstrate that only a small fraction of the training data needs to be paired with 3D annotations. 
We make use of the recent UP-3D dataset~\cite{Lassner:UP:2017} that contains 8000 training images in the wild 
along with 3D pose annotations. Larger 2D datasets exist, but UP-3D allows us to perform a controlled study.

In summary, our contribution is twofold: first we introduce NBF, which unites
deep learning-based with traditional model-based methods taking advantage of
both. Second, we provide an in-depth analysis of the necessary components to
achieve good performance in hybrid architectures and provide insights for its
real-world applicability that we believe may hold for many related methods as
well.

 \section{Related Work}
Human pose estimation is a well-researched field and we focus on 3D methods; for a recent extensive
survey on the field we refer to~\cite{sarafianos_posesurvey_cviu2016}.\\
\noindent {\bf Model-based Methods.} To estimate human pose and shape from images,
model-based~\cite{PonsModelBased} works use a parametric body model or template.
Early models were based on geometric
primitives~\cite{metaxas1993shape,gavrila1996,plankers2001articulated,sigal2004tracking,stoll2011fast},
while more recent 
ones are estimated from 1000s of scans of real people, and are typically parameterized by separate body pose and
shape components~\cite{anguelov2005scape,hasler2009statistical,smpl2015loper,zuffi2015stitched,pons2015dyna}, with few exceptions, \eg, \cite{joo2018total}.
Most model-based approaches fit a model to image evidence through complex non-linear optimization, requiring careful initialization to avoid poor local minima.

To reduce the complexity of the fitting procedure, the output of
2D keypoint detection has been used as additional guidance.
The progress in 2D pose
estimation using CNNs~\cite{CPM,cao2016realtime,insafutdinov17cvpr} has contributed
significantly to the task of 3D pose estimation even in challenging in-the-wild
scenarios.
For example, the 3D parameters of SMPL~\cite{smpl2015loper} can be obtained with
reasonable accuracy by fitting it to 2D
keypoints~\cite{bogo_smpl_eccv16,Lassner:UP:2017}. However, lifting to 3D from 2D
information alone is an ambiguous problem. Adversarial learning can potentially identify plausible poses~\cite{jack2017adversarially,hmrKanazawa17}.
By contrast, the seminal works of~\cite{taylor_articulated_cvpr00,sminchisescu2003kinematic} address lifting by reasoning about kinematic depth ambiguities. 
Recently, using monocular video and geometric reasoning, accurate and detailed 3D shape, including clothing is obtained~\cite{thiemo2018, thiemo2018_3DV}. 

\noindent {\bf Learning-Based Models.} Recent methods in this category typically
predict 3D keypoints or stick figures from a single image using a CNN.
High capacity models are trained on standard 3D
datasets~\cite{ionescu_human36_pami14,sigal_humaneva_ijcv10}, which are limited
in terms of appearance variation, pose, backgrounds and occlusions.
Consequently, it is not clear -- despite excellent performance on standard
benchmarks --
how methods~\cite{tekin_structured_bmvc16,li_accv14,pavlakos_volumetric_cvpr17,li_maximum_iccv2015}
generalize to in-the-wild images. To add variation, some methods resort to generating synthetic 
images ~\cite{rogez_mocap_nips16,varol17,Lassner:GP:2017} but it is complex to approximate 
fully realistic images with sufficient variance.
Similar to model-based methods, learning approaches have benefited from the
advent of robust 2D pose methods -- by matching 2D detections to a 3D pose
database~\cite{chen_2d_match_cvpr17,yasin_dual_source_cvpr16}, by regressing
pose from 2D joint distance matrices~\cite{moreno_distance_matrix_cvpr17}, by
exploiting pose and geometric priors for
lifting~\cite{zhou_sparseness_deepness_cvpr15,akhter_pose_conditioned_cvpr15,simo_joint_CVPR2013,Jahangiri2017,mehta2017single,zhou2018monocap,rogez2018lcr};
or simply by training a feed forward network to directly predict 3D pose from 2D
joints~\cite{martinez20173dbaseline}. Another way to exploit images with only 2D
data is by re-using the first layers of a 2D pose CNN for the task of 3D pose
estimation~\cite{tekin_fusion_arxiv16,VNect_SIGGRAPH2017,mehta_mono_3dv17}. Pavlakos et al.~\cite{pavlakos2018ordinal} take another approach by relying on weak 3D supervision in form of a relative 3D ordering of joints, similar to the previously proposed PoseBits~\cite{posebits_cvpr14}.

Closer to ours are approaches that train using separate 2D and 3D
losses~\cite{popa2017deep,zhou2017towards,sun2017compositional}. 
However, since they do not integrate a statistical
body model in the network, limbs and body proportions might be
unnatural. Most importantly, they only predict 3D stick
figures as opposed to a full mesh.

Some works regress correspondences to a body model which are then used to fit the model to depth data~\cite{pons2015metric,taylor2012vitruvian}. 
Recently, correspondences to the SMPL body surface are regressed from images directly~\cite{guler2018densepose} by leveraging dense keypoint annotations; 
however, the approach can not recover 3D human pose and shape. \cite{varol2018bodynet} fits SMPL to CNN volumetric outputs as a post-process step.
3D model fitting within a CNN have been proposed for faces~\cite{tewari2017mofa}; faces however, are not articulated like bodies which simplifies the regression problem.

A few recent works (concurrent to ours) integrate the SMPL~\cite{smpl2015loper} model within a network~\cite{tung2017self,hmrKanazawa17,pavlakos2018humanshape}. 
The approaches differ primarily in the proxy representation used to lift to 3D: 
RGB images~\cite{hmrKanazawa17}, images and 2D keypoints~\cite{tung2017self} and 2D keypoints and silhouettes~\cite{pavlakos2018humanshape}, 
and the kind of supervision (3D vs 2D) used for training. 
In contrast to previous work, we analyze the importance of such components for good performance. Kanazawa et al.~\cite{hmrKanazawa17} also integrate a learned prior on the space of poses.
We draw inspiration from model-based and learning approaches in several aspects of our model design. 
Firstly, NBF addresses an important limitation: it does not require an initialization for optimization because it incorporates a CNN based bottom-up component. 
Furthermore, at test time, NBF predictions are fast and do not require optimization. 
By integrating SMPL directly into the CNN, we do not require multiple network heads to backpropagate 2D and 3D losses. 
Lastly, we use a semantic segmentation as proxy representation, which (1) abstracts away irrelevant image information for 3D pose, (2) is a richer semantic representation than keypoints or silhouettes, and (3) allows us to analyze the importance of part granularity and placement for 3D prediction.


 \section{Method}

\begin{figure*}

  \includegraphics[width=\textwidth]{figures/nugget/nugget}
  \vspace*{-.8cm}
  \caption{\textit{Summary of our proposed pipeline.} We process the image with a standard semantic segmentation CNN into 12 semantic parts (see Sec.~\ref{sec:impl-details}). An encoding CNN processes the semantic part probability maps to predict SMPL body model parameters (see Sec.~\ref{sec:neural-body-fitting-params}). We then use our SMPL implementation in Tensorflow to obtain a projection of the pose-defining points to 2D. With these points, a loss on 2D vertex positions can be back propagated through the entire model (see Sec.~\ref{sec:loss-functions}).}
  \label{fig:summary}

\end{figure*}

Our goal is to fit a 3D mesh of a human to a single static image (see Figure ~\ref{fig:summary}). This task involves multiple steps and we want to apply 3D but also 2D losses due to the strongly varying difficulty to obtain ground truth data. Nevertheless, we aim to build a simple processing pipeline with parts that can be optimized in isolation and avoiding multiple network heads. This reduces the number of hyperparameters and interactions, \eg, loss weights, and allows us to consecutively train the model parts. There are two main stages in the proposed architecture: in a first stage, a body part segmentation is predicted from the RGB image. The second stage takes this segmentation to predict a low-dimensional parameterization of a body mesh.\\

\vspace{-1em}

\subsection{Body Model}
\label{subsec:body_model}

For our experiments we use the SMPL body model due to its good trade-off between high anatomic flexibility and realism. SMPL parameterizes a triangulated mesh with  vertices with pose parameters  and shape parameters  -- optionally the translation parameters  can be taken into account as well.
Shape  and pose dependent deformations  are first applied to a base template ; then the mesh is posed by rotating each body part around skeleton joints  using a skinning function :


where  is the SMPL function, and  outputs an intermediate mesh in a T-pose after pose and shape deformations are applied. SMPL produces realistic results using relatively simple mathematical operations -- most importantly for us SMPL is fully differentiable with respect to pose  and shape . All these operations, including the ones to determine projected points of a posed and parameterized 3D body, are available in Tensorflow. We use them to make the 3D body a part of our deep learning model.

\subsection{Neural Body Fitting Parameterization}\label{sec:neural-body-fitting-params}

NBF predicts the parameters of the body model from a colour-coded part segmentation map  using a CNN-based predictor parameterized by weights ~. The estimators for pose and shape are thus given by  and  respectively. 

We integrate the SMPL model and a simple 2D projection layer into our CNN estimator, as described in Sec.~\ref{subsec:body_model}. 
This allows us to output a 3D mesh, 3D skeleton joint locations or 2D joints, depending on the kind of supervision we want to apply for training while keeping the CNN monolithic.

Mathematically, the function  that maps from semantic images to meshes is given by

which is the SMPL Equation~\eqref{eq:smpl} parameterized by network parameters . NBF can also predict the 3D joints , because they are a function of the model parameters. Furthermore, using a projection operation  we can project the 3D joints onto the image plane

where  is the NBF function that outputs 2D joint locations. All of these operations are differentiable and allow us to use gradient-based optimization to update model parameters with a suitable loss function.

\subsection{Loss Functions}\label{sec:loss-functions}

We experiment with the following loss functions:\\
{\em 3D latent parameter loss: } This is an L1 loss on the model parameters  and . Given a paired dataset , the loss is given by:

where  are the vectorized rotation matrices of the 24 parts of the body. Similar to~\cite{Lassner:UP:2017, pavlakos2018humanshape}, we observed better performance by imposing the loss on the rotation matrix representation of  rather than on its `native' axis angle encoding as defined in SMPL. This requires us to project the predicted matrices to the manifold of rotation matrices. We perform this step using SVD to maintain differentiability.
\\
{\em 3D joint loss: }  Given a paired dataset with skeleton annotations  we compute the loss in terms of 3D joint position differences as:

{\em 2D joint loss:} If the dataset  provides solely 2D joint position ground truth, we define a similar loss in terms of 2D distance and rely on error backpropagation through the projection:

{\em Joint 2D and 3D loss: }
To maximize the amounts of usable training data, ideally multiple data sources can be combined with a subset of the data  providing 3D annotations and another subset  providing 2D annotations. We can trivially integrate all the data with different kinds of supervision by falling back to the relevant losses and setting them to zero if not applicable.

 In our experiments, we analyze the performance of each loss and their combinations. In particular, we evaluate how much gain in 3D estimation accuracy can be obtained from weak 2D annotations which are much cheaper to obtain than accurate 3D annotations. 

 \section{Results}
\subsection{Evaluation Settings}

We used the following three datasets for evaluation: UP-3D,
\cite{Lassner:UP:2017},  HumanEva-I \cite{sigal_humaneva_ijcv10} 
and Human3.6M\cite{ionescu_human36_pami14}.
We perform a detailed analysis of our approach on UP-3D and Human3.6M, 
and compare against state-of-the-art methods on HumanEVA-I and Human3.6M.

UP-3D, is a challenging, in-the-wild dataset that draws from existing pose
datasets: LSP \cite{johnson_lsp_bmvc10}, LSP-extended \cite{johnson_lsp_bmvc10},
MPII HumanPose \cite{andriluka_mpii2d_cvpr14}, and FashionPose \cite{dantone14fashionpose}. It
augments images from these datasets with rich 3D annotations in the form of SMPL
model parameters that fully capture shape and pose, allowing us to derive 2D
and 3D joint as well as fine-grained segmentation annotations. The dataset consists of training 
(5703 images), validation (1423 images) and test (1389 images) sets. 
For our analysis, we use the training set and provide results on the validation set.

The HumanEVA-I dataset is recorded in a controlled environment with marker-based ground truth synchronized with video.
The dataset includes 3 subjects and 2 motion sequences per subject. Human3.6M also consists of similarly recorded data
but covers more subjects, with 15 action sequences per subject repeated over two trials. For our analysis
on this dataset, we reserve subjects S1, S5, S6 and S7 for training, holding out subject S8 for validation. 
We compare to the state of art on the test sequences S9 and S11.

\subsection{Implementation Details}\label{sec:impl-details}

\textbf{Data preparation:} To train our model, we require images paired with 3D body model fits 
(i.e. SMPL parameters) as well as pixelwise part labels. The UP-3D dataset provides such annotations, 
while Human3.6M does not. However, by applying MoSH \cite{Loper:SIGASIA:2014} 
to the 3D MoCap marker data provided by the latter we obtain the corresponding SMPL parameters,
which in turn allows us to generate part labels by rendering an appropriately annotated SMPL mesh \cite{Lassner:UP:2017}. 

\textbf{Scale ambiguity:} The SMPL shape parameters encode among other factors a person's size. 
Additionally, both distance to the camera and focal length determine how large a person appears in an image.
To eliminate this ambiguity during training, we constrain scale information to the shape parameters by making the following 
assumptions: The camera is always at the SMPL coordinate origin, the optical axis always points in the same direction, and a person
is always at a fixed distance from the camera. We render the ground truth SMPL fits and scale the training images to fit the renderings (using the corresponding 2D joints). 
This guarantees that the the only factor affecting person size in the image are the SMPL shape parameters. At test-time, 
we estimate person height and location in the image using 2D DeeperCut keypoints~\cite{insafutdinov_deepercut_eccv16},
and center the person within a 512x512 crop such that they have a height of 440px, which roughly corresponds to the setting seen during training.

\textbf{Architecture:} We use a two-stage approach: The first stage receives the 512x512 input crop and produces a part
segmentation. We use a RefineNet~\cite{lin17refinenet} model (based on ResNet-101 \cite{he_resnet_cvpr2016}). 
This part segmentation is color-coded, resized to 224x224 and fed as an RGB image to the second stage, 
itself composed of two parts: a regression network (ResNet-50) that outputs the 226 SMPL parameters (shape and pose), 
and a non-trainable set of layers that implement the SMPL model and an image projection. Such layers can produce a 3D mesh, 3D joints or 2D joints
given the predicted pose and shape. Training both stages requires a total of 18 (12+6) hours on a single Volta V100 GPU.
More details are provided in the supplementary material as well as in the code (to be released). 

\subsection{Analysis}

\paragraph{\textbf{Which Input Encoding?}}

We investigate here what input representation is effective for pose and shape prediction. 
Full RGB images certainly contain more information than for example silhouettes, part segmentations or 2D joints. 
However, some information may not be relevant for 3D inference, such as appearance, illumination or clothing, 
which might make the network overfit to nuisances factors 

To this end, we train a network on different image representations and compare their performance on the UP-3D and
Human3.6M validation sets. We compare RGB images, color-coded part segmentations of 
varying granularities, and color-coded joint heatmaps (see supplementary material for examples). 
We generate both using the ground truth SMPL annotations to establish an upper bound
on performance, and later consider the case where we do not have access to such
information at test time.

\begin{table}
\caption{\textit{Input Type vs. 3D error in millimeters}}\label{tab:inptype}
\begin{tabular}{l||x{1.5cm}x{1.5cm}}\toprule  
  \emph{type of input}          & \emph{UP}      & \emph{H36M}        \tabularnewline   
  \hline
  \hline
    RGB                         & 98.5           & 48.9                 \\\midrule
    Segmentation (1 part)       & 95.5           & 43.0                 \\
    Segmentation (3 parts)      & 36.5           & 37.5                 \\
    Segmentation (6 parts)      & 29.4           & 36.2                 \\
    Segmentation (12 parts)     & 27.8           & 33.5                 \\
    Segmentation (24 parts)     & 28.8           & 31.8                 \\\midrule
    Joints (14)                 & 28.8           & 33.4                 \\
    Joints (24)                 & 27.7           & 33.4                 \\
\end{tabular}
\end{table}

The results are reported in Table~\ref{tab:inptype}. We observe that explicit part representations (part segmentations or joint heatmaps) are more useful for
3D shape/pose estimation compared to RGB images and plain silhouettes. The difference is especially pronounced on the UP-3D dataset, which contains more visual 
variety than the images of Human3.6M, with an error drop from 98.5 mm to 27.8 mm when using a 12 part segmentation. 
This demonstrates that a 2D segmentation of the person into sufficient parts carries a lot of information about 3D pose/shape,
while also providing full spatial coverage of the person (compared to joint heatmaps).
Is it then worth learning separate mappings first from image to part segmentation, and then 
from part segmentation to 3D shape/pose? To answer this question we first need to examine how 3D accuracy is affected by the quality of real predicted part segmentations.

\textbf{Which Input Quality?} To determine the effect of segmentation quality on the results, we train three different \emph{part segmentation networks}.
Besides RefineNet, we also train two variants of DeepLab~\cite{chen2016deeplab}, based on VGG-16~\cite{simonyan2014very} and ResNet-101~\cite{he_resnet_cvpr2016}. 
These networks result in IoU scores of , , and  
respectively on the UP validation set. Given these results, we then train four \emph{3D prediction networks} - one for each of the part segmentation networks, and 
an additional one using the ground truth segmentations. We report 3D accuracy on the validation set of UP3D for each of the four 3D networks, diagonal numbers of Table~\ref{tab:Effect-of-training-set}.
As one would expect, the better the segmentation, the better the 3D prediction accuracy. 
As can also be seen in Table~\ref{tab:Effect-of-training-set}, better segmenters at test time always lead to improved 3D accuracy, even when the 3D prediction networks are trained with poorer segmenters.  
This is perhaps surprising, and it indicates that mimicking the statistics of a particular segmentation method at training time plays only a minor role (for example a network trained with GT segmentations and tested using RefineNet segmentations performs comparably to a network that is trained using RefineNet segmentations (83.3mm vs 82mm)). 
To further analyze the correlation between segmentation quality and 3D accuracy, in Figure~\ref{fitsegm} we plot the relationship 
between F-1 score and 3D reconstruction error. Each dot represents one image, and the color its respective difficulty -- we use the distance to mean pose as a proxy measure for difficulty.
The plot clearly shows that the higher the F-1 score, the lower the 3D joint error.

\begin{figure}
  \includegraphics[width=1.0\textwidth]{figures/fit_vs_seg_quality.pdf}
  
  \vspace{-1.5em}
  \caption{\textit{Segmentation quality (F1-score) vs. fit quality (3D joint error).} The darkness indicates the difficulty of the pose, i.e. the distance from the upright pose with arms by the sides. 
  \label{fitsegm}}\end{figure}

\begin{table}
  {\setlength{\extrarowheight}{1pt}
  \begin{tabular}{c|cccc} \toprule
  \diaghead{\hskip5.2em}{Val}{Train} & VGG & ResNet & RefineNet & GT \tabularnewline
  \hline
  \hline
  VGG         &           &           &           &              \tabularnewline
  ResNet      &           &           &           &              \tabularnewline
  RefineNet      &           &           &           &              \tabularnewline
  GT             &           &           &           &              \tabularnewline
  \end{tabular}}
  \caption{\label{tab:Effect-of-training-set}\textit{Effect of segmentation quality on the quality of the 3D fit prediction modules ()}}\vspace{-0.5em}
\end{table}

\textbf{Which Types of Supervision?} We now examine different combinations of loss terms.
The losses we consider are  (on the latent parameters),  (on 3D joint/vertex locations), 
 (on the projected joint/vertex locations).
We compare performance using three different error measures: 
     \begin{inparaenum}[(i)]
       \item , the Euclidean distance between ground truth and predicted SMPL joints (in mm).
       \item  \cite{andriluka_mpii2d_cvpr14}, the percentage of correct keypoints with the error threshold being  of head size, which we measure on a per-example basis.
       \item , quaternion distance error of the predicted joint rotations (in radians).
     \end{inparaenum}

Given sufficient data - the full 3D-annotated UTP training set with mirrored examples (11406) - only applying a loss on the 
model parameters yields reasonable results, and in this setting, additional loss terms don't provide benefits. \\
When only training with , we obtain similar results in terms of , however, interestingly 
is significantly higher. This indicates that predictions produce accurate 3D joints positions in space, but the limb orientations are incorrect. 
This further demonstrates that methods trained to produce only 3D keypoints do not capture orientation, which is needed for many applications.

We also observe that only training with the 2D reprojection loss (perhaps unsurprisingly) 
results in poor performance in terms of 3D error, showing that some amount of 3D annotations are necessary to overcome 
the ambiguity inherent to 2D keypoints as a source of supervision for 3D. 

Due to the SMPL layers, we can supervise learning with any number of joints/mesh vertices. We thus experimented with the 91 landmarks used by~\cite{Lassner:UP:2017} for their fitting method but find that the 24 SMPL joints are sufficient in this setting.
\begin{table}
  \caption{\textit{Loss ablation study.} Results in 2D and 3D error metrics (\textit{joints3D}: Euclidean 3D distance, \textit{mesh}: average vertex to vertex distance, \textit{quat}: average body part rotation error in radians).}\label{wrap-tab:1}
\begin{center}
\small
\begin{tabular*}{1.0\textwidth}{l||x{1.9cm}x{1.05cm}x{1.2cm}}\toprule  
\emph{Loss}                 &    &      &   \\\midrule
                   & 83.7                    & 93.1                 & 0.278                 \\
          & 82.3                    & 93.4                 & 0.280                 \\  
          & 83.1                    & 93.5                 & 0.278                 \\
 & 82.0                    & 93.5                 & 0.279                 \\
                    & 83.7                    & 93.5                 & 1.962                 \\
                    & 198.0                   & 94.0                 & 1.971                 \\
\end{tabular*}
\end{center}
\end{table}

\vspace{-0.2em}
\textbf{How Much 3D Supervision Do We Need?} The use of these additional loss terms also allows us to leverage data for which 
no 3D annotations are available. With the following set of experiments,
we attempt to answer two questions: (i) Given a small amount of 3D-annotated 
data, does extra 2D-annotated data help?, (ii) What amount of 3D data is necessary? 
To this end we train multiple networks, each time progressively disabling the 3D latent 
loss and replacing it with the 2D loss for more training examples. The results are depicted in Table \ref{tab:more2d}. 
We find that performance barely degrades as long as we have a small amount of 3D annotations. 
In contrast, using small amounts of 3D data and no extra data with 2D annotations yields poor performance. 
This is an important finding since obtaining 3D annotations is difficult compared to simple 2D keypoint annotations.
\begin{center}
\begin{table}
  \caption{\textit{Effect of 3D labeled data.} We show the 3D as well as the estimated body part rotation error for varying ratios of data with 3D labels. For all of the data, we assume that 2D pose labels are available. Both errors saturate at 20\% of 3D labeled training examples.}\label{wrap-tab:1}
  \begin{center}
\small
\label{tab:more2d}
\begin{tabular*}{1.0\textwidth}{l||y{0.4cm}y{0.4cm}y{0.4cm}y{0.4cm}y{0.4cm}y{0.4cm}y{0.4cm}y{0.4cm}}\toprule  
\diaghead{\hskip5.2em}{Error}{{Ann.perc.}}& &  &  &  &   &  &  &  \\\midrule
                                & 83.1 & 82.8 & 82.8   & 83.6   & 84.5   & 88.1  & 93.9  & 198 \\
                                & 0.28 & 0.28 & 0.27   & 0.28   & 0.29   & 0.30  & 0.33  & 1.97  \\  
\end{tabular*}
\end{center}
\end{table}
\end{center}
\begin{figure*}
   \caption{\textit{Qualitative results by error quartile in terms of .} 
   The rows show representative examples from different error quartiles, top to bottom: 0-25\%, 25-50\%, 50-75\%, 75-100\%} \includegraphics[width=\textwidth]{figures/quali_prepress.pdf}
    \vspace{-1.5em}
   \label{fig:quali}
\end{figure*}
\vspace{-0.2em}
\textbf{Qualitative Results} A selection of qualitative results from the UP-3D dataset can be found in Figure ~\ref{fig:quali}. 
We show examples from the four different error quartiles. Fits from the first three quartiles still reproduce 
the body pose somewhat faithfully, and only in the last row and percentile, problems become clearly visible. 
We show more failure modes in the supplementary material. 

\vspace{-0.3em}

\subsection{Comparison to State-of-the-Art}
Here we compare to the state of the art on HumanEva-I (Table~\ref{table:heva}) and Human3.6M (Table~\ref{table:H36}).
We perform a per-frame rigid alignment of the 3D estimates to the ground truth using Procrustes Analysis
and report results in terms of reconstruction error, i.e. the mean per joint position error after alignment 
(given in ). The model we use here is trained on Human3.6M data.

\begin{table}[t]
\centering
\begin{tabular}{lrr}
\hline
Method & Mean & Median \\
\hline
Ramakrishna et al. \cite{ramakrishna2012reconstructing}   & 168.4 & 145.9\\
Zhou et al.        \cite{zhou_convexrelaxation_cvpr2015}  & 110.0 &  98.9\\
SMPLify            \cite{bogo_smpl_eccv16}                &  79.9 &  61.9\\
Random Forests     \cite{Lassner:UP:2017}                 &  93.5 &  77.6\\
SMPLify (Dense)    \cite{Lassner:UP:2017}                 &  74.5 &  59.6\\
Ours                                                      &  64.0 &  49.4\\
\hline
\end{tabular}
\caption{{\bf HumanEva-I results.} 3D joint errors in mm. }
\label{table:heva}
\end{table}

\begin{table}
\begin{tabular}{lrrrrrrrrr}
\hline
 Method & \multicolumn{1}{c}{Mean}      & \multicolumn{1}{l}{Median} \\ \hline
 Akhter \& Black \cite{akhter_pose_conditioned_cvpr15}        & 181.1                         & 158.1               \\
 Ramakrishna et al. \cite{ramakrishna2012reconstructing}      & 157.3                         & 136.8               \\
 Zhou et al. \cite{zhou_convexrelaxation_cvpr2015}            & 106.7                         &  90.0               \\
 SMPLify         \cite{bogo_smpl_eccv16}                      &  82.3                         &  69.3               \\
 SMPLify (dense) \cite{Lassner:UP:2017}                       &  80.7                         &  70.0               \\ \hline
 SelfSup \cite{tung2017self}                                  &  98.4                         &    -                \\
 Pavlakos et al. \cite{pavlakos2018humanshape}                &  75.9                         &    -                \\
 HMR (H36M-trained)\cite{hmrKanazawa17}                       &  77.6                         &  72.1               \\
 HMR               \cite{hmrKanazawa17}                       &  \bf{56.8}                    &    -                \\
 Ours                                                         &  59.9                         &  52.3               \\ \hline
 \end{tabular}
\caption{{\bf Human 3.6M.} {3D joint errors in mm}.
 }
 \label{table:H36}
 \end{table}

We compare favourably to similar methods, but these are not strictly 
comparable since they train on different datasets. Pavlakos et al. \cite{pavlakos2018humanshape} 
do not use any data from Human3.6M, whereas HMR \cite{hmrKanazawa17} does, along with several other datasets.
We retrained the latter with the original code only using Human3.6M data for a more direct comparison to ours (HMR (H36M-trained) in Table~\ref{table:H36}). 
Given Table~\ref{tab:inptype}, we hypothesize that their approach 
requires more training data for good performance because it uses RGB images as input.
 \section{Conclusion}

In this paper, we make several principled steps towards a full integration of
parametric 3D human pose models into deep CNN architectures. We analyze (1) how
the 3D model can be integrated into a deep neural network, (2) how loss
functions can be combined and (3) how a training can be set up that works efficiently with scarce 3D data.

In contrast to existing methods we use a region-based 2D representation,
namely a 12-body-part segmentation, as an intermediate step prior to the 
mapping to 3D shape and pose. This segmentation provides full spatial coverage 
of a person as opposed to the commonly used sparse set of keypoints, while 
also retaining enough information about the arrangement of parts to allow
for effective lifting to 3D.

We used a stack of CNN layers on top of a segmentation model to predict
an encoding in the space of 3D model parameters, followed by a Tensorflow
implementation of the 3D model and a projection to the image plane. This full
integration allows us to finely tune the loss functions and enables end-to-end
training. We found a loss that combines 2D as well as 3D information to work best.
The flexible implementation allowed us to experiment with the 3D losses only for
parts of the data, moving towards a weakly supervised training scenario that
avoids expensive 3D labeled data. With 3D information for only 20\% of our
training data, we could reach similar performance as with full 3D annotations.

We believe that this encouraging result is an important finding for the
design of future datasets and the development of 3D prediction methods
that do not require expensive 3D annotations for training. Future work
will involve extending this to more challenging settings involving
multiple, possibly occluded, people. \vspace{0.1em}

\textbf{Acknowledgements} We would like to thank Dingfan Chen for help with training the HMR \cite{hmrKanazawa17} model.
 
\clearpage
\bibliographystyle{ieee}
\bibliography{egbib}


\appendix

\section{Further Qualitative Results}

One of our findings is the high correlation between input segmentation
quality and output fit quality. We provide some additional qualitative examples that
illustrate this correlation. In Fig.~\ref{fig:corr1}, we present the four worst examples from the
validation set in terms of 3D joint reconstruction error when we use our trained
part segmentation network; in Fig.~\ref{fig:corr2}, we present the worst examples when the
network is trained to predict body model parameters given the ground truth
segmentations. This does not correct all estimated 3D bodies, but the
remaining errors are noticeably less severe.

\begin{figure*}[t]
\caption{Worst examples from the validation set in terms of 3D error given imperfect segmentations.}
\label{fig:corr1}
\includegraphics[width=\textwidth]{figures/failures.pdf}
\end{figure*}

\begin{figure*}[t]
\caption{Worst examples from the validation set in terms of 3D error given perfect segmentations.}
\label{fig:corr2}
\includegraphics[width=\textwidth]{figures/failures_2.pdf}
\end{figure*}

\section{Training Details}

We present examples of paired training examples and ground truth in Fig~\ref{fig:trex}.

\begin{figure*}[t]
\caption{Example training images annotations illustrating different types and granularities.}
\label{fig:trex}
\includegraphics[width=\textwidth]{figures/trex.pdf}
\end{figure*}
\vspace{-0.5em}
\paragraph{Segmentation Network} We train our own TensorFlow implementation of a RefineNet~\citeapndx{lin17refinenet2} network
(based on ResNet-101) to predict the part segmentations.
The images are cropped to 512x512 pixels, and we train for 20 epochs with a batch size 
of 5 using the Adam~\citeapndx{kingma15adam} optimizer. Learning rate and weight decay are set 
to  and  respectively, with a polynomial learning rate decay. 
Data augmentation improved performance a lot, in particular horizontal 
reflection (which requires re-mapping the labels for left and right limbs), 
scale augmentation (0.9 - 1.1 of the original size) as well as rotations (up to 45 degrees). 
For training the segmentation network on UP-3D we used the 5703 training images. For Human3.6M we subsampled the videos, 
only using every 10th frame from each video, which results in about 32000 frames. 
Depending on the amount of data, training the segmentation networks takes about 6-12 hours on a Volta V100 machine.
\vspace{-0.5em}
\paragraph{Fitting Network} For the fitting network we repurpose a ResNet-50 network pretrained on ImageNet to regress
the SMPL model parameters. We replace the final pooling layer with a single fully-connected
layer that outputs the 10 shape and 216 pose parameters. We train this network for 75 epochs with a 
batch size of 5 using the Adam optimizer. The learning rate is set to 0.00004 with polynomial decay 
and we use a weight decay setting of 0.0001. We found that an L1 loss on the SMPL parameters was
a little better than an L2 loss. We also experimented with robust losses (e.g. Geman-McClure~\citeapndx{GemanMcclure1987} 
and Tukey's biweight loss~\citeapndx{BelagiannisRCN15}) but did not observe benefits. Training this network takes 
about 1.5 hours for the UP-3D dataset and six hours for Human3.6M.
\vspace{-0.5em}
\paragraph{Data Augmentation} At test-time we cannot guarantee that the person will be perfectly centered in the input crop,
which can lead to degraded performance. We found it thus critical to train both the segmentation network and the fitting network
with strong data augmentation, especially by introducing random jitter and scaling. For the fitting network, 
such augmentation has to take place prior to training since it affects the SMPL parameters. We also mirror the data, but this
requires careful mirroring of both the part labels as well as the SMPL parameters. This involves remapping the parts, 
as well as inverting the part rotations.



\bibliographystyleapndx{ieee}
\bibliographyapndx{egbib}



\end{document}
