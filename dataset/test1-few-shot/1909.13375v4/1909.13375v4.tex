

\documentclass[11pt,a4paper]{article}

\newcommand{\isready}{} \newcommand{\ifreadyelse}[2]{\ifdef{\isready}{#1}{#2}}

\newcommand{\ifappendix}[2]{\ifdef{\withappendix}{#1}{#2}}

\usepackage[hyperref]{emnlp2020}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

\usepackage{microtype}

\aclfinalcopy 



\newcommand\BibTeX{B\textsc{ib}\TeX}

\newcommand{\papername}{A Simple and Effective Model for Answering Multi-span Questions}
\newcommand{\drop}{\textsc{DROP}}
\newcommand{\quoref}{\textsc{Quoref}}
\newcommand{\MTMSN}{\textsc{MTMSN}}
\newcommand{\BERTCalc}{\textsc{BERT-Calc}}
\newcommand{\BERTCalculator}{\textsc{BERT-Calculator}}
\newcommand{\embedding}{}
\newcommand{\fone}{F{\scriptsize 1}}
\newcommand{\roberta}{RoBERTa\textsubscript{\tiny{\textsc{LARGE}}}\text{}}
\newcommand{\bert}{BERT\textsubscript{\tiny{\textsc{LARGE}}}\text{}}
\newcommand{\nerd}{NeRd}

\newcommand\jb[1]{\textcolor{blue}{[JB: #1]}}
\newcommand\ag[1]{\textcolor{red}{[AG: #1]}}
\newcommand\es[1]{\textcolor{purple}{[ES: #1]}}
\newcommand\avia[1]{\textcolor{cyan}{[AE: #1]}}
\newcommand\comment[1]{}

\ifreadyelse{}{
\renewcommand\jb[1]{}
\renewcommand\ag[1]{}
\renewcommand\es[1]{}
\renewcommand\avia[1]{}
}

\usepackage{amsfonts} \usepackage{amsmath} \usepackage{bm} \usepackage[capitalise]{cleveref} \usepackage{multirow}
\usepackage{graphicx}
\usepackage{makecell}
\graphicspath{ {./images/} }
\usepackage{textcomp} \usepackage{booktabs} 



\begin{document}

\setlength{\abovedisplayskip}{2pt}
\setlength{\belowdisplayskip}{2pt}

\renewcommand\thetable{2}
\appendix
\section*{Appendix for ``\papername''}
\section{Experimental Setup}
\label{app:setup}
We experiment with model variations that use either SSE, TASE, or their combination as a multi-head model. For \drop{}, we additionally use \textit{arithmetic} and \textit{count} heads based on \cite{Dua2019DROP, nabert+}.
Our model is implemented with PyTorch \cite{NEURIPS2019_9015} and AllenNLP \cite{Gardner2017AllenNLP}.
For  in \ifappendix{Eq.~\eqref{eq:tag_prob}}{Eq. (1)} we use a 2-layer feed-forward network with ReLU activations and  outputs.
We use the Hugging Face implementation of \roberta{} \cite{Wolf2019HuggingFacesTS, liu2019roberta} as the encoder in our model. 5\% of \drop{} and 30\% of \quoref{} are inputs with over 512 tokens.
Due to \roberta{}'s limitation of 512 positional embeddings, we truncate inputs by removing over-flowing tokens from the passage, both at train and test time. We discard 3.87\% of the training examples of \drop{} and 5.05\% of the training example of \quoref{}, which are cases when the answer cannot be outputted by the model (due to a dataset error, or truncation of the correct answer span).
For training, the BertAdam\footnote{\url{https://github.com/huggingface/transformers/blob/694e2117f33d752ae89542e70b84533c52cb9142/README.md\#optimizers}} optimizer is used with default parameters and learning rates of either  or . Hyperparameter search was not performed. We train on a single NVIDIA Titan XP with a batch size of 2 and gradient accumulation of 6, resulting in an effective batch size of 12, for 20 epochs with an early-stopping patience of 10. The average runtime per epoch is 3.5 hours.
Evaluation was performed with the official evaluation scripts of \drop{} and \quoref{}.
Our full implementation can be found at \href{\ifreadyelse{https://github.com/eladsegal/tag-based-multi-span-extraction}{https://anonymized}}{\ifreadyelse{https://github.com/eladsegal/tag-based-multi-span-extraction}{https://anonymized}}. \section{Failure Cases Examples}
\label{app:error_analysis}
Table~\ref{tab:failurecases} contains example failure cases of TASE{\scriptsize BIO}+SSE on \drop{}.

\begin{table*}[ht]
\centering
\footnotesize
\begin{tabular}{@{}p{4.5cm} p{5cm} p{2.2cm} p{2.8cm} @{} }
\toprule
\emph{\textbf{Question}} & \emph{\textbf{Excerpt from Context}} & \emph{\textbf{Gold Answer}} & \emph{\textbf{Prediction}}  \\ \midrule 
\addlinespace

Which two nationalities have the same number of immigrants in Bahrain? &
Indians, 125,000 Bangladeshis, 45,000 Pakistanis, 45,000 Filipinos, and 8,000 Indonesians &
\{``Filipinos", \newline ``Pakistanis"\} &
\{``Filipinos", \newline ``Pakistanis", \newline ``Indonesians"\}
\\ \addlinespace \midrule \addlinespace

What event happened first, Spain losing all territories it had gained since 1909, or the Spanish retaking their major fort at Monte Arruit? &
August 1921, Spain lost all the territories it had gained since 1909 [...] By January 1922 the Spanish had retaken their major fort at Monte Arruit &
\{``Spain lost all the territories it had gained since 1909"\} &
\{``August 1921, Spain lost all the", ``territories it had gained since 1909"\}
\\\addlinespace \bottomrule
\end{tabular}
\caption{Example failure cases of TASE{\scriptsize BIO}+SSE on \drop{}. The first answer exhibits a lack of numeric reasoning and ignores the expected number of spans stated in the question. The second splits a correct span into two spans.}
\label{tab:failurecases}
\end{table*}
 \bibliographystyle{acl_natbib}
\bibliography{emnlp2020}

\end{document}
