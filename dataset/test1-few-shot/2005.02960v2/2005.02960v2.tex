\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{amsmath,amsfonts,amsthm,amssymb}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{nicefrac}       \usepackage{microtype}      

\usepackage{times}
\usepackage{color}
\usepackage{comment}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{float}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{mathrsfs}
\usepackage{newfloat}
\usepackage{relsize}
\usepackage{dsfont}

\usepackage{enumitem}
\usepackage{nicefrac}
\usepackage{caption}
\usepackage{booktabs,siunitx,caption}
\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage{subfigure}

\usepackage{algorithm}
\usepackage{algorithmic}

\newcommand{\A}{\mathcal{A}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\CC}{C^*}
\newcommand{\E}{\mathbb{E}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\U}{\mathcal{U}_{\vec a}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\OPT}{\mathcal{OPT}}
\newcommand{\ind}{\mathbb{I}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\tI}{\widetilde{\I}}
\newcommand{\ttx}[1]{\texttt{#1}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\todo}[1]{\textcolor{blue}{[#1]}}
\newcommand{\colin}[1]{\textcolor{red}{[#1]}}

\def\C{{\mathcal{Y}}}
\def\OPT{{\mathcal{OPT}}}
\def\expec#1#2{{\bf \mathbb{E}}_{#1}[ #2 ]}
\def\expecf#1#2{{\bf \mathbb{E}}_{#1}\left[ #2 \right]}
\def\var#1{\mbox{\bf Var}[ #1 ]}
\def\varf#1{\mbox{\bf Var}\left[ #1 \right]}

\def\pdf{{\text{pdf}}}
\def\query{{\text{query\_until\_lower}}}
\def\cont{{\text{continue\_at\_min}}}

\def\pdfn{{\text{pdf}_n}}
\def\pdfe{{\text{pdf}_e}}
\def\ls{{\texttt{LS}}}

\let\citep\cite

\numberwithin{equation}{section}
\numberwithin{figure}{section}



\theoremstyle{plain}
	\newtheorem{theorem}{Theorem}[section]


	\newtheorem{lem}[theorem]{Lemma}
	\newtheorem{lemma}[theorem]{Lemma}

	\newtheorem{corollary}[theorem]{Corollary}
 	\newtheorem{cor}[theorem]{Corollary}

	\newtheorem{fact}[theorem]{Fact}

	\newtheorem{claim}[theorem]{Claim}
\theoremstyle{definition}
	\newtheorem{definition}[theorem]{Definition}
	\newtheorem{defn}[theorem]{Definition}
	\newtheorem*{remark*}{Remark}

	
\usepackage{thm-restate}
\declaretheorem[name=Theorem, sibling=theorem]{rethm}
\declaretheorem[name=Lemma, sibling=theorem]{relem}

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

\newenvironment{sproof}
{
 \par\noindent{\bfseries\upshape Proof sketch\ }
}
{\qed}

\title{Local Search is State of the Art for Neural Architecture Search Benchmarks}

\author{
Colin White
\\
RealityEngines.AI\\
colin@realityengines.ai
\and 
Sam Nolen
\\
Realityengines.AI\\
sam@realityengines.ai
\\
\and
Yash Savani
\\
RealityEngines.AI\\
yash@realityengines.ai
}

\date{}

\begin{document}

\maketitle


\begin{abstract}Local search is one of the simplest families of algorithms in 
combinatorial optimization, yet it yields
strong approximation guarantees for canonical NP-Complete 
problems such as the traveling salesman problem
and vertex cover.
While it is a ubiquitous algorithm in theoretical computer science, 
local search is often neglected in hyperparameter optimization
and neural architecture search.

We show that the simplest local search instantiations achieve 
state-of-the-art results on 
multiple NAS benchmarks (NASBench-101 and NASBench-201),
outperforming the most popular recent NAS algorithms.
However, local search fails to perform well on the much larger DARTS search space.
Motivated by these observations, we present a theoretical study which
characterizes the performance of local search on graph optimization problems,
backed by simulation results. This may be of independent interest beyond NAS.
All code and materials needed to reproduce our results are
publicly available at \url{https://github.com/realityengines/local_search}.
\end{abstract}

\begin{comment}
Local search is one of the simplest families of algorithms in combinatorial optimization, yet it yields strong approximation guarantees for canonical NP-Complete problems such as the traveling salesman problem and vertex cover. While it is a ubiquitous algorithm in theoretical computer science, local search has been widely neglected in hyperparameter optimization, and has never been used to perform neural architecture search (NAS).

We show that the simplest local search instantiations achieve state-of-the-art results on multiple NAS benchmarks (NASBench-101 and NASBench-201), outperforming the most popular recent NAS algorithms. However, local search fails to perform well on the much larger DARTS search space. Motivated by these observations, we present a theoretical study which characterizes the performance of local search on graph optimization problems, backed by simulation results. This may be of independent interest beyond NAS. All code and materials needed to reproduce our results are
publicly available at https://github.com/realityengines/local_search.
\end{comment}
 
\section{Introduction} \label{sec:intro}
Neural architecture search (NAS) is a widely popular area of machine learning, 
with the
goal of automating the development of the best neural network for a given dataset.
Hundreds of NAS algorithms have been proposed~\citep{nas-survey,zoph2017neural}, and
with the release of two NAS benchmark datasets~\citep{nasbench201, nasbench},
the extreme computational cost for NAS is no longer a barrier, 
and it is easier to fairly compare different NAS algorithms.
Most of the recently proposed state-of-the-art algorithms are becoming increasingly more
complex, many of which use neural networks as subroutines~\citep{wen2019neural, bananas}.
This trend is problematic because as the complexity of NAS algorithms increases, 
the amount of
necessary ``hyper-hyperparameter tuning'', or tuning the NAS algorithm itself, increases.
Not only is this a vicious cycle (will we start using AutoML algorithms to tune AutoML algorithms?),
but the runtime for any hyper-hyperparameter tuning on a new dataset
must be added to the total runtime of the NAS algorithm~\citep{lindauer2019best, yang2019evaluation}.
Since this information is not always recorded, some NAS algorithms may have under-reported runtimes, making it harder to compare different algorithms.


In contrast to prior work, we study a NAS algorithm which can be implemented in five lines
of code (Algorithm~\ref{alg:local_search}).
Local search is a simple and canonical family of greedy algorithms in combinatorial optimization 
and has led to famous results in the study of approximation 
algorithms~\citep{cohen2016local, friggstad2019local, michiels2007theoretical}.
The most basic form of local search, often called the hill-climbing algorithm,
consists of starting with a random architecture, 
and then iteratively training all architectures in its neighborhood, 
choosing the best one for the next iteration.
The neighborhood is typically defined as all architectures which differ by one 
operation or edge.
Local search finishes when it reaches a (local or global) optimum,
or when it exhausts its runtime budget.


Despite its simplicity, we show that local search achieves state-of-the-art 
results on all four datasets from the NASBench-101 and NASBench-201 benchmarks,
beating out many recent algorithms which claimed state of the art.
However, these benchmark datasets contain at most  architectures.
On the DARTS~\citep{darts} search space which contains  architectures,
we show that local search performs worse than random search.
This suggests more generally that strong performance on NAS benchmark datasets 
does not necessarily imply strong performance on large-scale NAS applications.

Motivated by the stark contrast between the performance of local search
on NASBench datasets and DARTS, we present a theoretical study
to better understand the performance of local search for NAS on different search spaces.
The underlying optimization problem in NAS is a hybrid between discrete
optimization, on a graph topology, and continuous optimization, on the
distribution of architecture accuracies.
We formally define a NAS problem instance by the graph topology, 
a global probability density function (PDF) on the architecture accuracies, 
and a local PDF on the accuracies between neighboring architectures, 
and we derive a set of equations which
calculate the probability that a randomly drawn architecture will converge to within 
of the global optimum, for all .
As a corollary, we give equations for the expected number of local minima,
and the expected size of the preimage of a local minimum.
These results completely characterize the performance of local search.
To the best of our knowledge, this is the first result which theoretically predicts the
performance of a NAS algorithm, and may be of independent interest within discrete optimization.
We run simulations which suggest that our theoretical results predict the performance
of real datasets reasonably well.
Altogether, our results show that the performance 
of local search depends on the level of \emph{locality} of the search space, 
as well as the average neighborhood size in the search space.
We make available all code and materials needed to reproduce our results.


\paragraph{Our contributions.} We summarize our main contributions below.
\begin{itemize} [topsep=0pt, itemsep=2pt, parsep=0pt, leftmargin=5mm]
    \item We implement the simple local search algorithm as a baseline
    for NAS, showing that it achieves state-of-the-art performance on two
    NASBench datasets (which both have size ) as well as subpar performance on a large search
    space (of size ). We suggest that existing NAS benchmarks may be too small to
    adequately evaluate NAS algorithms.
    \item We give a full theoretical characterization of the properties of a dataset 
    necessary for local search to give strong performance. 
    We experimentally validate these results on real datasets.
Our results improve the theoretical understanding of local search and
    lay the groundwork for future studies.
    \end{itemize}


 
\section{Related Work} \label{sec:related}

\paragraph{Local search in theoretical computer science.}
Local search has been studied since at least the 1950s in the context of the 
traveling salesman problem~\citep{bock1958algorithm, croes1958method},
machine scheduling~\citep{page1961approach}.
and graph partitioning~\citep{kernighan1970efficient}.
Local search has consistently seen significant attention in 
theory~\citep{aarts1997local, balcan2020k, johnson1988easy}
and practice~\citep{bentley1992fast, johnson1997traveling}.

\paragraph{Neural architecture search.}
NAS has gained significant attention in
recent years~\citep{zoph2017neural},
although the first few techniques have been around since at least the 1990s
~\citep{maziarz2018evolutionary, amoebanet}.
Popular techniques include
Bayesian optimization~\citep{nasbot, auto-keras}, 
reinforcement learning \citep{zoph2017neural, enas, pnas}, 
gradient descent \citep{darts},
neural prediction~\citep{bananas, shi2019multi},
and evolution~\citep{real2019regularized}.
NAS can be broadly split into \emph{macro} search, in which the search is
over the entire neural network, or \emph{micro} search, in which the search
is over a small `cell', which is then duplicated many times to create a
large neural network.
Recent papers have highlighted the need for fair and reproducible NAS 
comparisons~\citep{randomnas, lindauer2019best},
spurring the release of two cell-based 
NAS benchmark datasets~\citep{nasbench201,nasbench},
each of which include tens of thousands of pretrained neural networks.
See the recent survey~\citep{nas-survey} for a more comprehensive overview on NAS.


Prior work has performed local search for NAS using network morphisms, 
guided by cosine annealing~\citep{elsken2017simple}.
This is a more complex variation of local search for NAS.
Very recently, concurrent work has also shown that simple local search 
is a strong baseline for NAS~\citep{ottelander2020local}.
Their work considers multi-objective NAS
(the objective is a function of accuracy and network complexity),
focuses on macro search rather than cell-based search, 
and gives no theoretical results.
Therefore, the concurrent work uses different techniques to achieve a similar conclusion.
The existence of their work strengthens our conclusions, as they were independently
and simultaneously verified.



 
\section{Broader Impact} \label{sec:impact}

Our work seeks to improve the understanding of neural architecture search
by analyzing the differences between benchmark vs.\ large-scale
search spaces, characterizing the performance of local search,
and helping to lay the groundwork for future theoretical results.
While this does not have an immediate societal impact the same way
that a GAN for deep fakes or a deep learning optimizer for  emissions
would, our work, and more broadly the field of NAS, will be the algorithms
powering and refining the deep learning applications with direct societal
implications.

Since our work only indirectly affects society,
we have less control over its net implications.
However, we see two benefits to the AI community that will foster
a positive impact. First, we help to lay the groundwork for future theoretical
results which can improve our understanding of the workings of different algorithms.
Second, we present an easily implementable NAS algorithm which gives strong performance
under some settings, and we advocate for simpler NAS algorithms in the future.
Not only will this exhibit better baselining of NAS algorithms, but it also helps to
democratize neural architecture search. Easily implementable and understandable NAS
algorithms will lead to more widespread use.
Because of the recent push for explicitly reasoning about the impact of
research in AI~\cite{hecht2018time}, we are hopeful that downstream deep learning
applications will be used to benefit society.

 
\section{Preliminaries} \label{sec:prelim}
In this section, we formally define the local search algorithm,
and we define notation that will be used for the rest of the paper.
Given a set , denote an objective function .
Although our theory works generally for any  and , 
we refer to  as a search space of neural architectures, 
and  as the validation loss of  over a fixed dataset and training pipeline.
The goal is to find , the neural architecture with the 
minimum validation loss, or else find an architecture whose validation loss is within 
 of the minimum, for some small .
We define a neighborhood function . For instance,  might represent 
the set of all neural architectures which differ from  by one operation or edge.


Local search in its simplest form (also called the hill-climbing algorithm) 
is defined as follows. 
Start with a random architecture  and evaluate  by training .
Iteratively train all architectures in , and then replace  with the architecture  
such that 
Continue until we reach an architecture 
such that , i.e., we reach a local minimum.
See Algorithm~\ref{alg:local_search}.
We often place a runtime bound on the algorithm, in which case the algorithm returns the
architecture  with the lowest value of  when it exhausts the runtime budget.
In Section~\ref{sec:experiments}, 
we also consider two simple variants.
In the  variant, instead of evaluating every architecture 
in the neighborhood  and picking the best one, 
we draw architectures  at random without replacement
and move to the next iteration as soon as 
In the  variant, we do not stop at a local minimum, instead
moving to the second-best architecture found so far and continuing until we exhaust
the runtime budget.
One final variant, which we explore in Appendix~\ref{app:experiments},
is choosing  initial architectures at random instead of just one,
and setting  to be the architecture with the lowest objective value.

\begin{comment} Local search will only stop when it reaches a \emph{local minimum}: an architecture 
with the lowest objective value among all architectures in its neighborhood.
Note that the \emph{global minimum}, , 
is also a local minimum.
In many applications, we place a runtime bound on the algorithm.
Then the algorithm returns the architecture  with the lowest value of  
that it has trained so far when it reaches the runtime bound.
In Section~\ref{sec:experiments}, we consider two simple variants 
of Algorithm~\ref{alg:local_search}.
First, instead of evaluating every architecture in the set  
and choosing  as the best one,
we draw architectures  from  randomly without replacement,
and we choose  as the first architecture  we find such that .
We call this the  variant.
In the second variant of local search,
we do not stop once we reach a local minimum, 
instead moving to the second-best architecture we have evaluated so far, 
and evaluating its neighborhood.
We call this the  variant.
One final variant, which we explore in Appendix~\ref{app:experiments},
is choosing  initial architectures at random instead of just one
and setting  as the random architecture with the lowest objective value.
\end{comment}

\begin{algorithm}
\caption{Local search}\label{alg:local_search}
\begin{algorithmic} \STATE {\bfseries Input:} Search space , objective function , 
neighborhood function 
\STATE 1. Pick an architecture  uniformly at random
\STATE 2. Evaluate ; denote a dummy variable ; set 
\STATE 3. While 
\begin{enumerate}[label=\roman*., itemsep=1pt, parsep=1mm, topsep=1pt, leftmargin=8mm]
    \item Evaluate  for all 
    \item Set ; set 
\end{enumerate}
\STATE {\bfseries Output:} Architecture 
\end{algorithmic}
\end{algorithm}

Now we define the notation used in Sections~\ref{sec:method} and~\ref{sec:experiments}.
Given a search space  and a neighborhood function ,
we define the neighborhood graph  such that for , 
the edge  is in  if and only if .
We only consider symmetric neighborhood functions, that is,  implies . 
Therefore, we may assume that the neighborhood graph is undirected.
Given , , and a loss function , define  such
that ,  if 
, and  otherwise.
In other words,  denotes the architecture after performing one iteration 
of local search starting from . 
For integers , recursively define .
We set  and denote ,
that is, the output when running local search to convergence, starting at .
Similarly, define the preimage  for integers 
and .
That is,  is a multifunction which defines the set of all points  
which reach  at some point during local search.
We refer to  as the full preimage of .
 
\section{A theory of local search} \label{sec:method}


In this section, we give a theoretical analysis of local search for NAS,
including a complete characterization of its performance.
We present both a general result on local search, as well as a closed-form solution for search spaces satisfying certain constraints.
We give an experimental validation of our theoretical results in the next section.


In a NAS application,
the topology of the search space is fixed and discrete, 
while the distribution of validation losses for architectures is randomized and continuous,
due to the non-deterministic nature of training a neural network.
For example, both NASBench-101 and NASBench-201 include validation and test accuracies 
for three different random seeds for each architecture, to better simulate real NAS experiments.
Therefore, we assume that the validation loss for a trained architecture is sampled from
a global probability distribution, and for each architecture, 
the validation losses of its neighbors are sampled from a local probability distribution.
Given a graph , each node  has a loss 
 sampled from a PDF which we denote by . 
For any two neighbors , the PDF for the validation loss  
of architecture  is given by .
Choices for the distribution  are constrained by the 
fixed topology of the search space, as well as the distribution .
In Appendix~\ref{app:method}, we discuss this further by formally defining
measurable spaces for all random variables in our framework.


Our main result is a formula for the fraction of nodes in the search space which are local
minima, as well as a formula for the fraction of nodes  such that the loss of  
is within  of the loss of the global optimum, for all 
In other words, we give a formula for the probability that the local search algorithm 
outputs a solution that is close to optimal.
Note that such a formula characterizes the performance of local search.
We give the full proofs for all of our results in Appendix~\ref{app:method}.
For the rest of this section, 
we assume for all , , and
we assume  is vertex transitive
(given , there exists an automorphism of  which maps  to ).
Let  denote the architecture with the global minimum loss, therefore
the support of the distribution of validation losses is a subset of  
Technically, the integrals in this section are Lebesgue integrals. However, we use the more standard Riemann-Stieltjes notation for clarity. We also slightly abuse notation and
define  when 

In the following theorems and lemmas, we assume there is a fixed graph , and the 
validation accuracies are randomly assigned from a distribution defined by  
and . Therefore, the expectations are over the random draws from  and .
\footnote{
In particular, given a node  with validation loss 
the probability distribution for the validation loss of a neighbor depends only 
on  and , which makes the local search procedure similar to a 
Markov process.
}


\begin{restatable}{rethm}{probopt}\label{thm:prob_opt}
Given , , , , , and , we have

\end{restatable}


\begin{proof}[\textbf{Proof sketch.}]



To prove the first statement,
we introduce an indicator random variable on the architecture space to test if the 
architecture is a local minimum:  Then

Intuitively, in the proof of the second statement, we follow similar reasoning but multiply
the probability in the outer integral by the expected size of 's full preimage to weight
the integral by the probability a random point will converge to .
Formally, we introduce an indicator random variable on the architecture space 
that tests if a node will terminate on a local minimum that is
within  of the global minimum:


We use this random variable to prove the second statement of the theorem.


where the last equality follows from the first half of this theorem.
\end{proof}

In the next lemma, we derive a recursive equation for 
We define the \emph{branching fraction} of graph  as ,
where  denotes the set of nodes which are distance  to  in .
For example, the branching fraction of a tree with degree  is  for all ,
and the branching fraction of a clique is  and  for all 
One more example is as follows.
In Appendix~\ref{app:experiments}, we show that the neighborhood graph of the
NASBench-201 search space is  and therefore its branching factor is


\begin{restatable}{relem}{geneqns}\label{lem:gen_eqns}
Given , , , , and ,
then for all , we have the following equations.

\end{restatable}

For some PDFs, it is not possible to find a closed-form solution
for  because arbitrary 
functions may not have closed-form antiderivatives.
By assuming there exists a function  such that  for all ,
we can use induction to find a closed-form expression for .
This includes the uniform distribution ( for ),
as well as distributions that are polynomials in .
In Appendix~\ref{app:method}, we use this to show that 
can be approximated by 
where 
Now we use a similar technique to give a closed-form expression for Theorem~\ref{thm:prob_opt}
when the local and global distributions are uniform.

\begin{restatable}{relem}{fulluniform}\label{lem:full_uniform}
If ,
then  and

\end{restatable}

\begin{proof}[\textbf{Proof sketch.}]
The probability density function of  is equal to 1 on  and 0 otherwise.
Then 
We use this in combination with Theorem~\ref{thm:prob_opt} to prove the first statement:


To prove the second statement, first
we use induction on the recursive expression in Lemma~\ref{lem:gen_eqns}
to show that for all , 

We plug this into the second part of Theorem~\ref{thm:prob_opt}:

\end{proof}

In the next section, we will show that Theorem~\ref{thm:prob_opt} and 
Lemma~\ref{lem:full_uniform} can be used
to predict the performance of local search.




 
\section{Experiments} \label{sec:experiments}

In this section, we discuss our experimental setup and results.
To promote reproducible research,
we discuss how our experiments follow the best practices 
checklist~\citep{lindauer2019best} in Appendix~\ref{app:experiments},
and we release our code at \url{https://github.com/realityengines/local_search}.
In particular, we run experiments on NAS benchmark datasets, 
we run enough trials to reach statistical significance, and we release our code
and all materials needed to reproduce our results.
We start by describing the search spaces used in our experiments.


\paragraph{NASBench-101~\citep{nasbench}.}
The NASBench-101 dataset consists of over 423,000 
unique neural architectures from a cell-based search space,
and each architecture comes with precomputed validation, 
and test accuracies for 108 epochs on CIFAR-10. 
The search space consists of a cell with 7 nodes. 
The first node is the input, and the last node is the output. 
The remaining five nodes can be either  convolution, 
 convolution, or  max pooling.
The cell can take on any DAG structure from the input to the output with at most 9 edges.


\paragraph{NASBench-201~\citep{nasbench201}.}
The NASBench-201 dataset consists of  unique neural architectures, 
with precomputed training, validation,
and test accuracies for 200 epochs on CIFAR-10, CIFAR-100, and ImageNet-16-120.
The search space consists of a cell which is a complete directed acyclic graph 
over 4 nodes.
Therefore, there are  edges.
Each \emph{edge} takes an operation, and there are five possible operations: 
 convolution,  convolution, 
 avg.\ pooling, skip connect, or none.


\paragraph{DARTS search space~\citep{darts}.}
The DARTS search space is a popular search space for large-scale cell-based
NAS experiments on CIFAR-10.
The search space consists of roughly  architectures.
It consists of two cells: a convolutional cell and a reduction cell, each with six nodes. 
The first two nodes are input from previous layers, and the last four nodes can take
on any DAG structure such that each node has degree two.
Each edge can take one of eight operations.




\subsection{Local search performance}
We evaluate the effectiveness of local search for NAS.
First we compare local search to other NAS algorithms on the
four benchmark dataset/search space pairs.
On the three NASBench-201 datasets, we compare local search to 
random search, DNGO~\citep{snoek2015scalable}, 
Regularized Evolution~\citep{real2019regularized},
Bayesian Optimization, BANANAS~\citep{bananas},
and NASBOT~\citep{nasbot}.
On NASBench-101, we test local search
with the aforementioned algorithms, as well as
REINFORCE~\citep{reinforce} and AlphaX~\citep{alphax}.
For every algorithm, we used the code directly from the 
corresponding open source repositories and kept the hyperparameters
unchanged for almost all algorithms. 
For more details on the implementations, see Appendix~\ref{app:experiments}.
We gave each algorithm a budget of 300 queries.
For each algorithm, we recorded the test loss of the architecture with the best validation 
loss that has been queried so far.
We ran 200 trials of each algorithm and averaged the results.
For local search, we set  to denote all architectures which differ by one operation
or edge.
If local search converged before its budget, it started a new run.
On NASBench-101 and ImageNet-16-120, 
we used the  variant of local search, and
on NASBench-201 CIFAR-10 and CIFAR-100, we used the  variant.
(In Appendix~\ref{app:experiments}, we evaluate all four variants.)
See Figure~\ref{fig:ls_baselines_201}.
Local search consistently performs the strongest on all four datasets.
Random search performed significantly worse than all other algorithms
on NASBench-201, so we omitted it from the plots.
Note that on ImageNet16-120, some algorithms such as NASBOT overfit
to the training set, causing performance to decline over time.




In Appendix~\ref{app:experiments}, we evaluate local search with a different 
number  of random initializations, showing that there is little effect on 
the performance when 
We also report local search statistics such as the number of local minima and the 
average number of iterations to convergence.



Next, we evaluate local search with the  variant on the DARTS search space.
We ran one trial training all queried architecture to 25 epochs, 
and another trial training to 50 epochs.
The runtime is 11.8 GPU days on a Tesla V100.
We trained the final returned architectures for 600 epochs, using the same
training pipeline as prior work~\citep{randomnas, darts}.
See Table~\ref{tab:darts}.
Local search performed worse than random search, and significantly worse than DARTS.
One reason for the subpar performance is because the degree of the neighborhood graph is 136, 
much larger than NASBench-201's 24. 
For instance, in the 50 epoch trial, 100 queries and 11.8 GPU days
was not sufficient to get through a single iteration of local search 
even with the  variant.


\begin{table*}[t]
\caption{Percent error on the test set of the best architectures
returned by several NAS algorithms. 
The runtime is in total GPU-days on a Tesla V100.}
\setlength\tabcolsep{0pt}
\begin{tabular*}{\textwidth}{l @{\extracolsep{\fill}}*{8}{S[table-format=1.4]}} 
\toprule
\multicolumn{1}{c}{NAS Algorithm} & \multicolumn{1}{c}{Source} & \multicolumn{1}{c}{Test error} & \multicolumn{1}{c}{Queries} & \multicolumn{1}{c}{Runtime} \\
\midrule
Random search & \hspace{2mm}\citep{darts} & 3.29 &  & 4 \\
DARTS & \hspace{2mm}\citep{darts} & 2.68 & & 5 \\
ASHA & \hspace{2mm}\citep{randomnas} & 3.08 & \hspace{1mm}700 & 9 \\
BANANAS & \hspace{2mm}\citep{bananas} &   2.64 & \hspace{1mm}100 & 11.8 \\
Local Search 50 epochs & Ours &   3.49 & \hspace{1mm}100 & 11.8 \\
Local Search 25 epochs & Ours &  3.93 & \hspace{1mm}200 & 11.8 \\
\bottomrule
\end{tabular*} 
\label{tab:darts}
\end{table*} 

\paragraph{Discussion.}
The simple local search algorithm achieved state-of-the-art performance on all four
NAS benchmark search space/datasets, beating out several popular NAS algorithms.
However, the poor performance on DARTS shows that local search is not efficient on search
spaces with high degree. As a consequence, we suggest that existing NAS benchmarks including
NASBench-101 and -201 may be too small and/or simple to adequately evaluate NAS algorithms.
For example, while NASBench-201 contains 15k architectures, 
the DARTS search space contains  architectures, and similarly the search spaces
from ENAS~\citep{enas} and PNAS~\citep{pnas} contain  and  architectures.
Since local search can be implemented in five lines of code, we encourage local search to
be used as a benchmark in future work, especially when experimenting on smaller search
spaces.


\paragraph{Simulation Results.}
We run a local search simulation using the equations in
Section~\ref{sec:method}
as a means of experimentally validating our theoretical results
with real data (we use NASBench-201).
In order to use these equations, first we must approximate the local
and global probability density functions of the three 
datasets in NASBench-201.
We start by visualizing the probability density functions of the three datasets
See Figure~\ref{fig:201_local_pdfs}.
We see the most density along the diagonal, meaning that architectures
with similar accuracy are more likely to be neighbors.
Therefore, we can approximate the PDFs by using the following equation:
\begin{figure*}
\centering \includegraphics[width=0.33\textwidth]{fig/cifar10_local_pdf.pdf}
\hspace{-3pt}
\includegraphics[width=0.33\textwidth]{fig/cifar100_local_pdf.pdf}
\hspace{-3pt}
\includegraphics[width=0.33\textwidth]{fig/imagenet16_local_pdf.pdf}
\caption{
Probability density function for CIFAR-10 (left), CIFAR-100 (middle), and
ImageNet16-120 (right) on NASBench-201. For each coordinate , a darker color indicates
that architectures with accuracy  and  are more likely to be neighbors.
}
\label{fig:201_local_pdfs}
\end{figure*}

This is a normal distribution with mean  
and standard deviation of , truncated so that it is a valid PDF
in 
To model the global PDF for each dataset,
we plot a histogram of the validation losses and match them to the closest-fitting
values of  and .
See Figure~\ref{fig:global_histogram}.
The best values of  are  and  for CIFAR-10, CIFAR-100, 
and ImageNet16-120, respectively, and the best values for  are all .
\footnote{
Note that plotting a histogram of all validation losses is impractical 
for real-world NAS search spaces; we do this on NASBench-201 as a means 
of validating our theoretical results.
}
To model the local PDF for each dataset, we compute
the random walk autocorrelation (RWA) on each dataset.
RWA is defined as the autocorrelation of the accuracies of points visited during a
random walk on the neighborhood 
graph~\citep{weinberger1990correlated, stadler1996landscapes},
and was used to measure locality in NASBench-101 in prior work~\citep{nasbench}.
For the full details of how we modeled the datasets in NASBench-201, 
see Appendix~\ref{app:experiments}.





\begin{figure*}
\centering \includegraphics[width=0.32\textwidth]{fig/global_histogram_cifar10.pdf}
\hspace{-3pt}
\includegraphics[width=0.32\textwidth]{fig/global_histogram_cifar100.pdf}
\hspace{-3pt}
\includegraphics[width=0.32\textwidth]{fig/global_histogram_ImageNet16-120.pdf}
\caption{
Histogram of validation losses for the three datasets in NASBench-201,
fitted with the best values of  and  in Equation~\ref{eq:normal_pdf}.
}
\label{fig:global_histogram}
\end{figure*}


Now we use Theorem~\ref{thm:prob_opt} to compute the 
probability that a randomly drawn architecture will converge to within 
of the global minimum when running local search.
Since there is no closed-form solution for the expression in Lemma~\ref{lem:gen_eqns},
we compute Theorem~\ref{thm:prob_opt} up to the 5th preimage.
We compare this to the experimental results on NASBench-201.
We also compare the performance of the NASBench-201 search space with validation losses drawn uniformly at random, 
to the performance predicted by Lemma~\ref{lem:full_uniform}.
Finally, we compare the preimage sizes of the architectures in NASBench-201 with
randomly drawn validation losses to the sizes predicted in Lemma~\ref{lem:gen_eqns}.
See Figure~\ref{fig:ls_baselines_201}.
Our theory exactly predicts the performance and the preimage sizes of the uniform 
random NASBench-201 dataset.
On the three image datasets, our theory predicts the performance
fairly accurately, but is not perfect due to our assumption that the distribution
of accuracies is unimodal.

\begin{figure*}
\centering \includegraphics[width=0.33\textwidth]{fig/ls_cifar10.pdf}
\hspace{-3pt}
\includegraphics[width=0.33\textwidth]{fig/ls_cifar100.pdf}
\hspace{-3pt}
\includegraphics[width=0.33\textwidth]{fig/ls_imagenet.pdf}
\includegraphics[width=0.33\textwidth]{fig/ls_baselines_101.pdf}
\hspace{-3pt}
\includegraphics[width=0.33\textwidth]{fig/real_synth_data.pdf}
\hspace{-3pt}
\includegraphics[width=0.33\textwidth]{fig/uniform_preimages.pdf}
\caption{
Results for NAS algorithms on NASBench-201 (top) and NASBench-101 (bottom left).
Probability that local search will converge to within 
of the global optimum, compared to Theorem~\ref{thm:prob_opt} (bottom middle).
Validation loss vs.\ size of preimages, compared to Lemma~\ref{lem:gen_eqns} (bottom right).
}
\label{fig:ls_baselines_201}

\end{figure*}


 
\section{Conclusion} \label{sec:conclusion}
We show that the simplest local search algorithm  achieves state-of-the-art results on 
the most popular existing NAS benchmarks (NASBench-101 and NASBench-201).
We also show that it has subpar performance on the DARTS search space,
suggesting that the NAS benchmarks may be too simple and/or small to adequately evaluate NAS methods.
Since local search is a simple technique that sometimes gives state-of-the-art performance,
we encourage local search to be used as a benchmark for NAS in the future,
especially for smaller search spaces.

Motivated by the stark contrast between the performance of local search on NASBench datasets
and DARTS, we give a theoretical study which explains the performance of local search for
NAS on different search spaces.
We define a probabilistic graph optimization framework to study NAS problems, and
we give a characterization of the performance of local search for NAS in our framework. 
In particular, we find that local search performs well on search spaces with high locality
and with a neighborhood graph of low degree.
Our theoretical results may be of independent interest.
We validate our theory with experimental results.
Investigating more sophisticated variants of local search for NAS such as 
Tabu search, simulated annealing,
or multi-fidelity local search, are interesting next steps.



 
\section*{Acknowledgements}
We thank Willie Neiswanger and Murali Narayanaswamy for their help with this project.

\newpage

\bibliography{main}
\bibliographystyle{plain}

\newpage
\appendix

\section{Details from Section~\ref{sec:method}} \label{app:method}

In this section, we give details from Section~\ref{sec:method}.
For convenience, we restate all theorems and lemmas here.




We start by formally defining all measurable spaces in our theoretical framework.
Recall that the topology of the search space is fixed and discrete, 
while the distribution of validation losses for architectures is randomized and continuous.
This is because training a neural network is not deterministic; in fact, both NASBench-101 and
NASBench-201 include validation and test accuracies for three different random seeds for each
architecture, to better simulate real NAS experiments.
Therefore, we assume that the validation loss for a trained architecture is sampled from
a global probability distribution, and for each architecture, 
the validation losses of its neighbors are sampled from a local probability distribution.

Let  denote a measurable space for the global validation losses induced by the dataset on the architectures,
where  is the Borel -algebra on . 
The distribution for the validation loss of any architecture in the search space is given by . 

Let  denote a measurable space for validation losses in a neighborhood of an architecture. Let  denote a random variable mapping the validation losses of two neighboring architectures to the loss of the second architecture, .  has a distribution that is characterized by probability density function . This gives us a probability over the validation loss for a neighboring architecture.

Every architecture  has a loss  that is sampled from . For any two neighbors , the PDF for the validation loss  of architecture  is given by .
Note that choices for the distribution  are constrained by the 
fixed topology of the search space, as well as the selected distribution .
Let  denote a measurable space over the nodes of the graph. 


For the rest of this section, we fix an arbitrary neighborhood graph
 with vertex set  such that for all , , i.e.,
 has regular degree , and we assume that  is vertex transitive.
Each vertex in  is assigned a validation loss according to
 and  defined above.
The expectations in the following theorem and lemmas are over the random
draws from  and .

\probopt*

\begin{proof}[\textbf{Proof.}]
To prove the first statement,
we introduce an indicator random variable on the architecture space to test if the architecture is a local minimum ,
where 



The expected number of local minima in  is equal to  times the
fraction of nodes in  which are local minima. Therefore, we have



In line one we use the notation .


To prove the second statement, we introduce an indicator random variable on the architecture space that tests if a node will terminate on a local minimum that is
within  of the global minimum, , where



We use this random variable to prove the second statement of the theorem.



where the last equality follows from the first half of this theorem.
This concludes the proof.
\end{proof}


Recall that we defined the \emph{branching fraction} of graph  as ,
where  denotes the set of nodes which are distance  to  in .
For example, the branching fraction of a tree with degree  is  for all ,
and the branching fraction of a clique is  and  for all 
Also, for any graph, .
We will see in Section~\ref{app:experiments} that the neighborhood graph of 
the NASBench-201 search space is  and therefore its branching factor 
is 

Now we restate and prove Lemma~\ref{lem:gen_eqns}, which
gives a formula for the 'th preimage of the local search function.

\geneqns*

\begin{proof}[\textbf{Proof.}]
The function  returns a set of nodes which form the 
preimage of node , namely, the set of all neighbors  
with higher validation loss than , and whose neighbors  excluding  
have higher validation loss than . Formally,


Let  denote a random variable where . The probability distribution for  gives 
the probability that a neighbor of  is in the preimage of . 
We can multiply this probability by  to express the expected number of nodes in the preimage of .




Note that the inner integral is raised to the power of , not ,
so as not to double count node .
We can use this result to find the preimage of node  after  steps. Let  denote a random variable where




Following a similar argument as above, we 
compute the expected size of the m'th preimage set.


\end{proof}


\paragraph{Closed-form solution for single-variate PDFs.}
Now we give the details for Lemma~\ref{lem:full_uniform}.
We start with a lemma that will help us prove Lemma~\ref{lem:full_uniform}.
This lemma uses induction to derive a closed-form solution to
Lemma~\ref{lem:gen_eqns} in the case where  is independent of .


\begin{restatable}{relem}{exindependent}\label{lem:exindependent}
Assume there exists a function  such that  for all .
Given , for , 

\end{restatable}

\begin{proof}[\textbf{Proof.}]
Given , 

where the first equality follows from Lemma~\ref{lem:gen_eqns}.
Now we give a proof by induction for the closed-form equation.
The base case, , is proven above.
Given an integer , assume that

Then


In the first equality, we used Lemma~\ref{lem:gen_eqns}, and
in the fourth equality, we used the fact that


This concludes the proof.
\end{proof}

Next, we prove a lemma which gives strong approximation guarantees
on the size of the full preimage of an architecture, again assuming
that  is independent if .
For this lemma , we need to assume that  is large compared to .
However, this is the only lemma that assumes  is large. In particular,
Lemma~\ref{lem:full_uniform} will not need this assumption.

\begin{restatable}{relem}{preimages}\label{lem:preimages}
Assume there exists  such that  for all .
Denote .
Given , there exists  such that for all ,
for all , we have

\end{restatable}

\begin{proof}[\textbf{Proof.}]
From Lemma~\ref{lem:exindependent},
we have 


We start with the upper bound.
For all , 
because  for all .
Therefore for all ,


It follows that

The final equality comes from the well-known Taylor
series  
(e.g.\ \citep{abramowitz1948handbook}) evaluated at .

Now we prove the lower bound.
 by definition for all graphs, and for ,
, where  denotes the diameter of the graph.
(Since  for all ,  is meaningless for .)
Recall that all of our arguments assume vertex transitivity.
It follows that .
Now, for a fixed ,  approaches 1 as  approaches infinity. 
Therefore, given , there exists  such that for all ,

Then for all ,


Therefore,


It follows that

The final equality again comes from the Taylor series , 
this time evaluated at 
\end{proof}

Note that Equation~\ref{eq:full_preimage} does not require the assumption
that  is large. Now we can use Equation~\ref{eq:full_preimage} and
Theorem~\ref{thm:prob_opt} to prove Lemma~\ref{lem:full_uniform}.

\fulluniform*

\begin{proof}[\textbf{Proof.}]

The probability density function of  is equal to 1 on  and 0 otherwise.
Let .
Then 
Using Theorem~\ref{thm:prob_opt},
we have 


Now we plug in Equation~\ref{eq:full_preimage}
to the second part of Theorem~\ref{thm:prob_opt}.

\end{proof} 
\section{Details from Section~\ref{sec:experiments}} \label{app:experiments}

In this section, we give details and supplementary results for Section~\ref{sec:experiments}.
We start by giving full details for all search spaces used in our experiments.

\paragraph{NASBench-101~\citep{nasbench}.}
The NASBench-101 benchmark dataset consists of over 423,000 
unique neural architectures from a cell-based search space,
and each architecture comes with precomputed validation, 
and test accuracies for 108 epochs on CIFAR-10. 

The search space consists of a cell with 7 nodes. 
The first node is the input, and the last node is the output. 
The remaining five nodes can be either  convolution, 
 convolution, or  max pooling.
The cell can take on any DAG structure from the input to the output with at most 9 edges.
The hyper-architecture consists of nine cells stacked sequentially, 
with each set of three cells separated by downsampling layers.
The first layer before the first cell is a convolutional layer, 
and the hyper-architecture ends with a global average pooling layer and a fully
connected layer. 

The neighbors of a cell consist of the set of all cells that differ by one operation or edge.
Since each cell can have between 1 and 9 edges, the number of neighbors of a cell can range
from 1 to 29.

\paragraph{NASBench-201~\citep{nasbench201}.}
The NASBench-201 dataset consists of over  unique neural architectures, 
with precomputed training, validation,
and test accuracies for 200 epochs on CIFAR-10, CIFAR-100, and ImageNet-16-120.

The search space consists of a cell which is a complete directed acyclic graph 
over 4 nodes.
Therefore, there are  edges.
Each \emph{edge} takes an operation, and there are five possible operations: 
 convolution,  convolution, 
 avg.\ pooling, skip connect, or none.
The hyper-architecture consists of 15 cells stacked sequentially, with each set of five
cells separated by residual blocks. The first layer before the first cell is a
convolution layer, and the hyper-architecture ends with a global average pooling layer
and a fully connected layer.

Since every cell has exactly 6 edges, the total number of possible cells is .
We say that two cells are neighbors if they differ by exactly one operation.
Then the diameter of the neighborhood graph is 6, because any cell can reach
any other cell by swapping out all 6 of its operations.
Each cell has exactly 24 neighbors, because there are 6 edges, and each edge
has 4 other choices for an operation.
The neighborhood graph is , that is, the Cartesian product
of six cliques of size five.


\paragraph{DARTS search space~\citep{darts}.}
The DARTS search space is a popular search space for large-scale NAS experiments.
It is a convolutional cell-based search space used for CIFAR-10.
The search space consists of two cells, a convolutional cell and a reduction cell,
each with six nodes. The hyper-architecture stacks  convolutional cells together with one
reduction cell. 
For each cell, the first two nodes are the outputs from the previous two cells in the
hyper-architecture. The next four nodes contain two edges as input, creating a DAG.
Each edge can take on one of seven operations.

\subsection{Details and additional local search experiments}
In this section, we give details and additional experiments for local search 
on the datasets described above.

For every benchmark NAS algorithm, we used the code directly from its corresponding
open-source repository.
For regularized evolution, we changed the population
size from 50 to 30 to account for fewer queries.
For NASBOT, which was designed for macro (non cell-based) NAS, we computed the
distance between two cells by taking the earth-mover's distance between the
set of row sums, column sums, and node operations, similar to~\citep{bananas}.
We did not change any hyperparameters for the other baseline algorithms.
For vanilla Bayesian optimization, we used the ProBO 
implementation~\citep{neiswanger2019probo}.
Our experimental setup is the same as prior work (e.g.,~\citep{nasbench}).
At each timestep , we report the test error of the architecture with the best
validation error found so far, and we run 200 trials of each algorithm and
average the result.


First, we compute local search statistics for each 
of the datasets in NASBench-201.
The experimental setup is the same as in Section~\ref{sec:experiments}.
We also construct a randomized dataset by replacing the validation
error for each architecture in NASBench-201 with a number drawn from  
For the three image datasets, we ran standard local search as well as the
 variant.
For each experiment, we started local search from \emph{all} 15625 initial seeds
for local search, and averaged the results.
See Table~\ref{tab:nasbench_201}.

\begin{table*}
\centering
\caption{Statistics of local search for NASBench-201 datasets.}
\begin{minipage}[c]{.98\textwidth}
\setlength\tabcolsep{0pt}
\begin{tabular*}{\textwidth}{l @{\extracolsep{\fill}}*{8}{S[table-format=1.4]}} 
\toprule
\multicolumn{1}{c}{Dataset}
& \multicolumn{1}{c}{} 
& \multicolumn{1}{c}{Avg.\ path length} 
& \multicolumn{1}{c}{\# local min.} & 
\multicolumn{1}{c}{\% reached global min.} \\
\midrule
CIFAR-10 & \hspace{1cm}\text{No} & 5.36 & 21 & 47.4 \\
CIFAR-100 & \hspace{1cm}\text{No} & 5.59 & 29 & 58.5 \\
ImageNet-16-120 & \hspace{1cm}\text{No} & 4.67 & 47 & 10.9 \\
Random & \hspace{1cm}\text{No} & 2.56 & 616 & 0.717 \\
CIFAR-10 & \hspace{1cm}\text{Yes} & 9.59 & 21 & 59.8 \\
CIFAR-100 & \hspace{1cm}\text{Yes} & 7.31 & 1 & 100.0 \\
ImageNet-16-120 & \hspace{1cm}\text{Yes} & 13.33 & 1 & 100.0 \\
\bottomrule
\end{tabular*} 
\label{tab:nasbench_201}
\end{minipage}
\end{table*} 



Now we evaluate all four variants of local search on all three NASBench-201 datasets.
We use the same experimental setup as described in Section~\ref{sec:experiments}.
See Figure~\ref{fig:ls_full_201}.
See Section~\ref{sec:method} for an explanation of the  and  variants to local search.

\begin{figure*}
\centering \includegraphics[width=0.33\textwidth]{fig/ls_full_cifar10.pdf}
\hspace{-3pt}
\includegraphics[width=0.33\textwidth]{fig/ls_full_cifar100.pdf}
\hspace{-3pt}
\includegraphics[width=0.33\textwidth]{fig/ls_full_imagenet.pdf}
\caption{
Results for local search variants on CIFAR-10 (left), CIFAR-100 (middle),
and ImageNet-16-120 (right) on NASBench-201.
}
\label{fig:ls_full_201}
\end{figure*}


Now we evaluate the performance of local search as a function of the number
of initial random architectures drawn at the beginning.
We run local search with the number of initial random architectures set to 1, 
and 10 to 100 in increments of 10. For each number of initial random architectures,
we ran 2000 trials and averaged the results.
See Figure~\ref{fig:ls_init_201}.

\begin{figure*}
\centering \includegraphics[width=0.98\textwidth]{fig/ls_init_201.pdf}
\caption{
Results for local search performance vs.\ number of inital randomly drawn architectures
on NASBench-201 for CIFAR-10 (left), CIFAR-100 (middle),
and ImageNet-16-120 (right).
}
\label{fig:ls_init_201}
\end{figure*}



\subsection{Details from simulation experiments}

In this section, we give more details for our simulation experiment
described in Section~\ref{sec:experiments}.


For convenience, we restate Equation~\ref{eq:normal_pdf}, the function used to approximate
the datasets in NASBench-201.

This is a normal distribution with mean  
and standard deviation of , truncated so that it is a valid PDF
in 
For a visualization, see Figure~\ref{fig:norm_pdf}.
In order to choose an appropriate probability density function for modelling the
datasets in NASBench-201, we approximate the  values for
both the local and global PDFs.

To model the global PDF for each dataset,
we plot a histogram of the validation losses and match them to the closest-fitting
values of  and .
See Figure~\ref{fig:global_histogram} in Section~\ref{sec:experiments}.
The best values are  and  for CIFAR-10, CIFAR-100, 
and ImageNet16-120, respectively.



\begin{figure*}
\centering \includegraphics[width=0.98\textwidth]{fig/norm_pdf.pdf}
\caption{
Normal PDF from Equation~\ref{eq:normal_pdf} plotted with three values of .
}
\label{fig:norm_pdf}
\end{figure*}



\begin{figure*}
\centering \includegraphics[width=0.4\textwidth]{fig/rwa_201_truncated.pdf}
\caption{
RWA vs.\ distance for three datasets in NASBench-201, as well as three values of 
 in Equation~\ref{eq:normal_pdf}.
Since a random walk reaches a mean distance of  after  steps,
we plot the -axis as the square root as the autocorrelation
shift, similar to prior work~\cite{nasbench}. 
}
\label{fig:rwa}
\end{figure*}


Now we plot the random-walk autocorrelation (RWA) described in Section~\ref{sec:experiments}.
Recall that RWA is defined as the 
autocorrelation of the accuracies of points visited during a
walk of random single changes through the 
search space~\citep{weinberger1990correlated, stadler1996landscapes},
and was used to measure locality in NASBench-101 in prior work~\citep{nasbench}.
We compute the RWA for all three datasets in NASBench-201, by performing a random
walk of length 100,000.
See Figure~\ref{fig:rwa}.
We see that all three datasets in NASBench-201, as evidenced because there is a
high correlation at distances close to 0.
As the diameter of NASBench-201 is 6, the correlation approaches zero at distances 
beyond about 3.5.
In order to model the local pdfs of each dataset,
we also compute the RWA for Equation~\ref{eq:normal_pdf}, and match each dataset
with the closest value of .
We see that a value of  is the closest match for all three datasets.


Now for each of the three NASBench-201 datasets, we have estimates for 
the  and 
distributions. We plug each (, ) pair into Theorem~\ref{thm:prob_opt},
which gives a plot of  vs.\ percent of architectures that converge to within
 of the global optimum after running local search.
We compare these to the true plot in Figure~\ref{fig:ls_baselines_201}.
For the random simulation, we are modeling the case where ,
so we can use Lemma~\ref{lem:full_uniform} directly.





\subsection{Best practices for NAS research}
The area of NAS research has had issues with reproducibility and fairness in empirical
comparisons~\citep{randomnas, nasbench}, and there is now a checklist for best 
practices~\citep{lindauer2019best}. In order to promote best practices, we discuss each
point on the list, and encourage all NAS research papers to do the same.

\begin{itemize}
    \item \textbf{Releasing code.}
    Our code is publicly available at\\ \url{https://github.com/realityengines/local\_search}.
    The training pipelines and search spaces are from popular existing NAS work:
    NASBench-101, NASBench-201, and DARTS.
    One thing that is missing is the set of random seeds we used for the DARTS experiments.
    \item \textbf{Comparing NAS methods.}
    We made fair comparisons due to our use of NASBench-101 and NASBench-201.
    For baseline comparisons, we used open-source code, a few times adjusting hyperparameters
    to be more appropriate for the search space.
    We ran ablation studies, compared to random search, and compared performance over time.
    We performed 200 trials on tabular benchmarks.
    \item \textbf{Reporting important details.}
    Local search only has two boolean hyperparameters, so we did not need to tune
    hyperparameters. We reported the times for the full NAS method and all details for
    our experimental setup.
\end{itemize}

 

\end{document}
