\documentclass{article} 

\usepackage{iclr2021_conference,times}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}   
\usepackage{framed}
\usepackage{tcolorbox}
\usepackage{wrapfig}
\usepackage{color}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[english]{babel}
\usepackage{mathrsfs}
\usepackage{multirow}
\usepackage{pifont}
\newcommand{\xmark}{\ding{55}}

\usepackage{threeparttable}
\usepackage{latexsym}
\usepackage{natbib}
\def\given{\,|\,}
\def\biggiven{\,\big{|}\,}
\def\Biggiven{\,\Big{|}\,}
\def\bigggiven{\,\bigg{|}\,}
\def\tr{\mathop{\text{tr}}\kern.2ex}
\def\tZ{{\tilde Z}}
\def\tX{{\tilde X}}
\def\tR{{\tilde r}}
\def\tU{{\tilde U}}
\def\P{{\mathbb P}}
\def\Q{{\mathbb Q}}
\def\E{{\mathbb E}}
\def\B{{\mathbb B}}
\def\X{{\mathcal X}}
\def\Y{{\mathcal Y}}
\def\Z{{\mathcal Z}}
\def\vd{\textsf{VD}}
\def\bias{\textsf{Bias}}


\def\ny{{\tilde{Y}}}


\def\sign{\mathop{\text{sign}}}
\def\supp{\mathop{\text{supp}}}
\def\card{\mathop{\text{card}}}
\def\rank{\mathrm{rank}}
\long\def\comment#1{}
\def\skeptic{{\sc skeptic}}
\def\NPN{\bPiox{\it NPN}}
\def\vec{\mathop{\text{vec}}}
\def\tr{\mathop{\text{Tr}}}
\def\trc{\mathop{\text{TRC}}}
\def\cS{{\mathcal{S}}}
\def\dr{\displaystyle \rm}
\def\skeptic{{\sc skeptic}}
\providecommand{\bnorm}[1]{\Big\vvvert#1\Big\vvvert}
\providecommand{\norm}[1]{\vvvert#1\vvvert}

\newcommand{\bel}{}
\newcommand{\bes}{}
\def\bess{\bes\small }
\def\Shat{{\widehat S}}
\def\lam{\rho}
\def\real{{\mathbb{R}}}
\def\R{{\real}}
\def\Ti{T_{\rm init}}
\def\Tm{T_{\rm main}}
\def\bcdot{{\bm \cdot}}
\def\ud{\, \text{d}}

\newcommand{\red}{\color{red}}
\newcommand{\blue}{\color{blue}}
\newcommand{\la}{\langle}
\newcommand{\ra}{\rangle}
\newcommand{\cIs}{\cI_{\hat{s}}}
\newcommand{\F}{{\rm F}}
\newcommand{\BR}{{\mathbbm 1}}
\newcommand{\IM}{{\textsf{FIT}}}

\newcommand{\squishlist}{
\begin{list}{{{\small{}}}}
{\setlength{\itemsep}{3pt}      \setlength{\parsep}{1pt}
\setlength{\topsep}{1pt}       \setlength{\partopsep}{0pt}
\setlength{\leftmargin}{1em} \setlength{\labelwidth}{1em}
\setlength{\labelsep}{0.5em} } }
\newcommand{\squishend}{  \end{list}  }


\newcommand{\tran}{{g}}
\let\oldemptyset\emptyset
\let\emptyset\varnothing

\newcommand{\op}{o_{\raisemath{-1.5pt}\PP}}
\newcommand{\Op}{O_{\raisemath{-1.5pt}\PP}}

\usepackage[]{mathtools} 
\def\##1\#{}
\def\{}
\usepackage{enumitem}
\usepackage[]{amsthm}
\usepackage[]{amssymb}
\usepackage{bbm}
\usepackage{hyperref}
\usepackage{url}
\usepackage{xcolor}

\newtheorem{theorem}{Theorem}
\newtheorem{assumption}{Assumption}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}

\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}{Example}


\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\newcommand{\PP}{P}
\newcommand{\QQ}{Q}
\newcommand{\RR}{\mathbb R}
\newcommand{\Hy}{\mathcal{H}}
\newcommand{\A}{\textsf{A}}
\newcommand{\FNR}{\textsf{FNR}}
\newcommand{\FPR}{\textsf{FPR}}
\newcommand{\TPR}{\textsf{TPR}}
\newcommand{\f}{f^*}
\newcommand{\rf}{\hat{f}}

\newcommand{\D}{\mathcal D}
\newcommand{\x}{\mathbf{x}}
\newcommand{\nY}{\tilde{Y}}

\newcommand{\yl}[1]{\textbf{\color{blue}(Yang: #1)}}
\newcommand{\wjh}[1]{\textbf{\color{purple}(Jiaheng: #1)}}


\title{When Optimizing  -divergence is Robust \\with Label Noise}

\author{Jiaheng Wei ~and~ Yang Liu\thanks{Corresponding author. \texttt{yangliu@ucsc.edu}.} \\
Department of Computer Science and Engineering\\
University of California, Santa Cruz\\
Santa Cruz, CA 95060, USA \\
\texttt{\{jiahengwei, yangliu\}@ucsc.edu} \\
}

\begin{document}
\iclrfinalcopy

\maketitle
\begin{abstract}
    We show when maximizing a properly defined -divergence measure with respect to a classifier's predictions and the supervised labels is robust with label noise. Leveraging its variational form, we derive a nice decoupling property for a family of -divergence measures when label noise presents, where the divergence is shown to be a linear combination of the variational difference defined on the clean distribution and a bias term introduced due to the noise. The above derivation helps us analyze the robustness of different -divergence functions. With established robustness, this family of -divergence functions arises as useful metrics for the problem of learning with noisy labels, which do not require the specification of the labels' noise rate. When they are possibly not robust, we propose fixes to make them so. In addition to the analytical results, we present thorough experimental evidence. Our code is available at \url{https://github.com/UCSC-REAL/Robust-f-divergence-measures}.
\end{abstract}



\section{Introduction}

A machine learning system continuously observes noisy training annotations and it remains a challenge to perform robust training in such scenarios. Earlier and classical approaches rely on estimation processes to understand the noise rate of the labels and then leverage this knowledge to perform label correction \citep{patrini2017making,lukasik2020does}, or loss correction \citep{natarajan2013learning,Ltl_2015_reweighting,patrini2017making}, or both, among many other more carefully designed approaches (please refer to our related work section for more detailed coverage). Recent works have started to propose robust loss functions or metrics that do not require the above estimation \citep{charoenphakdee2019symmetric,xu2019l_dmi,liu2019peer,cheng2020learning}. Clear advantages of the latter approaches include their easiness in implementation, as well as their robustness to noisy estimates of the parameters. This work mainly contributes to the second line of studies and aimed to propose relevant loss functions and measures that are inherently robust with label noise. 

We start with formulating the problem of maximizing an -divergence defined between a classifier's prediction and the labels:

where in above  is an -divergence function,  and  are the joint and product (marginal) distribution of the classifier 's predictions on a feature space  and label . Though optimizing the -divergence measure is in general not the same as finding the Bayes optimal classifiers, we show these measures encourage a classifier that maximizes an extended definition of -mutual information between the classifier's prediction and the true label distribution. We will also provide analysis for when the maximizer of this -divergence coincides with the Bayes optimal classifier.


Building on a careful treatment of its variational form, we then reveal a nice property that helps establish the robustness of the -divergence specified in Eqn. (\ref{eqn:fd}): the variational difference term defined with noisy labels is an affine transformation of the clean variational difference, subject to an addition of a bias term. Using this result, we analyze under which conditions maximizing an -divergence measure would be robust to label noise. In particular, we demonstrate strong robustness results for Total Variation divergence, identify conditions under which several other divergences, including Jenson-Shannon divergence and Pearson  divergence, are robust.  The resultant -divergence functions offer ways to learn with noisy labels, without estimating the noise parameters. As mentioned above, this distinguishes our solutions from a major line of previous studies that would require such estimates. When the -divergence functions are possibly not robust with label noise, our analysis also offers a new way to perform ``loss correction". We'd like to emphasize that instead of offering one method/loss/measure, our results effectively offer a family of functions that can be used to perform this noisy training task. Our contributions summarize as follows:
\squishlist    
\item We show a certain set of -divergence measures that are robust with label noise (some under certain conditions). The corresponding -divergence functions provide the community with robust learning measures that do not require the knowledge of the noise rates. 
    \item When the -divergence measures are possibly not robust with label noise, our analysis provides ways to correct the -divergence functions to offer robustness.
    This process would require the estimation of the noise rates and our results contribute new ways to leverage existing estimation techniques to make the training more robust. 
    \item We empirically verified the effectiveness of optimizing -divergences when noisy labels present. We opensource our solutions at  \url{https://github.com/UCSC-REAL/Robust-f-divergence-measures}.
    \squishend
\subsection{Related works}

The now most popular approach of dealing with label noise is to first estimate the noise transition matrix and then use this knowledge to perform loss or sample correction  \citep{scott2013classification,natarajan2013learning,patrini2017making,lu2018minimal,han2018co,tanaka2018joint,yao2020dual,zhu2021clusterability}. In particular, the surrogate loss \citep{scott2013classification,natarajan2013learning,scott2015rate,van2015learning,menon2015learning}  uses the transition matrix to define unbiased estimates of the true losses. 
Other works include \citep{sukhbaatar2014learning,xiao2015learning}, which consider building a neural network to facilitate the learning of noise rates or noise transition matrix. Symmetric loss has been studied and conditions have been identified for when there is no need to estimate noise rate \citep{manwani2013noise,ghosh2015making,ghosh2017robust,van2015learning,charoenphakdee2019symmetric}. Nonetheless, it remains a challenge to develop training approaches without requiring knowing the noise rates for more generic settings.

More recently, \citep{zhang2018generalized,amid2019robust} proposed robust losses for neural networks. 
When noise rates are asymmetric (label class-dependent), \citep{xu2019l_dmi} proposed an information-theoretic loss that is also robust to asymmetric noise rates. There are also some trials on modifying the regularization term to improve generalization ability with the existence of label noise \citep{jenni2018deep,Yi_2019_CVPR}, and on providing complementary negative labels \citep{kim2019nlnl}.
Peer loss \citep{liu2019peer} is a recently proposed loss function that does not require knowing noise rates. 

-divergence is a popular information theoretical measure, and has been widely used and studied.

Most relevant to us, -GAN was proposed in \citep{nowozin2016f} to study -divergence in training generative neural samplers.
To our best knowledge, ours is the first to study the robustness of -divergence measures in the context of improving the robustness of training with noisy labels.  \section{Learning with noisy labels using -divergence}


Our solution ties to the definition of -divergence. The -divergence between two distributions  and  with probability density function  and  being measures for \footnote{We use  instead of  as conventionally done for a good reason - we will be reserving  to explicitly denote the features.} is defined as: 

 is a convex function such that . Examples include KL-divergence when  and Total Variation (TV) divergence with . Other examples can be found in Table \ref{table:f_div}. 
Following from Fenchel's convex duality, -divergence admits the following variational form:

where  is the Fenchel duality of the function , which is defined as . We use  to denote the domain of .

We consider the classification problem of learning a classifier  that maps features  to labels , where in above  denote the random variables for features and labels.  jointly draw from a distribution . For a clear presentation, we will often focus on presenting the binary classification setting , but most of our core results extend to multi-class classification problems, and we shall provide corresponding justifications. 

Instead of having access to sampled training data from , we consider a setting  with noisy labels where the noisy label  generates according to a transition matrix  defined between  and the true label . The  element of  is defined as  where . For the ease of presentation, when we present for the binary case, we adopt the following notation:
 Suppose we have access to a noisy training dataset , where  generates according to .


\subsection{Learning using }


We will start with presenting our idea of training a classifier using  with the clean training data. Then we will proceed to the case with noisy labels. For an arbitrary classifier , let's denote by  the joint distribution of  and :

And we use  to denote the product (marginal) distribution of  and :

When it is clear from context we will also shorthand the above two distributions as  and . We formulate the problem of learning using -divergence as follows: the goal of the learner is to find a classifier  that maximizes the following divergence measure between  and :

Effectively the goal is to find a classifier that maximizes the divergence between the joint distribution and the product distribution. Define a -mutual information based on -divergence:
\underline{}
, equivalently the maximization in Eqn. (\ref{eqn:fmi}) tries to find the classifier that maximizes the -mutual information between a classifier's output distribution and the true label distribution. 
A notable example is when , the corresponding  and  become the famous KL divergence and the mutual information. It is important to note in general maximizing (-) mutual information between the classifier's predictions and labels does not promise the \underline{Bayes optimal classifier }. Nonetheless, maximizing it often returns a quality one. We provide further analysis in Section \ref{sec:hf}.

\paragraph{Variational representation} As we mentioned earlier, -divergence admits a variational form which further allows us to focus on maximizing the following variational difference:

where we use  to shorthand the tuple . Denote the variational difference as follows:

Let  be the corresponding optimal variational function  for .
This variational form allows us to use a training dataset  to perform the above maximization problem listed in Eqn. (\ref{eqn:fmi}) \citep{nowozin2016f}. A list of -divergence functions together with the optimal variational/conjugate functions  is summarized in Table \ref{table:f_div}.

\begin{table*}[!ht]
\tiny
\begin{center}
\begin{tabular}{ l l l l l} 
 \hline
 Name &   &  &  &  \\ 
 \hline
 Total Variation &  &   &  & \\
 Jenson-Shannon &  &  &   & \\
 Pearson   &  &  &  &  \\
 KL  &   & & & \\
\hline
\end{tabular}
\end{center}
\caption{s, optimal variational  (), conjugate functions (). A more complete table, including Jeffrey, Squared Hellinger, Neyman , Reverse KL, is provided in the Appendix.}
\label{table:f_div}
\end{table*}

\subsection{How good is ?}
\label{sec:hf}

As we mentioned earlier, maximizing our defined -divergence measures (or maximizing the -mutual information) between the classifier's predictions and labels is not always returning the Bayes optimal classifier. However, for a binary classification problem, we prove below that with balanced dataset,  maximizing Total Variation (TV) divergence returns the Bayes optimal classifier:

\begin{theorem}\label{thm:TV}
For TV, when  (balanced), 
 is the Bayes optimal classifier. 
\end{theorem}
\begin{remark}
The above theorem extends to the multi-class setting when we restrict attentions to confident classifiers. See Appendix for details.
\end{remark}
\vspace{-0.03in}
The above observation is not easily true for other -divergence. Nonetheless, denote by  the Bayes optimal label for an instance : . Denote by  the joint and product distribution  defined w.r.t.  and . We prove:
\begin{theorem}\label{thm:bayes}
When  (balanced), maximizing  returns the Bayes optimal classifier, if  is monotonically increasing in  on .
\end{theorem}
\vspace{-0.03in}
For example, Pearson  () satisfies the monotonicity condition.
In practice, when the label distribution  has small uncertainties, the ground truth labels are approximately equivalent to the Bayes optimal label. Therefore, the above theorem implies that maximizing  is also likely to return a high-quality classifier for other -divergences.

 
\subsection{Learning with noisy labels}
 Consider an arbitrary classifier . Denote by  the joint distribution of  and :

\vspace{-0.03in}
Similarly, we use  to denote the product (marginal) distribution of  and :

\vspace{-0.03in}
When it is clear from context, we shorthand using . We are interested in understanding the robustness in \underline{maximizing }. Using training samples , there exists algorithms to compute the gradient of  leveraging its variational form \citep{nowozin2016f}, such that one can apply gradient descent or ascent to optimize it. We provide details in Section \ref{sec:exp}.

\section{Variational difference with noisy labels}
\label{sec:vd}

 For an arbitrary , we define the variational difference term w.r.t. the noisy label as follows:

where we use  to denote . Denote by  the corresponding optimal variational function  for . In this section, we show that the variational difference term under noisy labels is closely related to the variational difference term defined on the clean distributions . Define the following quantity:
\underline{}
for example  
For a binary classification problem, further denote by \underline{
}
We derive the following fact:
\begin{theorem}\label{thm:variational}
For binary classification, the variational difference between the noisy distributions  and  relates to the one defined on the clean distributions in the following way:

\end{theorem}

The above decoupling result is inspiring:  can be viewed as the additional bias term introduced by label noise. If this term has negligible effect in the maximization problem, maximizing the noisy variational difference term will be equivalent to maximizing , and therefore the clean variational difference term. 

If the above is true, we have established the robustness of the corresponding -divergence. This result also points out that when the effects from the bias term are non-negligible, finding ways to counter the additional bias term will help us retain the robustness of  measures. Next we show that Theorem \ref{thm:variational} extends to the multi-class setting under two broad families of noise rate models, both covering the binary setting as a special case.

\paragraph{Multi-class extension of Theorem \ref{thm:variational}: uniform off-diagonal case}
We first consider the following transition matrix: uniform off-diagonal transition matrix, where , that is any other classes  has the same chance of being flipped to class . The diagonal entry  (chance of a correct label) becomes . We further require that . Note that the binary noise rate model is easily a uniform off-diagonal  transition matrix. 


\begin{theorem}\label{thm:multi1}
[Multi-class] For uniform off-diagonal noise transition model, the noisy variational difference term relates to the clean one in the following way:

\end{theorem}
If we define \underline{}, we reproduced the results in Theorem \ref{thm:variational}: for binary case, relabel class . Then . Another case of noise model we consider is \emph{sparse noise}. Mathematically, assume  is an even number, sparse noise model specifies  disjoint pairs of classes  where  and . The labels flip between each pair. We provide details in the Appendix. 
 \section{When  is robust with label noise}

Denote by  an arbitrary hypothesis space for training a candidate classifier . We will focus on  throughout this section, and with abusing notation a bit, let  
We first define formally what we mean by robustness of . 
\begin{definition}
 is -robust if 

\end{definition}
The above definition is stating that the label noise does not disrupt the optimality of  when maximizing  instead of .


\subsection{Impact of the  terms}\label{sec:bias}
In this section, we take a closer look at the  terms and argue that they have diminishing effects as compared to the  terms when label noise increases. Recall  are the corresponding optimal variational functions for  and .
\paragraph{Total variation (TV)}
For TV, since  , , we immediately have 
\underline{} 
and therefore , and further . This fact helps establish the robustness of TV divergence measure (Theorem \ref{thm:tv}).


\paragraph{Other divergences} The above nice property generally does not hold for other -divergence functions. Next we focus on the binary classification setting and prove the following lemma:
\begin{lemma}
\label{lm: bias}
For -divergence listed in Table \ref{table:f_div-full} (Appendix), .
\end{lemma}
\vspace{-0.05in}
Note the variational form will be used when optimizing   (and therefore we will be using ). This lemma simplifies Eqn. \ref{thm:variational} to .
Since , when the noise rate  is high, the effect of  term diminishes. When the  term becomes negligible, we will have  if , establishing the fact that optimizing  is approximately the same as optimizing .


\subsection{How robust are s?}\label{sec:robust}
We first prove the following result:
\begin{theorem}\label{thm:main}
  is -robust when  satisfies either of the following conditions: (I) , ; (II) , .\end{theorem}
\vspace{-0.05in}
Theorem \ref{thm:main} gives sufficient conditions when the  term does not get in the way of reaching the optimality . Intuitively, when  is an upper bound of , the  term will not interfere with the convergence of the  term.
Next we provide specific examples of -divergence functions that would satisfy these conditions.  

\paragraph{Total Variation (TV) is robust} For TV, the fact that  allows us to prove:

\begin{theorem}\label{thm:tv}
For TV divergence,  and  is -robust with label noise for any arbitrary hypothesis space . 
\end{theorem}
This result establishes TV as a strong measure that does not require specifying the noise rates.

\paragraph{Divergences that are conditionally robust}
Other divergences functions do not enjoy the above nice property as TV has. The robustness of these functions need more careful analysis. Define the following measures that capture the degree a classifier fits to a particular label distribution:
\begin{definition}The fitness of  to  is defined as .
\label{def: IM}
\end{definition}
 measures capture the degree of fit of the classifier to the corresponding label distribution. A high  (same label) 
indicates a potential overfit to the noisy label. Denote by 

The  in the ``" above corresponds to the  for a random classifier.  contains the classifiers that are likely to overfit to the noisy labels. We argue, and also as observed in training, that  is the set of classifiers the training should avoid converging to, especially when the training only sees noisy labels.
Suppose  (balanced clean labels) and  (symmetric noise rate), we have the following theorem for binary classification:

\begin{theorem} \label{thm:robust}
-divergences listed in Table \ref{table:f_div-full} (Appendix, except for Jeffrey) are  -robust. 
\end{theorem}


\subsection{Making  measures robust to label noise} 

For the general case, to further improve robustness of  measures, we will need to estimate the noise rates (e.g., ) and then subtract  from the noisy variational difference term to correct the bias introduced by the noisy labels. As a corollary of Theorem \ref{thm:variational} we have:
\begin{corollary}\label{coro:biascorrection}
Maximizing the following bias-corrected  defined over  and  leads to 

\end{corollary}
By removing the  term, maximizing  becomes the same with maximizing the divergence defined on the clean distribution . The Corollary follows trivially from this fact. The calculation of the  terms will require the inputs of noise rates. Our work does not intend to particularly focus on noise rate estimation. But rather, we can leverage the existing results in performing efficient noise rate estimation.  
There are existing literature on estimating noise rates
(noise transition matrix) which can be implemented without the need of ground truth labels. For interested readers, please refer to \citep{Ltl_2015_reweighting, menon2015learning, Scott_kernel_embedding, patrini2017making, LNM_loss_correction,yao2020dual,zhu2021clusterability}. We will test the effectiveness of this bias correction step in Section \ref{sec:exp}.

 \section{Experiments}\label{sec:exp}
In this section, we validate our analysis of  measures' robustness via a set of empirical evaluations on 5 datasets: MNIST (\cite{lecun1998gradient}), Fashion-MNIST (\cite{xiao2017fashionmnist}), CIFAR-10 and CIFAR-100 (\cite{krizhevsky2009learning}), and Clothing1M (\cite{xiao2015learning}). Omitted experiment details are available in the appendix.

\vspace{-0.1in}
\paragraph{Baselines} We compare our approach with five baseline methods: \textbf{Cross-Entropy (CE)}, \textbf{Backward (BLC) and Forward Loss Correction (FLC)} methods as introduced in \citep{patrini2017making}, the \textbf{determinant-based mutual information (DMI)} method introduced in \citep{xu2019l_dmi} and \textbf{Peer-Loss (PL)} functions in \citep{liu2019peer}. BLC and FLC methods require estimating the noise transition matrix. 
DMI  and  PL are approaches that do not require such estimation.
\vspace{-0.1in}
\paragraph{Noise model} We test three types of noise transition models: uniform noise, sparse noise, and random noise. All details of the noise are in the Appendix. Here we briefly overview them. The uniform and sparse noise are as specified at the end of Section 3 for which our theoretical analyses mainly focus on. The noise rates of low-level uniform noise and sparse noise are both approximately 0.2 (the average probability of a label being wrong). 
The high-levels are about 0.55 and 0.4 respectively. In the random noise setting, each class randomly flips to one of 10 classes with probability  (Random ). For CIFAR-100, the noise rate of uniform noise is about 0.25. The sparse label noise is generated by randomly dividing 100 classes into 50 pairs, and the noise rate is about 0.4. 

\vspace{-0.02in}
\paragraph{Optimizing  using noisy samples}
With the noisy training dataset , we optimize  using gradient ascent of its variational form. Sketch is given in Algorithm \ref{alg:main1}. For the bias correction version of our algorithm, the gradient will simply include the . The variational function  can be updated progressively or can be fixed beforehand using an approximate activation function for each  (see e.g., \citep{nowozin2016f}). 
\begin{algorithm}
\caption{Maximizing  measures: one step gradient}\label{alg:main1}
\begin{algorithmic}[1]
\STATE \textbf{Inputs}: Training data , , variational function , conjugate , classifier . 
\STATE Randomly sample three mini-batches , ,  from . : simulate samples ;  
 to simulate . 
\STATE Use  to denote model prediction on  for label ,   to denote the empirical sample mean calculated using the mini-batch data.
\STATE At step , update  by ascending its stochastic gradient with learning rate : 
 
Tips: In practice, we suggest (also implemented in our experiments) using the fixed form of  which appears as  in Table \ref{table:f_div-full} (appendix).
\end{algorithmic}
\end{algorithm}


\subsection{How good is  on clean data}
As a supplementary of Section \ref{sec:hf}, we validate the quality of  on clean dataset of MNIST, Fashion MNIST, CIFAR-10 and CIFAR-100. In experiments, since the estimation of product noisy distribution are unstable when trained on CIFAR-100 training dataset, we use CE as a warm-up (120 epochs) and then switch to train with  measures. For other datasets, we train with  measures without the warm-up stage. Results in Table \ref{Tab:hg} demonstrate that optimizing divergence on clean dataset returns a high-quality  by referring to the performance of CE. Even though  measures can't outperform CE on clean dataset, we do observe that the gap between CE and  measures are negligible, for example, the largest gap of Total-Variation (TV) is only  among four datasets.  


\begin{table*}[!ht]
\scriptsize
\centering
\begin{threeparttable}
\begin{tabular}{c|c|c|c|c|c|c|c}
\hline
Dataset &  CE &  \textbf{TV} & Gap & \textbf{J-S} & Gap & \textbf{KL} & Gap \\ \hline\hline
\multirow{1}{*}{MNIST}
& 99.39(99.380.01) & 99.37(99.340.02) &{\color{blue}\textbf{-0.04}}  & 99.35(99.310.04) & {\color{blue}\textbf{-0.07}} & 99.31(99.210.06) & {\color{blue}\textbf{-0.17}}
\\ 
\hline
\multirow{1}{*}{Fashion MNIST}
& 90.44(90.340.12) & 89.98(89.940.06) & {\color{blue}\textbf{-0.40 }}& 90.40(90.170.24)  & {\color{blue}\textbf{-0.17}} & 90.19(89.960.14) & {\color{blue}\textbf{-0.38}}
\\ 
\hline
\multirow{1}{*}{CIFAR-10}
& 93.58(93.470.08) & 92.80(92.660.13) &{\color{blue}\textbf{-0.81}} & 92.35(92.230.07) & -1.24 & 90.55(90.380.15) & -3.09
\\ 
\hline
\multirow{1}{*}{CIFAR-100}
& 73.47(73.390.05) & 73.43(73.390.06) & {\color{blue}\textbf{0.00}} & 73.47(73.260.17)& {\color{blue}\textbf{-0.13}} & 73.33(73.160.10) & {\color{blue}\textbf{-0.23}}
\\ 
\hline
\end{tabular}
\end{threeparttable}
\caption{Experiment results comparison on clean datasets: We report the maximum accuracy of CE and each  measures along with (mean  standard deviation); Gap: mean performance comparison w.r.t. CE. Numbers highlighted in \color{blue}\textbf{{blue}} \color{black} indicate the gap is less than 1\%.
}
\label{Tab:hg}
\end{table*}

\subsection{Robustness of  measures}
\begin{figure}[ht]
\vspace{-0.2in}
    \centering
    {\includegraphics[width=.6\textwidth]{figures/CIFAR_fig1.pdf}
    }
        \vspace{-5pt}
        \caption{Robustness of TV, JS, PS divergences. 
        \vspace{-15pt}
    }
    \label{fig: robust_f_div}
\end{figure}
As a demonstration, we apply the uniform noise model to CIFAR-10 dataset to test the robustness of three  measures: Total-Variation (TV), Jenson-Shannon (JS) and Pearson (PS). We trained models with  measures using Algorithm \ref{alg:main1} on 10 noise settings with an increasing noise rate from 0\% to approximately 81\%. The visualization of the  values and accuracy w.r.t. noise rates are shown in Figure \ref{fig: robust_f_div}. Both the  values and test accuracy are calculated on the reserved clean test data. We observe that almost all  measures are robust to noisy labels, especially when the percentage of noisy labels is not overwhelmingly large, e.g., . Note that the curves for other -divergences are almost the same as the curve of total variation (TV), which is proved to be robust theoretically. This partially validates the analytical evidences we provided for the robustness of other -divergences in Section \ref{sec:bias} and \ref{sec:robust}. 

\subsection{Performance Evaluation and Comparison}

From Table \ref{Tab:Experiment_Results_no_bias}, several  measures arise as competitive solutions in a variety of noise scenarios. Among the proposed -divergences, Total Variation (TV) has been consistently ranked as one of the top performing method. This aligns also with our analyses that TV is inherently robust. For most settings, the presented -divergences outperformed the baselines we compare to, while they fell short to DMI (once) and Peer Loss (5 times) on several cases, particularly when the noise is sparse and high.
The sparse high noise setting tends to be a challenging setting for all methods. We conjecture this is because sparse high noise setting creates a highly imbalanced dataset, model training is more likely to converge to a ``sub-optimal" early in the training process. It is also possible that with sparse noise, the impact of  terms becomes non-negligible.  We do observe better performances with very careful and intensive hyper-parameter tuning, but the results are not confident and we chose to not report it. Fully understanding the limitation of our approach in this setting remains an interesting on-going investigation. 


In Table \ref{Tab:Experiment_Results_bias} (full details on MNIST and Fashion MNIST can be found in Appendix), we use noise transition estimation method in (\cite{patrini2017making}) to estimate the noise rate. The estimates help us define the bias term and perform bias correction for  measures. We observe that while adding bias correction can further improve the performance of several divergence functions (Gap being positive), the improvement or difference is not significant. This partially justified our analysis of the bias term, especially when the noise is dense and high (uniform and random high).

\begin{table*}[!ht]
\centering
\tiny
\begin{threeparttable}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c}
\hline
Dataset & Noise & CE & BLC & FLC & DMI& PL & \textbf{TV} & \textbf{J-S} & \textbf{KL} \\ \hline\hline

\multirow{5}{*}{MNIST}             
& Sparse, Low   & 97.21  & 95.23 & 97.37 & 97.76  & 98.59 & {\color{blue}\textbf{99.23(99.110.08)}} & {\color{blue}\textbf{99.15(99.030.09)}} & {\color{blue}\textbf{99.21(99.150.05)}}  \\ \cline{2-10} 
& Sparse, High  & 48.55 & 55.86 & 49.67 &  49.61 & {\color{blue}\textbf{60.27}} & 58.27(54.724.36) & 58.93(55.801.93) & 49.24(49.170.06)  \\ \cline{2-10} 
& Uniform, Low  & 97.14 & 94.27 & 95.51 & 97.72  & 99.06 & {\color{blue}\textbf{99.23(99.170.05)}} & {\color{blue}\textbf{99.1(99.080.04)}}& {\color{blue}\textbf{99.13(99.060.07)}}    \\ \cline{2-10} 
& Uniform, High & 93.25 & 85.92 & 87.75 & 95.50  & 97.77& {\color{blue}\textbf{98.09(97.960.13)}} & {\color{black}\text{97.86(97.710.10)}} &{\color{blue}\textbf{98.14(97.880.18)}} \\\cline{2-10}  
& Random (0.2) & 98.26 & 97.46 & 97.61 & 98.82  & 99.25 & 99.26(99.190.05) & {\color{blue}\textbf{99.29(99.270.02)}} & 99.26(99.190.06) \\ \cline{2-10} 
& Random (0.7) & 97.00 & 93.52 & 87.74 & 95.47  & 98.52 & {\color{blue}\textbf{98.81(98.730.06)}} & {\color{blue}\textbf{98.72(98.630.08)}} & {\color{blue}\textbf{98.76(98.650.10)}} \\ \cline{2-10} 
\hline\hline

\multirow{5}{*}{\begin{tabular}[c]{@{}l@{}}Fashion\\ MNIST\end{tabular}} 
& Sparse, Low   & 84.36 & 86.02 & 88.15 & 85.65  & 88.32 & {\color{blue}\textbf{89.74(89.340.33)}} & {\color{blue}\textbf{88.80(88.790.01)}} &  {\color{blue}\textbf{89.77(89.420.34)}}\\ \cline{2-10} 
& Sparse, High  & 43.33 & 46.97 & 47.63 & 47.16 & {\color{blue}\textbf{51.92}} & 45.66(45.220.26) & 47.46(46.390.70) & 38.96(38.900.06)\\ \cline{2-10} 
& Uniform, Low  & 82.98 & 84.48 & 86.58 &  83.69  & {\color{blue}\textbf{89.31}} & 89.00(88.750.16) & 88.58(88.460.18) &   88.32(88.160.11)  \\ \cline{2-10} 
& Uniform, High & 79.52 & 78.10 & 82.41 & 77.94 & 84.69 & {\color{blue}\textbf{85.58(85.070.31)}} & {\color{blue}\textbf{85.62(85.39 0.33)}} &  {\color{blue}\textbf{85.69(85.430.30)}} \\\cline{2-10}  
  
& Random (0.2) & 85.47 & 83.40 & 77.61 & 86.21  & 89.78 & {\color{blue}\textbf{90.22(90.090.19)}} & 89.73(89.430.24) & 89.24(89.050.14) \\ \cline{2-10} 
& Random (0.7) & 82.05 & 78.41 & 73.42 & 80.89  & 87.22 & 86.69(86.490.16) & {\color{blue}\textbf{87.79(87.330.29)}} & 87.06(87.000.06) \\ \cline{2-10} 


\hline\hline

\multirow{5}{*}{CIFAR-10}             
& Sparse, Low   & 87.20    & 72.96    & 76.17   & {\color{blue}\textbf{92.32}}  &   91.35   &  91.81(91.560.16) & 91.49 (91.430.08)  &   91.62(91.320.31) \\ \cline{2-10} 
& Sparse, High  & 61.81  & 56.30    & 66.12   & 27.94 & {\color{blue} \textbf{69.70}}        & 63.96(62.251.00) & 67.33(65.271.34) & 46.55(46.430.08)  \\\cline{2-10}
& Uniform, Low  & 85.68 & 72.73 & 77.12 & 90.39 & 91.70 & {\color{blue}\textbf{92.10(92.010.09)}} & 91.52(91.470.08) & {\color{blue}\textbf{92.26(92.080.12)}}   \\ \cline{2-10} 
& Uniform, High & 71.38  & 54.41 & 64.22 & 82.68 & 83.42 & {\color{blue}\textbf{85.56(85.440.08)}}  & {\color{blue}\textbf{84.49(84.350.13)}} &  {\color{blue}\textbf{84.36(84.190.13)}}  \\ \cline{2-10}  
& Random (0.5) & 78.40 & 59.31 & 68.97 & 85.06 & 86.47  & {\color{blue}\textbf{87.28(87.030.17)}} & {\color{blue}\textbf{86.92 (86.800.10)}}  &   {\color{blue}\textbf{86.93(86.850.11)}}  \\ 
\cline{2-10}  
& Random (0.7) & 68.26 & 38.59  &  54.39 &  77.91  & 57.81 &  {\color{blue}\textbf{80.59(80.450.10)}} & {\color{blue}\textbf{80.50(80.270.15)}} &  {\color{blue}\textbf{78.93(78.590.30)}}   \\ \hline\hline
\multirow{5}{*}{CIFAR-100}
& Uniform & 63.87 &  51.40& 60.04 & 64.39  & 67.94 & {\color{blue}\textbf{69.15(68.900.17)}} & {\color{blue}\textbf{69.13(68.800.21)}} &   {\color{blue}\textbf{68.79(68.600.11)}}\\ \cline{2-10}
& Sparse & 40.45  & 36.57 & 43.39 & 40.53  & {\color{blue}\textbf{44.25}} & 42.45(38.062.82) & 38.09(38.000.08) & 37.74(37.630.08) \\ \cline{2-10}
& Random (0.2) & 65.84  & 61.21 & 61.52 & 66.23  & 62.92 & {\color{blue}\textbf{70.43(70.220.13)}} & {\color{blue}\textbf{70.40(70.120.21)}} &  {\color{blue}\textbf{70.28(70.060.14)}}  \\ \cline{2-10}
& Random (0.5) & 56.92  & 22.21 & 55.88 & 56.06  & 49.62  & {\color{blue}\textbf{62.14(61.890.18)}} & {\color{blue}\textbf{61.58(61.150.27)}} &   {\color{blue}\textbf{61.68(61.490.13)}}  \\ \hline

\end{tabular}
\end{threeparttable}
\caption{Experiment results comparison (w/o bias correction): The best performance in each setting is highlighted in {\color{blue}\textbf{{blue}}}. \color{black}We report the maximum accuracy of each  measures along with (mean  standard deviation). All -divergences will be highlighted if their mean performances are better (or no worse) than all baselines we compare to. A supplementary table including Pearson  and Jeffrey (JF) is attached in Table \ref{Tab:Experiment_Results_no_bias_full} (Appendix).
}
\label{Tab:Experiment_Results_no_bias}
\end{table*}

\begin{table*}[!ht]
\tiny
\centering
\begin{threeparttable}
\begin{tabular}{c|c|c|c|c|c|c|c|c}
\hline
Noise &  \textbf{J-S} & Gap & \textbf{PS} & Gap & \textbf{KL} & Gap & \textbf{JF} & Gap \\ \hline\hline
Sparse, Low   & 91.23(90.930.34)  & -0.26& {\color{black} \text{91.48(91.120.42)}} & {\color{red}\textbf{+0.08}}&  {\color{black} \text{91.73(91.570.18)}}& {\color{red}\textbf{+0.11}}& {\color{black} \text{91.45(91.180.21)}}  & -0.10\\ \cline{1-9} 
Sparse, High   & 46.45(46.310.14) &-20.88 & 46.31(45.900.44) & -0.05 & 46.59(46.520.05) & {\color{red}\textbf{+0.04}} & 46.25(45.770.50) & {\color{red}\textbf{+0.04}}\\ \cline{1-9}
Uniform, Low    & {\color{blue} \textbf{92.16(92.090.09)}} & {\color{red}\textbf{+0.64}}& {\color{blue} \textbf{92.25(92.130.09)}} & -0.12&  {\color{black} \text{90.92(90.840.10)}} & -1.34& {\color{blue} \textbf{92.19(92.100.08)}} & {\color{red}\textbf{+0.02}}\\ \cline{1-9}
Uniform, High   & {\color{blue} \textbf{84.31(84.130.10)}} & -0.18& {\color{blue} \textbf{83.79(83.610.12)}}  & {\color{red}\textbf{+0.18}}& {\color{blue} \textbf{83.98(83.790.12)}} & -0.38& {\color{blue} \textbf{83.93(83.620.22)}} & {\color{red}\textbf{+0.13}}\\ \cline{1-9}
 \hline
\end{tabular}
\end{threeparttable}
\caption{ measures with bias correction on CIFAR-10: Numbers highlighted in \color{blue}\textbf{{blue}} \color{black} indicate better than all baseline methods; Gap: relative performance w.r.t. their version w/o bias correction (Table \ref{Tab:Experiment_Results_no_bias}); those in \color{red}\textbf{{red}} \color{black} indicate better than w/o bias correction.  
}
\label{Tab:Experiment_Results_bias}
\end{table*}



\paragraph{Clothing1M}
Clothing1M is a large-scale clothes dataset with comprehensive annotations and can be categorized as a feature-dependent human-level noise dataset. Although this noise setting does not exactly follow our assumption, we are interested in testing the robustness of our -divergence approaches. Experiment results in Table \ref{Tab:Experiment_Results_C1M} demonstrate the robustness of the  measures. TV and KL divergences have outperformed other baseline methods.


\begin{table*}[!ht]
\scriptsize
\centering
\begin{threeparttable}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c}
\hline
Dataset & Noise & CE & BLC & FLC & DMI& PL & T-V & J-S & Pear  & KL & Jeffrey \\ \hline\hline
\multirow{1}{*}{Clothing1M}
& Human Noise & 68.94 & 69.13 & 69.84 & 72.46  & 72.60  & {\color{blue}\textbf{73.09}} & 72.32 &  72.22 & {\color{blue}\textbf{72.65}}  & 72.46
\\ \cline{2-12}
\hline
\end{tabular}
\end{threeparttable}
\caption{Experiment results comparison on Clothing1M dataset.
}
\label{Tab:Experiment_Results_C1M}
\end{table*}
 
\section{Conclusion}
In this paper, we explore the robustness of a properly defined -divergence measure when used to train a classifier in the presence of label noise.
We identified a set of nice robustness properties for a family of -divergence functions. We also experimentally verified our findings. Our work primarily contributed to the problem of learning with noisy labels without requiring the knowledge of noise rate. Beyond this noisy learning problem, the derivation and analysis might be useful for understanding the robustness of -divergences for other learning tasks. 

\paragraph{Acknowledgement} This work is partially supported by the National Science Foundation (NSF) under grant IIS-2007951 and the Office of Naval Research under grant
N00014-20-1-22.

\newpage
\bibliographystyle{iclr2021_conference}
\bibliography{library,myref,noise_learning, estimate_noise_rate, f_div}

\newpage
\appendix

\section{Proof and additional theorems}
\subsection{Short notations}
Throughout the appendix, we will denote by  and . We will also shorthand  for  and  for .
\subsection{Full tables of Table \ref{table:f_div}}
In experiments, we adopt the output activation function  instead of optimal activation functions  by referring to the implementation of f-GAN \citep{nowozin2016f}.
\begin{table*}[!ht]
\scriptsize
\begin{center}
\begin{tabular}{ l l l l l} 
 \hline
 Name  &  &  &  & \\
 \hline
 Total Variation (\checkmark)  &  &  &  &  \\
 Jenson-Shannon (\checkmark) &  & &   & \\
 Squared Hellinger (\xmark)  & &  &  & \\
 Pearson   (\checkmark) &  &  &  &  \\
 Neyman   (\xmark)  & & &  & \\
 KL (\checkmark)  &  & & & \\
 Reverse KL (\xmark)  &  &  &   & \\
 Jeffrey (\checkmark)  & &  &  & \\
\hline
\end{tabular}
\end{center}
\caption{Exemplary output activation functions  (used for approximating , see e.g. \citep{nowozin2016f}), optimal activation functions, optimal conjugate functions (full table).  is the Lambert product log function. `' indicates that the divergence function (in practice) is robust to label noise and `\xmark'  means non-robust.}
\vspace{-10pt}
\label{table:f_div-full}
\end{table*}

\subsection{Main Results: Proof of Theorem \ref{thm:variational}}

\begin{proof}
First note 

The second term in the variational difference derives as:

Combining  and : the leading terms combine into

and the rest:

we proved the claim.
\end{proof}

\subsection{Proof of Theorem \ref{thm:multi1}: Multi-class extension I of Theorem \ref{thm:variational}}
\begin{proof}
Denote , , . 
We have:



Similar to the binary case, combining  and  we proved the claim.
\end{proof}


\subsection{Multi-class extension II of Theorem \ref{thm:variational}: sparse case}
For sparse transition matrix, assume  is an even number, sparse noise model specifies  disjoint pairs of classes  where  and . The diagonal entry  becomes . Suppose .
\begin{theorem}\label{thm:multi2}
[Multi-class extension II] In the scenario of sparse noise transition model, the variational difference between the noisy distributions  and  relates to the one defined over the clean distributions in the following way:

\end{theorem}
\begin{proof}

Similarly, we have:

Combining  and  we proved the claim.
\end{proof}



\subsection{Proof of Theorem \ref{thm:TV}: total-variation generates Bayes optimal}

\begin{proof}
For total variation, we have

We again present the main proof for the binary classification setting.

First note the following fact that

    and
    
have opposite signs. This is simply because 

Because of the above, there are four possible combinations of cases:
\paragraph{Case 1} :

Therefore, maximizing  total variation returns the Bayes optimal classifier , and the optimal value arrives at .

\paragraph{Case 2} : 

    
\paragraph{Case 3}
: This case is symmetrical to Case 2. 

\paragraph{Case 4}
This is symmetrical to Case 1:

 The optimal classifier is then the opposite of , but
 
 so the maximizer returns a smaller value compared to Case 1.
 
 
 \paragraph{Multi-class extension}
We provide arguments for the multi-class generalization. First note that
 
For any classifier  and for each , one of the following terms 
 
 must be non-negative as:
 
 
Our following derivation focuses on confident classifiers:
 \begin{definition}
 We call a classifier confident if for each label class , only one class  returns positive correlation:
 
 while for all other , we have

 \end{definition}
 This above definition is saying the classifier  is ``dominantly" confident in predicting one class for the each true label class.
 
For a given class , if all other classes  are negative in  , the total variation becomes:

Summing up, for a confident classifier, the total variation becomes (ignoring constant ):

where the last inequality is due to the fact that the Bayes optimal classifier selects the highest  for each .

\end{proof}

\subsection{Proof of Theorem \ref{thm:bayes}}
\begin{proof}
By definition of :

First we want to prove

This is because:


On the other hand

When , , therefore . Further

When , denote . We have

Therefore we proved

Because  is monotonically increasing in , we proved that

The last equality is because  always agrees with , so  and .

\end{proof}



\subsection{Proof of Theorem \ref{thm:main}: -robust}
\begin{proof}
The proofs for the multi-class case under uniform diagonal and sparse noise setting are entirely symmetrical due to Theorem \ref{thm:multi1} and \ref{thm:multi2}. We deliver the main idea for the binary case.

The proof for condition (I) is easy to see:

Now we prove the robustness of  under condition (II). Denote by  the classifier that maximizes  and 

But

which is a contradiction. 
\end{proof}





\subsection{Proof of Lemma \ref{lm: bias}: Impact of  term for different -divergences}
Denote , .
We first prove that
 approaches to 0 as a function of :
\begin{proposition}
\label{prop:pq}
When , 

\end{proposition}
\begin{proof}

 derives as

The last equation is satisfied because : 

Now focus on :

Putting everything up together:

When , we have

Therefore

\end{proof}


Next, we prove Lemma \ref{lm: bias}:
\begin{proof}
Shorthand .
    Denote . Next we prove   for different -divergences.
    
Because , we analyze each of the term in expectation: .
\paragraph{Jenson-Shannon} For Jenson-Shannon divergence, we have:

Using Taylor expansion we know




\paragraph{Squared Hellinger}






\paragraph{Pearson } 




\paragraph{Neyman } 



\paragraph{KL}


\paragraph{Reverse KL}




\paragraph{Jeffrey}

And



\end{proof}



\subsection{Proof of Theorem \ref{thm:tv}: -robustness of total-variation}
\begin{proof}
We present the binary derivation but it extends easily to the multi-class case.  


For TV, since  , , we immediately have 
 and therefore 
and further for the binary case

and for the multi-class case

Therefore . We then know TV is -robust for an arbitrary  using Theorem \ref{thm:main}.

TV's robustness can also be derived straightforwardly for binary classification:

That is for total variation, minimizing the -divergence between  and  is the same as minimizing the -divergence between  and the clean distribution .
The above proof generalizes to multi-class easily. 
For example for the uniform diagonal noise, we have

Similar argument holds for sparse noise too. 


\end{proof}

\subsection{When -divergence measure is -robust?}
\begin{theorem}
\label{thm:bias}
For binary classification, suppose  has the following form:

where . When  is monotonically decreasing as a function of  on both sides of  and , then . Further, according to Theorem \ref{thm:main}, the corresponding -divergence measure is -robust.
\end{theorem}
\begin{proof}
For binary case, when  and , we have .

Denote , we first prove . It is equivalent to prove, , :


Since  is monotonically decreasing as a function of , for ,



Thus, . According to Theorem \ref{thm:main}, the corresponding -divergence measure is -robust.

\end{proof}


\subsection{Proof of Theorem \ref{thm:robust}: Robustness of }

\begin{proof}
Earlier we proved Theorem
\ref{thm:bias}, next we show presented conditions in Eqn. (\ref{eqn:deltah}) and (\ref{eqn:delta*}) and  can be satisfied by the listed divergences:


The proof for  can be viewed as a special case of . The following derivations will therefore focus on  and will not repeat for . 

\item


\paragraph{Jenson-Shannon}
For Jenson-Shannon, we have , and 


Therefore,  

 satisfies the requirement specified in Theorem \ref{thm:bias}.



\paragraph{Squared-Hellinger}

For Squared-Hellinger, we have , and 


Therefore 

Clearly  satisfies the requirement specified in Theorem \ref{thm:bias}.

\paragraph{Pearson }




Correspondingly , which satisfies the requirement specified in Theorem \ref{thm:bias}. 
\paragraph{Neyman }
 
For Neyman  we have , and 


Since

Therefore 

And further we have

 satisfies the requirement specified in Theorem \ref{thm:bias}.




\paragraph{KL}

For KL, we have , and 


Therefore 

Clearly  satisfies the requirement specified in Theorem \ref{thm:bias}.


\paragraph{Reverse-KL}

For Reverse-KL, we have , and 


Therefore 

Clearly  satisfies the requirement specified in Theorem \ref{thm:bias}.

\end{proof}





 
\section{Supplementary experiment results}
In our experiment settings,  measures fail to work well on almost all sparse high noise setting. This is largely due to the super unbalanced noisy labels, e.g., for each pair, the ratio of samples between the two classes is in the range of . 
\subsection{Supplementary table of Table \ref{Tab:Experiment_Results_no_bias}: Methods comparison without bias correction}
In Table \ref{Tab:Experiment_Results_no_bias_full}, the performance of Pearson , Jeffrey divergence on MNIST, Fashion MNIST, CIFAR-10 and CIFAR-100 are included in Table \ref{Tab:Experiment_Results_no_bias_full}.

\begin{table*}[!ht]
\centering
\scriptsize
\begin{threeparttable}
\begin{tabular}{c|c|c|c|c|c|c|c|c}
\hline
Dataset & Noise & CE & BLC & FLC & DMI& PL  & \textbf{Pearson} & \textbf{Jeffrey} \\ \hline\hline

\multirow{5}{*}{MNIST}             
& Sparse, Low   & 97.21  & 95.23 & 97.37 & 97.76  & 98.59 &  {\color{blue}\textbf{99.24(99.070.16)}}&  {\color{blue}\textbf{99.24(99.110.08)}}\\ \cline{2-9} 
& Sparse, High  & 48.55 & 55.86 & 49.67 &  49.61 & {\color{blue}\textbf{60.27}} &  58.63(58.580.05) & 49.21(49.170.04) \\ \cline{2-9} 
& Uniform, Low  & 97.14 & 94.27 & 95.51 & 97.72  & 99.06 & {\color{black}\text{99.13(99.030.09)}}  & {\color{blue}\textbf{99.14(99.060.05)}} \\ \cline{2-9} 
& Uniform, High & 93.25 & 85.92 & 87.75 & 95.50  & 97.77& {\color{black}\text{97.89(97.760.10)}}   & {\color{blue}\textbf{97.94(97.800.12)}}  \\\cline{2-9} 
& Random (0.2) & 98.26 & 97.46 & 97.61 & 98.82  & 99.25 & {\color{blue}\textbf{99.28(99.270.02)}} & 99.29(99.220.11) \\ \cline{2-9} 
& Random (0.7) & 97.00 & 93.52 & 87.74 & 95.47  & 98.52 & {\color{blue}\textbf{98.70(98.540.10)}} & {\color{blue}\textbf{98.67(98.530.15)}} \\ \cline{2-9} 
\hline\hline

\multirow{5}{*}{\begin{tabular}[c]{@{}l@{}}Fashion\\ MNIST\end{tabular}} 
& Sparse, Low   & 84.36 & 86.02 & 88.15 & 85.65  & 88.32& {\color{blue}\textbf{88.93(88.810.11)}}  & {\color{blue}\textbf{88.96(88.680.21)}} \\ \cline{2-9} 
& Sparse, High  & 43.33 & 46.97 & 47.63 & 47.16 & {\color{blue}\textbf{51.92}} & 44.62(44.360.21)  &  45.57(45.26 0.28) \\ \cline{2-9} 
& Uniform, Low  & 82.98 & 84.48 & 86.58 &  83.69  & {\color{blue}\textbf{89.31}} & 87.27(87.150.09) &88.13(87.850.18)  \\ \cline{2-9} 
& Uniform, High & 79.52 & 78.10 & 82.41 & 77.94 & 84.69 & {\color{blue} \textbf{85.30(85.260.06)}} & 84.92(84.630.26) \\\cline{2-9}
& Random (0.2) & 85.47 & 83.40 & 77.61 & 86.21  & {\color{blue} \textbf{89.78}} &  89.65(89.440.21)  & 89.74(89.33+0.29) \\ \cline{2-9} 
& Random (0.7) & 82.05 & 78.41 & 73.42 & 80.89  & {\color{blue} \textbf{87.22}} &86.72(86.290.32)  & 87.21(87.190.04)  \\ \cline{2-9} 
\hline\hline
\multirow{5}{*}{CIFAR-10}             
& Sparse, Low   & 87.20    & 72.96    & 76.17   & {\color{blue}\textbf{92.32}}  &   91.35   & 91.40(91.240.27)& 91.55(91.240.15) \\ \cline{2-9} 
& Sparse, High  & 61.81  & 56.30    & 66.12   & 27.94 & {\color{blue}\textbf{69.70}}          & 46.36(46.270.07)  & 46.21(45.780.27)  \\\cline{2-9}
& Uniform, Low  & 85.68 & 72.73 & 77.12 & 90.39 & 91.70  & {\color{blue}\textbf{92.37(92.270.07)}} & {\color{blue}\textbf{92.17(92.020.08)}}  \\ \cline{2-9} 
& Uniform, High & 71.38  & 54.41 & 64.22 & 82.68 & 83.42  & {\color{black}\text{83.61(83.090.38)}}  & {\color{blue}\textbf{83.80(83.730.05)}} \\ \cline{2-9}  
& Random (0.5) & 78.40 & 59.31 & 68.97 & 85.06 & 86.47  & 86.03(85.560.32) & 86.04(85.750.19) \\ 
\cline{2-9}  
& Random (0.7) & 68.26 & 38.59  &  54.39 &  77.91  & 57.81 & 76.92 (76.820.07)& {\color{blue}\textbf{79.46(79.080.25)}}

\\ \hline\hline
\multirow{5}{*}{CIFAR-100}
& Uniform & 63.87 &  51.40& 60.04 & 64.39  & 67.94 & {\color{blue}\textbf{68.42(68.160.16)}} & {\color{blue}\textbf{68.86(68.630.18)}}\\ \cline{2-9}
& Sparse & 40.45  & 36.57 & 43.39 & 40.53  & {\color{blue}\textbf{44.25}}  &  37.54(37.500.07)&  37.43(37.060.25) \\ \cline{2-9}
& Random (0.2) & 65.84  & 61.21 & 61.52 & 66.23  & 62.92  & {\color{blue}\textbf{69.90(69.720.15)}}& {\color{blue}\textbf{69.56(69.420.14)}} \\ \cline{2-9}
& Random (0.5) & 56.92  & 22.21 & 55.88 & 56.06  & 49.62& {\color{blue}\textbf{60.81(60.360.26)}}& {\color{blue}\textbf{60.95(60.730.15)}} 
 \\ \hline

\end{tabular}
\end{threeparttable}
\caption{Experiment results comparison (w/o bias correction): The best performance in each setting (row) is highlighted in {\color{blue}\textbf{{blue}}}. All -divergences will be highlighted if they are better than the baselines we compare to. \color{black}We report the maximum accuracy of each  measures along with (mean  standard deviation).
\vspace{-7pt}
}
\label{Tab:Experiment_Results_no_bias_full}
\end{table*}

\subsection{ measures with bias correction on MNIST} 
In Table \ref{Tab:Experiment_Results_bias_mnist}, we test the impact of bias correction on MNIST with 4 noise settings. Except for the spare high noise setting which is a huge challenge for all implemented methods, experiment results of other 3 noise settings further demonstrate the negligible effect of bias term in the optimization of  measures.
\begin{table*}[!ht]
\vspace{2pt}
\tiny
\centering
\begin{threeparttable}
\begin{tabular}{c|c|c|c|c|c|c|c|c}
\hline
Noise &  J-S & Gap & PS & Gap & KL & Gap & Jeffrey & Gap \\ \hline\hline

Sparse, Low   & {\color{blue}\textbf{98.88(98.820.06)}} &-0.27 & {\color{blue}\textbf{99.05(98.980.05)}} & -0.19 & {\color{blue}\textbf{99.29(99.190.09)}}  & {\color{red}\textbf{+0.08}} & {\color{blue}\textbf{99.13(99.060.06)}} & -0.11 \\ \cline{1-9} 
 Sparse, High   & 21.39(21.360.03)  & -37.54 & 49.22(49.170.05) &-9.41 & 49.07(49.050.02) & -0.07 & 49.14(49.060.09) & -0.07\\ \cline{1-9}
 Uniform, Low    & {\color{blue}\textbf{99.18(99.100.05)}} & {\color{red}\textbf{+0.05}}& 99.13(99.010.10) & {\color{red}\textbf{+0.00}}&  {\color{blue}\textbf{99.30(99.240.08)}} & {\color{red}\textbf{+0.20}}& {\color{blue}\textbf{99.20(99.120.09)}} & {\color{red}\textbf{+0.06}}\\ \cline{1-9}
Uniform, High   & 97.76(97.680.07)& -0.10& 97.72(97.650.06) &-0.17& 97.91(97.740.14)& -0.23& {\color{blue}\textbf{98.16(97.980.14)}} &{\color{red}\textbf{+0.22}}\\ \cline{1-9}
 \hline
\end{tabular}
\end{threeparttable}
\caption{ measures with bias correction on MNIST: Digits highlighted in \color{blue}\textbf{{blue}} \color{black} means better than baseline methods, in \color{red}\textbf{{red}} \color{black} means better than without bias correction. PS: Pearson.  
\vspace{-5pt}
}
\label{Tab:Experiment_Results_bias_mnist}
\end{table*}

\subsection{ measures with bias correction on Fashion MNIST} 
In Table \ref{Tab:Experiment_Results_bias_fashion_mnist}, we test the impact of bias correction on Fashion MNIST with 4 noise settings. We can reach the same conclusion on bias correction as MNIST.
\begin{table*}[!ht]
\vspace{2pt}
\tiny
\centering
\begin{threeparttable}
\begin{tabular}{c|c|c|c|c|c|c|c|c}
\hline
Noise &  J-S & Gap & PS & Gap & KL & Gap & Jeffrey & Gap \\ \hline\hline
Sparse, Low    & {\color{blue}\textbf{89.37(88.830.34)}} & {\color{red}\textbf{+0.57}}&  87.99(87.900.13) & -0.94 & 82.29(82.030.22) & -7.48 & 82.04(81.650.26)  & -6.92 \\ \cline{1-9} 
Sparse, High   & \xmark & \xmark & 39.02(38.850.10)   & -5.60 &46.98(46.230.62) & {\color{red}\textbf{+8.02}} & 38.94(38.750.15) & -6.63  \\ \cline{1-9} 
Uniform, Low   & 88.98(88.610.26)  & {\color{red}\textbf{+0.40}}& 87.72(87.660.06) &{\color{red}\textbf{+0.45}} & 89.04(88.780.18)  & {\color{red}\textbf{+0.72}}& 89.05(88.870.15)   & {\color{red}\textbf{+0.92}}\\ \cline{1-9} 
Uniform, High  & {\color{blue}\textbf{85.56(85.33 0.19)}}& -0.06 & {\color{blue} \textbf{85.57(85.030.37)}}   & {\color{red}\textbf{+0.27}}& {\color{blue}\textbf{85.15(84.940.15)}}  &-0.54&  84.76(84.480.31)  & -0.16 \\\cline{1-9}  
 \hline
\end{tabular}
\end{threeparttable}
\caption{ measures with bias correction on Fashion MNIST: Digits highlighted in \color{blue}\textbf{{blue}} \color{black} means better than baseline methods, in \color{red}\textbf{{red}} \color{black} means better than without bias correction.  : experiment failed to stablize. PS: Pearson.
\vspace{-5pt}
}
\label{Tab:Experiment_Results_bias_fashion_mnist}
\end{table*}


\section{Experiment details}

\subsection{Noise transition matrix for Section 5.2: robustness of  measures}
We use CIFAR-10 dataset together with the uniform noise transition matrix to flip the noisy labels. In the following noise transition matrix,  is in  and the noise rate of each set of noisy labels is .

{\tiny{}}

\subsection{Noise transition matrix for MNIST and Fashion MNIST dataset}

Sparse-low noise matrix:
{\tiny{}}

\noindent Sparse-high noise matrix:
{\tiny{}}

\noindent Uniform-low noise matrix:
{\tiny{}}

\noindent Uniform-high noise matrix:
{\tiny{}}

\noindent Random 0.2 noise matrix:
{\tiny{}}



\noindent Random 0.7 noise matrix:
{\tiny{}}


\subsection{Noise transition matrix for CIFAR-10 dataset}
Sparse-low noise matrix:
{\tiny{}}

\noindent Sparse-high noise matrix:
{\tiny{}}

\noindent Uniform-low noise matrix:
{\tiny{}}

\noindent Uniform-high noise matrix:
{\tiny{}}


\noindent Random 0.5 noise matrix:
{\tiny{}}



\noindent Random 0.7 noise matrix:
{\tiny{}}


\subsection{Noise transition matrix for CIFAR-100 dataset}
For sparse noise matrix, we randomly divide 100 classes into 50 disjoint pairs, the flipping probability  in each pair is randomly chosen from , , , . 


\subsection{Parameter settings  on noised dataset}


\paragraph{MNIST, Fashion MNIST, CIFAR-10}
For experiments on MNIST and Fashion-MNIST datasets, we use the convolutional neural network used in DMI for DMI, PL and -divergences. All the experiments are performed with batch size 128. PL and -divergences adopt two kinds of learning rate setting and trained for 80 epochs, either with initial learning rate 5e-4 or 1e-3, then decay 0.2, 0.5, 0.2 every 20 epochs. We choose the default learning rate setting for DMI, BLC and FLC. For DMI's convolutional neural network, Adam(~\cite{Kingma2014AdamAM}) with default parameters is used as the optimizer, while for loss-correction's fully-connected neural network case we use AdaGrad(~\cite{Duchi2010}) in order to be consistent with their works.

\paragraph{CIFAR-10 and CIFAR-100}
For all methods and both datasets, we unify the model to be an 18-layer PreAct Resnet (\cite{he2016identity}) and train it using SGD with a momentum of
0.9, a weight decay of 0.0005, and a batch size of 128. All methods firstly train with CE warm-up for 120 (CIFAR-10) or 240 (CIFAR-100) epochs on CIFAR-10 and CIFAR-100 respectively. For DMI, BLC and FLC, we use the default learning rate settings. For PL and f-divergences, we train 100 epochs after the warm-up with initial learning rate 0.01, and decays 0.1 every 30 epochs.

\paragraph{Clothing 1M}
For clothing 1M, we use pre-trained ResNet50, SGD optimizer with momentum 0.9 and weight decay 1e-3. The initial learning rate is 0.002. All mentioned -divergences trained 40 epochs, after 10 epochs, the learning rate becomes 5e-5. Then it decays 0.2, 0.5 consequently for every 5 epochs. We compare with reported best result for all our baseline methods.

\subsection{Parameter settings on clean dataset}
We adopt the same setting (except for the number of epochs and the learning rate setting) as used in the noised dataset for each dataset.
\paragraph{MNIST, Fashion MNIST}
For CE, we trained the model for 40 epochs. The initial learning rate is 5e-4, and it decays 0.2 after 20 epochs. For  measures, the learning rate setting is the same as that in the noised dataset.  

\paragraph{CIFAR-10}
For CE, we trained the model for 300 epochs. Learning rate is 0.1 for first 150 epochs. From 150-th epoch to 250-th epoch, the learning rate is 0.01. Then, 0.001 till the end. For  measures, we trained the model for 240 epochs. The initial learning rate is 0.1, and it decays 0.1 for every 60 epochs.  


\paragraph{CIFAR-100}
For CE, we trained the model for 200 epochs. Learning rate is 0.1 for first 60 epochs. From 61-th epoch to 120-th epoch, the learning rate is 0.02 (save the model at 120-th epoch as a warm-up model for  measures). From 121-th epoch to 160-th epoch, the learning rate is 0.004. Then, 0.0008 till the end. For  measures, we load pre-trained CE model and trained for another 100 epochs. The initial learning rate is 0.01, and it decays 0.1 for every 30 epochs.  



\subsection{Computing infrastructure}
In our experiments, we use a GPU cluster (8 TITAN V GPUs and 16 GeForce GTX 1080 GPUs) for training and evaluation.
 
\end{document}
