\def\year{2020}\relax
\documentclass[letterpaper]{article} \usepackage{aaai20}  \usepackage{times}  \usepackage{helvet} \usepackage{courier}  \usepackage[hyphens]{url}  \usepackage{graphicx} \urlstyle{rm} \def\UrlFont{\rm}  \usepackage{graphicx}  \frenchspacing  \setlength{\pdfpagewidth}{8.5in}  \setlength{\pdfpageheight}{11in}  






\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{enumitem}
\usepackage{multirow}
\usepackage{array}

\usepackage{times}
\usepackage{soul}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{amsmath,amsthm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amssymb}
\urlstyle{same}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{question}{Q}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{model}{Model}
\newtheorem{example}{Example}

\DeclareMathOperator*{\concat}{\scalebox{1}[1.0]{}}






\pdfinfo{
/Title (Enhancing Attention Mechanism in Graph Neural Networks via Cardinality Preservation)
/Author (Anonymous authors)
} 



\setcounter{secnumdepth}{0} 

\setlength\titlebox{2.5in}

\title{Improving Attention Mechanism in Graph Neural Networks via Cardinality Preservation}

\author{
Shuo Zhang,\textsuperscript{1} Lei Xie\textsuperscript{1,2,3} \\
\textsuperscript{1}Ph.D. Program in Computer Science, The Graduate Center, The City University of New York \\
\textsuperscript{2}Department of Computer Science, Hunter College, The City University of New York \\
\textsuperscript{3}Helen and Robert Appel Alzheimerâ€™s Disease Research Institute, Feil Family Brain and Mind Research Institute, \\Weill Cornell Medicine, Cornell University \\
szhang4@gradcenter.cuny.edu, lei.xie@hunter.cuny.edu \\
}


\begin{document}

\maketitle

\begin{abstract}
Graph Neural Networks (GNNs) are powerful to learn the representation of graph-structured data. Most of the GNNs use the message-passing scheme, where the embedding of a node is iteratively updated by aggregating the information of its neighbors. To achieve a better expressive capability of node influences, attention mechanism has grown to be popular to assign trainable weights to the nodes in aggregation. Though the attention-based GNNs have achieved remarkable results in various tasks, a clear understanding of their discriminative capacities is missing. In this work, we present a theoretical analysis of the representational properties of the GNN that adopts the attention mechanism as an aggregator. Our analysis determines all cases when those attention-based GNNs can always fail to distinguish certain distinct structures. Those cases appear due to the ignorance of cardinality information in attention-based aggregation. To improve the performance of attention-based GNNs, we propose cardinality preserved attention (CPA) models that can be applied to any kind of attention mechanisms. Our experiments on node and graph classification confirm our theoretical analysis and show the competitive performance of our CPA models.

\end{abstract}

\section{Introduction}
Graph, as a kind of powerful data structure in non-Euclidean domain, can represent a set of instances (nodes) and the relationships (edges) between them, thus has a broad application in various fields~\cite{zhou2018graph}. Different from regular Euclidean data such as texts, images and videos, which have clear grid structures that are relatively easy to generalize fundamental mathematical operations~\cite{shuman2013emerging}, graph structured data are irregular so it is not straightforward to apply important operations in deep learning (e.g. convolutions). Consequently, the analysis of graph-structured data remains a challenging and ubiquitous question.

In recent years, Graph Neural Networks (GNNs) have been proposed to learn the representations of graph-structured data and attract a growing interest~\cite{scarselli2009graph,li2015gated,duvenaud2015convolutional,niepert2016learning,kipf2017semi,hamilton2017inductive,zhang2018end,ying2018hierarchical,morris2018weisfeiler,xu2018how}. GNNs can iteratively update node embeddings by aggregating/passing node features and structural information in the graph. The generated node embeddings can be fed into an extra classification/prediction layer and the whole model is trained end-to-end for different tasks.

Though many GNNs have been proposed, it is noted that when updating the embedding of a node  by aggregating the embeddings of its neighbor nodes , most of the GNN variants will assign non-parametric weight between  and  in their aggregators~\cite{kipf2017semi,hamilton2017inductive,xu2018how}. However, such aggregators (e.g. sum or mean) fail to learn and distinguish the information between a target node and its neighbors during the training. Taking account of different contributions from the nodes in a graph is important in real-world data as not all edges have similar impacts. A natural alternative solution is making the edge weights trainable to have a better expressive capability.

To assign learnable weights in the aggregation, attention mechanism~\cite{bahdanau2014neural,vaswani2017attention} is incorporated in GNNs. Thus the weights can be directly represented by attention coefficients between nodes and give interpretability~\cite{velickovic2018graph,thekumparampil2018attention,zhou2018commonsense}. Though GNNs with the attention-based aggregators achieve promising performance on various tasks empirically, a clear understanding of their discriminative power is missing for the designing of more powerful attention-based GNNs. Recent works~\cite{morris2019weisfeiler,xu2018how,maron2019provably} have theoretically analyzed the expressive power of GNNs. However, they are unaware of the attention mechanism in their analysis. So that it's unclear whether using attention mechanism in aggregation will constrain the expressive power of GNNs.

In this work, we make efforts to theoretically analyze the discriminative power of GNNs with attention-based aggregators. Our findings reveal that previous proposed attention-based aggregators fail to distinguish certain distinct structures. By determining all such cases, we reveal the reason for those failures is the ignorance of cardinality information in aggregation. It inspires us to improve the attention mechanism via cardinality preservation. We propose models that can be applied to any kind of attention mechanisms and achieve the goal. In our experiments on node and graph classifications, we confirm our theoretical analysis and validate the power of our proposed models. The best-performed one can achieve competitive results comparing to other baselines. Specifically, our key contributions are summarized as follows:
\begin{itemize}
\item We show that previously proposed attention-based aggregators in message-passing GNNs always fail to distinguish certain distinct structures. We determine all of those cases and demonstrate the reason is the ignorance of the cardinality information in attention-based aggregation.
\item We propose Cardinality Preserved Attention (CPA) methods to improve the original attention-based aggregator. With them, we can distinguish all cases that previously always fail an attention-based aggregator.
\item Experiments on node and graph classification validate our theoretical analysis and the power of our CPA models. Comparing to baselines, CPA models can reach state-of-the-art level.
\end{itemize}

\section{Preliminaries}

\subsection{Notations}
Let  be a graph with set of nodes  and set of edges . The nearest neighbors of node  are defined as , where  is the shortest distance between node  and . We denote the set of node  and its nearest neighbors as . For the nodes in , their feature vectors form a {\em multiset} , where  is the {\em ground set} of , and  is the {\em multiplicity function} that gives the {\em multiplicity} of each . The {\em cardinality}  of a multiset is the number of elements (with multiplicity) in the multiset.

\subsection{Graph Neural Networks}

\subsubsection{General GNNs} Graph Neural Networks (GNNs) adopt element (node or edge) features  and the graph structure  as input to learn the representation of each element, , or each graph, , for different tasks. In this work, we focus on the GNNs under massage-passing framework, which updates the node embeddings by aggregating its nearest neighbor node embeddings iteratively. In previous surveys, this type of GNNs is referred as Graph Convolutional Networks in~\cite{wu2019comprehensive} or the GNNs with convolutional aggregator in~\cite{zhou2018graph}. Under the framework, a learned representation of the node after  aggregation layers can contain the features and the structural information within -step neighborhoods of the node. The -th layer of a GNN can be formally represented as:

where the superscript  denotes the -th layer and  is initialized as . The aggregation function  in equation \ref{aggregate} propagates information between nodes and updates the hidden state of nodes.

In the final layer, since the node representation  after  iterations contains the -step neighborhood information, it can be directly used for local/node-level tasks. While for global/graph-level tasks, the whole graph representation  is needed, which requiring an extra readout function  to compute  from all :


\subsubsection{Attention-Based GNNs} In a GNN, when the aggregation function  in equation \ref{aggregate} adopts attention mechanism, we consider it as an attention-based GNN. In previous survey (Section 6 of~\cite{lee2018attention}), this is referred to the first two types of attentions which have been applied to graph data. The attention-based aggregator in -th layer can be formulated as follows:

where the superscript  denotes the -th layer and  is the attention coefficient computed by an attention function  to measure the relation between node  and node .  is the attention weight calculated by the softmax function. Equation \ref{weight-sum} is a weighted summation that uses all  as weights followed with a nonlinear function .

\subsection{Related Works}
Since GNNs have achieved remarkable results in practice, a clear understanding of the power of GNNs in graph representational learning is needed to design better models and make further improvements. Recent works~\cite{morris2019weisfeiler,xu2018how,maron2019provably} focus on understanding the discriminative power of GNNs by comparing it to the Weisfeiler-Lehman (WL) test~\cite{weisfeiler1968reduction} when deciding the graph isomorphism. It is proved that massage-passing-based GNNs which aggregate the nearest neighbor node features of a node for embedding are at most as powerful as the 1-WL test~\cite{xu2018how}. Inspired by the higher discriminative power of the -WL test ()~\cite{cai1992optimal} than the 1-WL test, GNNs that have a theoretically higher discriminative power than the massage-passing-based GNNs have been proposed based on the -WL test~\cite{morris2019weisfeiler,maron2019provably}. However, the GNNs proposed in those works don't specifically contain the attention mechanism as the part of their analysis. So it's currently unknown whether the attention mechanism will constrain the discriminative power. Our work focuses on the massage-passing-based GNNs with attention mechanism, which are upper bounded by the 1-WL test.

Another recent work~\cite{knyazev2019understanding} aims to understand the attention mechanism over nodes in GNNs with experiments in a controlled environment. However, the attention mechanism discussed in the work is used in the pooling layer for the pooling of nodes, while our work investigates the usage of attention mechanism in the aggregation layer for the updating of nodes.

\section{Limitation of Attention-Based GNNs}
In this section, we theoretically analyze the discriminative power of attention-based GNNs and show their limitations. The discriminative power means how well an attention-based GNN can distinguish different elements (local or global structures). We find that previously proposed attention-based GNNs can fail in certain cases and the discriminative power is limited. Besides, by theoretically finding out all cases that always fail an attention-based GNN, we reveal that those failures come from the lack of cardinality preservation in attention-based aggregators. The details of proofs are included in the Supplemental Material.

\subsection{Discriminative Power of Attention-based GNNs}

We assume the node input feature space is countable. For any attention-based GNNs, we give the conditions in Lemma \ref{lemma:condition} to make them reach the upper bound of discriminative power when distinguishing different elements (local or global structures). In particular, each local structure belongs to a node and is the -height subtree structure rooted at the node, which is naturally captured in the node feature  after  iterations in a GNN. The global structure contains the information of all such subtrees in a graph.

\begin{lemma} \label{lemma:condition}
Let  be a GNN following the neighborhood aggregation scheme with the attention-based aggregator (Equation \ref{weight-sum}). For global-level task, an extra readout function (Equation \ref{READOUT}) is used in the final layer.  can reach its upper bound of discriminative power (can distinguish all distinct local structures or be as powerful as the 1-WL test when distinguishing distinct global structures) after sufficient iterations with the following conditions:
\begin{itemize}
\item : Function  and the weighted summation in Equation \ref{weight-sum} are injective.
\item : Besides the conditions for local-level, 's readout function (Equation \ref{READOUT}) is injective.
\end{itemize}
\end{lemma}

With Lemma \ref{lemma:condition}, we are interested in whether its conditions can always be satisfied, so as to reach the upper bound of discriminative capacity of an attention-based GNN. Since the function  and the global-level readout function can be predetermined to be injective, we focus on whether the weighted summation function in attention-based aggregator can be injective.

\subsection{The Non-Injectivity of Attention-Based Aggregator}\label{Examples}

In this part, we aim to answer the following two questions:

\begin{question} \label{Q1}
Can the attention-based GNNs actually reach the upper bound of discriminative power? In other words, can the weighted summation function in an attention-based aggregator be injective?
\end{question}

\begin{question} \label{Q2}
If not, can we determine \textbf {all} of the cases that prevent any kind of weighted summation function being injective?
\end{question}

Given a countable feature space , a weighted summation function is a mapping . The exact  is determined by the attention weights  computed from  in Equation \ref{attention}. Since  is affected by stochastic optimization algorithms (e.g. SGD) which introduce stochasticity in , we have to pay attention that  is not fixed when dealing with the two questions. 

In Theorem \ref{theorem:att}, we answer Q\ref{Q1} with {\em  No} by giving the cases that make  not to be injective. So that the attention-based GNNs can \textbf {never} meet their upper bound of discriminative power, which is stated in Corollary \ref{corollary:WL}. Moreover, we answer Q\ref{Q2} with {\em  Yes} in Theorem \ref{theorem:att} by pointing out those cases are the \textbf {only} reason to always prevent  being injective. This alleviates the difficulty of summarizing the properties of those cases. Besides, we can specifically propose methods to avoid those cases so as to let  to be injective.

\begin{theorem} \label{theorem:att}
Assume the input feature space  is countable. Given a multiset  and the node feature  of the central node, the weighted summation function  in aggregation is defined as , where  is a mapping of input feature vector and  is the attention weight between  and  calculated by the attention function  in Equation \ref{attention} and the softmax function in Equation \ref{softmax}. For all  and ,  \textbf {if and only if} ,  and  for . In other words,  will map different multisets to the same embedding if and only if the multisets have the same central node feature and the same distribution of node features.
\end{theorem}

\begin{corollary} \label{corollary:WL}
Let  be the GNN defined in Lemma \ref{lemma:condition}.  never reaches its upper bound of discriminative power:

There exists two different subtrees  and  or two graphs  and  that the Weisfeiler-Lehman test decides as non-isomorphic, such that  always maps the two subtrees/graphs to the same embeddings.
\end{corollary}

\subsection{Attention Mechanism Fails to Preserve Cardinality}\label{flaw}

With Theorem \ref{theorem:att}, we are now interested in the properties of all cases that always prevent the weighted summation functions  being injective. Since the multisets that all  fail to distinguish share the same distribution of node features, we can say that  ignores the multiplicity information of each identical element in the multisets. Thus the cardinality of the multiset is not preserved:

\begin{corollary} \label{corollary:cardinality}
Let  be the GNN defined in Lemma \ref{lemma:condition}. The attention-based aggregator in  cannot preserve the cardinality information of the multiset of node features in aggregation.
\end{corollary}

In the next section, we aim to propose improved attention-based models to preserve the cardinality in aggregation.

\begin{figure}[t]
	\centering
	\includegraphics[scale=0.9]{CPA.pdf}
	\caption{An illustration of different attention-based aggregators on multiset of node features. Given two distinct multisets  and  that have the same central node feature  and the same distribution of node features, aggregators will map  to  and  for  and . The \textit{Original} model will get  and fail to distinguish  and , while our \textit{Additive} and \textit{Scaled} models can always distinguish  and  with  and .} \label{CPA}
\end{figure}

\section{Cardinality Preserved Attention (CPA) Model}

Since the cardinality of the multiset is not preserved in attention-based aggregators, our goal is to propose modifications to any kind of attention mechanism to make them capture the cardinality information. So that all of the cases that always prevent attention-based aggregator being injective can be avoided. 

To achieve our goal, we modify the weighted summation function in Equation \ref{weight-sum} to incorporate the cardinality information and don't change the attention function in Equation \ref{attention} so as to keep its original expressive power. Two different models named as \textit{Additive} and \textit{Scaled} are proposed to modify the \textit{Original} model in Equation \ref{weight-sum}:

\begin{model}\label{Additive-model}
(Additive)

\end{model}
\begin{model}\label{Scaled-model}
(Scaled)

where  is a non-zero vector ,  denotes the element-wise multiplication,  equals to the cardinality of the multiset ,  is an injective function.
\end{model}

In the \textit{Additive} model, each element in the multiset will contribute to the term that we added to preserve the cardinality information. In the \textit{Scaled} model, the original weighted summation is directly multiplied by a representational vector of the cardinality value. So with these models, distinct multisets with the same distribution will result in different embedding . Note that both of our models don't change the  function, such that they can keep the learning power of the original attention mechanism. We summarize the effect of our models in Corollary \ref{corollary:modified-att} and illustrate it in Figure \ref{CPA}.

\begin{corollary} \label{corollary:modified-att}
Let  be the original attention-based aggregator in Equation \ref{weight-sum}. With our proposed \textbf {Cardinality Preserved Attention (CPA)} models in Equation \ref{Additive} and \ref{Scaled},  discriminative power is increased:  can now distinguish \textbf {all} different multisets in aggregation that it previously always fails to distinguish.
\end{corollary}

While the original attention-based aggregator is never injective as we mentioned in previous sections, our cardinality preserved attention-based aggregator can be injective with certain learned attention weights to reach its upper bound of discriminative power. We validate this in our experiments. 

For the time and space complexity of our CPA models comparing to the original attention-based aggregator, it is obvious that the Model \ref{Additive-model} and \ref{Scaled-model} take more time and space than the original one due to our introduced vectors  and . Thus we further simplify our models by fixing the values in  and  and define two CPA variants:

\begin{model}\label{f-Additive-model}
(f-Additive)

\end{model}
\begin{model}\label{f-Scaled-model}
(f-Scaled)

\end{model}

Model \ref{f-Additive-model} and \ref{f-Scaled-model} still preserve the cardinality information and have reduced time and space complexity comparing to Model \ref{Additive-model} and \ref{Scaled-model}. Actually, since  and  are degenerate into constants, Model \ref{f-Additive-model} and \ref{f-Scaled-model} have the \textbf {same} time and space complexity as the original model in Equation \ref{weight-sum}. In our experiments, we will examine all 4 models together with the original one.

\section{Experiments}
In our experiments, we focus on the following questions:

\begin{question} \label{Q3}
Since attention-based GNNs (e.g. GAT) are originally proposed for local-level tasks like node classification, will those models fail or not meet the upper bound of discriminative power when solving certain node classification tasks? If so, can our proposed CPA models improve the original model?
\end{question}

\begin{question} \label{Q4}
For global-level tasks like graph classification, how well can the original attention-based GNNs perform? Can our proposed CPA models improve the original model?
\end{question}

\begin{question} \label{Q5}
How the attention-based GNNs with our CPA models perform compared to baselines?
\end{question}

To answer Question \ref{Q3}, we design a node classification task which is to predict whether or not a node is included in a triangle as one vertex in a graph. To answer Question \ref{Q4} and \ref{Q5}, we perform experiments on graph classification benchmarks and evaluate the performance of attention-based GNNs with CPA models.

\subsection{Experimental Setup}

\subsubsection{Datasets}
In our synthetic task (TRIANGLE-NODE) for predicting whether or not a node is included in a triangle, we generate a graph with different node features. In our experiment on graph classification, we use 6 benchmark datasets: 2 social network datasets (REDDIT-BINARY (RE-B), REDDIT-MULTI5K (RE-M5K)) and 4 bioinformatics datasets (MUTAG, PROTEINS, ENZYMES, NCI1). More details of the datasets are provided in Supplemental Material. 

\begin{table}[t]
\centering
\small
\caption{Testing accuracies(\%) of GAT variants (the original GAT and the GAT applied with each of our 4 CPA models) on TRIANGLE-NODE dataset for node classification. We highlight the result of the best performed model. The proportion  of multisets that hold the properties in Theorem \ref{theorem:att} among all multisets is also reported.} \label{Triangle}
\smallskip
\begin{tabular}{cc}
\toprule
Dataset & {TRIANGLE-NODE}\\
  & 29.2 \\
\midrule
\midrule
{Original}   &  78.40  7.65\\
\midrule
{Additive}   &  91.31  1.19\\
{Scaled}     &  {\bf 91.38  1.23}\\
{f-Additive} &  91.18  1.24\\
{f-Scaled}   &  91.36  1.26\\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[t]
\centering
\small
\caption{Testing accuracies(\%) of GAT-GC variants (the original one and the ones applied with each of our 4 CPA models) on social network datasets. We highlight the result of the best performed model per dataset. The proportion  of multisets that hold the properties in Theorem \ref{theorem:att} among all multisets is also reported for each dataset.} \label{Social}
\smallskip
\begin{tabular}{ccc}
\toprule
Datasets &{RE-B}&{RE-M5K}\\
  & 100.0 &  100.0 \\
\midrule
\midrule
{Original}  & 50.00  0.00   & 20.00  0.00 \\
\midrule
{Additive}  & {\bf 93.07  1.82} & {\bf 57.39  2.09} \\
{Scaled}  &  92.36  2.27  &  56.76  2.26 \\
{f-Additive}  & 93.05  1.87   &  56.43  2.38 \\
{f-Scaled}  &  92.57  2.06 &  57.22  2.20 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[t]
	\centering
	\includegraphics[scale=0.2]{Training_Curve_GAT.png}
	\caption{Training curves of GAT-GC variants on bioinformatics datasets.} \label{Training_curve}
\end{figure}

\begin{table}[t]
\centering
\scriptsize
\caption{Testing accuracies(\%) of GAT-GC variants (the original one and the ones applied with each of our 4 CPA models) on bioinformatics datasets. We highlight the result of the best performed model per dataset. The highlighted results are significantly higher than those from the corresponding \textit{Original} model under paired t-test at significance level . The proportion  of multisets that hold the properties in Theorem \ref{theorem:att} among all multisets is also reported for each dataset.} \label{Bioinfo}
\smallskip
\begin{tabular}{ccccc}
\toprule
{Datasets}&{MUTAG}&{PROTEINS}&{ENZYMES}&{NCI1}\\
 & 56.9 & 29.3 & 29.4 & 43.3 \\
\midrule
\midrule
{Original}  & 84.96  7.65   & 75.64  3.96   &  58.08  6.82 &  80.29  1.89\\
\midrule
{Additive}  & 89.75  6.39   &  76.61  3.80  &  58.90  6.96 &  81.92  1.89\\
{Scaled}  &  89.65  7.47  &  76.44  3.77   &  58.35  6.97 &  82.18  1.67\\
{f-Additive} & 90.34  6.05  &  76.60  3.91  &  {\bf 59.80  6.18} &  81.96  2.01\\
{f-Scaled}  & {\bf 90.44  6.44}  &  {\bf 76.81  3.77}   &  58.45  6.35 &  {\bf 82.28  1.81} \\
\bottomrule
\end{tabular}
\end{table}

\begin{table*}[t]
\centering
\small
\caption{Testing accuracies(\%) for graph classification. We highlight the result of the best performed model for each dataset. Our GAT-GC (f-Scaled) model achieves the top 2 on all 6 datasets.} \label{Testing_results}
\smallskip
\begin{tabular}{cccccccc}
\toprule
&Datasets  &{MUTAG}&{PROTEINS}&{ENZYMES}&{NCI1}&{RE-B}&{RE-M5K}\\
\midrule
\midrule
\multirow{5}{*}{\rotatebox{90}{\hspace*{-1pt}Baselines}}    
&{WL} & 82.05  0.36   & 74.68  0.49   & 52.22  1.26 &  82.19  0.18  & 81.10  1.90  & 49.44  2.36\\
&{PSCN} & 88.95  4.37  & 75.00  2.51  & -   &  76.34  1.68 & 86.30  1.58  & 49.10  0.70  \\
&{DGCNN}& 85.83  1.66   & 75.54  0.94  & 51.00  7.29  & 74.44  0.47  & 76.02  1.73  & 48.70  4.54  \\
&{GIN}  &  89.40  5.60     & 76.20  2.80   & -  & {\bf 82.70  1.70}  & 92.40  2.50    & {\bf 57.50  1.50} \\
&{CapsGNN}  &  86.67  6.88     & 76.28  3.63   & 54.67  5.67  & 78.35  1.55  & -    & 52.88  1.48 \\
\midrule
&{GAT-GC (f-Scaled)}  & {\bf 90.44  6.44}   & {\bf 76.81  3.77}   &  {\bf 58.45  6.35} &  82.28  1.81 &  {\bf 92.57  2.06} & 57.22  2.20 \\
\bottomrule
\end{tabular}
\end{table*}

\subsubsection{Models}
In our experiments, the \textbf {\textit{Original}} model is the one that uses the original version of an attention mechanism. We apply each of our 4 CPA models (\textbf {\textit{Additive}}, \textbf {\textit{Scaled}}, \textbf {\textit{f-Additive}} and \textbf {\textit{f-Scaled}}) to the original attention mechanism for comparison. In the \textit{Additive} and \textit{Scaled} models, we take advantage of the approximation capability of multi-layer perceptron (MLP)~\cite{hornik1989multilayer,hornik1991approximation} to model  and .

For node classification, we use GAT~\cite{velickovic2018graph} as the \textit{Original} model. For graph classification, we build a GNN (GAT-GC) based on GAT as the \textit{Original} model:  We adopt the attention mechanism in GAT to specify the form of Equation \ref{attention}: . For the readout function, a naive way is to only consider the node embeddings from the last iteration. Although a sufficient number of iterations can help to avoid the cases in Theorem \ref{theorem:att} by aggregating more diverse node features, the features from the latter iterations may generalize worse and the GNNs usually have shallow structures~\cite{xu2018how,zhou2018graph}. So the GAT-GC adopts the same function as used in~\cite{xu2018representation,xu2018how,lee2019self,li2019deepgcns}, which concatenates graph embeddings from all iterations: ,  function can be sum or mean. With CPA models, the cases in Theorem \ref{theorem:att} can be avoided in each iteration. Full experimental settings are included in Supplemental Material. 

\subsection{Node Classification}
For the TRIANGLE-NODE dataset, the proportion P of multisets that hold the properties in Theorem \ref{theorem:att} is , as shown in Table \ref{Triangle}. The classification accuracy of the \textit{Original} model (GAT) is significantly lower than the CPA models. It supports the claim in Corollary \ref{corollary:WL}: the \textit{Original} model fails to distinguish all distinct multisets in the dataset and exhibits constrained discriminate power. On the contrary, CPA models can distinguish all different multisets in the graph as suggested in Corollary \ref{corollary:modified-att} and indeed significantly improve the accuracy of the \textit{Original} model as shown in Table \ref{Triangle}. This experiment thus well answers Question \ref{Q3} that we raised.

\subsection{Graph Classification}

In this section, we aim to answer Question \ref{Q4} by evaluating the performance of variants of GAT-based GNN (GAT-GC) on graph classification benchmarks. Besides, we compare our best-performed CPA model with baseline models to answer Question \ref{Q5}.

\subsubsection{Social Network Datasets}
Since the RE-B and RE-M5K datasets don't have original node features and we assign all the node features to be the same, we have  in those datasets. Thus \textbf {all} multisets in aggregation will be mapped to the same embedding by the \textit{Original} GAT-GC. After a mean readout function on all multisets, \textbf {all} graphs are finally mapped to the same embedding. The performance of the \textit{Original} model is just random guessing of the graph labels as reported in Table \ref{Social}. While our CPA models can distinguish all different multisets and are confirmed to be significantly better than the \textit{Original} one. 

Here we examine a naive approach to incorporate the cardinality information in the \textit{Original} model by assigning node degrees as input node labels. By doing this way, the node features are diverse and we get , which means that the cases in Theorem \ref{theorem:att} can be all avoided. However, the testing accuracies of \textit{Original} can only reach  on \textsc{RE-B} and  on \textsc{RE-M5K}, which are significantly lower than the results of CPA models in Table \ref{Social}. Thus in practice, our proposed models exhibit good generalization power comparing to the naive approach.

\subsubsection{Bioinformatics Datasets}
For bioinformatics datasets that contain diverse node labels, we also report the  values in Table \ref{Bioinfo}. The results reveal the existence () of the cases in those datasets that can fool the \textit{Original} model, thus the discriminative power of the \textit{Original} model is {\em theoretically} constrained. 

To empirically validate this, we compare the training accuracies of GAT-GC variants, since the discriminative power can be directly indicated by the accuracies on {\em training sets}. Higher training accuracy indicates a better fitting ability to distinguish different graphs. The training curves of GAT-GC variants are shown in Figure \ref{Training_curve}. From these curves, we can see even though the \textit{Original} model has overfitted different datasets, the fitting accuracies that it converges to can never be higher than those of our CPA models. Compared to the WL kernel, CPA models can get training accuracies close to  on several datasets, which reach those obtained from the WL kernel (equal to  as shown in~\cite{xu2018how}). These findings validate that the discriminative power of the \textit{Original} model is constrained while our CPA models can approach the upper bound of discriminative power with certain learned weights.

In Table \ref{Bioinfo} we report the testing accuracies of GAT-GC variants on bioinformatics datasets. The \textit{Original} model can get meaningful results. However, we find our proposed CPA models further improve the testing accuracies of the \textit{Original} model on all datasets. This indicates that the preservation of cardinality can also benefit the generalization power of the model besides the discriminative power.

From previous results in Table \ref{Social} and \ref{Bioinfo}, we find the \textit{f-Scaled} model performs the best with an average ranking measure~\cite{taheri2018learning}. The good performance of the fixed-weight models (\textit{f-Additive} and \textit{f-Scaled}) comparing to the full models (\textit{Additive} and \textit{Scaled}) demonstrates that the improvements achieved by CPA models are not simply due to the increased capacities given by the additional vectors embedded.

\subsubsection{Comparison to Baselines}
We further compare the best-performed GAT-GC variant (\textit{f-Scaled}) with other baselines (WL kernel (WL)~\cite{shervashidze2011weisfeiler}, PATCHY-SAN (PSCN)~\cite{niepert2016learning}, Deep Graph CNN (DGCNN)~\cite{zhang2018end}, Graph Isomorphism Network (GIN)~\cite{xu2018how} and Capsule Graph Neural Network (CapsGNN)~\cite{xinyi2019capsule}). In Table \ref{Testing_results}, we report the results. Our GAT-GC (f-Scaled) model achieves 4 top 1 and 2 top 2 on all 6 datasets. It is expected that even better performance can be achieved with certain choices of attention mechanism besides the GAT one.

\section{Conclusion}
In this paper, we theoretically analyze the representational power of GNNs with attention-based aggregators: We determine all cases when those GNNs always fail to distinguish distinct structures. The finding shows that the missing cardinality information in aggregation is the only reason to cause those failures. To improve, we propose cardinality preserved attention (CPA) models to solve this issue. In our experiments, we validate our theoretical analysis that the performances of the original attention-based GNNs are limited. With our models, the original models can be improved. Compared to other baselines, our best-performed model achieves competitive performance. In future work, a challenging problem is to better learn the attention weights so as to guarantee the injectivity of our cardinality preserved attention models after the training. Besides, it would be interesting to analyze the effects of different attention mechanisms.

\bibliographystyle{aaai}
\bibliography{main}

\newpage
\appendix
\section{Proof for Lemma \ref{lemma:condition}}
\begin{proof} Local-level: For the aggregator in the first layer, it will map different 1-height subtree structures to different embeddings from the distinct input multisets of neighborhood node features, since it's injective. Iteratively, the aggregator in the -th layer can distinguish different -height subtree structures by mapping them to different embeddings from the distinct input multisets of -1-height subtree features, since it's injective.

Global level: From Lemma 2 and Theorem 3 in~\cite{xu2018how}, we know: When all functions in  are injective,  can reach its upper bound of discriminative power, which is the same as the Weisfeiler-Lehman (WL) test~\cite{weisfeiler1968reduction} when deciding the graph isomorphism. 
\end{proof}

\section{Proof for Theorem \ref{theorem:att}}
\begin{proof}
To prove Theorem \ref{theorem:att}, we have consider both two directions in the iff statement:

\paragraph{(1)} If given ,  and , as , we have:

where  is the attention weight belongs to , and between  and , .

We can rewrite the equations using  and : 


where  is the multiplicity function, and  is the attention weight belongs to , and between  and , .

Considering the softmax function in Equation 2 of our paper, we can use attention coefficient  to rewrite the equations:

where  is the attention coefficient belongs to , and between  and , . Moreover,  is the attention coefficient belongs to , and between  and , .

As attention coefficient  is computed by function , which is regardless of , thus ,  and , . We denote , . Remind that  has  copies of the elements in , so that


Using this equation, we can get


From all equations above, we finally have





\paragraph{(2)} If given  for all , , we have 

where  is the attention weight belongs to , and between  and , .

We denote  and  and rewrite the equation: 

where  is the multiplicity function of . Moreover,  is the attention weight belongs to , and between  and , . 

When considering the relations between  and , we have: 


If we assume the equality of Equation \ref{Equation1} is true for all  and , we can define such two functions  and : 


If given the equality of Equation \ref{Equation1} is true for , we have:


We can rewrite Equation \ref{Equation_f1} using :


Thus we know


Note that the LHS of Equation \ref{Equation_f2} is just the LHS of Equation \ref{Equation1} when . As  due to the definition of multiplicity,  due to the softmax function, we have . Thus the RHS of Equation \ref{Equation_f2} > 0 and we now know the equality in Equation \ref{Equation1} is not true for . So the assumption of  is false. 

We denote . To let the remaining summation term always equal to 0, we have 


Considering Equation 2 in our paper, we can rewrite the equation above: 

where  is the attention coefficient belongs to , and between  and , . And  is the attention coefficient belongs to , and between  and , .

The LHS of Equation \ref{Equation2} is a rational number. However if , the RHS of Equation \ref{Equation2} can be irrational: We assume  contains at least two elements  and . If not, we can directly get . We consider any attention mechanism that results in: 


Thus when , the RHS of the equation become:

where  is the multiplicity of  in . It is obvious that the value of RHS is irrational. So we have  to always hold the equality.

With , we know ,  and , . We denote , Equation \ref{Equation2} becomes


We further denote . So that . Finally by denoting , we have ,  and .
\end{proof}

\section{Proof for Corollary \ref{corollary:WL}}
\begin{proof}
For subtrees, if  and  are 1-height subtrees that have the same root node feature and the same distribution of node features,  will get the same embeddings for  and  according to Theorem \ref{theorem:att}.

For graphs, let  be a fully connect graph with  nodes and  be a ring-like graph with  nodes. All nodes in  and  have the same feature . It is clear that the Weisfeiler-Lehman test of isomorphism decides  and  as non-isomorphic.

We denote  as the set of multisets for aggregation in , and  as the set of multisets for aggregation in . As  is a fully connect graph, all multisets in  contain  central node and  neighbors. As  is a ring-like graph, all multisets in  contain  central node and  neighbors. Thus we have


where  is the multiplicity function of the node with feature  in .

From Theorem 1, we know that . Considering the Equation 3 of our paper, we have  in each iteration . Besides, as the number of node in  and  are equals to ,  will always map  and  to the same set of multisets of node features  in each iteration  and finally get the same embedding for each graph.
\end{proof}

\section{Proof for Corollary \ref{corollary:cardinality}}
\begin{proof}
Given two distinct multiset of node features  and  that have the same central node feature and the same distribution of node features: ,  and  for , we know the cardinality of  is  times of the cardinality of . Thus  and  can be distinguished by their cardinality. 

However, the weighted summation function  in attention-based aggregator  will map them to the same embedding:  according to Theorem \ref{theorem:att}. Thus we cannot distinguish  and  via . To conclude,  lost the cardinality information after aggregation.
\end{proof}

\section{Proof for Corollary \ref{corollary:modified-att}}
\begin{proof}
For any two distinct multisets  and  that  previously always fail to distinguish according to Theorem \ref{theorem:att}, we denote  and  for some  and . Thus , where  is the attention weight belongs to , and between  and , . We denote . When applying CPA models, the aggregation functions in  can be rewritten as:


We consider the following example: All elements in  equal to 1. Function  maps  to a n-dimensional vector which all elements in it equal to . And , where  and . So that the aggregation functions become:


For , we have . According to Lemma 5 of~\cite{xu2018how}, when , . So .

For , we have . As  due to the softmax function, and  in our example, we know . Moreover as , we can get .
\end{proof}

\section{Details of Datasets}

For the node classification task, we generate a graph with 4800 nodes and 32400 edges.  of the nodes are included in triangles as vertices while  are not. There are 4000 nodes assigned with feature '0', 400 with feature '1' and 400 with feature '2'. The label of each node for prediction is whether or not it's included in a triangle.

For the graph classification task, detailed statistics of the bioinformatics and social network datasets are listed in Table \ref{Detail}. All of the datasets are available at https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets.

In all datasets, if the original node features are provided, we use the one-hot encodings of them as input.

\begin{table}[H]
\centering
\scriptsize
\caption{Dataset Description} \label{Detail}
\smallskip
\begin{tabular}{lccccc}
\toprule
Datasets  &{Graphs}&{Classes}&{Features}&{Node Avg.}&{Edge Avg.}\\
\midrule
{MUTAG}    & 188   & 2  & 7  & 17.93  & 19.79 \\
{PROTEINS} & 1113  & 2  & 4  & 39.06  & 72.81\\
{ENZYMES}  & 600   & 6  & 6  & 32.63  & 62.14 \\
{NCI1}     & 4110  & 2  & 23 & 29.87  & 32.30 \\
{RE-B}     & 2000  & 2  & -  & 429.63 & 995.51 \\
{RE-M5K}   & 4999  & 5  & -  & 508.52 & 1189.75 \\
\bottomrule
\end{tabular}
\end{table}

\section{Details of Experiment Settings}
For all experiments, we perform 10-fold cross-validation and repeat the experiments 10 times for each dataset and each model. To get a final accuracy for each run, we select the epoch with the best cross-validation accuracy averaged over all 10 folds. The average accuracies and their standard deviations are reported based on the results across the folds in all runs.

In our \textit{Additive} and \textit{Scaled} models, all MLPs have 2 layers with ReLU activation.

In the GAT variants, we use 2 GNN layers and a hidden dimensionality of 32. The negative input slope of  in the  attention mechanism is 0.2. The number of heads in multi-head attention is 1.

In the GAT-GC variants, we use 4 GNN layers. For the  function in all models, we use sum for bioinformatics datasets and mean for social network datasets. We apply Batch normalization~\cite{ioffe2015batch} after every hidden layers. The hidden dimensionality is set as 32 for bioinformatics datasets and 64 for social network datasets. The negative input slope of  in the  attention mechanism is 0.2. We use a single head in the multi-head attention in all models.

All models are trained using the Adam optimizer~\cite{kingma2014adam} and the learning rate is dropped by a factor of 0.5 every 400 epochs in the node classification task and every 50 epochs in the graph classification task. We use an initial learning rate of 0.01 for the TRIANGLE-NODE and bioinformatics datasets and 0.0025 for the social network datasets. For the GAT variants, we use a dropout ratio of 0 and a weight decay value of 0. For the GAT-GC variants on each dataset, the following hyper-parameters are tuned: (1) Batch size in ; (2) Dropout ratio in  after dense layer; (3)  regularization from  to . On each dataset, we use the same hyper-parameter configurations in all model variants for a fair comparison.

For the results of the baselines for comparison, we use the results reported in their original works by default. If results are not available, we use the best testing results reported in~\cite{xinyi2019capsule,ivanov2018anonymous}.

\end{document}
