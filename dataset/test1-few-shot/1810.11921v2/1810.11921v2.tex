\section{Experiment}
In this section, we move forward to evaluate the effectiveness of our proposed approach. We aim to answer the following questions:



\begin{itemize}[leftmargin=6ex,labelsep=1ex]
\raggedright
	\item[\textbf{RQ1}] How does our proposed AutoInt perform on the problem of CTR prediction? Is it efficient for large-scale sparse and high-dimensional data?
	\item[\textbf{RQ2}] What are the influences of different model configurations?
	\item[\textbf{RQ3}] What are the dependency structures between different features? Is our proposed model explainable?
   	\item[\textbf{RQ4}] Will integrating implicit feature interactions further improve the performance?
\end{itemize}
We first describe the experimental settings before answering these questions.

\subsection{Experiment Setup}

\subsubsection{Data Sets}\label{subsubsec::data}
We use four public real-world data sets. The statistics of the data sets are summarized in Table~\ref{tab::dataset}.
\textbf{Criteo}\footnote{https://www.kaggle.com/c/criteo-display-ad-challenge} This is a benchmark dataset for CTR prediction, which has 45 million users' clicking records on displayed ads. It contains 26 categorical feature fields and 13 numerical feature fields. \textbf{Avazu}\footnote{https://www.kaggle.com/c/avazu-ctr-prediction} This dataset contains users' mobile behaviors including whether a displayed mobile ad is clicked by a user or not. It has 23 feature fields spanning from user/device features to ad attributes.  
\textbf{KDD12}\footnote{https://www.kaggle.com/c/kddcup2012-track2} This data set was released by KDDCup 2012, which originally aimed to predict the number of clicks. Since our work focuses on CTR prediction rather than the exact number of clicks, we treat this problem as a binary classification problem (1 for clicks>0, 0 for without click), which is similar to FFM~\cite{juan2016field}.
\textbf{MovieLens-1M}\footnote{https://grouplens.org/datasets/movielens/} This dataset contains users' ratings on movies. During binarization, we treat samples with a rating less than 3 as negative samples because a low score indicates that the user does not like the movie. We treat samples with a rating greater than 3 as positive samples and remove neutral samples, i.e., a rating equal to 3.

\noindent\textbf{Data Preparation} First, we remove the infrequent features (appearing in less than \textit{threshold} instances) and treat them as a single feature ``<unknown>'', where \textit{threshold} is set to \{10, 5, 10\} for Criteo, Avazu and KDD12 data sets respectively.
Second, since numerical features may have large variance and hurt machine learning algorithms, we normalize numerical values by transforming a value  to  if , which is proposed by the winner of Criteo Competition\footnote{\url{https://www.csie.ntu.edu.tw/~r01922136/kaggle-2014-criteo.pdf}}. Third, we randomly select 80\% of all samples for training and randomly split the rest into validation and test sets of equal size.

\begin{table}
\centering\caption{Statistics of evaluation data sets.}
\begin{tabular}{cccc} 
\toprule
Data & \#Samples & \#Fields & \#Features (Sparse)  \\
\midrule
Criteo & 45,840,617 & 39 & 998,960 \\
Avazu & 40,428,967 & 23 & 1,544,488 \\
KDD12 & 149,639,105 & 13 & 6,019,086\\
MovieLens-1M & 739,012 & 7 & 3,529 \\
\bottomrule
\end{tabular}\label{tab::dataset}
\end{table}



\begin{table*}
\begin{threeparttable}
\centering\caption{Effectiveness Comparison of Different Algorithms. We highlight that our proposed model almost outperforms all baselines across four data sets and both metrics. Further analysis is provided in Section~\ref{sec::result}.}\label{tab::results}
\tabcolsep=0.24cm
\begin{tabular}{llcccccccc}
\toprule
\multirow{2}{*}{Model Class} & \multirow{2}{*}{Model} & \multicolumn{2}{c}{Criteo} & \multicolumn{2}{c}{Avazu} & \multicolumn{2}{c}{KDD12} & \multicolumn{2}{c}{MovieLens-1M} \\
& & AUC & Logloss & AUC & Logloss & AUC & Logloss & AUC & Logloss \\
\midrule
\multirow{1}{*}{First-order} & LR & 0.7820 & 0.4695 & 0.7560 & 0.3964 & 0.7361 & 0.1684 & 0.7716 & 0.4424 \\
\midrule
\multirow{2}{*}{Second-order} & FM~\cite{rendle2010factorization} & 0.7836 & 0.4700 & 0.7706 & 0.3856 & 0.7759 & 0.1573 & 0.8252 & 0.3998 \\ 
& AFM\cite{xiao2017attentional} & 0.7938 & 0.4584 & 0.7718 & 0.3854 & 0.7659 & 0.1591 & 0.8227 & 0.4048 \\
\midrule
\multirow{6}{*}{High-order} & DeepCrossing~\cite{shan2016deep} & 0.8009 &0.4513 & 0.7643 & 0.3889 & 0.7715 & 0.1591 & 0.8448 & 0.3814 \\
& NFM~\cite{he2017neural} & 0.7957 & 0.4562 & 0.7708 & 0.3864 & 0.7515 & 0.1631 & 0.8357 & 0.3883 \\
& CrossNet~\cite{wang2017deep} & 0.7907 & 0.4591 & 0.7667 & 0.3868 & 0.7773 & 0.1572 & 0.7968 & 0.4266 \\
& CIN~\cite{lian2018xdeepfm} & 0.8009 & 0.4517 & \textbf{0.7758} & 0.3829 & 0.7799 & 0.1566 & 0.8286 & 0.4108 \\
& HOFM~\cite{blondel2016higher} & 0.8005 & 0.4508 & 0.7701 & 0.3854 & 0.7707 & 0.1586 & 0.8304 & 0.4013 \\
& AutoInt (ours)  & \textbf{0.8061**} & \textbf{0.4455**} & 0.7752 & \textbf{0.3824} & \textbf{0.7883**} & \textbf{0.1546**} & \textbf{0.8456*} & \textbf{0.3797**}  \\
\bottomrule
\end{tabular}
    \begin{tablenotes}
    \centering
      \small
      \item AutoInt outperforms the strongest baseline w.r.t. Criteo, KDD12 and MovieLens-1M data at the: \textbf{**} 0.01 and \textbf{*} 0.05 level, unpaired t-test.
    \end{tablenotes}
\end{threeparttable}
\end{table*}



\subsubsection{Evaluation Metrics}
We use two popular metrics to evaluate the performance of all methods.

\textbf{AUC} Area Under the ROC Curve (AUC) measures the probability that a CTR predictor will assign a higher score to a randomly chosen positive item than a randomly chosen negative item. A higher AUC indicates a better performance.

\textbf{Logloss} Since all models attempt to minimize the \textit{Logloss} defined by Equation~\ref{eqa::loss}, we use it as a straightforward metric.

It is noticeable that a slightly higher AUC or lower \textit{Logloss} at \textit{\textbf{0.001-level}} is regarded significant for CTR prediction task, which has also been pointed out in existing works~\cite{cheng2016wide,guo2017deepfm,wang2017deep}.

\subsubsection{Competing Models} We compare the proposed approach with three classes of previous models. (A) the linear approach that only uses individual features. (B) factorization machines-based methods that take into account second-order combinatorial features. (C) techniques that can capture high-order feature interactions. We associate the model classes with model names accordingly.

\textbf{LR} (A). LR only models the linear combination of raw features.  

\textbf{FM}~\cite{rendle2010factorization} (B). FM uses factorization techniques to model second-order feature interactions.

\textbf{AFM}~\cite{xiao2017attentional} (B). AFM is one of the state-of-the-art models that capture second-order feature interactions. It extends FM by using attention mechanism to distinguish the different importance of second-order combinatorial features.

\textbf{DeepCrossing}~\cite{shan2016deep} (C). DeepCrossing utilizes deep fully-connected neural networks with residual connections to learn non-linear feature interactions in an implicit fashion.

\textbf{NFM}~\cite{he2017neural} (C). NFM stacks deep neural networks on top of second-order feature interaction layer. High-order feature interactions are implicitly captured by the nonlinearity of neural networks.

\textbf{CrossNet}~\cite{wang2017deep} (C). Cross Network, which is the core of Deep\&Cross model, takes outer product of concatenated feature vector at the bit-wise level to model feature interactions explicitly.

\textbf{CIN}~\cite{lian2018xdeepfm} (C). Compressed Interaction Network, which is the core of xDeepFM model, takes outer product of stacked feature matrix at vector-wise level.

\textbf{HOFM}~\cite{blondel2016higher} (C). HOFM proposes efficient kernel-based algorithms for training high-order factorization machines. Follow settings in~\citeauthor{blondel2016higher}~\cite{blondel2016higher} and \citeauthor{he2017neural}~\cite{he2017neural}, we build a third-order factorization machine using public implementation.

We will compare with the full models of CrossNet and CIN, i.e., Deep\&Cross and xDeepFM, under the setting of joint training with plain DNN later (i.e., Section~\ref{sec::joint}).




\begin{figure*}
\centering
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=\linewidth]{figures/run_time_criteo_3.pdf}
    \caption{Criteo}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=\linewidth]{figures/run_time_avazu_3.pdf}
    \caption{Avazu}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=\linewidth]{figures/run_time_kdd_3.pdf}
    \caption{KDD12}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=\linewidth]{figures/run_time_movielens_3.pdf}
    \caption{MovieLens-1M}
  \end{subfigure}
  \caption{Efficiency Comparison of Different Algorithms in terms of \textit{Run Time}. ``DC'' and ``CN'' are DeepCrossing and CrossNet for short, respectively. Since HOFM cannot be fit on one GPU card for the KDD12 dataset, extra communication cost makes it most time-consuming. Further analysis is presented in Section~\ref{sec::result}.}\label{fig::run_time}
  \vspace{-8pt}
\end{figure*}



\subsubsection{Implementation Details}\label{sec::impl}
All methods are implemented in TensorFlow\cite{abadi2016tensorflow}. For AutoInt and all baseline methods, we empirically set embedding dimension  to 16 and batch size to 1024. AutoInt has three interacting layers and the number of hidden units  is 32 in default setting. Within each interacting layer, the number of attention head is two\footnote{We also tried different number of attention heads. The performance of using one head is inferior to that of two heads, and the improvement of further increasing head number is not significant.}. To prevent overfitting, we use grid search to select dropout rate~\cite{srivastava2014dropout} from \{0.1 - 0.9\} for MovieLens-1M data set, and we found dropout is not necessary for other three large data sets.
For baseline methods, we use one hidden layer of size 200 on top of Bi-Interaction layer for NFM as recommended by their paper. For CN and CIN, we use three interaction layers following AutoInt.  DeepCrossing has four feed-forward layers and the number of hidden units is 100, because it performs poorly when using three neural layers. Once all network structures are fixed, we also apply grid search to baseline methods for optimal hype-parameters. 
Finally, we use Adam~\cite{kingma2014adam} to optimize all deep neural network-based models.

\subsection{Quantitative Results (RQ1)}\label{sec::result}

\noindent\textbf{Evaluation of Effectiveness}\\
We summarize the results averaged over 10 different runs into Table~\ref{tab::results}.
We have the following observations: 
(1) FM and AFM, which explore second-order feature interactions, consistently outperform LR by a large margin on all datasets, which indicates that individual features are insufficient in CTR prediction.
(2) An interesting observation is the inferiority of some models which capture high-order feature interactions. For example, although DeepCrossing and NFM use the deep neural network as a core component to learning high-order feature interactions, they do not guarantee improvement over FM and AFM. This may attribute to the fact that they learn feature interactions in an implicit fashion. On the contrary, CIN does it explicitly and outperforms low-order models consistently.
(3) HOFM significantly outperforms FM on Criteo and MovieLens-1M datasets, which indicates that modeling third-order feature interactions can be beneficial to prediction performance.
(4) AutoInt achieves the best performance overall baseline methods on three of four real-world data sets. On Avazu data set, CIN performs a little better than AutoInt in AUC evaluation, but we get lower \textit{Logloss}. Note that our proposed AutoInt shares the same structures as DeepCrossing except the feature interacting layer, which indicates using the attention mechanism to learn explicit combinatorial features is crucial. \\




\noindent\textbf{Evaluation of Model Efficiency}\\
We present the runtime results of different algorithms on four data sets in Figure~\ref{fig::run_time}. Unsurprisingly, LR is the most efficient algorithm due to its simplicity. FM and NFM perform similarly in terms of runtime because NFM only stacks a single feed-forward hidden layer on top of the second-order interaction layer. Among all listed methods, CIN, which achieves the best performance for prediction among all the baselines, is much more time-consuming due to its complicated crossing layer. This may make it impractical in the industrial scenarios. Note that AutoInt is sufficiently efficient, which is comparable to the efficient algorithms DeepCrossing and NFM.

We also compare the sizes of different models (i.e., the number of parameters) as another criterion for efficiency evaluation. As shown in Table~\ref{tab::param}, comparing to the best model CIN in the baseline models, the number of parameters in AutoInt is much smaller. 



To summarize, our proposed AutoInt achieves the best performance among all the compared models. Compared to the most competitive baseline model CIN, AutoInt requires much fewer parameters and is much more efficient during online inference. 





\begin{table}
\centering\caption{Efficiency Comparison of Different Algorithms in terms of \textit{Model Size} on Criteo data set. ``DC'' and ``CN'' are DeepCrossing and CrossNet for short, respectively. The counted parameters exclude the embedding layer.}
\begin{tabularx}{\linewidth}{cccccc}
\toprule
Model & DC & CN & CIN & NFM & AutoInt \\
\midrule
\#Params &  &  &  &  &  \\
\bottomrule
\end{tabularx}
\vspace{-6pt}
\label{tab::param}
\end{table}



\begin{table}
\centering\caption{Ablation study comparing the performance of AutoInt with and without residual connections. AutoInt is the complete model while the AutoInt is the model without residual connection.}
\begin{tabularx}{1.0\linewidth}{l>{\centering\arraybackslash}l>{\centering\arraybackslash}X>{\centering\arraybackslash}X}
\toprule
Data Sets & Models & AUC & Logloss \\
\midrule
\multirow{2}{*}{Criteo} & AutoInt & 0.8061 & 0.4454 \\
 & AutoInt & 0.8033 & 0.4478 \\
\midrule
\multirow{2}{*}{Avazu} & AutoInt & 0.7752 & 0.3823 \\
 & AutoInt & 0.7729 & 0.3836 \\
 \midrule
\multirow{2}{*}{KDD12} & AutoInt & 0.7888 & 0.1545 \\
 & AutoInt & 0.7831 & 0.1557 \\
\midrule
\multirow{2}{*}{MovieLens-1M} & AutoInt & 0.8460 & 0.3784 \\ 
 & AutoInt & 0.8299 & 0.3959 \\
\bottomrule
\end{tabularx}
\vspace{-6pt}
\label{tab::residual}
\end{table}

\subsection{Analysis (RQ2)}
To further validate and gain deep insights into the proposed model, we conduct ablation study and compare several variants of AutoInt. 

\subsubsection{Influence of Residual Structure}
The standard AutoInt utilizes residual connections, which carry through all learned combinatorial features and therefore allow modeling very high-order combinations. To justify the contribution of residual units, we tease apart them from our standard model and keep other structures as they are. As presented in Table~\ref{tab::residual}, we observe that the performance decrease on all datasets if residual connections are removed. Specifically, the full model outperforms the variant by a large margin on the KDD12 and MovieLens-1M data, which indicates residual connections are crucial to model high-order feature interactions in our proposed method.



\subsubsection{Influence of Network Depths}
Our model learns high-order feature combinations by stacking multiple interacting layers (introduced in Section~\ref{sec::model}). Therefore, we are interested in how the performance change w.r.t. the number of interacting layers, i.e., the order of combinatorial features. Note that when there is no interacting layer (i.e., \textit{Number of layers} equals zero), our model takes the weighted sum of raw individual features as input, i.e., no combinatorial features are considered. 

The results are summarized in Figure~\ref{fig::layer}. We can see that if one interacting layer is used, i.e., feature interactions are taken into account, the performance increase dramatically on both data sets, showing that combinatorial features are very informative for prediction. As the number of interacting layers further increases, i.e., higher-order combinatorial features are taken into account, the performance of the model further increases. When the number of layers reaches three, the performance becomes stable, showing that adding extremely high-order features are not informative for prediction. 






\begin{table*}
\begin{threeparttable}
\centering\caption{Results of Integrating Implicit Feature Interactions. We indicate the base model behind each method. The last two columns are average changes of AUC and \textit{Logloss} compared to corresponding base models (``+'': increase, ``-'': decrease).}\label{tab::ensemble}
\begin{tabular}{lcccccccc||cc}
\toprule
\multirow{2}{*}{Model} & \multicolumn{2}{c}{Criteo} & \multicolumn{2}{c}{Avazu} & \multicolumn{2}{c}{KDD12} & \multicolumn{2}{c}{MovieLens-1M} & \multicolumn{2}{c}{Avg. Changes}\\
& AUC & Logloss & AUC & Logloss & AUC & Logloss & AUC & Logloss & AUC & Logloss\\
\midrule
Wide\&Deep (LR) & 0.8026 & 0.4494 & 0.7749 & 0.3824 & 0.7549 & 0.1619 & 0.8300 & 0.3976 & +0.0292 & -0.0213 \\
DeepFM (FM) & 0.8066 & 0.4449 & 0.7751 & 0.3829 & 0.7867 & 0.1549 & 0.8437 & 0.3846 & +0.0142 & -0.0113\\
Deep\&Cross (CN) & 0.8067 & 0.4447 & 0.7731 & 0.3836 & 0.7872 & 0.1549 & 0.8446 & 0.3809 & +0.0200 & -0.0164 \\
xDeepFM (CIN) & 0.8070 & 0.4447 & 0.7770 & 0.3823 & 0.7820 & 0.1560 & 0.8463 & 0.3808 & +0.0068 & -0.0096 \\
AutoInt+ (ours) & \textbf{0.8083**} & \textbf{0.4434**} & \textbf{0.7774*} & \textbf{0.3811**} & \textbf{0.7898**} & \textbf{0.1543**} & \textbf{0.8488**} & \textbf{0.3753**} & +0.0023 & -0.0020 \\ 
\bottomrule
\end{tabular}
    \begin{tablenotes}
    \centering
      \small
      \item AutoInt+ outperforms the strongest baseline w.r.t. each data at the: \textbf{**} 0.01 and \textbf{*} 0.05 level, unpaired t-test.
    \end{tablenotes}
\end{threeparttable}
\vspace{-5pt}
\end{table*}


\begin{figure}
\centering
\begin{subfigure}[b]{0.49\linewidth}
    \includegraphics[width=\linewidth]{figures/layer_auc_2.pdf}
    \caption{AUC}
  \end{subfigure}
  \begin{subfigure}[b]{0.49\linewidth}
    \includegraphics[width=\linewidth]{figures/layer_loss_2.pdf}
    \caption{Logloss}
  \end{subfigure}
  \caption{Performance w.r.t. the number of interacting layers. Results on Criteo and Avazu data sets are similar and hence omitted.}
  \vspace{-10pt}
  \label{fig::layer}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}[b]{0.49\linewidth}
    \includegraphics[width=\linewidth]{figures/dim_auc_2.pdf}
    \caption{AUC}
  \end{subfigure}
  \begin{subfigure}[b]{0.49\linewidth}
    \includegraphics[width=\linewidth]{figures/dim_loss_2.pdf}
    \caption{Logloss}
  \end{subfigure}
  \caption{Performance w.r.t. number of embedding dimensions. Results on Criteo and Avazu data sets are similar and hence omitted.}
  \vspace{-10pt}
  \label{fig::dim}
\end{figure}



\subsubsection{Influence of Different Dimensions}
Next, we investigate the performance w.r.t. the parameter , which is the output dimension of the embedding layer. On the KDD12 dataset, we can see that the performance continuously increase as we increase the dimension size since larger models are used for prediction. The results are different on the MovieLens-1M dataset. When the dimension size reaches 24, the performance begins to decrease. The reason is that this data set is small, and the model is overfitted when too many parameters are used. 







\begin{figure}
\centering
\begin{subfigure}[b]{0.495\linewidth}
    \includegraphics[width=\linewidth]{figures/label1_v4.pdf}
    \caption{Label=1, Predicted CTR=0.89}
  \end{subfigure}
\begin{subfigure}[b]{0.495\linewidth}
      \includegraphics[width=\linewidth]{figures/global_v2.pdf}
      \caption{Overall feature interactions}
  	\end{subfigure}
  \caption{Heat maps of attention weights for both case- and global-level feature interactions on MovieLens-1M. The axises represent feature fields <\textit{Gender, Age, Occupation, Zipcode, RequestTime, RealeaseTime, Genre}>. We highlight some learned combinatorial features in rectangles.}
  \vspace{-11pt}
  \label{fig::case}
\end{figure}

\subsection{Explainable Recommendations (RQ3)}
A good recommender system can not only provide good recommendations but also offer good explainability. Therefore, in this part, we present how our AutoInt is able to explain the recommendation results. 
We take the MovieLens-1M dataset as an example. 




Let's look at a recommendation result suggested by our algorithm, i.e., a user likes an item. Figure~\ref{fig::case} (a) presents the correlations between different fields of input features, which are obtained by the attention score. We can see that AutoInt is able to identify the meaningful combinatorial feature <\textit{Gender=Male, Age=[18-24), MovieGenre=Action\&Triller}> (i.e., red dotted rectangle). This is very reasonable since young men are very likely to prefer action\&triller movies. 



We are also interested in what the correlations between different feature fields in the data are. Therefore, we measure the correlations between the feature fields according to their average attention score in the entire data. The correlations between different fields are summarized into Figure~\ref{fig::case} (b). We can see that <\textit{Gender, Genre}>, <\textit{Age, Genre}>, <\textit{RequestTime, ReleaseTime}> and <\textit{Gender, Age, Genre}> (i.e., solid green region) are strongly correlated, which are the explainable rules for recommendation in this domain. 




\vspace{-2pt}
\subsection{Integrating Implicit Interactions (RQ4)}\label{sec::joint}
Feed-forward neural networks are capable of modeling implicit feature interactions and have been widely integrated into existing CTR prediction methods~\cite{cheng2016wide,guo2017deepfm,lian2018xdeepfm}. To investigate whether integrating implicit feature interactions further improves the performance, we combine AutoInt with a two-layer feed-forward neural network by joint training. 
We name the joint model \textit{AutoInt+} and compare it with the following algorithms:
\begin{itemize}[leftmargin=*]
\item Wide\&Deep~\cite{cheng2016wide}. Wide\&Deep integrates the outputs of logistic regression and feed-forward neural networks.
\item DeepFM~\cite{guo2017deepfm}. DeepFM combines trainditional second-order factorization machines and feed-forward neural network, with a shared embedding layer.
\item Deep\&Cross~\cite{wang2017deep}. Deep\&Cross is the extension of CrossNet by integrating feed-forward neural networks.
\item xDeepFM~\cite{lian2018xdeepfm}. xDeepFM is the extension of CIN by integrating feed-forward neural networks.
\end{itemize}

Table~\ref{tab::ensemble} presents the averaged results (over 10 runs) of joint-training models. We have the following observations: 1) The performance of our method improves by joint training with feed-forward neural networks on all datasets. This indicates that integrating implicit feature interactions indeed boosts the predictive ability of our proposed model. However, as can be seen from last two columns, the magnitude of performance improvement is fairly small compared to other models, showing that our individual model AutoInt is quite powerful. 2) After integrating implicit feature interactions, AutoInt+ outperforms all competitive methods, and achieves new state-of-the-art performances on used CTR prediction data sets. 
