

\documentclass{article}

\usepackage{times}
\usepackage{graphicx} \usepackage{subfigure} 

\usepackage{natbib}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{listings}

\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}


\newcommand{\GLR}[1]{\ensuremath{\operatorname{GL}(#1, \mathbb{R})}}
\newcommand{\SLR}[1]{\ensuremath{\operatorname{SL}(#1, \mathbb{R})}}
\newcommand{\SE}[1]{\ensuremath{\operatorname{SE}(#1)}}
\newcommand{\SO}[1]{\ensuremath{\operatorname{SO}(#1)}}
\newcommand{\Aff}[1]{\ensuremath{\operatorname{Aff}(#1, \mathbb{R})}}

\newcommand{\Tr}{\ensuremath{\operatorname{Tr}}}

\newcommand{\corr}{\ensuremath{\star}}
\newcommand{\conv}{\ensuremath{*}}

\newcommand\supp{\mathop{\rm supp}}

\newcommand{\rpm}{\raisebox{.2ex}{}}
\newcommand{\rrpm}{\sbox0{}\sbox2{}
  \raise\dimexpr(\ht0-\ht2)/2\relax\box2 }

\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}



\usepackage[accepted]{icml2016}



\icmltitlerunning{Group Equivariant Convolutional Networks}

\begin{document}

\twocolumn[
\icmltitle{Group Equivariant Convolutional Networks}

\icmlauthor{Taco S. Cohen}{t.s.cohen@uva.nl}
\icmladdress{University of Amsterdam}
\icmlauthor{Max Welling}{m.welling@uva.nl}
\icmladdress{
  University of Amsterdam \\
  University of California Irvine \\
  Canadian Institute for Advanced Research}

\icmlkeywords{}

\vskip 0.3in
]

\begin{abstract}
  We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity by exploiting symmetries.
  G-CNNs use G-convolutions, 
  a new type of layer that enjoys a substantially higher degree of weight sharing than regular convolution layers.
  G-convolutions increase the expressive capacity of the network without increasing the number of parameters.
  Group convolution layers are easy to use and can be implemented with negligible computational overhead for discrete groups generated by translations, reflections and rotations.
  G-CNNs achieve state of the art results on CIFAR10 and rotated MNIST.
\end{abstract}


\section{Introduction}
\label{sec:introduction}

Deep convolutional neural networks (CNNs, convnets) have proven to be very powerful models of sensory data such as images, video, and audio.
Although a strong theory of neural network design is currently lacking,
a large amount of empirical evidence supports the notion that both \emph{convolutional weight sharing} and \emph{depth} (among other factors) are important for good predictive performance.

Convolutional weight sharing is effective because there is a \emph{translation symmetry} in most perception tasks:
the label function and data distribution are both approximately invariant to shifts.
By using the same weights to analyze or model each part of the image,
a convolution layer uses far fewer parameters than a fully connected one, while preserving the capacity to learn many useful transformations.

Convolution layers can be used effectively in a \emph{deep} network because all the layers in such a network are \emph{translation equivariant}:
shifting the image and then feeding it through a number of layers is the same as feeding the original image through the same layers and then shifting the resulting feature maps (at least up to edge-effects).
In other words, the symmetry (translation) is preserved by each layer, which makes it possible to exploit it not just in the first, but also in higher layers of the network.

In this paper we show how convolutional networks can be generalized to exploit larger groups of symmetries, including rotations and reflections.
The notion of equivariance is key to this generalization,
so in section \ref{sec:equivariance} we will discuss this concept and its role in deep representation learning.
After discussing related work in section \ref{sec:related_work}, we recall a number of mathematical concepts in section \ref{sec:framework} that allow us to define and analyze the G-convolution in a generic manner. 

In section \ref{sec:translation_equivariance_of_CNNs}, we analyze the equivariance properties of standard CNNs, and show that they are equivariant to translations but may fail to equivary with more general transformations.
Using the mathematical framework from section \ref{sec:framework}, we can define G-CNNs (section \ref{sec:equivariance_of_GCNNs}) by analogy to standard CNNs (the latter being the G-CNN for the translation group).
We show that G-convolutions, as well as various kinds of layers used in modern CNNs, such as pooling, arbitrary pointwise nonlinearities, batch normalization and residual blocks are all equivariant, and thus compatible with G-CNNs.
In section \ref{sec:implementation} we provide concrete implementation details for group convolutions.

In section \ref{sec:experiments} we report experimental results on MNIST-rot and CIFAR10, where G-CNNs achieve state of the art results ( error on MNIST-rot, and  resp.  on augmented and plain CIFAR10).
We show that replacing planar convolutions with G-convolutions consistently improves results without additional tuning.
In section \ref{sec:discussion} we provide a discussion of these results and consider several extensions of the method, before concluding in section \ref{sec:conclusion}.


\section{Structured \& Equivariant Representations}
\label{sec:equivariance}

Deep neural networks produce a sequence of progressively more abstract representations by mapping the input through a series of parameterized functions \cite{LeCun2015}.
In the current generation of neural networks, the representation spaces are usually endowed with very minimal internal structure, such as that of a linear space .

In this paper we construct representations that have the structure of a linear -space, for some chosen group .
This means that each vector in the representation space has a \emph{pose} associated with it, which can be transformed by the elements of some group of transformations .
This additional structure allows us to model data more efficiently:
A filter in a G-CNN detects co-occurrences of features that have the preferred relative pose,
and can match such a feature constellation in every global pose through an operation called the G-convolution.

A representation space can obtain its structure from other representation spaces to which it is connected.
For this to work, the network or layer  that maps one representation to another should be \emph{structure preserving}. For -spaces this means that  has to be equivariant:

That is, transforming an input  by a transformation  (forming ) and then passing it through the learned map  should give the same result as first mapping  through  and then transforming the representation.

Equivariance can be realized in many ways, and in particular the operators  and  need not be the same.
The only requirement for  and  is that for any two transformations  and ,
 we have  (i.e.  is a \emph{linear representation} of ).

From equation \ref{eq:equivariance_definition} we see that the familiar concept of invariance is a special kind of equivariance where  is the identity transformation for all .
In deep learning, general equivariance is more useful than invariance because it is impossible to determine if features are in the right spatial configuration if they are invariant.

Besides improving statistical efficiency and facilitating geometrical reasoning, equivariance to symmetry transformations constrains the network in a way that can aid generalization.
A network  can be non-injective, meaning that non-identical vectors  and  in the input space become identical in the output space (for example, two instances of a face may be mapped onto a single vector indicating the presence of any face).
If  is equivariant, then the -transformed inputs  and  must also be mapped to the same output.
Their ``sameness'' (as judged by the network) is preserved under symmetry transformations.


\section{Related Work}
\label{sec:related_work}

There is a large body of literature on invariant representations.
Invariance can be achieved by pose normalization using an equivariant detector \cite{Lowe2004, Jaderberg2015} or by averaging a possibly nonlinear function over a group \cite{Reisert2008, Skibbe2013, Manay2006, Kondor2007}.

Scattering convolution networks use wavelet convolutions, nonlinearities and group averaging to produce stable invariants \cite{Bruna2013_TPAMI}.
Scattering networks have been extended to use convolutions on the group of translations, rotations and scalings, and have been applied to object and texture recognition \cite{Sifre2013, Oyallon2015}.

A number of recent works have addressed the problem of learning or constructing equivariant representations.
This includes work on transforming autoencoders \cite{Hinton2011}, equivariant Boltzmann machines \cite{Kivinen2011, Sohn2012}, equivariant descriptors \cite{Schmidt2012}, and equivariant filtering \cite{Skibbe2013}.

\citet{Lenc2014} show that the AlexNet CNN \cite{Krizhevsky2012} trained on imagenet spontaneously learns representations that are equivariant to flips, scaling and rotation.
This supports the idea that equivariance is a good inductive bias for deep convolutional networks.
\citet{Agrawal} show that useful representations can be learned in an unsupervised manner by training a convolutional network to be equivariant to ego-motion.

\citet{Anselmi2014, Anselmi2015} use the theory of locally compact topological groups to develop a theory of statistically efficient learning in sensory cortex.
This theory was implemented for the commutative group consisting of time- and vocal tract length shifts for an application to speech recognition by \citet{Zhang2015}.

\citet{Gensa} proposed an approximately equivariant convolutional architecture that uses sparse, high-dimensional feature maps to deal with high-dimensional groups of transformations.
\citet{Dieleman2015} showed that rotation symmetry can be exploited in convolutional networks for the problem of galaxy morphology prediction by rotating feature maps, effectively learning an equivariant representation.
This work was later extended \cite{Dieleman2016} and evaluated on various computer vision problems that have cyclic symmetry.

\citet{Cohen2014} showed that the concept of disentangling can be understood as a reduction of the operators  in an equivariant representation, and later related this notion of disentangling to the more familiar statistical notion of decorrelation \cite{Cohen2015a}.

\section{Mathematical Framework}
\label{sec:framework}

In this section we present a mathematical framework that enables a simple and generic definition and analysis of G-CNNs for various groups .
We begin by defining symmetry groups, and study in particular two groups that are used in the G-CNNs we have built so far.
Then we take a look at functions on groups (used to model feature maps in G-CNNs) and their transformation properties.


\subsection{Symmetry Groups}

A \emph{symmetry} of an object is a transformation that leaves the object invariant.
For example, if we take the sampling grid of our image, , and flip it over we get .
So the flipping operation is a symmetry of the sampling grid.

If we have two symmetry transformations  and  and we compose them, the result  is another symmetry transformation (i.e. it leaves the object invariant as well).
Furthermore, the inverse  of any symmetry is also a symmetry, and composing it with  gives the identity transformation .
A set of transformations with these properties is called a \emph{symmetry group}.

One simple example of a group is the set of 2D integer translations, .
Here the group operation (``composition of transformations'') is addition: 
One can verify that the sum of two translations is again a translation, and that the inverse (negative) of a translation is a translation, so this is indeed a group.

Although it may seem fancy to call 2-tuples of integers a group, this is helpful in our case because as we will see in section \ref{sec:equivariance_of_GCNNs},
a useful notion of convolution can be defined for functions on any group\footnote{At least, on any locally compact group.}, of which  is only one example.
The important properties of the convolution, such as equivariance, arise primarily from the group structure.
 

\subsection{The group }
\label{sec:group_p4}

The group  consists of all compositions of translations and rotations by  degrees about any center of rotation in a square grid.
A convenient parameterization of this group in terms of three integers  is

where  and .
The group operation is given by matrix multiplication.

The composition and inversion operations could also be represented directly in terms of integers , but the equations are cumbersome.
Hence, our preferred method of composing two group elements represented by integer tuples is to convert them to matrices, multiply these matrices, and then convert the resulting matrix back to a tuple of integers (using the  function to obtain ).

The group  acts on points in  (pixel coordinates) by multiplying the matrix  by the homogeneous coordinate vector  of a point :



\subsection{The group }
\label{sec:group_p4m}

The group  consists of all compositions of translations, mirror reflections, and rotations by  degrees about any center of rotation in the grid.
Like , we can parameterize this group by integers:

where ,  and .
The reader may verify that this is indeed a group.

Again, composition is most easily performed using the matrix representation.
Computing  from a given matrix  can be done using the same method we use for , and for  we have .


\subsection{Functions on groups}
\label{sec:feature_maps}

We model images and stacks of feature maps in a conventional CNN as functions  supported on a bounded (typically rectangular) domain.
At each pixel coordinate , the stack of feature maps returns a -dimensional vector , where  denotes the number of channels.

Although the feature maps must always be stored in finite arrays, modeling them as functions that extend to infinity (while being non-zero on a finite region only) simplifies the mathematical analysis of CNNs.

We will be concerned with transformations of the feature maps, so we introduce the following notation for a transformation  acting on a set of feature maps:

Computationally, this says that to get the value of the -transformed feature map  at the point , we need to do a lookup in the original feature map  at the point , which is the unique point that gets mapped to  by .
This operator  is a concrete instantiation of the transformation operator  referenced in section \ref{sec:equivariance}, and one may verify that


If  represents a pure translation  then  simply means .
The inverse on  in equation \ref{eq:left_translation} ensures that the function is shifted in the positive direction when using a positive translation, and that  satisfies the criterion for being a homomorphism (eq. \ref{eq:l_homomorphism}) even for transformations  and  that do not commute (i.e. ).

As will be explained in section \ref{sec:g_equivariant_convolution}, feature maps in a -CNN are functions on the group , instead of functions on the group .
For functions on , the definition of  is still valid if we simply replace  (an element of ) by  (an element of ), and interpret  as composition.

It is easy to mentally visualize a planar feature map  undergoing a transformation, but we are not used to visualizing functions on groups.
To visualize a feature map or filter on , we plot the four patches associated with the four pure rotations on a circle, as shown in figure \ref{fig:p4_fmap} (left).
Each pixel in this figure has a rotation coordinate (the patch in which the pixel appears), and two translation coordinates (the pixel position within the patch).

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.20\textwidth]{p4_fmap_e_mini}
  \includegraphics[width=0.20\textwidth]{p4_fmap_r_mini}
  \caption{A p4 feature map and its rotation by .}
  \label{fig:p4_fmap}
\end{figure}


When we apply the  degree rotation  to a function on , each planar patch follows its red -arrow
(thus incrementing the rotation coordinate by  (mod )),
and simultaneously undergoes a -degree rotation.
The result of this operation is shown on the right of figure \ref{fig:p4_fmap}.
As we will see in section \ref{sec:equivariance_of_GCNNs}, a  feature map in a -CNN undergoes exactly this motion under rotation of the input image.

For , we can make a similar plot, shown in figure \ref{fig:p4m_fmap}.
A  function has  planar patches, each one associated with a mirroring  and rotation .
Besides red rotation arrows, the figure now includes small blue reflection lines (which are undirected, since reflections are self-inverse).

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.20\textwidth]{p4m_fmap_e_mini}
  \includegraphics[width=0.20\textwidth]{p4m_fmap_r_mini}
  \caption{A p4m feature map and its rotation by .}
  \label{fig:p4m_fmap}
\end{figure}

Upon rotation of a  function, each patch again follows its red -arrows and undergoes a  degree rotation.
Under a mirroring, the patches connected by a blue line will change places and undergo the mirroring transformation.
  
This rich transformation structure arises from the group operation of  or , combined with equation \ref{eq:left_translation} which describes the transformation of a function on a group.

Finally, we define the \emph{involution} of a feature map, which will appear in section \ref{sec:g_equivariant_convolution} when we study the behavior of the G-convolution, and which also appears in the gradient of the G-convolution.
We have:

For  feature maps the involution is just a point reflection, but for -feature maps the meaning depends on the structure of .
In all cases, .


\section{Equivariance properties of CNNs}
\label{sec:translation_equivariance_of_CNNs}

In this section we recall the definitions of the convolution and correlation operations used in conventional CNNs, and show that these operations are equivariant to translations but not to other transformations such as rotation.
This is certainly well known and easy to see by mental visualization, but deriving it explicitly will make it easier to follow the derivation of group equivariance of the group convolution defined in the next section. 

At each layer , a regular convnet takes as input a stack of feature maps  and convolves or correlates it with a set of  filters :


If one employs convolution () in the forward pass, the correlation () will appear in the backward pass when computing gradients, and vice versa.
We will use the correlation in the forward pass, and refer generically to both operations as ``convolution''.

Using the substitution , and leaving out the summation over feature maps for clarity, we see that a translation followed by a correlation is the same as a correlation followed by a translation:

And so we say that ``correlation is an equivariant map for the translation group'', or that ``correlation and translation commute''.
Using an analogous computation one can show that also for the convolution, .

Although convolutions are equivariant to \emph{translation}, they are not equivariant to other isometries of the sampling lattice.
For instance, as shown in the supplementary material, rotating the image and then convolving with a fixed filter is not the same as first convolving and then rotating the result:

In words, this says that the correlation of a rotated image  with a filter  is the same as the rotation by  of the original image  convolved with the inverse-rotated filter .
Hence, if an ordinary CNN learns rotated copies of the same filter, the \emph{stack} of feature maps is equivariant, although individual feature maps are not.


\section{Group Equivariant Networks}
\label{sec:equivariance_of_GCNNs}

In this section we will define the three layers used in a G-CNN (-convolution, -pooling, nonlinearity) and show that each one commutes with -transformations of the domain of the image.


\subsection{G-Equivariant correlation}
\label{sec:g_equivariant_convolution}

The correlation (eq. \ref{eq:conv_corr}) is computed by \emph{shifting} a filter and then computing a dot product with the feature maps.
By replacing the shift by a more general transformation from some group , we get the -correlation used in the first layer of a -CNN:

Notice that both the input image  and the filter  are functions of the plane , but the feature map  is a function on the discrete group  (which may contain translations as a subgroup).
Hence, for all layers after the first, the filters  must also be functions on , and the correlation operation becomes


The equivariance of this operation is derived in complete analogy to eq. \ref{eq:conv_equivariance}, now using the substitution :

The equivariance of eq. \ref{eq:gconv_invariant} is derived similarly.
Note that although equivariance is expressed by the same formula  for both first-layer G-correlation (eq. \ref{eq:gconv_invariant}) and full G-correlation (\ref{eq:gconv}), the meaning of the operator  is different:
for the first layer correlation, the inputs  and  are functions on , so  denotes the transformation of such a function, while  denotes the transformation of the feature map, which is a function on .
For the full -correlation, both the inputs  and  and the output  are functions on .

Note that if  is not commutative, neither the -convolution nor the -correlation is commutative.
However, the feature maps  and  are related by the involution (eq. \ref{eq:involution}):

Since the involution is invertible (it is its own inverse), the information content of  and  is the same.
However,  is more efficient to compute when using the method described in section \ref{sec:implementation}, because transforming a small filter is faster than transforming a large feature map.

It is customary to add a bias term to each feature map in a convolution layer.
This can be done for -conv layers as well, as long as there is only one bias per -feature map (instead of one bias per spatial feature plane within a -feature map).
Similarly, batch normalization \cite{Ioffe2015} should be implemented with a single scale and bias parameter per -feature map in order to preserve equivariance.
The sum of two -equivariant feature maps is also -equivariant, thus -conv layers can be used in highway networks and residual networks \cite{Srivastava2015a, He2015}.


\subsection{Pointwise nonlinearities}

Equation \ref{eq:gconv_equivariance} shows that -correlation preserves the transformation properties of the previous layer.
What about nonlinearities and pooling?

Recall that we think of feature maps as functions on .
In this view, applying a nonlinearity  to a feature map amounts to \emph{function composition}.
We introduce the composition operator

which acts on functions by \emph{post}-composing them with .

Since the left transformation operator  acts by \emph{pre}-composition,  and  commute:

so the rectified feature map inherits the transformation properties of the previous layer.


\subsection{Subgroup pooling and coset pooling}

In order to simplify the analysis, we split the pooling operation into two steps: the pooling itself (performed without stride), and a subsampling step.
The non-strided max-pooling operation applied to a feature map  can be modeled as an operator  that acts on  as

where  is the -transformation of some pooling domain  (typically a neighborhood of the identity transformation).
In a regular convnet,  is usually a  or  square including the origin , and  is a translation.

As shown in the supplementary material, pooling commutes with :



Since pooling tends to reduce the variation in a feature map, it makes sense to sub-sample the pooled feature map, or equivalently, to do a ``pooling with stride''.
In a G-CNN, the notion of ``stride'' is generalized by subsampling on a subgroup .
That is,  is a subset of  that is itself a group (i.e. closed under multiplication and inverses).
The subsampled feature map is then equivariant to  but not .

In a standard convnet, pooling with stride  is the same as pooling and then subsampling on  which is a subgroup of .
For the -CNN, we may subsample on the subgroup  containing all  rotations, as well as shifts by multiples of  pixels.

We can obtain full -equivariance by choosing our pooling region  to be a subgroup .
The pooling domains  that result are called \emph{cosets} in group theory.
The cosets partition the group into non-overlapping regions.
The feature map that results from pooling over cosets is invariant to the \emph{right}-action of , because the cosets are similarly invariant ().
Hence, we can arbitrarily choose one coset representative per coset to subsample on.
The feature map that results from coset pooling may be thought of as a function on the quotient space ,
in which two transformations are considered equivalent if they are related by a transformation in .

As an example, in a  feature map, we can pool over all four rotations at each spatial position (the cosets of the subgroup  of rotations around the origin).
The resulting feature map is a function on , i.e. it will transform in the same way as the input image.
Another example is given by a feature map on , where we could pool over the cosets of the subgroup  of shifts by multiples of .
This gives a feature map on , which has a cyclic transformation law under translations.

This concludes our analysis of -CNNs.
Since all layer types are equivariant, we can freely stack them into deep networks and expect G-conv parameter sharing to be effective at arbitrary depth.


\section{Efficient Implementation}
\label{sec:implementation}

Computing the G-convolution for involves nothing more than indexing arithmetic and inner products, so it can be implemented straightforwardly.
Here we present the details for a G-convolution implementation that can leverage recent advances in fast computation of planar convolutions \cite{Mathieu2014, Vasilache2015, Lavin2015}.

A plane symmetry group  is called \emph{split} if any transformation  can be decomposed into a translation  and a transformation  in the stabilizer of the origin (i.e.  leaves the origin invariant).
For the group , we can write  for  a translation and  a rotation about the origin, while  splits into translations and rotation-flips.
Using this split of  and the fact that , we can rewrite the G-correlation (eq. \ref{eq:gconv_invariant} and \ref{eq:gconv}) as follows:

where  in layer one and  in further layers.

Thus, to compute the  (or ) correlation  we can first compute  (``filter transformation'') for all four rotations (or all eight rotation-flips) and then call a fast planar correlation routine on  and the augmented filter bank.

The computational cost of the algorithm presented here is roughly equal to that of a planar convolution with a filter bank that is the same size as the augmented filter bank used in the G-convolution, because the cost of the filter transformation is negligible.


\subsection{Filter transformation}

The set of filters at layer  is stored in an array  of shape , where  is the number of channels at layer ,  denotes the number of transformations in  that leave the origin invariant (e.g. ,  or  for ,  or  filters, respectively), and  is the spatial (or translational) extent of the filter.
Note that typically,  for 2D images, while  or  for .

The filter transformation  amounts to a permutation of the entries of each of the  scalar-valued filter channels in .
Since we are applying  transformations to each filter, the output of this operation is an array of shape , which we call .

The permutation can be implemented efficiently by a GPU kernel that does a lookup into  for each output cell of , using a precomputed index associated with the output cell.
To precompute the indices, we define an invertible map  that takes an input index (valid for an array of shape ) and produces the associated group element  as a matrix (section \ref{sec:group_p4} and \ref{sec:group_p4m}).
For each input index  and each transformation , we compute .
This index is used to set  for all .

The G-convolution for a new group can be added by simply implementing a map  from indices to matrices.

\subsection{Planar convolution}

The second part of the G-convolution algorithm is a planar convolution using the expanded filter bank .
If , the sum over  in eq. \ref{eq:gcorr_decomp} involves a sum over the stabilizer.
This sum can be folded into the sum over feature channels performed by the planar convolution routine by reshaping  from  to .
The resulting array can be interpreted as a conventional filter bank with  planar input channels and  planar output channels, which can be correlated with the feature maps  (similarly reshaped).

\section{Experiments}
\label{sec:experiments}

\subsection{Rotated MNIST}

The rotated MNIST dataset \cite{Larochelle2007} contains  randomly rotated handwritten digits.
The dataset is split into a training, validation and test sets of size ,  and , respectively.

We performed model selection using the validation set, yielding a CNN architecture (Z2CNN) with  layers of  convolutions ( in the final layer),  channels in each layer, relu activation functions, batch normalization, dropout, and max-pooling after layer .
For optimization, we used the Adam algorithm \cite{Kingma2015}.
This baseline architecture outperforms the models tested by \citet{Larochelle2007} (when trained on k and evaluated on k), but does not match the previous state of the art, which uses prior knowledge about rotations \cite{Schmidt2012} (see table \ref{tbl:mnist_rot_results}).

Next, we replaced each convolution by a -convolution (eq. \ref{eq:gconv_invariant} and \ref{eq:gconv}), divided the number of filters by  (so as to keep the number of parameters approximately fixed), and added max-pooling over rotations after the last convolution layer.
This architecture (P4CNN) was found to perform better without dropout, so we removed it.
The P4CNN almost halves the error rate of the previous state of the art ( vs  error).

We then tested the hypothesis that premature invariance is undesirable in a deep architecture (section \ref{sec:equivariance}).
We took the Z2CNN, replaced each convolution layer by a -convolution (eq. \ref{eq:gconv_invariant}) followed by a coset max-pooling over rotations.
The resulting feature maps consist of rotation-invariant features, and have the same transformation law as the input image.
This network (P4CNNRotationPooling) outperforms the baseline and the previous state of the art, but performs significantly worse than the P4CNN which does not pool over rotations in intermediate layers.

\begin{table}[h!]
  \centering
  \begin{tabular}{l r}
    \hline
    Network & Test Error (\%) \\
    \hline
    \citet{Larochelle2007} & 10.38 \rpm \, 0.27  \\
    \citet{Sohn2012}       & 4.2 \\
    \citet{Schmidt2012}    & 3.98  \\
    \hline
    Z2CNN                  & 5.03 \rpm \, 0.0020 \\
    P4CNNRotationPooling   & 3.21 \rpm \, 0.0012 \\
    \bf{P4CNN}             & \bf{2.28} \rpm \, 0.0004 \\
    \hline 
  \end{tabular}
  \caption{Error rates on rotated MNIST (with standard deviation under variation of the random seed).}
  \label{tbl:mnist_rot_results}
\end{table}


\subsection{CIFAR-10}

The CIFAR-10 dataset consists of  images of size , divided into  classes.
The dataset is split into  training,  validation and  testing splits.

We compared the -, - and standard planar  convolutions on two kinds of baseline architectures.
Our first baseline is the All-CNN-C architecture by \citet{Springenberg2015},
which consists of a sequence of  strided and non-strided convolution layers, interspersed with rectified linear activation units, and nothing else.
Our second baseline is a residual network \cite{He2016}, which consists of an initial convolution layer, followed by three stages of  convolution layers using  filters at stage , followed by a final classification layer ( layers in total).
The first convolution in each stage  uses a stride of 2, so the feature map sizes are 32, 16, and 8 for the three stages.
We use ,  yielding a wide -layer network called ResNet44.

To evaluate G-CNNs, we replaced all convolution layers of the baseline architectures by  or  convolutions.
For a constant number of filters, this increases the size of the feature maps  or -fold, which in turn increases the number of parameters required per filter in the next layer.
Hence, we halve the number of filters in each -conv layer, and divide it by roughly  in each -conv layer.
This way, the number of parameters is left approximately invariant, while the size of the internal representation is increased.
Specifically, we used  for -ResNet44.

To evaluate the impact of data augmentation, we compare the networks on CIFAR10 and augmented CIFAR10+.
The latter denotes moderate data augmentation with horizontal flips and small translations, following \citet{Goodfellow2013} and many others.

The training procedure for training the All-CNN was reproduced as closely as possible from \citet{Springenberg2015}.
For the ResNets, we used stochastic gradient descent with initial learning rate of  and momentum .
The learning rate was divided by  at epoch  and , and training was continued for  epochs.

\begin{table}[h!]
  \centering
  \begin{tabular}{l | c c c r}
    \hline
    Network   &     & CIFAR10 & CIFAR10+ & Param. \\
    \hline
    All-CNN   &   & 9.44 & 8.86 & 1.37M \\
              &     & 8.84 & 7.67 & 1.37M \\
              &    & 7.59 & 7.04 & 1.22M \\
    ResNet44  &   & 9.45 & 5.61 & 2.64M \\
              &    & \bf{6.46} & \bf{4.94} & 2.62M \\
    \hline
  \end{tabular}
  \caption{Comparison of conventional (i.e. ),  and  CNNs on CIFAR10 and augmented CIFAR10+. Test set error rates and number of parameters are reported.}
  \label{tbl:cifar10_results}
\end{table}


To the best of our knowledge, the -CNN outperforms all published results on plain CIFAR10 \cite{Wan2013, Goodfellow2013, Lin2013, Lee2015, Srivastava2015a, Clevert2015, Lee2015a}.
However, due to radical differences in model sizes and architectures, it is difficult to infer much about the intrinsic merit of the various techniques.
It is quite possible that the cited methods would yield better results when deployed in larger networks or in combination with other techniques.
Extreme data augmentation and model ensembles can also further improve the numbers \cite{Graham}.

Inspired by the wide ResNets of \citet{Zagoruyko2016}, we trained another ResNet with  layers and  (for planar convolutions) or  (for  convolutions).
When trained with moderate data augmentation, this network achieves an error rate of  using planar convolutions, and  with  convolutions.
This result is comparable to the  error reported by \citet{Zagoruyko2016}, but using fewer parameters (7.2M vs 36.5M).


\section{Discussion \& Future work}
\label{sec:discussion}

Our results show that  and  convolution layers can be used as a drop-in replacement of standard convolutions that consistently improves the results.

G-CNNs benefit from data augmentation in the same way as convolutional networks,
as long as the augmentation comes from a group larger than .
Augmenting with flips and small translations consistently improves the results for the  and -CNN.

The CIFAR dataset is not actually symmetric, since objects typically appear upright.
Nevertheless, we see substantial increases in accuracy on this dataset, indicating that there need not be a full symmetry for G-convolutions to be beneficial.

In future work, we want to implement G-CNNs that work on hexagonal lattices which have an increased number of symmetries relative to square grids, as well as G-CNNs for 3D space groups.
All of the theory presented in this paper is directly applicable to these groups, and the G-convolution can be implemented in such a way that new groups can be added by simply specifying the group operation and a bijective map between the group and the set of indices.

One limitation of the method as presented here is that it only works for discrete groups.
Convolution on continuous (locally compact) groups is mathematically well-defined, but may be hard to approximate in an equivariant manner.
A further challenge, already identified by \citet{Gensa}, is that a full enumeration of transformations in a group may not be feasible if the group is large.

Finally, we hope that the current work can serve as a concrete example of the general philosophy of ``structured representations'', outlined in section \ref{sec:equivariance}.
We believe that adding mathematical structure to a representation (making sure that maps between representations preserve this structure), could enhance the ability of neural nets to see abstract similarities between superficially different concepts.


\section{Conclusion}
\label{sec:conclusion}
We have introduced G-CNNs, a generalization of convolutional networks that 
substantially increases the expressive capacity of a network without increasing the number of parameters.
By exploiting symmetries, G-CNNs achieve state of the art results on rotated MNIST and CIFAR10.
We have developed the general theory of G-CNNs for discrete groups, showing that all layer types are equivariant to the action of the chosen group .
Our experimental results show that G-convolutions can be used as a drop-in replacement for spatial convolutions in modern network architectures, improving their performance without further tuning.


\section*{Acknowledgements}
We would like to thank Joan Bruna, Sander Dieleman, Robert Gens, Chris Olah, and Stefano Soatto for helpful discussions.
This research was supported by NWO (grant number NAI.14.108), Google and Facebook.


\bibliography{icml2016}
\bibliographystyle{icml2016}

\newpage
\section*{Appendix A: Equivariance Derivations}

We claim in the paper that planar correlation is not equivariant to rotations.
Let  be an image with  channels, and let  be a filter.
Take a rotation  about the origin.
The ordinary planar correlation  is not equivariant to rotations, i.e., .
Instead we have:


Line by line, we used the following definitions, facts and manipulations:
\begin{enumerate}
  \item The definition of the correlation .
  \item The definition of , i.e. .
  \item The substitution , which does not change the summation bounds since rotation is a symmetry of the sampling grid .
  \item Distributivity.
  \item The definition of .
  \item The definition of the correlation .
  \item The definition of .
\end{enumerate}

A visual proof can be found in \cite{Dieleman2016}.

Using a similar line of reasoning, we can show that pooling commutes with the group action:



\section*{Appendix B: Gradients}

To train a G-CNN, we need to compute gradients of a loss function with respect to the parameters of the filters.
If we use the fast algorithm explained in section 7 of the main paper, we only have to implement the gradient of the indexing operation (section 7.1, ``filter transformation''), because the 2D convolution routine and its gradient are given.

This gradient is computed as follows.
The gradient of the loss with respect to cell  in the input of the indexing operation is the sum of the gradients of the output cells  that index cell .
On current GPU hardware, this can be implemented efficiently using a kernel that is instantiated for each cell  in the \emph{output} array.
The kernel adds the value of the gradient of the loss with respect to cell  to cell  of the array that holds the gradient of the loss with respect to the input of the indexing operation (this array is to be initialized at zero).
Since multiple kernels write to the same cell , the additions must be done using atomic operations to avoid concurrency problems.

Alternatively, one could implement the filter transformation using a precomputed permutation matrix.
This is not as efficient, but the gradient is trivial, and most computation graph / deep learning packages will have implemented the matrix multiplication and its gradient.


\section*{Appendix C: G-conv calculus}

Although the gradient of the filter transformation operation is all that is needed to do backpropagation in a G-CNN for a split group , it is instructive to derive the analytical gradients of the G-correlation operation.
This leads to an elegant ``G-conv calculus'', included here for the interested reader.

Let feature map  at layer  be denoted , where  is the stack of feature maps in the previous layer.
At some point in the backprop algorithm, we will have computed the derivative  for all , and we need to compute  (to backpropagate to lower layers) as well as  (to update the parameters).
We find that,

where the superscript  denotes the involution 

and  is the set of filter components applied to input feature map  at layer :


To compute the gradient with respect to component  of filter , we have to G-convolve the -th input feature map with the -th output feature map:


So we see that both the forward and backward passes involve convolution or correlation operations, as is the case in standard convnets.


\end{document} 
