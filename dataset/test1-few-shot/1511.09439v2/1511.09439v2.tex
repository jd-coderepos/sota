\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{epstopdf}
\usepackage{float}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage[lined,boxed,ruled]{algorithm2e}



\newcommand{\R}[1]{\mathbb{R}^{#1}}
\newcommand{\RR}[2]{\mathbb{R}^{#1 \times #2}}
\newcommand{\Bin}[2]{[0,1]^{#1 \times #2}}
\newcommand{\G}{\mathcal{G}}

\newcommand{\inner}[2]{\langle #1, #2\rangle}
\newcommand{\norm}[2]{\left\|#1\right\|_{#2}}
\newcommand{\med}[1]{\mbox{median}\left{#1\right}}
\newcommand{\trace}[1]{\mbox{trace}\left(#1\right)}
\newcommand{\rank}[1]{\mbox{rank}\left(#1\right)}
\newcommand{\vect}[1]{\mbox{vec}\left(#1\right)}
\newcommand{\conv}[1]{\mbox{conv}\left(#1\right)}
\newcommand{\diag}{\mbox{diag}}
\newcommand{\proj}{\mathcal{P}}
\newcommand{\svt}{\mathcal{D}}
\newcommand{\prox}{\mbox{prox}}
\newcommand{\st}{\mbox{s.t.~}}
\newcommand{\pr}{\mbox{Pr}}

\DeclareMathOperator*{\argmax}{arg\!\max}
\DeclareMathOperator*{\argmin}{arg\!\min}

\newcommand{\refLemma}[1]{Lemma~\ref{#1}}
\newcommand{\refEq}[1]{(\ref{#1})}
\newcommand{\refFig}[1]{Figure~\ref{#1}}
\newcommand{\refProp}[1]{Proposition~\ref{#1}}
\newcommand{\refTheor}[1]{Theorem~\ref{#1}}
\newcommand{\refSec}[1]{Section~\ref{#1}}
\newcommand{\refCh}[1]{Chapter~\ref{#1}}
\newcommand{\refAlg}[1]{Algorithm~\ref{#1}}
\newcommand{\refTab}[1]{Table~\ref{#1}}
\newcommand{\refBib}[1]{\citep{#1}}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}


\def\Pr{\text{Pr}}
\def\eg{{e.g.~}}
\def\ie{{i.e.~}}
\def\vs{{vs.~}}
\def\wrt{w.r.t.~}
\def\etal{{et al.~}}
\def\iid{i.i.d.~}
\def\etc{{etc.~}}

\def\bfa{{\boldsymbol{a}}}
\def\bfb{{\boldsymbol{b}}}
\def\bfc{{\boldsymbol{c}}}
\def\bfd{{\boldsymbol{d}}}
\def\bfe{{\boldsymbol{e}}}
\def\bff{{\boldsymbol{f}}}
\def\bfg{{\boldsymbol{g}}}
\def\bfh{{\boldsymbol{h}}}
\def\bfi{{\boldsymbol{i}}}
\def\bfj{{\boldsymbol{j}}}
\def\bfk{{\boldsymbol{k}}}
\def\bfl{{\boldsymbol{l}}}
\def\bfm{{\boldsymbol{m}}}
\def\bfn{{\boldsymbol{n}}}
\def\bfo{{\boldsymbol{o}}}
\def\bfp{{\boldsymbol{p}}}
\def\bfq{{\boldsymbol{q}}}
\def\bfr{{\boldsymbol{r}}}
\def\bfs{{\boldsymbol{s}}}
\def\bft{{\boldsymbol{t}}}
\def\bfu{{\boldsymbol{u}}}
\def\bfv{{\boldsymbol{v}}}
\def\bfw{{\boldsymbol{w}}}
\def\bfx{{\boldsymbol{x}}}
\def\bfy{{\boldsymbol{y}}}
\def\bfz{{\boldsymbol{z}}}
\def\bfA{{\boldsymbol{A}}}
\def\bfB{{\boldsymbol{B}}}
\def\bfC{{\boldsymbol{C}}}
\def\bfD{{\boldsymbol{D}}}
\def\bfE{{\boldsymbol{E}}}
\def\bfF{{\boldsymbol{F}}}
\def\bfG{{\boldsymbol{G}}}
\def\bfH{{\boldsymbol{H}}}
\def\bfI{{\boldsymbol{I}}}
\def\bfJ{{\boldsymbol{J}}}
\def\bfK{{\boldsymbol{K}}}
\def\bfL{{\boldsymbol{L}}}
\def\bfM{{\boldsymbol{M}}}
\def\bfN{{\boldsymbol{N}}}
\def\bfO{{\boldsymbol{O}}}
\def\bfP{{\boldsymbol{P}}}
\def\bfQ{{\boldsymbol{Q}}}
\def\bfR{{\boldsymbol{R}}}
\def\bfS{{\boldsymbol{S}}}
\def\bfT{{\boldsymbol{T}}}
\def\bfU{{\boldsymbol{U}}}
\def\bfV{{\boldsymbol{V}}}
\def\bfW{{\boldsymbol{W}}}
\def\bfX{{\boldsymbol{X}}}
\def\bfY{{\boldsymbol{Y}}}
\def\bfZ{{\boldsymbol{Z}}}

\def \bfalpha {\boldsymbol{\alpha}}
\def \bfbeta {\boldsymbol{\beta}}
\def \bfgamma {\boldsymbol{\gamma}}
\def \bfmu {\boldsymbol{\mu}}
\def \bfsigma {\boldsymbol{\sigma}}

\def\bfzero{{\boldsymbol{0}}}
\def\bfone{{\boldsymbol{1}}}
\def\half{\frac{1}{2}~}
\def\reg{\mathcal{R}}
\def\incmat{{\mathcal{A}}}
 




\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\newcommand{\todo}[1]{ \textcolor{red}{\bf #1}}


\cvprfinalcopy 

\def\cvprPaperID{1648} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}



\begin{document}

\title{\vspace{-0.5em} Sparseness Meets Deepness: 3D Human Pose Estimation from Monocular Video}
\author{Xiaowei Zhou\thanks{The first two authors contributed equally to this work.}, Menglong Zhu, Spyridon Leonardos, Konstantinos G.\ Derpanis, Kostas Daniilidis \\label{eq:shape-model}
    \bfS_t = \sum_{i=1}^{k} c_{it}\bfB_i,
\label{eq:camera-model}
    \bfW_t = \bfR_t\bfS_t + \bfT_t\bfone^\top,
\label{eq:likelihood}
\pr(\bfW|\theta) \propto e^{-\mathcal{L}(\theta;\bfW)},
\label{eq:loss}
\mathcal{L}(\theta;\bfW) = \frac{\nu}{2}\sum_{t=1}^{n}\left\|\bfW_t - \bfR_t\sum_{i=1}^{k} c_{it}\bfB_i - \bfT_t\bfone^\top\right\|_F^2,
\label{eq:likelihood-complete}
\pr(\bfI,\bfW|\theta) = \pr(\bfI|\bfW)\pr(\bfW|\theta),
\label{eq:pr-I-W}
\pr(\bfI|\bfW) \propto \pr(\bfW|\bfI) = \Pi_{t}\Pi_{j}h_j(\bfw_{jt};\bfI_t),

\mathcal{R}(\theta) = \alpha\|\bfC\|_1 + \frac{\beta}{2}\|\nabla_t\bfC\|_F^2 + \frac{\gamma}{2}\|\nabla_t\bfR\|_F^2,
\label{eq:pmle}
\theta^* &= \argmax_{\theta} ~ \ln\pr(\bfW|\theta) - \mathcal{R}(\theta) \nonumber \\
& = \argmin_{\theta} ~ \mathcal{L}(\theta;\bfW) + \mathcal{R}(\theta).
\label{eq:C-step}
\bfC \leftarrow \argmin_{\bfC} ~~ \mathcal{L}(\bfC;\bfW) + \alpha\|\bfC\|_1 + \frac{\beta}{2}\|\nabla_t\bfC\|_F^2,
\label{eq:R-step}
\bfR \leftarrow \argmin_{\bfR} ~~ \mathcal{L}(\bfR;\bfW) + \frac{\gamma}{2}\|\nabla_t\bfR\|_F^2,
\label{eq:T-step}
\bfT_t \leftarrow \mbox{row mean}\left\{\bfW_t - \bfR_t\sum_{i=1}^{k} c_{it}\bfB_i\right\}.

\pr(\bfI|\theta) = \int\pr(\bfI,\bfW|\theta)d\bfW, \label{eq:marginalization}

&Q(\theta|\theta') = \int \left\{\ln\pr(\bfI,\bfW|\theta) - \mathcal{R}(\theta)\right\} ~ \pr(\bfW|\bfI,\theta') d\bfW \nonumber \\
&= \int \left\{\ln\pr(\bfI|\bfW) + \ln\pr(\bfW|\theta) - \mathcal{R}(\theta)\right\} \pr(\bfW|\bfI,\theta') d\bfW \nonumber \\
&= \mbox{const} - \int \mathcal{L}(\theta;\bfW) \pr(\bfW|\bfI,\theta') d\bfW - \mathcal{R}(\theta).
\label{eq:der1}
\int \mathcal{L}(\theta;\bfW) \pr(\bfW|\bfI,\theta') d\bfW = \mathcal{L}(\theta;\mathbb{E}\left[\bfW|\bfI,\theta'\right]) + \mbox{const},
\label{eq:der2}
\mathbb{E}\left[\bfW|\bfI,\theta'\right]
&= \int \pr(\bfW|\bfI,\theta')~ \bfW ~ d\bfW \nonumber \\
&= \int \frac{\pr(\bfI|\bfW)\pr(\bfW|\theta')}{Z} ~ \bfW ~ d\bfW,

\theta \leftarrow &\argmax_{\theta} Q(\theta|\theta') \nonumber \\
= &\argmin_{\theta} ~~ \mathcal{L}(\theta;\mathbb{E}\left[\bfW|\bfI,\theta'\right]) + \mathcal{R}(\theta),

which can be solved by \refAlg{alg:bcd}.

The entire EM algorithm is summarized in \refAlg{alg:em} with the initialization scheme described next in \refSec{sec:initialization}.

\begin{algorithm}[t]\label{alg:em}
\LinesNumbered
\caption{The EM algorithm for pose from video.}
\vspace{0.3em}
\KwIn{ \tcp*[r]{\small heat maps}}
\KwOut{ \tcp*[r]{\small pose parameters}}
\vspace{0.3em}
initialize the parameters \tcp*[r]{\small \refSec{sec:initialization}}
\While{not converged}{
\;
\vspace{0.3em}
\tcp{\small Compute the expectation of }
\;
\vspace{0.3em}
\tcp{\small Update  by \refAlg{alg:bcd}}
 \;
}
\vspace{0.5em}
\end{algorithm}

\subsection{Initialization}\label{sec:initialization}

A convex relaxation approach \cite{zhou20153d,zhou2015sparse} is used to initialize the parameters. In \cite{zhou20153d},
a convex formulation was proposed to solve the single frame pose estimation problem given 2D correspondences, which is a special case of \refEq{eq:pmle}.
The approach was later extended to handle 2D correspondence outliers \cite{zhou2015sparse}. If the 2D poses are given, the model parameters are initialized for each frame separately with the convex method proposed in \cite{zhou20153d}.
Alternatively, if the 2D poses are unknown, for each joint, the image location with the maximum heat map value is used.
Next, the robust estimation algorithm from \cite{zhou2015sparse} is applied to initialize the parameters.
 
\section{CNN-based joint uncertainty regression}\label{sec:cnn}
A CNN is used to learn the mapping , where  denotes an input image and  represents a heat map for joint . Instead of learning  networks for  joints, a fully convolutional neural network \cite{long2015fully} is trained to regress  joint distributions simultaneously by taking into account the full-body information.

During training, a rectangular patch is extracted around the subject from each image and is resized to   pixels. Random shifts are applied during cropping and RGB channel-wise random noise is added for data augmentation. Channel-wise RGB mean values are computed from the dataset and subtracted from the images for data normalization. The training labels to be regressed are multi-channel heat maps with each channel corresponding to the image location uncertainty distribution for each joint. The uncertainty is modelled by a Gaussian centered at the annotated joint location with variance . The heat map resolution is reduced to  to decrease the CNN model size which allows a large batch size in training and prevents overfitting.

The CNN architecture used is similar to the SpatialNet model proposed elsewhere \cite{pfister2015flowing} but without any spatial fusion or temporal pooling. The network consists of seven convolutional layers with  filters followed by ReLU layers and a last convolutional layer with  filters to provide dense prediction for all joints. A  max pooling layer is inserted after each of the first three convolutional layers. The network is trained by minimizing the  loss between the prediction and the label with the open source Caffe framework \cite{jia2014caffe}. Stochastic gradient descent (SGD) with momentum of 0.9 and a mini-batch size of 128 is used. 

During testing, consistent with previous 3D pose methods (e.g., \cite{li2015maximum,tekin2015predicting}), a bounding box around the subject is assumed and the image patch in the bounding box  is cropped in frame  and fed forward through the network to predict the heat maps, .





 
\section{Empirical evaluation}
\subsection{Datasets and implementation details}

Empirical evaluation was performed on two datasets -- Human3.6M \cite{ionescu2014human} and PennAction \cite{zhang2013actemes}.

The Human3.6M dataset \cite{ionescu2014human} is a recently published large-scale dataset for 3D human sensing. It includes millions of 3D human poses acquired from a MoCap system with corresponding images from calibrated cameras.
This setup provides synchronized videos and 2D-3D pose data for evaluation. It includes 11 subjects performing 15 actions, 
such as eating, sitting and walking. The same data partition protocol as in previous work was used \cite{li2015maximum,tekin2015predicting}: the data from five subjects (S1, S5, S6, S7, S8) was used for training and the data from two subjects (S9, S11) was used for testing. The original frame rate is 50 fps and is downsampled to 10 fps.

The PennAction dataset \cite{zhang2013actemes} is a recently introduced in-the-wild human action dataset
containing 2326 challenging consumer videos.  The dataset consists of
15 actions, such as golf swing, bowling, and tennis swing. Each of the video sequences is manually annotated frame-by-frame with 13 human body joints in 2D.
In evaluation, PennAction's training and testing split was used which consists of an even split of the videos between training and testing.

The algorithm in \cite{zhou2015sparse} was used to learn the pose dictionaries. The dictionary size was set to  for action-specific dictionaries and  for the nonspecific action case.
For all experiments, the parameters of the proposed model were fixed (, , ,  in a normalized 2D coordinate system).

\subsection{Evaluation with known 2D poses} 

First, the evaluation of the 3D reconstructability of the proposed method with known 2D poses is presented. The generic approach to 3D reconstruction from 2D correspondences across a sequence is NRSFM. The proposed method is compared to the state-of-the-art method for NRSFM \cite{dai2012simple} on the Human3.6M dataset. A recent baseline method for single-view pose reconstruction Projected Matching Pursuit (PMP) \cite{ramakrishna2012reconstructing} is also included in comparison.

The sequences of S9 and S11 from the first camera in the Human 3.6M dataset were used for evaluation and frames beyond 30 seconds were truncated for each sequence. The 2D orthographic projections of the 3D poses provided in the dataset were used as the input. Performance was evaluated by the mean per joint error (mm) in 3D by comparing the reconstructed pose against the ground truth. As the standard protocol for evaluating NRSFM, the error was calculated up to a similarity transformation via the Procrustes analysis. To demonstrate the generality of the proposed approach, a single pose dictionary from all the training pose data, irrespective of the action type, was used, i.e., a non-action specific model. The method from Dai et al.\ \cite{dai2012simple} requires a predefined rank .  Here, various
values of  were considered with the best result for each sequence reported.



\begin{table}
\centering
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l*{15}{c}}
\toprule
 & Original & Synthesized \\
\toprule
PMP \cite{ramakrishna2012reconstructing} & 89.50 & 84.16 \\
NRSFM \cite{dai2012simple} & 72.98 & 48.88 \\
Single frame initialization & 50.04 & 48.08 \\
Optimization by \refAlg{alg:bcd} & \textbf{49.64} & \textbf{47.57} \\
\toprule
\end{tabular}
\vspace{0.25em}
\caption{3D reconstruction given 2D poses. Two input cases are considered: original 2D pose data from Human3.6M and synthesized 2D pose data with artificial camera motion. The numbers are the mean per joint errors (mm) in 3D. }\label{tab:nrsfm}
\vspace{-1em}
\end{table}

\begin{table*}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l*{15}{c}}
\toprule
 & Directions & Discussion & Eating & Greeting & Phoning & Photo & Posing & Purchases \\
\toprule
LinKDE \cite{ionescu2014human} & 132.71 & 183.55 & 132.37 & 164.39 & 162.12 & 205.94 & 150.61 & 171.31 \\
Li et al. \cite{li2015maximum} & - & 136.88 & 96.94 & 124.74 & - & 168.68 & - & - \\
Tekin et al. \cite{tekin2015predicting} & 102.39 & 158.52 & 87.95 & 126.83 & 118.37 & 185.02 & 114.69 & 107.61 \\
Proposed & \textbf{87.36} & \textbf{109.31} & \textbf{87.05} & \textbf{103.16} & \textbf{116.18} &  \textbf{143.32} & \textbf{106.88} & \textbf{99.78} \\
\toprule
 & Sitting & SittingDown & Smoking & Waiting & WalkDog & Walking & WalkTogether & Average \\
\toprule
LinKDE \cite{ionescu2014human} & 151.57 & 243.03 & 162.14 & 170.69 & 177.13 & 96.60 & 127.88 & 162.14  \\
Li et al. \cite{li2015maximum} & - & - & - & - & 132.17 & 69.97 & - & - \\
Tekin et al. \cite{tekin2015predicting} & 136.15 & 205.65 & 118.21 & 146.66 & 128.11 & \textbf{65.86} & \textbf{77.21} & 125.28  \\
Proposed & \textbf{124.52} & \textbf{199.23} & \textbf{107.42} & \textbf{118.09} & \textbf{114.23} & 79.39 & 97.70 & \textbf{113.01} \\
\toprule
\end{tabular}
\vspace{0.25em}
\caption{Quantitative comparison on Human 3.6M datasets. The numbers are the mean per joint errors (mm) in 3D evaluated for different actions of Subjects 9 and 11. }\label{tab:h36m}
\vspace{-1em}
\end{table*}

The results are shown in the second column of \refTab{tab:nrsfm}. The proposed method clearly outperforms the NRSFM baseline. The reason is that the videos are captured by stationary cameras. Although the subject is occasionally rotating, the ``baseline" between frames is generally small, and neighboring views provide insufficient geometric constraints for 3D reconstruction. In other words, NRSFM is very difficult to compute with slow camera motion. This observation is consistent with prior findings in the NRSFM literature, e.g., \cite{akhter2011trajectory}. To validate this issue, an artificial rotation was applied to the 3D poses by 15 degrees per second and the 2D joint locations were synthesized by projecting the rotated 3D poses into 2D. The corresponding results are presented in the third column of \refTab{tab:nrsfm}. In this case, the performance of NRSFM improved dramatically. Overall, the experiments demonstrate that the structure prior (even a non-action specific one) from existing pose data is critical for reconstruction.
This is especially true for videos with small camera motion, which is common in real world applications. The temporal smoothness helps but the change is not significant since the single frame initialization is very stable with known 2D poses. Nevertheless,
in the next section it is shown that the temporal smoothness is important when 2D poses are not given.

\subsection{Evaluation with unknown poses: Human3.6M }\label{sec:h36m}


\begin{table}
\centering
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l*{15}{c}}
\toprule
 & 3D (mm) & 2D (pixel) \\
\toprule
Single frame initialization & 143.85 & 15.00 \\
Optimization by \refAlg{alg:em} & 125.55 & 10.85 \\
Perspective adjustment & \textbf{113.01} & \textbf{10.85} \\
\hline
No smoothness & 120.99 & 11.25 \\
No action label & 116.49 & 10.87 \\
\toprule
\end{tabular}
\vspace{0.25em}
\caption{The estimation errors after separate steps and under additional settings. The numbers are the average per joint errors for all testing data in both 3D and 2D.}\label{tab:steps}
\vspace{-1em}
\end{table}

\begin{figure*}
  \centering
\includegraphics[width=0.42\linewidth]{figures/examples/S11-correction-Directions.pdf}\hspace{3em}
  \includegraphics[width=0.42\linewidth]{figures/examples/S11-correction-Discussion.pdf}
  \includegraphics[width=0.42\linewidth]{figures/examples/S11-correction-Eating.pdf}\hspace{3em}
  \includegraphics[width=0.42\linewidth]{figures/examples/S11-correction-Greeting.pdf}
  \includegraphics[width=0.42\linewidth]{figures/examples/S11-correction-Phoning.pdf}\hspace{3em}
  \includegraphics[width=0.42\linewidth]{figures/examples/S11-correction-Photo.pdf}
  \includegraphics[width=0.42\linewidth]{figures/examples/S11-correction-Posing.pdf}\hspace{3em}
  \includegraphics[width=0.42\linewidth]{figures/examples/S11-correction-Purchases.pdf}
  \includegraphics[width=0.42\linewidth]{figures/examples/S11-correction-Sitting.pdf}\hspace{3em}
  \includegraphics[width=0.42\linewidth]{figures/examples/S11-correction-SittingDown.pdf}
  \includegraphics[width=0.42\linewidth]{figures/examples/S11-correction-Smoking.pdf}\hspace{3em}
  \includegraphics[width=0.42\linewidth]{figures/examples/S11-correction-Waiting.pdf}
  \includegraphics[width=0.42\linewidth]{figures/examples/S11-correction-WalkDog.pdf}\hspace{3em}
  \includegraphics[width=0.42\linewidth]{figures/examples/S11-correction-Walking.pdf}
  \caption{Example frame results on Human3.6M, where the errors in the 2D heat maps are corrected after considering the pose and temporal smoothness priors. Each row includes two examples from two actions. The figures from left-to-right correspond to the heat map (all joints combined), the 2D pose by greedily locating each joint separately according to the heat map, the estimated 2D pose by the proposed EM algorithm, and the estimated 3D pose visualized in a novel view. The original viewpoint is also shown. }\label{fig:h36m}
\end{figure*}

Next, results on the Human3.6M dataset are reported when 2D poses are not given. The proposed method is compared to three recent baseline methods. The first baseline method is LinKDE which is provided with the Human3.6M dataset \cite{ionescu2014human}.  This baseline is based on single frame regression. The second one is from Tekin et al.\ \cite{tekin2015predicting} which extends the first baseline method by exploring motion information in a short sequence. The third one is a recently published CNN-based method from Li et al.\ \cite{li2015maximum}.

In this experiment, the sequences of S9 and S11 from all cameras were used for evaluation. The standard evaluation protocol of the Human3.6M dataset was adopted, i.e., the mean per joint error (mm) in 3D is calculated between the reconstructed pose and the ground truth in the camera frame with their root locations aligned. Note that the Procrustes alignment is not allowed here. In general, it is impossible to determine the scale of the object in monocular images. The baseline methods learned the scale from training subjects.  For a fair comparison, the reconstructed pose by the proposed method was scaled such that the mean limb length of the reconstructed pose was identical to the average value of all training subjects. As the alignment to the ground truth was not allowed, the joint error was largely affected by the camera rotation estimate, and empirically the misalignment was largely due to the adopted weak perspective camera model. To compensate the misalignment, the rotation estimate was refined for each frame with a perspective camera model (the 2D and 3D human pose estimates were fixed) by a perspective-n-point (PnP) algorithm \cite{lu2000fast}

The results are summarized in \refTab{tab:h36m}. The table shows that the proposed method achieves the best results on most of the actions except for ``walk" and ``walk together", which involve very predictable and repetitive motions and might favor the direct regression approach \cite{tekin2015predicting}. In addition, the results of the proposed approach have the smallest variation across all actions with a standard deviation of   versus  from Tekin et al.

In \refTab{tab:steps},  3D reconstruction and 2D joint localization results are provided under several setup variations of the proposed approach.
Note that the 2D errors are with respect to the normalized bounding box size . The table shows that the convex initialization provides suitable initial estimates, which are further improved by the EM algorithm that integrates joint detection uncertainty and temporal smoothness. The perspective adjustment is important under the Human3.6M evaluation protocol, where Procrustes alignment to the ground truth is not allowed.  The proposed approach was also evaluated under two additional settings.
In the first setting, the smoothness constraint was removed from the proposed model by setting .  As a result, the average error significantly increased.
This demonstrates the importance of incorporating temporal smoothness. In the second setting, a single CNN and pose dictionary was learned from all training data.  These models were then applied
to all testing data without distinguishing the videos by their action class.
As a result, the estimation error increased, which is attributed to the fact that the 3D reconstruction ambiguity is greatly enlarged if the pose prior is not restricted to an action class.

\refFig{fig:h36m} visualizes the results of some example frames. While the heat maps may be erroneous due to occlusion, left-right ambiguity, and other uncertainty from the detectors, the proposed EM algorithm can largely correct the errors by leveraging the pose prior, integrating temporal smoothness, and modelling the uncertainty.

\subsection{Evaluation with unknown poses: PennAction}


\begin{figure*}
  \centering
\includegraphics[width=0.45\linewidth]{figures/examples/penn-golf_swing-0897.pdf}\hspace{2em}
  \includegraphics[width=0.45\linewidth]{figures/examples/penn-tennis_forehand-2042.pdf}
  \includegraphics[width=0.45\linewidth]{figures/examples/penn-golf_swing-0891.pdf}\hspace{2em}
  \includegraphics[width=0.45\linewidth]{figures/examples/penn-tennis_forehand-2030.pdf}
  \includegraphics[width=0.45\linewidth]{figures/examples/penn-golf_swing-0953.pdf}\hspace{2em}
  \includegraphics[width=0.45\linewidth]{figures/examples/penn-tennis_forehand-2077.pdf}
  \includegraphics[width=0.45\linewidth]{figures/examples/penn-golf_swing-0890.pdf}\hspace{2em}
  \includegraphics[width=0.45\linewidth]{figures/examples/penn-tennis_forehand-2118.pdf}
  \caption{Example results on PennAction. Each row includes two examples. In each example, the figures from left-to-right correspond to the ground truth superimposed on the image, the estimated pose using the baseline approach \cite{yang2011articulated}, the estimated pose by the proposed approach, and the estimated 3D pose visualized in a novel view. The original viewpoint is also shown. }\label{fig:penn}
\end{figure*}

Finally, the applicability of the proposed approach for pose estimation with in-the-wild videos is demonstrated. Results are reported using two actions from the PennAction dataset:
``golf swing" and ``tennis forehand", both of which are very challenging due to large pose variability, self-occlusion, and image blur caused by fast motion.
For the proposed approach, the CNN was trained using the annotated training images from the PennAction dataset, while the pose dictionary was learned with publicly available MoCap data\footnote{Data sources: \url{http://mocap.cs.cmu.edu} and \url{http://www.motioncapturedata.com}}. Due to the lack of 3D ground truth, quantitative 2D pose estimation results are reported and compared with the publicly available 2D pose detector from Yang and Ramanan \cite{yang2011articulated}.  The baseline was retrained on the PennAction dataset. Note that the baseline methods considered in \refSec{sec:h36m} are not applicable here since they require synchronized 2D image and 3D pose data for training.

To measure joint localization accuracy, both the widely used per joint distance errors and the probability of correct keypoint (PCK) metrics are used.
The PCK metric measures the fraction of correctly located joints with respect to a threshold. Here, the threshold is set to  pixels which is roughly the half length of a head segment.

 \refTab{tab:penn} summarizes the quantitative results.
The initialization step alone outperformed the baseline. This demonstrates the effectiveness of CNN-based approaches, which has been shown in many recent works, e.g., \cite{toshev2014deep,pfister2015flowing}. The proposed EM algorithm further improves upon the initialization results by a large margin by integrating the geometric and smoothness priors.
Several example results are shown in \refFig{fig:penn}. It can be seen that the proposed method successfully recovers the poses for various subjects under a variety of viewpoints. In particular, compared to the baseline, the proposed method does not suffer from the well-known ``double-counting'' problem for tree-based models \cite{yang2011articulated} due to the holistic 3D pose prior.

\begin{table}
\centering
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l*{15}{c}}
\toprule
 & Baseline & Initial & Optimized \\
\toprule
Golf & 24.78 / 0.38 & 18.73 / 0.45 & \textbf{14.03 / 0.54} \\
Tennis & 29.15 / 0.40 & 25.75 / 0.42 & \textbf{20.99 / 0.45} \\
\toprule
\end{tabular}
\vspace{0.25em}
\caption{2D pose errors on PennAction. Each pair of numbers correspond to the per joint distance error (pixels) and the PCK metric. The baseline is the retrained model from Yang and  Ramanan \cite{yang2011articulated}. The last two columns correspond to the errors after initialization and EM optimization in the proposed approach.}\label{tab:penn}
\vspace{-1em}
\end{table}

\subsection{Running time}

The experiments were performed on a desktop with an Intel i7 3.4G CPU, 8G RAM and a TitanZ GPU.
The running times for CNN-based heat map generation and convex initialization were roughly 1s and 0.6s per frame, respectively; both steps can be easily parallelized. The EM algorithm usually converged in 20 iterations with a CPU time less than 100s for a sequence of 300 frames.
 
\section{Summary}
In summary, a  3D pose estimation framework from video has been presented that consists of a novel synthesis between a deep learning-based 2D part
regressor, a sparsity-driven 3D reconstruction approach and a 3D temporal smoothness prior.  This joint consideration combines the discriminative power of state-of-the-art 2D part detectors,
the expressiveness of 3D pose models and regularization by way of aggregating information over time.
In practice, alternative joint detectors, pose representations and temporal models can be conveniently integrated in the proposed framework by replacing the original components.  Experiments demonstrated that 3D geometric priors and temporal coherence can not only help 3D reconstruction but also improve 2D joint localization. Future extensions may include incremental algorithms for online tracking-by-detection and handling multiple subjects.   
\small

\bigskip
\noindent\textbf{Supplementary material}:
The MATLAB code, evaluation on the HumanEva I dataset, demonstration videos, and other supplementary materials are available at: \url{http://cis.upenn.edu/~xiaowz/monocap.html}.

\bigskip
\noindent\textbf{Acknowledgments}: The authors are grateful for support through the following grants:
NSF-DGE-0966142, NSF-IIS-1317788, NSF-IIP-1439681, NSF-IIS-1426840, ARL MAST-CTA W911NF-08-2-0004, ARL RCTA W911NF-10-2-0016, ONR N000141310778, and NSERC Discovery.
 
\newpage
\small
\bibliographystyle{ieee}
\bibliography{bibref_definitions_short,bibref}

\end{document}
