\documentclass[preprint]{article}









\usepackage{arxiv}



\usepackage[utf8]{inputenc} \usepackage[numbers]{natbib}
\usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{wrapfig,booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{wrapfig}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathtools}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}

\usepackage{xcolor}

\title{Self-supervised Smoothing Graph Neural Networks}






\author{Lu Yu, Shichao Pei, Chuxu Zhang, Lizhong Ding\\
     \textbf{Jun zhou}, \textbf{Longfei Li}, \textbf{Xiangliang Zhang}\\
   1. King Abdullah University of Science and Technology, Saudi Arabia\\
  2. Ant Financial Services Group, China\\
  3. Brandeis University, USA\\
  4.  Inception Institute of Artificial Intelligence, UAE\\
   \texttt{\{lu.yu, xiangliang.zhang\}@kaust.edu.sa} \\
}

\begin{document}

\maketitle

\begin{abstract}
This paper studies  learning node representations with GNNs for unsupervised scenarios. We make a theoretical understanding and empirical demonstration about the non-steady performance of GNNs over different graph datasets, when the supervision signals are not appropriately defined. The performance of GNNs depends on both the node feature smoothness and the graph locality.  To smooth the discrepancy of node proximity measured by graph topology and node feature, we proposed KS2L - a novel graph \underline{K}nowledge distillation regularized \underline{S}elf-\underline{S}upervised \underline{L}earning framework, with two complementary regularization modules, for intra-and cross-model graph knowledge distillation.  We demonstrate the competitive performance of KS2L on a variety of  benchmarks. Even with a single GCN layer,  KS2L  has consistently competitive or even better performance on various benchmark datasets.
\end{abstract}

\section{Introduction}
Graph Neural Networks (GNNs) have been leading an effective framework of learning graph representations, and have been demonstrated powerful in numerous tasks \cite{wu2020comprehensive}. The key of GNNs roots at the repeated aggregation over local neighbors to obtain smoothing node representations, making close nodes have similar representations by filtering out noise existing in the raw node features. 
Learning GNN models to maintain local smoothness usually depends on supervision signals such as node labels or self-supervision signals extracted from the input graph. 
Whereas, labels are not always available in many scenarios. Besides, the trained GNNs in supervised and semi-supervised ways are not universal applicable, and only serve the defined learning tasks, e.g., node classification when the GNNs were trained with node labels, or graph classification when the GNNs are trained with graph labels.
The real-world application scenarios might necessitate that GNNs are trained in an unsupervised way, without any requirement of labels. However, the key challenge is the definition of the self-supervision signals from the input graph. 

\begin{figure}
\label{fig:loc:cc}
\centering
\includegraphics[scale=0.6]{cc.eps}
\vspace{-0.2cm}
\caption{Locality and node feature smoothness in the six graphs used in experimental evaluation. The larger feature smoothness value is, the more smoothing feature we have.}
\end{figure}

There are a group of unsupervised node representation learning models in the spirit of \emph{self-supervised learning}. They   construct supervision signals   from the graph structure (\emph{e.g.}, local structure) to enforce the local nodes to have similar representations. There already exist several successful solutions based on random walks (\emph{e.g.,} DeepWalk~\cite{perozzi2014deepwalk}, Node2Vec~\cite{grover2016node2vec}) or unified factorization method (\emph{e.g.} NetMF~\cite{qiu2018network}) to enforce nodes in the same local context (\emph{e.g.}, neighbors in a window size) to be close (\emph{i.e.}, smoothness). As an initial GNN study, GraphSage~\cite{hamilton2017inductive} directly applies the unsupervised loss function borrowed from DeepWalk~\cite{perozzi2014deepwalk}. It has been shown  that its performance downgraded in certain graphs that have weak locality measured by averaged clustering coefficient \cite{saramaki2007generalizations}. For example, PubMed is such a graph with weak locality, as shown in Figure~\ref{fig:loc:cc}. GraphSage and its variants were hard to make significant performance improvements when making usage of  the learned   node representation in PubMed.

The story behind the   inefficacy is that
the self-supervising signals in DeepWalk are designed to extract local structure dependency, but discard the fact that the representations quality of GNNs depend on both node feature smoothness and the locality of the graph structure, referring to Theorem~\ref{gcn:high:prox}. We also empirically found that most of the neural encoders (e.g., GNNs, even simple MLPs) performed well on   node classification   in graphs like CS, which has a strong locality and 
 large feature smoothness (defined as the inverse of average pairwise distance among the node features~\cite{Hou2020Measuring}), shown in Figure~\ref{fig:loc:cc}. However, for graphs with a strong locality but a low node feature smoothness (e.g.,  ``Computers", ``Photo"), unsupervised methods directly optimizing structure proximity can have significant better performance than those   (semi-)supervised methods working on both node features and structure proximity. Based on our theoretical understanding in Theorem 1 and empirical observation, we conjecture that the discrepancy of node proximity measured by graph topology and node 
feature is the key reason of such non-steady performance of GNNs over different graph datasets, when the supervision signals are not appropriately defined.

We propose a new self-supervised  learning strategy to smooth the discrepancy of node proximity measured by graph topology and node feature, especially in the cases that either the node feature smoothness is low or the graph locality is weak. We introduced a global-level consistency regularization module, known as \emph{intra-} and \emph{cross-model graph knowledge distillation}. The intra-model regularization aims at forcing the learnt representations and node features to have consistent subgraph-level relation distribution. Through stacking the cross-model distillation module, we implicitly mimic the deep smoothing operation with a shallow GNN (\emph{e.g.} only a single GNN layer), while avoiding to bring noisy information from high-order neighbors since each GNN only depends on the neighbors at low-order. Thus our proposed model KS2L - a novel graph \underline{K}nowledge distillation regularized \underline{S}elf-\underline{S}upervised \underline{L}earning framework  can learn shallow but powerful GNN. Even with a single GCN layer, it has consistently competitive or even better performance on various benchmark datasets.

\section{Related Work} \label{sec:relatedwork}
\textbf{Graph Neural Networks.}  In recent years, we have witnessed a fast progress of graph neural network in both methodology study and its applications. As one of the pioneering research, Defferrard \emph{et al.}~\cite{defferrard2016convolutional} generalizes the convolution operation to non-Euclidean graph data through bridging the gap between graph spectral theory and deep learning methods. Kipf \emph{et al.}~\cite{kipf2016semi} reduce the computation complexity to 1-order Chebyshev approximation with an affined assumption. Different sampling strategies~\cite{chen2018fastgcn} are explored to control the receptive field (\emph{i.e.} the number of neighbors) while keeping the estimated aggregated embedding unbiased. NT \emph{et al.}~\cite{Hoang:2019:LPF} justify that classical graph convolution network (GCN) and its variants are just low-pass filter. At the same time, both of studies~\cite{li:2019:label,klicpera:2019:diffusion} propose to replace standard GCN layer with a normalized high-order low-pass filters (\emph{e.g.} personalized PageRank, heats kernel). This conclusion can help to answer why simplified GCN proposed by Wu \emph{et al.}~\cite{wu2019simplifying} has competitive performance with complicated multi-layer GNNs. Besides GCNs, many novel GNNs have been proposed, such as multi-head attention models~\cite{velivckovic2017graph,ma2019disentangled}, graph U-Net~\cite{gao2019graph} \emph{etc}.

\textbf{Self-supervised Learning Strategies in Graph Representation Learning.}  
In addition to the line of work following Deepwalk for constructing self-supervising signals, mutual information (MI) maximization~\cite{velivckovic2018deep} over the input and output representations shows up as an alternative line. In analogy to distinguishing that output image representation is generated from the input image patch or noisy image, Veli{\v{c}}kovi{\'c} \emph{et al.}~\cite{velivckovic2018deep} propose deep graph infomax (DGI) criterion to maximize the mutual information between a high-level ``global" graph summary vector and a ``local" patch representation.  However, the graph-level vector summarizes the global context, which might not be shared by all nodes, and thus can easily transfer noisy information into the interactions among nodes.   Another line of self-supervised learning methods like InfoGraph~\cite{sun2019infograph} and pre-training graph neural networks~\cite{hu2019strategies} are designed for learning representations for a whole graph, not for nodes.

\section{Methodology} \label{sec:mtd}
Let  denote an   attribute graph where  is the node set  ,   is the edge set, and  is the node \emph{feature matrix} where each row  stands for the \emph{node} feature of . We use \textbf{A} represents the node \emph{relation matrix}  where   if there existing a link between node  and , i.e., , otherwise . We define the degree matrix  where each element equals to the row-sum of adjacency matrix . 

Our goal is to learn an \emph{encoder}  , where  is the representation learned for nodes in . The vector    
can be used for various downstream tasks, \emph{e.g.}, node classification and link prediction. GNNs generate  through repeated aggregation over local neighbors' features.  Formally the \emph{l}-th layer of a GNN can be defined as

where  denotes the representation of node  at the \emph{l}-th layer,  is the set of neighbors of node . The  can be addition or concatenation operations, and  can be weighted sum over neighbors. 
The vector    actually summarizes a graph \emph{patch} centered around node , the size of the summarized  region    exponentially grows as the increasing of . In what follows, we   refer to  as \emph{patch} representations.
Let  denote the low-level \emph{node feature} which can be regarded as the -th layer. Since the low-level feature  only depends on node  itself, we refer to it as the \emph{node feature}.   Besides the \emph{node} and \emph{patch} representations, one can also summarize a graph-level representation   through applying a READOUT operation over . Typical operations include mean or max pooling functions or more sophisticated ones~\cite{ying2018hierarchical}. 

\begin{theorem}\label{gcn:high:prox}
Suppose that a GNN aggregates node representations as , where  can be generated through attention operation or the element of a normalized relation matrix. The graph neural operator approximately equals to a second-order proximity graph regularization over the patch representations, whose smoothness depends on both node feature smoothness and second-order node proximity.
\end{theorem}
The proof of Theorem 1 is provided in the supplementary materials. 

\subsection{Self-Supervised Learning of GNNs}\label{ssl:gnn}
The weights in \emph{COMBINE/AGGREGATE} functions of  GNNs are learned to generate  to be used in downstream applications. For (semi-)supervised learning (e.g., node classification), the loss function of such GNNs can be defined by the classification loss. When there is no labeled data available, self-supervised learning as a brunch of unsupervised learning sheds light to design learning tasks with supervised signal from input graph itself. 
Contrastive learning~\cite{perozzi2014deepwalk,mnih2013learning}, one of the  self-supervised learning strategies,  can be adopted  to distinguish the connected (\emph{positive sample}) and unconnected (\emph{negative} sample) nodes in the graph. The objective is to enforce the predicted proximity derived from  for existing edges  to be larger than that defined on    for non-existing edges .    Despite theoretically reasonable, the loss function formed by the constrastive samples in practice (e.g., academic networks) has several limitations. First,  and  are significantly closer  than  and  in the embedding space if  and  have a longer shortest path  than  and . Such discrepancy is already exhibited by the distance on graph. The representation  optimized to  discriminate such constrastive samples captures more the feature of their location difference on graph topology, than the node attribute difference. In other words, the learned  can be dominated by the difference on graph location, and make content difference under-represented. 

To overcome the above-discussed issue in contrastive loss, we propose to compare  with , rather than comparing two of . Concretely,  
instead of constructing contrastive learning loss over the patch representations  at the same GNN layer, we turn to maximize the predicting probability between a patch representation  and its input node features  in its neighbors. Formally, for a sample set  where   but , the loss  is defined on the pairwise comparison of   and . Therefore, our self-supervised learning for GNN has the loss function defined below,

where  can be an arbitrary contrastive loss function,  is a scoring function, and  is the regularization function with weight  for implementing graph structural constraints (to be introduced in next section). There are lots of candidates for contrastive loss , typically including logistic pairwise loss , or cross-entropy loss , where . 
\begin{figure}
    \centering
    \label{overall:architecture}
    \includegraphics[scale=0.48]{intra-cross2.eps}
    \caption{Overall architecture of the proposed self-supervised learning GNN with Intra- and Cross-model Distillation (top); The pioneering works  \emph{deep graph infomax} and \emph{InfoGraph} (bottom).}
    \label{fig:my_label}
\end{figure}

\textbf{Discussion}: It's noted that the proposed objective function is related to Deep Graph Infomax (DGI)~\cite{velivckovic2018deep}. Both of them are designed for learning  node representations in unsupervised ways. DGI focuses on maximizing mutual information between the graph summary vector and patch representation. By doing so, the graph summary representation shared by all of nodes may transfer noisy information to among unrelated nodes. While the proposed strategy here emphasizes more on the dependency   between a patch representation and its input node features.



\subsection{Graph Knowledge Distillation Regularization}

The objective function defined in Equation (\ref{ssl:obj:1}) models the interactions between output patch representations  and input node features , which can be regarded as an \emph{intra-model knowledge distillation} process from smoothed node embeddings to denoise low-level node features.
However,  the raised contrastive samples over edge connectivity might fail to represent a whole picture of node representation distribution.  To supplement the loss defined on the individual pairwise  samples,    we introduce a regularization term ensuring the distribution consistency on the relations between the learned patch representations  and the node features  over a set of randomly sampled nodes.
Let  denote the randomly sampled pseudo relation graph, where  and  is the number of sampled pseudo local neighbors for center node . The estimated proximity for each node in -th local structure  is computed by

where  and  denote the similarity estimated from different node representations between node  and . The  will act as the teacher signal to guide the node features  to agree on the relation distribution over a random sampled graph. For the node , the relation distribution similarity can be measured as 
Then we can compute the relation similarity distribution over all the nodes as 

where  acts as a regularization term generated from the intra-model knowledge, and    to push the learned patch representation  and node features  being consistent at a subgraph-level. 

The second regularization is to introduce the \emph{cross-model} distillation for addressing the over-smoothing issue that limits most of GNNs to go deep. The cross-model distillation can guide the target GNN model by transferring the learned self-supervised knowledge. Through stacking the cross-model distillation module, we implicitly mimic the deep smoothing operation with a shallow GNN (\emph{e.g.} only a single GNN layer), while avoiding to bring noisy information from high-order neighbors, since each GNN only depends on the neighbors at low-order. The overall cross-model distillation framework is shown in Figure 2. Working with a sequence of  GNN models ,   each    distills knowledge from the previous  model . Since no labels are available, we propose to implement \emph{representation distillation}~\cite{tian2019contrastive} with the constraint of graph structure. The knowledge distillation module consists of two parts, defined as

where  is the patch representations from GNN model , and  = . To define    module , we should meet several requirements: 1) this function should be easy to compute and friendly to back-propagation strategy; and 2) it should stick to  the graph structure constraint. We resort to the conditional random field (CRF)~\cite{lafferty2001conditional} to capture the pairwise relationship between different nodes. 
For a general      knowledge distillation  module , the dependency of  on  can be given following the CRF model:

where  is the normalization factor and  stands for the energy function, defined as follows:

where  and  are the unary and pairwise energy function, respectively. The parameter  is to control the importance of two energy  functions.
When  is the node feature of student model and  is the patch representation from teacher model ,  the energy function defined in Equation~(\ref{reg:crf})    enforces the node  representation from student model to be close to that in the teacher model and its neighbor nodes. After obtaining the energy function, we can resolve the CRF objective with the mean-field approximation method by employing a simple distribution  to approximate the distribution . Specifically distribution  can be initialized as the product marginal distributions as . Through minimizing the KL divergence between these two distributions as follows:

Then we can get the optimal  as follows:

According to Equation~(\ref{crf:p}) and (\ref{reg:crf}), we can get 

which shows that  is a Gaussian function. By computing its expectation, we have the optimal solution for  as follows:

Then we can get the cross-model knowledge distillation rule by enforcing the node representations from student model to have minimized metric distance to . After replacing the random variables  as the node representation  of teacher model , then we can get the final distillation regularization as follows:

where  denotes the   feature of node  from -th GNN model, and  denotes the output patch representation for node  of -th GNN model. In terms of  in Equation (\ref{crf:com}), we have many choices  such as attentive weight, or mean pooling \emph{etc.} In this work, we simply initialize it with mean-pooling operation over the node representations. 

The overall self-supervised learning objective in this work can be extended as follows by taking the two regularization terms:

where  can be initialized through optimizing the proposed self-supervised learning without the cross-model distillation regularization.
 

 

\section{Experimental Evaluation}
\textbf{Dataset and Baselines.} To evaluate the proposed self-supervised training method for GNN, we compare it with various state-of-the-art methods on six datasets including three citation networks (Cora, Citeseer, Pubmed), two Amazon product co-purchase networks (Computers, Photo), and one co-author network subjected to a subfield Computer Science (CS). The citation networks can be obtained from the official implementation\footnote{https://github.com/tkipf/gcn} of GCN~\cite{kipf2016semi}. The Amazon and co-author graphs are benchmark datasets officially available at pytorch\_geometric~\cite{fey2019fast}. The data statistics can be found in Table~\ref{stat:data}.
The baselines we compared  include Label Propagation (LP)~\cite{zhu2003semi}, Planetoid~\cite{yang2016revisiting}, Chebyshev filters (ChebNet)~\cite{defferrard2016convolutional}, MLP~\cite{kipf2016semi}, Graph Convolution Network (GCN)~\cite{kipf2016semi}, Simplified GCN (SGC)~\cite{wu2019simplifying}, Disentangled Graph Convolution Network (DisenGCN)~\cite{ma2019disentangled}, Graph Attention Network (GAT)~\cite{velivckovic2017graph}, Graph Markov Neural Network (GMNN)~\cite{qu2019gmnn}, Deep Graph Infomax (DGI)~\cite{velivckovic2018deep}.

\textbf{Experiment Configuration.}
LP and  Planetoid are two basic methods.
We quote their performance results from~\cite{kipf2016semi}. 
In terms of the rest of baselines, we use the official code provided by the authors. If the results are not previously reported, we implement them and conduct a hyper-parameter search according to the original paper. We use a one-layer GCN as the base model , for the proposed self-supervised method (KS2L) to demonstrate its capability to learn a shallow GNN with competitive or even better performance than deep GNNs. The specific model configuration can be found from the supplementary materials. The results shown in this work are average value over 10 runs for each task.

For the node classification, the experimental configuration for the three citation data is the same as~\cite{kipf2016semi}, and we use the default hyper-parameters for the baselines suggested by the authors. To make a fair comparison on the other co-purchase and co-author networks, we randomly select 20\% nodes for supervised training GNNs, 10\% node as validation set, and the rest of nodes are left for testing. For the unsupervised methods (\emph{i.e.} DGI and the proposed KS2L), they can access to the complete graph without using node labels.

For the link prediction task, we use leave-one-out strategy to randomly select one edge for each node as testing, and the rest as the training set. All baselines involved in this experiment only work on the graph built from the training set. For the supervised learning models like GCN, GAT, DisenGCN, GMNN, we still use the same configure as the node classification task to select labelled nodes to train the model, which means the nodes in the selected testing edges can still be labelled nodes for the supervised training strategy. While the unsupervised methods like DGI and KS2L can only access to the edges in the training set.

\begin{table}[!tp]
\caption{Summary of datasets used for experimental analysis}
\label{stat:data}
\centering
\footnotesize
\begin{tabular}{c c c c c c c}
\toprule
	\textbf{Data} & \textbf{Type} & \textbf{\#Nodes} & \textbf{Edges} & \textbf{Features} & \textbf{Labels} & \textbf{Label Rate}\\
\midrule
	 Cora & Citation & 2,708 & 5,429 & 1,403 & 7 & 0.052 \\
	 Citeseer & Citation & 3,327 & 4,732 & 3,703 & 6 & 0.036\\
         Pubmed & Citation & 19,717 & 44,338 & 500 & 3 & 0.003 \\
	Computers & Co-purchase & 13,752 & 287,209 & 767 & 10 & 0.2\\
	Photo & Co-purchase & 7,650 & 143,663 & 745 & 8 & 0.2\\
	CS & Co-author & 18,333 & 81,894 & 6,805 & 15 & 0.2\\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!tp]
\caption{Accuracy of node classification (in ).}
\label{result:node:clf}
\centering
\footnotesize
\begin{tabular}{c c c c c c c c}
\toprule
	\textbf{Input} &\textbf{Methods} & \textbf{Cora} & \textbf{Citeseer} & \textbf{PubMed} & \textbf{Computers} & \textbf{Photo} & \textbf{CS}\\
\midrule
	\multirow{10}{*}{\textbf{X},\textbf{A},\textbf{Y}} &ChebNet & 81.2 & 69.8 & 74.4 & 70.5 & 76.9 & 92.3\\
	&MLP & 55.1 & 46.5 & 71.4 & 55.3 & 71.1 & 85.5\\
    &LP & 68.0 & 45.3 & 63.0 & - & - & - \\
	&Planetoid & 75.7 & 64.7 & 77.2 & - & - & -\\
	&GCN & 81.5 & 70.3 & 79.0 & *77.6 & 85.2 & 92.4\\
	&SGC & 81.0 & 71.9 & 78.9 & 55.7 & 69.7 & 92.3\\
	&GAT & 83.0 & 72.5 & 79.0 & 71.4 & 86.1 & 92.2\\
	&DisenGCN & *83.7 & *73.4 & 80.5 & 52.7 & *88.7 & *93.3\\
	&GMNN & 83.7 & 73.1 & *81.8 & 68.8 & 85.9 & 92.7\\
\midrule
	\multirow{2}{*}{\textbf{X},\textbf{A}} &DGI & 82.3 & 71.8 & 76.8 & 68.2 & 78.2 & 92.4\\
	& KS2L-GCN (ours) & \textbf{84.6} & \textbf{74.2} & \textbf{83.8}&\textbf{86.8} & \textbf{92.4} & *93.3\\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!tp]
\caption{AUC of link prediction (in ).}
\label{result:link:clf}
\centering
\footnotesize
\begin{tabular}{c c c c c c c c}
\toprule
	\textbf{Input} &\textbf{Methods} & \textbf{Cora} & \textbf{Citeseer} & \textbf{PubMed} & \textbf{Computers} & \textbf{Photo} & \textbf{CS}\\
\midrule
	\multirow{5}{*}{\textbf{X},\textbf{A},\textbf{Y}}
	&GCN & 82.4 & 84.1 & 89.2 & 75.3 & 77.5 & 88.8\\
	&GAT & 81.5 & 85.6 & 79.8 & 78.8 & *86.0 & 90.8\\
	&DisenGCN & 85.6 & 87.5 & 87.8 & *80.9 & 81.8 & 91.2\\
	&GMNN & 85.2 & 85.9 & 89.6 & 70.6 & 70.6 & 91.1\\
\midrule
	\multirow{2}{*}{\textbf{X},\textbf{A}}
	&DGI & \textbf{94.4} & *94.8 & *91.9 & 68.8 & 66.1 & *93.7\\
	& KS2L-GCN (ours) & *93.9 & \textbf{95.4} & \textbf{97.2}& \textbf{85.9} & \textbf{87.4} & \textbf{94.5}\\
\bottomrule
\end{tabular}
\end{table}

\textbf{Node Classification.} The GNN methods compared here use either convolution or attentive neural networks. For a fair comparison, the proposed KS2L is applied to a one-layer SGC with the second-order adjacent matrix. Table~\ref{result:node:clf} shows that the performance of simple GCN learned by the proposed method KS2L consistently and significantly outperforms the other baselines learned by supervised and unsupervised objectives. It's noted that DisenGCN iteratively applies attentive routing operation to dynamically reshape node relationships in each layer. By default, it has 5-layers and iteratively applies 6 times, which typically is a deep GNN. Our proposed model KS2L can empower the single-layer GNN through iterative cross-model knowledge distillation. The results shown in the Table~\ref{result:node:clf} are obtained with  iterations on  \emph{Citeseer} data, but less than  iterations for the other data.

\textbf{Link Prediction.} The goal of this task is to answer the question about whether the learned node representations can keep the node proximity. The performance of each method is measured with AUC value. All of the methods in this experiment have the same model configuration as the node classification task. From the results shown in Table~\ref{result:link:clf}, we can see that KS2L still outperforms the baselines including GNNs learned with supervised and unsupervised objectives.

Linking Figure 1 to the results in Table~\ref{result:node:clf}-\ref{result:link:clf}, we see that classification of nodes in graph  \emph{CS} is indeed an easy task, and most of the GNNs models have similar good performance. However, for link prediction, unsupervised models (DGI and our model) learned better  than those with supervision information, obviously because the supervision information is for node classification, not for link prediction.  On  \emph{Computers} and \emph{Photo} dataset, our KS2L-GCN made significant improvement on the node classification accuracy (around 9.2\% and 3.7\% over the second best, respectively), and good improvement on link prediction as well (5\% and 1.4\% over the second best, respectively). These two datasets with strong locality but low node feature smoothness are in fact not GNN-friendly. Our self-supervised strategy well addresses the limits which GNNs are suffering.  \emph{PubMed} is another typical graph which has high feature smoothness but weak locality. Such discrepancy challenges the GNNs on learning high-quality , especially for link prediction. Our model improves the best supervised models by 7.6\%,  and DGI by 5.3\%.   



\begin{table}[!tp]
\caption{Accuracy (in ) of node classification   after leave-one-out neighbor removal. The symbol  means that the classification accuracy downgrades comparing with the results shown in Table~\ref{result:node:clf}.}
\label{result:node:link:clf}
\centering
\begin{adjustbox}{max width=14cm}
\begin{tabular}{c c c c c c c}
\toprule
	\textbf{Methods} & \textbf{Cora} & \textbf{Citeseer} & \textbf{PubMed} & \textbf{Computers} & \textbf{Photo} & \textbf{CS}\\
\midrule
	GCN & 76.2  6.5\% & 68.7  2.2\% & 76.6  3.0\% & 65.5  15.5\% & 71.2  16.4\% &  92.1  0.3\%\\
\midrule
	GAT & 72.1  13.1\%& 68.8  5.1\% & 75.0  5.1\%& *70.1  1.8\% & 56.5  34.3\% & 91.7  0.5\%\\
\midrule
	DisenGCN & *77.1  8.0\% & 68.6 9.8\% & 75.5 6.5\% & 48.6  7.8\% & 80.6  9.1\% & \textbf{92.8}  0.4\%\\
\midrule
	GMNN & 74.1  11.4\% & 68.4  6.4\% & *80.5  1.5\%& 66.8  21.8\% & *82.6  4.8\%& 92.2  0.5\%\\
\midrule
	DGI & 69.1  16.1\% & *70.8  1.4\% & 68.3  11.1\% & 52.9  22.4\% & 57.1  27.1\% & 91.8  0.6\%\\
\midrule
	KS2L-GCN & \textbf{80.1}  5.5\% & \textbf{71.7}  2.9\% & \textbf{82.7} 1.3\%&\textbf{84.5} 2.6\% & \textbf{89.0}3.7\% & *92.4  0.9\%\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\textbf{Robustness Against Incomplete Graphs.} With the same experimental configuration as link prediction task, we also validate the performance of learned node embeddings from incomplete graph for node classification task. According to the results in Table~\ref{result:node:link:clf}, KS2L still outperforms baseline methods in most cases,  demonstrating the robustness of KS2L performance.
 
 
\begin{table}
\caption{Ablation study of the distillation component influence in node classification accuracy (in ).}
\label{abl:std}
\centering
\footnotesize
\begin{tabular}{c c c c}
\toprule
	\textbf{Distillation} & \textbf{Cora} & \textbf{Citeseer} & \textbf{Pubmed}\\
\midrule
 & 82.4 & 72.8 & 82.3\\
	  & 83.5 & 73.3 & 83.5\\
  & 83.2 & 73.1 & 83.6\\
		  & {\bf 84.6} & {\bf 74.2} & {\bf 83.8}\\
\bottomrule
\end{tabular}
\vspace{-0.4cm}
\end{table}

\textbf{Ablation Study.} We conduct node classification experiments to validate the contribution of each component of the proposed KS2L, which are denoted by ,  and   in Equation (13). From Table~\ref{abl:std}, we can see that the  intra- and cross-model knowledge distillation jointly improve the performance of the proposed self-supervised learning for smoothing GCN. 

\section{Conclusions}
In this work, we propose a self-supervised learning method (KS2L) regularized by graph structure to learn unsupervised node representations for downstream tasks. We conduct thorough experiments on node classification and link prediction tasks to evaluate the unsupervised node representations. Experimental results show that KS2L helps to learn competitive shallow GNN outperforming   the state-of-the-art GNN learned with supervised or unsupervised objectives. GNN has already been applied to diverse applications. In the future, we'd like to explore potential applications of self-supervised learning method for few-shot tasks, and draw lessons from unsupervised network representations to develop more robust and efficient self-supervised learning method.



\section*{Broader Impact}
Our work presents a new design of self-supervised learning GNN model. The model is generally applicable on attributed graphs, such as academic citation graphs, co-author graphs, social graphs, and recommendation bipartite graphs. Since GNN is an active research topic, a broad group of researchers will be benefited from our work, because this is a new model for the first time addressing the discrepancy of node proximity measured by graph topology and node attributes. The datasets used in our experimental evaluation are widely used benchmark graphs, based on which we made a  fair comparison between our proposed model and baseline models. Our implementation code will be available in public for the verification and reproducibility, and the usage by others.

\iffalse
Authors are required to include a statement of the broader impact of their work, including its ethical aspects and future societal consequences.
Authors should discuss both positive and negative outcomes, if any. For instance, authors should discuss
a)  who may benefit from this research,
b) who may be put at disadvantage from this research,
c) what are the consequences of failure of the system, and
d) whether the task/method leverages biases in the data. If authors believe this is not applicable to them, authors can simply state this.
\fi 


\bibliographystyle{plainnat}
\bibliography{gcn-ref-short}

\clearpage
\section*{A. Proof for Theorem 1}
\begin{definition}[Feature Smoothness~\cite{Hou2020Measuring}] We define the feature smoothness  over the normalized space  as follows:

where  denotes the \emph{Manhattan norm}.
\end{definition}
Noted that we use normalized  in \emph{Introduction} section for better showing the influence of feature smoothness and locality on the model performance.
\begin{definition}[Second-order Graph Regularization]
The objective of second-order graph regularization is to  minimize the following equation

where  is the second-order similarity which can be defined as cosine similarity , and  denotes the node representation.
\end{definition}
\begin{proof}
Here we mainly focus on analyzing GNNs whose aggregation operator mainly roots on weighted sum over the neighbors, \emph{i.e.} . Typical examples include but limited to GCN~\cite{kipf2016semi} where  can be the element of normalized adjacent matrix , GAT~\cite{velivckovic2017graph} where  can be the attentive weight for each head. The node representation  can be divided into three parts: the node representations , the sum of common neighbor representations , the sum of non-common neighbor representations . The distance between the representations  and  is:

From Equation~\ref{eq:distance} we can see that the upper bound of similarity of a pair of nodes is mainly influenced by \emph{local feature smoothness} and \emph{structure proximity}. According to the Definition 2 and the proof shown above, if a pair of node  has smoothed local features and similar structure proximity with many common similar neighbors (), the obtained node representation of a GNN will also enforce their node representations to be similar.
\end{proof}

\section*{B. Model Configuration}
The proposed method has hyper-paramters including regularization weight ,  for balancing the contribution of teacher model and local neighbors, \emph{M}.  means that we only train the given GNN without cross-model distillation module. If , we use the last model () as the target model to generate the node representations. Since we use the mean pooling operation, then the  equals to  by default. The GNN we used in this paper can be defined as:

where  and  denotes the activation function. For each hyper-parameter, we use grid search method to valid the best configuration from pre-defined search space, specifically, , , , and .

\begin{table}[htp]
\caption{The best model configuration for reproducing the experimental results of node and link classification tasks.}
\label{stat}
\centering
\begin{tabular}{c c c c c}
\toprule
	\textbf{Data} &  &  & \emph{M} &  for each \\
\midrule
	Cora &  0.5 & 1.0  & 2 & \\
	Citeseer & 0.0 & 1.0 & 5 & \\
	Pubmed & 0.0 & 0.1 & 2 & \\
	Computers & 0.0 & 0.0 & 1 &  \\
	Photo & 0.0 & 0.0 & 1 &  \\
	CS & 0.0 & 0.0 & 0 & \\
\bottomrule
\end{tabular}
\end{table}

\end{document}
