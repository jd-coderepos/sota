\documentclass[a4paper]{article}

\usepackage{INTERSPEECH2021}
\usepackage{multirow}
\usepackage{verbatim}
\usepackage{booktabs}
\usepackage{color}

\newcommand{\PTrial}{}
\newcommand{\Penroll}{}
\newcommand{\NTrial}{\textcolor{red}{}}
\newcommand{\Nenroll}{}

\title{Attention Back-end for Automatic Speaker Verification with \\ Multiple Enrollment Utterances}
\name{Chang Zeng, Xin Wang, Erica Cooper, Junichi Yamagishi}
\address{National Institute of Informatics, Japan SOKENDAI, Japan}
\email{\{zengchang, wangxin, ecooper, jyamagis\}@nii.ac.jp}

\begin{document}

\maketitle
\begin{abstract}
A back-end model is a key element of modern speaker verification systems. Probabilistic linear discriminant analysis (PLDA) has been widely used as a back-end model in speaker verification. However, it cannot fully make use of multiple utterances from enrollment speakers. In this paper, we propose a novel attention-based back-end model, which can be used for both text-independent (TI) and text-dependent (TD) speaker verification with multiple enrollment utterances, and employ scaled-dot self-attention and feed-forward self-attention networks as architectures that learn the intra-relationships of the enrollment utterances.  
In order to verify the proposed attention back-end, 
we combine it with two completely different but dominant speaker encoders, which are time delay neural network (TDNN) and ResNet trained using the additive-margin-based softmax loss and the uniform loss, and compare them with the conventional PLDA or cosine scoring approaches. Experimental results on a multi-genre dataset called CN-Celeb show that the performance of our proposed approach outperforms PLDA scoring with TDNN and cosine scoring with ResNet by around 14.1\% and 7.8\% in relative EER, respectively. Additionally, an ablation experiment is also reported in this paper for examining the impact of some significant hyper-parameters for the proposed back-end model.
\end{abstract}
\noindent\textbf{Index Terms}: speaker verification, TDNN, ResNet, Attention    

\section{Introduction}
Speaker verification is a task which determines whether the input speech is uttered by the claimed speaker or not. 
Until recently, generative models such as GMM-UBM \cite{Reynolds2000-GMMUBM} and i-vector \cite{Dehak2010-IV} with PLDA \cite{Ioffe2006-PLDA} were dominant for this task. With the development of deep neural networks, these models have been replaced by neural networks.
A good example is neural speaker embeddings \cite{Variani2014-DV, Snyder2017-DNNE} extracted from a hidden layer of a neural speaker encoder network, such as x-vectors \cite{Snyder2018-XV, Raj2019-XVProbe}, and it is reported that these have outperformed conventional i-vectors. For the case of  x-vectors, speaker embedding vectors are extracted from the first fully-connected layer after a statistical pooling layer of TDNN \cite{Snyder2017-DNNE}. Then, PLDA is normally adopted as a back-end model to handle the channel mismatch between enrollment speakers and evaluation speech. In addition to TDNN, ResNet \cite{He2016-ResNet, Li2017-DS} is also a popular choice for extracting speaker embedding vectors.  There are also attempts that combine speaker encoders with metric learning loss such as triplet loss \cite{Li2017-DS} or with discriminative training loss of softmax \cite{Liu2017-Angular, Wang2018-AM}. Some of them use 
simple cosine similarity between enrollment speakers and evaluation speech as a back-end model. In either case, a pair of speaker embedding vectors is used, one extracted from an evaluation trial, and one typically averaged or concatenated from the enrollment utterances for the final scoring.  

The objective of this paper is to explore a new way for better handling multiple enrollment utterances and to propose a new back-end model which can use a varying number of embedding vectors corresponding to multiple enrollment utterances directly since, in a real-world setting, enrollment speakers may provide multiple utterances to register their identity. We hypothesize that this is important since the simply averaging of speaker embedding vectors does not fully use intra-relationships of the enrollment utterances, which may contain important information such as expected acoustic variations of the enrolled speakers. Therefore, we propose a new back-end based on scaled-dot self-attention and feed-forward self-attention networks to learn intra-variations of multiple enrollment utterances. 





The multi-head scaled-dot self attention was first proposed in \cite{Vaswani2017-Attention}. In the proposed back-end, multiple speaker embedding vectors are first stacked as a matrix which is used as the input for multi-head self attention, which estimates self-attention probabilities of the input matrix. The self-attention matrix is then multiplied with the speaker embedding matrix in order to get bias parameters that emphasize or de-emphasize not only multiple enrollment utterances but also individual dimensions of their embedding vectors. The second step is multi-head feed-forward self-attention, which was first proposed in \cite{Raffel2015-FFNATT} and was extended to multi-head in \cite{Lin2017-StructureATT}. This additional operation aggregates a matrix modified using the multi-head self-attention described above into one representative vector. Finally, the cosine scoring using the representative vector and one extracted from an evaluation trial is done with a learnable logistic transform for better score calibration. 

For experiments and deep analysis, we have used a new multi-genre database called CN-Celeb \cite{Fan2020-CN1, Li2020-CN2} instead of VoxCeleb \cite{Nagrani2017-VOX1,Chung2018-VOX2} since this database has a varying number of enrollment utterances per speaker and thus is suitable for our model\footnote{VoxCeleb has a single enrollment utterance for all speakers.}. The remainder of the paper is organized as follows: TDNN and ResNet speaker encoders are briefly reviewed in Section 2. Section 3 gives the details of the proposed attention back-end model. The experimental setup and results are outlined in Section 4. Finally, conclusions and future work are presented in Section 5.

\section{Speaker encoders}

Before we explain the proposed attention back-end, we briefly review popular speaker encoders: the TDNN-based speaker encoder used for x-vectors, and ResNet based ones \cite{Li2017-DS, Zeinali2019-BUT, Garcia2020-MagNetO}. We use them as our front-end models in our experiments. 



\vspace{-1mm}
\subsection{Time delay neural network (TDNN)}

TDNN-based neural networks and variants such as \cite{Povey2018-FTDNN} and \cite{Villalba2019-ETDNN} have achieved great success in speaker verification. In our experiment, we choose the original TDNN \cite{Snyder2018-XV} as one of the most standard architectures. Its architecture is shown in Table \ref{tdnn}. The x-vectors are extracted from the output of the first fully-connected (FC) layer.


\setlength{\tabcolsep}{0.9mm}
\begin{table}[t]
\footnotesize
  \caption{TDNN architecture used as a front-end model in our experiment. The normal cross entropy based on speaker labels was used to train this model. }
  \label{tdnn}
  \centering
  \vspace{-2mm}
  \begin{tabular}{l c c c}
    \toprule
    \multicolumn{1}{l}{\textbf{Layer}} & \multicolumn{1}{c}{\textbf{Layer context}} & \multicolumn{1}{c}{\textbf{Total contexts}} & \multicolumn{1}{c}{\textbf{Input  output}}\\
    \midrule
    TDNN1           & [-2,+2]       & 5  & 120    512                          \\
    TDNN2           & \{-2, 0, +2\} & 9  & 1536   512                          \\
    TDNN3           & \{-3, 0, +3\} & 15 & 1536   512                          \\
    TDNN4           & \{0\}         & 15 & 512    512                          \\
    TDNN5           & \{0\}         & 15 & 512    1500                         \\
    Stats.\ Pooling   & [0, T)        &   & 1500  3000                         \\
    FC1             & \{0\}         &   & 3000   512                          \\
    FC2             & \{0\}         &   & 512    512                          \\
    Softmax         & \{0\}         &   & 512                                \\
    \bottomrule
  \end{tabular}
  \vspace{-2mm}
\end{table}


\vspace{-1mm}
\subsection{Residual Convolutional Network (ResNet)}


The residual convolutional network, ResNet for short, was first proposed for image tasks \cite{He2016-ResNet} and was later applied to speaker verification in \cite{Li2017-DS}. We choose this ResNet as one of the advanced architectures for speaker encoders. Its architecture is shown in Table \ref{tab:resnet}. In order to investigate the state-of-the-art performance, we further introduce two recently proposed losses, AM-Softmax loss \cite{Wang2018-AM,Wang2018-LMCL} and uniform loss \cite{Duan2019-Uniform}, and use them for training the ResNet\footnote{Although we used the CN-Celeb database for our main experiment, we have also evaluated our TDNN and ResNet based speaker encoders on the VoxCeleb database. EERs [\%] on the VoxCeleb1 test-set using PLDA with TDNN and simple cosine similarity with ResNet were 3.34\% and 2.89\%, respectively.}. 

\setlength{\tabcolsep}{4mm}
\begin{table}[t]
\footnotesize
  \caption{ResNet architecture used as a front-end model in our experiment. AM-Softmax and uniform losses described in Section 2.2 are used to train this model instead of the normal cross entropy.}
  \label{tab:resnet}
  \centering
  \vspace{-2mm}
  \begin{tabular}{l c c}
    \toprule
    \multicolumn{1}{l}{\textbf{Layer}} & \multicolumn{1}{c}{\textbf{Structure}} & \multicolumn{1}{c}{\textbf{Stride}}\\
    \midrule
    Conv64                    & 5x5, 64       & 2x2             \\
    Residual64                     & [3x3,64] x 2  & 1x1              \\
Conv128                   & 5x5, 128      & 2x2              \\
    Residual128                    & [3x3,128] x 2 & 1x1              \\
Conv256                   & 5x5, 256      & 2x2            \\
    Residual256                    & [3x3,256] x 2 & 1x1             \\
Conv512                   & 5x5, 512      & 2x2              \\
    Residual512                    & [3x3,512] x 2 & 1x1             \\
Adaptive avg.\ pooling          & -         & -             \\
    FC                        & 2048 x 512    & -                \\
    AM-softmax \& Uniform loss          & -        & -                    \\
    \bottomrule
  \end{tabular}
  \vspace{-5mm}
\end{table}





AM-Softmax is a variant of the softmax function that can enlarge the distance between decision boundaries among different classes. On a training set with  speech trials and  classes, a cross-entropy loss based on the AM-Softmax is defined as 

where  is the angle between the speaker embedding of the -th trial and the weight vector of its target class. The hyper-parameter  re-scales the value of , and  controls the decision boundary margin among different classes. 
In our experiment,  and  are fixed to 35 and 0.25, respectively.




The other loss function called Uniform loss \cite{Duan2019-Uniform} is also introduced to train the ResNet speaker encoder, and it is expected to learn \textit{uniformly-distributed class centers on the hyper-sphere manifold} \cite{Duan2019-Uniform}.
The Uniform loss is defined as

where  and  denote the center vectors of classes  and , respectively, and where the distance  is defined as

In this paper, Eq.~(\ref{uni:distance}) is used as a distance function for preventing too large of a repulsion, and it is combined with the AM-softmax loss as the ultimate training loss .


\section{Attention back-end}
\subsection{Attention of multiple enrollment speaker embeddings}

\begin{figure}[t]
  \centering
  \includegraphics[trim=0 70 0 80, clip, width=0.9\linewidth]{back-end-2.pdf}
  \caption{Back-end architecture. Dashed boxes show detailed implementation of the two multi-head attention blocks.}
  \label{fig:back-end}
  \vspace{-5mm}
\end{figure}





Next, we explain the proposed back-end that uses attention mechanisms to merge multiple enrollment speaker embeddings. The detailed architecture of the resulting back-end model is illustrated by Figure \ref{fig:back-end}. 
Note that the speaker encoder (based on either TDNN or ResNet) has been pre-trained in a discriminative manner and is used to transform utterances with various durations into fixed-length embedding vectors.



Suppose a speaker has  enrollment utterances. With a speaker encoder ,  we get  enrollment embedding vectors , where , and  is the waveform data of the -th enrollment utterance.
Since  varies across speakers, we need to aggregate  into a single vector  using two different attention mechanisms in sequence. Specifically, after stacking a matrix , we use multi-head scaled dot attention \cite{Vaswani2017-Attention} to transform  into a hidden matrix . With  attention heads, we get 

where the -th head matrix  is computed by

The query, key, and value matrices are set to , , and , respectively\footnote{To avoid confusion, we use  to denote matrix-matrix or vector-matrix multiplication}, and they all have the shape . The matrices ,  are the trainable parameters of the attention mechanism. 
The multiple heads in Equation~(\ref{multihead}) are expected to attend to information from different representation subspaces at different positions. 


Given  from Equation~(\ref{multihead}), the second multi-head (feed-forward) attention mechanism aggregates it into the single representative embedding vector  by 

The -th head vector  is computed as

where  and  are trainable parameters, and  is sub-matrix of , i.e., .
This is inspired by the multi-head attention-based aggregation proposed in \cite{Lin2017-StructureATT}.

After obtaining the representative vector for an enrollment speaker, the cosine similarity score between an evaluation speaker embedding, which is denoted as , and the representative vector  is calculated. Further, a logistic regression is employed for better calibration as shown below:

where  denotes the probability of  and  belonging to the same speaker and is used for cross-entropy calculation,  is the final score used for the decision making, and  and  are trainable parameters.













\vspace{-1mm}
\subsection{Varying number of enrollment utterances}

Although our back-end model uses two different self-attention mechanisms, it can also handle a varying number of enrollment utterances. As described in \cite{Vaswani2017-Attention}, the shape of the output of multi-head scaled dot self-attention is the same as the input. As for feed-forward self-attention, this mechanism will produce a weight vector whose length is equal to the number of enrollment utterances for weighted average of the matrix . In this way, regardless of the number of enrollment utterances, they will be aggregated into a representative embedding vector for each enrollment speaker. 



\vspace{-2mm}
\section{Experiment}

\noindent 
\textbf{Datasets}
We use a multi-genre dataset called CN-Celeb rather than VoxCeleb to verify our proposed back-end model because each enrollment speaker in this dataset has multiple enrollment utterances, whose number ranges from one to 14. This challenging dataset includes in-the-wild speech utterances of 3,000 speakers in 11 different genres, which leads to complex and composite inter-session variations, both intrinsic (i.e., speaking style, physiological status) and extrinsic (i.e., recording device, background noise). For example, a speaker may be enrolled with read speech, while test conditions may be conversational or singing audio \cite{Li2020-CN2}. The dataset protocol uses 2,800 speakers for training and 200 speakers for evaluation. There are more than 600,000 utterances in the training dataset. Additionally, MUSAN \cite{musan} and RIRs \cite{rirs} datasets are also employed to augment it. Therefore, there are more than 1.2 million utterances in the training dataset, which we obtained by using the cnceleb \cite{cnceleb} example of Kaldi \cite{kaldi}. As for the evaluation dataset, apart from around 800 utterances for enrollment, there are 18,024 utterances spoken by enrollment speakers, forming more than 3.6 million trials.

\noindent 
\textbf{Baselines}
There are two baseline systems in this paper. One is the x-vector speaker encoder with PLDA back-end. It uses 30-dimensional MFCC acoustic features as input. The result is produced by the cnceleb \cite{cnceleb} example in Kaldi. The other is the ResNet speaker encoder with cosine similarity back-end. The input features are 257-dimensional STFT. The ResNet backbone is combined with AM-Softmax as well as uniform loss. After training, speaker embeddings whose size is 512 in our experiment can be extracted from the fully-connected layer, and cosine similarity is calculated as the score for each trial.

\begin{table}[t]
\scriptsize
    \centering
    \caption{Composition of pairs of (test-trial, enrollment-trials) to train back-end model and ground-truth labels from a mini batch. A, B, and C are speaker IDs, and 1, 2, 3, 4 are trial IDs. \PTrial and \Penroll denote test and enrollment trials, respectively. \label{fig:train}}
    \vspace{-2mm}
    \setlength{\tabcolsep}{3pt}
    {
    \begin{tabular}{ccccccccccccccc}
\cmidrule{2-13} 
 & \multicolumn{4}{c}{} & \multicolumn{4}{c}{} & \multicolumn{4}{c}{} \\
 \cmidrule{2-13} 
 & 1 & 2 & 3 & 4 & 1 & 2 & 3 & 4 & 1 & 2 & 3 & 4 & & Label\\
 \cmidrule{2-13}
 \multirow{10}*{\rotatebox[origin=c]{90}{\textbf{Trials to be used for training}}} & \PTrial & \Penroll  & \Penroll & \Penroll & & & & & & & & & & P\\
 
 & \PTrial &  &  &  &  & \Nenroll  & \Nenroll & \Nenroll  & &  & & & & N \\
 
 & \PTrial &  &  &  &  & &  & & & \Nenroll  & \Nenroll & \Nenroll  & & N \\
 
 
 & \Penroll & \PTrial & \Penroll  & \Penroll & & & & & & & & & & P\\
 
 & & \PTrial &  &  & \Nenroll & & \Nenroll  & \Nenroll &  &  & & & & N \\
 
 & & \PTrial &  &  &  &  &  & & \Nenroll & & \Nenroll & \Nenroll & & N \\
 
 






 & & & & &  & & \vdots & & & & &  & & \\

 
 & &  &  &   & & & & &  \Penroll & \Penroll & \Penroll & \PTrial & & P\\

 & \Nenroll & \Nenroll & \Nenroll & &  &  &  &  &  &  & & \PTrial  &  & N \\
 
 & & & &  & \Nenroll  & \Nenroll & \Nenroll &   &  &  &  & \PTrial & & N \\
 \cmidrule{2-13}
\end{tabular}
}
\vspace{-5mm}
\end{table}

\begin{comment}

\begin{table}[t]
\scriptesize
    \centering
    \caption{Composition of pairs of (test-trial, enrollment- trials) to train back-end model. A, B, and C are speaker IDs, and 1, 2, 3, 4 are trial IDs. Rows and columns denote test and enrollment trials, respectively.}
    \vspace{-2mm}
    \setlength{\tabcolsep}{3pt}
    {
    \begin{tabular}{ccccccccccccccc}
 & & \multicolumn{11}{c}{\textbf{Enrollment trial}} \\   
 \cmidrule{2-15}
 & & & \multicolumn{4}{c}{} & \multicolumn{4}{c}{} & \multicolumn{4}{c}{} \\
 \cmidrule{4-15}
 & & & 1 & 2 & 3 & 4 & 1 & 2 & 3 & 4 & 1 & 2 & 3 & 4 \\
 \cmidrule{2-15}
 \multirow{10}*{\rotatebox[origin=c]{90}{\textbf{Test trial}}} & \multirow{4}{*}{} & 1 & \PTrial & \Penroll  & \Penroll & \Penroll & \NTrial & \Nenroll  & \Nenroll & \Nenroll  & \NTrial & \Nenroll  & \Nenroll & \Nenroll \\
 & & 2 & \Penroll & \PTrial  & \Penroll & \Penroll & \Nenroll  & \NTrial & \Nenroll & \Nenroll  & \Nenroll & \NTrial  & \Nenroll & \Nenroll \\
 & & 3 & \Penroll & \Penroll & \PTrial  & \Penroll & \Nenroll  & \Nenroll & \NTrial & \Nenroll  & \Nenroll & \Nenroll & \NTrial  & \Nenroll \\
 & & 4 & \Penroll & \Penroll & \Penroll & \PTrial  & \Nenroll  & \Nenroll & \Nenroll & \NTrial  & \Nenroll & \Nenroll & \Nenroll & \NTrial  \\
 \cmidrule{2-15}
 & \multirow{4}{*}{} & 1 & \NTrial   & \Nenroll & \Nenroll & \Nenroll   & \PTrial  & \Penroll & \Penroll & \Penroll & \NTrial  & \Nenroll  & \Nenroll & \Nenroll \\
 & & 2 & \Nenroll  & \NTrial  & \Nenroll & \Nenroll   & \Penroll & \PTrial  & \Penroll & \Penroll & \Nenroll & \NTrial   & \Nenroll & \Nenroll \\
 & & 3 & \Nenroll  & \Nenroll & \NTrial  & \Nenroll   & \Penroll & \Penroll & \PTrial  & \Penroll & \Nenroll & \Nenroll  & \NTrial  & \Nenroll \\
 & & 4 & \Nenroll  & \Nenroll & \Nenroll & \NTrial    & \Penroll & \Penroll & \Penroll & \PTrial  & \Nenroll & \Nenroll  & \Nenroll & \NTrial  \\
 \cmidrule{2-15}
 & \multirow{4}{*}{} & 1 & \NTrial   & \Nenroll & \Nenroll & \Nenroll  & \NTrial  & \Nenroll  & \Nenroll & \Nenroll & \PTrial  & \Penroll & \Penroll & \Penroll \\
 & & 2 & \Nenroll  & \NTrial  & \Nenroll & \Nenroll  & \Nenroll & \NTrial   & \Nenroll & \Nenroll & \Penroll & \PTrial  & \Penroll & \Penroll \\
 & & 3 & \Nenroll  & \Nenroll & \NTrial  & \Nenroll  & \Nenroll & \Nenroll  & \NTrial  & \Nenroll & \Penroll & \Penroll & \PTrial  & \Penroll \\
 & & 4 & \Nenroll  & \Nenroll & \Nenroll & \NTrial   & \Nenroll & \Nenroll  & \Nenroll & \NTrial  & \Penroll & \Penroll & \Penroll & \PTrial  \\
 \cmidrule{2-15}
 \vspace{-5mm}
\end{tabular}
}
    \label{fig:train}
\end{table}

\begin{figure}
    \centering
    \includegraphics[scale=0.31]{training_methodology.pdf}
    \caption{Training methodology of back-end model}
    \label{fig:train}
\end{figure}
\vspace{-3mm}
\end{comment}



\noindent 
\textbf{Front-end training}
For both TDNN and ResNet speaker encoders, 1,000 utterances are randomly split from the training dataset as a development dataset. The TDNN speaker encoder is trained for 3 epochs using the SGD optimizer with 0.001 learning rate and 0.5 momentum to optimize the parameters. The ResNet speaker encoder is trained for 20 epochs using the SGD optimizer with 0.1 learning rate and 0.9 momentum to optimize the parameters. Both of them used a learning rate decay strategy according to the loss value on the development dataset for better optimization.

\noindent 
\textbf{Back-end training}
A balanced batch strategy is adopted to train the attention-based back-end model. For each mini batch, assuming it has  speakers and  speaker embeddings per speaker, the size of one mini batch is , where  is the size of the embedding. In our experiment,  and  are set as 1024 and 5, respectively. When one mini batch of data is fed into the back-end model, it will be rearranged to form speaker verification trials which have multiple enrollment utterances (or embedding vectors). 

Table~\ref{fig:train} illustrates one example to form the pairs of \textit{(input-trial, enrollment-data)}. In this example, one mini batch of data has 3 speakers , , and , and each speaker has 4 embeddings whose index ranges from 1 to 4. 
There are numerous ways to compose the pairs, but we only consider the following cases. 
For positive pairs where input trial and enrollment data are from the same speaker, one input trial is selected from the speaker's data, and the rest are left for enrollment. 
For negative pairs where the speaker of the input trial is different from that of the enrollment data, we only consider pairs marked by (input-trial=\PTrial, enroll=(\Nenroll, \Nenroll, \Nenroll) of other speakers included in a mini batch. Such a strategy avoids heavy imbalance of positive and negative pairs.


\setlength{\tabcolsep}{1.5mm}
\begin{table}[t]
\footnotesize
  \caption{Performance of two encoders with different back-ends}
  \label{tab:result1}
  \vspace{-2mm}
  \centering
  \begin{tabular}{l c c c c}
    \toprule
    \multicolumn{1}{c}{\textbf{Spk encoder}} & \multicolumn{1}{c}{\textbf{Back-end}} & \multicolumn{1}{c}{\textbf{Enroll process.}} & \multicolumn{1}{c}{\textbf{EER(\%)}} & \multicolumn{1}{c}{\textbf{minDCF}}\\
    \midrule
    TDNN   & Cosine     & Concat    & 20.82    & 0.8114                     \\
    TDNN   & Cosine     & Mean      & 19.86    & 0.8350                     \\
    TDNN \cite{Li2020-CN2} & PLDA & Concat & 12.52   & -                    \\
    TDNN   & PLDA     & Concat    & 12.09           & 0.6105                \\
    TDNN   & PLDA     & Mean      & 13.29           & 0.6522                \\
    TDNN   & Proposed & Attention & \textbf{10.39}  & \textbf{0.5758}       \\ \midrule
    ResNet & Cosine   & Concat    & 12.46           & 0.5484                \\
    ResNet & Cosine   & Mean      & 11.86           & 0.5045                \\
    ResNet & PLDA     & Concat    & 15.66           & 0.5890                \\
    ResNet & PLDA     & Mean      & 14.56           & 0.5692                \\
    ResNet & Proposed & Attention & \textbf{10.94}  & \textbf{0.5006}       \\
    \bottomrule
  \end{tabular}
   \vspace{-2mm}
\end{table}


\begin{figure}
    \centering
    \caption{DET curves for combinations of TDNN and ResNet speaker encoders and several different back-end models.} \vspace{-1mm}
    \label{fig:det_tdnn}
    \includegraphics[width=1\linewidth]{merged_det.pdf}
    \vspace{-7mm}
\end{figure}


\setlength{\tabcolsep}{1.5mm}
\begin{table}[t]
\footnotesize
  \caption{Breakdown EER(\%) results according to number of enrollment utterances of individual speakers included in the eval set. For enrollment processing, concatenation and mean operation were used for TDNN and ResNet cases, respectively.}
  \label{tab:result1-breakdown}
  \vspace{-2mm}
  \centering
  \begin{tabular}{l c c c c c}
    \toprule
    \textbf{Systems} & \textbf{} & \textbf{} & \textbf{} & \textbf{} & \textbf{}\\
    \midrule
    TDNN/PLDA     & 14.00    & 10.55   & 9.51  &    11.87 & 13.40         \\
    Proposed      & 12.75    & 8.88    & 9.41  &    9.14  & 10.82        \\ \midrule
    ResNet/Cosine & 14.00    & 11.55   & 11.43 &    11.63 & 11.39        \\
    Proposed      & 12.09    & 11.13   & 10.25 &    10.86 & 10.76        \\
    \bottomrule
  \end{tabular}
\end{table}

\vspace{-1.5mm}
\subsection{Results}
Table \ref{tab:result1} shows the results of TDNN and ResNet with different back-end models. The third line of this table is the best result of others' work that we know so far on this dataset. It is clear that our attention back-end model is much better than PLDA for the TDNN encoder, and than cosine similarity for the ResNet encoder, in terms of EER. Concretely, the attention-based back-end respectively outperforms PLDA and cosine similarity 14.1\% and 7.8\% relatively. As for the minDCF score, our attention back-end model is slightly better. DET curves for these two speaker encoders are shown in Figure \ref{fig:det_tdnn}.

For further exploration of our back-end model for varying number of enrollment utterances, we conducted a experiment, whose result is shown in Table \ref{tab:result1-breakdown}, to examine our back-end model according to number of enrollment utterances of individual speakers. The result reveals a fact that enrollment with multiple utterances is much better than single one in the all situations and our back-end model not only performs better for multiple enrollment utterances but is also competitive in the occasion of single enrollment utterance.




\setlength{\tabcolsep}{1.2mm}
\begin{table}[t]
\footnotesize
  \caption{Ablation experiment results}
  \label{tab:ablation}
  \centering
  \vspace{-2mm}
  \begin{tabular}{c c c c c}
    \toprule
    \multicolumn{1}{c}{\textbf{\#Speakers}} & \multicolumn{1}{c}{\textbf{\#Utterances}} & \multicolumn{1}{c}{\textbf{\#Heads}} & \multicolumn{1}{c}{\textbf{EER(\%)}} & \multicolumn{1}{c}{\textbf{minDCF}}\\
    \midrule
    128          & 5          & 8          & 10.84          & 0.6376            \\
    \textit{256} & \textit{5} & \textit{8} & \textit{10.57} & \textit{0.6019}   \\
    512          & 5          & 8          & 10.60          & 0.5974            \\
    1024         & 5          & 8          & 10.62          & \textbf{0.5792}   \\
    \midrule
    256          & 3          & 8          & 11.55          & 0.6426            \\
    256          & 4          & 8          & 10.86          & 0.6308            \\
    256          & 6          & 8          & 10.67          & 0.6039            \\
    256          & 10         & 8          & 10.68          & 0.6041            \\
    \midrule
    256          & 5          & 1          & 10.36          & 0.5949            \\
    256          & 5          & 2          & \textbf{10.28} & 0.5896            \\
    256          & 5          & 4          & 10.44          & 0.5980            \\
    \bottomrule
  \end{tabular}
   \vspace{-5mm}
\end{table}

\vspace{-1.5mm}
\subsection{Ablation experiments}


In this section, we conducted three groups of ablation experiments to show the impact of some important hyper-parameters for our proposed back-end model. The results of these experiments are shown in Table \ref{tab:ablation}, where each group is divided by a solid line. The second line of this table can be used as a reference result for all ablation experiments. 

For the first group, we increased the number of speakers in one batch from 128 to 1024 and kept the other hyper-parameters constant. We found that the performance in terms of EER improved as speakers increased to 256, and then appears to have converged even if the number of speakers increases further. In contrast, the performance in terms of minDCF continually improved with the increase of number of speakers in one batch.

Additionally, we varied the number of utterances per speaker, which ranges from 3 to 10 in one batch, in the second group. The performance of EER and minDCF both show a rapid improvement until the number of utterances equals 5, and then the results become a little worse, but finally they converge with the further increase of number of utterances.

Moreover, the number of heads in the multi-head attention mechanism was also examined in the third group. Unlike the conclusion in \cite{Zhu2018-SelfAtt}, we found that more heads was not better in our back-end model. The performance of EER and minDCF both peaks when the number of heads is equal to 2, and then begins to decay with the increase of the number of heads. 

\section{Conclusions}
In this paper, we have proposed a new attention back-end model for speaker verification using a varying number of multiple enrollment utterances. 
On the CN-Celeb dataset, we have achieved 14.1\% improvement compared to the PLDA back-end for the TDNN speaker encoder and 7.8\% improvement compared to cosine similarity for the ResNet speaker encoder. Our future work includes the evaluation of the proposed attention back-end model on TD-ASV corpora such as RedDots \cite{RedDots}.




\noindent
\textbf{Acknowledgements}
\footnotesize{This study is supported by JST CREST Grants (JPMJCR18A6, JPMJCR20D3), MEXT KAKENHI Grants (16H06302, 18H04120, 18H04112, 18KT0051), Japan, and Google AI for Japan program. }


\bibliographystyle{IEEEtran}

\bibliography{mybib}



\end{document}
