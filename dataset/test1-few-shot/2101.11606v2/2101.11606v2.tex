\documentclass[final]{cvpr}
\usepackage{times}
\usepackage{booktabs}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage[utf8x]{inputenc} 
\usepackage{csquotes}
\usepackage[table,xcdraw]{xcolor}
\usepackage{nicefrac}       



\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false,citecolor=blue,linkcolor=blue,urlcolor=blue]{hyperref}

\def\confYear{CVPR 2021}
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\newcommand{\clswgan}{\texttt{f-CLSWGAN}}
\newcommand{\vaegan}{\texttt{f-VAEGAN}}
\def\proposed{\texttt{CLF-Net}{}}
\newcommand{\highlight}[1]{\textcolor{blue}{#1}}

\begin{document}

\title{\vspace{-0.7em}Generative Multi-Label Zero-Shot Learning}

\author{Akshita Gupta* \quad Sanath Narayan* \quad Salman Khan \quad 
Fahad Shahbaz Khan \\
\quad Ling Shao  \quad Joost van de Weijer \\
Inception Institute of Artificial Intelligence, UAE \quad Mohamed Bin Zayed University of AI, UAE \\
 Universitat Aut√≤noma de Barcelona
}

\maketitle
\begin{abstract}\vspace{-1.0em}
 Multi-label zero-shot learning strives to classify images into multiple unseen categories for which no data is available during training. The test samples can additionally contain seen categories in the generalized variant. Existing approaches rely on learning either shared or label-specific attention from the seen classes. Nevertheless, computing reliable attention maps for unseen classes during inference in a multi-label setting is still a challenge. In contrast, state-of-the-art single-label generative adversarial network (GAN) based approaches learn to directly synthesize the class-specific visual features from the corresponding class attribute embeddings. However, synthesizing multi-label features from GANs is still unexplored in the context of zero-shot setting.
In this work, we introduce different fusion approaches at the attribute-level, feature-level and cross-level (across attribute and feature-levels) for synthesizing multi-label features from their corresponding multi-label class embeddings.
To the best of our knowledge, our work is the first to tackle the problem of multi-label feature synthesis in the (generalized) zero-shot setting. Comprehensive experiments are performed on three zero-shot image classification benchmarks: NUS-WIDE, Open Images and MS COCO. Our cross-level fusion-based generative approach outperforms the state-of-the-art on all three datasets. Furthermore, we show the generalization capabilities of our fusion approach in the zero-shot detection task on MS COCO, achieving favorable performance against existing methods.
The source code is available at 
\url{https://github.com/akshitac8/Generative_MLZSL}.
\end{abstract}
\vspace{-0.4cm}

\section{Introduction}

Multi-label classification is a challenging problem where the task is to recognize all labels in an image.
Typical examples of multi-label classification include, MS COCO~\cite{coco} and NUS-WIDE~\cite{nuswide} datasets, where an image may contain several different categories (labels). Most recent multi-label classification approaches address the problem by utilizing attention mechanisms~\cite{wang2017multi,yeattention,you2020cross}, recurrent neural networks~\cite{wang2016cnn,yazici2020orderless,nam2017maximizing}, graph CNNs~\cite{kipf2016semi,chen2019multi} and label correlations~\cite{weston2011wsabie,durand2019learning}. However, these approaches do not tackle the problem of multi-label zero-shot classification, where the task is to classify images into multiple new ``unseen'' categories at test time, without being given any corresponding visual example during the training. 
Different from zero-shot learning (ZSL), the test samples can belong to the seen or unseen classes in generalized zero-shot learning (GZSL). Here, we tackle the challenging problem of large-scale multi-label ZSL and GZSL. 

Existing multi-label (G)ZSL approaches address the problem by utilizing global image representations~\cite{mensink2014costa,zhang2016fast}, structured knowledge graphs~\cite{lee2018multi} and attention-based mechanisms~\cite{huynh2020shared}. In contrast to the multi-label setting, single-label (generalized) zero-shot learning, where an image contains at most one category label, has received significant attention~\cite{jayaraman14nips,fu15pami,frome13nips,romera15icml,rohrbach13nips,Ye17cvpr,akata2015label,zsl-good-bad-ugly,xian2018feature,xian2019f}. State-of-the-art single-label (G)ZSL approaches~\cite{xian2018feature,Rafael18eccv,li19leveraging,huang19generative,Mandal19cvpr,xian2019f,narayan2020latent} are generative in nature. These approaches exploit the power of generative models, such as generative adversarial networks (GANs)~\cite{gan} and variational autoencoder (VAE)~\cite{kingma13iclr} to synthesize unseen class features. Typically, a feature synthesizing generator is utilized to construct single-label features. The generative approaches currently dominate single-label ZSL due to their ability to synthesize unseen class (fake) features by learning the underlying data distribution of seen classes (real) features. Nevertheless, the generator only synthesizes single-label features in the existing ZSL frameworks. To the best of our knowledge, the problem of designing a feature synthesizing generator for \textit{multi-label} ZSL paradigm is yet to be explored.

In this work, we address the problem of multi-label (generalized) zero-shot learning by introducing an approach based on the generative paradigm. 
When designing a generative multi-label zero-shot approach, the main objective is to synthesize semantically consistent \textit{multi-label} visual features from their corresponding class attributes (embeddings). Multi-label visual features can be synthesized in two ways. (i) One approach is to integrate class-specific attribute embeddings at the input of the generator to produce a global image-level embedding vector. We call this approach attribute-level fusion (ALF). Here, the image-level embedding represents the holistic distribution of the positive labels in the image (see Fig.~\ref{fig:intro}). Since the generator in ALF performs global image-level feature generation, it is able to better capture label dependencies (correlations among the labels) in an image. However, such a feature generation has lower class-specific discriminability since the discriminative information with respect to the individual classes is not explicitly encoded.  (ii) A second approach is to synthesize the features from the class-specific embeddings individually and then integrate them in the visual feature space. We call this approach feature-level fusion (FLF), as shown in Fig.~\ref{fig:intro}. Although FLF better preserves the class-specific discriminative information in the synthesized features, it does not explicitly encode the label dependencies in an image due to synthesizing features independently of each other. This motivates us to investigate an alternative fusion approach for
multi-label feature synthesis.


\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{images/intro.png}
\caption{A conceptual illustration of our three fusion approaches: attribute-level (ALF), feature-level (FLF) and cross-level feature (CLF), on an example image with three classes. Given an image, a feature extractor   
extracts real visual features (green diamond) in .
ALF integrates the individual class-specific embeddings to generate a global image-level embedding (red diamond) in  . The image-level embedding is then used to synthesize the multi-label features (blue diamond). Different from ALF, the FLF synthesizes class-specific features and integrates them in the feature space . These integrated features are shown as the yellow diamond. Our CLF takes as input synthesized features from ALF and FLF and enriches each respective feature by taking guidance from the other. The resulting enriched features are integrated to obtain the final image-level feature representation (purple diamond). The final image representation combines the advantage of label dependency of ALF and class-specific discriminability of FLF. \vspace{-0.3cm}
}
\label{fig:intro}
\end{figure}


In this work, we introduce an alternative fusion approach that combines the advantage of label dependency of ALF and the class-specific discriminability of FLF, during multi-label feature synthesis. We call this approach as cross-level feature fusion (CLF), as in Fig.~\ref{fig:intro}. The CLF approach utilizes each individual-level feature and attends to the bi-level context (from ALF and FLF). As a result, individual-level features adapt themselves to produce enriched synthesized features, which are then pooled to obtain the CLF output. In  addition  to  multi-label  zero-shot  classification,  we investigate the proposed multi-label feature generation CLF approach for (generalized) zero-shot object detection.


\noindent \textbf{Contributions:} We propose a generative approach for multi-label (generalized) zero-shot learning. To the best of our knowledge, we are the first to explore the problem of multi-label feature synthesis in the zero-shot setting. 
We investigate three different fusion approaches (ALF, FLF and CLF) to synthesize multi-label features. Our CLF approach combines the  advantage  of  label  dependency of  ALF  and  the  class-specific discriminability of FLF. Further, we integrate our fusion approaches into two representative generative architectures: \clswgan{}~\cite{xian2018feature} and \vaegan{}~\cite{xian2019f}. 


We evaluate our (generalized) zero-shot classification approach on three datasets: NUS-WIDE~\cite{nuswide}, Open Images~\cite{openimages} and MS COCO~\cite{coco}. Our CLF approach achieves consistent improvement in performance over both ALF and FLF. Furthermore, CLF outperforms state-of-the-art methods on \textit{all} datasets. On the large-scale Open Images dataset, our CLF achieves absolute gains of  and  over the state-of-the-art in terms of GZSL F1 score at \textit{top}- predictions, , respectively.
In addition to classification, we evaluate CLF for (generalized) zero-shot object detection, achieving favorable results against existing methods on MS COCO.

\section{\hspace{-0.09em}Generative Single-Label Zero-Shot Learning\label{sec:slzsl}}
As discussed earlier, state-of-the-art single-label zero-shot approaches~\cite{xian2018feature,Rafael18eccv,li19leveraging,huang19generative,Mandal19cvpr,xian2019f} are generative in nature, utilizing the power of generative models (\eg, GAN \cite{gan}, VAE~\cite{kingma13iclr}) to synthesize unseen class features. 
Here, each image is assumed to have a \textit{single} object category label (\eg, CUB~\cite{cub}, FLO~\cite{flo} and AWA~\cite{zsl-good-bad-ugly}).

\noindent\textbf{Problem Formulation:}
Let  denote the encoded feature instances of images and  the corresponding class labels from the set of  seen class labels . Let  denote the set of  unseen classes, which is disjoint from the seen classes . Here, the total number of seen and unseen classes is denoted by . The relationships among all the seen and unseen classes are described by the category-specific semantic embeddings , . 
To learn the ZSL and GZSL classifiers, existing single-label GAN-based approaches~\cite{xian2018feature,li19leveraging,huang19generative,xian2019f} first learn a generator using the seen class features  and corresponding class embeddings . Then, the learned generator and the unseen class embeddings  are used to synthesize the unseen class features . 
The resulting synthesized features , along with the real seen class features , are further deployed to train the final classifiers  and . Next, we briefly describe the feature synthesis stage employed in existing single-label GAN-based zero-shot frameworks, to generate unseen class features. 


Typically, GAN-based zero-shot classification frameworks utilize a feature synthesizing generator  and a discriminator . Both  and  compete against each other in a two player minimax game. While  attempts to accurately distinguish real image features  from generated features ,  attempts to fool  by generating features that are semantically close to real features. Since class-specific features are to be synthesized, a conditional Wasserstein GAN~\cite{wgan} is employed due to its more stable training, by conditioning both  and  on the embeddings . Here,  learns to synthesize class-specific features  from the corresponding single-label embeddings , given by . Nevertheless, the generator only synthesizes single-label features in existing zero-shot learning frameworks. To the best of our knowledge, the problem of designing a feature synthesizing generator for the multi-label zero-shot learning paradigm is yet to be investigated.

\section{Generative Multi-Label Zero-Shot Learning\label{sec:method}}
As discussed earlier, most real-world tasks involve multi-label recognition, where an image can contain multiple and wide range of category labels (\eg, MS COCO~\cite{coco}, NUS-WIDE~\cite{nuswide} and Open Images~\cite{openimages}). The multi-label classification problem becomes more challenging in the zero-shot learning setting, where the test set either contains only unseen classes (ZSL) or both seen and unseen classes (GZSL). In this work, we propose a generative multi-label zero-shot learning approach that exploits the capabilities of generative models to learn the underlying data distribution of seen classes. This helps to mimic the fully-supervised setting by synthesizing (fake) features for unseen classes. While generative approaches have been extensively studied for single-label zero-shot learning, we are the first to address the problem of multi-label feature synthesis in the multi-label (generalized) zero-shot setting. Next, we describe the problem formulation of generative multi-label (generalized) zero-shot learning. 

\noindent\textbf{Problem Formulation:} 
In contrast to single-label zero-shot learning, here,  denotes the encoded feature instances of multi-label images and  the corresponding multi-hot labels from the set of  seen class labels . Let  denote the multi-label feature instance of an image and , its multi-hot label with  positive classes in the image. Then, the set of attribute embeddings for the image can be denoted as , where . Here, we use GloVe~\cite{pennington2014glove} vectors of the class names as the attribute embeddings , as in~\cite{huynh2020shared}.
Now, the generator's task is to learn to synthesize the multi-label features  from the set of associated embedding vectors .
Post-training of , multi-label features corresponding to the unseen classes are synthesized. The resulting synthesized features along with the real seen class features are deployed to train the final ZSL/GZSL classifiers  and . Next, we investigate three different approaches to synthesize unseen multi-label features. 




\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{images/framework.png}
    \caption{Overview of three different approaches to synthesize unseen multi-label features. The attribute-level fusion (ALF) generates a global image-level embedding vector from a set of class-specific embedding vectors corresponding to multiple labels in an image (Sec.~\ref{sec:att_fuse}). In ALF, the generator synthesizes global features  that capture the correlations among the labels in the image. On the other hand, the feature-level fusion (FLF) synthesizes features from individual class-specific embeddings (Sec.~\ref{sec:feat_fuse}). As a result, the generator produces class-specific latent features which are then integrated to obtain synthesized features . The cross-level fusion (CLF) combines the advantages of ALF and FLF, during feature generation (Sec.~\ref{sec:cross_fuse}). Specifically, it uses each individual-level feature () to attend to the bi-level context and adapt itself to generate . These enriched features are then pooled to obtain the CLF output, which represents our final synthesized feature .\vspace{-0.2cm} }
    \label{fig:caf_fuse}
\end{figure*}







\subsection{Generative Multi-Label Feature Synthesis}

To synthesize multi-label features, we introduce three fusion approaches: attribute-level fusion (ALF), feature-level fusion (FLF) and cross-level feature fusion (CLF). 

\subsubsection{Attribute-Level Fusion\label{sec:att_fuse}}

In attribute-level fusion (ALF) approach, a global image-level embedding vector is obtained from a set of class-specific embedding vectors that correspond to multiple labels in the image. The image-level embedding represents the global description of the positive labels in an image.  The global embedding 
 is obtained by averaging the individual class embeddings . This embedding  is then input to the generator  along with the noise  for synthesizing the feature . The attribute-level fusion is then given by


Fig.~\ref{fig:caf_fuse} shows the feature generation of  from the global embedding .~The generator  in ALF performs global image-level feature generation, thereby capturing label dependencies (correlations among labels) in an image. However, such a feature generation from the global embedding  has lower class-specific discriminability since it does not explicitly encode discriminative information with respect to individual classes. 

\subsubsection{Feature-Level Fusion}\label{sec:feat_fuse}
Here, we introduce a feature-level fusion (FLF) approach,
which synthesizes the features from the class-specific embeddings individually.
This allows the FLF approach to better preserve the class-specific discriminative information in the synthesized features.
Different from the ALF that integrates the embedding vectors, the FLF approach first inputs the  class-specific embeddings  to  in order to generate  class-specific latent features . These features are then integrated through an average operation to obtain the final synthesized feature . The feature-level fusion (FLF) is denoted by

Fig.~\ref{fig:caf_fuse} shows the feature generation of  from the individual embeddings . We observe from Eq.~\ref{eq:feat_fuse} that for a fixed noise , the generator  synthesizes a fixed latent feature  for class , regardless of the presence/absence of other classes in an image.
Thus, while the generated latent features  better preserve class-specific discriminative information of the positive classes  present in the image,   synthesizes them independently of each other.
As a result, the synthesized feature  does not explicitly encode the label dependencies in an image. 

As discussed above, both the aforementioned fusion approaches (ALF and FLF) have shortcomings when synthesizing multi-label features. Next, we introduce a fusion approach that combines the advantage of the label dependency of ALF and class-specific discriminability of FLF. 

\subsubsection{Cross-Level Feature Fusion}
\label{sec:cross_fuse}
The proposed cross-level feature fusion (CLF) aims to combine the advantages of both ALF and FLF. The CLF approach (see Fig.~\ref{fig:caf_fuse}) incorporates label dependency and class-specific discriminability in the feature generation stage as in the ALF and FLF, respectively. To this end,  and  are forwarded to a feature fusion block. Inspired by the multi-headed self-attention \cite{vaswani2017attention}, the feature fusion block enriches each respective feature by taking guidance from the other branch. Specifically, we create a matrix  by stacking the individual features  and . Then,  these features are linearly projected to a low-dimensional space () to create query-key-value triplets using a total of  projection heads,

where  and  . For each feature, a status of its current form is kept in the `\emph{value}' embedding, while the \emph{query} vector derived from each input feature is used to find its correlation with the \emph{keys} obtained from both the features, as we elaborate below.

Given these triplets from each head, the features undergo two levels of processing (i) intra-head processing on the triplet and (ii) cross-head processing. For the first case, the following equation is used to relate each query vector with `\emph{keys}' derived from both the features.  The resulting normalized relation scores () are used to reweight the corresponding value vectors, thereby obtaining the attended features , 

To aggregate information across all heads, these attended low-dimensional features from each head are concatenated and processed by an output layer to generate the original -dimensional output vectors , 

where  is a learnable weight matrix and  denotes concatenation. 

After obtaining the self-attended features , a residual branch is added from the input to the attended features  and further processed with a small residual sub-network  to help the network first focus on the local neighbourhood and then progressively pay attention to the other-level features, 

This encourages the network to selectively focus on adding complimentary information to the source vectors . 
Finally, we mean-pool the matrix  along the row dimension to obtain a single cross-level fused feature , 


The cross-level fused feature  is obtained by effectively fusing the features generated from ALF and FLF. As a result,  explicitly encodes the correlation among labels in the image in addition to the class-specific discriminative information of the positive classes present in the image. Next, we describe the integration of our CLF approach in two representative generative architectures for multi-label (generalized) zero-shot classification. 


\subsection{Multi-Label Zero-Shot Classification\label{sec:feat_syn}}
We integrate our fusion approaches in two representative generative architectures (\clswgan{}~\cite{xian2018feature} and \vaegan{}~\cite{xian2019f}) for  multi-label zero-shot classification. Both \clswgan{} and \vaegan{} have been shown to achieve promising performance for single-label zero-shot classification. Since CLF integrates both ALF and FLF, we only describe here the integration of CLF in the two classification frameworks. Next, we describe the integration of CLF in \clswgan{}, followed by \vaegan{}.

Briefly, \clswgan{} comprises a conditional WGAN (conditioned on the embeddings ) and a seen class classifier . Here, we replace the standard generator of the \clswgan{} with our multi-label feature generation (CLF) to synthesize multi-label features . The resulting multi-label WGAN loss is given by

where  is synthesized using Eq.~\ref{eq:clf_out} for the seen classes,
 is the penalty coefficient and  is a convex combination of  and . Furthermore, a classifier , trained on the seen classes, is employed to encourage the generator to synthesize features that are well suited for final ZSL/GZSL classification. The final objective for training our CLF-based \clswgan{} in a multi-label setting is given by

where  denotes the standard binary cross entropy loss between the predicted multi-label  and the ground-truth multi-label  of feature .

In addition to \clswgan{}, we also integrate our CLF into \vaegan{}~\cite{xian2019f} framework to perform multi-label feature synthesis. The \vaegan{} extends \clswgan{} by combining a conditional VAE~\cite{kingma13iclr} along with a conditional WGAN, utilizing a shared generator between them. For more details on \vaegan{}, we refer to~\cite{xian2019f}. Similar to our CLF-based \clswgan{} described earlier, we replace the single-label shared generator in \vaegan{} with our multi-label generator (CLF) for synthesizing multi-label . The resulting CLF-based \vaegan{} is trained similar to the standard \vaegan{}, using the original loss formulation~\cite{xian2019f}. Fig.~\ref{fig:overall_arch_vaegan} shows our CLF-based \vaegan{} architecture for multi-label (generalized) zero-shot classification. 


\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{images/clf_vaegan.png}
    \caption{Our CLF-based \vaegan{} architecture, which integrates the proposed multi-label feature synthesis CLF approach into the \vaegan{}. The standard \vaegan{} extends \clswgan{} by integrating a conditional VAE along with a conditional WGAN, utilizing a shared generator between them. \vaegan{} comprises an encoder , shared generator and a discriminator , conditioned on the class-specific embeddings . In the proposed CLF-based \vaegan{} shown here, we replace the single-label shared generator in \vaegan{} with our multi-label generator  for synthesizing multi-label . The resulting synthesized features  are then passed to the discriminator for training the generator using the loss terms: ,  and .\vspace{-0.3cm}}
    \label{fig:overall_arch_vaegan}
\end{figure}

\section{Experiments}

\subsection{Experimental Setup\label{sec:exp_setup}}
\noindent\textbf{Datasets:} We evaluate our generative multi-label zero-shot approach on three benchmarks: NUS-WIDE~\cite{nuswide}, Open Images~\cite{openimages} and MS COCO~\cite{coco}.
The \textbf{NUS-WIDE} dataset comprises nearly K images with  human-annotated categories, in addition to the  labels obtained from Flickr user tags. As in~\cite{huynh2020shared,zhang2016fast}, the  and  labels are used as seen and unseen classes, respectively. 
The \textbf{Open Images} (v4) is a large-scale dataset consisting of  million training images along with  and  validation and testing images, respectively. The scale of Open Images is larger than other multi-label
datasets, such as NUS-WIDE and MS COCO. This dataset is partially annotated with human labels and machine-generated labels. Here,  labels, having at least  images in the training, are selected as seen classes. The most frequent  test labels, which are not present in the training data are selected as unseen classes, as in \cite{huynh2020shared}. 
The \textbf{MS COCO} dataset is divided into training and validation sets with  and  images, respectively. It has been previously used for multi-label zero-shot object detection~\cite{bansal2018zero,hayat2020synthesizing} but not for multi-label zero-shot classification. Here, we perform multi-label zero-shot classification experiments by using the same split ( seen and  unseen classes), as in detection works~\cite{bansal2018zero,hayat2020synthesizing}.

\noindent\textbf{Evaluation Metrics:}
For evaluating our approach on multi-label (generalized) zero-shot classification, we use the mean Average Precision (mAP) and F1 score at \textit{top}- predictions, similar to~\cite{huynh2020shared,veit2017learning,zhang2016fast}. While the mAP measure captures the image ranking accuracy of the model for each label, the F1 measure captures the model's ability to correctly rank the labels in each image. 

\noindent\textbf{Implementation Details:}
Following existing zero-shot classification works~\cite{zhang2016fast,huynh2020shared}, we use the pretrained VGG-19 backbone to extract features from multi-label images.

Image-level features of size , corresponding to FC layer output are used as input to our GAN. We use the  normalized  dimensional GloVe~\cite{pennington2014glove} vectors corresponding to the category names as the attribute embeddings , as in~\cite{huynh2020shared}. The encoder , discriminator  and generators  and  are all two layer fully connected (FC) networks with  hidden units and have Leaky ReLU as the non-linearity. The number of heads  in our CLF is set to . The sub-network  in CLF is a two layer FC network with  hidden units. 
The feature synthesizing network is trained with a learning rate of . The WGAN is trained with (\textit{batch size}, \textit{epoch}) of ,  and  on NUS-WIDE, Open Images and MS COCO, respectively. For the \clswgan{} variant,  is set to , while the coefficient  is set to  in the \vaegan{} variant. The ZSL and GZSL classifiers:  and  are trained for  epochs with (\textit{batch size}, \textit{learning rate}) of ,  and  on NUS-WIDE, Open Images and MS COCO, respectively. The WGAN and the classifiers are trained using ADAM  with () as (, ). All parameters are chosen via cross-validation.

\begin{table}
\centering
\caption{Classification performance comparison of the three fusion approaches (ALF, FLF and CLF) for both ZSL and GZSL tasks on the NUS-WIDE dataset. The comparison is shown in terms of F1 score () and mAP. We present the evaluation of our fusion approaches with both architectures: \clswgan{} and \vaegan{}. Regardless of the underlying generative architecture, our CLF approach achieves consistent improvement in performance, on all metrics, over both ALF and FLF for ZSL and GZSL tasks. Best results are in bold.\vspace{0.2em}}
\adjustbox{width=1\columnwidth}{
\begin{tabular}{cccccc} 
\toprule[0.15em]
\textbf{Fusion} & \textbf{GAN} & \textbf{Task} & \begin{tabular}[c]{@{}c@{}}\cellcolor[HTML]{EEEEEE} \textbf{~F1 }\textbf{(K=3)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\cellcolor[HTML]{DAE8FC} ~\textbf{F1}~\textbf{(K=5)}\end{tabular} & \textbf{mAP} \\ 
\toprule[0.15em]
\multirow{4}{*}{ALF} & \multirow{2}{*}{\texttt{f-CLSWGAN}} & ZSL & 29.8~ & 26.7 & 22.1 \\
 &  & GZSL & 16.8 & 19.7 & 7.1 \\
 & \multirow{2}{*}{\texttt{f-VAEGAN}} & ZSL & 30.9 & 27.9 & 22.9 \\
 &  & GZSL & 17.7 & 20.6 & 7.5 \\ 
\cmidrule(r){2-6}
\multirow{4}{*}{FLF} & \multirow{2}{*}{\texttt{f-CLSWGAN}} & ZSL & 29.8 & 26.8 & 22.6 \\
 &  & GZSL & 17.0 & 19.6 & 7.2 \\
 & \multirow{2}{*}{\texttt{f-VAEGAN}} & ZSL & 29.9 & 27.0 & 23.3 \\
 &  & GZSL & 17.2 & 19.9 & 7.6 \\ 
 \cmidrule(r){2-6}
\multirow{4}{*}{CLF} & \multirow{2}{*}{\texttt{f-CLSWGAN}} & ZSL & 31.1 & 27.6 & 23.7 \\
 &  & GZSL & 17.9 & 20.7 & 8.0 \\
 & \multirow{2}{*}{\texttt{f-VAEGAN}} & ZSL & \textbf{32.8} & \textbf{29.3} & \textbf{25.7} \\
 &  & GZSL & \textbf{18.9} & \textbf{22.0} & \textbf{8.9} \\
\bottomrule[0.1em]
\end{tabular}
}
\vspace{-0.2cm}
\label{tab:ablation}
\end{table}



\begin{table*}[t]
\centering
\caption{State-of-the-art comparison for ZSL and GZSL tasks on the NUS-WIDE and Open Images datasets. We report the results in terms of mAP and F1 score at  for NUS-WIDE and  for Open Images. Our approach outperforms the state-of-the-art for both ZSL and GZSL tasks, in terms of mAP and F1 score, on both datasets. Best results are in bold. }
\adjustbox{width=\linewidth}{
\begin{tabular}{ccccccccc|ccccccc} 
\toprule[0.15em]
\multirow{3}{*}{\textbf{ Method}} & \multirow{3}{*}{\textbf{Task }} & \multicolumn{7}{c}{\textbf{NUS-WIDE ( \#seen / \#unseen = 925/81) }} & \multicolumn{7}{c}{\textbf{Open-Images ( \#seen / \#unseen = 7186/400) }} \\
 &  & \multicolumn{3}{c}{\cellcolor[HTML]{EEEEEE}\textbf{K = 3 }} & \multicolumn{3}{c}{\cellcolor[HTML]{DAE8FC}\textbf{K = 5 }} & \multirow{2}{*}{\textbf{mAP }} & \multicolumn{3}{c}{\cellcolor[HTML]{EEEEEE}\textbf{K = 10 }} & \multicolumn{3}{c}{\cellcolor[HTML]{DAE8FC}\textbf{K = 20 }} & \multirow{2}{*}{\textbf{mAP }} \\
 &  & \textbf{P } & \textbf{R } & \textbf{F1 } & \textbf{P } & \textbf{R } & \textbf{F1 } &  & \textbf{P } & \textbf{R } & \textbf{F1 } & \textbf{P } & \textbf{R } & \textbf{F1 } &  \\ 
\toprule[0.15em]
\multirow{2}{*}{CONSE~\cite{norouzi2013zero}} & ZSL & 17.5 & 28.0 & 21.6 & 13.9 & 37.0 & 20.2 & 9.4 & 0.2 & 7.3 & 0.4 & 0.2 & 11.3 & 0.3 & 40.4 \\
 & GZSL & 11.5 & 5.1 & 7.0 & 9.6 & 7.1 & 8.1 & 2.1 & 2.4 & 2.8 & 2.6 & 1.7 & 3.9 & 2.4 & 43.5 \\ 
\cmidrule(lr){2-16}
\multirow{2}{*}{LabelEM~\cite{akata2015label}} & ZSL & 15.6 & 25.0 & 19.2 & 13.4 & 35.7 & 19.5 & 7.1 & 0.2 & 8.7 & 0.5 & 0.2 & 15.8 & 0.4 & 40.5 \\
 & GZSL & 15.5 & 6.8 & 9.5 & 13.4 & 9.8 & 11.3 & 2.2 & 4.8 & 5.6 & 5.2 & 3.7 & 8.5 & 5.1 & 45.2 \\ 
\cmidrule(lr){2-16}
\multirow{2}{*}{Fast0Tag~\cite{zhang2016fast}} & ZSL & 22.6 & 36.2 & 27.8 & 18.2 & 48.4 & 26.4 & 15.1 & 0.3 & 12.6 & 0.7 & 0.3 & 21.3 & 0.6 & 41.2 \\
 & GZSL & 18.8 & 8.3 & 11.5 & 15.9 & 11.7 & 13.5 & 3.7 & 14.8 & 17.3 & 16.0 & 9.3 & 21.5 & 12.9 & 45.2 \\ 
\cmidrule(lr){2-16}
\multirow{2}{*}{One Attention per Label~\cite{kim2018bilinear}} & ZS & 20.9 & 33.5 & 25.8 & 16.2 & 43.2 & 23.6 & 10.4 & - & - & - & - & - & - & - \\
 & GZSL & 17.9 & 7.9 & 10.9 & 15.6 & 11.5 & 13.2 & 3.7 & - & - & - & - & - & - & - \\ 
\cmidrule(lr){2-16}
\multirow{2}{*}{One Attention per Cluster (M=10)~\cite{huynh2020shared}} & ZSL & 20.0 & 31.9 & 24.6 & 15.7 & 41.9 & 22.9 & 12.9 & 0.6 & 22.9 & 1.2 & 0.4 & 32.4 & 0.9 & 40.7 \\
 & GZSL & 10.4 & 4.6 & 6.4 & 9.1 & 6.7 & 7.7 & 2.6 & 15.7 & 18.3 & 16.9 & 9.6 & 22.4 & 13.5 & 44.9 \\ 
\cmidrule(lr){2-16}
\multirow{2}{*}{LESA (M=10)~\cite{huynh2020shared}} & ZSL & 25.7 & 41.1 & 31.6 & 19.7 & 52.5 & 28.7 & 19.4 & 0.7 & 25.6 & 1.4 & 0.5 & 37.4 & 1.0 & 41.7 \\
 & GZSL & 23.6 & 10.4 & 14.4 & 19.8 & 14.6 & 16.8 & 5.6 & 16.2 & 18.9 & 17.4 & 10.2 & 23.9 & 14.3 & 45.4 \\ 
\cmidrule(lr){2-16}
\multirow{2}{*}{\textbf{Our Approach}} & ZSL & \textbf{26.6}  & \textbf{42.8}  & \textbf{32.8}  & \textbf{20.1}  & \textbf{53.6}  & \textbf{29.3}  & \textbf{25.7}  & \textbf{1.3} & \textbf{42.4} & \textbf{2.5} & \textbf{1.1}  & \textbf{52.1} & \textbf{2.2} & \textbf{43.0} \\
 & GZSL & \textbf{30.9}  & \textbf{13.6}  & \textbf{18.9}  & \textbf{26.0}  & \textbf{19.1}  & \textbf{22.0}  & \textbf{8.9}  & \textbf{33.6} & \textbf{38.9} & \textbf{36.1} & \textbf{22.8} & \textbf{52.8} & \textbf{31.9} & \textbf{49.7} \\
\bottomrule[0.1em]
\end{tabular}
}
\vspace{-0.15cm}
\label{tab:sota_nuswide_openimages}
\end{table*}

\subsection{Ablation Study\label{sec:ablation}}
We first present an ablation study \wrt our fusion approaches: attribute-level (ALF), feature-level (FLF) and cross-level feature (CLF) on the NUS-WIDE dataset. We evaluate our fusion strategies with both architectures: \clswgan{} and \vaegan{}. Tab.~\ref{tab:ablation} shows the comparison, in terms of F1 score () and mAP for both ZSL and GZSL tasks. In the case of ZSL, the ALF-based \clswgan{} achieves mAP score of . The FLF-based \clswgan{} obtains similar performance with mAP score of . The CLF-based \clswgan{} achieves improved performance with mAP score of . Similarly, the CLF-based \clswgan{} obtains consistent improvement over both ALF and FLF-based \clswgan{} in terms of F1 score (). In the case of GZSL, ALF and FLF obtain  F1 scores () of  and , respectively. Our CLF approach achieves improved performance with F1 score of . Similarly, CLF performs favorably against both ALF and FLF in terms of F1 at  and mAP metrics.  

As in \clswgan{}, we also observe the CLF approach to achieve consistent improvement in performance over both ALF and FLF, when integrated in the more sophisticated \vaegan{}. In the case of ZSL, our CLF-based \vaegan{} achieves absolute gains of  and  in terms of mAP over ALF and FLF, respectively. Similarly, CLF-based \vaegan{} achieves consistent improvement in performance in terms of F1 score, over both ALF and FLF. Furthermore, CLF-based \vaegan{}  performs favorably against the other two fusion approaches in case of GZSL.

The aforementioned results show that our CLF approach achieves consistent improvement in performance over both ALF and FLF for both ZSL and GZSL, regardless of the underlying architecture. Furthermore, the best results are obtained when integrating our CLF in \vaegan{}. Next, we compare our CLF-based \vaegan{}, denoted as \textit{our approach} hereafter, with state-of-the-art methods.

\subsection{State-of-the-art Comparison\label{sec:sota_compare}}

\noindent\textbf{NUS-WIDE:} 
Tab.~\ref{tab:sota_nuswide_openimages} shows the state-of-the-art comparison for zero-shot (ZSL) and generalized zero-shot (GZSL) classification. The results are reported in terms of mAP and F1 score at \textit{top}- predictions (). In addition, we also report the precision (P) and recall (R) for each F1 score.  For the ZSL task, Fast0Tag~\cite{zhang2016fast} approach that finds principal directions in the word vector space for ranking the relevant tags ahead of the irrelevant tags, achieves mAP score of . The recently introduced LESA approach \cite{huynh2020shared}, which utilizes a shared multi-attention mechanism to predict all labels in an image, achieves improved performance over Fast0Tag~\cite{zhang2016fast}, with mAP score of . LESA also obtains better performance over both one attention per label and per cluster. Our approach achieves state-of-the-art results with an absolute gain of  in terms mAP, over the best existing approach LESA. Similarly, our approach obtains consistent improvement in classification performance over the state-of-the-art in terms of F1 score (). 

For the GZSL task, Fast0Tag and one attention per label achieve similar performance in terms of mAP. LESA obtains improved classification results among existing methods with mAP score of . Our approach achieves mAP score of , outperforming LESA with an absolute gain of . Similarly, our approach achieves consistent improvement in classification performance with absolute gains of  and  over LESA in terms of F1 score at  and , respectively.

Fig.~\ref{fig:nuswide_compare} shows the classification mAP improvement of our approach \wrt LESA for  unseen labels with the largest
performance improvement as well as  unseen labels with the largest drop. Our proposed approach significantly improves (more than ) on several unseen labels, such as \textit{waterfall}, \textit{bear}, \textit{dog}, \textit{protest} and \textit{food}, while having relatively smaller (less than ) negative impact on labels, such as \textit{sunset}, \textit{person}, \textit{sky}, \textit{water} and \textit{rocks}. We observe our approach to be particularly better on animal categories ( out of ). We also observe our approach to struggle in case of abstract concepts (\eg \textit{sunset}, \textit{sky}, \textit{reflection}). In total, our approach outperforms LESA on  out of  labels.

\begin{figure}[t]
\centering
\includegraphics[width=0.97\columnwidth]{images/improvement.pdf}
\vspace{-0.3cm}
\caption{Classification mAP improvement comparison between our approach and LESA on NUS-WIDE. We show the comparison for  unseen labels with the largest gain as well  unseen labels with the largest drop. Our approach significantly improves (more than ) on several unseen labels (\eg, \textit{waterfall}, \textit{bear}, \textit{dog}, \textit{protest} and \textit{food}), while having relatively smaller (less than ) negative impact on other labels. Best viewed zoomed in.\vspace{-0.35cm}}
\label{fig:nuswide_compare}
\end{figure}

\noindent\textbf{Open Images:} Tab.~\ref{tab:sota_nuswide_openimages} shows the state-of-the-art comparison for ZSL and GZSL classification. The results are reported in terms of mAP and F1 score at \textit{top}- predictions (). Compared to the NUS-WIDE dataset, Open Images has significantly larger number of labels. This makes the ranking problem within an image more challenging, reflected by the lower F1 scores in the table. For the ZSL task, LESA obtains F1 scores of  and  at  and , respectively. Our approach performs favorably against LESA with F1 scores of  and  at  and , respectively. A similar performance gain is also observed for the mAP metric.
It is worth noting that this dataset has  unseen labels, thereby making the problem of ZSL challenging. As in ZSL, our approach also achieves consistent improvement in performance, in terms of both F1 and mAP, over the state-of-the-art for the GZSL task. 

\noindent\textbf{MS COCO:} Although the problem of zero-shot object detection is investigated, we are the first to evaluate zero-shot classification on this dataset. Tab.~\ref{tab:sota_coco} shows the state-of-the-art comparison for ZSL and GZSL classification. Since the maximum number of unseen classes in an image is  in the validation set, we only report the F1 score at . We re-implement LabelEM, CONSE and Fast0tag since their codes are not publicly available. For LESA, we obtain the results using the code-base from authors.
Our approach obtains superior (G)ZSL classification performance, in terms of F1 score and mAP, compared to existing methods. 


\begin{table}[t]
\centering
\caption{State-of-the-art comparison for ZSL and GZSL tasks on the MS COCO dataset having  seen and  unseen classes. We report the results in terms of mAP and F1 score at . Our approach performs favorably against existing approaches for both ZSL and GZSL tasks. Best results are in bold.}\vspace{0.2em} 
\setlength{\tabcolsep}{12pt}
\adjustbox{width=1\linewidth}{
\begin{tabular}{ccccc} 
\toprule[0.15em]
\textbf{Method} & \textbf{Task} & \begin{tabular}[c]{@{}c@{}} \cellcolor[HTML]{EEEEEE}\textbf{F1 (K=3)} \end{tabular} & \begin{tabular}[c]{@{}c@{}}\cellcolor[HTML]{DAE8FC} \textbf{F1 (K=5)} \end{tabular} & \textbf{mAP} \\
\toprule[0.15em]
\multirow{2}{*}{CONSE~\cite{norouzi2013zero}} & ZSL & 18.4 & - & 13.2 \\
 & GZSL & 19.6 & 18.9 & 7.7 \\ 
\cmidrule(lr){2-5}
\multirow{2}{*}{LabelEM~\cite{akata2015label}} & ZSL & 10.3 & - & 9.6 \\
 & GZSL & 6.7 & 7.9 & 4.0 \\ 
\cmidrule(r){2-5}
\multirow{2}{*}{Fast0tag~\cite{zhang2016fast}} & ZSL & 37.5 & - & 43.3 \\
 & GZSL & 33.8 & 34.6 & 27.9 \\ 
\cmidrule(lr){2-5}
\multirow{2}{*}{LESA~\cite{huynh2020shared}} & ZSL & 33.6 & - & 31.8 \\
 & GZSL & 26.7 & 28.0 & 17.5 \\ 
\cmidrule(r){2-5}
\multirow{2}{*}{\textbf{Our Approach}} & ZSL & \textbf{43.5} & - & \textbf{52.2} \\
 & GZSL & \textbf{44.1} & \textbf{43.4} & \textbf{33.2} \\
\bottomrule[0.1em]
\end{tabular}}
\vspace{-0.2cm}
\label{tab:sota_coco}
\end{table}

\begin{table}[t]
\centering
\caption{Standard multi-label classification performance comparison on NUS-WIDE. The results are reported in terms of mAP and F1 score at . Our approach achieves superior performance compared to existing methods. Best results are in bold.}
\adjustbox{width=\linewidth}{
\begin{tabular}{cccccccc} 
\toprule[0.15em]
\multirow{2}{*}{\textbf{Method} } & \multicolumn{3}{c}{\cellcolor[HTML]{EEEEEE}\textbf{K=3}} & \multicolumn{3}{c}{\cellcolor[HTML]{DAE8FC}\textbf{K=5}} & \multirow{2}{*}{\textbf{mAP}} \\
 & \textbf{P} & \textbf{R} & \textbf{F1} & \textbf{P} & \textbf{R} & \textbf{F1} &  \\ 
\toprule[0.15em]
Logistic~\cite{tsoumakas2007multi} & 46.1 & 57.3 & 51.1 & 34.2 & 70.8 & 46.1 & 21.6 \\ 
\cmidrule(r){2-8}
WARP~\cite{gong2013deep} & 49.1 & 61.0 & 54.4 & 36.6 & 75.9 & 49.4 & 3.1 \\ 
\cmidrule(r){2-8}
WSABIE~\cite{weston2011wsabie} & 48.5 & 60.4 & 53.8 & 36.5 & 75.6 & 49.2 & 3.1 \\ 
\cmidrule(r){2-8}
Fast0Tag~\cite{zhang2016fast} & 48.6 & 60.4 & 53.8 & 36.0 & 74.6 & 48.6 & 22.4 \\ 
\cmidrule(r){2-8}
CNN-RNN~\cite{wang2016cnn} & 49.9 & 61.7 & 55.2 & 37.7 & 78.1 & 50.8 & 28.3 \\ 
\cmidrule(r){2-8}
One Attention per Label~\cite{kim2018bilinear} & 51.3 & 63.7 & 56.8 & 38.0 & 78.8 & 51.3 & 32.6 \\ 
\cmidrule(r){2-8}
One Attention per Cluster~\cite{huynh2020shared} & 51.1 & 63.5 & 56.6 & 37.6 & 77.9 & 50.7 & 31.7 \\ 
\cmidrule(r){2-8}
LESA~\cite{huynh2020shared} & 52.3 & 65.1 & 58.0 & 38.6 & 80.0 & 52.0 & 31.5 \\ 
\cmidrule(r){2-8}
\textbf{Our Approach} & \textbf{53.5} & \textbf{66.5} & \textbf{59.3} & \textbf{39.4} & \textbf{81.6} & \textbf{53.1} & \textbf{46.7} \\
\bottomrule[0.1em]
\end{tabular}}
\vspace{-0.2cm}
\label{tab:mll_nuswide}
\end{table}

\subsection{Standard Multi-label Classification\label{sec:stnd_ML}}
In addition to zero-shot classification, we evaluate our approach for standard multi-label classification where all labels have training images. Tab.~\ref{tab:mll_nuswide} shows the state-of-the-art comparison for standard multi-label classification on NUS-WIDE with  human annotated labels. As in~\cite{huynh2020shared}, we remove all test samples without any label in the  label set. 
Among existing methods, CNN-RNN and LESA achieve mAP scores of  and , respectively. Our approach outperforms existing methods, achieving mAP score of . The proposed approach also performs favorably against existing methods in terms of F1 scores. 

\section{Extension to Zero-shot Object Detection\label{sec:obj_det}}
Lastly, we also evaluate our multi-label feature generation approach (CLF) for zero-shot object detection (ZSD). ZSD strives for simultaneous classification and localization of previously unseen objects. In the generalized settings, the test set contains both seen and unseen object categories (GZSD). Recently, the work of~\cite{hayat2020synthesizing} introduce a zero-shot detection approach (SUZOD) where features are synthesized, conditioned upon class-embeddings, and integrated in the popular Faster R-CNN framework~\cite{ren2015faster}. The feature generation stage is jointly driven by the classification loss in the semantic embedding space for both seen and unseen categories. Their approach addresses multi-label zero-shot detection by constructing single-label features for each region of interest (RoI). Different from SUZOD~\cite{hayat2020synthesizing}, we first generate a pool of multi-label RoI features by integrating random sets of individual single-label RoI features. These integrated multi-label RoI features are then used as real features to train our CLF-based classification architecture.  

Tab.~\ref{tab:sota_det_coco} shows the state-of-the-art comparison, in terms of recall, for ZSD and GZSD detection on MS COCO. For GZSD, Harmonic Mean (HM) of performances for seen and unseen classes are reported. Similar to SUZOD~\cite{hayat2020synthesizing}, we also report the results by using \clswgan{} as the underlying generative architecture. Our approach performs favorably against existing methods for both ZSD and GZSD. 

\begin{table}[t]
\centering
\caption{State-of-the-art comparison for ZSD and GZSD tasks on MS COCO. The results are reported in terms of Recall. For GZSD, we report the harmonic mean (HM) between seen and unseen classes. Our approach performs favorably against existing methods. Best results are in bold. }\vspace{0.2em}
\label{tab:sota_det_coco}
\adjustbox{width=\linewidth}{
\begin{tabular}{cccccc} 
\toprule[0.15em]
\multirow{2}{*}{Method} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}~Seen/Unseen \\Split~ \end{tabular}} & \multirow{2}{*}{~ZSD~} & \multicolumn{3}{c}{GZSD} \\
 &  &  & ~Seen~ & ~Unseen~ & ~HM~ \\ 
\toprule[0.15em]
SB~\cite{bansal2018zero} & 48/17 & 24.39 & - & - & - \\
DSES~\cite{bansal2018zero} & 48/17 & 27.19 & 15.02 & 15.32 & 15.17 \\
PL-48~\cite{rahman2018polarity} & 48/17 & 43.56 & 38.24 & 26.32 & 31.18 \\
PL-65~\cite{rahman2018polarity} & 65/15 & 37.72 & 36.38 & 37.16 & 36.76 \\
SUZOD~\cite{hayat2020synthesizing} & 65/15 & 54.40 & 57.70 & 54.40 & 55.74 \\
\textbf{Our Approach} & 65/15 & \textbf{58.10} & \textbf{58.90} & \textbf{58.10} & \textbf{58.50} \\
\bottomrule[0.1em]
\end{tabular}
}
\vspace{-0.2cm}
\end{table}

\section{Conclusion}
We investigate the problem of multi-label feature synthesis in the zero-shot setting. We introduce three different fusion approaches (ALF, FLF and CLF) to synthesize multi-label features. Our ALF synthesizes features by integrating class-specific attribute embeddings at the generator input.
On the other hand, FLF synthesizes features from class-specific embeddings individually and integrates them in feature space. Our CLF combines the advantages of both ALF and FLF, using each individual-level feature and attends to the bi-level context. Consequently, individual-level features adapt themselves producing enriched synthesized features that are pooled to obtain final output. We integrate our fusion approaches in two generative architectures. Our approach outperforms existing zero-shot methods on three multi-label classification benchmarks. Lastly, we also show the effectiveness of our approach for zero-shot detection. 



{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
