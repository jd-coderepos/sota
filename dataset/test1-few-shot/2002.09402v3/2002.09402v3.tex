\PassOptionsToPackage{table}{xcolor}
\documentclass{article} \usepackage{iclr2021_conference}
\usepackage{times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 \usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      

\usepackage{graphicx}
\usepackage{subfigure}
\usepackage[export]{adjustbox}


\usepackage{caption}
\captionsetup[table]{skip=10pt}

\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}            \usepackage{amsfonts}       \usepackage{xcolor}  \usepackage{spverbatim}
\usepackage{multirow} 

\newcommand{\secc}[1]{Section~\ref{sec:#1}}
\newcommand{\fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\tab}[1]{Table~\ref{tab:#1}}
\newcommand{\eq}[1]{Eq.~\ref{eq:#1}}
\newcommand{\app}[1]{Appendix~\ref{app:#1}}

\newcommand{\armand}[1]{{\color{red}[\textbf{MB}:#1]}}
\newcommand{\sainaa}[1]{{\color{blue}[\textbf{S}:#1]}}
\newcommand{\edouard}[1]{{\color{orange}[\textbf{E:}#1]}}
\newcommand{\af}[1]{{\color{purple}[\textbf{angela:}#1]}}

\title{Addressing Some Limitations of Transformers with Feedback Memory}



\author{Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, Sainbayar Sukhbaatar \\ 
Facebook AI Research, LORIA\\
\texttt{\{angelafan,thibautlav,egrave,ajoulin,sainbar\}@fb.com}
}
\iclrfinalcopy



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}


\maketitle
\lhead{}

\begin{abstract}
Transformers have been successfully applied to sequential, auto-regressive tasks despite being feedforward networks.
Unlike recurrent neural networks, Transformers use attention to capture temporal relations while processing input tokens in parallel.
While this parallelization makes them computationally efficient, 
it restricts the model from fully exploiting the sequential nature of the input.
The representation at a given layer can only access representations from lower layers, rather than the higher level representations already available.
In this work, we propose the Feedback Transformer architecture that exposes all previous representations to all future representations, meaning the lowest representation of the current timestep is formed from the highest-level abstract representation of the past.
We demonstrate on a variety of benchmarks in language modeling, machine translation, and reinforcement learning that the increased representation capacity can create small, shallow models with much stronger performance than comparable Transformers. 

\end{abstract}

\section{Introduction}

In recent years, the Transformer architecture~\citep{vaswani2017attention} has brought large improvements to a wide range of Natural Language Processing tasks such as machine translation, sentence representation~\citep{devlin2018bert}, and summarization~\citep{edunov2019pre}. 
Transformers are also successfully used as an autoregressive model on sequential tasks such as language modeling~\citep{dai2019transformer,rae2020compressive} and reinforcement learning~\citep{Parisotto2019StabilizingTF}.
Unlike more traditional recurrent architectures such as RNNs and LSTMs, the Transformer architecture processes a sequence in parallel in an order-invariant way.
Techniques such as position embeddings~\citep{sukhbaatar2015end,shaw2018self} and attention masking are required to capture  input order information. In this work, we focus on several limitations of the Transformer architecture as an autoregressive model and present a straightforward solution --- \textit{Feedback memory}.  These limitations and our proposed solution target sequential token prediction tasks, such as language modeling or other auto-regressive generative tasks.

The feedforward nature of Transformers makes them efficient on modern hardware, but restricts the Transformer from taking full advantage of the input's sequential property.
In particular, the current hidden representation of a Transformer only accesses the past representations of lower layers, even though higher level representations of the past have already been computed as an autoregressive model.
At generation, the Transformer generates only one token at a time, so it could access these representations for better performance, but  does not exploit these at training time due to parallelization.
However, if these past higher level representations could be used at training time, they would enrich future lower level representations, enabling shallower models to have the same representation power.

Another inherent limitation of Transformers on sequential tasks is the lack of recursive computation~\citep{dehghani2018universal}, and the number of transformations possible on the input is bounded by the model depth. 
Such disadvantages have impact on tasks that require careful tracking of a world state or modeling hierarchical structures~\citep{tran2018importance,hahn2020theoretical}.
On the other hand, while RNNs can maintain an internal state for an unbounded time while accumulating more computations upon it, the size of this internal state is limited by the dimension of the hidden state. 

In this work, we propose a novel autoregressive model, the \textit{Feedback Transformer}, that makes all previous hidden representations accessible to the computation of a representation at any depth --- the model \emph{feeds back} previous computations to itself.
The feedback allows the model to perform recursive computation, building stronger representations iteratively upon previous states.
To achieve this, we modify self-attention to attend to higher level representations rather than lower ones.

As shown in \fig{pullfig}, the Feedback Transformer merges the hidden states from all layers into a single vector for every time step and stores them in a memory. 
Instead of self-attention, all subsequent layers attend to this memory, which means every previously computed representation is accessible by all future layers, mediated by the memory.
This allows Feedback Transformers to recursively compute and transform an input as many times as the input length, which is something Transformers cannot achieve.
While RNNs can perform recursive computation, the amount of information that Feedback Transformers can maintain is not limited by the number of layers.

There are computational benefits to this straightforward modification. First, it uses less memory because all the layers share a single Feedback memory, thus reducing the memory size by  times, where  is the number of layers. There is also less computation because we share the key and value projections during attention computation, which increases the speed of the attention over the Feedback Memory.
Further, the GPU memory usage is reduced due to the memory sharing --- the overall model is 2x smaller --- allowing the batch size to be increased for computational efficiency. During inference, the increased batch size contributes to substantially faster decoding speeds. 

In summary, our main contributions are: (1) The Feedback Transformer architecture, which completely changes the way a Transformer works to access available higher level representations immediately. (2) We show the Feedback Transformer can achieve state of the art results with smaller, shallower models that have faster decoding speed and smaller memory footprint. (3) The Feedback Transformer uses substantially less memory during training and inference time. 
 \section{Related work}

Several previous works have analyzed the limitations of Transformer architectures, such as the inability to process input sequentially~\citep{dehghani2018universal} or represent hierarchical structure~\citep{tran2018importance}. \citet{hahn2020theoretical} demonstrate that Transformers cannot model structures involving bounded recursion, such as closing parentheses. \citet{perez2019turing} study Transformers in the context of Turing machines, where they must produce unbounded numbers of decoding steps. Various work in probing Transformers identified several limitations where Transformers may not have the computational capacity of recurrent architecture like an LSTM~\citep{hahn2020theoretical}. 

From the architectural perspective, our work shares similarities with recurrent networks augmented with external shared memories~\citep{graves2014neural,joulin2015inferring,sukhbaatar2015end}.
For example, the stack augmented RNN of~\citet{joulin2015inferring} adds an external memory to a recurrent network to keep long term dependencies.
Closer to our work, the Neural Turing Machine of~\citet{graves2014neural} models an unconstrained memory that resembles the self-attention layer of a Transformer.
Further improvements to recurrent networks, such as the Gated Feedback RNN~\citep{chung2015gated}, are based on better controlling signal from different layers and extended to feedback through multiple pathways~\citep{jin2017multi}. 
These works are built on recurrent networks with additional components to store long term dependencies. 

Other works have studied modifications to the Transformer architecture by enriching its structure with components inspired by recurrent networks. 
For example, \citet{wang2019r} propose adding a local recurrent sublayer to the Transformer layer to remove the need for position embeddings in the multi-head self-attention layers.
Universal Transformer~\citep{dehghani2018universal} share the parameters between the layers of a Transformer, leading a recurrent network in depth.
\citet{hao2019modeling} and \citet{chen2018best} augment Transformers with a second, recurrent encoder. 
As opposed to our work, these prior investigations do not change the computational path in a Transformer to reduce the discrepancy between the training and inference time.
Closer to our work,~\citet{merity2019single} proposes adding a self-attention layer on top of the past outputs from an LSTM cell.
However, this approach keeps the recurrent and the self-attention mechanisms decoupled, as opposed to ours which makes the attention mechanism recurrent.
In particular, the LSTM layer of~\citet{merity2019single} still intrinsically has a bottleneck corresponding to the dimension of the hidden layer.
 
\begin{figure}[t]
	\centering
	\begin{minipage}[t]{0.47\textwidth}
		\centering
		\includegraphics[height=1.7in]{figs/model.pdf}
		\caption{The \textbf{Feedback Transformer} merges past hidden representations from all layers into a single vector and stores it in memory.}
		\label{fig:pullfig}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.47\textwidth}
		\centering
		\includegraphics[height=1.7in]{figs/pullfig.png}
		\caption{\textbf{Difference between Feedback and Transformer.}   indicates the timestep and  indicates the layer.}
		\label{fig:summary}
	\end{minipage}
\end{figure}


\section{Method}
\label{sec:method}

In this section, we propose the Feedback Transformer, which provides capacity to build richer representations of each timestep  of a sequential modeling task.

\subsection{Transformer Architectures}

We briefly describe the Transformer~\citep{vaswani2017attention}.
Each layer is composed of a multi-head self-attention sublayer (\texttt{Attn}) followed by a feedforward sublayer (\texttt{FF}), and each sublayer is followed by an add-norm operation that combines a skip-connection~\citep{he2016deep} and layer normalization~\citep{ba2016layer}.
The -th layer of a Transformer processes an input sequence of vectors  into a sequence of vectors of the same length.
First, the self-attention sublayer computes a representation for each time step  by taking its related input vector  along with its past context, :

Within the self-attention sublayer,  is used to form query vectors while its context is used to compute key and value vectors, forming a memory of the past information.
Then the feedforward sublayer processes each vector  independently, i.e., .
The Transformer layer transforms its input sequence into an output sequence .

In practice, a block of steps  is computed in parallel during training, where  can be seen as the backpropagation through time (BPTT) length. This makes training Transformers efficient on hardware such as GPUs. However, to operate on sequences of unbounded length, Transformers require modifications such as caching and relative position embeddings~\citep{dai2019transformer,sukhbaatar2019adaptive}.

\subsection{Limitations of Transformers}
Previous work has analyzed the impact of several limitations of the Transformer architecture, such as the inability to track long sequences and process hierarchical inputs \citep{hahn2020theoretical}. In this work, we focus on two major limitations of Transformer architectures.

\paragraph{Limited Access to Higher Level Representations.} Layer by layer, Transformers build more abstract, high level representations of the input sequence. 
At each layer, the representations for the input sequence are treated in parallel.
As a consequence, a Transformer does not leverage the highest level representations from the past to compute the current representation, even though these highest level representations have \emph{already been computed} for autoregressive models.

\paragraph{Maintaining a Belief State.} 
Many sequential tasks require models to maintain an internal state for two main purposes.
First, internal states act as memory for recalling past inputs, where Transformers excel because their internal state  is directly accessible to future steps through self-attention.

The second role of an internal state is to act as a belief state that tracks the world state that is not directly observable in inputs. 
For example, when inputs are actions taken on a Markov Decision Process, an internal state can apply those changes to the current belief state and correctly predict the outcome. 
As a feedforward model, Transformer have inherent limitations in this area --- only a fixed number of transformations can be applied to its internal states. 
Since both  and  sublayers contain a fixed number of transformations and there are  layers of them, the total number of transformations between the input and output is limited by the depth. 
This means Transformers cannot maintain an internal state for long time if it has to be frequently updated.

\subsection{Feedback Transformer}

We propose to change the Transformer architecture by using the most abstract representations from the past directly as inputs for the current timestep.
This means that the model does not form its representation in parallel, but sequentially token by token.
More precisely, we replace the context inputs to attention modules with memory vectors that are computed over the past, i.e., 

where memory vectors  are computed by summing the representations of all layers at time step :

where  are learnable scalar parameters.
Note these scalars are the only new parameters introduced by our change, with all else the same as the standard Transformer. Here  corresponds to token embeddings.
The weighting of different layers by a softmax output gives the model more flexibility as it can average them or select one of them.


This modification of the self-attention input adapts the computation of the Transformer from parallel to sequential, summarized in Figure~\ref{fig:summary}. 
Indeed, it provides the ability to formulate the representation  based on past representations from any layer , while in a standard Transformer this is only true for . 
This change can be viewed as exposing all previous computations to all future computations, providing better representations of the input.
Such capacity would allow much shallower models to capture the same level of abstraction as a deeper architecture.
This has several practical advantages, as more shallow models have reduced memory footprint and increased decoding speed.

An alternative view of such an architecture modification is providing the capacity for recursive computation --- outputs from a sublayer can feed back to the same sublayer through the memory. 
The model can then maintain an internal state for unbounded time.
This is a clear advantage over Transformers, in which a submodule never looks at its own output.
While an RNN can also repeat its computation on its internal state, its  internal state has a limited capacity determined by the number of layers and their hidden dimension. 
In contrast, the internal state of a Feedback Transformer is its whole memory, which can grow with the input length. 
This allows the model to keep track of a large number of things within its internal state.

While our modification requires sequential computation, we significantly improve training speed by sharing the key and value projections  and  across all layers.
This sharing reduces computation because we need to compute key and value vectors only once instead of computing them per layer
 
For the same reason, the memory footprint is smaller than a standard Transformer because only one set of ,  needs to be stored.
To be more precise, the memory requirement for processing a single token is reduced from  to , where  is the number of layers and  is the context size.
Further, the reduced memory usage allows the batch size to be increased to recover some of the lost parallelism, which improves training speed.
Thus, the Feedback Transformer is not much slower compared to the standard Transformer. 
Note that the same sharing of projections will not make the standard Transformer efficient because those projections are applied to different representations at each layer (the key and value vectors will not the same for all layers).


Lastly, we note that the sequential nature of the Feedback Transformer does not affect the performance during generation where one needs to compute one step at a time anyway.
The same is true for online reinforcement learning where the input must be processed sequentially even during training. 












 \section{Experiments}


We explore different sequential input tasks in natural language processing and reinforcement learning. First, we demonstrate the downsides of the standard Transformer architecture on tasks where the Transformer performs poorly. We show that the Feedback Transformer is able to overcome challenges and retain long memory. Next, we highlight the strength of the Feedback architecture in building complex, high level representations even with shallow models. We demonstrate that the Feedback model can achieve significantly stronger results than Transformer models, an effect that is exaggerated as models get smaller. Finally, we compare the Feedback architecture to the Transformer architecture with other work on standard long-context language modeling tasks. In experiments on large datasets, we use the shared key-value projections to improve training time. Additional experimental details and results can be found in the appendix.

\begin{table}
	\centering
	\begin{minipage}{.45\textwidth}
	    \centering
\begin{tabular}[t]{llcc}
			\toprule
			\multicolumn{2}{l}{\bf Task} & \bf Trans- & \bf Feedback \\
			&  & \bf former & \bf Trans. \\
			\midrule
			\bf Copy & Char & 59.1 & 76.2 \\
			     & Seq & 6.2 & 23.6 \\
			 \midrule
			\bf Reverse & Char & 50.2 & 74.8 \\
			        & Seq & 5.9 & 29.2 \\
			\midrule
			\bf Counting & Len 50 & 99.6 & 99.7 \\
			& Len 1K & 82.4 & 95.3 \\
			\midrule
			\multicolumn{2}{l}{\bf Random Walk} & 68 & 100 \\
			\midrule
			\bf Algorithmic  & 3 vars & 33.7 & 99.1 \\
			 & 5 vars & 37.5 & 92.6 \\
			\bottomrule
		\end{tabular}		
	    \caption{
	    \textbf{Accuracy on toy tasks.} Char is character accuracy, Seq is sequence accuracy.}  
	    \label{tab:toy_tasks}
	\end{minipage}
	\hfill
	\begin{minipage}{.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figs/corridor.pdf}
		\captionof{figure}{\textbf{Results on the Corridor task.} The Transformer degrades as the memory size decreases, but the Feedback Transformer maintains performance.}
		\label{fig:corridor_result}
	\end{minipage}

\end{table}

\subsection{Limitations of Transformer: Illustrative Tasks}

\subsubsection{Limited Access to Long Memory}

First, we examine the Transformer's limited access to long memory on several simple, straightforward tasks that illustrate this. Unlike the standard Transformer, the Feedback architecture is able to remember information over many timesteps. 

\paragraph{Walking down a Corridor.}

In this reinforcement learning task, each agent is placed at the start of a long corridor with either a \textit{blue} or \textit{green} object. The agent must look at the object's color, walk down the corridor, and go through the corresponding colored door at the end. The only task is to remember the color and not become distracted by walking down the very long hallway. Results are shown in Figure~\ref{fig:corridor_result} and show that the performance of the Transformer degrades quickly as the memory size shrinks, but the Feedback Transformer maintains strong performance at all memory sizes. 

\paragraph{Copy and Reverse.}
We experiment next on two algorithmic tasks, copy and reverse~\citep{kaiser2015neural}.
We train on sequences of length 40 consisting of integers 0 through 9, and test on sequences of length 400. 
Models read the input and then either copy or reverse, which requires memory over the sequence and the ability to track position, as well as generalization capability as the train and test settings are different lengths. 
We consider two variations of copying and reversing: either at the character level or at the sequence level. 
Results are shown in Table~\ref{tab:toy_tasks}. 
The Feedback architecture has large improvements in accuracy, indicating improved memory and positional tracking.

\paragraph{Counting.}

Finally, we experiment on a counting task, where models have a sequence of \textit{A}'s in a row, and must output the corresponding quantity of the letter \textit{B}. The model must count the number of the A's to output the correct number of B's. We consider two settings: training on short sequences of lengths up to 50 and training on long sequences of lengths up to 1000. We show results in Table~\ref{tab:toy_tasks}, where we demonstrate the Feedback model is much better at counting over long sequences.


\subsubsection{Limited State Updates}

The complexity of the representations the Transformer is able to formulate is strictly dependent on the depth, as each layer of the Transformer allows for additional nonlinearity. The Transformer, then, can only update its state the same number of times as it has layers. We demonstrate that the Feedback Transformer does not have this limitation --- in tasks where the model must carefully track and update its state, the Feedback architecture is able to update its state at each timestep. 

\paragraph{Random Walk.}

We consider a random walk in a small grid where actions are: go forward 1 step, left turn, and right turn. Given a history of actions and the agent's initial position, it is strictly possible to calculate the current position. 
The task is trivial because a human could write down the current location and direction and keep updating with each action. 
However, Transformers cannot do this because they lack a storage that can be updated with each input. Its hidden state can store this information, but with each update, that information has to go up one layer. 

An alternative approach to this task is to solve it all at once given a sequence of actions, which is feasible for Transformers since they can access all inputs with their attention.
However, this approach is challenging because the effect of each action depends on the direction at that point and whether the agent is on the edges, which itself is not known yet.
This can be seen in \tab{toy_tasks}, where the Transformer struggles and only reaches  accuracy. In contrast, the Feedback Transformer achieves  accuracy, which indicates the ability to track state for a long period of time. Both models are trained on 10K sequences, each containing 100 random actions and positions. 

\paragraph{Algorithmic task.}
A more complex setting where tracking and updating of a state is crucial is code executions. 
A model needs keep track of all variable values and update them if necessary. 
To demonstrate this, we create a simple algorithmic task that consists of the following simple statements: assignments (e.g.\ \verb), increments and decrements (e.g.\ \verb), conditionals (e.g.\ \verb), and print commands (e.g.\ \verb). Each task consists of 100 randomly selected statements. We consider two settings with 3 and 5 different variables.

Processing of each statement in parallel will not work because conditional statements cannot be executed without knowing the current variable value, which itself can depend on another conditional. As shown \tab{toy_tasks}, Transformers cannot solve this task because every time a variable increment or decrement, its value can only be found one layer up in the model, and eventually will be lost. 
Doubling their layers from 4 to 8 does help little, bringing the accuracy to 47.4\% on the 3 variable version and 29.1\% on the 5 variable version, but their performance is far from perfect.
A recurrent model like LSTM is capable of storing a variable value while updating it, thus perform well on the 3 variables version with an accuracy of 82.8\%. However, its performance drop to 32.1\% when there are more variables because it has to store all their values in a single vector. The Feedback Transformer does not have this bottleneck, and can access updated variable values from the lowest layer, so it gives strong performance on this task. 


\subsection{Advantages of Feedback Architecture}

We examined two limitations of standard Transformers that we improve upon: limited memory span and limited ability to update state. In the Feedback model, we improve on these limitations and now analyze performance on practical tasks including translation and reinforcement learning. 


\subsubsection{Strong Performance with Small, Shallow models}

The Feedback Transformer is able to create higher level, more abstract representations with fewer layers and less capacity, as a layer can use all of the most recently created representations of previous timesteps. We demonstrate on neural machine translation that the Feedback model performs much better than Transformers at small, shallow sizes. Note that for sequence to sequence, we use Feedback Transformers only in the decoder because the encoder inputs are available simultaneously. 

We evaluate the performance of the Feedback Transformer on the \texttt{WMT14 En-De} machine translation benchmark of 4.5 million pairs.
We follow \cite{vaswani2017attention} and train on \texttt{WMT16} using \texttt{newstest2013} as dev and \texttt{newstest2014} as test. 
We learn 32K joint byte pair encodings \citep{sennrich2015neural}, generate with beam size 5, tuning a length penalty on the dev set.
We average the last 10 checkpoints and apply compound splitting and compute tokenized BLEU.

In~\fig{seq2seq_depth} (left), we display results when making the model \textit{shallower only} --- layers are removed from a Feedback Transformer decoder compared to Transformers. 
As the decoder becomes shallow, the gap in performance between the two architectures widens. 
While the 1-layer Transformer model can only reach 27.3, the Feedback Transformer has 28.3 BLEU. Shallow decoders are critical to fast inference --- reducing to 1-layer improves decoding speed by 4.2x, while only losing 1 BLEU with the Feedback architecture. Such results are useful for practical applications, where the speed of producing a translation is very important.
We report decoding speed in tokens per second on 1 GPU. 

We further experiment with large encoder but shallow decoders. The Feedback Transformer achieves \textbf{29.0} BLEU with 12 layer encoder and 2 layer decoder. As the encoder is parallelized even during inference, the increased size of the encoder has negligible impact on decoding speed. To stabilize the training of deeper models, we use LayerDrop~\citep{fan2019reducing}.

\subsubsection{Long Memory Tracks State}



We apply Feedback to a reinforcement learning maze task that requires long memory to optimally solve because agents have limited vision.
Note that in such reinforcement learning tasks, the models are trained online using A2C, so the input must be processed sequentially even during training time. Thus, the non-parallelized nature of the Feedback Transformer is not a drawback, and training Feedback Transformers is as fast as Transformers. 

The goal is to navigate a procedurally generated random maze where colored objects are placed. 
One of the colors will be randomly selected as a target, and the agent has to reach it for a reward and a new target.
For optimal performance, the agent must remember the maze and object locations.
In addition, the agent has turn actions like the Random Walk task, which makes it necessary to keep track of its location and orientation.
As shown in Figure~\ref{fig:seq2seq_depth} (right), the Feedback Transformer converges to reach higher average reward, compared to Transformers. Results are shown averaged over 10 trials.  


\begin{figure*}
	\centering
	\includegraphics[width=.44\linewidth]{figs/wmt_depth.pdf}
	\hfill
	 \includegraphics[width=.45\linewidth]{figs/maze_goto_new.pdf}
	\caption{\textbf{(left)} \textbf{Machine Translation on \texttt{WMT14 En-De},} test set BLEU and decoding speed in words-per-second for varying decoder depths. \textbf{(right) Maze Navigation in Gridworld.} We display average reward comparing Feedback Transformer to standard Transformers.}
	\label{fig:seq2seq_depth}
\end{figure*}



\subsection{Comparison to Other Architectures}

In this section, we first, we compare Feedback to recurrent architectures such as LSTM, as well as hybrid RNN-Transformer architectures, and show that the Feedback is more powerful than recurrence alone. Next, we compare our construction of the Feedback Memory with other possible compositions. Lastly, we compare to other Transformer architectures on competitive benchmarks.

\subsubsection{Comparison to Recurrent Architectures}

\begin{table}
	\begin{minipage}{.45\textwidth}
	\setlength{\tabcolsep}{4pt}
	    \centering
	    \begin{tabular}{l@{}c}
	    \toprule
	    Model & Test\\
	    \midrule
	    \textbf{Recurrent Architectures} & \\ 
	    DenseNMT~\small{\cite{shen2018dense}} & 25.5 \\ 
	    RNMT+~\small{\citep{chen2018best}} & 28.5 \\
	    \midrule 
	    \textbf{Hybrid Architectures} & \\ 
	    BiARN~\small{\citep{hao2019modeling}}  & 28.9 \\
	    SRU~\small{\citep{lei2017simple}} & 28.4 \\ 
	    \midrule 
	    \textbf{Transformer Architectures} & \\ 
	    Transformer~\small{\citep{vaswani2017attention}} & 28.4 \\ 
	    Transformer~\small{\citep{ott2018scaling}} & 29.3 \\
	    Feedback Transformer & 29.5 \\
	    \bottomrule
	    \end{tabular}
	    \caption{
	        \textbf{Results on} \texttt{WMT En-De} comparing the Feedback Transformer to Recurrent architectures, hybrid Recurrent-Transformer models, and standard Transformers.
		}
	    \label{tab:lstm_comparison}
	\end{minipage}
	\hfill
	\begin{minipage}{.50\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figs/ptb_abl.pdf}
		\captionof{figure}{Comparison of different memory composition strategies on \texttt{char-PTB}. The recurrent connection alone is not as effective as feedback connections from a higher layer.} 
	    \label{fig:ptb_ablation}
	\end{minipage}
\end{table}

We compare the Feedback Transformer architecture to recurrent architectures like LSTMs as well as hybrid RNN-Transformer architectures. In Table~\ref{tab:lstm_comparison}, we display that the Feedback Transformer has stronger performance than the Transformer, RNN, and RNN-Transformer hybrid model. We note that recurrent models address some limitations of Transformer architectures, but the Feedback mechanism goes beyond that. By allowing all past representations to be immediately available for the computation of future representations, Feedback is stronger than Recurrence alone --- Recurrent models can only see representations from the previous layer (as depicted in Table~\ref{tab:lstm_comparison}).

\subsubsection{Memory Composition}
\label{sec:ptb_abl}

We next investigate the importance of the specific memory mechanism of the Feedback architecture on \texttt{char-PTB}. The Feedback architecture uses all layers when creating the memory, motivated by providing access to the entire past of all computations, but other ways of creating the memory as possible. For example, Recurrent architectures have a different memory structure. In multi-layer RNNs, each layer has recurrent connections to the same layer, but not to higher layers. This is an advantage of Feedback architectures --- even the highest level abstractions are immediately available. 

In Figure~\ref{fig:ptb_ablation}, we examine the construction of the Feedback memory, comparing our choice of making \textit{all} computation accessible with \textit{recurrent} memory that can access all previous layers plus the same layer, and \textit{top-only} memory that can attend only to the topmost layer. The Feedback Transformer has the best performance, closely matched by \textit{top-only} memory. This indicates the importance of high level representations (see \app{lm_abl} for further analysis on this). Note that recurrence alone is not enough for good performance, and thus the Feedback memory provides richer representations beyond the capacity of recurrent networks. 

\subsubsection{Comparison to Other Transformer Architectures}

We examine the performance of Feedback Transformer on long context language modeling benchmarks. We use caching~\citep{dai2019transformer} and relative position embeddings. 
Mechanisms applied at inference time~\citep{khandelwal2019generalization,krause2019dynamic} can further improve all models, so we do not focus on these.

\paragraph{Wikitext-103.}

We evaluate on word-level language modeling on \texttt{Wikitext-103}~\citep{merity2017regularizing}. Our Feedback architecture takes 3.5 days to train, compared to the Transformer which takes 1.2 days. 
We train a small Feedback model, about half the size of Transformer-XL, and find that it can match the performance of Transformer-XL, as shown in Table~\ref{tab:wikitext103}. This indicates the additional representational capacity of Feedback memory. 
If we train a standard Transformer that is approximately the same size as our Feedback Transformer, we find it has worse performance (19.9 PPL rather than 18.3). 
Further, mechanisms like the Routing Transformer can be added to the Feedback Transformer as well. We focus on starting with Transformer-XL as a baseline and showing we can match the performance with a much smaller model.


\paragraph{Enwiki8.}
Finally, we test our model on  character-level language modeling in \texttt{Enwiki8}~\citep{mahoney2011large}, containing 100M unprocessed bytes from Wikipedia.
We train a relatively small 12-layer model, that is one third of the size of the Transformer-XL baseline.
Since the task requires very long context, we use adaptive attention span~\citep{sukhbaatar2019adaptive}.
As shown in \tab{enwik8}, the Feedback Transformer model achieves a new SOTA performance of  bit-per-byte despite its small size. 

\begin{table}
	\begin{minipage}[t]{.48\textwidth}
	\centering
	    \begin{tabular}{l@{}cc}
	    \toprule
	    Model & Params & Test\\
	    \midrule
	    Best Existing~\footnotesize{\citep{roy2020efficient}} & --- & 15.8 \\ 
	    Trans-XL ~\footnotesize{\citep{dai2019transformer}} & 257M & 18.3 \\ 
	    \midrule 
	    Our Transformer & 140M  & 19.9 \\
	    Feedback Transformer & 126M  & 18.3 \\ 
	    \bottomrule
	    \end{tabular}
	    \caption{
	        \textbf{Results on \texttt{WikiText-103}.}
	        We report perplexity on test.
		}
	    \label{tab:wikitext103}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{.48\textwidth}
		\setlength{\tabcolsep}{2.8pt}
		    \centering
		    \begin{tabular}{lcc}
		    \toprule
		    Model & Params & Test\\
		    \midrule
		    Best Existing~\footnotesize{\citep{rae2020compressive}} & 277M & 0.97 \\
		    Trans-XL ~\footnotesize{\citep{dai2019transformer}} & 277M  & 0.99 \\
		    \midrule 
		    Feedback Transformer & 77M  & 0.96 \\  
		    \bottomrule
		    \end{tabular}
		    \caption{
		    \textbf{Results on \texttt{Enwiki8}.} We report bit-per-byte on test.
		    }
		    \label{tab:enwik8}
	\end{minipage}
\end{table}



\subsubsection{Training and Inference Speed}

Finally, we compare the training and inference speed of the Feedback Transformer with standard Transformer architectures. Results are shown in Table~\ref{tab:speed}. The Feedback Transformer has faster inference, because the key-value projection sharing substantially reduces the memory footprint of the model and reduces computation. Further, shallow Feedback models perform well, so the batch size can be increased. In language modeling, for example, sharing key-value provides almost 3X inference speed improvement. The shallow model size provides the remaining 10\% of speed improvement at inference time. Finally, note that for certain problems (such as in RL), the data must be processed strictly sequentially anyway and Feedback Transformer is not any slower.
\begin{table*}
\centering
		    \begin{tabular}{lllcc}
		    \toprule
		    Task & Model && Training Speed & Inference Speed \\
		    \midrule
		    \multirow{2}{*}{Language Modeling} & Transformer && 296K & 592 \\ 
		    & Feedback Transformer && 84.4K & 2176 \\ 
		    \midrule 
		    \multirow{2}{*}{Translation} & Transformer && 280K & 3190 \\ 
		    & Feedback Transformer && 126K & 5410 \\ 		    
	       	\midrule
		    \multirow{2}{*}{Reinforcement Learning} & Transformer &&22.3K & --- \\ 
		    & Feedback Transformer && 22.3K & ---\\ 
		    \bottomrule
		    \end{tabular}
		    \caption{
		    \textbf{Results comparing Training and Inference Speed} for three different tasks. For language modeling, we measure words-per-second on Wikitext-103 fixing model size and attention span. For translation, we measure words-per-second on WMT En-De, both models with a 6 layer encoder and 2 layer decoder. For RL, we measure the training frame-per-second on maze navigation (with 20 CPU cores and 1 GPU). All inference speed is reported on 1 GPU. 
		    }
		    \label{tab:speed}
\end{table*}


 
\section{Conclusion}
We propose a novel reformulation of the Transformer that fully exploits sequential input --- the increased representation power and recursive computation of the Feedback Transformer allows shallow and small models to have much stronger performance compared to a Transformer of the same size. This architecture addresses two fundamental limitations of Transformers as an autoregressive model --- limited access to long memory and limited ability to update state. We demonstrate on a variety of tasks the advantages of the Feedback architecture to illustrate the strong performance of this straightforward modification.


\bibliographystyle{iclr2021_conference}
\bibliography{egbib}

\clearpage 
\section{Additional Results}

\subsection{Reinforcement Learning}
\paragraph{Maze Navigation Easy.}
We experiment with a slightly different version of the Maze Navigation task. Instead of an agent with forward, turn-left and turn-right actions, the agent has no orientation and there are only 4 movement actions corresponding to 4 cardinal directions. 
This makes navigation easier because the agent do not need to keep track of its orientation.
Further, it is much easier to compute relative locations given a history of actions.
This might explain why standard Transformers are not far behind Feedback Transformers in performance as shown in Figure~\ref{fig:water_maze} (left). We also compare to LSTMs, which performs much worse. See Section~\ref{sec:maze_detail} for more implementation details.

\paragraph{Water Maze.}
We modify the Morris Water Maze task~\citep{morris1981spatial} to make it more challenging. The maze is defined by a goal position and a mapping of cell to ID --- these remain fixed within an episode but change between episodes. The agent receives as an observation the cell IDs of its current location and the target cell.  When the agent finds the target, it receives +1 reward and is randomly teleported. During the same episode, if the agent reaches a previously seen cell, it needs to remember how it reached the target from there to go back. 
Results are shown averaged over 10 trials (the reward is reported averaged over the last 500 episodes for each trial).  As shown in Figure~\ref{fig:water_maze} (right), the Feedback Transformer converges to higher average reward. 

\begin{figure*}[h]
    \centering
    \includegraphics[width=.43\linewidth]{figs/maze_goto.pdf}
    \hspace{0.7cm}
    \includegraphics[width=.41\linewidth]{figs/water_maze.pdf}
    \caption{Averaged cumulative reward during training on \textbf{(left)} \textbf{Maze Navigation Easy} and \textbf{(right)} \textbf{Water Maze} tasks.}
    \label{fig:water_maze}
\end{figure*}
 
\subsection{\texttt{IWSLT De-En}}

We additionally evaluate the Feedback Transformer on \texttt{IWSLT De-En}, a small machine translation dataset. We train a small Transformer model with 6 layers. For generation, we use beam size 5 without checkpoint averaging. Model quality is evaluated using tokenized BLEU. Results are shown in Figure~\ref{fig:summ_depth} (left) and show that for shallower models, the Feedback Transformer has better performance than the standard Transformer.

\subsection{Summarization on CNN-Dailymail}

We evaluate on the CNN-Dailymail multi-sentence summarization benchmark of 280K news articles \cite{hermann15}, modeling the first 400 words of the article \cite{see2017get}. 
We evaluate using ROUGE \cite{lin2004rouge}. and use 3-gram blocking and tune length \cite{fan2017controllable}. 
Figure~\ref{fig:summ_depth} (right) displays the performance of the Feedback Transformer as the decoder layers are reduced, making the model \textit{shallower only}. 
For all model depths, the Feedback architecture maintains a consistent improvement in ROUGE compared to the standard Transformer. 
Compared to sentence-level tasks such as translation, this summarization benchmark requires multi-sentence generation, and the increased capacity of the Feedback architecture is beneficial.

\begin{figure}
	\centering
    \includegraphics[width=.45\linewidth,trim=0 10 0 -20,clip]{figs/iwslt_depth.pdf}
    \hspace{0.5cm}
	\includegraphics[width=.45\textwidth,trim=0 10 0 -20,clip]{figs/summ_depth}
    \caption{
    Results on \textbf{(left) the \texttt{IWSLT De-En} dataset,} and
    \textbf{(right) Summarization on \texttt{CNN-Dailymail},} test set ROUGE-L for varying decoder depths.}
	\label{fig:summ_depth}
\end{figure}

\subsection{Ablation Studies on Language Models}
\label{app:lm_abl}

We investigate which layer of a model has the best representation to be used as a Feedback memory.
In Feedback Transformers, a weighted sum of all layers is used as the memory, and feeds to all layers.
An alternative approach is to manually select one of the layers as the memory and let all layers attend to it. 
In \fig{ptb_layer}, we explore this approach, using the same 6-layer \texttt{char-PTB} models as \secc{ptb_abl} (\textit{top-only} memory there corresponds to using the last 6th layer as memory).
We can see that representations from higher layers work better as memory, confirming our assumption of the importance of higher level representations. 
Simply averaging all layers together works reasonably well as well. 
Interestingly, when all layer attend to the first layer output, it works as good as the standard Transformer. 
The weighted sum approach matches the best performance because it can adopt to select any of the layers. 

\begin{figure}
    \centering
    \includegraphics[width=.45\textwidth]{figs/ptb_layer.pdf}
    \caption{Ablation results on \texttt{char-PTB}: instead of a weighted sum of all layers as Feedback memory, only a single layer is used as memory for all layers. We also include a setting where the average of all layers is used.}
    \label{fig:ptb_layer}
\end{figure}

Here we study how different techniques affect the model performance on \texttt{WikiText-103}.
The results shown in \tab{wiki_abbl} indicate: 
\begin{itemize}
    \item Pre-normalization combined with higher learning rates helps the performance, particularly for the standard Transformer.
    \item Increasing the context size with adaptive span further improves the performance for both models.
    \item The technique of increasing the BPTT length during training for efficiency does not affect the final performance.
    \item The gap between two model is consistent along those variations.
\end{itemize}


\begin{table}[t]
    \centering
    \begin{tabular}{lcccc}
    \toprule
    Model & Pre-norm + & Adapt. & Increase & dev \\
     & higher LR & span & BPTT & ppl \\
    \midrule
    Transformer  & no & no & no & 22.9 \\
    Transformer  & no & no & yes & 22.9 \\
    Transformer  & yes & no & yes & 21.0 \\
    Transformer  & yes & yes & no & 20.6 \\
    Feedback  & no & no & no & 19.7 \\
    Feedback  & no & no & yes & 19.9 \\
    Feedback  & yes & no & yes & 19.6 \\
    Feedback  & yes & yes & yes & 19.0 \\
    \bottomrule
    \end{tabular}
    \caption{
    Ablation on \texttt{WikiText-103} of various modeing choices. Results are shown without finetuning.   }
    \label{tab:wiki_abbl}
\end{table}

Next, we examine the effect of the model depth on performance on \texttt{char-PTB} and \texttt{WikiText-103}
This time, we keep the total number of parameters constant and only vary the number of layers to isolate the effect of depth. 
This is achieved by proportionally increasing the head dimension and the ReLU layer size when we decrease the number of layers.
The results in \fig{wiki_depth} demonstrate that for the standard Transformer improves as the depth increase.
In contrast, the Feedback architecture is much robust reduced depth, even achieving the best performance on \texttt{char-PTB} with only two layers.

\begin{figure*}[t]
   \centering
   \begin{tabular}{cc}
   \includegraphics[width=.45\linewidth]{figs/ptb_depth.pdf}&
   \includegraphics[width=.45\linewidth]{figs/wiki103_depth.pdf}
   \end{tabular}
   \vskip -4mm
   \caption{The performance on \textbf{(left)} \texttt{char-PTB} and \textbf{(right)}  \texttt{Wikitext-103} as a function of the model depth. The number of parameters is kept constant by increasing the width.}
   \label{fig:wiki_depth}
\end{figure*}

\section{Additional Implementation Details}

\subsection{Random Walk Task Details}
We provide additional details for the random walk toy task we explore. 
The agent starts at a fixed position of a  grid.
Available actions are 1) move one step forward, 2) turn left and 3) turn right.
At every time step, the agent randomly picks on of the three actions and executes it. An action would be ignored if it can't be executed like going out of the grid. After 100 actions, the agent is reset back to the initial position.

The input to the model is a sequence of actions taken by the agent, and a special symbol if there was a reset. The output is a sequence of location symbols corresponding to the agent's location after each action. We generate 10k training episodes, totalling 1M tokens.

We use the same setup as our language modeling experiments, except now the model predicts separate output tokens rather than a next token.
We concatenate all the episodes and feed them to the model as a single sequence.
The training is done with the negative-log-likelihood loss. 
See \tab{hyper_lm} for the hyperparameters used in the experiment.
The attention span is set to 100, so that the models can attend to all the information they needs to solve the task. 



\subsection{Maze Navigation Details}
\label{sec:maze_detail}

\begin{figure*}
    \centering
    \includegraphics[width=.25\linewidth, valign=t, trim={0 0 0 -0.1cm}, clip]{figs/maze_goto2.pdf}
    \hspace{1cm}
    \includegraphics[width=.25\linewidth, valign=t, trim={0 0 0 -0.1cm}, clip]{figs/water_maze_v2.png}
    \caption{\textbf{(left)} \textbf{Maze Navigation} task and \textbf{(right)}  \textbf{Water Maze} task.
 }
    \label{fig:maze_goto}
\end{figure*}
 
We generate random  mazes using Kruskal's algorithm.
Dead ends are eliminated by randomly removing one of the blocks surrounding them. 
We  randomly place 8 target objects with different colors as shown in \fig{maze_goto} (left).
The agent is given a randomly selected color as a target. 
If the agent manages to reach the correct target, it gets a reward of  and a new target color is sampled. An episode ends after 200 steps. The observation includes the  area around the agent and target color.

We train -layer Transformers with a hidden size  and  heads.  
We set the BPTT to  and the batch size to . The reward discount rate is . The attention span is 200 so the agent can keep an entire episode in memory.
All agents were trained using A2C with Adam with a learning rate of  and a entropy cost of . 
For the easy version of the task, we use RMSprop with a batch size of  and a learning rate of . The RMSProp epsilon regularization parameter is set to 
The LSTM model is a -layer LSTM with a hidden size of .

\subsection{Water Maze Details}

The water maze task we designed is depicted visually in \fig{maze_goto} (right).
The grid size is . To help exploration, the agent can see if the goal is within a  area around it. An episode ends after 200 steps.
We train for M steps (M episodes).  We use -layer Transformers with hidden size of  and  head. The attention span is  so the agent can put an entire episode in memory. 

All agents where trained using A2C with RMSprop with entropy cost of , RMSProp epsilon regularisation parameter of , batch size of , and BPTT . Feedback Transformer and Transformer baseline were trained with a learning rate of . LSTM model is a -layer LSTM with hidden size of . For LSTM model we used a learning rate of . 

\subsection{Algorithmic Task Details}

\begin{table}
    \centering
    \begin{tabular}{|c|}
        \hline 
        \begin{minipage}{\textwidth}
            \scriptsize
            \begin{spverbatim}x = 1 ; print x ; x ++ ; print x ; z = 8 ; print z ; print z ; x -- ; if x > z : z -- ; z ++ ; print z ; print x ; print x ; if z < x : z ++ ; x ++ ; z -- ; x -- ; if z > x : z -- ; z ++ ; if x > z : z ++ ; if z < 5 : y = 7 ; print x ; if x > z : z ++ ; x ++ ; y = 7 ; if x > 10 : x -- ; y -- ; x ++ ; z ++ ; print z ; y -- ; print x ; print x ; z ++ ; y ++ ; y ++ ; if z < 3 : y ++ ; if x > 4 : x ++ ; z -- ; x -- ; x -- ; print x ; y ++ ; z ++ ; y -- ; if x > z : z -- ; x ++ ; z -- ; print x ; z ++ ; print y ; y ++ ; y -- ; x -- ; print x ; y ++ ; print y ; y -- ; if z < x : x ++ ; if z > 4 : y -- ; z -- ; x ++ ; if y < x : y ++ ; print y ; print z ; z -- ; y -- ; x ++ ; y -- ; y ++ ; if y > 3 : z -- ; y ++ ; if z < 10 : z ++ ; z ++ ; y -- ; z ++ ; print z ; x -- ; y -- ; x -- ; x ++ ; if x < 4 : y -- ; print y ; print z ; if z > x : y -- ; print z ; if y < x : x -- ; print x ; print z ; if x < 4 : z -- ; if z < y : z ++ ; z -- ; x -- ; print x ; if z < x : y ++ ; print x ; print z ; y -- ; if z < 6 : x ++ ; z -- ; END \end{spverbatim}
        \end{minipage} 
        \\
        \hline
    \end{tabular}
    \caption{An example program from the algorithmic task with 3 variables.}
    \label{tab:algo_example}
\end{table}

In this task, each program consists of 100 simple statements that should be sequentially executed. The available statement types are:
\begin{enumerate}
    \item \textbf{Initialization.} Assign an initial value to a variable like \verb. A variable can only be initialized once in each program.
    \item \textbf{Increment and decrement.} Increment or decrement a variable value by 1, like \verb or \verb.
    \item \textbf{Print.} Output the value of a certain variable like \verb. Only this statement requires model to make a prediction. 
    \item \textbf{Conditional.} Execute the nested statement only if a variable has a certain value, e.g., \verb. Note that conditional and print statements cannot be nested.
\end{enumerate}
A program is generated by randomly choosing a statement one after another, but with the following conditions: a variable must be initialized before being used, and a variable value have to between 1 and 10. 
The training data contains 10k such programs concatenated with a special separator keyword.
We generate two version the data with 3 and 5 different variables in them. 
An example program is shown in \tab{algo_example}.
We used the same hyperparameters as the random walk task as show in \tab{hyper_lm}.

\subsection{Machine Translation and Summarization}

We detail the hyperparameters in Table~\ref{tab:hyper_s2s}. Summarization experiments are done with the Transformer base architecture size and WMT En-De experiments are done with the Transformer big architecture size. As IWSLT De-En is a smaller dataset, we use a smaller model. For all sequence to sequence experiments, only the decoder is modified to have the Feedback Transformer architecture. 

\begin{table*}[t]
    \centering
    \begin{tabular}{lccc}
    \toprule
    Hyperparameter & Summarization & WMT En-De & IWSLT De-En  \\
    \midrule
    Encoder Layers & 6 & 6 & 6 \\ 
    Decoder Layers & 6 & 6 & 6 \\ 
    FFN Size & 2048 & 4096 & 1024 \\ 
    Attention Heads & 8 & 16 & 4 \\
    Dropout & 0.3 & 0.3 & 0.3 \\ 
    Hidden Size & 512 & 1024 & 512 \\  
    Learning Rate & 0.0005 & 0.001 & 0.0005 \\ 
    \bottomrule
    \end{tabular}
    \caption{Hyperparamers for sequence to sequence experiments.}
    \label{tab:hyper_s2s}
\end{table*}

\subsection{Language modeling}

\begin{table*}[t]
    \centering
    \begin{tabular}{lccccc}
    \toprule
    Hyperparameter & Random Walk & char-PTB & Enwik8 & WikiText-103 & WikiText-103 \\
    & Algorithmic & & & small & large \\
    \midrule
    Layers              & 4     & 6     & 12    & 4     & 8 \\
    Hidden size ()   & 256   & 384   & 512   & 512   & 1024 \\
    FF size             &   &   &   &   &  \\
    Head count ()    & 4     & 4     & 8     & 8     & 8 \\
    Head dim            &  &  & & &  \\
    Attention span      & 100   & 512   & 8192*   & 512 & 512, 2048* \\
    Dropout rate        & 0.2   & 0.5   & 0.5   & 0.1   & 0.3 \\
    Embed. dropout      & -     & -     & -     & 0.1   & 0.2 \\
    BPTT len ()      & 64    & 128   & 128   & 256   & 256 \\
    Batch size ()    & 512   & 2048  & 1024  & 512   & 512 \\ 
    Learning rate       & 0.0001& 0.0015& 0.0015& 0.0007& 0.0007 \\
    Gradient clip       & 0.1   & 1.0   & 0.1   & 0.1   & 0.1 \\
    LR warm-up steps    & 1k    & 1k    & 8k    & 8k    & 8k \\
    \midrule
    Parameters          & 3.2M  & 10.7M & 77M   & 44M   & 139M \\
    \bottomrule
    \end{tabular}
    \caption{Hyperparamers for language modeling experiments. Here * indicates the adaptive span.}
    \label{tab:hyper_lm}
\end{table*}

In the language modeling experiments, we added several improvements on top of the original Transformer~\cite{vaswani2017attention} to better adapt to unbounded sequences:
\begin{itemize}
    \setlength\itemsep{-0.3em}
  \item \textbf{Hidden representation caching~\cite{dai2019transformer}:} Since the input to the model is an unbounded sequence and the model needs to process it in small blocks, hidden representations from previous blocks are kept in cache so that any token in the current block will the same context length regardless of its position in the block.
  \item \textbf{Relative position embedding~\cite{shaw2018self}:} Relative position embeddings allow each token in a block to be processed in the same way regardless of its absolute position in the block. We found that adding shared embeddings to key vectors at every layer to be effective.
  \item \textbf{Adaptive attention span~\cite{sukhbaatar2019adaptive}}
  Language modeling requires a model to have a very long attention span, which is computationally expensive. The adaptive span mechanism allows each attention head to learn different attention spans for efficiency.
  \item \textbf{Pre-normalization~\cite{child2019generating}}: We observed that pre-normalization makes training more stable for Transformers, which allowed us to use larger batch sizes for better parallelization.
\end{itemize}

Dropouts are applied to attention and ReLU activations. In \texttt{WikiText-103} models, additional dropouts are added to the embedding layer output and the last sublayer output.

In Table~\ref{tab:hyper_lm}, we present the hyperparameter values used for our experiments. We use the same hyperparameters for both Transformers and Feedback Transformers, and optimize them with Adam.
The final performances are obtained by finetuning the models with a 10x smaller learning rate.

\paragraph{Details on the \texttt{char-PTB} experiments}
We trained the models for 15k updates (or earlier if the validation loss stops decreasing), and funetined them for 1k steps.
We varied the depth of the models while keeping the number of parameters constant. This is achieved by changing the FF size and the head dimension inverse proportionally to the depth.

\paragraph{Details on the \texttt{enwik8} experiments}
We used an adaptive span limited to 8192 tokens with a loss of 0.0000005. The training is done for 100k updates and another 10k steps is used for finetuning.
The warming up BPTT length is used for speeding up the training, where the BPTT length is decreased to 64 for the first half of the training.

\paragraph{Details for Training on \texttt{WikiText-103}}
We employed the adaptive input~\cite{baevski2018adaptive} and the adaptive softmax~\cite{grave2017efficient} techniques for reducing the number of parameters within word embeddings.
The models are trained for 200k steps and the finetuned for additional 10k steps. 


While most of the models have a fixed attention span of 512, the best performance is achieved by extending the attention span to 2048 with adaptive span loss 0.00001.

After training our models, we noticed that our tokenization method differed from others by omitting end-of-line (EOL) symbols. Since our dictionary already contained the EOL token, we were able finetune our trained models on the data with EOL tokens, rather than training them from scratch. This change alone brought about 1ppl improvement.
 

\end{document}
