

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2018}
\usepackage{times}
\usepackage{latexsym}
\usepackage{bm}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{booktabs}
\usepackage{url}


\usepackage{xcolor}
\newcommand\todo[1]{\textcolor{red}{#1}}

\aclfinalcopy \def\aclpaperid{1465} 



\newcommand\BibTeX{B{\sc ib}\TeX}

\title{Enhancing Drug-Drug Interaction Extraction from Texts by \\Molecular Structure Information}

\author{Masaki Asada{\rm{,}} Makoto Miwa \and Yutaka Sasaki \\
  Computational Intelligence Laboratory \\
  Toyota Technological Institute \\
  2-12-1 Hisakata, Tempaku-ku, Nagoya, Aichi, 468-8511, Japan \\
  {\tt \{sd17402, makoto-miwa, yutaka.sasaki\}@toyota-ti.ac.jp} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
We propose a novel neural method to extract drug-drug interactions (DDIs) from texts using external drug molecular structure information. We encode textual drug pairs with convolutional neural networks and their molecular pairs with graph convolutional networks (GCNs), and then we concatenate the outputs of these two networks. In the experiments, we show that GCNs can predict DDIs from the molecular structures of drugs in high accuracy and the molecular information can enhance text-based DDI extraction by 2.39 percent points in the F-score on the DDIExtraction 2013 shared task data set. 
\end{abstract}

\section{Introduction}

When drugs are concomitantly administered to a patient, the effects of the drugs may be enhanced or weakened, which may also cause side effects. These kinds of interactions are called Drug-Drug Interactions (DDIs). Several drug databases have been maintained to summarize drug and DDI information such as DrugBank~\cite{drugbank}, Therapeutic Target database~\cite{yang2016therapeutic}, 
and PharmGKB~\cite{thorn2013pharmgkb}. 
Automatic DDI extraction from texts is expected to support the maintenance of databases with high coverage and quick update to help medical experts. Deep neural network-based methods have recently drawn a considerable attention~\cite{liu2016drug,sahu2017drug,Zheng2017,lim2018drug} since they show state-of-the-art performance without manual feature engineering. 

In parallel to the progress in DDI extraction from texts, Graph Convolutional Networks (GCNs) have been proposed and applied to estimate physical and chemical properties of molecular graphs such as solubility and toxicity~\cite{NIPS2015_5954,li2015gated,gilmer2017neural}. 

In this study, we propose a novel method to utilize both textual and molecular information for DDI extraction from texts. We illustrate the overview of the proposed model in Figure~\ref{figure:model}. 
We obtain the representations of drug pairs in molecular graph structures using GCNs and concatenate the representations with the representations of the textual mention pairs obtained by convolutional neural networks (CNNs). 
We trained the molecule-based model using interacting pairs mentioned in the DrugBank database and then trained the entire model using the labeled pairs in the text data set of the DDIExtraction 2013 shared task (SemEval-2013 Task 9)~\cite{segura2013semeval}.   
In the experiment, we show GCNs can predict DDIs from molecular graphs in a high accuracy. 
We also show molecular information can enhance the performance of DDI extraction from texts in 2.39 percent points in F-score. 

The contribution of this paper is three-fold: 
\begin{itemize}[nolistsep]
\item{We propose a novel neural method to extract DDIs from texts with the related molecular structure information.} 
\item{We apply GCNs to pairwise drug molecules for the first time and show GCNs can predict DDIs between drug molecular structures in a high accuracy.} 
\item{We show the molecular information is useful in extracting DDIs from texts.}
\end{itemize}

\section{Methods}
\begin{figure*}[t]
  \centering
  \includegraphics[width=.8\linewidth]{acl_model3.png}
  \caption{Overview of the proposed model}\label{figure:model}
\end{figure*} 

\subsection{Text-based DDI Extraction}

Our model for extracting DDIs from texts is based on the CNN model by \newcite{zeng2014relation}.
When an input sentence  is given, 
We prepare word embedding  of  and word position embeddings  and  that correspond to the relative positions from the first and second target entities, respectively. 
We concatenate these embeddings as in Equation~(\ref{eq:10}), and we use the resulting vector as the input to the subsequent convolution layer:

where  denotes the concatenation.
We calculate the expression for each filter  with the window size .

where  is the number of windows,  and  are the weight and bias of CNN, and  indicates max pooling~\cite{maxpooling}.

We convert the output of the convolution layer into a fixed-size vector that represents a textual pair as follows:

where  is the number of filters. 

We get a prediction  by the following fully connected neural networks: 

where  and  are weights and  and  are bias terms.

\subsection{Molecular Structure-based DDI Classification}

We represent drug pairs in molecular graph structures using two GCN methods: CNNs for fingerprints (NFP)~\cite{NIPS2015_5954} and Gated Graph Neural Networks (GGNN)~\cite{li2015gated}. They both convert a drug molecule graph  into a fixed size vector  by aggregating the representation  of an atom node  in . 
We represent atoms as nodes and bonds as edges in the graph.

\textbf{NFP} first obtains the representation  by the following equations~\cite{NIPS2015_5954}.

where  is the representation of  in the -th step,  is the neighbors of , 
and  is a weight parameter.
 is initialized by the \textit{atom features} of .  is the degree of a node  and  is a sigmoid function.
NFP then acquires the representation of the graph structure 

where  is a weight matrix.

\textbf{GGNN} first obtains the representation  by using Gated Recurrent Unit (GRU)-based recurrent neural networks~\citep{li2015gated} as follows:

where  is a weight for the \textit{bond type} of each edge .
GGNN then acquires the representation of the graph structure.

where  and  are linear layers and  is the element-wise product.

We obtain the representation of a molecular pair by concatenating the molecular graph representations of drugs  and , i.e., . 

We get a prediction  as follows:

where  and  are weights and  and  are bias terms.

\subsection{DDI Extraction from Texts Using Molecular Structures}

We realize the simultaneous use of textual and molecular information by concatenating a text-based and molecule-based vectors: 
. 
We normalize molecule-based vectors. We then use  instead of  in Equation~\ref{eq:y}. 

In training, we first train the molecular-based DDI classification model.
The molecular-based classification is performed by minimizing the loss function 
.
We then fix the parameters for GCNs and train text-based DDI extraction model by minimizing the loss function 
.


\section{Experimental Settings}
In this section, we explain the textual and molecular data and task settings and training settings.

\subsection{Text Corpus and Task Setting}



We followed the task setting of Task 9.2 in the DDIExtraction 2013 shared task~\cite{segura2013semeval,herrero2013ddi} for the evaluation. 
This data set is composed of documents annotated with drug mentions and their four types of interactions: \textsl{Mechanism}, \textsl{Effect}, \textsl{Advice} and \textsl{Int}. For the data statistics, please refer to the supplementary materials.



The task is a multi-class classification task, i.e., to classify a given pair of drugs into the four interaction types or no interaction. 
We evaluated the performance with micro-averaged precision (P), recall (R), and F-score (F) on all the interaction types. We used the official evaluation script provided by the task organizers.

As preprocessing, we split sentences into words using the GENIA tagger~\cite{geniatagger}. 
We replaced the drug mentions of the target pair with \textsl{DRUG1} and \textsl{DRUG2} according to their order of appearance.   
We also replaced other drug mentions with \textsl{DRUGOTHER}.
We did not employ negative instance filtering unlike other existing methods, e.g., \citet{liu2016drug}, since our focus is to evaluate the effect of the molecular information on texts.

We linked mentions in texts to DrugBank entries by string matching. We lowercased the mentions and the names in the entries and chose the entries with the most overlaps. 
As a result, 92.15\% and 93.09\% of drug mentions in train and test data set matched the DrugBank entries. 


\subsection{Data and Task for Molecular Structures}
We extracted 255,229 interacting (positive) pairs from DrugBank. 
We note that, unlike text-based interactions, DrugBank only contains the information of interacting pairs; 
there are no detailed labels and no information for non-interacting (negative) pairs.
We thus generated the same number of pseudo negative pairs by randomly pairing drugs and removing those in positive pairs. 
To avoid overestimation of the performance, we also deleted drug pairs mentioned in the test set of the text corpus.
We split positive and negative pairs into 4:1 for training and test data, and we evaluated the classification accuracy using only the molecular information. 



\begin{figure}[t]
  \centering
  \includegraphics[width=1.\linewidth]{smiles_rdkit2.png}
  \caption{Associating DrugBank entries with texts and molecular graph structures}
  \label{figure:smiles}
\end{figure} 

To obtain the graph of a drug molecule, we took as input the SMILES~\cite{weininger1988smiles} string encoding of the molecule from DrugBank and then converted it into the graph using RDKit~\cite{landrum2016rdkit} as illustrated in Figure~\ref{figure:smiles}.
For the \textit{atom features}, we used randomly embedded vectors for each atoms (i.e., C, O, N, ...). We also used 4 \textit{bond types}: single, double, triple, or aromatic.

\subsection{Training Settings}

We employed mini-batch training using the Adam optimizer~\cite{kingma2014adam}. We used L2 regularization to avoid over-fitting. We tuned the bias term  for negative examples in the final softmax layer. 
For the hyper-parameters, please refer to the supplementary materials. 

We employed pre-trained word embeddings trained by using the word2vec tool~\cite{mikolov2013distributed} on the 2014 MEDLINE/PubMed baseline distribution. The vocabulary size was 215,840.
The embedding of the drugs, i.e., \textsl{DRUG1} and \textsl{DRUG2} were initialized with the pre-trained embedding of the word \textsl{drug}.
The embeddings of training words that did not appear in the pre-trained embeddings were initialized with the average of all pre-trained word embeddings. Words that appeared only once in the training data were replaced with an \textsl{UNK} word during training, and the embedding of words in the test data set that did not appear in both training and pre-trained embeddings were set to the embedding of the \textsl{UNK} word. Word position embeddings are initialized with random values drawn from a uniform distribution.

We set the molecule-based vectors of unmatched entities to zero vectors. 

\section{Results}



\begin{table}[t!]
\centering
\begin{tabular}{p{3.5cm}lll} \hline
Methods & P & R & F (\%)\\\hline
\citet{liu2016drug} & 75.29 & 60.37 & 67.01\\
\citet{Zheng2017} & 75.9 & 68.7 & 71.5 \\
  \citet{lim2018drug} & 74.4 & 69.3 & 71.7\\\hline
Text-only  & 71.97 & 68.44 & 70.16\\
  + NFP & 72.62 & 71.81 & 72.21\\
  + GGNN & 73.31 & 71.81 & 72.55\\
  \hline
  \end{tabular}
  \caption{Evaluation on DDI extraction from texts}
  \label{table:comparison}
\end{table}

\begin{table}[t!]
\centering
\begin{tabular}{lllll} \hline
DDI Type & \textsl{Mech.} & \textsl{Effect} & \textsl{Adv.} & \textsl{Int} (\%)\\\hline
  Text-only & 69.52 & 69.27 & 79.81 & 48.18 \\
  + NFP & 72.70 & 72.44 & 79.56 & 46.98 \\
  + GGNN & 73.83 & 71.03 & 81.62 & 45.83 \\
  \hline
  \end{tabular}
  \caption{Performance on individual DDI types in F-scores}
  \label{table:type}
\end{table}

\begin{table}[t!]
  \centering
  \begin{tabular}{lr} \hline
  Methods & Accuracy (\%) \\\hline
  NFP &  94.19\\
  GGNN & 98.00 \\\hline
  \end{tabular}
  \caption{Accuracy of binary classification on DrugBank pairs}
   \label{table:bin}
\end{table}
\begin{table}[t!]
\centering
\begin{tabular}{lllll} \hline
Methods & P & R  & F & Acc. (\%)\\\hline
  NFP & 15.56 & 48.93 & 23.61 & 45.78\\
  GGNN & 15.11 & 57.10 & 23.90 & 37.72\\
  \hline
  \end{tabular}
  \caption{Classification of DDIs in texts by molecular structure-based DDI classification model}
  \label{table:detection}
\end{table}


Table~\ref{table:comparison} shows the performance of DDI extraction models. We show the performance without negative instance filtering or ensemble for the fair comparison. We observe the increase of recall and F-score by using molecular information, which results in the state-of-the-art performance with GGNN. 

Both GCNs improvements were statistically significant ( for NFP and  for GGNN) with randomized shuffled test. 


Table~\ref{table:type} shows F-scores on individual DDI types. The molecular information improves F-scores especially on type \textsl{Mechanism} and \textsl{Effect}.

We also evaluated the accuracy of binary classification on DrugBank pairs by using only the molecular information in Table~\ref{table:bin}. 
The performance is high, although the accuracy is evaluated on automatically generated negative instances.

Finally, we applied the molecular-based DDI classification model trained on DrugBank to the DDIExtraction 2013 task data set. 
Since the DrugBank has no detailed labels, we mapped all four types of interactions to positive interactions and evaluated the classification performance. The results in Table~\ref{table:detection} show that GCNs produce higher recall than precision and the overall performance is low considering the high performance on DrugBank pairs. This might be because the interactions of drugs are not always mentioned in texts even if the drugs can interact with each other and because hedged DDI mentions are annotated as DDIs in the text data set. 
We also trained the DDI extraction model only with molecular information by replacing  with , but the F-scores were quite low ( 5\%). These results show that we cannot predict textual relations only with molecular information.


\section{Related Work}

Various feature-based methods have been proposed during and after the DDIExtraction-2013 shared task~\cite{segura2013semeval}.
\newcite{kim2015extracting} proposed a two-phase SVM-based approach that employed a linear SVM with rich features that consist of word, word pair, dependency graph, parse tree, and noun phrase-based constrained coordination features. 
\newcite{zheng2016graph} proposed a context vector graph kernel to exploit various types of contexts. 
\newcite{raihani2017rich} also employed a two-phase SVM-based approach using non-linear kernels and they proposed five groups of features: word, drug, pair of drug, main verb and negative sentence features.
Our model does not use any features or kernels.  

Various neural DDI extraction models have been recently proposed using CNNs and Recurrent Neural Networks (RNNs).
\newcite{liu2016drug} built a CNN-based model based on word and position embeddings. 
\newcite{Zheng2017} proposed a Bidirectional Long Short-Term Memory RNN (Bi-LSTM)-based model with an input attention mechanism, which obtained  target drug-specific word representations before the Bi-LSTM. 
\newcite{lim2018drug} proposed Recursive neural network-based model with a subtree containment feature and an ensemble method. This model showed the state-of-the-art performance on the DDIExtraction 2013 shared task data set if systems do not use negative instance filtering. These approaches did not consider molecular information, and they can also be enhanced by the molecular information. 

\newcite{vilar2017detection} focused on detecting DDIs from different sources such as pharmacovigilance sources, scientific biomedical literature and social media. They did not use deep neural networks and they did not consider molecular information.

Learning representations of graphs are widely studied in several tasks such as knowledge base completion, drug discovery, and material science~\cite{wang2017knowledge,gilmer2017neural}. 
Several graph convolutional neural networks have been proposed such as NFP~\cite{NIPS2015_5954}, GGNN~\cite{li2015gated}, and Molecular Graph Convolutions~\cite{kearnes2016molecular}, but they have not been applied to DDI extraction.


\section{Conclusions}

We proposed a novel neural method for DDI extraction using both textual and molecular information.
The results show that DDIs can be predicted with high accuracy from molecular structure information and that the molecular information can improve DDI extraction from texts by 2.39 percept points in F-score on the data set of the DDIExtraction 2013 shared task.

As future work, we would like to seek the way to model the textual and molecular representations jointly with alleviating the differences in labels. We will also investigate the use of other information in DrugBank.


\section*{Acknowledgments}

This work was supported by JSPS KAKENHI Grant Number 17K12741.


\bibliography{acl2018}
\bibliographystyle{acl_natbib}

\clearpage

\appendix
\section{Supplemental Material}
\subsection{Data Statistics}

The statistics of the data set is shown in Table~\ref{table:dataset}. This shows that the data is highly unbalanced and includes more negative pairs than positive pairs. 

\begin{table}[ht]
  \centering
  \begin{tabular}{llrr}\hline
  &DDI type & Train & Test\\\hline
  Positive & \textsl{Mechanism} & 1,319 & 302 \\
  & \textsl{Effect} & 1,687 & 360 \\
  & \textsl{Advice} & 826 & 221 \\
  & \textsl{Int} & 189 & 96 \\
  & Total & 4,021 & 979 \\
  Negative &  & 23,771 & 4,737 \\
  Total &  & 27,792 & 5,716 \\\hline
  \end{tabular}
  \caption{Statistics of the DDIExtraction 2013 shared task data set}
  \label{table:dataset}
\end{table}


\subsection{Hyper-parameter Settings}

We show hyper-parameters for training text-based model and molecular-based model in Tables \ref{table:param} and \ref{table:gcn_hyperparams}, respectively. 

\begin{table}[ht]
  \centering
  \begin{tabular}{lr}\hline
  Parameter & Value \\\hline
  Word embedding size & 200 \\
  Word position embedding size & 20 \\ 
  Convolution window size & [3, 5, 7] \\
  Convolution filter size & 100 \\
  Hidden layer size & 500 \\
  Initial learning rate & 0.001 \\
  Mini-batch size & 50 \\
  L2 regularization parameter & 0.0001 \\
  
  \hline
  \end{tabular}
  \caption{Hyperparameters for text-based model}
  \label{table:param}
\end{table}

\begin{table}[ht]
  \centering
  \begin{tabular}{lr} \hline
  Parameter & Value \\\hline
  Molecular vector size & 50 \\
  Number of steps & 4 \\
  Hidden layer size & 1,000 \\
  Initial learning rate & 0.001 \\
  Mini-batch size & 100 \\
  Hidden layer size of NFP & 50 \\
  GRU unit size of GGNN & 50 \\\hline
  \end{tabular}
  \caption{Hyperparameters for molecular-based model}
  \label{table:gcn_hyperparams}
\end{table}

\end{document}
