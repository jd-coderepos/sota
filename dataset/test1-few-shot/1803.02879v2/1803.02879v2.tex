

\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} \usepackage{relsize}

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}



\usepackage{nicefrac}       \usepackage{url}
\usepackage{todonotes}


\newcommand{\all}[1]{\todo[inline,color=red!50]{ALL: #1}}
\newcommand{\jh}[1]{\todo[inline,color=blue!50]{JH: #1}}
\newcommand{\dg}[1]{\todo[inline,color=purple!50]{DG: #1}}
\newcommand{\sr}[1]{\todo[inline,color=orange!50]{SR: #1}}
\newcommand{\klb}[1]{\todo[inline,color=green!50]{KLB: #1}}
\newcommand{\tablefont}{\fontfamily{cmss}\fontseries{uc}\fontsize{10pt}{9pt}\selectfont\centering}
\newcommand{\tablefontsmaller}{\fontfamily{cmss}\fontseries{uc}\fontsize{8pt}{7pt}\selectfont\centering}

\usepackage{amsbsy}
\usepackage{amsmath}
\usepackage{amsmath}\usepackage{ntheorem}       \usepackage{dsfont}
\usepackage{MnSymbol}\usepackage{wasysym}\usepackage[cal=dutchcal,
calscaled=1,
bb=boondox,
scr=euler]{mathalfa}
\usepackage{wrapfig}
\usepackage[capitalise]{cleveref}
\crefformat{equation}{(#2#1#3)}

\newcommand{\ie}[0]{\emph{i.e.},~}
\newcommand{\eg}[0]{\emph{e.g.},~}
\newcommand{\aka}[0]{a.k.a.~}
\newcommand{\etal}{\emph{et al.~}}
\newcommand{\etc}{\emph{etc.~}}
\newcommand{\wrt}{w.r.t.~}
\def\old#1{{\color{gray}{{old:\ #1\ }}}}
\newcommand{\defeq}{\ensuremath{\doteq}}
\newcommand{\diag}{\operatorname{diag}}

\newcommand{\mat}[1]{\ensuremath{{#1}}}
\newcommand{\ident}[0]{\ensuremath{\mathbf{1}}}
\newcommand{\gr}[1]{\ensuremath{\mathcal{#1}}}
\newcommand{\set}[1]{\ensuremath{\mathbb{#1}}}
\renewcommand{\vec}[1]{\ensuremath{\operatorname{vec}({#1})}}
\newcommand{\unvec}[1]{\ensuremath{\operatorname{vec}^{-1}({#1})}}

\newcommand{\pN}[0]{\ensuremath{\mathscr{N}}}
\newcommand{\bN}[0]{\ensuremath{\underline{N}}}
\newcommand{\bn}[0]{\ensuremath{\underline{n}}}
\newcommand{\bm}[0]{\ensuremath{\underline{m}}}
\newcommand{\bi}[0]{\ensuremath{\underline{k}}}
\newcommand{\st}{\;:\;}
\newcommand{\prm}[1]{\ensuremath{^{(#1)}}}
\newcommand{\chn}[1]{\ensuremath{^{\langle#1 \rangle}}}

\newcommand{\ttt}[1]{\ensuremath{^{(#1)}}}
\newcommand{\grn}[2]{\ensuremath{\gr{#1}\prm{#2}}}
\newcommand{\Trp}[0]{\ensuremath{^{\mathsf{T}}}}
\newcommand{\A}[0]{\ensuremath{\mat{A}}}
\newcommand{\XX}[0]{\ensuremath{\mat{X}}}
\newcommand{\YY}[0]{\ensuremath{\mat{Y}}}
\newcommand{\WW}[0]{\ensuremath{\mat{W}}}
\newcommand{\ZZ}[0]{\ensuremath{\mat{Z}}}
\newcommand{\xx}[0]{\ensuremath{\mat{x}}}
\newcommand{\yy}[0]{\ensuremath{\mat{y}}}
\newcommand{\zz}[0]{\ensuremath{\mat{z}}}
\newcommand{\Xset}[0]{\ensuremath{\set{X}}}
\newcommand{\Yset}[0]{\ensuremath{\set{Y}}}
\newcommand{\Rset}[0]{\ensuremath{\set{R}}}
\newcommand{\Nat}[0]{\ensuremath{\set{N}}}
\renewcommand{\Re}[0]{\ensuremath{\mathds{R}}}
\newcommand{\thetas}[0]{\ensuremath{\boldsymbol{\theta}}}
\newcommand{\WWs}[0]{\ensuremath{\boldsymbol{theta}}}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{objective}[theorem]{Objective}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]
\theoremsymbol{\ensuremath{\blacksquare}}
\newtheorem*{proof}{Proof}\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\row}[0]{\ensuremath{\set{R}}}
\newcommand{\col}[0]{\ensuremath{\set{C}}}
\newcommand{\ind}[0]{\ensuremath{\set{I}}}


\usepackage[accepted]{icml2018}



\begin{document}

\twocolumn[
\icmltitle{Deep Models of Interactions Across Sets}






\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Jason Hartford}{equal,ubc}
\icmlauthor{Devon R Graham}{equal,ubc}
\icmlauthor{Kevin Leyton-Brown}{ubc}
\icmlauthor{Siamak Ravanbakhsh}{ubc}
\end{icmlauthorlist}

\icmlaffiliation{ubc}{Department of Computer Science, University of British Columbia, Canada}

\icmlcorrespondingauthor{Devon Graham}{drgraham@cs.ubc.ca}
\icmlcorrespondingauthor{Jason Hartford}{jasonhar@cs.ubc.ca}

\icmlkeywords{deep learning, exchangeability, equivariance, invariance, tensor, matrix completion, recommender systems}

\vskip 0.3in
]





\printAffiliationsAndNotice{\icmlEqualContribution} 

\begin{abstract}
    We use deep learning to 
    model interactions across two or more sets of objects, such as user--movie ratings, protein--drug bindings, or ternary user-item-tag interactions. 
    The canonical representation of such interactions is a matrix (or a higher-dimensional tensor) with an exchangeability property: the encoding's meaning is not changed by permuting rows or columns. We argue that models should hence be \emph{Permutation Equivariant (PE)}: constrained to make the same predictions across such permutations. We present a parameter-sharing scheme and prove that it could not be made any more expressive without violating PE.
This scheme yields three benefits. First, we demonstrate state-of-the-art performance on multiple matrix completion benchmarks. Second, our models require a number of parameters independent of the numbers of objects, and thus scale well to large datasets. Third, models can be queried about new objects that were not available at training time, but for which interactions have since been observed. In experiments, our models achieved surprisingly good generalization performance on this \emph{matrix extrapolation} task, both within domains (e.g., new users and new movies drawn from the same distribution used for training) and even across domains (e.g., predicting music ratings after training on movies).
\end{abstract}



\section{Introduction}\label{sec:intro}









Suppose we are given a set of users , a set of movies , and their interaction in the form of tuples  with ,  and . Our goal is to learn a function that models the interaction between users and movies, i.e., mapping from  to . The canonical representation of such a function is a matrix ; of course, we want  for each . Learning our function corresponds to \emph{matrix completion}: using patterns in  to predict values for the remaining elements of .
 is what we will call an \emph{exchangeable matrix}: any row- and column-wise permutation of  represents the same set of ratings and hence the same matrix completion problem. 

{Exchangeability}  has a long history in  machine learning and statistics.
{de Finetti}'s theorem states that exchangeable sequences are the product of a {latent variable model}. Extensions of this theorem characterize distributions over other exchangeable structures, from matrices to graphs; see \citet{orbanz2015bayesian} for a detailed treatment.
In machine learning, a variety of frameworks formalize exchangeability in data, from plate notation to {statistical relational models}~\citep{raedt2016statistical,getoor2007introduction}. 
When dealing with exchangeable arrays (or tensors), a common approach is {tensor factorization}. In particular, one thread of work leverages tensor decomposition for inference in latent variable models~\citep{anandkumar2014tensor}.
However, in addition to having limited expressive power, tensor factorization requires retraining models for each new input. 



We call a learning algorithm \emph{Permutation Equivariant (PE)} if it is constrained to give the same answer across all exchangeable inputs; we argue that PE is an important form of inductive bias in exchangeable domains. However, it is not trivial to achieve; indeed, all existing parametric factorization and matrix/tensor completion methods associate parameters with each row and column, and thus are not PE. How can we enforce this property? One approach is to augment the input with all  permutations of rows and columns.
However, this is computationally wasteful and becomes infeasible for all but the smallest  and . 
Instead, we show that a simple {parameter-sharing} scheme suffices to ensure that a deep model can represent only PE functions. 
The result is analogous to the idea of a convolution layer: a lower-dimensional effective parameter space that enforces a desired equivariance property. 
Indeed, parameter-sharing is a generic and efficient approach for achieving equivariance in deep models~\citep{ravanbakhsh_symmetry}. 









\begin{figure}[t]\centering
\includegraphics[width=\columnwidth]{img/theta_123D}
\caption{Structure of our parameter matrix for the 1D (left), 2D (centre), and 3D (right) input arrays. 
The parameter-sharing patterns for the weight matrix of the higher dimensional arrays can be produced via the \textit{Kronecker product} of the weight matrix for the 1D array (\ie vector).}
\label{fig:parameter_matrix}
\end{figure}



When the matrix models the interaction between the members of the same group, one could further constrain permutations to be identical across both rows and columns. 
An example of such a \textit{jointly exchangeable matrix} \mbox{\citep{orbanz2015bayesian}}, modelling the interaction of the nodes in a graph, is the adjacency matrix deployed by graph convolution. Our approach reduces to graph convolution in the special case of 2D arrays with this additional parameter-sharing constraint, but also applies to arbitrary matrices and higher dimensional arrays.






Finally, we explain connections to some of our own past work. First, we introduced a similar parameter-sharing scheme in the context of behavioral game theory \citep{hartford2016deep}: rows and columns represented players' actions and the exchangeable matrix encoded payoffs. The current work provides a theoretical foundation for these ideas and shows how a similar architecture can be applied much more generally. Second, our model for exchangeable matrices can be seen as a generalization of the \emph{deep sets} architecture \citep{zaheer_deepsets}, where a set can be seen as a one-dimensional exchangeable array. 

In what follows, we begin by introducing our parameter-sharing approach in \cref{sec:layer}, considering the cases of both dense and sparse matrices.
In \cref{sec:architectures} we study two architectures for matrix completion that use an exchangeable matrix layer. In particular
the factorized autoencoding model provides a powerful alternative to commonly used matrix factorization methods; notably, it does not require retraining to be evaluated on previously unseen data.
In \cref{sec:empirical} we present extensive results on benchmark matrix completion datasets. We generalize our approach to higher-dimensional tensors in \cref{sec:tensors}. 

\section{Exchangeable Matrix Layer}\label{sec:layer}
\begin{figure}[t]\centering
\includegraphics[width=\columnwidth]{img/layer}
\vspace{-2em}\caption{Parameter sharing in an exchangeable matrix layer. The application of different tied parameters to input elements is illustrated using dark blue for , green for  , red for , and light blue for . The same structure (with the same four parameters) repeats across all units of the output layer. }
\label{fig:parameter_sharing}
\end{figure}



Let   be our ``exchangeable'' input matrix. We use  to denote its vectorized form and  to denote the inverse of the vectorization that reshapes a vector of length  into an  matrix---\ie .
Consider a fully connected layer  where  is an element-wise nonlinear function such as sigmoid, ,
and  is the output matrix. 
Exchangeablity of  motivates the following property:
suppose we permute the rows and columns 
using two arbitrary permutation matrices  and 
to get . Permutation Equivariance (PE) requires the new output matrix  to 
experience the same permutation of rows and columns---that is, we require .


\begin{definition}[exchangeable matrix layer, simplified\footnotemark]\footnotetext{This definition is simplified to ease exposition; the full definition (see \cref{sec:tensors}) adds the additional constraint that the layer \emph{not} be equivariant wrt any other permutation of the elements of . Otherwise, a trivial layer with a constant weight matrix  would also satisfy the stated equivariance property.}\label{def:2pe_layer}


Given , a fully connected layer 
 with  is called an {exchangeable matrix layer} if, for all permutation matrices  and , permutation of the rows and columns results in the same permutations of the output:
 
\end{definition}

This requirement heavily constrains the weight matrix : indeed, its number of effective degrees of freedom cannot even grow with  and . Instead, the resulting layer is forced to have the following, simple form:


For each output element , we have the following parameters: one connecting it to its counterpart ; 
one each connecting it to the inputs of the same row and the inputs of the same column; and one shared by all the other connections. We also include a bias parameter; see \cref{fig:parameter_sharing} for an illustration of the action of this parameter matrix, and \cref{fig:parameter_matrix} for a visual illustration of it. Theorem \ref{thm:pe_layer} formalizes the requirement on our parameter matrix. All proofs are deferred to the appendix. 



\begin{theorem}\label{thm:pe_layer}
Given a strictly monotonic function , a neural layer  is an \textbf{exchangeable matrix layer} \textit{iff} the elements of the parameter matrix  are tied together such that the resulting fully connected layer simplifies to 

where  and .
\end{theorem}
\vspace{-.6em minus .3em}
This parameter sharing is translated to summing or averaging elements across rows and columns; more generally, PE is preserved by any commutative pooling operation.
Moreover, stacking multiple layers with the same equivariance properties
preserves equivariance~\cite{ravanbakhsh_symmetry}. This allows us to build \textit{deep} PE models.

\paragraph{{Multiple Input--Output Channels}}
Equation~\cref{eq:layer_simple_mat} describes the layer as though it has single input and output matrices.
However, as with convolution, we may have  input and  output channels. We use superscripts  and 
to denote such channels. Cross-channel interactions are fully connected---that is, we have five unique parameters  for \textit{each combination}
of input--output channels; note that the bias parameter  does not depend on the input.
Similar to convolution, the number of channels provides a tuning nob for the expressive 
power of the model.
In this setting, the scalar output  is given as
 




\vspace{-.6em minus .3em}\paragraph{{Input Features for Rows and Columns}} 
In some applications, in addition to the matrix , where  is the number of input channels/features, we may have additional features for rows  and/or columns . We can preserve PE by broadcasting these row/column features over the whole matrix and treating them as additional input channels.

\vspace{-.6em minus .3em}\paragraph{{Jointly Exchangeable Matrices}}
For jointly exchangeable matrices, such as adjacency matrix, Equation~\cref{eq:1} is constrained to have  and .
This will in turn constrain the corresponding parameter-sharing so that  in Equation~\cref{eq:w}.

\subsection{Sparse Inputs}\label{sec:sparse}
Real-world arrays are often extremely sparse. 
Indeed, matrix and tensor completion is only meaningful with missing entries. Fortunately, the equivariance properties of \cref{thm:pe_layer} continue to hold when we only consider the nonzero (observed) entries. For sparse matrices, we continue to use the same parameter-sharing scheme across rows and columns, with the only difference being that we limit the model to observed entries. We now make this precise.

Let  be a sparse exchangeable array with  channels, where \textit{all} the channels for each row-column pair  are either fully observed or completely missing.
Let  identify the set of such non-zero indices.
Let  be the non-zero entries in the  row of , and let  be the non-zero entries of its  column. For this sparse matrix, the terms in the layer of \cref{eq:layer_multi_channel_mat} are adapted as one would expect:



\section{Matrix Factorization and Completion}\label{sec:fact_comp}
Recommender systems are very widely applied, with many modern applications suggesting new items (\eg movies, friends, restaurants, etc.) to users based on previous ratings of other items. 
The core underlying problem is naturally posed as a matrix completion task: each row corresponds to a user and each column corresponds to an item; the matrix has a value for each rating given to an item by a given user; the goal is to fill in the missing values of this matrix. 

In \cref{sec:ind_trans} we review two types of analysis in dealing with exchangeable matrices.
\cref{sec:architectures} introduces two architectures: a self-supervised model---a simple composition of exchangeable matrix layers---that is trained to produce randomly removed entries of the observed matrix during the training; and a factorized model that uses an auto-encoding nonlinear factorization scheme. While there are innumerable methods for (nonlinear) matrix factorization and completion, both of these models are the first to generalize to inductive settings while achieving competitive performance in transductive settings. 
\cref{sec:subsampling} introduces two subsampling techniques for large sparse matrices followed by a literature review in \cref{sec:related}. 

\subsection{Inductive and Transductive Analysis}\label{sec:ind_trans}
In matrix completion, during training we are given a sparse input matrix  with observed entries . At test time, we are given  with observed entries , and we are interested in predicting (some of) the missing entries of , expressed through .
In the transductive or \textit{matrix interpolation} setting,  and  have overlapping rows and/or columns---that is, at least one of the following is true:
 or
.
In fact, often  and  are identical.
Conversely, in the inductive or \textit{matrix extrapolation} setting, we are interested in making predictions about completely unseen entries: the training and test row/column indices are completely disjoint.
We will even consider cases where  and  are completely different datasets---\eg movie-rating vs music-rating.
The same distinction applies in matrix factorization. Training a model to factorize a 
particular given matrix is transductive, while factorizing unseen 
matrices \emph{after} training is inductive. 

\subsection{Architectures}\label{sec:architectures}
\begin{figure}[t]\centering
\includegraphics[width=\columnwidth,trim=0px 72px 0px 70px,clip=true]{img/fact_ae_copy}
\vspace{-1.3em}\caption{Factorized exchangeable autoencoder. The encoder maps from the input tensor to an embedding layer of row / column factors via one or more hidden layers. The decoder attempts to reconstruct the input using the factors via one or more hidden layers.}
\label{fig:fact_ae}
\end{figure}

\paragraph{Self-supervised Exchangeable Model} When the task is matrix completion, a simple deep yet PE model is a 
composition of exchangeable matrix layers.
That is the function  is simply a composition of exchangeable matrix layers. Given the matrix  with observed entries ,
we divide  into disjoint input and prediction entries. 
We then train  to predict the prediction entries .



\vspace{-.6em minus .3em}\paragraph{Factorized Exchangeable Autoencoder (FEA)}
Our factorized autoencoder is composed of an encoder and a decoder. 
The encoder  maps
the (sparse) input matrix  to a row-factor  and a column-factor . To do so, the encoder uses a composition of
exchangeable matrix layers. The output of the final layer  is pooled across rows and columns (and optionally passed through a feed-forward layer) to 
produce latent factors  and .
The decoder  also uses a composition of exchangeable matrix layers, and reconstructs the input matrix  from the factors. The optimization objective is to minimize reconstruction error
; similar to classical auto-encoders. 

This procedure is also analogous to classical matrix factorization, with an an important distinction that
once trained, we can readily factorize the unseen matrices without performing any optimization. Note that the same architecture trivially extends to tensor-factorization, where we use an exchangeable tensor layer (see \cref{sec:tensors}). 

\vspace{-.6em minus .3em}\paragraph{Channel Dropout}
Both the factorized autoencoder and self-supervised exchangeable model are flexible enough to make regularization important for good generalization performance. Dropout \citep{srivastava2014dropout} can be extended to apply to exchangeable matrix layers by noticing that each channel in an exchangeable matrix layer is analogous to a single unit in a standard feed-forward network. We therefore randomly drop out whole channels during training (as opposed to dropping out individual elements). This procedure is equivalent to the \emph{SpatialDropout} technique used in convolutional networks  \citep{tompson2015efficient}.


\subsection{Subsampling in Large Matrices}\label{sec:subsampling}
A key practical challenge arising from our approach is that our models are designed to take the whole data matrix  as input and will make different (and typically worse) predictions if given only a subset of the data matrix. As datasets grow, the model and input data become too large to fit within fixed-size GPU memory. This is problematic both during training and at evaluation time because our models rely on aggregating shared representations across data points to make their predictions. To address this, we explore two subsampling procedures. 
\vspace{-.6em minus .3em}\paragraph{Uniform sampling} The simplest approach is to sample sub-matrices of  by uniformly sampling from its (typically sparse) elements. 
This has the advantage that each submatrix is an unbiased sample of the full matrix and that the procedure is computationally cheap, but has the potential to limit the performance of the model because the relationships between the elements of  are sparsified.

\vspace{-.6em minus .3em}\paragraph{Conditional sampling} Rather than sparsifying interactions between all set members, we can pick a subset of rows and columns and maintain all their interactions; see Figure \ref{fig:sampling}. This procedure is unbiased as long as each element  has the same probability of being sampled.
To achieve this, we first sample a subset of rows  from the marginal , followed by subsampling of colums using the marginal distribution over the columns, within the selected rows: . This gives us a set of columns . We consider any observation within  as our subsample: . 
This sampling procedure is more expensive than uniform sampling, as we have to calculate 
conditional marginal distributions for each set of samples.

\begin{figure}[h]\centering
\includegraphics[width=\columnwidth]{img/sampling}
\caption{Uniform sampling (left) selects samples (red) uniformly from the non-zero indices of the the matrix  while conditional sampling (right) first samples a set of rows (shown in orange) from the row marginal distribution (green) and then selects sample from the resulting column conditional distribution.}
\label{fig:sampling}
\end{figure}

\paragraph{Sampling at test time} At training time all we must ensure is that we sample in an unbiased way; however, at test time we need to ensure that the test indices  of large test matrix  are all sampled.
Fortunately, according to the \textit{coupon collectors' lemma}, in expectation we only need to repeat
random sampling of indices 
 times more ( in practice) than an ideal partitioning of , in order to cover all the relevant indices. 









\subsection{Related Literature}\label{sec:related}
The literature in matrix factorization and completion is vast. 
The classical approach to solving the matrix completion problem is to find some low rank (or sparse) approximation that minimizes a reconstruction loss for the observed ratings \citep[see e.g.,][]{candes2009exact, koren2009matrix, mnih2008probabilistic}. 
Because these procedures learn embeddings for each user and item to make predictions, they are {transductive}, meaning they can only make predictions about users and items observed during training. To our knowledge this is also true for all recent   
deep factorization and other collaborative filtering techniques~\citep[\eg][]{salakhutdinov2007restricted,deng2017factorized,sedhain2015autorec,wang2015collaborative,li2015deep,zheng2016neural,dziugaite2015neural}.
An exception is a recent work by \citet{yang2016revisiting} that extends factorization-style approaches to the {inductive} setting (where predictions can be made on unseen users and items). However their method relies on additional side information to represent users and items. By contrast, our approach is able to make inductive completions on rating matrices that may differ from that which was observed during training without using any side information (though our approach can easily incorporate side information).

Matrix completion may also be posed as predicting edge weights in bipartite graphs~\citep{berg2017graph,monti_geomatrix}.
This approach builds on recent work applying convolutional neural networks to graph-structured data \citep{scarselli2009graph,bruna2013spectral,duvenaud2015convolutional, defferrard2016convolutional, kipf2016semi,hamilton2017inductive}. As we saw, parameter sharing in graph convolution is closely related to parameter sharing in exchangeable matrices, and indeed it is a special case where  in Equation~\cref{eq:w}.






\section{Empirical Results}\label{sec:empirical}
For reproducibility we have released Tensorflow and Pytorch implementations of our model.\footnote{\noindent Tensorflow: \url{https://github.com/mravanba/deep_exchangeable_tensors}. Pytorch: \url{https://github.com/jhartford/AutoEncSets}.} 
Details on the training and architectures appear in the appendix. 
\cref{sec:results_trans} reports experimental results in the standard transductive (matrix interpolation)
setting. However, more interesting results are reported in \cref{sec:results_inductive}, where 
we test a trained deep model on a completely different dataset. Finally \cref{sec:results_sampling} compares the
model's performance when using different sampling procedures. The datasets used in our experiments are summarized in \cref{results:datasets}.

\begin{table}[h]
\tablefontsmaller
\begin{tabular}{l r r r r} 
\toprule
\textbf{Dataset} & \textbf{Users} & \textbf{Items} & \textbf{Ratings} & \textbf{Sparsity} \\ [0.5ex] 
\midrule
MovieLens 100K & 943 & 1682 & 100,000 & 6.30\% \\ 
MovieLens 1M & 6040 & 3706 & 1,000,209 & 4.47\%\\
Flixster & 3000& 3000& 26173  & 0.291\%\\
Douban & 3000& 3000& 136891 & 1.521\%\\
Yahoo Music& 3000& 3000 & 5335 & 0.059\%\\
Netflix & 480,189 & 17,770 & 100,480,507 & 1.178\%\\
\bottomrule
\end{tabular}
\caption {Number of users, items and ratings for the  data sets used in our experiments. MovieLens data sets are standard \citep{movielens}, as is Netflix, while for Flixster, Douban and Yahoo Music we used the  submatrix presented by \citep{monti_geomatrix} for comparison purposes.}\label{results:datasets}
\end{table}

\subsection{Transductive Setting (Matrix Interpolation)}\label{sec:results_trans}
Here, we demonstrate that exploiting the PE structure of the exchangeable matrices
allows us to achieve results competitive with state-of-the-art, 
while maintaining a constant number of parameters.
Note that the number of parameters in all the competing methods grow with  and/or .




In \cref{results:perf100k} we report that the self-supervised exchangeable model is able to achieve state of the art performance on 
MovieLens-100K dataset. For MovieLens-1M dataset, we cannot fit the whole dataset into the GPU memory for training and therefore use conditional subsampling; also see \cref{sec:results_sampling}. Our results on this dataset are summarized in table \cref{results:perf1M}. On this larger dataset both models gave comparatively weaker performance than what we observed on the smaller ML-100k dataset and in the extrapolation results. We suspect this is largely due to memory constraints: there is a trade-off between the size of the model (in terms of number of layers and units per layer) and the batch size one can train. We found that both larger batches and deeper models tended to offer better performance, but on these larger datasets it is not currently possible to have both.
The results for the factorized exchangeable autoencoder architecture are similar and reported in the same table.












\begin{table}[h]
\tablefont
\begin{tabular}{l r} 
  \toprule
  \textbf{Model} & \textbf{ML-100K} \\ [0.5ex] 
  \midrule
  MC {\smaller{\citep{candes2009exact}}} & 0.973 \\
  GMC {\smaller{\citep{kalofolias2014completion}}}  & 0.996 \\
  GRALS {\smaller{\citep{rao2015collab}}}  & 0.945 \\
  sRGCNN {\smaller{\citep{monti_geomatrix}}} & 0.929 \\
  Factorized EAE (ours)&0.920 \\
  GC-MC {\smaller{\citep{berg2017graph}}} & \textbf{0.910} \\
  Self-Supervised Model (ours) & 0.910 \\
\bottomrule
\end{tabular}
\caption {Comparison of RMSE scores for the MovieLens-100k dataset, based on the canonical 80/20 training/test split. Baseline numbers are taken from \citep{berg2017graph}.} \label{results:perf100k}
\end{table}

\begin{table}[h]
\tablefont
\begin{tabular}{l r} 
  \toprule
  \textbf{Model} & \textbf{ML-1M}  \\ [0.5ex] 
  \midrule
  PMF {\smaller{\citep{mnih2008probabilistic}}} & 0.883  \\
  Self-Supervised Model (ours)& 0.863 \\
  Factorized EAE (ours)& 0.860\\
  I-RBM {\smaller{\citep{salakhutdinov2007restricted}}}  & 0.854 \\
  BiasMF {\smaller{\citep{koren2009matrix}}}  & 0.845 \\
  NNMF {\smaller{\citep{dziugaite2015neural}}} & 0.843 \\
  LLORMA-Local {\smaller{\citep{lee2013local}}} & 0.833 \\
  GC-MC {\smaller{\citep{berg2017graph}}} & 0.832\\
  I-AUTOREC {\smaller{\citep{sedhain2015autorec}}} & 0.831 \\
  CF-NADE {\smaller{\citep{zheng2016neural}}} & \textbf{0.829} \\
  \bottomrule
\end{tabular}
\caption {Comparison of RMSE scores for the MovieLens-1M dataset on random 90/10 training/test split. Baseline numbers are taken from \citep{berg2017graph}.}\label{results:perf1M}
\end{table}

\subsection{Inductive Setting (Matrix Extrapolation)}\label{sec:results_inductive}
Because our model does not rely on any per-user or per-movie parameters, it should be able to generalize to new users and movies that were not present during training. We tested this by training an exchangeable factorized autoencoder on the MovieLens-100k dataset and then evaluating it on a subsample of data from the MovieLens-1M dataset. At test time, the model was shown a portion of the new ratings and then made to make predictions on the remaining ratings. 

Figure \ref{fig:missing} summarizes the results where we vary the amount of data shown to the model from 5\% of the new ratings up to 95\% and compare against K-nearest neighbours approaches. Our model significantly outperforms the baselines in this task and performance degrades gracefully as we reduce the amount of data observed.

\begin{figure}[h]\centering
\includegraphics[width=0.95\columnwidth, trim=0px 0px 0px 0px,clip=true]{img/percent}
\caption{Evaluation of our model's ability to generalize. We trained on ML-100k and evaluated on a subset of the ML-1M data. At evaluation time,  of the ML-1M data was treated as \emph{observed} and the model was required to complete the remaining  ( varied from  to ). The model outperforms nearest-neighbour approaches for all values of  and degrades gracefully to the baseline of predicting the mean in the small data case.}
\label{fig:missing}
\end{figure}

\vspace{-.6em minus .3em}\paragraph{Extrapolation to new datasets}
Perhaps most surprisingly, we were able to achieve competitive results when training and testing on completely disjoint datasets. For this experiment we stress-tested our model's inductive ability by testing how it generalizes to new datasets \emph{without retraining}. We used a Factorized Exchangeable Autoencoder that was trained to make predictions on the MovieLens-100k dataset and tasked it with making predictions on the Flixster, Douban and YahooMusic datasets. We then evaluated its performance against models trained for each of these individual datasets. All the datasets involve rating prediction tasks, so they share some similar semantics with MovieLens, but they have different user bases and (in the case of YahooMusic) deal with music not movie ratings, so we may expect some change in the rating distributions and user-item interactions. Furthermore, the Flixster ratings are in 0.5 point increments from  and YahooMusic allows ratings from , while Douban and MovieLens ratings are on  scale. To account for the different rating scales, we simply binned the inputs to our model to a  range and, where applicable, linearly rescaled the output before comparing it to the true rating\footnote{Because of this binning procedure, our model received input data that is considerably coarser-grained than that which was used for the comparison models.}. Despite all of this, Table \ref{results:generalize} shows that our model achieves very competitive results with models that were trained for the task. 

For comparison, we also include the performance of a Factorized EAE trained on the respective datasets. This improves performance of our model over previous state of the art results on the Flixster and YahooMusic datasets and gives very similar performance to \citet{berg2017graph}'s GC-MC model on the Douban dataset. Interestingly, we see the largest performance gains over existing approaches on the datasets in which ratings are sparse (see Table \ref{results:datasets}).






\subsection{Comparison of sampling procedures}
\label{sec:results_sampling}



We evaluated the effect of subsampling the input matrix  on performance, for the MovieLens-100k dataset. The results are summarized in Figure \ref{fig:sampling_perf}. 
The two key findings are: I) even with large batch sizes of 20 000 examples, performance for both sampling methods is diminished compared to the full batch case. We suspect that our models' weaker results on the larger ML-1M dataset may be partially attributed to the need to subsample. II) the conditional sampling method was able to recover some of the diminished performance. We believe it is likely that more sophisticated sampling schemes that explicitly take into account the dependence between hidden nodes will lead to better performance but we leave that to future work.






\begin{table}[t]
  \tablefontsmaller
  \smaller{\begin{tabular}{l r r r r} 
  \toprule
  \textbf{Model} & \rotatebox{90}{\textbf{Flixster}} & \rotatebox{90}{\textbf{Douban}} & \rotatebox{90}{\textbf{YahooMusic}} & \rotatebox{90}{\textbf{Netflix}} \\ [0.5ex] 
  \midrule
  GRALS {\smaller{\citep{rao2015collab}}}  &1.313 & 0.833 & 38.0 & - \\
  sRGCNN {\smaller{\citep{monti_geomatrix}}} & 1.179 &  0.801 & 22.4 & - \\
GC-MC {\smaller{\citep{berg2017graph}}} & 0.941 & \textbf{0.734} & 20.5 & - \\
  Factorize EAE (ours) & \textbf{0.908} & 0.738 & \textbf{20.0} & -\\
Factorize EAE {\smaller{\color{red}{(trained on ML100k)}}} & 0.987 & 0.766 & 23.3 & 0.918\\
  Netflix Baseline & - & - & - & 0.951 \\
  PMF {\smaller{\citep{mnih2008probabilistic}}} & - & - & - & 0.897 \\
  LLORMA-Local {\smaller{\citep{lee2013local}}} & - & - & - & 0.834 \\
  I-AUTOREC {\smaller{\citep{sedhain2015autorec}}} & - & - & - & 0.823 \\
  CF-NADE {\smaller{\citep{zheng2016neural}}} & - & - & - & \textbf{0.803} \\       
  \bottomrule
\end{tabular}}
\caption {Evaluation of our model's ability to generalize across datasets. We trained a factorized model on ML100k and then evaluated it on four new datasets. Results for the smaller datasets are taken from \citep{berg2017graph}. Netflix Baseline shows state of the art prior to the Netflix Challenge.}\label{results:generalize}
\end{table}

\begin{figure}[h]\centering
\includegraphics[width=0.95\columnwidth, trim=0px 0px 0px 0px,clip=true]{img/sampling_perf}
\caption{Performance difference between sampling methods on the ML-100k. The two sampling methods use minibatches of size 20 000, while the full batch method used all 75 000 training examples. Note that the y-axis does not begin at 0.}
\label{fig:sampling_perf}
\end{figure}

\section{Extention to Tensors}\label{sec:tensors}
In this section we generalize the exchangeable matrix layer to higher-dimensional arrays (tensors) and formalize its optimal qualities.
Suppose  is our -dimensional data tensor, and  its vectorized form. We index  by tuples , corresponding to the original dimensions of . The precise method of vectorization is irrelevant, provided it is used consistently. Let  and let  be an element of  such that  is the value of the -th entry of , and  the values of the remaining entries (where it is understood that the ordering of the elements of  is unchanged). We seek a layer that is equivariant only to certain, meaningful, permutations of . This motivates our definition of an exchangeable tensor layer in a manner that is completely analogous to Definition \ref{def:2pe_layer} for matrices. 

For any positive integer , let  denote the symmetric group of all permutations of  objects. Then  refers to the product group of all permutations of  through  objects, while  refers to the group of all permutations of  objects. So  is a proper subgroup of  having  members, compared to  members in . We want a layer that is equivariant to \textit{only} those permutations in , but not to any other member of .

\begin{figure}[t]\centering
\includegraphics[width=\columnwidth]{img/matrix_tensor_pooling_color}
\caption{Pooling structure implied by the tied weights for matrices (left) and 3D tensors (right). The pink cube highlights one element of the output. It is calculated as a function of the corresponding element from the input (dark blue), pooled aggregations over the rows and columns of the input (green and yellow), and pooled aggregation over the whole input matrix (red). In the tensor case (right), we pool over all sub-tensors (orange and purple sub-matrices, green sub-vectors and red scalar). For clarity, the output connections are not shown in the tensor case.
}
\label{fig:exch_matrix_layer}
\end{figure}


\begin{definition}[exchangeable tensor layer]
Let 
and  be the corresponding permutation matrix. 
Then the neural layer  with  
and  is an exchangeable tensor layer if permuting the elements of the input along any set of axes results in the same permutation of the output tensor:

and moreover for any other permutation of the elements  (\ie permutations that are not along axes), 
there exists an input  for which this equality does not hold. 
\end{definition}


The following theorem asserts that a simple parameter tying scheme achieves the desired permutation equivariance for tensor-structured data. 

\begin{theorem}\label{thm:equivar}
The layer , where  is any strictly monotonic, element-wise nonlinearity (\eg sigmoid), is an exchangeable tensor layer iff the parameter matrix  is defined as follows. 

For each , define a distinct parameter , and tie the 
entries of  as follows 

That is, the -th element of  is uniquely determined by the set of indices at which  and  are equal. \end{theorem}

In the special case that  is a matrix, this gives the formulation of  described in Section \ref{sec:layer}. 
Theorem \ref{thm:equivar} says that a layer constructed in this manner is equivariant with respect to \textit{only} those permutations of  objects that correspond to permutations of  sub-tensors along the  dimensions of the input. The proof is in the Appendix. Equivariance with respect to permutations in ) follows as a simple corollary of Theorem 2.1 in \cite{ravanbakhsh_symmetry}. Non-equivariance with respect to other permutations follows from the following Propositions. \begin{proposition}\label{prop:equivar1}
Let  be an ``illegal'' permutation of elements of the tensor  -- that is . Then there exists a dimension  such that, for some :
    
\end{proposition}

If we consider the sub-tensor of  obtained by fixing the value of the -th dimension to , we expect a ``legal'' permutation to move this whole subtensor to some  (it could additionally shuffle the elements within this subtensor.) This Proposition asserts that an ``illegal'' permutation  is guaranteed to violate this constraint for some dimension/subtensor combination. 
The next proposition asserts that if we can identify such inconsistency in permutation of sub-tensors then we can
identify and entry in  that will differ from  , and therefore
for some input tensor , the equivariance property is violated -- \ie .

\begin{proposition}\label{prop:equivar2}
  Let  with  the corresponding permutation matrix. Suppose , and let  be as defined above. If there exists an , and some  such that 
    
    then
    
\end{proposition}

Proposition \ref{prop:equivar2} makes explicit a particular element at which the products  and  will differ, provided  is not of the desired form.

Theorem \ref{thm:equivar} shows that this parameter sharing scheme produces a layer that is equvariant to exactly those permutations we desire, and moreover, it is optimal in the sense that any layer having fewer ties in its parameters (\ie more parameters) would fail to be equivariant.  

\section*{Conclusion}\label{sec:discussion}
This paper has considered the problem of predicting relationships between the elements of two or more distinct sets of objects, where the data can also be expressed as an exchangeable matrix or tensor. We introduced a weight tying scheme enabling the application of deep models to this type of data. We proved that our scheme always produces permutation equivariant models and that no increase in model expressiveness is possible without violating this guarantee. Experimentally, we showed that our models achieve state-of-the-art or competitive performance on widely studied matrix completion benchmarks. Notably, our models achieved this performance despite having a number of parameters independent of the size of the matrix to be completed, unlike \emph{all} other approaches that offer strong performance. Also unlike these other approaches, our models can achieve competitive results for the problem of \emph{matrix extrapolation}: asking an already-trained model to complete a new matrix involving new objects that were unobserved at training time. 
Finally, we observed that our methods were surprisingly strong on various transfer learning tasks: extrapolating from MovieLens ratings to Fixter, Douban, and YahooMusic ratings. All of these contained different user populations and different distributions over the objects being rated; indeed, in the YahooMusic dataset, the underlying objects were not even of the same kind as those rated in training data.

\section*{Acknowledgment}\label{sec:ack}
We want to thank the anonymous reviewers for their constructive feedback.
This research was enabled in part by support provided by NSERC Discovery Grant, WestGrid and Compute Canada.

\bibliography{refs.bib}
\bibliographystyle{icml2018}

\newpage
\clearpage
\appendix

\section{Notation}
\begin{itemize}
\item , the data tensor
\item , vectorized , also denoted by .
\item : the sequence 
\item , the sequence  of -dimensional tuples over 
\item , an element of 
\item 
\item , symmetric group of \textit{all} permutations of  objects
\item , a permutation group of  objects 
\item  or , both can refer to the matrix form of the permutation .
\item  refers to the result of applying  to , for any .
\item , an arbitrary, element-wise, strictly monotonic, nonlinear function such as sigmoid.
\end{itemize}

\section{Proofs}\label{app:proofs}

\subsection{Proof of Proposition \ref{prop:equivar1}}
\begin{proof}
Let  be the data matrix. We prove the contrapositive by induction on , the dimension of . Suppose that, for all  and all , we have that  implies . This means that, for any  the action  takes on  is independent of the action  takes on , the remaining dimensions. Thus we can write 
  
  Where it is understood that the order of the group product is maintained (this is a slight abuse of notation). If  (base case) we have . So , and we are done. Otherwise, an inductive argument on  allows us to write . And so , completing the proof.
\end{proof}

\subsection{Proof of Proposition \ref{prop:equivar2}}
\begin{proof}
First, observe that 

  Now, let  be such that, for some  we have
    
  Then by the observation above we have:
    
  And so: 
  
  The last line follows from the observation above and the fact that  is a permutation matrix and so has only one 1 per row. Similarly, 
   
  Where again the last line follows from the above observation. Now, consider  and . Observe that  differs from  at exactly the same indices that  differs from . Let  be the set of indices at which  differs from . We therefore have 
  
  Which completes the proof. 
\end{proof}

\subsection{Proof of Theorem \ref{thm:equivar}}
\begin{proof}
  We will prove both the forward and backward direction: \\
   Suppose  has the form given by (\ref{eqn:params_tensor}). We must show the layer is only equivariant with respect to permutations in : 
  \begin{itemize}
    \item \textbf{Equivariance}: Let , and let  be the corresponding permutation matrix. Then a simple extension of Theorem 2.1 in \citep{ravanbakhsh_symmetry} implies  for all , and thus the layer is equivariant. Intuitively, if  we can ``decompose''   into  permutations  which act independently on the  dimensions of . 
    \item \textbf{No equivariance wrt any other permutation}: Let , with , and let  be the corresponding permutation matrix.
    Using Propositions \ref{prop:equivar1} and \ref{prop:equivar2} we have:
   
  So there exists an index at which these two matrices differ, call it . Then if we take  to be the vector of all 0's with a single 1 in position , we will have:
     
   And since  is assumed to be strictly monotonic, we have:
     
   And finally, since \mat{G}\prm{\pN} is a permutation matrix and  is applied element-wise, we have:
        
   Therefore, the layer  is not a exchangeable tensor layer, and the proof is completed.
   \end{itemize}
  This proves the first direction. \\
  \\
   We prove the contrapositive.  Suppose  for some  with . We want to show that the layer  is not equivariant to some permutation . We define this permutation as follows: 
   
  That is,  ``swaps''  with  and  with . This is a valid permutation first since it acts element-wise, but also since  implies that  iff  (and so  is injective, and thus bijective). So if  is the permutation matrix of  then we have , and:
  

  And so  and by an argument identical to the above, with appropriate choice of , this layer does not satisfy the requirements of an exchangeable tensor layer. This completes the proof. 
\end{proof}

\subsection{Proof of Theorem \ref{thm:pe_layer}}
A simple reparameterization allows us to write the matrix  of (\ref{eqn:params_tensor}) in the form of (\ref{eq:layer_simple_mat}). Thus Theorem \ref{thm:pe_layer} is just the special case of Theorem \ref{thm:equivar} where  and the proof follows from that. 

\section{Details of Architecture and Training}

\textbf{\textsc{Self-Supervised Model.}}
Details of architecture and training: we train a simple feed-forward network with 9 exchangeable matrix layers using a leaky ReLU activation function. Each hidden layer has 256 channels and we apply a channel-wise dropout with probability 0.5 after the first to seventh layers. We found this channel-wise dropout to be crucial to achieving good performance. Before the input layer we mask out a proportion of the ratings be setting their values to 0 uniformly at random with probability 0.15. We convert the input ratings to one-hot vectors and interpret the model output as a probability distribution over potential rating levels. We tuned hyper-parameters by training on 75\% of the data, evaluating on a 5\% validation set. We test this model using the canonical u1.base/u1.test training/test split, which reserves 20\% of the ratings for testing.
For the MovieLens-1M dataset, we use the same architecture as for ML-100k and trained on 85\% of the data, validating on 5\%, and reserving 10\% for testing. The limited size of GPU memory becomes an issue for this larger dataset, so we had to employ conditional sampling for training. At validation time we used full batch predictions using the CPU in order to avoid memory issues.

\textbf{\textsc{Factorized Exchangeable Autoencoder Model.}}
Details of architecture and training: we use three exchangeable matrix layers for the encoder. The first two have 220 channels, and the third layer maps the input to 100 features for each entry, with no activation function applied. This is followed by mean pooling along both dimensions of the input. Thus, each user and movie is encoded into a length 100 vector of real-valued latent features. The decoder uses five similar exchangeable matrix layers, with the final layer having five channels. We apply a channel-wise dropout with probability 0.5 after the third and fourth layers, which we again found to be crucial for good performance. We convert the input ratings to one-hot vectors and optimize using cross-entropy loss.

\end{document}
