\documentclass[english]{article}
\usepackage[latin9]{inputenc}
\usepackage{array}
\usepackage{float}
\usepackage{textcomp}
\usepackage{mathrsfs}
\usepackage{multirow}
\usepackage{tablefootnote}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{stackrel}
\usepackage{graphicx}

\makeatletter

\providecommand{\tabularnewline}{\\}
\floatstyle{ruled}
\newfloat{algorithm}{tbp}{loa}
\providecommand{\algorithmname}{Algorithm}
\floatname{algorithm}{\protect\algorithmname}

\usepackage{algorithm,algpseudocode}
\usepackage{amsmath}
\algnewcommand\algorithmicforeach{\textbf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}
\DeclareMathOperator*{\argmax}{arg\,max}
\usepackage{iclr2020_conference}
\iclrfinalcopy 
\renewcommand{\cite}{\citep}
\usepackage{hyperref} 
\addtolength{\belowcaptionskip}{-1mm}

\makeatother

\usepackage{babel}
\begin{document}
\title{Neural Stored-program Memory}
\author{Hung Le, Truyen Tran and Svetha Venkatesh\\
Applied AI Institute, Deakin University, Geelong, Australia\\
\texttt{\{lethai,truyen.tran,svetha.venkatesh\}@deakin.edu.au}}
\maketitle
\begin{abstract}
Neural networks powered with external memory simulate computer behaviors.
These models, which use the memory to store data for a neural controller,
can learn algorithms and other complex tasks. In this paper, we introduce
a new memory to store \emph{weights} for the controller, analogous
to the stored-program memory in modern computer architectures. The
proposed model, dubbed Neural Stored-program Memory, augments current
memory-augmented neural networks, creating differentiable machines
that can switch programs through time, adapt to variable contexts
and thus resemble the Universal Turing Machine. A wide range of experiments
demonstrate that the resulting machines not only excel in classical
algorithmic problems, but also have potential for compositional, continual,
few-shot learning and question-answering tasks. 
 \end{abstract}

\section{Introduction}

Recurrent Neural Networks (RNNs) are Turing-complete \cite{siegelmann1995computational}.
However, in practice RNNs struggle to learn simple procedures as they
lack explicit memory \cite{graves2014neural,mozer1993connectionist}.
These findings have sparked a new research direction called Memory
Augmented Neural Networks (MANNs) that emulate modern computer behavior
by detaching memorization from computation via memory and controller
network, respectively. MANNs have demonstrated significant improvements
over memory-less RNNs in various sequential learning tasks \cite{graves2016hybrid,le2018variational,NIPS2015_5846}.
Nonetheless, MANNs have barely simulated general-purpose computers.

Current MANNs miss a key concept in computer design: stored-program
memory. The concept has emerged from the idea of Universal Turing
Machine (UTM) \cite{turing1936} and further developed in Harvard
Architecture \cite{BROESCH2009135}, Von Neumann Architecture \cite{vonNeumann:1993:FDR:612487.612553}.
In UTM, both data and programs that manipulate the data are stored
in memory. A control unit then reads the programs from the memory
and executes them with the data. This mechanism allows flexibility
to perform universal computations. Unfortunately, current MANNs such
as Neural Turing Machine (NTM) \cite{graves2014neural}, Differentiable
Neural Computer (DNC) \cite{graves2016hybrid} and Least Recently
Used Access (LRUA) \cite{santoro2016meta} only support memory for
data and embed a single program into the controller network, which
goes against the stored-program memory principle. 

Our goal is to advance a step further towards UTM by coupling a MANN
with an external program memory. The program memory co-exists with
the data memory in the MANN, providing more flexibility, reuseability
and modularity in learning complicated tasks. The program memory stores
the weights of the MANN's controller network, which are retrieved
quickly via a key-value attention mechanism across timesteps yet updated
slowly via backpropagation. By introducing a meta network to moderate
the operations of the program memory, our model, henceforth referred
to as Neural Stored-program Memory (NSM), can learn to switch the
programs/weights in the controller network appropriately, adapting
to different functionalities aligning with different parts of a sequential
task, or different tasks in continual and few-shot learning.

To validate our proposal, the NTM armed with NSM, namely Neural Universal
Turing Machine (NUTM), is tested on a variety of synthetic tasks including
algorithmic tasks from \citet{graves2014neural}, composition of algorithmic
tasks and continual procedure learning. For these algorithmic problems,
we demonstrate clear improvements of NUTM over NTM. Further, we investigate
NUTM in few-shot learning by using LRUA as the MANN and achieve notably
better results. Finally, we expand NUTM application to linguistic
problems by equipping NUTM with DNC core and achieve competitive performances
against state-of-the-arts in the bAbI task \cite{weston2015towards}. 

Taken together, our study advances neural network simulation of Turing
Machines to neural architecture for Universal Turing Machines. This
develops a new class of MANNs that can store and query both the weights
and data of their own controllers, thereby following the stored-program
principle. A set of five diverse experiments demonstrate the computational
universality of the approach.
 
\section{Background\label{sec:Background}}

In this section, we briefly review MANN and its relations to Turing
Machines. A MANN consists of a controller network and an external
memory , which is a collection
of  -dimensional vectors. The controller network is responsible
for accessing the memory, updating its state and optionally producing
output at each timestep. The first two functions are executed by an
interface network and a state network\footnote{Some MANNs (e.g., NTM with Feedforward Controller) neglect the state
network, only implementing the interface network and thus analogous
to one-state Turing Machine. }, respectively. Usually, the interface network is a Feedforward neural
network whose input is  - the output of the state network
implemented as RNNs. Let  denote the weight of the interface
network, then the state update and memory control are as follows,

\begin{minipage}[t]{0.5\textwidth}
\end{minipage}\begin{minipage}[t]{0.5\textwidth}
\end{minipage}

where  and  are data from current input and the
previous memory read, respectively. The interface vector 
then is used to read from and write to the memory . We
use a generic notation  to
represent these memory operations that either update or retrieve read
value  from the memory. To support multiple memory accesses
per step, the interface network may produce multiple interfaces, also
known as control heads. Readers are referred to App. \ref{subsec:Example-of-memory}
and \citet{graves2014neural,graves2016hybrid,santoro2016meta} for
details of memory read/write examples. 

A deterministic one-tape Turing Machine can be defined by 4-tuple
, in which  is finite set
of states,  is an initial state,  is finite
set of symbol stored in the tape (the data) and  is the transition
function (the program), .
At each step, the machine performs the transition function, which
takes the current state and the read value from the tape as inputs
and outputs actions including writing new values, moving tape head
to new location (left/right) and jumping to another state. Roughly
mapping to current MANNs, ,  and  map to the
set of the controller states, the read values and the controller network,
respectively. Further, the function  can be factorized into
two sub functions: 
and , which correspond to the interface
and state networks, respectively. 

By encoding a Turing Machine into the tape, one can build a UTM that
simulates the encoded machine \cite{turing1936}. The transition function
 of the UTM queries the encoded Turing Machine that solves
the considering task. Amongst 4 tuples,  is the most important
and hence uses most of the encoding bits. In other words, if we assume
that the space of ,  and  are shared amongst
Turing Machines, we can simulate any Turing Machine by encoding only
its transition function . Translating to neural language,
if we can store the controller network into a queriable memory and
make use of it, we can build a Neural Universal Turing Machine. Using
NSM is a simple way to achieve this goal, which we introduce in the
subsequent section.
 
\section{Methods}


\subsection{Neural Stored-program Memory}

A Neural Stored-program Memory (NSM) is a key-value memory ,
whose values are the basis weights of another neural networkthe
programs. , , and  are the number of programs, the key
space dimension and the program size, respectively. This concept is
a hybrid between the traditional slow-weight and fast-weight \cite{hinton1987using}.
Like slow-weight, the keys and values in NSM are updated gradually
by backpropagation. However, the values are dynamically interpolated
to produce the working weight on-the-fly during the processing of
a sequence, which resembles fast-weight computation. Let us denote
 and 
as the key and the program of the -th memory slot. At timestep
 given a query key , the working program is retrieved
as follows,






where  is cosine similarity and 
is the scalar program strength parameter. The vector working program
 is then reshaped to its matrix form and ready to be used
as the weight of other neural networks. 

The key-value design is essential for convenient memory access as
the size of the program stored in  can be millions
of dimensions and thus, direct content-based addressing as in \citet{graves2014neural,graves2016hybrid,santoro2016meta}
is infeasible. More importantly, we can inject external control on
the behavior of the memory by imposing constraints on the key space.
For example, program collapse will happen when the keys stored in
the memory stay close to each other. When this happens,  is
a balanced mixture of all programs regardless of the query key and
thus having multiple programs is useless. We can avoid this phenomenon
by minimizing a regularization loss defined as the following,




\subsection{Neural Universal Turing Machine\label{subsec:Neural-Universal-Turing}}

It turns out that the combination of MANN and NSM approximates a Universal
Turing Machine (Sec. \ref{sec:Background}). At each timestep, the
controller in MANN reads its state and memory to generate control
signal to the memory via the interface network , then updates
its state using the state network . Since the parameters of
 and  represent the encoding of , we should
store both into NSM to completely encode an MANN. For simplicity,
in this paper, we only use NSM to store , which is equivalent
to the Universal Turing Machine that can simulate any one-state Turing
Machine. 

In traditional MANN,  is constant across timesteps and only
updated slowly during training, typically through backpropagation.
In our design, we compute  from NSM for every timestep
and thus, we need a program interface networkthe meta network
that generates an interface vector for
the program memory: ,
where . Together
with the ,  simulates 
of the UTM and is implemented as a Feedforward neural network. The
procedure for computing  is executed by following Eqs.
(\ref{eq:d_p})-(\ref{eq:pt}), hereafter referred to as .
Figure \ref{fig:NUTM-diagram} depicts the integration of NSM into
MANN.

In this implementation, key-value NSM offers a more flexible learning
scheme than direct attention, in which the meta-network can generate
the weight  directly without matching  with
. That is, only the meta-network
learns the mapping from context  to program. When it falls
into some local-minima (generating suboptimal ), the meta-network
struggles to escape. In our proposal, together with the meta-network,
the memory keys are learnable. When the memory keys are slowly updated,
the meta-network will shift its query key generation to match the
new memory keys and possibly escape from the local-minima. 

\begin{figure}
\begin{centering}
\includegraphics[width=0.9\columnwidth]{fig/nutm2}
\par\end{centering}
\caption{Introducing NSM into MANN. At each timestep, the program interface
network () receives input from the state
network and queries the program memory , acquiring
the working weight for the interface network (). The interface
network then operates on the data memory . \label{fig:NUTM-diagram}}
\end{figure}
For the case of multi-head NTM, we implement one NSM per control head
and name this model Neural Universal Turing Machine (NUTM). One NSM
per head is to ensure programs for one head do not interfere with
other heads and thus, encourage functionality separation amongst heads.
Each control head will read from (for read head) or write to (for
write head) the data memory  via 
as described in \citet{graves2014neural} . It should be noted that
using multiple heads is unlike using multiple controllers per head.
The former increases the number of accesses to the data memory at
each timestep and employs a fixed controller to compute multiple heads,
which may improve capacity yet does not enable adaptability. On the
contrary, the latter varies the property of each memory access across
timesteps by switching the controllers and thus potential for adaptation. 

Other MANNs such as DNC \cite{graves2016hybrid} and LRUA \cite{santoro2016meta}
can be armed with NSM in this manner. We also employ the regularization
loss  to prevent the programs from collapsing, resulting in
a final loss as follows,


where  is the prediction loss and  is annealing
factor, reducing as the training step increases. The details of NUTM
operations are presented in Algorithm \ref{alg:Neural-Uinversal-Turing}. 

\begin{algorithm}[t]
\begin{algorithmic}[1]
\Require{a sequence , a data memory  and  program memories  corresponding to  control heads}
\State{Initilize , }
\For{}
\State{} \Comment{ can be replaced by GRU/LSTM}
\For{}
\State{Compute the program interface }
\State{Compute the program }
\State{Compute the data interface }
\State{\parbox[t]{0.9\linewidth}{Read  from memory  (if read head) or update memory  (if write head) using }}
\EndFor
\State{}
\EndFor
\end{algorithmic} 

\caption{Neural Universal Turing Machine\label{alg:Neural-Uinversal-Turing}}
\end{algorithm}

\subsection{On the Benefit of NSM to MANN: An Explanation from Multilevel Modeling\label{subsec:On-the-Benefit}}

Learning to access memory is a multi-dimensional regression problem.
Given the input , which is derived from the state 
of the controller, the aim is to generate a correct interface vector
 via optimizing the interface network. Instead of searching
for one transformation that maps the whole space of  to the
optimal space of , NSM first partitions the space of 
into subspaces, then finds multiple transformations, each of which
covers subspace of . The program interface network 
is a meta learner that routes  to the appropriate transformation,
which then maps  to the  space. This is analogous
to multilevel regression in statistics \cite{andrew2006mr}. Practical
studies have shown that multilevel regression is better than ordinary
regression if the input is clustered \cite{cohen2014applied,huang2018multilevel}. 

RNNs have the capacity to learn to perform finite state computations
\cite{casey1996dynamics,tivno1998finite}. The states of a RNN must
be grouped into partitions representing the states of the generating
automaton. As Turing Machines are finite state automata augmented
with an external memory tape, we expect MANN, if learnt well, will
organize its state space clustered in a way to reflect the states
of the emulated Turing Machine. That is,  as well as 
should be clustered. We realize that NSM helps NTM learn better clusterization
over this space (see App. \ref{subsec:Clustering-on-The}), thereby
improving NTM's performances. 
 
\section{Results}


\subsection{NTM Single Tasks\label{subsec:NTM-Single-Tasks}}

\begin{figure}
\begin{centering}
\includegraphics[width=1\textwidth]{fig/learning_curves/ntm_tasks_abl3}
\par\end{centering}
\caption{Learning curves on NTM tasks (a) and Associative Recall (AR) ablation
study (b). Only mean is plotted in (b) for better visualization.\label{fig:Learning-curves-on}}
\end{figure}
\begin{table}
\begin{centering}
\begin{tabular}{c|cccccc}
\hline 
Task & Copy & R. Copy & A. Recall & D. N-grams & P. Sort & L. Copy\tabularnewline
\hline 
NTM & \textbf{0.00} & 405.10 & 7.66 & 132.59 & 24.41 & 16.04\tabularnewline
NUTM (p=2) & \textbf{0.00} & \textbf{366.69} & \textbf{1.35} & \textbf{127.68} & \textbf{20.00} & \textbf{0.02}\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
~

\caption{Generalization performance of best models measured in average bit
error per sequence (lower is better). For each task, we pick 1,000
longer sequences as test data. \label{tab:Generalisation-performance-of}}
\end{table}
In this section, we investigate the performance of NUTM on algorithmic
tasks introduced in \citet{graves2014neural} : Copy, Repeat Copy,
Associative Recall, Dynamic N-Grams and Priority Sort. Besides these
five NTM tasks, we add another task named Long Copy which doubles
the length of training sequences in the Copy task. In these tasks,
the model will be fed a sequence of input items and is required to
infer a sequence of output items. Each item is represented by a binary
vector.

In the experiment, we compare two models: NTM\footnote{For algorithmic tasks, we choose NTM as the only baseline as NTM is
known to perform and generalize well on these tasks. If NSM can help
NTM in these tasks, it will probably help other MANNs as well.} and NUTM with two programs. Although the tasks are atomic, we argue
that there should be at least two memory manipulation schemes across
timesteps, one for encoding the inputs to the memory and another for
decoding the output from the memory. The two models are trained with
cross-entropy objective function under the same setting as in \citet{graves2014neural}
. For fair comparison, the controller hidden dimension of NUTM is
set smaller to make the total number of parameters of NUTM equivalent
to that of NTM. The number of memory heads for both models are always
equal and set to the same value as in the original paper (details
in App. \ref{subsec:Details-on-Synthetic}).

We run each experiments five times and report the mean with error
bars of training losses for NTM tasks in Fig. \ref{fig:Learning-curves-on}
(a). Except for the Copy task, which is too simple, other tasks observe
convergence speed improvement of NUTM over that of NTM, thereby validating
the benefit of using two programs across timesteps even for the single
task setting. NUTM requires fewer training samples to converge and
it generalizes better to unseen sequences that are longer than training
sequences. Table \ref{tab:Generalisation-performance-of} reports
the test results of the best models chosen after five runs and confirms
the outperformance of NUTM over NTM for generalization. 

To illustrate the program usage, we plot NUTM's program distributions
across timesteps for Repeat Copy and Priority Sort in Fig. \ref{fig:Memory-read-(a,c,d)/write}
(a) and (b), respectively. Examining the read head for Repeat Copy,
we observe two program usage patterns corresponding to the encoding
and decoding phases. As there is no reading in encoding, NUTM assigns
the ``no-read'' strategy mainly to the ``orange program''. In
decoding, the sequential reading is mostly done by the ``blue program''
with some contributions from the ``orange program'' when resetting
reading head. Similar behaviors can be found in the write head for
Priority Sort. While the encoding ``fitting writing'' (see \citet{graves2014neural}
for explanation on the strategy) is often executed by the ``blue
program'', the decoding writing is completely taken by the ``orange''
program (more visualizations in App. \ref{subsec:Program-Usage-Visualizations}). 

\begin{figure}
\begin{centering}
\includegraphics[width=0.95\linewidth]{fig/visualizations/single_ex_rcp3}
\par\end{centering}
\caption{(a,b,c) visualizes NUTM's executions in synthetic tasks: the upper
rows are memory read (left)/write (right) locations; the lower rows
are program distributions over timesteps. The green line indicates
the start of the decoding phase. (d) visualizes perseveration in NTM:
the upper row are input, output, predicted output with errors (orange
bits); the lower row is reading location. \label{fig:Memory-read-(a,c,d)/write}}
\end{figure}

\subsection{Ablation study on Associative Recall}

In this section, we conduct an ablation study on Associative Recall
(AR) to validate the benefit of proposed components that constitute
NSM. We run the task with three additional baselines: NUTM using direct
attention (DA), NUTM using key-value without regularization (KV),
NUTM using fixed, uniform program distribution (UP) and a vanilla
NTM with 2 memory heads (). The meta-network 
in DA generates the attention weight  directly. The KV
employs key-value attention yet excludes the regularization loss presented
in Eq. (\ref{eq:l_p}). The training curves over 5 runs are plotted
in Fig. \ref{fig:Learning-curves-on} (b). The results demonstrate
that DA exhibits fast yet shallow convergence. It tends to fall into
local minima, which finally fails to reach zero loss. Key-value attention
helps NUTM converge completely with fewer iterations. The performance
is further improved with the proposed regularization loss. UP underperforms
NUTM as it lacks dynamic programs. The NTM with 2 heads shows slightly
better convergence compared to the NTM, yet obviously underperforms
NUTM () with 1 head and fewer parameters. This validates our
argument on the difference between using multiple heads and multiple
programs (Sec. \ref{subsec:Neural-Universal-Turing}). 


\subsection{NTM Sequencing Tasks}

In neuroscience, sequencing tasks test the ability to remember a series
of tasks and switch tasks alternatively \cite{hal2019neur}. A dysfunctional
brain may have difficulty in changing from one task to the next and
get stuck in its preferred task (perseveration phenomenon). To analyze
this problem in NTM, we propose a new set of experiments in which
a task is generated by sequencing a list of subtasks. The set of subtasks
is chosen from the NTM single tasks (excluding Dynamic N-grams for
format discrepancy) and the order of subtasks in the sequence is dictated
by an indicator vector put at the beginning of the sequence. Amongst
possible combinations of subtasks, we choose \{Copy, Repeat Copy\}(C+RC),
\{Copy, Associative Recall\} (C+AR), \{Copy, Priority Sort\} (C+PS)
and all (C+RC+AC+PS)\footnote{We focus on the combinations that contain Copy as Copy is the only
task where NTM reach NUTM's performance. If NTM fails in these combinations,
it will most likely fail in others.}. The learner observes the order indicator followed by a sequence
of subtasks' input items and is requested to consecutively produce
the output items of each subtasks. 

As shown in Fig. \ref{fig:Learning-curves-on-1}, some tasks such
as Copy and Associative Recall, which are easy to solve if trained
separately, become unsolvable by NTM when sequenced together. One
reason is NTM fails to change the memory access behavior (perseveration).
For examples, NTM keeps following repeat copy reading strategy for
all timesteps in C+RC task (Fig. \ref{fig:Memory-read-(a,c,d)/write}
(d)). Meanwhile, NUTM can learn to change program distribution when
a new subtask appears in the sequence and thus ensure different accessing
strategy per subtask (Fig. \ref{fig:Memory-read-(a,c,d)/write} (c)).

\begin{figure}
\begin{centering}
\includegraphics[width=0.95\linewidth]{fig/learning_curves/ntm_tasks_mix2}
\par\end{centering}
\caption{Learning curves on sequencing NTM tasks.\label{fig:Learning-curves-on-1}}
\end{figure}

\subsection{Continual Procedure Learning}

In continual learning, catastrophic forgetting happens when a neural
network quickly forgets previously acquired skills upon learning new
skills \cite{french1999catastrophic}. In this section, we prove the
versatility of NSM by showing that a naive application of NSM without
much modification can help NTM to mitigate catastrophic forgetting.
We design an experiment similar to the Split MNIST \cite{zenke2017continual}
to investigate whether NSM can improve NTM's performance. In our experiment,
we let the models see the training data from the 4 tasks: Copy (C),
Repeat Copy (RC), Associative Recall (AR) and Priority Sort (PS),
consecutively in this order. Each task is trained in 20,000 iterations
with batch size 16 (see App. \ref{subsec:Details-on-Synthetic} for
task details). To encourage NUTM to spend exactly one program per
task while freezing others, we force ``hard'' attention over the
programs by replacing the softmax function in Eq. \ref{eq:pt} with
the Gumbel-softmax \cite{jang2016categorical}. Also, to ignore catastrophic
forgetting in the state network, we use Feedforward controllers in
the two baselines.

After finishing one task, we evaluate the bit accuracy measured
by (bit error per sequence/total bits per sequence) over 4 tasks.
As shown in in Fig. \ref{fig:Mean-bit-accuracy}, NUTM outperforms
NTM by a moderate margin (10-40\% per task). Although NUTM also experiences
catastrophic forgetting, it somehow preserves some memories of previous
tasks. Especially, NUTM keeps performing perfectly on Copy even after
it learns Repeat Copy. For other dissimilar task transitions, the
performance drops significantly, which requires more effort to bring
NSM to continual learning. 

\begin{figure}
\centering{}\includegraphics[width=0.95\linewidth]{fig/learning_curves/cpl2}\caption{Mean bit accuracy with error bars for the continual algorithmic tasks.
Each of the first four panels show bit accuracy on four tasks after
finishing a task. The rightmost shows the average accuracy.\label{fig:Mean-bit-accuracy}}
\end{figure}

\subsection{Few-shot Learning}

Few-shot learning or meta learning tests the ability to rapidly adapt
within a task while gradually capturing the way the task structure
varies \cite{thrun1998lifelong}. By storing sample-class bindings,
MANNs are capable of classifying new data after seeing only few samples
 \cite{santoro2016meta}. As NSM gives flexible memory controls, it
makes MANN more adaptive to changes and thus perform better in this
setting. To verify that, we apply NSM to the LRUA memory and follow
the experiments introduced in \citet{santoro2016meta} , using the
Omniglot dataset to measure few-shot classification accuracy. The
dataset includes images of 1623 characters, with 20 examples of each
character. During training, a sequence (episode) of images are randomly
selected from  classes of characters in the training set (1200
characters), where  corresponding to sequence length of 50,
75, respectively. Each class is assigned a random label which shuffles
between episodes and is revealed to the models after each prediction.
After 100,000 episodes of training, the models are tested with unseen
images from the testing set (423 characters). The two baselines are
MANN and NUTM (both use LRUA core). For NUTM, we only tune  and
pick the best values:  and  for 5 classes and 10 classes,
respectively. 

Table \ref{tab:meta} reports the classification accuracy when the
models see characters for the second, third and fifth time. NUTM generally
achieves better results than MANN, especially when the number of classes
increases, demanding more adaptation within an episode. For the persistent
memory mode, which demands fast forgetting old experiences in previous
episodes, NUTM outperforms MANN significantly (10-20\%)\footnote{It should be noted that our goal was not to achieve state of the art
performance on this dataset. It was to exhibit the benefit of NSM
to MANN. Compared to current methods, the MANN and NUTM used in our
experiments do not use CNN to extract visual features, thus achieve
lower accuracy than recent state-of-the-arts. }. Readers are referred to App. \ref{subsec:Details-on-Few-shot} for
more details on learning curves and more results of the models. 

\begin{table}
\begin{centering}
\begin{tabular}{l|c|ccc|ccc}
\hline 
\multirow{2}{*}{Model} & Persistent & \multicolumn{3}{c|}{5 classes} & \multicolumn{3}{c}{10 classes}\tabularnewline
 & memory\tablefootnote{If the memory is not artificially erased between episodes, it is called
persistent. This mode is hard for the case of 5 classes as shown in
\cite{santoro2016meta} } &  &  &  &  &  & \tabularnewline
\hline 
MANN (LRUA){*} & No & 82.8 & 91.0 & 94.9 & - & - & -\tabularnewline
MANN (LRUA) & No & 82.3 & 88.7 & 92.3 & 52.7 & 60.6 & 64.7\tabularnewline
NUTM (LRUA) & No & \textbf{85.7} & \textbf{91.3} & \textbf{95.5} & \textbf{68.0} & \textbf{78.1} & \textbf{82.8}\tabularnewline
\hline 
MANN (LRUA) & Yes & 66.2 & 73.4 & 81.0 & 51.3 & 59.2 & 63.3\tabularnewline
NUTM (LRUA) & Yes & \textbf{77.8} & \textbf{85.8} & \textbf{89.8} & \textbf{69.0} & \textbf{77.9} & \textbf{82.7}\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
~

\caption{Test-set classification accuracy (\%) on the Omniglot dataset after
100,000 episodes of training. {*} denotes available results from \cite{santoro2016meta}.}
\end{table}

\subsection{Text Question Answering }

Reading comprehension typically involves an iterative process of multiple
actions such as reading the story, reading the question, outputting
the answers and other implicit reasoning steps \cite{weston2015towards}.
We apply NUTM to the question answering domain by replacing the NTM
core with DNC \cite{graves2016hybrid}. Compared to NTM's sequential
addressing, dynamic memory addressing in DNC is more powerful and
thus suitable for NSM integration to solve non-algorithmic problems
such as question answering. Following previous works of DNC, we use
bAbI dataset \cite{weston2015towards} to measure the performance
of the NUTM with DNC core (three variants ,  and ).
In the dataset, each story is followed by a series of questions and
the network reads all word by word, then predicts the answers. Although
synthetically generated, bAbI is a good benchmark that tests 20 aspects
of natural language reasoning including complex skills such as induction
and counting, 

We found that increasing number of programs helps NUTM improve performance.
In particular, NUTM with 4 programs, after 50 epochs jointly trained
on all 20 question types, can achieve a mean test error rate of 3.3\%
and manages to solve 19/20 tasks (a task is considered solved if its
error \textless 5\%). The mean and s.d. across 10 runs are also compared
with other results reported by recent works (see Table \ref{tab:Mean-bAbI-error}).
Excluding baselines under different setups, our result is the best
reported mean result on bAbI that we are aware of. More details are
described in App. \ref{subsec:Details-on-bAbI}. 

\begin{table}
\begin{centering}
\begin{tabular}{ll}
\hline 
\multicolumn{1}{l}{Model} & Error\tabularnewline
\hline 
\multicolumn{1}{l}{DNC\cite{graves2016hybrid}} & 16.7 \textpm{} 7.6 \tabularnewline
\multicolumn{1}{l}{SDNC\cite{rae2016scaling}} & 6.4 \textpm{} 2.5 \tabularnewline
\multicolumn{1}{l}{ADNC\cite{W18-2606}} & 6.3 \textpm{} 2.7 \tabularnewline
\multicolumn{1}{l}{DNC-MD\cite{csordas2018improving}} & 9.5 \textpm{} 1.6\tabularnewline
\hline 
NUTM (DNC core, p=1) & 9.7 \textpm{} 3.5\tabularnewline
NUTM (DNC core, p=2) & 7.5 \textpm{} 1.6\tabularnewline
NUTM (DNC core, p=4) & \textbf{5.6 \textpm{} 1.9}\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
~

\caption{Mean and s.d. for bAbI error ().\label{tab:Mean-bAbI-error}}

\end{table}

 
\section{Related Work}

Previous investigations into MANNs mostly revolve around memory access
mechanisms. The works in \citet{graves2014neural,graves2016hybrid}
introduce content-based, location-based and dynamic memory reading/writing.
Further, \citet{rae2016scaling} scales to bigger memory by sparse
access; \citet{le2018learning} optimizes memory operations with uniform
writing; and MANNs with extra memory have been proposed \cite{Le:2018:DMN:3219819.3219981}.
However, these works keep using memory for storing data rather than
the weights of the network and thus parallel to our approach. Other
DNC modifications \cite{csordas2018improving,W18-2606} are also orthogonal
to our work. 

Another line of related work involves modularization of neural networks,
which is designed for visual question answering. In module networks
\cite{andreas2016neural,andreas-etal-2016-learning}, the modules
are manually aligned with predefined concepts and the order of execution
is decided by the question. Although the module in these works resembles
the program in NSM, our model is more generic and flexible with soft-attention
over programs and thus fully differentiable. Further, the motivation
of NSM does not limit to a specific application. Rather, NSM aims
to help MANN reach general-purpose computability. 

If we view NSM network as a dynamic weight generator, the program
in NSM can be linked to fast weight \cite{cogprints1380,hinton1987using,schmidhuber1993self}.
These papers share the idea of using different weights across timesteps
to enable dynamic adaptation. Using outer-product is a common way
to implement fast-weight \cite{schmidhuber1993reducing,ba2016using,schlag2017gated}.
These fast weights are directly generated and thus different from
our programs, which are interpolated from a set of slow weights. 

Tensor/Multiplicative RNN \cite{sutskever2011generating} and Hypernetwork
\cite{ha2016hypernetworks} are also relevant related works. These
methods attempt to make the working weight of RNNs dependent on the
input to enable quick adaption through time. Nevertheless, they do
not support modularity. In particular, Hypernetwork generates scaling
factors for the single weight of the main RNN. It does not aim to
use multiple slow-weights (programs) and thus, different from our
approach. Tensor RNN is closer to our idea when the authors propose
to store  slow-weights, where  is the number of input dimension,
which is acknowledged impractical. Unlike our approach, they do not
use a meta-network to generate convex combinations amongst weights.
Instead, they propose Multiplicative RNN that factorizes the working
weight to product of three matrices, which looses modularity. On the
contrary, we explicitly model the working weight as an interpolation
of multiple programs and use a meta-network to generate the coefficients.
This design facilitates modularity because each program is trained
towards some functionality and can be switched or combined with each
other to perform the current task. Last but not least, while the related
works focus on improving RNN with fast-weight, we aim to reach a neural
simulation of Universal Turing Machine, in which fast-weight is a
way to implement stored-program principle. 
 
\section{Conclusions}

This paper introduces the Neural Stored-program Memory (NSM), a new
type of external memory for neural networks. The memory, which takes
inspirations from the stored-program memory in computer architecture,
gives memory-augmented neural networks (MANNs) flexibility to change
their control programs through time while maintaining differentiability.
The mechanism simulates modern computer behavior, potential making
MANNs truly neural computers. Our experiments demonstrated that when
coupled with our model, the Neural Turing Machine learns algorithms
better and adapts faster to new tasks at both sequence and sample
levels. When used in few-shot learning, our method helps MANN as well.
We also applied the NSM to the Differentiable Neural Computer and
observed a significant improvement, reaching the state-of-the-arts
in the bAbI task. Although this paper limits to MANN integration,
other neural networks can also reap benefits from our proposed model,
which will be explored in future works.
 
\bibliographystyle{iclr2020_conference}
\bibliography{nva}

\newpage{}

\section*{Appendix}

\renewcommand\thesubsection{\Alph{subsection}}


\subsection{Clustering on The Latent Space\label{subsec:Clustering-on-The}}

As previously mentioned in Sec. 3.3, MANN should let its states form
clusters to well-simulate Turing Machine. Fig. \ref{fig:Visualisation-of-the}
(a) and (c) show NTM actually organizes its  space into clusters
corresponding to processing states (e.g, encoding and decoding). NUTM,
which explicitly partitions this space, clearly learn better clusters
of  (see Fig. \ref{fig:Visualisation-of-the} (b) and (d)).
This contributes to NUTM's outperformance over NTM. 

\begin{figure}[H]
\begin{centering}
\includegraphics[width=0.95\linewidth]{fig/visualizations/hspace_copy_all}
\par\end{centering}
\caption{Visualization of the first two principal components of  space
in NTM (a,c) and NUTM (b,d) for Copy (red) and Repeat Copy (blue).
Fader color denotes lower timestep in a sequence. Both can learn clusters
of hidden states yet NUTM exhibits clearer partition. \label{fig:Visualisation-of-the} }
\end{figure}

\subsection{Program Usage Visualizations \label{subsec:Program-Usage-Visualizations}}

\ref{subsec:Visualization-on-program} and \ref{subsec:Visualization-on-program-1}
visualize the best inferences of NUTM on test data from single and
sequencing tasks. Each plot starts with the input sequence and the
predicted output sequence with error bits in the first row\footnote{Normally, black is bit 0, white is bit 1 in vector data. Orange is
prediction error. In tasks including priority sort, because data vectors
not only include value 0-1, but also other float values (e.g., priority
score), the color scale is automatically changed. Basically, error
bit is given darker color than 0 and lighter color than 1. For example,
in priority sort task, yellow is prediction error, and orange is bit
1.}. The second and fourth rows depict the read and write locations on
data memory, respectively. The third and fifth rows depict the program
distribution of the read head and write head, respectively. \ref{subsec:Perseveration-phenomenon-in}
visualizes random failed predictions of NTM on sequencing tasks. The
plots follow previous pattern except for the program distribution
rows. 

\subsubsection{Visualization on program distribution across timesteps (single tasks)\label{subsec:Visualization-on-program}}

\begin{figure}[H]
\begin{centering}
\includegraphics[width=1\linewidth]{fig/visualizations/copy_pw}
\par\end{centering}
\caption{Copy (p=2).}

\end{figure}
\begin{figure}[H]
\begin{centering}
\includegraphics[width=1\linewidth]{fig/visualizations/repeatcopy_pw}
\par\end{centering}
\caption{Repeat Copy (p=2).}
\end{figure}
\begin{figure}[H]
\begin{centering}
\includegraphics[width=1\linewidth]{fig/visualizations/associative_pw}
\par\end{centering}
\caption{Associative Recall (p=2).}
\end{figure}
\begin{figure}[H]
\begin{centering}
\includegraphics[width=1\linewidth]{fig/visualizations/ngram_pw}
\par\end{centering}
\caption{Dynamic N-grams (p=2).}
\end{figure}
\begin{figure}[H]
\begin{centering}
\includegraphics[width=1\linewidth]{fig/visualizations/prioritysort_pw}
\par\end{centering}
\caption{Priority Sort (p=2).}
\end{figure}
\begin{figure}[H]
\begin{centering}
\includegraphics[width=1\linewidth]{fig/visualizations/copy_long_pw}
\par\end{centering}
\caption{Long Copy (p=2).}
\end{figure}

\subsubsection{Visualization on program distribution across timesteps (sequencing
tasks)\label{subsec:Visualization-on-program-1}}

\begin{figure}[H]
\begin{centering}
\includegraphics[width=1\linewidth]{fig/visualizations/mix_cp_repeatcp_pw}
\par\end{centering}
\caption{Copy+Repeat Copy (p=3).}
\end{figure}
\begin{figure}[H]
\begin{centering}
\includegraphics[width=1\linewidth]{fig/visualizations/mix_cp_arecall_pw}
\par\end{centering}
\caption{Copy+Associative Recall (p=3).}
\end{figure}
\begin{figure}[H]
\begin{centering}
\includegraphics[width=1\linewidth]{fig/visualizations/mix_cp_psort_pw}
\par\end{centering}
\caption{Copy+Priority Sort (p=3).}
\end{figure}
\begin{figure}[H]
\begin{centering}
\includegraphics[width=1\linewidth]{fig/visualizations/mix_cp_rco_arecall_psort_pw3}
\par\end{centering}
\caption{Copy+Repeat Copy+Associative Recall+Priority Sort (p=4).}
\end{figure}

\subsubsection{Perseveration phenomenon in NTM (sequencing tasks)\label{subsec:Perseveration-phenomenon-in}}

\begin{figure}[H]
\begin{centering}
\includegraphics[width=1\linewidth]{fig/visualizations/mix_cp_repeatcp_pw2}
\par\end{centering}
\caption{Copy+Repeat Copy perseveration (only Repeat Copy).}
\end{figure}
\begin{figure}[H]
\begin{centering}
\includegraphics[width=1\linewidth]{fig/visualizations/mix_cp_arecall_pw2}
\par\end{centering}
\caption{Copy+Associative Recall perseveration (only Copy).}
\end{figure}
\begin{figure}[H]
\begin{centering}
\includegraphics[width=1\linewidth]{fig/visualizations/mix_cp_psort_pw2}
\par\end{centering}
\caption{Copy+Priority Sort perseveration (only Copy).}
\end{figure}
\begin{figure}[H]
\begin{centering}
\includegraphics[width=1\linewidth]{fig/visualizations/mix_cp_rco_arecall_psort_pw2}
\par\end{centering}
\caption{Copy+Repeat Copy+Associative Recall+Priority Sort perseveration (only
Repeat Copy).}
\end{figure}

\subsection{Details on Synthetic Tasks\label{subsec:Details-on-Synthetic}}

\subsubsection{NTM single tasks}

\begin{table}[H]
\begin{centering}
\begin{tabular}{ccccccccc}
\hline 
\multirow{2}{*}{Tasks} & \multicolumn{2}{c}{\#Read/Write Head\tablefootnote{In NTM, the number of read and write heads are equal.}} & \multicolumn{2}{c}{Controller Size} & \multicolumn{2}{c}{Memory Size} & \multicolumn{2}{c}{\#Parameters}\tabularnewline
\cline{2-9} 
 & NTM & NUTM & NTM & NUTM & NTM & NUTM & NTM & NUTM\tabularnewline
\hline 
Copy & 1 & 1 & 100 & 80 & 128 & 128 & 63,260 & 52,206\tabularnewline
\hline 
Repeat Copy & 1 & 1 & 100 & 80 & 128 & 128 & 63,381 & 52,307\tabularnewline
\hline 
Associative Recall & 1 & 1 & 100 & 80 & 128 & 128 & 62,218 & 51,364\tabularnewline
\hline 
Dynamic N-grams & 1 & 1 & 100 & 80 & 128 & 128 & 58,813 & 48,619\tabularnewline
\hline 
Priority Sort & 5 & 5 & 200 & 150 & 128 & 128 & 344,068 & 302,398\tabularnewline
\hline 
Long Copy & 1 & 1 & 100 & 80 & 256 & 256 & 63,260 & 52,206\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
~

\caption{Model hyper-parameters (single tasks).}

\end{table}
\begin{table}[H]
\begin{centering}
\begin{tabular}{lll}
\hline 
\multirow{1}{*}{Tasks} & Training & Testing\tabularnewline
\hline 
Copy & Sequence length range: {[}1, 20{]} & Sequence length: 120\tabularnewline
\hline 
\multirow{2}{*}{Repeat Copy} & Sequence length range: {[}1, 10{]} & Sequence length range: {[}10, 20{]}\tabularnewline
 & \#Repeat range: {[}1, 10{]} & \#Repeat range: {[}10, 20{]}\tabularnewline
\hline 
\multirow{2}{*}{Associative Recall} & \#Item range: {[}2, 6{]} & \#Item range: {[}6, 20{]}\tabularnewline
 & Item length: 3 & Item length: 3\tabularnewline
\hline 
Dynamic N-grams & Sequence length: 50 & Sequence length: 200\tabularnewline
\hline 
\multirow{2}{*}{Priority Sort} & \#Item: 20 & \#Item: 20\tabularnewline
 & \#Sorted Item: 16 & \#Sorted Item: 20\tabularnewline
\hline 
Long Copy & Sequence length range: {[}1, 40{]} & Sequence length: 200\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
~

\caption{Task settings (single tasks).}
\end{table}

\subsubsection{NTM sequencing tasks}

\begin{table}[H]
\begin{centering}
\begin{tabular}{ccccccccc}
\hline 
\multirow{2}{*}{Tasks} & \multicolumn{2}{c}{\#Read/Write Head} & \multicolumn{2}{c}{Controller Size} & \multicolumn{2}{c}{Memory Size} & \multicolumn{2}{c}{\#Parameters}\tabularnewline
\cline{2-9} 
 & NTM & NUTM & NTM & NUTM & NTM & NUTM & NTM & NUTM\tabularnewline
\hline 
C+RC & 1 & 1 & 200 & 150 & 128 & 128 & 206,481 & 153,941\tabularnewline
\hline 
C+AR & 1 & 1 & 200 & 150 & 128 & 128 & 206,260 & 153,770\tabularnewline
\hline 
C+PS & 3 & 3 & 200 & 150 & 128 & 128 & 275,564 & 263,894\tabularnewline
\hline 
C+RC+AR+PS & 3 & 3 & 250 & 200 & 128 & 128 & 394,575 & 448,379\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
~

\caption{Model hyper-parameters (sequencing tasks).}
\end{table}
\begin{table}[H]
\begin{centering}
\begin{tabular}{lll}
\hline 
\multirow{1}{*}{Tasks} & Training & Testing\tabularnewline
\hline 
\multirow{2}{*}{C+RC} & Sequence length range: {[}1, 10{]} & Sequence length range: {[}10, 20{]}\tabularnewline
 & \#Repeat range: {[}1, 10{]} & \#Repeat range: {[}10, 15{]}\tabularnewline
\hline 
\multirow{3}{*}{C+AR} & Sequence length range: {[}1, 10{]} & Sequence length range: {[}10, 20{]}\tabularnewline
 & \#Item range: {[}2, 4{]} & \#Item range: {[}4, 6{]}\tabularnewline
 & Item length: 8 & Item length: 8\tabularnewline
\hline 
\multirow{3}{*}{C+PS} & Sequence length range: {[}1, 10{]} & Sequence length range: {[}10, 20{]}\tabularnewline
 & \#Item: 10 & \#Item: 10\tabularnewline
 & \#Sorted Item: 8 & \#Sorted Item: 10\tabularnewline
\hline 
\multirow{6}{*}{C+RC+AR+PS} & Sequence length range: {[}1, 10{]} & Sequence length range: {[}10, 20{]}\tabularnewline
 & \#Repeat range: {[}1, 5{]} & \#Repeat: 6\tabularnewline
 & \#Item range: {[}2, 4{]} & \#Item: 5\tabularnewline
 & Item length: 6 & Item length: 6\tabularnewline
 & \#Item: 10 & \#Item: 10\tabularnewline
 & \#Sorted Item: 8 & \#Sorted Item: 10\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
~

\caption{Task settings (sequencing tasks).}
\end{table}

\subsubsection{Continual procedure learning tasks}

\begin{table}[H]
\begin{centering}
\begin{tabular}{cccccccc}
\hline 
\multicolumn{2}{c}{\#Read/Write Head} & \multicolumn{2}{c}{Controller Size} & \multicolumn{2}{c}{Memory Size} & \multicolumn{2}{c}{\#Parameters}\tabularnewline
\hline 
NTM & NUTM & NTM & NUTM & NTM & NUTM & NTM & NUTM\tabularnewline
\hline 
1 & 1 & 200 & 150 & 128 & 128 & 206,444 & 196,590\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
~

\caption{Model hyper-parameters (continual procedure learning tasks). NUTM
uses 6 programs per head.}
\end{table}
\begin{table}[H]
\begin{centering}
\begin{tabular}{lll}
\hline 
\multirow{1}{*}{Tasks} & Training & Testing\tabularnewline
\hline 
Copy & Sequence length range: {[}1, 10{]} & Sequence length range: {[}1, 10{]}\tabularnewline
\hline 
\multirow{2}{*}{Repeat Copy} & Sequence length range: {[}1, 5{]} & Sequence length range: {[}1, 5{]}\tabularnewline
 & \#Repeat range: {[}1, 5{]} & \#Repeat range: {[}1, 5{]}\tabularnewline
\hline 
\multirow{3}{*}{Associative Recall} & Sequence length: 3 & Sequence length: 3\tabularnewline
 & \#Item range: {[}2, 3{]} & \#Item range: {[}2, 3{]}\tabularnewline
 & Item length: 3 & Item length: 3\tabularnewline
\hline 
\multirow{2}{*}{Priority Sort} & \#Item: 10 & \#Item: 10\tabularnewline
 & \#Sorted Item: 8 & \#Sorted Item: 8\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
~

\caption{Task settings (continual procedure learning tasks).}
\end{table}

\subsection{Details on Few-shot Learning Task\label{subsec:Details-on-Few-shot}}

We use similar hyper-parameters as in \citet{santoro2016meta} , which
are reported in Tab. \ref{tab:NUTM-hyper-parameters-for-1}.

\begin{table}[H]
\begin{centering}
\begin{tabular}{cccccccc}
\hline 
Model &  & \#Read Head & \#Write Head & Controller Size &  &  &  Size\tabularnewline
\hline 
MANN (LRUA) & 1 & 4 & 1 & 200 & 128 & 40 & 0\tabularnewline
\hline 
NUTM (LRUA) & 2 & 4 & 1 & 180 & 128 & 40 & 2\tabularnewline
\hline 
NUTM (LRUA) & 3 & 4 & 1 & 150 & 128 & 40 & 3\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
~

\caption{Hyper-parameters for few-shot learning. All models use RMSprop optimizer
with learning rate .\label{tab:NUTM-hyper-parameters-for-1}}
\end{table}
Testing accuracy through time is listed below,

\begin{figure}[H]
\begin{centering}
\includegraphics[width=1\linewidth]{fig/learning_curves/fewshot-5-50}
\par\end{centering}
\caption{Testing accuracy during training (five random classes/episode, one-hot
vector labels, of length 50).}
\end{figure}
\begin{figure}[H]
\begin{centering}
\includegraphics[width=1\linewidth]{fig/learning_curves/fewshot-10-75}
\par\end{centering}
\caption{Testing accuracy during training (ten random classes/episode, one-hot
vector labels, of length 75).}
\end{figure}
\begin{table}
\begin{centering}
\begin{tabular}{l|c|ccc|ccc}
\hline 
\multirow{2}{*}{Model} & Persistent & \multicolumn{3}{c|}{5 classes} & \multicolumn{3}{c}{10 classes}\tabularnewline
\cline{3-8} 
 & memory\tablefootnote{If the memory is not artificially erased between episodes, it is called
persistent. This mode is hard for the case of 5 classes as shown in
\citet{santoro2016meta}} &  &  &  &  &  & \tabularnewline
\hline 
MANN (LRUA){*} & No & 82.8 & 91.0 & 94.9 & - & - & -\tabularnewline
MANN (LRUA) & No & 82.3 & 88.7 & 92.3 & 52.7 & 60.6 & 64.7\tabularnewline
NUTM (LRUA) & No & \textbf{85.7} & \textbf{91.3} & \textbf{95.5} & \textbf{68.0} & \textbf{78.1} & \textbf{82.8}\tabularnewline
\hline 
Human{*} & Yes & 57.3 & 70.1 & 81.4 & - & - & -\tabularnewline
MANN (LRUA){*} & Yes &  & - &  &  & - & \tabularnewline
MANN (LRUA) & Yes & 66.2 & 73.4 & 81.0 & 51.3 & 59.2 & 63.3\tabularnewline
NUTM (LRUA) & Yes & \textbf{77.8} & \textbf{85.8} & \textbf{89.8} & \textbf{69.0} & \textbf{77.9} & \textbf{82.7}\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
~

\caption{Test-set classification accuracy (\%) on the Omniglot dataset after
100,000 episodes of training. {*} denotes available results from \citet{santoro2016meta}
(some are estimated from plotted figures).\label{tab:meta}}
\end{table}

\subsection{Details on bAbI Task\label{subsec:Details-on-bAbI}}

We train the models using RMSprop optimizer with fixed learning rate
of  and momentum of 0.9. The batch size is 32 and we adopt
layer normalization \cite{lei2016layer} to DNC's layers. Following
\citet{W18-2606} practice, we also remove temporal linkage for faster
training. The details of hyper-parameters are listed in Table \ref{tab:NUTM-hyper-parameters-for}.
Full NUTM () results are reported in Table \ref{tab:babifull}.

\begin{table}[H]
\begin{centering}
\begin{tabular}{cccccccc}
\hline 
\#Read Head & \#Write Head & Controller Size &  &  &  &  Size & \#Parameters\tabularnewline
\hline 
4 & 1 & 256 & 196 & 64 & 1\tablefootnote{When , the model converges to layer-normed DNC} & 1 & 891,136\tabularnewline
\hline 
4 & 1 & 200 & 196 & 64 & 2 & 2 & 934,787\tabularnewline
\hline 
4 & 1 & 172 & 196 & 64 & 4 & 4 & 794,773\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
~

\caption{NUTM hyper-parameters for bAbI.\label{tab:NUTM-hyper-parameters-for}}
\end{table}
\begin{table}[H]
\begin{centering}
\begin{tabular}{lcc}
\hline 
Task & bAbI Best Results & bAbI Mean Results\tabularnewline
\hline 
1: 1 supporting fact & 0.0 & 0.0  0.0\tabularnewline
2: 2 supporting facts  & 0.2 & 0.6  0.3\tabularnewline
3: 3 supporting facts  & 4.0 & 7.6  3.9\tabularnewline
4: 2 argument relations  & 0.0 & 0.0  0.0\tabularnewline
5: 3 argument relations  & 0.4 & 1.0  0.4\tabularnewline
6: yes/no questions  & 0.0 & 0.0  0.0\tabularnewline
7: counting  & 1.9 & 1.5  0.8\tabularnewline
8: lists/sets  & 0.6 & 0.3  0.2\tabularnewline
9: simple negation  & 0.0 & 0.0  0.0\tabularnewline
10: indefinite knowledge  & 0.1 & 0.1  0.0\tabularnewline
11: basic coreference  & 0.0 & 0.0  0.0\tabularnewline
12: conjunction  & 0.0 & 0.0  0.0\tabularnewline
13: compound coreference  & 0.1 & 0.0  0.0\tabularnewline
14: time reasoning  & 0.3 & 1.6  2.2\tabularnewline
15: basic deduction  & 0.0 & 2.6  8.3\tabularnewline
16: basic induction  & 49.3 & 52.0  1.7\tabularnewline
17: positional reasoning  & 4.7 & 18.4  12.7\tabularnewline
18: size reasoning  & 0.4 & 1.6  1.1\tabularnewline
19: path finding  & 4.3 & 23.7  32.2\tabularnewline
20: agent\textquoteright s motivation  & 0.0 & 0.0  0.0\tabularnewline
\hline 
Mean Error (\%) & 3.3 & 5.6  1.9\tabularnewline
\hline 
Failed (Err. \textgreater 5\%) & 1 & 3  1.2\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
~

\caption{NUTM () bAbI best and mean errors (\%).\label{tab:babifull}}
\end{table}

\subsection{Example of memory operation function in NTM\label{subsec:Example-of-memory}}

In NTM, .
The memory addressing weight is initially computed by content-based
attention,


Here,  is the content-based
weight,  is a strength scalar, and  is implemented
as cosine similarity


In addition, NTM supports location-based addressing started with an
interpolation between content-based weight and the previous weight


where  is the interpolation gate that determines to use (or
ignore) content-based addressing. Then, NTM can shift the address
to other rows by performing convolution shift modulo ,


where  is the shift weighting. To prevent the shifted weight
from blurring, sharpening is applied


Then, the memory is updated as follows,


where  and 
are erase vector and update vector, respectively. The read value is
computed using the same address weight as follows,




\subsection{Others}

If we deliberately set the key dimension equal to the number of programs,
we can even place an orthogonal basis constraint on the key space
of NSM by minimizing the following loss, 


where  and  denote the key part in
NSM and the identity matrix, respectively. 

Direct attention is one special case of key-value attention when the
memory keys form orthogonal basis. When this happens, the generated
key  plays a direct role as the attention weight .
Thus, using key-value attention is more generic.

For all tasks,  is fixed to , reducing with decay
rate of . 

Ablation study's learning losses with mean and error bar are plotted
in Fig. \ref{fig:Learning-curves-on-2}.

\begin{figure}

\begin{centering}
\includegraphics[width=1\textwidth]{fig/learning_curves/big_abl}
\par\end{centering}
\caption{Learning curves on Associative Recall (AR) ablation study.\label{fig:Learning-curves-on-2}}

\end{figure}

 \end{document}
