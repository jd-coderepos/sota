\documentclass[11pt]{article}
\usepackage[usenames,dvipsnames]{color}
\usepackage[unicode,colorlinks=true,citecolor=black,filecolor=black,linkcolor=black,urlcolor=black, pdfpagelabels, plainpages=false]{hyperref}
\usepackage{multirow}
\usepackage
{
 amssymb, amsthm, amsmath,fullpage, nicefrac, tikz, prettyref
}
\usepackage{bbm}
\usepackage{caption}
\def\compactify{\itemsep=0pt \topsep=0pt \partopsep=0pt \parsep=0pt}
\usepackage{times}
\usepackage[margin=2.5cm]{geometry}


\clearpage{}\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{templemma}{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{claim}[theorem]{Claim}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{calculation}[theorem]{Calculation}

\newcommand{\I}{\mathbbm{1}}

\newcommand {\bbN}  {\mathbb{N}}
\newcommand {\bbR}  {\mathbb{R}}

\newcommand{\calV}{{\cal V}}
\newcommand{\calF}{{\cal F}}
\newcommand{\calM}{{\cal M}}
\newcommand{\calP}{{\cal P}}
\newcommand{\calZ}{{\cal Z}}
\newcommand{\calE}{{\cal E}}
\newcommand{\calC}{{\cal C}}

\newcommand{\Exp}{\mathbb{E}}

\newcommand{\pr}{\mathbf{Pr}}
\newcommand{\e}{\mathbf{E}}
\newcommand{\var}{\mathbf{Var}}

\newcommand{\OpenFrame}{\rule{0pt}{12pt} \hrule height 0.8pt \rule{0pt}{1pt} \hrule height 0.4pt \rule{0pt}{6pt}}
\newcommand{\CloseFrame}{\vskip -8pt\rule{0pt}{1pt}\hrule height 0.4pt \rule{0pt}{1pt} \hrule height 0.8pt \rule{0pt}{12pt}}

\newcommand{\given}{\;\mid\;}
\newcommand{\Given}{\;\;\mid\;\;}

\newcommand{\fpos}{f^{+}}
\newcommand{\fneg}{f^{-}}
\newcommand{\fno}{f^{\circ}}
\newcommand{\ecost}{e.cost}
\newcommand{\elp}{e.lp}
\DeclareMathOperator{\lp}{lp}
\DeclareMathOperator{\In}{In}
\DeclareMathOperator{\InOut}{InOut}
\DeclareMathOperator{\Out}{Out}


\newcommand{\ppp}{\ensuremath{(+,+,+)}}
\newcommand{\ppm}{\ensuremath{(+,+,-)}}
\newcommand{\pmm}{\ensuremath{(+,-,-)}}
\newcommand{\mmm}{\ensuremath{(-,-,-)}}
\newcommand{\mmn}{\ensuremath{(-,-,\varnothing)}}
\newcommand{\pmn}{\ensuremath{(+,-,\varnothing)}}
\newcommand{\ppn}{\ensuremath{(+,+,\varnothing)}}
\newcommand{\fpk}{f^+_3}
\newcommand{\fmk}{f^-_3}
\newcommand{\fnk}{f^\circ_3}
\newcommand{\fp}{f^+}
\newcommand{\fm}{f^-}
\newcommand{\Cppp}{\mathcal{C}_{\ppp}}
\newcommand{\Cppm}{\mathcal{C}_{\ppm}}
\newcommand{\Cpmm}{\mathcal{C}_{\pmm}}
\newcommand{\Cmmm}{\mathcal{C}_{\mmm}}

\newcommand{\kostya}[1]{\footnote{\color{red} #1 -- Kostya}}


\newrefformat{app}{Appendix \ref{#1}}
\newrefformat{cor}{Corollary \ref{#1}}
\newrefformat{calc}{Calculation \ref{#1}}

\clearpage{}



\begin{document}

\title{Near Optimal LP Rounding Algorithm for Correlation Clustering\\on Complete and
Complete -partite Graphs}
\author{Shuchi Chawla \\ University of Wisconsin-Madison \and Konstantin Makarychev\\ Microsoft Research\and Tselil Schramm \\ UC Berkeley \and Grigory Yaroslavtsev \\ University of Pennsylvania}
\date{\textbf{\small{}}}


\maketitle


\begin{abstract}
We give new rounding schemes for the standard linear programming relaxation of
the correlation clustering problem, achieving approximation factors almost
matching the integrality gaps:
\begin{itemize}
\item For complete graphs our approximation is , which
	almost matches the previously known integrality gap of .
\item For complete -partite graphs our approximation is . We also show a
	matching integrality gap.
\item For complete graphs with edge weights satisfying triangle inequalities
	and probability constraints, our approximation is , and we show an
	integrality gap of .
\end{itemize}
Our results improve a long line of work on approximation algorithms for
correlation clustering in complete graphs, previously culminating in a ratio of
 for the complete case by Ailon, Charikar and Newman (JACM'08).  In the
weighted complete case satisfying triangle inequalities and probability
constraints, the same authors give a -approximation; for the bipartite case,
Ailon, Avigdor-Elgrabli, Liberty and van Zuylen give a -approximation
(SICOMP'12).
\end{abstract}

\setcounter{page}{0}
\thispagestyle{empty} \pagebreak

\tableofcontents

\pagebreak

\section{Introduction}
We study the \textit{correlation clustering} problem -- given inconsistent
pairwise similarity/dissimilarity information over a set of objects, our goal
is to partition the vertices into an \textit{arbitrary} number of clusters
that match this information as closely as possible.
The task of clustering is made interesting by the fact that the similarity
information is inherently noisy. For example, we may be asked to cluster 
and  together, and  and  together, but  and  separately. In this
case there is no clustering that matches the data exactly. The optimal
clustering is the one that differs from the given constraints at the fewest
possible number of pairs, and it may use anywhere between one and  clusters.
In some contexts this problem is also known as \textit{cluster editing}: given a graph
between objects where every pair deemed similar is connected by an edge, add or
remove the fewest number of edges so as to convert the graph into a collection
of disjoint cliques.

Correlation clustering is quite different from other common clustering
objectives in that the given data is qualitative (similar versus
dissimilar pairs) rather than quantitative (e.g. objects embedded in a
metric space). As such, it applies to many different problems that
arise in machine learning, biology, data mining and other areas. From
a learning perspective, correlation clustering is essentially an
agnostic learning problem: the goal is to fit a classifier from a
certain concept class (namely all clusterings) as best as possible to
noisy examples (namely the pairwise similarity information). The
correlation clustering objective has been successfully employed for a
number of learning problems, for example: coreference
resolution~\cite{CR01, CR02, MW03},
where the goal is to determine which references in a news article
refer to the same object; cross-lingual link detection~\cite{VZ07},
where the goal is to find news articles in different languages that
report on the same event; email clustering by topic or relevance; and
image segmentation~\cite{Wirth10}. In biology, the problem of clustering gene
expression patterns can be cast into the framework of correlation
clustering~\cite{DSY99, A04}. Another application, arising in data
mining, is that of aggregating inconsistent clusterings taken from
different sources~\cite{Fil03}. In this setting, the cost of an aggregate
clustering is the sum over pairs of objects of the fraction of
clusterings that it differs from. This special case of correlation
clustering is known as the \textit{consensus clustering} problem.

In many of the above applications, we have access to a binary classifier that
takes in pairs of objects and returns a ``similar'' or ``dissimilar'' label. We
can interpret this information in the form of a complete graph with edges
labeled ``'' (denoting  similarity) and ``'' (denoting dissimilarity).
The correlation clustering problem can then be restated as one of producing a
clustering such that most of the ``'' edges are within clusters and most
``'' edges cross different clusters. In some cases, it may not be possible
to compare all pairs of objects, leaving ``missing'' edges so that the underlying
graph is not complete. However, correlation clustering on general
graphs is equivalent to the multicut problem~\cite{DEFI06}, and obtaining any
constant factor approximation is Unique-Games hard~\cite{CKKRS06}.\footnote{The
best known algorithms due to Charikar, Guruswami, Wirth~\cite{CGW05} and  
Demaine, Emanuel, Fiat, Immorlica~\cite{DEFI06}
give an  approximation.}
Still, it is possible to get approximations for some practical cases with
missing edges; for example, when the underlying graph is a complete -partite graph.
Ailon, Avigdor-Elgrabli, Liberty, and van
Zuylen~\cite{AALZ12} give several applications of complete bipartite
correlation clustering.


Since its introduction a decade ago by Bansal, Blum, and Chawla~\cite{BBC04},
correlation clustering has gained a lot of prominence within the theory and
learning communities (see, e.g., the survey by Wirth~\cite{Wirth10}, and
references therein) and has become one of the textbook examples in the design
of approximation algorithms (Williamson and Shmoys~\cite{WS11}
consider the correlation clustering problem with the maximization objective).
Bansal et al. gave the first constant factor approximation algorithm for the
problem on complete graphs. The factor
has since then been improved several times, culminating in a factor of~
for complete graphs due to Ailon, Charikar, and Newman~\cite{ACN08}, which
relies on a natural LP formulation of the problem. On the other hand, the
problem is known to be APX-hard~\cite{DEFI06}, and the best known integrality
gap of the LP is ~\cite{CGW05}, leaving a 20\% margin for improvement.

\subsection{Our Results}
In this paper, we nearly close the gap between
the approximation ratio and the integrality gap for complete graphs and complete
-partite graphs:
For the correlation clustering problem on complete graphs, we obtain a
-approximation for some fixed , nearly matching an integrality gap of ~\cite{CGW05}.
For the correlation clustering problem on complete -partite graphs,
we obtain a -approximation and exhibit an integrality gap instance with
a gap of .  The previously best known algorithm for the bipartite variant
of the problem due to Ailon et al.~\cite{AALZ12} gives
a -approximation.


\begin{theorem}\label{thm:main-intro}
There is a deterministic polynomial-time algorithm for the Correlation
Clustering Problem that gives a -approximation for complete
graphs, where  is some fixed constant smaller than , and a
-approximation for complete -partite graphs.
\end{theorem}


\begin{table}[t]
	\begin{center}
		\begin{tabular}{|l || c | c| c| c|}
			\hline
			\multicolumn{5}{|c|}{\bf Approximation Algorithms for Correlation Clustering} \\
			\hline
			& \textsc{Previous Factor}
			& \textsc{Our Factor}
			& \textsc{Integrality Gap}
			& \textsc{Limitation}

			\\


			\hline
			Complete
			& ~\cite{BBC04}, 4~\cite{CGW05}, 2.5~\cite{ACN08, ZHJW07}
			& , Thm~\ref{thm:main-intro}
			& 2~\cite{CGW05}
			& 2.025, Thm~\ref{thm:limitation-complete}

			\\
			\hline
            Triangle Inequality
			& 3~\cite{GMT07}, 2~\cite{ACN08,ZHJW07}
			& 1.5, Sec~\ref{sec:triangle-inequalities}
			& 1.2, Thm~\ref{thm:integrality-gap-triangle-inequalities}
			& 1.5, Sec~\ref{sec:triangle-inequalities}

			\\
			\hline
			Bipartite
			& 11 ~\cite{A04},4~\cite{AALZ12}
			& 3, Thm~\ref{thm:main-intro}
			& 3, Thm~\ref{thm:integrality-gap-bipartite}
			& ---
			\\
			\hline
			-partite
			& ---
			& 3, Thm~\ref{thm:main-intro}
			& 3, Thm~\ref{thm:integrality-gap-bipartite}
			& ---
		\\
			\hline
		\end{tabular}
	\end{center}
	\vskip -10 pt
	\caption{Previous and our approximation factors, integrality gaps and limitations of our approach.}
	\label{table:results}
	\vskip -10 pt
\end{table}
 
We also study a weighted variant of the problem. In this variant, each edge has
a positive weight  and a negative weight , the
goal is to minimize the total weight of violated edges (for details see
Section~\ref{sec:weighted}).  We show that Weighted Correlated Clustering is
equivalent to the unweighted Correlated Clustering on complete graphs if
 for every .  Gionis, Mannila
and Tsaparas~\cite{GMT07} introduce a natural special case of this problem
in which the negative weights satisfy the triangle inequality
(for every  it holds that ).  For this problem -- Weighted Correlation Clustering with
Triangle Inequalities -- we give a -approximation algorithm, improving on
the -approximation of Ailon et al.~\cite{ACN08}  (see Theorem~\ref{thm:weighted-triang-ineq} in
Section~\ref{sec:weighted}). Our proof of the last
result is computer assisted.


The main technical contribution of our paper is an approach towards obtaining
a tight rounding scheme given an LP solution. At a high level our algorithm
is similar to that of Ailon et al.~\cite{ACN08}, but we perform the actual rounding decisions
in a novel way, using carefully designed functions of the LP solution to get
rounding probabilities for edges in the graph, allowing us to obtain a
near-optimal approximation ratio.
We emphasize that although we employ a lengthy and complicated analysis
to prove that our rounding scheme achieves a -approximation,
our algorithm itself is very simple and runs in time  given the LP solution.
The linear programming relaxation that we use has been studied very extensively and heuristic approaches have been developed for solving it~\cite{DSW10}.

 We demonstrate our technique for correlation clustering in complete
graphs, in complete -partite graphs, and in the special case of weighted edges
satisfying triangle inequality constraints.
In each case, we obtain significant improvements over the previously best known
results, and nearly or exactly match the integrality gap of the LP.
When we are unable to match the integrality gap, we prove a lower bound on the
ratio that may be achieved by any functions within our rounding scheme.  Our
results and a comparison with the previous work are summarized in
Table~\ref{table:results}.

\subsection{Our Methodology}

As is the case for many graph partitioning problems, the correlation clustering
objective can be captured in the form of a linear program over variables that
encode lengths of edges. A long edge signifies that its endpoints should be
placed in different clusters, and a short edge signifies that its endpoints
should be in the same cluster. For consistency, edge lengths must satisfy the
triangle inequality.

A natural approach to rounding this relaxation is to interpret each edge's
length, , as the probability with which it should be cut. The challenge
is to ensure consistency. For example, consider a triangle with two positive
edges and one negative edge where the negative edge has LP length
. If we first cut the negative edge with probability , then
in order to return a consistent clustering we are forced to cut one of the positive edges. In
this way, an independent decision to cut or not cut one edge may force a
decision on a different edge, resulting in ``collateral damage.'' Ailon,
Charikar, and Newman~\cite{ACN08} give a simple rounding
algorithm and a charging scheme that cleanly bounds the cost of this collateral
damage. Their algorithm picks a random vertex  in the graph and rounds every
edge  incident on this ``pivot'' with probability equal to the length of
the edge; vertices  corresponding to the edges  that are not cut by
this procedure form 's cluster; this cluster is then removed from the graph,
and the algorithm recurses on the remaining graph. This approach gives the best
previously known approximation ratio for the correlation clustering problem on
complete graphs, a factor of .



Our main technical contribution is a more subtle treatment of the probability
of cutting an edge:  rather than cutting edge  with probability
, we cut  with probability given by some function 
(this idea was previously used by Ailon in his algorithm for ranking
aggregation~\cite{ailon2010}).
In a departure from all of the other LP-rounding algorithms for correlation
clustering, we use different rounding functions for positive and
negative edges.
Though it may at first be surprising that the latter distinction can be
helpful, we remark that positive and negative constraints do not behave
symmetrically. For example, in a triangle with two positive edges and one
negative edge, the negative edge forces an inconsistency: any clustering of
this triangle must violate at least one constraint. On the other hand, in a
triangle with two negative edges and one positive edge, there is a valid
clustering that does not violate any constraints. Thus, we see that the
positive and negative edges behave differently, and we prove that rounding
positive and negative edges of the same LP length with different probabilities
gives a correspondingly better approximation ratio.

In some cases, this distinction leads to results that run counter to our
intuition. One might expect that negative edges should be cut with higher
probability than positive edges. However, this is not always the
case.  It turns out that it helps to cut long positive edges with probability ,
because we can charge them to the LP. On the other hand, it pays to be careful
about cutting long negative edges, because this might cause too much collateral
damage to other edges.

Our methodology for selecting the rounding functions  and  is interesting in its
own right. By regarding the cost of the algorithm on each kind of triangle as a
polynomial in , ,  and in , ,
,  then characterizing these multivariate polynomials, we are able to
obtain analytic upper and lower bounds on  and . While
this does not force our choices of  or , it suggests natural candidate functions
that can then be further analyzed. This same worst-case polynomial
identification and bounding approach yields lower bounds on the best possible
approximation ratio that can be achieved by a similar algorithm.

\subsection{Other Related Work}

As mentioned above, there has been a series of works giving constant
factor approximations for correlation clustering in complete
graphs~\cite{BBC04, CGW05, ACN08}.  A modified version of the Ailon et
al.~\cite{ACN08} -approximation algorithm can be used as a basis for parallel
algorithms~\cite{CDK14}. Van Zuylen et al.~\cite{ZHJW07} showed that the
-approximation of Ailon et al. can be derandomized without any loss in
approximation factor. Correlation clustering on complete bipartite graphs was
first studied by Amit~\cite{A04}, who presents an -approximation. This was
subsequently improved to a -approximation by Ailon et al.~\cite{AALZ12}.
For complete graphs with weights satisfying triangle inequalities a
3-approximation was obtained by Gionis et al.~\cite{GMT07} and a
2-approximation by Ailon et al.~\cite{ACN08}. These prior works are summarized
in Table~\ref{table:results}.  In general graphs, the problem can be
approximated to within a factor of , and because it is equivalent to
the multicut problem, this is suspected to be the best possible~\cite{CGW05,
DEFI06}.

Bansal et al. \cite{BBC04} also studied an alternative version of the problem
in which the objective is to approximately maximize the number of edges that
the clustering gets correct: that is, the number of ``'' edges inside
clusters and ``'' edges going across clusters. They noted that this version
can be trivially approximated to within a factor of  in arbitrary weighted
graphs, and presented a polynomial time approximation scheme for the version on
complete graphs. Subsequently, Swamy~\cite{Swamy04} and Charikar, Guruswami and
Wirth~\cite{CGW05} developed an improved SDP-based approximations for this
``MaxAgree'' problem on arbitrary weighted graphs. MaxAgree is known to be hard
to approximate within a factor of  for both the unweighted (complete
graph) and weighted versions~\cite{CGW05, Tan08}.

A number of other variants of the correlation clustering problem have
been studied. Giotis and Guruswami~\cite{Giotis06} and Karpinski et
al.~\cite{KS09} studied the variant where the solution is stipulated
to contain only a few (constant number of) clusters, and under that
constraint presented polynomial time approximation schemes. Mathieu,
Sankur, and Schudy~\cite{MSS10} studied the online version of the
problem and give an -competitive algorithm, which is also the
best possible within constant factors. Mathieu and Schudy~\cite{MS10}
introduced a semi-random model, in which every
graph is generated as follows: start with an arbitrary planted partitioning
of the graph into clusters, set labels consistent with
the planted partitioning on all edges, then independently pick every edge into a random subset
of \textit{corrupted} edges with probability ,
and let the adversary arbitrarily change labels on the corrupted edges.
Mathieu and Schudy~\cite{MS10} showed how to get  approximation under very mild assumptions on .
Their result was extended to other semi-random models in~\cite{MMV, Yudong}.
The problem was studied in another interesting stochastic model in~\cite{AL09}.

\subsection{Organization}




In \prettyref{sec:alg}, we formally describe and generalize the algorithm and
analytical framework of Ailon et al.~\cite{ACN08} to accomodate our algorithm and analysis, and lay
the groundwork for our analysis with observations about algorithms within this
framework.
In \prettyref{sec:choosing_f}, we give the analysis that leads to our choices
of rounding functions. \prettyref{sec:pic_proofs} gives an overview of the analysis
of the triples for complete correlation clustering, as well as pictoral proofs of the main
theorem. In \prettyref{sec:gaps}, we introduce new integrality gaps for the -partite
case and the weighted case with triangle inequalities; in \prettyref{sec:lbd} we
prove a lower bound of  on the approximation ratio of any algorithm
within our framework.
\prettyref{sec:weighted} contains a summary of our results for the weighted cases of the problem.
In \prettyref{sec:derand}, we give a derandomization of our algorithm.
\prettyref{sec:kpart} and \prettyref{app:complete} contain the analytical
proofs of the -approximation for -partite correlation clustering and the
-approximation for the complete case respectively.
 


\section{Approximation Algorithm}\label{sec:alg}
In this section, we present an approximation algorithm for the Correlation
Clustering Problem that works both for complete graphs and complete -partite
graphs. For the weighted case of the problem, we refer the reader to Section~\ref{sec:weighted}.
We denote the set of positive edges by  and the set of negative edges by .
The algorithm is based on the approach of Ailon et al.~\cite{ACN08}. It iteratively finds
clusters and removes them from the graph. Once all vertices are clustered, the
algorithm outputs all found clusters and terminates.

Initially, the algorithm marks all vertices as active. At step , it picks a
random pivot  among active vertices, and then adds each active vertex  to
 with probability  independently of other vertices. Then,
the algorithm removes the cluster  from the graph and marks all vertices in
 as inactive.  The probability  depends on the LP solution
and the type of the pair : we set , if  is
a positive edge; , if  is a negative edge; and
, if there is no edge between  and . Here,
 is the LP variable corresponding to the pair  (we describe the
LP in a moment) and , , and  are special functions which we
will define later. Below we give pseudo-code for the algorithm.

\OpenFrame

\noindent \textbf{Input:} Graph ,  solution .

\noindent \textbf{Output:} Partitioning of vertices into disjoint sets.

\begin{itemize}\compactify
\item Let  be the set of active vertices; let .
\item while ()
\begin{itemize}\compactify
\item Pick a pivot  uniformly at random.
\item For each vertex , set :

\item For each vertex , add  to  with probability  independently of all other vertices.
\item Remove  from  i.e., let . Let .
\end{itemize}
\item Output , where  is the index of the last iteration of the algorithm.
\end{itemize}
\CloseFrame

This algorithm is probabilistic. We show how to derandomize it in \prettyref{sec:derand}.
The main new ingredient of our algorithm is a procedure for picking the
probabilities .  To compute these probabilities, we use the standard LP
relaxation introduced by~\cite{CGW05}. We first formulate an integer program for
Correlation Clustering. For every pair of vertices  and , we have a
variable  that is equal to the distance between  and 
in the ``multicut metric'':  if  and  are in the same cluster;
and  if  and  are in different clusters. Variables 
satisfy the triangle inequality constraints (\ref{lp:triang}). They are also
symmetric, i.e. . Instead of writing the constraint
, we have only one variable for each edge .
We refer to this variable as  or . The IP objective
is to minimize the number of violated constraints. We write it as follows:
.
Note that a term  in the first sum equals 1 if and only if the
corresponding positive edge  is cut; and a term  in the
second sum equals 1 if and only if the corresponding negative edge  is
contracted. Thus, this integer program is exactly equivalent to the Correlation
Clustering Problem.
We relax the integrality constraints  and obtain the
following LP.



The approximation ratio of the algorithm depends on the set of functions
 we use for rounding. We explain how we pick
these functions in \prettyref{sec:choosing_f}. In
Section~\ref{sec:kpart} and Appendix~\ref{app:complete}, we analyze the specific
rounding functions that give improved approximation guarantees for complete
graphs and complete -partite graphs. We prove the following theorems.

\begin{theorem}[See \prettyref{app:complete}]\label{thm:complete}
For complete graphs, the approximation algorithm with rounding functions

gives a -approximation for  and
, and a constant  with .
\end{theorem}

\begin{theorem}[See \prettyref{sec:kpart}]\label{thm:kpart}
For complete -partite graphs, the approximation algorithm with rounding functions

gives a 3-approximation.
\end{theorem}

Note that the integrality gap for complete graphs is ~\cite{CGW05}, and the
integrality gap for complete -partite graphs is  (see
Section~\ref{sec:gaps}).  So the algorithm for complete -partite graphs
optimally rounds the LP; the algorithm for complete graphs nearly optimally
rounds the LP.

\subsection{Analysis}
In this section, we prove a general statement -- Lemma~\ref{lem:alpha} -- that
asserts that the approximation ratio of the algorithm is at most  if a
certain condition (depending on ) holds for every triple of vertices
.  We shall assume that  for each , and, particularly, that the algorithm always puts the pivot  in
the set .

Consider step  of the algorithm. At this step, the algorithm finds and
removes set  from the graph. Observe, that if  or ,
then the constraint  is either violated or satisfied right after step
. Specifically, if  is a positive edge, then the constraint 
is violated if exactly one of the vertices --  or  -- is in . If
 is a negative constraint, then  is violated if both  and 
are in . Denote the number of violated constraints at step  by .
Then,

Here  denotes the indicator function of the event .  We want
to charge the cost of constraints violated at step  to the LP cost of edges
removed at step . The LP cost of edges removed at this step equals

Note, that , since every edge is removed from the graph
exactly once (compare the expression above with the objective
function~(\ref{lp:obj})).  If we show that  for all , then we will immediately get an upper bound on the
expected total cost of the clustering:

Here, we use that  is a
submartingale (i.e., ) , and  is a stopping time.

Let  be the conditional probability of violating the constraint
 given that the pivot  is  (assuming , and ). We say that  is the expected cost of the constraint 
given pivot . Similarly, let  be the conditional probability of
removing the edge  given the pivot is  multiplied by the LP cost of
the edge . (The LP cost equals  for positive edges; and
 for negative edges.) That is,

The expressions above do not depend on the set of active vertices .
The cut probabilities  and  are defined by the algorithm
(see equation~(\ref{eq:def-p})). Note
that , , , and  are
well defined. We also formally define  and  using
formulas~(\ref{eq:ecost}) and~(\ref{eq:elp}). In the analysis of the algorithm
for complete graphs, we assume that each vertex  has a positive self-loop,
thus , .


We now write  and  in terms of  and :

We divided the expressions on the right hand side by 2, because, in the sum, we
count every  and  twice (e.g., the first sum
contains the terms  and ).  We now add terms
 to the first sum and terms  to the second sum.
Then, we group all terms containing , , and  together. Note that
 and . We get

We denote each term in the first sum by  and each term in the second
sum by . Observe, that if 
for all , then
, and, hence,
. We thus obtain the following lemma.

\begin{lemma}\label{lem:alpha}
Fix a set of functions  with .  If  for every  (see
(\ref{eq:ecost}), (\ref{eq:elp}), (\ref{def:ALGuvw}), and (\ref{def:LPuvw}) for
definitions), then the expected number of violated constraints at step  is
bounded by  times the expected LP volume removed at step :

Consequently, the expected cost of the clustering returned by the algorithm is
upper bounded by .
\end{lemma}


\subsection{Triple-Based Analysis}

\label{sec:performing_analysis}
To finish analysis we need to show that  for every
triple of vertices . We separately analyze our choice of functions
 for complete graphs and complete -partite graphs in
Section~\ref{app:complete} and in Section~\ref{sec:kpart} respectively.  We
show that functions from Theorem~\ref{thm:complete} satisfy the conditions of
Lemma~\ref{lem:alpha} with  for complete graphs; and functions
from Theorem~\ref{thm:kpart} satisfy the conditions of Lemma~\ref{lem:alpha}
with  for complete -partite graphs. To show that  for every triangle , we consider all triangles 
with LP values satisfying triangle inequalities with all possible types of
edges: positive, negative, and ``missing'' or ``neutral'' edges. For brevity,
we refer to triangles as  where each  is one of the
symbols ``'', ``'' or ``''.  For example, a
-triangle is a triangle having two edges: a positive edge
and a negative edge; the third edge is missing.

The analysis of the functions  requires considering many cases. We
show that  for all
-triangles with edge lengths  satisfying
triangle inequalities that may possibly appear in each type of graph
(we use that there are no neutral edges in a complete graph, and that
no triangle in a complete -partite graph contains exactly two
neutral edges).  In other words, we fix types of edges for every
triangle and then consider a function of edge lengths
,, , and cut probabilities , ,
 formally defined using algebraic expressions
(\ref{eq:ecost}), (\ref{eq:elp}), (\ref{def:ALGuvw}), and
(\ref{def:LPuvw}):  Note, that this
function is defined even for those triangles that are not present in
our graph. We show that

for all 
satisfying the triangle
inequality. Here, ,  and  are rounding functions for
the edges ,  and  respectively. We first prove that for
many rounding functions , and, particularly, for rounding functions we use
in this paper, it is sufficient to verify the inequality  only for
those , , and  for which the triangle inequality is
tight, and a few corner cases.

\begin{lemma}\label{lem:tight-triangle}
Suppose that  is a monotonically non-decreasing piecewise convex
function;  is a monotonically non-decreasing piecewise concave function;
and  is a monotonically non-decreasing function. Let  be the set of
endpoints of convex pieces of , and  be the set of endpoints of the
concave pieces of . (Note that  and .) If the conditions of Lemma~\ref{lem:alpha} are violated for some
-triangle with edge lengths  i.e., , then there exists possibly
another triangle  (satisfying triangle inequalities)
for which  such that either
\begin{enumerate}
\item the triangle inequality is tight for ; or
\item the lengths of all positive edges of the triangle belong to ;
the lengths of all negative edges of the triangle belong to ; the lengths
of all neutral edges belong to .
\end{enumerate}
\end{lemma}
\begin{proof}
We show that the function  has the global minimum over the region
satisfying triangle inequalities at a point 
satisfying (1) or (2). By slightly perturbing functions , we may assume that
these functions are strictly increasing.\footnote{Formally, we consider a sequence of strictly monotone functions
uniformly converging to our rounding functions.}
Suppose that  has a minimum at point . Consider one of the edges,
say,  whose length does not lie in the corresponding set , 
or . Let ; and let
. Note that the function  is a
multilinear polynomial of 's and 's; in particular, it is linear in
 and . Moreover, it does not have monomials containing both
 and . This follows from the formal definition of . Let
us fix all variables except for  and . We now express 
as a function of : . The
function  is locally concave if  is a positive edge; and it is
locally convex if  is a negative edge. Here we use that  is not
in  or .  Observe that the only term containing  in 
comes from the expression for . Thus, the coefficient of  is
positive if  is a positive edge; and it is negative if  is a
negative edge. The function  does not depend on  if  is a
neutral edge (of course,  may depend on ). So the function

is a concave function of  (when  and  are fixed).
Therefore, if we slightly decrease or increase  the value of 
will decrease. But we assumed that  has the global minimum at .
Hence,  lies on the boundary of the region constrained by the triangle
inequality.  This concludes the proof.
\end{proof}


We give the analysis for the complete case in \prettyref{app:complete} and the
considerably simpler analysis for the -partite case in
\prettyref{sec:kpart}.

 \section{Choosing the Rounding Functions}\label{sec:choosing_f}
We now discuss how we came up with the rounding functions  for complete graphs.
We need to find functions  and  that satisfy the conditions
of Lemma~\ref{lem:alpha}, i.e., such that for all triangles ,
. (For brevity, we shall drop the argument  of
the  and  functions from now on.) Lemma~\ref{lem:tight-triangle} suggests that
we only need to care about triangles for which the triangle inequality is
tight. We focus on some special families of such triangles and obtain lower and
upper bounds of  and . Then, we find functions satisfying these constraints.
Later, we formally prove that the functions we found indeed give 
approximation. We sketch the proof in Section~\ref{sec:pic_proofs} and give a detailed
proof in \prettyref{app:complete}.

We first consider a -triangle with edge lengths .
\begin{lemma}
For a -triangle with edge lengths ,

\end{lemma}

\begin{proof}
We simply calculate  and  using \prettyref{eq:ecost} and
\prettyref{eq:elp}.
\end{proof}

\begin{corollary}\label{cor:fmbound}
Any function  that achieves an -approximation on all \pmm-triangles satisfies
	
for all .
\end{corollary}


\begin{claim}
	The function  does not violate the conditions of \prettyref{cor:fmbound} for .
\end{claim}

Fixing , the function we select is restricted to the blue region below.
\begin{center}
	\includegraphics[width=0.4\textwidth,natwidth=239,natheight=166]{images/pmm_bound_1.pdf}
\end{center}

Thus we take , as this choice is an easy candidate for the analysis.
Now, we bound  using the tight case for the linear rounding, a
\ppm-triangle with edge lengths .

\begin{lemma}
	\label{lem:fp_lower}
	Any function  that achieves an -approximation
        ratio on all \ppm-triangles has

for , if .
\end{lemma}

\begin{proof}
	Again, we calculate  using \prettyref{eq:ecost} and
	\prettyref{eq:elp}. This yields a quadratic function in :
	
	Solving for  in terms of , we get our result.
\end{proof}

This lower bound on  is necessary for approximating \ppm-triangles well,
but choosing a large  has consequences for the approximation ratio of
-triangles. We use a \ppp-triangle with edge lengths  to obtain
an upper bound on .

\begin{lemma}
	\label{lem:fp_upper}
Any function  that achieves an -approximation
ratio on all \ppp-triangles satisfies for all ,

\end{lemma}

\begin{proof}
	We compute  using \prettyref{eq:ecost} and
	\prettyref{eq:elp} as before. This yields a different quadratic function in :
	
	Solving for  in terms of , we get our result.
\end{proof}

The bounds from \prettyref{lem:fp_upper} and \prettyref{lem:fp_lower} give a
restricted region in which  may be for  and  to get an -approximation. We chose 
so that it would violate neither constraint, and also be easy to analyze.

\begin{claim}
\iffalse
	Fix . Then
	
	does not violate
\fi
Functions  and  from \prettyref{thm:complete} do not violate the conditions in \prettyref{lem:fp_upper} or \prettyref{lem:fp_lower} for an  approximation when .
\end{claim}
\begin{center}
	\includegraphics[width=0.4\textwidth,natwidth=239,natheight=168]{images/ppp_bound.pdf}
\end{center}

The parameters
 were chosen computationally within these analytic bounds so as to
minimize .

\iffalse
\begin{remark}
It turns out that the bound from the \ppp-triangle is the looser one, since a
\ppp-triangle with lenghts  is not the worst case, and indeed the
chosen values put  closer to the bound from the \ppm-triangle, which was
given by a tight case.
\todo{Tselil: add pointer to the complete analysis about this}
\end{remark}
\fi
 \section{Pictorial Proofs} \label{sec:pic_proofs}
Here we give a \emph{pictorial proof} of our main result, \prettyref{thm:complete}. This
proof serves as an illustration for an analytical proof we present in
\prettyref{app:complete}.

To prove \prettyref{thm:complete}, we use the framework presented in \prettyref{sec:performing_analysis}, bounding the approximation ratio of each triangle for every set of LP weights permitted by the constraints. \prettyref{lem:tight-triangle} allows us to consider only triangles for which
the triangle inequality is tight, that is, triangles of the form .
\prettyref{fig:ppp_ratio}, \prettyref{fig:ppm_ratio}, \prettyref{fig:pmm_ratio}, and \prettyref{fig:mmm_ratio} are plots of the polynomials  when the triangle inequality is tight; the fact that each of these polynomials is positive in the range of possible LP weights proves \prettyref{thm:complete}.

In \prettyref{app:complete}, we provide an analytical proof of
\prettyref{thm:complete}. For each case, we show
that this difference polynomial is positive for all possible LP weights. In the first two cases,
we take partial derivatives to find the worst triangle lengths in terms of a single
variable, then bound the roots of the polynomials; in the latter two cases we
are able to provide a factorization for the polynomial that is positive
term-by-term. For the complete argument, see \prettyref{app:complete}.

\begin{figure}[h!]
	\begin{center}
	\includegraphics[width=0.4\textwidth,natwidth=288,natheight=240]{images/ppp_fullrange.pdf}\hspace{1cm}
	\includegraphics[width=0.4\textwidth,natwidth=288,natheight=240]{images/ppp_fullrange_2.pdf}
	\caption{The difference between  for \ppp-triangles with tight triangle inequality constraints.}
	\label{fig:ppp_ratio}
\end{center}
\end{figure}
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.4\textwidth,natwidth=288,natheight=240]{images/ppm_fullrange.pdf} \hspace{1cm}
	\includegraphics[width=0.4\textwidth,natwidth=288,natheight=240]{images/ppm_fullrange_2.pdf}
	\caption{The difference between  for \ppm-triangles with tight triangle inequality constraints.}
	\label{fig:ppm_ratio}
\end{figure}
\begin{figure}[ht!]
	\centering
		\begin{minipage}{0.45\textwidth}
	\includegraphics[width=\textwidth,natwidth=288,natheight=240]{images/pmm_fullrange.pdf}
	\begin{flushleft}
	\captionof{figure}{The difference between  for
	\pmm-triangles with tight triangle inequality constraints.}
	\label{fig:pmm_ratio}
	\end{flushleft}
	\end{minipage}\qquad
	\begin{minipage}{0.4\textwidth}
	\centering
	\includegraphics[width=\textwidth,natwidth=288,natheight=240]{images/mmm_fullrange.pdf}
	\begin{flushright}
	\captionof{figure}{The difference between  for \mmm-triangles with tight triangle inequality constraints.}
	\label{fig:mmm_ratio}
	\end{flushright}
	\end{minipage}
\end{figure}
\pagebreak
 

\section{Lower Bounds}\label{sec:gaps}
\subsection{Integrality Gap for Bipartite Graphs}
\begin{theorem}\label{thm:integrality-gap-bipartite}
For every constant , there exists an instance of bipartite correlation
clustering with an integrality gap of .
\end{theorem}

\begin{proof}
We use the following family of bipartite expanders.
\begin{theorem}[see e.g.,~\cite{LPS88}]\label{thm:bipartite-expander}
For every  (where  is prime), and for sufficiently large  there exists a regular bipartite graph of
degree  with girth .
\end{theorem}

Let  be a bipartite expander as in
Theorem~\ref{thm:bipartite-expander}.  Let .  We define a
weight function  as follows:

We define three sets of pairs:

Consider the following solution to the linear program:

Note that all triples of LP values satisfy triangle inequality except -triples.
These latter triples don't exist by the bipartiteness of .
Thus, we have a feasible LP solution of cost .
Using Lemma~\ref{lem:clustering-cost-bipartite} below we can conclude that the ratio between the cost of the optimum solution and the best solution for the linear program is at least . Taking large enough  the proof or the theorem follows.
\end{proof}
It remains to show the following lemma:
\begin{lemma}\label{lem:clustering-cost-bipartite}
The cost of the optimum clustering for the weight function  is at least .
\end{lemma}
\begin{proof}
Consider the optimum clustering of  denoted .
\begin{claim}\label{clm:cluster-size}
No cluster  has size greater than .
\end{claim}
\begin{proof}
We give a proof by contradiction.
Assume there is a cluster  of size greater than .
Let  and . W.l.o.g we can assume that .
We will show that splitting  into two new clusters  and  gives a solution of smaller cost.
Let  consist of  arbitrary vertices from  and  arbitrary vertices from . Let . The difference between the cost of the clustering  and  is equal to  or in other words the difference between the number of positive and negative edges between  and .
Note that:

This implies that if  then the difference is negative, a contradiction.
\end{proof}
Using Claim~\ref{clm:cluster-size} and the fact that the girth of  is  we conclude that the subset of  contained inside each  forms a forest.
Thus, the total number of edges from  which are not contained inside any cluster is at least .
\end{proof}


\subsection{Integrality Gap for Complete Graphs with Triangle Inequalities}
See Section~\ref{sec:triangle-inequalities} for a formal definition of correlation clustering
in weighted complete graphs with triangle inequalities.
\begin{theorem}\label{thm:integrality-gap-triangle-inequalities}
For every  there exists an instance of correlation clustering in complete graphs, which satisfies triangle inequalities and has integrality gap .
\end{theorem}
\begin{proof}
Let  where .
We define a weight function  as follows:

Consider the following solution to the linear program:

This solution satisfies triangle inequalities. The cost of each edge  is  and the cost of each edge  is . Overall, the cost is .
As we will show below in Lemma~\ref{lem:clustering-cost-triangle-inequalities}, the cost of the optimum clustering is at least  concluding the proof.
\end{proof}

\begin{lemma}\label{lem:clustering-cost-triangle-inequalities}
The cost of the optimum clustering for the weight function  is at least .
\end{lemma}
\begin{proof}
We will show that the optimum clustering is a single cluster . Then the lemma follows since the cost of such clustering is  for the edges in  plus  for the edges not in , giving the overall cost of .

We will give a proof by contradiction, assuming that the optimum clustering has multiple clusters .
For every cluster  let  and .
Because  there exists a pair of clusters  and  such that  while .
Consider a new clustering where  and  are replaced with , while all other clusters are unchanged.
The difference between the cost of the new clustering and  is equal to:


\end{proof}





 \subsection{\texorpdfstring{Lower Bound of  for the Complete Case}{Lower Bound of 2.025 for the Complete Case}}\label{sec:lbd}
We will now argue that within the algorithm and analysis framework described in Section~\ref{sec:alg}, no choice of the rounding functions  and  can give an approximation factor better than . More specifically, no functions  and 
satisfy the conditions of Lemma~\ref{lem:alpha} with .

\begin{theorem}\label{thm:limitation-complete}
For any functions  and  that satisfy the conditions of
Lemma~\ref{lem:alpha} on all triangle types, the ratio  must be larger than .
\end{theorem}
\begin{proof}
Our lower bound argument follows the same kind of approach we used in Section~\ref{sec:choosing_f} to identify good rounding functions for the upper bound, however, presents a better analysis of the \ppm-triangles with edge lengths .

Fix  and  satisfying the conditions of Lemma~\ref{lem:alpha} on all triangle types
for some . From \prettyref{lem:fp_upper} and \prettyref{cor:fmbound}, we know that

Using equations \prettyref{eq:ecost} and  \prettyref{eq:elp}
for \ppm-triangles with edge lengths  (where ),
we write  as follows:

Rearranging terms, we get

We claim that the coefficient  of  is always
positive: From~(\ref{eq:fp-bound}) using , we get

The last inequality holds, since .
We now replace  in (\ref{eq:sq2}) with .
By inequality~(\ref{eq:fn-bound}), we have

Once again rearranging terms we get

This is a quadratic equation with a positive leading coefficient. The left hand side
is negative if  lies between the roots of the equation.  For  and , we get  (the first root is rounded down; the second root is rounded up). This contradicts to inequality~(\ref{eq:fp-bound}), since  (See \prettyref{fig:lb}).
We conclude that  does not satisfy the conditions of Lemma~\ref{lem:alpha}.
\end{proof}

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth,natwidth=288,natheight=212]{images/lowerbound.png}
\caption{Plot of the bounds on  as a function of the
  approximation ration ; the upper bound from \prettyref{eq:fp-bound} is shown in rainbow, and the lower bound from \prettyref{eq:root_bounds} is shown in gray. The region of intersection is where we obtain the contradiction.}
\label{fig:lb}
\end{figure}

\subsection{\texorpdfstring{Lower bound of  for Weighted Triangle Inequalities}{Lower bound of 1.5 for Weighted Triangle Inequalities}}
A similar bound can be obtained for the weighted triangle inequalities
case.

\begin{theorem}\label{thm:limitation-wti}
	For any monotone function , the approximation ratio  must be at least .
\end{theorem}

The proof proceeds in a manner similar to that of
\prettyref{thm:limitation-complete}; we omit some of the details, but give a
proof sketch below.
\begin{proof}(Sketch)
Let  be the probability of cutting an edge with edge weight  and LP
value .

Let  be the polynomial given by
 for a triangle with edge weights  and
corresponding LP lengths  (note that the costs are
calculated differently in the case of weighted edges, as compared to the
integral edge constraints case). We prove our lower bound by showing that there
is no  that gets an approximation ; that is, for the
purpose of this proof we will only take , and
, and so we abbreviate  for ease of
notation.

We first consider a triangle with weights . Clearly, it must be that
. Thus, we can solve for the roots of  in
, which has leading positive coefficient, and so we
have that to get approximation ,

for  there are values of  for which these roots are complex, and in particular this means that there must be a discontinuity in ; the figure below shows the area in which  may be to achieve .
\begin{center}
	\includegraphics[width=0.4\textwidth]{images/triangle_inequalities.pdf}
\end{center}

It thus remains to rule out a discontinuous . Thus, suppose to the
contrary that there is a discontinuity in  at . By considering a
triangle that has one edge just barely longer than  and another edge just
barely shorter than , we will obtain our contradiction.  The minimum value
of the upper root is larger than , and the maximum value of the
lower root is less than , therefore as ,
 for some fixed positive ,
and similarly  for some fixed
positive . By examining the roots of  in  as , we see that there are no values
of  for which  gives an approximation better than , and we have our conclusion.
\end{proof}
 \section{Weighted Case}\label{sec:weighted}
In this section, we consider weighted instances of the problem. We assume that every
edge is partially positive and partially negative. Specifically, every edge 
has a positive weight  and a negative weight . The goal now
is to minimize the weight of unsatisfied constraints defined as follows:

We study complete graphs with .

This problem has been studied by Charikar et. al.~\cite{CGW05}, Ailon et.
al.~\cite{ACN08}, and others. The edge weights represent our confidence level
that the edge is positive or negative.

A slight modification of our algorithm gives  for this problem: For every edge  the algorithm flips a coin and assigns  with probability  and 
with probability . The analysis of the
algorithm are essentially the same as for unweighted case, since in the
weighted case, the expression  can be represented
as a convex combination of unweighted expressions .

Here we give a black box reduction of the weighted problem to the unweighted problem.
The reduction works with any approximation algorithm.

\begin{theorem}
If there exists an -approximation algorithm for the unweighted Correlation Clustering Problem on complete graphs, then there exists an -approximation algorithm
for the Weighted Correlation Clustering problem on complete graphs.
\end{theorem}
\begin{proof}[Sketch of the proof]
Given a weighted instance of the problem, we construct an unweighted instance:
We pick a sufficiently large integer number  e.g. . Then, we replace
every vertex  with  vertices , . We add a positive edge between
 and  with probability  and a negative edge with
probability . Then, we add positive edges between all  and .
Thus, we get a complete unweighted graph . We run the -approximation algorithm
on  and obtain a clustering of vertices of . Now, for every ,
we pick a random  and assign  to the cluster of .

It is easy to see that the algorithm gives  approximation, since
(1) the cost of the optimal solution for  is at most  times the cost of the optimal
solution for ; (2) the expected cost of clustering for   equals to the cost of
the solution for  scaled by  up to
an additive error term of . The latter statement follows from the following lemma.

\begin{lemma}
With high probability over choice of random labels  on the edges ,
for all clusterings of vertices  and  the following inequality holds:

Here, probability is taken over random .
\end{lemma}
\begin{proof}
The proof is very standard: There are at most  ways to partition
vertices  and . For a every fixed partition, the probability that
(\ref{eq:weighted-chernoff}) is violated is at most  (by the
Chernoff bound; note, that the number of pairs  is ). Hence, by the union bound,
(\ref{eq:weighted-chernoff}) holds for all partitions with high probability.
\end{proof}
\end{proof}

\subsection{Weights Satisfying the Triangle Inequality}\label{sec:triangle-inequalities}
Gionis, Mannila, and Tsaparas~\cite{GMT07} proposed a variant of the Weighted
Correlation Clustering problems with negative weights satisfying the triangle
inequality i.e., 
for all . They gave a 3-approximation algorithm for the problem.
Ailon, Charikar, and Newman~\cite{ACN08} improved the approximation factor to .
We show that for the rounding functions  and ,
the approximation ratio is . Our proof is computer assisted.  Note that
 is a convex function; and  is a concave function, so we can
apply~Lemma~\ref{lem:tight-triangle} and limit the case analysis only to
triangles with tight triangle inequality constraints. For an appropriate ,
if we pick  and , we
get a  approximation.
\begin{theorem}\label{thm:weighted-triang-ineq}
There exists a deterministic polynomial-time algorithm for the Correlation
Clustering Problem that gives a -approximation for weighted complete
graphs satisfying the triangle inequalities on .
\end{theorem}
The proof is also computer assisted.
 \section{Derandomization}\label{sec:derand}
We now derandomize the approximation algorithm presented in the previous section. The deterministic algorithm
will always return a solution of cost at most , where  is the parameter from Lemma~\ref{lem:alpha}.

\begin{theorem}
Suppose that the conditions of Lemma~\ref{lem:alpha} are satisfied for a set of functions 
and a parameter . Then there exists a polynomial-time deterministic algorithm with approximation ratio
.
\end{theorem}
\begin{proof}
We slightly tweak the algorithm: After the algorithm set probabilities  using functions ,
we change the value of each  to either  or  to maximize the function:

which is equal to (see the previous section for details).
For each , the algorithm fixes the value of all other variables  and picks the value for  greedily.
Then, the algorithm picks the pivot  for which 
is maximized. Then, the algorithm as before adds every  to  with probability .
Now, however, all probabilities  are equal to  or . So the algorithm assigns each  to  deterministically.

The main observation is that function~(\ref{eq:sum-uvw}) is convex in every variable . Indeed, all terms
 and  are quadratic functions of , , and . Moreover,
all terms  and  (except for ) are linear in each variable . The only nonlinear terms are
 for  having a self-loop. But it is concave as a function of . So  is a convex function in every variable
.

Therefore, after rounding each  to 0 or 1 the value of~(\ref{eq:sum-uvw}) may only increase. Initially, after setting 's using functions , it is nonnegative. So after replacing all 's with 's and 's, it is also nonnegative. Hence, , and,
consequently, for some pivot , . We showed that for some pivot , the number
of violated constraints at step  is upper bounded by  times the LP volume removed at this step. Hence, the total number of violated constraints
is upper bounded by .
\end{proof}

 
\section{\texorpdfstring{Analysis of the -partite case}{Analysis of the k-partite case}}\label{sec:kpart}
\begin{proof}[Proof of Theorem~\ref{thm:kpart}]
By Lemma~\ref{lem:alpha}, it suffices to show that  for
every triangle~ in the graph. There are  possible types of triangles in a complete -partite graph:
\ppp-triangles, \ppm-triangles, \pmm-triangles, and \mmm-triangles may exist
when each vertex is a member of different partitions;
\ppn-triangles, \pmn-triangles, and \mmn-triangles can occur when two vertices
are members of the same partition and the third vertex is a member of a
different partition. When all three vertices belong to one partition, all edges
are neutral, and so none contribute to the cost.

We adopt the convention that the triangles have LP lengths , with vertex
 opposite edge , vertex  opposite edge , and vertex  opposite
edge  (see figure below). Costs incurred when vertex  is a pivot will be enclosed in square brackets with subscript , as in .
Instead of writing,  and , we write  and .
		\begin{center}
			\tikz\draw (1,1.3) node[above]{u} -- node[right]{a}
			(2,0) node[right]{v} -- node[below]{b}
			(0,0) node[left]{w} -- node[left]{c} (1,1.3);
		\end{center}

\medskip

\noindent\textbf{\pmn-Triangles:}
We first analyze the most interesting case: what happens if our triangle has edge labels \pmn. Consider a \pmn-triangle with side lengths
 respectively. We have
5pt]
LP  &= [(1-b)\cdot(1-p_{uw}p_{uv})]_u + [a\cdot(1-p_{uw}p_{vw})]_w\\
&= (1-b)(1-f^+_3(a)\fnk(c)) + a(1 - f^-_3(b)\fnk(c)).

3\cdot LP - ALG &= 2 + 3a + b(3c - 4 - \frac{9}{2}ac)\ge 2 + 3a + (a+c)(3c - 4 - \frac{9}{2}ac)\\
				&= (2 - a) + ac(3 - \frac{9}{2}a - \frac{9}{2}c) + (3c^2 - 4c).
3\cdot LP - ALG \ge \tfrac{1}{3} - \tfrac{3}{2}ac \ge 0.
3\cdot LP - ALG &= 3 + 3a - 4(b+c) + \tfrac{15}{2}bc - \tfrac{9}{2}abc
				\ge 4 - 4(b+c) + \tfrac{15}{2}bc - \tfrac{3}{2}bc\\
				&= 4(1-b)(1-c),

				ALG &= [f^+_3(c)(1-f^+_3(a)) + f^+_3(a)(1-f^+_3(c))]_u
					 + [f^+_3(a)(1-f^+_3(b)) + f^+_3(b)(1-f^+_3(a))]_v\\
					 &\qquad + [f^+_3(b)(1-f^+_3(c)) + f^+_3(c)(1-f^+_3(b))]_w,\\
				LP &= [b(1-f^+_3(c)f^+_3(a))]_u
					 + [c(1-f^+_3(a)f^+_3(b))]_v
					 + [a(1-f^+_3(b)f^+_3(c))]_w.
				
			ALG &= [(1-f^-_3(a))(1-f^-_3(c))]_u + [(1-f^-_3(a))(1-f^-_3(b))]_v + [(1-f^-_3(c))(1-f^-_3(b))]_w\\
				&= 3 - 2a - 2b - 2c + ab + bc + ac,\\
			LP &= [(1-b)(1-f^-_3(a)f^-_3(c))]_u + [(1-c)(1-f^-_3(a)f^-_3(b))]_v + [(1-a)(1-f^-_3(c))f^-_3(b))]_w\\
			   &= 3 - a - b - c - ab - ac - bc + 3abc.
		
			3\cdot LP - ALG &= 6 - a - b - c - 4ab - 4ac - 4bc + 9abc \\
							&= 6 - a - b - 4ab - c(4a(1-b) + 4b(1-a) + (1 - ab))\\
							&\ge 5 - 5a - 5b + 5ab= 5(1-a)(1-b),
		
		ALG &= [\fpk(a)(1-\fmk(c)) + \fmk(c)(1-\fpk(a))]_u
		+ [(1-\fpk(a))(1-\fpk(b))]_v
			\\&\qquad  + [f^+_3(b)(1-f^-_3(c)) + f^-_3(c)(1-f^+_3(b))]_w,\\
		LP &= [b(1 - \fpk(a)\fmk(c))]_u + [(1-c)(1-\fpk(a)\fpk(b))]_v + [a(1-\fpk(b)\fmk(c))]_w.
	
		ALG &= [(1-\fpk(a))(1-\fmk(c))]_u
		      + [(1-\fpk(a))(1-\fmk(b))]_v\\
			  &\qquad + [\fmk(c)(1-\fmk(b)) + \fmk(b)(1-\fmk(c))]_w,\\
		LP &= [(1-b)(1-\fpk(a)\fmk(c))]_u +[(1-c)(1-\fpk(a)\fmk(b))]_v + [a(1-\fmk(b)\fmk(c))]_w.
	
		3\cdot LP - ALG &= 4 - 3b - 3c + 3a + 2bc - 3abc
		= 4(1-b)(1-c) + b + c - 2bc + 3a(1-bc)\\
		&= (1 - bc) + 3(1-b)(1-c) + 3a(1-bc)
		\ge 0,
	
		3\cdot LP - ALG &= 6 - 7b - 7c + 8bc + 3a(1-bc) \ge 7 - 7b - 7c + 7bc \\
			   &= 7(1-b)(1-c) \ge 0,
	
		ALG &= [\fnk(c)(1-\fpk(a)) + \fpk(a)(1-\fnk(c))]_u
			  +[\fnk(c)(1-\fpk(b)) + \fpk(b)(1-\fnk(c))]_w\\
		LP &=[b(1-\fnk(c)\fpk(a))]_u
			  +[a(1-\fnk(c)\fpk(b))]_w
	
		ALG &= [(1-\fnk(c))(1-\fmk(a))]_u
		+[(1-\fnk(c))(1-\fmk(b))]_w\\
		&= (1-\tfrac{3}{2}c)(2-a-b),\\
		LP &=[(1-b)(1-\fnk(c)\fmk(a))]_u
		+[(1-a)(1-\fnk(c)\fmk(b))]_w\\
		&= (1-b)(1-\tfrac{3}{2}ac) + (1-a)(1-\tfrac{3}{2}bc)
	
			LP - ALG= 3c(1-b)(1-a)\ge 0.
		
ALG
&= [(1-\fm(y))(1-\fm(z))]_u
+ [(1-\fm(x))(1-\fm(z))]_v
+ [(1-\fm(y))(1-\fm(x))]_w, \\
&= 3 - 2x - 2y - 2z + xy + xz + yz,\\
LP
&=
[(1-x)(1-\fm(y)\fm(z))]_u
[(1-y)(1-\fm(x)\fm(z))]_v
[(1-z)(1-\fm(x)\fm(y))]_w,\\
&= 3 - x - y - z - xy - xz - yz + 3xyz.

LP - ALG
&= x + y + z - 2xy - 2yz - 2xz + 3xyz\\
&= x(1-y)(1-z) + y(1-x)(1-z) + z(1-x)(1-y)\ge 0.

ALG &= [\fm(y)(1-\fm(z)) + \fm(z)(1-\fm(y))]_u
     + [(1-\fp(x))(1-\fm(z))]_v\\
     & \qquad + [(1-\fp(x))(1-\fm(y))]_w,\\
LP &= [x(1-\fm(z)\fm(y))]_u
     +[(1-y)(1-\fp(x)\fm(z))]_v
     +[(1-z)(1-\fp(x)\fm(y))]_w.

2\cdot LP - ALG &= 2(2 + x -y - z -xyz - \fp(x)z - \fp(x) y + 2y\fp(x)z
\\ & \qquad- (2 - 2yz  -  2\fp(x) + \fp(x)z + \fp(x)y)\\
&= 2(1-y)(1-z) + 2x(1 -yz) + \fp(x)(2 - 3z - 3y + 4yz) \\
&\ge 2(1-y)(1-z) + 3\fp(x)(1 - z - y + yz) \\
&= 2(1-y)(1-z) + 3\fp(x)(1 - z)(1 - y) \\
&\ge 0,

\fp(x) = \begin{cases} 0 & x < a \\
\tfrac{(x-a)^2}{(b-a)^2}  & a \le x \le b \\
1 & b < x\end{cases},

		\Cppm(x,y,z) &=
		\alpha\left(1 + x + y - z
 - yz\fp(x)
 - xz\fp(y)
 - (1-z)\fp(x)\fp(y)\right)\label{eq:def-Cppm}\\
 &\qquad	-\left(1 + 2z - 2z\fp(x) -2z\fp(y) + \fp(y)\fp(x)\right)\notag
 
\tfrac{\partial}{\partial c} \Cppm(x+c, x-c, y) = 0 \ \  \text{ at }\ \ c = 0.

\tfrac{\partial}{\partial c} \Cppm(x+c, x-c, y) \ge 0.

\alpha \cdot LP - ALG
&= \alpha(1 + 2x - y) - 1\\
& \qquad + y(2 + \alpha c - \alpha x)\fp(x+c) \ + y(2 - \alpha c - \alpha x)\fp(x-c)\\
&\qquad - (1+\alpha-\alpha y)\fp(x+c)\fp(x-c),

\tfrac{\partial}{\partial c}(\alpha \cdot LP - ALG)
&= \alpha y\fp(x+c) - \alpha y\fp(x-c)\\
&\qquad + y(2 + \alpha c - \alpha x)\tfrac{\partial}{\partial c}\fp(x+c) \ +
y(2 - \alpha c - \alpha x)\tfrac{\partial}{\partial c}\fp(x-c)\\
&\qquad- (1+\alpha - \alpha y)\fp(x+c)\tfrac{\partial}{\partial c}\fp(x-c)\ - (1+\alpha - \alpha y)\fp(x-c)\tfrac{\partial}{\partial c}\fp(x+c).

\tfrac{\partial}{\partial c}(\alpha \cdot LP - ALG)
&= \alpha y\fp(x+c) + y(2 + \alpha c - \alpha x)\tfrac{\partial}{\partial c}\fp(x+c),

		\pm\sqrt{
		\frac{
			(a-x)^2 +2y(1-\alpha \cdot a)(a-b)^2 + \alpha \left(
				(1-y)(x-a)^2
				+xy(b-a)^2\right)
	}{\alpha(1-y) + 1}},
	
		\frac{-(1 + \alpha(1-y))}{(b-a)^4} c^4,
	
		\Cppm(x,y,z)
		&= \left(\alpha(x+y) - 2z\right) + \left(\alpha(1-z) - 1\right)\\
		&\ge (\alpha-2)z + \left(\alpha(1 -2a) - 1 \right)\ge 0,
	
		\Cppm(x,y,z) = \alpha(1+ x+y - z - xz) - 1
		\ge \alpha - \alpha xz - 1\ge 0.
	
		\alpha\cdot LP - ALG
		&= (\alpha - 1)  - (\alpha + 1)\fp(x)^2 + 4x(2\fp(x)-1) - 2\alpha x\fp(x)(2x-\fp(x)).
	\Cppm(x,y,z) = x^2\fp(y) + x(\alpha(2-y\fp(y)) + 2(1-\fp(y))) + (\alpha-1-2y+2y\fp(y)) 
	\Cppm &= \alpha(1+2x-\fp(x)^2) - (1+\fp(x)^2)\\
				   &= (\alpha - 2) + (1 - \fp(x)^2) + \alpha(2x-\fp(x)^2),\\

\Cppp(x,y,z) &= \alpha\left(
x + y + z - y\fp(z)\fp(x)
 - x\fp(z)\fp(y)
 -z\fp(x)\fp(y)
	\right) \label{eq:def-Cppp}\\
	&\qquad - 2\left(\fp(x) + \fp(y) + \fp(z) - \fp(y)\fp(x) - \fp(y)\fp(z) - \fp(z)\fp(x)\right)\notag
 
		\frac{(2 - \alpha y)}{(b-a)^4} c^4,
	
		\pm\sqrt{
		\frac{
			(2 - \fp(y) ( 2  - \alpha 2a +\alpha x) )(a-b)^2 + (2-\alpha y)(a-x)^2
	}{2-\alpha y}}.
	
		(2 - \fp(y) ( 2  - \alpha 2a +\alpha x) )
		&\ge 2 - \fp(2x)(2 - \alpha 2a + \alpha x)\\
		&\ge 2 - \fp(b)(2 - \alpha 2a + \alpha \tfrac{b}{2})\\
		&\approx 0.25
	
		x_{uv} &= x + r(x),\\
		x_{vw} &= x - r(x),\\
		x_{uw} &= 2x,
	
		r(x) =
		\sqrt{
		\frac{
			2(1-\fp(2x))(a-b)^2 + (2- 2 \alpha x)(a-x)^2
			+\alpha \fp(2x)( 2 a - x)(a-b)^2
	}{2-2 \alpha x }}.
	
		\Cppp(a,2x-a,2x)
		&\approx -2.73172+27.4989 x+133.045 x^2-1407.86 x^3+2469.93 x^4
	
	\Cppp(x-c,x+c,2x)
	&=
	4 \alpha x - 2\fp(x + c) + \fp(2x)((2 + \alpha c - \alpha x)\fp(x + c)-2)

	r(x) & = \frac{4b(b-2a) + \alpha a^3 + 16 a x - 3\alpha a^2 x - 16{x}^2 + 4\alpha {x}^3 }{3\alpha(2x-a)^2}.

	x &\ge r(x),
	3x\alpha(2x - a)^2 -\Big( 4b(b-2a) + \alpha a^3 + 16 a x - 3\alpha a^2 x - 16{x}^2 + 4\alpha {x}^3\Big)  &\ge 0.

	\Cppp(x-r(x),x+r(x),2x)
	&=-\frac{59980.5}{(0.19-2x)^6} (x-0.254936) (x -0.237264) (x+0.132462)\\
 & \qquad \times \left(0.739503-1.53615 x+{x}^2\right) \left(0.079347-0.452156 x+{x}^2\right)\\
	&\qquad \times \left(0.009025-0.19 x+{x}^2\right) \left(0.000908218+0.0367349 x+{x}^2\right)

	\Cppp(0,y,y)
	&= 2(\alpha y - 2\fp(y) + \fp(y)^2)

		\Cppp(x,y,z)
		= \alpha(x+y+z) - 2\fp(z)
		\ge 2\alpha z - 2\fp(z),
	
	where we have applied the triangle inequality, and since ,
	this quantity is positive.


\medskip
 \noindent III. 	In the case ,
we simply appeal to \prettyref{lem:ppp_all_in}.

\medskip
\noindent IV. In the case , we appeal to \prettyref{lem:ppp-less-in-in}.

This completes our case analysis.
\end{proof}
 \end{document}
