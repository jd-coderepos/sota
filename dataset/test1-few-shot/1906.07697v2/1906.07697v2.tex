\documentclass{article}
\usepackage[T1]{fontenc}    

\newcommand{\Versa}{\textsc{Versa}}
\renewcommand{\d}{\mathrm{d}}

\usepackage[numbers]{natbib}
\renewcommand{\bibname}{References}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}
\usepackage{adjustbox}
\usepackage{caption}
\captionsetup{font=footnotesize}
\captionsetup[table]{font=footnotesize,skip=0pt}
\captionsetup[figure]{font=footnotesize,skip=0pt}

\usepackage{microtype}      \usepackage[utf8]{inputenc} \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{tabularx}       \usepackage{makecell}       \usepackage{ragged2e}       \usepackage{paralist}       \usepackage{arydshln}       

\usepackage{import}         \usepackage{standalone}     \usepackage{titlesec}       \usepackage{multirow}
\usepackage{enumitem}
  \newlist{inlinelist}{enumerate*}{1}
  \setlist*[inlinelist,1]{label=(\roman*),
      }
\usepackage{amsmath}        \usepackage{cases}          \usepackage{amsthm}         \usepackage{thmtools}       \usepackage{mathtools}      \usepackage{subcaption}     \usepackage{floatrow}       \usepackage{pgf}
\usepackage{tikz}
\usepackage{pgfplots}
	\usepgfplotslibrary{groupplots}
  \usetikzlibrary{calc}
  \usetikzlibrary{positioning}
  \usetikzlibrary{angles,quotes}
  \usetikzlibrary{backgrounds}
  \usetikzlibrary{external}
  \usetikzlibrary{fit}
  \usetikzlibrary{bayesnet}
  \usetikzlibrary{calc}
  \usetikzlibrary{fit}
  \usetikzlibrary{spy, shadows}
  \usetikzlibrary{arrows.meta}
  \usetikzlibrary{shadings}
  \usetikzlibrary{fadings}
\usetikzlibrary{matrix}     \usepackage[
    noabbrev,
    capitalize,
    nameinlink]{cleveref}   

\DeclareRobustCommand{\openplus}{\begin{tikzpicture}[baseline=-.8ex, line width=.5, scale=0.075]
        \draw (0,0) -- (0,1) -- (1,1) -- (1,0) -- (2,0) -- (2,-1) -- (1,-1) -- (1,-2) -- (0,-2) -- (0,-1) -- (-1,-1) -- (-1,0) -- cycle;
    \end{tikzpicture}}


\begingroup
    \makeatletter
    \@for\theoremstyle:=theorem,corollary,lemma,model,definition,remark,plain\do{\expandafter\g@addto@macro\csname th@\theoremstyle\endcsname{\setlength\thm@preskip\parskip
            \setlength\thm@postskip\parskip \addtolength\thm@preskip\parskip
            }}
    \makeatother
\endgroup

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}

\crefname{theorem}{Theorem}{Theorems}
\Crefname{theorem}{Theorem}{Theorems}
\crefname{lemma}{Lemma}{Lemmas}
\Crefname{lemma}{Lemma}{Lemmas}
\crefname{proposition}{Proposition}{Propositions}
\Crefname{proposition}{Proposition}{Propositions}
\crefname{definition}{Definition}{Definitions}
\Crefname{definition}{Definition}{Definitions}

\newcommand{\ts}{\textsuperscript}

\newcolumntype{L}{>{\RaggedRight\arraybackslash}X}


\newcommand{\cnaps}{\textsc{CNAPs}}

\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\hypersetup{
    colorlinks=true,
    linkcolor=mydarkblue,
    citecolor=mydarkblue,
    filecolor=mydarkblue,
    urlcolor=mydarkblue}


\definecolor{tabgreen}{HTML}{59a14f}


\titlespacing{\section}{0pt}{.5\parskip}{.25\parskip}



\newcommand{\jradd}[1]{}
\newcommand{\jbadd}[1]{}
\newcommand{\jgadd}[1]{}
\newcommand{\retadd}[1]{}
\newcommand{\snadd}[1]{}

 

\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\vgamma{{\bm{\gamma}}}
\def\vbeta{{\bm{\beta}}}
\def\vphi{{\bm{\phi}}}
\def\vpi{{\bm{\pi}}}
\def\vpsi{{\bm{\psi}}}
\def\vsigma{{\bm{\sigma}}}
\def\vepsilon{{\bm{\epsilon}}}
\def\vvarphi{{\bm{\varphi}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}
\def\mPsi{{\bm{\Psi}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 \usepackage{nips_2018}
\usepackage{standalone} \standalonetrue
\usepackage{algorithm}
\usepackage{algpseudocode}


\begin{document}

\newpage
\appendix
\title{Supplementary Material for Fast and Flexible Multi-Task Classification Using Conditional Neural Adaptive Processes}
\author{
 James Requeima\footnotemark[1] \\
 University of Cambridge \\
 Invenia Labs \\
 \texttt{jrr41@cam.ac.uk} \\
 \And
 Jonathan Gordon\footnotemark[1] \\
 University of Cambridge \\
 \texttt{jg801@cam.ac.uk} \\
 \And
 John Bronskill\footnotemark[1] \\
 University of Cambridge \\
 \texttt{jfb54@cam.ac.uk} \\
 \And
 Sebastian Nowozin \\
 Google Research Berlin \\
 \texttt{nowozin@google.com}\\
 \And
 Richard E.~Turner \\
 University of Cambridge \\
 Microsoft Research \\
 \texttt{ret26@cam.ac.uk}
}




\section{Algorithm for Constructing Stochastic Estimator}
\label{app:stochastic_estimator}

An algorithm for constructing the stochastic training objective   for a single task  is given in \cref{alg:stochastic_estimator}.  denotes a the likelihood of a categorical distribution with parameter vector . This algorithm can be used on a batch of tasks to construct an unbiased estimator for the auto-regressive likelihood of the task outputs.


\begin{algorithm}[h]
\caption{Stochastic Objective Estimator for Meta-Training.}\label{alg:stochastic_estimator}
\begin{algorithmic}[1]
\Procedure{Meta-Training}{}
\State 
\State 
\For{\texttt{}}
    \State  
    \State  
\EndFor
\State \textbf{return} 
\EndProcedure
\end{algorithmic}
\end{algorithm}




\section{Additional Related Work Details}
\label{app:related_work}

\paragraph{The choice of task-specific parameters .} Clearly, any approach to multi-task classification must adapt, at the very least, the top-level classifier layer of the model. A number of successful models have proposed doing just this with e.g., neighbourhood-based approaches \citep{snell2017prototypical}, variational inference \citep{bauer2017discriminative}, or inference networks \citep{gordon2018meta}. On the other end of the spectrum are models that adapt \textit{all} the parameters of the classifier, e.g., \citep{finn2017model,nichol2018reptile,kim2018bayesian}. The trade-off here is clear: as more parameters are adapted, the resulting model is more flexible, but also slow and prone to over-fitting. For this reason we modulate a small portion of the network parameters, following recent work on multi-task learning \citep{rebuffi2017learning, rebuffi2018efficient, perez2018film}.

We argue that just adapting the linear classification layer is sufficient when the task distribution is not diverse, as in the standard benchmarks used for few-shot classification (OMNIGLOT \citep{lake2011one} and \textit{mini}-imageNet \citep{ravi2016optimization}). However, when faced with a diverse set of tasks, such as that introduced recently by \citet{triantafillou2019meta}, it is important to adapt the feature extractor on a per-task basis as well. 

\paragraph{The adaptation mechanism .} Adaptation varies in the literature from performing full gradient descent learning with  \cite{yosinski2014transferable} to relying on simple operations such as taking the mean of class-specific feature representations \citep{snell2017prototypical,vinyals2016matching}. Recent work has focused on reducing the number of required gradient steps by learning a global initialization \citep{finn2017model,nichol2018reptile} or additional parameters of the optimization procedure \citep{ravi2016optimization}. Gradient-based procedures have the benefit of being flexible, but are computationally demanding, and prone to over-fitting in the low-data regime. Another line of work has focused on learning neural networks to output the values of , which we denote \textit{amortization} \citep{gordon2018meta}. Amortization greatly reduces the cost of adaptation and enables sharing of global parameters, but may suffer from the amortization gap \citep{cremer2018inference} (i.e., underfitting), particularly in the large data regime. Recent work has proposed using semi-amortized inference \citep{triantafillou2019meta,rusu2018meta}, but have done so while only adapting the classification layer parameters. 


\section{Experimentation Details}
\label{app: experimentation_details}
All experiments were implemented in PyTorch \citep{paszke2017automatic} and executed either on NVIDIA Tesla P100-PCIE-16GB or Tesla V100-SXM2-16GB GPUs. The full \cnaps{} model runs in a distributed fashion across 2 GPUs and takes approximately one and a half days to complete episodic training and testing.

\subsection{\textsc{Meta-Dataset} Training and Evaluation Procedure}
\label{app:meta_dataset_procedure}

\subsubsection{Feature Extractor Weights \texorpdfstring{}{TEXT} Pretraining}
\label{app:feature_extractor_pretraining}
We first reduce the size of the images in the ImageNet ILSVRC-2012 dataset \citep{russakovsky2015imagenet} to 84  84 pixels. Some images in the ImageNet ILSVRC-2012 dataset are duplicates of images in other datasets included in \textsc{Meta-Dataset}, so these were removed. We then split the 1000 training classes of the ImageNet ILSVRC-2012 dataset into training, validation, and test sets according to the criteria detailed in \citep{triantafillou2019meta}. The test set consists of the 130 leaf-node subclasses of the ``device" synset node, the validation set consists of the the 158 leaf-node subclasses of the ``carnivore" synset node, and the training set consists of the remaining 712 leaf-node classes. We then pretrain a feature extractor with parameters  based on a modified ResNet-18 \citep{he2016deep} architecture on the above 712 training classes. The ResNet-18 architecture is detailed in \cref{table:pre_trained_resnet_architecture}. Compared to a standard ResNet-18, we reduced the initial convolution kernel size from 7 to 5 and eliminated the initial max-pool step. These changes were made to accommodate the reduced size of the imagenet training images. We train for 125 epochs using stochastic gradient descent with momentum of 0.9, weight decay equal to 0.0001, a batch size of 256, and an initial learning rate of 0.1 that decreases by a factor of 10 every 25 epochs. During pretraining, the training dataset was augmented with random crops, random horizontal flips, and random color jitter. The top-1 accuracy after pretraining was 63.9. For all subsequent training and evaluation steps, the ResNet-18 weights were frozen.The dimensionality of the feature extractor output is . The hyper-parameters used were derived from the PyTorch \citep{paszke2017automatic} ResNet training tutorial. The only tuning that was performed was on the number of epochs used for training and the interval at which the learning rate was decreased. For the number of epochs, we tried both 90 and 125 epochs and selected 125, which resulted in slightly higher accuracy. We also found that dropping the learning rate at an interval of 25 versus 30 epochs resulted in slightly higher accuracy. 

\subsubsection{Episodic Training of \texorpdfstring{}{TEXT}}
\label{app:hypernet_meta_training}
Next we train the functions that generate the parameters ,  for the feature extractor adapters and the linear classifier, respectively. We train two variants of \cnaps{} (on ImageNet ILSVRC-2012 only and all datasets - see \cref{table:datasets}). We generate training and validation episodes using the reader from \citep{Triantafillou2019code}. We train in an end-to-end fashion for 110,000 episodes with the Adam \citep{kingma2014adam} optimizer, using a batch size of 16 episodes, and a fixed learning rate of 0.0005. We validate using 200 episodes per validation dataset. Note that when training on ILSVRC only, we validate on ILSVRC only, however, when training on all datasets, we validate on all datasets that have validation data (see \cref{table:datasets}) and consider a model to be better if more than half of the datasets have a higher classification accuracy than the current best model. No data augmentation was employed during the training of . Note that while training  the feature extractor  is in `eval' mode (i.e. it will use the fixed batch normalization statistics learned during pretraining the feature extractor weights  with a moving average). No batch normalization is used in any of the functions generating the  parameters, with the exception of the set encoder  (that generates the global task representation ). Note that the target points are never passed through the set encoder . Again, very little hyper-parameter tuning was performed. No grid search or other hyper-parameter search was used. For learning rate we tried both 0.0001 and 0.0005, and selected the latter. We experimented with the number of training episodes in the range of 80,000 to 140,000, with 110,000 episodes generally yielding the best results. We also tried lowering the batch size to 8, but that led to decreased accuracy.


\subsubsection{Evaluation}
\label{app:evaluation}
 We generate test episodes using the reader from \citep{Triantafillou2019code}. We test all models with 600 episodes each on all test datasets. The classification accuracy is averaged over the episodes and a 95\% confidence interval is computed. We compare the best validation and fully trained models in terms of accuracy and use the best of the two. Note that during evaluation, the feature extractor  is also in `eval' mode.

\begin{table}[h]
    \caption{Datasets used to train, validate, and test models.}
    \label{table:datasets}
    \tiny
	\centering
	\begin{tabular}{lll|lll}
		\toprule
		\multicolumn{3}{c}{ImageNet ILSVRC-2012} & \multicolumn{3}{c}{All Datasets}\\
		\midrule
		\textbf{Train} & \textbf{Validation} & \textbf{Test} & \textbf{Train} & \textbf{Validation} & \textbf{Test} \\
        \midrule
        ILSVRC \citep{russakovsky2015imagenet} & ILSVRC \citep{russakovsky2015imagenet} & ILSVRC \citep{russakovsky2015imagenet}    & ILSVRC \citep{russakovsky2015imagenet}    & ILSVRC \citep{russakovsky2015imagenet}    & ILSVRC \citep{russakovsky2015imagenet}\\
               &        & Omniglot \citep{lake2011one}  & Omniglot  \citep{lake2011one}   & Omniglot  \citep{lake2011one}  & Omniglot  \citep{lake2011one} \\
               &        & Aircraft \citep{maji2013fine}  & Aircraft \citep{maji2013fine}  & Aircraft \citep{maji2013fine}  & Aircraft \citep{maji2013fine} \\
               &        & Birds \citep{wah2011caltech}     & Birds  \citep{wah2011caltech}    & Birds \citep{wah2011caltech}     & Birds \citep{wah2011caltech} \\
               &        & Textures \citep{cimpoi2014describing}  & Textures \citep{cimpoi2014describing}  & Textures \citep{cimpoi2014describing}  & Textures \citep{cimpoi2014describing} \\
               &        & Quick Draw \citep{ha2017neural} & Quick Draw \citep{ha2017neural} & Quick Draw \citep{ha2017neural} & Quick Draw \citep{ha2017neural} \\
               &        & Fungi \citep{Schroeder2018FGVCx}     & Fungi \citep{Schroeder2018FGVCx}     & Fungi \citep{Schroeder2018FGVCx}     & Fungi \citep{Schroeder2018FGVCx} \\
               &        & VGG Flower \citep{nilsback2008automated} & VGG Flower \citep{nilsback2008automated} & VGG Flower \citep{nilsback2008automated} & VGG Flower \citep{nilsback2008automated} \\
               &        & MSCOCO \citep{lin2014microsoft}    &            & MSCOCO \citep{lin2014microsoft}    & MSCOCO \citep{lin2014microsoft} \\
               &        & Traffic Signs \citep{houben2013detection} &         &            & Traffic Signs \citep{houben2013detection} \\
               &        & MNIST   \citep{lecun2010mnist}   &            &               & MNIST \citep{lecun2010mnist} \\
               &        & CIFAR10  \cite{krizhevsky2009learning}  &            &               & CIFAR10 \cite{krizhevsky2009learning} \\
               &        & CIFAR100 \cite{krizhevsky2009learning}  &            &               & CIFAR100 \cite{krizhevsky2009learning} \\        \bottomrule
	\end{tabular}
    \vspace{2mm}
\end{table}



\section{Additional Few-Shot Classification Results}
\label{app:additional_results}

\subsection{Few-Shot Classification Results When Training on ILSVRC-2012 only}
\label{app:meta_dataset_imagenet}

\cref{table:meta_dataset_results_imagenet} shows few-shot classification results on \textsc{Meta-Dataset} when trained on ILSVRC-2012 only. We emphasize that this scenario does not capture the key focus of our work, and that these results are provided mainly for completeness and compatibility with the work of \citet{triantafillou2019meta}. In particular, our method relies on training the parameters  to adapt the conditional predictive distribution to new datasets. In this setting, the model is never presented with data that has not been used to pre-train , and therefore cannot learn to appropriately adapt the network to new datasets. Despite this, \cnaps{} demonstrate competitive results with the methods evaluated by \citet{triantafillou2019meta} even in this scenario.




\begin{table}[t]
\caption{Few-shot classification results on \textsc{Meta-Dataset} \citep{triantafillou2019meta} using models trained on ILSVRC-2012 only. All figures are percentages and the  sign indicates the 95\% confidence interval. Bold text indicates the highest scores that overlap in their confidence intervals. Results from competitive methods from \citep{triantafillou2019meta}}
\label{table:meta_dataset_results_imagenet}
\small
\centering
\begin{tabular}{@{}lcccccc@{}}
\toprule
Dataset                                   & Finetune              & MatchingNet  & ProtoNet              & fo-MAML      & Proto-MAML            & \cnaps                \\
\midrule
ILSVRC \citep{russakovsky2015imagenet}    & 45.81.1          & 45.01.1 & \textbf{50.51.1} & 36.11.0 & \textbf{51.01.1} & \textbf{50.61.1} \\
\hdashline
Omniglot \citep{lake2011one}              & \textbf{60.91.6}          & 52.31.3 & 60.01.4          & 38.71.4 & \textbf{63.01.4} & 45.21.4          \\
Aircraft \citep{maji2013fine}             & \textbf{68.71.3} & 49.00.9 & 53.11.0          & 34.50.9 & 55.31.0          & 36.00.8          \\
Birds \citep{wah2011caltech}              & 57.31.3          & 62.21.0 & \textbf{68.81.0} & 49.11.2 & \textbf{66.91.0} & 60.70.9          \\
Textures \citep{cimpoi2014describing}     & \textbf{69.10.9} & 64.20.9 & 66.60.8          & 56.50.8 & \textbf{67.80.8} & \textbf{67.50.7} \\
Quick Draw \citep{ha2017neural}           & 42.61.2          & 42.91.1 & 49.01.1          & 27.21.2 & \textbf{53.71.1} & 42.31.0          \\
Fungi \citep{Schroeder2018FGVCx}          & \textbf{38.21.0} & 34.01.0 & \textbf{39.71.1} & 23.51.0 & \textbf{38.01.1} & 30.10.9          \\
VGG Flower \citep{nilsback2008automated}  & \textbf{85.50.7} & 80.10.7 & \textbf{85.30.8} & 66.41.0 & \textbf{86.90.8} & 70.70.7          \\
Traffic Signs \citep{houben2013detection} & \textbf{66.81.3} & 47.81.1 & 47.11.1          & 33.21.3 & 51.21.1          & 53.30.9          \\
MSCOCO \citep{lin2014microsoft}           & 34.91.0          & 35.01.0 & 41.01.1          & 27.51.1 & \textbf{43.41.1} & \textbf{45.21.1} \\
MNIST \citep{lecun2010mnist}              &                       &              &                       &              &                       & \textbf{70.40.8} \\
CIFAR10 \citep{krizhevsky2009learning}    &                       &              &                       &              &                       & \textbf{65.20.8} \\
CIFAR100 \citep{krizhevsky2009learning}   &                       &              &                       &              &                       & \textbf{53.61.0} \\
\bottomrule
\end{tabular}
\end{table}


\subsection{Feature Extractor Parameter Learning}
\label{app:film_visualization}

\cref{fig:adaptation_tsne} shows t-SNE \citep{maaten2008visualizing} plots that visualize the output of the set encoder  and the FiLM layer parameters following the first and last convolutional layers of the feature extractor at test time. Even with unseen test data, the set encoder has learned to clearly separate examples arising from diverse datasets. The FiLM generators learn to generate feature extractor adaptation parameters unique to each dataset. The only significant overlap in the FiLM parameter plots is between CIFAR10 and CIFAR100 datasets which are closely related.

\begin{figure}[ht]

\centering
\includegraphics[width=1.0\textwidth]{figures/visual_analyses/film_tsnes_crop.pdf} \hfill
\caption{t-SNE plots of the output of the set encoder  and the FiLM layer parameters at the start () and end () of the feature extraction process at test time.}
\label{fig:adaptation_tsne}

\end{figure}

\subsection{Joint Training of \texorpdfstring{}{TEXT} and \textbf{\texorpdfstring{}{TEXT}} }
\label{app:training_experiments}

Our experiments in jointly training  and  show that the two-stage training procedure proposed in \cref{sec:training} is crucially important. In particular, we found that joint training diverged in almost all cases we attempted. We were only able to train jointly in two circumstances:
\begin{inlinelist}
\item Using batch normalization in ``train'' mode for both context \textit{and} target sets. We stress that this implies computing the batch statistics at test time, and using those to normalize the batches. This is in contrast to the methodology we propose in the main text: only using batch normalization in ``eval'' mode, which enforces that no information is transferred across tasks or datasets.
\item ``Warm-start" the training procedure with batch normalization in ``train'' mode, and after a number of epochs (we use 50 for the results shown below), switch to proper usage of batch normalization.
\end{inlinelist}
All other training procedures we attempted diverged.
\begin{table}[]
\caption{Few-shot classification results on \textsc{Meta-Dataset} \citep{triantafillou2019meta} comparing joint training for  and  (columns 2 and 3) to two-stage training (column 4). All figures are percentages and the  sign indicates the 95\% confidence interval. Bold text indicates the highest scores that overlap in their confidence intervals.}
\label{table:training_comparison}
\begin{tabular}{@{}lccc@{}}
\toprule
Dataset & \begin{tabular}[c]{@{}c@{}}Joint Training\\ (warmstart BN)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Joint Training\\ (BN train mode)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Two-Stage Training\\ (BN test mode)\end{tabular} \\ \midrule
ILSVRC \citep{russakovsky2015imagenet} & 17.30.7 & 41.61.0 & 49.51.0 \\
Omniglot \citep{lake2011one} & 74.91.0 & 80.80.9 & 89.70.5 \\
Aircraft \citep{maji2013fine} & 51.40.8 & 70.50.7 & 87.20.5 \\
Birds \citep{wah2011caltech} & 44.11.0 & 48.31.0 & 76.70.9 \\
Textures \citep{cimpoi2014describing} & 49.10.7 & 73.50.6 & 83.00.6 \\
Quick Draw \citep{ha2017neural} & 46.61.0 & 71.50.8 & 72.30.8 \\
Fungi \citep{Schroeder2018FGVCx} & 20.40.9 & 43.11.1 & 50.51.1 \\
VGG Flower \citep{nilsback2008automated} & 66.60.8 & 71.00.7 & 92.50.4 \\
\hdashline
Traffic Signs \citep{houben2013detection} & 21.20.8 & 40.41.1 & 48.41.1 \\
MSCOCO \citep{lin2014microsoft} & 18.80.7 & 37.11.0 & 39.70.9 \\ \bottomrule
\end{tabular}
\end{table}


\cref{table:training_comparison} details the results of our study on training procedures. The results demonstrate that the two-stage greatly improves performance of the model, even compared to using batch normalization in ``train mode'', which gives the model an unfair advantage over our standard model. 











\subsection{Comparison Between \cnaps{} and Parallel Residual Adapters \citep{rebuffi2018efficient}}
\label{app:parallel_residual_adapter_comparison}
 \cnaps{} adds FiLM layers \citep{perez2018film} in series with each convolutional layer to adapt the feature extractor to a particular task while parallel residual adapters from \citet{rebuffi2018efficient} adds  convolutions in parallel with each convolution layer to do the same. However, if the number of feature channels is , then the number of parameters required for each convolutional layer in the feature extractor is  for \cnaps{} and  for parallel residual adapters. Hence, parallel residual adapters have  times the capacity compared to FiLM layers. Despite this advantage, CNAPs achieves superior results as can be seen in \cref{table:parallel_residual_adapter_comparison}.

\begin{table}[t]
    \caption{Few-shot classification results on \textsc{Meta-Dataset} \citep{triantafillou2019meta} using models trained on all training datasets for Parallel Residual Adapters \citep{rebuffi2018efficient} and \cnaps{}. All figures are percentages and the  sign indicates the 95\% confidence interval over tasks. Bold text indicates the scores within the confidence interval of the highest score. Tasks from datasets below the dashed line were not used for training.}
    \label{table:parallel_residual_adapter_comparison}
	\centering
	\begin{tabular}{lcc}
	\toprule
	Dataset                                   & Parallel Residual Adapter & \cnaps{} \\
    \midrule
	ILSVRC \citep{russakovsky2015imagenet}    & \textbf{51.2  1.0}   & \textbf{52.3  1.0} \\
	Omniglot \citep{lake2011one}              & \textbf{87.3  0.7}   & \textbf{88.4  0.7} \\
    Aircraft \citep{maji2013fine}             & 78.3  0.7            & \textbf{80.5  0.6} \\
    Birds \citep{wah2011caltech}              & 67.8  0.9            & \textbf{72.2  0.9} \\
    Textures \citep{cimpoi2014describing}     & 55.5  0.7            & \textbf{58.3  0.7} \\
    Quick Draw \citep{ha2017neural}           & 70.9  0.7            & \textbf{72.5  0.8} \\
    Fungi \citep{Schroeder2018FGVCx}          & 44.6  1.1            & \textbf{47.4  1.0} \\
    VGG Flower \citep{nilsback2008automated}  & 81.7  0.7            & \textbf{86.0  0.5} \\
	\hdashline
    Traffic Signs \citep{houben2013detection} & 57.2  0.9            & \textbf{60.2  0.9} \\ 
    MSCOCO \citep{lin2014microsoft}           & \textbf{43.7  1.0}   & \textbf{42.6  1.1} \\
    MNIST \citep{lecun2010mnist}              & 91.1  0.4            & \textbf{92.7  0.4} \\
    CIFAR10 \citep{krizhevsky2009learning}    & \textbf{64.5  0.8}   &         61.5  0.7 \\ 
    CIFAR100 \citep{krizhevsky2009learning}   & \textbf{50.4  0.9}   & \textbf{50.1  1.0} \\
    \bottomrule
\end{tabular}
\end{table}




\section{Network Architecture Details}
\label{app:network_architecture_details}

\subsection{ResNet18 Architecture details}
\label{app:resnet_architecture}

Throughout our experiments in \cref{sec:experiments}, we use a ResNet18 \citep{he2016deep} as our feature extractor, the parameters of which we denote . \cref{table:basic_resnet_block} and \cref{table:basic_resnet_scaling_block} detail the architectures of the basic block (left) and basic scaling block (right) that are the fundamental components of the ResNet that we employ. \cref{table:pre_trained_resnet_architecture} details how these blocks are composed to generate the overall feature extractor network. We use the implementation that is provided by the PyTorch \citep{paszke2017automatic}\footnote{https://pytorch.org/docs/stable/torchvision/models.html}, though we adapt the code to enable the use of FiLM layers.

\begin{table}[!htb]
    \RawFloats
    \adjustbox{valign=t}{\begin{minipage}{.5\linewidth}
      \caption{ResNet-18 basic block .}
      \label{table:basic_resnet_block}
      \centering
        \begin{tabular}{l}
		\toprule
		\textbf{Layers} \\
        \midrule
        Input \\
        Conv2d (, stride 1, pad 1) \\ 
        BatchNorm \\
        FiLM () \\
        ReLU \\
        Conv2d (, stride 1, pad 1) \\ 
        BatchNorm \\
        FiLM () \\
        Sum with Input \\
        ReLU \\
        \bottomrule
	\end{tabular}
    \end{minipage}}\adjustbox{valign=t}{\begin{minipage}{.5\linewidth}
      \centering
        \caption{ResNet-18 basic scaling block .}
        \label{table:basic_resnet_scaling_block}
        \begin{tabular}{l}
		\toprule
		\textbf{Layers} \\
        \midrule
        Input \\
        Conv2d (, stride 2, pad 1) \\ 
        BatchNorm \\
        FiLM () \\
        ReLU \\
        Conv2d (, stride 1, pad 1) \\ 
        BatchNorm \\
        FiLM () \\
        Downsample Input by factor of 2 \\
        Sum with Downsampled Input \\
        ReLU \\
        \bottomrule
	\end{tabular}
    \end{minipage}}
\end{table}


\begin{table}[h]
    \caption{ResNet-18 feature extractor network.}
    \label{table:pre_trained_resnet_architecture}
	\centering
	\begin{tabular}{lcl}
		\multicolumn{3}{l}{\textbf{ResNet-18 Feature Extractor () with FiLM Layers:} ,  } \\
		\toprule
		\textbf{Stage}  & \textbf{Output size}      & \textbf{Layers} \\
        \midrule
        Input           &    & Input image \\
        Pre-processing  &   & Conv2d (, stride 2, pad 1, BatchNorm, ReLU) \\
        Layer          &   & Basic Block  2 \\
        Layer          &  & Basic Block, Basic Scaling Block \\
        Layer          &  & Basic Block, Basic Scaling Block \\
        Layer          &    & Basic Block, Basic Scaling Block \\
        Post-Processing & 512                       & AvgPool, Flatten \\
        \bottomrule
	\end{tabular}
    \vspace{2mm}
\end{table}

\subsection{Adaptation Network Architecture Details}
\label{app:adaptation_network_architectures}

In this section, we provide the details of the architectures used for our adaptation networks. \cref{table:global_set_encoder} details the architecture of the set encoder  that maps context sets to global representations. 
\begin{table}[h]
    \caption{Set encoder .}
    \label{table:global_set_encoder}
	\centering
	\begin{tabular}{cl}
		\multicolumn{2}{l}{\textbf{Set Encoder ():} } \\
		\toprule
		\textbf{Output size} & \textbf{Layers} \\
        \midrule
		 & Input image \\
		 & Conv2d (, stride 1, pad 1, ReLU), MaxPool (, stride 2) \\
		 & Conv2d (, stride 1, pad 1, ReLU), MaxPool (, stride 2) \\
		 & Conv2d (, stride 1, pad 1, ReLU), MaxPool (, stride 2) \\
		 & Conv2d (, stride 1, pad 1, ReLU), MaxPool (, stride 2) \\
		 & Conv2d (, stride 1, pad 1, ReLU),  MaxPool (, stride 2) \\
        64 & AdaptiveAvgPool2d \\
        \bottomrule
	\end{tabular}
\end{table}


\cref{table:ar_set_encoders} details the architecture used in the auto-regressive parameterization of . In our experiments, there is one such network for every block in the ResNet18 (detailed in \cref{table:pre_trained_resnet_architecture}). These networks accept as input the set of activations from the previous block, and map them (through the permutation invariant structure) to a vector representation of the output of the layer. The representation  is then generated by concatenating the global and auto-regressive representations, and fed into the adaptation network that provides the FiLM layer parameters for the next layer. This network is detailed in \cref{table:film_generator}, and illustrated in \cref{fig:film_generators}. Note that, as depicted in \cref{fig:film_generators}, each layer has four networks with architectures as detailed in \cref{table:film_generator}, one for each  and , for each convolutional layer in the block.

\begin{table}[h]
    \caption{Network of set encoder .}
    \label{table:ar_set_encoders}
    \centering
	\begin{tabular}{cl}
	  \multicolumn{2}{l}{\textbf{Set Encoder ():} 
	    } \\
      \toprule
      \textbf{Output size} & \textbf{Layers} \\
      \midrule
        channels   channel size & Input  \\
        channels   channel size & AvgPool, Flatten \\
        channels & fully connected, ReLU \\
        channels & 2  fully connected with residual skip connection, ReLU \\
        channels & fully connected with residual skip connection \\
        channels & mean pooling over instances \\
        channels & Input from mean pooling \\
        channels & fully connected, ReLU \\      \bottomrule
	\end{tabular}
\end{table}

\begin{table}[h]
    \caption{Network .}
    \label{table:film_generator}
	\centering
	\begin{tabular}{cl}
	  \multicolumn{2}{l}{\textbf{Network ():} }\\
 		\toprule
        \textbf{Output size} & \textbf{Layers} \\
        \midrule
         channels &  Input from Concatenate \\
         channels & fully connected, ReLU \\
         channels & 2  fully connected with residual skip connection, ReLU \\
         channels & fully connected with residual skip connection \\
		\bottomrule
	\end{tabular}
\end{table}

\subsection{Linear Classifier Adaptation Network}
\label{app:linear_classifier}

Finally, in this section we give the details for the linear classifer , and the adaptation network that provides these task-specific parameters . The adaptation network accepts a class-specific representation that is generated by applying a mean-pooling operation to the adapted feature activations of each instance associated with the class in the context set: , where  denotes the number of context instances associated with class  in task .  is comprised of two separate networks (one for the weights   and one for the biases ) detailed in \cref{table:psi_w_adaptation_networks} and \cref{table:psi_b_adaptation_networks}. The resulting weights and biases (for each class in task ) can then be used as a linear classification layer, as detailed in \cref{table:linear_classifier}.

\begin{table}[!htb]
    \RawFloats
    \adjustbox{valign=t}{\begin{minipage}{.5\linewidth}
      \caption{Network .}
      \label{table:psi_w_adaptation_networks}
      \centering
      \begin{tabular}{cl}
	    \multicolumn{2}{l}{\textbf{Network ():}} \\
	    \multicolumn{2}{l}{} \\
        \toprule
        \textbf{Output size} & \textbf{Layers} \\
        \midrule
         & Input from mean pooling \\
         & 2  fully connected, ELU \\
         & fully connected \\
         & Sum with Input \\
        \bottomrule
	  \end{tabular}
    \end{minipage}}\adjustbox{valign=t}{\begin{minipage}{.5\linewidth}
      \centering
      \caption{Network .}
      \label{table:psi_b_adaptation_networks}
      \begin{tabular}{cl}
	    \multicolumn{2}{l}{\textbf{Network ():}} \\
	    \multicolumn{2}{l}{} \\
        \toprule
        \textbf{Output size} & \textbf{Layers} \\
        \midrule
         & Input from mean pooling \\
         & 2  fully connected, ELU \\
         & fully connected \\
        \bottomrule
	  \end{tabular}
    \end{minipage}}
\end{table}

\begin{table}[h]
    \caption{Linear classifier network.}
	\centering
	\begin{tabular}{ll}
		\multicolumn{2}{l}{\textbf{Linear Classifier ():} } \\
 		\toprule
        \textbf{Output size} & \textbf{Layers} \\
        \midrule
		 & Input features  \\
		 & Input weights  \\
		 & Input biases  \\
		 & fully connected \\
		 & softmax \\ 
		\bottomrule
	\end{tabular}
	\label{table:linear_classifier}
\end{table}


\clearpage
\newpage

\section{Continual Learning Implementation Details}
\label{app:additional_continual_learning_details}

As noted in \cref{sec:model,sec:experiments}, our model can be applied to continual learning with one small modification: we store a compact representation of our training data that can be updated at each step of the continual learning procedure. Notice that \cref{fig:classifier} indicates that the functional representation of our linear classification layer  contains a mean pooling layer that combines the per-class output of our feature extractor . The result of this pooling, 

where , is supplied as input to the network . This network yields the class conditional parameters of the linear classifier , resulting in (along with the feature extractor parameters ) the full paramterization of . We store  as the training dataset representation for, class . 

If at any point in our continual learning procedure we observe new training data for class  we can update our representation for class  by computing 
 the pooled average resulting from  new training examples  for class . We then update  with the weighted average:
. At prediction time, we supply  to  to produce classification parameters for class . 

Similar to the input to , the input to  also contains a mean-pooled representation, this time of the entire training dataset . This representation is also stored and updated in the same way.

One issue with our procedure is that it is not completely invariant to the order in which we observe the sequence of training data during our continual learning procedure. The feature extractor adaptation parameters are only conditioned on the most recent training data, meaning that if data from class  is not present in the most recent training data,  was generated using "old" feature extractor adaptation parameters (from a previous time step). This creates a potential disconnect between the classification parameters from previous time steps and the feature extractor output. Fortunately, in our experiment we noticed little within dataset variance for the adaptation parameters. Since all of our experiments on continual learning were within a single dataset, this did not seem to be an issue as \cnaps{} were able to achieved good performance. However, for continual learning experiments that contain multiple datasets, we anticipate that this issue will need to be addressed.


\section{Additional Continual Learning Results}
\label{app:additional_continual_learning_results}

In \cref{sec:experiments} we provided results for continual learning experiments with Split MNIST \citep{zenke2017continual} and Split CIFAR100 \citep{chaudhry2018riemannian}. The results showed the average performance as more tasks were observed for the single and multi head settings. Here, we provide more complete results, detailing the performance through ``time" at the task level. 
\begin{figure}[htb]
     \includegraphics[width=\linewidth]{figures/continual_learning_plots/Composite_MNIST_multi.pdf} \\
    \includegraphics[width=\linewidth]{figures/continual_learning_plots/Composite_MNIST_single.pdf} \\
    \caption{Continual learning results on Split MNIST. Top row is multi-head, bottom row is single-head.}
    \label{fig:continual_learning_mnist}
\end{figure}
\cref{fig:continual_learning_mnist} details the performance of \cnaps{} (with varying number of observed examples) and Riemannian Walk (RWalk) \citep{chaudhry2018riemannian} on the five tasks of Split MNIST through time. Note that RWalk makes explicit use of training data from previous time steps when new data is observed, while \cnaps{} do not. 

\cref{fig:continual_learning_mnist} implies that \cnaps{} is competitive with RWalk in this scenario, despite seeing far less data per task, and not using old data to retrain the model at every time-step. Further, we see that \cnaps{} is naturally resistant to forgetting, as it uses internal task representations to maintain important information about tasks seen at previous time-steps.

\cref{fig:continual_learning_cifar100} demonstrates that \cnaps{} maintains similar results when scaling up to considerably more difficult datasets such as CIFAR100. Here too, \cnaps{} has not been trained on this dataset, yet demonstrates performance comparable to (and even better than) RWalk, a method explicitly trained for this task that makes use of samples from previous tasks at each time step. 

\begin{figure}[htb]
    \includegraphics[width=\linewidth]{figures/continual_learning_plots/Composite_CIFAR100_multi.pdf} \\
    \includegraphics[width=\linewidth]{figures/continual_learning_plots/Composite_CIFAR100_single.pdf} \\
    \caption{Continual learning results on Split CIFAR100. Top two rows are multi-head, bottom two rows are single-head.}
    \label{fig:continual_learning_cifar100}
\end{figure}


\clearpage
\newpage


\section{Additional Active Learning Results}
\label{app:additional_active_learning_results}

In \cref{sec:experiments} we provided active learning results for \cnaps{} and 
Prototypical Networks on the VGG Flowers dataset and three held out test languages from the Omniglot dataset. Here, we provide the results from all twenty held-out languages in Omniglot.

\begin{figure}[htb]
     \includegraphics[width=\linewidth]{figures/active_learning/active_learning_plot_Angelic_Atemayar_Qelisayer_Atlantean_Aurek-Besh.pdf} \\
     \includegraphics[width=\linewidth]{figures/active_learning/active_learning_plot_Avesta_Ge_ez_Glagolitic_Gurmukhi.pdf} \\
     \includegraphics[width=\linewidth]{figures/active_learning/active_learning_plot_Kannada_Keble_Malayalam_Manipuri.pdf} \\
     \includegraphics[width=\linewidth]{figures/active_learning/active_learning_plot_Mongolian_Old_Church_Slavonic_Cyrillic_Oriya_Sylheti.pdf} \\
     \includegraphics[width=\linewidth]{figures/active_learning/active_learning_plot_Syriac_Serto_Tibetan_Tengwar_ULOG.pdf} \\
     \caption{Active learning results on all twenty held-out \textsc{Omniglot} languages.}
     \label{fig:active_learning_complete}
\end{figure}

\cref{fig:active_learning_complete} demonstrates that in almost all held-out languages, using the predictive distribution of \cnaps{} not only improves overall performance, but also enables the model to make use of standard acquisition functions \citep{cohn1996active} to improve data efficiency over random acquisition. In contrast, we see that in most cases, random acquisition performs as well or better than acquisition functions that rely on the predictive distribution of Prototypical Networks. This provides empirical evidence that in addition to achieving overall better performance, the predictive distribution of \cnaps{} is more calibrated, and thus better suited to tasks such as active learning that require uncertainty in predictions.



\clearpage
\newpage



\end{document}