\documentclass[final]{cvpr}

\renewcommand{\rmdefault}{ptm}
\renewcommand{\sfdefault}{phv}

\usepackage{booktabs}
\usepackage{subcaption}

\usepackage{nicefrac}
\usepackage{eucal}


\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb,xspace}
\usepackage{mathtools}


\usepackage{cite}


\def\varnothing{\emptyset}

\newcommand{\code}[1]{{\ensuremath{\tt #1}}} 





\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}

\usepackage[margin=4pt,font=small,labelfont=bf,labelsep=endash,tableposition=top]{caption}

\def\Ours{{VisTR}\xspace}
\newcommand{\myparagraph}[1]{{\vspace{0.01cm} \noindent \bf #1}}






\begin{document}
\pagenumbering{gobble}
\title{End-to-End Video Instance Segmentation with Transformers}


\author{
Yuqing Wang, 
Zhaoliang Xu, 
Xinlong Wang,
Chunhua Shen, 
Baoshan Cheng,
Hao Shen\thanks{Corresponding author.}, 
Huaxia Xia
\
\PE(\pos,i)=
\begin{cases}
 \sin \bigl( \pos \cdot \omega_{k} \bigr),  ~~~~\text{for } i=2k, 
\\
\cos \bigl( \pos \cdot \omega_{k} \bigr),   ~~~~\text{for } i=2k+1;
\end{cases}
\label{eq:pe}

\hat{\sigma}=\underset{\sigma \in S_{n}}{\arg \min } \sum_{i}^{n} \mathcal{L}_{\text{match}}\left(y_{i}, \hat{y}_{\sigma(i)}\right)
\label{eq:1}

y_{i} = \{
      (c_{i},c_{i}  ...,c_{i}),  (b_{i,0},b_{i,1}...,b_{i,T}) 
         \}

{\hat p}_{(\sigma(i))}(c_{i})  =  \{{\hat p}_{(\sigma(i),0)} (c_{i})...,\hat p_{(\sigma(i),T)}(c_{i}) \}

\hat b_{\sigma(i)}= \left \{ \hat b_{(\sigma(i),0)},\hat b_{(\sigma(i),1)}..., \hat b_{(\sigma(i),T)} \right \}

\mathcal{L}_{\text{match}}\left(y_{i}, \hat{y}_{\sigma(i)}\right)=- \hat{p}_{\sigma(i)}\left(c_{i}\right)+ \mathcal{L}_{\mathrm{box}}
\bigl(b_{i}, \hat{b}_{\sigma(i)}
\bigr),
\label{eq:2}

\mathcal{L}_{\text{Hung}}(y, \hat{y})  
&= 
\sum_{i=1}^{N} \Bigl[ 
    ( -\log \hat{p}_{\hat{\sigma}(i)}(c_{i})) 
+ \mathcal{L}_{\text {box }}(b_{i}, \hat{b}_{\hat{\sigma}}(i)) 
\notag
\\ 
&+
\mathcal{L}_{\text {mask }}(m_{i}, \hat{m}_{\hat{\sigma}}(i)) 
    \Bigr].
\label{eq:3}

\mathcal{L}_{\mathrm{box}}\bigl(b_{i}, \hat{b}_{\sigma(i)} 
\bigr)
& =
\frac{1}{T} \sum_{t=1}^{T}   
\Bigl[ 
    \lambda_{\mathrm{iou}}  \cdot   \mathcal{L}_{\mathrm{iou}}\bigl( b_{i,t}, \hat{b}_{\sigma(i),t} \bigr) 
\notag 
\\ 
& +
\lambda_{\mathrm{L} 1}\left\|b_{i,t}-\hat{b}_{\sigma(i), t}\right\|_{1} 
\Bigr]. 
\label{eq:4}

\mathcal{L}_{\text{mask}}\left(m_{i}, \hat{m}_{\sigma(i)}\right)
& =
\lambda_{\text{mask}}\frac{1}{T}\sum_{t=1}^{T} 
\Bigl[ 
 \Dice(m_{i,t}, \hat{m}_{\sigma(i),t}
) 
\notag
\\
&+
\Focal(m_{i,t}, \hat{m}_{\sigma(i),t}) 
\Bigr].  
\label{eq:5}





\section{Experiments}

In this section, we conduct experiments on the YouTube-VIS \cite{vis2019} dataset, which contains 2238 training, 302 validation and 343 test video clips. Each video of the dataset is annotated with per pixel segmentation mask, category and instance labels. The object category number is 40. As the test set evaluation is closed, we evaluate our method in the validation set. The evaluation metrics are average precision (AP) and average recall (AR), with the video Intersection over Union (IoU) of the mask sequences as the threshold.




\subsection{Implementation Details}
\myparagraph{Model settings.} 
As the largest number of the annotated video length for YouTube-VIS \cite{vis2019} is 36, we take this value as the default input video clip length .
Thus, no post-processing is needed to associate different clips from one video, which makes our model totally end-to-end trainable. The model predicts 10 objects for each frame, thus the total object query number is 360. For the Transformer we use 6 encoder, 6 decoder layers of width 384 with 8 attention heads. Unless otherwise specified, ResNet-50 \cite{he2016deep} is used as our backbone networks and the same hyper-parameters of 
DETR \cite{Detr} are used.


\myparagraph{Training.} The model is implemented with PyTorch-1.6 \cite{paszke2019pytorch}, trained with AdamW \cite{loshchilov2018decoupled} of initial Transformer’s learning rate being  , the backbone’s learning rate being . The models are trained for 18 epochs, with the learning rate decays by 10x at 12 epochs. We initialize our backbone networks with the weights of DETR pretrained on COCO \cite{lin2014microsoft}. The models are trained on 8  V100 GPUs of 32G RAM, with 1 video clip per GPU. The frame sizes are downsampled to 300540 to fit the GPU memory.  


\myparagraph{Inference.} During inference, we follow the same scale setting as training. No post-processing is needed for associating instances. Instances with scores larger than 0.001 are kept. The mean score for all the frames is used as the instance score. For instances that have been classified to different categories in different frames, we use the most frequently predicted category as the final instance category. 



\subsection{Ablation Study}

In this section we conduct extensive ablation experiments to study the core factors of \Ours.
Comparison results are 
reported
in Table \ref{tab:1}.

\begin{table*}[!t]
\small
\begin{subtable}{0.5\linewidth}
\centering
\captionsetup{width=0.9\linewidth}
\begin{tabular}{c|c|cccc}
Length & AP &  &  &  &  \\
\hline
18&29.7&50.4&31.1&29.5&34.4\\
24&30.5&47.8&33.0&29.5&34.4\\
30&31.7&53.2&32.8&31.3&36.0\\
36&33.3&53.4&35.1&33.1&38.5
\end{tabular}
\caption{\textbf{Video sequence length.} 
The performance improves as the sequence length increases.
}
\label{tab:length}
\end{subtable}\begin{subtable}{0.5\linewidth}
\centering
\captionsetup{width=0.9\linewidth}
\setlength{\tabcolsep}{1mm}{
\begin{tabular}{ r |c|c|cccc}
 & \# 
 &AP &  &  &  &  \\
\hline
video level& 1&8.4&13.2&9.5&20.0&20.8\\
frame level& 36&13.7&23.3&14.5&30.4&35.1\\
ins. level& 10&32.0&52.8&34.0&31.6&37.2\\
pred. level& 360&33.3&53.4&35.1&33.1&38.5
\end{tabular}
}
\caption{\textbf{Instance query embedding}. 
Instance-level query is only 1.3\%  lower in AP than the prediction-level query with 36 fewer embeddings. }
\label{tab:query}
\end{subtable}

\begin{subtable}{0.5\linewidth}
\centering
\captionsetup{width=0.9\linewidth}
\begin{tabular}{c|c|cccc}
time order & AP &  &  &  &  \\
\hline
random&32.3&52.1&34.3&33.8&37.3\\
in order&33.3&53.4&35.1&33.1&38.5
\end{tabular}
\caption{\textbf{Video sequence order.} Sequence in time order is 1.0\%  better  in AP than sequence in random order.}
\label{tab:order}
\end{subtable}\vspace{-0.3cm}
\begin{subtable}{0.5\linewidth}
\centering
\captionsetup{width=0.9\linewidth}
\begin{tabular}{c|c|cccc}
 & AP &  &  &  &  \\
\hline
w/o&28.4&50.1&29.5&29.6&33.3\\
w &33.3&53.4&35.1&33.1&38.5
\end{tabular}
\caption{\textbf{Position encoding}. Position encoding brings about 5\% AP gains to \Ours.}
\label{tab:pos}
\end{subtable}

\begin{subtable}{0.5\linewidth}
\centering
\captionsetup{width=0.9\linewidth}
\begin{tabular}{c|c|cccc}
 & AP &  &  &  &  \\
\hline
CNN &32.0&54.5&31.5&31.6&37.7\\
Transformer&33.3&53.4&35.1&33.1&38.5
\end{tabular}
\caption{\textbf{CNN-encoded feature vs.\  Transformer-encoded feature} for mask prediction. The transformer improves the feature quality.}
\label{tab:encoded}
\end{subtable}\begin{subtable}{0.5\linewidth}
\centering
\captionsetup{width=0.9\linewidth}
\begin{tabular}{c|c|cccc}
& AP &  &  &  &  \\
\hline
w/o &33.3&53.4&35.1&33.1&38.5\\
w&34.4&55.7&36.5&33.5&38.9
\end{tabular}
\caption{\textbf{Instance sequence segmentation module.} The module with 3D convolutions brings 1.1\% AP gains.}
\label{tab:refine}
\end{subtable}
\caption{Ablation experiments for \Ours. All models are trained on YouTubeVIS \texttt{train} for 10 epochs and tested on YouTubeVIS \texttt{val}, using the ResNet-50 backbone.}\label{tab:1}
\vspace{-0.7cm}
\end{table*}




The main difference between video and image is that video contains  temporal information.
How to effectively learn and exploit temporal information is the key to video understanding.
Firstly, we study the importance of temporal information to \Ours in two dimensions: the amount and the order. 

\myparagraph{Video sequence length.}
To evaluate the importance of the amount of temporal information to \Ours, we experiment with models trained with different input video sequence lengths. As reported in Table~\ref{tab:length}, with the length varying 
from 18 to 36, the AP increases monotonically from 29.7\%  to 33.3\%. This result shows that more temporal information indeed helps the model learn better. As the largest video length of the dataset is 36, we argue that, if with a larger dataset, \Ours can achieve even better results.
Note that for this experiment, if the clip length is less than the video length, instance matching in overlapping frames is used for associating them from different clips. 


\myparagraph{Video sequence order.}
As the movement of objects in real scenes 
are continuous, we believe 
that the order of temporal information is also important. To evaluate, we perform a comparison of the model trained with input video sequence in random order vs.\  time order. Results in Table~\ref{tab:order} show that the model learned according to the
time order information achieves 1 point higher, which verifies the importance of the temporal order.



\myparagraph{Positional encoding.}
Position information is important for the dense prediction problem of VIS. As the original feature sequence contains no positional information, we supplement with the spatial and temporal positional encodings,
which indicate the relative positions in the video sequence. Experiments of models with and without positional encoding are presented 
in Table~\ref{tab:pos}. The model without positional encoding manages to 
achieve 28.4\% AP.
Our explanation is that the ordered format of the sequence supervision and the correspondence between the input and output order of the Transformer provide some relative positional information \textit{implicitly}. In the second experiment, the performance improves by about 5 points, which verifies the necessity of explicit positional encoding.

\myparagraph{Instance queries.}
The instance queries are learned embeddings for decoding the representative instance predictions.
In this experiment, we study the effect of instance queries and attempt to exploit the inner connections among them by varying the embedding number. Suppose the model decode  instances each frame, and the frame number is . The input instance query number should be  to decode the same number for predictions.
In the default setting, one embedding is responsible for one prediction, the model directly learns  unique embeddings, termed as `prediction level' in Table~\ref{tab:query}. 
In the `video level setting', one embedding is learned for all the instance predictions, \textit{i.e.}, the same embedding is repeated  times as the input of decoder. In the `frame-level' setting, the model only learns  unique embeddings and repeats them by  times.
In the `instance level' setting, the model only learns  unique embeddings and repeats them by  times. 
The n and T corresponds to the value of 10 and 36 in the table respectively. The result is 8.4\% AP and 13.7\% AP for `video level' and `frame level' settings respectively.
Surprisingly, the `instance level' queries can achieve 32.0\% AP, which is only 1.3 points lower than the default setting. The result shows that the queries for one instance can be shared for the \Ours model, which makes the tracking natural. But the queries for one frame can not be shared. 


\myparagraph{Transformers for feature encoding.}
As illustrated in the 
`instance sequence segmentation' module of \figref{fig:2}.
The module takes three types of features as input: 
the features `B' from the backbone, the feature `E' from the encoder and the attention map computed by the feature `E' and `O'. 
To show the superiority of Transformers in feature encoding, we compare the results of using the original input `O' vs.\ output `E' of the encoder for the second feature, \textit{a.k.a.}, CNN-encoded features vs.\  Transformer-encoded features. As reported in Table~\ref{tab:encoded}, the CNN-encoded features achieves 32.0\% AP, and the Transformer-encoded features achieve 1.3 points higher. This demonstrates that features are learned better after the Transformer updates them based on all pairwise similarities between them through self-attention. The result also shows the superiority of modeling the spatial and temporal features as a whole.


\myparagraph{Instance sequence segmentation.}
The segmentation process contains both the instance mask feature accumulation and instance sequence segmentation modules. The instance sequence segmentation module takes the instance sequence as a whole. We expect that it can strengthen the mask prediction by learning the temporal information through 3D convolutions. Thus, when objects are in challenging situations such as occlusions or motion blurs, the module can learn to propagate information from other frames to help the segmentation. Besides, the features of the same instance from multiple frames could help the network recognize the instance better. In this experiment, we perform a study of models with or without the 3D instance sequence segmentation module. For the former case, we apply a 2D convolutional layer with the output channel being 1 to the mask features for each instance of each frame to obtain the masks. The comparison is shown in Table~\ref{tab:refine}.
The instance sequence segmentation module improves the result by 1.1 points, which verifies the effectiveness of the proposed module. 


With
these ablation studies, we conclude that in 
\Ours design: the temporal information, positional encodings, instance queries, global self-attention in the encoder and the instance sequence segmentation module, 
all play important roles \textit{w.r.t.}\ the final performance. 



\subsection{Main Results}

\begin{table*}
\small 
\centering 
\begin{tabular}{ r|l|c|c|cccc}
 Method&backbone& FPS& AP &  &  &  &  \\
\hline
DeepSORT\cite{wojke2017simple}&ResNet-50&-&26.1&42.9&26.1&27.8&31.3\\
FEELVOS\cite{voigtlaender2019feelvos}&ResNet-50&-&26.9&42.0&29.7&29.9&33.4\\
OSMN\cite{yang2018efficient}&ResNet-50&-&27.5&45.1&29.1&28.6&33.1\\
MaskTrack R-CNN\cite{vis2019}&ResNet-50&20.0&30.3&51.1&32.6&31.0&35.5\\
STEm-Seg\cite{Athar_Mahadevan20ECCV}&ResNet-50&-&30.6&50.7&33.5&31.6&37.1\\
STEm-Seg\cite{Athar_Mahadevan20ECCV}&ResNet-101&2.1&34.6&55.8&37.9&34.4&41.6\\
MaskProp\cite{bertasius2020classifying}&ResNet-50&-&40.0&-&42.9&-&-\\
MaskProp\cite{bertasius2020classifying}&ResNet-101&-&42.5&-&45.6&-&-\\
\hline
\textbf{\Ours}&ResNet-50&30.0/69.9&36.2&59.8&36.9&37.2&42.4\\
\textbf{\Ours}&ResNet-101&27.7/57.7&40.1&64.0&45.0&38.3&44.9\\
\end{tabular}
\caption{\textbf{Video instance segmentation} AP (\%) on the YouTube-VIS \cite{vis2019} validation dataset. Note that,
for the first three methods, we have cited the results reported by the re-implementations in \cite{vis2019} for VIS. Other results are adopted from their original paper.
For the speed of \Ours we report the FPS results with and without the data loading process. 
Here we naively load the images serially, taking unnecessarily long time.
The data loading process can be much faster by parallelizing.
}
\label{tab:sota}
\vspace{-0.4cm}
\end{table*}

\begin{figure*}[h]
\centering
\includegraphics[width=\linewidth]{figs/3.pdf}
\caption{\textbf{Visualization of \Ours} on the YouTube-VIS \cite{vis2019} validation dataset. Each row contains images from the same video. For each video, here the same colors 
depict the mask sequences of the same instances (Best viewed on screen).
}
\label{fig:3}
\vspace{-0.5cm}
\end{figure*}


We compare \Ours against some state-of-the-art methods in video instance segmentation in Table~\ref{tab:sota}. The comparison is performed in terms of both accuracy and speed. 
The methods in the first three rows are originally proposed for tracking or VOS.
We have cited the results reported by the re-implementations in 
\cite{vis2019} for VIS. Other methods including the MaskTrack RCNN, MaskProp \cite{bertasius2020classifying} and STEm-Seg \cite{Athar_Mahadevan20ECCV} are originally proposed for the VIS task in the temporal order. 

For the accuracy measured by AP, \textit{\Ours achieves the best result among methods using a single model without any bells and whistles}. Using the same backbone of ResNet-50 \cite{he2016deep}, \Ours achieves about 6 points higher in AP
than the MaskTrack R-CNN and the recently proposed STEm-Seg method. Besides, we argue the AP gap between \Ours and MaskProp mainly comes from its combination of multiple networks, \textit{i.e.}, Spatiotemporal Sampling Network \cite{bertasius2018object}, Feature Pyramid Network \cite{lin2017feature}, Hybrid Task Cascade Network \cite{chen2019hybrid} and the High-Resolution Mask Refinement post-processing. 
Since our aim is to design a conceptually simple and end-to-end framework, many improvements methods, such as complex video data augmentation and multi-stage mask refinement are beyond the scope of this work. For the speed measured by FPS (frames per second), \Ours shows a
significant 
advantage among all the reported results, achieving 27.7 FPS with the ResNet-101 backbone. If excluding the data loading process of multiple images, the speed 
can
achieve 57.7 FPS. Note that,
as we load the images in serial, the data loading process can be easily parallelized.
The fast speed of \Ours owes to its design of parallel decoding and no post-processing. 

The visualization of \Ours on the YouTube-VIS\cite{vis2019} validation dataset is shown in \figref{fig:3}, with each row containing images sampled from the same video. 
\Ours can track and segment instances well in challenging situations
such as: (a) instances overlapping, (b) changes of relative positions between instance, (c) confusion by the same category instances 
that are close together and (d) instances in various 
poses. 



\section{Conclusion}
In this paper, we have proposed a new video instance segmentation framework built upon Transformers, which views the VIS task as a direct end-to-end parallel sequence decoding/prediction problem. In this way, instance tracking is achieved \textit{seamlessly and naturally} in the same framework of instance segmentation, which is significantly different
from and simpler than existing approaches, considerably simplifying the overall pipeline. 
Without bells and whistles, \Ours achieves 
the best result and the highest speed among methods using a single model on the YouTube-VIS dataset. 
To our knowledge, our work is the first one that applies the Transformer 
to video instance segmentation. We hope that similar approaches can 
be applied to many more video understanding tasks in the future. 




\textbf{Acknowledgements}
This work was in part supported by Beijing Science and Technology Project (No.\ Z1\-8\-1\-1\-0\-0\-0\-0\-8\-9\-1\-8\-0\-1\-8).
CS and his employer received no financial
support for the research, authorship, and/or publication of this article.



{\small
\bibliographystyle{ieee_fullname}
\bibliography{draft}
}




\end{document}
