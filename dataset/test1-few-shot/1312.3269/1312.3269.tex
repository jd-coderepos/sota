
\documentclass[journal]{IEEEtran}

\usepackage{moreverb}

\usepackage[colorlinks,citecolor=red,urlcolor=red]{hyperref}

\usepackage{algorithm, algorithmic}
\usepackage{bm}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amstext,amsxtra,amssymb,underscore}
\usepackage{accents}
\usepackage{subfigure}
\usepackage{caption2}

\usepackage{color}

\makeatletter
\newcommand{\rmnum}[1]{\romannumeral #1}
\newcommand{\Rmnum}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother


\makeatletter
\def\wideubar{\underaccent{{\cc@style\underline{\mskip8mu}}}}
\def\Wideubar{\underaccent{{\cc@style\underline{\mskip6mu}}}}
\makeatother

\makeatletter
\def\widebar{\accentset{{\cc@style\underline{\mskip8mu}}}}
\def\Widebar{\accentset{{\cc@style\underline{\mskip6mu}}}}
\makeatother

\newtheorem{theorem}{Theorem}
\newtheorem{assumption}{Assumption}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}

\newcounter{MYtempeqncnt}


\hyphenation{op-tical net-works semi-conduc-tor}
\usepackage{amstext,amsxtra,amssymb,underscore}
\usepackage{amssymb,bm}







\def\proof      {\noindent\hspace{0pt}{\bf{Proof: }}}
\def\myQED      {\hfill\vspace{0.3cm}}

\def\bma{{\ensuremath{\bm{a}}}}
\def\bmb{{\ensuremath{\bm{b}}}}
\def\bmc{{\ensuremath{\bm{c}}}}
\def\bmd{{\ensuremath{\bm{d}}}}
\def\bme{{\ensuremath{\bm{e}}}}
\def\bmf{{\ensuremath{\bm{f}}}}
\def\bmg{{\ensuremath{\bm{g}}}}
\def\bmh{{\ensuremath{\bm{h}}}}
\def\bmi{{\ensuremath{\bm{i}}}}
\def\bmj{{\ensuremath{\bm{j}}}}
\def\bmk{{\ensuremath{\bm{k}}}}
\def\bml{{\ensuremath{\bm{l}}}}
\def\bmm{{\ensuremath{\bm{m}}}}
\def\bmn{{\ensuremath{\bm{n}}}}
\def\bmo{{\ensuremath{\bm{o}}}}
\def\bmp{{\ensuremath{\bm{p}}}}
\def\bmq{{\ensuremath{\bm{q}}}}
\def\bmr{{\ensuremath{\bm{r}}}}
\def\bms{{\ensuremath{\bm{s}}}}
\def\bmt{{\ensuremath{\bm{t}}}}
\def\bmu{{\ensuremath{\bm{u}}}}
\def\bmv{{\ensuremath{\bm{v}}}}
\def\bmw{{\ensuremath{\bm{w}}}}
\def\bmx{{\ensuremath{\bm{x}}}}
\def\bmy{{\ensuremath{\bm{y}}}}
\def\bmz{{\ensuremath{\bm{z}}}}
\def\bmA{{\ensuremath{\bm{A}}}}
\def\bmB{{\ensuremath{\bm{B}}}}
\def\bmC{{\ensuremath{\bm{C}}}}
\def\bmD{{\ensuremath{\bm{D}}}}
\def\bmE{{\ensuremath{\bm{E}}}}
\def\bmF{{\ensuremath{\bm{F}}}}
\def\bmG{{\ensuremath{\bm{G}}}}
\def\bmH{{\ensuremath{\bm{H}}}}
\def\bmI{{\ensuremath{\bm{I}}}}
\def\bmJ{{\ensuremath{\bm{J}}}}
\def\bmK{{\ensuremath{\bm{K}}}}
\def\bmL{{\ensuremath{\bm{L}}}}
\def\bmM{{\ensuremath{\bm{M}}}}
\def\bmN{{\ensuremath{\bm{N}}}}
\def\bmO{{\ensuremath{\bm{O}}}}
\def\bmP{{\ensuremath{\bm{P}}}}
\def\bmQ{{\ensuremath{\bm{Q}}}}
\def\bmR{{\ensuremath{\bm{R}}}}
\def\bmS{{\ensuremath{\bm{S}}}}
\def\bmT{{\ensuremath{\bm{T}}}}
\def\bmU{{\ensuremath{\bm{U}}}}
\def\bmV{{\ensuremath{\bm{V}}}}
\def\bmW{{\ensuremath{\bm{W}}}}
\def\bmX{{\ensuremath{\bm{X}}}}
\def\bmY{{\ensuremath{\bm{Y}}}}
\def\bmZ{{\ensuremath{\bm{Z}}}}
\def\bmomega{{\ensuremath{\bm{\omega}}}}
\def\bmupsilon{{\ensuremath{\bm{\upsilon}}}}
\def\bmxi{{\ensuremath{\bm{\xi}}}}
\def\bmeta{{\ensuremath{\bm{\eta}}}}
\def\bmphi{{\ensuremath{\bm{\phi}}}}
\def\bmvarphi{{\ensuremath{\bm{\varphi}}}}
\def\bmdelta{{\ensuremath{\bm{\delta}}}}
\def\bmalpha{{\ensuremath{\bm{\alpha}}}}
\def\bmbeta{{\ensuremath{\bm{\beta}}}}
\def\bmgamma{{\ensuremath{\bm{\gamma}}}}
\def\bmtheta{{\ensuremath{\bm{\theta}}}}
\def\bmsigma{{\ensuremath{\bm{\sigma}}}}
\def\bmdelta{{\ensuremath{\bm{\delta}}}}
\def\bmchi{{\ensuremath{\bm{\chi}}}}
\def\bmLambda{{\ensuremath{\bm{\Lambda}}}}
\def\bmepsilon{{\ensuremath{\bm{\epsilon}}}}

\def\bss{{\ensuremath{\boldsymbol{s}}}}
\def\bsz{{\ensuremath{\boldsymbol{z}}}}
\def\bsv{{\ensuremath{\boldsymbol{v}}}}

\def\bs{{\ensuremath{\boldsymbol{}}}}

\def\bbarA{{\ensuremath{\bar A}}}
\def\bbarB{{\ensuremath{\bar B}}}
\def\bbarC{{\ensuremath{\bar C}}}
\def\bbarD{{\ensuremath{\bar D}}}
\def\bbarE{{\ensuremath{\bar E}}}
\def\bbarF{{\ensuremath{\bar F}}}
\def\bbarG{{\ensuremath{\bar G}}}
\def\bbarH{{\ensuremath{\bar H}}}
\def\bbarI{{\ensuremath{\bar I}}}
\def\bbarJ{{\ensuremath{\bar J}}}
\def\bbarK{{\ensuremath{\bar K}}}
\def\bbarL{{\ensuremath{\bar L}}}
\def\bbarM{{\ensuremath{\bar M}}}
\def\bbarN{{\ensuremath{\bar N}}}
\def\bbarO{{\ensuremath{\bar O}}}
\def\bbarP{{\ensuremath{\bar P}}}
\def\bbarQ{{\ensuremath{\bar Q}}}
\def\bbarR{{\ensuremath{\bar R}}}
\def\bbarW{{\ensuremath{\bar W}}}
\def\bbarU{{\ensuremath{\bar U}}}
\def\bbarV{{\ensuremath{\bar V}}}
\def\bbarS{{\ensuremath{\bar S}}}
\def\bbarT{{\ensuremath{\bar T}}}
\def\bbarX{{\ensuremath{\bar X}}}
\def\bbarY{{\ensuremath{\bar Y}}}
\def\bbarZ{{\ensuremath{\bar Z}}}
\def\bbara{{\ensuremath{\bar a}}}
\def\bbarb{{\ensuremath{\bar b}}}
\def\bbarc{{\ensuremath{\bar c}}}
\def\bbard{{\ensuremath{\bar d}}}
\def\bbare{{\ensuremath{\bar e}}}
\def\bbarf{{\ensuremath{\bar f}}}
\def\bbarg{{\ensuremath{\bar g}}}
\def\bbarh{{\ensuremath{\bar h}}}
\def\bbari{{\ensuremath{\bar i}}}
\def\bbarj{{\ensuremath{\bar j}}}
\def\bbark{{\ensuremath{\bar k}}}
\def\bbarl{{\ensuremath{\bar l}}}
\def\bbarm{{\ensuremath{\bar m}}}
\def\bbarn{{\ensuremath{\bar n}}}
\def\bbaro{{\ensuremath{\bar o}}}
\def\bbarp{{\ensuremath{\bar p}}}
\def\bbarq{{\ensuremath{\bar q}}}
\def\bbarr{{\ensuremath{\bar r}}}
\def\bbarw{{\ensuremath{\bar w}}}
\def\bbaru{{\ensuremath{\bar u}}}
\def\bbarv{{\ensuremath{\bar v}}}
\def\bbars{{\ensuremath{\bar s}}}
\def\bbart{{\ensuremath{\bar t}}}
\def\bbarx{{\ensuremath{\bar x}}}
\def\bbary{{\ensuremath{\bar y}}}
\def\bbarz{{\ensuremath{\bar z}}}
\def\ccalA{{\ensuremath{\mathcal A}}}
\def\ccalB{{\ensuremath{\mathcal B}}}
\def\ccalC{{\ensuremath{\mathcal C}}}
\def\ccalD{{\ensuremath{\mathcal D}}}
\def\ccalE{{\ensuremath{\mathcal E}}}
\def\ccalF{{\ensuremath{\mathcal F}}}
\def\ccalG{{\ensuremath{\mathcal G}}}
\def\ccalH{{\ensuremath{\mathcal H}}}
\def\ccalI{{\ensuremath{\mathcal I}}}
\def\ccalJ{{\ensuremath{\mathcal J}}}
\def\ccalK{{\ensuremath{\mathcal K}}}
\def\ccalL{{\ensuremath{\mathcal L}}}
\def\ccalM{{\ensuremath{\mathcal M}}}
\def\ccalN{{\ensuremath{\mathcal N}}}
\def\ccalO{{\ensuremath{\mathcal O}}}
\def\ccalP{{\ensuremath{\mathcal P}}}
\def\ccalQ{{\ensuremath{\mathcal Q}}}
\def\ccalR{{\ensuremath{\mathcal R}}}
\def\ccalW{{\ensuremath{\mathcal W}}}
\def\ccalU{{\ensuremath{\mathcal U}}}
\def\ccalV{{\ensuremath{\mathcal V}}}
\def\ccalS{{\ensuremath{\mathcal S}}}
\def\ccalT{{\ensuremath{\mathcal T}}}
\def\ccalX{{\ensuremath{\mathcal X}}}
\def\ccalY{{\ensuremath{\mathcal Y}}}
\def\ccalZ{{\ensuremath{\mathcal Z}}}
\def\ccala{{\ensuremath{\mathcal a}}}
\def\ccalb{{\ensuremath{\mathcal b}}}
\def\ccalc{{\ensuremath{\mathcal c}}}
\def\ccald{{\ensuremath{\mathcal d}}}
\def\ccale{{\ensuremath{\mathcal e}}}
\def\ccalf{{\ensuremath{\mathcal f}}}
\def\ccalg{{\ensuremath{\mathcal g}}}
\def\ccalh{{\ensuremath{\mathcal h}}}
\def\ccali{{\ensuremath{\mathcal i}}}
\def\ccalj{{\ensuremath{\mathcal j}}}
\def\ccalk{{\ensuremath{\mathcal k}}}
\def\ccall{{\ensuremath{\mathcal l}}}
\def\ccalm{{\ensuremath{\mathcal m}}}
\def\ccaln{{\ensuremath{\mathcal n}}}
\def\ccalo{{\ensuremath{\mathcal o}}}
\def\ccalp{{\ensuremath{\mathcal p}}}
\def\ccalq{{\ensuremath{\mathcal q}}}
\def\ccalr{{\ensuremath{\mathcal r}}}
\def\ccalw{{\ensuremath{\mathcal w}}}
\def\ccalu{{\ensuremath{\mathcal u}}}
\def\ccalv{{\ensuremath{\mathcal v}}}
\def\ccals{{\ensuremath{\mathcal s}}}
\def\ccalt{{\ensuremath{\mathcal t}}}
\def\ccalx{{\ensuremath{\mathcal x}}}
\def\ccaly{{\ensuremath{\mathcal y}}}
\def\ccalz{{\ensuremath{\mathcal z}}}
\def\ccal0{{\ensuremath{\mathcal 0}}}
\def\hhatA{{\ensuremath{\hat A}}}
\def\hhatB{{\ensuremath{\hat B}}}
\def\hhatC{{\ensuremath{\hat C}}}
\def\hhatD{{\ensuremath{\hat D}}}
\def\hhatE{{\ensuremath{\hat E}}}
\def\hhatF{{\ensuremath{\hat F}}}
\def\hhatG{{\ensuremath{\hat G}}}
\def\hhatH{{\ensuremath{\hat H}}}
\def\hhatI{{\ensuremath{\hat I}}}
\def\hhatJ{{\ensuremath{\hat J}}}
\def\hhatK{{\ensuremath{\hat K}}}
\def\hhatL{{\ensuremath{\hat L}}}
\def\hhatM{{\ensuremath{\hat M}}}
\def\hhatN{{\ensuremath{\hat N}}}
\def\hhatO{{\ensuremath{\hat O}}}
\def\hhatP{{\ensuremath{\hat P}}}
\def\hhatQ{{\ensuremath{\hat Q}}}
\def\hhatR{{\ensuremath{\hat R}}}
\def\hhatW{{\ensuremath{\hat W}}}
\def\hhatU{{\ensuremath{\hat U}}}
\def\hhatV{{\ensuremath{\hat V}}}
\def\hhatS{{\ensuremath{\hat S}}}
\def\hhatT{{\ensuremath{\hat T}}}
\def\hhatX{{\ensuremath{\hat X}}}
\def\hhatY{{\ensuremath{\hat Y}}}
\def\hhatZ{{\ensuremath{\hat Z}}}
\def\hhata{{\ensuremath{\hat a}}}
\def\hhatb{{\ensuremath{\hat b}}}
\def\hhatc{{\ensuremath{\hat c}}}
\def\hhatd{{\ensuremath{\hat d}}}
\def\hhate{{\ensuremath{\hat e}}}
\def\hhatf{{\ensuremath{\hat f}}}
\def\hhatg{{\ensuremath{\hat g}}}
\def\hhath{{\ensuremath{\hat h}}}
\def\hhati{{\ensuremath{\hat i}}}
\def\hhatj{{\ensuremath{\hat j}}}
\def\hhatk{{\ensuremath{\hat k}}}
\def\hhatl{{\ensuremath{\hat l}}}
\def\hhatm{{\ensuremath{\hat m}}}
\def\hhatn{{\ensuremath{\hat n}}}
\def\hhato{{\ensuremath{\hat o}}}
\def\hhatp{{\ensuremath{\hat p}}}
\def\hhatq{{\ensuremath{\hat q}}}
\def\hhatr{{\ensuremath{\hat r}}}
\def\hhatw{{\ensuremath{\hat w}}}
\def\hhatu{{\ensuremath{\hat u}}}
\def\hhatv{{\ensuremath{\hat v}}}
\def\hhats{{\ensuremath{\hat s}}}
\def\hhatt{{\ensuremath{\hat t}}}
\def\hhatx{{\ensuremath{\hat x}}}
\def\hhaty{{\ensuremath{\hat y}}}
\def\hhatz{{\ensuremath{\hat z}}}
\def\tdA{{\ensuremath{\tilde A}}}
\def\tdB{{\ensuremath{\tilde B}}}
\def\tdC{{\ensuremath{\tilde C}}}
\def\tdD{{\ensuremath{\tilde D}}}
\def\tdE{{\ensuremath{\tilde E}}}
\def\tdF{{\ensuremath{\tilde F}}}
\def\tdG{{\ensuremath{\tilde G}}}
\def\tdH{{\ensuremath{\tilde H}}}
\def\tdI{{\ensuremath{\tilde I}}}
\def\tdJ{{\ensuremath{\tilde J}}}
\def\tdK{{\ensuremath{\tilde K}}}
\def\tdL{{\ensuremath{\tilde L}}}
\def\tdM{{\ensuremath{\tilde M}}}
\def\tdN{{\ensuremath{\tilde N}}}
\def\tdO{{\ensuremath{\tilde O}}}
\def\tdP{{\ensuremath{\tilde P}}}
\def\tdQ{{\ensuremath{\tilde Q}}}
\def\tdR{{\ensuremath{\tilde R}}}
\def\tdW{{\ensuremath{\tilde W}}}
\def\tdU{{\ensuremath{\tilde U}}}
\def\tdV{{\ensuremath{\tilde V}}}
\def\tdS{{\ensuremath{\tilde S}}}
\def\tdT{{\ensuremath{\tilde T}}}
\def\tdX{{\ensuremath{\tilde X}}}
\def\tdY{{\ensuremath{\tilde Y}}}
\def\tdZ{{\ensuremath{\tilde Z}}}
\def\tda{{\ensuremath{\tilde a}}}
\def\tdb{{\ensuremath{\tilde b}}}
\def\tdc{{\ensuremath{\tilde c}}}
\def\tdd{{\ensuremath{\tilde d}}}
\def\tde{{\ensuremath{\tilde e}}}
\def\tdf{{\ensuremath{\tilde f}}}
\def\tdg{{\ensuremath{\tilde g}}}
\def\tdh{{\ensuremath{\tilde h}}}
\def\tdi{{\ensuremath{\tilde i}}}
\def\tdj{{\ensuremath{\tilde j}}}
\def\tdk{{\ensuremath{\tilde k}}}
\def\tdl{{\ensuremath{\tilde l}}}
\def\tdm{{\ensuremath{\tilde m}}}
\def\tdn{{\ensuremath{\tilde n}}}
\def\tdo{{\ensuremath{\tilde o}}}
\def\tdp{{\ensuremath{\tilde p}}}
\def\tdq{{\ensuremath{\tilde q}}}
\def\tdr{{\ensuremath{\tilde r}}}
\def\tdw{{\ensuremath{\tilde w}}}
\def\tdu{{\ensuremath{\tilde u}}}
\def\tdv{{\ensuremath{\tilde r}}}
\def\tds{{\ensuremath{\tilde s}}}
\def\tdt{{\ensuremath{\tilde t}}}
\def\tdx{{\ensuremath{\tilde x}}}
\def\tdy{{\ensuremath{\tilde y}}}
\def\tdz{{\ensuremath{\tilde z}}}
\def\chka{{\ensuremath{\check a}}}
\def\chkb{{\ensuremath{\check b}}}
\def\chkc{{\ensuremath{\check c}}}
\def\chkd{{\ensuremath{\check d}}}
\def\chke{{\ensuremath{\check e}}}
\def\chkf{{\ensuremath{\check f}}}
\def\chkg{{\ensuremath{\check g}}}
\def\chkh{{\ensuremath{\check h}}}
\def\chki{{\ensuremath{\check i}}}
\def\chkj{{\ensuremath{\check j}}}
\def\chkk{{\ensuremath{\check k}}}
\def\chkl{{\ensuremath{\check l}}}
\def\chkm{{\ensuremath{\check m}}}
\def\chkn{{\ensuremath{\check n}}}
\def\chko{{\ensuremath{\check o}}}
\def\chkp{{\ensuremath{\check p}}}
\def\chkq{{\ensuremath{\check q}}}
\def\chkr{{\ensuremath{\check r}}}
\def\chkw{{\ensuremath{\check w}}}
\def\chku{{\ensuremath{\check u}}}
\def\chkv{{\ensuremath{\check v}}}
\def\chks{{\ensuremath{\check s}}}
\def\chkt{{\ensuremath{\check t}}}
\def\chkx{{\ensuremath{\check x}}}
\def\chky{{\ensuremath{\check y}}}
\def\chkz{{\ensuremath{\check z}}}
\def\bbA{{\ensuremath{\mathbf A}}}
\def\bbB{{\ensuremath{\mathbf B}}}
\def\bbC{{\ensuremath{\mathbf C}}}
\def\bbD{{\ensuremath{\mathbf D}}}
\def\bbE{{\ensuremath{\mathbf E}}}
\def\bbF{{\ensuremath{\mathbf F}}}
\def\bbG{{\ensuremath{\mathbf G}}}
\def\bbH{{\ensuremath{\mathbf H}}}
\def\bbI{{\ensuremath{\mathbf I}}}
\def\bbJ{{\ensuremath{\mathbf J}}}
\def\bbK{{\ensuremath{\mathbf K}}}
\def\bbL{{\ensuremath{\mathbf L}}}
\def\bbM{{\ensuremath{\mathbf M}}}
\def\bbN{{\ensuremath{\mathbf N}}}
\def\bbO{{\ensuremath{\mathbf O}}}
\def\bbP{{\ensuremath{\mathbf P}}}
\def\bbQ{{\ensuremath{\mathbf Q}}}
\def\bbR{{\ensuremath{\mathbf R}}}
\def\bbW{{\ensuremath{\mathbf W}}}
\def\bbU{{\ensuremath{\mathbf U}}}
\def\bbV{{\ensuremath{\mathbf V}}}
\def\bbS{{\ensuremath{\mathbf S}}}
\def\bbT{{\ensuremath{\mathbf T}}}
\def\bbX{{\ensuremath{\mathbf X}}}
\def\bbY{{\ensuremath{\mathbf Y}}}
\def\bbZ{{\ensuremath{\mathbf Z}}}
\def\bba{{\ensuremath{\mathbf a}}}
\def\bbb{{\ensuremath{\mathbf b}}}
\def\bbc{{\ensuremath{\mathbf c}}}
\def\bbd{{\ensuremath{\mathbf d}}}
\def\bbe{{\ensuremath{\mathbf e}}}
\def\bbf{{\ensuremath{\mathbf f}}}
\def\bbg{{\ensuremath{\mathbf g}}}
\def\bbh{{\ensuremath{\mathbf h}}}
\def\bbi{{\ensuremath{\mathbf i}}}
\def\bbj{{\ensuremath{\mathbf j}}}
\def\bbk{{\ensuremath{\mathbf k}}}
\def\bbl{{\ensuremath{\mathbf l}}}
\def\bbm{{\ensuremath{\mathbf m}}}
\def\bbn{{\ensuremath{\mathbf n}}}
\def\bbo{{\ensuremath{\mathbf o}}}
\def\bbp{{\ensuremath{\mathbf p}}}
\def\bbq{{\ensuremath{\mathbf q}}}
\def\bbr{{\ensuremath{\mathbf r}}}
\def\bbw{{\ensuremath{\mathbf w}}}
\def\bbu{{\ensuremath{\mathbf u}}}
\def\bbv{{\ensuremath{\mathbf v}}}
\def\bbs{{\ensuremath{\mathbf s}}}
\def\bbt{{\ensuremath{\mathbf t}}}
\def\bbx{{\ensuremath{\mathbf x}}}
\def\bby{{\ensuremath{\mathbf y}}}
\def\bbz{{\ensuremath{\mathbf z}}}
\def\bb0{{\ensuremath{\mathbf 0}}}


\def\bbarbbA{{\bar{\ensuremath{\mathbf A}} }}
\def\bbarbbB{{\bar{\ensuremath{\mathbf B}} }}
\def\bbarbbC{{\bar{\ensuremath{\mathbf C}} }}
\def\bbarbbD{{\bar{\ensuremath{\mathbf D}} }}
\def\bbarbbE{{\bar{\ensuremath{\mathbf E}} }}
\def\bbarbbF{{\bar{\ensuremath{\mathbf F}} }}
\def\bbarbbG{{\bar{\ensuremath{\mathbf G}} }}
\def\bbarbbH{{\bar{\ensuremath{\mathbf H}} }}
\def\bbarbbI{{\bar{\ensuremath{\mathbf I}} }}
\def\bbarbbJ{{\bar{\ensuremath{\mathbf J}} }}
\def\bbarbbK{{\bar{\ensuremath{\mathbf K}} }}
\def\bbarbbL{{\bar{\ensuremath{\mathbf L}} }}
\def\bbarbbM{{\bar{\ensuremath{\mathbf M}} }}
\def\bbarbbN{{\bar{\ensuremath{\mathbf N}} }}
\def\bbarbbO{{\bar{\ensuremath{\mathbf O}} }}
\def\bbarbbP{{\bar{\ensuremath{\mathbf P}} }}
\def\bbarbbQ{{\bar{\ensuremath{\mathbf Q}} }}
\def\bbarbbR{{\bar{\ensuremath{\mathbf R}} }}
\def\bbarbbS{{\bar{\ensuremath{\mathbf S}} }}
\def\bbarbbT{{\bar{\ensuremath{\mathbf T}} }}
\def\bbarbbU{{\bar{\ensuremath{\mathbf U}} }}
\def\bbarbbV{{\bar{\ensuremath{\mathbf V}} }}
\def\bbarbbW{{\bar{\ensuremath{\mathbf W}} }}
\def\bbarbbX{{\bar{\ensuremath{\mathbf X}} }}
\def\bbarbbY{{\bar{\ensuremath{\mathbf Y}} }}
\def\bbarbbZ{{\bar{\ensuremath{\mathbf Z}} }}
\def\bbarbba{{\bar{\ensuremath{\mathbf a}} }}
\def\bbarbbb{{\bar{\ensuremath{\mathbf b}} }}
\def\bbarbbc{{\bar{\ensuremath{\mathbf c}} }}
\def\bbarbbd{{\bar{\ensuremath{\mathbf d}} }}
\def\bbarbbe{{\bar{\ensuremath{\mathbf e}} }}
\def\bbarbbf{{\bar{\ensuremath{\mathbf f}} }}
\def\bbarbbg{{\bar{\ensuremath{\mathbf g}} }}
\def\bbarbbh{{\bar{\ensuremath{\mathbf h}} }}
\def\bbarbbi{{\bar{\ensuremath{\mathbf i}} }}
\def\bbarbbj{{\bar{\ensuremath{\mathbf j}} }}
\def\bbarbbk{{\bar{\ensuremath{\mathbf k}} }}
\def\bbarbbl{{\bar{\ensuremath{\mathbf l}} }}
\def\bbarbbm{{\bar{\ensuremath{\mathbf m}} }}
\def\bbarbbn{{\bar{\ensuremath{\mathbf n}} }}
\def\bbarbbo{{\bar{\ensuremath{\mathbf o}} }}
\def\bbarbbp{{\bar{\ensuremath{\mathbf p}} }}
\def\bbarbbq{{\bar{\ensuremath{\mathbf q}} }}
\def\bbarbbr{{\bar{\ensuremath{\mathbf r}} }}
\def\bbarbbs{{\bar{\ensuremath{\mathbf s}} }}
\def\bbarbbt{{\bar{\ensuremath{\mathbf t}} }}
\def\bbarbbu{{\bar{\ensuremath{\mathbf u}} }}
\def\bbarbbv{{\bar{\ensuremath{\mathbf v}} }}
\def\bbarbbw{{\bar{\ensuremath{\mathbf w}} }}
\def\bbarbbx{{\bar{\ensuremath{\mathbf x}} }}
\def\bbarbby{{\bar{\ensuremath{\mathbf y}} }}
\def\bbarbbz{{\bar{\ensuremath{\mathbf z}} }}
\def\hhatbbA{{\hat{\ensuremath{\mathbf A}} }}
\def\hhatbbB{{\hat{\ensuremath{\mathbf B}} }}
\def\hhatbbC{{\hat{\ensuremath{\mathbf C}} }}
\def\hhatbbD{{\hat{\ensuremath{\mathbf D}} }}
\def\hhatbbE{{\hat{\ensuremath{\mathbf E}} }}
\def\hhatbbF{{\hat{\ensuremath{\mathbf F}} }}
\def\hhatbbG{{\hat{\ensuremath{\mathbf G}} }}
\def\hhatbbH{{\hat{\ensuremath{\mathbf H}} }}
\def\hhatbbI{{\hat{\ensuremath{\mathbf I}} }}
\def\hhatbbJ{{\hat{\ensuremath{\mathbf J}} }}
\def\hhatbbK{{\hat{\ensuremath{\mathbf K}} }}
\def\hhatbbL{{\hat{\ensuremath{\mathbf L}} }}
\def\hhatbbM{{\hat{\ensuremath{\mathbf M}} }}
\def\hhatbbN{{\hat{\ensuremath{\mathbf N}} }}
\def\hhatbbO{{\hat{\ensuremath{\mathbf O}} }}
\def\hhatbbP{{\hat{\ensuremath{\mathbf P}} }}
\def\hhatbbQ{{\hat{\ensuremath{\mathbf Q}} }}
\def\hhatbbR{{\hat{\ensuremath{\mathbf R}} }}
\def\hhatbbS{{\hat{\ensuremath{\mathbf S}} }}
\def\hhatbbT{{\hat{\ensuremath{\mathbf T}} }}
\def\hhatbbU{{\hat{\ensuremath{\mathbf U}} }}
\def\hhatbbV{{\hat{\ensuremath{\mathbf V}} }}
\def\hhatbbW{{\hat{\ensuremath{\mathbf W}} }}
\def\hhatbbX{{\hat{\ensuremath{\mathbf X}} }}
\def\hhatbbY{{\hat{\ensuremath{\mathbf Y}} }}
\def\hhatbbZ{{\hat{\ensuremath{\mathbf Z}} }}
\def\hhatbba{{\hat{\ensuremath{\mathbf a}} }}
\def\hhatbbb{{\hat{\ensuremath{\mathbf b}} }}
\def\hhatbbc{{\hat{\ensuremath{\mathbf c}} }}
\def\hhatbbd{{\hat{\ensuremath{\mathbf d}} }}
\def\hhatbbe{{\hat{\ensuremath{\mathbf e}} }}
\def\hhatbbf{{\hat{\ensuremath{\mathbf f}} }}
\def\hhatbbg{{\hat{\ensuremath{\mathbf g}} }}
\def\hhatbbh{{\hat{\ensuremath{\mathbf h}} }}
\def\hhatbbi{{\hat{\ensuremath{\mathbf i}} }}
\def\hhatbbj{{\hat{\ensuremath{\mathbf j}} }}
\def\hhatbbk{{\hat{\ensuremath{\mathbf k}} }}
\def\hhatbbl{{\hat{\ensuremath{\mathbf l}} }}
\def\hhatbbm{{\hat{\ensuremath{\mathbf m}} }}
\def\hhatbbn{{\hat{\ensuremath{\mathbf n}} }}
\def\hhatbbo{{\hat{\ensuremath{\mathbf o}} }}
\def\hhatbbp{{\hat{\ensuremath{\mathbf p}} }}
\def\hhatbbq{{\hat{\ensuremath{\mathbf q}} }}
\def\hhatbbr{{\hat{\ensuremath{\mathbf r}} }}
\def\hhatbbs{{\hat{\ensuremath{\mathbf s}} }}
\def\hhatbbt{{\hat{\ensuremath{\mathbf t}} }}
\def\hhatbbu{{\hat{\ensuremath{\mathbf u}} }}
\def\hhatbbv{{\hat{\ensuremath{\mathbf v}} }}
\def\hhatbbw{{\hat{\ensuremath{\mathbf w}} }}
\def\hhatbbx{{\hat{\ensuremath{\mathbf x}} }}
\def\hhatbby{{\hat{\ensuremath{\mathbf y}} }}
\def\hhatbbz{{\hat{\ensuremath{\mathbf z}} }}
\def\tdbbA{{\tilde{\ensuremath{\mathbf A}} }}
\def\tdbbB{{\tilde{\ensuremath{\mathbf B}} }}
\def\tdbbC{{\tilde{\ensuremath{\mathbf C}} }}
\def\tdbbD{{\tilde{\ensuremath{\mathbf D}} }}
\def\tdbbE{{\tilde{\ensuremath{\mathbf E}} }}
\def\tdbbF{{\tilde{\ensuremath{\mathbf F}} }}
\def\tdbbG{{\tilde{\ensuremath{\mathbf G}} }}
\def\tdbbH{{\tilde{\ensuremath{\mathbf H}} }}
\def\tdbbI{{\tilde{\ensuremath{\mathbf I}} }}
\def\tdbbJ{{\tilde{\ensuremath{\mathbf J}} }}
\def\tdbbK{{\tilde{\ensuremath{\mathbf K}} }}
\def\tdbbL{{\tilde{\ensuremath{\mathbf L}} }}
\def\tdbbM{{\tilde{\ensuremath{\mathbf M}} }}
\def\tdbbN{{\tilde{\ensuremath{\mathbf N}} }}
\def\tdbbO{{\tilde{\ensuremath{\mathbf O}} }}
\def\tdbbP{{\tilde{\ensuremath{\mathbf P}} }}
\def\tdbbQ{{\tilde{\ensuremath{\mathbf Q}} }}
\def\tdbbR{{\tilde{\ensuremath{\mathbf R}} }}
\def\tdbbS{{\tilde{\ensuremath{\mathbf S}} }}
\def\tdbbT{{\tilde{\ensuremath{\mathbf T}} }}
\def\tdbbU{{\tilde{\ensuremath{\mathbf U}} }}
\def\tdbbV{{\tilde{\ensuremath{\mathbf V}} }}
\def\tdbbW{{\tilde{\ensuremath{\mathbf W}} }}
\def\tdbbX{{\tilde{\ensuremath{\mathbf X}} }}
\def\tdbbY{{\tilde{\ensuremath{\mathbf Y}} }}
\def\tdbbZ{{\tilde{\ensuremath{\mathbf Z}} }}
\def\tdbba{{\tilde{\ensuremath{\mathbf a}} }}
\def\tdbbb{{\tilde{\ensuremath{\mathbf b}} }}
\def\tdbbc{{\tilde{\ensuremath{\mathbf c}} }}
\def\tdbbd{{\tilde{\ensuremath{\mathbf d}} }}
\def\tdbbe{{\tilde{\ensuremath{\mathbf e}} }}
\def\tdbbf{{\tilde{\ensuremath{\mathbf f}} }}
\def\tdbbg{{\tilde{\ensuremath{\mathbf g}} }}
\def\tdbbh{{\tilde{\ensuremath{\mathbf h}} }}
\def\tdbbi{{\tilde{\ensuremath{\mathbf i}} }}
\def\tdbbj{{\tilde{\ensuremath{\mathbf j}} }}
\def\tdbbk{{\tilde{\ensuremath{\mathbf k}} }}
\def\tdbbl{{\tilde{\ensuremath{\mathbf l}} }}
\def\tdbbm{{\tilde{\ensuremath{\mathbf m}} }}
\def\tdbbn{{\tilde{\ensuremath{\mathbf n}} }}
\def\tdbbo{{\tilde{\ensuremath{\mathbf o}} }}
\def\tdbbp{{\tilde{\ensuremath{\mathbf p}} }}
\def\tdbbq{{\tilde{\ensuremath{\mathbf q}} }}
\def\tdbbr{{\tilde{\ensuremath{\mathbf r}} }}
\def\tdbbs{{\tilde{\ensuremath{\mathbf s}} }}
\def\tdbbt{{\tilde{\ensuremath{\mathbf t}} }}
\def\tdbbu{{\tilde{\ensuremath{\mathbf u}} }}
\def\tdbbv{{\tilde{\ensuremath{\mathbf v}} }}
\def\tdbbw{{\tilde{\ensuremath{\mathbf w}} }}
\def\tdbbx{{\tilde{\ensuremath{\mathbf x}} }}
\def\tdbby{{\tilde{\ensuremath{\mathbf y}} }}
\def\tdbbz{{\tilde{\ensuremath{\mathbf z}} }}
\def\bbcalA{\mbox{\boldmath }}
\def\bbcalB{\mbox{\boldmath }}
\def\bbcalC{\mbox{\boldmath }}
\def\bbcalD{\mbox{\boldmath }}
\def\bbcalE{\mbox{\boldmath }}
\def\bbcalF{\mbox{\boldmath }}
\def\bbcalG{\mbox{\boldmath }}
\def\bbcalH{\mbox{\boldmath }}
\def\bbcalI{\mbox{\boldmath }}
\def\bbcalJ{\mbox{\boldmath }}
\def\bbcalK{\mbox{\boldmath }}
\def\bbcalL{\mbox{\boldmath }}
\def\bbcalM{\mbox{\boldmath }}
\def\bbcalN{\mbox{\boldmath }}
\def\bbcalO{\mbox{\boldmath }}
\def\bbcalP{\mbox{\boldmath }}
\def\bbcalQ{\mbox{\boldmath }}
\def\bbcalR{\mbox{\boldmath }}
\def\bbcalW{\mbox{\boldmath }}
\def\bbcalU{\mbox{\boldmath }}
\def\bbcalV{\mbox{\boldmath }}
\def\bbcalS{\mbox{\boldmath }}
\def\bbcalT{\mbox{\boldmath }}
\def\bbcalX{\mbox{\boldmath }}
\def\bbcalY{\mbox{\boldmath }}
\def\bbcalZ{\mbox{\boldmath }}
\def\bbcale{\mbox{\boldmath }}
\def\tdupsilon{\tilde\upsilon}
\def\tdalpha{\tilde\alpha}
\def\tdbeta{\tilde\beta}
\def\tdgamma{\tilde\gamma}
\def\tddelta{\tilde\delta}
\def\tdepsilon{\tilde\epsilon}
\def\tdvarepsilon{\tilde\varepsilon}
\def\tdzeta{\tilde\zeta}
\def\tdeta{\tilde\eta}
\def\tdtheta{\tilde\theta}
\def\tdvartheta{\tilde\vartheta}

\def\tdiota{\tilde\iota}
\def\tdkappa{\tilde\kappa}
\def\tdlambda{\tilde\lambda}
\def\tdmu{\tilde\mu}
\def\tdnu{\tilde\nu}
\def\tdxi{\tilde\xi}
\def\tdpi{\tilde\pi}
\def\tdrho{\tilde\rho}
\def\tdvarrho{\tilde\varrho}
\def\tdsigma{\tilde\sigma}
\def\tdvarsigma{\tilde\varsigma}
\def\tdtau{\tilde\tau}
\def\tdupsilon{\tilde\upsilon}
\def\tdphi{\tilde\phi}
\def\tdvarphi{\tilde\varphi}
\def\tdchi{\tilde\chi}
\def\tdpsi{\tilde\psi}
\def\tdomega{\tilde\omega}

\def\tdGamma{\tilde\Gamma}
\def\tdDelta{\tilde\Delta}
\def\tdTheta{\tilde\Theta}
\def\tdLambda{\tilde\Lambda}
\def\tdXi{\tilde\Xi}
\def\tdPi{\tilde\Pi}
\def\tdSigma{\tilde\Sigma}
\def\tdUpsilon{\tilde\Upsilon}
\def\tdPhi{\tilde\Phi}
\def\tdPsi{\tilde\Psi}
\def\bbarupsilon{\bar\upsilon}
\def\bbaralpha{\bar\alpha}
\def\bbarbeta{\bar\beta}
\def\bbargamma{\bar\gamma}
\def\bbarsigma{\bar\sigma}
\def\bbardelta{\bar\delta}
\def\bbarepsilon{\bar\epsilon}
\def\bbarvarepsilon{\bar\varepsilon}
\def\bbarzeta{\bar\zeta}
\def\bbareta{\bar\eta}
\def\bbartheta{\bar\theta}
\def\bbarvartheta{\bar\vartheta}

\def\bbariota{\bar\iota}
\def\bbarkappa{\bar\kappa}
\def\bbarlambda{\bar\lambda}
\def\bbarmu{\bar\mu}
\def\bbarnu{\bar\nu}
\def\bbarxi{\bar\xi}
\def\bbarpi{\bar\pi}
\def\bbarrho{\bar\rho}
\def\bbarvarrho{\bar\varrho}
\def\bbarvarsigma{\bar\varsigma}
\def\bbarphi{\bar\phi}
\def\bbarvarphi{\bar\varphi}
\def\bbarchi{\bar\chi}
\def\bbarpsi{\bar\psi}
\def\bbaromega{\bar\omega}

\def\bbarGamma{\bar\Gamma}
\def\bbarDelta{\bar\Delta}
\def\bbarTheta{\bar\Theta}
\def\bbarLambda{\bar\Lambda}
\def\bbarXi{\bar\Xi}
\def\bbarPi{\bar\Pi}
\def\bbarSigma{\bar\Sigma}
\def\bbarUpsilon{\bar\Upsilon}
\def\bbarPhi{\bar\Phi}
\def\bbarPsi{\bar\Psi}
\def\chkupsilon{\check\upsilon}
\def\chkalpha{\check\alpha}
\def\chkbeta{\check\beta}
\def\chkgamma{\check\gamma}
\def\chkdelta{\check\delta}
\def\chkepsilon{\check\epsilon}
\def\chkvarepsilon{\check\varepsilon}
\def\chkzeta{\check\zeta}
\def\chketa{\check\eta}
\def\chktheta{\check\theta}
\def\chkvartheta{\check\vartheta}

\def\chkiota{\check\iota}
\def\chkkappa{\check\kappa}
\def\chklambda{\check\lambda}
\def\chkmu{\check\mu}
\def\chknu{\check\nu}
\def\chkxi{\check\xi}
\def\chkpi{\check\pi}
\def\chkrho{\check\rho}
\def\chkvarrho{\check\varrho}
\def\chksigma{\check\sigma}
\def\chkvarsigma{\check\varsigma}
\def\chktau{\check\tau}
\def\chkupsilon{\check\upsilon}
\def\chkphi{\check\phi}
\def\chkvarphi{\check\varphi}
\def\chkchi{\check\chi}
\def\chkpsi{\check\psi}
\def\chkomega{\check\omega}

\def\chkGamma{\check\Gamma}
\def\chkDelta{\check\Delta}
\def\chkTheta{\check\Theta}
\def\chkLambda{\check\Lambda}
\def\chkXi{\check\Xi}
\def\chkPi{\check\Pi}
\def\chkSigma{\check\Sigma}
\def\chkUpsilon{\check\Upsilon}
\def\chkPhi{\check\Phi}
\def\chkPsi{\check\Psi}
\def\bbupsilon{{\mbox{\boldmath }}}
\def\bbalpha{{\mbox{\boldmath }}}
\def\bbbeta{{\mbox{\boldmath }}}
\def\bbgamma{{\mbox{\boldmath }}}
\def\bbdelta{{\mbox{\boldmath }}}
\def\bbepsilon{{\mbox{\boldmath }}}
\def\bbvarepsilon{{\mbox{\boldmath }}}
\def\bbzeta{{\mbox{\boldmath }}}
\def\bbeta{{\mbox{\boldmath }}}
\def\bbtheta{{\mbox{\boldmath }}}
\def\bbvartheta{{\mbox{\boldmath }}}

\def\bbiota{{\mbox{\boldmath }}}
\def\bbkappa{{\mbox{\boldmath }}}
\def\bblambda{{\mbox{\boldmath }}}
\def\bbmu{{\mbox{\boldmath }}}
\def\bbnu{{\mbox{\boldmath }}}
\def\bbxi{{\mbox{\boldmath }}}
\def\bbpi{{\mbox{\boldmath }}}
\def\bbrho{{\mbox{\boldmath }}}
\def\bbvarrho{{\mbox{\boldmath }}}
\def\bbvarsigma{{\mbox{\boldmath }}}
\def\bbphi{{\mbox{\boldmath }}}
\def\bbvarphi{{\mbox{\boldmath }}}
\def\bbchi{{\mbox{\boldmath }}}
\def\bbpsi{{\mbox{\boldmath }}}
\def\bbomega{{\mbox{\boldmath }}}

\def\bbGamma{{\mbox{\boldmath }}}
\def\bbDelta{{\mbox{\boldmath }}}
\def\bbTheta{{\mbox{\boldmath }}}
\def\bbLambda{{\mbox{\boldmath }}}
\def\bbXi{{\mbox{\boldmath }}}
\def\bbPi{{\mbox{\boldmath }}}
\def\bbSigma{{\mbox{\boldmath }}}
\def\bbUpsilon{{\mbox{\boldmath }}}
\def\bbPhi{{\mbox{\boldmath }}}
\def\bbPsi{{\mbox{\boldmath }}}

\def\bbarbbupsilon{\bar\bbupsilon}
\def\bbarbbalpha{\bar\bbalpha}
\def\bbarbbbeta{\bar\bbbeta}
\def\bbarbbgamma{\bar\bbgamma}
\def\bbarbbdelta{\bar\bbdelta}
\def\bbarbbepsilon{\bar\bbepsilon}
\def\bbarbbvarepsilon{\bar\bbvarepsilon}
\def\bbarbbzeta{\bar\bbzeta}
\def\bbarbbeta{\bar\bbeta}
\def\bbarbbtheta{\bar\bbtheta}
\def\bbarbbvartheta{\bar\bbvartheta}

\def\bbarbbiota{\bar\bbiota}
\def\bbarbbkappa{\bar\bbkappa}
\def\bbarbblambda{\bar\bblambda}
\def\bbarbbmu{\bar\bbmu}
\def\bbarbbnu{\bar\bbnu}
\def\bbarbbxi{\bar\bbxi}
\def\bbarbbpi{\bar\bbpi}
\def\bbarbbrho{\bar\bbrho}
\def\bbarbbvarrho{\bar\bbvarrho}
\def\bbarbbvarsigma{\bar\bbvarsigma}
\def\bbarbbphi{\bar\bbphi}
\def\bbarbbvarphi{\bar\bbvarphi}
\def\bbarbbchi{\bar\bbchi}
\def\bbarbbpsi{\bar\bbpsi}
\def\bbarbbomega{\bar\bbomega}

\def\bbarbbGamma{\bar\bbGamma}
\def\bbarbbDelta{\bar\bbDelta}
\def\bbarbbTheta{\bar\bbTheta}
\def\bbarbbLambda{\bar\bbLambda}
\def\bbarbbXi{\bar\bbXi}
\def\bbarbbPi{\bar\bbPi}
\def\bbarbbSigma{\bar\bbSigma}
\def\bbarbbUpsilon{\bar\bbUpsilon}
\def\bbarbbPhi{\bar\bbPhi}
\def\bbarbbPsi{\bar\bbPsi}
\def\hhatbbupsilon{\hat\bbupsilon}
\def\hhatbbalpha{\hat\bbalpha}
\def\hhatbbbeta{\hat\bbbeta}
\def\hhatbbgamma{\hat\bbgamma}
\def\hhatbbdelta{\hat\bbdelta}
\def\hhatbbepsilon{\hat\bbepsilon}
\def\hhatbbvarepsilon{\hat\bbvarepsilon}
\def\hhatbbzeta{\hat\bbzeta}
\def\hhatbbeta{\hat\bbeta}
\def\hhatbbtheta{\hat\bbtheta}
\def\hhatbbvartheta{\hat\bbvartheta}

\def\hhatbbiota{\hat\bbiota}
\def\hhatbbkappa{\hat\bbkappa}
\def\hhatbblambda{\hat\bblambda}
\def\hhatbbmu{\hat\bbmu}
\def\hhatbbnu{\hat\bbnu}
\def\hhatbbxi{\hat\bbxi}
\def\hhatbbpi{\hat\bbpi}
\def\hhatbbrho{\hat\bbrho}
\def\hhatbbvarrho{\hat\bbvarrho}
\def\hhatbbvarsigma{\hat\bbvarsigma}
\def\hhatbbphi{\hat\bbphi}
\def\hhatbbvarphi{\hat\bbvarphi}
\def\hhatbbchi{\hat\bbchi}
\def\hhatbbpsi{\hat\bbpsi}
\def\hhatbbomega{\hat\bbomega}

\def\hhatbbGamma{\hat\bbGamma}
\def\hhatbbDelta{\hat\bbDelta}
\def\hhatbbTheta{\hat\bbTheta}
\def\hhatbbLambda{\hat\bbLambda}
\def\hhatbbXi{\hat\bbXi}
\def\hhatbbPi{\hat\bbPi}
\def\hhatbbSigma{\hat\bbSigma}
\def\hhatbbUpsilon{\hat\bbUpsilon}
\def\hhatbbPhi{\hat\bbPhi}
\def\hhatbbPsi{\hat\bbPsi}
\def\tdbbupsilon{\tilde\bbupsilon}
\def\tdbbalpha{\tilde\bbalpha}
\def\tdbbbeta{\tilde\bbbeta}
\def\tdbbgamma{\tilde\bbgamma}
\def\tdbbdelta{\tilde\bbdelta}
\def\tdbbepsilon{\tilde\bbepsilon}
\def\tdbbvarepsilon{\tilde\bbvarepsilon}
\def\tdbbzeta{\tilde\bbzeta}
\def\tdbbeta{\tilde\bbeta}
\def\tdbbtheta{\tilde\bbtheta}
\def\tdbbvartheta{\tilde\bbvartheta}

\def\tdbbiota{\tilde\bbiota}
\def\tdbbkappa{\tilde\bbkappa}
\def\tdbblambda{\tilde\bblambda}
\def\tdbbmu{\tilde\bbmu}
\def\tdbbnu{\tilde\bbnu}
\def\tdbbxi{\tilde\bbxi}
\def\tdbbpi{\tilde\bbpi}
\def\tdbbrho{\tilde\bbrho}
\def\tdbbvarrho{\tilde\bbvarrho}
\def\tdbbvarsigma{\tilde\bbvarsigma}
\def\tdbbphi{\tilde\bbphi}
\def\tdbbvarphi{\tilde\bbvarphi}
\def\tdbbchi{\tilde\bbchi}
\def\tdbbpsi{\tilde\bbpsi}
\def\tdbbomega{\tilde\bbomega}

\def\tdbbGamma{\tilde\bbGamma}
\def\tdbbDelta{\tilde\bbDelta}
\def\tdbbTheta{\tilde\bbTheta}
\def\tdbbLambda{\tilde\bbLambda}
\def\tdbbXi{\tilde\bbXi}
\def\tdbbPi{\tilde\bbPi}
\def\tdbbSigma{\tilde\bbSigma}
\def\tdbbUpsilon{\tilde\bbUpsilon}
\def\tdbbPhi{\tilde\bbPhi}
\def\tdbbPsi{\tilde\bbPsi}
\def\deltat{\triangle t}
\def\hhattheta{\hat\theta}
\def\var{{\rm var}}
\def\bbtau{{\mbox{\boldmath }}}

\makeatletter
\def\wideubar{\underaccent{{\cc@style\underline{\mskip8mu}}}}
\def\Wideubar{\underaccent{{\cc@style\underline{\mskip6mu}}}}
\makeatother

\makeatletter
\def\widebar{\accentset{{\cc@style\underline{\mskip8mu}}}}
\def\Widebar{\accentset{{\cc@style\underline{\mskip6mu}}}}
\makeatother



 
\begin{document}

\newenvironment{centergpic}{}{\begin{center}~\box\graph~\end{center}}



\title{{\color{black}{Power Scheduling of Kalman Filtering in Wireless Sensor Networks with Data Packet Drops}}
}

\author{Gang~Wang,
~\IEEEmembership{Student~Member,~IEEE,}~Jie~Chen,~\IEEEmembership{Senior~Member,~IEEE,}\\Jian~Sun,~\IEEEmembership{Member,~IEEE},~and~Yongjian Cai
\thanks{This work was supported in part by the Natural Science Foundation of China under Grant 61104097, National Science Foundation for
Distinguished Young Scholars of China under Grant 60925011, Projects of Major International (Regional) Joint Research Program NSFC under Grant 61120106010, Beijing Education Committee Cooperation Building Foundation Project XK100070532, and Research Fund for the Doctoral Program of Higher Education of China 20111101120027. G. Wang was supported in part by
the China Scholarship Council.}
\thanks{The authors are with the School of Automation, Beijing Institute of Technology,
Beijing 100081, China, and also with Key Laboratory of Intelligent Control and Decision of Complex Systems. (E-mail: wang4937@umn.edu, chenjie@bit.edu.cn, sunjian@bit.edu.cn, caiyongjian@bit.edu.cn).

The corresponding author of this paper is J. Sun.

}
}

\iffalse
\and
\IEEEauthorblockN{Jie Chen}
\IEEEauthorblockA{School of Automation\\
Beijing Institute of Technology\\
Beijing, China\\
Email: chenjie@bit.edu.cn}
\and
\IEEEauthorblockN{Jian Sun}
\IEEEauthorblockA{School of Automation\\
Beijing Institute of Technology\\
Beijing, China\\
Telephone: (800) 555--1212\\
Fax: (888) 555--1212}\fi


\maketitle

\begin{abstract}
For a wireless sensor network (WSN) with a large number of low-cost, battery-driven, multiple transmission power leveled sensor nodes of limited transmission bandwidth, then conservation of transmission resources (power and bandwidth) is of paramount importance. Towards this end, this paper considers the problem of power scheduling of Kalman filtering for general linear stochastic systems subject to data packet drops (over a packet-dropping wireless network). The transmission of the acquired measurement from the sensor to the remote estimator is realized by sequentially transmitting every single component of the measurement to the remote estimator in one time period. The sensor node decides separately whether to use a high or low transmission power to communicate every component to the estimator across a packet-dropping wireless network based on the rule that promotes the power scheduling with the least impact on the estimator mean squared error. {\color{black}{Under the customary assumption that the predicted density is (approximately) Gaussian, leveraging the statistical distribution of sensor data, the mechanism of power scheduling, the wireless network effect and the received data, the minimum mean squared error estimator is derived. By investigating the statistical convergence properties of the estimation error covariance, we establish, for general linear systems, both the sufficient condition and the necessary condition guaranteeing the stability of the estimator.}}
\end{abstract}

\begin{IEEEkeywords}
Power scheduling, Kalman filtering, data packet drops, wireless sensor networks, linear stochastic systems, stability.
\end{IEEEkeywords}


\section{Introduction}
\label{section 1}
With the groundbreaking advances of microsensor technology and wireless communication technology, wireless sensor
networks (WSNs) have been found in a plethora of applications. The proposed and/or already deployed applications include, but not limited to, battlefield surveillance, intelligent transportation systems, health care, environment monitoring and control, disaster prevention and recovery, and more efficient electric power grids \cite{cn2002su, tsp2006ribeiro1, tsp2006ribeiro2, tsp2006ribeiro, tsp2008msechu, tsp2012msechu, tac2012jia}. However, there are still some severe limitations in current WSNs that prevent them from better serving the people, such as, limited power at each battery-driven sensor, limited communication ability, limited computation ability and limited wireless bandwidth \cite{tac2012jia}. These limitations will ineluctably bring some challenging problems to the study of estimation and control over WSNs. Therefore, it is of great significance to investigate how to conserve transmission power and bandwidth while achieving a similar estimation performance.

Towards this end, recurring attention has been paid to the research of remote estimation under communication resources (energy constraint and bandwidth) requirement in the last decade and a multitude of publications can be widely found in the literature; see, for example, \cite{tsp2012you, tsp2006ribeiro1,tsp2006ribeiro2,cs2010ribeiro,tsp2007schizas,tsp2008msechu,tsp2012msechu,tit2005luo,tsp2006luo,tac2012jia,tac2009savage,
spm2006luo,tsp2006ribeiro,auto2007suh,timc2011you,tsp2012shixie,auto2012battistelli,tac2013shi,auto2011shi,tsp2011yang,tsp2013you,iet2013wang, ccc2012you} and references therein. Among them, by the desire of conserving transmission energy and bandwidth, various methods regarding measurement quantization, censoring, and dimensionality-reduction were specialized in \cite{tsp2006ribeiro, tsp2006ribeiro1, tsp2006ribeiro2, cs2010ribeiro, tsp2007schizas, tsp2008msechu, tsp2012msechu,
tsp2006luo, spm2006luo, timc2011you}. Another creative method in terms of measurement scheduling has been extensively studied in \cite{tsp2012you, tac2009savage, auto2007suh, tsp2012shixie, auto2012battistelli, tac2013shi, auto2011shi, tsp2011yang, tsp2013you} etc.

 To be more specific, owing to the power-limited nature of wireless sensors and the fact that replacing the exhausted batteries are costly operations and may even be impossible, only a limited number of measurement transmissions can thereby be made by the wireless devices in most WSNs applications. In \cite{tac2009savage}, optimal measurement scheduling policies were devised for a particular class of scalar Gauss-Markov systems to minimize the terminal estimation error variance over a given time horizon , \iffalse say  \fi in which only  measurements can be taken and transmitted to the remote estimator side. In practical, most commercially available sensor nodes nowadays have multiple transmission power levels \cite{tsp2006luo} and it is assumed that high transmission power leads to reliable data flow while low transmission power may cause unreliable data flow \cite{auto2011shi} and therefore data packet drops may occur. {\color{black}The results in \cite{tac2009savage} were then recently extended to a special class of high-order Gauss-Markov systems in \cite{tsp2012shixie}, where both the sensor energy constraint and data packet drops were taken into account and furthermore, two scenarios in terms of sensor nodes with limited or sufficient computation capacity are considered. Under some appropriate conditions, the optimal schedulers derived indicate that the  measurement transmissions should be distributed along the last  time steps over the time horizon  that is, from  to  It is worth noticing that the optimal measurement schedulers above are deterministic, which are so-called ``offline schedulers,''  and therefore, this kind of offline schedulers have the apparent advantage of offline determination of optimal scheduling schemes. Nevertheless, also noticed that the estimation error covariance matrix increases drastically for unstable systems in the first  time steps owing to no measurements transmitted to the estimator side to update the covariance prediction, which is a disadvantage of these offline schedulers. More discussions and generalizations on offline schedulers can also be found in \cite{auto2011shi} and \cite{tsp2011yang}.}

 On the other hand, to avoid the disadvantage mentioned above, schedulers taking the current measurement value into consideration were devised in \cite{tsp2012you,auto2007suh,auto2012battistelli,tsp2013you,tac2013shi} and considering the modified Kalman filter therein is very much involved with a stochastic variable, these schedulers are called ``online schedulers.'' The send-on-delta strategy was adopted in \cite{auto2007suh} to reduce sensor data traffic by transmitting sensor data only if their values change exceeds a prescribed threshold. However, the threshold has no analytic relationship with the estimation performance and no stability and performance analysis were given with respect to the proposed modified Kalman filter. Innovation-based measurement schedulers were primarily constructed in \cite{tsp2012you}, \cite{tac2013shi} by quantifying the ``importance'' of every measurement using the normalized measurement innovations. The main idea is that only ``important" enough measurements will be transmitted to the estimator side to update the state prediction and covariance prediction, and when the transmission does not occur, the additionally known information based on given threshold of the scheduler will be utilized. Moreover, some stability analysis of Kalman filtering with the aforementioned two stochastic schedulers was presented in \cite{tsp2012you} and {\color{black}{however, only necessary conditions guaranteeing the convergence of expected estimation error covariance were established for systems with full-row-ranked observation matrix therein.}}


Inspired by those observations, this paper builds on and considerably broads the scope of \cite{tsp2012shixie} and \cite{tsp2012you}, where the power scheduler is dependent on the time-horizon  and the covariance increases drastically during the first  time steps. {\color{black}In comparison, the main contributions of this work are twofold and summarized as follows.
\begin{enumerate}
\item We consider power scheduling problem of remote state estimation of general high-order linear stochastic systems. Data packet drop, a typical and natural phenomenon in wireless networks, is also considered and modeled as one Bernoulli i.i.d. process. See Fig. \ref{fig1} for an illustration, where the power scheduler is embedded in the sensor node. We devise a component-wise innovation-based power scheduler and the corresponding minimum mean squared error estimator (MMSE).

\item We investigate the statistical convergence properties of the estimation error covariance matrix by constructing one auxiliary function and we establish both the sufficient condition and the necessary condition for convergence of the averaged estimation error covariance. Theorem \ref{sufficient condition} originally establishes the sufficient condition for mean square stability of estimation error covariance matrix and Theorem \ref{necessary condition} extends the results for systems with full-row-ranked observation matrix in the literature to general linear systems.  Therefore, this work is an important generalization of and a necessary complementary to the literature of state estimation of WSNs in the sense of both estimation framework and theoretical stability analysis; see, for example, \cite{tsp2012you, tsp2006ribeiro, cs2010ribeiro, tac2004sinopoli, cdc2007garone, auto2011youxie}.

\end{enumerate}

\begin{figure}
\begin{minipage}[b]{0.5\textwidth}
\centering
\includegraphics[scale=0.6]{test.eps}
{\color{black}{\caption{Network architecture.}}}
\label{fig1}
\end{minipage}\end{figure}
}

Coincidentally, from a mathematical point of view, the stability analysis can be cast into the well-received category of Kalman filtering with incomplete (dropped or delayed) observations primarily studied in \cite{tac2004sinopoli, cdc2004goldsmith, cdc2007garone, tac2008luca, auto2009shixie} and lately in \cite{tac2012sinopoli, arxiv2009sinopoli}. Explicit comparisons made between the present work and those pioneering works definitely show the implications and necessity of this work. Part of the material in this paper was presented in \cite{cyber2013wang}.

 The remainder of this paper is organized as follows. We briefly introduce the measurement model as well as some standard assumptions in Section \ref{section 2} and devise the minimum mean squared error estimator with power scheduler in Section \ref{section 3}. In Section \ref{section c}, we provide both sufficient condition and necessary condition that guarantee the convergence of averaged estimation error covariance matrix. Finally, conclusions and current research threads are outlined in Section \ref{section 4}.
\emph{Notations:}
{\color{black}{Straight boldface denote the multivariate quantities such as vectors (lowercase) and matrices (uppercase). Let  be the tail probability of the standard normal distribution, i.e.,  Denote  or  by a normally distributed vector  with mean  and covariance  For random vectors  and   denotes the expectation value of  and  denotes the conditional random vector when  is given. Furthermore, we use  to denote the transpose of a matrix, use  to represent the positive definite (positive semi-definite) matrix  and use  to denote the diagonal matrix with the main diagonal elements   denotes the  identity matrix and  denotes the zero matrix of appropriate dimensions. The notation  denotes the Kronecker product of two matrices. The mean square stability of the filter, i.e.,  implies there always exists a positive definite matrix  such that  for all  \cite{auto2011youxie}, where the mathematical expectation is taken with respect to both the random power scheduling process and random packet drop process in this paper. For two positive definite matrices  and  the matrix inequality  means matrix  is positive semidefinite. Similar notations will be made for  and }}




\section{Problem Formulation and Preliminaries}
\label{section 2}
Consider the following linear discrete-time stochastic system:

where  is the state vector and  is the measurement vector,  and  are Gaussian random vectors with zero-means and covariance matrices  and  respectively. The initial state  is also assumed to be a Gaussian random vector with mean  and covariance matrix  It is further posited that the random vectors  are mutually independent.

{\color{black}{ We assume a high transmission energy leads to reliable data flow while a low transmission energy may result in data packet drops during wireless network communications. This assumption is reasonable and motivated by the two facts: Most economically available sensors in the market have multiple transmission energy levels to choose from \cite{tsp2006luo} and higher transmission energy leads to a higher signal-to-noise ratio (SNR) at the remote estimator, which can be simply interpreted as a higher packet arrival rate \cite{book2007}.}} Therefore, once communication failure occurs, the whole data packet will drop. {\color{black}For simplicity, the present paper considers that the sensor node has only two transmission power levels \cite{auto2011shi}, \cite{tsp2012shixie} and, though, results derived in this paper can be easily generalized to multiple transmission power level case. Specifically, when a high transmission power  is employed, the data packet can be successfully delivered to the estimator side; when a low transmission power  is employed, then the data packet is supposed to arrive at the estimator side only with a probability . Similar power scheduling has also been considered in \cite{tsp2012shixie} with a different estimation framework.}


Before delving into the mechanism of power scheduling, the following two standard assumptions are presented.
\begin{assumption}\label{assumption 4}
 is controllable and  is observable.
\end{assumption}
\begin{assumption}\label{assumption 1}
The covariance  is diagonal, i.e., .
\end{assumption}

{\color{black}\begin{remark}
In fact, if the measurement noise vectors  are white, then covariance matrix  is diagonal. If  are not white and  is thus a general positive definite matrix, the idea is primarily to whiten the observations. To this end, we define the square root matrix of a positive definite matrix  as  Instead of using  we consider a transformed measurement

where  Therefore without loss of generality, we can assume the measurement noise covariance  to be diagonal.
\end{remark}}

\section{Power Scheduling and \\Sequential Kalman Filtering}
\label{section 3}
{\color{black}{In this paper, a round-robin, slotted-time measurement transmission policy is envisioned such that, only a scalar is allowed to be communicated to the estimator at every transmission and one sampling interval (i.e., one time instant from each  to ) can be explicitly partitioned into  (the dimension of measurement vector ) time slots, and at the th time slot  the scheduler located at the sensor node decides whether to use the high or low transmission energy to transmit  the th component of   This sensor scheduling protocol was also used in \cite{tsp2012msechu}.}} 
It is well acknowledged that the measurement innovation indicates new information of the current measurement that is not contained in all historical measurements and intuitively speaking, a large innovation represents the current measurement is quite different than the predicted measurement and therefore contains much useful information to update the estimate. Thus, we define the measurement of large innovation as ``important'' measurement and otherwise, less ``important'' measurement. In this sense, we devise an innovation-based power scheduling policy, which compares the normalized measurement innovation with a given threshold to quantify the ``importance'' of every measurement and then uses a high (or low) transmission power to communicate the ``important'' (or less ``important'') measurement.

Specifically, at time instant  let the binary random variables   represent whether the transmission power  or  is utilized for transmission of  Let another sequence of random variables  or  for  indicate whether the data packet  arrives at estimator side successfully or not. {\color{black}{Throughout this paper, we postulate that the values of  and  for all  at every  can be observed; since we can employ TCP-like protocols where the packet acknowledgements are guaranteed at every time instant to notify estimator whether the data packet is received \cite{tac2012sinopoli, cdc2007garone}.}} For future reference, define  and 


{\color{black}{For any time instant  denote by  the mean squared error estimate of  at estimator based upon all received information at the end of th time slot and likewise, by  the estimation error covariance, i.e.,

Let  {\color{black} {and  be given fixed thresholds.}} Then the developed power scheduler and the corresponding MMSE estimator are together tabulated as Algorithm \ref{ps}.}}

\begin{algorithm}
\caption{(Local Power Scheduler and Remote MMSE Estimator)}\label{ps}
\begin{algorithmic}
\STATE {\textbf{Initialization:}\\
,}
\STATE {\textbf{Time prediction:} given  and , do\\
\\
\STATE {\textbf{Power scheduling, measurement transmission and measurement update:}}\\
Define  \\  and \\
For  set\\
{\color{black}{({\bf{Estimator side}})}}\\
\\
Transmit  back to sensor\\
{\color{black}{({\bf{Sensor side}})\\}}
\\
\\
{Power scheduling:}\\
Define the scheduling variable as\\
\\
{Measurement transmission:}\\
If  send  to the estimator with high transmission power  otherwise, transmit  with low transmission power \\
If  then use variable  to represent whether  is successfully transmitted to the remote estimator. Namely,
 indicates  arrives successfully at the remote estimator and  means  drops during the transmission. \\
{\color{black}{({\bf{Estimator side}})}}\\
{Measurement update:}\\
For  to  let
\\
do\\
\\
\\
where

\\
\\


End, do  and 
}
\end{algorithmic}
\end{algorithm}
{\color{black}{{\color{black}{
The mechanism of the proposed innovation-based scheduler will be further elaborated in this part. The power scheduler located at the sensor side will sequentially decide whether to adopt a high or low energy to transmit every single component  to remote estimator. For instance, the scheduler is now in a position to make decision on assigning energy for transmission of  at time slot  of time instant  Suppose the estimator has already reliably sent back current state estimate  and error covariance  to local sensor side, where, by ``reliably'' we mean the remote estimator can always adopt a high energy to broadcast information to the sensor so no packet drop happens for the transmission from estimator back to sensor side. This also makes sense because usually wireless sensors consume much less energy for receiving one packet than sending one packet \cite{mica}. Thus, after receiving  and  the sensor can compute a normalized innovation  of current single component  of measurement vector  Then comparing the normalized innovation with a given threshold  if greater than the threshold, this component  will be transmitted to estimator by a high energy; otherwise, a low energy will be used at this time slot  The estimator will correspondingly leverage different rules to update  to obtain  and then reliably send them back to the local sensor side for the next cycle.
}}
}}


{\color{black}{
\begin{remark}
It should be noticed that the effect of channel medium between the sensor and estimator has not been considered here. As pointed out in \cite{handbook}, most of the existing work in distributed estimation assumed perfect channels between sensors and the fusion center. However, since only one scalar  is transmitted at every time slot, let us consider transmitting  to estimator over a channel with channel gain  We envision that the channel undergoes slow fading such that the phase of complex channel can be estimated and therefore compensated for at the receiver side, so that  defines the real-valued envelop of the complex channel gain \cite{auto2009dey}. Also, suppose that the channel gain remains invariant over the time slot to send  Then the estimator receives a scaled version of  corrupted with the channel noise which is independent of the measurement noise. For simplicity, let the channel noise  be zero-mean Gaussian white noise with variance  and, let   be mutually independent for 

In light of the round-robin, time-slotted transmission policy and (\ref{measurement}), one further arrives at  Denote  and it follows that

If channel gain  is constant over time instant  then by letting  this model reduces to (\ref{measurement}). Nonetheless, for faded channel case, Kalman filtering with faded observations was considered in \cite{auto2009dey} and stability analysis was also presented therein. So results in this paper can be generalized to the faded channel case by adopting similar method in \cite{auto2009dey} to tackle  of fading distribution, which will be explored in the future.
\end{remark}
}
}
\begin{proposition}
\label{proposition}
It is postulated that the conditional distribution of  given  is approximately Gaussian, i.e., the probability density function (pdf) 
Then  in Algorithm \ref{ps} is a minimum mean squared error estimator.
\end{proposition}

{\color{black}{
\begin{remark}
 Recall further that the pdf  is in general non-Gaussian and therefore, (computationally expensive) numerical integrations and (memory intensive) propagation of the posterior pdf are required for the computation of the exact MMSE estimate \cite{tsp2006ribeiro}. However, based upon customary simplifications adopted in nonlinear filtering \cite{tsp2003gpf} and Kalman filtering with quantized measurements/innovations \cite{tsp2006ribeiro, tsp2013nair, spm2006luo}, the assumption on an approximately Gaussian distribution of the predicted density is made. This assumption can be widely found in the literature; see, for example, \cite{tsp2008msechu, timc2011you, tsp2012you} and references therein for further discussion.\end{remark}
}
}

\begin{IEEEproof}[Proof of Proposition 1]
Provided that we already have an MMSE estimator  that is,  and  We prove the proposition by conditioning on whether the measurement is received by the estimator. Specifically, when the new measurement  is present at the estimator side, that is,  the case  or the case  one can easily verify that

and likewise,


When the estimator does not receive the new measurement  that is,  then it follows that

{\color{black}{Here,  is the pdf of the random variable  similarly,  is the pdf of a random variable  conditional on variable 
Given  then  follows Gaussian distribution with zero-mean and unit covariance. Thus, the conditional pdf above follows directly from conditional probability theory:

where  Therefore, (\ref{pdf}) becomes
}}where the first integration equals to  and the second becomes  because  is even over the origin-centered symmetric integration interval.

In the sequel, we compute the covariance  for  and  case as follows:

where {\color{black}{(b) follows directly from  when the new measurement component  is not received by the estimator, which has been proved in (\ref{noy}), and (d) is because  in Algorithm \ref{ps}, and (e) is because  is zero-mean Gaussian noise with covariance  or,  }} Meanwhile, we have

{\color{black}{Therefore, from (\ref{pkki})-(\ref{prob}) and  one arrives at


To write the two scenarios discussed above in a more compact form, it follows that

which completes the proof.}}

\end{IEEEproof}


Given the new filter formulation in Algorithm \ref{ps}, the processes   form a sequence of independent and identically distributed (i.i.d.) processes under the Gaussian approximation \cite{tsp2012you} and also, assume the processes   are mutually independent Bernoulli i.i.d. processes. Define  For  let

and  in addition,

{\color{black}where, in fact, we have  and  Moreover,  is one strictly decreasing function in threshold  this makes sense since the greater the threshold is, the less information will be transmitted through high energy. Then, one can easily verify  and therefore,  can be somehow physically interpreted as the normalized averaged information received by remote estimator resulting from the power scheduling and networked effect on transmitting  (and  quantifies the corresponding averaged information loss rate). All s together will governor the mean square stability of estimation error covariance matrix, which will be investigated in the ensuing section. Therefore, we will refer to  hereafter other than the specific parameters }


\section{Statistical Properties and Sufficient, Necessary Convergence Conditions}
\label{section c}
{\color{black}{In this section, the convergence conditions for the expected estimation error covariance will be provided by discussing properties of a constructed function. Denote   and   Since they are inherently stochastic and cannot be determined offline, therefore, only statistical properties can be derived. Before delving into main results, some preliminaries will be given in the following.}}

Let   Define the function  and the function  as follows:

and here denote the notation  by the function composite.
Therefore, the covariance update in the sequential Kalman filter formulation in Algorithm \ref{ps} becomes

Let

Denote the function  by the transformation from  to  namely,


In order to analyze the convergence of the estimation error covariance matrix, we then define the modified algebraic Riccati equation (MARE) in the following way:

where we used the simplified notation  {\color{black}{Meanwhile, as explained, the covariance matrices  depend nonlinearly on the specific realization of the stochastic processes  and  so the sequential Kalman filter is inherently stochastic and cannot be determined offline. Then, only statistical properties with respect to the covariance matrices of the proposed sequential Kalman filter can therefore be established.}}
\begin{remark}
\label{remark mare}
It is noted in passing that the modified algebraic Riccati equation defined in (\ref{varphi function}) is a more generalized form than the original MARE specified for Kalman filtering with only one or two lossy channels in \cite{tac2004sinopoli} and \cite{cdc2004goldsmith}, respectively, where the analysis might be much easier than that of (\ref{varphi function}). Moreover, since the MARE in (\ref{varphi function}) is sequentially composited by  original MAREs with different parameters, namely,  then the MARE in (\ref{varphi function}) is also quite different from the MARE discussed in \cite{tac2012sinopoli} defined for Kalman filtering for multiple-input multiple-output systems with control signals and sensored measurements transmitting across multiple TCP-like erasure channels. Accurately speaking, the MARE in \cite{tac2012sinopoli} follows directly from that in \cite{tac2004sinopoli} by replacing the observation matrix  with  Therefore, for the sake of completeness on stability theory of Kalman filtering with intermittent observations, the investigation on properties of the MARE in (\ref{varphi function}) in the following sections is also of great implications, which significantly contributes to the derivation of sufficient conditions for stability of sequential Kalman filtering with scheduled measurements in \cite{tsp2012you}.
\end{remark}

 The following lemma on the properties of the auxiliary function  is presented before we will formally study the convergence properties of the MARE in (\ref{varphi function}).
\begin{lemma}[\cite{tac2004sinopoli}]\label{psi lemma}
Let the function  be

where  Then the following facts hold:
\begin{enumerate}
\item With given  \label{fact11}
\item \label{fact12}
\item If  then \label{fact13}
\item If  then \label{fact14}
\item If  then \label{fact15}.
\end{enumerate}
\iffalse pwhere, a small covariance matrix is indeed a covariance matrix with small trace and the Kalman filter minimizes the trace of the ocvariance matrix. Therefore, (\ref{fact12}) implies \fi
\end{lemma}
\begin{IEEEproof}
The proofs for these statements are analogous to those of Lemma 1 in \cite{tac2004sinopoli} with some appropriate notation adaptations.
\end{IEEEproof}


\iffalse
{\emph{Proof:}}
The proof is quite similar to that of Lemma 1 in \cite{tac2004sinopoli}. \\
(1) Note that

Given that  then it easily follows that

Therefore, by adding the following auxiliary term  to  it yields that

(2) Define function  Obviously,

For  considering  is a quadratic and convex function of variable  so the minimizer can be obtained via solving  and it follows that

Clearly, the fact can be derived from fact (1).\newline
(3) Observe that  is affine in  so  for  and any   Assume  then

The remaining facts (4)-(7) can be easily proved, so here omitted.

\fi

\iffalse
We are now in a position to establish some properties of the function  in form of lemmas before the main results will be given.
\begin{lemma}\label{concave varphi}
 is a concave function of variable  then 
\end{lemma}

{\emph{Proof:}} From (\ref{psi function}), it follows that  is affine in  \iffalse From fact (5) in Lemma \ref{psi lemma}, we know  are concave functions.\fi Let  then


where  directly follows from fact (2) in Lemma ,  from fact (1),  from fact (3) and  from fact (5). Then similarly and sequentially for  it yields that

That is,  is concave in variable  Considering the monotonicity and linearity of function  in variable  and  it can be obtained that

Therefore,  is also a concave function of variable  Then according to the well-known {\emph{Jensen's inequality}}, it yields that


\begin{lemma}\label{nondecreasing varphi}
 is nondecreasing in variable  That is, for  then 
\end{lemma}

{\emph{Proof:}} The lemma can be verified directly from the monotonically increasing property of functions  and  and the details are thereby omitted here.


\begin{lemma}
\label{varphi lowerbound}
Given any  and  then 
\end{lemma}

{\emph{Proof:}} From fact (1) in Lemma \ref{properties psi}, it yields that for 

Therefore, owing to the monotonically increasing property of functions  then  That is,


\begin{lemma}\label{varphi bound}
Assume  is a random variable, then

\end{lemma}

{\emph{Proof:}}
This Lemma combines Lemma \ref{varphi lowerbound} and Lemma \ref{concave varphi}. In \ref{varphi lowerbound}, we have  Then by taking the expectation of both sides, it yields that  That is, the left part is proved. In Lemma \ref{concave varphi}, it follows that  That accomplishes the proof.
\begin{remark}\label{p bound}
Observe that if we substitute  into Lemma \ref{varphi bound}, it follows that  Since  and  then  That is, the expected value of  can be lower-bounded and upper-bounded by  and  both as functions of  respectively.
\end{remark}

\fi

Notice, that the relationship between the function  and the function  has been built, and now, in order to investigate the convergence properties of the MARE in (\ref{varphi function}), the relationship between the composite function  and the introduced auxiliary function  will be constructed in the following way.

According to (\ref{psi function}), observe that the function  is a function with respect to two matrix variables . With a slight abuse of notation, denote  by the composite function  with respect to the second variable  where 

In the sequel,  can be derived as follows.
Let us define   Then,

where, to make the expression more concrete, we defined  and 

For the sake of brevity, denote

More importantly, it is easy to exploit the fact that the sum of  coefficients  is identically 1, i.e.,

Alternatively, (\ref{ts}) can be given by

Therefore,

where  and  and  is defined in (\ref{eqts}) with  given by (\ref{etajs2}).

\iffalse 


Therefore,   and

By taking the function  into consideration,  can therefore be partitioned into  parts corresponding to the  coefficients , that is,  can be rewritten as

Then (\ref{eqtm}) can be rewritten in the following way

where   For convenience of notation, define  and  Therefore, the function  can be given in a more compact form

where  and  and  is defined in (\ref{ts}) with  given by (\ref{etajs1}), (\ref{etajs2}).

\fi 
\begin{remark}
Note that the auxiliary function  defined by (\ref{tm function}) is of similar form to that in \cite{tac2012sinopoli}, \cite{arxiv2009sinopoli}. To be more specific, the latter auxiliary function is referred as follows:

with  and constants 
One can easily observe that there are  terms in (\ref{lqg}), and the computation burden will become catastrophic when the dimension  of the measurement vector tends to be very large. Meanwhile, only  terms will be needed for the sequential Kalman filter in this paper, which may significantly reduce the computation burden and is therefore of great importance.
\end{remark}

We are now in a position to establish some properties of the function  in form of lemmas in the following.
\begin{lemma}\label{m lemma}
Consider the function  as stated by (\ref{tm function}) with  Assume  Then, the following facts hold:
\begin{enumerate}
\item With given  
    where \label{fact 1}
\item  \label{fact 2}
\item If  then  \label{fact 3}
\item If  then \label{fact 5}
\item \label{fact 6}
\item For a random variable  \label{fact 7}
\end{enumerate}
\end{lemma}
\begin{IEEEproof}



\begin{enumerate}
\item\label{proof1} Fact \ref{fact 1}) together with Fact \ref{fact 2}) is equivalent to showing the minimizer and the minimum value of matrix-valued function  with respect to multiple vector-valued variables  For convenience of notation, denote  We first make extensive use of differential of general matrix-valued function  with respect to a matrix argument  see, for instance, \cite{tif2009payaro}. \newline
    \begin{definition}
    Let  be a differentiable  real matrix function of a  matrix of real variables  The {\emph{Jacobian matrix}} of  at  is given by the  matrix
    
    \end{definition}

    \iffalse With slightly abuse of notation, we omit the notation  for brevity. For example, let  \fi
    Then by vectorizing the differential  it gives that:
    
    where the Jacobian matrix of matrix  with respect to matrix variable  is defined as   To make the results more concrete, let us define:
    
Therefore, after complicated and tedious matrix computations, the Jacobian matrices can be obtained as follows:
    
    where intentionally,  was not replaced by  for the compactness of the structure of 

    By solving  it follows straightforwardly that
    
    where 

    Then similarly, by solving  it gives that
    
    where  It should be clearly noticed that   and then plugging  into
    (\ref{tm function}) verifies that 

    \item We show this fact by mathematical induction. When  one can easily verify that  minimizes 
        Suppose now that it holds for  that is, the point  minimizes  Then for 
    
    and
    
    so one necessary condition for some point  minimizing  is that the point should also minimize 
    or,  minimizes  Therefore,  or,  when minimizing  Given that  is independent of  and,  and meanwhile,  is quadratic and convex in the variable  and therefore, the minimizer for  can be found by letting
    
    which leads to the unique solution 
    Therefore, the point  minimizes  This completes the proof.










\item Observe that the function  is affine in the variable  Let  and it yields that
    
    where (a) is because  minimizes the function  with respect to variables  then for any  say,  that is, (a) holds true. (b) is due to  is affine in the variable  and (c) follows straightforwardly from Fact \ref{fact 2}) above.


\item Let  where  Notice that
    
    Assume that  Then, we have
    
    Therefore, the fact holds true.
\item Note that
    
    where  and  
\item The first inequality follows straightforwardly from Fact \ref{fact 6}) above and linearity of expectation, that is,
    
    The second inequality is due to Fact \ref{fact 5}) above which implies the concavity of the function 
    and therefore in the light of Jensen's inequality, it readily gives that
    




\iffalse
\item Note that  can be viewed as the covariance matrices of the sequential Kalman filter, then the mean squared error may be minimized by minimizing the trace of  Then we calculate the differentials of  with respect to matrix variables   in the following:
    
    implying that
    
    Let the derivative above be zero, so that
    
    Now, we calculate the differential of  with respect to matrix variables 
    
    implying that
    
    Setting the derivative above be zero yields that
    
    Therefore, given any  the point   is the unique critical point of the function 
\fi
\end{enumerate}
\end{IEEEproof}




To take  into consideration, the auxiliary function  can be given in the following way:


\begin{lemma}\label{varphi lemma}
Consider the function  as stated by (\ref{phi function}) with   Assume  Then, the following facts hold:
\begin{enumerate}
\item With given  
    where \label{fact31}
\item  \label{fact32}
\item If  then  \label{fact33}
\item If  then \label{fact34}
\item \label{fact35}
\item If  then  \label{fact36}
\item For a random variable  \label{fact37}
\end{enumerate}
\end{lemma}
\begin{IEEEproof}
We only prove Fact \ref{fact36}) because the others can be derived directly from Lemma \ref{m lemma}.{\color{black}{

\begin{enumerate}
\item[6)] According to Fact \ref{fact37}) above, it gives that  Since  is controllable, then there must exist an  subject to the Lyapunov equation  if  is asymptotically stable. Accordingly, it follows that
    
    implying there exists a  such that
    
    Therefore,  or  This completes the proof.
\end{enumerate}
}}
\end{IEEEproof}


\begin{remark}\label{p bound}
Observe that if we substitute  into Fact \ref{fact37}) in Lemma \ref{varphi lemma}, it follows that  Since  and  then  That is, the expected value of  can be lower-bounded and upper-bounded by  and  both as functions of  respectively.
\end{remark}

To facilitate the convergence analysis, let us define the linear part of function  in terms of variable  as another auxiliary function, namely

where  are defined in (\ref{phi function}). Then, the following lemma can be readily presented.
\begin{lemma}\label{l lemma}
Consider the function  as stated in (\ref{l function}). If there exists a positive definite matrix  such that  then
\begin{enumerate}
\item  
\item Given  let the following sequence

initialized at  Then, the sequence  is bounded.
\end{enumerate}
\end{lemma}
\begin{IEEEproof}\begin{enumerate}
\item Note that  is affine in  and  and  for  There exist constants  and  such that  and  respectively. Then

Therefore, it can be readily obtained that  given that 
\item {\color{black}{Based on (\ref{ineq}) above, for any initialization  and any   there always exist two constants  and  such that  and  which are independent of  }}Therefore, similar arguments in (\ref{ineq}) lead to

Obviously, the result on the boundedness of the sequence  holds true.
\end{enumerate}
\end{IEEEproof}

\iffalse
\begin{figure*}[htbp]
\normalsize
\setcounter{MYtempeqncnt}{\value{equation}}
\setcounter{equation}{15}

\setcounter{equation}{\value{MYtempeqncnt}}
\setcounter{equation}{16}

\setcounter{equation}{\value{MYtempeqncnt}}
\hrulefill
\vspace*{4pt}
\end{figure*}
\fi



\begin{lemma}\label{phi convergence}
Consider the function  defined in (\ref{phi function}). Assume there exist  gain matrices  and
a positive definite matrix  such that

Then, the sequence  is bounded for any given  That is, there exists a positive definite matrix  depending on  such that

\end{lemma}
\begin{IEEEproof}Observe that  where  with   and  Therefore,

That is,  hence, the function  satisfies the condition of Lemma \ref{l lemma}. Considering the definition of  it yields that

where  Then based on fact 2) in Lemma \ref{l lemma}, it can be concluded that the sequence  is bounded for any .
\end{IEEEproof}
\begin{lemma}\label{lemma hh}
Let  and  Suppose that the function  is monotonically increase in  Then:

\end{lemma}
\begin{IEEEproof}The three statements can be similarly proved by mathematical induction. Thus, due to page limitation, we here only prove the first one. Since  then the first statement is true for  Then assume that  holds, so  holds owing to the monotonicity of function 
\end{IEEEproof}

{\color{black}{
After building these lemmas above, we are now in a position to establish the sufficient condition for mean square stability of the averaged estimation error covariance matrix.

\begin{theorem}[Sufficient condition]
\label{sufficient condition}
Consider the function  defined in (\ref{phi function}).
\iffalse
with  \fi If there exist  matrices  and a positive definite matrix  such that

Then, the following facts are true:
\begin{enumerate}\item The MARE converges for any initial condition  and the limit

is independent of the initial condition 
\item  is the unique positive definite fixed point of the MARE.
\end{enumerate}
\end{theorem}
}}

\begin{IEEEproof}1) To begin with, we verify the convergence of the MARE sequence initialized at  and therefore   Then it directly follows that  and in the light of Fact \ref{fact33} in Lemma \ref{varphi lemma}, it gives that

From Lemma \ref{lemma hh} and according to Lemma \ref{phi convergence}, a monotonically nondecreasing sequence of matrices follow straightforwardly from a simple inductive argument and the sequence is also upper-bounded, that is,

Here, one can easily verify that the monotonically nondecreasing and upper-bounded sequence converges from the Bolzano-Weierstrass theorem, that is,

where  is a fixed point of the following modified Riccati iteration


Then, we show that the modified Riccati iteration initialized at  also converges to the same point  By resorting to (\ref{l function}), it gives that

where  Therefore, the function  satisfies the condition of Lemma \ref{l lemma}. Accordingly, we realize that

Assume that  and then,

where is due to the monotonically increase property of the function  and (\ref{barp}). By induction, it establishes that

Meanwhile, we have

Then, since  it directly follows that 
That is, we have shown  as  when 

In the following, we are ready to justify that the modified Riccati iteration  converges to  for all initial conditions  Let  and  Then consider the three Riccati iterations initialized at  and  respectively. Clearly,  and in the light of Lemma \ref{lemma hh}, it gives that 
Given that both the sequence  and the sequence  converge to  consequently, we have 


2) Let us further postulate there exists another positive semi-definite matrix  such that  Let us consider the Riccati iteration initialized at  and therefore, we can derive the following sequence

From analysis above, it has been shown that every Riccati iteration converges to the same limit  Therefore, we have 
\end{IEEEproof}

{\color{black}In the sequel, we will provide an example of a scalar-state vector-observation system to justify the existence of sufficient condition in Theorem \ref{sufficient condition}.


{\emph{Example:}} We consider the following system 
where   noise covariances are  and  For simplicity, consider  and let  be, for instance, such that  or  Then one can always find  such that  satisfy condition (\ref{cond}) in Theorem \ref{sufficient condition}. That is, the expected estimation error covariance matrix will converge.}
{\color{black}{
In the ensuing part, we will present one necessary condition for ensuring mean square stability of expected estimation error covariance matrix which extends the result in \cite{tsp2012you} to general linear systems with data packet drops.

\begin{theorem}[Necessary condition]\label{necessary condition}
Consider system (\ref{system}) and Algorithm \ref{ps}. Assume that  is unstable, that  is controllable and that  is observable. If  holds for any initial condition  then  defined in (\ref{lambda}) should satisfy the following condition

where {\color{black}{ are all eigenvalues of square matrix }} and  depends on the initial condition 
\end{theorem}
}}

\begin{IEEEproof}
The proof follows straightforwardly from Fact \ref{fact37}) in Lemma \ref{varphi lemma}.
\end{IEEEproof}

\iffalse
\begin{theorem}\label{lmi}
Assume that  is controllable and  is observable. Then the following statements are equivalent:
\begin{enumerate}\item There exists  such that 
\item There exist  and  such that 
\item There exist  and  such that  where  is defined in (\ref{Psi function}) at the top of next page.
\end{enumerate}
\end{theorem}


To make solving  in (\ref{Psi function}) more tractably, the following corollary can be readily stated.
\begin{corollary}
\label{corollary}
One sufficient condition for the existence of  and  such that  is that there exist  and  such that  where  is stated in (\ref{Gamma function}) at the top of next page.
\end{corollary}
\fi



\iffalse

{\emph{Proof:}}
According to Algorithm \ref{skf}, we have

where  In fact, for   and for  it follows that
\thispagestyle{plain}

Therefore,  holds for all  Iteratively, it yields that

Taking the expectation of the last equation with respect to random variables  yields

Meanwhile, given that  so it can be obtained that

Set  and  to yield that

\iffalse
Define the Lyapunov equation  with  If  is controllable, then  is also controllable. Moreover, it is well acknowledged that the Lyapunov equation has a unique positive definite solution  if and only if  namely,  Therefore, there is no positive (semi-)definite solution to the Lyapunov equation  provided that  under the assumption  is controllable.

Consider the following difference equation  with initial condition 
\fi
It should be noted that in (\ref{skf final}), since  is controllable, if  is not asymptotically stable, that is, if  it easily follows that  with  diverges as  Considering  it can be concluded that if  for all initial condition  it follows that

\thispagestyle{plain}
\begin{lemma}\label{critical value}
Consider system (\ref{system}) and Algorithm \ref{skf}. Assume  is unstable,  is controllable and  is observable. Let us fix  packet arrival probabilities; say given  Under the Gaussian approximation, if for   holds for any initial condition  while for   for some initial conditions  then there exists a critical value  such that

and there exists a positive definite matrix  such that

The similar conclusions hold for the other  packet arrival probabilities.
\end{lemma}
{\emph{Proof:}}

\fi


\section{Concluding Remarks}
\label{section 4}
In this paper we devised a measurement innovation componentwise based power scheduler for wireless sensors in terms of optimally deciding whether to use a high or low transmission power to communicate each component of a measurement to the remote estimator side. The high transmission power is used to transmit the well-defined ``important'' measurements and low transmission power to transmit the less ``important'' measurements. Meanwhile, the high power transmission power is assumed to lead to reliable data flow while the low transmission power leads to unreliable data flow, that is, data packet drops. Under this new framework, the MMSE estimator was derived. Then convergence analysis of the averaged estimation error covariance was provided and moreover, both the sufficient condition and necessary condition guaranteeing its convergence were established for general linear stochastic systems. {\color{black}{Since the assumption of modeling the arrival of measurements as independent Bernoulli i.i.d. processes can be clearly improved upon, and therefore, future work will concentrate on accounting for communication channel modeling in this filtering framework \cite{auto2009dey, tsp2006luo}.}}




\section*{Acknowledgment}
The authors would like to express thanks to the anonymous reviewers for their insightful and constructive comments
that helped improving the quality of this paper.




\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}\typeout{** loaded for the language `#1'. Using the pattern for}\typeout{** the default language instead.}\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl


\bibitem{cn2002su}
I.~Akyildiz, W.~Su, Y.~Sankarasubramaniam, and E.~Cayirci, ``Wireless sensor
  networks: a survey,'' \emph{Comput. networks}, vol.~38, no.~4, pp. 393--422, Mar.
  2002.

\bibitem{tsp2006ribeiro1}
A.~Ribeiro and G.~Giannakis, ``Bandwidth-constrained distributed estimation
  for wireless sensor networks-Part I: Gaussian case,'' \emph{IEEE Trans. Signal Process.},
   vol.~54, no.~3, pp. 1131--1143, Mar. 2006.

\bibitem{tsp2006ribeiro2}
------, ``Bandwidth-constrained distributed estimation for wireless sensor
  networks-Part II: Unknown probability density function,'' \emph{IEEE Trans. Signal Process.},
   vol.~54, no.~7, pp. 2784--2796, Jul. 2006.

\bibitem{tsp2006ribeiro}
A.~Ribeiro, G.~Giannakis, and S.~Roumeliotis, ``SOI-KF: Distributed Kalman filtering with low-cost communications using the sign of innovations,''
 \emph{IEEE Trans. Signal Process.}, vol.~54, no.~12, pp.
  4782--4795, Dec. 2006.

\bibitem{tsp2008msechu}
E.~Msechu, S.~Roumeliotis, A.~Ribeiro, and G.~Giannakis,
  ``Decentralized quantized Kalman filtering with scalable communication
  cost,'' \emph{IEEE Trans. Signal Process.}, vol.~56, no.~8, pp.
  3727--3741, Aug. 2008.

\bibitem{tsp2012msechu}
E.~Msechu and G.~Giannakis, ``Sensor-centric data reduction for
  estimation with WSNs via censoring and quantization,'' \emph{IEEE Trans. Signal Process.},
   vol.~60, no.~1, pp. 400--414, Jan. 2012.
{\color{black}{
\bibitem{tac2012jia}
Q.~Jia, L.~Shi, Y.~Mo, and B.~Sinopoli, ``On optimal partial broadcasting of wireless sensor networks for Kalman filtering,''
\emph{IEEE Trans. Autom. Control}, vol. 57, no. 3, pp. 715--721, Mar. 2012.}}

\bibitem{cs2010ribeiro}
A.~Ribeiro, I.~Schizas, S.~Roumeliotis, and G.~Giannakis, ``Kalman filtering in
  wireless sensor networks,'' \emph{IEEE Control Sys. Mag.}, vol.~30, no.~2, pp.
  66--86, Apr. 2010.

\bibitem{tsp2007schizas}
I.~Schizas, G.~Giannakis, and Z.~Luo, ``Distributed estimation using
  reduced-dimensionality sensor observations,'' \emph{IEEE Trans. Signal Process.},
   vol.~55, no.~8, pp. 4284--4299, Aug. 2007.

{\color{black}{
 \bibitem{spm2006luo}
J.~Xiao, A.~Ribeiro, Z.~Luo, and G.~Giannakis, ``Distributed
  compression-estimation using wireless sensor networks,'' \emph{IEEE Signal
  Process. Mag.}, vol.~23, no.~4, pp. 27--41, Jul. 2006.}}

\bibitem{tsp2006luo}
J.~Xiao, S.~Cui, Z.~Luo, and A.~Goldsmith, ``Power scheduling of universal
  decentralized estimation in sensor networks,'' \emph{IEEE Trans. Signal Process.},
   vol.~54, no.~2, pp. 413--422, Feb. 2006.

\bibitem{timc2011you}
K.~You, L.~Xie, S.~Sun, and W.~Xiao, ``Quantized filtering of linear stochastic
  systems,'' \emph{Trans. Inst. Meas. Control},
  vol.~33, no.~6, pp. 683--698, Jul. 2011.

\bibitem{tac2009savage}
C.~Savage and B.~Scala, ``Optimal scheduling of scalar Gauss-Markov systems
  with a terminal cost function,'' \emph{IEEE Trans. Auto. Control},, vol.~54, no.~5,
  pp. 1100--1105, May 2009.

\bibitem{auto2011shi}
L.~Shi, P.~Cheng, and J.~Chen, ``Sensor data scheduling for optimal state
  estimation with communication energy constraint,'' \emph{Automatica},
  vol.~47, no.~8, pp. 1693--1698, Aug. 2011.

\bibitem{tsp2012shixie}
L.~Shi and L.~Xie, ``Optimal sensor power scheduling for state estimation of
  Gauss-Markov systems over a packet-dropping network,'' \emph{IEEE Trans. Signal Process.},
   vol.~60, no.~5, pp. 2701--2705, May 2012.


\bibitem{tit2005luo}
Z. Luo, ``Universal decentralized estimation in a bandwidth constrained
  sensor network,'' \emph{IEEE Trans. Inf. Theory}, vol.~51,
  no.~6, pp. 2210--2219, Jun. 2005.

\bibitem{auto2007suh}
Y.~Suh, V.~Nguyen, and Y.~Ro, ``Modified Kalman filter for networked monitoring
  systems employing a send-on-delta method,'' \emph{Automatica}, vol.~43,
  no.~2, pp. 332--338, Feb. 2007.

\bibitem{tsp2012you}
K.~You and L.~Xie, ``Kalman filtering with scheduled measurements,'' \emph{IEEE Trans. Signal Process.},
 vol.~61, no.~6, pp. 1520--1530, Mar. 2013.

\bibitem{tac2013shi}
J.~Wu, Q.~Jia, K.~Johansson, and L.~Shi, ``Event-based sensor data scheduling:
  Trade-off between communication rate and estimation quality,''
  \emph{IEEE Trans. Autom. Control}, vol.~58, no.~4, pp.
  1041--1046, Apr. 2013.

\bibitem{tsp2013you}
K.~You, L.~Xie, and S.~Song, ``Asymptotically optimal parameter estimation with
  scheduled measurements,'' \emph{IEEE Trans. Signal Process.},
  vol.~61, no.~14, pp. 3521--3531, Jul. 2013.

\bibitem{auto2012battistelli}
G.~Battistelli, A.~Benavoli, and L.~Chisci, ``Data-driven communication for
  state estimation with sensor networks,'' \emph{Automatica}, vol. 48, no.~5, pp. 926--935, May 2012.

\bibitem{tsp2011yang}
C.~Yang and L.~Shi, ``Deterministic sensor data scheduling under limited
  communication resource,'' \emph{IEEE Trans. Signal Process.},
  vol.~59, no.~10, pp. 5050--5056, Oct. 2011.

\bibitem{iet2013wang}
G.~Wang, J.~Chen, and J.~Sun, ``Stochastic stability of extended filtering for non-linear systems with measurement packet losses,''
\emph{IET Control Theory Appl.}, vol. 7, no.~17, pp. 2048--2055, Nov. 2013.

\bibitem{ccc2012you}
K.~You and L.~Xie, ``Kalman filtering with scheduled measurements-Part II:
Stability and performance analysis,'' in \emph{Proc. Chinese Control Conf.}, Hefei, China, July 25-27, pp. 5791--5796.


\bibitem{auto2011youxie}
K.~You, M.~Fu, and L.~Xie, ``Mean square stability for Kalman filtering with Markovian packet losses,'' \emph{Automatica}, vol.~47, no.~12, pp. 2647--2657, Dec. 2011.

\newpage

\bibitem{tac2004sinopoli}
B.~Sinopoli, L.~Schenato, M.~Franceschetti, K.~Poolla, M.~Jordan, and
  S.~Sastry, ``Kalman filtering with intermittent observations,''
  \emph{IEEE Trans. Autom. Control}, vol.~49, no.~9, pp.
  1453--1464, Sep. 2004.

{\color{black}{
\bibitem{cdc2007garone}
E.~Garone, B.~Sinopoli,~and A. Casavola, ``LQG control for distributed systems over TCP-like erasure channels,''
in \emph{Proc. 48th IEEE Conf. Decision Control}, New Orleans, LA, USA, Dec. 12-14, 2007, pp. 44--49.}}

{\color{black}{
\bibitem{auto2009shixie}
L. Shi, L. Xie, and R. Murray, ``Kalman filtering over a packet-delaying network: A probabilistic approach,'' \emph{Automatica}, vol. 45, no. 9,
pp. 2134--2140, Sep. 2009.}}


{\color{black}{
\bibitem{tac2008luca}
L. Schenato, ``Optimal estimation in networked control systems subject to random delay and packet drop,''
\emph{IEEE Trans. Autom. Control}, vol.~53, no.~5, pp.
1311--1317, Jun. 2008.}}

\bibitem{cdc2004goldsmith}
X.~Liu and A.~Goldsmith, ``Kalman filtering with partial observation losses,''
  in \emph{Proc. 43th IEEE Conf. Control Decision}, Altantis, Paradise Island, Bahamas, vol. 4, Dec. 14--17, 2004, pp. 4180--4186.


{\color{black}{
\bibitem{auto2009dey}
S.~Dey, A. Leong, and J. Evans, ``Kalman filtering with faded measurements,''
\emph{Automatica}, vol. 45, no.~10, pp. 2223--2233, Oct. 2009.
}}

\bibitem{tac2012sinopoli}
E.~Garone, B.~Sinopoli, A.~Goldsmith, and A.~Casavola, ``LQG control for MIMO
  systems over multiple erasure channels with perfect acknowledgment,''
  \emph{IEEE Trans. Autom. Control}, vol.~57, no.~2, pp. 450--456, Feb.
  2012.

\bibitem{arxiv2009sinopoli}
------, ``Proofs of LQG control for MIMO systems over multiple TCP--like erasure
  channels,'' \emph{arXiv preprint arXiv: 0909.2172}, 2009.

\bibitem{cyber2013wang}
G.~Wang, J.~Chen, and J.~Sun, ``On sequential Kalman filtering with scheduled measurements,''
\emph{Proc. 3rd IEEE Intl. Conf. Cyber Tech. Automation, Control,
and Intelli. Sys.}, Nanjing, China, May 26--29, 2013,
pp. 450--455.

{\color{black}{
\bibitem{book2007}
N.~Mahalik, \emph{Sensor Networks and Configuration: Fundamentals, Standards, Platforms, and Applications}, Berlin Heidelberg: Springer--Verlag, 2007.

\bibitem{mica}
A. Mainwaring, D. Culler, J. Polastre, R. Szewczyk, and J. Anderson, ``Wireless sensor networks for habitat monitoring,'' \emph{Intl. Wrksp. WSN Appl.}, Altanlta, GA, USA, Sep. 28--28, 2002, pp. 88-98.

\bibitem{handbook}
J. Xiao, S. Cui, and Z. Luo, ``Energy-efficient decentralized estimation,'' \emph{Handbook on Array Processing and Sensor Networks}, John Wiley  Sons, Inc., 469--497.

\bibitem{tsp2003gpf}
J.~Kotecha and P.~Djuric, ``Gaussian particle filtering,''
\emph{IEEE Trans. Signal Process.}, vol. 51, no. 10, pp. 2592--2601, Oct. 2003.

\bibitem{tsp2013nair}
A.~Leong, S. Dey, and G. Nair, ``Quantized filtering schemes for multi-sensor linear state estimation: Stability and performance under
high rate quantization,'' \emph{IEEE Trans. Signal Process.},
  vol.~61, no.~15, pp. 3852-3865, Aug. 2013.
  }}

\bibitem{tif2009payaro}
M.~Payar\'{o} and D.~Palomar, ``Hessian and concavity of mutual information,
  differential entropy, and entropy power in linear vector Gaussian channels,''
  \emph{IEEE Trans. Inf. Theory}, vol.~55, no.~8, pp.
  3613--3628, Aug. 2009.

\end{thebibliography}


\end{document}
