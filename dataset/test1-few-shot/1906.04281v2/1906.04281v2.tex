
\documentclass{article} \usepackage{iclr2020_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      







\clearpage{}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{mathtools} 

\usepackage{verbatim}

\usepackage{anyfontsize}

\usepackage{color}
\usepackage{tikz}
\usetikzlibrary{arrows,shapes,snakes,automata,backgrounds,fit,petri}
\usepackage{adjustbox}

\newcommand{\RN}[1]{\textup{\lowercase\expandafter{\it \romannumeral#1}}}



\makeatletter
\newcommand{\distas}[1]{\mathbin{\overset{#1}{\kern\z@\sim}}}


\usepackage{enumitem}




\usepackage[lined,boxed,commentsnumbered,ruled,linesnumbered]{algorithm2e}



\newcommand{\cl}[1]{{\color{blue}{\bf\sf [CL: #1]}}}


\def\Dir{\textsf{Dir}} \def\Bern{\textsf{Ber}} \def\Pois{\textsf{Pois}} \def\Std{\textsf{Std}} \def\Var{\textsf{Var}} \def\Cor{\textsf{Cor}} \def\Cov{\textsf{Cov}} \def\Gal{\textsf{Gamma}} \def\Beta{\textsf{Beta}} \def\KL{\textsf{KL}} \def\VI{\textsf{VI}} \def\MI{\textsf{MI}} \def\H{\textsf{H}} 

\newcommand{\ie}[0]{\emph{i.e., }}
\newcommand{\ea}[0]{\emph{et al. }}
\newcommand{\eg}[0]{\emph{e.g., }}
\newcommand{\cf}[0]{\emph{cf. }}
\newcommand{\etc}[0]{\emph{etc.}}

\newcommand{\zerov}{\ensuremath{{\bf 0}}}
\newcommand{\onev}{\ensuremath{{\bf 1}}}

\newcommand{\beq}{\vspace{0mm}}
\newcommand{\beqs}{\vspace{0mm}}
\newcommand{\barr}{\begin{array}}
\newcommand{\earr}{\end{array}}

\newcommand{\Amat}[0]{{{\bf A}}}
\newcommand{\Bmat}{{\bf B}}
\newcommand{\Cmat}{{\bf C}}
\newcommand{\Dmat}{{\bf D}}
\newcommand{\Emat}[0]{{{\bf E}}}
\newcommand{\Fmat}[0]{{{\bf F}}\xspace}
\newcommand{\Gmat}{{\bf G}}
\newcommand{\Hmat}{{\bf H}}
\newcommand{\Imat}{{\bf I}}
\newcommand{\Jmat}[0]{{{\bf J}}\xspace}
\newcommand{\Kmat}[0]{{{\bf K}}\xspace}
\newcommand{\Lmat}[0]{{{\bf L}}}
\newcommand{\Mmat}{{\bf M}}
\newcommand{\Nmat}[0]{{{\bf N}}\xspace}
\newcommand{\Omat}[0]{{{\bf O}}}
\newcommand{\Pmat}{{\bf P}}
\newcommand{\Qmat}[0]{{{\bf Q}}\xspace}
\newcommand{\Rmat}[0]{{{\bf R}}}
\newcommand{\Smat}[0]{{{\bf S}}}
\newcommand{\Tmat}[0]{{{\bf T}}}
\newcommand{\Umat}[0]{{{\bf U}}}
\newcommand{\Vmat}[0]{{{\bf V}}}
\newcommand{\Wmat}[0]{{{\bf W}}}
\newcommand{\Xmat}[0]{{{\bf X}}}
\newcommand{\Ymat}{{\bf Y}}
\newcommand{\Zmat}{{\bf Z}}

\newcommand{\av}[0]{{\boldsymbol{a}}}
\newcommand{\bv}[0]{{\boldsymbol{b}}}
\newcommand{\cv}[0]{{\boldsymbol{c}}}
\newcommand{\dv}{\boldsymbol{d}}
\newcommand{\ev}[0]{{\boldsymbol{e}}\xspace}
\newcommand{\fv}[0]{{\boldsymbol{f}}}
\newcommand{\gv}[0]{{\boldsymbol{g}}}
\newcommand{\hv}[0]{{\boldsymbol{h}}}
\newcommand{\iv}[0]{{\boldsymbol{i}}\xspace}
\newcommand{\jv}[0]{{\boldsymbol{j}}\xspace}
\newcommand{\kv}[0]{{\boldsymbol{k}}\xspace}
\newcommand{\lv}[0]{{\boldsymbol{l}}}
\newcommand{\mv}[0]{{\boldsymbol{m}}}
\newcommand{\nv}[0]{{\boldsymbol{n}}\xspace}
\newcommand{\ov}[0]{{\boldsymbol{o}}\xspace}
\newcommand{\pv}[0]{{\boldsymbol{p}}}
\newcommand{\qv}[0]{{\boldsymbol{q}}}
\newcommand{\sv}[0]{{\boldsymbol{s}}}
\newcommand{\tv}[0]{{\boldsymbol{t}}}
\newcommand{\uv}{\boldsymbol{u}}
\newcommand{\wv}{\boldsymbol{w}}
\newcommand{\xv}{\boldsymbol{x}}
\newcommand{\yv}{\boldsymbol{y}}
\newcommand{\zv}{\boldsymbol{z}}
\newcommand{\cdotv}{\boldsymbol{\cdot}}

\newcommand{\Gammamat}[0]{{\boldsymbol{\Gamma}}\xspace}
\newcommand{\Deltamat}[0]{{\boldsymbol{\Delta}}\xspace}
\newcommand{\Thetamat}{\boldsymbol{\Theta}}
\newcommand{\Betamat}{\boldsymbol{\Beta}}
\newcommand{\Lambdamat}{\boldsymbol{\Lambda}}
\newcommand{\Ximat}[0]{{\boldsymbol{\Xi}}\xspace}
\newcommand{\Pimat}[0]{{\boldsymbol{\Pi}}\xspace}
\newcommand{\Sigmamat}[0]{{\boldsymbol{\Sigma}}}
\newcommand{\Upsilonmat}[0]{{\boldsymbol{\Upsilon}}\xspace}
\newcommand{\Phimat}{\boldsymbol{\Phi}}
\newcommand{\Psimat}{\boldsymbol{\Psi}}
\newcommand{\Omegamat}[0]{{\boldsymbol{\Omega}}}

\newcommand{\alphav}{\boldsymbol{\alpha}}
\newcommand{\chiv}{\boldsymbol{\chi}}
\newcommand{\betav}[0]{{\boldsymbol{\beta}}}
\newcommand{\gammav}[0]{{\boldsymbol{\gamma}}\xspace}
\newcommand{\deltav}[0]{{\boldsymbol{\delta}}\xspace}
\newcommand{\epsilonv}{\boldsymbol{\epsilon}}
\newcommand{\zetav}{\boldsymbol{\zeta}}
\newcommand{\etav}{\boldsymbol{\eta}}
\newcommand{\ellv}[0]{{\boldsymbol{\ell}}}
\newcommand{\thetav}{\boldsymbol{\theta}}
\newcommand{\iotav}[0]{{\boldsymbol{\iota}}}
\newcommand{\kappav}[0]{{\boldsymbol{\kappa}}\xspace}
\newcommand{\lambdav}[0]{{\boldsymbol{\lambda}}}
\newcommand{\muv}[0]{{\boldsymbol{\mu}}}
\newcommand{\nuv}[0]{{\boldsymbol{\nu}}}
\newcommand{\xiv}[0]{{\boldsymbol{\xi}}}
\newcommand{\omicronv}[0]{{\boldsymbol{\omicron}}\xspace}
\newcommand{\piv}{\boldsymbol{\pi}}
\newcommand{\rhov}[0]{{\boldsymbol{\rho}}\xspace}
\newcommand{\sigmav}[0]{{\boldsymbol{\sigma}}}
\newcommand{\tauv}[0]{{\boldsymbol{\tau}}\xspace}
\newcommand{\upsilonv}[0]{{\boldsymbol{\upsilon}}\xspace}
\newcommand{\phiv}{\boldsymbol{\phi}}
\newcommand{\psiv}{\boldsymbol{\psi}}
\newcommand{\varthetav}{\boldsymbol{\vartheta}}
\newcommand{\omegav}[0]{{\boldsymbol{\omega}}}
\newcommand{\Z}{\mathbb{Z}}

\newcommand{\Rcalmat}{\mathcal{\bf R}}
\newcommand{\Pcalmat}{\mathcal{\bf P}}


\newcommand{\Xcal}{\mathcal{X}}
\newcommand{\Ycal}{\mathcal{Y}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Acal}{\mathcal{A}}
\newcommand{\Bcal}{\mathcal{B}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Fcal}{\mathcal{F}}
\newcommand{\Tcal}{\mathcal{T}}
\newcommand{\Pcal}{\mathcal{P}}
\newcommand{\Ical}{\mathcal{I}}
\newcommand{\Kcal}{\mathcal{K}}
\newcommand{\Gcal}{\mathcal{G}}
\newcommand{\Qcal}{\mathcal{Q}}
\newcommand{\Rcal}{\mathcal{R}}
\newcommand{\Scal}{\mathcal{S}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\Vcal}{\mathcal{V}}
\newcommand{\Zcal}{\mathcal{Z}}
\newcommand{\Ucal}{\mathcal{U}}
\newcommand{\Jcal}{\mathcal{J}}
\newcommand{\scal}{\mathcal{s}}

\newtheorem{theorem}{Theorem} \newtheorem{lemma}{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}{Corollary}

\DeclareMathOperator{\CC}{\mathbb{C}} \DeclareMathOperator{\EE}{\mathbb{E}} \DeclareMathOperator{\KK}{\mathbb{K}} \DeclareMathOperator{\MM}{\mathbb{M}} \DeclareMathOperator{\NN}{\mathbb{N}} \DeclareMathOperator{\PP}{\mathbb{P}} \DeclareMathOperator{\QQ}{\mathbb{Q}} \DeclareMathOperator{\RR}{\mathbb{R}} \DeclareMathOperator{\ZZ}{\mathbb{Z}} 

\ifx\assumption\undefined
\newtheorem{assumption}{Assumption}
\fi

\ifx\definition\undefined
\newtheorem{definition}{Definition}
\fi

\ifx\remark\undefined
\newtheorem{remark}{Remark}
\fi



\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}


 \newcommand{\hlightP}[1]{\ooalign{\hss\makebox[0pt]{\fcolorbox{red!30}{green!10}{}}\hss\cr\phantom{}}}
 
 \newcommand{\hlightC}[1]{\ooalign{\hss\makebox[0pt]{\fcolorbox{green!30}{red!40}{}}\hss\cr\phantom{}}}




%
\clearpage{}




\title{RaCT: Towards Amortized Ranking-Critical \\Training for Collaborative Filtering}




\author{Sam Lobel, ~~Chunyuan Li\thanks{Corresponding author~~ Equal Contribution}~, ~~\bf{Jianfeng Gao}, ~~Lawrence Carin \\ 
Brown University ~~~ Microsoft Research, Redmond ~~~   Duke University  \\
{\tt samuel\_lobel@brown.edu} ~~
{\tt chunyl@microsoft.com} 
}





\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
We investigate new methods for training collaborative filtering models based on actor-critic reinforcement learning, to more directly maximize ranking-based objective functions. Specifically, we train a {\it critic} network to approximate ranking-based metrics, and then update the {\it actor} network to directly optimize against the learned metrics. In contrast to traditional learning-to-rank methods that require re-running the optimization procedure for new lists, our critic-based method amortizes the scoring process with a neural network, and can directly provide the (approximate) ranking scores for new lists.
We demonstrate the actor-critic's ability to significantly improve the performance of a variety of prediction models, and achieve better or comparable performance to a variety of strong baselines on three large-scale datasets.





\end{abstract}



\section{Introduction}

Recommender systems are an important means of improving a user's web experience.
Collaborative filtering is a widely-applied technique in recommender systems~\citep{ricci2015recommender}, in which patterns across similar users and items are leveraged to predict user preferences~\citep{su2009survey}. This naturally fits within the learning paradigm of latent variable models (LVMs)~\citep{bishop2006pattern}, where latent representations capture the shared patterns. Due to their simplicity and effectiveness, LVMs are still a dominant approach.
Traditional LVMs employ linear mappings of limited modeling capacity~\citep{paterek2007improving,mnih2008probabilistic}, and a growing body of literature involves applying deep neural networks (DNNs) to collaborative filtering to create more expressive models~\citep{he2017neural,wu2016collaborative,liang2018variational}.
Among them, variational autoencoders (VAEs)~\citep{kingma2013auto,rezende2014stochastic} have been proposed as non-linear extensions of LVMs~\citep{liang2018variational}. Empirically, VAEs significantly outperform many competing LVM-based methods. One essential contribution to the improved performance is the use of the multinomial likelihood, which is argued by ~\citet{liang2018variational} to be a close proxy to ranking loss.



This property is desirable, because in recommender systems we generally care more about the ranking of predictions than an individual item's score.
Hence, prediction results are often evaluated using top- ranking-based metrics, such as Normalized Discounted Cumulative Gain (NDCG)~\citep{jarvelin2002cumulated}. The VAE is trained to maximize the likelihood of observations; as shown below, this does not necessarily result in higher ranking-based scores. 
A natural question concerns whether one may directly optimize against ranking-based metrics, which are by nature non-differentiable and piecewise-constant.
Previous work on learning-to-rank has been explored this question in the information-retrieval community, where relaxations/approximations of ranking loss are considered~\citep{weimer2008cofi,liu2009learning,li2014learning,weston2013learning}.



In this paper, we borrow the actor-critic idea from reinforcement learning (RL)~\citep{sutton1998reinforcement} to propose an efficient and scalable learning-to-rank algorithm. The critic is trained to approximate the ranking metric, while the actor is trained to optimize against this learned metric. 
Specifically, with the goal of making the actor-critic approach practical for recommender systems,
we introduce a novel feature-based critic architecture. Instead of treating raw predictions as the critic input, and hoping the neural network will discover the metric's structure from massive data, we consider engineering sufficient statistics for efficient critic learning.
Experimental results on three large-scale datasets demonstrate the actor-critic's ability to significantly improve the performance of a variety of latent-variable models, and achieve better or comparable performance to strong baseline methods.







\section{Background: VAEs for Collaborative Filtering}

Vectors are denoted as bold lower-case letters , matrices as bold uppercase letters , and scalars as lower-case non-bold letters . We use  for function composition,  for the element-wise multiplication, and  for cardinality of a set.  is the indicator function.

We use  to index users, and  to index items. The user-item interaction matrix  collected from the users' implicit feedback is defined as:

Note that  does not necessarily mean user  dislikes item ;
they may simply be unaware of the item.
Further,  is not equivalent to saying user  likes item , but that there is at least interest.





\paragraph{VAE model}~\hspace{-4mm}
VAEs have been investigated for collaborative filtering \citep{liang2018variational}, where this principled Bayesian approach is shown to achieve strong performance on large-scale datasets.
Given the user's interaction history , our goal is to predict the full interaction behavior with all remaining items. 
To simulate this process during training, a random binary mask  is introduced, with the entry  as {\it un-masked}, and  as {\it masked}. Thus,   is the user's partial interaction history. The goal becomes recovering the masked interactions: , which is equivalent to recovering the full  as  is known.

In LVMs, each user's binary interaction behavior is assumed to be controlled by a -dimensional user-dependent latent representation . When applying VAEs to collaborative filtering~\citep{liang2018variational}, the user's latent feature  is represented as a distribution , obtained from some partial history  of . With the assumption that  follows a Gaussian form, the {\em inference} of  for the corresponding  is performed as:

where  is the hyper-parameter of a Bernoulli distribution,  is a -parameterized neural network, which outputs the mean  and variance  of the Gaussian distribution. 



After obtaining a user's latent representation , we use the {\em generative} process to make predictions. In \citet{liang2018variational} a multinomial distribution is used to model the likelihood of items.
Specifically, to construct ,  is transformed to produce a probability distribution  over  items, from which the interaction vector  is assumed to
have been drawn:
\vspace{-0mm}

where  is a -parameterized neural network. 
The output  is normalized via a softmax function to produce a probability vector 
(an ()-simplex) over the entire item set.



\paragraph{Training Objective}~\hspace{-2mm}
Learning VAE parameters  yields the following generalized objective:
\vspace{-1mm}

where  is the {\it negative log likelihood} (NLL) term,  is the KL regularization term with standard normal prior , and  is a weighting hyper-parameter.
When , we can lower-bound the log marginal likelihood of the data using \eqref{eq_reg_elbo} as
.
This is commonly known as the {\it evidence lower bound} (ELBO) in variational inference~\citep{blei2017variational}. Thus \eqref{eq_reg_elbo} is the negative -regularized ELBO. To improve the optimization efficiency, the {\it reparametrization trick}~\citep{kingma2013auto,rezende2014stochastic} is used to draw samples  to obtain an unbiased estimate of the ELBO, which is further optimized via stochastic optimization.
We call this procedure {\it maximum likelihood estimate (MLE)}-based training, as it effectively maximizes the (regularized) ELBO. The testing stage of VAEs for collaborative filtering is detailed in Section~\ref{sec:testing_vae} of the Supplement.




















\paragraph{Advantages of VAEs}
The VAE framework successfully scales to relatively large datasets by making use of amortized inference~\citep{gershman2014amortized}: the prediction for all users share the same procedure, which effectively requires evaluating two functions -- the encoder  and the decoder . Crucially, as all users share the same encoder/decoder, the number of parameters required for an autoencoder is independent of the number of users. This is in contrast to some traditional latent factor collaborative filtering
models~\citep{paterek2007improving,hu2008collaborative,mnih2008probabilistic}, where a unique latent vector is learned for each user. The reuse of encoder/decoder for all users is well-aligned with collaborative filtering, where user preferences are analyzed by exploiting the similar patterns inferred from past experiences~\citep{liang2018variational}. VAEs has the two advantages {\em simultaneously}: expressive representation power as a non-linear model, and the number of parameters being independent of the number of users.



\begin{wrapfigure}{R}{0.52\textwidth}
\vspace{-4mm}
	\begin{tabular}{c}		
		\includegraphics[width=7.00cm]{figs/example_nll_dcg.pdf} \\
	\end{tabular}
	\vspace{-0mm}
	\caption{\small Difference between MLE-based training loss and ranking-based evaluation.
    For A, ; For B, .
	NLL assigns a better value to the misranked example than to the properly-ranked one. NDCG always assigns maximum value to properly-ranked scorings.
}
	\label{fig:example_div}
\end{wrapfigure}
\paragraph{Pitfalls of VAEs} Among various likelihood forms, it was argued in~\citet{liang2018variational} that multinomial likelihoods are a closer proxy to the ranking loss than the traditional Gaussian or logistic likelihoods.
Though simple and effective, the MLE procedure
may still diverge with the ultimate goal in recommendation of correctly suggesting the top-ranked items. To illustrate the divergence between MLE-based training and ranking-based evaluation, consider the example in Figure~\ref{fig:example_div}. For the target , two different predictions  and  are provided. In MLE, the training loss is the multinomial NLL: , where  is the predicted probability. From the NLL point of view,  is a better prediction than , because  shows a lower loss than . However,  ranks an incorrect item highest, and therefore would return a worse recommendation than . Fortunately, NDCG is calculated directly from the ranking, and so captures this dependence. This inspired us to directly use ranking-based evaluation metrics to guide training. For details on calculating NDCG, refer to Section~\ref{sec:evaluation_protocol} of the Supplement.

























\vspace{-3mm}
\section{Ranking-Critical Training}
\vspace{-3mm}

We introduce a novel algorithm for recommender system training, which we call Ranking-Critical Training (RaCT). RaCT learns a differentiable approximation to the ranking metric, which the prediction network then leverages as a target for optimization through gradient ascent. This is in contrast to existing methods in collaborative filtering, which define an objective relaxation ahead of time. This methodology of learning approximations to functions which cannot be optimized directly stems from the actor-critic paradigm of RL, which we adapt for collaborative filtering.

Any ranking-based evaluation metric can be considered as a ``black box'' function , which takes in the prediction  to compare with the ground-truth  (conditioned on the mask ), and outputs a scalar  to rate the prediction quality.
As in \eqref{eq_inference},  partitions a user's interactions into those that are ``observed'' and ``unobserved'' during inference.
As we are only interested in recovering the unobserved items in recommendation, we compute the ranking score of predicted items  based on the ground-truth items .

One salient component of a ranking-based Oracle metric  is to sort . The sorting operation is non-differentiable, rendering it impossible to directly use  as the critic.
While REINFORCE~\citep{williams1992simple} may appear to be suited to tackle the non-differentiable problem, it suffers from large estimate variance~\citep{silver2014deterministic}, especially in the collaborative filtering problem, which has a very large prediction space. This motivates consideration of a differentiable neural network to approximate the mapping executed by the Oracle.
In the actor-critic framework, the prediction network is called the \textit{actor}, and the network which approximates the oracle is called the \textit{critic}. The actor begins by making a prediction (action) given the user's interaction history as the state. The critic learns to estimate the value of each action, which we define as the task-specific reward, \ie the Oracle's output.
The value predicted by the critic is then used to train the actor.
Under the assumption that the critic produces the exact values, the actor is trained based on an unbiased estimate of the gradient of the prediction value in terms of relevant ranking quality metrics.
In Figure~\ref{fig:schemes}, we illustrate the actor-critic paradigm in (b), and the traditional auto-encoder shown in (a) can be used as the actor in our paradigm.



















\begin{figure*}[t!]\vspace{-0mm}\centering
	\begin{tabular}{c c}		
		\hspace{-0mm}
		\includegraphics[height=2.2cm]{figs/ae_scheme.pdf} &
		\hspace{0mm}
		\includegraphics[height=2.2cm]{figs/rct_scheme.pdf} \\
		(a) Traditional auto-encoder paradigm \vspace{-0mm}   & 
		(b) Proposed actor-critic paradigm \hspace{-0mm} 
	\end{tabular}
	\vspace{-1mm}
	\caption{Illustration of learning parameters  in the two different paradigms. (a) Learning with MLE, as in VAEs; (b) Learning with a learned ranking-critic. The {\it actor} can be viewed as the function composition of encoder  and  in VAEs. The {\it critic} mimics the ranking-based evaluation scores, so that it can provide ranking-sensitive feedback in the actor learning.}
	\vspace{-5mm}
	\label{fig:schemes}
\end{figure*}



\paragraph{Naive critic}
Conventionally one may concatenate vectors  as input to a neural network, and train a network to output the measured ranking scores .
However, this naive critic is impractical, and failed in our experiments. Our hypothesis is that since this network architecture has a huge number of parameters to train (as the input data layer is of length , where ), it would require rich data for training. Unfortunately, this is impractical:  are very high-dimensional, and the implicit feedback used in collaborative filtering is naturally sparse.

\vspace{-0mm}
\paragraph{Feature-based critic}
The naive critic hopes a deep network can discover structure from massive data by itself, leaving much valuable domain knowledge unused.
We propose a more efficient critic, that takes into account the structure underlined by the assumed likelihood in MLE~\citep{miyato2018cgans}. We describe our intuition and method below, and provide the justification from the perspective of adversarial learning in Section~\ref{sec:gan} of the Supplement. 



Consider the computation procedure of the evaluation metric as a function decomposition , including two steps:


\vspace{-2mm}
\begin{itemize} 
		\item
	    , feature engineering of prediction  into the {\it sufficient statistics}  ; 
		\item 
		, neural approximation of the mapping from the statistics  to the estimated ranking score , using a -parameterized neural network.
\end{itemize}

The success of this two-step critic largely depends on the effectiveness of the feature . We hope feature  is   {\it compact} so that fewer parameters in the critic  can simplify training;  {\it easy-to-compute} so that training and testing is efficient; and   {\it informative} so that the necessary information is preserved. 
We suggest to use a 3-dimensional vector as the feature, and leave more complicated feature engineering as future work. In summary, our feature is 

where
   is the negative log-likelihood in~\eqref{eq_reg_elbo}, defined in the MLE training loss.
   is the number of unobserved items that a user will interact, with . 
   is the number of observed items that a user has interacted, with . 


The NLL characterizes the prediction quality of the actor's output  against the ground-truth  in an item-to-item comparison manner, \eg the inner product between two vectors  as in the multinomial NLL~\citep{liang2018variational}. Ranking is made easier when there are many acceptable items to rank highly (e.g. when  is large), and made difficult when predicting from very few interactions (e.g. when  is small), motivating these two features. Including these three features allows the critic to guide training by weighting the NLL's relation to ranking given this context about the user. Interestingly, this idea to consider the importance of user behavior statistics coincides with the scaling trick in SVD~\citep{nikolakopoulos2019eigenrec}.







Note that  and  are user-specific, indicating the user's frequency to interact with the system, which can be viewed as side-information about the user. They are only used as features in training the critic to better approximate the ranking scores, and not in training the actor. Hence, we do not use additional information in the testing stage.

\paragraph{Actor Pre-training} In order to be a helpful feature for the critic, the NLL must hold some relationship to the ranking-based objective function. But for the high-dimensional datasets common to collaborative filtering, the ranking score is near-uniformly zero for a randomly-initialized actor. In this situation, a trained critic will not propagate derivatives to the actor, and therefore the actor will not improve. We mitigate this problem by using a pre-trained actor, such as VAEs that have been trained via MLE.

\paragraph{Critic Pre-training}
Training a generic critic to approximate the ranking scores for all possible predictions is difficult and cumbersome. Furthermore, it is unnecessary. 
In practice, a critic only needs to estimate the ranking scores on the restricted domain of the current actor's outputs. Therefore, we train the critic offline on top of the pre-trained MLE-based actor.
To train the critic, we minimize the Mean Square Error (MSE) between the critic output and true ranking score  from the Oracle:

where the target  is generated using its non-differential definition, which plays the role of ground truth simulator in training.

\paragraph{Actor-critic Training} 
Once the critic is well trained, we fix its parameters  and update the actor parameters  to maximize the estimated ranking score

where  is defined in~\eqref{eq_features},
including NLL feature extracted from the prediction made in~\eqref{eq_reg_elbo}, together with count features. 
During back-propagation, the gradient of  wrt the prediction  is
 It further updates the actor parameters, with the encoder gradient 

and the decoder gradient 
.
Updating the actor changes its predictions, so we must update the critic to produce the correct ranking scores for its new input domain.

The full RaCT training procedure is summarized in Algorithm~1 in the Supplement.
Stochastic optimization is used, where a batch of users 
  is drawn at each iteration, with  as a random subset of user index in . The pre-training of the actor in Stage 1 and the critic in Stage 2 are important; they provide good initialization to the actor-critic training in Stage 3 for fast convergence. Further, we provide an alternative interpretation to view our actor-critic approach in \eqref{eq_critic} and \eqref{eq_actor} from the perspective of adversarial learning~\citep{goodfellow2014generative} in the Supplement. This can partially justify our choice of feature engineering.










\section{Related Work}

{\bf Deep Learning for Collaborative Filtering}
There are many recent efforts focused on developing deep learning models for collaborative filtering~\citep{sedhain2015autorec,xue2017deep,he2018outer,he2018adversarial,zhang2017deep,chen2017attentive}. 
Early work on DNNs focused on explicit feedback settings~\citep{georgiev2013non,salakhutdinov2007restricted,zheng2016neural}, such as rating predictions. 
Recent research gradually recognized the importance of implicit feedback~\citep{wu2016collaborative,he2017neural,liang2018variational}, where the user's preference is not explicitly presented~\citep{hu2008collaborative}. This setting is more practical but challenging, and is the focus of our work.
The proposed actor-critic method belongs to the general two-level architectures for recommendation systems, where a coarse to fine prediction procedure is used. For a systematic method comparison for top-N recommendation tasks, we suggest referring to~\citet{dacrema2019we}.
Our method is closely related to three papers, on VAEs~\citep{liang2018variational}, collaborative denoising autoencoder (CDAE)~\citep{wu2016collaborative} and neural collaborative filtering (NCF)~\citep{he2017neural}.
CDAE and NCF may suffer from scalability issues: the model size grows linearly with both the number of users as well as items. The VAE~\citep{liang2018variational} alleviates this problem via amortized inference. 
Our work builds on top of the VAE, and improves it by optimizing to the ranking-based metric.




{\bf Learned Metrics in Vision \& Languages } Recent research in computer vision and natural language processing has generated excellent results, using learned instead of hand-crafted metrics. Among the rich literature of generating realistic images via generative adversarial networks (GANs)~\citep{goodfellow2014generative,radford2015unsupervised,karras2017progressive}, our work is most similar to ~\citet{larsen2016autoencoding}, where the VAE objective~\citep{kingma2013auto} is augmented with the learned representations in the GAN discriminator~\citep{goodfellow2014generative} to better measure image similarities. 
For language generation, the discrepancy between word-level MLE training and sequence-level semantic evaluation has been alleviated with GANs or RL techniques~\citep{bahdanau2016actor,ren2017deep,lin2017adversarial}. The RL approach directly optimizes the metric used at test time, 
and has shown improvement on various applications, including dialogue~\citep{li2016deep}, image captioning~\citep{rennie2017self} and translations~\citep{ranzato2015sequence}.
Despite the significant successes in other domains, there has been little if any research reported for directly learning the metrics with deep neural networks for collaborative filtering. Our work fills the gap, and we hope it inspires more research in this direction.

{\bf Learning to Rank (L2R)} The idea of L2R has existed for two decades in the information-retrieval community. The goal is to maximize a given ranking-based evaluation metric~\citep{liu2009learning,li2014learning}, generally through optimizing objective relaxations~\citep{weimer2008cofi}. 
Many L2R methods used in recommendation, such as the popular pairwise L2R methods BPR ~\citep{rendle2009bpr} and WARP~\citep{weston2011wsabie}, are trained by optimizing a pairwise classification function that penalizes mis-ranked pairs of items. Through negative sampling~\citep{hu2008collaborative}, these methods can scale to extremely high-dimensional output spaces. However, it is computationally expensive to compute low-variance updates to a model when the number of items is large.

An alternative to the pairwise approach is \textit{listwise} loss functions, which minimize a loss calculated from a user's entire interaction history. By considering the entire interaction history these methods can more closely model ranking, and generally perform better than their pairwise counterparts~\citep{xia2008listmle}.
Furthermore, compared to methods which calculate relative ranking for each pair ~\citep{weston2011wsabie}, the per-user amortization of rank-calculation can be computed more efficiently.
NLL is an example of a listwise loss function, as it is calculated over a user's entire interaction history. Interestingly, NLL is also used as the loss function for ListNet~\citep{cao2007listnet}, a classic listwise L2R method designed to probabilistically maximize Top-1 Recall. The VAE framework under NLL can be seen as a principled extension of this method to Top-N collaborative filtering. Our ranking-critical training further extends this methodology by explicitly calculating the relationship between a differentiable listwise loss function and the desired ranking-based evaluation function.









\vspace{-2mm}
\section{Experiments}
\vspace{-2mm}
\paragraph{Experimental Settings} 
We implemented our algorithm in TensorFlow. The source code to reproduce the experimental results and plots is included as Supplementary Material. We conduct experiments on three publicly available
large-scale datasets, which represent different item recommendation scenarios, including user-movie ratings and user-song play counts. This is the same set of user-item consumption datasets used in~\citet{liang2018variational}, and we keep the same pre-processing steps for fair comparison. 
The statistics of the datasets, evaluation protocols and hyper-parameters are summarized in the Supplement. 
VAE~\citep{liang2018variational} is used as the baseline, which plays the role of our actor pre-training. The NCDG@100 ranking metric is used as the critic's target in training. 



























{\bf Baseline Methods}~~~
We use ranking-critical training to improve the three MLE-based methods described in Section 2.1: VAE, DAE, and MF. We also adapt traditional L2R methods as the actors in our framework, where the L2R loss is used to replace  in \eqref{eq_features} to construct the feature. We consider WARP and LambdaRank, two pairwise loss functions designed for optimizing NDCG, for these experiments. We also compare our approaches with four representative baseline methods in collaborative filtering. CDAE~\citep{wu2016collaborative} is a strongly-performing neural-network based method, weighted MF~\citep{hu2008collaborative} is a linear latent-factor model, and SLIM~\citep{ning2011slim} and EASE~\citep{steck2019ease} are item-to-item similarity models. We additionally compare with Bayesian Pairwise Ranking~\citep{rendle2009bpr}, but as this method did not yield competitive performance on these datasets, we omit the results.







\begin{figure*}[t!]\vspace{-0mm}\centering
	\begin{tabular}{c c c}		
		\hspace{-2mm}
		\includegraphics[height=3.4cm]{figs/improvement_ndcg/improve_ndcg_ml-20m.pdf} &
		\hspace{-4mm}
		\includegraphics[height=3.4cm]{figs/improvement_ndcg/improve_ndcg_netflix.pdf} &
		\hspace{-4mm}
		\includegraphics[height=3.4cm]{figs/improvement_ndcg/improve_ndcg_msd.pdf}	
		\vspace{-2mm}
		\\
		(a) ML-20M dataset \vspace{-0mm}   & 
		(b) Netflix dataset \hspace{-0mm} &
		(c) MSD dataset\hspace{-0mm} \\ 
	\end{tabular}
	\vspace{-2mm}
	\caption{Performance improvement (NDCG@100) with RaCT over the VAE baseline.}
\vspace{-4mm}
	\label{fig:improvement}
\end{figure*}


\subsection{Overall Performance of RaCT}
\vspace{-2mm}


\paragraph{Improvement over VAE} 
In Figure~\ref{fig:improvement}, we show the learning curves of RaCT and VAE on the validation set. The VAE converges to a plateau by the time that the RaCT finishes its actor pre-training stage, \eg 150 epochs on ML-20 dataset, after which the VAE's performance is not improving. By contrast, when the RaCT is plugged in, the performance shows a significant immediate boost. For the amount of improvement gain, RaCT takes only half the number of epochs that VAE takes in the end of actor pre-training. For example, RaCT takes 50 epochs (from 150 to 200) to achieve an improvement of 0.44-0.43 = 0.01, while VAE takes 100 epochs (from 50 to 150) to achieve an improvement of 0.43-0.424 = 0.006.


\begin{wrapfigure}{R}{0.55\textwidth}
\vspace{-0mm}
	\centering
	\begin{tabular}{c c}
		\hspace{-4mm}
		\includegraphics[height=3.5cm]{figs/correlation_objective/correlation_scatter_training_softmax.png} &
		\hspace{-7mm}
		\includegraphics[height=3.5cm]{figs/correlation_objective/correlation_scatter_training_ract.png}
		\\
		(a) MLE\vspace{-0mm}   & 
		(b) RaCT \hspace{-0mm} \\ 	
	\end{tabular}
	\vspace{-2mm}
	\caption{Correlation between the learning objectives (MLE or RaCT) and evaluation metrics on training.}
\vspace{-2mm}
	\label{fig:correlation}
\end{wrapfigure}

\paragraph{Training/Evaluation Correlation} We visualize scatter plots between learning objectives and evaluation metric for all users on ML-20M dataset in Figure~\ref{fig:correlation}. More details and an enlarged visualization is shown in Figure~\ref{fig:correlation_supp} of the Supplement.
The Pearson's correlation  is computed. NLL exhibits low correlation with the target NDCG ( is close to zero), while the learned metric in RaCT shows much higher positive correlation. It strongly indicates RaCT optimizes a more direct objective than an MLE approach. Further, NLL should in theory have a negative correlation with the target NDCG, as we wish that minimizing NLL can maximize NDCG. However, in practice it yields positive correlation. We hypothesize that this is because the number of interactions for each user may dominate the NLL values. That partially motivates us to consider the number of user interactions as features.







\begin{table*}[t!]
\vspace{-2mm}
  \caption{ Comparison on three large datasets. The best testing set performance is reported. The results below the line are from~\citet{liang2018variational}, and VAE shows the VAE results based on our runs. {\color{blue} Blue} indicates improvement over the VAE baseline, and {\bf bold} indicates overall best. }
  \label{tab:compare_sota}
\begin{adjustbox}{scale=.80,tabular=c|ccc|ccc|ccc}
    \toprule
    Dataset &
        \multicolumn{3}{ c|}{ML-20M}  & 
        \multicolumn{3}{ c|}{Netflix}  & 
        \multicolumn{3}{ c }{MSD}  \\ \hline
    Metric 
         & R@20 & R@50 & NDCG@100 
         & R@20 & R@50 & NDCG@100 
         & R@20 & R@50 & NDCG@100  \\
    \midrule       
    RaCT         
         & \textbf{\color{blue}  0.403} & \textbf{\color{blue}  0.543} & \textbf{\color{blue} 0.434}
         & {\color{blue}0.357} & \textbf{\color{blue}  0.450} & {\color{blue}0.392}
         & {\color{blue} 0.268} & {\color{blue} 0.364} & {\color{blue} 0.319}  \\
    VAE
         & 0.396 & 0.536 & 0.426 
         & 0.350 & 0.443 & 0.385 
         & 0.260 & 0.356 & 0.310  \\   \hline
WARP& 0.310 & 0.448	& 0.348
        & 0.273	& 0.360	& 0.312
        & 0.162	& 0.253	& 0.210 \\ 
    LambdaRank& 0.395	& 0.534	& 0.427
        & 0.352	& 0.441	& 0.386
        & 0.259	& 0.355	& 0.308  \\ 
        \hline
    EASE& 0.391 & 0.521 & 0.420
        & 0.\textbf{362} & 0.445 & \textbf{0.393}
        & \textbf{0.333} & \textbf{0.428} & \textbf{0.389} \\
    VAE& 0.395 & 0.537 & 0.426 
         & 0.351 & 0.444 & 0.386 
         & 0.266 & 0.364 & 0.316  \\ 
    CDAE& 0.391 & 0.523 & 0.418
         & 0.343 & 0.428 & 0.376
         & 0.188 & 0.283 & 0.237 \\
    WMF& 0.360 & 0.498 & 0.386
         & 0.316 & 0.404 & 0.351
         & 0.211 & 0.312 & 0.257 \\
    SLIM& 0.370 & 0.495 & 0.401   
         & 0.347 & 0.428 & 0.379
         & --    & --    & --     \\
  \bottomrule
\end{adjustbox}
  \vspace{-3mm}
\end{table*}


{\bf Comparison with traditional L2R methods}
As examples of traditional L2R methods, we compare to our method using WARP~\citep{weston2011wsabie} and LambdaRank~\citep{burges2007learning} as the ranking-critical objectives. We use implementations of both methods designed specifically to maximize NDCG. We observe that WARP and LambdaRank are roughly 2 and 10 times more computationally expensive than RaCT per epoch, respectively. Table~\ref{tab:compare_sota} shows the results of RaCT, WARP and LambdaRank, using the same amount of wall-clock training time. We observe the trends that WARP degrades performance, and LambdaRank provides performance roughly equal to VAE. WARP's poor performance is perhaps due to poor approximation of the ranking when the number of items is large.









{\bf Comparison with existing methods}
In Table~\ref{tab:compare_sota}, we report our RaCT performance, and compare with competing methods in terms of three evaluation metrics: NDCG@100, Recall@20, and Recall@50. 
We use the published code\footnote{\url{https://github.com/dawenl/vae_cf}} of~\citet{liang2018variational}, and reproduce the VAE as our actor pre-training. We further use their reported values for the classic collaborative filtering methods CDAE, WMF, and SLIM.
Our reproduced VAE results are very close to~\citet{liang2018variational} on the ML-20M and Netflix datasets, but slightly lower on the MSD dataset. The RaCT is built on top of our VAE runs, and consistently improves its baseline actor for all the evaluation metrics and datasets, as seen by comparing the rows RaCT and VAE.
The proposed RaCT also significantly outperforms competing LVMs, including VAE, CDAE, and WMF.

When comparing to EASE~\citep{steck2019ease}, our method performs substantially better for ML-20M, comparably for Netflix, and is substantially outperformed for MSD. We observe a similar trend when comparing SLIM (an item-to-item similarity method) and CDAE (a latent variable method).
As SLIM and EASE rely on recreating the Gram-matrix , their performance should improve with the the number of users~\citep{steck2019ease}. However this performance may come at a computational cost, as inference requires multiplication with an unfactored  matrix. EASE requires computing a dense item-to-item similarity matrix, making its inference on MSD roughly 30 times more expensive than for VAE or RaCT. A practitioner's choice between these two methods should be informed by the specifics of the dataset as well as demands of the system.








In the Supplement, we study the generalization of RaCT trained with different ranking-metrics in Section~\ref{sec_metrics_supp}, and break down the performance improvement with different cut-off values of NDCG in Section~\ref{sec_cut_off_supp}, and with different number of interactions of  in Section~\ref{sec_interactions_supp}.










\begin{table}[t!]
	\begin{minipage}{0.53\linewidth}
    \centering
\begin{adjustbox}{scale=.93,tabular=l|ccc}
        \toprule
Actor  & Before & After & Gain \\
        \midrule      
        VAE& 0.4258 & 0.4339 & 8.09  \\ 
        VAE (Gaussian) 
             & 0.4202 & 0.4224 & 2.21  \\
        VAE  ()        
             & 0.4203 & 0.4255  & 5.17   \\
        VAE (Linear)
             & 0.4156  & 0.4162  &  0.53  \\ \hline
        DAE~\citep{liang2018variational}       
             & 0.4205 & 0.4214 & 0.87  \\
        MF~\citep{liang2018variational}     
             & 0.4159 & 0.4172 & 1.37  \\ \hline
        WARP& 0.3123 & 0.3439  & 31.63   \\ 
      \bottomrule
    \end{adjustbox}
    \vspace{1mm}
      \caption{\small Performance gain () for various actors.}
      \vspace{-0mm}
      \label{tab:compare_actors}
	\end{minipage}\hfill
	\begin{minipage}{0.45\linewidth}
		\vspace{-2mm}
		\centering
    	\begin{tabular}{c}
    	    \hspace{-5mm}
    		\includegraphics[height=3.50cm]{figs/plot_feature_ablation.pdf} \\
    	\end{tabular}
    	\vspace{-1mm}
    	\captionof{figure}{\small Ablation study on features.}
    	\vspace{-0mm}
    	\label{fig:feature_ablation}
	\end{minipage}
\vspace{-4mm}

\end{table}
\subsection{What Actor Can Be Improved by RaCT?}
In RL, the choice of policy plays a crucial role in the agent's performance. Similarly, we would like to study how different actor designs impact RaCT performance. Table~\ref{tab:compare_actors} shows the performance of various policies before and after applying RaCT. The results on NDCG@100 are reported. The VAE, DAE and MF models follow the setup in~\citet{liang2018variational}.


We modify one component of the VAE at a time, and check the change of performance improvement that RaCT can provide. 
(1) VAE (Gaussian): we change likelihood form from multinomial to Gaussian, and observe a smaller performance improvement. This shows the importance of having a closer proxy of ranking-based loss.
(2) VAE (): we remove the KL regularization by setting , and replace the posterior sampling with a delta distribution. We see a marginally smaller performance improvement. This compares a stochastic and deterministic policy.  The stochastic policy (\ie posterior sampling) provides higher exploration ability for the actor, allowing more diverse samples generated for the critic's training. This is essential for better critic learning. 
(3) VAE (Linear): we limit the expressive ability of the actor by using a linear encoder and decoder. This significantly degrades performance, and the RaCT cannot help much in this case. RaCT shows improvements for all MLE-based methods, including DAE and MF from ~\citet{liang2018variational}. It also shows significant improvement over WARP. 
Please see detailed discussion in Section \ref{sec:actors_supp} of the Supplement.
\subsection{Ablation Study on Feature-based Critic} 
In Figure~\ref{fig:feature_ablation}, we investigate the importance of the features we designed in~\eqref{eq_features}, using results from the ML-20M dataset.
The full feature vector consists of three elements: 
. 
 is mandatory, because it links the actor to the critic; removing it would break the back-propagation to train the actor. 
We carefully remove 
 or  from  at each time, and observe that it leads to performance degradation. In particular, removing  results in a severe over-fitting issue. 
When both counts are removed, we observe an immediate performance drop, as depicted by the orange curve. Overall, the results indicate that all three features are necessary to our performance improvement.


\section{Conclusion \& Discussion} We have proposed an actor-critic framework for collaborative filtering on implicit data. The critic learns to approximate the ranking scores, which in turn improves the traditional MLE-based nonlinear LVMs with the learned ranking-critical objectives.
To make it practical and efficient, we introduce a few techniques: a feature-based critic to reduce the number of learnable parameters, posterior sampling as exploration for better critic estimates, and pre-training of actor and critic for fast convergence.
The experimental results on three large-scale datasets demonstrate the actor-critic's ability to significantly improve the results of a variety of latent-variable models, and achieve better or comparable performance to strong baseline methods.


Though RaCT improves VAEs, it does not start from the best performing actor model. The very recent work by~\citet{dacrema2019we} conducts a systematic analysis of algorithmic proposals for top-N recommendation tasks. There are other simple and efficient methods that perform better than VAEs, such as pure SVD-based models~\citep{cremonesi2010performance,nikolakopoulos2019eigenrec}, RecWalk~\citep{nikolakopoulos2019recwalk} and Personalized Diffusions~\citep{nikolakopoulos2019personalized}.
One interesting future research direction is to explore learning-to-rank techniques for them.






\medskip

\small

\bibliographystyle{iclr2020_conference}
\bibliography{neurips_2019}


\normalsize

\appendix
\newpage
\title{\Large Appendix: \\
RaCT}

\paragraph{Summary of contributions: }  Sam and Chunyuan conceptualized learning-to-rank for VAEs. Sam created and implemented the current algorithm, made the model work, and ran all experiments. Chunyuan set up the experiments, led and completed the manuscript writing. Lawrence edited every version of the manuscript. Jianfeng proofread an early version of the manuscript. 

\section{Testing stage of VAEs for Collaborative Filtering}~\label{sec:testing_vae}
We focus on studying the performance of various models under strong generalization~\citep{liang2015content} as in~\citep{liang2018variational}. All users are split into training/validation/test sets. The models are learned using the entire interaction history of the users in the training set. To evaluate, we use a part of the interaction history from held-out (validation
and test) users to infer the user-level representations from the model, and compute quality metrics by quantifying how well the
model ranks the rest of the unseen interaction history from the held-out users.
Specifically, for a held-out user with the full history , we take  offline using the randomly generated mask .   is then frozen as the testing input, and is fed into various trained models during the evaluation stage to get the prediction . The recovered  interaction  for the masked seen part is then evaluated by ranking-based metrics.

\section{Background on Traditional Learning-to-Rank Methods}~\label{sec:two_l2r}
Formally, the {\it Bayesian Personalized Ranking} (BPR)~\citep{rendle2009bpr} loss for the -th user is

where  is the sigmoid function,  denotes the set of items that the user has interacted with before, and  denotes the complement item set.


The {\it Weighted Approximate-Rank Pairwise} (WARP) model~\citep{weston2011wsabie} has been shown to perform better than BPR for implicit feedback~\citep{kula2015metadata}:

where  is a weighting function for different ranks, and  is the rank for the -th item for -th user. A common choice of weighting function  for optimizing NDCG is
. WARP improves BPR by the weights  and the margin between positive and negative items. 



\section{Pseudo-code for RaCT}~\label{sec:code}
We summarize the full training procedure of RaCT in Algorithm 1. \begin{algorithm}[h!]
	\SetKwInOut{Input}{Input}
	\caption{Our full ranking-critical training with stochastic optimization.}
	\Input{
		{\small Interaction matrix ; 
		Actor parameters (encoder  and decoder ), Critic parameters }. }
{\bf Initialize}: Randomly initialize weights  ,  and  \\
	{\small \color{blue}  \tcc{Stage~1: Pretrain~~the~~actor~~via~~MLE} }
\While{not converged do}{
	 Sample a batch of users ; \\
	 Update  with gradient
	 
     and 
     
	 in~\eqref{eq_reg_elbo};
	 } 

\vspace{2mm}
	{\color{blue} \small  \tcc{Stage~2: Pretrain~the~critic~via~MSE} }	

	 \While{not converged do}{
	 Sample a batch of users ; \\
	 Construct features  in~\eqref{eq_features} and target  from the Oracle;\\ 
	 Update  with gradient 
	 
	 in~\eqref{eq_critic};
	 }

\vspace{2mm}
	{\small \color{blue} \tcc{Stage 3: Alternative~training~of~actor~and~critic}} 	
	\For {} {
	    Sample a batch of users ; \\
		{\small \color{blue} \tcc{~Actor~~step} } 
	 Update  with gradient
	 
     and 
     
	 in~\eqref{eq_actor};
		
		{\small \color{blue}  \tcc{~Critic~~step} } 
     Construct features  in~\eqref{eq_features} and target  from the Oracle;\\ 
	 Update  with gradient 
	 
	 in~\eqref{eq_critic};

	}
	\label{alg:actor_critic}
\end{algorithm}







\section{Interpretation with GANs}~\label{sec:gan}
We can view our actor-critic approach in \eqref{eq_critic} and \eqref{eq_actor} from the perspective of Generative Adversarial Networks (GANs). GANs constitute a framework to construct a {\it generator}  that can mimic a target distribution, and have achieved significant success in generating realistic images~\citep{goodfellow2014generative,radford2015unsupervised,karras2017progressive,brock2018large}. The most distinctive feature
of GANs is the {\it discriminator}  that evaluates the divergence between the current generator distribution and the target distribution~\citep{goodfellow2014generative,li2017alice}. The GAN learning procedure performs iterative training between the discriminator and generator, with the discriminator acting as an increasingly meticulous critic to refine the generator. In our work, the actor can be interpreted as the generator, while the critic can be viewed as the discriminator. 

Note that GANs and actor-critic models learn the metric functions~\citep{finn2016connection}, and it has been shown in~\citet{pfau2016connecting} that GANs can be viewed as actor-critic in an environment where the actor cannot affect the reward. This is exactly our setup. 
One key difference is that we know the Oracle metric, and the critic is trained to mimic the Oracle's behaviour. 


Conditioned on interaction history  corrupted from , the actor predicts the distribution parameter  over items, which further constructs the likelihood . We use  to designate the data empirical distribution, the target conditional is . It can be formulated as the standard adversarial loss for the conditional GAN~\citep{mirza2014conditional}. 
It has been shown that the optimal critic~\citep{goodfellow2014generative,li2017alice} for a conditional GAN can be represented as the log likelihood ratio

In the collaborative filtering setup, we often make the assumptions that  are simple distributions, such as multinomial in VAEs~\citep{liang2018variational} and Gaussian in MF. This simplification allows the parameterization of critic following the following form~\citep{miyato2018cgans}:

where  is the target,  is a layer of the critic with input , and  and  are the parameters to learn.  Most notably, this formulation introduces the prediction information via an inner product, as opposed to concatenation. The form~\eqref{eq:log_ratio_ip} is indeed the form we proposed for NLL feature , with  and .  includes the normalizer for the prediction probability~\citep{miyato2018cgans}, which is related to the count features in~\eqref{eq_features}.


\section{Experimental setup} 

\subsection{Datasets}
We conduct experiments on three publicly available
datasets. Table~\ref{tab:dataset} summarizes the statistics of the data. These three ten-million-size datasets represent different item recommendation scenarios, including user-movie ratings and user-song play counts. This is the same set of medium- to large-scale user-item consumption datasets used in~\citet{liang2018variational}, and we keep the same pre-processing steps for fair comparison. 

\begin{enumerate}
\item {\bf MovieLens-20M (ML-20M)}: This is the user-movie rating data collected from a movie recommendation service\footnote{\url{https://grouplens.org/datasets/movielens/20m/}}. The data is binarized by keeping ratings of four or higher and setting other entries as unobserved. Only users who have watched at least
five movies are considered.
\item {\bf Netflix Prize (Netflix)}: This is the user-movie rating data from the Netflix Prize\footnote{\url{https://www.netflixprize.com/}}. Similarly to ML-20M, the data is binarized by keeping ratings of four or higher, and only users who have
watched at least five movies are kept.
\item {\bf Million Song Dataset (MSD)}: This is the user-song play count data from the Million Song Dataset~\citep{bertin2011million}. We
binarize play counts, and keep users who have listened to at least 20 songs as well as songs that are listened to by at least 200 users.
\end{enumerate}


\subsection{Evaluation Protocol}~\label{sec:evaluation_protocol}
In the testing stage, we get the predicted ranking by sorting the multinomial probability . For each user, we compare the predicted ranking of the held-out items with their true ranking. 
Two ranking-based metrics are considered, Recall@R and the truncated NDCG (NDCG@R), where  is the cut-off hyper-parameter.
While Recall@R considers all items ranked within the first  to be equally important, NDCG@R uses a monotonically increasing discount to emphasize the importance of higher ranks versus lower ones. 

Formally, we define  as the item at rank , and  as the held-out unobserved items that a user will interact. 

By dividing DCG@R by its best possible value, we obtain NDCG@R in .

The denominator normalizes Recall@R in , with maximum value  corresponding to the case that all relevant items are ranked in the top  positions.




\begin{table}[t!]
  \caption{Summary Statistics of datasets after all pre-processing steps. {\it Interactions} is the number of non-zero entries.  {\it Sparsity} refers to the percentage of zero entries in the user-item interaction matrix . {\it Items} is the number of total items. {\it HO} is the number of validation/test users held out of the total number of users in the 5th column {\it Users}.}
  \label{tab:dataset}
  \centering
  \begin{tabular}{c|c|c|cc|c}
    \toprule
    Dataset & 
    Interaction\#  & \hspace{-0mm}Sparsity\% &  Item\#  &  User\#  
    &  \hspace{-1mm}  HO\# \\
    \midrule       
    ML-20M &  10.0M & 99.64\% & 20,108 & 136,677 & 10K \\
   Netflix &  56.9M & 99.31\% & 17,769 & 463,435 & 40K \\
    MSD    &  33.6M & 99.86\% & 41,140 & 571,355 & 50K \\
  \bottomrule
\end{tabular}
\end{table}
\begin{table}[t!]
\caption{Network architectures. The arrow indicates the flow between two layers. For each layer, we show the number of units on top of its following activation function.  indicates Batch Normalization.}
\label{tab:network}
\centering
\begin{tabular}{c|c|c}
\toprule
\multicolumn{2}{ c|}{Networks}  & Architectures   \\ \hline
\multirow{2}{*}{Actor} 
& Encoder & 
   \\ \cline{2-3}
& Decoder &    \\ \hline
\multicolumn{2}{ c|}{Critic}  & 

\\ 
\bottomrule
\end{tabular}
\end{table}


\subsection{Experiment Hyper-parameters}
We set hyper-parameters by following~\citet{liang2018variational} for comparisons. For VAE, the dimension of the latent representation is 200. 
When KL regularization is removed (), \ie for DAE and MF, we instead apply  regularization () on weights to prevent overfitting.
Adam optimizer~\citep{kingma2014adam} is used, with batch size of  users. 
For ML-20M, the actor is pre-trained for 150 epochs, and alternative training for 50 epochs. On the other two datasets, the actor is pre-trained for 75 epochs, and alternative training for 25 epochs. The critic is pre-trained for 50 epochs for all three datasets. The alternative training has equal update frequency for actor and critic.
This schedule ensures that we the have the same total number of actor training epochs as ~\citet{liang2018variational}: 200 epochs for ML-20M, 100 epochs for the other two datasets.


A fully-connected (FC) architecture is used for all networks, as detailed in Table~\ref{tab:network}. Please refer to~\citet{goodfellow2016deep} for the activation functions. Batch Normalization~\citep{ioffe2015batch} is used to normalize the input features, 
because the magnitude of the inputs (NLL) change as training progresses. 
The encoder outputs the mean and variance of the varational distribution; the variance is implemented via an exponential function.

\begin{table}[t!]
  \caption{Summary of training schedule hyper-parameters.  indicates the maximum value of . In the actor pre-training stage, the number of epochs used for increasing and fixing  are shown in row 3 and 4, respectively. }
    \centering
  \label{tab:beta}
  \begin{tabular}{c|c|c|c}
    \toprule
    Dataset & ML-20M & Netflix & MSD \\ 
    \midrule  
      &  0.2 &  0.2  &  0.1 \\
    \# epochs for annealing & 100 & 75 & 75 \\
     \# epochs for fixing & 50 & 0  & 0\\ \midrule  
    \# epochs for actor pre-training & 150 & 75 & 75 \\
    \# epochs for critic pre-training & 50 & 50 & 50 \\
    \# epochs for alternative training & 50 & 25 & 25 \\
  \bottomrule
\end{tabular}
\end{table}

\section{Additional Experimental Results}


\begin{figure}[t!]\vspace{-0mm}\centering
	\begin{tabular}{c c}		
		\hspace{-4mm}
		\includegraphics[height=5.0cm]{figs/correlation_objective/correlation_scatter_training_softmax.png} &
		\hspace{-5mm}
		\includegraphics[height=5.0cm]{figs/correlation_objective/correlation_scatter_training_ract.png} \\
		(a) Training NLL\vspace{-0mm}   & 
		(b) Training NDCG \hspace{-0mm}  \\ 		
		\hspace{-5mm}
		\includegraphics[height=5.0cm]{figs/correlation_objective/correlation_scatter_testing_softmax.png} &
		\hspace{-5mm}
		\includegraphics[height=5.0cm]{figs/correlation_objective/correlation_scatter_testing_ract.png}			
		\\
		(c) Testing NLL \vspace{-0mm}   & 
		(d) Testing NDCG\hspace{-0mm} \\ 	
	\end{tabular}
	\vspace{-2mm}
	\caption{Correlation between the learning objectives (NLL or RaCT) and evaluation metrics NDCG.}
	\vspace{-2mm}
	\label{fig:correlation_supp}
\end{figure}

\subsection{Generalization across ranking metrics} ~\label{sec_metrics_supp}
To study the generalization ability of RaCT, we consider training the critic against Recall@100, in addition to NDCG@100. The only difference is that Recall treats each item as equally important, while NDCG treats the higher ranking items as more important. The results are shown in Table~\ref{tab:generalize_metrics}. Indeed, the RaCT gets slightly better testing Recall values when trained against the Recall metric, and the reverse holds for NDCG. 
More importantly, RaCT allows generalization across different ranking metrics: all testing metric values are significantly improved when trained against either Recall or NDCG.


\begin{table}[t!]
  \caption{Performance trained with different metrics. (ML-20M)}
  \label{tab:generalize_metrics}
    \centering
  \begin{tabular}{c|ccc}
    \toprule
      Training   & \multicolumn{3}{c}{Testing}   \\ \cline{2-4}
         & Recall@20 & Recall@50 & NDCG@100   \\
    \midrule   
       RaCT (Recall@100)  & \textbf{0.40316} & \textbf{0.54317} & 0.43392 \\
       RaCT (NDCG@100)  & 0.40269 & 0.54304 & \textbf{0.43395} \\
       VAE   & 0.39623 & 0.53632 & 0.42586 \\
  \bottomrule
\end{tabular}
\end{table}

Following~\citet{liang2018variational}, we compare with NCF on two small datasets, ML-1M (6,040 users, 3,704 items) and Pinterest (55,187 users, 9,916 items). This is because the prediction stage of NCF is slow, due to a lack of amortized inference as in VAE. We use their publicly available datasets and metrics for fair comparison.
The results are evaluated with a small cut-off value , to only study the highly ranked items: NDCG@10 and Recall@10. The performance are compared in Table~\ref{tab:compare_ncf}. 
Our observation that DAE performs better than VAE on these two datasets is consistent with~\citet{liang2018variational}. 
In general, RaCT shows higher improvement when a larger dataset (Pinterest), or a stochastic actor (VAE) is considered. This is because the sizes of the two datasets are relatively small, the critic can be better trained when more samples are observed.
On the larger Pinterest dataset, the auto-encoder variants perform better than NCF by a big margin, and our RaCT further boosts the performance.


\begin{table}[t!]
  \caption{Comparison between our RaCT with NCF on two small datasets. NCF results are from~\cite{liang2018variational}.}
  \vspace{0mm}
  \label{tab:compare_ncf}
    \centering
  \begin{tabular}{c|c|c|cc|cc}
    \toprule
   Dataset & Metric  &  NCF & DAE  & RaCT & VAE & RaCT \\ 
    \midrule   
    \multirow{2}{*}{ML-1M}
     & Recall@10   & 0.705 & 0.722 & 0.722 & 0.704 & 0.706 \\
     & NDCG@10     & 0.426 & 0.446 & 0.446 & 0.433 & 0.434 \\ \hline
    \multirow{2}{*}{Pinterest}
     & Recall@10    & 0.872 & 0.886 & 0.887 & 0.873 & 0.878 \\
     & NDCG@10      & 0.551 & 0.580 & 0.581 & 0.564 & 0.568  \\
  \bottomrule
\end{tabular}
\vspace{-0mm}
\end{table}



\begin{figure}[t!] \centering
	\vspace{-0mm}
	\begin{tabular}{c}
	    \hspace{-0mm}
		\includegraphics[height=4.00cm]{figs/plot_cutoff.pdf} \\
	\end{tabular}
	\vspace{-0mm}
	\caption{The improvement at various cut-off value R in evaluation. Given a specific R, the dashed line shows the VAE, and square dot shows the RaCT.}
	\vspace{-0mm}
	\label{fig:cut_off}
\end{figure}

\subsection{Correlation between training metrics}~\label{sec_training_metrics}
Figure~\ref{fig:correlation_supp} explores the relationship between NLL, the learned RaCT metric, and NDCG. We ensure that the best model for each method is used: the model after actor pre-training (Stage 1) is used for NLL plots, and the model after the actor-critic alternative training (Stage 3) is used for RaCT plots. The bottom row of plots displays the output of these models on the testing data. This demonstrates that RaCT's connection to the ranking metric generalizes to unseen data.


\subsection{Breakdown analysis for different cut-off values}~\label{sec_cut_off_supp} 
NDCG@100 only reflects the ranking quality at the cut-off value . \ie the top-100 ranking items. To study the ranking quality at different range of the predicted list, we consider a large range of , and report the corresponding NDCG values. We consider , and report the results in Figure~\ref{fig:cut_off}. The NDCG@R values are improved for various R, though the critic is trained against NDCG@100. This is because the NDCG metrics of different R are highly correlated, the RaCT can generalize across them.

\subsection{Breakdown analysis for different number of interactions}~\label{sec_interactions_supp} 
In Figure~\ref{fig:breakdown}, we show performance improvement across increasing user interactions.
We use ML-20M dataset for this case study.
The \# interactions is the number of items
each user interacts with (ground-truth), indicating the user's activity level. Figure~\ref{fig:breakdown}(a) shows the scatter plots between NDCG@100 values and various number of interactions on the testing dataset, for both VAE and our RaCT methods. RaCT generally improves VAE for a large range of user interactions. We further categorize the users in four groups according to their number of interactions: , , , , and plot the mean of NDCG@100 values for two methods in Figure~\ref{fig:breakdown}(b). RaCT improves VAE except for users with high activity level (). This is probably because the number of the most active users is small, as observed in Figure~\ref{fig:breakdown}(a). It yields a lack of training data for critic learning, which potentially hurts the performance.

\begin{figure}[t!]\vspace{-2mm}\centering
	\begin{tabular}{c c }		
		\hspace{-4mm}
		\includegraphics[height=3.2cm]{figs/breakdown/bd_scatter_plot_ml.pdf} \vspace{-0mm} & 
		\hspace{-4mm}
		\includegraphics[height=3.1cm]{figs/breakdown/bd_mean_ndcg_ml.pdf} \vspace{-0mm} \\
		(a) Scatter plot \vspace{0mm}   &
		(b) NDCG mean 
	\end{tabular}
	\vspace{-3mm}
	\caption{Improvement breakdown over different user interactions. (a) Scatter plot between NDCG@100 and activity levels. Note only  interactions  1000 is visualized, there is a long tail (1000) in the distribution. (b) Comparison of the mean NDCG@100 values for four user groups.  }
	\vspace{-2mm}
	\label{fig:breakdown}
\end{figure}


\subsection{On the performance improvement of actors via RaCT.}\label{sec:actors_supp}
We also consider the two other auto-encoder variants used in~\citet{liang2018variational} as the actor. (1) The DAE in~\citet{liang2018variational} chooses a smaller architecture , which achieves better performance than the larger architecture as in our VAE () by prevent over-fitting. While we observe the same result, it is interesting to note that the VAE () shows a much larger improvement gain than DAE~\citep{liang2018variational} when trained with our RaCT technique, and eventually significantly outperforms the latter. This shows that the additional modeling capacity is necessary to capture the more complex relationship in prediction, when the goal is ranking rather than MLE. (2) The MF in~\citet{liang2018variational} employs a Gaussian likelihood, which also gets slight improvement with the RaCT. Overall, we can conclude that the RaCT method improves all the MLE-based variants.

We also use ranking-loss-based WARP as the actor. 
For the large datasets considered in this paper, calculating the full WARP-loss for each user is impractically slow. We derive a simple approximation to WARP which runs in quasilinear time to the number of items. 
Even so, it takes around 30 minutes per epoch on ML-20M dataset, roughly 30 times slower than the VAE. WARP yields the score 0.312, which is lower than other baseline methods. This is consistent with the studies in~\citet{liang2018variational,sedhain2016effectiveness}. However, when RaCT is applied, WARP gets a significant improvement; in fact, the largest improvement gain of all the actors. This indicates the RaCT is a more direct and effective approach for learning to rank on large datasets.




\end{document}
