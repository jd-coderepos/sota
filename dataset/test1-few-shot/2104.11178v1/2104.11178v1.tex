\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xspace}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage[dvipsnames]{xcolor}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false,citecolor=ForestGreen]{hyperref}

\makeatletter\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}
  {.5em \@plus1ex \@minus.2ex}{-.5em}{\normalfont\normalsize\bfseries}}\makeatother

\iccvfinalcopy 

\newcommand{\yin}[1]{\textcolor{Green}{[Yin: #1]}}
\newcommand{\boqing}[1]{\textcolor{blue}{Boqing: #1}}
\newcommand{\hassan}[1]{\textcolor{Red}{Hassan: #1}}
\newcommand{\lzyuan}[1]{\textcolor{Purple}{Liangzhe: #1}}

\newcommand{\ours}{VATT\xspace}

\def\iccvPaperID{7094} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}





\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\begin{document}

\title{VATT: Transformers for Multimodal Self-Supervised Learning \\from Raw Video, Audio and Text}

\author{Hassan Akbari\thanks{Work done during an internship at Google.}~~\textsuperscript{1,2},~~Linagzhe Yuan\textsuperscript{1},~~Rui Qian\footnotemark[1]~~\textsuperscript{1,3},~~Wei-Hong Chuang\textsuperscript{1},~~Shih-Fu Chang\textsuperscript{2},\\Yin Cui\textsuperscript{1},~~Boqing Gong\textsuperscript{1}\\
\textsuperscript{1}Google\textsuperscript{2}Columbia University\textsuperscript{3}Cornell University\\
\texttt{\tt\small\{lzyuan,whchuang,yincui,bgong\}@google.com}~~
\texttt{\tt\small\{ha2436,sc250\}@columbia.edu}~~
\texttt{\tt\small\{rq49\}@cornell.edu}
}

\maketitle


\begin{abstract}
We present a framework for learning multimodal representations from unlabeled data using convolution-free Transformer architectures. Specifically, our \textbf{V}ideo-\textbf{A}udio-\textbf{T}ext \textbf{T}ransformer (\textbf{VATT}) takes raw signals as inputs and extracts multimodal representations that are rich enough to benefit a variety of downstream tasks. We train VATT end-to-end from scratch using multimodal contrastive losses and evaluate its performance by the downstream tasks of video action recognition, audio event classification, image classification, and text-to-video retrieval. Furthermore, we study a modality-agnostic, single-backbone Transformer by sharing weights among the three modalities. 
We show that the convolution-free VATT outperforms state-of-the-art ConvNet-based architectures in the downstream tasks. 
Especially, VATT's vision Transformer achieves the top-1 accuracy of 82.1\% on Kinetics-400, 83.6\% on Kinetics-600, and 41.1\% on Moments in Time, new records while avoiding supervised pre-training.
Transferring to image classification leads to  top-1 accuracy on ImageNet compared to  by training the same Transformer from scratch, showing the generalizability of our model despite the domain gap between videos and images.
VATT's audio Transformer also sets a new record on waveform-based audio event recognition by achieving the mAP of 39.4\% on AudioSet without any supervised pre-training.
\end{abstract}


\begin{figure*}[t]
\centering
   \includegraphics[width=0.95\linewidth]{figures/overview.pdf}
   \caption{\textbf{Overview of the \ours architecture and the self-supervised, multimodal learning strategy}. VATT linearly projects each modality into a feature vector and feeds it into a Transformer encoder. We define a semantically hierarchical common space to account for the granularity of different modalities and employ the noise contrastive estimation to train the model.}
\label{fig:overview}
\vspace{-4mm}
\end{figure*}


\section{Introduction}
Convolutional neural networks (CNNs)~\cite{lecun1998gradient,krizhevsky2012imagenet} have triumphed over various computer vision tasks. The inductive bias induced by convolutions, namely translation invariance and locality, are proven effective for the visual data. In the meantime, however, we witness in the natural language processing (NLP) community a paradigm shift from the models with strong inductive bias, such as recurrent neural networks~\cite{hochreiter1997long,bahdanau2014neural} and CNNs~\cite{zhang2015character,gehring2017convolutional}, to more general architectures constructed upon self-attention. Particularly, Transformers~\cite{vaswani2017attention} become the de facto model architecture for NLP tasks~\cite{devlin2018bert,radford2018improving,radford2019language,brown2020language}. Pre-training a Transformer on large text corpora followed by fine-tuning gives rise to state-of-the-art results for different downstream tasks. 

In view of the success of the attention mechanism in NLP, there has been a rich line of works exploring its potential in computer vision. Early work studied hybrid models consisting of both convolutions and attention modules~\cite{wang2017residual,woo2018cbam,girdhar2017attentional,zhang2019residual}. Recent studies showed that convolution-free, specially designed all-attention models can match CNNs' performance on image recognition tasks~\cite{zhao2020exploring,hu2019local,ramachandran2019stand}. Most recently, Dosovitskiy \etal~\cite{dosovitskiy2021an} achieved impressive performance on several image recognition tasks, including ImageNet~\cite{deng2009imagenet}, using a pre-trained Transformer with minimal architecture changes. Their work delivered a compelling message that ``large scale (supervised) training trumps inductive bias (for image classification).'' This conclusion was further extended to video recognition tasks by~\cite{bertasius2021space,vivit}. 

However, the large-scale supervised training of Transformers is essentially troubling for two main reasons. First, it rules out the much larger other part of ``big visual data,'' \ie, the vast amount of unlabeled, unstructured visual data. As a result, the supervised training strategy could produce biased systems that require even more labeled data to correct their biases. Second, this strategy fundamentally limits the application scope of Transformers in computer vision because it is costly and extremely time-consuming to collect enough labeled images or videos for training the millions of parameters, choosing hyper-parameters, and validating their expected generalization. 

Hence, this work poses another pressing question about the Transformers that take raw signals as input. \emph{How to empower them with large-scale, unlabeled data?} To answer this question, we draw insights from NLP. BERT~\cite{devlin2018bert} and the GPT family~\cite{radford2018improving, radford2019language, brown2020language}
use masked language modeling as their pre-training tasks. Natural languages are organic supervision for Transformers. They sequentially place words, phrases, and sentences into context, granting them semantics and syntax. For visual data, \emph{the most organic supervision is arguably the multimodal videos.}  They are abundantly available in the digital world, and their temporal, cross-modality regulation, and therefore supervision, requires no human annotation. The extreme scale of multimodal videos is potentially capable to teach Transformers necessary priors, as opposed to predefined inductive bias, to model the visual world.

To this end, we study self-supervised, multimodal pre-training of three Transformers~\cite{vaswani2017attention}, which take as input the raw RGB frames of internet videos, audio waveforms, and text transcripts of the speech audio, respectively. We call the video, audio, text Transformers \ours. Figure~\ref{fig:overview} illustrates the architecture. VATT borrows the exact architecture from BERT~\cite{devlin2018bert} and ViT~\cite{dosovitskiy2021an} except the layer of tokenization and linear projection reserved for each modality separately. This design follows the same spirit as ViT that we make the minimal changes to the architecture so that the learned model can transfer its weights to various frameworks and tasks. Furthermore, the self-supervised, multimodal learning strategy resonates the spirit of BERT and GPT that the pre-training requires minimal human curated labels. 

We evaluate the pre-trained Transformers on a variety of downstream tasks: \emph{image classification, video action recognition, audio event
classification, and zero-shot video retrieval}. Fine-tuning the vision-modality Transformer on ImageNet~\cite{deng2009imagenet} obtains the top-1 accuracy of , which is comparable to  achieved by ViT. This result is especially appealing considering the domain gap between videos and images, and that ViT is pre-trained using a large-scale, human-curated image dataset. Furthermore, we set new records on Kinetics-400~\cite{kinetics400}, Kinetics-600~\cite{kinetics600}, Moments in Time~\cite{monfort2019moments}, and AudioSet~\cite{audioset} without supervised pre-training.

Our VATT results, along with others reported for NLP tasks~\cite{devlin2018bert,brown2020language}, image recognition~\cite{dosovitskiy2021an}, semantic segmentation~\cite{zheng2020rethinking}, point cloud classification~\cite{zhao2020point}, and action recoginition~\cite{bertasius2021space}, demonstrate that Transformer is a capable general-purpose architecture for different types of data. 

To move one step forward, we challenge the Transformers in VATT by a seemingly too strong constraint: sharing weights among the video, audio, and text modalities. The idea is to test whether there exists a single, general-purpose model for all the modalities --- of course, they still have their own layers of tokenization and linear projection. Preliminary results on zero-shot video retrieval are encouraging. This modality-agnostic Transformer performs comparably with the modality-specific ones.

Finally, another contribution of this work is DropToken, a simple yet effective technique to reduce the training complexity for Transformers with a minor reduction of the end model's performance. DropToken randomly drops a portion of the video and audio tokens from each input sequence during training, leveraging high-resolution multimodal inputs. This is significant for Transformers because their computational complexity is quadratic with respect to the number of input tokens. 


\section{Related work}
\subsection{Transformers in Vision}
Transformer was originally built for NLP tasks~\cite{vaswani2017attention} and the design of multi-head attention shows its effectiveness on modeling long-term correlation of words. A few attempts have been made to use Transformer for vision tasks like image super-resolution~\cite{yang2020learning}, object detection~\cite{carion2020end} and multimodal video understanding~\cite{sun2019learning, chen2020uniter, luo2020univilm}. However these methods still rely on the feature extracted by CNNs.  Recently, \cite{dosovitskiy2021an} proposes a set of convolution-free vision Transformers which directly work on raw images and obtain competitive performance with CNNs. \cite{touvron2020training} improves the training data efficiency of~\cite{dosovitskiy2021an} by using stronger data augmentations and knowledge distillation. Since then, the pure Transformer design has been adopted to various vision tasks including semantic segmentation~\cite{zheng2020rethinking}, point cloud classification~\cite{zhao2020point}, action recoginition~\cite{bertasius2021space,sharir2021image,vivit}. To the best of our knowledge, our \ours is the first Transformer model on raw multimodal inputs of video, audio and text.

\subsection{Self-Supervised Learning}
\paragraph{Single vision modality.} 
Early work of self-supervised visual representation learning usually learns from unlabeled images via manually specified pretext tasks, like auto-encoding~\cite{pathak2016context,zhang2016colorful,zhang2017split}, patch location prediction~\cite{doersch2015unsupervised}, solving jigsaw puzzles~\cite{noroozi2016unsupervised}, and image rotation prediction~\cite{gidaris2018unsupervised}. Wu~\etal~\cite{wu2018unsupervised} propose a novel instance discrimination objective. The recent trend of contrastive learning~\cite{moco,simclr,ye2019unsupervised,byol,henaff2019data,tian2019contrastive} integrates data augmentations and instance discrimination by maintaining relative consistency between representations of an image and its augmented view. Clustering can also provide an effective addition~\cite{swav}. 
Recently, Chen~\etal~\cite{chen2021empirical} conduct contrastive learning using ViT~\cite{dosovitskiy2021an} and achieve impressive results.
As for the video domain, it is natural to exploit the temporal signals as the pretext task. Examples include predicting the future frame~\cite{srivastava2015unsupervised}, motion and appearance statistics~\cite{wang2019self}, speed~\cite{benaim2020speednet, wang2020self} and encodings~\cite{lotter2016deep,han2019video,han2020memory}, sorting frames or video clips~\cite{lee2017unsupervised,xu2019self,kim2019self,fernando2017self}. Recently, Qian~\etal~\cite{qian2020spatiotemporal} apply contrastive learning to videos with a temporal sampling strategy and temporally consistent spatial augmentation. 

\paragraph{Multimodal video.} Video is a natural source of multimodal data.
Multimodal self-supervised learning can be achieved by predicting whether a video has correspondence with an audio stream~\cite{arandjelovic2017look,arandjelovic2018objects,morgado2020audio,korbar2018cooperative}, cross-modality clustering~\cite{alwassel2019self}, and evolving losses~\cite{piergiovanni2020evolving}.
Recently, Alayrac~\etal~\cite{mmv} use contrastive loss to learn from video, audio and text; Recasens~\etal~\cite{recasens2021broaden} learn to predict a broad view that spans a longer temporal context from a narrow view. 
\ours serves as a first work combining the strength of convolution-free Transformer and multimodal contrastive learning. 


\section{Approach}
In this section, we introduce our convolution-free \ours architecture and elaborate on the self-supervised multimodal objectives for training \ours from scratch.

Figure~\ref{fig:overview} is an overview of the architecture. We feed each modality to a tokenization layer, where the raw input is projected to an embedding vector followed by a Transformer. There are two major settings: 1) The backbone Transformers are separate and have specific weights for each modality, and 2) The Transformers share weights, namely, there is a single backbone Transformer applied to any of the modalities. In either setting, the backbone extracts modality-specific representations, which are then mapped to common spaces to be compared with each other by contrastive losses. We describe each module in the following.

\subsection{Tokenization and Positional Encoding}
\ours operates on raw signals. The vision-modality input consists of 3-channel RGB pixels of video frames, the audio input is in the form of air density amplitudes (waveforms), and the text input is a sequence of words. We first define a modality-specific tokenization layer that takes as input the raw signals and returns a sequence of vectors to be fed to the Transformers. Besides, each modality has its own positional encoding, which injects the order of tokens into Transformers~\cite{vaswani2017attention}.

\paragraph{Video:} we partition an entire video clip of size  to a sequence of  patches, where each patch contains  voxels. We apply a linear projection on the entire voxels in each patch to get a -dimensional vector representation. This projection is performed by a learnable weight . This can be seen as a 3D extension of the patching mechanism proposed in~\cite{dosovitskiy2021an}. We use a simple method to encode the position of each patch. We define a dimension-specific sequence of learnable embeddings, each encoding location  along a dimension in the 3D space as follows:

where  is the -th row of . This scheme allows us to use  positional embeddings to encode all the  patches in a video clip.

\paragraph{Audio:} the raw audio waveform is a 1D input with length , and we partition it to  segments each containing  waveform amplitudes. Similar to video, we apply a linear projection with a learnable weight  to all elements in a patch to get a -dimensional vector representation. We use  learnable embeddings to encode the position of each waveform segment.

\paragraph{Text:} we first construct a vocabulary of size  out of all words in our training dataset. For an input text sequence, we then map each word to a -dimensional one-hot vector followed by a linear projection with a learnable weight . This is equivalent to an embedding dictionary lookup, which has been widely used in natural language understanding~\cite{mikolov2013efficient}.

\begin{figure}[t]
\centering
   \includegraphics[width=0.8\linewidth]{figures/droptoken.pdf}
   \caption{\textbf{DropToken}. During training, we leverage the high redundancy in multimodal video data and propose to randomly drop input tokens. This simple and effective technique significantly reduces training time with little loss of quality.
   }
\label{fig:drop-token}
\vspace{-4mm}
\end{figure}

\subsubsection{DropToken}
We introduce DropToken, a simple yet effective strategy to reduce the computational complexity during training (Figure \ref{fig:drop-token}). Once we get the token sequence for the video or audio modality, we randomly sample a portion of the tokens and then feed the sampled sequence, not the complete set of tokens, to the Transformer. This is crucial for reducing the computational cost because Transformers' computation complexity is quadratic, , where  is number of tokens in the input sequence. Any effort on reducing the input length would reduce the number of FLOPs quadratically. This has an immediate impact on the wall clock time for training these models and makes it possible to host large models in limited hardware. We argue that instead of reducing the resolution or dimension of the raw inputs, it is better to keep a high-fidelity input and randomly sample the tokens via DropToken. DropToken is appealing especially with the raw video and audio inputs, which may contain high redundancies.

\subsection{The Transformer Architecture}
\label{tx-arch}
For simplicity, we adopt the most established Transformer architecture~\cite{devlin2018bert}, which has been widely used in NLP. Similar to ViT~\cite{dosovitskiy2021an}, we do not tweak the architecture so that our weights can be easily transferred to any standard Transformer implementation. We will briefly elaborate on our choices and refer readers to~\cite{dosovitskiy2021an,devlin2018bert} for more details of the standard Transformer architecture. All of our backbones follow the below formulation and hierarchy (also illustrated in Figure~\ref{fig:overview} middle panel):

where  is the learnable embedding of a special aggregation token whose corresponding output in the Transformer () is used as the aggregated representation for the entire input sequence. This will be later used for classification and common space mapping. MHA stands for Multi-Head-Attention, which performs the standard self-attention~\cite{vaswani2017attention} on the input sequence. MLP stands for Multi-Layer-Perceptron and contains a dense linear projection with a GeLU activation~\cite{hendrycks2016gaussian} followed by another dense linear projection. LN stands for Layer Normalization~\cite{ba2016layer}. In our text model, we remove the position encoding  and add a learnable relative bias to each attention score of the first layer in the MHA module. This simple change makes our text model's weights directly transferable to the state-of-the-art text model T5~\cite{raffel2020exploring}.

\subsection{Common Space Projection}
We use common space projection and contrastive learning in that common space to train our networks. More specifically, given a video-audio-text triplet, we define a semantically hierarchical common space mapping that enables us to directly compare video-audio pairs as well as video-text pairs by the cosine similarity. As argued in~\cite{mmv}, such comparison is more feasible if we assume there are different levels of semantic granularity for these spaces. To achieve this, we define multi-level projections as below:

where  and  are the projection heads to respectively map the video and audio Transformers' outputs to the video-audio common space . Moreover,  and  project the text Transformer's outputs and the video embedding in the  space to video-text common space, . This multi-level common space projection is depicted in Figure~\ref{fig:overview} (the rightmost panel). The main intuition behind this hierarchy is that different modalities have different levels of semantic granularity, so we should impose this as an inductive bias in the common space projection. Similar to~\cite{mmv}, we use a linear projection for , , and , and a two-layer projection with ReLU in between for . To ease the training, a batch normalization is used after each linear layer.

\subsection{Multimodal Contrastive Learning}
The unlabeled multimodal videos are abundantly available in the wild, so we use self-supervised objectives to train our \ours. Inspired by~\cite{mmv,arandjelovic2017look,miech2020end}, we use Noise-Contrastive-Estimation (NCE) to align video-text and video-audio pairs. Assuming that a stream of video-audio-text is given, we compose video-text and video-audio pairs from different temporal locations. Positive pairs across two modalities are constructed by simply selecting video-audio and video-text pairs from the same video clip. Negative pairs are obtained by randomly sampling any video, audio, or text sequence separately. The NCE objective function maximizes the similarity between a positive pair and meanwhile minimizes the similarities between negative pairs. 

In our pre-training datasets, the text is speech transcripts by an off-the-shelf ASR, which leads to many noisy text sequences. Besides, some video sequences contain no speech audio or transcript. Hence, as suggested by~\cite{mmv}, we use Multiple-Instance-Learning-NCE (MIL-NCE), an extension to NCE, to match a video input to multiple text inputs that are temporally close to the video input. This variant improves the vanilla NCE for video-text matching in~\cite{miech2020end}. We use the regular NCE for the video-audio pairs and MIL-NCE for the video-text pairs. Concretely, given the common space specified in Section~\ref{eq:common_space}, the loss objectives can be written as follows:



where  in Equation~\ref{eq:NCE} is the batch size; namely, we construct in each iteration  video-audio pairs, with one positive pair. In Equation~\ref{eq:mil-nce},  and  are positive and negative text clips temporally surrounding a video clip , respectively.  Specifically,  contains five text clips that are nearest neighbors to the video clip in time.  is a temperature to adjust the softness of the objectives in distinguishing the positive pairs from the negative pairs.

The overall objective for training the entire \ours model end-to-end is as follows:

where  balances the two losses.


\section{Experiments}
In this section, we describe the experiment setup and results for the pre-training and downstream evaluation of our \ours model. We also present ablation studies about some design choices in \ours.

\subsection{Pre-Training Datasets}
The pre-training follows~\cite{mmv,miech2020end,howto100m,audioset}, where a large-scale dataset of internet videos and AudioSet~\cite{audioset} are used to pre-train \ours. The former contains 1.2M unique videos, each providing multiple clips with audio and narration scripts resulting in 136M video-audio-text triplets in total. The narration scripts are extracted from speech audio using an off-the-shelf ASR. AudioSet consists of 10-second clips sampled from two million videos from YouTube. The dataset contains a variety of audio events with their corresponding video without any narration, so we do not have any text input from this dataset. We do not use any labels from the datasets. We uniformly sample clips from these datasets; a mini-batch in the pre-training contains samples from both datasets. In order to fill in the empty text in AudioSet, we feed a sequence of zeros to the text Transformer and exclude those samples from the MIL-NCE loss. 

\subsection{Downstream Tasks and Datasets}
We evaluate the pre-trained \ours on a set of diverse, representative downstream tasks to test different aspects of the learned representations. 

\paragraph{Video action recognition:} We evaluate the visual representations on UCF101~\cite{soomro2012ucf101} (101 classes, 13,320 videos), HMDB51~\cite{hmdb} (51 classes, 6,766 videos), Kinetics-400~\cite{kinetics400} (400 classes, 234,584 videos), Kinetics-600~\cite{kinetics600} (600 classes, 366,016 videos), and Moments in Time~\cite{monfort2019moments} (339 classes, 791,297 videos). Since UCF101 and HMDB51 are small datasets compared to the size of our model, we freeze the vision backbone and use its outputs to train a linear classifier. We use the split \#1 results of the two datasets as a reference in our design exploration.
For Kinetics-400, Kinetics-600, and Moments in Time, we fine-tune our vision backbone initialized from the pre-trained checkpoint.

\paragraph{Audio event classification:} We use ESC50~\cite{esc} (50 classes, 2000 audio clips) and AudioSet~\cite{audioset} (527 classes, 2M audio clips) to evaluate our audio Transformer on audio event classification. We use ESC50 to train a linear classifier on top of the frozen audio Transformer. We use the split \#1 results of this dataset as a reference in our design exploration. We also use AudioSet to fine-tune our audio backbone initialized from the pre-trained checkpoint.

\paragraph{Zero-shot video retrieval:} We evaluate the quality of our video-text common space representations by \textit{zero-shot} text-to-video retrieval on two of the most established datasets in this area: YouCook2~\cite{youcook2} and MSR-VTT~\cite{xu2016msr-vtt} with 3.1k and 1k video-text pairs, respectively. We follow the same evaluation pipeline described in~\cite{mmv} and report the Recall at 10 (R@10).

\paragraph{Image classification:} Although there exists a domain gap between images and the video datasets used for pre-training \ours, we test the learned vision Transformer in the image domain. We fine-tune the last checkpoint of the vision Transformer on ImageNet~\cite{deng2009imagenet} with no modification to our architecture or the tokenization pipeline. We will elaborate on this in the sequel.

\subsection{Experimental Setup}
\label{sec:exp_setup}
\paragraph{Inputs:} During pre-training, we sample 32 frames at 10 fps for both pre-training datasets. For these frames, we randomly crop a temporally consistent spatial region whose relative area is in the range of [0.08, 1] and its aspect ratio in [0.5, 2]. These crops are then resized to , followed by a horizontal flip and color augmentation. The color augmentation follows~\cite{mmv} and randomizes brightness (max delta = 32/255), saturation (max delta = 0.4),
contrast (max delta=0.4), and hue (max delta=0.2). We clip values to ensure the RGB is in [0, 1]. The audio waveforms are sampled in sync with the video frames at 48kHz. Both video and audio inputs are normalized between [-1, 1] for numerical stability. We use patch sizes of  and  for video and raw waveform tokenization, respectively. We use one-hot vectors to encode text sequences with the vocabulary size of , which is the same as word2vec~\cite{mikolov2013efficient}. The resulting sequence retains a maximum of 16 words by either clipping or padding. We use DropToken with a drop rate of  during pre-training. For video fine-tuning and evaluation, 32 frames with a temporal stride of 2 are sampled at 25 fps (2.56 seconds) with a crop size of  (with similar video augmentation during pre-training), and we do not drop any tokens. We do not change the input size for audio and text during evaluation.

\paragraph{Network setup in \ours:} 
We use the same Transformer architecture described in Section~\ref{tx-arch} with various sizes shown in Table~\ref{table:arch-size}. We use the Medium model for our modality-agnostic variant (\ours-MA). For the experiments with modality-specific Transformers, we use the Small and Base models for the text and audio modalities, respectively, while varying the model sizes for the video modality. This results in 3 variants for the modality-specific video-audio-text backbones: Base-Base-Small (BBS), Medium-Base-Small (MBS), and Large-Base-Small (LBS).
\begin{table}[h!]
    \small
    \centering
    \setlength{\tabcolsep}{2pt}
    \begin{tabular}{@{}lcccccc@{}}
    \toprule
    Model & Layers & Hidden Size & MLP Size & Heads & Params \\
    \midrule
    Small & 6 & 512 & 2048 & 8 & 20.9 M \\
    Base & 12 & 768 & 3072 & 12 & 87.9 M \\
    Medium & 12 & 1024 & 4096 & 16 & 155.0 M \\
    Large & 24 & 1024 & 4096 & 16 & 306.1 M \\
    \bottomrule
    \end{tabular}
    \vspace{2mm}
    \caption{Details of the Transformer architectures in \ours.}
    \label{table:arch-size}
\end{table} 
\paragraph{Projection heads and contrastive losses:} 
We use  and  for the projection to the common spaces  and , respectively. We normalize the vectors before calculating the NCE and MIL-NCE objectives and use the temperature of  and the weight of  in the loss in Equation~\ref{eq:overall_loss}. We choose these values following the previously established practice~\cite{mmv}; we may achieve better results by varying these hyper-parameters.

\paragraph{Pre-training setup:} 
We pre-train \ours from scratch using Adam~\cite{kingma2014adam} with an initial learning rate of 1-4, 10k warmup steps, 500k steps in total, a batch size of 2048, and a quarter-period cosine schedule to anneal the learning rate from 1-4 to 5-5. In the exploration experiments, we use a batch size of 512 while keeping the rest of the training parameters the same. Our pipeline is implemented in Tensorflow (v2.4), and our models are trained for 3 days using 256 TPUs (v3).

\paragraph{Video fine-tuning setup:} 
For video action recognition, we use the SGD with a momentum of 0.9 and an initial learning rate of , 2.5k warmup steps, a batch size of 64, 100k steps in total, and a half-period cosine schedule to anneal the learning rate to 0. We use label smoothing with smoothing factor . The video frame resolution is , which results in an increase in the number of positional encoding weights. This increase is due to the fact that, in the pre-train time, we have 8+14+14 positional encoding buckets, while 8+20+20 positional buckets are required to completely encode  horizontal and  vertical locations in fine-tune. To generate the new positional embeddings, we create a new set of positional encoding buckets by bi-cubic interpolation from the original buckets. After this step, we fine-tune the entire network, including the positional encoding buckets, end-to-end. We tried fixed positional embeddings (solely based on interpolation for the missing locations) and did not observe significant improvements. We uniformly sample 4 clips to cover the entire 10 seconds of the video and apply a standard 3-crop evaluation following~\cite{slowfast}. We average the logits across the resulting 12 views before having the final class predictions.


\paragraph{Audio fine-tuning setup:} 
For audio event classification, we use the SGD with a momentum of 0.9, an initial learning rate of , 5k warmup steps, a batch size of 1024, 50k steps in total, and a half-period cosine schedule to anneal the learning rate to 0. We observe that increasing the effective receptive field improves the overall performance. We suggest that this might be due to the fact that the AudioSet annotations are multi-label and each event might occur in different temporal positions. Hence, we employ the duration of 6.4s with 24kHz sampling rate (153.6k total input samples). Similar to~\cite{kong2020panns}, we use mixup~\cite{zhang2017mixup} on input-label (-) pairs in a mini-batch as below:

where the input-label pairs are randomly sampled from a mini-batch, and the mixing rate  is sampled from a Beta distribution. We also perform data balancing by penalizing the loss value of a sample with the inverse of the per-batch number of repetitive labels it carries. This is crucial for avoiding over-fitting since AudioSet has a long-tailed distribution, and a few dominant classes may disrupt the training~\cite{kong2020panns}.

\paragraph{Image fine-tuning setup:} 
We finetune the pre-trained \ours on ImageNet for 50 epochs with  input resolution, 512 batch size, SGD with momentum of 0.9, cosine learning rate decay with an initial learning rate of 8-2, and label smoothing of 0.1. No weight decay is used.

\paragraph{Linear evaluation setup:} 
We use a linear classifier with fixed backbones across all datasets and tasks. We observe that using matrix factorization on the classifier weight~\cite{rendle2010factorization} leads to a more stable result across experiments. More specifically, we use a factorized weight , where  and  are learnable weights. During training this classifier, we randomly choose a subset of the  components in  and , hence leading to a low-rank classifier weight, . The classifier weight, , is trained using the Adam optimizer with a learning rate of 5-4, a batch size of 64, a total of 50k training steps, and a sampling rate of 10\% on its  components.

\paragraph{Zero-shot retrieval setup:} 
For zero-shot text-to-video retrieval, we use the 1k split of MSR-VTT and the entire test split of YouCook2 as the pool for retrieval. We use  central crops for 32 frames with a temporal stride of 2 sampled at 25 fps. Since each input clip covers 2.56 seconds, and the full clip length is 10 seconds, we average the embeddings over 4 uniformly sampled clips before calculating the similarity with a text query's embedding. We -normalize each vector to assure that a dot product results in the cosine similarity.

\subsection{Results}

\subsubsection{Fine-tuning for video action recognition}
We fine-tune \ours's vision Transformer on Kinetics-400, Kinetics-600, and Moments in Time, three of the arguably most established large-scale datasets for video action recognition. Table~\ref{table:video-classification-k400} and Table~\ref{table:video-classification-k600} show the results for Kinetics-400 and Kinetics-600, respectively, and Table~\ref{table:video-classification-mit} shows the results on Moments in Time, all comparing our models to state-of-the-art video models.

\begin{table}[h]
    \small
    \centering
    \begin{tabular}{@{}lcccc@{}}
    \toprule
        \sc Method & \sc Top-1 & \sc Top-5 & \sc TFLOPs\\
    \midrule
    ARTNet~\cite{wang2018nonlocal} & 69.2 & 88.3 & 6.0\\
    I3D~\cite{carreira2017quo} & 71.1 & 89.3 & -\\
    R(2+1)D~\cite{du2017r2+1d}  & 72.0 & 90.0 & 17.5\\
    MFNet~\cite{lee2018motion} & 72.8 & 90.4 & -\\
    Inception-ResNet~\cite{alom2020improved} & 73.0 & 90.9 & -\\
    bLVNet~\cite{fan2019blvnet} & 73.5 & 91.2 & 0.84\\
    -Net~\cite{chen20182} & 74.6 & 91.5 & -\\
    TSM~\cite{lin2019tsm} & 74.7 & - & -\\
    S3D-G~\cite{xie2018rethinking} & 74.7 & 93.4 & -\\
    Oct-I3D+NL~\cite{chen2019drop} & 75.7 & - & 0.84\\
    D3D~\cite{stroud2020d3d} & 75.9 & - & -\\
    GloRe~\cite{glore} & 76.1 & - & -\\
    I3D+NL~\cite{wang2018nonlocal} & 77.7 & 93.3 & 10.8\\
    ip-CSN-152~\cite{tran2019video} & 77.8 & 92.8 & -\\
    MoViNet-A5~\cite{kondratyuk2021movinets} & 78.2 & - & 0.29 \\
    CorrNet~\cite{corrnet} & 79.2 & - & 6.7\\
    LGD-3D-101~\cite{lgdnet} & 79.4 & 94.4 & -\\
    SlowFast~\cite{slowfast} & 79.8 & 93.9 & 7.0\\
    X3D-XXL~\cite{feichtenhofer2020x3d} & 80.4 & 94.6 & 5.8\\
    TimeSFormer-L~\cite{bertasius2021space} & 80.7 & 94.7 & 7.14\\
\midrule
    \ours-Base & 79.6 & 94.9 & 9.09\\
    \ours-Medium & 81.1 & \textbf{95.6} & 15.02\\
    \ours-Large & \textbf{82.1} & 95.5 & 29.80\\
    \midrule
    \ours-MA-Medium & 79.9 & 94.9 & 15.02\\
    \bottomrule
    \end{tabular}
    \vspace{2mm}
    \caption{Results for video action recognition on Kinetics-400.}
    \label{table:video-classification-k400}
    \vspace{-2mm}
\end{table}
 \begin{table}[h!]
    \small
    \centering
    \begin{tabular}{@{}lccc@{}}
    \toprule
        \sc Method & \sc Top-1 & \sc Top-5 \\
    \midrule
    I3D-R50+Cell~\cite{wang2020attentionnas} & 79.8 & 94.4\\
    LGD-3D-101~\cite{lgdnet} & 81.5 & 95.6\\
    SlowFast~\cite{slowfast} & 81.8 & 95.1\\
    X3D-XL~\cite{feichtenhofer2020x3d} & 81.9 & 95.5\\
    TimeSFormer-HR~\cite{bertasius2021space} & 82.4 & 96.0\\
    MoViNet-A5~\cite{kondratyuk2021movinets} & 82.7 & 95.7 \\
\midrule
    \ours-Base & 80.5 & 95.5 \\
    \ours-Medium & 82.4 & 96.1 \\
    \ours-Large & \textbf{83.6} & \textbf{96.6} \\
    \midrule
    \ours-MA-Medium & 80.8 & 95.5 \\
    \bottomrule
    \end{tabular}
    \vspace{2mm}
    \caption{Results for video action recognition on Kinetics-600.}
    \label{table:video-classification-k600}
    \vspace{-2mm}
\end{table} \begin{table}[h!]
    \small
    \centering
    \begin{tabular}{@{}lccc@{}}
    \toprule
        \sc Method & \sc Top-1 & \sc Top-5 \\
    \midrule
    TSN~\cite{wang2016temporal} & 25.3 & 50.1\\
    R3D-50~\cite{ryoo2019assemblenet} & 27.2 & 51.7\\
    TRN~\cite{zhou2018temporal} & 28.3 & 53.4\\
    I3D~\cite{carreira2017quo} & 29.5 & 56.1\\
    blVNet~\cite{fan2019more} & 31.4 & 59.3\\
    SRTG-R3D-101\cite{stergiou2021learn} & 33.6 & 58.5\\
    AssembleNet-101~\cite{ryoo2019assemblenet} & 34.3 & 62.7\\
MoViNet-A5~\cite{kondratyuk2021movinets} & 39.1 & - \\
    \midrule
    \ours-Base & 38.7 & 67.5 \\
    \ours-Medium & 39.5 & \textbf{68.2} \\
    \ours-Large & \textbf{41.1} & 67.7 \\
    \midrule
    \ours-MA-Medium & 37.8 & 65.9 \\
    \bottomrule
    \end{tabular}
    \vspace{2mm}
    \caption{Results for video action recognition on Moments in Time.}
    \label{table:video-classification-mit}
    \vspace{-2mm}
\end{table} 
On all datasets, we achieve higher accuracy than previous works including TimeSFormer \cite{bertasius2021space}, a recent effort in fine-tuning the ViT checkpoints obtained by \textit{supervised} pre-training on image classification. In contrast, we do not rely on any labels curated by human. To the best of our knowledge, \ours provides the first vision Transformer backbone that is pre-trained from scratch using self-supervision on multimodal videos and achieves state-of-the-art results on video action recognition.

To further test how much our multimodal self-supervised pre-training helps in achieving these numbers, we train a variant from scratch without any pre-training and observe the top-1 and top-5 accuracies of  and  on Kinetics-400, respectively. This low accuracy verifies the efficacy of the \ours pre-training. 

Finally, we find that VATT-MA-Medium, the modality-agnostic backbone shared by the video, audio, and text modalities, is on par with the modality-specific VATT-Base when fine-tuned for the video action recognition. This result is encouraging as it indicates the potential of unifying three data modalities by a \textit{single} Transformer backbone.

\subsubsection{Fine-tuning for audio event classification}
We fine-tune \ours's audio Transformer on AudioSet, which benchmarks the task of multi-label audio event classification. Same as vision Transformer fine-tuning, we initialize the backbone from the last pre-trained checkpoint. Table~\ref{table:audio-classification} shows the results compared to state-of-the-art models. Following common practice~\cite{gemmeke2017audio,kong2019weakly}, we report mean Average Precision (mAP), Area Under Curve (AUC), and d-prime directly calculated based on AUC~\cite{gemmeke2017audio}.
\begin{table}[h!]
    \small
    \centering
    \begin{tabular}{@{}lcccc@{}}
    \toprule
        \sc Method & mAP & AUC & d-prime \\
    \midrule
    DaiNet~\cite{dai2017very} & 29.5 & 95.8 & 2.437\\
    LeeNet11~\cite{lee2017sample} & 26.6 & 95.3 & 2.371\\
    LeeNet24~\cite{lee2017sample} & 33.6 & 96.3 & 2.525\\
    Res1dNet31~\cite{kong2020panns} & 36.5 & 95.8 & 2.444\\
    Res1dNet51~\cite{kong2020panns} & 35.5 & 94.8 & 2.295\\
    Wavegram-CNN~\cite{kong2020panns} & 38.9 & 96.8 & 2.612\\
    \midrule
    \ours-Base & \textbf{39.4} & \textbf{97.1}  & \textbf{2.895}\\
    \midrule
    \ours-MA-Medium & 39.3 & 97.0  & 2.884\\
    \bottomrule
    \end{tabular}
    \vspace{2mm}
    \caption{Results for audio event classification on AudioSet.}
    \label{table:audio-classification}
\end{table} 
Our audio Transformer consistently outperforms the existing CNN-based models in all metrics. More interestingly, fine-tuning the modality-agnostic backbone (VATT-MA-Medium) is on par with fine-tuning the modality-specific one (VATT-Base). To the best of our knowledge, \ours is the first Transformer that outperforms CNN-based models in audio event recognition. \ours operates on raw waveforms and does not utilize any handcrafted features.

\subsubsection{Fine-tuning for image classification}
In this section, we show that our pipeline is capable of transferring the learned knowledge into another domain by performing the image classification task, even though the models are pre-trained in the multimodal video domain.
We fine-tune the vision Transformer in \ours on ImageNet without any modification to the backbone architecture. Instead, to satisfy the voxel-to-patch layer we replicate the input image 4 times and feed it to the network. The network sees the input as a single-frame video clip and performs spatial self-attention.

\begin{table}[h!]
    \small
    \centering
    \resizebox{0.95\columnwidth}{!}{\begin{tabular}{@{}lccc@{}}
    \toprule
        \sc Method & \sc Pre-training data & \sc Top-1 & \sc Top-5 \\
        
    \midrule
    iGPT~\cite{chen2020generative} & ImageNet & 66.5 & - \\
    ViT-Base~\cite{dosovitskiy2021an} & JFT & \textbf{79.9} & - \\
    \midrule
    \ours-Base & - & 64.7 & 83.9 \\
    \ours-Base & HowTo100M & 78.7 & 93.9\\
    \bottomrule
    \end{tabular}
    }
    \vspace{1mm}
    \caption{Finetuning results for  ImageNet classification.}
    \label{table:image-classification}
    \vspace{-4mm}
\end{table} 
Table~\ref{table:image-classification} shows the results for fine-tuning the vision Transformer end-to-end on ImageNet. We can see that our pre-training leads to a significant boost in the accuracy compared to training from scratch. We also observe that even though our pre-training happens in the video domain, we still achieve comparable results to the self-supervised pre-training using large-scale \emph{image} data~\cite{dosovitskiy2021an}.

\begin{figure*}[t]
\centering
   \includegraphics[width=0.95\linewidth]{figures/k400_tsne_out.pdf}
   \caption{t-SNE visualization of the feature representations extracted by the vision Transformer trained from scratch on Kinetics-400 validation set, the modality-specific \ours's vision Transformer after fine-tuning, and the modality-agnostic Transformer after fine-tuning.  For better visualization, we show 100 random classes from Kinetics-400.}
\label{fig:tsne_k400}
\vspace{-2mm}
\end{figure*}

\subsubsection{Zero-shot retrieval}
We feed video-text pairs to our VATT-MBS, which uses the Medium, Base, and Small Transformers for the video, audio, and text modalities, respectively, and extract representations in the  space. We then calculate the similarity between each video-text pair from YouCook2 and MSR-VTT. Given a text query, we rank the videos based on their similarities to the text. We then measure the recall for the correct video in the top-10 ranked videos. We also measure the median of the rank of the correct video.
\begin{table}[h!]
    \small
    \centering
    \setlength{\tabcolsep}{1pt}
    \begin{tabular}{@{}lccccccc@{}}
    \toprule
        & & & \multicolumn{2}{c}{YouCook2} &  \multicolumn{2}{c}{MSR-VTT}\\ \cline{4-5} \cline{6-7}
        \sc Method & \sc Batch & \sc Epoch & R@10 & MedR & R@10 & MedR \\
        \midrule
    MIL-NCE ~\cite{miech2020end} & 8192 & 27 & \textbf{51.2} & \textbf{10} & \textbf{32.4} & \textbf{30} \\
    MMV ~\cite{mmv} & 4096 & 8 & 45.4 & 13 & 31.1 & 38 \\
    \midrule
\ours-MBS & 2048 & 4 & 45.5 & 13 & 29.7 & 49 \\
    \midrule
    \ours-MA-Medium & 2048 & 4 & 40.6 & 17 & 23.6 & 67 \\
    \bottomrule
    \end{tabular}
    \vspace{2mm}
    \caption{Zero-shot text-to-video retrieval. We evaluate our text and video Transformers of the final pre-training checkpoint.}
    \label{table:retrieval} 
    \vspace{-2mm}
\end{table} 
Table~\ref{table:retrieval} compares our video retrieval results to two baselines. In our experiments we observed that the zero-shot retrieval results are heavily affected by the batch size and number of epochs, confirming the observation made in \cite{mmv}. That said, our model still delivers comparable results to MMV~\cite{mmv} while being pre-trained with a half number of epochs and a half batch size. We also experimented with a larger batch size 8192 and longer training for 6 epochs, arriving at exactly the same results as MIL-NCE~\cite{miech2020end} on YouCook2 and the R@10 of 29.2 and MedR of 42 on MSR-VTT. We also notice that, probably due to the noisy nature of text transcripts, a sophisticated language model like ours is underrated. As shown in \cite{mmv}, using a simple linear projection would still perform reasonably well. It is worth exploring other, higher-quality text sources in future work.

\subsubsection{Linear evaluation}
We also test \ours's ability to generalize to other datasets when the entire backbone is frozen. In this setting, we focus on the video and audio modalities and train a linear classifier on the outputs of the frozen backbones. In addition to the low-rank classifier (LRC) described in Section~\ref{sec:exp_setup}, we also report the results of a SVM classifier following the same pipeline as~\cite{mmv}. Table~\ref{table:linear-eval} shows the performance of our model on three datasets. We observe that \ours does not outperform the best CNN counterparts in~\cite{mmv}, and achieves comparable numbers to other baselines. This could suggest that \ours's backbones learn less-linearly-separable feature, especially given that the contrastive estimation head includes non-linear projections.

\begin{table}[ht]
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{@{}lccccc@{}}
    \toprule
        \sc Method & UCF101 & HMDB51 & ESC50\\
    \midrule
    MIL-NCE~\cite{miech2020end} & 83.4 & 54.8 & - \\
    AVTS~\cite{korbar2018cooperative} & - & - & 82.3 \\
    XDC~\cite{alwassel2019self} & - & - & 84.8 \\
    ELo~\cite{piergiovanni2020evolving} & - & 64.5 & - \\
    AVID~\cite{smith2019avid} & - & - & \textbf{89.2} \\
    GDT~\cite{patrick2020multi} & - & - & 88.5 \\
    MMV~\cite{mmv} & \textbf{91.8} & \textbf{67.1} & 88.9 \\
    \midrule
    \ours-Medium + SVM & 89.2 & 63.3 & 82.5\\
    \ours-Medium + LRC & 89.6 & 65.2 & 84.7\\
    \midrule
    \ours-MA-Medium + LRC & 84.4 & 63.1 & 81.2\\
    \bottomrule
    \end{tabular}
    }
    \vspace{2mm}
    \caption{Linear evaluation results for video action recognition on UCF101 and HMDB51 and audio event classification on ESC50. MA refers to the modality-agnostic backbone.}
    \label{table:linear-eval}
    \vspace{-4mm}
\end{table} 
\begin{figure}[t]
\centering
   \includegraphics[width=0.95\columnwidth]{figures/vt_tsne_inp_out.pdf}
   \caption{t-SNE visualization of the input space vs.\ output space for modality-specific and modality-agnostic backbones when different modalities are fed.}
\label{fig:tsne_inp_out}
\vspace{-2mm}
\end{figure}


\subsubsection{Feature visualization}
We take our modality-specific and modality-agnostic \ours fine-tuned on Kinetics-400 and visualize their output feature representations using t-SNE. For comparison, we also include the feature visualization of the vision Transformer trained from scratch on Kinetics-400. From Figure~\ref{fig:tsne_k400}, we observe that the fine-tuned \ours yields a much better separation than the model trained from scratch. Furthermore, it is worth noting that there is no clear difference between the modality-agnostic features and the modality-specific ones.


We further investigate the \ours backbones without any fine-tuning. We randomly choose 1k video clips from the YouCook2 dataset and store the representations from two points of our \ours model. One is after the tokenization layer (input space for the Transformer), and the other is after the common space projection (output space), where the loss is computed. Figure~\ref{fig:tsne_inp_out} visualizes the representations, comparing modality-specific \ours to modality-agnostic \ours. Interestingly, we observe that the representations are slightly more mixed together in the modality-agnostic setting compared to the modality-specific ones, implying that the modality-agnostic backbone sees different modalities as different symbols describing the same concept. This is analogous to a unified language model in NLP that supports multiple languages.

To see how well \ours distinguishes positive video-text pairs from randomly sampled pairs, we calculate pair-wise similarities for all possible pairs and perform a Kernel Density Estimation (KDE) to visualize the distributions of the similarities of the positive pairs vs.\ negative pairs. We perform this procedure for both input and output spaces of the modality-specific and modality-agnostic backbones. Figure~\ref{fig:sim_inp_out} shows the KDE curves of these similarities. We can clearly see that \ours in both settings separates the positive and negative pairs in its output space. This verifies \ours's efficacy in learning a semantic common space for different modalities, even if we share the backbone across modalities.

\begin{figure}[t]
\centering
   \includegraphics[width=0.95\linewidth]{figures/similarity_inp_out.pdf}
   \caption{Distributions of pair-wise similarities for modality-specific vs.\ modality-agnostic {\ours}s.}
\label{fig:sim_inp_out}
\vspace{-2mm}
\end{figure}

\subsection{Ablation study}
In this section we examine the effect of different parameters on the model performance and justify our design choices based on this observation.

\subsubsection{Inputs}
Since \ours takes raw multimodal signals as inputs, the choice of input size and how they are patched has a significant impact on the final performance. First, we alter the frame crop size and the number of sampled frames from each video clip while keeping the patch size fixed to .  Table~\ref{table:frame_patch} shows that using a small frame crop size and a larger number of frames hurts the video-related results, but it does not significantly change the audio classification numbers.

\begin{table}[h!]
    \small
    \centering
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{@{}lcccccc@{}}
    \toprule
    Video Size & UCF & HMDB & YC2 & MSRVTT & ESC \\
    \midrule
    32200200 & 87.16 & 67.08 & 23.98 & 17.84 & 86.25 \\
    32224224 & \textbf{87.74} & \textbf{67.6} & \textbf{27.47} & \textbf{17.96} & \textbf{87} \\
    64224224 & 86.57 & 63.09 & 18.52 & 12.5 & 86.25 \\
    \bottomrule
    \end{tabular}
    \vspace{2mm}
    \caption{Effect of video input size on downstream tasks.}
    \label{table:frame_patch}
    \vspace{-2mm}
\end{table} 
Then, we keep the best frame size from Table~\ref{table:frame_patch} and vary the video patch size. Table~\ref{table:video_patch} reports the results on downstream tasks. We find going beyond  along either the time or spatial dimensions is not helpful. We avoid patches that are smaller than  because of the significantly increaseed wall clock time in experiments.
\begin{table}[h!]
    \small
    \centering
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{@{}lcccccc@{}}
    \toprule
    Patch Size & UCF & HMDB & YC2 & MSRVTT & ESC \\
    \midrule
    41616 & 87.8 & \textbf{67.7} & \textbf{27.53} & \textbf{17.99} & 87 \\
    51616 & \textbf{88.4} & 67.02 & 26.45 & 16.83 & \textbf{88} \\
    81616 & 86.52 & 65.64 & 23.43 & 16.14 & 84 \\
    83232 & 82.68 & 60.73 & 15.27 & 13.79 & 87 \\
    \bottomrule
    \end{tabular}
    \vspace{2mm}
    \caption{Effect of video patch size on downstream results.}
    \label{table:video_patch}
    \vspace{-2mm}
\end{table} 
Finally, we compare different audio patch sizes and perform an experiment using spectrograms, as opposed to the raw waveforms, as audio input. The goal is to see how the raw waveforms compare to the handcrafted spectrograms. We use the MEL spectrogram with 80 bins, the STFT length of 42 ms, and the STFT step of 21 ms following a similar setup in~\cite{mmv}. Tables~\ref{table:autio_patch}~and~\ref{table:wave_spec} summarize the results, in which we observe that the patch size of 128 gives rise to the best waveform-based results, and using spectrogram does not lead to any conclusive improvement. The experiment with the spectrograms demonstrates that \ours is able to learn semantic representations from raw audios. To the best of our knowledge, this is the first time that raw audio waveforms are used for multimodal self-supervised learning.
\begin{table}[h!]
    \small
    \centering
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{@{}lcccccc@{}}
    \toprule
    Patch Size & UCF & HMDB & YC2 & MSRVTT & ESC \\
    \midrule
    128 & \textbf{88.14} & \textbf{68.13} & 25.72 & \textbf{17.31} & \textbf{87.75} \\
    256 & 87.74 & 66.1 & 24.19 & 16.55 & 83.75 \\
    512 & 87.21 & 67.34 & \textbf{26.11} & 16.91 & 82.5 \\
    1024 & 86.41 & 66.36 & 24.46 & 16.38 & 82.5 \\
    \bottomrule
    \end{tabular}
    \vspace{2mm}
    \caption{Effect of the audio patch size on downstream results.}
    \label{table:autio_patch}
    \vspace{-2mm}
\end{table} \begin{table}[h!]
    \small
    \centering
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{@{}lcccccc@{}}
    \toprule
    Audio & UCF & HMDB & YC2 & MSRVTT & ESC \\
    \midrule
    Spectrogram & \textbf{88.3} & 67.52 & \textbf{26.62} & 16.86 & \textbf{88} \\
    Waveform & 88.14 & \textbf{68.13} & 25.72 & \textbf{17.31} & 87.75 \\
    \bottomrule
    \end{tabular}
    \vspace{2mm}
    \caption{Effect of the audio representation on downstream results.}
    \label{table:wave_spec}
    \vspace{-2mm}
\end{table} 
\subsubsection{DropToken}
To study the effect of the proposed DropToken method on downstream applications and the pre-training computation, we perform pre-training by randomly dropping , , , and 0\% (no drop) of the tokens from the video and audio inputs. Table~\ref{table:drop-token-acc-pre-train} shows the accuracy of linear classification on HMDB51, UCF101, ESC50 vs.\ the drop rate along with GFLOPs during a forward call. We choose  sampling rate for our large-scale pre-training as it offers a good trade-off between accuracy and computational costs.
\begin{table}[h!]
    \small
    \centering
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{@{}lccccc@{}}
    \toprule
    & \multicolumn{4}{c}{DropToken Drop Rate} \\
    \cline{2-5}
     &  &  &  &  \\
    \midrule
    Multimodal GFLOPs & 188.1 & 375.4 & 574.2 & 784.8 \\
    \midrule
    HMDB51 & 62.5 & 64.8 & 65.6 & 66.4 \\
    UCF101 & 84.0 & 85.5 & 87.2 & 87.6 \\
    ESC50  & 78.9 & 84.1 & 84.6 & 84.9 \\
    \bottomrule
    \end{tabular}
    \vspace{2mm}
    \caption{Linear classification top-1 accuracy vs.\ sampling rate vs.\ inference GFLOPs in the Medium-Base-Small (MBS) setting.}
    \label{table:drop-token-acc-pre-train}
    \vspace{-2mm}
\end{table} 
We then take the final checkpoint of the pre-trained \ours with  DropToken rate and perform fine-tuning on Kinetics-400 at different DropToken rates and at different spatial and temporal resolutions to see how high-resolution inputs coupled with DropToken compare to low-resolution inputs with no tokens dropped during fine-tuning. Table~\ref{table:drop-token-acc-k400} shows the top-1 accuracy on Kinetics-400. We argue against using low-resolution inputs, which is the most common approach to reduce the computational cost during training. Instead, we suggest using high-resolution inputs with DropToken, whose accuracy and training cost are comparable to or better than low-resolution counterparts.
\begin{table}[h!]
    \small
    \centering
    \setlength{\tabcolsep}{5pt}
    \begin{tabular}{@{}l|ccccc@{}}
    \toprule
    Resolution/ & \multicolumn{4}{c}{DropToken Drop Rate} \\
    \cline{2-5}
    FLOPs &  &  &  &  \\
    \midrule
     & - & - & - & 79.9 \\
    Inference (GFLOPs) & - & - & - & 548.1 \\
    \midrule
     & - & - & - & 80.8 \\
    Inference (GFLOPs) & - & - & - & 1222.1 \\
    \midrule
     & 79.3 & 80.2 & 80.7 & 81.1 \\
    Inference (GFLOPs) & 279.8 & 572.5 & 898.9 & 1252.3 \\
    \bottomrule
    \end{tabular}
    \vspace{2mm}
    \caption{Top-1 accuracy of video action recognition on Kinetics400 using high-resolution inputs coupled with DropToken vs.\ low-resolution inputs.}
    \label{table:drop-token-acc-k400}
    \vspace{-2mm}
\end{table} 
\section{Conclusion}
In this paper, we present a self-supervised multimodal representation learning framework based on Transformer architecture. 
With pure attention-based model on multimodal video inputs, our study suggests that large-scale self-supervised pre-training is a promising direction to lighten the data burden for Transformer architectures and enables the Transformers to triumph Convolutional Neural Network (CNN) on various downstream tasks. 
A simple yet effective technique, DropToken, is proposed to solve the quadratic training complexity with respect to the input length of attention based model, making it more approachable for vision and raw audio processing.
Achieving state-of-the-art in video action recognition and audio event classification, and also competitive performance on image classification and video retrieval tasks also showed the great generalizability and transferrability of our learned representations by self-supervised learning across different modalities.
We hope our work make one step further in leveraging the strong expressiveness of the Transformer-based model for multimodal understanding and open the door for developing the grand multimodal model.
In the future, we plan to study data augmentation techniques to train the Transformer and how to properly regularize the model, especially the modality-agnostic backbone, for diverse multimodal tasks.

\paragraph{Acknowledgements:} We would like to thank Min-Hsuan Tsai, Jean-Baptise Alayrac, Andrew Audibert, Yeqing Li, Vidush Mukund, and the TensorFlow team for their help with codes, infrastructure, and insightful discussions.



{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
