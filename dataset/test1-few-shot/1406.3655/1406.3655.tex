\documentclass{llncs}

\pagestyle{plain}

\let\proof\relax
\let\endproof\relax

\usepackage{amsmath,amsthm}
\usepackage{amssymb}
\usepackage{comment}


\begin{document}

\title{Trading off Worst and Expected Cost in Decision Tree Problems and a Value Dependent Model}
\author{Aline Saettler\inst{1} \and Eduardo Laber\inst{1}  \and Ferdinando Cicalese\inst{2}}
\institute{PUC-Rio, Brazil\\\email{\{asaettler,laber\}@inf.puc-rio.br} \and University of Salerno, Italy\\\email{cicalese@dia.unisa.it}}
\maketitle


\begin{abstract}
We study the problem of evaluating a discrete function by adaptively querying the values of its variables until the values read uniquely determine the value of the function. Reading the value of a variable is done at the expense of some cost, and the goal is to design a strategy (decision tree) for evaluating the function incurring as little cost as possible in the worst case or in expectation (according to a prior distribution on the possible variables assignments). Except for particular cases of the problem, in general, only the minimization of one of these two measures is addressed in the literature. However, there are instances of the problem for which the minimization of one measure leads to a strategy with a high cost with respect to the other measure (even exponentially bigger than the optimal).  We provide a new construction which can guarantee a trade-off between the two criteria. More precisely, given a decision tree guaranteeing expected cost  and a decision tree guaranteeing worst cost  our method can guarantee for any chosen trade-off value  to produce a decision tree whose worst cost is  and whose expected cost is
 These bounds are improved for the  relevant case of uniform testing costs.

Motivated by applications, we also study a variant of the problem where the cost of reading a variable depends on the variableâ€™s value.
We provide an  approximation algorithm for the minimization of the worst cost measure, which is best possible
under the assumption .
\end{abstract}


\newcommand{\remove}[1]{}



\section{Introduction}

Decision tree construction is a central problem in several areas of computer science, e.g., in data base theory, in computational learning and in artificial intelligence in general. 
In a typical scenario there are several possible hypotheses, which can explain some unknown phenomenon and we want to decide which hypothesis provides the 
correct explanation. We have a prior distribution on the hypotheses and we can use tests to
discriminate among the hypotheses. Each test's outcome eliminates some of the hypotheses, and the set of tests 
is complete, in the sense that by using all the tests we can definitely find the correct hypothesis. Moreover, different tests may have different associated costs. 
The aim is to define the best testing strategy that allows to reach the correct decision spending as little as possible	. 
If the testing is adaptive a strategy is representable by a tree (called decision tree) with each node being a test and each leaf being a hypothesis. 
In a generalization of this scenario, one is only interested in identifying a class of possible hypothesis explaining the situation. 

In an example of automatic diagnosis, the hypotheses
 are possible diseases and we look for the testing strategy (decision tree) which can always identify the disease by using a cheap sequence of tests. 
In the case we are interested in deciding the drug to administer to the patient rather than exactly identifying the disease we have an instance of the more general variant 
of the decision tree construction where we are looking for the class of hypotheses containing the correct explanation. 

What is the right measure to optimize when constructing the  decision tree? Usually, the expected cost of the tests needed to reach the correct decision and
the maximum total cost needed to reach the correct decision are used. However, these measures can lead to very different trees and in 
particular it is possible that the decision tree minimizing one measure is very inefficient with respect to the other measure. A very skewed distribution can induce
 a tree optimizing the expected cost with a very skewed shape. As a consequence, in such a tree some decision might induce a very high cost, even exponentially bigger than 
 the worst cost spent by a strategy that optimizes with respect to the worst case. Conversely optimizing with respect to the worst case can lead to very 
 bad expected cost. The choice of which measure to choose is crucial especially since in practical applications the real distribution might not be known but only 
 estimated and possibly be wrong. Therefore, it might  be preferable to have decision trees which while optimizing one criteria guarantees to be 
 efficient with respect to the other. 
 
In this paper, we address the issue regarding the existence of a trade-off between the minimization 
of the worst testing cost and the
expected testing cost of decision trees. Is it is possible to 
construct decision trees that are efficient with 
respect to both measures? As mentioned before, these two goals can be incompatible.


\remove{
We present a polynomial time procedure
 that given a parameter  and  two decision trees
 and , the former  with worst testing cost  and the latter with expected testing cost ,  produces
a decision tree  with worst testing cost at most  and expected
testing cost at most . In words, the combined decision tree loses at most small constant factor with respect to the 
performance of both the original decision trees which were optimizing with respect to only one measure.  
For the case of uniform testing costs,
the bound can be improved to  and  through a more involved analysis. 
}
 
The second issue on which we focus in this paper is the way the cost of the tests is defined. 
We refer the interested reader to \cite{Turney} and references quoted therein for a 
remarkable account of several types of costs to be taken into account in inference procedures. 
In most decision tree problems, the assumption is that the cost of the tests is fixed in advance and known to the algorithm. In particular, 
the cost is independent of the outcome of the test. However, there are also several scenarios in medical 
applications---one of the main fields motivating automatic diagnosis---where the assumption that a test has a fixed cost independent 
of the outcome of the test does not apply. Many diagnostic tests  actually consist of a multi-stage procedure, e.g.,  
in a first stage the sample is tested against some reagent to check for the presence or absence of an antigene. If this appears to be present below a certain level 
the test is considered to be negative and no further analysis is performed. Otherwise, the test is {\em necessarily} followed by a second stage 
where several new reagents are used with  significantly higher final costs. Notice that in such a situation there is no real decision left to the strategy between the first and the second stage, so it is reasonable to consider such a two stage procedure as a single test whose cost depends on the outcome. 


Value dependent test costs are also useful in application where disruptive tests are used.
 Consider the use of bacterial colonies or {\em caviae} to test for toxicity of a samples. In the case no toxicity is found, the testing colony can be reused, as opposed to the case where toxicity is verified leading to the disruption of the colony or the death of the 
{\em cavia} (a similar model has been studied in \cite{Elser-Kleber}). Analogously, a chemical reagent might be used for performing a test and the outcome of the test is either some chemical reaction changing the nature of the reagents and making them unusable again, 
or the absence of the reaction in which case the reagent can be (partially) reused. 
Again we have a test that when positive has higher cost---the necessity of buying new reagents---than in the case of a negative outcome.



For this extended version, where the cost of a test may depend on its outcome, 
we present an algorithm for building a decision tree that aims to  minimize the worst testing cost for 
identifying the class of the correct hypothesis.




 
\subsection{Problem Formalization} 
\noindent
{\bf The Discrete Function Evaluation Problem} (DFEP). Our results are presented in terms of the problem of evaluating 
a discrete function. This problems generalizes most decision tree construction problems studied in the literature. 




An instance of the problem is defined by  a quintuple   
where  is a set of objects, 
 is a partition of  into  classes,  is a set of tests,  is a probability distribution on 
 and  is a cost function assigning to each test  a cost . 


A test ,  when applied to an object  , outputs a number  in the set   and incurs a cost .
It is assumed that the set of tests is complete, in the sense that for any distinct  there exists a test
 such that  
The goal is to define a testing procedure which uses tests from  and minimizes the testing cost 
(in expectation and/or in the worst case)
for identifying the class of an unknown object  chosen according to the distribution  
We also work with the extended version of the DFEP where the cost of a test
is a function that assigns each pair (test , object ) to a value .



The DFEP can be rephrased in terms of minimizing the cost of evaluating a discrete function that 
maps points (corresponding to objects) from some finite subset of  into
values from  (corresponding to classes), where  each object  corresponds to the point  
 which is obtained by applying each test of   to .  
This perspective motivates the name we chose for the problem. However, for the sake of uniformity with more recent work \cite{golovin,bellala} we employ the definition of the problem in terms of objects/tests/classes.


\medskip



\noindent
{\bf Decision Tree Optimization.} Any testing procedure can be represented by a \emph{decision tree}, which is a tree
where every internal node is associated with a test and every leaf is associated with a set
of objects that belong to the same class.
More formally, a decision tree  for   is a leaf
associated with class  
if every object of  belongs to the same class . Otherwise, the root   of  is associated with some test  and  
the children of  are  decision trees for the non empty sets in ,
where  is the subset of  that outputs  for test .
 
Given a decision tree , rooted at , we can identify the class of an unknown object 
 by following a path from    to a leaf as follows:
first, we ask for the result of the test associated with  when performed on ; then,  
we follow the branch of  associated with the result of the test to
reach a child  of ; next,  we apply the same steps recursively for the decision tree rooted at .
The procedure ends when a leaf is reached, which determines the class of .

We define  as the sum of the tests' cost on 
the root-to-leaf path from the root of 
 to the leaf associated with object . 
Then, the \emph{worst testing cost}  and the \emph{expected testing cost} of  are, respectively, defined as





\begin{comment}

An important aspect of this formulation is that here the costs are associated with both the tests and 
its outcomes. Previous works consider a particular case of this problem in which 
 (i. e., the cost depends only on the test).


\end{comment}











\subsection{Our Results}
 We present a polynomial time procedure
 that given a parameter  and  two decision trees
 and , the former  with worst testing cost  and the latter with expected testing cost ,  produces
a decision tree  with worst testing cost at most  and expected
testing cost at most . For the relevant case of uniform costs,
the bound can be improved to  and  through a more involved analysis. 

In addition, we 
present an algorithm 
for the minimization of  the worst testing cost for 
the extended version of the  where the cost of a test  depend on its outcome. 
We prove that our  algorithm is  an  approximation for the 
case of binary tests.  This bound 
 is the best possible
under the assumption that .




\subsection{Related work}

In a recent paper \cite{labericml}, the authors
show  that for any instance  of the
DFEP, with  objects, it is possible to construct in polynomial time a decision tree
 such that  is 
and  is ,
where  and  are,
respectively, the minimum expected  testing cost
and the minimum worst testing cost for instance . 

Note that the questions we are studying here are different and possibly more fundamental than those studied in \cite{labericml}: is it possible, even
allowing exponential construction time, to build a decision tree whose expected cost is very close to the best possible expected cost achievable
and whose worst testing cost is very close to the best possible worst case achievable? How close can we get or better what is the best trade off
we can simultaneously guarantee?




For the prefix code problem
there are some studies related to the  simultaneous minimization
of  the expected testing cost and the worst case testing cost \cite{garey,larmore,fastalg,laber}.
The problem of constructing a prefix code 
is a particular case of the DFEP in which each object belongs to a distinct class, the testing costs are uniform 
and the set of tests is in one to one correspondence with the set of all binary
strings of length  so that the test corresponding to a binary
string  outputs 0 (1) for object  if and only if the
 bit of  is 0 (1).

A number of algorithms with different  time complexities
were proposed to 
construct  decision trees with minimum expected path length (expected testing cost in DFEP terminology)
among the decision trees with depth (worst testing cost) at most ,
where  is a given integer \cite{garey,larmore,fastalg}.

The results of Milidiu and Laber \cite{laber} imply
that for any instance   of the the prefix code problem,
 there is a decision tree  such that for any integer  ,
with ,
 and
,
where  is the golden ratio .










 





When the goal is to minimize only one 
measure (worst or expected testing cost), there are several 
algorithms in the literature to solve the particular version of the  in which 
each object belongs to a distinct class 
(\cite{garey_id,Kos_id,pandit_id,adler_id,guillory_id,laber_id,guillory2,gupta}).
 Approximation algorithms for the general version of the problem, where the number of classes  can be smaller than the number of objects, were presented by
   \cite{bellala}, \cite{golovin} and \cite{labericml}. 
For the minimization of the worst testing cost of DFEP, Moshkov  has studied the problem 
in the general case of multiway tests and non-uniform costs and provided 
an -approximation in \cite{Moshkov2}. 
Our algorithm in Section 3, generalizes Moshkov's algorithm to the value-dependent-test-cost variant of the DFEP
Moshkov \cite{Moshkov2} also proved  that
that 
no -approximation algorithm is possible under the standard  complexity assumption
 The minimization of the worst testing cost is also investigated in 
\cite{conf/icml/GuilloryB11}  under the framework of covering and learning.
	Both \cite{bellala} and \cite{golovin} show 
  approximations for the expected testing cost (where  is the minimum probability among the 
 objects in ) \---- the former for binary tests, and the latter for multiway 
 tests.  












\section{Preliminaries and notation}
\label{sec:prelim}

In order to explain our results, we use  and , respectively, to denote the cost
of the decision tree with minimum worst testing cost
and  minimum expected testing cost for the input .
Whenever the context permits (it will always permit) we  use the simpler notations
 and .


Let  be an instance
of DFEP and let   be a subset of .
In addition, let  and  be, respectively, the restrictions of  and 
to the set .
Our first observation is that every decision tree
 for   is also a decision tree
for .
The following proposition is a direct consequence of this  observation.

\begin{proposition}
\label{prop:Subadditivity}
Let  be an instance
of the DFEP and let   be a subset of .
Then,  and
.
\end{proposition}

We say that a pair of objects   from a set  
is {\em separable} if  and  belong to different classes.
For a set of objects  we use 
 to denote the number of separable pairs in
. In formulae,

where  is the number of objects in  that belong to class .
We say that a test  {\em separates} a pair of separable objects 
if .


\section{A logarithmic approximation for value dependent testing costs}\label{sec:binary}

We first consider the goal of approximating optimal decision trees with respect to the 
worst testing cost. Recall that if we apply a test  
on an object , getting an answer , we pay a cost . Thus, each test can be associated with 
 different costs since . Note that now each 
branch of a decision tree is associated with a cost, while in the classical version of the problem each internal node is associated 
with a cost.


Our algorithm, called \textsc{DividePairs}, chooses the test  that minimizes:


over all available tests for the root of the tree. Then the objects in  are splitted according to the values of  for each object, 
and \textsc{DividePairs} is recursively called for each (non empty) new group of objects. When all objects in a group are from the same class, a leaf is created. We analyze the approximation of the algorithm when  . 
Recall that we use  to denote  
 the subset of objects of  for which test  outputs
.

In this case, each test  splits  in two subsets:  and .


In order to analyze the algorithm, 
we use  to denote the cost of the decision tree that \textsc{DividePairs} constructs
for a set of objects .
Let  be the first test selected by \textsc{DividePairs}.
  We can write
 the ratio between the worst testing cost of the decision tree generated by
 \textsc{DividePairs} and the cost of the decision tree with minimum worst testing cost as



Let  be such that  in equation 
(\ref{eqratio}). We have that:


where the inequality follows from Proposition \ref{prop:Subadditivity}. The 
following lemma shows  that  is at least .

\begin{lemma}
 is a lower bound on the worst testing cost of the optimal tree.
\end{lemma}

\textit{\textbf{Proof:}} First, we note 
that in the set of decision trees with optimal worst testing cost, 
there is a tree  in which every internal node has two children. 
Let  be an arbitrarily chosen internal node in ,
let  be the test associated with  and let
 be the set of objects associated with the leaves of the subtree rooted at .
Let  be such that  is maximized and  be such that  
is maximized.
We have that:
	


The last inequality in (\ref{eqgreedy1}) holds due 
to the greedy choice. To prove inequality (\ref{eqgreedy2}),
we only have to show that .  
Let  (resp. ) be  
the number of  pairs 
in  (resp.\ ) separated by test .
Since   we have that  and
 for .
Also, note that:





Hence, we have that . Thus, we have concluded that 
inequality (\ref{eqgreedy2}) holds. 
	
For a node , let  be the set of objects
associated with the leaves of the subtree rooted at . 
Let  be a root-to-leaf path on  as follows:
 is the root of the tree, and for each  the node  is a child of 
  associated with the branch  that maximizes , where  is the test associated with .  
 We denote by  the cost that we have to pay going from  to .
It follows from inequaltity (\ref{eqgreedy2})
that  

for .
Since the cost of the  path from  to  is not larger than the worst testing cost of
the optimal decision tree,
we have that


where the second inequality follows from (\ref{eqaux})
and the last identity holds because  and .


Replacing the bound on 

given by the previous lemma in  equation (\ref{eqratio2}) we get that



Note that:



By induction on the number of  pairs,  we assume  that for each , , where 
. 
From (\ref{eq:ratio_lb}) and (\ref{eq:firstterm}) we have that


Thus,
we have the following theorem

\begin{theorem}
There is an  approximation for version of the DFEP with binary tests and  value dependent
costs.
\end{theorem}

\section{A bicriteria approximation}
\label{sec:bicriteria}
In this section,
we present an algorithm that provides a simultaneous approximation
for the minimization of expected testing cost and worst testing cost.
 There are examples in which the minimization of the expected testing
cost produces a decision tree with high worst testing cost, and the 
minimization of the worst testing
cost produces a decision tree with high expected testing cost \cite{labericml}.
Therefore, it makes sense to look for a trade-off between minimizing
both measures.




Given a positive number , two decision trees  and  for the instance ,
 the former  with expected testing cost   and the latter with worst testing cost  , we devise a polynomial time procedure 
 to construct a new decision tree , from  and , with
 expected cost at most  and
  worst testing cost at most .
The procedure is very simple:

\medskip

{\tt CombineTrees}(,,)


\begin{enumerate}


\item Define a node  from  as replaceable
if the cost of the path from the root of  to   (including ) is
at least  and the cost of the path from the root of  to the parent of 
is smaller than . At this step we traverse  to 
find the set   of the replaceable nodes.

\item For every node  do 

\begin{enumerate}

\item Let    be  the set of objects associated with leaves 
located at the subtree rooted at  in . In addition, let 
 be a decision tree for  obtained
by disassociating every object in  from .

\item Replace the subtree of  rooted at  with the decision tree


\end{enumerate}

\item Return the tree   obtained by the end of Step 2. 

\end{enumerate}

\normalsize


\begin{theorem}
The decision tree  has expected testing cost at most  and worst testing cost at most .
\end{theorem}
\begin{proof}
First we argue that the worst testing cost of  is at most
.
Let  be an object in .
If  is not a descendant of a replaceable node in  then
the cost of the path from the root of  to  is at most .
Since this path remains the same in  ,
we have that the cost to reach  in  is at most .
On the other hand, if  is  a descendant of a replaceable node  in , then
the cost to reach  in  is at most  because the cost of the path from the root of   to the parent of  is at 
 most  and the cost to reach  from the root of the tree  is at most
.

Now, we prove that the expected testing
 cost of  is at most
.
For that it is enough 
to show that for every object , the cost
to reach  in  is at most  times the cost of
reaching  in .
We split the analysis into two cases:

{\bf Case 1.}
  is not a descendant of a replaceable node in . In this case, the cost
to reach  in   is equal to the cost of reaching  in .

\medskip
{\bf Case 2}.   is  a descendant of a replaceable node  in .
Let  be the cost of the path from the root of  to  . 
Then, the cost  to reach  in  
is at least .   In addition, since  is replaceable we have that
 .
On the other hand, the cost to reach  in 
 is at most . 
Since  we have that the cost
to reach  in  is at most  times the cost of
reaching  in .
\end{proof}


We can improve the approximation for 
the case where the costs are uniform.
In this case, we can assume unitary testing costs so that   is the height
of the decision tree .
Let   and , with , be two positive integers whose values  will be defined during our analysis.

To obtain a better approximation,  we consider an algorithm that picks the decision tree, say  , with minimum expected testing cost among
the decision trees , where  is the decision tree returned
by {\tt CombineTrees} when it is executed
with parameters .
It follows from the previous theorem that 


The analysis of the  expected testing cost of  is more involved.
First,  we have that 



Let  be the height
of the decision tree .
For , let 
   be the contribution of the leaves located at  level  for
  the cost of  so that .
  It follows that 
 
because the objects associated with leaves that are located at levels smaller than
or equal to  are not modified from  to   while the remaining objects are located at levels
smaller than or equal to  in . Note that  in the previous inequality is the sum of the probabilities
of the leaves at level .
By replacing the last
expression in (\ref{eq:ub-uniformcosts}) and
grouping the terms around the 's we get that 





where

 




First, note that the maximum of  in the range  is
, which is attained when .
Moreover, if we replace  in the formula 
of  for  the range  we get exactly 
. Thus, 
it follows that 

By simple calculus we can conclude that the expression attains 
the maximum when .
Thus,




To verify the last  inequality we need
to do some calculations (squaring the terms) and use the fact that  .

Let  be a number in the interval
.
We can verify that
the righthand side of the  equation (\ref{eq:finalbound}) is upper bounded
by  
whenever     and , where  (the proof is presented in the appendix). 

Thus, by setting      and
, where
  is a positive number that can be written as 
for some integer ,
we obtain  the following theorem.


\begin{theorem}
Let 
an instance of the DFEP where all the tests have unitary  costs.
Given  two decision trees  and  for the instance ,
 the former  with expected testing cost   and the latter with height   and a positive number  that
can be written as  for some integer , there exists
 a polynomial time algorithm that construts a  decision tree  with  height at most 
 and  expected testing cost at most  
 \end{theorem}

As an  example, for  this new algorithm guarantees that the 
expected testing cost is at most  while
the initial algorithm guarantees a  upper bound.

\section{Conclusions}

We presented a polynomial time procedure that given a parameter , a decision tree 
 with worst testing cost  and a decision tree  with expected testing cost , produces
a decision tree  with worst testing cost at most  and expected
testing cost at most . When the costs are uniform,
the bound can be improved to  and . 
The main question that remains open in this topic is whether for 
every , there is some integer  such that
every instance  of the DFEP with more than
 objects admits a tree  such that  and 
.
For the prefix code problem, a particular version of the DFEP explained  in the introduction, this result
holds \cite{laber}.



We also presented an approximation algorithm for the 
extended version of the DFEP where the 
cost of the tests  depend also on the answers.
For the particular case where the tests are binary, our algorithm provides a logarithmic approximation which is the best
approximation unless .
An interesting question that deserves more investigation is if there exists also a logarithmic approximation algorithm for the most general 
case where the  tests can output more than two values.


\bibliographystyle{abbrv}
\bibliography{esa2014}


\appendix

\section{Calculation of Section \ref{sec:bicriteria}}

Let  be a number in the interval . We have to prove that:



By simple algebraic manipulations we conclude that we have to prove that



or equivalently,

 

Replacing  and using the fact that , it suffices to show

 



 

 


 


 
  


This can be shown by verifying that the following inequalities hold:




and




\end{document}
