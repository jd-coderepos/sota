\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}


\usepackage{color}
\usepackage{xcolor}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{csvsimple}
\usepackage{array}
\usepackage{dblfloatfix}
\usepackage [english]{babel}
\usepackage [autostyle, english = american]{csquotes}
\MakeOuterQuote{"}
\usepackage{subcaption}

\newcommand{\todo}[1]{ {\color{red} \textbf{TODO: #1}}}
\newcommand{\red}[1]{ {\color{red} #1}}
\newcommand{\tbd}[1]{ {\color{blue} \textbf{TBD: #1}}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\norm}[1]{\vert \vert#1\vert \vert}
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\class}[1]{\mathrm{class}(#1)}
\newcommand{\Lwalker}{\mathcal{L_{\mathrm{walker}}}}
\newcommand{\Lvisit}{\mathcal{L_{\mathrm{visit}}}}
\newcommand{\Lclass}{\mathcal{L_{\mathrm{classification}}}}
\newcommand{\LclassS}{\mathcal{L_{\mathrm{classification}}^{{\mathrm{source}}}}}
\newcommand{\LclassT}{\mathcal{L_{\mathrm{classification}}^{{\mathrm{target}}}}}
\newcommand{\Lassoc}{\mathcal{L_{\mathrm{assoc}}}}
\newcommand{\Lsim}{\mathcal{L_{\mathrm{sim}}}}
\newcommand{\DAassoc}{\mathrm{DA}_{\mathrm{assoc}}}
\newcommand{\DAmmd}{\mathrm{DA}_{\mathrm{MMD}}}
\renewcommand{\vec}[1]{\bm{#1}}
\newcommand{\specialcell}[2][c]{\begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}
\newcommand{\sprod}[2]{\left\langle #1,#2 \right\rangle}
\newcommand{\abs}[1]{\vert #1 \vert}
\newcommand*{\defeq}{\mathrel{\vcenter{\baselineskip0.5ex \lineskiplimit0pt
                     \hbox{\scriptsize.}\hbox{\scriptsize.}}}=}
                     \definecolor{dgreen}{rgb}{0.0,0.6,0.0}
\definecolor{dred}{rgb}{0.6,0.0,0.0}
\definecolor{darkgreen}{cmyk}{0.90,0.30,0.95,0.30}

\graphicspath{{./figures/}}



\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\addto\extrasenglish{\renewcommand{\sectionautorefname}{Section}\renewcommand{\subsectionautorefname}{Section}}
\hypersetup{citecolor=red} \hypersetup{urlcolor=black}
\hypersetup{linkcolor=dgreen} \hypersetup{pdftitle={Associative Domain Adaptation}, pdfauthor={Philip Haeusser, Thomas Frerix, Daniel Cremers}, 
            pdfsubject={ICCV 2017 Submission (arXiv.org version)}, 
            pdfkeywords={deep learning, semi-supervised, training, classification, domain adaptation}} 

\iccvfinalcopy 

\def\iccvPaperID{1052} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\begin{document}
\title{Associative Domain Adaptation}

\author{Philip Haeusser\\
{\tt\small haeusser@in.tum.de}
\and
Thomas Frerix\\
{\tt\small thomas.frerix@tum.de}
\and
Alexander Mordvintsev\\
{\tt\small moralex@google.com}
\and
Daniel Cremers\\
{\tt\small cremers@tum.de}
\and
Dept. of Informatics, TU Munich\\
\and
Google, Inc.
}

\maketitle



\begin{abstract}
We propose \emph{associative domain adaptation}, a novel technique for end-to-end domain adaptation with neural networks, the task of inferring class labels for an unlabeled target domain based on the statistical properties of a labeled source domain.
Our training scheme follows the paradigm that in order to effectively derive class labels for the target domain, a network should produce statistically domain invariant embeddings, while minimizing the classification error on the labeled source domain. 
We accomplish this by reinforcing \textnormal{associations} between source and target data directly in embedding space. 
Our method can easily be added to any existing classification network with no structural and almost no computational overhead. 
We demonstrate the effectiveness of our approach on various benchmarks and achieve state-of-the-art results across the board with a generic convolutional neural network architecture not specifically tuned to the respective tasks. Finally, we show that the proposed association loss produces embeddings that are more effective for domain adaptation compared to methods employing maximum mean discrepancy as a similarity measure in embedding space.  
\end{abstract}



\vspace{-\baselineskip}  \section{Introduction} \label{sec:introduction}
Since the publication of LeNet \cite{LeCun1998} and AlexNet \cite{Krizhevsky2012}, a methodological shift has been observable in the field of computer vision. Deep convolutional neural networks have proved to solve a growing number of problems \cite{szegedy2013deep, Eigen-et-al-14, toshev2014deeppose, szegedy2015going, dosovitskiy2015flownet, mayer2016large}. On the downside, due to a large amount of model parameters, an equally rapidly growing amount of labeled data is needed for training, such as ImageNet \cite{Russakovsky2015}, comprising millions of labeled training examples. This data may be costly to obtain or even nonexistent.

In this paper, we focus on an approach to train neural networks with a minimum of labeled data: domain adaptation.
We refer to domain adaptation as the task to train a model on labeled data from a source domain while minimizing test error on a target domain, for which no labels are available at training time.

\begin{figure}[t!]
	\centering
    \includegraphics[width=1.\linewidth]{teaser.png}
    \caption{Associative domain adaptation. In order to maximize classification accuracy on an unlabeled target domain, the discrepancy between neural network embeddings of source and target samples (red and blue, respectively) is reduced by an associative loss (), while minimizing a classification error on the labeled source domain.}
	\label{fig:teaser}
\end{figure}

\subsection{Domain adaptation}
\label{sec:domain_adaptation}
In more formal terms, we consider a source domain  and a target domain .
Here,  are the data vectors and  the respective labels, where the target labels  are \textit{not} available for training.
Note that for domain adaption it is assumed that source and target domains are associated with the same label space, while  and  are drawn from distributions  and , which are assumed to be \textit{different}, i.e. the source and target distribution have different joint distributions of data  and labels , .


The value of domain adaptation has even more increased with generative tools producing synthetic datasets.
The idea is compelling: rather than labeling vast amounts of real-world data, one renders a similar but synthetic dataset that is automatically labeled. With an effective method for domain adaptation it becomes possible to train models without the need for one single labeled target  example at training time. 

In order to combine labeled and unlabeled data for a predictive task, a variety of notions has emerged.  To be clear, we explicitly distinguish \emph{domain adaptation} from related approaches.
For semi-supervised learning, labeled source data is leveraged by unlabeled target data drawn from the \textit{same} distribution, i.e. .
In transfer learning, not only source and target domain are drawn from different distributions, also their label spaces are generally different.
An example of supervised transfer learning is training a neural network on a source domain and subsequently fine-tuning the model on a labeled target domain for a different task \cite{Yosinski2014, Donahue2014}.

The problem of domain adaptation was theoretically studied in \cite{Ben-David2010}, relating source and target error with a statistical similarity measure of the respective domains.
Their results suggest that a good domain adaptation method should be based on features that are as similar as possible for source and target domain (\emph{assimilation}), while reducing the prediction error in the source domain as much as possible (\emph{discrimination}).
These effects are opposing each other since source and target domains are drawn from different distributions. This can be formulated as a cost function that consists of two terms:



Here, the classification loss,  encourages discrimination between different classes, maximizing the margin between clusters of embeddings that belong to the same class.
We define the second term as a generic similarity loss , which enforces statistically similar latent representations.

Intuitively, for similar latent representations of the source and target domain, the target class labels can be more accurately inferred from the labeled source samples.

In the following, we show how previous methods approached this optimization and then propose a new loss for .

\subsection{Related work} \label{sec:related_work}
Several works have approached the problem of domain adaptation.
Here, we mainly focus on methods that are based on deep learning, as these have proved to be powerful learning systems and are closest to our scheme.

The CORAL method \cite{Sun2016b} explicitly forces the covariance of the target data onto the source data (\emph{assimilation}).
The authors then apply supervised training to this transformed source domain with original labels (\emph{discrimination}).
This idea is extended to second order statistics of features in deep neural networks in \cite{Sun2016}.

Building on the idea of adversarial training \cite{Goodfellow2014a}, the authors of \cite{Ganin2015a} propose an architecture in which a class label and a domain label predictor are built on top of a general feature extractor. 
While the class label predictor is supposed to correctly classify the labeled training examples (\emph{discrimination}), the domain label predictor for all training samples is used in a way to make the feature distributions similar (\emph{assimilation}).
The authors of \cite{Bousmalis2016b} use an adversarial approach to train for similarity in data space instead of feature space.
Their training scheme is closer to standard generative adversarial networks \cite{Goodfellow2014a}, however, it does not only condition on noise, but also on an image from the source domain.
 
Within the paradigm of training for domain invariant features, one popular metric is the maximum mean discrepancy (MMD) \cite{Gretton2012}.
This measure is the distance between the mean embeddings of two probability distributions in a reproducing kernel Hilbert space  with a characteristic kernel .
More precisely, the mean embedding of a distribution  in  is the unique element  such that .
The MMD distance between source and target domain then reads .
In practice, this distance is computed via the kernel trick \cite{Vapnik1995}, which leads to an algorithm with quadratic runtime in the number of samples.
Linear time estimators have previously been proposed \cite{Long2015a}.

Most works, which explicitly minimize latent feature discrepancy, use MMD in some variant. That is, they use MMD as  in order to achieve \emph{assimilation} as defined above.
The authors of \cite{Long2015a} propose the Deep Adaptation Network architecture.
Exploiting that learned features transition from general to specific within the network, they train the first layers of a CNN commonly for source and target domain, then train individual task-specific layers while minimizing the multiple kernel maximum mean discrepancies between these layers. 

The technique of task-specific but coupled layers is further explored in \cite{Rozantsev2016} and \cite{Bousmalis2016}.
The authors of \cite{Rozantsev2016} propose to individually train source and target domains while the network parameters of each layer are regularized to be linear transformations of each other.
In order to train for domain invariant features, they minimize the MMD of the embedding layer.
On the other hand, the authors of \cite{Bousmalis2016} maintain a shared representation of both domains and private representations of each individual domain in their Domain Separation architecture.

As becomes evident in these works, the MMD minimizes domain discrepancy in some abstract space and requires a choice of kernels with appropriate hyperparameters, such as the standard deviation of the Gaussian kernel. In this work, we propose a different loss for  which is more intuitive in embedding space, less computationally complex and better suitable to obtain effective embeddings.

\subsection{Contribution}
We propose the association loss  as an alternative discrepancy measure () within the domain adaptation paradigm described in \autoref{sec:domain_adaptation}.
The reasoning behind our approach is the following: Ultimately, we want to minimize the classification error on the target domain . This is not directly possible since no labels are available at training time. Therefore, we minimize the classification error on the source domain  as a proxy while enforcing representations of  to have similar statistics to those of . This is accomplished by enforcing \emph{associations} \cite{Haeusser2017} between feature representations of  with those of  that are in the same class.
Therefore, in contrast to MMD as , this approach also leverages knowledge about labels of the source domain and hence avoids unwanted \emph{assimilation} across class clusters.
The implementation is simple yet powerful as we show in \autoref{sec:domain_adaptation_by_associative_learning}. It works with any existing architecture and, unlike most deep learning approaches for domain adaptation, does not introduce a structural and almost no computational overhead. 
In fact, we used the same generic and simple architecture for \emph{all} our experiments, each of which achieved state-of-the-art results.

In summary, our contributions are:
\begin{itemize}
    \item A straightforward training schedule for domain adaptation with neural networks. 
    \item An integration of our approach into the prevailing domain adaptation formalism and a detailed comparison with the most commonly used explicit : the maximum mean discrepancy (MMD).    
    \item A simple implementation that works with arbitrary architectures\footnote{\url{https://git.io/vyzrl}}.
    \item Extensive experiments on various benchmarks for domain adaptation that outperform related deep learning methods.
    \item A detailed analysis demonstrating that associative domain adaptation results in effective embeddings in terms of classifying target domain samples.
\end{itemize}

\section{Associative domain adaptation}\label{sec:domain_adaptation_by_associative_learning}
We start from the approach of learning by association \cite{Haeusser2017} which is geared towards semi-supervised training. 
Labeled and unlabeled data are related by associating their embeddings, i.e. features of a neural network's last layer before the softmax layer.
Our work generalizes this approach for domain adaptation.
For the new task, we identify labeled data with the source domain and unlabeled data with the target domain.
Specifically, for  and the embedding map  of an -layer neural network, denote by  the respective embeddings of source and target domain.
Then, similarity is measured by the embedding vectors' dot product as .

If one considers transitions between the parts  of a bipartite graph, the intuition is that transitions are more probable if embeddings are more similar. This is formalized by the transition probability from embedding  to embedding :


The basis of associative similarity is the two-step round-trip probability of an imaginary random walker starting from an embedding  of the labeled source domain and returning to another embedding  via the (unlabeled) target domain embeddings ,



The authors of \cite{Haeusser2017} observed that higher order round trips do not improve performance.
The two-step probabilities are forced to be similar to the uniform distribution over the class labels via a cross-entropy loss term called the \textit{walker loss},

where


This means that all association cycles within the same class are forced to have equal probability. The walker loss by itself could be minimized by only visiting target samples that are easily associated, skipping difficult examples. This would lead to poor generalization to the target domain.
Therefore, a regularizer is necessary such that each target sample is visited with equal probability.
This is the function of the \textit{visit loss}. It is defined by the cross entropy between the uniform distribution over target samples and the probability of visiting some target sample starting in any source sample,

where 


Note that this formulation assumes that the class distribution is the same for source and target domain. If this is not the case, using a low weight for  may yield better results.

Together, these terms form a loss that enforces associations between similar embeddings of both domains,  

where  is a weight factor.
At the same time, the network is trained to minimize the prediction error on the labeled source data via a softmax cross-entropy loss term, .

The overall neural network loss for our training scheme is given by


We want to emphasize once more the essential motivation for our approach: The association loss enforces similar embeddings (\emph{assimilation}) for the source and target samples, while the classification loss minimizes the prediction error of the source data (\emph{discrimination}).
Without , we have the case of a neural network that is trained conventionally \cite{Krizhevsky2012} on the source domain only. 
As we show in this work, the (scheduled) addition of  during training allows to incorporate unlabeled data from a different domain improving the effectiveness of embeddings for classification.
Adding  enables an arbitrary neural network to be trained for domain adaptation.
The neural network learning algorithm is then able to model the shift in distribution between source and target domain.
More formally, if  is minimized, \emph{associated} embeddings from both source and target domain become more similar in terms of their dot product. 

In contrast to MMD,  incorporates knowledge about source domain classes and hence prevents the case that source and target domain embeddings are statistically similar, but not class discriminative. We demonstrate this experimentally in \autoref{sec:analysis}.

We emphasize that not every semi-supervised training method can be adapted for domain adaptation in this manner. It is necessary that the method explicitly models the shift between the source and target distributions, in order to reduce the discrepancy between both domains, which is accomplished by .

In this respect, associative domain adaptation parallels the approaches mentioned in \autoref{sec:related_work}. As we demonstrate experimentally in the next section,  is employed as a compact, intuitive and effective training signal for \emph{assimilation} yielding superior performance on all tested benchmarks.

\section{Experiments} 

\begin{table}[t!]
    \centering
    \begin{tabular}{>{\centering\arraybackslash}m{2cm}  >{\centering\arraybackslash}m{4cm}}
        \shortstack{\textsc{MNIST} \\\\ \textsc{MNIST-M}\\\small(10 classes)}  & \includegraphics[width=3cm]{mnist_to_mnistm.png} \
&C(32,3)\rightarrow C(32, 3)\rightarrow P(2)\\
\rightarrow \; &C(64, 3)\rightarrow C(64, 3)\rightarrow P(2)\\
\rightarrow\;  &C(128, 3)\rightarrow C(128, 3)\rightarrow P(2)\rightarrow FC(128)
\label{eqn:coverage}
    \frac{DA - SO}{TO - SO} \; ,

as it is a measure of how much label information is successfully transferred from the source to the target domain. In order to assess a method's performance on domain adaptation, one should always consider both coverage and absolute error on the target test set since a high coverage could also stem from poor performance in the SO or TO setting.

Where available, we report the coverage of other methods (with respect to their own performance on SO and TO).

\autoref{tbl:da} shows the results of our experiments. In all four popular domain adaptation settings our method performs best. On average, our approach improves the performance by 87.17~\% compared to training on source only (\emph{coverage}). In order to make our results as comparable as possible, we used a generic architecture that was not handcrafted for the respective tasks (cf. \autoref{sec:setup}).


\subsection{Analysis of the embedding quality}
\label{sec:analysis}
As described in \autoref{sec:introduction}, a good intuition for the formalism of domain adaptation is the following.
On the one hand, the latent features should cluster in embedding space, if they belong to the same class (\emph{assimilation}). 
On the other hand, these clusters should separate well in order to facilitate classification (\emph{discrimination}). 

We claim that our proposed  is well suited for this task compared with maximum mean discrepancy. We use four points to support this claim:

\begin{itemize}
    \item t-SNE visualizations show that employing  produces embeddings that cluster better compared to MMD.
    \item  simultaneously reduces the maximum mean discrepancy (MMD) in most cases.
    \item Lower MMD values do not imply lower target test errors in these settings.
    \item In all cases, the target domain test error of our approach is lower compared to training with an MMD loss.
\end{itemize}

\subsubsection{Qualitative evaluation: t-SNE embeddings}\label{sec:tsne}
A popular method to visualize high-dimensional data in 2D is t-SNE \cite{maaten2008visualizing}. 
We are interested in the distribution of embeddings for source and target domain when we employ our training scheme. 
\autoref{fig:tsne} shows such visualizations. We always plotted embeddings of the target domain test set.
The embeddings are obtained with networks trained semi-supervisedly \cite{Haeusser2017} on the source domain only (SO), with our proposed associative domain adaptation () and with MMD instead of  (, cf. \autoref{sec:setup}).

In the SO setting, samples from the source domain fall into clusters as expected. Samples from the target domain are more scattered.
For , samples from both domains cluster well and become separable.
For , the resulting distributions are similar, but not visibly class discriminative. 

For completeness, however, we explicitly mention that t-SNE embeddings are obtained via a non-linear, stochastic optimization procedure that depends on the choice of parameters like the perplexity (\cite{maaten2008visualizing, wattenberg2016how}). 
We therefore interpret these plots only qualitatively and infer that associative domain adaptation learns consistent embeddings for source and target domain that cluster well with observable margins.


\subsubsection{Quantitative evaluation: MMD values}
While t-SNE plots provide qualitative insights into the latent feature representation of a learning algorithm, we want to complement this with a quantitative evaluation and compute the discrepancy in embedding space for target and source domains.
We estimated the MMD with a Gaussian RBF kernel using the TensorFlow implementation provided by the authors of \cite{sutherland2016generative}.

The results are shown in \autoref{tbl:mmd}.
In parentheses we copied the test accuracies on the respective target domains from \autoref{tbl:da}. 

We observe that  yields the lowest maximum mean discrepancy, as expected, since this training setup explicitly minimizes this quantity.
At the same time,  also reduces this metric in most cases.
Interestingly though, for the setup SVHN  MNIST, we actually obtain a particularly high MMD. Nevertheless, the test error of the network trained with  is one of the best results.
We ascribe this to the fact that MMD enforces domain invariant feature representations regardless of the source labels, whereas  takes into account the labels of associated source samples, resulting in better separation of the clusters and higher similarity within the same class.
Consequently,  achieves lower test error on the target domain, which is the actual goal of domain adaptation.


\section{Conclusion} We have introduced a novel, intuitive domain adaptation scheme for neural networks termed \emph{associative domain adaptation} that generalizes a recent approach for semi-supervised learning\cite{Haeusser2017} to the domain adaptation setting. 
The key idea is to optimize a joint loss function combining the classification loss on the source domain with an association loss that imposes consistency of source and target embeddings.
The implementation is simple, works with arbitrary architectures in an end-to-end manner and introduces no significant additional computational and structural complexity. 
We have demonstrated the capabilities of associative domain adaptation on various benchmarks and achieved state-of-the-art results for all our experiments. 
Finally, we quantitatively and qualitatively examined how well our approach reduces the discrepancy between network embeddings from the source and target domain. 
We have observed that, compared to explicitly modelling the maximum mean discrepancy as a cost function, the proposed association loss results in embeddings that are more effective for classification in the target domain, the actual goal of domain adaptation.


{\small
\bibliographystyle{ieee.bst}
\bibliography{iccv2017}
}

\clearpage
\onecolumn
   \newpage
   \null
   \vskip .375in
   \begin{center}
      {\Large \bf Supplementary Material for `Associative Domain Adaptation' \par}
\vspace*{24pt}
      {
      \large
      \lineskip .5em
      \begin{tabular}[t]{c}
          
      \end{tabular}
      \par
      }
\vskip .5em
\vspace*{12pt}
   \end{center}


\setcounter{section}{0}
\setcounter{figure}{0}
\setcounter{table}{0}
\setcounter{footnote}{0}

\renewcommand*{\theHsection}{A\thesection}
\renewcommand*{\theHfigure}{A\thefigure}
\renewcommand*{\theHtable}{A\thetable}
\graphicspath{{./supplementary_figures/}}

We provide additional information that is necessary to reproduce our results, as well as plots complementing the evaluation section of the main paper.
To this end, we begin by stating implementation details for our neural network learning algorithm.
Furthermore, we show additional t-SNE embeddings of source target domain for the different domain adaptation tasks analyzed in the paper.

\section{Hyperparameters}
We report the hyperparameters that we used for our experiments for the sake of reproducibility as detailed in \autoref{tbl:params}.


\begin{table*}[b!]
	\begin{center}
		\begin{tabular}{|c||c|c|c|c|}
			\hline
			\multirow{ 2}{*}{Hyperparameter} & \multicolumn{4}{c|}{Domains (source  target) } \\
& MNIST  MNIST-M
			& Syn. Digits  SVHN 
			& SVHN  MNIST
			& Syn. Signs  GTSRB\\
			\hline
			\hline
 New width/height                           & 32                          & -                        & 32                        & -                               \\
Source domain batch size                    & 1000                        & 1000                     & 1000                     & 1032                              \\
 Target domain batch size                   & 1000                        & 1000                     & 1000                      & 1032                            \\
 Learning rate decay steps                  & 9000                        & 9000                     & 9000                      & 9000                            \\
 Visit loss weight                          & 0.6                         & 0.2                      & 0.2                       & 0.1                             \\
 Delay (steps) for                         & 500                         & 2000                     & 500                       & 0                               \\

            
			\hline
		\end{tabular}

	\end{center}
			        \caption{Hyperparameters for our domain adaptation experiments.} 
    	\label{tbl:params}

\end{table*}


\section{t-SNE embeddings}
We complement our analysis in Section 3.4.1 of the main document, \textit{Qualitative evaluation: t-SNE embeddings}. 
In \autoref{fig:supplementary_tsne} we show the t-SNE embeddings for all domain adaptation tasks that we have analyzed (cf. Table 3 of the main paper).
The qualitative interpretation that we provide for the task Synthetic Digits to SVHN in the main paper is consistent across all tasks: when trained on source only, the target domain distribution is diffuse, the respective target classes can be visibly separated after domain adaptation and the separation is less clear when training with an MMD loss instead of our associative loss.
Note that for the task Synthetic Signs to GTSRB, the target domain test error for the network trained on source only is already rather low. Subsequent domain adaptation improves the numerical result, which is, however, difficult to observe qualitatively due to the relatively small \emph{coverage} compared to the previous settings.

\begin{figure*}[h!]
	\centering
        \begin{subfigure}{0.3\textwidth}
            \includegraphics[width=\textwidth]{tsne/tsne_synth2svhn_SO.png}
        \end{subfigure}\begin{subfigure}{0.3\textwidth}
            \includegraphics[width=\textwidth]{tsne/tsne_synth2svhn_DA.png}
        \end{subfigure}\begin{subfigure}{0.3\textwidth}
            \includegraphics[width=\textwidth]{tsne/tsne_synth2svhn_MMD.png}
        \end{subfigure}\\
    	\begin{subfigure}{0.3\textwidth}
            \includegraphics[width=\textwidth]{tsne/tsne_mnist2mnistm_SO.png}
        \end{subfigure}\begin{subfigure}{0.3\textwidth}
            \includegraphics[width=\textwidth]{tsne/tsne_mnist2mnistm_DA.png}
        \end{subfigure}\begin{subfigure}{0.3\textwidth}
            \includegraphics[width=\textwidth]{tsne/tsne_mnist2mnistm_MMD.png}
        \end{subfigure}\\
        \begin{subfigure}{0.3\textwidth}
            \includegraphics[width=\textwidth]{tsne/tsne_svhn2mnist_SO.png}
        \end{subfigure}\begin{subfigure}{0.3\textwidth}
            \includegraphics[width=\textwidth]{tsne/tsne_svhn2mnist_DA.png}
        \end{subfigure}\begin{subfigure}{0.3\textwidth}
            \includegraphics[width=\textwidth]{tsne/tsne_svhn2mnist_MMD.png}
        \end{subfigure}\\
        \begin{subfigure}{0.3\textwidth}
            \includegraphics[width=\textwidth]{tsne/tsne_signs2gtsrb_SO.png}
        \end{subfigure}\begin{subfigure}{0.3\textwidth}
            \includegraphics[width=\textwidth]{tsne/tsne_signs2gtsrb_DA.png}
        \end{subfigure}\begin{subfigure}{0.3\textwidth}
            \includegraphics[width=\textwidth]{tsne/tsne_signs2gtsrb_MMD.png}
        \end{subfigure}\caption{t-SNE embeddings of test samples for source (red) and target (blue).
        \textbf{First row:} MNIST to MNIST-M, perplexity .
        \textbf{Second row:} SVHN to MNIST, perplexity .
        \textbf{Third row:} Synthetic Signs to GTSRB, perplexity .
        1,000 samples per domain, except for Synthetic Signs to GTSRB, where we took 60 samples for each of the 43 classes due to class imbalance in GTSRB.
    	\textbf{Left}: After training on \emph{source only}. \textbf{Middle}: after training with \emph{associative domain adaptation} (). \textbf{Right}: after training with \emph{MMD loss} ().} 
    	\label{fig:supplementary_tsne}
\end{figure*}



\end{document}
