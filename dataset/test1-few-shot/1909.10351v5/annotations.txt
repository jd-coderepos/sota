[{'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'SQuAD1.1 dev', 'Metric': 'EM', 'Score': '79.7'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'SQuAD1.1 dev', 'Metric': 'F1', 'Score': '87.5'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'SQuAD2.0 dev', 'Metric': 'F1', 'Score': '73.4'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'SQuAD2.0 dev', 'Metric': 'EM', 'Score': '69.9'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'QNLI', 'Metric': 'Accuracy', 'Score': '87.7%'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'MultiNLI Dev', 'Metric': 'Matched', 'Score': '84.5'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'MultiNLI Dev', 'Metric': 'Mismatched', 'Score': '84.5'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'RTE', 'Metric': 'Accuracy', 'Score': '62.9%'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'MultiNLI', 'Metric': 'Matched', 'Score': '82.5'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'MultiNLI', 'Metric': 'Mismatched', 'Score': '81.8'}}, {'LEADERBOARD': {'Task': 'Semantic Textual Similarity', 'Dataset': 'MRPC', 'Metric': 'Accuracy', 'Score': '86.4%'}}, {'LEADERBOARD': {'Task': 'Semantic Textual Similarity', 'Dataset': 'STS Benchmark', 'Metric': 'Pearson Correlation', 'Score': '0.799'}}, {'LEADERBOARD': {'Task': 'Sentiment Analysis', 'Dataset': 'SST-2 Binary classification', 'Metric': 'Accuracy', 'Score': '92.6'}}, {'LEADERBOARD': {'Task': 'Linguistic Acceptability', 'Dataset': 'CoLA', 'Metric': 'Accuracy', 'Score': '43.3%'}}, {'LEADERBOARD': {'Task': 'Linguistic Acceptability', 'Dataset': 'CoLA Dev', 'Metric': 'Accuracy', 'Score': '54'}}]
