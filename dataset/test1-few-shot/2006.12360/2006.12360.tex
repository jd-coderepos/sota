\documentclass{article}



\PassOptionsToPackage{numbers, compress}{natbib}
\usepackage[preprint]{neurips_2020}







\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage[subtle,mathspacing=normal,mathdisplays=tight,tracking=normal]{savetrees}

\usepackage{amssymb,amsmath}
\usepackage{txfonts}
\usepackage{microtype}
\usepackage{xspace}
\usepackage{xcolor}
\xspaceaddexceptions{\%}
\usepackage{comment}
\usepackage{xcolor}
\usepackage{paralist}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{rotating}
\usepackage{pdflscape}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{multirow}
\DeclareMathOperator{\st}{s.t.}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\newcommand{\keypoint}[1]{\textbf{#1}\quad}
\newcommand{\doublecheck}[1]{\textcolor{blue}{#1}}
\newcommand{\cut}[1]{}
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{4pt}
\urlstyle{same}
\usepackage[capitalise]{cleveref}
\newcommand*\rot{\rotatebox{90}}

\usepackage{subfiles} 

\title{Donâ€™t Wait, Just~Weight: Improving Unsupervised Representations by Learning Goal-Driven Instance Weights}




\author{Linus Ericsson \\
School of Informatics\\
  University of Edinburgh\\
  Edinburgh, UK \\
  \texttt{linus.ericsson@ed.ac.uk} \\
\And
  Henry Gouk \\
  School of Informatics\\
  University of Edinburgh\\
  Edinburgh, UK \\
  \texttt{hgouk@inf.ed.ac.uk} \\
  \And
  Timothy M.~Hospedales \\
  School of Informatics\\
  University of Edinburgh\\
  Edinburgh, UK \\
  \texttt{t.hospedales@ed.ac.uk} \\
}

\begin{document}

\maketitle

\begin{abstract}
In the absence of large labelled datasets, self-supervised learning techniques can boost performance by learning useful representations from unlabelled data, which is often more readily available. However, there is often a domain shift between the unlabelled collection and the downstream target problem data. We show that by learning Bayesian instance weights for the unlabelled data, we can improve the downstream classification accuracy by prioritising the most useful instances. Additionally, we show that the training time can be reduced by discarding unnecessary datapoints. Our method, BetaDataWeighter is evaluated using the popular self-supervised rotation prediction task on STL-10 and Visual Decathlon. We compare to related instance weighting schemes, both hand-designed heuristics and meta-learning, as well as conventional self-supervised learning. BetaDataWeighter achieves both the highest average accuracy and rank across datasets, and on STL-10 it prunes up to 78\% of unlabelled images without significant loss in accuracy, corresponding to over 50\% reduction in training time.
\end{abstract}

\subfile{sections/1-intro}
\subfile{sections/2-related}
\subfile{sections/3-background}
\subfile{sections/4-method}
\subfile{sections/5-exps}
\subfile{sections/6-conc}
\subfile{sections/7-impact}

\begin{ack}
This work was supported in part by the EPSRC Centre for Doctoral Training in Data Science, funded by the UK Engineering and Physical Sciences Research Council (grant EP/L016427/1) and the University of Edinburgh.
\end{ack}

\small
\bibliographystyle{custombib}
\bibliography{references,morerefs}

\newpage
\appendix
\subfile{sections/8-appendix}

\end{document}
