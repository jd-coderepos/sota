

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{emnlp2020}
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{multirow} 
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{booktabs}
\renewcommand{\UrlFont}{\ttfamily\small}

\usepackage{microtype}
\special{papersize=210mm,297mm}
\aclfinalcopy \def\aclpaperid{158} 

\setlength\titlebox{5cm}


\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Double Graph Based Reasoning for Document-level Relation Extraction}

\author{
  Shuang Zeng\footnotemark[1], 
  Runxin Xu\footnotemark[1], 
  Baobao Chang\footnotemark[2] \and 
  Lei Li\\
  Key Laboratory of Computational Linguistics, Peking University, MOE, China\\
  School of Software and Microelectronics, Peking University, China\\
  ByteDance AI Lab, China\\
  \texttt{
    \{zengs,chbb\}@pku.edu.cn runxinxu@gmail.com lileilab@bytedance.com
  }
}

\date{}

\begin{document}
\maketitle

\renewcommand{\thefootnote}{\fnsymbol{footnote}} \footnotetext[1]{Equal contribution.} \footnotetext[2]{Corresponding author.} \renewcommand{\thefootnote}{\fnsymbol{footnote}}

\begin{abstract}




Document-level relation extraction aims to extract relations among entities within a document. 
Different from sentence-level relation extraction, it requires reasoning over multiple sentences across a document. 
In this paper, we propose \textbf{G}raph \textbf{A}ggregation-and-\textbf{I}nference \textbf{N}etwork (GAIN) featuring double graphs. 
GAIN first constructs a \textit{heterogeneous mention-level graph} (hMG) to model complex interaction among different mentions across the document. 
It also constructs an \textit{entity-level graph} (EG), based on which we propose a novel path reasoning mechanism to infer relations between entities. Experiments on the public dataset, DocRED, show GAIN achieves a significant performance improvement ( on F1) over the previous state-of-the-art. Our code is available at \url{https://github.com/DreamInvoker/GAIN}.


\end{abstract} \section{Introduction}
The task of identifying semantic relations between entities from text, namely relation extraction (RE), plays a crucial role in a variety of knowledge-based applications, such as question answering (\citealp{yu-etal-17-improved}) and large-scale knowledge graph construction.
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/case.pdf}
    \caption{An example document and its desired relations from DocRED \citep{yao-etal-19-docred}. Entity mentions and relations involved in these relation instances are colored. Other mentions are underlined for clarity.}
    \label{fig:running-example}
\end{figure}
Previous methods (\citealp{zeng-etal-14-relation}; \citealp{zeng-etal-15-distant}; \citealp{xiao-liu-16-semantic}; \citealp{zhang-etal-17-position}; \citealp{zhang-etal-18-graph}; \citealp{baldini-soares-etal-19-matching}) focus on sentence-level RE, which predicts relations among entities in a single sentence.
However, sentence-level RE models suffer from an inevitable limitation -- they fail to recognize relations between entities across sentences. 
Hence, extracting relations at the document-level is necessary for a holistic understanding of knowledge in text.

There are several major challenges in effective relation extraction at the document-level.
Firstly, the subject and object entities involved in a relation may appear in different sentences. Therefore a relation cannot be identified based solely on a single sentence.
Secondly, the same entity may be mentioned multiple times in different sentences. Cross-sentence context information has to be aggregated to represent the entity better.
Thirdly, the identification of many relations requires techniques of logical reasoning. This means these relations can only be successfully extracted when other entities and relations, usually spread across sentences, are identified implicitly or explicitly.  
As Figure~\ref{fig:running-example} shows, it is easy to recognize the intra-sentence relations (\textit{Maryland}, country, \textit{U.S.}), (\textit{Baltimore}, located in the administrative territorial entity, \textit{Maryland}), and (\textit{Eldersburg}, located in the administrative territorial entity, \textit{Maryland}), since the subject and object appear in the same sentence.
However, it is non-trivial to predict the inter-sentence relations between \textit{Baltimore} and \textit{U.S.}, as well as \textit{Eldersburg} and \textit{U.S.}, whose mentions do not appear in the same sentence and have long-distance dependencies.
Besides, the identification of these two relation instances also requires logical reasoning. For example, \textit{Eldersburg} belongs to \textit{U.S.} because \textit{Eldersburg} is located in \textit{Maryland}, which belongs to \textit{U.S.}.

Recently, \citet{yao-etal-19-docred} proposed a large-scale human-annotated document-level RE dataset, DocRED, to push sentence-level RE forward to document-level and it contains massive relation facts. Figure~\ref{fig:running-example} shows an example from DocRED. 
We randomly sample 100 documents from the DocRED dev set and manually analyze the bad cases predicted by a BiLSTM-based model proposed by \citet{yao-etal-19-docred}. As shown in Table~\ref{table:bad-cases}, the error type of inter-sentence and that of logical reasoning take up a large proportion of all bad cases, with  and  respectively.
Therefore, in this paper, we aim to tackle these problems to extract relations from documents better.

\begin{table}
\centering
\begin{tabular}{lc}
\hline
Error Type & Count \\
\hline
Intra-sentence & 535 \\
Inter-sentence & 615 \\
\hline
\hline
Logical Reasoning & 242 \\
\hline
\end{tabular}
\caption{Statistics of bad cases in randomly sampled 100 documents from DocRED dev set for BiLSTM \citep{yao-etal-19-docred}, with 1150 bad cases in total.}
\label{table:bad-cases}
\end{table}

Previous work in document-level RE do not consider reasoning (\citealp{DBLP:conf/aaai/GuptaRSR19}; \citealp{jia-etal-19-document}; \citealp{yao-etal-19-docred}), or only use graph-based or hierarchical neural network to conduct reasoning in an implicit way (\citealp{peng-etal-17-cross}; \citealp{sahu-etal-19-inter}; \citealp{LSR}).
In this paper, we propose a \textbf{G}raph \textbf{A}ggregation-and-\textbf{I}nference \textbf{N}etwork (GAIN) for document-level relation extraction. 
It is designed to tackle the challenges mentioned above directly.
GAIN constructs a \textbf{h}eterogeneous \textbf{M}ention-level \textbf{G}raph (hMG) with two types of nodes, namely mention node and document node, and three different types of edges, i.e., intra-entity edge, inter-entity edge and document edge, to capture the context information of entities in the document.
Then, we apply Graph Convolutional Network \citep{kipf2017semi} on hMG to get a document-aware representation for each mention. 
\textbf{E}ntity-level \textbf{G}raph (EG) is then constructed by merging mentions that refer to the same entity in hMG, on top of which we propose a novel path reasoning mechanism. This reasoning mechanism allows our model to infer multi-hop relations between entities.

In summary, our main contributions are as follows:
\begin{itemize}
    \item We propose a novel method, Graph Aggregation-and-Inference Network (GAIN), which features a double graph design, to better cope with document-level RE task.
    
    \item We introduce a heterogeneous Mention-level Graph (hMG) with a graph-based neural network to model the interaction among different mentions across the document and offer document-aware mention representations.
    
    \item We introduce an Entity-level Graph (EG) and propose a novel path reasoning mechanism for relational reasoning among entities.
\end{itemize}

We evaluate GAIN on the public DocRED dataset. It significantly outperforms the previous state-of-the-art model by  F1 score. 
Further analysis demonstrates the capability of GAIN to aggregate document-aware context information and to infer logical relations over documents. 
 \section{Task Formulation}
We formulate the document-level relation extraction task as follows. Given a document comprised of  sentences  and a variety of entities , where  refers to the -th sentence consisting of  words,  and  refers to a span of words belonging to the -th mention of the -th entity, the task aims to extract the relations between different entities in , namely , where  is a pre-defined relation type set.

In our paper, a relation  between entity  and  is defined as inter-sentential, if and only if , where  denotes those sentences containing mentions of . 
Instead, a relation  is defined as intra-sentential, if and only if .
We also define -hop relational reasoning as predicting relation  based on a -length chain of existing relations, with  and  being the head and tail of the reasoning chain, i.e., .







 \begin{figure*}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/model.pdf}
    \caption{The overall architecture of GAIN. First, A context encoder consumes the input document to get a contextualized representation of each word. Then, the Mention-level Graph is constructed with mention nodes and a document node. After applying GCN, the graph is transformed into Entity-level Graph, where the paths between entities are identified for reasoning. Finally, the classification module predicts target relations based on the above information. Different entities are in different colors. The number  in the mention node denotes that it belongs to the -th sentence.}
    \label{fig:model}
\end{figure*}
\section{Graph Aggregation and Inference Network (GAIN)\label{sec:model}}

GAIN mainly consists of 4 modules: 
encoding module (Sec.~\ref{ssec:encoding}), 
mention-level graph aggregation module (Sec.~\ref{ssec:aggregation}),
entity-level graph inference module (Sec.~\ref{ssec:inference}), classification module (Sec.~\ref{ssec:classification}), as is shown in Figure~\ref{fig:model}.

\subsection{Encoding Module\label{ssec:encoding}}
In the encoding module, we convert a document  containing  words into a sequence of vectors .
Following \citet{yao-etal-19-docred}, for each word  in , we first concatenate its word embedding with entity type embedding and coreference embedding:

where  ,  and  denote the word embedding layer, entity type embedding layer and coreference embedding layer, respectively.  and  are named entity type and entity id. 
We introduce \textit{None} entity type and id for those words not belonging to any entity.

Then the vectorized word representations are fed into an encoder to obtain the context sensitive representation for each word:

where the  can be LSTM or other models.

\subsection{Mention-level Graph Aggregation Module\label{ssec:aggregation}}
To model the document-level information and interactions between mentions and entities, a heterogeneous Mention-level Graph (hMG) is constructed.


hMG has two different kinds of nodes: mention node and document node. Each mention node denotes one particular mention of an entity.
And hMG also has one document node that aims to model the overall document information. We argue that this node could serve as a pivot to interact with different mentions and thus reduce the long distance among them in the document.


There are three types of edges in hMG:
\begin{itemize}
    \item \textbf{Intra-Entity Edge:} Mentions referring to the same entity are fully connected with intra-entity edges. In this way, the interaction among different mentions of the same entity could be modeled.
 
    \item \textbf{Inter-Entity Edge:} Two mentions of different entities are connected with an inter-entity edge if they co-occur in a single sentence. In this way, interactions among entities could be modeled by co-occurrences of their mentions. 
\item \textbf{Document Edge:} All mentions are connected to the document node with the document edge. With such connections, the document node can attend to all the mentions and enable interactions between document and mentions. Besides, the distance between two mention nodes is at most two with the document node as a pivot. Therefore long-distance dependency can be better modeled.
\end{itemize}




Next, we apply Graph Convolution Network \citep{kipf2017semi} on hMG to aggregate the features from neighbors. Given node  at the -th layer, the graph convolutional operation can be defined as:

where  are different types of edges,  and  are trainable parameters.
 denotes neighbors for node  connected in -th type edge.
 is an activation function (e.g., ReLU).


Different layers of GCN express features of different abstract levels, and therefore in order to cover features of all levels, we concatenate hidden states of each layer to form the final representation of node :

where  is the initial representation of node . For a mention ranging from the -th word to the -th word in the document,  and for document node, it is initialized with the document representation output from the encoding module.



\subsection{Entity-level Graph Inference Module\label{ssec:inference}}


In this subsection, we introduce Entity-level Graph (EG) and path reasoning mechanism. 
First, mentions that refer to the same entity are merged to entity node so as to get the nodes in EG. Note that we do not consider document node in EG. For -th entity node  mentioned  times, it is represented by the average of its  mention representations:


Then, we merge all inter-entity edges that connect mentions of the same two entities so as to get the edges in EG. The representation of directed edge from  to  in the EG is defined as :

where  and  are trainable parameters, and  is an activation function (e.g., ReLU).

Based on the vectorized edge representation, the -th path between head entity  and tail entity  passing through entity  is represented as:

Note that we only consider two-hop paths here, while it can easily extend to multi-hop paths.

We also introduce attention mechanism \citep{attention-15}, using the entity pair  as query, to fuse the information of different paths between  and . 



where  is the normalized attention weight for -th path. Consequently, the model will pay more attention to useful paths.  is an activation function.

With this module, an entity can be represented by fusing information from its mentions, which usually spread in multiple sentences. Moreover, potential reasoning clues are modeled by different paths between entities. Then they can be integrated with the attention mechanism so that we will take into account latent logical reasoning chains to predict relations.



\subsection{Classification Module\label{ssec:classification}}
For each entity pair , we concatenate the following representations: (1) the head and tail entity representation  and  derived in the Entity-level Graph, with the comparing operation \citep{mou-etal-16-natural} to strengthen features, i.e., absolute value of subtraction between the representation of two entities, , and element-wise multiplication, ; (2) the representation of document node in Mention-level Graph, , as it can help aggregate cross-sentence information and provide document-aware representation; (3) the comprehensive inferential path information .


Finally, we formulate the task as multi-label classification task and predict relations between entities:

where , , ,  are trainable parameters,  is an activation function (e.g., ReLU). We use binary cross entropy as the classification loss to train our model in an end-to-end way:

where  denotes the whole corpus, and  refers to indication function.
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ef{fig:experiment_case} also shows the case study of our proposed model GAIN, in comparison with other baselines. As is shown, BiLSTM can only identify two relations within the first sentence. 
Both BERT-RE and GAIN-BERT can successfully predict \textit{Without Me} is part of \textit{The Eminem Show}. But only GAIN-BERT is able to deduce the performer and publication date of \textit{Without Me} are the same as those of \textit{The Eminem Show}, namely \textit{Eminem} and \textit{May 26, 2002}, where it requires logical inference across sentences.

%
 \section{Related Work}
Previous approaches focus on sentence-level relation extraction (\citealp{zeng-etal-14-relation};
\citealp{zeng-etal-15-distant};
\citealp{wang-etal-16-relation}; \citealp{zhou-etal-16-attention}; \citealp{xiao-liu-16-semantic};
\citealp{zhang-etal-17-position};
\citealp{feng2018reinforcement}; \citealp{zhu-etal-19-graph}). But sentence-level RE models face an inevitable restriction in practice, where many real-world relation facts can only be extracted across sentences. Therefore, many researchers gradually shift their attention into document-level relation extraction.

Several approaches (\citealp{quirk-poon-17-distant}; \citealp{peng-etal-17-cross}; \citealp{DBLP:conf/aaai/GuptaRSR19}; \citealp{song-etal-2018}; \citealp{jia-etal-19-document}) leverage dependency graph to better capture document-specific features, but they ignore ubiquitous relational inference in document. 
Recently, many models are proposed to address this problem.
\citet{tang2020hin} proposed a hierarchical inference network by considering information from entity-level, sentence-level, and document-level. 
However, it conducts relational inference implicitly based on a hierarchical network while we adopt the path reasoning mechanism, which is a more explicit way.


\citep{christopoulou-etal-2019-connecting} is one of the most powerful systems on document-level RE tasks recently. 
Compared to \citep{christopoulou-etal-2019-connecting} and other graph-based approaches to relation extraction, our architecture features many different designs with different motivations behind them. First, the ways of graph construction are different. We create two separate graphs of different levels to capture long-distance document-aware interactions and entity path inference information, respectively. While \citet{christopoulou-etal-2019-connecting} put mentions and entities in the same graph. Moreover, they do not conduct graph node representation learning like GCN to aggregate interactive information on the constructed graph, only using the features from BiLSTMs to represent nodes. Second, the processes of path inference are different. \citet{christopoulou-etal-2019-connecting} use a walk-based method to iteratively generate a path for every entity pair, which requires the extra overhead of hyper-parameter tuning to control the process of inference. Instead, we use an attention mechanism to selectively fuse all possible path information for the entity pair while without extra overhead.

When we were writing this paper, \citep{LSR} make their work public as preprints, which adopt the dependency tree to capture the semantic information in the document. They put mention and entity nodes in the same graph and conduct inference implicitly by using GCN.
Unlike their work, our GAIN presents mention node and entity node in different graphs to better conduct inter-sentence information aggregation and infer relations more explicitly. 

Some other attempts (\citealp{verga-2018-simultaneously, sahu-etal-19-inter, christopoulou-etal-2019-connecting}) study document-level RE in a specific domain like biomedical RE.
However, the datasets they use usually contain very limited relation types and entity types. 
For instance, CDR \citep{li2016biocreative} only has one type of relation and two types of entities, which may not be the ideal testbed for relational reasoning. 
%
 \section{Conclusion}











Extracting inter-sentence relations and conducting relational reasoning are challenging in document-level relation extraction. 

In this paper, we introduce Graph Aggregation-and-Inference Network (GAIN) to better cope with document-level relation extraction, which features double graphs in different granularity.
GAIN utilizes a heterogeneous Mention-level Graph to model the interaction among different mentions across the document and capture document-aware features.
It also uses an Entity-level Graph with a proposed path reasoning mechanism to infer relations more explicitly.

Experimental results on the large-scale human-annotated dataset, DocRED, show GAIN outperforms previous methods, especially in inter-sentence and inferential relations scenarios. 
The ablation study also confirms the effectiveness of different modules in our model.
 
\section*{Acknowledgments}
The authors would like to thank the anonymous reviewers for their thoughtful and constructive comments and ByteDance AI Lab for providing the computational resources for this work.
This paper is supported in part by the National Key R\&D Program of China under Grand No.2018AAA0102003, the National Science Foundation of China under Grant No.61876004 and 61936012.

\bibliography{anthology,emnlp2020}
\bibliographystyle{acl_natbib}

\appendix


\section{Hyperparameter settings\label{sec:appendix}}
We use development set to manually tune the optimal hyperparameters for GAIN, based on the Ign F1 score. Hyperparameter settings for GAIN-GloVe, GAIN-BERT and GAIN-BERT are listed in Table~\ref{tab:hyperparam1}, ~\ref{tab:hyperparam2} and ~\ref{tab:hyperparam3}, respectively. The value of hyperparameters we finally adopted are in bold. Note that we do not tune all the hyperparameters.






\begin{table}[!htbp]
\centering
\begin{tabular}{lr}
\hline
\textbf{Hyperparameter} & Value \\
\hline
Batch Size &  16, \textbf{32} \\
Learning Rate  & \textbf{0.001}  \\
Activation Function & \textbf{ReLU}, Tanh \\
Positive v.s. Negative Ratio & 1, 0.5, \textbf{0.25} \\
Word Embedding Size  & \textbf{100}\\
Entity Type Embedding Size & \textbf{20}  \\
Coreference Embedding Size & \textbf{20}  \\
Encoder Hidden Size &  128, \textbf{256} \\
Dropout &  0.2, \textbf{0.6}, 0.8 \\
Layers of GCN & 1, \textbf{2}, 3 \\
GCN Hidden Size & \textbf{512} \\
Weight Decay & \textbf{0.0001} \\
\hline \hline
Numbers of Parameters & 63M \\ Hyperparameter Search Trials & 12 \\
\hline
\end{tabular}
\caption{Settings for GAIN-GloVe.}
\label{tab:hyperparam1}
\end{table}


\begin{table}[!htbp]
\centering
\begin{tabular}{lr}
\hline
\textbf{Hyperparameter} & Value \\
\hline
Batch Size & \textbf{5} \\
Learning Rate  & \textbf{0.001}  \\
Activation Function & \textbf{ReLU}, Tanh \\
Positive v.s. Negative Ratio & 1, 0.5, \textbf{0.25} \\
Entity Type Embedding Size & \textbf{20} \\
Coreference Embedding Size & \textbf{20} \\
Dropout & 0.2, \textbf{0.6}, 0.8 \\
Layers of GCN & 1, \textbf{2}, 3 \\
GCN Hidden Size & \textbf{808} \\
Weight Decay & \textbf{0.0001} \\
\hline \hline
Numbers of Parameters & 217M \\ Hyperparameter Search Trials & 20 \\
\hline
\end{tabular}
\caption{Settings for GAIN-BERT.}
\label{tab:hyperparam2}
\end{table}

\begin{table}[!htbp]
\centering
\begin{tabular}{lr}
\hline
\textbf{Hyperparameter} & Value\\
\hline
Batch Size & \textbf{5}\\
Learning Rate  & \textbf{0.001} \\
Activation Function &  \textbf{ReLU}, Tanh \\
Positive v.s. Negative Ratio & 1, 0.5, \textbf{0.25} \\
Entity Type Embedding Size & \textbf{20} \\
Coreference Embedding Size & \textbf{20} \\
Dropout & 0.2, \textbf{0.6}, 0.8 \\
Layers of GCN & 1, \textbf{2}, 3 \\
GCN Hidden Size & \textbf{1064} \\
Weight Decay & \textbf{0.0001} \\
\hline \hline
Numbers of Parameters &  512M \\ Hyperparameter Search Trials & 20 \\
\hline
\end{tabular}
\caption{Settings for GAIN-BERT.}
\label{tab:hyperparam3}
\end{table}
 








\end{document}
