

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{emnlp-ijcnlp-2019}
\usepackage{times}
\usepackage{latexsym}

\usepackage{url}

\aclfinalcopy 



\newcommand\BibTeX{B{\sc ib}\TeX}
\newcommand\confname{EMNLP-IJCNLP 2019}
\newcommand\conforg{SIGDAT}




\usepackage{amsmath}
\usepackage{cleveref}

\usepackage{amsfonts}
\usepackage{xcolor}
\newcommand{\TODO}[1]{\textbf{(\color{red}T\color{green}O\color{blue}D\color{orange}O\color{black}: {#1})}}
\newcommand{\TODOh}[1]{\textbf{(\color{red}T\color{green}O\color{blue}D\color{orange}O\color{black}: {#1})}}


\newcommand{\keyword}[1]{\textit{#1}}



\usepackage{tabu}
\usepackage{booktabs}
\usepackage{multirow}


\usepackage{subcaption}

\usepackage{mathtools}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lvert\lvert}{\rvert\rvert}

\usepackage{bm}
\clearpage{}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak\clearpage{}
\usepackage[warn]{textcomp}
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
breaklines=true,
  postbreak=\mbox{\textcolor{red}{}\space},
}
\newcommand{\LN}{\operatorname{LN}}
\newcommand{\arTOen}{ar\textrightarrow en}
\newcommand{\enTOhe}{en\textrightarrow he}
\newcommand{\enTOvi}{\textit{en\textrightarrow vi}}
\newcommand{\glTOen}{gl\textrightarrow en}
\newcommand{\skTOen}{sk\textrightarrow en}

\newcommand{\BNorm}{\textsc{BatchNorm}}
\newcommand{\LNorm}{\textsc{LayerNorm}}
\newcommand{\SCNorm}{\textsc{ScaleNorm}}
\newcommand{\SCNormOne}{\textsc{ScaleNorm} (=)}
\newcommand{\RMSNorm}{\textsc{RMSNorm}}
\newcommand{\PreNorm}{\textsc{PreNorm}}
\newcommand{\PostNorm}{\textsc{PostNorm}}
\newcommand{\FairSeq}{\textsc{fairseq}}
\newcommand{\fixnorm}{\textsc{FixNorm}}
\newcommand{\radam}{\textsc{RAdam}}
\newcommand{\fixup}{\textsc{Fixup}}

\newcommand{\ORGLR}{\textsc{InvSqrtDecay}}
\newcommand{\NOWARMUP}{\textsc{NoWarmup}}
\newcommand{\VALBASED}{\textsc{ValDecay}}

\newcommand{\WMT}{WMT\,\textquotesingle 14}
\newcommand{\IWSLT}{IWSLT\,\textquotesingle 15}

\newcommand{\SmallInit}{\textsc{SmallInit}}
\newcommand{\lrtwo}{\textsc{2LR}}

\renewcommand{\paragraph}[1]{\par\medskip\noindent\textbf{#1}}

\newcommand*\samethanks[1][\value{footnote}]{\hyperlink{*}{\footnotemark[#1]}}




\title{Transformers without Tears:\\Improving the Normalization of Self-Attention}

\author{Toan Q. Nguyen\Thanks{\ Equal contribution.} \\
  University of Notre Dame\Thanks{\ Work done during an internship at Amazon AWS AI.} \\
  {\tt tnguye28@nd.edu} \\\And
  Julian Salazar\samethanks[1]\\
  Amazon AWS AI \\
  {\tt julsal@amazon.com} \\
  }

\date{}

\begin{document}
\maketitle

\begin{abstract}
We evaluate three simple, normalization-centric changes to improve  Transformer training. First, we show that pre-norm residual connections (\PreNorm) and smaller initializations enable warmup-free, validation-based training with large learning rates. Second, we propose  normalization with a single scale parameter (\SCNorm) for faster training and better performance. Finally, we reaffirm the effectiveness of normalizing word embeddings to a fixed length (\fixnorm). On five low-resource translation pairs from TED Talks-based corpora, these changes always converge, giving an average +1.1 BLEU over state-of-the-art bilingual baselines and a new 32.8 BLEU on \IWSLT\ English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Surprisingly, in the high-resource setting (\WMT\ English-German), \SCNorm\ and \fixnorm\ remain competitive but \PreNorm\ degrades performance. Preprocessing scripts and code are released at\\ \url{https://github.com/tnq177/transformers_without_tears}.
\end{abstract}

\section{Introduction}
The Transformer \cite{NIPS2017_7181} has become the dominant architecture for neural machine translation (NMT) due to its train-time parallelism and strong downstream performance. Various modifications have been proposed to improve the efficiency of its multi-head attention and feedforward sublayers \cite{Guo2019,Sukhbaatar2019}. Our work focuses on \keyword{layer normalization} (\LNorm) \cite{Ba2015}, which we show has an outsized role in the convergence and performance of the Transformer in two ways:

\paragraph{Placement of normalization.} The original Transformer uses \keyword{post-norm residual units} (\PostNorm), where layer normalization occurs after the sublayer and residual addition. However, \citet{Chen2018} found that \keyword{pre-norm residual units} (\PreNorm), where layer normalization occurs immediately before the sublayer, were instrumental to their model's performance. \citet{Wang2019-learning-deep-transformers} compare the two, showing that \PreNorm\ makes backpropagation more efficient over depth and training Transformers with deep, 30-layer encoders.

Our work demonstrates additional consequences in the base (6-layer encoder) Transformer regime. We show that \PreNorm\ enables warmup-free, validation-based training with large learning rates even for small batches, in contrast to past work on scaling NMT \cite{Ott2018}. We also partly reclaim \PostNorm's stability via smaller initializations, although \PreNorm\ is less sensitive to this magnitude and can improve performance. However, despite \PreNorm's recent adoption in many NMT frameworks, we find it degrades base Transformer performance on \WMT\ English-German.

\paragraph{Choice of normalization.} \citet{Santurkar2018} show that batch normalization's effectiveness is not from reducing internal covariate shift, but from smoothing the loss landscape. They achieve similar or better performance with non-variance-based normalizations in image classification. Hence, we propose replacing \LNorm\ with the simpler \keyword{scaled  normalization} (\SCNorm), which normalizes activation vectors to a \keyword{single} learned length . This is both inspired by and synergistic with jointly fixing the word embedding lengths (\fixnorm) \cite{Nguyen2018-improving-lexical-choice}. These changes improve the training speed and low-resource performance of the Transformer without affecting high-resource performance.
\par\medskip

On five low-resource pairs from the TED Talks \cite{Qi2018-word-embeddings-nmt} and \IWSLT\ \cite{Cettolo2015} corpora, we first train state-of-the-art Transformer models (+4.0 BLEU on average over the best published NMT bitext-only numbers). We then apply \PreNorm, \fixnorm, and \SCNorm\ for an average total improvement of +1.1 BLEU, where each addition contributes at least +0.3 BLEU (\Cref{sec:experiments}), and attain a new 32.8 BLEU on \IWSLT\ English-Vietnamese. We validate our intuitions in \Cref{sec:analysis} by showing sharper performance curves (i.e., improvements occur at earlier epochs) and more consistent gradient norms. We also examine the per-sublayer 's learned by \SCNorm, which suggest future study.
 \section{Background}

\subsection{Identity mappings for transformers}
\label{ssec:identity-mappings}
\keyword{Residual connections} \cite{He2016} were first introduced to facilitate the training of deep convolutional networks, where the output of the -th layer  is summed with its input:

The identity term  is crucial to greatly extending the depth of such networks \cite{He2016-identity-mappings}. If one were to scale  by a scalar , then the contribution of  to the final layer  is . For deep networks with dozens or even hundreds of layers , the term  becomes very large if  or very small if , for enough . When backpropagating from the last layer  back to , these multiplicative terms can cause exploding or vanishing gradients, respectively. Therefore they fix , keeping the total residual path an identity map.

The original Transformer applies \LNorm\ after the sublayer and residual addition (\PostNorm):  

We conjecture this has caused past convergence failures \cite{Popel2018, Shazeer2018}, with {\LNorm}s in the residual path acting similarly to ; furthermore, warmup was needed to let \LNorm\ safely adjust scale during early parts of training. Inspired by \citet{He2016-identity-mappings}, we apply \LNorm\ immediately before each sublayer (\PreNorm): 
  
This is cited as a stabilizer for Transformer training \cite{Chen2018, Wang2019-learning-deep-transformers} and is already implemented in popular toolkits \cite{tensor2tensor, fairseq, sockeye}, though not necessarily used by their default recipes. \citet{Wang2019-learning-deep-transformers} make a similar argument to motivate the success of \PreNorm\ in training very deep Transformers. Note that one must append an additional normalization after both encoder and decoder so their outputs are appropriately scaled. We compare \PostNorm\ and \PreNorm\ throughout \Cref{sec:experiments}.

\subsection{Weight initialization}
\label{ssec:weight-init}

Xavier normal initialization \cite{Glorot2010} initializes a layer's weights  ( is the hidden dimension) with samples from a centered normal distribution with layer-dependent variance:

Our experiments with this default initializer find that \PostNorm\ sometimes fails to converge, especially in our low-resource setting, even with a large number of warmup steps. One explanation is that Xavier normal yields initial weights that are too large. In implementations of the Transformer, one scales the word embeddings by a large value (e.g.,  for ), giving vectors with an expected square norm of . \LNorm's unit scale at initialization preserves this same effect. Since feedforward layers already have their weights initialized to a smaller standard deviation, i.e., , we propose reducing the attention layers' initializations from  to  as well (\SmallInit), as a corresponding mitigation. We evaluate the effect of this on \PostNorm\ vs.\ \PreNorm\ in \Cref{experiment_weight_init}.

\subsection{Scaled  normalization and \fixnorm}
\label{ssec:scaled-cosine}

\LNorm\ is inspired by batch normalization \cite{Ioffe2015}, both of which aim to reduce internal covariate shift by fixing the mean and variance of activation distributions. Both have been applied to self-attention \cite{NIPS2017_7181,Kool2019}. However, \citet{Santurkar2018} show that batch normalization's success has little to do with covariate shift, but comes instead from smoothing the loss landscape. For example, they divide by the pre-centered  norm instead of the variance and achieve similar or better results in image classification.

Hence, we propose replacing \LNorm\ with \keyword{scaled  normalization}:

This can be viewed as projecting -dimensional vectors onto a -dimensional hypersphere with learned radius . This expresses the inductive bias that each sublayer's activations has an ideal ``global scale,'' a notion we empirically validate in \Cref{ssec:g-values}. \SCNorm\ replaces the  scale and shift parameters of \LNorm\ with a single learned scalar, improving computational and parameter efficiency while potentially regularizing the loss landscape.

This bias has an explicit interpretation at the final layer: large inner products sharpen the output distribution, causing frequent words to disproportionately dominate rare words. This led \citet{Nguyen2018-improving-lexical-choice} to introduce  with fixed  at the last linear layer, to maximize the angular difference of output representations and aid rare word translation. By making  learnable, we can apply \SCNorm\ and \fixnorm\ jointly, which means applying the following at the final linear layer:

Note that this combination at the last layer is equivalent to cosine normalization \cite{Luo2017} with a learned scale.

\begin{table*}[ht!]
\small
\begin{minipage}{1.0\linewidth}
	\centering
	
	\begin{tabu}{@{}c|cccccccc@{}}
\toprule
      & \# egs. & \# src.\ + tgt.\ toks. & \# iters./epoch & max.\ epoch & \# enc./dec.\ layers & \# heads/layer & dropout & \# BPE \\ \midrule
\textbf{\glTOen} & 10k         & 0.37M                     & 100                 & 1000          & 4         & 4        & 0.4     & 3k     \\
\textbf{\skTOen} & 61k         & 2.32M                     & 600                 & 200           & 6         & 8        & 0.3     & 8k     \\
\textbf{\enTOvi} & 133k        & 5.99M                     & 1500                & 200           & 6         & 8        & 0.3     & 8k     \\
\textbf{\enTOhe} & 212k        & 7.88M                     & 2000                & 200           & 6         & 8        & 0.3     & 8k     \\
\textbf{\arTOen} & 214k        & 8.09M                     & 2000                & 200           & 6         & 8        & 0.3     & 8k     \\
\bottomrule
\end{tabu}
	
\end{minipage}
\caption{Data and model properties for low-resource NMT. \enTOvi\ is from IWSLT 2015; the rest are from the TED Talks corpus.}
\label{tab:stats}
\end{table*}
 

\subsection{Learning rates}
\label{ssec:learning-rate}

Despite using an adaptive optimizer, Adam \cite{Kingma2014}, Transformer training uses a learning rate (LR) schedule with a linear \keyword{warmup} and an inverse square root \keyword{decay} (\ORGLR):

where  is the hidden dimension of the self-attention layers, and ,  are hyperparameters that determine the highest learning rate achieved and the number of steps to reach it, respectively. These two hyperparameters have been the subject of much empirical study \cite{Popel2018, Ott2018}. In light of our modifications however, we revisit various aspects of this schedule:

\paragraph{Warmup-free training.} We conjectured that warmup is primarily needed when using \PostNorm\ to gradually learn \LNorm\ parameters without gradient explosion/vanishing (\Cref{ssec:identity-mappings}). Hence, we evaluate both \PreNorm\ and \PostNorm\ without warmup in \Cref{experiment_lr}.

\paragraph{Large learning rates.} To speed up training, one often explores using larger learning rates. In the context of Transformer, \citet{Ott2018} and \citet{Aharoni2019} take  instead of the conventional . \citet{Ott2018} showed that one can scale up Adam's learning rate to  with an extremely large batch (400k tokens). However, the improved convergence provided by our modifications could enable higher learning rates with much small batch sizes (4k tokens), as examined in \Cref{experiment_lr}.

\paragraph{Validation-based decay.} For similar reasons, one might wish to adopt a classic validation-based decay, i.e., training at a high learning rate for as long as tenable, decaying rapidly when development scores flatline. This has inspired usage of fixed decay schemes upon convergence with \ORGLR\ \cite{Dong2018, Salazar2019}. We revisit \VALBASED\ under our modifications, where we still perform a linear warmup but then multiply by a scale  when performance on a development set does not improve over  evaluations.
 \section{Experiments and results}
\label{sec:experiments}
We train Transformer models for a diverse set of five low-resource translation pairs from the TED Talks \cite{Qi2018-word-embeddings-nmt} and the \IWSLT\ \cite{Cettolo2015} corpora. Details are summarized in \Cref{tab:stats}. For more information motivating our choice of pairs and for exact training details, refer to \Cref{appendix:setup}.

\subsection{Large vs. small initialization} \label{experiment_weight_init}
To see the impact of weight initialization, we run training on the \enTOvi\ dataset using warmup steps of {4k, 8k, 16k} (\Cref{tab:big_small_init}). With default initialization, \PostNorm\ fails to converge on this dataset even with a long warmup of 16k steps, only reaching 5.76 BLEU.

\begin{table}[h]
\small
\begin{minipage}{1.0\linewidth}
	\centering
\begin{tabu}{@{}l|l|ccc@{}}
\toprule
      \multirow{2}{*}{Xavier normal} & & \multicolumn{3}{c}{\# warmup steps}       \\
      & & 4k & 8k & 16k \\ \midrule
\multirow{2}{*}{Baseline}   & \PostNorm    & fail  & fail  & 5.76  \\
                            & \PreNorm   & 28.52 & 28.73 & 28.32 \\ \midrule
\multirow{2}{*}{\SmallInit} & \PostNorm    & 28.17 & 28.20  & 28.62 \\
                            & \PreNorm    & 28.26 & 28.44 & 28.33 \\
\bottomrule
\end{tabu}
\end{minipage}
\caption{Development BLEU on \enTOvi\ using Xavier normal initialization (baseline versus \SmallInit).}
\label{tab:big_small_init}
\end{table} 

\begin{table*}[ht]
\small
\begin{minipage}{1.0\linewidth}
    \centering
\begin{tabu}{@{}r|c|c|c|c|c|c@{}}
\toprule
                 & \multicolumn{1}{c|}{\textbf{\glTOen}}       & \multicolumn{1}{c|}{\textbf{\skTOen}}       & \multicolumn{1}{c|}{\textbf{\enTOvi}}       & \multicolumn{1}{c|}{\textbf{\enTOhe}}       & \multicolumn{1}{c|}{\textbf{\arTOen}}       &
                     \multicolumn{1}{c}{\textbf{average }} \\ \midrule
\PostNorm\ + \LNorm\ (published) & 16.2 & 24.0 & 29.09 & 23.66 & 27.84 & -4.05 \\\midrule
\PostNorm\ + \LNorm\ (1)  & 18.47 & 29.37 & 31.94 & 27.85 & 33.39 & +0.00 \\
\PreNorm\ + \LNorm\ (2)  & 19.09 & 29.45 & 31.92 & 28.13 & 33.79 & +0.27 \\
\PreNorm\ + \fixnorm\ + \LNorm\ (3) & 19.38 & 29.50 & 32.45 & 28.39 & 34.35 & +0.61 \\
\PreNorm\ + \fixnorm\ + \SCNorm\ (4) & 20.91 & 30.25 & 32.79 & 28.44 & 34.15 & +1.10 \\
\bottomrule
\end{tabu}
\end{minipage}
\caption{Test BLEU using \PostNorm\ or \PreNorm\ and different normalization techniques. Published values are from \citet{Wang2018-switchout, Neubig2019, Aharoni2019}. ,  and  indicate significant improvement of (3) over (2), (4) over (3), and (4) over (1), respectively;  via bootstrap resampling \cite{Koehn2004}.}
\label{tab:lnorm-scnorm}
\end{table*} \begin{table*}[ht!]
\small
\begin{minipage}{1.0\linewidth}
    \centering
\begin{tabu}{@{}l|c|c|c|c|c@{}}
\toprule
                 & \multicolumn{1}{c|}{\textbf{\glTOen}}       & \multicolumn{1}{c|}{\textbf{\skTOen}}       & \multicolumn{1}{c|}{\textbf{\enTOvi}}       & \multicolumn{1}{c|}{\textbf{\enTOhe}}       & \multicolumn{1}{c}{\textbf{\arTOen}}       \\ \midrule
\NOWARMUP\          & 18.00 & 28.92 & 28.91 & 30.33 & 35.40 \\
\ORGLR\          & 22.18 & 29.08 & 28.84 & 30.30 & 35.33 \\
\VALBASED\          & 21.45 & 29.46 & 28.67 & 30.69 & 35.46 \\
\ORGLR\ + \lrtwo & 21.92 & 29.03 & 28.76 & 30.50 & 35.33 \\
\VALBASED\ + \lrtwo & 21.63 & 29.49 & 28.46 & 30.13 & 34.95 \\
\bottomrule
\end{tabu}
\end{minipage}
\caption{Development BLEU for \PreNorm\ + \fixnorm\ + \SCNorm, trained with different learning rate schedulers.}
\label{tab:learning-rate}
\end{table*} 

The second row shows that taking a smaller standard deviation on the attention weights (\SmallInit) restores convergence to \PostNorm. Though the  adjustment used here seems marginal, operations like residual connections and the products between queries and keys can compound differences in scale. Though both models now achieve similar performance, we note that \PreNorm\ works in all setups, suggesting greater stability during training. For all remaining experiments, we use \PostNorm\ and \PreNorm\ with \SmallInit. We find this choice does not affect the performance of \PreNorm.


\subsection{Scaled  normalization and \fixnorm}
\label{sec:experiments_scnorm}

To compare \SCNorm\ and \LNorm, we take 8k warmup steps for all further experiments. Since we tie the target input word embedding and the last linear layer's weight (\Cref{appendix:setup}), \fixnorm\ is implemented by applying  normalization to the word embedding, with each component initialized uniformly in . For non-\fixnorm\ models, word embeddings are initialized with mean 0 and standard deviation  so they sum to unit variance. All 's in \SCNorm\ are initialized to .

\Cref{tab:lnorm-scnorm} shows our results along with some published baselines. First, note that our Transformer baselines with \PostNorm\ + \LNorm\ (1) are very strong non-multilingual NMT models on these pairs. They outperform the best published numbers, which are all Transformer models in the past year, by an average margin of +4.0 BLEU. Then, we see that \PreNorm\ (2) achieves comparable or slightly better results than \PostNorm\ on all tasks. \fixnorm\ (3) gives an additional gain, especially on \arTOen\ ().

Finally, we replace \LNorm\ with \SCNorm\ (4). \SCNorm\ significantly improves on \LNorm\ for two very low-resource pairs, \glTOen\ and \skTOen. On the other tasks, it performs comparably to \LNorm. Upon aggregating all changes, our final model with \SCNorm\ and \fixnorm\ improves over our strong baseline with \PostNorm\ on all tasks by an average of +1.1 BLEU (), with each change contributing an average of at least +0.3 BLEU. In \Cref{ssec:g-values} and \Cref{ssec:generalization}, we further examine where the performance gains of \SCNorm\ come from.

Moreover, \SCNorm\ is also faster than \LNorm. Recall that for each vector of size , \LNorm\ needs to compute mean, standard deviation, scaling, and shifting, which costs  operations. For \SCNorm, we only need  operations to perform normalization and global scaling. This does not account for further gains due to reduction in parameters. In our implementation, training with \SCNorm\ is around 5\% faster than with \LNorm, similar to the speedups on NMT observed by \citet{Zhang2019}'s \RMSNorm\ (which can be viewed as \SCNorm\ with per-unit scales; see \Cref{ssec:g-values}).

\subsection{Learning rates} \label{experiment_lr}

We compare the original learning rate schedule in equation \ref{xmer-lr} (\ORGLR) with validation-based decay (\VALBASED), possibly with no warmup (\NOWARMUP). We use ,  for \ORGLR\ and \VALBASED. For \NOWARMUP, we instead use a learning rate of  for all datasets. For both \VALBASED\ and \NOWARMUP, we take  and . For experiments with high learning rate, we use either \VALBASED\ or \ORGLR\ with  (giving a peak learning rate of ). All experiments use \PreNorm\ + \fixnorm\ + \SCNorm.

In \Cref{tab:learning-rate}, we see that \NOWARMUP\ performs comparably to \ORGLR\ and \VALBASED\ except on \glTOen. We believe that in general, one can do without warmup, though it remains useful in the lowest resource settings. In our \lrtwo\ experiments, we can still attain a maximum learning rate of  without disproportionately overfitting to small datasets like \glTOen.
 
One might hypothesize that \VALBASED\ converges more quickly to better minima than \ORGLR\ by staying at high learning rates for longer. However, both schedulers achieve similar results with or without doubling the learning rate. This may be due to the tail-end behavior of \VALBASED\ methods, which can involve multiplicative decays in rapid succession. Finally, our \lrtwo\ experiments, while not yielding better performance, show that \PreNorm\ allows us to train the Transformer with a very high learning rate despite small batches (4k tokens).  

Since \PreNorm\ can train without warmup, we wonder if \PostNorm\ can do the same. We run experiments on \enTOvi\ with \NOWARMUP, varying the number of encoder/decoder layers.  As seen in \Cref{tab:no-warm-post-prev}, \PostNorm\ often fails without warmup even with 5 or 6 layers. Even at 4 layers, one achieves a subpar result compared to \PreNorm. This reaffirms \Cref{experiment_weight_init} in showing that \PreNorm\ is more stable than \PostNorm\ under different settings.
\begin{table}[h]
\small
\begin{minipage}{1.0\linewidth}
	\centering
\begin{tabu}{@{}r|ccc@{}}
\toprule
    & 4 layers & 5 layers & 6 layers              \\ \midrule
\PostNorm & 18.31 & fails & fails \\
\PreNorm & 28.33 & 28.13 & 28.32           \\
\bottomrule
\end{tabu}
\end{minipage}
\caption{Development BLEU on \enTOvi\ using \NOWARMUP, as number of encoder/decoder layers increases.}
\label{tab:no-warm-post-prev}
\end{table} 

\subsection{High-resource setting} \label{experiment_highres}

Since all preceding experiments were in low-resource settings, we examine if our claims hold in a high-resource setting. We train the Transformer base model on \WMT\ English-German using \FairSeq\ and report tokenized BLEU scores on \keyword{newstest2014}. Implementation of our methods in \FairSeq\ can be found in \Cref{listing-fairseq}.

In \Cref{tab:high-resource}, \SCNorm\ and \fixnorm\ achieve equal or better results than \LNorm. Since \SCNorm\ is also faster, we recommend using both as drop-in replacements for \LNorm\ in all settings. Surprisingly, in this task \PostNorm\ works notably better than \PreNorm; one observes similar behavior in \citet{Wang2019-learning-deep-transformers}. We speculate this is related to identity residual networks acting like shallow ensembles \cite{resnetensemble} and thus undermining the learning of the longest path; further study is required.

\begin{table}[h]
\small
\begin{minipage}{1.0\linewidth}
	\centering
\begin{tabu}{@{}r|c@{}}
\toprule
    & \multicolumn{1}{c}{\textbf{newstest2014}}       \\\midrule
\PostNorm\ + \LNorm\ (published) & 27.3 \\\midrule
\PreNorm\ + \LNorm & 26.83 \\
\PreNorm\ + \fixnorm\ + \SCNorm & 27.07 \\
\PostNorm\ + \LNorm & 27.58 \\
\PostNorm\ + \fixnorm\ + \SCNorm & 27.57 \\
\bottomrule
\end{tabu}
\end{minipage}
\caption{BLEU scores from \WMT\ English-to-German. Published value is from \citet{NIPS2017_7181}.}
\label{tab:high-resource}
\end{table}  \section{Analysis}
\label{sec:analysis}

\subsection{Performance curves}
\label{sec:perf-curve}

\Cref{fig:dev_bleus} shows that \PreNorm\ not only learns faster than \PostNorm, but also outperforms it throughout training. Adding \fixnorm\ also gives faster learning at first, but only achieves close performance to that with \PreNorm\ and no \fixnorm. However, once paired with \SCNorm, we attain a better BLEU score at the end. Because of the slow warmup period, \SCNorm\ with warmup learns slower than \SCNorm\ without warmup initially; however, they all converge at about the same rate.  

\begin{figure}[h!]
\centering
\includegraphics[width=0.45\textwidth]{figures/dev_bleus.pdf}
\caption{Development BLEU on \enTOvi\ with \PostNorm\ or \PreNorm, and with \LNorm\ or \SCNorm.}
\label{fig:dev_bleus}
\end{figure}

To visualize how \PreNorm\ helps backpropagation, we plot the global gradient norms from our runs in \Cref{fig:gnorm}. \PostNorm\ produces noisy gradients with many sharp spikes, even towards the end of training. On the other hand, \PreNorm\ has fewer noisy gradients with smaller sizes, even without warmup. \LNorm\ has lower global norms than \SCNorm\ + \fixnorm\, but it has more gradient components corresponding to normalization.

\begin{table*}[ht!]
\small
\begin{minipage}{1.0\linewidth}
    \centering
\begin{tabu}{@{}l|c|c|c|c|c@{}}
\toprule
                 & \multicolumn{1}{c|}{\textbf{\glTOen}}       & \multicolumn{1}{c|}{\textbf{\skTOen}}       & \multicolumn{1}{c|}{\textbf{\enTOvi}}       & \multicolumn{1}{c|}{\textbf{\enTOhe}}       & \multicolumn{1}{c}{\textbf{\arTOen}}       \\ \midrule
\RMSNorm\ + \fixnorm\        & 20.92 & 30.36 & 32.54 & 28.29 & 33.67 \\
\SCNorm\ + \fixnorm\          & 20.91 & 30.25 & 32.79 & 28.44 & 34.15 \\
\SCNormOne\ + \fixnorm\ (learned)          & 21.18 & 30.36 & 32.66 & 28.19 & 34.11 \\
\SCNormOne\ + \fixnorm\ (learned) + \VALBASED\ & 20.36 & 30.45 & 32.83 & 27.97 & 33.98 \\
\SCNormOne\ + \fixnorm\ (learned)  + \VALBASED\ + \lrtwo\ & 21.15 & 30.57 & 31.81 & 25.00 & 28.92 \\
\bottomrule
\end{tabu}
\end{minipage}
\caption{Test BLEU of -based normalization techniques with different numbers of learned :  vs.\  vs.\ .}
\label{tab:g-ablation}
\end{table*} 

\begin{figure}[h!]
\centering
\includegraphics[width=0.45\textwidth]{figures/gnorm.pdf}
\caption{The global norm of gradients when using \PostNorm\ or \PreNorm, and with \LNorm, \SCNorm\ and \fixnorm. Best viewed in color.}
\label{fig:gnorm}
\end{figure}

\begin{figure*}[ht!]
\centering
\begin{subfigure}[b]{0.50\textwidth}
    \includegraphics[width=0.9\textwidth]{figures/att_scales.pdf}
\end{subfigure}~
\begin{subfigure}[b]{0.50\textwidth}
    \includegraphics[width=0.9\textwidth]{figures/non_att_scales.pdf}
\end{subfigure}
\caption{Learned  values for \PreNorm\ + \SCNorm\ + \fixnorm\ models, versus depth. \textbf{Left:} Attention sublayers (\keyword{decoder-encoder} denotes decoder sublayers attending on the encoder). \textbf{Right:} Feedforward sublayers and the final linear layer.}
\label{fig:scales}
\end{figure*}

\begin{figure*}[ht!]
\centering
\begin{subfigure}[b]{0.50\textwidth}
    \includegraphics[width=0.9\textwidth]{figures/lb_att_scales.pdf}
\end{subfigure}~
\begin{subfigure}[b]{0.50\textwidth}
    \includegraphics[width=0.9\textwidth]{figures/lb_non_att_scales.pdf}
\end{subfigure}
\caption{Learned  values for our \PreNorm\ + \SCNorm\ + \fixnorm\ \enTOvi\ model (with and without label smoothing), versus depth. \textbf{Left} and \textbf{Right} are the same as in \Cref{fig:scales}.}
\label{fig:scales-ls}
\end{figure*}

\subsection{Activation scaling and the role of }
\label{ssec:g-values}

One motivation for \SCNorm\ was that it expressed a good inductive bias for the global scaling of activations, independent of distributional stability (\Cref{ssec:scaled-cosine}). In contrast, a contemporaneous work \cite{Zhang2019} proposes \keyword{root mean square layer normalization} (\RMSNorm), which still follows layer normalization's motivation but reduces overhead by forgoing additive adjustments, using only a scaling  per activation . Despite their differing motives, tying the  of \RMSNorm\ and dividing by  retrieves \SCNorm.

Hence we can frame our comparisons in terms of number of learnable parameters. We rerun our \PreNorm\ experiments with \RMSNorm. We also consider fixing  for \SCNorm, where only \fixnorm\ has learnable . \Cref{tab:g-ablation} shows that \SCNorm\ always performs comparably or better than \RMSNorm. Surprisingly, the fixed- model performs comparably to the one with learnable . However, at higher learning rates (\VALBASED\ with and without \lrtwo), fixed- models perform much worse on \arTOen, \enTOhe\, and \enTOvi. We conjecture that learning  is required to accommodate layer gradients.

 In \Cref{fig:scales}, we plot the learned  values for pairs with 100k+ examples. For all but the decoder-encoder sublayers, we observe a positive correlation between depth and , giving credence to \SCNorm's inductive bias of global scaling. This trend is clearest in the decoder, where  linearly scales up to the output layer, perhaps in tandem with the discriminativeness of the hidden representations \cite{LiangHL18}. We also note a negative correlation between the number of training examples and the magnitude of  for attention sublayers, which may reflect overfitting.

Finally, to affirm our intuition for interpreting , we plot  values with and without label smoothing (\Cref{fig:scales-ls}). We see a difference in later layers of the decoder; there, removing label smoothing results in lower  values except at the output layer, where  increases sharply. This corresponds to the known overconfidence of translation models' logits, on which label smoothing has a downscaling effect \cite{Muller2019}.
 \section{Conclusion}
In this work, we presented three simple, normalization-centric changes to the Transformer model, with a focus on NMT. First, we show that while \PostNorm\ performs better for high-resource NMT in the original base Transformer regime, \PreNorm\ is both more stable and more competent in low-resource settings. Second, we propose replacing \LNorm\ with \SCNorm, a fast and effective \keyword{scaled  normalization} technique which requires only a single learned parameter. Finally, we reaffirm the effectiveness of fixing the word embedding norm (\fixnorm). Altogether, \PreNorm\ + \fixnorm\ + \SCNorm\ significantly improves NMT on low-resource pairs, with the latter two performing comparably in the high-resource setting, but faster.

In the future, we would like to investigate the relationship between \PostNorm\ and \PreNorm\ when using other optimizers such as \radam\ \cite{radam}, which has been shown to improve Transformer training without warmup. We are also interested in seeing if \fixnorm\ or \SCNorm\ at the final linear layer remains effective when paired with an initialization method such as \fixup\ \cite{fixupinit}, which enables the training of deep neural networks without normalization. One could also explore using other  norms \cite{Santurkar2018}. 
\section*{Acknowledgements}
The authors would like to thank David Chiang and Katrin Kirchhoff for their support of this research.
 
\bibliography{paper}
\bibliographystyle{acl_natbib}

\appendix

\section{Training details}
\label{appendix:setup}

\paragraph{Data and preprocessing.} The pairs are English (en) to {Hebrew (he), Vietnamese (vi)}, and {Galician (gl), Slovak (sk), Arabic (ar)} to English (en). Because the data is already preprocessed, we only apply BPE \cite{Sennrich2016} with \texttt{fastBPE}\footnote{\url{https://github.com/glample/fastBPE}}. Depending on the data size, we use different numbers of BPE operations.

We wanted to compare with the latest low-resource works of \cite{Neubig2019, Aharoni2019} on the TED Talks corpus \cite{Qi2018-word-embeddings-nmt}. In particular, \citet{Aharoni2019} identified 4 very low-resource pairs (70k); we took the two (\glTOen, \skTOen) that were not extremely low (6k). They then identified 4 low-resource pairs with 100k-300k examples; we took the top two (\arTOen, \enTOhe). To introduce a second English-source pair and to showcase on a well-understood task, we used the \enTOvi\ pair from \IWSLT\ with an in-between number of examples (133k). In this way, we have examples of different resource levels, language families, writing directions, and English-source versus -target.

\paragraph{Model configuration.} We set the hidden dimension of the feedforward sublayer to 2048 and the rest to 512, matching \citet{NIPS2017_7181}. We use the same dropout rate for output of sublayers, ReLU, and attention weights. Additionally, we also do word dropout \cite{Sennrich2016dropout} with probability 0.1. However, instead of zeroing the word embeddings, we randomly replace tokens with \texttt{UNK}. For all experiments, we use label smoothing of 0.1 \cite{label_smoothing_1, label_smoothing_2}. The source and target's input and output embeddings are shared \cite{tiedembed}, but we mask out words that are not in the target's vocabulary at the final output layer before softmax, by setting their logits to .

\paragraph{Training.} We use a batch size of 4096 and optimize using Adam \cite{Kingma2014} with the default parameters , , . Gradients are clipped when global norm exceeds 1.0 \cite{Pascanu2013}. An epoch is a predefined number of iterations for each pair. We stop training when a maximum number of epochs has been met or the learning rate becomes too small (). We also do early stopping when the development BLEU has not improved for 20 evaluations. For \glTOen, this number is 50. When doing validation-based decay, we use  and . For complete data and model statistics, please refer to \Cref{tab:stats}. The best checkpoint is selected based on the development BLEU score during training.

\paragraph{Evaluation.} We report tokenized BLEU \cite{papineni-etal-2002-bleu} with \texttt{multi-bleu.perl} to be comparable with previous works. We also measure statistical significance using bootstrap resampling \cite{Koehn2004}. For \WMT\ English-German, note that one needs to put compounds in ATAT format\footnote{\url{https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/utils/get_ende_bleu.sh}} before calculating BLEU score to be comparable with previous works.\\

\section{Further analysis}
\label{ssec:generalization}

\begin{table}[h!]
\small
\begin{minipage}{1.0\linewidth}
    \centering
\begin{tabu}{@{}c|cc|cc@{}}
\toprule
      & \multicolumn{2}{c|}{\LNorm}        & \multicolumn{2}{c}{\SCNorm}       \\
      & \textbf{train} & \textbf{test} & \textbf{train} & \textbf{test} \\ \midrule
\textbf{\glTOen} & 11.792 & 54.300  & 10.151 & 45.770 \\
\textbf{\skTOen} & 14.078 & 20.460 & 14.004 & 19.080 \\
\textbf{\enTOvi} & 15.961 & 17.950 & 16.719 & 17.100  \\
\textbf{\enTOhe} & 15.562 & 14.950 & 15.906 & 15.080 \\
\textbf{\arTOen} & 14.372 & 13.450 & 14.165 & 13.290 \\
\bottomrule
\end{tabu}
\end{minipage}
\caption{Label-smoothed train/test perplexities when using \LNorm\ and \SCNorm. }
\label{tab:train-test-ppl}
\end{table} 

We ask if improvements from \SCNorm\ on our low-resource tasks are due to improved regularization (a smaller generalization gap) or improved overall performance. We record smoothed train and test perplexities of our \PreNorm\ models in \Cref{tab:train-test-ppl}. We see suggestive results but no conclusive trends. For \arTOen, \glTOen, and \skTOen, train and test drop slightly, with test more so than train. For \enTOvi, train perplexity increases and test perplexity decreases an equivalent amount. For \enTOhe, our smallest change between \SCNorm\ and \LNorm, train perplexity negligibly increased and test perplexity remains the same.

\section{Listings}

See the following page.

\onecolumn

\paragraph{\SCNorm.}
\small
\begin{lstlisting}[language=Python]
  class ScaleNorm(nn.Module):
    """ScaleNorm"""
    def __init__(self, scale, eps=1e-5):
        super(ScaleNorm, self).__init__()
        self.scale = Parameter(torch.tensor(scale))
        self.eps = eps

    def forward(self, x):
        norm = self.scale / torch.norm(x, dim=-1, keepdim=True).clamp(min=self.eps)
        return x * norm
\end{lstlisting}
\normalsize

\paragraph{\FairSeq.}
\label{listing-fairseq}
We follow \FairSeq's tutorial\footnote{\url{https://github.com/pytorch/fairseq/blob/master/examples/scaling_nmt/README.md}} and train a \PostNorm\ Transformer base model using the following configuration:

\small
\begin{lstlisting}[language=bash]
  fairseq-train \
  data-bin/wmt16_en_de_bpe32k/ \
  --arch transformer_wmt_en_de \
  --share-all-embeddings \
  --optimizer adam \
  --adam-betas '(0.9, 0.98)' \
  --clip-norm 1.0 \
  --lr 0.001 \
  --lr-scheduler inverse_sqrt \
  --warmup-updates 4000 \
  --warmup-init-lr 1e-07 \
  --dropout 0.1 \
  --weight-decay 0.0 \
  --criterion label_smoothed_cross_entropy \
  --label-smoothing 0.1 \
  --max-tokens 8192 \
  --update-freq 10 \
  --attention-dropout 0.1 \
  --activation-dropout 0.1 \
  --max-epoch 40
\end{lstlisting}
\normalsize

For \PreNorm, simply include the flags:
\small
\begin{lstlisting}[language=bash]
	--encoder-normalize-before --decoder-normalize-before
\end{lstlisting}
\normalsize

For \SCNorm, we replace all {\LNorm}s in {\texttt{fairseq/models/transformer.py}} and {\texttt{fairseq/modules/transformer\_layer.py}} with \SCNorm\ (implemented above). For \fixnorm, we change the word embedding initialization to uniform with range  and normalize with {\lstinline{torch.nn.functional.normalize}}.  

We note that \FairSeq\ uses Xavier uniform initialization, which is big compared to our \SmallInit\ (\Cref{experiment_weight_init}). We conjecture that \FairSeq\ training remains stable thanks to its large batch size, which gives more stable gradients. 
 
\end{document}
