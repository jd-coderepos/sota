\documentclass{article} \usepackage{iclr2020_conference,times}

\usepackage{wrapfig,lipsum,booktabs}
\usepackage{hyperref}
\usepackage{url}
\usepackage{inconsolata}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{caption}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{mwe}
\usepackage{multirow}
\usepackage{siunitx}
\usepackage{wrapfig}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}

\renewcommand{\P}{\mathop{\mathbb{P}}}
\newcommand{\E}{\mathop{\mathbb{E}}}
\newcommand{\V}{\mathop{\mathbb{V}}}
\newcommand{\I}{\mathbb{I}}

\newcommand{\scrA}{\mathcal{A}}
\newcommand{\scrB}{\mathcal{B}}
\newcommand{\scrC}{\mathcal{C}}
\newcommand{\scrD}{\mathcal{D}}
\newcommand{\scrE}{\mathcal{E}}
\newcommand{\scrF}{\mathcal{F}}
\newcommand{\scrJ}{\mathcal{J}}
\newcommand{\scrH}{\mathcal{H}}
\newcommand{\scrI}{\mathcal{I}}
\newcommand{\scrG}{\mathcal{G}}
\newcommand{\scrK}{\mathcal{K}}
\newcommand{\scrL}{\mathcal{L}}
\newcommand{\scrM}{\mathcal{M}}
\newcommand{\scrN}{\mathcal{N}}
\newcommand{\scrO}{\mathcal{O}}
\newcommand{\scrP}{\mathcal{P}}
\newcommand{\scrQ}{\mathcal{Q}}
\newcommand{\scrR}{\mathcal{R}}
\newcommand{\scrS}{\mathcal{S}}
\newcommand{\scrT}{\mathcal{T}}
\newcommand{\scrU}{\mathcal{U}}
\newcommand{\scrV}{\mathcal{V}}
\newcommand{\scrW}{\mathcal{W}}
\newcommand{\scrX}{\mathcal{X}}
\newcommand{\scrY}{\mathcal{Y}}
\newcommand{\scrZ}{\mathcal{Z}}

\newcommand{\xb}{\bar{X}}
\newcommand{\vb}{\bar{V}}

\newcommand{\proj}{\mathop{\mathbf{proj}}}
\newcommand{\var}{\mathop{\mathbf{Var}}}
\newcommand{\ind}{\mathbf{1}}

\newcommand{\sep}{\;|\;}
\DeclareMathOperator{\sgn}{sgn}
\usepackage{mathtools}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

\newcommand{\LHS}{\text{LHS}}
\newcommand{\RHS}{\text{RHS}}

\newcommand{\unif}{\text{uniform}}
\newcommand{\gaus}{\text{Gaussian}}
\newcommand{\pois}{\text{Poisson}}
\newcommand{\lap}{\text{Laplace}}
\newcommand{\dir}{\text{Dirichlet}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\cip}{\overset{p}{\to}}

 \newcommand*\rot{\rotatebox{90}}

\title{Unsupervised Domain Adaptation \\
	through Self-Supervision}

\author{Yu Sun,
	Eric Tzeng,
	Trevor Darrell,
	Alexei A. Efros\\
	University of California, Berkeley\\
	\texttt{yusun@berkeley.edu,
	\{etzeng, trevor, efros\}@eecs.berkeley.edu} \\
}
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\iclrfinalcopy
\begin{document}
\maketitle

\begin{abstract}
   This paper addresses unsupervised domain adaptation, the setting where labeled training data is available on a source domain, but the goal is to have good performance on a target domain with only unlabeled data.
Like much of previous work, we seek to align the learned representations of the source and target domains while preserving discriminability. The way we accomplish alignment is by learning to perform auxiliary self-supervised task(s) on both domains simultaneously.  Each self-supervised task brings the two domains closer together along the direction relevant to that task. Training this jointly with the main task classifier on the source domain is shown to successfully generalize to the unlabeled target domain. 
The presented objective is straightforward to implement and easy to optimize.
We achieve state-of-the-art results on four out of seven standard benchmarks, and competitive results on segmentation adaptation. We also demonstrate that our method composes well with another popular pixel-level adaptation method.
\end{abstract}

\makeatletter
\def\blfootnote{\xdef\@thefnmark{}\@footnotetext}
\makeatother

\urlstyle{same}

\section{Introduction}
\label{intro}

Visual distribution shifts are fundamental to our constantly evolving world.
We humans face them all the time, e.g. when we navigate a foreign city, read text in a new font, or recognize objects in an environment we have never encountered before.
These real-world challenges to the human visual perception have direct parallels in computer vision. 
Formally, a distribution shift happens when a model is trained on data from one distribution (source), but the goal is to make good predictions on some other distribution (target) that shares the label space with the source.
Often computational models struggle even for pairs of distributions that humans find intuitively similar.

Our paper studies the setting of unsupervised domain adaptation, with labeled data in the source domain, but only \textit{unlabeled} data in the target domain. 
The general philosophy of the field is to induce alignment of the source and target domains through some transformation.
In the context of deep learning,
a convolutional neural network maps images to learned representations in some feature space, so inducing alignment is done by making the distribution shifts small between the source and target in this shared feature space
~\citep{csurka2017domain, wang2018deep, gopalan2011domain}.
If, in addition, such representations preserve discriminability on the source domain, then we can learn a good classifier on the source, which now generalizes to the target under the reasonable assumption that the representations of the two domains have the same ground truth.

\begin{figure}
	\begin{center}
		\setlength{\fboxsep}{0pt}
		\begin{subfigure}[b]{0.32\columnwidth}
			\fbox{\includegraphics[width=\textwidth]{figures/fig1-a}}\quad
			\caption{Source classifier only}
		\end{subfigure}
		\hfill
		\begin{subfigure}[b]{0.32\columnwidth}
			\fbox{\includegraphics[width=\textwidth]{figures/fig1-b}}\quad
			\caption{adding a self-supervised task}
		\end{subfigure}
		\hfill
		\begin{subfigure}[b]{0.32\columnwidth}
			\fbox{\includegraphics[width=\textwidth]{figures/fig1-c}}\quad
			\caption{adding more tasks}
		\end{subfigure}
	\end{center}
	\vspace{-.15in}
	\caption{\small
		We propose a method for unsupervised domain adaptation that uses self-supervision to align the learned representations of two domains in a shared feature space. Here we visualize how these representations might be aligned in the feature space.
		a) Without our method, the source domain is far away from the target domain, and a source classifier cannot generalize to the target.
		b) Training a shared representation to support one self-supervised task on both domains can align the source and target along one direction.
		c) Using multiple self-supervised tasks can further align the domains along multiple directions. Now the source and target are close in this shared feature space, and the source classifier can hope to generalize to the target.
	}
	\label{fig:alignment}
\end{figure}

Most existing approaches implement this philosophy of alignment by minimizing a measurement of distributional discrepancy in the feature space (details in \autoref{related}), often some form of maximum mean discrepancy (MMD) e.g. \cite{long2017deep}, or a learned discriminator of the source and target as an approximation to the total variation distance e.g. \cite{ganin2016domain}. Both measurements lead to the formulation of the training objective as a minimax optimization problem a.k.a. adversarial learning, which is known to be very difficult to solve.
Unless carefully balanced,
the push and pull in opposite directions can often cause wild fluctuations in the discrepancy loss and lead to sudden divergence (details in \autoref{related}).
Therefore, we propose to avoid minimax optimization altogether through a very different approach.

Our main idea is to achieve alignment between the source and target domains by training a model on {\em the same task} in both domains simultaneously. 
Indeed, if we had labels in both domains, we could simply use our original classification task for this.    However, since we lack labels in the target domain, we propose to use a {\em self-supervised} auxiliary task, which creates its own labels directly from the data (see \autoref{design}).  
In fact, we can use multiple self-supervised tasks, each one aligning the two domains along a direction of variation relevant to that task.   Jointly training all the self-supervised tasks on both domains together with the original task on the source domain produces well-aligned representations as shown in \autoref{fig:alignment}.

Like all of deep learning, we can only empirically verify that at least in our experiments, the model does not overfit by internally creating a different decision boundary for each domain along different dimensions, which would then yield bad results.  Recent research suggests that stochastic gradient descent is indeed unlikely to find such overfitting solutions with a costly decision boundary of high complexity (implicit regularization), even though the models have enough capacity \citep{zhang2016understanding, neyshabur2017geometry, arora2018stronger}.



The key contribution of our work is to draw a connection between unsupervised domain adaptation and self-supervised learning. 
While we do not propose any fundamentally new self-supervised tasks, 
we offer insights in \autoref{design} on how to select the right ones for adaptation, and propose in \autoref{method} a novel training algorithm on those tasks, using batches of samples from both domains. 
Additionally, we demonstrate that domain alignment could be achieved with a simple and stable algorithm, without the need for adversarial learning.  In \autoref{results}, we report state-of-the-art results on several standard benchmarks. 

 \interfootnotelinepenalty=10000

\section{Related work}
\label{related}

In this section we provide a brief overview of the two fields that our work would like to bridge. 

\subsection{Unsupervised domain adaptation}

Methods for unsupervised domain adaptation in computer vision can be divided into three broad classes.
The dominant class, which our work belongs to, aims to induce alignment between the source and the target domains in some feature space.
This has been done by 
optimizing for some measurement of distributional discrepancy.
One popular measurement is the maximum mean discrepancy (MMD)
-- the distance between the mean of the two domains in some reproducing kernel Hilbert space, where the kernel is chosen to maximize the distance
\citep{bousmalis2016domain, long2015learning, long2017deep}.
Another way to obtain a measurement of discrepancy is to train an adversarial discriminator that distinguishes between the two domains \citep{ganin2014unsupervised, ganin2016domain, tzeng2017adversarial}. 
However, both MMD and adversarial training are formulated as minimax optimization problems, which are widely known, both in theory and practice, to be very difficult
\citep{fedus2017many, duchi2016local, liang2017well, jin2019minmax}.
Since the optimization landscape is much more complex than in standard supervised learning, training often does not converge or converges to a bad local minimum \citep{goodfellow2016nips, nagarajan2017gradient, li2017limitations}, and requires carefully balancing the two sets of parameters (for minimization and maximization) so one does not dominate the other \citep{salimans2016improved, neyshabur2017stabilizing}.

To make minimax optimization easier, researchers have proposed numerous modifications to the loss function, network design, and training procedure
\citep{arjovsky2017wasserstein, gulrajani2017improved, karras2017progressive, courty2017optimal, sun2016deep, shu2018dirt, sener2016learning}.
Over the years, these modifications have yielded practical improvements on many standard benchmarks, but have also made the state-of-the-art algorithms very complicated.
Often practitioners are not sure which tricks are necessary for which applications, and implementing these tricks can be bug-prone and frustrating.
To make matters worse, since there is no labeled target data available for a validation set, practitioners have no way to perform hyper-parameter tuning or early stopping.

The second class of methods directly transforms the source images to resemble the target images with generative models
\citep{taigman2016unsupervised,hoffman2017cycada,bousmalis2017unsupervised}.
While similar to the first class in the philosophy of alignment, 
these methods operate on image pixels directly instead of an intermediate representation space, and therefore can benefit from an additional round of adaptation in some representation space.
In \autoref{gta2cityscapes} we demonstrate that composing our method with a popular pixel-level method yields stronger performance than either alone.

The third class of methods uses a model trained on the labeled source data to estimate labels on the target data, then trains on some of those estimated  pseudo-labels (e.g. the most confident ones), therefore bootstrapping through the unlabeled target data.
Sometimes called self-ensembling \citep{french2017self}, this technique is borrowed from semi-supervised learning, where it is called co-training \citep{saito2017asymmetric, zou2018unsupervised, chen2018adversarial, chen2011co}.
In contrast, our method uses joint training (of the main and self-supervised tasks), different from co-training in every aspect except the name.

\subsection{Self-supervised Feature Learning}
The emerging field of self-supervised learning uses the machinery of supervised learning on problems where external supervision is not available.
The idea is to use data itself as supervision for auxiliary (also called ``pretext'') tasks that learn deep feature representations which will hopefully be informative for downstream ``real'' tasks. 
Many such auxiliary tasks have been proposed in the literature, including colorization (predicting the chrominance channels of an image given its luminance) 
\citep{zhang2016colorful,larsson2017colorproxy,zhang2017split},
image inpainting~\cite{pathak2016context},
spatial context prediction~\citep{doersch2015unsupervised},
solving jigsaw puzzles~\citep{noroozi2016unsupervised}, 
image rotation prediction~\citep{gidaris2018unsupervised},
predicting audio from video~\citep{owens2016ambient},
contrastive predictive coding \citep{oord2018representation}, etc.
Researchers have also experimented with self-supervision on videos~\citep{Wang_UnsupICCV2015, CVPR2019_CycleTime}, and
combining multiple self-supervised tasks together~\citep{doersch2017multi}.

Typically, self-supervision is used as a pre-training step on unlabeled data (e.g. the ImageNet training set without labels) to initialize a deep learning model, followed by fine-tuning on a labeled training set (e.g. PASCAL VOC) and evaluating on the corresponding test set.  Instead, in this paper, we train the self-supervised tasks {\em together} with the main supervised task, encouraging a consistent representation that both aligns the two domains and does well on the main task\footnote{Note that it is possible to apply the standard pre-training followed by fine-tuning regime to unsupervised domain adaptation, doing self-supervised pre-training on the target domain (or both the source and the target) and then fine-tuning on the source.  However, this gives almost no benefit over no-adaptation baseline, and is far from being competitive.}.



Recently, self-supervision has also been used for other problem settings, such as improving robustness  \citep{hendrycks2019using}, domain generalization \citep{carlucci2019domain} and few-short learning \citep{su2019boosting}.  The most relevant paper to us is
\cite{ghifary2016deep}, which uses self-supervision for unsupervised domain adaptation, but not through alignment. 
Their algorithm trains a denoising autoencoder~\cite{vincent2008extracting} only on the target data, together with the main classifier only on the labeled source data.  They argue theoretically that this is better than training the autoencoder on both domains together.  However, their theory is based on the critical assumption that the domains are already aligned, which is rarely true in practice.  Consequently, their empirical results are much weaker than ours, as discussed in \autoref{results}.
Please see \autoref{more_related} for more detailed comparisons with these works. 
 \interfootnotelinepenalty=10000
\section{Designing self-supervised tasks for adaptation}
\label{design}

\begin{wraptable}{r}{0.45\textwidth}
   \vspace{-3ex}
  \centering
  \small
  \setlength{\tabcolsep}{2pt}
  \begin{tabular}{lcccc}
    \toprule
    \textbf{Task} & \multicolumn{4}{c}{\textbf{Images and self-supervised labels}} \\
    \midrule
    \multirow{2}{*}[2em]{Rotation} & \includegraphics[width=0.08\columnwidth]{figures/examples/orig_stl024.png} & \includegraphics[width=0.08\columnwidth]{figures/examples/rot090.png} & \includegraphics[width=0.08\columnwidth]{figures/examples/rot180.png} & \includegraphics[width=0.08\columnwidth]{figures/examples/rot270.png} \\
                                   & \enspace\ang{0} & \enspace{\ang{90}} & \enspace{\ang{180}} & \enspace{\ang{270}} \\
    \ \scrL_0(S; \phi, h_0) = 
\sum_{(x,y) \in S} L_0(h_0(\phi(x)), y).

\scrL_k(S,T; \phi,h_k) = 
\sum_{(f_k(x),\tilde{y}) \in F(S)} L_k(h_k(\phi(f_k(x))), \tilde{y}) +
\sum_{(f_k(x),\tilde{y}) \in F(T)} L_k(h_k(\phi(f_k(x))), \tilde{y}).

\label{optimize}
\begin{aligned}
& \underset{\phi, h_k, k=1...K}{\text{min.}}
& & \scrL_0(S; \phi,h_0) + \sum_{k=1}^{K} \scrL_k(S,T; \phi,h_k).
\end{aligned}

D(S',T'; \phi) = \bigg\|\frac{1}{m}\sum_{x \in S'} \phi(x) - 
\frac{1}{n}\sum_{x \in T'} \phi(x)\bigg\|_2,

where  and  are \emph{unlabeled}  source and target validation sets
\footnote{  above is also known as the discrepancy under the linear kernel from the perspective of kernel MMD.
\autoref{additional_selection} contains some additional explanation of why the mean distance is suitable for our heuristic, even though it is a specific form of MMD, which we claim to be difficult to optimize. The short answer is that it does not require minimax optimization.}.

Our heuristic combines  and the main task error on the (labeled) source validation set.
Denote  and  the measurement vectors of those two quantities respectively over  epochs. The final measurement vector is 

i.e. a normalized sum of the two vectors; the epoch at which we perform early stopping is then simply .
Intuitively, this heuristic roughly corresponds to our goal of inducing alignment while preserving discriminability.
 \section{Experiments}
\label{results}
\begin{table*}[t!]
\begin{center}
{
\vspace{-3ex}
\small
\setlength\tabcolsep{3pt}
\begin{tabular}{cccccccc}
\hline
Source & MNIST    	& MNIST   & SVHN & MNIST & USPS 	& CIFAR-10 & STL-10\\
Target & MNIST-M   & SVHN   & MNIST & USPS & MNIST	& STL-10 & CIFAR-10\\
\hline
DANN \citep{ganin2016domain}
	   				& 81.5 	   & 35.7		   & 73.6 	&	-			&	-			&	-			&	-	\\
DRCN \citep{ghifary2016deep}
					& -	     	   & 40.1		   & 82.0 	&	-			&	-			&	66.4	&	58.7\\
DSN  \citep{bousmalis2016domain}
					& 83.2 	   & -		   			& 82.7 	&	-			&	-			&	-			&	-	\\
kNN-Ad \citep{sener2016learning}
					& 86.7 	   & 40.3		   & 78.8 	&	-			&	-			&	-			&	-	\\
PixelDA \citep{bousmalis2017unsupervised}
					& 98.2 	   & -		   			& - 		&	-			&	-			&	-			&	-	\\
ATT \citep{saito2017asymmetric}
					& 94.2 	   & 52.8		   & 86.2 	&	-			&	-			&	-			&	-	\\
-Model \citep{french2017self}
					& -		 	   & 71.4		   & 92.0 	&	-			&	-			&	76.3	&	64.2\\
ADDA \citep{tzeng2017adversarial}
					& -		 	   & -		   			& 76.0 	&	89.4	& 90.1		&	-			&	-	\\
CyCADA \citep{hoffman2017cycada}
					& -		 	   & -		   			& 90.4 	&	95.6	& \textbf{96.5}		&	-	&	-	\\
VADA \citep{shu2018dirt}
					& 97.7 	   & 47.5		   & 97.9 	& -				&	-			&	80.0	& 73.5\\
DIRT-T \citep{shu2018dirt}
					& \textbf{98.9} 	   & 54.5   	&\textbf{99.4}	&	-	&	-	&	-	& 75.3\\
VADA (IN) \citep{shu2018dirt}
					& 95.7 	   & 73.3		   & 94.5 		& -				&	-			&	78.3	& 71.4\\
DIRT-T (IN) \citep{shu2018dirt}
					& 98.7 	   & \textbf{76.5}	 & \textbf{99.4} &	-	&	-	&	-	& 73.3\\
\hline
Source only VADA \& DIRT-T
					& 58.5 	& 27.9			& 77.0		& -				& - 			& 76.3		& 63.6\\
Source only VADA \& DIRT-T (IN)
					& 59.9 	& 40.9			& 82.4		& -				& - 			& 77.0		& 62.6\\
Source only our method
					& 44.9 	& 30.5			& 92.2		& 94.7		& 81.4 		& 75.6		& 56.1\\
\hline
R  & \textbf{98.9} & 61.3	& 85.8
\footnote{This entry uses both rotation and flip.}
							& \textbf{96.5}	& 90.2	& \textbf{81.2}	& 65.6\\
R+L+F		& -	& -	& -	& -	& -	& \textbf{82.1}	& \textbf{74.0}\\
\hline
\end{tabular}
}
\end{center}
\caption{
  Test accuracy (\%) on standard domain adaptation benchmarks.
  Our results are organized according to the self-supervised task(s) used: R for rotation, L for location, and F for flip.
  We achieve state-of-the-art accuracy on four out of the seven benchmarks.
} 
\vspace{-1ex}
\label{table.elementary}
\end{table*} 

 \blfootnote{
	Our code is available at 
 	\texttt{\url{https://github.com/yueatsprograms/uda_release}} for object recognition
	and \texttt{\url{https://github.com/erictzeng/ssa-segmentation-release}} for segmentation.
}

\subsection{Seven benchmarks for object recognition}
The seven benchmarks are based on the six datasets described in \autoref{details_six}, each with a predefined training set / test set split, and labels are available on both splits. 
Previous works (cited in \autoref{table.elementary}) have created those seven benchmarks by picking pairs of datasets with the same label space, treating one as the source and the other as the target, 
and training on the training set of the source with labels revealed and of the target with labels hidden.
Following the standard setup of the field, labels on the target test set should only be used for evaluation, not for hyper-parameter tuning or early stopping; 
therefore we apply the heuristic described in \autoref{selection}.

For the two natural scene benchmarks, we use all three tasks described in \autoref{design}: rotation, location and flip prediction. 
For the five benchmarks on digits we do not use location because it yields trivial solutions that do not encourage the learning of semantic concepts.
Given the image of a digit cropped into the four quadrants, location prediction can be trivially solved by looking at the four corners where a white stroke determines the category.
Adding flip to the digits does not hurt performance, but does not improve significantly either, so we do not report those results separately.

As shown in \autoref{table.elementary}, despite the simplicity of our method, we achieve state-of-the-art accuracy on four out of the seven benchmarks.
In addition, we show the source only results from our closest competitor (VADA and DIRT-T), and note that our source only results are in fact lower than theirs on those very benchmarks that we perform the best on; this indicates that our success is indeed due to effectiveness in adaptation instead of the base architecture. 

Our method fails on the pair of benchmarks with SVHN, on which rotation yields trivial solutions. 
Because SVHN digits are cropped from house numbers with multiple digits, majority of the images have parts of the adjacent digits on the side.
The main task head needs to look at the center, but the rotation head learns to look at the periphery and cheat.
This failure case shows that the success of our method is tied to how well the self-supervised task fits the application.
Practioners should use their domain knowledge evaluate how well the task fits, instead of blithely apply it to everything.

Also seen in \autoref{table.elementary} is that our method excels at object recognition in natural scenes, especially with all three tasks together.
For STL-10CIFAR-10, our base model is much worse than that of VADA and DIRT-T, but still beats all the baselines.
Adding location and flip gives an improvement of 9\%, on top of the 9\% already over source only.
This is not surprising since those tasks were originally developed for ImageNet pre-training i.e. object recognition in natural scenes
-- our method is very successful when the task fits the application well.

\subsection{Benchmark for semantic segmentation}
\label{gta2cityscapes}
\begin{table*}
\vspace{-3ex}

  \begin{center}
  \scriptsize
  \setlength{\tabcolsep}{2.5pt}
  \begin{tabular}{lcccccccccccccccccccc}
    \toprule
    \multicolumn{21}{c}{\textbf{GTA5  Cityscapes}} \\
    \midrule
     & \rot{road} & \rot{sidewalk} & \rot{building} & \rot{wall} & \rot{fence} & \rot{pole} & \rot{traffic light} & \rot{traffic sign} & \rot{vegetation} & \rot{terrain} & \rot{sky} & \rot{person} & \rot{rider} & \rot{car} & \rot{truck} & \rot{bus} & \rot{train} & \rot{motorbike} & \rot{bicycle} & \textbf{mIoU} \\ \midrule

    Source only                 & 28.8 &     12.7 &     39.6 &      9.4 &      3.5 &     18.1 &     22.7 &  9.4 &     80.9 &     12.4 &     45.8 &     53.9 &  9.6 &     74.7 &     20.9 &     15.0 &     0.0 &     19.4 &  3.9 &     25.3 \\
	Ours                        &     69.9 &     22.7 &     69.7 &     18.1 &      9.9 &     13.5 &     18.7 &  8.9 &     80.3 &     19.4 &     58.4 &     53.8 &  2.6 &     75.1 &     13.6 &      5.2 &     0.3 &      8.1 &  1.2   &   28.9 \\
    \midrule
    CyCADA    &     79.1 &     33.1 &     77.9 &     23.4 &     17.3 &     32.1 &     33.3 & 31.8 &     81.5 &     26.7 &     69.0 &     62.8 & 14.7 &     74.5 &     20.9 &     25.6 &     6.9 &     18.8 & 20.4 &     39.5 \\
    Ours + CyCADA               &     86.6 &     37.8 &     80.8 &     29.7 &     16.4 &     28.9 &     30.9 & 22.2 &     83.8 &     37.1 &     76.9 &     60.1 &  7.8 &     84.1 &     30.8 &     32.1 &     1.2 &     23.2 & 13.3 &     41.2 \\
    \midrule
    Oracle                      &     97.3 &     79.8 &     88.6 &     32.5 &     48.2 &     56.3 &     63.6 & 73.3 &     89.0 &     58.9 &     93.0 &     78.2 & 55.2 &     92.2 &     45.0 &     67.3 &    39.6 &     49.9 & 73.6 &     67.4 \\
    \bottomrule
  \end{tabular}
  \caption{
    Test accuracy (\%) on GTA5  Cityscapes.
    Our method significantly improves over source only, and also over CyCADA when combined. This indicates that additional self-supervision using our training algorithm further aligns the domains.
  }
  \label{table:gta-cityscapes}
  \vspace{-3ex}
  \end{center}
\end{table*}

To experiment with our method in more diverse applications, we also evaluate on a challenging simulation-to-real benchmark for semantic segmentation -- GTA5  Cityscapes. 
GTA5 \citep{richter2016playing} contains 24,966 video frames taken from the computer game, where dense segmentation labels are automatically given by the game engine.
Cityscapes \citep{cordts_cityscapes} contains 5,000 video frames taken from real-world dash-cams.
The main task is to classify every pixel in an image as one of the 19 classes shared across both datasets, and accuracy is measured by intersection over union (IoU).
The best possible results are given as the oracle, when the labels on Cityscapes are available for training, so typical supervised learning methods are applicable.

Our results are shown in \autoref{table:gta-cityscapes}, and implementation details in \autoref{details_seg}.
Our self-supervised tasks were designed for classification, where the label on an image depends on its global content, 
while in segmentation the labels tend to be highly local.
Nevertheless, with very little modification, 
we see significant improvements over the source only baseline.

We also experiment with combining our method with another popular unsupervised domain adaptation method --  CyCADA \citep{hoffman2017cycada}, designed specifically for segmentation.
Surprisingly, when operating on top of images produced by this already very strong baseline, our method further improves performance.
This demonstrates that pixel-level adaptation methods might still benefit from an additional round of adaptation by inducing alignment through self-supervision.
We emphasize that these results are obtained with a very simple instantiation of our method, as a start towards the development of
self-supervised tasks more suitable for semantic segmentation.
  \section{Discussion}

We hope that this work encourages future researchers in unsupervised domain adaptation to consider the study of self-supervision as an alternative to adversarial learning, and researchers in self-supervision to consider designing tasks and evaluating them in our problem setting. 
Most self-supervised tasks today were originally designed for pre-training and evaluated in terms of accuracy gains on a downstream recognition, localization or detection tasks.
It will be interesting to see if new self-supervised tasks can arise from the motivation of adaptation, for which alignment is the key objective.  Moreover, domain experts could perhaps incorporate their dataset specific knowledge into the design of a self-supervised task specifically for their application.

One additional advantage of our method, not considered in this paper, is that it might be particularly amenable to very small target sample size, when those other methods based on adversarial learning cannot accurately estimate the target distribution. We leave this topic for the future work.
 
\bibliography{iclr2020_conference}
\bibliographystyle{iclr2020_conference}

\newpage
\appendix
\section{Additional discussion on specific papers related to ours}
\label{more_related}
In this section we discuss the four papers mentioned in \autoref{related} that are related to ours, but different in training algorithm, philosophy or problem setting.

\emph{Deep Reconstruction-Classification Networks} \citep{ghifary2016deep}.
This method works in unsupervised domain adaptation, the same problem setting as ours. It learns a denoising autoencoder on the target, together with the main classifier on the source.
However, they use only the target domain for reconstruction, claiming both empirically and theoretically that it is better than using both domains together.
In contrast, we use both the source and target for self-supervision.
These two algorithmic differences really reflect our fundamental difference in philosophy.
They take the target data as analogous to the unlabeled (source) data in semi-supervised learning, so any form of self-supervision suitable for semi-supervised learning is good enough;
their theoretical analysis is also directly borrowed from that of semi-supervised learning, which concludes that it is necessary and sufficient to only use the target data for self-supervision.
We take data from both domains for self-supervision in order to align their feature distributions;
also, both conceptually and empirically, we cannot use reconstruction tasks because they are unsuitable for inducing alignment (see \autoref{design}).

Empirical experiments further support our arguments in three ways:
1) If we were to only use the target for our self-supervision tasks, we would perform barely better than the source only baseline, even on MNISTMNIST-M, the easiest benchmark where we would otherwise observe a huge improvement.
2) If we were to use the reconstruction task i.e. a denoising autoencoder, we would again perform barely better than the source only baseline, and sometimes even worse, as described in \autoref{design}.
3) As shown in \autoref{table.elementary}, our results are much better than theirs on all of the benchmarks by a large margin. This shows that implementing the philosophy of alignment, developed for unsupervised domain adaptation, is much superior in our problem setting to blithely borrowing from semi-supervised learning.

\emph{Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty} \citep{hendrycks2019using}.
This paper studies the setting of robustness, where there is no sample provided, even unlabeled, from the target domain.
Their method jointly trains for the main task and the self-supervised task \emph{only on the source domain}, really because there is no target data of any kind.
Because their problem setting is very challenging (with so little information provided on the target), their primary evaluation is on a dataset of CIFAR-10 images with the addition of 15 types of common corruptions (e.g. impulse noise, JPEG compression, snow weather, motion blur and pixelation), simulated by an algorithm. 
No idea on unsupervised domain adaptation is mentioned.

\emph{Domain Generalization by Solving Jigsaw Puzzles} \citep{carlucci2019domain}.
This paper studies two setting. The first is the robustness setting, exactly the same as in \cite{hendrycks2019using}, except that evaluation is done on MNISTMNIST-M and MNISTSVHN. 
Their baseline is also a method from the robustness community that trains on adversarial examples and uses no target data.
Again, because their problem setting is very challenging, the accuracy is low for both their proposed method and the baseline.
The second setting is called domain generalization, which is very similar to meta-learning. 
The goal is to perform well simultaenously on multiple distributions, all labeled.
Evaluation is done using the mean accuracy on all the domains.
Beside the name, there is little similarity between the setting of unsupervised domain adaptation and domain generalization, which has no unsupervised component.

\emph{Boosting Supervision with Self-Supervision for Few-shot Learning} \citep{su2019boosting}.
As evident from the title, this paper studies the setting of few-shot learning, where the goal is to perform well on a very small dataset. Again, there is no unsupervised component and little connection to our setting.

Most of our results have already been produced when \cite{carlucci2019domain} and \cite{su2019boosting} were presented at a conference. \cite{hendrycks2019using} was submitted to NeurIPS 2019 and should be considered concurrent work.

\section{Additional algorithmic details}
\label{practical}
In practice and for our experiments, the source and target datasets are often imbalanced in size. 
If we were to blithely solve for the objective in \autoref{optimize}, the domain with a larger dataset would carry more weight for every  because we are summing over all samples in both datasets. 
We would like to keep the two sums inside  roughly balanced such that in terms of features produced by , neither of the two domains would dominate the other. 
This is easy to achieve in our implementation through balanced batches. When we need to sample a batch for task , we simply sample half the batch from the source, and another half from the target, then put them together.
In the end, our implementation requires little change on top of an existing supervised learning codebase. Each self-supervised task is defined as a module, and adding a new one only amounts to defining the structural modifications and the loss function.

We optimize the objective in \autoref{optimize} with stochastic gradient descent (SGD). 
One can simply take a joint step on \autoref{optimize}, which covers all the losses for  and . 
However, the implementation would then have to store the gradients with respect to all  losses together.
For memory efficiency, our implementation loops over ; 
for each self-supervised task , it samples a batch of combined source and target data (without their original labels), structurally modifies the images by , creates new labels according to the modification, and obtains a loss for  and  on this batch. So a gradient step is taken for each , before finally a batch of original source images and labels are sampled for a gradient step on  and . 
In terms of test accuracy, these two implementations make little difference (usually less than 1\%), and the choice simply comes down to a time-memory trade-off.

\section{Additional discussion on the mean distance}
\label{additional_selection}
In this section we answer some potential questions about our selection rule, which uses the mean distance, from the perspective of a possibly confused reader.

\emph{Q}: 
The motivation of the paper is that methods based on minimax optimization, such as those using kernel-MMD, are difficult to optimize. Since you are using the mean distance (MMD under the linear kernel) for hyper-parameter tuning and early stopping, which is a form of model selection, how are you different from those other methods and why is optimization easy for you?

\emph{A}: 
Our method is fundamentally different from those other methods based on MMD, and therefore more amenable to optimization in the following two ways:
1) Even though we use the mean distance, which is a form of MMD, we never pose a minimax optimization problem. 
The model parameters minimize the loss functions of the tasks, which we hope makes the mean distance small (see \autoref{fig:mmd}).
Model selection using \autoref{selection} also \emph{minimizes} the mean distance.
In our method, all the loss functions, for model parameters and hyper-parameters, work towards the same goal of inducing alignment, so optimization is easier.
2) Even though hyper-parameter tuning and early stopping are a forms of model selection just like training, there is a qualitative difference in the degrees of freedom involved. For \autoref{selection}, when performing early stopping for example, optimization amounts to a grid search over the epochs after training is finished, and there is only one degree of freedom.

\emph{Q}: Why is the mean distance suitable for model selection when it comes to hyper-parameter tuning and early stopping, but not regular training?

\emph{A}: Again, we agree that hyper-parameter tuning and early stopping are a forms of model selection. However, unlike model parameters ranging in the hundred of thousands, there are at most a few hyper-parameters and only one parameter for early stopping. 
Model parameters can easily overfit to the mean distance while 
those few degrees of freedom we use cannot. 
This is precisely also the reason why previous works using MMD resort to minimax optimization. Since they use MMD for training, they must also give MMD enough degrees of freedom to work towards the opposite direction of optimization as the model parameters, so that it is not as easy to overfit.

\section{Details of the six datasets used for object recognition}
\label{details_six}
1) MNIST \citep{lecun1998gradient}: greyscale images of handwritten digits 0 to 9; 60,000 samples in the training set and 10,000 in the test set.

2) MNIST-M \citep{ganin2016domain}: constructed by blending MNIST digits with random color patches from the BSDS500 dataset \cite{arbelaez2011contour}; same training / test set size as MNIST.

3) SVHN \citep{netzer2011reading}: colored images of cropped out house numbers from Google Street View; the task is to classify the digit at the center; 73,257 samples in the training set, 26,032 in the test set and 531,131 easier samples for additional training.

4) USPS: greyscale images of handwritten digits only slightly different from MNIST; 7291 samples in the training set and 2007 in the test set.

5) CIFAR-10 \citep{krizhevsky2009learning}: colored images of 10 classes of centered natural scene objects; 50,000 samples in the training set and 10,000 in the test set.

6) STL-10 \citep{coates2011analysis}: colored images of objects only slightly different from CIFAR-10; 5000 samples in the training set and 8000 in the test set.

Because CIFAR-10 and STL-10 differ in one class category, we follow common practice \citep{shu2018dirt,french2017self, ghifary2016deep} and delete the offending categories, so each of these two datasets actually only has 9 classes.

\section{Implementation details on the object recognition benchmarks}
We use a 26-layer pre-activation ResNet \citep{ he2016identity} as our test-time model , where  is the last linear layer that makes the predictions, and  is everything before that. 
For unsupervised domain adaptation, there is no consensus on what base architecture to use among the previous works.
Our choice is simply base on the widespread adoption of the ResNet architecture and the ease of implementation.
In \autoref{table.elementary} we provide the source only results using our base architecture, and the ones from \cite{shu2018dirt}, our closest competitor.
Our source only results are in fact worse than theirs, indicating that our improvements are indeed made through adaptation.

At training time, the self-supervised heads  are simply linear layers connected to the end of  as discussed in \autoref{method}. 
There is no other modification on the standard ResNet. 
For all experiments on the object recognition benchmarks, we optimize our model with SGD using weight decay 5e-4 and momentum 0.9, with a batch size of 128.
We use an initial learning rate of 0.1 and a two milestone schedule, where the learning rate drops by a factor of 10 at each milestone. 
All these optimization hyper-parameters are taken directly from the standard literature \citep{he2016identity, huang2016deep, guo2017calibration} without any modification for our problem setting.
We select the total number of epochs and the two milestones based on convergence of the source classifier  and unsupervised classifiers .
Early stopping is done using the selection heuristic discussed in \autoref{selection}. 
For fair comparison with our baselines, we do not perform data augmentation, following previous works
\citep{hoffman2017cycada, sener2016learning, ghifary2016deep}.

\section{Implementation details on GTA5  Cityscapes}
\label{details_seg}
For our experiments, we initialize our model from the DeepLab-v3 architecture \citep{deeplabv3}, pre-trained on ImageNet, as commonly done in the field.
Each self-supervised head consists of a global average pooling layer on the pre-logit layer, followed by a single linear layer.
To take advantage of the large size of the natural scene images,
we use the continuous i.e. regression version of location prediction.
The self-supervised head is trained on the square loss,
to regress the coordinates (in two dimensions) that the patch is cropped from.
Natural for the regression version, 
instead of cropping from the quadrants like for the classification version on the small datasets, 
we instead crop out 400400 patches taken at random from the segmentation scenes. 
We optimize our model with SGD using a learning rate of 0.007 for 15,000 iterations, with a batch size of 48.
Once again, we use the selection heuristic in \autoref{selection} for early-stopping.
 \end{document}