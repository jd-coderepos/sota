\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor,colortbl}
\usepackage{booktabs}
\usepackage{pifont}
\usepackage{subfig}
\usepackage{subfiles}
\usepackage{csquotes}
\usepackage{multirow}

\def\ours{\texttt{\textbf{SLM}}\xspace}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\begin{document}

\title{Select, Label, and Mix: Learning Discriminative Invariant Feature Representations for Partial Domain Adaptation}



\author{Aadarsh Sahoo \ \ \ \ Rameswar Panda \ \ \ \ Rogerio Feris \ \ \ \ Kate Saenko \ \ \ \ Abir Das \\
 IIT Kharagpur,  MIT-IBM Watson AI Lab,  Boston University
\\
{\tt \small \{sahoo\_aadarsh@, abir@cse.\}iitkgp.ac.in}, {\tt \small \{rpanda@, rsferis@us.\}ibm.com},
{\tt \small saenko@bu.edu}
} 

\maketitle


\begin{abstract}
Partial domain adaptation which assumes that the unknown target label space is a subset of the source label space has attracted much attention in computer vision. Despite recent progress, existing methods often suffer from three key problems: negative transfer, lack of discriminability and domain invariance in the latent space. To alleviate the above issues, we develop a novel `Select, Label, and Mix' (SLM) framework that aims to learn discriminative invariant feature representations for partial domain adaptation. 
First, we present a simple yet efficient \enquote{select} module that automatically filters out the outlier source samples to avoid negative transfer while aligning distributions across both domains. Second, the \enquote{label} module iteratively trains the classifier using both the labeled source domain data and the generated pseudo-labels for the target domain to enhance the discriminability of the latent space.
Finally, the \enquote{mix} module utilizes domain mixup regularization jointly with the other two modules to explore more intrinsic structures across domains leading to a domain-invariant latent space for partial domain adaptation.
Extensive experiments on several benchmark datasets demonstrate the superiority of our proposed framework over state-of-the-art methods. 

\end{abstract} \vspace{-2mm}

\begin{figure*}[!t]
	\begin{center}
	\includegraphics[width=0.9\linewidth]{figures/UpdatedColorsNewTeaser.pdf}
	\end{center}
	\vspace{-3mm}
	\caption{\small \textbf{A conceptual overview of our approach}. Our approach adopts three unique strategies namely Select, Label and Mix in a unified framework to mitigate domain shift and generalize the model to an unlabelled target domain possessing a label space which is subset of that of the labelled source domain. Our Select module discards outlier samples from the source domain to eliminate negative transfer of untransferable knowledge. On the other hand, Label and Mix modules ensure discriminability and invariance of the latent space respectively while adapting the source classifier to the target domain in partial domain adaptation. Best viewed in color.}
	\label{fig:teaser}
 	\vskip -0.1in
\end{figure*}
 

\section{Introduction}
\label{sec:introduction}

Deep neural networks 
usually do not generalize well to domains that are not distributed identically to the training data.
Domain adaptation~\cite{csurka2017domain, wang2018deep} addresses this problem by transferring knowledge from a label-rich source domain to a target domain where labels are scarce or unavailable.
Despite impressive results on commonly used benchmark datasets, domain adaptation algorithms often assume that the source and target domains share the same label space~\cite{ganin2015unsupervised, gretton2012kernel, long2015learning, long2018conditional, long2016unsupervised, shen2017wasserstein, sun2016deep}. However, since large-scale labelled datasets are readily accessible as source domain data, a more realistic scenario is partial domain adaptation (PDA), which assumes that the target label space is a subset of the source label space, that has received increasing research attention recently~\cite{bucci2019tackling, chen2019domain, chen2020selective, hu2019multi, xu2019larger, zhang2018importance}. 


Several methods have been proposed to solve partial domain adaptation by reweighting source samples~\cite{bucci2019tackling, chen2019domain, chen2020selective, hu2019multi, xu2019larger, zhang2018importance}. 
However, (1) most of the existing methods still suffer from negative transfer due to the presence of outlier source domain classes, 
which cripples domain-wise transfer with untransferable knowledge; (2) in absence of labels, they often neglect the class-aware information in the target domain which fails to guarantee the discriminability of the latent space; and (3) given filtering of the outliers, samples from the source and the target domain are not alone sufficient to learn domain invariant features for such a complex problem. As a result, the domain classifier may falsely align unlabeled target samples with samples of a different class in the source domain, leading to inconsistent predictions. 

To address the above mentioned challenges, we propose a novel end-to-end \textbf{Select, Label, and Mix (\ours)} framework for learning discriminative invariant feature representation while preventing negative transfer in partial domain adaptation. Our framework consists of three unique modules working in concert, \textit{i.e.}, select, label and mix, as shown in Figure~\ref{fig:teaser}. First, the select module facilitates the identification of relevant source samples while preventing the negative transfer of untransferrable knowledge. To be specific, our main idea is to learn a model (referred to as the selector network) that outputs the posterior probabilities of all the binary decisions for selecting or discarding each source domain sample before aligning source and target distributions using an adversarial discriminator~\cite{ganin2016domain,long2018conditional}. As these decision functions are discrete and non-differentiable, we rely on a recent Gumbel Softmax sampling approach~\cite{jang2016categorical} to learn the policy jointly with the network parameters through standard back-propagation, without resorting to complex reinforcement learning settings, as in~\cite{chen2019domain, chen2020selective}. Second, we develop an efficient self-labeling strategy that iteratively trains the classifier using both the labeled source domain data and the generated soft pseudo-labels for the target domain to enhance the discriminabilty of the latent space. Finally, the mix module utilizes both intra-domain and inter-domain mixup regularization~\cite{zhang2017mixup} to generate convex combinations of pairs of training samples and their corresponding labels in both domains. Our proposed mix strategy not only helps to explore more intrinsic structures across domains leading to an invariant latent space, but also helps to stabilize the domain discriminator while bridging the distribution shift across domains. In each mini-batch, our unified framework simultaneously eliminates negative transfer by removing outlier source samples and learns discriminative invariant features by labeling and mixing samples respectively for partial domain adaptation. 
Experiments on several datasets illustrate the effectiveness of our proposed framework with state-of-the-art performance (e.g., our approach outperforms DRCN~\cite{li2020deep} (TPAMI'20) by 18.91\% on the challenging VisDA-2017~\cite{peng2017visda} benchmark).
To summarize, our key contributions include:

\begin{itemize}
\setlength{\itemsep}{-1pt}
    \item We propose a novel Select, Label, and Mix (\ours) framework for learning discriminative and invariant feature representation while preventing intrinsic negative transfer in partial domain adaptation. 
    \item We develop a simple and efficient source sample selection strategy where the selector network is jointly trained with the domain adaptation model using backpropagation through Gumbel Softmax sampling.
    \item We conduct extensive experiments on several PDA benchmark datasets, including Office31~\cite{saenko2010adapting}, Office-Home~\cite{venkateswara2017deep}, ImageNet-Caltech, and VisDA-2017~\cite{peng2017visda} to demonstrate the superiority of our proposed approach over state-of-the-art methods.
\end{itemize}
 

\begin{figure*}[!thbp]
	\begin{center}
	\includegraphics[width=\linewidth]{figures/NewMainDiagramUpdated.pdf}
	\end{center}
	\vskip -0.25in
	\caption{\small \textbf{Illustration of our proposed framework}.
	Our framework consists of a feature extractor  which maps the images to a common latent feature space, a classifier network  to provide class-wise predictions, a domain discriminator  to reduce domain discrepancy, and a selector network  for discarding outlier source samples (\enquote{Select}) to mitigate the problem of negative transfer in partial domain adaptation. Our approach also comprises of two additional modules namely \enquote{Label} and \enquote{Mix} that works in conjunction with the \enquote{Select} module to ensure the discriminability and domain invariance of the latent space. Given a mini-batch of source and target domain images, all the components are optimized jointly in an iterative manner. See Section~\ref{sec:proposedmethod} for more details. Best viewed in color.}
	\label{fig:main_figure}
 	\vskip -0.1in
\end{figure*} 
\section{Related Work}
\label{sec:relatedwork}

\noindent\textbf{Unsupervised Domain Adaptation.} Unsupervised domain adaptation which aims to leverage labeled source domain data to learn to classify unlabeled target domain data has been studied from multiple perspectives (see reviews~\cite{csurka2017domain, wang2018deep}). Various strategies have been developed, including methods for reducing the cross-domain divergence~\cite{gretton2012kernel, long2015learning, shen2017wasserstein, sun2016deep}, adding domain discriminators for adversarial training~\cite{chen2019joint, ganin2015unsupervised, ganin2016domain, long2015learning, long2018conditional, long2016unsupervised, pei2018multi, tzeng2017adversarial}, and image-to-image translation techniques~\cite{hoffman2018cycada,hu2018duplex,murez2018image}. Despite remarkable progress, UDA methods assume that label spaces across source and target domains are identical unlike the practical problem we consider in this work. 

\vspace{1mm}
\noindent\textbf{Partial Domain Adaptation.} 
Representative PDA methods train domain discriminators~\cite{cao2018partialsan, cao2018partialpada, zhang2018importance} with weighting, or use residual correction blocks~\cite{li2020deep}, or weight source examples based on their similarities to target domain~\cite{cao2019learning}. Most relevant to our approach is the work in~\cite{chen2019domain, chen2020selective} which uses Reinforcement Learning (RL) for source data selection in partial domain adaptation. RL policy gradients are
often complex, unwieldy to train and require techniques to reduce variance during training.
By contrast, our approach utilizes a gradient based optimization for relevant source sample selection which is extremely fast and computationally efficient.  
Moreover, while prior PDA methods try to reweigh source samples in some form or other, they often do not take class-aware information in target domain into consideration. Our approach instead, ensures discriminability and invariance of the latent space by considering both pseudo-labeling and cross-domain mixup with sample selection in an unified framework for partial domain adaptation.

\vspace{1mm}
\noindent\textbf{Self-training with Pseudo-Labels.} Deep self-training methods that focus on iteratively training the model by using both labeled source data and generated target pseudo-labels have been proposed for aligning both domains~\cite{inoue2018cross, mei2020instance, saito2017asymmetric, zhang2020label}. While majority of the methods directly choose hard pseudo-labels with high prediction confidence, the works in~\cite{zou2019confidence,zou2018unsupervised} use class-balanced confidence regularizers to generate soft pseudo-labels for unsupervised domain adaptation that share same label space across domains. Our work on the other hand iteratively utilizes soft pseudo-labels within a batch by smoothing one-hot pseudo-label to a conservative target distribution for partial domain adaptation.

\vspace{1mm}
\noindent\textbf{Mixup Regularization.} Mixup regularization~\cite{zhang2017mixup} or its variants~\cite{berthelot2019mixmatch, verma2019manifold} that train models on virtual examples constructed as convex combinations of pairs of inputs and labels are recently used in many computer vision tasks to improve the generalization of neural networks. A few very recent methods apply Mixup, but mainly for UDA to stabilize the domain discriminator~\cite{wu2020dual, xu2019adversarial, yan2020improve} or to smoothen the predictions~\cite{mao2019virtual}. Our proposed SLM strategy can be regarded as an extension of this line of research by introducing both intra-domain and inter-domain mixup not only to stabilize the discriminator but also to guide the classifier in enriching the intrinsic structure of the latent space to solve the more challenging partial domain adaptation task.

 

\section{Proposed Method}
\label{sec:proposedmethod}

Partial domain adaptation aims to mitigate the domain shift and generalize the model to an unlabelled target domain with a label space which is a subset of that of the labelled source domain. Formally, given a set of labelled source domain samples  and unlabelled target domain samples , with label spaces  and , respectively, where . Let  and  represent the probability distribution of data in source and target domain respectively. In case of PDA, we further have  and , where  is the distribution of source domain data in . Our goal is to develop an approach with the above given data to improve the performance of a model on .

\subsection{Approach Overview}
Figure~\ref{fig:main_figure} illustrates an overview of our proposed approach. Our framework consists of a feature extractor , a classifier network , a domain discriminator  and a selector network . Our goal is to improve the classification performance of the combined network  on . While the feature extractor  maps the images to a common latent space, the task of the classifier  is to output a probability distribution over the classes for a given feature from . Given a feature from , the discriminator  helps in minimizing the domain discrepancy by identifying the domain (either source or target) to which it belongs. The selector network  helps in reducing negative transfer by identifying outlier source samples from . On the other hand, label module utilizes the predictions of  to obtain the soft pseudo-labels for the target domain samples. Finally, the mix module leverages both pseudo-labeled target samples and source samples to generate augmented images for achieving a domain invariance in the latent space. During training, for a mini-batch of images, all the components are trained jointly in an iterative manner and during testing, we evaluate the performance using classification accuracy of the network  on target domain data .


\subsection{Select, Label, and Mix Framework} Our approach adopts three unique modules namely Select, Label and Mix in a unified framework for learning discriminative invariant feature representation while eliminating negative transfer in partial domain adaptation.
The individual modules are discussed below.


\vspace{1mm}
\noindent\textbf{Select Module.}
This module aims to identify and get rid of the outlier class samples in the source domain to minimize negative transfer in partial domain adaptation. Instead of using different heuristically designed criteria for weighting source samples, we develop a novel selector network , that takes images from the source domain as input and decides the relevant source samples to align with the target samples. 
Specifically, the selector network  performs robust selection by providing a discrete output of either a 0 (discard) or 1 (select) for each source sample, \textit{i.e.}, . 
However,  the fact that the decision policy is discrete makes the network non-differentiable and therefore difficult to optimize via standard backpropagation.
To resolve the non-differentiability and enable gradient descent optimization for the selector, we adopt Gumbel-Softmax distribution~\cite{jang2016categorical, maddison2016concrete} that uses the Gumbel-Max trick with softmax as a continuous relaxation to . For a binary decision space, as in our case (select or discard), the formulation is as follows: 

where 's are i.i.d samples from standard Gumbel distribution , where ,
 and  are output probability of the selector network for a sample to be selected and discarded respectively.  denotes temperature of the softmax. Clearly, when  0, the Gumbel-Softmax distribution is smooth and hence gradients can be computed with respect to  logits 's to train the selector network using backpropagation. As  approaches 0, the decision  becomes one-hot and discrete.
Note that to avoid any interference from the backbone feature extractor , we use a separate feature extractor for the select module, while making these decisions.   

For a given batch of source domain images  and target domain images , of size , the selector results in two subsets of source samples  and . For training the selector, we propose a triplet loss  that tries to bring  \&  closer while pushing apart  \&  in the latent feature space of  as follows:

where  represents the average Hausdorff distance between set of features  and . , with  being the entropy loss,  is the Softmax prediction of  and  is mean prediction of output .  is a regularization to restrict  from producing trivial all-0 or all-1 outputs as well as ensuring confident and diverse predictions by  for . During forward pass, we obtain discrete decisions for source images to discard the outliers, and during backward pass, we learn the parameters of selector network using Gumbel-Softmax while jointly training it with other components.

\vspace{1mm}
\noindent\textbf{Label Module.} While our select module helps in removing source domain outliers, it fails to guarantee the discriminability of the latent space due to the absence of class-aware information in the target domain. Specifically, given our main objective is to improve the classification performance on target domain samples, it becomes essential for the classifier to learn confident decision boundaries in the latent feature space. To this end, we propose a label module that provides additional self-supervision for target domain samples. Motivated by the effectiveness of confidence guided self-training~\cite{zou2019confidence}, we generate soft pseudo-labels for the target domain samples that efficiently attenuates the unwanted deviations caused by false and noisy one-hot pseudo-labels. For a target domain sample , the soft-pseudo-label  is computed as follows:

where  is the softmax probability of the classifier for class  given  as input, and  is a hyper-parameter that controls the softness of the label. 
The soft pseudo-label  is then used to compute the loss  for a given batch of target samples  as follows:

where  represents the cross-entropy loss.

\vspace{1mm}
\noindent\textbf{Mix Module.}
Learning a domain-invariant latent space is crucial for effective adaptation of a classifier from source to target domain. However, with limited samples per batch and after discarding the outlier samples, it becomes even more challenging in preventing over-fitting and learning domain invariant representation in partial domain adaptation. To mitigate this problem, we apply MixUp~\cite{zhang2017mixup} on the relevant source samples and the target samples for discovering ingrained structures in establishing domain invariance. Given  from the select module and  with corresponding labels  from the label module, we perform convex combinations of the images belonging to these two sets on pixel-level in three different ways namely, inter-domain, intra-source domain and intra-target domain to obtain the following sets of augmented data:

where , while  with   being the corresponding soft-pseudo-labels.  is the mix-ratio randomly sampled from a beta distribution  for . We use  in all our experiments. 
We utilize the new augmented images in training both the classifier  and the domain discriminator  as follows:

where  and  represent loss for classifier and domain discriminator respectively. Our mix strategy with the combined loss  not only helps to explore more intrinsic structures across domains leading to an invariant latent space, but also helps to stabilize the domain discriminator while bridging the distribution shift across domains.

\subsection{Optimization}
Besides the above three unique modules that are tailored for PDA, we use the standard supervised loss on the labeled source data and domain adversarial loss as follows:

where  is standard entropy-conditioned domain adversarial loss with weights   and  for source and target domain respectively~\cite{long2018conditional}. The overall loss  is

where , , and  are given by Equations (\ref{eq:loss_select}), (\ref{eq:loss_label}), and (\ref{eq:loss_mix}) respectively. We integrate all the modules into one framework, as shown in the Figure~\ref{fig:main_figure} and train the network jointly for partial domain adaptation.   
 
\setlength{\tabcolsep}{2pt}
\begin{table*}[!thbp]

\definecolor{Gray}{gray}{0.90}
\definecolor{LightCyan}{rgb}{0.88,1,1}

\newcolumntype{a}{>{\columncolor{Gray}}c}


\scriptsize
\begin{center}
\resizebox{\linewidth}{!}{
\begin{tabular}{ l || c  c   c   c   c   c | a  }
\hline
\multicolumn{8}{c}{\textbf{Office31}} \\
\hline
 \textbf{Method} &  \textbf{A  W} & \textbf{D  W} & \textbf{W  D} & \textbf{A  D} & \textbf{D  A} & \textbf{W  A} & \textbf{Average} \\
\hline
\hline
ResNet-50~\cite{he2016deep} \tiny(CVPR'16)        & 76.5  & 99.2  & 97.7  & 87.5  & 87.2  & 84.1  & 88.7 \\

\hline

 DAN~\cite{long2015learning} \tiny(ICML'15)    & 53.6  & 62.7  & 57.8  & 47.7  & 61.2  & 69.7  & 58.8 \\
 DANN~\cite{ganin2016domain} \tiny(JMLR'16)  & 62.8  & 71.6  & 65.6  & 65.1  & 78.9  & 79.2  & 70.5 \\
 CORAL~\cite{sun2016deep} \tiny(ECCV'16) & 52.1  & 65.2  & 64.1  & 58.0  & 73.1  & 77.9  & 65.1 \\
 ADDA~\cite{tzeng2017adversarial} \tiny(CVPR'16)  & 75.7  &  95.4  &  \underline{99.9}  &  83.4  &  83.6  &  84.3  &  87.0 \\
 RTN~\cite{long2016unsupervised} \tiny(NeurIPS'16) & 75.3  & 97.1  & 98.3  & 66.9  & 85.6  & 85.7  & 84.8 \\
 CDAN+E~\cite{long2018conditional} \tiny(NeurIPS'18)  & 80.5  &  99.0  &  98.1  &  77.1  &  93.6  &  91.7  &  90.0 \\
 JDDA~\cite{chen2019joint} \tiny(AAAI'19)  & 73.5  & 93.1  & 89.3  & 76.4  & 77.6  & 82.8  & 82.1 \\

\hline

 PADA~\cite{cao2018partialpada} \tiny(ECCV'18)  & 86.3  & \underline{99.3}  & \textbf{100}   & 90.4  & 91.3  & 92.6  & 93.3 \\
 SAN~\cite{cao2018partialsan} \tiny(CVPR'18)  & 93.9  &  \underline{99.3}  &  99.4  &  94.3  &  94.2  &  88.7  &  95.0 \\
 IWAN~\cite{zhang2018importance} \tiny(CVPR'18)  & 89.2  &  \underline{99.3}  &  99.4  &  90.5  &  \underline{95.6}  &  94.3  &  94.7 \\
ETN~\cite{cao2019learning} \tiny(CVPR'19)   & 93.4  & \underline{99.3}  & 99.2  & 95.5  & 95.4  & 91.7  & 95.8 \\
 DRCN~\cite{li2020deep} \tiny(TPAMI'20)	& 88.1 & \textbf{100.0} & \textbf{100.0} & 86.0 & \underline{95.6} & \underline{95.8} & 94.3 \\
 RTNet~\cite{chen2020selective} \tiny(CVPR'20) & 95.1  & \textbf{100.0}   & \textbf{100.0}   & \underline{97.8}  & 93.9  & 94.1  & 96.8 \\
 RTNet\textnormal{~\cite{chen2020selective} \tiny(CVPR'20)} & \underline{96.2}  & \textbf{100.0}   & \textbf{100.0}   & 97.6  & 92.3  & 95.4  & \underline{96.9} \\



\hline
 \textbf{\ours (Ours)} & \textbf{99.77}  & \textbf{100.00}  & 99.79  & \textbf{98.73}  & \textbf{96.10}  & \textbf{95.89}  & \textbf{98.38}   \\
\hline
\end{tabular}}
\end{center}
\vspace{-4mm}
\caption{\small \textbf{Performance on Office31.} 
Numbers show the accuracy (\%) of different methods on partial domain adaptation setting. We highlight the \textbf{best} and \underline{second best} method on each transfer task. While the upper section shows the results of some popular unsupervised domain adaptation approaches, the lower section shows results of existing partial domain adaptation methods. Our proposed framework, \ours achieves the best performance on 5 out of 6 transfer tasks including the best average performance among all compared methods.
}
\label{table:cls-office-31} \vspace{-1mm}
\end{table*}
 
\section{Experiments}
\label{sec:experiments}

We conduct extensive experiments to show that our \ours framework outperforms many competing approaches to achieve the new state-of-the-art on several PDA benchmark datasets. We also perform comprehensive ablation experiments and feature visualizations to verify the effectiveness of different components in detail.

\subsection{Experimental Setup}

\noindent\textbf{Datasets.} We evaluate the performance of our approach using several standard domain adaptation datasets under PDA setting, namely Office31~\cite{saenko2010adapting}, Office-Home~\cite{venkateswara2017deep} ImageNet-Caltech and VisDA-2017~\cite{peng2017visda}. 
Office31 contains 4,110 images of 31 classes from three distinct domains, namely Amazon (A), Webcam (W) and DSLR (D). We follow the setting in~\cite{chen2020selective,li2020deep} and select 10 classes shared by Office31 and Caltech256~\cite{griffin2007caltech} as target categories. Office-Home is a challenging dataset that contains images of everyday objects from four domains: Artistic images (Ar), Clipart images (Cl), Product images (Pr) and Real-World images (Rw). We follow~\cite{chen2020selective} to select the first 25 categories (in alphabetic order) in each domain as the target classes. 
ImageNet-Caltech is another challenging dataset that consists of two subsets, ImageNet1K (I)~\cite{russakovsky2015imagenet} and Caltech256 (C)~\cite{griffin2007caltech}. While source domain contains 1,000 and 256 classes for ImageNet and Caltech respectively, each target domain contains only 84 classes that are common across both domains.
VisDA-2017 is a large-scale dataset with 12 categories across 2 domains: one consists photo-realistic images or real images (R) and the other comprises of synthetic 2D renderings of 3D models (S). Following~\cite{li2020deep}, we select the first 6 categories (in alphabetical order) in each of the domain as the target categories.

\vspace{1mm}
\noindent\textbf{Baselines.} We compare our approach with several methods that fall into two main categories: (1) popular unsupervised domain adaptation methods like DAN~\cite{long2015learning}, DANN~\cite{ganin2016domain}, and CORAL~\cite{sun2016deep}, (2) existing partial domain adaptation methods including PADA~\cite{cao2018partialpada}, SAN~\cite{cao2018partialsan}, ETN~\cite{cao2019learning}, and DRCN~\cite{li2020deep}. We also compare with the recent state-of-the-art method, RTNet~\cite{chen2020selective} (CVPR'20) that uses reinforcement learning for source dataset selection in partial domain adaptation. We directly quote the numbers reported in published papers~\cite{chen2020selective,li2020deep} and use the same backbone network in our approach to make a fair comparison with different baselines.

\vspace{1mm}
\noindent\textbf{Implementation Details.} 
We use ResNet-50~\cite{he2016deep} as the backbone network for the feature extractor while we use ResNet-18 for the selector network, initialized with ImageNet~\cite{russakovsky2015imagenet} pretrained weights. 
In Eqn.~\ref{eq:loss_select} we set  and  as  and , respectively. We use gradient reversal layer (GRL) for adversarially training the discriminator. We set  in Eqn.~\ref{eq:gumbel_softmax},  in Eqn.~\ref{eq:soft_pseudo_label}, and  for the GRL as initial values and gradually anneal  and  down to 0 while increase  to 1.0 during the training, as in~\cite{jang2016categorical}. We set a threshold of  while obtaining the soft pseudo-labels for all the datasets except for the case when ImageNet was used as the source domain where we did not use any threshold value. Additionally, we use label-smoothing for all the losses for the feature extractor involving source domain images as in~\cite{liang2020we, muller2019does}, with .
We use SGD for optimization with momentum=0.9 while a weight decay of 1e-3 and 5e-4 for the selector network and the other networks respectively. We use an initial learning rate of 5e-3 for the selector and the classifier, while 5e-4 for the rest of the networks and decay it following a cosine annealing strategy. A batch size of 64 is used for all datasets except ImageNet-Caltech for which a batch size of 128 is used. We report average classification accuracy and standard deviation over 3 random trials. More implementation details are included in Appendix and we will make our source codes publicly available at \url{https://github.com/CVIR/Select-Label-Mix-SLM-PDA}.


\setlength{\tabcolsep}{1pt}
\begin{table*}[!thbp]

\definecolor{Gray}{gray}{0.90}
\definecolor{LightCyan}{rgb}{0.88,1,1}

\newcolumntype{a}{>{\columncolor{Gray}}c}


\scriptsize
\begin{center}
\resizebox{\linewidth}{!}{
\begin{tabular}{ l || c c c c c c c c c c c c | a  }
\hline
\multicolumn{14}{c}{\textbf{Office-Home}} \\
\hline
 \textbf{Method} &  \textbf{Ar  Cl} & \textbf{Ar  Pr} & \textbf{Ar  Rw} & \textbf{Cl  Ar} & \textbf{Cl  Pr} & \textbf{Cl  Rw} & \textbf{Pr  Ar} & \textbf{Pr  Cl} & \textbf{Pr  Rw} & \textbf{Rw  Ar} & \textbf{Rw  Cl} & \textbf{Rw  Pr} & \textbf{Average} \\
\hline
\hline
ResNet-50~\cite{he2016deep} \tiny(CVPR'16)   & 47.2  & 66.8  & 76.9  & 57.6  & 58.4  & 62.5  & 59.4  & 40.6  & 75.9  & 65.6  & 49.1  & 75.8  & 61.3 \\

\hline

 DAN~\cite{long2015learning} \tiny(ICML'15)      & 35.7  & 52.9  & 63.7  & 45.0  & 51.7  & 49.3  & 42.4  & 31.5  & 68.7  & 59.7  & 34.6  & 67.8  & 50.3 \\
 DANN~\cite{ganin2016domain} \tiny(JMLR'16)     & 43.2  & 61.9  & 72.1  & 52.3  & 53.5  & 57.9  & 47.2  & 35.4  & 70.1  & 61.3  & 37.0  & 71.7  & 55.3 \\
 CORAL~\cite{sun2016deep} \tiny(ECCV'16)    & 38.2  & 55.6  & 65.9  & 48.4  & 52.5  & 51.3  & 48.9  & 32.6  & 67.1  & 63.8  & 35.9  & 69.8  & 52.5 \\
 ADDA~\cite{tzeng2017adversarial} \tiny(CVPR'16)   &  45.2  &  68.8  &  79.2  &  64.6  &  60.0  &  68.3 &  57.6 &  38.9 &  77.5 &  70.3 &  45.2 &  78.3 &  62.8 \\
 RTN~\cite{long2016unsupervised} \tiny(NeurIPS'16) & 49.4  & 64.3  & 76.2  & 47.6  & 51.7  & 57.7  & 50.4  & 41.5  & 75.5  & 70.2  & 51.8  & 74.8  & 59.3 \\
 CDAN+E~\cite{long2018conditional} \tiny(NeurIPS'18) &  47.5  &  65.9  &  75.7  &  57.1  &  54.1  &  63.4 &  59.6 &  44.3 &  72.4 &  66.0 &  49.9 &  72.8 &  60.7 \\
 JDDA~\cite{chen2019joint} \tiny(AAAI'19)     & 45.8  & 63.9  & 74.1  & 51.8  & 55.2  & 60.3  & 53.7  & 38.3  & 72.6  & 62.5  & 43.3  & 71.3  & 57.7 \\
\hline

 PADA~\cite{cao2018partialpada} \tiny(ECCV'18)    & 53.2  & 69.5  & 78.6  & 61.7  & 62.7  & 60.9  & 56.4  & 44.6  & 79.3  & 74.2  & 55.1  & 77.4  & 64.5 \\
 SAN~\cite{cao2018partialsan} \tiny(CVPR'18)   &  44.4  &  68.7  &  74.6  &  67.5  &  65.0  &  \underline{77.8} &  59.8 &  44.7 &  80.1 &  72.2 &  50.2 &  78.7 &  65.3 \\
 IWAN~\cite{zhang2018importance} \tiny(CVPR'18)   &  53.9  &  54.5  &  78.1  &  61.3  &  48.0  &  63.3 &  54.2 &  52.0 &  81.3 &  76.5 &  56.8 &  82.9 &  63.6 \\
ETN~\cite{cao2019learning} \tiny(CVPR'19)      & 60.4  & 76.5  & 77.2  & 64.3  & 67.5  & 75.8  & 69.3  & 54.2  & 83.7  & 75.6  & 56.7  & \underline{84.5}  & 70.5 \\
 SAFN~\cite{xu2019larger} \tiny(ICCV'19)  & 58.9 & 76.3 & 81.4 & \underline{70.4} & \underline{73.0} & \underline{77.8} & \underline{72.4} & \underline{55.3} & 80.4 & 75.8 & \textbf{60.4} & 79.9 & 71.8 \\
 DRCN~\cite{li2020deep} \tiny(TPAMI'20)  & 54.0	& 76.4	& \underline{83.0}	& 62.1	& 64.5	& 71.0	& 70.8	& 49.8	& 80.5	& \underline{77.5}	& 59.1	& 79.9	& 69.0 \\
 RTNet~\cite{chen2020selective} \tiny(CVPR'20)    & \underline{62.7}  & 79.3  & 81.2  & 65.1  & 68.4  & 76.5  & 70.8  & \underline{55.3}  & \underline{85.2}  & 76.9  & 59.1  & 83.4  & 72.0 \\
 RTNet\textnormal{~\cite{chen2020selective} \tiny(CVPR'20)} & \textbf{63.2}  & \underline{80.1}  & 80.7  & 66.7  & 69.3  & 77.2  & 71.6  & 53.9  & 84.6  & 77.4  & 57.9  & \textbf{85.5}  & \underline{72.3} \\
\hline
 \textbf{\ours (Ours)} & 56.54  & \textbf{83.75}  & \textbf{90.48}  & \textbf{76.03}  & \textbf{73.99}  & \textbf{80.95}  & \textbf{72.97}  & \textbf{56.60}  & \textbf{87.32}  & \textbf{82.55}  & \underline{59.76}  & 82.52  & \textbf{75.29} \\
\hline
\end{tabular}}
\end{center}
\vspace{-5mm}
\caption{\small \textbf{Performance on Office-Home.} 
We highlight the \textbf{best} and \underline{second best} method on each task. 
While the upper section shows the results of unsupervised domain adaptation approaches, the lower section shows results of existing partial domain adaptation methods.
Our \ours framework achieves the best performance on 9 out of 12 tasks including the best average performance among all compared methods.
}
\label{table:cls-office-home} \vspace{-2mm}
\end{table*}
 
\subsection{Results and Analysis}

Table~\ref{table:cls-office-31} shows the results of our method and other competing approaches on Office31 dataset. We have the following key observations.
(1) As expected, the popular UDA methods fail to outperform the simple no adaptation model (ResNet-50), which implies that they suffer from negative transfer due to the presence of outlier source samples.
(2) Overall, our \ours framework outperforms all the existing PDA methods by achieving the best results on \textbf{5 out of 6} transfer tasks. Among PDA methods, ~\cite{chen2020selective} is the most competitive.
However, the gap is still significant (96.9\% vs 98.3\%) due to our two novel components working in concert with the removal of outliers: enhancing discriminability of the latent space via iterative pseudo-labeling of target domain samples and learning domain-invariance through mixup regularizations.
(3) Our approach performed remarkably well on transfer tasks from a large source domain to small target domains, e.g., on A  W, \ours outperforms  by \textbf{3.5\%}.
Likewise, our approach obtains \textbf{3.8\%} improvement over  on D  A task where the source domain is very small compared to the target domain. These results well demonstrate that our method improves the generalization ability of the source classifier in the target domain while reducing the negative transfer.


On the challenging Office-Home dataset, our proposed approach significantly outperforms all the compared methods to obtain the best performance of 75.3\% which is about \textbf{3\%} more than the previous state-of-the-art performance on this dataset (Table~\ref{table:cls-office-home}).
Our method obtains the best on \textbf{9 out of 12} transfer tasks.
Table~\ref{table:image} summarizes the results of different baselines on ImageNet-Caltech and VisDA-2017 datasets. Our approach achieves new state-of-the-art result, outperforming the next competitive method by a margin of about \textbf{2.8\%} and \textbf{18.9\%} on ImageNet-Caltech and VisDA-2017 datasets respectively. Especially for task S  R on VisDA-2017, our approach significantly outperforms SAFN~\cite{xu2019larger} and DRCN~\cite{li2020deep} by an increase of \textbf{24.1\%} and \textbf{33.5\%} respectively. Note that on the most challenging VisDA-2017 dataset, our approach is still able to distill more positive knowledge from the synthetic to the real domain despite significant domain gap across them. In summary, our \ours framework clearly outperforms the existing PDA methods on all four datasets, showing the effectiveness of our approach in not only identifying the most relevant source classes but also learning more transferable features for partial domain adaptation.

\setlength{\tabcolsep}{1pt}
\begin{table}[tbp]

\definecolor{Gray}{gray}{0.90}
\definecolor{LightCyan}{rgb}{0.88,1,1}

\newcolumntype{a}{>{\columncolor{Gray}}c}


\scriptsize
\resizebox{\columnwidth}{!}{
\begin{tabular}{ l || c  c | a | c c | a  }
\hline
\multicolumn{1}{c}{} & \multicolumn{3}{c}{\textbf{ImageNet-Caltech}} & \multicolumn{3}{c}{\textbf{VisDA-2017}} \\
\hline
 \textbf{Method} &  \textbf{I  C} & \textbf{C  I} & \textbf{Average} & \textbf{R  S} & \textbf{S  R} & \textbf{Average} \\
\hline
\hline
 ResNet-50~\cite{he2016deep} \tiny(CVPR'16)   & 69.69 & 71.29 & 70.49 & 64.28 & 45.26 &  54.77 \\

\hline

 DAN~\cite{long2015learning} \tiny(ICML'15) & 71.57 & 66.48 & 69.03 & 68.35 & 47.60 & 57.98 \\
 DANN~\cite{ganin2016domain} \tiny(JMLR'16) & 68.67 & 52.97 & 60.82 & 73.84 & 51.01 & 62.43 \\
 ADDA~\cite{tzeng2017adversarial} \tiny(CVPR'16) & 71.82 & 69.32 & 70.57 & - & - & - \\
 RTN~\cite{long2016unsupervised} \tiny(NeurIPS'16) & 72.24 & 68.33 & 70.29 & 72.93 & 50.04 & 61.49 \\
 CDAN+E~\cite{long2018conditional} & 72.45 & 72.02 & 72.24 & - & - & - \\


\hline

 PADA~\cite{cao2018partialpada} \tiny(ECCV'18)  & 75.03 & 70.48 & 72.76 & \underline{76.50} & 53.53 & 65.01 \\
 SAN~\cite{cao2018partialsan} \tiny(CVPR'18)   & 77.75 & 75.26 & 76.51 & 69.70  & 49.90 & \\
 IWAN~\cite{zhang2018importance} \tiny(CVPR'18)   & 78.06 & 73.33 & 75.70 & 71.30  & 48.60 & \\
 ETN~\cite{cao2019learning} \tiny(CVPR'19)  & \textbf{83.23} & 74.93 & \underline{79.08} & - & - & - \\
 SAFN~\cite{xu2019larger} \tiny(ICCV'19) & - & - & - & - & \underline{67.65} & - \\
 DRCN~\cite{li2020deep} \tiny(TPAMI'20)    & 75.30 & \underline{78.90} & 77.10 & 73.20 & 58.20 & \underline{65.70} \\


\hline
 \textbf{\ours (Ours)} & \underline{82.31} & \textbf{81.41} & \textbf{81.86} & \textbf{77.48} & \textbf{91.74} & \textbf{84.61} \\


\hline
\end{tabular}}
\vspace{-2mm}
\caption{\small \textbf{Performance on ImageNet-Caltech and VisDA-2017.} Our \ours framework achieves new state-of-the-art performance on both datasets by significantly outperforming existing methods.
}
\label{table:image} \vspace{-2mm}
\end{table}
 
\setlength{\tabcolsep}{1pt}
\begin{table*}[htbp]

\definecolor{Gray}{gray}{0.90}
\definecolor{LightCyan}{rgb}{0.88,1,1}

\newcolumntype{a}{>{\columncolor{Gray}}c}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\scriptsize
\begin{center}
\resizebox{\linewidth}{!}{
\begin{tabular}{  P{0.8cm} | P{0.8cm} | P{0.8cm} || c c c c c c c c c c c c | a  }
\hline
\multicolumn{3}{c}{\textbf{Modules}} & \multicolumn{11}{c}{\textbf{Office-Home}} \\
\hline
\textbf{Select} & \textbf{Label} & \textbf{Mix} &  \textbf{Ar  Cl} & \textbf{Ar  Pr} & \textbf{Ar  Rw} & \textbf{Cl  Ar} & \textbf{Cl  Pr} & \textbf{Cl  Rw} & \textbf{Pr  Ar} & \textbf{Pr  Cl} & \textbf{Pr  Rw} & \textbf{Rw  Ar} & \textbf{Pr  Cl} & \textbf{Pr  Rw} & \textbf{Average} \\
\hline
\hline
\xmark & \xmark & \xmark & 44.16 & 61.64 & 75.94 & 54.58 & 55.18 & 65.03 & 50.99 & 37.25 & 69.59 & 64.80 & 42.37 & 71.37 & 57.74 \\
\cmark & \xmark & \xmark & 50.55 & 72.87 & 79.16 & 65.44 & 67.21 & 71.71 & 60.76 & 46.69 & 77.05 & 71.90 & 49.39 & 76.97 & 65.81 \\
\cmark & \cmark & \xmark & 56.14 & 82.37 & 89.82 & 74.20 & 72.96 & 81.56 & 70.83 & 48.40 & 87.04 & 80.10 & 53.11 & 81.70 & 73.19 \\
\hline
\cmark & \cmark & \cmark  & 56.54 & 83.75 & 90.48 & 76.03 & 73.99 & 80.95 & 72.97 & 56.60 & 87.32 & 82.55 & 59.76 & 82.52 & 75.29 \\
\hline
\end{tabular}}
\end{center}
\vspace{-6mm}
\caption{\small \textbf{Effectiveness of Different Modules on Office-Home Dataset.} Our proposed approach achieves the best performance with all the modules working jointly for learning discriminative invariant features in partial domain adaptation. 
}
\label{table:cls-office-home-ablation} \vspace{-4mm}
\end{table*}
 
\subsection{Ablation Studies} We perform the following experiments to test the effectiveness of the proposed modules including the effect of number of target classes on different datasets.

\vspace{1mm}
\noindent\textbf{Effectiveness of Individual Modules.} We conduct experiments to investigate the importance of our three unique modules on Office-Home dataset. 
As seen from Table~\ref{table:cls-office-home-ablation}, while the Select only module improves the vanilla performance by 8\%, addition of Label and Mix modules progressively improves the result to obtain the best performance of 75.29\% on Office-Home dataset. This
corroborates the fact that both discriminability and invariance of the latent space plays a crucial role in partial domain adaptation in addition to the removal of source domain outlier samples. 

\vspace{1mm}
\noindent\textbf{Comparison with Varying Number of Target Classes.} We compare different methods by varying the number of target classes. Figure~\ref{fig:var_tgt_cls} shows that our \ours framework consistently obtains the best results indicating its advantage in alleviating negative transfer by removing outlier source samples. Moreover, our approach outperforms all the compared methods even in the case of completely shared space (A31  W31), which shows that it does not discard relevant samples incorrectly when there are no outlier classes. 

\vspace{1mm}
\noindent\textbf{Effectiveness of Hausdorff Distance.} We investigate the effect of Hausdorff distance (Eqn.~\ref{eq:loss_select}) in training the selector network and find that by removing it from the total loss lowers down the performance from 75.29\% to 73.67\% on Office-Home dataset, showing its importance in guiding the selector network to discard the outlier source samples for effective reduction in negative transfer.

\begin{figure*}[htbp]
\captionsetup[subfigure]{labelformat=empty}
\centering
\subfloat[\textbf{Vanilla}]{
\label{fig:tsne_vanilla}
\fbox{\includegraphics[width=0.225\linewidth]{figures/tSNE_V_AW_5Kiter_perplex5.pdf}}}
\subfloat[\textbf{Select}]{
\label{fig:tsne_sel}
\fbox{\includegraphics[width=0.225\linewidth]{figures/tSNE_S_AW_5Kiter_perplex5.pdf}}}
\subfloat[\textbf{Select + Label}]{
\label{fig:tsne_sel_lab}
\fbox{\includegraphics[width=0.225\linewidth]{figures/tSNE_SL_AW_5Kiter_perplex5.pdf}}}
\subfloat[\textbf{Select + Label + Mix} (\ours)]{
\label{fig:tsne_sel_lab_mix}
\fbox{\includegraphics[width=0.225\linewidth]{figures/tSNE_SLM_AW_5Kiter_perplex5.pdf}}}
\vspace{-2mm}
\caption{\small \textbf{Feature Visualizations using t-SNE.} Plots show visualization of our approach with different modules on AW task from Office-31 dataset. \textcolor{blue}{Blue} and \textcolor{red}{red} dots represent source and target data respectively.  
As can be seen, features for both target as well as source domain become progressively discriminative and improve from left to right by adoption of our proposed modules. Best viewed in color.}
\label{fig:tsne} \vspace{-4mm}
\end{figure*} \begin{figure}[htbp]
\centering

\includegraphics[width=\columnwidth]{figures/UpdatedPlotVaryTargetClass.pdf} \vspace{-7mm}
\caption{\small Performance by varying the number of target classes on AW task from Office31 dataset. Our \ours framework consistently obtains the best results. Best viewed in color.}
\label{fig:var_tgt_cls} \vspace{-4mm}
\end{figure} 
\vspace{1mm}
\noindent\textbf{Distance between Domains.} Following~\cite{chen2020selective}, we compute the Wasserstein distance between the probability distribution of the target samples (T) with that of the selected () and discarded samples () by the selector network. Table~\ref{table:wasser} shows that  is smaller than , while  is greater than  on two randomly sampled adaptation tasks from Office31 and Office-Home datasets. This results indicate that the samples selected by our selector network is closer to the target domain while  the discarded samples are very dissimilar to the target domain. 

\vspace{1mm}
\noindent\textbf{Effectiveness of Soft Pseudo-Labels.} We also test the effectiveness of soft pseudo-labels by replacing them with hard pseudo-labels for the target samples and observe that hard pseudo-labels decreases the average performance from 75.29\% to 71.96\% on Office-Home dataset. This confirms that soft pseudo-labels are critical for good performance as it efficiently attenuates the unwanted deviations caused by the false and noisy hard pseudo-labels. 

\vspace{1mm}
\noindent\textbf{Effectiveness of Different Mixup.} We examine the effect of mixup regularization on both domain discriminator and classifier on Office-Home dataset. With mixup regularizations working for both discriminator and classifier, the average performance on Office-Home dataset is 75.29\%. By removing mixup regularization from the training of domain discriminator, the performance decreases to 73.58\%. Similarly, by removing mixup regularization from the classifier training, the average performance becomes 73.85\%. This corroborates the fact that our Mix strategy not only helps to explore intrinsic structures across domains, but also helps to stabilize the domain discriminator.  

\subsection{Feature Visualizations}

We use t-SNE~\cite{maaten2008visualizing} to visualize the features learned using different components of our \ours framework. We choose an UDA setup (similar to DANN~\cite{ganin2016domain}) as a vanilla method and add different modules one-by-one to visualize the contribution of each of the modules in learning discriminative features for partial domain adaptation. As can be seen from Figure~\ref{fig:tsne}, the feature space for the vanilla setup lacks dicriminability for both source and target features. The discriminability improves for both source as well as the target features as we add ``Select" and ``Label" to the Vanilla setup. The best results are obtained when all the three modules ``Select", ``Label" and ``Mix" i.e.~\ours are added and trained jointly in an end-to-end manner. 
  
Additional results, discussions and more visualization are included in the Appendix.


\setlength{\tabcolsep}{1pt}
\begin{table}[tbp]

\definecolor{Gray}{gray}{0.90}
\definecolor{LightCyan}{rgb}{0.88,1,1}

\newcolumntype{a}{>{\columncolor{Gray}}c}


\scriptsize
\resizebox{\columnwidth}{!}{
\begin{tabular}{  l || c  c | c  c }
\hline
\textbf{Distance} & \textbf{A  D} & \textbf{W  A} & \textbf{Cl  Pr} & \textbf{Rw  Pr} \\
\hline
 & 0.999 & 0.893 & 0.819 & 0.947\\
 & 1.013 & 1.144 & 1.418 & 1.008\\
\hline

\end{tabular}}

\vspace{-2mm}
\caption{\small \textbf{Wasserstein Distance between Domains.} 
Table shows values for two randomly sampled tasks from \textbf{Office-31} and \textbf{Office-Home}. The values are normalized by assuming the distance for  to be equal to 1.0, where  represents all source samples for the corresponding tasks. Numbers show that samples selected by our selector network is closer to the target domain while discarded samples are very dissimilar to the target domain.
}
\label{table:wasser} \vspace{-3mm}
\end{table}
  

\section{Conclusion}
\label{sec:conclusions}

In this paper, we propose an end-to-end framework for learning discriminative invariant feature representation while preventing negative transfer in partial domain adaptation. While our select module facilitates the identification of relevant source samples for adaptation, the label module enhances the discriminability of the latent space by utilizing pseudo-labels for the target domain samples. The mix module uses mixup regularizations jointly with the other two strategies to enforce domain invariance in latent space.     
We demonstrate the effectiveness of our approach on four standard datasets, outperforming several competing methods. 
\vspace{2mm}
\noindent\textbf{Acknowledgements.} This work was partially supported by the SERB Grant SRG/2019/001205.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\clearpage
\appendix
\label{appendix: Appendix}




\section{Dataset Details}
We evaluate the performance of our approach on several benchmark datasets for partial domain adaptation, namely Office31~\cite{saenko2010adapting}, Office-Home~\cite{venkateswara2017deep}, ImageNet-Caltech and VisDA-2017~\cite{peng2017visda}. The following are the detailed descriptions of the above datasets: \\

\vspace{-3mm}
\noindent \textbf{Office31.} This dataset contains 4,110 images distributed among 31 different classes and collected from three different domains: Amazon (A), Webcam (W) and DSLR (D), resulting in 6 transfer tasks. The dataset is imbalanced across domains with 2,817 images belonging to Amazon, 795 images to Webcam, and 498 images to DSLR, making Amazon a larger domain as compared to Webcam and DSLR. For all our experiments, we select the 10 classes shared by Office31 and Caltech256~\cite{griffin2007caltech} as the target categories and obtain the following label spaces: \\
.\\
.\\
Number of Outlier Classes = .\\
Figure~\ref{fig:office-31-gallery} shows few randomly sampled images from this dataset. The dataset is publicly available to download at: \url{https://people.eecs.berkeley.edu/~jhoffman/domainadapt/#datasets_code}. 

\begin{figure}[!htbp]
	\begin{center}
	\includegraphics[width=\columnwidth]{figures/Office31GalleryPDF.pdf}
	\end{center}
	\vskip -0.15in
	\caption{\small \textbf{Sampled Images from Office31 Dataset}. Each row from top to bottom corresponds to the domains Amazon, Dslr and Webcam, respectively. The images in the same column belong to the same class. Best viewed in color.}
	\label{fig:office-31-gallery}
 	\vskip -0.1in
\end{figure} 
\vspace{2mm}
\noindent \textbf{Office-Home.} This dataset contains 15,588 images distributed among 65 different classes and collected from four different domains: Art (Ar), Clipart (Cl), Product (Pr), and RealWorld (Rw), resulting in 12 transfer tasks. The dataset is split across domains with 2427 images belonging to Art, 4365 images to Clipart, 4439 images to Product, and 4347 images to RealWorld. We select the first 25 categories (in alphabetic order) in each domain as the target classes and obtain the following label spaces:\\
.\\
.\\
Number of Outlier Classes = .\\
Figure~\ref{fig:office-home-gallery} displays a gallery of sample images for this dataset. The dataset is publicly available to download at: \url{http://hemanthdv.org/OfficeHome-Dataset/}. \\

\begin{figure}[!htbp]
	\begin{center}
	\includegraphics[width=\columnwidth]{figures/OfficeHomeGalleryPDF.pdf}
	\end{center}
	\vskip -0.15in
	\caption{\small \textbf{Sampled Images from Office-Home Dataset}.
	Each row from top to bottom corresponds to the domains Art, Clipart, Product and RealWorld, respectively. The images in the same column belong to the same class. Best viewed in color.}
	\label{fig:office-home-gallery}
 	\vskip -0.1in
\end{figure} 
\vspace{2mm}
\noindent
\textbf{ImageNet-Caltech.} This large-scale dataset consists of two datasets (ImageNet1K~\cite{russakovsky2015imagenet} (I) \& Caltech256~\cite{griffin2007caltech} (C)) as two separate domains and consist of over 14 million images combined. 2 transfer tasks are formed for this dataset. While source domain contains 1,000 and 256 classes for ImageNet and Caltech respectively, each target domain contains only 84 classes that are common across both domains. As it is a general practice to use ImageNet pretrained weights for network initialization, we use the validation set images when using ImageNet as the target domain.
\noindent
Number of Outlier Classes =  for CI,  for IC.
Figure~\ref{fig:imagenet-caltech-gallery} displays a gallery of sample images for this dataset. The datasets are publicly available to download at: \url{http://www.image-net.org/} \\ \url{http://www.vision.caltech.edu/Image_Datasets/Caltech256/}.

\begin{figure}[!tbp]
	\begin{center}
	\includegraphics[width=\columnwidth]{figures/ImageNetCaltechGalleryPDF.pdf}
	\end{center}
	\vskip -0.15in
	\caption{\small \textbf{Sampled Images from ImageNet-Caltech Dataset}.
The top row corresponds to the ImageNet domain, while the bottom row to the Caltech domain. The images in the same column belong to the same class. Best viewed in color.}
	\label{fig:imagenet-caltech-gallery}
 	\vskip -0.1in
\end{figure} 
\vspace{2mm}
\noindent
\textbf{VisDA-2017.} This dataset contains 280,157 images distributed among 12 different classes and two domains. The dataset contains three sets of images: training, validation and testing. The training set contains 152,397 synthetic (S) images, the validation set contains 55,388 real-world (R) images, while the test set contains 72,372 real-world images. For the experiments, the training set is considered as the Synthetic (S) domain, while the validation set as the Real (R) domain, following~\cite{li2020deep}. This results in 2 transfer tasks. The first 6 categories (in alphabetical order) are selected in each of the domains as the target classes, and the following label spaces are obtained: \\
.\\
.\\
Number of Outlier Classes = .\\
Figure~\ref{fig:visda-gallery} displays a gallery of sample images for this dataset. The dataset is publicly available to download at:\\ \url{http://ai.bu.edu/visda-2017/#download}.

\begin{figure}[!tbp]
	\begin{center}
	\includegraphics[width=\columnwidth]{figures/VisDAGalleryPDA.pdf}
	\end{center}
	\vskip -0.15in
	\caption{\small \textbf{Sampled Images from VisDA-2017 Dataset}.
The top row corresponds to the Synthetic domain, while the bottom row to the Real domain. The images in the same column belong to the same class. Best viewed in color.}
	\label{fig:visda-gallery}
 	\vskip -0.15in
\end{figure}  
\section{Implementation Details}
Following are the detailed description of the implementation we follow for various components of the framework: 

\vspace{1mm}
\noindent
\textbf{Feature Extractor ().} We use ResNet-50~\cite{he2016deep} backbone for the feature extractor. The overall structure of ResNet-50 is \texttt{Initial Layers, Layer-1, Layer-2, Layer-3, Layer-4, AvgPool, Fc}. The model is initialized with ImageNet~\cite{russakovsky2015imagenet} pretrained weights. Additionally, we add a bottleneck layer of width 256 just after the \texttt{AvgPool} layer to obtain the features and replace all the BatchNorm layers with Domain-Specific Batch-Normalization~\cite{chang2019domain} layers. All the layers till \texttt{Layer-3} are frozen and only the rest of the layers are fine-tuned.

\vspace{1mm}
\noindent\textbf{Selector Network ().} We use a ResNet-18~\cite{he2016deep} network with the \texttt{Fc} layer replaced with a binary-length fully connected layer as the selector network in our framework. The network is initialized with ImageNet pretrained weights and all the layers are trained while optimization. 

\vspace{1mm}
\noindent\textbf{Classifier ().} The final \texttt{Fc} layer of ResNet-50 described above is replaced with a task-specific fully-connected layer to form the classifier network of our framework.

\vspace{1mm}
\noindent\textbf{Domain Discriminator ().} A three-layer fully-connected network is used as the domain discriminator network. It takes the 256-length features obtained from the feature extractor as input. The adversarial training is incorporated using a gradient reversal layer (GRL).

\vspace{1mm}
\noindent\textbf{Hyperparameters.}
All the networks are optimised with using mini-batch stochastic gradient descent with a momentum of 0.9. A batch size of 64 is used for Office31, Office-Home and VisDA-2017, while a batch size of 128 is used for ImageNet-Caltech. For feature extractor an initial learning rate of 5e-5 for the convolutional layers while an initial learning rate of 5e-4 for all the fully-connected layers is used. For the selector network and the domain discriminator an initial learning rate of 5e-3 and 5e-4 are used respectively. The learning rates are decayed following a cosine-annealing strategy as the training progresses. The best models are captured by obtaining the performance on a validation set. We do not follow the ten-crop technique~\cite{cao2018partialpada, cao2019learning}, to improve the performance in the inference phase. 
\section{Additional Experimental Results}

\setlength{\tabcolsep}{2pt}
\begin{table*}[!htbp]

\definecolor{Gray}{gray}{0.90}
\definecolor{LightCyan}{rgb}{0.88,1,1}

\newcolumntype{a}{>{\columncolor{Gray}}c}


\scriptsize
\begin{center}
\resizebox{\linewidth}{!}{
\begin{tabular}{ l || c  c   c   c   c   c | a  }
\hline
\multicolumn{8}{c}{\textbf{Office31}} \\
\hline
 \textbf{Method} &  \textbf{A  W} & \textbf{D  W} & \textbf{W  D} & \textbf{A  D} & \textbf{D  A} & \textbf{W  A} & \textbf{Average} \\
\hline
\hline
VGG-16~\cite{Simonyan15} \tiny(ICLR'15)        & 60.34 & 97.97 & 99.36 & 76.43 & 72.96 & 79.12 & 81.03   \\

\hline

 PADA~\cite{cao2018partialpada} \tiny(ECCV'18)  & \underline{86.05} & \textbf{100.0} & \textbf{100.0} & 81.73 & 93.00 & \underline{95.26} & 92.54   \\
 SAN~\cite{cao2018partialsan} \tiny(CVPR'18)  & 83.39 & 99.32 & \textbf{100.0} & 90.70 & 87.16 & 91.85 & 92.07   \\
 IWAN~\cite{zhang2018importance} \tiny(CVPR'18)  & 82.90 & 79.75 & 88.53 & \underline{90.95} & 89.57 & 93.36 & 87.51   \\
ETN~\cite{cao2019learning} \tiny(CVPR'19)   & 85.66 & \textbf{100.0} & \textbf{100.0} & 89.43 & \underline{95.93} & 92.28 & \underline{93.88}   \\



\hline
 \textbf{\ours (Ours)} & \textbf{91.98} & \underline{99.77} & \underline{99.58} & \textbf{98.09} & \textbf{96.14} & \textbf{96.03} &  \textbf{96.93}  \\
\hline
\end{tabular}}
\end{center}
\vspace{-4mm}
\caption{\small \textbf{Performance on Office31 with VGG-16 backbone.} 
Numbers show the accuracy (\%) of different methods on partial domain adaptation setting. We highlight the \textbf{best} and \underline{second best} method on each transfer task. Our proposed framework, \ours achieves the best performance on 4 out of 6 transfer tasks including the best average performance among all compared methods. 
}
\label{table:cls-office-31-vgg16} \vspace{-1mm}
\end{table*}
 \setlength{\tabcolsep}{1pt}
\begin{table*}[!htbp]

\definecolor{Gray}{gray}{0.90}
\definecolor{LightCyan}{rgb}{0.88,1,1}

\newcolumntype{a}{>{\columncolor{Gray}}c}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\scriptsize
\begin{center}
\resizebox{\linewidth}{!}{
\begin{tabular}{  P{0.8cm} | P{0.8cm} | P{0.8cm} || c c c c c c | a | c c | a  }
\hline
\multicolumn{3}{c}{\textbf{Modules}} & \multicolumn{7}{c}{\textbf{Office31}} & \multicolumn{3}{c}{\textbf{VisDA-2017}} \\
\hline
\textbf{Select} & \textbf{Label} & \textbf{Mix} &  \textbf{A  W} & \textbf{D  W} & \textbf{W  D} & \textbf{A  D} & \textbf{D  A} & \textbf{W  A} & \textbf{Average} & \textbf{R  S} & \textbf{S  R} & \textbf{Average} \\
\hline
\hline
\xmark & \xmark & \xmark & 
88.04 & 98.31 & 95.75 & 88.75 & 84.48 & 80.20 & 89.25 &   57.68 & 56.40 & 57.04\\
\cmark & \xmark & \xmark &  
91.75 & 99.32 & 96.60 & 93.84 & 94.22 & 93.46 & 94.87 &   69.04 & 68.40 & 68.72\\
\cmark & \cmark & \xmark &  
92.43 & 99.89 & 99.15 & 94.90 & 95.51 & 93.84 & 95.95 &   77.24 & 84.84 & 81.04\\
\hline
\cmark & \cmark & \cmark &  
99.77 & 100.00 & 99.79 & 98.73 & 96.10 & 95.89 & 98.38 &   77.48 & 91.74 & 84.61\\
\hline
\end{tabular}}
\end{center}
\vspace{-3mm}
\caption{\small \textbf{Effectiveness of Different Modules on Office31 and VisDA-2017 Datasets.} Our proposed approach achieves the best performance with all the modules working jointly for learning discriminative invariant features in partial domain adaptation.}
\label{table:cls-office31-visda17-ablation} \vspace{1mm}
\end{table*}
 \setlength{\tabcolsep}{1pt}
\begin{table*}[!htbp]

\definecolor{Gray}{gray}{0.90}
\definecolor{LightCyan}{rgb}{0.88,1,1}

\newcolumntype{a}{>{\columncolor{Gray}}c}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\scriptsize
\begin{center}
\resizebox{\linewidth}{!}{
\begin{tabular}{  l || c c c c c c c c c c c c | a  }
\hline
 &  \textbf{Ar  Cl} & \textbf{Ar  Pr} & \textbf{Ar  Rw} & \textbf{Cl  Ar} & \textbf{Cl  Pr} & \textbf{Cl  Rw} & \textbf{Pr  Ar} & \textbf{Pr  Cl} & \textbf{Pr  Rw} & \textbf{Rw  Ar} & \textbf{Pr  Cl} & \textbf{Pr  Rw} & \textbf{Average} \\
\hline
\hline
W/o Hausdorff Loss & 56.22 &	83.14 &	90.26 &	72.60 &	71.45 &	80.78 &	71.44 &	51.64 &	84.80 &	82.49 &	57.51 &	81.66 &	73.67 \\
\hline
Ours (\ours)  & 56.54 & 83.75 & 90.48 & 76.03 & 73.99 & 80.95 & 72.97 & 56.60 & 87.32 & 82.55 & 59.76 & 82.52 & 75.29 \\
\hline
\end{tabular}}
\end{center}
\vspace{-3mm}
\caption{\small \textbf{Effectiveness of Hausdorff Triplet Loss on Office-Home Dataset.}  
The table shows the performance of the framework without (top-row) and with (bottom-row) the inclusion of the Hausdorff distance triplet loss. The results highlight the importance of the Hausdorff distance loss in our proposed framework.
}
\label{table:cls-office-home-hausdorff} \end{table*}
 

\noindent
\textbf{Effectiveness on Different Backbone Networks.}
To show that the proposed framework is backbone-agnostic, i.e. it provides the best performance irrespective of the architecture of the feature extractor, we conduct experiments using a VGG-16~\cite{Simonyan15} backbone for the feature extractor. We report the results on the transfer tasks from the Office31 dataset in Table~\ref{table:cls-office-31-vgg16} and compare it with the current state-of-the-art methods. Our method outperforms the previously best results by a margin of \textbf{3.05\%} on average and achieves new state-of-the-art results. This confirms that our proposed framework for partial domain adaptation is robust with respect to the change of backbone network. 


\vspace{1mm}
\noindent
\textbf{Effectiveness of Individual Modules.}
In Section 4.3 of the main paper, we discussed the importance of the proposed three unique modules on Office-Home dataset. Here, we extend the experiments to Office31 and VisDA-2017 and provide the performance on the transfer tasks in Table~\ref{table:cls-office31-visda17-ablation}. Similar to the results on Office-Home dataset, our approach with all the three modules (Select, Label and Mix) working jointly, works the best on both datasets.

\setlength{\tabcolsep}{1pt}
\begin{table*}[!htbp]

\definecolor{Gray}{gray}{0.90}
\definecolor{LightCyan}{rgb}{0.88,1,1}

\newcolumntype{a}{>{\columncolor{Gray}}c}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\scriptsize
\begin{center}
\resizebox{\linewidth}{!}{
\begin{tabular}{  l || c c c c c c c c c c c c | a  }
\hline
 &  \textbf{Ar  Cl} & \textbf{Ar  Pr} & \textbf{Ar  Rw} & \textbf{Cl  Ar} & \textbf{Cl  Pr} & \textbf{Cl  Rw} & \textbf{Pr  Ar} & \textbf{Pr  Cl} & \textbf{Pr  Rw} & \textbf{Rw  Ar} & \textbf{Pr  Cl} & \textbf{Pr  Rw} & \textbf{Average} \\
\hline
\hline
W/ Hard Pseudo-labels & 52.52 &	79.89 &	90.17 &	73.46 &	72.61 &	78.17 &	69.88 &	47.54 &	87.50 &	78.57 &	50.59 &	82.67 &	71.96 \\
\hline
Ours (\ours)  & 56.54 & 83.75 & 90.48 & 76.03 & 73.99 & 80.95 & 72.97 & 56.60 & 87.32 & 82.55 & 59.76 & 82.52 & 75.29 \\
\hline
\end{tabular}}
\end{center}
\vspace{-4mm}
\caption{\small \textbf{Effectiveness of Soft Pseudo-labels on Office-Home Dataset.} 
Table shows the performance of the framework when we replace the soft pseudo-labels with hard pseudo-labels (top-row) for the target samples. The results justify that the soft pseudo-labels are critical for our framework and attenuate unwanted deviations caused by hard pseudo-labels.
}
\label{table:cls-office-home-ablation-hardpl} \vspace{-1mm}
\end{table*}
 \setlength{\tabcolsep}{1pt}
\begin{table*}[!htbp]

\definecolor{Gray}{gray}{0.90}
\definecolor{LightCyan}{rgb}{0.88,1,1}

\newcolumntype{a}{>{\columncolor{Gray}}c}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\scriptsize
\begin{center}
\resizebox{\linewidth}{!}{
\begin{tabular}{  l || c c c c c c c c c c c c | a  }
\hline
 &  \textbf{Ar  Cl} & \textbf{Ar  Pr} & \textbf{Ar  Rw} & \textbf{Cl  Ar} & \textbf{Cl  Pr} & \textbf{Cl  Rw} & \textbf{Pr  Ar} & \textbf{Pr  Cl} & \textbf{Pr  Rw} & \textbf{Rw  Ar} & \textbf{Pr  Cl} & \textbf{Pr  Rw} & \textbf{Average} \\
\hline
\hline
No Domain Discriminator MixUp & 56.18 &	81.49 &	90.02 &	74.01 &	71.76 &	80.31 &	72.18 &	50.93 &	86.25 &	79.80 &	57.99 &	82.00 &	73.58 \\
No Classifier MixUp & 57.81 &	82.88 &	88.53 &	75.05 &	73.63 &	79.26 &	69.02 &	54.85 &	86.64 &	79.77 &	57.57 &	81.21 &	73.85 \\
\hline
Ours (\ours)  & 56.54 & 83.75 & 90.48 & 76.03 & 73.99 & 80.95 & 72.97 & 56.60 & 87.32 & 82.55 & 59.76 & 82.52 & 75.29 \\
\hline
\end{tabular}}
\end{center}
\vspace{-4mm}
\caption{\small \textbf{Effectiveness of Different MixUp on Office-Home Dataset.}  
The table shows the performance of the framework with the exclusion of mixup regularization from the domain discriminator (top-row) and the classsifier (middle-row). The final row shows the results of the proposed \ours framework, which provides the best performance confirming the importance of our Mix strategy. 
}
\label{table:cls-office-home-ablation-mix} \vspace{-2mm}
\end{table*}
 
\vspace{1mm}
\noindent
\textbf{Effectiveness of Hausdorff Distance.} 
In Section 4.3 of the main paper, we discussed the importance of Hausdorff Distance loss in guiding the selector network to discard the outlier source samples. Here we provide the individual performance of all the transfer tasks on Office-Home dataset in Table~\ref{table:cls-office-home-hausdorff}, which shows that our approach with Hausdorff distance loss works the best in all cases.

\vspace{1mm}
\noindent
\textbf{Effectiveness of Soft Pseudo-Labels.}
As discussed in the Section 4.3, we confirmed the importance of soft pseudo-labels for our framework as it attenuates the unwanted deviations because of noisy and false hard pseudo-labels. Here, we provide the performance on each of the transfer tasks from Office-Home in Table~\ref{table:cls-office-home-ablation-hardpl}.

\vspace{1mm}
\noindent
\textbf{Effectiveness of Different MixUp.} 
We examined the effect of mixup regularization on both domain discriminator and classifier separately in Section 4.3 of the main paper. We concluded that our Mix strategy not only helps to explore intrinsic structures across domains, but also helps
to stabilize the domain discriminator. Here, we provide the corresponding performance on each of the transfer tasks of Office-Home in Table~\ref{table:cls-office-home-ablation-mix}.
 
\section{Qualitative Results}
\vspace{-0.8mm}
\begin{figure*}[!t]
\captionsetup[subfigure]{labelformat=empty}
\centering
\subfloat[\textbf{Vanilla}]{
\label{fig:tsne_vanilla}
\fbox{\includegraphics[width=0.225\linewidth]{figures/tSNE_V_AD_5Kiter_perplex5.pdf}}}
\subfloat[\textbf{Select}]{
\label{fig:tsne_sel}
\fbox{\includegraphics[width=0.225\linewidth]{figures/tSNE_S_AD_5Kiter_perplex5.pdf}}}
\subfloat[\textbf{Select + Label}]{
\label{fig:tsne_sel_lab}
\fbox{\includegraphics[width=0.225\linewidth]{figures/tSNE_SL_AD_5Kiter_perplex5.pdf}}}
\subfloat[\textbf{Select + Label + Mix} (\ours)]{
\label{fig:tsne_sel_lab_mix}
\fbox{\includegraphics[width=0.225\linewidth]{figures/tSNE_SLM_AD_5Kiter_perplex5.pdf}}}

\subfloat[\textbf{Vanilla}]{
\label{fig:tsne_vanilla}
\fbox{\includegraphics[width=0.225\linewidth]{figures/tSNE_V_WA_5Kiter_perplex5.pdf}}}
\subfloat[\textbf{Select}]{
\label{fig:tsne_sel}
\fbox{\includegraphics[width=0.225\linewidth]{figures/tSNE_S_WA_5Kiter_perplex5.pdf}}}
\subfloat[\textbf{Select + Label}]{
\label{fig:tsne_sel_lab}
\fbox{\includegraphics[width=0.225\linewidth]{figures/tSNE_SL_WA_5Kiter_perplex5.pdf}}}
\subfloat[\textbf{Select + Label + Mix} (\ours)]{
\label{fig:tsne_sel_lab_mix}
\fbox{\includegraphics[width=0.225\linewidth]{figures/tSNE_SLM_WA_5Kiter_perplex5.pdf}}}

\subfloat[\textbf{Vanilla}]{
\label{fig:tsne_vanilla}
\fbox{\includegraphics[width=0.225\linewidth]{figures/tSNE_V_DA_5Kiter_perplex5.pdf}}}
\subfloat[\textbf{Select}]{
\label{fig:tsne_sel}
\fbox{\includegraphics[width=0.225\linewidth]{figures/tSNE_S_DA_5Kiter_perplex5.pdf}}}
\subfloat[\textbf{Select + Label}]{
\label{fig:tsne_sel_lab}
\fbox{\includegraphics[width=0.225\linewidth]{figures/tSNE_SL_DA_5Kiter_perplex5.pdf}}}
\subfloat[\textbf{Select + Label + Mix} (\ours)]{
\label{fig:tsne_sel_lab_mix}
\fbox{\includegraphics[width=0.225\linewidth]{figures/tSNE_SLM_DA_5Kiter_perplex5.pdf}}}
\vspace{-1mm}
\caption{\small \textbf{Feature Visualizations using t-SNE.} Plots show visualization of our approach with different modules on AW, WA, and DA tasks repectively (top to down) from Office31 dataset. \textcolor{blue}{Blue} and \textcolor{red}{red} dots represent source and target data respectively.  
As can be seen, features for both target as well as source domain become progressively discriminative and improve from left to right by adoption of our proposed modules. Best viewed in color.}
\label{fig:tsne_supp} \vspace{-3mm}
\end{figure*} 
\noindent
\textbf{Feature Visualizations.}
We provide some additional feature visualizations using t-SNE~\cite{maaten2008visualizing} in Figure~\ref{fig:tsne_supp}. Similar to Section 4.4 in the main paper, we choose an UDA setup as a vanilla method and add the proposed modules one-by-one to visualize the contribution of each of the modules in learning discriminative features for partial domain adaptation.


 
 
\end{document}
