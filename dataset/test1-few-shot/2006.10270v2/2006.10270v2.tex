\documentclass{article}









\usepackage[nonatbib,preprint]{neurips_2020}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{amsmath}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}

\usepackage{graphicx}
\usepackage{bm}
\usepackage{multirow}
\usepackage{multicol}


\usepackage{subcaption}

\title{Multi-branch Attentive Transformer\thanks{This work is conducted at Microsoft Research Asia.}}



\iffalse
\begin{icmlauthorlist}
\icmlauthor{Yang Fan}{ustc}
\icmlauthor{}{msra}
\icmlauthor{}{msra}
\icmlauthor{}{sun}
\icmlauthor{}{msra}
\icmlauthor{}{msra}
\end{icmlauthorlist}
\fi

\author{Yang Fan, Shufang Xie, Yingce Xia, Lijun Wu, Tao Qin, Xiang{-}Yang Li, Tie{-}Yan Liu\\
 University of Science and Technology of China\quad Microsoft Research Asia\\
  \texttt{fyabc@mail.ustc.edu.cn,\;xiangyangli@ustc.edu.cn}\\\quad\texttt{\{shufxi,yingce.xia,lijun.wu,taoqin,tyliu\}@microsoft.com} \\
}

\newcommand{\enc}{\texttt{enc}}
\newcommand{\dec}{\texttt{dec}}
\newcommand{\concat}{\texttt{concat}}
\newcommand{\add}{\texttt{add}}
\newcommand{\multiattn}[1]{\texttt{attn}_{#1}}
\newcommand{\maattn}{\texttt{mAttn}}
\newcommand{\attn}{\texttt{attn}}
\newcommand{\ffn}{\texttt{FFN}}
\newcommand{\hffn}{\texttt{hFFN}}

\begin{document}

\maketitle

\begin{abstract}
While the multi-branch architecture is one of the key ingredients to the success of computer vision tasks, it has not been well investigated in natural language processing, especially sequence learning tasks. In this work, we propose a simple yet effective variant of Transformer~\cite{vaswani2017attention} called multi-branch attentive Transformer (briefly, MAT), where the attention layer is the average of multiple branches and each branch is an independent multi-head attention layer. We leverage two training techniques to regularize the training: drop-branch, which randomly drops individual branches during training, and proximal initialization, which uses a pre-trained Transformer model to initialize multiple branches. Experiments on machine translation, code generation and natural language understanding demonstrate that such a simple variant of Transformer brings significant improvements. Our code is available at  \url{https://github.com/HA-Transformer}.
\end{abstract}

\section{Introduction}\label{sec:intro}
The multi-branch architecture of neural networks, where each block consists of more than one parallel components, is one of the key ingredients to the success of deep neural models and has been well studied in computer vision.
Typical structures include the inception architectures~\cite{szegedy2015going,szegedy2016rethinking,szegedy2017inception}, ResNet~\cite{he2016deep}, ResNeXt~\cite{xie2017aggregated}, DenseNet~\cite{huang2017densely}, and the network architectures discovered by neural architecture search algorithms~\cite{pham2018efficient,liu2019darts}. Models for sequence learning, a typical natural language processing problem, also benefit from multi-branch architectures. Bi-directional LSTM (BiLSTM) models can be regarded as a two-branch architecture, where a left-to-right LSTM and a right-to-left LSTM are incorporated into one model. BiLSTM has been applied in neural machine translation (briefly, NMT)~\cite{zhou2016deep,wu2016google} and pre-training~\cite{peters2018elmo}. Transformer~\cite{vaswani2017attention}, the state-of-the-art model for sequence learning, also leverages multi-branch architecture in its multi-head attention layers. 


Although multi-branch architecture plays an important role in Transformer, it has not been well studied and explored. Specifically, using hybrid structures with both averaging and concatenation operations, which has been widely used in image classification tasks~\cite{xie2017aggregated,szegedy2016rethinking}, are missing in current literature for sequence learning. This motivates us to explore along this direction. We propose a simple yet effective variant of Transformer, which treats a multi-head attention layer as a branch, duplicates such an attention branch for multiple times, and averages the outputs of those branches. Since both the concatenation operation (for the multiple attention heads) and the averaging operation (for the multiple attention branches) are used in our model, we call our model {\em multi-branch attentive Transformer} (briefly, MAT) and such an attention layer with both the concatenation and adding operations as the multi-branch attention layer.




Due to the increased structure complexity, it is challenging to directly train MAT. Thus, we leverage two techniques for MAT training. (1) {\em Drop branch}: During training, each branch should be independently dropped so as to avoid possible co-adaption between those branches. Note that similar to Dropout, during inference, all branches are used. (2) {\em Proximal initialization}: We initialize MAT with the corresponding parameters trained on a standard single-branch Transformer.

Our contributions are summarized as follows: 

\noindent(1) We propose a simple yet effective variant of Transformer, multi-branch attentive Transformer (MAT). We leverage two techniques, drop branch and proximal initialization, to train this new variant. 

\noindent(2) We conduct experiments on three sequence learning tasks: neural machine translation, code generation and natural language understanding. On these tasks, MAT significantly outperforms the standard Transformer baselines, demonstrating the effectiveness of our method. 

\noindent(3) We explore another variant which introduces multiple branches into feed-forward layers. We find that such a modification slightly hurts the performance. 



\section{Background}\label{sec:background}

\subsection{Introduction to Transformer}
A Transformer model consists of an encoder and a decoder. Both the encoder and the decoder are stacks of blocks. Each block is mainly made up of two types of layers: the multi-head attention layer and the feed-forward layer (briefly, FFN). We will mathematically describe them.

Let  denote the concatenation operation, where all inputs are combined into a larger matrix along the last dimension. Let  and  denote a multi-head attention layer with  heads () and a standard attention layer. A standard  attention layer~\cite{bahdanau2015neural,vaswani2017attention} takes three elements as inputs, including query , key  and value , whose sizes are ,  and  ( are integers).  is defined as follows: 

where \texttt{softmax} is the softmax operation. A multi-head attention layer aggregates multiple attention layers in the concatenation way:

where  denotes the set . In Eqn.\eqref{eq:multi_head_attention}, the 's are the parameters to be learned. Each  is of dimension . The output of  is the same size as , i.e., .

In standard Transformer, an FFN layer, denoted by , is implemented as follows:

where  is a -dimension input,  is an element-wise operator,  and  are  and  matrices,  and  are  and  dimension vectors. Usually, .



In the encoder of a Transformer, each block consists of a self-attention layer, implemented as a  where the query , key  and value  are the outputs of previous layer, and an FFN layer. In the decoder side, an additional multi-head attention is inserted between the self-attention layer and FFN layer, which is known as the encoder-decoder attention:  is the output of the previous block,  and  are the outputs of the last block in the encoder. 

\subsection{Multi-branch architectures for sequence learning}
As shown in Section~\ref{sec:intro}, multi-branch architectures have been well investigated in image processing. In comparison, the corresponding work for sequence learning is limited. The bidirectional LSTM, a two-branch architecture, has been applied in machine translation~\cite{zhou2016deep,wu2016google} and pre-training techniques like ELMo~\cite{peters2018elmo}. \cite{song2018double} proposed two use both convolutional neural networks and Transformer in the encoder and decoder. Similar idea is further expanded in \cite{zhao2019muse}. A common practice of the previous work is that they focus on which network components (convolution, attention, etc.) should be used in a multi-branch architecture. In this work, we do not want to introduce additional operations into Transformer but focus on how to boost Transformer with its own components, especially, where and how to apply the multi-branch topology, and figure out several useful techniques to train multi-branch architectures for sequence learning. Such aspects are missing in previous literature.

\section{Multi-branch attentive Transformer}\label{sec:alg}
In this section, we first introduce the structure of multi-branch attentive Transformer (MAT) in Section~\ref{sec:network_arch}, and then we introduce the drop branch technique in Section~\ref{sec:drop_branch}. The proximal initialization is described in Section~\ref{sec:prox_init}. 

\subsection{Network architecture}\label{sec:network_arch}
The network architecture of our proposed MAT adopts the backbone of standard Transformer, except that all multi-head attention layers (including both self-attention layers and encoder-decoder attention layers) are replaced with the {\em multi-branch attention layers}.

Let  denote a multi-branch attention layer, where  represents the number of branches and each branch is a multi-head attention layer with  heads . ,  and  denote query, key and value, which are defined in Section~\ref{sec:background}. Mathematically,  works as follows:

where  (see Eqn.\eqref{eq:multi_head_attention}) is the collection of all parameters of the -th multi-head attention layer. 

\subsection{Drop branch technique}\label{sec:drop_branch}
As mentioned above, multiple branches in multi-branch attention layers have the same structure. To avoid the co-adaptation among them, we leverage the drop branch technique. The inspiration comes from the dropout~\cite{srivastava2014dropout} and drop path technique~\cite{larsson2017fractalnet}, where some branches are randomly dropped during training.

Let  denote a multi-branch attention layer with drop branch rate , which is an extension of that in Section~\ref{sec:network_arch}. Equipped with drop branch technique, the -th branch of the multi-branch attention layer, denoted as , works as follows:

where  is uniformly sampled from  (briefly, );  is the indicator function.
During training, we may set , where any branch might be skipped with probability . During inference, we must set , where all branches are leveraged with equal weights . The multi-branch attention layer  works as follows:

Note when , Eqn.\eqref{eq:multi-branch_attention_layer} degenerates to Eqn.\eqref{eq:multi-branch_attention_layer_infer}.

During training, it is possible that all branches in multi-branch attention layer are dropped. The residual connection ensures that even if all branches are dropped, the output from the previous layer can be fed to top layers through the identical mapping. That is, the network is never blocked. When  and , a multi-branch attention layer degenerates to a vanilla multi-head attention layer. When  and , it is the standard Transformer with randomly dropped attention layers during training, which is similar to the drop layer~\cite{fan2020reducing}. (See Section~\ref{sec:hyper_explore} for more discussions.)

We also apply the drop branch technique to FFN layers. That is, the revised FFN layer is defined as 

where  is the output of the previous layer. An illustration of multi-branch attentive Transformer is in Figure~\ref{fig:arch_multi_branch}. Each block in the encoder consists of a multi-branch attentive self-attention layer and an FFN layer. Each block in the decoder consists of another multi-branch attentive encoder-decoder attention layer. Layer normalization~\cite{ba2016layer} remains unchanged.

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.95\linewidth]{HAT_arch_encoder.png}
\vspace{-3mm}
\caption{Architecture of a block in the encoder of MAT. ``LN'' refers to layer normalization.}
\label{fig:arch_multi_branch}
\vspace{-3mm}
\end{figure}

\subsection{Proximal initialization}\label{sec:prox_init}
MAT can be optimized like the standard version by first randomly initializing all parameters and  training until convergence. However, the multi-branch attention layer increases the training complexity of MAT, since there are multiple branches to be optimized.

Recently, proximal algorithms~\cite{neal2013proximal} have been widely used in pretraining-and-finetuning framework to regularize training~\cite{jiang2019smart}. Then main idea of proximal algorithms is to balance the trade-off between minimizing the objective function and minimizing the weight distance with a pre-defined weight. Inspired by those algorithms, we design a two-stage warm-start training strategy:

\noindent(1) Train a standard Transformer with embedding dimension , FFN dimension .

\noindent(2) Duplicate both the self-attention layers and encoder-decoder attention layers for  times to initialize MAT. Train this new model until convergence.
 
We empirically find that the proximal initialization scheme can boost the performance than that obtained by training from scratch.

\section{Application to neural machine translation}
In this section, we conduct two groups of experiments: one with relatively small scale data, including IWSLT'14 GermanEnglish, IWSLT'14 SpanishEnglish and IWSLT'17 FrenchEnglish translation tasks; the other with larger training corpus, WMT'14 EnglishGerman translation and WMT'19 GermanFrench translation. We briefly denote English, German, Spanish and French as En, De, Es and Fr respectively.   

\subsection{Settings}
\noindent{\em Data preprocessing}:
Our implementation of NMT experiments is based on fairseq\footnote{\url{https://github.com/pytorch/fairseq}}.
For IWSLT'14 DeEn, we follow~\cite{edunov2018classical} to get and preprocess the training, validation and test data\footnote{The URLs of scripts we used in this paper are summarized in Appendix C.}, including lowercasing all words, tokenization and applying BPE~\cite{sennrich2016bpe}. For the other two IWSLT tasks, we do not lowercase the words and keep them case-sensitive. We apply tokenization and BPE as those used in preprocessing IWSLT DeEn. Training data sizes for IWSLT DeEn, EsEn and FrEn are ,  and  respectively. We choose IWSLT'13 EsEn, IWSLT'16 FrEn for validation purpose, and choose IWSLT'14 EsEn and IWSLT'17 FrEn as test sets. The numbers of BPE merge operation for the three tasks are all . The source and target data are merged to get the BPE table.

For WMT'14 EnglishGerman translation, we follow \cite{ott2018scaling} to preprocess the data , and eventually obtain  training data. We use newstest 2013 as the validation set, and choose newstest 2014 as the test set. The number of BPE merge operation is . The preprocess steps for WMT'19 DeFr are the same as those for WMT'14 EnDe, and we eventually get  training data in total. We concatenate newstest2008 to newstest 2014 together as the validation set and use newstest 2019 as the test set. 


\noindent{\em Model configuration and training strategy}: For the three IWSLT tasks, we choose the default setting provided by fairseq official code\footnote{\url{https://github.com/pytorch/fairseq/blob/master/fairseq/models/transformer.py}} as the baseline with embedding dimension , hidden dimension  and number of heads . For WMT'14 EnDe, we mainly follow the big transformer setting, where the above three numbers are ,  and  respectively. The dropout rates are . For the more detailed parameters like  and , we will introduce the details in corresponding subsections. We use the Adam~\cite{kingma2015adam} optimizer with initial learning rate , ,  and the \texttt{inverse\_sqrt} learning rate scheduler~\cite{vaswani2017attention} to control training. Each model is trained until convergence. The source embedding, target embedding and output embedding of each task are shared. The batch size is 4096 for both IWSLT and WMT tasks. For IWSLT tasks, we train on single P40 GPU; for WMT tasks, we train on eight P40 GPUs.

\noindent{\em Evaluation} We evaluate the translation quality by BLEU scores. For IWSLT'14 DeEn and WMT'14 EnDe, following the common practice, we use \texttt{multi-bleu.perl}. For other tasks, we choose \texttt{sacreBLEU}.  




\subsection{Exploring hyper-parameters of MAT}\label{sec:hyper_explore}
Due to resource limitation, we first explore hyper-parameters and proximal initialization on IWSLT'14 DeEn dataset to get some empirical results, then transfer them to larger datasets.

We try different combination of ,  and . We ensure the number of total parameters not exceeding , the size of the default model for IWSLT'14 DeEn. All results are reported in Table~\ref{tab:iwslt_deen_multi-branch_attn}, where the network architecture is described by a four-element tuple, with each position representing the number of branches , embedding dimension , hidden dimension  and model sizes.

\begin{table}[!htbp]
\vspace{-1mm}
\centering
\small
\caption{Results on IWSLT'14 DeEn with different architectures. The left and right subtables are experiments of standard transformer and MAT respectively. From left to right in each subtable, the columns represent the network architecture, number of parameters, BLEU scores with  ranging from  to .}
\begin{tabular}{p{2.44cm}p{.62cm}p{.56cm}p{.56cm}p{.56cm}|p{2.44cm}p{.56cm}p{.56cm}p{.56cm}p{.56cm}}
\toprule
 &  &  &  &  &  &  &  &  &  \\
\midrule
\multicolumn{5}{c|}{Standard Transformer + Drop Branch} & \multicolumn{5}{c}{MAT + Drop Branch} \\ 
\midrule
 &     &  &     &    &  &     &  &  &  \\
 &	 &	 &	 &	 &  &     &  &  &  \\
 &	 &	 &    &	 &  &     &  &	 &	 \\
 & 	 &	 &	 &	 &  &     &  &	 &	 \\
 & & & & &  & 	 &	 &  &	 \\
 & & & & &  & 	 &	 &	 &	 \\
\bottomrule
\end{tabular}
\label{tab:iwslt_deen_multi-branch_attn}
\vspace{-1mm}
\end{table}

The baselines correspond to the architectures with  and . The most widely adopted baseline (marked with ) is  and the model size is M. Reducing  to  results in slightly BLEU score . We have the following  observations:

\noindent(1) Using multi-branch attention layers with more than one branches can boost the translation BLEU scores, with a proper . When setting  and increasing  from  to , the BLEU scores are  (with ) and  (with ), outperforming the standard baseline , as well as the BLEU score obtained from architecture . However, it is not always good to enlarge . The BLEU score of MAT with  is , which is a minor improvement over the baseline.

Multi-branch attention layers also benefits from larger hidden dimension . When setting , we obtain consistent improvement compared to  with the same . Among all architectures, we find that the model  achieves the best BLEU score, which is  (we also evaluate the validation perplexity of all architectures, and the model  still get the lowest perplexity ). Compared with the corresponding Transformer  with , we get  improvement. We also enlarge  to  and , but the BLEU scores drop. Results are shown in Appendix A.

\noindent(2) The drop branch is important technique for training multi-branch attention layers. Take the network architecture  as an example: If we do not use drop branch technique, i.e., , we can only get  BLEU score, which is  point below the baseline. As we increase  from  to , the BLEU scores become better and better, demonstrating the effectiveness of drop branch. 

\noindent(3) When setting  and , the architecture still benefits from the drop branch technique. For architecture , the vanilla baseline is . As we increase  to , we can obtain  point improvement. Wider networks with  also benefits from this technique, which shows that drop branch is generally a useful trick for Transformer. This is consistent with the discoveries in~\cite{fan2020reducing}. With , enlarging the hidden dimension to  and  lead to  and  BLEU scores, corresponding to  and  score improvements compared with that without using drop branch. But we found that when , drop branch might lead to unstable results. For example, when ,  cannot lead to a reasonable result. We re-run the experiments with five random seeds but always fail. In comparison, MAT can always obtain reasonable results with drop branch.

We also try to turn off the drop branch at the FFN layers. In this case, we found that by ranging  from  to , architecture  can achieve ,  and  BLEU scores, which are worse than those obtained by using drop branch at all layers. This shows that the drop branch at every layers is important for our proposed model.

\subsection{Exploring proximal initialization}\label{sec:proximal_init}
In this section, we explore the effect of proximal initialization, i.e., warm start from an existing standard Transformer model. The results are reported in Table~\ref{tab:result_iwslt_deen}. Generally, compared with the results without proximal initialization, the models can achieve more than  BLEU score improvement. Specifically, with  and , we can achieve a  BLEU score, setting a state-of-the-art record on this task. We also evaluate the validation perplexity of all architectures in Table~\ref{tab:result_iwslt_deen}, and the model with  and  still get the lowest perplexity .

\begin{table}[!htbp]
\vspace{-2mm}
\centering
\caption{Results of using proximal initialization. Columns from left to right represent the network architecture, BLEU scores with drop branch ratio  from  to , and the increment compared to the best results without proximal initialization.}
\begin{tabular}{cccccccc}
\toprule
&     &    &    &     &    \\
\midrule
 &   &	 &  &   &  \\
 &   &		&  &	  &  \\
 &   &		&  &	  &  \\
 &   &		&  &	  &  \\
 &   &		&  &	  &  \\
\bottomrule
\end{tabular}
\label{tab:result_iwslt_deen}
\vspace{-1mm}
\end{table}

In summary, according to the exploration in Section~\ref{sec:hyper_explore} and Section~\ref{sec:proximal_init}, we empirically get the following conclusions: (1) A multi-branch attention layer is helpful to improve Transformer. We suggest to try  or  first. (2) A larger  is helpful to bring better performance. Enlarging  to consume the remaining parameters of reducing  (of the standard Transformer) is a better choice. (3) The drop branch and proximal initialization are two key techniques to the success of MAT in NMT.

\subsection{Application to other NMT tasks}\label{sec:results_apply}

\noindent{\it Results of other IWSLT tasks}: We apply the discoveries to SpanishEnglish and FrenchEnglish, and report the results in Table~\ref{tab:results_iwslt_esfren}.
For each setting, both the highest BLEU scores according to validation performance as well as the corresponding 's are reported. We choose  as the default baseline, whose model size is the upper bound of our MAT models. We also implement two other baselines, one with smaller  and the other with larger . In baselines, the  is fixed as zero.

Compared to the standard Transformer with one branch, various MAT with different model configurations outperform the baseline. The architecture  with  generally obtains promising results. Compared with the default baseline , it achieves , , ,  improvement on EsEn, EnEs, FrEn and EnFr. 

\begin{table}[!htbp]
\vspace{-2mm}
\centering
\caption{Results on IWSLT \{Es, Fr\}En. }
\begin{tabular}{c|ccc|ccccccc}
\toprule
// & \#Params(M)  & EsEn /  & EnEs /  & \#Params(M)  & EsEn /  & EnEs /  \\
\midrule
1/512/1024 &  &  /   &  /  &  &  /  &  /  \\
1/256/1024 &  &  /   &  /  &  &  /  &  /  \\
1/256/2048 &  &  /   &  /  &  &  /  &  /  \\
\hline
2/256/1024 &  &  /   &  /  &  &  /  &  /  \\
2/256/2048 &  &  /   &  /  &  &  /  &  /  \\
3/256/1024 &  &  /   &  /  &  &  /  &  /  \\
3/256/2048 &  &  /   &  /  &  &  /  &  /  \\
\bottomrule
\end{tabular}
\label{tab:results_iwslt_esfren}
\vspace{-4mm}
\end{table}

\noindent{\it Results on larger datasets}: After obtaining results on small-scale datasets, we apply our discoveries to three larger datasets: WMT'14 EnDe translation, WMT'19 DeFr translation and WMT'19 EnDe translation.

The results of WMT'14 EnDe are shown in Table~\ref{tab:wmt14_en2de}. The standard baseline is  BLEU score, and the model contains  parameters. We implement an MAT with  and another with . We can see that our MAT also works for the large-scale dataset. When , we can obtain  BLEU score. When increasing  to , we can get , which is comparable with the the  variant. Besides, for large-scale datasets, the drop branch technique is important too.  works best of all settings. 

\begin{table}[!htbp]
\vspace{-2mm}
\centering
\caption{Results on WMT'14 EnDe translation. }
\begin{tabular}{lccccc}
\toprule
 & \#Param (M) &  & BLEU \\
\midrule
 &  &  &   \\
 &  &  &  \\
 &  &  &  \\
\midrule
 &  &  & \\
 &  &  &   \\
 &  &  &   \\
 &  &  &   \\
 &  &  &   \\
 &  &  &   \\
\bottomrule
\end{tabular}
\label{tab:wmt14_en2de}
\vspace{-1mm}
\end{table}

We summarize previous results on WMT'14 EnDe in Table~\ref{tab:prev_wmt14_en2de}. MAT can achieve comparable or slightly better results than carefully designed architectures like weighted Transformer and DynamicConv, and than the evolved Transformer discovered by neural architecture search.

The results of WMT'19 DeFr are shown in Table~\ref{tab:wmt19_de2fr}. Compared with standard big transformer model of architecture , MAT with  can improve the baseline by  point.

\begin{table}[!htbp]
\vspace{-3mm}
\begin{minipage}{0.49\linewidth}
\vspace{-4mm}
\centering
\caption{BLEU scores of WMT'14 EnDe \\ in previous work.}
\begin{tabular}{lc}
\toprule
Algorithm & BLEU \\
\midrule
Weighted Transformer~\cite{ahmed2017weighted}&  \\
Evolved Transformer~\cite{so2019evolved} &  \\
DynamicConv~\cite{wu2019pay} &  \\
\midrule
Our MAT & \\
\bottomrule
\label{tab:prev_wmt14_en2de}
\end{tabular}
\end{minipage}
\hfill
\begin{minipage}{0.49\linewidth}
\centering
\caption{Results on WMT'19 DeFr translation.}
\begin{tabular}{lccccc}
\toprule
 & \#Param (M) &  & BLEU  \\
\midrule
 &  &  &   \\
 &  &  &   \\
\midrule
 &  &  &   \\
 &  &  &  \\
 &  &  &  \\
 &  &  &  \\
\bottomrule
\label{tab:wmt19_de2fr}
\end{tabular}
\end{minipage}
\vspace{-3mm}
\end{table}

\noindent{\it Results with larger models}: Finally, we explore whether our proposed MAT works for larger models. We conduct experiments on WMT'19 EnDe translation task. The data is downloaded from WMT'19 website\footnote{The data is available at \url{http://www.statmt.org/wmt19/translation-task.html}. We concatenate \texttt{Europarl v9}, \texttt{Common Crawl corpus}, \texttt{News Commentary v14} and \texttt{Document-split Rapid} corpus. }. We change the number of encoder layers to , set \texttt{attention-dropout} and \texttt{activation-dropout} as , and keep the other settings the same as WMT'14 EnDe. To validate the effectiveness of MAT, we evaluate the trained models on three test sets: WMT'14 EnDe, WMT'18 EnDe and WMT'19 EnDe. We use \texttt{sacreBLEU} to evaluate the translation quality. Furthermore, to facilitate comparison with previous work, we also use \texttt{multi-bleu.perl} to calculate the BLEU score for WMT'14 EnDe.


The results are shown in Table~\ref{tab:wmt19_en2de}. Compared with the standard big transformer model of architecture , in terms of \texttt{sacreBLEU}, MAT with  can improve the baseline by ,  and  points on WMT14, WMT18 and WMT19 test sets respectively. Specially, on WMT'14 EnDe, in terms of \texttt{multi-bleu}, we achieve  BLEU score, setting a new record on this work under the supervised setting.

\begin{table}[!htbp]
\vspace{-2mm}
\centering
\caption{Results of EnDe with larger models. For WMT'14, both \texttt{multi-bleu} (left) and \texttt{sacreBLEU} (right) are reported.}
\begin{tabular}{lcccccccc}
\toprule
 & \#Param (M) &  & WMT14  & WMT18 & WMT19  \\
\midrule
 &   &  &  /  &  &  \\
 &   &  &  /  &  &  \\
\midrule
 &   &  &  /  &  &  \\
 &   &  &  /  &  &  \\
 &   &  &  /  &  &  \\
 &   &  &  /  &  &  \\
\bottomrule
\end{tabular}
\label{tab:wmt19_en2de}
\vspace{-2mm}
\end{table}


\section{Application to code generation}
We verify our proposed method on code generation, which is to map natural language sentences into code.


\noindent{\em Datasets} Following~\cite{wei2019code}, we conduct experiments on a Java dataset\footnote{The urls of the datasets and tools we used for code generation are summarized in Appendix C.\label{foot:codegen_script}}~\cite{ijcai2018-314} and a Python dataset~\cite{wan2018improving}. In the Java dataset, the numbers of training, validation and test sequences are ,  and  respectively, and the corresponding numbers for Python are ,  and . All samples are tokenized. We use the downloaded Java dataset without further processing and use Python standard AST moduleto further process the python code. The source and target vocabulary sizes in natural language to Java code generation are  and , and those for natural language to Python code generation are  and . In this case, following~\cite{wei2019code}, we do not apply subword tokenization like BPE to the sequences.

\noindent{\em Model configuration} We set ,  and  respectively. Both the encoder  and the decoder consist of three blocks. The MAT model contains about  parameters. For the Transformer baseline, we set  and , with M parameters.

\noindent{\em Evaluation} Following~\cite{wei2019code,ijcai2018-314}, we use sentence-level BLEU scores, which is the average BLEU score of all sequences to evaluate the generation quality. We choose the percentage of valid code (PoV) as another metric for evaluation, which is the percentage of code that can be parsed into an AST.
\iffalse
\noindent{\em Baselines} We compare our results with the following baselines:
\begin{enumerate}
    \item {\textbf{Dual}~\cite{wei2019code}}: This approach proposes a dual training framework between code summarization and code generation to train the two tasks simultaneously. The approach uses two 3-layer LSTM models with the attention mechanism, more than  parameters in total. Compared with Transformer and our method, the dual framework requires extra data and more training cost.
    \item {\textbf{Transformer}~\cite{vaswani2017attention}}: We use the standard Transformer architecture, with , , number of encoder layers and decoder layers set to . The model size is about .
\end{enumerate}
\fi

We report the results on code generation task in Table~\ref{tab:results_codegen}. It is obvious that Transformer-based models (standard Transformer and MAT) is significantly better than the LSTM-based dual model. Compared with standard Transformer, our MAT can get  and  BLEU score improvement in Java and Python datasets respectively. MAT reaches the best BLEU scores when  on both on Java and Python datasets, which shows that the drop branch technique is important in code generation task. When , in terms of PoV, our MAT can boost the Transformer baseline by  and  points. 

\begin{table}[!htbp]
\centering
\caption{Results on code generation.}
\begin{tabular}{lcccc}
\toprule
\multirow{2}{*}{Algorithm} & \multicolumn{2}{c}{Java} & \multicolumn{2}{c}{Python} \\
& BLEU & PoV & BLEU & PoV \\
\hline
Dual~\cite{wei2019code} &  &  &  &  \\
Transformer &  &  &  &  \\
\hline
Ours,  &  &  &  &  \\
Ours,  &  &  &  &  \\
Ours,  &  &  &  &  \\
Ours,  &  &  &  &  \\
\bottomrule
\end{tabular}
\label{tab:results_codegen}
\end{table}

\section{Application to natural language understanding}
We verify our proposed MAT on natural language understanding (NLU) tasks. Pre-training techniques have achieved state-of-the-art results on benchmark datasets/tasks like GLUE~\cite{wang2019glue}, RACE~\cite{lai2017large}, etc. To use pre-training, a masked language model is first pre-trained on large amount of unlabeled data. Then the obtained model is used to initialize weights for downstream tasks. We choose RoBERTa~\cite{liu2019roberta} as the backbone. For GLUE tasks, we focus more on accuracy. Therefore, we do not control the parameters subconsciously. We set  for all experiments.

Following~\cite{fan2020reducing}, we conduct experiments on MNLI-m, MPRC, QNLI abd SST-2 tasks in GLUE benchmark~\cite{wang2019glue}. SST-2 is a single-sentence task, which is to check whether the input movie review is a positive one or negative. The remaining tasks are two-sentence tasks. In MNLI, given a premise sentence and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis (entailment), contradicts the hypothesis (contradiction), or neither (neutral). In MRPC, the task is to check whether the two inputs are matched. In QNLI, the task is to determine whether the context sentence contains the answer to the question. 
We choose the 24-layer RoBERTa\footnote{Models from page \url{https://github.com/pytorch/fairseq/tree/master/examples/roberta}.} to initialize our MAT. We report the validation accuracy following~\cite{fan2020reducing}. 

\begin{table}[!htpb]
\centering
\caption{Validation results on various NLU tasks. Results of RoBERTa and RoBERTa + LayerDrop (briefly, ``+LayerDrop'') are from \cite{fan2020reducing}.} 
\begin{tabular}{lcccc}
\toprule
Algorithm & MNLI-m & MRPC & QNLI & SST-2 \\
\midrule
RoBERTa &  &  &  &  \\
+ LayerDrop &  &  &  &  \\
\midrule
Ours &  &  &  &  \\
\bottomrule
\end{tabular}
\label{tab:result_glue}
\end{table}

The results are reported in Table~\ref{tab:result_glue}. Compared with vanilla RoBERTa model, on the four tasks MNLI-m, MRPC, QNLI abd SST-2, we can improve them by , ,  and  scores. Compared with the layer drop technique~\cite{fan2020reducing}, we also achieve promising improvement on these four tasks. The results demonstrate that our method not only works for sequence generation tasks, but also for text classification tasks. 

Drop branch also plays a central role on NLU tasks. We take the MRPC task as an example. The results are shown in Table~\ref{tab:mrpc_rho}. MRPC achieves the best result at .

\begin{table}[!htbp]
\centering
\caption{MRPC validation accuracy w.r.t. . }
\begin{tabular}{ccccccc}
\toprule
 &  &  &  &  &   \\
\midrule
accuracy &  &  &  &  &  \\
\bottomrule
\end{tabular}
\label{tab:mrpc_rho}
\end{table}

\section{Exploring other variants}
In this section, we first discuss whether applying multi-branch architectures to FFN layers is helpful. Then we discuss a new variant of drop branch technique. Finally we verify the importance of the multi-branch architecture, where both concatenation and averaging operations are used. We mainly conduct experiments on IWSLT tasks to verify the variants. We conduct experiments on IWSLT'14 DeEn.

\subsection{Multi-branch FFN layer}
\noindent{\em Algorithm}: Denote the revised FFN layer as  with  branches. Mathematically, given an input  from the previous layer,

where  is the parameter of the -th branch of the FFN layer,  is the output from the previous layer. 

\noindent{\em Experiments}: We fix  as  and change . We set the attention layers as both standard multi-head attention layers  and multi-branch attention layers with . We set  to ensure the number of parameters unchanged with different number of branches. Results are reported in Table~\ref{tab:iwslt_deen_diff_ffn_branch}.

\begin{table}[!htbp]
	\centering
	\caption{Results on IWSLT'14 DeEn with multi-branch FFN layers. From left to right, the columns represent the network architecture (with the number of parameters included), BLEU scores with  ranging from  to .}
\begin{tabular}{lccccc}
		\toprule
		\#Param &   &  &  &  \\
		\midrule
		 &  &  &  & \\
		 &  &  &  &  \\
		 &  &  &  &  \\
		 &  &  &  &  \\
		\midrule
		 &  &	 &  &	 \\
		 &  &	 &  &	 \\
		 &  &	 & &	 \\
		 &  &	 &	&	 \\
		\bottomrule
	\end{tabular}
	\label{tab:iwslt_deen_diff_ffn_branch}
\end{table}

We can see that increasing the branches of FFN layer while decreasing the corresponding hidden dimension (i.e., ) will hurt the performance. For standard Transformer with , as we increase  from  to , , , the BLEU scores drop from  to ,  and . For MAT with , as  grows from  to , the BLEU decreases from  to ,  and . That is, constraint by the number of total parameters, we do not need to separate FFN layers into multiple branches.



\subsection{Variants of drop branch}


\noindent{\em Algorithm}: In drop branch, the attention heads within a branch are either dropped together or kept together. However, we are not sure whether dropping a complete branch is the best choice. Therefore, we design a more general way to leverage dropout. Mathematically, the -th attention head of the -th branch, denoted as , works as follows:

where superscripts  and  represent the branch id and head id in a multi-head attention layer respectively, , . Based on Eqn.\eqref{eq:general_head}, we can define a more general multi-branch attentive architecture:

Keep in mind that Eqn.\eqref{eq:general_dropout} is associated with  for any . We apply several constraints to , which can lead to different ways to regularize the training:
\begin{enumerate}
\item For any , sample  and set  for any . This is the drop branch as we introduced in Section~\ref{sec:network_arch}.\item Each  is independently sampled.\end{enumerate}

\iffalse
\begin{figure}[!htbp]
	\centering
	\subfigure[Drop a complete branch.]{
		\includegraphics[width=0.46\linewidth]{figures/dropout_branch.png}
	}
\subfigure[Randomly drop heads.]{
		\includegraphics[width=0.46\linewidth]{figures/dropout_head_rand.png}
	}
	\caption{Variants of drop branch. In this figure, we show a -branch -head multi-branch attention. In each figure, the bottom row represents input and top row is the output. Red dashed lines represented dropped parts, i.e., vectors with all elements zero.}
	\label{fig:diff_dropout_scheme}
\end{figure}
\fi

\noindent{\em Experiments}: We conduct experiments on IWSLT'14 DeEn and the results are reported in Table~\ref{tab:iwslt_deen_diff_dr_var}. Proximal initialization is leveraged. From left to right, each column represents architecture, number of parameters, BLEU scores with  ranging from  to . The last column marked with  represents the improvement/decrease of the best results compared to those in Table~\ref{tab:result_iwslt_deen}.




\begin{table}[!htbp]
\vspace{-2mm}
\begin{minipage}{0.48\linewidth}
\centering
\small
\caption{Results on IWSLT'14 DeEn with randomly dropping heads technique. Embedding dimension  is fixed as .}
\begin{tabular}{p{1.0cm}p{0.57cm}p{0.57cm}p{0.57cm}p{0.57cm}p{0.57cm}}
\toprule
		 &   &  &  &  &  \\
\midrule
		 &  &  &  &	 &  \\
		 &  &  &  &	 & \\
		 &  &  &  &	 & \\
		 &  &  &  &	 & \\
		 &  &  &  &	 & \\
\bottomrule
\end{tabular}
\label{tab:iwslt_deen_diff_dr_var}
\end{minipage}
\hfill
\begin{minipage}{0.48\linewidth}
\vspace{-11mm}
\centering
\small
\caption{Results on IWSLT'14 DeEn \\ with different numbers of heads.}
\begin{tabular}{lcccc}
\toprule
 &  &  & \\
\midrule
 &  &  & \\
 &  &  & \\
 &  &  & \\
\bottomrule
\end{tabular}
\label{tab:iwslt_deen_m_heads_multi-branch}
\end{minipage}
\vspace{-2mm}
\end{table}


We can see that randomly dropping heads leads to slightly worse results than the drop branch technique. Take the network architecture with ,  as an example. The best BLEU score that drop branch technique achieves is , and that obtained with randomly dropping heads is . Similarly observations can be found from other settings in Table~\ref{tab:iwslt_deen_diff_dr_var}. Therefore, we suggest to use the drop branch technique, which requires minimal revision to the benchmark code.

\subsection{Ablation study on multi-branch attentions}
MAT introduces multi-branch attention, in addition to the multi-head attention with the concatenation manner.\footnote{The multi-head attention in standard Transformer is also one kind of multi-branch attention. For simplicity, we use multi-head attention to denote the original attention layer in standard Transformer, and multi-branch attention to denote the newly introduced multi-branch attention in the  multi-branch manner (both averaging and concatenation manner).} It is interesting to know whether the multi-head attention is still needed given the multi-branch attention. We conduct experiments with , ,  and vary the number of heads  in . Results are reported in Table~\ref{tab:iwslt_deen_m_heads_multi-branch}.






We can see that the multi-head attention is still helpful. When , i.e., removing multiple heads and using a single head, it performs worse than multiple heads (e.g., ). Therefore, the multi-branch of them is the best choice.





\section{Conclusion and future work}
In this work, we proposed a simple yet effective variant of Transformer called multi-branch attentive Transformer and leveraged two techniques, drop branch and proximal initialization, to train the model. Rich experiments on neural machine translation, code generation and natural language understanding tasks have demonstrated the effectiveness of our model. 

For future work, we will apply our model to more sequence learning tasks. We will also combine our discoveries with neural architecture search, i.e., searching for better neural models for sequence learning in the search space with enriched multi-branch structures.




 





\appendix

\section*{Appendix}

\section{Exploring the Drop Branch in MAT}

\iffalse
The results of setting  are reported in Table~\ref{tab:results_iwslt_deen}.
 
\begin{table}[!htbp]
\centering
\caption{Results on IWSLT'14 DeEn with different number of branches, model size and dropout branch rates .  is fixed as .}
\begin{tabular}{cccccc}
\toprule
\#branch &   & \#params (M) &  & BLEU \\
\midrule
1  &  &  &  &  \\
1  &  &  &  &  \\
\midrule
2 &  &  &  &  \\
2 &  &  &  &  \\
2 &  &  &  &  \\
2 &  &  &  &  \\
\midrule
3 &  &  &  &  \\
3 &  &  &  &  \\
3 &  &  &  &  \\
3 &  &  &  &  \\
3 &  &  &  &  \\
3 &  &  &  &  \\
\midrule
\iffalse
4 &  &  &  &  &  \\
4 &  &  &  &  &  \\
4 &  &  &  &  &  \\
4 &  &  &  &  &  \\
4 &  &  &  &  &  \\
4 &  &  &  &  &  \\
\midrule
\fi
2 &   &  &  &  \\
2 &   &  &  &  \\
2 &   &  &  &  \\
2 &   &  &  &  \\
2 &   &  &  &  \\
2 &   &  &  &  \\
\midrule
3 &  &  &  &  \\
3 &  &  &  &  \\
3 &  &  &  &  \\
3 &  &  &  &  \\
3 &  &  &  &  \\
3 &  &  &  &  \\
\bottomrule
\end{tabular}
\label{tab:results_iwslt_deen}
\end{table}

We start from . The results are summarized in Table~\ref{tab:results_iwslt_deen}, where from left to right, the columns represent the number of branches, embedding dimension , number of parameters of the model, drop branch rates , and BLEU scores. The hidden dimension  is fixed as  across different settings. The baseline model with  contains  parameters, and the corresponding BLEU score is . Simply increasing the / to  or  will increase the number of parameters from  and . Therefore, we calibrate  to . For reference, we also reproduce the baseline with , whose BLEU is , which is almost the same as that of . From Table~\ref{tab:results_iwslt_deen}, we have the following observations: 

\noindent(1) No matter for  or , leveraging more branches is always helpful to improve performance with a proper drop branch. If we constrain the number of parameters under the standard baseline , we can see that using two-branch architecture can lead to  BLEU score, which is about  point gain over the baseline. Using three branch is not as good as using two branches, which is  improvement over the baseline. If we do not put constraint on the number of parameters, we can see that when , the two-branch and three-branch architectures can make more improvements, reaching  and  respectively.

\noindent(2) The drop branch technique is important for training multi-branch architectures. Take the network with two branches and  as an example: If we do not use drop branch technique, i.e., , we can only get  BLEU score, which is  point below the baseline. As we increase  from  to , the BLEU scores become better and better, demonstrating the effectiveness of drop branch. When the network contains more parameters, generally we need to set larger : When the number of parameters increase from  to  and to , the 's of the corresponding best BLEU are ,  and . 

Then we conduct several study on difference choices of  and . We control the number of parameters below . Results are reported in Table~\ref{tab:iwslt_deen_diffbranch}:

\begin{table}[!htbp]
\centering
\small
\caption{Results on IWSLT'14 DeEn with different  and . From left to right, the columns represent the network architecture, number of parameters, BLEU scores with  ranging from  to .}
\begin{tabular}{lccccc}
\toprule
/// &  \#P(M) &   &  &  &  \\
\midrule
2/1/256/2048 &	 24.7 & 	34.51&	35.46&	35.59&	35.33\\
2/4/256/512  &	 24.7 & 	33.86&	34.9&	35&	35.07\\
2-8-256-256  &	 24.7 & 	34.05&	34.59&	34.7&	34.55\\
\midrule
2-1-256-1024 & \\
3-1-256-1024 &	  & 	34.01&	34.92&	35.39&	35.44\\
3-1-256-2048 &	  & 	33.98&	35.03&	35.4&	\\
4-1-256-1024 &	  & 	33.77&	34.84&	35.08&	35.14\\
4-1-256-2048 &	  & 	33.79&	34.81&	35.08&	35.46\\
\midrule
1-1-256-1024 &  &	 &	 &	 &	\\
1-1-256-2048 &  &	34.66 &	35.45 & 32.75 &	1.37 \\
1-1-256-3072 &  & 	34.41 &	35.37 &	34.64 &	0.75 \\
1-1-512-256	 &  & 	34.74 &	3.24 &	24.7 &	None \\
1-1-512-512	 &  &	35.11 &	4.58	& 27.12 & 9.62\\
1-1-512-1024 &  & 	34.95&	3.94&	22.43&	0.78\\
\bottomrule
\end{tabular}
\label{tab:iwslt_deen_diffbranch}
\end{table}



\noindent(2) Even if , the architecture still benefits from the drop branch technique. For architecture 1-1-256-2014, the vanilla baseline is . As we increase  to , we can obtain  point improvement. Wider networks with  also benefits from this technique. This shows that drop branch is generally a useful trick for Transformer.

\fi


We explore more values of drop branch rate , and the results are reported in Table~\ref{tab:iwslt_deen_multi-branch_attn-more}. 




\begin{table}[!htbp]
\centering
\caption{Results on IWSLT'14 DeEn with different architectures. From left to right, the columns represent the network architecture, number of parameters, BLEU scores with  ranging from  to . Results of  are in Table 1 of the main text.}
\begin{tabular}{lcccc}
\toprule
 &   &  &  &  \\
\midrule
\multicolumn{5}{c}{Standard Transformer + Drop Branch} \\ 
\midrule
 &     &    &     &     \\
 &	 &	 &    &    \\
 &	 &	 &     &     \\
 & 	 &	 &    &     \\
\midrule
\multicolumn{5}{c}{MAT + Drop Branch} \\
\midrule
 &     &  &     &    \\
 &     &  &     &    \\
 &    &	 &    &    \\
 &     &	 &   &    \\
 & 	 &	 &    &    \\
 & 	 &	 &    &    \\
\bottomrule
\end{tabular}
\label{tab:iwslt_deen_multi-branch_attn-more}
\end{table}

From Table~\ref{tab:iwslt_deen_multi-branch_attn-more}, we can see that increasing  to  and  will hurt the performance of MAT.

\begin{table}[!htbp]
\centering
\caption{Results of using proximal initialization. Columns from left to right represent the network architecture, BLEU scores with drop branch ratio  from  to , and the increment compared to the best results without proximal initialization.}
\begin{tabular}{cccccc}
\toprule
&     &    &    &    &    \\
\midrule
 &   &  &  &  &  \\
 &   &	 &  &  &  \\
 &   &	 &  &  &  \\
 &   &	 &  &  &  \\
 &   &	 &  &  &  \\
\bottomrule
\end{tabular}
\label{tab:result_iwslt_deen-more}
\end{table}

On IWSLT'14 DeEn, with proximal initialization, we also explore increasing  to  and . The results are in Table~\ref{tab:result_iwslt_deen-more}. Still, setting  larger than  will hurt the performance.

\section{Exploration on Other IWSLT Tasks}

\subsection{Results of MAT}

We apply the settings in Table~ of the main paper to all four other IWSLT tasks. The results without proximal initialization are reported from Table~\ref{tab:iwslt_esen_multi-branch_attn} to Table~\ref{tab:iwslt_enfr_multi-branch_attn}. The results with promixal initialization are shown from Table~\ref{tab:result_iwslt_esen} to Table~\ref{tab:result_iwslt_enfr}. 


\begin{table}[!htbp]
\centering
\small
\caption{Results on IWSLT EsEn with different architectures. From left to right, the columns represent the network architecture, number of parameters, BLEU scores with  ranging from  to .}
\begin{tabular}{lcccccc}
\toprule
 &   &  &  &   \\
\midrule
\multicolumn{5}{c}{Standard Transformer + Drop Branch} \\ 
\midrule    
 &     &  &     &    \\
 &	 &	 &	 &	  \\
 &	 &	 &    &	  \\
\midrule
\multicolumn{5}{c}{MAT + Drop Branch} \\
\midrule
 &     &  &  &  \\
 &     &  &  &   \\
 &     &  &  &   \\
 &     &  &  &  \\
 & 	 &  &  &  \\
 & 	 &  &  &  \\
\bottomrule
\end{tabular}
\label{tab:iwslt_esen_multi-branch_attn}
\end{table}


\begin{table}[!htbp]
\centering
\small
\caption{Results on IWSLT EnEs with different architectures. From left to right, the columns represent the network architecture, number of parameters, BLEU scores with  ranging from  to .}
\begin{tabular}{lcccccc}
\toprule
 &   &  &  &   \\
\midrule
\multicolumn{5}{c}{Standard Transformer + Drop Branch} \\ 
\midrule
 &     &  &     &    \\
 &	 &	 &	 &	  \\
 &	 &	 &    &	  \\   \midrule
\multicolumn{5}{c}{MAT + Drop Branch} \\
\midrule
 &     &  &  &  \\
 &     &  &  &   \\
 &     &  &  &   \\
 &     &  &  &  \\
 & 	 &  &  &  \\
 & 	 &  &  &  \\
\bottomrule
\end{tabular}
\label{tab:iwslt_enes_multi-branch_attn}
\end{table}


\begin{table}[!htbp]
\centering
\small
\caption{Results on IWSLT FrEn with different architectures. From left to right, the columns represent the network architecture, number of parameters, BLEU scores with  ranging from  to .}
\begin{tabular}{lcccccc}
\toprule
 &   &  &  &   \\
\midrule
\multicolumn{5}{c}{Standard Transformer + Drop Branch} \\ 
\midrule
 &     &  &     &    \\
 &	 &	 &	 &	  \\
 &	 &	 &    &	  \\
\midrule
\multicolumn{5}{c}{MAT + Drop Branch} \\
\midrule
 &     &  &  &  \\
 &     &  &  &   \\
 &     &  &  &   \\
 &     &  &  &  \\
 & 	 &  &  &  \\
 & 	 &  &  &  \\
\bottomrule
\end{tabular}
\label{tab:iwslt_fren_multi-branch_attn}
\end{table}


\begin{table}[!htbp]
\centering
\small
\caption{Results on IWSLT EnFr with different architectures. From left to right, the columns represent the network architecture, number of parameters, BLEU scores with  ranging from  to .}
\begin{tabular}{lcccccc}
\toprule
 &   &  &  &   \\
\midrule
\multicolumn{5}{c}{Standard Transformer + Drop Branch} \\ 
\midrule
 &     &  &     &    \\
 &	 &	 &	 &	  \\
 &	 &	 &    &	  \\
\midrule
\multicolumn{5}{c}{MAT + Drop Branch} \\
\midrule
 &     &  &  &  \\
 &     &  &  &   \\
 &     &  &  &   \\
 &     &  &  &  \\
 & 	 &  &  &  \\
 & 	 &  &  &  \\
\bottomrule
\end{tabular}
\label{tab:iwslt_enfr_multi-branch_attn}
\end{table}















\begin{table}[!htbp]
\centering
\caption{Results of using proximal initialization on IWSLT EsEn. Columns from left to right represent the network architecture, BLEU scores with drop branch ratio  from  to , and the increment compared to the best results without proximal initialization.}
\begin{tabular}{cccccccc}
\toprule
&     &    &    &     &    \\
\midrule
 &   &	 &  &   &  \\
 &   &		&  &	  &  \\
 &   &		&  &	  &  \\
 &   & 	&  &   &  \\
 &   &		&  &	  &  \\
\bottomrule
\end{tabular}
\label{tab:result_iwslt_esen}
\end{table}


\begin{table}[!htbp]
\centering
\caption{Results of using proximal initialization on IWSLT EnEs. Columns from left to right represent the network architecture, BLEU scores with drop branch ratio  from  to , and the increment compared to the best results without proximal initialization.}
\begin{tabular}{cccccccc}
\toprule
&     &    &    &     &    \\
\midrule
 &   &	 &  &   &  \\
 &   &		&  &	  &  \\
 &   &		&  &	  &  \\
 &   & 	&  &   &  \\
 &   &		&  &	  &  \\
\bottomrule
\end{tabular}
\label{tab:result_iwslt_enes}
\end{table}


\begin{table}[!htbp]
\centering
\caption{Results of using proximal initialization on IWSLT FrEn. Columns from left to right represent the network architecture, BLEU scores with drop branch ratio  from  to , and the increment compared to the best results without proximal initialization.}
\begin{tabular}{cccccccc}
\toprule
&     &    &    &     &    \\
\midrule
 &   &	 &  &   &  \\
 &   &		&  &	  &  \\
 &   &		&  &	  &  \\
 &   & 	&  &   &  \\
 &   &		&  &	  &  \\
\bottomrule
\end{tabular}
\label{tab:result_iwslt_fren}
\end{table}


\begin{table}[!htbp]
\centering
\caption{Results of using proximal initialization on IWSLT EnFr. Columns from left to right represent the network architecture, BLEU scores with drop branch ratio  from  to , and the increment compared to the best results without proximal initialization.}
\begin{tabular}{cccccccc}
\toprule
&     &    &    &     &    \\
\midrule
 &   &	 &  &   &  \\
 &   &		&  &	  &  \\
 &   &		&  &	  &  \\
 &   & 	&  &   &  \\
 &   &		&  &	  &  \\
\bottomrule
\end{tabular}
\label{tab:result_iwslt_enfr}
\end{table}

\iffalse
\subsection{More Results on Larger Datasets}

\begin{table*}[!htbp]
\centering
\caption{Results on WMT'14 EnDe translation. News13 and News14 denote validation and test BLEU scores respectively.}
\begin{tabular}{lccccc}
\toprule
 & \#Param (M) &  & News14 & News13 \\
\midrule
 &  &  &  &  \\
 &  &  &  &  \\
 &  &  &  &  \\
\midrule
 &  &  &  &  \\
 &  &  &  &  \\
 &  &  &  &  \\
 &  &  &  &  \\
 &  &  &  &  \\
 &  &  &  &  \\
\bottomrule
\end{tabular}
\label{tab:wmt14_en2de}
\end{table*}


\begin{table*}[!htbp]
\centering
\caption{Results on WMT'19 DeFr translation.}
\begin{tabular}{lccccc}
\toprule
 & \#Param (M) &  & BLEU & BLEU(best.pt)[TODO] \\
\midrule
 &  &  &  & 33.55 \\
 &  &  &  & 33.33 \\
 &  &  &  & 33.83 \\
 &  &  &  & 33.20 \\
\midrule
 &  &  &  & 33.31 \\
 &  &  &  & 33.67 \\
 &  &  &  & 33.60 \\
 &  &  &  & 33.23 \\
 &  &  &  & 33.83 \\
 &  &  &  & 33.81 \\
 &  &  &  & 33.59 \\
 &  &  &  & 33.80 \\
 &  &  &  & 33.52 \\
 &  &  &  & 33.79 \\
 &  &  &  & 33.50 \\
 &  &  &  & 33.12 \\
\bottomrule
\end{tabular}
\label{tab:wmt19_de2fr}
\end{table*}
\fi

\subsection{Variants of Drop Branch}
We explore the variant of drop branch described in Section 7.2 on the other four IWSLT tasks. We apply all settings in Table~11 of the main paper to the remaining language pairs. Results are reported from Table~\ref{tab:iwslt_esen_diff_dr_var} to Table~\ref{tab:iwslt_enfr_diff_dr_var}. Generally, the two types of drop branch achieve similar results. 


\begin{table}[!htbp]
\centering
\caption{Results on IWSLT EsEn with randomly dropping heads technique. Embedding dimension  is fixed as .}
\begin{tabular}{lccccc}
\toprule
		 &   &  &  &  &  \\
\midrule
		 &  &  &  &	 &  \\
		 &  &  &  &	 &  \\
		 &  &  &  &	 &  \\
		 &  &  &  &	 &  \\
		 &  &  &  &	 &  \\
\bottomrule
\end{tabular}
\label{tab:iwslt_esen_diff_dr_var}
\end{table}


\begin{table}[!htbp]
\centering
\caption{Results on IWSLT EnEs with randomly dropping heads technique. Embedding dimension  is fixed as .}
\begin{tabular}{lccccc}
\toprule
		 &   &  &  &  &  \\
\midrule
		 &  &  &  &	 &  \\
		 &  &  &  &	 &  \\
		 &  &  &  &	 &  \\
		 &  &  &  &	 &  \\
		 &  &  &  &	 &  \\
\bottomrule
\end{tabular}
\label{tab:iwslt_enes_diff_dr_var}
\end{table}


\begin{table}[!htbp]
\centering
\caption{Results on IWSLT FrEn with randomly dropping heads technique. Embedding dimension  is fixed as .}
\begin{tabular}{lccccc}
\toprule
		 &   &  &  &  &  \\
\midrule
		 &  &  &  &	 &  \\
		 &  &  &  &	 &  \\
		 &  &  &  &	 &  \\
		 &  &  &  &	 &  \\
		 &  &  &  &	 &  \\
\bottomrule
\end{tabular}
\label{tab:iwslt_fren_diff_dr_var}
\end{table}


\begin{table}[!htbp]
\centering
\caption{Results on IWSLT EnFr with randomly dropping heads technique. Embedding dimension  is fixed as .}
\begin{tabular}{lccccc}
\toprule
		 &   &  &  &  &  \\
\midrule
		 &  &  &  &	 &  \\
		 &  &  &  &	 &  \\
		 &  &  &  &	 &  \\
		 &  &  &  &	 &  \\
		 &  &  &  &	 &  \\
\bottomrule
\end{tabular}
\label{tab:iwslt_enfr_diff_dr_var}
\end{table}






\section{Scripts}
In this section, we summarize the scripts we used in our paper.

\noindent(1) \texttt{multi-bleu.perl}: \url{https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl}

\noindent(2) \texttt{sacreBLEU}: \url{https://github.com/mjpost/sacreBLEU}

\noindent(3) The script to preprocess the IWSLT data: \url{https://github.com/pytorch/fairseq/blob/master/examples/translation/prepare-iwslt14.sh}\label{foot:fb_script}

\noindent(4) Path to Java dataset: \url{https://github.com/xing-hu/TL-CodeSum}

\noindent(5) Path to Python dataset: \url{https://github.com/wanyao1992/code_summarization_public}

\noindent(6) Python standard AST module to process the Python code: \url{https://docs.python.org/3/library/ast.html}



\bibliography{mybib}
\bibliographystyle{plain}

\end{document}