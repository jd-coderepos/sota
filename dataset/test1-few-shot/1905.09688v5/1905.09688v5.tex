\documentclass{article}
\usepackage[]{arxiv}



\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      

\usepackage{multirow}
\usepackage{multicol}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{dsfont}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{stackengine}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode} 
\usepackage{makecell}
\usepackage{color}
\usepackage{todonotes}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{pbox}

\usepackage{tikz}
\usepackage{pgfplots, pgfplotstable}
\pgfplotsset{compat=1.9}

\usepackage{mathabx}

\newcommand{\True}{\mbox{1}}
\newcommand{\False}{\mbox{0}}

\newtheorem{mytheorem}{Theorem}
\newtheorem{mylemma}{Lemma}
\newtheorem{definition}{Definition}

\newcommand{\Input}{{\hspace*{\algorithmicindent} \textbf{Input }}}

\newcommand{\Output}{{\hspace*{\algorithmicindent} \textbf{Output }}}

\newcommand{\Notation}{{\hspace*{\algorithmicindent} \textbf{Notation }}}

\algnewcommand{\LineComment}[1]{\State  #1}
\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\newcommand{\Break}{\State \textbf{break} }

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=blue,      
    urlcolor=blue,
    citecolor=.
}

\title{The Convolutional Tsetlin Machine\thanks{Source code and demos for this paper can be found at \href{https://github.com/cair/pyTsetlinMachineParallel}{https://github.com/cair/pyTsetlinMachineParallel}.}}

\author{
 Ole-Christoffer Granmo\\
 CAIR, University of Agder\\
\And
  Sondre Glimsdal\\
  CAIR, University of Agder\\
\And
  Lei Jiao\\
  CAIR, University of Agder\\
\And
  Morten Goodwin\\
  CAIR, University of Agder\\
\And
  Christian W. Omlin\\
  CAIR, University of Agder\\
\And
  Geir Thore Berge\\
  SSHF and CAIR\\
}

\begin{document}

\maketitle

\begin{abstract}
Convolutional neural networks (CNNs) have obtained astounding successes for important pattern recognition tasks, but they suffer from high computational complexity and the lack of interpretability. The recent Tsetlin Machine (TM) attempts to address this lack by using easy-to-interpret \emph{conjunctive clauses} in propositional logic to solve complex pattern recognition problems. The TM provides competitive accuracy in several benchmarks, while keeping the important property of interpretability. It further facilitates hardware-near implementation since inputs, patterns, and outputs are expressed as bits, while recognition and learning rely on straightforward bit manipulation. In this paper, we exploit the TM paradigm by introducing the \emph{Convolutional Tsetlin Machine} (CTM), as an interpretable alternative to CNNs. Whereas the TM categorizes an image by employing each clause once to the whole image, the CTM uses each clause as a convolution filter. That is, a clause is evaluated multiple times, once per image patch taking part in the convolution. To make the clauses location-aware, each patch is further augmented with its coordinates within the image. The output of a convolution clause is obtained simply by ORing the outcome of evaluating the clause on each patch. In the learning phase of the TM, clauses that evaluate to  are contrasted against the input. For the CTM, we instead contrast against one of the patches, \emph{randomly} selected among the patches that made the clause evaluate to . Accordingly, the standard Type I and Type II feedback of the classic TM can be employed directly, without further modification. The CTM obtains a peak test accuracy of \% on MNIST, \% on Kuzushiji-MNIST, \% on Fashion-MNIST, and \% on the 2D Noisy XOR Problem, which is competitive with results reported for simple 4-layer CNNs, BinaryConnect, Logistic Circuits, and a recent FPGA-accelerated Binary CNN. 
\end{abstract}


\section{Introduction}
The Tsetlin Machine (TM) \cite{granmo2018tsetlin} is a novel  machine learning paradigm introduced in 2018. It is based on the Tsetlin Automaton (TA) \cite{Tsetlin1961}, one of the pioneering solutions to the well-known multi-armed bandit problem \cite{Robbins1952,Gittins1979} and the first Finite State Learning Automaton (FSLA) \cite{Narendra1989}. The TM has the following main properties that make it attractive as a building block for machine learning~\cite{granmo2018tsetlin}: (a) it solves complex pattern recognition problems with interpretable propositional formulae. This is crucial for high stakes decisions \cite{Rudin2019}. (b) It learns on-line, persistently increasing both training- and test accuracy, before converging to a Nash equilibrium. The Nash equilibrium balances false positive against false negative classifications, while combating overfitting with frequent pattern mining principles. (c) Resource allocation dynamics are leveraged to optimize usage of limited pattern representation resources. By allocating resources uniformly across sub-patterns, local optima are avoided. (d) Inputs, patterns, and outputs are expressed as bits, while recognition and learning rely on straightforward bit manipulation. This facilitates low-energy consuming hardware design \cite{wheeldon2020hardware}. (e) Finally, the TM has provided competitive accuracy in comparison with classical and neural network based techniques, while keeping the important property of interpretability \cite{berge2019,abeyrathna2019nonlinear}.

The TM currently is state-of-the-art in FSLA-based pattern recognition. However, when compared with CNNs, it struggles with attaining competitive accuracy, providing e.g. \% mean accuracy on MNIST (without augmenting the data) \cite{granmo2018tsetlin}. To address this deficiency, we here introduce the Convolutional Tsetlin Machine (CTM), a new kind of TM designed for image classification. 

\textbf{FSLA.} The simple Tsetlin Automaton approach has formed the core of more advanced FSLA designs that solve a wide range of problems. This includes resource allocation \cite{Granmo2010g},
decentralized control \cite{Tung1996},
knapsack problems \cite{Granmo2007d}, searching on the line \cite{Oommen1997}, meta-learning \cite{Oommen2008}, the satisfiability problem \cite{Granmo2007c}, graph colouring \cite{Bouhmala2010}, preference learning \cite{Yazidi2012b}, frequent itemset mining \cite{Haugland2014}, adaptive sampling \cite{Granmo2010},
spatio-temporal event detection \cite{Yazidi2013},
equi-partitioning \cite{Oommen1988}, streaming sampling for social activity networks \cite{Ghavipour2018}, routing bandwidth-guaranteed paths \cite{Oommen2007a},
faulty dichotomous search \cite{Yazidi2018}, and learning in deceptive environments \cite{Zhang2016a}, to list a few examples. The unique strength of all of these FSLA designs is that they provide state-of-the-art performance when problem properties are unknown and stochastic, and the problem must be solved as quickly as possible through trial and error. 

\textbf{Rule-based Machine Learning.}
While the present paper focuses on extending the field of FSLA, we acknowledge the extensive work on rule-based interpretable pattern recognition from other fields of machine learning. Learning propositional formulae to represent patterns in data has a long history. One prominent example is frequent itemset mining for association rule learning~\cite{Agrawal1993}, for instance applied to predicting sequential events \cite{rudin13,mccormick15}.  Other examples include the work of Feldman who investigated the hardness of learning formulae in disjunctive normal form (DNF) \cite{feldman9}. Furthermore, Probably Approximately Correct (PAC) learning has provided fundamental insight into machine learning, as well as a framework for learning formulae in DNF \cite{valiant12}. Approximate Bayesian approaches have recently been introduced to provide more robust learning of rules \cite{wang6,hauser13}.  However, in general, rule-based machine learning scales poorly and is prone to noise. Indeed, for data-rich problems, in particular those involving natural language and sensory inputs, state-of-the-art rule-based machine learning is inferior to deep learning.
The recent hybrid Logistic Circuits \cite{LiangAAAI19}, however, have had success in image classification. This approach uses local search to build Bayesian models that capture logical expressions, and learns to classify by employing stochastic gradient descent.
The CTM, on the other hand, attempts to bridge the gap between the interpretability of pure rule-based machine learning and the accuracy of deep learning, by allowing the TM to more effectively deal with images.

\textbf{CNNs.} A myriad of image recognition techniques have been reported. However, after AlexNet won the ImageNet recognition challenge by a significant margin in 2012, the entire field of computer vision has been dominated by CNNs. The AlexNet architecture was built upon earlier work by LeCun et al. \cite{lecun1998gradient}  who introduced CNNs in 1998. Countless CNN architectures, all following the same basic principles, have since been published, including the now state-of-the-art Squeeze-and-Excitation networks  \cite{hu2018squeeze}. In this paper, we introduce convolution to the TM paradigm of machine learning. By doing so, we simultaneously propose a new kind of convolution filter: an interpretable filter expressed as a  propositional formula. Our intent is to address a well-known disadvantage of CNNs, namely, that the CNN models in general are complex and non-transparent, making them hard to interpret. Consequently, the knowledge on why CNNs perform so well and what steps are need to improve the models is limited \cite{zeiler2014visualizing}.

\textbf{Binary CNNs.} Relying on a large number of multiply-accumulate operations, training CNNs is computationally intensive. To mitigate this, there is an increasing interest in binarizing the CNNs. With only two possible values for the synapse weights, e.g.,  and , many of the multiplication operations can be replaced with simple accumulations. This could potentially open up for more specialized hardware and more compressed and efficient models. One recent approach is BinaryConnect \cite{courbariaux2015binaryconnect}, which reached a near state-of-the-art accuracy of \% on MNIST, and \% in combination with an SVM. Binary CNNs have been further improved with the introduction of XNOR-Nets, which replace the standard CNN filters with binary equivalents \cite{rastegari2016xnor}. BNN+ \cite{Darabi2018} is the most recent binary CNN, extending the XNOR-Nets with two additional regularization functions and an adaptive scaling factor. The CTM can be seen as an extreme Binary CNN in the sense that it is entirely based on fast summation and logical operators. Additionally, learning in the CTM is bandit based (online learning from reinforcement), while Binary CNNs are based on backpropagation.

\textbf{Contributions and Paper Outline.} Our contributions can be summarized as follows. First, in Sect. \ref{sec:TM}, we provide a brief introduction to the TM and a succinct definition of both recognition and learning. Then, in Sect. \ref{sec:CTM}, we introduce the concept of convolution to TMs. Whereas the classic TM categorizes an image by employing each clause once on the whole image, the CTM uses each clause as a convolution filter. In all brevity, we propose a recognition and learning approach for clause-based convolution that produces interpretable filters. In Sect. \ref{sec:empirical_results}, we evaluate the CTM on MNIST, Kuzushiji-MNIST, Fashion-MNIST, and the 2D Noisy XOR Problem, and discuss the merits of the new scheme. Finally, we conclude and provide pointers to further work in Sect. \ref{sec:conclusion}.

\section{The Tsetlin Machine}
\label{sec:TM}

\begin{figure}[ht]
\centering
\includegraphics[width=0.9\textwidth]{Tsetlin_Machine_Example_Configuration_Full.pdf}
\caption{The basic TM structure.}\label{figure:architecture_basic}
\end{figure}

The TM solves complex pattern recognition problems using \emph{conjunctive clauses} in propositional logic, composed by a collective of TAs. Its roots in game theory, the bandit problem, resource allocation, and frequent pattern mining are explored in depth in the original paper on the TM, which also includes pseudo code \cite{granmo2018tsetlin}. We here provide a more succinct overview, together with a running example to clarify core concepts.

\subsection{Structure}
The structure of the TM is shown in Figure~\ref{figure:architecture_basic}. As seen, the TM takes a vector  of  propositional variables as input: , with  being the size of the vector and  its domain. The input vector in the figure, for instance, is of size  and consists of the variables  and  with domain . 

The TM formulates patterns using \emph{conjunctive clauses}. A conjunctive clause is built from  \emph{literals}, that is, the input variables and their negations. With  input variables, we have  literals, :

Above, the first  literals are the unnegated variables, while the  following are the negated ones. Thus, in our example, there are four literals: , and . 

The number of clauses is a user set parameter that decides the expression power of the TM. Assume we have  clauses in our TM structure. Each clause, denoted by  and indexed by , is simply a conjunction of a subset  of the literals:

or equivalently:

The subset  contains the indexes of the literals that have been \emph{Included} in the clause . For the special case of , i.e., an empty clause, we have:

That is, during learning, empty clauses output  and during classification they output .
Accordingly, overall, the output  of a clause is fully specified by the input  and the selection of literals . Continuing our example, in the figure we have two clauses (). The first consists of the literals with indexes : . The second consists of the literals with indexes : .

Each clause is further assigned a fixed polarity , which decides whether the clause output is negative or positive. In the figure, the first clause has positive polarity, whereas the second has negative. Positive clauses are used to recognize class , while negative clauses are used to recognize class . By default, half of the clauses are positive and half are negative. 

In the last step, the outputs of the clauses decide the class  assigned to the input . That is, a summation operator aggregates the output of the clauses to form a majority vote. 

\subsection{Recognition}

We now look at recognition in more detail.  First of all, the output  of each clause is organized as vectors  and , grouped by polarity. Given an input , these outputs can then be calculated as follows (equivalent to Eqn. \ref{eq:clause}):

Here, the subset  contains the indexes of the literals that take the value  for the current input : . To express this, the lower index  stands for the literals and the upper index  constrains these to those of value . Thus, Eqn. \ref{eq:clause_output} compactly states that a clause outputs  if and only if all of its literals are of value . In Figure \ref{figure:architecture_basic}, for instance, the input  produces the literal values , , , and . Since the first clause only consists of  and , it outputs . The second clause, on the other hand, consists of  and , hence it outputs .

The final classification decision is made as follows. A majority vote is first organized by summing the output   of the positive clauses, and subtracting the output  of the negative clauses: . This majority vote decides the prediction of the TM: ), with a tie resolved in favor of .\footnote{Multi-class pattern recognition problems are modelled by employing multiple instances of this structure, replacing the threshold operator with an \textit{argmax} operator \cite{granmo2018tsetlin}.} So, in our example, the input  gathers a single positive vote, and no negative votes. Consequently, the classification becomes .

\subsection{Tsetlin Automata Collective}

We now introduce the collective of TAs, which decides the composition of each clause. There are  TAs per clause , one for each literal . Each individual TA has a state , and the states of all of the TAs are organized in two  matrices. The first matrix, , contains the states of the TAs belonging to the positive polarity clauses, whereas the matrix  covers the negative polarity clauses. In Figure \ref{figure:architecture_basic}, there are eight TAs, one per literal for clause  (positive polarity) and one per literal for clause  (negative polarity). Each of these TAs has  states: .

The states decide the actions taken by the TAs. A TA decides to exclude its literal if in states , otherwise, it includes the literal. That is, for each clause , some literals are \emph{Included}: , while the remaining are \emph{Excluded}:  (the complement of the included ones). So, in the figure, the TA associated with literal  in the first clause is in state  and thus  is included in that clause. The second TA, on the other hand, is in state  and, accordingly, literal  is excluded. By checking the state of each TA in this manner, the composition of all of the clauses is decided.

\subsection{Learning}

Learning in the TM is based on coordinating the collective of TAs using a novel FSLA game. The game leverages resource-allocation \cite{Granmo2007d}
and frequent pattern mining \cite{Haugland2014}
principles, indicated by the feedback loop in Figure~\ref{figure:architecture_basic}. The feedback is handed out based on training examples , consisting of an input  and an output .

As explored below, the TM employs two kinds of feedback: Type~I and Type II. Type~I feedback jointly combats false negatives and overfitting by stimulating recognition of frequent patterns. Type II feedback, on the other hand, suppresses false positives by increasing the discrimination power of the patterns learnt.

Feedback is further regulated by the sum of clause outputs  (the majority vote) and a target value  set for  by the user. A larger  (with a corresponding increase in the number of clauses) makes the learning more robust. This is because more clauses are involved in learning each specific pattern, introducing an ensemble effect. However, note that the resulting gain in accuracy comes at the cost of increased computational cost (cf. Table \ref{table:clauses_vs_accuracy_and_execution_time}).

\begin{figure}[ht]
\centering
\includegraphics[width=0.7\textwidth]{clause_activation_probability.png}
\caption{Clause feedback probability for .}\label{figure:clause_activation_probability}
\end{figure}

In the following, we first define the Type I and Type II feedback operations for clauses with positive polarity. As opposed to the feedback tables in \cite{granmo2018tsetlin}, the representation here is in a more compact matrix form.
\begin{itemize}
\item[Type I:] For training output , TAs belonging to positive clauses are given Type I feedback to make  approach . A vector  picks out the positive clauses selected for feedback (the lower index of  refers to the output ):

Here, the clamp operation restricts  to lie between  and . Further, the lower index of  refers to the clause  and the output . As illustrated by plot  in Figure \ref{figure:clause_activation_probability}, clauses are randomly selected for feedback, with a larger chance of being selected for lower values of . In effect, clauses are updated more aggressively the farther away the voting sum is from , up to .

We now divide the Type I feedback into two parts, Type Ia and Type Ib. Type Ia reinforces \emph{Include} actions to make the patterns finer. Only TAs taking part in clauses that output  and whose associated literal takes the value  are select for Type Ia feedback. We collect the indexes of these TAs in the set .

Type Ib feedback is designed to reinforce \emph{Exclude} actions to combat over-fitting. Type Ib feedback is handed out to the TAs stochastically, using a user set parameter  (a larger  provides finer patterns). The stochastic part of the calculation is organized in a matrix  with entries:

Here, the lower index of  refers to the clause  and the TA , considering the positive clauses (upper index). The indexes of the TAs selected for Type Ib feedback are collected in the set .
That is, TAs taking part in clauses that output , or whose associated literal takes the value 0, are selected for Type Ib feedback, however, only if stochastically pinpointed by  and . 

After the TAs has been selected for feedback, their states are updated by two state update operators  and :

These operators add/subtract  from the states of the singled out TAs, however, not beyond the given state space. In our example, if the TA associated with literal  in the first clause is singled out for Type Ia feedback, it would move from state  to state , reinforcing the \emph{Include} action. If it instead receives Type Ib feedback, it would move to state , reinforcing \emph{Exclude}. 

\item[Type II:] For training output , TAs belonging to positive clauses are given Type II feedback to suppress clause output . This, together with the negative clauses, makes  approach . A matrix  picks out the positive clauses selected for feedback:

The lower index of  refers to the clause  and the output , respectively. Here too, clauses are randomly selected for feedback, but now with a larger chance of being selected for higher  (cf. plot  in Figure \ref{figure:clause_activation_probability}). Next, the TAs selected are the ones that will turn clauses that output  into clauses that output , that is, those that have excluded literals that take the value :
.
Again, the states of the selected TAs are updated using the dedicated operators:
. Thus, if the TA associated with literal  in the first clause of our example is selected for Type II feedback, it would change state from  to , reinforcing the \emph{Include} action.
\end{itemize}

All of the above operations are for positive clauses. For negative clauses, Type I feedback is simply replaced with Type II feedback and vice versa!

\begin{figure}[!h]
\centering
\begin{subfigure}[t]{.49\textwidth}
\includegraphics[width=\linewidth]{Example_Inference.pdf}
\caption{}
\label{figure:inference_example}
\end{subfigure}
\hspace{0mm}
\begin{subfigure}[t]{.49\textwidth}
\includegraphics[width=\linewidth]{Example_Learning.pdf}
\caption{}
\label{figure:learning_example}
\end{subfigure}
\caption{Example of inference (a) and learning (b) for the Noisy 2D XOR Problem.}
\end{figure}

\begin{figure}[!h]
\centering
\begin{subfigure}[t]{.49\textwidth}
\includegraphics[width=\linewidth]{Example_Inference_II.pdf}
\caption{}
\label{figure:goal_state}
\end{subfigure}
\hspace{0mm}
\begin{subfigure}[t]{.4\textwidth}
\includegraphics[width=\linewidth]{3by3.pdf}
\caption{}
\label{figure:convolution}
\end{subfigure}
\caption{(a) Goal state for the Noisy 2D XOR Problem. (b) Illustration of image, filter and patches.}\label{figure:example}
\end{figure}

\subsection{Integer Clause Weighting}

It turns out that introducing clause weighting improves TM performance, both accuracy- and computation-wise \cite{phoulady2019weighted}. By weighting the output of the clauses, one clause can replace multiple and the impact of a clause can be fine-tuned. In all brevity, classification is then performed as a weighted majority vote:



Whereas real-valued weighting was introduced in \cite{phoulady2019weighted}, we here address learning of integer weights, based on Stochastic Searching on the Line \cite{Oommen1997}:

\paragraph{Initialization.} First of all, the weights are initialized to :

In other words, the behaviour of the clauses are identical to those of a standard TM at the start of learning.

\paragraph{Weight Updating.} The TA state updating is unmodified with weights. The weights, in turn, are updated based on Type~I and Type~II feedback.
\begin{itemize}
\item[Type I:] For Type~I feedback, a clause weight is increased by , however, only if the clause outputted :

\item[Type II:] For Type~II feedback, we instead update the weights by subtracting , with  being the minimum weight value:

\end{itemize}
When a clause evaluates to , its weight remains unchanged:

As seen, for Type~I feedback, the weights are increased to strengthen the impact of the associated clauses, thus reinforcing true positive frequent patterns. For Type~II feedback, on the other hand, the weights are decreased instead. This is to diminish the impact of the associated clauses, thus combating false positives, increasing discrimination power.

\section{The Convolutional Tsetlin Machine}
\label{sec:CTM}

Consider a set of images , where  is the index of the images. Each image is of size , and consists of  binary layers (which together encode the pixel colors using thresholding \cite{Abeyrathna2019}, one-hot encoding, or any other appropriate binary encoding). A classic TM models such an image with an input vector  that contains  propositional variables. Further, each clause is composed from  literals. Inspired by the impact convolution has had on deep neural networks, we here introduce the Convolutional Tsetlin Machine (CTM).

\textbf{Interpretable Rule-based Filters.} The CTM uses filters with spatial dimensions , again with  binary layers. Further, the clauses of the CTM take the role of filters. Each clause is accordingly composed from  literals. Additionally, to make the clauses location-aware \cite{Liu2018}, we augment each clause with binary encoded coordinates. Location awareness may prove useful in applications where both patterns and their location are distinguishing features, e.g. recognition of facial features such as eyes, eyebrows, nose, mouth, etc. in facial expression recognition. In all brevity, when applying a filter of size  on an image , the filter will be evaluated on  image patches. Here,  and ,  with  being the step size of the convolution. Each image patch thus has a certain location within the image, and we augment the input vector with the coordinates of this location. We denote the resulting augmented input vector :  . As seen, the input vector is extended with one propositional variable per position along each dimension, with the position being encoded using thresholding \cite{Abeyrathna2019} or one-hot encoding. Figure \ref{figure:convolution} illustrates an example of the image, patches, and a filter for , , and . In this example, the 33 filter moves from left to right, from top to bottom,   pixels per step.

\textbf{Recognition.} The CTM uses the classic TM procedure for recognition (see Sect. \ref{sec:TM}). However, for the CTM each clause outputs  values per image (one value per patch), as opposed to a single output for the TM (Eq. \ref{eq:clause_output}). We denote the output of a positive clause  on patch  by .  To turn the multiple outputs 
of clause  into a single output denoted by , we simply OR the individual outputs:


\textbf{Learning.} Learning in the CTM leverages the TM learning procedure. As seen in Sect. \ref{sec:TM}, Type Ia, Type Ib, and Type II feedback are influencing each clause  based on the literals of the input vector . For the CTM, the input vector is an image patch, and there are  patches in an image. There is thus  literal inputs , , per clause. Therefore, to decide which patch to use when updating a clause, the CTM randomly  selects a single patch among the patches that made the clause evaluate to . The clause is then updated according to this patch. That is, the input  is drawn from the set: . Observe that if the set is empty, only Type Ib feedback is applicable, and then the input vector is not needed. For non-empty sets, the TAs to be updated are finally singled out using the randomly selected patch:

The reason for randomly selecting a patch is to have each clause extract a certain sub-pattern, and the randomness of the uniform distribution statistically spreads the clauses for different sub-patterns in the target image.  Finally, observe that the computational complexity of the CTM grows linearly with the number of clauses , and with the number of patches . However, the computations can be easily parallelized due to their decentralized nature.

\textbf{Step-by-step Walk-through of Inference on Noisy 2D XOR.} Rather than providing hand-crafted features which can be used for image classification, the CTM learns feature detectors. We will explain the workings of the CTM by an illustrative example of noisy 2D XOR recognition and learning (see Figure \ref{figure:example} and Sect. \ref{sec:empirical_results}). Consider the CTM depicted in Figure \ref{figure:inference_example}. It consists of four positive clauses which represent XOR patterns that must be present in a positive example image (positive features) and four negative clauses which represent patterns that will not trigger a positive image classification (negative features). The number of positive and negative clauses is a user-defined parameter. The bit patterns inside each clause are represented by the output of eight TAs, two for each bit in a  filter.

Consider the  image shown in Figure \ref{figure:learning_example}. The filter represented by the second positive clause matches the patch in the top-right corner of the image and it is the only clause with output ; similarly, none of the negative clauses respond since their patterns do not match the pattern found in the current patch (Figure \ref{figure:learning_example}). Thus, the TM's combined output is . Learning of feature detectors proceeds as follows: with the CTM’s voting target set to , the probability of feedback is , and thus learning takes place, which pushes the CTM’s output  towards . Note that Type I feedback reinforces true positive output and reduces false negative output whereas Type II feedback reduces false positive output.

A subsequent state of the CTM is shown in Figure \ref{figure:goal_state}. There are now two positive clauses which detect their pattern in the top-right corner patch. The combined output of all clauses is ; thus, no further learning is necessary for the detection of the XOR pattern in this patch. Also, the location of the occurrence of each pattern is included. The location information uses a bit representation as follows: Suppose an XOR pattern occurs at the three X-coordinates , , and . For the corresponding binary location representation, these coordinates are considered as thresholds: If a coordinate is greater than a threshold, then the corresponding bit in the binary representation will be ; otherwise, it is set to . Thus, the representation of the X-coordinates , , and  will be ``111", ``011" and ``001", respectively. These representations of the location of  patterns are also learned by TAs.

\begin{table*}[!h]
\centering
\begin{tabular}{ c|c|c|c|c|c } 
\hline
&Search Range& 2D Noisy XOR & MNIST & K-MNIST & Fashion-MNIST\\
\hline
\hline
\#Class Clauses&&&&&\\
T&&&&&\\
s&&&&&\\
W&&&&&\\
\big|Z\big|&&&&&\\
Clause Weighting&Yes/No&No&Yes&Yes&Yes\\
\hline
\end{tabular}
\caption{CTM configurations.}\label{table:configuration}
\end{table*}

\begin{table*}[!!h]
    \centering
    \begin{tabular}{ccccc}
    \textbf{0: U L} &  \textbf{0: U R} &  \textbf{9: U L} &  \textbf{9: U R} &  \textbf{4: L L}\\
    \begin{tabular}{c}
    00*000*000\\
    *000*0*000\\
    **00*00000\\
    00000000**\\
    00*000****\\
    000000***1\\
    000**0*111\\
    0*00**111*\\
    *****111*0\\
    00**11**00\\
    \hline
    0  X  6\\
    5  Y  8
    \end{tabular}
    &
    \begin{tabular}{c}
    0*********\\
    0******0**\\
    ********00\\
    *111111**0\\
    *111*11**0\\
    *1*1*11***\\
    111**11**0\\
    1****111**\\
    11***111**\\
    *****111**\\
    \hline
    8  X  17\\
    0  Y  6
    \end{tabular}
    &
    \begin{tabular}{c}
    0*0*****1*\\
    00000**1**\\
    000*******\\
    00***1**00\\
    000**1**00\\
    00**1*00*0\\
    00****00**\\
    0*******11\\
    0*********\\
    000*******\\
    \hline
    2  X  7\\
    5  Y  10
    \end{tabular}
    &
    \begin{tabular}{c}
    0**0*****0\\
    *00*0*****\\
    **0*******\\
    *******0**\\
    *11****0*0\\
    ****0**000\\
    ****1**00*\\
    ****1***0*\\
    ****11**00\\
    ********0*\\
    \hline
    12  X  18\\
    2  Y  8
    \end{tabular}
    &
    \begin{tabular}{c}
    ******1***\\
    *11*1*1***\\
    ***111****\\
    *****11***\\
    *000**1***\\
    000**1****\\
    0000*1*0**\\
    0*00*1**0*\\
    0**0***0**\\
    0000****0*\\
    \hline
    7  X  11\\
    12  Y  17
    \end{tabular}\\
    \textbf{0: L L} & \textbf{0: L R} &  \textbf{9: L L} &  \textbf{9: L R} &  \textbf{3: U R}\\
    \begin{tabular}{c}
    *000**1***\\
    **********\\
    **0******0\\
    *00****110\\
    *********0\\
    *0*0***1*0\\
    ******11**\\
    ***0******\\
    **000**1*1\\
    0*0****11*\\
    \hline
    0  X  5\\
    10  Y  16
    \end{tabular}
    &
    \begin{tabular}{c}
    *******1**\\
    ******1***\\
    ******1***\\
    *****1**1*\\
    ****1*1***\\
    ****11****\\
    *1*11*1***\\
    1**11*****\\
    *11*11****\\
    *1*1******\\
    \hline
    4  X  18\\
    13 Y  18
    \end{tabular}
    &
    \begin{tabular}{c}
    0000000***\\
    00000001**\\
    0000000110\\
    0000000*1*\\
    000000*0*1\\
    00000000**\\
    0000000000\\
    0000*00000\\
    0000000000\\
    0000000000\\
    \hline
    0  X  3\\
    8  Y  14
    \end{tabular}
    &
    \begin{tabular}{c}
    ******00**\\
    *****000*0\\
    **11*0000*\\
    **11*00000\\
    1*11000*00\\
    11**0000*0\\
    111****00*\\
    1**0000000\\
    1**00*00**\\
    1*0**00*00\\
    \hline
    12 X  16\\
    9  Y  15
    \end{tabular}
    &
    \begin{tabular}{c}
    **0000*000\\
    00000*0000\\
    ***0000000\\
    *11**00000\\
    *1*1**0000\\
    0**1*00*00\\
    0*1**0***0\\
    **1*0000*0\\
    *1***0*000\\
    ***0000000\\
    \hline
    12  X  18\\
    0 Y  3
    \end{tabular}
    \end{tabular}
    \caption{Example 1010 bit patterns produced by CTM for MNIST, including valid convolution positions. Here ``0: UL" means ``upper left" of the image for digit ``0". We can see clearly that ``0: UL", ``0: UR", ``0: LL" and ``0: LR" jointly construct the shape ``0''. The clauses for the other digits behave similarly, and we thus just illustrate selected patch positions for each digit.}\label{table:example_patterns}
    \label{tab:bit_pattern}
\end{table*}

\section{Empirical Results}
\label{sec:empirical_results}
In this section, we evaluate the CTM on four different datasets.



\textbf{2D Noisy XOR.} The 2D Noisy XOR dataset contains  binary images,  training examples and  test examples. The image bits have been set randomly, except for the  patch in the upper right corner, which reveals the class of the image. A diagonal line is associated with class , while a horizontal or vertical line is associated with class .
Thus the dataset models a 2D version of the XOR-relation. Furthermore, the dataset contains a large number of random non-informative features to measure susceptibility towards the curse of dimensionality. To examine robustness towards noise we have further randomly inverted  of the outputs in the training data.

\textbf{MNIST.} The MNIST dataset  has been used extensively to benchmark machine learning algorithms, consisting of  grey scale images of handwritten digits~\cite{lecun1998gradient}.

\textbf{Kuzushiji-MNIST.} This dataset contains  grayscale images of Kuzushiji characters, cursive Japanese. Kuzushiji-MNIST is more challenging than MNIST because there are multiple distinct ways to write some of the characters  \cite{Clanuwat2018}.

\textbf{Fashion-MNIST.} This dataset contains  grayscale images of articles from the Zalando catalogue, such as t-shirts, sandals, and pullovers  \cite{xiao2017}. This dataset is quite challenging, with a human accuracy of \%.

The latter three datasets contain  training examples and  test examples. We binarize these datasets using an adaptive Gaussian thresholding procedure with window size  and threshold value . Accordingly, the CTM operates on images with merely 1 bit per pixel. Table \ref{table:results} reports test accuracy for the CTM, while Table \ref{table:configuration} contains the corresponding configurations. The results are based on single-run averages, obtained from the last  epochs of . All of the experiments were run on a NVIDIA DGX-2 server. Note that these datasets are rather simple to solve for deep CNNs, and have been selected to facilitate our study of interpretability.

\begin{table*}[!!h]
\centering
\begin{tabular}{c|c|c|c|c|c|c} 
\hline
\#Clauses per class&250&500&1000&2000&4000&8000\\
\hline
\hline
MNIST accuracy (\%)&98.82&98.98&99.14&99.22&99.28&99.33\\
MNIST execution time (s)&15.6&15.8&18.9&20.4&32.4&39.0\\
\hline
K-MNIST accuracy (\%)&92.75&93.86&94.89&95.40&95.85&96.08\\
K-MNIST execution time (s)&15.5&17.2&19.7&21.1&33.5&40.5\\
\hline
F-MNIST accuracy (\%)&88.25&88.79&89.42&89.89&90.65&91.18\\
F-MNIST execution time (s)&16.9&17.2&17.8&25.0&27.8&52.2\\
\hline
\end{tabular}
\caption{CTM mean test accuracy and execution time per epoch for an increasing number of clauses.}\label{table:clauses_vs_accuracy_and_execution_time}
\end{table*}

\begin{table*}[!!h]
\centering
\begin{tabular}{ c|c|c|c|c } 
\hline
Model & 2D N-XOR & MNIST & K-MNIST & F-MNIST\\
\hline
\hline
4-Nearest Neighbour \cite{Clanuwat2018,xiao2017}&&&&\\
SVM \cite{Clanuwat2018}&&&&\\
Random Forest \cite{LiangAAAI19}&70.73&&-&\\
Gradient Boosting Classifier \cite{xiao2017}&87.15&&-&\\
Simple CNN \cite{Clanuwat2018,LiangAAAI19}&&&&\\
BinaryConnect \cite{courbariaux2015binaryconnect}&-&&-&-\\
FPGA-accelerated BNN \cite{Lammie2019}&-&&-&-\\
Logistic Circuit (binary) \cite{LiangAAAI19}&-&&-&\\
Logistic Circuit (real-valued) \cite{LiangAAAI19}&-&&-&\\
PreActResNet-18 \cite{Clanuwat2018}&-&&&\\
ResNet18 + VGG Ensemble \cite{Clanuwat2018}&-&&&-\\
TM&&&&\\
\hline
CTM (Mean)&&&&\\
CTM (95 \%ile)&&&&\\
CTM (Peak)&&&&\\
\hline
\end{tabular}
\caption{Empirical results - test accuracy in percent.}\label{table:results}
\end{table*}

\begin{table*}[!!h]
\centering
\begin{tabular}{ c|c|c } 
\hline
Model & MNIST & F-MNIST\\
\hline
\hline
Logistic Circuit (binary) \cite{LiangAAAI19}& kB& kB\\
Logistic Circuit (real-valued) \cite{LiangAAAI19}& kB& kB\\
CNN w/3 conv. layers \cite{LiangAAAI19}& kB& kB\\
ResNet \cite{LiangAAAI19}& kB& kB\\
TM w/ class clauses& kB& kB\\
\hline
CTM w/ class clauses& kB& kB\\
CTM w/ class clauses& kB& kB\\
\hline
\end{tabular}
\caption{Model size in kilobytes (kB), assuming 32-bit real-valued weights.}\label{table:model_size}
\end{table*}

\begin{figure}[ht]
\centering
\pgfplotstableread{mnist_stats_8000_10000_5.00.dat}{\mnist}
\pgfplotstableread{kmnist_stats_8000_10000_10.00.dat}{\kmnist}
\pgfplotstableread{fmnist_stats_8000_10000_10.00.dat}{\fmnist}
\begin{tikzpicture}
\tikzstyle{more densely dashed}=[dash pattern=on 5pt off 1pt]
	\begin{axis}[
	    ymin=75, ymax=100,
	    xmin=0, xmax=250,
		xlabel=Epoch,
		ytick distance=2,
		ylabel=Accuracy (\%),
        width=0.8\textwidth,
        height=0.6\textwidth,
        legend pos=south east, legend cell align={left},
        ymajorgrids=true, grid style={dotted,gray}
	]
	\addplot [black, dashed] table {\mnist};
	\addplot [black, more densely dashed] table {\kmnist};
	\addplot [black, densely dotted] table {\fmnist};
	\addlegendentry{MNIST}
	\addlegendentry{K-MNIST}
	\addlegendentry{F-MNIST}
	\end{axis}
\end{tikzpicture}
\caption{Single-run test accuracy per epoch for CTM on MNIST, K-MNIST, and F-MNIST.}
\label{figure:accuracy_per_epoch}
\end{figure}

 The hyperparameters were set using a light manual binary search. One particularly critical hyperparameter is the number of clauses employed per class. Table \ref{table:clauses_vs_accuracy_and_execution_time} reports execution time per epoch as well as mean test accuracy for an increasing number of class clauses. As seen, test accuracy increases steadily with the number of clauses employed, while execution time increases sub-linearly due to parallelization.
 
  The CTM performs rather robustly, providing tight \% confidence intervals for the mean performance. As seen in Table  \ref{table:results}, we have also included results for selected popular algorithms, as points of reference. Note that the CTM is an interpretable approach that does not employ multiple layers at this stage. Therefore, we believe a fair comparison should target traditional/simple CNNs. Results listed in italic are reported in the corresponding papers. Results for BinaryConnect and FPGA-accelerated BNNs on K-MNIST and Fashion-MNIST were not available, so are not reported. Notice that the CTM outperforms the binary CNNs for all the datasets, as well several simple baseline configurations, including 4-layer CNNs, Random Forests, Gradient Boosting, SVMs, and a 4-nearest neighbour classifier. Further observe that both the TM and the CTM obtain higher accuracy than Logistic Circuits \cite{LiangAAAI19} with binary data. Thus, in our experiments, the TM learning paradigm outperforms the probabilistic circuits paradigm \cite{LiangAAAI19,Peharzetal18} at extracting information from binary representations. However, only the CTM performs competitively with Logistic Circuits when the Logistic Circuits are given the advantage of real-valued inputs. Note that the CTM supports using multiple bits to encode the color of each pixel, however, in the present paper, we only investigate single bit representations. Finally, the CTM is outperformed by the more advanced deep learning network architectures PreActResNet-18 and ResNET18+VGG.
  
  Figure \ref{figure:accuracy_per_epoch} depicts test accuracy for the CTM on MNIST, K-MNIST, and F-MNIST, epoch-by-epoch. As seen, test accuracy climbs quickly in the first epochs, e.g., passing \% already in epoch  for MNIST. MNIST test accuracy peaks at \% after  epochs, while K-MNIST peaks at \% after  epochs. For F-MNIST, test accuracy surpasses \% in epoch  and peaks at \% in epoch .
  
In Table \ref{table:model_size}, the size of the trained models are listed for MNIST and F-MNIST. Size is measured in kilobytes of memory used. The memory usage of CTM is similar to Logistic Circuits, which both use significantly less memory than the neural network architectures. A potential advantage of CTM is the ability to trade off accuracy against computation time and memory usage, by reducing the number of clauses employed. E.g., a CTM with 250 clauses per class reduces memory usage by more than an order of magnitude at a relatively small loss in test accuracy (cf. Table \ref{table:clauses_vs_accuracy_and_execution_time}). 
 
Table \ref{table:example_patterns} contains example patterns produced by CTM for MNIST (with one bit per pixel using  clauses). The ``*" symbol can either take the value ``0" or ``1". The remaining bit values require strict matching. The examples have been selected to illustrate how several patterns together form complete numbers. As seen, the patterns are relatively easy to interpret for humans compared to, e.g., a neural network. They are also efficient to evaluate for computers, involving only logical operators, followed by summation.
 
\section{Conclusion and Further Work}
\label{sec:conclusion}

This paper introduced the Convolutional Tsetlin Machine (CTM), leveraging the learning mechanism of the Tsetlin Machine (TM). Whereas the TM categorizes images by employing each clause once per  input, the CTM uses each clause as a   convolution filter. The filters learned by the CTM are interpretable, being formulated using propositional formulae. To make the clauses location-aware, each patch is further enhanced with its coordinates within the image. Location awareness may prove useful in applications where both patterns and their location are distinguishing features. By \emph{randomly} selecting which patch to learn from, the standard Type I and Type II feedback of the classic TM can be employed directly. In this manner, the CTM obtains results on MNIST, Kuzushiji-MNIST, Fashion-MNIST, and the 2D Noisy XOR Problem that compare favorably with simple 4-layer CNNs, Logistic Circuits, as well as two binary neural network architectures.

In our further work, we intend to investigate more advanced binary encoding schemes, to go beyond grey-scale images (e.g., addressing CIFAR-10 and ImageNet). We further intend to develop schemes for deeper CTMs, with the first step being a two-layer CTM, to introduce more compact and expressive patterns with nested propositional formulae.

\bibliographystyle{abbrv}
\bibliography{references}



\end{document}
