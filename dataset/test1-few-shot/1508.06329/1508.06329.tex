\documentclass[a4paper, 11pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{latexsym}
\usepackage{amssymb,amsmath,amsthm}
\usepackage{graphicx}
\usepackage{wrapfig}

\newtheorem{tw}{Theory}[section]
\newtheorem{lm}[tw]{Lemma}
\newtheorem{obs}[tw]{Observation}

\newenvironment{pr}[1][Property]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\usepackage{indentfirst}

\begin{document}

\begin{titlepage}
\begin{center}
{\LARGE Uniwersytet Jagielloński} \3pt]
{\Large Zespół Katedr i Zakładów Informatyki Matematycznej}
\vspace{2.5cm}

{\Large Agnieszka Łupińska}\\

\vspace{3cm}
{\huge \bf A Parallel Algorithm to Test Chordality of Graphs}\\
\vspace{4cm}

\end{center}
\begin{flushright}
Promoter:\\
{Dr Maciej Ślusarek}\\
\end{flushright}
\vspace{1.5cm}
\begin{center}
{\Large Kraków 2013}
\end{center}
\end{titlepage}
\newpage
\tableofcontents
\newpage

\section{Introduction}

A graph  is chordal if each cycle of size greater than 3 in  has a~chord, 
that is an edge between two non-adjacent vertices on the cycle. 
We present a simple parallel algorithm to test chordality of graphs 
which is based on~the~parallel Lexicographical Breadth-First Search 
algorithm. In~total, the algorithm takes time  on -threads machine and it 
performs work , where  is the number of vertices in a graph. Our~implementation 
of the algorithm uses a GPU environment Nvidia CUDA~C. 
The~algorithm is implemented in CUDA 4.2 and it has been tested on Nvidia 
GeForce GTX 560 Ti of compute capability 2.1. At the end of the thesis 
we~present the~results achieved by our implementation and compare them 
with the results achieved by the sequential algorithm.

This thesis is organized as follows. Section 2 is an introduction to~the~parallel programming
using the GPU environment Nvidia CUDA C. Section 3 introduces the basic graph definitions used
throughout the paper. Then~it~provides an overview of the graph theory related to~the~LexBFS 
algorithm and chordal graphs. Section 4 introduces the LexBFS algorithm and its two most 
known implementations. Section 5 provides the sequential algorithm to test chordality of graphs 
and the analysis of its correctness and time complexity. In section 6 we 
present our parallel LexBFS algorithm and a parallel algorithm to test chordality of graphs. 
In section 7 we give the performance results of our parallel implementation compared to
the sequential algorithm. In~section 8 we discuss our results and the possible further work.
\newpage

\section{CUDA Programming}

CUDA (Compute Unified Device Architecture) is a general-purpose parallel computing architecture 
for Nvidia GPUs. We present the main features of CUDA C used in our implementation. For more details, 
we recommend NVIDIA CUDA C Programming Guide~[4] and CUDA C Best Practices Guide [5]. 

CUDA C extends the C/C++ programming model to the heterogeneous programming model which operates on 
the CPU called the host, and on the~GPU called the device. In CUDA, a kernel is a function
executed in~parallel by many threads on the device. A thread is a sequence of executions. The~threads 
are grouped into blocks which are grouped into a~grid. Each~thread has a unique identifier in a grid. 
It can be computed within a~kernel through a combination of the built-in variables: 
,  and . 

All the threads may access data from the local, shared, constant, texture and global memory. To learn 
the texture memory and the constant memory see [4]. The local memory is a private memory of 
a thread. The shared memory is common to all threads within the same block and its lifetime is 
the same as the block. All theads have access to the same global memory.

The simple model of a program using the CUDA architecture is as follows: allocate and initialize
data on the host, allocate data on the device, transfer data from the host to the device, run the 
CUDA kernels on the device and transfer data from the device back to the host.

The CUDA architecture allows to synchronize executions of the threads in one block by using the 
 function. It works as a lock: the~threads, which reach that point in the code, wait for 
other threads which have not done it yet. 

One of the methods to synchronize the threads between blocks, is to split the computitions in 
the synchronization points and to run each of that piece as a separate kernel. We use this method 
in our work.

\section{Background}

\subsection{Basic graph definitions}

We introduce the following terminology to be used throughout this thesis. Let  be 
an undirected graph with the vertex set  and the edge set , where  consists of unordered 
pairs of vertices in . We denote the~size of  by N and the size of  by M. If  
then we abbreviate it to . We use  to denote the neighborhood of a vertex  
excluding . 

Let  be the set of the natural numbers and let  be the label of~, where 
 is a string over the alphabet . We use  to denote the~concatenation 
operator for labels.

A bijection  is called an \textit{ordering} of .
Let~ denote the inverse of  and thus  is the index of  
in~the~ordering of~. Let  be the ordering of . We use  to
denote the~neighborhood of  in the subgraph induced by .

We say that an ordering  of  is a BFS order if it is generated by the~well-known 
BFS algorithm (see for example [7]). We present this algorithm in~the~next chapter. 
Note that a graph can have many different BFS orderings.

A graph  is \textit{chordal} if each cycle of size greater than 3 in  has a \textit{chord}, 
that~is an edge between two non-adjacent vertices on the cycle. A vertex  is \textit{simplicial} 
if  induces a clique. An order  is a~\textit{perfect elimination order} 
if, for each ,  is a simplicial vertex in the~graph induced by .



\subsection{Overview}

The LexBFS algorithm, in addition to the recognition of chordal graphs, has many other applications. 
The LexBFS algorithm is used as a part of~many graph algorithms such as recognizing interval graphs, 
or computing transitive orientation of comparability graphs, \text{co-comparability} graphs
and~interval graphs.

An \textit{orientation} of an undirected graph  is a directed graph which is~created by assigning 
a direction to each edge. An orientation of edges is \textit{acyclic} if it does not contain a directed 
cycle. An orientation of edges is \textit{transitive} for all , if  is an edge 
and  is an edge then  is also an edge. A \textit{comparability} graph 
is an undirected graph that has an~acyclic transitive orientation on edges. A \textit{co-comparability} 
graph is a~graph  whose complement  is a comparability graph. An \textit{interval} 
graph  is an undirected graph that is the~intersection graph of intervals on the real line, i.e.~
, where ,   is an interval on~the~real line 
and . 

Gilmore and Hoffman [1] proved that a graph is an interval graph if~and~only if it is a chordal 
graph and a co-comparability graph (Figure 1). 

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=6cm]{kontrprzyklady.png}
        \caption{From left:  is a co-comparability graph and it is not an interval graph.
        The next graph is a chordal graph and it is not an interval graph.}
    \end{center}
\end{figure}

\newpage

Habib, McConnell, Paul and Viennot [2] gave a  algorithm for the~transitive orientation 
of a comparability graph and a~\text{} algorithm to recognize interval graphs. Both of them
use the LexBFS algorithm. It can be proved [2] that if  is a co-comparability graph \text{and } is a LexBFS order of  then there exists a transitive orientation 
of  such~that  is a sink/source of the orientation. Moreover if  is a 
comparability graph and  is a LexBFS order of  then there 
exists a~transitive orientation of  such that  is a sink/source of the orientation. 



\section{Lexicographic Breadth-First Search}

The Lexicographic Breadth-First Search (LexBFS) algorithm was introduced by D. Rose, R. Tarjan 
and G. Lueker in 1976 for finding a perfect elimination order, if any exists. The LexBFS 
algorithm is a restriction of~the~widely used Breadth-First Search (BFS) algorithm in the 
following sense: each possible order of vertices produced by LexBFS is a BFS order. The 
difference between them is that the LexBFS algorithm additionally assigns labels to nodes 
and then in each step of the algorithm chooses a node, whose label is lexicographically 
the largest. 

\subsection{Characterization of BFS and LexBFS orderings}

We present and compare two characterizations of the vertex orderings that can by obtained 
by the BFS algorithm and the LexBFS algorithm.\\

Let  be vertex of a graph  and let  denote its neighborhood. Let  be 
a FIFO queue. We present an equivalent version of Tarjan's BFS algorithm:\\

\begin{tabular}{l}
\hline
Breadth-Frist Search algorithm\\
\hline
\\
BFS()\\
\hspace{0.5cm}  for  to  do \\
\hspace{0.5cm}  \\
\hspace{0.5cm}  for  to  do\\
\hspace{1cm}        if .nonEmpty() then .dequeue()\\
\hspace{1cm}        else  = any node s.t. \\
\hspace{1cm}        , \\
\hspace{1cm}        for each  s.t.  do\\
\hspace{1.5cm}          if  then .enqueue()\\
\\
\hline
\end{tabular}
\\
\\
\begin{pr}[Property B.]
    If ,  and  then exists  such that . 
\end{pr}

\begin{lm}
     is a BFS order   satisfies the B-property.
\end{lm}

\begin{proof}
    ()
    Let  be a BFS order and let  be a FIFO queue used by~the~algorithm. We assume 
    that the nodes of~the~graph~ are~renumbered according to~. 
   
    \begin{figure}[h]
        \begin{center}
            \includegraphics[width=3cm]{bProperty1.png}
        \end{center}
    \end{figure}

    During the algorithm  was visited before  which was visited before .
    When the algorithm visits  then it adds  to , because of , 
    and~it~does not add  to , because of . Then  can be first in  
    before  if and only if the algorithm had visited some  before it visited  
    and  then the algorithm had added  to  before it added  to .
    In~the~same words, there exists  such that .

    \begin{figure}[h]
        \begin{center}
            \includegraphics[width=4cm]{bProperty2.png}
        \end{center}
    \end{figure}

    ()
    Let  be an order satisfying a property . We want to show that  is
    a BFS order. Let  be some vertex of graph . When the BFS algorithm visits  
    it pushes on a queue all neighbors of  which have not been visited yet. Then
    BFS pops the next vertex from a queue. Let  be another vertex of a graph . If the 
    BFS algorithm visits  before  then all not visited neighbors of  are placed 
    in  before all not visited neighbors of . Hence it is sufficient to show 
    that if  in order , then all neighbors of  which are placed on to the
    right of  in , lie before all neighbors of  (which are to the right of 
    in ). But it is equivalent to property . See the figure above.
    
\end{proof}

Let  be a vertex of a graph  and let  be the neighborhood of . Let~ 
denote the label of . Let  be a priority queue of vertices with~priority on 
lexicographically the largest label. Consider the following algorithm:\\

\begin{tabular}{l}
\hline
Lexicographic Breadth-Frist Search algorithm\\
\hline
\\
LexBFS()\\
\hspace{0.5cm}  for  =  to  do\\
\hspace{1cm}         = \\\hspace{1cm}         = \\
\hspace{1cm}        .enqueue()\\
\hspace{0.5cm}  for  =  to  do\\
\hspace{1cm}         = .dequeue() // is lexicographically the largest\\
\hspace{1cm}         = ,  = \\
\hspace{1cm}        for each  s.t.  do\\
\hspace{1.5cm}           = \\
\\
\hline

\end{tabular}\\

\begin{pr}[Property LB.]
    If ,  and  then exists  such that  and 
    . 
\end{pr}

\begin{lm}
     is a LexBFS order   satisfies LB-property.
\end{lm}

\begin{proof}
    () Let  be the LexBFS order and let  be a priority queue used 
    by~the~LexBFS algorithm. We assume that the nodes of the graph  are~renumbered according to . 
    
    \begin{figure}[h]
        \begin{center}
            \includegraphics[width=3cm]{bProperty1.png}
        \end{center}
    \end{figure}

    During the algorithm  was visited before  which was visited before .
    Let  be the label of  and  be the label of .
    Let  be the~number of iteration and  be the number of vertices in .
    
    When the algorithm visits , it concatenates the label of  with \text{}, as~, 
    and it does not concatenate the label of  with , as \text{}. Since the~label 
    of  is lexicographically larger than the label of  then there~is an index  in 
    the labels such that  and~. The~index  
    is the first index at which the labels were updated in different iterations. The label of  
    was updated before the label of  because  and~in~each iteration the 
    number  decreases as  increases. The  index exists if~and~only if the algorithm 
    visited some  such that the algorithm concatenated the label of  with  and 
    it did not concatenate the label of  with~. It must be that  because 
    otherwise we would have:

    \begin{enumerate}
        \item if  then , as the numbers  decreased and~.
        \item if  then , as  and .
    \end{enumerate}
    Both cases are in contradiction to . Therefore, there exists  such that 
     and . 

    \begin{figure}[h]
        \begin{center}
            \includegraphics[width=4cm]{bProperty3.png}
        \end{center}
    \end{figure}
   
    () 
    Let  be an order satisfying a property LB. We want to show that~ is a LexBFS
    order. Let  be a vertex of graph . When the LexBFS algorithm visits  then it updates 
    the labels of all not visited neighbors of~. Let  be another vertex of . 
    If the LexBFS algorithm visits  before  then~all neighbors of  which are not adjacent 
    to  have labels greater than~the labels of all neighbors of , because the numbers 
    added to~the~end of~labels decrease for successive vertices. So in order to prove the claim 
    we~must show that if  in  then all neighobrs of , which are to~the~right of 
    in  and they are not adjacent to , lie before all neighbors of , which~are to the right
    of  in . But again, it is equivalent to property LB. See the figure above.

\end{proof}

It is easy to see that the LB-property implies B-property so the LexBFS algorithm is 
a restriction of the BFS algorithm.

\subsection{Two implementations}

The first implementation of the LexBFS algorithm was proposed by D.J. Rose, R.E. Tarjan, G. S. 
Leuker in 1976 [3]. They use a double-linked list  to store vertices of the same label .
All lists  are stored in the list  in descending order given by labels .
Additionally, each vertex  has a pair of pointers the first of which is leading to the 
list  containing  and~the~second one is leading to the place of  on the list . 

At the beginning of the algorithm, all the vertices have the same label  and they are 
on the list . There are two operations: getting a vertex  with the lexicographically
largest label and updating labels of all nodes adjacent to . To perform the first 
operation the algorithm takes the first list  from  and then returns the first vertex from 
. In the second operation, for each  adjacent to , the algorithm concatenates 
the label  of the vertex  with the number . Then the algorithm removes  from 
the list  and inserts it to the list , where  is the iteration number. 
If~the~list  does not existed in  then the algorithm creates it.
This~implementation has the  time complexity. 

The second implemenation was proposed by Habib, McConnell, Paul and~Viennot in 2000 [2] and
it uses the partition refinement technique. Let~ be a~doubly-linked list consisting of all 
vertices of . Let  be a doubly-linked list of classes of vertices. All vertices in a class
occupy consecutive elements in  and the class is represented by a pair of pointers to the first 
and the last element in the class. Each~vertex  has a pointer to the class containing~.\\ 

\begin{tabular}{l}
    \hline
    The LexBFS algorithm using partition refinement\\
    \hline
    \\
    LexBFS()\\
    \hspace{0.5cm}   - a single-element list of a class containing all vertices\\
    \hspace{0.5cm}  for  =  to  do\\
    \hspace{1cm}         - the first element of the first class on the list \\
    \hspace{1cm}        remove  from \\
    \hspace{1cm}        \\
    \hspace{0.5cm}  //\textit{partition}\\
    \hspace{0.5cm}  for each class  do\\
    \hspace{1cm}        \\
    \hspace{1cm}        \\
    \hspace{1cm}        replace  by ,  in \\
    \\
    \hline
\end{tabular}\\

During the partition, each  is removed from an old class  and it is inserted
to some new class . The \textit{partition} procedure can be implemented in  time.

\section{Chordal graphs}

Before we present the algorithm to test chordality of graphs we prove a~theorem introduced
by D.J. Rose, R.E. Tarjan, G. S. Leuker [3]. 



\begin{tw}[Rose, Tarjan, Leuker]
    A graph  is chordal if and only if a LexBFS order of  is a perfect elimination 
    order.
\end{tw}

\begin{proof}
    () Let  be a chordal graph and let  be its LexBFS
    order. We assume that the nodes of  are renumbered according to . We~show that each 
    vertex  is simplicial in the graph induced by . 

    Assume by contradiction that some  is not simplicial. Then there exist  
    such that  both adjacent to  and not adjacent themselves.

    \begin{figure}[h]
        \begin{center}
            \includegraphics[width=3.1cm]{chordal1.png}
        \end{center}
    \end{figure}

    Because  satisfies LB-property then there exists some  \text{such that~} and  
    is adjacent to  and it is not adjacent to . Note that  because  is 
    chordal and otherwise we would have a chordless cycle .

    \begin{figure}[h]
        \begin{center}
            \includegraphics[width=4.1cm]{chordal2.png}
        \end{center}
    \end{figure}

    Now we have got ,  and . Again, we use the LB-property 
    in respect to  and obtain : , , . Moreover  
    is not adjacent to  because of the cycle  and chordality of .

    \begin{figure}[h]
        \begin{center}
            \includegraphics[width=5.1cm]{chordal3.png}
        \end{center}
    \end{figure}

    Next time we can apply the LB-property to the vertices . This step can be repeated 
    infinitely, thus contradicting the general assuption that  is finite.

    So we have proved that each vertex  is simplicial in the graph induced by , that is the order  is the perfect elimination order.

    () It suffices to prove that if graph  has any perfect elimination order
    then it is chordal.
    
    Let  be a perfect elimination order of . Assume that in  
    there is a cycle  of length  and let  be the vertex of the greatest 
    index in . Let  and  be vertices adjacent to  in the cycle . 
    Because  and  are on the left from  in  then they are adjacent as  
    is a perfect elimination order. Therefore  has the chord , which finishes the proof.

\end{proof}

\subsection{Maximum Cardinality Search}

In 1984 Robert E. Tarjan and M. Yannakakis introduced in [6] the Maximum Cardinality Search
algorithm (MCS) as an alternative method for~finding a perfect elimination ordering of chordal 
graphs. The Maximum Cardinality Search instead of strings uses natural numbers as labels for 
vertices. In each iteration, the algorithm chooses a new vertex of the largest label, that is 
the vertex whose neighborhood in the graph induced by the nodes chosen so far is the largest 
among all vertices have not chosen yet. The MCS algorithm has a  time implementation [6].

Let  be a graph,  be a priority queue. For each  .
We present Tarjan and Yannakakis's MCS algorithm:\\

    \begin{tabular}{l}
        \hline
        Maximum Cardinality Search algorithm\\
        \hline
        \\
        MCS()\\
        \hspace{0.5cm}  for  =  to  do\\
        \hspace{1cm}         = \\
        \hspace{1cm}        \\
        \hspace{1cm}        .enqueue()\\
        \hspace{0.5cm}  for  =  to  do\\
        \hspace{1cm}         = .dequeue()\\
        \hspace{1cm}         = ,  = \\
        \hspace{1cm}        for each  such that  do\\
        \hspace{1.5cm}           = \\
        \\
        \hline
        
    \end{tabular}\\

    Robert E. Tarjan and M. Yannakakis proved the following theorem [6].

\begin{tw}
     is a chordal graph if and only if -order of  is a perfect elimination order.
\end{tw}


\subsection{Algorithm to test chordality}

Theorem 5.1. gives us the following tool to test if a given graph is chordal. First we run 
the LexBFS algorithm to produce a LexBFS order. Next we~check if the LexBFS order is 
the perfect elimination order. 

Let  be an order returned by the LexBFS algorithm. For~each~ 
let  be vertices adjacent to  on the left from  in 
and let~ be the right most vertex in . 

The algorithm presented below tests if  is a perfect elimination order. This is performed
by checking if for each  it holds that .
The correctness of such the approach is proved later.

To test if  is a perfect elimiantion order we only need to check if~for~each~
is .\\ 

\begin{tabular}{l}
\hline
Test if a LexBFS order is a perfect elimination order\\
\hline
\\
chordalityTest()\\
\hspace{0.5cm}  for  =  to  do \\
\hspace{0.5cm}  for each  that  do\\
\hspace{1cm}        .add()\\
\hspace{1cm}        if  then \\ 
\hspace{0.5cm}  for  =  to  do\\
\hspace{1cm}        for each  do\\
\hspace{1.5cm}          \\
\hspace{1cm}        for each  do\\
\hspace{1.5cm}          if  then\\
\hspace{2cm}                // check if \\
\hspace{2cm}                for each  such that  do\\
\hspace{2.5cm}                  if  then\\
\hspace{3cm}                        return false\\
\hspace{1cm}        for each  do\\
\hspace{1.5cm}          \\
\hspace{0.5cm}  return true\\
\\
\hline
\end{tabular}\\


\subsection{Correctness and complexity of algorithm}

We prove that the algorithm for testing if a LexBFS order is a perfect elimination order 
is correct.

Let  be a perfect elimination order. We show that then the algorithm returns . 
Let  be some node of . There are two cases:

\begin{enumerate}
    \item . 
        Then  is empty and the algorithm does not return false. 
    \item , for some .
        Then algorithm checks if . 
    
        \begin{figure}[h!]
            \begin{center}
                \includegraphics[width=9cm]{correctness.png}
            \end{center}
        \end{figure}

        
        Because all nodes of set  are on the left of  then~they~are 
        candidates to members of . It remains to show that they are adjacent to~. 
        Because  then  and  is a clique (because  is a perfect 
        elimination order) therefore all vertices of  are adjacent to  and 
        . So the algorithm never return false, hence it returns
        true in the end.
\end{enumerate}

Let  be not a perfect elimination order. We show that then the algorithm returns .
We assume that vertices of  are renumbered according to . As  is not a perfect 
elimination order, there exists some node in  that~its left neighborhood does not induce 
a clique. Let  be the first such~node in  and let  for some . Because  
in  then  is~a~clique (as  is the~first vertex in  for which  is not 
a clique). Therefore  and the algorithm returns .\\

The time complexity of the algorithm is determined by the nested loops. Let  be some node of . 
Note that the size of  is . Let us see how many times the algorithm scans . 

\begin{enumerate}
    \item marking  array
    \item looking for  
    \item unmarking  array
    \item for  such that  and for each  there is at most one such vertex~
\end{enumerate}

It means that each list  is read at most four times which gives  time for the whole 
graph. Summing up with the~time of producing the~LexBFS order, the test of chordality takes 
 time.

\section{Parallel algorithm}

Testing chordality of graphs has two steps: finding a LexBFS order and~checking if the LexBFS 
order is a perfect elimination order. To parallelize the chordality test we need to parallelize 
each of these steps separately. 

\subsection{Parallel LexBFS}

In our approach to the parallel version of LexBFS, the main loop of algorithm runs on the CPU and 
during each iteration  two task are performed on the GPU. The first one is choosing the vertex 
 with the lexicographically largest label and the second one is concatenating labels of vertices 
adjacent to  with . Both jobs are performed by N threads assigned to N vertices in a graph.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=7.5cm]{cpuGpuPodstawowy2.png}
        \caption{LexBFS algorithm}
    \end{center}
\end{figure}

    
We use the following data structures. The graph  is stored in an adjacency matrix . 
A linked list  is a list of sets . Each set  includes all vertices whose labels 
are equal . We identify the label of a set with~the~label of nodes in that set. These sets
form a partition of the vertex set, defined by means of labels.  is sorted lexicographically 
ascending. As  is a linked list, each set of  has a pointer  leading to the next 
set in the order,  in the last set in . The  array stores the order 
of~nodes computed by the algorithm. We say that a node is  if it has not processed yet.
(see Figure 3.)

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=6.5cm]{b.png}
        \caption{List  including sets: , , , , . 
            The last set on~the~list is .}
    \end{center}
\end{figure}

At the begining of algorithm all nodes of  are active and they have the~same label . 
It means that the list  has only one set  consisting of~all nodes of  and 
 leads to .

In a sequential version of algorithm, to find the vertex with the lexicographically largest label, 
the algorithm returns any vertex belonging to~the~last set on . As the last set on  is 
characterized by the pointer next equal , then this procedure can be performed in parallel 
by  threads as~follows. 

Let  be the vertex assigned to the thread~. Let  be the set 
including . Let  be a global variable shared by all threads.
For~each~thread  in parallel do:\\

\hspace{0.75cm} if  then \\

After this procedure the  variable stores a vertex whose label is~lexicographically 
the largest. Note that if there is more than one such~vertex then we cannot predict which one 
will be stored in .

Let  be the iteration of the main loop in which  has the lexicographically largest label.
Let  be some neighbor of ,  be the label of  and let  be in the set .

The update operation concatenates label  in back with number . Note that the number  
has not appeared in any label so far and it is the smallest among all numbers occuring in the labels. 

Next, the algorithm removes  from  and inserts it to the new set containing the nodes with
the label . If the new set has not existed yet then the algorithm creates it.

Let us look closely at the operation of creating a new set. Let  and~ be~two sets of nodes on 
the list  containing nodes labeled  and  respectively. Assume that , i.e. 
 comes before  in .

\begin{lm} If  is a number of iteration during which the new set containing nodes with the label 
     is created then  in .
\end{lm}

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=11cm]{b4.png}
        \caption{The list  before and after moving , ,  to the new set. 
        Vertices  are adjacent to  vertex.}
    \end{center}
\end{figure}

\begin{proof} Each label is a string of numbers. Let , 
     and 
    .
    Let~ be the smallest index such that  and . There~are two cases:
    \begin{enumerate}
        \item  and \\
            Then  and this two numbers determine . 
        \item  and \\
            Then  is a prefix of  and concatenation also gives  
            because  is less than all numbers in  so in particular smaller than .
    \end{enumerate}
    Note that always  because of .
\end{proof}

The lemma gives us the following observation:

\begin{obs}
When a new set is created for vertices from a given set  then it should be inserted between 
and its successor on the list . 
\end{obs}

Based on observation 6.1., for each new set we can determine its place in~the~list without 
any additional list traversal or label comparisons. 

How many new sets are created during one iteration? The answers is: at most one for each old one.
Indeed if  and  are neighbors of  vertex and belonging to some set  then their
labels are equal both before and after concatenating them with .

Since in our algorithm updating labels is performed in parallel, it could happen that for some 
new label several threads would simultaneously create several new sets and then insert them to 
the list. In order to avoid such~a~mistake we use synchronization between performed instructions.
\newpage
Let  be the number of iteration and let  be a vertex with lexicographically the largest 
label chosen during  iteration. Let  be the vertex assigned to the thread . Let  
belong to set . Let  and~ be private variables of the thread . 
For~all threads  in~parallel do:\\

\hspace{0.75cm} 1. if  is not active or  is not adjacent to  then stop

\hspace{0.75cm} 2. 

\hspace{0.75cm} 3. create a new set 

\hspace{0.75cm} 4. synchronization: wait for other threads

\hspace{0.75cm} 5. set pointers: 

\hspace{1.4cm}      

\hspace{1.4cm}      

\hspace{0.75cm} 6. synchronization: wait for other threads

\hspace{0.75cm} 7. insert  to \\

Each thread creates its new set  and inserts it to the order. After that for each 
node ,  but only for one of~them we will have . 
See Figure 5.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=8cm]{inserting3.png}
        \caption{Insert a new set  into the list }
    \end{center}
\end{figure}

Note that we cannot predict which  will be in  so synchronization is performed 
to all threads read the same  inserted after~ in the list. The new sets of other 
threads are forgotten.

Now let us look at removing vertices from the sets. After this operation some sets can be empty. 
To get a vertex of the lexicographically largest label in the list, we take the last set on the 
list and this set cannot be empty. Therefore after each update operation, when removing vertices 
from sets is~performed, we remove all empty sets from the list.\\
\newpage
Before we show the procedure of removing the empty sets, consider the~following lemma. 

\begin{lm}
    If, after the update operation, the set  is empty then its successor on the list, if
    exists, is nonempty.
\end{lm}

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=9cm]{deleting.png}
    \end{center}
\end{figure}

\begin{proof}
    Let  be not empty set before the update operation. If after the~update operation 
     is empty then from Observation 6.2. we know that the~set  includes all 
    vertices which were in set  before the update. 
\end{proof}

The observation that for each empty set, its predecessor and successor are~nonempty means that 
all empty sets can be removed in parallel in one time. The removal of one set requires changing 
only two links of two adjacent non-empty sets. It is correct because it does not require any 
additional traversing through the list and it is independent from removing other empty sets.

To find out which sets are empty, we use an additional  array. For each set in the list,
 stores  if the set includes at least one node or  otherwise. At~the beginning 
all slots of  are . 

Let  be the vertex assigned to the thread . Let  be in set . Let  be the number 
of iteration. Let  be the vertex with lexicographically the largest label during  
iteration. Let  and  be private variables of the thread . 
For all threads  in parallel do:\\

\hspace{0.75cm} 1. if  is not active then stop

\hspace{0.75cm} 2. 

\hspace{0.75cm} 3. 

\hspace{0.75cm} 4. synchronizetion: wait for other threads

\hspace{0.75cm} 5. if  then stop

\hspace{0.75cm} 6. set \\

We use synchronization between instructions to make sure that counting vertices and removing 
empty sets are correct. Otherwise some sets could be~removed despite they are not empty and 
pointers could be changed improperly.\\
\newpage
In our implementation, getting the vertex of lexicographically the largest label and updating 
the labels of all adjacent vertices are performed in four stages. Each stage run on the GPU 
and they are synchronized - the~new one does not start until the last one has not finished. 
Because we use the~CUDA language to implement the algorithm we use the term \textit{kernel} 
instead of \textit{stage}. Each kernel is executed by  threads, one for each vertex.

At the beginning of algorithm all vertices have lexicographically the same label then we set 
 to random vertex . Next, in each iteration of  loop, one vertex is choosen 
to the LexBFS order. 

The first kernel adds  vertex to the LexBFS order and marks it as~non-active. Next,
the first part of the labels updating is performed: setting counters of sets on  and 
saving the  pointers of each set.

In the second kernel, the new sets are inserted to the list .

In the third kernel, each vertex that is active and adjacent to  is moved to the next 
set. Next, the counting is performed: for each active vertex, the counter of its set is made 
equal . At the end, the first part of~deleting the empty set is performed: saving the  
pointers of each set.

The last kernel deletes all empty sets and chooses the new  vertex.

At the end of the LexBFS algorithm, the  array stores the LexBFS order.\\

\begin{tabular}{l}
\hline
Parallel Lexicographic Breadth-Frist Search algorithm\\
\hline
\\
parallelLexBFS()\\
\hspace{0.5cm} \\
\hspace{0.5cm} for  to  do\\
\hspace{0.5cm}\hspace{0.5cm} kernel1()\\
\hspace{0.5cm}\hspace{0.5cm} kernel2()\\
\hspace{0.5cm}\hspace{0.5cm} kernel3()\\
\hspace{0.5cm}\hspace{0.5cm} kernel4()\\
\\
kernel1()\\
\hspace{0.5cm} if  is active then\\
\hspace{1cm}        \\
\hspace{1cm}        \\
\hspace{1cm}        create \\
\hspace{0.5cm} if  is  then\\
\hspace{1cm}        \\
\hspace{1cm}        mark  as non-active\\
\\
kernel2()\\
\hspace{0.5cm} if  is active and  is adjacent to  then\\
\hspace{1cm}        \textit{//inserting new sets to list}\\
\hspace{1cm}        \\
\hspace{1cm}        \\
\\
kernel3()\\
\hspace{0.5cm} if  is active and  is adjacent to  then\\
\hspace{1cm}        \textit{//moving to new sets}\\ 
\hspace{1cm}        move  to \\
\hspace{0.5cm} if  is active then\\
\hspace{1cm}        \\
\hspace{1cm}        \\
\\
kernel4()\\
\hspace{0.5cm}  if  is active then\\
\hspace{1cm}        if  then\\
\hspace{1.5cm}          \textit{//deleting empty sets}\\
\hspace{1.5cm}          \\
\hspace{1cm}        if  then\\
\hspace{1.5cm}          \textit{//updating current}\\
\hspace{1.5cm}          \\
\\

\hline

\end{tabular}\\

\newpage

\subsection{Parallel test for perfect elimination order}

Now we are given the LexBFS order . The second step of testing chordality of graphs 
is checking if the LexBFS order is the perfect elimination order, that is, if for each 
vertex , the neighborhood of  on the left in the order forms a~clique. 

Let  be the set of all nodes adjacent to  that lie on~the~left of~
in  and let  be the right most node in . In the sequential version of the 
algorithm, for~each node  we check if . Now~we~do~this 
in parallel for~all nodes. 

The algorithm has two kernels. The first one, for each , in parallel computes the left 
neighborhood  and the right most vertex in . In~the~second kernel, each thread 
 processes the left neighborhood of  and the left neighborhood of . If some 
left neighbor of , different from , is not a left neighbor of  then  
marks the global variable  on false. At~the~end, if  is true then the order 
is the perfect elimiantion order.\\

\begin{tabular}{l}
\hline
Parallel Test for Perfect Elimination Order\\
\hline
\\
//run on the cpu\\
parallelTestPEO()\\
\hspace{0.5cm}  \\
\hspace{0.5cm}  preparationLNandP()\\
\hspace{0.5cm}  testing()\\
\hspace{0.5cm}  if  then return YES\\
\hspace{0.5cm}  else return NO\\
\\
//run on the gpu\\
preperationLNandP()\\
\hspace{0.5cm}  \\
\hspace{0.5cm}  for each  adjacent to  do\\
\hspace{1cm}        if  then\\
\hspace{1.5cm}          .insert(y)\\
\hspace{1.5cm}          if  then\\
\hspace{2cm}                \\
\\
//run on the gpu\\
testing()\\
\hspace{0.5cm}  for each  adjacent to  do\\
\hspace{1cm}        if  and  then\\
\hspace{1.5cm}          \\
\\

\hline

\end{tabular}\\

\subsection{Details of the parallel implementation}

After~reading~the~input,~the~algorithm~copies~~the~~adjacency~~matrix of~the~graph to the array 
on the device memory. During the LexBFS algorithm, no other memory transfer between host 
and device is performed.

Since the algorithm does not compare any labels, the label concatenation can be omitted. 
Instead of this, the algorithm assignes to the new sets the~numbers which have not appeared 
yet. In each iteration, there are at~most  new sets, hence during the whole algorithm 
there are at~most  new sets. Since each set has a pointer to the next set on a list 
then~to~store all~pointers, the algorithm uses an array~of~~size~,~~which~is~indexed 
by~the~numbers of the sets.

Our parallel implementation of the LexBFS algorithm uses the following arrays:

\begin{enumerate}
    \item the  array of size  for the boolean adjacency matrix.
    \item the  array of size  for the integer labels of sets.
    \item the ~~array~~of~~size~~~~for~~the~~integer~~indices~~of~~vertices in~the~LexBFS~order.
    \item the  array of size  for the integer indices of the next sets.
    \item the  auxiliary array of size  for the saved values 
        from~the~ array, one value for each vertex.
    \item the  array of size  for the boolean flags for recognizing 
        if~a~set is empty.
    \item the  variable for the integer number of vertex whose label is 
        lexicographically the largest.
\end{enumerate}

During an iteration, the algorithm processes one  vertex which~is assigned to one row 
of every 2-dimensional array and all threads process that~row in one time. See figure below.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=5cm]{readingData0.png}
    \end{center}
\end{figure}

Because each  is unique and unrepeatable throughout the LexBFS algorithm then each row 
is visited only once. Therefore, in order to reduce the amount of the device memory used by our 
algorithm, we use the~\text{2-dimensional}  array for two purposes: first as the adjacency matrix, 
next as the counter array. 

The second part of the chordality algorithm uses two arrays: the~ array and the  array. 
Because the~ array is overwitten after the~LexBFS algorithm then the algorithm 
copies again the adjacency matrix from the host memory to the  array on the device memory.

\section{Tests and results}

We introduce the following terminology to be used in this section. A~graph  with the 
vertex set  of size  is  if the size of  is . A graph  is  
if the size of  is . We consider the following classes of graphs:

\begin{enumerate}
    \item Cliques on -vertices, for .
    \item Dense random graphs on  vertices.
    \item Sparse random graphs on  vertices.
    \item Trees on  vertices.
    \item Chordal random graphs on  vertices.
\end{enumerate}

We test two implementations. The sequential implementation is the~Habib, McConnell, Paul and 
Viennot algorithm presented in [2], which use a static memory allocation. For each class, we 
also present the time excluding the~input reading and the dynamic allocation of the device 
memory. For the parallel implementation, the time of reading the input and the dynamic allocation 
on the GPU is many times greater than the remaining time of the algorithm. However algorithm 
cannot be implemented without these operations.

\newpage
\subsection{Cliques}

Figure 6 presents timing results for cliques. For graphs of size smaller than 1000, the
sequential version is faster. When vertices number is 10000, the parallel implementation is 
two times faster than the sequential one. 

\begin{figure}[h!]
    \caption{Cliques}
    \begin{center}
    \begin{tabular}{|r|c|c|c|c|}
        \hline
        & \multicolumn{4}{|c|}{the time (ms)}\\
        \cline{2-5}
        & \multicolumn{2}{|c|}{GPU} & \multicolumn{2}{|c|}{CPU}\\
        \cline{2-5}
            & without input and &  & without & \\
        N   & memory allocation time &  & input time & \\
        \hline
        1000 & 0.4 &  1.5 &  1.3 &  2.1\\
        2000 & 0.8 &  4.9 &  4.9 &  8.3\\
        3000 & 1.4 &  9.8 & 11.1 & 18.9\\
        4000 & 2.1 & 17.0 & 19.5 & 33.7\\
        5000 & 2.7 & 26.2 & 30.5 & 52.1\\
        6000 & 3.7 & 37.1 & 44.0 & 74.8\\
        7000 & 4.4 & 50.1 & 60.0 &101.7\\
        8000 & 5.4 & 66.0 & 77.8 &132.6\\
        9000 & 6.6 & 83.2 & 99.0 &168.3\\
       10000 & 7.8 &101.8 &121.9 &207.3\\
       11000 & 8.9 &126.0 &147.0 &251.4\\
        \hline
    \end{tabular}
\includegraphics[width=14.1cm]{testKlik5.png}\\
    \end{center}
\end{figure}

\newpage
\subsection{Dense graphs}

Figure 7 presents timing results for dense random graphs. For each test the parallel 
implementation is almost two times faster than the sequential implementation.

\begin{figure}[h!]
    \caption{Dense random graphs: , }
    \begin{center}
    \begin{tabular}{|r|c|c|c|c|}
        \hline
        & \multicolumn{4}{|c|}{the time (ms)}\\
        \cline{2-5}
        & \multicolumn{2}{|c|}{GPU} & \multicolumn{2}{|c|}{CPU}\\
        \cline{2-5}
            & without input and &  & without & \\
           & memory allocation time &  & input time & \\
        \hline
        test1 & 9.0 & 107.4 & 107.1 & 191.9\\
        test2 & 8.9 & 108.6 & 106.6 & 191.0\\
        test3 & 8.9 & 106.2 & 107.4 & 191.6\\
        test4 & 8.9 & 106.7 & 106.4 & 191.3\\
        test5 & 8.9 & 107.2 & 109.0 & 191.3\\
        \hline
    \end{tabular}
    \includegraphics[width=13cm]{testDenseRandom5.png}\\
\end{center}
\end{figure}
\newpage

\subsection{Sparse graphs}

We have tested our implementation on sparse random graphs which .
The parallel implementation is slower than the sequential implementation.
(Figure 8)

\begin{figure}[h!]
    \caption{Sparse random graphs: , }
    \begin{center}
    \begin{tabular}{|r|c|c|c|c|}
        \hline
        & \multicolumn{4}{|c|}{the time (ms)}\\
        \cline{2-5}
        & \multicolumn{2}{|c|}{GPU} & \multicolumn{2}{|c|}{CPU}\\
        \cline{2-5}
            & without input and &  & without & \\
           & memory allocation time&  & input time & \\
        \hline
        test1 & 11.3 & 92.7 & 0.9 & 71.6\\
        test2 & 11.2 & 91.5 & 0.7 & 71.0\\
        test3 & 11.2 & 91.1 & 0.8 & 71.1\\
        test4 & 11.2 & 90.9 & 0.8 & 71.1\\
        test5 & 11.2 & 92.0 & 0.8 & 72.0\\
        \hline
    \end{tabular}


    \includegraphics[width=13cm]{testSparse20n5.png}\\
    \end{center}
\end{figure}
\newpage

\subsection{Trees}

The results for trees are very similar to the results for sparse random graphs (Figure 9).

\begin{figure}[h!]
    \caption{Trees: N=10000}
    \begin{center}
    \begin{tabular}{|r|c|c|c|c|}
        \hline
        & \multicolumn{4}{|c|}{the time (ms)}\\
        \cline{2-5}
        & \multicolumn{2}{|c|}{GPU} & \multicolumn{2}{|c|}{CPU}\\
        \cline{2-5}
            & without input and &  & without & \\
           & memory allocation time&  & input time & \\
        \hline
        test1 & 7.2 & 86.9 & 1.4 & 73.0\\
        test2 & 7.3 & 87.6 & 1.4 & 73.3\\
        test3 & 7.4 & 87.6 & 1.1 & 71.0\\
        test4 & 7.3 & 86.6 & 0.0 & 70.3\\
        test5 & 7.3 & 88.0 & 0.1 & 70.0\\
        test6 & 7.5 & 87.7 & 0.1 & 70.3\\
        test7 & 7.2 & 87.6 & 1.0 & 71.2\\
        \hline
    \end{tabular}
\includegraphics[width=13cm]{tree100005.png}\\
    \end{center}
\end{figure}
\newpage

\subsection{Chordal graphs}

Figure 10 presents timing results for chordal random graphs, including dense and sparse 
graphs. Only for sparse graphs the parallel implementation is slower. On this figure it is easy to see
that the parallel implementation is stable - the time of algorithm is independent from the number of 
edges, in~contrast to the sequential implementation.

\begin{figure}[h!]
    \caption{Chordal random graphs, N=10000}
    \begin{center}
    \begin{tabular}{|r|c|c|c|c|}
        \hline
        & \multicolumn{4}{|c|}{the time (ms)}\\
        \cline{2-5}
        & \multicolumn{2}{|c|}{GPU} & \multicolumn{2}{|c|}{CPU}\\
        \cline{2-5}
            & without input and &  & without & \\
           & memory allocation time&  & input time & \\
        \hline
        test1 & 7.2 &  92.0 & 19.2 & 92.8\\
        test2 & 7.9 &  99.0 & 66.4 & 146.5\\
        test3 & 7.9 &  99.2 & 68.9 & 149.2\\
        test4 & 7.6 &  98.1 & 62.8 & 142.5\\
        test5 & 7.8 &  95.7 & 42.4 & 120.2\\
        test6 & 7.4 &  90.0 & 12.8 & 86.0\\
        test7 & 7.5 &  90.7 & 13.1 & 85.4\\
        test8 & 7.4 &  90.1 & 11.7 & 83.8\\
        \hline
    \end{tabular}
\includegraphics[width=13cm]{testChordalRandomGraphs5.png}
    \end{center}
\end{figure}


\section{Conclusion and Future Work}

The main result of this paper is the parallel algorithm to test chordality of graphs 
based on our own efficient parallel version the LexBFS algorithm. For a graph  of 
 vertices and  edges, the algorithm takes the  time and performs the  
work on the -threads machine. We use the~CUDA multithreads architecture to implement 
these algorithms.

Our parallel implementation achives best results for cliques and dense graphs. For 
graphs of 1000 and more vertices, the parallel algorithm is significantly faster 
than our fast sequential implementation and for graphs of~10000 vertices, the parallel 
implementation is two times faster than~the~sequential version. For trees, sparse graphs 
and small graphs (less than 1000 vertices) the sequential algorithm outperforms the
parallel one. However, for~this kind of data the parallel implementation is stable, 
the execution time is independent of the size of a graph.

It would be interesting if the parallel LexBFS algorithm could be used as~a~core for 
efficient parallel testing of interval graphs. Further research could be also made 
towards parallel implementation of the MCS algorithm. 




\newpage

\begin{thebibliography}{}
    \bibitem{1} P. C. Gilmore, A. J. Hoffman: \textit{A characterization of comparability 
        graphs and of interval graphs}\\
        Canad. J. Math. 16 (1964) 539-548.
        
    \bibitem{2} Michel Habib, Ross McConnell, Christophe Paul, Laurent Viennot:
        \textit{Lex-BFS and partition refinement, with applications to transitive orientation,
        interval graph recognition and consecutive ones testing}\\
        Theoretical Computer Science 234 (2000) 59-84.

    \bibitem{3} Donald J. Rose, Robert E. Tarjan, and George S. Lueker:
        \textit{Algorithmic aspects of vertex elimination on graphs}\\
        SIAM J. Comput., 5(2):266–283, 1976.
    
    \bibitem{4} NVIDIA: 
        \textit{NVIDIA CUDA C Programming Guide}\\
        http://docs.nvidia.com/cuda/pdf/CUDA\_C\_Programming\_Guide.pdf

    \bibitem{5} NVIDIA: 
        \textit{CUDA C BEST PRACTICES GUIDE}\\
        http://docs.nvidia.com/cuda/pdf/CUDA\_C\_Best\_Practices\_Guide.pdf

    \bibitem{6} Robert E. Tarjan, Mihalis Yannakakis: 
        \textit{Simple linear-time algorithms to test chordality of graphs, test 
            acyclicity of hypergraphs, and selectively reduce acyclic hypergraphs}\\
            SIAM J. Comput., 13(3):566-579, August 1984.

    \bibitem{7} Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, Clifford Stein:
            \textit{Introduction to Algorithms}\\
            The MIT Press, 2009.

\end{thebibliography}


\end{document}
