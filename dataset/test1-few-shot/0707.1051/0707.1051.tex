\documentclass[11pt]{article}

\usepackage{times}  

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amscd}
\usepackage{fullpage}

\newcommand{\ignore}[1]{}

\ignore{
\setlength{\textwidth}{6.5 true in}  \setlength{\oddsidemargin}{0. true in}
\setlength{\evensidemargin}{0. true in}
\setlength{\textheight}{9.0 true in}  \setlength{\topmargin}{-0.0 true in}   \setlength{\parindent}{15pt}
\setlength{\parskip}{1pt}
}


\ignore{
\usepackage{atbeginend}
\BeforeBegin{lemma}{\vspace{-2mm} }
\BeforeBegin{theorem}{\vspace{-2mm} }
\BeforeBegin{definition}{\vspace{-2mm} }
\BeforeBegin{cor}{\vspace{-2mm} }
\AfterEnd{lemma}{\vspace{-2mm} }
\AfterEnd{theorem}{\vspace{-2mm} }
\AfterEnd{definition}{\vspace{-2mm} }
\AfterEnd{cor}{\vspace{-2mm} }
 }

\newtheorem{theorem}{Theorem}
\newtheorem{cor}[theorem]{Corollary}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{lemma}[theorem]{Lemma} 
\newtheorem{remark}[theorem]{Remark}
\newtheorem{question}[theorem]{Question}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{conjecture}[theorem]{Conjecture}
\newcommand{\E}{\textup{E}}
\renewcommand{\P}{\textup{P}}
\def\eps{{\epsilon}}
\def\citizens{{R}}
\def\mafia{{M}}
\def\detectives{{D}}
\def\c{{R}}
\def\m{{M}}
\def\d{{D}}
\def\v{{V}}
\usepackage{color}

\definecolor{Red}{rgb}{1,0,0}
\definecolor{Blue}{rgb}{0,0,1}
\definecolor{Olive}{rgb}{0.41,0.55,0.13}
\definecolor{Green}{rgb}{0,1,0}
\definecolor{MGreen}{rgb}{0,0.8,0}
\definecolor{DGreen}{rgb}{0,0.55,0}
\definecolor{Yellow}{rgb}{1,1,0}
\definecolor{Cyan}{rgb}{0,1,1}
\definecolor{Magenta}{rgb}{1,0,1}
\definecolor{Orange}{rgb}{1,.5,0}
\definecolor{Violet}{rgb}{.5,0,.5}
\definecolor{Purple}{rgb}{.75,0,.25}
\definecolor{Brown}{rgb}{.75,.5,.25}
\definecolor{Grey}{rgb}{.5,.5,.5}
\definecolor{Black}{rgb}{0,0,0}
\definecolor{Black}{rgb}{0,0,0}

\def\red{\color{Red}}
\def\blue{\color{Blue}}
\def\olive{\color{Olive}}
\def\green{\color{Green}}
\def\mgreen{\color{MGreen}}
\def\dgreen{\color{DGreen}}
\def\yellow{\color{Yellow}}
\def\cyan{\color{Cyan}}
\def\magenta{\color{Magenta}}
\def\orange{\color{Orange}}
\def\violet{\color{Violet}}
\def\purple{\color{Purple}}
\def\brown{\color{Brown}}
\def\grey{\color{Grey}}
\def\black{\color{black}}

\newcommand{\ve}{\varepsilon}
\newcommand{\ra}{\rightarrow}
\newcommand{\ga}{\gamma}
\newcommand{\be}{\beta}
\newcommand{\tiO}{\tilde{O}}
\newcommand{\ov}[1]{\overline{#1}}
\newcommand{\si}{\sigma}
\newcommand{\widgraph}[2]{\includegraphics[keepaspectratio,width=#1]{#2}}



\begin{document}
\title{Noisy sorting without resampling}
\author{Mark Braverman\thanks{C.S. University of Toronto, partially 
supported by and NSERC CGS scholarship. Part of the work was done
while on a visit to IPAM, UCLA}
\and Elchanan Mossel
\thanks{Dept. of
Statistics, U.C. Berkeley. Supported by an Alfred Sloan fellowship
in Mathematics, by NSF grants DMS-0528488
and DMS-0548249 (CAREER) and by DOD ONR grant N0014-07-1-05-06. 
Part of this work was done while the author was visiting IPAM, UCLA}}
\date{\today}

\maketitle

\begin{abstract}
In this paper we study noisy sorting without re-sampling. 
In this problem there is an unknown order 
 where  is a 
permutation on  elements. 
The input is the status of  
queries of the form , where   
with probability at least   if  
for all pairs , where  is a constant and 
 for all  and . It is assumed 
that the errors are independent. 
Given the status of the queries the goal is to find 
the maximum likelihood order. 
In other words, the goal is find a permutation  that minimizes 
the number of pairs  where 
. The problem so defined is 
the feedback arc set problem on 
distributions of inputs, each of which is a tournament 
obtained as a noisy perturbations of a linear 
order. Note that when  and  is large, 
it is impossible to recover the original order .

It is known that the weighted feedback are set problem on tournaments 
is NP-hard in general. Here we present an algorithm of running time 
 and sampling complexity  that 
with high probability solves the noisy sorting without re-sampling problem. 
We also show that if  is an optimal 
solution of the problem then it is ``close'' to the original order. 
More formally, with high probability it holds that  
 and 
. 

Our results are of interest in applications to ranking, such as ranking in sports, or ranking of search items based on comparisons by experts. 

 \end{abstract}


\newpage
\section{Introduction}
We study the problem of sorting in the presence of noise. 
While sorting linear orders is a classical well studied problem, 
the introduction of noise poses very interesting challenges. 
Noise has to be considered when ranking or sorting is applied in many 
real life scenarios. 

A natural example comes from sports. How do we rank a league of soccer teams 
based on the outcome of the games? It is natural to assume that there is a true 
underlying order of which team is better and that the games outcome represent 
noisy versions of the pairwise comparisons between teams. Note that in 
this problem it is impossible to ``re-sample'' the order between 
a pair of teams.  
As a second example, consider experts comparing various items  
according to their importance where each pair of elements is compared by one 
expert. 
It is natural to assume that the experts opinions 
represent a noisy view of the actual order of significance. The question is 
then how to  aggregate this information? 

\subsection{The Sorting Model}
We will consider the following probabilistic model of instances. 
There will be  items denoted .  
There will be a {\em true order} given by a permutation  on 
 elements such that under the true order 
. 
The algorithm will have access to  queries defined as follows.

\begin{definition} \label{def:noise}
For each pair  the outcome of the comparison between  and 
 is denoted by  where for all  it holds 
that . We assume that the probability 
 is at least  if  
and that the queries 

are independent conditioned on the true order. 
In other words, for any set 
 
any vector  and 
 it holds that 

It is further assumed that .
\end{definition}

We will be interested in finding a ranking that will minimize the number of 
upsets. More formally:
\begin{definition}
Given  queries  the score  
of a ranking (permutation)  is given by 

We say that a ranking  is {\em optimal} for  if  
is a maximizer~(\ref{eq:score}) among all ranking. 

The {\em Noisy Sorting Without Resampling (NSWR)} 
problem is the problem of finding 
an optimal  given  assuming that  is generated as in 
Definition~\ref{def:noise}.
\end{definition}
The problem of maximizing~(\ref{eq:score}) without any assumptions on the 
input distribution is called the {\em feedback arc set problem for tournaments} 
which is known to be NP-hard, see subsection~\ref{subsec:related} 
for references, more background and related models. 

The score~(\ref{eq:score}) has a clear statistical interpretation in the 
case where each query is answered correctly with probability  exactly
In this case, for each permutation  we can calculate the probability 
 of observing  given that  is the true order. It is 
immediate to verify that  
for two constants  . Thus in this case the optimal solution to the 
NSWR problem is identical with the {\em maximum likelihood} order that is 
consistent with . This in particular implies that given a prior uniform 
distribution on the  rankings, any order  
maximizing~(\ref{eq:score}) is also a maximizers of the posterior probability 
given . So by analogy to problems in coding theory, 
see e.g.~\cite{Romann:97},  
 is a maximum likelihood decoding of the original order .  
 
Note furthermore that one should not expect to be able to find the true order 
if  is noisy. Indeed for any pair of adjacent elements we are only given 
one noisy bit to determine which of the two is bigger. 

\subsection{Related Sorting Models and Results}~\label{subsec:related} 
It is natural to consider the problem of finding an a ranking  that 
minimizes the score  without making any assumptions on the input 
. This problem, called the {\em feedback arc set problem for tournaments}  
is known to be NP hard~\cite{AiChNe:05,Alon:06}. 
However, it does admit PTAS~\cite{KenyonSchudy:07} achieving a  
approximation for 

in time that is polynomial in  and doubly exponential in .
The results of~\cite{KenyonSchudy:07} are the latest in a long line of work 
starting in the 1960's and including~\cite{AiChNe:05,Alon:06}. 
See~\cite{KenyonSchudy:07} 
for a detailed history of the feedback arc set problem. 
 
A problem that is in a sense easier than NSWR is the problem where 
repetitions are allowed in querying. 
In this case it is easy to observe that the original 
order may be recovered in  queries with high probability. 
Indeed, one may perform any of the standard  sorting 
algorithms and repeat each query  times in order to obtain the 
actual order between the queries elements with error probability  (say). 
More sophisticated methods allow to show that in fact the true order may be 
found in query complexity  with high probability~\cite{FPRU:90}, 
see also~\cite{KarpKleinberg:07}. 

\subsection{Main Results}
In our main results we show that the NSWR problem is solvable in polynomial 
time with high probability and that any optimal order is close to the true 
order. More formally we show that

\begin{theorem} \label{thm:sort}
There exists a randomized algorithm that for any  and  finds 
an optimal solution to the noisy sorting without resampling (NSWR) problem 
in time  except with probability . 
\end{theorem}

\begin{theorem} \label{thm:dist}
Consider the NSWR problem and let  be the {\em true} order and 
 be any optimal order than except with probability  
it holds that 


\end{theorem}

Utilizing some of the techniques of~\cite{FPRU:90} 
it is possible to obtain the results of 
Theorem~\ref{thm:sort} with low sampling complexity. More formally, 
\begin{theorem} \label{thm:sampling}
There is an implementation of a sorting algorithm with the same guarantees 
as in Theorem~\ref{thm:sort} and whose sampling complexity is  
where . 
\end{theorem}

It should be noted that the proofs can be modified to a more general case
where the conditional probability from \eqref{cond:prob} is always bounded 
from below by  without necessarily being independent.

\subsection{Techniques}
In order to obtain a polynomial time algorithm for the NSWR problem is important to identify that any optimal solution to the problem is close to the true one. 
Thus the main step of the analysis is the proof of Theorem~\ref{thm:dist}.

To find efficient sorting we use an insertion algorithm. Given an optimal 
order on a subset of the items we show how to insert a new element. 
Since the optimal order both before and after the insertion of the element has 
to satisfy Theorem~\ref{thm:dist}, it is also the case that no element moves 
more than  after the insertion and re-sorting. 
Using this and a dynamic programing approach we derive an insertion algorithm 
in Section~\ref{sec:presorted}. The results of this section may be 
of independent interest in cases where it is known that a single element 
insertion into an optimal suborder cannot result in a new optimal order 
where some elements moved by much. 

The main task is to to prove Theorem~\ref{thm:dist} in 
Section~\ref{sec:dist}. We first prove~(\ref{eq:lin_dist}) by showing that 
for a large enough constant , it 
is unlikely that any order  whose total distance is more than  will have , where  is the original order.  
We then establish~(\ref{eq:log_dev}) in subsection~\ref{subsec:log_dev} 
using a bootstrap argument. 
The argument is based on the idea that if the discrepancy in the position of 
an element  in an optimal order compared to the true order is more 
than  for a large constant , then there must exist many elements 
that are ``close'' to  that have also moved by much. 
This then leads to a contradiction with~(\ref{eq:lin_dist}). 

The final analysis of the insertion algorithm and the proof of Theorem~\ref{thm:sort} are provided in Section~\ref{sec:alg}. Section~\ref{sec:query} shows 
how using a variant of the sorting algorithm it is possible to achieve 
polynomial running time in sampling complexity .

\subsection{Distances between rankings}
Here we define a few measures of distance between rankings that will be used 
later. First, given two permutations  and  we define the 
{\em dislocation distance} by 

Given a ranking  we define  
so that  if
 and  otherwise. 
Note that using this notation  is obtained from  by flipping each 
entry independently with probability . 
Given  
we denote by 
 
We will write  for  where  is the identity 
permutation and  for . 
Below we will often use the following well known 
claim~\cite{DiaconisGraham:77}.
\begin{claim}
\label{cl:sd}
For any ,

\end{claim}





\section{Sorting a presorted list} \label{sec:presorted}

In this section we prove that if a list is pre-sorted so that each 
element is at most  positions away from its location in the optimal
ordering, then the optimal sorting can be found in time .

\begin{lemma}
\label{lem:presort}
Let , , ,   be  elements 
together with noisy queries . Suppose that 
we are given that there is an optimal ordering 
, such
that  for all .  
Then we can find such an optimal  in time . 
\end{lemma}
In the applications below  will be .  
Note that a brute force search over all possible  would require time 
. Instead we use dynamic programing to reduce the running time. 

\begin{proof}

We use a dynamic programming technique to find an optimal sorting. 
In order to simplify notation we assume that the true ranking  is the 
identity ranking. In other words, . 
Let  be any 
indices, then by the assumption, the elements in the optimally ordered interval 
 satisfy 
 where

Hence selecting the set  involves 
choosing a set of size  that contains the elements of  and is contained in
. This involves selecting  elements from the list (or from a subset of the list)

which has  elements. Thus the number of such 's is bounded by . 

We may assume without loss of generality that  is an exact power of .  
Denote by 
 the interval containing all the elements. Denote by  the left
half of  and by  its right half. Denote by  the left half
of  and so on. In total, we will have  intervals of lengths . 

For each  let  denote the possible () sets of 
the elements . We use dynamic programming to store an
optimal ordering of each such . The total number of 's we will 
have to consider is bounded by . We proceed from  down
to  producing and storing an optimal sort for each possible . For 
 the length of each  is , and the optimal sort 
can be found in  steps. 

Now let . We are trying to find an optimal sort of a given . We do 
this by dividing the optimal sort into two halves  and  and trying to 
sort them separately. We know that  must contain all the elements in
 that come from the interval  and must be contained
in the interval . Thus there are at most  choices for 
the elements of , and the choice of  determines  uniquely. For 
each such choice we look up an optimum solution for  and for  in the
dynamic programming table. 
Among all possible choices of  we pick the best one. This is done 
by recomputing the score  for the joined interval, 
and takes at most  time. Thus the total 
cost will be 

\end{proof}

\section{The Discrepancy between the true order and Optima} 
\label{sec:dist}

The goal of this section is to establish that with high probability any optimum
solution will not be far from the original solution. We first establish that 
the orders are close on average, and then that they are pointwise close to each 
other. 

\subsection{Average proximity} \label{subsec:lin_dist}

We prove that with high probability, the total difference between the original and any optimal ordering is linear in the length of the interval.

We begin by bounding the probability that a specific permutation  
will beat the original ordering. 

\begin{lemma} \label{lem:two_perms}
Suppose that the original ordering is . 
Let  be another permutation. Then the probability that 
 beats the identity permutation is bounded from above by

\end{lemma}

\begin{proof}
In order for  to beat the identity, it needs to beat it in at least 
half of the  pairwise relation where they differ. This proves 
that the probability that it beats the identity is exactly 
. The last inequality follows by a 
Chernoff bound. 
\end{proof}

\begin{lemma} \label{lem:distance_distribution}
The number of permutations  on  satisfying  
is at most

\end{lemma}

Here  is the binary entropy of  defined by 

for small . 

\begin{proof}
Note
that each  can be uniquely specified by the values of 
, that we are given that  is exactly 
. Thus there is an injection of 's with  into 
sequences of  numbers which in absolute values add up to . 
It thus suffices to bound the number of such sequences.
The number of unsigned sequences equals the number of 
ways of placing  balls in  bins, which is equal to .
Signs multiply the possibilities by at most . Hence the total number of 
's with  is bounded by . Summing 
up over the possible values of  we obtain

\end{proof}

\begin{lemma}\label{lem:p0}
Suppose that the true ordering is  and  is large enough. 
Then if  and 

the probability that any ranking  is optimal and 
 is at most  for sufficiently large .
In particular, as , it suffices to take 

\end{lemma}

\begin{proof}
Let  be an ordering with . Then by Claim~\ref{cl:sd} 
we have . Therefore the probability that such 
an ordering will beat the identity is bounded by 
 by Lemma~\ref{lem:two_perms}. 
We now use union bound and Lemma~\ref{lem:distance_distribution} to obtain 
the desired result. 
\end{proof}





\subsection{Pointwise proximity} \label{subsec:log_dev}

In the previous section we have seen that it is unlikely that the {\em average} element 
in the optimal order is more than a constant number of positions away from its original
location. Our next goal is to show that the {\em maximum} dislocation of an element 
is bounded by . As a first step, we show that one ``big" dislocation
is likely to entail many ``big" dislocations.

\begin{lemma}
\label{lem:p1}
Suppose that the true ordering of  is given by the identity ranking, i.e., . Let  
be two indices and . 
Let  be the event that there is an optimum ordering  such 
that  and 

i.e., at most  elements are mapped to the interval  from 
outside the interval  by , where . Then

where .
\end{lemma}

\begin{proof}
The assumption that  is optimal implies in particular that moving the
-th element from the -th position where it is mapped by  back to the 
-th position does not improve the solution. The event  implies that 
among the elements  for  at least  
satisfy . This means that at least 

of the elements  for  must satisfy . 
The probability of this occurring is less than

using Chernoff bounds.
\end{proof}

As a corollary to Lemma \ref{lem:p1} we obtain the following using a simple
union-bound. For the rest of the proof all the 's are base . 

\begin{cor}
\label{cor:p1}
Let 

 then  does not occur for any  with
 with probability .
\end{cor}

Next, we formulate a corollary to Lemma \ref{lem:p0}. 

\begin{cor}
\label{cor:p0}
 Suppose that  is 
the true ordering. Set . For each interval  with
at least  elements consider 
all the sets   which   contain the elements from

and are contained in the interval 

Then with probability  all such sets  do not have an optimal ordering that has 
a total deviation
from the true of more than , with 

 a constant.
\end{cor}


\begin{proof}
There are at most  such intervals. The probability of each interval
not satisfying the conclusion is bounded by Lemma \ref{lem:p0} with 

The last inequality holds because .
By taking a union bound over all the sets we obtain the statement of the corollary. 
\end{proof}












We are now ready to prove the main result on the pointwise distance between an optimal 
ordering and the original. 

\begin{lemma}
\label{lem:pmain}
Assuming that the events from Corollaries \ref{cor:p1} and \ref{cor:p0} hold, 
if follows that for each optimal ordering  and for each , , 
where 

 is a constant.
 In particular, this conclusion holds with probability .
\end{lemma}

\begin{proof}
Assume that the events from both corollaries hold, and let  be an optimal
ordering.  We say that a position  is {\em good} if 
there is no index  such that  is on the other side of  from  and 
. In other words,  is good if there is 
no ''long'' jump over  in . 
In the case when  or  for a long
jump, it is not considered good. An index that is not good is bad. 
An interval  is 
bad if all of its indices are bad. Our goal is to show that there are no bad intervals of length . 
This would prove the lemma, since if there is an  with 
 then there is a bad interval of length at least 
.

Assume, for contradiction, that  is a bad interval 
of length , such that  and  are both good (or lie
beyond the endpoints of ). Denote by  the set of elements 
that is mapped to  by . Denote the  indices in  in their original 
order by , i.e., we have: 
. 

By the goodness of the endpoints of  we have
 
Denote the permutation induced by  on  by  so 
 is equivalent to .
 The permutation 
 is optimal, for otherwise it would have been possible to improve 
by improving .

By Corollary \ref{cor:p0} and Claim \ref{cl:sd}, we have 

In how many switches can
the elements of  participate under ? They participate in switches with other
elements of  to a total of . In addition, they participate in switches
with elements that are not in . These elements must originate at the margins of the 
interval : either in the interval  or the interval .
Thus, each contributes at most  switches with elements of . There are at most  
such elements. Hence the total number of switches between elements in  and in  is at most
. Hence


We assumed that the entire interval  is bad, hence for every position  there is 
an index  such that  and such that  is in the interval
 (or the interval , depending on the order). 
Consider all such 's. By a Vitali covering lemma argument we can 
choose a disjoint collection of them   whose total length is at 
least . The argument proceeds as follows: Order the intervals  
in a decreasing length order (break ties arbitrarily). Go through the list and 
add a  to our collection if it is disjoint from all the currently selected intervals.
We obtain a collection  of disjoint intervals of the for . 
Denote the length of the -th interval by . 
Let  be the ''tripling" of the interval : . 
We claim that the -s cover the entire interval . Let  be a position on the interval 
. Then there is an interval of the form  (or ) that covers
. Choose the longest such interval . If  has been selected to 
our collection then we are done. If not, it means that  intersects a longer interval 
 that has been selected. This means that  is covered by the tripled interval . 
In particular,  is covered by . We conclude that 

Thus . This concludes the covering argument.

We now apply Corollary \ref{cor:p1} to the intervals . We conclude that on an interval
 the contribution of the elements of  that are mapped to  to the sum of 
deviations under  is at least  where .
Thus 

for sufficiently large .
The result contradicts \eqref{eq1} above.
Hence there are no bad intervals of length , which completes the proof. 
\end{proof}



\section{The algorithm} \label{sec:alg}

We are now ready to give an algorithm for computing the optimal ordering with high
probability in polynomial time. Note that Lemma \ref{lem:pmain} holds for any interval
of length  (not just length exactly ). Set . 
Given an input, let  
 be a random set of size . The probability that there is an 
optimal ordering  of  and an index  such that , where

is bounded by  by  Lemma \ref{lem:pmain}. Let 

be a randomly selected chain of sets such that . Then the probability that 
an element of an optimal order of any of the  's deviates from its original 
location by more than     is bounded by 
. We obtain: 

\begin{lemma}
\label{lem:chain}
Let  be a chain of randomly chosen subsets with . 
Denote by  an optimal ordering on . 
Then with probability ,
for each   and for each , , where 
 is a constant.
\end{lemma}

We are now ready to prove the main result.

\begin{theorem}
\label{thm:main}
There is an algorithm that runs in time  where 
 is a constant
that outputs an optimal ordering with probability .
\end{theorem}

\begin{proof} 
First, we choose a random chain of sets  such 
that . Then by Lemma \ref{lem:chain}, with probability ,
 for each optimal order  of  and for each , .
 We will find the orders  iteratively until we reach  which will be 
 an optimal order for our problem. Denote . Suppose that we have 
 computed  and we would like to compute . We first  insert
  into a location that is close to its original location as follows.
Break  into blocks  of length . We claim 
that with probability  we can pinpoint the block  belongs to 
within an error of , thus locating  within  of its original 
location. 

Suppose that  should belong to block . Then by our assumption on 
,  is bigger than any element in  and smaller
than any element in . By comparing  to each element in 
the block and taking majority, we see that the probability of having an incorrect
comparison result with a block  is bounded by . Hence 
the probability that  will not be placed correctly up to an error of two 
blocks is bounded by  using union bound.
 
 


  Hence after inserting  we obtain an ordering of  in which each element is at most 
   positions away from its original location. Hence each element is at most  
  positions away from its optimal location in . Thus, by Lemma \ref{lem:presort} we can obtain 
   in time . The process is then repeated. 
  
  The probability of each stage failing is bounded by . Hence the probability of the 
  algorithm failing assuming the chain   satisfies Lemma \ref{lem:chain}
  is bounded by . 
  Thus the algorithm runs in time  and has a failure probability of 
  at most 
 
\end{proof}

\section{Query Complexity} \label{sec:query}
Here we briefly sketch the proof of Theorem~\ref{thm:sampling}. Recall that the 
theorem states that  
although the running time of the algorithm is a polynomial of  whose degree 
depends on , the query complexity of a variant of the algorithm is 
. Note that there are two types of queries. The first type 
is comparing elements in the dynamic programing, while the second is when inserting new elements. 

\begin{lemma} \label{lem:dynamic_query}
For all  there exists  
such that the total number of comparisons performed in the dynamic 
programing stage is  of the algorithm is at most 
 except with probability .
\end{lemma}

\begin{proof}
Recall that in the dynamic programing stage, each element is compared with 
elements that are at current distance at most  from it where 
. 

Consider a random insertion order of the elements . 
Let  denote the set of elements inserted up to the  insertion. 
Then by standard concentration results it follows that there exists 
 such that for all  it holds that 

and for all  it holds that 

except with probability at most . 
Note that when~(\ref{eq:up_int_query}) and~(\ref{eq:low_int_query}) both hold 
the number of different queries used in the dynamic programing while inserting the 
elements in  is at most . 

Repeating the argument above for the insertions performed from 
 to , from  to  etc. we obtain that the total 
number of queries used is bounded by:

except with probability . This concludes the proof. 
\end{proof}

Next we show that there is implementation of insertion that requires only 
 comparisons per insertion.
\begin{lemma} \label{lem:insertion_query}
For all  and  there exists a  
and  
such that except with probability  it is 
 possible to perform the insertion in the proof of Theorem \ref{thm:main} 
so that each element is inserted using at most  
comparisons,  time
and the element 
is placed a distance of at most  from its optimal location. 
\end{lemma}


\begin{proof}
Bellow we assume (as in the proof of Theorem \ref{thm:main}) that there exists 
 
such that 
at all stages 
of the insertion and for each item, the distance between the location of the 
item in the original order and the optimal order is at most . 
This will result in an error with probability at most . 
Let  be a constant such that 


Let  be chosen so that 

Let .  

We now describe an insertion step. 
Let  denote a currently optimally sorted set. We will partition  into 
consecutive intervals of length between  and  denoted
. We will use the notation  
for the sub-interval of  defined by 
. 
We say that a newly inserted element  {\em belongs} 
to one of the interval  
if one of the two closest elements to it in the original order belongs to 
. Note that  can belong to at most two intervals. An element in  belongs to  iff it is one of the elements in .
Note furthermore 
that if  belongs to the interval  then its optimal insertion location 
is determined up to . Similarly, if we know 
it belongs to one of two intervals then its optimal insertion location 
is determined up to , therefore we can take
.

Note that by the choice of  
we may assume that all elements belonging to  are 
smaller than all elements of  if  in the true order. 
Similarly, all elements belonging to  are larger than all elements of 
 if .  
We define formally the interval  to be an interval of elements that are smaller than all the items and the interval  
to be an interval of elements that is bigger than all items. 



We construct a binary search tree on the set  labeled by 
sub-intervals of  such that the root is labeled by  and 
if a node is labeled by an interval  with 
 then its two children 
are labeled by  and , where  is chosen so that the 
length of the two intervals is the same up to . Note that the two 
sub-interval overlap at . This branching process terminates at intervals 
of the form . Each such node will have a path of descendants of length 
 all labeled by . 

We will use a variant of binary insertion closely related to 
the algorithm described in Section 3 of~\cite{FPRU:90}. The algorithm 
will run for  steps starting at the root of the tree. 
At each step the algorithm will proceed from a node of the tree to either 
one of the two children of the node or to the parent of that node. 

Suppose that the algorithm is at the node labeled by  and 
. 
The algorithm will first take  elements from  that have not been 
explored before and will check that the current item is greater than the 
majority of them. Similarly, it will make a comparison with  elements from 
. If either test fails it would backtrack to the 
parent of the current node. Note that if the test fails then it is the case
that the element does not belong to  except with probability 
. 

Otherwise, let  and  denote the two children of 
. The algorithm will now perform a majority test against  elements from  according to which it would choose one of the 
two sub-interval  or . Note again that a correct 
sub-interval is chosen except with probability at most  (note that 
in this case there may be two ``correct'' intervals). 

In the case where  we perform only the first test. If it fails 
we move to the parent of the node. It it succeeds, we move to the single child. 
Again, note that we will move toward the leaf if the interval is correct 
with probability at least . Similarly, we will move away from the leaf 
if the interval is incorrect with probability at least . 

Overall, the analysis shows that at each step we move toward a leaf including 
the correct interval with probability at least . From~\eqref{eq:rw_bias} it follows that with probability at least  after  steps 
the label of the current node will be  where the inserted element 
belongs to either  or . Thus the total number 
of queries is bounded by  and we can take
. This concluded the proof. 
\end{proof}


\newpage 

\bibliographystyle{abbrv}
\bibliography{all}

\end{document}
