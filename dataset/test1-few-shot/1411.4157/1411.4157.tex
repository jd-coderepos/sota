\documentclass{llncs}
\usepackage[english]{babel}
\usepackage{etex}


\usepackage[english]{babel}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage[all]{xy}



\pagestyle{plain}

\title{A Polynomial Time Algorithm for Deciding Branching Bisimilarity on Totally Normed 
}


\author{Chaodong He}
\institute{BASICS, Department of Computer Science, Shanghai Jiao Tong University}
\date{January 7, 2014}

\begin{document}

\maketitle

\begin{abstract}
Strong bisimilarity on normed BPA is polynomial-time decidable, while weak bisimilarity on totally normed BPA is -hard.  It is natural to ask where the computational complexity of branching bisimilarity on totally normed BPA lies.  This paper confirms that this problem is polynomial-time decidable.  To our knowledge, in the presence of silent
transitions, this is the first bisimilarity checking algorithm on infinite state systems which runs in polynomial time.  This result spots an instance in which branching
bisimilarity and weak bisimilarity are both decidable but lie in different complexity classes (unless ), which is not known before.

The algorithm takes the partition refinement approach and the final implementation can be thought of as a generalization of the previous algorithm of Czerwi\'{n}ski and Lasota~\cite{DBLP:conf/fsttcs/CzerwinskiL10,CzerwinskiPhD}.  However, unexpectedly, the correctness of the algorithm cannot be directly generalized from previous works, and the correctness  proof turns out to be subtle.  The proof depends on the existence of a carefully defined refinement operation fitted for our algorithm and the proposal of elaborately developed techniques, which are quite different from previous works.
\end{abstract}

\section{Introduction}

Basic process algebra (BPA)~\cite{Baeten:1991:PA:103272} is a fundamental model of infinite state systems, with its famous counterpart in the theory of formal languages: context free grammars in Greibach normal forms, which generate the entire context free languages.
In 1987, Baeten, Bergstra and Klop~\cite{BaetenBergstraKlop1987,DBLP:journals/jacm/BaetenBK93} proved a
surprising result that strong bisimilarity on normed BPA is decidable.  This result is in sharp contrast to the classical fact that language equivalence is undecidable for context free grammar~\cite{Hopcroft:1990:IAT:574901}.
After this remarkable discovery, decidability and complexity issues of bisimilarity checking on infinite state systems have been intensively investigated. See~\cite{DBLP:conf/concur/JancarM99,Burkart00verificationon,DBLP:journals/iandc/MollerSS04,Srba2004,Kucera2006}
for a number of surveys.

As regards to the strong bisimilarity checking on normed BPA,
H\"uttel and Stirling~\cite{DBLP:conf/lics/HuttelS91} improved
the result of Baeten, Bergstra and Klop using a more simplified proof by relating
the strong bisimilarity of two normed BPA processes to the existence of a successful
tableau system.  Later, Huynh and Tian~\cite{DBLP:journals/tcs/HuynhT94} showed
that the problem is in , the second level of the polynomial hierarchy.
Before long, another significant discovery was made by Hirshfeld,
Jerrum and Moller~\cite{DBLP:journals/tcs/HirshfeldJM96} who showed that the problem can even be decided in  polynomial
time, with the complexity .
The running time was later improved~\cite{DBLP:conf/mfcs/LasotaR06,DBLP:conf/fsttcs/CzerwinskiL10}. All these algorithms take the approach of partition refinement, relying on the unique decomposition property and some
efficient way of equality checking on compressed long strings.
It deserves special mention that Czerwi\'{n}ski and Lasota~\cite{DBLP:conf/fsttcs/CzerwinskiL10} create a different refinement scheme. This refinement scheme was previously used in developing an polynomial-time algorithm of checking strong bisimilarity on normed basic parallel processes (normed )~\cite{DBLP:journals/mscs/HirshfeldJM96}. In this way  Czerwi\'{n}ski and Lasota improve the running time to .  Hitherto, the best algorithm was reported in~\cite{CzerwinskiPhD}, whose running time is  .

In the presence of silent actions the picture is less clear.  Even the decidability for weak bisimilarity is still open.  A remarkable discovery is made by Fu~\cite{DBLP:conf/icalp/Fu13} recently that branching bisimilarity~\cite{GlabbeekW96}, a standard refined alternative of weak bisimilarity, is decidable on normed BPA. Very recently, Czerwi\'{n}ski and Jan\v{c}ar confirm this problem to be in ~\cite{DBLP:journals/corr/CzerwinskiJ14}.  The current best lowerbound for weak bisimilarity  is the -hardness established by Mayr~\cite{Mayr2005}, whose proof can be slightly modified to show
the -hardness for branching bisimilarity as well.



In retrospect one cannot help thinking that more attention should
have been paid to the branching bisimilarity. Going back to the original motivation
to equivalence checking, one would agree that a specification  normally
contains no silent actions because silent actions are about how-to-do. It follows
that  is weakly bisimilar to an implementation  if and only if  is branching
bisimilar to  (Theorem~5.8.18 in~\cite{Baeten:1991:PA:103272}).  In addition, in majority of practical examples, the branching
bisimilarity and the weak bisimilarity coincide.  What these observations tell us is that as far as verification is
concerned the branching bisimilarity ought to play a bigger role than the weak
bisimilarity, especially in the situations where branching bismilarity is easily decided.



One major difficulty of checking weak or branching bisimilarity on normed  stems from the lack of nice structural properties such as unique decomposition property.  By forcing the final action of every process to be observable, we have an important subset of normed , called totally normed , in which unique decomposition property still holds for branching bisimilarity.  The bisimilarity checking on totally normed  also has a long history.
In 1991, H\"uttel~\cite{DBLP:conf/cav/Huttel91} repeated the tableau construction developed in~\cite{DBLP:conf/lics/HuttelS91} for branching
bisimilarity on totally normed BPA. Although H\"uttel's construction is not sound for weak bisimilarity, the relevant decidability can also be established~\cite{DBLP:journals/entcs/Hirshfeld96}.  For the lower bound, -hardness is established by
St\v{r}\'{i}brn\'{a}~\cite{DBLP:journals/entcs/Stribrna98} for weak bisimilarity via a reduction from the knapsack problem.
By inspecting St\v{r}\'{i}brn\'{a}'s proof, we are aware that the -hardness still holds for any other bisimilarity, such as delay bisimilarity,  -bisimilarity, and even  quasi-branching bisimilarity~\cite{GlabbeekW96}, except for  branching bisimilarity.
The requirement of branching bisimilarity that change-of-state silent actions must be explicitly bisimulated makes it impossible to realize nondeterminism by designing some gadgets via a bisimulation game.   These crucial observations inspire us to rethink the possibility of designing more efficient algorithm for the problem of checking branching bisimilarity on totally normed .


The paper provides a polynomial time algorithm for checking branching bisimilarity on totally normed .  Therefore an instance is spotted that branching bisimilarity and weak bisimilarity are both decidable but lie in different complexity classes.

For brevity, in the rest of this paper, `branching bisimilarity' will usually be referred to as `bisimilarity'.   We avoid using the term `strong bisimilarity', since the strong bisimilarity can be interpreted as the bisimilarity for `realtime' processes. A realtime process is a process which can perform no silent action.


The algorithm developed in this paper takes a similar partition refinement approach and the framework adopted in~\cite{DBLP:conf/fsttcs/CzerwinskiL10,CzerwinskiPhD}, which was designed to decide bisimilarity for realtime normed . This algorithm is called CL algorithm in this paper.  The final efficient implementation of our algorithm is a generalized version of CL algorithm in the sense that, for realtime systems, our algorithm and CL algorithm are essentially the same.

Our algorithm heavily relies on the technique of dynamic programming, which makes our implementation has the same computational complexity as CL algorithm.   Although our algorithm seems very similar to the previous one,  the technical details, including the definition of expansion and refinement operation, the theoretical development of its correctness are quite difficult than the previous CL algorithm.


Without doubt, the consecutive silent transitions in the definition of branching bisimilarity cause severe problems in two aspects: the correctness and the efficiency. Note that the totally normedness guarantees that the number of consecutive silent actions are bounded by the number of constants. It is not hard to use this observation, together with the game theoretical view of branching bisimilarity, to design an algorithm which runs in polynomial space.   However, the consecutive silent actions, which cause nondeterminism, did make checking branching bisimulation property take exponential time if the naive way was taken.  The only way to overcome this difficulty is a proper usage of the technique of dynamic programming.  When consecutive silent actions are eliminated by means of dynamic programming, we have a severe problem: why is the resulting algorithm still correct?

In the situation of CL algorithm for realtime normed , there is a pre-defined refinement operation, . In that situation, we had a canonical definition of expansion and a canonical definition of relative decreasing bisimilarity (in our terminology). The final refinement operation  was defined as the decreasing bisimilarity wrt.~the expansion of .  The refined equivalence relation was then constructed by a greedy algorithm, in each step of which two memberships were efficiently tested.  Therefore, the correctness of CL algorithm was comparatively obvious.

Unfortunately it is unlikely, if not impossible, to generate the above proof structure for CL algorithm to our algorithm, because there is no clear way to define the expansion relation like that in CL algorithm. Note that the expansion relation should be both correct and efficient. We had several aborted attempts before finally we decided to take some other ways.

The correctness of CL algorithm depends on a clearly defined refinement operation which relies on two steps of operations: the expansion operation and the relative decreasing bisimilarity.  Our crucial insight is that, there is no need to separate these two steps of operations.    The main technical line is briefly outlined below.  It takes several stages:
\begin{itemize}
\item
 At first, for realtime systems, we define  the refinement operations by a way of combining the two steps of operation which was taken in CL algorithm into a cohesive whole.  In this way, we have noticed that we defines exactly the same refinement operation as that in CL algorithm.



\item
Then,
the refinement operation defined in the above way  is smoothly generated for the style of branching bisimilarity. In this stage, our attention is centred on the property of the refined relation. The efficiency is never cared about.  We prove that the refinement operation preserves  congruence and the unique decomposition property.

\item
Then a characterization theorem is established for the refined congruence.  In this characterization, the consecutive silent actions are completely eliminated. Thus the problem of efficiency is mainly solved. Using this characterization, the correctness proof for realtime systems can be obtained.  But for systems with silent actions, it is not enough.


\item
Finally, the proof is finished by developing a simpler characterization  which corresponds to our algorithm directly. In this stage,  a special property of branching bisimilarity for processes in prime decomposition turns out to be quite useful.
\end{itemize}

The rest of the paper is organized as follows. Section~\ref{sec:Preliminaries} lays down the preliminaries. Section~\ref{sec:finite_representations} focuses on the unique decomposition property for branching bisimilarity on totally normed BPA. Then we describe our algorithm in Section~\ref{sec:naive-algorithm}. The suitable definition of refinement steps are discussed in Section~\ref{sec:refinement_steps},  and the correctness proof are provided in Section~\ref{sec:correctness}.  Finally, Section~\ref{sec:remark} gives additional remarks.




\section{Preliminaries}\label{sec:Preliminaries}
\subsubsection{Basic Process Algebra}
A {\em basic process algebra} () system is a triple , where  is a finite set of process constants,  is a finite set of actions, and  is a finite set of transition rules.
The {\em processes}, ranged over by ,  are generated by the following grammar:

The syntactic equality is denoted by .
We assume that the sequential composition  is associative up to  and . Sometimes  is shortened as .  The set of processes is exactly , the strings over .
There can be a special symbol  in  for silent transition.  Typically,  is used to denote actions, while  are used to denote visible (i.e. non-silent) actions.
The transition rules in  are of the form .
The following labelled transition rules define the operational semantics of the processes.

The operational semantics is structural, meaning that  whenever .
We write  for the reflexive transitive closure of , and  for  if  and for  otherwise.


A process  is {\em normed} if  for some . A process  is {\em totally normed} if it is normed, and moreover,  whenever .
A  definition   is (totally) normed if all processes defined in it are (totally) normed. We write (t)(n)BPA for the (totally)(normed) basic process algebra model. In other words, a  system is a  system in which rules of the form  are forbidden.

We call a BPA system {\em realtime} if . That is to say, a realtime system can not perform silent actions.
Clearly, realtime totally normed BPA is exactly realtime normed BPA.

\subsubsection{Bisimulations and Bisimilarities}
In the presence of silent actions two well known process equalities are the branching bisimilarity~\cite{GlabbeekW96} and the weak bisimilarity~\cite{Milner1989}.


\begin{definition}\label{def:beq}
Let  be a relation on processes.  is a {\em branching bisimulation},  if the following hold whenever :
\begin{enumerate}
\item
If , then either
 \begin{enumerate}
  \item
   and ; or

 \item
     and  and  for some .
 \end{enumerate}

\item
If  , then either
 \begin{enumerate}
 \item
     and
    ; or

 \item
  and  and  for some .
    \end{enumerate}
\end{enumerate}
The {\em branching bisimilarity}   is the largest branching bisimulation.
\end{definition}

\begin{definition}
A relation  is a {\em weak bisimulation} if the following are valid:
\begin{enumerate}
\item
Whenever  and , then  and  for some .
\item
Whenever  and , then  and  for some .
\end{enumerate}
The {\em weak bisimilarity}  is the largest weak bisimulation.
\end{definition}

Both  and  are congruence relations for (t)nBPA. We remark that transitivity of  is not straightforward according to Definition~\ref{def:beq}, because the branching bisimulation  defined in Definition~\ref{def:beq} need not be transitive~\cite{DBLP:journals/ipl/Basten96}.  To solve this problem, van Glabbeek and Weijland~\cite{GlabbeekW96} introduce a slightly different notion called {\em semi-branching bisimulation}.

\begin{definition}\label{def:semi-beq}
Let  be a relation on processes.  is a {\em semi-branching bisimulation}  if the following hold whenever :
\begin{enumerate}
\item
If , then either
 \begin{enumerate}
  \item
   and  for some  such that  and ; or

 \item
     and  and  for some .
 \end{enumerate}

\item
If  , then either
 \begin{enumerate}
 \item
     and
      for some  such that  and ; or

 \item
  and  and  for some .
 \end{enumerate}
\end{enumerate}
\end{definition}

Then it is easy to establish the following facts:
\begin{enumerate}
\item
A branching bisimulation is a semi-branching bisimulation.

\item
A semi-branching bisimulation is transitive.

\item
The largest semi-branching bisimulation is an equivalence.

\item
The largest semi-branching bisimulation is a branching bisimulation.
\end{enumerate}

Now the largest semi-branching bisimulation is the same as , the largest branching bisimulation.



If the involved system is realtime,  then the branching bisimilarity and the weak bisimilarity are coincident. They are called the {\em strong bisimilarity}  and are denoted by  in literature.
In this paper, branching bisimilarity is often abbreviated as {\em bisimilarity}. If the system is realtime, we also use the term bisimilarity to indicate strong bisimilarity.  However, we tend to use the term `branching bisimilarity' in the situation of discussing on its relationship with weak bisimilarity.


The following lemma, first noticed by van Glabbeek and Weijland~\cite{GlabbeekW96}, plays a fundamental role in the study of bisimilarity.
\begin{lemma}\label{computation-lemma}
If  then .
\end{lemma}

Let  be a process equivalence.
A silent action  is {\em state-preserving} with regards to  if ;
it is {\em change-of-state} with regards to  if .
Branching bisimilarity strictly refines weak bisimilarity in the sense that only state-preserving silent actions can be ignored; a change-of-state must be explicitly bisimulated.
Suppose that  and  is matched by the transition sequence .
By definition one has .
It follows from Lemma~\ref{computation-lemma} that , meaning that all silent actions in  are necessarily state-preserving.  This property fails for the weak bisimilarity as the following example demonstrates.
\begin{example}\label{example}
Consider the  system whose rules are defined by

One has .
However  since .
\end{example}




\subsubsection{Norm}
Given an tnBPA system .
We relate a natural number , the {\em norm} of , to every constant , defined as the least  such that .  Silent actions contribute zero to norm.    is extended to processes by taking  and .

\begin{lemma}\label{lem:normal_one}
In a  system,  if and only if .
\end{lemma}

A transition  is {\em decreasing}, denoted by  if either
 and , or
 and .  The notion of
decreasing transitions formalizes the intuition that a transition can be extended to a path which witnesses the norm of .




\subsubsection{Standard Input}

For technical convenience, we require the input  system  to be {\em standard}, which have the following two additional properties:
\begin{enumerate}
\item
The constants in  are ordered by non-decreasing norm, that is:


\item
 Let  be the set  for . In particular,  and . Assume , we need the property .   This property does not hold in general because of the existence of loops like .  In this case we have  by Lemma~\ref{computation-lemma}, and we can transform the system by contracting  and  into one constant (removing  and substituting all occurrences of  in  by ) and eliminating the loop rules.   All loops can be eliminated in this way.  (By totally normedness,  is impossible.)  Afterwards, we specify a partial order  such that  if and only if either  or . Then the order of constants are chosen to be any total order which extends . These works can be done by computing the `dependency graph' and then calling an algorithm for topological sort.
\end{enumerate}

The size of a  system  is denoted by . A procedure is said to be {\em efficient} if it runs in polynomial time.  The above discussion confirms that any  system can be efficiently transformed to a standard one with no size growing.

\begin{lemma}\label{lem:normal_two}
For every  system , there is a standard  system  computable in at most  time, in which  and .
\end{lemma}
From now on, the input  system is supposed to be standard, and is fixed as  where .  We will invariantly use  to denote the size of , and  to denote the size of the related  system.

The problem is formally defined as follows:
\begin {center}\small
 \begin{tabular}{|rp{9.5cm}|}\hline
        Problem:   \quad  &  \textsc{Branching Bisimilarity on tnBPA} \\
        Instance:  \quad  &  A standard tnBPA system , and .  \\
        Question:  \quad  &  ? \\  \hline
    \end{tabular}
\end {center}



We restate the important property for standard systems as the following lemma.
\begin{lemma}\label{lem:decreasing_transition}
Assume , we have .
\end{lemma}


\subsubsection{Other Conventions}

We will always use notation  to denote an equivalence/congruence relation on .  An equivalence/congruence relation  is {\em norm-preserving} if   whenever . In this paper, all the equivalence/congruence relations are supposed to be norm-preserving. This fact is not always explicitly stated.






\section{Finite Representations}\label{sec:finite_representations}

In this section, we propose  a convenient way of representing bisimilarity and the approximating congruences \footnote{The proofs in this section is a generalization of the corresponding work for realtime normed BPA, say~\cite{DBLP:journals/tcs/HirshfeldJM96}. The readers familiar with these former works can only skim this part.}.

From the algebraic view, the set of processes of  is exactly the free monoid generated by .  The question is how to represent a congruence relation on .
We will show that the bisimilarity  is a very special congruence. Not only is it finitely generated, but it enjoys a highly structured property called {\em unique decomposition property}.


\subsection{Unique Decomposition Property of }

Unique decomposition property plays a central role in all the algorithms for bisimilarity checking on realtime . This important property also holds for bisimilarity on .

Recall that a congruence  is {\em norm-preserving} if  whenever . The following lemma is a direct consequence of Definition~\ref{def:beq}.
\begin{lemma}\label{lem:norm-preserving}
 is a norm-preserving congruence.
\end{lemma}
Let  be an arbitrary norm-preserving congruence.
Intuitively, a constant process  is a composite if  for some . In this case we also have  from Lemma~\ref{lem:normal_one}.
For technical convenience we will define  to be a {\em composite} modulo  if  for some . Otherwise,  is called a {\em prime} modulo .

Let  be the set of primes modulo .  By Lemma~\ref{lem:norm-preserving} and the well-foundedness of natural numbers, every  has a {\em prime decomposition}  such that .   We say that  has unique decomposition property, or simply  is {\em decompositional} if every process has exactly one prime decomposition.


It is the time to establish the unique decomposition property of . The following Lemma~\ref{lem:right-cancellation} and Theorem~\ref{thm:unique-decomposition} is standard, as is in the case of bisimilarity for realtime ~\cite{DBLP:journals/tcs/HirshfeldJM96}.
The {\em right cancellation} property is established first.

\begin{lemma}[Right Cancellation]\label{lem:right-cancellation}
 entails .
\end{lemma}

\begin{proof}

is a bisimulation.  \qed
\end{proof}

\begin{theorem}[Unique Decomposition Property of ]\label{thm:unique-decomposition}  is decompositional.
Let  and   be two irreducible decompositions such that . Then,   and  for every .
\end{theorem}

\begin{proof}
Assume on the contrary that  and   be two different irreducible decompositions with the least norm such that

Suppose that

These actions must be bisimulated (matched) by

for some  such that .
Since the norm of  and  is strictly decremented, we have  from the induction hypothesis.  Now by right cancellation lemma, . This contradicts with the minimum norm assumption.  \qed
\end{proof}
On the other direction,  right or left cancellation property is an implication of unique decomposition property.
\begin{lemma}\label{lemma:udp_to_cancellation}
Let  be decompositional. Then  (or ) implies .
\end{lemma}

\begin{remark}
The proof of Lemma~\ref{lem:right-cancellation} and Theorem~\ref{thm:unique-decomposition} is standard~\cite{BaetenBergstraKlop1987,DBLP:journals/tcs/HirshfeldJM96}.
Although the proof is fairly straightforward, it heavily depends  on {\em branching} bisimilarity and {\em totally} normedness. For example in the above proof when actions coming from  in (\ref{eqn:proof_UDP1}) are matched by the actions in (\ref{eqn:proof_UDP2}),  the crucial point is that  is never used.  This cannot be proved in the case of weak bisimilarity, or in the case without totally normedness.  We will have the following two counterexamples if branching bisimilarity is replaced by {\em weak} bisimilarity, or if the condition of totally normedness is abandoned.
\end{remark}

\begin{example}\label{example:weak_bisimilarity_decom}
This counterexample is borrowed from~\cite{DBLP:conf/cav/Huttel91}. Consider the tnBPA system , with

Clearly,  but . Right cancellation property does not hold, neither does the unique decomposition property hold.
\end{example}

\begin{example}
Consider the nBPA system , with

Clearly, .  Unique decomposition property fails in this example merely because the existence of {\em idempotent} processes.
\end{example}


\subsection{Decomposition Bases}\label{subset:decomposition_base}

A decompositional congruence over  can be represented by a decomposition base.  A {\em decomposition base}  is a pair , in which  specifies the set of primes, and  is a finite set of equations of the form  for every  and .  The equation  realizes the fact that every composite  is equal to a string of primes  which is the {\em prime decomposition} of .  The congruence relation generated by  is denoted by .



The {\em prime decomposition} of a process  with regard to  is denoted by .
Formally, we set  when , and  wherever the equation  is in .
The domain of  is extended to  naturally by setting  and  .

The following lemma makes checking  fairly easy by only computing the prime decompositions of  and .



\begin{lemma}\label{lem:dcmp}
 if and only if .
\end{lemma}

In the rest of the paper, every congruence  generated by a decomposition base  is assumed to be norm-preserving.  Thus we must have  if the equation  is in .



The following lemma formalizes the important observation that prime constants  do not have state-preserving silent actions.
\begin{lemma}\label{lem:tau_prime}
Let  be a decomposition base, and . Assume , we have .
\end{lemma}
\begin{proof}
According to Lemma~\ref{lem:decreasing_transition}, .

\begin{enumerate}
\item
If  and .  In this case, if we have , then according to the fact that  being prime and Lemma~\ref{lem:dcmp},  . This is a contradiction.

\item
If  and , we cannot have  because  is norm preserving.  \qed
\end{enumerate}



\end{proof}
The above property can be lifted from constants to processes, regarding Lemma~\ref{lemma:udp_to_cancellation}.
\begin{lemma}\label{lem:tau_prime_string}
Let  be a decomposition base, and . Assume , we have .
\end{lemma}

\begin{remark}
Algebraically, a decomposition base  can be understood as a {\em finite presentation} of a monoid. In fact,  specifies the quotient monoid . Moreover,  the unique decomposition property says that the quotient monoid  is a free monoid.
From computational point of view,  is a {\em string rewriting system}. Rewriting rules are exact the equations in  from left to right.  Strings in {\em normal forms} are exact , the free monoid generated by .  All composites can be reduced to its prime decompositions. Any  has a normal form.  Church-Rosser property is guaranteed by the unique decomposition property, which makes checking  fairly easy by merely rewriting  and  to their normal forms.
\end{remark}


\section{Description of the Algorithm}\label{sec:naive-algorithm}
This section serves as the description of our algorithm.
The algorithm takes the partition refinement approach.  It is a generalized version of the one in~\cite{DBLP:conf/fsttcs/CzerwinskiL10}, which we call CL algorithm. However, unlike the original CL algorithm, the correctness of our algorithm is not obvious and is much more difficult to prove.  This is the reason why we describe the algorithm  before we prove its correctness.  During the description, we also show some properties and requirements which make the algorithm work. A few properties are not proved until Section~\ref{sec:refinement_steps}.

\subsection{Partition Refinements with Decomposition Bases}
In order to decide whether , we start with
an initial congruence relation ,
and iteratively refine it. The refinement operation will be denoted by . By taking , we have a sequence of congruence relations

which satisfy

The correctness of the refinement operation adopted in this paper depends on the following requirements:
\begin{enumerate}
\item
.

\item
.

\item
If , then .
\end{enumerate}
Once the sequence becomes stable, say , we have .

\begin{remark}
The refinement operation taken in this paper leads to a monotonic sequence .  Namely,

This property is not necessary in a general framework of refinement. One alternative is to replace the third requirement above by the following two:
\begin{enumerate}
\item[3'.]
 is monotone.  whenever .


\item[4'.]
If , then .
\end{enumerate}
\end{remark}

In the algorithm,  the congruences  and 's are all represented by decomposition bases. That is, all the intermediate  must be decompositional congruences. In the following, we will develop an implementation of the refinement steps in polynomial time.




On the whole, the algorithm is an iteration:
\begin{center}
\small
 \begin{tabular}{|p{12cm}|}\hline\vspace{-2ex}
\begin{itemize}
\item[1.]
Compute the initial base  and set .

\item[2.]
Compute the base  from .

\item[3.]
If   equals  then halt and return .

\item[4.]
Assign new base  to  and go to step 2. \vspace{-1ex}
\end{itemize} \\
        \hline
\end{tabular}
\end{center}


Apparently, the algorithm relies on the base  of the initial congruence  and the refinement step, computing  from .

\subsection{Outline of the Algorithm}\label{subsec:naive-algorithm}

The framework of the algorithm is described as Fig.~\ref{Efficient_Algorithm}.









\subsubsection{Initial Congruence}

The base  of the initial congruence  is set as:
\begin{itemize}
\item
,

\item
 contains  for every .
\end{itemize}

For  , we have the following properties.
\begin{lemma}
   if and only if .
\end{lemma}

\begin{lemma}
\begin{enumerate}
\item
.

\item
 is a norm-preserving and decompositional congruence.
\end{enumerate}
\end{lemma}



\subsubsection{Properties of Refinement Steps}
In order to understand the framework of the algorithm,
We need to investigate the relationship between   and   in step~2.  Later from the algorithm, we will confirm that . Under this condition,  we have the following key observation.
\begin{lemma}\label{lem:PP}
Let   and   be two decomposition bases.
\begin{enumerate}
\item
If  , then  .

\item
If , then .
\end{enumerate}
\end{lemma}

\begin{proof}
\begin{enumerate}
\item
Suppose , we show .  Since , there is an equation  in   for some , which means . Because  , we have , which means that  is not a prime modulo . That is, .

\item
Suppose that . Then there is some  such that .  We have  and   Since  , we have , thus
 .
Since
 and  are both in , we have  , a contradiction. \qed
\end{enumerate}

\end{proof}


According to Lemma~\ref{lem:PP}, we call constants in  {\em old primes} and constants in 
{\em new primes}.  During the iterative procedure of refinement, once a constant becomes prime, it is a prime thereafter.
If at certain step of iteration there is no new prime to add, the algorithm terminates.  Thus  we have the following property.
\begin{proposition}
There can be at most  steps of iteration in the algorithm.
\end{proposition}
This confirms the termination of the algorithm and provides an implementation of the step 3 by checking if there are new primes. The remaining thing is to study the
implementation of step~2.


\begin{figure}[tbp]

\begin{center}
 \begin{tabular}{|p{12cm}|}\hline\vspace{0ex}

\textbf{\normalsize  Framework of the algorithm:}

\begin{enumerate}
\item
Initialize ;

\item
;

\item
\textbf{repeat}

\item
\qquad
;
;
;

\item
\qquad \textbf{for} each  \textbf{do}

\item
\qquad\qquad
;

\item
\qquad\qquad ;

\item
\qquad\qquad ;

\item
\qquad\qquad  \textbf{for} each  \textbf{do}

\item
\qquad\qquad\qquad
\textbf{if} 
\textbf{then}


\item
\qquad\qquad\qquad\qquad
;

\item
\qquad\qquad\qquad\qquad
;

\item
\qquad\qquad\qquad
\textbf{end if}

\item
\qquad\qquad
\textbf{end for}

\item
\qquad\qquad
\textbf{if}  \textbf{then}

\item
\qquad\qquad\qquad
;

\item
\qquad\qquad
\textbf{end if}

\item
\qquad
\textbf{end for}

\item
\textbf{until}  \vspace{-1ex}
\end{enumerate}\\
        \hline
    \end{tabular}\vspace{-1ex}
\end{center}
\caption{Framework of Efficient Algorithm}\label{Efficient_Algorithm}
\end{figure}



\subsubsection{Computing   from }

Computation of  proceeds as follows. First we assign  and . Then we add
appropriate constants to  and appropriate equations to . For every  with , we check whether there exists  such that . If not, we
add  to , otherwise we add the appropriate equation  to . We emphasize that at the time  is treated, we have already known whether  and  for every .

The efficient computation of    from  relies on the following three aspects:
\begin{enumerate}
\item
The candidates  for testing  must be `small'.

\item
We need an correct and efficient way of deciding whether  can be put into , i.e.~.

\item
We need an efficient representation and manipulation on strings.
\end{enumerate}




The representation and operations on long strings can be implemented in a systematic way  and will be discussed shortly in Section~\ref{subsec:longstring}.    For the moment, we suppose that all the operations on strings  appears in the algorithm are polynomial time computable.



\subsection{Small Set of Candidates}\label{subsec:candidates}






Now we confirm that, for every , there is a small number of 's which are required to determine whether .  In the case of realtime , this is a significant discovery in CL algorithm, for it greatly reduces the expense of the algorithm.  The same way is taken here, but the rationality will be confirmed later.

Let  be a decomposition base.  We say that prime constant  is the
{\em leftmost prime factor} of  wrt.~, denoted by , if  for some . Clearly,  is unique.

Now fix one decreasing  transition rule  ( is allowed. ) for every .   We use  to denote the suffix of string  with norm . Note that  is undefined unless  has such a suffix with norm .
\begin{proposition}\label{prop:lpf1}
Let  be a decomposition base such that  is a decreasing branching bisimulation (Definition~\ref{def:dec_bisimulation_expansion}, Section~\ref{subsec:Expansion_in_general}).  If , then

\end{proposition}

\begin{proof}
From , we have  for some  satisfying . Knowing  is a decreasing branching bisimulation, we consider the transition . There are two cases:
\begin{itemize}
\item
 and . In this case, let  and we have .

\item
 or . In this case, we have  such that .
\end{itemize}
In either case, we have  for some . According to the fact that  is decompositional, we get , and consequently , hence . Recall that , we get . \qed
\end{proof}

Assume that  .
Comparing  with , there are two possibilities:  or . If , the following property confirms that  must be a new prime.
\begin{proposition}\label{prop:lpf2}
Assume that  .  Let  and  . If , then  and .
\end{proposition}

\begin{proof}
Assume on the contrary that , we have , which violates unique decomposition property of . \qed
\end{proof}



Now we can illustrate the algorithm framework in Fig.~\ref{Efficient_Algorithm}.  The \textbf{repeat} block at line~3 realize the procedure of iteration. At every iteration,  is updated to . During an iteration, every constant  which is current composite is treated in the fixed index order via the outer \textbf{for} block at line~5. Note that, when  is treated,  can be determined. Then the inner \textbf{for} block at line~9 is used for discovering a new decomposition of  for  by determining the leftmost prime factor  of . By Proposition~\ref{prop:lpf2},  can be unchanged (in the case ), or be a new prime less than  (in the case  and  ), or be  itself (in the case no  is found in the inner \textbf{for} block at line~9).   In the last case, variable  which is set true at line~7 remains being  and  is added to the set  of new primes (line~15).  The operation  returns index  such that .
Using Proposition~\ref{prop:lpf1} and Proposition~\ref{prop:lpf2}, the set of candidates can be confined into the form of
,
Note that, in the inner \textbf{for} block,  procedure  is used  to check whether  is the leftmost prime factor of  modulo . In fact, it tests whether


In the rest part of this paper,  the right hand side of Equation~(\ref{eqn:testing_Xi_alpha}) is denoted by . We remark that .  Our goal is to find an efficient way to check whether .

\begin{remark}
The small number of candidates of  relies on Proposition~\ref{prop:lpf1}, which requires that  be a decreasing bisimulation. The definition of decreasing bisimulation will be introduced in Section~\ref{subsec:Expansion_in_general}. According to the refinement operation defined in Section~\ref{sec:refinement_steps},   is assured to be a decreasing bisimulation.
\end{remark}

\subsection{Efficient Way of Testing }

The algorithm framework described in Fig.~\ref{Efficient_Algorithm} tells us an efficient way for the implementation of partition refinement on the unique decomposition congruences.  Up to now, we have not discuss how the refinement operation is and how shall we realize it efficiently. That is, how  at line~10 is implemented. Now we present the details.  That is , we present an efficient way to check whether . In this way, we define  from  via the algorithm.

The whole testing is described in Fig.~\ref{Checking_EXP}. In later sections, we have further discussions on this implementation.  For now, we only remark that, in the situation of realtime , this implementation coincides with CL algorithm.   The proof of correctness is deferred to Section~\ref{sec:refinement_steps}.


\begin{figure}[tbp]
\begin{center}
\begin{tabular}{|p{12cm}|}\hline \vspace{0ex}
\textbf{\normalsize Checking  :}
\begin{enumerate}
\item
\textbf{test}  . if so, goto step~2; else \textbf{reject} . \vspace{1ex}

\item
\textbf{test} for every , we have
\begin{enumerate}
\item
    either   and ;

\item
     or  for some  and .
\end{enumerate}
If so, goto step~3; else, \textbf{reject} . \vspace{1ex}

\item
\textbf{test} for every , we have

  for some  and .

If so, goto step~4; else, \textbf{reject} . \vspace{1ex}



\item
\textbf{test} whether  for some  such that .

If so, goto step~7; else, goto step~5. \vspace{1ex}

\item
\textbf{test}  for every , we have

     for some  such that .

If so, goto step~6; else, \textbf{reject} . \vspace{1ex}

\item
\textbf{test}  for every  , we have

     for some  such that .

If so, goto step~7; else, \textbf{reject} . \vspace{1ex}

\item
\textbf{accept} .
\vspace{-1ex}
\end{enumerate} \\
        \hline
\end{tabular}\vspace{-1ex}
\end{center}
\caption{Checking  }\label{Checking_EXP}
\end{figure}

We can state two properties which need to be used to make the whole framework Fig.~\ref{Efficient_Algorithm} work.
\begin{lemma}
In every iteration of Fig.~\ref{Efficient_Algorithm}, we get a decomposition base  from . The following hold:
\begin{enumerate}
\item
.

\item
 is a decreasing bisimulation.
\end{enumerate}
\end{lemma}

\begin{proof}
 Item~1  is an inference directly from Fig.~\ref{Checking_EXP}. Item~2  will be discussed in detail in Section~\ref{sec:refinement_steps}.  \qed
\end{proof}

\subsection{Operations on Long Strings}\label{subsec:longstring}
In the algorithm, we meet quite a few operations on strings whose length is exponential. Thus we need an efficient way to represent and manipulate them.
This sort of improvement actually appears in all the previous work on strong bisimilarity checking on normed . There are many different ways to do so, and nothing special in our situation.  Thus we only sketch the idea and provide some literature.

In the previous work~\cite{DBLP:journals/tcs/HirshfeldJM96,DBLP:conf/mfcs/LasotaR06,DBLP:conf/fsttcs/CzerwinskiL10}, a long string is represented by a {\em straight-line program} (SLP), a
context-free grammar (typically in Chomsky normal form) which generates only one word.
The efficient algorithms rely on an efficient implementation of equality checking on SLP-compressed strings, which is typically implemented (as a special case) by an efficient algorithm of compressed pattern matching such as~\cite{DBLP:conf/cpm/MiyazakiST97,DBLP:conf/dagstuhl/Lifshits06}.  Lohrey~\cite{DBLP:journals/gcc/Lohrey12} gives a nice survey on algorithms on SLP-compressed strings.

One deficiency of the above scheme is that  the procedure for string equality checking is called every time two strings need to compare, and previous computations are completely ignored. In~\cite{DBLP:journals/algorithmica/MehlhornSU97} and its improved version~\cite{DBLP:conf/soda/AlstrupBR00}, a data structure for finite set of strings is maintained, which supports concatenation, splitting, and equality checking operations.  Czerwi\'{n}ski~\cite{CzerwinskiPhD} uses this technique to improve his previous algorithm~\cite{DBLP:conf/fsttcs/CzerwinskiL10}.

\subsection{Analysis of Time Complexity}
Now we give a very brief discussion of the time complexity of the whole algorithm.  Some less important factors are deliberately neglected.  Readers are  referred to~Czerwi\'{n}ski~\cite{CzerwinskiPhD}.

Consider the algorithm described in Fig.~\ref{Efficient_Algorithm}.   The dominating factor is the operation  at line~10.  We claim that there are totally  invocations of .

In the implementation of , we call the procedures  described in Fig.~\ref{Checking_EXP}.  The procedure treats processes as {\em normed strings}.  Therefore, the time consumed depends on the costs of the operations on normed strings. We suppose that there are three operations of `normed' strings: , , and , which are supposed to spend time , , and , respectively.  Claimed in~\cite{CzerwinskiPhD},   the best implementation  is , , and .

Consider the procedures in Fig.~\ref{Checking_EXP}.  The most time-consuming part is still the part of matching, which can perform  times of  operations.   This makes the total time of checking branching bisimilarity no difference from checking strong bisimilarity. The overall running time is .



\section{The Refinement Operation}\label{sec:refinement_steps}

Now, we start to discuss the correctness of the algorithm.
In order to prove the correctness, we need to answer two questions:

\begin{enumerate}
\item
What is the refinement operation corresponding to a step of iteration in our algorithm?

\item
How our algorithm can be derived from the refinement operation.
 \end{enumerate}

In this section we answer the first question, and the second question will be answered in Section~\ref{sec:correctness}.

Actually how to define the refinement operation for our algorithm is really not clear at the first glance. Thus we review the refinement operation adopted in CL algorithm in Section~\ref{subsec:Expansion_Pre}. Then in Section~\ref{subsec:Expansion_Pre} we find another way to define and understand the refinement operation in Section~\ref{subsec:Expansion_Another_undestanding}.  Following this understanding, we attempt to define the refinement operation which turns out to be suitable for our algorithm in Section~\ref{subsec:Expansion_in_general}, and then show some basic properties.





\subsection{The Refinement Operation for Realtime }\label{subsec:Expansion_Pre}
Before going into the tricky part of our definition of the refinement relation, let us review the reason why the algorithm is correct for the realtime  . This special case is comparatively easy.  For convenience, we describe the procedure of checking  for realtime  in Fig.~\ref{Checking_EXP_REALTIME}. This is nothing but a special case of~Fig.~\ref{Checking_EXP}, and it is a slightly simplified version of the corresponding procedure in CL algorithm.

\begin{figure}[tbp]
\begin{center}
\begin{tabular}{|p{12cm}|}\hline \vspace{0ex}
\textbf{\normalsize Checking   in the case of REALTIME systems:}
\begin{enumerate}
\item
\textbf{test}
  .

If so, goto step~2; else \textbf{reject} . \vspace{1ex}

\item
\textbf{test} for every , we have
\begin{quote}
 for some  and .
\end{quote}
If so, goto step~3; else, \textbf{reject} . \vspace{1ex}

\item
\textbf{test} for every , we have
\begin{quote}
  for some  and .
\end{quote}
If so, goto step~4; else, \textbf{reject} . \vspace{1ex}




\item
\textbf{test}  for every , we have
\begin{quote}
     for some  such that .
\end{quote}
If so, goto step~5; else, \textbf{reject} . \vspace{1ex}

\item
\textbf{test}  for every  , we have
\begin{quote}
     for some  such that .
\end{quote}
If so, goto step~6; else, \textbf{reject} . \vspace{1ex}

\item
\textbf{accept} .
\vspace{-1ex}
\end{enumerate} \\
        \hline
\end{tabular}\vspace{-1ex}
\end{center}
\caption{Checking   for Realtime Systems}\label{Checking_EXP_REALTIME}
\end{figure}



At first we review the framework of the correctness proof for CL algorithm.

In the case of bisimilarity for realtime  , we can define the following well-known {\em expansion} relation directly from the definition of bisimulation.

 \begin{definition}\label{def:sexp}
Let  be a binary relation on \textbf{realtime} processes.
The {\em expansion} of ,  , contains all pairs
 satisfying the  following conditions:

\begin{enumerate}
\item
Whenever , then  and  for some .
\item
Whenever , then  and  for some .
\end{enumerate}
\end{definition}

For realtime system,  a relation  is a bisimulation if and only if . Bisimilarity  is the largest relation  which  satisfies .

Definition~\ref{def:sexp} is well-behaved in the sense that  if  is not a bisimulation, and  is a norm-preserving congruence suppose that  is. However, we cannot simply define the refinement relation  to be , because  may not be a decompositional congruence even if  is.  In other words, we cannot always find a  such that .  The way to solve this problem is to find a decompositional congruence   which lies between  and . The way suggested in~\cite{DBLP:journals/mscs/HirshfeldJM96} is that   be the decreasing bisimilarity wrt.~.

\begin{definition}\label{def:relative_realtime}
Let  be a relation on \textbf{realtime} processes.  is a {\em decreasing bisimulation}  if the following hold whenever :
\begin{enumerate}
\item
Whenever , then
 such that .

\item
Whenever , then
 such that .
\end{enumerate}


Let  be a norm-preserving congruence.  The {\em decreasing bisimilarity wrt. }, denoted by  , is the largest decreasing bisimulation contained in .
\end{definition}

We do not justify the rationality of the relation . The fact is  that  is a congruence, and moreover, it satisfies the following:
\begin{enumerate}
\item
 is decompositional if  is right-cancellative.

\item
 and also  is right-cancellative if  is decompositional.
\end{enumerate}




According to these two facts,  is decompositional whenever  is.  From here, we can define  to be .

In order to get a characterization of , we need the following
 expansion relation for decreasing bisimilarity.

\begin{definition}
Let  be a binary relation on \textbf{realtime} processes. The {\em decreasing expansion} of ,  , contains all pairs
 satisfying the following conditions:
\begin{enumerate}
\item
Whenever , then
   and   for some .


\item
Whenever  , then
   and   for some .
\end{enumerate}
\end{definition}
Then
we can establish the following important property for realtime :
 if and only if
\begin{center}
 and .
\end{center}
From this fact, considering that , we have:
 if and only if
\begin{center}
 and
  and .
\end{center}
Now, to prove , it suffices to prove:
\begin{center}
 if and only if  and  and .
\end{center}

According to this characterization, apparently we have .


Now it is time to  explain that
 the procedure in Fig.~\ref{Checking_EXP_REALTIME} is actually based on this characterization.
Suppose we want to check whether .  It suffices to check the following three conditions:
\begin{enumerate}
\item
.

\item
.

\item
.
\end{enumerate}

Notice that these three conditions are deliberately arranged in the above order.
Now we study the procedure described in~Fig.~\ref{Checking_EXP_REALTIME}. Step~1 corresponds to Condition~1: checking . Step~2 and Step~4 correspond to Condition~2: checking . Step~3 and Step~5 partly  correspond to Condition~3: checking . In  Step~3 and Step~5, we find that only increasing transitions are treated. This is because the decreasing transitions are already treated in Step~2 and Step~4, in which stricter requirements are tested, considering  .


\subsection{Another Understanding of the Refinement Operation}\label{subsec:Expansion_Another_undestanding}

The characterization of the refinement operation defined in Section~\ref{subsec:Expansion_Pre} is fine. However, currently we do not know how to generalize this characterization to non-realtime systems.  The main problem is that we cannot find a feasible way to define the expansion relation. This is because the technique of dynamic programming is used in the algorithm. This makes the expansion of , if there is a way to define, not only depend on , but also depend on . This fact makes it very difficult to generalize the correctness proof in the way taken in CL algorithm.  Thus we hope to find another better way to prove the correctness of our algorithm.

Before doing this in non-realtime systems,  the attempt is first made in  realtime systems.  That is, we develop another characterization of the refinement operation for the procedure in Fig.~\ref{Checking_EXP_REALTIME}.


The basic idea is to integrate the three parts into a whole concept, which we called {\em decreasing bisimilarity with expansion}.

To avoid confusion, readers are suggested to forget the terminologies and notations taken in Section~\ref{subsec:Expansion_Pre}, because the forms of the following terminologies and  notations can be close to the ones in Section~\ref{subsec:Expansion_Pre}, but their meanings are different.

 We do not provide proofs for the lemmas  and theorems below, because they are special cases for those in Section~\ref{subsec:Expansion_in_general}.


\begin{definition}\label{def:dec_bisimulation_expansion_realtime}
Let  be a norm-preserving congruence on \textbf{realtime} processes, and
let  be a relation on \textbf{realtime} processes.  We say  is a {\em decreasing bisimulation with expansion of}   if the following conditions hold whenever :
\begin{enumerate}
\item
Whenever ,
\begin{enumerate}
\item
if  ,
then
 for some  such that ;

\item
if  ,
then
 for some  such that .
\end{enumerate}

\item
Whenever ,
\begin{enumerate}
\item
if ,
then
 for some  such that .

\item
if ,
then
 for some  such that .
\end{enumerate}
\end{enumerate}

The {\em decreasing bisimilarity with expansion of }, denoted by  , is the largest decreasing bisimulation with expansion of .
\end{definition}

The following lemma confirms the validity of Definition~\ref{def:dec_bisimulation_expansion_realtime}.

\begin{lemma}\label{lem:property_decreasing_bisimulation_expansion_realtime}
The following properties hold:
\begin{enumerate}
\item
The identity relation is a decreasing bisimulation with expansion of .

\item
Let  be a decreasing bisimulation with expansion of . Then,  is also a decreasing  bisimulation with expansion of .

\item
Let  and  be two decreasing  bisimulation with expansion of . Then,   is also a decreasing  bisimulation with expansion of .

\item
Let  be a set of  decreasing  bisimulation with expansion of . Then,  is a decreasing  bisimulation with expansion of .
\end{enumerate}
\end{lemma}

According to Lemma~\ref{lem:property_decreasing_bisimulation_expansion_realtime},  is an equivalence relation. According to Definition~\ref{def:dec_bisimulation_expansion_realtime}, any decreasing  bisimulation with expansion of  must be norm-preserving, thus  is also norm-preserving.  Moreover, we have

\begin{lemma}
 is a norm-preserving congruence.
\end{lemma}



Now we can define . The validity depends on the following two properties.
\begin{lemma}
\begin{enumerate}
\item
.

\item
If , then .
\end{enumerate}
\end{lemma}

The unique decomposition property of  can be established in the same way as that of , but relies on the right cancellation property of .


\begin{theorem}[Unique Decomposition Property of ]\label{thm:unique-decomposition-relative-realtime}
Let  be a norm-preserving congruence which is right-cancellative. Then,
 is decompositional.
\end{theorem}



It is not hard to establish the following characterization theorem of .

\begin{theorem}\label{lem:char_relative_bisimilarity_realtime}
Let ,  be realtime nBPA processes.  Then,
 if and only if  and \begin{enumerate}
\item
Whenever ,
\begin{enumerate}
\item
if  ,
then
 for some  such that ;

\item
if  ,
then
 for some  such that .
\end{enumerate}

\item
Whenever ,
\begin{enumerate}
\item
if ,
then
 for some  such that .

\item
if ,
then
 for some  such that .
\end{enumerate}
\end{enumerate}
\end{theorem}
When  is defined as , namely , using Theorem~\ref{lem:char_relative_bisimilarity_realtime}, we can get exactly the procedure of checking  in Fig.~\ref{Checking_EXP_REALTIME}.

It should be stressed that  defined in Definition~\ref{def:dec_bisimulation_expansion_realtime} is exact the same as  according to  in Section~\ref{subsec:Expansion_Pre}.  They are two different understandings of the same refinement operation.


\subsection{The Refinement Operation for Non-realtime Systems}\label{subsec:Expansion_in_general}

We spend a lot of space to discuss the correctness of the algorithm for realtime processes. The reason is that we want to generalize the way to show the correctness of our algorithm of checking branching bisimilarity for totally normed BPA. It turns out that the classical proof for CL algorithm cannot be generalized directly.  So we find another characterization of the refinement operation in Section~\ref{subsec:Expansion_Another_undestanding}.  It turns out that this one, as expected, can be used to show the correctness of our algorithm described in Fig.~\ref{Checking_EXP}.  In this section  we discuss the refinement operation in detail.

We start from the notion of {\em decreasing bisimilarity with expansion}.

\begin{definition}\label{def:dec_bisimulation_expansion}
Let  be a norm-preserving congruence on processes, and
let  be a relation on processes.  We say  is a {\em decreasing bisimulation with expansion of}   if the following conditions hold whenever :


\begin{enumerate}
\item
Whenever , then
\begin{enumerate}
\item
either  and  for some  and  such that  and  for every ;

\item
or  for some  and  and  such that  and  for every .

\item
or  for some  and  and  such that  and  for every .
\end{enumerate}

\item
Whenever , then
\begin{enumerate}
\item
either  and     for some  and  such that  and  for every ;

\item
or  for some  and  and  such that  and  for every .

\item
or  for some  and  and  such that  and  for every .
\end{enumerate}
\end{enumerate}


The {\em decreasing bisimilarity with expansion of }, denoted by  , is the largest decreasing bisimulation with expansion of .

If a relation  satisfies the above conditions except for 1(c) and 2(c) whenever , then we call  a {\em decreasing bisimulation}.
\end{definition}



Some explanation should be made on Definition~\ref{def:dec_bisimulation_expansion}.

Firstly, assume that  for instance.
We know the corresponding transition is increasing or decreasing.  If the transition is increasing, the only possibility is to take the matched transitions as the item~1(c).  If the transition is decreasing, there are two subcases. The item~1(a) corresponds to the situation of  and this silent transition can be vacantly matched.  The item~1(b) corresponds to the situation that either  is not silent, or the silent transition must be explicitly matched.  Whenever , we cannot tell which one of item~1(a) or item~1(b) should be chosen. So we must test the condition~1(a), and if 1(a) does not hold then we test condition~1(b).


Secondly, when a transition  is matched by , Definition~\ref{def:dec_bisimulation_expansion} takes a different style from Definition~\ref{def:beq}, the common definition of branching bisimulation.  Consider the condition~1(b) for example. In this case we require that the matching sequence of  to be

such that   and  for every .  That is , every intermediate  must be related to .  In Definition~\ref{def:beq}, however, we take the simplified matching sequence of :

such that   and .  The reason is explained as follows.  In the normal definition of branching bisimulation, although we do not require  for every intermediate , the largest bisimulation, , satisfy the Computation Lemma (Lemma~\ref{computation-lemma}). Thus if  is replaced by , namely if  and  is matched by

such that   and , then we immediately have  for every .  But at present we cannot establish Computation Lemma for , since this property depends on another equivalence , and in Definition~\ref{def:dec_bisimulation_expansion} we do not impose any restrictions on . Thus Computation Lemma could not be established if normal style matchings are taken in Definition~\ref{def:dec_bisimulation_expansion}.  Thus one way of defining decreasing bisimulation with expansion of  is to strengthen the relevant requirements.  We take this style not because we need the Computation Lemma, but because we need the conditions appearing in Definition~\ref{def:dec_bisimulation_expansion} to be close to the conditions checked by the algorithm.

Thirdly, the `semi-branching' style (see Definition~\ref{def:semi-beq}) is taken in the case of vacant matching. This is not necessary but is helpful to show the transitivity of .


The following lemma confirms that the relation  is well-defined.

\begin{lemma}\label{lem:property_decreasing_bisimulation_expansion}
The following properties hold:
\begin{enumerate}
\item
The identity relation is a decreasing bisimulation with expansion of .

\item
Let  be a decreasing bisimulation with expansion of . Then,  is also a decreasing  bisimulation with expansion of .

\item
Let  and  be two decreasing  bisimulation with expansion of . Then,   is also a decreasing  bisimulation with expansion of .

\item
Let  be a set of  decreasing  bisimulation with expansion of . Then,  is a decreasing  bisimulation with expansion of .
\end{enumerate}
\end{lemma}

According to Lemma~\ref{lem:property_decreasing_bisimulation_expansion},  is an equivalence relation.

Since  is a decreasing  bisimulation with expansion of  according to Definition~\ref{def:dec_bisimulation_expansion}, we have
\begin{lemma}\label{lem:decreasing_bisimulation_inequality}
.
\end{lemma}

According to Definition~\ref{def:dec_bisimulation_expansion}, any decreasing  bisimulation with expansion of  must be norm-preserving, thus  is also norm-preserving.  Moreover,  is a congruence.

\begin{lemma}
 is a norm-preserving congruence.
\end{lemma}

\begin{proof}
We only show that  is a congruence.
Let

We show  is a decreasing  bisimulation with expansion of . This is done by
checking the conditions in Definition~\ref{def:dec_bisimulation_expansion}
for every .

If .  This is a trivial case.

If  and . This proof is done by case studies.
We study only two cases. Other cases are similar.

\begin{itemize}
 \item
 Suppose there is a transition , we shall find the matching from . Remember  , thus every transition  has a matching from . Say, we have the matching:

such that   and  for every .  Then we have

According to the definition of  and the fact ,  we have , and  for every .

\item
 Suppose there is a transition , we shall find the matching from . Remember  , thus every transition  has a matching from . Say, we have the matching:

such that   and  for every .  Then we have

According to the definition of  and the fact ,  we have   for every .   Knowing , we have , and by congruence of  we have . \qed

\end{itemize}
\end{proof}

Now we can define the refinement operation  as . The validity of this definition depends on the following lemma.
\begin{lemma}
The following two properties hold.
\begin{enumerate}
\item
.

\item
If , then .
\end{enumerate}
\end{lemma}

\begin{proof}
\begin{enumerate}
\item
At first, we show that  for every . As a special case, we have . Assume that , then we can check that conditions in Definition~\ref{def:dec_bisimulation_expansion}, taking . This is a routine work, by applying the Computation Lemma (Lemma~\ref{computation-lemma}).
To see  why , we notice  (Lemma~\ref{lem:decreasing_bisimulation_inequality}), and take  to be .

\item
By the proof of the first item and Lemma~\ref{lem:decreasing_bisimulation_inequality}, we already have  whenever . Now we assume further that , we will show that . It suffices to show  is a branching bisimulation (Definition~\ref{def:beq}).  Because  is a decreasing bisimulation with expansion of , it satisfies the conditions in Definition~\ref{def:dec_bisimulation_expansion}. By taking  to be both  and , we see that bisimulation property in Definition~\ref{def:beq} can be inferred.  \qed
\end{enumerate}
\end{proof}


The unique decomposition property of  can be established in the same way as that of , but relies on the right cancellation property of .


\begin{theorem}[Unique Decomposition Property of ]\label{thm:unique-decomposition-relative}
Let  be a norm-preserving congruence which is right-cancellative. Then,
 is decompositional.
\end{theorem}

\begin{proof}
It suffices to show that to show  that

is a decreasing branching bisimulation wrt.~.
In the proof the right cancellativity of  is used.  Then the proof goes in the same way as in Theorem~\ref{thm:unique-decomposition}.  \qed
\end{proof}

According to Theorem~\ref{thm:unique-decomposition-relative},  is decompositional whenever  is.  This is the key property to define refinement operation. Now, our refinement operation  can be defined as .


\section{The Correctness of the Algorithm}\label{sec:correctness}

In this section we will show that the  constructed from  during an iteration is exactly the decomposition base of  defined in Section~\ref{subsec:Expansion_in_general}.


\subsection{The Characterization of }\label{subsec:Expansion_characterization}


Remember in Section~\ref{subsec:Expansion_Another_undestanding} we have remarked that the procedure in Fig.~\ref{Checking_EXP_REALTIME} is correct for realtime systems. At that time the proof is straightforward, because the procedure checks exactly the conditions in the characterization theorem (Theorem~\ref{lem:char_relative_bisimilarity_realtime}), which are exactly the conditions in Definition~\ref{def:dec_bisimulation_expansion_realtime}.  However, this is not the case now, and there are a number of subtleties.


In the following, we will develop some terminologies, which make us easier to formulate our results. First we need an adequate notion of `expansion' relation which is suitable for Definition~\ref{def:dec_bisimulation_expansion} and close to the testing procedure.  We call this notion {\em compound expansion}.








\begin{definition}\label{def:compound_expansion}
Let  be a norm-preserving congruence on processes, and
let  be a relation on processes. The {\em compound expansion} wrt.~ and , denoted by , contains all pairs
 which satisfy  and  the following conditions:
\begin{enumerate}
\item
Whenever , then either
 \begin{enumerate}
 \item
   and ; or

  \item
   and   for some ; or

 \item
   and   for some ; or

  \item
   and   for some .
\end{enumerate}

\item
Whenever  , then either
 \begin{enumerate}
 \item
     and ; or

 \item
   and   for some ; or

 \item
   and   for some ; or

  \item
   and   for some .
   \end{enumerate}
\end{enumerate}
\end{definition}
The correctness of Definition~\ref{def:compound_expansion} is confirmed by the following lemmas.
\begin{lemma}\label{lem:decreasing_branching_bisimulation_oneside_contain}
If  is a decreasing bisimulation with expansion of  (see Definition~\ref{def:dec_bisimulation_expansion}),
then . In particular,
 .
\end{lemma}

\begin{proof}
This fact is an inference of  Definition~\ref{def:compound_expansion} and Definition~\ref{def:dec_bisimulation_expansion}.  Compare the conditions in these two definitions. When  is a decreasing bisimulation with expansion of   and ,  satisfies the conditions in Definition~\ref{def:dec_bisimulation_expansion}. Then we can find that  also satisfies the conditions in Definition~\ref{def:compound_expansion}. \qed
\end{proof}



\begin{lemma}\label{lem:decreasing_branching_bisimulation_twoside_contain}
 is a decreasing bisimulation with expansion of . In particular, .
\end{lemma}

\begin{proof}
At first, remember that   according to Definition~\ref{def:compound_expansion}. This is the prerequisite of  being a decreasing bisimulation with expansion of .   This fact will be implicitly used in the remaining proof.

Let  and .    According to the definition of  (Definition~\ref{def:compound_expansion}), there are four cases:
 \begin{enumerate}
 \item
   and .  In this case,  we have  according to Lemma~\ref{lem:decreasing_branching_bisimulation_oneside_contain}. Thus condition~1(a) of Definition~\ref{def:dec_bisimulation_expansion} holds (with ).

  \item
   and   for some . In this case, we have   according to  Lemma~\ref{lem:decreasing_branching_bisimulation_oneside_contain}. This is the special case of the condition~1(b) of Definition~\ref{def:dec_bisimulation_expansion} in which . Thus condition~1(b) of Definition~\ref{def:dec_bisimulation_expansion} holds.

  \item
   and   for some . This is the special case of the condition~1(c) of Definition~\ref{def:dec_bisimulation_expansion} in which . Thus condition~1(c) of Definition~\ref{def:dec_bisimulation_expansion} holds.


  \item
   and   for some . In this case, we have   according to  Lemma~\ref{lem:decreasing_branching_bisimulation_oneside_contain}.  We can now use induction hypothesis on the pair .  Note that this case can not happen forever.  Finally, case~1 or case~2 or case~3 must happen.
   \begin{itemize}
    \item
   If case~1 happens finally, then we have  for some  and   such that  and  for every .  Now we also have  according to  Lemma~\ref{lem:decreasing_branching_bisimulation_oneside_contain}.
   Consequently condition~1(a) of Definition~\ref{def:dec_bisimulation_expansion} in which  holds.

  \item
   If case~2 happens finally, then we get  for some  and  and  such that  and  for every , according to  Lemma~\ref{lem:decreasing_branching_bisimulation_oneside_contain}.
   Consequently condition~1(b) of Definition~\ref{def:dec_bisimulation_expansion} in which  holds.

 \item
    If case~3 happens finally, then we get  for some  and  and  such that  and  for every , according to  Lemma~\ref{lem:decreasing_branching_bisimulation_oneside_contain}.
   Consequently condition~1(c) of Definition~\ref{def:dec_bisimulation_expansion} in which  holds. \qed
  \end{itemize}
\end{enumerate}
\end{proof}
From Lemma~\ref{lem:decreasing_branching_bisimulation_oneside_contain} and  Lemma~\ref{lem:decreasing_branching_bisimulation_twoside_contain}, we conclude the following important characterization of .

\begin{theorem}\label{thm:char_relative_bisimilarity}
 if and only if  .
\end{theorem}



\begin{remark}
The inverse of Lemma~\ref{lem:decreasing_branching_bisimulation_oneside_contain} also holds. That is,
If a relation  satisfies , then  is a decreasing bisimulation with expansion of .    According to this fact and Theorem~\ref{thm:char_relative_bisimilarity}, the congruence  is the greatest fixpoint of
. Thus the congruence  can be completely characterized via the operation .


Readers may have noticed that the conditions in Definition~\ref{def:compound_expansion} are quite different from the conditions in Definition~\ref{def:dec_bisimulation_expansion}. In Definition~\ref{def:dec_bisimulation_expansion}
we lay stress on  getting a congruence relation from a congruence relation.    On the other hand,  in Definition~\ref{def:compound_expansion},  the purpose is to give a characterization which makes the conditions easy to check in the algorithm. We do not need  to satisfy a lot of  favourite properties. The difference between these two definitions must be highlighted, because it does not happen in the case of realtime , and the existence of silent actions do make things difficult. However, according to Theorem~\ref{thm:char_relative_bisimilarity},  is definitely a favourite congruence.
\end{remark}

\subsection{The Correctness of the Algorithm }\label{subsec:Expansion_correctness}





Theorem~\ref{thm:char_relative_bisimilarity} gives us a potential way to get an implementation of the refinement operation.   That is, it provides a potential way to implement  at line~10.


In the following discussion, for convenience we presuppose that  is equal to .  We will develop more properties of .


According to Theorem~\ref{thm:char_relative_bisimilarity}, checking   is equivalent to checking .
Note at first that  concerns relation  itself, and  is not completely known at the moment. Fortunately, we have the following two critical observations.
\begin{description}
\item[Observation~1.]
At the moment of testing on the pair ,  we have already known the base  and a profile of   whose constances with indexes less than . Thus we can suppose that  is known for every  such that , and  is known for every  such that .  Therefore we are capable to answer  whether  for any , and whether  for any .

\item[Observation~2.]
 Whenever decreasing transitions are concerned, say ,  according to Lemma~\ref{lem:decreasing_transition}, we have .
\end{description}
With these two observations, we can develop the efficient procedure for checking  for realtime system (remember Theorem~\ref{lem:char_relative_bisimilarity_realtime}).

But at present, the situation is more complicated. In the presence of silent actions,
the above two observations cannot directly lead to the efficient procedure.   The consecutive silent actions do cause inconvenience. Investigate the following scenario. Assume we want to show  and let  be a transition which is required to be matched by  with . In this situation we still do not know  whether  because  still needs computing.

To handle this difficulty, we need some other techniques. Before doing this,  we notice the following critical observation:

\begin{description}
\item[Observation~3.]
According to the fact of  and Lemma~\ref{lem:tau_prime_string},  has no transition of the form  which satisfies .
\end{description}

Whenever ,
the above critical observation gives rise to the following lemma.
\begin{lemma}\label{lem:critical}
Assume .  When  and  , then  we do \textbf{not} have .
\end{lemma}
According to Lemma~\ref{lem:critical}, we can draw the following two assertions.
First, when transition  is matched by , the vacantly matching cannot happen.
Second, when transition  is matched by , the `state-preserving' silent transitions cannot occurred.

Within these two assertions, Theorem~\ref{thm:char_relative_bisimilarity} can be written as follows.

\begin{theorem}\label{thm:exp_relative_membership_simplified}
Let  and  be two decomposition bases  which validate  .    Assume
, then   if and only if  and the following conditions are satisfied:
\begin{enumerate}
\item
Whenever , then either
\begin{enumerate}
\item
 and  ; or

\item
 and  for some ; or

\item
 and  for some .
\end{enumerate}

\item
Either  and  for some ;

or, whenever , either
\begin{enumerate}
\item
  and  for some , or

\item
 and  for some .
\end{enumerate}
\end{enumerate}
\end{theorem}

\begin{proof}
Remember that Theorem~\ref{thm:char_relative_bisimilarity} confirms that  if and only if  .  According to Definition~\ref{def:compound_expansion},
 if and only if  and:
\begin{enumerate}
\item
Whenever , then either
 \begin{enumerate}
 \item
   and ; or

  \item
   and   for some ; or

 \item
   and   for some ; or

  \item
   and   for some .
\end{enumerate}

\item
Whenever  , then either
 \begin{enumerate}
 \item
     and ; or

 \item
   and   for some ; or

 \item
   and   for some ; or

  \item
   and   for some .
   \end{enumerate}
\end{enumerate}
Now making use of Lemma~\ref{lem:critical}, we can draw the conclusion that the case~1(d) and case~2(a) cannot happen! Now the conditions above become the conditions in Theorem~\ref{thm:exp_relative_membership_simplified}. \qed
\end{proof}

Comparing with Theorem~\ref{thm:char_relative_bisimilarity}, Theorem~\ref{thm:exp_relative_membership_simplified} has a great advantage.  When we need to determine whether  or not, according to Theorem~\ref{thm:exp_relative_membership_simplified}, we only require to checking several conditions which depends only on  and the profile of  in which only constants with index less than  are involved. Thus we can use this fact to construct  in the `bottom-up' way, which is exactly the procedure described in Fig.~\ref{Checking_EXP}. The proof of correctness of the algorithm is now finished.





\section{Remark}\label{sec:remark}




\subsection{Other Bisimilarities On Totally Normed }
Comparing with branching bisimilarity, other bisimilarities tend to be more flexible so that they are currently known to be NP-hard on .  On the occasion of weak bisimilarity, there are two different problems deserving to consideration. First, it is no longer decompositional, as is shown in Example~\ref{example:weak_bisimilarity_decom}.  Second, it is capable to encode NP-complete problem due to its more flexible matching style.

There is a variant of weak bisimilarity called delay bisimilarity, which is still decompositional on . Using unique decomposition property, we can confirm that delay bisimilarity is in PSPACE. The way is barely to guess a decomposition base  and check that  a delay bisimulation.  Still, the bisimulation property needs to carefully defined.  Anyway, it is technically much easier than checking branching bisimilarity.

Finally we conjecture that deciding bisimilarities other than branching bisimilarity on tnBPA is PSPACE complete.





\subsection{On Branching Bisimilarity Checking}

In the situation that silent transitions are treated unobservable, branching bisimilarity arouses interest of researchers. In most of the cases, previous decidability and complexity results for weak bisimilarity still hold for branching bisimilarity. There are two remarkable exceptions. The decidability of branching bisimilarity is established by Czerwi\'{n}ski, Hofman and Lasota~\cite{DBLP:conf/concur/CzerwinskiHL11} on normed BPP, and by Fu~\cite{DBLP:conf/icalp/Fu13} on  normed BPA.  In these two cases, decidability of weak bisimilarity is unknown.  Recently, we have proven that branching (and weak) bisimilarity is undecidable on every model above BPA and BPP in the PRS hierarchy even in the normed case~\cite{DBLP:conf/icalp/YinFHHT14}. It is believed that branching bisimilarity is easier to decide than weak bisimilarity. Currently, there is no real instance to support this belief. This paper provides an interesting instance. We expect that more instances will be discovered in the future.





\vspace*{4.5mm}\noindent {\bf Acknowledgement}.  The author would like to thank S{\l}awomir Lasota for letting me know the work of Czerwi\'{n}ski~\cite{CzerwinskiPhD}, the current fastest algorithm for checking strong bisimilarity on normed ;
to thank the members of BASICS for their helpful discussions on related topics.




\bibliographystyle{plain}
\bibliography{arxiv}



\newpage

\end{document}
