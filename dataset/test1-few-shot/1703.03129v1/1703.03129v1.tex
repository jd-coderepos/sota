\documentclass{article} \usepackage{iclr2017_conference,times}
\usepackage[colorlinks=true, linkcolor=black, citecolor=black, filecolor=black, urlcolor=black]{hyperref}
\usepackage{enumerate}
\usepackage{url}
\usepackage{amsmath}
\usepackage[draft]{fixme}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{tikz}


\newcommand\sigmoid{\sigma}
\newcommand\argmax{\mathrm{argmax}}
\newcommand\gru{\ensuremath{\mathrm{GRU}}}
\newcommand\cgru{\ensuremath{\mathrm{CGRU}}}
\newcommand\dcgru{\ensuremath{\mathrm{CGRU}^d}}
\newcommand\sfin{s_\mathrm{fin}}
\newcommand\floor[1]{\left \lfloor{#1}\right \rfloor}
\newcommand{\calM}{\mathcal{M}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\newcommand\cgruhands[1]{
\draw (0.6, 2.9) -- (1.79, 2.9);
\draw (0.6, 2.7) -- (1.79, 2.9);

\draw (0.6, 2.9) -- (1.79, 2.7);
\draw (0.6, 2.7) -- (1.79, 2.7);
\draw (0.6, 2.5) -- (1.79, 2.7);

\draw (0.6, 2.7) -- (1.79, 2.5);
\draw (0.6, 2.5) -- (1.79, 2.5);
\draw (0.6, 2.3) -- (1.79, 2.5);

\node (cg) at (1.2, 1.5) {{#1}};

\draw (0.65, 0.3) -- (1.79, 0.1);
\draw (0.65, 0.1) -- (1.79, 0.1);

\draw (0.65, 0.3) -- (1.79, 0.5);
\draw (0.65, 0.5) -- (1.79, 0.5);
\draw (0.65, 0.7) -- (1.79, 0.5);

\draw (0.65, 0.1) -- (1.79, 0.3);
\draw (0.65, 0.3) -- (1.79, 0.3);
\draw (0.65, 0.5) -- (1.79, 0.3);
}

\newcommand\cgrupic[1]{
\draw[step=0.2] (0, 0) grid (0.6, 3.0);
\cgruhands{{#1}}
}


\title{Learning to Remember Rare Events}


\author{\L{}ukasz Kaiser\thanks{First two authors contributed equally.} \\
  Google Brain\\
  \texttt{lukaszkaiser@google.com} \\
\And
  Ofir Nachum\thanks{Work done as a member of the Google Brain Residency program (\url{g.co/brainresidency}).} \\
  Google Brain\\
  \texttt{ofirnachum@google.com} \\
\And
  Aurko Roy\thanks{Work done during internship at Google Brain.} \\
  Georgia Tech \\
  \texttt{aurko@gatech.edu} \\
\And
  Samy Bengio \\
  Google Brain \\
  \texttt{bengio@google.com} \\
}

\iclrfinalcopy 

\begin{document}

\maketitle

\begin{abstract}
Despite recent advances, memory-augmented deep neural networks are still limited
when it comes to life-long and one-shot learning, especially in remembering rare events.
We present a large-scale life-long memory module for use in deep learning.
The module exploits fast nearest-neighbor algorithms for efficiency and
thus scales to large memory sizes.
Except for the nearest-neighbor query, the module is fully differentiable
and trained end-to-end with no extra supervision.  It operates in
a life-long manner, i.e., without the need to reset it during training.

Our memory module can be easily added to any part of a supervised neural network.
To show its versatility we add it to a number of networks, from simple
convolutional ones tested on image classification to deep sequence-to-sequence
and recurrent-convolutional models.
In all cases, the enhanced network gains the ability to remember
and do life-long one-shot learning.
Our module remembers training examples shown many thousands
of steps in the past and it can successfully generalize from them.
We set new state-of-the-art for one-shot learning on the Omniglot dataset
and demonstrate, for the first time, life-long one-shot learning in
recurrent neural networks on a large-scale machine translation task.
\end{abstract}

\section{Introduction}

Machine learning systems have been successful in many domains, from computer vision \citep{img12} to
speech recognition \citep{speech-nn} and machine translation \citep{sutskever14,bahdanau2014neural,cho2014learning}.
Neural machine translation (NMT) is so successful that for some language pairs it approaches,
\emph{on average}, the quality of human translators \citep{gnmt}.
The words \emph{on average} are crucial though. When a sentence resembles
one from the abundant training data, the translation will be accurate.
However, when encountering a rare word such as \emph{Dostoevsky} (in German,
\emph{Dostojewski}), many models will fail.
The correct German translation of \emph{Dostoevsky} does not appear enough times 
in the training data for the model to sufficiently learn its translation.

While more example sentences concerning the famous Russian author might eventually be added to the training data, there
are many other rare words or rare events of other kinds.
This illustrates a general problem with current deep learning models: it is necessary
to extend the training data and re-train them to handle such rare or new events.
Humans, on the other hand, learn in a life-long fashion, often from single examples.

We present a life-long memory module that enables one-shot learning
in a variety of neural networks. Our memory module consists of key-value pairs.
Keys are activations of a chosen layer of a neural network, and values are
the ground-truth targets for the given example. 
This way, as the network is trained, its memory increases and becomes more useful.  
Eventually it can give predictions that leverage on knowledge from past data with 
similar activations.  Given a new example, the network writes it to memory
and is able to use it afterwards, even if the example was presented just once.

There are many advantages of having a long-term memory. One-shot learning
is a desirable property in its own right, and some tasks, as we will show
below, are simply not solvable without it. Even real-world tasks where we
have large training sets, such as translation, can benefit from long-term
memory. Finally, since the memory can be traced back to training examples,
it might help explain the decisions that the model
is making and thus improve understandability of the model.

It is not immediately clear how to measure the performance of
a life-long one-shot learning model, since most deep learning
evaluations focus on the average performance and do not have
a one-shot component. We therefore evaluate in a few ways,
to show that our memory module indeed works:
\begin{enumerate}[(1)]
    \item We evaluate on the well-known one-shot learning task Omniglot,
      which is the only dataset with explicit one-shot learning evaluation.
      This dataset is small and does not benefit from life-long learning
      capability of our module, but we still exceed the best previous results and set new state-of-the-art.
    \item We devise a synthetic task that requires life-long one-shot learning.
      On this task, standard models fare poorly while our model can solve
      it well, demonstrating its strengths.
    \item Finally, we train an English-German translation model that has our
      life-long one-shot learning module. It retains very good performance
      on average and is also capable of one-shot learning. On the qualitative side,
      we find that it can translate rarely-occurring words like Dostoevsky.
      On the quantitative side, we see that the BLEU score for the generated translations
      can be significantly increased by showing it related translations before evaluating.
\end{enumerate}

\section{Memory Module}

Our memory consists of a matrix  of memory keys, a vector  of memory values,
and an additional vector  that tracks the age of items stored in memory.
Keys can be arbitrary vectors of size \texttt{key-size}, and
we assume that the memory values are single integers representing 
a class or token ID.
We define a memory of size \texttt{memory-size} as a triple:


A memory query is a vector of size \texttt{key-size} which we assume
to be normalized, i.e., . Given a query , we define the nearest
neighbor of  in  as any of the keys that maximize the dot product with :

Since the keys are normalized, the above notion
corresponds to the nearest neighbor with respect to cosine similarity.
We will also use the natural extension of it to  nearest
neighbors, which we denote . In our
experiments we always used the set of  nearest neighbors.

When given a query , the memory  will compute
 nearest neighbors (sorted by decreasing cosine similarity):

and return, as the main result, the value .
Additionally, we will compute the cosine similarities 
and return .
The parameter  denotes the inverse of softmax temperature and we set it to  in our experiments. In models where the memory output
is again embedded into a dense vector, we multiply the embedded output
by the corresponding softmax component so as to provide a signal
about confidence of the memory.

The forward computation of the memory module is thus very simple,
the only interesting part being how to compute nearest neighbors efficiently,
which we discuss below. But we must also answer the question how the memory is trained.

\paragraph{Memory Loss.}
Assume now that in addition to a query  we are also given the correct desired (supervised) value .
In the case of classification, this  would be the class label.  
In a sequence-to-sequence task,  would be the desired output token of the
current time step.
After computing the  nearest neighbors  as above,
let  be the smallest index such that  and  the smallest
index such that . We call  the \emph{positive neighbor} and  the \emph{negative neighbor}.
When no positive neighbor is among the top-, we pick any vector from memory with value  instead of .  We define the memory loss as:


Recall that both  and the keys in memory are normalized,
so the products in the above loss term correspond to cosine similarities
between , the positive key, and the negative key. Since cosine similarity
is maximal for equal terms, we want to maximize the similarity to the positive key
and minimize the similarity to the negative one. But once they are far enough apart
(by the margin ,  in all our experiments), we
do not propagate any loss. This definition and reasoning behind it are almost
identical to the one in \cite{facenet} and similar to many other distance
metric learning works \citep{weinberger, wsabie}.

\paragraph{Memory Update.}
In addition to computing the loss, we will also update the memory 
to account for the fact that the newly presented query  corresponds to .
The update is done in a different way depending on whether the main value
returned by the memory module already is the correct value  or not.
As before, let  be the nearest neighbor to .

If the memory already returns the correct value, i.e., if ,
then we only update the key for  by taking the average of
the current key and  and normalizing it:

When doing this, we also re-set the age: .

Otherwise, when , we find a new place in the memory
and write the pair  there. Which place should we choose? We find
memory items with maximum age, and write to one of those (randomly chosen).
More formally, we pick  where  is
a random number that introduces some randomness in the choice so as to
avoid race conditions in asynchronous multi-replica training.  We then set:

With every memory update we also increment the age of all non-updated indices by .
The full operation of the memory module is depicted in Figure~\ref{fig:mem}.

\begin{figure}
\begin{center}
\begin{tikzpicture}[yscale=1.0,xscale=0.68] \small

\node (K) at (-0.5, 0.5) {};
\draw (0, 0) rectangle (8.0, 1.0);

\node (q) at (4.0, -1.0) {};
\draw[dotted] (4.0, -0.8) -- (1.5, 0.0);
\draw[dotted] (4.0, -0.8) -- (3.5, 0.0);
\draw[dotted] (4.0, -0.8) -- (5.5, 0.0);
\draw[dotted] (4.0, -0.8) -- (7.5, 0.0);

\draw[fill=gray!50] (1.0, 0.0) rectangle (2.0, 1.0);
\node (k1) at (1.5, 0.5) {};
\draw (1.5, -0.1) -- (1.5, 0.1);
\draw (1.5, 0.9) -- (1.5, 1.1);
\node (n1) at (1.5, 1.5) {};
\draw (1.5, 1.9) -- (1.5, 2.1);

\draw[fill=gray!50] (5.0, 0.0) rectangle (6.0, 1.0);
\node (kb) at (5.5, 0.5) {};
\draw (5.5, -0.1) -- (5.5, 0.1);
\draw (5.5, 0.9) -- (5.5, 1.1);
\node (nn) at (5.5, 1.5) {};
\draw (5.5, 1.9) -- (5.5, 2.1);

\node (V) at (-0.5, 2.0) {};
\draw[thick] (0, 2.0) -- (8.0, 2.0);
\node (v1) at (1.5, 2.5) {};
\node (vb) at (5.7, 2.5) {};

\node[anchor=west] (case) at (-1.0, 4.0) {Case 1: ;
   Loss = };
\node[anchor=west] (upd) at (-0.5, 3.5) {Update:
  
  };
  
\begin{scope}[xshift=11cm]
\node (K) at (-0.5, 0.5) {};
\draw (0, 0) rectangle (8.0, 1.0);

\node (q) at (4.0, -1.0) {};
\draw[dotted] (4.0, -0.8) -- (1.5, 0.0);
\draw[dotted] (4.0, -0.8) -- (3.5, 0.0);
\draw[dotted] (4.0, -0.8) -- (5.5, 0.0);
\draw[dotted] (4.0, -0.8) -- (7.5, 0.0);

\draw[fill=gray!50] (1.0, 0.0) rectangle (2.0, 1.0);
\node (k1) at (1.5, 0.5) {};
\draw (1.5, -0.1) -- (1.5, 0.1);
\draw (1.5, 0.9) -- (1.5, 1.1);
\node (n1) at (1.5, 1.5) {};
\draw (1.5, 1.9) -- (1.5, 2.1);

\draw[fill=gray!50] (5.0, 0.0) rectangle (6.0, 1.0);
\node (kb) at (5.5, 0.5) {};
\draw (5.5, -0.1) -- (5.5, 0.1);
\draw (5.5, 0.9) -- (5.5, 1.1);
\node (nn) at (5.5, 1.5) {};
\draw (5.5, 1.9) -- (5.5, 2.1);

\node (V) at (-0.5, 2.0) {};
\draw[thick] (0, 2.0) -- (8.0, 2.0);
\node (v1) at (1.5, 2.5) {};
\node (vb) at (5.7, 2.5) {};

\node[anchor=west] (case) at (-1.0, 4.0) {Case 2: ;
   Loss = };
\node[anchor=west] (upd) at (-0.5, 3.5) {Update:
   
  };
\end{scope}

\end{tikzpicture}
\end{center}
\caption{The operation of the memory module on a query  with correct value ;
  see text for details.}
\label{fig:mem}
\end{figure}

\paragraph{Efficient nearest neighbor computation.}
The most expensive operation in our memory module is
the computation of  nearest neighbors. This can be done exactly or in an approximate way.

In the exact mode, to calculate the nearest neighbors in  to a mini-batch
of queries , we perform a single matrix multiplication:
. This multiplies the \texttt{batch-size}  \texttt{key-size}
matrix  by the \texttt{key-size}  \texttt{memory-size} matrix ,
and the result is the \texttt{batch-size}  \texttt{memory-size} matrix
of all distances, from which we can choose the top-. This procedure is
linear in \texttt{memory-size}, so it can be expensive for very large memory
sizes. But matrix multiplication is very heavily optimized, so in our experiments
on GPUs we find that this operation is not a bottleneck for memory sizes up to
half a million.

If the exact mode is too slow, the  nearest neighbors can be computed
approximately using locality sensitive hashing (LSH). LSH is a hashing 
scheme so that near neighbors get similar hashes \citep{indyk1998approximate,  lsh}.
For cosine similarity, the computation of
an LSH is very simple. We pick a number of random normalized hash vectors
. The hash of a query  is a sequence of  bits,
, such that  if, and only if, .
It turns out that near neighbors will, with high probability, have
a large number of identical bits in their hash. To compute the nearest
neighbors it is therefore sufficient to only look into parts of the memory
with similar hashes. This makes the nearest neighbor computation work in
approximately constant time -- we only need to multiply the query by the hash
vectors, and then only use the nearest buckets.

\subsection{Using the Memory Module}

The memory module presented above can be added to any classification network.
There are two main choices: which layer to use to generate queries,
and how to use the output of the module.

In the simplest case, we use the final layer of a network as query and
the output of the module is directly used for classification. This simplest
case is similar to matching networks \citep{matching_nets}
and our memory module yields good results already
in this setting (see below).

Instead of using the output of the module directly,
it is possible to embed it again into a dense representation and mix it with
other predictions made by the network. To study this setting,
we add the memory module to sequence-to-sequence recurrent neural networks.
As described in detail below, a query to memory is made in every step
of the decoder network. Memory output is embedded again into a dense
representation and combined with inputs from other layers of the network.


\paragraph{Convolutional Network with Memory.}
To test our memory module in a simple setting, we first add it to
a basic convolutional network network for image classification.
Our network consists of two convolutional layers with ReLU non-linearity,
followed by a max-pooling layer, another two convolutional-ReLU layers,
another max-pooling, and two fully connected layers. All convolutions use 
filters with  channels in the first pair, and  in the second.
The fully connected layers have dimension  and dropout applied between them.
The output of the final layer is used as query to our memory module
and the nearest neighbor returned by the memory is used as the final network prediction.
Even this basic architecture yields good results in one-shot learning,
as discussed below.

\begin{figure}
\begin{center}
\includegraphics[width=0.75\textwidth,height=5cm]{bnmt.pdf}
\end{center}
\caption{The GNMT model with added memory module.  
On each decoding step , the result of the attention  is used to 
query the memory.  The resulting value is combined with the output of the
final LSTM layer to produce the predicted logits . See text for further details.}
\label{fig:gnmt}
\end{figure}

\paragraph{Sequence-to-sequence with Memory.}
For large-scale experiments, we add the memory module into a large
sequence-to-sequence model. Such sequence-to-sequence recurrent neural
networks (RNNs) with long short-term memory (LSTM) cells \citep{hochreiter1997}
have proven especially successful at natural language processing (NLP) tasks,
including machine translation \citep{sutskever14,bahdanau2014neural,cho2014learning}.
We add the memory module to the Google Neural Machine Translation (GNMT) model 
\citep{gnmt}. This model consists of an encoder RNN, which creates a representation
of the source language sentence, and a decoder RNN that outputs the target language
sentence. We left the encoder RNN unmodified. In the decoder RNN, we use the vector
retrieved by the attention mechanism as query to the memory module.
In the GNMT model, the attention vector is used in all LSTM layers beyond the second one,
so the computation of the other layers and the memory can happen in parallel.
Before the final softmax layer, we combine the embedded memory output with
the output of the final LSTM layer using an additional linear layer,
as depicted in Figure~\ref{fig:gnmt}.

\paragraph{Extended Neural GPU with Memory.}
To test versatility of our memory module, we also add it to the Extended Neural GPU,
a convolutional-recurrent model introduced by \cite{engpu}. The Extended Neural GPU
is a sequence-to-sequence model too, but its decoder is convolutional and the size
of its state changes depending on the size of the input. Again, we leave the encoder
part of the model intact, and extend the decoder part by a memory query.
This time, we use the position one step ahead to query memory, and we put
the embedded result to the output tape, as shown in Figure~\ref{fig:engpu}.
Note that in this model the result of the memory will be processed by two
recurrent-convolutional cells before the corresponding output is produced.
The fact that this model still does one-shot learning confirms that the output
of our memory module can be used deep inside a network, not just near the output layer.

\begin{figure}\begin{center}
\begin{tikzpicture}[yscale=1.0,xscale=0.9] \small

\draw[fill=gray] (0, 0) rectangle (0.2, 3.0);

\node(i1) at (-0.6, 2.9) {};
\node (idots) at (-0.6, 1.6) {};
\foreach \y in {0.1, 2.9} {
  \draw (-0.35, \y) -- (0.1, \y);
}
\node(in) at (-0.6, 0.1) {};

\node (s0) at (0.3, -0.5) {};
\cgrupic{CGRU}
\begin{scope}[xshift=1.79cm]
\cgrupic{CGRU}
\end{scope}
\node (s1) at (2.1, -0.5) {};

\begin{scope}[xshift=3.59cm]
\cgruhands{CGRU}
\end{scope}
\node (dots) at (3.9, 1.5) {};


\begin{scope}[xshift=5.39cm]
\cgrupic{\dcgru}
\end{scope}
\node (d0) at (5.7, -0.5) {};

\draw[fill=gray] (7.2, 3.0) rectangle (7.4, 2.8);
\draw[fill=gray!60] (7.2, 2.8) rectangle (7.4, 2.6);
\begin{scope}[xshift=7.19cm]
\cgrupic{\dcgru}
\end{scope}
\node (d1) at (7.5, -0.5) {};

\draw[thick,gray] (6.6, 3.1) -- (6.6, 1.8);

\draw[fill=gray] (9.0, 2.8) rectangle (9.2, 2.6);
\draw[fill=gray!60] (9.0, 2.6) rectangle (9.2, 2.4);
\begin{scope}[xshift=8.99cm]
\cgrupic{\dcgru}
\end{scope}
\node (sp) at (9.3, -0.5) {};

\begin{scope}[xshift=10.79cm]
\cgruhands{\dcgru}
\end{scope}
\node (dots) at (11.1, 1.5) {};

\draw[fill=gray] (12.6, 0.2) rectangle (12.8, 0.0);
\draw[step=0.2] (12.59, 0) grid (13.2, 3.0);
\node (dn) at (12.9, -0.5) {};

\draw (7.5, 3.6) rectangle (8.2, 4.0);
\node (m) at (7.85, 3.8) {};
\draw[->] (7.3, 2.7) -- (7.7, 3.6);
\draw[->] (7.9, 3.6) -- (8.25, 3.45);

\draw (9.3, 3.6) rectangle (10.0, 4.0);
\node (m) at (9.65, 3.8) {};
\draw[->] (9.1, 2.5) -- (9.5, 3.6);
\draw[->] (9.7, 3.6) -- (10.05, 3.45);

\draw[->] (7.3, 2.9) -- (7.3, 3.25);
\node(o1) at (7.3, 3.4) {};
\draw[->] (9.1, 2.7) -- (9.1, 3.25);
\node(o2) at (9.1, 3.4) {};
\node (dots) at (11.1, 3.4) {};
\draw[->] (12.7, 0.1) -- (12.7, 3.25);
\node(o2) at (12.7, 3.4) {};

\node(p0) at (6.6, 3.4) {};
\draw[thick,gray,->] (6.6, 3.2) -- (6.6, 1.8);
\draw[thick,gray] (6.8, 3.4) -- (7.1, 3.4);

\draw[thick,gray] (7.5, 3.4) -- (8.2, 3.4);
\node(p1) at (8.4, 3.4) {};
\draw[thick,gray,->] (8.4, 3.2) -- (8.4, 1.8);
\draw[thick,gray] (8.6, 3.4) -- (8.9, 3.4);

\draw[thick,gray] (9.3, 3.4) -- (10.0, 3.4);
\node(p2) at (10.2, 3.4) {};
\draw[thick,gray,->] (10.2, 3.2) -- (10.2, 1.8);
\draw[thick,gray] (10.4, 3.4) -- (10.7, 3.4);


\draw[thick,gray] (11.4, 3.4) -- (11.7, 3.4);
\node(pn) at (12.1, 3.4) {};
\draw[thick,gray,->] (12.0, 3.2) -- (12.0, 1.8);
\end{tikzpicture}
\end{center}
\caption{Extended Neural GPU with memory module.
  Memory query is read from the position one below the current
  output logit, and the embedded memory value is put at the same
  position of the output tape . The network learns to use these
  values to produce the output in the next step.}
\label{fig:engpu}
\end{figure}


\section{Related Work}

\paragraph{Memory in Neural Networks.}
Augmenting neural networks with memory has been heavily studied recently.
Many of these approaches design a memory component that is intended as a generalization
of the memory in standard recurrent neural networks.  In recurrent networks, 
the state passed from one time step to the next can be interpreted as the 
network's memory representation of the current example.  Moving away from this
fixed-length vector representation of memory to a larger and more versatile form 
is at the core of these methods.

Augmenting recurrent neural networks with attention~\citep{bahdanau2014neural} can
be interpreted as creating a large memory component that allows content-based 
addressing.  More generally, \citet{ntm14} augmented a 
recurrent neural network with 
a computing-inspired memory component that can be addressed via both
content- and address-based queries.  \citet{endend_mem_net} present
a similar augmentation and show the importance of allowing
multiple reads and writes to memory between inputs.  These approaches
excel at tasks where it is necessary to store large parts of a
sequential input in a representation that can later be precisely queried.
Such tasks include algorithmic sequence manipulation tasks, natural language
modelling, and question-answering tasks.

The success of these approaches hinges on making the memory component fully 
differentiable and backpropagating signal through every access of memory.
In this setting, computational requirements necessitate that the memory be small.
Some attempts have been made at making hard access queries to memory~\citep{reinforceNTM15, xuetal2015}, but it was usually
challenging to match the soft version. Recently, more successful
training for hard queries was reported \citep{hardntm} that makes
use of a curriculum strategy that mixes soft and hard queries at
training time.
Our approach applies hard access as well,
but we encourage the model to make good queries via a special memory loss.

Modifications to allow for large-scale memory in neural networks have been 
proposed.  The original implementation of memory networks~\citep{mem_nets}
and later work on scaling it~\citep{large_mem_nets, hier_mem_nets} 
used memory with size in the millions.  The cost of doing so is that the
memory must be fixed prior to training.  Moreover, since during the beginning 
of training the model is unlikely to query the memory correctly, strong supervision
is used to encourage the model to query memory locations that are useful.
These hints are either given as additional supervising information by the 
task or determined heuristically as in \citet{goldilocks}.

All the work discussed so far has either used a memory that is fixed before training
or used a memory that is not persistent between different examples.
For one-shot and lifelong learning, a memory must necessarily be both volatile
during training and persistent between examples.  To bridge this gap, \citet{santoro16} propose to partition
training into distinct episodes consisting of a sequence of labelled 
examples .  A network augmented
with a fully-differentiable memory is trained to predict  given 
the previous sequence . This way,
the model learns to store important examples with their
corresponding labels in memory and later re-use this information to correctly classify new examples.
This model successfully exhibits one-shot learning on Omniglot.

However, this approach again requires fully-differentiable memory access and thus limits
the size of the memory as well as the length of an episode.  
This restriction has recently been alleviated by \citet{jack_rae}.
Their model can utilize large memories, but unlike our work
does not have an explicit cost to guide the formation of memory keys.

For classification tasks
like Omniglot, it is easy to construct short episodes so that they include
a few examples from each of several classes.  However, this becomes harder as 
the output becomes richer.  For example, in the difficult
sequence-to-sequence tasks which we consider,
it is hard to determine which examples would be helpful for correctly 
predicting others \emph{a priori}, and so constructing short episodes each 
containing examples that are similar and act as hints to each other is intractable.


\paragraph{One-shot Learning.}
While the recent work of \citet{santoro16} succeeded in bridging the gap between
memory-based models and one-shot learning, the field of one-shot learning
has seen a variety of different approaches over time.

Early work utilized Bayesian methods to model data generatively~\citep{feifei, oneshot}.
The paper that introduced the Omniglot dataset~\citep{oneshot} approached 
the task with a generative model for strokes.  This way, given a
single character image, the probability of a different image
being of the same character may be approximated via standard techniques.
One early neural network approach to one-shot learning was given by Siamese networks~\citep{siamese}.
When our approach is applied to the Omniglot image classification dataset, 
the resulting training algorithm is actually similar to that of Siamese networks.  
The only difference is in the loss function: Siamese networks utilize a
cross-entropy loss whereas our method uses a margin triplet loss.

A more sophisticated neural network approach is given by~\citet{matching}.
The strengths of this approach are (1) the model architecture utilizes
recent advances in attention-augmented neural networks for set-to-set
learning~\citep{set2set}, and (2) the training algorithm is designed
to exactly match the testing phase (given  distinct images and an
additional image, the model must predict which of the  images 
is of the same class as the additional image).
This approach may also be considered as a generalization of previous work
on metric learning.


\section{Experiments}

We perform experiments using all three architectures described above.
We experiment both on real-world data and on synthetic tasks that give
us some insight into the performance and limitations of the memory module.
In all our experiments we use the Adam optimizer \citep{adam} and the
parameters for the memory module remain unchanged ().
Good performance with a single set of parameters shows the versatility
of our memory module. The source code for the memory module, together
with our settings for Omniglot, is available on github\footnote{
\url{https://github.com/tensorflow/models/tree/master/learning_to_remember_rare_events}}.

\paragraph{Omniglot.}
The Omniglot dataset~\citep{oneshot} consists of 1623 characters from
50 different alphabets, each hand-drawn by 20 different people.
The large number of classes (characters) with relatively few data per class
(20), makes this an ideal data set for testing one-shot classification.
In the -way Omniglot task setup we pick  unseen character classes,
independent of alphabet. We provide the model with one drawing of each character
and measure its accuracy the -th time it sees the character class.
Our setup is identical to \cite{matching_nets}, so we also augmented
the data set with random rotations by multiples of  degrees and use 
characters for training, and the remaining character classes for evaluation.
We present the results from \cite{matching_nets} and ours in Table~\ref{tab:omniglot}.
Even with a simpler network without batch normalization, we get similar results.

\begin{table}\caption{Results on the Omniglot dataset.  Although our model uses only a simple
convolutional neural network, the addition of our memory module allows it to approach
much more complex models on 1-shot and multi-shot learning tasks.}
\begin{center}
\begin{tabular}{|l||c|c|c|c|}
\hline
{\bf Model}                 & 5-way 1-shot & 5-way 5-shot & 20-way 1-shot & 20-way 5-shot \\ \hline
Pixels Nearest Neighbor     &  41.7\%      & 63.2\%       & 26.7\%        & 42.6\% \\
MANN (no convolutions)      &  82.8\%      & 94.9\%       & --            & -- \\
Convolutional Siamese Net   &  96.7\%      & 98.4\%       & 88.0\%        & 96.5\% \\
Matching Network            &  98.1\%      & 98.9\%       & 93.8\%        & 98.5\% \\
\hline
ConvNet with Memory Module  &  98.4\%      & 99.6\%       & 95.0\%        & 98.6\% \\
\hline
\end{tabular}
\end{center}
\label{tab:omniglot}
\end{table}

\paragraph{Synthetic task.}
To better understand the memory module operation and to test what
it can remember, we devise a synthetic task
and train the Extended Neural GPU with and without memory
(we use a small Extended Neural GPU with  channels and memory of size half a million).

To create training and test data for our synthetic task,
we use symbols from the set 
and first fix a random function . The function  is
chosen at random, but fixed and the same for all training and testing examples (we used K training examples).

In our synthetic task, the input is a sequence consisting of
As and Bs with one continuous substring of  digits from
the set . The substring is interpreted as a number
written in base-, e.g., , so the string
 would be interpreted as . The corresponding
output is created by copying all As and Bs, but mapping
the number through the random function . For instance,
assuming , the output corresponding to 
 would be  as .
Here is an example of an input-output pair:

\begin{center}
\begin{tabular}{|c||c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
{\bf Input}  & A & 0 & 1 & 3 & 2 & 3 & 3 & 2 & B & A & B & A & B \\ \hline
{\bf Output} & A & 0 & 3 & 2 & 2 & 0 & 3 & 2 & B & A & B & A & B \\ \hline
\end{tabular}
\end{center}

This task clearly requires memory to store the fixed random function.
Since there are 16K elements to learn, it is hard to memorize,
and each single instance occurs quite rarely.
The raw Extended Neural GPU (or any other sequence-to-sequence model)
are limited by their size. With long training, the small model can
memorize some of the sequences, but it is only a small fraction.

Additionally, there is no direct indication in the data what part
of the input should trigger the production of each output symbol.
For example, to produce the first  output in the above example,
the memory key needs to encode all base- symbols from the input.
Not just one or two aligned symbols, but a number of them.
Moreover, it should not encode more symbols or it will not generalize
to the test set. Similarly, a basic nearest neighbor classifier
fails on this task. We use sequences of length up to  during
training, but there are only  relevant symbols. The simple
nearest neighbor by Hamming distance will most probably select
some sequence with similar prefix or suffix of As and Bs,
and not the one with the corresponding base- part.
We also trained a large sequence-to-sequence model with attention
on this task (a 2-layer LSTM model with 256 units in each layer).
This model can memorize the whole training set, but it suffers from
a similar problem as the Hamming nearest neighbor -- it almost doesn't
generalize, its accuracy on the test set is only about .
The same model with a memory module generalizes much better,
reaching over  accuracy. The Extended Neural GPU with our memory module yields even better results, see Table~\ref{tab:synthetic}.

\begin{table}\caption{Results on the synthetic task.
  We report the percentage of fully correct sequences
  from the test set, which contains 10000 random examples.
  See text for details.}
\begin{center}
\begin{tabular}{|l||c|c|}
\hline
{\bf Model}                   & Accuracy  \\ \hline
Hamming Nearest Neighbor      & 0.1\%     \\
Baseline Sequence-to-Sequence with Attention & 0.9\% \\
Baseline Extended Neural GPU  & 12.2\%    \\
\hline
Sequence-to-Sequence with Attention and Memory & 35.2\% \\
Extended Neural GPU with Memory Module &  71.3\% \\
\hline
\end{tabular}
\end{center}
\label{tab:synthetic}
\end{table}


\paragraph{Translation.}
To evaluate the memory module in a large-scale setting we use the GNMT model~\citep{gnmt}
extended with our memory module on the WMT14 English-to-German translation task.
We evaluate the model both qualitatively and quantitatively.

On the qualitative side, we note that our memory-augmented 
model can successfully translate rare words like Dostoevsky, unlike the baseline model 
which predicts an identity-mapped \emph{Dostoevsky} for the 
German translation of Dostoevsky.

On the quantitative side, we use the WMT test set.
We find that in terms of BLEU score, an aggregate measure, 
the memory-augmented GNMT is on par
with the baseline GNMT, see Table~\ref{tab:translate}.  

To evaluate our memory-augmented model for
one-shot capabilities we split the test set in two.  
We take the even
lines of the test set (index starting at 0) as a context set and the
odd lines of the test set as the one-shot evaluation set.
While showing the context set to the model, no 
additional training occurs, only memory updates are allowed.
So the weights of the model do not change, but the memory does.
Since the sentences in the test set are highly-correlated to each other
(they come from paragraphs with preserved order),
we expect that if we allow a one-shot capable model to use the context set to update its
memory and then evaluate it on the other half of the test set,
its accuracy will increase.
For our GNMT with memory model, we passed the context set through
the memory update operations 3 times.
As seen in Table~\ref{tab:translate}, the context set indeed
helps when evaluating on the odd lines, increasing
the BLEU score by almost . 
As further indication that our memory module works properly, 
we also evaluate the model after showing the whole test set
as a context set. Note that this is essentially an oracle:
the memory module gets to see all the correct answers,
we do this only to test and debug.
As expected, this increases BLEU score dramatically, by over  points.

\begin{table}\caption{Results on the WMT En-De task.
As described in the text, we split the test set in two (odd lines and even lines) to evaluate the
model on one-shot learning.  Given the even test set, the model
can perform better on the odd test set.  We also see a dramatic improvement when
the model is provided with the whole test set, validating that the memory module
is working as intended.}
\begin{center}
\begin{tabular}{|l||c|c|}
\hline
{\bf Model}   & Full Test & Odd Test \\ \hline
GNMT                                            & 23.25 & 23.17 \\ \hline
GNMT with Memory Module                         & 23.29 & 23.16 \\ \hline
GNMT with Memory Module and Even Test context   & --    & 23.60 \\ \hline
GNMT with Memory Module and Whole Test context  & 31.11 & -- \\ \hline
\end{tabular}
\end{center}
\label{tab:translate}
\end{table}


\section{Discussion}

We presented a long-term memory module that can be used for life-long learning.
It is versatile, so it can be added to different deep learning models and
at different layers to give the networks one-shot learning capability.
Several parts of the presented memory module could be tuned and studied
in more detail. The update rule that averages the query with the correct
key could be parametrized. Instead of returning only the single nearest neighbor
we could also return a number of them to be processed by other layers of
the network. We leave these questions for future research.

The main issue we encountered, though, is that evaluating one-shot learning
is difficult, as standard metrics do not focus on this scenario. In this work,
we adapted the standard metrics to investigate our approach. For example,
in the translation task we used half of the test set as context for the other half,
and we still report the standard BLEU score. This allows us to show that our
module works, but it is only a temporary solution.
Better metrics are needed to accelerate progress of one-shot and life-long learning.
Thus, we consider the present work as just a first step on the way to making
deep models learn to remember rare events through their lifetime.




\bibliography{learning_to_remember}
\bibliographystyle{iclr2017_conference}

\end{document}
