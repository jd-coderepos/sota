\documentclass{article} \usepackage{iclr2020_conference,times}


\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    

\usepackage{amsmath,amsthm,amsfonts}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage{stmaryrd} \usepackage{colonequals}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}\usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage[inline]{enumitem}

\usepackage{nicefrac}       \usepackage{microtype}      

\newcommand{\guokun}[1]{\textbf{\textcolor{blue}{Guokun: #1}}}
\newcommand{\barlas}[1]{\textbf{\textcolor{orange}{Barlas: #1}}}
\newcommand{\ves}[1]{\textbf{\textcolor{green}{Ves: #1}}}

\newtheorem{prop}{Proposition}
\newtheorem{assum}{Assumption}
\newtheorem{lemma}{Lemma}
\newtheorem{cor}{Corollary}
\newtheorem{defi}{Definition}
\hypersetup{
    urlcolor  = NavyBlue,  linkcolor = cyan,      citecolor = purple,    filecolor = magenta    }

\usepackage{amsmath,amsfonts,bm}

\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}
 \newcommand{\mb}[1]{\boldsymbol{\mathbf{#1}}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\mf}[1]{\mathfrak{#1}}
\newcommand{\ms}[1]{\mathscr{#1}}
\newcommand{\mbb}[1]{\mathbb{#1}}

\DeclareMathOperator*{\argmax}{argmax\;}
\DeclareMathOperator*{\argmin}{argmin\;}

\DeclareMathOperator*{\E}{\mathbb{E}}

\DeclarePairedDelimiter\roundbracket{(}{)}
\DeclarePairedDelimiter\squarebracket{[}{]}
\DeclarePairedDelimiter\curlybracket{\{}{\}}
\makeatletter
\def\rbr{\@ifnextchar[{\roundbracket}{\roundbracket*}}
\def\sbr{\@ifnextchar[{\squarebracket}{\squarebracket*}}
\def\cbr{\@ifnextchar[{\curlybracket}{\curlybracket*}}
\makeatother

\newcommand{\dom}{\mathrm{dom}}
\newcommand{\diff}{\mathrm{d}}
\newcommand{\liff}{\ratio\Leftrightarrow}

\newcommand{\set}[1]{\left\{ {#1} \right\}}
\newcommand{\setgiven}[2]{\left\{{#1} \;\middle|\; {#2} \right\}}
\newcommand{\seq}[1]{\left[ {#1} \right]}
\newcommand{\tuple}[1]{\left( {#1} \right)}
\newcommand{\idx}[1]{\llbracket {#1} \rrbracket}
\newcommand{\tr}[1]{\mathrm{tr}\left( {#1} \right)}
\newcommand{\abs}[1]{\left| {#1} \right|}
\newcommand{\norm}[1]{\left\| {#1} \right\|}
\newcommand{\diag}[1]{\mathrm{diag}\left({#1}\right)}
\newcommand{\expon}[1]{e^{ {#1} }}
\newcommand{\expo}[1]{\exp \left( {#1} \right)}
\newcommand{\inner}[2]{\left\langle {#1}, {#2} \right\rangle}
\newcommand{\fdiv}[3][D]{{#1}\left({#2} \;\middle\|\; {#3}\right)}

\newcommand{\mx}[1]{\begin{matrix}#1\end{matrix}}
\newcommand{\pmx}[1]{\begin{pmatrix}#1\end{pmatrix}}
\newcommand{\bmx}[1]{\begin{bmatrix}#1\end{bmatrix}}
\newcommand{\smx}[1]{\begin{smallmatrix}#1\end{smallmatrix}}
\newcommand{\spmx}[1]{\left(\begin{smallmatrix}#1\end{smallmatrix}\right)}
\newcommand{\sbmx}[1]{\left[\begin{smallmatrix}#1\end{smallmatrix}\right]}

\newcommand{\C}{\mbb{C}}
\newcommand{\R}{\mbb{R}}


 
\title{Bridging the domain gap in cross-lingual \\ document classification}

\author{Guokun Lai\thanks{Work completed at Facebook AI} , Barlas Oguz, Yiming Yang, Veselin Stoyanov   \\
	Carnegie Mellon University, Facebook AI  \\
	\texttt{\{guokun,yiming\}@cs.cmu.edu}, \texttt{\{barlaso,ves\}@fb.com} \\
}
\iclrfinalcopy
\begin{document}

\maketitle

\begin{abstract}
The scarcity of labeled training data often prohibits the internationalization of NLP models to multiple languages.  Recent developments in cross-lingual understanding (XLU) has made progress in this area, trying to bridge the language barrier using language universal representations.  However, even if the language problem was resolved,  models trained in one language would not transfer to another language perfectly due to the natural domain drift across languages and cultures.  We consider the setting of semi-supervised cross-lingual understanding, where labeled data is available in a source language (English), but only unlabeled data is available in the target language.  We combine state-of-the-art cross-lingual methods with recently proposed methods for weakly supervised learning such as unsupervised pre-training and unsupervised data augmentation to simultaneously close both the language gap and the domain gap in XLU.  We show that addressing the domain gap is crucial.  We improve over strong baselines and achieve a new state-of-the-art for cross-lingual document classification.
\end{abstract} 
\section{Introduction}
\label{sec:intro}



Recent advances in Natural Language Processing have enabled us to train high-accuracy systems for many language tasks. 
However, training an accurate system still requires a large amount of training data. 
It is inefficient to collect data for a new task and it is virtually impossible to annotate a separate data set for each language. 
To go beyond English and a few popular languages, we need methods that can learn from data in one language and apply it to others.  

Cross-Lingual Understanding (XLU) has emerged as a field concerned with learning models on data in one language and applying it to others. 
Much of the work in XLU focuses on the zero-shot setting, which assumes that labeled data is available in one source language (usually English) and not in any of the target languages in which the model is evaluated. 
The labeled data can be used to train a high quality model in the source language.  
One then relies on general domain parallel corpora and monolingual corpora to learn to `transfer' from the source language to the target language.  
Transfer methods can explicitly rely on machine translation models built from such parallel corpora.  
Alternatively, one can use such corpora to learn language universal representations to produce features to train a model in one language, which one can directly apply to other languages.  
Such representations can be in the form of cross-lingual word embeddings, contextual word embeddings, or sentence embeddings (\cite{ruder2017survey, lample2019cross, schwenk2017learning}).  
Using such techniques, recent work has demonstrated reasonable zero-shot performance for cross-lingual document classification (\cite{schwenk2018corpus}) and natural language inference (\cite{conneau2018xnli}). 

What we have so far described is a simplified view of XLU, which focuses solely on the problem of aligning languages.  
This view assumes that, if we had access to a perfect translation system, and translated our source training data into the target language, the resulting model would perform as well as if we had collected a similarly sized labeled dataset directly in our target language. 
Existing work in XLU to date also works under this assumption.  
However, in real world applications, we must also bridge the domain gap across different languages, as well as the language gap.  
No task is ever identical in two languages, even if we group them under the same label, e.g. `news document classification' or `product reviews'.  
A Chinese customer might express sentiment differently than his American counterpart.  
Or French news might simply cover different topics than English news.  
As a result, any approach which ignores this domain drift will fall short of native in-language performance in real world XLU.

In this paper, we propose to jointly tackle both language and domain transfer.  
We consider the semi-supervised XLU setting, where in addition to labeled data in a source language, we have access to unlabeled data in the target language.  
Using this unlabeled data, we combine the aforementioned cross-lingual methods with recently proposed unsupervised domain adaptation and weak supervision techniques on the task of cross-lingual document classification.  
In particular, we focus on two approaches for domain adaptation.  The first method is based on masked language model (MLM) pre-training (as in \cite{devlin2018bert}) using unlabeled target language corpora.  
Such methods have been shown to improve over general purpose pre-trained models such as BERT in the weakly supervised setting (\cite{lee2019biobert, han2019unsupervised}).  
The second method is unsupervised data augmentation (UDA) (\cite{xie2019unsupervised}), where synthetic paraphrases are generated from the unlabeled corpus, and the model is trained on a label consistency loss.

While both of these techniques were proposed previously, in both cases it is non-trivial to extend them to the cross-lingual setting.  
For instance when performing data augmentation, one could generate paraphrases in either the source or the target language or both. 
We experiment with various approaches and provide guidelines with ablation studies. 
Furthermore, we find that the value of additional labeled data in the source language is limited due to the train-test discrepancy of XLU tasks. 
We propose to alleviate this issue by using self-training technique to do the domain adaptation from the source language into the target language. 
By combining these methods, we are able to reduce error rates by an average 44\% over a strong XLM baseline, setting a new state-of-the-art for cross-lingual document classification. 
\section{Related Work}

\subsection{Cross-lingual understanding}
Cross-lingual document classification was first introduced in \citep{bel2003cross}.
The subsequent work \citep{prettenhofer2010cross} proposes the cross-lingual sentiment classification datasets, and \citep{lewis2004rcv1, klementiev2012inducing, schwenk2018corpus} have extended this to the news domain.  Cross-lingual understanding has also been applied to other NLP tasks, with datasets available in dependency parsing \citep{nivre2016universal}, natural language inference (XNLI) \citep{conneau2018xnli} and question answering (\cite{liu2019xqa}).

Cross-lingual methods gained popularity with the advent of cross-lingual word embeddings (\cite{mikolov2013exploiting}).  Since then, many methods have been proposed to better align the word embedding spaces of different languages (see \cite{ruder2017survey} for a survey).  Recently, more sophisticated extensions have been proposed based on seq2seq training of cross-lingual sentence embeddings (\cite{schwenk2017learning, artetxe2018massively}) and contextual word embeddings pre-trained on masked language modeling, notably multilingual BERT (\cite{devlin2018bert}) and the cross-lingual language model (XLM) of \cite{lample2019cross}.  We use XLM as our baseline representation in all experiments, as it's the current state-of-the-art on the commonly used XNLI benchmark for cross-lingual understanding.


\subsection{Domain adaptation}
Domain adaptation, closely related to transfer learning, has a rich history in machine learning and natural language processing (\cite{pan2009survey}). Such methods have long been applied to document classification tasks \citep{blitzer2007biographies, glorot2011domain, al2017approaches, xu2017cross}.

Domain adaptation for NLP is intimately related to transfer learning and semi-supervised learning (\cite{chapelle2009semi}). Transfer learning has made tremendous advances recently due to the success of pre-training representations using language modeling as a source task (\cite{radford2018improving, peters2018deep, devlin2018bert}).  While such representations trained on large amounts of general domain text have been shown to transfer well generally, performance still suffers when the target domain is sufficiently different than what the models were pre-trained on.  In such cases, it is known that further pre-training the language model on in-domain text is helpful (\cite{howard2018universal, chronopoulou2019embarrassingly}).  It is natural to use unsupervised domain data for this task, when available (\cite{lee2019biobert, han2019unsupervised}).  


The study of weakly supervised learning in language processing is relatively new \citep{johnson2016supervised, yu2018diverse}.  Most recently, \cite{xie2019unsupervised} has introduced an unsupervised data augmentation (UDA) technique to demonstrate improvements in the few-shot learning setting.  Here, we extend this technique to facilitate cross-lingual and cross-domain transfer.
 
\section{Framework}
\label{sec:framework}

In this section, we formally define the problem discussed in this paper and describe the proposed approach in detail. 

\subsection{Problem Formulation}
\label{sec:formulation}

In vanilla zero-shot cross-lingual document classification, it is assumed that we have available a labeled dataset in a source language (English in our case), which we can denote by , where  is the prior distribution of task data in the source language.  It is assumed that no data is available for the task in the target language.  General purpose parallel and monolingual resources are used to train a cross-lingual classifier on the labeled source data, which is then applied to the target language data at test time.

In this work, we also assume access to a large unlabeled corpus in the target language, , which is usually the case in practical applications.  We aim to utilize this domain-specific unlabeled data to learn the best classifier for the data from the target language.  We refer to this setting as \textit{semi-supervised XLU}, although we're still in the zero-shot setting, in that no labeled data is used in the target language.

\subsection{Baseline Approaches}
There are two standard ways to transfer knowledge across the languages in the vanilla zero-shot setting: (1) using a translation system to translate the labeled samples, such as translate-train and translate-test methods, and (2) learning a multilingual embedding system to obtain a language irrelevant representations of the data.  
In this paper, we adopt the second approach as the basic model, and utilize the XLM model \citep{lample2019cross} as our base model, which has been pre-trained by large-scale parallel and monolingual data from various languages. 
Because XLM is a multilingual embedding system, a baseline is obtained by fine-tuning XLM with the labeled set  and directly applying the resulting model to the target language. In the experiments section, we also discuss the combination of the XLM and the translation based approaches.


\subsection{Semi-supervised XLU}
\label{sec:uda}

As argued in Introduction, even with a perfect translation or multilingual embedding system, we still face the domain-mismatch problem.     
This mismatch may limit the generalization ability of the model during testing time. 
To fully adapt the classifier to the target distribution, we explore the following approaches, each of which leverages unlabeled data in the target language in different ways.

\paragraph{Masked Language Model pre-training} BERT \citep{devlin2018bert} and its derivations (such as XLM) are trained on general domain corpora.  It is standard practice to further pre-train to adapt to a particular domain when data is available.  This technique can lead to improved performance for the target domain. We refer to this approach as the MLM pre-training.

However, in the cross-lingual setting, fine-tuning the XLM model in the target language can make the model degenerate in the source language, decreasing its ability to transfer across languages.  Therefore, in this case, we take care to use this method in combination with the translate-train method, which translates all labeled samples into the target language.


\paragraph{Unsupervised Data Augmentation} The second approach is utilizing the state-of-the-art semi-supervised learning technique, Unsupervised Data Augmentation (UDA) algorithm \citep{xie2019unsupervised}, to leverage the unlabeled data. The objective function of UDA can be written as,

where  is an augmented sample generated by a predefined augmentation function . 
The augmentation function can be a paraphrase generation model, or a noising heuristic.  
Here, we use a machine translation system for this purpose.
The UDA loss enforces the classifier to produce label consistent predictions for pairs of original and augmented samples. 

In the cross-lingual setting, there are multiple ways of generating augmented samples using translation.  One could translate samples from the target language into the source language and use this cross-lingual pair as the augmented sample.  Alternatively, one could translate back into the target language and use only target-language augmented samples.  We find that the latter works best.
It is also possible to do data augmentation using source domain unlabeled data.  The results of these comparisons are included in out detailed ablation study in the experiments section. 


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{fig/model.pdf}
    \caption{The process diagram of the proposed framework.}
    \label{fig:process}
\end{figure}

\paragraph{Alleviating the Train-Test Discrepancy of the UDA Method}
With the UDA algorithm, the classifier is able to learn some prior information on the target domain, however it still suffers from the train-test discrepancy. 
During the testing phase, our goal is to maximize the classifier performance on the target language, which can be written as,

where  is the ground-truth label of the sample . 
Upon observing the training objective of the UDA method, Eq. \eqref{eq:uda}, one can see that the data  that feed to model in the training phrase is sampled from three domains: (1) the source domain , (2) the target domain  and (3) the augmented sample domain . 
On the other hand, the testing phrase only processes data from the target domain .

The source and target domain are mismatched, due to differences in language as argued earlier.  Furthermore, the augmented domain, although generated from the target domain, can also be mismatched, due to artifacts introduced by the translation system.  This can be especially problematic, since the UDA method needs diversity in the augmented samples to perform well \citep{xie2019unsupervised}, which trades off against their quality.

We propose to apply the self-training technique \citep{lee2013pseudo} to tackle this problem. 
We first train a classifier based on the UDA algorithm and denote it as , which is the teacher model used to score the unlabeled data  from the target domain.  
Then we fine-tune a new XLM model using the soft classification loss function with the pseudo-labeled data, which is written as,

Follwing this process, we obtain a new classifier trained only based on the target domain, which does not suffer from the train-test mismatch problem. We show that this process provides better generalization ability compared to the teacher model. A process diagram of the final model is presented in figure \ref{fig:process}.













 \section{Experiments}
\label{sec:experiment}

In this section, we present a comprehensive study on two benchmark tasks, cross-lingual sentiment classification, and cross-lingual news classification.

\subsection{Datasets}

\paragraph{Sentiment Classification}
In this task, we test the proposed framework on a sentiment classification benchmark in three target languages, i.e. French, German and Chinese. 
The French, German and English data come from the benchmark cross-lingual Amazon reviews dataset \citep{prettenhofer2010cross}, which we denote as \textit{amazon-fr}, \textit{amazon-de} and \textit{amazon-en}. 
We merge training and testing samples from all product categories in one language, which leads to 6000 training samples. 
However, for the purpose of facilitating fair comparison with previous work, we also provide results for specific categories. 
In addition, we use 54K unlabeled samples from amazon-fr and 310K unlabeled samples from amazon-de.


For Chinese, we use the Chinese Amazon (\textit{amazon-cn}) \citep{zhang2015daily} and Dianping \citep{zhang2014explicit} datasets. Dianping is a business review website similar to Yelp. The training data for amazon-cn is amazon-en, and for dianping it is the Yelp dataset \citep{zhang2015character}. 
In these two cases, the size of the training sample is 2000. For both amazon-cn and dianping datasets, we have 4M unlabeled examples. 
Because the number of the unlabeled set is very large, we randomly sample 10\% for the UDA algorithm. 

\paragraph{News Classification} We use the MLDoc dataset \citep{schwenk2018corpus} for this task. The MLdoc dataset is a subset of RCV2 multilingual news dataset \citep{lewis2004rcv1}. It has 4 categories, i.e.  Corporate/Industrial, Economics, Government/Social and Markets, and each category has 250 training samples. We use the rest of the news documents in RCV2 dataset as the unlabeled data. The number of unlabeled samples for each language ranges from 5K to 20K, which is relatively smaller compared to the sentiment classification task. 

Because the XLM model is pre-trained on 15 languages, we ignore languages which are not supported by XLM in the above benchmark datasets. 

The pre-processing scripts for the above datasets, augmented samples and experiment settings needed to reproduce results are released in the Github repo \footnote{https://github.com/laiguokun/xlu-data}. 

\subsection{Masked language model pre-training strategy}

As introduced in section \ref{sec:uda}, we apply MLM pre-training on the unlabeled data corpus to obtain a domain-specific XLM, denoted as XLM in the following sections. The pre-training strategies for the two tasks are slightly different. 
In the sentiment classification task, because the size of the unlabeled corpus in each target domain is large enough, we fine-tune an XLM with MLM loss for each target domain respectively. 
In contrast, we do not have enough unlabeled data in each language in the MLDoc dataset, therefore we integrate unlabeled data from all languages as the training corpus.
As a result the XLM still preserves its language universality in this task.

\subsection{Main Results}
\label{sec:results}
We compare the follwing models:

\begin{itemize}[leftmargin=*]
    \item Fine-tune (Ft): Fine-tuning the pre-trained model with the source-domain training set.  In the case of XLM, the training set is translated into the target language.
    \item Fine-tune with UDA (UDA): This method utilizes the unlabeled data from the target domain by optimizing the UDA loss function (Eq. \eqref{eq:uda}).
    \item Self-training based on the UDA model (UDA+Self): We first train the Ft model and UDA model, and choose the better one as the teacher model. The teacher model is used to train a new XLM student using only unlabeled data  in the target domain, as described above.
\end{itemize}

We report the results of applying these three methods on both the original XLM model and the XLM model. In order to keep the notation simple, we use parenthesis after the method name to indicate which basic model was used, such as UDA(XLM).  The details about the implementation and hyper-parameter tuning are included in Appendix \ref{app:details}.

The results for the cross-lingual sentiment classification task are summarized in table \ref{tab:sentiment1}. 
As our experiment setting on the cross-lingual amazon dataset is different from previous publications,
in order to provide a fair comparison with previous works, we summarize the results of the standard category-wise setting in table \ref{tab:sentiment2}. 
The results for cross-lingual news classification is included in table \ref{tab:news}. The last column ``Unlabeled'' in these tables indicates whether this method utilizes the unlabeled data. 
For the monolingual baselines, the models are trained with labeled data from the target domain. The size of the labeled set is the same as the English training set used for cross-lingual experiments. 

We can summarize our findings as follows: 
\begin{itemize}[leftmargin=*]
\item Looking at Ft(XLM) results, it is clear that without the help of unlabeled data from the target domain, there still exists a substantial gap between the model performance of the cross-lingual settings and the monolingual baselines, even when using state-of-the-art pre-trained cross-lingual representations.  
\item  Both the UDA algorithm and MLM pre-training can offer significant improvements by utilizing the unlabeled data. 
In the sentiment classification task, where the unlabeled data size is larger, Ft(XLM) model usnig MLM pre-training consistently provides larger improvements compared with the UDA method. 
On the other hand, the MLM method is relatively more resource intensive and takes longer to converge (see Appendix \ref{sec:time}). In contrast, in the MLdoc dataset, when the size of the unlabeled samples is limited, the UDA method is more helpful. 
\item The combination of both methods - as in the UDA(XLM) model - consistently outperforms either method alone. In this case the additional improvement provided by the UDA algorithm is smaller, but still consistent.
\item In the sentiment classification task, we observe the self-training technique consistently improves over its teacher model. 
It offers best results in both XLM and XLM based classifiers. 
The results demonstrate that self-training process is able to alleviate the train-test distribution mismatch problem and provide better generalization ability.
In the MLdoc dataset, self-training also achieves the best results overall, however the gains are less clear.  We hypothesize that this technique is not as useful without enough number of unlabeled samples. 
\item Finally, comparing with the best cross-lingual results and monolingual fine-tune baseline, we are able to completely close the performance gap by utilizing unlabeled data in the target language. 
Furthermore, our framework reaches new state-of-the-art results, improving over vanilla XLM baselines by 44\% on average. 
\end{itemize}

Furthermore, we provide an additional baseline, which only uses English samples to perform semi-supervised learning, whose details are in Appendix \ref{app:english-baseline}. The experment results show that it lags behind the ones using unlabeled data from the target domain. This observation also justifies the importance of information from the target domain in the XLU task. 

\begin{table}[!ht]
\centering
    \begin{tabular}{l|c|cccc|c}
        \toprule
        Base Model & Train    & amazon-en & amazon-en & amazon-en & yelp     & Unlabeled \\
        & Test     & amazon-fr & amazon-de & amazon-cn & dianping &\\
        \midrule
        & Ft       & 11.35     & 12.75     & 15.5     &  12.66    & \xmark\\
        XLM        & UDA      & 9.83      & 10.08     & 11.92      &  8.37   & \cmark\\
        & UDA+Self & 9.03      & 9.32      & 11.19      &  7.8   & \cmark\\
        \midrule
        & Ft       & 7.67      & 6.7       & 11.32      &  6.15   & \cmark\\
        XLM     & UDA      & 7.15      & 6.3       & 11.92      &  5.21   & \cmark\\
        & UDA+Self & \textbf{6.67}      & \textbf{5.77}      & \textbf{9.46}       &  \textbf{4.8}    & \cmark\\
        \midrule
        \midrule
        \multicolumn{7}{c}{Monolingual Baselines}  \\
        \midrule
        XLM & Ft & 8.32 & 10.67 & 11.69 & 8.73 & \xmark \\
        \midrule 
        XLM & UDA &  5.95 & 6.12 & 7.74 & 4.64 & \cmark \\ 
        \bottomrule
    \end{tabular}
    \caption{Error rates for the sentiment classification task.}
\label{tab:sentiment1}
\end{table}

\begin{table}[!ht]
\centering
    \begin{tabular}{l|c|ccccc|c}
        \toprule
        Base Model & Train         &       &       & en    &       &       & Unlabeled \\
        & Test          & fr    & de    & es    & zh    & ru    &           \\
        \midrule
        & pre-XLM sota & 21.97 & 13.75 & 20.7  & 25.27 & 33.22 & \xmark         \\
        \midrule
        & Ft            & 7.95  & 6.03  & 12.08 & 11.95 & 26.95 & \xmark          \\
        XLM        & UDA           & 4.97  & 4.3   & 8.67  & 9.85  & 27.22 & \cmark          \\
        & UDA+Self      & 4.75  & 4.35  &  8.78     & 9.7   & 27.68 & \cmark          \\
        \midrule
        & Ft            & 6.3   & 6.43  & 8.7   & 11.02 & 25.78 & \cmark          \\
        XLM     & UDA           & 4.65  & 4.63  & 6.97  & \textbf{7.15}  & 16.5  & \cmark          \\
        & UDA+Self      & \textbf{4.6}   & \textbf{4.27}  & \textbf{6.53}      & \textbf{7.15}  & \textbf{15.65} & \cmark  \\
        \midrule
        \midrule
        \multicolumn{8}{c}{Monolingual Baselines}  \\
        \midrule
        XLM & Ft & 5.35 & 3.77 & 3.95 & 7.85 & 11.05 &  \xmark \\
        \midrule 
        XLM & UDA &  3.95 & 3.05 & 3.2 & 6.68 & 10.3 &\cmark \\ 
        \bottomrule    
    \end{tabular}
    \caption{Error rates for news document classification. The pre-XLM sota results are provided by \cite{artetxe2018massively}}
\label{tab:news}
\end{table}

\begin{table}[!ht]
\centering
    \begin{tabular}{l|c|ccc|ccc|c}
        \toprule
        Base Model & dataset         & \multicolumn{3}{|c|}{amazon-fr} & \multicolumn{3}{|c|}{amazon-de}  & Unlabeled         \\
        & category        & book & dvd    & music & book & dvd    & music & \\
        \midrule
        & pre-XLM sota   & 14      & 14.3      & 14       & 13.6    & 13.9      & 12.2 &  \cmark  \\
        \midrule
        XLM & UDA+Self   & 7.75    & 8.7       & 9.35     & 8.3     & 10        & 9.96 &  \cmark  \\
        XLM & UDA+Self & \textbf{6.5}     & \textbf{7.05}      & \textbf{7.15}     & \textbf{4.8}     & \textbf{7.1}       & \textbf{5.55} & \cmark \\    
        \bottomrule
    \end{tabular}
    \caption{Error rates for the sentiment classification task by product category. The pre-XLM sota results are provided by \cite{chen2019emoji}.}
\label{tab:sentiment2}
\end{table}

\subsection{Labeled Data in the Source Language has Limited Value}

In this section, we provide evidence for the train-test domain discrepancy in the context of the UDA method, by showing that adding more labeled data in the source language does not improve target task accuracy after a certain point.
Figure \ref{fig:num_tr} plots the target model performance vs. the number of labeled training samples in the cross-lingual and monolingual settings respectively. 
The figures are based on the UDA(XLM) method with 6 runs in the Yelp-Dianping cross-lingual setting. 
The dot is the average accuracy and the filling area contains one standard derivation. 

We observe that, in the cross-lingual setting, the model performance peaks at around 10k training samples per category, and becomes worse with the larger training set. 
In contrast, the performance of the model improves consistently with more labeled data in the monolingual setting.  
This suggests that more training data from the source domain could harm model generalization ability in the target domain with UDA approach in the cross-lingual setting. 
In order to alleviate this issue, we propose to utilize the self-training technique, which abandons the data from the source domain and the augmentation domain, to maximize its performance in the target domain.


\begin{figure}
    \centering
    \includegraphics[width=0.4\linewidth]{fig/ablation.png}
    \includegraphics[width=0.4\linewidth]{fig/ablation1.png}
    \caption{Plot of the accuracy vs. the number of training samples in log scale when running UDA(XLM) method. On the left is the cross-lingual setting. On the right is the monolingual setting.}
    \label{fig:num_tr}
\end{figure}

\subsection{Ablation study: Augmentation Strategies}

Next, we explore different augmentation strategies and their influence on the final performance. 
As stated in section \ref{sec:uda}, the augmentation strategy used in the main experiment is that we first translate the samples into English and translate them back to its original language.  We refer to this strategy as augmenting "from target domain to target domain" and abbreviate it as \textbf{t2t}.
 
We also explore two additional augmentation strategies:
(1) First, we do not translate the samples back to the target language and directly use English samples as the augmented samples, denoted as \textbf{t2s}.  
Naturally, the parallel samples in two languages have the same sentiment information and different input format which are suitable to be used as the augmentation sample pairs for the multilingual system such as XLM.
(2) The second approach is to leverage unlabeled data from other language domains. 
Here, we attempt to use the English unlabeled data. We translate them into the target language as the augmented samples. This strategy is denoted as \textbf{s2t}.


\begin{table}[!ht]
\centering
    \begin{tabular}{l|ccc||ccccc}
    \toprule
    train  &          & amazon-en &          &       \multicolumn{5}{c}{MLDoc en}          \\
    test   & amazon-fr & amazon-de & amazon-cn & fr   & de   & es   & zh   & ru    \\
    \midrule
    \textbf{t2t}       & 9.83     & \textbf{10.08}    & 11.92    & \textbf{4.65} & 4.63 & \textbf{6.97} & \textbf{8.15} & \textbf{16.5}  \\
    \textbf{t2s}       & 10.97    & 13.15    & 15.07    & 5.03 & 5    & 9.38 & 8.35 & 27.9  \\
    \textbf{t2t} + \textbf{t2s} & 9.82     & 12.07    & 11.93    & 5.07 & \textbf{4.33} & 7.15 & 8.9  & 28.6  \\
    \midrule
    \textbf{s2t}       & 9.32     & 10.38    & 12.28    & 5.53 & 6.25 & 7.73 & 8.5  & 29.17 \\
    \textbf{t2t} + \textbf{s2t} & \textbf{8.98}     & 10.87    & \textbf{11.79}    & 4.85 & 4.4  & 7.75 & 8.27 & 30.33 \\
    \bottomrule
    \end{tabular}
    \caption{Error rates when using different augmentation strategies and their combinations. Results for sentiment classification shown on the left, and news document classification on the right.}
    \label{tab:aug}
\end{table}

Table \ref{tab:aug} summarizes the performance of the proposed augmentation strategies and their combinations with the UDA(XLM) method in the sentiment classification and the UDA(XLM) in the news classification settings.
From the results, we conclude that \textbf{t2t} is the best performing approach, as it's the best matched to the target domain. Leveraging the unlabeled data from other domains does not offer consistent improvement, however can provide additional value in isolated cases. 

We include additional ablations regarding translation system in the appendix, including the application of translate-train method in our experiments (section \ref{app:ablationtranslate}) and effects of hyper-parameters (section \ref{app:temp}). 
 \section{Conclusion}

In this paper, we tackled the domain mismatch challenge in cross-lingual document classification - an important, yet often overlooked problem in cross-lingual understanding.  We provided evidence for the existence and importance of this problem, even when utilizing strong pre-trained cross-lingual representations.

We proposed a framework combining cross-lingual transfer techniques with three domain adaptation methods; unsupervised data augmentation, masked language model pre-training and self-training, which can leverage unlabeled data in the target language to moderate the domain gap.  Our results show that by removing the domain discrepancy, we can close the performance gap between cross-lingual transfer and monolingual baselines almost completely for the document classification task.  We are also able to improve the state-of-the-art in this area by a large margin.  

While document classification is by no means the most challenging task for XLU, we believe the strong gains that we demonstrated will encourage the community to pay more attention to domain drift in the cross-lingual setting.  Developing cross-lingual methods which are competitive with in-language models for real world, semantically challenging NLP problems remains an open problem and subject of future research. \bibliography{iclr2020_conference}
\bibliographystyle{iclr2020_conference}
\appendix

\section{Appendix of Cross-ligual Experiments}


\subsection{Implementation Details}
\label{app:details}

The experiments in this paper are based on the PyTorch \citep{paszke2017automatic} and Pytext \citep{aly2018pytext} package. We use the Adam \citep{kingma2014adam} as the optimizer. 
For all experiments, we grid search the learning rate in the set . 
When using UDA method, we also try the three different annealing strategies introduced in the UDA paper \citep{xie2019unsupervised}, and the  in \eqref{eq:uda} is always set as 1. 
The batch size in the Ft and UDA+Self method is 128. In the UDA method, the batch size is 16 for the labeled data and 80 for the unlabeled data. 
Due to the limitation of the GPU memory, in all experiments, we set the length of samples as 256, and cut the input tokens exceeding this length. 
Finally, we report the results with the best hyper-parameters.

As for the augmentation process, we sweep the temperature which controls the diversity of beam search in translation. 
The best temperature for ``en-de, en-fr, en-es'' and  ``en-ru'' are 1.0 and 0.6, the sampling space is the whole vocabulary. 
In the ``en-zh'' setting, the temperature is 1.0 and the sampling space is the top 100 tokens in the vocabulary. 
We note that this uses the Facebook production translation models, and results could vary when other translation systems are applied.  For reproducibility, we will release the augmented datasets that we generated.

\subsection{Ablation Study: The Baseline with English Unlabeled Data}
\label{app:english-baseline}

Here, we provide a baseline only using English samples to perform semi-supervised learning. More specifically, we first train the model with English unlabeled data and augmented samples, then tests it on different target domains. 
This approach is similar to the traditional translate-test method.
This method offers a baseline, which merely increases the size of data but without providing the target domain information. 
During the test phrasing, we experiment with two input strategies. One is using the original test samples, and another is translating the samples into English. 
We report the results (Table \ref{tab:trtest}) of the UDA(XLM) method with two input strategies and compare them with the main results, which uses the unlabeled data from the target domain. 
First, we observe that the performance of using original and translated samples is similar. 
Second, compared with Ft(XLM) baselines in section \ref{sec:results}, utilizing the unlabeled data from the English domain is slightly better than only training with labeled data, but it still lags behind the performance of using the unlabeled data from the target domain.  

\begin{table}[!ht]
\centering
    \resizebox{1.\textwidth}{!}{
    \begin{tabular}{lc|ccc||ccccc}
    \toprule
    lang of test  & Unlabeled & amazon-fr & amazon-de & amazon-cn & fr   & de   & es   & zh   & ru    \\
    \midrule
    original & English & 10.57    & 12.98    & 13.73    & 6.47 & 6.13 & 12.35 & 11.6  & 27.52 \\
    translated & English & 10.68    & 12.4     & 15.21    & 6.2  & 6.2  & 11.88 & 10.12 & 31.37 \\
    \midrule
    \midrule
    original & Target & \textbf{9.83} & \textbf{10.08} & \textbf{11.92} & \textbf{4.97} & \textbf{4.3} & \textbf{8.67} & \textbf{9.85} & \textbf{27.22} \\
    \bottomrule
    \end{tabular}
    }
    \caption{The first part is the baseline results using the English unlabeled data. The second part is the results using the unlabeled data from the target domain, which are copied from the section \ref{sec:results}. }
    \label{tab:trtest}
\end{table}


\subsection{Ablation study: Translate-Train}
\label{app:ablationtranslate}

As discussed earlier, fine-tuning XLM on the target language would depreciate the multilingual ability of the model. We apply the translate-train method to tackle this problem. In order to understand the influence of this strategy when using the proposed framework, we perform an ablation study. We test 3 input strategies: (1) English: use the original English data as training data. (2) tr-train: use the translate-train strategy, which translate the training data into the target language. (3) both: we combine the (1) and (2) as the training data. We report the results of the UDA(XLM) method in the sentiment classification tasks and UDA(XLM) method in the news classification tasks in table \ref{tab:trt-lang}.

\begin{table}[!ht]
\centering
    \begin{tabular}{l|ccc||ccccc}
    \toprule
    train &           & amazon-en      &           &      \multicolumn{5}{c}{MLDoc en}       \\
    test  & amazon-fr & amazon-de & amazon-cn & fr   & de   & es   & zh   & ru    \\
    \midrule
    English   & 9.83      & \textbf{10.08}     & \textbf{12.02}     & \textbf{4.75} & \textbf{4.63} & 8.07 & \textbf{8.15} & 28.28 \\
    tr-train   & 9.92      & 11.7      & 15.31     & 5.57 & 5    & 6.97 & 9.2  & \textbf{16.5}  \\
    both  & \textbf{9.75}      & 11.02     & 16.02     & 4.95 & 4.87 & \textbf{6.33} & 8.35 & 28.63 \\
    \bottomrule
    \end{tabular}
    \caption{Ablation Study about the translate-train strategies. Results for sentiment classification shown on the left, and news document classification on the right.}
    \label{tab:trt-lang}
\end{table}

We observe that in most cases, using the original training examples achieves the best performance. However, in special cases such as MLDoc-ru, the translate-train method achieves better performance. We recommend trying both approaches in practice. 




\subsection{Ablation study: diversity in beam decoding of data augmentation}
\label{app:temp}

Given a translation system, we use the sample decoding strategy to translate the sample. 
The sample space is the entire vocabulary space. We tune the temperature of  of the softmax distribution. 
As discussed in \cite{xie2019unsupervised}, this controls the trade-off between quality and diversity. 
When , the sampling reduces to the greedy search and produce the best quality samples.
When , the sampling produces diverse outputs but loses some semantic information. 
The table \ref{tab:temperature} illustrates the influence of  value to the final performance in the English-to-French and English-to-German settings. 
The results show that temperature has a significant influence on the final performance. 
However, because the quality of translation systems for different language pairs are not the same, their best temperature also varies. 
In the appendix \ref{app:details}, we include the best temperature values for the translation systems used in this paper.  

\begin{table}[!ht]
\centering
    \begin{tabular}{l|cccc}
    \toprule
     & amazon-fr & amazon-de & mldoc-fr & mldoc-de \\
    \midrule
    0.6  & 11.92    & 13.27   & 8.95    & 7.4     \\
    0.8  & \textbf{9.63}     & 10.6    & 5.55    & 4.7     \\
    1.0    & 9.83     & \textbf{10.08}   & \textbf{4.97}    & \textbf{4.3}     \\
    1.2  & 10.83    & 13.58   & 7.87    & 5.5    \\
    \bottomrule
    \end{tabular}
    \label{tab:temperature}
    \caption{Effect of the temperature of the translation sampling decoder.}
\end{table}


\subsection{Computation Time of UDA and MLM Pretraining}
\label{sec:time}

From the main results in section \ref{sec:results}, we can see that MLM pre-training can offer better improvements over the UDA method. However, it is also more resource intensive, since MLM pre-training is a token level task with a large output space, which leads to more computationally intensive updates and also takes longer to converge.
In our experiments, we used NVIDIA V100-16G GPUs to train all models. 8 GPUs were used to train Ft and UDA methods, and 32 GPUs to perform MLM pretraining. In the "amazonen->amazonfr" setting, for example, the unlabeled set contains 50K unlabeled samples and 8M tokens after BPE tokenization. The Ft method takes 3.2 GPU hours to converge. The UDA method training takes 16.8 GPU hours, excluding the time it takes to generate augmented samples (which we handle as part of data pre-processing). MLM pre-training takes upwards of 500 GPU hours to converge.  This is another factor which should be taken into account.


\section{Monolingual domain adaptation}

As further evidence that our method addresses the domain mismatch, we apply out framework to the monolingual cross-domain document classification problem. We again focus on sentiment classification where data comes from two different domains, product reviews (amazon-en, amazon-cn)  and business reviews (Yelp and Dianping). We train and test on the same language, only transferring across domains. We consider the two domain-pairs, amazonen-yelp and amazoncn-dianping. The results are illustrated in table \ref{tab:cross-domain}.  Conclusions are similar to the cross-domain setting (section \ref{sec:results}):
\begin{itemize}[leftmargin=*]
    \item There exists a clear gap between the cross-domain and in-domain results of the Ft method, even when using strong pre-trained representations. 
    \item By leveraging the unlabeled data from the target domain, we can significantly boost the model performance.
    \item Best results are achieved with our combined approach and almost completely matches the in-domain baselines.
\end{itemize}

\begin{table}[!ht]
\centering
    \begin{tabular}{l|c|cccc|c}
    \toprule
    Base Model & Train    & amazon-en & yelp     & amazon-cn & dianping  & Unlabeled \\
          & Test     & yelp     & amazon-en & dianping & amazon-cn  &\\
    \midrule
          & Ft       & 8.46     & 14.52    & 12.86    & 14.96    & \xmark \\
    XLM   & UDA      & 5.43     & 10.1     & 8.31     & 11.19    & \cmark \\
          & UDA+Self & 4.94     & 9.58     & 8.71     & 10.69    & \cmark \\
    \midrule
          & Ft       & 4.51     & 10.8     & 7.06     & 9.96     & \cmark \\
    XLM & UDA      & 3.78     & 7.87     & 5.54     & \textbf{7.38}     & \cmark \\
          & UDA+Self & \textbf{ 3.36}     & \textbf{7.47}     & \textbf{5.34}     & 7.4      & \cmark \\
    \midrule
    \midrule
    \multicolumn{7}{c}{In-Domain Baselines}  \\
    \midrule
    XLM & Ft & 5.8 & 10.9 & 11.43  & 12.31 & \xmark \\
    \midrule 
    XLM & UDA &  3.34 & 7.57 & 4.64 & 7.74 & \cmark \\ 
    \bottomrule
    \end{tabular}
    \caption{Error rates for cross-domain document classification}
    \label{tab:cross-domain}
\end{table} \end{document}
