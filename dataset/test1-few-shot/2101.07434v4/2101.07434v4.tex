









\documentclass[journal]{IEEEtran}








\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{float}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{xcolor}

\usepackage{caption}
\usepackage{subcaption}















\ifCLASSINFOpdf
\else
\fi



















































\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
\title{Channelized Axial Attention for Semantic Segmentation}


\author{Ye~Huang,~\IEEEmembership{Member,~IEEE,}
        Wenjing~Jia,~\IEEEmembership{Member,~IEEE,}
        Xiangjian~He,~\IEEEmembership{Member,~IEEE,}
        Liu~Liu,~\IEEEmembership{Member,~IEEE,}
        Yuxin~Li,
        and~Dacheng~Tao,~\IEEEmembership{Fellow,~IEEE}}





\markboth{Channelized Axial Attention for Semantic Segmentation}{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}














\maketitle

\begin{abstract}
Self-attention and channel attention, modelling the semantic interdependencies in spatial and channel dimensions respectively, have recently been widely used for semantic segmentation.
However, computing spatial-attention  and channel attention separately and then fusing them directly can cause conflicting feature representations. 
In this paper, we propose the Channelized Axial Attention (CAA) to seamlessly integrate channel attention and axial attention with reduced computational complexity. 
After computing axial attention maps, we propose to channelize the intermediate results obtained from the transposed dot-product so that the channel importance of each axial representation is optimized across the whole receptive field. 
We further develop grouped vectorization, which allows our model to be run with very little memory consumption at a speed comparable to the full vectorization. 
Comparative experiments conducted on multiple benchmark datasets, including Cityscapes, PASCAL Context and COCO-Stuff, demonstrate that our CAA not only requires much less computation resources compared with other dual attention models such as DANet~\cite{cDualAttention}, but also outperforms the state-of-the-art ResNet-101-based segmentation models on all tested datasets.
\end{abstract}

\begin{IEEEkeywords}
Semantic Segmentation, Axial Attention, Channelization, Grouped Vectorization.
\end{IEEEkeywords}






\IEEEpeerreviewmaketitle



\section{Introduction}
\label {sIntroduction}



\IEEEPARstart{S}{emantic} segmentation is a fundamental task in many computer vision applications, which assigns a class label to each pixel in the image. 
Most of the existing approaches for semantic segmentation (\textit{e.g.},~\cite{cDeepLab,cDeepLabV3,cOCR,cDenseASPP,cDualAttention,cEMANet}) have adopted a pipeline similar to the one that is defined by Fully Convolutional Networks (FCNs)~\cite{cFCN} using fully convolutional layers to output pixel-level segmentation results of the input image, and have achieved state-of-the-art performance. After the FCN approach, there have been many approaches dedicated to extracting enhanced pixel representations from the backbone. 
Earlier approaches, including PSPNet~\cite{cPSPNet} and DeepLab~\cite{cDeepLabV3Plus}, used a Pyramid Pooling Module (PPM) or an Atrous Spatial Pyramid Pooling (ASPP) module to expand the receptive field and capture multiple-range information to enhance the representation capabilities. 

\begin{figure}[t]
	\centering
	\includegraphics[width=3in]{sample1.pdf}
	\caption{Different dual attention designs}
\label{fig0}
\end{figure}

The latest research works on segmentation head in the past few years have mainly focused on using the attention mechanisms to improve the performance. 
During the early days of the attention mechanisms, the Squeeze and Excitation Networks (SENets)~\cite{cSENet} introduced a simple and yet efficient channel attention module to explicitly model the interdependencies between channels. 
Meanwhile, the Non-Local Networks in~\cite{cNonLocal} proposed self-attention to model long-range dependencies in spatial domain, so as to produce more correct pixel representations. Thus, for each pixel in the feature maps, spatial self-attention makes its representation more similar to the representations of the pixels that are closer, whereas channel attention finds important channels in the entire feature maps and applies different weights to the extracted features.

To enjoy the advantages of both spatial attention and channel attention, some approaches (\textit{e.g.},~\cite{cDualAttention}) proposed to directly fuse their results with an element-wise addition (see Fig.~\ref{fig0}(a)). 
Although they have achieved improved performance, the relationship between the contributions of spatial self-attention and channel attention to the final result is unclear. 

Moreover, calculating the two attentions separately not only increases the computational complexity, but may also result in conflicting importance of feature representations. 
For example, some channels may appear to be important in channel attention for a pixel that belongs to a partial region in the feature maps, But spatial attention may have its own perspective, which is calculated by summing up the similarities over the entire feature maps, and weakens the impact of the channel attention. 

The existing designs (\textit{e.g.}~\cite{cCBAM}) combining channel attention and spatial attention in a cascaded, sequential manner (Fig.~\ref{fig0}(a)) have similar issues. 
Channel attention can \textbf{ignore} the partial region representation obtained from the overall perspective, which may be required by spatial attention. 
Thus, directly fusing the spatial attention results with channel attention results may yield incorrect importance weights for pixel representations. 
In the Experiments section of this paper, we develop an approach to visualize the impact of the conflicting feature representation on the final segmentation results. 

Attempting to combine the advantages of spatial-attention and channel attention seamlessly and efficiently in a complementary way, we propose a Channelized Axial Attention (CAA), which is based on a redefined axial attention to reduce the computation cost of self-attention. 
Specifically, when applying the redefined axial attention maps to the input signal~\cite{cNonLocal}, we capture the intermediate results of the dot product before they are summed up along the corresponding axes. 
Capturing these intermediate results allows channel attention to be integrated for each column and each row, instead of computing on the mean or sum of the features in the entire feature maps.
More importantly, when applying the attention maps, we propose a novel transposed approach, which allows the channel attention to be conducted in the whole receptive field.
Last but not the least, we develop a novel grouped vectorization approach to maximize the computation speed under limited GPU memory.


In summary, our contributions of this paper include:
\begin{itemize}
\item We propose a novel Channelized Axial Attention, which integrates spatial self-attention with channel attention seamlessly and efficiently and significantly boosts the segmentation performance with only minor computation overhead of the original axial attention. 
\item We develop a novel approach to visualize the impact of the conflicting pixel representation of the existing dual attention designs on segmentation.
\item To balance the computation speed and GPU memory usage, we propose a novel grouped vectorization approach to compute the channelized attentions, which is particularly advantageous when processing large images with a limited GPU memory.
	
	\item Extensive experiments on three challenging benchmark datasets, \textit{i.e.}, PASCAL Context~\cite{cPascalVOC}, COCO-Stuff~\cite{cCocoStuff} and Cityscapes~\cite{cCityScapes}, demonstrate the superiority of our approach over the state-of-the-art approaches.
\end{itemize}

Next,  
Sect.~\ref{sRelatedWorks} briefly summarizes the related work. 
Then, we illustrate the details of our proposed approach in Sect.~\ref{sectMethods}. 
Sect.~\ref{sExperements} presents the experiments and ablation studies. 
The paper concludes in Sect.~\ref{secConclusion}.



\section{Related Work}
\label {sRelatedWorks}

Towards using the attention mechanisms to improve the performance of semantic segmentation, many research works have been reported. In this section, we introduce these approaches in the way of their evolution. 

\subsection{Capturing Information from Fixed Ranges}

The PSPNet~\cite{cPSPNet} proposed a PPM, which used multiple average pooling layers with different sizes together to get average pixel representations in multiple receptive fields, and then upsampled and concatenated them together. 
Similarly, the ASPP in DeepLab~\cite{cDeepLab,cDeepLabV3Plus} used parallel atrous convolutions with different rates to capture information from multiple ranges. 
The core ideas of both models are to utilize the surrounding information of each pixel in multiple ranges to achieve better pixel representations. 
Both methods have achieved highest scores in some popular public datasets~\cite{cPascalVOC, cCityScapes}. 
However, as claimed in~\cite{cDenseASPP}, fixed receptive fields may lose important information, to which stacking more receptive fields can be a solution, at the cost of dramatically increased computation.


\subsection{Attention Mechanisms}

\textbf{Spatial Self-Attention.}
Non-Local networks~\cite{cNonLocal} introduced the self-attention mechanism to examine the pixel relationship in spatial domain. 
It usually calculates dot-product similarity or cosine similarity to obtain the similarity measurement between every two pixels in feature maps, and recalculate the feature representation of each pixel according to its similarity with others. 
Spatial self-attention has successfully addressed the feature map coverage issue of multiple fixed-range approaches~\cite{cDeepLab, cPSPNet, cDenseASPP}, but it has also introduced huge computation cost for computing the full feature map. 
This means, for each pixel in the feature maps, its attention similarity concerns all other pixels. 
Recently, many approaches~\cite{cA2Net,cCCNet,cANNN,cAxialAttention} have been developed to optimize the spatial self-attention. They have not only reduced computation and GPU memory costs but also improved the performance.

\textbf{Channel Attention.}
Channel attention~\cite{cSENet} examined the relationships between channels, and enhanced the important channels so as to improve the performance. 
SENets~\cite{cSENet} conducted a global average pooling to get mean feature representations, and then went through two fully connected layers, where the first one had reduced channels and the second one recovered the original channels, resulting in channel-wise weights according to the importance of channels. 
In DANet~\cite{cDualAttention}, channel-wise relationships were modelled by a 2D attention matrix, similar to the spatial self-attention mechanism except that it computed the attention with a dimension of  rather than  ( denotes the number of channels, and  and  denote the height and width of the feature maps, respectively).

\subsection{Spatial Attention + Channel Attention}
\label{secDualAttentionDesign}

Combining spatial attention and channel attention can provide fully optimized pixel representations in a feature map. 
However, it is not easy to enjoy both advantages seamlessly. 
In the DANet~\cite{cDualAttention}, the results of the channel attention and spatial attention are directly added together.
Supposing that there is a pixel belonging to a semantic class that has a tiny region in the feature maps, spatial-attention can find its similar pixels. 
However, channel representation of the semantic class with a partial region of the feature maps may not be important in the perspective of entire feature maps, so it may be ignored when conducting channel attention computation. 
Computing self-attention and channel attention separately (as illustrated in Fig.~\ref{fig0}(a)) can cause conflicting results, and thus weaken their performance when both results are summarized together.  
In the cascaded model (see Fig.~\ref{fig0}(b)), the spatial attention module after the channel attention module may pick up the incorrect pixel representation enhanced by channel attention, as channel attention computes the channel importance according to the entire feature maps. 

In our work, we propose a Channelized Axial Attention approach, which first computes the spatial attention row-by-row and column-by-column, and then inserts the channel attention module to integrate both approaches seamlessly, as detailed next.  




\section{Methods}
\label {sectMethods}

\begin{figure*}[h]
	\centering
	\includegraphics[width=1.0\linewidth]{overall.pdf}
	\caption{The detailed architecture of our proposed Channelized Axial Attention model. To obtain  inputs for the channel attention, we apply the resultant column and row attentions in a transposed way. 
		The bottom section illustrates the channelization of the column attention for .}
	\label{fdetail}
	\label{figOverall}
\end{figure*}


\subsection {Formulation of the Spatial Self-Attention}
\label {sAxialAttention}

Following~\cite{cNonLocal, cStandAloneSelfAttention}, a 2D self-attention operation in spatial domain of neural networks can be defined by:

Here, a pairwise function  computes the similarity between the pixel representations , at the position  and the pixel representation  at all other possible positions , producing a spatial attention map over the whole feature maps. 
The unary function  maps the original representation at position  to a new domain. In our work, we also take the softmax function as , \textit{i.e.},
\begin {equation}
\footnotesize
f( \mathbf{x}_{i,j}, \mathbf{x}_{m,n}) = \text{softmax}_{m,n}(\theta( \mathbf{x}_{i,j})^T \theta( \mathbf{x}_{m,n})).
\label{eq:fullattentionmap}
\end {equation}

Thus, given a feature map output from a backbone network such as ResNet~\cite{cResnet}, the self-attention operation firstly uses a  convolution  to map the feature maps  to a new domain, and then applies dot-product similarity \cite{cNonLocal} between every two pixels.
Then, using this similarity as the weight, Eq.~\eqref{eq:fullattentionapply} calculates a weighted global sum over all pixels in the feature maps and outputs a new pixel representation  at the position . 

It can be seen from
Eq.~\eqref{eq:fullattentionmap} that, the original 
similarity map is of  dimensions, 
and is computed as the dot product over the whole feature maps for each pixel. 

Axial Attention, proposed in~\cite{cAxialAttention, cAxialDeepLab} for NLP and Panoptic Segmentation, has a computation complexity of , smaller than the self attention's  because its attention is computed within the same column or row only for each pixel. 
However, it has not yet had a baseline in semantic segmentation. 
In this work, in order to take the computation complexity advantage of the axial attention, we redefine the axial attention in~\cite{cAxialAttention, cAxialDeepLab} and convert it to a specialized semantic segmentation model.
In the next section, we first formulate the axial attention and then introduce our proposed channelized axial attention.

\subsection {Formulation of the Spatial Axial Attention}
\label{sGetAttentionMap}

In axial attention, the spatial attention map is calculated along the column axis and row axis, respectively.
For the convenience of reference, we call the partial attention map calculated along the  axis as `column attention' and `row attention' for the partial attention map calculated along the  axis. 
For the -th column attention, 
the attention similarity tensor is calculated by the similarity between the current position  and each of the other positions   in the -th column (instead of all other positions, as in the self-attention), \textit{i.e.},
\begin {equation} 
\footnotesize
A_\text{col}( \mathbf{x}_{i,j},\mathbf{x}_{m,j})=\text{softmax}_{m}\left(\theta (\mathbf{x}_{i,j})^T \theta( \mathbf{x}_{m,j})\right) \ , \ j \in [W].
\footnote{We use  to denote that  is generated from .}
\label{eq:ColumnAttention}
\end {equation}
Here,  represents the learned feature extraction process for the  axis. 
Each  represents the similarity between  and  for , so each  corresponds to  column-attention maps . 
Thus, the resultant column attention map  is a tensor of  dimensions.

Similarly, for the -th row attention, 
the similarity attention tensor calculates the similarity between the current position  and other positions   in the -th row, \textit{i.e.},
\begin {equation} 
\footnotesize
A_\text{row}( \mathbf{x}_{i,j}, \mathbf{x}_{i,n}) = \text{softmax}_{n}\left(\phi( \mathbf{x}_{i,j})^T \phi( \mathbf{x}_{i,n})\right) \ , \ i \in [H],
\label{eq:RowAttention}
\end {equation}
where  represents the learned feature extraction process for the  axis. 
Similarly, each  corresponds to  row-attention maps .
Thus, the resultant row attention map  is a tensor of  dimensions.

It is also worth of pointing out that, in Eqs.~\eqref{eq:ColumnAttention} and~\eqref{eq:RowAttention}, the calculations of column and row attention maps both use the same feature  extracted from the backbone module as the input, as shown in Fig.~\ref{figOverall}. 
This is different from~\cite{cAxialDeepLab}, where the row attention map was computed based on the result of the column attention. 
By using the same feature as the input, the dependency of the final output  on the feature  has been enhanced effectively, instead of using skip connections, as in~\cite{cAxialDeepLab}. 

With the column and row attention maps  and , the final value weighted by the column and row attention maps can be represented as: 


For the convenience of illustration, we introduce two variables  and  to capture the intermediate, weighted features, respectively, where

and


As illustrated later in Sect.~\ref{sChannelizedAttention}, capturing the intermediate attention results brings opportunity to conduct independent channel attentions for each partial attention result.

Thus, Eq.~\eqref{eq:SumAcrossAxialAttention} can be simplified as:


The above Eqs.~\eqref{eq:ColAttentionApply},~\eqref{eq:RowAttentionApply} and~\eqref{eq:SumAcrossAxialAttentionSplit} show that, the computation of the dot product is composed of two steps: 1) The element-wise multiplication for applying the column attention as shown in Eq.~\eqref{eq:ColAttentionApply} and for applying the row attention as shown in Eq.~\eqref{eq:RowAttentionApply} for column and row attentions, respectively; 
2) The summarization of the elements along each row and column according to Eq.~\eqref{eq:SumAcrossAxialAttentionSplit}.




\subsection {The Proposed Channelized Axial Attention}
\label {sChannelizedAttention}

In order to address the feature conflicting issue of the dual attention and seamlessly combine the advantages of spatial attention and channel attention, we propose a novel \textit{Channelized} Axial Attention, which takes the intermediate results  and  in Eqs.~\eqref{eq:ColAttentionApply} and~\eqref{eq:RowAttentionApply} as input.

Note that, in Eqs.~\eqref{eq:ColAttentionApply} and~\eqref{eq:RowAttentionApply}, we apply the column and row attention maps in a transposed order. 
That is to say, the column and row attention results are decomposed along the transposed axis (\textit{i.e.}, decomposing  along the row direction and  along the column direction), instead of along the column and row, into multiple 3-dimension column or row attention results for different  or .  
This is illustrated in Fig.~\ref{figOverall}. 

This transpositional way of applying the axial attentions not only produces partial column and row attention results with consistent dimensions, but also enables them to capture the dependencies inherent in the other axis so as to conduct channelization in the whole receptive field.








Now, we introduce our channelized attentions  and , corresponding to the column attention and row attention, respectively, as: 

and

where , ,  and  represent the learned relationships between different channels in  and , which will be discussed later in Sect.~\ref{sGoingDeeper}.

Thus, instead of directly using  and  as in Eq.~\eqref{eq:SumAcrossAxialAttentionSplit}, 
for each column and row, we obtain the seamlessly mixed attention results for spatial channels, 
where the intermediate results    and  are weighted by the channelized axial attention defined in  Eqs.~\eqref{eq:ChannelAttentionColumn} and~\eqref{eq:ChannelAttentionRow} as: 


The bottom section in Fig.~\ref{figOverall} illustrates the channelization of the column attention at . 
Later in Sect.~\ref{expAblation} (\tablename{~\ref{table:cdp}} and Fig.~\ref{fvis1}), we will show with ablation experiments and visualized feature maps the impact of the channelization on improving the performance of the segmentation. 






\subsection{Grouped Vectorization}
\label {sec:secGroupVect}

Computing spatial attention row by row and column by column can save computation but it is still too slow even with parallelization. 
Vectorization can achieve a very high speed but it has a high requirement on GPU memory for storing the intermediate partial axial attention results  (which has a dimension of ) and  (which has a dimension of ) in Eqs.~\eqref{eq:ColAttentionApply} and~\eqref{eq:RowAttentionApply}. 
To enjoy the high speed benefit of the vectorized computation with reduced GPU memory usage, in our implementation we propose \textit{grouped} vectorization to dynamically batch rows and columns into multiple groups, and then perform vectorization for each group respectively. 
Algorithm~\ref{alg:groupvect} shows the pseudo code of implementing the grouped vectorization. 

\begin{algorithm}
    \small
    \setlength{\abovecaptionskip}{0.1cm}
    \setlength{\belowcaptionskip}{0.1cm}

    \caption{Our proposed grouped vectorization algorithm}

    \begin{algorithmic}[1]
        \Require : Group Number, : Attention Map , : Feature Map 
        
        \State 
        \State  Transpose  into 
        \State 
        \State  padding zero to   into 
        \State  Reshape  into 
        \For{}
            \State  Channelization 
        \EndFor
        \State  Concat
        \State  Reshape   into 
        \State  Remove padding from  into 
        \State  Transpose  into 
        
        \Return Y
    \end{algorithmic}
    \label{alg:groupvect}
\end{algorithm}


\subsection{Going Deeper in Channel Attention}
\label{sGoingDeeper}

The channel attention in our method firstly uses a fully connected layer with a smaller rate to compress channels, and then uses another fully connected layer with the same rate as the original channels, followed by a sigmoid function to generate the final channel attention weights. 
To further boost the performance, we explore the design of more powerful channel attention modules in channelization. 

The simplest way of gaining performance is enhancing the representation ability of the neural networks, and it is usually achieved by increasing the depth and width of the networks. 
Here, we simply add more hidden layers before the last layer. 
This design allows channel attention to find better relationship between channels and find more important channels for each axial attention's intermediate results. 
We also find that it is not effective to increase the width (\textit{i.e.}, adding more hidden units to each layer except for the last layer), so we keep the original settings.

Furthermore, in the spatial domain, each channel of a pixel contains unique information that can lead to unique semantic representation. 
In our channel attention module, we find that using Leaky ReLU~\cite{cLeakyrelu}, instead of ReLU, is more effective in preventing the loss of information along deeper activations~\cite{cMobileNetV2}. 
Apparently, this replacement only works in our channel attention module.





\section{Experiments}
\label {sExperements}

To demonstrate the performance of our proposed CAA, comprehensive experiments are conducted with results compared with the state-of-the-art results on three benchmark datasets, \textit{i.e.}, PASCAL Context~\cite{cPascalVOC}, COCO-Stuff~\cite{cCocoStuff} and Cityscapes~\cite{cCityScapes}. 

The same as the other existing works~\cite{cDualAttention, cDeepLab, cEMANet, cOCR}, we measure the segmentation accuracy using mIOU (Mean Intersection Over Union). 
Moreover, to demonstrate the efficiency of our CAA, we also report and compare the FLOPS (Floating Point Operations per Second) of different approaches. 
Note that, a higher mIOU value means more accurate segmentation, whereas a lower FLOPS value indicates less computation operations.
Experimental results show that our CAA outperforms the state-of-the-art performance on all tested datasets.

Moreover, as we have analysed earlier in Sect.~\ref{secDualAttentionDesign}, computing spatial attention and channel attention separately and then fusing them together directly can cause conflicting feature representations. 
This means, channels important for a class in spatial domain may not dominate and therefore can be ignored in the computation of the global channel attention. 
In our experiments, to illustrate the feature conflicting issue caused by existing dual attention approaches, we design a simple way to visualize the effects of spatial attention and channel attention on pixel representation.

For the parallel dual attention design such as DANet~\cite{cDualAttention}, since it has two auxiliary losses for spatial attention and channel attention respectively, we directly use their logits during inference and generate their segmentation results to compare with the result generated by the main logits.
For the sequential dual attention design, we add an extra branch that directly uses the pixel representation obtained from channel attention to perform the segmentation logits. 
Note that, since the original sequential design does not have independent logits of channel attention, we stop the gradient from the main branch to make sure our newly added branch has no effect on the main branch.

Next, we first present the implementation details. 
This is followed by a series of ablation experiments conducted on the PASCAL Context dataset showing the effectiveness of each of our proposed ideas. 
Then, we report the comparative results obtained on PASCAL Context~\cite{cPascalVOC}, COCO-Stuff~\cite{cCocoStuff} and Cityscapes~\cite{cCityScapes} datasets, respectively. 
For fair comparison, we only compare with the methods that use ResNet-101 and naive 8 bilinear upsampling.



\subsection{Implementation Details}

\textbf{Backbone:} 
Our network is built on ResNet-101~\cite{cResnet} pre-trained on ImageNet. 
The original ResNet results in a feature map of  of the input size. 
Following other similar works~\cite{cDeepLabV3Plus, cAxialDeepLab, cEMANet}, we apply dilated convolution at the output stride = 16 during training for most of the ablation experiments. We conduct experiments with the output stride = 8 during training to compare with the state of the arts. 

\textbf{Segmentation Head:} 
We use a 33 convolution to reduce the number of feature map channels from 2,048 to 512~\cite{cDualAttention,cEMANet}, which is then followed by our proposed Channelized Axial Attention module. 
Note that, our Axial Attention generates the column attention map and the row attention map from the same feature maps, instead of generating one based on the computation results of the other, as in~\cite{cAxialDeepLab}. 
Also, after the computation of the attention maps, we do not add the original pixel representations to the resultant feature maps. 
In the end, we directly upsample our logits to the input size by applying bilinear interpolation.

\textbf{Training Settings:} 
We employ SGD (Stochastic Gradient Descent) for optimization, where the poly decay learning rate policy  is applied with an initial learning rate = 0.007. We use synchronized batch normalization during training.
Our experiments are conducted on TPUv3 and V100.
For data argumentation, we only apply the most basic data argumentation strategies in~\cite{cDeepLabV3Plus} including random flip, random scale and random crop, same as in the other works.

\begin{table}[t]
	\centering
\caption{Comparison results with different segmentation heads in the PASCAL Context dataset~\cite{cPascalContext}}
	\begin{tabular}{l|c|l} 
		\toprule[1pt]
		Methods & mIOU\% & FLOPS \\
		\midrule[0.5pt]
		\midrule[0.5pt]
		ResNet-101~\cite{cResnet} & - & 59.85G \\
		\midrule
		FCN~\cite{cFCN} &  48.12  &+0G\\
		ASPP~\cite{cDeepLabV3Plus} &50.47    &+16.7G              \\
		Non-Local~\cite{cNonLocal} &50.42  &+11.18G  \\
		Redefined Axial Attention&   50.27 (0.2)  &+8.85G   \\
		\bottomrule[1.pt]
	\end{tabular}
	\label{table:segheads}
\end{table}



\subsection{Experiments on PASCAL Context Dataset}
\label{expAblation}

PASCAL Context~\cite{cPascalContext} dataset has 59 classes with 4,998 images for training and 5,105 images for testing. 
We train the network model on PASCAL Context Training set with the batch size = 16 with 70k iterations. During training , we set the output stride = 16 and use an output stride = 8 for inference.
Later in \tablename{~\ref{tabPascalContextSOTA}}, we present our CAA results with an output stride = 16 and 8, where it can be seen clearly a  increase can be observed with the output stride = 8. 


Next, we first present a series of ablation experiments conducted on the PASCAL Context dataset to show the effectiveness of our proposed channelized axial attention. 
Then, quantitative and qualitative comparisons with the state of the arts are presented. 

\subsubsection{Axial Attention for Semantic Segmentation}

To verify the effectiveness of Axial Attention for semantic segmentation (see Sect.~\ref{sGetAttentionMap}), we compare the mIOU and FLOPS achieved with our channelized Axial Attention with other segmentation heads implemented by us, as shown in \tablename{~\ref{table:segheads}}. 
Note that our redefined Axial Attention used for semantic segmentation is different from~\cite{cAxialDeepLab}, as mentioned in Sect.~\ref{sAxialAttention}, and in this table we only compare with the methods that are independent with backbone~\cite{cResnet}. 
Also, all results in this table are obtained with an output stride = 16. 

From \tablename{~\ref{table:segheads}}, we can easily see that our redefined Axial Attention improves mIOU a lot compared to the Dilation-FCN ( vs ), which has a naive segmentation head. 
The mIOU obtained with our redefined axial attention is also comparable with other approaches, such as ASPP~\cite{cDeepLab, cDeepLabV3Plus} and Non-Local~\cite{cNonLocal}. 
However, the redefined axial attention has much lower FLOPS than the original self-attention~\cite{cNonLocal} (an increase of G vs G over the baseline), which demonstrates that the redefined axial attention for semantic segmentation can achieve comparable performance with the original self-attention at much lower computation cost. \\


\subsubsection{Examples of Conflicting Feature Representations}

To visualize the impact of the feature conflicting issue of the existing dual attention designs (see Sect.~\ref{secDualAttentionDesign}), 
Fig.~\ref{faxialse_se_sa} shows two groups of examples of the segmentation results obtained with the conflicting features in the parallel dual attention design (see Figs.~\ref{fdanet_pam_cam} and~\ref{fdanet_cam_pam}) and the sequential dual attention design (see Fig.~\ref{fig:five over x}). 

As it can be observed from Figs.~\ref{fdanet_pam_cam} and~\ref{fdanet_cam_pam}, the parallel design of dual attentions directly sums up the pixel representations obtained from spatial attention and channel attention. 
With this approach, the advantages of the pixel representations obtained from one can be weakened by the other. 

The sequential way of combining the dual attentions avoids taking their average but still has its issue. 
As shown in Fig.~\ref{faxialse_se_sa}, the pixel representation obtained from the spatial attention abandons the correct pixel representation obtained from the channel attention, and worsens the prediction result.\\



\begin{figure}[t]
     \centering
     \begin{subfigure}[t]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{danet_vis.pdf}
         \caption{The bad channel attention representation negatively influences the good spatial attention representation. See the highlighted areas.}
         \label{fdanet_pam_cam}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{danet_vis_2.pdf}
         \caption{The bad spatial attention representation negatively influences the good channel attention representation. See the highlighted areas.}
         \label{fdanet_cam_pam}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{axialse_vis.pdf}
         \caption{The spatial attention representation abandons the correct channel attention representation.}
         \label{fig:five over x}
     \end{subfigure}
        \caption{Examples of conflicting feature representation in the parallel (a and b) and sequential dual attention (c) designs.}
        \label{faxialse_se_sa}
\end{figure}


\subsubsection{Effectiveness of the Proposed Channelization}
We then use our proposed channelized dot product to replace the naive dot product in Axial Attention (see Sect.~\ref{sChannelizedAttention}). 
We report the impact of adding channelized dot product and with different depth and width in \tablename{~\ref{table:cdp}}, where `-' for the baseline result indicates no channelization is performed. 

\begin{table}[t]
	\centering
	\scriptsize
	\caption{ Result comparison without using channelization (Row 1) and using channelization with different layer counts and channel numbers.}
	\begin{tabular}{cccc|ccc|c|c} 
		\toprule[1pt]
		\multicolumn{4}{c|}{Layer Counts} &\multicolumn{3}{c|}{\# of Channels} & \multirow{2}{*}{mIOU\%} & \multirow{2}{*}{FLOPS} \\
		1  & 3 & 5 & 7& 64 & 128 & 256 &\\
		\midrule[0.5pt]
		\midrule[0.5pt]
		-& -& -& -& -& -& -& 50.27(0.2)&  68.7G\\
		\midrule
		\checkmark& & & & & \checkmark& & 50.75(0.2)& +0.00024G\\
		& \checkmark& & & & \checkmark& & 50.85(0.2) & +0.00027G\\
		& & \checkmark& & & \checkmark& & \textbf{51.06(0.2)} & +0.00030G \\
		& & & \checkmark& & \checkmark& & 50.40(0.3)  & +0.00043G \\
		\midrule
		& &\checkmark & & \checkmark& & & 50.12(0.2)& +0.00015G\\
		& & \checkmark& & & &\checkmark & 50.35(0.4) & +0.00098G \\
		\bottomrule[1pt]
	\end{tabular}
	\label{table:cdp}
\end{table}

\begin{table}[t]
    \centering
\caption{ Result comparison between axial attention, channelized axial attention and axial attention + SE~\cite{cSENet}  }
    \begin{tabular}{c|c|c}
        \toprule[1pt]
        Axial Attention& + Channelization & + SE \\
        \midrule[0.5pt]
		\midrule[0.5pt]
        50.27(0.2) & 51.06(0.2)& 50.37(0.2) \\
        \bottomrule[1pt]
    \end{tabular}
    \label{table:sedesign}
\end{table}


As it can be seen from this table, our proposed channelization improves the mIOU performance over the baseline regardless of the layer counts and the number of channels used. 
In particular, a best performance is achieved when the Layer Counts = 5 and the number of Channels = 128. 

We also compare our model with the sequential design of Axial Attention + SE, as shown in \tablename{~\ref{table:sedesign}}. 
We repeated the experiments many times but found the sequential design only brings slightly contributions on the performance, indicating that our purposed channelization method can combine the advantages of both spatial attention and channel attention effectively. 
Also note that, Fig.~\ref{fig0}(b) shows a failure result of the sequential design.\\

\subsubsection{Channelized Self-Attention}
In this section, we conduct additional experiments on the PASCAL Context testing set by applying channelization to the original self-attention. 
We report its single-scale performance in~\tablename{ \ref{tabCSA}} with ResNet-101~\cite{cResnet}. 

We can see from the table that our proposed channelized method can further improve the performance of self-attention slightly by 0.67\%.
It also shows the current channelized design is more effective for our Axial Attention (0.79\% vs 0.67\%). 


\begin{table}[t]
	\centering
\caption{Ablation study of applying our Channelized Attention on self-attention with ResNet-101~\cite{cResnet}. \textbf{Eval OS}: Output strides~\cite{cDeepLabV3Plus} during evaluation.}
	\begin{tabular}{l|c|c|c} 
		\toprule
		Attention Base                    & Eval OS & Channelized     & mIOU\%   \\
		\midrule[0.5pt]
		\midrule[0.5pt]
		\multirow{2}{*}{Axial Attention  }& 16       &             & 50.27  \\
		& 16  	 & \checkmark  & 51.06  \\
		\midrule
		\multirow{2}{*}{  Self Attention} & 16  	 &             & 50.42  \\
		& 16  	 & \checkmark  & 51.09  \\
		\bottomrule[0.5pt]
	\end{tabular}
	 
	\label{tabCSA}
\end{table}


\subsubsection{Impact of the Testing Strategies} 
We report and compare the performance and computation cost of our proposed model against the baseline and the DANet with different testing strategies. This is shown in \tablename{~\ref{table:teststrategies}}. 
Same as the settings in other works~\cite{cPSPNet, cDualAttention}, we add multi-scale, left-right flip and aux loss~\cite{cPSPNet, cDualAttention} during inference. 
Note that, in this table, we report the mean mIOU figures with a dynamic range to show the stability of our algorithm.
As it shows in this table, We found  our proposed CAA can be further boosted with OS = 8 since the channel attention can learn and optimize three times more pixels.
\\

\begin{table*}[t]
	\centering
	\caption{Comparison results with different testing strategies. 
\textbf{Train OS:} Output stride in training. \textbf{Eval OS:} Output stride in inference. \textbf{MS:} Apply multi-scale during inference. \textbf{Aux loss:} Add auxiliary loss during training.
		``'' refers to the FLOPS over the baseline FLOPS of ResNet-101. }
	\begin{tabular}{c|cc|cc|cc|c|c|c} 
		\toprule[1pt]
		\multirow{2}{*}{Methods} & \multicolumn{2}{c|}{Train OS} & \multicolumn{2}{c|}{Eval OS} & \multicolumn{2}{c|}{Strategies} &Aux Loss & mIOU\% & FLOPs   \\
		& 16 & 8 & 16 & 8 & MS & flip & & & \\ 
		\midrule[0.5pt]
		\midrule[0.5pt]
		ResNet-101~\cite{cResnet} & - & - & \checkmark &  &  &  & - & - & 59.85G \\
		& - & - &  & \checkmark &  &  & - & - & 190.70G  \\
		\midrule
		DANet~\cite{cDualAttention}  &           & \checkmark &           & \checkmark &    &     & - & -    & +101.25G\\ 
		&           & \checkmark &           & \checkmark & \checkmark & \checkmark  & \checkmark &52.60 & -   \\ 
		\midrule
		Our CAA           & \checkmark &     & \checkmark &         &         &      &     & 51.06(0.2)&  +8.85G\\
		& \checkmark &          & \checkmark &    & \checkmark    & \checkmark   &   &53.09(0.3)      & -  \\
		\midrule
		Our CAA  + Aux loss     & \checkmark &       & \checkmark &     &     &      & \checkmark & 51.80(0.2)&  +8.85G\\
		& \checkmark &       & \checkmark &     &\checkmark&\checkmark& \checkmark & 53.52(0.2)& - \\
		&  &\checkmark&           & \checkmark   &              &       & \checkmark& 53.48(0.3)   & +34.33G\\
		&  &\checkmark        &           & \checkmark   & \checkmark    & \checkmark   &\checkmark &54.65(0.4)      & - \\
		\bottomrule[1pt]
	\end{tabular}
	\label{table:teststrategies}
\end{table*}

\begin{table}[t]
	\centering
\caption{Result comparison with the state-of-the-art approaches on the PASCAL Context testing set for multi-scale prediction. For fair comparison, we only compare with the methods that use ResNet-101 and naive decoder.}
	\begin{tabular}{l|c|c}
		\toprule[1pt]
		\rule{0pt}{2ex} Methods & mIOU\% & Ref \\
		\midrule[0.5pt]
		\midrule[0.5pt]
		FCN~\cite{cFCN} & 50.8 & CVPR2015 \\
		ENCNet~\cite{cENCNet} & 51.7 & CVPR2018 \\
		Deeplab~\cite{cDeepLabV3Plus} & 52.7 & ECCV2018\\
		ANNet~\cite{cANNN} & 52.8 & ICCV2019 \\
		EMANet~\cite{cEMANet} & 53.1 & ICCV2019 \\
		SVCNet~\cite{cSVCNet} & 53.2 & CVPR2019 \\
		SPYGR~\cite{cSPYGR} & 52.8 & CVPR2020 \\
		CPN~\cite{cCPN} & 53.9 & CVPR2020 \\
		CFNet~\cite{cCFNet} & 54.0 & CVPR2019 \\
		\midrule[0.5pt]
		DANet~\cite{cDualAttention} & 52.6 & CVPR2019\\
		\midrule[0.5pt]
		Our CAA (OS = 16) & 53.7 & - \\
		\textbf{Our CAA (OS =\enspace8)} & \textbf{55.0} & - \\
		\bottomrule[1pt]
	\end{tabular}
	\label{tabPascalContextSOTA}
\end{table}

\subsubsection{Comparison with the State of the Arts}
\label{lab:PascalCtxSOTA}

Finally, we compare our proposed approach with the state-of-the-art approaches. 
The results on the PASCAL Context dataset is shown in \tablename{~\ref{tabPascalContextSOTA}}. 
Like other similar works, we apply multi-scale and left-right flip during inference. 
For fair comparison, we only compare with the methods that use ResNet-101 and naive decoder (directly upsampling logits). 
Also note that, in this and the following tables, we report the best results of our approach obtained in experiments. 

As shown in this table, our proposed CAA method achieves the highest score in the methods trained with an output stride = 16 with ResNet-101 and naive decoder, and even outperforms some methods trained with an output stride = 8.
Moreover, after we train our model with an output stride = 8, the performance of our model has been further improved and outperforms all of the state-of-the-art models, including the ones recently published in CVPR2019 and CVPR2020.

In Fig.~\ref{fvis0}, we provide the visualizations of the prediction results obtained with our CAA model in comparison with the state-of-the-art approaches. 
As shown in the figure, our model is able to segment objects very well without requiring any post-processing.

To further demonstrate the effectiveness of our proposed channelization, in Fig.~\ref{fvis1} we visualize the feature maps obtained after applying the column attention and row attention maps and the difference between the corresponding feature maps with and without applying the channel attentions.\\

\begin{figure}[t]
	\centering
\includegraphics[width=1\linewidth]{vis0.pdf}
	\caption{Examples of the segmentation results obtained on the PASCAL Context dataset~\cite{cPascalContext} with our proposed CAA approach in comparison to the results obtained with FCN~\cite{cFCN}, DANet~\cite{cDualAttention} and the ground truth. All results are inferenced with an output stride of 8. }
	\label{fvis0}
\end{figure}

\begin{figure*}[t]
	\centering
	\includegraphics[width=1.0\linewidth]{vis1.pdf}
	\caption{Visualization of the feature maps (, ,  and ) on PASCAL Context~\cite{cPascalContext} (top two rows) and Cityscapes~\cite{cCityScapes} (bottom two rows). 
		For each input image, we list the feature maps obtained after applying the column attention map and row attention map, the difference between the corresponding feature maps with and without applying the channel attentions, as well as our prediction and the ground truth segmentation, respectively
		For more details, please refer to Sect.~\ref{fdetail}.} \label{fvis1}
\end{figure*}


\subsubsection{Alternative Backbones}
In previous sections, we have reported our CAA's performance using ResNet-101~\cite{cResnet} as backbone, which is widely used in semantic segmentation~\cite{cDeepLabV3Plus, cDualAttention, cCCNet, cEMANet, cDenseASPP, cOCR, cENCNet, cCFNet, cPSPNet, cSANet, cANNN}. 
In this section, we conduct additional experiments on Pascal Context by attaching our CAA module with some other backbones. 
We report our results obtained with single scale without flipping in~\tablename{ \ref{tabBackbones}}.\\


\begin{table}[t]
	\centering
	\scriptsize
	\caption{Ablation study of applying our Channelized Axial Attention to other backbones.
		All results are obtained in single scale without flipping.
		\textbf{Axial Attention:} Using our Axial Attention after backbone.
		\textbf{Channelized}: Applying our Channelized approach.
		\textbf{Eval OS}: Output strides~\cite{cDeepLabV3Plus} during evaluation.}
	
	\begin{tabular}{l |c|c|c|c}
		\toprule
		Backbone 				   										& Eval OS  	&Axial Attention    & Channelized   & mIOU\%     \\
		\midrule[0.5pt]
		\midrule[0.5pt]
		\multirow{3}{*}{ResNet-50~\cite{cResnet}  }						& 16       	&       		    &              	& 46.92     \\
		& 16       	&\checkmark		    &              	& 49.73     \\
		& 16  	   	&\checkmark		    & \checkmark  	& 50.23     \\
		\midrule
		\multirow{3}{*}{ResNet-101~\cite{cResnet}  }					& 16       	&       		    &              	& 48.12     \\
		& 16       	&\checkmark		    &              	& 50.27     \\
		& 16  	   	&\checkmark		    & \checkmark  	& 51.06     \\
		\midrule
		\multirow{3}{*}{Xception65~\cite{cXception, cDeepLabV3Plus} }	& 16       	&       		    &              	& 49.40     \\
		& 16       	&\checkmark		    &              	& 52.42     \\
		& 16  	   	&\checkmark		    & \checkmark  	& 52.65     \\
		\midrule
		\multirow{4}{*}{EfficientNetB7~\cite{cEfficientNet}}			& 16       	&       		    &              	& 56.80     \\
		& 16       	&\checkmark		    &              	& 57.24     \\
		& 16  	   	&\checkmark		    & \checkmark  	& 57.93     \\
		& 8  	   	&\checkmark		    & \checkmark  	& 58.40     \\         
		
		\bottomrule[0.5pt]
		
	\end{tabular}
	
	
	\label{tabBackbones}
	
\end{table}


\subsubsection{Result with EfficientNet}

As mentioned in Sect.~\ref{lab:PascalCtxSOTA}, our CAA outperforms the SOTA methods~\cite{cCFNet, cEMANet} with the same settings (ResNet-101 w/o decoder). Furthermore, \tablename{ \ref{tabBackbones}} shows the universality of our proposed CAA with different backbones. 
In this section, we report our CAA's performance with EfficientNet-B7~\cite{cEfficientNet} in \tablename{\ref{tabPascalContextFree}}. 
Note that, this is not a fair comparison, since the listed methods were not trained under the same settings, or using the same backbone. 
The results show that our method can still improve the performance even with a strong CNN backbone Efficientnet-B7, and outperform the latest Transformer~\cite{cViT} based hybrid models such as SETR~\cite{cSETR} and DPT~\cite{cDPT}. \\


\begin{table}[t]
	\centering
\caption{Result comparison with the state-of-the-art approaches on the PASCAL Context testing set for multi-scale prediction. Note that, this is not a fair comparison, since all listed methods were not trained under same settings, or using same backbone.}
	\begin{tabular}{l|c}
		\toprule[1pt]
		\rule{0pt}{2ex} Methods & mIOU\% \\
		\midrule[0.5pt]
		\midrule[0.5pt]
		SETR-MLA~\cite{cSETR} & 55.83 \\
		HRNetV2 + OCR ~\cite{cSVCNet} & 56.2\\
		ResNeSt-269~\cite{cResnest} + DeepLab V3+~\cite{cDeepLabV3Plus} & 58.9 \\
		HRNetV2 + OCR + RMI ~\cite{cOCR} & 59.6 \\
		DPT-Hybrid~\cite{cDPT} & 60.46 \\
		\midrule[0.5pt]
		Our CAA (EfficientNet-B7, w/o decoder) & 60.12 \\
		\textbf{Our CAA (EfficientNet-B7 + simple decoder~\cite{cDeepLabV3Plus}) } & \textbf{60.50}\\
		\bottomrule[1pt]
	\end{tabular}
	\label{tabPascalContextFree}
\end{table}



\subsection{Results on the COCO-Stuff 10K Dataset}

\subsubsection{Comparison with the State of the Arts}
Following the other works~\cite{cEMANet, cOCR, cDualAttention}, we demonstrate that our model can handle complex images with a large number of classes. 
We further evaluate our model on the COCO-Stuff 10K dataset~\cite{cCocoStuff}, which contains 9,000 training images and 1,000 testing images, as shown in \tablename{~\ref{tabCocostuffSOTA}}.
As it can been from the table, our proposed CAA outperforms all other state-of-the-art approaches by a large margin of .

We also report results obtained with our CAA with Efficientnet-b7~\cite{cEfficientNet} in \tablename{~\ref{tabCocostuffFree}}. 



\begin{table}[t]
	\centering
\caption{Comparison results with  the state-of-the-art approaches on the COCO-Stuff 10K testing set for multi-scale prediction. For fair comparison, we only compare with the methods that use ResNet-101 and naive decoder.}
	\begin{tabular}{l|c|c}
		\toprule[1pt]
		\rule{0pt}{2ex} Methods & mIOU\% & Ref \\
		\midrule[0.5pt]
		\midrule[0.5pt]
		DSSPN~\cite{cDSSPN} & 38.9 & CVPR2018 \\
		SVCNet~\cite{cSVCNet} & 39.6 & CVPR2019 \\
		EMANet~\cite{cEMANet} & 39.9 & ICCV2019 \\
		SPYGR~\cite{cSPYGR} & 39.9 & CVPR2020 \\
		OCR~\cite{cOCR} & 39.5 & ECCV2020 \\
		\midrule[0.5pt]
		DANet~\cite{cDualAttention} & 39.7 & CVPR2019\\
		\midrule[0.5pt]
		\textbf{Our CAA} & \textbf{41.2} & - \\
		\bottomrule[0.5pt]
	\end{tabular}
	
	\label{tabCocostuffSOTA}
\end{table}


\begin{table}[t]
	\centering
\caption{Result comparison with the state-of-the-art approaches on the COCO-Stuff-10K testing set for multi-scale prediction. Note that, this is not a fair comparison, since all listed methods were not trained under same settings, or using same backbone. }
	\begin{tabular}{l|c}
		\toprule[1pt]
		\rule{0pt}{2ex} Methods & mIOU\% \\
		\midrule[0.5pt]
		\midrule[0.5pt]
		HRNetV2 + OCR ~\cite{cSVCNet} & 40.5\\
		DRAN & 41.2 \\
		HRNetV2 + OCR + RMI ~\cite{cOCR} & 45.2 \\
		\midrule[0.5pt]
		\textbf{Our CAA (EfficientNet-B7)} & \textbf{45.4}\\
		\bottomrule[1pt]
	\end{tabular}
	\label{tabCocostuffFree}
\end{table}


\subsubsection{Visualization of the Segmentation Results}

Fig.~\ref{fviscocostuff} show some examples of the segmentation results obtained on the COCO-Stuff 10K dataset~\cite{cCocoStuff} with our proposed CAA in comparison to the results of FCN~\cite{cFCN}, DANet~\cite{cDualAttention} and the ground truth. 
All results are inferenced with an output stride of 8. 
As it can be seen, our CAA can segment common objects such as building, human, or sea very well. 

\begin{figure}[h]
	\centering
	\includegraphics[width=3.4in]{vis_cocostuff.pdf}
	\caption{Examples of the segmentation results obtained on the COCO-Stuff 10K dataset~\cite{cCocoStuff} with our proposed CAA approach in comparison to the results obtained with FCN~\cite{cFCN}, DANet~\cite{cDualAttention} and the ground truth. All results are inferenced with an output stride of 8.} \label{fviscocostuff}
\end{figure}



\subsection{Results on the Cityscapes Dataset}

The Cityscapes dataset~\cite{cCityScapes} has 19 classes. Its \textit{fine} set contains high quality pixel-level annotations of 5,000 images, where there are 2,975, 500 and 1,525 images in the Training, Validation, and Test sets, respectively.
Like other works~\cite{cSPYGR,cDualAttention}, We only use \textit{fine} set with a crop size 769769 during training, and our training iteration is set to 90k.
We report our results on \textit{test} set in \tablename{~\ref{tabCityscapesSOTA}}  and also visualize our feature maps and results in Fig.~\ref{fvis1} (the bottom two rows). 

\begin{table}[t]
	\centering
\caption{Comparison results with other state-of-the-art approaches on the Cityscapes Test set for multi-scale prediction. For fair comparison, we only compare with the methods that use ResNet-101 and naive decoder.}
	\begin{tabular}{l|c|c}
		\toprule[1pt]
		\rule{0pt}{2ex} Methods & mIOU\% & Ref \\
		\midrule[0.5pt]
		\midrule[0.5pt]
		PSPNet~\cite{cPSPNet} & 78.4 & CVPR2017 \\
		CFNet~\cite{cCFNet} & 79.6 & CVPR2019 \\
		ANNet~\cite{cANNN} & 81.3 & ICCV2019 \\
		CCNet~\cite{cCCNet} & 81.4 & ICCV2019 \\
		CPN~\cite{cCPN} & 81.3 & CVPR2020 \\
		SPYGR~\cite{cSPYGR} & 81.6 & CVPR2020 \\
		OCR~\cite{cOCR} & 81.8 & ECCV2020 \\
		\midrule[0.5pt]
		DANet~\cite{cDualAttention} & 81.5 & CVPR2019\\
		\midrule[0.5pt]
		\textbf{Our CAA} & \textbf{82.6} & - \\
		\bottomrule[1pt]
	\end{tabular}
	
	\label{tabCityscapesSOTA}
\end{table}


\subsection{Effectiveness of Our Grouped Vectorization}

In Sect.~\ref{sec:secGroupVect}, we developed the grouped vectorization to split tensors into multiple groups so as to reduce the GPU memory usage when preforming channel attention in Eqs.~\eqref{eq:ChannelAttentionColumn} and~\eqref{eq:ChannelAttentionRow}. 
The more groups used in group vectorization, the proportionally less GPU memory is needed for the computation, yet with longer inference time.  
In this section, we conduct experiments to show the variation of the inference time (seconds/image) when different numbers of groups are used in group vectorization. 

Fig.~\ref{figSpeedVSGroups} shows the results where three different input resolutions are tested. 
As shown in this graph, when splitting the vectorization into smaller numbers of groups, \textit{e.g.}, 2 or 4, our grouped vectorization can achieve comparable inference speed with one half or one quarter of the original spacial complexity.


\begin{figure}[t]
	\centering
	\includegraphics[width=3in]{speedvsgroups.png}
	\caption{Inference time (seconds/image) when applying different numbers of groups in grouped vectorization.}
\label{figSpeedVSGroups}
\end{figure}

\section{Conclusion}
\label {secConclusion}

In this paper, aiming to combine the advantages of the popular spatial-attention and channel attention, we have proposed a novel and effective Channelized Axial Attention approach for semantic segmentation. 
After computing column and row attentions, we proposed to capture the intermediate results and perform the corresponding channel attention on each of them. 
Our proposed approach of applying the column and row attentions transpositionally has allowed the channelization to be conducted in the whole respective field. 
Experiments on the three popular benchmark datasets have demonstrated the superiority and effectiveness of our proposed channelized axial attention in terms of both segmentation performance and computational complexity. 










\appendices






\tablename{ \ref{tabBackbones}} shows that both our Axial Attention and Channelization approaches have improved the mIOU of the baseline in multiple well-know backbones. 
We also find that our Channelization approach is more effective with ResNet and EfficientNet, whereas the improvement on Xception65 is relatively small. 



\ifCLASSOPTIONcaptionsoff
  \newpage
\fi











\bibliographystyle{IEEEtran}
\bibliography{bare_jrnl}













\end{document}
