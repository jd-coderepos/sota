\def\year{2021}\relax
\documentclass[letterpaper]{article} \usepackage{aaai21}  \usepackage{times}  \usepackage{helvet} \usepackage{courier}  \usepackage[hyphens]{url}  \usepackage{graphicx} \urlstyle{rm} \def\UrlFont{\rm}  \usepackage{natbib}  \usepackage{caption} \frenchspacing  \setlength{\pdfpagewidth}{8.5in}  \setlength{\pdfpageheight}{11in}  \newcommand{\etal}{\emph{et al. }}
\newcommand{\eg}{\emph{e.g. }}
\newcommand{\ie}{\emph{i.e. }} 
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{boldline}
\usepackage[switch]{lineno}
\usepackage{soul}
\newcommand{\R}{\mathbb{R}}
\usepackage[ruled]{algorithm2e}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\pdfinfo{
/Title (CompFeat: Comprehensive Feature Aggregation for Video Instance Segmentation)
/Author (Yang Fu, Linjie Yang, Ding Liu, Thomas S. Huang, Humphrey Shi)
/TemplateVersion (2021.1)
} 





\setcounter{secnumdepth}{0} 







\title{CompFeat: Comprehensive Feature Aggregation for Video Instance Segmentation}
\author{Yang Fu,  Linjie Yang, Ding Liu, Thomas S. Huang, Humphrey Shi \\ 
{\small University of Illinois at Urbana-Champaign},
{\small ByteDance Inc.}, {\small University of Oregon}\\ }
\begin{document}
\maketitle
\begin{abstract}
Video instance segmentation is a complex task in which we need to detect, segment, and track each object for any given video. Previous approaches only utilize single-frame features for the detection, segmentation, and tracking of objects and they suffer in the video scenario due to several distinct challenges such as motion blur and drastic appearance change. 
To eliminate ambiguities introduced by only using single-frame features, we propose a novel comprehensive feature aggregation approach (\textbf{CompFeat}) to refine features at both frame-level and object-level with temporal and spatial context information. 
The aggregation process is carefully designed with a new attention mechanism which significantly increases the discriminative power of the learned features.
We further improve the tracking capability of our model through a siamese design by incorporating both feature similarities and spatial similarities. Experiments conducted on the YouTube-VIS dataset validate the effectiveness of proposed CompFeat.
Our code will be available at \url{https://github.com/SHI-Labs/CompFeat-for-Video-Instance-Segmentation}.

 \end{abstract}

\section{Introduction}
Video instance segmentation (VIS) is a joint task of detection, segmentation and tracking of object instances in videos~\cite{yang2019video}. Different from instance segmentation in image domain~\cite{hariharan2014simultaneous}, video instance segmentation not only requires to segment object masks on individual frames, but also to track the identities of objects across different frames. Also, unlike semi-supervised video object segmentation~\cite{voigtlaender2019feelvos, voigtlaender2017online, xu2018youtube, wug2018fast, oh2019video, xu2019spatiotemporal}, video instance segmentation does not require a ground truth mask in the first frame and all objects appear in the video should be processed. It has essential applications in many video-based tasks, including video editing, autonomous driving and augmented reality.

Video instance segmentation has several distinct challenges. For example, if an object is recognized as a wrong category in one frame of the video, tracking of this object will be extremely hard due to inconsistency of object categories. When there are multiple similar objects, finding the correspondences of them across the video is also challenging.
VIS is an important but underexplored task. The pioneering work for VIS is MaskTrack-RCNN~\cite{yang2019video}, which is built upon Mask-RCNN~\cite{he2017mask}, a state-of-the-art method for image instance segmentation. 
A new tracking branch is tailored and added in order to track object instances.
However, MaskTrack-RCNN relies on only single frame object features and neglects critical temporal information, \ie temporal consistency of an object, motion pattern of different objects, etc, all of which can provide abundant information for category recognition, object detection and mask segmentation across video frames. And lots of recent video understanding work~\cite{wang2018non, wu2019long} focused on how to utilize the temporal information.
In addition, the proposed tracking head in MaskTrack-RCNN is preliminary and ignores the spatial layout of objects with simple object features, 
which has been proven crucial by modern object tracking algorithms~\cite{zhang2019deeper,Zhu_2018_ECCV,li2019siamrpn++} to improve the tracking of video instances.


\begin{figure}[t]
	\centering
	\includegraphics[width=0.45\textwidth]{figs/illus2.pdf}
\caption{An illustration of the proposed comprehensive feature aggregation (CompFeat) method.  denotes the video frame at time . (a) Previous video instance segmentation method without feature aggregation. (b) Our proposed comprehensive feature aggregation approach for video instance segmentation}
	\label{fig:illus}
\end{figure}

In order to utilize the abundant information in videos and to harvest the benefits of modern object tracking approaches, we propose a comprehensive feature aggregation approach for video instance segmentation, termed \textbf{CompFeat}. The main idea of CompFeat is illustrated in Fig~\ref{fig:illus}.  
As shown in Fig~\ref{fig:illus}(a), the key object is not detected and fails to be tracked due to using only the unclear visual cues of a single frame, while other frames in the same video contain helpful information for locating and tracking the correct object. 
Hence, we propose a dual attention module with both temporal attention and spatial attention to aggregate contextual information from neighboring frames and other positions in the current frame as described in Fig~\ref{fig:illus}(b). 
We also enhance the features of detected objects by extending the dual attention module to object level, which substantially improves the discriminative power of the object features, enabling more accuracy object detection and segmentation. 
In addition, we introduce a novel correlation-based tracking module to improve instance tracking across different frames. Instead of using a holistic similarity between a pair of detected object and reference object to determine object correspondence, our correlation-based module not only employs depth-wise correlation between an object pair to generate a matching score with spatial awareness, but also computes a correlation map between a reference object and the current frame to better localize the target object similar to Siamese object tracking.

To summarize, the main contributions of this work are threefold as follows:
\begin{itemize}
\item We propose a comprehensive feature aggregation approach for video instance segmentation, including temporal and spatial attention modules on both frame-level and object-level features.
\item We introduce a correlation-based tracking module to track instances across frames, which predicts cross-correlation maps in both object-to-object and object-to-frame manners to produce multiple similarity cues for object tracking.
\item We conduct extensive experiments and ablation study on YouTube-VIS~\cite{yang2019video} to demonstrate the effectiveness of our proposed framework and each of the individual components.
\end{itemize}

%
 \section{Related Work}~\label{related}
In this section we review  video instance segmentation and several closely-related tasks such as video object detection and video object tracking.





{\bf Video Object Detection.} Video object detection aims to detect all objects in videos such as shown in the ImageNet VID challenge~\cite{russakovsky2015imagenet,han2016seq}. Feature aggregation is widely used in video detection~\cite{zhu2017flow,feichtenhofer2017detect,chen2018optimizing,liu2019looking}. For instance, Zhu~\etal proposed to aggregate features from nearby frames to enhance the feature quality of an input frame. However, its speed is pretty slow due to the dense detection and optical flow estimation. In~\cite{chen2018optimizing}, Chen~\etal proposed to use a scale-time lattice to generate detection on sparse key frames and designed a temporal propagation approach for detection in an effective way. Inspired by these work, we propose to improve the feature quality for video instance segmentation via feature aggregation using attention mechanism.

{\bf Video Object Tracking}. Video object tracking can be viewed as a sub-task of video instance segmentation, which has two scenarios: detection-based tracking and detection-free tracking. In detection-free tracking, given the ground truth location of the target object in the first frame, algorithm is required to track the target object through the whole video. Recently, the Siamese network based trackers have received significant attentions due to their well-balanced accuracy and efficiency~\cite{li2019siamrpn++, zhang2019deeper, wang2018learning, valmadre2017end}. In particular, these trackers attempt to produce a similarity map from cross-correlation of the two feature branches, one for the target object and the other for the search region, where the similarity map embeds more semantic meanings. On the other hand, detection-based tracking~\cite{sadeghian2017tracking, son2017multi,shi2018geometry} simultaneously detect and track multiple video objects, which is more similar to the setting of video instance segmentation. In our proposed CompFeat, we borrow ideas from both detection-based tracking and detection-free tracking.

{\bf Video Instance Segmentation}. MaskTrack-RCNN~\cite{yang2019video} is the first attempt to address the video instance segmentation problem. It proposes a large-scale video dateset named YouTube-VIS for benchmarking video instance segmentation algorithms. Several methods in the Large-Scale Video Object Segmentation Challenge achieve impressive results but they utilize large quantity of external data and complex algorithm pipelines~\cite{wang2019empirical, dong2019temporal, luiten2019video}. A closely related work is multi-object tracking and segmentation~\cite{voigtlaender2019mots} which is proposed to evaluate multi-object tracking along with instance segmentation. However, because of its limited data scale and few object categories, we do not compare with it in this paper. MaskTrack-RCNN only uses image features but not temporal information of video sequences. We extend this work with a more sophisticated comprehensive feature aggregation approach which greatly boosts the performance on video instance segmentation.

\begin{figure*}[t]
	\centering
	\includegraphics[width=0.65\textwidth]{figs/model1.pdf}
\caption{An overview of our proposed CompFeat approach for video instance segmentation. CompFeat consists of three major components: (a) A frame level attention module incorporating both temporal and spatial attention modules. (b) An object level attention module which has a similar structure as frame level attention. (c) A correlation-based tracking module to predict the correlation score and correlation map simultaneously.}
	\label{fig:frame}
\end{figure*} \section{Proposed Method}\label{method}
In our framework, the video frames are sequentially processed. During the video processing, we randomly select one frame as the current frame and sample other several frames as support frames which are used for temporal feature aggregation. Meanwhile, a correlation-based tracking module is proposed to track object identities across frames with comprehensive cues. The overview of the proposed CompFeat framework is shown in Fig~\ref{fig:frame}. The current frame and all support frames are first fed into ResNet50~\cite{he2016deep} for feature extraction. Then, our proposed {\bf temporal attention module} takes the features of the current frame and support frames as inputs for feature aggregation over different frames. Meanwhile, the features of the current frame are processed by a {\bf spatial attention module} for global context feature aggregation on a single frame. The similar process is performed on object level with same model structures. In addition, we enhance the tracking branch of the network via a {\bf correlation-based tracking module} for a more accurate object tracking. The correlation-based tracking module combines cross-correlation between a pair of reference object and newly detected object and correlation between the reference object and the current frame (search region). Finally, we integrate the three proposed modules into a complete framework to perform three different tasks, object detection, mask segmentation and object tracking simultaneously. Next we describe each proposed component in details.

\subsection{Temporal Attention Module}
Inspired by the non-local networks~\cite{wang2018non}, we propose a novel temporal attention module to refine the features of the current frame via aggregating information from other support frames. Different from the original non-local block, which aims to model the long-range dependencies by attention mechanism, our proposed temporal attention module focuses more on embedding information from other frames and use them to refine features of the current frame by cross-attention mechanism. Specifically, the temporal attention module has three steps: embedding of current frame embedding, embedding of support frames and features aggregation as described in Fig~\ref{fig:att}(a).

{\bf Embedding of current frame}. Given the features of the current frame , where  are the height, width, and the feature dimension of output feature map from the backbone network. We first feed it into a convolution layer to generate a feature map , where , and a non-linearity activation is applied. The  can store the key features of the current frame including which and where objects may exist. Therefore, the feature map  is learnt to encode the key information of visual semantics in the current frame, \ie object categories, object locations and masks.

{\bf Embedding of support frames}. Given a stack of feature maps obtained from support frames  ( is the number of the support frames), each feature map  is first encoded into a pair of feature maps  and  by two parallel convolution layers, where .  Non-linear activation is applied as well. If there are more than one support frames (), we concatenate features of different frames along temporal dimension and obtain .  contains the information of key features of support frames. The similarities between  and  shows when-and-where the features of support frames are suitable to be aggregated for the current frame. In addition, the feature map  is learnt to represent the context information of all support frames.

{\bf Features aggregation}. In the feature aggregation step, we first compute the attention weights by similarities between all pixel in feature map  and . The similarities are computed as the correlation of every spatial location in the feature map  and every spatial-temporal location in feature map . Then the attention weights are used to aggregate features from  to obtain context features from support frames. In addition, we perform a feature transformation by a  convolution layer. The whole process of feature aggregation for every position of the current frame can be summarized as the following equation,

where  is the similarity matrix between the current frame and support frames with size of , ,  are the indices of every position in the similarity matrix and the feature map,  is the total number of positions in the feature map,  is dot product,  is a transformation function with non-linear activation,  is the aggregated feature map after the transformation. Note that, all feature maps in above equation may be processed by some necessary reshaping or permutation operations. Finally, the refined feature map  is obtained by summing up the aggregated feature map  and the feature map of the current frame .
 
After aggregation, we can obtain a feature map , which not only preserves some information key visual semantics of current frame, but also extracts useful contextual information existing in other frames within the same video. 

\begin{figure}[t]
	\centering
	\includegraphics[width=0.4\textwidth]{figs/att.pdf}
\caption{An illustration of (a) temporal attention module and (b) spatial attention module. Best viewed in color.}
	\label{fig:att}
\end{figure}

\subsection{Spatial Attention Module}
Besides aggregating temporal contextual information from support frames, we also propose a spatial attention module to model the spatial context of the current frame. The proposed spatial attention module is based on the non-local block~\cite{wang2018non} with several 
modifications. As shown in Fig~\ref{fig:att}(b), instead of computing the attention maps of all spatial positions for each channel, we simplify it by sharing a single attention weight for all spatial positions in each channel, which is known as the channel attention. As reported in~\cite{cao2019gcnet}, this simplified channel attention module can achieve very similar performance compared to original non-local block. Also, since it doesn't need to compute a specific attention weight for every spatial position, it can be more efficient. Then, we use two more convolution layers to transform the features. Finally, the channel attention is added back to the feature map of the current frame for global context feature aggregation. 


\subsection{Combining Two Attention Modules}
Furthermore, we integrate the two attention modules into a dual attention module. The dual attention model takes the features of the current frame and support frames as inputs. The features of the current frame  is first processed by spatial attention model to obtain an attention map with context features of itself. And features of current frames  and features of support frames  are fed into the temporal attention module to generate another attention map with context information of support frames. Then, we aggregate the features of the current frame with two attention maps by adding them to the original feature map  as follows,  

where  represent proposed temporal attention module and spatial attention module respectively.

\subsection{Attention Module on Object Level}\label{sec3.4}
Attention modules described above are all processed on frame level features, which means they aggregate the context information for the whole feature map from the current frame and support frames. However, for instance segmentation with a two-stage framework, aggregating context information for each object proposal is also critical. Object proposals act as valuable candidates for the final object predictions and they encode more focused features for the individual objects. Proper feature aggregation onto these object proposals can elucidate confusing feature representations and improve recognition accuracy. We attempt to extend the two proposed attention modules to object level by applying similar operations to the object features produced by ROI Align~\cite{he2017mask}. Since the proposed attention modules can be applied to feature maps with arbitrary size, we adopt the two proposed attention modules in a similar fashion. 
We denote the features of detected object proposals as . , where  is the number of object proposals,  is the channel dimension, and ,  are height and width of the proposal. The features of each proposal are fed into a dual attention module along with feature maps of support frames. Since the size of the features of proposals is much smaller than the whole feature map, aggregating features on object level can be very efficient. In the next section, we show the performance gain of adding object-level attention modules is comparable or even better than adding attention module on frame level. 

The proposed attention module is inspired by some early work of video object detection and video object segmentation~\cite{wu2019long,oh2019video}, where the non-local blocks are used for extracting self-attention features. However, the motivation of proposed attention module is different as it is designed to aggregate the features from other frames (support frames) to current frame through feature matching, but not to itself by self-attention mechanisms. In addition, we extend the proposed attention module to object level, which largely reduces the computational complexity while improves the performance as frame-level attention module. Furthermore, frame-level and object-level module can be integrated into a single framework for a better performance. To our best knowledge, there are few work on enhancing object level feature by other frames. And the experiments in Sec.~\ref{exp} show the effectiveness of our proposed attention module.

By performing proposed dual attention module on both frame level and object level, we achieve feature aggregation on both spatial and temporal dimension, and in both local and global granularities, which is a comprehensive approach to aggregate and enhance intermediate features in a video instance segmentation framework.

\subsection{Correlation-based Tracking Module}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.4\textwidth]{figs/depth.pdf}
\caption{An illustration of the correlation-based tracking module. The convolution layers are omitted here.}
	\label{fig:track}
\end{figure}

In order to track an object across frames in a more robust and consistent way, we propose a new correlation-based tracking module to generate both spatial likelihoods and object similarities, which is more powerful than the previous approach~\cite{yang2019video}.

Recent works on object tracking shows the effectiveness of correlation-based tracking which has achieved state-of-the-art performance on several tracking benchmarks. Inspired by SiamRPN++~\cite{li2019siamrpn++}, our proposed tracking module incorporates knowledge from both object similarities and cross-correlations of the target object and the search region. As shown in Fig~\ref{fig:track}, our tracking module can be abstracted into three procedures:(a) pair-wise similarity computation, which uses depth-wise correlation instead of matrix multiplication~\cite{yang2019video} to predict a similarity matrix between two groups of objects. (b) cross-correlation computation, which generates correlation maps with depth-wise correlation between a single object and the whole frame. To train the model to produce high quality correlation maps, we employ a pseudo likelihood map represented by a two dimensional gaussian distribution centered at the location of the object as the supervision signal. Then we compute the aggregated similarity vector for a detected object with ROI align within the corresponding bounding box on the correlation map. 
After that, we sum up the two similarity vectors to obtain the final depth-wise similarity vector. (c) The final similarity score is mapped from the similarity vector with  two  convolution layers. The proposed correlation-based tracking module considers both feature similarity and correlation-based similarity scores on the raw image features to increase the robustness of tracking.
 \section{Experiments}\label{exp}
In this section, we conduct extensive experiments on YouTube-VIS~\cite{yang2019video} to evaluate the effectiveness of each proposed component and compare our proposed method with previous approaches.
\subsection{Data and Evaluation Metric}
{\bf Data} YouTube-VIS is the first and largest dataset for video instance segmentation, which is a subset of YouTube-VOS dataset~\cite{xu2018youtube}.
YouTube-VIS is comprised of 2,883 high resolution YouTube videos with 40 common object categories. In each video, several objects with bounding boxes and masks are labeled manually and the identities cross different frames are annotated as well. Since only the validation set is available for evaluation, all results reported in this paper are evaluated on the validation set.

{\bf Evaluation Metrics}
To evaluate the performance of the proposed method, we use the metrics mentioned in~\cite{yang2019video}, which are average precision(AP) and average recall(AR) based on a spatial-temporal Intersection-over-Union (IoU). Following the COCO evaluation, AP is computed by averaging over multiple IoU thresholds, \eg from from 50\% to 95\% at step 5\% and AR is the maximum recall given some fixed number of segmented instances per video. Both metrics are first calculated for each category and then averaged over 40 categories. 

\subsection{Implementation Details}

{\bf Training}. Our proposed method is built on Mask-RCNN~\cite{he2017mask}. The backbone network structure is ResNet50 with FPN~\cite{lin2017feature}, which is pre-trained on MSCOCO dataset~\cite{lin2014microsoft}. 
In the tracking branch, we use two convolution layers to refine the correlation features generated by depth-wise correlations, respectively. The first convolution layer has 256 channels which is the same dimension as the correlation features while the second one is used for correlation map prediction with only one output channel. 
Our model is implemented based on MMDetection~\cite{chen2019mmdetection} and the whole framework is trained end-to-end in 12 epochs with two NVIDIA 2080TI GPUs. We resize the original frame size to  for both training and testing. During training, the initial learning rate is set to 0.0125 and decays with a factor of 10 at epoch 8 and 11. For each input frame, we randomly select three frames from the same video, two used as support frames in the dual attention module and the other used as reference frame in the tracking module.

{\bf Testing}. During evaluation, the testing video is processed by the proposed method frame by frame in an online fashion. For each input frame, four additional frames are sampled from the testing video as support frames. Note that the number of support frames used in training and testing can be different, since testing with more support frames can help improve performance. We conduct an ablation study on the number of support frames in the following section. For the tracking head, we ignore the correlation map and use the predicted correlation score as the tracking score. Then we follow the inference procedure described in~\cite{yang2019video} to predict the category, bounding box and mask for the object instance. In addition, we combine other cues, \ie detection confidence, bounding box IoU, and category consistency, along with tracking scores to improve the tracking accuracy as a powerful post-processing.

\subsection{Ablation Study}
\begin{table}\setlength{\tabcolsep}{1pt}
\centering
\footnotesize
\begin{tabular} {l|c|c|c}
\hlineB{2}
Methods & AP & AP & AP \\ \hline 
Mask-Track RCNN~\cite{yang2019video} & 21.1 & 37.7 & 23.6 \\ \hline
Our Implementation & 20.9 & 37.9 & 21.6 \\ 
Our Implementation + MSCOCO & {\bf 24.1} & {\bf 42.6} & {\bf 24.9} \\ \hline
\hlineB{2}
\end{tabular}
\caption{Performance of the baseline model on YouTube-VIS validation set. ``Our Implementation" means our reproduced results of Mask-Track RCNN.``Our Implementation + MSCOCO" is used as the baseline model in all ablation studies hereafter.}
\label{exp:t1}
\end{table}


\begin{table}\setlength{\tabcolsep}{4pt}
\centering
\footnotesize
\begin{tabular} {l|c|c|c}
\hlineB{2}
\multicolumn{4}{c}{(a) {\bf Attention on Frame Level} }\\ 
& AP & AP & AP \\ \hline 
Baseline + Temporal Attention & 25.2 & 43.9 & 25.6 \\
Baseline + Spatial Attention & 24.9 & 43.4 & 25.2 \\
Baseline + Spatial-Temporal Attention & 25.8 & 44.5 & 27.0 \\ \hline
\multicolumn{4}{c}{(b) {\bf Attention on Object Level}}\\ 
& AP & AP & AP \\ \hline 
Baseline + Temporal Attention  & 25.4 & 44.4 & 25.9  \\ 
Baseline + Spatial Attention  & 24.8 & 43.3 & 25.1 \\ 
Baseline + Spatial-Temporal Attention & 26.1 & 45.6	& 26.7 \\ \hline
\multicolumn{4}{c}{(c) {\bf Attention on Both Frame and  Object Level}}\\ 
& AP & AP & AP \\ \hline 
Baseline + Spatial-Temporal Attention & {\bf 27.5} & {\bf 46.1} & {\bf 28.9} \\
\hlineB{2}
\end{tabular}

\caption{Ablation Study of our proposed attention module on YouTube-VIS validation set. The best results are highlighted in bold.}
\label{exp:t2}
\end{table}
{\bf Baseline Model and Data Augmentation}
Since our proposed method is built on Mask-Track RCNN~\cite{yang2019video}, we take Mask-Track RCNN as the baseline model to validate the effectiveness of each contribution. We first reproduce the Mask-Track RCNN with publicly available codes and the results are listed in Table~\ref{exp:t1}. Our result is close to the result reported in original paper. Note that all results in Table~\ref{exp:t1} are without any post-processing. 

In addition, we find that the number of object instances in YouTube-VIS dataset is limited. Since the proposed method does not depend on the temporal smoothness of a video, some image-based datasets can be adopted to increase the training samples. We choose MSCOCO~\cite{lin2014microsoft} as external data which has a large overlap on the object categories with YouTube-VIS. In order to make use of the image data, we generate support frames and reference frame based on a single image by some affine transformations, \ie \ie rotation, translation and shearing.
The identity annotations across different images can be generated automatically in this process. The performance after using external data is listed in Table~\ref{exp:t1} as well.
We use this model as a baseline model for all the following ablation experiments.



\begin{table}\setlength{\tabcolsep}{2pt}
\centering
\footnotesize
\begin{tabular} {l|c|c|c}
\hlineB{2}
Modules & AP & AP & AP \\ \hline 
Baseline + CM & 25.1 & 44.3 & 26.7 \\
Baseline + CM + FDA & 26.3 & 45.3 & 27.1 \\
Baseline + CM + BDA & 26.7 & 45.9 & 27.3 \\
Baseline + CM + FDA + BDA (CompFeat) & {\bf 28.4} & {\bf 47.4} & {\bf 30.7} \\
\hlineB{2}
\end{tabular}
\caption{Ablation Study of the proposed track module on YouTube-VIS validation set. CM, FDA, BDA denote the proposed tracking module with correlation map, the frame level dual attention module and object level dual attention module, respectively. The best results are highlighted in bold.}
\label{exp:t3}
\end{table}



\begin{table}\setlength{\tabcolsep}{3pt}
\centering
\footnotesize
\begin{tabular} {l|c|c|c|c}
\hlineB{2}
Train/Test(Uniform) &  2 frames &  3 frames &   3 frames & 4 frames \\ \hline 
Uniform 2 frames & 27.1 & 27.4 & 27.8 & 27.3  \\
Uniform 3 frames & 26.2 & 26.6 & 26.6 & 26.8  \\ 
Random 2 frames & 27.3 & 27.7 & 28.4 & 27.4\\
Random 3 frames  & 26.1 & 26.8 & 26.8 & 26.7 \\ \hline
\hlineB{2}
\end{tabular}
\caption{Performance of different sampling methods and different frames during training/testing on the validation set of Youtube-VIS. The performance is reported in AP.}
\label{exp:t6}
\end{table}

\begin{table*}[t]\setlength{\tabcolsep}{6pt}
\centering
\footnotesize
\begin{tabular} {c|c|c|c|c|c}
\hlineB{2}
Methods & AP & AP & AP & AR & AR \\ \hline
IoUTracker+~\cite{bochinski2017high}  & 23.6 & 39.2 & 25.5 & 26.2 & 30.9 \\
OSMN~\cite{yang2018efficient} & 27.5 & 45.1 & 29.1 & 28.6 & 33.1 \\
DeepSORT~\cite{wojke2017simple} & 26.1 & 42.9& 26.1 & 27.8 & 31.3 \\
SeqTracker & 27.5 & 45.7 & 28.7 & 29.7 & 32.5 \\
MaskTrack R-CNN~\cite{yang2019video} & 30.3 & 51.1 & 32.6 & 31.0 & 35.5 \\
SipMask~\cite{Cao_SipMask_ECCV_2020} & 32.5 & 53.0 & 33.3 & 33.5 & 38.9 \\
\hline
MaskTrack R-CNN + MSCOCO & 32.2 & 52.1 & 34.6 & 31.8 & 37.2 \\
CompFeat & {\bf 35.3} & {\bf 56.0} & {\bf 38.6} & {\bf 33.1} & {\bf 40.3} \\
\hlineB{2}
\end{tabular}
\caption{Comparison of the proposed approach with the state-of-the-arts on YouTube-VIS validation set. Note that all results in this Table including the post-processing. The best results are highlighted in bold.}
\label{exp:t4}
\end{table*}

{\bf Effectiveness of Attention Module}
We conduct ablation study to prove the effectiveness of the proposed attention module: temporal attention and spatial attention. The results are listed in Table~\ref{exp:t2}. Note all results here are without post-processing. We first evaluate each attention module on frame level in Table~\ref{exp:t2}(a). With the temporal attention module, we improve baseline model by 1.1\% in AP and  1.3\% in AP respectively. Similarly, spatial attention module can also slightly improve the baseline performance by 1\%. When combining both temporal and spatial attention together as the dual attention module, we further boost the baseline model by 1.7\%, 1.9\% and 2.1\% in AP, AP and AP. These experimental results prove that our proposed attention module can aggregate helpful context feature from both other frames and the input frame. 

We then experiment with temporal and spatial attention on object level. Table~\ref{exp:t2}(b) shows that proposed attention method on object level can always achieve comparable or even better results compared with the frame level one. For instance, by performing both temporal and spatial attention on object level, the performance gain becomes 2.0\%, 3.0\% and 1.8\% in AP, AP and AP respectively. 


Furthermore, Table~\ref{exp:t2}(c) lists the performance of combining attention modules on both frame level and object level. Comparing with the performance only on frame or object level, the combination one is superior. Specifically, with both attention on two different levels, we achieve AP/AP , which outperforms the performance with attention module on frame level by 1.7\%/1.6\%. 
This further improvement shows that by using attention module on frame level and object level, we can aggregate context information in a global-to-local manner, which can greatly improve the baseline model by 3.4\%, 3.5\% and 4.0\% in AP, AP and AP.

\begin{figure}[t]
\begin{center}
\bgroup 
 \def\arraystretch{0.1} 
 \setlength\tabcolsep{0.5pt}
\begin{tabular}{ccccc}
\includegraphics[width=0.2\linewidth]{figs/108/1.png} &
\includegraphics[width=0.2\linewidth]{figs/108/4.png} & 
\includegraphics[width=0.2\linewidth]{figs/108/7.png} & 
\includegraphics[width=0.2\linewidth]{figs/108/10.png} &
\includegraphics[width=0.2\linewidth]{figs/108/13.png} \\
\includegraphics[width=0.2\linewidth]{figs/40/2.png} &
\includegraphics[width=0.2\linewidth]{figs/40/3.png} &
\includegraphics[width=0.2\linewidth]{figs/40/4.png} &
\includegraphics[width=0.2\linewidth]{figs/40/5.png} &
\includegraphics[width=0.2\linewidth]{figs/40/6.png} \\
\includegraphics[width=0.2\linewidth]{figs/235/2.png} &
\includegraphics[width=0.2\linewidth]{figs/235/4.png} &
\includegraphics[width=0.2\linewidth]{figs/235/7.png} &
\includegraphics[width=0.2\linewidth]{figs/235/9.png} &
\includegraphics[width=0.2\linewidth]{figs/235/12.png} \\
\includegraphics[width=0.2\linewidth]{figs/96/18.png} &
\includegraphics[width=0.2\linewidth]{figs/96/19.png} &
\includegraphics[width=0.2\linewidth]{figs/96/20.png} &
\includegraphics[width=0.2\linewidth]{figs/96/21.png} &
\includegraphics[width=0.2\linewidth]{figs/96/23.png} \\
\includegraphics[width=0.2\linewidth]{figs/154/5.png} &
\includegraphics[width=0.2\linewidth]{figs/154/7.png} &
\includegraphics[width=0.2\linewidth]{figs/154/9.png} &
\includegraphics[width=0.2\linewidth]{figs/154/11.png} &
\includegraphics[width=0.2\linewidth]{figs/154/13.png} \\
\includegraphics[width=0.2\linewidth]{figs/215/1.png} &
\includegraphics[width=0.2\linewidth]{figs/215/2.png} &
\includegraphics[width=0.2\linewidth]{figs/215/5.png} &
\includegraphics[width=0.2\linewidth]{figs/215/10.png} &
\includegraphics[width=0.2\linewidth]{figs/215/13.png} \\
\end{tabular} \egroup
\end{center}
\caption{Visualization results of CompFeat. Each row has five sampled frames from a video sequence. Categories, bounding boxes and instance masks are shown for each object. Note objects with the same predicated identity across frames are marked with the same color. Zoom in to see details.}
\label{exp:v1}
\end{figure}


{\bf Effectiveness of Correlation-based Tracking Module}
The ablation study on the proposed correlation-based tracking module is shown in Table~\ref{exp:t3}. Again, the results are without post-processing. Compared to the baseline model, the tracking module with correlation map outperforms the baseline model by more than 1\% on AP, AP and AP. This improvement indicates the cross-correlation map between objects and the whole frames contains more semantic information than the object features used by the baseline. In addition, when integrating it into the whole framework with dual attention on frame level or object level, we obtain consistently better performance. In particular, by using correlation maps and dual attention module on frame level, we improve the performance in AP from 25.8\% to 26.3\%. Finally, we evaluate the performance of video instance segmentation with all our proposed modules, \eg frame level dual attention module, object level dual attention module and correlation maps. As shown in the last row in Table~\ref{exp:t3}, we obtain the best performance. For instance, we achieve 28.4\% and 47.4\% on AP and  with all proposed modules, which surpasses the baseline model by 4.3\% and 4.8\%. 



{\bf Sampling Method.} The VIS performance under different sampling methods are shown in Table~\ref{exp:t6}. Note that, for fairness, all experiment use the uniform sampling during testing. From Table~\ref{exp:t6}, it can be observed that the random sampling is always powerful than the uniform one during training. The reason is that random sampling increases the variety of samples during training, which can lead our proposed ComFeat model discovers more temporal and spatial correspondences between current frames and support frames.





\subsection{Comparison with the State-of-the-Arts}
Mask-Track RCNN~\cite{yang2019video} is the first work on video instance segmentation. There are several work proposed in the Large-Scale Video Object Segmentation challenge~\cite{luiten2019video,wang2019empirical, dong2019temporal}, but it is hard to compare with them since they use different backbone networks and different external training data. We borrow experimental results of other approaches from~\cite{yang2019video}. The comparison results are presented in Table~\ref{exp:t4}. 
Mask-Track RCNN~\cite{yang2019video} is an online method which learns feature similarities for object matching. And SipMask~\cite{Cao_SipMask_ECCV_2020} shares the similar structure while replace the instance segmentation branch with an one stage instance segmentation module. Compared with these methods, our proposed CompFeat achieves the best performance under all evaluation metrics. Compared with our baseline Mask-Track RCNN, the proposed CompFeat outperform it by a large margin. 
Note this performance gain is not from the additional training data since MaskTrack R-CNN with the same training data only achieves 32.2\% and 52.1\% on AP and AP.

\subsection{Qualitative Results}
Fig.~\ref{exp:v1} shows some qualitative results of our proposed CompFeat on YouTube-VIS validation set. Each row represents the predicted results on different frames in a video. The objects with the same identity are shown in the same color. As shown, CompFeat makes accurate predictions on object categories, bounding boxes, masks and identities under challenging conditions, \ie  multiple similar objects (row 1, 2), moderate occlusions (row 3), and drastic appearance changes (row 4).
The last row shows a challenging case with six fish where our algorithm performs much better than MaskTrack-RCNN~\cite{yang2019video} although it misses a fish in the third image. 


 \section{Conclusion}
In this paper, we develop a comprehensive approach for feature aggregation for video instance segmentation, which is an underexplored direction in this area.
Attention mechanisms are careful crafted for feature aggregations on both frame-level and object-level in both temporal and spatial manner.
A new tracking module is  designed to enhance local discriminative power of features with local and global correlation maps, in order to improve robustness of object tracking and re-identification. The effectiveness of the proposed modules is systematically evaluated with extensive experiments and ablation studies on the YouTube-VIS dataset.
%
 %
 


\bibliography{sample-base}

\end{document}
