\section{Experiments}
\label{experiments}

We present benchmarks on four different application domains: speech, images, natural language and reinforcement learning. For every domain we train CPC models and probe what the representations contain with either a linear classification task or qualitative evaluations, and in reinforcement learning we measure how the auxiliary CPC loss speeds up learning of the agent.

\subsection{Audio}
\label{audio_exp}

\begin{figure}[t]

\begin{minipage}{0.47\textwidth}
  \centering
  \includegraphics[width=0.99\textwidth]{figures/speakers_tsne.png}
  \caption{t-SNE visualization of audio (speech) representations for a subset of 10 speakers (out of 251). Every color represents a different speaker.}
  \label{speaker_tsne}
\end{minipage}\hfill
\begin{minipage}{0.47\textwidth}
  \centering
  \includegraphics[width=0.99\textwidth]{figures/au_prediction_acc.png}
  \caption{Average accuracy of predicting the positive sample in the contrastive loss for 1 to 20 latent steps in the future of a speech waveform. The model predicts up to 200ms in the future as every step consists of 10ms of audio.}
  \label{pred_acc}
\end{minipage}
\hfill
\end{figure}

\begin{table}[t]
  \hspace*{\fill}
  \begin{minipage}{0.4\textwidth}
      \begin{tabularx}{1.0\textwidth}{l|c}
        \toprule
        \textbf{Method} & \textbf{ACC} \\
        \midrule\midrule
        \textbf{Phone classification} & \\
        Random initialization & 27.6 \\
        MFCC features & 39.7 \\
        CPC & 64.6 \\
        Supervised & 74.6 \\
        \midrule
        \textbf{Speaker classification} & \\
        Random initialization & 1.87 \\
        MFCC features & 17.6 \\
        CPC & 97.4 \\
        Supervised & 98.5 \\    
        \bottomrule
      \end{tabularx}
      \vspace{5pt}
      \caption{LibriSpeech phone and speaker classification results. For phone classification there are 41 possible classes and for speaker classification 251. All models used the same architecture and the same audio input sizes.}
      \label{tab:librispeech}
    \end{minipage}
  \hfill
  \begin{minipage}{0.4\textwidth}
      \centering
      \begin{tabularx}{1.0\textwidth}{l|c}
        \toprule
        \textbf{Method} & \textbf{ACC} \\
        \midrule\midrule
        \textbf{\#steps predicted} & \\
        2 steps & 28.5 \\
        4 steps & 57.6 \\
        8 steps & 63.6 \\
        12 steps & 64.6 \\
        16 steps & 63.8 \\
        \textbf{Negative samples from} & \\
        Mixed speaker & 64.6 \\
        Same speaker & 65.5 \\
        Mixed speaker (excl.) & 57.3 \\
        Same speaker (excl.) & 64.6 \\
        Current sequence only & 65.2 \\
        \bottomrule
      \end{tabularx}
      \vspace{5pt}
      \caption{LibriSpeech phone classification ablation experiments. More details can be found in Section \ref{audio_exp}.
      }
      \label{tab:librispeech_ablation}
    \end{minipage}
    \hspace*{\fill}
    \vspace{-0.5cm}
\end{table}

For audio, we use a 100-hour subset of the publicly available LibriSpeech dataset \cite{panayotov2015librispeech}. Although the dataset does not provide labels other than the raw text, we obtained force-aligned phone sequences with the Kaldi toolkit \cite{povey2011kaldi} and pre-trained models on Librispeech\footnote{www.kaldi-asr.org/downloads/build/6/trunk/egs/librispeech/}. We have made the aligned phone labels and our train/test split available for download on Google Drive\footnote{https://drive.google.com/drive/folders/1BhJ2umKH3whguxMwifaKtSra0TgAbtfb}.
The dataset contains speech from 251 different speakers.

The encoder architecture  used in our experiments consists of a strided convolutional neural network that runs directly on the 16KHz PCM audio waveform. We use five convolutional layers with strides [5, 4, 2, 2, 2], filter-sizes [10, 8, 4, 4, 4] and 512 hidden units with ReLU activations. The total downsampling factor of the network is 160 so that there is a feature vector for every 10ms of speech, which is also the rate of the phoneme sequence labels obtained with Kaldi. We then use a GRU RNN \cite{cho2014learning} for the autoregressive part of the model,  with 256 dimensional hidden state. The output of the GRU at every timestep is used as the context  from which we predict 12 timesteps in the future using the contrastive loss. We train on sampled audio windows of length 20480. We use the Adam optimizer \cite{kingma2014adam} with a learning rate of 2e-4, and use 8 GPUs each with a minibatch of 8 examples from which the negative samples in the contrastive loss are drawn. The model is trained until convergence, which happens roughly at 300,000 updates.

Figure \ref{pred_acc} shows the accuracy of the model to predict latents in the future, from 1 to 20 timesteps. We report the average number of times the logit for the positive sample is higher than for the negative samples in the probabilistic contrastive loss. This figure also shows that the objective is neither trivial nor impossible, and as expected the prediction task becomes harder as the target is further away.

To understand the representations extracted by CPC, we measure the phone prediction performance with a linear classifier trained on top of these features, which shows how linearly separable the relevant classes are under these features. We extract the outputs of the GRU (256 dimensional), i.e. , for the whole dataset after model convergence and train a multi-class linear logistic regression classifier. The results are shown in Table \ref{tab:librispeech} (top). We compare the accuracy with three baselines: representations from a random initialized model (i.e.,  and  are untrained), MFCC features, and a model that is trained end-to-end supervised with the labeled data. These two models have the same architecture as the one used to extract the CPC representations. The fully supervised model serves as an indication for what is achievable with this architecture.
We also found that not all the information encoded is linearly accessible. When we used a single hidden layer instead the accuracy increases from \textbf{64.6} to \textbf{72.5}, which is closer to the accuracy of the fully supervised model.

Table \ref{tab:librispeech_ablation} gives an overview of two ablation studies of CPC for phone classification. In the first set we vary the number of steps the model predicts showing that predicting multiple steps is important for learning useful features. In the second set we compare different strategies for drawing negative sample, all predicting 12 steps (which gave the best result in the first ablation). In the mixed speaker experiment the negative samples contain examples of different speakers (first row), in contrast to same speaker experiment (second row). In the third and fourth experiment we exclude the current sequence to draw negative samples from (so only other examples in the minibatch are present in ) and in the last experiment we only draw negative samples within the sequence (thus all samples are from the same speaker).

Beyond phone classification, Table \ref{tab:librispeech} (bottom) shows the accuracy of performing speaker identity (out of 251) with a linear classifier from the same representation (we do not average utterances over time). Interestingly, CPCs capture both speaker identity and speech contents, as demonstrated by the good accuracies attained with a simple linear classifier, which also gets close to the oracle, fully supervised networks.

Additionally, Figure \ref{speaker_tsne} shows a t-SNE visualization \cite{maaten2008visualizing} of how discriminative the embeddings are for speaker voice-characteristics.
It is important to note that the window size (maximum context size for the GRU) has a big impact on the performance, and longer segments would give better results. Our model had a maximum of 20480 timesteps to process, which is slightly longer than a second.

\subsection{Vision}

\begin{figure*}[t!]
  \center
  \includegraphics[width=0.90\textwidth]{figures/contrastive_vision.pdf}
  \caption{Visualization of Contrastive Predictive Coding for images (2D adaptation of Figure \ref{fig:overview}).}
  \label{fig:overview_img}
\end{figure*}

\begin{figure}[t]
  \centering
  \begin{minipage}{1.0\textwidth}
  \includegraphics[width=1.0\textwidth]{figures/activ_1.png}
  \vspace{0.1cm}
  \includegraphics[width=1.0\textwidth]{figures/activ_2.png}
  \vspace{0.1cm}
  \includegraphics[width=1.0\textwidth]{figures/activ_3.png}
  \vspace{0.1cm}
  \includegraphics[width=1.0\textwidth]{figures/activ_4.png}
  \vspace{0.1cm}
  \includegraphics[width=1.0\textwidth]{figures/activ_5.png}
  \vspace{0.1cm}
  \includegraphics[width=1.0\textwidth]{figures/activ_6.png}
  \vspace{0.1cm}
  \includegraphics[width=1.0\textwidth]{figures/activ_7.png}
  \vspace{0.1cm}
  \includegraphics[width=1.0\textwidth]{figures/activ_8.png}
  \vspace{0.1cm}
  \includegraphics[width=1.0\textwidth]{figures/activ_9.png}
  \vspace{0.1cm}
  \includegraphics[width=1.0\textwidth]{figures/activ_10.png}
  \vspace{0.1cm}
  \includegraphics[width=1.0\textwidth]{figures/activ_11.png}
  \caption{Every row shows image patches that activate a certain neuron in the CPC architecture.}
  \label{neuron_activation}
  \end{minipage}
   \vspace{-0.5cm}
\end{figure}

In our visual representation experiments we use the ILSVRC ImageNet competition dataset \cite{ILSVRC15}. The ImageNet dataset has been used to evaluate unsupervised vision models by many authors \cite{wang2015unsupervised, Doersch_2015_ICCV, donahue2016adversarial, zhang2016colorful, noroozi2016unsupervised, doersch2017multi}. We follow the same setup as \cite{doersch2017multi} and use a ResNet v2 101 architecture \cite{he2016identity} as the image encoder  to extract CPC representations (note that this encoder is not pretrained). We did not use Batch-Norm \cite{ioffe2015batch}. After unsupervised training, a linear layer is trained to measure classification accuracy on ImageNet labels.

The training procedure is as follows: from a 256x256 image we extract a 7x7 grid of 64x64 crops with 32 pixels overlap. Simple data augmentation proved helpful on both the 256x256 images and the 64x64 crops. The 256x256 images are randomly cropped from a 300x300 image, horizontally flipped with a probability of 50\% and converted to greyscale. For each of the 64x64 crops we randomly take a 60x60 subcrop and pad them back to a 64x64 image. 

Each crop is then encoded by the ResNet-v2-101 encoder. We use the outputs from the third residual block, and spatially mean-pool to get a single 1024-d vector per 64x64 patch. This results in a 7x7x1024 tensor. Next, we use a PixelCNN-style autoregressive model \cite{aaron2016pixelcnn} (a convolutional row-GRU PixelRNN \cite{aaron2016pixelrnn} gave similar results) to make predictions about the latent activations in following rows top-to-bottom, visualized in Figure \ref{fig:overview_img}. We predict up to five rows from the 7x7 grid, and we apply the contrastive loss for each patch in the row. We used Adam optimizer with a learning rate of 2e-4 and trained on 32 GPUs each with a batch size of 16.

For the linear classifier trained on top of the CPC features we use SGD with a momentum of 0.9, a learning rate schedule of 0.1, 0.01 and 0.001 for 50k, 25k and 10k updates and batch size of 2048 on a single GPU. Note that when training the linear classifier we first spatially mean-pool the 7x7x1024 representation to a single 1024 dimensional vector. This is slightly different from \cite{doersch2017multi} which uses a 3x3x1024 representation without pooling, and thus has more parameters in the supervised linear mapping (which could be advantageous).

Tables \ref{tab:imagenet_top1} and \ref{tab:imagenet} show the top-1 and top-5 classification accuracies compared with the state-of-the-art. Despite being relatively domain agnostic, CPCs improve upon state-of-the-art by 9\% absolute in top-1 accuracy, and 4\% absolute in top-5 accuracy.

\begin{table}[t]
\begin{minipage}{0.47\textwidth}
  \begin{tabularx}{0.99\textwidth}{l|c}
    \toprule
    \textbf{Method} & \textbf{Top-1 ACC} \\
    \midrule\midrule
    \textbf{Using AlexNet conv5} \\
    Video \cite{wang2015unsupervised} & 29.8 \\
    Relative Position \cite{Doersch_2015_ICCV} & 30.4 \\
    BiGan \cite{donahue2016adversarial} & 34.8 \\
    Colorization \cite{zhang2016colorful} & 35.2 \\
    Jigsaw \cite{noroozi2016unsupervised} * & 38.1 \\
    \midrule
    \textbf{Using ResNet-V2} \\
    Motion Segmentation \cite{doersch2017multi} & 27.6 \\
    Exemplar \cite{doersch2017multi} & 31.5 \\
    Relative Position \cite{doersch2017multi} & 36.2 \\ 
    Colorization \cite{doersch2017multi} & 39.6 \\
    \textbf{CPC} & \textbf{48.7} \\
    \bottomrule
  \end{tabularx}
  \vspace{5pt}
  \caption{ImageNet top-1 unsupervised classification results. *Jigsaw is not directly comparable to the other AlexNet results because of architectural differences.}
  \label{tab:imagenet_top1}
\end{minipage}\hfill
\begin{minipage}{0.47\textwidth}
  \centering
  \vspace{58pt}
  \begin{tabularx}{0.99\textwidth}{l|c}
    \toprule
    \textbf{Method} & \textbf{Top-5 ACC} \\
    \midrule\midrule
    Motion Segmentation (MS) & 48.3 \\
    Exemplar (Ex) & 53.1 \\
    Relative Position (RP) & 59.2 \\ 
    Colorization (Col) & 62.5 \\
    Combination of & \\
    \quad MS + Ex + RP + Col & 69.3 \\
    \textbf{CPC} & \textbf{73.6} \\
    \bottomrule
  \end{tabularx}
  \vspace{5pt}
  \caption{ImageNet top-5 unsupervised classification results. Previous results with MS, Ex, RP and Col were taken from \cite{doersch2017multi} and are the best reported results on this task.}
  \label{tab:imagenet}
\end{minipage}
\hfill
\end{table}

\subsection{Natural Language}

\begin{table*}[ht]
  \centering
  \begin{tabularx}{0.73\textwidth}{l|c|c|c|c|c}
    \toprule
    \textbf{Method} & \textbf{MR} & \textbf{CR} & \textbf{Subj} & \textbf{MPQA} & \textbf{TREC}  \\
    \midrule\midrule
    Paragraph-vector \cite{le2014distributed} & 74.8 & 78.1 & 90.5 & 74.2 & 91.8 \\  
    Skip-thought vector \cite{kiros2015skip} & 75.5 & 79.3 & 92.1 & 86.9 & 91.4 \\
    Skip-thought + LN \cite{ba2016layernorm} & 79.5 & 82.6 & 93.4 & 89.0 & -	\\
    \midrule
    CPC & 76.9 & 80.1 & 91.2 &	87.7 & 96.8  \\
    \bottomrule
  \end{tabularx}
  \vspace{5pt}
  \caption{Classification accuracy on five common NLP benchmarks. We follow the same transfer learning setup from Skip-thought vectors \cite{kiros2015skip} and use the BookCorpus dataset as source. \cite{le2014distributed} is an unsupervised approach to learning sentence-level representations. \cite{kiros2015skip} is an alternative unsupervised learning approach. \cite{ba2016layernorm} is the same skip-thought model with layer normalization trained for 1M iterations.}
  \label{tab:textclass}
\end{table*}

Our natural language experiments follow closely the procedure from \cite{kiros2015skip} which was used for the skip-thought vectors model. We first learn our unsupervised model on the BookCorpus dataset \cite{zhu2015aligning}, and evaluate the capability of our model as a generic feature extractor by using CPC representations for a set of classification tasks.
To cope with words that are not seen during training, we employ vocabulary expansion the same way as \cite{kiros2015skip}, where a linear mapping is constructed between word2vec and the word embeddings learned by the model.

For the classification tasks we used the following datasets: movie review sentiment (MR) \cite{pang2005seeing}, customer product reviews (CR) \cite{hu2004mining}, subjectivity/objectivity \cite{pang2004sentimental}, opinion polarity (MPQA) \cite{wiebe2005annotating} and question-type classification (TREC) \cite{li2002learning}. As in \cite{kiros2015skip} we train a logistic regression classifier and evaluate with 10-fold cross-validation for MR, CR, Subj, MPQA and use the train/test split for TREC. A L2 regularization weight was chosen via cross-validation (therefore nested cross-validation for the first 4 datasets). 

Our model consists of a simple sentence encoder  (a 1D-convolution + ReLU + mean-pooling) that embeds a whole sentence into a 2400-dimension vector , followed by a GRU (2400 hidden units) which predicts up to 3 future sentence embeddings with the contrastive loss to form . We used Adam optimizer with a learning rate of 2e-4 trained on 8 GPUs, each with a batch size of 64. We found that more advanced sentence encoders did not significantly improve the results, which may be due to the simplicity of the transfer tasks (e.g., in MPQA most datapoints consists of one or a few words), and the fact that bag-of-words models usually perform well on many NLP tasks \cite{wang2012nlpclassification}.

Results on evaluation tasks are shown in Table \ref{tab:textclass} where we compare our model against other models that have been used using the same datasets. The performance of our method is very similar to the skip-thought vector model, with the advantage that it does not require a powerful LSTM as word-level decoder, therefore much faster to train. Although this is a standard transfer learning benchmark, we found that models that learn better relationships in the childeren books did not necessarily perform better on the target tasks (which are very different: movie reviews etc). We note that better  \cite{zhao2015self,radford2017learning} results have been published on these target datasets, by transfer learning from a different source task.

\subsection{Reinforcement Learning}

\begin{figure*}[t]
  \includegraphics[width=\textwidth]{figures/rl_1.png}
  \caption{Reinforcement Learning results for 5 DeepMind Lab tasks used in \cite{lasse2018impala}. Black: batched A2C baseline, Red: with auxiliary contrastive loss.}
  \label{fig:dmlab_rl}
  \vspace{-0.3cm}
\end{figure*}

Finally, we evaluate the proposed unsupervised learning approach on five reinforcement learning in 3D environments of DeepMind Lab \cite{beattie2016deepmind}: \mbox{rooms\_watermaze}, \mbox{explore\_goal\_locations\_small}, \mbox{seekavoid\_arena\_01}, \mbox{lasertag\_three\_opponents\_small} and \mbox{rooms\_keys\_doors\_puzzle}.

This setup differs from the previous three. Here, we take the standard batched A2C \cite{mnih2016asynchronous} agent as base model and add CPC as an auxiliary loss. We do not use a replay buffer, so the predictions have to adapt to the changing behavior of the policy. The learned representation encodes a distribution over its future observations.

Following the same approach as \cite{lasse2018impala}, we perform a random search over the entropy regularization weight, the learning-rate and epsilon hyperparameters for RMSProp \cite{hinton2012neural}. The unroll length for the A2C is 100 steps and we predict up to 30 steps in the future to derive the contrastive loss. The baseline agent consists of a convolutional encoder which maps every input frame into a single vector followed by a temporal LSTM. We use the same encoder as in the baseline agent and only add the linear prediction mappings for the contrastive loss, resulting in minimal overhead which also showcases the simplicity of implementing our method on top of an existing architecture that has been designed and tuned for a particular task. We refer to \cite{lasse2018impala} for all other hyperparameter and implementation details. 

Figure \ref{fig:dmlab_rl} shows that for 4 out of the 5 games performance of the agent improves significantly with the contrastive loss after training on 1 billion frames. For \mbox{lasertag\_three\_opponents\_small}, contrastive loss does not help nor hurt. We suspect that this is due to the task design, which does not require memory and thus yields a purely reactive policy.
