\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{amssymb} \usepackage[colorlinks=true,linkcolor=blue,anchorcolor=blue,citecolor=blue,urlcolor=blue]{hyperref}
 





\begin{document}
\title{Multi-branch and Multi-scale Attention Learning for Fine-Grained Visual Categorization}
\titlerunning{Multi-branch and Multi-scale Attention Learning for FGVC}
\author{Fan Zhang\inst{}\orcidID{0000-0003-2504-4651} \and
Meng Li\inst{}\orcidID{0000-0002-1543-5029} \and
Guisheng Zhai\inst{}\orcidID{0000-0001-9901-010X}\and
Yizhao Liu\inst{}\orcidID{0000-0002-9549-7317}}
\authorrunning{F. Zhang et al.}
\institute{School of MEIE , China University of Mining and Technology (Beijing), Beijing, China \\
\email{sqt1800407119@student.cumtb.edu.cn}}
\maketitle              \begin{abstract}
ImageNet Large Scale Visual Recognition Challenge (ILSVRC) is one of the most authoritative academic competitions in the field of Computer Vision (CV) in recent years. But applying ILSVRC's annual champion directly to fine-grained visual categorization (FGVC) tasks does not achieve good performance. To FGVC tasks, the small inter-class variations and the large intra-class variations make it a challenging problem. Our attention object location module (AOLM) can predict the position of the object and attention part proposal module (APPM) can propose informative part regions without the need of bounding-box or part annotations. The obtained object images not only contain almost the entire structure of the object, but also contains more details, part images have many different scales and more fine-grained features, and the raw images contain the complete object. The three kinds of training images are supervised by our multi-branch network. Therefore, our multi-branch and multi-scale learning network(MMAL-Net) has good classification ability and robustness for images of different scales. Our approach can be trained end-to-end, while provides short inference time. Through the comprehensive experiments demonstrate that our approach can achieves state-of-the-art results on CUB-200-2011, FGVC-Aircraft and Stanford Cars datasets. Our code will be available at \url{https://github.com/ZF1044404254/MMAL-Net}
\keywords{FGVC, Classification, Attention, Location, Scale.}
\end{abstract}
\section{Introduction}
How to tell a dog's breed? This is a frequently asked question because dogs have similar characteristics. The FGVC direction of CV research focuses on such issues, and it is also called sub-category recognition. In recent years, it is a very popular research topic in CV, pattern recognition and other fields. It's purpose is to make a more detailed sub-class division for coarse-grained large categories (e.g. classifying bird species \cite{WahCUB_200_2011}, aircraft models \cite{maji2013fine}, car models \cite{KrauseStarkDengFei-Fei_3DRR2013}, etc.).\\
\indent Many papers \cite{lin2015bilinear,zhang2016spda,wei2016mask,lam2017fine} have shown that  the key to fine-grained visual categorization tasks lies in developing effective methods to accurately identify informative regions in an image. They leverage the extra annotations of bounding box and part annotations to localize significant regions. However, obtaining such dense annotations of bounding box and part annotations is labor-intensive, which limits both scalability and practicality of real-world fine-grained applications. Other methods \cite{zhao2017diversified,zheng2017learning,yang2018learning,zheng2019looking} use an unsupervised learning scheme to localize informative regions. They eliminate the need for the expensive annotations, but how to focus on the right areas and use them is still worth investigating.\\
\indent An overview of our MMAL-Net is shown in Fig. \ref{fg1}. Our method has three branches in training phase,  whose raw branch mainly studies the overall characteristics of the object, and AOLM needs to obtain the object's bounding box information with the help of the feature maps of the raw image from this branch. As the input of object branch, the finer scale of object image is very helpful for classification, because it contains the structural features of the target as well as the fine-grained features. Then, APPM proposes several part regions with the most discrimency and less redundancy according to the feature maps of object image. The part branch sends the part image clipped from the object image to the network for training. It enables the network to learn fine-grained features of different parts in different scales. Unlike RA-CNN \cite{fu2017look}, the parameters of CNN and FC in our three branches are shared. Therefore, through the common learning process of the three branches, the trained model has a good classification ability for different scales and parts of object. In the testing phase, unlike RA-CNN \cite{fu2017look} and NTS-Net \cite{yang2018learning} need to calculate the feature vectors of the multiple part region images and then concat these vectors for classification. After our repeated experiments, the best classification performance is simply obtained by the result of object branch, so our approach can reduce some calculations and inference time while achieving high accuracy.\\
\indent Our main contributions can be summarized as follows: 
\begin{itemize}
\item[]Our multi-branch network can be trained end-to-end and learn object's discriminative regions for recognition effectively.
\item[]The AOLM does not increase the number of parameters, so we do not need to train a proposal network like RA-CNN \cite{fu2017look}. The accuracy of object localization is achieved by only using category labels.
\item[]We present an attention part proposal method(APPM) without the need of part annotations. It can select multiple ordered discriminative part images, so that the model can effectively learn different scales parts's fine-grained features.
\item[]State-of-the-art performances are reported on three standard benchmark datasets, where our method stable outperforms the state-of-the-art methods and baselines.
\end{itemize}
\begin{figure}[h]
\includegraphics[width=\textwidth]{network_v10}
\caption{The overview of our proposed MMAL-Net in the training phase, The red branch is raw branch, the orange branch is object branch, and the blue branch is part branch. In the dotted green box is the network structure for the test phase. The CNN (Convolutional Neural Networks) and FC (Fully Connection) layer of the same color represent parameter sharing. Our multi-branch all use cross entropy loss as the classification loss.} \label{fg1}
\end{figure}
\section{Related works}
In the past few years, the accuracy of benchmark on open datasets has been improved based on deep learning and fine-grained classification methods. They can be classified as follows: 1) By end-to-end feature encoding; 2) By localization-classification subnetworks. 
\subsection{By End-to-End Feature Encoding}
This kind of method directly learns a more discriminative feature representation by developing powerful deep models for fine-grained recognition. The most representative method among them is Bilinear-CNN \cite{lin2015bilinear}, which represents an image as a pooled outer product of features derived from two bilinear models, and thus encodes higher order statistics of convolutional activations to enhance the mid-level learning capability. Thanks to its high model capacity, it achieves clear performance improvement on a wide range of visual tasks. However, the high dimensionality of bilinear features still limits its further generalization. In order to solve this problem, \cite{gao2016compact,cui2017kernel} try to aggregate low-dimensional embeddings by applying tensor sketching. They can reduce the dimensionality of bilinear features and achieve comparable or higher classification accuracy.
\subsection{By Localization-Classification Subnetworks}
This kind of method trains a localization subnetwork with supervised or weakly supervised to locates key part regions. Then the classification subnetwork uses the information of fine-grained regions captured by the localization subnetwork to further enhance its classification capability. Earlier works \cite{zhang2016spda,wei2016mask,lam2017fine} belong to full supervision method depend on more than image-level annotations to locate semantic key parts. \cite{lam2017fine} trained a region proposal network to generate proposals of informative image parts and concatenate multiple part-level features as a whole image representation toward final fine-grained recognition. However, maintaining such dense part annotations increases additional location labeling cost.
Therefore, \cite{zhao2017diversified,zheng2017learning,yang2018learning,zheng2019looking} take advantage of attention mechanisms to avoid this problem. There is no need of bounding box annotations and part annotations except image-level annotation.
\section{Method}
\subsection{Attention Object Location Module (AOLM)}
This method was inspired by SCDA \cite{wei2017selective}. SCDA uses a pre-trained model to extract image features for fine-grained image retrieval tasks. We improve it's positioning performance as much as possible through some measures. At the first, we describe the process of generating object location coordinates by processing the CNNs feature map as the Fig. \ref{fg2} illustrated. 
\begin{figure}
\includegraphics[width=\textwidth]{aolm}
\caption{The pipeline of the AOLM, we first get an activation map by aggregating the feature maps in the channel dimension, then obtain a bounding box according to activation map.} \label{fg2}
\end{figure}

We use   to represent feature maps with  channels and spatial size  output from the last convolutional layer of an input image  and  is the i-th feature map of the corresponding channel. As shown in Equ \ref{eq1}, 

activation map  can be obtained by aggregating the feature maps . We can visualizes where the deep neural networks focus on for recognition simply and locates the object regions accurately from A. As shown in Equ \ref{eq2},  is the mean value of .

 is used as the threshold to determine whether the element at that position in  is a part of object, and  is a particular position in a  activation map. Then we initially obtained a coarse mask map  from the last convolutional layer   of ResNet-50 \cite{he2016deep} according to Equ \ref{eq3}.

On the basis of the experimental results, we find that the object is often in the largest connected component of , so the smallest bounding box containing the largest connected area is used as the result of our object location. Only using a pre-trained VGG16 \cite{simonyan2014very} in SCDA \cite{wei2017selective} achieved better position accuracy, but our pre-trained ResNet-50 does not reach a similar accuracy rate and dropped significantly. So we use the training set to train ResNet-50 for improving object location accuracy and experiments in subsection 4.5 verify the effectiveness of this approach. Then, inspired by \cite{wei2017selective} and \cite{long2015fully} the performance of their methods all benefit from the ensembel of Multiple layers. So we get the activation map of the output of  according to Equ \ref{eq1}, which is one block in front of . Then we can get  according to Equ \ref{eq3}, and finally we can get a more accurate mask M after taking the intersection of  and . As shown in Equ \ref{eq4}.

Subsequent experimental results prove the effectiveness of these approaches to improve object location accuracy. This improved weakly supervised object location method can achieve higher localization accuracy than ACOL \cite{zhang2018adversarial}, ADL \cite{choe2019attention} and SCDA \cite{wei2017selective}, without adding trainable parameters.
\subsection{Attention Part Proposal Module(APPM)}
Although AOLM can achieve higher localization accuracy, but there are some positioning results are part of the object. We improve the robustness of the model to this situation through APPM and part branch. It will be demonstrated in the next section. By observing the activation map , we find that the area with high activation value of the activation map are often the area where the key part are located, such as the head area in the example. Using the idea of sliding window in object detection to find the windows with information as part images. Moreover, we implemented the traditional sliding window approach with full convolutional network to reduce the amount of calculation, just like Overfeat \cite{sermanet2013overfeat} gets the feature map of different windows from the feature map output from the previous branch. Then we aggregate each window's activation map  in the channel dimension and get its activation mean value  according to Equ \ref{eq5}, 

 ,  are the height and width of a window's feature map. We sort by the  value of all windows. The larger the  is, the larger the informativeness of this part region is, as shown in Fig. \ref {fg3}. However, we cannot directly select the first few windows, because they are often adjacent to the largest  windows and contain approximate the same part, but we hope to select as many different parts as possible. In order to reduce region redundancy, we adopt non-maximum suppression (NMS) to select a fixed number of windows as part images with different scales. By visualizing the output of this module in Fig. \ref {fg4}, it can be seen that this method proposed some ordered, different importance degree part regions.
\begin{figure}
\includegraphics[width=\textwidth]{APPM_v4}
\caption{The simple pipeline of the APPM. We use red, orange, yellow, green colors to indicate the order of windows' .} \label{fg3}
\end{figure}
\subsection{Architecture of MMAL-Net}
In order to make the model fully and effectively learn the images obtained through AOLM and APPM. During the training phase, we construct a three branches network structure consisting of raw branch, object branch, and part branch, as shown in fig. \ref {fg1}. The three branches share a CNN for feature extraction and a FC layer for classification. Our three branches all use cross entropy loss as the classification loss. As shown in Equ \ref{eq6}, \ref{eq7}, and \ref{eq8}, respectively.



Where  is the ground truth label of the input image, ,  are the category probabilities of the last softmax layer output of the raw branch and object branch, respectively ,  is the output of the softmax layer of the part branch corresponding to the th part image,  is the number of part images. The total loss is defined as:

The total loss is the sum of the losses of the three branches, which work together to optimize the performance of the model during backpropagation. It enables the final convergent model to make classification predictions based on the overall structural characteristics of the object or the fine-grained characteristics of a part. The model has good object scale adaptability, which improves the robustness in the case of inaccurate AOLM localization. During the testing phase, we removed the part branch so as to reduce a large amount of calculations, so our method will not take too long to predict in practical applications. Due to our reasonable and efficient framework. MMAL-Net can achieves the state-of-the-art performance so far.
\section{Experiments}
\subsection{Datasets}
These three datasets are widely used as benchmarks for fine-grained classification(shown in Table~\ref {tb1}). In our experiments, we only use the image classification labels provided by these datasets. 
\begin{table}
\centering
\caption{Introduction of the three datasets used in this paper.}\label{tb1}
\begin{tabular}{|c|c|c|c|c|}
\hline
Datasets & Object & Class & Train & Test\\
\hline
CUB-200-2011(CUB) \cite{WahCUB_200_2011} & Bird & 200 & 5994 &  5794\\
FGVC-Aircraft(AIR) \cite{maji2013fine} & Aircraft & 100  & 6667 &  3333\\
Stanford Cars(CAR) \cite{KrauseStarkDengFei-Fei_3DRR2013} & Car &  196  & 8144 &  8041\\
\hline
\end{tabular}
\end{table}
\subsection{Implementation Details}
In all our experiments, we first preprocess images to size  to get input image for raw branch and object branch. The object image is also scaled into  , but all part images are resized to  for part branch. We construct windows with three broad categories of scales: \{[, ], [, ], [, , , ]\} for activation map of  size, and the number of a raw image's part images is  , among them . ,  and  are the number of three broad categories of scales windows mentioned above. ResNet-50 \cite{he2016deep} pre-trained on ImageNet is used as the backbone of our network structure. During training and testing, we do not use any other annotations other than image-level labels. Our optimizer is SGD with the momentum of 0.9 and the weight decay of 0.0001, and a mini-batch size of 6 on a Tesla P100 GPU. The initial learning rate is 0.001 and multiplied by 0.1 after 60 epoch. We use Pytorch as our code-base.
\subsection{Performance Comparison}
 We compared the baseline methods mentioned above on three commonly used fine-grained classification datasets. The experimental results are shown in the Table~\ref {tb2}. By comparison, we can see that our method achieves the best accuracy currently available on these three datasets.
\begin{table}
\centering
\caption{Comparison results on three common datasets. Train Anno. represents using bounding box or part annotations in training.}\label{tb2}
\begin{tabular}{|c|c|c|c|c|}
	\hline
	Methods & Train Anno. & CUB & AIR & CAR \\
	\hline
	ResNet-50 & & & & \\
	\hline
	Bilinear-CNN \cite{lin2015bilinear} &  & 84.1 & 84.1 & 91.3 \\
	SPDA-CNN \cite{zhang2016spda} & \checkmark & 85.1 & - & - \\
	KP \cite{cui2017kernel} & & 86.2 & 86.9 & 92.4 \\
	RA-CNN \cite{fu2017look} & & 85.3 & - & 92.5 \\
	MA-CNN \cite{zheng2017learning} & & 86.5 & 89.9 & 92.8 \\
	OSME+MAMC \cite{sun2018multi} & & 86.5 & - & 93.0 \\
	PC \cite{dubey2018pairwise} & & 86.9 & 89.2 & 92.9 \\
	HBP \cite{yu2018hierarchical} & & 87.1 & 90.3 & 93.7 \\
	Mask-CNN \cite{wei2016mask} & \checkmark & 87.3 & - & - \\
	DFL-CNN \cite{wang2018learning} & & 87.4 & 92.0 & 93.8 \\
	HSnet \cite{lam2017fine} & \checkmark & 87.5 & - & - \\
	NTS-Net \cite{yang2018learning} & & 87.5 & 91.4 & 93.9 \\
	MetaFGNet \cite{zhang2018fine} & & 87.6 & - & - \\
	DCL \cite{chen2019destruction} & & 87.8 & \textbf{92.2} & \textbf{94.5} \\
	TASN \cite{zheng2019looking} & & \textbf{87.9} & - & 93.8 \\
	\hline
	Ours & & \textbf{89.6} & \textbf{94.7} & \textbf{95.0} \\
	\hline
\end{tabular}
\end{table}
\subsection{Ablation Studies}
The ablation study is performed on the CUB dataset. Without adding any of our proposed methods, the ResNet-50 \cite{he2016deep} obtained an accuracy of 84.5\% under the condition that the input image resolution is .  In order to verify the rationality of the training structure of our three branches, we remove the object branch and part branch respectively. After removing the object branch, the best accuracy is 85.0\% from the raw branch, a drop of 4.6\%. This proves the great contribution of AOLM and object branch to improve the classification accuracy. After removing the part branch, the best accuracy is 87.3\% from the object branch, down 2.3\% . The results of the experiment show that part branch and APPM can improve the robustness of the model when facing AOLM unstable positioning results. The above experiment has shown that the three branches of our method all have a significant contribution to the final accuracy. 
\subsection{Object Localization Performance}
Percentage of Correctly Localized Parts (PCP) metric is the percentage of predicted boxes that are correctly localized with more than 50\% IOU with the ground-truth bounding boxes. On the CUB dataset, the best result of AOLM in terms of the PCP metric for object localization is 85.1\%. AOLM clearly exceeds SCDA's \cite{wei2017selective} 76.8\% and the recent weakly supervised object location methods ACOL’s \cite{zhang2018adversarial} 46.0\% and  ADL’s \cite{choe2019attention} 62.3\%. As shown in Table~\ref {tb3}, the ensemble of multiple layers significantly improves object location accuracy. Through the experiment, we find that the object location accuracy using the pre-trained model directly is 65.0\%. However, it rise to 85.1\% after one iteration of training. And as the training progressed, CNN paid more and more attention to the most discerning region to improve classification accuracy which leads to localization accuracy drops to 71.1\%. Even so, due to part branch and APPM make model has good adaptability to object's scale, it still achieved excellent classification performance.
\begin{table}
\centering
\caption{Object localization accuracy on CUB-200-2011.}\label{tb3}
\begin{tabular}{|c|c|}
	\hline
	Methods & localization Accuracy \\ 
	\hline
	ACOL \cite{zhang2018adversarial} & 46.0\% \\ 
	ADL \cite{choe2019attention} & 62.3\% \\ 
	SCDA \cite{wei2017selective} &  \textbf{76.8\%} \\ 
	\hline
	AOLM(conv\_5c) & 82.2\% \\ 
	AOLM(conv\_5b \& conv\_5c) & \textbf{85.1\%} \\
	\hline
\end{tabular}
\end{table}
\subsection{Model and Inference Time Complexity}
Unlike RA-CNN \cite{fu2017look} and NTS-Net \cite{yang2018learning}, the former has three branches with independent parameters and needs to train a subnetwork to propose finer scale images and the latter needs to train a navigator network to propose regions with large amount of information (such as different body parts of birds). Our MMAL-Net has some advantages in terms of parameter volume over them. First, the three branches parameters are shared, and secondly the AOLM and APPM modules do not require training, do not add additional parameter amounts. Thirdly their calculation amount is relatively smaller. Finally, better classification performance is achieved by MMAL-Net. Compared with the ResNet-50 baseline, our method yields a significantly better result (+4.1\%) with the same parameter volume. As for inference time, RA-CNN needs to calculate the output of three branches and fuse them; NTS-Net needs to extract and fuse the 4 proposal local image features of the input image. Above reasons make their inference time relatively longer and our method has a shorter inference time. It only needs to calculate the output of the first two branches and does not need to fuse them, because the classification results are based on the output of the second branch (object branch). For more accurate comparison, we conducted an inference time test on Tesla P100 and the input image size is  . The inference time of MMAL-Net and NTS-Net  are summarized as follows: the running time on an image of NTS-Net is about 5.61 ms, and ours method’s running time on an image is about 1.80 ms. Based on the above analysis, lower model and inference time complexity both add extra practical value to our proposed method.
\subsection{Visualization of Object and Part Regions}
In order to visually analyze the areas of concern for our AOLM and APPM, we draw the object's bounding boxes and part regions proposed by AOLM and APPM in Fig. \ref {fg4}. In the first column, we use red and green rectangles to denote the ground truth and predicted bounding box in raw image. It is very helpful for classification that the positioned areas of objects often cover an almost complete object. In columns two through four, we use red, orange, yellow, and green rectangles to represent the regions with the highest average activation values in different scales proposed by APPM, with red rectangle denoting the highest one.  Fig. \ref {fg4} conveys that the proposed area does contain more fine-grained information and the order is more reasonable on the same scale, which are very helpful for model's robustness to scale. We can find that the most discriminative regions of birds are head firstly , then is body, which is similar to human cognition.
\begin{figure}
\centering
\includegraphics[width=10cm]{vis_v1}
\caption{Visualization of object and part regions.} \label{fg4}
\end{figure}
\section{Conclusion}
In this paper, we propose an effective method for fine-grained classification without bounding box or part annotations. The multi-branch structure can make full use of the images obtained by AOLM and APPM to achieve excellent performance. Our algorithm is end-to-end trainable and achieves state-of-the-art results in CUB-200-2001 \cite{WahCUB_200_2011}, FGVC Aircraft \cite{maji2013fine} and Stanford Cars \cite{KrauseStarkDengFei-Fei_3DRR2013}  datasets.The future work is how to set the number and size of windows adaptively to further improve the classification accuracy. \\




\bibliographystyle{splncs04unsort}
\bibliography{refs}
\end{document}
