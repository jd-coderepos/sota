 \documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{soul}
\usepackage{multirow}
\usepackage[percent]{overpic}
\usepackage{xcolor}
\usepackage{anyfontsize}
\usepackage{booktabs}

\usepackage{array}

\newlength\Origarrayrulewidth
\newcommand{\Cline}[1]{\noalign{\global\setlength\Origarrayrulewidth{\arrayrulewidth}}\noalign{\global\setlength\arrayrulewidth{2pt}}\cline{#1}\noalign{\global\setlength\arrayrulewidth{\Origarrayrulewidth}}}

\newcommand\Thickvrulel[1]{\multicolumn{1}{!{\vrule width 2pt}c}{#1}}

\newcommand\Thickvruler[1]{\multicolumn{1}{c!{\vrule width 2pt}}{#1}}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy 

\def\cvprPaperID{****} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

\title{HAA500: Human-Centric Atomic Action Dataset with Curated Videos}



\author{
\begin{tabular}{ccccc}
Jihoon Chung & Cheng-hsin Wuu & Hsuan-ru Yang & Yu-Wing Tai & Chi-Keung Tang
\end{tabular}
\\
HKUST,  Carnegie Mellon University,  Kuaishou
\\
{\tt\small \{jchungaa,cktang\}@cse.ust.hk, cwuu@andrew.cmu.edu, hyangap@ust.hk, yuwing@gmail.com}
}


\maketitle


\begin{abstract}


    We contribute HAA500~\footnote{
    \ifcvprfinal
        HAA500 dataset can be downloaded at https://www.cse.ust.hk/haa
    \else
        HAA500 can be downloaded from our project website, which is available in the camera-ready version of the paper.
    \fi
    }, a manually annotated human-centric atomic action dataset for action recognition on 500 classes with over 591k labeled frames. Unlike existing atomic action datasets, where coarse-grained atomic actions were labeled with action-verbs, \eg, ``Throw'', HAA500 contains fine-grained atomic actions where only consistent actions fall under the same label, \eg, ``Baseball Pitching" vs ``Free Throw in Basketball", to minimize ambiguities in action classification. 
    HAA500 has been carefully curated to capture the movement of human figures with less spatio-temporal label noises to greatly enhance the training of deep neural networks. The advantages of HAA500 include: 1) human-centric actions  with a high average of 69.7\% detectable joints for the relevant human poses; 2) each video captures the essential elements of an atomic action without irrelevant frames; 3) fine-grained atomic action classes. 
    Our extensive experiments validate the benefits of human-centric and atomic characteristics of HAA, which enables the trained model to improve prediction by attending to atomic human poses.
    We detail the HAA500 dataset statistics and collection methodology, and  compare quantitatively with existing action recognition datasets.
    
\end{abstract}



\section{Introduction}

Observe the {\em coarse} annotation provided by commonly-used action recognition datasets such as~\cite{kinetics400,momentsintime,zhao2019hacs}, 
where the same action label was assigned to a given complex video action sequence (e.g. \textit{Play Soccer}, \textit{Play Baseball}) typically lasting 
10 seconds or 300 frames, thus introducing a lot of ambiguities during training as two or more action categories may contain the same \textbf{atomic action}
(e.g., \textit{Run} is one of the atomic actions for both \textit{Play Soccer} and \textit{Play Baseball}). 




\begin{figure*}[ht!]
\begin{center}
\begin{overpic}[width=\linewidth]{images/teaser4.png}

     \put (2, 22) {Sports/Athletics}
     \put (51.5, 24.5) {Playing Musical Instruments}
     \put (51.5, 11) {Daily Actions}
     
     \put (2.5, 15.5)   {\rotatebox{90}{ \scriptsize{Soccer}}}
     \put (2.5,  6.5)   {\rotatebox{90}{ \scriptsize{Baseball}}}
     
     \put ( 4, 13.5) {\footnotesize{Run (Dribble)}}
     \put (17, 13.5) {\footnotesize{Throw In}}
     \put (29, 13.5) {\footnotesize{Shoot}}
     \put (41, 13.5) {\footnotesize{Save}}
     
     \put ( 7.5, 4.7) {\footnotesize{Run}}
     \put (18.5, 4.7) {\footnotesize{Pitch}}
     \put (29, 4.7) {\footnotesize{Swing}}
     \put (38, 4.7) {\footnotesize{Catch Flyball}}
     
     \put (54, 16) {\footnotesize{Grand Piano}}
     \put (67, 16) {\footnotesize{Cello}}
     \put (79, 16) {\footnotesize{Gong}}
     \put (88.5, 16) {\footnotesize{Recorder}}
     
     \put (55, 2.8) {\footnotesize{Applaud}}
     \put (65, 2.8) {\footnotesize{Waist Bow}}
     \put (77, 2.8) {\footnotesize{Fist Bump}}
     \put (89, 2.8) {\footnotesize{Salute}}
     
\end{overpic}
\end{center}
\vspace{-1.5em}
\caption{
    HAA500 is a fine-grained atomic action dataset, with fine-level action annotations (\eg \textit{Soccer-Dribble}, \textit{Soccer-Throw In}) compared to the traditional composite action annotations (\eg \textit{Soccer}, \textit{Baseball}). 
HAA500 is comparable to existing coarse-grained atomic action datasets, where 
we have distinctions (\eg \textit{Soccer-Throw In}, \textit{Baseball-Pitch}) within an atomic action (\eg \textit{Throw Something}) when the action difference is visible. Figure above displays sample videos from three different areas of HAA. Observe that each video contains one or few dominant human figures performing the pertinent action.
}
\vspace{-1em}
\label{fig:teaser2}
\end{figure*}

Recently, atomic action datasets~\cite{ava_speech,goyal2017something,AVA,ava_speaker,finegym} have been introduced in an attempt to resolve the aforementioned issue. Google's AVA actions dataset~\cite{AVA} provides dense annotations of 80 atomic visual actions in 430 fifteen-minute video clips where actions are localized in space and time. AVA spoken activity dataset~\cite{ava_speaker} contains temporally labeled face tracks in videos, where each face instance is labeled as speaking or not, and whether the speech is audible. The something-something dataset~\cite{goyal2017something} contains clips of humans performing pre-defined basic actions with daily objects.

However, some of their actions are still coarse which can be further split into atomic classes with significantly different motion gestures. \eg, AVA~\cite{AVA} and something-something~\cite{goyal2017something} contain \textit{Play Musical Instrument} and \textit{Throw Something} as a class, respectively, where the former should be further divided into sub-classes such as \textit{Play Piano} and \textit{Play Cello}, and the latter into \textit{Soccer Throw In} and \textit{Pitch Baseball}, etc, because each of these atomic actions has significantly different gestures. Encompassing different visual postures into a single class poses a deep neural network almost insurmountable challenge to properly learn the pertinent atomic action, which probably explains the prevailing low performance employing even the most state-of-the-art architecture, SlowFast (mAP: 34.25\%)~\cite{slowfast}, in AVA~\cite{AVA}, despite only having 80 classes. 

The other problem with existing action recognition video datasets is that their training examples contain actions irrelevant to the target action. 
Video datasets tend to have fixed clip length, allowing unrelated video frames to be easily included during the data collection stage. Kinetics 400 dataset~\cite{kinetics400}, with a fixed 10 second clip length, contains a lot of irrelevant actions, \eg, showing the audience before the main \textit{violin playing}, or a person takes a long run before \textit{kicking} the ball.
Another problem is having too limited or too broad field-of-view, where a video only exhibits a part of a human interacting with an object~\cite{goyal2017something}, or a single video contains multiple human figures with different actions present~\cite{AVA,kinetics400,zhao2019hacs}.

Recently, FineGym~\cite{finegym} has been introduced to solve the aforementioned limitations by proposing fine-grained action annotations, e.g. \textit{Balance Beam-Dismount-Salto forward Tucked}. But due to expensive data collection process, they only contain 4 events with atomic action annotations (\textit{Balance Beam}, \textit{Floor Exercise}, \textit{Uneven Bars}, and \textit{Vault-Women}), and their clips were extracted from professional gymnasium videos in athletic or competitive events.




In this paper, we contribute Human-centric Atomic Action dataset (\textbf{HAA500}) which has been constructed with carefully curated videos with an average of 69.7\% detectable joints, where a dominant human figure is present to perform the labeled action. The curated videos have been annotated with fine-grained labels to avoid ambiguity, and with dense per-frame action labeling and no unrelated frames being included in the collection as well as annotation.
HAA500 contains wide-variety of atomic actions, ranging from athletic atomic action~(\textit{Figure Skating - Ina Bauer}) to daily atomic action~(\textit{Eating a Burger}).
The clips are class-balanced and contain clear visual signals with little occlusion. As opposed to ``in the wild" atomic action datasets, our ``cultivated" clean, class-balanced dataset provides an effective alternative to advance research in atomic visual actions recognition and thus video understanding. An example of the collected atomic action is shown in Figure~\ref{fig:teaser2}.





\section{Related Works}

Table~\ref{table:Action_datasets} summarizes of representative action recognition datasets.

\subsection{Action Recognition Dataset}
\paragraph{Composite Action Dataset}
Representative action recognition datasets, such as HMDB51~\cite{HMDB51}, UCF101~\cite{ucf101}, Hollywood-2~\cite{DBLP:conf/cvpr/MarszalekLS09}, ActivityNet~\cite{activitynet}, and Kinetics~\cite{kinetics700,kinetics400}x     consist of short clips which are manually trimmed to capture a single action. These datasets are ideally suited for training fully-supervised, whole-clip video classifiers. A few datasets used in action recognition research, such as MSR Actions~\cite{DBLP:conf/cvpr/YuanLW09}, UCF Sports~\cite{DBLP:conf/cvpr/RodriguezAS08} and JHMDB~\cite{DBLP:conf/iccv/JhuangGZSB13}, provide spatio-temporal annotations in each frame for short videos, but they only contain few actions. 
Aside from the subcategory of shortening the video length, recent extensions such as UCF101~\cite{ucf101}, DALY~\cite{DBLP:journals/corr/WeinzaepfelMS16}, and Hollywood2Tubes~\cite{DBLP:conf/eccv/MettesGS16} evaluate spatio-temporal localization in untrimmed videos, resulting in a performance drop due to the more difficult nature of the task. One common issue on these aforementioned datasets is that they are annotated with composite action classes (e.g. \textit{tennis}), thus different human action gestures (\eg, \textit{backhand swing}, \textit{forehand swing}) are annotated under a single class. Another  issue is that they tend to capture in wide field-of-view and thus include multiple human figures (e.g. tennis player, referee, audience) with different actions in a single frame, which inevitably introduce confusion to action analysis and recognition. 




\setlength{\tabcolsep}{4pt}
\begin{table}
\begin{center}
        \begin{tabular}{c|c|c|c}
            \hline
            Dataset & Videos &  Actions & Atomic \\ 
            \hline 
            KTH~\cite{DBLP:conf/icpr/SchuldtLC04} & 600 & 6 & \checkmark \\
            Weizmann~\cite{DBLP:conf/iccv/BlankGSIB05} & 90 & 10 & \checkmark  \\
            UCF Sports~\cite{DBLP:conf/cvpr/RodriguezAS08} & 150 & 10 &   \\
            Hollywood-2~\cite{DBLP:conf/cvpr/MarszalekLS09} & 1,707 & 12 &  \\
            HMDB51~\cite{HMDB51} & 7,000 & 51 &   \\
            UCF101~\cite{ucf101} & 13,320 & 101 &   \\
            DALY~\cite{DBLP:journals/corr/WeinzaepfelMS16} & 510 & 10 &  \\
            AVA~\cite{AVA} & 57,600 & 80 & \checkmark  \\
            Kinetics 700~\cite{kinetics700} & 650,317 & 700 & \\
            HACS~\cite{zhao2019hacs} & 1,550,000 & 200 & \checkmark  \\
            Moments in Times~\cite{momentsintime} & 1,000,000 & 339 & \checkmark\\
            FineGym~\cite{finegym} & 32,687 & 530 & \checkmark\\
            \textbf{HAA500} & \textbf{10,000} & \textbf{500} & \checkmark \\ \hline
        \end{tabular}
\end{center}
\caption{Summary of representative action recognition datasets.}
\vspace{-1em}
\label{table:Action_datasets}
\end{table}



\paragraph{Atomic Action Dataset}

To model finer-level events, the AVA dataset~\cite{AVA} was introduced to provide person-centric spatio-temporal annotations on atomic actions similar to some of the earlier works~\cite{DBLP:conf/iccv/BlankGSIB05,DBLP:conf/icpr/SchuldtLC04}.
Other specialized datasets such as Moments in Times~\cite{momentsintime}, HACS~\cite{zhao2019hacs}, Something-Something~\cite{goyal2017something}, and Charades-Ego~\cite{sigurdsson2016hollywood} provide classes for atomic actions but none of them are human-centric atomic action, where some of the video frames are ego-centric which only show part of a human body (e.g. hand), or no human action at all. Atomic action datasets~\cite{AVA,momentsintime} tend to have atomicity under English linguistics, e.g. In Moments in Times~\cite{momentsintime} \textit{open} is annotated on video clips with a tulip opening, an eye opening, a person opening a door, or a person opening a package, which are fundamentally different actions only sharing the verb~\textit{open}, thus giving the possibility of finer division.


\paragraph{Fine-Grained Action Dataset} A different approach to designing fine-grained action datasets has been used to tackle the same problem. These methods (\eg, \cite{epickitchens,jigsaws,breakfast,diving48,MPIICooking2,finegym}) use systematic action labeling to annotate fine-grained labels on a small domain of actions. Breakfast~\cite{breakfast}, MPII Cooking 2~\cite{MPIICooking2}, and EPIC-KITCHENS~\cite{epickitchens} offer fine-grained actions for cooking and preparing dishes, \eg, \textit{twist milk bottle cap}~\cite{breakfast}.
JIGSAWS~\cite{jigsaws}, Diving48~\cite{diving48}, and FineGym~\cite{finegym} respectively offer fine-grained action dataset for surgery, diving, and gymnastics. While existing fine-grained action datasets are well suited for benchmarks, due to their low-variety and the narrow domain of the classes, they cannot be extended easily in a general-purpose action recognition.

Our HAA500 dataset differs from all of the aforementioned datasets as we provide a wide variety of 500 fine-grained atomic human action classes in various domains, where videos in each class only exhibit the relevant human atomic actions. 

\subsection{Action Recognition Architectures}

\setlength{\tabcolsep}{4pt}
\begin{table}[t]
\begin{center}
        \begin{tabular}{|c|c|c|c|c|}
            \hline
                   & \multicolumn{2}{|c|}{Kinetics 400~\cite{kinetics400}}
                   & \multicolumn{2}{|c|}{Something V1~\cite{goyal2017something}}\\ 
            Models & Top-1 &  Top-5 & Top-1 & Top-5  \\ 
            \hline 
            TSN (R-50)~\cite{TSN}   & 70.6\% & 89.2\% & 20.5\% & 47.5\% \\
            2-Stream I3D~\cite{i3d} & 71.6\% & 90.0\% & 41.6\% & 72.2\% \\
            TSM (R-50)~\cite{TSM}   & 74.1\% & 91.2\% & 47.3\% & 76.2\% \\
            TPN (TSM)~\cite{TPN}    & 78.9\% & 93.9\% & 50.2\% & 75.8\% \\
            \hline
            \hline Skeleton-based
                   & \multicolumn{2}{|c|}{Kinetics 400~\cite{kinetics400}}
                   & \multicolumn{2}{|c|}{NTU-RGB+D~\cite{nturgbd}}\\ 
            Models & Top-1 &  Top-5 & X-Sub & X-View  \\ 
            \hline
            Deep LSTM~\cite{nturgbd} & 16.4\% & 35.3\% & 62.9\% & 70.3\% \\
            ST-GCN~\cite{stgcn}      & 30.7\% & 52.8\% & 81.5\% & 88.3\% \\
            \hline
        \end{tabular}
\end{center}
\caption{Performance of previous works on Kinetics 400~\cite{kinetics400}, Something-Something~\cite{goyal2017something}, and NTU-RGB+D~\cite{nturgbd} dataset.
We evaluate on both cross subject (X-Sub) and cross view (X-View) benchmarks for NTU-RGB+D.
For fair comparison, in this paper we use~\cite{kinetics400} rather than~\cite{kinetics700}  where 400 classes~\cite{kinetics400} are still used and at the time of writing, representative action recognition models still use~\cite{kinetics400} for pre-training or benchmarking.   
\vspace{-1em}
\label{table:action_recognition_models}
}
\end{table}

Current action recognition architectures can be categorized into two major approaches: 2D-CNN and 3D-CNN. 2D-CNN~\cite{LRCN,feichtenhofer2016convolutional,TSM,DBLP:conf/nips/SimonyanZ14,TSN,TRN} based models utilize image-based 2D-CNN models on a single frame where features are aggregated to predict the action. While some methods (\eg, \cite{LRCN}) use RNN modules for temporal aggregation over visual features, TSN~\cite{TSN} shows that simple average pooling can be an effective method to cope with temporal aggregation. To incorporate temporal information to 2D-CNN, two-stream structure~\cite{feichtenhofer2016convolutional,DBLP:conf/nips/SimonyanZ14} has been proposed to use RGB-frames and optical flow as separate inputs to convolutional networks.
3D-CNN~\cite{i3d,slowfast,stm} takes a more natural approach by incorporating spatio-temporal filters to the image frames. Inspired from~\cite{DBLP:conf/nips/SimonyanZ14}, two-streamed inflated 3D-CNN (I3D)~\cite{i3d} incorporates two-stream structure on 3D-CNN. SlowFast~\cite{slowfast} improves from I3D by showing that the accuracy increases when the 3D kernels are used only in the later layers of the model. A different approach is adopted TPN~\cite{TPN} where a high-level structure is designed adopting a temporal pyramid network which can use either 2D-CNN or 3D-CNN as a backbone. Some models~\cite{ke2017new,kim2017interpretable,stgcn} use alternative information to predict video action. Specifically, ST-GCN~\cite{stgcn} uses a graph convolutional network to predict video action from pose estimation. However, their pose-based models cannot demonstrate better performance than RGB-frame based models. Table~\ref{table:action_recognition_models} tabulates the performance of representative action recognition models on video action datasets, where 2D-skeleton based models~\cite{nturgbd,stgcn} show considerably low accuracy in Kinetics 400~\cite{kinetics400}.
  









\begin{figure}[t]
\begin{center}
    \begin{overpic}[width=\linewidth]{images/hierarchy.png}
     \put (42,29.5) {\scriptsize{HAA500}}
     
     \put (3.2,21.2) {\scriptsize{Daily Actions}}
     \put (24.5,21.2) {\scriptsize{Sports/Athletics}}
     \put (48,21.2) {\scriptsize{Musical Instruments}}
     \put (75.5,21.2) {\scriptsize{Games and Hobbies}}
     
     
     \put (11,13) {...}
     \put (16,12) {\scriptsize{Baseball}}
     \put (30,12) {\scriptsize{Soccer}}
     \put (60.5,12) {\scriptsize{String}}
     \put (74,12) {\scriptsize{Percussion}}
     \put (90,13) {...}
     
     \put (11,3.5) {...}
     \put (16,2.5) {\scriptsize{Header}}
     \put (29.5,2.5) {\scriptsize{Dribble}}
     \put (43,2.5) {\scriptsize{Shoot}}
     \put (55,2.5) {\scriptsize{Play Guitar}}
     \put (72,2.5) {\scriptsize{Play Cello}}
     \put (87,3.5) {...}
    \end{overpic}
\end{center}
    \caption{Example of HAA500 hierarchy.}
    \label{fig:class_structure}
\end{figure}


\section{HAA500}

\subsection{Data Collection}
The annotation of HAA500 consists of two stages: vocabulary collection and video clip selection. 
While the bottom-up approach which annotates action labels on selected long videos was often used in atomic/fine-grained action datasets~\cite{AVA,finegym},
we aim to build a clean and fine-grained dataset for atomic action recognition, thus the video clips are collected based on pre-defined atomic actions following a top-down approach.



\begin{figure*}[t]
\begin{center}
	\minipage[t]{0.49\linewidth}
	    \begin{center}
	    
            \begin{overpic}[width=\linewidth]{images/noise1.PNG}
                 \put (2,0) {\scriptsize{0:0.00}}
                 \put (30,0) {\scriptsize{Dribbling}}
                 \put (68,0) {\scriptsize{0:8.00}}
                 \put (78,0) {\scriptsize{Shooting}}
                 \put (90,0) {\scriptsize{0:10.00}}
            \end{overpic}


        	(a) Kinetics400 - Shooting Basketball
        	
            \begin{overpic}[width=\linewidth]{images/noise2.PNG}
                 \put (2,0)  {\scriptsize{0:0.00}}
                 \put (29,0) {\scriptsize{Long Jump}}
                 \put (68,0) {\scriptsize{0:8.00}}
                 \put (77,0) {\scriptsize{Audience}}
                 \put (90,0) {\scriptsize{0:10.00}}
            \end{overpic}
        	
        	(b) Kinetics400 - Singing
    	\end{center}
	\endminipage\hfill
	\minipage[t]{0.49\linewidth}
	    \begin{center}
            \begin{overpic}[width=\linewidth]{images/noise3.PNG}
                 \put (2,0) {\scriptsize{0:0.00}}
                 \put (44,0) {\scriptsize{Long Jump}}
                 \put (90,0) {\scriptsize{0:3.00}}
            \end{overpic}
        	
        	(c) HACS - Long Jump
        	
            \begin{overpic}[width=\linewidth]{images/noise4.PNG}
                 \put (2,0) {\scriptsize{0:0.00}}
                 \put (90,0) {\scriptsize{0:3.20}}
            \end{overpic}
        	
        	(d) HAA500 - Uneven Bars : Land
    	\end{center}
	\endminipage\hfill
\vspace{0.2em}
\caption{Different types of label noise in action recognition datasets. 
\textbf{(a)}: Kinetics400 has a fixed video length of 10 seconds which cannot accurately annotate quick actions like \text{``Shooting Basketball''} where the irrelevant action of dribbling the ball is included in the clip. 
\textbf{(b)}: A camera cut can be seen, showing unrelated frames (audience) after the main action. 
\textbf{(c)}: By not having a frame-accurate clipping, the clip starts with a person-of-interest in the midair, and quickly disappears after few frames, causing the rest of the video clip to not have any person in action. 
\textbf{(d)}: Our HAA500 accurately annotates the full motion of \text{``Uneven Bars - Land''} without any irrelevant frames. All the videos in the class starts with the exact frame an athlete puts the hand off the bar, to the exact frame when he/she finishes the landing pose. }
\label{fig:comparison_noise}
\end{center}
\vspace{-1em}
\end{figure*}

\subsubsection{Vocabulary Collection}

To make the dataset as clean as possible and useful for recognizing fine-grained atomic actions, we narrowed down the scope of our super-classes into 4 areas; \textit{Sport/Athletics}, \textit{Musical Instruments}, \textit{Games and Hobbies}, and \textit{Daily Actions}, where future extension beyond the existing classes is feasible. 
We select action labels where the variations within a class are typically indistinguishable. For example, instead of \textit{Hand Whistling}, we have \textit{Whistling with One Hand} and \textit{Whistling with Two Hands}, as the variation is large and distinguishable. 
Our vocabulary collection methodology makes the dataset hierarchical where atomic actions may be combined to form a composite action, \eg, \textit{Whistling} or \textit{Soccer}. 

Consequently, HAA500 contains 500 atomic action classes, where 212 are \textit{Sport/Athletics}, 51 are \textit{Musical Instruments}, 82 are \textit{Games and Hobbies} and the rest are \textit{Daily Actions}.
\ifcvprfinal
    Figure~\ref{fig:class_structure} shows the hierarchy of HAA500.
\else
    Figure~\ref{fig:class_structure} shows the hierarchy of HAA500 with the full class list provided in the supplementary materials.
\fi



\begin{table}[t]
\setlength\extrarowheight{3pt}
\begin{center}
\begin{tabular}{|c c c c c|}
\Cline{1-5}
\Thickvrulel{action} & clips & mean length & duration & \Thickvruler{frames} \\ \Cline{1-5}
500 & 10,000 & 2.12s & 21,207s & 591K \\ \hline
\addlinespace
\Cline{0-3}
\Thickvrulel{\# of People} & 1 & 2 & \Thickvruler{2} \\ \Cline{0-3}
             & 8,309 & 859 & \multicolumn{1}{c|}{832} \\ \cline{0-3}
\addlinespace
\Cline{0-2}
\Thickvrulel{Moving Cam.} & O & \Thickvruler{X} \\ \Cline{0-2}
              & 2,373 & \multicolumn{1}{c|}{7,627} \\ \cline{0-2}
\end{tabular}
\end{center}
\caption{Summary of HAA500}
\label{table:dataset_summary}
\end{table}



\subsubsection{Video Clip Selection}
To ensure our dataset is clean and class-balanced, all the video clips are collected from YouTube with the majority having a resolution of at least 720p, and each class of atomic action containing 16 training clips.
We manually select the clips with apparent human-centric actions where the person-of-interest is the only dominant person in the frame at the center with their body clearly visible. 
To increase diversity among the video clips and avoid unwanted bias, all the clips were collected from different YouTube videos, with different environment settings so that the action recognition task cannot be trivially reduced to identifying the corresponding backgrounds. Clips are properly trimmed in a frame-accurate manner to cover the desired actions, while assuring every video clip to have compatible actions within each class (\eg every video in the class \textit{salute} starts on the exact frame where the person is standing still before moving the arm, and the video ends when the hand goes next to the eyebrow). Figure~\ref{fig:teaser2} shows examples of the selected videos.







\subsubsection{Statistics}

Table~\ref{table:dataset_summary} summarizes the HAA500 statistics. HAA500 includes 500 atomic action classes where each class contains 20 clips, with an average length of 2.12 seconds. 
Each clip was annotated with meta-information which contains the following two fields: the number of dominant people in the video and the camera movement.

\vspace{-1em}
\subsubsection{Training/Validation/Test Sets}
Since the clips in different classes are mutually exclusive, all clips appear only in one split. The 10,000 clips are split as 16:1:3, resulting in segments of 8,000 training, 500 validation, and 1,500 test clips.

\begin{figure*}[t]
\begin{center}
\begin{overpic}[width=\linewidth]{images/human_centric.png}
 \put (0.3,-0.1)   {\rotatebox{90}{ {\fontsize{3}{6}\selectfont Kinetics 400}}}
 \put (50.2,1.5)  {\rotatebox{90}{ {\fontsize{4}{6}\selectfont AVA}}}
 \put (0.2, 8)  {\rotatebox{90}{ {\fontsize{4}{6}\selectfont Something}}}
 \put (50.2, 9.5) {\rotatebox{90}{ {\fontsize{4}{6}\selectfont HACS}}}
\end{overpic}
\caption{The video clips in AVA, HACS, and Kinetics 400 contain multiple human figures with different actions in the same frame. Something-Something focuses on the target object and barely shows any human body parts. In contrast, all video clips in HAA500 (in Figure \ref{fig:teaser2}) are carefully curated where each video shows either a single person or the person-of-interest as the most dominant figure in a given frame.}
\label{fig:comparison_human_centric}
\end{center}
\end{figure*}   

\setlength{\tabcolsep}{4pt}
\begin{table}[t]
\begin{center}
\begin{tabular}{|r|c|c|c|}
\hline 
Dataset                     & Clip Length & Irr. Actions & Camera Cuts  \\ \hline
UCF101~\cite{ucf101}        & Varies      &      &  \\ \hline
HMDB51~\cite{HMDB51}        & Varies      &      &   \\ \hline
AVA~\cite{AVA}              & 1 second    &       &   \\ \hline
HACS~\cite{zhao2019hacs}    & 2 second    &       &  \\ \hline
K.400~\cite{kinetics400}    & 10 second   &       &   \\ \hline
M.i.T.~\cite{momentsintime} & 3 second    &      &  \\ \hline
\textbf{HAA500}             & Just Right  &      &  \\ \hline
\end{tabular}
\end{center}
\caption{Clip length and irrelevent frames of video action datasets.}
\label{table:comparison_sampling_rate}

\end{table}
\setlength{\tabcolsep}{1.4pt}

\subsection{Properties and Comparison}

\subsubsection{Clean Labels for Every Frame}


Most video datasets~\cite{AVA,kinetics400,ucf101} show strong label noises, due to the difficulties of collecting clean video action dataset. Some~\cite{kinetics400,HMDB51,ucf101} often focus on the ``Scene" of the video clip, neglecting the human ``action" thus including irrelevant actions or frames with visible camera cuts in the clip. Video action datasets~\cite{AVA,kinetics400,momentsintime,zhao2019hacs} also have a fixed length of video clips, where irrelevant frames are inevitable for shorter or longer actions. Our properly trimmed video collection guarantees clean label for every frame. 

Table~\ref{table:comparison_sampling_rate} tabulates clip lengths and label noises of video action datasets. 
Figure~\ref{fig:comparison_noise} shows examples of label noises. As HAA500 are constructed gearing to accurate temporal annotation, we are almost free from any adverse effect due to these noises.


\subsubsection{Human-Centric}


One of the difficulties in action recognition is that the neural network tends to predict by trivially comparing the background scene in the video, or detecting key elements in a frame (e.g., a basketball to detect \textit{Playing Basketball}) rather than analyzing human gesture, thus causing the action recognition to have no better performance improvements over scene/object recognition. The other difficulty stems from the video action datasets where videos captured in wide field-of-view contain multiple people in a single frame~\cite{AVA,kinetics400,zhao2019hacs}, while videos captured using narrow field-of-view only exhibit very little body part in interaction with an object~\cite{goyal2017something,momentsintime}. 
In~\cite{AVA} attempts were made to overcome this issue through spatial annotation of each individual in a given frame; this introduces another problem of action localization and thus further complicating the difficult recognition task. Figure~\ref{fig:comparison_human_centric} illustrates example frames of different video action datasets. 

HAA500 contributes a curated dataset where each human joint can be clearly detected over any given frame, thus allowing the model to benefit from learning human movements than just performing scene recognition. As tabulated in Table~\ref{table:comparison_joint}, HAA500 have high detectable joints~\cite{alphapose} of 69.7\%, well above other representative action datasets.


\begin{table}
\begin{center}
\begin{tabular}{|c|c|}
\hline 
Dataset                         & Detectable Joints            \\ \hline
Kinetics 400~\cite{kinetics400} &         41.0\%               \\ \hline
UCF101~\cite{ucf101}            &         37.8\%               \\ \hline
HMDB51~\cite{HMDB51}            &         41.8\%               \\ \hline
FineGym~\cite{finegym}          &         44.7\%               \\ \hline
\textbf{HAA500}                 & \textbf{69.7\%}              \\ \hline
\end{tabular}
\end{center}
\caption{Detectable joints of video action datasets. We use AlphaPose~\cite{alphapose} to detect the largest person in the frame, and count the number of joints with a score higher than . }
\label{table:comparison_joint}
\end{table}


\begin{table*}[hbt!]
\begin{center}
\minipage[t]{0.425 \linewidth}
        \begin{tabular}{|c|c|c|c|}
            \hline
            \multicolumn{2}{|c|}{} &
            \multicolumn{2}{|c|}{500 Atomic} \\
            \hline
              \multicolumn{2}{|c|}{Model}   & Top-1 & Top-3  \\
            \hline
            \multirow{4}{*}{I3D~\cite{i3d}} & RGB & 33.53\% & 53.00\%   \\
            & Flow                                & 34.73\% & 52.40\% \\
            & Pose                                & 35.73\% & 54.07\%  \\
            & Three-Stream                        & 49.87\% & 66.60\%  \\
            \hline
            \multirow{4}{*}{SlowFast~\cite{slowfast}} & RGB & 25.07\% & 44.07\%  \\
            & Flow                                          & 22.87\% & 36.93\%  \\
            & Pose                                          & 28.33\% & 45.20\%  \\
            & Three-Stream                                  & 39.93\% & 56.00\%  \\
            \hline
            \multirow{3}{*}{TSN~\cite{TSN}} & RGB & 55.33\% & 75.00\% \\
                                 & Flow & 49.13\% & 66.60\% \\
                                 & Two-Stream & 64.40\% & 80.13\% \\
            \hline
            \multirow{1}{*}{TPN~\cite{TPN}} & RGB & 50.53\% & 68.13\% \\
            \hline
            \multirow{1}{*}{ST-GCN~\cite{stgcn}} & Pose & 29.67\% & 47.13\% \\
            \hline
        \end{tabular}
\endminipage
\minipage[t]{0.245 \linewidth}
        \begin{tabular}{|c|c|}
            \hline
            \multicolumn{1}{|c|}{Inst.} &
            \multicolumn{1}{|c|}{Inst. with Atomic} \\
            \hline
            Top-1 & Top-1  \\
            \hline
             70.59\% & \bf{71.90}\%  \\
             73.20\% & \bf{77.79}\%  \\
             69.28\% & \bf{71.90}\%  \\
             81.70\% &     82.35\%   \\
            \hline
             40.52\% & \bf{50.98}\%  \\
             71.90\% &     71.90 \%  \\
             64.71\% & \bf{66.01}\%  \\
             67.97\% & \bf{73.86}\%  \\
            \hline
             \bf{86.93}\% & 84.31\%  \\
             79.08\% & \bf{86.27}\%  \\
             89.54\% & 90.20\%  \\
            \hline
             73.20\% & \bf{75.82}\%  \\
            \hline
             67.32\% & 67.97\%  \\
             \hline
        \end{tabular}
\endminipage
\minipage[t]{0.24 \linewidth}
        \begin{tabular}{|c|c|}
            \hline
            \multicolumn{1}{|c|}{Sport} &
            \multicolumn{1}{|c|}{Sport with Atomic} \\
            \hline
            Top-1 & Top-1  \\
            \hline
             47.48\% & \bf{53.93}\%  \\
             51.42\% & \bf{54.40}\%  \\
             54.87\% &     55.03 \%  \\
             68.55\% & \bf{69.81}\%   \\
            \hline
             42.92\% & \bf{44.18}\%  \\
             44.81\% & \bf{45.91}\%  \\
             42.45\% & \bf{50.00}\%  \\
             59.91\% & \bf{62.89}\%  \\
            \hline
             72.64\% & 72.48\%  \\
             \bf{69.97}\% & 68.24\%  \\
             \bf{81.13}\% & 78.93\%  \\
            \hline
             61.64\% & \bf{64.15}\%  \\
            \hline
             40.25\% & \bf{43.87}\%  \\
             \hline
        \end{tabular}
\endminipage
\end{center}
\caption{ \textbf{Left} : HAA500 trained over different models. \textbf{Right} : Composite action detection accuracy of different models when they are trained with/without atomic action detection. Numbers are bolded when the difference is larger than 1\%.}
\label{table:Experiments}

\end{table*}

\subsubsection{Atomic}

\begin{figure}[!t]
\begin{center}
\includegraphics[width=\linewidth]{images/atomic.png}
\caption{Coarse-grained atomic action datasets label different actions under a single English action verb. HAA500 (Bottom) has fine-grained classes where the action ambiguities are eliminated as much as possible.}
\label{fig:comparison_atomicity}

\end{center}
\vspace{-1em}
\end{figure}

Existing atomic action datasets such as \cite{ava_speech,AVA,momentsintime} is limited by English linguistics, where action verbs (\eg, walk, throw, pull, etc) are decomposed. Such classification does not fully eliminate the aforementioned problems of composite action dataset. Figure~\ref{fig:comparison_atomicity} shows cases of different atomic action datasets where a single action class contains fundamentally different actions. 

On the other hand, our fine-grained atomic actions contain only a single type of action under each class, \eg \textit{Baseball - Pitch}, \textit{Yoga - Tree}, \textit{Hopscotch - Spin}, etc. 


\section{Empirical Studies}

We study HAA500 and  representative action datasets over multiple aspects using widely used action recognition models. Left of Table~\ref{table:Experiments} shows the performance of the model when they are trained with HAA500. For fair comparison between RGB frame based models with optical flow~\cite{flownet2} or pose~\cite{alphapose} based models, all the experiments have been done without any ImageNet~\cite{imagenet} pre-training. For Pose models except ST-GCN~\cite{stgcn}, we use three-channel pose joint heatmaps~\cite{alphapose} to train pose models. RGB, Flow and Pose all show relatively similar performance in HAA500, without none of them showing superior performance than the other.  Given that the information of pose heatmap is far less than the image information given from RGB frames or optical flow frames, we expect that easily detectable joints of HAA500 is benefiting the pose-based model performance. 

Furthermore, we study the benefits of atomic action annotation on video recognition, as well as  the importance of human-centric characteristics of HAA500.
In this paper, we use I3D-RGB~\cite{i3d} with 32 frames for all of our experiments unless otherwise specified. We use AlphaPose~\cite{alphapose} for the models that require human pose estimation. 


\subsection{Atomic Action}
We have previously discussed that modern action recognition datasets introduce ambiguities where two or more composite actions sharing the same atomic actions, while a single composite action class containing multiple distinguishable actions. HAA500 addresses this issue by providing fine-grained atomic action labels that distinguish similar atomic action in different composite actions. To study the benefits of atomic action labels, specifically, how it helps on composite action detection for ambiguous classes, we selected two areas from HAA500, \textit{Sports/Athletics} and \textit{Musical Instruments}, in which composite actions contain strong ambiguities with other actions in the area. We compare models trained with two different types of labels: 1) only composite labels and 2) atomic + composite labels, then we evaluate the performance on composite action detection. Results are tabulated on the right of Table \ref{table:Experiments}. Accuracy from the models trained with only composite labels are under \textit{Inst.} and \textit{Sport} column, and the accuracy of composite action detection while trained with atomic action detection is listed on the other columns. 

We can observe improvements in composite action detection when atomic action detection is incorporated. The fine-grained action decomposition in HAA enables the models to resolve ambiguities of similar atomic actions and helps the model to learn the subtle differences in the atomic actions across different composite actions. 
This demonstrates the importance of a proper label of fine-grained atomic action which can increase the performance for composite action detection without change of the model architecture or the training set. 




\subsection{Human-Centric}

HAA500 is designed to contain action clips with a high percentage of detectable human-figures.
To study the importance of human-pose in fine-grained atomic action recognition, we compare the performance of HAA500 and FineGym when both RGB and pose estimation are given as input. For pose estimation, we obtain the 17 joint heatmaps from AlphaPose~\cite{alphapose} and merge them into 3 channels; head, upper-body, and lower-body.

\begin{table}[h]
    {\small 
    \begin{center}
        \begin{tabular}{|l |c |c || c |}
        \hline
          & RGB  & Pose & RGB + Pose \\
        \hline
        HAA500                    & 33.53\% & 35.73\% & 42.80\% \\
        \:\:\: Sport              & 38.52\% & 47.33\% & 50.94\% \\ 
        \:\:\: Instrument         & 30.72\% & 24.18\% & 32.03\% \\ 
        \:\:\: Hobbies            & 31.30\% & 26.42\% & 35.37\% \\ 
        \:\:\: Daily              & 28.82\% & 28.60\% & 39.14\% \\
Gym288~\cite{finegym} & 76.11\% & 65.16\% & 77.31\% \\
\hline
        \end{tabular}
    \end{center}}
    \caption{Atomic action detection accuracy when both RGB image and pose estimation are given as an input. We also show performance when they are trained separately for comparison.}
    \label{table:human-pose}
\end{table}

Table~\ref{table:human-pose} tabulates the results. In three out of four areas of HAA500, I3D-RGB shows better performance than I3D-Pose, due to the vast amount of information given to the model. I3D-Pose shows the highest performance on \textit{Sports/Athletics} with vibrant and distinctive action, while I3D-Pose fails to show comparable performance on \textit{Musical Instrument} area, where predicting the atomic action from only 17 joints is quite challenging.  Nonetheless, our experiments show a performance boost when both pose estimation and RGB frame are fed to the atomic action detection model, implicating the importance of human action in HAA500 action detection. For FineGym - Gym288, due to the rapid athletic movements resulting in blurred frames, human pose is not easily recognizable which accounts for the observed low accuracy. 





\section{Observations}
We present notable characteristics observed from HAA500.
\vspace{-1em}

\paragraph{Effects of Fine-Tuning over HAA500}

\begin{table}[t]
{\small 
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
 & \multicolumn{1}{c|}{UCF101~\cite{ucf101}} & \multicolumn{1}{c|}{ActNet 100~\cite{activitynet}} & HMDB51~\cite{HMDB51} \\
\textbf{Pre-trained} & Top-1  & Top-1 & Top-1 \\
\hline
None                            & 58.87\% & 35.68\% & 28.56\% \\ 
AVA~\cite{AVA}                  & 48.54\% & 21.10\% & 25.28\% \\ 
Gym288~\cite{finegym}           & \textbf{69.94\%} & 29.88\% & 36.24\% \\ 
UCF101~\cite{ucf101}            & -       & 33.56\% & 32.37\% \\ 
ActNet 100~\cite{activitynet}   & 58.90\% & -       & 27.71\% \\
HMDB51~\cite{HMDB51}            & 53.36\% & 28.60\% &  -      \\
\hline
HAA500                          & 68.70\% & \textbf{36.53\%} & \textbf{40.45\%} \\ 
\hline
\end{tabular}
\end{center}}
\caption{Fine-tuning Performance on I3D.}
\label{table:transfer}
\vspace{-1em}
\end{table}



Here, we test how to exploit the curated HAA500 dataset to detect action in ``in the wild" action datasets. We pre-train using HAA500 and other datasets on I3D-RGB~\cite{i3d} and freeze all the layers except for the last three for feature extraction. We then fine-tune on the last three layers with ``in the wild" composite action datasets~\cite{activitynet,HMDB51,ucf101}.

Table~\ref{table:transfer} tabulates the fine-tuning result.
Our dataset is carefully curated to have a high variety of backgrounds and people while having consistent actions over each class. While being comparably smaller and ``human-centric" than other action recognition datasets, HAA500's cleanness and high-variety makes it easily transferable to different tasks and datasets. 





\paragraph{Effects of Scale Normalization} 

HAA500 has high diversity in human positions across the video collection. Here, we choose an area of HAA500, \textit{Musical Instruments}, to investigate the effect of human-figure normalization on detection accuracy. We have manually annotated the bounding box of the person-of-interest in each frame and cropped them for the model to focus on the human action. In Table~\ref{table:normalize} we test models that were trained to detect the composite actions or both composite and atomic actions. 

\begin{table}[h]
    {\small 
    \begin{center}
        \begin{tabular}{| l |c |c | c | c | }
        \hline
          & \multicolumn{2}{|c|}{Original} & \multicolumn{2}{|c|}{Normalized} \\
          \hline
          & Composite  & Both & Composite  & Both  \\
        \hline
        I3D-RGB  & 66.01\%  & 56.86\%          & \textbf{75.82\%} & \textbf{77.12\%} \\ 
        I3D-Flow & 73.20\%  & \textbf{77.78\%} & \textbf{75.16\%} & 74.51\% \\ 
        2-Stream & 77.78\%  & 80.39\%          & \textbf{83.01\%} & 80.39\% \\ 
        \hline
\end{tabular}
    \end{center}}
    \caption{Accuracy improvements on person-of-interest normalization. Numbers indicate the composite action detection accuracy.}
    \label{table:normalize}
\end{table}

While HAA500 is highly human-centric with person-of-interest as the most dominant figure of the frame, action detections on the normalized frames still show considerable improvements when trained on either atomic action annotations or composite action annotations. This indicates that spatial annotation is an important information in video action recognition.

\paragraph{Effects of Object Detection} 
In most video action dataset, non-human objects exist as a strong bias to the classes (\eg basketball in \textit{Playing Basketball}). 
When the action dataset annotates highly diverse actions (\eg \textit{Shooting a Basketball}, \textit{Dribbling a Basketball}, etc.) under a single class, general deep-learning models tend to suffer from the bias and will learn to detect the easiest common factor (Basketball) among the video clips, rather than ``seeing" the human action. Poorly designed video action dataset encourages the action detection model to trivially become an object detection model. 

In HAA500, every video clip in the same class contains compatible actions, making the common factor to be the ``action", while objects are ambiguity that spreads among different classes (\eg basketball exists in both \textit{Shooting a Basketball} and \textit{Dribbling a Basketball}). To test the influence of ``object" in HAA500, we design an experiment similar to investigating the effect of human poses, as presented in Table~\ref{table:human-pose}, while we use object detection heatmap instead. Here we use Fast RCNN~\cite{fastrcnn} trained with COCO~\cite{coco} dataset to generate the object heatmap. Among 80 detectable objects in COCO, we select 5 types of objects (sports equipments, food, animals, cutleries, and vehicles) to draw 5-channel heatmap. Similar to Table~\ref{table:human-pose}, the heatmap channel is appended to the RGB channel as an input. 

\begin{table}[h]
    {\small 
    \begin{center}
        \begin{tabular}{| l |c |c | }
        \hline
             & RGB & + Object\\
         \hline
            HAA500            & 33.53\% & 33.73\%  \\
            \:\:  Sports      & 38.52\% & 38.68\%  \\
            \:\:  Instruments & 30.72\% & 30.07\%  \\
            \:\:  HAA-COCO    & 34.26\% & 34.26\%  \\
        \hline
            UCF101            & 57.65\% & 60.19\%   \\
        \hline
        \end{tabular}
    \end{center}}
    \caption{Accuracy of I3D when it is trained with object heatmap. HAA-COCO denotes 147 classes of HAA500 expected to have objects that were detected in the experiment.}
    \label{table:object_detection}
\end{table}

Table~\ref{table:object_detection} tabulates the negligible effect of object detection in atomic action detection of HAA500, including the classes that are expected to use the selected objects (HAA-COCO), while UCF101 shows improvements when object heatmap is used as a visual cue. 
Given the negligible effect of object heatmaps, 
we believe that fine-grained annotation of actions can effectively eliminates unwanted bias (``objects") from affecting the prediction, while in UCF101 (composite action dataset), ``objects" can still affect the prediction. 

\paragraph{Effects of Dense Temporal Sampling}

The top of Table~\ref{table:action_oriented} tabulates the performance difference of HAA500 and other datasets over the number of frames that are used during training and testing. The bottom of Table~\ref{table:action_oriented} tabulates the performance with varying strides with a window size of 32 frames, except AVA which we test with 16 frames. Top-1 accuracies on action recognition are shown except AVA which shows mIOU due to its multi-labeled nature of the dataset. 

As expected, most datasets show the best performance when 32 frames are fed. AVA shows a drop in performance due to the irrelevant frames (\eg, action changes, camera cuts, etc.) included in the wider window. While all the datasets show comparable accuracy when the model only uses a single frame (\ie when the problem has been reduced to ``Scene Recognition" problem), both HAA500 and Gym288 show a significant drop compared to their accuracy in 32 frames. While having an identical background contributes to the performance difference for Gym288, from HAA500, we see how temporal action movements are crucial for the detection of atomic actions, and they cannot be trivially detected using a simple scene detecting model. 

We also see that the density of the temporal window is another important factor in atomic action detection. We see that in both HAA500 and Gym288, which are fine-grained action datasets, show larger performance drops when the frames have been sampled with strides of 2 or more, reflecting the importance of short temporal action movements in fine-grained action detection.








\begin{table}[t]
{\small 
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
 \# of frames & HAA500 & UCF101~\cite{ucf101} & AVA~\cite{AVA} & Gym288~\cite{finegym} \\
\hline
1  & 19.93\% & 45.57\% & 33.57\% & 39.77\%\\
2  & 23.27\% & 47.26\% & 39.42\% & 44.68\%\\
4  & 24.40\% & 49.30\% & 39.48\% & 51.22\%\\
8  & 24.07\% & 49.80\% & 42.38\% & 59.64\%\\
16 & 28.20\% & 52.31\% & 43.11\% & 69.25\% \\
32 & 33.53\% & 57.65\% & 29.88\% & 76.11\% \\
\hline
stride 2 & 27.47\% & 57.23\%  & 41.49\% & 68.68\%\\
stride 4 & 23.87\% & 52.29\%  & 40.52\% & 60.76\%\\
stride 8 & 18.47\% & 47.95\%  & 38.45\% & 39.31\%\\
\hline
\end{tabular}
\end{center}}
\caption{Performance comparison on I3D-RGB over the number of frames and strides, where in the latter a window size of 32 frames is used except AVA which we test with 16 frames.}
\label{table:action_oriented}
\end{table}

\section{Conclusion}

This paper introduces HAA500, a new human action dataset with fine-grained atomic action labels and human-centric clip annotations, where the videos are carefully selected such that the relevant human poses are apparent and detectable. With carefully curated action videos, HAA500 does not suffer from irrelevant frames, where videos clips only exhibit the annotated action. With a small number of clips per class, HAA500 is highly scalable to include more action classes. We have demonstrated the efficacy of HAA500 where action recognition can be greatly benefited from our clean, highly diversified, class-balanced fine-grained atomic action dataset which is human-centric with high percentage of detectable pose. On top of HAA500, we have also empirically investigated several important factors that can affect the performance of action recognition. We hope HAA500 and our findings could facilitate new advances in the field of action recognition.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}
\end{document}
