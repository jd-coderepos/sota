

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  



\IEEEoverridecommandlockouts                              

\overrideIEEEmargins                                      





\usepackage{graphics} \usepackage{graphicx}
\usepackage{amsmath} \usepackage{amssymb}  \usepackage{color}
\title{\LARGE \bf
Self-supervised Learning for Single View Depth and Surface Normal Estimation}

\author{Huangying Zhan, Chamara Saroj Weerasekera, Ravi Garg, Ian Reid
\thanks{All authors are with the School of Computer Science, at the University
of Adelaide, and Australian Centre for Robotic Vision}}

\begin{document}
\maketitle
\thispagestyle{empty}
\pagestyle{empty}

\newcommand{\Ravi}[1]{\textcolor{red}{#1}}
\newcommand{\HY}[1]{\textcolor{blue}{#1}}
\newcommand{\Saroj}[1]{\textcolor{green}{#1}}
\newcommand{\etal}{\textit{et al}.}
\begin{abstract}
In this work we present a self-supervised learning framework to simultaneously train two Convolutional Neural Networks (CNNs) to predict depth and surface normals from a single image. In contrast to most existing frameworks which represent outdoor scenes as fronto-parallel planes at piece-wise smooth depth, we propose to predict depth with surface orientation while assuming that natural scenes have piece-wise smooth normals. We show that a simple depth-normal consistency as a soft-constraint on the predictions is sufficient and effective for training both these networks simultaneously. The trained normal network provides state-of-the-art predictions while the depth network, relying on much realistic smooth normal assumption, outperforms the traditional self-supervised depth prediction network by a large margin on the KITTI benchmark.
\end{abstract}





\section{Introduction} \label{Sec:intro}
Recovering 3D scene structure from image data is long-studied problem in the robotics and computer vision communities. Decades of research has been focused on recovering the 3D structure of the scene from multiple 2D images as a geometric inverse problem, but the recent surge in deep learning has produced promising solutions for recovering scene geometry even from a single RGB image. Supervised deep neural networks trained on large indoor RGBD data-sets like NYUD \cite{Silberman2012nyuv2} and SUN-RGBD \cite{Song_2015_CVPR} can currently produce high fidelity depth and surface normal predictions from single RGB image. 

While the availability of cheap range sensors like Kinect makes large-scale supervised training of single-view depth and normal predictions possible for indoor scenes, the absence of high quality, dense depth sensing outdoors makes supervised learning difficult. To address this problem, Garg \etal~\cite{garg2016depth} introduced a self-supervised learning paradigm where large stereo data-set can be used to train the single view depth prediction networks by using the traditional dense stereo loss function of \cite{horn1981determining} as the supervisory signal. Various extensions of the self-supervised learning for depth estimation have been proposed in recent times, including the use of robust image alignment costs, Godard \etal~\cite{godard2016depth}. \cite{zhou2017sfmlearner} extended the framework to enable training depth predictors using monocular sequence whereas \cite{li2017undeepvo, zhan2018depthVO} advocate using stereo sequences to train a network for unsupervised two-frame visual odometry, because this addresses the issue of metric scale (which is otherwise unobservable). \cite{zhan2018depthVO} additionally propose to learn good features to match and \cite{aleottigenerative} advocate to evaluate the quality of warp using Generative Adversarial Networks \cite{goodfellow2014generative}. 

Most of these self-supervised frameworks minimise a loss based on image data even though they are predicting a depth map.  This means that they must regularise the predicted depth maps during training in regions where there is no strong photometric information; this is usually done by encouraging the predicted depth maps to be piece-wise smooth, or constant with depth discontinuities aligning image edges. These assumptions are rarely realistic and lead to fronto-parallel planar artifacts in the estimated structures in homogeneous regions.

In this work, we propose to address these issues by jointly learning two convolutional neural networks predicting depths and surface normals from a single input image in a self-supervised framework. Estimating surface normals in conjunction with depth-maps allows for a richer geometric reasoning where we can relax the piece-wise smooth/constant depth-map assumption to allow for smooth or planer surfaces in the scene. Similar to \cite{li2017undeepvo,zhan2018depthVO}, in addition to surface prediction networks, we learn a two-frame relative camera pose estimation network to predict visual odometry which allows us to use both stereo and temporal information available in KITTI dataset for accurate depth prediction. Our training schema is described in detail in Section \ref{sec:depth_normal} and the test-time setup is depicted in Fig. \ref{fig:testOverview}.

\begin{figure}[!t] 
\centering
\includegraphics[width=1\columnwidth]{OverviewFigure.pdf}
\caption{Our test-time setup where depths and surface normals are predicted from a single image, and ego-motion is predicted from two views. At train-time, all three networks are trained in a self-supervised manner from stereo image sequence data. \vspace{-5mm}}
\label{fig:testOverview}
\end{figure}

Very closely related to our work are the recently published works of \cite{yang2017depthnormal} and \cite{yang2018lego}. In these works, the authors also advocate replacing the piece-wise smooth depth assumption with that of piece-wise smooth normals, however our proposed framework is different in the following aspects:
\begin{itemize}
    \item Most importantly, unlike our proposed method, \cite{yang2017depthnormal,yang2018lego} do not explicitly learn to predict surface normals from a single image. Instead, these methods estimate the normals from the predicted depths and propose to regularize them to iteratively refine the depth predictions. This amounts to imposing a hard constraint on the normals to be the function of depth, limiting the normal accuracy, since the normals computed from depth are bound to have severe depth discretization artifacts and are very noisy. We show in our experiments that the combination of a dedicated network for normals and a soft constraint between inverse depths and normals leads to better predictions.
    \item Both \cite{yang2017depthnormal,yang2018lego} propose to regularize second order depth discontinuity along with the normal discontinuity which is redundant. We show that no additional prior on depth is required for learning state-of-the-art normals.
    \item Both \cite{yang2017depthnormal,yang2018lego} use monocular setup for training whereas we use stereo information to produce metric visual odometry given a frame pair addressing depth-translation scale ambiguity.
    \item We introduce a depth and normal consistency term over time and penalize the inconsistent depth and normal predictions for two consecutive frames of the video sequences during training. This leads to improvement in accuracy of the estimations.
\end{itemize}

In summary, the proposed method to the best of our knowledge, is the first self-supervised framework to jointly train for single view depth, single view normal, and two frame visual odometry prediction without depth/translation scale ambiguity.
Our system produces state-of-the-art surface normal estimation and significantly better depth prediction compared to the corresponding depth smoothness based self-supervised framework on KITTI benchmark dataset.













\section{Related Work} \label{Sec:rel_work}

 Depth estimation from a monocular color image is a long-standing problem in computer vision. Early work mainly aimed at estimating surface normals from a single image and in turn integrated them to form depth maps \cite{NormalIntegrationSurvey}. Normals were estimated using shape from shading \cite{Zhang99shapefrom}, shape-from-defocus \cite{suwajanakorn2015depth} and other low-level image features \cite{hoiem2005automatic,hoiem2007recovering} or perspective geometry based reasoning using vanishing points and lines \cite{criminisi1999single}. In contrast, learning based approaches such as \cite{2014discriminativelyladicky, fouhey2013data}, especially those using neural nets \cite{eigen2015predicting, Wang15, bansal2016marr, bansal2016pixelnet} in recent times have achieved state-of-the art performance in depth/surface normal prediction. In the following section we give a brief overview of both supervised and unsupervised methods for learning to predict geometry from single image, the latter of which is more relavent to this work. 
 
\textbf{Supervised learning of single-view geometry}
Supervised learning based methods for depth prediction rely upon availability of sensory depth data, such as that acquired by the Kinect sensor and LIDAR sensor (for indoor and outdoor scenes respectively). One of the early learning based methods to achieve reasonable success in single image depth estimation is Make3D \cite{saxena2006learning,saxena2009make3d} which relied upon a set of handcrafted features to map patches from the input image to some feature space, and in turn learn to regress from that feature space to depth values. They also use an additional pairwise depth smoothness prior, modeled as either a Gaussian or Laplacian distribution with a data dependent learned variance, in order to regularize and globally optimize for a depth map, with the belief that the process of recovering a depth map requires global reasoning on the image. 

Recently, deep learning-based methods dominate this area \cite{eigen2014depth, xie2016deep3d,kendall2017uncertainties}. For example, \cite{eigen2014depth} train a multi-scale Convolutional Neural Network, operating at coarse and fine image resolutions, to regress a depth map from a single image, and in \cite{eigen2015predicting} they extend their network to a three-scale architecture and regress for depth maps, normal maps, and semantic labels in real-time from a single image. The semantic label maps were predicted from a single RGB-D image as the additional depth channel improved results. In \cite{dharmasiri2017joint} the latter work was extended to jointly predict depth, surface normals and surface curvature, which improved the results of all three tasks.

Liu \etal~\cite{liu2016learning} proposed to formulate depth estimation as a deep continuous Conditional Random Fields (CRF) learning problem. Given the continuous nature of the depth values, they learn the unary depth values and weightings for the pairwise smoothness potential functions via CNNs in an end-to-end framework. \cite{laina2016deeper} used a fully convolutional network architecture based on ResNet \cite{he2016deep} with a novel upsampler for decoding the depth map at input resolution. Kendall \etal~\cite{kendall2017uncertainties} adapted the DenseNet architecture for several regression tasks including depth prediction, and showed that jointly predicting pixelwise depths and confidences, where the output is modeled as a multivariate Gaussian distribution, improves depth estimation results. \cite{roy2016monocular} combined shallow convolutional networks with regression forests to reduce the need for large training sets. Recently in \cite{hu2018revisitingsingleview}, it was proposed that sharper predictions at depth boundaries can be achieved by emphasizing local depth error gradients. This same phenomenon was observed in \cite{eigen2015predicting,ummenhofer2017demon} that also emphasized local depth errors during training. Our inverse-depth normal consistency terms also emphasizes local depth errors based on the predicted normals and achieves a similar effect but in a more implicit unsupervised fashion (i.e. with no dedicated sensory depth data).
 






 \textbf{Self-supervised learning of single-view geometry}
Recent work have started to incorporate multi-view geometry based loss functions for depth regression resulting in a self-supervised learning framework for inferring depth from single image. This stream of work aims to replace the more explicit sensory-data based ground truth supervision with a good image alignment loss between different views observing the same scene (by using stereo data or monocular videos for supervision). The ubiquity and relatively low price of RGB cameras and availability of large datasets of recorded videos increase the attractiveness of this approach. 


Using stereo pairs for training,  \cite{garg2016depth}\cite{godard2016depth} deploy an auto-encoder like framework where the authors propose to predict the disparity (inverse depth) of the left image, using which the right image of the stereo pair can be warped to synthesize the left image. The photometric difference between the input (left) image and the warped image is minimized to train the single view depth predictor. An inverse depth smoothness prior on the predicted depths is used to regularize the solution, encouraging piece-wise smooth depth maps.
\cite{zhou2017sfmlearner} extended the above framework to jointly estimate depth and ego-motion using monocular videos - upto a scale. Methods like \cite{li2017undeepvo}\cite{zhan2018depthVO} proposed to combine the advantages of using both spatial and temporal information available in KITTI sequences for improving depth predictions while solving the scaling ambiguity issue. A large body of work since have been targeted to use better loss functions, in particular the image alignment loss \cite{godard2016depth,aleottigenerative} propose to use SSIM and GANs respectively for image matching. Enforcing temporal consistency in the predicted depths by aligning the back-projected depth maps via differentiable approximation of ICP has been studied in \cite{mahjourian2018unsupervised}.





While most of the self supervised approaches have mainly focused on getting accurate depth-maps, little attention has been devoted to use other scene representations.  
We are aware of two recent  works \cite{yang2017depthnormal,yang2018lego} which incorporate the surface orientation (normal) estimation for single view geometric understanding. 
Similar to \cite{zhou2017sfmlearner}, \cite{yang2017depthnormal}\cite{yang2018lego} learn depth from monocular sequences using a self-supervised photometric loss but additionally  they compute surface normals from the predicted depths using a weighted mean cross product \cite{jia2006crossproduct}. They propose to regularize the inverse depths and the normals computed from the depth predictions simultaneously. We believe that this is redundant and a separate normal prediction is beneficial then relying on the normals to be computed from predicted depth. The differences between our work and \cite{yang2017depthnormal,yang2018lego} is detailed in section \ref{Sec:intro} and a extensive comparison of the proposed work with these methods is described in section \ref{sec:exp_eval}.





\section{Framework for Joint Learning of Depths and Surface Normals} \label{sec:depth_normal}
In this section, we present our system which consists of three CNNs. One each for per-pixel single-view depth prediction, for single view surface normal prediction and a pose-net which takes two images -- consecutive images of a monocular video -- as input to predict the camera motion (vehicle's egomotion in KITTI) between these two frames in metric units. Our system is trained in a self-supervised manner, which means no ground truth data (depths or surface normals) is required for training. Instead we use stereo sequences for training for depths and surface normals from a single image where two consecutive stereo-pairs  form a single training instance. 

The goal is to predict the depth map  and surface normal map  of  (the left image at time ) which we define as the reference image  for a particular training instance. At the same time we also want to predict  which is the relative pose (ego-motion) between the left/right image at time  and the left/right image at time .



Our proposed loss function to train these three networks jointly consists of six terms:

where the 's are the relative weights for losses used for training.
 denotes the photometric alignment cost involving the scene's depth observed by the left camera at time  and  with the estimated egomotion,  enforces the estimated depths and normals to be consistent,  enforces the predicted normals to face the camera and  is a smoothness prior which favors the predicted normals to be piece-wise smooth. Additionally, assuming the scene is rigid,  two temporal geometric consistency terms  and  enforce the estimated depths and normals at the two time instances to be consistent given the egomotion. Each of these terms are elaborated in the following sections. 



\subsection{Enforcing Multi-View Photometric Consistency} \label{sec:learn_depth}
We enforce multi-view photometric consistency (both spatially and temporally) by minimizing the photometric cost that aligns image  and  (the left and right stereo images at time ) as well as the photometric cost that aligns image  and  (the left images at time  and ).



It follows that the photometric loss based on dense image alignment can be written as follows,

where,  and  are the synthesized left images reconstructed from  and  respectively.  is a differentiable bilinear interpolation function \cite{jaderberg2015stn} for indexing into a particular non-integer location in a given input image.
The corresponding pixel  in  for a pixel  in  is defined by the camera intrinsics, ; the \emph{known} stereo baseline, ; and the depth map,  as follows,

where  represents an integer pixel location in  (the reference image) in homogeneous coordinates. Similarly, the corresponding pixel  in  for a pixel in  in  is given by, 

The relative camera transformation, , is represented by 6 parameters in , and consists of rotation and translation components.

 Note that the depth CNN predicts inverse depth  which is better constrained, e.g. sky in infinite depth is zero is inverse depth. The depth is converted from our predicted inverse depth as follows: . 



\begin{figure*}[t!] 
\centering
\includegraphics[width=2\columnwidth]{depthNormalEg_2.pdf}
\caption{Qualitative comparison of depths and surface normals between different methods. 
The ground truth (GT) depths are inpainted from sparse LIDAR ground truth depths. The ground truth surface normals are computed from the inpainted ground truth depths, and are not reliable for all the points (especially the upper part of the images where the LIDAR depths are missing).\vspace{-5mm}}
\label{fig:depthNormalEg}
\end{figure*}

\subsection{Normal Smoothness Prior}
 Relying on photometric loss described in the previous section for learning depth suffers from well known aperture problem. A single pixel from a uniformly colored area in a  image can be matched to many pixels with similar color intensity in the next view making the depth estimation ambiguous. 
 To counter the problem, previous works \cite{garg2016depth} \cite{godard2016depth} \cite{zhan2018depthVO} adopt an inverse depth/disparity smoothness as a prior. 
 However, as explained in the introduction such disparity smoothness assumption is not very realistic and strong regularization of disparity discontinuity leads to fronto-parallel artifacts in the predictions.
 
 In this work we rely on smooth normal assumption whereby we apply edge-aware regularization to the discontinuities in the predicted surface normal by minimizing:
 
where  and  are gradients in horizontal and vertical direction respectively. 
Similar to previous works \cite{godard2016depth,zhan2018depthVO}, we assume that the image edges are very good indicator of scene discontinuity, however we use them to guide the normal smoothness. Eqn. \eqref{eq:normal_smooth} allows for normal discontinuities at the areas where strong image gradient is present while penalizing the normal discontinuities in the homogeneous regions of the image.

 
 
\subsection{Depth - Normal Consistency} \label{sec:invDepth_normal}
For enforcing consistency of predicted depth with predicted surface normals, we use the inverse-depth-normal consistency term proposed in \cite{weerasekera2017normals} as a soft constraint in the form of a loss for training our networks. This loss is based on the geometric relationship between the predicted normal  of the scene corresponding to point  in the reference image, its predicted depth  and the predicted depth  of 's neighbour . The depth-normal consistency term can be written as:

where  is the dot product operator and , . Note that  is normalized to have unit magnitude and is expressed in Cartesian coordinates.
Eqn. (\ref{eqn:dot_points_normal}) can be simplified as:

By dividing the above equation by  we get:


Finally, we minimize the following energy , penalizing inconsistency between predicted inverse depths and normals:





In our experiments, the neighbourhood  comprises just the pixel itself and its two neighbours immediately below and to the right. The image-edge based weight  reduces regularization at image edges, under the assumption that these regions align with depth discontinuities.  and  are tunable parameters, which we use .

It is easy to note that, in the special case when ,
i.e. the normal  is pointed directly at the camera, Eqn. (\ref{eqn:energy_normal}) reduces to the traditionally used inverse depth smoothness prior in unsupervised learning methods such as \cite{garg2016depth}\cite{godard2016depth}\cite{zhan2018depthVO}.

\subsection{Fixing Surface Normal Direction Ambiguity}\label{sec:normal_dir}
It is to be noted that the surface normal prediction network in our framework only rely upon the depth-normal consistency loss . However, normals estimated from the depth maps using Eqn. \eqref{eqn:dot_points_normal} have directional ambiguity. i.e. given the depth map the computed surface normal can face the camera or be in the opposite direction. To fix this ambiguity, we  first  compute  an  approximated  surface  normal  from the predicted depth using mean cross product\footnote{It should be noted that this approximation was used in \cite{yang2017depthnormal} to compute normals from depth maps.} and then penalize the deviation of the predicted normals  from these approximated normals my minimizing:









\begin{figure}[t!] 
\centering
\includegraphics[width=\columnwidth]{ComputedVsPredictedNormals.pdf}
\caption{Qualitative comparison between surface normals computed from CNN depths (Stereo+Normal+Temporal) and surface normals predicted from the Normal CNN, showing the importance of having a dedicated Normal CNN. Left: Groundtruth (GT); Middle: Computed normals from predicted depths; Right: Predicted normals.
\vspace{-5mm}
}
\label{fig:normalComparison}
\end{figure}

\subsection{Enforcing Temporal Consistency of Predicted Geometry} \label{sec:geo_consistency}
Finally, while the accumulated loss terms defined in sections \ref{sec:learn_depth}-\ref{sec:normal_dir} makes the simultaneous learning of depth, normal and egomotion well posed, we enforce temporal consistency in our predictions to improve the accuracy of our predictions. Using the rigid scene assumption as the cameras move in space over time we want the predicted depths and normals at time  to be consistent with the respective predictions at time . This is done by correctly transforming the scene geometry (inverse depth and normal maps) from frame  to frame  much like the image warping over time as described in section \ref{sec:learn_depth}. In particular, we define two consistency-terms  and :

where, 


 and  in the above two equations are the transformed depths and normals from frame  to frame  (based on the predicted pose  where  is the rotation component) for use in  and .

To further clarify, it is important to bring the depth/normals which are estimated in the camera reference frame at any given time step into the current reference frame before applying consistency of depth/normal over time. For depth consistency we transform the estimated 3D points (back-projection of the depth map) at time  to the camera reference frame of time  using the estimated pose  before warping. Similarly, the normals need to transformed from one reference frame to the other using the estimated rotation .







\subsection{Network Architecture}
For both the depth CNN and normal CNN, we use the same architecture except for the last prediction layer, which has a 1-channel output for the depth CNN while a 3-channel output for the normal CNN. Following previous works \cite{garg2016depth}\cite{godard2016depth}\cite{zhan2018depthVO}, we use a fully convolutional neural network with skip-connections. 
The network consists of an encoder and a decoder. For the encoder, we use the ResNet50 \cite{he2016resnet} variant (ResNet50-1by2), which has much less learnable parameters and also faster computation speed. 
For the decoder network, we have studied two architectures, including the simple bilinear upsampler used in \cite{garg2016depth}\cite{zhan2018depthVO} and bilinear upsampling with convolutions, i.e. upsample coarser feature maps, concatenate with features in ResNet50-1by2 (skip connection), and apply convolution (including batch normalization, and ReLU activation). Our experiments show that although both upsamplers have similar quantitative performance for depth estimation as observed in \cite{zhan2018depthVO}, the latter architecture is able to produce sharper predictions, especially useful for predicting surface normals.
We use sigmoid activation at the end of the depth and normal network. We additionally apply a L2-normalization on the surface normals to get unit normals.
For the pose network, we use the one proposed in \cite{zhan2018depthVO}.


\section{Experiments and Evaluations} \label{sec:exp_eval}
In this section we describe the details of our experimental evaluation of our proposed framework. We evaluate our framework on KITTI dataset \cite{Geiger2012kitti}\cite{Geiger2013kitti} and compare with prior art on both depth esimtation and surface normal estimation.
Moreover, we present an ablation study to show the contribution of each component in our framework.

\subsection{Implementation Details} \label{sec:implementation}
We train our CNNs with the Caffe \cite{jia2014caffe} framework. Adam optimizer \cite{kingma2014adam} is used with the following settings, . The initial learning rate is 0.001 and is decreased to one tenth manually when the training loss converges. 
We found the loss weightings for Eqn. \eqref{eqn:final_loss} via grid search and by referring to previous loss weightings adopted by \cite{weerasekera2017normals,zhan2018depthVO}, and are as follows: .

\subsection{Depth Evaluation} \label{sec:depth_eval}
\begin{table*} [!t] 
\begin{center}
\resizebox{2\columnwidth}{!}{\begin{tabular}{| l c  || c c c c | c c c|}
\hline
Method & Supervision & 
\multicolumn{4}{c|}{Error metric} &
\multicolumn{3}{c|}{Accuracy metric} \\
 & & Abs Rel & SqRel & RMSE & RMSE log &
 &  & 
\\
\hline\hline


Train set mean  & Depth & 
0.361 & 4.826 & 8.102 & 0.377 &
0.638 & 0.804 & 0.894\\





Zhou \textit{et al.} \cite{zhou2017sfmlearner}   & Mono. & 
0.208 & 1.768 & 6.856 & 0.283 &
0.678 & 0.885 & 0.957\\

Yang \textit{et al.}\cite{yang2017depthnormal} &
Mono. &
0.182 & 1.481 & 6.501 & 0.267 &
- & - & -\\

Yang \textit{et al.}\cite{yang2018lego} &
Mono. &
0.162 & 1.352 & 6.276 & 0.252 &
- & - & -\\

Garg \textit{et al.} \cite{garg2016depth}   & Stereo & 
0.152 & 1.226 & 5.849 & 0.246 & 
0.784 & 0.921 & 0.967  \\

Godard \textit{et al.} \cite{godard2016depth}  & Stereo & 
0.148 & 1.344 & 5.927 & 0.247 &
0.803 & 0.922 & 0.964\\

\hline

Stereo+Inv.Depth.Smoothness (Baseline) & 
Stereo &
0.150 & 1.409 & 5.800 & 0.249 &
0.800 & 0.923 & 0.964 \\ 

     
Stereo+Normal & 
 Stereo & 
0.135 & 1.194 & 5.718 & 0.237 &
0.816 & 0.929 & 0.968\\     

\textbf{Stereo+Normal+Temporal} & 
 Stereo & 
0.133 & 1.083 & 5.580 & 0.229 &
0.816 & 0.932 & 0.971\\















\hline
\end{tabular}
}
\end{center}
\caption{
Self-supervised depth estimation performance comparison on KITTI dataset (Train\&Test on Eigen Split, 80m cap). The results are evaluated on cropped region used in \cite{godard2016depth}. Our ablation study is presented on the bottom of the table.\vspace{-8mm}}
\label{table:depth_benchmark}
\end{table*}

We evaluate our depth estimation result on the Eigen split provided by \cite{eigen2014depth} for fair comparison with prior works. We follow the same depth evaluation protocol as in \cite{garg2016depth,godard2016depth}. 
Our results for depth estimation using our full framework (Stereo+Normal+Temporal) are presented in Table \ref{table:depth_benchmark} and is compared against relevant prior works and our own `Baseline' network that are all trained using the photometric loss with inverse depth smoothness prior. Our full method performs best in all measures compared to all the baselines. This reaffirms the importance of the more realistic normal smoothness prior over the traditional inverse-depth smoothness prior.

We also compare against our method without temporal photometric and geometric (depth and normal) consistency (Stereo+Normal) which leads to less accurate results as seen in Table \ref{table:depth_benchmark}, signifying the importance of temporal geometric consistency especially for reconstructing far points in the scene.



In Fig. \ref{fig:depthNormalEg} it is apparent that the depth maps predicted using our proposed framework are superior, particularly to reconstruct the road (Rows 1-3) and building in Row 3 when compared with the Baseline approach, and other prior works. 
While the results of \cite{yang2018lego,yang2017depthnormal} are blurry our proposed methods is able to retain sharp edges with correct reconstruction of non-fronto parallel planes.

\subsection{Surface Normal Evaluation} \label{sec:normal_eval}
\begin{table} [!b]
\begin{center}
\vspace{-3mm}
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{| l || c c | c c c|}
            \hline
            Method & 
            \multicolumn{2}{c|}{Error metric} &
            \multicolumn{3}{c|}{Accuracy metric} \\
             &  Mean & Median & 
             &  & 
            \\
            \hline\hline
            
            Yang \textit{et al.}\cite{yang2017depthnormal} & 
            37.44 & 24.32 & 
            0.275 & 0.477 & 0.560\\
            
            Yang \textit{et al.} \cite{yang2018lego} & 
            35.69 & 22.33 & 
            0.293 & 0.502 & 0.585\\
            
            
            \hline \hline
            Baseline (Computed) & 
            36.03 & 24.00 & 
            0.283 & 0.481 & 0.565\\
            \hline           
            Stereo+Normal (Computed) &
            33.43 & 21.15 &
            0.305 & 0.519 & 0.607\\
            
            Stereo+Normal+Temporal (Computed)&
            32.01 & 20.17 &
            0.319 & 0.534 & 0.622\\
            
            \hline 
            Stereo+Normal (CNN) &
            30.37 & 19.13 &
            0.335 & 0.551 & 0.640 \\
            \textbf{Stereo+Normal+Temporal (CNN)} &
            30.23 & 19.11 &
            0.336 & 0.551 & 0.638 \\

            \hline
        \end{tabular}
    }
\end{center}
\caption{Surface Normal evaluated on KITTI Split (108/200 samples, excluding 92 samples in Eigen Split). We evaluated on centre cropped region as depth evaluation in \cite{godard2016depth}. 
\vspace{-5mm}
}
\label{table:normal_benchmark}
\end{table}


There is no surface normal ground truth available in KITTI dataset. 
In particular, the depth ground truth in Eigen split provided by KITTI raw dataset is very sparse and unsuitable to generate surface normal. 
To have better evaluation on surface normal prediction, \cite{yang2017depthnormal}\cite{yang2018lego} use KITTI's official stereo split which contains 200 high quality disparity images from which reasonably high quality surface normals can be generated for evaluation. 
Following \cite{yang2017depthnormal}\cite{yang2018lego}, we use the KITTI split for surface normal evaluation and inpaint the depth ground truth following the approach used in \cite{Silberman2012nyuv2}. We use the mean cross product to generate surface normal ground truth from these inpainted depths.
As, 92 out of 200 images in KITTI split are used in the training set of Eigen split, we only use the remaining 108 images for evaluation.
Moreover, we follow the depth evaluation protocol \cite{godard2016depth} that only use the centre part of the prediction and evaluate normals only at the pixels where the ground truth depths exist to reduce the normal errors introduces due to inpainting.

The quantitative evaluation of unsupervised normal prediction frameworks is presented in Table \ref{table:normal_benchmark}, where we compare against surface normals estimated via different methods. The bottom-most row (Stereo+Normal+Temporal (CNN)) shows our best result. This is the normal predicted by the Normal CNN using our full loss function.
We show that using inverse depth-normal consistency (Stereo+Normal) gives better surface normals than the inverse depth smoothness (Baseline) and the result is further improved by using temporal geometric consistency.
On the bottom part of the table, `- (CNN)' are the surface normals predicted from CNNs. 
We can see that the surface normals predicted from the CNNs are better than the corresponding results which are computed from predicted depths in all cases, signifying the importance of a dedicated Normal CNN. 


Fig. \ref{fig:depthNormalEg} (bottom) compares the normals predicted by our framework with that of \cite{yang2018lego,yang2017unsupervised} and our `Baseline' approach (where the normals are computed using the mean cross product rule). 
It can be easily seen that while the inverse depth smoothness regularization produces detailed but very noisy normal maps, our predicted normals are of a significantly high quality preserving the normal edges while being largely smooth/constant on the smooth objects/road and buildings.

It is important to note that unlike the proposed method, our `Baseline', \cite{yang2017depthnormal} and  \cite{yang2018lego} do not have an explicit normal prediction network which allows deviation from the predicted depths. 
Our claim is that a dedicated network and a soft constraint allowing deviation from depth normal consistency is critical in good normal estimation performance. 
A clear visual indication is shown in Fig. \ref{fig:normalComparison} where we compare the predicted normals  by our Normal CNN with the ones   which are computed via the mean cross product of the corresponding depth predictions coming from our joint training framework. 
It can be seen that even after the joint training of depths and normals, computing the normals from the predicted depth leaves us with very noisy undesirable output. This effect is additionally quantified in Table \ref{table:normal_benchmark}.
\section{Conclusion} \label{sec:conclusion}
In this work we have proposed to simultaneously learn a single view depth and normal prediction network, along with a 2-frame visual odometry network in a self-supervised manner from stereo sequence training data. We show that these three networks can be learned together using a soft depth-normal consistency constraint while assuming the surface normals to be piece-wise smooth, to give state of the art surface normal predictions and significantly improved depth predictions when compared to prediction reliant on inverse-depth smoothness prior currently prevalent for self-supervised learning.

\section{Acknowledgement} \label{sec:conclusion}
This work was supported by the UoA Scholarship to HZ, the ARC Laureate Fellowship FL130100102 to IR and the Australian Centre of Excellence for Robotic Vision CE140100016.


\clearpage
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}\typeout{** loaded for the language `#1'. Using the pattern for}\typeout{** the default language instead.}\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{Silberman2012nyuv2}
P.~K. Nathan~Silberman, Derek~Hoiem and R.~Fergus, ``Indoor segmentation and
  support inference from rgbd images,'' in \emph{ECCV}, 2012.

\bibitem{Song_2015_CVPR}
S.~Song, S.~P. Lichtenberg, and J.~Xiao, ``Sun rgb-d: A rgb-d scene
  understanding benchmark suite,'' June 2015.

\bibitem{garg2016depth}
R.~Garg, V.~K. B~G, G.~Carneiro, and I.~Reid, ``Unsupervised cnn for single
  view depth estimation: Geometry to the rescue,'' in \emph{European Conference
  on Computer Vision}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2016,
  pp. 740--756.

\bibitem{horn1981determining}
B.~K. Horn and B.~G. Schunck, ``Determining optical flow,'' \emph{Artificial
  intelligence}, vol.~17, no. 1-3, pp. 185--203, 1981.

\bibitem{godard2016depth}
C.~Godard, O.~{Mac Aodha}, and G.~J. Brostow, ``Unsupervised monocular depth
  estimation with left-right consistency,'' in \emph{CVPR}, 2017.

\bibitem{zhou2017sfmlearner}
T.~Zhou, M.~Brown, N.~Snavely, and D.~G. Lowe, ``Unsupervised learning of depth
  and ego-motion from video,'' in \emph{CVPR}, 2017.

\bibitem{li2017undeepvo}
R.~Li, S.~Wang, Z.~Long, and D.~Gu, ``Undeepvo: Monocular visual odometry
  through unsupervised deep learning,'' \emph{arXiv preprint arXiv:1709.06841},
  2017.

\bibitem{zhan2018depthVO}
H.~Zhan, R.~Garg, C.~S. Weerasekera, K.~Li, H.~Agarwal, and I.~Reid,
  ``Unsupervised learning of monocular depth estimation and visual odometry
  with deep feature reconstruction,'' in \emph{Computer Vision and Pattern
  Recognition (CVPR), 2018 IEEE Conference on}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2018, pp. 340--349.

\bibitem{aleottigenerative}
F.~Aleotti, F.~Tosi, M.~Poggi, and S.~Mattoccia, ``Generative adversarial
  networks for unsupervised monocular depth prediction.''

\bibitem{goodfellow2014generative}
I.~Goodfellow, J.~Pouget-Abadie, M.~Mirza, B.~Xu, D.~Warde-Farley, S.~Ozair,
  A.~Courville, and Y.~Bengio, ``Generative adversarial nets,'' in
  \emph{Advances in neural information processing systems}, 2014, pp.
  2672--2680.

\bibitem{yang2017depthnormal}
Z.~Yang, P.~Wang, W.~Xu, L.~Zhao, and R.~Nevatia, ``Unsupervised learning of
  geometry with edge-aware depth-normal consistency,'' in \emph{AAAI}, 2017.

\bibitem{yang2018lego}
Z.~Yang, P.~Wang, Y.~Wang, W.~Xu, and R.~Nevatia, ``Lego: Learning edge with
  geometry all at once by watching videos,'' in \emph{CVPR}, 2018.

\bibitem{NormalIntegrationSurvey}
\BIBentryALTinterwordspacing
J.-D. Durou, Y.~Qu{\'{e}}au, and J.-F. Aujol, ``{Normal Integration -- Part I:
  A Survey},'' jun 2016. [Online]. Available:
  \url{https://hal.archives-ouvertes.fr/hal-01334349}
\BIBentrySTDinterwordspacing

\bibitem{Zhang99shapefrom}
R.~Zhang, P.-S. Tsai, J.~E. Cryer, and M.~Shah, ``{Shape from Shading: A
  Survey},'' \emph{IEEE Transactions on Pattern Analaysis and Machine
  Intelligence}, vol.~21, no.~8, pp. 690--706, 1999.

\bibitem{suwajanakorn2015depth}
S.~Suwajanakorn, C.~Hernandez, and S.~M. Seitz, ``Depth from focus with your
  mobile phone,'' in \emph{Proceedings of the IEEE Conference on Computer
  Vision and Pattern Recognition}, 2015, pp. 3497--3506.

\bibitem{hoiem2005automatic}
D.~Hoiem, A.~A. Efros, and M.~Hebert, ``Automatic photo pop-up,'' in \emph{ACM
  transactions on graphics (TOG)}, vol.~24, no.~3.\hskip 1em plus 0.5em minus
  0.4em\relax ACM, 2005, pp. 577--584.

\bibitem{hoiem2007recovering}
------, ``Recovering surface layout from an image,'' \emph{International
  Journal of Computer Vision}, vol.~75, no.~1, pp. 151--172, 2007.

\bibitem{criminisi1999single}
A.~Criminisi, I.~Reid, and A.~Zisserman, ``Single view metrology,'' in
  \emph{Computer Vision, 1999. The Proceedings of the Seventh IEEE
  International Conference on}, vol.~1.\hskip 1em plus 0.5em minus 0.4em\relax
  IEEE, 1999, pp. 434--441.

\bibitem{2014discriminativelyladicky}
L.~Ladick{\'{y}}, B.~Zeisl, and M.~Pollefeys, ``{Discriminatively Trained Dense
  Surface Normal Estimation},'' in \emph{ECCV}, 2014, vol. 8693, pp. 468--484.

\bibitem{fouhey2013data}
D.~F. Fouhey, A.~Gupta, and M.~Hebert, ``{Data-driven 3D primitives for single
  image understanding},'' in \emph{Computer Vision (ICCV), 2013 IEEE
  International Conference on}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE,
  2013, pp. 3392--3399.

\bibitem{eigen2015predicting}
D.~Eigen and R.~Fergus, ``Predicting depth, surface normals and semantic labels
  with a common multi-scale convolutional architecture,'' in \emph{Proceedings
  of the IEEE International Conference on Computer Vision}, 2015, pp.
  2650--2658.

\bibitem{Wang15}
X.~Wang, D.~F. Fouhey, and A.~Gupta, ``{Designing Deep Networks for Surface
  Normal Estimation},'' in \emph{CVPR}, 2015.

\bibitem{bansal2016marr}
A.~Bansal, B.~Russell, and A.~Gupta, ``Marr revisited: 2d-3d alignment via
  surface normal prediction,'' in \emph{Proceedings of the IEEE conference on
  computer vision and pattern recognition}, 2016, pp. 5965--5974.

\bibitem{bansal2016pixelnet}
A.~Bansal, X.~Chen, B.~Russell, A.~Gupta, and D.~Ramanan, ``Pixelnet: Towards a
  general pixel-level architecture,'' \emph{arXiv preprint arXiv:1609.06694},
  2016.

\bibitem{saxena2006learning}
A.~Saxena, S.~H. Chung, and A.~Y. Ng, ``Learning depth from single monocular
  images,'' in \emph{Advances in neural information processing systems}, 2006,
  pp. 1161--1168.

\bibitem{saxena2009make3d}
A.~Saxena, M.~Sun, and A.~Y. Ng, ``Make3d: Learning 3d scene structure from a
  single still image,'' \emph{IEEE transactions on pattern analysis and machine
  intelligence}, vol.~31, no.~5, pp. 824--840, 2009.

\bibitem{eigen2014depth}
D.~Eigen, C.~Puhrsch, and R.~Fergus, ``Depth map prediction from a single image
  using a multi-scale deep network,'' in \emph{Advances in neural information
  processing systems}, 2014, pp. 2366--2374.

\bibitem{xie2016deep3d}
J.~Xie, R.~Girshick, and A.~Farhadi, ``Deep3d: Fully automatic 2d-to-3d video
  conversion with deep convolutional neural networks,'' in \emph{European
  Conference on Computer Vision}.\hskip 1em plus 0.5em minus 0.4em\relax
  Springer, 2016, pp. 842--857.

\bibitem{kendall2017uncertainties}
A.~Kendall and Y.~Gal, ``What uncertainties do we need in bayesian deep
  learning for computer vision?'' in \emph{Advances in Neural Information
  Processing Systems}, 2017, pp. 5580--5590.

\bibitem{dharmasiri2017joint}
T.~Dharmasiri, A.~Spek, and T.~Drummond, ``Joint prediction of depths, normals
  and surface curvature from rgb images using cnns,'' in \emph{Intelligent
  Robots and Systems (IROS), 2017 IEEE/RSJ International Conference on}.\hskip
  1em plus 0.5em minus 0.4em\relax IEEE, 2017, pp. 1505--1512.

\bibitem{liu2016learning}
F.~Liu, C.~Shen, G.~Lin, and I.~Reid, ``Learning depth from single monocular
  images using deep convolutional neural fields,'' \emph{IEEE transactions on
  pattern analysis and machine intelligence}, vol.~38, no.~10, pp. 2024--2039,
  2016.

\bibitem{laina2016deeper}
I.~Laina, C.~Rupprecht, V.~Belagiannis, F.~Tombari, and N.~Navab, ``Deeper
  depth prediction with fully convolutional residual networks,'' in \emph{3D
  Vision (3DV), 2016 Fourth International Conference on}.\hskip 1em plus 0.5em
  minus 0.4em\relax IEEE, 2016, pp. 239--248.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun, ``Deep residual learning for image
  recognition,'' in \emph{Proceedings of the IEEE conference on computer vision
  and pattern recognition}, 2016, pp. 770--778.

\bibitem{roy2016monocular}
A.~Roy and S.~Todorovic, ``Monocular depth estimation using neural regression
  forest,'' in \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, 2016, pp. 5506--5514.

\bibitem{hu2018revisitingsingleview}
\BIBentryALTinterwordspacing
J.~Hu, M.~Ozay, Y.~Zhang, and T.~Okatani, ``Revisiting single image depth
  estimation: Toward higher resolution maps with accurate object boundaries,''
  \emph{CoRR}, vol. abs/1803.08673, 2018. [Online]. Available:
  \url{http://arxiv.org/abs/1803.08673}
\BIBentrySTDinterwordspacing

\bibitem{ummenhofer2017demon}
B.~Ummenhofer, H.~Zhou, J.~Uhrig, N.~Mayer, E.~Ilg, A.~Dosovitskiy, and
  T.~Brox, ``Demon: Depth and motion network for learning monocular stereo,''
  in \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern
  Recognition}, 2017, pp. 5038--5047.

\bibitem{mahjourian2018unsupervised}
R.~Mahjourian, M.~Wicke, and A.~Angelova, ``Unsupervised learning of depth and
  ego-motion from monocular video using 3d geometric constraints,'' \emph{arXiv
  preprint arXiv:1802.05522}, 2018.

\bibitem{jia2006crossproduct}
Z.~Jia, ``Using cross-product matrices to compute the svd,'' \emph{Numerical
  Algorithms}, vol.~42, no.~1, pp. 31--61, 2006.

\bibitem{jaderberg2015stn}
M.~Jaderberg, K.~Simonyan, A.~Zisserman \emph{et~al.}, ``Spatial transformer
  networks,'' in \emph{Advances in Neural Information Processing Systems},
  2015, pp. 2017--2025.

\bibitem{weerasekera2017normals}
C.~S. Weerasekera, Y.~Latif, R.~Garg, and I.~Reid, ``Dense monocular
  reconstruction using surface normals,'' in \emph{2017 IEEE International
  Conference on Robotics and Automation (ICRA)}, May 2017, pp. 2524--2531.

\bibitem{he2016resnet}
K.~He, X.~Zhang, S.~Ren, and J.~Sun, ``Deep residual learning for image
  recognition,'' in \emph{Proceedings of the IEEE conference on computer vision
  and pattern recognition}, 2016, pp. 770--778.

\bibitem{Geiger2012kitti}
A.~Geiger, P.~Lenz, and R.~Urtasun, ``Are we ready for autonomous driving? the
  kitti vision benchmark suite,'' in \emph{Conference on Computer Vision and
  Pattern Recognition (CVPR)}, 2012.

\bibitem{Geiger2013kitti}
A.~Geiger, P.~Lenz, C.~Stiller, and R.~Urtasun, ``Vision meets robotics: The
  kitti dataset,'' \emph{International Journal of Robotics Research (IJRR)},
  2013.

\bibitem{jia2014caffe}
Y.~Jia, E.~Shelhamer, J.~Donahue, S.~Karayev, J.~Long, R.~Girshick,
  S.~Guadarrama, and T.~Darrell, ``Caffe: Convolutional architecture for fast
  feature embedding,'' in \emph{Proceedings of the 22nd ACM international
  conference on Multimedia}.\hskip 1em plus 0.5em minus 0.4em\relax ACM, 2014,
  pp. 675--678.

\bibitem{kingma2014adam}
D.~Kingma and J.~Ba, ``Adam: A method for stochastic optimization,''
  \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{yang2017unsupervised}
Z.~Yang, P.~Wang, W.~Xu, L.~Zhao, and R.~Nevatia, ``Unsupervised learning of
  geometry with edge-aware depth-normal consistency,'' \emph{arXiv preprint
  arXiv:1711.03665}, 2017.

\end{thebibliography}
 




\end{document}
