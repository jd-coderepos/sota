

\documentclass[11pt,a4paper]{article}
\usepackage[acceptedWithA]{acl2020}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}




\usepackage{natbib}
\usepackage{graphicx}
\usepackage{todonotes}
\usepackage{times}
\usepackage{latexsym}
\usepackage{color} 
\usepackage{graphicx}
\usepackage{stfloats}
\usepackage{booktabs}
\usepackage{array}
\usepackage{amssymb, amsmath}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{threeparttable}
\usepackage{algorithm}
\usepackage{algorithmic}
\renewcommand{\UrlFont}{\ttfamily\small}
\renewcommand{\algorithmicrequire}{ \textbf{Input:}}
\renewcommand{\algorithmicensure}{ \textbf{Output:}}
\usepackage{arydshln}


\newcommand{\pbl}[1]{{\color{red}{[{\bf pbl}: #1]}}}

\newcommand{\sagent}{\textit{Agent} }
\DeclareMathOperator*{\argmax}{argmax}
\usepackage{enumitem}
\usepackage{makecell}
\usepackage{xcolor}
\definecolor{mygreen}{HTML}{3cb44b}
\interfootnotelinepenalty=10000
\usepackage{microtype}



\clearpage{}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{mathtools} 

\usepackage{verbatim}

\usepackage{anyfontsize}

\usepackage{color}
\usepackage{tikz}
\usetikzlibrary{arrows,shapes,snakes,automata,backgrounds,fit,petri}
\usepackage{adjustbox}

\newcommand{\RN}[1]{\textup{\lowercase\expandafter{\it \romannumeral#1}}}


\usepackage{booktabs}

\usepackage{tcolorbox}



\makeatletter
\newcommand{\distas}[1]{\mathbin{\overset{#1}{\kern\z@\sim}}}


\usepackage{enumitem}








\newcommand{\cl}[1]{{\color{blue}{\bf\sf [Chunyuan: #1]}}}


\def\Dir{\textsf{Dir}} \def\Bern{\textsf{Ber}} \def\Pois{\textsf{Pois}} \def\Std{\textsf{Std}} \def\Var{\textsf{Var}} \def\Cor{\textsf{Cor}} \def\Cov{\textsf{Cov}} \def\Gal{\textsf{Gamma}} \def\Beta{\textsf{Beta}} \def\KL{\textsf{KL}} \def\VI{\textsf{VI}} \def\MI{\textsf{MI}} \def\H{\textsf{H}} 

\newcommand{\ie}[0]{\emph{i.e., }}
\newcommand{\ea}[0]{\emph{et al. }}
\newcommand{\eg}[0]{\emph{e.g., }}
\newcommand{\cf}[0]{\emph{cf. }}
\newcommand{\etc}[0]{\emph{etc.}}

\newcommand{\zerov}{\ensuremath{{\bf 0}}}
\newcommand{\onev}{\ensuremath{{\bf 1}}}

\newcommand{\beq}{\vspace{0mm}}
\newcommand{\beqs}{\vspace{0mm}}
\newcommand{\barr}{\begin{array}}
\newcommand{\earr}{\end{array}}

\newcommand{\Amat}[0]{{{\bf A}}}
\newcommand{\Bmat}{{\bf B}}
\newcommand{\Cmat}{{\bf C}}
\newcommand{\Dmat}{{\bf D}}
\newcommand{\Emat}[0]{{{\bf E}}}
\newcommand{\Fmat}[0]{{{\bf F}}\xspace}
\newcommand{\Gmat}{{\bf G}}
\newcommand{\Hmat}{{\bf H}}
\newcommand{\Imat}{{\bf I}}
\newcommand{\Jmat}[0]{{{\bf J}}\xspace}
\newcommand{\Kmat}[0]{{{\bf K}}\xspace}
\newcommand{\Lmat}[0]{{{\bf L}}}
\newcommand{\Mmat}{{\bf M}}
\newcommand{\Nmat}[0]{{{\bf N}}\xspace}
\newcommand{\Omat}[0]{{{\bf O}}}
\newcommand{\Pmat}{{\bf P}}
\newcommand{\Qmat}[0]{{{\bf Q}}\xspace}
\newcommand{\Rmat}[0]{{{\bf R}}}
\newcommand{\Smat}[0]{{{\bf S}}}
\newcommand{\Tmat}[0]{{{\bf T}}}
\newcommand{\Umat}[0]{{{\bf U}}}
\newcommand{\Vmat}[0]{{{\bf V}}}
\newcommand{\Wmat}[0]{{{\bf W}}}
\newcommand{\Xmat}[0]{{{\bf X}}}
\newcommand{\Ymat}{{\bf Y}}
\newcommand{\Zmat}{{\bf Z}}

\newcommand{\av}[0]{{\boldsymbol{a}}}
\newcommand{\bv}[0]{{\boldsymbol{b}}}
\newcommand{\cv}[0]{{\boldsymbol{c}}}
\newcommand{\dv}{\boldsymbol{d}}
\newcommand{\ev}[0]{{\boldsymbol{e}}\xspace}
\newcommand{\fv}[0]{{\boldsymbol{f}}}
\newcommand{\gv}[0]{{\boldsymbol{g}}}
\newcommand{\hv}[0]{{\boldsymbol{h}}}
\newcommand{\iv}[0]{{\boldsymbol{i}}\xspace}
\newcommand{\jv}[0]{{\boldsymbol{j}}\xspace}
\newcommand{\kv}[0]{{\boldsymbol{k}}\xspace}
\newcommand{\lv}[0]{{\boldsymbol{l}}}
\newcommand{\mv}[0]{{\boldsymbol{m}}}
\newcommand{\nv}[0]{{\boldsymbol{n}}\xspace}
\newcommand{\ov}[0]{{\boldsymbol{o}}\xspace}
\newcommand{\pv}[0]{{\boldsymbol{p}}}
\newcommand{\qv}[0]{{\boldsymbol{q}}}
\newcommand{\rv}{\boldsymbol{r}}
\newcommand{\sv}[0]{{\boldsymbol{s}}}
\newcommand{\tv}[0]{{\boldsymbol{t}}}
\newcommand{\uv}{\boldsymbol{u}}
\newcommand{\vv}{\boldsymbol{v}}
\newcommand{\wv}{\boldsymbol{w}}
\newcommand{\xv}{\boldsymbol{x}}
\newcommand{\yv}{\boldsymbol{y}}
\newcommand{\zv}{\boldsymbol{z}}
\newcommand{\cdotv}{\boldsymbol{\cdot}}

\newcommand{\Gammamat}[0]{{\boldsymbol{\Gamma}}\xspace}
\newcommand{\Deltamat}[0]{{\boldsymbol{\Delta}}\xspace}
\newcommand{\Thetamat}{\boldsymbol{\Theta}}
\newcommand{\Betamat}{\boldsymbol{\Beta}}
\newcommand{\Lambdamat}{\boldsymbol{\Lambda}}
\newcommand{\Ximat}[0]{{\boldsymbol{\Xi}}\xspace}
\newcommand{\Pimat}[0]{{\boldsymbol{\Pi}}\xspace}
\newcommand{\Sigmamat}[0]{{\boldsymbol{\Sigma}}}
\newcommand{\Upsilonmat}[0]{{\boldsymbol{\Upsilon}}\xspace}
\newcommand{\Phimat}{\boldsymbol{\Phi}}
\newcommand{\Psimat}{\boldsymbol{\Psi}}
\newcommand{\Omegamat}[0]{{\boldsymbol{\Omega}}}

\newcommand{\alphav}{\boldsymbol{\alpha}}
\newcommand{\chiv}{\boldsymbol{\chi}}
\newcommand{\betav}[0]{{\boldsymbol{\beta}}}
\newcommand{\gammav}[0]{{\boldsymbol{\gamma}}\xspace}
\newcommand{\deltav}[0]{{\boldsymbol{\delta}}\xspace}
\newcommand{\epsilonv}{\boldsymbol{\epsilon}}
\newcommand{\zetav}{\boldsymbol{\zeta}}
\newcommand{\etav}{\boldsymbol{\eta}}
\newcommand{\ellv}[0]{{\boldsymbol{\ell}}}
\newcommand{\thetav}{\boldsymbol{\theta}}
\newcommand{\iotav}[0]{{\boldsymbol{\iota}}}
\newcommand{\kappav}[0]{{\boldsymbol{\kappa}}\xspace}
\newcommand{\lambdav}[0]{{\boldsymbol{\lambda}}}
\newcommand{\muv}[0]{{\boldsymbol{\mu}}}
\newcommand{\nuv}[0]{{\boldsymbol{\nu}}}
\newcommand{\xiv}[0]{{\boldsymbol{\xi}}}
\newcommand{\omicronv}[0]{{\boldsymbol{\omicron}}\xspace}
\newcommand{\piv}{\boldsymbol{\pi}}
\newcommand{\rhov}[0]{{\boldsymbol{\rho}}\xspace}
\newcommand{\sigmav}[0]{{\boldsymbol{\sigma}}}
\newcommand{\tauv}[0]{{\boldsymbol{\tau}}\xspace}
\newcommand{\upsilonv}[0]{{\boldsymbol{\upsilon}}\xspace}
\newcommand{\phiv}{\boldsymbol{\phi}}
\newcommand{\psiv}{\boldsymbol{\psi}}
\newcommand{\varthetav}{\boldsymbol{\vartheta}}
\newcommand{\omegav}[0]{{\boldsymbol{\omega}}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Z}{\mathbb{Z}}

\newcommand{\Rcalmat}{\mathcal{\bf R}}
\newcommand{\Pcalmat}{\mathcal{\bf P}}


\newcommand{\Xcal}{\mathcal{X}}
\newcommand{\Ycal}{\mathcal{Y}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Acal}{\mathcal{A}}
\newcommand{\Bcal}{\mathcal{B}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Fcal}{\mathcal{F}}
\newcommand{\Tcal}{\mathcal{T}}
\newcommand{\Pcal}{\mathcal{P}}
\newcommand{\Ical}{\mathcal{I}}
\newcommand{\Kcal}{\mathcal{K}}
\newcommand{\Gcal}{\mathcal{G}}
\newcommand{\Qcal}{\mathcal{Q}}
\newcommand{\Rcal}{\mathcal{R}}
\newcommand{\Scal}{\mathcal{S}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\Vcal}{\mathcal{V}}
\newcommand{\Zcal}{\mathcal{Z}}
\newcommand{\Ucal}{\mathcal{U}}
\newcommand{\Jcal}{\mathcal{J}}
\newcommand{\scal}{\mathcal{s}}

\newtheorem{theorem}{Theorem} \newtheorem{lemma}{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}{Corollary}

\DeclareMathOperator{\CC}{\mathbb{C}} \DeclareMathOperator{\EE}{\mathbb{E}} \DeclareMathOperator{\KK}{\mathbb{K}} \DeclareMathOperator{\MM}{\mathbb{M}} \DeclareMathOperator{\NN}{\mathbb{N}} \DeclareMathOperator{\PP}{\mathbb{P}} \DeclareMathOperator{\QQ}{\mathbb{Q}} \DeclareMathOperator{\RR}{\mathbb{R}} \DeclareMathOperator{\ZZ}{\mathbb{Z}} 

\ifx\assumption\undefined
\newtheorem{assumption}{Assumption}
\fi

\ifx\definition\undefined
\newtheorem{definition}{Definition}
\fi

\ifx\remark\undefined
\newtheorem{remark}{Remark}
\fi



\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}


 \newcommand{\hlightP}[1]{\ooalign{\hss\makebox[0pt]{\fcolorbox{red!30}{green!10}{}}\hss\cr\phantom{}}}
 
 \newcommand{\hlightC}[1]{\ooalign{\hss\makebox[0pt]{\fcolorbox{green!30}{red!40}{}}\hss\cr\phantom{}}}




%
\clearpage{}
\usepackage{color, colortbl}
\definecolor{Gray}{gray}{0.93}


\newcommand\BibTeX{B\textsc{ib}\TeX}
\newcommand{\data}{\textsc{FewShotWOZ}}
\newcommand{\model}{\textsc{SC-GPT}}



\title{Few-shot Natural Language Generation for Task-Oriented Dialog}


\author{Baolin Peng, Chenguang Zhu, Chunyuan Li \\ \textbf{Xiujun Li, Jinchao Li, Michael Zeng, Jianfeng Gao} \\
  Microsoft Research, Redmond \\
\texttt{\{bapeng,chezhu,chunyl,xiul,jincli,nzeng,jfgao\}@microsoft.com}
 }

\date{}

\begin{document}
\maketitle
\begin{abstract}
As a crucial component in task-oriented dialog systems, the Natural Language Generation (NLG) module converts a dialog act represented in a semantic form into a response in natural language. The success of traditional template-based or statistical models typically relies on heavily annotated data, which is infeasible for new domains.
Therefore, it is pivotal for an NLG system to generalize well with limited labelled data in real applications. To this end, we present \data{}, the first NLG benchmark to simulate the few-shot learning setting in task-oriented dialog systems. Further, we develop the SC-GPT\footnote{{\bf S}emantically-{\bf C}onditioned {\bf G}enerative {\bf P}re-{\bf T}raining} model. It is pre-trained on a large set of annotated NLG corpus to acquire the controllable generation ability, and fine-tuned with only a few domain-specific labels 
to adapt to new domains. 
Experiments on \data{} and the large Multi-Domain-WOZ datasets show that the proposed SC-GPT significantly outperforms existing methods, measured by various automatic metrics and human evaluations.
\end{abstract}



\section{Introduction}

Task-oriented dialog systems are becoming increasingly popular, as they can assist users in various daily activities such as ticket booking and restaurant reservations. In a typical task-oriented dialog system, the {\it Natural Language Generation} (NLG) module plays a crucial role: it converts a system action (\eg often specified in a semantic form selected by a dialog policy) into a final response in natural language. 
Hence, the response should be 
{\it adequate} to represent semantic dialog actions, and 
{\it fluent} to engage users' attention. As the ultimate interface to interacts with users, NLG plays a significant impact on the users' experience.



Existing methods for NLG can be broadly summarized into two major categories.
 {\it Template-based methods} require domain experts to handcraft templates for each domain, and the system fills in slot-values afterward \citep{rule,halogen}. Thus, the produced responses are often adequate to contain the required semantic information, but not always fluent and nature, hurting users' experiences.
 {\it Statistical language models} such as neural networks \citep{gao2019neural} learn to generate fluent responses via training from labelled corpus. One canonical model is {\it semantically conditioned LSTM} (SC-LSTM) \citep{wen-etal-2015-semantically}, which encodes dialog acts with one-hot representations and uses it as an extra feature to inform the sentence generation process. Despite its good performance on simple domains, it requires large amounts of domain-specific annotated data which is not available for many domains in real-world applications. Even worse, this renders severe scalability issues when the number of possible combinations of dialog acts grows exponentially with the number of slots in more complex domains. 













We revisit the current research benchmarks for NLG, and notice that each dialog domain is extensively labelled to favor model training. However, this is in contrast to the real-world application scenarios, where only very limited amounts of labelled data are available for new domains. To simulate such a few-shot learning setting, we have developed a new benchmark dataset, called \data{}, based on the MultiWOZ \citep{budzianowski2018multiwoz} and Cambridge NLG datasets \citep{rnnlg}. 
\data{} consists of dialog utterances from 7 domains. For each domain, we provide less than 50 labeled utterances for fine-tuning.  We believe that \data{} can better inspire  research to address the challenge of learning  data-hungry statistical models with very limited amounts of labelled data in real-world scenarios. 


To deal with the challenge of few-shot learning, we develop the SC-GPT model.  
SC-GPT is a multi-layer Transformer neural language model, trained in three steps:
 Pre-trained on plain text, similar to GPT-2 \citep{gpt2}; 
 Continuously pre-trained on large amounts of dialog-act labeled utterances corpora to acquire the ability of controllable generation;
 Fine-tuned for a target domain using very limited amounts of domain labels.
Unlike GPT-2, SC-GPT generates semantically controlled responses that are conditioned on the given semantic form, similar to SC-LSTM but requiring much less domain labels to generalize to new domains. 













\begin{figure*}[t!]
\centering
\begin{tabular}{c c}
	\hspace{-2mm}
	\includegraphics[height=3.0cm]{figures/overall_task_dialog.pdf}  & 
	 \hspace{-2mm}
	\includegraphics[height=3.0cm]{figures/dialog_act.pdf} \\
	(a) The overall framework of a task-oriented dialog system \vspace{2mm} & 
	\hspace{-2mm}  (b) Dialog act \& Response \hspace{-0mm} \\ 
\end{tabular}

\caption{Illustration of the NLG module in the overall task-oriented dialog system. (a) The NLG module is highlighted with glowing black bounding boxes. (b) One example of dialog act (including intent and slot-value pairs) and its corresponding natural language response. }
\label{fig:dailog_system}
\end{figure*}


In summary, our key contributions are three-fold:
\begin{itemize}
\setlength{\itemsep}{-3pt}
    \item A new benchmark \data{} is introduced to simulate the few-shot adaptation setting where only a handful of training data from each domain is available.
    \item We propose a new model SC-GPT. To our best knowledge, this work is the first study of exploiting state-of-the-art pre-trained language models for NLG in task-oriented dialog systems.
    \item On the MultiWOZ dataset, SC-GPT creates a new SOTA, outperforming previous models by 4 points in BLEU. 
     On \data{}, SC-GPT outperforms several strong baselines such as SC-LSTM and HDSA \citep{chen-etal-2019-semantically}, showing that SC-GPT adapts to new domain much more effectively, requiring much smaller amounts of in-domain labels.
     We release our code\footnote{\url{https://github.com/pengbaolin/SC-GPT}} and dataset\footnote{Project website: \url{https://aka.ms/scgpt}} for reproducible research.
\end{itemize}









\section{Background}
A typical task-oriented spoken dialog system uses a pipeline architecture, as shown in Figure~\ref{fig:dailog_system} (a), where each dialog turn is processed using a four-step procedure.
 Transcriptions of user’s input are first passed to the natural language understanding (NLU)
module, where the user’s intention and other key information are extracted. 
 This information is then formatted as the input to dialog state tracking (DST), which maintains
the current state of the dialog. 
  Outputs of DST are passed to the dialog policy module, which produces a dialog act based on the facts or entities retrieved from external resources (such as a database or a knowledge base).
  The dialog act emitted by the dialog policy module serves as the input to
the NLG, through which a system response in natural language is generated. 
In this paper, we focus
on the NLG component of task-oriented dialog
systems, \ie how to produce natural language responses conditioned on dialog acts.

Specifically, {\it dialog act}
 is defined as the combination of intent  and slot-value pairs :

where  is the number of pairs\footnote{In some literature, dialog act denotes only the type of system actions, slot-value pairs are defined as meaning representations. Throughout this paper, we follow the usage in \citet{budzianowski2018multiwoz} and use dialog acts to indicate system action and associated slot-value pairs.}, which varies in different dialog acts.   

\begin{itemize}
\setlength{\itemsep}{-3pt}
    \item {\it Intents} are usually used to distinguish different types of system actions. Typical examples include {\it inform}, {\it request}, {\it confirm}, {\it select} \etc~
    \item {\it Slot-value pairs} indicate the category and content of the information to express in the utterance, respectively.
\end{itemize}

The goal of NLG is to translate  into a natural language response , where  is the sequence length. In Figure~\ref{fig:dailog_system} (b), we show an  example of the dialog act: 
, and the corresponding natural language response is ``{\it Let me confirm that you are searching for Hilton in the center area}''. 










\begin{figure*}[t!]
\centering
\includegraphics[width=2\columnwidth]{figures/ScGPT.pdf}
\caption{Illustration of SC-GPT. In this example, SC-GPT generates a new word token (\eg ``\texttt{confirm}'' or ``\texttt{center}'') by attending the entire dialog act and word tokens on the left within the response.}
\label{fig:scGPT}
\end{figure*}

\section{Semantically Conditioned GPT}


We tackle this generation problem using conditional neural language models. Given training data of  samples , our goal is to build a statistical model parameterized by  to characterize . To leverage the sequential structure of response, one may further decompose the joint probability of  using the chain rule, casting an auto-regressive generation process as follows:

where  indicates all tokens before .

Learning  is performed via maximizing the log-likelihood (MLE) of the conditional probabilities in \eqref{eq:conditional} over the entire training dataset:



In this paper, we employ the Transformers \citep{transformer} to parameterize the conditionals in \eqref{eq:conditional}. To enable strong generalization and controllable ability for the learned model, we propose the following three-stage procedure as the training recipe. 

\paragraph{Massive Plain Language Pre-training.} Large models trained on massive training corpus usually generalize better to new domains. Inspired by this, we inherit the GPT-2 architecture \citep{gpt2} as the backbone language model. GPT-2 is an auto-regressive language model that leverages 12-24 layers of masked, multi-head self-attention Transformers. GPT-2 is pre-trained on extremely massive text data OpenWebText~\citep{gpt2}. It has demonstrated superior performance on characterizing human language data distribution and knowledge transfer. Given text prompts, GPT-2 can often generate realistic sentences.

\paragraph{Dialog-Act Controlled Pre-training.} To enable the guidance of dialog act in response generation, we propose to continuously pre-train the GPT-2 model on large amounts of annotated (dialog act, response) pairs. 
The pre-training dataset\footnote{The domains appearing in fine-tuning are excluded.} includes annotated training pairs from Schema-Guided Dialog corpus, MultiWOZ corpus, Frame corpus, and Facebook Multilingual Dialog Corpus. The total size of the pre-training corpus is around 400k examples.

We firstly pre-process dialog act  into a sequence of control codes using the following format: 

Meanwhile, the output sequence  is pre-processed via appending  with a special start token \texttt{[BOS]} and an end token \texttt{[EOS]}.  Finally, the sequentialized dialog act  is concatenated with its augmented response , and then fed into GPT-2. During training, the prediction loss is only computed for , and  provides the attended conditions. 
Since the dialog act represents the semantics of the generated sentences, we follow the naming convention of SC-LSTM, and term our model as {\it Semantically Conditioned Generative Pre-training} (SC-GPT). 
The overall architecture of SC-GPT is illustrated in Figure~\ref{fig:scGPT}.






\paragraph{Fine-tuning.} For a new domain, a dialog act usually contains novel intents or slot-value pairs, and annotated training samples are often limited. We fine-tune SC-GPT on limited amounts of domain-specific labels for adaptation. 
The fine-tuning follows the same procedure of dialog-act controlled pre-training, as described above, but uses only a few dozens of domain labels.  


It is worth noticing that the above recipe has several favorable properties:


\begin{itemize}
\setlength{\itemsep}{-3pt}
    \item {\it Flexibility.} SC-GPT operates on a sequence of tokens without delexicalization, which means that SC-GPT does not assume a fixed one-hot or tree-structured dialog act representation vectors. Hence, it has great flexibility in extending to novel dialog acts. 
    \item {\it Controllability.} In contrast to GPT-2 that generates natural sentences without high-level semantic guidance, SC-GPT can generate sentences with adequate intent and slot-value information and maintain its fluency.
\item {\it Generalizability.}  SC-GPT is able to generalize significantly better than SC-LSTM, due to the pre-training on massive plain text corpora and annotated dialog datasets.
\end{itemize}














\begin{table*}[htbp]
    \centering
    \footnotesize
    \setlength{\tabcolsep}{1.0mm}{
    \begin{tabular}{@{}p{5.5cm}@{} | @{}p{2.25cm}@{} @{}p{2.25cm}@{} @{}p{2.25cm}@{} @{}p{2.25cm}@{}  }
    \toprule
         Statistics & \textbf{E2E NLG} & \textbf{BAGEL} & \textbf{RNNLG} &   \textbf{\data{}} \\
         \midrule
         \# Domains & 1 & 1 & 4 & 7 \\
         Avg. \# Intents & 1 & 8 & 11.25 & 8.14 \\
         Avg. \# Slots & 8 & 10 & 21 & 16.15 \\
         Avg. \# Delexicalised DAs in Training & 109 &  23.9 & 794.5 & 50 \\
         Avg. \# Delexicalised DAs in Testing & 7 & 14.3 & 566.5 & 472.857 \\
         Overlap Percentage & 100\% & 99.6\% & 94.00\% & 8.82\% \\
         Avg. \# Training Instances & 42056 & 363 & 4625.5 & 50 \\
         Avg. \# Testing  Instances & 630 & 41 & 1792.5 & 472.86 \\
    \bottomrule
    \end{tabular}
    }
    \caption{Comparison of existing NLG datasets, including E2E NLG \cite{e2enlg}, BAGEL\cite{bagel}, Cambridge NLG\cite{rnnlg} and the proposed \data{}. }
    \label{tab:dataset_compare}
\end{table*}



\section{Dataset: \data{}}
\paragraph{Revisiting NLG Benchmarks.}
The three commonly used NLG datasets in developing and evaluating task-oriented dialog systems are E2E NLG~\citep{e2enlg}  BAGEL~\citep{bagel} and RNNLG~\citep{rnnlg}, as summarized in Table \ref{tab:dataset_compare}. 
We observe two issues from their shared statistics:   
  
All the datasets contain a large number of labelled training samples for each domain, ranging from hundreds to tens of thousands. However, the cost of labeling is high in practice, \eg labeling 50 utterances is 5 hours per domain. Creating such an extensively annotated dataset for each new domain is prohibitively expensive. 
 
The percentage of distinct delexicalised dialog acts between training and testing data is quite small. For example, the delexicalised dialog acts in testing is 100\% covered by the training set for the E2E NLG dataset. It renders difficulties in evaluating the model's generalization ability for new domains. 

\paragraph{\data{}.}
To build a setting for more pragmatic NLG scenarios, we introduce a new dataset \data{} to better reflect real application complexity, and encourage the community to develop algorithms that are capable of generalizing with only a few domain-specific labels for each (new) domain. The dataset statistics are shown in the last column of Table \ref{tab:dataset_compare}. We see that \data{} is different from the other datasets in three aspects:
  {\it More domains}. 
\data{} contains seven domains in total, which is larger than any existing NLG datasets. 
 {\it Less training instances}. 
Importantly, \data{} has a much smaller number of training instances per domain, aiming to evaluate the few-shot learning ability. 
 {\it Lower training/testing overlap}. \data{} has only 8.82\% overlap, significantly smaller than the other datasets, which amount to more than 90\% overlap. The average number of intents per instance in /~/~ domain is 2, 1.33, and 2.05, respectively. In contrast, there is only one intent for each example in the other datasets.  The NLG task defined on \data{} requires the models to learn to generalize over new compositions of intents. The details of \data{} is shown in Table~\ref{tab:fewshotwoz}.















\begin{table*}[htbp]
    \centering
    \footnotesize
    \setlength{\tabcolsep}{1.0mm}{
\begin{tabular}{@{}p{3.5cm}@{}  @{}p{2.0cm}@{}   @{}p{1.7cm}@{}  @{}p{1.7cm}@{}  @{}p{1.5cm}@{}  @{}p{2.0cm}@{}  @{}p{1.5cm}@{}  @{}p{1.5cm}@{} }
     \toprule
         Statistics & \!\!	
         &		&		&		
         &	\!\!	&		&	 \\
         \midrule
        \# Intent	&	9	&	10	&	13	&	13	&	5	&	2	&	5 \\
        \# Slot	&	21	&	19	&	22	&	22	&	10	&	7	&	13 \\
        \# DAs in training	&	50	&	50	&	50	&	50	&	50	&	40	&	50 \\
        \# DAs in testing	&	129	&	78	&	1379	&	680	&	340	&	47	&	657 \\
        Overlap Percentage	&	35.56	&	60.26	&	2.61	&	5.74	&	13.82	&	72.34	&	6.55 \\
        Avg. \#DAs per Instance	&	1	&	1	&	1	&	1	&	2	&1.33	&	2.05 \\
        \# Training Instances	&	50	&	50	&	50	&	50	&	50	&	40	&	50 \\
        \# Testing Instances	&	129	&	78	&	1379	&	680	&	340	&	47	&	657 \\
     \bottomrule
     \end{tabular}
}
    \caption{\data{} statistics over 7 different domains. }
    \label{tab:fewshotwoz}
    \vspace{-2mm}
\end{table*}

\paragraph{Collection Protocols.} We construct \data{} via re-organizing data samples from RNNLG and MultiWOZ datasets~\citep{budzianowski2018multiwoz}. For each domain in RNNLG, we first group utterances according to their delexicalised dialog acts, and keep only one utterance as the target sentence. 
To ensure diversity, we consider three domains from MultiWOZ: , , and . 
Since MultiWOZ is a cross-domain dataset, the dialog act of an utterance may exist in multiple domains. 
We choose to keep utterances whose dialog act appears only in one domain. 
Similar delexicalising processing is applied to ensure that each dialog act has only one target utterance. 
Finally, to simulate the few-shot learning in practice, we randomly sample 50 training examples for each domain, except the  domain, which has 40 examples. 






\section{Related Work}







\paragraph{Pre-trained Models.} 
Pre-trained language models (PLMs) have substantially advanced the state-of-the-art across a variety of natural language processing (NLP) tasks~\cite{peters2018deep,devlin2019bert,yang2019xlnet,liu2019roberta,keskar2019ctrl,raffel2019exploring}. PLMs are often trained to predict words based on their context on massive text data, and the learned models can be fine-tuned to adapt to various downstream tasks.
The closest line of research to ours are GPT-2~\cite{gpt2}, CTRL~\cite{keskar2019ctrl} and Grover~\cite{zellers2019defending}. GPT-2 first investigated missive Transformer-based auto-regressive language models with large-scale text data for pre-training. After fine-tuning, GPT-2 achieves drastic improvements on several generation tasks. One drawback of GPT-2 is the lack of high-level semantic controlling ability in language generation.
To alleviate this issue, CTRL~\citep{keskar2019ctrl} was introduced to train the model based on pre-defined codes such as text style, content description, and task-specific behavior, meanwhile Grover~\cite{zellers2019defending} was proposed to generate news articles conditioned on authors, dates \etc ~Although conceptually similar to our SC-GPT, CTRL and Grover cannot be readily applied to NLG in task-oriented dialog systems, as the conditioning codes are quite different. Another controllable generation work for GPT-2 is PPLM~\citep{dathathri2019plug}, which provides a decoding scheme to guide the generation process using key-words or classifiers, without re-training the model. 
In this paper, we focus on pre-training an NLG model conditioned on finer-grained semantic dialog acts, which are more desirable for dialog systems.




\paragraph{Dialog.} 
Various dialog systems have been developed \citep{gao2019neural}, including task-oriented dialog systems such as Rasa\footnote{https://rasa.com/}, Microsoft Bot Framework\footnote{https://dev.botframework.com/}, and Conversational Learner\footnote{https://www.microsoft.com/en-us/research/project/conversation-learner/}, and chit-chat systems such as XiaoIce~\cite{zhou2018design}, DialoGPT~\cite{zhang2019dialogpt}, Meena~\cite{adiwardana2020towards}. 
In this paper, we focus on task-oriented systems, particularly the NLG module. 
With the blooming of deep learning, neural sequential models have shown powerful capability and flexibility in NLG. Extensive efforts have been made, including new architecture choices such as RNNs~\citep{wen-etal-2015-stochastic}, attention RNNs~\citep{dusek-jurcicek-2016-sequence}, SC-LSTM  \citep{wen-etal-2015-semantically} and its variants~\citep{tran-etal-2017-neural, tran-nguyen-2017-natural}, as well as learning objectives 
\citep{zhu-etal-2019-multi}. However, they all require large amounts of annotated data to reach satisfactory performance. 
A more realistic scenario is to require much less labeling and improve the sample efficiency of models, This is especially important when deploying the models to new domains, where dialog acts need to be labelled from scratch. Our paper aims to formally set up such a research scenario by proposing a new dataset \data{}, and a new model SC-GPT.


















\begin{table*}[t!]
\footnotesize
\centering
\includegraphics[height=2.5cm]{figures/fewshot_results_v2.pdf}
\caption{Performance of different methods on \data{}} 
\label{tab:res_fewshotwoz}
\vspace{-2mm}
\end{table*}

\begin{table}
\footnotesize
\centering
\setlength{\tabcolsep}{1.6mm}{
\begin{tabular}{c cc} 
\toprule 
\textbf{Model} & \textbf{Informativeness} & \textbf{Naturalness} \\
\midrule
SC-LSTM & 2.29 & 2.13\\
GPT-2 & \ 2.54\textsuperscript{*}  &  \ 2.38\textsuperscript{*} \\
SC-GPT & \ \ \ 2.64\textsuperscript{*\textdagger} & \ \ \ \  2.47\textsuperscript{*\textdagger} \\
\midrule
\textit{Human} & 2.92 & 2.72 \\
\bottomrule \-2mm]
\textsuperscript{*} 
\end{tabular} }
\caption{Human evaluation on MultiWOZ. Statistical significance was computed with a two-tailed t-test between SC-GPT and HDSA.}
\label{tab:human_eval_multiwoz}
\end{table}


\begin{table*}[t!]\centering
\begin{minipage}{15.6cm}\vspace{0mm}    \centering
\begin{tcolorbox} 
    \centering
    \scriptsize
     \hspace{-6mm}
    \begin{tabular}{ccp{0.83\columnwidth}}
\# & \textbf{Model} & \textbf{Generated Responses from Different Models} \\
    \midrule
    1 & \textit{\textbf{Input DA}} & Laptop\{inform(name=satellite proteus 84; type=laptop; memory=8 gb; drive=1 tb; weight=2.3 kg)\} \2.5pt]
    \ & SC-LSTM & the satellite proteus 84 is a laptop with 8 gb , with a 1 tb drive , and is \colorbox{mygreen!30}{for business computing} , and is \colorbox{mygreen!30}{for business computing} 
    ~~~\% {\it \colorbox{mygreen!30}{[businesscomputing=true]} \colorbox{blue!30}{[weight=2.3kg]} }\2.5pt]
    \ & SC-GPT & the satellite proteus 84 is a laptop with 8 gb memory , 1 tb drive , and a weight of 2.3 kg \2.5pt]
    \ & \textit{\textbf{Reference}} & marnee thai and thanh long are the only 2 moderately priced restaurants near outer sunset that are good for dinner  \2.5pt]
    \ & GPT-2 & there is a moderately priced restaurant called marnee thai and thanh long that is near the outer sunset area 
    ~~~\% {\it \colorbox{blue!30}{[goodformeal=dinner]} \colorbox{blue!30}{[inform\_only\_match]}  }\\ [2.5pt]
    \ & SC-GPT & marnee thai and thanh long is the only restaurant that serves moderately priced food near outer sunset and good for dinner \2.5pt]
    \ & \textit{\textbf{Reference}} &  travel time is 50 minutes . i found 5 hotels you may like . do you have a star rating request or an area you prefer \2.5pt] \vspace{-3mm}
    \end{tabular}
\end{tcolorbox}
\vspace{-2mm}
\caption{Examples of generated utterances from different models, along with its corresponding dialog acts (DAs) and references. The first two examples are sampled from \data{} and the last one is from MultiWOZ. Each generated utterance is followed by a brief description explaining the errors (starting with ``\%''). (Better viewed in color. \colorbox{red!30}{wrong}, \colorbox{mygreen!30} {redundant}, \colorbox{blue!30} {missing} information)}
    \label{tab:examples}
\end{minipage}
\end{table*}



\subsection{MultiWOZ}
The results on MultiWOZ are shown in Table \ref{tab:res_multiwoz}. 
Following~\citet{chen-etal-2019-semantically}, Entity F1~\cite{wen2016network} is used to evaluate the entity coverage accuracy (including all slot values, days, numbers, and reference, \etc).
Again, SC-GPT achieves the best performance on BLEU score. Note that GPT-2 performs similarly with SC-GPT on the full MultiWOZ dataset, this is because MultiWOZ contains 57k utterances, which is large enough for GPT-2 to achieve good performance. 
The results also confirm that with enough annotated data, conditional language model formulation performs significantly better than HDSA, a strong competitor that leverages graph/tree-structure information to encode dialog acts. 

To study how SC-GPT performs with different training data sizes. We further conduct experiments with varying percentages of training data on MultiWOZ, ranging from 0.1\% (50 examples) to 50\%. As shown in Table \ref{tab:res_multiwoz_pct}, the observations are consistent with \data{}. SC-GPT performs consistently better than GPT-2, HDSA, and SC-LSTM for a wide range of dataset sizes, and the improvement is more substantial when the fewer numbers of in-domain labels are used for fine-tuning.


Table \ref{tab:human_eval_multiwoz} shows the human assessment results on MultiWOZ. The results are consistent with the automatic evaluation. It is interesting to see that 
 the gap between the new state-of-the-art method (\ie SC-GPT ) and human performance on \data{} (as shown in Table~\ref{tab:human_eval_few}) is much larger than that on MultiWOZ; 
 the human rating on the naturalness of SC-GPT is even higher than humans on MultiWOZ, while there is a visible gap on \data{}. These results demonstrate that \data{} presents a challenging few-shot learning setting, SG-GPT serves as a simple and strong baseline in this setting, and the combined provides a platform for researchers to develop NLG models that are able to generalize to new domains and generate semantically conditioned and fluent responses.


\subsection{Analysis}
\begin{table}[t!]
\footnotesize
\centering
\setlength{\tabcolsep}{1.0mm}{
\begin{tabular}{l cc cc } 
\toprule 
\multirow{2}{*}{Model} &
\multicolumn{2}{c}{} &
\multicolumn{2}{c}{} \\


\cmidrule(l){2-3} \cmidrule(l){4-5} 

&  &   
&  &  \\
\midrule
SC-LSTM & 
23.05 & 40.82 & 12.83 & 51.98 \\
GPT-2 &
30.43 & 3.26 & 27.92 & 17.36 \\
SC-GPT  &
 \textbf{40.28} & \textbf{1.09} & 
\textbf{36.69} & \textbf{4.96} \\
\bottomrule 
\end{tabular}
}
\caption{Performance of different methods on seen DAs and unseen DAs in restaurant domain.} 
\label{tab:res_seen_unseen}
\end{table}

We perform detailed analysis to investigate  SG-GPT's \textit{flexibility}, \textit{controllability} and \textit{generalizability}. 
The test set is split into two subsets - \textit{seen} and \textit{unseen}. If a dialog act of an example appears in the training set, the example is marked as \textit{seen}; otherwise, it is marked as \textit{unseen}. Table \ref{tab:res_seen_unseen} compares different models on the seen and unseen subsets in the  domain. SC-GPT yields higher BLEU and lower ERR, and the improvement is more significant on the unseen set. 
For example, SC-GPT reduces ERR to 4.96, an order of magnitude lower than SC-LSTM and only 1/3 of GPT-2. This demonstrates that SC-GPT generalizes well to novel dialog acts, and is able to precisely ground in them to compose fluent responses.
This is further confirmed by the quantitative comparison in Table \ref{tab:examples},  where we compare the generated utterance examples of different models. While the baseline methods prone to over-generate or miss important slots, SC-GPT can successfully generate fluent natural language utterances that share precise semantic conditions with the ground-truth references.







\begin{table*}[t!]\centering
\begin{minipage}{15.6cm}\vspace{0mm}    \centering
\begin{tcolorbox} 
    \centering
    \scriptsize
     \hspace{-6mm}
    \begin{tabular}{ccp{0.83\columnwidth}}
\# & \textbf{Model} & \textbf{Generated Responses from Different Models} \\
    \midrule
    \ & \textit{\textbf{Original DA}} & ack\_makereservation ( price = moderate ; restaurantcusine = chinese ; location = center ; peoplecount = 2 ) \2.5pt]
    \midrule
    1 & \textit{\textbf{Input DA}} & ack\_makereservation ( price = moderate ; restaurantcusine = \colorbox{blue!30}{japanese} ; location = \colorbox{blue!30}{bellevue square}; peoplecount = 2 ) \2.5pt]
    \ & SC-GPT & ok making a reservation for a moderate japanese cuisine restaurant in bellevue square for 2 people? \2.5pt]
    \ & GPT-2 & ok making a reservation for 2 people sakura moderate japanese restaurant located bellevue sqare for 10 minutes? \\ [2.5pt]
    \ & SC-GPT & ok making a reservation for sakura, moderate japanese cuisine in bellevue square for 2 people? \2.5pt]
    \ & GPT-2 & ok making reservation for japanese cuisine expensive? \\ [2.5pt]
    \ & SC-GPT &  ok making reservation for expensive japanese cuisine? \2.5pt]
    \ & \textit{\textbf{Reference}} & make reservation for a children friendly restaurant ? \2.5pt]
    \ & GPT-2 & make reservation at sakura child friendly restaurant and make table moderate price? \\ [2.5pt]
    \ & SC-GPT &  make reservation for restaurant sakura moderate price and children friendly restaurant? \2.5pt] \vspace{-3mm}
    \end{tabular}
\end{tcolorbox}
\vspace{-2mm}
\caption{Examples of generated utterances with novel dialog acts. SC-GPT produces better utterances than GPT-2 for with edited dialog acts. Since both models produce similar responses to references for the original dialog act, the results are not shown here.  (Better viewed in color. \colorbox{mygreen!30}{insert a slot}, \colorbox{blue!30}{substitute a slot value}, \colorbox{red!30}{ delete a slot}).}
    \label{tab:new_domain_examples}
\end{minipage}
\vspace{-2mm}
\end{table*}


We further simulate the process when deploying SC-GPT for a new domain, using the examples provided in the RASA dialog toolkit \footnote{https://github.com/RasaHQ/rasa/tree/master\\/examples/restaurantbot}. 
We first fine-tune SC-GPT using a few training examples (only 16 instances in this new domain), and then generate utterances based on novel dialog acts that are unseen in training data, shown in Table \ref{tab:new_domain_examples}.
In practice, it is desirable for an NLG system to deal with an extending domain whose dialog acts change dynamically. We simulate the setting by editing the original input dialog acts, such as inserting or deleting a slot, or substituting a slot value.

Since SC-LSTM is infeasible in the setting of an extending domain, we compare SC-GPT with GPT-2.
Results show that SC-GPT produces better utterances than GPT-2. 
SC-GPT can generate reasonably good natural language responses with different combinations of editing operations, showing its high flexibility to generalize to new dialog acts with very limited training data, and produce controllable responses.
\vspace{-2mm}
\section{Conclusion and Future Work}
\vspace{-2mm}
In this paper, we have made two major contributions towards developing a more pragmatic NLG module for task-oriented dialog systems:
 
A new benchmark \data{} is introduced to simulate the few-shot learning scenarios with scarce labelled data in real-world applications. 
 
A new model SC-GPT is proposed to endow the NLG module with strong semantically controlling and generalization ability.
Empirical results on both \data{} and MultiWOZ show that SC-GPT achieves the best overall performance in both automatic and human evaluations.


There are two interesting directions for future work. The first is to design mechanisms to generate more interpersonal responses which are proven to help improve user experiences \citep{li2016diversity,zhou2018design}. 
The other is to generalize the generative pre-training idea to all four modules in the dialog system pipeline for end-to-end training. Since these four modules process information in order, one may organize their input/output as segments, and pre-train a segment-level auto-regressive model.



\bibliography{acl2020}
\bibliographystyle{acl_natbib}

\appendix



\end{document}
