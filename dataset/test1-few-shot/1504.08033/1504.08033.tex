Programs in higher-order languages heavily use function calls and method dispatch for control flow.
Standard flow analyses' imprecise handling of returns damages all specific analyses' precision.
Recent techniques match calls and returns precisely \citep{ianjohnson:vardoulakis-lmcs11, dvanhorn:Earl2010Pushdown} and build smaller models more quickly than a standard \zcfa (evaluation predicted 2-5 times more constant bindings).
These works, called CFA2 and PDCFA respectively, use pushdown automata as their approximation's target model of computation.
They are hence called ``pushdown analyses.''\footnote{I refer to finite model analyses as ``regular analyses'' after the regular languages of traces they realize.}
CFA2 and PDCFA have difficult details to easily apply to an off-the-shelf semantics---especially if they feature non-local control transfer that breaks the pushdown model.


The AAM method we discussed in \autoref{chap:aam} and \autoref{chap:oaam} is a process to construct \emph{regular} analyses.
This chapter describes a systematic process to construct \emph{pushdown} analyses of programming languages, due to the precision benefits.

\section{Tradeoffs of approximation strength}

Static analysis is the process of soundly predicting properties of
programs.
It necessarily involves a tradeoff between the precision of those
predictions and the computational complexity of producing them.
At one end of the spectrum, an analysis may predict nothing, using no
resources.  At the other end, an analysis may predict everything, at
the cost of computability.


Abstract interpretation~\cite{dvanhorn:Cousot:1977:AI} is a form of
static analysis that involves the \emph{approximate} running of a
program by interpreting a program over an abstraction of the program's
values, e.g. by using intervals in place of
integers~\cite{Cousot-TASE07tutorial}, or types instead of
values~\cite{dvanhorn:esop:kmf07}.
By considering the sound abstract interpretation of a program, it is
possible to predict the behavior of concretely running the program. 
For example, if abstract running a program never causes a
buffer-overflow, run-time type error, or null-pointer dereference, we
can conclude actually running the program can never cause any of these
errors either.  If a fragment of code is not executed during the
abstract running, it can safely be deemed dead-code and removed.  More
fine-grained properties can be predicted too; to enable inlining, the
abstract running of a program can identify all of the functions that
are called exactly once and the corresponding call-site.  Temporal
properties can be discovered as well: perhaps we want to determine if
one function is always called before another, or if reads from a file
occur within the opening and closing of it.


In general, we can model the abstract running of a program by
considering each program state as a node in a graph, and track
evolution steps as edges, where each node and path through the graph
is an \emph{approximation} of concrete program behavior.
The art and science of static analysis design is the way we represent this graph of states; how little or how much detail we choose to represent in each state determines the precision and, often, the \emph{cost} of such an analysis.
First-order data-structures, numbers, arrays all have an abundance of
literature for precise and effective approximations, so this paper
focuses on higher-order data: closures and continuations, and their
interaction with state evolution.

A major issue with designing a higher-order abstract interpreter is
approximating closures and continuations in such a way that the
interpreter always terminates while still producing sound and precise
approximations.  Traditionally, both have been approximated by finite
sets, but in the case of continuations, this means the control stack
of the abstract interpreter is modeled as a finite graph and
therefore cannot be precise with regards to function calls and
returns.

\paragraph{Why pushdown return flow matters: an example}
Higher-order programs often create proxies, or monitors, to ensure an object or function interacts with another object or function in a sanitized way.
One example of this is behavioral contracts~\citep{dvanhorn:Findler2002Contracts}.
Simplified, here is how one might write an ad-hoc contract monitor for
a given function and predicates for its inputs and outputs:
\begin{center}
\ifpdf
  \includegraphics[scale=0.45]{monitor}
\else
  \includegraphics[scale=0.45]{monitor.eps}
\fi
 \end{center}

It is well known that wrapping functions like this thwarts the
precision of regular \zcfa{} and higher \kcfa{} as more wrappings are
introduced.
In the case of this innocent program
\begin{center}
\ifpdf
  \includegraphics[scale=0.45]{pair}
\else
  \includegraphics[scale=0.45]{pair.eps}
\fi
 \end{center}

according to \zcfa{} the call to the wrapped \texttt{factorial}
function within the second \texttt{map} may return to within the
first \texttt{map}.  Hence \zcfa{} is not sufficiently precise to 
prove \texttt{factorial} cannot be blamed.
Using more a context-sensitive analysis such as 1CFA, 2CFA, etc.,
would solve the problem for this example, but would fail for nested
proxies.
In general, for any , \kcfa{} will confuse the return flow of some
programs as in this example.
Yet, a pushdown abstraction that properly matches calls and returns
has no trouble with this example, regardless of proxy-nesting depth.

\paragraph{A systematic approach to pushdown analysis}

At this point, several pushdown analyses for higher-order languages
have been developed~\cite{dvanhorn:Vardoulakis2011CFA2,
dvanhorn:Earl2010Pushdown}, and the basic idea is simple: instead of
approximating a program with a finite state machine, use a pushdown
automata.  The control stack of the automata models the control stack
of the concrete interpreter, while stack frames, which contain
closures, are subject to the same abstraction as values in the
program.

This approach works well for simple languages which obey the stack
discipline of a PDA.  But most languages provide features that
transgress that discipline, such as garbage collection, first-class
control operators, stack inspection, and so on.  Some of these
features have been successfully combined with pushdown analysis, but
required technical innovation and
effort~\cite{dvanhorn:Vardoulakis2011Pushdown,
ianjohnson:DBLP:journals/jfp/JohnsonSEMH14,
dvanhorn:Earl2012Introspective}.  To avoid further one-off efforts, we
develop a general technique for creating pushdown analyses for
languages with control operators and reflective mechanisms.

\section{Refinement of AAM for exact stacks}\label{sec:pushdown}
We can exactly represent the stack in the  machine with a modified allocation scheme for stacks.
The key idea is that if the address is ``precise enough,'' then every path that leads to the allocation will proceed exactly the same way until the address is dereferenced.


\paragraph{``Precise enough'':}
For the  machine, every function evaluates the same way, regardless of the stack.
We should then represent the stack addresses as the components of a function call.
The one place in the  machine that continuations are allocated is at  evaluation.
The expression itself, the environment, the store and the timestamp are necessary components for evaluating , so then we just represent the stack address as those four things.
The stack is not relevant for its evaluation, so we do not want to store the stack addresses in the same store -- that would also lead to a recursive store structure.
I call this new table , because it looks like a stack.


By not storing the continuations in the value store, we separate ``relevant'' components from ``irrelevant'' components.
We split the stack store from the value store and use only the value store in stack addresses.
Stack addresses generally describe the relevant context that lead to their allocation, so we will refer to them henceforth as \emph{contexts}.
The resulting state space is updated here:
  

The semantics is modified slightly in \autoref{fig:ceskkstart-semantics} to use  instead of  for continuation allocation and lookup.
Given finite allocation, contexts are drawn from a finite space, but are still precise enough to describe an unbounded stack: they hold all the relevant components to find which stacks are possible.
The computed  relation thus represents the full description of a pushdown system of reachable states (and the set of paths).
Of course this semantics does not always define a pushdown system since  can have an unbounded codomain.
The correctness claim is therefore a correspondence between the same machine but with an unbounded stack, no , and  functions that behave the same disregarding the different representations (a reasonable assumption).

\begin{figure}
  \centering
   \quad  \quad  \\
  \begin{tabular}{r|l}
    \hline\vspace{-3mm}\\
    
    &
     if 
    \\

    &
     \\
    where &  \\
          & 
    \\

    &
    
    \\

    &
     if  \\
    where &  \\
          & 
  \end{tabular}
  \caption{ semantics}
  \label{fig:ceskkstart-semantics}
\end{figure}

\subsection{Correctness}

The high level argument for correctness exploits properties of both machines.
Where the stack is unbounded (call this ), if every state in a trace shares a common tail in their continuations, that tail is \emph{irrelevant}.
This means the tail can be replaced with anything and still produce a valid trace.
This property is more generally, ``context irrelevance.''
The  machine maintains an invariant on  that says that  represents a trace in  that starts at the base of  and reaches  with  on top.
We can use this invariant and context irrelevance to translate steps in the  machine into steps in .
The other way around, we use a proposition that a full stack is represented by  via unrolling and follow a simple simulation argument.

The common tail proposition we will call  and the replacement function we will call ; they both have obvious inductive and recursive definitions respectively.
The invariant is stated with respect to the entire program, :
\begin{mathpar}
  \inferrule{ }{\invmktab(\bot)} \quad
  \inferrule{\invmktab(\mktab) \\
      \forall \makont_c \in K. \startstate(\makont_c) \stepto_\CESKt^* \tpl{\mexpr_c,\menv_c,\mstore_c,\append{\makont_c}{\epsilon}}_{\mtime_c}}
            {\invmktab(\extm{\mktab}{\tpl{\mexpr_c,\menv_c,\mstore_c}_{\mtime_c}}{K})} \\

  \inferrule{
\startstate(\makont) \stepto_\CESKt^* \tpl{\mexpr,\menv,\mstore,\append{\makont}{\epsilon}}_\mtime \\
\invmktab(\mktab)}
    {\inv(\tpl{\mexpr,\menv,\mstore,\makont}_\mtime,\mktab)}
  \end{mathpar}
where

We use  to treat  like  and construct a continuation in  rather than .
\begin{lemma}[Context irrelevance]\label{lem:irrelevance}
  For all traces  and continuations  such that , for any ,  is a valid trace.
\end{lemma}
\begin{proof}
  Simple induction on  and cases on .
\end{proof}
\begin{lemma}[ Invariant]\label{lem:invariant}
  For all , if  and , then 
\end{lemma}
\begin{proof}
  Routine case analysis.
\end{proof}
Note that the injection of  into , , trivially satisfies .

The unrolling proposition is the following
\begin{mathpar}
  \inferrule{ }{\epsilon \in \unroll{\mktab}{\epsilon}} \quad
  \inferrule{\makont \in \mktab(\mctx),
             \mkont \in \unroll{\mktab}{\makont}}
            {\kcons{\mkframe}{\mkont} \in \unroll{\mktab}{\kcons{\mkframe}{\mctx}}}
\end{mathpar}
\begin{theorem}[Correctness]\label{thm:pushdown-correct}
  For all expressions ,
  \begin{itemize}
  \item{{\bf Soundness: } if ,
,
        and , then
        there are  such that
         and }
  \item{{\bf Local completeness:} if 
      and ,
      for all , if  then
      there is a  such that
       and
       .}
  \end{itemize}
\end{theorem}

The completeness result is ``local'' because it only applies to trace slices, and not entire traces - some starts of traces may not be reachable.
As a mode of running, however, there will be no spuriously added states due to the short-circuiting via the memo-use rule.
I conjecture full completeness (all traces with  are reachable traces in the stack model) is attainable by adding the calling expression to the representation of a context.
By adding the calling expression, there should be an invariant that the range of  is always singleton sets.
Thanks to Jens Nicolay for pointing out the incompleteness for traces in the concrete.

\paragraph{Revisiting the example}

First we consider what \zcfa{} gives us, to see where pushdown analysis improves.
The important difference is that in \kcfa{}, return points are stored in an address that is linked to the textual location of the function call, plus a -bounded amount of calling history.
So, considering the common , the unknown function call within map (either \texttt{render{-}int} or \texttt{fact}) returns from the context of the second call to \texttt{map} to the context of the first call to \texttt{map}.
Non-tail calls aren't safe from imprecise return flow: the recursive call to \texttt{map} returns directly to both calls in the outer \texttt{cons}.
All nonsense.


In our presentation, return points are stored in an address that represents the \emph{exact} calling context with respect to the abstract machine's components.
This means when there is a ``merging'' of return points, it really means that two places in the program have requested the exact same thing of a function, even with the same global values.
The function \emph{will} return to both places.
The predicted control flow in the example is as one would expect, or \emph{hope}, an analysis would predict: the correct flow.

\subsection{Engineered semantics for efficiency}\label{sec:eng-frontier}
I cover three optimizations that may be employed to accelerate the fixed-point computation.
\begin{enumerate}
\item{\label{item:chunk}Continuations can be ``chunked'' more coarsely at function boundaries instead of at each frame in order to minimize table lookups.}
\item{We can globalize  with no loss in precision, unlike a global store;
      it will not need to be stored in the frontier but will need to be tracked by seen states.
The seen states only need comparison, and a global  increases monotonically, so we can use Shivers' timestamp technique~\citep{ianjohnson:Shivers:1991:CFA}.
The timestamp technique does not store an entire  in the seen set at each state, but rather how many times the global  has increased.}
\item{Since evaluation is the same regardless of the stack, we can memoize results to short-circuit to the answer.
      The irrelevance of the stack then precludes the need for timestamping the global .}
\end{enumerate}
This last optimization will be covered in more detail in \autoref{sec:memo}.
From here on, this chapter will not explicitly mention timestamps.

A secondary motivation for the representation change in \ref{item:chunk} is that flow analyses commonly split control-flow graphs at function call boundaries to enable the combination of intra- and inter-procedural analyses.
In an abstract machine, this split looks like installing a continuation prompt at function calls.
We borrow a representation from literature on delimited continuations~\citep{ianjohnson:Biernacki2006274} to split the continuation into two components: the continuation and meta-continuation.
Our delimiters are special since each continuation ``chunk'' until the next prompt has bounded length.
The bound is roughly the deepest nesting depth of an expression in functions' bodies.
Instead of ``continuation'' and ``meta-continuation'' then, I will use terminology from CFA2 and call the top chunk a ``local continuation,'' and the rest the ``continuation.''\sidefootnote{Since the continuation is either  or a context, CFA2 calls these ``entries'' to mean execution entry into the program () or a function (). One can also understand these as entries in a table (). I stay with the ``continuation'' nomenclature because they represent full continuations.}


\autoref{fig:pushdown-vis} has a visualization of a hypothetical state space.
Reduction relations can be thought of as graphs: each state is a node, and if a state  reduces to , then there is an edge .
We can also view our various environments that contain pointers (addresses, contexts) as graphs: each pointer is a node, and if the pointer  references an object  that contains another pointer , then there is a labeled edge .
States' contexts point into  to associate each state with a \emph{regular language} of continuations.
The reversed  graph can be read as a collection of finite state machines that accepts all the continuations that are possible at each state that the reversed pointers lead to.
The  continuation is this graph's starting state.

\begin{figure}
  \centering
  \begin{tabular}{rlrl}
     &\hspace{-3mm}&  &\hspace{-3mm} \\
    & &  &\hspace{-3mm}
  \end{tabular}
  \caption{ semantic spaces}
  \label{fig:pushdown-spaces}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[scale=0.65]{xigraph-cloud}
  \caption{Graph visualization of states and }
  \label{fig:pushdown-vis}
\end{figure}

The resulting shuffling of the semantics to accommodate this new representation is in \autoref{fig:cesikkstar-semantics}.
The extension to  happens in a different rule -- function entry -- so the shape of the context changes to hold the function, argument, and store.
We have a choice of whether to introduce an administrative step to dereference  once  is empty, or to use a helper metafunction to describe a ``pop'' of both  and .
Suppose we choose the second because the resulting semantics has a 1-to-1 correspondence with the previous semantics.
A first attempt might land us here:

However, tail calls make the dereferenced  lead to .
Because abstraction makes the store grow monotonically in a finite space, it's possible that  and a naive recursive definition of  will diverge chasing these contexts.
Now  must save all the contexts it dereferences in order to guard against divergence.
So  where


In practice, one would not expect  to grow very large.
Had we chosen the first strategy, the issue of divergence is delegated to the machinery from the fixed-point computation.\footnote{CFA2 employs the first strategy and calls it ``transitive summaries.''}
However, when adding the administrative state, the ``seen'' check requires searching a far larger set than we would expect  to be.

\begin{figure}
  \centering
   \quad  \\
  \begin{tabular}{r|l}
    \hline\vspace{-3mm}\\
    
    &
     if 
    \\

    &
    
    \\

    &
     \\
    &
    if  \\

    &
     \\
    & if  \\
    where &  \\
    &  \\
    & 
  \end{tabular} \\
  \caption{ semantics}
  \label{fig:cesikkstar-semantics}
\end{figure}

We run the the stepping relation along all nondeterministic paths.
The continuation table can be global and use the same counting mechanism we used for the global stores in \autoref{chap:oaam}, without loss of precision.
For ease of exposition, I will keep a map from state without  to largest  at which it has been seen.
The fixed point computation thus computes over the following system:


We compute all next steps from the frontier, combine all changes to , and continue with a new frontier of states we stepped to that we haven't seen at the current .


For a program , we will say  is the bottom element of 's domain.
The ``analysis'' then is then the pair of the  and  components of .

\paragraph{Correctness} The correctness argument for this semantics is not about single steps but instead about the entire relation that  computes.
The argument is that the  and  components of the system represent a slice of the unbounded relation  (restricted to reachable states).
We will show that traces in any  times we \emph{unfold}  from the initial state, there is a corresponding  applications of  that reify into a relation that exhibit the same trace.
Conversely, any trace in the reification of  has the same trace in some  unfoldings of .
For an arbitrary  function, we cannot expect  to have a fixed point, so this property is the best we can get.
For a finite  function, Kleene's fixed point theorem dictates there is a  such that  is a fixed point, so every trace in the reified relation is also a trace in an unbounded number of unfoldings of .
This is the corresponding local completeness argument for the algorithm.


The reification simply realizes all possible complete continuations that a state could have, given :
\begin{mathpar}
  \inferrule{
  \tpl{\tpl{\mexpr,\menv,\mstore,\mlkont,\makont},
      \tpl{\mexpr',\menv',\mstore',\mlkont',\makont'}} \in R \\
  \mkont \in \unrollp{\mktab}{\makont}}
  {\tpl{\mexpr,\menv,\mstore,\append{\mlkont}{\mkont}} \stepto_{\reify(S,R,F,\mktab)}
   \tpl{\mexpr',\menv',\mstore',\append{\mlkont'}{\mkont}}}
\end{mathpar}
The  judgment is like , but with prepending of local continuations:
\begin{mathpar}
  \inferrule{ }{\epsilon \in \unrollp{\mktab}{\epsilon}} \quad
  \inferrule{(\mlkont,\makont) \in \mktab(\mctx) \\
             \mkont \in \unrollp{\mktab}{\makont}}
            {\append{\mlkont}{\mkont} \in \unrollp{\mktab}{\mctx}}
\end{mathpar}

\begin{theorem}[Correctness]\label{thm:global-pushdown}
  For all , let  in
  :
  \begin{itemize}
  \item{if  then
      there is an  such that \\
      }
  \item{if  then
      there is an  such that  is in }
  \end{itemize}
\end{theorem}
\begin{proof}
  By induction on .
\end{proof}

\subsection{Remarks about cost}

The common tradeoff for performance over precision is to use a global store.
A representation win originally exploited by Shivers~\citep{ianjohnson:Shivers:1991:CFA} is to represent the seen states' stores by the \emph{age} of the store.
A context in this case contains the store age for faster comparison.
Old stores are mostly useless since the current one subsumes them, so a useful representation for the seen set is as a map from the \emph{rest of the state} to the store age it was last visited with.
We will align with the analysis literature and call these ``rest of state'' objects \emph{points}.
Note that since the store age becomes part of the state representation due to ``context,'' there are considerably more points than in the comparable finite state approach.
When we revisit a state because the store age (or  age) is different from the last time we visited it (hence we're visiting a \emph{new state}), we can clobber the old ages.
A finite state approach will use less memory because the seen set will have a smaller domain (fewer distinctions made because of the lack of a ``context'' component).


\section{Stack inspection and recursive metafunctions}\label{sec:inspection}

Since we just showed how to produce a pushdown system from an abstract machine, some readers may be concerned that we have lost the ability to reason about the stack as a whole.
This is not the case.
The semantics may still refer to  to make judgments about the possible stacks that can be realized at each state.
A metafunction in the semantics that operates over a whole stack can be recast as a transition system that we overapproximate and run to fixed point using the AAM methodology.

Some semantic features allow a language to inspect some arbitrarily deep part of the stack, or compute a property of the whole stack before continuing.
Java's access control security features are an example of the first form of inspection, and garbage collection is an example of the second.
I will demonstrate both forms are simple first-order metafunctions that the AAM methodology will soundly interpret.
Access control can be modeled with continuation marks, so I demonstrate with the CM machine of \citeauthor{dvanhorn:Clements2004Tailrecursive}.

Semantics that inspect the stack do so with metafunction calls that recur down the stack.
Recursive metafunctions have a semantics as well, hence fair game for AAM.
And, they should always terminate (otherwise the semantics is hosed).
We can think of a simple pattern-matching recursive function as a set of rewrite rules that apply repeatedly until it reaches a result.
Interpreted via AAM, non-deterministic metafunction evaluation leads to a set of possible results.


The finite restriction on the state space carries over to metafunction inputs, so we can always detect infinite loops that abstraction may have introduced and bail out of that execution path.
Specifically, a metafunction call can be seen as an initial state, , that will evaluate through the metafunction's rewrite rules  to compute all terminal states (outputs):
2pt]
  \text{where } \terminalaux(S, \emptyset, T) &= T \\
   \terminalaux(S, F, T) &= \terminalaux(S\cup F, F', T\cup T') \\
   & \text{where } T' = \bigcup\limits_{s \in F}{\post(s) \deceq\emptyset \to \set{s},\emptyset} \\
             &\phantom{\text{where }} F' = \bigcup\limits_{s\in F}{\post(s)} \setminus S \\
                   \post(s) &= \setbuild{s'}{s \stepto s'}

  \kontlive &: \Frame^* \to \wp(\Addr) \\
  \kontlive(\mkont) &= \kontliveaux(\mkont,\emptyset) \

When interpreted via AAM, the continuation is indirected through  and leads to multiple results, and possibly loops through .
Thus this is more properly understood as
2pt]
  \kontliveaux(\mktab,\epsilon,L) &\stepto L \\
  \kontliveaux(\mktab,\kcons{\mkframe}{\mctx}, L) &\stepto \kontliveaux(\mktab,\makont, L\cup\touches(\mkframe)) \text{ if } \makont \in \mktab(\mctx)

  \reaches(\mathit{root},\mstore) &=
 \setbuild{\maddralt}{\maddr \in \mathit{root}, \maddr \leadsto_\mstore^* \maddralt} \\
&  \infer{\mval \in \mstore(\maddr) \\ \maddralt \in \touches(\mval)}{\maddr \leadsto_\mstore \maddralt}

  \Gamma^*(\mastate,\mktab) &=
    \setbuild{\mastate\set{\mstore:=\mastate.\mstore|_L}}{L \in \live^*(\mastate,\mktab)} \\
  \live^*(\tpl{\mexpr,\menv,\mstore,\makont},\mktab) &=
    \setbuild{\reaches(\touches(\mexpr,\menv) \cup L, \mstore)}{L \in \kontlive(\mktab,\makont)}

  \infer{\mastate,\mktab \stepto \mastate',\mktab' \\
         \mastate' \in \Gamma^*(\mstate',\mktab')}
        {\mastate,\mktab \stepto_{\Gamma^*} \mastate',\mktab}

  \hat\Gamma(\mastate,\mktab) &=
  \mastate\set{\mstore:=\mastate.\mstore|_{\widehat{\live}(\mastate,\mktab)}} \\
  \widehat{\live}(\tpl{\mexpr,\menv,\mstore,\makont},\mktab) &=
    \reaches(\touches(\mexpr,\menv) \cup \bigcup\kontlive(\mktab,\makont), \mstore)

  \infer{\mastate,\mktab \stepto \mastate',\mktab'}
        {\mastate,\mktab \stepto_{\hat\Gamma} \hat\Gamma(\mstate',\mktab'),\mktab'}

  \Gamma(\mstate) &= \mstate\set{\mstore:=\mstate.\mstore|_{\live(\mstate)}} \\
  \live(\mexpr,\menv,\mstore,\mkont) &= \reaches(\touches(\mexpr,\menv)\cup \kontlive(\mkont), \mstore)

  \infer{\mstate \stepto \mstate'}
        {\mstate \stepto_{\Gamma} \Gamma(\mstate')}  

  \mathit{ctx\text-congruent} &: \forall S. \wp(S \times S) \times \wp(\Kont\times \Kont) \to \mathit{Prop} \\
  \mathit{ctx\text-congruent}(\stepto,\equiv_K) &=
  \forall \mtrace \in S^*, \mkont. \IsTrace(\mtrace,\stepto) \hastail(\mtrace,\mkont) \implies \\
  &\phantom{=} \forall \mkont'. \mkont \equiv_K \mkont' \implies \\
  &\phantom{=} \IsTrace(\replacetail(\mtrace,\mkont,\mkont'),\stepto)

  \mperm \in \Permissions & \text{ a set} \\
  \Expr &::= \ldots \alt \sgrant{\mperm}{\mexpr} \alt \sframe{\mperm}{\mexpr} \alt \stest{\mperm}{\mexpr}{\mexpr}

  \OK(\emptyset,\mkont) &= \mathit{True} \\
  \OK(\mperm,\epsilon^\mpermmap) &= \passp(\mperm,\mpermmap) \\
  \OK(\mperm,\kconsm{\mkframe}{\mpermmap}{\mkont}) &= \passp(\mperm,\mpermmap) \textbf{ and } \OK(\mperm \setminus \mpermmap^{-1}(\Grant), \mkont) \\
  \text{where }\passp(\mperm,\mpermmap) &= \mperm \cap \mpermmap^{-1}(\Deny) \deceq \emptyset

  \mpermmap \in \PermissionMap &= \Permissions \finto \GD \\
  \mgd \in \GD &::= \Grant \alt \Deny \\
  \mkont \in \Kont &::= \epsilon^\mpermmap \alt \kconsm{\mkframe}{\mpermmap}{\mkont}

  \extm{\mpermmap}{\mperm}{\mgd} &= \lambda x. x \decin \mperm \to \mgd, \mpermmap(x) \\
  \extm{\mpermmap}{\overline{\mperm}}{\mgd} &= \lambda x. x \decin \mperm \to \mpermmap(x),\mgd 
  \tpl{\stest{\mperm}{\mexpri0}{\mexpri1}, \menv, \mstore, \makont},\mktab
  &\stepto
  \tpl{\mexpri0,\menv,\mstore, \makont},\mktab \text{ if } \mathit{True} \in \widehat{\OK}(\mktab,\mperm,\makont)
  \\
  &\stepto
  \tpl{\mexpri1,\menv,\mstore,\makont},\mktab \text{ if } \mathit{False} \in \widehat{\OK}(\mktab,\mperm,\makont)

  \widehat{\OK}(\mktab,\mperm,\makont) &= \terminal(\stepto,\widehat{\OK}^*(\mktab,\mperm,\makont)) \

\begin{lemma}[Correctness of ]\label{lem:wide-ok-correct}
  For all ,
  \begin{itemize}
  \item{\textbf{Soundness:} if  then .}
  \item{\textbf{Local completeness:} if  then there is a  such that .}
  \end{itemize}
\end{lemma}
\begin{proof}
  Soundness follows by induction on the unrolling. Local completeness follows by induction on the trace from local completeness in \lemref{lem:term-correct}.
\end{proof}

With this lemma in hand, the correctness proof is almost identical to the core proof of correctness.
\begin{theorem}[Correctness]\label{thm:cm-machine-correct}
  The abstract semantics is sound and locally complete in the same sense as \thmref{thm:pushdown-correct}.
\end{theorem}


\section{Relaxing contexts for delimited continuations}\label{sec:delim}

In \autoref{sec:pushdown} we showed how to get a pushdown abstraction by separating continuations from the value store.
This separation breaks down when continuations themselves become values via first-class control operators.
The glaring issue is that continuations become ``storable'' and relevant to the execution of functions.
But, it was precisely the \emph{irrelevance} that allowed the separation of  and .
Specifically, the store components of continuations become elements of the store's codomain --- a recursion that can lead to an unbounded state space and therefore a non-terminating analysis.
We apply the AAM methodology to cut out the recursion; whenever a continuation is captured to go into the store, we allocate an address to approximate the store component of the continuation.


We introduce a new environment, , that maps these addresses to the stores they represent.
The stores that contain addresses in  are then \emph{open}, and must be paired with  to be \emph{closed}.
This poses the same problem as before with contexts in storable continuations.
Therefore, we give up some precision to regain termination by \emph{flattening} these environments when we capture continuations.
Fresh allocation still maintains the concrete semantics, but we necessarily lose some ability to distinguish contexts in the abstract.


\subsection{Case study of first-class control: shift and reset}
I choose to study {\tt shift} and {\tt reset}~\citep{ianjohnson:danvy:filinski:delim:1990} because delimited continuations have proven useful for implementing web servers~\citep{dvanhorn:Queinnec2004Continuations,jay-communication}, providing processes isolation in operating systems~\citep{dvanhorn:Kiselyov2007Delimited}, representing computational effects~\citep{dvanhorn:Filinski1994Representing}, modularly implementing error-correcting parsers~\citep{dvanhorn:Shivers2011Modular}, and finally undelimited continuations are \emph{pass\'e} for good reason~\citep{ianjohnson:kiselyov:against-callcc}.
Even with all their uses, however, their semantics can yield control-flow possibilities that surprise their users.
A \emph{precise} static analysis that illuminates their behavior is then a valuable tool.


Our concrete test subject is the abstract machine for shift and reset adapted from \citet{ianjohnson:Biernacki2006274} in the ``{\bf ev}al, {\bf co}ntinue'' style in \autoref{fig:shift-reset}.
The figure elides the rules for standard function calls.
The new additions to the state space are a new kind of value, , and a \emph{meta-continuation},  for separating continuations by their different prompts.
Composable continuations are indistinguishable from functions, so even though the meta-continuation is concretely a list of continuations, its conses are notated as function composition: .

\begin{figure}
  \centering
   \\
  \begin{tabular}{r|l}\hline

    &
    
\\

    &
    
\\

    &
    
    \\
    where & 
\\

    &
    
\end{tabular}  
  \caption{Machine semantics for shift/reset}
  \label{fig:shift-reset}
\end{figure}


\subsection{Reformulated with continuation stores}
The machine in \autoref{fig:shift-reset} is transformed now to have three new tables: one for continuations (), one to close stored continuations (), and one for meta-continuations ().
The first is like previous sections, albeit continuations may now have the approximate form that is storable.
The meta-continuation table is more like previous sections because meta-contexts are not storable.
Meta-continuations do not have simple syntactic strategies for bounding their size, so I choose to bound them to size 0.
They could be paired with lists of  bounded at an arbitrary , but I simplify for presentation.

Contexts for continuations are still at function application, but now contain the .
Contexts for meta-continuations are in two places: manual prompt introduction via {\tt reset}, or via continuation invocation.
At continuation capture time, continuation contexts are approximated to remove  and  components.
The different context spaces are thus:



Revisiting the graphical intuitions of the state space, we have now  in states' stores, which represent an \emph{overapproximation} of a set of continuations.
We augment the illustration from \autoref{fig:pushdown-vis} in \autoref{fig:shiftreset-vis} to include the new  and the overapproximating behavior of .
The informal notation  suggests that the state's store \emph{contains}, or \emph{refers to} some .

\begin{figure}
  \centering
  \includegraphics[scale=0.6]{xigraph-approx}
  \caption{Graphical visualization of states,  and .}
  \label{fig:shiftreset-vis}
\end{figure}

\begin{figure}
  \centering
  \begin{tabular}{rlrl}
     &\multicolumn{3}{l}{\hspace{-3mm}} \\
     & \multicolumn{3}{l}{\hspace{-3mm}} \\
     &\multicolumn{3}{l}{\hspace{-3mm}} \\
     &\multicolumn{3}{l}{\hspace{-3mm}} \\
     &\multicolumn{3}{l}{\hspace{-3mm}} \\
     &\hspace{-3mm} &  &\hspace{-3mm} \\
     &\hspace{-3mm} &  &\hspace{-3mm}
  \end{tabular}
  \caption{Shift/reset abstract semantic spaces}
  \label{fig:shiftreset-spaces}
\end{figure}
The approximation and flattening happens in :


The third case is where continuation closures get flattened together.
The fourth case is when an already approximate continuation is approximated: the approximation is inherited.
Approximating the context and allocating the continuation in the store require two addresses, so we relax the specification of  to allow multiple address allocations in this case.

Each of the four rules of the original shift/reset machine has a corresponding rule that we explain piecemeal.
I will use  for steps that do not modify the continuation stores for notational brevity.
We use the above  function in the rule for continuation capture, as modified here.

where


The rule for {\tt reset} stores the continuation and meta-continuation in :


The prompt-popping rule simply dereferences :


The continuation installation rule extends  at the different context:

Again we have a metafunction , but this time to interpret approximated continuations:

Notice that since we flatten s together, we need to compare for containment rather than for equality (in ).
A variant of this semantics with GC is available in the PLT redex models.

\paragraph{Comparison to CPS transform to remove {\tt shift} and {\tt reset}:}{
We lose precision if we use a CPS transform to compile away {\tt shift} and {\tt reset} forms, because variables are treated less precisely than continuations.
Consider the following program and its CPS transform for comparison:
\begin{small}
\begin{SCodeFlow}\begin{RktBlk}\begin{SingleColumn}\RktPn{(}\RktSym{let*}\mbox{\hphantom{\Scribtexttt{x}}}\RktPn{(}\RktPn{[}\RktSym{id}\mbox{\hphantom{\Scribtexttt{x}}}\RktPn{(}\RktSym{}\mbox{\hphantom{\Scribtexttt{x}}}\RktPn{(}\RktSym{x}\RktPn{)}\mbox{\hphantom{\Scribtexttt{x}}}\RktSym{x}\RktPn{)}\RktPn{]}

\mbox{\hphantom{\Scribtexttt{xxxxxxx}}}\RktPn{[}\RktSym{f}\mbox{\hphantom{\Scribtexttt{x}}}\RktPn{(}\RktSym{}\mbox{\hphantom{\Scribtexttt{x}}}\RktPn{(}\RktSym{y}\RktPn{)}\mbox{\hphantom{\Scribtexttt{x}}}\RktPn{(}\RktSym{shift}\mbox{\hphantom{\Scribtexttt{x}}}\RktSym{k}\mbox{\hphantom{\Scribtexttt{x}}}\RktPn{(}\RktSym{k}\mbox{\hphantom{\Scribtexttt{x}}}\RktPn{(}\RktSym{k}\mbox{\hphantom{\Scribtexttt{x}}}\RktSym{y}\RktPn{)}\RktPn{)}\RktPn{)}\RktPn{)}\RktPn{]}

\mbox{\hphantom{\Scribtexttt{xxxxxxx}}}\RktPn{[}\RktSym{g}\mbox{\hphantom{\Scribtexttt{x}}}\RktPn{(}\RktSym{}\mbox{\hphantom{\Scribtexttt{x}}}\RktPn{(}\RktSym{z}\RktPn{)}\mbox{\hphantom{\Scribtexttt{x}}}\RktPn{(}\RktSym{reset}\mbox{\hphantom{\Scribtexttt{x}}}\RktPn{(}\RktSym{id}\mbox{\hphantom{\Scribtexttt{x}}}\RktPn{(}\RktSym{f}\mbox{\hphantom{\Scribtexttt{x}}}\RktSym{z}\RktPn{)}\RktPn{)}\RktPn{)}\RktPn{)}\RktPn{]}\RktPn{)}

\mbox{\hphantom{\Scribtexttt{xx}}}\RktPn{(}\RktSym{}\mbox{\hphantom{\Scribtexttt{x}}}\RktPn{(}\RktSym{g}\mbox{\hphantom{\Scribtexttt{x}}}\RktVal{0}\RktPn{)}\mbox{\hphantom{\Scribtexttt{x}}}\RktPn{(}\RktSym{g}\mbox{\hphantom{\Scribtexttt{x}}}\RktVal{1}\RktPn{)}\RktPn{)}\RktPn{)}\end{SingleColumn}\end{RktBlk}\end{SCodeFlow}

\begin{SCodeFlow}\begin{RktBlk}\begin{SingleColumn}\RktPn{(}\RktSym{let*}\mbox{\hphantom{\Scribtexttt{x}}}\RktPn{(}\RktPn{[}\RktSym{id}\mbox{\hphantom{\Scribtexttt{x}}}\RktPn{(}\RktSym{}\mbox{\hphantom{\Scribtexttt{x}}}\RktPn{(}\RktSym{x}\mbox{\hphantom{\Scribtexttt{x}}}\RktSym{k}\RktPn{)}\mbox{\hphantom{\Scribtexttt{x}}}\RktPn{(}\RktSym{k}\mbox{\hphantom{\Scribtexttt{x}}}\RktSym{x}\RktPn{)}\RktPn{)}\RktPn{]}

\mbox{\hphantom{\Scribtexttt{xxxxxxx}}}\RktPn{[}\RktSym{f}\mbox{\hphantom{\Scribtexttt{x}}}\RktPn{(}\RktSym{}\mbox{\hphantom{\Scribtexttt{x}}}\RktPn{(}\RktSym{y}\mbox{\hphantom{\Scribtexttt{x}}}\RktSym{j}\RktPn{)}\mbox{\hphantom{\Scribtexttt{x}}}\RktPn{(}\RktSym{j}\mbox{\hphantom{\Scribtexttt{x}}}\RktPn{(}\RktSym{j}\mbox{\hphantom{\Scribtexttt{x}}}\RktSym{y}\RktPn{)}\RktPn{)}\RktPn{)}\RktPn{]}

\mbox{\hphantom{\Scribtexttt{xxxxxxx}}}\RktPn{[}\RktSym{g}\mbox{\hphantom{\Scribtexttt{x}}}\RktPn{(}\RktSym{}\mbox{\hphantom{\Scribtexttt{x}}}\RktPn{(}\RktSym{z}\mbox{\hphantom{\Scribtexttt{x}}}\RktSym{h}\RktPn{)}
\\\mbox{\hphantom{\Scribtexttt{xxxxxxxxxxx}}}\RktPn{(}\RktSym{h}\mbox{\hphantom{\Scribtexttt{x}}}\RktPn{(}\RktSym{f}\mbox{\hphantom{\Scribtexttt{x}}}\RktSym{z}\mbox{\hphantom{\Scribtexttt{x}}}\RktPn{(}\RktSym{}\mbox{\hphantom{\Scribtexttt{x}}}\RktPn{(}\RktSym{fv}\RktPn{)}
\\\mbox{\hphantom{\Scribtexttt{xxxxxxxxxxxxxxxxxxxx}}}\RktPn{(}\RktSym{id}\mbox{\hphantom{\Scribtexttt{x}}}\RktSym{fv}\mbox{\hphantom{\Scribtexttt{x}}}\RktPn{(}\RktSym{}\mbox{\hphantom{\Scribtexttt{x}}}\RktPn{(}\RktSym{i}\RktPn{)}\mbox{\hphantom{\Scribtexttt{x}}}\RktSym{i}\RktPn{)}\RktPn{)}\RktPn{)}\RktPn{)}\RktPn{)}\RktPn{)}\RktPn{]}\RktPn{)}

\mbox{\hphantom{\Scribtexttt{xx}}}\RktPn{(}\RktSym{g}\mbox{\hphantom{\Scribtexttt{x}}}\RktVal{0}\mbox{\hphantom{\Scribtexttt{x}}}\RktPn{(}\RktSym{}\mbox{\hphantom{\Scribtexttt{x}}}\RktPn{(}\RktSym{g0v}\RktPn{)}\mbox{\hphantom{\Scribtexttt{x}}}\RktPn{(}\RktSym{g}\mbox{\hphantom{\Scribtexttt{x}}}\RktVal{1}\mbox{\hphantom{\Scribtexttt{x}}}\RktPn{(}\RktSym{}\mbox{\hphantom{\Scribtexttt{x}}}\RktPn{(}\RktSym{g1v}\RktPn{)}\mbox{\hphantom{\Scribtexttt{x}}}\RktPn{(}\RktSym{}\mbox{\hphantom{\Scribtexttt{x}}}\RktSym{g0v}\mbox{\hphantom{\Scribtexttt{x}}}\RktSym{g1v}\RktPn{)}\RktPn{)}\RktPn{)}\RktPn{)}\RktPn{)}\RktPn{)}\end{SingleColumn}\end{RktBlk}\end{SCodeFlow}
\end{small}
The  machine with a monovariant allocation strategy will predict the CPS'd version returns true or false.
In analysis literature, ``monovariant'' means variables get one address, namely themselves.
Our specialized analysis for delimited control will predict the non-CPS'd version returns true.}

\subsection{Correctness}
We impose an order on values since stored continuations are more approximate in the analysis than in :
\begin{mathpar}
  \inferrule{ }{\mval \sqsubseteq_{\mktab,\mmktab} \mval} \quad
  \inferrule{\mkont \sqsubseteq \unroll{\mktab,\mmktab}{\mvkont}}
            {\vcomp{\mkont} \sqsubseteq_{\mktab,\mmktab} \mvkont} \quad
  \inferrule{\forall \mval\in\mstore(\maddr).
             \exists \maval\in\mastore(\maddr).
             \mval \sqsubseteq_{\mktab,\mmktab} \maval}
            {\mstore \sqsubseteq_{\mktab,\mmktab} \mastore} \\
  \inferrule{\mkont \sqsubseteq \unroll{\mktab_{\makont},\mmktab}{\makont} \\
             \mmkont \sqsubseteq \unrollC{\mktab_{\makont},\mktab_{\mamkont},\mmktab}{\mamkont} \\
             \mstore \sqsubseteq_{\mktab_{\makont},\mmktab} \mastore}
            {\ev{\mexpr,\menv,\mstore,\mkont,\mmkont} \sqsubseteq
             \ev{\mexpr,\menv,\mastore, \mmktab,\makont,\mamkont}, \mktab_{\makont}, \mktab_{\mamkont}} \\
  \inferrule{\mval \sqsubseteq_{\mktab_{\makont},\mmktab} \maval \\
             \mkont \sqsubseteq \unroll{\mktab_{\makont},\mmktab}{\makont} \\
             \mmkont \sqsubseteq \unrollC{\mktab_{\makont},\mktab_{\mamkont},\mmktab}{\mamkont} \\
             \mstore \sqsubseteq_{\mktab_{\makont},\mmktab} \mastore}
            {\co{\mkont,\mmkont,\mval,\mstore} \sqsubseteq
             \co{\makont,\mamkont,\maval,\mastore, \mmktab}, \mktab_{\makont}, \mktab_{\mamkont}}
\end{mathpar}
Unrolling differs from the previous sections because the values in frames can be approximate.
Thus, instead of expecting the exact continuation to be in the unrolling, we have a judgment that an unrolling approximates a given continuation in \autoref{fig:cont-order} (note we reuse  from 's definition).

\begin{figure}
  \centering
  \begin{mathpar}
    \inferrule{ }{\apleft{\mexpr,\menv} \sqsubseteq_{\mktab,\mmktab}
      \apleft{\mexpr,\menv}} \quad \inferrule{\mval
      \sqsubseteq_{\mktab,\mmktab}{\maval}}
    {\apright{\mval} \sqsubseteq_{\mktab,\mmktab} \apright{\maval}} \\
    \inferrule{ }{\epsilon \sqsubseteq
      \unroll{\mktab,\mmktab}{\epsilon}} \quad
    \inferrule{\mkframe \sqsubseteq_{\mktab,\mmktab} \makframe \\
      \mkont \sqsubseteq \unroll{\mktab,\mmktab}{\mctx}}
    {\kcons{\mkframe}{\mkont} \sqsubseteq
      \unroll{\mktab,\mmktab}{\kcons{\makframe}{\mctx}}}
    \\
    \inferrule{\makont \in \mktab(\msctx) \quad
      \mkont \sqsubseteq \unroll{\mktab,\mmktab}{\makont}} {\mkont
      \sqsubseteq \unroll{\mktab,\mmktab}{\msctx}}
    \quad
    \inferrule{\msctx \in I(\mktab,\mmktab,\mactx) \quad
      \mkont \sqsubseteq \unroll{\mktab,\mmktab}{\msctx}} {\mkont
      \sqsubseteq \unroll{\mktab,\mmktab}{\mactx}}
    \\
    \inferrule{ }
              {\epsilon \sqsubseteq \unrollC{\mktab_{\makont},\mktab_{\mamkont},\mmktab}{\epsilon}}
    \\
    \inferrule{(\makont,\mamkont) \in \mktab_{\mamkont}(\mmctx) \\
               \mkont \sqsubseteq \unroll{\mktab_{\makont},\mmktab}{\makont} \\
               \mmkont \sqsubseteq \unrollC{\mktab_{\makont},\mktab_{\mamkont},\mmktab}{\mamkont}}
              {\mkapp{\mkont}{\mmkont} \sqsubseteq \unrollC{\mktab_{\makont},\mktab_{\mamkont},\mmktab}{\mmctx}}
  \end{mathpar}
  
  \caption{Order on (meta-)continuations}
\label{fig:cont-order}
\end{figure}
\begin{theorem}[Soundness]\label{thm:delim-sound}
  If , and  then there is  such that  and
.
\end{theorem}

\paragraph{Freshness implies completeness}
The high level proof idea is that fresh allocation separates evaluation into a sequence of bounded length paths that have the same store, but the store only grows and distinguishes contexts such that each continuation and metacontinuation have a unique unrolling.
It is an open question whether the addition of garbage collection preserves completeness.
Each context with the same store will have different expressions in them since expressions can only get smaller until a function call, at which point the store grows.
This forms an order on contexts: smaller store means smaller context, and same store but smaller expression (indeed a subexpression) means a smaller context.
Every entry in each enviroment () will map to a unique element, and the continuation stores will have no circular references (the context in the tail of a continuation is strictly smaller than the context that maps to the continuation).
There can only be one context that  maps to for approximate contexts because of the property of stores in contexts.

We distill these intuitions into an invariant about states that we will then use to prove completeness.
\begin{mathpar}
  \inferrule{\forall \maddr\in\dom(\mastore).\exists \maval. \mastore(\maddr)=\set{\maval}\wedge\maval\preceq_\mmktab\mktab_\makont \\
    \forall \maddr\in\dom(\mmktab).\exists\mastore'.\mmktab(\maddr) =\set{\mastore'}\wedge\mastore'\in\pi_3(\dom(\mktab_\makont)) \\
    \forall \msctx\in\dom(\mktab_\makont).\exists\makont. \mktab_\makont(\msctx) = \set{\makont}\wedge \makont \sqsubset_\mmktab^{\mktab_\makont} \msctx\\
    \forall \mmctx\in\dom(\mktab_\mamkont).\exists\mamkont.\mktab_\mamkont(\mmctx) = \set{\mamkont}\wedge \mamkont \sqsubset \mmctx \\
}{\inv^*(\mastore, \mmktab, \mktab_{\makont}, \mktab_{\mamkont})}
 \\
\inferrule{\inv^*(\mastore,\mmktab,\mktab_\makont,\mktab_\mamkont) \\
           \tpl{\mexpr,\menv,\mastore,\mmktab} \sqsubset \dom(\mktab_\makont) \cup \dom(\mktab_\mamkont) \\
           (\exists \tpl{\mexpr_c,\menv,\mastore,\mmktab} \in \dom(\mktab_\makont)) \implies \mexpr \in \mathit{subexpressions}(\mexpr_c) \\
           \makont \preceq_\mmktab \mktab_\makont \\
           \mamkont \preceq \mktab_\mamkont}
          {\inv_\fresh(\ev{\mexpr,\menv,\mastore,\mmktab,\makont,\mamkont},\mktab_\makont,\mktab_\mamkont)} \\
\inferrule{\inv^*(\mastore,\mmktab,\mktab_\makont,\mktab_\mamkont) \\
           \maval \preceq_\mmktab \mktab_\makont \\
           \makont \preceq_\mmktab \mktab_\makont \\
           \mamkont \preceq \mktab_\mamkont}
          {\inv_\fresh(\co{\makont,\mamkont,\maval,\mastore,\mmktab},\mktab_\makont,\mktab_\mamkont)}
\end{mathpar}
Where the order  states that any contexts in the (meta-)continuation are mapped in the given table.
\begin{mathpar}
  \inferrule{ }{(\mlam,\menv) \preceq_\mmktab \mktab_\makont} \quad
  \inferrule{ }{\epsilon \preceq_\mmktab \mktab_\makont} \quad
  \inferrule{ }{\epsilon \preceq \mktab_\mamkont} \quad
  \inferrule{\msctx \in \dom(\mktab_\makont)}{\msctx\preceq_\mmktab \mktab_\makont} \quad
  \inferrule{\mmctx \in \dom(\mktab_\mamkont)}{\mmctx \preceq \mktab_\mamkont}\\
  \inferrule{\exists\mastore.\mmktab(\maddr)=\set{\mastore} \\
             \exists!\mmktab'.\tpl{\mexpr,\menv,\mastore,\mmktab'} \in\dom(\mktab_\makont)\wedge\mmktab' \sqsubseteq \mmktab
           }
            {\tpl{\mexpr,\menv,\maddr} \preceq_\mmktab \mktab_\makont}
\end{mathpar}
And the order  states that the contexts in the (meta-)continuation are strictly smaller than the given context.
\begin{mathpar}
  \inferrule{ }{\epsilon \sqsubset_\mmktab^{\mktab_{\makont}}} \quad \inferrule{ }{\epsilon \sqsubset \mmctx} \quad \inferrule{\mctx \sqsubset_\mmktab^{\mktab_\makont} \msctx}{\kcons{\mkframe}{\mctx} \sqsubset_\mmktab^{\mktab_\makont} \msctx} \\
  \inferrule{\mexpr' \in \mathit{subexpressions}(\mexpr)}{\tpl{\mexpr',\menv,\mastore,\mmktab} \sqsubset_\mmktab^{\mktab_\makont} \tpl{\mexpr,\menv,\mastore,\mmktab}} \quad
  \inferrule{\mexpr' \in \mathit{subexpressions}(\mexpr)}{\tpl{\mexpr',\menv,\mastore,\mmktab} \sqsubset \tpl{\mexpr,\menv,\mastore,\mmktab}} \\
  \inferrule{\dom(\mastore) \sqsubset \dom(\mastore')}{\tpl{\_,\_,\mastore,\_} \sqsubset_\mmktab^{\mktab_\makont} \tpl{\_,\_,\mastore',\_}} \quad
  \inferrule{\dom(\mastore) \sqsubset \dom(\mastore')}{\tpl{\_,\_,\mastore,\_} \sqsubset \tpl{\_,\_,\mastore',\_}} \\
  \inferrule{\forall \msctx' \in I(\mktab_\makont,\mmktab,\mactx) \\ \msctx' \sqsubset_\mmktab^{\mktab_\mkont} \msctx}{\mactx \sqsubset_\mmktab^{\mktab_\makont} \msctx}
\end{mathpar}

\begin{lemma}[Freshness invariant]\label{lem:fresh-inv}
  If  produces fresh addresses, \\
 and
 then
.
\end{lemma}
\begin{proof}
  By case analysis on the step.
\end{proof}
\begin{theorem}[Complete for fresh allocation]\label{thm:fresh-complete}
  If  produces fresh addresses then the resulting semantics is complete with respect to states satisfying the invariant.
\end{theorem}
\begin{proof}[Proof sketch]
  By case analysis and use of the invariant to exploit the fact the unrollings are unique and the singleton codomains pigeon-hole the possible steps to only concrete ones.
\end{proof}

\section{Short-circuiting via ``summarization''}\label{sec:memo}

All the semantics of previous sections have a performance weakness that many analyses share: unnecessary propagation.
Consider two portions of a program that do not affect one another's behavior.
Both can change the store, and the semantics will be unaware that the changes will not interfere with the other's execution.
The more possible stores there are in execution, the more possible contexts in which a function will be evaluated.
Multiple independent portions of a program may be reused with the same arguments and store contents they depend on, but changes to irrelevant parts of the store lead to redundant computation.
The idea of skipping from a change past several otherwise unchanged states to uses of the change is called ``sparseness'' in the literature~\citep{dvanhorn:Reif1977Symbolic,dvanhorn:Wegman1991Constant,dvanhorn:Oh2012Design}.


Memoization is a specialized instance of sparseness; the base stack may change, but the evaluation of the function does not, so given an already computed result we can jump straight to the answer.
I use the vocabulary of ``relevance'' and ``irrelevance'' so that future work can adopt the ideas of sparseness to reuse contexts in more ways.


Recall the core notion of irrelevance: if we have seen the results of a computation before from a different context, we can reuse them.
The semantic counterpart to this idea is a memo table that we extend when popping and appeal to when about to push.
This simple idea works well with a deterministic semantics, but the nondeterminism of abstraction requires care.
In particular, memo table entries can end up mapping to multiple results, but not all results will be found at the same time.
Note the memo table space:

There are a few ways to deal with multiple results:
\begin{enumerate}
\item{rerun the analysis with the last memo table until the table doesn't change (expensive),}
\item{short-circuit to the answer but also continue evaluating anyway (negates most benefit of short-circuiting), or}
\item{use a frontier-based semantics like in \autoref{sec:eng-frontier} with global  and , taking care to
    \begin{enumerate}
    \item{at memo-use time, still extend  so later memo table extensions will ``flow'' to previous memo table uses, and}
    \item{when  and  are extended at the same context at the same time, also create states that act like the  extension point also returned to the new continuations stored in .}
    \end{enumerate}}
\end{enumerate}

I will only discuss the final approach.
The same result can be achieved with a one-state-at-a-time frontier semantics, but I believe this is cleaner and more parallelizable.
Its second sub-point I will call the ``push/pop rendezvous.''
The rendezvous is necessary because there may be no later push or pop steps that would regularly appeal to either (then extended) table at the same context.
The frontier-based semantics then makes sure these pushes and pops find each other to continue on evaluating.
In pushdown and nested word automata literature, the push to pop short-circuiting step is called a ``summary edge'' or with respect to the entire technique, ``summarization.''
I find the memoization analogy appeals to programmers' and semanticists' operational intuitions.


A second concern for using memo tables is soundness.
Without the completeness property of the semantics, memoized results in, \eg{}, an inexactly GC'd machine, can have dangling addresses since the possible stacks may have grown to include addresses that were previously garbage.
These addresses would not be garbage at first, since they must be mapped in the store for the contexts to coincide, but during the function evaluation the addresses can become garbage.
If they are supposed to then be live, and are used (presumably they are reallocated post-collection), the analysis will miss paths it must explore for soundness.
Thus we generalized context irrelevance to context congruence.


Context congruence is a property of the semantics \emph{without} continuation stores, so there is an additional invariant to that of \autoref{sec:pushdown} for the semantics with  and :  respects context congruence.
Contexts must carry enough information to define an \emph{acceptability} proposition to apply context congruence.
A context abstracts over a set of continuations, so all continuations in this set must be congruent to each other.


Let's abstract a bit from our specific representations with some named concepts.
A context is extendable to a state in the following way:

A result is plugged into a context to create a state in the following way:

So if for  the notion of acceptability is well-behaved,

then we state the invariant on  as follows:
\begin{mathpar}
  \inferrule{ }{\inv_M(\bot)} \\
  \inferrule{\inv_M(\mmemo) \\
    \forall r \in R, \mkont.
    A(\mctx,\mkont) \implies \exists \mtrace\equiv \mathit{extend}(\mctx,\mkont) \stepto_M^* \mathit{plug}(r,\mkont). \hastail(\mtrace,\mkont)}
  {\inv_M(\extm{\mmemo}{\mctx}{R})}
\end{mathpar}
We can prove this invariant with appeals to the context congruence lemma and the  invariant to stitch together the trace.


Inexact GC does \emph{not} respect context congruence for the same reasons it is not complete: some states are spurious due to inequivalent continuations' effect on GC.
This means that some memo table entries will be spurious, and the expected path in the invariant will not exist.
The reason we use unrolled continuations instead of simply  for this (balanced) path is precisely for stack inspection reasons.

 \begin{figure}
   \begin{center}
     
     \begin{tabular}{r|l}
       \hline\vspace{-3mm}\\
       
       &
        \\
       & \quad if , or \\
       &
        \\
       & \quad if  \\
       where &  \\
       & 
       \\
       
       &
        if  \\
       where &  \\
       &  \\
       & 
     \end{tabular}
   \end{center}
   \caption{Important memoization rules}
   \label{fig:memo}
 \end{figure}

The rules in \autoref{fig:memo} are the importantly changed rules from \autoref{sec:pushdown} that short-circuit to memoized results.
The technique looks more like memoization with a  machine, since the memoization points are truly at function call and return boundaries.
The  function would need to also update  if it dereferences through a context, but otherwise the semantics are updated \emph{mutatis mutandis}.



where

\begin{tabular}{rlrlrl}
   &
  \multicolumn{5}{l}{
    \hspace{-3mm}}
\\
    &\hspace{-3mm} &  & \hspace{-3mm} &  & \hspace{-3mm} \\
    &\hspace{-3mm} &  & \hspace{-3mm} & & \\
    &
   \multicolumn{5}{l}{
     \hspace{-3mm}}
   \\ &\multicolumn{5}{l}{\hspace{-3mm}}
 \end{tabular}

The  notation is for projecting out pairs, lifted over sets.
This worklist algorithm describes unambiguously what is meant by ``rendezvous.''
After stepping each state in the frontier, the differences to the  and  tables are collected and then combined in  as calling contexts' continuations matched with their memoized results.


\begin{mathpar}
  \inferrule[Reify]{
  \tpl{\tpl{\mexpr,\menv,\mstore,\makont},
      \tpl{\mexpr',\menv',\mstore',\makont}} \in R \\
  \mkont \in \unroll{\mktab}{\makont}}
  {\tpl{\mexpr,\menv,\mstore,\mkont} \stepto_{\reifyM(S,R,F,\mktab,\mmemo)}
   \tpl{\mexpr',\menv',\mstore',\mkont}}
\\
  \inferrule[Reify]{
  \tpl{\tpl{\mexpr,\menv,\mstore,\kcons{\mkframe}{\mctx}},
      \tpl{\mexpr',\menv',\mstore',\kcons{\mkframe'}{\mctx}}} \in R \\
      \makont \in \mktab(\mctx) \\
      \mkont \in \unroll{\mktab}{\makont}}
  {\tpl{\mexpr,\menv,\mstore,\kcons{\mkframe}{\mkont}} \stepto_{\reifyM(S,R,F,\mktab,\mmemo)}
   \tpl{\mexpr',\menv',\mstore',\kcons{\mkframe'}{\mkont}}}
\\
  \inferrule[Reify+]{
  \tpl{\tpl{\mexpr,\menv,\mstore,\makont},
      \tpl{\mexpr',\menv',\mstore',\kcons{\mkframe}{\tpl{\mexpr,\menv,\mstore}}}} \in R \\
      \mkont \in \unroll{\mktab}{\makont}}
  {\tpl{\mexpr,\menv,\mstore,\mkont} \stepto_{\reifyM(S,R,F,\mktab,\mmemo)}
   \tpl{\mexpr',\menv',\mstore',\kcons{\mkframe}{\mkont}}}
\\
  \inferrule[Reify-]{
  \tpl{\tpl{\mexpr,\menv,\mstore,\kcons{\apright{\mval}}{\mctx}},
      \tpl{\mexpr',\menv',\mstore',\makont'}} \in R \\
    \makont \in \mktab(\mctx) \\
      \mkont \in \unroll{\mktab}{\makont}}
  {\tpl{\mexpr,\menv,\mstore,\mkont} \stepto_{\reifyM(S,R,F,\mktab,\mmemo)}
   \tpl{\mexpr',\menv',\mstore',\mkont}}
\end{mathpar}

\begin{theorem}[Correctness]\label{thm:memo-correct}
  For all , let  in
  :
  \begin{itemize}
  \item{if  then
      there is an  such that \\
      }
  \item{if  then
      there is an  such that  is in }
  \end{itemize}
\end{theorem}

The proof appeals to the invariant on  whose proof involves an additional argument for the short-circuiting step that reconstructs the path from a memoized result using both context congruence and the table invariants.

