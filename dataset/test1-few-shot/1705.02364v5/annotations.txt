[{'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'SNLI', 'Metric': '% Test Accuracy', 'Score': '84.5'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'SNLI', 'Metric': '% Train Accuracy', 'Score': '85.6'}}, {'LEADERBOARD': {'Task': 'Natural Language Inference', 'Dataset': 'SNLI', 'Metric': 'Parameters', 'Score': '40m'}}, {'LEADERBOARD': {'Task': 'Semantic Textual Similarity', 'Dataset': 'SentEval', 'Metric': 'MRPC', 'Score': '76.2/83.1'}}, {'LEADERBOARD': {'Task': 'Semantic Textual Similarity', 'Dataset': 'SentEval', 'Metric': 'SICK-R', 'Score': '0.884'}}, {'LEADERBOARD': {'Task': 'Semantic Textual Similarity', 'Dataset': 'SentEval', 'Metric': 'SICK-E', 'Score': '86.3'}}, {'LEADERBOARD': {'Task': 'Semantic Textual Similarity', 'Dataset': 'SentEval', 'Metric': 'STS', 'Score': '75.8/75.5'}}, {'LEADERBOARD': {'Task': 'Semantic Textual Similarity', 'Dataset': 'MRPC', 'Metric': 'Accuracy', 'Score': '76.2%'}}, {'LEADERBOARD': {'Task': 'Semantic Textual Similarity', 'Dataset': 'MRPC', 'Metric': 'F1', 'Score': '83.1%'}}]
