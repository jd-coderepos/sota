\PassOptionsToPackage{table}{xcolor}
\documentclass{article} \usepackage{iclr2021_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}

\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{mathrsfs}
\usepackage{color}
\def\cc{\color{blue}}
\usepackage[normalem]{ulem}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{pgfplots}
\usepackage{dsfont}
\usepackage{bbm}
\usepackage{tcolorbox}

\usepackage{algorithm}
\usepackage[noend]{algorithmic}

\pgfplotsset{compat=1.17}


\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{cor}{Corollary}
\newtheorem{example}{Example}

\usepackage{array}
\usepackage{arydshln}
\setlength\dashlinedash{0.2pt}
\setlength\dashlinegap{1.5pt}
\setlength\arrayrulewidth{0.3pt}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{float}
\usepackage{titlesec}
\usepackage{capt-of}
\usepackage[table]{xcolor}
\usepackage{booktabs}

\DeclareMathOperator{\sgn}{sign}
\newcommand{\BK}[1]{\textcolor{red}{BK: #1}}


\title{\emph{Multi-Prize Lottery Ticket Hypothesis}: \\Finding Accurate Binary Neural Networks by Pruning A Randomly Weighted Network}


\author{James Diffenderfer \& Bhavya Kailkhura\\
Center for Applied Scientific Computing\\
Lawrence Livermore National Laboratory\\
Livermore, CA 94550, USA \\
\texttt{\{diffenderfer2,kailkhura1\}@llnl.gov}
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
Recently, \cite{frankle2018lottery} demonstrated that randomly-initialized dense networks contain subnetworks that once found can be trained to reach test accuracy comparable to the trained dense network. However, finding these high performing trainable subnetworks is expensive, requiring iterative process of training and pruning weights. 
In this paper, we propose (and prove) a stronger \emph{Multi-Prize Lottery Ticket Hypothesis}:

\emph{A sufficiently over-parameterized neural network with random weights contains several subnetworks (winning tickets) that (a) have comparable accuracy to a dense target network with learned weights (prize 1), (b) do not require any further training to achieve prize 1 (prize 2), and (c) is robust to extreme forms of quantization (i.e., binary weights and/or activation) (prize 3).}

\noindent This provides a new paradigm for learning compact yet highly accurate binary neural networks simply by pruning and quantizing randomly weighted full precision neural networks.
We also propose an algorithm for finding multi-prize tickets (MPTs) and test it by performing a series of experiments on CIFAR-10 and ImageNet datasets. Empirical results indicate that as models grow deeper and wider, multi-prize tickets start to reach similar (and sometimes even higher) test accuracy compared to their significantly larger and full-precision counterparts that have been weight-trained.
Without ever updating the weight values, our MPTs-1/32 not only set new binary weight network state-of-the-art (SOTA) Top-1 accuracy -- 94.8\% on CIFAR-10 and 74.03\% on ImageNet -- but also outperform their full-precision counterparts by 1.78\% and 0.76\%, respectively.
Further, our MPT-1/1 achieves SOTA Top-1 accuracy (91.9\%) for binary neural networks on CIFAR-10.
Code and pre-trained models are available at: \url{https://github.com/chrundle/biprop}.
\end{abstract}


\section{Introduction}

Deep learning (DL) has made a significant breakthroughs in a wide range of applications~\citep{goodfellow2016deep}.
These performance improvements can be attributed to the significant growth in the model size and the availability of massive computational resources to train such models. 
Therefore, these gains have come at the cost of large memory consumption, high inference time, and increased power consumption. 
This not only limits the potential applications where DL can make an impact but also have some serious consequences, such as, (a) generating huge carbon footprint, and (b) creating roadblocks to the democratization of AI. 
Note that significant parameter redundancy and a large number of floating-point operations are key factors incurring the these costs. Thus, for discarding the redundancy from DNNs, one can either (a) \emph{Prune:} remove non-essential connections from an existing dense network, or (b) \emph{Quantize:} constrain the full-precision (FP) weight and activation values to a set of discrete values which allows them to be represented using fewer bits. Further, one can exploit the complementary nature of pruning and quantization to combine their strengths.


Although pruning and quantization\footnote{A detailed discussion on related work on pruning and quantization is provided in Appendix~\ref{sec:relatedwork}.
} are typical approaches used for compressing DNNs~\citep{cheng2017survey}, it is not clear under what conditions and to what extent compression can be achieved without sacrificing the accuracy. The most extreme form of quanitization is binarization, where weights and/or activations can only have two possible values, namely  or  (the interest of this paper). In addition to saving memory, binarization results in more power efficient networks with significant computation acceleration since expensive multiply-accumulate operations (MACs) can be replaced by cheap XNOR and bit-counting operations~\citep{qin2020binary}. In light of these benefits, it is of interest to question if conditions exists such that a binarized DNN can be pruned to achieve accuracy comparable to the dense FP DNN.
More importantly, even if these favourable conditions are met then how do we find these extremely compressed (or compact) and highly accurate subnetworks?



Traditional pruning schemes have shown that a pretrained DNN can be pruned without a significant loss in the performance. Recently, \citep{frankle2018lottery} made a breakthrough by showing that dense network contain sparse subnetworks that can match the performance of the original network when trained from scratch with weights being reset to their initialization (\emph{Lottery Ticket Hypothesis}).
Although the original approach to find these subnetworks still required training the dense network, some efforts~\citep{2wang2020pruning, you2019drawing, 1wang2020picking} have been carried out to overcome this limitation. Recently a more intriguing phenomenon has been reported -- a dense network with random initialization contains subnetworks that achieve high accuracy, without any further training~\citep{zhou2019deconstructing, ramanujan2019whats, malach2020proving, orseau2020logarithmic}. These trends highlight good progress being made towards \emph{efficiently} and \emph{accurately} pruning DNNs.

In contrast to these positive developments for pruning, results on binarizing DNNs have been mostly negative. To the best of our knowledge, post-training schemes have not been successful in binarizing pretrained models without retraining. 
Even with training binary neural networks (BNNs) from scratch (though inefficient), the community has not been able to make BNNs achieve comparable results to their full precision counterparts. The main reason being that network structures and weight optimization techniques are predominantly developed for full precision DNNs and may not be suitable for training BNNs. Thus, closing the gap in accuracy between the full precision and the binarized version may require a paradigm shift. Furthermore, this also makes one wonder if \emph{efficiently} and \emph{accurately} binarizing DNNs similar to the recent trends in pruning is ever feasible.

In this paper, we show that a randomly initialized dense network contains extremely sparse binary subnetworks that without any weight training (i.e., \emph{efficient}) have comparable performance to their trained dense and full-precision counterparts (i.e., \emph{accurate}). Based on this, we state our hypothesis:

\noindent
\begin{tcolorbox}[colframe=black,colback=lightgray!50,boxrule=1pt,boxsep=4pt,left=1pt,right=1pt,top=0pt,bottom=0pt]
\noindent \textbf{Multi-Prize Lottery Ticket Hypothesis.} \emph{A sufficiently over-parameterized neural network with random weights contains several subnetworks (winning tickets) that (a) have comparable accuracy to a dense target network with learned weights (prize 1), (b) do not require any further training to achieve prize 1 (prize 2), and (c) is robust to extreme forms of quantization (i.e., binary weights and/or activation) (prize 3).}
\end{tcolorbox}
\vspace*{-2.5mm}

\iffalse
\noindent\fcolorbox{black}{lightgray!60}{\begin{minipage}{0.98\textwidth}
\noindent \textbf{Multi-Prize Lottery Ticket Hypothesis.} \emph{A sufficiently over-parameterized neural network with random weights contains several subnetworks (winning tickets) that (a) have comparable accuracy to a dense target network with learned weights (prize 1), (b) do not require any further training to achieve prize 1 (prize 2), and (c) is robust to extreme forms of quantization (i.e., binary weights and/or activation) (prize 3).}
\end{minipage}}
\fi

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.77\textwidth]{figures/final-teaser-figure.png}
    \caption{{\bfseries Multi-Prize Ticket Performance}: Multi-prize tickets, obtained only by pruning and binarizing random networks, outperforms trained full precision and SOTA binary weight networks.}
    \label{fig:teaser-figure}
\end{figure}

\paragraph{Contributions.}
First, we propose the multi-prize lottery ticket hypothesis as a new perspective on finding neural networks with drastically reduced memory size, much faster test-time inference and lower power consumption compared to their dense and full-precision counterparts. 
Next, we provide theoretical evidence of the existence of highly accurate binary subnetworks within a randomly weighted DNN (i.e., proving the multi-prize lottery ticket hypothesis). 
Specifically, we mathematically prove that we can find an -approximation of a fully-connected ReLU DNN with width  and depth  using a sparse binary-weight DNN of sufficient width. Our proof indicates that this can be accomplished by pruning and binarizing the weights of a randomly weighted neural network that is a factor  wider and  deeper.
To the best of our knowledge, this is the first theoretical work proving the existence of highly accurate binary subnetworks within a sufficiently overparameterized randomly initialized neural network.
Finally, we provide \textbf{biprop} (\textbf{bi}narize-\textbf{pr}une \textbf{op}timizer) in Algorithm~\ref{alg:quant-opt} to identify MPTs within randomly weighted DNNs and empirically test our hypothesis. This provides a completely new way to learn BNNs without relying on weight-optimization.

\paragraph{Results.}
We explore two variants of multi-prize tickets -- one with binary weights (MPT-1/32) and other with binary weights and activation (MPT-1/1) where  denotes  and  bits to represent weights and activation, respectively. MPTs we find have  fewer parameters than the original network. 
We perform a series of experiments on on small and large scale datasets for image recognition, namely CIFAR-10~\citep{krizhevsky2009learning} and ImageNet~\citep{deng2009imagenet}. 
On CIFAR-10, we test the performance of multi-prize tickets against the trend of making the model deeper and wider. We found that as models grow deeper and wider, both variants of multi-prize tickets start to reach similar (and sometimes even higher) test accuracy compared to the dense and full precision original network with learned weights. 
In other words, the performance of multi-prize tickets improves with the amount of redundancy in the original network.  
We also carry out experiments with state-of-the-art (SOTA) architectures on CIFAR-10 and ImageNet datasets with an aim to investigate their redundancy. We find that within most randomly weighted SOTA DNNs reside extremely compact (i.e., sparse and binary) subnetworks which are smaller than, but match the performance of trained target dense and full precision networks. Furthermore, with minimal hyperparameter tuning, our MPTs achieve Top-1 accuracy comparable to (or higher than) SOTA BNNs.
The performance of MPTs is further improved by allowing the parameters in BatchNorm layer to be learned.
Finally, on both CIFAR-10 and ImageNet, MPT-1/32 subnetworks outperform their significantly larger and full-precision counterparts that have been weight-trained. 




\section{Multi-Prize Lottery Tickets: Theory and Algorithms}
We first prove the existence of MPTs in an overparameterized randomly weighted DNN. For ease of presentation, we state an informal version of Theorem~\ref{thm:bin-subnetwork} which can be found in Appendix~\ref{sec:new-existence-bin-init}. We then explore two variants of tickets (MPT-1/32 and MPT-1/1) and provide an algorithm to find them. 

\subsection{Proving the Multi-Prize Lottery Tickets Hypothesis} \label{sec:proving-mplth}
In this section we seek to answer the following question: \emph{What is the required amount of over-parameterization such that a randomly weighted neural network can be compressed to a sparse binary subnetwork that approximates a dense trained target network?}



\begin{theorem}(Informal Statement of Theorem~\ref{thm:bin-subnetwork}) \label{thm:informal-binary-weight-subnetwork}
Let . For every fully-connected (FC) target network with ReLU activations of depth  and width  with bounded weights, a random binary FC network with ReLU activations of depth  and width  contains with probability  a binary subnetwork that approximates the target network with error at most .
\end{theorem}

\begin{proof}[Sketch of Proof]
Consider a FC ReLU network , where , , , , and . Additionally, consider a FC network with binary weights given by 
, where , , and . Our goal is to determine a lower bound on the depth, , and the widths, , such that with probability  the network  contains a subnetwork  satisfying , for any  and .
We first establish lower bounds on the width of a network of the form  such that with probability  there exists a subnetwork  of  s.t. , for any  and . This process is carried out in detail in Lemmas~\ref{lem:new-bin-init-two-layer}, \ref{lem:bin-init-two-layer-2}, and \ref{lem:bin-init-two-layer-3} in Appendix~\ref{sec:new-existence-bin-init}. We have now approximated a single layer FC real-valued network using a subnetwork of a two-layer FC binary network. Hence, we can take  and Lemma~\ref{lem:bin-init-two-layer-3} provides lower bounds on the width of each intermediate layer such that with probability  there exists a subnetwork  of  satisfying . This is accomplished in Theorem~\ref{thm:bin-subnetwork} in Appendix~\ref{sec:new-existence-bin-init}.
\end{proof}

To the best of our knowledge this is the first theoretical result proving that a sparse binary-weight DNN that can approximate a real-valued target DNN. As it has been established that real-valued DNNs are universal approximators \citep{scarselli1998universal}, our result carries the implication that sparse binary-weight DNNs are also universal approximators. In relation to the first result establishing the existence of real-valued subnetworks in a randomly weighted DNN approximating a real-valued target DNN \citep{malach2020proving}, the lower bound on the width established in Theorem~\ref{thm:bin-subnetwork} is better than their lower bound of . 

\iffalse
To prove the main result, we compose two-layer binary networks of sufficient width together and using Lemma~\ref{lem:bin-init-two-layer-3} together with analysis from \cite{malach2020proving} prove that the resulting FC binary network of depth  is an -approximation of the FC real-valued network of depth .

To motivate and summarize our approach for deriving theoretical lower bounds on, consider a single-layer FC ReLU network , where , , and . If we can approximate this fundamental building block of a FC network then we should be able to approximate an entire FC network. Hence, our first step is to determine a FC network with binary weights, say , such that

for all .

Consider a FC ReLU network with real weights given by 

where , , , , and . Additionally, consider a FC network with binary weights given by 

where , , and . Our goal is to determine a subnetwork of  that approximates . As such, we make use of binary-masks , for , to define a subnetwork of  by
, where  denotes the Hadamard product. Now given , we would like to determine lower bounds on the depth, , and layer widths, , such that

our approach can be summarized as follows.
We first prove that a two-layer binary network of sufficient width contains a subnetwork that approximates a single-layer real-valued network. Then by composing the two-layer binary networks of sufficient width together, we are able to prove that the resulting FC binary network of depth  is an -approximation of the FC real-valued network of depth . \fi



\iffalse
We now provide some necessary notations. We denote a target network with  layers, width , and input dimension  by the parameterized function  with weights . The target network has the form
, where  for , , , and . We use  to denote the larger network of the form
, where , , , , 
, and 
    .
Note that each  is a two-layer FC binary network. Hence,  has twice the depth of the target network, . As in \citep{malach2020proving}, the choice of  is made in order to simplify the analysis but the analysis holds for weight matrices  of arbitrary dimension , for . Finally, note that we write  to denote matrices of dimension  whose components are sampled from a binomial distribution with elements  and .

\begin{theorem} \label{thm:binary-weight-subnetwork}
Let , , , and . Assume that for each  we have  and . Fix . Let  be sampled from ,  be sampled from ,  be sampled from  and  sampled from . If
, then with probability at least  there exist binary masks  and  for  and , respectively, such that for all  with  we have  and 

\end{theorem}

The above theorem  shows that for every target network of depth  with bounded weights, a random network of depth  and  width  contains, with high probability (over the randomness of the weights), a binary subnetwork that approximates the target network. Note that the inclusion of  inside of the binary network is commonly referred to as a \textit{gain term} and can be found in many binary-weight network configurations \citep{rastegari2016xnornet, lin2017towards, martinez2020training, bulat2019xnor}. A detailed proof of Theorem~\ref{thm:binary-weight-subnetwork} can be found in Appendix~\ref{sec:new-existence-bin-init}.
\fi

\subsection{Finding Multi-Prize Winning Tickets} \label{sec:finding-mpts}
Given the existence of multi-prize winning tickets from Theorem~\ref{thm:bin-subnetwork}, a natural question arises -- \emph{How should we find them?}
In this section, we answer this question by introducing an algorithm for finding multi-prize tickets.\footnote{Although our results are derived under certain assumptions (e.g., fully-connected, ReLU neural network approximated by a subnetwork with binary weights), our algorithm is not restricted by these assumptions.} Specifically, we explore two variants of multi-prize tickets in this paper -- 1) MPT-1/32 where weights are quantized to 1-bit with activations being real valued (i.e., 32-bits) and 2) MPT-1/1 where both weights and activations are quantized to 1-bit. We first outline a generic process for identifying MPTs along with some theoretical motivation for our approach.
 
Given a neural network  with weights , we can express a subnetwork of  using a binary mask  as , where  denotes the Hadamard product. Hence, a binary subnetwork can be expressed as  , where . Lemma~\ref{lem:new-bin-init-two-layer} in Appendix~\ref{sec:new-existence-bin-init} indicates that rescaling the binary weights to  using a gain term   is necessary to achieve good performance of the resulting subnetwork. We note that the use of gain terms is common in binary neural networks \citep{qin2020binary,martinez2020training, bulat2019xnor}. Combining all this allows us to represent a binary subnetwork as . 

Now we focus on how to update , , and . Suppose  is a target network with optimized weights  that we wish to approximate. Assuming  is -Lipschitz continuous yields
\small

\normalsize
\noindent Hence, the MPT error is bounded above by the error of the subnetwork of  with the original weights and the error from binarizing the current subnetwork. This informs our approach for identifying MPTs: 1) Update a pruning mask  that reduces the subnetwork error (lines 7 -- 9 in Algorithm~\ref{alg:quant-opt}), and 2) apply binarization with a gain term that minimizes the binarization error (lines 4 and 10).

We first discuss how to update . 
While we could search for  by minimizing the subnetwork error in (\ref{eq:paper-subnetwork-err-bound}), this would require the use of a pretrained target network (i.e., ). To avoid requiring a target network in our method we instead aim to minimize the training loss w.r.t.  in the current binary subnetwork.
Directly optimizing over the pruning mask is a combinatorial problem. So to update the pruning mask efficiently we optimize over a set of scores  corresponding to each randomly initialized weight in the network. In this approach, each component of the randomly initialized weights is assigned a pruning score. The pruning scores are updated via backpropagation by computing the gradient of the loss function over minibatches with respect to the pruning scores (line 7). Then the magnitude of the scores in absolute value are used to identify the  percent of weights in each layer that are least important to the success of the binary subnetwork (line 8). The components of the pruning mask corresponding to these indices are set to  and the remaining components are set to  (line 9). To avoid unintentionally pruning an entire layer of the network, we use a pruning mask for each layer that prunes  percent of the weights in that layer. The choice to use pruning scores to update the mask  was due to the fact that it is computationally efficient. The use of pruning scores is a well-established optimization technique used in a range of applications \citep{boyd2009sensor,ramanujan2019whats}.



We now consider how to update  and . By keeping  fixed, we can derive the following closed form expressions that minimize the binarization error in (\ref{eq:paper-subnetwork-err-bound}):
 and . These closed form expressions indicate that only the gain term needs to be recomputed after each update to . Hence,  throughout our entire approach (line 4). We update a gain term for each layer of the subnetwork in our approach based on the formula for  (line 10). More details on the derivation of  and  are provided in Appendix~\ref{sec:biprop-motivation}. 

Pseudocode for our method \textbf{biprop} (\textbf{bi}narize-\textbf{pr}une \textbf{op}timizer) is provided in Algorithm~\ref{alg:quant-opt} and cross-entropy loss is used in our experiments. Note that the process for identifying MPT-1/32 and MPT-1/1 differs only in computation of the gradient. Next, we explain how these gradients can be computed.  









\subsubsection{Updating Pruning Scores for Binary-Weight Tickets (MPT-1/32)} \label{sec:biprop-1-32}
As an example, for a FC network where the state at each layer is defined recursively by  and  we have . We use the straight-through estimator \citep{bengio2013estimating} for  which yields , where  is computed via backpropagation.







\begin{algorithm}[t!]
\begin{algorithmic}[1] 
\STATE{\textbf{Input}: Neural network  with 1- or 32-bit activations; Network depth ; Layer widths ; Loss function ; Training data ; Pruning percentage .}
\STATE{\textit{Randomly Initialize FP Parameters}:
Network weights ; Pruning scores .}
\STATE{\textit{Initialize Layerwise Pruning Masks}:
 each to .} 
\STATE{\textit{Initialize Binary Subnetwork Weights}: .}
\STATE{\textit{Initialize Layerwise Gain Terms}: .} \vspace{1mm}
\FOR{ to } \vspace{0.5mm} \STATE{ \hfill Update pruning scores at layer } \vspace{1mm}
\STATE{ Sorting of indices  s.t.  \hfill Index sort over values } \STATE{ \hfill Update pruning mask at layer } \vspace{1mm}
\STATE{ \hfill Update gain term at layer } \vspace{1mm}
\ENDFOR
\STATE{\textbf{Output}: Return Binarized Subnetwork .}
\end{algorithmic}
\caption{\textbf{biprop}: Finding multi-prize tickets in a randomly weighted neural network}
\label{alg:quant-opt}
\end{algorithm}



\iffalse
\begin{algorithm}[t!]
\begin{algorithmic} 
\STATE{\textbf{Input}: Neural network  with 1- or 32-bit activations; Pruning percentage ; Loss function ; Training features/labels .}
\STATE{\textit{Randomly Initialize Parameters}:
Network weights ; Pruning scores .}
\STATE{\textit{Init. MPT Parameters}:
Pruning mask ; Binary subnetwork weights ; Gain term .}
\FOR{ to }
    \STATE{ \hfill Update pruning scores} \vspace{1mm}
    \STATE{ Sorting of indices  s.t.  \hfill Nondecreasing sort of } \vspace{1mm}
    \STATE{ \hfill Update pruning mask components} \vspace{1mm}
    \STATE{ \hfill Update binary subnetwork weights} \vspace{1mm}
    \STATE{ \hfill Update gain term} \vspace{1mm}
\ENDFOR
\STATE{\textbf{Output}: Return Binarized Subnetwork .}
\end{algorithmic}
\caption{\textbf{biprop}: Finding multi-prize tickets in a randomly weighted neural network}
\label{alg:quant-opt}
\end{algorithm}
\fi



\subsubsection{Updating Pruning Scores for Binary-Activation Tickets (MPT-1/1)} \label{sec:biprop-1-1}
Note that MPT-1/1 uses the  activation function. From Section~\ref{sec:biprop-1-32}, it immediately follows that . However, updating  via backpropagation requires a gradient estimator for the  activation function. To motivate our choice of estimator note that we can approximate the  function using a quadratic spline parameterized by some :  

In (\ref{eq:quad-spline}),  and suitable values for the coefficients are derived using the following zero- and first-order constraints: , , , , , , and . This yields  and . As  approximates , we can use  as our gradient estimator. Since  and  it follows that . The choice to approximate  using a quadratic spline instead of a cubic spline results in a gradient estimator that can be implemented efficiently in PyTorch as \texttt{torch.clamp(2*(1-torch.abs(x)/t)/t,min=0.0)}. We note that , which suggests that smaller values of  yield more suitable approximations. Our experiments use  as the gradient estimator since we found it to work well in practice. Finally, we note that taking  in our gradient estimator yields the same value as the gradient estimator in \citep{liu2018bi}, however, our implementation in PyTorch is 6 more memory efficient. 



\iffalse

\fi

\section{Experimental Results}

The primary goal of the experiments in  Section~\ref{sec:exp-mplth} is to empirically verify our \textit{Multi-Prize Lottery Ticket Hypothesis}. As a secondary objective, we would like to determine tunable factors that make randomly-initialized networks amenable to containing readily identifiable Multi-Prize Tickets (MPTs). Thus, we test our hypothesis against the general trend of increasing the model size (depth and width) and monitor the accuracy of the identified MPTs. 
After verifying our \textit{Multi-Prize Lottery Ticket Hypothesis}, we consider the performance of MPTs compared to state-of-the-arts in binary neural networks and their dense counterparts on CIFAR-10 and ImageNet datasets in Section~\ref{sec:exp-sota}. 
Building upon {edge-popup}~\citep{ramanujan2019whats}, we implement Algorithm~\ref{alg:quant-opt} to identify MPTs.\footnote{A comparison of MPT-1/32 found using \textbf{biprop} and edgepopup is provided in Appendix~\ref{sec:mpt-vs-edgepopup}, which demonstrates that \textbf{biprop} outperforms edgepopup.}









\subsection{Where can we expect to find multi-prize tickets?} \label{sec:exp-mplth}



In this section, we empirically test the effect of overparameterization on the performance of MPTs. We overparameterize networks by making them (a) deeper (Sec.~\ref{sec:exp-deep}) and (b) wider (Sec.~\ref{sec:exp-wide}).  

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth, trim={6cm 0 6cm 2.5cm}, clip]{figures/prune_experiment_plot.png}
    \caption{{\bfseries Effect of Varying Depth and Pruning Rate}: Comparing the Top-1 accuracy of small and binary MPTs to a large, full-precision, and weight-optimized network on CIFAR-10.}
    \label{fig:prune-plot}
\end{figure}


We use VGG~\citep{simonyan2014very} variants as our network architectures for searching for MPTs. In each randomly weighted network, we find winning tickets MPT-1/32 and MPT-1/1 for different pruning rates using Algorithm~\ref{alg:quant-opt}. We choose our baselines as dense full-precision models with learned weights. In all experiments, we use three independent initializations and report the average of Top-1 accuracy with with error bars extending to the lowest and highest Top-1 accuracy. Additional experiment configuration details are provided in Appendix~\ref{sec:hyperparameters}.


\subsubsection{Do winning tickets exist in deep networks?} \label{sec:exp-deep}

In this experiment, we empirically test the following hypothesis: \emph{As a network grows deeper, the performance of multi-prize tickets in the randomly initialized network will approach the performance of the same network with learned weights. We are further interested in exploring the required network depth for our hypothesis to be true.}

In Figure \ref{fig:prune-plot}, we vary the depth of VGG architectures ( to ) and compare the Top-1 accuracy of MPTs (at different pruning rates) with weight-trained dense network. We notice that there exist a range of pruning rates where the performance of MPTs are very similar, and beyond this range the performance drops quickly. Interestingly, as the network depth increases, more parameters can be pruned without hurting the performance of MPTs. For example, MPT-1/32 can match the performance of trained Conv-8 while having only  of its parameter count. Interestingly, the performance gap between MPT-1/32 and MPT-1/1 does not change much with depth across different pruning rates. 
We further note that the performance of MPTs improve when increasing the depth and both start to approach the performance of the dense model with learned weights. This gain starts to plateau beyond a certain depth, suggesting that the MPTs might be approaching the limit of their achievable accuracy. 
Surprisingly, MPT-1/32 performs equally good (or better) than the weight-trained model regardless of having  lesser parameters and weights being binarized. 

\subsubsection{Do winning tickets exist in wide networks?} \label{sec:exp-wide}
\begin{figure}[!t]
    \centering
    \includegraphics[width=\textwidth, trim={6cm 0 6cm 2.5cm}, clip]{figures/mpt1_32_width_experiment_plot_update.png}
    \caption{{\bfseries Effect of Varying Width on MPT-1/32}: Comparing the Top-1 accuracy of sparse and binary MPT-1/32 to dense, full-precision, and weight-optimized network on CIFAR-10.}
    \label{fig:mpt1-32-plot}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth, trim={6cm 0 6cm 2.5cm}, clip]{figures/mpt1_1_width_experiment_plot_update.png}
    \caption{{\bfseries Effect of Varying Width on MPT-1/1}: Comparing the Top-1 accuracy of sparse and binary MPT-1/1 to dense, full-precision, and weight-optimized network on CIFAR-10.}
    \label{fig:mpt1-1-plot}
\end{figure}
Similar to the previous experiment, in this experiment, we empirically test the following hypothesis: \emph{As a network grows wider, the performance of multi-prize tickets in the randomly initialized network will approach the performance of the same network with learned weights. We are further interested in exploring the required layer width for our hypothesis to be true.}


In Figures~\ref{fig:mpt1-32-plot} and \ref{fig:mpt1-1-plot}, we vary the width of different VGG architectures and compare the Top-1 accuracy of MPT-1/32 and MPT-1/1 tickets (at different pruning rates) with weight-trained dense network. A width multiplier of value  corresponds to the models in Figure~\ref{fig:prune-plot}. Performance of all the models improves when increasing the width   
and the performance of both MPT-1/32 and MPT-1/1 start to approach the performance of the dense model with learned weights. Although, this gain starts to plateau beyond a certain width. For both MPT-1/32 and MPT-1/1, as the width and depth increase the performance at different pruning rates approach the same value. This observed phenomenon yields a more significant gain in the performance for MPTs with higher pruning rates. 
Similar to the previous experiment, the performance of MPT-1/32 matches (or exceeds) the performance of dense models for a large range of pruning rates. Furthermore, in the high width regime, a large number of weights () can be pruned without having a noticeable impact on the performance of MPTs. We also notice that the performance gap between MPT-1/32 and MPT-1/1 decreases significantly with an increase the width which is in sharp contrast with the with the depth experiments where the performance gap between MPT-1/32 and MPT-1/1 appeared to be largely independent of the depth.

\paragraph{Key Takeaways.}
Our experiments verify \textit{Multi-Prize Lottery Ticket Hypothesis} and additionally convey the significance of choosing appropriate network depth and layer width for a given pruning rate. In particular, we find that a network with a large width can be pruned more aggressively without sacrificing much accuracy, while the accuracy of a network with smaller widths suffers when pruning a large percentage of the weights. Similar patterns hold for the depth of the networks as well. The amount of overparametrization needed to approach the performance of dense networks seems to differ for MPT variants -- MPT-1/1 requires higher depth and width compared to MPT-1/32.




\subsection{How Redundant Are State-of-the-Art Deep Neural Networks?} \label{sec:exp-sota}

Having shown that MPTs can perform equally good (or better) than overparameterized networks, this experiment aims to answer: \emph{Are state-of-the-art weight-trained DNNs overparametrized enough that significantly smaller multi-prize tickets can match (or beat) their performance?}




\paragraph{Experimental Configuration.}
Instead of focusing on extremely large DNNs, we experiment with small to moderate size DNNs. Specifically, we analyze the redundancy of following backbone models: (1) VGG-Small and ResNet-18 on CIFAR-10, and (2) WideResNet-34 and WideResNet-50 on ImageNet. As we will show later that even these models are highly redundant, thus, our finding automatically extends to larger models. In this process, we also perform a comprehensive comparison of the performance of our multi-prize winning tickets with state-of-the-art in binary neural networks (BNNs). Details on the experimental configuration are provided in Appendix~\ref{sec:hyperparameters}.


This experiment uses Algorithm~\ref{alg:quant-opt} to find MPTs within randomly initialized backbone networks. We compare the Top-1 accuracy and number of non-zero parameters for our MPT-1/32 and MPT-1/1 tickets with selected baselines in BNNs \citep{qin2020binary}. 
Results for CIFAR-10 and ImageNet are shown in  Tables~\ref{table:cifar-mpt-1-32}, \ref{table:cifar-mpt-1-1} and Tables~\ref{table:imagenet-mpt-1-32}, \ref{table:imagenet-mpt-1-1}, respectively.
Next to each MPT method we include the percentage of weights pruned in parentheses. Motivated by \citep{frankle2020training}, we also include models in which the BatchNorm parameters are learned when identifying the random subnetwork using biprop, indicated by BN. A more comprehensive comparison can be found in Appendix~\ref{sec:sota-comp}.

\begin{table*}[h] 
\parbox{.45\linewidth}{
\centering
\begin{small}
\begin{tabular}{@{}lrrr@{}}\toprule
\textbf{Method} & \textbf{Model} & \textbf{Top-1} & \textbf{Params} \\ \midrule
BinaryConnect & VGG-Small & 91.7 & 4.6 M \\ \hdashline
ProxQuant & ResNet-56 & 92.3 & 0.85 M \\ \hdashline
DSQ & ResNet-20 & 90.2 & 0.27 M \\ \hdashline
IR-Net & ResNet-20 & 90.8 & 0.27 M \\ \hdashline
Full-Precision & ResNet-18 & 93.02 & 11.2 M \\ \hline
MPT (80) & ResNet-18 & 94.66 & 2.2 M \\ \hdashline
\textbf{MPT (80) +BN} & \textbf{ResNet-18} & \textbf{94.8} & \textbf{2.2 M} \\ 
\bottomrule
\end{tabular}
\end{small}
\vspace{-0.1in}
\caption{Comparison of MPT-1/32 with trained binary-1/32 networks on CIFAR-10.}
\label{table:cifar-mpt-1-32}
}
\hspace{.055\linewidth}
\parbox{.45\linewidth}{
\centering
\begin{small}
\begin{tabular}{@{}lrrr@{}}\toprule
\textbf{Method} & \textbf{Model} & \textbf{Top-1} & \textbf{Params} \\ \midrule
BNN & VGG-Small & 89.9 & 4.6 M \\ \hdashline
XNOR-Net & VGG-Small & 89.8 & 4.6 M \\ \hdashline
DSQ & VGG-Small & 91.7 & 4.6 M \\ \hdashline
IR-Net & ResNet-18 & 91.5 & 11.2 M \\
{Full-Precision} & VGG-Small & 93.6 & 4.6 M \\ \hline
MPT (75) & VGG-Small & 88.52 & 1.44 M \\ \hdashline
\textbf{MPT (75) +BN} & \textbf{VGG-Small} & \textbf{91.9} & \textbf{1.44 M} \\
\bottomrule
\end{tabular}
\end{small}
\vspace{-0.1in}
\caption{Comparison of MPT-1/1 with trained binary-1/1 networks on CIFAR-10.}
\label{table:cifar-mpt-1-1}
}
\end{table*} 
\vspace*{-0.5mm}






\begin{table*}[h] 
\parbox{.45\linewidth}{
\centering
\begin{small}
\begin{tabular}{@{}lrrr@{}}\toprule
\textbf{Method} & \textbf{Model} & \textbf{Top-1} & \textbf{Params} \\ \midrule
ABC-Net & ResNet-18 & 62.8 & 11.2 M \\ \hdashline
BWN & ResNet-18 & 60.8 & 11.2 M \\ \hdashline
IR-Net & ResNet-34 & 70.4 & 21.8 M \\ \hdashline
Quant-Net & ResNet-50 & 72.8 & 25.6 M \\ \hdashline
Full-Precision & ResNet-34 & 73.27 & 21.8 M \\ \hline
MPT (80) & WRN-50 & 72.67 & 13.7 M \\ \hdashline
\textbf{MPT (80) +BN} &
\textbf{WRN-50} & \textbf{74.03} & \textbf{13.7 M} \\
\bottomrule
\end{tabular}
\end{small}
\vspace{-0.1in}
\caption{Comparison of MPT-1/32 with trained binary-1/32 networks on ImageNet.}
\label{table:imagenet-mpt-1-32}
}
\hspace{.072\linewidth}
\parbox{.45\linewidth}{
\centering
\begin{small}
\begin{tabular}{@{}lrrr@{}}\toprule
\textbf{Method} & \textbf{Model} & \textbf{Top-1} & \textbf{Params} \\ \midrule
BNN & AlexNet & 27.9 & 62.3 M \\ \hdashline
XNOR-Net & AlexNet & 44.2 & 62.3 M\\ \hdashline
ABC-Net & ResNet-34 & 52.4 & 21.8 M \\ \hdashline
\textbf{IR-Net} & \textbf{ResNet-34} & \textbf{62.9} & \textbf{21.8 M} \\ \hdashline
Full-Precision & ResNet-34 & 73.27& 21.8 M\\ \hline
MPT (60) & WRN-34 & 45.06 & 19.3 M \\ \hdashline
MPT (60) +BN & WRN-34 & 52.07 & 19.3 M \\
\bottomrule
\end{tabular}
\end{small}
\vspace{-0.1in}
\caption{Comparison of MPT-1/1 with trained binary-1/1 networks on ImageNet.}
\label{table:imagenet-mpt-1-1}
}
\end{table*}
\vspace{0.1in}

Our results highlight that SOTA DNN models are extremely redundant. 
For similar parameter count, our binary MPT-1/32 models outperform even full-precision models with learned weights. 
When compared to state-of-the-art in BNNs, with minimal hyperparameter tuning our multi-prize tickets achieve comparable (or higher) Top-1 accuracy.
Specifically, our MPT-1/32 outperform trained binary weight networks on CIFAR-10 and ImageNet and our MPT-1/1 outperforms trained binary weight and activation networks on CIFAR-10. Further, on CIFAR-10 and ImageNet, MPT-1/32 networks with significantly reduced parameter counts outperform dense and full precision networks with learned weights.
Searches for MPT-1/1 in BNN-specific architectures~\citep{bnas,bats} and adopting other commonly used tricks to improve model \& representation capacities~\citep{highcapacity,slbn,rbnn,siman} are likely to yield MPT-1/1 networks with improved performance.
For example, up to a 7\% gain in the MPT-1/1 accuracy was achieved by simply allowing BatchNorm parameters to be updated.
Additionally, alternative approaches for updating the pruning mask in \textbf{biprop} could alleviate issues with back-propagating gradients through binary activation networks. 

\section{Discussion and Implications}

Existing compression approaches (e.g., pruning and binarization) typically rely on some form of weight-training. This paper showed that a sufficiently overparametrized randomly weighted network contains binary subnetworks that achieve high accuracy (comparable to dense and full precision original network with learned weights) without any training. We referred to this finding as the \emph{Multi-Prize Lottery Ticket Hypothesis.} We also proved the existence of such winning tickets and presented a generic procedure to find them. 
Our comparison with state-of-the-art neural networks corroborated our hypothesis. 
With minimal hyperparameter tuning, our binary weight multi-prize tickets outperformed current state-of-the-art in BNNs and proved its practical importance.
Our work has several important practical and theoretical implications.

\paragraph{Algorithmic.}
Our \textbf{biprop} framework enjoys certain advantages over traditional weight-optimization. 
First, contemporary experience suggests that sparse BNN training from scratch is challenging. Both sparseness and binarization bring their own challenges for gradient-based weight training -- getting stuck at bad local minima in the sparse regime, incompatibility of back-propagation due to discontinuity in activation function, etc. Although we used gradient-based approaches in this paper, \textbf{biprop} is flexible to accommodate different class of algorithms that might avoid the pitfalls of gradient-based weight training.  
Next, in contrast to weight-optimization that requires large model size and massive compute resources to achieve high performance, our hypothesis suggests that one can achieve similar performance without ever training the large model. Therefore, strategies such as fast ticket search~\citep{you2019drawing} or forward ticket selection~\citep{ye2020good} can be developed to enable more efficient ways of finding--or even designing--MPTs.
Finally, as opposed to weight-optimization, \textbf{biprop} by design achieves compact yet accurate models.

\paragraph{Theoretical.}
MPTs achieve similar performance as the model with learned weights. 
First, this observation notes the benefit of overparameterization in the neural network learning and reinforces the idea that an important task of gradient descent (and learning in general) may be to effectively compress overparametrized models to find multi-prize tickets.  
Next, our results highlight the expressive power of MPTs -- since we showed that compressed subnetworks can approximate any target neural network who are known to be universal approximators, our MPTs are also universal approximators. 
Finally, the multi-prize lottery ticket hypothesis also uncovers the generalization properties of DNNs. Generalization theory for DL is still in its infancy and its not clear what and how DNNs learn~\citep{neyshabur2017exploring}. 
Multi-prize lottery ticket hypothesis may serve as a valuable tool for answering such questions as it indicates the dependence of generalization on the compressiblity.


\paragraph{Practical.}

Huge storage and heavy computation requirements of state-of-the-art deep neural networks inevitably limit their applications in practice. Multi-prize tickets are significantly lighter, faster, and efficient while maintaining performance. This unlocks a range of potential applications DL could be applied to (e.g., applications with resource-constrained devices such as mobile phones, embedded devices, etc.). Our results also indicate that existing SOTA models might be spending far more compute and power than is needed to achieve a certain performance. In other words, SOTA DL models have terrible energy efficiency and significant carbon footprint~\citep{strubell2019energy}. In this regard, MPTs have the potential to enable environmentally friendly artificial intelligence.


















\bibliography{ml}
\bibliographystyle{iclr2021_conference}

\section*{Acknowledgements} 
The authors would like to thank Shreya Chaganti for her valuable contributions to the \textbf{biprop} open source code development and for her help on training MPT models for the final version of the paper.

This work was performed under the auspices of the U.S. Department of Energy by the Lawrence Livermore National Laboratory under Contract No. DE-AC52-07NA27344, Lawrence Livermore National Security, LLC. This document was prepared as an account of the work sponsored by an agency of the United States Government. Neither the United States Government nor Lawrence Livermore National Security, LLC, nor any of their employees makes any warranty, expressed or implied, or assumes any legal liability or responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed, or represents that its use would not infringe privately owned rights. Reference herein to any specific commercial product, process, or service by trade name, trademark, manufacturer, or otherwise does not necessarily constitute or imply its endorsement, recommendation, or favoring by the United States Government or Lawrence Livermore National Security, LLC. The views and opinions of the authors expressed herein do not necessarily state or reflect those of the United States Government or Lawrence Livermore National Security, LLC, and shall not be used for advertising or product endorsement purposes. This work was supported by LLNL Laboratory Directed Research and Development project 20-ER-014 and released with LLNL tracking number LLNL-CONF-815432.

\newpage
\appendix
\section{Hyperparameter Configurations}
\label{sec:hyperparameters}
\subsection{Hyperparameters for Section~\ref{sec:exp-mplth}}
\paragraph{Experimental Configuration.} 
For MPT-1/32 tickets, the network structure is not modified from the original. For MPT-1/1 tickets, the network structure is modified by moving the max-pooling layer directly after the convolution layer and adding a batch-normalization layer before the binary activation function, as is common in many BNN architectures \citep{rastegari2016xnornet}. We choose our baselines as dense full precision models with learned weights. The baselines were obtained by training backbone networks using the Adam optimizer with learning rate of  for  epochs and with a batch size of . In each randomly weighted backbone network, we find winning tickets MPT-1/32 and MPT-1/1 for different pruning rates using Algorithm~\ref{alg:quant-opt}. For both the weight-optimized and MPT networks, the weights are initialized using the Kaiming Normal distribution \citep{he2015delving}. All training routines make use of a cosine decay learning rate policy.

\begin{table*}[h] 
\centering
\begin{small}
\begin{tabular}{@{}lrrrrrrr@{}}\toprule
\textbf{Method} & \textbf{Model} & \textbf{Optimizer} & \textbf{LR} & \textbf{Momentum} & \textbf{Weight Decay} & \textbf{Batch} & \textbf{Epochs} \\ \midrule
MPT-1/32 & Conv2/4/6/8 & SGD & 0.1 & 0.9 & 1e-4 & 128 & 250 \\ \hdashline
MPT-1/1 & Conv2/4/6/8 & Adam & 0.1 & - & 1e-4 & 128 & 250 \\
\bottomrule
\end{tabular}
\end{small}
\caption{Hyperparameter Configurations for CIFAR-10 Experiments}
\label{table:cifar-10-hyperparams}
\end{table*} 

\subsection{Hyperparameters for Section~\ref{sec:exp-sota}}

In these experiments, the weights are initialized using the Kaiming Normal distribution \citep{he2015delving} for all the models except for MPT-1/32 on ImageNet where we use the Signed Constant initialization \citep{ramanujan2019whats} as it yielded slightly better performance. All training routines make use of a cosine decay learning rate policy. For ImageNet training we used a label smoothing value of 0.1 and a learning rate warmup length of 5 epochs. 

\begin{table*}[h] 
\centering
\begin{small}
\begin{tabular}{@{}lrrrrrrr@{}}\toprule
\textbf{Method} & \textbf{Model} & \textbf{Opt.} & \textbf{LR} & \textbf{Momentum} & \textbf{Weight Decay} & \textbf{Batch} & \textbf{Epochs} \\ \midrule
MPT-1/32 & ResNet-18 & SGD & 0.1 & 0.9 & 5e-4 & 256 & 250 \\ \hdashline
MPT-1/32 +BN & ResNet-18 & SGD & 0.1 & 0.9 & 5e-4 & 256 & 250 \\ \hdashline
MPT-1/1 & VGG-Small & Adam & 3.63e-3 & - & 17.335 & 128 & 600 \\ \hdashline
MPT-1/1 +BN & VGG-Small & Adam & 3.63e-3 & - & 1e-4 & 128 & 600 \\ 
\bottomrule
\end{tabular}
\end{small}
\caption{Hyperparameter Configurations for CIFAR-10 Experiments}
\label{table:cifar-10-hyperparams-2}
\end{table*} 

\begin{table*}[h] 
\centering
\begin{small}
\begin{tabular}{@{}lrrrrrrr@{}}\toprule
\textbf{Method} & \textbf{Model} & \textbf{Optimizer} & \textbf{LR} & \textbf{Momentum} & \textbf{Weight Decay} & \textbf{Batch} & \textbf{Epochs} \\ \midrule
MPT-1/32 & WRN-50 & SGD & 0.256 & 0.875 & 3.051757812e-5 & 256 & 120 \\ \hdashline
MPT-1/32 +BN & WRN-50 & SGD & 0.256 & 0.875 & {3.051757812e-5} & 256 & 120 \\ \hdashline
MPT-1/1 & WRN-34 & Adam & 2.56e-4 & - & 3.051757812e-5 & 256 & 250 \\ \hdashline
MPT-1/1 +BN & WRN-34 & Adam & 2.56e-4 & - & 3.051757812e-5 & 256 & 250 \\ 
\bottomrule
\end{tabular}
\end{small}
\caption{Hyperparameter Configurations for ImageNet Experiments}
\label{table:imagenet-hyperparams}
\end{table*} 











\section{Existence of Binary-Weight Subnetwork Approximating Target Network} \label{sec:new-existence-bin-init}

In the following analysis, note that we write  to denote matrices of dimension  whose components are independently sampled from a binomial distribution with elements  and probability .

\begin{lemma} \label{lem:new-bin-init-two-layer}
Let , , , and  be given. Let  be chosen randomly from  and  be chosen randomly from . If 
then with probability at least  there exist masks  and  such that the function  defined by

satisfies

for all . Furthermore, , and .
\end{lemma}
\begin{proof}
If  then taking  yields the desired result. Suppose that . Then there exists a  such that 

Hence, it follows that

where the final inequality follows from (\ref{eq:new-bin-init-two-layer.0}) and the hypothesis that . Our goal now is to show that with probability  the random initialization of  and  yield masks  and   such that .

Now fix  and take . First, we consider the probability

As  and  are each sampled from a binomial distribution with  trials, the distribution that the pair  is sampled from is a multinomial distribution with four possible events each having a probability of . Since we are only interested in the event  occurring, we can instead consider a binomial distribution where  and . Hence, using Hoeffding's inequality we have that

where the final inequality follows since  is an increasing function and .
\iffalse
First, we consider the probability

As  and  are sampled in the same way and are of the same dimension, the probabilities in the right hand side of (\ref{eq:new-bin-init-two-layer.1}) are the same. Furthermore, as they are sampled from a binomial distribution with  trials, we can bound these probabilities using Hoeffding's inequality. Using Hoeffding's inequality, we find that

\fi
From (\ref{eq:new-bin-init-two-layer.0}) and the fact that , it follows that

Combining our hypothesis in (\ref{hypothesis:new-bin-init-two-layer}) with (\ref{eq:new-bin-init-two-layer.3}) yields that

Substituting (\ref{eq:new-bin-init-two-layer.4}) into (\ref{eq:new-bin-init-two-layer.2}) yields

\iffalse
Combining (\ref{eq:new-bin-init-two-layer.5}) with (\ref{eq:new-bin-init-two-layer.1}) we now have

\fi

Additionally, it follows from the same argument that

From (\ref{eq:new-bin-init-two-layer.5}) and (\ref{eq:new-bin-init-two-layer.7}) it follows with probability at least  that there exist sets  and  satisfying  and . Using these sets, we define the components of the mask  and  by

and

Using the definition of  in (\ref{def:g-new-bin-init-two-layer}) we now have that

where the final equality follows from the identity , for all . This concludes the proof of (\ref{result:new-bin-init-two-layer}).

Lastly, by our choice of  in (\ref{eq:new-bin-init-two-layer.8}),  in (\ref{eq:new-bin-init-two-layer.8.1}), and (\ref{eq:new-bin-init-two-layer.3}), it follows that 

and 

which concludes the proof.
\end{proof}


The next step is to consider an analogue for Lemma A.2 from \citep{malach2020proving} which we provide in Lemma~\ref{lem:bin-init-two-layer-2}.

\begin{lemma} \label{lem:bin-init-two-layer-2}
Let ,  with , and  be given. Let  be chosen randomly from  and  be chosen randomly from . If
 
then with probability at least  there exist masks  and  such that the function  defined by

satisfies

Furthermore,  and .
\end{lemma}
\begin{proof}
Assume  and set . Note that if  then the excess neurons can be masked yielding the desired value for . We decompose , , , and  into  equal size submatrices by defining

for . Note that these submatrices satisfy 


Now let . By our hypothesis that , it follows that . WLOG, assume that . Now fix  and define  by

By (\ref{hypothesis:bin-init-two-layer-2}), taking  and  yields that . Hence, it follows from Lemma~\ref{lem:new-bin-init-two-layer} that with probability at least  there exist  and  such that 

for every  with , and


By the definition of  in (\ref{def:g-bin-init-two-layer-2}), using (\ref{eq:bin-init-two-layer-2.4}) yields

Hence, combining (\ref{eq:bin-init-two-layer-2.8}) for all , it follows that with probability at least  we have

Finally, it follows from (\ref{eq:bin-init-two-layer-2.4}) and (\ref{eq:bin-init-two-layer-2.9}) that

which concludes the proof.
\end{proof}

We now state and prove an analogue to Lemma A.5 in \citep{malach2020proving} which is the last lemma we will need to establish the desired result.

\begin{lemma} \label{lem:bin-init-two-layer-3}
Let ,  with ,  defined by , and  be given. Let  be chosen randomly from  and  be chosen randomly from . If 

then with probability at least  there exist masks  and  such that the function  defined by

satisfies

Furthermore, .
\end{lemma}
\begin{proof}
Assume  and set . Note that if  then excess neurons can be masked to yield the desired value for . As in the proof of Lemma~\ref{lem:bin-init-two-layer-2}, we can split , , , and  into  submatrices, denoted , , , and  for , such that


To simplify notation in the following definition, we define the vectors  and . Now we define the functions  by

for each . Taking  and , it follows from (\ref{hypothesis:bin-init-two-layer-3}) that . As the hypotheses of Lemma~\ref{lem:bin-init-two-layer-2} are satisfied, with probability at least  there exist masks  and  with 

such that

For each , note that this results in choosing the columns of the mask  by 

Combining this choice with (\ref{eq:bin-init-two-layer-3.1}) yields

By the definition of  in (\ref{def:G-bin-init-two-layer-3}), it follows from (\ref{eq:bin-init-two-layer-3.4}) that

Combining (\ref{eq:bin-init-two-layer-3.3}) and (\ref{eq:bin-init-two-layer-3.5}), we have with probability at least  that

Finally, it follows from (\ref{eq:bin-init-two-layer-3.2.1}) and (\ref{eq:bin-init-two-layer-3.3.1}) that
 
which concludes the proof.
\end{proof}

We are now ready to prove the main result in Theorem~\ref{thm:bin-subnetwork}.

\begin{theorem} \label{thm:bin-subnetwork}
Let , , , and . Assume that for each  we have  and . Define  where  for  and . Fix . 

Let  be sampled from ,  be sampled from ,  be sampled from  and  sampled from . If

then with probability at least  there exist binary masks  and  for  and , respectively, such that the function  defined by

where 
 
satisfies

Additionally, .
\end{theorem}
\begin{proof}
Let . Using Lemma~\ref{lem:bin-init-two-layer-3} with  and , with probability at least  there exist  and  such that 

and

The remainder of the proof follows from applying the same argument as in the proof of Theorem~A.6 from \citep{malach2020proving}.
\end{proof}

\section{Motivation for Framework to Identify MPTs} \label{sec:biprop-motivation}

Suppose that  with optimized weights  is a target network that we wish to approximate. Let  denote the network in which we want to identify a MPT-1/32 that is an -approximation of , for some . 

Now assume that  is Lipschitz continuous with constant ,  are binary parameters for , and  is gain term. It follows that

If we take  to be a fixed binary mask, we can minimize the error of binarizing the subnetwork parameters  by solving the optimization problem

where , , and  are stacked into vectors of some length, say . As the pruning mask  is applied to both  and , solving problem (\ref{prob:binary-weight-subnetwork}) is equivalent to solving problem (2) in \citep{rastegari2016xnornet} with a different dimension. Hence, it immediately follows that one closed form solution for  in problem (\ref{prob:binary-weight-subnetwork}) is

Taking the derivative of the cost function in (\ref{prob:binary-weight-subnetwork}) with respect to  and setting it equal to zero yields

Recalling that  and using (\ref{eq:bin-weight-sub-B-sol}), we have

and

Substituting (\ref{eq:bin-weight-sub-beta.2}) and (\ref{eq:bin-weight-sub-beta.3}) into (\ref{eq:bin-weight-sub-beta.1}) and solving for  yields the closed form solution

Hence,  and  minimize the right hand side of (\ref{eq:subnetwork-err-bound}) and, consequently, reduce the approximation error of the MPT-1/32. So when the binarization error, , and the subnetwork error, , are sufficiently small then the binarized subnetwork  serves as a good approximation to the target network. 

These closed form expressions for the gain term and the binarized weights are the updates used for the gain term and binary subnetwork weights in \textbf{biprop} after updating the binary pruning mask.


\section{Comparison of MPTs with binary neural network SOTA}
\label{sec:sota-comp}

Here we provide a more exhaustive comparison of MPT--1/32 and MPT--1/1 on CIFAR-10 and ImageNet to SOTA methods -- BinaryConnect~\citep{courbariaux2015binaryconnect}, BNN~\citep{courbariaux2016binarized}, 
DoReFa-Net~\citep{zhou2016dorefa},
LQ-Nets~\citep{zhang2018lq},
BWN and XNOR-Net~\citep{rastegari2016xnornet},
ABC-Net~\citep{lin2017towards},
IR-Net~\citep{qin2020forward},
LAB~\citep{hou2016loss},
ProxQuant~\citep{bai2018proxquant},
DSQ~\citep{gong2019differentiable}, and
BBG~\citep{shen2020balanced}. 
Results for CIFAR-10 can be found in Tables~\ref{table:app-cifar-mpt-1-32} and \ref{table:app-cifar-mpt-1-1} and results for ImageNet can be found in Tables~\ref{table:app-imagenet-mpt-1-32} and \ref{table:app-imagenet-mpt-1-1}. Next to the MPT method we include the percentage of weights pruned and the layer width multiplier (if larger than 1) in parentheses. 

\begin{table*}[!t] 
\centering
\begin{small}
\begin{tabular}{@{}lrrr@{}}\toprule
\textbf{Method} & \textbf{Model} & \textbf{Top-1} & \textbf{Params} \\ \midrule
BinaryConnect & VGG-Small & 91.7 & 4.6 M \\ \hdashline
BWN & VGG-Small & 90.1 & 4.6 M \\ \hdashline
DoReFa-Net & ResNet-20 & 90.0 & 0.27 M \\ \hdashline
LQ-Nets & ResNet-20 & 90.1 & 0.27 M \\ \hdashline
LAB & VGG-Small & 89.5 & 4.6 M \\ \hdashline
ProxQuant & ResNet-56 & 92.3 & 0.85 M \\ \hdashline
DSQ & ResNet-20 & 90.2 & 0.27 M \\ \hdashline
IR-Net & ResNet-20 & 90.8 & 0.27 M \\ \hdashline
Full-Precision & ResNet-18 & 93.02 & 11.2 M \\ \hline
MPT-1/32 (95) & VGG-Small & 91.48 & 0.23 M \\ \hdashline
MPT (80) & ResNet-18 & 94.66 & 2.2 M \\ \hdashline
MPT (80) +BN & \textbf{ResNet-18} & \textbf{94.8} & \textbf{2.2 M} \\ 
\bottomrule
\end{tabular}
\end{small}
\caption{Comparison of MPT-1/32 with Trained Binary (1/32) Networks on CIFAR-10}
\label{table:app-cifar-mpt-1-32}
\end{table*} 

\begin{table*}
\centering
\begin{small}
\begin{tabular}{@{}lrrrr@{}}\toprule
\textbf{Method} & \textbf{Model} & \textbf{Top-1} & \textbf{Params} \\ \midrule
BNN & VGG-Small & 89.9 & 4.6 M \\ \hdashline
XNOR-Net & VGG-Small & 89.8 & 4.6 M \\ \hdashline
DoReFa-Net & ResNet-20 & 79.3 & 0.27 M \\ \hdashline
BBG & ResNet-20 & 85.3 & 0.27 M \\ \hdashline
LAB & VGG-Small & 87.7 & 4.6 M \\ \hdashline
DSQ & VGG-Small & 91.7 & 4.6 M \\ \hdashline
IR-Net & ResNet-18 & 91.5 & 4.6 M \\ \hdashline
{Full-Precision} & VGG-Small & 93.6 & 4.6 M \\ \hline
MPT (75, 1.25x) & VGG-Small & 88.49 & 1.44 M \\ \hdashline
\textbf{MPT (75, 1.25x) +BN} & \textbf{VGG-Small} & \textbf{91.9} & \textbf{1.44 M} \\
\bottomrule
\end{tabular}
\end{small}
\caption{Comparison of MPT-1/1 with Trained Binary (1/1) Networks on CIFAR-10}
\label{table:app-cifar-mpt-1-1}
\end{table*} 





\begin{table*}[!t] 
\centering
\begin{small}
\begin{tabular}{@{}lrrr@{}}\toprule
\textbf{Method} & \textbf{Model} & \textbf{Top-1} & \textbf{Params} \\ \midrule
ABC-Net & ResNet-18 & 62.8 & 11.2 M \\ \hdashline
BWN & ResNet-18 & 60.8 & 11.2 M \\ \hdashline
BWNH & ResNet-18 & 64.3 & 11.2 M \\ \hdashline
PACT & ResNet-18 & 65.8 & 11.2 M \\ \hdashline
IR-Net & ResNet-34 & 70.4 & 21.8 M \\ \hdashline
Quantization-Networks & ResNet-18 & 66.5 & 11.2 M \\ \hdashline
Quantization-Networks & ResNet-50 & 72.8 & 25.6 M \\ \hdashline
Full-Precision & ResNet-34 & 73.27 & 21.8 M \\ \hline
MPT (80) & WRN-50 & 72.67 & 13.7 M \\ \hdashline
\textbf{MPT (80) +BN} &
\textbf{WRN-50} & \textbf{74.03} & \textbf{13.7 M} \\
\bottomrule
\end{tabular}
\end{small}
\caption{Comparison of MPT-1/32 with Trained Binary (1/32) Networks on ImageNet}
\label{table:app-imagenet-mpt-1-32}
\end{table*}


\begin{table*}[!t] 
\centering
\begin{small}
\begin{tabular}{@{}lrrr@{}}\toprule
\textbf{Method} & \textbf{Model} & \textbf{Top-1} & \textbf{Params} \\ \midrule
BNN & AlexNet & 27.9 & 62.3 M \\ \hdashline
XNOR-Net & AlexNet & 44.2 & 62.3 M\\ \hdashline
ABC-Net & ResNet-18 & 42.7 & 11.2 M \\ \hdashline
ABC-Net & ResNet-34 & 52.4 & 21.8 M \\ \hdashline
TSQ & AlexNet & 58.0 & 62.3 M\\ \hdashline
WRPN & ResNet-34 & 60.5 & 21.8 M \\ \hdashline
HWGQ & AlexNet & 52.7 & 62.3 M\\ \hdashline
IR-Net & ResNet-18 & 58.1 & 11.2 M \\ \hdashline
\textbf{IR-Net} & \textbf{ResNet-34} & \textbf{62.9} & \textbf{21.8 M} \\ \hdashline
Full-Precision & ResNet-34 & 73.27& 21.8 M\\ \hline
MPT (60) & WRN-34 & 45.06 & 19.3 M \\ \hdashline
MPT (60) +BN & WRN-34 & 52.07 & 19.3 M \\
\bottomrule
\end{tabular}
\end{small}
\caption{Comparison of MPT-1/1 with Trained Binary (1/1) Networks on ImageNet}
\label{table:app-imagenet-mpt-1-1}
\end{table*}


\section{Comparison to edgepopup for MPT-1/32}
\label{sec:mpt-vs-edgepopup}
Note that binarization step of \textbf{biprop} can be avoided while finding MPT-1/32 -- by initializing (and pruning) our backbone neural network with binary initialization (e.g., edgepopup with Signed Constant initialization~\citep{ramanujan2019whats}). In this specific instance, \textbf{biprop} boils down to edgepopup with proper scaling. Next, we compare the performance of MPT-1/32 networks identified using these two approaches. 
Both networks presented below use the same hyperparameter configurations and are trained for 250 epochs on the CIFAR-10 dataset. We initialize the networks identified with edgepopup using the Signed Constant initialization as it yielded their best performance. MPT-1/32 networks identified using \textbf{biprop} are initialized using the Kaiming Normal initialization. We plot the average over three experiments for each pruning percentage and bars extending to the minimum and maximum accuracy for each pruning percentage. Additionally, for each network we include the Top-1 accuracy of a dense model with learned weights. These plots can be found in Figure~\ref{fig:mpt-vs-edgepopup}. We find that the performance of MPT-1/32 identified with \textbf{biprop} outperforms networks identified using edgepopup. This highlights the benefit of binarization (in conjunction with pruning) as a learning strategy.  

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth, trim={6cm 0 6cm 2.5cm}, clip]{figures/mpt_vs_edgepopup.png}
    \caption{{\bfseries Comparing biprop and edgepopup}: Comparing the Top-1 accuracy of MPT-1/32 to binary weight networks of the same size identified using edgepopup on CIFAR-10.}
    \label{fig:mpt-vs-edgepopup}
\end{figure}


\section{Related Work}
\label{sec:relatedwork}
\subsection{Pruning}
We categorize pruning methods based on whether a model is pruned either after the training or before the training (see \citep{cheng2017survey} for a comprehensive review).

\paragraph{Post-Training Pruning.}
The traditional pruning methods leverage a three-stage pipeline -- pre-training  (a large model), pruning, and fine-tuning. The main distinction lies among these approaches is what type of criteria is used for pruning. 
One of the most popular approach is the magnitude-based pruning where the weights with the magnitude below a certain threshold are discarded~\citep{hagiwara1993removal}.
Further, certain penalty term (e.g., ,  or lasso weight regularization) can be used during training to encourage a model to learn certain smaller magnitude weights and removing them post-training~\citep{weigend1991generalization}. 
Models can also be pruned by measuring the importance of weights by computing the sensitivity of the loss function when weights are removed and prune those which cause the smallest change in the loss~\citep{lecun1990optimal}. 


\paragraph{Pruning Before Training.}

Thus far, we have have discussed methods for pruning pretrained DNNs. 

Recently, \citep{frankle2018lottery} proposed the \emph{Lottery Ticket Hypothesis} and showed that  randomly-initialized neural networks contain sparse subnetworks that can be effectively trained from scratch when reset to their initialization.
Further, \citep{liu2018rethinking} showed that the training an over-parameterized model is often not necessary to obtain an efficient final model and network architecture itself is more important than the remaining weights after pruning pretrained networks. These findings has revived interest in finding approaches for searching sparse and trainable subnetworks. For example, \citep{lee2018snip, 2wang2020pruning, you2019drawing, 1wang2020picking} explored efficient approaches to search for these sparse and trainable subnetworks.
Along this line of work, a striking finding was reported by
~\citep{zhou2019deconstructing, ramanujan2019whats} showing that randomly-initialized neural networks contain sparse subnetworks that achieve good performance without any training.
\citep{malach2020proving, pensia2020optimal} provided theoretical evidences for this phenomenon and showed that one can approximate any target neural network, by pruning a sufficiently over-parameterized network of random weights.



\subsection{Binarization}
Similar to pruning, we categorize binarization methods based on whether a model is binarized either after the training or during the training (see \citep{qin2020binary} for a comprehensive review).

\paragraph{Post-Training Binarization.}
To the best of our knowledge, none of the post-training schemes have been successful in binarizing pretrained models with or without retraining to achieve reasonable test accuracy. Most existing works~\citep{han2015deep, zhou2017incremental} are limited to ternary weight quantization.

\paragraph{Training-Aware Binarization.}
There are several efforts to improve the performance of BNN training. This is a challenging problem as binarization introduces discontinuities which
makes differentiation during backpropogation difficult. 
Binaryconnect~\citep{courbariaux2015binaryconnect} established how to train networks with binary weights within the familiar back-propagation paradigm.
BinaryNet~\citep{courbariaux2016binarized} further quantize both the weights and the activations to 1-bit values.
Unfortunately, these early schemes resulted in a staggering drop in the accuracy compared to their full precision counterparts. 
In an attempt to improve the performance, XNOR-Net~\citep{rastegari2016xnornet} proposed to add a real-valued channel-wise scaling factor. 
Dorefa-Net~\citep{zhou2016dorefa} extends XNOR-Net to accelerate the training process using quantized gradients.
ABC-Net~\citep{lin2017towards} improved the performance by using more weight bases and activation bases at the cost of increase in memory and computation. There have also been efforts in making modifications to the network architectures to make them amenable for the binary neural network training. For example, Bireal-Net~\citep{liu2018bi} added layer-wise identity short-cut, and AutoBNN~\citep{shen2020balanced} proposed to widen or squeeze the channels in an automatic manner. \citep{han2020training} proposed to learn to binarize neurons with noisy supervision.
Some efforts also have been carried out to designing gradient estimators extending straight-through estimator (STE) ~\citep{bengio2013estimating} for accurate gradient back-propagation. DSQ~\citep{gong2019differentiable} used differentiable soft quantization to have accurate gradients in backward propagation.
On the other hand, PCNN~\cite{gu2019projection} proposed a new discrete back-propagation via projection algorithm to build BNNs.

\subsection{Other Related Directions}
\cite{gaier2019weight} proposed a search method for neural network architectures that can already perform a task without any explicit weight training, i.e., each weight in the network has the same shared value. Recent work in randomly wired neural networks~\citep{xie2019exploring} showed that constructing neural networks with random graph algorithms often outperforms a manually engineered architecture. As opposed to fixed wirings in~\citep{xie2019exploring}, \citep{wortsman2019discovering} learned the network parameters as well as the structure. 
This show that finding a good architecture is akin to finding a sparse subnetwork of the complete graph.

\end{document}
