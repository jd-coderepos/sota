\documentclass[times,twocolumn,final,authoryear]{elsarticle}

\usepackage{amsmath} \usepackage{prletters}
\usepackage{framed,multirow}

\usepackage{amssymb}
\usepackage{latexsym}

\usepackage{url}
\usepackage{xcolor}
\definecolor{newcolor}{rgb}{.8,.349,.1}


\journal{Pattern Recognition Letters}




  


\usepackage[round]{natbib}
\usepackage{bbm}
\usepackage{breqn}
\usepackage{hyperref}
\usepackage{amsthm}
\usepackage{placeins}

\usepackage{booktabs} \usepackage{multirow} 

\usepackage{atbegshi,picture}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\usepackage{stackengine}
\def\delequal{\mathrel{\ensurestackMath{\stackon[1pt]{=}{\scriptstyle\Delta}}}}
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\newcommand{\red}[1]{\textcolor{red}{#1}}



\AtBeginShipout{\AtBeginShipoutFirst{\put(\dimexpr\paperwidth-4.7cm\relax,-1.1cm){\makebox[0pt][r]{{Published in \textit{Pattern Recognition Letters}, (2020), 325-331, 140; \url{https://doi.org/10.1016/j.patrec.2020.11.008}}}}}}



\begin{document}



\ifpreprint
  \setcounter{page}{1}
\else
  \setcounter{page}{1}
\fi

\begin{frontmatter}

\title{Rank consistent ordinal regression for neural networks with application to age estimation}

\author[1]{Wenzhi \snm{Cao}} 
\author[2]{Vahid \snm{Mirjalili}}
\author[1]{Sebastian \snm{Raschka}\corref{cor1}}
\cortext[cor1]{Corresponding author:}
\ead{sraschka@wisc.edu}


\address[1]{University of Wisconsin-Madison, Department of Statistics, 1300 University Ave, Madison, WI 53705, USA}
\address[2]{Michigan State University, Department of Computer Science \& Engineering, 428 South Shaw Lane, East Lansing, MI 48824, USA}




\begin{abstract}
In many real-world prediction tasks, class labels include information about the relative ordering between labels, which is not captured by commonly-used loss functions such as multi-category cross-entropy. Recently, the deep learning community adopted ordinal regression frameworks to take such ordering information into account. Neural networks were equipped with ordinal regression capabilities by transforming ordinal targets into binary classification subtasks. However, this method suffers from inconsistencies among the different binary classifiers. To resolve these inconsistencies, we propose the COnsistent RAnk Logits (CORAL) framework with strong theoretical guarantees for rank-monotonicity and consistent confidence scores. Moreover, the proposed method is architecture-agnostic and can extend arbitrary state-of-the-art deep neural network classifiers for ordinal regression tasks. The empirical evaluation of the proposed rank-consistent method on a range of face-image datasets for age prediction shows a substantial reduction of the prediction error compared to the reference ordinal regression network.
\end{abstract}

\begin{keyword}
\MSC 41A05\sep 41A10\sep 65D05\sep 65D17
\KWD Deep learning\sep Ordinal regression\sep Convolutional neural networks \sep Age prediction \sep Machine learning \sep biometrics
\end{keyword}

\end{frontmatter}













\section{Introduction}
\label{sec:introduction}

Ordinal regression (also called ordinal classification), describes the task of predicting labels on an ordinal scale. Here, a ranking rule or classifier  maps each object  into an ordered set , where . In contrast to classification, the labels provide enough information to order objects. However, as opposed to metric regression, the difference between label values is arbitrary.

While the field of machine learning has developed many powerful algorithms for predictive modeling, most algorithms have been designed for classification tasks. The extended binary classification approach proposed by~\cite{li2007ordinal} forms the basis of many ordinal regression implementations. However, neural network-based implementations of this approach commonly suffer from classifier inconsistencies among the binary rankings~\citep{niu2016ordinal}.
This inconsistency problem among the predictions of individual binary classifiers is illustrated in Figure~\ref{fig:inconsistency-issue}.
We propose a new method and theorem for guaranteed classifier consistency that can easily be implemented in various neural network architectures.


Furthermore, along with the theoretical rank-consistency guarantees, this paper presents an empirical analysis of our approach to challenging real-world datasets for predicting the age of individuals from face images using our method with convolutional neural networks (CNNs). Aging can be regarded as a non-stationary process since age progression effects appear differently depending on the person's age. During childhood, facial aging is primarily associated with changes in the shape of the face, whereas aging during adulthood is defined mainly by changes in skin texture~\citep{ramanathan2009age,niu2016ordinal}. Based on this assumption, age prediction can be modeled using ordinal regression-based approaches~\citep{yang2010ranking,chang2011ordinal,cao2012human,li2012learning}.

The main contributions of this paper are as follows:

\begin{enumerate}
    \item The consistent rank logits (CORAL) framework for ordinal regression with theoretical guarantees for classifier consistency;
    \item Implementation of CORAL to adapt common CNN architectures, such as ResNet~\citep{he2016deep}, for ordinal regression;
    \item Experiments on different age estimation datasets showing that CORAL's guaranteed binary classifier consistency improves predictive performance compared to the reference framework for ordinal regression.
\end{enumerate}



Note that this work focuses on age estimation to study the proposed method's efficacy for ordinal regression. However, the proposed technique can be used for other ordinal regression problems, such as crowd-counting, depth estimation, biological cell counting, customer satisfaction, and others.

\begin{figure}
\begin{center}
\centerline{\includegraphics[width=0.98\linewidth]{figures/abstract-figure}}
\caption{Schematic illustration of inconsistencies that can occur among individual classifiers in the general reduction framework from ordinal regression to binary classification: a rank-inconsistent model (left) versus a rank-consistent model where the probabilities decrease consistently (right)}
\label{fig:inconsistency-issue}
\end{center}
\end{figure}


\section{Related work}
\label{sec:related-work}

\subsection{Ordinal regression and ranking}

Several multivariate extensions of generalized linear models have been developed for ordinal regression in the past, including the popular proportional odds and proportional hazards models~\citep{mccullagh1980regression}. Moreover, the machine learning field developed ordinal regression models based on extensions of well-studied classification algorithms, by reformulating the problem to utilize multiple binary classification tasks~\citep{baccianella2009evaluation}. Early work in this regard includes the use of perceptrons~\citep{crammer2002pranking,shen2005ranking} and support vector machines~\citep{herbrich1999support,shashua2003ranking,rajaram2003classification,chu2005new}. \cite{li2007ordinal} proposed a general reduction framework that unified the view of a number of these existing algorithms.

\subsection{Ordinal regression CNN}

While earlier works on using CNNs for ordinal targets have employed conventional classification approaches~\citep{levi2015age,rothe2015dex}, the general reduction framework from ordinal regression to binary classification by \cite{li2007ordinal} was recently adopted by \cite{niu2016ordinal} as \textit{Ordinal Regression CNN} (OR-CNN). In the OR-CNN approach, an ordinal regression problem with  ranks is transformed into  binary classification problems, with the -th task predicting whether the age label of a face image exceeds rank , . All  tasks share the same intermediate layers but are assigned distinct weight parameters in the output layer. 

While the OR-CNN was able to achieve state-of-the-art performance on benchmark datasets, it does not guarantee consistent predictions, such that predictions for individual binary tasks may disagree. For example, in an age estimation setting, it would be contradictory if the -th binary task predicted that the age of a person was more than 30, but a previous task predicted the person's age was less than 20. This inconsistency could be suboptimal when the  task predictions are combined to obtain the estimated age.

\cite{niu2016ordinal} acknowledged the classifier inconsistency as not being ideal and also noted that ensuring the  binary classifiers are consistent would increase the training complexity substantially~\citep{niu2016ordinal}. The CORAL method proposed in this paper addresses both these issues with a theoretical guarantee for classifier consistency and without increasing the training complexity.

\begin{figure*}
\begin{center}
\centerline{\includegraphics[width=0.98\linewidth]{figures/resnet-with-layer.png}}
\caption{Illustration of the consistent rank logits CNN (CORAL-CNN) used for age prediction. From the estimated probability values, the binary labels are obtained via Eq.~\ref{eq:proba-to-binary} and converted to the age label via Eq.~\ref{eq:predicted-label}.}
\label{fig:resnet}
\end{center}
\end{figure*}





\subsection{Other CNN architectures for age estimation}

\cite{chen2017using} proposed a modification of the OR-CNN~\citep{niu2016ordinal}, known as Ranking-CNN, that uses an ensemble of CNNs for binary classifications and aggregates the predictions to estimate the age label of a given face image.  The researchers showed that training an ensemble of CNNs improves the predictive performance over a single CNN with multiple binary outputs~\citep{chen2017using}, which is consistent with the well-known fact that an ensemble model can achieve better generalization performance than each individual classifier in the ensemble~\citep{raschka2019pyml3}.

Recent research has also shown that training a multi-task CNN that shares lower-layer parameters for various face analysis tasks (face detection, gender prediction, age estimation, etc.) can improve the overall performance across different tasks compared to a single-task CNN~\citep{ranjan2017all}. 

Another approach for utilizing binary classifiers for ordinal regression is the siamese CNN architecture proposed by~\cite{polania2018ordinal}, which computes the rank from pair-wise comparisons between the input image and multiple, carefully selected anchor images.



\section{Proposed method}
\label{sec:proposed}

This section describes our proposed CORAL framework that addresses the problem of classifier inconsistency in the OR-CNN by~\cite{niu2016ordinal}, which is based on multiple binary classification tasks for ranking.

\subsection{Preliminaries}

Let  be the training dataset consisting of  training examples. Here,  denotes the -th training example and  the corresponding rank, where  with ordered rank .  The ordinal regression task is to find a ranking rule  such that a loss function  is minimized. 

Let  be a  \emph{cost matrix}, where  is the cost of predicting an example  as rank ~\citep{li2007ordinal}. Typically,  and  for . In ordinal regression, we generally prefer each row of the cost matrix to be \emph{V-shaped}, that is,  if   and  if . The \emph{classification} cost matrix has entries  that do not consider ordering information. In ordinal regression, where the ranks are treated as numerical values, the \emph{absolute} cost matrix is commonly defined by .

\cite{li2007ordinal} proposed a general reduction framework for extending an ordinal regression problem into several binary classification problems. This framework requires a cost matrix that is convex in each row ( for each ) to obtain a rank-monotonic threshold model. Since the cost-related weighting of each binary task is specific for each training example, this approach is considered as infeasible in practice due to its high training complexity~\citep{niu2016ordinal}. 

Our proposed CORAL framework does neither require a cost matrix with convex-row conditions nor explicit weighting terms that depend on each training example to obtain a rank-monotonic threshold model and produce consistent predictions for each binary task. 


\subsection{Ordinal regression with a consistent rank logits model}
 
In this section, we describe our proposed consistent rank logits (CORAL) framework for ordinal regression. Subsection~\ref{sec:label-extension} describes the label extension into binary tasks used for rank prediction. The loss function of the CORAL framework is described in Subsection~\ref{sec:loss}. In subsection~\ref{sec:theoretical-guarantees}, we prove the theorem for rank consistency among the binary classification tasks that guarantee that the binary tasks produce consistently ranked predictions. 


\subsubsection{Label extension and rank prediction} 
\label{sec:label-extension}

Given a training dataset , a rank   is first extended into  binary labels  such that  indicates whether  exceeds rank , for instance, . The indicator function  is  if the inner condition is true and  otherwise. Using the extended binary labels during model training, we train a single CNN with  binary classifiers in the output layer, which is illustrated in Figure~\ref{fig:resnet}. 

Based on the binary task responses, the predicted rank label for an input  is obtained via . The rank index\footnote{While the rank label  is application-specific and defined by the user, for example {} or {}, the rank index  is an integer in the range .}  is given by

where  is the prediction of the -th binary classifier in the output layer. We require that  reflect the ordinal information and are  \emph{rank-monotonic}, , which guarantees consistent predictions. To achieve rank-monotonicity and guarantee binary classifier consistency (Theorem~\ref{th:ordered_thres}), the  binary tasks share the same weight parameters\footnote{To provide further intuition for the weight sharing requirement, we may consider a simplified version, that is, the linear form  or  with a single feature . If the weight  is not shared across the  equations, the S-shaped curves of the probability scores  will intersect, making the p_i`s non-monotone at some given input . Only if  is shared across the  equations, the S-shaped curves are horizontally shifted without intersecting.} but have independent bias units (Figure~\ref{fig:resnet}).





\subsubsection{Loss function} 
\label{sec:loss}

Let  denote the weight parameters of the neural network excluding the bias units of the final layer. The penultimate layer, whose output is denoted as , shares a single weight with all nodes in the final output layer;  independent bias units are then added to  such that  are the inputs to the corresponding binary classifiers in the final layer. Let 

be the logistic sigmoid function. The predicted empirical probability for task  is defined as
\begin{dmath}
\widehat{P}(y_i^{(k)}=1)=\sigma(g(\mathbf{x}_i,\mathbf{W})+b_k).
\end{dmath}


For model training, we minimize the loss function
\begin{dmath}\label{eq:loss_fun}
    L(\mathbf{W},\mathbf{b})=\\
    - \sum_{i=1}^N\sum_{k=1}^{K-1} \lambda^{(k)}
    \big[ \log(\sigma(g(\mathbf{x}_i,\mathbf{W})+b_k))y_i^{(k)}  \\
    + \log(1-\sigma(g(\mathbf{x}_i,\mathbf{W})+b_k))(1-y_i^{(k)}) \big],
\end{dmath}



which is the weighted cross-entropy of  binary classifiers. For rank prediction (Eq.~\ref{eq:predicted-label}), the binary labels are obtained via



In Eq.~\ref{eq:loss_fun},  denotes the weight of the loss associated with the -th classifier (assuming ). In the remainder of the paper, we refer to  as the importance parameter for task . Some tasks may be less robust or harder to optimize, which can be considered by choosing a non-uniform task weighting scheme. For simplicity, we carried out all experiments with uniform task weighting, that is, . In the next section, we provide the theoretical guarantee for classifier consistency under uniform and non-uniform task importance weighting given that the task importance weights are positive numbers.



\subsubsection{Theoretical guarantees for classifier consistency} 
\label{sec:theoretical-guarantees}

The following theorem shows that by minimizing the loss  (Eq.~\ref{eq:loss_fun}), the learned bias units of the output layer are non-increasing such that 



Consequently, the predicted confidence scores or probability estimates of the  tasks are decreasing, for instance, 


	

\noindent for all , ensuring classifier consistency. Consequently,  (Eq.~\ref{eq:proba-to-binary}) are also rank-monotonic.
\begin{theorem}[Ordered bias units]\label{th:ordered_thres}
 By minimizing the loss function defined in Eq.~\ref{eq:loss_fun}, the optimal solution  satisfies . 
\end{theorem}

\begin{proof}
Suppose  is an optimal solution and {} for some . Claim: replacing  with  , or replacing  with , decreases the objective value . Let 

By the ordering relationship, we have 

\noindent Denote  and 

Since  is increasing in , we have
 and . 

\noindent If we replace  with , the loss terms related to the -th task are updated.
The change of loss  (Eq.~\ref{eq:loss_fun}) is given as


\noindent Accordingly, if we replace  with , the change of  is given as 


\noindent By adding  and , we have

and know that either  or .
 Thus, our claim is justified. We conclude that any optimal solution  that minimizes  satisfies 
 
\end{proof}




Note that the theorem for rank-monotonicity proposed by~\cite{li2007ordinal}, in contrast to Theorem~\ref{th:ordered_thres}, requires a cost matrix  with each row  being convex. Under this convexity condition, let  be the weight of the loss associated with the -th task on the -th training example, which depends on the label . \cite{li2007ordinal} proved that by using  training example-specific task weights ,  the optimal thresholds are ordered -- ~\cite{niu2016ordinal} noted that example-specific task weights are infeasible in practice. Moreover, this assumption requires that  when  and  when . Theorem~\ref{th:ordered_thres} is free from this requirement and allows us to choose a fixed weight for each task that does not depend on the individual training examples, which greatly reduces the training complexity. Also, Theorem~\ref{th:ordered_thres} allows for choosing either a simple uniform task weighting or taking dataset imbalances into account under the guarantee of non-decreasing predicted probabilities and consistent task predictions. Under Theorem~\ref{th:ordered_thres}, the only requirement for guaranteeing rank monotonicity is that the task weights are non-negative.










\section{Experiments}
\label{sec:experiments}




\subsection{Datasets and preprocessing}
\label{sec:datasets}

The MORPH-2 dataset~\citep{ricanek2006morph}, containing 55,608 face images, was downloaded from \url{https://www.faceaginggroup.com/morph/} and preprocessed by locating the average eye-position in the respective dataset using facial landmark detection~\citep{sagonas2016300} and then aligning each image in the dataset to the average eye position using \textit{EyepadAlign} function in MLxtend~v0.14~\citep{raschka2018mlxtend}. The faces were then re-aligned such that the tip of the nose was located in the center of each image. The age labels used in this study were in the range of 16-70 years. 

The CACD dataset~\citep{chen2014cross} was downloaded from \url{http://bcsiriuschen.github.io/CARC/} and preprocessed similar to MORPH-2 such that the faces spanned the whole image with the nose tip at the center. The total number of images is 159,449 in the age range of 14-62 years. 

The Asian Face Database (AFAD) by~\cite{niu2016ordinal} was obtained from \url{https://github.com/afad-dataset/tarball}. The AFAD database used in this study contained 165,501 faces in the range of 15-40 years. Since the faces were already centered,  no further preprocessing was required.

Following the procedure described in~\cite{niu2016ordinal}, each image database was randomly divided into 80\% training data and 20\% test data. All images were resized to 1281283 pixels and then randomly cropped to 1201203 pixels to augment the model training. During model evaluation, the 1281283 RGB face images were center-cropped to a model input size of 1201203. 

We share the training and test partitions for all datasets, along with all preprocessing code used in this paper in the code repository (Section~\ref{sec:hardware-and-software}).




\subsection{Neural network architectures}
\label{sec:architecture}

To evaluate the performance of CORAL for age estimation from face images, we chose the ResNet-34 architecture~\citep{he2016deep}, which is a modern CNN architecture that achieves good performance on a variety of image classification tasks~\citep{goceri2019analysis}. For the remainder of this paper, we refer to the original ResNet-34 CNN with standard cross-entropy loss as {CE-CNN}. To implement a ResNet-34 CNN for ordinal regression using the proposed CORAL method, we replaced the last output layer with the corresponding binary tasks (Figure~\ref{fig:resnet}) and refer to this implementation as {CORAL-CNN}. Similar to {CORAL-CNN}, we modified the output layer of ResNet-34 to implement the ordinal regression reference approach described in \citep{niu2016ordinal}; we refer to this architecture as {OR-CNN}.




\subsection{Training and evaluation}


For model evaluation and comparison, we computed the mean absolute error (MAE) and root mean squared error (RMSE), on the test set after the last training epoch:


where  is the ground truth rank of the -th test example and  is the predicted rank, respectively. 
 
The model training was repeated three times with different random seeds (0, 1, and 2) for model weight initialization, while the random seeds were consistent between the different methods to allow fair comparisons. Since this study focuses on investigating rank consistency, an extensive comparison between optimization algorithms is beyond the scope of this article, so that all CNNs were trained for 200 epochs with stochastic gradient descent via adaptive moment estimation~\citep{kingma2015adam}  using exponential decay rates  and  (default settings) and a batch size of 256. To avoid introducing empirical bias by designing our own CNN architecture for comparing the ordinal regression approaches, we adopted a standard architecture (ResNet-34~\citep{he2016deep};  Section~\ref{sec:architecture}) for this comparison. Moreover, we chose a uniform task weighting for the cross-entropy of  binary classifiers in CORAL-CNN, for instance, we set  in Eq.~\ref{eq:loss_fun}.


The learning rate was determined by hyperparameter tuning on the validation set. For the various losses (cross-entropy, ordinal regression CNN~\citep{niu2016ordinal}, and the proposed CORAL method), we found that a learning rate of  performed best across all models, which is likely due to using the same base architecture (ResNet-34). All models were trained for 200 epochs. From those 200 epochs, the best model was selected via MAE performance on the validation set. The selected model was then evaluated on the independent test set, from which the reported MAE and RMSE performance values were obtained. For all reported model performances, we reported the best test set performance within the 200 training epochs. We provide the complete training logs in the source code repository (Section~\ref{sec:hardware-and-software}).



\subsection{Hardware and software}
\label{sec:hardware-and-software}
All loss functions and neural network models were implemented in PyTorch 1.5~\citep{paszke2019pytorch} and trained on NVIDIA GeForce RTX 2080Ti and Titan V graphics cards. The source code is available at \url{https://github.com/Raschka-research-group/coral-cnn}.






\begin{table*}
\begin{center}
	\caption{Age prediction errors on the test sets. All models are based on the ResNet-34 architecture.}
\begin{tabular}{|l|c|c|c|c|c|c|c|} 
\hline
\multirow{2}{*}{Method} & \multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Random\\Seed \end{tabular}}} & \multicolumn{2}{c|}{MORPH-2} & \multicolumn{2}{c|}{AFAD} & 
\multicolumn{2}{c|}{CACD} \\ 
\cline{3-8}
 & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{MAE} & \multicolumn{1}{c|}{RMSE} & MAE & \multicolumn{1}{c|}{RMSE} 
 & MAE & \multicolumn{1}{c|}{RMSE}\\
\hline
\multirow{4}{*}{\begin{tabular}[c]{@{}l@{}}CE-CNN\end{tabular}} 
 & 0 & 3.26 & 4.62 & 3.58 & 5.01 & 5.74 & 8.20 \\
 & 1 & 3.36 & 4.77 & 3.58 & 5.01 & 5.68 & 8.09 \\
 & 2 & 3.39 & 4.84 & 3.62 & 5.06 & 5.53 & 7.92  \\ 
\cline{2-8}
 & \multicolumn{1}{l|}{{ AVG  SD}} & \multicolumn{1}{l|}{3.34  0.07} & \multicolumn{1}{l|}{4.74  0.11 } & \multicolumn{1}{l|}{3.60  0.02} & \multicolumn{1}{l|}{5.03  0.03} & 
 \multicolumn{1}{l|}{5.65  0.11} & 
 \multicolumn{1}{l|}{8.07  0.14}\\
\hline
\multirow{4}{*}{\begin{tabular}[c]{@{}l@{}}OR-CNN\\ {\small\citep{niu2016ordinal}} \end{tabular}} 
 & 0 & 2.87 & 4.08 & 3.56 & 4.80 & 5.36 & 7.61\\
 & 1 & 2.81 & 3.97 & 3.48 & 4.68 & 5.40 & 7.78\\
 & 2 & 2.82 & 3.87 & 3.50 & 4.78 & 5.37 & 7.70 \\ 
\cline{2-8}
 & \multicolumn{1}{c|}{{ AVG  SD}} & \multicolumn{1}{c|}{2.83   0.03} & \multicolumn{1}{c|}{3.97  0.11} & \multicolumn{1}{c|}{3.51  0.04} & \multicolumn{1}{c|}{4.75  0.06} &
 \multicolumn{1}{l|}{5.38  0.02}& \multicolumn{1}{l|}{7.70  0.09}\\ 
\hline
\multirow{4}{*}{\begin{tabular}[c]{@{}l@{}}CORAL-CNN\\ (ours) \end{tabular}} 
 & 0 & 2.66 & 3.69 & 3.42 & 4.65 & 5.25 & 7.41 \\
 & 1 & 2.64 & 3.64 & 3.51 & 4.76 & 5.25 & 7.50 \\
 & 2 & 2.62 & 3.62 & 3.48 & 4.73 & 5.24 & 7.52\\ 
\cline{2-8}
 &\multicolumn{1}{l|}{{ AVG   SD}} & \multicolumn{1}{l|}{\textbf{2.64  0.02} } & \multicolumn{1}{l|}{\textbf{3.65  0.04} } & \multicolumn{1}{l|}{\textbf{3.47  0.05}} & \multicolumn{1}{l|}{\textbf{4.71  0.06} } &
  \multicolumn{1}{l|}{\textbf{5.25  0.01} } & 
 \multicolumn{1}{l|}{\textbf{7.48  0.06} }\\
\hline
\end{tabular}
\label{tab:all-results}
\end{center}
\end{table*}


\section{Results and discussion}
\label{sec:results}

We conducted a series of experiments on three independent face image datasets for age estimation (Section~\ref{sec:datasets}) to compare the proposed CORAL method (CORAL-CNN) with the ordinal regression approach proposed by~\cite{niu2016ordinal} (OR-CNN). All implementations were based on the {ResNet-34} architecture, as described in Section \ref{sec:architecture}. We include the standard ResNet-34 classification network with cross-entropy loss (CE-CNN) as a performance baseline. 








\begin{figure*}
\begin{center}
\centerline{\includegraphics[width=0.75\linewidth]{figures/inconsistency-examplez.pdf}}
\caption{Graphs of the predicted probabilities for each binary classifier task on four different examples from the MORPH-2 test dataset. In all cases, OR-CNN suffers from one or more inconsistencies (indicated by arrows) in contrast to CORAL-CNN.}
\label{fig:inconsistency}
\end{center}
\end{figure*}



\begin{table*}
\centering
\caption{Average numbers of inconsistencies occurred on the different test datasets for CORAL-CNN and Niu et al's Ordinal CNN. The penultimate column and last column list the average numbers of inconsistencies focusing only on the correct and incorrect age predictions, respectively.}
\label{tab:inconsistency}
\begin{tabular}{|l|c|c|c|c|} 
\cline{2-5}
\multicolumn{1}{l|}{} & CORAL-CNN & OR-CNN \citep{niu2016ordinal} & OR-CNN \citep{niu2016ordinal}& OR-CNN \citep{niu2016ordinal}\\ 
\multicolumn{1}{l|}{} & All predictions & All predictions & Only correct predictions & Only incorrect predictions \\ 
\hline
\textbf{Morph}                 &             &               &                       &                      \\
Seed 0                & 0           & 2.28          & 1.80                  & 2.37                 \\
Seed 1                & 0           & 2.08          & 1.70                  & 2.15                 \\
Seed 2                & 0           & 0.86          & 0.65                  & 0.89                 \\ 
\hline
\textbf{AFAD}                  &             &               &                       &                      \\
Seed 0                & 0           & 1.97          & 1.88                  & 1.98                \\
Seed 1                & 0           & 1.91          & 1.81                  & 1.92                 \\
Seed 2                & 0           & 1.17        & 1.02                 & 1.19                \\ 
\hline
\textbf{CACD}                  &             &               &                       &                      \\
Seed 0                & 0           & 1.24          & 0.98                  & 1.26                 \\
Seed 1                & 0           & 1.68         & 1.29                 & 1.71                 \\
Seed 2                & 0           & 0.80         & 0.63                  & 0.81                 \\
\hline
\end{tabular}
\end{table*}




\subsection{Estimating the apparent age from face images}


Across all ordinal regression datasets (Table~\ref{tab:all-results})
we found that both OR-CNN and CORAL-CNN outperform the standard cross-entropy classification loss (CE-CNN), which does not utilize the rank ordering information. Similarly, as summarized in Table~\ref{tab:all-results}, the proposed rank-consistent CORAL method shows a substantial performance improvement over OR-CNN~\citep{niu2016ordinal}, which does not guarantee classifier consistency.

Moreover, we repeated each experiment three times using different random seeds for model weight initialization and dataset shuffling to ensure that the observed performance improvement of CORAL-CNN over OR-CNN is reproducible and not coincidental. We can conclude that guaranteed classifier consistency via CORAL has a noticeable positive effect on the predictive performance of an ordinal regression CNN (a more detailed analysis of the OR-CNN's rank inconsistency is provided in Section~\ref{sec:inconsistencies}).

For all methods (CE-CNN, CORAL-CNN, and OR-CNN), the overall performance on the different datasets appeared in the following order: MORPH-2  AFAD  CACD (Table~\ref{tab:all-results}). A possible explanation is that MORPH-2 has the best overall image quality, and the photos were taken under relatively consistent lighting conditions and viewing angles. For instance, we found that AFAD includes images with very low resolutions (for example, 20x20). CACD also contains some lower-quality images. Because CACD has approximately the same size as AFAD, the overall lower performance achieved on this dataset may also be explained by the wider age range that needs to be considered (CACD: 14-62 years, AFAD: 15-40 years).


\subsection{Empirical rank inconsistency analysis}
\label{sec:inconsistencies}

By design, our proposed CORAL guarantees rank consistency (Theorem~\ref{th:ordered_thres}). In addition, we analyzed the rank inconsistency empirically for both CORAL-CNN and OR-CNN (an example of rank inconsistency is shown in Figure~\ref{fig:inconsistency}). Table~\ref{tab:inconsistency} summarizes the average numbers of rank inconsistencies for the OR-CNN and CORAL-CNN models on each test dataset. As expected, CORAL-CNN has 0 rank inconsistencies. When comparing the average numbers of rank inconsistencies considering only those cases where OR-CNN predicted the age correctly versus incorrectly, the average number of inconsistencies is higher when OR-CNN makes wrong predictions. This observation can be seen as evidence that rank inconsistency harms predictive performance. Consequently, this finding suggests that addressing rank inconsistency via CORAL is beneficial for the predictive performance of ordinal regression CNNs.




\section{Conclusions}
\label{sec:conclusions}

In this paper, we developed the CORAL framework for ordinal regression via extended binary classification with theoretical guarantees for classifier consistency. Moreover, we proved classifier consistency without requiring rank- or training label-dependent weighting schemes, which permits straightforward implementations and efficient model training. 
CORAL can be readily implemented to extend common CNN architectures for ordinal regression tasks. The experimental results showed that the CORAL framework substantially improved the predictive performance of CNNs for age estimation on three independent age estimation datasets. Our method can be readily generalized to other ordinal regression problems and different types of neural network architectures, including multilayer perceptrons and recurrent neural networks.


\section{Acknowledgments}

This research was supported by the Office of the Vice Chancellor for Research and Graduate Education at the University of Wisconsin-Madison with funding from the Wisconsin Alumni Research Foundation. Also, we thank the NVIDIA Corporation for a GPU grant to support this study.



\FloatBarrier
\clearpage
\bibliographystyle{model2-names}
\bibliography{refs}


\FloatBarrier
\clearpage

\section{Supplementary Material}


\subsection{Generalization Bounds}

Based on well-known generalization bounds for binary classification, we can derive new generalization bounds for our ordinal regression approach that apply to a wide range of practical scenarios as we only require  and  . Moreover, Theorem~\ref{th:gener-error} shows that if each binary classification task in our model generalizes well in terms of the standard 0/1-loss, the final rank prediction via  (Eq.~\ref{eq:predicted-label}) also generalizes well.
\begin{theorem}[reduction of generalization error]\label{th:gener-error}
Suppose  is the cost matrix of the original ordinal label prediction problem, with  and  for .  is the underlying distribution of , for instance, .  If the binary classification rules  obtained by optimizing Eq.~\ref{eq:loss_fun} are rank-monotonic, then



\end{theorem}
\begin{proof}
For any , we have


\noindent If , then .\\
If , then . We have 

\noindent and 

\noindent Also, 

and 

Thus,  if and only if . Since 



Similarly, if , then  and

In any case, we have 

By taking the expectation on both sides with , we arrive at Eq.~\eqref{eq:gen-bound1}.
\end{proof}

 In \cite{li2007ordinal}, by assuming the cost matrix to have V-shaped rows, the researchers define generalization bounds by constructing a discrete distribution on  conditional on each , given that the binary classifications are rank-monotonic or every row of  is convex. However, the only case they provided for the existence of rank-monotonic binary classifiers was the ordered threshold model, which requires a cost matrix with convex rows and example-specific task weights. In other words, when the cost matrix is only V-shaped but does not meet the convex row condition, for instance,  for some , the method proposed in \cite{li2007ordinal} did not provide a practical way to bound the generalization error. 
Consequently, our result does not rely on cost matrices with V-shaped or convex rows and can be applied to a broader variety of real-world use cases.


\end{document}
