

\documentclass{article}
\usepackage{arxiv,times}

\usepackage{microtype}
\usepackage{graphicx,amsthm,amsmath}
\usepackage{subfigure,amsfonts}
\usepackage{multirow}
\usepackage{booktabs} \usepackage{natbib}
\usepackage{gensymb}

\RequirePackage[textsize=scriptsize]{todonotes}

\title{Efficient Domain Generalization via Common-Specific Low-Rank Decomposition}
\author{
  Vihari Piratla \\
  MSR India\\
\And
 Praneeth Netrapalli \\
 MSR India\\
 \And
 Sunita Sarawagi \\
 IIT Bombay\\
}

\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage{algorithm,algpseudocode}
\usepackage{bm}
\usepackage{dsfont}






\begin{document}
\maketitle
























\newcommand{\lab}{\ensuremath{y}}
\newcommand{\dom}{\ensuremath{d}}
\newcommand{\vgg}{\ensuremath{g}}
\newcommand{\pg}{P_{\vgg}}
\newcommand{\labspace}{\mathcal{Y}}
\newcommand{\domspace}{\mathcal{U}}
\newcommand{\ndom}{}
\newcommand{\dats}{\mathcal{D}_s}
\newcommand{\datt}{\mathcal{D}_t}
\newcommand{\model}{\mathcal{M}}
\newcommand{\func}{f}
\newcommand{\mos}{CSD}
\newcommand{\mosdescr}{Common-specific Decomposition.}
\newcommand{\mosr}{CSD-rand}
\newcommand{\cR}{\mathbf{r}_x}
\newcommand{\R}{\mathbb{R}}
\newcommand{\numC}{C}
\newcommand{\rank}{k}
\newcommand{\Span}[1]{\textrm{Span}\left({#1}\right)}
\newcommand{\Rank}[1]{\textrm{rank}\left({#1}\right)}
\newcommand{\defeq}{:=}
\newcommand{\What}{\widehat{W}}
\newcommand{\vtilde}{\widetilde{v}}
\newcommand{\iprod}[2]{\langle #1, #2 \rangle}
\newcommand{\norm}[1]{\left\|{#1} \right\|}
\newcommand{\ones}{\mathds{1}}
\newcommand{\frob}[1]{\left\|{#1} \right\|_F}
\newcommand{\trans}[1]{{#1}^{\top}}
\newcommand{\praneeth}[1]{{\color{red} PN: #1}}
\newcommand{\vihari}[1]{{\color{blue} Vihari: #1}}
\newcommand{\sunita}[1]{{\color{magenta} Sunita: #1}}
\newtheorem{theorem}{Theorem}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newcommand{\wtilde}{\widetilde{w}}
\newcommand{\Utilde}{\widetilde{U}}
\newcommand{\Stilde}{\widetilde{\Sigma}}
\newcommand{\Vtilde}{\widetilde{V}}
\newcommand{\Wtilde}{\widetilde{W}}





\begin{abstract}
Domain generalization refers to the task of training a model which generalizes to new domains that are not seen during training. We present CSD (Common Specific Decomposition), for this setting, which \emph{jointly} learns a common component (which generalizes to new domains) and a domain specific component (which overfits on training domains). The domain specific components are discarded after training and only the common component is retained. The algorithm is extremely simple and involves only modifying the final linear classification layer of any given neural network architecture.
We present a principled analysis to understand existing approaches, provide identifiability results of CSD, and study effect of low-rank on domain generalization.
We show that CSD either matches or beats state of the art approaches for domain generalization based on domain erasure,  domain perturbed data augmentation, and meta-learning.
Further diagnostics on rotated MNIST, where domains are interpretable, confirm the hypothesis that CSD successfully disentangles common and domain specific components and hence leads to better domain generalization.
The code and datasets can be found at the following URL: \url{https://github.com/vihari/CSD}.

\end{abstract}



\section{Introduction}
In the domain generalization (DG) task we are given domain-demarcated data from multiple domains during training, and our goal is to create a model that will generalize to instances from new domains during testing.  Unlike in the more popular domain adaptation task~\cite{MansourMA09,Ben-David:2006:ARD:2976456.2976474,Kumar2010} that explicitly adapts to a fixed target domain, DG requires zero-shot generalization to individual instances from multiple new domains.  Domain generalization is of particular significance in large-scale deep learning networks because large training sets often entail aggregation from multiple distinct domains, on which today's high-capacity networks easily overfit.  Standard methods of regularization only address generalization to unseen instances sampled from the distribution of training domains, and have been shown to perform poorly on instances from unseen domains.

Domain Generalization approaches have a rich history. Earlier methods were simpler and either sought to learn feature representations that were invariant across domains~\cite{motiian2017CCSA,MuandetBS13,GhifaryBZB15,WangZZ2019}, or decomposed parameters into shared and domain-specific components~\cite{ECCV12_Khosla,LiYSH17}.  Of late however the methods proposed for DG are significantly more complicated and expensive.  A recent favorite is gradient-based meta-learning that trains with sampled domain pairs to minimize either loss on one domain while updating parameters on another domain~\cite{BalajiSR2018,liYY2018}, or minimizes the divergence between their representations~\cite{DouCK19}.  Another approach is to augment training data with domain adversarial perturbations~\cite{VihariSSS18}. 




\textbf{Contributions}\\
Our paper starts with analyzing the domain generalization problem in a simple and natural generative setting.  We use this setting to provide a principled understanding of existing DG approaches and improve upon prior decomposition methods.  We design an algorithm \mos\ that decomposes only the last softmax parameters into a common component and  a low-rank domain-specific component with a regularizer to promote orthogonality of the two parts.  We prove identifiability of the shared parameters, which was missing in earlier decomposition-based approaches.  We analytically study the effect of rank in trading off domain-specific noise suppression and domain generalization, which in earlier work was largely heuristics-driven.  

We show that our method is almost an order of magnitude faster than state-of-the-art meta-learning based methods~\cite{DouCK19}, and provides higher accuracy than existing approaches, particularly when the number of domains is large.  Our experiments span both image and speech datasets and a large range of training domains (5 to 1000), in contrast to recent DG approaches evaluated only on a few domains.  
We provide empirical insights on the working of CSD on the rotated MNIST datasets where the domains are interpretable, and show that CSD indeed manages to separate out the generalizable shared parameters while training with simple domain-specific losses. We present an ablation study to evaluate the importance of the different terms in our training objective that led to improvements with regard to earlier decomposition approaches.













\section{Related Work}
The work on Domain Generalization is broadly characterized by four major themes:

\paragraph{Domain Erasure}
Many early approaches attempted to repair the feature representations so as to reduce divergence between representations of different training domains.   \citet{MuandetBS13} learns a kernel-based domain-invariant representation. \citet{GhifaryBZB15} estimates shared features by jointly learning multiple data-reconstruction tasks.   \citet{Li2018DomainGW} uses MMD to maximize the match in the feature distribution of two different domains. The  idea of domain erasure is further specialized in \cite{WangZZ2019} by trying to project superficial (say textural) features out using image specific kernels. 
Domain erasure is also the founding idea behind many domain adaptation approaches, example ~\cite{Ben-David:2006:ARD:2976456.2976474,HoffmanMN18,Ganin16} to name a few.



\paragraph{Augmentation} The idea behind these approaches is to train the classifier with instances obtained by domains hallucinated from the training domains, and thus make the network `ready' for these neighboring domains. \cite{VihariSSS18} proposes to augment training data with instances perturbed along directions of domain change. A second classifier is trained in parallel to capture directions of domain change. 
\citet{VolpiNSDM2018} applies such augmentation on single domain data.  Another type of augmentation is to simultaneously solve for an auxiliary task.  For example, \citet{CarlucciAS2019} achieves domain generalization for images by solving an auxiliary unsupervised jig-saw puzzle on the side.  



\paragraph{Meta-Learning/Meta-Training}
A recent popular approach is to pose the problem as a meta-learning task, whereby we update parameters using meta-train loss but simultaneously minimizing meta-test loss~\cite{liYY2018},\cite{BalajiSR2018} or learn discriminative features that will allow for semantic coherence across meta-train and meta-test domains~\cite{DouCK19}.  More recently, this problem is being pursued in the spirit of estimating an invariant optimizer across different domains~ and solved by a form of meta-learning in \cite{ArjovskyLID19}. Meta-learning approaches are complicated to implement, and slow to train.  

\paragraph{Decomposition} In these approaches the parameters of the network are expressed as the sum of a common parameter and  domain-specific parameters during training.  \citet{Daume2007} first applied this idea for domain adaptation.  \citet{ECCV12_Khosla} applied decomposition to DG by retaining only the common parameter for inference. \citet{LiYSH17} extended this work to CNNs where each layer of the network was decomposed into common and specific low-rank components.  
Our work provides a principled understanding of when and why these methods might work and uses this understanding to design an improved algorithm \mos.  Three key differences are:  \mos\ decomposes only the last layer, imposes loss on both the common and domain-specific parameters, and constrains the two parts to be orthogonal.  We show that orthogonality is required for theoretically proving identifiability. As a result, this newer avatar of an old decomposition-based approach surpasses recent, more involved augmentation and meta-learning approaches. 








    
\section{Our Approach}
Our approach is guided by the following assumption about domain generalization settings.

\textbf{Assumption}: There are common features in the data whose correlation with label is consistent across domains and domain specific features whose correlation with label varies (from positive to negative) across domains. Classifiers that rely on common features generalize to new unseen domains far better than those that rely on domain specific features.

Note that we make \emph{no} assumptions about a) the domain predictive power of common features and b) the net correlation between domain specific features and the label. Let us consider the following simple example which illustrates these points. There are  training domains and examples  from domain  are generated as follows:

where  with equal probability,  is the dimension of the training examples,  is a common feature whose correlation with the label is constant across domains and  is a domain specific feature whose correlation with the label, given by the coefficients , varies from domain to domain.
In particular, for each domain , suppose . Note that though  vary from positive to negative across various domains, there is a net positive correlation between  and the label .  denotes a  standard normal random variable with mean zero and covariance matrix . Since  varies across domains, every feature  
captures some domain information. {We note that the assumption  is indeed restrictive -- we use it here only to make the discussion and expressions simpler. We relax this assumption later in this section when we discuss the identifiability of domain generalizing classifier.}




Our assumption at the beginning of this section (which envisages the possibility of seeing  at test time) implies that the only classifier that generalizes to new domains is one that depends solely on  \footnote{Note that this last statement relies on the assumption that . 
If this is not the case, the correct domain generalizing classifier is the component of  that is orthogonal to  i.e., . See~\eqref{eq:proj}.}. Consider training a linear classifier on this dataset. We will describe the issues faced by existing domain generalization methods.
\begin{itemize}
    \item \textbf{Empirical risk minimization (ERM)}: When we empirically train a linear classifier using ERM with cross entropy loss on all of the training data,
the resulting classifier puts significant nonzero weight on the domain specific component . The reason for this is that there is a bias in the training data which gives an overall positive correlation between  and the label.
    \item \textbf{Domain erasure}  \cite{Ganin16}: Domain erasure methods seek to extract features that have the same distribution across different domains and construct a classifier using those features. The difference in noise variance in~\eqref{eqn:synth-example} across domains means that all features have domain signal. In fact, linear classifiers on any feature 
can obtain nontrivial domain classification accuracy. 
    The premise of domain erasure methods, that there exist features which have high prediction power of the label but do not capture domain information, does not apply in this setting and domain erasure methods do not perform well.
\item \textbf{Domain adversarial perturbations~\cite{VihariSSS18}}: Domain adversarial perturbations seek to augment the training dataset with adversarial examples obtained using domain classification loss, and train a classifier on the resulting augmented dataset. Since the common component  has domain signal, the adversarial examples will induce variation in this component and so the resulting classifier puts less weight on the common component.
     \item \textbf{Meta-learning}:
Meta-learning based DG approaches such as \cite{DouCK19} work with pairs of domains. Parameters updated using gradients on loss of one domain, when applied on samples of both domains in the pair should lead to similar class distributions.  If the method used to detect similarity is robust to domain-specific noise, meta-learning methods could work well in this setting.  But meta-learning methods require second order gradient updates, and/or are generally considered expensive to implement.     
\end{itemize}

\textbf{Decomposition based approaches~\cite{ECCV12_Khosla,LiYSH17}}: Decomposition based approaches rely on the observation that for problems like~\eqref{eqn:synth-example}, there exist good domain specific classifiers , one for each domain , such that:

where  is a function of . Note that all these domain specific classifiers share the common component  which is the domain generalizing classifier that we are looking for! If we are able to find domain specific classifiers of the form~\eqref{eqn:specific-classifiers}, we can extract  from them. This idea can be extended to a generalized version of~\eqref{eqn:synth-example}, where the latent dimension of the domain space is  i.e., say

, {and  for } are domain specific features whose correlation with the label, given by the coefficients , varies from domain to domain. In this setting, there exist good domain specific classifiers  such that:

where  is a domain generalizing classifier,  consists of domain specific components
and  is a domain specific combination of the domain specific components that depends on  for . With this observation, the algorithm is simple to state: train domain specific classifiers  that can be represented as

Here the training variables are  and . After training, discard all the domain specific components  and  and return the common classifier .
Note that~\eqref{eqn:classifier-decomp-1} can equivalently be written as

where ,  is the all ones vector and .

This framing of the decomposition approach, in the context of simple and concrete examples as in~\eqref{eqn:synth-example} and~\eqref{eqn:synth-example-k}, lets us understand the three main aspects that are not properly addressed by prior works in this space: 1) identifiability of , 2) choice of low rank and 3) extension to non-linear models such as neural networks.


\textbf{Identifiability of the common component }: None of the prior decomposition based approaches investigate identifiability of . In fact, given a general matrix  which can be written as , there are multiple ways of decomposing  into this form, so  cannot be uniquely determined by this decomposition alone. For example, given a decomposition~\eqref{eqn:classifier-decomp}, for any  invertible matrix , we can write . As long as the first row of  is equal to , the structure of the decomposition~\eqref{eqn:classifier-decomp} is preserved while  might no longer be the same. Out of all the different  that can be obtained this way, which one is the \emph{correct domain generalizing classifier}? In the setting of~\eqref{eqn:synth-example-k}, where , we proposed that the correct domain generalizing classifier is . In the setting where , we propose that
the correct domain generalizing classifier is the projection of  onto the space orthogonal to  i.e.,

where  is the projection matrix onto the span of the domain specific vectors . The following lemma characterizes this condition in terms of the decomposition~\eqref{eqn:classifier-decomp}.
\begin{lemma}\label{lem:char}
Suppose  is a rank- matrix, where  and  are all rank- matrices with . Then,  if and only if .
\end{lemma}
\begin{proof}
\textbf{If direction}: 
Suppose . Then, . Since  is a rank- matrix, we know that  and so it has to be the case that  and . Both of these together imply that  is the projection of  onto the space orthogonal to  i.e., .

\textbf{Only if direction}: Let . Then  is a rank- matrix and can be written as  with . Since , we also have .
\end{proof}
So we train for classifiers~\eqref{eqn:classifier-decomp} satisfying .


\textbf{Why low rank?}: An important choice in the decomposition approaches is the \emph{low} rank of the decomposition~\eqref{eqn:classifier-decomp}, which in prior works was justified heuristically, by appealing to number of parameters.
We prove the following result, which gives us a more principled reason for the choice of low rank parameter . \begin{theorem}\label{thm:main}
Given any matrix , the minimizers of the function , where  and  can be computed by the following steps:
\begin{itemize}
    \item .
    \item .
    \item .
    \item .
    \item Output 
\end{itemize}
\end{theorem}
The proof of this theorem is similar to that of the classical low rank approximation theorem of Eckart-Young-Mirsky, and is presented in the supplementary material.
As special cases of the above result, we see that for , we just obtain the average classifier over all domains , while for , we obtain . When , where  is a noise matrix (for example due to finite samples), both extremes  and  have different advantages/drawbacks:
\begin{itemize}
    \item \textbf{}: The averaging effectively reduces the noise component  but ends up retaining some domain specific components if there is net correlation with the label in the training data.
    \item \textbf{}: The pseudoinverse effectively removes domain specific components and retains only the common component (by Theorem~\ref{thm:main}).
However, the pseudoinverse does not reduce noise to the same extent as a plain averaging would (since empirical mean is often asymptotically the best estimator for mean).
\end{itemize}
In general, the sweet spot for  lies between  and  and its precise value depends on the dimension and magnitude of the domain specific components as well as the magnitude of noise.
In our implementation, we perform cross validation to choose a good value for  but also note that the performance of our algorithm is relatively stable with respect to this choice in Section~\ref{sec:lowrank}.

















































%
 

\textbf{Extension to neural networks}:
Finally, prior works extend this approach to non-linear models such as neural networks by imposing decomposition of the form~\eqref{eqn:classifier-decomp} for parameters in all layers separately. This increases the size of the model significantly and leads to worse generalization performance. Further, it is not clear whether any of the insights we gained above for linear models continue to hold when we include non-linearities and stack them together. So, we propose the following two simple modifications instead:
\begin{itemize}
    \item enforcing the structure~\eqref{eqn:classifier-decomp} only in the final linear layer, as opposed to the standard single softmax layer, and
\item including a loss term for predictions of common component, in addition to the domain specific losses,
\end{itemize}
both of which encourage learning of \emph{features} with common-specific structure.

Our experiments (Section~\ref{sec:ablation}) show that these modifications (orthogonality, changing only the final linear layer and including common loss) are instrumental in making decomposition methods state of the art for domain generalization. Our overall training algorithm below details the steps.

\subsection{Algorithm CSD}
Our method of training neural networks for domain generalization appears as Algorithm~\ref{alg:mos} and is called CSD for \mosdescr  The analysis above was for the binary setting, but we present the algorithm for the multi-class case with   \# classes.
The only extra parameters that CSD requires, beyond normal feature parameters  and softmax parameters , are the domain-specific low-rank parameters   and , for . Here  is the representation size in the penultimate layer. Thus,  can be viewed as a domain-specific embedding matrix of size .  Note that unlike a standard mixture of softmax, the  values are not required to be on the simplex.  
Each training instance consists of an input , true label , and a domain identifier  from 1 to .  Its domain-specific softmax parameter is computed by .

Instead of first computing the full-rank parameters and then performing SVD, we directly compute the low-rank decomposition along with training the network parameters .  For this we add a weighted combination of these three terms in our training objective:

(1) Orthonormality regularizers to make  orthogonal to domain-specific  softmax parameters for each label  and to avoid degeneracy by  controlling the norm of each softmax parameter to be close to 1.
    
    (2) A cross-entropy loss between  and distribution computed from the  parameters to train both the common and low-rank domain-specific parameters.
    
    (3) A cross-entropy loss between  and distribution computed from the  parameters. This loss might appear like normal ERM loss but when coupled with the orthogonality regularizer above it achieves domain generalization.



















\begin{algorithm}
\caption{Common-Specific Low-Rank Decomposition (\mos\ ) }
\label{alg:mos}
\begin{algorithmic}[1]
\State {\bf Given:} ,train-data
\State {Initialize params }
\State {Initialize  }
\State {Initialize params  of feature network }
\State{}
\State{} 
     \Comment{Orthonormality constraint}
\For{ train-data}


   \State{}
\State{loss }
\EndFor

\State{Optimize loss wrt }
\State{\bf Return  \Comment{for inference}}
  \end{algorithmic}
\end{algorithm}











\subsection{Synthetic setting: comparing CSD with ERM}
We use the data model proposed in Equation~\ref{eqn:synth-example} to simulate multi-domain training data with  domains and  features. For each domain, we sample  uniformly from -1, 2 and  uniformly from 0, 1. We set  and . We sample 100 data points for each domain using its corresponding values: . We then fit the parameters of a linear classifier with log loss using either standard expected risk minimization (ERM) estimator or \mos\ . 

The scaled solution obtained using ERM is [1, 0.2] and [1, 0.03] using \mos\ with high probability from ten runs. As expected, the solution of ERM has a positive but small coefficient on the second component due to the net positive correlation on this component. \mos\ on the other hand correctly decomposed the common component.

\iffalse
\subsection{How does it work?}
An easy fix to ensure there is no domain specialization is to reduce the dimension of the representations, that is by reducing the total number of features available, however that also reduces the expressivity of the model. Another approach is to ensure that each of the domain specific classifier are largely overlapping thereby sharing features and is our primary idea. 
How do we expect the representations/classifier to change across domain? Irrespective of the number of training domains, they are only discrete manifestations of the latent low-dimensional continuous space of domain embeddings, such as stroke width, color, pressure on the tip, inclination of the pen for a handwritten character dataset. Since it could be hard to completely disentangle such style correlation, we expect the representations and hence the classifier to change from one domain to another, however since the intervention from domain is restricted to only a small dimension, we also expect the representations of different domain to reflect this low-rank structure. 

The gradient update from a single example on the last layer can be imagined to have three components: the component that is specific to just this example: , specific to the class of examples in the same domain:  and one that can generalize beyond the example, domain: . Each of these directions are expected to be orthogonal to each other, since by definition to achieve the order of specificity. The gradient update for a particular example thereby is a combination of these three components in some proportions: . The final linear classifier obtained using a mean square reduction will take the form: , where each of  and  are the aggregate of  and  over several examples and domains respectively. As we increase the number of domains while keeping the size of representations fixed, it limits the number of orthogonal components in , thereby increasing the coefficient of the common component: . This aligns with the observation of diminishing gains observed when we increase the number of domains in speech: table~\ref{tab:speech}. In order to perform on out of domain samples, we need to erase the specific component:  and thereby encourage the coefficient of common: .

When the data is split in to non-overlapping partitions, the specific classifier for each of the data partition is expected to be of the form:  for a partition . However, since we force each of the partition specific classifier to lie on a very low dimensional subspace through the rank constraint, we expect either the specific directions () and their coefficients () to be same across partitions or we reduce the number of specific directions in each of  are reduced thereby boosting the common coefficient: .

Since we jointly learn the common classifier that performs on all the instances, we wish to bound the divergence of the common classifier to each of the domain specific classifier, i.e. . The second term in the expression is the form of common softmax. When the number of domains are large compared to largest norm of , then reducing the divergence as shown above amounts to diminishing  corresponding to the domain with largest domain specific classifier. It is expected then that one of the basis vectors specializes on the left out domain. Hence, as we increase the number of basis softmax vectors, we erase more domain specific classifier components, however increasing the number of basis vectors beyond certain saturation point is expected to give diminishing gains and increase the training time because of larger trainable parameters.
\fi

\iffalse
\begin{figure}[t]
  \subfigure[y vs first component]{
    \includegraphics[width=0.42\columnwidth]{images/y_vs_x1.png}
    \label{fig:toy:1}
  }
  \qquad
  \subfigure[y vs second component]{
    \includegraphics[width=0.42\columnwidth]{images/y_vs_x2.png}
    \label{fig:toy:2}
  }
  \caption{}
  \label{fig:toy}
\end{figure}


\subsection{Toy Dataset}
We consider a simple toy setting to illustrate the working of our algorithm. The input data is set to three dimensional and is sampled from a unit normal. Target  is generated from input, , using a linear model of the form:  where  is a 3-dimensional vector that is common across all the domains and  is again a 3-dimensional vector that is further multiplied by the domain specific scalar: . The train data for regression task constitutes of ten domains. The scalar  is uniformly sampled from 3 different non-overlapping ranges for train, test and dev splits i.e. [-1, 0], [0, 1], [1, 2] respectively. For the sake of simplicity  is fixed at [1, 0, 0] and  is [0, 1 1]. Figure~\ref{fig:toy} shows first and second components with respect to the target variable for data so generated. As can be seen in figure~\ref{fig:toy:1}, the distribution of different splits are aligned on the first component and show considerable drift across splits for second (figure~\ref{fig:toy:2}) and third components. 

The optimal regressor for each domain that uses the scalar  is , since the value of the optimal classifier varies along the dimensions of , it is not hard to see that the optimal domain generalizable solution is . That is  that satisfies  is  in this case. Vanilla Expected Risk Minimization (ERM) finds optimum at the average of optimal classifier for each of the domains in the train split which comes out to be [1, -0.5, -0.5]. We then use the MoS algorithm shown in algorithm~\ref{alg:mos} with  and  tuned using the development split; This finds the optimum at [0.97 0.03 0.03].

\todo[inline]{Nice example!  Use same notation as the in the algorithm?}
\fi






\section{Experiments}
We compare our method with three existing domain generalization methods: (1) {\bf MASF}~\cite{DouCK19} is a recently proposed meta-learning based strategy to learn domain-invariant features.  (2) {\bf CG}: As a representative of methods that augment data for domain generalization we compare with \cite{VihariSSS18}, and (3) {\bf LRD}: the low-rank decomposition approach of \cite{LiYSH17} but only at the last softmax layer.
Our baseline is standard expected risk minimization (ERM) using cross-entropy loss that ignores domain boundaries altogether. 

We evaluate on five different datasets spanning image and speech data types and varying number of training domains. We assess quality of domain generalization as accuracy on a set of test domains that are disjoint from the set of training domains.

\textbf{Experiment setup details}
We use ResNet-18 to evaluate on rotated image tasks, LeNet for Handwritten Character datasets, and a multi-layer convolution network similar to what was used for Speech tasks in~\cite{VihariSSS18}. 
We added a layer normalization just before the final layer in all these networks since it helped generalization error on all methods, including the baseline. \mos\ is relatively stable to hyper-parameter choice, we set the default rank to 1, and parameters of weighted loss to  and . These hyper-parameters along with learning rates of all other methods as well as number of meta-train/meta-test domains for MASF and step size of perturbation in CG are all picked using a task-specific development set. Further, we scale  using sigmoid activation. 

\textbf{Handwritten character datasets:}
In these datasets we have characters  written by many different people, where the person writing serves as domain and generalizing to new writers is a natural requirement. Handwriting datasets are challenging since it is difficult to disentangle a person's writing style from the character (label), and methods that attempt to erase domains are unlikely to succeed.
We have two such datasets.

(1) The LipitK dataset\footnote{\url{http://lipitk.sourceforge.net/datasets/dvngchardata.htm}} earlier used in \cite{VihariSSS18} is a Devanagari Character dataset which has classification over 111 characters (label) collected from 106 people (domain).   We train three different models on each of 25, 50, and 76 domains, and test on a disjoint set of 20 domains while using 10 domains for validation.

(2) Nepali Hand Written Character Dataset (NepaliC)\footnote{\url{https://www.kaggle.com/ashokpant/devanagari-character-dataset}} contains data collected from 41 different people on consonants as the character set which has 36 classes. Since the number of available domains is small, in this case we create a fixed split of 27 domains for training, 5 for validation and remaining 9 for testing. 

We use LeNet as the base classifier on both the datasets. 

\begin{table*}[htb]
    \centering
    \begin{tabular}{|l|lll|l|}
    \toprule
     & \multicolumn{3}{|c|}{LipitK} & \multicolumn{1}{|c|}{NepaliC} \\
    \hline
Method & 25 & 50 & 76 & 27 \\
\hline
ERM~(Baseline) & 74.5 (0.4) & 83.2 (0.8) & 85.5 (0.7) & 83.4 (0.4) \\
LRD~\cite{LiYSH17} & 76.2 (0.7) & 83.2 (0.4) & 84.4 (0.2) & 82.5 (0.5) \\
CG~\cite{VihariSSS18} & 75.3 (0.5) & 83.8 (0.3) & 85.5 (0.3) & 82.6 (0.5) \\
MASF~\cite{DouCK19} & {\bf 78.5} (0.5) & 84.3 (0.3) & 85.9 (0.3) & 83.3 (1.6) \\
\mos~(Ours) & 77.6 (0.4) & {\bf 85.1} (0.6) & {\bf 87.3} (0.4) & {\bf 84.1} (0.5) \\

\hline
    
    \end{tabular} 
    \caption{Comparison of our method on two handwritting datasets: LipitK and NepaliC.  For LipitK  since number of available training domains is large we also report results with increasing number of domains. The numbers are average (and standard deviation) from three runs. }
    \label{tab:image}
\end{table*}

In Table~\ref{tab:image} we show the accuracy using different methods for different number of training domains on the LipitK dataset, and on the Nepali dataset.  We observe that across all four models \mos\ provides significant gains in accuracy over the baseline (ERM), and all three existing methods LRD, CG and MASF.  The gap between prior decomposition-based approach (LRD) and ours, establishes the importance of our orthogonality regularizer and common loss term.   MASF is better than \mos\ only for 25 domains and as the number of domains increases to 76, \mos's accuracy is 87.3 whereas MASF's is 85.9.  



In terms of training time MASF is 5--10 times slower than \mos, and CG is 3--4 times slower than \mos. In contrast \mos\ is just 1.1 times slower than ERM.  Thus, the increased generalization of \mos\ incurs little additional overheads in terms of training time compared to existing methods. 

\begin{table}[htb]
    \centering
    \begin{tabular}{|l|r|r|r|r|}
    \hline
    Method & 50 & 100 & 200 & 1000 \\
    \hline
    ERM & 72.6 (.1) & 80.0 (.1) & 86.8 (.3) & 90.8 (.2) \\
    CG & 73.3 (.1) & 80.4 (.0) & 86.9 (.4) & 91.2 (.2) \\
    \mos & {\bf 73.7} (.1) & {\bf 81.4} (.4) & {\bf 87.5} (.1) & {\bf 91.3} (.2) \\
    \hline
    \end{tabular}
    \caption{Accuracy comparison on speech utterance data with varying number of training domains. The numbers are average (and standard deviation) from three runs.}
    \label{tab:speech}
\end{table}

\textbf{Speech utterances dataset}
We use the utterance data released by Google which was also used in \cite{VihariSSS18} and is collected from a large number of subjects\footnote{\url{https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html}}. The base classifier and the preprocessing pipeline for the utterances are borrowed from the implementation provided in the Tensorflow examples\footnote{\url{https://github.com/tensorflow/tensorflow/tree/r1.15/tensorflow/examples/speech_commands}}. We used the default ten (of the 30 total) classes for classification similar to \cite{VihariSSS18}. We use ten percent of total number of domains for each of validation and test. 

The accuracy comparison for each of the methods on varying number of training domains is shown in Table~\ref{tab:speech}. We could not compare with MASF since their implementation is only made available for image tasks.  Also, we skip comparison with LRD since earlier experiments established that it can be worse than even the baseline.  
Table~\ref{tab:speech} shows that CSD is better than  both the baseline and CG on all domain settings.  When the number of domains is very large (for example, 1000 in the table), even standard training can suffice since the training domains could 'cover' the variations in the test domains.


\begin{table*}[htb]
\begin{minipage}{0.55\linewidth}
    \begin{tabular}{|l|l|l|l|l|}
    \hline
     & \multicolumn{2}{|c|}{MNIST} & \multicolumn{2}{|c|}{Fashion-MNIST} \\
     & in-domain & out-domain & in-domain & out-domain \\
    \hline
ERM & 98.3 (0.0) & 93.6 (0.7) & 89.5 (0.1) & 76.5 (0.7)\\
    MASF & 98.2 (0.1) & 93.2 (0.2) &  86.9 (0.3) & 72.4 (2.9)\\
    \mos & \textbf{98.4} (0.0) & \textbf{94.7} (0.2) & \textbf{89.7} (0.2) & \textbf{78.9} (0.7)\\
    \hline
    \end{tabular}
    \caption{Performance comparison on rotated MNIST and rotated Fashion-MNIST, shown are the in-domain and out-domain accuracies averaged over three runs along with standard deviation in the brackets.}
    \label{tab:toy:results}
\end{minipage}\hfill
\begin{minipage}{0.4\linewidth}
\centering
    \begin{tabular}{|l|r|r|}
    \hline
        & \multicolumn{2}{|c|}{MNIST} \\
         & in-domain & out-domain \\\hline
         ERM & 97.7 (0.) & 89.0 (.8) \\ 
         MASF & 97.8 (0.) & 89.5 (.6) \\
         \mos\ & \textbf{97.8} (0.) & \textbf{90.8} (.3) \\\hline
    \end{tabular}
    \caption{In-domain and out-domain accuracies on rotated MNIST without batch augmentations. Shown are average and standard deviation from three runs.}
    \label{tab:expt:rmnist:woaug}
\end{minipage}
\end{table*}

\paragraph{Rotated MNIST and Fashion-MNIST:}
\label{sec:expt:rotation}
Rotated MNIST is a popular benchmark for evaluating domain generalization where the angle by which images are rotated is the proxy for domain.
We randomly select\footnote{ The earlier work on this dataset however lacks standardization of splits, train sizes, and baseline network across the various papers~\cite{VihariSSS18}~\cite{WangZZ2019}. Hence we rerun experiments using different methods on our split and baseline network.
} 
a subset of 2000 images for MNIST and 10,000 images for Fashion MNIST, the original set of images is considered to have rotated by 0{\degree} and is denoted as . Each of the images in the data split when  rotated by  degrees is denoted . The training data is union of all images rotated by 15{\degree} through 75{\degree} in intervals of 15{\degree}, creating a total of 5 domains.  We evaluate on .  In that sense only in this artificially created domains, are we truly sure of the test domains being outside the span of train domains.
Further, we employ batch augmentations such as flip left-right and random crop since they significantly improve generalization error and are commonly used in practice. We train using the ResNet-18 architecture.  

Table~\ref{tab:toy:results} compares the baseline, MASF, and \mos\ on MNIST and Fashion-MNIST. We show accuracy on test set from the same domains as training (in-domain) and test set from 0{\degree} and 90{\degree} that are outside the training domains. 
Note how the \mos's improvement on in-domain accuracy is insignificant, while gaining substantially on out of domain data.  This shows that CSD specifically targets domain generalization.  Surprisingly MASF does not perform well at all, and is significantly worse than even the baseline.  One possibility could be that the domain-invariance loss introduced by MASF conflicts with the standard data augmentations used on this dataset. To test this, we compared all the methods without such augmentation.  We observe that although all numbers have dropped 1--4\%, now MASF is showing sane improvements over baseline, but \mos\ is better than MASF even in this setting.  







\subsection{How does \mos\ work?}
\begin{figure*}[htb]
  \subfigure[Beta fit on estimated probabilities of correct class using common common component.]{
    \includegraphics[width=0.42\textwidth]{images/fig_1.png}
    \label{fig:rmnist:1}
  }
  \qquad
  \subfigure[Beta fit on estimated probabilities of correct class using specialized component.]{
    \includegraphics[width=0.42\textwidth]{images/fig_2.png}
    \label{fig:rmnist:2}
  }
  \caption{Distribution of probability assigned to the correct class using common or specialized components alone.}
  \label{fig:rmnist}
\end{figure*}


We provide empirical evidence in this section that \mos\ effectively decomposes common and low-rank specialized components. Consider the rotated MNIST task trained on ResNet-18 as discussed in Section~\ref{sec:expt:rotation}. Since each domain differs only in the amount of rotation, we expect  to be of rank 1 and so we chose  giving us one common and one specialized component. We are interested in finding out if the common component is agnostic to the domains and see how the specialized component varies across domains. 

We look at the probability assigned to the correct class for all the train instances using only common component  and using only specialized component . For probabilities assigned to examples in each domain using each component, we fit a Beta distribution. Shown in Figure~\ref{fig:rmnist:1} is fitted beta distribution on probability assigned using  and Figure~\ref{fig:rmnist:2} for . Note how in Figure~\ref{fig:rmnist:1}, the colors are distinguishable, yet are largely overlapping. However in Figure~\ref{fig:rmnist:2}, notice how modes corresponding to each domain are widely spaced, moreover the order of modes and spacing between them cleanly reflects the underlying degree of rotation from 15\degree\ to 75. 



These observations support our claims on utility of \mos\ for low-rank decomposition.





















\subsection{Ablation study}\label{sec:ablation}

In this section we study the importance of each of the three terms in CSD's final loss: common loss computed from  (), specialized loss  () computed from  that sums common () and domain-specific parameters (), orthonormal loss () that makes  orthogonal to domain specific softmax (Refer: Algorithm~1). In Table~\ref{tab:ablation}, we demonstrate the contribution of each term to \mos\ loss by comparing accuracy on LipitK with 76 domains. 



\begin{table}[htb]
    \centering
    \begin{tabular}{|c|c|c|r|}
    \hline
    Common  & Specialized  & Orthonormality & Accuracy \\
    loss  & loss  & regularizer  &  \\
    \hline
    Y & N & N & 85.5 (.7) \\
    N & Y & N & 84.4 (.2) \\
    N & Y & Y & 85.3 (.1) \\
    Y & N & Y & 85.7 (.4) \\
    Y & Y & N & 85.8 (.6) \\
    Y & Y & Y & 87.3 (.3) \\
    \hline
    \end{tabular}
    \caption{Ablation analysis on \mos\ loss using LipitK (76)}
    \label{tab:ablation}
\end{table}

The first row is the baseline with only the common loss.  The second row shows prior
decomposition methods that imposed only the specialized loss without any orthogonality or a separate common loss. This is worse than even the baseline.  
This can be attributed to decomposition without identifiability guarantees thereby losing part of  when the specialized  is discarded. Using orthogonal constraint, third row, fixes this ill-posed decomposition however, understandably, just fixing the last layer does not gain big over baseline. Using both common and specialized loss even without orthogonal constraint showed some merit, perhaps because feature sharing from common loss covered up for bad decomposition. Finally, fixing this bad decomposition with orthogonality constraint and using both common and specialized loss constitutes our \mos\ algorithm and is significantly better than any other variant.

This empirical study goes on to show that both  and  are important. Imposing  with  does not help feature sharing if it is not devoid of specialized components from bad decomposition. A good decomposition on final layer without  does not help generalize much. 

\subsection{Importance of Low-Rank}
\label{sec:lowrank}
Table~\ref{tab:speech:k} shows accuracy on various speech tasks with increasing k controlling the rank of the domain-specific component.  Rank-0 corresponds to the baseline ERM without any domain-specific part.  We observe that accuracy drops with increasing rank beyond 1 and the best is with  when number of domains .  As we increase D to 200 domains, a higher rank (4) becomes optimal and the results stay stable for a large range of rank values.  This matches our analytical understanding resulting from Theorem~\ref{thm:main} that we will be able to successfully disentangle only those domain specific components which have been observed in the training domains, and using a higher rank will increase noise in the estimation of .




\begin{table}[htb]
    \centering
    \begin{tabular}{|l|r|r|r|}
    \hline
    Rank  & 50 & 100 & 200 \\
    \hline
    0  &   72.6 (.1) & 80.0 (.1) & 86.8 (.3)  \\           
    1 & \textbf{74.1} (.3) &  \textbf{81.4} (.4) & 87.3 (.5) \\
    4 & 73.7 (.1) & 80.6 (.7) &  \textbf{ 87.5} (.1) \\
    9 & 73.0 (.6) & 80.1 (.5) &  \textbf{  87.5} (.2) \\
    24 & 72.3 (.2) & 80.5 (.4) & 87.4 (.3) \\
    \hline
    \end{tabular}
    \caption{Effect of rank constraint (k) on test accuracy for Speech task with varying number of train domains.}
    \label{tab:speech:k}
\end{table}

\section{Conclusion}


We considered a natural multi-domain setting and looked at how standard classifier could overfit on domain signals and delved on efficacy of several other existing solutions to the domain generalization problem. 
Motivated by this simple setting, we developed a new algorithm called CSD that effectively decomposes classifier parameters into a common part and a low-rank domain-specific part.  
We presented a principled analysis to provide identifiability results of CSD while delineating the underlying assumptions.
We analytically studied the effect of rank in trading off domain-specific noise suppression and domain generalization, which in earlier work was largely heuristics-driven. 
 
We empirically evaluated CSD against four existing algorithms on five datasets spanning speech and images and a large range of domains.  We show that CSD generalizes better and is considerably faster than existing algorithms, while being very simple to implement.
In future, we plan to investigate algorithms that combine data augmentation with parameter decomposition to gain even higher accuracy on test domains that are related to training domains.

\bibliography{main}
\bibliographystyle{icml2020}



\newpage
\section*{\centering Efficient Domain Generalization via Common-Specific Low-Rank Decomposition \\ (Appendix)}
\section{Evaluation on PACS dataset}
PACS\footnote{\url{https://domaingeneralization.github.io/}} is a popular domain generalization benchmark. The dataset in aggregate contains around 10,000 images from seven object categories collected from 4 different sources: Photo, Art, Cartoon and Sketch. Evaluation using this dataset trains on three of four sources and tests on left out domain. This setting is challenging since it tests generalization to a radically different target domain. Despite being a very popular dataset, evaluations using this dataset is laced with several inconsistent or unfair comparisons. \cite{CarlucciAS2019} uses ten percent of train data for validation (albeit different from the official split), while ~\cite{DouCK19},~\cite{BalajiSR2018} do not use any validation split. \cite{CarlucciAS2019},~\cite{DouCK19} use data augmentation techniques while ~\cite{BalajiSR2018} do not. Also, since the dataset is small and target domain is very far from source domains, the results are sensitive to optimization parameters such as learning rate, optimizer, learning rate schedule. As a result, the comparisons using this dataset across different implementations are rendered useless. Table 1 of ~\cite{CarlucciAS2019} highlights these differences even in baseline across different implementations; With such widely differing baseline numbers it is hard to compare across different methods. For this reason, we relegate evaluation of CSD on this dataset to supplementary material accompanied by this word of caution.

We present comparisons in Table~\ref{tab:expt:pacs} of our method with ~\cite{CarlucciAS2019} and baseline while using their implementation for a fair comparison. The baseline network is ResNet-18. We perform almost the same or slightly better than image specific JiGen~\cite{CarlucciAS2019}. We stay away from comparing with ~\cite{DouCK19} since their reported numbers from different implementation, which is unavailable for ResNet-18, could have different baseline number.

\begin{table}[htb]
    \centering
    \begin{tabular}{|l|r|r|r|r|r|}
    \hline
    Method & Photo & Art & Cartoon & Sketch & Average \\
    \hline
         ERM & 95.73 & 77.85 & 74.86 & 67.74 & 79.05 \\
         JiGen & 96.03 & 79.42 & 75.25 & 71.35 & 80.41 \\
         CSD & 95.45 & 79.79 & 75.04 & 72.46 & {\bf 80.69} \\
    \hline
    \end{tabular}
    \caption{Comparison between JiGen~\cite{CarlucciAS2019} and CSD (ours) using PACS datset with ResNet-18 architecture. The header of each column identifies the target domain.}
    \label{tab:expt:pacs}
\end{table}{}

\section{Proof of Theorem 1}\label{app:proof}
\begin{proof}[Proof of Theorem 1] The high level outline of the proof is to show that the first two steps obtain the best rank- approximation of  such that the row space contains . The last two steps retain this low rank approximation while ensuring that .

The proof that the first two steps obtain the best rank- approximation follows that of the classical low rank approximation theorem of Eckart-Young-Mirsky. We first note that the minimization problem of  under the given constraints, can be equivalently written as:

Let  and  be the SVD of . Since , we have that . We will first show that , where  is the top- component of , minimizes both  and  among all matrices  satisfying the conditions in~\eqref{eqn:opt}. Let  denote the  largest singular value of .

\textbf{Optimality in operator norm}: Fix any  satisfying the conditions in~\eqref{eqn:opt}. Since  and , there is a unit vector  such that . Let . Since  and  is an orthonormal matrix, we have . We have:

This proves the optimality in operator norm.

\textbf{Optimality in Frobenius norm}:
Fix again any  satisfying the conditions in~\eqref{eqn:opt}. Let  denote the  largest singular value of  and  denote the best rank- approximation to . Let . We have that:

where the minimum is taken over all  such that . Picking , we see that . It follows from this that .

For the last two steps, note that they preserve the matrix . If  is the unique way to write  such that , then we see that , meaning that . This proves the theorem.






\end{proof}
  
\end{document}
