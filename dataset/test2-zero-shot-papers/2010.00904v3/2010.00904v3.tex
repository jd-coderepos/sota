
\documentclass{article} \usepackage{main,times}


\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{array}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{xspace}
\usepackage{comment}


\usepackage[inline]{enumitem}


\def\genre{\textsc{GENRE}\@\xspace}

\makeatletter
\def\adl@drawiv#1#2#3{\hskip.5\tabcolsep
        \xleaders#3{#2.5\@tempdimb #1{1}#2.5\@tempdimb}#2\z@ plus1fil minus1fil\relax
        \hskip.5\tabcolsep}
\newcommand{\cdashlinelr}[1]{\noalign{\vskip\aboverulesep
           \global\let\@dashdrawstore\adl@draw
           \global\let\adl@draw\adl@drawiv}
  \cdashline{#1}
  \noalign{\global\let\adl@draw\@dashdrawstore
           \vskip\belowrulesep}}
\makeatother

\colorlet{punct}{red!60!black}
\definecolor{background}{HTML}{EEEEEE}
\definecolor{delim}{RGB}{20,105,176}
\colorlet{numb}{magenta!60!black}

\lstdefinelanguage{json}{
    basicstyle=\normalfont\ttfamily,
    numbers=left,
    numberstyle=\scriptsize,
    stepnumber=1,
    numbersep=8pt,
    showstringspaces=false,
    breaklines=true,
    frame=lines,
    backgroundcolor=\color{background},
    literate=
     *{0}{{{\color{numb}0}}}{1}
      {1}{{{\color{numb}1}}}{1}
      {2}{{{\color{numb}2}}}{1}
      {3}{{{\color{numb}3}}}{1}
      {4}{{{\color{numb}4}}}{1}
      {5}{{{\color{numb}5}}}{1}
      {6}{{{\color{numb}6}}}{1}
      {7}{{{\color{numb}7}}}{1}
      {8}{{{\color{numb}8}}}{1}
      {9}{{{\color{numb}9}}}{1}
      {:}{{{\color{punct}{:}}}}{1}
      {,}{{{\color{punct}{,}}}}{1}
      {\{}{{{\color{delim}{\{}}}}{1}
      {\}}{{{\color{delim}{\}}}}}{1}
      {[}{{{\color{delim}{[}}}}{1}
      {]}{{{\color{delim}{]}}}}{1},
}

\lstloadlanguages{Python}
\lstset{
  language=Python,
  basicstyle=\scriptsize\sffamily,
  numberstyle=\color{gray},
  stringstyle=\color[HTML]{933797},
  commentstyle=\color[HTML]{228B22}\sffamily,
  emph={[2]from,import,pass,return},
  emphstyle={[2]\color[HTML]{DD52F0}},
  emph={[3]range}, emphstyle={[3]\color[HTML]{D17032}},
  emph={[4]for,in,def}, emphstyle={[4]\color{blue}},
  showstringspaces=false,
  breaklines=true,
  prebreak=\mbox{{\color{gray}\tiny}},
  numbers=left,
  xleftmargin=15pt
} 
\title{Autoregressive Entity Retrieval}


\author{Nicola De Cao\textsuperscript{1,2}\thanks{~~Work done during internship with Facebook AI Research.}~, Gautier Izacard\textsuperscript{2,3,4}, Sebastian Riedel\textsuperscript{2,5}, Fabio Petroni\textsuperscript{2}\\
\textsuperscript{1}University of Amsterdam, 
\textsuperscript{2}Facebook AI Research \\
\textsuperscript{3}ENS, PSL University,
\textsuperscript{4}Inria, 
\textsuperscript{5}University College London\\
\texttt{nicola.decao@gmail.com},
\texttt{\{gizacard, sriedel, fabiopetroni\}@fb.com}
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}

\maketitle
\begin{abstract}

Entities are at the center of how we represent and aggregate knowledge. For instance, Encyclopedias such as Wikipedia are structured by entities (e.g., one per Wikipedia article). The ability to retrieve such entities given a query is fundamental for knowledge-intensive tasks such as entity linking and open-domain question answering. 
One way to understand current approaches is as classifiers among atomic labels, one for each entity. Their weight vectors are dense entity representations produced by encoding entity meta information such as their descriptions. This approach leads to several shortcomings:
\begin{enumerate*}[label=(\roman*)] 
\item context and entity affinity is mainly captured through a vector dot product, potentially missing fine-grained interactions between the two;
\item a large memory footprint 
is needed to store dense representations when considering large entity sets;
\item an appropriately hard set of negative data has to be subsampled at training time.
\end{enumerate*}
In this work, we propose \genre, the first system that retrieves entities by generating their unique names, left to right, token-by-token in an autoregressive fashion and conditioned on the context. 
This enables us to mitigate the aforementioned technical issues since:
\begin{enumerate*}[label=(\roman*)] 
\item the autoregressive formulation allows us to directly capture relations between context and entity name, effectively cross encoding both;
\item the memory footprint is greatly reduced because the parameters of our encoder-decoder architecture scale with vocabulary size, not entity count;
\item the exact softmax loss can be efficiently computed without the need to subsample negative data.
\end{enumerate*}
We show the efficacy of the approach, experimenting with more than 20 datasets on entity disambiguation, end-to-end entity linking and document retrieval tasks, achieving new state-of-the-art or very competitive results while using a tiny fraction of the memory footprint of competing systems.
Finally, we demonstrate that new entities can be added by simply specifying their unambiguous name. Code and pre-trained models at \url{https://github.com/facebookresearch/GENRE}.











 \end{abstract}


\section{Introduction}

The ability to retrieve the correct entity from large Knowledge Bases (KBs) given a textual input is a fundamental building block for several applications \citep{ferrucci2012introduction,slawski_2015,yang-etal-2018-collective}. Most commercial recommendation systems, for instance, include in their pipelines components to detect and disambiguate entity mentions in open text, in order to isolate relevant concepts from non-meaningful data~\citep{slawski_2015,yang-etal-2018-collective}.
Another example are chat-bots and question answering systems, that are often equipped with retrieval components to surface specific KB entries (e.g., Wikipedia articles) to find knowledge for sustaining a conversation or answering a question~\citep{ferrucci2012introduction,chen2017reading,lewis2020retrievalaugmented,roller2020recipes}. 

Although there has been extensive previous work on entity retrieval~\citep[e.g.][to name just a few]{hoffart-etal-2011-robust,10.1145/2633211.2634350,huang2015leveraging, le-titov-2018-improving,logeswaran2019zero,broscheit-2019-investigating,wu-etal-2020-scalable} there is a common design choice to most current solutions: entities are associated with a unique atomic label and the retrieval problem can be interpreted as multi-class classification across these labels. The match between input and label is calculated through a bi-encoder~\citep{wu-etal-2020-scalable,karpukhin2020dense}: a dot product between dense vector encodings of the input and the entity's meta information (such as title and description). Critically, this formulation enables sub-linear search using modern maximum-inner-product-search libraries~\citep{johnson2019billion} and hence supports retrieving from large entity databases.    

Unfortunately, the classifier approach to entity retrieval also has several shortcomings.
First, unless a costly cross-encoder is used for re-ranking~\citep{wu-etal-2020-scalable}, the dot-product can miss fine-grained interactions between input and entity meta information~\citep{humeau2019poly}. 
Second, storing dense vectors for the whole KB requires a large memory footprint, especially in real-world scenarios (i.e., 24GB to store 1024-dimensional vectors for all of the 6M Wikipedia pages), and the size linearly grows with the addition of new entities.
Third, computing an exact softmax over all entities is very expensive, hence current solutions need to subsample negative data~\citep{logeswaran2019zero,karpukhin2020dense} at training time. Tuning an appropriately hard set of negative instances can be challenging and time-consuming. 
Finally, existing systems can suffer from a cold-start problem since they cannot represent entities about which they have not yet gathered sufficient information, in the form, for instance, of a textual description or a set of relations with the existing entities. 




\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[scale=.25]{figures/example1_new.pdf}
        \caption{Type specification.}
        \label{fig:examples_a}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[scale=.25]{figures/example2_new.pdf}
        \caption{Composing from context.}
        \label{fig:examples_b}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[scale=.25]{figures/example3_new.pdf}
        \caption{Translation.}
        \label{fig:examples_c}
    \end{subfigure}
    \par\vspace{3pt}
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[scale=.25]{figures/example4_new.pdf}
        \caption{Entity normalization.}
        \label{fig:examples_d}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[scale=.25]{figures/example5_new.pdf}
        \caption{Implicit factual knowledge.}
        \label{fig:examples_e}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[scale=.25]{figures/example6_new.pdf}
        \caption{Exact copy.}
        \label{fig:examples_f}
    \end{subfigure}
    \caption{Examples of entities correctly retrieved from \genre (we show only the top-3 rank). On the \textit{top} three entity disambiguation instances and on the \textit{bottom} three document retrieval instances, two for open-domain question answering and one for fact checking. All of them are cast as sequence-to-sequence problems while inference is done using constrained beam search. Gold entities in \textbf{bold}. Sub-captions indicate the type of interaction between the input context and the entity names required.}
    \label{fig:examples}
\end{figure} 
The treatment of entity identifiers as atomic labels in a classifier ignores the fact that we often have unambiguous, highly structured and compositional entity names.
Wikipedia, for instance, associates unique titles to articles,\footnote{We use \textit{entity name} to refer to the corresponding Wikipedia article title throughout the rest of the paper.} that may be the name of the subject or a description of its topic, as well as potential distinctive information to disambiguate \footnote{often in the form of a description in parentheses after the name. Wikipedia naming conventions are described in  \url{https://en.wikipedia.org/wiki/Wikipedia:Article_titles}. } (see Figure~\ref{fig:examples} for some examples).
These entity names often interact with mention contexts in a predictable and regular fashion.
For example, often entity names are identical with the mention strings that refer to them (e.g., Fig.~\ref{fig:examples_f}). When this is not possible, they might be composed of tokens in the context (e.g., Fig.~\ref{fig:examples_b}), include a type specification that can inferred (e.g., Fig.~\ref{fig:examples_a}), be the translation of the string mention (e.g., Fig.~\ref{fig:examples_c}), require `normalization' such as referring to the correct alias of a mention (e.g., Fig.~\ref{fig:examples_d}), or require factual knowledge that might be stored in the parameters of a model (e.g., Fig.~\ref{fig:examples_e}). 
These observations suggest that inputs could be \emph{translated} into unique entity names, word by word, instead of being classified among a huge set of options.



In this paper, we propose \genre (for \textit{Generative ENtity REtrieval}),  the first entity retriever that exploits a sequence-to-sequence architecture to generate entity names in an autoregressive fashion conditioned on the context.
Concretely, \genre uses a transformer-based architecture, pre-trained with a language modeling objective (i.e., we use BART weights from \citet{lewis2019bart}) and fine-tuned to generate entity names. 
This architecture has been shown to retain factual knowledge to some extent~\citep{petroni-etal-2019-language} and language translation skills~\citep{Radford2019LanguageMA} among other things, both desirable properties for an entity retriever.
Naturally, the generated output might not always be a valid entity name. To solve this problem, \genre employs a constrained decoding strategy that forces each generated name to be in a predefined candidate set. 

The autoregressive formulation allows us to directly capture the aforementioned relations between context and entity name, effectively cross encoding both.
Also, the memory footprint required is orders of magnitude smaller than current systems, since the parameters of a sequence-to-sequence model scale linearly with the vocabulary size, not entity count.
Moreover, the exact softmax can be computed efficiently for each output token (i.e., all non-gold tokens are considered negative), thereby eliminating the need for negative data downsampling.
Finally, our model never accesses any explicit meta-information about the entity beyond their title, hence new entities can be added by simply appending their unambiguous name to the candidate set (e.g., Fig.~\ref{fig:examples_b} refers to an entity added after training).

 

We empirically evaluate the performance of \genre on more than 20 datasets, spanning three families of tasks: 
\begin{enumerate*}[label=(\roman*)] 
\item entity disambiguation, using popular datasets and settings (both in and out-of-domain);
\item end-to-end entity linking, with the GERBIL benchmarking tool~\citep{roder2018gerbil}{}, by using a novel dynamically markup-constrained decoding strategy;
\item document retrieval, with the recently proposed KILT benchmark~\citep{petroni2020kilt} which spans 5 different sub-tasks.
\end{enumerate*}
Our models achieve state-of-the-art or very competitive results on nearly all datasets, often with substantial improvement (+13.7 precision points on KILT for retrieval on average). Further, we show that compared with recent models, \genre requires substantially less memory (20 times smaller footprint on average).
Finally, we demonstrate that our model can be applied in scenarios where the only entity information available is its name.

We organize the paper as follows: in Section~\ref{sec:entity_retrieval} we describe our problem formulation. Then, in Section~\ref{sec:method} we present \genre and eventually in Section~\ref{sec:experiments} we extensively evaluate our method on the aforementioned settings.
We will release code and pre-processed data to reproduce our experiments. \section{Entity Retrieval} \label{sec:entity_retrieval}





We assume to have a collection of entities  (e.g., Wikipedia articles) where each entity is an entry in a Knowledge Base (KB) such as Wikipedia. We want to approach the following retrieval problem: given a textual input source  (e.g., question), a model has to return the most relevant entities from  with respect to . We assume that each  is uniquely assigned to a textual representation (i.e., its name): a sequence of tokens  (e.g., Wikipedia pages are identified by their titles).

A particular instance of this problem is Entity Disambiguation (ED) (see Figure~\ref{fig:examples} for an example) where an input  is annotated with a mention and a system has to select either its corresponding entity from ,
or to predict that there is no corresponding entry in the KB.
Another instance is page-level Document Retrieval (DR) where the input  is intended as a query and  as a collection of documents identified by their unique titles (e.g., Wikipedia articles).












 
\section{Method} \label{sec:method}


We address the retrieval problem with an sequence-to-sequence model that generates textual entity identifiers (i.e., entity names). Concretely, \genre
ranks each  by computing a score with an autoregressive formulation:  where  is the set of  tokens in the identifier of , and  the parameters of the model.
We take advantage of fine-tuning the BART~\citep{lewis2019bart} pre-trained language model.
We train \genre using a standard seq2seq objective, i.e., maximizing the output sequence likelihood with teacher forcing~\citep{sutskever2011generating,sutskever2014sequence} and regularized with dropout~\citep{JMLR:v15:srivastava14a} and label smoothing~\citep{szegedy2016rethinking}.
Concretely, we use the objective that is typically used for neural machine translation~\citep[NMT,][]{wu2016google}, that is maximizing  with respect to model's parameters  which, due to the factorized formulation, can be calculated exactly. We do not need negative sampling to approximate the loss normalizer.


\subsection{Inference with Constrained Beam Search}

Naturally, at test time, we could compute a score for every element in  and then sort them. Unfortunately, this might be prohibitively expensive when  is very large (e.g., Wikipedia has 6M entities).
Hence, we exploit Beam Search~\citep[BS,][]{sutskever2014sequence}, an established approximate decoding strategies to efficiently navigate the search space.
Instead of explicitly scoring all entities in , we search for the top- entities in  decoding from our model using BS with  beams.
Note that using BS implies that the time cost of our retriever does not depend on the size of ,
but only on the size of the beams and the average length of entity representations as we do autoregressive generation.
The average length of entity representations is tractable (e.g., Wikipedia titles have 6 BPE tokens on average) and we follow standard NMT settings where  is small (e.g., 10).


Since we want to output only entities from  we cannot use traditional BS while decoding. Indeed, allowing to generate any token from the vocabulary at every decoding step might lead the model to generate output strings that are not valid identifiers.
Hence, we resort to Constrained BS, forcing to only decode valid entity identifiers.
BS only considers one step ahead during decoding so we can only constrain the generation of a single next token conditioned on the previous ones. Thus, we define our constrain in terms of a prefix tree  (aka trie)~\citep{cormen2009introduction} where nodes are annotated with tokens from the vocabulary. For each node , its children indicate all the allowed continuations from the prefix defined traversing the trie from the root to .

See Figure~\ref{fig:trie} in Appendix~\ref{app:examples} for an exampled of a trie.
When the number of allowed outputs is tractable (e.g., generating a Wikipedia title among 6M) the trie is relatively small it can be pre-computed and stored into memory (e.g., constraining on Wikipedia titles using the BART tokenizer produces a trie with 6M leaves, 17M internal nodes that occupied 600MB of disk space).
We employed the constraints masking the log-probabilities of the invalid tokens and not their logits (i.e., we do not re-normalize the probability over the vocabulary).\footnote{We experimented with both versions and we find masking the log-probability more effective.}


\begin{figure}[t]
     \centering
     \begin{subfigure}[t]{0.25\textwidth}
         \centering
         \includegraphics[scale=.4]{figures/search1.pdf}
         \caption{Outside: we can either continue to generate the input or start a new mention.}
         \label{fig:mardown_out}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.31\textwidth}
         \centering
         \includegraphics[scale=.4]{figures/search2.pdf}
         \caption{Inside a mention: we can either continue to generate the input or end the current mention.}
         \label{fig:mardown_men}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{0.4\textwidth}
         \centering
         \includegraphics[scale=.4]{figures/search3.pdf}
         \caption{Inside an entity link: we can either generate from the entities prefix trie or close if the generated prefix is a valid entity.}
         \label{fig:mardown_ent}
     \end{subfigure}
        \caption{Example of dynamically constrained \textit{Markup} decoding for entity linking using ``\textit{In 1503, Leonardo began painting the Mona Lisa.}" as input. There are 3 cases: when we are outside a mention/entity (a), inside a mention generation step (b), and inside an entity link generation step (c). The model is supposed to output the input source annotating mentions and pointing them to the respective entities: ``\textit{In 1503, \textbf{[Leonardo]}(\underline{Leonardo da Vinci}) began painting the \textbf{[Mona Lisa]}(\underline{Mona Lisa})}".}
        \label{fig:mardown}
\end{figure} 

\subsection{Autoregressive End-to-End Entity Linking}

We additionally extend our autoregressive framework to address end-to-end Entity Linking (EL) where, given a document, a system has to both detect entity mentions and link those mentions to their respective KB entities. In this setting, we train the model to predict the source input again but with annotated spans. We use a \textit{Markup} annotation where spans boundaries are flagged with special tokens and accompanied by their corresponding entity identifiers.

Differently from a setting where the output space is relatively small (e.g., a pre-defined set ), the space of annotated outputs is exponentially large. Hence, it is intractable to pre-compute a trie for decoding, and we compute it dynamically instead. In Figure~\ref{fig:mardown} we show an example. At each generation step, the decoder is either generating a mention span, generating a link to a mention, or continuing from the input source.
When outside a mention/entity step the decoder has only two options: (i) to continue by copying the next token from the input source, or (ii) to generate the \textit{start of mention} token (i.e., `[') which makes the decoder enter the mention generating phase. While generating a mention, the decoder has either to continue with the next token in the input source or to generate the \textit{end of mention} token (i.e., `]') which makes the decoder enter the entity generating phase.
Finally, when generating an entity, the decoder employs the entities trie
such that it can only output a valid entity identifier as in Constrained Beam Search explained above.


 \section{Experiments} \label{sec:experiments}



We extensively evaluate \genre on more than 20 datasets across 3 tasks: Entity Disambiguation, end-to-end Entity Linking (EL), and page-level Document Retrieval. We describe the experimental settings in Section~\ref{sec:experiments_settings} where we discuss results in Section~\ref{sec:experiments_results}. All experiments are in English.

\subsection{Settings} \label{sec:experiments_settings}

\paragraph{Entity Disambiguation (ED)}

We reproduce the setting of~\citet{le-titov-2018-improving} using the same candidate sets, \emph{in-domain} and \emph{out-of-domain} datasets, and evaluating using the \textit{InKB} micro-.
We train \genre feeding each document where a single mention is flagged with two special start and end tokens and the target output is the textual representation of the corresponding entity.
At test time,
we decode using constrained beam search with a trie obtained using the provided candidate set (i.e., a subset of ). As large generative models benefit from large amount of data, we first pre-train \genre on the BLINK data~\citep{wu-etal-2020-scalable}, i.e., 9M unique triples document-mention-entity
from Wikipedia. Then, for the \emph{in-domain} scenario, we fine-tune using the AIDA-CoNLL dataset~\citep{hoffart-etal-2011-robust}. For the \emph{out-of-domain} scenario, we evaluate on five test sets: MSNBC, AQUAINT, ACE2004, WNED-CWEB (CWEB) and WNED-WIKI (WIKI)~\citep{gabrilovich2013facc1, guo2018robust}. More task details and hyperparameters setting are reported in Appendix~\ref{app:hyper_ed}.

\paragraph{End-to-End Entity Linking (EL)}
For EL, we reproduce the setting of~\citet{kolitsas-etal-2018-end} using the same \emph{in-domain} and \emph{out-of-domain} datasets as well as evaluating the \textit{InKB} micro- on the GERBIL benchmark platform~\citep{roder2018gerbil}.
Similarly to the ED setting, we first pre-traine our model on all abstract sections from Wikipedia\footnote{It is based on the 2019/08/01 Wikipedia dump pre-processed by~\citet{petroni2020kilt}.} enriched by a string matching heuristic to solve co-references (i.e., if there is a string that matches exactly with another hyperlink we also add it to the dataset as a mention/entity pairs). Then, for the \emph{in-domain} scenario, we fine-tune using the AIDA-CoNLL dataset. We evaluate on seven \emph{out-of-domain} test sets: MSNBC, Derczynski (Der)~\citep{derczynski2015analysis}, KORE 50 (K50)~\citep{10.1145/2396761.2396832}, N3-Reuters-128 (R128), N3-RSS-500 (R500)~\citep{roder2014n3}, and OKE challenge 2015 and 2016 (OKE15 and OKE16)~\citep{nuzzolese2015open}. More task details and hyperparameters setting are reported in Appendix~\ref{app:hyper_el}.

\paragraph{Page-level Document Retrieval (DR)}
For this setting, we test \genre on all the KILT benchmark tasks~\citep{petroni2020kilt}.
Here, whole Wikipedia is used as the candidate set and we evaluate using R-precision~\citep{rprecdefinition}.
KILT consists of five tasks that use the same Wikipedia dump as a knowledge source: fact checking with FEVER~\citep{thorne-etal-2018-fever}; open domain question answering using Natural Questions~\citep{kwiatkowski2019natural},
HotpotQA~\citep{yang-etal-2018-hotpotqa},
TriviaQA~\citep{joshi2017triviaqa},
ELI5~\citep{fan2019eli5}; slot filling with T-REx~\citep{elsahar2019t},
Zero Shot RE~\citep{levy2017zero}; entity disambiguation on AIDA CoNLL-YAGO, WNED-WIKI and WNED-CWEB; dialogue with Wizard of Wikipedia~\citep{dinan2018wizard}. We train \genre on BLINK and all KILT data simultaneously with a single model.\footnote{Note that not all dataset available in KILT have a training set. Concretely, we train on FEVER, Natural Questions, HotpotQA, TriviaQA, T-REx, Zero Shot RE, AIDA CoNLL-YAGO, and Wizard of Wikipedia.} More details on the hyperparameter setting are reported in Appendix~\ref{app:hyper_dr}. 



\subsection{Results} \label{sec:experiments_results}

Overall, \genre achieves very competitive results in all of the three settings being the best performing system on average across all of them.
See Appendix~\ref{app:examples} for examples of inputs, ground truth and model predictions for all of the three tasks. In the following, we discuss how \genre compares to SOTA systems as well as showing some quantitative analysis on its memory footprint, how it exploits the structured of the entity name space, and how it behaves on a \textit{cold-start} scenario where new unseen entities are added to the KB (descriptions of those entities are unobserved).

\paragraph{Comparing \genre to SOTA systems}
In ED the difference in average  score between \genre and the second best performing system is small (i.e., +0.8) however, ED is an established task with more than a decade of research that benchmarked on those datasets. Indeed all systems reported in Table~\ref{tab:ned-results_ablation} achieved high and similar results even if they were taken from three years back.

The improvements on EL are instead more evident. \genre is the best in-domain system for AIDA while 
performing remarkably well also on the out-of-domain setting (e.g., +13  points on Derczynski, and +4.7 on KORE50). Noticeably, in two datasets (OKE15 and OKE16) our model performs poorly. However, these datasets are annotated with coreference (pronouns and common nouns are linked to entities) while our model was not specifically trained for that. Conversely, most of the other systems, have a mention detection component in their pipelines that can be trained or biased to also solve these cases. We considered out of the aim of this work to additional train and evaluate on coreference and we leave it for future work.


On page-level DR, the superiority of \genre is remarkable. Our model is the best performing system across all 5 KILT tasks and all datasets except on Natural Questions where it is the second best. We achieve +13.7 R-precision points on average with respect to the best performing baseline.
In Table~\ref{tab:kilt-results} we compare \genre
against all methods reported in the public leaderboard: DPR~\citep{karpukhin2020dense}, DPR+BERT~\citep{devlin-etal-2019-bert}, DPR+BART, tf-idf~\citep{10.5555/2787930}, RAG~\citep{lewis2020retrievalaugmented}, and BLINK+flair~\citep{wu-etal-2020-scalable, akbik-etal-2019-flair}. No model except ours was trained on the entire KILT dataset at the same time. A RAG model was trained for every single task as well as for DPR+BERT. Note that this gives and advantage to RAG and DPR+BERT to specialize on single tasks where we have only a single model to solve all of them which still performs better. We speculate that multi-task training could have helped since the all tasks share a common objective to retrieve entities. Both DPR and BLINK+flair were not trained specifically on KILT. However, DPR was trained using several QA datasets which include Natural Question and TriviaQA. In Appendix~\ref{app:additional} we report additional results where we do not pre-train or fine-tune our models for both the ED and retrieval setting in Table~\ref{tab:ned-results_ablation} and~\ref{tab:kilt-results_ablation} respectively. When we train \genre only in the DPR or BLINK data, our model still outperforms them. 




\begin{table}[t]
\centering
\resizebox{\textwidth}{!}{   

\fontsize{8.4}{10.1}\selectfont \setlength{\tabcolsep}{0.5em}
\begin{tabular}{lcccccc|c}
\toprule
 & In-domain  & \multicolumn{5}{c}{Out-of-domain} &  \\
\textbf{Method} &  \textbf{AIDA} &  \textbf{MSNBC} &  \textbf{AQUAINT} &  \textbf{ACE2004} &  \textbf{CWEB} &  \textbf{WIKI*} &  \textbf{Avg.} \\
\midrule
\citet{ganea-hofmann-2017-deep}   & 92.2 & 93.7 & 88.5 & 88.5 & 77.9 & 77.5 & 86.4 \\
\citet{guo2018robust}  & 89 & 92 &   87 &   88 & 77 & \underline{84.5} & 86.2 \\
\citet{yang-etal-2018-collective} & \textbf{95.9} & 92.6 &  89.9 & 88.5 & \textbf{81.8} & 79.2 & \underline{88.0} \\
\citet{shahbazi2019entity}        & 93.5 & 92.3 &   \underline{90.1} &  88.7 & \underline{78.4} & 79.8 & 87.1 \\
\citet{yang-etal-2019-learning}   & 93.7 & \underline{93.8} & 88.2 & \underline{90.1} & 75.6 & 78.8 & 86.7 \\
\citet{le2019boosting} & 89.6 & 92.2 & \textbf{90.7} &  88.1 & 78.2 & 81.7 & 86.8 \\
\citet{fang2019joint}  & \underline{94.3} & 92.8 &  87.5 & \textbf{91.2} & 78.5 & 82.8 & 87.9 \\
BLINK w/o candidate set** & 79.6 & 80.0 & 80.3 & 82.5 & 64.2 & 75.5 & 77.0 \\
\midrule
\textbf{\genre}      &      93.3 & \textbf{94.3} & 89.9 & \underline{90.1} & 77.3 & \textbf{87.4} & \textbf{88.8} \\
\midrule
\midrule
\multicolumn{8}{c}{Ablations} \\
\midrule
\genre only AIDA data     & 88.6 & 88.1 & 77.1 & 82.3 & 71.9 & 71.7 & 80.0 \\
\genre only BLINK data      & 89.3 & 93.3 & 90.9 & 91.1 & 76.0 & 87.9 & 88.1 \\
\genre w/o candidate set & 91.2 & 86.9 & 87.2 & 87.5 & 71.1 & 86.4 & 85.1 \\
\genre w/o constraints & 86.4 & 80.0 & 81.7 & 82.1 & 66.0 & 81.1 & 79.6 \\
\bottomrule
\end{tabular} 
}

\caption{Micro  (InKB) on the in-domain test set and five out-of-domain test sets for the named entity disambiguation task. \textbf{Bold} indicates best model and \underline{underline} indicates second best (not for ablations).
*WIKI is usually considered out-of-domain but note that all methods use a part of Wikipedia to train. **results taken from \url{https://github.com/facebookresearch/BLINK} and normalized to accommodate entities not in KB.
}
\label{tab:ned-results_ablation}
\end{table} \begin{table}[t]
\centering
\resizebox{\textwidth}{!}{   

\fontsize{8.4}{10.1}\selectfont \setlength{\tabcolsep}{0.5em}
\begin{tabular}{lcccccccc|c}
\toprule
& In-domain  & \multicolumn{7}{c}{Out-of-domain}  \\
\textbf{Method} & \textbf{AIDA} & \textbf{MSNBC} &  \textbf{Der} &  \textbf{K50} & \textbf{R128} & \textbf{R500} & \textbf{OKE15*} & \textbf{OKE16*} & \textbf{Avg.}\\
\midrule
\citet{hoffart-etal-2011-robust}     &   72.8 &   65.1 &       32.6 &   55.4 &          46.4 &       \textbf{42.4} &     \textbf{63.1} &      0.0 & 47.2 \\
\citet{10.1007/978-3-642-38288-8_26} &   42.3 &   30.9 &       26.5 &  46.8  &   18.1 &       20.5 &     46.2 &     46.4 & 34.7 \\
\citet{moro-etal-2014-entity}        &   48.5 &   39.7 &       29.8 &   \underline{55.9} &  23.0 &       29.1 &  41.9 &     37.7 & 38.2 \\
\citet{kolitsas-etal-2018-end}       &   \underline{82.4} &   \underline{72.4} &       34.1 &   35.2&  \textbf{50.3} &       38.2 &     \underline{61.9} &     \underline{52.7} & 53.4\\
\citet{broscheit-2019-investigating} &   79.3 &    - &        - &    - & - &    - &  - &    -  \\
\citet{martins-etal-2019-joint}      &   81.9 &    - &        - & - &    -&    - &    - &  - \\
\citet{10.1145/3397271.3401416}      &   80.5 &   \underline{72.4} &       \underline{41.1} &   50.7 & \underline{49.9} & 35.0  & \textbf{63.1} & \textbf{58.3} & \underline{56.4} \\
\midrule
\textbf{\genre} &   \textbf{83.7} &   \textbf{73.7} & \textbf{54.1} &   \textbf{60.7} & 46.7 & \underline{40.3} & 56.1 & 50.0 & \textbf{58.2} \\ 

\bottomrule

\end{tabular}
}
\caption{Micro  (InKB) on the in-domain test set and four out-of-domain test sets for the entity linking task. \textbf{Bold} indicates best model and \underline{underline} indicates second best. 
results from the Wikipedia 2019 setting as opposed to the 2014 setting (older dump and fewer entities).}
\label{tab:el-results}
\end{table}
 
\paragraph{Memory Footprint} 
\genre is not only performing better than other SOTA models on DR but it has a significant reduction of memory footprint (disk space). In Figure~\ref{tab:efficiency_analysis} we compare the number of model/index parameter against DPR, RAG, and BLINK. \genre uses an order of magnitude less parameters (millions instead of billions) to store the entity index because it just has to use a prefix tree of the entity names as opposed to a dense vector for each entity. Concretely, \genre occupied 14 times less memory than BLINK and 34 times less memory than DPR.

\paragraph{Exploiting the Structured Name Space} 
We investigated some properties of \genre, comparing two variants of our model to BLINK on the ED task (using WNED-KILT validation set): one trained to generate entity names and another to generate numerical identifiers (IDs).
All models are trained on the same data and we report results in Figure~\ref{tab:entity_linking_analysis}.
When there is an exact match between a mention and its entity name, both BLINK and \genre almost always make an accurate prediction. Different is the case of partial and no match: \genre performance is much higher suggesting that our model 
uses the context more effectively,
as the autoregressive formulation allows to cross-encode mention context and entity candidates directly capturing fine-grained interactions between the two.
Moreover, when we switch to predicting IDs, the performance drops drastically (-20.3 points on average) indicating that it is important that entity names are meaningful, structured and compositional (as they are in Wikipedia)
conversely to atomic IDs. Surprisingly, when there is no overlap between a mention-entity pair, performance are still relatively high by using IDs.
This suggests that the model is good at memorizing and recalling identifiers even if numeric.

\paragraph{Ablation study}
We here discuss an ablation study on the entity disambiguation task (see Table~\ref{tab:ned-results_ablation}). Due to space limitation, we discuss an ablation study on document retrieval in Appendix~\ref{app:additional_retrival}.
In Table~\ref{tab:ned-results_ablation}, \genre only AIDA or BLINK data indicates the ablation for which we only train on one of the two datasets (i.e., only fine-tuning). \genre (full) is also used with constrained decoding (see Section~\ref{sec:method}) and in combination with a candidate set~\citep[as provided by][]{le-titov-2018-improving}. \genre without candidate set denotes ablating the provided (and small) candidate set and therefore using all the entities in the KB (in our case Wikipedia) as candidates. \genre without constraints indicates ablating constrained decoding which implies no use of the provided candidates set but also unconstrained generation (i.e., the model may generate entity names that are not in the KB). Eventually, using constrained generation and exploiting the candidate sets proved useful. Training only on AIDA data is insufficient to get high  (but AIDA is quite small compared to the 9M datapoints of BLINK data).

\paragraph{Entity frequency}
The performance of a model naturally depends on how many times entities appear in the training data. We show the data distribution of the mention-entity frequency in Figure~\ref{fig:kilt_el_mention_stats}. Most of the pairs appears in Wikipedia (10931 / 13354) where 2423 do not (first bin). The average accuracy is 82.5\% but noticeable it is higher for mention-entity pairs that are more frequent (right side of the plot). The accuracy for pairs that do not appear in Wikipedia is substantially lower than the average suggesting that those are harder cases (the very end tail of the distribution). The degradation in performance is minimal indicating that our model is good at predicting rare entities.

\paragraph{Cold-start}
We manually collect 50 Wikipedia articles that were created in 2020\footnote{Note that both pre-training and fine-tuning use dumps from 2019.}
to simulate a \textit{cold-start} setting where new entities are added to the KB and the only entity information available is their names. 
To create ED instances we resort to hyperlinks pointing to those entities in other Wikipedia articles.
19 out of 50 mentions have an exact match with their respective entity names and all of them were correctly classified by \genre. In combination with the results from Table~\ref{tab:entity_linking_analysis} we can conclude that \genre has a bias on exactly copying the mention, and this helps on unseen data.
\genre also correctly classified 14/31 of the remaining mentions (45.2\%). This demonstrates the ability of our solution to be applied in scenarios where entity metadata is unavailable (apart his name), a setting where, to the best of our knowledge, no existing system is capable to operate. 


We additionally test how \genre performs on unseen  mention-entity pairs on WikilinksNED Unseen-Mentions data~\citep{onoe2020fine} and we report all results in Table~\ref{tab:wned_unseen} in Appendix~\ref{app:additional_ned}.
Surprisingly, \genre performs almost the same for seen and unseen entity pairs  (64.4 vs 63.2 accuracy)
However, in the~\citet{onoe2020fine} setting we cannot guarantee entity descriptions have not been seen by BART during pre-training (given his training data contains Wikipedia). 


\begin{table}[t]
\centering

\resizebox{\textwidth}{!}{   

\fontsize{8.4}{10.1}\selectfont \setlength{\tabcolsep}{0.5em}
\begin{tabular}{lccccccccccc|c}
\toprule
& \multicolumn{1}{r}{Fact Check.}  & \multicolumn{3}{c}{Entity Disambiguation} & \multicolumn{2}{c}{Slot Filling} & \multicolumn{4}{c}{Open Domain QA} & \multicolumn{1}{c}{Dial.}  \\
\textbf{Model} & \textbf{FEV}  & \textbf{AY2} & \textbf{WnWi} & \textbf{WnCw} & \textbf{T-REx} & \textbf{zsRE}  & \textbf{NQ} & \textbf{HoPo} & \textbf{TQA} & \textbf{ELI5} & \textbf{WoW} & \textbf{Avg.}  \\
\midrule
DPR + BERT & \underline{72.9} & - & - & - & - & 40.1 & \textbf{60.7} & 25.0 & 43.4 & - & - & - \\
DPR & 55.3 & 1.8 & 0.3 & 0.5 & 13.3 & 28.9 & 54.3 & 25.0 & 44.5 & 10.7 & 25.5 & 23.6 \\ 
tf-idf & 50.9 & 3.7 & 0.24 & 2.1 & 44.7 & 60.8 & 28.1 & 34.1 & 46.4 & \underline{13.7} & 49.0 & 30.5 \\ 
DPR + BART & 55.3 & 75.5 & 45.2 & 46.9 & 13.3 & 28.9 & 54.3 & 25.0 & 44.4 & 10.7 & 25.4 & 38.6 \\
RAG & 61.9 & 72.6 & 48.1 & 47.6 & 28.7 & 53.7 & 59.5 & 30.6 & 48.7 & 11.0 & \underline{57.8} & 47.3 \\
BLINK + flair & 63.7 & \underline{81.5} & \underline{80.2} & \underline{68.8} & \underline{59.6} & \underline{78.8} & 24.5 & \underline{46.1} & \underline{65.6} & 9.3 & 38.2 & \underline{56.0} \\
\midrule
\textbf{\genre} & \textbf{83.6} & \textbf{89.9} & \textbf{87.4} & \textbf{71.2} & \textbf{79.4} & \textbf{95.8} & \underline{60.3} & \textbf{51.3} & \textbf{69.2} & \textbf{15.8} & \textbf{62.9} & \textbf{69.7} \\
\bottomrule
\end{tabular}
}
\caption{R-Precision for page-level retrieval on KILT test data. \textbf{Bold} indicates the best model and \underline{underline} indicates the second best. For our model, we indicated what datasets we used for training. 
}
\label{tab:kilt-results}
\end{table}
 
\begin{table}[t]
\begin{minipage}[t]{.45\textwidth}
    \centering

    \begin{tabular}{lrrr}
        \toprule
        \textbf{Model} & \textbf{Memory} & \textbf{Param.} & \textbf{Index} \\
        \midrule
        DPR & 70.9GB & 220M  & 15B\\
        RAG & 40.4GB & 626M & 15B \\
        BLINK & 30.1GB & 680M & 6B \\
        \midrule
        \textbf{\genre} & \textbf{2.1GB} & \textbf{406M} & \textbf{17M} \\
        \bottomrule
    \end{tabular}
    \caption{Comparison between retrieval models on memory (disk space) footprint and number of model/index parameters.}
    \label{tab:efficiency_analysis}
\end{minipage}\hfill
\begin{minipage}[t]{.53\textwidth}
    \centering

    \begin{tabular}{lccc}
        \toprule
        \textbf{Type {\tiny(support)}} &  \textbf{BLINK} & \textbf{\genre} & \textbf{IDs*} \\
        \midrule
        Exact match {\tiny(1543)} &  97.8 &  96.6 &  76.0 \\
        Partial match {\tiny(1531)} &  70.7 &  86.9 &  63.8 \\
        No match {\tiny(322)} &  49.4 &  59.9 &  55.0 \\
        \midrule
        Total {\tiny (3396)} & 81.0 & 88.8 & 68.5 \\
        \bottomrule
    \end{tabular}
    \caption{Different types of matches between mentions and their entity names on the WNED-KILT. *indicates \genre trained on numerical identifiers.}
    \label{tab:entity_linking_analysis}
\end{minipage}
\end{table}








 
 \section{Related Works}



Casting NLP tasks with a structured input or output into sequence-to-sequence problems has been explored for different problems, including semantic parsing~\citep{rongali2020dontparse}, semantic role labelling~\citep{daza2018sequence}, discourse representation structure parsing~\citep{liu-etal-2018-discourse}, generation of fluent natural language responses from structured semantic representations~\citep{balakrishnan-etal-2019-constrained}, generation and parsing of abstract meaning representation~\citep{konstas-etal-2017-neural}. In these works a structured representation, a tree or a graph for instance, is linearized into a sequence of symbols compatible with a seq2seq architecture. 
To the best of our knowledge, we are the first to cast entity retrieval as a sequence-to-sequence problem while decoding with an autoregressive formulation during inference. 


Related to our constrained generation mechanism, \cite{daza2018sequence, rongali2020dontparse} use a copying mechanism in order to limit lexical deviations between the input and output strings. In these tasks, as well as for our problem, it is natural to promote a copying mechanism due to the input and the output proximity. A different type of constraint, a structural constraint, is used in~\cite{balakrishnan-etal-2019-constrained} to maintain a valid tree structure. Our constrained beam search encompasses both aspects, a copying mechanism that restrains the vocabulary and a structural constraint to obtain a well-formed annotated output. 
In addition to these tasks with close input and output, the integration of a mechanism to guide the output of neural networks has been explored in various settings. Lexically constrained decoding has been used to force the inclusion of pre-specified words for machine translation~\citep{hokamp-liu-2017-lexically, post-vilar-2018-fast}, and image captioning~\citep{anderson-etal-2017-guided}. To the best of our knowledge, we are the first to exploit constrained generation for entity disambiguation, end-to-end entity linking, and query-based entity retrieval.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/kilt_el_mention_stats.pdf}
    \caption{Accuracy per mention-entity pair frequency (in Wikipedia) on the validation sets of all Entity Disambiguation tasks in KILT.}
    \label{fig:kilt_el_mention_stats}
\end{figure}

 \cite{nogueira2020document} propose to use a sequence-to-sequence model to re-rank document. Given a query and a document the model is trained to output the words "true" or "false" depending on whether the document is relevant or not. Differently from our approach for entity retrieval, it requires a limited list of candidates documents, obtained with BM25 for instance, in order to be computationally possible.
 \cite{massarelli2019decoding,petroni2020how} explore the idea of using an autoregressive language
model as neural retriever, by exploiting the implicit knowledge stored in their parameters to generate relevant sentences given a query. While intriguing, such solutions still lag behind retrievers with an explicit knowledge access (e.g., an explicit Wikipedia index).
The idea of using a generative model for entity disambiguation was proposed in~\citet{petroni2020kilt} as they trained both BART and T5 in a seq2seq fashion on all KILT tasks (including ED).  We expanded that intuition generalizing on multiple tasks (end-to-end EL and page-level retrieval) as well as introducing constrained decoding for an efficient and effective search.



 \section{Conclusions}

In this work, we propose \genre, a novel paradigm to addresses entity retrieval: generate entity names autoregressively. 
Entity names have several properties that might help (even humans) retrieving them, including a compositional structure and a predictable interaction with the context. 
The autoregressive formulation allows us to directly capture some of these properties, leading to several advantages with respect to current solutions, including an efficient way to cross encode mention context and entity candidates, a much smaller memory footprint, and the ability to compute an exact softmax without the need to subsample negative data. We empirically show that these characteristics, combined with constrained decoding strategies, led to state-of-the-art performance on a plethora of entity retrieval datasets, spanning entity disambiguation, end-to-end entity linking, and page-level document retrieval, while resulting in systems with a remarkably contained memory footprint, a space reduction by a factor of twenty on average. We additionally demonstrate that new entities can be effectively considered in our system by simply appending their unambiguous name to the candidate set.



 
\ificlrfinal
\subsection*{Acknowledgments}
Authors thank
Patrick Lewis,
Aleksandra Piktus,
Michael Schlichtkrull,
Ivan Titov,
Jean Maillard,
Edouard Grave,
Sergio De Cao,
Luisa Quarta
for helpful discussions and technical support. \else
\clearpage
\fi

\bibliography{main}
\bibliographystyle{main}

\clearpage
\appendix

\section{Experimental Details}

We implemented, trained, and evaluate our model using the \texttt{fariseq} library~\citep{ott2019fairseq}. We trained \genre for every task using Adam~\citep{kingma2014adam} with a learning rate  with a linear warm-up for 500 steps and then liner decay. The objective is sequence-to-sequence categorical cross-entropy loss with 0.1 of label smoothing.

\subsection{Named Entity Disambiguation} \label{app:hyper_ed}

\paragraph{Setting}
Given a document  (e.g., a sentence) containing a set of entity mentions , a system either has to assign, to each mention , either a KB entity (i.e., ), or predicts that there is no corresponding entry in the KB (i.e., ). Moreover, a restricted candidates set  for each mention  is provided.


\paragraph{Training}
We pre-trained \genre on BLINK data for 200k steps and then we do model selection on the validation set. Afterward, we fine-tuned on AIDA without resetting the learning rate nor the optimizer statistics for 10k steps and we do model selection on the validation set. Following previous works~\citep{yamada-etal-2016-joint, ganea-hofmann-2017-deep, le-titov-2018-improving}, we considered only mentions that have entities in the KB (i.e., Wikipedia). Training was done on 32 GPUs (with 32GB of memory) and it completed in 24h for a total of 32 GPU/day.

\paragraph{Inference}
At test time, we use Constrained Beam Search with 10 beams, and maximum decoding steps of 15. We restrict the input sequence to be at most 384 tokens cutting the left, right, or both parts of the context around a mention.  We normalize the log-probabilities by sequence length.


\subsection{Entity Linking} \label{app:hyper_el}

\paragraph{Setting}
Given a document  (e.g., a sentence) a system has to return a set of tuples  where  is a entity mentions (a span contained in ) and  its corresponding entity in the KB. Following~\citet{kolitsas-etal-2018-end}, we considered only mentions that have entities in the KB (i.e., Wikipedia) and we used their candidate sets with the additions of the table computed by~\citet{hoffart-etal-2011-robust}. 

\paragraph{Training}
We pre-trained \genre on all abstract sections from Wikipedia\footnote{It is based on the 2019/08/01 Wikipedia dump pre-processed by~\citet{petroni2020kilt}.} enriched by a string matching heuristic to solve co-references (i.e., if there is a string that matches exactly with another hyperlink we also add it to the dataset as a mention/entity pairs) data for 200k steps. Then we do model selection on the validation set. Afterward, we fine-tuned on AIDA resetting the learning rate and the optimizer statistics for 10k steps and we do model selection on the validation set. Again, following previous works~\citep{kolitsas-etal-2018-end}, we considered only mentions that have entities in Wikipedia. Training was done on 64 GPUs (with 32GB of memory) and it completed in 30h for a total of 80 GPU/day.

\paragraph{Inference}
At test time, we use Constrained Beam Search with 6 beams, and a maximum decoding step of 384. When the input sequence is too long, we split the input into multiple chunks of equal size. We normalize the log-probabilities by sequence length.

\subsection{Page-level Document Retrieval} \label{app:hyper_dr}

\paragraph{Setting}
Given a query  (e.g., a question) and a collection of documents  (in KILT are Wikipedia pages), a system has to rank documents in  based on their relevance to .

\paragraph{Training}
We trained \genre on all KILT data simultaneously for 200k steps and we do model selection on the validation set averaging the score across tasks.
Training was done on 128 GPUs (with 32GB of memory) and it completed in 33h for a total of 176 GPU/day.

\paragraph{Inference}
At test time, we use Constrained Beam Search with 10 beams. For the ED sub-task, we restrict the input sequence to be at most 384 tokens cutting the left, right, or both parts of the context around a mention. We normalize the log-probabilities by sequence length.

\section{Additional Results} \label{app:additional}


\subsection{Named Entity Disambiguation} \label{app:additional_ned}
Table~\ref{tab:wned_unseen} reports evaluation of \genre on on WikilinksNED Unseen-Mentions data~\citep{onoe2020fine}. We also report additional results on AIDA from the literature in Table~\ref{tab:aida-results}.

\begin{table}[!ht]
\centering
\begin{tabular}{lll|l}
\toprule
 &   \textbf{Seen} & \textbf{Unseen} & \textbf{Total} \\
\midrule
Exact match   &   87.48 {\tiny(751)} &  70.36 {\tiny(2227)} &   74.68 {\tiny(2978)} \\
Partial match &  56.39 {\tiny(1566)} &  61.47 {\tiny(4838)} &   60.23 {\tiny(6404)} \\
No match      &   41.46 {\tiny(205)} &   45.04 {\tiny(413)} &    43.85 {\tiny(618)} \\
\midrule
\textbf{Total}         &  64.43 {\tiny(2522)} &  63.21 {\tiny(7478)} &  63.52 {\tiny(10k)} \\
\bottomrule
\end{tabular}
\caption{Evaluation of \genre on WikilinksNED Unseen-Mentions data~\citep{onoe2020fine}. We train on the provided train set and we report accuracy scores (i.e., precision at 1) alongside with the number of supporting datapoints. We report scores splitting the test set in seen and unseen entities as well as in three different matchings between a mention and its gold entity.}
\label{tab:wned_unseen}
\end{table}








\begin{table}[!ht]
\centering
\begin{tabular}{lc}
\toprule
\textbf{Methods} &  \textbf{micro-} \\
\midrule
\citet{guo2018robust}  & 89 \\
\citet{le2019boosting} & 89.6 \\
\citet{yamada-etal-2016-joint} & 91.5 \\
\citet{ganea-hofmann-2017-deep}   & 92.2 \\
\citet{shahbazi2019entity}        & 93.5 \\
\citet{chen2020improving} & 93.5 \\
\citet{yang-etal-2019-learning}   & 93.7 \\
\citet{fang2019joint}  & 94.3  \\
\citet{raiman2018deeptype} &  94.9 \\
\citet{mulang2020evaluating} & 94.9 \\
\citet{yang-etal-2018-collective} & \textbf{95.9} \\
\midrule
\textbf{\genre}      &      93.3  \\
\bottomrule
\end{tabular}
\caption{Additional results on AIDA. We report Micro InKB  on test sets.}
\label{tab:aida-results}
\end{table}



\subsection{Document Retrieval} \label{app:additional_retrival}
Table~\ref{tab:kilt-results_ablation} extends Table~\ref{tab:kilt-results} with additional results (i.e., training \genre on the numerical identifiers) and an ablation study on the document retrieval task. The purpose of the experiment is to see whether \genre benefits from the entity names to be meaningful as well as compositional. Numerical IDs do not have that property. In both cases, the model uses its memorizing capabilities but when using IDs the performance is significantly low. Indeed, with IDs the model has no way to generalize nor to use ``implicit knowledge'' acquired during the unsupervised pre-training. We also ablate the training data. DPR data corresponds to training only on Natural Questions (NQ) and TriviaQA (TQA) as DPR was trained only for QA tasks on those datasets and two extra ones. Note that training on BLINK data corresponds to only training for entity disambiguation. However, every other task share similarities with entity disambiguation and thus the model is also capable to address the other tasks with non-zero performance. For the ablations, underlined cells indicate what are the results on the respective task on which a model was trained for (i.e., \textit{\genre only BLINK data} was trained only for ED where\textit{ \genre only DPR data} was trained only for QA). The ablation on data suggests that it is beneficial to train on all tasks simultaneously. \genre without constraints indicates ablating constrained decoding which implies unconstrained generation (i.e., the model may generate entity names that are not in the KB).

\begin{table}[!ht]
\centering

\resizebox{\textwidth}{!}{   

\fontsize{8.4}{10.1}\selectfont \setlength{\tabcolsep}{0.5em}
\begin{tabular}{lccccccccccc|c}
\toprule
& \multicolumn{1}{r}{Fact Check.}  & \multicolumn{3}{c}{Entity Disambiguation} & \multicolumn{2}{c}{Slot Filling} & \multicolumn{4}{c}{Open Domain QA} & \multicolumn{1}{c}{Dial.}  \\
\textbf{Model} & \textbf{FEV}  & \textbf{AY2} & \textbf{WnWi} & \textbf{WnCw} & \textbf{T-REx} & \textbf{zsRE}  & \textbf{NQ} & \textbf{HoPo} & \textbf{TQA} & \textbf{ELI5} & \textbf{WoW} & \textbf{Avg.}  \\
\midrule
DPR + BERT & 72.9 & - & - & - & - & 40.1 & 60.7 & 25.0 & 43.4 & - & - & - \\
DPR & 55.3 & 1.8 & 0.3 & 0.5 & 13.3 & 28.9 & 54.3 & 25.0 & 44.5 & 10.7 & 25.5 & 23.6 \\ 
tf-idf & 50.9 & 3.7 & 0.24 & 2.1 & 44.7 & 60.8 & 28.1 & 34.1 & 46.4 & 13.7 & 49.0 & 30.5 \\ 
DPR + BART & 55.3 & 75.5 & 45.2 & 46.9 & 13.3 & 28.9 & 54.3 & 25.0 & 44.4 & 10.7 & 25.4 & 38.6 \\
RAG & 61.9 & 72.6 & 48.1 & 47.6 & 28.7 & 53.7 & 59.5 & 30.6 & 48.7 & 11.0 & 57.8 & 47.3 \\
BLINK + flair & 63.7 & 81.5 & 80.2 & 68.8 & 59.6 & 78.8 & 24.5 & 46.1 & 65.6 & 9.3 & 38.2 & 56.0 \\
\midrule
\midrule
\genre only BLINK \textit{IDs} & 1.8 & \underline{65.0} & \underline{63.5} & \underline{58.6} & 0.1 & 0.2 & 0.4 & 0.3 & 5.4 & 0.3 & 13.3 & 19.0 \\
\genre only DPR data & 70.8 & 9.7 & 1.9 & 7.3 & 60.0 & 79.7 & \underline{58.3} & \underline{40.3} & \underline{69.6} & \underline{13.2} & 52.6 & 42.1 \\
\genre only BLINK data & 28.1 & \underline{82.5} & \underline{88.1} & \underline{69.9} & 44.8 & 66.1 & 15.0 & 16.4 & 25.6 & 6.8 & 38.7 & 43.8  \\
\genre w/o constraints & 78.9 & 87.2 & 83.2 & 36.5 & 74.4 & 93.6 & 53.3 & 45.2 & 63.7 & 14.3 & 62.7 & 63.0 \\
\midrule
\textbf{\genre full} & 83.6 & 89.9 & 87.4 & 71.2 & 79.4 & 95.8 & 60.3 & 51.3 & 69.2 & 15.8 & 62.9 & 69.7 \\
\bottomrule
\end{tabular}
}
\caption{Ablation study on KILT retrieval. We report R-Precision. \genre only BLINK \textit{IDs} denotes training on BLINK~\citep{wu-etal-2020-scalable} data where instead of using the textual entity representation as target we used a numerical ID.}
\label{tab:kilt-results_ablation}
\end{table}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/kilt_el_token_stats.pdf}
    \caption{Accuracy per number of BPE tokens of the Wikipedia title to generate on the validation sets of all KILT datasets except ELI5 (as it is fundamentally different from the others). We also show the data distribution of token lengths. Most of the titles have less than 15 BPE tokens while the mode of the distribution is 5. Here GENRE has an average accuracy of 78.6\% but it is higher for short titles (e.g., 10) and it is lower for long titles (e.g., 10). Degradation in performance does not directly follow the data distribution of the token lengths. Indeed, even if long titles are rare performance is not heavily affected (e.g., for length 15).}
    \label{fig:kilt_el_token_stats}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/kilt_el_entity_stats.pdf}
    \caption{Accuracy per number of incoming links in Wikipedia on the validation sets of all KILT datasets except ELI5 (as it is fundamentally different from the others). We also show the data distribution of the number of incoming links. Intuitively, a page/entity with few incoming links has been observed less than highly connected pages/entities. Indeed, for pages/entities never linked (first bin on the left) the average accuracy is ~20\% lower than the global average (78.6\%). However, for pages/entities linked at least once it is above the global average. This indicates that \genre seems effective on linking rare entities.}
    \label{fig:kilt_el_entity_stats}
\end{figure}

\newpage
\section{Examples} \label{app:examples}

\begin{figure*}[!ht]
\centering
\begin{lstlisting}[language=Python,firstnumber=1]
ID: '87d95287-707e-4bd9-9633-ca0c611a4a3a_World_Without_Superma:8'
inputs: '[..] When Superman leaves Earth for New Krypton , he appoints , newly freed from the Phantom Zone , to take his place as guardian of [START_ENT] Metropolis [END_ENT] .  Mon-El assumes the secret identity of Johnathan Kent as a tribute to Clark \'s adoptive father , posing as Clark \'s cousin . [..]'
gold_output: 'Metropolis (comics)'
predicted_outputs: [
    ('Metropolis_(comics)', -0.09),
    ('Themyscira_(DC_Comics)', -1.09),
    ('Metropolis_(disambiguation)', -1.27),
    ('Superman_(comic_book)', -1.51),
    ('Superman_(Earth-Two)', -1.52)
]
\end{lstlisting}
\caption{Example of a \genre prediction for named entity disambiguation on KILT WNED. The input is plain text where a mention is flagged with two special start and end tokens \texttt{[START\_ENT]} and \texttt{[END\_ENT]}. The output is a ranked list of entity (where we report the log-likelihood as well).}
\label{fig:example_ned}
\end{figure*} 
\begin{figure*}[!ht]
\centering
\begin{subfigure}[b]{.49\textwidth}
\begin{lstlisting}[language=Python,firstnumber=1]
ID: 'sfq_18245'
inputs: "Which Florentine painter 1535-1607 used the name Bronzino after the death of his 'uncle'?"
gold_output: 'Bronzino'
predicted_outputs: [
    ('Florence', -0.37),
    ('Bronzino', -0.62),
    ('Niccolo_Machiavelli', -0.64),
    ('Giorgio_de_Chirico', -0.71),
    ('Vitruvian_Man', -0.73)
]
\end{lstlisting}
\caption{TriviaQA (open domain question answering).}
\end{subfigure}
\hfill
\begin{subfigure}[b]{.49\textwidth}
\begin{lstlisting}[language=Python,firstnumber=1]
ID: '4713'
inputs: 'Tool has won three Oscars.'
gold_output: 'Tool (band)'
predicted_outputs: [
    ('Tool_(band)', -0.08),
    ('Tool_(disambiguation)', -1.59),
    ('Machine_Head_(band)', -1.73),
    ('Language_Arts_(album)', -1.97),
    ('Machine_Gun_(band)', -2.12)
]
\end{lstlisting}
\caption{FEVER (fact checking).}
\end{subfigure}
\caption{Example of \genre predictions for the retrieval task on KILT. The input is a query and the output is a ranked list of Wikipedia article titles (we also report the log-likelihood of the solutions).}
\label{fig:example_kilt}
\end{figure*} \begin{figure}[!ht]
\centering
\begin{lstlisting}[language=Python,firstnumber=1]
ID: '1106testa_SOCCER'
inputs: 'SOCCER - RESULT IN SPANISH FIRST DIVISION. MADRID 1996-08-31 Result of game played in the Spanish first division on Saturday: Deportivo Coruna 1 Real Madrid 1.'
gold_output: 'SOCCER - RESULT IN [SPANISH](Spain) FIRST DIVISION . [MADRID](Madrid) 1996-08-31 Result of game played in the [Spanish](Spain) first division on Saturday : Deportivo Coruna 1 [Real Madrid](Real Madrid C.F.) 1.'
predicted_output: 'SOCCER - RESULT IN [SPANISH](Spain) FIRST DIVISION . [MADRID](Madrid) 1996-08-31 Result of game played in the [Spanish](Spain) first division on Saturday : [Deportivo](Deportivo de La Coruna) Coruna 1 [Real Madrid](Real Madrid C.F.) 1.'
gold_spans: [
    [19, 7, 'Spain'],
    [44, 6, 'Madrid'],
    [91, 7, 'Spain'],
    [147, 11, 'Real_Madrid_C.F.']
]
predicted_spans: [
    [19, 7, 'Spain'],
    [44, 6, 'Madrid'],
    [91, 7, 'Spain'],
    [128, 9, 'Deportivo_de_La_Coruna'],
    [147, 11, 'Real_Madrid_C.F.']
]

Micro-precision:    0.80
Micro-recall:       1.00
Micro-F1:           0.88
\end{lstlisting}
\caption{Example of a \genre prediction for end-to-end entity linking on AIDA. The input is plain text and the output is a \textit{Markup} string where the links are Wikipedia titles. Spans are in the format : \textit{start of the mention}, \textit{length of the mention}, and \textit{title} respectively.}
\label{fig:example_el}
\end{figure}
 \begin{figure}[!ht]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/trie.pdf}
    \caption{Example of prefix tree (trie) structure where the allowed entities identifiers are `English language', `English literature' and `France'. Note that at the root there is the start-of-sequence token \texttt{SOS} and all leaves are end-of-sequence tokens \texttt{EOS}. Since more that one sequence has the same prefix (i.e., `English'), this end up being an internal node where branches are the possible continuations.}
    \label{fig:trie}
\end{figure}
  
\end{document}
