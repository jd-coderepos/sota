

\documentclass[runningheads]{llncs}
\pdfoutput=1 \usepackage[dvipsnames]{xcolor} 
\usepackage{graphicx}
\usepackage{comment}
\usepackage{amsmath,amssymb} \usepackage{color}
\usepackage{ulem}



\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}

\usepackage{cellspace}
\setlength\cellspacetoplimit{1.5ex}
\setlength\cellspacebottomlimit{1.5ex}

\usepackage{bm}
\usepackage{physics} 
\usepackage[protrusion=true,tracking=true,kerning=true,expansion,final]{microtype}      \usepackage[dvipsnames]{xcolor} \usepackage{booktabs} \usepackage{subcaption}
\usepackage{nicefrac} \usepackage{cancel}
\usepackage[round, compress]{natbib}
\newcommand{\yrcite}[1]{\citeyearpar{#1}}
\renewcommand{\cite}[1]{\citep{#1}}
\usepackage{xspace}
\usepackage{upref}

\usepackage{gensymb}
\usepackage{bbding}
\usepackage{mathtools}
\usepackage{centernot} \usepackage{bigints}
\usepackage{dsfont} \usepackage{esint} \usepackage{authblk}
\usepackage{multirow}
\usepackage{enumitem}

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{algorithmicx}

\usepackage[scaled=.98,sups,osf]{XCharter}\usepackage[libertine,vvarbb,scaled=1.07]{newtxmath}



\usepackage{varioref} \usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=True]{hyperref}
\usepackage[capitalise]{cleveref} 
\crefname{appsec}{appendix}{appendices}
\Crefname{appsec}{Appendix}{Appendices}

\usepackage{url}

\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\definecolor{urlcolor}{rgb}{0,.145,.698}
\definecolor{linkcolor}{rgb}{.71,0.21,0.01}
\hypersetup{ pdftitle={Self-Supervised Learning for Large-Scale Unsupervised Image Clustering},
	pdfauthor={Evgenii Zheltonozhskii, Chaim Baskin, Alex M. Bronstein, Avi Mendelson},
	pdfsubject={},  pdfkeywords={},
	pdfborder=0 0 0,     
    pdfpagemode=UseOutlines,
	breaklinks=true,  colorlinks=true,
	letterpaper=true,
	bookmarks=true,
	bookmarksnumbered=true,
	urlcolor=urlcolor,
	linkcolor=linkcolor,
	citecolor=mydarkblue,
	filecolor=mydarkblue,
	pdfview=FitH}

\renewcommand*{\backref}[1]{} \renewcommand*{\backrefalt}[4]{\ifcase #1 \or
	(cited on p. #2)\else
	(cited on pp. #2)\fi
}
\makeatletter
\renewcommand{\@biblabel}[1]{#1.}
\makeatother

\tolerance=1000
\hbadness=2000
\vbadness=\maxdimen
\sloppy 

\renewcommand{\topfraction}{0.95}
\renewcommand{\textfraction}{.05}
\renewcommand{\bottomfraction}{.95}
\renewcommand{\dbltopfraction}{.95}
\renewcommand{\floatpagefraction}{0.98}
\renewcommand{\dblfloatpagefraction}{0.95}
\setcounter{dbltopnumber}{2}   
\setcounter{topnumber}{4}   
\setcounter{bottomnumber}{4} 
\setcounter{totalnumber}{8}


\usepackage{trajan}
\usepackage{docmute} 
\usepackage{xr}


\renewcommand{\emph}{\textit}
\newcommand\ez[1]{\textcolor{magenta}{(\textbf{Evgenii}: #1)}} \newcommand\CB[1]{\textcolor{yellow}{(\textbf{Chaim}: #1)}} \newcommand\AB[1]{\textcolor{red}{(\textbf{Alex}: #1)}} \newcommand\AM[1]{\textcolor{brown}{(\textbf{Avi}: #1)}} 


\usepackage{bm}
\renewcommand{\vb}{\bm}
\begin{document}
\pagestyle{headings}
\mainmatter
\def\ECCVSubNumber{7380}  

\title{Self-Supervised Learning for Large-Scale Unsupervised Image Clustering} 

\begin{comment}
\titlerunning{ECCV-20 submission ID \ECCVSubNumber} 
\authorrunning{ECCV-20 submission ID \ECCVSubNumber} 
\author{Anonymous ECCV submission}
\institute{Paper ID \ECCVSubNumber}
\end{comment}


\titlerunning{Self-Supervised Learning for Large-Scale Unsupervised Image Clustering}


\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}

\author{Evgenii Zheltonozhskii\inst{1}\orcidID{0000-0002-5400-9321} \and Chaim Baskin\inst{1} \and Alex M. Bronstein\inst{1} \and Avi Mendelson\inst{1}}
\authorrunning{E. Zheltonozhskii et al.}
\institute{
Technion -- Israel Institute of Technology \\
\email{
\href{mailto:evgeniizh@campus.technion.ac.il}{evgeniizh@campus.technion.ac.il};
\href{mailto:chaimbaskin@campus.technion.ac.il}{chaimbaskin@campus.technion.ac.il};
\href{mailto:bron@cs.technion.ac.il}{bron@cs.technion.ac.il};
\href{mailto:avi.mendelson@cs.technion.ac.il}{avi.mendelson@cs.technion.ac.il} } }


{\maketitle}



\begin{abstract}
Unsupervised learning has always been appealing to machine learning researchers and practitioners, allowing them to avoid an expensive and complicated process of labeling the data. However, unsupervised learning of complex data is challenging, and even the best approaches show much weaker performance than their supervised counterparts.  
Self-supervised deep learning has become a strong instrument for representation learning in computer vision. However, those methods have not been evaluated in a fully unsupervised setting. In this paper, we propose a simple scheme for unsupervised classification based on self-supervised representations. We evaluate the proposed approach with several recent self-supervised methods showing that it achieves competitive results for ImageNet classification ( accuracy on ImageNet with  clusters and  with overclustering). We suggest adding the unsupervised evaluation to a set of standard benchmarks for self-supervised learning.  The code is available at \url{https://github.com/Randl/kmeans_selfsuper}.
\keywords{Deep Neural Networks, Unsupervised Deep Learning, Representation learning, Self-Supervised Learning}
\end{abstract}




\section{Introduction}
\label{sec:intro}
Deep learning has become the primary tool in various computer vision tasks, being especially successful in image classification, detection, and segmentation. However, along with massive computing resources required to train state-of-the-art neural networks (NNs), massive datasets with millions of labeled samples are a necessary part of its success. Since creating those datasets is a costly procedure, researchers have recently started looking at the methods of training NNs without labeled data. Those methods commonly referred to as \emph{self-supervised learning} recently has become a powerful instrument for large-scale computer vision. 


A result of training a network in a self-supervised manner is usually a representation: a vector in a latent space. Evaluation of those representations proceeds mainly by two following approaches: fine-tuning the network as a feature extractor for some task
(common choices are segmentation tasks or ImageNert classification on a small amount of data, e.g.,  of labels)
or training a linear classifier on the extracted features. A variation of the latter is to train a -nearest neighbor classifier instead of a linear classifier. While linear classification directly evaluates the learned representation, it is not always capable of predicting performance on downstream tasks \cite{resnick2019probing}. On the other hand, performance of a fine-tuned network strongly depends on the training procedure which is hard to separate from the quality of the representation itself.  



In computer vision, the main approaches to training a network in a self-supervised manner are contrastive losses, pretext tasks, and generative models.
Contrastive methods \cite{oord2018cpc,ye2019invariant,ermolov2020whitening} try to create different views of the same image and bring a representation of different views closer and representations of different images farther apart.
Alternatively, it is possible to train the network to perform some label-free pretext task, such as predicting  context \cite{doersch2015context}, image  rotation \cite{gidaris2018unsupervised,kolesnikov2019revisiting}, colorization \cite{zhang2016colorful}, solving ``jigsaw puzzle'' \cite{kim2018learning}, etc.
Generative-based representation learning uses the latent vectors of a generative model, e.g., Boltzmann machines \cite{lee2009convolutional}, autoencoders \cite{caron2018clustering} or GANs \cite{donahue2016bigan, donahue2019bigbigan}, as a representation. 


The overview of recent methods of self-supervised and unsupervised approaches is given in \cref{tab:ss-links}.
\begin{table}
\centering
\caption{Brief review of selected self-supervised methods, including links to code and linear-evaluation performance. Clustering from pretext \cite{gansbeke2020scan} is an unsupervised method. First plus in each row is a  hyperlink. }
\label{tab:ss-links}
\begin{tabular}{lcccc}
\toprule
\textbf{Method}   & \textbf{Code} & \textbf{Checkpoints} & \textbf{ResNet-50}&\textbf{Best}\\ \midrule
CPC \cite{oord2018cpc}&--&--&--&\\ 
CPC v2 \cite{henaff2019cpc2}&--&--&&\\ 
AMDIM \cite{bachman2019amdim}&\href{https://github.com/Philip-Bachman/amdim-public}{+}&+&--&\\ 
CMC \cite{tian2019contrastive}&\href{https://github.com/HobbitLong/CMC/}{+}&+&&\\ 
BigBiGAN \cite{donahue2019bigbigan}&--&\href{https://tfhub.dev/s?publisher=deepmind&q=bigbigan}{+}&--&\\ 
MoCo \cite{he2019moco}&\href{https://github.com/facebookresearch/moco}{+}&+&&\\ 
Self-Label \cite{asano2019selflabelling}&\href{https://github.com/yukimasano/self-label}{+}&+&&\\ 
SimCLR \cite{chen2020simclr}&\href{https://github.com/google-research/simclr}{+}&+&&\\ 
MoCo v2 \cite{chen2020moco2}&\href{https://github.com/facebookresearch/moco}{+}&+&&\\ 
InfoMin \cite{tian2020infomin}&\href{https://github.com/HobbitLong/PyContrast}{+}&+&&\\ 
BYOL \cite{grill2020byol}&\href{https://github.com/deepmind/deepmind-research/tree/master/byol}{+}&+&&\\ 
SwAV \cite{caron2020swav}&\href{https://github.com/facebookresearch/swav}{+}&+&&\\ 
SimCLRv2 \cite{chen2020simclr2}&\href{https://github.com/google-research/simclr}{+}&+&&\\ 
iGPT \cite{chen2020igpt}&\href{https://github.com/openai/image-gpt}{+}&+&&\\ \midrule
Clustering from pretext \cite{gansbeke2020scan}&\href{https://github.com/wvangansbeke/Unsupervised-Classification}{+}&+&--&--\\
 \bottomrule
\end{tabular}
\end{table}

\paragraph{Contribution}
In this paper, we propose an additional way of evaluating self-supervised learning: training a clustering algorithm on extracted features in an unsupervised manner. While this method suffers from similar disadvantages as linear evaluation, it can provide additional insights and a benchmark for unsupervised learning on large-scale datasets, such as ImageNet.  We also show that self-supervised learning provides a strong baseline for unsupervised computer vision and mentions some possible direction for the current self-supervised methods performance improvement.

Thanks to the increasing trend of publishing pre-trained models and code, we were able to test the existing approaches on the proposed benchmark. In particular, we show that the best-performing self-supervised algorithm achieves almost  top-1 accuracy on ImageNet without any supervision. Those results are on par with a specialized clustering approach by \citet{gansbeke2020scan}. We also evaluate ObjectNet \cite{barbu2019objectnet}, a dataset created for testing image classification algorithms in conditions closer to real-life, and conclude that it is hard to achieve generalization in unsupervised settings. 

This benchmark provides a more challenging task for future self-supervised learning approaches, allowing them to better track their progress.  \section {Method}
\label{sec:method}
\subsection{Metrics}
The evaluation of unsupervised learning methods is a complicated topic, and many different metrics were developed. In this section, we briefly review the metrics we utilized for clustering evaluation.
\paragraph{Accuracy}
In the presence of ground truth labels, it is possible to evaluate the prediction accuracy by assigning classes to predicted clusters. 
Similarly to previous works \cite{xie2015dec,jiang2016variational,gansbeke2020scan} we use linear assignment \cite{kuhn1955hungarian,crouse2016implementing} for assignment of clusters to the classes. In cases when the number of clusters is larger than the number of classes (overclustering), we assign one cluster to each class, while the rest is assigned greedily to maximize accuracy.
\paragraph{Normalized Mutual Information (V-measure)}
For a partition of the instances, , we define entropy as

and mutual information between two partitions as

where

To be able to compare mutual information in different cases, it is usually normalized \cite{kvalseth1987entropy}:

where  is some function, in our case the arithmetic mean.
\paragraph{Adjusted Mutual Information}
Since mutual information tends to have larger values when the number of clusters is large,  mutual information should be adjusted for random chance \cite{vinh2010information}

\paragraph{Adjusted Rand Index }
Rand index \cite{rand1971objective} is another measure of clustering quality. It can be viewed as an accuracy measure over pairs of instances: denoting the number of pairs as , the number of pairs of instances that belong to the same set in both partitions as TP, and the number of pairs of instances that belong to the different sets in both partitions as TN, we define Rand index as

We also adjust the index for chance in the usual manner \cite{hubert1985comparing}:

where  is the maximal value of Rand index. 

\subsection{Evaluation}
To train a clustering model, we extract features of both the training and the validation set with a pre-trained model. We do not apply any augmentations during feature extraction. As opposed to the existing clustering approaches, e.g., DeepCluster \cite{caron2018deepclustering}, our method does not utilize a clustering objective as a part of feature extractor training, but merely uses a feature extractor pre-trained in a self-supervised manner.

Modern clustering approaches are usually based on some distance between different samples. Unfortunately, if the dimension of space is high, the distance between samples provides a little information. In our case, since most of the methods provide at least -dimensional embeddings, we apply dimensional reduction. In particular, we train incremental PCA model with batch size , where  is the dimension of extracted features. 

After applying dimensional reduction, we train mini-batch variation of k-means with the transformed features. Since the features are extracted only once, the training clustering model is relatively cheap. Depending on the model, it takes a couple of hours on CPU.  Using augmentation and training first PCA and then clustering models can boost performance but is much more resource-demanding.


While by default, we set the number of clusters to be  (number of ImageNet classes), we also experiment with overclustering, following \citet{gansbeke2020scan}. \section{Experimental Results}
\label{sec:exp}

We evaluate recent state-of-the-art approaches with the proposed protocol: MoCo v2 \cite{chen2020moco2}, InfoMin \cite{tian2020infomin}, SwAV \cite{caron2020swav}, SimCLRv2 \cite{chen2020simclr2}, and BigBiGAN \cite{donahue2019bigbigan}. 
For every paper, we evaluate ResNet-50 and best performing network. 
We also add results for three models trained in supervised manner\footnote{We employ evaluation code by \citet{wightman2020models}.}: ResNet-152,  EfficientNet-L2 \cite{xie2019noisystudent}, and IG-ResNeXt-101 3248d 
 \cite{mahajan2018weaklyig}. 



For experiments, we utilize feature extracted from two different datasets: ImageNet and ObjectNet \cite{barbu2019objectnet}. 


We trained k-means for 60 epochs, but even 1 epoch often gets decent results.

\paragraph{ImageNet}
Experimental results for ImageNet are shown in \cref{tab:res_inet}. During accuracy calculation, we used training labels for cluster assignment. In addition, we visualize different metrics in \cref{fig:metrics}. We note a strong correlation between linear evaluation accuracy and k-means accuracy, except for SimCLRv2 (ResNet-152 3, SK) and SwAV. We note that SCAN \cite{gansbeke2020scan} gives significantly larger ARI for similar accuracy values. We also note that both supervised and self-supervised methods with high-dimensional embeddings ( ResNet-152 3 and EfficientNet-L2) show weaker results than their counterparts.

\begin{table}
\centering
\caption{Experimental results on ImageNet in form meanstd of 5 runs. Overclustering denoted as ``over.'', supervised models denoted as ``super.'' Bold denotes highest results among our experiments, and red denotes results within one standard deviation of best results. Results for self-label are taken from the paper's official repository.} 
\label{tab:res_inet}
\resizebox{\linewidth}{!}{
\begin{tabular}{lccccc}
\toprule
\textbf{Method}   & \textbf{Linear (super.)}& \textbf{ACC}& \textbf{ARI}& \textbf{AMI}& \textbf{NMI}\\ \midrule
MoCo v2 (ResNet-50)&&&&&\\ 
InfoMin (ResNet-50)&&&&&\\ 
SwAV (ResNet-50)&  &  &  & &  \\ 
SimCLRv2 (ResNet-50)&&&&&\\ \midrule
BigBiGAN (RevNet-50 4)&&&&&\\ 
InfoMin (ResNeXt-152)&&\textcolor{red}{}&&&\\ 
SimCLRv2 (ResNet-152, SK)&&&&&\\ 
SimCLRv2 (ResNet-152 3, SK)&&&&&\\\midrule
SimCLRv2 (ResNet-152, SK, 1.5 over.)&&&&&\\ \midrule
SCAN \cite{gansbeke2020scan}&&&&&\\   
Self-label \cite{asano2019selflabelling}&&&&&\\ 
Self-label  over. \cite{asano2019selflabelling}&&&&&\\ \midrule
ResNet-152 (super.) &&&&&\\  
IG-ResNeXt-101 3248d  (super.) &&&&  &\\  
EfficientNet-L2 (super.) &&&  & &\\  
 \bottomrule
\end{tabular}}
\end{table}



\begin{figure*}
	\centering
	\begin{subfigure}[b]{0.48\linewidth}
		\includegraphics[width=\linewidth]{figures/lin_vs_uns.pdf}
		\subcaption{ }
		\label{fig:lin_vs_uns}
	\end{subfigure}
	\begin{subfigure}[b]{0.48\linewidth}
		\includegraphics[width=\linewidth]{figures/uns_vs_ari.pdf}
		\subcaption{ }
		\label{fig:uns_vs_ari}
	\end{subfigure}
	\caption{ Visualization of different metrics: (a) unsupervised accuracy and  linear evaluation accuracy; (b) ARI and unsupervised accuracy. Green points are outliers (SimCLRv2 (ResNet-152 3, SK) and SwAV).
	}
	\label{fig:metrics}
\end{figure*}


\paragraph{ObjectNet}
To access the generalization of acquired clustering, in addition to ImageNet, we evaluate the proposed method on ObjectNet. ObjectNet is a test set for vision tasks, created to control the performance of vision algorithms in settings close to real life.

\cref{tab:res_onet} shows results for k-means trained on ImageNet training set. In this case, we evaluated only on intersecting classes between ImageNet and ObjectNet. We show accuracy both for cluster assignment based on ImageNet training set (ACC-tr) and ObjectNet itself (ACC-val). Note that since some ObjectNet classes are mapped to two different ImageNet classes, assigning all images to a single class will result in   accuracy. By manually inspecting the predictions of the k-means, we conclude that in many cases, assignment of a large part of instances to a single class indeed happens.

When the ImageNet cluster assignment is used, no network, including supervised ones, show better-than-random performance. 
For ObjectNet assignment, only BigBiGAN (as well as supervised networks) is significantly better than assigning all the instances to a single class. Moreover, ResNet-50 shows better results than larger networks among different self-supervised networks. We advise that, as for now, AMI should be used as a metric for tracking progress on this task.

\cref{tab:res_onet_tr} shows results for clustering trained directly on ObjectNet.  For pre-trained models, the performance on classes that are part of ImageNet is much better: for example, IG-ResNeXt-101 3248d, has  accuracy as compared to  for classes not in ImageNet. For self-supervised, the difference is much smaller: for InfoMin, performance on classes not included in ImageNet is the same ().

\begin{table}
\centering
\caption{
Experimental results on ObjectNet in form meanstd of 5 runs, using clusters acquired from ImageNet training. }
\label{tab:res_onet}
\resizebox{\linewidth}{!}{
\begin{tabular}{lccccc}
\toprule
\textbf{Method}   &  \textbf{ACC-tr}& \textbf{ACC-val}& \textbf{ARI}& \textbf{AMI}& \textbf{NMI}\\ \midrule
MoCo v2 (ResNet-50) &&&&&\\ 
InfoMin (ResNet-50) &&&&&\\ 
SwAV (ResNet-50)& & &&& \\
SimCLRv2 (ResNet-50)&&&&\textcolor{red}{}&\\ \midrule
BigBiGAN (RevNet-50 4) &&&&&\\ 
InfoMin (ResNeXt-152) &&&&&\\ 
SimCLRv2 (ResNet-152, SK)&&&&&\\ 
SimCLRv2 (ResNet-152 3, SK)&&&&&\\   \midrule
SimCLRv2 (ResNet-152, SK, 1.5 over.)&&&&&\\ \midrule
ResNet-152 (super.) && &&&\\ 
IG-ResNeXt-101 3248d  (super.) &&  &&&\\ 
EfficientNet-L2 (super.) &&  &&&\\  
 \bottomrule
\end{tabular}}
\end{table}

\begin{table}
\centering
\caption{Experimental results  on ObjectNet using clusters acquired by training k-means on ObjectNet itself.}
\label{tab:res_onet_tr}
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccc}
\toprule
\textbf{Method}   &  \textbf{ACC}& \textbf{ARI}& \textbf{AMI}& \textbf{NMI}\\ \midrule
MoCo v2 (ResNet-50)&&&&\\ 
InfoMin (ResNet-50) &&&&\\ 
SwAV (ResNet-50)& & & &\\ 
SimCLRv2 (ResNet-50)&&&&\\  \midrule
BigBiGAN (RevNet-50 4) &&&&\\ 
InfoMin (ResNeXt-152) &&&&\\ 
SimCLRv2 (ResNet-152, SK) &&&&\\ 
SimCLRv2 (ResNet-152 3, SK) &&&&\\  \midrule
SimCLRv2 (ResNet-152, SK, 1.5 over.) & & & &\\ \midrule
ResNet-152 (super.) &&&& \\ 
IG-ResNeXt-101 3248d (super.) &&& & \\ 
EfficientNet-L2 (super.) &&&& \\ 
 \bottomrule
\end{tabular}}
\end{table}
\subsection{Ablation study}


\begin{figure*}
	\centering
	\begin{subfigure}[b]{0.48\linewidth}
		\includegraphics[width=\linewidth]{figures/pca_ablation.pdf}
		\subcaption{Accuracy and ARI as a function of dimensions after dimensional reduction.}
		\label{fig:pca}
	\end{subfigure}
	\begin{subfigure}[b]{0.48\linewidth}
		\includegraphics[width=\linewidth]{figures/over_ablation.pdf}
		\subcaption{Accuracy and ARI as a function of number of clusters. }
		\label{fig:over}
	\end{subfigure}
	\caption{Ablation study for the best-performing model, SimCLRv2 (ResNet-152, SK).
	}
	\label{fig:ablation}
\end{figure*}

\paragraph{Dimensionality reduction}
\cref{fig:pca} shows the effect of the number of dimensions used for clustering. As expected, an increasing number of dimensions provide diminishing returns and might harm the results for more than 1024 dimensions.
\paragraph{Overclustering}
Since some classes in ImageNet may contain fairly different images, increasing the number of clusters beyond 1000 improves not only accuracy (since this metric uses real labels, its calculation inevitably involves passing information), but also ARI, as shown in \cref{fig:over}. For that reason, we add  overclustering version of the best-performing model to comparison. \section{Conclusion}
\label{sec:conclusion}
In this paper, we study the applications of self-supervised learning for unsupervised classification. We establish competitive baselines by just applying PCA dimensional reduction and k-means clustering to features extracted by existing self-supervised methods. Thanks to a practice of publishing both code and pre-trained models, we were able to evaluate multiple state-of-the-art approaches and achieve as high as  accuracy on ImageNet in an unsupervised manner and  with overclustering. 

Also, we propose an unsupervised clustering of extracted features as an additional way to evaluate self-supervised training approaches, along with linear evaluation and transfer learning.

Finally, we raise several issues and possible directions for future work. First, the question of whether severe underperformance of models with higher-dimensional feature space, such as ResNet , remains open. Is it the weakness of the proposed clustering method or rather a property of the model? Can the reduction of the dimension of the embeddings improve performance on other tasks?  

Second, the models' poor performance transferred to ObjectNet, even among supervised models, with a prominent exception of BigBiGAN, is of great interest. Is it possible to achieve high performance on ObjectNet and ImageNet simultaneously, at least at the linear classification level? What is the reason BigBiGAN is the only model showing better-than-random results on ObjectNet? What is performance on other ImageNet-related datasets such as ReaL labels \cite{beyer2020arewedone}, ImageNetV2 \cite{recht19imagenetv2}, ImageNet-R \cite{hendrycks2020imagenetr}, etc.?

Last, can the approach itself be improved? Can we take into account the method during the self-supervised training without significant performance degradation in other tasks? What is the better approach for dimensional reduction and clustering itself? What is the effect of augmentation in the clustering training phase?

We hope this paper will raise an interest in self-supervised approach to large-scale image clustering.   




\clearpage
\bibliographystyle{plainnat}
\bibliography{uns_eccv}

\newpage

\appendix{}

\renewcommand\thefigure{\thesection.\arabic{figure}} 
\renewcommand\thetable{\thesection.\arabic{table}} 
\renewcommand\theequation{\thesection.\arabic{equation}}  
\setcounter{figure}{0}  
\setcounter{table}{0}

\crefalias{section}{appsec}
\crefalias{subsection}{appsec}
\crefalias{subsubsection}{appsec}

\end{document}
