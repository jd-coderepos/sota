

\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} 



\usepackage[hyphens]{url} \usepackage{hyperref} 


\newcommand{\theHalgorithm}{\arabic{algorithm}}


\usepackage[accepted]{icml2020}



\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{caption}
\usepackage{mathtools}
\mathtoolsset{showonlyrefs}


\usepackage[mathcal]{eucal}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{todonotes}      \usepackage{siunitx}
\usepackage{mathtools}
\usepackage[inline]{enumitem}
\mathtoolsset{showonlyrefs=true}
\usepackage{multirow}



\usepackage{tikz}
\usetikzlibrary{shapes}

\pgfdeclarehorizontalshading{red}{150bp}{rgb(0bp)=(1,0.76,0.77);
rgb(37.5bp)=(1,0.76,0.77);
rgb(62.5bp)=(0.95,0.55,0.56);
rgb(100bp)=(0.95,0.55,0.56)}

\pgfdeclarehorizontalshading{grey}{150bp}{rgb(0bp)=(0.89,0.89,0.89);
rgb(37.5bp)=(0.89,0.89,0.89);
rgb(50bp)=(0.86,0.86,0.86);
rgb(50.25bp)=(0.82,0.82,0.82);
rgb(62.5bp)=(0.97,0.97,0.97);
rgb(100bp)=(1,1,1)}

\pgfdeclarehorizontalshading{blue}{150bp}{rgb(0bp)=(0.76,0.89,1);
rgb(37.5bp)=(0.76,0.89,1);
rgb(62.5bp)=(0.58,0.79,0.9);
rgb(100bp)=(0.58,0.79,0.9)}

\pgfdeclarehorizontalshading{green}{150bp}{rgb(0bp)=(0.71,0.94,0.73);
rgb(37.5bp)=(0.71,0.94,0.73);
rgb(62.5bp)=(0.49,0.79,0.48);
rgb(100bp)=(0.49,0.79,0.48)}

\tikzset{outer sep=0.1pt, inner sep=0,
	node/.style={rectangle, draw=black, minimum width=42, minimum height=20, outer sep=1.0pt, inner sep=0},
	every picture/.style={line width=0.75pt}}
 
\icmltitlerunning{Gaussian Processes with Signature Covariances}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}\DeclarePairedDelimiter\norm{\lVert}{\rVert}

\newcommand{\bbE}{\mathbb{E}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\bbN}{\mathbb{N}}
\newcommand{\bbP}{\mathbb{P}}
\newcommand{\bbR}{\mathbb{R}}
\newcommand{\R}{\mathbb{R}}

\newcommand{\GP}{\mathcal{GP}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cQ}{\mathcal{Q}}
\newcommand{\Seq}[1]{\mathcal{S}\left(#1\right)}
\newcommand{\Paths}[1]{\mathcal{P}\left( #1 \right)}
\newcommand{\kernel}{\operatorname{k}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bff}{\mathbf{f}}
\newcommand{\lattice}[2]{\cX_{lattice}^{{#1},{#2}}}

\newcommand{\KL}[2]{D_{KL}\left[#1 \;\|\; #2\right]}
\DeclareMathOperator{\diag}{diag}

\newcommand{\p}{\prime}

\newcommand{\ba}{\mathbf{a}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bi}{\mathbf{i}}
\newcommand{\bj}{\mathbf{j}}

\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bZ}{\mathbf{Z}}

\newcommand{\bphi}{\boldsymbol{\varphi}}
\newcommand{\TA}[1]{\prod_{m \ge 0} {#1}^{\otimes m}}
\newcommand{\DS}[1]{\bigoplus_{m \ge 0} {#1}^{\otimes m}}
\newcommand{\Fpara}{\mathcal{F}}\newcommand{\Fparainv}{\mathcal{F}_{\text{inv}}}
\newcommand{\Sig}{\operatorname{\mathbf{S}}}
\newcommand{\SigT}{\operatorname{\mathbf{\bar S}}}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}

\newtheorem{definition}{Definition}




\begin{document}
\twocolumn[
	\icmltitle{Bayesian Learning from Sequential Data using \\ Gaussian Processes with Signature Covariances}
	




\icmlsetsymbol{equal}{*}
	
	\begin{icmlauthorlist}
		\icmlauthor{Csaba Toth}{to}
		\icmlauthor{Harald Oberhauser}{to}
	\end{icmlauthorlist}
	
	\icmlaffiliation{to}{Mathematical Institute, University of Oxford, Oxford, United Kingdom}
	
	\icmlcorrespondingauthor{Csaba Toth}{csaba.toth@maths.ox.ac.uk}
	\icmlcorrespondingauthor{Harald Oberhauser}{harald.oberhauser@maths.ox.ac.uk}
	
\icmlkeywords{time series, Gaussian Processes, Signatures, Bayesian Machine Learning, Deep Learning, ICML}
	
	\vskip 0.3in
	]
	




	\printAffiliationsAndNotice{}  

	\begin{abstract}       
		We develop a Bayesian approach to learning from sequential data by using Gaussian processes (GPs) with so-called signature kernels as covariance functions.
    This allows to make sequences of different length comparable and to rely on strong theoretical results from stochastic analysis.
    Signatures capture sequential structure with tensors that can scale unfavourably in sequence length and state space dimension.
    To deal with this, we introduce a sparse variational approach with inducing tensors. 
    We then combine the resulting GP with LSTMs and GRUs to build larger models that leverage the strengths of each of these approaches and benchmark the resulting GPs on multivariate time series (TS) classification datasets.
    	\end{abstract}
	
	\section{Introduction} \label{section:introduction}
	The evolution of some state variable, parameter or object gives naturally gives rise to \textit{sequential data}, which is defined by having a notion of order on the incoming information.
  The ordering relation, or \textit{index set} does not have to represent physical time, but for simplicity we will call it as such.
  For example, besides time series, sources of sequential data are text \cite{Pennington2014Glove}, DNA \cite{Heather2016DNA}, or even topological data analysis \cite{chevyrev_nanda_oberhauser_2018}.
  This ubiquity of sequential data has received special attention by the machine learning community in recent years.
  This paper is motivated by the following three approaches:










 \paragraph{Deep learning approaches.} 
  Deep learning approaches, such as the celebrated LSTM network \cite{hochreiter1997long}, other forms of RNNs \cite{Cho2014} and convolutional networks have successfully been applied to a variety of tasks involving sequential data \cite{Sutskever2014Seq2Seq, Oord2016Wavenet}. Deep learning models can approximate any continuous function, but the cost is a large number of parameters, high variance and poor interpretability.
  This leaves the door open for alternative approaches not only as competitors, but as complementary building blocks in a larger model.
	
  \paragraph{Bayesian approaches.}
  Often not only point predictions, but estimates of the associated uncertainties are required \cite{Ghahramani2013Bayesian}.
  GPs \cite{Rasmussen2006Gaussian} provide flexible priors over functions of the data in nonparametric Bayesian models.
In the context of sequential data, two prominent ways to use GPs are: \begin{enumerate*}[label=(\arabic*)] \item using as covariance functions kernels specifically designed for sequences~\cite{lodhi2002text, Cuturi2011GA, Cuturi2011AR, AlShedivat2017Recurrent}, \item  modelling the evolution in a latent space, that emits the observations, as a discrete dynamical system with a GP prior on the transition function, a model called the Gaussian Process State Space Model (GPSSM) \cite{Frigola2013MCMC, Frigola2014Variational, Mattos2016Recurrent, Eleftheriadis2017Identification, Doerr2018Proba, Ialongo2019Overcoming}. \end{enumerate*}
  These two approaches are not mutually exclusive; if one models the latent system as a higher order Markov process, then sequence kernels can incorporate the effect of past states.
	
  \paragraph{Signature approaches.}
  The \textit{signature feature map} is a well-developed tool from stochastic analysis that represents a sequence as an element in a linear space of tensors, \cite{chen-58, Lyons2007Differential}.
  While not a mainstream machine learning approach, it is gaining attention since it can represent \emph{non-linear} functions of sequences as \emph{linear} functions of signature features, and can be made invariant to parametrization similar to dynamic time warping (DTW).
  For example,~\cite{Kidger2019DeepSig} use them as layer in a deep learning architecture;~\cite{KiralyOberhauser2019KSig} introduce kernels for sequences by taking inner products of signature features;
  \cite{ChevyrevOberhauser18} use them for maximum mean discrepancies between laws of stochastic processes. In particular, if  is a kernel for vector-valued data, then \cite{KiralyOberhauser2019KSig} uses signatures to derive the following kernel
  
  for two sequences  and , with the double difference operator defined as , and the sums are taken over multi-indices  with  (and analogous for ) and some explicitly computable coefficients .

	\paragraph{Our contribution.}
  In principle, one can just use the signature kernel and algorithms from~\cite{KiralyOberhauser2019KSig} as covariance to define a GP for sequential data. 
  However, the computational complexity becomes quickly prohibitive and the low-rank approximation too crude, which ultimately does not lead to competitive results on many TS benchmarks.
  We therefore develop a different approach to signature covariances that builds on two recent advances in GP inference, namely variational inference \cite{Titsias2009Variational, Hensman2015Scalable, matthews2016sparse} and inter-domain inducing points~\cite{lazaro2009inter} to alleviate the computational burden.
  In particular, we show that one can use sparse tensors as inter-domain inducing points by optimizing a variational bound.
  Moreover, we use this GP as a building block in combination with RNNs to build models that combine the strenghts of these different tools. 
  This results in scalable inference algorithms and we use this to benchmark on standard TS datasets \begin{enumerate*}[label=(\roman*)] \item against popular non-Bayesian time series classifiers purely in terms of accuracy, \item against alternative Bayesian models by comparing the calibration of uncertainties for predictions \end{enumerate*}.
    Code and benchmarks are publically available at \url{http://github.com/tgcsaba/GPSig}.
	


\section{Background and Notation}\label{sec:background}
Given data  consisting of  inputs  with labels , a common Bayesian approach is to put a prior on a set of functions , update this prior by conditioning on , and then use the resulting posterior to make inference about the label  of an unseen point . 
When this is done with Gaussian priors, the central object is a GP  which is specified by mean and covariance function.
Throughout we interchangibly use the notation  and .
Below we recall how covariances can be constructed from feature maps and discuss the case when  is a space of sequences of arbitrary length.

\paragraph{The feature space view.}
Given a map  that injects  into a linear space , a natural way to put a prior on a function class  is to consider \emph{linear} functions of  as model, that is  to model  for some ``weights`` . Uncertainty about  is then specified by uncertainty about  (and the hyperparameters of ). 
We refer to  as a \emph{feature map} and to  as \emph{feature space}.
Throughout we assume that  is a centered GP and predictions about unseen points can then be made by Gaussian conditioning.    
If the task is classification where the labels  are discrete, such an approach can be still applied by using a GP  as \emph{nuiscance function} to put a \emph{prior on the class membership probability} by specifying  where  is for example a sigmoid.

\paragraph{Polynomial features.} 
The classical example is  and

where  is a tensor. 
We recall background on tensors in Appendix \ref{app:tensors}. 
If we set  and put a centered Gaussian prior on , then by linearity of the tensor product and expectation it follows that

where .
Taking  to be an isotropic diagonal matrix  recovers the polynomial kernel,

where we use the convention .
Many variations exist, for example other classes of polynomials, such as Hermite polynomials (the eigenfunctions of the classic RBF kernel), can increase the effectiveness, since they allow to make the associated feature expansion infinite dimensional.
However, what makes any such class of polynomials a sensible choice for  is that by the Stone--Weierstrass theorem, any continuous compactly supported function  can be arbitrary well approximated as linear functions of .
This approximation property often runs under the name \emph{universality} \cite{Micchelli2006Universal, sriperumbudur2011universality}.

\paragraph{Sequences as paths.}
We study the case when one observation  is a sequence of  tuples, , and each tuple  specifies that at ``time''  a vector  was measured.
We denote with  the set of all such sequences.
and emphasize that the length  is not fixed, which is a common case in real-world data.
We now introduce an even larger set than : 
sequential data evolves in discrete time but often arises by sampling a quantity that evolves in continuous time.
Thus above the set  of sequences lurks the larger set of finite horizon paths 

which are simply continuous -valued functions on some bounded time-interval. 
Here  denotes the usual bounded variation norm\footnote{Our approach generalizes to much rougher paths, such as Brownian trajectories and we give details in Appendix~\ref{app:signatures}.}. 
The set  naturally embeds into  by mapping a sequence  to the path that is given by linear interpolation between the points ; formally  maps to the element of  defined as
 
for .
Henceforth, we implicitly use this embedding, i.e.~with slight abuse of notation, given  we also write . 
Key to our approach, and what makes it different to classic approaches, is to define a GP indexed by the larger set  rather than just .
At first, this looks wasteful since  is much bigger than  and in practice one only has access to discrete time data (already for storage reasons).
But on a theoretical side, going from discrete time to continuous time has two big advantages:
\begin{enumerate*}[label=(\roman*)]
  \item by construction, such a GP is consistent in the high-frequency limit (that is when we sample an object evolving in continuous time at higher and higher frequencies),
  \item we can make use of well-developed theory from stochastic analysis; in particular we use so-called signature features for paths.
\end{enumerate*}
\section{From signature features to covariances}\label{sec:our GP}
The signature feature map  can be seen as a generalization of the polynomial feature map  as defined in~\eqref{eq:moments} from the domain  of vectors to the domain of paths .
It is defined as

where  and . 
Signatures are classic objects in stochastic analysis, but probably unfamiliar to researchers in ML and we provide background in Appendix~\ref{app:signatures}. Additionally, we recommend \cite{CK16} for a hands-on introduction to signature features that provides a good complement to our presentation, motivating signatures as a generalization of polynomial features to sequences.

For what follows, only three facts will be used about :
\begin{enumerate*}[label=(\arabic*)]
\item
  it maps paths of different length to the same space, thus makes paths of different length comparable,  
\item functions of paths can be arbitrary well approximated by \emph{linear} functions of ,
\item it distinguishes paths that follow different trajectories, but not paths that only differ by parametrization).
\end{enumerate*}
These points explain why signature features  are a natural generalization of polynomial features : not only do they use the same feature space (sequences of tensors), they also have the same attractive properties such as being able to approximate continuous functions.
Below we discuss how to make  distinguish paths with different time-parametrizations.

\begin{table}[t]
    \begin{center}
    \begin{small}
        \begin{tabular}{lrr}
        \toprule
        & Vectors & Paths \\
        \midrule
        Domain  &  & \\
        Features &  & \\
        Feature space &  & \\
        Functions &  & \\ 
        Covariance & & \\
        \bottomrule
        \end{tabular}
    \end{small}
    \end{center}
    \label{table:vectors_vs_paths}
    \caption{Comparison of polynomial and signature features}
\end{table}

\paragraph{Parametrization (in)variance.}
A classic empirical finding that led to DTW is that functions of sequences are to a certain degree invariant to the time parametrization: for example, different speakers pronounce words at different speeds. 
However, sometimes the parametrization matters, e.g.~for financial data.
Thus we do not only care about the set of functions 

but also about the subset of it that consist of parametrization invariant functions. 
To make this precise, we call  a \emph{reparametrization} of  if there exists a a smooth increasing function  (the ``time change'') such that  for all .
We call an element  of~\eqref{eq:tree-like invariant F} \emph{parametrization invariant} if  when  and  are reparametrizations. 
Often the function we want to learn is invariant to a bit of reparametrization but not extreme reparametrization, so we need a more nuanced way to quantify parametrization (in)variance.
Hence, what we really want is a hyperparameter  that signifies the degree of parametrization invariance: for  all the mass of our prior should concentrate on the ``extreme case'' that is the subset of \eqref{eq:tree-like invariant F} consisting of functions that are parametrization invariant; and as  gets increased the probability mass should spread out to functions that are sensitive to parametrization. 
This would allow to infer the degree of parametrization invariance by automatic relevance discovery (ARD). 
To accomplish this, we parametrize signature features with a parameter  as follows

where .
This simply makes the parametrization part of the trajectory  by adding an extra coordinate.
Since signatures distinguish different trajectories (but not the speed at which we run through them), it follows that for ,

and for  we have, 

since the extra coordinate is ``switched off''. 
Here,  denotes tree-like equivalence, but we invite the reader to read  as saying that  is a reparametrization of .
This is strictly speaking not true and we give the precise mathematical statement in Appendix~\ref{app:signatures}, but note that for real-world data, tree-like equivalence is synonymous with reparametrization.

\paragraph{Signature covariances.}
Following the feature space view, we now argue in complete analogy to the case of the classical polynomial feature map  for , and define a centered GP  by putting a centered Gaussian prior on  and setting

The GP is hence fully specified by its covariance function 

that has  as hyperparameters.
In particular, choosing an isotropic covariance structure for  gives

where the integrations are over the simplices  and . Furthermore, for the special case of paths that arise as linear interpolation of sequences , this covariance reduces to iterated sums of increments

for some explicitly known constants , where the inner sums are over all -tuples  and  with  and , and the convention that the empty summation is equal to .


\paragraph{GP regularity.}
One expects that the GP with covariance \eqref{eq:signature covariance} has nice regularity properties, however, the index set  is a very large space so some care is needed. 
In Appendix~\ref{app:gpsig}, we compute covering numbers that yield explicit bounds on the modulus of continuity in terms of the path path length . 
\begin{theorem} \label{thm:continuity}
  Let  and .
  There exists a centered GP  with  as defined in~\eqref{eq:signature covariance} as covariance function and that has continuous sample paths .
  Further, an explicit bound on its modulus of continuity in terms of  is given in equation~\eqref{eq:modulus}. 
\end{theorem}
We now have a well-defined GP for Bayesian inference for sequences at hand that inherits many of the attractive properties of signature features. 
To turn this into useful models for large TS benchmarks we develop efficient inference algorithms in the next section.

\section{Sparse variational inducing tensors}\label{sec:sparse var tensor}
To reiterate, we are given data  consisting of  sequences  of maximal length  that evolve in  with labels , and the task is to predict labels  of unseen points .
For sequential data, the sample size  and associated covariance matrix inversion is not the only compational bottleneck but also the maximal length of sequences , and the dimension  of the state space matter: ,  and  can be simultaneously large. 

In this section, we introduce a sparse inference scheme to approximate the posterior of our GP, that locates the inducing points in a space other than the data-domain; an approach that is usually coined the term \textit{inter-domain} sparse variational inference \cite{lazaro2009inter, matthews2016sparse}. This allows for more efficient data-representation and faster inference. 
Key to our approach is that signature features take values in a well-understood subset of the feature space .
This allows as us to augment the index set with structured tensors, and locate inducing points in this larger index set.

    \paragraph{Variational inference.}
	As is well-known, inference for GPs scales as , see Section 3.3.~in~\cite{Rasmussen2006Gaussian}.
	This first led to \emph{sparse} models, \cite{quinonero2005unifying}, that select a subset  of  consisting of  points, and subsequently to \emph{pseudo-inputs}, \cite{snelson2006sparse}, that select points  that are not necessarily in .
	This was a big step towards complexity reduction, but pseudo-inputs are prone to overfitting, \cite{MatthewsDPhil}.
	A different idea is to treat  as parameters of a variational approximation \cite{Titsias2009Variational} and not as model parameters; that is the points  are choosen simultaneously with the hyperparameters of the GP by maximising a lower bound on the log-marginal likelhood , the so-called \emph{evidence lower bound} (ELBO), given as
	
	where  and  denotes the GP evaluated at the data-points and the inducing locations. Typically,  is given a free-form multivariate Gaussian to be learnt from the data, and then extended to other indices of the GP by \textit{prior conditional matching}, i.e. .	Initially applied to regression, this was extended to classification~\cite{chai2012variational, Hensman2015Scalable}.
	Among its advantages are that it gives a nonparametric approximation to the true posterior, adding inducing points only improves the approximation, and any optimization method can be used to maximize the ELBO, most importantly, stochastic optimization; see \cite{Hensman2013GaussianPF, bauer2016understanding,bui2016unifying}.
	

\paragraph{Inter-domain approaches.}Another idea is to go beyond the original index set and place inducing points  in a different space , that is, given a centered GP  one augments the original index set  by a set  to define a new GP  and then locates the inducing points in this bigger model.
	This was suggested in~\cite{lazaro2009inter} in the context of integral transforms, which was extended in~\cite{Hensman2016Fourier}, and studied in more generality in~\cite{matthews2016sparse}.
	In general, it is not obvious how to find a useful augmentation set  and define the covariance enlarged to .
  \subsection{A sparse variational tensor augmentation.}
	Given any GP with a covariance function  where  is explicitly known\footnote{Mercer's Theorem guarantees the existence of , but not in a sufficiently explicit form.}, we propose that a natural augmentation candidate is the ``feature space''  itself.
	The covariance function  of  can be simply extended to  by linearity,
  
   for , , ; analogous for  with . 
For our GP,  where we denote . 
We can thus extend our signature covariance~\eqref{eq:signature covariance} to  by~\eqref{eq:augment}. 
This provides a flexible class of inducing point locations  by optimizing over elements of the tensor algebra .
We coin these inducing point locations as \textit{inducing tensors}.
	
	\paragraph{Consistency of augmentation.}	
	A subtle point about augmenting the index set is that maximizing the ELBO in \eqref{eq:elbo} is not necessarily equivalent anymore to minimizing a rigorously defined KL divergence between the true posterior process and its approximation over the unaugmented index set. In \cite{matthews2016sparse}, a sufficient condition given for this to hold is that the prior GP evaluated at the newly added indices is deterministic conditioned on the original GP.
  In the case of~\eqref{eq:augment}, this is easily seen to be true, since the augmented indices arise as linear combinations of elements in the original index set. 
Therefore, the corresponding GP evaluations arise as linear combinations of evaluations of the original process by the fact that the feature space  is a \textit{representation} of the \textit{Hilbert space generated by the process} \cite{berlinet2003reproducing}.
	
	\paragraph{Representation of inducing tensors.}
We define our sparse inducing tensors as 
	
  where  and  for .
  We remark that this construction does not generally give tensors that can be signatures of paths.
  However, they can be represented as linear combinations of signatures, hence the previous argument about the augmentation carries over. Also, informally, what gives the data-efficiency of inducing tensors is exactly that they are not represented in a basis of signatures, but as sparse tensors.


By linearity of integration and the inner product, the inducing point covariance  equals the inner product 
	
the cross-covariance ,
	
where the integration is over the simplex .
  Finally, note that we just need to evaluate the above for piecewise linear paths since these are the only paths that arise via the embedding~\eqref{eq: pcw linear}, .
  For such paths, the above integrals reduce to iterated sums, hence  equals
	
  where the sum is taken over all -tuples  of the form  and  is given by an explicit calculation.
  Similarly to \eqref{eq:discrete_sig_cov}, replacing  with  if there are no repeating indices in  and otherwise with  gives a good approximation\footnote{It converges to  when the grid gets finer, , see \cite{ChevyrevOberhauser18} to~\eqref{eq:discr_cross_cov_n}.} .
Below we use this approximation to~\eqref{eq:discr_cross_cov_n} since it makes the recursive algorithms simpler but note that a simple modification exactly computes~\eqref{eq:discr_cross_cov_n} for a marginal computational overhead.\subsection{Algorithms.}
	We need to compute the three covariance matrices:~\begin{enumerate*}[label=(\arabic*)] \item  of inducing tensors  and inducing tensors , \label{it:tens2tens} \item  of inducing tensors  and sequences , \label{it:tens2stream} \item  of sequences  and sequences .\label{it:stream2stream} \end{enumerate*}
	Using the above tensor representations allows to give vectorized algorithms for~\ref{it:tens2tens} and~\ref{it:tens2stream} in Algorithms~\ref{alg:inducing_cov} and \ref{alg:cross_cov}, respectively.
	For~\ref{it:stream2stream} we use a modification of Algorithm 3 from~\cite{KiralyOberhauser2019KSig} which we recall in Appendix \ref{app:stream_covs}.
	We use notation defined in Appendix~\ref{app:alg_notation}, which can be briefly summarized as:  denotes the slice-wise sum operator,  the (forward) cumulative sum operator,  the shift operator, and  denotes the element-wise product of arrays.
  Additionally, we set  for .
  For ,  denotes the time to compute ,  the memory requirement of . 
	
  \begin{proposition}\label{eq:complexity}
	Algorithm~\ref{alg:inducing_cov} computes the covariance matrix  of  inducing points in  steps.
	Algorithm~\ref{alg:cross_cov} computes the cross-covariance matrix  in  steps.
  Additionally to storing the inducing tensors , Algorithm~\ref{alg:inducing_cov} requires  memory, Algorithm~\ref{alg:cross_cov} requires  memory.
  \end{proposition}
  Proposition~\ref{eq:complexity} follows by inspection of the algorithms and we emphasize the following points:
  \begin{enumerate*}[label=(\roman*)]
  \item Both algorithms are linear in the maximal sequence length .
  \item  is a hyperparameter, and in all our experiments we learnt from the data , thus the quadratic complexity in  is negligible.
  \item The memory cost of inducing tensors  is much less than for the data , which is stored in  memory, which is important because the inducing tensors are variational parameters, and not amenable to subsampling, while the learning inputs can be subsampled as noted by~\cite{Hensman2013GaussianPF}.
  Especially for GPUs memory cost is decisive and such savings are very important.
  \end{enumerate*}
  
  The computation of  detailed in Appendix \ref{app:stream_covs} has time complexity  and memory of . However, given a factorizing likelihood, one only requires , which eliminates the quadratic cost in .
  It turns out that this is enough to train on GPUs with reasonable minibatch sizes (e.g. ) on several real world datasets. We remark that the low-rank algorithms in \cite{KiralyOberhauser2019KSig} allow to trade off accuracy for linear cost in , but we found that using the full-rank algorithm performs much better, and the above will allow us to apply it to several datasets with great results. Finally, note that the ELBO \eqref{eq:elbo} requires an additional matrix inversion and multiplication in  time, which is not significant in our case.
	
\begin{algorithm}[tb]
		\caption{Computing the inducing covariances }
		\label{alg:inducing_cov}
		\begin{algorithmic}[1]
			\STATE {\bfseries Input:} Tensors , \\ scalars , depth  
			\STATE Compute  for ,  and  \\
			\STATE Initialize  for 
			\FOR{ {\bfseries to} }
			\STATE Assign 
			\FOR{ {\bfseries to} }
			\STATE Iterate 
			\ENDFOR
			\STATE Update 
			\ENDFOR
			\STATE {\bfseries Output:} Matrix of inducing covariances 
		\end{algorithmic}
	\end{algorithm}
	
	\begin{algorithm}[t]
		\caption{Computing the cross-covariances }
		\label{alg:cross_cov}
		\begin{algorithmic}[1]
			\STATE {\bfseries Input:} Tensors , \\ sequences , \\ scalars , depth  
			\STATE Compute  for , , ,  and  \\
			\STATE Initialize  for , 
			\FOR{ {\bfseries to} }
			\STATE Assign 
			\FOR{ {\bfseries to} }
			\STATE Iterate 
			\ENDFOR
			\STATE Update 
			\ENDFOR
			\STATE {\bfseries Output:} Matrix of cross-covariances 
		\end{algorithmic}
	\end{algorithm}
	
\begin{table*}[t]
	\caption{Average ranks of GPs on datasets \cite{baydogan2015multivarate} with the 1\textsuperscript{st} and 2\textsuperscript{nd} best in bold and italicized for each row}
	\label{table:avg_ranks}
\begin{center}
		\begin{small}
			\begin{sc}
				\begin{tabular}{lrrrrrr}\toprule
					 & GP-Sig-LSTM & GP-Sig-GRU & GP-Sig & GP-LSTM & GP-GRU & GP-KConv1D \\
					\midrule
					Mean rank (nlpp, ) &  &  &  &  &  &  \\
					Mean rank (acc., ) &  &  &  &  &  &  \\
					Mean rank (nlpp, ) &  &  &  &  &  &  \\
					Mean rank (acc., ) &  &  &  &  &  &  \\
					Mean rank (nlpp, all) &  &  &  &  &  &  \\
					Mean rank (acc., all) &  &  &  &  &  &  \\
					\bottomrule
				\end{tabular}
			\end{sc}
		\end{small}
	\end{center}
\end{table*}

\begin{figure*}
\begin{minipage}{0.44\textwidth}
        \centering
        \begin{tikzpicture}[shorten >=1pt,draw=black!50, node distance=\layersep]

        \node[node, text centered, shading=green] at (0.0,0.5) (x) {};
        \node[text centered] at (0.0,1.25) {};
        
        \node[node, text centered, shading=blue] at (1.8,0.5) (kx) {};
        \node[text centered] at (1.8, 1.25) {};
        
        \draw[->, color=black] (x) -- (kx);
        
        (4.0,0) rectangle ++(1,1);
        \node[node, text centered, shading=blue] at (3.6,0.5) (sigx) {};
        \node[text centered] at (3.6, 1.25) {};
        
        \draw[->, color=black] (kx) -- (sigx);
        
        \node[node, text centered, shading=red] at (5.4,0.5) (gp) {};
        \node[text centered] at (5.4, 1.25) {};
        
        \draw[->, color=black] (sigx) -- (gp);
        \end{tikzpicture}
        \captionof{figure}{The GP-Sig model}
        \label{fig:GPSig}     
\end{minipage}
\begin{minipage}{0.56\textwidth}
        \centering
        \begin{tikzpicture}[shorten >=1pt,draw=black!50, node distance=\layersep]

        \node[node, text centered, shading=green] at (0.0,0.5) (x) {};
        \node[text centered] at (0.0, 1.25) {};
        
        \node[node, text centered, shading=blue] at (1.8,0.5) (rnnx) {};
        \node[text centered] at (1.8, 1.25) {};
        
        \draw[->, color=black] (x) -- (rnnx);
        
        \node[node, text centered, shading=blue] at (3.6, 0.5) (krnnx) {};
        \node[text centered] at (3.6, 1.25) {};
        
        \draw[->, color=black] (rnnx) -- (krnnx);
        
        \node[node, text centered, shading=blue] at (5.4, 0.5) (sigkrnnx) {};
        \node[text centered] at (5.4, 1.25) {};
        
        \draw[->, color=black] (krnnx) -- (sigkrnnx);
        
        \node[node, text centered, shading=red] at (7.2, 0.5) (gp) {};
        \node[text centered] at (7.2, 1.25) {};
        
        \draw[->, color=black] (sigkrnnx) -- (gp);
        \end{tikzpicture}
        \captionof{figure}{The GP-Sig-RNN model}
        \label{fig:GPSigRNN}     
\end{minipage}
\end{figure*}

\paragraph{Variations.}
The following variations produce a more flexible covariance function:
\begin{enumerate*}[label=(\roman*)]
  \item given a nonlinear function  into a linear space , lift a sequence  to a path by taking the linear interpolation of ; with  the identity on  this recovers the original embedding~\eqref{eq: pcw linear},    
  \item 
adding lags is a classic time series pre-processing technique, justified by Takens' theorem, \cite{takens1981detecting}, that guarantees that attractors in a high-dimensional dynamical system can be reconstructed from low-dimensional observations.
\end{enumerate*}
Both points add non-linearities to the feature space which can make the learning more efficient.
If the original sequence evolves in , this preprocessing results in a sequence (and then path) that evolves in a general high-dimensional space .
However, formulas~\eqref{eq:augment},~\eqref{eq:inducing_cov_n}, and~\eqref{eq:discr_cross_cov_n} show that only inner product evaluations on  are used and these can be computationally cheap even if  is high or even infinite dimensional. 
For example, following~\cite{KiralyOberhauser2019KSig} we may take a kernel  and use , to build a sequence  in the RKHS of .
The only change in complexity is to replace  in the big -bounds by the cost of the kernel evaluation. 
Such extensions also increase the number of hyperparameters which can have adversarial effects, but in our experiments both extensions led generically to better results. 

\section{Experiments} \label{sec:experiments}


\paragraph{TS classification.}

Using GPFlow~\cite{Matthews2017GPflowAG}, Keras~\cite{Chollet2015Keras}, we implemented three models: GP-Sig, GP-Sig-LSTM, and GP-Sig-GRU.
All three use the signature covariance with the sparse inducing tensors of Section~\ref{sec:sparse var tensor}.
GP-Sig is a plain vanilla variational GP classifier.
Previous applications of neural nets to covariance constructions, in particular~\cite{Wilson2016DeepK, AlShedivat2017Recurrent}, inspired GP-Sig-LSTM and GP-Sig-GRU that include an RNN as a sequence-to-sequence transformation
with  hidden units; see Figures~\ref{fig:GPSig} and \ref{fig:GPSigRNN} where  denotes augmentation with lags and  a static kernel as in above variation paragraph. We benchmarked these GP models on  multivariate TS classification datasets, a collection introduced in~\cite{baydogan2015multivarate} that has become a semi-standard archive in TS classification, e.g. we cite 7 papers in Appendix \ref{app:benchmark} that use these datasets. The same datasets are also used in \cite{Fawaz2019review} to compare several deep learning architectures for TSC.


As Bayesian baselines we used three GP models: \begin{enumerate*}[label=(\roman*)] \item
GP-LSTM and \item GP-GRU consist of an LSTM and a GRU network with an RBF kernel on top, in which case the RNNs are used as a sequence-to-vector transformation from  to ; \item GP-KConv1D uses the convolutional kernel introduced in \cite{Wilk2017ConvGP} in -dimension (time) \end{enumerate*}. Throughout we used sparse variational inference: for GP-Sig-LSTM, GP-Sig-GRU, GP-Sig, the inducing tensors detailed in Section~\ref{sec:sparse var tensor} are used; for GP-LSTM and GP-GRU the inducing points are located in the output space of the RNN layer, ; for GP-KConv1D, the inducing patches of \cite{Wilk2017ConvGP} are used.

We used  for all models\footnote{Using  is clearly superfluous for small datasets, which is fixed for the sake of consistent settings across datasets.}; further all use a static kernel in one form or another, which we fixed to be the RBF kernel.
The signature kernel was truncated\footnote{For these experiments, the  value seemed to give an optimal trade-off between computational complexity and expressiveness of the kernel, see Appendix \ref{app:implementation} for more details.} at , and for GP-Sig  lags were used; the GP-Sig-RNNs did not use lags, as the sequence of hidden states already incorporate lagged information about past observations. The window size in GP-KConv-1D was set to . 
The RNN-architectures were selected independently for all models by grid-search among  variants, that is, the number of hidden units from  and with or without dropout. For training, early stopping was used with  epochs patience; a learning rate of ; a minibatch size of ; as optimizer Adam \cite{kingma2014adam} and Nadam \cite{Dozat2015IncorporatingNM} were employed. Implementations are detailed in Appendix \ref{app:implementation}, the datasets in Appendix \ref{app:datasets}, the training and grid-search methodology in Appendix \ref{app:training}.


\paragraph{Discussion of results.}
For GPs, we report accuracies and negative log-predictive probabilities (nlpp), the latter take not only accuracies, but the calibration of probabilities into account as well.
Table \ref{table:avg_ranks} shows the average ranks among the GPs.
The full table of nlpps and accuracies with mean and standard deviation over 5 model trains are reported in Appendix~\ref{app:benchmark} in Table~\ref{table:full_nlpp_results} and Table~\ref{table:full_acc_results}.
As non-Bayesian baselines, we report accuracies of eight frequentist TS classifiers in Table~\ref{table:freq_acc_results}. On Figure \ref{fig:boxplots}, we visualize the box-plot distributions of \begin{enumerate*}[label=(\roman*)] \item negative log-predictive probabilities of GPs, \item accuracies of both GPs and frequentist methods. \end{enumerate*}

The signature models perform consistently the best in terms of average rankings of both nlpp and accuracy among the GPs. Particularly, they achieve stronger mean performance and a smaller variance across datasets. To explain this, inspecting the results in Tables \ref{table:full_nlpp_results}, \ref{table:full_acc_results}, we observe that all other GP baselines perform very poorly on some datasets, while the signature based models perform at least moderately well on \emph{all datasets}. We believe this ties in to the universality property of signatures, see Appendix \ref{app:univ}. The convolutional GP, GP-KConv1D, which also has a very small parameter set, performed rather competitively with the deep kernel baselines, even on larger datasets. Comparison among variants of GP-Sig can be summarized as follows: for smaller datasets (), GP-Sig outperforms other variants as it has a very small parameter set; for larger datasets , GP-Sig-LSTM performs best which conforms with the intuition that RNNs suffer from small sample sizes.
A related observation is that GP-LSTM and GP-GRU perform about on par, while GP-Sig-LSTM does much better than GP-Sig-GRU, which suggests that the signature makes explicit use of the additional gate in the LSTM network.

Compared only in terms of accuracy, GP-Sig competes with frequentist classifiers: it outperforms the usual DTW baseline and competes with state-of-the art classifiers such as MUSE and MLSTMFCN. Purely based on accuracy, these win overall, but the difference is usually small, hence the extra Bayesian advantages come at a small cost. Furthermore, since the MLSTMFCN is also a deep learning baseline, it would be interesting to see how it performs incorporated into a deep kernel, possibly used as a sequence-to-sequence transformation with the signature kernel on top. Obviously TS classification is a vast field and many other models could be considered; e.g.~we did not use recurrent GPs or GPSSMs since
\begin{enumerate*}[label=(\arabic*)]
\item 
  they have so-far not been used for TS classification, possibly because there is no sequential nature in the output space,\item we did not find a GPflow implementation that would allow to use sequence kernels in the GP transition function. (Implementation of~\cite{Ialongo2019Overcoming} does currently not allow taking subsequences of past states.
  An implementation would require much further work, but an interesting project would be to combine our models.) 
\end{enumerate*}

\begin{figure}[t] 
	\centering
	\begin{minipage}{0.55\textwidth}
		\hspace{-40pt}
		\centering
		\includegraphics[width=3.25in]{images/tensors_vs_streams_ELBO_AUSLAN.pdf}
\end{minipage}
	\begin{minipage}{0.55\textwidth}
		\hspace{-40pt}
		\centering
		\includegraphics[width=3.25in]{images/tensors_vs_streams_ACC_AUSLAN.pdf}
\end{minipage}
	\begin{minipage}{0.55\textwidth}
	    \hspace{-40pt}
		\centering
		\includegraphics[width=3.25in]{images/tensors_vs_streams_NLPP_AUSLAN.pdf}
\end{minipage}
\caption{Achieved ELBO (top), accuracy (middle), mean nlpp (bottom) after  epochs of training the variational parameters with random initialization and pre-learnt kernel hyperparameters, that were treated as fixed. Solid is the mean over  independent runs, dashed is the -std region.}
\label{fig:ind_tens_vs_ind_seq}
	\vspace{-10pt}
\end{figure}

\paragraph{Inducing tensors vs inducing sequences.} Our results rely on the inter-domain approach using tensors to locate inducing points from Section~\ref{sec:sparse var tensor}. An alternative is to use sequences for the inducing points, , and controlling their maximal length  to be of order , i.e. .
We coin this approach \textit{inducing sequences}.
Intuitively, one expects the inducing tensors to be more efficient than inducing sequences, since they make full use of the structure of the signature feature space/covariance.
To test this intuition, we compared the performance of the inducing tensors and inducing sequences subject to both having the same computational complexitiy.
For this experiment, we took the AUSLAN dataset~\cite{Dua2017UCI}, which consists of  classes for  training examples.
This is a challenging dataset as the inducing variables need to characterize the abundance of classification boundaries.

We used GP-Sig with the same settings as in the previous experiments.
The hyperparameters of the kernel were a-priori learnt with  inducing tensors, and we purely investigated how the quality of the approximation changes for both approaches by varying the number of inducing points .
For each number of inducing variables, both approaches were trained independently  times for  epochs with random initialization of the inducing variables, for details on which see Appendix~\ref{app:training}.
We plot on Figure \ref{fig:ind_tens_vs_ind_seq} three metrics: \begin{enumerate*}[label=(\arabic*)] \item the achieved ELBO on the training set, \item the achieved accuracy, and \item nlpp on the testing set \end{enumerate*}.
At  both approaches are close to saturation, but the inducing tensors consistently perform better.
We remark that in practice, an important aspect is also how well the kernel hyperparameters can be recovered, that we did not consider here, and is a tricky question for sparse variational inference in general \cite{bauer2016understanding}.
Although, intuition suggests that the closer the model to saturation is with respect to the inducing points, the more consistent should the optimization be with un-sparsified variational inference.

\begin{figure}[t]
	\centering
	\begin{minipage}{0.55\textwidth}
        \hspace{-40pt}
        \centering
		\includegraphics[width=3.40in]{images/tensors_umap_AUSLAN.png}
\end{minipage} \hspace{-5pt}
	\vspace{-10pt}
	\caption{A UMAP visualization of the allocation of feature representations of data-points (coloured), and inducing tensors (black) in the feature space on the AUSLAN dataset.}
\label{fig:tens_umap}
	\vspace{-10pt}
\end{figure}

\paragraph{Visualizing inducing tensors.}
To gain more intuition, we visualized the feature space for one of the trained models on AUSLAN with  inducing tensors.
We used UMAP~\cite{mcinnes2018umap-software} with the semi-metric  for , see~Figure~\ref{fig:tens_umap}.
There are two imminent observations: \begin{enumerate*}[label=(\roman*)] \item in the point cloud corresponding to the data, the classes hardly look linearly separable; \label{umap:point1} \item the tensors, however, seem to live on a completely separate subspace than the data\label{umap:point2}.\end{enumerate*} 
The algorithm achieves  accuracy on this set, therefore, point \ref{umap:point1} is likely due to information being lost in the projection. 
However, point \ref{umap:point2} challenges the intuition about classical sparse variational inference, that the inducing points are located mixed-in with the data-points, concentrating close to the classification boundaries \cite{Hensman2015Scalable}.
In general, the mechanism of how inter-domain inducing points represent the information in the data seems to be more complicated than classically.

To explain point \ref{umap:point2}, we remark that this phenomenon is not surprising at all: signature features live in a manifold that is embedded in the linear tensor space . 
In general, sparse tensors of the form~\eqref{eq:sparse_tens} will \emph{not} be signatures of paths. 
We believe variational inference works because of an interplay of two factors:
Firstly, signatures of finite sequences can be written as finite linear combinations of such sparse tensors. 
Secondly, the prior conditional term used to define .
The feature space is \emph{congruent} to the prior GP~\cite{berlinet2003reproducing}, which means that for , the value of  given  is not only almost deterministic when  is close to any of , but when it is close to any linear combinations of elements in .
By the first remark this can always be achieved given a large enough .
To sum up, the inducing tensors do not represent signature features individually, but form atomic building blocks such that their linear combinations induce the actual variational posterior at the data-examples.



\section{Conclusion}
We used a classical object from stochastic analysis -- signatures -- to define a GP for sequential data.
The GP inherits many of the theoretical guarantees that are known for signature features such as universality and parametrization invariance.
To make it scalable, we develop ``inducing tensors'' that exploit the structure of the feature space, inter-domain inducing points, and variational inference. 
Applied in a plain vanilla variational framework, this yields a classifier, GP-Sig, that is not only competitive in terms of nlpp with other GP models, but also with state-of-the-art frequentist TS classifiers in terms of accuracy alone. As one of our reviewers remarked, several datasets we consider have a strong signal-to-noise ratio, which makes it worthwhile to point out that even for such datasets, the alternative GP baselines suffer on at least some of them, while the proposed models are consistently able to learn on all datasets. This observation ties in to the universality property, and it suggests that GPs with signatures can be a good starting point when building Bayesian models on time series datasets.


We also demonstrate that signatures can be used as a building block in deep kernels to build larger GP models that leverage the benefits of both, RNNs and signatures. Interestingly, we find that the vanilla GP-Sig model outperforms the GP-Sig-RNNs for smaller datasets, conforming to the intuition that smaller sample sizes are detrimental for recurrent neural nets. To really get the best of both worlds, one could insert an additional model selection step, that specifies whether a parametric transformation is layer used before feeding the input into the kernel or not. Alternatively, it could also be possible to increase the flexibility of the sequential GP model while staying within a purely nonparametric framework using deep GPs \cite{damianou2013deep} by e.g.~applying a GP layer as observation-wise state-space embedding before the kernel computation. The inference framework of \cite{salimbeni19deep} for deep GPs could also come in handy when moving to datasets with lower signal-to-noise ratios, which can require GP models capable of handling not only epistemic (reducible) uncertainty, but aleatoric (irreducible) uncertainty in the data. It would also be interesting to see if such sequence kernels can be used to improve recurrent GP models \cite{Mattos2016Recurrent, Ialongo2019Overcoming} by incorporating sequential information into the GP transition function, that could potentially allow for a more efficient latent state representation.





	
	
\section*{Acknowledgments}
CT is supported by the "Mathematical Institute Award" by the University of Oxford, HO is supported by the EPSRC grant “Datasig” [EP/S026347/1], the Alan Turing Institute, and the Oxford-Man Institute.
CT and HO would like to thank the reviewers for helpful and constructive comments.
\bibliography{roughpaths}
\bibliographystyle{icml2020}



\appendix








	\newpage 
	\clearpage
	
    \textbf{\large Supplementary material}
	
	\section{Tensors} \label{app:tensors}
  We recall classical constructions with tensors.




\subsection{Tensor products of vector spaces}
  If  and  then  is the -matrix with indices ,  and the -th entry given as .
  Similarly, for ,, , the tensor  has indices , ,  and its -th entry is given as , etc. 

  In the paragraph about variations in Section~\ref{sec:sparse var tensor}, we mention that one can also lift the sequence to a path evolving in an infinite-dimensional space  rather than  before computing its signatures.
  Since  this requires to take a tensor product of an infinite-dimensional space .
  Since this might be less known in ML, let us briefly recall a coordinate-free definition of the tensor product:
  If  and  are vector spaces (not necessarily finite dimensional) then there exists a linear space  and a bilinear map  such that any other bilinear map on  factors through , that is given any bilinear map  into a vector space , there exists a linear map  such that .
  Further, the vector space  is unique up to isomorphism.
  If  are finite dimensional it is easy to verify that one recovers the coordinate-wise definition we recalled at the beginning of this section.
  If  then we write  instead of ; further by convention we define .
  

\subsection{Sequences of tensors }
The direct product  of vector spaces  is the set of sequences 

In our setting, we apply this when  is a vector space and , the get the space 

That is, an element  is a sequence of tensors of increasing depth, that is  since by convention ,  is a vector, , etc.

The space  is itself a vector space if one defines addition and scalar multiplication coordinate-wise: for ,

That is, if  we add vectors to vectors, matrices to matrices, etc.
We note that the space  is not just a vector space but has also a natural algebra structure and the space  is often referred to as the tensor algebra over .
\subsection{Inner products of tensors}
We have seen how to build out of a linear space  another linear space  of tensors.
If  also carries an inner product,  this extends canonically to an inner product on subset of ; set

and extend linearly to .
In particular, we can take linear functionals of .

\subsection{Example: the classic polynomial features}
Take  with the standard Euclidean inner product.
In Section~\ref{sec:background} we recalled the classic ``polynomial feature map'' that takes a point in  to monomials in coordinates in ,

We can build a functions  by taking linear functionals of , that is for  define

It might be helpful for readers less familiar with tensor products to spell out the definition of  in coordinates:
by definition of the inner product we have 

Spelled out in coordinates,  and , the terms in the sum read as 

Thus formulated in coordinates one has 

which is how the polynomial feature map is often represented, see~\cite{Rasmussen2006Gaussian}.  


  \section{Signature features}\label{app:signatures}
  In this Section we give background on signature features. 
  Signature features can  be seen as a natural generalization of the polynomial feature map, but instead of mapping a point in  to a sequence of tensors, they map paths  to a sequence of tensors. 
  They generalize many of the nice properties of polynomial features such as universality and simulatenuously give the option to ignore the time-parametrization without an explicit search over all possible time changes (like in DTW approaches). 
	\subsection{Definition} \label{appendix:sig_properties}
	By Definition~\eqref{eq: signature}, the signature features are given as iterated integrals  

where  and by convention .
Hence, .
\subsection{Examples}
\paragraph{Coordinate-wise.}
For a path  that evolves in , one can spell this out in coordinates: the -th signature feature  is the tensor that has as its -th coordinate entry the real number computed by a Riemann--Stieltjes integral

where the integration is taken over  and .
\paragraph{Linear paths.}
Consider the path  that just runs along a straight line

where  is a given vector.
Plugging~\eqref{eq: sig coordinates} into the definition of the iterated integrals, we get by a direct calculation that 


We see that for this special case of a path  that is fully described by its increment , the signature features  equal the polynomial features  of the total increment  up to a rescaling by a constant . 
(This is one of the many reasons why signature features are regarded as ``polynomials of paths''). 

\paragraph{Piecewise linear paths.}
In general, these integrals need to be computed by standard integration techniques but for a piecewise linear path , that is  is partitioned into  disjoint intervals, , and  is piecewise linear on each of these pieces,  for  for a vector , then these iterated integrals just reduce to iterated sums,  equals 

where the sum is taken over all tuples  and  is the inverse of the natural number .  
	
\subsection{Parametrization invariance.}
A classic result going back to Chen~\cite{chen-58} shows that the map  is injective up to tree-like equivalence. 
Loosely speaking, tree-like equivalence is from a purely analytic point of view more natural to work with than reparametrization since tree-like equivalence between paths is analogous to Lebesgue almost sure equivalence between sets.
Howevever, we emphasize that from a practical point of view, the difference between paths that are tree-like equivalent and paths that differ by a reparametrization is negligible and we invite the reader to use them as synonyms throughout this article.
Nevertheless, we give the precise definition below and refer the interested reader to~\cite{MR2630037} for a detailed discussion. 
\begin{definition}
  A bounded variation path  is \emph{tree-like} if there exists a continuous function  such that  and such that for all 
  
\end{definition}
\begin{theorem}
  Let  and  be two paths of bounded variation.
  Then 

if and only if  is tree-like where  denotes path concatenation and  denotes time-reversal.
\end{theorem}
In particular this implies that for any function of the form 

 if and only if  and  differ by parametrization (strictly speaking, by a tree-like equivalence).
This ability to factor out time-invariance can be very powerful since the space of all possible time reparametrization is huge and we never make an explicit search over all possible time changes like in the calculation of DTW distance.

\subsection{Parametrization variance.}
Often the functions of sequences  one is interested in, are invariant up to a certain degree of reparametrization but not invariant to extreme reparametrizations.
For a stylized example consider TS that arise as blood pressure measurements from patients responding to medication: some patients respond slower, some faster, depending on metabolism and many other factors. 
Up to a certain degree of time-reparameterisation one should observe a similar shaped TS if the medication works. 
However, the feature map should allow to distinguish extreme cases, e.g.~where the blood pressure is rapidly falling.

To address, we added an extra coordinate to the path  before computing the signature features of this enhanced path , for .
The enhanced path  is never tree-like since the first coordinate  is strictly increasing.
Formulated, differently: this ''trick`' makes the parametrization part of the trajectory. 
Hence, the map

is injective for .


\subsection{Universality.} \label{app:univ}
One of the most attractive properties of the classical polynomial feature map  for vectors ,~\eqref{eq: polynomial feature map}, is that any continuous function  can be uniformly approximated on compact sets as a linear functional of , that is  for some .
The reason is that linear combinations of monomials (polynomials) form an algebra and the Stone--Weierstrass theorem applies.
Such approximation properties of feature maps are usually referred to as ``universality'' in the ML literature.

One of the most attractive properties of the signature feature map  for paths  is that a universality result holds. 
For every continuous ,  compact,  there exists a ,  such that  
 	
The analogous result holds for  when we replace the domain  by equivalence classes of paths (under reparameterisation/tree-like equivalence). 
For a proof and many extensions, see~\cite{ChevyrevOberhauser18}.
\subsection{High-frequency sampling} 
One way to think about the embedding of  is that  represents the ``real-world'' where quantities evolve in continuous time but due to pratical reasons such as storage cost we only have access to their preimage in . 
A natural question is what happens when the sampling gets finer and finer. 
We believe such consistency in the high-frequency sampling limit is important for the same reason, consistency in the number of samples  is important: although in practice we only deal with finite numbers (finite number of samples, sequences rather than paths), we want that our method makes sense as we get more and more information.
In the context of learning with sequences this does not only require to study  but also the limit as the mesh size of that sampling grid converges to . 

\paragraph{Consistency.}More formally, given  consider a sequence  of partitions

with vanishing mesh

Each partition  gives rise to sequence  by sampling  along the time points in .
Following our convention we identify  as a piecewise linear path in  and it is easy to verify that  as .
Informally, as  we go from discrete to continuous time. 
One of the nice properties of our GP covariance, is that it is consistent under such limits: given ,  as . 
Having a well-defined GP on paths that is consistent under such approximations from discrete to continuous time guarantee that no constants blow up as the sequences gets longer (sampling gets high frequent). 

\paragraph{Rough paths.}
So far we assumed that  consists of bounded variation paths but in the ``real-world'', the evolution of quantities is often subject to noise, e.g.~a classical model in physics and engineering is

where  is a bounded variation path but  is a Brownian sample path.
Since Brownian sample paths are not of bounded variation,  is not of bounded variation.
However, the same consistency arguments as above go through but one has to replace the iterated Riemann--Stieltjes integrals by Ito--Stratonovich integrals in the definition of .
Even rougher trajectories such as fractional Brownian motion and non-Markovian processes can be handled that way with so-called rough path integrals.
This is well-beyond the scope of the present article but we refer the interest reader to~\cite{ChevyrevOberhauser18} for such results. 

 \section{GPs with Signature Covariances} \label{app:gpsig}
 We specified a covariance function  on the set  as inner product of the signature map.
 This guarantees that  is a positive definite function and from the general theory of stochastic processes the existence of a centered GP  such that  follows.  
 However, this does not guarantee that the sample paths  are continuous. 
 Seminal work of Dudley~\cite{dudley2010sample} showed that such regularity estimates can be derived by bounding the growth of the covering number of the index set of the GP  under the semi-metric
 
 Already when when the index set is finite dimensional ``nice'' covariance functions can lead to discontinuous GPs, see e.g.~Section 1.4.~in \cite{adler2009random}. 
 Our GP has as index set the space of bounded variation paths  which is infinite-dimensional so some caution is needed. 
 However, as we show below we can cover this space by lattice paths and derive covering number estimates that imply continuity.
\begin{theorem}\label{thm:covering}
  For  and  denote with  the covering number of the set
   of bounded variation paths of length less or equal than  under the  pseudo-metric.
  Then
  
  \end{theorem}
  \begin{proof}
    By definition of the metric 
    
    By definition  and of the norm  on  this reads 
    
   where we denote .
   Let  be the set of lattice paths starting at  that take steps of size  and that are of total length at most . 
By the results in Section 4 of~\cite{lyons2011inversion}, for every  and every  there exists a  such that for every , 

Since  we can apply \eqref{eq:weijun} with
 to get .
Hence, there exists a lattice path  such that

Further, the set  is finite and we can bound it by

 where the first inequality follows since a lattice path has at every step  directions to choose from and in addition can choose not to make a step.  
The last equality follows from the definition of .
Since  was chosen arbitrary it follows that  can be covered by  balls of radius  centered at lattice paths.
\end{proof}
Theorem~\ref{thm:covering} combined with Dudley's celebrated entropy estimates gives regularity results for samples of our GP.
In fact, this even yields a modulus of continuity for our GP.
\begin{theorem}
  There exists a centered GP  that has a covariance  the signature covariance function .
  Moreover, if we denote its modulus of continuity on  with 
  
  then it holds with probability one that 
  
  where  denotes a universal constant.
\end{theorem}
\begin{proof}
The existence of a centered GP  with covariance  follows from general results about Gaussian processes. 
The existence of a continuous modification  of follows from Dudley's theorem if 
 
 but by Theorem~\ref{thm:covering} we have 
 Dudley's results immediately yield a modulus of continuity in probability.
 By standard arguments this can be strengthened to give an almost sure modulus of continuity. 
 Concretely, we use the formulation given in Theorem 2.7.1 in Chapter 5 of \cite{khoshnevisan2002multiparameter} which guarantees that
 
The bound~\eqref{eq:modulus} follows immediately since first term in the denominator equals 
 
\end{proof}

\section{Further algorithms}\label{app:algos}
  \subsection{Notation for computations.} \label{app:alg_notation}
  We define notation based on \cite{KiralyOberhauser2019KSig} for concisely describing vectorized computations. We use -based indexing for arrays to keep in line with the notation of the main text. Let  and  be k-fold arrays of size , indexed by  for . We define the following operations.
  \begin{enumerate}[label=(\roman*)]
  	\item  The cumulative sum along axis  as:
  	
  	\item The slice-wise sum along axis  as:
  	
  	\item The shift along axis  by  for  as:
  	
  	\item The element-wise product of arrays  and  as:
  	 
  \end{enumerate}

  Additionally, note that the use of the cumulative sum, , in conjunction with the shift by  operator, , along the same axis is equivalent to an exclusive cumulative sum, where in the new array the th index contains the sum of the original array's elements from  to .
  
  \subsection{Covariances between sequences and sequences} \label{app:stream_covs}
  
  \begin{algorithm}[h]
  	\caption{Computing covariances at sequences, }
  	\label{alg:streams_cov}
  	\begin{algorithmic}[1]
  		\STATE {\bfseries Input:} Sequences , \\ scalars , depth  
  		\STATE Compute  for ,  
  		\STATE Initialize  for 
  		\STATE Update 
  		\STATE Assign 
  		\FOR{ {\bfseries to} }
  		\STATE Iterate 
  		\STATE Update 
  		\ENDFOR
  		\STATE {\bfseries Output:} Matrix of covariances 
  	\end{algorithmic}
  \end{algorithm}

  We describe in Algorithm \ref{alg:streams_cov} the computation of the covariance matrix  of  for sequences , which is a modification of Algorithm 3 from \cite{KiralyOberhauser2019KSig}. The observant reader will notice that for the vectorization a requirement is that all sequences in  have the same length, . In practice, this is only a computational restriction and can be circumvented by tabulating each sequence to be the same length, e.g. by repeating the last observation as required. The convenience of the parametrization invariance of signatures is that the results remain unchanged.
  
  Simple inspection says that the complexity of Algorithm \ref{alg:streams_cov} is of  in time and  in memory. Although, note that for factorizing likelihoods the computation of the ELBO and making inference about unseen examples  with credible intervals only requires the diagonals of , i.e. . Hence, for convenience, we give vectorized pseudo-code in Algorithm \ref{alg:streams_var} for computing , which has complexities  in time and .
  
  \begin{algorithm}[h]
  	\caption{Computing variances at sequences, }
  	\label{alg:streams_var}
  	\begin{algorithmic}[1]
  		\STATE {\bfseries Input:} Sequences , \\ scalars , depth  
  		\STATE Compute  for ,  
  		\STATE Initialize  for 
  		\STATE Update 
  		\STATE Assign 
  		\FOR{ {\bfseries to} }
  		\STATE Iterate 
  		\STATE Update 
  		\ENDFOR
  		\STATE {\bfseries Output:} Vector of variances 
  	\end{algorithmic}
  \end{algorithm}

\section{Further details on experiments}    

\subsection{Implementation details} \label{app:implementation}
The implementation of all considered GP models are available at \url{GITHUBAUTHOR}. Here, we detail the technicalities related to the implementation of each model.

\paragraph{GP-Sig.} This is the standard GP model with the signature kernel over sequences. This is built on top of GPflow \cite{Matthews2017GPflowAG}, and other than a few tweaks, they interface with GPflow models in a straightforward manner. Particularly for the kernel, there are several variants available with different state space embeddings, including RBF and Matérn static kernels. The hyperparameters of the kernel which are learnt from the data are: \begin{enumerate*}[label=(\arabic*)] \item the lengthscales corresponding to each state space dimension, \item the scaling parameters that multiply each signature level, allowing to strengthen or weaken its effect, \item the lag values by which the additional lagged versions of each coordinate are shifted, that is a continuous parameter and is applied using linear interpolation and flat extrapolation (i.e. when the queried time-point is negative then the value at time  is used) \end{enumerate*}. In Section \ref{sec:experiments}, we denoted the augmented sequence with a time coordinate and  lags by . The lagged coordinates use the same lengthscales as the original ones, which in many cases leads to better generalization compared to not using lags (e.g. Takens' theorem \cite{takens1981detecting}).
The signature kernel is also normalized using the standard kernel normalization , which we apply individually to each signature level. The supported inducing variables are \texttt{InducingTensors} and \texttt{InducingSequences} corresponding to the two variants described in the main text.

\paragraph{GP-Sig-LR.} As previously mentioned, there exists a low-rank variant of the signature kernel as introduced in \cite{KiralyOberhauser2019KSig}, which aims to approximate the feature map using a low-rank approximation, rather than computing inner product of signature features directly. Our implementation first uses the Nyström approximation to find a low-dimensional approximation of the state-space embedding, and then uses the primal formulation of the signature algorithms (see Algorithm 5 in \cite{KiralyOberhauser2019KSig}) to compute the signature kernel, while keeping the size of the low-rank factors manageable with sparse randomized projections \cite{Li2006VerySparse}. Its advantage is that it extends to very long time series due to linear complexity in the time series length , while the quadratic complexity of the full-rank kernel needs to be addressed another way. We did not include this variant among the experiments because overall it performed much worse than the full-rank variant. There were two main issues: \begin{enumerate*}[label=(\roman*)] \item on several datasets it failed to fit the dataset due to being less flexible and noise, \item even when the predictive means are good, it can still give severely miscalibrated uncertainties similarly to classic kernel approximation techniques (Nyström, RFF), since an LR covariance matrix results in a degenerate GP prior. \end{enumerate*}  

\paragraph{GP(-Sig)-LSTM/GRU.} The RNN based models with a GP layer placed on top use the Keras implementation of the RNN architectures \cite{Chollet2015Keras}, while the GP parts use the GPflow API, which is possible as both packages can define the computational graph using the Tensorflow backend. However, since none of the packages supports the other, the resulting models have to be trained somewhat manually using the slightly more primitive Tensorflow API, and therefore are not very user friendly. It is up to future work to build a more user friendly API that makes it possible to deploy models that combine neural networks and sparse variational GPs in a convenient manner.

\paragraph{GP-KConv1D.} The -dimensional convolutional kernel essentially uses the same code as \cite{Wilk2017ConvGP} included in the GPflow package, with some tweaks that allow different length time series to be compared by padding each sequence with nans and masking the nan entries during the computation. We also normalize the features corresponding to this kernel to unit length in the feature space using the standard kernel normalization. In the experiments, we set the window size to , but a few datasets have , and in those cases we set . Also, as the sequence length , and hence, the number of windows can vary from instance to instance, the weighted version of the convolutional kernel from \cite{Wilk2017ConvGP} is not applicable in this case, and the translation invariant version is used.


\subsection{Datasets details} \label{app:datasets}

Table \ref{table:dataset_spec} details the datasets from \cite{baydogan2015multivarate} that we used for benchmarking. Here  denotes the number of classes,  the dimension of the sequence state space,  the range of sequence lengths,  and  respectively denote the number of examples in the pre-specified training and testing sets. In the experiments, all state space dimensions were normalized to zero mean and unit variance. For the models GP-Sig(-LSTM/GRU), GP-KConv1D, we subsampled very long time series to , in order to deal with the quadratic complexity of kernel evaluations and be able to fit within GPU memory limitations.

\begin{table}[t]
	\caption{Specification of datasets used for benchmarking}
	\label{table:dataset_spec}
\begin{center}
    \begin{small}
    \begin{sc}
    \begin{tabular}{lrrrrrr}
    \toprule
    Dataset  &  &  &  &  &  \\
    \midrule
        Arabic Digits & 10 & 13 & \numrange[range-phrase = --]{4}{93} & 6600 & 2200\\
        AUSLAN & 95 & 22 & \numrange[range-phrase = --]{45}{136} & 1140 & 1425\\
        Char.~Traj.& 20 & 3 & \numrange[range-phrase = --]{109}{205} & 300 & 2558\\
        CMUsubject16 & 2 & 62 & \numrange[range-phrase = --]{127}{580} & 29 & 29\\
        DigitShapes & 4 & 2 & \numrange[range-phrase = --]{30}{98} & 24 & 16\\
        ECG & 2 & 2 & \numrange[range-phrase = --]{39}{152} & 100 & 100\\
        Jap.~Vowels & 9 & 12 & \numrange[range-phrase = --]{7}{29} & 270 & 370\\
        Kick vs Punch & 2 & 62 & \numrange[range-phrase = --]{274}{841} & 16 & 10\\
        LIBRAS & 15 & 2 & 45 & 180 & 180\\
        NetFlow & 2 & 4 & \numrange[range-phrase = --]{50}{997} & 803 & 534\\
        PEMS & 7 & 963 & 144 & 267 & 173\\
        PenDigits & 10 & 2 & 8 & 300 & 10692\\
        Shapes & 3 & 2 & \numrange[range-phrase = --]{52}{98} & 18 & 12\\
        UWave & 8 & 3 & 315 & 896 & 3582\\
        Wafer & 2 & 6 & \numrange[range-phrase = --]{104}{198} & 298 & 896\\
        Walk vs Run & 2 & 62 & \numrange[range-phrase = --]{128}{1918} & 28 & 16\\
    \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
    \end{center}
\end{table}

\subsection{Training details} \label{app:training}

\paragraph{Initialization.} For all models considered in the main text in Section \ref{sec:experiments}, the RBF kernel was used as static kernel, which has lengthscale parameters , i.e. the RBF kernel over  is up to rescaling given by

with  a diagonal matrix. We used the initialization , where  are two independent copies of the -th input space coordinate, and we used a stochastic estimator of this with typically  observation samples from the data. 

All considered models in Section \ref{sec:experiments} used some form of inducing variables.
For the signature models, they were placed in the feature space of the signature map in the form of inducing tensors.
These inducing tensors given in \eqref{eq:sparse_tens} are tensor products of elements in .
As detailed at the end of Section \ref{sec:sparse var tensor}, although the state space of a sequence is , we can embed this sequence into a path that evolves in a linear space  that does not have to be .
One way to do this is to use an observation-wise state space embedding given by a kernel  and map a sequence  to a sequence  that evolves in the RKHS  of ; here .
Therefore signatures of depth  now live in the space , which is for most kernels  a genuine infinite-dimensional space.
However, all computations from Sections \ref{sec:our GP} and \ref{sec:sparse var tensor} carry on mutatis mutandis, with the difference being that we do not have the flexibility to represent the inducing tensors as tensor products of arbitrary elements in , which are generally infinite dimensional.
In this case, we take


with  and  with  for , , , where .
Put differently, the inducing tensors are also constrained to being tensor products of only such elements in  which arise as reproducing kernels\footnote{The reproducing kernel associated to a point  is simply the kernel function evaluated in one of its arguments at , i.e.~ for a kernel .}  associated to vectors in .
Hence, the complexity of evaluating  is the same as in , and storing an element  is the same memory.
Now, the initialization of the inducing tensors is simply done by sampling random observations from the input sequences in a two step manner: \begin{enumerate*}[label=(\arabic*)] \item a random input sequence is selected, \item from the sequence a time-increasing subset of its observations are selected and plugged into the tensor products given in \eqref{eq:sparse_tens_h0} \end{enumerate*}, and this procedure is repeated  times.

Other forms of inducing variables used by the models in Section \ref{sec:experiments} are inducing points for the GP-RNNs and inducing patches for GP-KConv1D. The inducing points are initialized randomly by selecting a  and computing its RNN-image , which is then used as an inducing point, and repeated for all . The inducing patches are also initialized in two steps: \begin{enumerate*}[label=(\arabic*)] \item select a random input sequence , \item select a random window from , , where  denotes the window length in the convolutional kernel. \end{enumerate*}

For the alternative sparse inference scheme for signatures described in Section \ref{sec:experiments}, denoted the method of inducing sequences, we use the same initialization as for the inducing patches: select a random sequence, and select a random window, and repeat for all .

The means and covariances of the inducing points used the usual whitening transformation, that is, reparametrization in terms of the Cholesky factor  of , , and parameters initialized from zeros and identity.

The RNNs use the usual initializations, that is, Glorot initialization for the weights \cite{Glorot10Understanding}, orthogonal initialization for the recurrent weights \cite{Saxe2014Exact}, and zeros for the bias.

\paragraph{Optimization details.} The training for the benchmarking experiment in Section \ref{sec:sparse var tensor} was performed on 11 GPUs overall: 4 Tesla K40Ms, 5 Geforce 2080 TIs and 2 Quadro GP100 graphics cards. All models were trained 5 times for the benchmarking and the RNN based models an additional 6 times for the grid-search. Thus, the training of overall 480 models required extensive computational resources.

In all experiments in Section \ref{sec:experiments}, we used similar optimization details, that is, optimization with early stopping and checkpointing by optimizing on  of the training data and monitoring the nlpp\footnote{We found that monitoring the validation nlpp rather than the validation accuracy leads to better generalization behaviour.} on a  validation set. We used a minibatch size of , fixed learning rate , and a patience value of  epochs.  As optimizer, GP-Sig and GP-KConv1D used Nadam \cite{Dozat2015IncorporatingNM}, while the RNN based models used Adam \cite{kingma2014adam}. Additionally, as is well-known for SVGPs \cite{bauer2016understanding}, first fixing the hyperparameters and only optimizing over the variational approximation for a fixed number of epochs is beneficial which we follow. Furthermore, after the main training phase of the hyperparameters has finished, to learn the rest of the validation data that was excluded from the optimization, we re-merge the validation set into the training set, fix the hyperparameters, and optimize only over the variational parameters again to assimilate the remaining information into the variational approximation.

Hence, the training for all models is split into the following phases \begin{enumerate*}[label=(\arabic*)] \item partition the data in an  ratio for optimization and monitoring, \item with fixed kernel hyperparameters initialized as described previously, train the variational parameters for fixed  epochs to tighten the ELBO bound; \item \label{enum:main_training_phase}  unfix the hyperparameters and train by monitoring the nlpp on the validation set, stopping after no improvement for  epochs, and restoring and best model; \item re-merge the validation set into the training data and train the variational distribution again only for a fixed  epochs with the kernel hyperparameters fixed \end{enumerate*}. In all scenarios, we used .

For GP-Sig, the insertion of an additional optimization phase was found to be beneficial. Particularly, we reparametrize the scaling parameters for the signature levels  as , where . Then, phase \ref{enum:main_training_phase} is split into two steps: first, train with unfixing all kernel hyperparameters except , which are a-priori all set as ; secondly, now unfixing \emph{all} parameters, continue training with early stopping. This trick allows to calibrate the overall variance of the GP using  in the first step, while fixing . The intution why this works is that the signature levels in general contain complementary information about a given sequence, and fixing them to be equal first enforces the model to find a fit of the data for all signature levels jointly, i.e. in some sense this is an implicit regularization step. The second step allows to slightly adjust the contribution of each level without relying too heavily on any one of them. On the RNN-based signature models this trick did not give substantial improvements, possibly because the variance of the RNN layer generally outweights the variance of the signature layer.

In our experience, when using GP-Sig on datasets with a larger , it can yield a further improvement to gradually increase the learning to rate to  to allow the optimizer to explore the space in more depth, and then decrease it back to  to drive it to the closest local optima. However, on the smaller datasets this was found to be counterproductive, and in the experiments we chose to stick with a unified scheme that worked consistently on all datasets. However, we also remark that without applying any of the previously described techniques, and training from front to back all parameters jointly with a small learning rate (e.g. ) gives good results already, but a few percents of test set accuracy can be gained on some datasets by using them.


\begin{table*}[tb]
	\caption{List of architectures used for the RNN based models}
	\label{table:architecture_spec}
\begin{center}
		\begin{small}
			\begin{sc}
\begin{tabular}{l  >{\ \ }l >{\ \ }l >{\ \ }l >{\ \ }l >{\ \ }l >{\ \ }l >{\ \ }l >{\ \ }l >{\ \ }l >{\ \ }l}
\toprule
					\multicolumn{1}{l}{Dataset} & \multicolumn{2}{c}{GP-Sig-LSTM} & \multicolumn{2}{c}{GP-Sig-GRU} &
					\multicolumn{2}{c}{GP-LSTM} & \multicolumn{2}{c}{GP-GRU}\\
\midrule 
                    Arabic Digits &  &  &  &  &  &  &  &  \\ 
                    AUSLAN &  &  &  &  &  &  &  &  \\ 
                    Character Traj. &  &  &  &  &  &  &  &  \\ 
                    CMUsubject16 &  &  &  &  &  &  &  &  \\ 
                    DigitShapes &  &  &  &  &  &  &  &  \\ 
                    ECG &  &  &  &  &  &  &  &  \\ 
                    Jap.~Vowels &  &  &  &  &  &  &  &  \\ 
                    Kick vs Punch &  &  &  &  &  &  &  &  \\ 
                    LIBRAS &  &  &  &  &  &  &  &  \\ 
                    NetFlow &  &  &  &  &  &  &  &  \\ 
                    PEMS &  &  &  &  &  &  &  &  \\ 
                    PenDigits &  &  &  &  &  &  &  &  \\ 
                    Shapes &  &  &  &  &  &  &  &  \\ 
                    UWave &  &  &  &  &  &  &  &  \\ 
                    Wafer &  &  &  &  &  &  &  &  \\ 
                    Walk vs Run &  &  &  &  &  &  &  &  \\
					\bottomrule
				\end{tabular}
			\end{sc}
		\end{small}
	\end{center}
\end{table*}

\paragraph{Architecture search.}
Table \ref{table:architecture_spec} details each of the architectures used for the models containing an RNN layer, where  denotes the number of hidden units used, and  is a boolean trigger, that specifies whether dropout was used for the given experiment or not. In the case , we used the settings  and , otherwise both were set to . To find the best performing architecture, we conducted a grid-search among  considered architectures, that is,  and . For the grid-search, only the training data was used, and the data was split in a  fashion, using  for training,  for early stopping and checkpointing, and the last  was used to evaluate the performance. The training itself was carried out using the same initialization and schedule as described, and was performed only once for each method and setting pair, due to the large number of datasets that we considered.



\subsection{Benchmark results} \label{app:benchmark}

We report in Table \ref{table:full_nlpp_results} and Table \ref{table:full_acc_results} the negative log-predictive probabilities and accuracies of the GP models considered in Section \ref{sec:experiments}. For each method-dataset pair, 5 models were trained with the initialization described in Appendix \ref{app:training}. The variance of the results is therefore due to random initialization of some parameters, and the minibatch randomness while training. The RNN based models used the architectures detailed in Table \ref{table:architecture_spec}. As non-Bayesian baselines, we report the results of recent frequentist TS classification methods from the respective publications, that is, \cite{Cuturi2011AR, baydogan2015learning, Baydogan2015TimeSR, karlsson2016generalized, tuncel2018autoregressive, Schfer2017MUSE, Karim2019LSTMFCN}. Particularly for MLSTMFCN, we report the same results as in \cite{Schfer2017MUSE}. In Figure \ref{fig:boxplots}, we visualize the box-plot distributions of \begin{enumerate*}[label=(\arabic*)] \item negative log-predictive probabilities of the GPs, \item classification accuracies of both the GPs and the frequentist baselines \end{enumerate*}.

\begin{table*}[ht]
	\caption{Mean and standard deviation of negative predictive log-probabilities (nlpp) on test sets over  independent runs}
	\label{table:full_nlpp_results}
	\vskip 0.15in
	\begin{center}
		\begin{small}
			\begin{sc}
				\begin{tabular}{lcccccc}\toprule
					Dataset & GP-Sig-LSTM & GP-Sig-GRU & GP-Sig & GP-LSTM  & GP-GRU & GP-KConv1D\\
					\midrule
                        Arabic Digits &  &  &  &  &  &  \\ 
                        AUSLAN &  &  &  &  &  &  \\ 
                        Character Traj. &  &  &  &  &  &  \\ 
                        CMUsubject16 &  &  &  &  &  &  \\ 
                        DigitShapes &  &  &  &  &  &  \\ 
                        ECG &  &  &  &  &  &  \\ 
                        Jap.~Vowels &  &  &  &  &  &  \\ 
                        Kick vs Punch &  &  &  &  &  &  \\ 
                        LIBRAS &  &  &  &  &  &  \\ 
                        NetFlow &  &  &  &  &  &  \\ 
                        PEMS &  &  &  &  &  &  \\ 
                        PenDigits &  &  &  &  &  &  \\ 
                        Shapes &  &  &  &  &  &  \\ 
                        UWave &  &  &  &  &  &  \\ 
                        Wafer &  &  &  &  &  &  \\ 
                        Walk vs Run &  &  &  &  &  &  \\
                        \midrule
                        Mean nlpp. &  &  &  &  &  &  \\
                        Med.~nlpp. &  &  &  &  &  &  \\
                        Sd.~nlpp. &  &  &  &  &  &  \\
                        \midrule
                        Mean rank () &  &  &  &  &  &  \\ 
                        Mean rank () &  &  &  &  &  &  \\ 
                        Mean rank (all) &  &  &  &  &  &  \\
                    \bottomrule
				\end{tabular}
			\end{sc}
		\end{small}
	\end{center}
\end{table*}

\begin{table*}[h]
	\caption{Mean and standard deviation of accuracies on test sets over  independent runs}
	\label{table:full_acc_results}
	\vskip 0.15in
	\begin{center}
		\begin{small}
			\begin{sc}
				\begin{tabular}{lcccccc}\toprule
					Dataset & GP-Sig-LSTM & GP-Sig-GRU & GP-Sig & GP-LSTM  & GP-GRU & GP-KConv1D\\
					\midrule 
                        Arabic Digits &  &  &  &  &  &  \\ 
                        AUSLAN &  &  &  &  &  &  \\ 
                        Character Traj. &  &  &  &  &  &  \\ 
                        CMUsubject16 &  &  &  &  &  &  \\ 
                        DigitShapes &  &  &  &  &  &  \\ 
                        ECG &  &  &  &  &  &  \\ 
                        Jap.~Vowels &  &  &  &  &  &  \\ 
                        Kick vs Punch &  &  &  &  &  &  \\ 
                        LIBRAS &  &  &  &  &  &  \\ 
                        NetFlow &  &  &  &  &  &  \\ 
                        PEMS &  &  &  &  &  &  \\ 
                        PenDigits &  &  &  &  &  &  \\ 
                        Shapes &  &  &  &  &  &  \\ 
                        UWave &  &  &  &  &  &  \\ 
                        Wafer &  &  &  &  &  &  \\ 
                        Walk vs Run &  &  &  &  &  &  \\
                        \midrule
                        Mean acc. &  &  &  &  &  &  \\
                        Med.~acc. &  &  &  &  &  &  \\
                        Sd.~acc. &  &  &  &  &  &  \\
                        \midrule
                        Mean rank () &  &  &  &  &  &  \\ 
                        Mean rank () &  &  &  &  &  &  \\ 
                        Mean rank (all) &  &  &  &  &  &  \\
					\bottomrule
				\end{tabular}
			\end{sc}
		\end{small}
	\end{center}
\end{table*}

\begin{table*}[h]
	\caption{Accuracies of frequentist time series classification methods}
	\label{table:freq_acc_results}
	\vskip 0.15in
	\begin{center}
		\begin{small}
			\begin{sc}
				\begin{tabular}{lrrrrrrrr}\toprule
					Dataset & SMTS & LPS & mvARF & DTW & ARKernel & gRSF & MLSTMFCN & MUSE \\
					\midrule
					Arabic Digits &  &  &  &  &  &  &  &  \\
					AUSLAN &  &  &  &  &  &  &  &  \\
					Character Traj. &  &  &  &  &  &  &  &  \\
					CMUsubject16 &  &  &  &  &  &  &  &  \\
					DigitShapes &  &  &  &  &  &  &  &  \\
					ECG &  &  &  &  &  &  &  &  \\
					Jap.~Vowels &  &  &  &  &  &  &  &  \\
					Kick vs Punch &  &  &  &  &  &  &  &  \\
					LIBRAS &  &  &  &  &  &  &  &  \\
					NetFlow &  &  & NA &  & NA &  &  &  \\
					PEMS &  &  & NA &  &  &  & NA & NA \\
					PenDigits &  &  &  &  &  &  &  &  \\
					Shapes &  &  &  &  &  &  &  &    \\
					UWave &  &  &  &  &  &  &  &  \\
					Wafer &  &  &  &  &  &  &  &  \\
					Walk vs Run &  &  &  &  &  &  &  &  \\
					\midrule
					Mean acc. &  &  &  &  &  &  &  &  \\
                    Med.~acc. &  &  &  &  &  &  &  &  \\
                    Sd.~acc. &  &  &  &  &  &  &  &  \\
					\bottomrule
				\end{tabular}
			\end{sc}
		\end{small}
	\end{center}
\end{table*}

\clearpage

\begin{figure*}[t]
    \centering
    \begin{minipage}{0.29\textwidth}
        \centering
        \includegraphics[height=1.85in]{images/nlpp_boxplot.pdf}
    \end{minipage}
    \begin{minipage}{0.69\textwidth}
        \centering
        \includegraphics[height=1.85in]{images/acc_boxplot.pdf}
    \end{minipage}
    \caption{Box-plots of negative log-predictive probabilities (left) and classification accuracies (right) on 16 TSC datasets}
    \label{fig:boxplots}
\end{figure*}

\clearpage



\end{document}
