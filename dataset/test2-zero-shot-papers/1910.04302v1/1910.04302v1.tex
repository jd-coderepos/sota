

\RequirePackage[l2tabu, orthodox]{nag}
\documentclass[11pt]{article}

\usepackage[T1]{fontenc}

\renewcommand*\ttdefault{lmvtt}





\usepackage[bitstream-charter]{mathdesign}
\usepackage{amsmath}
\usepackage[scaled=0.92]{PTSans}




\usepackage[
  paper  = letterpaper,
  left   = 1.65in,
  right  = 1.65in,
  top    = 1.0in,
  bottom = 1.0in,
  ]{geometry}

\usepackage[usenames,dvipsnames]{xcolor}
\definecolor{shadecolor}{gray}{0.9}

\usepackage[final,expansion=alltext]{microtype}
\usepackage[english]{babel}
\usepackage[parfill]{parskip}
\usepackage{afterpage}
\usepackage{framed}

\renewenvironment{leftbar}[1][\hsize]
{  \def\FrameCommand
  {    {\color{Gray}\vrule width 3pt}    \hspace{10pt}      }  \MakeFramed{\hsize#1\advance\hsize-\width\FrameRestore}}{\endMakeFramed}

\DeclareRobustCommand{\parhead}[1]{\textbf{#1}~}

\usepackage{lineno}
\renewcommand\linenumberfont{\normalfont
                             \footnotesize
                             \sffamily
                             \color{SkyBlue}}
\usepackage{ragged2e}
\DeclareRobustCommand{\sidenote}[1]{\marginpar{
                                    \RaggedRight
                                    \textcolor{Plum}{\textsf{#1}}}}
\newcommand{\parnum}{\bfseries\P\arabic{parcount}}
\newcounter{parcount}
\newcommand\p{    \stepcounter{parcount}    \leavevmode\marginpar[\hfill\parnum]{\parnum}}
\DeclareRobustCommand{\PP}{\textcolor{Plum}{\P} }

\renewcommand{\labelenumi}{\color{black!67}{\arabic{enumi}.}}
\renewcommand{\labelenumii}{{\color{black!67}(\alph{enumii})}}
\renewcommand{\labelitemi}{{\color{black!67}\textbullet}}

\usepackage{graphicx}
\usepackage[labelfont=bf]{caption}

\usepackage{booktabs}
\usepackage{multirow}

\usepackage{natbib}

\usepackage[algoruled]{algorithm2e}
\usepackage{listings}
\usepackage{fancyvrb}
\fvset{fontsize=\normalsize}



\usepackage[colorlinks,linktoc=all]{hyperref}
\usepackage[all]{hypcap}
\hypersetup{citecolor=MidnightBlue}
\hypersetup{linkcolor=black}
\hypersetup{urlcolor=MidnightBlue}

\usepackage[acronym,smallcaps,nowarn]{glossaries}

\newcommand{\red}[1]{\textcolor{BrickRed}{#1}}
\newcommand{\orange}[1]{\textcolor{BurntOrange}{#1}}
\newcommand{\green}[1]{\textcolor{OliveGreen}{#1}}
\newcommand{\blue}[1]{\textcolor{MidnightBlue}{#1}}
\newcommand{\gray}[1]{\textcolor{black!60}{#1}}

\lstdefinestyle{mystyle}{
    commentstyle=\color{OliveGreen},
    keywordstyle=\color{BurntOrange},
    numberstyle=\tiny\color{black!60},
    stringstyle=\color{MidnightBlue},
    basicstyle=\ttfamily,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}
\lstset{style=mystyle}

\usepackage[colorinlistoftodos,
           prependcaption,
           textsize=small,
           backgroundcolor=yellow,
           linecolor=lightgray,
           bordercolor=lightgray]{todonotes}
 \DeclareRobustCommand{\PP}{\textcolor{Plum}{\P} }

\DeclareRobustCommand{\mb}[1]{\ensuremath{\boldsymbol{\mathbf{#1}}}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareRobustCommand{\KL}[2]{\ensuremath{D_{\textrm{KL}}\left(#1\;\|\;#2\right)}}

\DeclareRobustCommand{\E}[2]{\mathbb{E}_{#1}\left[#2\right]}
\DeclareRobustCommand{\ent}[1]{\mathbb{H}\left[#1\right]}
\DeclareRobustCommand{\Var}[1]{\mathbb{V}\left[#1\right]}
\DeclareRobustCommand{\Ov}[2]{\textrm{OvExpFam}\left(#2\mid #1\right)}
\DeclareRobustCommand{\diag}[1]{\textrm{diag}\left(#1\right)}
\DeclareRobustCommand{\logit}[1]{\textrm{logit}\left(#1\right)}
\DeclareRobustCommand{\sigmoid}[1]{\textrm{sigmoid}\left(#1\right)}

\newcommand{\g}{\, | \,}
\newcommand{\prm}{\, ; \,}

\newcommand{\Bcal}{\mathcal{B}}
\newcommand{\Ccal}{\mathcal{C}}
\newcommand{\Ecal}{\mathcal{E}}
\newcommand{\Fcal}{\mathcal{F}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Ocal}{\mathcal{O}}
\newcommand{\Scal}{\mathcal{S}}
\newcommand{\Tcal}{\mathcal{T}}

\newcommand{\ccal}{\mathcal{c}}
\newcommand{\ycal}{\mathcal{y}}

\newcommand{\Naturals}{\mathbb{N}}
\newcommand{\Integers}{\mathbb{Z}}
\newcommand{\Rationals}{\mathbb{Q}}
\newcommand{\Reals}{\mathbb{R}}

\newcommand{\bzero}{\mathbf{0}}
\newcommand{\bone}{\mathbf{1}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bG}{\mathbf{G}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bD}{\mathbf{D}}
\newcommand{\beps}{\mathbf{\epsilon}}
\newcommand{\blambda}{\mathbold{\lambda}}
\newcommand{\bmu}{\mathbf{\mu}}
\newcommand{\btheta}{\mathbold{\theta}}
\newcommand{\bbeta}{\mathbold{\beta}}
\newcommand{\bdelta}{\mathbold{\delta}}
\newcommand{\bepsilon}{\mathbf{\epsilon}}
\newcommand{\bgamma}{\mathbold{\gamma}}
\newcommand{\brho}{\mathbold{\rho}}
\newcommand{\bomega}{\mathbold{\omega}}
\newcommand{\bsigma}{\bm{\sigma}}
\newcommand{\balpha}{\mathbold{\alpha}}
\newcommand{\bSigma}{\bm{\Sigma}}
\newcommand{\cL}{\mathcal{L}}

\newcommand{\hfun}{h}
\newcommand{\ufun}{u}

 \newacronym{ALI}{ali}{adversarially learned inference}
\newacronym{BIGAN}{bigan}{bidirectional generative adversarial network}
\newacronym{VI}{vi}{variational inference}
\newacronym{KL}{kl}{Kullback-Leibler}
\newacronym{ELBO}{elbo}{evidence lower bound}
\newacronym{MCMC}{mcmc}{Markov chain Monte Carlo}
\newacronym{HMC}{hmc}{Hamiltonian Monte Carlo}
\newacronym{RNN}{rnn}{recurrent neural network}
\newacronym{MLP}{mlp}{feed forward neural network}
\newacronym{VAE}{vae}{variational auto-encoder}
\newacronym{GAN}{gan}{generative adversarial network}
\newacronym{DCGAN}{dcgan}{deep convolutional generative adversarial network}
\newacronym{PresGAN}{presgan}{prescribed generative adversarial network}
\newacronym{DGM}{dgm}{deep generative model}
\newacronym{PGAN}{pgan}{prescribed generative adversarial network}
\newacronym{VEEGAN}{veegan}{veegan}
\newacronym{PACGAN}{pacgan}{packed {GAN}}
\newacronym{STYLEGAN}{stylegan}{Style {GAN}}
\newacronym{FID}{fid}{{F}r\'{e}chet {I}nception distance} 
\usepackage{subfigure}
\usepackage{authblk}
\usepackage{enumitem} 
\usepackage{bm} 


\title{\textbf{Prescribed Generative Adversarial Networks}}

\author[1]{Adji B. Dieng}
\author[2, 3]{Francisco J. R. Ruiz}
\author[1, 2]{\\David M. Blei}
\author[4]{Michalis K. Titsias}
\affil[1]{Department of Statistics, Columbia University}
\affil[2]{Department of Computer Science, Columbia University}
\affil[3]{Department of Engineering, University of Cambridge}
\affil[4]{DeepMind}


\begin{document}
\maketitle


\begin{abstract}
\noindent \Glspl{GAN} are a powerful approach to unsupervised learning. They have achieved state-of-the-art performance in the image domain. However, \glspl{GAN} are limited in two ways. They often learn distributions with low support---a phenomenon known as mode collapse---and they do not guarantee the existence of a probability density, which makes evaluating generalization using predictive log-likelihood impossible. In this paper, we develop the prescribed \gls{GAN} (Pres\gls{GAN}) to address these shortcomings. Pres\glspl{GAN} add noise to the output of a density network and optimize an entropy-regularized adversarial loss. The added noise renders tractable approximations of the predictive log-likelihood and stabilizes the training procedure. The entropy regularizer encourages Pres\glspl{GAN} to capture all the modes of the data distribution. Fitting Pres\glspl{GAN} involves computing the intractable gradients of the entropy regularization term; Pres\glspl{GAN} sidestep this intractability using unbiased stochastic estimates. We evaluate Pres\glspl{GAN} on several datasets and found they mitigate mode collapse and generate samples with high perceptual quality. We further found that Pres\glspl{GAN} reduce the gap in performance in terms of predictive log-likelihood between traditional \glspl{GAN} and \glspl{VAE}.\footnote{\textbf{Code:} The code for this paper can be found at \url{https://github.com/adjidieng/PresGANs}.}\\

\noindent \textbf{Keywords:} generative adversarial networks, entropy regularization, log-likelihood evaluation, mode collapse, diverse image generation, deep generative models
\end{abstract}


\section{Introduction}
\label{sec:introduction}
\glsresetall

\begin{figure*}[t]
	\centerline{\includegraphics[width=1.2\textwidth, height=4.2cm]{figures/synthetic}}
	\caption{Density estimation with \acrshort{GAN} and Pres\acrshort{GAN} on a toy two-dimensional experiment. The ground truth is a uniform mixture of  Gaussians organized on a ring. Given the right set of hyperparameters, a \acrshort{GAN} could perfectly fit this target distribution. In this example we chose the \acrshort{GAN} hyperparameters such that it collapses---here  out of  modes are missing. We then fit the Pres\acrshort{GAN} using the same hyperparameters as the collapsing \acrshort{GAN}. The Pres\acrshort{GAN} is able to correct the collapsing behavior of the \acrshort{GAN} and learns a good fit for the target distribution.}
	\label{fig:toy_example_fig1}
\end{figure*}

\Glspl{GAN}~\citep{goodfellow2014generative} are a family of generative models that have shown great promise.
They achieve state-of-the-art performance in the image domain; for example image generation \citep{karras2019style, brock2018large}, image super-resolution \citep{ledig2017photo}, and image translation \citep{isola2017image}. 

\glspl{GAN} learn densities by defining a sampling procedure. A latent variable  is sampled from a prior  and a sample  is generated by taking the output of a neural network with parameters , called a generator, that takes  as input. 
The density  implied by this sampling procedure is implicit and undefined \citep{mohamed2016learning}. However, \glspl{GAN} effectively learn the parameters  by introducing a classifier ---a deep neural network with parameters , called discriminator---that distinguishes between generated samples  and real data , with distribution . The parameters  and  are learned jointly by optimizing the \gls{GAN} objective,

\glspl{GAN} iteratively maximize the loss in Eq.\nobreakspace \ref {eq:gan_loss} with respect to  and minimize it with respect to . 

In practice, the minimax procedure described above is stopped when the generator produces realistic images. This is problematic because 
high perceptual quality does not necessarily correlate with goodness of fit to the target density. For example, memorizing the training data is a trivial solution to achieving high perceptual quality. Fortunately, \glspl{GAN} do not merely memorize the training data~\citep{zhang2017discrimination, arora2017generalization}. 

However \glspl{GAN} are able to produce images indistinguishable from real images while still failing to fully capture the target distribution~\citep{brock2018large, karras2019style}. Indeed \glspl{GAN} suffer from an issue known as \emph{mode collapse}. When mode collapse happens, the generative distribution  is degenerate and of low support~\citep{arora2017generalization, arora2018gans}. Mode collapse causes \glspl{GAN}, as density estimators, to fail both qualitatively and quantitatively. Qualitatively, mode collapse causes lack of diversity in the generated samples. This is problematic for certain applications of \glspl{GAN}, e.g. data augmentation. Quantitatively, mode collapse causes poor generalization to new data. This is because when mode collapse happens, there is a (support) mismatch between the learned distribution  and the data distribution. Using annealed importance sampling with a kernel density estimate of the likelihood, \citet{wu2016quantitative} report significantly worse log-likelihood scores for \glspl{GAN} when compared to \glspl{VAE}. Similarly poor generalization performance was reported by \citet{grover2018flow}. 

A natural way to prevent mode collapse in \glspl{GAN} is to maximize the entropy of the generator~\citep{belghazi2018mine}. Unfortunately the entropy of \glspl{GAN} is unavailable.
This is because the existence of the generative density  is not guaranteed \citep{mohamed2016learning, arjovsky2017wasserstein}.

In this paper, we propose a method to alleviate mode collapse in \glspl{GAN} resulting in a new family of \glspl{GAN} called prescribed \glspl{GAN} (Pres\glspl{GAN}). Pres\glspl{GAN} prevent mode collapse by explicitly maximizing the entropy of the generator. This is done by augmenting the loss in Eq.\nobreakspace \ref {eq:gan_loss} with the negative entropy of the generator, such that minimizing Eq.\nobreakspace \ref {eq:gan_loss} with respect to  corresponds to fitting the data while also maximizing the entropy of the generative distribution. The existence of the generative density is guaranteed by adding noise to the output of a density network \citep{mackay1995bayesian, diggle1984monte}. This process defines the generative distribution , not as an implicit distribution as in standard \glspl{GAN}, but as an infinite mixture of well-defined densities as in continuous \glspl{VAE}~\citep{kingma2013auto, rezende2014stochastic}. The generative distribution of Pres\glspl{GAN} is therefore very flexible. 

Although the entropy of the generative distribution of Pres\glspl{GAN} is well-defined, it is intractable. However, fitting a Pres\gls{GAN} to data only involves computing the gradients of the entropy and not the entropy itself. Pres\glspl{GAN} use unbiased Monte Carlo estimates of these gradients. 

\parhead{An illustrative example.}
To demonstrate how Pres\glspl{GAN} alleviate mode collapse, we form a target distribution by organizing a uniform mixture of  two-dimensional Gaussians on a ring. 
We draw  samples from this target distribution. We first fit a \gls{GAN}, setting the hyperparameters so that the \gls{GAN} suffers from mode collapse\footnote{A \gls{GAN} can perfectly fit this distribution when choosing the right hyperparameters.}. We then use the same settings for Pres\gls{GAN} to assess whether it can correct the collapsing behavior of the \gls{GAN}. Figure\nobreakspace \ref {fig:toy_example_fig1} 
shows the collapsing behavior of the \gls{GAN}, which misses  modes of the target distribution. The Pres\gls{GAN}, on the other hand, recovers all the modes. 
Section\nobreakspace \ref {sec:empirical} provides details about the settings of this synthetic experiment.

\parhead{Contributions.} This paper contributes to the literature on the two main open problems in the study of \glspl{GAN}: preventing mode collapse and evaluating log-likelihood.
\begin{itemize}[leftmargin=*]
	\item How can we perform entropy regularization of the generator of a \gls{GAN} so as to effectively prevent mode collapse? We achieve this by adding noise to the output of the generator; this ensures the existence of a density  and makes its entropy well-defined. We then regularize the \gls{GAN} loss to encourage densities  with high entropy. During training, we form unbiased estimators of the (intractable) gradients of the entropy regularizer. We show how this prevents mode collapse, as expected, in two sets of experiments (see Section\nobreakspace \ref {sec:empirical}). The first experiment follows the current standard for measuring mode collapse in the \gls{GAN} literature, which is to report the number of modes recovered by the \gls{GAN} on  ( modes) and  ( modes) and the \gls{KL} divergence between the true label distribution and the one induced by the \gls{GAN}. We conducted a second experiment which sheds light on another way mode collapse can occur in \glspl{GAN}, which is when the data is imbalanced.
	\item How can we measure log-likelihood in \glspl{GAN}? Evaluating log-likelihood for \glspl{GAN} allows assessing how they generalize to new data. Existing measures focus on sample quality, which is not a measure of generalization. This inability to measure predictive log-likelihood for \glspl{GAN} has restricted their use to domains where one can use perceptual quality measures (e.g., the image domain). 
	Existing methods for evaluating log-likelihood for \glspl{GAN} either use a proxy to log-likelihood~\citep{sanchez2019out} or define the likelihood of the generator only at test time, which creates a mismatch between training and testing~\citep{wu2016quantitative}, or assume invertibility of the generator of the \gls{GAN}~\citep{grover2018flow}. Adding noise to the output of the generator immediately makes tractable predictive log-likelihood evaluation via importance sampling. 
\end{itemize} 

\parhead{Outline.} The rest of the paper is organized as follows. In Section\nobreakspace \ref {sec:background} we set the notation and provide desiderata for deep generative modeling. In Section\nobreakspace \ref {sec:method} we describe Pres\glspl{GAN} and how we compute their predictive log-likelihood to assess generalization. In Section\nobreakspace \ref {sec:related} we discuss related work. We then assess the performance of Pres\glspl{GAN} in terms of mode collapse, sample quality, and log-likelihood in Section\nobreakspace \ref {sec:empirical}. Finally, we conclude and discuss key findings in Section\nobreakspace \ref {sec:discussion}.
 
\section{Prologue}
\label{sec:background}

In this paper, we characterize a \gls{DGM} by its generative process and by the loss used to fit its parameters. We denote by  the generative distribution induced by the generative process---it is parameterized by a deep neural network with parameters . The loss, that we denote by , often requires an additional set of parameters  that help learn the model parameters . We next describe choices for  and  and then specify desiderata for deep generative modeling.

\parhead{The generative distribution.} 
Recent \glspl{DGM} define the generative distribution either as an implicit distribution or as an infinite mixture~\citep{goodfellow2014generative, kingma2013auto, rezende2014stochastic}. 

Implicit generative models define a density using a sampling procedure. This is the approach of \glspl{GAN}~\citep{goodfellow2014generative}. A latent variable  is sampled from a prior , usually a standard Gaussian or a uniform distributon, and a sample is generated by taking the output of a neural network that takes  as input. The density  implied by this sampling procedure is undefined. Any measure that relies on an analytic form of the density  is therefore unavailable; e.g., the log-likelihood or the entropy.

An alternative way to define the generative distribution is by using the approach of \glspl{VAE}~\citep{kingma2013auto, rezende2014stochastic}. They define  as an infinite mixture,

Here the mixing distribution is the prior . The conditional distribution  is an exponential family distribution, such as a Gaussian or a Bernoulli, parameterized by a neural network with parameters . Although both the prior  and  are simple tractable distributions, the generative distribution  is highly flexible albeit intractable. Because  in Eq.\nobreakspace \ref {eq:vaes} is well-defined, the log-likelihood and the entropy are also well-defined (although they may be analytically intractable). 

\parhead{The loss function.} 
Fitting the models defined above requires defining a learning procedure by specifying a loss function. \glspl{GAN} introduce a classifier , a deep neural network parameterized by , to discriminate between samples from the data distribution  and the generative distribution . The auxiliary parameters  are learned jointly with the model parameters  by optimizing the loss in Eq.\nobreakspace \ref {eq:gan_loss}. This training procedure leads to high sample quality but often suffers from \emph{mode collapse} \citep{arora2017generalization, arora2018gans}. 

An alternative approach to learning  is via maximum likelihood. This requires a well-defined density  such as the one in Eq.\nobreakspace \ref {eq:vaes}. Although well-defined,  is intractable, making it difficult to learn the parameters  by maximum likelihood. \glspl{VAE} instead introduce a recognition network---a neural network with parameters  that takes data  as input and outputs a distribution over the latent variables ---and maximize a lower bound on  with respect to both  and ,

Here  denotes the \gls{KL} divergence. Maximizing  is equivalent to minimizing this \gls{KL} which leads to issues such as latent variable collapse~\citep{bowman2015generating, dieng2018avoiding}. Furthermore, optimizing Eq.\nobreakspace \ref {eq:elbo} may lead to blurriness in the generated samples because of a property of the reverse  known as \emph{zero-forcing} \citep{minka2005divergence}.

\parhead{Desiderata.} 
We now outline three desiderata for \glspl{DGM}.

\emph{High sample quality}. A \gls{DGM} whose parameters  have been fitted using real data should generate new data with the same qualitative precision as the data it was trained with. For example, if a \gls{DGM} is trained on a dataset composed of human faces, it should generate data with all features that make up a face at the same resolution as the training data.

\emph{High sample diversity}. High sample quality alone is not enough. For example, a degenerate \gls{DGM} that is only able to produce one single sample is not desirable, even if the sample quality is perfect.
Therefore we require sample diversity; a \gls{DGM} should ideally capture all modes of the data distribution.

\emph{Tractable predictive log-likelihood}. \glspl{DGM} are density estimators and as such we should evaluate how they generalize to new data. High sample quality and diversity are not measures of generalization. We therefore require tractable predictive log-likelihood as a desideratum for deep generative modeling.

We next introduce a new family of \glspl{GAN} that fulfills all the desiderata. 
 
\section{Prescribed Generative Adversarial Networks}
\label{sec:method}

Pres\glspl{GAN} generate data following the generative distribution in Eq.\nobreakspace \ref {eq:vaes}. 
Note that this generative process is the same as for standard \glspl{VAE} \citep{kingma2013auto,rezende2014stochastic}.
In particular, Pres\glspl{GAN} set the prior  and the likelihood  to be Gaussians, 

The mean  and covariance  of the conditional  are given by a neural network that takes  as input.

In general, both the mean  and the covariance  can be functions of . For simplicity, in order to speed up the learning procedure, we set
the covariance matrix to be diagonal with elements independent from , i.e., , and we learn the vector  together with . From now on, we parameterize the mean with , write , and define  as the parameters of the generative distribution.

To fit the model parameters , Pres\glspl{GAN} optimize an adversarial loss similarly to \glspl{GAN}.
In doing so, they keep \glspl{GAN}' ability to generate samples with high perceptual quality. Unlike \glspl{GAN}, the entropy of the generative distribution of Pres\glspl{GAN} is well-defined, and therefore Pres\glspl{GAN} can prevent mode collapse by adding an entropy regularizer to Eq.\nobreakspace \ref {eq:gan_loss}. Furthermore, because Pres\glspl{GAN} define a density over their generated samples, we can measure how they generalize to new data using predictive log-likelihood. 
We describe the entropy regularization in Section\nobreakspace \ref {subsec:entropy_reg} and how to approximate the predictive log-likelihood in Section\nobreakspace \ref {subsec:loglik_evaluation}.

\subsection{Avoiding mode collapse via entropy regularization}
\label{subsec:entropy_reg}

One of the major issues that \glspl{GAN} face is mode collapse, where the generator tends to model only some parts or modes of the data distribution \citep{arora2017generalization, arora2018gans}. 
Pres\glspl{GAN} mitigate this problem by explicitly maximizing the entropy of the generative distribution,

Here  denotes the entropy of the generative distribution. It is defined as

The loss  in Eq.\nobreakspace \ref {eq:sigan_loss} can be that of any of the existing \gls{GAN} variants. In Section\nobreakspace \ref {sec:empirical} we explore the standard \gls{DCGAN}~\citep{radford2015unsupervised} and the more recent Style\gls{GAN}~\citep{karras2019style} architectures. 

The constant  in Eq.\nobreakspace \ref {eq:sigan_loss} is a hyperparameter that controls the strength of the entropy regularization. 
In the extreme case when , the loss function of Pres\gls{GAN} coincides with the loss of a \gls{GAN}, where we replaced its implicit generative distribution with the infinite mixture in Eq.\nobreakspace \ref {eq:vaes}. In the other extreme when , optimizing  corresponds to fitting a maximum entropy generator that ignores the data. For any intermediate values of , the first term of  encourages the generator to fit the data distribution, whereas the second term encourages to cover all of the modes of the data distribution.

The entropy  is intractable because the integral in Eq.\nobreakspace \ref {eq:def_entropy} cannot be computed. However, fitting the parameters  of Pres\glspl{GAN} only requires the gradients of the entropy. In Section\nobreakspace \ref {sec:fitting} we describe how to form unbiased Monte Carlo estimates of these gradients.

\subsection{Fitting Prescribed Generative Adversarial Networks}\label{sec:fitting}
We fit Pres\glspl{GAN} following the same adversarial procedure used in \glspl{GAN}. That is, we alternate between updating the parameters of the generative distribution  and the parameters of the discriminator . The full procedure is given in Algorithm\nobreakspace \ref {alg:full_algo}.
We now describe each part in detail. 

\begin{algorithm}[t]
  \SetAlgoNoLine
    \DontPrintSemicolon
    \SetKwInOut{KwInput}{input}
    \SetKwInOut{KwOutput}{output}
    \KwInput{Data , entropy regularization level }
    Initialize parameters \;
    \For{\emph{iteration} }{
      Draw minibatch of observations \;
       \For{}{
      	Get noised data:  and \;
      	Draw latent variable \;
      	Generate data:  and \;
	}
      Compute  (Eq.\nobreakspace \ref {eq:grad_phi}) and take a gradient step for \;
      Initialize an \acrshort{HMC} sampler using \;
      Draw  for  and  using that sampler\;
      Compute  (Eq.\nobreakspace \ref {eq:grad_eta}) and 
      take a gradient step for \;
      Compute  (Eq.\nobreakspace \ref {eq:grad_sigma}) and 
      take a gradient step for \;
      Truncate  in the range \;
    }
    \caption{Learning with Prescribed Generative Adversarial Networks (Pres\glspl{GAN}) \label{alg:full_algo}}
 \end{algorithm}


\parhead{Fitting the generator.}
We fit the generator using stochastic gradient descent. This requires computing the gradients of the Pres\gls{GAN} loss with respect to ,

We form stochastic estimates of  based on reparameterization \citep{kingma2013auto,rezende2014stochastic,titsias2014doubly}; this requires differentiating Eq.\nobreakspace \ref {eq:gan_loss}. Specifically, we introduce a noise variable  to reparameterize the conditional from Eq.\nobreakspace \ref {eq:semi_implicit},\footnote{With this reparameterization we use the notation  instead of  to denote a sample from the generative distribution.}

where  and . Here  and  denote the mean and standard deviation of the conditional , respectively. 
We now write the first term of Eq.\nobreakspace \ref {eq:grad_theta} as an expectation with respect to the latent variable  and the noise variable  and push the gradient into the expectation,

In practice we use an estimate of Eq.\nobreakspace \ref {eq:grad_gan} using one sample from  and one sample from ,

The second term in Eq.\nobreakspace \ref {eq:grad_theta}, corresponding to the gradient of the entropy, is intractable.  
We estimate it using the same approach as \citet{titsias2018unbiased}. 
We first use the reparameterization in Eq.\nobreakspace \ref {eq:sigan_sample} to express the gradient of the entropy as an expectation,

where we have used the score function identity  on the second line.
We form a one-sample estimator of the gradient of the entropy as\looseness=-1

In Eq.\nobreakspace \ref {eq:estimate_grad_entropy}, the gradient with respect to the reparameterization transformation  is tractable and can be obtained via back-propagation.
We now derive ,

While this expression is still intractable, we can estimate it. One way is to use self-normalized importance sampling with a proposal learned using moment matching with an encoder~\citep{dieng2019reweighted}. However, this would lead to a biased (albeit asymptotically unbiased) estimate of the entropy. In this paper, we form an unbiased estimate of  using samples  from the posterior,

We obtain these samples using \gls{HMC} \citep{neal2011mcmc}.
Crucially, in order to speed up the algorithm, we initialize the \gls{HMC} sampler at stationarity. That is, we initialize the \gls{HMC} sampler with the sample  that was used to produce the generated sample  in Eq.\nobreakspace \ref {eq:sigan_sample}, which by construction is an exact sample from . 
This implies that only a few \gls{HMC} iterations suffice to get good estimates of the gradient \citep{titsias2018unbiased}. We also found this holds empirically; for example in Section\nobreakspace \ref {sec:empirical} we use  burn-in iterations and  \gls{HMC} samples to form the Monte Carlo estimate in Eq.\nobreakspace \ref {eq:estimate_logdensity}.

Finally, using 
Eqs.\nobreakspace \ref {eq:grad_theta} and\nobreakspace   \ref {eq:estimate_grad_gan} to\nobreakspace  \ref {eq:estimate_logdensity} 
we can approximate the gradient of the entropy-regularized adversarial loss with respect to the model parameters , 

In particular, the gradient with respect to the generator's parameters  is unbiasedly approximated by

and the gradient estimator with respect to the standard deviation  is 

These gradients are used in a stochastic optimization algorithm to fit the generative distribution of Pres\gls{GAN}.

\parhead{Fitting the discriminator.}
Since the entropy term in Eq.\nobreakspace \ref {eq:sigan_loss} does not depend on , optimizing the discriminator of a Pres\gls{GAN} is analogous to optimizing the discriminator of a \gls{GAN},


To prevent the discriminator from getting stuck in a bad local optimum where it can perfectly distinguish between real and generated data by relying on the added noise, we apply the same amount of noise to the real data  as the noise added to the generated data. That is, when we train the discriminator we corrupt the real data according to

where  is the standard deviation of the generative distribution and  denotes the real data. 
We then let the discriminator distinguish between  and  from Eq.\nobreakspace \ref {eq:sigan_sample}.

This data noising procedure is a form of \emph{instance noise} \citep{sonderby2016amortised}. However, instead of using a fixed annealing schedule for the noise variance as \citet{sonderby2016amortised}, we let  be part of the parameters of the generative distribution and fit it using gradient descent according to Eq.\nobreakspace \ref {eq:grad_sigma}.

\parhead{Stability.} Data noising stabilizes the training procedure and prevents the discriminator from perfectly being able to distinguish between real and generated samples using the background noise. We refer the reader to \citet{ferenc2016instance} for a detailed exposition. 

When fitting Pres\glspl{GAN}, data noising is not enough to stabilize training. This is because there are two failure cases brought in by learning the variance  using gradient descent. The first failure mode is when the variance gets very large, leading to a generator completely able to fool the discriminator. Because of data noising, the discriminator cannot distinguish between real and generated samples when the variance of the noise is large. 

The second failure mode is when  gets very small, which makes the gradient of the entropy in Eq.\nobreakspace \ref {eq:grad_eta} dominate the overall gradient of the generator. This is problematic because the learning signal from the discriminator is lost. 

To stabilize training and avoid the two failure cases discussed above we truncate the variance of the generative distribution,
 (we apply this truncation element-wise). The limits  and  are hyperparameters.

\subsection{Enabling tractable predictive log-likelihood approximation}
\label{subsec:loglik_evaluation}

Replacing the implicit generative distribution of \glspl{GAN} with the infinite mixture distribution defined in Eq.\nobreakspace \ref {eq:vaes} has the advantage that the predictive log-likelihood can be tractably approximated. 
Consider an unseen datapoint . We estimate its log marginal likelihood  using importance sampling,

where we draw  samples  from a proposal distribution .

There are different ways to form a good proposal , and we discuss several alternatives in Section\nobreakspace \ref {app:proposals} of the appendix. In this paper, we take the following approach. We define the proposal as a Gaussian distribution,

We set the mean parameter  to the \emph{maximum a posteriori} solution, i.e., . We initialize this maximization algorithm using the mean of a pre-fitted encoder, . The encoder is fitted by minimizing the reverse \gls{KL} divergence between  and the true posterior  using the training data. This \gls{KL} is 

Because the generative distribution is fixed at test time, minimizing the \gls{KL} here is equivalent to maximizing the second term in Eq.\nobreakspace \ref {eq:kl}, which is the \gls{ELBO} objective of \glspl{VAE}.

We set the proposal covariance  as an overdispersed version\footnote{In general, overdispersed proposals lead to better importance sampling estimates.} of the encoder's covariance matrix, which is diagonal. In particular, to obtain  we multiply the elements of the encoder's covariance by a factor . In Section\nobreakspace \ref {sec:empirical} we set  to .
 
\section{Related Work}
\label{sec:related}

\glspl{GAN} \citep{goodfellow2014generative} have been extended in multiple ways, using alternative distance metrics and optimization methods \citep[see, e.g.,][]{li2015generative,dziugaite2015training,nowozin2016f,arjovsky2017wasserstein,ravuri2018learning,genevay2017learning} or using ideas from \glspl{VAE}~\citep{makhzani2015adversarial, mescheder2017adversarial, dumoulin2016adversarially, donahue2016adversarial, tolstikhin2017wasserstein, ulyanov2018takes, rosca2017variational}.

Other extensions aim at improving the sample diversity of \glspl{GAN}. For example, \citet{srivastava2017veegan} 
use a reconstructor network that reverses the action of the generator. \citet{lin2018pacgan} use multiple observations (either real or generated) as an input to the discriminator to prevent mode collapse. \citet{azadi2018discriminator} and \citet{turner2018metropolis} use sampling mechanisms to correct errors of the generative distribution. \citet{xiao2018bourgan} relies on identifying the geometric structure of the data embodied under a specific distance metric.
Other works have combined adversarial learning with maximum likelihood \citep{grover2018flow, yin2019semi}; however, the low sample quality induced by maximum likelihood still occurs. Finally, \citet{cao2018improving} introduce a regularizer for the discriminator to encourage diverse activation patterns in the discriminator across different samples. 
In contrast to these works, Pres\glspl{GAN} regularize the entropy of the generator to prevent mode collapse. 

The idea of entropy regularization has been widely applied in many problems that involve estimation of unknown probability distributions. Examples include approximate Bayesian inference, where the variational objective contains an entropy penalty \citep{jordan1998learning,bishop2006pattern,wainwright2008graphical,blei2017variational}; reinforcement learning, where the entropy regularization allows to estimate more uncertain and explorative policies \citep{schulman2015trust, mnih2016asynchronous}; statistical learning, where entropy regularization allows an inferred probability distribution to  avoid collapsing to a deterministic solution \citep{freund1997decision, soofi2000principal, jaynes2003probability}; or optimal transport \citep{rigollet2018entropic}. More recently, \citet{kumar2019maximum} have developed maximum-entropy generators for energy-based models using mutual information as a proxy for entropy. 

Another body of related work is about how to quantitatively evaluate \glspl{GAN}. Inception scores measure the sample quality of \glspl{GAN} and are used extensively in the \gls{GAN} literature~\citep{salimans2016improved, heusel2017gans, binkowski2018demystifying}. However, sample quality measures only assess the quality of \glspl{GAN} as data generators and not as density estimators. Density estimators are evaluated for generalization to new data. Predictive log-likelihood is a measure of goodness of fit that has been used to assess generalization; for example in \glspl{VAE}. Finding ways to evaluate predictive log-likelihood for \glspl{GAN} has been an open problem, because \glspl{GAN} do not define a density on the generated samples. 
\citet{wu2016quantitative} use a kernel density estimate~\citep{parzen1962estimation} and estimate the log-likelihood with annealed importance sampling~\citep{neal2001annealed}.
\citet{balaji2018entropic} show that an optimal transport \gls{GAN} with entropy regularization can be viewed as a generative model that maximizes a variational lower bound on average sample likelihoods, which relates to the approach of \glspl{VAE} \citep{kingma2013auto}. 
\citet{sanchez2019out} propose Eval\acrshort{GAN}, a method to estimate the likelihood. Given an observation , Eval\acrshort{GAN} first finds the closest observation  that the \gls{GAN} is able to generate, and then it estimates the likelihood  by approximating the proportion of samples  that lead to samples  that are close to . Eval\gls{GAN} requires selecting an appropriate distance metric for each problem and evaluates \glspl{GAN} trained with the usual implicit generative distribution. Finally, \citet{grover2018flow} assume invertibility of the generator to make log-likelihood tractable. 
 
\section{Empirical Study}\label{sec:empirical}

Here we demonstrate Pres\glspl{GAN}' ability to prevent mode collapse and generate high-quality samples. We also evaluate its predictive performance as measured by log-likelihood.

\subsection{An Illustrative Example}
\label{subsec:illustrative}

In this section, we fit a \gls{GAN} to a toy synthetic dataset of  modes. We choose the hyperparameters such that the \gls{GAN} collapses. We then apply these same hyperparameters to fit a Pres\gls{GAN} on the same synthetic dataset. This experiment demonstrates the Pres\gls{GAN}'s ability to correct the mode collapse problem of a \gls{GAN}.

We form the target distribution by organizing a uniform mixture of  two-dimensional Gaussians on a ring. The radius of the ring is  and each Gaussian has standard deviation . We then slice the circle 
into  parts. The location of the centers of the mixture components are determined as follows. Consider the  mixture component. Its coordinates in the D space are

We draw  samples from the target distribution and fit a \gls{GAN} and a Pres\gls{GAN}.

We set the dimension of the latent variables  used as the input to the generators to . We let both the generators and the discriminators have three fully connected layers with tanh activations and  hidden units in each layer. We set the minibatch size to  and use Adam for optimization \citep{kingma2014adam}, with a learning rate of  and  for the discriminator and the generator respectively. The Adam hyperparameters are  and . We take one step to optimize the generator for each step of the discriminator. We pick a random minibatch at each iteration and run both the \gls{GAN} and the Pres\gls{GAN} for  epochs. 

For Pres\gls{GAN} we set the burn-in and the number of \gls{HMC} samples to . We choose a standard number of  leapfrog steps and set the \gls{HMC} learning rate to . The acceptance rate is fixed at . The log-variance of the noise of the generative distribution of Pres\gls{GAN} is initialized at . We put a threshold on the variance to a minimum value of  and a maximum value of . The regularization parameter  is . We fit the log-variance using Adam with a learning rate of . 

Figure\nobreakspace \ref {fig:toy_example_fig1} demonstrates how the Pres\gls{GAN} alleviates mode collapse. The distribution learned by the regular \gls{GAN} misses  modes of the target distribution. The Pres\gls{GAN} is able to recover all the modes of the target distribution. 

\subsection{Assessing mode collapse}
\label{subsec:assessing_mode_collapse}

In this section we evaluate Pres\glspl{GAN}' ability to mitigate mode collapse on real datasets. We run two sets of experiments. In the first set of experiments we adopt the current experimental protocol for assessing mode collapse in the \gls{GAN} literature. That is, we use the \textsc{mnist} and \textsc{stackedmnist} datasets, for which we know the true number of modes, and report two metrics: the number of modes recovered by the Pres\gls{GAN} and the \gls{KL} divergence between the label distribution induced by the Pres\gls{GAN} and the true label distribution. In the second set of experiments we demonstrate that mode collapse can happen in \glspl{GAN} even when the number of modes is as low as  but the data is imbalanced. 

\parhead{Increased number of modes.} We consider the \textsc{mnist} and \textsc{stackedmnist} datasets. \textsc{mnist} is a dataset of hand-written digits,\footnote{See \url{http://yann.lecun.com/exdb/mnist}.} in which each  image corresponds to a digit. There are  training digits and  digits in the test set. \textsc{mnist} has  modes, one for each digit. \textsc{stackedmnist} is formed by concatenating triplets of randomly chosen \textsc{mnist} digits along the color channel to form images of size  \citep{Metz2017}. We keep the same size as the original \textsc{mnist},  training digits for  test digits. The total number of modes in \textsc{stackedmnist} is , corresponding to the number of possible triplets.

We consider \gls{DCGAN} as the base architecture and, following \citet{radford2015unsupervised}, we resize the spatial resolution of images to  pixels.

\begin{table*}[t]
	\centering
	\small
	\captionof{table}{Assessing mode collapse on \textsc{mnist}. The true total number of modes is . All methods capture all the  modes. The \acrshort{KL} captures a notion of discrepancy between the labels of real versus generated images. Pres\acrshort{GAN} generates images whose distribution of labels is closer to the data distribution, as evidenced by lower \acrshort{KL} scores.}
	\begin{tabular}{ccc}
	\toprule
	 Method & Modes & KL \\
	 \hline
	 \acrshort{DCGAN} \citep{radford2015unsupervised} &   &  \\
	 \acrshort{VEEGAN} \citep{srivastava2017veegan} &  &   \\
	  \acrshort{PACGAN} \citep{lin2018pacgan} &  &  \\
	  Pres\acrshort{GAN} (this paper) &  &  \\
	\bottomrule
	\end{tabular}
	\label{tab:collapse_dimensionality_mnist}
\end{table*}

\begin{table*}[t]
	\centering
	\small
	\captionof{table}{Assessing mode collapse on \textsc{stackedmnist}. The true total number of modes is . All methods suffer from collapse except Pres\gls{GAN}, which captures nearly all the modes of the data distribution. Furthermore, Pres\acrshort{GAN} generates images whose distribution of labels is closer to the data distribution, as evidenced by lower \acrshort{KL} scores.}
	\begin{tabular}{ccc}
	\toprule
	 Method & Modes & KL \\
	 \hline
	 \acrshort{DCGAN} \citep{radford2015unsupervised} &   &  \\
	 \acrshort{VEEGAN} \citep{srivastava2017veegan} &  &  \\
	  \acrshort{PACGAN} \citep{lin2018pacgan} &   &  \\
	  Pres\acrshort{GAN} (this paper) &  &  \\
	\bottomrule
	\end{tabular}
	\label{tab:collapse_dimensionality_smnist}
\end{table*}

To measure the degree of mode collapse we form two diversity metrics, following \citet{srivastava2017veegan}. Both of these metrics require to fit a classifier to the training data. Once the classifier has been fit, we sample  images from the generator. The first diversity metric is the \emph{number of modes captured}, measured by the number of classes that are captured by the classifier. We say that a class  has been captured if there is at least one generated sample for which the probability of being assigned to class  is the largest. The second diversity metric is the \emph{\gls{KL} divergence} between two discrete distributions: the empirical average of the (soft) output of the classifier on generated images, and the empirical average of the (soft) output of the classifier on real images from the test set. We choose the number of generated images  to match the number of test samples on each dataset. That is,  for both \textsc{mnist} and \textsc{stackedmnist}. We expect the \gls{KL} divergence to be zero if the distribution of the generated samples is indistinguishable from that of the test samples. 

We measure the two mode collapse metrics described above against \gls{DCGAN} \citep{radford2015unsupervised} (the base architecture of Pres\gls{GAN} for this experiment). 
We also compare against other methods that aim at alleviating mode collapse in \glspl{GAN}, namely, \acrshort{VEEGAN} \citep{srivastava2017veegan} and \acrshort{PACGAN} \citep{lin2018pacgan}. 
For Pres\gls{GAN} we set the entropy regularization parameter  to . 
We chose the variance thresholds to be  and .

\begin{table*}[t]
	\centering
	\small
	\captionof{table}{Assessing the impact of the entropy regularization parameter  on mode collapse on \textsc{mnist} and \textsc{stackedmnist}. When  (i.e., no entropy regularization is applied to the generator), then mode collapse occurs as expected. When entropy regularization is applied but the value of  is very small () then mode collapse can still occur as the level of regularization is not enough. When the value of  is appropriate for the data then mode collapse does not occur. Finally, when  is too high then mode collapse can occur because the entropy maximization term dominates and the data is poorly fit.}
	\begin{tabular}{ccccc}
	\toprule
	   &  \multicolumn{2}{c}{\textsc{mnist}} &  \multicolumn{2}{c}{\textsc{stackedmnist}} \\
	 \midrule
	  & Modes & KL & Modes & KL \\
	 \hline
	 &  &  &  &  \\
	 &  &  & &  \\
	 &  &  & &  \\
	 &  &  &  &  \\
	 &  &  &  & \\
	 &  &  & &  \\
	\bottomrule
	\end{tabular}
	\label{tab:collapse_dim_bis}
\end{table*}


\begin{figure*}[t]
	\centering
	\vspace*{-10pt}
	\centerline{\includegraphics[width=1.2\textwidth]{figures/imbalance.pdf}}
	\caption{Assessing mode collapse under increased data imbalance on \textsc{mnist}. The figures show the number of modes captured (higher is better) and the \gls{KL} divergence (lower is better) under increasingly imbalanced settings. The maximum number of modes in each case is . All methods suffer from mode collapse as the level of imbalance increases except for the Pres\gls{GAN} which is robust to data imbalance.}
	\label{fig:imbalance}
	\vspace*{-8pt}
\end{figure*}

Tables\nobreakspace \ref {tab:collapse_dimensionality_mnist} and\nobreakspace  \ref {tab:collapse_dimensionality_smnist} show the number of captured modes and the \gls{KL} for each method. The results are averaged across  runs. All methods capture all the modes of \textsc{mnist}. This is not the case on \textsc{stackedmnist}, where the Pres\gls{GAN} is the only method that can capture all the modes. 
Finally, the proportion of observations in each mode of Pres\gls{GAN} is closer to the true proportion in the data, as evidenced by lower \acrshort{KL} divergence scores.

We also study the impact of the entropy regularization by varying the hyperparameter  from  to . Table\nobreakspace \ref {tab:collapse_dim_bis} illustrates the results. Unsurprisingly, when there is no entropy regularization, i.e., when , then mode collapse occurs. This is also the case when the level of regularization is not enough (). There is a whole range of values for  such that mode collapse does not occur (). Finally, when  is too high for the data and architecture under study, mode collapse can still occur. This is because when  is too high, the entropy regularization term dominates the loss in Eq.\nobreakspace \ref {eq:sigan_loss} and in turn the generator does not fit the data as well. This is also evidenced by the higher \gls{KL} divergence score when  vs.\ when .

\parhead{Increased data imbalance.} We now show that mode collapse can occur in \glspl{GAN} when the data is imbalanced, even when the number of modes of the data distribution is small. We follow \citet{dieng2018learning} and consider a perfectly balanced version of \textsc{mnist} as well as nine imbalanced versions. To construct the balanced dataset we used  training examples per class, totaling  training examples. We refer to this original balanced dataset as . Each additional training set  leaves only  training examples for each class , and  for the rest. (See the Appendix for all the class distributions.)

We used the same classifier trained on the unmodified \textsc{mnist} but fit each method on each of the  new \textsc{mnist} distributions. We chose  for Pres\gls{GAN}. Figure\nobreakspace \ref {fig:imbalance} illustrates the results in terms of both metrics---number of modes and \gls{KL} divergence. \gls{DCGAN}, \acrshort{VEEGAN}, and \acrshort{PACGAN} face mode collapse as the level of imbalance increases. This is not the case for Pres\gls{GAN}, which is robust to imbalance and captures all the  modes. 

\subsection{Assessing sample quality}
\label{subsec:assessing_sample_quality}

In this section we assess Pres\glspl{GAN}' ability to generate samples of high perceptual quality. We rely on perceptual quality of generated samples and on \gls{FID} scores \citep{heusel2017gans}. We also consider two different \gls{GAN} architectures, the standard \gls{DCGAN} and the more recent Style\gls{GAN}, to show robustness of Pres\glspl{GAN} vis-a-vis the underlying \gls{GAN} architecture.

\parhead{\gls{DCGAN}.} We use \gls{DCGAN} \citep{radford2015unsupervised} as the base architecture and build Pres\gls{GAN} on top of it. We consider four datasets: \textsc{mnist}, \textsc{stackedmnist}, \textsc{cifar}-10, and CelebA. \textsc{cifar}-10 \citep{krizhevsky2009learning} is a well-studied dataset of  images that are classified into one of the following categories: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. CelebA \citep{liu2015deep} is a large-scale face attributes dataset. Following \citet{radford2015unsupervised}, we resize all images to  pixels.
We use the default \gls{DCGAN} settings. We refer the reader to the code we used for \gls{DCGAN}, which was taken from \url{https://github.com/pytorch/examples/tree/master/dcgan}. We set the seed to  for reproducibility.


\begin{table*}[t]
	\centering
	\small
	\captionof{table}{\acrfull{FID} (lower is better). Pres\acrshort{GAN} has lower \acrshort{FID} scores than \acrshort{DCGAN}, \acrshort{VEEGAN}, and \acrshort{PACGAN}. This is because Pres\acrshort{GAN} mitigates mode collapse while preserving sample quality.}
	\begin{tabular}{ccc}
	\toprule
	 Method & Dataset &  \acrshort{FID} \\
	 \hline
	 \acrshort{DCGAN} \citep{radford2015unsupervised} & \textsc{mnist} &  \\
	 \acrshort{VEEGAN} \citep{srivastava2017veegan} & \textsc{mnist} &  \\
	  \acrshort{PACGAN}   \citep{lin2018pacgan}  & \textsc{mnist} &  \\
	  Pres\acrshort{GAN} (this paper) & \textsc{mnist} &  \\
	  \hline
	  \acrshort{DCGAN}  & \textsc{stackedmnist} &  \\
	 \acrshort{VEEGAN} & \textsc{stackedmnist} &   \\
	  \acrshort{PACGAN} & \textsc{stackedmnist} &  \\
	  Pres\acrshort{GAN} & \textsc{stackedmnist} &  \\
	  \hline
	  \acrshort{DCGAN}  & \textsc{cifar}-10 &   \\
	 \acrshort{VEEGAN} & \textsc{cifar}-10 &   \\
	  \acrshort{PACGAN} & \textsc{cifar}-10 &   \\
	  Pres\acrshort{GAN} & \textsc{cifar}-10 &  \\
	  \hline
	  \acrshort{DCGAN}  & \textsc{celeba} &  \\
	 \acrshort{VEEGAN} & \textsc{celeba} &   \\
	  \acrshort{PACGAN} & \textsc{celeba} &  \\
	  Pres\acrshort{GAN} & \textsc{celeba} &  \\
	\bottomrule
	\end{tabular}
	\label{tab:fid}
\end{table*}

There are hyperparameters specific to Pres\gls{GAN}. These are the noise and \gls{HMC} hyperparameters. We set the learning rate for the noise parameters  to  and constrain its values to be between  and  for all datasets. We initialize  to . We set the burn-in and the number of \gls{HMC} samples to . We choose a standard number of  leapfrog steps and set the \gls{HMC} learning rate to . The acceptance rate is fixed at . We found that different  values worked better for different datasets. We used  for \textsc{cifar}-10 and \textsc{celeba}  for \textsc{mnist} and \textsc{stackedmnist}.

We found the Pres\gls{GAN}'s performance to be robust to the default settings for most of these hyperparameters. However we found the initialization for  and its learning rate to play a role in the quality of the generated samples. The hyperparameters mentioned above for  worked well for all datasets. 

Table\nobreakspace \ref {tab:fid} shows the \gls{FID} scores for \gls{DCGAN} and Pres\gls{GAN} across the four datasets.  
We can conclude that Pres\gls{GAN} generates images of high visual quality. In addition, the \gls{FID} scores are lower because Pres\gls{GAN} explores more modes than \gls{DCGAN}. Indeed, when the generated images account for more modes, the \gls{FID} sufficient statistics (the mean and covariance of the Inception-v3 pool3 layer) of the generated data get closer to the sufficient statistics of the empirical data distribution.

We also report the \gls{FID} for \acrshort{VEEGAN} and  \acrshort{PACGAN} in Table\nobreakspace \ref {tab:fid}.
\acrshort{VEEGAN} achieves better \gls{FID} scores than \acrshort{DCGAN} on all datasets but \textsc{celeba}. This is because \acrshort{VEEGAN} collapses less than \acrshort{DCGAN} as evidenced by Table\nobreakspace \ref {tab:collapse_dimensionality_mnist} and Table\nobreakspace \ref {tab:collapse_dimensionality_smnist}.  \acrshort{PACGAN} achieves better \gls{FID} scores than both \acrshort{DCGAN} and \acrshort{VEEGAN} on all datasets but on \textsc{stackedmnist} where it achieves a significantly worse \gls{FID} score. Finally, Pres\gls{GAN} outperforms all of these methods on the \gls{FID} metric on all datasets signaling its ability to mitigate mode collapse while preserving sample quality.

Besides the \gls{FID} scores, we also assess the visual quality of the generated images. In Section\nobreakspace \ref {app:samples} of the appendix, we show randomly generated (not cherry-picked) images from \gls{DCGAN},  \acrshort{VEEGAN}, \acrshort{PACGAN}, and Pres\gls{GAN}.  
For Pres\gls{GAN}, we show the mean of the conditional distribution of  given . The samples generated by Pres\gls{GAN} have high visual quality; in fact their quality is comparable to or better than the \gls{DCGAN} samples.

\parhead{Style\gls{GAN}.} We now consider a more recent \gls{GAN} architecture (Style\gls{GAN})~\citep{karras2019style} and a higher resolution image dataset (\textsc{ffhq}). 
\textsc{ffhq} is a diverse dataset of faces from Flickr\footnote{See \url{https://github.com/NVlabs/ffhq-dataset}.} introduced by \citet{karras2019style}. The dataset contains  high-quality \textsc{png} images with considerable variation in terms of age, ethnicity, and image background. We use a resolution of  pixels. 

\begin{figure}[t]
	\includegraphics[width=0.45\textwidth]{figures/stylegan_ffhq_samples.png}\quad\quad\quad
	\includegraphics[width=0.45\textwidth]{figures/sigan_ffhq_samples.png}
	\caption{Generated images on \textsc{ffhq} for Style\gls{GAN} (left) and Pres\gls{GAN} (right). The Pres\gls{GAN} maintains the high perceptual quality of the Style\gls{GAN}.}
	\label{fig:ffhq}
\end{figure}

Style\gls{GAN} feeds multiple sources of noise  to the generator. In particular, it adds Gaussian noise after each convolutional layer before evaluating the nonlinearity. Building Pres\gls{GAN} on top of Style\gls{GAN} therefore requires to sample all noise variables  through \gls{HMC} at each training step. To speed up the training procedure, we only sample the noise variables corresponding to the input latent code and condition on all the other Gaussian noise variables. In addition, we do not follow the progressive growing of the networks of \citet{karras2019style} for simplicity.

For this experiment, we choose the same \gls{HMC} hyperparameters as for the previous experiments but restrict the variance of the generative distribution to be . We set  for this experiment. 

Figure\nobreakspace \ref {fig:ffhq} shows cherry-picked images generated from Style\gls{GAN} and Pres\gls{GAN}. We can observe that the Pres\gls{GAN} maintains as good perceptual quality as the base architecture. In addition, we also observed that the Style\gls{GAN} tends to produce some redundant images (these are not shown in Figure\nobreakspace \ref {fig:ffhq}), something that we did not observe with the Pres\gls{GAN}. This lack of diversity was also reflected in the \gls{FID} scores which were  for Style\gls{GAN} and  for Pres\gls{GAN}. These results suggest that entropy regularization effectively reduces mode collapse while preserving sample quality. 


\subsection{Assessing held-out predictive log-likelihood}
\label{subsec:assessing_log_lik}

In this section we evaluate Pres\glspl{GAN} for generalization using predictive log-likelihood. We use the \gls{DCGAN} architecture to build Pres\gls{GAN} and evaluate the log-likelihood on two benchmark datasets, \textsc{mnist} and \textsc{cifar}-10. We use images of size .

We compare the generalization performance of the Pres\gls{GAN} against the \gls{VAE} \citep{kingma2013auto, rezende2014stochastic} by controlling for the architecture and the evaluation procedure. In particular, we fit a \gls{VAE} that has the same decoder architecture as the Pres\gls{GAN}. We form the \gls{VAE} encoder by using the same architecture as the \gls{DCGAN} discriminator and getting rid of the output layer. We used linear maps to get the mean and the log-variance of the approximate posterior. 

To measure how Pres\glspl{GAN} compare to traditional \glspl{GAN} in terms of log-likelihood, we also fit a Pres\gls{GAN} with . 

\parhead{Evaluation.}
We control for the evaluation procedure and follow what's described in Section\nobreakspace \ref {subsec:loglik_evaluation} for all methods. We use  samples to form the importance sampling estimator. Since the pixel values are normalized in , we use a truncated Gaussian likelihood for evaluation. Specifically, for each pixel of the test image, we divide the Gaussian likelihood by the probability (under the generative model) that the pixel is within the interval . We use the truncated Gaussian likelihood at test time only.

\parhead{Settings.}
For the Pres\gls{GAN}, we use the same \gls{HMC} hyperparameters as for the previous experiments. We constrain the variance of the generative distribution using  and . We use the default \gls{DCGAN} values for the remaining hyperparameters, including the optimization settings. For the \textsc{cifar}-10 experiment, we choose . We set all learning rates to . We set the dimension of the latent variables to . We ran both the \gls{VAE} and the Pres\gls{GAN} for a maximum of  epochs. For \textsc{mnist}, we use the same settings as for \textsc{cifar}-10 but use  and ran all methods for a maximum of  epochs. 


\begin{table}[t]
	\centering
	\small
	\captionof{table}{Generalization performance as measured by negative log-likelihood (lower is better) on \textsc{mnist} and \textsc{cifar}-10. Here the \gls{GAN} denotes a Pres\gls{GAN} fitted without entropy regularization (). The Pres\gls{GAN} reduces the gap in performance between the \gls{GAN} and the \gls{VAE} on both datasets.}
	\begin{tabular}{ccccc}
		\toprule
		& \multicolumn{2}{c}{\textsc{mnist}} & \multicolumn{2}{c}{\textsc{cifar}-10} \\
		& Train & Test & Train & Test \\ \midrule
		\gls{VAE}     &  &  &  &    \\
		\gls{GAN} &  &  &  &   \\
		Pres\gls{GAN} &  &  &  &   \\
		\bottomrule
	\end{tabular}
	\label{tab:loglik}
\end{table}

\parhead{Results.}
Table\nobreakspace \ref {tab:loglik} summarizes the results. Here \gls{GAN} denotes the Pres\gls{GAN} fitted using . The \gls{VAE} outperforms both the \gls{GAN} and the Pres\gls{GAN} on both \textsc{mnist} and \textsc{cifar}-10. This is unsurprising given \glspl{VAE} are fitted to maximize log-likelihood. The \gls{GAN}'s performance on \textsc{cifar}-10 is particularly bad, suggesting it suffered from mode collapse. The Pres\gls{GAN}, which mitigates mode collapse achieves significantly better performance than the \gls{GAN} on \textsc{cifar}-10. To further analyze the generalization performance, we also report the log-likelihood on the training set in Table\nobreakspace \ref {tab:loglik}. We can observe that the difference between the training log-likelihood and the test log-likelihood is very small for all methods.
 
\section{Epilogue}
\label{sec:discussion}

We introduced the Pres\gls{GAN}, a variant of \glspl{GAN} that addresses two of their limitations. Pres\glspl{GAN} prevent mode collapse and are amenable to predictive log-likelihood evaluation. Pres\glspl{GAN} model data by adding noise to the output of a density network and optimize an entropy-regularized adversarial loss. The added noise stabilizes training, renders approximation of predictive log-likelihoods tractable, and enables unbiased estimators for the gradients of the entropy of the generative distribution. We evaluated Pres\glspl{GAN} on several image datasets. We found they effectively prevent mode collapse and generate samples of high perceptual quality. We further found that Pres\glspl{GAN} reduce the gap in performance between \glspl{GAN} and \glspl{VAE} in terms of predictive log-likelihood.

We found the level of entropy regularization  plays an important role in mode collapse. We leave as future work the task of finding the optimal . 
We now discuss some insights that we concluded from our empirical study in Section\nobreakspace \ref {sec:empirical}.

\parhead{Implicit distributions and sample quality.} It's been traditionally observed that \glspl{GAN} generate samples with higher perceptual quality than \glspl{VAE}. This can be explained by looking at the two ways in which \glspl{GAN} and \glspl{VAE} differ; the generative distribution and the objective function. \glspl{VAE} use prescribed generative distributions and optimize likelihood whereas \glspl{GAN} use implicit generative distributions and optimize an adversarial loss. Our results in Section\nobreakspace \ref {sec:empirical} suggest that the implicit generators of traditional \glspl{GAN} are not the key to high sample quality; rather, the key is the adversarial loss. This is because Pres\glspl{GAN} use the same prescribed generative distributions as \glspl{VAE} and achieve similar or sometimes better sample quality than \glspl{GAN}. 

\parhead{Mode collapse, diversity, and imbalanced data.}
The current literature on measuring mode collapse in \glspl{GAN} only focuses on showing that mode collapse happens when the number of modes in the data distribution is high. 
Our results show that mode collapse can happen not only when the number of modes of the data distribution is high, but also when the data is imbalanced; even when the number of modes is low. 
Imbalanced data are ubiquitous. Therefore, mitigating mode collapse in \glspl{GAN} is important for the purpose of diverse data generation.

\parhead{\glspl{GAN} and generalization.}
The main method to evaluate generalization for density estimators is predictive log-likelihood. Our results agree with the current literature that \glspl{GAN} don't generalize as well as \glspl{VAE} which are specifically trained to maximize log-likelihood. However, our results show that entropy-regularized adversarial learning can reduce the gap in generalization performance between \glspl{GAN} and \glspl{VAE}. Methods that regularize \glspl{GAN} with the maximum likelihood objective achieve good generalization performance when compared to \glspl{VAE} but they sacrifice sample quality when doing so~\citep{grover2018flow}. In fact we also experienced this tension between sample quality and high log-likelihood in practice. 

Why is there such a gap in generalization, as measured by predictive log-likelihood, between \glspl{GAN} and \glspl{VAE}? In our empirical study in Section\nobreakspace \ref {sec:empirical} we controlled for the architecture and the evaluation procedure which left us to compare maximizing likelihood against adversarial learning. Our results suggest mode collapse alone does not explain the gap in generalization performance between \glspl{GAN} and \glspl{VAE}. Indeed Table\nobreakspace \ref {tab:loglik} shows that even on \textsc{MNIST}, where mode collapse does not happen, the \gls{VAE} achieves significantly better log-likelihood than a \gls{GAN}. 

We looked more closely at the encoder fitted at test time to evaluate log-likelihood for both the \gls{VAE} and the \gls{GAN} (not shown in this paper). We found that the encoder implied by a fitted \gls{GAN} is very underdispersed compared to the encoder implied by a fitted \gls{VAE}. Underdispersed proposals have a negative impact on importance sampling estimates of log-likelihood. We tried to produce a more overdispersed proposal using the procedure described in Section\nobreakspace \ref {subsec:loglik_evaluation}. However we leave as future work learning overdispersed proposals for \glspl{GAN} for the purpose of log-likelihood evaluation. 


 
\subsection*{Acknowledgements}
We thank Ian Goodfellow, Andriy Mnih, Aaron Van den Oord, and Laurent Dinh for their comments. Francisco J.\ R.\ Ruiz is supported by the European Union's Horizon 2020 research and innovation programme under the Marie Sk\l{}odowska-Curie grant agreement No.\ 706760. Adji B.\ Dieng is supported by a Google PhD Fellowship.

\bibliographystyle{apa}
\bibliography{main}


\section*{Appendix}
\stepcounter{section}

\subsection{Other Ways to Compute Predictive Log-Likelihood}\label{app:proposals}

Here we discuss different ways to obtain a proposal in order to approximate the predictive log-likelihood.
For a test instance , we estimate the marginal log-likelihood  using importance sampling,

where we draw the  samples  from a proposal distribution . We next discuss different ways to form the proposal . 

One way to obtain the proposal is to set  as a Gaussian distribution whose mean and variance are computed using samples from an \acrshort{HMC} algorithm with stationary distribution . That is, the mean and variance of  are set to the empirical mean and variance of the \acrshort{HMC} samples.

The procedure above requires to run an \acrshort{HMC} sampler, and thus it may be slow. We can accelerate the procedure with a better initialization of the \acrshort{HMC} chain.
Indeed, the second way to evaluate the log-likelihood also requires the \acrshort{HMC} sampler, but
it is initialized using a mapping . 
The mapping  is a network that maps from observed space  to latent space . The parameters  of the network can be learned at test time using generated data. In particular,  can be obtained by generating data from the fitted generator of Pres\acrshort{GAN} and then fitting  to map  to  by maximum likelihood. This is, we first sample  pairs  from the learned generative distribution and then we obtain  by minimizing
.
Once the mapping is fitted, we use it to initialize the \acrshort{HMC} chain.

A third way to obtain the proposal is to learn an encoder network  jointly with the rest of the Pres\acrshort{GAN} parameters. This is effectively done by letting the discriminator distinguish between pairs  and  rather than discriminate  against samples from the generative distribution. These types of discriminator networks have been used to learn a richer latent space for \acrshort{GAN}~\citep{donahue2016adversarial, dumoulin2016adversarially}. 
In such cases, we can use the encoder network  to define the proposal, either by setting  or by initializing the \acrshort{HMC} sampler at the encoder mean.

The use of an encoder network is appealing but it requires a discriminator that takes pairs . The approach that we follow in the paper also uses an encoder network but keeps the discriminator the same as for the base \acrshort{DCGAN}. We found this approach to work better in practice. More in detail, we use an encoder network ; however the encoder is fitted at test time by maximizing the variational \acrshort{ELBO}, given by . We set the proposal . (Alternatively, the encoder can be used to initialize a sampler.)

\subsection{Assessing mode collapse under increased data imbalance}
In the main paper we show that mode collapse can happen not only when there are increasing number of modes, as done in the \gls{GAN} literature, but also when the data is imbalanced. We consider a perfectly balanced version of \textsc{mnist} by using 5,000 training examples per class, totalling 50,000 training examples. We refer to this original balanced dataset as {\bf D}. We build nine additional training sets from this balanced dataset. Each additional training set {\bf D} leaves only  training examples for each class . See Table~\ref{supp_tab:class_dist} for all the class distributions. 

\begin{table*}[!hbpt]
\centering
\caption{Class distributions using the \textsc{mnist} dataset. There are  class---one class for each of 
the  digits in \textsc{mnist}. The distribution D is uniform and the other distributions correspond 
to different imbalance settings as given by the proportions in the table. Note these proportions might not sum to one exactly because of rounding.}
\begin{tabular}[\textwidth]{ccccccccccc}
\toprule
Dist &  &  &   &  &  &    &    &    &   &   \\
\midrule
D &   &  &  &  &  &  &  &  &  &  \\
D &   &  &  &  &  &  &  &  &  &    \\
D &  &  &   &   &   &   &   &   &  &      \\
D &   &  &  &   &    &   &    &    &   &   \\
D &   &  &  &   &  &  &  &  &  &    \\
D &   & &   &  &   &  &  &  &  &  \\
D &  &  &  &   &  &   &  &  &  &    \\
D &   &  &   &   &  &   &  &   &  &   \\
D &   &  &  &   &  &   &  &   &    &   \\
D & &  &   &   &  &  & &   &    &    \\
\bottomrule
\end{tabular}
\label{supp_tab:class_dist}
\end{table*}

\subsection{Sample quality}\label{app:samples}

Here we show some sample images generated by \acrshort{DCGAN} and Pres\acrshort{GAN}, together with real images from each dataset. These images were not cherry-picked, we randomly selected samples from all models. For Pres\acrshort{GAN}, we show the mean of the generator distribution, conditioned on the latent variable .
In general, we observed the best image quality is achieved by the entropy-regularized Pres\acrshort{GAN}.

\newpage


\begin{figure}[t]
	\centering
	\subfigure[Real images.]{\includegraphics[width=\textwidth, height=2.5cm]{figures/celeba_real_samples.png}}
	\subfigure[\acrshort{DCGAN} Samples]{\includegraphics[width=\textwidth, height=2.5cm]{figures/dcgan_celeba_samples.png}}
	\subfigure[\acrshort{VEEGAN} Samples]{\includegraphics[width=\textwidth, height=2.5cm]{figures/veegan_celeba_samples.png}}
	\subfigure[\acrshort{PACGAN} Samples]{\includegraphics[width=\textwidth, height=2.5cm]{figures/pacgan_celeba_samples.png}}
	\subfigure[Pres\acrshort{GAN} Samples]{\includegraphics[width=\textwidth, height=2.5cm]{figures/presgan_celeba_samples.png}}
	\caption{Real and generated images on \textsc{celeba}.}
	\label{supp_fig:images_celeba}
\end{figure}

 
\end{document}
