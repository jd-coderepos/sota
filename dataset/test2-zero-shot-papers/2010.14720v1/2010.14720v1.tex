



\documentclass[11pt]{article}
\usepackage{coling2020}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{booktabs}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{graphicx}
\graphicspath{{images/}}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{multirow}


\colingfinalcopy 

\title{Second-Order Unsupervised Neural Dependency Parsing}

\author{Songlin Yang\textsuperscript{1,2,3}, Yong Jiang\textsuperscript{4}, Wenjuan Han\textsuperscript{5}, Kewei Tu\textsuperscript{1}\thanks{\, 	 Corresponding Author}\\
  \textsuperscript{1}School of Information Science and Technology, ShanghaiTech University \\
  \textsuperscript{2}{Shanghai Institute of Microsystem and Information Technology, Chinese Academy of Sciences}\\
  \textsuperscript{3}{University of Chinese Academy of Sciences}\\
\textsuperscript{4}Alibaba DAMO Academy, Alibaba Group\\
  \textsuperscript{5}Department of Computer Science, National University of Singapore \\
    {\tt \{yangsl,tukw\}@shanghaitech.edu.cn}\\
    {\tt yongjiang.jy@alibaba-inc.com}\qquad
    {\tt dcshanw@nus.edu.sg}\\
 }

\date{}

\begin{document}
\maketitle
\begin{abstract}
Most of the unsupervised dependency parsers are based on first-order probabilistic generative models that only consider local parent-child information. Inspired by second-order supervised dependency parsing, we proposed a second-order extension of unsupervised neural dependency models  that incorporate grandparent-child or sibling information.  We also propose novel design of the neural parameterization and optimization methods of the dependency models. In second-order models, the number of grammar rules grows cubically with the increase of vocabulary size, making it difficult to train lexicalized models that may contain thousands of words. To circumvent this problem while still benefiting from both second-order parsing and lexicalization, we use the agreement-based learning framework to jointly train a second-order unlexicalized model and a first-order lexicalized model.   Experiments on multiple datasets show the effectiveness of our second-order models compared with recent state-of-the-art methods. Our joint model achieves a 10\% improvement over the previous state-of-the-art parser on the full WSJ test set.\footnote{Our source code is available at: https://github.com/sustcsonglin/second-order-neural-dmv}
\end{abstract}


\section{Introduction}
\label{intro}



Dependency parsing is a classical task in natural language processing. The head-dependent relations produced by dependency parsing can provide an approximation to the semantic relationship between words, which is useful in many downstream NLP tasks such as machine translation, information extraction and question answering.  Nowadays, supervised dependency parsers can reach a very high accuracy \cite{DBLP:conf/iclr/DozatM17,DBLP:conf/acl/ZhangLZ20}. Unfortunately, supervised parsing requires treebanks (annotated parse trees) for training, which are very expensive and time-consuming to build. On the other hand, unsupervised dependency parsing requires only unannotated corpora for training, though the accuracy of unsupervised parsing still lags far behind that of supervised parsing.  We focus on unsupervised dependency parsing in this paper. 


Most methods in the literature of unsupervised dependency parsing are based on the Dependency Model with Valence (DMV) \cite{Klein2004CorpusBasedIO}, which is a probabilistic generative model. A main disadvantage of DMV and many of its extensions is that they lack expressiveness. The generation of a dependent token is only conditioned on its parent, the relative direction of the token to its parent, and whether its parent has already generated any child in this direction, hence ignoring other contextual information. To improve model expressiveness, researchers often turn to discriminative methods, which can incorporate more contextual information into the scoring or prediction of dependency arcs. For example, \newcite{Grave2015ACA} uses the idea of disrciminative clustering,  \newcite{Cai2017CRFAF} uses a discriminative parser in the CRF-autoencoder framework, and \newcite{Li2018DependencyGI} uses an encoder-decoder framework that contains a discriminative transitioned-based parser. For DMV, \newcite{Han2019EnhancingUG} proposes the discriminative neural DMV which uses a global sentence embedding to introduce contextual information into the calculation of grammar rule probabilities. In the literature of supervised graph-based dependency parsing, however, there exists another technique for incorporating contextual information and increasing expressiveness, namely high-order parsing \cite{Koo2010EfficientTD,Ma2012FourthOrderDP}. A first-order parser, such as the DMV, only considers local parent-children information. In comparison, a high-order parser takes into account the interaction between multiple dependency arcs.

In this work, we propose the second-order neural DMV model, which incorporates second-order information (e.g., sibling or grandparent) into the original (neural) DMV model. To achieve better learning accuracy, we design a new neural architecture for rule probability computation and promote direct marginal likelihood optimization \cite{Salakhutdinov2003OptimizationWE,tran-etal-2016-unsupervised} over the widely used expectation-maximization algorithm for training. One particular challenge faced by second-order neural DMVs is that the number of grammar rules grows cubically to the vocabulary size, making it difficult to store and train a lexicalized model containing thousands of words. Therefore, instead of learning a second-order lexicalized model, we propose to jointly learn a second-order unlexicalized model (whose vocabulary consists of POS tags instead of words) and a first-order lexicalized model based on the agreement-based learning framework \cite{Liang2007AgreementBasedL}. The jointly learned models have a manageable number of grammar rules while still benefiting from both second-order parsing and lexicalization.

We conduct experiments on the Wall Street Journal (WSJ) dataset and seven languages on the Universal Dependencies (UD) dataset. The experimental results demonstrate that our models achieve state-of-the-art accuracies on unsupervised dependency parsing. 











\section{Background}


\subsection{Dependency Model With Valence}
The Dependency Model with Valence (DMV) \cite{Klein2004CorpusBasedIO} is a probabilistic generative model of a sentence and its parse tree.  It generates a dependency parse tree from the imaginary root node in a recursive top-down manner. There are three types of probabilistic grammar rules in a DMV, namely ROOT, CHILD and DECISION rules, each associated with a set of multinomial distributions ,  and , where  is the parent token,  is the child token,  is the continue/stop decision,  indicates the direction of generation, and  indicates whether parent  has generated any child in direction . To generate a sequence of tokens along with its dependency parse tree, the DMV model generates a token  from the ROOT distribution  firstly. Then for each token  that has already been generated, it generates a decision from the DECISION distribution  to determine whether to generate a new child in direction . If  is CONTINUE, then a new child  is generated from the CHILD distribution . If  is STOP, then  stops generating children in direction . The joint probability of the sequence and its corresponding dependency parse tree can be calculated by taking product of the probabilities of all the generation steps. 



\subsection{Neuralized DMV Models}
\paragraph{Neural DMV} One limitation of the DMV model is that it does not consider the correlation between tokens.  \newcite{Jiang2016UnsupervisedND} proposed the Neural DMV (NDMV) model, which uses continuous POS embedding to represent discrete POS tags and calculate rule probabilities through neural networks based on the POS embedding. In this way, the model can learn the correlation between POS tags and smooth grammar rule probabilities accordingly.

\paragraph{Lexicalized NDMV}
Neural DMV is still an unlexicalized model which is based on POS tags and does not use word information. \newcite{Han2017DependencyGI} proposed the Lexicalized NDMV (L-NDMV) in which each token is a POS/word pair. The neural network that computes rule probabilities takes both the POS embedding and the word embedding as input. To reduce the vocabulary size, they replace low-frequency words with their POS tags. 

\section{Method}
\subsection{Second-Order Parsing}
In our proposed second-order NDMV, we calculate each rule probability based additionally on the information of the sibling or grandparent. We take sibling-NDMV for example to demonstrate the generative story.
\begin{itemize}
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}
    \item We start with the imaginary root token, generating its only child  with probability  
    \item For each token , we decide whether to generate a new child or not with probability , where  is the previous child token generated by  in direction .  If  has not generated any child in direction  yet, we use a special symbol
   NULL to represent . 
   \item If decision  is CONTINUE,  generates a new child  with probability . If decision  is STOP,  stops generating children in direction .
\end{itemize}

For parsing, we design dynamic programming algorithms adapted from \newcite{Koo2010EfficientTD}. Since the grandparent token is deterministic for each token, the parsing algorithm of our grand-NDMV model is similar to theirs. There are two options for determining the sibling token since the generation process of child tokens can be either from the inside out or from the outside in. \newcite{Koo2010EfficientTD} make the inside-out assumption, but in this paper, we make the outside-in assumption because it makes implementation easier and can achieve better performance empirically. We provide the pseudo code of the second-order inside algorithm and the second-order parsing algorithm in the appendix.

\subsection{Parameterization}
\label{section: parameterization}
In a neural DMV, we compute the probability of a grammar rule using a neural network. Below we formulate the computation of CHILD rule probabilities. The full architecture of the neural network is shown in Figure \ref{neural architecture}.
ROOT and DECISION rule probabilities are computed in a similar way.

In our second-order neural DMV, each CHILD rule  involves three tokens: parent , child , and sibling (or grandparent) . 
Denote the embedding of the parent, child and sibling (or grandparent) by , which are retrieved from a shared token embedding layer. We use three different linear transformations to produce the representations of a token as a parent, child, and sibling (or grandparent).

We feed  to the same neural network that consists of three consecutive MLPs. The first and second MLPs are used respectively to insert valence and direction information into the representations, and the last MLP is used to produce final hidden representations  (see the appendix for the complete formulation). We use different parameters of the first and second MLPs for different values of valence  and direction . We add skip-connections to the first and second MLPs because skip-connections have been found very useful in unsupervised neural parsing \cite{DBLP:conf/acl/KimDR19}. 

We then follow \newcite{Wang2019SecondOrderSD} and use a decomposed trilinear function to compute the unnormalized rule probability from the three vectors . 
 

where  are the parameters of the decomposed trilinear function and  is scalar multiplication.
Then we apply a softmax function to produce the final rule probability. 

where  is the vocabulary. 


\begin{figure}[ht]   \centering  

    \includegraphics[width=\textwidth ]{neural_architecture.pdf}   \caption{Illustration of our neural architecture.}   \label{neural architecture}  \end{figure}


\subsection{Learning}
\label{dmo}
The learning objective function  is the log-likelihood of training sentences 
   
where  is the parameters of the neural networks. The probability of each sentence  is defined as:

where  is the set of all possible dependency parse trees for sentence .  We use  to represent the number of times rule  is used in dependency parse tree  of sentence . Then we have 

where  is the collection of all DECISION, CHILD and ROOT rules.

\paragraph{Learning via EM algorithm}
We can rewrite the log-likelihood of sentence  as follows.

where  is an arbitrary distribution and  is the entropy function. In the E-step, we fix  and set . In the M-step, we fix  and update  with the following objective:

where  is the expected count of grammar rule  in sentence  based on , which can be obtained using the inside-outside algorithm. We can use gradient descent to update . 


\paragraph{Learning via direct marginal likelihood optimization} 
We can also use gradient descent to maximize  directly. Based on the derivation of \newcite{Salakhutdinov2003OptimizationWE}\footnote{See Equation 8 in \newcite{Salakhutdinov2003OptimizationWE}.}, we have 

where  is the expected count of grammar rule  in sentence  based on . Traditionally, we use the inside-outside algorithm to obtain the expected count . \newcite{Eisner2016InsideOutsideAF} points out that we can use back-propagation to calculate the expected count .

So we only need to use the inside algorithm to calculate  and then use back-propagation to update the parameters directly, without the need for the outside algorithm. 

\paragraph{Mini-batch gradient descent as online EM}
In Equation \ref{direct marginal optimization}, we note that the gradient contains the term . If we use mini-batch gradient descent to optimize , it is analogous to the online-EM algorithm \cite{Liang2009OnlineEF}. To compute the gradient for each mini-batch, we first need to compute the expected counts from the training sentences in the mini-batch, which is exactly what the online E-step does; we then use the expected counts  to compute the gradient and update the model parameters, which is similar to the M-step, except that here we only perform one update step, while in the EM algorithm multiple update steps may be taken based on the same expected counts. According to \newcite{Liang2009OnlineEF}, online-EM has a faster convergence speed and can even find a better solution. Empirically, we do find that direct marginal likelihood optimization outperforms the EM algorithm.

\subsection{Agreement-Based Learning}
In our second-order DMV model, the number of grammar rules is , which is cubic in the vocabulary size . When our model is lexicalized, the vocabulary may contain thousands of words or more, making the model size less manageable. Instead of learning a second-order lexicalized model, we propose to jointly learn a second-order unlexicalized model (whose vocabulary consists of POS tags instead of words) and a first-order lexicalized model based on the agreement-based learning framework \cite{Liang2007AgreementBasedL}. The jointly learned models have a manageable number of grammar rules while still benefiting from both second-order parsing and lexicalization. Empirically, we do find that the jointly trained models outperform lexicalized second-order models. 
 
Following \newcite{Liang2007AgreementBasedL}, we define the objective function for our jointly trained first-order L-NDMV and second-order NDMV as

where  is parameters of L-NDMV and  is parameters of second-order NDMV. Intuitively, the objective requires the two models to reach agreement on the probability distribution of dependency parse tree . We use joint decoding (parsing) to predict dependency parse tree  for sentence .
 
The inside and parsing algorithms for jointly trained models can be found in the appendix.

\paragraph{Learning via product EM algorithm} 
\newcite{Liang2007AgreementBasedL} propose to optimize the objective using the product EM algorithm based on the following lower bound of the objective.

The product EM algorithm performs coordinate-wise ascent on .   In the product E-step, we optimize  with respect to .
  
where  does not depend on  and .  In the product E-step, the maximum can be obtained  by setting  to minimize the KL term.
In the M-step, we optimize  with respect to .

where  does not depend on . It consists of one term for each model. We update the parameters of each model separately based on the expected counts obtained from the product E-step, which can be calculated through the inside-outside algorithm.

\paragraph{Learning via direct marginal likelihood optimization}   can be calculated through the inside algorithm. Similar to what we describe in Section \ref{dmo}, we can benefit from both agreement-based learning and the online-EM algorithm if we use gradient descent directly to optimize  instead of using the product EM algorithm.

\section{Experiment}
\subsection{Datasets and Setting}
\paragraph{English Penn Treebank}  We conduct experiments on the Wall Street Journal (WSJ) corpus, with section 2-21 for training, section 22 for validation and section 23 for testing.  We use sentences of length  10 in training and use sentences of length  10 (WSJ10) and all sentences (WSJ) in testing.

\paragraph{Universal Dependency Treebank} Following the setting of \newcite{Li2018DependencyGI} and \newcite{Han2019EnhancingUG}, we conduct experiments on selected languages from the Universal Dependency Treebank
1.4 \cite{Nivre2016UniversalDV}. We use sentences of length  15 in training and sentences of length  15 and  40 in testing. 

\paragraph{Setting} On the WSJ dataset, for fair comparison, we follow \newcite{Han2017DependencyGI} and \newcite{Han2019EnhancingUG} and use HDP-DEP \cite{Naseem2010UsingUL} to initialize our models. Specifically, we train the unsupervised HDP-DEP model on WSJ, use it to parse the training corpus, and then use the predicted parse trees to perform supervised learning of our model for several epochs. On the UD dataset, we use the K\&M initialization \cite{Klein2004CorpusBasedIO}.
We use direct marginal likelihood optimization (DMO) as the training method and use Adam \cite{Kingma2015AdamAM} as the optimizer with learning rate 0.001. The batch size is set to 64 for WSJ and 100 for UD.
The hyperparameters of the neural networks, the setting of L-NDMV and more details can be found in the appendix. We apply early stopping based on the log-likelihood of the development data and report the mean accuracy over 5 random restarts. 


\subsection{Result}
\paragraph{Result on WSJ}In Table \ref{wsjresult}, we compare our methods with previous unsupervised dependency parsers. Our sibling-NDMV model can outperform the previous state-of-the-art parser by 1.9 points on WSJ10 and 3.1 points on WSJ in the unlexicalized setting.  Our lexicalized sibling-NDMV achieves further improvement over the unlexicalized sibling-NDMV. On the other hand, our grand-NDMV performs significantly worse than the sibling-NDMV and lexicalization hurts its performance. Why grandparent information is less useful than sibling information in unsupervised parsing is an intriguing question that we leave for feature research. 
Joint training with a first-order L-NDMV can increase the performance of unlexicalized sibling-NDMV from 77.5 to 79.9 and that of unlexicalized grand-NDMV from 71.4 to 76.0 on WSJ10. The jointly trained models also outperform the lexicalized second-order models.

\paragraph{Result on UD} In Table \ref{ud_result}, we first compare our models with models which do not use the universal linguistic prior (UP)\footnote{The universal linguistic prior is a set of syntactic dependencies that are common in many languages, proposed by \newcite{Naseem2010UsingUL}}. The variational variant of D-NDMV \cite{Han2019EnhancingUG}
is the recent state-of-the-art model without UP. Our method outperforms theirs on six of the eight languages and also on average. We then compare our second-order models with recent state-of-the-art discriminative models, which rely heavily on the universal linguistic prior to achieve good performance (for example, \newcite{Li2018DependencyGI} reported bad results if they do not use the universal linguistic prior). We find that sibling-NDMV can outperform these discriminative models while grand-NDMV can achieve comparable results, even though we do not utilize the universal linguistic prior.

\begin{table}[tbp]
\centering
\small
\begin{tabular}{cccc}
\hline
\multicolumn{2}{c|}{\bf\textsc{Methods}}              & \multicolumn{1}{c|}{\bf\textsc{WSJ10}} & \bf\textsc{WSJ}  \\ \hline
\multicolumn{2}{c|}{DMV\cite{Klein2004CorpusBasedIO}}                  & \multicolumn{1}{c|}{58.3}  & 39.4 \\
\multicolumn{2}{c|}{LN \cite{cohen2009logistic}}                   & \multicolumn{1}{c|}{59.4}  & 40.5 \\
\multicolumn{2}{c|}{Convex-MST \cite{Grave2015ACA}}           & \multicolumn{1}{c|}{60.8}  & 48.6 \\
\multicolumn{2}{c|}{Shared LN \cite{Cohen2009SharedLN}}            & \multicolumn{1}{c|}{61.3}  & 41.4 \\
\multicolumn{2}{c|}{Feature DMV  \cite{berg-kirkpatrick-etal-2010-painless}}          & \multicolumn{1}{c|}{63.0}  & -    \\
\multicolumn{2}{c|}{PR-S \cite{Gillenwater2011PosteriorSI}}                 & \multicolumn{1}{c|}{64.3}  & 53.3 \\
\multicolumn{2}{c|}{E-DMV \cite{Headden2009ImprovingUD}}                & \multicolumn{1}{c|}{65.0}  & -    \\
\multicolumn{2}{c|}{TSG-DMV \cite{Blunsom2010UnsupervisedIO}}              & \multicolumn{1}{c|}{65.9}  & 53.1 \\
\multicolumn{2}{c|}{UR-A E-DMV \cite{Tu2012UnambiguityRF}}           & \multicolumn{1}{c|}{71.4}  & 57.0 \\
\multicolumn{2}{c|}{CRFAE \cite{Cai2017CRFAF}}                & \multicolumn{1}{c|}{71.7}  & 55.7 \\
\multicolumn{2}{c|}{Neural DMV \cite{Jiang2016UnsupervisedND}}         & \multicolumn{1}{c|}{72.5}  & 57.6 \\
\multicolumn{2}{c|}{HDP-DEP \cite{Naseem2010UsingUL}}               & \multicolumn{1}{c|}{73.8}  & -    \\
\multicolumn{2}{c|}{NVTP \cite{Li2018DependencyGI}}                 & \multicolumn{1}{c|}{54.7}  & 37.8 \\
\multicolumn{2}{c|}{Variational variant D-NDMV * \cite{Han2019EnhancingUG}}                          & \multicolumn{1}{c|}{75.5} & 60.4 \\
\multicolumn{2}{c|}{Deterministic variant D-NDMV * \cite{Han2019EnhancingUG}}                        & \multicolumn{1}{c|}{75.6} & 61.4 \\
\multicolumn{2}{c|}{L-NDMV * \cite{Han2017DependencyGI}}         & \multicolumn{1}{c|}{75.1}  & 59.5 \\ \hline
\multicolumn{2}{c|}{grand-NDMV *}                              & \multicolumn{1}{c|}{71.4} & 57.3 \\
\multicolumn{2}{c|}{sibling-NDMV *}                              & \multicolumn{1}{c|}{77.5} & 64.5 \\
\multicolumn{2}{c|}{Lexicalized grand-NDMV *} & \multicolumn{1}{c|}{63.0}      &  52.6    \\ 
\multicolumn{2}{c|}{Lexicalized sibling-NDMV *} & \multicolumn{1}{c|}{78.3}      & 66.4     \\ \hline
\multicolumn{2}{c|}{Joint training: grand-NDMV + L-NDMV *}   &
\multicolumn{1}{c|}{76.0}     &   64.3   \\
\multicolumn{2}{c|}{Joint training: sibling-NDMV + L-NDMV *}  &
\multicolumn{1}{c|}{ \textbf{79.9}}     &  \textbf{67.5}    \\ \hline
\multicolumn{4}{c}{Systems with Additional Training Data (for reference)}                                   \\ \hline
\multicolumn{2}{c|}{CS \cite{Spitkovsky2013BreakingOO}}                   & \multicolumn{1}{c|}{72.0}  & 64.4 \\
\multicolumn{2}{c|}{MaxEnc \cite{le-zuidema-2015-unsupervised}}               & \multicolumn{1}{c|}{73.2}  & 65.8 \\
\multicolumn{2}{c|}{L-NDMV * \cite{Han2017DependencyGI}}               & \multicolumn{1}{c|}{77.2}  & 63.2 \\ \hline
\end{tabular}
\caption{ Comparison on WSJ. *: Approaches which use  \newcite{Naseem2010UsingUL} for initialization.}
\label{wsjresult}
\end{table}

\begin{table}[tbp]
\centering
\small
\begin{tabular}{ccccccccc}
\hline
\multicolumn{1}{c|}{} &
  \multicolumn{6}{c|}{\bf\textsc{NO UP}} &
  \multicolumn{2}{c}{+\bf\textsc{UP}} \\ \cline{2-9} 
\multicolumn{1}{c|}{} &
  NDMV &
  LD &
  DV &
  \multicolumn{1}{c|}{VV} &
  +sibling &
  \multicolumn{1}{c|}{+grand} &
  NVTP &
  CM \\ \hline
\multicolumn{9}{c}{Length  15} \\ \hline
\multicolumn{1}{c|}{Basque} &
  48.3 &
  47.9 &
  40.6 &
  \multicolumn{1}{c|}{42.7} &
  30.8 &
  \multicolumn{1}{c|}{34.1} &
  \textbf{52.9} &
  52.5 \\
\multicolumn{1}{c|}{Dutch} &
  44.1 &
  35.5 &
  42.1 &
  \multicolumn{1}{c|}{43.0} &
  \textbf{46.5} &
  \multicolumn{1}{c|}{42.4} &
  39.6 &
  43.4 \\
\multicolumn{1}{c|}{French} &
  59.5 &
  52.1 &
  59.0 &
  \multicolumn{1}{c|}{61.7} &
  \textbf{65.7} &
  \multicolumn{1}{c|}{62.9} &
  59.9 &
  61.6 \\
\multicolumn{1}{c|}{German} &
  56.2 &
  51.9 &
  56.4 &
  \multicolumn{1}{c|}{58.5} &
  \textbf{63.6} &
  \multicolumn{1}{c|}{48.9} &
  57.5 &
  66.7 \\
\multicolumn{1}{c|}{Italian} &
  72.7 &
  73.1 &
  59.6 &
  \multicolumn{1}{c|}{63.5} &
  \textbf{76.3} &
  \multicolumn{1}{c|}{76.3} &
  59.7 &
  73.2 \\
\multicolumn{1}{c|}{Polish} &
  72.7 &
  66.2 &
  70.5 &
  \multicolumn{1}{c|}{75.8} &
  \textbf{77.1} &
  \multicolumn{1}{c|}{63.7} &
  57.1 &
  66.7 \\
\multicolumn{1}{c|}{Portuguese} &
  45.5 &
  \textbf{70.5} &
  68.8 &
  \multicolumn{1}{c|}{69.1} &
  67.8 &
  \multicolumn{1}{c|}{62.5} &
  52.7 &
  60.7 \\
\multicolumn{1}{c|}{Spanish} &
  38.1 &
  65.5 &
  63.8 &
  \multicolumn{1}{c|}{66.1} &
  67.2 &
  \multicolumn{1}{c|}{\textbf{67.5}} &
  55.6 &
  61.6 \\ \hline
\multicolumn{1}{c|}{\textbf{Average}} &
  53.3 &
  57.8 &
  57.6 &
  \multicolumn{1}{c|}{60.1} &
  \textbf{61.9} &
  \multicolumn{1}{c|}{57.2} &
  54.4 &
  59.3 \\ \hline
\multicolumn{9}{c}{Length  40} \\ \hline
\multicolumn{1}{c|}{Basque} &
  47.8 &
  45.4 &
  39.9 &
  \multicolumn{1}{c|}{42.4} &
  30.5 &
  \multicolumn{1}{c|}{33.0} &
  48.9 &
  \textbf{50.0} \\
\multicolumn{1}{c|}{Dutch} &
  35.6 &
  34.1 &
  42.4 &
  \multicolumn{1}{c|}{43.7} &
  45.0 &
  \multicolumn{1}{c|}{43.7} &
  42.5 &
  \textbf{45.3} \\
\multicolumn{1}{c|}{French} &
  38.1 &
  48.6 &
  57.2 &
  \multicolumn{1}{c|}{58.5} &
  \textbf{64.9} &
  \multicolumn{1}{c|}{61.4} &
  55.4 &
  62.0 \\
\multicolumn{1}{c|}{German} &
  50.4 &
  50.5 &
  54.5 &
  \multicolumn{1}{c|}{52.9} &
  \textbf{61.5} &
  \multicolumn{1}{c|}{46.7} &
  54.2 &
  51.4 \\
\multicolumn{1}{c|}{Italian} &
  63.6 &
  71.1 &
  60.2 &
  \multicolumn{1}{c|}{61.3} &
  \textbf{71.8} &
  \multicolumn{1}{c|}{69.7} &
  55.7 &
  69.1 \\
\multicolumn{1}{c|}{Polish} &
  62.8 &
  63.7 &
  66.7 &
  \multicolumn{1}{c|}{73.0} &
  \textbf{75.0} &
  \multicolumn{1}{c|}{62.4} &
  51.7 &
  63.4 \\
\multicolumn{1}{c|}{Portuguese} &
  49.0 &
  \textbf{67.2} &
  64.7 &
  \multicolumn{1}{c|}{65.7} &
  63.7 &
  \multicolumn{1}{c|}{60.3} &
  45.3 &
  57.1 \\
\multicolumn{1}{c|}{Spanish} &
  58.0 &
  61.9 &
  64.3 &
  \multicolumn{1}{c|}{64.4} &
  \textbf{66.8} &
  \multicolumn{1}{c|}{65.0} &
  52.5 &
  61.9 \\ \hline
\multicolumn{1}{c|}{\textbf{Average}} &
  50.7 &
  55.3 &
  56.2 &
  \multicolumn{1}{c|}{57.7} &
  \textbf{59.9} &
  \multicolumn{1}{c|}{55.2} &
  50.8 &
  57.5 \\ \hline
\end{tabular}
\caption{Comparison on Universal Dependency Treebanks. No UP: System using the universal linguistic prior. +UP : Systems using the universal linguistic prior.  LD: LC-DMV \cite{Noji2015LeftcornerPF}. DV,VV: The deterministic and variational variants of D-NDMV \cite{Han2019EnhancingUG}. +sibling: Our second-order sibling-NDMV. +grand: Our second-order grand-NDMV.  NVTP: Neural variational transition-based
parser \cite{Li2018DependencyGI}. CM: Convex-MST \cite{Grave2015ACA}.}
\label{ud_result}
\end{table}

\section{Analysis}
\subsection{Effect of Skip-Connections}
From Table \ref{UD: SC and EM} and \ref{WSJ: SC and EM}, we find that using skip-connections can achieve higher log-likelihood and better parsing accuracy in most cases. On UD, the performance is much better when using skip-connections except on Basque. 

\begin{table}[t]
\centering
\small
\begin{tabular}{c|cc|c|cc|c}
\hline
\bf\textsc{Language}  & \multicolumn{3}{c|}{\bf\textsc{Log-likelihood}}          & \multicolumn{3}{c}{\bf\textsc{UAS (  / )}}    \\ \cline{2-7} 
          & \multicolumn{2}{c|}{DMO}             & EM    & \multicolumn{2}{c|}{DMO}               & EM        \\ \cline{2-7} 
          & \multicolumn{1}{c|}{w. SC} & w.o. SC & w. SC & \multicolumn{1}{c|}{w. SC} & w.o. SC   & w. SC     \\ \hline
Basque    & -19.3                      & -19.7   & -20.0 & 30.8/30.5                  & 44.7/44.3 & 28.8/29.1 \\
Dutch     & -19.1                      & -19.4   & -19.3 & 46.5/45.0                  & 34.9/34.8 & 43.6/41.8 \\
French    & -19.8                      & -20.9   & -19.4 & 65.7/64.9                  & 47.2/50.1 & 55.0/54.5 \\
German    & -20.9                      & -22.4   & -21.2 & 63.6/61.5                  & 36.4/37.8 & 57.9/54.5 \\
Italian   & -16.7                      & -16.9   & -16.7 & 76.3/71.8                  & 65.6/58.1 & 71.0/65.2 \\
Polish    & -16.2                      & -16.9   & -17.0 & 77.1/75.0                  & 66.2/63.7 & 61.7/60.4 \\
Portugese & -17.9                      & -18.4   & -17.8 & 67.8/63.7                  & 60.0/59.1 & 57.7/51.9 \\
Spanish   & -20.8                      & -21.7   & -20.9 & 67.2/66.8                  & 49.6/48.8 & 61.4/58.1 \\ \hline
\end{tabular}
\caption{Effect of skip-connections and training methods on UD.}
\label{UD: SC and EM}
\end{table}

\begin{table}[t]
\centering
\small
\begin{tabular}{c|cc|c|cc|c}
\hline
  \bf\textsc{Models}                               & \multicolumn{3}{c|}{\bf\textsc{Log-likelihood}}          & \multicolumn{3}{c}{\bf\textsc{UAS ( WSJ10 / WSJ)}}           \\ \cline{2-7} 
                                      & \multicolumn{2}{c|}{DMO}             & EM    & \multicolumn{2}{c|}{DMO}               & EM        \\ \cline{2-7} 
                                      & \multicolumn{1}{c|}{w. SC} & w.o. SC & w. SC & \multicolumn{1}{c|}{w. SC} & w.o. SC   & w. SC     \\ \hline
sibling-NDMV                          & -17.3                      & -17.7   & -17.3 & 77.5/64.5                  & 75.4/63.1 & 77.7/64.5 \\
grand-NDMV                            & -16.9                      & -17.4   & -17.0 & 71.4/57.3                  & 68.4/55.1 & 72.7/61.8 \\
sibling-NDMV + L-NDMV & -53.2                      & -55.3   & -53.5 & 79.9/67.5                  & 78.2/64.7 & 79.1/66.5 \\
grand-NDMV + L-NDMV  & -53.8                      & -57.5   & -54.2 & 76.0/64.3                  & 73.7/60.7 & 75.2/65.3 \\ \hline
\end{tabular}
\caption{Effect of skip-connections and training methods on WSJ.}
\label{WSJ: SC and EM}
\end{table}



\subsection{Comparison of Training Methods}
In Table \ref{UD: SC and EM}, we find that the EM algorithm significantly underperforms DMO. On the other hand, Table \ref{WSJ: SC and EM} shows that the EM algorithm performs comparably to DMO on WSJ.
 
We also compare the learning curves of these two methods. For fair comparison, we use the same batch-size for both methods. First we conduct an experiment using the joint L-NDMV and sibling-NDMV model on WSJ. In Figure \ref{wsj_likelihood}, we find that DMO converges to a higher log-likelihood compared with EM and the convergence speed is roughly the same.  In Figure \ref{wsj_uas}, we find DMO can find a slightly better model compared with EM. Second, we conduct an experiment using sibling-NDMV model on the UD French dataset. In Figure \ref{ud-likelihood}, we find DMO converges faster than EM and converges to a higher log-likelihood. In Figure \ref{ud-likelihood}, we find that the model accuracy of DMO is much higher than that of EM at the beginning, but it drops significantly after epoch 23, suggesting that early-stop is necessary. We also find similar phenomena for other languages on UD. 

It should be noted that we use HDP-DEP \cite{Naseem2010UsingUL} for initialization on WSJ and use K\&M initialization \cite{Klein2004CorpusBasedIO} on UD. We see that HDP-DEP initialization leads to a very high initial UAS of 75\% (Figure \ref{wsj_uas}),
while K\&M initialization leads to a low initial UAS of 38.5\% (Figure \ref{ud_uas}). It can be seen that EM is more sensitive to the initialization while DMO can achieve good results even if the initialization is bad. 

\begin{figure}[tbp] 
\centering
  \begin{minipage}[b]{0.45\linewidth}
    \centering
        \includegraphics[width=7cm]{wsj-loglikelihood.pdf} 
    \caption{Comparison of training methods on log-likelihood for WSJ.} 
          \label{wsj_likelihood} 
    \vspace{0ex}
  \end{minipage}\hspace*{2em}
  \begin{minipage}[b]{0.45\linewidth}
    \centering
    \includegraphics[width=7cm]{wsj-uas.pdf} 
    \caption{Comparison of training methods on UAS for WSJ.} 
      \label{wsj_uas} 
    \vspace{0ex}
  \end{minipage} 
\end{figure}

\begin{figure}[tbp] 
\centering
  \begin{minipage}[b]{0.45\linewidth}
    \centering
    \includegraphics[width=7cm]{ud-loglikelihood.pdf}
    \caption{Comparison of training methods on log-likelihood for UD (French).}
    \label{ud-likelihood}
  \end{minipage}\hspace*{2em}
  \begin{minipage}[b]{0.45\linewidth}
    \centering
\includegraphics[width=7cm]{ud-uas.pdf} 
    \caption{Comparison of training methods on UAS for UD (French).} 
     \label{ud_uas} 
  \end{minipage} 
\end{figure}

\begin{table}[tb]
\centering
\small
\begin{tabular}{l|l|l|l}
\hline
Training method   & \multicolumn{3}{l|}{UAS  (WSJ10 / WSJ)} \\ \hline
                  & L-NDMV         & sibling-NDMV   & joint parsing   \\ \hline
separate training & 76.6 / 62.7    & 77.5 / 64.8    & 78.4 / 65.8      \\ \hline
joint training    & 79.2 / 65.4    & 78.7 / 65.6    & 79.9 / 67.5      \\ \hline
\end{tabular}
\caption{The effect of joint training and joint parsing}
\label{joint-decoding}
\end{table}


\subsection{Effect of Joint Training and Parsing}
In Table \ref{joint-decoding}, we compare the performance with different training and parsing settings. We find that joint parsing is better than separate parsing in both training settings. With joint training, each individual model can achieve better performance compared with separate training, which shows the effectiveness of agreement-based joint learning.  




\subsection{Limitations}
Our second-order NDMV model is more sensitive to the initialization compared with the first-order NDMV model. We fail to produce a good result under the K\&M initialization on WSJ: only 58.5\% UAS for sibling-NDMV on WSJ10, while the first-order NDMV model can achieve 69.7\% UAS. We rely on the parsing result of HDP-DEP to initialize our model in order to reach the state-of-the-art result on WSJ. This is similar to the case of L-NDMV, which performs badly when using the K\&M initialization according to \newcite{Han2017DependencyGI}. Because of the bad performance of L-NDMV with the K\&M initialization as well as the time constraint that prevents us from running HDP-DEP on UD, we did not conduct experiments of agreement-based learning with L-NDMV on the UD datasets. We leave this for future work. 
 
Our second-order model is also quite sensitive to the design of the neural architecture, which is similar to case of unsupervised constituency parsing reported by \newcite{DBLP:conf/acl/KimDR19}. We also try the third-order NDMV model (grand-sibling or tri-sibling) but are not able to get better results compared with sibling-NDMV. 

Our second-order parsing algorithm has a theoretical time complexity of , which is higher than the time complexity of  of transition-based unsupervised parsers \cite{Li2018DependencyGI} and the time complexity of  of first-order NDMV models, where  is the sentence length. However, transition-based parsers are hard to batchify, while our model can be parallelized efficiently following the methods introduced by Torch-Struct \cite{rush-2020-torch}. In practice, our second-order parser runs very fast on GPU, requiring only several minutes to train.

\section{Conclusion}
We propose second-order NDMV models, which incorporate sibling or grandparent information. We find that sibling information is very useful in unsupervised dependency parsing. We use agreement-based learning to combine the benefits of second-order parsing and lexicalization, achieving state-of-the-art results on the WSJ dataset. We also show the effectiveness of our neural parameterization architecture with skip-connections and the direct marginal likelihood optimization method.

\section*{Acknowledgement}
This work was supported by the National Natural Science Foundation of China (61976139).

\bibliographystyle{coling}
\bibliography{coling2020}








\appendix
\section{Appendix}
\subsection{Inside Algorithm and Parsing Algorithm}
\label{intro2}

We use the dynamic programming substructure proposed for second-order supervised dependency parsing.  For grandparent-child model, \newcite{Koo2010EfficientTD} augment both complete and incomplete spans with grandparent indices. They called the augmented span g-spans. Formally, they denote a complete g-span as , where  is a normal complete span in the Eisner algorithm, g is the grandparent's index, with the implication that  is a dependency. Incomplete g-span is defined similarly. 

For second-order NDMV, we further augment incomplete and complete g-spans with valence information. We distinguish the direction of span explicitly, denoting our augmented complete v-span as , where  is the direction,  is the valence,  is the start index and  is the end index of span compared with g-span. Incomplete v-span is defined similarly. 

For grand-NDMV, given sentence , we suppose that  is the imaginary root token and  are tokens.
We denote , , and . Given these definitions, the inside algorithm of grand-NDMV is shown in Algorithm \ref{eisner grand}. 

For sibling-NDMV,  in  stands for the index of sibling instead of the index of grandparent. Given sentence , we suppose that  is a special NULL token which stands for no sibling and  are tokens. We denote , , and . Given these definitions, the inside algorithm of sibling-NDMV is shown in Algorithm \ref{eisner sibling}.

For jointly trained L-NDMV and second-order NDMV model, we take jointly trained L-NDMV sibing-NDMV for example. We denote  for L-NDMV where  is the sequence of word/POS pairs which starts indexing at 1.
The inside algorithm of jointly trained L-NDMV and sibling-NDMV model is shown in Algorithm \ref{eisner joint}.
 
Following \newcite{Eisner2016InsideOutsideAF}, we use back-propagation to obtain expected counts of grammar rules.  For the parsing algorithm, we can replace  with  in Algorithm \ref{eisner grand}, \ref{eisner sibling} and \ref{eisner joint} to get the Viterbi log-likelihood of the sentence, then use back-propagation to get grammar rules which are used in the Viterbi parse tree, and finally reconstruct the parse tree based on these rules. 
 

\subsection{Full Parameterization}

Denote the embedding of the parent, child and sibling (or grandparent) by .  We use three different linear transformations to produce the representations of each token as a parent, child, and sibling (or grandparent).


We feed  to the same neural network which consists of three MLP with skip-connection layer. The first MLP aims at encoding valence information:
 where .

The second MLP aims at encoding direction information:
 where   

We use the final MLP to get final hidden representation :


For the UD dataset, we use a more expressive MLP to get the final hidden representation  since we find that the UD dataset is more difficult to train. 
 


For decision rules, we introduce two embedding  and .  We feed  and  to a fully connected layer  to get  and . We use the same neural network to get hidden representation  and .  For root rules, we introduce . We feed  to a fully connected layer  to get . Also, we use the same neural network to get hidden representation .  We use different decomposed trilinear function parameters for different types of rules. The calculation of the decision rule probability and root rule probability is similar to that of the child rule probability.

\subsection{Hyperparameters}
We set the dimension of POS embedding to 100. The dimension of all linear layers to calculate hidden representation is set to 100. We set the size of decomposed trilinear function parameters to 30 for child and root rules and 10 for decision rules in the unlexicalized setting. 

For the lexicalized model, we set the dimension of word embedding to 100. We concatenate the POS embedding and word embedding as input. The dimension of all linear layers to calculate hidden representation is set to 200.  We set the size of decomposed trilinear function parameters to 150 for child and root rules and 50 for decision rules. We use an additional dropout layer after the embedding layer to avoid over-fitting since the vocabulary size of the lexicalized model is much larger compared to the unlexicalized model. The dropout rate is set to 0.5.

\subsection{Setting of L-NDMV}
The vocabulary consists of word/POS pairs that appear for at least two times in the WSJ10 dataset. We use random embedding to initialize the POS embedding and FastText embedding to initialize the word embedding, which is different from the setting in the original paper \cite{Han2017DependencyGI}. We train FastText on the whole WSJ dataset for 100 epochs with window size 3 and embedding dimension 100. 
\begin{algorithm}[t]
\footnotesize
\SetAlgoLined
\textbf{notation} LEFT=0, RIGHT=0, HASCHILD=0, NOCHILD=1 \newline
\textbf{initialization}
  
  
  
   \newline
\bfseries{for}  \newline
 \hspace*{1em} \bfseries{for} \newline
 \hspace*{2em}  \newline
 \hspace*{2em} \bfseries{for}  \bfseries{or}   \newline
 \hspace*{3em}  \newline
  \hspace*{3em}  \newline
   \hspace*{3em}  \newline
  \hspace*{3em}  \newline
     \hspace*{3em}  \newline
     \hspace*{3em}  \newline
     \hspace*{3em}  \newline
  \hspace*{3em}  \newline
\bfseries{for} \newline
\hspace*{1em}  

 \newline
\Return P

 \caption{Inside algorithm for grand-NDMV}
 \label{eisner grand}
\end{algorithm}
\begin{algorithm}[t]
 \footnotesize
\SetAlgoLined
\textbf{notation} LEFT=0, RIGHT=0, HASCHILD=0, NOCHILD=1 \newline
 \textbf{initialization}
  
  
  
   \newline
\bfseries{for}  \newline
 \hspace*{1em} \bfseries{for} \newline
 \hspace*{2em}  \newline
 \hspace*{2em} \bfseries{for}  \bfseries{or}   \newline
 \hspace*{3em}  \newline
  \hspace*{3em}  \newline
   \hspace*{3em}  \newline
  \hspace*{3em}  \newline
     \hspace*{3em}  \newline
     \hspace*{3em}  \newline
     \hspace*{3em}  \newline
  \hspace*{3em}  \newline
\bfseries{for}  \newline
\hspace*{1em}  \newline
 \newline
\Return P
 \caption{Inside algorithm for sibling-NDMV} 
 \label{eisner sibling}
\end{algorithm}

\begin{algorithm}[t]
\footnotesize
\SetAlgoLined
\textbf{notation} LEFT=0, RIGHT=0, HASCHILD=0, NOCHILD=1 \newline
 \textbf{initialization}
  
  
  
   \newline
\bfseries{for}  \newline
 \hspace*{1em} \bfseries{for} \newline
 \hspace*{2em}  \newline
 \hspace*{2em} \bfseries{for}  \bfseries{or}   \newline
 \hspace*{3em}  \newline
  \hspace*{3em}  \newline
   \hspace*{3em}  \newline
  \hspace*{3em}  \newline
     \hspace*{3em}  \newline
     \hspace*{3em}  \newline
     \hspace*{3em}  \newline
  \hspace*{3em}  \newline
\bfseries{for}  \newline
\hspace*{1em}  \newline
 \newline
\Return P
 \caption{Inside algorithm for joint L-NDMV and sibling-NDMV} 
 \label{eisner joint}
\end{algorithm}

\end{document}