\documentclass{article}









\usepackage[final,nonatbib]{nips_2019}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{array}          

\usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{math}
\usepackage{bbold}
\usepackage{caption}

\captionsetup[table]{skip=0pt}
\DeclareCaptionLabelFormat{andtable}{#1~#2  \&  \tablename~\thetable}
\newcommand{\aka}{\emph{a.k.a.}{}}
\def\eg{\textit{e.g.}}
\def\ie{\textit{i.e.}}
\def\Eg{\textit{E.g.}}
\def\etal{\textit{et al.}}

\newcommand{\steph}[1]{\textcolor{red}{Steph: #1}}
\newcommand{\alex}[1]{\textcolor{blue}{Alex: #1}}
\newcommand{\sergey}[1]{\textcolor{blue}{Sergey: #1}}

\newcommand{\eli}[1]{\textcolor{magenta}{ #1}}

\title{First Order Motion Model for Image Animation}



\author{
Aliaksandr Siarohin\\
DISI, University of Trento\\
\texttt{aliaksandr.siarohin@unitn.it}
\And
St{\'e}phane Lathuili{\`e}re\\
DISI, University of Trento\\
LTCI, Télécom Paris, Institut polytechnique de Paris\\
\texttt{stephane.lathuilire@telecom-paris.fr}
\And
Sergey Tulyakov\\
Snap Inc.\\
\texttt{stulyakov@snap.com}
\And
Elisa Ricci\\
DISI, University of Trento\\
Fondazione Bruno Kessler\\
\texttt{e.ricci@unitn.it}\\
\And
Nicu Sebe\\
DISI, University of Trento\\
Huawei Technologies Ireland\\
\texttt{niculae.sebe@unitn.it}\\
}




\begin{document}

\maketitle

\begin{abstract}
Image animation consists of generating a video sequence so that an object in a source image is animated according to the motion of a driving video.
Our framework addresses this problem without using any annotation or prior information about the specific object to animate. Once trained on a set of videos depicting objects of the same category (\eg \ faces, human bodies), our method can be applied to any object of this class. 
   To achieve this, we decouple appearance and motion information using a self-supervised formulation. To support complex motions, we use a representation consisting of a set of learned keypoints along with their local affine transformations. A generator network models occlusions arising during target motions and combines the appearance extracted from the source image and the motion derived from the driving video. Our framework scores best on diverse benchmarks and on a variety of object categories. Our source code is publicly  available\footnotemark[1].
\end{abstract}

\footnotetext[1]{ \href{https://github.com/AliaksandrSiarohin/first-order-model}{https://github.com/AliaksandrSiarohin/first-order-model}}

\vspace{-0.2cm}
\section{Introduction}
\vspace{-0.2cm}


Generating videos by animating objects in still images has countless applications across areas of interest including movie production, photography, and e-commerce.  
More precisely, image animation refers to the task of automatically synthesizing videos by combining the appearance extracted from a \textit{source image}
 with motion patterns derived from a \textit{driving video}. For instance, a face image of a certain person can be animated following the facial expressions of another individual (see Fig.~\ref{fig:example-main}). In the literature, most methods tackle this problem by
assuming strong priors on the object representation (\eg \ 3D model) \cite{blanz1999morphable} and resorting to computer graphics techniques ~\cite{cao2014displaced,thies2016face2face}. These approaches can be referred to as \textit{object-specific} methods, as they assume knowledge about the model of the specific object to animate.

Recently, deep generative models have emerged as effective techniques for image animation and video retargeting \cite{balakrishnansynthesizing,zablotskaia2019dwnet, bansal2018recycle,Zakharov_2019_CVPR,Shysheya_2019_CVPR,siarohin2018animating,wang2018video,wiles2018x2face,hao2018Geaturegan,liu2019gesture}. In particular, Generative Adversarial Networks (GANs)~\cite{goodfellow2014generative} and Variational Auto-Encoders (VAEs)~\cite{kingma2013auto} have been used to transfer facial expressions~\cite{wang2018video} or motion patterns~\cite{bansal2018recycle} between human subjects in videos.
Nevertheless, these approaches usually rely on pre-trained models in order to extract object-specific representations such as keypoint locations. Unfortunately, these pre-trained models are built using costly ground-truth data annotations \cite{balakrishnansynthesizing,Shysheya_2019_CVPR,hao2018Geaturegan} and are not available in general for an arbitrary object category. To address this issues, recently
Siarohin \etal~\cite{siarohin2018animating} introduced Monkey-Net, the first object-agnostic deep model for image animation. Monkey-Net encodes motion information via keypoints learned in a self-supervised fashion. At test time, the {source image} is animated according to the corresponding keypoint trajectories estimated in the {driving video}.
The major weakness of Monkey-Net is that it poorly models object appearance transformations in the keypoint neighborhoods assuming a zeroth order model (as we show in Sec.~\ref{sec:locAffine}). This leads to poor generation quality in the case of large object pose changes (see Fig. \ref{fig:taichi-transfer-main}).
To tackle this issue, we propose to use a set of self-learned keypoints together with local affine transformations to model complex motions. We therefore call our method a first-order motion model. Second, we introduce an occlusion-aware generator, which adopts an occlusion mask automatically estimated to indicate object parts that are not visible in the source image and that should be inferred from the context. This is especially needed when the driving video contains large motion patterns and occlusions are typical.
Third, we extend the equivariance loss commonly used for keypoints detector training \cite{jakabunsupervised,zhao2018learning}, to improve the estimation of local affine transformations. Fourth, we experimentally show that our method significantly outperforms state-of-the-art image animation methods and can handle high-resolution datasets where other approaches generally fail. Finally, we release a new high resolution dataset, \textit{Thai-Chi-HD}, which we believe could become a reference benchmark for evaluating frameworks for image animation and video generation.




\begin{figure*}[t]
  \centering
\resizebox{0.99\linewidth}{!}{
\begin{tabular}{cccc}
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/tablecap.pdf}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/000vid002}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/001vid002}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/002vid002}}
\\
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/001app002}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/000jac002}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/001jac002}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/002jac002}}
 \\
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/001app008}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/000jac008}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/001jac008}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/002jac008}}
\end{tabular}
\hspace{0.2cm}
\begin{tabular}{cccc}
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/tablecap.pdf}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/000vid009}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/001vid009}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/002vid009}}
\\
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/001app009}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/000jac009}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/001jac009}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/002jac009}}
 \\
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/001app010}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/000jac010}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/001jac010}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/002jac010}}
\end{tabular}
}
\resizebox{0.05\linewidth}{!}{
\begin{tabular}{cccc}
&&&
\end{tabular}
}
\resizebox{0.99\linewidth}{!}{
\begin{tabular}{cccc}
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/tablecap.pdf}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/000vid003}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/001vid003}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/002vid003}}
\\
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/001app003}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/000jac003}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/001jac003}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/002jac003}}
 \\
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/001app006}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/000jac006}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/001jac006}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/002jac006}}
\end{tabular}
\hspace{0.2cm}
\begin{tabular}{cccc}
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/tablecap.pdf}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/000vid004}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/001vid004}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/002vid004}}
\\
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/001app004}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/000jac004}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/001jac004}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/002jac004}}
 \\
  \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/001app007}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/000jac007}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/001jac007}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/main/002jac007}}
\end{tabular}


}

\caption{Example animations produced by our method trained on different datasets: \emph{VoxCeleb}~\cite{Nagrani17} (top left), \emph{Tai-Chi-HD} (top right), \emph{Fashion-Videos}~\cite{zablotskaia2019dwnet} (bottom left) and \emph{MGif}~\cite{siarohin2018animating} (bottom right). We use relative motion transfer for \emph{VoxCeleb} and \emph{Fashion-Videos} and absolute transfer for \emph{MGif} and \emph{Tai-Chi-HD} see Sec.~\ref{sec:test}. Check our project page for more qualitative results\protect\footnotemark[2].}

\label{fig:example-main}
\vspace{-0.5cm}
\end{figure*}

\footnotetext[2]{\href{https://aliaksandrsiarohin.github.io/first-order-model-website/}{https://aliaksandrsiarohin.github.io/first-order-model-website/}}


 \vspace{-0.2cm}
\section{Related work}
\vspace{-0.2cm}

\textbf{Video Generation.}
Earlier works on deep video generation discussed how spatio-temporal neural networks could render video frames from noise vectors~\cite{vondrick2016generating,saito2017temporal}. More recently, several approaches tackled the problem of conditional video generation. For instance, Wang \etal~\cite{wang2018every} combine a recurrent neural network with a VAE in order to generate face videos. 
Considering a wider range of applications, Tulyakov
~\etal~\cite{tulyakov2017mocogan} introduced MoCoGAN, a recurrent architecture adversarially trained in order to synthesize videos from noise, categorical labels or static images. Another typical case of conditional generation is the problem of future frame prediction, in which the generated video is conditioned on the initial frame \cite{finn2016unsupervised,oh2015action,srivastava2015unsupervised,van2017transformation,zhao2018learning}. Note that in this task, realistic predictions can be obtained by simply warping the initial video frame \cite{babaeizadeh2017stochastic,finn2016unsupervised,van2017transformation}. Our approach is closely related to these previous works since we use a warping formulation to generate video sequences. However, in the case of image animation, the applied spatial deformations are not predicted but given by the driving video. 

\textbf{Image Animation.} Traditional approaches for image animation and video re-targeting \cite{cao2014displaced,thies2016face2face,geng20193d} were designed for specific domains such as faces~\cite{zollhofer2018state,Zakharov_2019_CVPR}, human silhouettes~\cite{chan2018everybody,wang2018video,Shysheya_2019_CVPR} or gestures~\cite{hao2018Geaturegan} and required a strong prior of the animated object. For example, in face animation, method of Zollhofer \etal~\cite{zollhofer2018state} produced realistic results at expense of relying on a 3D morphable model of the face. In many applications, however, such models are not available.
Image animation can also be treated as a translation problem from one visual domain to another. For instance, Wang \etal~\cite{wang2018video} transferred human motion using the image-to-image translation framework of Isola \etal~ \cite{pix2pix2016}. Similarly, Bansal \etal~\cite{bansal2018recycle} extended conditional GANs by incorporating spatio-temporal cues in order to improve video translation between two given domains.
Such approaches in order to animate a single person require hours of videos of that person labelled with semantic information, and therefore have to be retrained for each individual. In contrast to these works, we neither rely on labels, prior information about the animated objects, nor on specific training procedures for each object instance. Furthermore, our approach can be applied to any object within the same category (\eg, faces, human bodies, robot arms etc).


Several approaches were proposed that do not require priors about the object. X2Face~\cite{wiles2018x2face} uses a dense motion field in order to generate the output video via image warping. Similarly to us they employ a reference pose that is used to obtain a canonical representation of the object. In our formulation, we do not require an explicit reference pose, leading to significantly simpler optimization and improved image quality. Siarohin~\etal~\cite{siarohin2018animating} introduced Monkey-Net, a self-supervised framework for animating arbitrary objects by using sparse keypoint trajectories. In this work, we also employ sparse trajectories induced by self-supervised keypoints. However, we model object motion in the neighbourhood of each predicted keypoint by a local affine transformation. Additionally, we explicitly model occlusions in order to indicate to the generator network the image regions that can be generated by warping the source image and the occluded areas that need to be inpainted.
 \vspace{-0.2cm}
\section{Method}
\vspace{-0.2cm}

\label{sec:overview}
We are interested in animating an object depicted in a source image  based on the motion of a similar object in a driving video . Since direct supervision is not available (pairs of videos in which objects move similarly), we follow a self-supervised strategy inspired from Monkey-Net~\cite{siarohin2018animating}. For training, we employ a large collection of video sequences containing objects of the same object category. Our model is trained to reconstruct the training videos by combining a single frame and a learned latent representation of the motion in the video. Observing frame pairs, each extracted from the same video, it learns to encode motion as a combination of motion-specific keypoint displacements and local affine transformations. At test time we apply our model to pairs composed of the source image and of each frame of the driving video and perform image animation of the source object. 



\begin{figure*}[t]\centering
\includegraphics[width=0.99\linewidth]{figures/pipeline.pdf}
\caption{Overview of our approach. Our method assumes a source image  and a frame of a driving video frame  as inputs. The unsupervised keypoint detector extracts first order motion representation consisting of sparse keypoints and local affine transformations with respect to the reference frame . The dense motion network uses the motion representation to generate dense optical flow  from  to  and occlusion map . The source image and the outputs of the dense motion network are used by the generator to render the target image.}
\label{fig:pipeline}
\vspace{-0.5cm}
\end{figure*}


An overview of our approach is presented in Fig.~\ref{fig:pipeline}. Our framework is composed of two main modules: the motion estimation module and the image generation module.
The purpose of the motion estimation module is to predict a dense motion field from a frame  of dimension  of the driving video  to the source frame . The dense motion field is later used to align the feature maps computed from  with the object pose in . The motion field is modeled by a function  that maps each pixel location in  with its corresponding location in .  is often referred to as backward optical flow. We employ backward optical flow, rather than forward optical flow, since back-warping can be implemented efficiently in a differentiable manner using bilinear sampling \cite{jaderberg2015spatial}.
We assume there exists an abstract reference frame . We independently estimate two transformations: from  to   () and from  to  (). Note that unlike X2Face~\cite{wiles2018x2face} the reference frame is an abstract concept that cancels out in our derivations later. Therefore it is never explicitly computed and cannot be visualized. This choice allows us to independently process  and . This is desired since, at test time the model receives pairs of the source image and driving frames sampled from a different video, which can be very different visually. Instead of directly predicting  and , the motion estimator module proceeds in two steps. 

In the first step, we approximate both transformations from sets of sparse trajectories, obtained by using keypoints learned in a self-supervised way. The locations of the keypoints in  and  are separately predicted by an encoder-decoder network. The keypoint representation acts as a bottleneck resulting in a compact motion representation. As shown by Siarohin \etal~\cite{siarohin2018animating}, such sparse motion representation is well-suited for animation as at test time, the keypoints of the source image can be moved using the keypoints trajectories in the driving video. 
We model motion in the neighbourhood of each keypoint using local affine transformations. Compared to using keypoint displacements only, the local affine transformations allow us to model a larger family of transformations. We use Taylor expansion to represent  by a set of keypoint locations and affine transformations. To this end, the keypoint detector network outputs keypoint locations as well as the parameters of each affine transformation. 

During the second step, a dense motion network combines the local approximations to obtain the resulting dense motion field . Furthermore, in addition to the dense motion field, this network outputs an occlusion mask  that indicates which image parts of  can be reconstructed by warping of the source image and which parts should be inpainted, \ie inferred from the context.

Finally, the generation module renders an image of the source object moving as provided in the driving video. Here, we use a generator network  that warps the source image according to  and inpaints the image parts that are occluded in the source image. In the following sections we detail each of these step and the training procedure.


\subsection{Local Affine Transformations for Approximate Motion Description}
\label{sec:locAffine}
The motion estimation module estimates the backward optical flow  from a driving frame  to the source frame . As discussed above, we propose to approximate  by its first order Taylor expansion in a neighborhood of the keypoint locations. In the rest of this section, we describe the motivation behind this choice, and detail the proposed approximation of .


We assume there exist an abstract reference frame . Therefore, estimating  consists in estimating   and . Furthermore, given a frame , we estimate each transformation  in the neighbourhood of the learned keypoints. Formally, given a transformation , we consider its first order Taylor expansions in  keypoints . Here,  denote the coordinates of the keypoints in the reference frame . Note that for the sake of simplicity in the following the point locations in the reference pose space are all denoted by  while the point locations in the ,  or  pose spaces are denoted by . We obtain:

In this formulation, the motion function  is represented by its values in each keypoint  and its Jacobians computed in each  location:

Furthermore, in order to estimate , we assume that  is locally bijective in the neighbourhood of each keypoint. We need to estimate  near the keypoint  in , given that  is the pixel location corresponding to the keypoint location  in . To do so, we first estimate the transformation  near the point  in the driving frame , \eg \ . Then we estimate the transformation  near  in the reference . Finally  is obtained as follows:

After computing again the first order Taylor expansion of Eq.~\eqref{eq:s-to-d} (see \textit{Sup. Mat.}), we obtain:

with:


In practice,  and  in Eq.~\eqref{eq:main} are predicted by the keypoint predictor.
More precisely, we employ the standard U-Net architecture that estimates  heatmaps, one for each keypoint. The last layer of the decoder uses softmax activations in
order to predict heatmaps that can be interpreted as keypoint detection confidence map. Each expected keypoint location is estimated using the average operation as in ~\cite{siarohin2018animating,robinson2019laplace}. Note if we set  ( is  identity matrix), we get the motion model of Monkey-Net. Therefore Monkey-Net uses a zeroth-order approximation of .


For both frames  and , the keypoint predictor network also outputs four additional channels for each keypoint. From these channels, we obtain the coefficients of the matrices  and  in Eq.~\eqref{eq:Ak} by computing spatial weighted average using as weights the corresponding keypoint confidence map.

\textbf{Combining Local Motions.} We employ a convolutional network  to estimate  from the set of Taylor approximations of  in the keypoints and the original source frame .
Importantly, since  maps each pixel location in  with its corresponding location in , the local patterns in , such as edges or texture, are pixel-to-pixel aligned with  but not with . This misalignment issue makes the task harder for the network to predict  from . In order to provide inputs already roughly aligned with , we warp the source frame  according to local transformations estimated in Eq.~\eqref{eq:main}. Thus, we obtain  transformed images  that are each aligned with  in the neighbourhood of a keypoint. Importantly, we also consider an additional image  for the background.

For each keypoint  we additionally compute heatmaps  indicating to the dense motion network where each transformation happens. Each  is implemented as the difference of two heatmaps centered in  and :

In all our experiments, we employ 
following Jakab \etal~\cite{jakabunsupervised}.

The heatmaps  and the transformed images  are concatenated and processed by a U-Net~\cite{ronneberger2015u}.  is  estimated using a part-based model inspired by Monkey-Net~\cite{siarohin2018animating}. We assume that an object is composed of  rigid parts and that each part is moved according to Eq.~\eqref{eq:main}. Therefore we estimate +1 masks  that indicate where each local transformation holds. The final dense motion prediction   is given by: 

Note that, the term  is considered in order to model non-moving parts such as background.


\subsection{Occlusion-aware Image Generation}
As mentioned in Sec.\ref{sec:overview}, the source image  is not pixel-to-pixel aligned with the image to be generated . In order to handle this misalignment, we use a feature warping strategy similar to~\cite{siarohin2018deformable,siarohin2018animating,grigorev2019coordinate}. More precisely, after two down-sampling convolutional blocks, we obtain a feature map  of dimension . We then warp  according to . In the presence of occlusions in , optical flow may not be sufficient to generate . Indeed, the occluded parts in  cannot be recovered by image-warping and thus should be inpainted.
Consequently, we introduce an occlusion map  to mask out the feature map regions that should be inpainted. Thus, the occlusion mask diminishes the impact of the features corresponding to the occluded parts. The transformed feature map is written as:

where  denotes the back-warping operation and  denotes the Hadamard product. We estimate the occlusion mask from our sparse keypoint representation, by adding a channel to the final layer of the dense motion network. Finally, the transformed feature map  is fed to subsequent network layers of the generation module (see \emph{Sup. Mat.}) to render the sought image.
\subsection{Training Losses}

We train our system in an end-to-end fashion combining several losses. First, we use the reconstruction loss based on the perceptual loss of Johnson \etal~\cite{johnson2016perceptual} using the pre-trained VGG-19 network as our main driving loss. The loss is based on implementation of Wang \etal ~\cite{wang2018video}. 
With the input driving frame  and the corresponding reconstructed frame , the reconstruction loss is written as:

where  is the  channel feature extracted from a specific VGG-19 layer and  is the number of feature channels in this layer. Additionally we propose to use this loss on a number of resolutions, forming a pyramid obtained by down-sampling  and , similarly to MS-SSIM~\cite{wang2003multiscale, tang2018dual}. The resolutions are , ,  and . There are 20 loss terms in total.


\textbf{Imposing Equivariance Constraint.} Our keypoint predictor does not require any keypoint annotations during training. This may lead to unstable performance. Equivariance constraint is one of the most important factors driving the discovery of unsupervised keypoints \cite{jakabunsupervised,Zhang_2018_CVPR}. It forces the model to predict consistent keypoints with respect to known geometric transformations. We use thin plate splines deformations as they were previously used in unsupervised keypoint detection \cite{jakabunsupervised,Zhang_2018_CVPR} and are similar to natural image deformations. Since our motion estimator does not only predict the keypoints, but also the Jacobians, we extend the well-known equivariance loss to additionally include constraints on the Jacobians. 

We assume that an image  undergoes a known spatial deformation . In this case  can be an affine transformation or a thin plane spline deformation. After this deformation we obtain a new image . Now by applying our extended motion estimator to both images, we obtain a set of local approximations for  and . The standard equivariance constraint writes as: 

After computing the first order Taylor expansions of both sides, we obtain the following constraints (see derivation details in \textit{Sup. Mat.}):


Note that the constraint Eq.~\eqref{eq:equi-value} is strictly the same as the standard equivariance constraint for the keypoints~\cite{jakabunsupervised,Zhang_2018_CVPR}. During training, we constrain every keypoint location using a simple  loss between the two sides of Eq.~\eqref{eq:equi-value}. However, implementing the second constraint from Eq.~\eqref{eq:equi-jac} with  would force the magnitude of the Jacobians to zero and would lead to numerical problems. To this end, we reformulate this constraint in the following way:

where  is  identity matrix. Then,  loss is employed similarly to the keypoint location constraint. Finally, in our preliminary experiments, we observed that our model shows low sensitivity to the relative weights of the reconstruction and the two equivariance losses. Therefore, we use equal loss weights in all our experiments.



\subsection{Testing Stage: Relative Motion Transfer}
\label{sec:test}
  At this stage our goal is to animate an object in a source frame  using the driving video . Each frame  is independently processed to obtain . Rather than transferring the motion encoded in  to , we transfer the relative motion between  and  to . In other words, we apply a transformation  to the neighbourhood of each keypoint : 

with

Detailed mathematical derivations are provided in \textit{Sup. Mat.}. Intuitively, we transform the neighbourhood of each keypoint  in  according to its local deformation in the driving video. Indeed, transferring relative motion over absolute coordinates allows to transfer only relevant motion patterns, while preserving global object geometry. Conversely, when transferring absolute coordinates, as in X2Face~\cite{wiles2018x2face}, the generated frame inherits the object proportions of the driving video. It's important to note that one limitation of transferring relative motion is that we need to assume that the objects in  and  have similar poses (see ~\cite{siarohin2018animating}). Without initial rough alignment, Eq.~\eqref{eq:main-test} may lead to absolute keypoint locations physically impossible for the object of interest.

 \vspace{-0.2cm}





 



\section{Experiments}
\vspace{-0.2cm}
\label{Experiments}
\noindent
\textbf{Datasets.} 
We train and test our method on four different datasets containing various objects. Our model is capable of rendering videos of much higher resolution compared to~\cite{siarohin2018animating} in all our experiments. 
\begin{itemize}[noitemsep,topsep=0pt,wide=0pt]
\item The \emph{VoxCeleb} dataset \cite{Nagrani17} is a face dataset of 22496 videos, extracted from YouTube videos. For pre-processing, we extract an initial bounding box in the first video frame. We track this face until it is too far away from the initial position. Then, we crop the video frames using the smallest crop containing all the bounding boxes. The process is repeated until the end of the sequence. We filter out sequences that have resolution lower than  and the remaining videos are resized to  preserving the aspect ratio. It's important to note that compared to X2Face~\cite{wiles2018x2face}, we obtain more natural videos where faces move freely within the bounding box. Overall, we obtain 19522 training videos and 525 test videos, with lengths varying from 64 to 1024 frames.

\begin{figure}[t]
\begin{minipage}{\textwidth}
 \begin{minipage}[t]{.45\linewidth}
  \centering
\centering
    \vspace{-2.5cm}
    \captionof{table}{Quantitative ablation study for video reconstruction on \emph{Tai-Chi-HD}.        \label{tab:abla}
}
    \resizebox{0.90\textwidth}{!}{
      \begin{tabular}{c|ccc}
        \toprule
      & \multicolumn{3}{c}{\emph{Tai-Chi-HD}}  \\
      &   & ({AKD}, {MKR}) & {AED} \\ \toprule
        \emph{Baseline} & 0.073 & (8.945, 0.099) &  0.235 \\
        \emph{Pyr.} & 0.069 & (9.407, 0.065) & 0.213 \\
\emph{Pyr.}+  & 0.069 & (8.773, 0.050) & 0.205 \\
        \emph{Jac. w/o Eq.~\eqref{eq:equi-jac}} & 0.073 & (9.887, 0.052) & 0.220 \\
        \emph{Full}& \bf 0.063 & (\bf 6.862, \bf 0.036)  & \bf 0.179 \\
        \bottomrule

    \end{tabular}
    }
    \vspace{0.5cm}
\captionof{table}{Paired user study: user preferences in favour of our approach.} 
    \resizebox{0.90\textwidth}{!}{
\begin{tabular}{c|cc}
\toprule
      & X2Face ~\cite{wiles2018x2face} & Monkey-Net ~\cite{siarohin2018animating} \\
\midrule
        \emph{Tai-Chi-HD}& 92.0\% & 80.6\%  \\
        \emph{VoxCeleb} & 95.8\% & 68.4\%\\
         \emph{Nemo}& 79.8\% & 60.6\%  \\
        \emph{Bair}& 95.0\% & 67.0\%\\
        \bottomrule
    \end{tabular}

    }
           \label{tab:user}

\end{minipage} \hfill
 \begin{minipage}[t]{.55\linewidth}
\centering
  \setlength\tabcolsep{0.5pt}
\resizebox{0.90\columnwidth}{!}{\begin{tabular}{>{\centering\arraybackslash}m{1.8cm}ccccc}
Input   & \parbox{1.3cm}{\includegraphics[width=0.17\columnwidth]{figures/ablations/driving_0000}}&
\parbox{1.3cm}{\includegraphics[width=0.17\columnwidth]{figures/ablations/driving_0087}}&
\parbox{1.3cm}{\includegraphics[width=0.17\columnwidth]{figures/ablations/driving_0197}}&
\parbox{1.3cm}{\includegraphics[width=0.17\columnwidth]{figures/ablations/driving_0282}}&
\parbox{1.3cm}{\includegraphics[width=0.17\columnwidth]{figures/ablations/driving_0325}}
\\
\emph{Baseline} & \parbox{1.3cm}{\includegraphics[width=0.17\columnwidth]{figures/ablations/base_0000}}&
\parbox{1.3cm}{\includegraphics[width=0.17\columnwidth]{figures/ablations/base_0087}}&
\parbox{1.3cm}{\includegraphics[width=0.17\columnwidth]{figures/ablations/base_0197}}&
\parbox{1.3cm}{\includegraphics[width=0.17\columnwidth]{figures/ablations/base_0282}}&
\parbox{1.3cm}{\includegraphics[width=0.17\columnwidth]{figures/ablations/base_0325}}
\\
\emph{Pyr.}  & \parbox{1.3cm}{\includegraphics[width=0.17\columnwidth]{figures/ablations/base-pyramide_0000}}&
\parbox{1.3cm}{\includegraphics[width=0.17\columnwidth]{figures/ablations/base-pyramide_0087}}&
\parbox{1.3cm}{\includegraphics[width=0.17\columnwidth]{figures/ablations/base-pyramide_0197}}&
\parbox{1.3cm}{\includegraphics[width=0.17\columnwidth]{figures/ablations/base-pyramide_0282}}&
\parbox{1.3cm}{\includegraphics[width=0.17\columnwidth]{figures/ablations/base-pyramide_0325}}
\\
\emph{Pyr.}+  & \parbox{1.3cm}{\includegraphics[width=0.17\columnwidth]{figures/ablations/no-jac_0000}}&
\parbox{1.3cm}{\includegraphics[width=0.17\columnwidth]{figures/ablations/no-jac_0087}}&
\parbox{1.3cm}{\includegraphics[width=0.17\columnwidth]{figures/ablations/no-jac_0197}}&
\parbox{1.3cm}{\includegraphics[width=0.17\columnwidth]{figures/ablations/no-jac_0282}}&
\parbox{1.3cm}{\includegraphics[width=0.17\columnwidth]{figures/ablations/no-jac_0325}}
\\
\emph{Jac. w/o Eq.~\eqref{eq:equi-jac}}  & \parbox{1.3cm}{\includegraphics[width=0.17\columnwidth]{figures/ablations/jac-no-eq_0000}}&
\parbox{1.3cm}{\includegraphics[width=0.17\columnwidth]{figures/ablations/jac-no-eq_0087}}&
\parbox{1.3cm}{\includegraphics[width=0.17\columnwidth]{figures/ablations/jac-no-eq_0197}}&
\parbox{1.3cm}{\includegraphics[width=0.17\columnwidth]{figures/ablations/jac-no-eq_0282}}&
\parbox{1.3cm}{\includegraphics[width=0.17\columnwidth]{figures/ablations/jac-no-eq_0325}}
\\
\emph{Full}& \parbox{1.3cm}{\includegraphics[width=0.17\columnwidth]{figures/ablations/jac_0000}}&
\parbox{1.3cm}{\includegraphics[width=0.17\columnwidth]{figures/ablations/jac_0087}}&
\parbox{1.3cm}{\includegraphics[width=0.17\columnwidth]{figures/ablations/jac_0197}}&
\parbox{1.3cm}{\includegraphics[width=0.17\columnwidth]{figures/ablations/jac_0282}}&
\parbox{1.3cm}{\includegraphics[width=0.17\columnwidth]{figures/ablations/jac_0325}}
\end{tabular}}
\captionof{figure}{Qualitative ablation on \emph{Tai-Chi-HD}.}
\label{fig:ablations}
 \end{minipage}
\end{minipage}
\vspace{-0.5cm}
\end{figure}

\item The UvA-\emph{Nemo} dataset \cite{dibekliouglu2012you} is a facial analysis dataset that consists of 1240 videos.
We apply the exact same pre-processing as for \emph{VoxCeleb}. Each video starts with a neutral expression. Similar to Wang \etal~\cite{wang2018every}, we use 1116 videos for training and 124 for evaluation. 

\item The \emph{BAIR} robot pushing dataset \cite{ebert2017self} contains videos collected by a Sawyer robotic arm pushing diverse objects over a table. It consists of 42880 training and 128 test videos. Each video is 30 frame long and has a   resolution. 


\item Following Tulyakov \etal~\cite{tulyakov2017mocogan}, we collected 280 tai-chi videos from YouTube. We use 252 videos for training and 28 for testing. Each video is split in short clips as described in pre-processing of \emph{VoxCeleb} dataset. We retain only high quality videos and resized all the clips to  pixels (instead of  pixels in \cite{tulyakov2017mocogan}). Finally, we obtain 3049 and 285 video chunks for training and testing respectively with video length varying from 128 to 1024 frames. This dataset is referred to as the \emph{Tai-Chi-HD} dataset. The dataset will be made publicly available.
\end{itemize}


\noindent
\textbf{Evaluation Protocol.}\label{sec:pb}
Evaluating the quality of image animation is not obvious, since ground truth animations are not available. We follow the evaluation protocol of Monkey-Net~\cite{siarohin2018animating}. First, we quantitatively evaluate each method on the "proxy" task of video reconstruction. This task consists of reconstructing the input video from a representation in which appearance and motion are decoupled. In our case, we reconstruct the input video by combining the sparse motion representation in \eqref{eq:set} of each frame and the first video frame.
Second, we evaluate our model on image animation according to a user-study. In all experiments we use =10 as in \cite{siarohin2018animating}. Other implementation details are given in \emph{Sup. Mat.} 

\textbf{Metrics.}
 To evaluate video reconstruction, we adopt the metrics proposed in Monkey-Net~\cite{siarohin2018animating}:
\begin{itemize}[noitemsep,topsep=0pt,wide=0pt]
 \item . We report the average  distance between the generated and the ground-truth videos.
 \item \emph{Average Keypoint Distance (AKD)}. For the  \emph{Tai-Chi-HD},  \emph{VoxCeleb} and \emph{Nemo} datasets, we use 3rd-party pre-trained keypoint detectors in order to evaluate whether the motion of the input video is preserved. For the \emph{VoxCeleb} and \emph{Nemo} datasets we use the facial landmark detector of Bulat \etal~\cite{Bulat_2017_ICCV}. For the \emph{Tai-Chi-HD} dataset, we employ the human-pose estimator of Cao \etal~\cite{cao2017realtime}. 
These keypoints are independently computed for each frame. AKD is obtained by computing the average distance between the detected keypoints of the ground truth and of the generated video. 
\item \emph{ Missing Keypoint Rate (MKR)}. In the case of \emph{Tai-Chi-HD}, the human-pose estimator returns an additional binary label for each keypoint indicating whether or not the keypoints were successfully detected. Therefore, we also report the MKR defined as the percentage of keypoints that are detected in the ground truth frame but not in the generated one. This metric assesses the appearance quality of each generated frame. 
\item \emph{Average Euclidean Distance (AED)}. Considering an externally trained image representation, we report the average euclidean distance between the ground truth and generated frame representation, similarly to Esser \etal~\cite{esser2018variational}. We employ the feature embedding used in Monkey-Net~\cite{siarohin2018animating}.
\end{itemize}



 \textbf{Ablation Study.}
We compare the following variants of our model. \emph{Baseline}: the simplest model trained without using the occlusion mask (=1 in Eq.~\eqref{eq:wrap}),  jacobians ( in Eq.~\eqref{eq:main}) and is supervised with  at the highest resolution only; \emph{Pyr.}: the pyramid loss is added to \emph{Baseline}; \emph{Pyr.}+: with respect to \emph{Pyr.}, we replace the generator network with the occlusion-aware network; \emph{Jac. w/o Eq.~\eqref{eq:equi-jac}} our model with local affine transformations but without equivariance constraints on jacobians Eq.~\eqref{eq:equi-jac}; \emph{Full}: the full model including local affine transformations described in Sec.~\ref{sec:locAffine}.
 
In Fig.~\ref{fig:ablations}, we report the qualitative ablation. First, the pyramid loss leads to better results according to all the metrics except \emph{AKD}. Second, adding  to the model consistently improves all the metrics with respect to \emph{Pyr.}. This illustrates the benefit of explicitly modeling occlusions. We found that without equivariance constraint over the jacobians,  becomes unstable which leads to poor motion estimations. Finally, our \emph{Full} model further improves all the metrics. In particular, we note that, with respect to the \emph{Baseline} model, the MKR of the full model is smaller by the factor of 2.75. It shows that our rich motion representation helps generate more realistic images. These results are confirmed by our qualitative evaluation in Tab.~\ref{tab:abla} where we compare the \emph{Baseline} and the \emph{Full} models. In these experiments, each frame  of the input video is reconstructed from its first frame (first column) and the estimated keypoint trajectories. We note that the \emph{Baseline} model does not locate any keypoints in the arms area. Consequently, when the pose difference with the initial pose increases, the model cannot reconstruct the video (columns 3,4 and 5). In contrast, the \emph{Full} model learns to detect a keypoint on each arm, and therefore, to more accurately reconstruct the input video even in the case of complex motion. 









\textbf{Comparison with State of the Art.}
We now compare our method with state of the art for the video reconstruction task as in \cite{siarohin2018animating}. To the best of our knowledge, X2Face \cite{wiles2018x2face} and Monkey-Net \cite{siarohin2018animating} are the only previous approaches for model-free image animation. Quantitative results are reported in Tab.~\ref{tab:sota}. We observe that our approach consistently improves every single metric for each of the four different datasets. Even on the two face datasets, \emph{VoxCeleb} and \emph{Nemo} datasets, our approach clearly outperforms X2Face that was originally proposed for face generation. The better performance of our approach compared to X2Face is especially impressive X2Face exploits a larger motion embedding (128 floats) than our approach (60=K*(2+4) floats). Compared to Monkey-Net that uses a motion representation with a similar dimension (50=K*(2+3)), the advantages of our approach are clearly visible on the \emph{Tai-Chi-HD} dataset that contains highly non-rigid objects (\ie human body).

 \begin{table}[t]
    \centering
    \caption{Video reconstruction: comparison with the state of the art on four different datasets.}
    \resizebox{0.95\textwidth}{!}{
    \begin{tabular}{c|ccc|ccc|ccc|c}
    \toprule
      & \multicolumn{3}{c|}{\emph{Tai-Chi-HD}} & \multicolumn{3}{c|}{\emph{VoxCeleb}} & \multicolumn{3}{c|}{\emph{Nemo}} & \multicolumn{1}{c}{\emph{Bair}} \\
      &   & ({AKD}, {MKR}) & {AED} &  & {AKD} & {AED} &  & {AKD} & {AED} &   \\  \toprule
        X2Face ~\cite{wiles2018x2face}& 0.080 & (17.654, 0.109) & 0.272 & 0.078 & 7.687 & 0.405 & 0.031 & 3.539 & 0.221 & 0.065  \\
        Monkey-Net ~\cite{siarohin2018animating} & 0.077 & (10.798, 0.059) & 0.228 & 0.049 & 1.878 & 0.199 & 0.018 & 1.285 & 0.077 & 0.034 \\
        Ours & \bf0.063 & (\bf6.862, \bf0.036) & \bf0.179  & \bf0.043 & \bf1.294 & \bf0.140 & \bf0.016 & \bf1.119 & \bf0.048 & \bf0.027\\
        \bottomrule
    \end{tabular}
    }
    \label{tab:sota}
 \end{table}
We now report a qualitative comparison for image animation. Generated sequences are reported in Fig.~\ref{fig:taichi-transfer-main}. The results are well in line with the quantitative evaluation in Tab.~\ref{tab:sota}. Indeed, in both examples, X2Face and Monkey-Net are not able to correctly transfer the body notion in the driving video, instead warping the human body in the source image as a blob. Conversely, our approach is able to generate significantly better looking videos in which each body part is independently animated. This qualitative evaluation illustrates the potential of our rich motion description. 
We complete our evaluation with a user study. We ask users to select the most realistic image animation. Each question consists of the source image, the driving video, and the corresponding results of our method and a competitive method. We require each question to be answered by 10 AMT worker. This evaluation is repeated on 50 different input pairs. Results are reported in Tab.~\ref{tab:user}. We observe that our method is clearly preferred over the competitor methods. Interestingly, the largest difference with the state of the art is obtained on \emph{Tai-Chi-HD}: the most challenging dataset in our evaluation due to its rich motions.


\begin{figure*}[t]
  \centering
\resizebox{0.99\linewidth}{!}{\begin{tabular}{>{\centering\arraybackslash}m{2.5cm}ccccccc}
&  \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/tablecap.pdf}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/005vid001}}&
  \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/086vid001}}&
\parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/190vid001}}&
\parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/273vid001}}&
\parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/339vid001}}
\\
\Large X2Face~\cite{wiles2018x2face} & \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/001app001}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/005x2face001}}&
  \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/086x2face001}}&
\parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/190x2face001}}&
\parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/273x2face001}}&
\parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/339x2face001}}
\\
\Large Monkey-Net~\cite{siarohin2018animating} & \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/001app001}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/005monkey001}}&
  \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/086monkey001}}&
\parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/190monkey001}}&
\parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/273monkey001}}&
\parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/339monkey001}}
\\
\Large Ours&  \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/001app001}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/005jac001}}&
  \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/086jac001}}&
\parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/190jac001}}&
\parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/273jac001}}&
\parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/339jac001}}
\end{tabular}\hspace{0.2cm}
\begin{tabular}{ccccccc}
  \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/tablecap.pdf}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/005vid003}}&
  \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/018vid003}}&
\parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/065vid003}}&
\parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/113vid003}}&
\parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/167vid003}}
\\
  \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/001app003}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/005x2face003}}&
  \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/018x2face003}}&
\parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/065x2face003}}&
\parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/113x2face003}}&
\parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/167x2face003}}
\\
  \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/001app003}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/005monkey003}}&
  \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/018monkey003}}&
\parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/065monkey003}}&
\parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/113monkey003}}&
\parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/167monkey003}}
\\
  \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/001app003}}&
 \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/005jac003}}&
  \parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/018jac003}}&
\parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/065jac003}}&
\parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/113jac003}}&
\parbox{1.7cm}{\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer/167jac003}}
\end{tabular}
}





\caption{Qualitative comparison with state of the art for the task of image animation on two sequences and two source images from the \emph{Tai-Chi-HD} dataset.}
\label{fig:taichi-transfer-main}
\vspace{-0.5cm}
\end{figure*}
 
\vspace{-0.2cm}
\section{Conclusions}
\vspace{-0.2cm}
We presented a novel approach for image animation based on keypoints and local affine transformations. Our novel mathematical formulation describes the motion field between two frames and is efficiently computed by deriving a first order Taylor expansion approximation. In this way, motion is described as a set of keypoints displacements and local affine transformations. A generator network combines the appearance of the source image and the motion representation of the driving video. In addition, we proposed to explicitly model occlusions in order to indicate to the generator network which image parts should be inpainted. We evaluated the proposed method both quantitatively and qualitatively and showed that our approach clearly outperforms state of the art on all the benchmarks.

\clearpage
\begin{thebibliography}{10}
	\providecommand{\url}[1]{\texttt{#1}}
	\providecommand{\urlprefix}{URL }
	\providecommand{\doi}[1]{https://doi.org/#1}
	\bibitem{babaeizadeh2017stochastic}
	Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan, Roy~H Campbell, and Sergey
	Levine.
	\newblock Stochastic variational video prediction.
	\newblock In {\em ICLR}, 2017.
	
	\bibitem{balakrishnansynthesizing}
	Guha Balakrishnan, Amy Zhao, Adrian~V Dalca, Fredo Durand, and John Guttag.
	\newblock Synthesizing images of humans in unseen poses.
	\newblock In {\em CVPR}, 2018.
	
	\bibitem{bansal2018recycle}
	Aayush Bansal, Shugao Ma, Deva Ramanan, and Yaser Sheikh.
	\newblock Recycle-gan: Unsupervised video retargeting.
	\newblock In {\em ECCV}, 2018.
	
	\bibitem{blanz1999morphable}
	Volker Blanz and Thomas Vetter.
	\newblock A morphable model for the synthesis of 3d faces.
	\newblock In {\em SIGGRAPH}, 1999.
	
	\bibitem{Bulat_2017_ICCV}
	Adrian Bulat and Georgios Tzimiropoulos.
	\newblock How far are we from solving the 2d \& 3d face alignment problem? (and
	a dataset of 230,000 3d facial landmarks).
	\newblock In {\em ICCV}, 2017.
	
	\bibitem{cao2014displaced}
	Chen Cao, Qiming Hou, and Kun Zhou.
	\newblock Displaced dynamic expression regression for real-time facial tracking
	and animation.
	\newblock {\em TOG}, 2014.
	
	\bibitem{cao2017realtime}
	Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh.
	\newblock Realtime multi-person 2d pose estimation using part affinity fields.
	\newblock In {\em CVPR}, 2017.
	
	\bibitem{chan2018everybody}
	Caroline Chan, Shiry Ginosar, Tinghui Zhou, and Alexei~A Efros.
	\newblock Everybody dance now.
	\newblock In {\em ECCV}, 2018.
	
	\bibitem{dibekliouglu2012you}
	Hamdi Dibeklio{\u{g}}lu, Albert~Ali Salah, and Theo Gevers.
	\newblock Are you really smiling at me? spontaneous versus posed enjoyment
	smiles.
	\newblock In {\em ECCV}, 2012.
	
	\bibitem{ebert2017self}
	Frederik Ebert, Chelsea Finn, Alex~X Lee, and Sergey Levine.
	\newblock Self-supervised visual planning with temporal skip connections.
	\newblock In {\em CoRL}, 2017.
	
	\bibitem{esser2018variational}
	Patrick Esser, Ekaterina Sutter, and Bj{\"o}rn Ommer.
	\newblock A variational u-net for conditional appearance and shape generation.
	\newblock In {\em CVPR}, 2018.
	
	\bibitem{finn2016unsupervised}
	Chelsea Finn, Ian Goodfellow, and Sergey Levine.
	\newblock Unsupervised learning for physical interaction through video
	prediction.
	\newblock In {\em NIPS}, 2016.
	
	\bibitem{geng20193d}
	Zhenglin Geng, Chen Cao, and Sergey Tulyakov.
	\newblock 3d guided fine-grained face manipulation.
	\newblock In {\em CVPR}, 2019.
	
	\bibitem{goodfellow2014generative}
	Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
	Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
	\newblock Generative adversarial nets.
	\newblock In {\em NIPS}, 2014.
	
	\bibitem{grigorev2019coordinate}
	Artur Grigorev, Artem Sevastopolsky, Alexander Vakhitov, and Victor Lempitsky.
	\newblock Coordinate-based texture inpainting for pose-guided image generation.
	\newblock In {\em CVPR}, 2019.
	
	\bibitem{pix2pix2016}
	Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei~A Efros.
	\newblock Image-to-image translation with conditional adversarial networks.
	\newblock In {\em CVPR}, 2017.
	
	\bibitem{jaderberg2015spatial}
	Max Jaderberg, Karen Simonyan, Andrew Zisserman, et~al.
	\newblock Spatial transformer networks.
	\newblock In {\em NIPS}, 2015.
	
	\bibitem{jakabunsupervised}
	Tomas Jakab, Ankush Gupta, Hakan Bilen, and Andrea Vedaldi.
	\newblock Unsupervised learning of object landmarks through conditional image
	generation.
	\newblock In {\em NIPS}, 2018.
	
	\bibitem{johnson2016perceptual}
	Justin Johnson, Alexandre Alahi, and Li~Fei-Fei.
	\newblock Perceptual losses for real-time style transfer and super-resolution.
	\newblock In {\em ECCV}, 2016.
	
	\bibitem{kingma2014adam}
	Diederik~P Kingma and Jimmy Ba.
	\newblock Adam: A method for stochastic optimization.
	\newblock In {\em ICLR}, 2014.
	
	\bibitem{kingma2013auto}
	Diederik~P Kingma and Max Welling.
	\newblock Auto-encoding variational bayes.
	\newblock In {\em ICLR}, 2014.
	
	\bibitem{liu2019gesture}
	Yahui Liu, Marco De~Nadai, Gloria Zen, Nicu Sebe, and Bruno Lepri.
	\newblock Gesture-to-gesture translation in the wild via category-independent
	conditional maps.
	\newblock {\em ACM MM}, 2019.
	
	\bibitem{Nagrani17}
	A.~Nagrani, J.~S. Chung, and A.~Zisserman.
	\newblock Voxceleb: a large-scale speaker identification dataset.
	\newblock In {\em INTERSPEECH}, 2017.
	
	\bibitem{oh2015action}
	Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard~L Lewis, and Satinder Singh.
	\newblock Action-conditional video prediction using deep networks in atari
	games.
	\newblock In {\em NIPS}, 2015.
	
	\bibitem{robinson2019laplace}
	Joseph~P Robinson, Yuncheng Li, Ning Zhang, Yun Fu, and Sergey Tulyakov.
	\newblock Laplace landmark localization.
	\newblock In {\em ICCV}, 2019.
	
	\bibitem{ronneberger2015u}
	Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
	\newblock U-net: Convolutional networks for biomedical image segmentation.
	\newblock In {\em MICCAI}, 2015.
	
	\bibitem{saito2017temporal}
	Masaki Saito, Eiichi Matsumoto, and Shunta Saito.
	\newblock Temporal generative adversarial nets with singular value clipping.
	\newblock In {\em ICCV}, 2017.
	
	\bibitem{Shysheya_2019_CVPR}
	Aliaksandra Shysheya, Egor Zakharov, Kara-Ali Aliev, Renat Bashirov, Egor
	Burkov, Karim Iskakov, Aleksei Ivakhnenko, Yury Malkov, Igor Pasechnik,
	Dmitry Ulyanov, Alexander Vakhitov, and Victor Lempitsky.
	\newblock Textured neural avatars.
	\newblock In {\em CVPR}, June 2019.
	
	\bibitem{siarohin2018animating}
	Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci, and
	Nicu Sebe.
	\newblock Animating arbitrary objects via deep motion transfer.
	\newblock In {\em CVPR}, 2019.
	
	\bibitem{siarohin2018deformable}
	Aliaksandr Siarohin, Enver Sangineto, St{\'e}phane Lathuili{\`e}re, and Nicu
	Sebe.
	\newblock Deformable gans for pose-based human image generation.
	\newblock In {\em CVPR}, 2018.
	
	\bibitem{srivastava2015unsupervised}
	Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov.
	\newblock Unsupervised learning of video representations using lstms.
	\newblock In {\em ICML}, 2015.
	
	\bibitem{hao2018Geaturegan}
	Hao Tang, Wei Wang, Dan Xu, Yan Yan, and Nicu Sebe.
	\newblock Gesturegan for hand gesture-to-gesture translation in the wild.
	\newblock In {\em ACM MM}, 2018.
	
	\bibitem{tang2018dual}
	Hao Tang, Dan Xu, Wei Wang, Yan Yan, and Nicu Sebe.
	\newblock Dual generator generative adversarial networks for multi-domain
	image-to-image translation.
	\newblock In {\em ACCV}, 2018.
	
	\bibitem{thies2016face2face}
	Justus Thies, Michael Zollhofer, Marc Stamminger, Christian Theobalt, and
	Matthias Nie{\ss}ner.
	\newblock Face2face: Real-time face capture and reenactment of rgb videos.
	\newblock In {\em CVPR}, 2016.
	
	\bibitem{tulyakov2017mocogan}
	Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz.
	\newblock Mocogan: Decomposing motion and content for video generation.
	\newblock In {\em CVPR}, 2018.
	
	\bibitem{van2017transformation}
	Joost Van~Amersfoort, Anitha Kannan, Marc'Aurelio Ranzato, Arthur Szlam,
	Du~Tran, and Soumith Chintala.
	\newblock Transformation-based models of video sequences.
	\newblock {\em arXiv preprint arXiv:1701.08435}, 2017.
	
	\bibitem{vondrick2016generating}
	Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba.
	\newblock Generating videos with scene dynamics.
	\newblock In {\em NIPS}, 2016.
	
	\bibitem{wang2018video}
	Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz,
	and Bryan Catanzaro.
	\newblock Video-to-video synthesis.
	\newblock In {\em NIPS}, 2018.
	
	\bibitem{wang2018every}
	Wei Wang, Xavier Alameda-Pineda, Dan Xu, Pascal Fua, Elisa Ricci, and Nicu
	Sebe.
	\newblock Every smile is unique: Landmark-guided diverse smile generation.
	\newblock In {\em CVPR}, 2018.
	
	\bibitem{wang2003multiscale}
	Zhou Wang, Eero~P Simoncelli, and Alan~C Bovik.
	\newblock Multiscale structural similarity for image quality assessment.
	\newblock In {\em ACSSC}, 2003.
	
	\bibitem{wiles2018x2face}
	Olivia Wiles, A~Sophia Koepke, and Andrew Zisserman.
	\newblock X2face: A network for controlling face generation using images,
	audio, and pose codes.
	\newblock In {\em ECCV}, 2018.
	
	\bibitem{zablotskaia2019dwnet}
	Polina Zablotskaia, Aliaksandr Siarohin, Bo~Zhao, and Leonid Sigal.
	\newblock Dwnet: Dense warp-based network for pose-guided human video
	generation.
	\newblock In {\em BMVC}, 2019.
	
	\bibitem{Zakharov_2019_CVPR}
	Egor Zakharov, Aliaksandra Shysheya, Egor Burkov, and Victor Lempitsky.
	\newblock Few-shot adversarial learning of realistic neural talking head
	models.
	\newblock In {\em ICCV}, 2019.
	
	\bibitem{Zhang_2018_CVPR}
	Yuting Zhang, Yijie Guo, Yixin Jin, Yijun Luo, Zhiyuan He, and Honglak Lee.
	\newblock Unsupervised discovery of object landmarks as structural
	representations.
	\newblock In {\em CVPR}, 2018.
	
	\bibitem{zhao2018learning}
	Long Zhao, Xi~Peng, Yu~Tian, Mubbasir Kapadia, and Dimitris Metaxas.
	\newblock Learning to forecast and refine residual motion for image-to-video
	generation.
	\newblock In {\em ECCV}, 2018.
	
	\bibitem{zollhofer2018state}
	Michael Zollh{\"o}fer, Justus Thies, Pablo Garrido, Derek Bradley, Thabo
	Beeler, Patrick P{\'e}rez, Marc Stamminger, Matthias Nie{\ss}ner, and
	Christian Theobalt.
	\newblock State of the art on monocular 3d face reconstruction, tracking, and
	applications.
	\newblock In {\em Computer Graphics Forum}, 2018.
	
\end{thebibliography}
\clearpage
\renewcommand{\thesection}{\Alph{section}}
\setcounter{section}{0}
\section{Detailed Derivations}
\label{sec:deri}
\subsection{Approximating Motion with Local Affine Transformations}
Here, we detail the derivation leading to the approximation of   near the keypoint  in Eq.~\eqref{eq:main}. 
Using first order Taylor expansion we can obtain:


 can be written as the composition of two transformations:


In order to compute the zeroth order term,  we estimate the transformation  near the point  in the driving frame , e.g . Then we can estimate the transformation  near  in the reference . 
Since  and , we can write . Consequently, we obtain:



Concerning the first order term, we apply the function composition rule in Eq.~\eqref{eq-sup:s-to-d} and obtain:


Since the matrix inverse of the Jacobian is equal to the Jacobian of the inverse function, and since , Eq.~\eqref{eq:der} can be rewritten:


After injecting Eqs.~\eqref{eq:value} and \eqref{eq:jacobian} into ~\eqref{eq-sup:taylor}, we finally obtain:


\subsection{Equivariance Loss}
At training time, we use equivariance constraints that enforces:


After applying first order Taylor expansion on the left-hand side, we obtain:

 After applying first order Taylor expansion on the right-hand side in Eq.~\eqref{eq:equi}, we obtain:

We can further simplify this expression using derivative of function composition:

Eq.~\eqref{eq:equi} holds only when every coefficient in Taylor expansion of the right and left sides are equal. Thus, it leads us to the following constaints:


and



\subsection{Transferring Relative Motion}
In order to transfer only relative motion patterns, we propose to estimate  near the keypoint  by shifting the motion in the driving video to the location of keypoint  in the source. To this aim, we introduce  that is the 2D vector from the landmark position  in  to its position in . We proceed as follows. First, we shift point coordinates according to  in order to obtain coordinates in . Second, we apply the transformation . Finally, we translate the points back in the original coordinate space using . Formally, it can be written:




Now, we can compute the value and Jacobian in the :

and:

Now using Eq.~\eqref{eq:primitive} and treating  as source and  as driving frame, we obtain:


with

Note that, here,   canceled out.


\section{Implementation details}
\label{sec:details}
\subsection{Architecture details}
In order to reduce memory and computational requirements of our model, the keypoint detector and dense motion predictor both work on resolution of  (instead of ). For the two networks of the motion  module, we employ an architecture based on U-Net~\cite{ronneberger2015u} with five  -  -  -  blocks in the encoders and five  -  -  -  blocks in the decoders. In the generator network, we use the Johnson architecture~\cite{johnson2016perceptual} with two down-sampling blocks, six residual-blocks and two up-sampling blocks. We train our network using Adam~\cite{kingma2014adam} optimizer with learning rate  and batch size 20. We employ learning decay by dropping the learning rate at  and  iterations, where T is total number of iteration. We chose  for \emph{Tai-Chi-HD} and \emph{VoxCeleb}, and  for \emph{Nemo} and \emph{Bair}. The model converges in approximately 2 days using 2 TitanX gpus for \emph{Tai-Chi-HD} and \emph{VoxCeleb}.

\subsection{Equivariance loss implementation}
As explained above our equivariance losses force the keypoint detector to be equivariant to some transformations . In our experiments  is implemented using randomly sampled thin plate splines. We sample spline parameters from normal distributions with zero mean and variance equal to 0.005 for deformation component and 0.05 for the affine component. For deformation component we use uniform  grid.

\section{Additional experiments}
\label{sec:expadd}
\subsection{Image Animation}
In this section, we report additional qualitative results.  

We compare our approach with  X2face~\cite{wiles2018x2face} and Monkey-Net~\cite{siarohin2018animating}. In Fig.~\ref{fig:vox-transfer-main}, we show three animation examples from the \emph{VoxCeleb} dataset. First, X2face is not capable of generating realistic video sequences as we can see, for instance in the last frame of the last sequence. Then, Monkey-Net generates realistic frames but fails to generate specific facial expressions as in the third frame of the first sequence or in transferring the eye movements as in the last two frames of the second sequence.

In Fig.~\ref{fig:nemo-transfer-main}, we show three animation examples from the \emph{Nemo} dataset. First, we observe that this dataset is simpler than \emph{VoxCeleb} since the persons are facing a uniformly black background. With this simpler dataset, X2Face generates realistic videos. However, it is not capable of inpainting image parts that are not visible in the source image. For instance, X2Face does not generate the teeth. Our approach also perform better than Monkey-Net as we can see by comparing the generate teeth in the first sequence or the closed eyes in the fourth frames of the second and third sequences.

In Fig.~\ref{fig:nemo-transfer-main}, we report additional examples for the \emph{Tai-Chi-HD} dataset. These examples are well in line with what is reported in the main paper. Both X2Face and Monkey-Net completely fail to generate realistic videos. The source images are warped without respecting human body structure. Conversely, our approach is able to deform the person in foreground without affecting the background. Even though we can see few minor artifacts, our model is able to move each body part independently following the body motion in the driving video.

Finally, in Fig.~\ref{fig:bair-transfer-main} we show three image animation examples on the \emph{Bair} dataset.
Again, we see that X2Face is not able to transfer motion since it constantly returns frames almost identical with the source images. Compared to Monkey-Net, our approach performs slightly better since it preserves better the robot arm as we can see in the second frame of the first sequence or in the fourth frame of the last sequence.


\subsection{Keypoint detection}
 We now illustrate the keypoints that are learned by our self-supervised approach in Fig.~\ref{fig:keypoints}.
On the \emph{Tai-Chi-HD} dataset, the keypoints are semantically consistent since each of them corresponds to a body part: light green for the right foot, and blue and red for the face for instance. Note that, a light green keypoint is constantly located in the bottom left corner in order to model background or camera motion.
On \emph{VoxCeleb}, we observe that, overall, the obtained keypoints are semantically consistent except for the yellow and green keypoints. For instance, the red and purple keypoints constantly correspond to the nose and the chin respectively. We observe a similar consistency for the \emph{Nemo} dataset. For the Bair dataset, we note that two keypoints (dark blue and light green) correspond to the robotic arm.


\subsection{Visualizing occlusion masks}
In Fig.~\ref{fig:occ}, we visualize the predicted occlusion masks  on the \emph{Tai-Chi-HD},
\emph{VoxCeleb} and \emph{Nemo} datasets. In the first sequence, when the person in the driving video is moving backward (second to fourth frames), the occlusion mask becomes black (corresponding to 0) in the background regions that are occluded in the source frame. It indicates that these parts cannot be generated by warping the source image features and must be inpainted. 
A similar observation can be made on the example sequence of \emph{VoxCeleb}. Indeed, we see that when the face is rotating, the mask has low values (dark grey) in the neck region and in the right face side (in the left-hand side of the image) that are not visible in the source Frame. Then, since the driving video example from \emph{Nemo} contains only little motion, the predicted mask is almost completely white. 
Overall, these three examples show that the occlusion masks truly indicate occluded regions even if no specific training loss is employed in order to lead to this behaviour. 
Finally,  the predicted occlusion masks are more difficult to interpret in the case of the \emph{Bair} dataset. Indeed, the robotic arm is masked out in every frame whereas we could expect that the model generates it by warping. A possible explanation is that, since in this particular dataset, the moving object is always the same, the network can generate without warping the source image. We observe also that masks have low values for the regions corresponding to the arm shadow. It is explained by the fact that shadows cannot be obtained by image warping and that they need to be added by the generator. 
 
\begin{figure*}[t]
  \centering
\resizebox{1\linewidth}{!}{\begin{tabular}{>{\centering\arraybackslash}m{2.5cm}ccccccc}
\toprule
 & \includegraphics[height=0.14\columnwidth]{figures/tablecap.pdf}&
 \includegraphics[height=0.14\columnwidth]{figures/vox-transfer/005vid001}&
  \includegraphics[height=0.14\columnwidth]{figures/vox-transfer/026vid001}&
\includegraphics[height=0.14\columnwidth]{figures/vox-transfer/061vid001}&
\includegraphics[height=0.14\columnwidth]{figures/vox-transfer/105vid001}&
\includegraphics[height=0.14\columnwidth]{figures/vox-transfer/131vid001}
\\
  \vspace{-1.5cm} \Large X2face~\cite{wiles2018x2face} & \includegraphics[height=0.14\columnwidth]{figures/vox-transfer/001app001}&
 \includegraphics[height=0.14\columnwidth]{figures/vox-transfer/005x2face001}&
  \includegraphics[height=0.14\columnwidth]{figures/vox-transfer/026x2face001}&
\includegraphics[height=0.14\columnwidth]{figures/vox-transfer/061x2face001}&
\includegraphics[height=0.14\columnwidth]{figures/vox-transfer/105x2face001}&
\includegraphics[height=0.14\columnwidth]{figures/vox-transfer/131x2face001}
\\
  \vspace{-1.5cm} \Large Monkey-Net~\cite{siarohin2018animating} & \includegraphics[height=0.14\columnwidth]{figures/vox-transfer/001app001}&
 \includegraphics[height=0.14\columnwidth]{figures/vox-transfer/005monkey001}&
  \includegraphics[height=0.14\columnwidth]{figures/vox-transfer/026monkey001}&
\includegraphics[height=0.14\columnwidth]{figures/vox-transfer/061monkey001}&
\includegraphics[height=0.14\columnwidth]{figures/vox-transfer/105monkey001}&
\includegraphics[height=0.14\columnwidth]{figures/vox-transfer/131monkey001}
\\
  \vspace{-1.5cm} \Large Ours& \includegraphics[height=0.14\columnwidth]{figures/vox-transfer/001app001}&
 \includegraphics[height=0.14\columnwidth]{figures/vox-transfer/005jac001}&
  \includegraphics[height=0.14\columnwidth]{figures/vox-transfer/026jac001}&
\includegraphics[height=0.14\columnwidth]{figures/vox-transfer/061jac001}&
\includegraphics[height=0.14\columnwidth]{figures/vox-transfer/105jac001}&
\includegraphics[height=0.14\columnwidth]{figures/vox-transfer/131jac001}
\\
\midrule
\end{tabular}}
\resizebox{1\linewidth}{!}{\begin{tabular}{>{\centering\arraybackslash}m{2.5cm}ccccccc}
  & \includegraphics[height=0.14\columnwidth]{figures/tablecap.pdf}&
 \includegraphics[height=0.14\columnwidth]{figures/vox-transfer/005vid002}&
  \includegraphics[height=0.14\columnwidth]{figures/vox-transfer/071vid002}&
\includegraphics[height=0.14\columnwidth]{figures/vox-transfer/114vid002}&
\includegraphics[height=0.14\columnwidth]{figures/vox-transfer/165vid002}&
\includegraphics[height=0.14\columnwidth]{figures/vox-transfer/187vid002}
\\
    \vspace{-1.5cm} \Large X2face~\cite{wiles2018x2face} & \includegraphics[height=0.14\columnwidth]{figures/vox-transfer/001app002}&
 \includegraphics[height=0.14\columnwidth]{figures/vox-transfer/005x2face002}&
  \includegraphics[height=0.14\columnwidth]{figures/vox-transfer/071x2face002}&
\includegraphics[height=0.14\columnwidth]{figures/vox-transfer/114x2face002}&
\includegraphics[height=0.14\columnwidth]{figures/vox-transfer/165x2face002}&
\includegraphics[height=0.14\columnwidth]{figures/vox-transfer/187x2face002}
\\
    \vspace{-1.5cm} \Large Monkey-Net~\cite{siarohin2018animating} & \includegraphics[height=0.14\columnwidth]{figures/vox-transfer/001app002}&
 \includegraphics[height=0.14\columnwidth]{figures/vox-transfer/005monkey002}&
  \includegraphics[height=0.14\columnwidth]{figures/vox-transfer/071monkey002}&
\includegraphics[height=0.14\columnwidth]{figures/vox-transfer/114monkey002}&
\includegraphics[height=0.14\columnwidth]{figures/vox-transfer/165monkey002}&
\includegraphics[height=0.14\columnwidth]{figures/vox-transfer/187monkey002}
\\
  \vspace{-1.5cm} \Large Ours& \includegraphics[height=0.14\columnwidth]{figures/vox-transfer/001app002}&
 \includegraphics[height=0.14\columnwidth]{figures/vox-transfer/005jac002}&
  \includegraphics[height=0.14\columnwidth]{figures/vox-transfer/071jac002}&
\includegraphics[height=0.14\columnwidth]{figures/vox-transfer/114jac002}&
\includegraphics[height=0.14\columnwidth]{figures/vox-transfer/165jac002}&
\includegraphics[height=0.14\columnwidth]{figures/vox-transfer/187jac002}
\\
\midrule
\end{tabular}
}

\resizebox{1\linewidth}{!}{\begin{tabular}{>{\centering\arraybackslash}m{2.5cm}ccccccc}
  & \includegraphics[height=0.14\columnwidth]{figures/tablecap.pdf}&
 \includegraphics[height=0.14\columnwidth]{figures/vox-transfer/005vid003}&
  \includegraphics[height=0.14\columnwidth]{figures/vox-transfer/018vid003}&
\includegraphics[height=0.14\columnwidth]{figures/vox-transfer/053vid003}&
\includegraphics[height=0.14\columnwidth]{figures/vox-transfer/108vid003}&
\includegraphics[height=0.14\columnwidth]{figures/vox-transfer/129vid003}
\\
      \vspace{-1.5cm} \Large X2face~\cite{wiles2018x2face} & \includegraphics[height=0.14\columnwidth]{figures/vox-transfer/001app003}&
 \includegraphics[height=0.14\columnwidth]{figures/vox-transfer/005x2face003}&
  \includegraphics[height=0.14\columnwidth]{figures/vox-transfer/018x2face003}&
\includegraphics[height=0.14\columnwidth]{figures/vox-transfer/053x2face003}&
\includegraphics[height=0.14\columnwidth]{figures/vox-transfer/108x2face003}&
\includegraphics[height=0.14\columnwidth]{figures/vox-transfer/129x2face003}
\\
  \vspace{-1.5cm} \Large Monkey-Net~\cite{siarohin2018animating} & \includegraphics[height=0.14\columnwidth]{figures/vox-transfer/001app003}&
 \includegraphics[height=0.14\columnwidth]{figures/vox-transfer/005monkey003}&
  \includegraphics[height=0.14\columnwidth]{figures/vox-transfer/018monkey003}&
\includegraphics[height=0.14\columnwidth]{figures/vox-transfer/053monkey003}&
\includegraphics[height=0.14\columnwidth]{figures/vox-transfer/108monkey003}&
\includegraphics[height=0.14\columnwidth]{figures/vox-transfer/129monkey003}
\\
    \vspace{-1.5cm} \Large Ours& \includegraphics[height=0.14\columnwidth]{figures/vox-transfer/001app003}&
 \includegraphics[height=0.14\columnwidth]{figures/vox-transfer/005jac003}&
  \includegraphics[height=0.14\columnwidth]{figures/vox-transfer/018jac003}&
\includegraphics[height=0.14\columnwidth]{figures/vox-transfer/053jac003}&
\includegraphics[height=0.14\columnwidth]{figures/vox-transfer/108jac003}&
\includegraphics[height=0.14\columnwidth]{figures/vox-transfer/129jac003}
\\
\bottomrule
\end{tabular}
}


\caption{Qualitative comparison with state of the art for the task of image animation on different sequences from the \emph{VoxCeleb} dataset.}
\label{fig:vox-transfer-main}
\end{figure*}






\begin{figure*}[t]
  \centering
\resizebox{1\linewidth}{!}{\begin{tabular}{>{\centering\arraybackslash}m{2.5cm}ccccccc}
\toprule
 & \includegraphics[height=0.14\columnwidth]{figures/tablecap.pdf}&
 \includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/000vid001}&
  \includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/001vid001}&
\includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/002vid001}&
\includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/003vid001}&
\includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/004vid001}
\\
  \vspace{-1.5cm} \Large X2face~\cite{wiles2018x2face} & \includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/001app001}&
 \includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/000x2face001}&
  \includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/001x2face001}&
\includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/002x2face001}&
\includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/003x2face001}&
\includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/004x2face001}
\\
  \vspace{-1.5cm} \Large Monkey-Net~\cite{siarohin2018animating} & \includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/001app001}&
 \includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/000monkey001}&
  \includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/001monkey001}&
\includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/002monkey001}&
\includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/003monkey001}&
\includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/004monkey001}
\\
  \vspace{-1.5cm} \Large Ours& \includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/001app001}&
 \includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/000jac001}&
  \includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/001jac001}&
\includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/002jac001}&
\includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/003jac001}&
\includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/004jac001}
\\
\midrule
\end{tabular}}
\resizebox{1\linewidth}{!}{\begin{tabular}{>{\centering\arraybackslash}m{2.5cm}ccccccc}
 & \includegraphics[height=0.14\columnwidth]{figures/tablecap.pdf}&
 \includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/000vid002}&
  \includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/001vid002}&
\includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/002vid002}&
\includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/003vid002}&
\includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/004vid002}
\\
  \vspace{-1.5cm} \Large X2face~\cite{wiles2018x2face} & \includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/001app002}&
 \includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/000x2face002}&
  \includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/001x2face002}&
\includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/002x2face002}&
\includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/003x2face002}&
\includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/004x2face002}
\\
  \vspace{-1.5cm} \Large Monkey-Net~\cite{siarohin2018animating} & \includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/001app002}&
 \includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/000monkey002}&
  \includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/001monkey002}&
\includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/002monkey002}&
\includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/003monkey002}&
\includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/004monkey002}
\\
  \vspace{-1.5cm} \Large Ours& \includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/001app002}&
 \includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/000jac002}&
  \includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/001jac002}&
\includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/002jac002}&
\includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/003jac002}&
\includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/004jac002}
\\
\midrule
\end{tabular}}

\resizebox{1\linewidth}{!}{\begin{tabular}{>{\centering\arraybackslash}m{2.5cm}ccccccc}

 & \includegraphics[height=0.14\columnwidth]{figures/tablecap.pdf}&
 \includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/000vid003}&
  \includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/001vid003}&
\includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/002vid003}&
\includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/003vid003}&
\includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/004vid003}
\\
  \vspace{-1.5cm} \Large X2face~\cite{wiles2018x2face} & \includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/001app003}&
 \includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/000x2face003}&
  \includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/001x2face003}&
\includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/002x2face003}&
\includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/003x2face003}&
\includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/004x2face003}
\\
  \vspace{-1.5cm} \Large Monkey-Net~\cite{siarohin2018animating} & \includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/001app003}&
 \includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/000monkey003}&
  \includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/001monkey003}&
\includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/002monkey003}&
\includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/003monkey003}&
\includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/004monkey003}
\\
  \vspace{-1.5cm} \Large Ours& \includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/001app003}&
 \includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/000jac003}&
  \includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/001jac003}&
\includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/002jac003}&
\includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/003jac003}&
\includegraphics[height=0.14\columnwidth]{figures/nemo-transfer/004jac003}
\\
\bottomrule
\end{tabular}}


\caption{Qualitative comparison with state of the art for the task of image animation on different sequences from the \emph{Nemo} dataset.}
\label{fig:nemo-transfer-main}
\end{figure*}




\begin{figure*}[t]
  \centering
\resizebox{1\linewidth}{!}{\begin{tabular}{>{\centering\arraybackslash}m{2.5cm}ccccccc}
\toprule
 & \includegraphics[height=0.14\columnwidth]{figures/tablecap.pdf}&
 \includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/005vid001}&
  \includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/094vid001}&
\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/153vid001}&
\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/191vid001}&
\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/275vid001}
\\
  \vspace{-1.5cm} \Large X2face~\cite{wiles2018x2face} & \includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/001app001}&
 \includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/005x2face001}&
  \includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/094x2face001}&
\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/153x2face001}&
\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/191x2face001}&
\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/275x2face001}
\\
  \vspace{-1.5cm} \Large Monkey-Net~\cite{siarohin2018animating} & \includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/001app001}&
 \includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/005monkey001}&
  \includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/094monkey001}&
\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/153monkey001}&
\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/191monkey001}&
\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/275monkey001}
\\
  \vspace{-1.5cm} \Large Ours& \includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/001app001}&
 \includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/005jac001}&
  \includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/094jac001}&
\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/153jac001}&
\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/191jac001}&
\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/275jac001}
\\
\midrule
\end{tabular}}
\resizebox{1\linewidth}{!}{\begin{tabular}{>{\centering\arraybackslash}m{2.5cm}ccccccc}
 & \includegraphics[height=0.14\columnwidth]{figures/tablecap.pdf}&
 \includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/000vid002}&
  \includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/016vid002}&
\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/033vid002}&
\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/080vid002}&
\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/105vid002}
\\
  \vspace{-1.5cm} \Large X2face~\cite{wiles2018x2face} & \includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/001app002}&
 \includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/000x2face002}&
  \includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/016x2face002}&
\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/033x2face002}&
\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/080x2face002}&
\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/105x2face002}
\\
  \vspace{-1.5cm} \Large Monkey-Net~\cite{siarohin2018animating} & \includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/001app002}&
 \includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/000monkey002}&
  \includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/016monkey002}&
\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/033monkey002}&
\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/080monkey002}&
\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/105monkey002}
\\
  \vspace{-1.5cm} \Large Ours& \includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/001app002}&
 \includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/000jac002}&
  \includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/016jac002}&
\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/033jac002}&
\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/080jac002}&
\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/105jac002}
\\
\midrule
\end{tabular}}

\resizebox{1\linewidth}{!}{\begin{tabular}{>{\centering\arraybackslash}m{2.5cm}ccccccc}
 & \includegraphics[height=0.14\columnwidth]{figures/tablecap.pdf}&
 \includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/000vid003}&
  \includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/056vid003}&
\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/129vid003}&
\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/165vid003}&
\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/303vid003}
\\
  \vspace{-1.5cm} \Large X2face~\cite{wiles2018x2face} & \includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/001app003}&
 \includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/000x2face003}&
  \includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/056x2face003}&
\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/129x2face003}&
\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/165x2face003}&
\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/303x2face003}
\\
  \vspace{-1.5cm} \Large Monkey-Net~\cite{siarohin2018animating} & \includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/001app003}&
 \includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/000monkey003}&
  \includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/056monkey003}&
\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/129monkey003}&
\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/165monkey003}&
\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/303monkey003}
\\
  \vspace{-1.5cm} \Large Ours& \includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/001app003}&
 \includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/000jac003}&
  \includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/056jac003}&
\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/129jac003}&
\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/165jac003}&
\includegraphics[height=0.14\columnwidth]{figures/taichi-transfer-2/303jac003}
\\
\midrule
\end{tabular}}

\caption{Qualitative comparison with state of the art for the task of image animation on different sequences from the \emph{Tai-Chi-HD} dataset.}
\label{fig-sup:taichi-transfer-main}
\end{figure*}








\begin{figure*}[t]
  \centering

\resizebox{1\linewidth}{!}{\begin{tabular}{>{\centering\arraybackslash}m{2.5cm}ccccccc}
 & \includegraphics[height=0.14\columnwidth]{figures/tablecap.pdf}&
 \includegraphics[height=0.14\columnwidth]{figures/bair-transfer/000vid003}&
  \includegraphics[height=0.14\columnwidth]{figures/bair-transfer/001vid003}&
\includegraphics[height=0.14\columnwidth]{figures/bair-transfer/002vid003}&
\includegraphics[height=0.14\columnwidth]{figures/bair-transfer/003vid003}&
\includegraphics[height=0.14\columnwidth]{figures/bair-transfer/004vid003}
\\
  \vspace{-1.5cm} \Large X2face~\cite{wiles2018x2face} & \includegraphics[height=0.14\columnwidth]{figures/bair-transfer/001app003}&
 \includegraphics[height=0.14\columnwidth]{figures/bair-transfer/000x2face003}&
  \includegraphics[height=0.14\columnwidth]{figures/bair-transfer/001x2face003}&
\includegraphics[height=0.14\columnwidth]{figures/bair-transfer/002x2face003}&
\includegraphics[height=0.14\columnwidth]{figures/bair-transfer/003x2face003}&
\includegraphics[height=0.14\columnwidth]{figures/bair-transfer/004x2face003}
\\
  \vspace{-1.5cm} \Large Monkey-Net~\cite{siarohin2018animating} & \includegraphics[height=0.14\columnwidth]{figures/bair-transfer/001app003}&
 \includegraphics[height=0.14\columnwidth]{figures/bair-transfer/000monkey003}&
  \includegraphics[height=0.14\columnwidth]{figures/bair-transfer/001monkey003}&
\includegraphics[height=0.14\columnwidth]{figures/bair-transfer/002monkey003}&
\includegraphics[height=0.14\columnwidth]{figures/bair-transfer/003monkey003}&
\includegraphics[height=0.14\columnwidth]{figures/bair-transfer/004monkey003}
\\
  \vspace{-1.5cm} \Large Ours& \includegraphics[height=0.14\columnwidth]{figures/bair-transfer/001app003}&
 \includegraphics[height=0.14\columnwidth]{figures/bair-transfer/000jac003}&
  \includegraphics[height=0.14\columnwidth]{figures/bair-transfer/001jac003}&
\includegraphics[height=0.14\columnwidth]{figures/bair-transfer/002jac003}&
\includegraphics[height=0.14\columnwidth]{figures/bair-transfer/003jac003}&
\includegraphics[height=0.14\columnwidth]{figures/bair-transfer/004jac003}
\\
\midrule
\end{tabular}}

\resizebox{1\linewidth}{!}{\begin{tabular}{>{\centering\arraybackslash}m{2.5cm}ccccccc}
 & \includegraphics[height=0.14\columnwidth]{figures/tablecap.pdf}&
 \includegraphics[height=0.14\columnwidth]{figures/bair-transfer/000vid002}&
  \includegraphics[height=0.14\columnwidth]{figures/bair-transfer/001vid002}&
\includegraphics[height=0.14\columnwidth]{figures/bair-transfer/002vid002}&
\includegraphics[height=0.14\columnwidth]{figures/bair-transfer/003vid002}&
\includegraphics[height=0.14\columnwidth]{figures/bair-transfer/004vid002}
\\
  \vspace{-1.5cm} \Large X2face~\cite{wiles2018x2face} & \includegraphics[height=0.14\columnwidth]{figures/bair-transfer/001app002}&
 \includegraphics[height=0.14\columnwidth]{figures/bair-transfer/000x2face002}&
  \includegraphics[height=0.14\columnwidth]{figures/bair-transfer/001x2face002}&
\includegraphics[height=0.14\columnwidth]{figures/bair-transfer/002x2face002}&
\includegraphics[height=0.14\columnwidth]{figures/bair-transfer/003x2face002}&
\includegraphics[height=0.14\columnwidth]{figures/bair-transfer/004x2face002}
\\
  \vspace{-1.5cm} \Large Monkey-Net~\cite{siarohin2018animating} & \includegraphics[height=0.14\columnwidth]{figures/bair-transfer/001app002}&
 \includegraphics[height=0.14\columnwidth]{figures/bair-transfer/000monkey002}&
  \includegraphics[height=0.14\columnwidth]{figures/bair-transfer/001monkey002}&
\includegraphics[height=0.14\columnwidth]{figures/bair-transfer/002monkey002}&
\includegraphics[height=0.14\columnwidth]{figures/bair-transfer/003monkey002}&
\includegraphics[height=0.14\columnwidth]{figures/bair-transfer/004monkey002}
\\
  \vspace{-1.5cm} \Large Ours& \includegraphics[height=0.14\columnwidth]{figures/bair-transfer/001app002}&
 \includegraphics[height=0.14\columnwidth]{figures/bair-transfer/000jac002}&
  \includegraphics[height=0.14\columnwidth]{figures/bair-transfer/001jac002}&
\includegraphics[height=0.14\columnwidth]{figures/bair-transfer/002jac002}&
\includegraphics[height=0.14\columnwidth]{figures/bair-transfer/003jac002}&
\includegraphics[height=0.14\columnwidth]{figures/bair-transfer/004jac002}
\\
\midrule
\end{tabular}}


\resizebox{1\linewidth}{!}{\begin{tabular}{>{\centering\arraybackslash}m{2.5cm}ccccccc}
 & \includegraphics[height=0.14\columnwidth]{figures/tablecap.pdf}&
 \includegraphics[height=0.14\columnwidth]{figures/bair-transfer/000vid001}&
  \includegraphics[height=0.14\columnwidth]{figures/bair-transfer/001vid001}&
\includegraphics[height=0.14\columnwidth]{figures/bair-transfer/002vid001}&
\includegraphics[height=0.14\columnwidth]{figures/bair-transfer/003vid001}&
\includegraphics[height=0.14\columnwidth]{figures/bair-transfer/004vid001}
\\
  \vspace{-1.5cm} \Large X2face~\cite{wiles2018x2face} & \includegraphics[height=0.14\columnwidth]{figures/bair-transfer/001app001}&
 \includegraphics[height=0.14\columnwidth]{figures/bair-transfer/000x2face001}&
  \includegraphics[height=0.14\columnwidth]{figures/bair-transfer/001x2face001}&
\includegraphics[height=0.14\columnwidth]{figures/bair-transfer/002x2face001}&
\includegraphics[height=0.14\columnwidth]{figures/bair-transfer/003x2face001}&
\includegraphics[height=0.14\columnwidth]{figures/bair-transfer/004x2face001}
\\
  \vspace{-1.5cm} \Large Monkey-Net~\cite{siarohin2018animating} & \includegraphics[height=0.14\columnwidth]{figures/bair-transfer/001app001}&
 \includegraphics[height=0.14\columnwidth]{figures/bair-transfer/000monkey001}&
  \includegraphics[height=0.14\columnwidth]{figures/bair-transfer/001monkey001}&
\includegraphics[height=0.14\columnwidth]{figures/bair-transfer/002monkey001}&
\includegraphics[height=0.14\columnwidth]{figures/bair-transfer/003monkey001}&
\includegraphics[height=0.14\columnwidth]{figures/bair-transfer/004monkey001}
\\
  \vspace{-1.5cm} \Large Ours& \includegraphics[height=0.14\columnwidth]{figures/bair-transfer/001app001}&
 \includegraphics[height=0.14\columnwidth]{figures/bair-transfer/000jac001}&
  \includegraphics[height=0.14\columnwidth]{figures/bair-transfer/001jac001}&
\includegraphics[height=0.14\columnwidth]{figures/bair-transfer/002jac001}&
\includegraphics[height=0.14\columnwidth]{figures/bair-transfer/003jac001}&
\includegraphics[height=0.14\columnwidth]{figures/bair-transfer/004jac001}
\\
\bottomrule
\end{tabular}}

\caption{Qualitative comparison with state of the art for the task of image animation on different sequences from the \emph{Bair} dataset.}
\label{fig:bair-transfer-main}
\end{figure*}


\begin{figure*}[t]
  \centering
\resizebox{1\linewidth}{!}{\begin{tabular}{>{\centering\arraybackslash}m{2.5cm}ccccccc}
\toprule
 &\includegraphics[height=0.14\columnwidth]{figures/keypoints/taichi_002_0000}&
 \includegraphics[height=0.14\columnwidth]{figures/keypoints/taichi_002_0001}&
  \includegraphics[height=0.14\columnwidth]{figures/keypoints/taichi_002_0002}&
\includegraphics[height=0.14\columnwidth]{figures/keypoints/taichi_002_0003}&
\includegraphics[height=0.14\columnwidth]{figures/keypoints/taichi_002_0004}&
\includegraphics[height=0.14\columnwidth]{figures/keypoints/taichi_002_0005}
\\
  \vspace{-1.5cm} \Large \emph{Tai-Chi-HD} & \includegraphics[height=0.14\columnwidth]{figures/keypoints/taichi_003_0000}&
 \includegraphics[height=0.14\columnwidth]{figures/keypoints/taichi_003_0001}&
  \includegraphics[height=0.14\columnwidth]{figures/keypoints/taichi_003_0002}&
\includegraphics[height=0.14\columnwidth]{figures/keypoints/taichi_003_0003}&
\includegraphics[height=0.14\columnwidth]{figures/keypoints/taichi_003_0004}&
\includegraphics[height=0.14\columnwidth]{figures/keypoints/taichi_003_0005}
\\
  &\includegraphics[height=0.14\columnwidth]{figures/keypoints/taichi_011_0000}&
 \includegraphics[height=0.14\columnwidth]{figures/keypoints/taichi_011_0001}&
  \includegraphics[height=0.14\columnwidth]{figures/keypoints/taichi_011_0002}&
\includegraphics[height=0.14\columnwidth]{figures/keypoints/taichi_011_0003}&
\includegraphics[height=0.14\columnwidth]{figures/keypoints/taichi_011_0004}&
\includegraphics[height=0.14\columnwidth]{figures/keypoints/taichi_011_0005}
\\
\midrule
  &\includegraphics[height=0.14\columnwidth]{figures/keypoints/vox_005_0000}&
 \includegraphics[height=0.14\columnwidth]{figures/keypoints/vox_005_0001}&
  \includegraphics[height=0.14\columnwidth]{figures/keypoints/vox_005_0002}&
\includegraphics[height=0.14\columnwidth]{figures/keypoints/vox_005_0003}&
\includegraphics[height=0.14\columnwidth]{figures/keypoints/vox_005_0004}&
\includegraphics[height=0.14\columnwidth]{figures/keypoints/vox_005_0005}
\\
  \vspace{-1.5cm} \Large \emph{VoxCeleb}& \includegraphics[height=0.14\columnwidth]{figures/keypoints/vox_008_0000}&
 \includegraphics[height=0.14\columnwidth]{figures/keypoints/vox_008_0001}&
  \includegraphics[height=0.14\columnwidth]{figures/keypoints/vox_008_0002}&
\includegraphics[height=0.14\columnwidth]{figures/keypoints/vox_008_0003}&
\includegraphics[height=0.14\columnwidth]{figures/keypoints/vox_008_0004}&
\includegraphics[height=0.14\columnwidth]{figures/keypoints/vox_008_0005}
\\
  &\includegraphics[height=0.14\columnwidth]{figures/keypoints/vox_029_0000}&
 \includegraphics[height=0.14\columnwidth]{figures/keypoints/vox_029_0001}&
  \includegraphics[height=0.14\columnwidth]{figures/keypoints/vox_029_0002}&
\includegraphics[height=0.14\columnwidth]{figures/keypoints/vox_029_0003}&
\includegraphics[height=0.14\columnwidth]{figures/keypoints/vox_029_0004}&
\includegraphics[height=0.14\columnwidth]{figures/keypoints/vox_029_0005}
\\
\midrule
&\includegraphics[height=0.14\columnwidth]{figures/keypoints/nemo_000_0000}&
 \includegraphics[height=0.14\columnwidth]{figures/keypoints/nemo_000_0001}&
  \includegraphics[height=0.14\columnwidth]{figures/keypoints/nemo_000_0002}&
\includegraphics[height=0.14\columnwidth]{figures/keypoints/nemo_000_0003}&
\includegraphics[height=0.14\columnwidth]{figures/keypoints/nemo_000_0004}&
\includegraphics[height=0.14\columnwidth]{figures/keypoints/nemo_000_0005}
\\
\vspace{-1.5cm} \Large \emph{Nemo}&
  \includegraphics[height=0.14\columnwidth]{figures/keypoints/nemo_003_0000}&
 \includegraphics[height=0.14\columnwidth]{figures/keypoints/nemo_003_0001}&
  \includegraphics[height=0.14\columnwidth]{figures/keypoints/nemo_003_0002}&
\includegraphics[height=0.14\columnwidth]{figures/keypoints/nemo_003_0003}&
\includegraphics[height=0.14\columnwidth]{figures/keypoints/nemo_003_0004}&
\includegraphics[height=0.14\columnwidth]{figures/keypoints/nemo_003_0005}
\\
&\includegraphics[height=0.14\columnwidth]{figures/keypoints/nemo_004_0000}&
 \includegraphics[height=0.14\columnwidth]{figures/keypoints/nemo_004_0001}&
  \includegraphics[height=0.14\columnwidth]{figures/keypoints/nemo_004_0002}&
\includegraphics[height=0.14\columnwidth]{figures/keypoints/nemo_004_0003}&
\includegraphics[height=0.14\columnwidth]{figures/keypoints/nemo_004_0004}&
\includegraphics[height=0.14\columnwidth]{figures/keypoints/nemo_004_0005}
\\
\midrule
  &\includegraphics[height=0.14\columnwidth]{figures/keypoints/bair_000_0000}&
 \includegraphics[height=0.14\columnwidth]{figures/keypoints/bair_000_0001}&
  \includegraphics[height=0.14\columnwidth]{figures/keypoints/bair_000_0002}&
\includegraphics[height=0.14\columnwidth]{figures/keypoints/bair_000_0003}&
\includegraphics[height=0.14\columnwidth]{figures/keypoints/bair_000_0004}&
\includegraphics[height=0.14\columnwidth]{figures/keypoints/bair_000_0005}
\\
  \vspace{-1.5cm} \Large \emph{Bair}& \includegraphics[height=0.14\columnwidth]{figures/keypoints/bair_010_0000}&
 \includegraphics[height=0.14\columnwidth]{figures/keypoints/bair_010_0001}&
  \includegraphics[height=0.14\columnwidth]{figures/keypoints/bair_010_0002}&
\includegraphics[height=0.14\columnwidth]{figures/keypoints/bair_010_0003}&
\includegraphics[height=0.14\columnwidth]{figures/keypoints/bair_010_0004}&
\includegraphics[height=0.14\columnwidth]{figures/keypoints/bair_010_0005}
\\
 &\includegraphics[height=0.14\columnwidth]{figures/keypoints/bair_022_0000}&
 \includegraphics[height=0.14\columnwidth]{figures/keypoints/bair_022_0001}&
  \includegraphics[height=0.14\columnwidth]{figures/keypoints/bair_022_0002}&
\includegraphics[height=0.14\columnwidth]{figures/keypoints/bair_022_0003}&
\includegraphics[height=0.14\columnwidth]{figures/keypoints/bair_022_0004}&
\includegraphics[height=0.14\columnwidth]{figures/keypoints/bair_022_0005}\\
\bottomrule
\end{tabular}}
\caption{Keypoint visualization for the four datasets.}
\label{fig:keypoints}
\end{figure*}


\begin{figure*}[t]
  \centering
\resizebox{1\linewidth}{!}{\begin{tabular}{>{\centering\arraybackslash}m{2.5cm}ccccccc}
\toprule
  & \includegraphics[height=0.14\columnwidth]{figures/tablecap.pdf}&
 \includegraphics[height=0.14\columnwidth]{figures/occlusion/3_driving_0000}&
  \includegraphics[height=0.14\columnwidth]{figures/occlusion/3_driving_0001}&
\includegraphics[height=0.14\columnwidth]{figures/occlusion/3_driving_0002}&
\includegraphics[height=0.14\columnwidth]{figures/occlusion/3_driving_0003}&
\includegraphics[height=0.14\columnwidth]{figures/occlusion/3_driving_0004}
\\
\\
 \vspace{-1.5cm} \Large Occlusion & &\includegraphics[height=0.14\columnwidth]{figures/occlusion/3_occlusion_0000}&
  \includegraphics[height=0.14\columnwidth]{figures/occlusion/3_occlusion_0001}&
\includegraphics[height=0.14\columnwidth]{figures/occlusion/3_occlusion_0002}&
\includegraphics[height=0.14\columnwidth]{figures/occlusion/3_occlusion_0003}&
\includegraphics[height=0.14\columnwidth]{figures/occlusion/3_occlusion_0004}
\\
    \vspace{-1.5cm} \Large Output & \includegraphics[height=0.14\columnwidth]{figures/occlusion/3_source}&
 \includegraphics[height=0.14\columnwidth]{figures/occlusion/3_generated_0000}&
  \includegraphics[height=0.14\columnwidth]{figures/occlusion/3_generated_0001}&
\includegraphics[height=0.14\columnwidth]{figures/occlusion/3_generated_0002}&
\includegraphics[height=0.14\columnwidth]{figures/occlusion/3_generated_0003}&
\includegraphics[height=0.14\columnwidth]{figures/occlusion/3_generated_0004}\\
\midrule
\end{tabular}}
\resizebox{1\linewidth}{!}{\begin{tabular}{>{\centering\arraybackslash}m{2.5cm}ccccccc}
\toprule
  & \includegraphics[height=0.14\columnwidth]{figures/tablecap.pdf}&
 \includegraphics[height=0.14\columnwidth]{figures/occlusion/2_driving_0000}&
  \includegraphics[height=0.14\columnwidth]{figures/occlusion/2_driving_0001}&
\includegraphics[height=0.14\columnwidth]{figures/occlusion/2_driving_0002}&
\includegraphics[height=0.14\columnwidth]{figures/occlusion/2_driving_0003}&
\includegraphics[height=0.14\columnwidth]{figures/occlusion/2_driving_0004}
\\
\vspace{-1.5cm} \Large Occlusion & &\includegraphics[height=0.14\columnwidth]{figures/occlusion/2_occlusion_0000}&
  \includegraphics[height=0.14\columnwidth]{figures/occlusion/2_occlusion_0001}&
\includegraphics[height=0.14\columnwidth]{figures/occlusion/2_occlusion_0002}&
\includegraphics[height=0.14\columnwidth]{figures/occlusion/2_occlusion_0003}&
\includegraphics[height=0.14\columnwidth]{figures/occlusion/2_occlusion_0004}
\\
    \vspace{-1.5cm} \Large Output & \includegraphics[height=0.14\columnwidth]{figures/occlusion/2_source}&
 \includegraphics[height=0.14\columnwidth]{figures/occlusion/2_generated_0000}&
  \includegraphics[height=0.14\columnwidth]{figures/occlusion/2_generated_0001}&
\includegraphics[height=0.14\columnwidth]{figures/occlusion/2_generated_0002}&
\includegraphics[height=0.14\columnwidth]{figures/occlusion/2_generated_0003}&
\includegraphics[height=0.14\columnwidth]{figures/occlusion/2_generated_0004}\\
\midrule
\end{tabular}
}

\resizebox{1\linewidth}{!}{\begin{tabular}{>{\centering\arraybackslash}m{2.5cm}ccccccc}
  & \includegraphics[height=0.14\columnwidth]{figures/tablecap.pdf}&
 \includegraphics[height=0.14\columnwidth]{figures/occlusion/1_driving_0000}&
  \includegraphics[height=0.14\columnwidth]{figures/occlusion/1_driving_0001}&
\includegraphics[height=0.14\columnwidth]{figures/occlusion/1_driving_0002}&
\includegraphics[height=0.14\columnwidth]{figures/occlusion/1_driving_0003}&
\includegraphics[height=0.14\columnwidth]{figures/occlusion/1_driving_0004}
\\
\vspace{-1.5cm} \Large Occlusion & &\includegraphics[height=0.14\columnwidth]{figures/occlusion/1_occlusion_0000}&
  \includegraphics[height=0.14\columnwidth]{figures/occlusion/1_occlusion_0001}&
\includegraphics[height=0.14\columnwidth]{figures/occlusion/1_occlusion_0002}&
\includegraphics[height=0.14\columnwidth]{figures/occlusion/1_occlusion_0003}&
\includegraphics[height=0.14\columnwidth]{figures/occlusion/1_occlusion_0004}
\\
    \vspace{-1.5cm} \Large Output & \includegraphics[height=0.14\columnwidth]{figures/occlusion/1_source}&
 \includegraphics[height=0.14\columnwidth]{figures/occlusion/1_generated_0000}&
  \includegraphics[height=0.14\columnwidth]{figures/occlusion/1_generated_0001}&
\includegraphics[height=0.14\columnwidth]{figures/occlusion/1_generated_0002}&
\includegraphics[height=0.14\columnwidth]{figures/occlusion/1_generated_0003}&
\includegraphics[height=0.14\columnwidth]{figures/occlusion/1_generated_0004}\\
\bottomrule
\end{tabular}
}



\resizebox{1\linewidth}{!}{\begin{tabular}{>{\centering\arraybackslash}m{2.5cm}ccccccc}
  & \includegraphics[height=0.14\columnwidth]{figures/tablecap.pdf}&
 \includegraphics[height=0.14\columnwidth]{figures/occlusion/4_driving_0000}&
  \includegraphics[height=0.14\columnwidth]{figures/occlusion/4_driving_0001}&
\includegraphics[height=0.14\columnwidth]{figures/occlusion/4_driving_0002}&
\includegraphics[height=0.14\columnwidth]{figures/occlusion/4_driving_0003}&
\includegraphics[height=0.14\columnwidth]{figures/occlusion/4_driving_0004}
\\
\vspace{-1.5cm} \Large Occlusion & &\includegraphics[height=0.14\columnwidth]{figures/occlusion/4_occlusion_0000}&
  \includegraphics[height=0.14\columnwidth]{figures/occlusion/4_occlusion_0001}&
\includegraphics[height=0.14\columnwidth]{figures/occlusion/4_occlusion_0002}&
\includegraphics[height=0.14\columnwidth]{figures/occlusion/4_occlusion_0003}&
\includegraphics[height=0.14\columnwidth]{figures/occlusion/4_occlusion_0004}
\\
    \vspace{-1.5cm} \Large Output & \includegraphics[height=0.14\columnwidth]{figures/occlusion/4_source}&
 \includegraphics[height=0.14\columnwidth]{figures/occlusion/4_generated_0000}&
  \includegraphics[height=0.14\columnwidth]{figures/occlusion/4_generated_0001}&
\includegraphics[height=0.14\columnwidth]{figures/occlusion/4_generated_0002}&
\includegraphics[height=0.14\columnwidth]{figures/occlusion/4_generated_0003}&
\includegraphics[height=0.14\columnwidth]{figures/occlusion/4_generated_0004}\\
\bottomrule
\end{tabular}
}

\caption{Visualization of occlusion masks and images obtained after deformation on \emph{Tai-Chi-HD}, \emph{VoxCeleb}, \emph{Nemo} and \emph{Bair} datasets.}
\label{fig:occ}
\end{figure*} 

\end{document}
