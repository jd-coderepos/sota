\documentclass [11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{cite}

\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mathabx}


\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}

\newcommand{\qed}{\hfill\ensuremath{\Box}\medskip\\\noindent}
\newenvironment{proof}{\noindent\emph{Proof. }}{}


\newcommand{\ceil}[1]{\left\lceil{#1}\right\rceil}
\newcommand{\floor}[1]{\left\lfloor{#1}\right\rfloor}
\renewcommand{\angle}[1]{\langle{#1}\rangle}

\newcommand{\pre}{\ensuremath{\mathrm{pre}}}
\newcommand{\height}{\ensuremath{\mathrm{height}}}
\newcommand{\depth}{\ensuremath{\mathrm{depth}}}
\newcommand{\nca}{\ensuremath{\mathrm{nca}}}

\newcommand{\Taux}{\ensuremath{\widetilde{T}}}
\newcommand{\size}{\ensuremath{\mathrm{size}}}
\newcommand{\lab}{\ensuremath{\mathrm{label}}}
\newcommand{\T}{\ensuremath{T}}
\newcommand{\D}{\ensuremath{D}}
\newcommand{\TT}{\ensuremath{\mathcal{T}}}
\newcommand{\TD}{\ensuremath{\mathcal{TD}}}
\newcommand{\nt}{\ensuremath{n_T}}
\newcommand{\nd}{\ensuremath{n_{\D}}}
\newcommand{\ntd}{\ensuremath{n_\TD}}
\newcommand{\ntt}{\ensuremath{n_\TT}}
\newcommand{\Select}{\ensuremath\mathsf{Select}}
\newcommand{\Access}{\ensuremath\mathsf{Access}}
\newcommand{\Decompress}{\ensuremath\mathsf{Decompress}}
\newcommand{\Parent}{\ensuremath\mathsf{Parent}}
\newcommand{\Depth}{\ensuremath\mathsf{Depth}}
\newcommand{\Height}{\ensuremath\mathsf{Height}}
\newcommand{\Size}{\ensuremath\mathsf{Size}}
\newcommand{\FirstChild}{\ensuremath\mathsf{Firstchild}}
\newcommand{\NextSibling}{\ensuremath\mathsf{NextSibling}}
\newcommand{\LevelAncestor}{\ensuremath\mathsf{LevelAncestor}}
\newcommand{\NCA}{\ensuremath\mathsf{NCA}}
\newcommand{\decomp}{\ensuremath\mathsf{Decompress}}
\newcommand{\FindRepr}{\ensuremath\mathsf{FindRepresentatives}}




\usepackage[dvipsnames,usenames]{color}
\usepackage[colorlinks=true,urlcolor=Blue,citecolor=Green,linkcolor=BrickRed]{hyperref}
\urlstyle{same}


\begin{document}
\title{Tree Compression with Top Trees\thanks{An extended abstract of this paper appeared at the 40th International Colloquium on Automata, Languages and Programming.}}
\author{Philip Bille \thanks{Partially supported by the Danish Agency for Science, Technology and Innovation.} \\  \href{mailto:phbi@dtu.dk}{phbi@dtu.dk} \and Inge Li G{\o}rtz\\
 \href{mailto:inge@dtu.dk}{inge@dtu.dk}     \and Gad M. Landau\thanks{Partially supported by the National Science Foundation
Award 0904246, Israel Science Foundation grant 347/09,
Yahoo, Grant No. 2008217 from the United States-Israel
Binational Science Foundation (BSF) and DFG.} \\  \href{mailto:landau@cs.haifa.ac.il}{landau@cs.haifa.ac.il}   \and Oren Weimann  \thanks{Partially supported by  the Israel Science Foundation grant 794/13.} \\  \href{mailto:oren@cs.haifa.ac.il}{oren@cs.haifa.ac.il} }
\date{}

\maketitle


\begin{abstract}

We introduce a new compression scheme for labeled trees based on top trees~\cite{TopTrees}. 
Our  compression scheme is the first to simultaneously take advantage of internal repeats in the tree (as opposed to the classical DAG compression that only exploits rooted subtree repeats) while also supporting fast navigational queries directly on the compressed representation. We show that the new compression scheme achieves close to optimal worst-case compression, can compress exponentially better than DAG compression, is never much worse than DAG compression, and supports navigational queries in logarithmic time.
\end{abstract}




\section{Introduction} 
A labeled tree  is a rooted, ordered tree, where each node has a label from an alphabet . Labeled trees appear in computer science as tries, dictionaries, parse trees, suffix trees, XML trees, etc. In this paper, we study compression schemes for labeled trees that take advantage of \emph{repeated substructures} and support navigational queries, such as returning the label of a node , the parent of , the depth of , the size 's subtrees, etc., directly on the compressed representation. We consider the following two basic types of repeated substructures (see Figure~\ref{fig:subtree}). The first type is used in DAG compression~\cite{BKG03,FKG03} and the second in tree grammars~\cite{Busatto04grammarBasedtree,BLM08,LohreyEtAl,LM06,MB04}.

\begin{description}
\item[Subtree repeat.] A \emph{rooted subtree} is a subgraph of  consisting of a node and all its descendants. A \emph{subtree repeat} is an identical (both in structure and in labels) occurrence of a rooted subtree in . 

\item[Tree pattern repeat.] A  \emph{tree pattern} is any connected subgraph of . A  \emph{tree pattern repeat} is an identical (both in structure and in labels) occurrence of a tree pattern in .
\end{description}


In this paper, we introduce a simple new compression scheme, called \emph{top tree compression}, that exploits tree pattern repeats. Compared to the existing techniques our compression scheme has the following advantages: Let  be a tree of size  with nodes labeled from an alphabet of size . We support navigational queries in  time (a similar result is not known for tree grammars), the compression ratio is in the worst case at least  (no such result is known for either DAG compression or tree grammars), our scheme can compress exponentially better than DAG compression, and the compression ratio is never worse than DAG compression by more than a  factor.


\subsection{Previous Work}
The previous work on tree compression can be described by three major approaches: using subtree repeats, using tree pattern repeats, and using succinct data structures.  Below we briefly discuss these approaches and the existing  tree compression schemes. 
Extensive practical work has recently been done on all these tree compression schemes (see e.g., the recent survey of Sakr~\cite{sakr2009xml}).

\paragraph{DAG compression.} Using subtree repeats, a node in the tree  that has a child with subtree  can instead point to any other occurrence of . This way, it is possible to represent  as a Directed Acyclic Graph (DAG). Over all possible DAGs that can represent , the smallest one is unique and can be computed in  time~\cite{DST1980}. Its size can be exponentially smaller than . DAG representation of trees are broadly used for identifying and sharing common subexpressions, e.g., in programming languages~\cite{steven1997advanced}  and binary decision diagrams~\cite{meinel1998algorithms}. Compression based on DAGs has also been studied more recently in~\cite{BKG03,FKG03, lohrey2013xml} and a Lempel-Ziv analog of subtree repeats was suggested in~\cite{LZGonzalo}. It is possible to support navigational queries~\cite{SODA2011} and path queries~\cite{BKG03} directly on the DAG representation in logarithmic time. 
The problem with subtree repeats is that we can miss many internal repeats. Consider for example the case where  is a single path of  nodes with the same label. Even though  is highly compressible (we can represent it by just  storing the label and the path length) it does not contain a single subtree repeat and its minimal DAG is of size . 
\begin{figure}[t]
   \centering
   \includegraphics[scale=1]{subtree}
   \caption{A tree  with a {\em subtree} repeat  (left), and a  {\em tree pattern} repeat  (right).}
   \label{fig:subtree}
\end{figure}
\paragraph{Tree grammars.} Alternatively, \emph{tree grammars} are capable of exploiting tree pattern repeats.  Tree grammars generalize grammars from deriving strings to deriving trees and were studied in~\cite{Busatto04grammarBasedtree, BLM08, LohreyEtAl, LM06, MB04}. Compared to DAG compression, a tree grammar can be  exponentially smaller than the minimal DAG~\cite{LM06}. Unfortunately, computing a minimal tree grammar is NP-Hard~\cite{CLL+05}, and all known tree grammar based compression schemes can only support navigational queries in time proportional to the height of the grammar which can be .


\paragraph{Succinct  data structures.} A different approach to tree compression is \emph{succinct data structures} that compactly encode trees. Jacobson~\cite{Jacobson} was the first to observe that the naive pointer-based tree representation using  bits is wasteful. He showed that {\em unlabeled} trees can be represented using  bits and support various queries by inspection of  bits in the bit probe model. This space bound is asymptotically optimal with the information-theoretic lower bound. Munro and Raman~\cite{MunroRaman01} showed how to achieve the same bound in the RAM model while using only constant time for queries. Such representations are called \emph{succinct data structures}, and have  been generalized to include a richer set of queries such as subtree-size queries~\cite{MunroRaman01,BenoitEtAl05} and level-ancestor queries~\cite{Geary04succinctordinal}.
For {\em labeled} trees, Ferragina et al.~\cite{FerraginaFOCS2005} gave a representation using  bits that
supports basic navigational operations, such as find the parent of node , the 'th child of , and any child of  with label . Ferragina et al. also introduced the notion of 'th order {\em tree entropy}  in a restricted model. In this model, used by popular  XML compressors~\cite{XML2,XML1}, the label of a node is a function of the labels of all its ancestors. For such a tree ,  Ferragina et al. gave a representation requiring at most  bits. Note that  the above space bounds do not guarantee a  compact representation when the input contains many subtree repeats or tree pattern repeats. In particular, the total space is never  bits.



\subsection{Our Results.}
We propose a new compression scheme for labeled trees, which we call \emph{top tree compression}. To the best of our knowledge, this is the first compression scheme for trees that (i) takes advantage of tree pattern repeats (like tree grammars) but (ii) simultaneously supports navigational queries on the compressed representation in logarithmic time (like DAG compression). 
In the worst case, we show  that (iii) the compression ratio of top tree compression is always at least  (compared to the information-theoretic bound of ). This is in contrast to both tree grammars and DAG compression that have not yet been proven to have worst-case compression performance comparable to the information-theoretic bound. Finally, we compare the performance of top tree compression to DAG compression. We show that top tree compression (iv) can compress exponentially better than DAG compression, and (v) is never  worse than DAG compression by more than a   factor. 



The key idea in top tree compression is to transform the input tree  into another tree  such that tree pattern repeats in  become subtree repeats in . The transformation is based on  top trees~\cite{AHLT1997,AHT2000,TopTrees} -- a data structure originally designed for dynamic (uncompressed) trees. After the transformation, we compress the new tree  using the classical DAG compression resulting in the \emph{top DAG} . The top DAG  forms the basis for our compression scheme. We obtain our bounds on compression (iii), (iv), and (v) by analyzing the size of  , and we obtain efficient navigational queries (ii) by augmenting  with additional data structures.


To state our bounds, let  denote the total size (vertices plus edges) of the graph . 
We assume a standard word RAM model of computation with logarithmic word size. All space complexities refer to the number of words used by the data structure. 
We first show the following worst-case compression bound achieved by the top DAG.

\begin{theorem}\label{thm:worstcasebounds}
Let  be any ordered tree with nodes labeled from an alphabet of size  and let  be the corresponding top DAG. Then, .
\end{theorem}
This worst-case performance of the top DAG should be compared to the information-theoretic lower bound of . This lower bound applies already for strings (so it clearly holds for labeled trees). It is obtained by simply noticing that there are  string of length  over an alphabet of size , implying a lower bound of   bits or  words. Note that with standard DAG compression the worst-case bound is  since a single path is incompressible using subtree repeats. 


Secondly, we compare top DAG compression to standard DAG compression. 
\begin{theorem}\label{thm:comparison}
Let  be any ordered tree and let  and  be the corresponding DAG and top DAG, respectively. For any tree  we have   and there exist families of trees  such that .
\end{theorem}
Thus, top DAG compression can be exponentially better than DAG compression (since it's possible that ) and it is always within a logarithmic factor of DAG compression.  
To the best of our knowledge this is the first non-trivial bound shown for any tree compression scheme compared to the DAG. 


  

Finally, we show how to represent the top DAG  in  space such that we can quickly answer a wide range of queries about  without decompressing. 
\begin{theorem}\label{thm:navigation}
Let  be an ordered tree with top DAG . There is an  space representation of  that supports , , , , , , , , and  in  time. Furthermore, we can   a subtree  of  in time . 
\end{theorem}
The operations , , , , , , and  all take a node  in  as input\footnote{The nodes of  are uniquely identified by their preorder numbers. See Section~\ref{section4}.} and return its label, its depth, its height, the size of its subtree, its parent, its first child, and its sibling to the right, respectively. The  returns an ancestor at a specified distance from , and  returns the nearest common ancestor to a given pair of nodes. Finally, the  operation decompresses and returns any rooted subtree. 

\section{Top Trees and Top DAGs}

Top trees were introduced by Alstrup et al.~\cite{AHLT1997,AHT2000,TopTrees} for maintaining an uncompressed, unordered, and unlabeled tree under link and cut operations. We extend them to ordered and labeled trees, and then introduce top DAGs for compression. Our construction is related to well-known algorithms for top tree construction, but modified for our purposes. In particular, we need to carefully order the steps of the construction to guarantee efficient compression, and we disallow some combination of cluster merges to ensure fast navigation. 

\subsection{Clusters} 
Let  be a node in  with children  in left-to-right order. Define  to be the subtree induced by  and all proper descendants of . Define  to be the forest induced by all proper descendants of . For  let  be the  tree pattern induced by the nodes .

A {\em cluster} with {\em top boundary node}  is a tree pattern of the form , . A {\em cluster} with {\em top boundary node}  and {\em bottom boundary node}  is a tree pattern of the form , , where  is a node in .
Clusters can therefore have either one or two boundary nodes. For example, let  denote the parent of  then a single edge 
  of  is a cluster where  is the top boundary node. If  is a leaf then there is no bottom boundary node, otherwise  is a bottom boundary node. Nodes that are not boundary nodes are called 
\emph{internal} nodes. 

Two edge disjoint clusters  and  whose vertices overlap on a single boundary node can be \emph{merged} if their union  is also a cluster. There are five ways of merging clusters, as illustrated by Fig.~\ref{fig:merge}. Merges of type (a) and (b) can be done if the common boundary node is not a boundary node of any other cluster except  and . Merges of type (c),(d), and (e) can be done only if at least one of  or  does not have a bottom boundary node.
The original paper on top trees~\cite{AHLT1997,AHT2000,TopTrees} contains more ways to merge clusters, but allowing these would lead to a violation of our definition of clusters as a tree pattern of the form , which we need for navigational purposes.
\begin{figure}[tb]
   \centering
   \includegraphics[scale=0.4]{merge}
   \caption{Five ways of merging clusters. The  nodes are boundary nodes that remain  boundary nodes in the merged cluster. The  nodes are  boundary nodes that become internal (non-boundary) nodes in the merged cluster. Note that in the last four merges at least one of the  merged clusters has a top boundary node but no bottom boundary node.}
   \label{fig:merge}
\end{figure}

\subsection{Top Trees}
A \emph{top tree}  of  is a hierarchical decomposition of  into  clusters. It is 
an ordered, rooted, labeled, and binary tree defined as follows.
\begin{itemize}
\item[] The nodes of  correspond to clusters of .
\item[] The root of  corresponds to the cluster  itself.
\item[] The leaves of  correspond to the edges of . The label of each leaf is the pair of labels of the endpoints of its corresponding edge  in . The two labels are ordered so that the label of the parent  appears before the label of the child. 
\item[] Each internal node of  corresponds to the merged cluster of its two children. 
The label of each internal node is the type of merge it represents (out of the five merging options).  The children are ordered so  that the left child is the child cluster visited first in a preorder traversal of . 
\end{itemize}




\subsection{Constructing the Top Tree}\label{sec:toptreeconstruction}


We now describe a greedy algorithm for constructing a top tree  of  that has height . The algorithm constructs the top tree  bottom-up in  iterations starting with the edges of  as the leaves of . 
During the construction,  is a forest, and we maintain an auxiliary rooted ordered tree  initialized as . The edges of  will correspond to the nodes of  and to the clusters of . 
The internal nodes of  will correspond to boundary nodes of clusters in , and the leaves of    will correspond to a subset of the leaves of .

In the beginning, these clusters represent actual edges  of . In this case, if  is not a leaf in  then  is the bottom boundary node of the cluster and  is the top boundary node. If  is a leaf then there is no bottom boundary node. 

In each one of the  iterations, a constant fraction of 's edges (i.e., clusters of ) are merged. 
Each merge is performed on two overlapping edges  and  of  using one of the five types of merges from Fig.~\ref{fig:merge}: If  is the parent of  and the only child of  then a merge of type (a) or (b) contracts these edges in  into the edge . If  is the parent of both  and , and  or  are  leaves, then a merge of type (c), (d), or (e) replaces these edges in  with either the edge  or . In all cases, we create a new node in  whose two children are the clusters corresponding to  and to . 

We prove below that a single iteration shrinks the tree  (and the number of roots in ) by a constant factor. The process ends when  is a single edge. Each iteration is performed as follows:

\paragraph{Step 1: Horizontal Merges.} For each node  with  children , for  to ,
merge the edges  and   if  or  is a leaf.
If  is odd  and  is a leaf and both  and  are non-leaves then also merge  and .

\paragraph{Step 2: Vertical Merges.} For each maximal path  of nodes in  such that  is the parent of  and  have a single child: If  is even merge the following pairs of  edges . If  is odd  merge the following pairs of  edges 
, and if  was not merged in Step 1 then also merge .



\begin{lemma}\label{lem:contractionfactor}
A single iteration shrinks  by a factor of .
\end{lemma}
\begin{proof}
Suppose that  in the beginning of the iteration the tree  has  nodes. 
Any tree with  nodes has at least  nodes with less than  children. Consider the edges  of  where  has one or no children. We show that at least half of these   edges are merged in this iteration. This will imply that  edges of  are replaced with  edges and so the size of  shrinks to . To prove it, we charge each edge  that is not merged to a unique edge  that is merged.


\paragraph{Case 1.} 
Suppose that  has no children (i.e., is a leaf).   If  has at least one sibling and  is not merged it is because  has no right sibling and its left sibling  has already been merged (i.e., we have just merged  and  in Step 1 where ). 
We also know that at least one of  and  must be a leaf. We set  if  is a leaf, otherwise we set . 

\paragraph{Case 2.} 
Suppose that  has no children (i.e., is a leaf) and no siblings (i.e.,  has only one child). The only reason for not merging  with  in Step 2 is because   was just merged in Step 1. In this case, we set . Notice that we haven't already charged  in {\em Case 1} because  is not a leaf.

\paragraph{Case 3.} 
Suppose that  has exactly one child  and that  was not merged in Step 1. 
The only reason for not merging  with  in Step 2 is if  has only one child  and we just merged   with  . In this case, we set .
Notice that we haven't already charged  in {\em Case 1} because  is not a leaf. We also haven't  charged  in {\em Case 2} because  has only one child.  \qed
\end{proof}

\noindent Since each iteration can be done in linear time and shrinks  by a factor  we obtain the following.
\begin{corollary}
Given a tree , the greedy top tree construction creates a top tree of size   and height  in  time.
\end{corollary}

The next lemma follows from the construction of the top tree and Lemma~\ref{lem:contractionfactor}.
\begin{lemma}\label{lem:subtresize}
For any node  in the top tree corresponding to a cluster  of , the number of nodes in the subtree  is .
\end{lemma}


\subsection{Top Dags}
The \emph{top DAG} of , denoted , is the minimal DAG representation of the top tree .  It can be computed in  time from  using the algorithm of~\cite{DST1980}.
The entire  top DAG construction can thus be done in  time.





\section{Compression Analysis}


\subsection{Worst-case Bounds for Top Dag Compression}
We now prove Theorem~\ref{thm:worstcasebounds}. 
Let  be an ordered tree with  nodes labeled from an alphabet of size , let  be its top tree and  be its top DAG. We call two rooted subtrees of  {\em identical} if they have the same structure and labels, otherwise they are called {\em distinct}.  
To show that  the size of  is at most 
 is suffices to show that  has only  distinct rooted subtrees.

Recall that each node in the top tree  corresponds to a cluster in . A leaf of  corresponds to a cluster of a single edge of  and is labeled by this edges endpoints (so there are  possible labels). An internal node is labeled by the type of merge that formed it (there are five merging options so there are five possible labels). 

The bottom-up construction of   starts with the leaves of . By Lemma~\ref{lem:contractionfactor} each level in the top tree reduces the number of clusters by a factor , while at most doubling the size of the current clusters (the  size of a cluster is the number of nodes in the corresponding tree pattern). After round  we are therefore left with at most  clusters, each of size at most . 

To bound the total number of distinct rooted subtrees, we partition the clusters into \emph{small clusters} and \emph{large clusters}. 
The small clusters are those created in rounds  to  and the large clusters are those created in the remaining rounds from  to .  The total number of large clusters is at most 
 
In particular, there are at most  nodes of  that correspond to large clusters. So clearly there are at most  distinct subtrees rooted at these nodes. 

Next, we bound the total number of distinct subtrees of  rooted at nodes  corresponding to small clusters.  Each such subtree is of size at most  
most  and is a binary tree whose nodes have labels from an alphabet of size at most . 
The total number of distinct labeled binary trees of size at most   is given by 

where  denotes the th Catalan number. Since , this number is bounded by 
. In the last equality we assumed that . If  then the lemma trivially holds because .
We get that the total number of distinct subtrees of   rooted at small clusters is therefore  also .
This completes the proof of Theorem~\ref{thm:worstcasebounds}. 






\subsection{Comparison to Subtree Sharing}\label{sec:comparisonsubtreesharing}
We now prove Theorem~\ref{thm:comparison}. To do so we first show two useful properties of top trees and top DAGs. 

Let  be a tree with top tree . For any internal node  in , we say that the subtree  is \emph{represented} by a set of clusters  from  if . Here  denotes the graph with node set  and  edge set . Since each edge in  is a cluster in  we can always trivially represent  by at most  clusters. We prove that there always exists a set of clusters, denoted , of size  that represents . 

Let  be any internal node in  and let  be its  leftmost child. Since  is internal we have that  is the top boundary node of the leaf cluster  in . Let  be the smallest cluster in  containing all nodes of . We have that  is a descendant leaf of  in . Consider the path  in  from  to . An \emph{off-path}  cluster of  is a cluster  that is not on , but whose parent cluster is on . We define 


Since the length of  is  the number of clusters in  is . We want to prove that . 
By definition of  we have that all nodes in  are in . For the other direction, we first prove the following lemma. Let  denote the set of edges in  of a cluster .
\begin{lemma}\label{lem:offpath}
Let  be an off-path  cluster of . Then either  or .
\end{lemma}
\begin{proof}
We will show that any cluster in  containing edges from both  and  contains both  and , where  is the leftmost child of  and  is the parent of . Let  be a cluster containing edges from both  and . 
Consider the subtree  and let  be the smallest cluster in   containing edges from both  and . That is,   is the cluster found by descending down from  towards a child with both types of edges as long as such a child exists. Then  must be a merge of type (a) or (b), where the higher cluster  only contains edges from  and the bottom cluster, , only contains edges from . Also,  is the top boundary node of  and the bottom boundary node of . Clearly,  contains the edge , since all clusters are connected tree patterns. A merge of type (a) or (b) is only possible when  contains all children of its top boundary node. Thus  contains the edge . It follows that  (and therefore  since it is an ancestor of ) contains both  and .

We have  and therefore all clusters in  containing  lie on the path from  to the root. The path  is a subpath of this path, and thus no off-path clusters of  can contain . Therefore no off-path clusters of  can contain edges from both  and .\qed
\end{proof}


Any edge from  (except ) contained in a cluster on  must be contained in an off-path cluster of . Lemma~\ref{lem:offpath} therefore implies that  and the following corollary.



\begin{corollary}
Let  be a tree with top tree . For any internal node  in , the subtree  can be represented by a set of  clusters in .
\end{corollary}

Next we prove that our bottom-up top tree construction guarantees that two identical subtrees  are represented by two {\em identical} sets of clusters . Two sets of clusters are identical (denoted ) if there is a 1-1 correspondence between the clusters in  and , such that two clusters mapped to each other are identical tree patterns in  (have the same structure and labels).




\begin{lemma}\label{lem:top-tree-ident}
Let  be a tree with top tree . Let  and  be identical subtrees in . Then, . 
\end{lemma}
\begin{proof} 


Consider the tree  at some iteration of the construction of the top tree. We will say that an edge  in  \emph{belongs to}  (resp. ) if the cluster corresponding to  only contains edges from  (resp. ) in the original tree. Let  be the cluster in   containing  the edge , where  is the leftmost child of . Define  similarly.

We will say that a cluster  is added to  in the iteration where its parent on  is created, and we say that  is added to  right before the first round. Similarly for clusters in .

We will show that new clusters only are added to  (resp. )  if  (resp. ) is merged with an edge belonging to  (resp. ), and that these merges are identical for the two subtrees in each iteration. 

Recall that  is the smallest cluster in  containing all nodes of  and that   is the path of clusters in  from  to . By definition, all clusters on the path  contain . This implies that new off-path clusters are only  constructed when  (resp.\ ) is merged. Merges of identical edges belonging to  and  are the same in the two subtrees of , since we merge first horizontally, and then vertically bottom-up.  
By the same argument if  is merged with an edge belonging to  then  is merged with the corresponding edge from .
For a merge with an edge belonging  to   (resp. ) and an edge not belonging to  (resp. ), one of the edges must be  (resp. ). If  is merged in this iteration, but  is not, then  is merged with an edge not belonging to  (and vice versa). 
Thus, after the iteration all edges belonging to  in   are identical to the edges belonging to  in .  

New off-path clusters are only constructed when  (resp. ) are merged. It only adds new clusters to  (resp. ) if it is a merge with an edge belonging to  (resp. ). Since these merges are identical for the two subtrees in each iteration, and  is merged with an edge belonging to  iff   is merged with the corresponding edge belonging to , we have .\qed
\end{proof}




\begin{theorem}\label{theo:ub}
For any tree , .
\end{theorem}
\begin{proof}
Denote an edge in the DAG as shared if it is in a shared subtree of . We denote the edges in the DAG  that are shared as \emph{red} edges, and the edges that are not shared as \emph{blue}. Let  and  be the number of red  and blue edges in the DAG , respectively.

A cluster in the top tree  is \emph{red} if it only contains red edges from , \emph{blue} if it only contains blue edges from , and \emph{purple} if it contains both. Since clusters are connected subtrees we have the property that if cluster  is red (resp.\ blue), then all clusters in the subtree  are red (resp.\ blue). 
Let , , and  be the number of red, blue, and purple clusters in the top DAG , respectively. 





First we bound the number of red clusters in the top DAG . Consider a shared subtree  from the DAG compression.  is represented by at most  clusters in , and all these contain only edges from . Thus all the clusters in  are red.
It follows from Lemma~\ref{lem:top-tree-ident} that all the clusters representing  (and their subtrees in ) are identical for all copies of  . Therefore each of these will appear only once in the top DAG . 

The clusters representing  are edge-disjoint connected subtrees of . 
It follows from Lemma~\ref{lem:subtresize} that  for each cluster in . 
Therefore the total size of the subtrees of the clusters representing  in  is .  As argued above these are only represented once in the top DAG . Thus the number of red clusters . 


To bound the number of blue clusters in the top DAG, we first note that the blue clusters form rooted subtrees in the top tree.  Let  be the root of such a blue subtree in . Then  is a connected component of blue edges in . It follows from Lemma~\ref{lem:subtresize} that 
. Thus the number of blue clusters .

It remains to bound the number  of purple clusters (clusters containing both shared and non shared edges). The number of purple clusters in the top DAG  is bounded by the number of purple clusters in the top tree . For any purple cluster we have that all its ancestors in  are also purple. Consider the set  of purple clusters in  that have no purple descendants. Each of the clusters in  have a blue leaf cluster in its subtree. These blue leaf clusters are all distinct, and since the corresponding edges are not shared in the DAG , we have . Each cluster in  is the endpoint of a purple path from the root (and the union of these paths contains all purple clusters in \TT). Since the height of  is  the number of nodes on each path is at most . It follows that the number of purple clusters in  (and thus also in ) is at most .



The number of edges in the  is thus 
. \qed
\end{proof}

\begin{lemma}\label{lem:proofbypicture}
There exist trees , such that .
\end{lemma}
\begin{proof}
Caterpillars and paths (where all nodes  have  identical labels) have 
, whereas  (see Figure~\ref{fig:compare}). \qed
\end{proof}
\begin{figure}[tb]
   \centering
\includegraphics[scale=0.38]{path1}\hspace{1cm}
   \includegraphics[scale=0.38]{binarytree1}
   \caption{A  Top DAG  and a DAG D() of (a) a path  and (b) a complete binary tree. All labels are identical. On a path (and also a caterpillar and a star) the size of  is  whereas the size of D() is . On a complete binary tree (b) both  and D() are of size .}

  \label{fig:compare}
\end{figure}



\section{Supporting Navigational Queries}\label{section4}
In this section we prove Theorem~\ref{thm:navigation}. Let  be a tree with top DAG . To uniquely identify nodes of  we refer to them by their preorder numbers. For a node of  with preorder number  we want to support the following queries. 

\begin{description}
\item[:] Return the label associated with node .
\item[:] Return the tree .
\item[:] Return the parent of node .
\item[:] Return the depth of node .
\item[:] Return the height of node .
\item[:] Return the number of nodes in .
\item[:] Return the first child of .
\item[:] Return the sibling immediately to the right of .
\item[:] Return the ancestor of  whose distance from  is .
\item[:] Return the nearest common ancestor of the nodes  and .
\end{description}


\subsection{The Data Structure}
In order to enable the above queries, we augment the top DAG  of  with some additional information. Consider a cluster  in . Recall that if  is a leaf in  then  is a single edge in  and  stores the labels of this edge's endpoints. Otherwise,  is a cluster of  obtained by merging two clusters: the cluster   corresponding to 's left child and the cluster  corresponding to 's right child. Consider a preorder traversal of . Let  denote the first node visited in this traversal that is also a node in . Let  (resp. ) denote the last node visited that is also a node in  (resp. in ). 
We augment each cluster  with:

\begin{itemize}
\item[] The integers , , and . 
\item[] The type of merge that was applied to  and  to form . If  is a leaf cluster then the labels of its corresponding edge's endpoints in .
\item[] The height and size of  (i.e., of the tree pattern  in ).
\item[] The distance from the top boundary node of  to the top boundary nodes of  and .    
\end{itemize} 
Since we use constant space for each cluster of , the total space remains~. 

\paragraph{Local preorder numbers}
All of our queries are based on traversals of the augmented top DAG . During the traversal we identify nodes by computing preorder numbers  local to the cluster that we are currently visiting. Specifically, let  be a node in the cluster . Define the \emph{local preorder number of }, denoted , to be the position of  in a preorder traversal of . The following lemma states that in  time we can compute  and  from  and vise versa.  


\begin{lemma}\label{lem:localpreorder}
Let  be an internal node of  that corresponds to the cluster  of  obtained by merging the cluster   (corresponding to 's left child) and the cluster   (corresponding to 's right child).
For any node  in , given  we can tell in constant time if  is in  (and obtain ) in  (and obtain ) or in both. Similarly, if  is in  or in  we can obtain  in constant time from  or .
\end{lemma}

\begin{proof}
If  is a merge of  and  of type (a) or (b) then
\begin{itemize}
\item  iff  is the top boundary node of  and  and .
\item  iff  is an internal node of  and .
In this case .
\item  iff  is the shared boundary node of  and ,  , and .
\item  iff  is an internal node in . In this case .
\item  iff  is an internal node in  and . In this case .
\end{itemize}

\noindent Otherwise, if   is a merge of  and  of type (c), (d), or (e) then
\begin{itemize}
\item  iff  is the shared  boundary node of , , and  and .
\item  iff  is an internal node in . In this case .
\item  iff  is an internal node in . In this case .
\end{itemize}
\end{proof}


\subsection{Implementation of the procedures} We now show how to implement the queries using local preorder numbers in top-down and bottom-up traversals of . 



\subsubsection{Access and Depth}
The queries  and  ask for the label and depth of the node whose preorder number in  is . They are both  performed by a single top-down search of  starting from its root and ending with the leaf cluster containing . Since the depth of  is  the total time is .

\paragraph{Access.}
At each cluster  on the top-down search we compute the local preorder number . Initially, the root cluster corresponds to the entire  so we set . Let  be a cluster on the way. If  is a leaf cluster we return the label of the top boundary node if  and the label of the single internal node if . 
 If on the other hand  is an internal cluster with child clusters  and , we continue the search in the child cluster containing . We compute the new local preorder number according to Lemma~\ref{lem:localpreorder}. If   is the shared boundary node between  and  we continue the search in either  or .

\paragraph{Depth}  The only difference between  and   is that during the top-down search we also sum the distances between the top boundary nodes of the visited clusters. Let  be this distance. At the leaf cluster at the end of the search we return  if  and  if . 
Since the distances are stored the total time remains . 




\subsubsection{Firstchild, Level Ancestor, Parent, 
and NCA}
We answer these queries by a top-down search to find the local preorder number in a relevant cluster , and then a bottom-up search to compute the corresponding preorder number in .

\paragraph{Firstchild} We compute  in two steps. 
\paragraph{Step 1: Top-down Search.} We do a top-down search to find the first cluster with top boundary node . We use local preorder numbers as in the algorithm for . Let  be a cluster in the search. If  we stop the search. Otherwise we know that . If  is a leaf cluster we stop and report that  does not have a first child since it is a leaf in . 
 If on the other hand  is an internal cluster with child clusters  and , we continue the search in the child cluster containing . If  is the shared boundary node between  and  we always continue the search in . 
This ensures that we continue to the cluster containing the children of  (recall that  is the deeper cluster in merges of type (a) and (b)). Combined with the condition that we stop the search in the first cluster  where  is the top boundary node (and therefore the last merge before we stop must be of type (a) or (b)), this implies that all children of  are in . 

\paragraph{Step 2: Bottom-up Search.} Let  be the cluster found in Step 1. Since all children of  are in , the node with local preorder number  in  is the first child of . We do a bottom-up search from  to the root cluster to compute the preorder number in  of the node with .  

\paragraph{Level Ancestor and Parent}
Notice that  can be computed as . Since  we focus on    for . This is done in three steps:

\paragraph{Step 1: Compute Depth.} Compute the depth of  as 
. 

\paragraph{Step 2: Top-down Search.} We do a top-down search to find the cluster with top boundary node  of depth  such that  is a descendant of  (we will show that such a cluster exists). During the search we maintain the depth of the current top boundary node as in the algorithm for . At each cluster  in the search we also compute a local preorder number  to guide the search. The idea is that  either corresponds to  or to an ancestor of  within . Initially, for the root cluster  we set . Let  be an internal cluster in the search with top boundary node  and with children  and . If the depth of  is  we stop the search. Otherwise, we proceed as follows.
\begin{enumerate}
\item If  is of type (a) or (b),  is in , and the shared boundary node of  and  has depth , we continue the search in  and set  to be the bottom boundary of . 
\item In all other cases, we continue the search in the child cluster containing , and compute the new local preorder number for . 
\end{enumerate}
Note that if the shared boundary node in case 1 has depth  we continue the search in . Combined with the assumption that , it inductively follows that  becomes the top boundary node at some cluster during the top-down search. Hence, at some cluster in the top-down search the depth of the top boundary node is . 

\paragraph{Step 3: Bottom-up Search.} Let  be the cluster whose top boundary node  has depth  found in Step 2. We do a bottom-up search to compute the preorder number of  in . Finally, we report the result as . 



\paragraph{Nearest Common Ancestor.}
We compute  in the following steps. We assume w.l.o.g. that  in the following since .

\paragraph{Step 1: Top-down Search} We do a top-down search to find the first cluster, whose top boundary node is  (this cluster always exists since ). At each cluster  in the search we compute local preorder numbers  and . The idea is that   and  are either  or  or ancestors of  and  and their depth is at least the depth of . Initially, for the root cluster  we set  and . Let  be a cluster visited during the search. If  is a leaf cluster we stop the search. Otherwise,  is an internal cluster with children  and . We proceed as follows.
\begin{enumerate}
\item If  and  are in the same child cluster, we continue the search in that cluster, and compute new local preorder numbers for  and .
\item If  is of type (a) or (b) and  and  are in different child clusters we continue the search in . We update the local preorder number of the node in  to be the bottom boundary of .
\item If  is of type (c), (d), or (e) and  and  are in different child clusters we stop the search.
\end{enumerate}

\paragraph{Step 2: Bottom-up Search} Let  be the cluster computed in step 1. We do a bottom-up search to compute the preorder number of the top boundary node of  in the entire tree , and return the result. 

\subsubsection{Decompress, Height, Size, and Next Sibling}
To answer these queries,  the
key idea is to compute a small set of clusters representing . This set will be a subset of the set  defined in Sec.~\ref{sec:comparisonsubtreesharing} and will contain all the relevant information.


We need the following definitions. Let  be a node in . We say that  is on the \emph{spine path} in a cluster  if  is the top boundary node in , or  is on the path from the top boundary node in  to the bottom boundary node in . Since clusters are connected subtrees we immediately have the following.

\begin{lemma}\label{lem:spinepath}
Let  be a cluster with left child  and right child . A node  in  is on the spine path of  iff one of the following cases are true: 
\begin{itemize}
\item  is of type  and  is on the spine path in .
\item  is of type  and  is on the spine path in .
\item  is of type   and  is on the spine path in  or . \item  is the top boundary node of .
\end{itemize}
\end{lemma}

Let  be any internal node in . As in Section~\ref{sec:comparisonsubtreesharing}, let  be the leftmost leaf cluster in  such that  is the top boundary node and let  be the path of clusters from the smallest cluster  containing all nodes of  to . We also define  to be the highest cluster on  that has  as the top boundary node, i.e.,  is the highest cluster on  that only contains edges from . Recall that  is the set of  off-path  clusters of  that represent . We partition  into the set  that contains all clusters in  that are descendants of  and the set  that contains the remaining clusters. We characterize these sets as follows.

\begin{lemma}\label{lem:reprCluster}
Let  be an off-path  cluster of  with parent  and sibling . Then
\begin{enumerate}
\item  is in  iff  is a descendant of .
\item  is in  iff  is a merge of type (a) or (b),  is the right child of , and  is on the spine path of . 
\end{enumerate}
\end{lemma}
\begin{proof}
For the first property, first note that if  is in  it is by definition a descendant of . Conversely, if  is a descendant of , we have that . By definition of , we have that  is in . 

Next consider property 2. Suppose that  is in . Then, by Lemma~\ref{lem:offpath} and the definition of  we have that . Furthermore, since  is a proper ancestor of ,  contains edges from both  and , and therefore  must also contain edges from both  and . 

Assume for contradiction that  is of type (c), (d), or (e). Then, the top boundary node  of  is also the top boundary node in  and . Since  by definition of , we have by Lemma~\ref{lem:offpath} that  and thus  cannot be in . 

Hence, assume that  is of type (a) or (b). Assume for contradiction that  is the left child of . Since all clusters on  contain  and  contains edges from both  and , we have that the top boundary node of  is a proper ancestor of . Hence,  cannot be in . 

Finally, if  is of type (a) or (b) and is the right child of , then  iff the top boundary node  of  is a descendant of . But  is a descendant of  iff  is on the spine path of . Hence,  is in  iff  is on the spine path of .
\end{proof}
\medskip

\noindent In the following we show how to efficiently compute  using the procedure . We then use  to implement the remaining  procedures. 





\paragraph{FindRepresentatives}
Procedure  computes the set  and cluster  in two steps. 

\paragraph{Step 1: Top-down Search} We do a top-down search to find the cluster , i.e., the highest cluster on  that has  as the top boundary node. If no such node exists, then  is a leaf node in . 

\paragraph{Step 2: Bottom-up Search} We do a bottom-up search from  and add clusters according to Lemma~\ref{lem:reprCluster} as follows. Initially, set . Let  be a cluster on the path with sibling  and parent .
\begin{enumerate}
\item If  is of type  or  and  is the left child of , add  to .
\item  If one of the following conditions are true, stop  the traversal:
\begin{itemize}
\item  is of type  and  is the right child of .
\item  is of type  and  is the left child of .
\item  is of type  or .
\end{itemize}
\end{enumerate}
Note that, as long as we continue the bottom-up search and consider clusters on the path, we have that  is on the spine path of these clusters. This is because we continue the bottom-up search  according to the cases of Lemma~\ref{lem:spinepath}. It follows from  Lemma~\ref{lem:reprCluster} that the clusters we add to  are exactly the clusters in the set representing . The total time is .


\paragraph{Decompress} 
To compute , we use  to compute the sets of cluster  and . We construct  from  (and ) and the path  computed during the traversal of . First, we decompress all clusters in  (and ) by unfolding their subDAG and constructing their corresponding subtree of . We then combine these subtrees using the merge information stored for each cluster in .   

In total we use  time for  and computing the path . The total time to decompress a cluster  by unfolding is linear in its size. Hence, the total time used is .  

\paragraph{Height} First we compute the set of clusters  and cluster  using . Define the \emph{local height} of a cluster to be the length of the path from the top boundary node to the bottom boundary node if it is an internal cluster, and the height of the cluster if it is a leaf cluster.  We compute the height of  as the sum of the local heights of all clusters in  plus the height of . This correctly computes the height since all clusters in  are merged with their siblings by type (a) or (b). Since the  height and the distance from top boundary node to bottom boundary node for each cluster in  is stored we use  time in total. 

\paragraph{Size} Similar to height. We sum the sizes of clusters in  and  and subtract  (to exclude shared boundary nodes). This also uses  time.

\paragraph{Nextsibling} We compute  directly from  since . 




\section{Conclusion and Open Problems}
We have presented the new top tree compression scheme, and shown that it achieves close to optimal worst-case compression, can compress exponentially better than DAG compression, is never much worse than DAG compression, and supports navigational queries in logarithmic time. We conclude with some open problems.
\begin{itemize}
\item Surprisingly, top tree compression is the first compression scheme for trees that achieves any provable non-trivial compression guarantee compared to the classical DAG compression. We wonder how other tree compression schemes compare to DAG compression and if it is possible to construct a tree compression scheme that  exploits tree pattern repeats and always compresses better than a logarithmic factor of the DAG compression. 
\item Pattern matching in compressed \emph{strings} is a well-studied and well-developed area with numerous results, see e.g., the surveys \cite{Gasieniec1996,Rytter2004,Lohrey2012}. Pattern matching in compressed trees (especially within tree compression schemes that exploit tree pattern repeats) is a wide open area. 
\item We wonder if top tree compression is practical. In preliminary experiments we have compared our top DAG compression with standard DAG compression on typical XML datasets that were previously used in papers on DAG compression. The experiments match our theoretical expectations, i.e., that most trees compress better with top tree compression, and only balanced trees compress slightly better with standard DAG compression.
\end{itemize}




\section{Acknowledgments}
We would like to thank the anonymous reviewer for the important and helpful comments.





\bibliographystyle{abbrv}
\bibliography{paper}




\end{document}
