\documentclass[conference]{IEEEtran}
\bibliographystyle{IEEEtran}
\usepackage{amsmath,amssymb,amsthm,epsfig}
\usepackage{xcolor,graphicx}

\makeatletter
\def\ps@headings{\def\@oddhead{\mbox{}\scriptsize\rightmark \hfil \thepage}\def\@evenhead{\scriptsize\thepage \hfil \leftmark\mbox{}}\def\@oddfoot{}\def\@evenfoot{}}
\makeatother
\pagestyle{headings}


\newcommand{\Prob}[1]{\text{Pr}\{#1\}}
\newcommand{\Exp}[1]{\text{E}\{#1\}}
\newcommand{\Qcal}[1]{\mathcal{Q}(#1)}
\newcommand{\Qcalb}[1]{\mathcal{Q}\Big(#1\Big)}

\newcommand{\Mb}{\mathbf{M}}
\newcommand{\Ib}{\mathbf{I}}
\newcommand{\Wb}{\mathbf{W}}
\newcommand{\vb}{\mathbf{v}}
\newcommand{\xb}{\mathbf{x}}
\newcommand{\zb}{\mathbf{z}}
\newcommand{\qb}{\mathbf{q}}
\newcommand{\Tcal}{\mathcal{T}}
\newcommand{\DDD}{\textsc{Dsync~}}
\theoremstyle{definition}
\newtheorem{rem}{Remark}
\theoremstyle{definition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}

\newcounter{MYtempeqncnt}

\IEEEoverridecommandlockouts
\begin{document}
\title{Discrete Dithered Desynchronization}
\author{\IEEEauthorblockN{Saman Ashkiani\\
School of Electrical and Computer Engineering \\ 
University of California, Davis \\ Email: sashkiani@ucdavis.edu}
\and
\IEEEauthorblockN{Anna Scaglione\\School of Electrical and Computer Engineering \\ 
University of California, Davis \\ Email: ascaglione@ucdavis.edu}
\thanks{This work was supported by National Science Foundation under
grant NSF-E70-8502.}
}
\maketitle


\begin{abstract}
This paper introduces the Discrete Dithered Desynchronization (\DDD) algorithm which is a
decentralized Time Division Multiple Access (TDMA) technique in which a set of network 
nodes computes iteratively a conflict-free
schedule so that each node obtains a portion of a frame that is an integer multiple of a fixed
slot size. 
The algorithm is inspired by the dynamics of Pulse Coupled Oscillators (PCO), but unlike its
predecessors that divide arbitrarily the frame among the nodes in the network, the 
\DDD allocates discrete resources among the network nodes.

Our paper proves the convergence of the \DDD algorithm and gives an upperbound on the convergence time
of the algorithm.  
\end{abstract}
\begin{IEEEkeywords}
Pulse coupled oscillators, desynchronization, and decentralized scheduling.
\end{IEEEkeywords}
\section{Introduction}
Agreeing on a common timing (syncrhonization) and agreeing on a Time Division Multiple Access (TDMA) schedule are related problems. In centralized scenarios there is a fixed infrastructure of nodes (masters) that optimizes the schedule and communicates them back to the other nodes (slaves). In infrastructure-less networks TDMA schedules are typically attained using decentralized graph coloring algorithms  (e.g. \cite{herman2004distributed} and \cite{rhee2006drand}). In the first algorithm time synchronization between the nodes is presumed, and the second algorithm uses extra control messages to indicate the system state.  

Synchronization schemes themselves can be divided into these two categories. Centralized algorithms rely on the broadcast signal coming from some fixed units, equipped with accurate timing devices; all nodes synchronize their clocks independently. The main example is provided by the Global Positioning System (GPS). 
In decentralized algorithms, instead, reference signals are generated at random or by a GPS signals, but then all users share information about the reference signals they sense, either by flooding the time stamp (e.g. \cite{MaKuSiLe04}) or by reaching consensus (e.g. \cite{ElGiEs02}). 

Due to the issue of coverage, centralized algorithms have a considerable infrastructure cost.
Decentralized methods are cheaper and less vulnerable to failures; however, they are slower and generally less accurate than centralized methods.  



In the context of infrastructure-less networks, great attention has been paid over the past ten years to biologically inspired synchronization algorithms based on the Pulse Coupled Oscillators models \cite{strogatz2003sync}. These designs can overcome the
necessity of a reference signal for synchronization, since a reference signal emerges from the collective transmissions of the nodes. 
All PCO based algorithms, in fact, borrow a key implicit technique to communicate and update the scheduling decisions, which is through the emission of a special signal, called {\it firing} signal.
The nodes in range of the firing node, update a local state variable as they detect a firing event. Another variant of time synchronization is desynchronization \cite{nagpal}, which can be used to attain a TDMA schedule in continuous time in a decentralized fashion.  More specifically in \cite{nagpal} the authors proposed the algorithm called \textsc{Desync}  for a fully connected networks to attain a uniform TDMA schedule. Several different methodologies were utilized afterward to achieve deysnchronization, such as \cite{motskin2009lightweight} and \cite{Choochaisri2012}. In the former, desynchronization is aimed via a graph coloring algorithm using a message-passing method with a constraint on the complexity of the messages. While in the latter, the proposed algorithm uses a PCO based method with a new kind of updates based on the firing signals received from all other nodes. Other variants of the desynchronization were proposed later with various conditions and objectives. In \cite{pfs}  a \emph{Proportional Fair Scheduling} (PFS) algorithm was proposed to achieve a time schedule in which nodes are assigned a fraction of the frame proportional to their relative demand.  In \cite{anchored}, desynchronization is gained with another constraint on having a single node with a fixed time reference in the network. In general, compared to decentralized coloring, PCO based scheduling eliminates the need of synchronization to attain TDMA scheduling. However, most of the PCO based scheduling algorithms work under the condition of full connectivity in the network. In \cite{degesys2008towards} the authors tried to extend the \textsc{Desync} to multi-hop networks. 

The objective of this paper is to deal with the problem of providing a desynchronization algorithm that assigns not continuous but discrete portions of the frame. This modification is motivated by the fact that firing signals and modulated signals that are sent by the nodes are possibly of non-negligible duration compared to the frame size and, therefore, nodes cannot really use an arbitrarily short amount of the frame duration for their transmission. 

Our paper shows that, while quantizing the results of desynchronization leads to undesired fixed points for the algorithm, the algorithm we proposed, coined \emph{Discrete Dithered Desynchronization} (\DDD), converges almost surely to the desired resource allocation. A version of this protocol
was proposed in our previous work in \cite{PagliariRamy}, but  the effect of dithered quantization was assessed only by simulations.
The analysis we carry out in this paper also provides bounds on the convergence time of the algorithm. Some of our analysis takes inspiration from the methods used in \cite{quant} to prove the convergence of \emph{Quantized Consensus}. 

This paper is organized as follows, in section \ref{sec:DDD}, we introduce the \DDD algorithm. In order to analyze the characteristics of this algorithm, an equivalent consensus problem is defined in section \ref{sec:equi_prob}, and it is proved that this problem converges to a fixed group of states. In section \ref{sec:time}, an upper bound for the worst case expected convergence time of the \DDD algorithm is computed. Simulation results are shown in section \ref{sec:sim}, and a conclusion is made in section \ref{sec:conclusion}.
\section{Discrete Dithered Desynchronization Algorithm}\label{sec:DDD}
Consider a fully connected network  with  nodes each with a discrete time counter  for . Let the time unit for counters be  and assume that the node counters can only take values from the finite set , where .  Like an alarm that is advancing towards its termination, the counters advance in time as , where  for all  denotes their initial phases. 
Each node broadcasts a firing signal to the network upon overflow in its counter, i.e. when  is reset to 0 after  periods. In the rest of the paper, similar to \cite{nagpal}, the time that elapses between the firing of two consecutive counters is the time allocated to each node, in the order of firing. Without loss of generality we can relabel nodes to have . It is implicitly assumed that no two counters can acquire the same value initially\footnote{The assumption is that in the initial random configuration no two nodes have chosen the same counter. If this condition is violated there is a collision and the nodes will try again.}. Nodes update their own counters when they sense firing signals from others and, in the absence of processing errors, desynchronization algorithms preserve the ordering in their evolution. Throughout the article we assume that all the additions and subtractions of the nodes indices are done modulo , unless otherwise is stated. 

Because their initial position is random the nodes do not share a fair schedule, and the objective of \DDD update rule is to have the counters converge to a configuration for their counters within the frame approaching a uniform discrete TDMA schedule. It is conventional to describe the configuration of the counters over time around a circle that represents the frame duration, as shown in Fig. \ref{fig:network_model}.
\begin{figure}[htb]
\centering
\vspace{-0.1in}
\begin{minipage}[b]{0.8\linewidth}
\centerline{\includegraphics[width=1\linewidth]{DD_config_1.pdf}}
\hfill
\end{minipage}
\vspace{-0.2in}
\caption{Network  with 6 nodes at , where , , , , ,  and . The grey nodes inside represents a random deployment of the nodes in , and the green nodes on the circle are their counters around a dial, which advance in time towards the finish line, at the north pole.}
\label{fig:network_model}
\end{figure}
Given that the order is based on the temporal proximity of the counters, we will refer to as {\it time-neighbors} nodes whose indexes differ by one unit.   
However, as can be noted from  Fig. \ref{fig:network_model} the nodes geographical position is unrelated with the nodes relative counters position on the dial, which we assume to be completely random.  For simplicity, we assume that all nodes can hear each other; strictly speaking, as clarified next,
all we need is that nodes can hear their  time-neighbors.  

The \DDD~is a modification of the \textsc{Desync} algorithm. In \textsc{Desync} the update for node  occurs when node  
sends its firing signal at , so that its counter is closer to the midpoint (in time) of the  and  counters:
   
where , and  denotes the time right after the updates resulted from node  firing signal. Because node  just fired, its counter is reset at time , i.e. . Also, since only node  updates its counter in the network at , the counter of node  remains unchanged . Therefore, \eqref{eq.one} is equivalent to:

A simple way to modify this algorithm to obtain a discrete schedule so that  is to uniformly quantize the counter with the mapping . However, as it will be discussed in Lemma \ref{lemma:uniform_q}, this approach may not  converge to a desired fixed point. Hence, in \DDD we used a randomized , which is a dithered quantization \cite{wannamaker2000theory} over  defined as follows:
 
where  and  be a random variable, uniformly distributed over .  and  are statistically independent. Let , then from equation (\ref{eq:dither}) distribution of   can be written as 

The idea is analogous to the \emph{probabilistic quantization} used in \cite{can} to ensure the convergence of an average quantized consensus policy.

Using dithered quantization (\ref{eq:dither}) on (\ref{eq:desync_psi1})  is mapped onto :

Thus, \emph{Discrete Dithered Desynchronization} (\DDD)  algorithm works as shown in Fig. \ref{fig:DDDesync1} and {\ref{fig:DD_update}}.
\begin{figure}[htb]
\centering \vrule
\begin{minipage}{\linewidth}
\hrule \vspace{0.25cm}
\begin{minipage}{0.95\linewidth}
\def\baselinestretch{1} 
\begin{center} \DDD Algorithm: \end{center}
\begin{itemize}
\item[] {\bf Sending state}:
\item[] Node  fires when its counter overflows (i.e.  such that ).
\item[] Node {} resets its counter  after firing.
\end{itemize}
	\begin{itemize} 
	\item[] {\bf Receiving state}:
		\item  Node  receives the first firing signal after its Sending state (from node ) at \mbox{}.
		\item .
	\end{itemize}
\end{minipage}
\vspace{1em} \hrule
\end{minipage}\vrule \\
\caption{The \DDD algorithm for node . When its counter overflows (Sending state), or when it receives the firing signal (Receiving state). Note that all the node needs is to follow the evolution of its own counter and wait until it hears a node fire after its own firing event. The use
of indexes to refer to the nodes is for the convenience of our description, but it is totally irrelevant for the update.}\label{fig:DDDesync1}
\end{figure}
\begin{figure}
\centering
\vspace{-0.15in}
\begin{minipage}[b]{0.6\linewidth}
\centerline{\includegraphics[width=1\linewidth]{DD_config_3.pdf}}
\hfill
\end{minipage}
\vspace{-0.3in}
\caption{\DDD updates: Time evolution model of network  with 6 nodes at , where , , , , ,  and .} 
\label{fig:DD_update}
\end{figure}


It should be clear at this point that, if there are no errors, the order of firings will be preserved. In fact, if initially all counters have at least a unit difference between each other, by using the semi linear update (\ref{eq:desync_psi1}) the updated counter cannot cross the two fixed time-neighbors which are both on . In other words, the midpoint never falls inside an adjacent quantization bin of any two time-neighbors, and hence it is impossible to be quantized to any of their values.

It is important to note that node  does not have direct access to other nodes' counters (i.e.  and ). The only information it acquires is the counter difference between its own counter and the node which last fired, prior to its own firing event, and hence immediately after it. In particular in the \DDD algorithm, node  can compute the exact counter difference with node  and only an estimation of the counter difference with node . For the rest of this article, we assume that node  has exact information about the counter difference with the two time-adjacent neighbors (i.e. nodes  and )\footnote{ The assumption amounts to considering an ideal Physical (PHY) Layer, that detects and estimates perfecting the epoch of the firing signal. Simulations done in \cite{pfs} have shown that the algorithm is robust and can recover from errors (false alarms or missed detection).}. 

The key ingredient of our incoming analysis, is a mapping of the counters used in \DDD onto a set of auxiliary variables. In fact, for the sake of the analysis it is preferable to consider the counter differences instead of the absolute counter values, since the sought TDMA schedule is equivalent to attain consensus on the counter differences. More specifically, let  denotes the difference between consecutive nodes counters, defined as follows:
 
In order to rewrite the update equations based on the counter differences, after substituting (\ref{eq:def_q}) in (\ref{eq:desync_psi1}) and (\ref{eq:desync_psi2}):

It is clear from the definition in (\ref{eq:def_q}) and Fig. \ref{fig:network_model} that for any  the sum of all counter differences remain constant 

Hence, as  and  are the only participants in the update equation (\ref{eq:desync_q2}) at time , the sum of the two counter differences should be preserved after the update:


As it was stated before, our final objective toward the implementation of a TDMA schedule over discrete resources is to divide a finite number of time slots equally among all the nodes in the network. But, this is only possible when  for some integer , and therefore each node will have  slots (i.e.  for all ). We define more precisely what we mean by a TDMA schedule in general in the following definition:
\begin{definition}
\emph{TDM state}: If  for , then we denote TDM states as all subdivisions of the slots among the nodes in which  of nodes have  slots and others have  slots.  
\end{definition}
In the following section we define an equivalent description of the evolution of the network state that eases the analysis of the behavior of the \DDD algorithm. The goal of our analysis is first to prove that the network dynamics will converge to one TDM state from any initial condition. We also provide a bound on the longest expected amount of time it takes to reach a TDM state, starting from the set of worst initial conditions.  
\section{Analysis of the equivalent consensus problem}\label{sec:equi_prob}
As stated at the end of the previous section, in \DDD the values of the counters' differences in (\ref{eq:desync_q2}) at any firing event fully determine the evolution of the network  schedule over time. It is assumed that all counters have a synchronized rise edge. Thus,  (which is the counter difference between the pair of nodes  and ) may only change right after the firing events at  or , and it remains unchanged for all other firing events until the next round. Hence, we state that \DDD corresponds to the equivalent consensus problem described next. 
Suppose  is a ring graph with  vertices , each possessing a value  for . These vertices are connected to each other by the ring graph  as depicted in Fig. \ref{fig:DD_eq}, where a single edge, named as \emph{active edge}, is distinguished from all other \emph{inactive} edges. The two vertices connected to the active edge are called active nodes. The active edge also rotates counter clock-wise. Let each disposition of the active edge in time represents an event in  denoted as . For instance, if  is an active edge at , then  will be an active edge at . Now, if  for all  and , and also at each event active nodes perform an interaction on their values based on update equations similar to (\ref{eq:desync_q2}) and (\ref{eq:q_const2}), then this consensus problem would be equivalent to performing \DDD algorithm on network . In general if  is an active edge at , then the stored values  and  are updated as follows:


where , and  and  represent the updated values of  and  respectively and right before . Note that for any  and , it follows that . Thus, as , (\ref{eq:active1}) can be written as

\begin{figure}
\centering
\begin{minipage}[b]{0.6\linewidth}
\centerline{\includegraphics[width=1\linewidth]{DD_eq.pdf}}
\hfill
\end{minipage}
\vspace{-0.2in}
\caption{The equivalent consensus problem for graph  with N nodes. The active edge is currently , and it will rotate in a counter clock wise direction at each time step.} 
\label{fig:DD_eq}
\end{figure}


In the following parts, we investigate the behavior of the network's state  in an evolving graph . Our objective is to show that the \DDD algorithms converges to the TDM states almost surely. The effect of uniform quantization in \eqref{eq:active1} is discussed in the following lemma.
\begin{lemma}\label{lemma:uniform_q}
In the \DDD equivalent problem, if uniform quantization is used instead of the dithered quantization. Then there exists cases where the algorithm converges to a non-TDM state.
\end{lemma}
\begin{proof}
The proof for this lemma is in Appendix \ref{App:uniform}.
\end{proof}
\begin{definition}
Let  be an active edge in graph  at . Then, the possible interactions are:
\begin{enumerate}
\item \emph{Null}: When  and .
\item \emph{Swap}: When  and .
\item \emph{Compression}: When .   
\end{enumerate}
\end{definition}
The following lemma discusses the behavior of the interactions in the \DDD equivalent consensus problem.
\begin{lemma}\label{lemma:DD_prop}
Let  be an active edge in graph  and , then:
\begin{enumerate}
\item If , with probability one .
\item  If , the updated values will have the same difference afterward .
\item If : 
	\begin{enumerate}
	\item If : 
	
	\item If :
	
	\end{enumerate}
\item After each interaction .
\end{enumerate}
\end{lemma}
\begin{proof} The proof for this lemma is in Appendix \ref{app_1}. \end{proof}
This lemma characterizes all the possible interactions in . Let  be an active edge in . If both of the active vertices have the same values, with probability one the null interaction will happen. Also, if the values of the active vertices have more than one unit difference (like in Lemma \ref{lemma:DD_prop} case 3), the smaller one will increase and the bigger one will decrease, i.e. the interaction is a compression.
It is important to note that in general the difference between the values would decrease with a positive probability, which depends on the choice of .
\begin{corollary}\label{cor.prob}
If active nodes have a difference of  in their values (Lemma \ref{lemma:DD_prop} case 2) two cases may happen: 1) a null interaction, with probability , or 2) a swap interaction, with probability . 
\end{corollary}

Based on the results of Lemma \ref{lemma:DD_prop}, it is simple to see that the properties of the \DDD equivalent consensus problem are similar to the Quantized Consensus in \cite{quant} on the ring graph . Reaching an average quantized consensus over  is equivalent to having the desired discrete desynchronization in the original network . There are two important differences, however, between the \DDD algorithm and the Quantized Consensus: 1) The edge selection in our algorithm is deterministic (i.e. the active edge deterministically rotate counter clockwise), while in the Quantized Consensus the edges are chosen at random. 2) In our algorithm the updates are randomized, similar to \cite{can}, while in Quantized Consensus there is a deterministic operation over the chosen edge. The random evolution of the network state  in  is a Markov chain. In fact, to specify the distribution of the next state  all is needed is the current position of the active edge and the current network state  (in particular the values of the active nodes). Also in the equivalent consensus problem  and  (based on the constraints of the \DDD algorithm). Consequently, there are finite number of possible states for network . Because of the deterministic movement of the active edge, \DDD equivalent consensus problem  is a cyclo-stationary Markov process. In contrast, the Quantized Consensus in \cite{quant} and the average consensus method in \cite{can} form stationary Markov processes, and hence the evolution can be modeled as a homogenous Markov chain. This difference is particularly important in characterizing the convergence time, which is more complex for \DDD than for Quantized Consensus. Establishing almost sure convergence is simpler, and done to prove the following results. 






\begin{theorem}\label{thm:AS}
In the \DDD equivalent consensus problem on network , TDM states are absorbing states.
\end{theorem}
\begin{proof}
Let  denotes the set of all TDM states in .
It suffices to show that once the network is in a TDM state (i.e. ), the updated network will also be in one of the TDM state (i.e. ).
Suppose  and  is the active edge. Then for those interactions in which both vertices of the active edge have equal value (i.e. ), based on Lemma~\ref{lemma:DD_prop} case 1, . Therefore, . However, for those interactions in which , based on Lemma~\ref{lemma:DD_prop} case 2, , which means either a null or swap interaction will occur for  and . In either case, all the other inactive vertices will have their values unchanged and, hence, , which proves the statement. 
\end{proof}


\begin{theorem}\label{thm:convergence}
In \DDD equivalent consensus problem on network , the TDM states are the only absorbing states and the network converges to the TDM states almost surely.
\end{theorem}
\begin{proof}
Let  be the initial state of the network . We want to show that after sufficient number of interactions, with probability one, the network reaches the absorbing state:
 
Let  denote the range between the maximum and minimum  in network  at the th time step. 
Based on Lemma~\ref{lemma:DD_prop} case 4, after each interaction the difference between the values of the two active vertices is a non-increasing quantity, i.e. . In particular,  when there is a single node with the maximum (or minimum) value in the network which participate in a compression. Also,  only when the network is in ; in fact,  only when  and  otherwise. Instead,  for all the states which are not in . Therefore, to prove the theorem, it is sufficient to show that while , , for some . In other words, it is possible for the network to move into a state with lower range after finite number of interactions. Suppose the network state is not in . As the active edge rotates over the entire network from this initial state, there exists a time step  in which one of the two vertices of the active edge has the value . We denote that active vertex as . Since, , there should exist at least one other node in  with value lower than , otherwise the initial network state considered must be in , which is a contradiction. Now moving counter clockwise from , let  be the closest node such that: 

Then all the nodes between  and  (moving counter clockwise) have values equal to either  or . Based on Lemmas \ref{lemma:DD_prop}-1 and \ref{lemma:DD_prop}-2, with positive probability the  value of  can swap until  , and then, with positive probability, a compression will happen on the active edge . Depending on the number of nodes having the value equal to , by following the previous steps, one can prove that, with positive probability, the number of nodes with maximum value is reduced by one, until all of them have participated in a compression, which is the case where  is strictly decreased. Thus, independent of the initial state, with positive probability  is decreased step by step and with a finite number of interactions, until it reaches the minimum value which correspond to an absorbing state.  Since, the sum of the  is preserved during each interaction, it is not necessary to keep track of the minimum values, but the same process happens on the minimum values too.

It was shown that, starting from any state that  (i.e. states which are not in ), with positive probability  is strictly decreased after sufficient number of interactions. Thus, the only possible absorbing states are  which .
\end{proof}
\section{Analysis of the Convergence Time for the \DDD Algorithm}\label{sec:time}
Up to this point, it was shown that irrespective of the initial network state (i.e. ), after a sufficient number of interaction the system reaches a TDM state in  almost surely. It is now of interest to evaluate the expected number of interactions  needed for a system to reach a state in . 

Let  be the set of all possible states in the network , and \mbox{}, where . A similar function is defined in \cite{quant} to characterize the dynamics of the average consensus with quantized values.
Similar to Lemmas 4 and 5 in \cite{quant}, it can be shown that:
\begin{lemma}\label{remark1}
If ,  has the following properties for the network  with :
\begin{enumerate}
\item .
\item After each interaction: 

\end{enumerate} 
\end{lemma}
\begin{proof} As , and , , which can only occur when all other values are equal to . Thus, the first property is achieved by modifying Lemma 4 in \cite{quant} by the following bounds. The second property is another representation of Lemma 5 in \cite{quant} for this scenario.\end{proof}

 is a positive function which is decreasing in , and will reach its minimum once the system reaches the TDM states (i.e. ). The intuition behind this fact is that once the system is in , all  are either equal or can only have a unit slot difference, then from the second property of Lemma \ref{remark1}  does not change anymore. Also, from Theorem \ref{thm:convergence} the system will eventually reach the TDM states in , equivalently  will reach its minimum.

As it was mentioned earlier, the dynamics of the network  can be modeled as a Markov chain. Let  be the absorption time of the Markov chain. Let   be a random variable defined as the number of interactions, starting from step  and state , until the first compression in the network . Let  be the maximum expected number of interactions one has to wait until network  experiences a compression. Then based on the results from Lemma \ref{remark1}, it can be shown just like in \cite{quant} that:

The intuition behind the inequality (\ref{eq:bounds1}) is as follows: Based on the second property of Lemma \ref{remark1}, depending on the initial state ,  for ,   will decrease only if a successful compression happens. In the worst case,  is maximum and at each compression it is decreased by the smallest possible amount, which is by . Also in this scenario, two consecutive compressions  happen after at most  number of interactions (in expectation), given the fact that at the th time step a compression occurred. Next we provide the main analytical result of this paper:
\begin{theorem}\label{thm:timebound}
In the network  with  nodes and update parameter , the worst case maximum expected number of null and swap interactions until a compression occurs, , can be calculated for large  as:

where . The exact value of  for any  can be found in equation (41) in Appendix \ref{app_2}.
\end{theorem}
\begin{proof} The proof of this theorem is in Appendix \ref{app_2}.\end{proof}
Based on Theorems \ref{thm:timebound} and (\ref{eq:bounds1}), the greatest expected number of interactions to reach a TDM state  is upper bounded by . Also,  is a constant factor which is an strictly increasing function of  for . As it will be shown in simulation results in Section \ref{sec:sim}, in general this upper bound is not tight for the \DDD algorithm, and it represents an upper bound for the worst case possible.
\section{Simulation Results} \label{sec:sim}
In this section performance of the \DDD algorithm in TDMA scheduling and also its convergence time is assessed by using computer simulations.

The \DDD algorithm has a single parameter  to be chosen. It represents the algorithm's inertia in updating the current phase counters based on the received firing signal. As it is clear from \eqref{eq:thm3}, large  shows more resistance toward the update, and hence its convergence time is greater in comparison with a smaller .

In Fig. \ref{fig:Convergence}, convergence of the \DDD algorithm to the TDM states is shown under two different scenarios. Network  is consisted of  nodes with . In (a)  and the network has a single TDM state (i.e. ), while on (b)  and hence there is not a single TDM state. After six round of firings, where each round is defined as a complete cycle of firing events in the network and by all of the nodes, the network is absorbed into . There are  different TDM states and the network changes its state among these states afterward.

\begin{figure}
\centering
\begin{minipage}[b]{0.48\linewidth}
\centerline{\includegraphics[width=1.0\linewidth]{Convergence_strict.pdf}}
\centerline{(a)}\medskip
\hfill
\end{minipage}
\begin{minipage}[b]{0.48\linewidth}
\centerline{\includegraphics[width=1.0\linewidth]{Convergence_weak.pdf}}
\centerline{(b)}\medskip
\hfill
\end{minipage}
\vspace{-0.35in}
\caption{Convergence of the \DDD algorithm over two scenarios for a network  with 6 nodes and : (a) , where there exists a unique absorbing state. (b) , where . Both scenarios are converged after 6 rounds of firings.} 
\label{fig:Convergence}
\end{figure}
Number of interactions until absorption for the \DDD algorithm is depicted in Fig. \ref{fig:time_Convergence}-a. As it is clear from the figure, the upper bound in \eqref{eq:bounds1} is very pessimistic, because it is based on the performance of the worst possible case  as for the initial state. Theorem \ref{thm:timebound} found an expression for the expected number of interaction until the next compression in network . As it is explained in Appendix \ref{app_2}, this is the case where all nodes have a common value, except for the two of them, one of which a unit higher and the other a unit lower than the common value (also shown in \eqref{eq:App_dist}). The average number of interactions until absorption for this case is shown in Fig. \ref{fig:time_Convergence}-b. The evaluated absorption time in Appendix \ref{app_2} which was discussed in Theorem \ref{thm:timebound} is also shown in this figure by dashed lines. 
\begin{figure}
\centering
\begin{minipage}[b]{0.48\linewidth}
\centerline{\includegraphics[width=1.0\linewidth]{DD_time_normal.pdf}}
\hfill
\end{minipage}
\begin{minipage}[b]{0.48\linewidth}
\centerline{\includegraphics[width=1.0\linewidth]{DD_time_wsc.pdf}}
\hfill
\end{minipage}
\vspace{-0.2in}
\caption{(a) Network  with . Number of interactions until absorption versus  averaged over  different random initial states. (b) Network , with all nodes having a common value, except for two of them, one of which a unit higher and the other a unit lower than the common value. Number of interactions until absorption versus  is depicted averaged over all possible initial positions and each with   independent trials. The dashed lines represent the theoretical number of needed interactions computed in Theorem \ref{thm:timebound}.} 
\label{fig:time_Convergence}
\end{figure}


\section{Conclusion}\label{sec:conclusion}
In this paper we showed that by using the \DDD algorithm, desynchronization and , hence, TDMA scheduling is possible over discrete resources and in a decentralized manner. This approach can also be used to reach TDMA in more realistic situations, such as allocating resources based on each node's demand, or also considering the case where rise edge times of counters are not matched.
\appendices
\section{Proof of Lemma \ref{lemma:uniform_q}.}\label{App:uniform}
It was stated in Section \ref{sec:DDD} that by using uniform quantization on \eqref{eq:desync_psi1}, the algorithm may not converge. We continue our discussion on the \DDD equivalent consensus problem. Let , then by replacing the uniform quantization in \eqref{eq:active1_2} (note that  denotes the uniform quantization):
 
where the right hand side equality is because for any , . Similarly, if , then 

In a network with ,  is a fixed point, but it is not a TDM state.
\section{Proof of Lemma \ref{lemma:DD_prop}}\label{app_1}
To prove statement 1) is true we observe that, since , by replacing  in equation (\ref{eq:active1}), , and consequently from (\ref{eq:active2}) . 

To prove statement 2), we consider equation (\ref{eq:active1_2}). Two cases may happen. First, suppose . Then 
,
and since , then . Thus:

In either case, it can be inferred from (\ref{eq:active2}) that:

Thus in general, 

Second, assume . 
Note that , because of the fact that the distribution only depends on the distances and hence any transformation on  which keeps distances unchanged (like taking the complement) has the same exact effect on . Using this fact, and equation (\ref{eq:active1_2}), the update can be written as 
.
With similar reasoning as before, the updated difference can be finally written as:

Therefore this proves that, if , after the update .

For the proof of statement 3), suppose without loss of generality that ,  and . By using equation (\ref{eq:active1}),
,
and hence,

Now, since  is a positive value, the right hand side is greater than or equal to the left hand side, an hence . The sum of the two values updated will be preserved after the update, thus, from equation (\ref{eq:active2}) it can be  readily concluded that  and the first part of the statement is proved. For the second part, the proof is  the same as the first one because of the symmetry between the two cases.

Statement 4 combines the previous ones. If  or . If , ,  suppose that . Thus in equation (\ref{eq:lem_other1}), if  and  the second term in the right hand side can be written as

Thus, equation (\ref{eq:lem_other1}) can be manipulated as follows:

A linear combination of equation (\ref{eq:active2}) and (\ref{eq:lem_other3}) is

Since  and ,  . Similarly,  if  by the same approach it can be shown that  , and this completes the proof.
\section{Proof of theorem \ref{thm:timebound}}\label{app_2}
\begin{figure*}[!t]
\normalsize
\setcounter{MYtempeqncnt}{\value{equation}}
\setcounter{equation}{40}

\setcounter{equation}{\value{MYtempeqncnt}}
\hrulefill
\vspace*{4pt}
\end{figure*}
For the proof of \ref{thm:timebound} we imitate the clever methodology used in \cite{quant} to calculate . By definition  is the expected number of interactions needed, in the worst case scenario, for network  to have its next compression, starting from iteration . For a compression to happen, it is necessary that the active edge  is such that . Thus, the worst case (in expectation) is when there are the least possible number of potential nodes in the network  that can participate in a compression, i.e. only two nodes  and  have values  and all remaining values are equal. In this case, to fall in  the absorbing state, one would have to wait for a compression of the two outliers, while in all other interactions  the value of  will be unchanged. It is clear that any other initial state increases the probability of having a compression, and therefore this is the worst case scenario. It should be noted that if , the stated worst case scenario happens only when  and all other cases (i.e. , ) require less time in expectation.

Let us now focus on the worst case scenario, where there exists two nodes  and  such that:

Any compression of the values that differ by  from each other (initially at  and ) will lead to a TDM state, which is an absorbing state. Otherwise, the outliers can change position, but there will always be only two of them. Thus, the evolution of the network can be viewed as a Markov chain with a single absorbing state () and  transient states that are all possible permutations of positions for the outliers around the ring graph . In the following we study this Markov chain, and derive how long it takes on average for these two values, which are outliers, to get into a compression thereby settling the network in a TDM state. 
\begin{figure}[htp]
\centering
\centerline{\includegraphics[width=1.0\linewidth]{MC_appendix.pdf}}
\hfill
\caption{The Markov chain for analysis of the absorption time of the proposed problem. The state space is . Transition probabilities are also shown on each arc. The expected number of interactions starting from each state is written inside the corresponding circle.}
\label{fig:MC_app}
\end{figure}
The state space is defined as , where , and  represents the relative position of the node with value  with respect to the node with value  (e.g.  in the case of \eqref{eq:App_dist}), and  represents  all the  possible cases of the current active edge, where if  then the active edge  is assumed to be labeled as  and others are labeled until .   denotes the absorbing state. So, without loss of generality we have fixed our reference to the node with value  and the current state is identified based on that.
In Fig. \ref{fig:MC_app} this Markov chain is shown, and the circle in row  and column , represents the state . In Fig. \ref{fig:MC_app}, the expected number of interactions until absorption starting from each state, is written in the corresponding circle. The transition probabilities of the Markov chain can be derived based on the update equations \eqref{eq:active1} and \eqref{eq:active2}. Recall that there is only one possible compression which can occur on the two outlier values, and it is possible only if both of their vertices are connected to the active edge. Based on the outlier values from (\ref{eq:App_dist}) and replacing them into equations (\ref{eq:active1}) and (\ref{eq:active2}), the probability of a compression in this case is  and, hence, the probability of a null interaction is . If the active edge has two nodes in consensus, the state moves forward with probability one to activating a new edge with a distance between the outliers unchanged. For all other cases where the interaction is between an outlier value and non-outlier one, we know from Corollary \ref{cor.prob} the probability of a swap is  and the probability of a null interaction is . There exists a symmetry in this Markov chain, which can be easily assessed by the choice of the shown variables , and  and the path through the absorbing states  in Fig. \ref{fig:MC_app}.
The expected number of interactions until absorption can be calculated as:

Based on the Markovian property, , and  for all valid  and . By eliminating all the  variables, the following equations can be written for all  and :

for all  and . Summing over all  and  in \eqref{eq:sol2} and \eqref{eq:sol3}, together with \eqref{eq:sol1},  and  can be calculated as:

The other unknowns can be calculated recursively for  as:

where , , and .  can be calculated based on \eqref{eq:rec1} and by using  and  as:

Considering all of the solutions by equations \eqref{eq:rec1}, it is not hard to see that , and . So, by using  and  we can calculate  and  as follows:



Also, by doing some algebraic manipulations we have:

Substituting \eqref{eq:sum_x}, \eqref{eq:sum_y} and \eqref{eq:sum_z} into \eqref{eq:exp_main}, the expected number of interactions until absorption () can be computed as in equation (41) at the top of the page. It can also be seen that for large N,  where .
\bibliography{CPS-proposal,mac-references,cotdm}
\end{document}