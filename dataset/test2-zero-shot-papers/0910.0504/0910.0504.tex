\documentclass{article}
\pdfoutput=1
\usepackage{fullpage,hyperref}
\usepackage{xcolor}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black},
    pdfauthor={Jos√© A. Soto},
    pdftitle={Improved Analysis of a Max-Cut Algorithm Based on Spectral Partitioning.}
}
\usepackage{mathtools,amsmath}
\usepackage{amsthm,amssymb}
\usepackage{algpseudocode,algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\algdef{SE}[IF]{NoThenIf}{EndIf}[1]{\algorithmicif\ #1}{\algorithmicend\ \algorithmicif}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{prop}[theorem]{Proposition}
\theoremstyle{definition}
\newtheorem{defn}[theorem]{Definition}
\theoremstyle{remark}
\newtheorem{rem}[theorem]{Remark}
\numberwithin{equation}{section}


\newcommand{\R}{\mathbb R}
\newcommand{\OPT}{\mathrm{OPT}}
\newcommand{\Adj}{\mathrm{Adj}}
\newcommand{\Deg}{\mathrm{Deg}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Good}{\mathrm{Good}}
\newcommand{\Bad}{\mathrm{Bad}}
\newcommand{\Cross}{\mathrm{Cross}}
\newcommand{\Inc}{\mathrm{Inc}}
\begin{document}
\title{Improved Analysis of a Max-Cut Algorithm Based on Spectral Partitioning.}


\author{Jos\'e A.~Soto\thanks{
Departamento de Ingenier\'ia Matem\'atica and Centro de Modelamiento Matem\'atico (UMI
2807 CNRS), Universidad de Chile. \url{jsoto@dim.uchile.cl}. Supported by FONDECYT 11130266 and N\'ucleo Milenio Informaci\'on y Coordinaci\'on en Redes ICM/FIC P10-024F.}}


\maketitle
\begin{abstract}
Trevisan [SICOMP 2012] presented an algorithm for Max-Cut based on spectral partitioning techniques. This is the first algorithm for Max-Cut with an approximation 
guarantee strictly larger than 1/2 that is not based on semidefinite programming. Trevisan showed that its approximation ratio is of at least 0.531. In  this paper we improve this bound up to 0.614247. We also define and extend this result for the more general Maximum Colored Cut problem.
\end{abstract}

\section{Introduction}
The maximum cut (Max-Cut) problem consists of finding a bipartition of the vertices of a weighted graph that maximizes the total weight of the edges crossing it. Max-Cut is one of Karp's original NP-complete problems, so finding heuristics and approximation algorithms for it has 
attracted researchers for years. It is easy to find a solution for Max-Cut whose weight is at least half of the optimum: a partition chosen 
uniformly at random will cut, in expectation, half of the total weight of the graph, and this process can be derandomized using standard 
techniques.
No approximation asymptotically better than 1/2 was known for this problem until Goemans and Williamson \cite{GW} devised an algorithm based on 
semidefinite programming (SDP) yielding a  approximation. Although  SDP is 
solvable in polynomial time, it is still computationally expensive in practice, so it is an intriguing question to develop approximation algorithms 
for Max-Cut that do not use SDP. Semidefinite programming was the only known method to achieve a guarantee of more than a half until 
Trevisan~\cite{Luca} devised an algorithm based on eigenvector computations whose approximation factor is at least 0.531. In this article, we present 
a new analysis of Trevisan's algorithm that improves that bound up to 0.614247. We also apply a modification of his algorithm to the slightly more 
general maximum colored cut problem.

\paragraph{Organization} In Section 2 we describe the maximum colored cut problem and our notation. We also prove our main technical lemma, 
Lemma~\ref{lemaimportante} which is the heart of the new analysis. In Section 3 we revisit 
Trevisan's algorithm and prove our new guarantee.

\section{Maximum Colored Cut Problem}
In the \emph{maximum colored cut} problem (MaxCC), we are given a graph  with , .
The edges in  are said to be  red and the edges in  are said to be blue. We are also given a nonnegative weight function . The goal is to partition  into two sets  and  such that the weight of the red edges that are cut by the partition plus the weight of the blue edges that are not cut (or \emph{uncut}) is maximized. This problem generalizes the Max-Cut problem, in which .

Given a bipartition of , we say that a red edge is \emph{good} if it belongs to the associated cut, and that a blue edge is \emph{good} if it is 
not in the cut. Any edge which is not good is called \emph{bad}. With this notation, the objective of the MaxCC problem is to find a partition 
that maximizes the total weight of good edges.

Trevisan's approach for Max-Cut~\cite{Luca} can be described as follows: Devise an algorithm  that finds a tripartition  of , 
where  is the set of nodes that are still ``undecided'' between  and  and then recursively use  on the set of undecided nodes until 
every node is decided. Observe that for a given tripartition , the edges of the graph induced by  are already labeled as 
good or bad, and no matter what the recursive partition of the undecided vertices into two pieces  and  is, we can always impose that half of 
the total weight of the edges between  and  is good, by assigning  to  and  to  or vice versa. This 
observation suggests that the objective of  should be to find a tripartition for which the ratio of the weight of the good edges induced by  plus half of the total weight crossing from  to  with respect to the total weight of the edges involved (the ones 
incident to ) is as high as possible. We call this ratio the \emph{recoverable ratio} of the tripartition. As we will later see, it is 
possible to find a partition with high recoverable ratio by using spectral partitioning techniques.

The algorithm we describe and analyze in Section 3 is the same as the one by Trevisan in~\cite{Luca}, so we assume certain familiarity with that 
paper. The previous analysis for the approximation ratio of that algorithm involve upper bounding the number of uncut edges (bad edges, in our 
nomenclature) in each iteration via an application of Cauchy-Schwarz inequality. We give a tighter analysis by directly lower bounding the 
number of cut edges (good edges).

\subsection{Notation}
For a graph , a set of edges , and a set of vertices , we use  to denote the set of edges in  with both endpoints inside of . Similarly, for disjoint subsets  and  of  we use  to denote the set of edges in  with one endpoint in  and the other in . The indicator vector of a bipartition  of  is the vector  with  if  and  if . Similarly, the indicator vector of a tripartition  of~ is the vector  with  equals to ,  or  whenever  is in ,  or~, respectively. As usual, for a weight function , and a set , we use  as a shorthand for . For the rest of the paper, we fix a graph  with , , , and a nonnegative weight function .

The following quadratic formulation gives the value of the maximum colored cut.


Let  be the matrix associated to the quadratic form involving edge  for MaxCC:

By letting , MaxCC can be expressed as . 

It will also be convenient to define matrices  such that

and let . Note that for every ,  .
It is also easy to check that

where  and  are the weighted adjacency and degree matrices associated to the edge set .

The following lemma, which is a restatement of Lemma 2 of~\cite{Luca} in our terminology, relates the value of the MaxCC to the highest eigenvalue of a certain positive semidefinite matrix.

\begin{lemma}\label{eigen}
If , then there exists a vector  such that

\end{lemma}
\begin{proof}
Let  be the vector solving  , and  be the incident vector of a maximum colored cut. Then

To find  we can simply find a unit eigenvector  associated to the maximum eigenvalue of the positive semidefinite matrix , and then set . \qquad
\end{proof}

\medskip
In practice, the vector  guaranteed by the previous lemma can not be found exactly in polynomial time. However, it is possible to efficiently find a vector  such that  in time inversely proportional to  (see the discussion of Lemma 2 in \cite{Luca}). 
We show next how to find a good tripartition, this is, one having a good recoverable ratio, using vector  as a starting 
point.

\subsection{Finding a good tripartition}

Let  be the indicator vector of a partition  with . Define the weights of the good, bad, crossing  and incident edges associated to the partition as:

respectively. The next two observations are direct from the previous definitions:



The \emph{recoverable ratio} of  is defined to be   
divided by , if the denominator is not zero. If , this ratio is defined\footnote{The definitions of this section only apply to nonzero vectors . Even if ,  may be zero, but for that to happen we need that all the edges incident to  have zero weight. In this case, the vertices in  are irrelevant for the maximium colored cut instance.  Note that this always occurs in certain situations, for instance if .} as 1. Note that if  is the indicator vector of an optimal colored cut, (in particular, ), its recoverable ratio is exactly 
. 

Given a vector  such that  is large, we want to find a rounded vector  with high recoverable ratio (the condition that  is nonzero is added explicitly in order to avoid situations in which all vertices are left ``undecided'').
The following lemma, which can be seen as an improvement of Lemma 3 in \cite{Luca}, is useful for this task.
\begin{lemma}\label{rounding}
Let , with . Construct  as follows. Pick  uniformly at random from the open interval  and let, for each ,
  
Let 
be the probabilities that an edge  is cut, uncut or crossing the tripartition induced by , respectively.
Then for all ,

\end{lemma}

\begin{proof}
First note that the random vertex  is nonzero since for the indices  such that , we also have . Also observe that \eqref{eq2} follows if we apply \eqref{eq1} to the vector  obtained from   by switching the sign of .
To prove \eqref{eq1} we consider two cases.

\smallskip
\noindent \textbf{Case 1}: . Assume, w.l.o.g.~that . In this case,  equals the probability that 
both  and  are bigger than , thus . Similarly,  is equal to the probability that  is between  and 
. This is, .

Using a version of  Bergstr\"om's inequality (see, e.g.~\cite{inequalities}), 
 which is valid for  and , we obtain that


\noindent \textbf{Case 2}: . Assume again, w.l.o.g.~that . It is easy to see that  
and that .

Since , we have . Using that , we get

\end{proof}

\bigskip

The main technical result of this article is given by the following lemma.

\begin{lemma}\label{lemaimportante}
Given a nonzero vector  such that  , for some , we can efficiently find  
an indicator vector  of a tripartition satisfying
10pt] \displaystyle\frac{1}{1+2\sqrt{\varepsilon(1-\varepsilon)}}\cdot \Inc(z) &\text{if .}
\end{cases}
\displaystyle \frac{1}{1+2\sqrt{\varepsilon(1-\varepsilon)}} = 
\frac{-1 +\sqrt{4\varepsilon^2 -8\varepsilon + 5}}{2(1-\varepsilon)}.
\sum_{\{i,j\} \in R}& w_{i,j}(C(i,j) + \beta X(i,j)) + \sum_{\{i,j\} \in B} w_{i,j}(U(i,j) + \beta X(i,j))\\
&\geq \beta(1-\beta) \sum_{\{i,j\} \in R} w_{i,j}(x_i - x_j)^2 + \beta(1-\beta) \sum_{\{i,j\} \in B} w_{i,j}(x_i + x_j)^2\\
&= \beta(1-\beta) x^T M x \geq 2(1-\varepsilon)\beta(1-\beta)  x^T D x.\\
&= 2(1-\varepsilon)\beta(1-\beta)\E[2\Inc(y)-\Cross(y)].
 \label{eqbeta}
2A\E[\Inc(y)] &\leq \E[\Good(y)] + (A +\beta)\E[\Cross(y)],
 
(2A-\alpha)\E[\Inc(y)] &\leq \E[\Good(y)](1-\alpha) + (A+\beta-\alpha)\E[\Cross(y)].
 
(1-2\beta)\E[\Inc(y)] &\leq 2(1-A-\beta)\left(\E[\Good(y)]+ \frac12 \E[\Cross(y)]\right).
 
\frac{1-2\beta}{2(1-A-\beta)}\E[\Inc(y)] &\leq \E[\Good(y)]+ \frac12 \E[\Cross(y)].

R(\beta,\varepsilon)&:=\frac{1-2\beta}{2(1-\beta)(1-2\beta(1-\varepsilon))},\\ \text{
subject to }\quad
\beta_{\min}(\varepsilon)&:=\frac{1}{3-2\varepsilon+\sqrt{5-8\varepsilon +4\varepsilon^2}} \leq  \beta < \frac{1}{2(1-\varepsilon)}.
 
f_1(\varepsilon) = R(\beta_{\min}(\varepsilon))&=\frac{-1+\sqrt{4\varepsilon^2-8\varepsilon+5}}{2(1-\varepsilon)},\qquad  f_2(\varepsilon) = R(\beta_-(\varepsilon)) =\frac{1}{1+2\sqrt{\varepsilon(1-\varepsilon)}},\notag \
From the previous discussion, we deduce that if , . Therefore,


Observe that the previous inequality holds even if . Since the expectation above is always nonnegative, there must exist a vector , among those that participate in the expectation, with recoverable ratio at least . We can find  by testing all  thresholds  for  in the proof of Lemma~\ref{rounding}, and keeping the associated vector with greatest recoverable ratio. Note that the vector  obtained like this is not zero, since for the index  such that , we must also have . \qquad
\end{proof}


\begin{figure}[ht!]
\begin{minipage}[t]{.5\textwidth}
  \centering
  \includegraphics[scale=0.5]{plot1}
\end{minipage}\begin{minipage}[t]{.5\textwidth}
  \centering
  \includegraphics[scale=0.5]{plot2}
\end{minipage}
  \caption{On the left, the functions  (bottom function) and  (top function) used to defined . On the right, the relationship between the recoverable ratio guaranteed by  (top function), and that guaranteed by  (bottom function). }
  \label{fig:plot1}
\end{figure}
\medskip
\begin{rem}\label{remark}
{\rm Lemmas \ref{eigen} and \ref{lemaimportante} imply that if  has a bipartition with at least  fraction of the total weight being good (`cut'), then we can  find a partition into three pieces  with recoverable ratio at least . 
Previously, Trevisan proved, using an application of Cauchy-Schwarz inequality, that the same is true for the function . In Figure~\ref{fig:plot1} we can see the relationship between these two bounds. Note that we always have . Also, it is worth noting that a random bipartition has a recoverable ratio of 1/2 in expectation. The previous guarantee  beats 1/2 when , while the new guarantee  implies that we can do better than a random cut for any .}

\end{rem}



\section{Trevisan's algorithm and our new analysis}
Our extension of Trevisan's algorithm for the MaxCC problem is depicted as Algorithm~\ref{alg:trevisan} below. In what follows, we assume that the vector  given by Lemma \ref{eigen} can be found exactly in order to keep the argument simpler.

\begin{algorithm}[!ht]
\caption{(Spectral Partitioning).}
\label{alg:trevisan}
\begin{algorithmic}[1]
\Require{A colored graph , with nonnegative weights  .}
\Ensure{A bipartition  of .}
\State{Compute a nonzero vector  maximizing  (Lemma \ref{eigen}).}
\State{Determine from  the rounded vector  given by Lemma \ref{lemaimportante}}
\If{}\State{Return a bipartition  of  such that the weight of good edges is at least half of \, (a random cut suffices).}
\Else\State{Let  be the tripartition induced by .}
\If{} 
\State{Return .}
\Else 
\State{Set .}
\State{Return the best of
   and .} \EndIf\EndIf
\end{algorithmic}
\end{algorithm}


\begin{theorem}\label{theoremfinal}
If , then Algorithm~\ref{alg:trevisan} returns a partition  whose indicator vector  satisfies

with  as in \eqref{function-f}.
\end{theorem}

\begin{proof} 
Note first that the algorithm terminates since in every recursive call the residual graph  considered has at least one fewer node. Assume that the algorithm performs  recursive calls and let  be the graph at the beginning of the -th iteration, so that . Let also  be the empty graph and  for every . We observe that
  
The previous holds since for the optimal bipartition of  defining , the total weight of the bad edges is at least the weight of the bad edges inside  for the same partition, which in turn  is at least the weight of the bad edges for the bipartition of  defining . From the previous observation we get that  . 

Recall that a random cut has a recoverable ratio of at least half.  Combining this with Remark~\ref{remark} and the intuition about the recoverable ratio given in the introduction we get that the total weight of good edges in  with respect to the partition  is at least 
 
Using that  is decreasing, and that the sets  are mutually disjoint, we obtain that the total good weight returned by the algorithm is at least

\end{proof}

\begin{theorem}
Algorithm~\ref{alg:trevisan} guarantees a 0.6142 approximation for MaxCC.
\end{theorem}

\begin{proof}
Let . By Theorem \ref{theoremfinal},  is a lower bound on the good weight returned by Algorithm~\ref{alg:trevisan}. Note that  when . Using this, and the definition of  it is easy to see that

10pt]
\qquad + \displaystyle \int_{\varepsilon/\varepsilon_0}^1\frac{1}{1+2\sqrt{(\varepsilon/r)(1-\varepsilon/r)}}dr , &\text{ if .\vspace{10pt}}
\end{cases}

  G(\varepsilon):= \frac{1}{2(1-\varepsilon)}.

 G(\varepsilon):=\frac{1}{2(1-\varepsilon)}\cdot \bigg(&\varepsilon-1  + \sqrt{4\varepsilon^2-8\varepsilon+5}-\varepsilon\ln\left(\frac{1+\sqrt{4\varepsilon^2-8\varepsilon+5}}{8\varepsilon}\right)\\
 &+\frac{\sqrt{5}}{5}\varepsilon\ln\left(\frac{5-4\varepsilon+\sqrt{5(4\varepsilon^2-8\varepsilon+5)}}{\left(11+5\sqrt{5}\right)\varepsilon}\right)\bigg);

G(\varepsilon):=\frac{1}{2(1-\varepsilon)}&\cdot \Bigg(\varepsilon\left(1-\frac{3}{\varepsilon_0}\right)+2+\frac{\varepsilon}{\varepsilon_0}\sqrt{4\varepsilon_0^2-8\varepsilon_0+5}-\varepsilon\ln\left(\frac{1+\sqrt{4\varepsilon_0^2-8\varepsilon_0+5}}{8\varepsilon_0}\right)\\
&+\frac{\sqrt{5}}{5}\varepsilon\ln\left(\frac{5-4\varepsilon_0+\sqrt{5(4\varepsilon_0^2-8\varepsilon_0+5)}}{(11+5\sqrt{5})\varepsilon_0}\right)+16\varepsilon\ln\left(\frac{\sqrt{\varepsilon}+\sqrt{1-\varepsilon}}{\sqrt{\varepsilon}+\sqrt{\frac{\varepsilon}{\varepsilon_0}-\varepsilon}}\right)\\
&+8\varepsilon\frac{\sqrt{\varepsilon_0(1-\varepsilon_0)}+1-2\varepsilon_0}{\varepsilon_0+\sqrt{\varepsilon_0(1-\varepsilon_0)}}-8\sqrt{\varepsilon}\frac{\sqrt{\varepsilon(1-\varepsilon)}+1-2\varepsilon}{\sqrt{\varepsilon}+\sqrt{1-\varepsilon}}\Bigg).
\displaystyle \frac{1}{1+2\sqrt{\varepsilon(1-\varepsilon)}} = \frac{-1 +\sqrt{4\varepsilon^2 -8\varepsilon + 5}}{2(1-\varepsilon)}.


\begin{figure}[th!]
\centering
 \includegraphics[scale=0.5]{funcionesmatlabmaxcut}\\
  \caption{The top curve corresponds to the new approximation guarantee  with minimum ; the gray curve below corresponds to the guarantee  given  in \cite{Luca} with minimum 0.531: , for  and , for .} \label{plot2}
\end{figure}

\section{Conclusions} In this paper we have improved the analysis of Trevisan's spectral Max-Cut algorithm, showing that its approximation ratio is at least 0.614247. Furthermore, we have extended its applicability to the more general Maximum Colored Cut problem. 

We leave as an open problem to adapt the spectral algorithm and its  analysis to other problems that can be formulated as the maximization of a quadratic form over -vectors, such as Max-SAT, Max--SAT, Max-CUTGAIN \cite{Luca} and other constraint satisfaction problems. We believe that in many situations a spectral based algorithm can be useful to obtain good approximation algorithms without relying on the full power of semidefinite programming.

\begin{thebibliography}{1}
\bibitem{inequalities}  E.~Beckenbach and R.~Bellman, {\em An introduction to inequalities.} Random House, New York, 1961. 
\bibitem{Luca} L.~Trevisan, Max Cut and the smallest eigenvalue, {\em SIAM J. Comput.} 41(6):1769--1786, 2012.
\bibitem{GW} M.X.~Goemans and D.P.~Williamson,  Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming, {\em Journal of the ACM}, 42(6):1115--1145, 1995.
\end{thebibliography}
\end{document}
