\documentclass{acm_proc_article-sp}   \usepackage{graphicx,url}
\usepackage{enumerate}
\usepackage[ruled,section]{algorithm}
\usepackage{algorithmic}
\newcommand{\algrdRat}{RatLU}
\newcommand{\algrdDet}{PrecDetLU}
\newcommand{\algrdMat}{PrecMatLU}
\newcommand{\algrdDixon}{PrecMatDixon}

\textwidth=6in
\hoffset=-1.3cm
\textheight=20.5cm


\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}








\title{Towards an exact adaptive algorithm for the determinant of a rational matrix}
\author{Anna Urba\'nska\\
Laboratoire Jean Kuntzmann\\
Universit\'e Joseph Fourier, Grenoble I\\
\texttt{E.mail: Anna.Urbanska@imag.fr}\\
}

\date{}

\DeclareMathOperator{\Otilde}{O^{\approx}}
\DeclareMathOperator{\OO}{O^{\sim}}
\DeclareMathOperator{\lcm}{\operatorname{lcm}}
\DeclareMathOperator{\ord}{\operatorname{ord}}
\DeclareMathOperator{\mmod}{\operatorname{mod}}
\DeclareMathOperator{\diag}{\operatorname{diag}}
\DeclareMathOperator{\EEA}{\operatorname{EEA}}

\begin{document}

\maketitle

\begin{abstract}
In this paper we propose several strategies for the exact computation
of the determinant of a rational matrix. First, we use the Chinese
Remaindering Theorem and the rational reconstruction to recover the
rational determinant from its modular images. Then we show a
preconditioning for the determinant which allows us to skip the
rational reconstruction process and reconstruct an integer result. We
compare those approaches with matrix preconditioning which allow us to
treat integer instead of rational matrices. This allows us to introduce
integer determinant algorithms to the rational determinant problem. In particular, we discuss the applicability of the adaptive determinant algorithm of \cite{jgd:2006:det} and compare it with the integer Chinese Remaindering scheme.
We present an analysis of the complexity of the strategies and
evaluate their experimental performance on numerous examples. This
experience allows us to develop an adaptive strategy which would
choose the best solution at the run time, depending on matrix
properties.
All strategies have been implemented in LinBox linear algebra
library.
\end{abstract}

\section{Introduction}
The determinant computation is one of the core problems in linear algebra.
To our knowledge, the problem of the exact computation of the
determinant of a rational matrix (i.e a matrix with rational
entries) has not yet been widely studied. In general, exact
algorithms can be used everywhere where large precision is required.
For example, the determinant can be too close to 0 or 
and thus cannot be computed by floating point precision algorithms.
In the case of ill-conditioned matrices symbolic methods can be
preferred as rounding errors can spoil the computation. It can also
be interesting to compare the use of decimal and continued fractions
approximations of the entries of real-valued matrices. Continued fractions are
the best approximants with small denominators, see \cite[Ch.
4]{gathen}. In this paper, we will try to face the question of how
efficient an exact determinant computation can be in both cases.


LinBox library \cite{Dumas2002Linbox} implements exact algorithms for the determinant computation in the case of modular and integer domains. By using fast modular routines \cite{Dumas2002issac,Dumas2004} it can offer solutions an order of magnitude faster than other existing implementations \cite{jgd:2006:det}. We apply these procedures to the computation of the determinant of a rational matrix.


Rational field arithmetics is implemented in GMP \cite{GMP} and
Givaro \cite{Givaro} libraries. In general, rational numbers are
difficult to treat from the exact computation point of view. Mainly,
the size of the numerator and denominator can increase very quickly
with every addition and multiplication. When we add or multiply two
fractions with numerators and denominators bounded by , the
numerator and denominator of the result are bounded by .
Moreover, one addition requires 3, and one multiplication requires 2
integer products, as well as a  computation. Therefore, the
cost of an exact matrix-vector or matrix-matrix product can be
prohibitive in practice.
This prohibits the use of the rational field  in most exact linear algebra algorithms which rely on matrix-matrix or matrix-vector products.


However, the cost of computing a modular image of a rational number , where  are of moderate size, should be comparable with the cost of computing a modular image of a large integer number. This allows us to compute a modular image of a rational matrix at a reasonable cost and thus enables us to use modular procedures. 

To compute the determinant of a rational matrix
 the problem of matrix
storage has to be considered. First, we can store the entries of 
as rational numbers. Furthermore, one could store the common
denominator  of all entries of  and an integer matrix 
given by the formula . This approach can be
useful in the case when the entries of  are decimal fractions and
 can be set to a power of . But if we only assume that the
values  are less than , both  and
 are bounded by . Still, we may store the common
denominator for each row (column) separately. Then the integer
vectors  are given by the equation
, where  is the matrix row
(column) and  is the common denominator of its entries. Vectors
 form matrix , the norm of which is bounded
by . The product  gives a more accurate
approximation of the denominator  than .

The purpose of this paper is to propose the strategies to compute the
denominator of a rational matrix. All approaches are based on modular
computation. Depending on the matrix storage determinant and/or matrix
preconditioning is proposed. The resulting algorithms can use the
rational reconstruction \cite[Ch.5]{gathen} and/or existing integer determinant algorithms.


The rest of the paper is organized as follows. In section \ref{sec:ea} we give a short description of the existing algorithms for the rational reconstruction and the integer determinant problem. In section \ref{sec:rda} we present the main result i.e. two preconditioning strategies and four new algorithms to compute the rational determinant. The cost of the algorithms can be described in terms of the number of modular images of  and modular determinant computations needed. Depending on the strategy, the cost of the rational reconstruction or -adic lifting is taken into account. In section \ref{sec:cpl} we discuss the cost of computing a modular image of a matrix and the overall cost of the algorithms. In section \ref{sec:exp} we present the experimental results and discuss the best choice of the strategy in practice. We conclude the paper by proposing some mixed solutions in section \ref{sec:ccl}.



















\section{Existing Algorithms}\label{sec:ea}

The aim of this section is to introduce the algorithms that will be
used later in section \ref{sec:rda}. In subsection \ref{sec:rr} we
give a short description of the rational reconstruction procedure.
On the example of -adic system solving \cite{Dixon1982}, we
present the application of this procedure to the computation of a
rational solution. We show how to change the procedure in the case
of early terminated reconstruction \cite{Wang} and give the
complexity estimation in this case. Then in subsection \ref{sec:ida}
we present the classical CRA algorithm for the determinant and its
modifications by \cite{Abbott1999} and \cite{jgd:2006:det}.



\subsection{Rational reconstruction and its application}\label{sec:rr}

A modular image of a rational number  mod  can be
computed by taking the modular images of  and  and applying
the modular division. This fact can be written as

It should be noticed is that the opposite procedure can also be
performed. One can reconstruct the fraction  where
 from it modular image . The solution is
usually not unique but when we additionally require that , , then there exists at most one
solution, see \cite[Ch.5]{gathen}.



The solution to the rational reconstruction problem can be computed by
applying the extended Euclidean algorithm EEA which searches for the
 of  and . The procedure Ratrec(a,b,u,M, N, D) takes as the input modulus ,  and the bounds  and , and returns a fraction  such that  or FAIL if no such solution exists.
The worst case complexity of {\em Ratrec} is thus the same as for the
EEA algorithm i.e.  for the classical
algorithm and  for the fast Euclidean
algorithm, see \cite[Ch.11]{gathen}. We will use the notation  for the complexity of the Extended Euclidean Algorithm with entries bounded by .


In many application, the cost of rational reconstruction is usually
small compared with the cost of computing  and . The general scheme is to recursively compute , where  or  until  and then to apply the rational reconstruction. The complexity of the procedure depends on the number  of steps, which can be quite large
Reducing the number of steps can be the easiest way to enhance the
performance of the algorithm.

This can be seen on the example of the Dixon algorithm
\cite{Dixon1982} to solve a linear system  of integer
equations. Let  be the bound for the numerator and denominator
of . In the classical approach we compute the -adic
approximation in  steps and then
reconstruct the result, which gives the complexity
 when we use the bound of
Hadamard for  and assume .
See \cite{Mulders1999} for a detailed complexity study. In fact, the number of entries in  which we need to reconstruct can often be reduced, see \cite{Dumas2002Linbox}.



One should however notice, that the bounds  and  can be much
bigger than the actual result.
The idea is therefore to apply the rational reconstruction
periodically and check the solution for correctness.
If  is the modulus in the current step, the method of Wang
\cite{Wang} prompts us to set  as the current
bound for numerators and the denominator in {\em Ratrec}. The
algorithm is guaranteed to return the result if , where  are the values of the numerator and the
denominator. In the opposite case,
 should
fail with large probability.
If we apply Wang's idea to the -adic lifting we can reduce the
number of steps to  and the complexity becomes 
Current work on this field focus on further reducing the number of steps in the case when  or . A purely heuristic idea is to use the bounds ,  instead of . For other approaches, see \cite{kho-mon:2006,ol-sto:2006}.



\subsection{Integer Determinant Algorithms}\label{sec:ida}

For an integer matrix  one has several alternatives to compute the
determinant.
The classical approach is to use Chinese Remaindering Algorithm (CRA)
to reconstruct the value from sufficiently many modular images.
The modular determinant is computed  by LU factorization in the time
, where  is the matrix dimension.Each step of the algorithm consist of computation mod  and a
reconstruction of the determinant mod  by the Chinese
Remaindering Theorem. The computation is stopped when the early
termination (ET) condition is fulfilled i.e. the reconstructed
result rests the same for several iterations. The algorithm is Monte
Carlo type, where the probability of success is controlled by the
number of repetitions. See \cite{Dumas2001, jgd:2006:det} for a
detailed description.



A mixture of CRA loop and Dixon -adic lifting is used to compute the integer determinant in  \cite{Abbott1999} and in the hybrid algorithm of \cite{jgd:2006:det}. The principle is to reduce the value reconstructed by CRA algorithm by computing a large fraction of the determinant. By solving several linear systems we can compute some largest invariant factors . Their product  is potentially a large part of the determinant. An early terminated CRA loop which reconstructs  mod  usually requires only a few modular determinant computations. Informally, the algorithm can be described as follows.
\begin{enumerate}
\item For  to  do
\begin{enumerate}
\item Solve  by Dixon -adic lifting to find ;
\item ;
\item Run CRA for several iterations to determine ;
\item if ET break;
\end{enumerate}
\item Run another determinant algorithm to get the result;
\end{enumerate}
Here,  should not exceed the expected number of invariant factors which is  see \cite{jgd:2006:det}.
The expected complexity of the hybrid determinant algorithm
\cite{jgd:2006:det} for random dense matrices is . In the worst case (step 2) we can
choose between the CRA algorithm and the algorithms of
\cite{Storjohann2004, Eberly2000, Kaltofen2005}. In fact, in the
expected case we do not need to run this step.
The experiment proved that thanks the adaptive solutions this algorithm performs better than other implementation for a larger group of matrices.

\section{Rational Determinant Algorithms}\label{sec:rda}




The algorithms to compute the rational determinant are based on the
ideas described in section \ref{sec:ea}. We present four main
strategies to compute the rational determinant. They all use CRA
which allows us to compute the determinant of the matrix modulo a
product  of primes. Then the first variant uses the
rational reconstruction to obtain the rational result. In order to
make use of Early Termination condition we have to precondition the
determinant to obtain its integer multiplication. Preconditioning of
the matrix allows us to use the integers determinant algorithms. The
application of two determinant algorithms is studied here. The
common requirements for all algorithm are shown in \ref{alg:req}.
The algorithms are Monte Carlo type due to the early termination
used.



\floatname{algorithm}{Requirements}
\setcounter{algorithm}{-1}
\begin{algorithm}\caption{}\label{alg:req}
\begin{algorithmic}
\REQUIRE  - an  rational matrix; \REQUIRE ,
 - the common denominator of the entries of the th
row (column); \REQUIRE  - the bounds for the numerator and the
denominator of , ;
\REQUIRE A set  of random primes;
\REQUIRE  - a procedure which reconstructs  or returns FAIL.
\ENSURE  - the determinant of the matrix.
\end{algorithmic}
\end{algorithm}

\floatname{algorithm}{Algorithm} The effectiveness of our methods
depends heavily on the number of modular determinants computed and
thus on the bound  and  for the numerator and the denominator
of the determinant. One can compute  as the product of lcm of all
denominators in a row (or a column). Then  can be computed as
, where  is the Hadamard bound for matrix . One
should notice that the bounds can be largely overestimated. Thus, we
proposed output-dependant approach which allows us to reduce the
number of iteration.

The first idea is to employ the CRA scheme and compute the
determinant for the modular images of a rational matrix. In the case
when the determinant is rational, early termination condition never
holds. Instead, we have to compute the bounds  and  for the
denominator and numerator of the determinant.
As soon as the product of primes  overcomes  we can apply
rational reconstruction and reconstruct the determinant from the
modular image. We can also use an output dependent rational
reconstruction as described in section \ref{sec:rr}. This strategy is
presented as algorithm \algrdRat.  An early termination in the
rational case would required applying the rational reconstruction from
time to time with the bounds  and wait for the result
to re-occur. This leads to solution when ,
where  are the numerator and denominator of the determinant.

\begin{algorithm}\caption{\algrdRat}
\begin{algorithmic}[1]
\STATE 
\REPEAT
\STATE ++; Get  from ;
\STATE Compute ;
\STATE Compute ;
\STATE Reconstruct  using , ;
\IF {}
    \STATE s = Ratrec();++;
    \IFTHENEND{s  FAIL}{return ;}
\ENDIF
\UNTIL{}
    \STATE status = Ratrec();
    \IFTHENEND{status  FAIL}{return ;}
\end{algorithmic}
\end{algorithm}

The second method can use the denominator bound~ to make the
CRA loop look for an integer value. Again, we compute the modular
image of a rational matrix  but this time we call CRA to
look for  which is integer. Now the classic ET
condition can be used and the result is obtained as soon
as . The effectiveness of this method depends
therefore on the exactness of denominator bound . Experimental
results show that it is sufficient in practice, see sec. \ref{sec:exp}
table \ref{tab:app}. This strategy is presented as algorithm \algrdDet.

\begin{algorithm}\caption{\algrdDet}
\begin{algorithmic}[1]
\STATE ; \REPEAT \STATE ++;Get  from ;
\STATE Compute ; \STATE Compute ; \STATE reconstruct  using ,  \IFTHENEND {ET
holds}{return ;} \UNTIL{} \STATE return ;
\end{algorithmic}
\end{algorithm}

The last two strategies require an integer matrix  which can be obtained by preconditioning the rational matrix . In order to obtain an integer matrix, the easiest way would be to take matrix , where  is the common denominator of all entries. In the general case, where the entries of  are fractions  with numerator and denominator bounded by , this is not the best choice as the size of  can be as large as . This causes  to be . Moreover, the denominator approximation is  in this case, which is  in size. We have already defined a tighter bound for the denominator of  by , which is  in size. Now, if we want to use the integer matrix  then we can precondition  by taking , where  are the common denominators of the rows (or , where  are the common denominators of the columns).
For the preconditioned matrix  all integer determinant
algorithms can be applied. In particular the hybrid determinant
algorithm of \cite{jgd:2006:det} can be used. The drawback of this
approach is the size of the coefficients of  compared to
, see section \ref{sec:exp} table \ref{tab:him}. This forced us
to use early terminated rational reconstruction for system solving
in the Dixon -adic lifting algorithm. The strategies that use the
CRA loop or the hybrid algorithm are presented as algorithms
\algrdMat~ and \algrdDixon~ respectively.

\begin{algorithm}\caption{\algrdMat}
\begin{algorithmic}[1]
\STATE  \STATE Compute  (or
 \REPEAT \STATE Get  from ; \STATE Compute
; \STATE Compute ; \STATE
reconstruct  using ,  \IFTHENEND {ET holds}{ return
;} \UNTIL{} \STATE
return ;
\end{algorithmic}
\end{algorithm}

\begin{algorithm}\caption{\algrdDixon}
\begin{algorithmic}[1]
\STATE Compute  (or ;
\STATE Compute  by HybridDet \cite{jgd:2006:det};
\STATE return ;
\end{algorithmic}
\end{algorithm}

















\section{Complexity Analysis}\label{sec:cpl}

In this section we study the complexity of the algorithms presented in section \ref{sec:rda}. In subsection \ref{sec:gen} we present the analysis of the general case, where we assume that the entries of the matrix are fractions with numerators and denominators bounded by . Then, in subsection \ref{sec:spc}, we will focus on two special cases i.e. matrices of decimal fractions and Hilbert matrices.



The complexity of the strategies described in section \ref{sec:rda}
depends on the number of iterations required by the {\bf while}
loop of CRA. Then, depending on the strategy, we have to include the
cost of computing the homomorphic image of the matrix, the cost of the rational reconstruction or the cost of -adic lifting. If we use the early termination condition, the number of steps required for the computation of  depends on the values:  - the size of the matrix,  - the real values of the numerator and denominator of  and  - the bound for the denominator. The cost of homomorphic imaging depends on the maximum norm of the matrix i.e.  and .

\subsection{General case}\label{sec:gen}
We start this section by the analysis of the  rational homomorphic imaging schemes. We have the following lemma.

\begin{lemma}\label{lem:him}
Let  be a word-size prime. Then the complexity of computing the
modu\-lar image at  for a rational matrix  is
 word operations.
\end{lemma}
\begin{proof}
For a matrix without a pattern we compute an image for all 
entries. For a rational fraction the cost is  for the
computation of the modular image of the numerator and denominator and
 for the modular inverse computation by fast
extended Euclidean algorithm. Therefore for a word-size  the cost of
computing the image is  yet important, due to the constant for
computing the inverse of an element mod .
\end{proof}


For the integer case, the cost is .
We can notice that  can be  in the worst case, so the complexity of homomorphic imaging in terms of  is  for the rational and  in the integer case. But if  the cost of imaging for one element is . 
Thus, if both  and  are less than , the complexity of the homomorphic imaging becomes  for the rational and  for the integer case. In this case, it is better to use integer imaging. On the other hand, if matrix  is structured, for example it is Hankel-type, we have the complexity  for rational imaging. Due to the preconditioning, we loose the structure pattern for  and the complexity of integer imaging rests without change. Finally we notice, that for sparse matrices with  elements, we can take  instead of  in the complexity formula.

Putting it together we have the following theorem.

\begin{theorem}\label{thm:com}
The worst case complexities of the strategies for computing the determinant of a rational matrix  of size  are
\begin{enumerate}[1.]
\item 
    for \algrdRat, where  hides some  factors;
\item 
for \algrdDet;
\item 
for \algrdMat;
\item  for \algrdDixon, where  and  is
the size of solution to .
\end{enumerate}
Here  is equal to  as in  section
\ref{sec:rda}; ,  are the numerator and denominator of
 and .
\end{theorem}

\begin{proof}

The complexities can be obtained by a careful examination of the number of CRA
steps.
The result for alg. \algrdRat~ takes into account the cost of the rational reconstruction which is
performed at most  times. In alg. \algrdDixon~ we
introduce  to estimate the cost of early terminated -adic
lifting. The size of  can generally vary depending on the choice of  but
is  in the worst case. To further
evaluate the worst case complexity of alg. \algrdDixon~ we assumed
that {\em HybridDet} continues to use CRA loop in the worst case. Thus
the number of iterations  and the complexity.
\end{proof}

Special care should be taken if we consider the use of alg. \algrdDixon.
As  can potentially be  in size and with a
pessimistic bound on , its worst case complexity can be , which is worse than for the CRA computation. Nevertheless, the gain of computing   can be important, as it is the case in the {\em HybridDet} algorithm, see \cite{jgd:2006:det}.

\subsection{Complexity in the special cases}\label{sec:spc}
By the precedent remarks it should be visible, that the analysis of the strategies should be divided into two main cases. One would consist of the matrices, whose entries are given by decimal fraction, or more generally, where the common denominator of all entries, the common denominator of the rows and the norm of  are of the same order i.e. . In the other case matrix entries are given as fractions with different denominators. We will study the complexity of the algorithms on the example of Hilbert matrices.

In the case of matrices of decimal fractions let us further assume that  is . This would be the case of numerous ill-conditioned matrices emerging from different applications in science and engineering. In order to better describe the differences between the algorithms, we include the cost of EEA when it is relevant. The theorem is a straightforward consequence of theorem \ref{thm:com}.
\begin{theorem}
The complexities of the strategies in the case when  are:
\begin{enumerate}
\item 
    for alg. \algrdRat;
\item 
for alg. \algrdDet;
\item 
for alg. \algrdMat;
\item 
    for alg. \algrdDixon.
\end{enumerate}
where  are as in theorem \ref{thm:com}.
\end{theorem}
The analysis suggests that the algorithm \algrdMat~ should be better than
\algrdDet~\-(see \ref{lem:him} for the homomorphic image
complexity). The performance analysis in section \ref{sec:exp}
confirms this observation. Furthermore, as long as the Smith form of
 is simple, we encourage the use of strategy \algrdDixon.
In particular, we can establish an equivalence between matrices 
of random decimal fractions with  decimal places taken randomly
an uniformly from the interval  and matrices ,
. This allows us to use the expected
complexity of the hybrid algorithms of \cite{jgd:2006:det} as the
expected complexity of the rational determinant computation by alg.
\algrdDixon.
Also,
the preconditioning should be used instead of strategy \algrdRat. For more details see section \ref{sec:exp}.

The other group consists of matrices with rational entries given by
fractions with very different denominators. As a model case we can
consider Hilbert matrices. Hilbert matrices are the matrices of the
form . They are benchmarks examples for
many numerical methods. The formula for the determinant of a Hilbert matrix is
well known and is given by the equation




\begin{theorem}\label{thm:hil}
The complexities for rational determinant strategies in the case of Hilbert matrices are
\begin{enumerate}
\item 
    for alg. \algrdRat;
\item 
for alg. \algrdDet;
\item 
for alg. \algrdMat;
\item 
    for alg. \algrdDixon.
\end{enumerate}
\end{theorem}
\begin{proof}
One should notice that  is . The size of entries of  is  and .
\end{proof}

In the case of Hilbert matrices algorithm \algrdDet~\-has the best time complexity and also performed best in the experiments, see section \ref{sec:exp}. Since the numerator is equal to , we only have to recover the size of the over-approximation. Experimental results show, that its size is equal to about 8\% of the denominator size. Therefore, alg. \algrdDet, \algrdMat~ perform about 25 times less iterations than \algrdRat. As for the algorithm \algrdDixon, the study of the Smith form of  has revealed that it is quite complex, with about  nontrivial factors and the size  equal . Thus, it is not worth computing \algrdDixon~ due to the high cost of the algorithm and poor gain.



\section{Performance comparison}\label{sec:exp}

In this section we present the experimental results for four strategies
from section~\ref{sec:rda}. We have tested the performance of four strategies on three
matrix sets: random, ill-conditioned and Hilbert matrices. 


We generated the random matrices using Matlab procedure {\em rand}. The entries of the matrices are decimal fractions with 6 decimal places chosen randomly from the interval . The determinant  of the resulting matrices is large in the absolute value. The result of the numerical procedure of  Matlab is .

Ill-conditioned matrices have been chosen from the Matrix Market \cite{matrixMarket} Harwell-Boeing collection. We chose three sets: Grenoble, Astroph and Bcsstruc3.
Grenoble set represents the results of the simulation of computer systems.
The sizes of the matrices varies from 115 to 1107 and the condition numbers range from  in the case of the smallest matrix to  for the biggest.
The decimal precision of the entries depends on the matrix and ranges from 1 to 5 decimal places.
The determinants are close to 0.
For these matrices, Matlab procedure {\em det} computes the result correctly up to the th decimal place.
Since matrix entries seem to be represented as rounded expansions of rational numbers, we computed the determinant of the matrices ''as is'' and then we took continued fractions approximants of the entries with the same precision as the decimal fractions.

Astroph set describes the process of nonlinear radiative transfer and statistical equilibrium in astrophysics.
The condition number is  for the small  matrix and  for the  one.
The result of Matlab computation is .
Bcsstruc3 gives dynamic analyses in structural engineering.
All matrices are symmetric.
The condition number is about  for matrices 19 and 20 and  for matrix 22.
The result of Matlab computation is .


We split the analysis of the performance of the algorithms in three phases. First, we will consider the cost of rational-modular vs. integer-modular imaging and compare it with the results for  and . Then we will take a look on the numerator and denominator approximations  and  computed by our algorithms. Finally, we give the timings for all strategies and compare their performance.

As we can see in table \ref{tab:him}, the time of computing an integer image can be several times shorter than for the rational image provided that the size of preconditioned matrix is still small. This is not the case for Hilbert matrices of dimension  , when the time of rational image computation is better. Furthermore, for structured matrices, like Hilbert, we can reduce  the number of images computed. For a Hankel-type matrix, there are only  images to compute, which makes the cost of imaging negligible.

\begin{table}\centering
\begin{tiny}
\begin{tabular}{|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|}
\hline
A & RatIm & IntIm & IntIm/RatIm &  & \\\hline
bccstk817   &0.14587    &0.03126    &4.66696 & 60&66\\\hline
bccstk485   &0.05189    &0.01123    &4.61980 & 65&69\\\hline
bccstk138   &0.00280    &0.00050    &5.53681 & 42&42\\\hline
mmca180     &0.00808    &0.00120    &6.74795 & 77&76\\\hline
mccf765     &0.13222    &0.03215    &4.11219 & 70&68\\\hline
grenoble115 &0.00162    &0.00019    &8.51887 & 19&19\\\hline
grenoble185 &0.00746    &0.00096    &7.8125  & 19&19\\\hline
grenoble216a&0.01055    &0.00145    &7.25    & 2&1\\\hline
grenoble216b&0.0105     &0.00106    &9.90055 & 19&19\\\hline
grenoble343 &0.0264     &0.00507    &5.21053 & 2&1\\\hline
grenoble512 &0.0588     &0.0126     &4.66667 & 2&1\\\hline
grenoble1107&0.26762    &0.05682    &4.70958 & 16&16\\\hline
random200   &0.037      &0.003      &11.692  & 19&19\\\hline
random500   &0.330      &0.028      &11.831  & 19&19\\\hline
random800   &0.599      &0.071      &8.436   & 19&19\\\hline
random1000  &0.934      &0.111      &8.452   & 19&19\\\hline\hline
hilbert100  &0.00414    &0.00255    &1.62264 & 7&289\\\hline
hilbert200  &0.02174    &0.01984    &1.09552 & 8&567\\\hline
hilbert250 &0.03481    &0.03629    &0.95942 & 8&714\\\hline
hilbert300  &0.05093    &0.05967    &0.85350 & 9&847\\\hline
hilbert400  &0.09307    &0.13343    &0.69756 & 9&1134\\\hline
hilbert600  &0.21485    &0.41759    &0.51450 & 10&1711\\\hline
hilbert800  &0.38839    &0.94920    &0.40917 & 10&2294\\\hline
hilbert1000 &0.61425    &1.81285    &0.33883 & 10&2866\\\hline
\end{tabular}\caption{Comparison of the times (in seconds) for homomorphic imaging are given in columns RatIm (for rational) and IntIm (for integer). The ratio of the timings is given in column 3. Last two columns give the size of entries for  and . Matrix size is included in its name.}\label{tab:him}
\end{tiny}
\end{table}


The performance of the algorithms depends on the accuracy of denominator approximation used. For the bound , the resulting size of the over-approximation is shown in table \ref{tab:app}, column 4. In algorithm \algrdDixon~ we additionally approximate the numerator by computing . In this case we are interested in the value  and  which we compute instead of the numerator. As we can see in the table, the quality of the approximation of the denominator depends on the matrix and ranges from 1-2\% in the case of sparse matrices in the Grenoble set, to 80\% for Bccstk matrices. For Hilbert matrices the approximation is quite efficient, the over-approximation is always less than 10\%. Table \ref{tab:res} shows that despite the size of the over-approximation, preconditioning allow us to gain enough to beat the naive \algrdRat~ algorithm. If the size of  is small, as is the case for sparse matrices, we can compute  at a  relatively low cost and efficiently approximate the numerator.





\begin{table}\centering
\begin{tiny}
\begin{tabular}{|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|}\hline
 &  & &&  & & \\\hline
bccstk817   &7845   &36169  &6294   &0.802  &25923  &16540\\\hline
bccstk485   &3903   &21921  &2538   &0.650  &16225  &8234\\\hline
bccstk138   &2576   &5040   &139    &0.054  &3880   &299\\\hline
mmca180        &1663   &7341   &571    &0.343  &7375   &537\\\hline
mccf765        &5503   &32451  &2626   &0.477  &32483  &2594\\\hline
grenoble115 &2243   &2136   &36     &0.016  &1526   &646\\\hline
grenoble185 &3072   &2785   &3      &0.001  &2777   &11\\\hline
grenoble216a&423    &131    &9      &0.021  &124    &16\\\hline
grenoble216b&4110   &3278   &193    &0.047  &683    &2788\\\hline
grenoble343 &678    &209    &8      &0.012  &201    &16\\\hline
grenoble512 &1009   &303    &15     &0.015  &306    &12\\\hline
grenoble1107&15639  &14002  &2707   &0.173  &7184   &9525\\\hline
random200   &3986   &4255   &0      &0      &4255   &0\\\hline
random500   &9961   &10952  &4      &0      &10956  &0\\\hline
random800   &15944  &17797  &1      &0      &17798  &0\\\hline
random1000  &19931  &22407  &0      &0      &22404  &3\\\hline\hline
hilbert100  &19737  &1      &1690   &0.086  &130    &1561\\\hline
hilbert200  &79472  &1      &6493   &0.082  &290    &6204\\\hline
hilbert300  &179207 &1      &14323  &0.080  &424    &13900\\\hline
hilbert400  &318942 &1      &26509  &0.083  &563    &25947\\\hline
hilbert600  &718412 &1      &59948  &0.083  &848    &59101\\\hline
hilbert800  &1277881&1      &103581 &0.081  &1133   &102449\\\hline
hilbert1000 &1997351&1      &164550 &0.082  &1424   &163127\\\hline
\end{tabular}\caption{The size of the numerator  and denominator  of , the size of the denominator over-approximation  computed by \algrdDet~ and \algrdMat; the numerator approximation  obtained as  in \algrdDixon, and the size of the part remaining to compute.  depends on  and the over-approximation .}\label{tab:app}
\end{tiny}
\end{table}
\begin{table}\centering
\begin{tiny}
\begin{tabular}{|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|}
\hline
Matrix  &\algrdRat  &\algrdDet  &\algrdMat  &\algrdDixon\\\hline
bccstk817   &*           &789.02    &553.624&{\bf 318.62}\\\hline
bccstk485   &278.964    &143.888&95.836 &{\bf 57.144}\\\hline
bccstk138   &4.12       &1.868  &1.324  &{\bf 0.764}\\\hline
mmca180    &14.404 &5.896  &3.644  &{\bf 1.604}\\\hline
mccf765    &*  &585.724    &416.352    &{\bf 128.24}\\\hline
grenoble115 &1.444  &0.591813   &0.456  &{\bf 0.288}\\\hline
grenoble185 &5.86   &2.34   &1.456  &{\bf 0.468}\\\hline
grenoble216a    &1.052  &0.268  &{\bf 0.248}    &0.26\\\hline
grenoble216b    &10.448 &3.852  &{\bf 2.204}    &2.128\\\hline
grenoble343 &4.292  &0.924  &0.832  &{\bf 0.732}\\\hline
grenoble512 &14.844 &2.868  &2.48   &{\bf 1.072}\\\hline
grenoble1107    &*  &698.436    &519.368    &{\bf 367.448}\\\hline
random200   &24.096     &10.776  &3.996     &{\bf 2.980}\\\hline
random500   &432.448    &180.448 &71.492    &{\bf 54.996}\\\hline
random800   &1715.316   &789.154 &331.008   &{\bf 205.188}\\\hline
random1000  &*          &1572.024&662.956   &{\bf 403.232}\\\hline\hline
hilbert100  &17.860 &0.664  &{\bf 0.548}    &0.712\\\hline
hilbert200  &330.280    &11.104 &{\bf 10.52}    &11.312\\\hline
hilbert300  &*  &{\bf 59.144}   &65.236 &66.872\\\hline
hilbert400  &*  &{\bf 200.844}  &252.676    &265.276\\\hline
hilbert600  &*  &{\bf 1072.754} &1664.738   &1735.574\\\hline
hilbert800  &*  &{\bf 3476.188} &6299.98    &8830.372\\\hline
hilbert1000 &*  &{\bf 8870.534} &18466.348  & 19328.66
\\\hline
\end{tabular}\caption{Timing comparison for 4 rational determinant
strategies. All times in seconds. Best times in bold.}\label{tab:res}
\end{tiny}
\end{table}
The timings for all algorithms are shown in table \ref{tab:res}. The
results for Hilbert matrices agree with the complexity estimation in
Thm. \ref{thm:hil}. Note that alg. \algrdDixon~ is usually the best
for the matrices from MatrixMarket collection. 

For the Grenoble set, the approximation by continued fractions allowed quite well, in our opinion, to reconstruct the orginal rational matrix connected to the problem. Despite the difference in properties, the running times for the decimal and continued fractions variants were simmilar. However, although the matrices were close in the maximum norm, the determinants ratio reached as much as 2 in the case of grenoble1107.   


In figure \ref{fig:hil} we present the results of the determinant computation for Hilbert matrices. We compare the timings for algorithm \algrdRat,  \algrdDet, \algrdMat, \algrdDixon, and the Maple LinearAlgebra::Determinant algorithm with {\em method=rational}. The best performance is observed for a variant of algorithm \algrdDet~ which takes into account the Hankel structure of the matrix.

\begin{figure}\label{fig:hil}
\centering
\includegraphics[width=200pt]{q_det}
\caption{Comparison of the timings for the exact computation of the
rational determinant of Hilbert matrices. The results for algorithms
\algrdRat, \algrdDet, \algrdMat~ and \algrdDixon~ implemented in
LinBox and Maple Determinant procedure are shown. Algorithm
\algrdDet~ is used in the classic and symmetric variant, which takes
into account the Hankel structure of the matrix. All times in
seconds. }
\end{figure}







\section{Conclusions}\label{sec:ccl}

It this paper we have presented four strategies for exact computation of the determinant of a rational matrix. We have evaluated the performance of these algorithms on several sets of matrices. The performance of the algorithms suggests that there exists a clear division between the matrices given as a rational approximation (by decimal fractions) of real valued matrices and the matrices with a great diversity of the denominators of the entries. For the first case, matrix preconditioning which leads to a integer matrix is proposed, which allows us to use integer determinant algorithms, see solution \algrdDixon. For the second case, determinant preconditioning is preferred, which does not lead to matrix coefficient blow-up. In general, preconditioning proved more useful than rational reconstruction tools, although better early termination methods where the modulus  is linear in the size of the output  and  can bring a change, see \cite{kho-mon:2006,ol-sto:2006}.

An adaptive solution should be able to choose the best storage method and homomorphic imaging scheme,
and work independently of the determinant over-approximation.


We propose the following solution, which incorporates the elements of all algorithms \begin{enumerate}
\item Compute , ; set ;
\item If  compute  - see alg. \algrdDixon
\item Compute the modular image of the rational matrix  and integer matrix , determine whether to use \algrdDet~ or \algrdMat~ based on the timings.
\item Run the ET CRA loop for  using \algrdDet~ or \algrdMat.
\item From time to time check by rational reconstruction the early termination condition on  - see \algrdRat.
\end{enumerate}
This algorithm can be further developed to compute other invariant factors as in alg. \algrdDixon~ if relevant. Notice, that the cost of introducing solution \algrdRat~ to the adaptive algorithm is virtually that of rational reconstruction.

Further work can include intertwining algorithms \algrdRat~ and \algrdDet~ to include the use of less exact determinant preconditioners, which potentially are not a multiple of . The aim would to reduce a factor of the denominator by preconditioning and reconstruct the remaining part by rational reconstruction. The strategy should be effective, if the over-approximation caused by preconditioning is reduced but a large fraction of the denominator is obtained at the same time. For example,  could be considered.




Further work can then focus on the implementation of the solution in the case of sparse matrices and on the parallelization of the algorithms.

In this paper we have considered the case of dense matrices in the analysis of the complexity of the strategies as well as in the implementation. However, sparse matrix counterparts of the algorithms can also be used. For the modular determinant computation one could used the algorithm of Wiedemann \cite{Wiedemann:1986:SSLE} that computes the determinant by finding the characteristic polynomial of the matrix. In alg. \algrdDixon~ the sparse solver of \cite{EbGies2006} can be used.

The strategies described in this paper contain elements that allow parallelization. This concerns in particular the CRA loop, where several iterations can be performed at the same time, see \cite{Dumas2000}. The question of an optimally distributed early termination in the case of integer Chinese reconstruction (alg. \algrdDet, \algrdMat, \algrdDixon) as well as the rational reconstruction (alg. \algrdRat) has not yet been addressed. For a parallel -adic lifting for alg. \algrdDixon, see \cite{Dumas2002}.

In this paper we have developed and compared four strategies to compute the rational determinant of a matrix. We have proposed two preconditioning methods that allow us to transfer the problem from rational to integer domain. We believe that the approach described in this article can also be applied in other problems of exact computation in rational numbers such as rank computation or system solving. 

\begin{thebibliography}{99}

\bibitem{Abbott1999} J. Abbott, M. Bronstein, T. Mulders. Fast
deterministic computation of determinants of dense matrices. {\em ISAAC'1999}, pp. 197-204, ACM Press, 1999.





\bibitem{Dixon1982} J. Dixon. Exact Solution of Linear Equations Using
-Adic Expansions. {\em Numer.Math.} 40(1), pp. 137-141, 1982.

\bibitem{Dumas2000} J.G. Dumas. Calcul parallele du polynome minimal entier en Athapascan-1 et Linbox. {\em RenPar'2000}. pp119-124.  2000.

\bibitem{Dumas2001} J.G. Dumas, D. Saunders, G. Villard. On Efficient
Sparse Integer Matrix Smith Normal Form Computations. {\em Journal
of Symbolic Computations.} 32 (1/2), pp. 71-99, 2001.

\bibitem{Dumas2002}
J.G. Dumas, W. Turner, Z. Wan.  Exact Solution to Large Sparse
Integer Linear Systems. {\em ECCAD'2002}, 2002.

\bibitem{Dumas2002issac}
J.G. Dumas, T. Gautier, C. Pernet. FFLAS: Finite field linear algebra
subroutines. {\em ISSAC'2002}. 2002.

\bibitem{Dumas2002Linbox}
J.G. Dumas, T. Gautier, M. Giesbrecht, P. Giorgi, B. Hovinen, E. Kaltofen, D. Saunders, W. Turner, G. Villard.
 LinBox: A Generic Library for Exact Linear Algebra. {\em ICMS'2002}. 2002.



\bibitem{Dumas2004}
J.G. Dumas, P. Giorgi, C. Pernet. FFPACK: finite field linear algebra
package. {\em ISSAC'2004}. 2004.





\bibitem{jgd:2006:det}
J.G. Dumas, A. Urba\'nska. An introspective algorithm for the integer determinant.
Research report. http://arxiv.org/abs/cs.SC/0511066.



\bibitem{Eberly2000} W. Eberly, M. Giesbrecht, G. Villard. On computing
the determinant and Smith form of an integer matrix. {\em Proc.
41st FOCS}, pp. 675-687, 2000.

\bibitem{EbGies2006} W. Eberly, M.Giesbrecht, P. Giorgi,
A. Storjohann, G. Villard. Solving Sparse Integer Linear Systems. {\em
ISSAC'2006}. 2006.

\bibitem{gathen} J. von Gathen, J. Gerhard. Modern Computer Algebra. {\em Cambridge University Press} 1999.





\bibitem{Kaltofen2005} E. Kaltofen, G. Villard. On the complexity of
computing determinants. {\em Computational Complexity}, 31(3-4), pp.
91--130, 2005.

\bibitem{kho-mon:2006} S. Khodadad and M. Monagan. Fast rational function reconstruction.
{\em ISSAC'2005}, pp. 184--90. ACM Press, New York, 2006.

\bibitem{Mulders1999} T. Mulders, A. Storjohann. Diophantine Linear
System Solving. {\em ISAAC'1999}, pp. 181-188. 1999.







\bibitem{Storjohann2004} A.Storjohann. The shifted number system for
fast linear algebra on integer matrices. {\em Journal of Complexity}, 21(4),
pp. 609--650, 2005.



\bibitem{ol-sto:2006} Z. Olesh, A. Storjohann. The vector rational function reconstruction problem. {\em WWCA 2006}.



\bibitem{Wang} P.S. Wang. A -adic Algorithm for Univariate Partial Fractions. {\em Proc. of the 4th ACM Symp. on Symb. and Alg. Comp}. pp 212-217. 1981.

\bibitem{Wiedemann:1986:SSLE} D. Wiedemann. Solving sparse linear equations over Finite Fields. {\em IEEE Trans. Inf. Theory}, pp. 54-62. 1986.

\bibitem{matrixMarket} Matrix Market.
http://math.nist.gov/MatrixMarket/

\bibitem{GMP} GNU Multiprecision Package.
http://www.swox.com/gmp/
\bibitem{Givaro} Givaro library.
http://ljk.imag.fr/CASYS/LOGICIELS/givaro/



\end{thebibliography}

\end{document}
