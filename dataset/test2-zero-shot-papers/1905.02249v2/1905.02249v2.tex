\documentclass{article}









\newif\ifsubmission\submissionfalse
\PassOptionsToPackage{numbers, compress}{natbib}
\ifsubmission
 \usepackage{neurips_2019}
\else
 \usepackage[final]{neurips_2019}
\fi


\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage[textsize=tiny,textwidth=8em,backgroundcolor=yellow]{todonotes} 
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{setspace}
\usepackage{caption}

\renewcommand{\theHalgorithm}{\arabic{algorithm}}
\definecolor{mygray}{gray}{0.5}
\renewcommand{\algorithmiccomment}[1]{\;\;\;\textcolor{mygray}{\;\textit{#1}}}
\makeatletter
\newcommand{\algorithmicbreak}{\textbf{break}}
\newcommand{\BREAK}{\STATE \algorithmicbreak}
\makeatother



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\appref#1{appendix~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\Appref#1{Appendix~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\veta{{\bm{{\eta}}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\mtr}{m_{\rm{train}}}
\newcommand{\mte}{m_{\rm{test}}}
\newcommand{\padv}{p_{\rm{adv}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{{\rm p}_{\rm{model}}}
\newcommand{\Pmodel}{\rm{P}_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 

\DeclareMathOperator{\augment}{Augment}
\DeclareMathOperator{\betadist}{Beta}
\DeclareMathOperator{\guesslabel}{GuessLabel}
\DeclareMathOperator{\onehot}{OneHot}
\DeclareMathOperator{\sharpen}{Sharpen}
\DeclareMathOperator{\shuffle}{Shuffle}
\DeclareMathOperator{\concat}{Concat}
\DeclareMathOperator{\uniform}{Uniform}
\DeclareMathOperator{\mixup}{MixUp}
\DeclareMathOperator{\mixmatch}{MixMatch}
\DeclareMathOperator{\xent}{H}

\newcommand{\plabeled}{p_{\rm{labeled}}}
\newcommand{\punlabeled}{p_{\rm{unlabeled}}}
\usepackage{cleveref}

\title{MixMatch: A Holistic Approach to \\Semi-Supervised Learning}



\author{David Berthelot \\
  Google Research \\
  \texttt{dberth@google.com} \\
  \And
  Nicholas Carlini \\
  Google Research \\
  \texttt{ncarlini@google.com} \\
  \And
  Ian Goodfellow \\
  Work done at Google \\
  \texttt{ian-academic@mailfence.com} \\
  \And
  Avital Oliver \\
  Google Research \\
  \texttt{avitalo@google.com} \\
  \And
  Nicolas Papernot \\
  Google Research \\
  \texttt{papernot@google.com} \\
  \And
  Colin Raffel \\
  Google Research \\
  \texttt{craffel@google.com}
}

\begin{document}

\maketitle

\begin{abstract}
Semi-supervised learning has proven to be a powerful paradigm for leveraging unlabeled data to mitigate the reliance on large labeled datasets.
In this work, we unify the current dominant approaches for semi-supervised learning to produce a new algorithm, , that
guesses low-entropy labels for data-augmented unlabeled examples and mixes labeled and unlabeled data using .
 obtains state-of-the-art results by a large margin across many datasets and labeled data amounts. For example,
on CIFAR-10 with 250 labels, we reduce error rate by a factor of 4 (from  to ) and by a factor of 2 on STL-10.
We also demonstrate how  can help achieve a dramatically better accuracy-privacy trade-off for differential privacy.
Finally, we perform an ablation study to tease apart which components of  are most important for its success.
We release all code used in our experiments.\footnote{\url{https://github.com/google-research/mixmatch}}
\end{abstract}

\section{Introduction}

Much of the recent success in training large, deep neural networks is thanks in part to the existence of large labeled datasets.
Yet, collecting labeled data is expensive for many learning tasks because it necessarily involves expert knowledge.
This is perhaps best illustrated by medical tasks where measurements call for expensive machinery and labels are the fruit of a time-consuming analysis that draws from multiple human experts.
Furthermore, data labels may contain private information.
In comparison, in many tasks it is much easier or cheaper to obtain unlabeled data.

Semi-supervised learning \cite{chapelle2006semi} (SSL) seeks to largely alleviate the need for labeled data by allowing a model to leverage unlabeled data.
Many recent approaches for semi-supervised learning add a loss term which is computed on unlabeled data and encourages the model to generalize better to unseen data.
In much recent work, this loss term falls into one of three classes (discussed further in \Cref{sec:related_work}): entropy minimization \cite{grandvalet2005semi,lee2013pseudo}---which encourages the model to output confident predictions on unlabeled data; consistency regularization---which encourages the model to produce the same output distribution when its inputs are perturbed; and generic regularization---which encourages the model to generalize well and avoid overfitting the training data.

In this paper, we introduce , an SSL algorithm which introduces a single loss that gracefully unifies these dominant approaches to semi-supervised learning.
Unlike previous methods,  targets all the properties at once which we find leads to the following benefits:

\begin{itemize}
    \item Experimentally, we show that  obtains state-of-the-art results on all standard image benchmarks (\cref{sec:ssl_experiments}), and reducing the error rate on CIFAR-10 by a factor of 4;
    \item We further show in an ablation study that  is greater than the sum of its parts;
    \item We demonstrate in \cref{sec:dp_experiments} that  is useful for differentially private learning, enabling students in the PATE framework~\cite{papernot2016semi} to obtain new state-of-the-art results that simultaneously strengthen both privacy guarantees and accuracy.
\end{itemize}

In short,  introduces a unified loss term for unlabeled data that seamlessly reduces entropy while maintaining consistency and remaining compatible with traditional regularization techniques.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/guess_label.pdf}
    \caption{Diagram of the label guessing process used in . Stochastic data augmentation is applied to an unlabeled image  times, and each augmented image is fed through the classifier. Then, the average of these  predictions is ``sharpened'' by adjusting the distribution's temperature. See \cref{alg:mixmatch} for a full description.}
    \label{fig:guess_label}
\end{figure}

\section{Related Work}
\label{sec:related_work}

To set the stage for , we first introduce existing methods for SSL.
We focus mainly on those which are currently state-of-the-art and that  builds on; there is a wide literature on SSL techniques that we do not discuss here (e.g.,\ ``transductive'' models \cite{gammerman1998learning,joachims2003transductive,joachims1999transductive}, graph-based methods \cite{zhu2003semi,bengio2006label,liu2018deep}, generative modeling \cite{Belkin+Niyogi-2002,LasserreJ2006,Russ+Geoff-nips-2007,Coates2011b,Goodfellow2011,kingma2014semi,pu2016variational,odena2016semi,salimans2016improved}, etc.).
More comprehensive overviews are provided in \cite{zhu2003semi,chapelle2006semi}.
In the following, we will refer to a generic model  which produces a distribution over class labels  for an input  with parameters .

\subsection{Consistency Regularization}
\label{sec:consistency}

A common regularization technique in supervised learning is \textit{data augmentation}, which applies input transformations assumed to leave class semantics unaffected.
For example, in image classification, it is common to elastically deform or add noise to an input image, which can dramatically change the pixel content of an image without altering its label \cite{ciresan2010deep,simard2003best,cubuk2018autoaugment}.
Roughly speaking, this can artificially expand the size of a training set by generating a near-infinite stream of new, modified data.
\textit{Consistency regularization} applies data augmentation to semi-supervised learning by leveraging the idea that a classifier should output the same class distribution for an unlabeled example even after it has been augmented.
More formally, consistency regularization enforces that an unlabeled example  should be classified the same as , an augmentation of itself.

In the simplest case, for unlabeled points , prior work \cite{laine2016temporal,sajjadi2016regularization} adds the loss term

Note that  is a stochastic transformation, so the two terms in \cref{eq:pi_model} are not identical.
``Mean Teacher'' \cite{tarvainen2017weight} replaces one of the terms in \cref{eq:pi_model} with the output of the model using an exponential moving average of model parameter values.
This provides a more stable target and was found empirically to significantly improve results.
A drawback to these approaches is that they use domain-specific data augmentation strategies.
``Virtual Adversarial Training'' \cite{miyato2018virtual} (VAT) addresses this by instead computing an additive perturbation to apply to the input which maximally changes the output class distribution.
MixMatch utilizes a form of consistency regularization through the use of standard data augmentation for images (random horizontal flips and crops).

\subsection{Entropy Minimization}
\label{sec:entmin}

A common underlying assumption in many semi-supervised learning methods is that the classifier's decision boundary should not pass through high-density regions of the marginal data distribution.
One way to enforce this is to require that the classifier output low-entropy predictions on unlabeled data.
This is done explicitly in \cite{grandvalet2005semi} with a loss term which minimizes the entropy of  for unlabeled data .
This form of entropy minimization was combined with VAT in \cite{miyato2018virtual} to obtain stronger results.
``Pseudo-Label'' \cite{lee2013pseudo} does entropy minimization implicitly by constructing hard (1-hot) labels from high-confidence predictions on unlabeled data and using these as training targets in a standard cross-entropy loss.
MixMatch also implicitly achieves entropy minimization through the use of a ``sharpening'' function on the target distribution for unlabeled data, described in \cref{sec:label_guessing}.

\subsection{Traditional Regularization}
Regularization refers to the general approach of imposing a constraint on a model to make it harder to memorize the training data and therefore hopefully make it generalize better to unseen data \cite{hinton1993keeping}.
We use weight decay which penalizes the  norm of the model parameters \cite{loshchilov2017fixing,zhang2018three}.
We also use  \cite{zhang2017mixup} in  to encourage convex behavior ``between'' examples.
We utilize  as both as a regularizer (applied to labeled datapoints) and a semi-supervised learning method (applied to unlabeled datapoints).
 has been previously applied to semi-supervised learning; in particular, the concurrent work of \cite{verma2019interpolation} uses a subset of the methodology used in MixMatch.
We clarify the differences in our ablation study (\cref{sec:ablation}).

\section{MixMatch}
\label{sec:mixmatch}

In this section, we introduce , our proposed semi-supervised learning method.
 is a ``holistic'' approach which incorporates ideas and components from the dominant paradigms for SSL discussed in \cref{sec:related_work}.
Given a batch  of labeled examples with one-hot targets (representing one of  possible labels) and an equally-sized batch  of unlabeled examples,  produces a processed batch of augmented labeled examples  and a batch of augmented unlabeled examples with ``guessed'' labels .
 and  are then used in computing separate labeled and unlabeled loss terms.
More formally, the combined loss  for semi-supervised learning is defined as

where  is the cross-entropy between distributions  and , and , , , and  are hyperparameters described below.
The full  algorithm is provided in \cref{alg:mixmatch}, and a diagram of the label guessing process is shown in \cref{fig:guess_label}.
Next, we describe each part of .

\begin{algorithm*}[t]
        \caption{ takes a batch of labeled data  and a batch of unlabeled data  and produces a collection  (resp. ) of processed labeled examples (resp. unlabeled with guessed labels).}
   \label{alg:mixmatch}
\begin{algorithmic}[1]
   \footnotesize
   \STATE {\bfseries Input:} Batch of labeled examples and their one-hot labels , batch of unlabeled examples , sharpening temperature , number of augmentations ,  distribution parameter  for .
   \FOR{ \TO }
   \STATE  \COMMENT{\textit{Apply data augmentation to }} \label{line:augment_labeled} \\
   \FOR{ \TO }
   \STATE  \COMMENT{\textit{Apply  round of data augmentation to }} \label{line:augment_unlabeled} \\
   \ENDFOR
   \STATE  \COMMENT{\textit{Compute average predictions across all augmentations of }} \label{line:average_prediction} \\
   \STATE  \COMMENT{\textit{Apply temperature sharpening to the average prediction (see \cref{eqn:sharpen})}} \label{line:sharpen} \\
   \ENDFOR
   \STATE  \COMMENT{\textit{Augmented labeled examples and their labels}} \label{line:hat_x} \\
   \STATE  \COMMENT{\textit{Augmented unlabeled examples, guessed labels}} \label{line:hat_u} \\
   \STATE  \COMMENT{\textit{Combine and shuffle labeled and unlabeled data}} \label{line:w} \\
   \STATE  \COMMENT{Apply \textit{ to labeled data and entries from }} \label{line:x_prime} \\
   \STATE  \COMMENT{\textit{Apply  to unlabeled data and the rest of }} \label{line:u_prime} \\
   \RETURN 
\end{algorithmic}
\end{algorithm*}

\subsection{Data Augmentation}

As is typical in many SSL methods, we use data augmentation both on labeled and unlabeled data.
For each  in the batch of labeled data , we generate a transformed version  (\cref{alg:mixmatch}, line \ref{line:augment_labeled}).
For each  in the batch of unlabeled data , we generate  augmentations  (\cref{alg:mixmatch}, line \ref{line:augment_unlabeled}).
We use these individual augmentations to generate a ``guessed label''  for each , through a process we describe in the following subsection.

\subsection{Label Guessing}
\label{sec:label_guessing}

For each unlabeled example in ,  produces a ``guess'' for the example's label using the model's predictions.
This guess is later used in the unsupervised loss term.
To do so, we compute the average of the model's predicted class distributions across all the  augmentations of  by

in \cref{alg:mixmatch}, line \ref{line:average_prediction}.
Using data augmentation to obtain an artificial target for an unlabeled example is common in consistency regularization methods \cite{laine2016temporal,sajjadi2016regularization,tarvainen2017weight}.

\paragraph{Sharpening.} In generating a label guess, we perform one additional step inspired by the success of entropy minimization in semi-supervised learning (discussed in \cref{sec:entmin}).
Given the average prediction over augmentations , we apply a sharpening function to reduce the entropy of the label distribution.
In practice, for the sharpening function, we use the common approach of adjusting the ``temperature'' of this categorical distribution \cite{goodfellow2016deep}, which is defined as the operation

where  is some input categorical distribution (specifically in ,  is the average class prediction over augmentations , as shown in \cref{alg:mixmatch}, line \ref{line:sharpen}) and  is a hyperparameter.
As , the output of  will approach a Dirac (``one-hot'') distribution.
Since we will later use  as a target for the model's prediction for an augmentation of , lowering the temperature encourages the model to produce lower-entropy predictions.

\subsection{MixUp}

We use  for semi-supervised learning, and unlike past work for SSL we mix both labeled examples and unlabeled examples with label guesses (generated as described in \cref{sec:label_guessing}).
To be compatible with our separate loss terms, we define a slightly modified version of .
For a pair of two examples with their corresponding labels probabilities  we compute  by

where  is a hyperparameter.
Vanilla  omits \cref{eqn:lambda_prime} (i.e.\ it sets ).
Given that labeled and unlabeled examples are concatenated in the same batch, we need to preserve the order of the batch to compute individual loss components appropriately.
This is achieved by \cref{eqn:lambda_prime} which ensures that  is closer to  than to .
To apply , we first collect all augmented labeled examples with their labels and all unlabeled examples with their guessed labels into
 (\cref{alg:mixmatch}, lines \ref{line:hat_x}--\ref{line:hat_u}).
Then, we combine these collections and shuffle the result to form  which will serve as a data source for  (\cref{alg:mixmatch}, line \ref{line:w}).
For each the  example-label pair in , we compute  and add the result to the collection  (\cref{alg:mixmatch}, line \ref{line:x_prime}).
We compute  for , intentionally using the remainder of  that was not used in the construction of  (\cref{alg:mixmatch}, line \ref{line:u_prime}).
To summarize,  transforms  into , a collection of labeled examples which have had data augmentation and  (potentially mixed with an unlabeled example) applied.
Similarly,  is transformed into , a collection of multiple augmentations of each unlabeled example with corresponding label guesses.

\subsection{Loss Function}
\label{sec:loss_function}

Given our processed batches  and , we use the standard semi-supervised loss shown in \cref{eqn:l_x,eqn:l_u,eqn:l_combined}.
\Cref{eqn:l_combined} combines the typical cross-entropy loss between labels and model predictions from  with the squared  loss on predictions and guessed labels from .
We use this  loss in \cref{eqn:l_u} (the multiclass Brier score \cite{brier1950verification})  because, unlike the cross-entropy, it is bounded and less sensitive to incorrect predictions.
For this reason, it is often used as the unlabeled data loss in SSL \cite{laine2016temporal,tarvainen2017weight} as well as a measure of predictive uncertainty \cite{lakshminarayanan2017simple}.
We do not propagate gradients through computing the guessed labels, as is standard \cite{laine2016temporal,tarvainen2017weight,miyato2018virtual,oliver2018realistic}

\subsection{Hyperparameters}

Since  combines multiple mechanisms for leveraging unlabeled data, it introduces various hyperparameters -- specifically, the sharpening temperature , number of unlabeled augmentations ,  parameter for  in , and the unsupervised loss weight .
In practice, semi-supervised learning methods with many hyperparameters can be problematic because cross-validation is difficult with small validation sets \cite{oliver2018realistic,rasmus2015semi,oliver2018realistic}.
However, we find in practice that most of 's hyperparameters can be fixed and do not need to be tuned on a per-experiment or per-dataset basis.
Specifically, for all experiments we set  and .
Further, we only change  and  on a per-dataset basis; we found that  and  are good starting points for tuning.
In all experiments, we linearly ramp up  to its maximum value over the first  steps of training as is common practice \cite{tarvainen2017weight}.

\section{Experiments}
We test the effectiveness of  on standard SSL benchmarks (\cref{sec:ssl_experiments}). Our  ablation study teases apart the contribution of each of 's components (\cref{sec:ablation}).
As an additional application, we consider privacy-preserving learning in \cref{sec:dp_experiments}.


\subsection{Implementation details}
\label{sec:implementation}

Unless otherwise noted, in all experiments we use the ``Wide ResNet-28'' model from \cite{oliver2018realistic}.
Our implementation of the model and training procedure closely matches that of \cite{oliver2018realistic} (including using 5000 examples to select the hyperparameters), except for the following differences:
First, instead of decaying the learning rate, we evaluate models using an exponential moving average of their parameters with a decay rate of .
Second, we apply a weight decay of  at each update for the Wide ResNet-28 model.
Finally, we checkpoint every  training samples and report the median error rate of the last 20 checkpoints.
This simplifies the analysis at a potential cost to accuracy by,
for example, averaging checkpoints \cite{athiwaratkun2018improving} or choosing the checkpoint with the lowest validation error.

\subsection{Semi-Supervised Learning}
\label{sec:ssl_experiments}

First, we evaluate the effectiveness of  on four standard benchmark datasets: CIFAR-10 and CIFAR-100 \cite{krizhevsky2009learning}, SVHN \cite{netzer2011reading}, and STL-10 \cite{coates2011analysis}.
Standard practice for evaluating semi-supervised learning on the first three datasets is to treat most of the dataset as unlabeled and use a small portion as labeled data.
STL-10 is a dataset specifically designed for SSL, with 5,000 labeled images and 100,000 unlabeled images which are drawn from a slightly different distribution than the labeled data.

\subsubsection{Baseline Methods}

As baselines, we consider the four methods considered in \cite{oliver2018realistic} (-Model \cite{laine2016temporal,sajjadi2016regularization}, Mean Teacher \cite{tarvainen2017weight}, Virtual Adversarial Training \cite{miyato2018virtual}, and Pseudo-Label \cite{lee2013pseudo}) which are described in \cref{sec:related_work}.
We also use  \cite{zhang2017mixup} on its own as a baseline.
 is designed as a regularizer for supervised learning, so we modify it for SSL by applying it both to augmented labeled examples and augmented unlabeled examples with their corresponding predictions.
In accordance with standard usage of , we use a cross-entropy loss between the -generated guess label and the model's prediction.
As advocated by \cite{oliver2018realistic}, we reimplemented each of these methods in the same codebase and applied them to the same model (described in \cref{sec:implementation}) to ensure a fair comparison.
We re-tuned the hyperparameters for each baseline method, which generally resulted in a marginal accuracy improvement compared to those in \cite{oliver2018realistic}, thereby providing a more competitive experimental setting for testing out .

\begin{figure}[t]
  \begin{minipage}[l]{\textwidth}
    \begin{minipage}[t]{0.48\textwidth}
      \centering
      \centerline{\includegraphics[width=\columnwidth]{figures/cifar10_vary_labeled.pdf}}
      \captionof{figure}{
        Error rate comparison of  to baseline methods on CIFAR-10 for a varying number of labels.
        Exact numbers are provided in \cref{tab:cifar10} (appendix).
        ``Supervised'' refers to training with all  training examples and no unlabeled data.
        With  labels  reaches an error rate comparable to next-best method's performance with  labels.}
      \label{fig:vary_cifar10}
    \end{minipage}\hfill
    \begin{minipage}[t]{0.48\textwidth}
      \centering
      \centerline{\includegraphics[width=\columnwidth]{figures/svhn_noextra_vary_labeled.pdf}}
      \captionof{figure}{
        Error rate comparison of  to baseline methods on SVHN for a varying number of labels.
        Exact numbers are provided in \cref{tab:svhn} (appendix).
        ``Supervised'' refers to training with all  training examples and no unlabeled data.
        With  examples  nearly reaches the accuracy of supervised training for this model.}
    \label{fig:vary_svhn}
\end{minipage}
  \end{minipage}
\end{figure}

\subsubsection{Results}

\paragraph{CIFAR-10} For CIFAR-10, we evaluate the accuracy of each method with a varying number of labeled examples from  to  (as is standard practice).
The results can be seen in \cref{fig:vary_cifar10}.
We used  for CIFAR-10.
We created 5 splits for each number of labeled points, each with a different random seed.
Each model was trained on each split and the error rates were reported by the mean and variance across splits.
We find that  outperforms all other methods by a significant margin, for example reaching an error rate of  with  labels.
For reference, on the same model, fully supervised training on all  samples achieves an error rate of .
Furthermore,  obtains an error rate of  with only  labels.
For comparison, at  labels the next-best-performing method (VAT \cite{miyato2018virtual}) achieves an error rate of , over  higher than  considering that  is the error limit obtained on our model with fully supervised learning.
In addition, at  labels the next-best-performing method (Mean Teacher \cite{tarvainen2017weight}) obtains an error rate of , which suggests that  can achieve similar performance with only  as many labels.
We believe that the most interesting comparisons are with very few labeled data points since it reveals the method's sample efficiency which is central to SSL. 

\paragraph{CIFAR-10 and CIFAR-100 with a larger model} Some prior work \cite{tarvainen2017weight,athiwaratkun2018improving} has also considered the use of a larger,  million-parameter model.
Our base model, as used in \cite{oliver2018realistic}, has only  million parameters which confounds comparison with these results.
For a more reasonable comparison to these results, we measure the effect of increasing the width of our base ResNet model and evaluate 's performance on a 28-layer Wide Resnet model which has  filters per layer, resulting in  million parameters.
We also evaluate  on this larger model on CIFAR-100 with  labels, to compare to the corresponding result from \cite{athiwaratkun2018improving}.
The results are shown in \cref{tab:large_model}.
In general,  matches or outperforms the best results from \cite{athiwaratkun2018improving}, though we note that the comparison still remains problematic due to the fact that the model from \cite{tarvainen2017weight,athiwaratkun2018improving} also uses more sophisticated ``shake-shake'' regularization \cite{gastaldi2017shake}.
For this model, we used a weight decay of .
We used  for CIFAR-10 and  for CIFAR-100.

\begin{table}
  \parbox{.5\linewidth}{
    \centering
    \small
    \begin{tabular}{lrr}
    \toprule
      Method & CIFAR-10 & CIFAR-100 \\
      \midrule
      Mean Teacher \cite{tarvainen2017weight} &  & -\\
      SWA \cite{athiwaratkun2018improving}    &  &   \\
      \midrule
          &  &  \\
    \bottomrule
    \end{tabular}
    \vskip 0.1in
    \caption{CIFAR-10 and CIFAR-100 error rate (with  and  labels respectively) with larger models ( million parameters).}
    \label{tab:large_model}
  }
  \hfill
  \parbox{.46\linewidth}{
    \centering
    \small
    \begin{tabular}{lrr}
    \toprule
      Method &  labels &  labels \\
      \midrule
      CutOut \cite{devries2017improved} & - &  \\
      IIC \cite{ji2018invariant} & -  &  \\
      SWWAE \cite{zhao2015stacked} &  & - \\
      CC-GAN \cite{denton2016semi} &  & - \\
      \midrule
          &  & 5.59 \\
    \bottomrule
    \end{tabular}
    \vskip 0.1in
    \caption{STL-10 error rate using -label splits or the entire -label training set.}
    \label{tab:stl10}
  }
  \vskip -0.2in
\end{table}

\paragraph{SVHN and SVHN+Extra}
As with CIFAR-10, we evaluate the performance of each SSL method on SVHN with a varying number of labels from  to .
As is standard practice, we first consider the setting where the -example training set is split into labeled and unlabeled data.
The results are shown in \cref{fig:vary_svhn}.
We used .
Here again the models were evaluated on 5 splits for each number of labeled points, each with a different random seed.
We found 's performance to be relatively constant (and better than all other methods) across all amounts of labeled data.
Surprisingly, after additional tuning we were able to obtain extremely good performance from Mean Teacher \cite{tarvainen2017weight}, though its error rate was consistently slightly higher than 's.

Note that SVHN has two training sets: \textit{train} and \textit{extra}.
In fully-supervised learning, both sets are concatenated to form the full training set ( samples).
In SSL, for historical reasons the \textit{extra} set was left aside and only train was used ( samples).
We argue that leveraging both \textit{train} and \textit{extra} for the unlabeled data is more interesting since it exhibits a higher ratio of unlabeled samples over labeled ones.
We report error rates for both SVHN and SVHN+Extra in \cref{tab:svhn_nox}.
For SVHN+Extra we used  and a lower weight decay of  due to the larger amount of available data.
We found that on both training sets,  nearly matches the fully-supervised performance on the same training set almost immediately -- for example,  achieves an error rate of  with only 250 labels on SVHN+Extra compared to the fully-supervised performance of .
Interestingly, on SVHN+Extra  outperformed fully supervised training on SVHN without \textit{extra} ( error) for every labeled data amount considered.
To emphasize the importance of this, consider the following scenario:
You have  examples from SVHN with  examples labeled and are given a choice: You can either obtain  more unlabeled data and use  or obtain  more labeled data and use fully-supervised learning.
Our results suggest that obtaining additional unlabeled data and using  is more effective, which conveniently is likely much cheaper than obtaining  more labels.

\begin{table}
\centering
\begin{tabular}{lrrrrrr}
\toprule
Labels &  &  &  &  &  & All \\
\midrule
 SVHN &  &  &  &  &  &  \\
 SVHN+Extra &  &  &  &  &  &  \\
\bottomrule
\end{tabular}
\vskip 0.1in
\caption{Comparison of error rates for SVHN and SVHN+Extra for . The last column (``All'') contains the fully-supervised performance with all labels in the corresponding training set.}
\label{tab:svhn_nox}
\vskip -0.2in
\end{table}

\paragraph{STL-10}
STL-10 contains  training examples aimed at being used with  predefined folds (we use the first 5 only) with  examples each.
However, some prior work trains on all  examples.
We thus compare in both experimental settings.
With  examples  surpasses both the state-of-the-art for  examples as well as the state-of-the-art using all  labeled examples.
Note that none of the baselines in \cref{tab:stl10} use the same experimental setup (i.e. model), so it is difficult to directly compare the results; however, because  obtains the lowest error by a factor of two, we take this to be a vote in confidence of our method.
We used .

\subsubsection{Ablation Study}
\label{sec:ablation}

Since  combines various semi-supervised learning mechanisms, it has a good deal in common with existing methods in the literature.
As a result, we study the effect of removing or adding components in order to provide additional insight into what makes  performant.
Specifically, we measure the effect of
\begin{itemize}
    \item using the mean class distribution over  augmentations or using the class distribution for a single augmentation (i.e.\ setting )
    \item removing temperature sharpening (i.e.\ setting )
    \item using an exponential moving average (EMA) of model parameters when producing guessed labels, as is done by Mean Teacher \cite{tarvainen2017weight}
    \item performing  between labeled examples only, unlabeled examples only, and without mixing across labeled and unlabeled examples
    \item using Interpolation Consistency Training \cite{verma2019interpolation}, which can be seen as a special case of this ablation study where only unlabeled mixup is used, no sharpening is applied and EMA parameters are used for label guessing.
\end{itemize}
We carried out the ablation on CIFAR-10 with  and  labels; the results are shown in \cref{tab:ablation}.
We find that each component contributes to 's performance, with the most dramatic differences in the -label setting.
Despite Mean Teacher's effectiveness on SVHN (\cref{fig:vary_svhn}), we found that using a similar EMA of parameter values hurt 's performance slightly.

\begin{table}
\centering
\begin{tabular}{lrr}
\toprule
Ablation &  labels &  labels \\
\midrule
 &  &  \\
 without distribution averaging () &  &   \\
 with  &  &   \\
 with  &  &   \\
 without temperature sharpening () &  &  \\
 with parameter EMA &  &  \\
 without  &  &  \\
 with  on labeled only &  &  \\
 with  on unlabeled only &  &  \\
 with  on separate labeled and unlabeled &  &  \\
Interpolation Consistency Training \cite{verma2019interpolation} &  &  \\
\bottomrule
\end{tabular}
\vskip 0.1in
\caption{Ablation study results. All values are error rates on CIFAR-10 with  or  labels.
}
\label{tab:ablation}
\vskip -0.2in
\end{table}

\subsection{Privacy-Preserving Learning and Generalization}
\label{sec:dp_experiments}

Learning with privacy allows us to measure our approach's ability to generalize. Indeed, protecting the privacy of training data amounts to proving that the model does not overfit: a learning algorithm is said to be differentially private (the most widely accepted technical definition of privacy) if adding, modifying, or removing any of its training samples is guaranteed not to result in a statistically significant difference in the model parameters learned~\cite{dwork2016calibrating}. For this reason, learning with differential privacy is, in practice, a form of regularization~\cite{nissim2015generalization}.
Each training data access constitutes a potential privacy leakage, encoded as the pair of the input and its label. Hence, approaches for deep learning from private training data, such as DP-SGD~\citep{abadi2016deep} and PATE~\citep{papernot2016semi}, benefit from accessing as few labeled private training points as possible when computing updates to the model parameters. Semi-supervised learning is a natural fit for this setting.

We use the PATE framework for learning with privacy. A student is trained in a semi-supervised way from public \textit{unlabeled} data, part of which is labeled by an ensemble of teachers with access to private \textit{labeled} training data. The fewer labels a student requires to reach a fixed accuracy, the stronger is the privacy guarantee it provides. Teachers use a noisy voting mechanism to respond to label queries from the student, and they may choose \textit{not} to provide a label when they cannot reach a sufficiently strong consensus.
For this reason, if  improves the performance of PATE, it would also illustrate 's improved generalization from few canonical exemplars of each class. 

We compare the accuracy-privacy trade-off achieved by  to a VAT~\cite{miyato2018virtual} baseline on SVHN. VAT achieved the previous state-of-the-art of  test accuracy for a privacy loss of ~\cite{papernot2018scalable}.
Because  performs well with few labeled points, it is able to achieve  test accuracy for a much smaller privacy loss of .
Because  is used to measure the degree of privacy, the  improvement is approximately , a significant improvement. A privacy loss  below 1 corresponds to a much stronger privacy guarantee.
Note that in the private training setting the student model only uses 10,000 total examples.

\section{Conclusion}

We introduced , a semi-supervised learning method which combines ideas and components from the current dominant paradigms for SSL.
Through extensive experiments on semi-supervised and privacy-preserving learning, we found that  exhibited significantly improved performance compared to other methods in all settings we studied, often by a factor of two or more reduction in error rate.
In future work, we are interested in incorporating additional ideas from the semi-supervised learning literature into hybrid methods and continuing to explore which components result in effective algorithms.
Separately, most modern work on semi-supervised learning algorithms is evaluated on image benchmarks; we are interested in exploring the effectiveness of  in other domains.

\ifsubmission
\else
\subsubsection*{Acknowledgement}
We would like to thank Balaji Lakshminarayanan for his helpful theoretical insights.
\fi

\bibliography{biblio}
\bibliographystyle{plain}

\appendix

\newpage
\section{Notation and definitions}
{\footnotesize 
\begin{tabular}{p{1in}p{4in}}
\toprule
\textbf{Notation} & \textbf{Definition} \\
\midrule
 & Cross-entropy between ``target'' distribution  and ``predicted'' distribution \\
\midrule
 & A labeled example, used as input to a model \\
\midrule
 & A (one-hot) label\\
\midrule
 & The number of possible label classes (the dimensionality of ) \\
\midrule
 & A batch of labeled examples and their labels \\
\midrule
 & A batch of processed labeled examples produced by  \\
\midrule
 & An unlabeled example, used as input to a model \\
\midrule
 & A guessed label distribution for an unlabeled example \\
\midrule
 & A batch of unlabeled examples \\
\midrule
 & A batch of processed unlabeled examples with their label guesses produced by  \\
\midrule
 & The model's parameters \\
\midrule
 & The model's predicted distribution over classes \\
\midrule
 &  A stochastic data augmentation function that returns a modified version of . For example,  could implement randomly shifting an input image, or implement adding a perturbation sampled from a Gaussian distribution to .\\
\midrule
 & A hyper-parameter weighting the contribution of the unlabeled examples to the training loss\\
\midrule
 & Hyperparameter for the  distribution used in  \\
\midrule
 & Temperature parameter for sharpening used in  \\
\midrule
 & Number of augmentations used when guessing labels in  \\
\midrule
\end{tabular}
}

\newpage
\section{Tabular results}
\subsection{CIFAR-10}
Training the same model with supervised learning on the entire -example training set achieved an error rate of .
\begin{table}[H]
\centering
\begin{tabular}{lrrrrr}
\toprule
Methods/Labels & 250 & 500 & 1000 & 2000 & 4000 \\
\midrule
PiModel &  &  &  &  &  \\
PseudoLabel &  &  &  &  &  \\
Mixup &  &  &  &  &  \\
VAT &  &  &  &  &  \\
MeanTeacher &  &  &  &  &  \\
MixMatch &  &  &  &  &  \\
\bottomrule
\end{tabular}
\vskip 0.1in
\caption{Error rate (\%) for CIFAR10.}
\label{tab:cifar10}
\end{table}


\subsection{SVHN}
Training the same model with supervised learning on the entire -example training set achieved an error rate of .
\begin{table}[H]
\centering
\begin{tabular}{lrrrrr}
\toprule
Methods/Labels & 250 & 500 & 1000 & 2000 & 4000 \\
\midrule
PiModel &  &  &  &  &  \\
PseudoLabel &  &  &  &  &  \\
Mixup &  &  &  &  &  \\
VAT &  &  &  &  &  \\
MeanTeacher &  &  &  &  &  \\
MixMatch &  &  &  &  &  \\
\bottomrule
\end{tabular}
\vskip 0.1in
\caption{Error rate (\%) for SVHN.}
\label{tab:svhn}
\end{table}

\newpage
\subsection{SVHN+Extra}
Training the same model with supervised learning on the entire -example training set achieved an error rate of .
\begin{table}[H]
\centering
\begin{tabular}{lrrrrr}
\toprule
Methods/Labels & 250 & 500 & 1000 & 2000 & 4000 \\
\midrule
PiModel &  &  &  &  &  \\
PseudoLabel &  &  &  &  &  \\
Mixup &  &  &  &  &  \\
VAT &  &  &  &  &  \\
MeanTeacher &  &  &  &  &  \\
MixMatch &  &  &  &  &  \\
\bottomrule
\end{tabular}
\vskip 0.1in
\caption{Error rate (\%) for SVHN+Extra.}
\label{tab:svhn_extra}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/svhn_vary_labeled.pdf}
    \caption{Error rate comparison of MixMatch to baseline methods on SVHN+Extra for
    a varying number of labels. With  examples we reach nearly the state of the art compared to supervised training for this model.}
    \label{fig:vary_svhn_extra}
\end{figure}

\section{13-layer ConvNet results}

Early work on semi-supervised learning used a 13-layer convolutional network architecture \cite{miyato2018virtual,tarvainen2017weight,laine2016temporal}.
In \cref{tab:conv13} we present results on a similar architecture.
We caution against comparing these numbers directly to previous work as we use a different implementation and training process \cite{oliver2018realistic}.

\begin{table}[H]
\footnotesize
\begin{center}
\begin{tabular}{lrrrr}
\toprule
Method & \multicolumn{2}{c}{CIFAR-10} & \multicolumn{2}{c}{SVHN} \\
& 250 & 4000 & 250 & 1000 \\ 
\midrule
Mean Teacher & 46.34 & 88.57 &94.00 &96.00 \\
\textbf{MixMatch} & \textbf{85.69} & \textbf{93.16} & \textbf{96.41} & \textbf{96.61} \\
\bottomrule
\end{tabular}
\end{center}
\caption{Results on a -layer convolutional network architecture.}\label{tab:conv13}
\end{table}

\end{document}