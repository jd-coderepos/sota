\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{subfigure}
\usepackage{nidanfloat}
\graphicspath{{./images/}} 
\usepackage[caption=false]{subfig}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Thermal to Visible Face Recognition Using Deep Autoencoders}
\renewcommand{\refname}{References}
\author{\IEEEauthorblockN{Alperen Kantarc{\i}}
\IEEEauthorblockA{\textit{Department of Computer Engineering} \\
\textit{Istanbul Technical University}\\
Istanbul, Turkey\\
kantarcia@itu.edu.tr}
\and
\IEEEauthorblockN{Haz{\i}m Kemal Ekenel}
\IEEEauthorblockA{\textit{Department of Computer Engineering} \\
\textit{Istanbul Technical University}\\
Istanbul, Turkey\\
ekenel@itu.edu.tr}
}

\maketitle

\begin{abstract}
Visible face recognition systems achieve nearly perfect recognition accuracies using deep learning. 
However, in lack of light, these systems perform poorly. A way to deal with this problem is thermal to visible cross-domain face matching. This is a desired technology because of its usefulness in night time surveillance. Nevertheless, due to differences between two domains, it is a very challenging face recognition problem. In this paper, we present a deep autoencoder based system to learn the mapping between visible and thermal face images. 
Also, we assess the impact of alignment in thermal to visible face recognition. For this purpose, we manually annotate the facial landmarks on the Carl and EURECOM datasets. The proposed approach is extensively tested on three publicly available datasets: Carl, UND-X1, and EURECOM. Experimental results show that the proposed approach improves the state-of-the-art significantly. We observe that alignment increases the performance by around 2\%. Annotated facial landmark positions in this study can be downloaded from the following link: github.com/Alpkant/Thermal-to-Visible-Face-Recognition-Using-Deep-Autoencoders .
\end{abstract}

\begin{IEEEkeywords}
Convolutional neural networks, autoencoders, heterogeneous face recognition, thermal to visible matching
\end{IEEEkeywords}

\section{Introduction}
Face recognition has been a popular research area for both military and commercial purposes. Most of the face recognition technologies use visible spectrum images. In the past few years, many studies presented very high recognition accuracies. However, in the night time, thermal images are better options because thermal cameras capture heat waves of the objects without any illumination source. Night time surveillance is very crucial for the police forces and military operations in order to recognize the suspects. Most of the visible face recognition methods give poor performance under the illumination changes and low lighting. Therefore, face images that are taken at the night time lead to poor face recognition accuracies. Cross-domain face recognition is used between thermal and visible images to overcome this limitation. In thermal to visible face recognition, the task is to match the thermal probe image of a person with the visible image of the same person within the visible gallery. It is a challenging problem to match faces between thermal and visible domains due to the difference in appearance characteristics of these domains. Moreover, most thermal cameras capture low resolution images, whereas visible images are high resolution, and this increases the domain gap. 



In this paper, we aim at learning a mapping between thermal and visible domains using a deep autoencoder model. We use three thermal-visible datasets to evaluate the performance of the proposed system. In addition, we manually annotated six facial landmarks on the Carl and EURECOM datasets in order to align the faces and assess alignment's impact on the performance. Our experiments show around 14\% and 3.5\%  absolute increase in rank-1 accuracies on Carl \cite{carl2010,carl2013} and UND-X1 \cite{notredame,notredame2} datasets, respectively. We also present thermal to visible face recognition results on the recently collected EURECOM dataset \cite{eurecom}.  

The contributions of this paper can be summarized as follows: First, we have shown that deep convolutional autoencoders can learn non-linear mapping between thermal and visible images for cross domain face recognition task. Second, we analyze the impact of alignment for cross domain face recognition. Moreover, these produced annotations can also be used to develop facial landmark detection methods for thermal face images. Finally, we have improved the state-of-the-art face recognition accuracies on publicly available thermal-visible face datasets. 

The remainder of this paper is organized as follows. In Section 2, we give a brief overview of the related work. Then, in Section 3, we explain the utilized model and the approach. In Section 4, details of the datasets, benchmarks, and experimental results are presented and discussed. The last section concludes the paper and gives directions for the future work.

\section{Related Work}

One of the initial successful works before deep learning approaches for thermal to visible matching is presented in \cite{partialleastsquares}. In this study, the authors utilized partial least squares method. They investigated the effects of preprocessing and domain gap very profoundly. Mean filtering, geometric normalization, and Difference of Gaussians (DoG) filtering methods were employed and compared. We benefited from the proposed preprocessing methods in this paper.



One of the first and most successful approaches to the thermal to visible face recognition using neural networks is Deep Perceptual Mapping (DPM) \cite{deepperceptual}. In the proposed system, the authors aim at learning a mapping between the thermal and visible spectrum face images. Their simple feed forward neural network uses densely computed hand-crafted feature vectors. Deep Perceptual Mapping network has been trained with the overlapping patch crops of the faces, which correspond to the same area of the face both in thermal and visible domain. Different from this study, in this paper, we use whole face images instead of small patches of the face, the feature vectors are generated using convolutional neural networks (CNN), and the mapping is learned using a deep convolutional autoencoder. 

After the success of generative adversial networks (GANs) in image synthesis, researchers employed GANs to the thermal to visible face recognition task. TV-GAN \cite{TVGAN} is one of the most successful GAN based approaches to the problem. Aim of the TV-GAN is to transform thermal faces to visible light faces while preserving the identity information. Authors trained the discriminator to perform both binary classification of real or fake and perform closed-set face recognition task. 

SG-GAN \cite{SGGAN} synthesizes visible images from thermal images by regularizing the GAN training with semantic priors which are extracted by a face parsing network. Main novelty of this work is semantic loss function employed to regularize the training process to reduce the per-pixel loss value calculated between the synthesized visible image and thermal image.

\section{Proposed Approach}
To learn the mapping between the visible and thermal face images, we utilized a deep autoencoder architecture. Specifically, we benefited from the U-Net architecure \cite{UNetCN} and modified it for the purposes of this study.



Autoencoders are special types of neural networks that learn data embeddings in an unsupervised manner. Autoencoders are useful to encode the data to the latent code and retrieve the original data using this latent code, since their inputs are the same with their outputs. They have been used for different purposes, such as denoising, dimensionality reduction, and image reconstruction. One of the most well-known autoencoder architectures is the U-Net~\cite{UNetCN}, which is proposed for the medical segmetation task. 

\begin{figure*}[t]
	\centering
	\includegraphics[width=\textwidth]{images/architecture.png}
	\caption{\label{fig:arch-illustration}Illustration of the network architecture }
\end{figure*}

Our network is based on the U-Net architecture~\cite{UNetCN}. U-Net uses 1024 channels and  bottleneck layer. Since the amount of training samples is limited in thermal to visible face recognition, in order to decrease the number of trainable parameters, we use less channels. Our model has 512 channels and  pixels in the bottleneck layer. In the proposed approach, input images are visible face images and outputs  are  the face images from the LWIR band to learn a mapping between visible face images and thermal face images. The network takes  pixel resolution color images and the output thermal image has the same size as the input. Convolution filter size is  and window size of the max pooling is two, which is the same size as in U-Net~\cite{UNetCN}. Activation function after each batch normalization layer is Rectified Linear Unit (ReLU). For the decoder, we use two different upsampling methods. First one is bilinear upsampling, which is a standard interpolation technique. As the number of trainable parameters decrease by using the bilinear upsampling, required GPU memory reduces and training time decreases. However, information for the decoding part is lost and it degrades the performance. Therefore, besides bilinear upsampling, we also employ up convolution with  filter size as proposed in the U-Net \cite{UNetCN}. The number of total trainable parameters of the up convolution version is 14,789,059. The loss function for this network is mean square loss, since the network tries to make its output as similar as possible to the ground truth thermal image. Fig. \ref{fig:arch-illustration} shows the proposed architecture. 


\section{Datasets and Experiments}
This section presents the datasets that have been used to assess the performance of the proposed approach, the experiments carried out, and the obtained results.
\subsection{Datasets}
Three thermal-visible datasets have been used in this paper. All of them are paired LWIR-visible face datasets. For the Carl and EURECOM datasets, we manually annotated six facial landmark points: two corners for each eye and mouth corners. 

\textbf{Carl Dataset:} Carl dataset \cite{carl2010,carl2013} contains 41 different subjects with NIR, LWIR, and visible paired images. We only use LWIR and visible images. In total, there are 4920 images, half of them are visible images and the other half are LWIR thermal images. We use the same benchmark settings proposed in \cite{deepperceptual} in terms of train and test subject ratio. That is, we used images of 20 randomly chosen subjects as training data and the remaining 21 subjects as testing data. Training and testing subjects are completely different. All available thermal images of 21 subjects have been used as thermal probes which sums to 1260. There are three different visible gallery settings that have been used: one image per subject, two images per subject, and all available images of the subjects. So visible gallery contains 21, 42, and 1260 images, respectively. 

\textbf{UND-X1 Collection:} University of Notre Dame X1 Collection dataset \cite{notredame,notredame2} contains 82 different subjects with  LWIR and visible paired images. There are different expressions and images collected in different sessions. We follow the same benchmarking protocol presented in \cite{partialleastsquares} and \cite{deepperceptual}. Training set contains 41 subjects and testing set contains remaining 41 subjects. However, for each subject there are different number of images in this dataset, unlike the other datasets. Visible gallery settings are the same with the ones that we use on the Carl dataset. For each setting the number of total visible images are 41, 82, and 1146. 

\textbf{EURECOM Dataset:} EURECOM dataset has been recently presented in \cite{eurecom}. It contains seven different facial variations, five different illumination conditions, five different types of occlusions, and four different head poses that belong to 50 subjects. In total, there are 2100 images in the dataset. We manually marked six facial landmarks also on this dataset. However, it is impossible to annotate some facial landmarks in different variations, therefore we excluded some of them. More specifically, we excluded the following variations: all lights off, eyeglasses, sunglasses, mouth occluded by hand, eye occluded by hand, and poses with 30 left and right. In the end, for each subject, we have twelve images from each domain that sums up to 1200 images. Since we excluded some images and number of available images is relatively smaller than the other datasets, we chose 30 subjects to the training set and used remaining 20 subjects for testing. For testing, all available thermal images of the subjects are used as thermal probes, that sums up to 240 thermal images. For visible gallery image per subject settings are the same with the Carl and UND-X1 datasets. For the three settings 20, 40, and 240 images are used in the gallery, respectively.
\begin{figure*}[t]
	\centering
	\includegraphics[width=\linewidth]{imageprocessingpipeline.png}
	\caption{\label{fig:preprocessing}Preprocessing steps }
\end{figure*}
\subsection{Preprocessing}
Preprocessing of the images is an important task in thermal to visible face recognition. Due to the significant domain gap between the thermal and visible images, preprocessing methods would help to close the gap between these two domains and make it easier to learn the mapping between them. All the images pass through the same preprocessing steps shown in Fig. \ref{fig:preprocessing}. First  mean filter is applied in order to remove the dead pixels in the thermal images, then DoG filter is applied to enhance the edges and then both visible and thermal images are aligned according to the manually annotated facial landmarks. Because of the resolution difference between thermal images and visible images, both thermal and visible images are downsampled to  pixels which is closer to the resolution of the thermal image. Downsampling the high resolution image to lower resolution to match with the thermal image resolution helps to decrease the domain gap between two images, since resolution difference decreases between two images. Then both images are upsampled to , which is input and output resolution of the CNN. We have also tested the network without applying DoG filter and the results are presented in the following section.    



\subsection{Experiments}
As a first step, we perform preprocessing on the images. Then for each dataset, these processed thermal and visible pairs are given as input and output to the deep autoencoder architecture, respectively. Initial learning rate is 0.01 with Adam optimizer. Learning rate is halved when validation error does not change multiple epochs. Batch size is 32 for all the experiments. Training time changes according to the dataset size. Average training time takes five hours with Tesla K80 GPU. 

To perform face recognition, each gallery image has been given to the network as input and the network generated corresponding thermal image. Then each thermal probe image has been compared with these thermal images generated by the network.

We conduct experiments with three different setups. In the first two experiments we investigate the effect of the upsampling method for the decoder. For these experiments, DoG filter were not applied to images as preprocessing step. For the third experiment, we test the effect of the DoG filter on the upconvolutional autoencoder model. In order to make a fair comparison with the \cite{deepperceptual}, we also use face images that are not aligned on the Carl dataset. Results are presented as averaged rank-1 recognition accuracies over ten independent experiments. That is, for each dataset, each model is trained and tested ten different times and the obtained results are averaged.

\subsection{Results}

Table 1, 2, and 3 present the averaged rank-1 recognition scores on the Carl, UND-X1, and EURECOM datasets, respectively.
In the tables, we also include the accuracies of the state-of-the-art methods for comparison purposes. For the Carl and UND-X1 datasets, state-of-the-art models are Deep Perceptual Mapping \cite{deepperceptual} and Hu et al. \cite{partialleastsquares}. On EURECOM dataset, only \cite{crossspectrumeurecom} presented a study for the thermal to visible recognition. The authors propose an image synthesis method by using cascaded refinement network. However, their benchmark setup is different than ours and earlier thermal to visible face recognition studies. 

\begin{table*}[tb]
\caption{\label{tbl:carl}Rank-1 recognition accuracies on the Carl dataset}
\centering
\begin{tabular}{lccc}
\multicolumn{1}{c}{Models} & \multicolumn{3}{c}{\#Visible Image Gallery/Subject} \\ \hline
 & 1/subject & 2/subject & all/subject \\ \cline{2-4} 
Deep Perceptual Mapping \cite{deepperceptual} & 56.33\% & 60.08\% & 71\% \\
Partial Least Squares \cite{partialleastsquares} & 31.75\% & 34.66\% & 51.58\% \\
Model with Bilinear Upsample  & 40.2\% & 45.8\% & 75.5\% \\
Model with Bilinear Upsample (Aligned) & 42\% & 48.75\% & 77.25\% \\
Model with Up Convolution  & 41\% & 49.75\% & 80.2\% \\
Model with Up Convolution (Aligned) & 43.75\% & 52.5\% & 82.5\% \\
\textbf{Model with Up Convolution + DoG Filter } & \textbf{46.8\%} & \textbf{58.5\%} & \textbf{82.5\%}\\
\textbf{Model with Up Convolution + DoG Filter (Aligned)} & \textbf{48\%} & \textbf{60.25\%} & \textbf{85\%}
\end{tabular}
\end{table*}

The highest accuracies are reached when preprocessing, up convolution, and alignment are used. As can be seen in Table \ref{tbl:carl}, the proposed method is superior to the DPM, when two visible images per subject and all available visible images per subject are used in the gallery. Table \ref{tbl:undx1} shows that our proposed method outperforms DPM on the UND-X1 dataset in all the gallery settings. We improved the state-of-the-art by an absolute increase of 14\% in accuracy on the Carl dataset and by 3.5\% on the UND-X1 dataset when all the samples of the subjects are used in the gallery. From Table \ref{tbl:carl}, it can also be observed that alignment improves the performance by around 2\%. Up convolution is found to be more useful compared to upsampling leading to increased accuracies on all there datasets. Similarly, applying preprocessing has enhanced the performance further on all three datasets, which indicates its importance. Please note that the results presented in Table \ref{tbl:eurecom} are not directly comparable with the ones presented in \cite{crossspectrumeurecom}. In \cite{crossspectrumeurecom} authors use 45 subjects for training and 5 subjects for testing, whereas we use 30 subjects for training and 20 subjects for testing. Considering that we use less amount of training samples and have more classes to discriminate, our setup poses a more challenging problem.   

\begin{table}[b]
\caption{\label{tbl:undx1}Rank-1 recognition accuracies on the UND-X1 dataset}
\centering
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{lccc}
\multicolumn{1}{c}{Models} & \multicolumn{3}{c}{\#Visible Image Gallery/Subject} \\ \hline
 & 1/subject & 2/subject & all/subject \\ \cline{2-4} 
Deep Perceptual Mapping \cite{deepperceptual} & 55.36\% & 60.83\% & 83.73\% \\
Partial Least Squares \cite{partialleastsquares} & 44.75\% & 50.89\% & 69.86\% \\
Model with Bilinear Upsample & 42\% & 50.25\% & 75.4\% \\
Model with Up Convolution & 49.25\% & 57.5\% & 82\% \\
\textbf{Model with Up Convolution + DoG Filter} & \textbf{58.75\%} & \textbf{65.25\%} & \textbf{87.2\%}
\end{tabular}}
\end{table}



\begin{table}[b]
\caption{\label{tbl:eurecom}Rank-1 recognition accuracies on the EURECOM dataset}
\centering
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{lccc}
\multicolumn{1}{c}{Models} & \multicolumn{3}{c}{\#Visible Image Gallery/Subject} \\ \hline
 & 1/subject & 2/subject & all/subject \\ \cline{2-4} 
Model with Bilinear Upsample & 50.41\% & 58.5\% & 80\% \\
Model with Up Convolution & 53.75\% & 61.25\% & 81.25\% \\
\textbf{Model with Up Convolution + DoG Filter} & \textbf{57.91\%} & \textbf{70\%} & \textbf{88.33\%}
\end{tabular}}
\end{table}






Fig. \ref{fig:modeloutput} shows correctly matched and mismatched samples. Top row contains the outputs of the autoencoder and bottom row contains the ground truth thermal images of the subjects. As can be seen, for the correct matches, the autoencoder provides visually satisfying results. However, as in the case of mismatches, if the autoencoder fails to generate a good mapping, then consequently the classification system also fails.





\begin{figure}[t]
    \centering
    \subfigure[Correctly matched subjects]{
    \includegraphics[height=3.5cm]{images/matched_output.png}}
\qquad
\subfigure[Mismatched subjects]{
\includegraphics[height=3.5cm]{images/mismatched_output.png}}
	\caption{\label{fig:modeloutput} Top row shows the outputs of the model and bottom row shows the ground truth thermal images of the subjects. Correctly predicted subject (a) shows visually satisfying outputs and (b) shows the mismatched subject images. }
\end{figure}



\section{Conclusion}
In this paper, we show that mapping between thermal and visible images can be learned successfully by using a deep convolutional autoencoder. In addition, we show that applying preprocessing and alignment improves the performance further. We improve the state-of-the-art face recognition accuracies on two different and widely used datasets and also present results on a recently collected dataset. As a future work, we will explore different deep convolutional autoencoder architectures. We also plan to address several appearance variations, such as occlusion and head pose, in thermal face images. 


\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
