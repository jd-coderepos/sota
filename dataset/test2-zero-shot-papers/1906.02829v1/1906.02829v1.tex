\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2019}
\usepackage{times}
\usepackage{latexsym}
\usepackage{arydshln}
\usepackage{url}
\usepackage{lineno}
\usepackage{times}
\usepackage{color}
\usepackage{bm}
\usepackage{latexsym}
\usepackage{amssymb,amsmath}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subfigure}
\usepackage{url}
\usepackage{caption}
\usepackage{tabularx}
\aclfinalcopy \def\aclpaperid{2376} \usepackage{booktabs}
\usepackage{mathrsfs}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{todonotes}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\newcommand{\sfunction}[1]{\textsf{\textsc{#1}}}
\algrenewcommand\algorithmicforall{\textbf{foreach}}
\algrenewcommand\algorithmicindent{.8em}



\newcommand{\nwei}[1]{\todo[backgroundcolor=orange!40!white]{\footnotesize{Wei: #1}}}



\newcommand{\wei}[1]{{\bf \color{red} [wei:`#1']}}
\newcommand{\yangmin}[1]{{\bf \color{red} [Min:`#1']}}




\newcommand\BibTeX{B{\sc ib}\TeX}

\usepackage[normalem]{ulem}
\newcommand{\metric}[1]{\textsc{#1}\xspace}
\newcommand{\R}{\mathbb{R}}
\newcommand{\W}{\mathcal{W}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\V}{\mathcal{V}}
\newcommand{\1}{\boldsymbol{1}}

\newcommand{\bh}{\boldsymbol{h}}
\newcommand{\bv}{\boldsymbol{v}}
\newcommand{\bu}{\boldsymbol{u}}
\newcommand{\bz}{\boldsymbol{z}}
\newcommand{\bx}{\boldsymbol{x}}
\newcommand{\by}{\boldsymbol{y}}
\newcommand{\bp}{\boldsymbol{p}}
\newcommand{\bvm}{\boldsymbol{m}}
\newcommand{\f}{\boldsymbol{f}}
\newcommand{\g}{\boldsymbol{g}}
\newcommand{\fk}{\boldsymbol{k}}
\newcommand{\fd}{\boldsymbol{d}}

\newcommand{\T}{\mathsf{T}}
\newcommand{\E}{\mathcal{E}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}


\newtheorem{lemma}{Lemma}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{thm}{Theorem}
\newtheorem{corl}{Corollary}
\newtheorem{lem}{Lemma}
\newtheorem{prop}{Proposition}
\DeclareMathOperator*{\bmax}{\mathbf{max}}
\DeclareMathOperator*{\bmin}{\mathbf{min}}
\newcommand{\softmin}{\mathit{softmin}}
\newcommand{\softmax}{\mathit{softmax}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\bert}{\textsc{BERT}\xspace}
\newcommand{\bertbase}{\textsc{BERT}_{{\rm base}}\xspace}
\newcommand{\elmobase}{\textsc{ELMo}_{{\rm original}}\xspace}
\newcommand{\methodp}{P_\bert}
\newcommand{\methodr}{R_\bert}
\newcommand{\methodf}{F_\bert}
\newcommand{\idf}{{\rm idf}}
\newcommand{\WMD}{\text{WMD}}
\newcommand{\SMD}{\text{SMD}}

\title{Towards Scalable and Reliable Capsule Networks\\for Challenging NLP Applications}



\author{Wei Zhao, Haiyun Peng, Steffen Eger, Erik Cambria and Min Yang\\
{ Computer Science Department, Technische Universit\"at Darmstadt, Germany}\\
{ School of Computer Science and
Engineering, Nanyang Technological University, Singapore}\\
{ Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, China}\\
{\tt www.aiphes.tu-darmstadt.de}
}
\date{}

\begin{document}
\maketitle
\begin{abstract}
Obstacles hindering the development of capsule networks for challenging NLP applications include poor scalability to large output spaces and less reliable routing processes.
In this paper, we introduce (\romannumeral1) an agreement score to evaluate the performance of routing processes at instance level; (\romannumeral2) an adaptive optimizer to enhance the reliability of routing; (\romannumeral3) capsule compression and partial routing to improve the scalability of capsule networks. We validate our approach on two NLP tasks, namely: multi-label text classification and question answering. Experimental results show that our approach considerably improves over strong competitors on both tasks. In addition, we gain the best results in low-resource settings with few training instances.\footnote{Our code is publicly available at \href{http://bit.ly/311Dcod}{http://bit.ly/311Dcod}} 
\end{abstract}

\section{Introduction}\label{sec:introduction}
In recent years, deep neural networks have achieved outstanding success in natural language processing (NLP), computer vision and speech recognition. However, these deep models are data-hungry and generalize poorly from small datasets, very much unlike humans~\cite{Lake2015}. 

This is 
an important issue in NLP 
since 
sentences with different surface forms can convey the same meaning (paraphrases) 
and 
not all of 
them 
can be enumerated
in the training set. For example, 
\textit{Peter did not accept the offer} and \textit{Peter turned down the offer}
are semantically equivalent, but use different surface realizations. 

\iffalse
\begin{figure}
\centering
\includegraphics[width=.6\linewidth]{figures/capsule-crop.pdf}
\caption{The capsule  holds one vector representing a bigram \textit{Peter was}, and its probability  shows if it is useful for the task at hand.}
\label{fig:capsules}
\vspace{-0.1in}
\end{figure}
\fi

\begin{figure}
\centering
\includegraphics[width=\linewidth]{figures/generalization-crop.pdf}
\caption{
The extrapolation regime 
for an observed sentence can be found during training. Then, the unseen sentences in this regime may be generalized successfully.
}


\label{fig:extrapolation}
\vspace{-0.15in}
\end{figure}

In image classification, progress on the generalization ability of 
deep networks 
has been made by 
capsule networks~\cite{sabour2017dynamic,hinton2018matrix}. They are capable of generalizing to the same object in different 3D images with various viewpoints.



Such generalization capability can be learned from examples with few viewpoints by extrapolation~\cite{hinton2011transforming}.
This suggests that capsule networks can similarly abstract away from different surface realizations in NLP applications. 


Figure~\ref{fig:extrapolation} illustrates this idea of 
how observed sentences in the training set are generalized to unseen sentences by extrapolation.
In contrast, traditional neural networks require massive amounts of training samples for generalization. This is especially true in the case of convolutional neural networks (CNNs), 
where pooling operations
wrongly discard positional information and do not consider hierarchical relationships between local features~\citep{sabour2017dynamic}.



\begin{figure}[b]
\centering
\includegraphics[width=\linewidth]{figures/routing-crop.pdf}
\caption{Outputs attend to a) active neurons found by pooling operations b) all neurons c) relevant capsules found in routing processes.
}
\label{fig:layers-comparision}
\vspace{-0.1in}
\end{figure}

Capsule networks, instead, have the potential for learning hierarchical relationships between consecutive layers by using routing processes without parameters, which are clustering-like methods~\cite{sabour2017dynamic} and additionally
improve the generalization capability. We contrast such routing processes with pooling and fully connected layers in Figure~\ref{fig:layers-comparision}. 


Despite some recent success in NLP tasks~\cite{wang2018towards,xia2018zero,xiao2018mcapsnet,zhang2018attention,zhao2018investigating}, a few important 
obstacles still hinder the development of capsule networks for mature NLP applications.


For example, selecting the number of iterations is crucial for routing processes, because they iteratively route low-level capsules to high-level capsules in order to learn hierarchical relationships between layers. However, existing routing algorithms use the same number of iterations for all examples, which is not 
reliable to judge the convergence of routing.
As shown in Figure~\ref{fig:evaluate-routing}, a routing process with five iterations on all examples converges to a lower training loss at system level, but 
on instance level for one example, convergence has still not obtained. 

Additionally, training capsule networks is more difficult than traditional neural networks like CNN and long short-term memory (LSTM) due to the large number of capsules and potentially large output spaces,
which 
requires extensive computational resources in the routing process. 


In this work, we address these issues via the following contributions:
\begin{itemize}[topsep=5pt,itemsep=0pt,leftmargin=*]

\item We formulate routing processes as a proxy problem minimizing a total negative agreement score in order to 
evaluate how routing processes perform 
at instance level, which will be discussed more in depth later.

\item We introduce an adaptive optimizer to self-adjust the number of iterations for each example in order to improve instance-level convergence and enhance the reliability of routing processes.

\item We present capsule compression and partial routing to achieve better scalability of capsule networks on datasets with large output spaces.



\item Our framework outperforms strong baselines on multi-label text classification and question answering. We also demonstrate its superior generalization capability in low-resource settings. 





\end{itemize}

\begin{figure}
\begin{minipage}{0.49\linewidth} 
	\centerline{\includegraphics[width=\linewidth,height=1.25in]{figures/learning_curve_reuters-crop.pdf}} 

\end{minipage} 
\begin{minipage}{0.49\linewidth} 
	\centerline{\includegraphics[width=\linewidth]{figures/KDE_loss_curve.pdf}}

\end{minipage} 
 \caption{left) System-level routing evaluation on all examples; right) Instance-level routing evaluation on one example. 
}
\label{fig:evaluate-routing}
\vspace{-0.15in}
\end{figure} 



\section{NLP-Capsule Framework}\label{sec:approach}



We have motivated the need for better capsule networks being capable of scaling to large output spaces and higher reliability for routing processes at instance level. We now build a unified capsule framework, which we call NLP-Capsule. It is shown in Figure~\ref{fig:1} and described below.



\begin{figure*}
\centering
\includegraphics[width=\linewidth]{figures/architecture-crop.pdf}
\caption{An illustration of NLP-Capsule framework.}
\label{fig:1}
\vspace{-0.15in}
\end{figure*}

\subsection{Convolutional Layer} 
We use a convolutional operation 
to extract features from documents by taking a sliding window over document embeddings.

Let  be a matrix of stacked -dimensional word embeddings for an input document with  tokens. 
Furthermore, let  be a convolutional filter with a width . We apply this filter
to a local region  to generate one feature: 

where  denotes element-wise multiplication, and  is a nonlinear activation function (i.e., ReLU). For ease of exposition, we omit all bias terms. 

Then, we can collect all  into one feature map  after sliding the filter 
over the current document. To increase the diversity of features extraction, we concatenate multiple feature maps extracted by three filters with different window sizes (2,4,8) and pass them to the primary capsule layer.

\subsection{Primary Capsule Layer}
In this layer, we use a group-convolution operation to transform feature maps into primary capsules.
As opposed to using a scalar for each element in the feature maps,
capsules use a group of neurons to represent each element in the current layer,
which has the potential for preserving more information.




Using  filters , in total  groups are used to 
transform each scalar  in feature maps to one capsule , a - dimensional vector, denoted as:

where  and  is the concatenation operator.
Furthermore,  is a non-linear function (i.e., squashing function).
The length  of each capsule  
indicates the probability of it being useful for the task at hand.
Hence, a capsule's length has to be constrained into the unit interval  by the squashing function : 


\paragraph{Capsule Compression} 

One major issue in this layer is that the number of primary capsules becomes large in proportion to the size of the 
input documents, which requires extensive computational resources
in routing processes 
(see Section~\ref{sec:agg}). To mitigate this issue, we condense the large number of primary capsules into a smaller amount. In this way, we can merge similar capsules and remove outliers.
Each condensed capsule  is calculated by using a weighted sum over all primary capsules, denoted as: 

where the parameter  is learned by supervision.


\subsection{Aggregation Layer}\label{sec:agg}
Pooling is the simplest aggregation function routing condensed capsules into the subsequent layer, 
but it loses almost all information during aggregation. 
Alternatively, routing processes are introduced to iteratively route condensed capsules into the next layer
for
learning hierarchical relationships between two consecutive layers.
We now describe this iterative routing algorithm. 
Let  and  be a set of condensed capsules in layer  and a set of high-level capsules in layer , respectively. The basic idea of routing is two-fold.

First, we transform the condensed capsules into a collection of candidates  
for the 
-th high-level capsule in layer . 
Following~\citet{sabour2017dynamic}, each element  is calculated by: 
\vspace{-0.02in}

where  is a linear transformation matrix.

Then, we represent a high-level capsule  by a weighted sum over those candidates,
denoted as:
\vspace{-0.19in}



where  is a coupling coefficient iteratively updated by a clustering-like method.

\paragraph{Our Routing}

As discussed earlier, routing algorithms like dynamic routing~\citep{sabour2017dynamic} and EM routing~\citep{hinton2018matrix}, 
which use the same number of iterations for all samples, 
perform well according to training loss at system level, but on instance level for individual examples, 
convergence has still not been reached. This 
increases 
the risk of 
unreliability 
for routing processes (see Figure~\ref{fig:evaluate-routing}).








To evaluate the performance of routing processes at instance level, we 
formulate them as a proxy problem minimizing 
the negative agreement score (NAS) function:


The basic intuition behind this is to assign 
higher weights  to one agreeable pair  if the capsule  and  are close to each other 
such that the total agreement score  is maximized.
However, the choice of NAS functions remains an open problem.
\citet{hinton2018matrix} hypothesize that the agreeable pairs in NAS functions are from Gaussian distributions. Instead, we study NAS functions by introducing Kernel Density Estimation (KDE) since this yields a 
non-parametric density estimator requiring no
assumptions that the agreeable pairs are drawn from parametric distributions. Here, we formulate the NAS function in a KDE form.

where
 is a distance metric with  norm, and  is a Epanechnikov kernel function~\cite{wand1994kernel} with:
\vspace{-0.05in}





The solution we used for KDE is taking Mean Shift~\cite{comaniciu2002mean} to minimize the NAS function :

First,  can be updated while  is fixed:

Then,  can be updated using standard gradient descent: 

where  is the hyper-parameter to control step size.

To address the issue of convergence not being reached at instance level,
we present an adaptive optimizer to self-adjust the number of iterations for individual examples according to their negative agreement scores
(see Algorithm~\ref{alg:1}).
Following~\citet{zhao2018investigating}, we replace standard softmax with leaky-softmax, which decreases the strength of noisy capsules. 









\begin{algorithm}
\caption{Our Adaptive KDE Routing}\label{alg:1}
\begin{algorithmic}[1]
\State \textbf{procedure} ROUTING(, )
\State Initialize 
\While {true}
 \ForAll{capsule ,  in layer , }
  \State 
 \EndFor  
 
 \ForAll{capsule  in layer }
  \State 
 \EndFor  

 \ForAll{capsule ,  in layer , }
  \State 
 \EndFor 
 
 \ForAll{capsule  in layer }
  \State  
 \EndFor
 
 \State 
 \If{}
   \State \textbf{break}
 \Else
   \State 
 \EndIf
\EndWhile
\State \textbf{return} , 
\end{algorithmic}
\end{algorithm}


\subsection{Representation Layer}
This is the top-level layer containing final capsules calculated by iteratively minimizing the NAS function (See Eq.~\ref{eq:kde_loss}), where the number of final capsules corresponds to the entire output space. Therefore,
as long as the size of an output space goes to a large scale (thousands of labels), the computation of this function would become extremely expensive, which yields the bottleneck of scalability of capsule networks.





\paragraph{Partial Routing} As opposed to the entire output space on data sets, the sub-output space corresponding to individual examples is rather small, i.e., only few labels are assigned to one document in text classification, for example. As a consequence, it is 
redundant to route low-level capsules to the entire output space for each example 
in the training stage, which motivated us to
present a partial routing algorithm with constrained output spaces, such that our NAS function is described as:

where  and  denote the sets of real (positive) and randomly selected (negative) outputs for each example, respectively. Both sets are far smaller than the entire output space.



\section{Experiments}\label{sec:setup}
The major focus of this work is to investigate the scalability of our approach on datasets with a large output space, and generalizability in low-resource settings with few training examples. 
Therefore, we validate our capsule-based approach on two specific NLP tasks: (\romannumeral1) multi-label text classification with a large label scale; (\romannumeral2) question answering with a data imbalance issue.




\subsection{Multi-label Text Classification}
Multi-label text classification task refers to assigning multiple 
relevant labels to each input document, while the entire label set might be extremely large. We use our approach to encode an input document and generate the final capsules corresponding to the number of labels in the representation layer. 
The length of final capsule for each label indicates the probability 
whether the document has this label.

\begin{table}[htbp]
\centering
	\resizebox{0.8\columnwidth}{!}{
\begin{tabular}{|l|r r |}
		\toprule
		Dataset & \#Train/Test/Labels & Avg-docs\\
		\midrule
    RCV1 & 23.1K/781.2K/103 & 729.67\\
		EUR-Lex & 15.4K/3.8K/3.9K & 15.59\\
		\bottomrule
	\end{tabular}}
	\caption{Characteristics of the datasets. Each label of RCV1 has about 729.67 training examples, while each label of EUR-Lex has merely about 15.59 examples.}

	\label{tab:data}
	\vspace{-0.1in}
\end{table}

\paragraph{Experimental Setup}
We conduct our experiments 
on 
two datasets selected from the extreme classification repository:\footnote{https://manikvarma.github.io \label{repo}} a regular label scale dataset (RCV1), with 103 labels~\cite{lewis2004rcv1}, and a large label scale dataset (EUR-Lex), with 3,956 labels~\cite{mencia2008efficient}, described 
in Table~\ref{tab:data}. 
The intuition behind our datasets selection is that EUR-Lex, with 3,956 labels and 15.59 examples per label, fits well 
with our goal of investigating the scalability and generalizability of our approach. We contrast EUR-Lex with RCV1, a dataset with a regular label scale, and 
leave the study of datasets with extremely large labels, e.g., Wikipedia-500K with 501,069 labels, to future work.

\paragraph{Baselines} We compare our approach to the following baselines: non-deep learning approaches using TF-IDF features of documents as inputs: FastXML~\cite{prabhu2014fastxml}, and PD-Sparse~\cite{yen2016pd},
deep learning approaches using raw text of documents as inputs: FastText~\cite{joulin2016bag}, Bow-CNN~\cite{johnson2014effective}, CNN-Kim~\cite{kim2014convolutional}, XML-CNN~\cite{liu2017deep}), and a capsule-based approach Cap-Zhao~\cite{zhao2018investigating}. 
For evaluation, 
we use standard rank-based measures~\citep{liu2017deep} such as Precision@k, and Normalized Discounted Cumulative Gain (NDCG@). 

\paragraph{Implementation Details} The word embeddings are initialized as 300-dimensional GloVe vectors~\cite{pennington2014glove}. In the convolutional layer, we use a convolution operation with three different window sizes 
(2,4,8) to extract features from input documents. Each feature is transformed into a primary capsule with 16 dimensions by a group-convolution operation. All capsules in the primary capsule layer are condensed into 256 capsules for RCV1 and 128 capsules for EUR-Lex by a capsule compression operation.

To avoid routing low-level capsules to the entire label space in the inference stage, we use a CNN baseline~\citep{kim2014convolutional} trained on the same dataset with our approach, to generate 200 candidate labels and take these labels as a constrained output space for each example. 


\begin{table*}[htbp]
	\centering
	\resizebox{2.1\columnwidth}{!}{
	\begin{tabular}{|l|l|c|c|c|c|c|c|c|c|r|}
		\toprule
		 \textbf{Datasets} & \textbf{Metrics}& \textbf{FastXML} & 
		 \textbf{PD-Sparse} & \textbf{FastText}& \textbf{Bow-CNN} & \textbf{CNN-Kim} & \textbf{XML-CNN} & \textbf{Cap-Zhao}& \textbf{NLP-Cap} & \textbf{Impv} \\
		\midrule
		\multirow{3}{*}{RCV1}& 
		 PREC@1 & 94.62 
		 & 95.16 & 95.40 & 96.40 & 93.54 &96.86 & 96.63 & \textbf{97.05} &+0.20\% \\ 
		 & PREC@3 & 78.40 
		 & 79.46 & 79.96 & 81.17 & 76.15 & 81.11 & 81.02 & \textbf{81.27} &+0.20\% \\ 
		 & PREC@5 & 54.82 
		 & 55.61 & 55.64 & \textbf{56.74} & 52.94 & 56.07 & 56.12 & 56.33 &-0.72\%\\ 
		 & NDCG@1 & 94.62 
		 & 95.16 & 95.40 & 96.40 & 93.54 & 96.88 & 96.63 & \textbf{97.05} &+0.20\%\\ 
		 & NDCG@3 & 89.21 
		 & 90.29 & 90.95 & 92.04 & 87.26 & 92.22 & 92.31 & \textbf{92.47} &+0.17\%\\ 
		 & NDCG@5 & 90.27 
		 & 91.29 & 91.68 & 92.89 & 88.20 & 92.63 & 92.75 & \textbf{93.11} &+0.52\%\\ 
		\midrule
		
    \multirow{3}{*}{EUR-Lex}& 
		  PREC@1 & 68.12 
		  & 72.10 & 71.51 & 64.99 & 68.35 & 75.65 & - & \textbf{80.20} & +6.01\% \\
		 & PREC@3 & 57.93 
		 & 57.74 & 60.37 & 51.68 & 54.45 & 61.81 & - & \textbf{65.48} & +5.93\% \\
		 & PREC@5 & 48.97 
		 & 47.48 & 50.41 & 42.32 & 44.07 & 50.90 & - & \textbf{52.83} & +3.79\%\\
		 & NDCG@1 & 68.12 
		 & 72.10 & 71.51 & 64.99 & 68.35 & 75.65 & - & \textbf{80.20} & +6.01\% \\
		 & NDCG@3 & 60.66 
		 & 61.33 & 63.32 & 55.03 & 59.81 & 66.71 & - & \textbf{71.11} & +6.59\% \\
		 & NDCG@5 & 56.42 
		 & 55.93 & 58.56 & 49.92 & 57.99 & 64.45 & - & \textbf{68.80} & +6.75\% \\
		\bottomrule
	\end{tabular}
	}
  \caption{Comparisons of our NLP-Cap approach and baselines on two text classiï¬cation benchmarks, where '-' denotes methods that failed to scale due to memory issues.}
  \label{tab:experiment}
  \vspace{-0.05in}
\end{table*}


\paragraph{Experimental Results}
In Table~\ref{tab:experiment},
we can see a noticeable margin brought by our capsule-based approach over the strong baselines on EUR-Lex, and competitive results on RCV1. 
These results appear to indicate that our approach 
has superior generalization ability 
on datasets with fewer training examples, i.e., RCV1 has 729.67 examples per label while EUR-Lex has 15.59 examples.

In contrast to the strongest baseline XML-CNN with 22.52M parameters and 0.08 seconds per batch, our approach has 14.06M parameters, and takes 0.25 seconds in an acceleration setting with capsule compression and partial routing, and 1.7 seconds without acceleration.
This demonstrates that our approach 
provides competitive computational speed with fewer parameters compared to the competitors.

\paragraph{Discussion on Generalization}
To further study the generalization capability of our approach, we 
vary the percentage of training examples from 100\% to 50\% on the entire training set, 
leading to 
the 
number of training examples per label 
decreasing
from 15.59 to 7.77. Figure~\ref{fig:percentage} shows that our approach outperforms 
the strongest baseline XML-CNN with different fractions of the training examples. 

This finding agrees with our speculation on generalization:
the distance between our approach and XML-CNN increases as fewer training data samples are available. In Table~\ref{tab:percentage}, we also find that our approach with 70\% of training examples achieves about 5\% improvement over XML-CNN with 100\% of examples on 4 out of 6 metrics.

\begin{figure}[t]
\centering
\subfigure{\includegraphics[width=0.49\columnwidth]{figures/Examples-P3-crop.pdf}}
\subfigure{\includegraphics[width=0.49\columnwidth]{figures/Examples-NDCG3-crop.pdf}}
\caption{Performance on EUR-Lex by varying the percentage of training examples (X-axis).}\label{fig:percentage}
\vspace{-5pt}
\end{figure}

\begin{table}[t]
\centering
\resizebox{1\columnwidth}{!}{
\begin{tabular}{|l|c|c|c|c|}
\toprule
Method & \#Training & PREC@1 & PREC@3 & PREC@5 \\
\midrule
\multirow{1}{*}{XML-CNN}
&100\% examples & 75.65 & 61.81 & 50.90 \\ 
\midrule
\multirow{3}{*}{NLP-Capsule}&
50\% examples & 73.69 & 56.62 & 44.36 \\
&60\% examples & 74.83 & 58.48 & 46.33 \\
&70\% examples & 77.26 & 60.90 & 47.73\\ 
&80\% examples & 77.68 & 61.06 & 48.28 \\
&90\% examples & 79.45 & 63.95 & 50.90 \\
&100\% examples & \textbf{80.20} & \textbf{65.48} & \textbf{52.83}\\
\bottomrule
Method & \#Training & NDCG@1 & NDCG@3 & NDCG@5 \\
\midrule
\multirow{1}{*}{XML-CNN}
&100\% examples & 75.65 & 66.71 & 64.45 \\ 
\midrule
\multirow{3}{*}{NLP-Capsule}&
50\% examples & 73.69 & 66.65 & 67.36 \\
&60\% examples & 74.83 & 67.87 & 68.62 \\
&70\% examples & 77.26 & 69.79 & 69.65 \\ 
&80\% examples & 77.67 & 69.43 & 69.27 \\
&90\% examples & 79.45 & \textbf{71.64} & \textbf{71.06} \\
&100\% examples & \textbf{80.21} & 71.11 & 68.80 \\
\bottomrule
\end{tabular}
}
\caption{Experimental results on different fractions of training examples from 50\% to 100\% on EUR-Lex. \label{tab:percentage}}
\end{table}


\paragraph{Routing Comparison}
We compare our routing with \cite{sabour2017dynamic} and \cite{zhang2018fast} on EUR-Lex dataset
and observe that it performs best on all metrics (Table~\ref{tab:ablation}). We speculate that the improvement comes from enhanced reliability of routing processes at instance level.


\begin{table}[b]
	\centering
	\resizebox{1\columnwidth}{!}{
	\begin{tabular}{|l |c | c | c |}
		\toprule
     \textbf{Method}& \textbf{PREC@1} & \textbf{PREC@3} & \textbf{PREC@5} \\ 
     \midrule
    XML-CNN& 75.65 & 61.81 & 50.90 \\ 
    
    NLP-Capsule + Sabour`s Routing & 79.14 & 64.33 & 51.85 \\
    
    NLP-Capsule + Zhang`s Routing& 80.20 & 65.48 &52.83 \\
		\bottomrule
		NLP-Capsule + Our Routing & \textbf{80.62} & \textbf{65.61} &
    \textbf{53.66}\\

  	\toprule
     \textbf{Method}& \textbf{NDCG@1} & \textbf{NDCG@3} & \textbf{NDCG@5} \\ 
     \midrule
    XML-CNN& 75.65 & 66.71 & 64.45 \\ 
    
    NLP-Capsule + Sabour`s Routing & 79.14 & 70.13 & 67.02 \\
    
    NLP-Capsule + Zhang`s Routing& 80.20 & 71.11 & 68.80\\
		\bottomrule
		NLP-Capsule + Our Routing & \textbf{80.62} & \textbf{71.34} & \textbf{69.57}\\
    
  	\bottomrule
	\end{tabular}
	}
  \caption{Performance on EUR-Lex dataset with different routing process.}\label{tab:ablation}
  \vspace{-0.3cm}
\end{table}

\subsection{Question Answering}
Question-Answering (QA) selection task refers to selecting
the best answer from candidates to each question.
For a question-answer pair , we use our capsule-based approach to generate two final capsules  and  corresponding to the respective question and answer.
The relevance score of question-answer pair 
can be defined as their cosine similarity:

\paragraph{Experiment Setup}
In Table~\ref{tab:qa_data}, we conduct our experiments on the TREC QA dataset collected from TREC QA track 8-13 data~\cite{wang2007jeopardy}. The intuition behind this dataset selection is that the cost of hiring human annotators to collect positive answers for individual questions can be prohibitive since positive answers can be conveyed in multiple different surface forms. Such issue arises particularly in TREC QA with only 12\% positive answers. Therefore, we use this dataset to investigate the generalizability of our approach.


\paragraph{Baselines}We compare our approach to the following baselines: CNN + LR~\cite{yu2014deep} using unigrams and bigrams, CNN~\cite{severyn2015learning} using additional bilinear similarity features, CNTN~\cite{qiu2015convolutional} using neural tensor network, LSTM~\cite{tay2017learning} using single and multi-layer, MV-LSTM~\cite{wan2016deep}, NTN-LSTM and HD-LSTM~\cite{tay2017learning} using holographic dual LSTM and Capsule-Zhao~\cite{zhao2018investigating} using capsule networks.
For evaluation, 
we use standard measures~\cite{wang2007jeopardy} such as Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR).

\begin{table}[t]
\centering
	\resizebox{1\columnwidth}{!}{
	\begin{tabular}{| c| c | c| c|}
		\toprule
		Dataset & \#Questions & \#QA Pairs & \%Positive \\
		\midrule
		Train/Dev/Test & 1229/82/100 & 53417/1148/1517 & 12\%\\
		\bottomrule
	\end{tabular}}
	\caption{Characteristic of TREC QA dataset. \%Positive denotes the percentage of positive answers.}
	\label{tab:qa_data}
\end{table}

\begin{table}[b]
\centering
\small
	\begin{tabular}{|l| c |c|}
		\toprule
		Method & MAP & MRR\\
		\midrule
		CNN + LR (unigram) & 54.70 & 63.29 \\
		CNN + LR (bigram) & 56.93 & 66.13 \\
		CNN & 66.91 & 68.80 \\
		CNTN & 65.80 & 69.78 \\
		\midrule
		LSTM (1 layer) & 62.04 & 66.85 \\
		LSTM & 59.75 & 65.33 \\
		MV-LSTM & 64.88 & 68.24 \\
		NTN-LSTM & 63.40 & 67.72 \\
		HD-LSTM & 67.44 & \textbf{75.11} \\
		\midrule
		Capsule-Zhao & 73.63 & 70.12 \\
		\midrule
		NLP-Capsule & \textbf{77.73} & 74.16\\       
		\bottomrule
	\end{tabular}
	\caption{Experimental results on TREC QA dataset.}
	\label{tab:qa_results}
\end{table}


\paragraph{Implementation Details} The word embeddings used for question answering pairs are initialized as 300-dimensional GloVe vectors. In the convolutional layer, we use a convolution operation with three different window sizes (3,4,5). All 16-dimensional capsules in the primary capsule layer are condensed into 256 capsules by the capsule compression operation.

\paragraph{Experimental Results and Discussions}

In Table~\ref{tab:qa_results}, 
the best performance on MAP metric is achieved by our approach,

which verifies the effectiveness of our model. 
We also observe that our approach exceeds traditional neural models like CNN, LSTM and NTN-LSTM by a noticeable margin. 

This finding also agrees with the observation we found in multi-label classification: our approach has superior generalization capability in low-resource setting with few training examples.

In contrast to the strongest baseline HD-LSTM with 34.51M and 0.03 seconds for one batch, our approach has 17.84M parameters and takes 0.06 seconds in an acceleration setting, and 0.12 seconds without acceleration.

\section{Related Work}\label{sec:related}


\subsection{Multi-label Text Classification}
 Multi-label text classification aims at assigning a document to a subset of labels whose label set might be extremely large. With increasing numbers of labels, issues of data sparsity and scalability arise. 
Several methods have been proposed for the large multi-label classification case. 
 
\textbf{Tree-based models}~\cite{agrawal2013multi,weston2013label} induce a tree structure that recursively partitions the feature space with non-leaf nodes. Then, the restricted label spaces at leaf nodes are used for classification.
Such a solution entails higher robustness because of a dynamic hyper-plane design and its
computational efficiency. 
FastXML~\cite{prabhu2014fastxml} is one such tree-based model, which learns a hierarchy of training instances and optimizes an NDCG-based objective function for nodes in the tree structure.


\textbf{Label embedding models}~\cite{balasubramanian2012landmark,chen2012feature,cisse2013robust,bi2013efficient,ferng2011multi,hsu2009multi,ji2008extracting,kapoor2012multilabel,lewis2004rcv1,yu2014large} address the data sparsity issue with two steps: compression and decompression. The compression step learns a low-dimensional label embedding that is projected from original and high-dimensional label space. When data instances are classified to these label embeddings, they will be projected back to the high-dimensional label space, 
which is the decompression step. Recent works came up with different compression or decompression techniques, e.g., SLEEC~\cite{bhatia2015sparse}. 

\textbf{Deep learning models}: FastText~\cite{joulin2016bag} uses averaged word embeddings to classify documents,
which is computationally efficient but 
ignores word order. 
Various CNNs inspired by~\citet{kim2014convolutional} explored MTC with dynamic pooling, such as Bow-CNN~\cite{johnson2014effective} and XML-CNN~\cite{liu2017deep}. 

\textbf{Linear classifiers}: PD-Sparse~\cite{yen2016pd} introduces a Fully-Corrective Block-Coordinate Frank-Wolfe algorithm to address data sparsity. 





\subsection{Question and Answering}
State-of-the-art approaches to QA fall into two categories: IR-based and knowledge-based QA.

\textbf{IR-based QA} firstly preprocesses the question and employ information retrieval techniques to retrieve a list of relevant passages to questions. Next, reading comprehension techniques are adopted to extract answers within the span of retrieved text. For answer extraction, early methods manually designed patterns to get them~\cite{pacsca2003open}. A recent popular trend is neural answer extraction. Various neural network models are employed to represent questions~\cite{severyn2015learning,wang2015long}. Since the attention mechanism naturally explores relevancy, it has been widely used in QA models to relate the question to candidate answers 
\cite{tan2016improved,santos2016attentive,sha2018multi}. Moreover, some researchers leveraged external large-scale knowledge bases to assist answer selection~\cite{savenkov2017evinets,shen2018knowledge,deng2018knowledge}. 

\textbf{Knowledge-based QA} conducts semantic parsing on questions and transforms parsing results into logical forms.
Those forms are adopted to match answers from structured knowledge bases 
\cite{yao2014information,yih2015semantic,bordes2015large,yin2016simple,hao2017end}. Recent developments focused on modeling 
the interaction between question and answer pairs: Tensor layers~\cite{qiu2015convolutional,wan2016deep} and holographic composition~\cite{tay2017learning} have pushed the state-of-the-art.



\subsection{Capsule Networks}
Capsule networks were initially proposed by Hinton~\cite{hinton2011transforming} to improve representations learned by neural networks against vanilla CNNs. Subsequently, \citet{sabour2017dynamic} replaced the scalar-output feature detectors of CNNs with vector-output capsules and max-pooling with routing-by-agreement.

\citet{hinton2018matrix} then proposed a new iterative routing procedure between capsule layers based on the EM algorithm, which achieves better accuracy on the smallNORB dataset. 
\citet{zhang2018attention} applied capsule networks to relation extraction in a multi-instance multi-label learning framework.~\citet{xiao2018mcapsnet} explored capsule networks for multi-task learning.

\citet{xia2018zero} studied the zero-shot intent detection problem with capsule networks, which aims to detect emerging user intents in an unsupervised manner.
\citet{zhao2018investigating} investigated capsule networks with dynamic routing for text classification, and transferred knowledge from the single-label to multi-label cases.~\citet{Cho:2019} studied capsule networks with determinantal point processes for extractive multi-document summarization.

Our work is different from our predecessors in the following aspects: (\romannumeral1) we evaluate the performance of routing processes at instance level, and introduce an adaptive optimizer to enhance the reliability of routing processes; 
(\romannumeral2) we present capsule compression and partial routing to achieve better scalability of capsule networks on datasets with a large output space.

\section{Conclusion}\label{sec:conclusion}
Making computers perform more like humans is a major issue in NLP and machine learning. This not only includes making them perform on similar levels \cite{Hassan:2018}, but also requests them 
to be robust to adversarial examples \cite{Eger:2019} and generalize from few data points \cite{Rueckle:2019}. In this work, we have addressed the latter issue.  

In particular, we
extended existing capsule networks 
into a new framework with advantages concerning scalability, reliability and generalizability. 
Our experimental results have demonstrated its effectiveness on two NLP tasks: multi-label text classification and question answering.


Through our modifications and enhancements, we hope to have made capsule networks more suitable to large-scale problems and, hence, more mature for real-world applications.
In the future, we plan to apply capsule networks to even more challenging NLP problems such as language modeling and text generation.



\section{Acknowledgments}
We thank the anonymous reviewers for their comments, which greatly improved the final version of the paper. 
This work has been supported by the German Research Foundation as part of the Research Training
Group Adaptive Preparation of Information from Heterogeneous Sources (AIPHES) at the Technische
Universit\"at Darmstadt under grant No. GRK 1994/1.

\bibliography{acl2019}
\bibliographystyle{acl_natbib}

\clearpage

\end{document}