\subsection{Experimental Setup}

\paragraph{COCO and COCO-OCC}
We conduct experiments on COCO dataset~\cite{lin2014microsoft}, where we train on 2017{\it train} (115k images) and evaluate results on both 2017{\it val} and 2017{\it test-dev} using the standard metrics. For further investigating segmentation performance with occlusion handling, we propose a subset split, called COCO-OCC, which contains 1,005 images extracted from the validation set (5k images) where the overlapping ratio between the bounding boxes of objects is at least 0.2. Segmenting COCO-OCC with highly overlapping objects is much more difficult than 2017{\it val}, where we observe a performance gap around 3.0 for the same model in the experiment section. 

\begin{comment}
\begin{table*}[t]
	\begin{minipage}[t]{0.48\linewidth}
		\caption{Effect of the first GCN for  occlusion modeling by predicting contours and masks on COCO with ResNet-50-FPN model.}
\centering
		\resizebox{0.8\linewidth}{!}{
			\begin{tabular}{c | c | c | c | c | c }
				\toprule
				\multicolumn{2}{c|}{Occlusion Modeling} & \multicolumn{2}{c|}{COCO-OCC} & \multicolumn{2}{c}{COCO} \\
				\cline{1-6} 
				Contour & Mask &  &  &  & \\
				\midrule
				&  & 29.04 & 49.22 & 32.65 & 52.39 \\
				& \checkmark & 29.65 & 49.42 & 33.25 & 52.82 \\
				\checkmark  & & 30.18 & 49.94 & 33.41 & 53.02 \\
				\checkmark  & \checkmark  & \textbf{30.37} & \textbf{50.40} & \textbf{33.43} &  \textbf{53.12} \\
				\bottomrule
			\end{tabular}
		}
	\vspace{-0.1in}
		\label{tab:fist_GCN}
	\end{minipage}
	\label{tab:test1}
	\hfill
	\begin{minipage}[t]{0.48\linewidth}
		\caption{Effect of the second GCN for detecting target object contours for final mask prediction guided by the output of first GCN.}
\centering
		\resizebox{0.80\linewidth}{!}{
			\begin{tabular}{c | c | c | c | c | c }
				\toprule
				\multicolumn{2}{c|}{Target Modeling} & \multicolumn{2}{c|}{COCO-OCC} & \multicolumn{2}{c}{COCO} \\
				\cline{1-6} 
				Contour & Mask &  &  &  & \\
				\midrule
				& \checkmark & 30.22 & 50.25 & 33.33 & 52.96 \\
				\checkmark  & \checkmark  & \textbf{30.68} & \textbf{50.42} & \textbf{33.62} & \textbf{53.26} \\
				\bottomrule
			\end{tabular}
		}
		\vspace{-0.1in}
		\label{tab:second_GCN}
	\end{minipage}
\end{table*}
\end{comment}

\paragraph{KINS and COCOA}
We also evaluate BCNet on two amodal instance segmentation benchmarks: 
(1) \textbf{KINS}~\cite{qi2019amodal}, built on the original KITTI~\cite{geiger2012we}, is the largest amodal segmentation benchmark for traffic scenes with both annotated amodal and modal masks for instances. BCNet is trained on the training split (7,474 images and 95,311 instances) and tested on the testing split (7,517 images and 92,492 instances) following the setting in~\cite{qi2019amodal}.
(2) \textbf{COCOA}~\cite{zhu2017semantic} is a subpart of COCO~\cite{lin2014microsoft}, where we train BCNet on the official training split (2,500 images) and test on the validation split (1,323 images). Note that each instance has no class label and we only use the modal and amodal mask labels for the COCOA dataset.

\paragraph{Synthetic Occlusion Dataset}
Since most objects in COCO do not exhibit significant occlusions, we synthesize a large-scale instance segmentation dataset which contains 100k  images following uniform class distribution for instances among the 80 categories in COCO. Each synthetic image has \textit{true and complete} object contours for {\em both} occluding and partially occluded objects, thus allowing the explicit modeling of occlusion relationship between the occlusion regions and occluded objects. On the other hand, COCOA~\cite{zhu2017semantic}, which has only 5,000 images, relies on user annotation on a given training image for ``guessing" occluded object boundaries. More details on our occlusion dataset synthesis process are provided in the supplementary file.  

\subsection{Ablation Study}

\paragraph{Effect of Explicit Occlusion Modeling} We validate the efficacy of different components proposed for explicit occlusion modeling on the first GCN layer. Table~\ref{tab:fist_GCN} tabulates the  quantitative comparison: 1) Baseline: BCNet with no explicit occlusion modeling targets; 2) modeling segmentation masks for occluding regions (\textbf{occluder}); 3)  modeling contours of the occluding regions; 4) \textbf{joint} occlusion modeling on both masks and contours.   Compared to the baseline, joint occlusion modeling produces the most obvious improvement especially for the heavy occlusion cases, which promotes mask  on the standard validation set from 32.65 to 33.43, and the  on the proposed COCO-OCC split is increased from 29.04 to 30.37.

\begin{table}[!h]
	\caption{Effect of the first GCN for  occlusion modeling by predicting contours and masks on COCO with ResNet-50-FPN model.}
\centering
	\resizebox{0.85\linewidth}{!}{
		\begin{tabular}{c | c | c | c | c | c }
			\toprule
			\multicolumn{2}{c|}{Occlusion (Occluder) Modeling} & \multicolumn{2}{c|}{COCO-OCC} & \multicolumn{2}{c}{COCO} \\
			\cline{1-6} 
			Contour & Mask &  &  &  & \\
			\midrule
			&  & 29.04 & 49.22 & 32.65 & 52.39 \\
			& \checkmark & 29.65 & 49.42 & 33.25 & 52.82 \\
			\checkmark  & & 30.18 & 49.94 & 33.41 & 53.02 \\
			\checkmark  & \checkmark  & \textbf{30.37} & \textbf{50.40} & \textbf{33.43} &  \textbf{53.12} \\
			\bottomrule
		\end{tabular}
	}
	\vspace{-0.2in}
	\label{tab:fist_GCN}
\end{table}

\paragraph{Effect of Bilayer Occluder-occludee Modeling} Built on the first GCN layer with explicit occlusion modeling, we further validate the second GCN layer in Table~\ref{tab:second_GCN}, which demonstrates the importance of~\textit{occlusion-aware} feature \textit{guidance} for the second GCN layer to segment target object (\textbf{occludee}) by boosting 1.23  on COCO-OCC, and 1.06  on COCO respectively. Table~\ref{tab:bilayer} shows the results comparison on adopting the proposed~\textit{bilayer structure} and existing direct regression model with single layer. On the COCO-OCC split, bilayer GCN improves  from 29.63 to 30.68 compared to single GCN, and bilayer FCN boosts the performance of single FCN from 28.43 to 30.12.

\begin{table}[!h]
	\caption{Effect of the second GCN for detecting occludee contours for final mask prediction \textbf{\textit{guided}} by the output of first GCN.}
\centering
	\resizebox{0.95\linewidth}{!}{
		\begin{tabular}{c | c | c | c | c | c | c }
			\toprule
			\multicolumn{3}{c|}{Target (Occludee) Modeling} & \multicolumn{2}{c|}{COCO-OCC} & \multicolumn{2}{c}{COCO} \\
			\cline{1-7} 
			Guidance & Contour & Mask &  &  &  & \\
			\midrule
			& & \checkmark & 29.45 & 49.73 & 32.56 & 52.21 \\
			\checkmark & & \checkmark & 30.37 & 50.40 & 33.43 & 53.12 \\
			\checkmark & \checkmark  & \checkmark  & \textbf{30.68} & \textbf{50.62} & \textbf{33.62} & \textbf{53.26} \\
			\bottomrule
		\end{tabular}
	}
	\vspace{-0.2in}
	\label{tab:second_GCN}
\end{table}

\vspace{-0.1in}
\paragraph{Using FCN or GCN?} Table~\ref{tab:bilayer} also reveals the advantage of GCN over FCN, where GCN achieves consistent superior performance both in the singe layer and bilayer structure.
We also compute the number of parameters of each model and find that although GCN has more trainable parameters, the increased model size is acceptable compared to performance gain, because the feature size of input ROI has been down-sampled to only 1414 (spatial size) with 256 channels.

\begin{table}[!h]
	\caption{Effect of~\textbf{bilayer structure} using \textbf{GCN vs. FCN} implementation.}
\centering
	\resizebox{0.95\linewidth}{!}{
		\begin{tabular}{c | c | c | c | c | c | c | c}
			\toprule
			\multirow{2}{*}{Structure} & \multirow{2}{*}{FCN} & \multirow{2}{*}{GCN} & \multicolumn{2}{c|}{COCO-OCC} & \multicolumn{2}{c|}{COCO} & \multirow{2}{*}{Params}\\
			\cline{4-7} 
			& & &  &  &  &  &\\
			\midrule
			\multirow{2}{*}{Single Layer} & \checkmark  & & 28.43 & 48.24 & 33.01 & 52.62 & 51.0M\\
			& & \checkmark & 29.63 & 49.59 & 33.14 & 52.81 & 51.4M\\
			\midrule
			\multirow{2}{*}{Bilayer} & \checkmark  & & 30.12 & 49.04 & 33.16 & 52.80 & 53.4M\\
			& & \checkmark & \textbf{30.68} & \textbf{50.62} & \textbf{33.62} & \textbf{53.26} & 54.0M\\
			\bottomrule
		\end{tabular}
	}
	\vspace{-0.3in}
	\label{tab:bilayer}
\end{table}

\paragraph{Influence of Object Detector} 
To investigate the influence of object detectors to BCNet, besides using one-stage detector FCOS~\cite{tian2019fcos}, we also use representative two-stage detector Faster R-CNN~\cite{ren2015faster} to perform experiments.  As shown in Table~\ref{tab:detector}, the performance gain brought by BCNet is consistent, with an improvement of 2.23 (for FCOS) and 2.04 (for Faster R-CNN) mask  on COCO-OCC respectively. Here, baseline denotes mask head design in Mask R-CNN.

\begin{table}[!h]
	\caption{Influence of the object detector (FCOS vs. Faster R-CNN) on BCNet.}
\centering
	\resizebox{0.95\linewidth}{!}{
		\begin{tabular}{ l | c | c | c | c | c}
			\toprule
			\multirow{2}{*}{Model} & \multicolumn{2}{c|}{COCO-OCC} & \multicolumn{2}{c|}{COCO} & \multirow{2}{*}{Params} \\
			\cline{2-5} 
			&  &  &  &  \\
			\midrule
FCOS~\cite{lee2019centermask} + Baseline  & 28.43 & 48.24 & 33.01 & 52.62 & 51.0M\\
			FCOS~\cite{tian2019fcos} + Ours & \textbf{30.68} & \textbf{50.62} & \textbf{33.62} & \textbf{53.26} & 54.0M\\
			\midrule
Faster R-CNN~\cite{he2017mask} + Baseline & 29.67 & 49.95 & 33.45 & 53.70 & 60.0M\\
			Faster R-CNN~\cite{ren2015faster} + Ours & \textbf{31.71} & \textbf{51.15} & \textbf{34.61} & \textbf{54.41} & 63.2M \\
			\bottomrule
		\end{tabular}
	}
	\vspace{-0.15in}
	\label{tab:detector}
\end{table}

\subsection{Performance Comparison and Analysis}

\paragraph{Comparison with SOTA Methods}
Table~\ref{table:fully} compares BCNet with  state-of-the-art instance segmentation methods on COCO dataset.
BCGN achieves consistent improvement on different backbones and object detectors, demonstrating its effectiveness by outperforming both PANet~\cite{liu2018path} and Mask Scoring R-CNN~\cite{huang2019mask} by 1.5  \textit{AP} using Faster R-CNN, and exceeding CenterMask~\cite{lee2019centermask} by 1.3 \textit{AP} using FCOS. Our single model achieves comparable result with HTC~\cite{chen2019hybrid}, which uses a 3-stage cascade refinement with multiple object detectors and mask heads, and far more parameters.


\vspace{-0.2in}
\begin{table*}[h]
	\begin{minipage}[t]{0.33\linewidth}
		\caption{Results on the COCOA dataset.}
		\centering
		\resizebox{1.0\linewidth}{!}{
			\begin{tabular}{l | c | c | c}
				\toprule
				Model &  &  & \\
				\midrule
				AmodalMask~\cite{zhu2017semantic} & 5.7 & 5.9 & 0.8 \\
				AmodalMRCNN~\cite{follmann2019learning} & 21.51 & 21.09 & 9.0 \\
				ORCNN~\cite{follmann2019learning} & 20.32 & 20.63 & 7.8 \\
				\midrule
				\textbf{BCNet} & \textbf{23.09} & \textbf{22.72} & \textbf{9.53} \\
				\bottomrule
			\end{tabular}
		}
		\label{tab:cocoa}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.33\linewidth}
		\caption{Results on the KINS dataset.}
		\centering
		\resizebox{0.85\linewidth}{!}{
			\begin{tabular}{l | c | c }
				\toprule
				Model &  &  \\
				\midrule
				Mask R-CNN~\cite{follmann2019learning} & 26.97 & 24.93 \\
				Mask R-CNN + ASN~\cite{qi2019amodal} & 27.86 & 25.62 \\
				PANet~\cite{liu2018path} & 27.39 & 25.99 \\
				PANet + ASN~\cite{qi2019amodal} & 28.41 & 26.81 \\
				\midrule
				\textbf{BCNet} & \textbf{28.87} & \textbf{27.30} \\
				\bottomrule
			\end{tabular}
		}
		\label{tab:kins}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.33\linewidth}
		\caption{Results on COCO-OCC split.}
		\centering
		\resizebox{0.65\linewidth}{!}{
			\begin{tabular}{l | c | c }
				\toprule
				Model &  &  \\
				\midrule
				Mask R-CNN~\cite{he2016deep} & 29.67 & 49.95 \\
				CenterMask~\cite{lee2019centermask} & 29.05 & 49.07 \\
				MS R-CNN~\cite{huang2019mask} & 30.32 & 50.01 \\
				\midrule
				\textbf{Ours} & 31.71 & 51.15\\
				\textbf{Ours + Synthetic} & \textbf{32.89} & \textbf{53.25}\\
				\bottomrule
			\end{tabular}
		}
		\label{tab:occ_split}
	\end{minipage}
	\vspace{-0.1in}
\end{table*}

\begin{table*}[!t]
	\caption{Comparison with SOTA methods on COCO {\it test-dev} set. The mask AP is reported and all entries are single-model results. Note that HTC~\cite{chen2019hybrid} adopts 3-stage cascade refinement with multiple object detectors and mask heads. All methods are trained on COCO \emph{train2017}.} 
	\vspace{-0.05in}
	\begin{center}{\small
			\resizebox{0.8\linewidth}{!}{
				\begin{tabular}{cc|c|ccc|ccc}
					\hline
					& Method & Backbone &  &  &  &  &  &  \\ \hline
\multirow{4}{*}{} & Mask R-CNN~\cite{he2017mask} & ResNet-50 & 35.6 & 57.6 & 38.1 & 18.7 & 38.3 & 46.6 \\ & PANet~\cite{liu2018path} & ResNet-50 & 36.6 & 58.0 & 39.3 & 16.3 & 38.1 & \textbf{52.4} \\
					& \textbf{BCNet + Faster R-CNN~\cite{ren2015faster}} & ResNet-50 & \textbf{38.4} & \textbf{59.6} & \textbf{41.5} & \textbf{21.9} & \textbf{40.9} & 49.3 \\
					\hline
					& Mask R-CNN~\cite{he2017mask} & ResNet-101 & 37.0 & 59.2 & 39.5 & 17.1 & 39.3 & 52.9 \\ & MaskLab~\cite{chen2018masklab} & ResNet-101 & 37.3 & 59.8 & 39.6 & 19.1 & 40.5 & 50.6 \\
					& Mask Scoring R-CNN~\cite{huang2019mask} & ResNet-101 & 38.3 & 58.8 & 41.5 & 17.8 & 40.4 & 54.4 \\
					& BMask R-CNN~\cite{ChengWHL20} & ResNet-101 & 37.7 & 59.3 & 40.6 & 16.8 & 39.9 & \textbf{54.6} \\
					& HTC~\cite{chen2019hybrid} & ResNet-101 & 39.7 & \textbf{61.8} & 43.1 & 21.0 & 42.2 & 53.5 \\
					& \textbf{BCNet + Faster R-CNN~\cite{ren2015faster}}      & ResNet-101 & {\bf 39.8}    & 61.5 & {\bf 43.1} & {\bf 22.7} & {\bf 42.4} & 51.1  \\
					\hline
					\hline
\multirow{5}{*}{}& YOLACT~\cite{bolya2019yolact}       & ResNet-101 & 31.2 & 50.6 & 32.8 & 12.1 & 33.3 & 47.1 \\
& TensorMask~\cite{chen2019tensormask}   & ResNet-101 & 37.1 & 59.3 & 39.4 & 17.4 & 39.1 & 51.6 \\
					& ShapeMask~\cite{kuo2019shapemask}      & ResNet-101 & 37.4 & 58.1 & 40.0 & 16.1 & 40.1 & 53.8 \\ & CenterMask~\cite{lee2019centermask}    & ResNet-101 & 38.3 & - & - & 17.7 & 40.8 & {\bf 54.5} \\
					& BlendMask~\cite{chen2020blendmask}    & ResNet-101 & 38.4 & 60.7 & 41.3 & 18.2 & 41.5 & 53.3 \\
					& \textbf{BCNet + FCOS~\cite{tian2019fcos}}  & ResNet-101 & {\bf 39.6}    & {\bf 61.2} & {\bf 42.7} & {\bf 22.3} & {\bf 42.3} & 51.0  \\
					\hline
		\end{tabular}}}
	\end{center}
	\label{table:fully}
	\vspace{-0.2in}
\end{table*}

\begin{comment}
\begin{wrapfigure}{R}{0.4\textwidth}
	\centering
	\includegraphics[width=0.35\textwidth]{figures/panet_2.png}
	\caption{\label{fig:frog1}This is a fcaption xxxxxxxxxxxxxx.}
\end{wrapfigure}\leavevmode
\lipsum[1-2]


\begin{table}
	\caption{Performance comparison on COCO-OCC split.}
	\centering
	\label{wrap-tab:1}
	\resizebox{0.6\linewidth}{!}{
	\begin{tabular}{l | c | c }
	\toprule
	Model &  &  \\
	\midrule
	Mask R-CNN~\cite{he2016deep} & 29.67 & 49.95 \\
	MS R-CNN~\cite{huang2019mask} & 30.32 & 50.01 \\
	CenterMask~\cite{lee2019centermask} & 29.05 & 49.07 \\
	\midrule
	\textbf{Ours} & 31.71 & 51.15\\
	\textbf{Ours + Synthetic} & \textbf{32.89} & \textbf{53.25}\\
	\bottomrule
    \end{tabular}
}
\label{table:occ}
\end{table} 
\end{comment}

\begin{figure}[!t]
	\centering
\includegraphics[width=1.0\linewidth]{figures/amodal_cocoa_new_1009_new.pdf}
	\caption{Qualitative results comparison of the \textbf{amodal }mask predictions on \textbf{COCOA}~\cite{zhu2017semantic} by AmodalMRCNN~\cite{follmann2019learning}, ORCNN~\cite{follmann2019learning} and our method using ResNet-50, where BCNet hallucinates a more reasonable shape for the baby carriage without producing a large portion of segmentation error. We remove the ``stuff'' background for more clarity. }
	\label{fig:amodal_example1}
	\vspace{-0.1in}
\end{figure}

\begin{figure}[!t]
	\centering
\includegraphics[width=1.0\linewidth]{figures/amodal_examples_newc.pdf}
	\caption{Qualitative results comparison of the \textbf{amodal} mask predictions on \textbf{KINS}~\cite{qi2019amodal} by Mask R-CNN + ASN~\cite{qi2019amodal} and ours, both using ResNet-101-FPN, where the boundaries of the two neighboring cars parked beside green-masked car are more reasonably estimated by BCNet. }
\label{fig:amodal_example2}
	\vspace{-0.1in}
\end{figure}

\begin{figure*}[!h]
	\centering
\includegraphics[width=1.0\linewidth]{figures/qualitative_1031_new2.pdf}
\vspace{-0.1in}
	\caption{Qualitative instance segmentation results of CenterMask~\cite{lee2019centermask} (top row) and our BCNet (middle row) on \textbf{COCO}~\cite{lin2014microsoft}, both using ResNet-101-FPN and FCOS detector~\cite{tian2019fcos}. The bottom row visualizes squared heatmap of contour and mask predictions by the two GCN layers for the occluder and occludee in the same~\textbf{ROI region} specified by the {\color{red} red} bounding box, which also makes the final segmentation result of BCNet more explainable than previous methods. More qualitative results are available in the supplementary file.}
	\label{fig:qualitative}
	\vspace{-0.1in}
\end{figure*}

\paragraph{Comparison with Amodal Segmentation Methods}
Table~\ref{tab:cocoa} and Table~\ref{tab:kins} compare BCNet with other SOTA amodal segmentation methods on both the COCOA~\cite{zhu2017semantic} and KINS~\cite{qi2019amodal} datasets, where: 1) AmodalMask~\cite{zhu2017semantic} directly predicts amodal masks from image patches; 2) Occlusion RCNN (ORCNN)~\cite{follmann2019learning} is an extension of Mask R-CNN with both amodal and modal mask heads; 3) ASN module~\cite{qi2019amodal} contains additional occlusion classification branch and multi-level coding. 
Compared to these occlusion handling approaches, our bilayer GCN with cascaded structure still performs favorably against the state-of-the-art methods, which shows the effectiveness of BCNet in decoupling overlapping objects and mask completion under the amodal segmentation setting.
Figure~\ref{fig:amodal_example1} and Figure~\ref{fig:amodal_example2} show the qualitative comparison on COCOA and KINS respectively.
\vspace{-0.1in}

\paragraph{Evaluation on Occluded Images}
We adopt COCO-OCC split to compare the occlusion handling ability of BCNet with other methods on images with highly overlapping objects. As shown in Table~\ref{tab:occ_split}, our BCNet with Faster R-CNN detector has 31.71 \textit{AP} \textit{vs.} 30.32 for the Mask Scoring R-CNN~\cite{huang2019mask}. By further training BCNet on the synthetic occlusion dataset, the performance of~\textit{AP} and \textit{AP} is significantly promoted to 32.89 and 53.25 respectively, which shows the advantage brought by this new dataset.
\vspace{-0.1in}

\begin{comment}
\begin{figure}[!h]
\centering
\includegraphics[width=1.0\linewidth]{figures/amodal_example.pdf}
\caption{Visualization comparison of the amodal mask prediction by Mask R-CNN + ASN~\cite{qi2019amodal} and ours, both using ResNet-101-FPN, where the occluded region of the front car is estimated more completely by BCNet. }
\label{fig:amodal_example}
\vspace{-0.3in}
\end{figure}
\end{comment}

\begin{comment}
\begin{table}[!h]
	\caption{Performance comparison on COCO-OCC split.}
	\centering
	\label{amodal_tab:1}
	\resizebox{0.6\linewidth}{!}{
		\begin{tabular}{l | c | c }
			\toprule
			Model &  &  \\
			\midrule
			Mask R-CNN~\cite{he2016deep} & 29.67 & 49.95 \\
			MS R-CNN~\cite{huang2019mask} & 30.32 & 50.01 \\
			CenterMask~\cite{lee2019centermask} & 29.05 & 49.07 \\
			\midrule
			\textbf{Ours} & 31.71 & 51.15\\
			\textbf{Ours + Synthetic} & \textbf{32.89} & \textbf{53.25}\\
			\bottomrule
		\end{tabular}
	}
\end{table} 
\end{comment}

\paragraph{Qualitative Evaluation.}
Figure~\ref{fig:qualitative} shows 
qualitative comparison of CenterMask~\cite{lee2019centermask} and BCNet on images with overlapping objects. In each ROI region, GCN-1 detects occluding regions while GCN-2 models the partially occluded instance by directly regressing the contours and masks. For example, BCNet decouples the occluding and occluded baseball players in similar clothes into GCN-1 and GCN-2 respectively, and detects the left leg missed by CenterMask. See supplementary file for more visual comparisons.
\vspace{-0.05in}

\begin{comment}
\begin{table*}[t]
	\begin{minipage}[t]{0.48\linewidth}
		\caption{Effect of~\textbf{bilayer structure} using \textbf{GCN vs. FCN} implementation.}
\centering
		\resizebox{0.9\linewidth}{!}{
			\begin{tabular}{c | c | c | c | c | c | c | c}
				\toprule
				\multirow{2}{*}{Structure} & \multirow{2}{*}{FCN} & \multirow{2}{*}{GCN} & \multicolumn{2}{c|}{COCO-OCC} & \multicolumn{2}{c|}{COCO} & \multirow{2}{*}{Params}\\
				\cline{4-7} 
				& & &  &  &  &  &\\
				\midrule
				\multirow{2}{*}{Bilayer} & \checkmark  & & 30.12 & 49.04 & 33.16 & 52.80 & 53.4M\\
				& & \checkmark & \textbf{30.68} & \textbf{50.42} & \textbf{33.62} & \textbf{53.26} & 54.0M\\
				\midrule
				\multirow{2}{*}{Single} & \checkmark  & & 28.43 & 48.24 & 33.01 & 52.62 & 51.0M\\
				& & \checkmark & 29.63 & 49.59 & 33.46 & 53.02 & 51.4M\\
				\bottomrule
			\end{tabular}
		}
		\vspace{-0.2in}
		\label{tab:bilayer}
	\end{minipage}
	\label{tab:test2}
	\hfill
	\begin{minipage}[t]{0.48\linewidth}
		\caption{Influence of the object detector (FCOS vs. Faster R-CNN) on BCNet.}
\centering
		\resizebox{1.0\linewidth}{!}{
			\begin{tabular}{ l | c | c | c | c | c}
				\toprule
				\multirow{2}{*}{Model} & \multicolumn{2}{c|}{COCO-OCC} & \multicolumn{2}{c|}{COCO} & \multirow{2}{*}{Params} \\
				\cline{2-5} 
				&  &  &  &  \\
				\midrule
FCOS~\cite{tian2019fcos} + Baseline  & 28.43 & 48.24 & 33.01 & 52.62 & 51.0M\\
				FCOS~\cite{tian2019fcos} + Ours & \textbf{30.68} & \textbf{50.42} & \textbf{33.62} & \textbf{53.26} & 54.0M\\
				\midrule
Faster R-CNN~\cite{ren2015faster} + Baseline & 29.67 & 49.95 & 33.45 & 53.70 & 60.0M\\
				Faster R-CNN~\cite{ren2015faster} + Ours & \textbf{31.71} & \textbf{51.15} & \textbf{34.61} & \textbf{54.41} & 63.2M \\
				\bottomrule
			\end{tabular}
		}
		\vspace{-0.2in}
		\label{tab:detector}
	\end{minipage}
\end{table*}
\end{comment}

\begin{comment}
The documentation for \verb+natbib+ may be found at
\begin{center}
	\url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
\end{center}
Of note is the command \verb+\citet+, which produces citations appropriate for
use in inline text.  For example,
\begin{verbatim}
\citet{hasselmo} investigated\dots
\end{verbatim}
produces
\begin{quote}
	Hasselmo, et al.\ (1995) investigated\dots
\end{quote}

If you wish to load the \verb+natbib+ package with options, you may add the
following before loading the \verb+neurips_2020+ package:
\begin{verbatim}
\PassOptionsToPackage{options}{natbib}
\end{verbatim}

If \verb+natbib+ clashes with another package you load, you can add the optional
argument \verb+nonatbib+ when loading the style file:
\begin{verbatim}
\usepackage[nonatbib]{neurips_2020}
\end{verbatim}

As submission is double blind, refer to your own published work in the third
person. That is, use ``In the previous work of Jones et al.\ [4],'' not ``In our
previous work [4].'' If you cite your other papers that are not widely available
(e.g., a journal paper under review), use anonymous author names in the~\cite{aubry2014seeing}
citation, e.g., an author of the form ``A.\ Anonymous.''

\subsection{Footnotes}

Footnotes should be used sparingly.  If you do require a footnote, indicate
footnotes with a number\footnote{Sample of the first footnote.} in the
text. Place the footnotes at the bottom of the page on which they appear.
Precede the footnote with a horizontal rule of 2~inches (12~picas).

Note that footnotes are properly typeset \emph{after} punctuation
marks.\footnote{As in this example.}

\subsection{Figures}

\begin{figure}
	\centering
	\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
	\caption{Sample figure caption.}
\end{figure}

All artwork must be neat, clean, and legible. Lines should be dark enough for
purposes of reproduction. The figure number and caption always appear after the
figure. Place one line space before the figure caption and one line space after
the figure. The figure caption should be lower case (except for first word and
proper nouns); figures are numbered consecutively.

You may use color figures.  However, it is best for the figure captions and the
paper body to be legible if the paper is printed in either black/white or in
color.

\subsection{Tables}

All tables must be centered, neat, clean and legible.  The table number and
title always appear before the table.  See Table~\ref{sample-table}.

Place one line space before the table title, one line space after the
table title, and one line space after the table. The table title must
be lower case (except for first word and proper nouns); tables are
numbered consecutively.

Note that publication-quality tables \emph{do not contain vertical rules.} We
strongly suggest the use of the \verb+booktabs+ package, which allows for
typesetting high-quality, professional tables:
\begin{center}
	\url{https://www.ctan.org/pkg/booktabs}
\end{center}
This package was used to typeset Table~\ref{sample-table}.

\begin{table}
	\caption{Sample table title}
	\label{sample-table}
	\centering
	\begin{tabular}{lll}
		\toprule
		\multicolumn{2}{c}{Part}                   \\
		\cmidrule(r){1-2}
		Name     & Description     & Size (m) \\
		\midrule
		Dendrite & Input terminal  & 100     \\
		Axon     & Output terminal & 10      \\
		Soma     & Cell body       & up to   \\
		\bottomrule
	\end{tabular}
\end{table}
\end{comment}