\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}


\usepackage{appendix}
\usepackage{pdfpages}

\usepackage{multirow}
\usepackage{float}
\usepackage[belowskip=-0pt,aboveskip=0pt]{caption}
\usepackage{bbding}
\usepackage{caption}
\DeclareCaptionFont{9pt}{\fontsize{9pt}{10pt}\selectfont}
\captionsetup{font={9pt}}
\captionsetup[figure]{name={Figure},labelsep=period}
\captionsetup[table]{name={Table},labelsep=period}
\usepackage[marginal]{footmisc}

\usepackage[colorlinks,linkcolor=red]{hyperref}




\iccvfinalcopy 

\def\iccvPaperID{****} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\begin{document}

\makeatletter
\renewcommand*{\@fnsymbol}[1]{\ensuremath{\ifcase#1\or \dagger\or \dagger\or \ddagger\or
    \mathsection\or \mathparagraph\or \|\or **\or \dagger\dagger
    \or \ddagger\ddagger \else\@ctrerr\fi}}
\makeatother

\title{RankSRGAN: Generative Adversarial Networks with Ranker\\ for Image Super-Resolution}

\author{Wenlong Zhang{\textsuperscript{1}} \qquad Yihao Liu{\textsuperscript{1,2}} \qquad  Chao Dong{\textsuperscript{1,\thanks{Corresponding author (e-mail: chao.dong@siat.ac.cn) }}} \qquad Yu Qiao{\textsuperscript{1}}\\
\textsuperscript{1}ShenZhen Key Lab of Computer Vision and Pattern Recognition, SIAT-SenseTime Joint Lab,\\
 Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, China\\
\textsuperscript{2}University of Chinese Academy of Sciences \\
{\tt\small \{wl.zhang1, yh.liu4, chao.dong, yu.qiao\}@siat.ac.cn}
}



\maketitle
\ificcvfinal\thispagestyle{empty}\fi



\begin{abstract}
Generative Adversarial Networks (GAN) have demonstrated the potential to recover realistic details for single image super-resolution (SISR). To further improve the visual quality of super-resolved results, PIRM2018-SR Challenge employed perceptual metrics to assess the perceptual quality, such as PI, NIQE, and Ma. However, existing methods cannot directly optimize these indifferentiable perceptual metrics, which are shown to be highly correlated with human ratings. To address the problem, we propose Super-Resolution Generative Adversarial Networks with Ranker (RankSRGAN) to optimize generator in the direction of perceptual metrics. Specifically, we first train a Ranker which can learn the behavior of perceptual metrics and then introduce a novel rank-content loss to optimize the perceptual quality. The most appealing part is that the proposed method can combine the strengths of different SR methods to generate better results. Extensive experiments show that RankSRGAN achieves visually pleasing results and reaches state-of-the-art performance in perceptual metrics. Project page: \url{https://wenlongzhang0724.github.io/Projects/RankSRGAN}

\end{abstract}




\section{Introduction}

Single image super resolution aims at reconstructing/generating a high-resolution (HR) image from a low-resolution (LR) observation. Thanks to the strong learning capability, Convolutional Neural Networks (CNNs) have demonstrated superior performance   \cite{dong2014learning,lim2017enhanced,zhang2018residual} to the conventional example-based \cite{zhang2012single} and interpolation-based \cite{zhang2006edge} algorithms. Recent CNN-based methods can be divided into two groups. The first one regards SR as a reconstruction problem and adopts MSE as the loss function to achieve high PSNR values. However, due to the conflict between the reconstruction accuracy and visual quality, they tend to produce overly smoothed/sharpened images. To favor better visual quality, the second group casts SR as an image generation problem \cite{ledig2017photo}. By incorporating the perceptual loss \cite{bruna2015super,johnson2016perceptual} and adversarial learning \cite{ledig2017photo}, these perceptual SR methods have potential to generate realistic textures and details, thus attracted increasing attention in recent years. 

\begin{figure}[t]
\setlength{\abovecaptionskip}{-0.3cm}
\setlength{\belowcaptionskip}{-0.24cm}
\begin{center}
\includegraphics[width=0.92\linewidth]{introduction} 

\end{center}
   \caption{The comparison of RankSRGAN and the state-of-the-art perceptual SR methods on 4. NIQE: lower is better. PSNR: higher is better.}
\label{fig:1}
\label{fig:onecol}
\vskip -0.5cm
\end{figure}
The most challenging problem faced with perceptual SR methods is the evaluation. Most related works resort to user study for subjectively evaluating the visual quality \cite{blau20182018, wang2018recovering}. However, without an objective metric like PSNR/SSIM, it is hard to compare different algorithms on a fair platform, which largely prevents them from rapid development. To address this issue, a number of no-reference image quality assessment (NR-IQA) metrics are proposed, and some of them are proven to be highly correlated with human ratings \cite{blau20182018}, such as NIQE \cite{mittal2013making} (correlation 0.76) and PI \cite{blau20182018} (correlation 0.83). Specially, the PIRM2018-SR challenge \cite{blau20182018} introduced the PI metric as perceptual criteria and successfully ranked the entries. Nevertheless, most of these NR-IQA metrics are not differentiable (e.g., they include hand-crafted feature extraction or statistic regression operation), making them infeasible to serve as loss functions. Without considering NR-IQA metrics in optimization, existing perceptual SR methods could not show stable performance in the orientation of objective perceptual criteria.

To overcome this obstacle, we propose a general and differentiable model -- Ranker, which can mimic any NR-IQA metric and provide a clear goal (as loss function) for optimizing perceptual quality. Specifically, Ranker is a Siamese CNN that simulates the behavior of the perceptual metric by learning to rank approach \cite{burges2005learning}. Notably, as NR-IQA metrics have various dynamic ranges, Ranker learns their output ranking orders instead of absolute values. Just like in the real world, people tend to rank the quality of images rather than give a specific value. We equip Ranker with the standard SRGAN model and form a new perceptual SR framework -- RankSRGAN (Super-Resolution Generative Adversarial Networks with Ranker). In addition to SRGAN, the proposed framework has a rank-content loss using a well-trained Ranker to measure the output image quality. Then the SR model can be stably optimized in the orientation of specific perceptual metrics.

To train the proposed Ranker, we prepare another training dataset by labeling the outputs of different SR algorithms. Then the Ranker, with a Siamese-like architecture, could learn these ranking orders with high accuracy. The effectiveness of the Ranker is largely determined by the selected SR algorithms. To achieve the best performance, we adopt two state-of-the-art perceptual SR models -- SRGAN \cite{ledig2017photo} and ESRGAN \cite{Wang_2018_ECCV_Workshops}. As the champion of PIRM2018-SR challenge \cite{blau20182018}, ESRGAN is superior to SRGAN on average scores, but can not outperform SRGAN on all test images. When evaluating with NIQE \cite{mittal2013making}, we obtain mixed orders for these two methods. Then the Ranker will favor different algorithms on different images, rather than simply classifying an image into a binary class (SRGAN/ESRGAN). After adopting the rank-content loss, the generative network will output results with higher ranking scores. In other words, the learned SR model could combine the better parts of SRGAN and ESRGAN, and achieve superior performance both in perceptual metric and visual quality. Figure \ref{fig:1} shows an example of RankSRGAN, which fuses the imagery effects of SRGAN and ESRGAN and obtains better NIQE score. 

We have done comprehensive ablation studies to further validate the effectiveness of the proposed method. First, we distinguish our Ranker from the regression/classification network that could also mimic the perceptual metric. Then, we train and test RankSRGAN with several perceptual metrics (i.e. NIQE \cite{mittal2013making}, Ma \cite{ma2017learning}, PI \cite{blau20182018}). We further show that adopting different SR algorithms to build the dataset achieves different performance. Besides, we have also investigated the effect of different loss designs and combinations. With proper formulation, our method can clearly surpass ESRGAN and achieve state-of-the-art performance. 

In summary, the contributions of this paper are three-fold.
(1) We propose a general perceptual SR framework -- RankSRGAN that can optimize generator in the direction of indifferentiable perceptual metrics and achieve the state-of-the-art performance. 
(2) We, for the first time, utilize results of other SR methods to build training dataset. The proposed method combines the strengths of different SR methods and generates better results.  
(3) The proposed SR framework is highly flexible and produce diverse results given different rank datasets, perceptual metrics, and loss combinations.

\section{Related work}
\label{sec:2}
\textbf{Super resolution.} Since Dong et al. \cite{dong2014learning} first introduced convolutional neural networks (CNNs) to the SR task, a series of learning-based works \cite{zhang2012single,haris2018deep,kim2016accurate, He_2019_CVPR, feng2019suppressing, gu2019blind} have achieved great improvements in terms of PSNR. For example, Kim et al. \cite{kim2016accurate} propose a deep network VDSR with gradient clipping. The residual and dense block  \cite{lim2017enhanced,zhang2018residual} are explored to improve the super-resolved results. In addition, SRGAN \cite{ledig2017photo} is proposed to generate more realistic images. Then, texture matching  \cite{sajjadi2017enhancenet} and semantic prior \cite{wang2018recovering} are introduced to improve perceptual quality. Furthermore, the perceptual index  \cite{blau20182018} consisting of NIQE \cite{mittal2013making} and  Ma \cite{ma2017learning} is adopted to measure the perceptual SR methods in the PIRM2018-SR Challenge at ECCV \cite{blau20182018}. In the Challenge, ESRGAN \cite{Wang_2018_ECCV_Workshops} achieves the state-of-the-art performance by improving network architecture and loss functions. 

\textbf{CNN for NR-IQA.} No-reference Image Quality Assessment (NR-IQA) can be implemented by learning-based models, which extract hand-crafted features from Natural Scene Statistics (NSS), such as CBIQ \cite{ye2011no}, NIQE \cite{mittal2013making}, and Ma \cite{ma2017learning}, etc. In  \cite{li2011blind}, Li et al. develop a general regression neural network to fit human subjective opinion scores with pre-extracted features. Kang et al.  \cite{kang2014convolutional,bosse2018deep} integrate a general CNN framework which can predict image quality on local regions. In addition, Liu et al.  \cite{liu2017rankiqa} propose RankIQA to tackle the problem of lacking human-annotated data in NR-IQA. They first generate large distorted images in different distortion level. Then they train a Siamese Network to learn the rank of the quality of those images, which can improve the performance of the image quality scores.

\textbf{Learning to rank.} It has been demonstrated that learning to rank approach is effective in computer vision. For instance, Devi Parikh et al.  \cite{parikh2011relative} model relative attributes using a well-learned ranking function. Yang et al. \cite{yang2016deep} first employ CNN for relative attribute ranking in a unified framework. One of the most relevant studies to our work is RankCGAN \cite{saquil2018ranking}, which investigates the use of GAN to tackle the task of image generation with semantic attributes. Unlike standard GANs that generate the image from noise input (CGAN \cite{mirza2014conditional}), RankCGAN incorporates a pairwise Ranker into CGAN architecture so that it can handle continuous attribute values with subjective measures. 

\begin{figure*}[h!]
\setlength{\abovecaptionskip}{-0.4cm}
\setlength{\belowcaptionskip}{-0.2cm}
\begin{center}
\includegraphics[width=1\linewidth]{method-cr1} 
\end{center}
   \caption{\textbf{Overview of the proposed method.} \textbf{Stage 1:} Generate pair-wise rank images by different SR models in the orientation of perceptual metrics. \textbf{Stage 2:} Train Siamese-like Ranker network. \textbf{Stage 3:} Introduce rank-content loss derived from well-trained Ranker to guide GAN training. RankSRGAN consists of a generator(G), discriminator(D), a fixed Feature extractor(F) and Ranker(R). }
\label{fig:2}
\vskip -0.25cm
\end{figure*}

\section{Method}
\label{sec:3}



\subsection{Overview of RankSRGAN}
The proposed framework is built upon the GAN-based \cite{ledig2017photo} SR approach, which consists of a generator and a discriminator. The discriminator network tries to distinguish the ground-truth images from the super-resolved results, while the generator network is trained to fool the discriminator. To obtain more natural textures, we propose to add additional constraints on the standard SRGAN \cite{ledig2017photo} by exploiting the prior knowledge of perceptual metrics to improve the visual quality of output images. The overall framework of our approach is depicted in Figure \ref{fig:2}. The pipeline involves the following three stages:

\textbf{Stage 1: Generate pair-wise rank images.} First, we employ different SR methods to generate super-resolved images on public SR datasets. Then we apply a chosen perceptual metric (e.g. NIQE) on the generated images. After that, we can pick up two images of the same content to form a pair and rank the pair-wise images according to the quality score calculated by the perceptual metric. Finally, we obtain the pair-wise images and the associated ranking labels. More details will be presented in Section \ref{4.1}.

\textbf{Stage 2: Train Ranker.} The Ranker adopts a Siamese architecture to learn the beheviour of perceptual metrics and the network structure is depicted in Section \ref{section:3.2}. We adopt margin-ranking loss, which is commonly used in ``learning to rank'' \cite{burges2005learning}, as the cost function to optimize Ranker. The learned Ranker is supposed to have the ability to rank images according to their perceptual scores.

\textbf{Stage 3: Introduce rank-content loss.} Once the Ranker is well-trained, we use it to define a rank-content loss for a standard SRGAN to generate visually pleasing images. Please see the rank-content loss in Section \ref{3.3}.

\subsection{Ranker}
\label{section:3.2}
\textbf{Rank dataset.} Similar to \cite{choi2018deep,liu2017rankiqa}, we use super-resolution results of different SR methods to represent different perceptual levels.
With a given perceptual metric, we can rank these results in a pair-wise manner. Picking any two SR images, we can get their ranking order according to the quality score measured by the perpetual metric. These pair-wise data with ranking labels form a new dataset, which is defined as the rank dataset. Then we let the proposed Ranker learn the ranking orders. Specifically, given two input images , , the ranking scores  and  can be obtained by

\begin{small}


\end{small}where  represents the network weights and  indicates the mapping function of Ranker. In order to make the Ranker output similar ranking orders as the perceptual metric, we can formulate:
\begin{small}

\end{small}where  and  represent the quality scores of image  and image , respectively. A well-trained Ranker could guide the SR model to be optimized in the orientation of the given perceptual metric.

\textbf{Siamese architecture.} The Ranker uses a Siamese-like architecture \cite{bromley1994signature,chopra2005learning,zagoruyko2015learning}, which is effective for pair-wise inputs. The architecture of Ranker is shown in Figure \ref{fig:2}. It has two identical network branches which contain a series of convolutional, LeakyReLU, pooling and full-connected layers. Here we use a Global Average Pooling layer after the Feature Extractor, thus the architecture can get rid of the limit of input size. To obtain the ranking scores, we employ a fully-connected layer as a regressor to quantify the rank results. Note that we do not aim to predict the real values of the perceptual metric since we only care about the ranking information.  Finally, the outputs of two branches are passed to the margin-ranking loss module, where we can compute the gradients and apply back-propagation to update parameters of the whole network.

\textbf{Optimization.} To train Ranker, we employ margin-ranking loss that is commonly used in sorting problems \cite{yang2016deep,liu2017rankiqa}. The margin-ranking loss is given below:
\begin{small}

\end{small}where the  and  represent the ranking scores of pair-wise images. The  is the rank label of the pair-wise training images. The margin  can control the distance between  and . Therefore, the  pair-wise training images can be optimized by:

\begin{small}

\end{small}

\subsection{RankSRGAN}
\label{3.3}
RankSRGAN consists of a standard SRGAN and the proposed Ranker, as shown in Figure \ref{fig:2}. Compared with existing SRGAN, our framework simply adds a well-trained Ranker to constrain the generator in SR space. To obtain visually pleasing super-resolved results, adversarial learning \cite{ledig2017photo,sajjadi2017enhancenet} is applied to our framework where the generator and discriminator are jointly optimized with the objective given below:
\begin{small}

\end{small}where  and  represent the probability distributions of  and  samples, respectively. In order to demonstrate the effectiveness of the proposed Ranker, we do not use complex architectural designs of GAN \cite{Wang_2018_ECCV_Workshops} but use the general SRGAN \cite{ledig2017photo}.

\textbf{Perceptual loss.} In \cite{dosovitskiy2016generating,johnson2016perceptual}, the perceptual loss is proposed to measure the perceptual similarity between two images. Instead of computing distances in image pixel space, the images are first mapped into feature space and the perceptual loss can be presented as:
\begin{small}

\end{small}where  and  represent the feature maps of HR and SR images, respectively. Here  is obtained by the 5-th convolution (before maxpooling) layer within VGG19 network \cite{simonyan2014very}.

\textbf{Adversarial loss.} Adversarial training \cite{ledig2017photo,sajjadi2017enhancenet} is recently used to produce natural-looking images. A discriminator is trained to distinguish the real image from the generated image. This is a minimax game approach where the generator loss  is defined based on the output of discriminator:
\begin{small}

\end{small}where  is the  image,  represents the probability of the discriminator over all training samples.

\textbf{Rank-content loss.} The generated image is put into the Ranker to predict the ranking score. Then, the rank-content loss can  be defined as:

where  is the ranking score of the generated image. A lower ranking score indicates better perceptual quality. After applying the sigmoid function,  represents ranking-content loss ranging from  to .


\subsection{Analysis of Ranker}
\label{3.4}
The proposed Ranker possesses an appealing property: by elaborately selecting the SR algorithms and the perceptual metric, the RankSRGAN has the potential to surpass the upper bound of these methods and achieve superior performance. To validate this comment, we select the state-of-the-art perceptual SR methods -- SRGAN \cite{ledig2017photo} and ESRGAN \cite{Wang_2018_ECCV_Workshops} to build the rank dataset.  Then we use the perceptual metric NIQE \cite{mittal2013making} for evaluation. NIQE is demonstrated to be highly correlated with human ratings and easy to implement. A lower NIQE value indicates better perceptual quality. When measured with NIQE on the PIRM-Test \cite{blau20182018} dataset, the average scores of SRGAN and ESRGAN are 2.70 and 2.55, respectively. ESRGAN obtains better NIQE scores for most images but not all images, indicating that SRGAN and ESRGAN have mixed ranking orders with NIQE.

In order to examine the effectiveness of our proposed Ranker, we compare two ranking strategies -- metric rank and model classification. Metric rank, which is our proposed method, uses perceptual metrics to rank the images. For example, in each image pair, the one with a lower score is labeled to 1 and the other is 2. The model classification, as the comparison method, ranks images according to the used SR methods, i.e., all results of ESRGAN are labeled to 1 and those of SRGAN are labeled to 2. We then give an analysis of the upper bound of these two methods. The upper bound can be calculated as:
\begin{small}

\end{small}where  and  represent the upper bound of model classification and metric rank, respectively.  (Perceptual Metric) is the perceptual score for each image in the corresponding class. (SR1, SR2) represents two SR results of the same LR. Subscripts  and  indicate the Lower and Higher perceptual score in (SR1, SR2). We use (SRGAN, ESRGAN) as (SR1, SR2) to obtain the upper bound of these methods, as shown in Figure \ref{fig:4}. Obviously, metric rank could combine the better parts of different algorithms and exceed the upper bound of a single algorithm.

\begin{figure}[t]
\setlength{\abovecaptionskip}{-0.2cm}
\setlength{\belowcaptionskip}{-0.2cm}
\begin{center}
	\includegraphics[width=1\linewidth]{upperbound1.pdf} 

\end{center}
   \caption{The upper bound (average NIQE value) of SRGAN, ESRGAN, model rank and model classification.}
\label{fig:4}
\vskip -0.4cm
\end{figure}
We further conduct SR experiments to support the above analysis. We use the metric rank and model classification approach to label the rank dataset. Then the Ranker-MC (model classification) and Ranker-MR (metric rank) are used to train separate RankSRGAN models. Figure \ref{fig:5} shows the quantitative results (NIQE), where RankSRGAN-MR outperforms ESRGAN and RankSRGAN-MC. This demonstrates that our method can exceed the upper bound of all chosen SR algorithms.

\section{Experiments}
\subsection{Training details of Ranker}
\label{4.1}
\textbf{Datasets.} We use DIV2K (800 images) \cite{agustsson2017ntire} and Flickr2K (2650 images) \cite{agustsson2017ntire} datasets to generate pair-wise images as rank dataset for training. Three different SR algorithms (SRResNet \cite{ledig2017photo}, SRGAN \cite{ledig2017photo} and ESRGAN \cite{Wang_2018_ECCV_Workshops}) are used to generate super-resolved images as three perceptual levels, as shown in Table \ref{table:1}. 
\vskip -0.2cm

\begin{table}[!ht]
\small \setlength{\abovecaptionskip}{-3pt}
\setlength{\belowcaptionskip}{-10pt}
\begin{center}
\begin{tabular}{c|ccc}
\hline\hline
PIRM-Test & SRResNet & SRGAN & ESRGAN \\
\hline
NIQE & 5.968 & 2.705 & 2.557\\
PSNR & 28.33 & 25.62 & 25.30\\
\hline
\end{tabular}
\end{center}
\caption{The performance of super-resolved results with three perceptual levels in PIRM-Test \cite{blau20182018}}
\label{table:1}
\vskip -0.1cm
\end{table}
We extract patches from those pair-wise images with a stride of  and size of . For one perceptual level (SR algorithm), we can generate 150 K patches (10 for validation, 90 for training). Inspired by PIRM2018-SR Challenge \cite{blau20182018}, we use NIQE \cite{mittal2013making} as the perceptual metric, while other metrics will be investigated in Section \ref{section4.5}. Finally, we label every image pair to (3,2,1) according to the order of corresponding NIQE value (the one with the best NIQE value is set to 1).
\begin{figure}[t]
\setlength{\abovecaptionskip}{-0.1cm}
\setlength{\belowcaptionskip}{-0.6cm}
\begin{center}
	\includegraphics[width=1\linewidth]{upperbound2.pdf} 

\end{center}
   \caption{The NIQE of RankSRGAN-MR exceeds that of SRGAN, ESRGAN and RankSRGAN-MC.}
\label{fig:5}
\end{figure}

\textbf{Implementation details.} As shown in Figure \ref{fig:2}, we  utilize VGG \cite{simonyan2014very} structure to implement the Ranker \cite{liu2017rankiqa}, which includes 10 convolutional layers, a series of batch normalization and LeakyReLU operations. Instead of max-pooling, we apply convolutional layer with a kernel size  and stride  to downsample the features. In one iteration, two patches with different perceptual levels are randomly selected as the input of Ranker. 
For optimization, we use Adam \cite{kingma2014adam} optimizer with weight decay . The learning rate is initialized to  and decreases with a factor  of every  iterations for total  iterations. The margin  of margin-ranking loss is set to . For weight initialization, we use He. \cite{he2015delving} method to initialize the weights of Ranker. 

\textbf{Evaluation.} The Spearman Rank Order Correlation Coefficient (SROCC) \cite{liu2017rankiqa} is a traditional evaluation metric to evaluate the performance of image quality assessment algorithms. In our experiment, SROCC is employed to measure the monotonic relationship between the label and the ranking score. Given  images, the SROCC is computed as:
\begin{small}

\end{small}where  represents the order of label, and  is the order of output score of the Ranker. SROCC has the ability to measure the accuracy of Ranker. The larger value of SROCC represents the better accuracy of Ranker. For validation dataset, the ranker achieves a SROCC of , which is an adequate performance compared with those in the related work \cite{choi2018deep, liu2017rankiqa}.

\begin{table*}
\setlength{\abovecaptionskip}{0.2cm}
\setlength{\belowcaptionskip}{-0.8cm}
\begin{center}
\begin{tabular}{c|c|cccccc}
\hline\hline
Dataset & Metric & Bicubic & FSRCNN & SRResNet & SRGAN & ESRGAN & RankSRGAN (ours)\\
\hline
\multirow{3}*{Set14}&NIQE & 7.61 &6.92&6.12&3.82&3.28&\textbf{3.28}\\
       & PI & 6.97 & 6.16 &5.36&2.98&2.61&\textbf{2.61}\\
			   & PSNR & 26.08 & 27.66 &\textbf{28.57}&26.68&26.39&26.57\\
\hline
\multirow{3}*{BSD100}&NIQE & 7.60 &7.11&6.43&3.29&3.21&\textbf{3.01}\\
        & PI & 6.94 & 6.17 &5.34&2.37&2.27&\textbf{2.15}\\
	            & PSNR & 25.96 & 26.94 &\textbf{27.61}&25.67 &25.72 &25.57\\
\hline
\multirow{3}*{PIRM-Test}&NIQE & 7.45 &6.86& 5.98&2.71&2.56& \textbf{2.51}  \\
           & PI & 7.33 & 6.02 &5.18&2.09&1.98&\textbf{1.95}\\
          		   & PSNR & 26.45 & 27.57 &\textbf{28.33}&25.60&25.30&25.62\\
\hline
\end{tabular}

\caption{\label{table:2}Average NIQE \cite{mittal2013making}, PI \cite{blau20182018} and PSNR values on the Set14 \cite{zeyde2010single}, BSD100 \cite{martin2001database} and PIRM-Test \cite{blau20182018}.}

\end{center}

\end{table*}



\begin{figure*}[h]
\setlength{\abovecaptionskip}{-0.2cm}
\setlength{\belowcaptionskip}{-0.1cm}
\begin{center}
\includegraphics[width=1\linewidth]{VScr.pdf} 
\end{center}
   \caption{Visual results comparison of our model with other works on  super-resolution. Lower NIQE value indicates better perceptual quality, while higher PSNR indicates less distortion.}
\label{fig:Visual results}
\vskip -0.5cm
\end{figure*}

\subsection{Training details of RankSRGAN}
\label{4.2}
We use the DIV2K \cite{agustsson2017ntire} dataset to train RankSRGAN. The patch sizes of HR and LR are set to 296 and 74, respectively. For testing, we use benchmark datasets Set14 \cite{zeyde2010single}, BSD100 \cite{martin2001database} and PIRM-test \cite{blau20182018}. PIRM-test is used to measure the
perceptual quality of SR methods in PIRM2018-SR \cite{blau20182018}. Following the settings of SRGAN \cite{ledig2017photo}, we employ a standard SRGAN \cite{ledig2017photo} as our base model. The generator is built with 16 residual blocks, and the batch-normalization layers are removed \cite{Wang_2018_ECCV_Workshops}. The discriminator utilizes the VGG network \cite{simonyan2014very} with ten convolutional layers. The mini-batch size is set to 8. At each training step, the combination of loss functions (Section \ref{3.3}) for the generator is:
\vskip -0.2cm

where the weights of  and  are determined empirically to obtain high perceptual improvement\cite{choi2018deep,ledig2017photo,Wang_2018_ECCV_Workshops}. The Adam\cite{kingma2014adam} optimization method with  is used for training. For generator and discriminator, the initial learning rate is set to  which is reduced by a half for multi-step . A total of  iterations are executed by PyTorch. 
In training procedure, we add Ranker to the standard SRGAN. The Ranker takes some time to predict the ranking score, thus the traning time is a little slower (about 1.18 times) than standard SRGAN\cite{ledig2017photo}. For the generator, the number of parameters remains the same as SRGAN\cite{ledig2017photo}.

\subsection{Comparison with the-state-of-the-arts}
\label{section4.3}
We compare the performance of the proposed method with the state-of-the-art perceptual SR methods ESRGAN \cite{Wang_2018_ECCV_Workshops}/ SRGAN \cite{ledig2017photo} and the PSNR-orientated methods FSRCNN \cite{dong2016accelerating} and SRResNet \cite{ledig2017photo} \footnote{Our implementation of SRResNet and SRGAN achieve even better performance than that reported in the original paper.}. The evaluation metrics include NIQE \cite{mittal2013making}, PI \cite{blau20182018} and PSNR. Table \ref{table:2} shows their performance on three test datasets -- Set14, BSD100 and PIRM-Test. Note that lower NIQE/PI indicates better visual quality. When comparing our method with SRGAN and ESRGAN, we find that RankSRGAN achieves the best NIQE and PI performance on all test sets. Furthermore, the improvement of perceptual scores does not come at the price of PSNR. Note that in PIRM-Test, RankSRGAN also obtains the highest PSNR values among perceptual SR methods. Figure \ref{fig:Visual results} shows some visual examples, where we observe that our method could generate more realistic textures without introducing additional artifacts (please see the windows in Img 233 and feathers in Img 242). 

As the results may vary across different iterations, we further show the convergence curves of RankSRGAN in Figure \ref{fig:curve}. Their performance on NIQE and PSNR are relatively stable during the training process. For PSNR, they obtain comparable results. But for NIQE, RankSRGAN is consistently better than SRGAN by a large margin. 

\begin{figure}[t]
\setlength{\abovecaptionskip}{-0.4cm}
\setlength{\belowcaptionskip}{-0.5cm}
\begin{center}
	\includegraphics[width=1\linewidth]{NIQE_PSNR_distribution} 

\end{center}
   \caption{Convergence curves of RankSRGAN in PSNR/ NIQE.}
\label{fig:curve}
\end{figure}

\subsection{Ablation study}
\label{section4.5}
\textbf{Effect of different rank datasets.} The key factor that influences the performance of Ranker is the choice of SR algorithms. In the main experiments, we use (SRResNet, SRGAN, ESRGAN) to generate the rank dataset. Then what if we select other SR algorithms? Will we always obtain better results than SRGAN? To answer the question, we first analyze the reason of using these three algorithms, then conduct another experiment using a different combination. 

As our baseline model is SRGAN, we need the Ranker to have the ability to rank the outputs of SRGAN. Since the training of SRGAN starts from the pre-trained model SRResNet, the Ranker should recognize the results between SRResNet and SRGAN. That is the reason why we choose SRResNet and SRGAN. Then the next step is to find a better algorithm that could guide the model to achieve better results. We choose ESRGAN as it surpasses SRGAN by a large margin in the PIRM-SR2018 challenge \cite{blau20182018}. Therefore, we believe that a better algorithm than SRGAN could always lead to better performance. 
\vskip -0.1cm

\begin{table}[h]
\renewcommand\tabcolsep{2.0pt}
\small
\setlength{\abovecaptionskip}{-0.2cm}
\setlength{\belowcaptionskip}{-0.3cm}
\begin{center}
\begin{tabular}{l|ccc}
\hline\hline
Method &SRGAN&RankSRGAN&RankSRGAN-HR \\
\hline
NIQE&2.70&\textbf{2.51}&2.58\\
PSNR&25.62&25.60&\textbf{26.00}\\
\hline
\end{tabular}
\end{center}
\caption{Comparison with RankSRGAN and RankSRGAN-HR.}
\label{table:RankSRGAN-HR}
\end{table}

To validate this comment, we directly use the ground truth HR as the third algorithm, which is the extreme case. We still apply NIQE for evaluation. Interestingly, although HR images have infinite PSNR values, they cannot surpass all the results of SRGAN on NIQE. Similar to ESRGAN, HR and SRGAN have mixed ranking orders. We train our Ranker with (SRResNet, SRGAN, HR) and obtain the new SR model -- RankSRGAN-HR. Table \ref{table:RankSRGAN-HR} compares its results with SRGAN and RankSRGAN. As expected, RankSRGAN-HR achieves better NIQE values than SRGAN. But at the same time, RankSRGAN-HR also improves the PSNR by almost 0.4 dB. It achieves a good balance between the perceptual metric and PSNR. This also indicates that the model could always have an improvement space as long as we have better algorithms for guidance. 



\textbf{Effect of different perceptual metrics.} As we claim that Ranker can guide the SR model to be optimized in the direction of perceptual metrics, we need to verify whether it works for other perceptual metrics. We choose Ma \cite{martin2001database} and PI \cite{blau20182018}, which show high correlation with Mean-Opinion-Score (Ma: 0.61, PI: 0.83) in \cite{blau20182018}. We use Ma and PI as the evaluation metric to generate the rank dataset. All other settings remain the same as RankSRGAN with NIQE. The only difference in these experiments is the ranking labels in the rank dataset. The results are summarized in Table \ref{table:RankSRGAN-metric}, where we observe that the Ranker could help RankSRGAN achieve the best performance in the chosen metric. This shows that our method can generalize well on different perceptual metrics.

\begin{table}[h]
\begin{center}
\small \setlength{\abovecaptionskip}{-0.3cm}
\setlength{\belowcaptionskip}{-0.4cm}
\begin{tabular}{l|cccc}
\hline\hline
Method &NIQE&10-Ma&PI&PSNR \\
\hline
SRGAN&2.71&1.47&2.09&25.62\\
ESRGAN&2.56&1.40&1.98&25.30\\
\hline
RankSRGAN&\textbf{2.51}&1.39&1.95&25.62\\
RankSRGAN&2.65&\textbf{1.38}&2.01&25.21\\
RankSRGAN&2.49&1.39&\textbf{1.94}&25.49\\
\hline
\end{tabular}
\end{center}
\caption{The performance of RankSRGAN with different Rankers. : Ranker with NIQE \cite{mittal2013making}, : Ranker with Ma \cite{ma2017learning} and : Ranker with PI \cite{blau20182018}.}
\label{table:RankSRGAN-metric}
\end{table}
\vskip -0.3cm

\textbf{Effect of Ranker: Rank VS. Regression.} To train our Ranker, we choose to use the ranking orders instead of the real values of the perceptual metric. Actually, we can also let the network directly learn the real values. In \cite{choi2018deep}, Choi et al. use a regression network to predict a subjective score for a given image and define a corresponding subjective score loss. To compare these two strategies, we train a ``regression'' Ranker with MSE loss instead of the margin-ranking loss. The labels in the rank dataset are real values of the perceptual metric. We use NIQE and Ma to generate the labels of rank dataset. All the other settings remain the same as RankSRGAN. 
\begin{table}[h]
\small \setlength{\abovecaptionskip}{-0.2cm} 
\setlength{\belowcaptionskip}{-0.4cm}
\begin{center}
\begin{tabular}{l|c c}
\hline\hline
Metric&Method&\\
\hline
NIQE&regression&0.06\\
NIQE&rank&\textbf{0.11}\\
\hline
Ma&regression&0.09\\
Ma&rank&\textbf{0.15}\\
\hline
\end{tabular}
\end{center}
\caption{The distance between  and  with regression and rank.}
\label{table:Ranker-distance}
\end{table}


Theoretically, the real values of perceptual metrics may distribute unevenly among different algorithms. For example, SRGAN and ESRGAN are very close to each other on NIQE values. This presents a difficulty for the learning of regression. On the contrary, learning ranking orders can simply ignore these variances. In experiments, we first measure the distances between the outputs of SRGAN and ESRGAN with different strategies. Table \ref{table:Ranker-distance} shows the mean absolute distances of these two strategies. Obviously, results with rank have larger distances than results with regression. When applying these Rankers in SR training, the rank strategy achieves better performance than the regression strategy on the selected perceptual metric. Results are shown in Table \ref{table:RankSRGAN-regression}.
\vskip -0.1cm
\begin{table}[h]
\small \setlength{\abovecaptionskip}{-0.1cm}
\setlength{\belowcaptionskip}{-0.3cm}
\begin{center}
\begin{tabular}{l|ccc}
\hline\hline
Method&NIQE&10-Ma&PSNR \\
\hline
SRGAN&2.71&1.47&25.62\\
ESRGAN&2.55&1.40&25.30\\
\hline
RankSRGAN-Re&2.53&1.42&25.58\\
RankSRGAN&\textbf{2.51}&1.39&25.60\\
\hline
RankSRGAN-Re&2.61&1.43&25.23\\
RankSRGAN&2.65&\textbf{1.38}&25.21\\
\hline
\end{tabular}
\end{center}
\caption{The performance of RankSRGAN with different rankers. Re: Ranker with regression, : Ranker with NIQE, : Ranker with Ma.}
\label{table:RankSRGAN-regression}
\end{table}

\textbf{Effect of different losses.} To test the effects of rank-content loss, we expect to add MSE loss to achieve improvement in PSNR. Table \ref{table:different loss} shows the performance of our method trained with the combination of loss functions. 

\begin{table}[h]
\small \setlength{\abovecaptionskip}{-0.1cm} 
\setlength{\belowcaptionskip}{-0.4cm}
\begin{center}
\begin{tabular}{l|c|cc}
\hline\hline
Method & Loss &NIQE&PSNR \\
\hline
SRGAN&&2.71&25.62\\
ESRGAN&&2.55&25.30\\

\hline
RankSRGAN&&\textbf{2.51}&25.62\\
RankSRGAN-M&&2.55&25.87\\
RankSRGAN-M&&2.72&\textbf{26.62}\\

\hline
\end{tabular}
\end{center}
\caption{The performance of RankSRGAN with the combination of loss functions (P: perceptual loss, R: rank-content loss, M: MSE loss).  }
\label{table:different loss}
\end{table}
As expected, increasing the contribution of the MSE loss with the larger  results in higher PSNR values. On the other hand, the NIQE values are increased which is a tradeoff between PSNR and NIQE as mentioned in \cite{blau2018perception}, and our method has a capability to deal with the priorities by adjusting the weights of the loss functions.  


\subsection{User study}
\label{section: user study}

To demonstrate the effectiveness and superiority of RankSRGAN, we conduct a user study against state-of-the-art models, i.e. SRGAN \cite{ledig2017photo} and ESRGAN \cite{Wang_2018_ECCV_Workshops}. In the first session, two different SR images are shown at the same time where one is generated by the proposed RankSRGAN and the other is generated by SRGAN or ESRGAN. The participants are required to pick the image that is more visually pleasant (more natural and realistic). We use the PIRM-Test \cite{blau20182018} dataset as the testing dataset. There are a total of 100 images, from which 30 images are randomly selected for each participant. To make a better comparison, one small patch from the image is zoomed in. 
In the second session, we focus on the perceptual quality of different typical SR methods in a sorting manner. The participants are asked to rank 4 versions of each image: SRResNet \cite{ledig2017photo}, ESRGAN \cite{Wang_2018_ECCV_Workshops}, RankSRGAN, and the Ground Truth (GT) image according to their visual qualities. Similar to the first session, 20 images are randomly shown for each participant. There are totally 30 participants to finish the user study. 
\begin{figure}
\setlength{\abovecaptionskip}{-0.1cm}
\setlength{\belowcaptionskip}{-0.5cm}
\begin{center}
\includegraphics[width=1\linewidth]{US1} 
\end{center}
   \caption{The results of user studies, comparing our method with SRGAN  \cite{ledig2017photo} and ESRGAN \cite{Wang_2018_ECCV_Workshops}.}
\label{fig:user study1}
\end{figure}

\begin{figure}[h]
\setlength{\abovecaptionskip}{-0.1cm}
\setlength{\belowcaptionskip}{-0.3cm}
\begin{center}
\includegraphics[width=1\linewidth]{US2} 
\end{center}
   \caption{The ranking results of user studies: SRResNet \cite{ledig2017photo}, ESRGAN \cite{Wang_2018_ECCV_Workshops}, RankSRGAN (ours), and the original HR image.}
\label{fig:user study2}
\end{figure}


As suggested in Figure \ref{fig:user study1}, RankSRGAN has achieved better visual performance against ESRGAN and SRGAN. Since RankSRGAN consists of a base model SRGAN and the proposed Ranker, it can naturally inherit the characteristics of SRGAN and achieve better performance in perceptual metric. Thus, RankSRGAN performs more similar to SRGAN than ESRGAN. Figure \ref{fig:user study2} shows the ranking results of different SR methods. As RankSRGAN has the best performance in perceptual metric, the ranking results of RankSRGAN are second to GT images, but sometimes it even produces images comparable to GT. 


\section{Conclusion}
For perceptual super-resolution, we propose RankSRGAN to optimize SR model in the orientation of perceptual metrics. The key idea is introducing a Ranker to learn the behavior  of the perceptual metrics by learning to rank approach. Moreover, our proposed method can combine the strengths of different SR methods and generate better results. Extensive experiments well demonstrate that our RankSRGAN is a flexible framework, which can achieve superiority over state-of-the-art methods in perceptual metric and have the ability to recover more realistic textures. 

\textbf{Acknowledgements}. This work is partially supported by National Natural Science Foundation of China (61876176,  U1613211), Shenzhen Basic Research Program (JCYJ20170818164704758), the Joint Lab of CAS-HK.



{\small
\bibliographystyle{ieee_fullname}
\bibliography{egpaper_final}
}
\newpage

\includepdf[pages={1}]{supp.pdf}
\includepdf[pages={2}]{supp.pdf}
\includepdf[pages={3}]{supp.pdf}
\includepdf[pages={4}]{supp.pdf}
\includepdf[pages={5}]{supp.pdf}
\includepdf[pages={6}]{supp.pdf}
\includepdf[pages={7}]{supp.pdf}
\includepdf[pages={8}]{supp.pdf}
\includepdf[pages={9}]{supp.pdf}

\end{document}
