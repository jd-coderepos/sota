\documentclass[11pt]{article}
\usepackage{algorithm2e,framed}
\usepackage{amssymb,amsfonts,amsmath}
\usepackage{algorithm2e,framed}
\usepackage{amsfonts,amsmath}
\usepackage{graphicx}

\addtolength{\textwidth}{1.4in}
\addtolength{\oddsidemargin}{-0.5in}
\addtolength{\evensidemargin}{-0.5in}
\addtolength{\topmargin}{-0.5in}
\addtolength{\textheight}{1.7in}



\newcommand{\Probab}[1]{\mbox{}{\bf{Pr}}\left[#1\right]}
\newcommand{\Expect}[1]{\mbox{}{\bf{E}}\left[#1\right]}
\newcommand{\Varnce}[1]{\mbox{}{\bf{Var}}\left[#1\right]}
\newcommand{\Trace }[1]{\mbox{}{\bf{Tr}}\left(#1\right)}
\newcommand{\Sqrt  }[1]{\mbox{}\left(#1\right)^{1/2}}
\newcommand{\Qdrt  }[1]{\mbox{}\left(#1\right)^{1/4}}
\newcommand{\FNorm }[1]{\mbox{}\left\|#1\right\|_F  }
\newcommand{\XiNorm }[1]{\mbox{}\left\|#1\right\|_{\xi}  }
\newcommand{\FNormS}[1]{\mbox{}\left\|#1\right\|_F^2}
\newcommand{\FNormQ}[1]{\mbox{}\left\|#1\right\|_F^4}
\newcommand{\TNorm }[1]{\mbox{}\left\|#1\right\|_2  }
\newcommand{\TNormS}[1]{\mbox{}\left\|#1\right\|_2^2}
\newcommand{\TNormQ}[1]{\mbox{}\left\|#1\right\|_2^4}
\newcommand{\XNorm }[1]{\mbox{}\left\|#1\right\|_{\xi}  }
\newcommand{\XNormS}[1]{\mbox{}\left\|#1\right\|_{\xi}^2}
\newcommand{\XNormQ}[1]{\mbox{}\left\|#1\right\|_{\xi}^4}
\newcommand{\VTNorm }[1]{\mbox{}\left|#1\right|  }
\newcommand{\VTNormS}[1]{\mbox{}\left|#1\right|^2}
\newcommand{\VTNormQ}[1]{\mbox{}\left|#1\right|^4}
\newcommand{\VTTNorm }[1]{\mbox{}\left|#1\right|_2  }
\newcommand{\VTTNormS}[1]{\mbox{}\left|#1\right|_2^2}
\newcommand{\VTTNormQ}[1]{\mbox{}\left|#1\right|_2^4}

\newcommand{\setlinespacing}[1]{\setlength{\baselineskip}{#1 \defbaselineskip}}
\newcommand{\doublespacing}{\setlength{\baselineskip}{2.0 \defbaselineskip}}
\newcommand{\singlespacing}{\setlength{\baselineskip}{\defbaselineskip}}

\newcommand{\rank}[1]{{\bf rank}{\left(#1\right)}}
\newcommand{\abs }[1]{\left|#1\right|}
\newcommand{\absS}[1]{\left|#1\right|^{2}}
\newcommand{\spann}[1]{\mbox{\bf span}\left(#1\right)}
\newcommand{\mcut}[1]{\mbox{\bf MAX-CUT}\left[#1\right]}



\newcommand{\BlockDiagk}[1]{\mbox{}\left(\begin{array}{cc}
  \Sigma_{k} & \bf{0} \\
  \bf{0} &  \Sigma_{\rho-k}\\
\end{array}\right)}

\newcommand{\BlockDiagkk}[1]{\mbox{}\left(\begin{array}{cc}
  \Sigma_{k} & \bf{0} \\
  \bf{0} & \bf{0} \\
\end{array}\right)}

\newcommand{\BlockDiagkrk}[1]{\mbox{}\left(\begin{array}{cc}
  \bf{0} & \bf{0} \\
  \bf{0} & \Sigma_{\rho-k} \\
\end{array}\right)}

\newcommand{\BlockDiagkkh}[1]{\mbox{}\left(\begin{array}{c}
  \Sigma_{k} \\
  \bf{0} \\
\end{array}\right)}

\newcommand{\BlockDiagkrkh}[1]{\mbox{}\left(\begin{array}{c}
  \bf{0} \\
  \Sigma_{\rho-k} \\
\end{array}\right)}



\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newenvironment{Proof}{\noindent {\em Proof:}}{\\\hspace*{\fill}\mbox{}}



\long\def\killtext#1{}

\def\Prob{\hbox{\bf{Pr}}}
\def\E{\hbox{\bf{E}}}
\def\Exp{\hbox{\bf{E}}}
\def\Pick{\hbox{pick}}
\def\var{\hbox{\bf{Var}}}
\def\bG{\hbox{\bf{G}}}
\def\tr{\hbox{\bf{Tr}}}
\def\Tr{\hbox{\bf{Tr}}}
\def\U{\tilde{U}}
\def\Q{\tilde{Q}}
\def\R{\tilde{R}}
\def\n{n}
\def\l{\ell}
\def\sn{n_\lambda}

\def\IP{\hbox{\bf{IP}}}
\def\LP{\hbox{\bf{LP}}}
\def\diag{\hbox{\bf{diag}}}

\def\Vol{\hbox{\bf{Vol}}}
\def\Sketch{\hbox{{\bf Sketch}}}



 
\title{\textbf{Random Projections for the Nonnegative Least-Squares Problem}}

\author{
\textbf{Christos }\textbf{Boutsidis}\\
Computer Science Department\\
Rensselaer Polytechnic Institute\\
Troy, NY 12180 \\
\texttt{boutsc@cs.rpi.edu} \\
\and
\textbf{Petros Drineas}\\
Computer Science Department\\
Rensselaer Polytechnic Institute\\
Troy, NY 12180 \\
\texttt{drinep@cs.rpi.edu} \\
}

\begin{document}

\maketitle

\begin{abstract}
Constrained least-squares regression problems, such as the
Nonnegative Least Squares (NNLS) problem, where the variables are
restricted to take only nonnegative values, often arise in
applications. Motivated by the recent development of the fast
Johnson-Lindestrauss transform, we present a fast random
projection type approximation algorithm for the NNLS problem. Our
algorithm employs a randomized Hadamard transform to construct a
much smaller NNLS problem and solves this smaller problem using a
standard NNLS solver. We prove that our approach finds a
nonnegative solution vector that, with high probability, is close
to the optimum nonnegative solution in a relative error
approximation sense. We experimentally evaluate our approach on a
large collection of term-document data and verify that it does
offer considerable speedups without a significant loss in
accuracy. Our analysis is based on a novel random projection type
result that might be of independent interest. In particular, given
a tall and thin matrix  ()  and a vector , we prove that the
Euclidean length of  can be estimated very accurately by
the Euclidean length of , where 
consists of a small subset of (appropriately rescaled) rows of
.
\end{abstract}

\section{Introduction}
The Nonnegative Least Squares (NNLS) problem is a constrained
least-squares regression problem where the variables are allowed
to take only nonnegative values. More specifically, the NNLS
problem is defined as follows:
\begin{definition} \label{def:NNLS}
\textbf{\textsc{[Nonnegative Least Squares~(NNLS)]}}\\
Given a matrix  and a target vector
, find a nonnegative vector  such that

\end{definition}
NNLS is a quadratic optimization problem with linear inequality
constraints. As such, it is a convex optimization problem and thus
it is solvable (up to arbitrary accuracy) in polynomial time
\cite{Bjo96}. In words, NNLS seeks to find the best nonnegative
vector  in order to approximately express  as a
strictly nonnegative linear combination of the columns of ,
i.e., .

The motivation for NNLS problems in data mining and machine
learning stems from the fact that given least-squares regression
problems on nonnegative data such as images, text, etc., it is
natural to seek \emph{nonnegative} solution vectors. (Examples of
data applications are described in \cite{CP07}.) NNLS is also
useful in the computation of the Nonnegative Matrix
Factorization~\cite{KP07a}, which has received considerable
attention in the past few years. Finally, NNLS is the core
optimization problem and the computational bottleneck in designing
a class of Support Vector Machines~\cite{SSL02}. Since modern
datasets are often massive, there is continuous need for faster,
more efficient algorithms for NNLS.

In this paper we discuss the applicability of random projection
algorithms for solving constrained regression problems, and in
particular NNLS problems. Our goal is to provide fast
approximation algorithms as alternatives to the existing exact,
albeit expensive, NNLS methods. We focus on input matrices 
that are tall and thin, i.e., , and we present, analyze,
and experimentally evaluate a random projection type algorithm for
the nonnegative least-squares problem. Our algorithm utilizes a
novel random projection type result which might be of independent
interest. We argue that the proposed algorithm (described in
detail in Section \ref{sec:algo}), provides relative error
approximation guarantees for the NNLS problem. Our work is
motivated by recent progress in the design of fast randomized
approximation algorithms for unconstrained  regression
problems~\cite{DMMS07,DDHKM08}.

The following theorem is the main
quality-of-approximation result for our randomized NNLS algorithm.
\begin{theorem}
\label{thm:main_result} Let . Let  and  be the inputs
of the NNLS problem with . If the input parameter  of
the \textsc{RandomizedNNLS} algorithm of Section~\ref{sec:algo}
satisfies

(for a sufficiently large constant )\footnote{ is an
unspecified constant in \cite{RV06}.} then the
\textsc{RandomizedNNLS} algorithm returns a nonnegative vector
 such that

holds with probability at least \footnote{Note that a small
number of repetitions of the algorithm suffices to boost its
success probability.}. The running time of the
\textsc{RandomizedNNLS} algorithm is

The latter term corresponds to the time required to exactly solve
an NNLS problem on an input matrix of dimensions .
\end{theorem}
One should compare the running time of our method to
, which corresponds to the time
required to solve the NNLS problem exactly. We experimentally
evaluate our approach on 3,000 NNLS problems constructed from a
large and sparse term-document data collection. On average (see
section \ref{sec:exppart1}), the proposed algorithm achieves a
three-fold speedup when compared to a state-of-the-art NNLS
solver~\cite{KSD07} with a small (approx. ) loss in
accuracy; a two-fold speedup is achieved with a  loss in
accuracy. Computational savings are more pronounced for NNLS
problems with denser input matrices  and vectors  (see
section \ref{sec:sparsedense}).

The remainder of the paper is organized as follows. Section
\ref{sec:priorwork} reviews basic linear algebraic definitions and
discusses related work. In Section \ref{sec:algo} we present our
randomized algorithm for approximating the NNLS problem, discuss
its running time, and give the proof of Theorem
\ref{thm:main_result}. Finally, in section \ref{sec:exp} we
provide an experimental evaluation of our method.

\section{Background and related work} \label{sec:priorwork}
Let  denote the set . For any matrix  with  let 
denote the -th row of  as a row vector, and let  denote the -th column of  as a column vector. The
Singular Value Decomposition (SVD) of  can be written as

Assuming that  has full rank, 
and   are orthonormal matrices,
while  is a  diagonal matrix. Finally,  denotes the
square of the Frobenius norm of  and  denotes the spectral
norm of .

The (non-normalized)  matrix of the Hadamard-Walsh
transform  is defined recursively as follows:

The  normalized matrix of the Hadamard-Walsh transform
is equal to ; hereafter, we will denote
this normalized matrix by  ( is a power of ). For
simplicity, throughout this paper we will assume that  is a
power of two; padding  and  with all-zero rows suffices to
remove the assumption. Finally, all logarithms are base two.

\subsection{Random projection algorithms for unconstrained  problems}
The unconstrained least-squares regression problem ()
takes as input a matrix  and a
vector , and returns as output a vector
 that minimizes the distance
. Assuming , various algorithms solve the
problem exactly in  time~\cite{GV89}. Drineas et
al.~\cite{DMM06, DMMS07} and Sarlos~\cite{Sar06} give randomized
algorithms to approximate the solution to such problems. The basic
idea of these algorithms is to select a subset of rows from 
and a subset of elements from , and solve the induced problem
exactly. The fundamental algorithmic challenge is how to form the
induced problem. It turns out that sampling rows of  and
elements of  with probabilities that are proportional to the
 norms of the rows of the matrix of the left singular
vectors of  suffices \cite{DMM06}. This approach is not
computationally efficient, since computing these probabilities
takes  time. However, by leveraging the Fast
Johnson-Lindenstrauss Transform of~\cite{AC06}, one can design an
 algorithm for this problem~\cite{DMMS07}. The algorithm
of this paper applies the same preconditioning step as the main
Algorithm of \cite{DMMS07}. The analysis though is very different
from the analysis of \cite{DMMS07} and is based on a novel random
projection type result that is presented in section
\ref{sec:algo}. The difficulty of applying the analysis of
\cite{DMMS07} here is the fact that the solution of an NNLS
problem cannot be written in a closed form. Finally, it should be
noted that similar ideas are discussed in \cite{DDHKM08}, where
the authors present sampling-based approximation algorithms for
the  regression problem for . The
preconditioning step of the algorithm of this paper is different
from the preconditioning step of the algorithm of \cite{DDHKM08}.

\subsection{Algorithms for the NNLS problem}
We briefly review NNLS algorithms following the extensive review
in~\cite{CP07}. Recall that the NNLS Problem is a quadratic
optimization problem. Hence, all quadratic programming algorithms
may be used to solve it. Methods for solving NNLS problems can be
divided into three general categories: () active set methods,
() iterative methods, and () other methods. The approach
of Lawson and Hanson in \cite{LH74} seems to be the first
technique to solve NNLS problems. It is a typical example of an
active set method and is implemented as the function
\emph{lsqnonneg} in Matlab. Immediate followups to this work
include the technique of Bro and Jong~\cite{BJ97} which is
suitable for problems with multiple right hand sides, as well as
the combinatorial NNLS approach of Dax~\cite{Dax91}. The
Projective Quasi-Newton NNLS algorithm of \cite{KSD07} is an
example from the second category. It is an iterative approach
based on the Newton iteration and the efficient approximation of
the Hessian matrix. Numerical experiments in \cite{KSD07} indicate
that it is a very fast alternative to the aforementioned active
set methods. The sequential coordinate-wise approach
of~\cite{FHN05} is another example of an iterative NNLS method.
Finally, interior point methods are suitable for NNLS
computations~\cite{PJV94}. A different approach appeared in
\cite{SSL02}. It starts with a random nonnegative vector  and updates it via elementwise multiplicative rules.
Surveys on NNLS algorithms include~\cite{Bjo96, LH74, KSD07}.

\section{A Random Projection Type Algorithm for the NNLS problem}
\label{sec:algo}

This section describes our main algorithm for the NNLS problem.
Our algorithm employs a randomized Hadamard transform to construct
a much smaller NNLS problem and solves this smaller problem
exactly using a standard NNLS solver. The approximation accuracy
of our algorithm is a function of the size of the small NNLS
problem.

\subsection{The \textsc{RandomizedNNLS} Algorithm}
\label{sec:RNNLS}

Algorithm \textsc{RandomizedNNLS} takes as inputs an 
matrix  (), an -dimensional target vector , and
a positive integer . It outputs a nonnegative
-dimensional vector  that approximately solves
the original NNLS problem. Our algorithm starts by premultiplying
the matrix  and the right hand side vector  with a random  diagonal matrix , whose diagonal entries are set to
 or  with equal probability. It then multiplies the
resulting matrix  and the vector  with a small submatrix
of the  normalized Hadamard-Walsh matrix  (see
section \ref{sec:priorwork}). This submatrix of  -- denoted
by  -- is constructed as follows: for all ,
the -th row of  is included in  with
probability . Clearly, the expected number of rows of the
matrix  is equal to . Finally, our algorithm returns
the nonnegative vector  that satisfies

In section \ref{sec:optr} we will argue that, for any , if we set

then  is at most 
worse than the true optimum . This is a
sufficient (but not necessary) condition for  in order to
satisfy the relative error guarantees of
equation~(\ref{eqn:result1_intro}). Indeed, in the experiments of
section \ref{sec:exp} we will argue that empirically a much
smaller value of , for example , suffices.
\begin{algorithm}[h]
\begin{framed}

\textbf{Inputs:} , , positive integer .

\vspace{0.1in}

\textbf{Output:} a nonnegative vector .

\begin{enumerate}

\item Let  be the  normalized Hadamard-Walsh matrix.

\item Let  be an  diagonal matrix such that for all ,
    


\item Let  be the matrix consisting of the non-zero rows of . \\
(Notice that  has -- in expectation --  rows.)

\item Construct the  diagonal matrix  such that, for all , \\  with probability ; otherwise .

\item Solve

using any standard NNLS solver and return the vector
.
\end{enumerate}

\end{framed}
\caption{The \textsc{RandomizedNNLS} algorithm.}
\label{alg:alg_sample_fast}
\end{algorithm}


\subsection{The proof of Theorem \ref{thm:main_result}}

\subsubsection{A random projection type result}
\label{sxn:subspacesampling}

In this section we prove a random projection type result based on the
so-called subspace sampling procedure~\cite{DMM07} that might be
of independent interest. In particular, given a matrix  with  (a.k.a.,  is tall
and thin), and \textit{any} vector , we argue
that the  norm of  can be estimated very
accurately by , where  consists of
small subset of (appropriately rescaled) rows of .

More specifically, consider the \textsc{SubspaceSampling}
algorithm described below. This algorithm selects a small subset
of rows of  to construct ; notice that
 has -- in expectation -- at most  rows. Also
notice that  contains the -th row of 
(appropriately rescaled) if and only if  is non-zero.
Lemma \ref{lemma:subspace-sampling} bounds the approximation error
for our subspace sampling algorithm.

\begin{algorithm}[h]
\begin{framed}

\textbf{Input:} , integer , set of probabilities ,  s.t. .

\textbf{Output:} , with .

\vspace{0.1in}

\begin{enumerate}
\item Let  be the  diagonal matrix such that for all ,
    
\item Let  be the matrix consisting of the non-zero rows of
    . \\ (Notice that  has - in expectation -  rows.)
\end{enumerate}
\end{framed}
\caption{\textsc{SubspaceSampling} algorithm} \label{alg:alg2}
\end{algorithm}


\begin{lemma}
\label{lemma:subspace-sampling} Let . Let
 be an  matrix (),  be the  matrix containing the left singular vectors of ,
and  denote the -th row of . Let
 be constructed using the \textsc{SubspaceSampling}
algorithm with inputs , , and sampling probabilities
. If for all ,

for some , and the parameter  satisfies

for a sufficiently large constant , then with probability at
least  all -dimensional vectors  satisfy,

\end{lemma}
\begin{Proof}
Let  be the SVD of
 with , , and . Let  be the  diagonal matrix
constructed at the first step of algorithm
\textsc{SubspaceSampling}, and let , where  is the  identity matrix, and  some  matrix. Then, using these two definitions,
submultiplicativity, and the orthogonality and unitary invariance
of  and ,

Using Theorem 7 of \cite{DMM07} (originally proven by Rudelson and
Virshynin in~\cite{RV06}), we see that

for a sufficiently large constant  ( is not specified
in~\cite{RV06}). Markov's inequality implies that

with probability at least 2/3. Finally, using
equation~(\ref{eqn:r}) concludes the proof of the lemma.
\end{Proof}
\clearpage
\subsubsection{Another useful result}
Results in~\cite{AC06} imply the following lemma.
\begin{lemma}
\label{lem:HU} Let  be an  orthogonal matrix ( and ). Then, for all ,

holds with probability at least 0.9.
\end{lemma}


\subsubsection{The proof of Theorem \ref{thm:main_result}}
We are now ready to prove Theorem \ref{thm:main_result}. We apply
lemma \ref{lemma:subspace-sampling} for
, the parameter  of Theorem
\ref{thm:main_result}, sampling probabilities , for all
, , and , where  is the
parameter of Theorem \ref{thm:main_result}. Let  be the
 matrix of the left singular vectors of .
Note that  is exactly equal to , where  is the
 matrix of the left singular vectors of . Lemma \ref{lem:HU} for  and our choice of , guarantee that
for all , with probability at least 

The latter inequality implies that assumption~(\ref{eqn:p}) of
Lemma \ref{lemma:subspace-sampling} holds. Also, our choice of
, which satisfies inequality~(\ref{eqn:result0_intro}) in
Theorem \ref{thm:main_result}, our choice of , and our
choice of , guarantee that assumption~(\ref{eqn:r}) of
Lemma \ref{lemma:subspace-sampling}
is also true. Since all -dimensional vectors  satisfy  equation~(\ref{eqn:ally}), we pick  and  thus getting that with probability at least 2/3:

and

Manipulating equations~(\ref{eqn:rel1}) and (\ref{eqn:rel2}) we
get

The second inequality follows since  is the
optimal solution of the NNLS problem of eqn.
(\ref{eqn:tildexopt}), thus  is a sub-optimal solution,
and the fourth inequality follows since
, for all . In the last
inequality, we set . To conclude the
proof, notice that  is an orthonormal square matrix and can
be dropped without changing a unitarilly invariant norm. Finally,
since Lemmas \ref{lemma:subspace-sampling} and \ref{lem:HU} fail
with probability at most  and  respectively, the union
bound implies that Theorem \ref{thm:main_result} fails with
probability at most .



\subsection{What is the minimal value of  ?}
\label{sec:optr} To derive values of  for which the
\textsc{RandomizedNNLS} algorithm satisfies the relative error
guarantees of Theorem \ref{thm:main_result}, we need to solve
equation~(\ref{eqn:result0_intro}); this is hard since the
solution depends on the Lambart  function. Thus, we identify a
range of values of  that are sufficient for our purposes. Using
the fact that for any , and for any ,

and by setting  in
equation~(\ref{eqn:result0_intro}) (note that ), it can be proved that every 
such that

satisfies the inequality \ref{eqn:result0_intro} ( is the
constant of Theorem \ref{thm:main_result}).

\subsection{Running time analysis}
\label{sec:time} In this subsection we analyze the running time of
our algorithm. Let  be the minimal value that satisfies
equation (\ref{eqn:result2_intro}). First, computing  and 
takes  time. Since  has in expectation  rows,
Ailon and Liberty in~\cite{AL08} argue that the computation of
 and  takes  time. For our
choice of , this is
 After
this preconditioning step, we employ an NNLS solver on the smaller
problem. The computational cost of the NNLS solver on the small
problem was denoted as  in Theorem
\ref{thm:main_result}.  cannot be specified exactly
since theoretical running times for exact NNLS solvers are
unknown. In the sequel we comment on the computational costs of
some well defined segments of some NNLS solvers.

The NNLS formulation of Definition \ref{def:NNLS} is a convex
quadratic program, and is equivalent to

where  and . Computing  and  takes  time, and
then the time required to solve the above formulation of the NNLS
problem is independent of . Using this formulation, our
algorithm would necessitate  time for the computation
of  (the preconditioning step described above), and
then  and
 can be computed in
 time; given our choice of , this implies
 Overall, the standard
approach would take  time to compute , whereas our
method would need only  +  time for the
construction of . Note, for example, that when
 and regarding  as a constant,  can
be computed  times faster than .

On the other hand, many standard implementations of NNLS solvers
(and in particular those that are based on active set methods)
work directly on the formulation of Definition \ref{def:NNLS}. A
typical cost of these implementations is of the order 
per iteration. Other approaches, for example the NNLS method of
\cite{KSD07}, proceed by computing matrix-vector products of the
form , for an appropriate -dimensional vector , thus
cost typically  time per iteration. In these cases our
algorithm needs again  preprocessing time, but costs
only  or  time per iteration, respectively. Again,
if given our choice of , the computational savings \emph{per
iteration} are comparable with the  speedup
described above.

\section{Experimental Evaluation}
\label{sec:exp}

In this section, we experimentally evaluate our
\textsc{RandomizedNNLS} algorithm on \textbf{(i)} large, sparse
matrices from a text-mining application, and \textbf{(ii)}
random matrices with varying sparsity. We mainly focus on
employing the state-of-the-art solver of \cite{KSD07} to solve the small NNLS problem.

\subsection{The TechTC300 dataset}
\label{sec:exppart1}

Our data come from the Open Directory Project (ODP)~\cite{odp}, a
multilingual open content directory of WWW links that is
constructed and maintained by a community of volunteer editors.
ODP uses a hierarchical ontology scheme for organizing site
listings. Listings on similar topics are grouped into categories,
which can then include smaller subcategories. Gabrilovich and
Markovitch constructed a benchmark set of 300 term-document
matrices from ODP, called TechTC300 (Technion Repository of Text
Categorization Datasets~\cite{GM04}), which they made publicly
available. Each term-document matrix of the TechTC300 dataset
consists of a total of 150 to 400 documents from two different ODP
categories, and a total of 15,000 to 35,000 terms. We chose this
dataset because we believe that it does represent an important
application area, namely text mining, and we do believe that the
results from our experiments will be representative of the
potential usefulness of our randomized NNLS algorithm in large,
sparse, term-document NNLS problems.

We present average results from 3,000 NNLS problems. More
specifically, for each of the 300 matrices of the TechTC300
dataset, we randomly choose a column from the term-document matrix
as the vector , we assign the remaining columns of the same
term-document matrix to the matrix , and solve the resulting
NNLS problem with inputs  and . We repeat this process ten
times for each term-document matrix of the TechTC300 dataset, and
thus solve a total of 3,000 problems. Whenever an NNLS routine is
called, it is initialized with the all-zeros vector. We evaluate
the accuracy and the running time of our algorithm when compared
to two standard NNLS algorithms. The first one is described
in~\cite{KSD07}\footnote{We would like to thank the authors
of~\cite{KSD07} for providing us with a Matlab
implementation of their algorithm.}, and the second
one is the active set method of \cite{LH74}, implemented as the
built-in function \emph{lsqnonneg} in Matlab. We would also like
to emphasize that in~\cite{KSD07} the authors compare their
approach to other NNLS approaches and conclude that their
algorithm is significantly faster. Note that the method of
\cite{KSD07} operates on the quadratic programming formulation
discussed in Section \ref{sec:RNNLS}\footnote{The actual
implementation involves computations of the form  and
, avoiding the computation and storage of the matrix
.}, while \emph{lsqnonneg} operates on the formulation of
Definition \ref{def:NNLS}. Finally, we implemented our
\textsc{RandomizedNNLS} algorithm in Matlab. The platform used for
the experiments was a 2.0 GHz Pentium IV with 1GB RAM.

\begin{figure}
\label{fig:results}
    \begin{center}
    \includegraphics[scale=0.52,angle=0]{kimerror}
    \includegraphics[scale=0.52,angle=0]{kimtime}
    \end{center}
    \caption{
    Average results of the \textsc{RandomizedNNLS}
    algorithm compared to the algorithm of \cite{KSD07} on 3000 NNLS
    problems constructed from the TechTC300 dataset.
    \emph{Relative Error}, while \emph{Overall Time}.  is computed with the method of \cite{KSD07} in
     time, and   with the \textsc{RandomizedNNLS} algorithm (the last step employs the method of \cite{KSD07}) in
     time. Points one through eight on the -axis of the plots
    correspond to values of the parameter  for , where  is the number of columns of .
    On the right panel, \emph{Preprocessing
    time} stands for the cost of the multiplication of  and  with  (multiplied by  and divided by ), and \emph{Small-problem time} stands
    for the cost of solving the small problem using the algorithm
    of \cite{KSD07} (multiplied by  and divided by ). For each point of the -axis: \emph{Overall time} = \emph{Preprocessing
    time} +  \emph{Small-problem time}.}\label{fig:fig1}
\end{figure}

Our (average) results are shown in Figure \ref{fig:fig1}. We only
focus on the algorithm of~\cite{KSD07}, which was significantly
faster, running (on average) in five seconds, compared to more
than one minute for the \emph{lsqnonneg} function. We experimented
with eight different values of the parameter , which dictates
the size of the small subproblem (see the \textsc{RandomizedNNLS}
algorithm). More specifically, we set  to , for
, where  is the number of columns in the matrix
. Our results verify that () the \textsc{RandomizedNNLS}
algorithm is very accurate, () that it reduces the running
time of the NNLS method of~\cite{KSD07}, and  that there
exists a natural tradeoff between the approximation accuracy and
the number of sampled rows. Notice, for example, that the running
time of the state-of-the-art NNLS solver of~\cite{KSD07} can be
reduced from two to three times, while the residual error is from
 up to  worse than the optimal residual error.

We briefly comment on the performance of \textsc{RandomizedNNLS}
when compared to the \emph{lsqnonneg} algorithm. As expected, the
accuracy results are essentially identical with the method
of~\cite{KSD07}, since both methods solve the NNLS problem
exactly. Our speedup, however, was much more significant, ranging
from 14-fold to 10-fold for  and 
respectively (data not shown).

\subsection{Sparse vs dense NNLS problems}
\label{sec:sparsedense}

The astute reader might notice that we evaluated the performance of our
algorithm in a rather adversarial setting. The TechTC300 data are
quite sparse, hence existing NNLS methods would operate on sparse
matrices. However, our preprocessing step in the
\textsc{RandomizedNNLS} algorithm destroys the sparsity, and the
induced subproblem becomes dense. Thus, we are essentially
comparing the time required to solve a sparse, large NNLS problem
to the time required to solve a dense, small NNLS problem. If the
original problem were dense as well, we would expect more
pronounced computational savings. In this section we experiment
with random matrices of varying density in order to confirm this hypothesis.

First, it is worth noting that the sparsity of the input matrix
 and/or the target vector  do not seem to affect the
approximation accuracy of the \textsc{RandomizedNNLS} algorithm.
This should not come as a surprise since our results in Theorem
\ref{thm:main_result} do not make any assumptions on the inputs
 and . Indeed, our experiments in Figure 2 confirm our
expectations.

Prior to discussing our experiments on random matrices of varying
density, it is worth noting that the NNLS solver of~\cite{KSD07}
has a running time that is a function of the number of non-zero
entries of . Indeed, the method of~\cite{KSD07} is an iterative
method where the computational bottleneck in the -th iteration
involves computations of the form , for a -dimensional
vector . \cite{KSD07} implemented their algorithm by computing
the two matrix-vector products  and 
separately, thus never forming the matrix  and thus taking
advantage of the sparsity of . Indeed, NNLS problems with
sparse coefficient matrices  are solved faster than NNLS
problems with similar-size dense coefficient matrices  by using
the method of \cite{KSD07}\footnote{The authors of \cite{KSD07}
performed extensive numerical experiments to verify that
observation; for example see the last row of Table 4 on page 14 in
\cite{KSD07} and notice that the running time of their method
increases as the density of  increases.}.

In order to measure how the speedup of our \textsc{RandomizedNNLS}
algorithm improves as the matrix  and vector  become denser,
we designed the following experiment. First, let the
\textit{density} of an NNLS problem denote the percentage of
non-zero entries in  and ; for example,  means that approximately  entries in the  matrix  and the  vector  are zero. We
chose six density parameters (, , , , ,
and ) and generated  NNLS problems for each density
parameter. More specifically, we first constructed ten  random matrices with the target density (the non-zero
entries are normally distributed in ). Then, for each
matrix, we randomly selected one column to form the vector  and
assigned the remaining  columns to the matrix . We repeated
this selection process ten times for each of the ten  matrices, thus forming a set of  NNLS problems with
inputs  and . We fixed the dimensions to  and
 and we experimented with four values of . In Figure 2 we present average results over the
 NNLS problems for each choice of the density parameter.
Notice that increasing the density of the inputs  and , the
computational gains increase as well. On top of that, our method
becomes more accurate while the number of the zero entries in 
and  become fewer. Notice for example, on the right plot of
Figure 2, when , the two extreme cases ( and ) correspond to an  and a  loss in
accuracy, respectively. Given these two observations as well as
the actual times of  (see the caption of Figure 2),
we conclude that the random projection ideas empirically seem more
promising for dense rather than sparse NNLS problems.


\begin{figure}
\label{fig:results2}
    \begin{center}
    \includegraphics[scale=0.52,angle=0]{sp1}
    \includegraphics[scale=0.52,angle=0]{sp2}
    \end{center}
    \caption{
    Average results of the \textsc{RandomizedNNLS}
    algorithm compared to the algorithm of \cite{KSD07} on six sets of  NNLS
    problems with different density.
    \emph{Relative Error}, while \emph{Overall Time}.  is computed with the method of \cite{KSD07} in
     time, and   with the \textsc{RandomizedNNLS} algorithm (the last step employs the method of \cite{KSD07}) in
     time. For the six density parameters ,  was on average (0.26 sec, 0.40 sec, 0.94 sec,
    2.89 sec, 6.27 sec, 10.57 sec), respectively.    }
\end{figure}





\section{Conclusions}

We presented a random projection algorithm for the Nonnegative
Least Squares Problem. We experimentally evaluated our algorithm
on a large, text-mining dataset, and verified that, as promised in
our theoretical findings, practically it does give very accurate
approximate solutions, while outperforming two standard NNLS
methods in terms of computational efficiency. Future work includes
the extension of our theoretical findings of Theorem 1 to NNLS
problems with multiple right hand side vectors. An immediate
application of this would be the computation of Nonnegative Matrix
Factorizations based on Alternating Least Squares type
approaches~\cite{KP07a, KP08}. Finally, notice that, since our
analysis is independent of the type of constraints on the vector
, our main algorithm can be employed to approximate a
least-squares problem with any type of constraints on .

\paragraph{Acknowledgements:} We would like to thank Kristin P. Bennett
and Michael W. Mahoney for useful discussions. The first author
would like also thank the Institute of Pure and Applied
Mathematics of the University of California at Los Angeles for its
generous hospitality during the period Sept. 2008 - Dec. 2008,
when part of this work was done.

\bibliographystyle{abbrv}
\bibliography{RPI_BIB}

\end{document}
