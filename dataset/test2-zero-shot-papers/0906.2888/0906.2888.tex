\documentclass{amsart}   
\usepackage[utf8]{inputenc}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsthm}     
\usepackage[plainpages=false,pdfpagelabels,colorlinks=true,citecolor=blue,hypertexnames=false]{hyperref}  

\def\K{\mathbb K}
\def\Q{\mathbb Q}
\def\Z{\mathbb Z}
\def\N{\mathbb N}
\def\M{\mathsf M}
\def\Int{I}
\renewcommand{\algorithmicrequire} {\textbf{\textsc{Input:}}}
\renewcommand{\algorithmicensure}  {\textbf{\textsc{Output:}}}
\newcommand{\lclm}{\operatorname{lclm}} \newcommand{\lcrm}{\operatorname{lcrm}} \newcommand{\gcld}{\operatorname{gcld}} \newcommand{\gcrd}{\operatorname{gcrd}} \newtheorem{theo}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{corollary}{Corollary}
\theoremstyle{definition}
\newtheorem{example}{Example}




\begin{document}


\title{Chebyshev Expansions for\\ Solutions of Linear Differential Equations}

\address{Algorithms Project, Inria Paris-Rocquencourt}
\author{Alexandre Benoit}
\author{Bruno Salvy}
\email{Alexandre.Benoit@inria.fr,Bruno.Salvy@inria.fr}




\thanks{This work was supported in part by the Inria-Microsoft Research Joint Centre.}
\thanks{To appear in the proceedings of ISSAC'09.}
       
\begin{abstract}
A Chebyshev expansion is a series in the basis of Chebyshev polynomials of the first kind. When such a series solves a linear differential equation, its coefficients satisfy a linear recurrence equation. We interpret this equation as the numerator of a fraction of linear recurrence operators. This interpretation lets us give a simple view of previous algorithms, analyze their complexity, and design a faster one for large orders.
\end{abstract}

\keywords{Chebyshev series, Ore polynomials}
\maketitle


\section{Introduction}
Chebyshev series are series of the form

where  denotes the th Chebyshev polynomial of the first kind. These polynomials can be defined by

so that these series behave like Fourier series. Thus in particular, this series converges pointwise to~ on~ if  is continuous there, while the convergence is uniform if~ satisfies a Dini-Lipschitz condition or is of bounded variation (and \emph{a fortiori} if it is differentiable), see, e.g.~\cite{GilSeguraTemme2007a,MasonHandscomb2003}.
Then truncations of the series provide polynomials with good approximation properties on the interval , which makes these series an interesting data structure for real functions~\cite{Trefethen2007}. 

Orthogonality of the~ leads to the following integral representation of the coefficients:

We say that~ \emph{admits a Chebyshev expansion}  when these integrals converge, the symbol~ accounting for the factor~ in front of~ in~\eqref{chebseries}.

In the frequent case when~ is a solution to a linear differential equation, Clenshaw~\cite{Clenshaw1957} has given a numerical scheme to compute the coefficients~ without computing all these integrals.
In that case, the coefficients~ obey a linear recurrence equation. A method for the computation of this recurrence has been showed by several authors, first for small orders~\cite{FoxParker1968,Luke1969a}, then in more generality by Paszkowski~\cite{Paszkowski1975} and in the context of (early) symbolic computation by Geddes~\cite{Geddes1977}. We call this method ``Paszkowski's algorithm''.
The  use of this recurrence to compute the coefficients numerically is discussed in~\cite{Wimp1984}.
Paszkowski's method has been further improved by Lewanowicz~\cite{Lewanowicz1976} who gave an algorithm computing a smaller order recurrence in some cases. However, Lewanowicz's algorithm is not much discussed in the literature since it looks complicated (see the original article and the comment in \cite[p.~186]{Wimp1984}).
More recently, other methods have been given by Rebillard~\cite{Rebillard1998} and Rebillard and Zakraj\v{s}ek~\cite{RebillardZakrajsek2006}. 

In this work, we give a simple unified presentation of most of these algorithms, and design a faster one for large orders. Postponing the proofs and rigorous definitions, the basic idea can be presented by analogy with the computation of a recurrence for coefficients of Taylor series. The monomial basis~
satisfies

The analogous relations on the Chebyshev polynomials are easily derived from~\eqref{defTn} and trigonometry: 
Given a series , \eqref{taylor} leads to expressions for the coefficient of  in~ and : multiplication by~ maps to a negative shift on the indices; differentiation maps to a positive shift of the index followed by multiplication by~. Algebraically, we thus get an algebra morphism mapping~ to~ and  to~, with , . Here,  denotes the shift operator: , that does not commute with multiplication by~.
Now, if  is solution of a linear differential equation 

we deduce a recurrence operator  for its Taylor coefficients.
\begin{example} The simplest example is the exponential, for which  translates into~ ( denotes identity), which gives the recurrence  satisfied by~. 
\end{example}
The procedure for a series~ starts similarly: multiplication by~ maps to

The difference comes from the factor~ in~\eqref{eq:Cheb2}. The operation of differentiation followed by multiplication by~ is readily seen to map to~, but no simple linear operation for the Chebyshev coefficients of  exists. The idea at this stage is to divide by  afterwards by introducing a \emph{formal inverse} of~. Thus we write~. This can be further simplified since
,
so that 

We call such an expression a \emph{fraction of recurrence operators}.
\begin{example}\label{example exp} For the exponential, we now get
	 
The last term is an analogue of reduction to the same denominator. The final factor will be called the numerator of the fraction. It corresponds to the recurrence

It turns out that in this example, the Chebyshev coefficients are known:
, where~ is the modified Bessel function of the first kind, and they do satisfy~\eqref{recIn}.
\end{example}              
This example generalizes. We show here that all the algorithms mentioned above can be interpreted as first rewriting the input linear differential equation in one way or another, then applying the morphism above and finally returning the numerator of the result. In the case of Lewanowicz's algorithm the fraction is normalized (its numerator and denominator are relatively prime), which is why its output may have smaller order.

In Section~\ref{sec:Ore}, we give the formal setting for fractions of recurrence operators, together with the basic algorithms. This is then applied to the specific case of Chebyshev series in Section~\ref{sec:Chebyshev}. Then we give a compact presentation of Paszkowski's and Rebillard's algorithms, provide a complexity analysis and design a faster algorithm in Section~\ref{sec:algo}. We briefly comment on the different approach taken by Rebillard and Zakraj\v{s}ek in~\S\ref{rebillard-zakrajsek}. We conclude in Section~\ref{sec:examples} with a few examples.











\section{Fractions of Recurrence Operators}\label{sec:Ore}
We use  Ore's framework of non-commutative polynomials~\cite{Ore33}, that we now recall.
                                                                      
\subsection{Ore Polynomials}
The rings of linear differential operators and of linear recurrence operators are special cases of rings of Ore polynomials. They possess the commutation rules

More generally, a ring of polynomials in an indeterminate~ with coefficients in a field~ is an \emph{Ore polynomial ring} when its product is defined by associativity from

where for all  and  in~, 

The ring is denoted~. Linear differential operators are obtained with~ and~; linear recurrences operators with~ and~. 

The main property of these rings is that the degree (with respect to~) of a product is the sum of the degrees of its factors. (In particular, there are no zero-divisors). From there, it is not difficult to write an algorithm for Euclidean division on the right. Once right Euclidean division is available, the Euclidean algorithm and its extended version follow and can be used to compute: greatest common right divisors, denoted gcrd; least common left  multiples, denoted lclm; the corresponding cofactors for the BÃ©zout identity and for the lclm~\cite{Ore33,BronsteinPetkovsek1996}.

When~ is invertible, we also get Euclidean division on the left, and from there greatest common left divisors (gcld), least common right multiples (lcrm) and the corresponding cofactors by the Euclidean algorithm.
If moreover , as is the case for recurrence operators, it is also possible to define Laurent polynomials with~ and . These are denoted~. 

The rings we use in this work are the ring of linear differential operators denoted  and the ring of linear recurrence operators  (with a different meaning for both~).


Apart from their non-commutativity, Ore polynomials generally behave like ordinary polynomials. A notable exception is divisibility. 
\begin{example}The recurrence operator  is relatively prime with~, but  is a right divisor of .
\end{example}
Still, the following property holds (and similarly for gcrd's when they exist):

Indeed,  is a left divisor of~ and the remaining factor has to be a left divisor of both~ and~. The converse divisibility is clear.

In order to distinguish the action of an operator from the product in these rings of operators, which corresponds to composition of actions, we use the notation~ for the former. Thus~, .

\subsection{Fractions}

Ore's construction of fractions parallels the commutative case.
Given two non-zero polynomials~ and~, by definition of the lclm, there exist two polynomials~ and~ such that

With this notation, the pairs~ and~ are called equivalent when
.
This can be verified to be an equivalence relation and the class is called a fraction and denoted~ (which is equal to ). This construction makes the set of fractions a non-commutative field. 

Reduction to the same denominator for sums is given by

as can be checked by left multiplication with~. 

To compute the reduction of a product of two fractions , the starting point is the lclm of~ and the numerator~. There exist two polynomials~ and~ such that

Then,  and , so that finally


\subsection{Irreducible Fractions}
Having in mind our use of fractions for recurrence operators, we now concentrate on the case when~ is invertible, so that gcld's are available. The results here are probably known, but we did not find them in the literature.

A fraction~ is called \emph{irreducible} when .
\begin{prop}
Assume  is invertible and let~ be a fraction. Then there exists an irreducible fraction equal to . Moreover, its numerator and denominator are unique up to a factor in~.
\end{prop}
\begin{proof}
Existence follows from dividing out numerator and denominator by~. 
Assume  and  . 
By definition of equivalence, , where . Moreover this lclm relation implies .
Now,

where we use~\eqref{gcld}. But since , necessarily  and then
 and .
\end{proof}

The following lemma is useful in the computation of recurrences for Chebyshev series.
\begin{lemma} \label{lemirred}
Assume~ is invertible and let  be an irreducible fraction and  a polynomial. Then  with  is irreducible and equal to .
\end{lemma}
\begin{proof}
We have   by definition of the .
The polynomial  is a left divisor of , and therefore is a left divisor of  Thus  is a left divisor of both  and , hence is~.
\end{proof}

\section{Recurrences for Chebyshev Coefficients}\label{sec:Chebyshev}
We now have the theoretical tools to prove that a morphism from linear differential operators to linear recurrence operators produces fractions whose numerators give recurrences for the coefficients of Chebyshev series solutions. 

The algorithms then become easy to state, their algorithmic difficulty being concentrated in the Euclidean algorithm in the previous section.

\subsection{Morphism}
We define a morphism of~-algebras from  to the field of fractions of  by

The proof that  is a well-defined morphism of non-commutative rings reduces to checking the commutation . Indeed,


\begin{algorithm}[t]
	\caption{Lewanowicz' algorithm\label{algo:Lewanowicz}}
	\begin{algorithmic}
		\REQUIRE 
		\ENSURE  such that 
		\STATE 
		\STATE 
		\FORALL { from  to }
			\STATE Compute .
			\STATE 
			\STATE 
		\ENDFOR
		\RETURN 
	\end{algorithmic}
\end{algorithm}
\subsection{Horner's Rule and Lewanowicz' Algorithm}
\begin{prop}\label{horner}
	Let~ be a linear differential operator in . The evaluation of  by Horner's rule
	
using Eqs.~\eqref{eq:redsum} and~\eqref{eq:redprod} for the computation of sums and products produces a fraction~ that is irreducible.
\end{prop}
The algorithm deduced from this statement (Algorithm~\ref{algo:Lewanowicz}) is due to Lewanowicz. It is made very clear by the use of fractions of recurrence operators. The proof that the numerator of its output gives a recurrence for the Chebyshev coefficients is given in the next section.
\begin{proof}
We prove that each iteration of the loop produces~ that are relatively prime and such that 

Initially,  and  is a polynomial, so that  and the property holds.
If it holds for~, the next stage of the loop computes .
Recall that~. Then let . Lemma~\ref{lemirred} applied to the inverse  implies that~. It follows that~. Again by Lemma~\ref{lemirred} applied to the inverse, multiplying by~ on the right preserves irreducibility and the property holds for~.
\end{proof}
We quote without proof the following result.
\begin{prop}[Lewanowicz]\label{prop:Lewanowicz}
When the leading coefficient  of the differential equation does not vanish at~ or~, then all the gcrd's are trivial,  at step~ and the resulting~ is~.
\end{prop}
This is related to the fact that~.
\subsection{Chebyshev Expansions}
\subsubsection{Main Theorem}
We now prove our main result: the morphism defined above behaves as expected with respect to Chebyshev expansions.
\begin{theo}\label{main}
Let  be a linear differential operator of order~ with polynomial coefficients. Let 
be such that either of the following hypotheses holds:

Then  admits a Chebyshev expansion ,
 admits a Chebyshev expansion  and the sequences  and  are related by , for any~ such that . In particular, if~, then the Chebyshev coefficients of~ satisfy  for any numerator of~.
\end{theo}
The easy case is when (H) holds. Hypothesis (H') makes it possible to deal with some functions that are singular at , but whose singularities are not ``too bad'': they are regular singular points. 
\begin{proof}
First, convergence of the integral in (H) or (H') implies convergence of the analogous integral where  is replaced by~ for~ as well as the integrals where these functions are multiplied by~, . This shows that both~ and~ admit Chebyshev expansions.

If the result holds for any numerator of~ then in particular it has to hold for the numerator of its irreducible form.
Conversely, if~, then  for any~, so that it is also sufficient to prove the result for an irreducible~.

\begin{lemma}[Basic Cases] Under the same hypotheses, the result holds for  a constant times identity, ,  if (H) holds,  if (H') holds.
\end{lemma}
\begin{proof}
If  is a constant times identity then ,  and  clearly holds.

If ,  Eq.~\eqref{eq:Cheb1} implies


If  and (H) holds, 
we use the following variant of Eq.~\eqref{eq:Cheb2} when 

that can be checked from~\eqref{defTn}. The continuity of  and the convergence of the integral in (H) imply that integrating by parts is possible and this gives

Both limits of the term between brackets are~0, by convergence of the integral~.

The case when  reduces to checking , that does not depend on~.

If  and (H') holds, we start from

An argument similar to the previous one then gives

which proves the result since~.
\end{proof}
\begin{lemma}[Product]\label{product}
Assume the result holds for~ with~, as well as for another operator~ with .
Let~ and~, these fractions being irreducible.
Let , and assume  is irreducible. Then the result holds for~ with~.
\end{lemma}
\begin{proof}
Let  be related by , . Then

whence the result.
\end{proof}
As a consequence, the result holds when~ is a monomial, by induction.





\begin{lemma}[Sum]\label{sum}Assume the result holds for an operator~ with  and for a polynomial~ with the same~. Then it holds for  with .
\end{lemma}
\begin{proof}
Let  be irreducible.
If~, , then
 
This proves the property for  since . 
\end{proof}
The result now holds for~ an arbitrary polynomial, as a sum of its monomials.

Let finally  if (H) holds and  if (H') does. In both cases,  can be written
 with polynomial . The hypothesis on~ implies that the result holds for~ with  for  and therefore also for  with  by Lemma~\ref{product}.


Let  and  for . Let 
 be irreducible.
We prove by induction that the result holds for  with .
For , the result has just been proved.
If the result holds for~ with , then we obtain an irreducible : when  this follows from Lemma~\ref{product}, while when~,  itself is a polynomial (by induction). Thus the result holds for~ with . Since it also holds for  with  and  is a polynomial, we get the result for their sum by Lemma~\ref{sum}. Thus by induction the result holds for~ with~, which concludes the proof of Theorem~\ref{main}.
\end{proof}

\subsubsection{Examples}
\begin{example}
The function  satisfies (H) for any~. This proves the recurrence~\eqref{recIn} computed in Example~\ref{example exp}.
\end{example}
\begin{example}\label{ex-5}
The function  is annihilated by~. Hypothesis (H) does not hold, but (H') does. Application of the morphism gives , , so that the theorem asserts that the Chebyshev coefficients satisfy 
 The actual values can be computed by standard properties of the Beta integrals and indeed

\end{example}
\begin{example}\label{ex arccos} The function  gives an example showing that analytic hypothesis such as (H) or (H') are necessary. This function is annihilated by . Direct application of the morphism gives , , which would suggest that the recurrence is . However, neither (H) nor (H') holds in this case. Left multiplying  by  gives a new operator such that (H') holds. Then the theorem proves that the coefficients are annihilated by

This can be checked against the actual coefficients:

\end{example}

\section{Algorithms}\label{sec:algo}
We now cast the algorithms of 
Paszkowski \cite{Paszkowski1975} and Rebillard~\cite{Rebillard1998} as computations of the numerator of a fraction of recurrence operators. We also propose a new faster algorithm. All three algorithms compute the same recurrence. Starting from

they avoid the need for fractions by replacing differentiations by integrations, exploiting the polynomial

These algorithms compute the polynomial~, that is a numerator of~. Thus, by Theorem~\ref{main}, their result is a recurrence operator annihilating the coefficients of Chebyshev series solutions of~. 

If~, Proposition~\ref{prop:Lewanowicz} shows that~ is the denominator of the irreducible fraction and therefore in that case all algorithms compute the irreducible fraction.
Otherwise, the result of these algorithms may have larger order than that returned by Lewanowicz' algorithm.
\begin{example} The function  has been dealt with in Example~\ref{ex-5}.
Lewanowicz' algorithm returns the second order recurrence~\eqref{eq coeffs ex-5}.
The numerator returned by the other algorithms has order~4:

It is however possible to recover the smaller order recurrence: the gcld of  with  is , so that  factors as  with~ as in Example~\ref{ex-5}.
\end{example} 
More generally, dividing out the result of the computation of  on the left by the gcld with~ yields the result of Lewanowicz's algorithm.
\subsection{Paszkowski's Algorithm}
The starting point of Paszkowski's algorithm is to rewrite~ from~\eqref{eq:L} as

The polynomials~ can be computed inductively starting with~ and subtracting~ to produce a smaller order operator.
Then

Algorithm~\ref{alg P1} follows.
\begin{algorithm}
  \caption{Paszkowski's Algorithm}
  \label{alg P1}
  \begin{algorithmic}
    \REQUIRE 
    \ENSURE 
	\STATE Compute 's such that 
	\STATE 
	\FORALL{ from  to  }
    	\STATE 
	\ENDFOR
 	\RETURN 
  \end{algorithmic}
\end{algorithm}


\subsection{Rebillard's Algorithm}                        
The starting point of Rebillard's algorithm is the identity

that follows from an easy induction.
From there, he deduces

Algorithm~\ref{alg Reb} follows.
\begin{algorithm}
  \caption{Rebillard's Algorithm}
  \label{alg Reb}
  \begin{algorithmic}
    \REQUIRE 
    \ENSURE 
	\STATE Compute , 
	\STATE 
	\FORALL{ from  to  }
    	\STATE 
	\ENDFOR
	\RETURN 
  \end{algorithmic}
\end{algorithm}

\subsection{Complexity Analysis}
We now give a complexity analysis of Paszkowski's, Rebillard's and Lewanowicz' algorithms. This reveals a source of inefficiency for large orders, that we correct in our new algorithm in the next section.

We need to consider the size of  polynomials in two variables~ and~. We say that a polynomial has \emph{bidegree}  in  when it has degree~ in~ and  in~.

First, we state more precisely the shape of~.
\begin{prop}[Rebillard \cite{Rebillard1998}]
	\label{Di}
	For all ,

where ,

and we use the Pochhammer symbol .
\end{prop}
In particular, the bidegree of~ in  is~. The proof is a tedious but easy induction that we omit here. From this formula follows a precise estimate of the size of the polynomial we are computing.
\begin{corollary}\label{coro:size}If  in~\eqref{eq:L} has bidegree~ in~, then  is a polynomial of bidegree  in~ at most .
\end{corollary}
\begin{proof}
First,  can be rewritten as in Paszkowski's algorithm  with . The identity

shows that this is a polynomial in~. Each term of the sum is the product of a polynomial of bidegree~, a polynomial of bidegree , a polynomial of bidegree at most~. Thus each summand has bidegree at most , whence the result.
\end{proof}
\begin{prop}\label{theoPasz}Given  as above for input, Paszkowski's algorithm requires~ arithmetic operations in~.
\end{prop}
\begin{proof}
The first step is the computation of the~'s from the~'s. 
The inductive method requires only  arithmetic operations. 
Using ideas from~\cite{BoChLR08}, it is actually possible to decrease this complexity further to  operations~\cite{Bo09} (here,  is the complexity of polynomial product, see, e.g., \cite{GathenGerhard1999}).

The next step is the loop. The main cost in step  is the multiplication of  by . We multiply a polynomial of bidegree , with a polynomial in  only, of degree~. The cost of this multiplication is  arithmetic operations.
Summing for  up to~ gives the result.
\end{proof}



\begin{prop}
In the same conditions,	Rebillard's algorithm requires  arithmetic operations.
\end{prop}
\begin{proof}
The first step is the computation of the . The polynomial  has bidegree  in . Then each  can be computed in  operations and all of them in  operations.

The cost of the th step of the loop is dominated by the cost of the multiplication of  by .
The polynomial  has bidegree  in , while  has bidegree . Naive multiplication then requires  operations. Summing over  gives the result.
\end{proof} 

The output of Lewanowicz' algorithm is different in general. We give a comparison in the cases when it coincides.
\begin{prop}In the same conditions, and if all the gcrd during its execution are trivial, Lewanowicz' algorithm requires 
 arithmetic operations.
\end{prop}
\begin{proof}
We only give a sketch. When all gcrd's are trivial, it turns out that the computation of lclm's and cofactors is of the same order of complexity as the computation of the product~, where moreover~. This is the same as in the analysis of Paszkowski's algorithm.
\end{proof}


\subsection{New Fast Algorithm}
We now give another algorithm for the same operator . The design of our algorithm is motivated by computational complexity issues. In the  analyses above, most of the complexity comes from the fact that during the computations, the bidegrees of the intermediate polynomials grow linearly and they are multiplied by polynomials of fixed degree. Instead, we aim at balancing degrees so as to make use of the recent fast algorithm for the product of linear differential operators~\cite{FFTjoris,BoChLR08}, that we denote FFT-mult. We achieve the following complexity.


\begin{theo}\label{complexity}
Algorithm \ref{alg new} computes the recurrence operator~ in  arithmetic operations. 
\end{theo}
Here,  is a feasible exponent for matrix multiplication with coefficients in  (see, e.g., \cite{GathenGerhard1999}). We now prove this result.
\begin{algorithm}
	\caption{Divide and Conquer}
	\label{alg new}
	\begin{algorithmic}
		\REQUIRE Polynomials 
		\ENSURE 
		\IF {} \RETURN 
		\ELSE
			\STATE 
			\STATE Compute recursively  and .
			\RETURN .
		\ENDIF
	\end{algorithmic}
\end{algorithm}  

Starting from~\eqref{Paszkowski}, we write

We choose~ and apply the same idea recursively.

The time consuming part of the computation is the product , for which we give a specialized algorithm.
\begin{algorithm}
	\caption{Fast Multiplication\label{fast-mult}}
	\label{mult}
	\begin{algorithmic}
		\REQUIRE  and 
		\ENSURE 
		\STATE Decompose  as in Eq.~\eqref{eq:decomp}
		\STATE 
		\FORALL{ from 0 to }
			\STATE 
		\ENDFOR
		\RETURN .
	\end{algorithmic}
\end{algorithm}

To simplify the presentation, assume~. Corollary~\ref{coro:size} implies that~ has degree~ in~,  has degree at most  in . They have rational function coefficients whose degrees are also bounded by this result. If  is large, the degrees in~ are unbalanced, so we first decompose

where the~'s have degree at most~ in~. Note that this decomposition is only an extraction of coefficients and does not use any arithmetic operation.

We are thus left with the product of~ with the~'s. Although both have rational function coefficients and thus cannot be multiplied directly by FFT-mult, we also have that  has polynomial coefficients in  of degree at most~ and therefore so does . To perform the product efficiently, we make use of the fact that FFT-mult proceeds by evaluation and interpolation: during the evaluation phase, we evaluate the \emph{rational function} coefficients of~ as if they were polynomials (and within the same complexity thanks to our degree bounds), avoiding the zeros~ of their denominators; similarly, we evaluate the polynomial coefficients of~. Then we compute the necessary products. With the bounds on the degree in~ we have for the \emph{polynomial} coefficients in the result, the interpolation phase then returns the result. The complexity of each of these multiplications is thus  operations. The algorithm for this multiplication is summarized in Algorithm~\ref{fast-mult}. Note also that a constant factor can be saved by not recomputing the ``FFT'' of  at each time.

\begin{prop}
The cost of multiplying  by  with~ using Algorithm~\ref{fast-mult} is  arithmetic operations.
\end{prop}

\begin{proof}
We have seen that each multiplication~ has complexity . This is performed  times. Right multiplication by powers of~ does not use any arithmetic operations. The additions require a smaller number of operations, whence the result.
\end{proof}

Now, let  be the complexity of Algorithm~\ref{alg new}. Using this proposition, we get
 
the complexity estimate in Theorem~\ref{complexity} follows from the convergence of the geometric series. (Again, a constant factor can be saved by computing the powers of~ only once.)


\subsection{Algorithm by Rebillard and Zakraj\v{s}ek}\label{rebillard-zakrajsek}
In~\cite{RebillardZakrajsek2006}, an algorithm of a different nature is proposed. It does not compute a numerator of~, but manages in some cases to derive a smaller order recurrence corresponding to a right factor of the numerator of~. We plan to come back to this algorithm in connexion to minimality issues in future work. Here, we merely give a few indications and comments on special cases.
\begin{example} The following is taken from~\cite{RebillardZakrajsek2006}. Starting from the differential operator , the computation of~ by Lewanowicz's algorithm leads to a numerator of order~4, whereas the algorithm in~\cite{RebillardZakrajsek2006} produces one of order only~3. We note that this operator can also be obtained by Lewanowicz's algorithm, applied to~ instead of~. In many cases, this technique applies.
\end{example}
Since the algorithm in~\cite{RebillardZakrajsek2006} computes a right factor of the numerator of~, analytic hypotheses such as (H) or (H') in our Theorem~\ref{main} are necessary.
\begin{example} In the case of , the numerator of  is a constant (see Example~\ref{ex arccos}), so that this is also the result of the algorithm in~\cite{RebillardZakrajsek2006}. Starting from , we obtained an operator of order~4 in Ex.~\ref{ex arccos}. On this operator, the algorithm in~\cite{RebillardZakrajsek2006} returns an operator of order~2. 
\end{example}

\section{Examples}\label{sec:examples}
The fast algorithm does not lend itself easily to an efficient implementation in Maple, since it relies on fast evaluation/interpolation and fast matrix product. We have however implemented the slow algorithms in Maple and show how other algorithms from computer algebra can sometimes be applied to the resulting recurrences, so that nice expression for the coefficients can be recovered. We have also implemented variants of Horner-like evaluations that seem to perform well, see~\cite{Benoit2008}.


\begin{example}[arctan] Starting from , we get
	
The initial conditions are computed by Maple as  and , . The recurrence can then be solved by Petkov\v sek's algorithm and we get

\end{example}

\begin{example}[error function] Starting from , we get a more complicated recurrence:
	
A closed form is known to be

but it seems that the algorithms in computer algebra are not strong enough to find this automatically, yet.
\end{example}

\begin{example}[arctanh] Starting from , we get 
by computing the numerator of~. Although neither of our hypotheses (H) or (H') holds here, this result is correct, as can be checked from the expansion

Again, this suggests that more work on obtaining a recurrence of minimal order is necessary.
\end{example}





\begin{thebibliography}{10}

\bibitem{Benoit2008}
Alexandre Benoit.
\newblock D{\'e}veloppements de fonctions {D}-finies sur des polyn{\^o}mes de
  {T}chebychev.
\newblock Master's thesis, Universit{\'e} Paris VI-MPRI, September 2008.

\bibitem{Bo09}
Alin Bostan, Fr{\'e}d{\'e}ric Chyzak, and Nicolas Le~Roux.
\newblock Skew-polynomial products by evaluation and interpolation.
\newblock In preparation.

\bibitem{BoChLR08}
Alin Bostan, Fr{\'e}d{\'e}ric Chyzak, and Nicolas Le~Roux.
\newblock Products of ordinary differential operators by evaluation and
  interpolation.
\newblock In David~J. Jeffrey, editor, {\em ISSAC'08: Proceedings of the
  twenty-first international symposium on Symbolic and algebraic computation},
  pages 23--30. ACM, 2008.

\bibitem{BronsteinPetkovsek1996}
Manuel Bronstein and Marko Petkov{\v s}ek.
\newblock An introduction to pseudo-linear algebra.
\newblock {\em Theoretical Computer Science}, 157:3--33, 1996.

\bibitem{Clenshaw1957}
C.~W. Clenshaw.
\newblock The numerical solution of linear differential equations in
  {C}hebyshev series.
\newblock {\em Proceedings of the Cambridge Philosophical Society},
  53:134--149, 1957.

\bibitem{FoxParker1968}
L.~Fox and I.~B. Parker.
\newblock {\em Chebyshev polynomials in numerical analysis}.
\newblock Oxford University Press, London, 1968.


\vfill\eject

\bibitem{Geddes1977}
K.~O. Geddes.
\newblock Symbolic computation of recurrence equations for the {C}hebyshev
  series solution of linear {ODE}'s.
\newblock In Carl~M. Andersen, editor, {\em Proceedings of the 1977 MACSYMA
  User's Conference}, pages 405--423, 1977.
\newblock NASA CP-2012.


\bibitem{GilSeguraTemme2007a}
Amparo Gil, Javier Segura, and Nico~M. Temme.
\newblock {\em Numerical methods for special functions}.
\newblock Society for Industrial and Applied Mathematics (SIAM), Philadelphia,
  PA, 2007.

\bibitem{Lewanowicz1976}
S.~Lewanowicz.
\newblock Construction of a recurrence relation of the lowest order for
  coefficients of the {G}egenbauer series.
\newblock {\em Zastosowania Matematyki}, XV(3):345--395, 1976.

\bibitem{Luke1969a}
Yudell~L. Luke.
\newblock {\em The special functions and their approximations, {V}ol. {II}}.
\newblock Mathematics in Science and Engineering, Vol. 53. Academic Press, New
  York, 1969.

\bibitem{MasonHandscomb2003}
J.~C. Mason and D.~C. Handscomb.
\newblock {\em Chebyshev Polynomials}.
\newblock Chapman \& Hall/CRC, 2003.

\bibitem{Ore33}
Oystein Ore.
\newblock Theory of non-commutative polynomials.
\newblock {\em Ann. of Math. (2)}, 34(3):480--508, 1933.

\bibitem{Paszkowski1975}
Stefan Paszkowski.
\newblock {\em Zastosowania numeryczne wielomian\'ow i szereg\'ow
  {C}zebyszewa}.
\newblock Pa\'nstwowe Wydawnictwo Naukowe, Warsaw, 1975.
\newblock Podstawowe Algorytmy Numeryczne. [Fundamental Numerical Algorithms].

\bibitem{RebillardZakrajsek2006}
L.~Rebillard and H.~Zakraj{\v s}ek.
\newblock Recurrence relations for the coefficients in hypergeometric series
  expansions.
\newblock In Ilias Kotsireas and Eugene Zima, editors, {\em Computer Algebra
  2006. Latest Advances in Symbolic Algorithms}, pages 158--180. World
  Scientific, 2006.

\bibitem{Rebillard1998}
Luc Rebillard.
\newblock {\em {\'E}tude th{\'e}orique et algorithmique des s{\'e}ries de
  {C}hebyshev solutions d'{\'e}quations diff{\'e}rentielles holonomes}.
\newblock PhD thesis, Institut National Polytechnique de Grenoble, Grenoble,
  July 1998.

\bibitem{Trefethen2007}
Lloyd~N. Trefethen.
\newblock Computing numerically with functions instead of numbers.
\newblock {\em Math. Comput. Sci.}, 1(1):9--19, 2007.

\bibitem{FFTjoris}
Joris van~der Hoeven.
\newblock F{FT}-like multiplication of linear differential operators.
\newblock {\em J. Symbolic Comput.}, 33(1):123--127, 2002.

\bibitem{GathenGerhard1999}
Joachim von~zur Gathen and J{\"u}rgen Gerhard.
\newblock {\em Modern computer algebra}.
\newblock Cambridge University Press, New York, 1999.

\bibitem{Wimp1984}
J.~Wimp.
\newblock {\em Computation with Recurrence Relations}.
\newblock Pitman, Boston, 1984.

\end{thebibliography}

\end{document}
