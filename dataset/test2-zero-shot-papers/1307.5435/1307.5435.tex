\documentclass[10pt,twocolumn,twoside]{IEEEtran}
\usepackage[dvips]{graphicx}
\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage{amssymb}
\usepackage{pmat}
\usepackage{balance}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{amsthm}
\usepackage{amssymb }
\usepackage{amsmath}
\usepackage{bm}
\usepackage{mathrsfs}
\usepackage{amsfonts}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{algorithm, algpseudocode}
\usepackage{cite}
\usepackage{stfloats}
\def\GQJ{\bm{J}_{\text{Q}}^{(\text{G})}}
\def\Nobs{{\aleph^{(l)}_\text{obs}}}
\def\Nfuse{\aleph^{(l)}_\text{fuse}}
\def\Nq{{\aleph^{(q)}_\text{obs}}}
\def\Nall{{\aleph}}
\def\x{\bm{x}}
\def\f{\bm{f}}
\def\g{\bm{g}}
\def\X{\bm{X}}
\def\V{\bm{V}}
\def\W{\bm{W}}
\def\w{\bm{w}}
\def\Pr{{\mathbf Pr}}
\def\L{{\cal L}}
\def\cx{{{\mathbb{X}}}}
\def\CF{\text{FF}}
\def\LF{\text{LF}}
\def\asym{\text{asym}}
\def\ggg{\text{global}}
\def\MC{\text{MC}}
\newtheorem{pro}{\bf{Proof}}
\newtheorem{rst}{Result}
\newtheorem{thm}{Theorem}
\newtheorem{theo}{\bf{Theorem}}
\newtheorem{lemma}{\bf{Lemma}}
\newtheorem{cor}{\bf{Corollary}}
\newcommand{\that}{\widehat{\theta}}
\newtheorem{Mydef}{\bf{Definition}}
\graphicspath{{New_Fig/}}
\newtheorem{prop}{Proposition}
\algnewcommand\Input{\item[\hspace{6pt}\textbf{Input:}]}
\algnewcommand\Output{\item[\hspace{6pt}\textbf{Output:}]}
\algnewcommand\OutputVal{\textbf{output} }
\def\FOJ{\bm{L}}
\def\FOM{\bm{M}}
\def\FOLA{\bm{L}_{\text{A}}}
\def\FOC{\bm{C}}
\def\FOD{\bm{D}}
\def\FOA{\bm{A}}
\def\FOE{\bm{E}}
\def\PA{P_a(k)}
\def\JAUX{\bm{J}_{\text{AUX}}}
\def\PC{P_c(k\!+\!1)}
\def\Lk{\bm{L}(k\!+\!1)}
\def\IAll{\bm{I}(0\!:\!k\!+\!1)}
\def\JAUXk{\bm{J}_{\text{AUX}}(k)}
\def\Q{\bm{Q}(\cdot)}
\def\NL{N_L}
\def\NS{N_s}
\def\z{\bm{z}}
\def\Z{\bm{Z}}
\def\y{\bm{y}}
\def\Y{\bm{Y}}
\def\NF{N_f}
\def\LJ{\bm{J}^{(l)}(\x(k\!+\!1))}
\def\LPJ{\bm{J}^{(l)}(\x(k\!+\!1|k))}
\def\Lqo{Y^{(l,m)}}
\def\Lqok{Y^{(l,m)}(k)}
\def\Lo{Z^{(l,m)}}
\def\Lok{Z^{(l,m)}(k)}
\def\Lrk{R^{(l,m)}(k)}
\def\Lgk{\bm{g}^{(l,m)}(\x(k))}
\def\Lg{\bm{g}^{(l,m)}}
\def\J{\bm{J}}
\def\QJ{\bm{J}_{\text{Q}}}
\def\LJk{\bm{J}^{(l)}(k)}
\def\LLk{\bm{L}^{(l)}(k+1)}
\def\LLpk{\bm{L}^{(l)}(k+1|k)}
\def\LJ{\bm{J}^{(l)}}
\def\LL{\bm{L}^{(l)}}
\def\LJQ{\LJ_{\text{Q}}}
\def\LLQ{\LL_{\text{Q}}}
\def\LLQk{\LL_{\text{Q}}(k+1)}
\def\Lhk{\bm{h}_i^{(l,m)}(k)}
\def\GJk{\bm{J}^{(\text{G})}(k\!+\!1)}
\def\GLk{\bm{L}^{(\text{G})}(k+1)}
\def\GJ{\bm{J}^{(\text{G})}}
\def\GQJk{\bm{J}_{\text{Q}}^{(\text{G})}(k\!+\!1)}
\def\GQLk{\bm{L}_{\text{Q}}^{(\text{G})}(k\!+\!1)}
\def\GQL{\bm{L}_{\text{Q}}^{(\text{G})}}
\def\Jk{\bm{J}(\x(k\!+\!1))}
\def\QJk{\bm{J}_{\text{Q}}(k\!+\!1)}
\def\RJk{\bm{J}_{\text{R}}(k\!+\!1)}
\def\RJ{\bm{J}_{\text{R}}}
\def\QC{\bm{C}_{Q}}
\def\JAUXlk{\bm{J}^{(l)}_{\text{AUX}}(k)}
\def\Il{\bm{I}^{(l)}}
\def\PCl{P^{(l)}_c(k\!+\!1)}
\def\JAUXlAll{\bm{J}^{(l)}_{\text{AUX}}(0\colon\!k)}
\def\JAUX{\bm{J}_{\text{AUX}}}
\title{Distributed Computation of the Conditional PCRLB for
  Quantized Decentralized Particle Filters}
\author{
\authorblockN{Arash Mohammadi, Amir Asif, Xionghu Zhong,
  and A. B. Premkumar}\\
\authorblockA{Computer Science and Engineering, York University,
  Email: marash, asif@cse.yorku.ca.}\\
\authorblockA{Computer Engineering, Nanyang Technological
  University, Email: xhzhong, asannamalai@ntu.edu.sg.}
}
\begin{document}
\maketitle
\begin{abstract}
The conditional posterior Cram\'er-Rao lower bound (PCRLB) is an
effective sensor resource management criteria for large,
geographically distributed sensor networks.  Existing algorithms for
distributed computation of the PCRLB (dPCRLB) are based on raw
observations leading to significant communication overhead to the
estimation mechanism.  This letter derives distributed computational
techniques for determining the conditional dPCRLB for quantized,
decentralized sensor networks (CQ/dPCRLB). Analytical expressions for the
CQ/dPCRLB are derived, which are particularly useful
for particle filter-based estimators. The CQ/dPCRLB is compared for
accuracy with its centralized counterpart through Monte-Carlo
simulations.
\end{abstract}
\begin{keywords}
Bayesian Estimation, Distributed Signal Processing, Particle Filters,
PCRLB, Sensor Resource Management.
\end{keywords}
\section{Introduction} \label{sec:Introduction}
Recent developments in sensor technologies and advances in
communication systems allow a large number of observation nodes
(sensors) to be deployed in geographically distributed sensor networks
typically configured using the decentralized topology without
employing a global fusion centre.  To elaborate further, the letter
considers a commonly reported decentralized sensor network
topology~\cite{Tharmarasa:2011} with two types of nodes:
(i)~\textit{Sensor nodes}: with limited power used to record
measurements,~and; (ii)~\textit{Local processing nodes}: responsible
for selecting sensors, processing the data locally, and cooperating
distributively with other connected processing nodes to reach a
consensual tracking estimate for the target. In such geographically
distributed sensor networks, limitations in power budget, system
bandwidth, and communication capabilities impose two critical
restrictions.  First, the maximum number of active sensors at a
particular time is constrained.  Second, only quantized observations
are exchanged between the sensors and processing nodes.  Within its
observation neighbourhood, a local processing node, therefore,
activates a small subset of sensors to receive the quantized version
of their observations.

\noindent
\textbf{Motivation}: The Posterior Caram\'er-Rao Lower Bound (PCRLB)
has been used as an effective criteria for adaptive sensor resource
management problems~\cite{Tharmarasa:2011,Zuo:2011} because it
provides a near-optimal bound of the achievable estimator's
performance and can be computed predictively. Further, it is
independent of and not constrained by a specific estimation
methodology.  Existing PCRLB
derivations~\cite{Msechu:2008,Zhou:2010,Duan:2008}
using quantized observations are limited to centralized estimation
architectures and have not yet been extended to decentralized
topologies.
The letter addresses this gap and derives the PCRLB for decentralized
state estimation in sensor networks with \textit{quantized}
observations. We refer to the distributed computation of the PCRLB as
dPCRLB.  Our previous work~\cite{Arash:TSP1} derives distributed
expressions for computing the \underline{non-conditional} dPCRLB for
full-order decentralized state estimation.  In~\cite{Arash:SPL}, we
extend our derivations to the \underline{conditional} dPCRLB (where
instead of using statistics for the observation model, actual
observations from the previous iterations are utilized).
Both~\cite{Arash:TSP1} and~\cite{Arash:SPL} consider raw observations
in the dPCRLB derivations, as is also the status quo in the
decentralized estimation literature~\cite{Tharmarasa:2011}.  Such a
setup leads to a large communication overhead between the sensors and
their associated processing node making the system impractical.

\noindent
\textbf{Contributions}: The letter extends the conditional dPCRLB
framework to quantized observations with emphasis on particle filter
estimators. Additional contributions of the letter include:
\noindent
(a) Both computational and communication complexity
of~\cite{Arash:SPL} are reduced in the proposed conditional dPCRLB
with quantized observations (CQ/dPCRLB).  (b) In~\cite{Arash:SPL}, the
conditional Fisher information matrix (FIM), i.e., the inverse of the
conditional dPCRLB, is expressed as a function of the auxiliary FIM
which is updated distributively at each iteration.  The CQ/dPCRLB updates the
conditional dPCRLB directly without the need of computing the
auxiliary FIM leading to significant communication savings.

\section{System Description} \label{sec:background}
We consider the non-linear dynamical system

where the state vector  and
 is the global uncertainty in the state model at
iteration .
Processing node , (), is connected to a set of sensor nodes: a subset of which  is
 active at each iteration. The active sensors connected to node  constitute its local observation
neighbourhood . The total number of active sensors in the network~is
, where  denotes the
cardinality operator.
Sensor  in the observation neighbourhood of
node , i.e., , makes observation . Instead of
transferring the raw observation, sensor  communicates its
quantized version  to the fusion node  based on the
following model

where  is the local quantization operator at
node , and  and  are,
respectively, the local observation model and uncertainty at sensor
 connected to fusion node .  For simplicity and without loss of
generality, the quantization operators  are
considered to be the same across the network (i.e.,
).  Collectively, the overall
quantized observation vector at node  is denoted~by

Depending on how many sensors are activated by the processing node
, the dimension of observation vector  is different at
each processing node.  As for the quantized observations
, vector  is the collection of all raw
observations associated with the processing node , i.e.,

We consider an -bit quantization scheme, where
node 's quantized observation  can take any discrete value
between  and .  The set of quantization threshold is
denoted by  where for
brevity  and .
The likelihood that  is at level ~is denoted by  with

Section~\ref{sec:cpcrlb} reviews the local conditional dPCRLB for raw
observations as presented in~\cite{Arash:SPL} with one proposed
modification.
\section{Conditional  for Raw Observations} \label{sec:cpcrlb}
Based on the conditional PCRLB inequality, the mean square error
associated with the local estimate  of
the state vector at node  is lower bounded~as~follows

where ,  denotes
expectation, and  is the estimation
error.
Defining the 1st and 2nd order partial derivatives as
 and
,
the local \underline{accumulated} conditional FIM 
corresponds to the state trajectory  from iteration  to  and is given~by

Another local FIM  is the local \underline{instantaneous} conditional FIM  associated with , which is obtained by taking
the inverse of ) right-lower block of .  Please refer to
 Appendix~A for differences in the two FIMs.
Node  updates  as follows.
\begin{rst}\label{Varsh}
The instantaneous local FIM  associated with estimate
 at node  is computed as follows


\end{rst}
\noindent
The derivation of Result~\ref{Varsh} is included in Appendix~B.
In~\cite{Arash:SPL},   is computed recursively from the local instantaneous auxiliary FIM  which is the inverse of  right-lower square block of~the accumulated auxiliary FIM . The latter is defined~as

with .
The  algorithm proposed in~\cite{Arash:SPL}, therefore, requires decentralized
 fusion  of both the local FIMs and the local auxiliary FIMs, while
Result~\ref{Varsh}
eliminates the need for fusing the local instantaneous auxiliary FIMs and, therefore,  cuts the communication overhead by half.

Distributed computation of the conditional PCRLB requires a recursive expression for the predictive local conditional FIM  which is~similar to~\eqref{qqww1} except  is
substituted with  as follows

Having computed the  local FIMs  and the local prediction FIMs
  at iteration , the next step in the conditional dPCRLB
 is to fuse these local FIMs to compute the global instantaneous conditional FIM
 .  Reference~\cite{Arash:SPL} derives a fusion rule for
 assimilating local conditional FIMs into the global conditional FIM
 when raw observations are available at each local node.
Section~\ref{QdPCRLB} extends our derivations
 to quantized observations and eliminates the need for fusion of local instantaneous auxiliary FIMs.
\section{ with Quantized Observations} \label{QdPCRLB}
In Proposition~\ref{Varsh}, raw observations  are replaced with
their quantized version , which results in the quantized
filtering conditional FIM .  Since terms
, ,
 are based on the state model, they remain the
same. Term  in Eq.~\eqref{qqww3} is now
computed using the quantized observation as follows

To compute , likelihood
 along with the second derivative of
its logarithmic function is needed. Because of quantized observations,
 transforms into a probability mass
function that is discrete with second derivative replaced by a double
summation as described below.
Given the state variables, local observations are assumed independent
such that

and  is the delta function. We note that
, where  was
defined immediately after Eq.~\eqref{hdef} previously and has the
second derivative

Under mild regularity conditions, the expected value of
\eqref{eq:g} is equal to the variance of its first moment,~i.e.,

Eqs.~\eqref{eq:Jyl}-\eqref{eq:likPartition} are used to compute
.  Finally, the local quantized
filtering FIM is given by

Eq.~\eqref{qlFIM} is derived  by applying the following factorization

to the quantized version of Eq.~\eqref{eq/def/FIM} and then taking the
inverse of the () right lower block of
.  The similarity
between Eqs.~\eqref{qqww1} and~\eqref{qlFIM} is intuitively pleasing.
The local predictive FIM  is derived in the similar
manner as~\eqref{qlFIM} with  replaced
by~\eqref{ext.crlb.1} 

\vspace{.05in}
\noindent
\textbf{Fusing Local FIMs (CQ/dPCRLB)}: Result~\ref{qdPCRLB} provides
a fusion rule for assimilating the local FIMs with quantized
observations to compute the global quantized FIM.
\begin{rst}\label{qdPCRLB}~The sequence 
corresponding to the global information submatrix (CQ/dPCRLB) with
quantized local observations follows the following recursion

\end{rst}
\noindent
The proof of Result~\ref{qdPCRLB} is included in Appendix~C.

\vspace{.05in}
\noindent
\textbf{Gaussian Observation Noise}: We derive analytical expressions
for the case when local observations  are zero-mean Gaussian
with variance , i.e., .  The
likelihood that  is at level ~is

where  is the standard cumulative Gaussian distribution.  Based on~\eqref{eq:hi}, each derivative term in~\eqref{eq:likPartition} is represented~as



\noindent
\textit{A. Computation of The Conditional dPCRLB}
\begin{figure*}[th]
\centering
\mbox{\subfigure[]{\includegraphics[scale=0.4]{Fig2_vf.eps}}
\subfigure[]{\includegraphics[scale=0.4]{Fig1_vrf.eps}}
\subfigure[]{\includegraphics[scale=0.4]{Fig4_vrf.eps}}}
\caption{\label{fig1}  A sample decentralized bearing only
  tracking setup.  Comparison of the conditional
  dPCRLBs~\cite{Arash:SPL} using raw observations with the CQ/dPCRLBs
  using 8-bit quantized observations. (c) Effect of quantization on
  the CQ/dPCRLB for different (4, 5, 6, 7, and 8 bit) quantization
  levels.}
\vspace{-.1in}
\end{figure*}

The analytical computation of the expectations in
Result~\ref{qdPCRLB}
is not practical and,
therefore, particle filter-based approaches are proposed.  If the
state estimator is based on distributed particle
filters~\cite{Arash:TSP1}, then the same particle set can be used in
the CQ/dPCRLB algorithm.
An active sensor  communicates its quantized observation to the associated processing  node. The processing nodes themselves communicate  the local conditional FIMs  and statistics of local posteriors  (i.e., local state estimates and their corresponding covariance matrices) to the neighbouring processing nodes which are then fused in a distributed fashion to compute the global state estimate and the global conditional FIM.
We explain the CQ/dPCRLB algorithm in the
context of the consensus based distributed particle filter
(CF/DPF)~\cite{Arash:TSP1} being used as the state estimator.  The
CF/DPF implements two particle filters at each node:
(i)~Local filter which approximates the local posterior at node 
with a set of weighted particles , and;
(ii)~Fusion filter which combines the local posteriors to estimate the
global posterior with a second set of particles .
All information regarding the observations collected up to time  at
node , are presented in the local particles ,
while the information available across the network is provided by the
global particles .
The CQ/dPCRLB comprises of the following steps:

\noindent
\textit{I. Local FIMs}:
\begin{enumerate}
\item[1.] Eqs.~\eqref{Qcond/d11}-\eqref{Qcond/d12} are computed at
  node  based on Monte-Carlo integration using local particles
  .
\item[2.] For computing Eq.~\eqref{d22/q}, first, node  computes
  the predictive particles  by propagating
   through , and then
  computes Eq.~\eqref{d22/q} using  and
  .
\item[3.] The local FIMs are then computed using Eq.~\eqref{qlFIM}.
\end{enumerate}
\noindent
\textit{II. Global FIM}:
\begin{enumerate}
\item[4.] The expectations in~\eqref{cond/QFIM}-\eqref{eq:c22/q} are
  computed using the global particles  to derive
  the FIMs . Eq.~\eqref{eq:c22/q} includes
  summation of local FIMs across the network typically computed using
  the average consensus algorithms~\cite{Arash:SPL} in a decentralized
  fashion.
\item[5.]
Result~\ref{qdPCRLB}
is used to compute the global FIM based on the local FIMs computed in
  Step~4.
\end{enumerate}

\vspace{.05in}
\noindent
\textit{B. Communication Savings}

First, the transfer of quantized observation (instead of raw data)
between sensors and associated processing nodes leads to significant
communication savings.  Second, the communication overhead for
computing the global auxiliary FIM from the local auxiliary FIMs
across the network is eliminated in the proposed CQ/dPCRLB algorithm.
With average consensus~\cite{Arash:TSP1}, the second savings is of
 (i.e., the communication complexity reduces by half), where  is number of states,
 the number of processing nodes in the neighbourhood of
processing node , and  the number of consensus iterations.
For complexity analysis of the CF/DPF refer to~\cite{Arash:TSP1}.
The CQ/dPCRLB can  be further extended to communicate quantized versions of the local state statistics (quantized local tracks~\cite{Ruan:2005}) and  local FIMs between neighbouring processing nodes during the fusion filter stage which will be considered in future work.

\section{Simulation} \label{sec:simu}
A decentralized bearing-only tracker with nonlinear clockwise
coordinate turn state model~\cite{Arash:TSP1} and observation model

is considered.  represents the coordinates of
 sensor (). Both process and observation noises are normally
 distributed with the observation noise 
 model assumed to be state dependent such that the bearing noise
 variance at sensor  depends on the distance between the
 observer and target.
A sensor network (Fig.~\ref{fig1}(a)) consisting of~ static
sensors and  =  processing nodes scattered in a square region
of dimension ()m is implemented. Our goal is to
evaluate the performance of the proposed CQ/dPCRLB, therefore, the
activated sensors are selected at random and limited to three sensors
per processing node. For simplicity, the sensors are distributed
uniformly with the processing node at the centre of its rectangular
()m neighbourhood. Each processing node communicates
only with its activated sensors within its ()m
neighbourhood and other processing nodes within a radius of m.

The objective of our Monte Carlo simulations is three folds. The first
objective is to validate the effectiveness of the conditional FIM
approximation (i.e., to replace the global auxiliary FIM with the
global conditional FIM) in Result~\ref{qdPCRLB}. Fig.~\ref{fig1}(b)
plots the conditional dPCRLB and CQ/dPCRLB with and without the
proposed global conditional FIM approximation. In each case, results
for both raw (bottom two plots) and quantized (top two plots)
observations are included. Within each set of plots in
Fig.~\ref{fig1}(b), the bounds virtually overlap verifying the
effectiveness of the global conditional FIM approximation. The second
objective is to compare the CQ/dPCRLB with quantized observations for
accuracy against the conditional dPCRLB computed from raw
observations~\cite{Arash:SPL}. Comparing bounds across the two sets of
plots in Fig.~\ref{fig1}(b), we note that the respective plots do not
overlap but are fairly close to each other. Despite using quantized
observations, the CQ/dPCRLB is a reasonable approximation of the
dPCRLB. Illustrated in Fig.~\ref{fig1}(c), the third objective is to
quantify the potential CQ/dPCRLB performance loss as a function of the
number of quantization levels. The CQ/dPCRLB approaches the dPCRLB as
the number of quantization levels are increased.

\section{Conclusion} \label{sec:conclusion}
The PCRLB has recently been proposed~\cite{Tharmarasa:2011} as an
effective selection criteria for decentralized sensor resource
management in large, geographically distributed sensor networks.
Existing decentralized algorithms for computing the PCRLB are
typically based on raw observations resulting in a significant
communication overhead. The letter derives the PCRLB for decentralized
estimators in sensor networks with quantized observations and tests it
in a bearing only tracking application.
\appendices
\section{} \label{app:0}
\noindent
Below, we highlight the relationship between the local accumulated conditional FIM  and
local instantaneous conditional FIM .
The local instantaneous conditional FIM  is computed using either of the following three approaches: (i) Directly by inverting large matrix ;
 (ii)  Recursively as a function of the previous local instantaneous auxiliary FIM  ~\cite{Arash:SPL}, and; (iii) Recursively as a function of the previous local instantaneous conditional FIM  presented in Result~\ref{Varsh}.
In approach (i), first the local accumulated conditional FIM  is factorized as follows

Then, the local instantaneous conditional FIM  associated with the estimate
  is obtained by taking the inverse of the 
right-lower square block of  by applying the following matrix inversion Lemma~\cite{Zuo:2011}.
\begin{lemma}\label{MIL}
Matrix inversion Lemma:

where subblocks  have conformable
dimensions, , and
.
\end{lemma}
\noindent
Based on Lemma~\ref{MIL},  the local instantaneous conditional FIM is given by

which requires inversion of large matrix .
The report describes approach (iii) in more details in Section~III-B.
\section{} \label{app:A}
\begin{figure*}[t]
\normalsize
\setcounter{equation}{34}

where .

\setcounter{equation}{29}
\hrulefill
\vspace*{4pt}
\end{figure*}
Here Result~1 is derived. We also show that under a minor constraint,  the result in~\cite{Arash:SPL} reduces to Result~1, which is equivalent to replacing the local instantaneous auxiliary FIM   by the local instantaneous conditional FIM .
The rational for the approximation is included after the proof.
\begin{proof}[Proof of Result~\ref{Varsh}]
The conditional FIM given observations up to and including time  is factorized as follows

where .
Term  is the inverse of the right lower block of  which is given by  (using the matrix inversion lemma)

For next iteration , we have

where  which can be factorized as follows

Taking  logarithm of Eq.~\eqref{eq:axr}.

Therefore, Eq.~\eqref{AppA:eq2} reduces to Eq.~\eqref{AppA:eq3} on top of the next page
where , , , and  are given by Eqs.~\eqref{Qcond/d11}-\eqref{qqww3}.
The four blocks on the top left sub-matrix of Eq.~\eqref{AppA:eq3} are functions of  which make them different from  in Eq.~\eqref{appA:eq1}. In order to recursively compute  from , these four terms are approximated by their expectations with respect to , i.e.,
\setcounter{equation}{35}

Similarly, it can be shown that

Finally, Eq.~\eqref{AppA:eq3} can be approximated as follows

Going back to complete the proof, we note
that the information sub-matrix  is given
by the inverse of the right bottom () block of 
(corresponding to  in Eq.~\eqref{AppA:eq32}), i.e.,

Based on Eq.~\eqref{AppA:eq8},
the middle term in Eq.~\eqref{AppA:eq6} reduces to  which by
substituting  in Eq.~\eqref{AppA:eq6} proves Result~1.
\end{proof}

Finally we note that Result 1 is valid with the following approximation:

The top left  four blocks of the accumulated conditional FIM given by Eq.~\eqref{AppA:eq3}
are replaced by their expectations with respect to .

As shown above, this leads to Eqs.~\eqref{qqww1}-\eqref{qqww3} of Result~1. Comparing Eqs.~\eqref{qqww1}-\eqref{qqww3} with our earlier result~\cite{Arash:SPL}, we note that the instantaneous auxiliary FIM  is replaced with the instantaneous conditional FIM . Consequently, the CQ/dPCRLB updates the
conditional dPCRLB directly without the need of computing the
auxiliary FIM leading to significant communication savings (by a factor of 2).
\section{} \label{app:B}
\begin{figure*}[t]
\normalsize
\setcounter{equation}{52}

where .

\setcounter{equation}{42}
\hrulefill
\vspace*{4pt}
\end{figure*}
Below, Result~\ref{qdPCRLB} is proved.
First, we derive Lemma~\ref{lemma/post/fac/CDpcrlb} which provides a factorization of the global quantized conditional
posterior distribution  at iteration  as a function of the local quantized conditional
posterior distribution  at iteration  and the global  quantized conditional
posterior distribution  at iteration .
\begin{lemma}\label{lemma/post/fac/CDpcrlb}
Assuming that the quantized observations conditioned on the state variables are
independent, the global posterior for a  network with  processing nodes is
factorized as follows

where , \\
and .
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lemma/post/fac/CDpcrlb}]
Using the Markovian property

Comparing Eq.~\eqref{eq/post/fac/CDpcrlb} with \eqref{eq/cond/fact1}, we need to prove: (i) , and; (ii) .

Relationship (i):
Given the state variables, the observations are assumed to be independent as is the case in most Bayesian estimators.
Then,  the first term on the right hand side (RHS) of~(\ref{eq/cond/fact1})~is  given by

We also factorize the local conditional distribution at node , for (), as follows

In terms of the local likelihood , Eq.~\eqref{eq:46} can be expressed as follows

Substituting Eq.~\eqref{eq47new} in Eq.~\eqref{eq/cond/fact2}, we have

which proves Relation~(i).

\noindent
Relationship~(ii): Term  can be factorized as follows

Since  is independent of the state
variables, Eq.~\eqref{eq/cond/fact3} can be expressed as follows

which proves Relation~(ii).

\noindent
This completes the proof for Lemma~1.
\end{proof}
\begin{proof}[Proof of Result~\ref{qdPCRLB}]
Given the quantized observations up to and including time , the global accumulated conditional FIM  can be decomposed as follows

As stated previously in Appendix~A, the  instantaneous conditional FIM  is obtained by taking the inverse of the right lower block of . Using Lemma~\ref{MIL} we get

For iteration , we
decompose .
As for Eq.~\eqref{cpp},
 the global accumulated conditional FIM for iteration  is
then given by

Using Lemma~\ref{lemma/post/fac/CDpcrlb}, Eq.~\eqref{eq49} reduces to Eq.~\eqref{AppA:eq7} given at the top of the page.
Similar to our discussion in Appendix~B,
the four blocks on the top left sub-matrix of Eq.~\eqref{AppA:eq7} are functions of , which make them different from  in Eq.~\eqref{cpp}.  In order to recursively compute   from , these four blocks are approximated by taking their expectations with respect to  resulting in
\setcounter{equation}{53}

where block  denotes a block of all zeros.
Terms ,  and  were defined previously
 in Eqs.~\eqref{cond/Qd12}-\eqref{eq:c22/q}.
Next, using Lemma~\ref{lemma/post/fac/CDpcrlb}, term  in Eq.~\eqref{fo/J/exp} is expressed as

Finally,  we note that the two summation terms in Eq.~\eqref{cond/c22-v2} are  individual sums of the local instantaneous conditional FIMs at iteration , i.e.,

and

Term  in Eq.~\eqref{cond/c22-v2}, therefore, reduces to

The information sub-matrix 
can then be calculated as the inverse of the right lower ()
sub-matrix of   (Eq.~\eqref{fo/J/exp}) as follows

Simplifying Eq.~\eqref{ext.crlb.12}, we get

where Eq.~\eqref{AppA:eqn4} has been used to obtain the final result.
This completes the proof for Result~2.
\end{proof}
\begin{thebibliography}{10}

\bibitem{Tharmarasa:2011} R. Tharmarasa, T. Kirubarajan, A. Sinha, and
  T. Lang, \newblock ``Decentralized Sensor Selection for Large-Scale
  Multisensor-Multitarget Tracking," \newblock {\em IEEE
    Trans. Aerospace \& Elec. Sys.,} vol.~47, no.~2,
  pp.~1307-1324,~2011.

\bibitem{Zuo:2011} L.~Zuo, R.~Niu, and P.K.~Varshney, \newblock
  ``Conditional Posterior Cram\'er-Rao Lower Bounds for Nonlinear
  Sequential Bayesian Estimation," \newblock {\em IEEE
    Trans. Sig. Proc.,} vol.~59, no.~1, pp.~1-14, 2011.


\bibitem{Msechu:2008} E.J. Msechu, S.I. Roumeliotis, A. Ribeiro,
  G.B. Giannakis, \newblock ``Decentralized Quantized Kalman Filtering
  with Scalable Communication Cost," \newblock {\em IEEE Trans. on
    Sig. Proc.}, vol. 56, no. 8, pp. 3727-3741, 2008.

\bibitem{Zhou:2010} Y. Zhou, J. Li, D. Wang, \newblock ``Posterior
  Cram\'er-Rao Lower Bounds for Target Tracking in Sensor Networks
  With Quantized Range-Only Measurements," \newblock {\em IEEE
    Sig. Proc. Letters}, vol.~17, pp.~157-160, no.~2,~2010.

\bibitem{Duan:2008} Z. Duan, V.P. Jilkov, and X.R. Li, \newblock
  ``Posterior Cramer-Rao Bounds for State Estimation with Quantized
  Measurement," \newblock {\em IEEE Southeastern Symposium on System
    Theory}, pp.~376-380, 2008.

\bibitem{Arash:TSP1} A. Mohammadi and A.Asif, \newblock ``Distributed
  Particle Filter Implementation with Intermittent/Irregular Consensus
  Convergence," \newblock {\em IEEE Trans. on Sig. Proc.}, In Press,
  2013.

\bibitem{Arash:SPL} A. Mohammadi and A.Asif, \newblock ``Decentralized
  Conditional Posterior Cram\'er-Rao Lower Bound for Nonlinear
  Distributed Estimation," \newblock {\em IEEE Sig. Proc. Letters},
  vol. 20, no. 2, pp. 165--68, Feb. 2013.

\bibitem{Ruan:2005}
Y. Ruan and P. Willett,
\newblock ``A quantization architecture for track fusion,"
\newblock {\em IEEE Transactions on Aerospace and Electronic Systems}, vol. 41, no. 2, pp.671-681, 2005.

\end{thebibliography}
\end{document}
