\documentclass{article}

\PassOptionsToPackage{numbers, compress}{natbib}






\usepackage[final]{neurips_2019}



\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{array}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{enumitem}



\title{Tri-graph Information Propagation for Polypharmacy Side Effect Prediction}


\author{Hao Xu, Shengqi Sang, and Haiping Lu  \\
  Department of Computer Science, University of Sheffield, UK\\
  Department of Physics and Astronomy, University of Waterloo, Canada\\
  Perimeter Institute for Theoretical Physics, Waterloo, Ontario, Canada\\
  \texttt{hxu31@sheffield.ac.uk}, \texttt{s4sang@uwaterloo.ca}, \texttt{h.lu@sheffield.ac.uk } }

\begin{document}

\maketitle

\begin{abstract}
The use of drug combinations often leads to polypharmacy side effects (POSE). A recent method formulates POSE prediction as a link prediction problem on a graph of drugs and proteins, and solves it with Graph Convolutional Networks (GCNs). However, due to the complex relationships in POSE, this method has high computational cost and memory demand. This paper proposes a flexible Tri-graph Information Propagation (TIP) model that operates on three subgraphs to learn representations progressively by propagation from protein-protein graph to drug-drug graph via protein-drug graph. Experiments show that TIP improves accuracy by 7\%+, time efficiency by 83, and space efficiency by 3. 

\end{abstract}



\section{Introduction}
\label{intro}






When treating complex or simultaneous diseases, patients often have to take more than one drugs concurrently, called \emph{polypharmacy}. This often causes additional side effects, i.e., \emph{polypharmacy side effects} (POSE) due to interactions between drugs. Graph convolutional network (GCN) is an emerging approach for graph representation learning \cite{repre, gae, gcn}. GCN-based drug representation learning has shown improved performance in POSE prediction \cite{multiview, kgc, deepddi, gamenet, decagon}. 

\begin{wrapfigure}{r}{0.45\textwidth}
\vspace{-5mm}
  \hfill
\includegraphics[width=0.5\columnwidth]{graph.png}
  \caption{A multi-modal biomedical graph with two types of nodes: Drug (D) and Protein (P), and three types of edges: Protein-Protein (P-P) edges labeled with  (fixed), Protein-Drug (P-D) edges labeled with  (fixed), and Drug-Drug (D-D) edges labeled by a side effect .}
  \label{fig:graph}
\vspace{-5mm}
\end{wrapfigure}
POSE prediction can be viewed as a link prediction problem. As shown in Fig. \ref{fig:graph}, a \emph{multi-modal graph} can be constructed using 1) drug-drug interactions (D-D) with side effects as edge labels, e.g., from \emph{POSE clinical records}, 2) protein-drug interactions (P-D) with edges labeled as ,  and 3) protein-protein interactions (P-P)) with edges labeled as , e.g., from \emph{pharmacological information}. On such a graph, Zitnik et al. \cite{decagon} proposed a GCN-based \emph{Decagon} model to learn drug/protein representation via weighted aggregation of local neighbourhood information, with different weights assigned to different edge labels. It predicts all relationships between all nodes (drug/protein). This formulation enables the prediction of side effects that have strong molecular origins. However, due to the large number of nodes and possible edge labels, the aggregation operation has both high computational cost and high memory demand. 



Inspired by the Decagon model and motivated by its limitations, we propose a Tri-graph Information Propagation (TIP) model for improving prediction accuracy, and time and space efficiency, as shown in Fig. \ref{fig:encoder}. We start from the same multi-modal biomedical graph as in \cite{decagon}, constructed from three open BioSNAP-Decagon datasets \cite{snap}, as detailed in Table \ref{tab:dataset}. Instead of viewing the graph as a whole, we propose to view it as three subgraphs: the P-P graph, P-D graph and D-D graph, as in Figs.\ref{fig:graph} and \ref{fig:encoder}. TIP focuses on predicting relationships (side effects) in the D-D graph only rather than all relationships in the whole graph in Decagon. Thus, we treat drug nodes and protein nodes differently. Specifically, TIP has four steps: \textbf{1)} learn protein embedding on the P-P graph; \textbf{2)} propagate such embedding to the D-D graph via the P-D graph; \textbf{3)} learn the final drug embedding; \textbf{4)} predict the side effects on the D-D graph. 

TIP embeds proteins and drugs into different spaces of possibly different dimensions, rather than the same space and dimensions as in Decagon. This enables the propagation of flexible protein embedding to drug embedding as supplementary information. 
This brings three key benefits: \textbf{1) Flexibility}. We design three information propagation GCN modules corresponding to the first three TIP steps and two ways to combine protein and drug information in the P-D graph (step 2). Thus, we have the flexibility to set the number of GCN layers to control the order of neighborhood considered in each module.
\textbf{2) Efficiency}. Separate embedding of proteins and drugs can greatly improve the time (83) and space (3) efficiency of GCN-based representation learning and information propagation for them,  
\textbf{3) Accuracy}. More focused learning of drug representation makes better use of available data sources and can lead to improved POSE prediction, e.g., by 7.2\% in our experiments. 





\par






















\section{Tri-graph Information Propagation (TIP)}
\label{model}




\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{encoder.png}
	\caption{Information propagation in TIP encoder.}
	\vspace{-4mm}
	\label{fig:encoder}
\end{figure}


\begin{table}[t]
  \caption{BioSNAP-Decagon \cite{snap} datasets. (P) denotes protein node, and (D) denotes drug node.}
  \label{tab:dataset}
  \centering
  \begin{tabular}{l c r r c}
    \toprule
    \textbf{Dataset}     & \textbf{Nodes}     & \textbf{Edges} & \textbf{Unique Labels} & \textbf{Graph Name}\\
    \midrule
    PP-Decagon & 19081(P) & 715612 & 1 & P-P graph\\
    GhG-TargetDecagon & 3648(P), 284(D) & 18690 & 1 & P-D graph\\
    ChChSe-Decagon & 645(D) & 63473 & 1317 & D-D graph\\
    \bottomrule
  \end{tabular}
  \vspace{-5mm}
\end{table}


	
TIP follows the popular encoder-decoder framework \cite{repre}. Figure \ref{fig:encoder} shows the structure of the TIP encoder, within which Pharmacological information is propagated from P-P to D-D graph via P-D graph. The drug representation is produced by combining protein embedding and other available drug information. Further, drug embedding is used as input to the decoder to compute a set of side-effect-specified scores. Given a side effect and a drug pair, a higher score means the side effect is more likely to exist.


	
\textbf{TIP Encoder:} 
We follow the same \emph{Message Passing Neural Networks} (MPNN) framework \cite{message} as GCN \cite{gcn}, Decagon  \cite{decagon} and R-GCNs \cite{rgcn}. Our Encoder can be considered as a sequence of different MPNN cases. The protein and drug input features are  and , where  is the total number of proteins/drugs. 

\textbf{1) P-P Graph Embedding Module (PPM):} 
PPM is a GCN module \cite{gcn} used to learn protein embedding. The input of PPM module is the protein features . The relation between two hidden layers is given by

where  and  is associated with a protein node . 

\textbf{2) Graph-to-Graph Information Propagation Module (GGM):} This module takes  and the protein embedding generated by PPM to learn the embedding of pharmacological information associated with each drugs. It contains two units: \\
\textit{\textbf{2a)} Graph-to-Graph unit}: a one-layer MPNN with

where  can be regraded as a higher level representation of a subset of proteins, inspired by the subgraph embedding algorithm \cite{subgraph} which simply sums over the feature vectors of the involved nodes.\\ \textit{\textbf{2b)} Drug feature dimension reduction unit}: A linear transformation followed by an activation function:

The output of GGM  is the concatenation (\textbf{TIP-cat}) or the sum (\textbf{TIP-sum}) of  and .

\textbf{3) D-D Graph Embedding Module (DDM):}
This module is a R-GCN encoder with a basis-decomposition  regularization \cite{rgcn}. The update rule between layers is:

where  and .  The weight  was regularized by basis-decomposition \cite{rgcn}, which decomposes the matrix into the linear combination of a small number of basis matrices  with side-effect-specified coefficients .


\textbf{TIP Decoder:}
TIP takes the final drug representation  learned from  TIP encoder, and computes the probability  of side effect  given a pair of drugs embedding . For the POSE task we only care about predicting edges and edge labels on the  D-D graph. We consider using the DistMult factorization \cite{distmult} or a 2-layer neural network multi-label classifier as the decoder.

\textbf{1) DistMult Factorization decoder  (DF):} For the DF decoder \cite{distmult}, we first compute a  score tensor , and then get the probability by acting the sigmoid function on it:

where  is a trainable diagonal matrix associated with the side effect .

\textbf{2) Neural Network Decoder (NN):}
 NN-decoder is a multi-classifier with each side effects corresponded to a classifier. It takes the concatenation of drug pair's representations as input and embeds it into a lower-dimensional space in the first layer. For second layer it predicts the probability of all the possible side effects with the sigmoid function.  

We will compare the performance of two decoders in the following chapter.
\section{Experimental Results and Discussions}
\label{exp}
We implement TIP in PyTorch \cite{torch} with PyTorch-Geometric package \cite{pytorch}. The code is available at {\url{https://github.com/NYXFLOWER/TIP}}. Hyper-parameter setting, model training, optimization  and performance measurement details are in the supplementary material. 


\textbf{Models and Baselines} As shown in Table \ref{tab:xor}, we study two TIP model implementations TIP-cat and TIP-sum with concatenation or sum in GGM, and two degenerated TIP (dTIP) models dTIP\textsubscript{D} and dTIP\textsubscript{P} focusing on modelling drug or protein, respectively. We compare them with two recent POSE prediction models reporting state-of-the-art performance on the same dataset: Decagon \cite{decagon} and DistMult \cite{distmult} (reported by \cite{kgc}).
We also study R-GCN \cite{rgcn}, which shows good performance on standard datasets. These models are described in detail in the supplementary materials.



\begin{table}[t]
		\caption{Performance comparison on the SNAP-Decagon dataset. The best result is in bold for each evaluation metric. For Decagon, we quote the accuracy score in \cite{decagon} (marked with ) and estimate the space and time cost from sub-set implementation (indicated by ). Acronyms are described Secs. 2 and 3. \textbf{ARCT}: architecture; \textbf{Mem}: peak memory usage; \textbf{TpE}: computational time per epoch (including training and testing score computation).}
		\vspace{1mm}
		\begin{tabular}{lcrrrrr}
			\toprule
			\textbf{Model}& \textbf{ARCT} & \textbf{AUPRC} & \textbf{AUROC} & \textbf{AP@50} & \textbf{Mem(G)} & \textbf{TpE(s)} \\
			\midrule
			Decagon & & 0.832 & 0.872& 0.803 & 28 & 9600  \\
			DistMult & DF         & 0.835 & 0.859 & 0.834 & 9.25 & 41 \\
			R-GCN &DDM-DF  & 0.882 & 0.908 & 0.883 & 10.49  & 82  \\
			\midrule
			dTIP\textsubscript{D} & DDM-NN & 0.791 & 0.847 & 0.792 & 9.49  &  118 \\
			dTIP\textsubscript{P} & PPM-GGM-NN  & 0.746 & 0.743 & 0.733 & \textbf{ 6.38} & \textbf{29}  \\
			TIP-cat & PPM-GGM-DDM-DF  & 0.889 & 0.913 & \textbf{0.890} & 9.47  & 116  \\
			TIP-sum & PPM-GGM-DDM-DF & \textbf{0.890} &\textbf{0.914} &\textbf{ 0.890} & 9.47  & 115   \\
			\bottomrule
		\end{tabular}
		\vspace{-5mm}
		\label{tab:xor}
	\end{table}

\textbf{Performance comparison} 
TIP-cat and TIP-sum are the top two performers, outperforming Decagon by 7.2+\% in AUPRC and much more in AUROC and AP@50. Compared to Decagon, TIP-cat and TIP-sum reduce Decagon's computational time by at least 98.9\% and the peak GPU usage by at least 66.1\%. 
TIP models achieve good performance because of the efficient information propagation between graphs. Learning the embedding of proteins in the P-P graph is efficient as all the propagation operations share the same trainable parameter at each layer. 
The most time and memory consuming part is the drug embedding learning on D-D graph, which takes  of the total training time and hits the peak GPU memory usage of 9.47G. 

\textbf{Learning drug embedding with pharmacological information} Pharmacological information does contain drug-drug interaction information. By using it directly in dTIP\textsubscript{P}, we can get decent result with the shortest time. However, compared with R-GCN, additional pharmacological information in TIP-sum only improves the performance slightly.  In addition, the comparable performance of TIP-cat and TIP-sum has an interesting implication:
information propagation from PPM to GGM can be considered as learning a higher-level representation of a subset of proteins, which captures the relationship between proteins, and between proteins and drugs.

\textbf{Drug representation learning on D-D graph} Compared with DistMult that uses the dimension-reduced drug features directly (DF), the additional use of DDM in R-GCN (i.e., DDM-DF) improves over DF only by 5.6\% (in AUPRC), and the further additional use of PPM and GGM in TIP-sum (i.e., PPM-GGM-DDM-DF) improves over DF only by 6.6\%. This is because when using DDM, the drug can learn from its local neighborhoods and capture the relationship information. While protein-protein interaction and protein-drug interaction are extracted as additional drug features when using PPM-GGM. When decoding the drug embedding, The DF decoder outperforms the NN decoder by 11.5\% in accuracy and 43.9\% in time cost. However, the DF decoder requires more memory than the NN one.







\textbf{Prediction of molecular-original side effects} We list side effects with 20 best and worst performance in TIP-cat in AUPRC score in Figs. 4 and 5 of the supplementary materials, which show consistent conclusion that TIP is particularly good at modeling side  effects with inter-molecular origins.  However, by comparing these side effects, we find that even if the model does not have access to pharmacological information, it can predict the side effects with molecular origins very well. As shown in Table \ref{tab:xor}, the R-GCN model with architecture DDM-DF achieves performance that is competitive with TIP-cat or TIP-sum. 


\section{Conclusion}
\label{furture}
In this work, we proposed a new Tri-graph Information Propagation (TIP) model for predicting more than one thousand side effects between hundreds of drugs, using pharmacological information and drug-drug interaction clinical records. TIP has achieved state-of-the-art performance on POSE prediction task with much less training time and memory consumption. It can be further improved by using general optimization strategies. It can also be applied to other problems such as cancer risk or drug response prediction.







\medskip
\small

\bibliographystyle{plain}
\bibliography{neurips_2019}
\newpage
\textbf{\Large Supplementary Materials}

This is the supplementary material, including detailed problem formulation, notation, information propagation between nodes and graphs in TIP, model variants definitions, experimental setup and results.

\section{Problem Formulation and Notation}

As shown in Figure.1, we construct a large multi-modal biomedical graph with Drug (D) nodes and Protein (P) nodes for polypharmacy side effect modeling. Given a set of drugs , a set of proteins  and a set of side effects , where  is the total number of drugs/proteins/side effects, the graph can be denoted as , where .

In the graph , edges are directed and labeled: ,  and  is a label of edges ( will be defined below). There are three edge types: protein-protein (P-P) edges , drug-protein (P-D) edges and drug-drug (D-D) edges, the labels of edges are associated with different edge types. Corresponding to the edge types, there are three subgraphs:

\begin{enumerate}
	\item Undirected P-P graph: . The edges  are labeled with , .
	
	\item Undirected D-D graph: , where  means that a pair of drugs  can cause multi-pharmacy side effect .
	
	\item Directed D-P graph: ,  is a set of edges directed from a protein to a drug with edge label .
\end{enumerate}

As shown above, the / have the same label  / , but the label of  is chosen from , where each  represents a side effect. Note that between a pair of drugs there might be more than one links with different labels (a pair of drug might cause more than one side effects). Use  represents all kinds of labels.

We here consider POSE prediction task as a graph completion problem which aims to find the undiscovered edges and labels on the graph. Specifically, we extract the representation of the drugs from the defined graph  i.e. , and predict the probability of all possible side effects of a queried drug pair , i.e. . 


\section{TIP Encoder Design - An MPNN Framework Perspective}
In our TIP encoder, each module corresponding to a special case of the Message Passing Neural Networks (MPNN) framework \cite{message} on a graph. A simple differentiable MPNN framework on a graph  is:
	
where  is associated with a node . The input of the framework  is a node feature vector, and  is the hidden state of this node in the  layer of the neural network.  is the set of type-specified message passed in the form of  related to node , and  is typically a neural network-like function of the node state  and its neighborhood . 

Inspired by this architecture, we define the  tri-graph interaction propagation (TIP) encoder for calculating the update in each graphs forwardly. Figure.\ref{fig:pass} shows an example for information propagation between nodes and graphs in a TIP-cat implementation.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.8\columnwidth]{pass.png}
	\caption{An example of information propagation between nodes and graphs in a TIP-cat implementation with a 2-layer PPM, a GGM with concatenation operation and a 2-layer DDM. }
	\label{fig:pass}
\end{figure}


\section{Detail of Models}

The number of layers for PPM, GGM and DDM are set to (2, 1, 2) in all the experiments.

\paragraph{TIP-cat and TIP-sum} They both use a two-layer PPM with  and , a one-layer GGM and a two-layer DDM with ,  and base number . Their difference lies in the choice of aggregation function in GGM: TIP-cat uses concatenation with , while TIP-sum uses summation with . 

\paragraph{R-GCN} It's composed of a two-layer DDM with ,  and a DistMult Factorization (DF) decoder. It models the D-D graph directly and is a special case of generic R-GCN for multi-relational link prediction \cite{rgcn}.

\paragraph{dTIP\textsubscript{D}} It uses the same DDM as DR-DF, and does not use any protein information. Drug embeddings are learned from DR module, and a 2-layer  neural network multi-classifier with ,  is used as a decoder.

\paragraph{dTIP\textsubscript{P}} This variant uses the protein information and relationship information between drugs and proteins only to predict drug side effects. It uses a two-layer PPM with  and , a one-layer GGM  with concatenation, and the same two layer NN decoder as DDM-NN.

\section{Experimental Setup}

\paragraph{Loss Function and Negative Sampling} We use cross-entropy loss to optimize model, aiming to assign higher probabilities to observed edges and lower probabilities to undiscovered ones. Given a set of positive samples  , the negative samples  are sampled randomly from  until  \cite{gcn}.

\paragraph{Training and Testing data} We pre-processed the whole dataset (See \ref{tab:dataset}) by removing the side effects with less than 500 occurrence in the dataset.\footnote{It's the same pre-processing as in Zitnik et al. \cite{decagon}} For each side effect, we use  of the total edges in D-D graph for model training and the remaining  for testing.

\paragraph{Optimization} We use the Adam optimizer \cite{adam} with learning rate of 0.01 and train for 100 epochs for all the experiments. The TIP model is optimized end-to-end which means all trainable parameters in both encoder and decoder are trained together. Due to the Graph-to-Graph information propagation architecture of TIP model, the memory cost is much less than Decagon model \cite{decagon}. TIP model therefore is optimized by full-batch, which means the whole dataset is fed into the model in each epoch.

\paragraph{Model Implementation} We implement our TIP model  in PyTorch \cite{torch} with the PyTorch-Geometric package \cite{pytorch}. The evaluation of peak GPU memory usage uses the tools provided by pytorch\_memlab package\footnote{\url{https://github.com/Stonesjtu/pytorch_memlab}}.

\paragraph{Performance Measurement} We measure the performance using: 1) AUPRC: area under precision-recall curve, 2) AUROC: area under the receiver-operating characteristic, and 3) AP@k: average precision for the top  predictions for each side effect. 4) The computing cost (i.e. training time and peak GPU memory usage) . 

\section{Prediction of Molecular-original Side Effects}
We visualize the top 20 best and worst performance side effects in the DDM-DF model as shown in \ref{fig:top20} and \ref{fig:top20-dd}. Via comparing these figures, we find that even if model does not have pharmacological information, they can predict the side effects which have molecular origins very well. See more discussion in the main body.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\columnwidth]{top20.png}
	\caption{Side effects with the top 20 best and worst performance in TIP-cat on AUPRC scores. The side effects marked with red rectangular is in the side effect rank of the top 10 best/worst performance in \cite{decagon}}
	\label{fig:top20}
\end{figure}

\begin{figure}[ht]
	\centering
	\includegraphics[width=\columnwidth]{top20-dd.png}
	\caption{Side effects with the top 20 best and worst performance in DDM-DF model on AUPRC scores. The side effects marked with red rectangular is in the side effect rank of the top 10 best/worst performance in \cite{decagon}}
	\label{fig:top20-dd}
\end{figure}


\end{document}
