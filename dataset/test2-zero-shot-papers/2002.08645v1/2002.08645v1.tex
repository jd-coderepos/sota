

\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} \usepackage{multirow}
\usepackage{textcomp}
\usepackage{caption}
\usepackage{lscape}
\usepackage{rotating}
\usepackage{bm}
\usepackage[super]{nth}

\usepackage{hyperref}
\usepackage{amsmath}

\newcommand{\theHalgorithm}{\arabic{algorithm}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}



\usepackage[accepted]{icml2020}

\icmltitlerunning{Uncovering Coresets for Classification With MOEAs}

\begin{document}

\twocolumn[
\icmltitle{Uncovering Coresets for Classification\\With Multi-Objective Evolutionary Algorithms}





\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Pietro Barbiero}{polito}
\icmlauthor{Giovanni Squillero}{polito}
\icmlauthor{Alberto Tonda}{inrae}
\end{icmlauthorlist}

\icmlaffiliation{polito}{DAUIN, Politecnico di Torino, Torino, Italy}
\icmlaffiliation{inrae}{UMR 518 MIA, INRAE, Universit\'{e} Paris-Saclay, France}

\icmlcorrespondingauthor{Pietro Barbiero}{pietro.barbiero@polito.it}

\icmlkeywords{Machine Learning, Coresets}

\vskip 0.3in
]





\printAffiliationsAndNotice{\icmlEqualContribution} 

\begin{abstract}


A \emph{coreset} is a subset of the training set, using which a machine learning algorithm obtains performances similar to what it would deliver if trained over the whole original data. Coreset discovery is an active and open line of research as it allows improving training speed for the algorithms and may help human understanding the results. Building on previous works, a novel approach is presented: candidate corsets are iteratively optimized, adding and removing samples. As there is an obvious trade-off between limiting training size and quality of the results, a multi-objective evolutionary algorithm is used to minimize simultaneously the number of points in the set and the classification error. Experimental results on non-trivial benchmarks show that the proposed approach is able to deliver results that allow a classifier to obtain lower error and better ability of generalizing on unseen data than state-of-the-art coreset discovery techniques. 
\end{abstract}






\section{Introduction}

A \emph{coreset} is like a small set of paradigmatic examples: a concept can be more effectively explained to a learner resorting to them than by enumerating a longer list of cases. Such an analogy, however, should not be pushed too far: a coreset in Machine Learning (ML) is more formally defined as the minimal set of training samples that allows a supervised algorithm to deliver a result as good as the one obtained when the whole set is used \cite{bachem2017practical}. Traditionally, the process of discovering a coreset consists in pinpointing the minimal subset of the data that is sufficient to achieve such good performances. The definition does not specify what is the task of the algorithm (classification, regression, or other), nor what is the quantitative measure used to evaluate its performances. The relevance of coresets is manifold: reducing the size of the training set can boost the performance of training, limit the memory requirements; and pinpointing the key elements required may provide an insight on the internal process, helping to explain, if not understand, ML results. 

While remarkable contributions to the problem date back to 1960s \cite{Efroymson1960}, discovering coresets for a specific ML task is still an open and active research line. Plainly, reducing the number of samples could impair the performance of the ML algorithm, and sometimes even state-of-the-art methodologies may not be able to preserve all the information of the original training set. However, coresets have so many advantages that the reduced quality might be acceptable. Therefore, the problem should be more productively framed as \emph{multi-objective}: minimize the size of the corset and maximize the quality of the final result, and eventually let the user to pinpoint the best compromise of these two conflicting goals. Moreover, as ML algorithms employ different techniques to accomplish the same goals, it is also reasonable to assume that they would need coresets of different size and shape to operate at the best of their possibilities. Starting from these two assumptions, and building on previous works~\cite{barbiero2019fundamental,barbiero2019making,barbiero2019evolutionary}, an Evolutionary Algorithm (EA) for coreset discovery is proposed.


When compared to similar works in literature \cite{barbiero2019evolutionary}, the proposed EA improves the state of the art as follows:
\begin{itemize}
    \item it exploits a new representation of coreset candidates in the EA, making it possible to tackle datasets of larger size;
    \item it does not require parameter tuning, as all relevant parameters are empirically derived from the size of the dataset.
\end{itemize}

The proposed approach exploits an Evolutionary Algorithm (EA) to drive the automatic selection of coresets. Preliminary experiments suggest that classifiers trained with such evolved samples are able to generalize better than those trained with the whole set. The rest of paper is organized as follows: a few necessary background elements are reported in Section \ref{sec:background}, the proposed approach is outlined in Section \ref{sec:alberto:proposed_approach}, while Section \ref{sec:experiments} reports the experimental evaluation, and Section \ref{sec:conclusions} concludes the paper.

\section{Background}
\label{sec:background}


\begin{figure*}[!htb]
    \centering
    \includegraphics[width=0.95\textwidth, trim=4.5cm 0cm 4.3cm 0cm, clip=true]{evolution.pdf}
    \caption{Scheme of the proposed approach. Given a dataset, split into training and test, the original training set is used to seed the initial population with coresets, consisting of sets training samples. Candidate solutions are evaluated on compactness (number of samples) and error (classifier trained on a candidate solution, tested on the original training set). New solutions are obtained through evolutionary operators. Once the evolution is complete, the coresets on the Pareto front undergo a final evaluation, training the classifier that will then compute its error on the (unseen) original test set, to evaluate the generality of the approach.}
    \label{figure:algorithm}
\end{figure*}


ML literature reports a number of approaches for the identification of coresets in different scenarios, starting from Forward Stagewise published in the 1966 \cite{Efroymson1960} up to the Greedy Iterative Geodesic Ascent (GIGA) appeared in 2018 \cite{campbell2018bayesian}. Other remarkable contributions to this research include Matching Pursuit \cite{mallat1993}, Orthogonal Matching Pursuit (Ortho Pursuit) \cite{Pati1993}, Frank-Wolfe \cite{clarkson2010coresets}, and Least-angle regression (LAR) \cite{Efron2004, Boutsidis2013}. The original Frank-Wolfe algorithm applies in the context of maximizing a concave function within a feasible polytope by means of a local linear approximation. In Section~\ref{sec:experiments}, we refer to the Bayesian implementation of the Frank-Wolfe algorithm designed for core set discovery. This technique, described in~\cite{campbell2017automated}, aims to find a linear combination of approximated likelihoods (which depends on the core set samples) that is similar to the full likelihood as much as possible. GIGA is a greedy algorithm that further improves Frank-Wolfe. In~\cite{campbell2018bayesian}, the authors show that computing the residual error between the full and the approximated likelihoods by using a geodesic alignment guarantees a lower upper bound to the error at the same computational cost. On the other hand, Forward Stagewise, Least-angle regression, Matching Pursuit and Orthogonal Matching Pursuit were all originally devised as greedy algorithms for dimensionality reduction, but have been later applied to coreset discovery, as this last problem represents the transpose of feature selection, choosing samples instead of features. The simplest of the group is Forward Stagewise, which projects high-dimensional data in a lower dimensional space by selecting, one at a time, the features whose inclusion in the model gives the most statistically significant improvement. Matching Pursuit, on the other hand, includes features having the highest inner product with a target signal, while its improved version Orthogonal Matching Pursuit at each step carries an orthogonal projection out. Similarly, Least-angle regression increases the weight of each feature in the direction equiangular to its correlations with the target signal.

While all the above approaches take the size of the coreset as an input of the problem, recently multi-objective EAs were proposed to determine the best trade-off between final performances and coreset size \cite{barbiero2019fundamental,barbiero2019evolutionary}.
Since the 1990s, Evolutionary Computation (EC) techniques have been used to optimize ML frameworks, demonstrating the possibility to determine semi-optimal topologies or to efficiently train artificial neural networks \cite{angeline1994evolutionary,maniezzo1994genetic,frean1990upstart}. A topic where the use of EC soon appeared promising is feature selection \cite{vafaie1992genetic,kim2000feature}: as the performance of ML algorithms are quite sensitive to the choice of the features, a smart selection procedure is quite beneficial, and EC allowed to automatize the procedure. Selecting features using the eventual performance of a ML algorithm as fitness function is indisputably related to the problem of selecting training-set elements with the same goal.

EAs are stochastic optimization techniques, loosely inspired by the neo-Darwinian paradigm of natural selection. Candidate solutions (\emph{individuals}) are encoded in appropriate data structures (\emph{genomes}); the algorithm manipulates a set of them (\emph{population}), trying to generate better ones. An evaluation function (\emph{fitness function}) is used to assess the extent to which a individual solves the problem; such value controls the probability that the individual is selected for reproduction and survival. In each discrete step of the algorithm  (\emph{generation}), new individual (\emph{offspring}) are first generated using \emph{recombination} and \emph{mutation} (cumulatively called \emph{genetic operators}), then evaluated, and eventually the less fit are discarded. The EA stops when a user-defined threshold is reached, typically a limit on the number of generations. 



Among the most successful applications of EC, multi-objective optimization often takes the center stage. Optimization problems with contrasting objectives have no single optimal solution. Each candidate represents a different compromise between the multiple conflicting aims. Yet, it is still possible to search for \emph{optimal trade-offs}, for which an objective cannot be improved without degrading the others. The set of such optimal compromises is called \emph{Pareto front} from the \nth{19} century Italian engineer Vilfredo Pareto. Multi-objective evolutionary algorithms (MOEA) currently represent the state of the art for problems with contradictory objectives, and are able to obtain good approximations of the true Pareto front in a reasonable amount of time. One of the most known MOEAs is the Non-Sorting Genetic Algorithm II (NSGA-II)~\cite{deb2002fast}, that makes use of a special mechanism to spread candidate solutions on the Pareto front as evenly as possible, with considerable efficiency for problems with few objectives.

Interestingly, conflicting objectives to optimize abound in ML, with models being trade-offs between fitting and complexity, and coresets being compromises between number of samples considered and quality of the final result.

\section{Proposed approach}
\label{sec:alberto:proposed_approach}


In ML, coresets are defined as a subset of the initial (training) dataset that, used on its own, does not significantly impact the quality of the results of a ML algorithm with respect to the original dataset. In other terms, coresets can be seen as a \textit{summary} of the information contained in the original dataset, encompassing all the essential data to obtain the correct behavior of the ML algorithm. 








As the search space of all possible coresets of varying size for a given problem is clearly vast, it is necessary to resort to stochastic optimization to efficiently explore it. As said before, oreset discovery is inherently a multi-objective problem. Building on previous works on evolutionary coreset discovery~\cite{barbiero2019fundamental,barbiero2019evolutionary}, the proposed algorithm extends and improves the evolutionary approach making it suitable for tackling datasets of larger size without requiring parameter tuning. While the focus of this work is on coreset discovery for classification, it could easily be extended to regression problems. A summary of the proposed algorithm is presented in Figure~\ref{figure:algorithm}.



The algorithm can be summarized as follows. The dataset  is split into three groups for training (,  of samples), validating (,  of samples), and testing (,  of samples). Given the training sample , where  is the feature vector of  and  the corresponding class label, respectively, the objective is to estimate the probability that it belongs to the coreset , given the ML model (i.e., the \textit{classifier}) :

Each coreset candidate  is then used to fit the model parameters:

Finally, the trained classifier is used to make inferences on the training set:

The proposed approach makes it possible to evolve several solutions  approximating the coreset problem, reducing both the set size and the classification error :

The classification error has been defined as  minus the weighted  score \cite{sorensen1948method,chinchor1991muc} to account for class imbalance in multi-class datasets:

where  is the set of labels,  the set of \textit{predicted} sample/label pairs,  the set of \textit{true} sample/label pairs,  the subset of  with label , and  given by:


At the end of the evolution, the validation set  is used in order to evaluate the final solutions along the Pareto front. The maximum likelihood estimation for the evolved solutions is given by the core set providing the best score on the validation set:

Finally,  is used to train the model:

to make inferences on an unseen test set  and to compute the classification error:




\subsection{Genotype of a candidate solution}
\label{ssec:alberto:genotype_of_a_candidate_solution}
The genotype of a candidate solution is represented by a list of integers encoding the position of core samples in the original dataset. The list size may vary for different candidate solutions according to the number of core samples selected.


\subsection{Fitness functions}
\label{ssec:alberto:fitness_functions}
The two fitness functions used in this multi-objective problem are the size of the coreset (to be minimized), and the error of the target classifier trained on core samples, evaluated on the original dataset (to be minimized).

\subsection{Population initialization}
\label{ssec:alberto:population_initialization}
The starting population of the MOEA is initialized with coresets of random size. The minimum size corresponds to the number of classes in the problem, so that each candidate solution has at least one data point associated to each class; the maximum size is defined as a 1/10 of the original dataset size. Data points in each of such coresets are randomly drawn from the original dataset.

\subsection{Evolutionary operators}
\label{ssec:alberto:evolutionary_operators}
When generating new candidate solutions, the MOEA randomly selects two individuals and applies the following operators: i. cross over the two candidate solutions, by randomly distributing core samples contained in both between two children solutions; ii. mutate the children solutions adding or removing one sample from their respective list; iii. repair children if the obtained representations violate feasibility constraints (e.g. minimum number of classes, maximum and minimum coreset size).


\subsection{Parameters setting}
\label{ssec:hyperparameters}
Most parameters in an EA can be derived from a single value, the population size . In many practical applications,  is set through a trial-and-error approach; but there are alternatives, for example the empirical formula used by the state-of-the-art single-objective optimizer Covariance Matrix Adaptation Evolution Strategy \cite{hansen2001completely}, where population size is determined starting from problem size.

We decided to follow a similar approach, and derive the parameters of the proposed algorithm from the size of the problem, that in the case of coreset discovery can be estimated as , the total number of possible different coresets of size , that represents the search space that the algorithm will need to explore in order to find the best coreset candidate. We fix , where  is the total number of samples in the considered data set, as in most practical cases desirable coresets are 10\% or less of the initial samples.

Following the empirical formulas presented in \cite{hansen2001completely}, starting from  we can then derive the other necessary parameters for NSGA-II:



Where , as mentioned above, is the size of the population;  is the size of the offspring, the number of new candidate solutions generated at each iteration; and  is the number of iterations after which the algorithm will stop. When compared to the empirical formulas of CMA-ES for deriving parameters, the main difference is that we increased the minimum size of the population and the minimum number of iterations, as the specific problem we are tackling is quite challenging.







\section{Experimental Evaluation}
\label{sec:experiments}

All the necessary code for the experiments has been implemented in Python 3, relying upon open-source libraries as \texttt{scikit-learn v0.22}\footnote{scikit-learn: Machine Learning in Python, \url{http://scikit-learn.org/}}~\cite{scikit-learn} and \texttt{inspyred v1.0}\footnote{inspyred: Bio-inspired Algorithms in Python, \url{https://pythonhosted.org/inspyred/}}~\cite{inspyred}, and \texttt{bayesiancoresets v0.8}\footnote{bayesiancoresets: Coresets for approximate Bayesian inference, \url{https://github.com/trevorcampbell/bayesian-coresets/}}~\cite{campbell2019sparse}. The code is freely available under GNU Public License from a GitHub public repository\footnote{GitHub, \url{https://github.com/albertotonda/prototype-set-discovery}}. NSGA-II is used with the default parameters set by \texttt{inspyred}, with the exceptions described in subsection \ref{ssec:hyperparameters}. In order to generate reproducible results, all algorithms that exploit pseudo-random elements in their training process have been set with a fixed seed. Before running coreset selection algorithms, each dataset has been standardized removing the mean and scaling to unit variance (\texttt{StandardScaler}~\cite{zill2011advanced}). All the experiments have been run on the same machine: nn AMD EPYC 7301 16-Core Processor at 2 GHz equipped with 64 MiB memory.

Table \ref{tab:data-sets} summarizes the main characteristics of the benchmark datasets used for the experiments. All of them have been downloaded from the OpenML public repository~\cite{OpenML2013,OpenMLPython2019}. 


\begin{table}[!htb]
\centering
\caption{Benchmark datasets}
\vskip 0.1in
\label{tab:data-sets}
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{@{}lrrrr@{}}
\toprule
data set & samples & features & classes & NaNs \\ \midrule
micro-mass & 571 & 1,301 & 20 & 0 \\
soybean & 683 & 36 & 19 & 2,337 \\
credit-g & 1,000 & 21 & 2 & 0 \\
kr-vs-kp & 3,196 & 37 & 2 & 0 \\
abalone & 4,177 & 9 & 28 & 0 \\
isolet & 7,797 & 618 & 26 & 0 \\
jm1 & 10,885 & 22 & 2 & 25 \\
gas-drift & 13,910 & 129 & 6 & 0 \\
mozilla4 & 15,545 & 6 & 2 & 0 \\
letter & 20,000 & 17 & 26 & 0 \\
Amazon & 32,769 & 10 & 2 & 0 \\
electricity & 45,312 & 9 & 2 & 0 \\
mnist & 70,000 & 785 & 10 & 0 \\
covertype & 581,012 & 55 & 17 & 0 \\ \bottomrule
\end{tabular}\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}


The proposed approach has been tested over a -fold cross-validation against state-of-the-art coreset discovery algorithms: GIGA~\cite{campbell2018bayesian}, Frank-Wolfe~\cite{clarkson2010coresets}, Matching Pursuit~\cite{Pati1993}, Orthogonal Mathcing Pursuit~\cite{Pati1993}, LAR~\cite{Efron2004,Boutsidis2013}, and Forward Stagewise~\cite{Efroymson1960}.
For each fold, coresets have been extracted and used to train an instance of the \texttt{Ridge}~\cite{Tikhonov1943} classifier. This classifier has been chosen both for its high train speed and its generalization ability in a variety of experimental settings. Figures \ref{fig:exp1} and \ref{fig:exp2} show for each benchmark dataset the quality of extracted coresets as a function of coreset size (lower is better), and classification error (lower is better). The coreset size has been reported both as the absolute number of core samples (in round brackets) and as the ratio
 (number of core samples over the total number of training samples).
The error bars represent the standard error of the mean.
Details about experimental results are shown in the Table \ref{tab:results1} and \ref{tab:results2}. 
The results can be summarized as follows:
\begin{itemize}
    \item Concerning just mean classification error, EvoCore outperformed state-of-the-art techniques on all datasets but two, namely Micro-mass and MNIST;
    \item Considering coreset discovery as a multi-objective problem (minimize classification error and minimize coreset size), EvoCore's coresets are never dominated by the coresets uncovered by other techniques, for all considered datasets. On the contrary, they often dominate a considerable number of other solutions.
    \item In most cases, the solution provided by EvoCore was in a region of the search space far away from the solutions provided by the other techniques, thus revealing compromises unexplored by competitors.
\end{itemize}

When compared to the state-of-the-art in coreset discovery, EvoCore thus proves to be extremely effective. The main drawback of the proposed approach is represented by the longer running time. However, the problem can be mitigated by parallelizing evaluations, as EAs can easily evaluate all individuals in the same generation at the same time, provided that enough computational resources are available.

\begin{figure*}[!htb]
    \centering
    
    \includegraphics[width=0.48\textwidth]{figs/micro-mass_pareto.pdf}
    \includegraphics[width=0.48\textwidth]{figs/soybean_pareto.pdf}\\
    \includegraphics[width=0.48\textwidth]{figs/credit-g_pareto.pdf}
    \includegraphics[width=0.48\textwidth]{figs/kr-vs-kp_pareto.pdf}\\
    \includegraphics[width=0.48\textwidth]{figs/abalone_pareto.pdf}
    \includegraphics[width=0.48\textwidth]{figs/isolet_pareto.pdf}\\    
    \includegraphics[width=0.48\textwidth]{figs/jm1_pareto.pdf}
    \includegraphics[width=0.48\textwidth]{figs/gas-drift_pareto.pdf}
    \includegraphics[width=0.8\textwidth, trim=1.5cm 1cm 1.5cm 2cm, clip=true]{figs/legend.pdf}
    
    
    \caption{The figures show for each benchmark dataset the quality of extracted coresets as a function of coreset size (lower is better), and classification error obtained using an instance of the \texttt{Ridge} classifier (lower is better). The error bars represent the standard error of the mean.}
    \label{fig:exp1}
    
\end{figure*}


\begin{figure*}[!htb]
    \centering
    

    \includegraphics[width=0.48\textwidth]{figs/mozilla4_pareto.pdf}
    \includegraphics[width=0.48\textwidth]{figs/letter_pareto.pdf}\\
    \includegraphics[width=0.48\textwidth]{figs/Amazon_employee_access_pareto.pdf}
    \includegraphics[width=0.48\textwidth]{figs/electricity_pareto.pdf}\\    \includegraphics[width=0.48\textwidth]{figs/mnist_784_pareto.pdf}
    \includegraphics[width=0.48\textwidth]{figs/covertype_pareto.pdf}
    \includegraphics[width=0.8\textwidth, trim=1.5cm 1cm 1.5cm 2cm, clip=true]{figs/legend.pdf}
    
    
    \caption{The figures show for each benchmark dataset the quality of extracted coresets as a function of coreset size (lower is better), and classification error obtained using an instance of the \texttt{Ridge} classifier (lower is better). The error bars represent the standard error of the mean.}
    \label{fig:exp2}
    
\end{figure*}




\begin{table}[!ht]
\centering
\caption{-fold cross-validation results. The mean and the standard error of the mean (s.e.m.) is reported in each column. The result with the best (highest or lowest) mean value for each metric is highlighted in \textbf{bold}.}
\label{tab:results1}
\begin{center}
\begin{small}
\begin{sc}
\resizebox{0.5\textwidth}{!}{\begin{tabular}{@{}lllll@{}}
\toprule
micro-mass & size & test   & train   & fit time (s) \\ 
\midrule
GIGA &  &  &  &  \\
Frank-Wolfe &  &  &  &  \\
Matching Pursuit &  &  &  &  \\
Forward Stagewise &  &  &  &  \\
Ortho Pursuit &  & \bm{} &  &  \\
LAR &  & \bm{} &  &  \\
EvoCore &  &  &  &  \\
\midrule
soybean & size & test   & train   & fit time (s) \\
\midrule
GIGA &  &  &  &  \\
Frank-Wolfe &  &  &  &  \\
Matching Pursuit &  &  &  &  \\
Forward Stagewise &  &  &  &  \\
Ortho Pursuit & \bm{} &  &  &  \\
LAR &  &  &  & \bm{} \\
EvoCore &  & \bm{} &  &  \\
\midrule
credit-g & size & test   & train   & fit time (s) \\ \midrule
GIGA &  &  &  &  \\
Frank-Wolfe &  &  &  &  \\
Matching Pursuit &  &  &  &  \\
Forward Stagewise &  &  &  &  \\
Ortho Pursuit & \bm{} &  &  &  \\
LAR &  &  &  &  \\
EvoCore &  & \bm{} &  &  \\
\midrule
kr-vs-kp & size & test   & train   & fit time (s) \\ 
\midrule
GIGA &  &  &  &  \\
Frank-Wolfe &  &  &  &  \\
Matching Pursuit &  &  &  &  \\
Forward Stagewise &  &  &  &  \\
Ortho Pursuit & \bm{} &  &  & \bm{} \\
LAR &  &  &  &  \\
EvoCore &  & \bm{} &  &  \\
\midrule
abalone & size & test   & train   & fit time (s) \\ 
\midrule
GIGA &  &  &  &  \\
Frank-Wolfe &  &  &  &  \\
Matching Pursuit &  &  &  &  \\
Forward Stagewise &  &  &  &  \\
Ortho Pursuit & \bm{} &  &  &  \\
LAR & \bm{} &  &  &  \\
EvoCore &  & \bm{} &  &  \\
\midrule
isolet & size & test   & train   & fit time (s) \\ 
\midrule
GIGA &  &  &  &  \\
Frank-Wolfe &  &  &  &  \\
Matching Pursuit &  &  &  &  \\
Forward Stagewise &  &  &  &  \\
Ortho Pursuit & \bm{} &  &  &  \\
LAR &  &  &  &  \\
EvoCore &  & \bm{} &  &  \\
\midrule
jm1 & size & test   & train   & fit time (s) \\ 
\midrule
GIGA &  &  &  &  \\
Frank-Wolfe &  &  &  &  \\
Matching Pursuit &  &  &  &  \\
Forward Stagewise &  &  &  &  \\
Ortho Pursuit & \bm{} &  &  & \bm{} \\
LAR &  &  &  &  \\
EvoCore &  & \bm{} &  &  \\
\bottomrule
\end{tabular}}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}


\begin{table}[!ht]
\centering
\caption{-fold cross-validation results. The mean and the standard error of the mean (s.e.m.) is reported in each column. The result with the best (highest or lowest) mean value for each metric is highlighted in \textbf{bold}.}
\label{tab:results2}
\begin{center}
\begin{small}
\begin{sc}
\resizebox{0.55\textwidth}{!}{\begin{tabular}{@{}lllll@{}}
\toprule
gas-drift & size & test   & train   & fit time (s) \\ 
\midrule
GIGA &  &  &  &  \\
Frank-Wolfe &  &  &  &  \\
Matching Pursuit &  &  &  &  \\
Forward Stagewise &  &  &  &  \\
Ortho Pursuit & \bm{} &  &  & \bm{} \\
LAR &  &  &  &  \\
EvoCore &  & \bm{} &  &  \\
\midrule
mozilla4 & size & test   & train   & fit time (s) \\ \midrule
GIGA &  &  &  &  \\
Frank-Wolfe &  &  &  &  \\
Matching Pursuit &  &  &  &  \\
Forward Stagewise &  &  &  &  \\
Ortho Pursuit & \bm{} &  &  & \bm{} \\
LAR &  &  &  &  \\
EvoCore &  & \bm{} &  &  \\
\midrule
letter & size & test   & train   & fit time (s) \\
\midrule
GIGA &  &  &  &  \\
Frank-Wolfe &  &  &  &  \\
Matching Pursuit &  &  &  &  \\
Forward Stagewise &  &  &  &  \\
Ortho Pursuit & \bm{} &  &  & \bm{} \\
LAR &  &  &  &  \\
EvoCore &  & \bm{} &  &  \\
\midrule
amazon-employee-access & size & test   & train   & fit time (s) \\ \midrule
GIGA &  &  &  &  \\
Frank-Wolfe &  &  &  &  \\
Matching Pursuit &  &  &  &  \\
Forward Stagewise &  &  &  &  \\
Ortho Pursuit & \bm{} &  &  & \bm{} \\
LAR &  &  &  &  \\
EvoCore &  & \bm{} &  &  \\
\midrule
electricity & size & test   & train   & fit time (s) \\ \midrule
GIGA &  &  &  &  \\
Frank-Wolfe &  &  &  &  \\
Matching Pursuit &  &  &  &  \\
Forward Stagewise &  &  &  &  \\
Ortho Pursuit & \bm{} &  &  & \bm{} \\
LAR &  &  &  &  \\
EvoCore &  & \bm{} &  &  \\
\midrule
mnist & size & test   & train   & fit time (s) \\ \midrule
GIGA &  &  &  & \bm{} \\
Frank-Wolfe &  & \bm{} &  &  \\
Matching Pursuit &  &  &  &  \\
Forward Stagewise &  &  &  &  \\
Ortho Pursuit &  &  &  &  \\
LAR &  &  &  &  \\
EvoCore & \bm{} &  &  &  \\
\midrule
covertype & size & test   & train   & fit time (s) \\ \midrule
GIGA &  &  &  &  \\
Frank-Wolfe &  &  &  &  \\
Matching Pursuit &  &  &  &  \\
Forward Stagewise &  &  &  &  \\
Ortho Pursuit & \bm{} &  &  &  \\
LAR &  &  &  &  \\
EvoCore &  & \bm{} &  &  \\
\bottomrule
\end{tabular}}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}


\section{Conclusions}
\label{sec:conclusions}
In this work, a novel alternative to coreset discovery is presented. 
Experimental results suggest that the performance of ML classifiers would not be a function of the \textit{size} of the training set, but rather a function of the \textit{mutual position} of the training samples in the feature space. 
Future works will explore the possibility of extending coreset discovery to regression and clustering.

\section*{Software and Data}

Should this paper be accepted, a link to a repository with all the code needed to reproduce the experiments will be provided.

\nocite{langley00}

\bibliography{bibliography}
\bibliographystyle{icml2020}













































\end{document}
