\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{epstopdf}
\usepackage{authblk}

\usepackage{mmstyle}

\newcommand{\dummyfig}[1]{
	\centering
	\fbox{
		\begin{minipage}[c][0.33\textheight][c]{0.4\textwidth}
			\centering{#1}
		\end{minipage}
	}
}

\newcommand{\SSN}{structured segment network}
\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}



\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy 
\def\iccvPaperID{1218} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi
\begin{document}

\title{Temporal Action Detection with Structured Segment Networks}

\author[1]{Yue Zhao}
\author[1]{Yuanjun Xiong}
\author[2]{Limin Wang}
\author[1]{Zhirong Wu}
\author[1]{Xiaoou Tang}
\author[1]{Dahua Lin}


\affil[1]{Department of Information Engineering, The Chinese University of Hong Kong}
\affil[2]{Computer Vision Laboratory, ETH Zurich, Switzerland}

\maketitle



\begin{abstract}
Detecting actions in untrimmed videos is an important yet
challenging task. 
In this paper, we present the structured segment network (SSN), a novel framework
which models the temporal structure of each action instance
via a structured temporal pyramid.
On top of the pyramid, we further introduce a decomposed discriminative model comprising two classifiers, respectively for classifying actions
and determining completeness. This allows the framework to effectively
distinguish positive proposals from background or incomplete ones,
thus leading to both accurate recognition and localization.
These components are integrated into a unified network that
can be efficiently trained in an end-to-end fashion.
Additionally, a simple yet effective
temporal action proposal scheme, dubbed temporal actionness grouping (TAG) is devised to generate high quality action proposals.
On two challenging benchmarks,
THUMOSâ€™14 and ActivityNet, our method remarkably outperforms previous
state-of-the-art methods, demonstrating superior accuracy and strong adaptivity
in handling actions with various temporal structures.
\footnote{Code available at \url{http://yjxiong.me/others/ssn}}
\end{abstract}
 






\section{Introduction}
\label{sec:intro}

Temporal action detection has drawn increasing attention from the research community,
owing to its numerous potential applications in surveillance, video analytics, and other areas~\cite{Oneata2013FV,Mettes2015Bofrag,Yeung2016FrameGlimpse,Shou2016SCNN}.
This task is to detect human action instances from untrimmed, and possibly very long videos.
Compared to action recognition, it is substantially more challenging,
as it is expected to output not only the action category, but also the precise starting and ending time points.

\begin{figure}
	\centering
	\includegraphics[height=.80\linewidth]{figures/teaser_v2}
	\caption{
		\small
		Importance of modeling stage structures in action detection.
		We slide window detectors through a video clip with an action instance of ``Tumbling'' (green box). \textbf{Top}: The detector builds features without any stage structure of the action, \emph{e.g.} average pooling throughout the window. It produces high responses whenever it sees any discriminative snippet related to tumbling, making it hard to localize the instance.
		\textbf{Bottom}: SSN detector utilizes stage structures (starting, course, and ending) via structured temporal pyramid pooling. Its response is only significant when the window is well aligned.}
	\label{fig:teaser_curve}
\end{figure}

Over the past several years, the advances in convolutional neural networks
have led to remarkable progress in video analysis. Notably, the accuracy
of action recognition has been significantly
improved~\cite{Simonyan14TwoStream,Tran15C3D,Fernando15Evolution,WangQT15TDD,Wang2016TSN}.
Yet, the performances of action detection methods remain unsatisfactory~\cite{Yuan2016ScorePyramids,Yeung2016FrameGlimpse,DBLP:journals/corr/SinghC16}.
For existing approaches, one major challenge in precise temporal localization is the large number of incomplete action fragments in the proposed temporal regions. 
Traditional snippet based classifiers rely on discriminative snippets of actions, which would also exist in these incomplete proposals. This makes them very hard to distinguish from valid detections (see Fig.~\ref{fig:teaser_curve}).
We argue that tackling this challenge requires the capability of temporal structure analysis,
or in other words, the ability to identify different stages
\eg~\emph{starting}, \emph{course}, and \emph{ending}, which together decide the \emph{completeness} of an actions instance.


Structural analysis is not new in computer vision.
It has been well studied in various tasks,
\eg~image segmentation~\cite{lafferty2001conditional}, scene understanding~\cite{hoiem2008putting}, and human pose estimation~\cite{andriluka2009pictorial}.
Take the most related object detection for example, in deformable part based models (DPM)~\cite{Felzenszwalb2010DPM},
the modeling of the spatial configurations among parts is crucial.
Even with the strong expressive power of convolutional networks~\cite{Girshick2014RCNN},
explicitly modeling spatial structures, in the form of spatial pyramids~\cite{Lazebnik2006Beyond,He2014SPP}, remains an effective way to achieve improved performance, as demonstrated in a number of
state-of-the-art object detection frameworks, \eg~Fast R-CNN~\cite{Girshick2015FRCNN} and region-based FCN~\cite{Li2016RFCN}.

In the context of video understanding, although temporal structures have played an crucial role in action recognition~\cite{Niebles2010Modeling,Wang2014Latent,Pirsiavash2014SegGrammar,Wang2016TPP}, their modeling in temporal action detection was not as common and successful.
Snippet based methods~\cite{Mettes2015Bofrag,DBLP:journals/corr/SinghC16}
often process individual snippets independently without considering the temporal
structures among them.
Later works attempt to incorporate temporal structures, but are often limited to
analyzing short clips.
S-CNN~\cite{Shou2016SCNN} models the temporal structures via the 3D convolution,
but its capability
is restricted by the underlying architecture~\cite{Tran15C3D}, which is designed to
accommodate only  frames.
The methods based on recurrent networks~\cite{DonahueJ2015LRCN,Montes_2016_NIPSWS}
rely on dense snippet sampling and thus are confronted with serious computational
challenges when modeling long-term structures.
Overall, existing works are limited in two key aspects.
First, the tremendous amount of visual data in videos restricts their capability of modeling long-term dependencies in an end-to-end manner.
Also, they neither provide \emph{explicit} modeling of different
stages in an activity (\eg~\emph{starting} and \emph{ending}) nor offer a mechanism
to assess the \emph{completeness}, which, as mentioned, is crucial for accurate action
detection.


In this work, we aim to move beyond these limitations and develop an effective technique for temporal action detection.
Specifically, we adopt the proven paradigm of ``proposal+classification'', but take
a significant step forward by utilizing explicit structural modeling in the temporal dimension.
In our model, each complete activity instance is considered as a composition of
three major stages, namely \emph{starting}, \emph{course}, and \emph{ending}.
We introduce structured temporal pyramid pooling to
produce a global representation of the entire proposal.
Then we introduce a decomposed discriminative model to jointly classify action categories and determine \emph{completeness} of the proposals, which work collectively to output only complete action instances. 
These components are integrated
into a unified network, called \emph{structured segment network} (SSN).
We adopt the sparse snippet sampling strategy~\cite{Wang2016TSN}, which overcomes
the computational issue for long-term modeling and
enables efficient end-to-end training of SSN.
Additionally, we propose to use multi-scale grouping upon the temporal actionness signal to generate action proposals, achieving higher temporal recall with less proposals to further boost the detection performance.

The proposed SSN framework excels in the following aspects:
1) It provides an effective mechanism to model the temporal structures of activities,
and thus the capability of discriminating between complete and incomplete proposals.
2) It can be efficiently learned in an end-to-end fashion
({ to } hours over a large video dataset, \eg~ActivityNet),
and once trained, can perform fast inference of temporal structures.
3) The method achieves superior detection performance on standard benchmark datasets, establishing new state-of-the-art for temporal action detection. 


\section{Related Work}
\label{related}

\noindent \textbf{Action Recognition.}
Action recognition has been extensively studied in the past few years~\cite{Laptev05STIP,WangS13IDT,Simonyan14TwoStream,Tran15C3D,WangQT15TDD,Wang2016TSN,ZhangWW0W16}.
Earlier methods are mostly based on hand-crafted visual features~\cite{Laptev05STIP,WangS13IDT}.
In the past several years, the wide adoption of convolutional networks (CNNs)
has resulted in remarkable performance gain.
CNNs are first introduced to this task in~\cite{KarpathyCVPR14Sports1M}.
Later, two-stream architectures~\cite{Simonyan14TwoStream} and 3D-CNN~\cite{Tran15C3D}
are proposed to incorporate both appearance and motion features.
These methods are primarily frame-based and snippet-based, with simple schemes to aggregate results.
There are also efforts that explore long-range temporal structures via temporal pooling or RNNs~\cite{WangQT15TDD,Ng15BeyondSnippet,DonahueJ2015LRCN}.
However, most methods assume well-trimmed videos, where
the action of interest lasts for nearly the entire duration.
Hence, they don't need to consider the issue of localizing the action instances.

\begin{figure*}[t]
	\centering
	\includegraphics[width=0.96\linewidth]{figures/overview_v2.pdf}
	\caption{\small
		An overview of the structured segment network framework. On a video from ActivityNet~\cite{caba2015activitynet} there is a candidate region (green box). We first build the augmented proposal (yellow box) by extending it.
		The augmented proposal is divided into starting (orange), course (green), and ending (blue) stages. An additional level of pyramid with two sub-parts is constructed on the course stage. Features from CNNs are pooled within these five parts and concatenated to form the global region representations.
		The activity classifier and the completeness classifier operate on the the region representations to produce activity probability and class conditional completeness probability.
		The final probability of the proposal being positive instance is decided by the joint probability from these two classifiers.
		During training, we sparsely sample  snippets from evenly divided segments to approximate the dense temporal pyramid pooling.}
	\label{fig:overview}
\end{figure*}

\noindent \textbf{Object Detection.}
Our action detection framework is closely related to object detection frameworks~\cite{Felzenszwalb2010DPM,Girshick2014RCNN,Ren2015FasterRCNN} in spatial images, where detection is performed by classifying object proposals into foreground classes and a background class.
Traditional object proposal methods rely on dense sliding windows~\cite{Felzenszwalb2010DPM} and bottom-up methods that exploit low-level boundary cues~\cite{Van2011SS,Dollar2014Edgebox}.
Recent proposal methods based on deep neural networks show better average recall while requiring less candidates~\cite{Ren2015FasterRCNN}.
Deep models also introduce great modeling capacity for capturing object appearances.
With strong visual features, spatial structural modeling~\cite{Lazebnik2006Beyond} remains a key component for detection.
In particular, the RoI pooling~\cite{Girshick2015FRCNN} is introduced to model the spatial configuration of object with minimal extra cost.
The idea is further reflected in R-FCN~\cite{Li2016RFCN} where the spatial configuration is handled with the position sensitive pooling.

\noindent \textbf{Temporal Action Detection.}
Previous works on activity detection mainly use sliding windows as candidates
and focus on designing hand-crafted feature representations for classification~\cite{Gaidon2013Actom,Tang2013RightFeature,Oneata2013FV,Mettes2015Bofrag,Yuan2016ScorePyramids,JainCVPR14Tubelet}.
Recent works incorporate deep networks into the detection frameworks and obtain improved performance~\cite{Yeung2016FrameGlimpse,Shou2016SCNN,de2016online}.
S-CNN~\cite{Shou2016SCNN} proposes a multi-stage CNN which boosts accuracy via a localization network.
However, S-CNN relies on C3D~\cite{Tran15C3D} as the feature extractor,
which is initially designed for snippet-wise action classification.
Extending it to detection with possibly long action proposals needs
enforcing an undesired large temporal kernel stride.
Another work~\cite{Yeung2016FrameGlimpse} uses Recurrent Neural Network (RNN) to learn a glimpse policy for predicting the starting and ending points of an action.
Such sequential prediction is often time-consuming for processing long videos and
it does not support joint training of the underlying feature extraction CNN.
Our method differs from these approaches in that
it explicitly models the action structure via structural temporal pyramid pooling.
By using sparse sampling, we further enable efficient end-to-end training.
Note there are also works on spatial-temporal detection~\cite{Gkioxari2015Tubes,Weinzaepfel2015Track,mettes2016spot,Wang2016Actionness,Peng2016ActionFRCNN} and
temporal video segmentation~\cite{Hoai2011Joint},
which are beyond the scope of this paper.
 


\section{Structured Segment Network}
\label{sec:overview}

The proposed \SSN~framework, as shown in Figure~\ref{fig:overview},
takes as input a video and a set of temporal action proposals.
It outputs a set of predicted \emph{activity instances}
each associated with a category label and a temporal range (delimited
by a starting point and an ending point).
From the input to the output, it takes three key steps.
First, the framework relies on a proposal method to produce a set of \emph{temporal proposals}
of varying durations, where each proposal comes with a starting and an ending time.
The proposal methods will be discussed in detail in Section~\ref{sec:proposal}.
Our framework considers each proposal as a composition of
three consecutive stages, \emph{starting}, \emph{course}, and \emph{ending},
which respectively capture how the action starts, proceeds, and ends.
Thus upon each proposal, structured temporal pyramid pooling (STPP) are performed by 1) splitting the proposal into the three stages; 2) building temporal pyramidal representation for each stage; 3) building global representation for the whole proposal by concatenating stage-level representations.
Finally, two classifiers respectively for recognizing the activity category
and assessing the completeness will be applied on the representation obtained by STPP and their predictions
will be combined, resulting in a subset of \emph{complete} instances tagged with category labels.
Other proposals, which are considered as either \emph{belonging to background} or
\emph{incomplete}, will be filtered out.
All the components outlined above are integrated into a unified network,
which will be trained in an end-to-end way.
For training, we adopt the sparse snippet sampling strategy~\cite{Wang2016TSN}
to approximate the temporal pyramid on dense samples.
By exploiting the redundancy among video snippets, this strategy can substantially
reduce the computational cost, thus allowing the crucial modeling of long-term temporal
structures.

\vspace{-5pt}
\subsection{Three-Stage Structures}

At the input level, a video can be represented as a sequence of  \emph{snippets},
denoted as .
Here, one snippet contains several consecutive frames, which, as a whole, is
characterized by a combination of RGB images and an optical flow stack~\cite{Simonyan14TwoStream}.
Consider a given set of  proposals .
Each proposal  is composed of a starting time  and an ending time .
The duration of  is thus .
To allow structural analysis and particularly to determine whether a proposal
captures a \emph{complete} instance, we need to put it in a context.
Hence, we augment each proposal   into
 with
where  and .
In other words, the augmented proposal  doubles the span of  by
extending beyond the starting and ending points, respectively by .
If a proposal accurately aligns well with a groundtruth instance, the augmented proposal
will capture not only the inherent process of the activity, but also how it starts and ends.
Following the three-stage notion, we divide the augmented proposal  into three
consecutive intervals:
, , and , which
are respectively corresponding to the \emph{starting}, \emph{course}, and \emph{ending} stages.


\subsection{Structured Temporal Pyramid Pooling}
\label{sec:stpp}

As mentioned, the \SSN~framework derives a global representation for
each proposal via temporal pyramid pooling.
This design is inspired by the success of spatial pyramid
pooling~\cite{Lazebnik2006Beyond,He2014SPP} in object recognition
and scene classification.
Specifically, given an augmented proposal  divided into three
stages , , and , we first compute the stage-wise
feature vectors , , and  respectively
via temporal pyramid pooling, and then concatenate them into
a global representation.

Specifically, a stage with interval  would cover a series
of snippets, denoted as . For each snippet,
we can obtain a feature vector .
Note that we can use any feature extractor here. In this work, we
adopt the effective two-stream feature representation first proposed in~\cite{Simonyan14TwoStream}.
Based on these features, we construct a -level temporal pyramid where each level evenly divides the interval into  parts.
For the -th part of the -th level, whose interval is ,
we can derive a pooled feature as

Then the overall representation of this stage can be obtained by
concatenating the pooled features across all parts at all levels
as
.

We treat the three stages differently.
Generally, we observed that the \emph{course} stage, which reflects
the activity process itself, usually contains richer structure
\eg~this process itself may contain sub-stages.
Hence, we use a two-level pyramid, \ie~, and , for the \emph{course}
stage, while using simpler one-level pyramids (which essentially
reduce to standard average pooling) for \emph{starting} and \emph{ending}
pyramids.
We found empirically that this setting strikes a good balance
between expressive power and complexity.
Finally, the stage-wise features are combined via concatenation.
Overall, this construction explicitly leverages the structure
of an activity instance and its surrounding context, and thus
we call it \emph{structured temporal pyramid pooling} (STPP).

\subsection{Activity and Completeness Classifiers}

On top of the structured features described above,
we introduce two types of classifiers, an \emph{activity classifier}
and a set of \emph{completeness classifiers}.
Specifically,
the \emph{activity classifier}  classifies input proposals
into  classes, \ie~ activity classes (with labels )
and an additional \emph{``background''} class (with label ).
This classifier restricts its scope to the \emph{course} stage,
making predictions based on the corresponding feature .
The \emph{completeness classifiers}  are a set of
binary classifiers, each for one activity class.
Particularly,  predicts whether a proposal captures a \emph{complete}
activity instance of class , based on the global representation  induced by STPP. In this way, the \emph{completeness}
is determined not only on the proposal itself but also on its
surrounding context.

Both types of classifiers are implemented as linear classifiers
on top of high-level features. Given a proposal ,
the activity classifier will produce
a vector of normalized responses via a softmax layer.
From a probabilistic view, it can be considered as a conditional
distribution , where  is the class label.
For each activity class , the corresponding completeness
classifier  will yield a probability value, which can
be understood as the conditional probability ,
where  indicates whether  is \emph{complete}.
Both outputs together form a joint distribution. When ,

Hence, we can define a \emph{unified classification loss}
jointly on both types of classifiers. With a proposal 
and its label :

Here, the \emph{completeness} term  is only used when
, \ie~the proposal  is not considered as part of the background.
Note that these classifiers together with STPP
are integrated into a single network that is trained in an \emph{end-to-end} way.

During training, we collect three types of proposal samples:
(1) \emph{positive proposals}, \ie~those overlap
with the closest groundtruth instances with at least  IoU;
(2) \emph{background proposals}, \ie~those that do not overlap
with any groundtruth instances; and
(3) \emph{incomplete proposals},
\ie~those that satisfy the following criteria:  of its own span is
contained in a groundtruth instance, while its IoU with that instance is
below  (in other words, it just covers a small part of the instance).
For these proposal types, we respectively have
, , and .
Each mini-batch is ensured to contain all three types of proposals.

\subsection{Location Regression and Multi-Task Loss}

With the structured information encoded in the global features,
we can not only make categorical predictions, but also refine the
proposal's temporal interval itself by \emph{location regression}.
We devise a set of location regressors ,
each for an activity class.
We follow the design in RCNN~\cite{Girshick2014RCNN}, but adapting
it for 1D temporal regions.
Particularly, for a \emph{positive proposal} , we regress
the relative changes of both the interval center  and the span  (in log-scale),
using the closest groundtruth instance as the target.
With both the classifiers and location regressors,
we define a multi-task loss over an training sample , as:

Here,  uses the smooth  loss function~\cite{Girshick2015FRCNN}.
 


\section{Efficient Training and Inference with SSN}

The huge amount of frames poses a serious challenge in computational cost
to video analysis. Our \SSN~also faces this challenge.
This section presents two techniques which we use to reduce
the cost and enable end-to-end training.

\vspace{-12pt}
\paragraph{Training with sparse sampling.}
The structured temporal pyramid, in its original form, rely on
densely sampled snippets. This would lead to excessive computational cost
and memory demand in end-to-end training over long proposals -- in practice,
proposals that span over hundreds of frames are not uncommon.
However, dense sampling is generally unnecessary in our framework.
Particularly, the \emph{pooling} operation is essentially to collect
feature statistics over a certain region. Such statistics can be well
approximated via a subset of snippets, due to the high redundancy
among them.

Motivated by this, we devise a \emph{sparse snippet sampling scheme}.
Specifically, given a augmented proposal ,
we evenly divide it into  segments,
randomly sampling only one snippet from each segment.
Structured temporal pyramid pooling is performed for each pooling region on its corresponding segments.
This scheme is inspired by the segmental architecture in~\cite{Wang2016TSN},
but differs in that it operates within STPP instead of a global average pooling.
In this way, we fix the number of features needed to be computed regardless
of how long the proposal is, thus effectively reducing the computational cost,
especially for modeling long-term structures.
More importantly, this enables end-to-end training of the entire framework
over a large number of long proposals.

\vspace{-12pt}
\paragraph{Inference with reordered computation.}
In testing, we sample video snippets with a fixed interval of  frames,
and construct the temporal pyramid thereon.
The original formulation of temporal pyramid first computes pooled features
and then applies the classifiers and regressors on top which is not efficient.
Actually, for each video, hundreds of proposals will be generated,
and these proposals can significantly overlap with each
other -- therefore, a considerable portion of the snippets and
the features derived thereon are shared among proposals.

To exploit this redundancy in the computation,
we adopt the idea introduced in position sensitive pooling~\cite{Li2016RFCN}
to improve testing efficiency.
Note that our classifiers and regressors are both linear.
So the key step in classification or regression is to multiply
a weight matrix  with the global feature vector .
Recall that  itself is a concatenation of multiple features,
each pooled over a certain interval. Hence the computation
can be written as ,
where  indexes different regions along the pyramid.
Here,  is obtained by \emph{average pooling} over all snippet-wise features
within the region . Thus, we have

 denotes the average pooling over ,
which is a linear operation and therefore can be exchanged with
the matrix multiplication.
Eq~\eqref{eq:reorder} suggests that
the linear responses \wrt~the classifiers/regressors can be computed
\emph{before} pooling.
In this way, the heavy matrix multiplication can be done in the CNN for
each video over all snippets, and for each proposal, we only
have to pool over the network outputs.
This technique can reduce the processing time after extracting network outputs from
around 10 seconds to less than 0.5 second per video on average.
 


\section{Temporal Region Proposals}
\label{sec:proposal}

\begin{figure}[t]
	\centering
	\includegraphics[width=.9\linewidth]{figures/tag_proposal_v2}
	\caption{\small
		Visualization of the temporal actionness grouping process for proposal generation.
		\textbf{Top}: Actionness probabilities as a 1D signal sequence.
		\textbf{Middle}: The complement signal. We flood it with different levels .
		\textbf{Bottom}: Regions obtained by different flooding levels.
		By merging the regions according to the grouping criterion, we
		get the final set of proposals (in orange color).
	}
	\label{fig:tag_proposal}
	\vspace{-10pt}
\end{figure}

In general, SSN accepts arbitrary proposals,
\eg~sliding windows~\cite{Shou2016SCNN,Yuan2016ScorePyramids}.
Yet, an effective proposal method can produce more accurate proposals,
and thus allowing a small number of proposals to reach a certain level
of performance. In this work, we devise an effective
proposal method called \emph{temporal actionness grouping (TAG)}.

This method uses an actionness classifier
to evaluate the binary \emph{actionness probabilities} for individual snippets.
The use of binary actionness for proposals is first introduced in spatial action detection by~\cite{Wang2016Actionness}.
Here we utilize it for temporal action detection.

Our basic idea is to find those continuous temporal regions with mostly
high actionness snippets to serve as proposals.
To this end, we repurpose a classic watershed algorithm~\cite{Roerdink2000Watershed},
applying it to the 1D signal formed by a sequence of \emph{complemented}
actionness values, as shown in Figure~\ref{fig:tag_proposal}.
Imagine the signal as 1D terrain with heights and basins. This algorithm floods water on this terrain with different \emph{``water level''}
, resulting in a set of \emph{``basins''} covered by water, denoted by .
Intuitively, each ``basin'' corresponds to a temporal region with high actionness.
The ridges above water then form the blank areas between basins, as illustrated in  Fig.~\ref{fig:tag_proposal}.

Given a set of basins , we devise a grouping scheme
similar to~\cite{PT2015MCG}, which tries to connect small basins into
proposal regions. The scheme works as follows: it begins with a
seed basin, and consecutively absorbs the basins that follow,
until the fraction of the basin durations over the total duration
(\ie~from the beginning of the first basin to the ending of the last)
drops below a certain threshold . The absorbed basins and the blank spaces between them are then grouped to form a single proposal. 
We treat each basin as seed and perform the grouping procedure to obtain a set of proposals denoted by .
Note that we do not choose a specific combination of  and .
Instead we uniformly sample  and  from   with an even step of .
The combination of these two thresholds leads to multiple sets of regions.
We then take the \emph{union} of them.
Finally, we apply non-maximal suppression to the union with IoU threshold ,
to filter out highly overlapped proposals. The retained proposals will be fed to
the SSN framework.

 


\section{Experimental Results}
\label{sec:experiment}

We conducted experiments to test the proposed framework
on two large-scale action detection benchmark datasets:
\emph{ActivityNet}~\cite{caba2015activitynet} and
\emph{THUMOS14}~\cite{Jiang2014THUMOS14}.
In this section we first introduce these datasets and other experimental settings and then investigate the impact of different components
via a set of ablation studies.
Finally we compare the performance of SSN with other state-of-the-art approaches.

\subsection{Experimental Settings}

\paragraph{Datasets.}
\textbf{ActivityNet}~\cite{caba2015activitynet} has two versions, \emph{v1.2} and \emph{v1.3}.
The former contains  videos in  classes, while the latter, which is a superset of v1.2 and
was used in the ActivityNet Challenge 2016, contains  videos in  classes.
In each version, the dataset is divided into three disjoint subsets,
training, validation, and testing, by ::.
\textbf{THUMOS14}~\cite{Jiang2014THUMOS14} has  videos for validation and  videos for testing.
This dataset does not provide the training set by itself.
Instead, the UCF101~\cite{Soomro2012Ucf101}, a trimmed video dataset is appointed as the official training set.
Following the standard practice, we train out models on the validation set and evaluate them
on the testing set.
On these two sets,  and  videos have temporal annotations in  classes, respectively.  falsely annotated videos (``270'',``1496'') in the test set are excluded in evaluation.
In our experiments,
we compare with our method with the states of the art on both
\emph{THUMOS14} and \emph{ActivityNet v1.3},
and perform ablation studies on \emph{ActivityNet v1.2}.

\vspace{-12pt}
\paragraph{Implementation Details.}
We train the \SSN~in an end-to-end manner,
with raw video frames and action proposals as the input.
Two-stream CNNs~\cite{Simonyan14TwoStream} are used for feature extraction.
We also use the spatial and temporal streams to harness both the appearance and motion features.
The binary actionness classifiers underlying the TAG proposals are trained with~\cite{Wang2016TSN} on the training subset of each dataset.
We use SGD to learn CNN parameters in our framework, with batch size  and momentum .
We initialize the CNNs with pre-trained models from ImageNet~\cite{Deng2009ImageNet}.
The initial learning rates are set to  for RGB networks and  for optical flow networks.
In each minibatch, we keep the ratio of three types of proposals, namely
\emph{positive}, \emph{background}, and \emph{incomplete}, to be ::.
For the completeness classifiers, only the samples with loss values ranked in the first  of a minibatch are used for calculating gradients, which resembles online hard negative mining~\cite{Shrivastava2016OHNM}.
On both versions of ActivityNet, the RGB and optical flow branches of the two-stream CNN
are respectively trained for  and  iterations,
with learning rates scaled down by  after every  and  iterations, respectively.
On THUMOS14, these two branches are respectively trained for  and  iterations,
with learning rates scaled down by  per  and  iterations.

\vspace{-12pt}
\paragraph{Evaluation Metrics.}
As both datasets originate from contests,
each dataset has its own convention of reporting performance metrics.
We follow their conventions, reporting mean average precision (mAP) at different IoU thresholds.
On both versions of ActivityNet, the IoU thresholds are .
The average of mAP values with IoU thresholds :: is used to compare the performance between different methods.
On THUMOS14, the IoU thresholds are .
The mAP at  IoU is used for comparing results from different methods.


\subsection{Ablation Studies}

\begin{figure}[t]
	\begin{center}
		\includegraphics[width=0.7\linewidth]{figures/recall_curves.pdf}
	\end{center}
	\caption{\small
		Recall rate at different tIoU thresholds on ActivityNet v1.2.
		High recall rates at high IoU  thresholds () indicate better proposal quality.}
	\label{fig:recall_iou}
\vspace{-5pt}
\end{figure}

\begin{table}[t]
\begin{center}
		\begin{tabular}{c|c|c|c|c}
			\hline
			\multirow{2}{*}{Proposal Method}& \multicolumn{2}{|c|}{\textbf{THUMOS14}}  & \multicolumn{2}{|c}{\textbf{ActivityNet v1.2}} \\
			\cline{2-5}
			& \# Prop. & AR & \# Prop. & AR \\
			\hline
			Sliding Windows&  204  & 21.2 & 100 & 34.8\\
			\hline
			SCNN-prop~\cite{Shou2016SCNN} & 200 & 20.0 & - & - \\
			\hline
			TAP~\cite{caba2016cvpr}& 200  & 23.0  & 90  & 14.9 \\
			\hline
			DAP~\cite{Escorcia2016DAP}& 200 & 37.0 &100 & 12.1  \\ 
			\hline\hline
			TAG & 200 & {\bf 48.9} & 100 & {\bf 71.7} \\
			\hline
		\end{tabular}
\end{center}
\caption{\small
	Comparison between different temporal action proposal methods with same number of proposals.
	``AR'' refers to the average recall rates.
	``-'' indicates the result is not available.}
\label{table:proposals}
\vspace{-12pt}
\end{table}

\paragraph{Temporal Action Proposal.}

We compare the performance of different action proposal schemes in three aspects, \emph{i.e.} recall, quality, and detection performance.
Particularly, we compare our TAG scheme with common sliding windows as well as other state-of-the-art proposal methods, including
SCNN-prop, a proposal networks presented in~\cite{Shou2016SCNN},
TAP~\cite{caba2016cvpr}, DAP~\cite{Escorcia2016DAP}.
For the sliding window scheme, we use  exponential scales starting from  second long
and step sizes of  times of window lengths.

We first evaluate the average recall rates, which are summarized in Table~\ref{table:proposals}.
We can see that TAG proposal have higher recall rates with the same number of proposals.
Then we investigate the quality of its proposals.
We plot the recall rates from different proposal methods at different IoU thresholds in Fig.~\ref{fig:recall_iou}.
We can see TAG retains relatively high recall at high IoU thresholds,
demonstrating that the proposals from TAG are generally more accurate.
In experiments we also tried applying the actionness classifier trained on ActivityNet v1.2 directly on THUMOS14.
We can still achieve a reasonable average recall of , while the one trained on THUMOS14 achieves  in Table~\ref{table:proposals}.
Finally, we evaluate the proposal methods in the context of action detection.
The detection mAP values using sliding window proposals and TAG proposals are shown in Table~\ref{table:component}.
The results confirm that, in most cases, the improved proposals can result in improved detection performance.

\vspace{-12pt}
\paragraph{Structured Temporal Pyramid Pooling.}~\label{sec:stpp_tpp}
Here we study the influence of different pooling strategies in STPP.
We denote one pooling configuration as , where  refers to the number of pyramid levels for the course stage and  the number of regions in each level.  indicates we use augmented proposal and model the starting and ending stage, while  indicates we only use the original proposal (without augmentation).
Additionally we compare two within-region pooling methods: average and max pooling.
The results are summarized in Table~\ref{table:pooling}.
Note that these configurations are evaluated in the stage-wise training scenario.
We observe that cases where  have inferior performance, showing that the introduction of the stage structure is very important for accurate detection.
Also, increasing the depth of the pyramids for the course stage can give slight performance gain.
Based on these results, we fix the configuration to  in later experiments.

\begin{table}[t]
\begin{center}
	\begin{tabular}{c|c|c|c|c}
		\hline
		Average mAP (\%) & (1)-0 & (1,2)-0 &(1)-1 &(1,2)-1 \\ \hline
		Max Pool         & 13.1   & 13.5     & 18.3 &18.4     \\ \hline
		Average Pool     & 4.48    & 4.34      & 24.3 &24.6    \\ \hline
	\end{tabular}
\end{center}
	\caption{\small
		Comparison between different temporal pooling settings.
		The setting  (1,2)-1 is used in the SSN framework. Please refer to Sec.~\ref{sec:stpp_tpp} for the definition of these settings.}
	\label{table:pooling}
	\vspace{-12pt}
\end{table}


\vspace{-12pt}
\paragraph{Classifier Design.}
In this work, we introduced the activity and completeness classifiers which work together to classify the proposal.
We verify the importance of this decomposed design by studying another design
that replaces it with a single set of classifiers, for which
both \emph{background} and \emph{incomplete} samples are uniformly treated as negative. We perform similar negative sample mining for this setting.
The results are summarized in Table~\ref{table:component}.
We observe that using only one classifier to distinguish \emph{positive} samples
from both \emph{background} and \emph{incomplete} would lead to 
worse result even with negative mining, where mAP decreased from  to .
We attribute this performance gain to the different natures of the two negative proposal types, which require different classifiers to handle.

\vspace{-12pt}
\paragraph{Location Regression \& Multi-Task Learning.}
Because of the contextual information contained in the starting and ending stages of the global region features, we are able to perform location regression.
We measure the contribution of this step to the detection performance in Table~\ref{table:component}.
From the results we can see that the location regression and multi-task learning, where we train the classifiers and the regressors together in an end-to-end manner, always improve the detection accuracy.

\vspace{-12pt}
\paragraph{Training: Stage-wise v.s. End-to-end.}
While the \SSN~is designed for end-to-end training, it is also possible to first densely extract features and train the classifiers and regressors with SVM and ridge regression, respectively.
We refer to this training scheme as stage-wise training.
We compare the performance of end-to-end training and stage-wise training in Table~\ref{table:component}.
We observe that models from end-to-end training can slightly outperform those learned with stage-wise training under the same settings.
This is remarkable as we are only sparsely sampling snippets in end-to-end training, which also demonstrates the importance of jointly optimizing the classifiers and feature extractors and justifies our framework design. 
Besides, end-to-end training has another major advantage that it does not need to store the extracted features for the training set, which could become quite storage intensive as training data grows.

\begin{table}[t]
\begin{center}
\setlength{\tabcolsep}{4.8pt}Â¬
\begin{tabular}{c|cccc|cc}
	\hline
	\multicolumn{1}{c|}{} & \multicolumn{4}{c|}{Stage-Wise}            & \multicolumn{2}{c}{End-to-End} \\ \hline
	STPP                  &     & \checkmark & \checkmark & \checkmark & \checkmark     & \checkmark    \\
	Act. + Comp.    &     &            & \checkmark & \checkmark & \checkmark     & \checkmark    \\
	Loc. Reg.   &     &            &            & \checkmark &                & \checkmark    \\ \hline
	SW       & 0.56 &  5.99  & 16.4         &    18.1      & -             &    -         \\ \hline
	TAG                   & 4.82 & 17.9         & 24.6         & 24.9         & 24.8             & 25.9            \\ \hline
\end{tabular}
\end{center}
\caption{\small
	Ablation study on ActivityNet~\cite{caba2015activitynet} v1.2. 
	Overall, end-to-end training is compared against stage wise training.
	We evaluate the performance using both sliding window proposals (``SW'') and TAG proposals (``TAG''), measured by mean average precision (mAP).
	Here, ``STPP'' refers to structure temporal pyramid pooling.
	``Act. + Comp.'' refers to the use of two classifiers design. 
	``Loc. Reg'' denotes the use the location regression.}
\label{table:component}
\vspace{-12pt}
\end{table}

\subsection{Comparison with the State of the Art}

Finally, we compare our method with other state-of-the-art temporal action detection methods on THUMOS14~\cite{Jiang2014THUMOS14} and ActivityNet v1.3~\cite{caba2015activitynet},
and report the performances using the metrics described above.
Note that the average action duration in THUMOS14 and ActivityNet are  and  seconds.
And the average video duration are  and  seconds, respectively.
This reflects the distinct natures of these datasets
in terms of the granularities and temporal structures of the action instances.
Hence, strong adaptivity is required to perform consistently well on both datasets.

\vspace{-12pt}
\paragraph{THUMOS14.}
On THUMOS 14,
We compare with the contest results~\cite{wang2014action,oneata2014lear,Richard2016Language}
and those from recent works, including the methods that use segment-based 3D CNN~\cite{Shou2016SCNN},
score pyramids~\cite{Yuan2016ScorePyramids}, and recurrent reinforcement learning~\cite{Yeung2016FrameGlimpse}.
The results are shown in Table~\ref{table: thumos14}.
In most cases, the proposed method outperforms previous state-of-the-art methods by over  in absolute mAP values.


\begin{table}[t]
	\begin{center}
		\begin{tabular}{c|ccccc}
			\hline
			\multicolumn{6}{c}{\textbf{THUMOS14}, \textbf{mAP@}}                 \\ \hline
			Method& 0.1  & 0.2  & 0.3  & 0.4  & 0.5  \\ \hline
			Wang \emph{et. al.}~\cite{wang2014action} & 18.2 & 17.0 & 14.0 & 11.7 & 8.3 \\ \hline
			Oneata \emph{et. al.}~\cite{oneata2014lear} & 36.6 & 33.6 & 27.0 &  20.8 & 14.4 \\ \hline
			Richard \emph{et. al.}~\cite{Richard2016Language} & 39.7 & 35.7 & 30.0 & 23.2 & 15.2 \\ \hline\hline
			S-CNN~\cite{Shou2016SCNN} & 47.7 & 43.5 & 36.3 & 28.7 & 19.0 \\ \hline
			Yeung \emph{et. al.}~\cite{Yeung2016FrameGlimpse} & 48.9 & 44.0 & 36.0 & 26.4 & 17.1 \\ \hline
			Yuan \emph{et. al.}~\cite{Yuan2016ScorePyramids} & 51.4 & 42.6& 33.6&26.1&18.8 \\\hline\hline
			SSN & \textbf{60.3} & \textbf{56.2} & \textbf{50.6} & \textbf{40.8} & \textbf{29.1} \\
			\hline
			SSN* & \textbf{66.0} & \textbf{59.4} & \textbf{51.9} & \textbf{41.0} & \textbf{29.8} \\
			\hline
		\end{tabular}
	\end{center}
	\caption{\small
		Action detection results on THUMOSâ€™14, measured by mAP at different IoU thresholds . The upper half of the table shows challenge results back in 2014. ``SSN*'' indicates metrics calculated in the PASCAL-VOC style used by ActivityNet~\cite{caba2015activitynet}.
	}
	\label{table: thumos14}
	\vspace{-5pt}
\end{table}

\begin{table}[t]
	\begin{center}
		\begin{tabular}{c|ccc|c}
			\hline
			\multicolumn{5}{c}{\textbf{ActivityNet v1.3} (testing),  \textbf{mAP@}}   \\ \hline
			Method & 0.5 & 0.75 & 0.95 &Average\\ \hline
			Wang \emph{et. al.}~\cite{UTS}  & 42.48 & 2.88 & 0.06 & 14.62 \\ \hline
			Singh \emph{et. al.}~\cite{Singh_2016_CVPR} & 28.67 & 17.78 & 2.88 & 17.68 \\ \hline
			Singh \emph{et. al.}~\cite{DBLP:journals/corr/SinghC16}& 36.40 & 11.05 & 0.14 & 17.83 \\ \hline\hline
			SSN & {\bf 43.26} & {\bf 28.70} & {\bf 5.63} &   {\bf 28.28}     \\ \hline
		\end{tabular}
	\end{center}
	\caption{\small
		Action detection results on ActivityNet v1.3, measured by mean average precision (mAP) for different IoU thresholds  and the average mAP of IoU thresholds from  to .
	}
	\label{table:anet_v1.3}
\vspace{-12pt}
\end{table}


\vspace{-12pt}
\paragraph{ActivityNet.}
The results on the testing set of ActivityNet v1.3 are shown in Table~\ref{table:anet_v1.3}.
For references, we list the performances of highest ranked entries in the ActivityNet 2016 challenge.
We submit our results to the test server of ActivityNet v1.3 and report the detection performance on the testing set.
The proposed framework, using a single model instead of an ensemble, is able to achieve an average mAP of  and perform well at high IOU thresholds, \emph{i.e.},  and .
This clearly demonstrates the superiority of our method.
Visualization of the detection results can be found in the supplementary materials~\cite{Supplement}.
 
\vspace{-5pt}
\section{Conclusion}
\label{sec:conclusion}
\vspace{-5pt}
In this paper, we presented a generic framework for temporal action detection,
which combines a structured temporal pyramid with two types of classifiers,
respectively for predicting activity class and completeness.
With this framework, we achieved significant performance gain over state-of-the-art
methods on both ActivityNet and THUMOS14.
Moreover, we demonstrated that our method is both accurate and generic,
being able to localize temporal boundaries precisely and working well
for activity classes with very different temporal structures.
\vspace{-15pt}
\paragraph{Acknowledgment.} This work is partially supported by the Big Data Collaboration Research grant from SenseTime Group (CUHK Agreement No. TS1610626), the General Research Fund (GRF) of Hong Kong (No. 14236516) and the Early Career Scheme (ECS) of Hong Kong (No. 24204215).
 

{\small
	\bibliographystyle{ieee}
	\bibliography{action_detection}
}

\end{document}
