\documentclass[11pt,a4paper]{article}
\usepackage{csquotes}
\usepackage[hyperref]{naaclhlt2018}
\usepackage{times}
\usepackage{microtype}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{verbatim}
\usepackage{soul}
\usepackage{url}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{paralist}
\usepackage{gb4e}\noautomath
\renewcommand{\sectionautorefname}{\S}
\renewcommand{\subsectionautorefname}{\S}

\aclfinalcopy \def\aclpaperid{420} 

\usepackage{booktabs}
\DeclareMathOperator*{\argmax}{argmax}
\usepackage{array}

\title{CliCR: A Dataset of Clinical Case Reports for Machine\\ Reading Comprehension\thanks{We provide the information about accessing the dataset, as well as the code for the experiments, at \url{http://github.com/clips/clicr}.}}

\author{Simon \v{S}uster \textnormal{and} Walter Daelemans\\
  Computational Linguistics \& Psycholinguistics Research Center,\\ University of Antwerp, Belgium \\
  {\tt \{simon.suster,walter.daelemans\}@uantwerpen.be}}
\date{}
\begin{document}
\maketitle
\begin{abstract}
We present a new dataset for machine comprehension in the medical domain. Our dataset uses clinical case reports with around 100,000 gap-filling queries about these cases. We apply several baselines and state-of-the-art neural readers to the dataset, and observe a considerable gap in performance (20\% F1) between the best human and machine readers. We analyze the skills required for successful answering and show how reader performance varies depending on the applicable skills. We find that inferences using domain knowledge and object tracking are the most frequently required skills, and that recognizing omitted information and spatio-temporal reasoning are the most difficult for the machines.

\end{abstract}

\section{Introduction}
Machine comprehension is a task in which a system reads a text passage and then answers questions about it. The progress in machine comprehension heavily depends on the introduction of new datasets \citep{Burges2013}, which encourages the development of new algorithms and deepens our understanding of the (linguistic) challenges that can or can not be tackled well by these algorithms.  Recently, a number of reading comprehension datasets have been proposed (\autoref{sec:related}), differing in various aspects such as mode of construction, answer-query formulation and required understanding skills. Most are open-domain datasets built from news, fiction and Wikipedia texts. For specialized domains, however, large machine comprehension datasets are extremely scarce \citep{WelblEtAl2017}, and the required comprehension skills poorly understood. With our work we hope to narrow this gap by proposing a new resource for reading comprehension in the clinical domain, and by analyzing the different types of comprehension skills that are triggered while answering \citep{SugawaraEtAl2017a,LaiEtAl2017}.

\begin{figure}
\small
\begin{tabular}{p{1.01\columnwidth}}
\textbf{passage:}\newline
[\ldots] A gradual improvement in clinical and laboratory status was achieved within 20 days of antituberculous treatment . The patient was then subjected to a thoracic CT scan that also showed significant radiological improvement . \textit{Thereafter , tapering of corticosteroids was initiated with no clinical relapse .} The patient was discharged after being treated for a total of 30 days and continued receiving antituberculous therapy with no reported problems for a total of 6 months under the supervision of his hometown physicians . [\ldots]\\
\textbf{query:}\newline
If steroids are used , great caution should be exercised on their gradual tapering to avoid \underline{\hspace{1cm}} .\\
\textbf{answer:}\newline 
relapse  (sem\_type=problem, cui=C0035020)\\\end{tabular}
\caption{An example from the dataset, with the passage sentence relevant for answering italicized. The passage has been shortened for clarity.}\label{fig:ex}
\end{figure}

Machine comprehension for healthcare and medicine has received little attention so far, although it offers great potential for practical use.
A typical application would be clinical decision support, where given a massive amount of text, a clinician asks questions about either external, medical knowledge (reading literature) or about particular patients (reading electronic health records). Currently, patient-specific questions are tackled by manually browsing or searching those records. This task can be facilitated by summarization and QA systems \citep{DemnerFushmanAndLin2007,DemnerFushmanEtAl2009}, and we believe, by fine-grained machine reading. Reading comprehension systems that perform on a finer level could play an important role especially when combined with document retrieval to perform machine reading at scale, such as in the models of  \citet{ChenEtAl2017} and \citet{WatanabeEtAl2017} for the general domain.\looseness=-1

For our dataset, we construct queries, answers and supporting passages from BMJ Case Reports, the largest online repository of such documents. A case report is a detailed description of a clinical case that focuses on rare diseases, unusual presentation of common conditions and novel treatment methods. Each report contains a \textit{Learning points} section, summarizing the key pieces of information from that report. The learning points are typically paraphrased portions of passage text and do not match passage sentences exactly. We use these learning points to create queries by blanking out a medical entity. To counteract potential errors and inconsistencies due to automated dataset creation, we perform several checks to improve the quality of the dataset (\autoref{sec:design}). Our dataset contains around 100,000 queries on 12,000 case reports, has long support passages (around 1,500 tokens on average) and includes answers which are single- or multi-word medical entities. We show an example from the dataset in Figure \ref{fig:ex}. 


We examine the performance on the dataset in two ways. First, we report machine performance for several baselines and neural readers. To enable a more flexible answer evaluation, we expand the answers with their respective synonyms from a medical knowledge base, and additionally supplement the standard evaluation metrics with BLEU and embedding-based methods. We investigate different ways of representing medical entities in the text and how this affects the neural readers. We obtain the best results with a recurrent neural network (RNN) with gated attention \citep{DhingraEtAl2017a}, but a simple approach based on embedding similarity proves to be a strong baseline as well. Second, we look at how well humans perform on this task, by asking both a medical expert and a novice to answer a portion of the validation set. When categorizing the skills necessary to find the right answer, we observe that a large number of comprehension skills get activated and that prior knowledge in the form of the ability to perform lexico-grammatical inferences matters the most. This suggests that for our dataset and possibly for domain-specific datasets more generally, more background knowledge should be incorporated in machine comprehension models. The current gap between the best machine and the best human performance is nearly 20\% F1, which leaves ample space for further study of machine readers on our dataset. In brief, the contributions of our paper are:
\begin{compactitem}
\item We propose a large dataset for reading comprehension in the medical domain, using clinical case descriptions.\looseness=-1
\item We carry out an empirical analysis of \begin{inparaenum}[\itshape a\upshape)] \item system and human performance on reading comprehension, and \item comprehension skills that are required for answering the queries correctly and that allow us to position the dataset according to its difficulty on each of the skills.\end{inparaenum}
\end{compactitem}

\section{Related datasets}\label{sec:related}
\begin{table}[t]
\small
\begin{tabular}{p{2.7cm} p{1.8cm} l r}
Dataset & \parbox[t]{5cm}{Question origin} &  Domain & Size \\
\cmidrule(lr){1-4}
\parbox[t]{5cm}{CliCR\\label{eq:simentity}
\textit{sim-entity}=\argmax_{i\in E} \text{cos}\big(\sum_{j\in C_i} c_j,\sum_{k\in Q} q_k\big),
\label{eq:sareaderpred}
\hat{a} = \argmax_{i\in E} e_o(i)^T o,
\label{eq:gareaderpred}
\hat{a} = \argmax_{i\in E} \sum_{t\in R(i,p)} \alpha_t,

m_{v}(f) = \frac{1}{|D_{\text{test}}|} \sum_{(p,q,A)\in D_{\text{test}}} \max_{a\in A} v(f(p,q), a).

Since there are multiple correct answers , we take the highest scoring answer  at each instance, as done in \citet{RajpurkarEtAl2016}. Note that in the dataset we do not supply the candidate answers; in the experiments, we constrain the candidates to the set of entities in the passage.

The two standardly used metrics for machine comprehension evaluation are the \textbf{exact match} (EM) and the \textbf{F1} score. For EM, the predicted and the ground truth answers must match precisely, safe for articles, punctuation and case distinction (same for other metrics). F1 metric is applied per instance and measures the overlap between the prediction  and the ground truth , which are treated as bags of words.\footnote{In precision, the number of correct words is divided by the number of all predicted words. In recall, the former is divided by the number of words in the ground-truth answer.} 
While these two metrics are arguably sufficient in news-style machine comprehension where the entities are proper nouns which allow for little variation and synonymy, in our case the medical entities are often mostly common nouns modified by specifiers and qualifiers. To take into account potentially large lexical and word-order variation, we use two additional metrics.
First, we measure \textbf{BLEU} \citep{PapineniEtAl2002} for n-grams of length 2 (shortly, B-2) and 4 (B-4) using the package by \citet{ChenEtAl2015}, with which we aim to capture contiguity of tokens in longer answers. Second, it may occur that answers contain no word overlap yet still be good candidates because of their semantical relatedness, as in  \enquote{renal failure}--\enquote{kidney breakdown}. We take this into account by using an \textbf{embedding metric} (E-avg), in which we construct mean vectors for both ground-truth and system answer sequences, and then compare them with the cosine similarity. This and other embedding metrics for evaluation were previously studied in dialog-system research \citep{LiuEtAl2016}.

\section{Results and analysis}
\begin{table}[t]
\centering
\begin{tabular}{l c c c c c}
Method & EM & F1 & B-2 & B-4 & E-avg \\ \cmidrule(lr){1-6}
rand-entity & 1.4 & 5.1 & .03 & .01 & .23 \\ maxfreq-entity & 8.5 & 12.6 & .10 & .05 & .31 \\ sim-entity & 20.8 & 29.4 & .22 & .15 & .45 \\ lang-model & 2.1 & 3.5 & .00 & .00 & .30 \\
\cmidrule(lr){1-6}
SA-Anonym & 19.6 & 27.2 & .22 & .16 & .43 \\ SA-Ent & 6.1 & 11.4 & .07 & .05 & .31 \\ \cmidrule(lr){1-6}
GA-Anonym & 24.5 & 33.2 & .28 & .20 & .48  \\ GA-Ent & 22.2& 30.2 & .25 & .18 & .46  \\ GA-NoEnt & 14.9 & 33.9 & .21 & .11 & .51 \\\cmidrule(lr){1-6}
\textit{human-expert}  & \textit{35} & \textit{53.7} & \textit{.46} & \textit{.23} & \textit{.67} \\ \textit{human-novice} & \textit{31} & \textit{45.1} & \textit{.43} & \textit{.24} & \textit{.62} \\ \end{tabular}

\caption{Answering results on the test set. EM and F1 scores are percentages. The human scores (in italics) are based on the validation set.}\label{tab:test}
\end{table}

We show the results in Table~\ref{tab:test}. We see that answer prediction based on contextual representation of queries and passages (sim-entity) achieves a strong base performance that is only outperformed by GA reader. The language model performs poorly on EM and F1, but the embedding-metric score is higher, likely reflecting the fact that the predicted answers---though mostly incorrect---are related to the ground-truth answers. The poor performance means that based on queries alone (without reading the passage), it is difficult to provide accurate answers. 
The GA reader performs well across all entity set-ups, even when the entities are not marked in the passage. Interestingly, the exact match and BLEU scores in this case are much lower compared to other entity set-ups. Upon inspecting the predicted answers more closely, we have observed that GA-NoEnt tends to predict longer answers than GA-Ent/Anonym. For example, the average predicted answer length for GA-NoEnt was as high as 3.7 tokens, whereas for the other two set-ups and the ground-truth answers the numbers range between 2.3 and 2.5. A plausible explanation for this lies in how GA reaches its prediction \eqref{eq:gareaderpred}, which is by accumulating the attention weights without normalizing. This would then drive the model to prefer longer answers. For example, for the ground-truth entity \enquote{chest CT}, GA-NoEnt predicts \enquote{interval CT scans of the chest}. Although all neural models use pre-trained word embeddings, for Ent and Anonym the multi-word entities do not have pre-trained embeddings since our embeddings are induced on the word level. This may partly explain the competitive performance of NoEnt compared to Ent. We leave the integration of entity embeddings for the future work. 



The results for SA reader are far below the performance of GA reader. We also see that it performs much better on anonymized entities than on non-anonymized ones. This is in line with \citet{WangEtAl2017} who find that SA reader suffers a drop of 19 points in exact match on Who-did-what dataset when anonymization is not done. A possible explanation is that anonymization reduces the output space to only several hundred entity candidates for which the output embedding needs to be trained. When we do not use anonymization, the set of output entities increases to the set of all entity types found in all passages, which is several orders of magnitude more. While this effect also occurs for GA reader, it is less pronounced because GA reader scores words in the passage and does not need to learn separate answer word embeddings.

\subsection{Human performance}
To measure the accuracy of human answering, we have used the same sample of data instances as used for the analysis of skills.\footnote{Human answers were collected before the skill analysis.} The queries were answered separately by a novice reader (linguistics background, little-to-none medical knowledge) and by an expert reader (both linguistics and medical background). The annotators needed around 15 minutes on average to read the passage and answer the query. The results are shown at the bottom of Table~\ref{tab:test}. The expert scores higher across all evaluation metrics, with as much as a 7-point advantage in \% F1. This advantage is largely coming from the better performance on those instances where bridging inferences are required (the average F1 score was 10 points higher on these queries), which suggests that domain knowledge is beneficial in the comprehension task. For a novice in a specialized domain, it is harder to build a good situation model that would lead to successful comprehension since it requires more effort---active, strategic processing and establishing ontological relationships in that specific domain. For an expert reader this process is more automatized \citep{KintschAndRawson2008}.

We can see from the table that the best human performance is well below its theoretical upper bound of 100\% F1. An important part of explanation for this lies in the automated dataset construction, which leaves certain queries unanswerable, especially when the authors do not refer to a part in the article but introduce completely new information. Another reason is the problem of \enquote{answer openness}: Typically more than one correct answer is possible and the answers can be correct to various degrees, which we aimed to capture with the use of the embedding metric in the evaluation. Nevertheless, the gap between the best human and machine F1 score is large (around 20 points), leaving considerable space for future applications of machine readers on our dataset.\footnote{For comparison, the gap for SQuAD was 12.2 and for NewsQA 19.8 \citep{TrischlerEtAl2016}.}

\subsection{Breakdown of results by skill}
\begin{figure}
\begin{center}
\includegraphics[width=\columnwidth]{images/plotskillsbreakdownGAnoent.pdf}
\caption{Performance per required skill for the human expert and GA-NoEnt reader.}
\label{fig:plotskillsbreakdown}
\end{center}
\end{figure}

To see how the answering performance relates to the skill requirements, we have analyzed the part of the validation set annotated with the skills by averaging F1 values for all instances with a particular skill. In this way, we are able to break down both human and machine performance skill-wise, as shown in Figure~\ref{fig:plotskillsbreakdown}. Because of the small sample size, the results should only be taken as a general indication. The most difficult cases for the GA reader are those annotated with \enquote{none} (unanswerable) and \enquote{ellipsis} (recognizing implicit and omitted information), ignoring \enquote{analogy} for which we only have a single annotated case. Furthermore, spatio-temporal reasoning, elaboration (inferences using general knowledge) and bridging---which is also the most commonly required skill---are the next most difficult ones. The human scores are mostly much higher, which is especially apparent for spatio-temporal reasoning, logical skills and the skill involving punctuation. 
Our findings align with those of \citet{ChuEtAl2017} on the Lambada dataset \citep{PapernoEtAl2016}: Although they used a different categorization of comprehension skills, they also find that GA reader has most difficulties with elaboration (which they refer to as \enquote{external knowledge}), followed by coreference resolution.

\section{Conclusion and future work}
We have introduced a new dataset for domain-specific reading comprehension in which we have constructed around 100,000 cloze queries from clinical case reports. We analyzed the dataset in terms of the skills required for successful comprehension, and applied various baseline methods and state-of-the-art neural readers. We showed that a large gap still exists between the best machine reader and the expert human reader. One direction for future research is improving the reading models on the queries that are currently the most challenging, i.e.\ those requiring world and background domain knowledge. Better representing background knowledge by inducing embeddings for entities or otherwise integrating ontological knowledge is in our opinion a promising avenue for future research.


\section*{Acknowledgments}
We would like to thank Madhumita Sushil and the anonymous reviewers for useful comments. We are also grateful to BMJ Case Reports for allowing the collection of case reports. This work was carried out in the framework of the Accumulate IWT SBO project (nr.\ 150056), funded by the government agency for Innovation by Science and Technology. We also acknowledge the support of the Nvidia GPU Grant Program.

\bibliography{naaclhlt2018}
\bibliographystyle{acl_natbib}

\appendix
\section{Training details and hyper-parameter optimization}\label{app:hyper}


We train the word embeddings using word2vec \citep{MikolovEtAl2013a}, and optimize the window size, the model type (CBOW, skip-gram), the dimensionality and the number of negative samples using random search. For the embedding baseline sim-entity, the evaluation was carried out 20 times on the validation part of our dataset, and we chose the parameter configuration that led to the highest-performing embedding model as measured by F1. We find that higher embedding dimensionality works better, that CBOW obtains somewhat better scores than Skipgram, and that medium-sized word windows work best. The best configuration: 'win\_size': 5, 'min\_freq': 200, 'model': 'cbow', 'dimension': 750, 'neg\_samples': 5. The difference between the lowest and the highest scoring model was 3.4 F1. At prediction time (equation \eqref{eq:simentity}) we set the window size to 3, which worked best on the validation set.




For inclusion in the neural readers, it would be impractical to use the high embedding dimensionality found in the hyper-parameter search from the previous paragraph, so we fix the input embedding dimensionality to 200, as done in \citet{ChenEtAl2016} to keep the training time practical. We optimize the remaining embedding hyper-parameters just like above. The best parameters were: 'win\_size': 4, 'min\_freq': 200, 'model': 'cbow', 'dimension': 200, 'neg\_samples': 9.


For SA reader, we optimized the hidden state size and the dropout rate using 20 different random configurations. The best values were 70 and 0.57, respectively. We explore the same parameters for the GA reader, but add to the search space the feature that indicates the presence of a passage token in the query, which was found useful in the NoEnt set-up.
The best hidden state number and dropout rate were 64 and 0.5, respectively. 
We used the default values for all the remaining hyper-parameters.

\section{List of skills with selected examples}\label{app:skills}
In annotating the skills, we followed the categorization by \citet{SugawaraEtAl2017a}:
\begin{enumerate}
\item Object tracking: tracking or grasping multiple objects; it is a version of list/enumeration skill used in previous skill classifications
\item Mathematical reasoning: whenever a mathematical operation is involved in finding the answer
\item Coreference resolution: direct reference to an object, includes anaphoras. These include inferential processes based on background knowledge or context.
\item Logical reasoning: conditionals, quantifiers, negation, transitivity
\item Analogy: metaphors, metonymy
\item Causal relation: explicit expression such as "why", "the reason of"
\item Spatio-temporal relations
\item Ellipsis: recognizing implicit or omitted information
\item Bridging: inference through grammatical and lexical knowledge (synonymy, idioms etc). 
This link however is not automatic or stereotypical, as in the category of elaboration.
\item Elaboration: inference through commonsense reasoning. 
Note that unlike in the previous category, there is no direct way in which grammatical, lexical or ontological knowledge could help.
\item Meta-knowledge: knowing about the text genre and the main topic being discussed assists in comprehending. In our dataset, knowing the way the queries are constructed (Learning points) is sometimes beneficial.
\item Schematic clause relation: complex sentences that include coordination or subordination
\item Punctuation: understanding parentheses, dashes, quotations, colons etc.
\end{enumerate}

In the following examples, we mark the medical entities in blue, and italicize the parts in the passage that are crucial for answering. Whenever we shorten a part of the passage, we use [...].

\subsection{Bridging inference}
\noindent
\textbf{passage}\\
We report a case of a 72 - year - old Caucasian woman with \textcolor{blue}{pl-7 positive antisynthetase syndrome} . Clinical presentation included \textcolor{blue}{interstitial lung disease} , \textcolor{blue}{myositis} , ‘ mechanic 's hands ’ and \textcolor{blue}{dysphagia} . As \textcolor{blue}{lung injury} was the main concern , \textcolor{blue}{treatment} consisted of \textit{\mbox{\textcolor{blue}{prednisolone}} and \mbox{\textcolor{blue}{cyclophosphamide}}} . Complete remission with reversal of \textcolor{blue}{pulmonary damage} was achieved , as reported by \textcolor{blue}{CT scan} , \textcolor{blue}{pulmonary function tests} and functional status . [...]\\
\textbf{query}\\
Therefore , in severe cases an \textcolor{blue}{aggressive treatment} , combining \textcolor{blue}{\textbf{\underline{\hspace{2cm}}}} and \textcolor{blue}{glucocorticoids} as used in \textcolor{blue}{systemic vasculitis} , is suggested .\\
\textbf{answer}\\
\textcolor{blue}{cyclophoshamide}\\
\textbf{explanation} The reader needs to have the background knowledge that \textcolor{blue}{prednisolone} is a \textcolor{blue}{glucocorticoid}, then it becomes obvious that the answer is \textcolor{blue}{cyclophoshamide}.

\subsection{Object tracking}
\noindent
\textbf{passage}\newline
[...] The patient was managed with \textcolor{blue}{supportive measures} and the National Poisons Information Service was contacted . 
A \textcolor{blue}{toxicology consultant} was involved in view of the unusual mode of administration . 
Although there was no precedent on how to treat a \textcolor{blue}{significant rectal overdose} of \textcolor{blue}{amitriptyline} , \textit{it was advised that the patient be administered a \mbox{\textcolor{blue}{phosphate enema}} and if failed to adequately remove the tablets then the patient should be given \mbox{\textcolor{blue}{whole bowel irrigation}}} with 2 litre of \textcolor{blue}{Klean - Prep} via a \textcolor{blue}{nasogastric tube} . It was also advised that we admit the patient to a high dependency unit and manage him according to the usual protocol for a \textcolor{blue}{tricyclic overdose} if \textcolor{blue}{complications} arose . [...]
\textbf{query}\\
It seems reasonable to attempt careful \textcolor{blue}{removal} of the \textcolor{blue}{drug} from the rectum and if that fails to consider \textcolor{blue}{\textbf{\underline{\hspace{2cm}}}} and \textcolor{blue}{whole bowel irrigation} .\\
\textbf{answer}\\
\textcolor{blue}{phosphate enemas}\\
\textbf{explanation} The query mentions \textcolor{blue}{removal} (A), then \textcolor{blue}{\textbf{\underline{\hspace{2cm}}}} (B) and \textcolor{blue}{whole bowel irrigation} (C). In the passage, one needs to track those elements and choose the right one. This skill should be considered whenever the gap is part of an enumeration or is mentioned as a part of another entity.

\subsection{Meta knowledge}
\noindent
\textbf{query}\\
bedaquiline , a \textcolor{blue}{new agent} with \textcolor{blue}{bactericidal} and sterilising activity against \textcolor{blue}{mycobacterium tuberculosis} , is effective against \textcolor{blue}{\textbf{\underline{\hspace{2cm}}}} when given together with a \textcolor{blue}{background regimen} , and is well tolerated and safe if there is awareness of drug interactions and precautions are taken to avoid \textcolor{blue}{potential qt prolongation} .\\
\textbf{answer}\\
\textcolor{blue}{tuberculosis}\\
\textbf{explanation}
The right answer can be inferred from several parts in the passage (not shown), or even from the title or the query. The query, though, is nowhere in the document explicitly answered.



\begin{comment}
Error analysis

ga-reader, \textsc{No-Ent}, 
a_hat=treatment
requires bridging, but GA reader gets it wrong.
bcr-2013-200832.4

oral neurovascular hamartoma : an extraordinary verdict in the oral cavity summary  the presence of a neurovascular hamartoma within the oral cavity is truly a rare entity . scarcely reported in the literature , these hamartomas are smooth , pinkish masses and are painless ,\
 and therefore difficult to diagnose . they are benign in nature and apply pressure to their surroundings . the histological diagnosis remains the gold standard as it comprises of neural tissue and vascular components . treatment is surgical excision with adequate margins . \
recurrence is reported in cases of incomplete resection .  background  the term hamartoma is derived from the greek word ‘ hamartion ’ which literally means bodily defect . it was initially used by albrecht in 1904.1 it refers to a benign malformation in different areas of t\
he body where growth occurs . the histology consists of an abnormal mixture of cells and tissues localised at the particular site secondary to a developmental error . they do not develop as part of an inflammatory or neoplastic process and have a self limiting proliferation \
. 2 on the other hand , they do not have a tendency to regress spontaneously .  while considering the oral cavity , the hamartoma might result secondary to the proliferation of smooth muscles , skeletal muscles , fat , salivary tissues , vessels and lymphatics . an oral neur\
ovascular hamartoma is rare because of proliferation of neural tissue solely or in combination with vascular elements .  described below is an unusual case of an oral hamartoma with neurovascular components .  case presentation  a 14 - year - old boy presented to the outpati\
ent department with right cheek swelling for the past 1.5 years . the swelling had been gradually increasing over a period of time and eventually became impossible to ignore . he also reported of decreased mouth opening and a burning sensation within the oral cavity on eatin\
g spicy food for 2 years . he did have a significant history of consuming betel nut since the age of 11 years , discontinuing the use once the mouth opening started reducing . his right cheek also had decreased sensation , but there were no symptoms of fever or pain .  the r\
est of the history was unremarkable .  on examination , the boy appeared to be physically fit with average height and weight . he had a diffuse 2 × 1.5 cm sized right cheek swelling about two finger breadths below the zygoma . the swelling was soft in consistency , non-tende\
r , non-mobile and non-translucent . the swelling would become firm on clenching of teeth and mobile on bimanual examination .  examination of the oral cavity showed significant submucous fibrosis with poor oral hygiene and fullness in the right buccal mucosa . neck examinat\
ion was unremarkable as was the systemic examination .  as part of his workup , his complete blood picture was normal . the patient brought with him a ct scan of poor quality and thick cuts . the lesion was not clearly outlined but owing to financial constraint the scan was \
not repeated ( figure 1 ) . no lymphadenopathy was identified on the scan , however .  investigations  histological findings confirmed the diagnosis . the tissue specimen showed haphazard distribution of mature skeletal muscles of varying sizes and nerve fibres closely inter\
twined with each other on h & amp ; e staining ( figure 2 ) . the specimen was also evaluated with immunehistochemical stains , which showed positive reactivity with s - 100 ( figure 3 ) . the lesion was concluded to be neurovascular hamartoma of the oral cavity .  different\
ial diagnosis  treatment  performing a 2 cm incision over the right buccal mucosa , below the stenson 's duct , the dissection was continued using dissection scissors . on reaching the masseter muscle and locating the mass adjacent to the ramus of the mandible , the mass was\
 removed with wide margins ( figure 4 ) . the cavity was closed with proper tissue reapproximation using absorbable sutures . postoperatively , the patient was provided with ice packs to prevent extensive swelling and started on intravenous augmentin 1.2 g / q8h and analgesi\
cs for 48 h after which he was switched to oral augmentin in a dose of 625 mg thrice a day and oral analgesics .  outcome and follow - up  discussion  hamartomas arising within the oral cavity are unusual and may show a variety of clinical presentations and histological and \
growth patterns . the biological behaviour is benign with reasons for development vague . while epithelial and mesenchymal hamartomas involving the oral cavity are rare , one can not deny that the presence of a neural or a neurovascular component in a hamartoma is even more \
infrequent . the literature has only reported a handful of these cases .  in 2007 , takeyama et al5 reported of a hamartoma on the hard palate associated with corpus callosum agenesis , microphthalmia and skin malformation . prior to that , goldsmith et al6 described a case \
of leiomyomatous hamartoma of the posterior tongue leading to dysphagia in a 16 - year - old , whereas miyamoto et al7 documented a lingual hamartoma in a child with a cleft palate . al habeeb et al8 reported a solitary but cutaneous neural hamartoma in a middle - aged indiv\
idual . the lesion had histological characteristics comprising of numerous mature nerve bundles within the papillary dermis . prior to that , a case of congenital neural hamartoma on the leg was reported by argenyi et al . vascular hamartomas are most common of all in the he\
ad and neck . 10  as well as being sporadic , hamartomas can present as a part of a syndrome . epithelial hamartomas can be a part of cronkhite – canada syndrome or orofacial digital syndrome , 11 whereas neuromuscular and vascular hamartomas may become apparent in crohn 's \
disease . 12  oral neurovascular hamartomas present as smooth surfaced exophytic lesions usually less than 1 cm in diameter . being pedunculated or wide based , their colour may range from pink to yellow . they are painless lesions leading to general discomfort in the patien\
t .  histologically , there is no clear boundary between the lesions and the surrounding normal structures as they are non-encapsulated and poorly circumscribed . composed of hypocellular connective tissue containing aggregates of loose vessels closely packed with small to m\
edium sized nerve bundles , they are not easily distinguishable from the surrounding non-hamartomatous tissues . these nerve bundles are reactive with s - 100 protein . this feature can be easily used as a criterion for diagnosis while examining . 13 this reactivity was also\
 pivotal in our case to reach the final diagnosis . distinction from quite a few oral benign lesions is still hard . oral neurovascular hamartomas resemble traumatic hamartomas and can be distinguished based on the fact that the blood vessels in the former are very closely e\
ntwined with the neural component . in traumatic neuromas the two components are separate .  once the diagnosis is confirmed the clinical management comprises of surgical excision . local excision is curative ; a management plan similar to most benign oral lesions . a recurr\
ence only follows an incomplete excision and one should not mistake it with malignant transformation . 14 close clinical follow - up is absolutely essential . the literature does not say much regarding local recurrence of oral neurovascular hamartomas but oral mesenchymal ha\
martomas arising from the cheek have a reported local recurrence rate of 20 cal presentation and treatment plan .

@placeholder cures the disease and recurrence is only in cases of incomplete resection .

surgical excision


\section*{Appendix: Examples}
Query uses paraphrasing, with a guideline tone:\\
\textbf{D}: ... He was transfused and a portovenous abdominopelvic CT scan was performed to define the portal venous anatomy and source of umbilical bleeding ...\\
\textbf{Q}: A portovenous abdominopelvic CT scan should be performed in patients with cirrhosis and portal hypertension who have recurrent umbilical bleeding , to identify @placeholder and site of bleeding .\\
\textbf{A}: ...

Requiring inference:\\
\textbf{D}: Diacon et al22 had also found that BDQ , when given together with a background regimen , led to faster culture conversion and higher rates of culture conversion at 6 months of treatment compared to a background regimen and placebo .\\
This suggests BDQ 's potential capacity to significantly reduce the treatment period and the debilitating and dangerous adverse effects associated with some of the existing second - line anti-TB drugs , as well as to improve adherence , achieve higher cure rates , and preserve the activity of the background regimen .\\
\textbf{Q}: Bedaquiline , a new agent with bactericidal and sterilising activity against Mycobacterium tuberculosis , is effective against @placeholder when given together with a background regimen , and is well tolerated and safe if there is awareness of drug interactions and precautions are taken to avoid potential QT prolongation .\\
Hard to answer right: Answer could be inferred from "anti-TB" mention, or from the reoccuring TB entity in the document and the title.

\textbf{D}: ...\\
\textbf{Q}: Early diagnosis will increase the quality of life of patients with @placeholder and improve the prognosis .\\
Here, it's necessary to understand what is the main topic of the case; the entity occurs several times in the text, but the query itself may not be mentioned (in any form) in the document

\textbf{D}: [...] A high level of suspicion is necessary to make an early diagnosis and avoid the irreversible changes in lung architecture that determine a persistent respiratory incapacity , worse prognosis and deterioration of functional status .

\textbf{Q}: A high level of suspicion is necessary to make an early diagnosis and start @placeholder before irreversible damage takes place .

\textbf{A}: appropriate treatment


Need domain knowledge that prednisolone is a glucocorticoid ("bridging"):

\textbf{D}: summary we report a case of a 72 - year - old caucasian woman with @pl\_-\_7\_positive\_antisynthetase\_syndrome . clinical presentation included @interstitial\_lung\_disease , @myositis , UNK mechanic "s hands UNK and @dysphagia . as @lung\_injury was the main concern , @treatment consisted of @prednisolone and @cyclophosphamide . complete remission with reversal of @pulmonary\_damage was achieved , as reported by @ct\_scan , @pulmonary\_function\_tests and functional status . background case presentation a 72 - year - old caucasian woman with no @previous\_significant\_medical\_history , medicated with @simvastatin 40 mg [...]

\textbf{Q}: therefore , in severe cases an @aggressive\_treatment , combining @placeholder and @glucocorticoids as used in @systemic\_vasculitis , is suggested .

\textbf{A}: cyclophosphamide
\end{comment}

\begin{comment}
\section*{Appendix: Annotating skills}

This analysis is to a large extent inspired by the categories listed in \citet{SugawaraEtAl2017a}.
\end{comment}
\end{document}
