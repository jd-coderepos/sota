\documentclass{article}

\usepackage[nonatbib,final]{neurips_2020}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[pagebackref=true,colorlinks]{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}

\usepackage{multirow}
\usepackage[font={small}]{caption}
\usepackage[labelformat=simple]{subcaption}
\renewcommand\thesubtable{(\alph{subtable})}
\renewcommand\thesubfigure{(\alph{subfigure})}
\captionsetup{compatibility=false}
\usepackage{tablefootnote}
\usepackage[dvipsnames]{xcolor}
\usepackage{graphicx}
\usepackage{pdfpages}
\usepackage{enumitem}

\title{Self-Supervised Learning by Cross-Modal Audio-Video Clustering}

\author{
Humam Alwassel \\
\texttt{\small humam.alwassel@kaust.edu.sa} \\ \And
Dhruv Mahajan \\
\texttt{\small dhruvm@fb.com} \\ \And
Bruno Korbar \\
\texttt{\small bkorbar@fb.com} \\ \AND
Lorenzo Torresani \\
\texttt{\small torresani@fb.com} \\ \And
Bernard Ghanem \\
\texttt{\small bernard.ghanem@kaust.edu.sa} \\ \And
Du Tran \\
\texttt{\small trandu@fb.com} \\ 
\AND \vspace{-16pt}
\\
King Abdullah University of Science and Technology (KAUST) \quad Facebook AI \\
{\url{http://humamalwassel.com/publication/xdc}}
}

\begin{document}

\maketitle

\begin{abstract}
Visual and audio modalities are highly correlated, yet they contain different information. Their strong correlation makes it possible to predict the semantics of one from the other with good accuracy. Their intrinsic differences make cross-modal prediction a potentially more rewarding pretext task for self-supervised learning of video and audio representations compared to within-modality learning. Based on this intuition, we propose \emph{Cross-Modal Deep Clustering (XDC)}, a novel self-supervised method that leverages unsupervised clustering in one modality (\emph{e.g.}, audio) as a supervisory signal for the other modality (\emph{e.g.}, video). This cross-modal supervision helps XDC utilize the semantic correlation and the differences between the two modalities. Our experiments show that XDC outperforms single-modality clustering and other multi-modal variants. XDC achieves state-of-the-art accuracy among self-supervised methods on multiple video and audio benchmarks. Most importantly, our video model pretrained on large-scale unlabeled data significantly outperforms the same model pretrained with full-supervision on ImageNet and Kinetics for action recognition on HMDB51 and UCF101. To the best of our knowledge, XDC is the first self-supervised learning method that outperforms large-scale fully-supervised pretraining for action recognition on the same architecture.
\end{abstract} \section{Introduction}\label{sec:intro}

Do we need to explicitly name the actions of ``laughing'' or ``sneezing'' in order to recognize them? Or can we learn to visually classify them without labels by associating characteristic sounds with these actions? Indeed, a wide literature in perceptual studies provides evidence that we rely heavily on hearing sounds to make sense of actions and dynamic events in the visual world. For example, objects moving together are perceived as bouncing off each other when the visual stimulus is accompanied by a brief sound~\cite{crossmodal}, and the location and timing of sounds are leveraged as important cues to direct our spatiotemporal visual attention~\cite{Heffner1992,Naatanen1992}. The influence of hearing sounds in visual perception is also suggested by perceptual studies showing that individuals affected by profound deafness exhibit poorer visual perceptual performance compared to age-matched hearing controls~\cite{gentile1969academic,myklebust1960psychology}. 

In this work, we investigate the hypothesis that spatiotemporal models for action recognition can be reliably pretrained from {\em unlabeled} videos by capturing cross-modal information from audio and video. The motivation for our study stems from two fundamental challenges facing a fully-supervised line of attack to learning video models. The first challenge is the exorbitant cost of scaling up the size of manually-labeled video datasets. The recent creation of large-scale action recognition datasets~\cite{caba2015activitynet,something_something,Sports1M,kinetics} has undoubtedly enabled a major leap forward in video models accuracies. However, it may be argued that additional significant gains by dataset growth would require scaling up existing labeled datasets by several orders of magnitude. 
The second challenge is posed by the unclear definition of suitable label spaces for action recognition. Recent video datasets differ substantially in their label spaces, which range from sports actions~\cite{Sports1M} to verb-noun pairs for kitchen activities~\cite{Damen2018EPICKITCHENS}. This suggests that the definition of the ``right'' label space for action recognition, and more generally for video understanding, is still very much up for debate. It also implies that finetuning models pretrained on large-scale labeled datasets is a suboptimal proxy for learning models for small- or medium-size datasets due to the label-space gap often encountered between source and target datasets. 

\vspace{-2pt}
In this paper, we present three approaches for training video models from self-supervised audio-visual information. At a high-level, the idea behind all three frameworks is to leverage one modality (say, audio) as a supervisory signal for the other (say, video). We posit that this is a promising avenue because of the simultaneous synergy and complementarity of audio and video: correlations between these two modalities make it possible to perform prediction from one to the other, while their intrinsic differences make cross-modal prediction an enriching self-supervised task compared to within-modality learning. Specifically, we adapt the single-modality DeepCluster work of Caron~\emph{et al.}~\cite{caron2018deep} to our multi-modal setting. DeepCluster was introduced as a self-supervised procedure for learning image representation. It alternates between unsupervised clustering of image features and using these cluster assignments as pseudo-labels to revise the image representation. 
In our work, the clusters learned from one modality are used as pseudo-labels to refine the representation for the other modality. In two of our approaches---Multi-Head Deep Clustering (MDC) and Concatenation Deep Clustering (CDC)---the pseudo-labels from the second modality are {\em supplementary}, \emph{i.e.}, they complement the pseudo-labels generated in the first modality. The third approach---Cross-Modal Deep Clustering (XDC)---instead uses the pseudo-labels from the other modality as an {\em exclusive} supervisory signal. This means that in XDC, the audio clusters drive the learning of the video representation and vice versa. 
Our experiments support several interesting conclusions:
\vspace{-5pt}
\begin{itemize}[nosep, leftmargin=.42cm]
\item All three of our cross-modal methods yield representations that generalize better to the downstream tasks of action recognition and audio classification, compared to their within-modality counterparts. 
\item XDC (\emph{i.e.}, the cross-modal deep clustering relying on the other modality as an exclusive supervisory signal) outperforms all the other approaches. This underscores the complementarity of audio and video and the benefits of learning label-spaces across modalities.
\item Self-supervised cross-modal learning with XDC on a large-scale video dataset yields an action recognition model that achieves higher accuracy when finetuned on HMDB51 or UCF101, compared to that produced by fully-supervised pretraining on Kinetics. To the best of our knowledge, this is the first method to demonstrate that self-supervised video representation learning outperforms large-scale fully-supervised pretraining for action recognition. 
Moreover, unlike previous self-supervised methods that are only pretrained on curated data (\emph{e.g.}, Kinetics~\cite{kinetics} without action labels), we also report results of XDC pretrained on a large-scale uncurated video dataset.
\end{itemize}
 \vspace{-5pt}
\section{Related work}
\vspace{-2pt}
\label{sec:related_work}

\vspace{-3pt}\noindent{\bf Early unsupervised representation learning.}
Pioneering works include deep belief networks~\cite{hinton2006fast}, autoencoders~\cite{hinton2006reducing,vincent2008extracting}, shift-invariant decoders~\cite{ranzato2007unsupervised}, sparse coding algorithms~\cite{lee2007efficient}, and stacked ISAs~\cite{le2011learning}. While these approaches learn by reconstructing the input, our approach learns from a self-supervised pretext task by generating pseudo-labels for supervised learning from unlabeled data.

\vspace{-3pt}\noindent{\bf Self-supervised representation learning from images and videos.}
Several pretext tasks exploit image spatial context, \emph{e.g.}, by predicting the relative position of patches~\cite{doersch2015unsupervised} or solving jigsaw puzzles~\cite{noroozi2016unsupervised}. 
Others include creating image classification pseudo-labels (\emph{e.g.}, through artificial rotations~\cite{gidaris2018unsupervised} or clustering features~\cite{caron2018deep}), colorization~\cite{zhang2016colorful}, inpainting~\cite{pathak2016context}, motion segmentation~\cite{pathak2017learning}, and instance counting~\cite{noroozi2017representation}.
Some works have extended image pretext tasks to video~\cite{kim2019self,motion_statistics,clip_order}. Other video pretext tasks include frame ordering~\cite{fernando2017self,lee2017unsupervised,misra2016shuffle,wei2018learning}, predicting flow or colors~\cite{lai2019self,vondrick2018tracking}, exploiting region correspondences across frames~\cite{isola2015learning,jayaraman2016slow,wang2015unsupervised,wang2019learning}, future frame prediction \cite{lotter2016deep,mathieu2015deep,srivastava2015unsupervised,vondrick2016anticipating,vondrick2016generating}, and tracking~\cite{XiaolongWang19}. Unlike this prior work, our model uses two modalities: video and audio.

\vspace{-3pt}\noindent{\bf Cross-modal learning and distillation.} 
Several works~\cite{aytar2016soundnet,gupta2016cross} train a fully-supervised encoder on one modality and distill its discriminative knowledge to an encoder of a different modality.
Other works learn from unlabeled data for a specific target task~\cite{zhao2018sound,rouditchenko2019self}. 
Unlike these methods, our work is purely self-supervised and aims at learning representations that transfer well to a wide range of downstream tasks.
Previous cross-modal self-supervised methods most relevant to our work include audio-visual correspondence~\cite{Arandjelovic17}, deep aligned representations~\cite{aytar2017see}, audio-visual temporal synchronization~\cite{avts,owens2018audio}, contrastive multiview coding~\cite{tian2020contrastive}, and learning image representations using ambient sound~\cite{owens2016ambient}.
While~\cite{Arandjelovic17,aytar2017see,owens2016ambient,tian2020contrastive} use only a single frame, we use a video clip. Unlike our method, \cite{owens2016ambient} clusters handcrafted audio features and does not iterate on the pseudo-labels.
\cite{avts,owens2018audio} require constructing positive/negative examples for in-sync and out-of-sync video-audio pairs. This sampling strategy makes these approaches more difficult to scale compared to ours, as many potential out-of-sync pairs can be generated, yielding largely different results depending on the sampling choice~\cite{avts}. 
Recent works, such as MIL-NCE~\cite{miech2020end} and CBT~\cite{sun2019learning}, learn from unlabeled instructional videos using text from ASR, while our approach makes use of the audio signal instead.
 \begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/xdc_pipeline_figure_blurred.pdf}
  \vspace{-16pt}
  \caption{\textbf{Overview of our framework.} We present Single-Modality Deep Clustering (SDC) baseline vs. our three multi-modal deep clustering models: Multi-Head Deep Clustering (MDC), Concatenation Deep Clustering (CDC), and Cross-Modal Deep Clustering (XDC). 
  The video and audio encoders ( and ) map unlabeled videos to visual and audio features ( and ). These features, or their concatenations, are clustered using -means. The cluster assignments are then used as pseudo-labels to train the encoders. We start with randomly-initialized encoders, then alternates between clustering to generate pseudo-labels and training to improve the encoders. The four models employ different ways to cluster features and generate self-supervision signals. Illustration video is from ~\cite{cc_video_pictures}.} 
  
  \vspace{-8pt}
  \label{fig:pipeline_fig}
\end{figure}

\vspace{-1pt}
\section{Technical approach}
\vspace{-3pt}

\label{sec:technical_appraoch}
Here, we briefly discuss previous work on single-modality deep clustering in images~\cite{caron2018deep}. Then, we introduce our three multi-modal deep clustering frameworks for representation learning (Figure~\ref{fig:pipeline_fig}).

\vspace{-3pt}
\subsection{Single-modality deep clustering}
\vspace{-3pt}

Caron~\emph{et al.}~\cite{caron2018deep} proposed DeepCluster for self-supervised representation learning from images. DeepCluster iteratively clusters deep features from a single-modality encoder, and then uses the cluster assignments to train the same encoder to improve its representation. Inspired by the simplicity of this work, our paper studies deep clustering in the large-scale multi-modal setting.  
For completeness, we summarize DeepCluster details.
Let  be the set of unlabeled inputs (\emph{e.g.}, images),  be an encoder that maps an input  to a deep feature vector . DeepCluster iterates between clustering the features  and discriminative training to improve  using the clustering assignments as pseudo-labels. The process starts with a randomly-initialized , and only the weights of the classification \texttt{fc}-layer are reset between clustering iterations when the supervision-taxonomy is switched. DeepCluster uses a 2D CNN (\emph{e.g.} ResNet-) for  and clusters the features after each epoch using -means. We refer to DeepCluster as \textbf{Single-Modality Deep Clustering (SDC)}.

\vspace{-2pt}
\subsection{Multi-modal deep clustering}
\vspace{-3pt}

Contrary to the single-modality case, there exist multiple encoders in a multi-modal setting, each of which encodes a different modality of the input. In our paper, we consider two modalities, the visual and the audio modalities from the unlabeled training video clips. In particular, let  be the set of unlabeled video clips, and  and  be the visual and audio encoders, respectively.  Let  and  be the set of visual and audio deep features produced by the two encoders, respectively. There are different ways we can adapt the deep clustering framework to a multi-modal input. We describe three approaches (MDC, CDC, and XDC) by detailing the steps taken at each deep clustering iteration. Refer to the \emph{supplementary material} for the implementation differences between SDC and our three approaches.

\noindent\textbf{Multi-Head Deep Clustering (MDC).} 
This model builds on SDC by adding a second classification head supervised by the other modality. Thus, in this model, each encoder has two classification heads. At each deep clustering iteration, MDC uses the cluster assignments of  as pseudo-labels for one head and that of  as pseudo-labels for the other head. Thus, each encoder needs to predict the cluster assignments of its own modality (as in SDC), but also those generated by the other modality.

\noindent\textbf{Concatenation Deep Clustering (CDC).} 
This model performs clustering of joint visual and audio features. Specifically, at each deep clustering iteration, CDC clusters vectors obtained by concatenating the visual and audio feature vectors, separately -normalized. Then, it uses the resulting cluster assignments as pseudo-labels to update the weights of both  and .

\noindent\textbf{Cross-Modal Deep Clustering (XDC).} 
Each encoder in this model relies exclusively on the clusters learned from the other modality as the supervisory signal. At each deep clustering iteration, XDC clusters the audio deep features, , and uses their cluster assignments as pseudo-labels to train the visual encoder, . Vice versa, XDC supervises  with the cluster assignments of .
 \vspace{-2pt}
\section{Experiments}
\label{sec:ablation_experiments}

\vspace{-2pt}
\subsection{Experimental setup}
\vspace{-2pt}

\noindent {\bf Pretraining datasets.}
We use four datasets: Kinetics~\cite{kinetics}, AudioSet~\cite{AudioSet}, IG-Kinetics~\cite{ghadiyaram2019large}, and IG-Random, which have K, M, M, and M training videos, respectively.
As our approach is self-supervised, thus the labels from the first three datasets are {\bf not used} during pretraining. While Kinetics and AudioSet are supervised benchmarks for action recognition and audio classification, IG-Kinetics is a weakly-supervised dataset collected from a social media website using tags related to Kinetics actions. IG-Random is an \textit{uncurated} dataset of random videos from the same website. Videos are -second long in Kinetics and AudioSet and -to--second long in IG-Kinetics and IG-Random. We filter out around K Kinetics videos that have no audio. Furthermore, we randomly sample K videos from AudioSet and denote this subset as AudioSet-K. We generate this subset to have AudioSet data of the same size as Kinetics, in order to study the effects of pretraining with the same data size but on a different data distribution and domain. 

\noindent {\bf Downstream datasets.}
We evaluate our pretraining performance on three downstream benchmarks: UCF101~\cite{UCF101}, HMBD51~\cite{HMDB51}, and ESC50~\cite{esc50}, which have K, K, and K examples from , , and  classes, respectively. UCF101 and HMDB51 are action recognition datasets, while ESC50 is a sound classification dataset. UCF101 and HMDB51 have  official train/test splits, while ESC50 has  splits. We conduct our ablation study (Subsection~\ref{sec:ablation_study}) using split-1 of each dataset. We also report our average performance over all splits when we compare with state-of-the-art methods in Section~\ref{sec:compare_sota}.

\noindent {\bf Baselines.}
We consider two baselines: \textit{{Scratch}} and \textit{{Supervised Pretraining (Superv)}}. The first is a randomly-initialized model trained from scratch directly on the downstream task, while the second is a model pretrained in a supervised fashion on a large labeled dataset (\emph{e.g.}, Kinetics) and then finetuned on the downstream task. We note that these two baselines are commonly regarded as the lower and upper bounds to gauge the quality of self-supervised representation learning methods~\cite{Arandjelovic17,avts}.

\noindent {\bf Backbone architectures.}
We employ R(2+1)D~\cite{Tran18} and ResNet~\cite{KaimingHe16} as  and , respectively. 's input is a  clip, where  refers to the RGB channels,  is the number of frames, and  and  are the frame height and width. 's input is a  spectrogram image extracted from the audio signal, where  is the number of MEL filters and  is the number of audio frames. 

\noindent {\bf Pretraining and evaluation details.}
We choose the -layer variants of R(2+1)D and ResNet encoders. We use clips of = frames for pretraining and finetuning our visual encoder . We scale frames such that the smallest dimension is  pixels and then random crop images of size . We extract video clips at  fps and employ temporal jittering during training. 
For the audio input, we sample  seconds and use = MEL filters and = audio frames. For inference on the downstream tasks, we uniformly sample  clips per testing example and average their predictions to make a video-level prediction. We use only one crop per clip: the center  crop for video and the full  crop for audio. We provide more details in the \emph{supplementary material}.

\vspace{-1pt}
\subsection{Ablation study}\label{sec:ablation_study}
\vspace{-1pt}

\begin{table}[t!]
    \small
    \centering
    \caption{\textbf{Single-modality vs. multi-modal deep clustering.} We compare the four self-supervised deep clustering models (Section \ref{sec:technical_appraoch}) and the three baselines: Scratch, Supervised Pretraining (Superv), and same-modality-XDC (XDC with two encoders defined on the same modality). Models are pretrained via self-supervision on Kinetics and fully finetuned on each downstream dataset. We report the top-1 accuracy on split-1 of each dataset. All multi-modal models significantly outperform the single-modality deep clustering model. We mark in bold the best and underline the second-best models.}
    \begin{tabular}{l | c c | c c c c | c c }
        \hline
                &         &        &     &      &                  &           & \multicolumn{2}{c}{same-modality-XDC} \\
        Dataset & Scratch & Superv & SDC & MDC  & CDC              & XDC       & 2 visual encoders & 2 audio encoders \\
        \hline 
        UCF101 & 54.5    & 90.9   & 61.8 & 68.4 & \underline{72.9} & \bf{74.2} & 61.3 & N/A  \\
        HMDB51 & 24.1    & 58.0   & 31.4 & 37.1 & \underline{37.5} & \bf{39.0} & 30.5 & N/A \\
        ESC50  & 54.3    & 82.3   & 66.5 & 70.3 & \underline{74.8} & \bf{78.0} & N/A  & 66.0  \\
        \hline 
    \end{tabular}
    \vspace{-8pt}
    \label{table:ablation_study_dc_models}
\end{table}
 \noindent {\bf Study 1: Single-modality vs. multi-modal deep clustering.}
This experiment compares the four models presented in Section~\ref{sec:technical_appraoch}. We pretrain SDC, MDC, CDC, and XDC on Kinetics and report their performance on the downstream tasks in Table~\ref{table:ablation_study_dc_models}. 
To better understand XDC, we also conduct a new set of baselines, called same-modality-XDC, where XDC is trained with two encoders defined on the \textit{same} modality (either visual or audio).
Note that all models in this ablation study use the same visual and audio encoders and only differ in the way they use self-supervision. It takes on average  to  deep clustering iterations for these models to converge. 
\textbf{\textit{Observations:}}
\textbf{{(I)}} The four self-supervised deep clustering models outperform the Scratch baseline on all downstream benchmarks. This shows that our self-supervised pretraining is effective and generalizes well to multiple tasks.
\textbf{{(II)}} All multi-modal models (MDC, CDC, and XDC) significantly outperform SDC by up to \%, \%, and \% on UCF101, HMDB51, and ESC50, respectively. This validates the importance of multi-modal modeling compared to single-modality.
\textbf{{(III)}} XDC achieves the best performance across all tasks. What distinguishes XDC from the other models is that each modality encoder in XDC is self-supervised purely by the signal from the other modality. The encoders in CDC, MDC, and SDC all employ a self-supervision signal coming from the same modality. Thus, this suggests that encoders learn better when purely supervised by a different modality. 
We provide the following intuition on why XDC is better than CDC and MDC. XDC groups samples together when they are similar in one of the two modalities (video to supervise the audio encoder, audio to supervise the visual encoder). Instead, CDC groups samples together only if they are similar according to both the audio \textit{and} the video modality (to supervise both encoders). Thus, XDC visual and audio clusters allow for more diversity than those of CDC. We hypothesize that this diversity allows XDC to learn richer representations, which translates into better performance on the downstream tasks. Also, recent work~\cite{Wang_2020_CVPR} has shown that models trained on different modalities learn and generalize at different speeds, and that training them jointly (as done in MDC which uses two-modality heads) is sub-optimal. We believe that this could contribute to MDC performing worse than XDC, which optimizes for each modality independently.
\textbf{{(IV)}} The same-modality-XDC baselines perform similarly to SDC and are -\% worse than multi-modal-XDC. This suggests that cross-modality provides a superior supervisory signal for self-supervised learning and that multi-modal-XDC is the best model not because of its optimization strategy but rather because of the use of the other modality for pseudo-labeling. Given the results of this study, we opt to use only XDC in the rest of the experiments.
Finally, to show that XDC works for different backbones, we re-do Study 1 with ResNet3D in the \emph{supplementary material}.

\noindent {\bf Study 2: The number of -means clusters.} 
This study explores the effects of changing the hyperparameter  in -means clustering. We pretrain XDC on three datasets, Kinetics, AudioSet-K, and AudioSet, using = and  clusters (Table~\ref{table:ablation_study_k}). 
\textbf{\textit{Observations:}}
\textbf{{(I)}} The best  value is not sensitive to the number of semantic labels in the downstream datasets. For example, HMDB51 and ESC50 have about the same number of labels but different best  value.
\textbf{{(II)}} Similarly, the best  value seems uncorrelated with the number of original semantic labels of the pretraining dataset, \emph{e.g.}  in Kinetics. We reiterate here that our approach is self-supervised and \textbf{does not use} the labels of the pretraining dataset.
\textbf{{(III)}} The best  value tends to get larger as the pretraining data size increases. For example, the best  for HMDB51 shifts from  to  when moving from pretraining on AudioSet-K to the full AudioSet. We hypothesize that there is a more diverse sample set to cluster when the pretraining data size increases. Thus, we can have more fine-grained clusters (higher ) and make our self-supervised classification problem harder. This aligns with previous self-supervised works~\cite{goyal2019scaling,avts} that showed benefits from making the pretext task harder.
\begin{table}[t!]
    \small
    \centering
    \caption{\textbf{The number of clusters ().} We show the effect of the number of -means clusters on XDC performance. XDC is pretrained on three large datasets, and then fully finetuned on three downstream tasks. We report the top-1 accuracy on split-1. The best  value increases as the size of the pretraining dataset increases.}
    \begin{tabular}{l | l | c c c c c }
        \hline
        \multirow{2}{7em}{Pretraining\\Dataset} & \multirow{2}{6em}{Downstream\\Dataset} & \multicolumn{5}{c}{} \\
        & & 64 & 128 & 256 & 512 & 1024 \\ 
        \hline
        \multirow{3}{7em}{Kinetics\2402240240240265659.8\%22.2\%24.1\%5.1\%0.6\%8116655.2\%2.1\%1.9\%11665\Delta\Delta\Delta\pm\pm\pm210k22^{*}10^{\dagger}10^{*}^{\dagger}^{*}^{\dagger}L^3L^3323232\_601.7\%1.5\%1.3\%3.8\%0.6\%0.3\%3.9\%5.6\%24076.9\%40.7\%1.1\%kkkkk\gamma\gamma3\gamma0.0110^{-4}10^{-5}10^{-4}10^{-4}10^{-4}10^{-4}8320.00250.01\gamma0.010.0050.0050.0050.001, 0.002, 0.004, 0.006, 0.008, 0.010.001, 0.002, 0.004, 0.006, 0.008, 0.01, 0.02, 0.0481/41051055$ most frequent concepts of each cluster. 

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/xdc_filters.pdf}
  \caption{\textbf{R(2+1)D filters learned with self-supervised XDC vs. fully-supervised training.} (a) R(2+1)D \texttt{conv\_1} filters learned by fully-supervised training on Kinetics. (b) The same filters learned by self-supervised XDC pretraining on IG-Kinetics. XDC learns a more diverse set of temporal filters compared to fully-supervised pretraining.
  }
  \label{fig:xdc_filters}
\end{figure}

\noindent{\bf XDC filters.} Figure~\ref{fig:xdc_filters} visualizes and compares \texttt{conv\_1} spatial and temporal filters of R(2+1)D learned by self-supervised XDC pretraining on IG-Kinetics versus fully-supervised pretraining on Kinetics. We observe some differences in both spatial and temporal filters between XDC and fully-supervised pretraining. In particular, XDC learns a more diverse set of motion filters.
 
\end{document}
