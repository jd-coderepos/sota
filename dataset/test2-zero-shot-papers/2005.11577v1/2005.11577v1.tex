\documentclass{article}


\usepackage{arxiv}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{lipsum}
\usepackage{graphicx}
\usepackage{multirow}

\usepackage{listings}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{varwidth}
\usepackage{array}  













\graphicspath{ {./figures/} }


\title{PhyAAt: \textbf{Phy}siology of \textbf{A}uditory \textbf{At}tention to Speech Dataset}

\date{}
\author{
  Nikesh~Bajaj, Jes\'us~Requena~Carri\'on, Francesco Bellotti\\
  Centre for Intelligent Sensing, School of Electronics Engineering and Computer Science (EECS),\\
  Queen Mary University of London, London, UK\\
  Elios Lab, Dipartimento di Ingegneria Navale, Elettrica, Elettronica e delle\\ Telecomunicazioni (DITEN), University of Genoa, Italy\\
  \texttt{\{n.bajaj, j.requena\}@qmul.ac.uk, franz@elios.unige.it}\\
}



\rhead{\href{https://phyaat.github.io}{\textit{https://phyaat.github.io}}}
\begin{document}

\maketitle

\begin{abstract}
Auditory attention to natural speech is a complex brain process. Its quantification from physiological signals can be valuable to improving and widening the range of applications of current brain-computer-interface systems, however it remains a challenging task. In this article, we present a dataset of physiological signals collected from an experiment on auditory attention to natural speech. In this experiment, auditory stimuli consisting of reproductions of English sentences in different auditory conditions were presented to 25 non-native participants, who were asked to transcribe the sentences. During the experiment, 14 channel electroencephalogram, galvanic skin response, and photoplethysmogram signals were collected from each participant. Based on the number of correctly transcribed words, an attention score was obtained for each auditory stimulus presented to subjects. A strong correlation () between the attention score and the auditory conditions was found. We also formulate four different predictive tasks involving the collected dataset and develop a feature extraction framework. The results for each predictive task are obtained using a Support Vector Machine with spectral features, and are better than chance level. The dataset has been made publicly available for further research, along with the python library \textit{phyaat} to facilitate the preprocessing, modeling, and reproduction of the results presented in this paper. The dataset and other resources are shared on webpage - \href{https://phyaat.github.io}{{\it https://phyaat.github.io}}.









\end{abstract}

\keywords{Auditory attention \and Physiological signals \and EEG \and GSR \and PPG \and Attention of speech \and Predictive modeling \and PhyAAt}


\section{Introduction}
\label{S:Intro}
Auditory attention is a cognitive process of great importance that still remains poorly understood \cite{AudAtten2000}. It plays a vital role in many human activities, from aircraft flying to learning in a classroom environment. Thus, understanding attention can be important to improve the effectiveness, experience, and performance of many tasks \cite{SelAtte2001}. 
A better understanding of auditory attention could also lead to improved brain-computer-interface (BCI) systems that use emotional and mental states to execute actions.

Auditory attention and auditory perception are closely related processes; while perception is the process of interpreting information, attention is the process by which discrete pieces of information are selected or discarded \cite{styles2004attention}. Auditory perception is a complex brain process that involves encoding sounds into patterns of neural activity \cite{theunissen2004methods}. This process can also modulate other physiological responses, such as the heart rate and the sympathetic tone. The brain activity can be investigated in the Electroencephalogram (EEG), and the heart rate and sympathetic response can be extracted from Photoplethysmogram (PPG) and Galvanic Skin Response (GSR) signals, respectively. Other physiological modalities such as the Electromyogram, the Electrooculogram, and the Electrocardiogram are also available. 

Physiological signals are used in many applications from healthcare to BCI system. Datasets of physiological signals are available for a wide variety of applications, from paralysis \cite{BCIPararylysis2008} and epilepsy \cite{andrzejak2012nonrandomness} studies, to applications for patients in a complete locked-in state \cite{BCISpeller2011, BCIAud2012, BCILockedin2011} to emotion recognition \cite{EmoReco2004, EmoPhys2005, EmoEEG2006, koelstra2012deap, EEGMusic2010} and serious games \cite{EmoGame2011, EEGgame2010, EEGgame2011}. Widely used EEG datasets for BCI applications include motor imagery datasets \cite{brunner2008bci, leeb2007brain, shin2017open}. There are several attention related datasets, including datasets for covert and overt visual attention \cite{aloise2012covert, arico2014influence, treder2011brain} and for auditory-visual attention shift \cite{vceponiene2008modality}. Specifically for auditory attention based on EEG signals, an auditory oddball paradigm has been presented, where the response of participants to oddball sounds inserted in streams of sinusoidal tones was analysed \cite{halder2010auditory, schreuder2010new}.

Among the wide variety of physiological datasets available, to the best of our knowledge there is a lack of datasets designed to study auditory attention to natural speech. With the goal of supporting future research in the field of auditory attention, we have designed an experiment based on the dichotic listening task \cite{dichoticTask2008}, and collected a dataset of physiological signals that include 14 channel EEG, PPG and GSR. The dataset is available to the wider community for it to be used freely\footnote{https://phyaat.github.io}. Based on this dataset, a model could be trained to predict from physiological signals the attention level of individuals in different scenarios, for instance during the delivery of a lecture. This dataset does not restrict itself to auditory attention studies and can also be used to investigate other mechanisms involved in brain auditory processing of natural speech. One example of such applications is the design of the difficulty levels of a game based on the level of auditory attention \cite{bajaj2018auditory}. 

The rest of the article is organised as follows. Section \ref{S:Exp} explains the experimental design, materials and procedures. Section \ref{S:Dataset} describes the collected dataset, labels and file structure. In Section \ref{S:AAtScore}, we analyse the correlation between attention score and auditory conditions. In Section \ref{S:pTasks}, we formulate the four predictive tasks and Section \ref{S:fExtraction} describes the framework to extract the features from physiological responses. Section \ref{S:Results} demonstrates the results of the predictive tasks using the data from one subject. Section \ref{S:Conclusion} concludes the presented work and discusses several directions for future research. The details of resources made available are explained in the Resources section at the end of the paper. 



\section{Experiment}\label{S:Exp}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\textwidth]{figures/ExModel4.jpg}
    \caption{Experiment Design}
    \label{fig:ED}
\end{figure}

\subsection{Overview of the experiment design}
\label{ss:ExDesign}


An experiment based on diochotic listening task was designed to create the dataset for auditory attention analysis. The main components of this experiment are shown in Figure \ref{fig:ED}. Unlike the diochotic listening task, only one audio stimuli per trial was presented to a participant. Audio stimuli were reproduced under three different auditory conditions (,, and ) and following previous studies, the attention score  for each trial was computed from a transcribed audio message, by counting the number of correctly identified words \cite{auralPerception1967, dichoticTask1987, dichoticTask2007, dichoticTask2003}. While the establishing attention score, minor spelling errors and typos were ignored such as {\it did/dids}, {\it beautiful/beutiful}, {\it dog/dogs}. For the entire duration of the experiment, three different modalities of physiological responses () were recorded at the sampling rate of 128Hz. The details of the methods and materials are explained in subsequent subsections.


\subsection{Stimuli}
\label{ss:stumli}
A total collection of 5000 audio files, each containing semantically correct English language sentence was obtained from the Tatoeba Project \cite{tatoeba}. The length of stimuli ranges from 3 to 13 words per sentence. All the stimuli were reproduced by the same speaker to avoid variations in physical properties such as pitch, rate of speech, etc. From the collection of semantic stimuli, a total 1700 non-semantic stimuli were created by suitably inserting random isolated words in between. The following are examples of semantic and non-semantic stimuli:

In the above examples, stimuli \textbf{Tx1} and \textbf{Tx2} are semantic and stimuli \textbf{Tx3} and \textbf{Tx4} are non-semantic stimuli generated by inserting the words highlighted in bold font. From each group of semantic and non-semantic stimuli, six additional groups of stimuli were created by adding different levels of background noise. The signal to noise ratios (SNR) of each background noise level were -6 dB, -3 dB, 0 dB, 3 dB, 6 dB and noise-free ( dB). In summary, we created two groups of audio stimuli, semantic and non-semantic, of lengths ranging from 3 to 13 words and six different levels of background noise was added to each audio stimuli during reproduction.







\subsection{Physiological signals}
We recorded three types of physiological responses from each participant, namely EEG, GSR, and PPG. A 14 channel Emotiv Epoc wireless device was used for recording EEG signals \cite{EmotivEpoc}. The EEG signals were collected using the API provided by the device manufacturer with the highest available sampling rate of 128Hz. The DC component was removed from EEG signals while recording. Two small copper plates placed on the index finger and middle finger were used to measure the GSR. The resulting GSR signal was low-pass filtered and recorded at a rate of 128 Hz. Both the raw and low-pass filtered signals were recorded. 

A pulse sensor \cite{PPG} positioned on the ring finger was used to record the PPG response, which measures microvascular blood volume changes. Pulse rate and the inter-beat interval (IBI) were obtained from the PPG response using software code provided by the sensor manufacturer \cite{PPGCode}. 

In summary, 19 signals streams were recorded from each participant, i.e. 14 EEG channels, 2 GSR streams (raw and low-pass filtered) and 3 PPG streams (raw signal, pulse rate, and IBI). All the physiological responses were labeled accordingly.  

\subsection{Participants}
\label{ss:participants}
A total of 25 university students, 21 male, 4 female, 1 left handed, 24 right handed,  participated in our study. All the participants were non-native English speakers (i.e. their first language was other than English) and had no known auditory processing disorder. The age of the participants ranged from 16 to 34 years. The majority of nationalities among the participants were Indian and Italian and the majority of the first languages were Arabic, Italian and Malayalam. 

\subsection{Experiment procedure}
\label{ss:Procedure}
After mounting all the sensors e.g. EEG headset, GSR plates, and PPG pulse sensor, participant were provided with a passive ear-phone and presented with a computer interface. Participants were then asked to enter demographic information that included nationality, gender, age-group, and first language. Participants were also asked to rate their English language skills in terms of listening, writing, reading, and speaking. Once all the information was entered, participants could initiate the experiment. 

A total of 144 stimuli were randomly selected without replacement, for each participant. Out of 144, 72 were selected from the semantic group and 72 from the non-semantic group. In each group, the 72 stimuli were equally split into six background noise levels, i.e. 12 stimuli were assigned to each noise level. Furthermore, the 12 stimuli within each noise level had different lengths. Specifically, 5 stimuli were short with an average of 4 words (L1), 4 stimuli had an average of 8 words (L2), and 3 stimuli were long, with an average of 12 words (L3). The chosen lengths followed closely previous studies \cite{auralPerception1967}. 
The variation in length within each category was  word and we assumed that this variation did not have a significant effect on perception, as suggested by previous studies \cite{primacyModel1998, WordLengthEffects1995}. 


The experiment thus consisted of 144 trials per participant, during which they had to transcribe one auditory stimulus. Each trial was divided into three tasks, namely \textit{Listening, Writing,} and \textit{Resting}. The timeline of a trial is shown in Figure \ref{fig:ex_timeline}. A participant could actively start a trial by pressing the play button on the computer interface, after which a listening task would start by reproducing one of the 144 audio stimuli. Once the audio was finished, participants were allowed to write the response in a text-box. Participants were not allowed to replay any stimuli and the interface remained disabled while stimuli were being played. After the transcription was written, participants could press the submit button to save the response and end the writing task. To start new trial, participants had to press the play button again. The time period between writing and the next listening task was labeled as resting. 

Participant were explained the entire procedure beforehand. The average time taken for a participant to complete the experiment was  minutes. Figure \ref{fig:ex_participant} shows one of the participants conducting the experiment.

\begin{figure}[t]
     \centering
     \begin{subfigure}[b]{0.40\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/ExperimentProcedure.jpg}
         \caption{A participant during the experiment}
         \label{fig:ex_participant}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.53\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/ExTimeline4.png}
         \caption{Timeline of a trial}
         \label{fig:ex_timeline}
     \end{subfigure}
        \caption{Experiment procedure}
        \label{fig:ex_procedure}
\end{figure}

\section{Collected dataset}

\label{S:Dataset}
\begin{figure}[ht]
 \centering
    \includegraphics[width=0.88\textwidth]{figures/SegmentOfSignalsV6.jpg}
    \caption{A segment of collected physiological responses with corresponding labels. Green color background is corresponds to listening task, white for writing, and blue for resting.}
    \label{fig:signals}
\end{figure}

The collected dataset includes the physiological responses of the 25 participants. Signals and time segments are conveniently labeled so as to identify auditory conditions and tasks, and the computed attention scores for each stimuli is also included. A segment of physiological responses and corresponding labels is shown in Figure \ref{fig:signals} and includes the 14 EEG channels, the low-pass GSR signal and the raw PPG response. Listening, writing, and resting segments are identified by different background colors (green, white, and blue, respectively) and the attention score for listening segments is also shown. A summary of collected dataset is described in Table \ref{table:Summary}. 

   

As shown in Figure \ref{fig:file_stcr}, the collected dataset is structured in a directory containing 25 sub-directories, one for each participant, named as \textit{S1, S2, ... S25}. Each sub-directory contains two files namely \textit{S[x]\_Signals.csv} and \textit{S[x]\_Textscores.csv}, where {\it [x]} is the participant ID, e.g. {\it S1} corresponds to the participant 1. The signal file includes 19 streams of signals, time-stamped, and three labels, namely \textit{Label\_N, Label\_S}, and \textit{Label\_T}, corresponding to noise level, semanticity, and task respectively. For the noise level  dB, the numerical value 1000 is used. Semanticity is encoded as 0 and 1, for semantic and non-semantic respectively. Tasks are encoded as 0, 1, and 2 for listening, writing, and resting respectively. The labels before first trial are set to -1. The text score file includes the attention score (\textit{Label\_A}) for each trial, along with noise level, semanticity, length of stimulus, and timestamp. The dataset has been anonymised and the time-stamp in each files are set to the same time, i.e. 00 hours. The dataset also contains the demographic information and English level self-rating score of each participant. Finally, a {\it ReadMe} file is included. 

A python library \textit{phyaat} has been created to allow users to download the dataset and extract the segments with the corresponding labels to use for predictive modeling. User guide and instructions are also provided \footnote{\url{https://phyaat.github.io/introduction}}.

\begin{table}[ht]
\begin{minipage}[b]{0.55\linewidth}
\centering
    {\renewcommand{\arraystretch}{1.3}
    \centering
    \caption{Dataset summary}
\begin{tabular}{p{3cm}|p{6cm}}
    \hline
\hline
    Types of signal &  EEG, GSR, PPG\\
    EEG channels & 14 channels;  AF3, AF4, F3, F4, FC5, FC6, F7, F8, T7, T8, P7, P8, O1, O2\\
    Total signals streams & 19 streams;  14 from EEG, 2 from GSR- instant sample and moving averaged, 3 from PPG - raw signal, beats per minutes (BPM), interval between beats (IBI)\\
    Sampling rate & 128Hz \\
    Participants & 25 participants; 21 male, 4 female \\
    Number of stimuli per participants & 144 randomly selected stimuli, 72 semantic and 72 non-semantic\\
    Independent variables & Noise level, Semanticity, Length of stimulus\\
    Noise levels & 6 levels; -6, -3, 0, 3, 6 and  dB SNR\\
    Semanticity levels & 2 levels; 0-semantic, 1-non-semantic\\
Length of stimulus & 3 to 13 words per stimulus, grouped into three categories; L1 (small), L2 (medium) and L3 (long) \\
    Average duration of a stimuli &  sec \\
    Average duration of entire recording of a participant &  mins\\
    Self-rating & Writing, listening, reading and speaking skill of English language, rating scale 1-5\\
\hline
    \hline
    \end{tabular}
    \label{table:Summary}
    }
\end{minipage}\hfill
\begin{minipage}[b]{0.4\linewidth}
    \centering
    \includegraphics[height=1.2\textwidth]{figures/Daataset1.png}
    \vspace*{.6in}
    \captionof{figure}{File structure of dataset directory}
    \label{fig:file_stcr}
\end{minipage}
\end{table}

\section{Analysis of Attention Score}
\label{S:AAtScore}
The Spearman rank correlation between the attention score and the auditory condition (i.e. noise level, semanticity, and length of stimulus) was carried out and its results are shown in Figure \ref{fig:att_score}. It can be observed that attention score has a strong positive correlation () with the background noise level and a negative correlation with the semancity () and length of the stimulus (), all with a p-value . Furthermore, a repeated-measure ANOVA test was performed on the attention score considering each auditory conditions individually and jointly. The p-values were found to be low (), suggesting a significant impact of the auditory conditions on the attention scores.

\begin{figure}[ht]
     \centering
     \begin{subfigure}[b]{0.39\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/LR_Att_SNR.png}
         \caption{Noise level}
         \label{fig:att_noise}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/LR_Att_Sem.png}
         \caption{Semanticity}
         \label{fig:att_sem}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/LR_Att_Len.png}
         \caption{Length of stimulus}
         \label{fig:att_len}
     \end{subfigure}
    \caption{Correlation of attention score and auditory conditions, }
    \label{fig:att_score}
\end{figure}

\section{Formulation of predictive tasks}
\label{S:pTasks}
Based on the analysis of the attention score and the experiment design (Figure \ref{fig:ED}), four different predictive tasks have been formulated. 

\subsection{\textbf{T1}: Attention score prediction}
\label{ss:T1}

In this task, the objective is to estimate the auditory attention level  of a participant based on the physiological signals recorded during the listening segment. Given a set of features  extracted from the listening segment this objective can be formulated as finding a model  such that the attention score can be approximated by the quantity

The model  is defined as the one minimising the expected risk 

where  is the chosen loss function and  is the expectation operator. The choice of the loss function can be defined as the mean square error, the mean absolute error or a combination of the two, such as the Huber loss.



The analysis of the attention score (c.f. Section \ref{S:AAtScore}) shows that there is a strong correlation between attention score and auditory conditions, such as noise level and semanticity. Therefore our knowledge about the auditory conditions can be informative for predicting the auditory attention. Although in general auditory conditions are not known, in our controlled experiment we do have access to their exact values. Consequently, we can formulate a predictive task for the auditory attention, that use the value of the auditory conditions as input.



\subsection{\textbf{T2}: Noise level prediction}
\label{ss:T2}
Assuming that the background noise level influences the physiological responses of a person, a model  can be obtained to provide an estimation  of the noise level experienced by a subject, based on the features extracted from the physiological responses during a listening segment, 
Since the defined noise level is an ordinal quantity, this predictive task can be formulated as a regression problem with a loss functions similar to the ones suggested for the predictive task \textbf{T1}. Alternatively, this predictive task can be formulated as a classification problems, where each of the six levels of noise constitute different classes.

\subsection{\textbf{T3}: Semanticity of stimulus prediction}
\label{ss:T3}
Assuming that the semanticity of a stimulus  modulates the physiological responses of a subject, a model  can be obtained to estimate the semanticity experienced by subject from the set of features :

Since semanticity is a binary valued auditory condition, this task can be formulated as a classification problem. Choices of loss functions include the cross-entropy or Hinge loss.

\subsection{\textbf{T4}: LWR classification} \label{ss:T4}
Tasks \textbf{T1}, \textbf{T2} and \textbf{T3} are based on listening segments only and therefore assume that the subject not engaged in any activities other than listening. However, there are scenarios where we might be interested in evaluating the activity that a subject might be engaged in.
The objective of the \textit{LWR}-classification task is to predict which activity  is being performed by the subject, considering listening, writing or resting as the candidate activities. Similar to other predictive tasks, this can be modeled as to predict  for unseen data, as follows

where  is a model that uses a set of features  extracted from a segment of physiological responses. 










\section{Feature extraction framework} \label{S:fExtraction}

In the context of the experiment described in Section \ref{ss:Procedure}, a segment corresponds to a continuous time interval during which a complete task (listening, reading or resting) is executed. By processing the physiological signals recorded during an entire segment, a set of features  can be extracted and used as an input in any of the predictive tasks \textbf{T1} through \textbf{T4}. Other time intervals that do not map to complete tasks could also be used to extract features. We will call these time intervals windows, to distinguish them from our definition of segment. A graphical illustration of segment-wise and window-wise feature extraction is shown in Figures  \ref{fig:fe_segment} and \ref{fig:fe_window}, respectively.
Different models could be built to estimate the target predictions based on features extracted from short windows or from an entire segment. An interesting question is to determine the minimum window length that will produce a set of features  that contain enough information to produce an accurate prediction.







By allowing moving windows, window-wise feature extraction could be used to train temporal models, such as Dynamic Bayesian Network or RNN, for predictive tasks \textbf{T1} and \textbf{T3}. Features extracted from each window could be modelled as non-Independent and Identically Distributed (IID) samples. In the case of predictive tasks \textbf{T2} and \textbf{T4}, window-wise feature extraction could be used to train temporal or non-temporal models and the set of features extracted from each window could be treated as IID samples instead.


\begin{figure}[ht]
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/FeatureExtraction_Seg.png}
         \caption{Segment-wise feature extraction}
         \label{fig:fe_segment}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/FeatureExtraction_Win.png}
         \caption{Window-wise feature extraction}
         \label{fig:fe_window}
     \end{subfigure}
        \caption{Feature extraction framework}
        \label{fig:feature_ext}
\end{figure}

\section{Results of predictive tasks}
\label{S:Results}
In this section a basic SVM model is created for each of the predictive tasks defined in Section \ref{S:pTasks}, by using data from a single subject, namely \textit{S1}. First the EEG signals from subject S1 are pre-processed by using a high-pass filter with cutoff frequency of  and artifact are removed by using an automatic and tunable wavelet approach \cite{bajaj2020automatic} with default parameter setting, namely  and . After the pre-processing stage,  six spectral features are extracted from each segment from the 14 EEG channels. The spectral features correspond to the total power in the commonly used EEG frequency bands,
namely delta (0.5–4 Hz), theta (4–8 Hz), alpha (8–14 Hz), beta (14–30 Hz), low gamma (30–47 Hz) and high gamma (47–64 Hz).The spectral power in each frequency band is computed based on Welch's method. In summary, a total of 84 spectral features () are extracted from each EEG segment. 

During the modeling stage, SVM with RBF kernel are used. Task \textbf{T1} and \textbf{T2} are modeled with a Support Vector Regressor. In task \textbf{T2}, noise level  dB is mapped to 10 dB. Task \textbf{T3} and \textbf{T4} are modeled with Support Vector Classifier. Mean absolute error (MEA) and accuracy are used as performance metrics for regression (\textbf{T1} \& \textbf{T2}) and classification (\textbf{T3} \& \textbf{T4}), respectively.

Since the sample size for each task (i.e.  for \textbf{T1, T2, T3} and  for \textbf{T4}) is small, K-Fold cross validation with  folds is used and to avoid overfitting a regularization parameter  is included. The average training and testing performance for each task together with the final value of the parameter  are reported in Table \ref{table:results} and shown in Figure \ref{fig:ptask_results}. It can be observed from Figure \ref{fig:ptask_results} that performance for each predictive task is better than chance level measure. Since the class ratio is balanced for classification tasks, the chance level is 1/number of classes (e.g. 1/2 for \textbf{T3} and 1/3 for \textbf{T4}). For regression, the standard deviation  is shown in Figure \ref{fig:ptask_results}. These results are reproducible and the corresponding python scripts are provided as part of the \textit{phyaat} library. 
\begin{table}[h!]
\caption{Results of basic modeling using SVM for each predictive task with subject \textit{S1}}
\label{table:results}
\centering
\begin{tabular}{l| l|c c |l}
\toprule
\textbf{Predictive Task} &\textbf{SVM} &  Training & Testing & Metric\\
\toprule
T1: Attention score &  & 23.80 & 29.65 & \multirow{2}{1.3cm}{MAE}\0.1cm]
\cline{1-4}
\noalign{\smallskip}
T4: LWR && 0.97 & 0.81 & \\
\bottomrule
\end{tabular}
\end{table}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.88\textwidth]{figures/phyaat_Results_pTasks_S1.png}
    \caption{Results of basic modeling using SVM for each predictive task with subject \textit{S1}}
    \label{fig:ptask_results}
\end{figure}

\section{Conclusions and Future work}
\label{S:Conclusion}
In this paper, we have presented a dataset of physiological signals for predictive analysis of auditory attention to natural speech. Our dataset is novel in its kind and contains three different physiological signals, namely EEG, GSR, and PPG. Signals were recorded during an experiment conducted with 25 subjects, which involved three tasks (listening, writing and resting) executed in 144 trials . In the listening stage, subjects were presented with an audio stimulus, which is to be transcribed in following writing stage, followed by a resting before the subsequent trial. In each trial, audio stimuli were presented in different auditory conditions, which included background noise level, semanticity, and length of the stimulus. The attention score from transcription of each trial was computed. Based on the collected dataset and experiment design, four predictive tasks have been formulated and two approaches of feature extraction have been proposed. For demonstration purposes, SVM models using EEG spectral features have been created for each predictive tasks using EEG data from a single subject. The correlation analysis of the attention score and the results of the predictive tasks demonstrate the potential of the collected dataset.

In order to advance the research in the field of auditory attention, the collected dataset has been made openly available to the wider research community, together with supporting tools (See Resources section below) To improve the performance of the predictive tasks, more sophisticated models such as DBN, RNN and LSTM can be used with spectral, wavelet-based and other features. Our dataset provides an excellent opportunity to investigate the auditory attention mechanisms of the brain. 

\section*{Resources}\label{S:Resources}
The dataset and supporting source code is distributed as a python library \textit{phyaat} (\href{https://pypi.org/project/phyaat/}{\textcolor{blue}{https://pypi.org/project/phyaat/}}) on the Python Package Index (PyPI) platform. A user guide and scripts to reproduce the results presented in this paper, can be downloaded from the project homepage (\href{https://phyaat.github.io/}{\textcolor{blue}{https://phyaat.github.io/}}). Supporting functions in the library include pre-processing, feature extraction and modeling functions.

\bibliographystyle{unsrt}
\bibliography{references}








































\end{document}
