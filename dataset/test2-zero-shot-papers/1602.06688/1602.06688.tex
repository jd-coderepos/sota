\documentclass[12pt,a4paper]{scrartcl}

\evensidemargin 0.0in
\oddsidemargin 0.0in
\topmargin 0.0in
\textwidth 6.5in
\textheight 8.0in
\headheight 0.0in
\headsep 0.0in
\topskip 0.0in
\textheight 9.0in

\usepackage{makeidx}
\usepackage{comment}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage[noend]{algpseudocode}
\usepackage{url}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{latexsym}

\newtheorem{Theorem}{Theorem}
\newtheorem{proof}{Proof}
\newtheorem{Lemma}{Lemma}
\newtheorem{Corollary}{Corollary}
\newtheorem{Proposition}{Proposition}
\newtheorem{Characterization}{Characterization}
\newtheorem{Problem}{Problem}
\newtheorem{Example}{Example}
\newtheorem{Definition}{Definition}

\newtheorem{fact}{Fact}
\newtheorem{prob}{Problem}

\newenvironment{Proof}{\trivlist \item[\hskip \labelsep{\bf proof.\/}]}{\hspace{\fill}\endtrivlist }

\newcommand{\logstar}{\lg^*\hspace{-.9mm}}

\begin{document}

\title{siEDM: an efficient string index and search algorithm for edit distance with moves}
\author{Yoshimasa Takabatake, Kenta Nakashima, Tetsuji Kuboyama, \\Yasuo Tabei and Hiroshi Sakamoto\thanks{mailto: hiroshi@ai.kyutech.ac.jp}\\
{\small  Kyushu Institute of Technology, Japan}\\
{\small  Gakushuin University, Japan}\\
{\small  PRESTO, Japan Science and Technology Agency, Japan}
}
\date{\empty}
\maketitle


    \abstract{ Although several self-indexes for highly repetitive text
      collections exist, developing an index and search algorithm with editing
      operations remains a challenge.  \emph{Edit distance with moves (EDM)}
      is a string-to-string distance measure that includes substring moves in
      addition to ordinal editing operations to turn one string into another.
      Although the problem of computing EDM is intractable, it has a wide
      range of potential applications, especially in approximate string
      retrieval.
      Despite the importance of computing EDM, there has been no efficient
      method for indexing and searching large text collections based on the
      EDM measure.  We propose the first algorithm, named \emph{string index
        for edit distance with moves (siEDM)}, for indexing and searching
      strings with EDM.
      The siEDM algorithm builds an index structure by leveraging the idea
      behind the \emph{edit sensitive parsing (ESP)}, an efficient algorithm
      enabling approximately computing EDM with guarantees of upper and lower
      bounds for the exact EDM.
      siEDM efficiently prunes the space for searching query strings
      by the proposed method, which enables fast query searches with
      the same guarantee as ESP.  We experimentally tested the ability
      of siEDM to index and search strings on benchmark datasets, and
      we showed siEDM's efficiency.  }


\section{Introduction}

The vast amounts of text data are created, replicated, and modified with the
increasing use of the internet and advances of data-centric technology. Many
of these data contain repetitions of long substrings with slight differences,
so called \emph{highly repetitive texts}, such as Wikipedia and software
repositories like GitHub with a large number of revisions.  Also recent
biological databases store a large amount of human genomes while the genetic
differences among individuals are less than  percent, which results in
the collections of human genomes to be highly repetitive.  Therefore, there is
a strong need to develop powerful methods for processing highly repetitive
text collections on a large scale.

Building indexes is the \emph{de facto} standard method to search large
databases of highly repetitive texts.  Several methods have been presented for
indexing and searching large-scale and highly repetitive text collections.
Examples include the ESP-index~\cite{Takabatake14}, SLP-index~\cite{Claude09}
and LZ77-based index~\cite{GagieGKNP14}.  Recently, Gagie and
Puglisi~\cite{Gagie:2015bj} presented a general framework called kernelization
for indexing and searching highly repetitive texts.  Although these methods
enable fast query searches, their applicability is limited to exact match
searches.

The edit distance between two strings is the minimum cost of edit operations
(insertions, deletions, and replacements of characters) to transform one
string to another. It has been proposed for detecting evolutionary changes in
biological sequences~\cite{Durbin98}, detecting typing errors in
documents~\cite{Crochemore94}, and correcting errors on lossy communication
channels~\cite{Levenshtein96}.  To accelerate the quadratic time upper bound
on computing the edit distance, Cormode and Muthukrishnan introduced a new
technique called \emph{edit sensitive parsing (ESP)}~\cite{Cormode07}.  This
technique allows us to compute a modified edit distance in near linear time by
sacrificing accuracy with theoretical bounds.  The modified distance is known
as \emph{edit distance with moves (EDM)}~\cite{Cormode07}, which includes
substring move operations in addition to insertions and deletions.
While the exact computation of EDM is known to be intractable~\cite{Shapira07},
the approximate computation of EDM using ESP achieves a good approximation ratio
, and runs in almost linear time  for the
string length , where  denotes the logarithm of base two.

ESP is extended to various applications for highly repetitive texts.  Examples
are data compressions called grammar compression~\cite{SakamotoMKS09,
  Maruyama12, Maruyama13, Maruyama14}, indexes for exact matches~\cite{ESP,
  Takabatake14, Takabatake15}, an approximated frequent pattern
discovery~\cite{Nakahara13} and an online pattern matching for
EDM~\cite{Takabatake14-2}.  Despite several attempts to efficiently compute
EDM and various extensions of ESP, there is no method for indexing and
searching texts with EDM.  Such a method is required in bioinformatics where
approximated text searches are used to analyze massive genome sequences.
Thus, an open challenge is to develop an efficient string index and search
algorithm for EDM.

We propose a novel method called siEDM that efficiently indexes massive text,
and performs query searches for EDM.  As far as we know, siEDM is the first
string index for searching queries for EDM.  A space-efficient index structure
for a string is built by succinctly encoding a parse tree obtained from ESP,
and query searches are performed on the encoded index structures.  siEDM
prunes useless portions of the search space based on the lower bound of
EDM without missing any matching patterns, enabling fast query searches.
As in existing methods, similarity searches of siEDM are approximate but have
the same guarantee of the approximation ratio as in ESP.

Experiments were performed on indexing and searching repetitive texts for EDM
on standard benchmark datasets.  The performance comparison with an online
pattern matching for EDM~\cite{Takabatake14-2} demonstrates siEDM's
practicality.


\section{Preliminaries}
\subsection{Basic notations}

Let  be a finite alphabet, and  be .  All elements
in  are totally ordered.  Let us denote by  the set of all
strings over , and by  the set of strings of length  over
, i.e.,  and an element in
 is called a -gram.  The length of a string  is denoted by
.  The empty string  is a string of length , namely
.  For a string , ,  and
 are called the prefix, substring, and suffix of , respectively.
The -th character of a string  is denoted by  for .
For a string  and interval  (), let
 denote the substring of  that begins at position  and ends at
position , and let  be  when .
For a string  and integer , let
 and .  
We assume a recursive enumerable set
 of variables with .  All elements in
 are totally ordered, where all elements in  must
be smaller than those in .  In this paper, we call a sequence of
symbols from  a string.  Let us define ,
and  for .  The iterated
logarithm of  is denoted by , and defined as the number of times
the logarithm function must be applied before the result is less than or equal
to , i.e., .

\subsection{Straight-line program (SLP)}

A context-free grammar (CFG) in Chomsky normal form is a quadruple
, where  is a finite subset of ,  is a
finite subset of , and  is the start
symbol.  An element in  is called a production rule.  Denote 
(resp. ) as a left symbol (resp. right symbol) on the right hand
side for a production rule with a variable  on the left hand side, i.e.,
.   for variable  denotes the
string derived from .  A grammar compression of  is a CFG  that
derives  and only .  The size of a CFG is the number of variables, i.e.,
 and let .

The parse tree of  is a rooted ordered binary tree such that (i) internal
nodes are labeled by variables in  and (ii) leaves are labeled by symbols in
, i.e., the label sequence in leaves is equal to the input string.  In
a parse tree, any internal node  corresponds to a production rule ,
and has the left child with label  and the right child with label .

\emph{Straight-line program (SLP)}~\cite{SLP} is defined as a grammar
compression over , and its production rules are in the form of
 where  and
.

\subsection{Rank/select dictionaries}

A rank/select dictionary for a bit string ~\cite{Jacobson89} supports the
following queries:  returns the number of occurrences of
 in ;  returns the position of
the -th occurrence of  in ;  returns
the -th bit in .  Data structures with only the  bits
storage to achieve  time rank and select queries~\cite{Raman07} have
been presented.

GMR~\cite{Golynski06} is a rank/select dictionary for large alphabets and
supports rank/ select/access queries for strings in .  GMR
uses  bits while
computing both rank and access queries in  times and
also computing select queries in  time.

\section{Problem}

We first review the notion of EDM.
The distance  between two strings  and  is 
the minimum number of edit operations to transform  into .
The edit operations are defined as follows:
\begin{enumerate}
\item Insertion: A character  is inserted at position  in , which generates ,
\item Deletion: A character is deleted at position  in , which generates ,
\item Replacement: A character is replaced by  at position  in , which generates ,
\item Substring move: A substring  is deleted from the position ,
  and inserted at the position  in , which generates
   for , and
   for .
\end{enumerate}

\begin{prob}[Query search for EDM] \label{prob:1}
For a string , a query  and a distance threshold , find all  
satisfying .
\end{prob}

Shapira and Storer~\cite{Shapira07} proved the NP-completeness of EDM and
proposed a polynomial-time algorithm for a restricted EDM.
Cormode and Muthukrishnan~\cite{Cormode07} presented an approximation
algorithm named ESP for computing EDM.  We present a string index and
search algorithm by leveraging the idea behind ESP for solving Problem~\ref{prob:1}.  
Our method consists of two parts: (i) an efficient index structure for a given string  and (ii) a fast
algorithm for searching query  on the index structure of  with
respect to EDM.  Although our method is also an approximation
algorithm, it guarantees upper and lower bounds for the exact EDM.  We
first review ESP in the next section and then discuss the two parts.

\section{Edit Sensitive Parsing~(ESP) for building SLPs}

\subsection{ESP revisit}

We review the edit sensitive parsing algorithm for building SLPs~\cite{SakamotoMKS09}.
This algorithm, referred to as ESP-comp, computes an SLP from an input sting .
The tasks of ESP-comp are to (i) partition  into  such that
 for each ,
(ii) if , generate the production rule  and replace  by 
(this subtree is referred to as a 2-tree), and 
if , generate the production rule  and  for ,
and replace  by  (referred to as a 2-2-tree),
(iii) iterate this process until  becomes a symbol.
Finally, the ESP-comp builds an SLP representing the string .

We focus on how to determine the partition .
A string of the form  with  and  is called a repetition.
First,  is uniquely partitioned into the form
  by its maximal repetitions, where each  is 
a maximal repetition of a symbol in , and each  contains no repetition.
Then, each  is called type1, each  of length at least  is type2, and any remaining  is type3.
If , this symbol is attached to  or  with preference  when both cases are possible.
Thus, if , each  and  is longer than or equal to two.
One of the substrings is referred to as .

Next, ESP-comp parses each  depending on the type.
For type1 and type3 substrings, the algorithm performs the \emph{left aligned parsing} as follows. 
If  is even, the algorithm builds 2-tree from  for each ;
otherwise, it builds a 2-tree from  for each  and 
builds a 2-2-tree from the last trigram . 
For type2 , the algorithm further partitions it into short substrings of length two or three by
\emph{alphabet reduction}~\cite{Cormode07}.

{\bf Alphabet reduction:} 
Given a type2 string , consider  and  as binary integers.
Let  be the position of the least significant bit, in which ,
and let  be the bit of  at the -th position.
Then,  is defined for any .
Because  is repetition-free (i.e., type2), the label string  is also type2.
If the number of different symbols in  is  (denoted by ), then .
For the , the next label string is iteratively computed until the final  satisfying
 is obtained.
 is called the \emph{landmark} if . 

The alphabet reduction transforms  into  such that any substring of  of 
length at least  contains at least one landmark because  is also type2.
Using this characteristic, the algorithm ESP-comp determines the bigrams  to be replaced
for any landmark , where any two landmarks are not adjacent, and then the replacement is deterministic.
After replacing all landmarks, any remaining maximal substring  is replaced by the left aligned parsing,
where if  =1, it is attached to its left or right block.

We give an example of the edit sensitive parsing of an input string in Figure~\ref{fig:reduction}-(i) and (ii).
The input string  is divided into three maximal substrings depending on the types.
The label string  is computed for the type2 string.
Originally,  is iteratively computed until .
This case shows that a single iteration satisfies this condition.
After the alphabet reduction, three landmarks  are found, and then each  is parsed.
Any other remaining substrings including type1 and type3 are parsed by the left aligned parsing
shown in Figure~\ref{fig:reduction}-(ii).
In this example, a dashed node denotes that it is an intermediate node in a 2-2-tree.
Originally, an ESP tree is a ternary tree in which each node has at most three children.
The intermediate node is introduced to represent ESP tree as a binary tree.

As shown in~\cite{Cormode07}, the alphabet reduction approximates the
minimum CFG as follows.  Let  be a type2 string containing a
substring  at least twice.  When  is sufficiently long
(e.g., ), there is a partition
 such that  and
each landmark of  within  is decided by only
.  This means the long prefix  of  is
replaced by the same variables, independent of the occurrence of
.

ESP-comp generates a new shorter string  of length from  to
, and it parses  iteratively.  Given a string , ESP builds
the ESP-tree of height  in  time and in
 space.  The approximation ratio of the
smallest grammar by ESP is ~\cite{SakamotoMKS09}.

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.8\textwidth]{reduction}
\end{center}
\vspace{0.5cm}
\caption{The edit sensitive parsing. In (i), an underlined  means a landmark,
and .
In (i) and (ii), a dashed node is corresponding to the intermediate node in a 2-2-tree.}
\label{fig:reduction}
\end{figure}

\subsection{Approximate computations of EDM from ESP-trees}

ESP-trees enable us to approximately compute EDM for two strings.
After constructing ESP-trees for two strings, 
their \emph{characteristic vectors} are defined as follows.
Let  be the ESP-tree for string .
We define that an integer vector  to be the characteristic vector if
 represents the number of times the variable  appears in  as the root of a 2-tree.
For a string ,  and its characteristic vector are illustrated in Figure~\ref{fig:esp}.
The EDM between two strings  and  can be approximated by -distance between two characteristic vectors 
 and  as follows:


Cormode and Muthukrishnan showed the upper and lower bounds on the
-distance between characteristic vectors for the exact EDM.
\begin{Theorem}[\cite{Cormode07}]\label{thm:approx}
For , 

\end{Theorem}

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.6\textwidth]{esp}
\end{center}
\vspace{-0.6cm}
\caption{Illustration of ESP-tree and characteristic vector.}
\label{fig:esp}
\end{figure}


\section{Index Structure for ESP-trees}\label{sec:isESP}
\subsection{Efficient encoding scheme}

\begin{figure}
\includegraphics[width=1.0\textwidth]{encoding}
\caption{Illustration of encoding scheme.}
\label{fig:encoding}
\end{figure}

siEDM encodes an ESP-tree built from a string for fast query searches.
This encoding scheme sorts the production rules in an ESP-tree such
that the left symbols on the right hand side of the production rules
are in monotonically increasing order, which enables encoding of these
production rules efficiently and supporting fast operations for
ESP-trees.  The encoding scheme is performed from the first and second
levels to the top level (i.e., root) in an ESP-tree.

First, the set of production rules at the first and second levels in
the ESP-tree is sorted in increasing order of the left symbols on the
right hand of the production rules, i.e.,  in the form of
, which results in a sorted sequence of these
production rules.  The variables in the left hand side in the sorted
production rules are renamed in the sorted order, generating a set of
new production rules that is assigned to the corresponding nodes in
the ESP-tree.  The same scheme is applied to the next level of the
ESP-tree, which iterates until it reaches the root node.

Figure~\ref{fig:encoding} shows an example of the encoding scheme for
the ESP-tree built from an input string .  
At the first and second levels in the ESP-tree, the set of production rules,
, 
is sorted in the lexicographic order of the left symbols on right hand sides of production rules,
which results in the sequence of production rules,
.  The variables on
the right hand side of the production rules are renamed in the sorted
order, resulting in the new sequence
, whose production
rules are assigned to the corresponding nodes in the ESP-tree.  This
scheme is repeated until it reaches level~4.

Using the above encoding scheme, we obtain a monotonically increasing
sequence of left symbols on the right hand side of the production
rules, i.e.,  in the form of . Let  be the increasing sequence;  can
be efficiently encoded into a bit string by using the gap-encoding and
the unary coding.  For example, the gap-encoding represents the
sequence  by , and it is further transformed
to the bit string  by unary coding.
Generally, for a sequence , its unary code  represents
 by .
Because the number of s and the number of s is  and , respectively, 
the size of  is  bits.  
The bit string is indexed by the rank/select dictionary.

Let  be the sequence consisting of the right symbols on the right
hand side of the production rules, i.e.,  in the form of
.   is represented using
 bits.  is indexed by
GMR~\cite{Golynski06}.

The space for storing  and  is
 bits in
total.   and  enable us to simulate fast queries on encoded
ESP-trees, which is presented in the next subsection.

\subsection{Query processing on tree}

The encoded ESP-trees support the operations
, , 
and , which are used in our search algorithm.
 returns the left child  of 
and can be implemented on bit string  in  time as
 and .
 returns the right child  of
 and can be implemented on array  in
 time as .

 and  return
sets of parents of  as left and right children, respectively,
i.e.,
 and
.  

Because  is a monotonic sequence, any  appears consecutively in .
Using the unary encoding of ,  is computed by
 in  time.  
 can be computed by repeatedly applying select operations for  on
 until no more  appear, i.e.,  for .  
Thus,  for  can be computed in  time.

\subsection{Other data structures}

As a supplemental data structure, siEDM computes the \emph{node
  characteristic vector}, denoted by , for each variable
: the characteristic vector consisting of the frequency of any
variable derived from .  The space for storing all node
characteristic vectors of  variables is at most  bits.
Figure~\ref{fig:encoding}-(V) shows an example of the node
characteristic vectors for ESP-tree in
Figure~\ref{fig:encoding}-(III).  In addition, let  be a set
of  and variables appearing in all the descendant nodes under
, i.e., .
Practically,  is represented by a sequence of a pair of
 and .  Additionally, because
 ( represents adding  to dimension ), the
characteristic vectors can be stored per level  of the
ESP-tree. The data structure is represented by a bit array 
indexed by a rank/select dictionary and the characteristic vectors
reduced per level  of ESP-tree.  is set to  for -th bit
if  is stored, otherwise it is .  Then,  can be
computed by -th characteristic vector if the -th bit
of  is ; otherwise,
.

Another data structure that siEDM uses is a non-negative integer
vector named \emph{length vector}, each dimension of which is the
length of the substring derived from the corresponding variable (See
Figure~\ref{fig:encoding}-(VI)).  The space for storing length vectors
of  variables is  bits.

From the above argument, the space of the siEDM's index structure for
 variables is

bits in total.

\section{Search Algorithm}\label{sec:sa}

\subsection{Baseline algorithm}

Given an ESP tree , the \emph{maximal subtree decomposition} of  
is a sequence  of variable in  defined recursively as follows.
 is the variable of the root of the maximal subtree satisfying that
 is its leftmost leaf and .
If , then  is the maximal subtree decomposition of .
Otherwise, let  be already determined and 
.
Then, let  be the variable of the root of the maximal subtree satisfying that
 is its leftmost leaf and .
Repeating this process until ,
the maximal subtree decomposition is determined.

Based on the maximal subtree decomposition, 
we explain the outline of the baseline algorithm, called online
ESP~\cite{Takabatake14-2}, for computing an approximation of EDM
between two strings.   is constructed beforehand.  
Given a pattern , the online ESP computes , and for each substring
 of length , it computes the approximate EDM as follows.
It computes the maximal subtree decomposition  of . 
Then, the distance  is approximated by
 because ESP-tree is balanced and
then .  This baseline
algorithm is, however, required to compute the characteristic vector
of  at each position .  Next, we improve the time and space
of the online ESP by finding those -grams for each variable 
in  instead of each position .

\subsection{Improvement}

The siEDM approximately solves Problem~\ref{prob:1} with the same
guarantees presented in Theorem~\ref{thm:approx}.  Let
 such that .  There are
-grams formed by the string
 with
.  Then, the variable  is said to
\emph{stab} the -grams.  The set of the -grams stabbed by
 is denoted by .  Let  be the
set of  for all  appearing in .  An
important fact is that  includes any -gram in
.  Using this characteristic, we can reduce the search space .

If a -gram  is in , there exists a maximal
subtree decomposition .  Then, the
-distance of  and  guarantees
the same upper bounds in the original ESP as follows.

\begin{Theorem}\label{thm:newapprox}
  Let  be a -gram on  and
   be its maximal subtree
  decomposition in the tree .  Then, it holds that

\end{Theorem}
\begin{Proof}
  By Theorem~\ref{thm:approx},
  .  On the other
  hand, for an occurrence of  in , let  be the smallest
  subtree in  containing the occurrence of , i.e.,
  .  For  and , let  and
   be the sequences of the level 2 symbols in  and
  , respectively.  By the definition of the ESP, it holds that
   and  for some
  strings satisfying , and
  this is true for the remaining string  iteratively.  Thus,
   since the trees are balanced.
  Hence, by the equation

we obtain the approximation ratio.
\end{Proof}
To further enhance the search efficiency, we present a lower bound of the
-distance between characteristic vectors, which can be used for reducing
the search space.

\begin{Theorem}[A lower bound ]\label{thm:lower}
For any , 
the inequality  holds where

\end{Theorem}
\begin{Proof}
  The  distance between  and  is divided into four classes of terms: (i) both members in  and  are non-zero, 
(ii) both members in  and  are zero, 
(iii) the members in  and  are zero and non-zero, 
(iv) the members in  and  are non-zero and zero, respectively.  
Terms consisting of class (iii) and (iv) can be written as , which is a lower bound of the -distance.  
Thus, we obtain the inequality . 
\end{Proof}
 

\begin{Theorem}[Monotonicity of ] \label{thm:mon}
If a variable  derives , the inequality  holds. 
\end{Theorem}
\begin{Proof}
Every entry in  is less than or equal to the corresponding entry in . Thus, the inequality holds. 
\end{Proof}

\subsection{Candidate finding}

By Theorems \ref{thm:newapprox}, \ref{thm:lower} and \ref{thm:mon},
the task of the algorithm is reduced to finding a maximal subtree
decomposition  within .  Given
a threshold , for each -gram in ,
the algorithm finds the \emph{candidate}: the maximal subtree
decomposition  satisfying
.

For an  and an occurrence of some -gram in ,
the -gram is formed by the expression

for a  .  
The algorithm computes the maximal
subtree decompositions  for 
 and
 for
, and outputs
 covering the -gram when
.
We illustrate the computation of candidates satisfying 
 in Figure~\ref{fig:search}
and show the pseudo-code in Algorithm~\ref{alg:search}.

Applying all variables to Algorithm~\ref{alg:search} enables us to
find the candidates covering all solutions.  There are no
possibilities for missing any -grams in  such
that the -distances between their characteristic vectors and
 are at most , i.e., false negatives.  The set may include
a false positive, i.e., the solution set encodes a -gram such
that the -distance between its characteristic vector and 
is more than .  However, false positives are efficiently removed
by computing the -distance
 as a post-processing.

\begin{figure}
\begin{center}
\includegraphics[width=0.63\textwidth]{search}
\caption{Illustration of candidate finding and -distance computation.}
\label{fig:search}
\end{center}
\end{figure}


\begin{Theorem}\label{thm:fc}
The time of {\sc FindCandidates} is .
\end{Theorem}
\begin{Proof}
Because the height of the ESP-tree is , 
for each variable , the number of visited nodes is .
The computation time of  and  is , and the
time of {\sc FindLeft} and {\sc FindRight} is .
Thus, for  iterations of the functions, the total computation time is .
\end{Proof}

\begin{algorithm}
  \caption{ to output the candidate  for ,
    a query pattern  and a distance threshold .  }
\label{alg:search}
{\footnotesize 
\begin{algorithmic}[1]
\Function{FindCandidates}{,,}
\For{}
\State  \Comment{Initialize solution set}
\State {\sc FindLeft}() \Comment{for left child}
\State {\sc FindRight}() \Comment{for right child}
\If{ and }
\State Output 
\EndIf
\EndFor
\EndFunction
\Function{FindLeft}{}
\If{}
\State return 
\ElsIf{}
\State return 
\ElsIf{}
\State ~\mbox{if }
\State 
\State return 
\ElsIf{}
\State 
\State 
\State return 
\EndIf
\State  {\sc FindLeft}()
\If{}
\State return {\sc FindLeft}
\EndIf
\EndFunction
\Function{FindRight}{}
\If{}
\State return 
\ElsIf{}
\State return 
\ElsIf{}
\State  \mbox{if }
\State 
\State return 
\ElsIf{}
\State 
\State 
\State return 
\EndIf
\State  {\sc FindRight}()
\If{}
\State return {\sc FindRight}
\EndIf
\EndFunction
\end{algorithmic}
}
\end{algorithm}

\subsection{Computing positions}

\begin{algorithm}
\caption{ to compute the set  of all occurrence of  on  for . 
}
\label{alg:position}
{\footnotesize 
\begin{algorithmic}[1]
\Function{ComputePosition}{}
\State  \Comment{Initialize solution set}
\State {\sc Recursion}(, )
\EndFunction
\Function{Recursion}{,}
\If{ is the root node}
\State 
\State return
\EndIf
\For{each } \Comment{ is the right child of }
\State {\sc Recursion}(,)
\EndFor
\For{each } \Comment{ is the left child of }
\State {\sc Recursion}(,)
\EndFor
\EndFunction
\end{algorithmic}
}
\end{algorithm}

The algorithm also computes all the positions of , denoted by
.  Starting from
, the algorithm goes up to the root in the ESP-tree built from .  
is initialized to  at .  If  through the pass from  to the
root is the parent with the right child  on the pass, non-negative
integer  is added to .  Otherwise, nothing is
added to .  When the algorithm reaches the root,  represents a start
position of  on , i.e., .  To
compute the set , the algorithm starts from  and goes up to the
root for each parent in  and
, which return sets of parents for .
Algorithm~\ref{alg:position} shows the pseudo-code.

\begin{Theorem}\label{thm:pc}
  The computation time of  is , where
   is the number of occurrences of  in .
\end{Theorem}
\begin{Proof}
Using the index structures of  and ,
we can traverse the path from any node with label  to the root of  counting the position.
The length of the path is .
\end{Proof}


\begin{Theorem}\label{thm:total}
The search time is  
using the data structure of size  bits.
\end{Theorem}
\begin{Proof}
The time for computing  and  is . 
The time for finding candidates is  by Theorem~\ref{thm:fc}. 
The time for computing positions is  by Theorem~\ref{thm:pc}.
Thus, the total time is .
The size of the data structure is derived by the results in Section~\ref{sec:isESP}. 
\end{Proof}

In Theorem~\ref{thm:total},
 and  are incomparable because 
 is possible for a highly repetitive string.

\section{Experiments}

\begin{table}
\begin{center}
  \caption{Summary of datasets.}
  \begin{tabular}{|l|c|c|c|}
    \hline
     Dataset        & Length &  & Size (MB) \\ 
\hline
     einstein &  &  &  \\
     cere     &  &    &  \\
\hline
  \end{tabular}
\label{tab:dataset}
\end{center}
\end{table}

We evaluated the performance of siEDM on one core of a quad-core Intel
Xeon Processor E5540 (2.53GHz) machine with 144GB memory.  We
implemented siEDM using the rank/select dictionary and GMR in
libcds\footnote{\url{https://github.com/fclaude/libcds}}.  We used two
standard benchmark datasets of einstein and cere from repetitive text
collections in the pizza \& chili
corpus\footnote{\url{http://pizzachili.dcc.uchile.cl/repcorpus.html}},
which is detailed in Table~\ref{tab:dataset}.  As a comparison method,
we used the online pattern matching for EDM called online ESP
(baseline)~\cite{Takabatake14-2} that approximates EDM between a query
 and substrings of the length of  of each position of an input
text.  We randomly selected  as the query pattern  for each
 and examined the performance.

\begin{table}
\begin{center}
  \caption{
Comparison of the memory consumption for the query search
}
\begin{tabular}{|l|r|r|}
    \hline
     Dataset                   & {\bf einstein} & {\bf cere} \\ \hline
     siEDM~(MB) &  &  \\
     baseline~(MB) &  &  \\  
     \hline
  \end{tabular}
\label{tab:memory}
\end{center}
\end{table}

\begin{table}
\begin{center}
  \caption{
Comparison of the index size and construction time
}
\begin{tabular}{|l|l|r|r|}
    \hline
     \multicolumn{2}{|c|}{Dataset} & {\bf einstein} & {\bf cere} \\ \hline

                & Encoded ESP-tree~(MB)   &  &  \\
     Index Size & Characteristic vector ~(MB)         &  &  \\
                & Length vector ~(MB)  &  &  \\
\hline
     \multicolumn{2}{|c|}{Construction time~(sec)} &  &   \\
\hline
  \end{tabular}
\label{tab:construct}
\end{center}
\end{table}

Table~\ref{tab:memory} shows the memory consumption in the search of the siEDM and baseline.
The memory consumption of siEDM was larger than the baseline for both texts
because the baseline does not have characteristic vectors of each node and length vector. 

Table~\ref{tab:construct} shows the size for each component of the index structure and the time 
for building the index structure on einstein and cere datasets. 
Most of the size of the index structure was consumed by the characteristic vector . 
The index size of cere was much larger than that of einstein. 
The index sizes of cere and einstein were approximately 16 megabytes and 256 megabytes, respectively,
because the number of variables generated from cere was much larger than that generated from einstein. 
The number of variables generated from einstein was  and the number of variables generated from cere was . 
The construction times of the index structures were  seconds for einstein and  seconds for cere. 
The results for constructing the index structures demonstrate the applicability of siEDM to moderately large, repetitive texts. 


\begin{comment}
\begin{table}
\begin{center}
  \caption{
Comparison of the search time
}
\begin{tabular}{|l|r|r|r|r|r|r|r|}
    \hline
     \multicolumn{2}{|c|}{Dataset}& \multicolumn{4}{c|}{\bf einstein} \\ \hline
     \multicolumn{2}{|c|}{Query length} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} \\ \hline
     \multicolumn{2}{|c|}{Method} & siEDM & Baseline & siEDM & Baseline \\ \hline 
     \multirow{6}{*}{search time~(sec)}                  &   &   &  & &  \\
					 &  &   &  & &  \\
                     &  &   &  & &  \\
                     &   &   &  & &  \\
                     &  &   &  & &  \\
                     &  &   &  & &  \\
     \hline
  \end{tabular}
\label{tab:searchx}
\end{center}

\begin{center}
 \begin{tabular}{|l|r|r|r|r|r|r|r|}
    \hline
     \multicolumn{2}{|c|}{Dataset} & \multicolumn{4}{c|}{\bf cere} \\ \hline
     \multicolumn{2}{|c|}{Query length} &\multicolumn{2}{c|}{} & \multicolumn{2}{c|}{}\\ \hline
     \multicolumn{2}{|c|}{Method} & siEDM & Baseline & siEDM & Baseline \\ \hline 
     \multirow{6}{*}{search time~(sec)}                  &   &  &  & & \\
                     &   &  &  &  & \\
					 &   &  &  &  & \\
                     &   & NA  &  & &  \\
                     &  & NA  &  & NA&  \\
                     &  & NA  &  & NA&  \\                     
     \hline
  \end{tabular}
\end{center}

\begin{center}
\begin{tabular}{|l|r|r|r|r|r|r|r|}
    \hline
     \multicolumn{2}{|c|}{Dataset}& \multicolumn{4}{c|}{\bf einstein} \\ \hline
     \multicolumn{2}{|c|}{Query length} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} \\ \hline
     \multicolumn{2}{|c|}{Method} & siEDM & Baseline & siEDM & Baseline \\ \hline 
     \multirow{6}{*}{search time~(sec)}                  &   &   &  & &  \\
					 &  &   &  & &  \\
					 &  &   &  & &  \\
                     &   &   &  & &  \\
                     &  &   &  & &  \\
                     &  &   &  & &  \\
     \hline
  \end{tabular}
\end{center}
\begin{center}
\begin{tabular}{|l|r|r|r|r|r|r|r|}
    \hline
     \multicolumn{2}{|c|}{Dataset}& \multicolumn{4}{c|}{\bf cere} \\ \hline
     \multicolumn{2}{|c|}{Query length} & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} \\ \hline
     \multicolumn{2}{|c|}{Method} & siEDM & Baseline & siEDM & Baseline \\ \hline 
     \multirow{6}{*}{search time~(sec)}       &   &   &  & &  \\
					 &  &   &  & &  \\
				     &  &   &  & &  \\
                     &   &   &  & &  \\
                     &  &   &  & &  \\
                     &  &   &  & &  \\
 
     \hline
  \end{tabular}
\end{center}
\end{table}

\begin{table}
\begin{center}
  \caption{
Details of the search time for different  and :
time for candidate findings (CF) in seconds (sec),
time for -distance computations (DIST) in sec, and
time for position computations (PC) in sec
}
	\begin{tabular}{|l|l|r|r|r|r|r|r|r|}
     \hline
     \multicolumn{3}{|c|}{Dataset}& \multicolumn{2}{c|}{\bf einstein} &   \multicolumn{2}{c|}{\bf cere} \\ \hline
     \multicolumn{3}{|c|}{Query length} &  &  &  & \\ \hline
     
     \multirow{18}{*}{Search time}  & \multirow{6}{*}{CF~(sec)} &&  &	 &		&  \\
	& && 	&  & 		&  \\
	& && 	& 	&  &	 \\
	& &&  & 	& NA	&  \\
	& &&  & 	& NA &	NA \\
    & &&  &  &	NA &	NA \\ \cline{2-7}
    & \multirow{6}{*}{DIST~(sec)} &&  &	 &  &	 \\
    & &&  &		&  &	 \\
    & && 	&  &		&  \\
    & &&  &	 &	NA	&  \\
    & &&  &		& NA &	NA \\
    & &&  &	 & 	NA &	NA \\ \cline{2-7}
    & \multirow{6}{*}{PC~(sec)} && 	& - &		& - \\
    & && 	& 	& 	&  \\
    & &&  &		&  &	 \\
    & &&  & 	 & 	NA & 	 \\
    & &&  &		& NA	& NA \\
    & &&  & 		 & NA & 	NA \\
	\hline
  \end{tabular}
  \label{tab:search1}
  \end{center}
  \end{table}
  
    \begin{table}
  \begin{center}
    \caption{
Statistical information of the query search: the number of traversed nodes (\#TN),
the number of candidate -grams (\#CAND), 
the number of true positives (\#TP), the number of occurrences (\#OCC)}
    \begin{tabular}{|l|l|r|r|r|r|r|r|r|}
	\hline
     \multicolumn{3}{|c|}{Dataset}& \multicolumn{2}{c|}{\bf einstein} &   \multicolumn{2}{c|}{\bf cere} \\ \hline
     \multicolumn{3}{|c|}{Query length} &  &  &  & \\ \hline
     \multicolumn{2}{|c|}{\multirow{6}{*}{\#TN} } &   & &  &  &  \\
     \multicolumn{2}{|c|}{} &   &   &  &   &  \\
     \multicolumn{2}{|c|}{} &   &   &  &  &  \\
     \multicolumn{2}{|c|}{}&&  & 	 & 	NA & 	 \\
     \multicolumn{2}{|c|}{}&&  &		& NA	& NA \\
      \multicolumn{2}{|c|}{} &&  & 		 & NA & 	NA \\ \hline
      \multicolumn{2}{|c|}{\multirow{6}{*}{\#CAND}  }&   & &  &  &  \\
     \multicolumn{2}{|c|}{}&   &   &  &   &  \\
     \multicolumn{2}{|c|}{}&   &   &  &  &  \\
     \multicolumn{2}{|c|}{} &&  &	 & 	NA & 	 \\
      \multicolumn{2}{|c|}{}&& 	& 	 & NA	& NA \\
      \multicolumn{2}{|c|}{}&&  &  & NA & 	NA \\ \hline
     \multicolumn{2}{|c|} {\multirow{6}{*}{\#TP} }&   &  &  &  &  \\
     \multicolumn{2}{|c|}{}&   &  &  &   &  \\
     \multicolumn{2}{|c|}{}&   &   &  &  &  \\
     \multicolumn{2}{|c|}{}&&  &  & 	NA & 	 \\
     \multicolumn{2}{|c|}{}&&  &		& NA	& NA \\
	 \multicolumn{2}{|c|}{}&&  & 		 & NA & NA \\ \hline
     \multicolumn{2}{|c|} {\multirow{6}{*}{\#OCC} }&   &     &  &  &  \\
     \multicolumn{2}{|c|}{}&   &   &  &   &  \\
     \multicolumn{2}{|c|}{}&   &   &  &  &  \\
     \multicolumn{2}{|c|}{}&&  &	 & 	NA & 	 \\
     \multicolumn{2}{|c|}{}&&  & 	& NA	& NA \\
	 \multicolumn{2}{|c|}{}&&  & 	 & NA &  NA \\ 
     \hline
  \end{tabular}
            \label{tab:proper1}
  \end{center}
  \end{table}
\newpage
  	\begin{table}
  	\begin{center}
  	\caption{Detailed search time for longer query patterns
}
        \begin{tabular}{|l|l|r|r|r|r|r|r|r|}
            \hline
     \multicolumn{3}{|c|}{Dataset}& \multicolumn{2}{c|}{\bf einstein} & \multicolumn{2}{c|}{\bf cere} \\ \hline
     \multicolumn{3}{|c|}{Query length} &  &  &  & \\ \hline
     \multirow{18}{*}{Search time} & \multirow{6}{*}{CF~(sec)}&&  &  &  &  \\
	& &&  &  &  &  \\
	& &&  &  &  &  \\
	& &&  & 	&  &  \\
	& &&  & 	&  &  \\
        & &&  &  &  &  \\ \cline{2-7}
        & \multirow{6}{*}{DIST~(sec)} &&  &  &  &  \\
     & &&  &  &  &  \\
     & &&  &  &  &  \\
    & &&  &	 &		&  \\
    & &&  &		&  &	 \\
    & &&  &	 & 	 &	 \\ \cline{2-7}
    & \multirow{6}{*}{PC~(sec)} && 	&  &		&  \\
    & && 	& 	& 	&  \\
    & &&  &		&  &	 \\
    & &&  & 	 & 	 & 	 \\
    & &&  &		& 	&  \\
    & &&  & 		 &  & 	 \\
     \hline
        \end{tabular}
        \label{tab:search2}
	\end{center}
  \end{table}
  
  \begin{table}
  \begin{center}
   \caption{
Statistical information for longer query patterns
}
  \begin{tabular}{|l|l|r|r|r|r|r|r|r|}
	\hline
    \multicolumn{3}{|c|}{Dataset}& \multicolumn{2}{c|}{\bf einstein} &   \multicolumn{2}{c|}{\bf cere} \\ \hline
    \multicolumn{3}{|c|}{Query length} &  &  &  & \\ \hline
    \multicolumn{2}{|c|}{\multirow{6}{*}{\#TN} } &   &  &	 &	 &	 \\
    \multicolumn{2}{|c|}{} &   &  &  &	 &  \\
    \multicolumn{2}{|c|}{} &   &  &  &	 &	 \\
    \multicolumn{2}{|c|}{}&&  &  &	 &	 \\
    \multicolumn{2}{|c|}{}&& 	&  &	 &	 \\
    \multicolumn{2}{|c|}{} &&  &		&  &	 \\ \hline
    \multicolumn{2}{|c|}{\multirow{6}{*}{\#CAND}  }&   & &  &  &  \\
    \multicolumn{2}{|c|}{}&   &   &  &   &  \\
    \multicolumn{2}{|c|}{}&   &   &  &  &  \\
    \multicolumn{2}{|c|}{}& &  & 	 & 	 & 	 \\
    \multicolumn{2}{|c|}{}&  &  &		& 	&  \\
    \multicolumn{2}{|c|}{}& &  & 		 &  & 	 \\ \hline
    \multicolumn{2}{|c|}{\multirow{6}{*}{\#TP} }&   &  &  &  &  \\
    \multicolumn{2}{|c|}{}&   &  &  &   &  \\
    \multicolumn{2}{|c|}{}&   &   &  &  &  \\
    \multicolumn{2}{|c|}{}&&  & 	 & 	 & 	 \\
    \multicolumn{2}{|c|}{}&&  &		& 	&  \\
    \multicolumn{2}{|c|}{}&&  & 		 &  & 	 \\ \hline
    \multicolumn{2}{|c|} {\multirow{6}{*}{\#OCC} }&   &     &  &  &  \\
    \multicolumn{2}{|c|}{}&   &   &  &   &  \\
    \multicolumn{2}{|c|}{}&   &  &	 &  &  \\
    \multicolumn{2}{|c|}{}&& 	&  &  &	 \\
    \multicolumn{2}{|c|}{}&&  &	 &  &  \\
    \multicolumn{2}{|c|}{}&&  &	 &  &  \\ 
     \hline
  \end{tabular}
            \label{tab:proper2}
\end{center}
\end{table}
\end{comment}

\begin{figure}[t]
\begin{center}
\begin{tabular}{cc}
\hspace{1.5cm}
\includegraphics[width=0.45\textwidth]{ein} &
\includegraphics[width=0.45\textwidth]{cere} \\\\
\end{tabular}
\end{center}
\caption{Comparison of the search time for einstein (left) and cere (right).}
\label{time}
\end{figure}

\begin{figure}[t]
\begin{center}
\begin{tabular}{cc}
\hspace{1.5cm}
\includegraphics[width=0.45\textwidth]{CF-ein} & 
\includegraphics[width=0.45\textwidth]{CF-cere} \\
\hspace{1.5cm}
\includegraphics[width=0.45\textwidth]{DIST-ein} & 
\includegraphics[width=0.45\textwidth]{DIST-cere} \\
\hspace{1.5cm}
\includegraphics[width=0.45\textwidth]{PC-ein} & 
\includegraphics[width=0.45\textwidth]{PC-cere} \\\\
\end{tabular}
\end{center}
\vspace{1.5cm}
\caption{Details of search time for different  and :
time for candidate findings, CF, time for -distance computations, DIST, and
time for position computations, PC.}
\label{time-detail}
\end{figure}

\begin{figure}[t]
\begin{center}
\begin{tabular}{cc}
\hspace{1.8cm}
\includegraphics[width=0.43\textwidth]{TN-ein} &
\includegraphics[width=0.43\textwidth]{TN-cere} \\
\hspace{1.8cm}
\includegraphics[width=0.43\textwidth]{CAND-ein} &
\includegraphics[width=0.43\textwidth]{CAND-cere} \\
\hspace{1.8cm}
\includegraphics[width=0.43\textwidth]{TP-ein} &
\includegraphics[width=0.43\textwidth]{TP-cere} \\
\hspace{1.8cm}
\includegraphics[width=0.43\textwidth]{OCC-ein} &
\includegraphics[width=0.43\textwidth]{OCC-cere} \\\\
\end{tabular}
\end{center}
\caption{Statistical information of the query search: the number of traversed nodes, \#TN,
the number of candidate -grams, \#CAND, 
the number of true positives, \#TP, the number of occurrences, \#OCC.}
\label{statistical}
\end{figure}


Figure~\ref{time} shows the total search time (sec.) of siEDM and the baseline 
for einstein and cere in distance thresholds 
from  to .
In addition, this result does not contain the case  because siEDM found no candidate under the condition.
The query length is one of . 
Because the search time of baseline is linear in , we show only the fastest case: .
The search time of siEDM  was faster than baseline in most cases.

Figure~\ref{time-detail} shows the detailed search time in second.
CF is the time for finding candidates of  in ,
DIST is the time for computing approximated  distance by characteristic vectors,
and PC is the time for determining the positions of all -grams within the threshold .

Figure~\ref{statistical} shows the number of nodes  visited by the algorithm, \#TN,
the number of candidate -grams computed by , \#CAND,
the number of true positives among candidate -grams, \#TP, and the number of occurrences, \#OCC.
The most time-consuming task is the candidate finding. 

By the monotonicity of characteristic vectors, 
pruning the search space for small distance thresholds and long query length is more efficient. 
Thus, it is expected that siEDM is faster for smaller distance thresholds and longer query lengths
and the experimental results support this.
The search time on cere is much slower than that on einstein
because the number of generated production rules from cere is much larger than that from einstein,
and a large number of iterations of {\sc FindCandidates} is executed.
In addition, the comparison of \#CAND and \#TP validates the efficiency of siEDM for candidate finding 
with the proposed pruning method.

In Figure~\ref{statistical}, the algorithm failed to find a candidate.
Such a phenomenon often appears when the required threshold  is too small, 
because the ESP-tree  is not necessarily identical to  even if .
Generally, the parsing of  is affected by a suffix of  and a prefix of  of length at most . 

As shown in Table~\ref{tab:construct} and Figure~\ref{time},
the search time of siEDM depends on the size of encoded ESP-tree for the input.
Finally, we confirm this feature by an additional experiment for other repetitive texts.
Table~\ref{tab:dataset2}, \ref{tab:memory2} and \ref{tab:construct2} is 
the description of several datasets from the pizza \& chili corpus.
Figure~\ref{extra} shows the search time of siEDM and baseline. 
This result supports our claim that siEDM is suitable for computing EDM of repetitive texts.


\begin{table}[t]
\begin{center}
  \caption{Summary of additional datasets.}
  \begin{tabular}{|l|c|c|c|}
    \hline
     Dataset        & Length &  & Size (MB) \\ 
\hline
     influenza &  &  &  \\
      Escherichia\_Coli   &  &    &  \\
\hline
  \end{tabular}
\label{tab:dataset2}
\end{center}
\end{table}

\begin{table}[t]
\begin{center}
  \caption{
Comparison of the memory consumption for the query search
}
\begin{tabular}{|l|r|r|}
    \hline
     Dataset                   & {\bf influenza} & {\bf Escherichia\_Coli} \\ \hline
     siEDM~(MB) &  &  \\
     baseline~(MB) &  &  \\  
     \hline
  \end{tabular}
\label{tab:memory2}
\end{center}
\end{table}

\begin{table}[t]
\begin{center}
  \caption{
Comparison of the index size and construction time for additional datasets.
}
\begin{tabular}{|l|l|r|r|}
    \hline
     \multicolumn{2}{|c|}{Dataset} & {\bf influenza} & {\bf Escherichia\_Coli} \\ \hline

                & Encoded ESP-tree~(MB)   &  &  \\
     Index Size & Characteristic vector ~(MB)&  &  \\
                & Length vector ~(MB)  &  &  \\
\hline
     \multicolumn{2}{|c|}{Construction time~(sec)} &  &   \\
\hline
  \end{tabular}
\label{tab:construct2}
\end{center}
\end{table}

\begin{comment}
\begin{table}
\begin{center}
  \caption{Additional datasets of repetitive text.}
  \begin{tabular}{|l|c|c|c|}
    \hline
     Dataset        & Size (MB)  & Index size (MB) & Compression ratio (\%)\\ 
\hline
influenza     &  &   &  \\
     Escherichia\_Coli  &   &  &  \\
\hline
  \end{tabular}
\label{tab:dataset2}
\end{center}
\end{table}
\end{comment}

\begin{figure}[t]
\begin{center}
\begin{tabular}{cc}
\hspace{1.5cm}
\includegraphics[width=0.45\textwidth]{influenza} &
\includegraphics[width=0.45\textwidth]{Ecoli}
\end{tabular}
\end{center}
\vspace{1.5cm}
\caption{Search time (sec.) for repetitive texts:
E.Coli (left) and influenza (right).
}
\label{extra}
\end{figure}



\section{Conclusion}

We have proposed siEDM, an efficient string index for computing approximate
searching based on EDM.  Experimental results demonstrated the applicability
of siEDM to real-world repetitive text collections as well as a longer pattern
search.  Future work will make the search algorithm in siEDM faster, which
would be beneficial for users performing query searches for EDM.


\bibliography{biblio}
\bibliographystyle{plain}

\end{document}
