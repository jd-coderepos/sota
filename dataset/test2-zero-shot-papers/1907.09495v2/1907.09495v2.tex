
\documentclass{article} \usepackage{iclr2020_conference,times}

\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{url}
\usepackage{stmaryrd}
\usepackage{balance}
\usepackage{amssymb}
\usepackage{pgfplots}
\usepackage{pifont}
\usepackage{epsfig}
\usepackage{booktabs}\usepackage{pgffor}
\usepackage{tikz}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{autobreak}
\usepackage[multiple]{footmisc}

\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}


\title{IsoNN: Isomorphic Neural Network for Graph Representation Learning and Classification}

\author{Lin Meng and Jiawei Zhang  \\
IFM Lab\\
Florida State University\\
\texttt{lin@ifmlab.org, jiawei@ifmlab.org} \\
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}










\makeatletter
\newcommand\xleftrightarrow[2][]{\ext@arrow 9999{\longleftrightarrowfill@}{#1}{#2}}
\newcommand\longleftrightarrowfill@{\arrowfill@\leftarrow\relbar\rightarrow}
\makeatother

\newcommand{\ie}[0]{\textit{i.e.}}
\newcommand{\eg}[0]{\textit{e.g.}}
\newcommand{\etc}[0]{\textit{etc.}}
\newcommand{\etal}[0]{\textit{et al.}}


\newcommand{\mb}{\mathbf}
\newcommand{\mc}{\mathcal}
\newcommand{\bigcell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}

\newtheorem{theo}{\textsc{Theorem}}
\newtheorem{defn}{\textsc{Definition}}
\newtheorem{coro}{\textsc{Corollary}}
\newtheorem{proof}{\textsc{Proof}}
\newtheorem{lemma}{\textsc{Lemma}}
\newtheorem{example}{\textsc{Example}}

\newcommand{\frank}{\small \color{blue}}
\newcommand{\fbaseline}{\it \color{gray}}
\newcommand{\fexp}{\color{red}}
\newcommand{\fmed}{\color{green}}
\newcommand{\fmod}{\color{violet}}
\newcommand{\fphi}{\color{blue}}

\newcommand{\subproblem}{\textsc{\textbf{C}ls}}
\newcommand{\our}{\textsc{IsoNN}}
\newcommand{\ourfast}{\textsc{IsoNN}-fast}
\newcommand{\autoencoder}{\textsc{Auto-Encoder}}

\newcommand{\linemodel}{\textsc{LINE}}
\newcommand{\deepwalk}{\textsc{DeepWalk}}
\newcommand{\pale}{\textsc{PALE}}
\newcommand{\wordvec}{\textsc{Word2Vec}}



\usepackage{algorithm}
\usepackage{algorithmic}




 
\begin{document}


\maketitle


\begin{abstract}
\vspace{-5pt}
	Deep learning models have achieved huge success in numerous fields, such as computer vision and natural language processing. However, unlike such fields, it is hard to apply traditional deep learning models on the graph data due to the `node-orderless' property. Normally, adjacency matrices will cast an artificial and random node-order on the graphs, which renders the performance of deep models on graph classification tasks extremely erratic, and the representations learned by such models lack clear interpretability. To eliminate the unnecessary node-order constraint, we propose a novel model named \textbf{Iso}morphic \textbf{N}eural \textbf{N}etwork ({\our}), which learns the graph representation by extracting its isomorphic features via the graph matching between input graph and templates.  {\our} has two main components: graph isomorphic feature extraction component and classification component. The graph isomorphic feature extraction component utilizes a set of subgraph templates as the kernel variables to learn the possible subgraph patterns existing in the input graph and then computes the isomorphic features. A set of permutation matrices is used in the component to break the node-order brought by the matrix representation. Three fully-connected layers are used as the classification component in {\our}. Extensive experiments are conducted on benchmark datasets, the experimental results can demonstrate the effectiveness of {\our}, especially compared with both classic and state-of-the-art graph classification methods.
	
\end{abstract}





 

\vspace*{-10pt}
\section{Introduction}\label{sec:introduction}
\vspace*{-5pt}
The graph structure is attracting increasing interests because of its great representation power on various types of data. Researchers have done many analyses based on different types of graphs, such as social networks, brain networks and biological networks. In this paper, we will focus on the binary graph classification problem, which has extensive applications in the real world. For example, one may wish to identify the social community categories according to the users' social interactions~\cite{gao2017identifying}, distinguish the brain states of patients via their brain networks~\cite{wang2017structural}, and classify the functions of proteins in a biological interaction network~\cite{hamilton2017inductive}.

To address the graph classification task, many approaches have been proposed.  One way to estimate the usefulness of subgraph features is feature evaluation criteria based on both labeled and unlabeled graphs~\citet{kong2010semi}. Some other works also proposed to design a pattern exploration approach based on pattern co-occurrence and build the classification model~\citet{jin2009graph} or develop a boosting algorithm~\citet{wu2014boosting}. However, such works based on BFS or DFS cannot avoid computing a large quantity of  possible subgraphs, which causes high computational complexity though the explicit subgraphs are maintained.  Recently, deep learning models are also widely used to solve the graph-oriented problems.  Although some deep models like MPNN~\cite{gilmer2017neural} and GCN~\cite{kipf2016semi} learn  implicit structural features, the explict structural information cannot be maintained for further research. Besides, most existing works on graph classification use the aggregation of the node features in graphs  as the graph representation~\cite{xu2018powerful, hamilton2017inductive}, but simply doing aggregation on the whole graph cannot capture the substructure precisely. While there are other models can capture the subgraphs, they often needs more complex computation and mechanism~\cite{wang2017structural, narayanan2017graph2vec}.


However, we should notice that when we deal with the graph-structured data, different node-orders will result in very different adjacency matrix representations for most existing deep models which take the adjacency matrices as input if there is no other information on graph.  Therefore, compared with the original graph, matrix naturally poses a redundant constraint on the graph node-order. Such a node-order is usually unnecessary and manually defined. The different graph matrix representations brought by the node-order differences may render the learning performance of the existing models to be extremely erratic and not robust. Formally, we summarize the encountered challenges in the graph classification problem as follows:

\begin{itemize}


		
	\item \textbf{Explicit useful subgraph extraction.} The existing works have proposed many discriminative models to discover useful subgraphs for graph classification, and most of them require manual efforts. Nevertheless, how to select the contributing subgraphs automatically without any additional manual involvement is a challenging problem.
\item \textbf{Graph representation learning.}  Representing graphs in the vector space is an important task since it facilitates the storage, parallelism and the usage of machine learning models for the graph data. Extensive works have been done on node representations~\cite{grover2016node2vec, lin2015learning, lai2017prune, hamilton2017inductive}, whereas learning the representation of the whole graph with clear interpretability is still an open problem requiring more explorations. 
	\item \textbf{Node-order elimination.}  Nodes in graphs are orderless, whereas the matrix representations of graphs cast an unnecessary order on the nodes, which also renders the features extracted with the existing learning models, e.g., CNN, to be useless for the graphs. Thus, how to break such a node-order constraint is challenging.
	
	\item \textbf{Efficient matching for large subgraphs.}  To break the node-order, we will try all possible node permutations  to find the best permutation for a subgraph. Clearly, trying all possible permutaions is a combinatorial explosion problem, which is extremly time-comsuming for finding large subgraph templates.  The problem shows that how  to accelerate the proposed model for large subgraphs also needs to be solved.

\end{itemize}






In this paper, we propose a novel model, namely  \textbf{Iso}morphic \textbf{N}eural \textbf{N}etwork ({\our}) and its variants, to address the aforementioned challenges in the graph representation learning and classification problem. {\our} is composed of two components: the graph isomorphic feature extraction component and the classification component, aiming at learning isomorphic features and classifying graph instances, respectively. In the graph isomorphic feature extraction component, {\our} automatically learns a group of subgraph templates of useful patterns from the input graph. {\our} makes use of a set of permutation matrices, which act as the node isomorphism mappings between the templates and the input graph. With the potential isomorphic features learned by all the permutation matrices and the templates, {\our} adopts one min-pooling layer to find the best node permutation for each template and one softmax layer to normalize and fuse all subgraph features learned by different kernels, respectively. Such features learned by different kernels will be fused together and fed as the input for the classification component. {\our} further adopts three fully-connected layers as the classification component to project the graph instances to their labels. Moreover, to accelerate the proposed model when dealing with large subgraphs, we also propose two variants of {\our} to gurantee the efficiency. 






%
 
\vspace*{-10pt}
\section{Related Work} \label{sec:related_work}
\vspace*{-10pt}
Our work relates to subgraph mining, graph neural networks, network embedding as well as graph classification. We will discuss them briefly in the followings.

\textbf{Subgraph Mining:} Mining subgraph features from graph data has been studied for many years. The aim is to extract useful subgraph features from a set of graphs by adopting some specific criteria. One classic unsupervised method (i.e., without label information) is gSpan~\citep{yan2002gspan}, which builds a lexicographic order among graphs and map each graph to a unique minimum DFS code as its canonical label. GRAMI~\citep{elseidy2014grami} only stores templates of frequent subgraphs and treat the frequency evaluation as a constraint satisfaction problem to find the minimal set. For the supervised model (i.e., with label information), CORK utilizes labels to guide the feature selection, where the features are generated by gSpan~\citep{thoma2009near}. Due to the mature development of the sub-graph mining field,  subgraph mining methods have also been adopted in life sciences~\citep{mrzic2018grasping}. Moreover, several parallel computing based methods~\citep{qiao2018parallel, hill2012iterative, lin2014large} have proposed to reduce the time cost. 

\textbf{Graph Neural Network and Network Embedding:} Graph Neural Networks~\citep{monti2017geometric, atwood2016diffusion, masci2015geodesic,kipf2016semi, battaglia2018relational} have been studied in recent years because of the prosperity of deep learning. Traditional deep models cannot be directly applied to graphs due to the special data structure. The general graph neural model MoNet~\citep{monti2017geometric} employs CNN architectures on non-Euclidean domains such as graphs and manifold. The GCN proposed in~\citep{kipf2016semi} utilizes the normalized adjacency matrix to learn the node features for node classification; \citep{bai2018convolutional} proposes the multi-scale convolutional model for pairwise graph similarity with a set matching based graph similarity computation. However, these existing works based on graph neural networks all fail to investigate the node-orderless property of the graph data  and to maintain the explicit structural information. Another important topic related to this paper is network embedding~\citep{NIPS2013_5071, lin2015learning, lai2017prune, abu2018watch, hamilton2017inductive}, which aims at learning the feature representation of each individual node in a network based on either the network structure or attribute information. Distinct from these network embedding works, the graph representation learning problem studied in this paper treats each graph as an individual instance and focuses on learning the representation of the whole graph instead.


\textbf{Graph Classification:} Graph classification is an important problem with many practical applications. Data like social networks, chemical compounds, brain networks can be represented as graphs naturally and they can have applications such as community detection~\citep{zhang2018end}, anti-cancer activity identification~\citep{kong2013discriminative, kong2010semi} and Alzheimer's patients diagnosis~\citep{tong2017multi, tong2015nonlinear} respectively. Traditionally, researchers mine the subgraphs by DFS or BFS ~\citep{saigo2009gboost, kong2013discriminative}, and use them as the features. With the rapid development of deep learning  (DL), many works are done based on DL methods. GAM builds the model by RNN with self-attention mechanism~\citep{lee2018graph}. DCNN extend CNN to general graph-structured data by introducing a ‘diffusion-convolution’ operation~\citep{atwood2016diffusion}.

 
\vspace*{-10pt}
\section{Terminology and Problem Definition} \label{sec:formulation}
\vspace*{-5pt}
In this section, we will define the notations and the terminologies used in this paper and give the formulation for the graph classification problem.
\vspace*{-10pt}
\subsection{Notations}
\vspace*{-5pt}
In the following sections, we will use lower case letters like  to denote scalars, lower case bold letters  (e.g. ) to represent vectors, bold-face capital letters (e.g. ) to show the matrices. For tensors or sets, capital calligraphic letters are used to denote them. We use  to represent the -th element in . Given a matrix , we use  to express the element in -th row and -th column.  For -th row vector and -th column vector, we use  and  to denote respectively. Moreover, notations  and  denote the transpose of vector  and matrix  respectively. Besides, the -norm of matrix  can be represented as .\\

\vspace*{-15pt}
\subsection{Problem Formulation}
\vspace*{-5pt}
Many real-world inter-connected data can be formally represented as the graph-structured data.
\begin{defn}  (Graph): Formally, a graph can be represented as , where the sets  and  denote the nodes and links involved in the graph, respectively. 
\end{defn}
\vspace*{-10pt}
Some representative examples include the human brain graphs (where the nodes denote brain regions and links represent the correlations among these regions), biological molecule graphs (with the nodes represent the atoms and links denote the atomic bonds), as well as the geographical graphs in the offline world (where the nodes denote the communities and the links represent the commute routes among communities). Meanwhile, many concrete real-world application problems, e.g., brain graph based patient disease diagnosis, molecule function classification and community vibrancy prediction can also be formulated as the graph classification problems. 

\noindent \textbf{Problem Definition}: Formally, given a  graph set  with a small number of labeled graph instances, the graph classification problem aims at learning a mapping, i.e., , to project each graph instance into a pre-defined label space . 

In this paper, we will take the graph binary classification as an example to illustrate the problem setting for {\our}. A simple extension of the model can be applied to handle more complicated learning scenarios with multi-class or multi-label as well.


 \begin{figure*}
\vspace*{-20pt}
    \centering
    \hspace*{-1.5cm}
    \includegraphics[width=1.2\textwidth]{./framework_architecture}
    \vspace{-25pt}
    \caption{IsoNN Framework Architecture. (The left subplot provides the outline of the proposed framework, including the \textit{graph isomorphic feature extraction} component and the \textit{classification} component respectively. Meanwhile, the right subplot illustrates the detailed architecture of the proposed framework, where the \textit{graph isomorphic features} are extracted with the \textit{graph isomorphic layer}, \textit{min-pooling layer} and \textit{softmax layer}, and the graphs are further classified with three fully-connected layers.)} 
    \label{fig:model}
    \vspace*{-15pt}
\end{figure*} 


\vspace*{-10pt}
\section{Proposed Method}\label{sec:method}
\vspace*{-8pt}
The overall architecture of {\our} is shown in Figure~\ref{fig:model}. The {\our} framework includes two main components: graph isomorphic feature extraction component and classification component. The graph isomorphic feature extraction component includes a graph isomorphic layer, a min-pooling layer as well as a softmax layer and the classification component is composed by three fully-connected layers. They will be discussed in detail in the following subsections.
\vspace*{-5pt}
\subsection{Graph Isomorphic Feature Extraction Component}
\vspace*{-5pt}

Graph isomorphic feature extraction component targets at learning the graph features. To achieve that objective, {\our} adopts an automatic feature extraction strategy for graph representation learning. In {\our}, one graph isomorphic feature extraction component involves three layers: the graph isomorphic layer, the min-pooling layer and the softmax layer. In addition, we can further construct a deep graph isomorphic neural network by stacking multiple isomorphic feature extraction components on top of each other.


\vspace*{-5pt}
\subsubsection{Graph Isomorphic Layer}
\vspace*{-5pt}
Graph isomorphic layer is the first effective layer in deep learning that handles the node-order restriction in graph representations. Assume we have a graph , and its adjacency matrix to be . In order to find the existence of specific subgraph patterns in the input graph, {\our} matches the input graph with a set of subgraph templates. Each template is denoted as a kernel variable . Here,  denotes the node number in subgraphs and  is the channel number (i.e., total template count). Meanwhile, to match one template with regions in the input graph (i.e., sub-matrices in ), we use  a set of permutation matrices, which map both rows and columns of the kernel matrix to the subgraphs effectively. The permutation matrix can be represented as  that shares the same dimension with the kernel matrix. Therefore, given a kernel matrix  and a sub-matrix  in  (i.e., a region in the input graph  and  denotes a starting index pair in ), there may exist  different such permutation matrices. The optimal should be the matrix  that minimizes the following term.
\vspace*{-5pt}

where  covers all the potential permutation matrices. Formally, the isomorphic feature extracted based on the kernel  for the regional sub-matrix  in  can be represented as
\vspace*{-10pt}

where vector  contains entry  denoting the isomorphic features computed by the -th permutation matrix . 

As indicated by the Figure~\ref{fig:model}, {\our} computes the final isomorphic features for the kernel  via two steps: (1) computing all the potential isomorphic features via different permutation matrices with the graph isomorphic layer, and (2) identifying and fusing the optimal features with the min-pooling layer and softmax layer to be introduced as follows. By shifting one kernel matrix  on regional sub-matrices, {\our} extracts the isomorphic features on the matrix , which can  be denoted as a 3-way tensor , where . In a similar way, we can also compute the isomorphic feature tensors based on the other kernels, which can be denoted as , , ,  respectively. 
\vspace*{-5pt}
\subsubsection{Min-pooling Layer}
\vspace*{-5pt}
Given the tensor  computed by  in the graph isomorphic layer, {\our} will identify the optimal permutation matrices via the min-pooling layer. Formally, we can represent results of the optimal permutation selection with  as matrix :

The min-pooling layer learns the optimal matrix  for kernel  along the first dimension (i.e., the dimension indexed by different permutation matrices), which can effectively identify the isomorphic features created by the optimal permutation matrices. For the remaining kernel matrices, we can also achieve their corresponding graph isomorphic feature matrices as , , ,  respectively.
\vspace*{-5pt}
\subsubsection{Softmax Layer}
\vspace*{-5pt}
Based on the above descriptions, a perfect matching between the subgraph templates with the input graph will lead to a very small isomorphic feature, e.g., a value approaching to . If we feed the small features into the classification component, the useful information will vanish and the relative useless information (\ie, features learned by the subgraphs dismatch the kernels) dominates the learning feature vector in the end. Meanwhile, the feature values computed in Equation~(\ref{eq:minpooling}) can also be in different scales for different kernels. To effectively normalize these features, we propose to apply the softmax function to matrices , , ,  across all  kernels. Compared with the raw features, e.g., , softmax as a non-linear mapping can also effectively highlight the useful features in  by rescaling them to relatively larger values especially compared with the useless ones. Formally, we can represent the fused graph isomorphic features after rescaling by all the kernels as a 3-way tensor , where slices along first dimension can be denoted as:



\vspace*{-15pt} 
\subsection{Classification Component}
\vspace*{-5pt}
After the isomorphic feature tensor  is obtained, we feed it into a classification component.  Let  denote the flattened vector representation of feature tensor , and we pass it to three fully-connected layers to get the predicted label vector . For the graph binary classification, suppose we have the ground truth  and the predicted label vector  for the sample  from the training batch set . We use cross-entropy as the loss function in {\our}. Formally, the fully-connected (FC) layers and the objective function can be represented as follows respectively: 

where  and  represent the weights and biases in -th layer respectively for . The  denotes the adopted the relu activation function. To train the proposed model, we adopt the back propagation algorithm to learn both the subgraph templates and the other involved variables.




\subsection{More Discussions on Graph Isomorphic Feature Extraction in {\our}}\label{subsec:discus}

Before introducing the empirical experiments to test the effectiveness of {\our}, we would like to provide more discussions about the computation time complexity of the graph isomorphic feature extraction component involved in {\our}. Formally, given the input graph  with  nodes, by shifting the kernel variables (of size ) among the dimensions of the corresponding graph adjacency matrix, we will be able to obtain  regional sub-matrices (or  regional sub-matrices for notation simplicity). Here, we assume {\our} has only one isomorphic layer involving  different kernels. In the forward propagation, the introduced time cost in computing the graph isomorphic features can be denoted as , where term  is introduced in enumerating all the potential permutation matrices and  corresponds to the matrix multiplication time cost.

According to the notation, we observe that  is fixed for the input graph. Once the kernel channel number  is decided, the time cost notation will be mainly dominated by . To lower down the above time complexity notation, in this part, we propose to further improve {\our} from two perspectives: (1) compute the optimal permutation matrix in a faster manner, and (2) use deeper model architectures with small-sized kernels.

\vspace*{-10pt}
\subsubsection{Fast Permutation Matrix Computation}
\vspace*{-5pt}
Instead of enumerating all the permutation matrices in the graph isomorphic feature extraction as indicated by Equations~\ref{eq:all_p}-\ref{eq:minpooling}, here we introduce a fast way to compute the optimal permutation matrix for the provided kernel variable matrix, e.g., , and input regional sub-matrix, , directly according to the following theorem.
 
\begin{theo}\label{theo1}
	Formally, let the kernel variable  and the input regional sub-matrix  be  real symmetric matrices with  distinct eigenvalues  and , respectively, and their eigendecomposition be represented by 
	 
	where  and  are orthogonal matrices of eigenvectors and . The minimum of  is attained for the following 's:
	 
	where .
\end{theo}
\vspace*{-5pt}
The proof of the theorem will be provided in appendix. In computing the optimal permutation matrix , trials of different  will be needed. Meanwhile, to avoid such time costs, we introduce to take the upper bound value of  as the approximated optimal permutation matrix instead, which together with the corresponding optimal feature  can be denoted as follows:

where  denotes the absolute value operator and  hold for . 

By replacing Equations~\ref{eq:all_p}-\ref{eq:minpooling} with Equation~\ref{eq:p}, we can compute the optimal graph isomorphic feature for the kernel  and input regional sub-matrix  with a much lower time cost. Furthermore, since the eigendecomposition time complexity of a  matrix is , based on the above theorem, we will be able to lower down the total time cost in graph isomorphic feature extraction to , which can be further optimized with the method introduced in the following subsection.



\begin{figure*}
	\vspace*{-20pt}
	\centering
\includegraphics[width=0.9\textwidth]{./deep_isonn_illustration.pdf}
		\vspace{-10pt}
	\caption{An Illustration of Deep Architecture of {\our}.} 
	\label{fig:deep_layer_example}
		\vspace*{-10pt}
\end{figure*} 



\subsubsection{Deep Graph Isomorphic Feature Extraction}
\vspace*{-5pt}
Here, we will illustrate the advantages of deep {\our} model with small-sized kernels compared against shallow {\our} model with large kernels. In Figure~\ref{fig:deep_layer_example}, we provide an example two {\our} models with different model architectures
\begin{itemize}
\item the left model has one single layer and  kernels, where the kernel size ;
\item the right model has two layers: layer 1 involves  kernels of size , and layer 2 involves  kernels of size .
\end{itemize}
By comparing these two different models, we observe that they have identical representation learning capacity. However, the time cost in feature extraction introduced by the left model is much higher than that introduced by the right model, which can be denoted as  and , respectively. 

Therefore, for the {\our} model, we tend to use small-sized kernels. Formally, according to the fast method provided in the previous part, given a 1-layer {\our} model with  large kernels of size , its graph isomorphic feature extraction time complexity can be denoted as . Inspired by Figure~\ref{fig:deep_layer_example}, without affecting the representation capacity, such a model can be replaced by a -layers deep {\our} model instead, where each layer involves  kernels of size . The graph isomorphic feature extraction time complexity of the deep model will be  (or  for simplicity).



 \vspace*{-10pt}
\section{Experiments}\label{sec:experiment}
\vspace*{-8pt}
To evaluate the performance of {\our}, in this section, we will talk about the experimental settings as well as five benchmark datasets. Finally, we will discuss the experimental results with parameter analyses on kernel size , channel number and time complexity.






\vspace*{-10pt}
\subsection{Experimental Settings}
\vspace*{-5pt}
In this subsection, we will use five real-world benchmark datasets:  HIV-fMRI~\citet{cao2015identifying}, HIV-DTI~\citet{cao2015identifying},  BP-fMRI~\citet{cao2015identification}, MUTAG\footnote{\label{note1}https://ls11-www.cs.tu-dortmund.de/people/morris/graphkerneldatasets/ }
 and PTC\footref{note1}. Both HIV-fMRI and HIV-DTI have 56 positive instances and 21 negative instances. Also, graph instances in both of them are represented as  matrices~\citet{cao2015identifying}. BP-fMRI has 52 positive and 45 negative instances and each instance is presented by an  matrix~\citet{cao2015identification}. MUTAG and PTC are two datasets which have been widely used in academia~\cite{xu2018powerful, shervashidze2009fast}. MUTAG has 125 positive and 63 negative graph instances with graph size . PTC is a relative large dataset, which has 152 positive and 192 negative graph instances with graph size . With these datasets, we first introduce the comparison methods used in this paper and then talk about the experimental setups and the adopted evaluation metrics in detail. 


\vspace*{-8pt}
\subsubsection{Comparison Methods}
\begin{itemize}
	\item \textbf{{\our} \& {\ourfast} }: The proposed method {\our} uses a set of template variables as well as the permutation matrices to extract the isomorphic features and feed these features to the classification component. The variant model named  {\ourfast} uses the Equation~\ref{eq:fast_p} to compute the optimal permutation matrices and other settings remain unchanged. 
	
	\item \textbf{Freq}: The method uses the top- frequent subgraphs as its features. This is also an unsupervised feature selection method based on frequency. 
	
	\item \textbf{AE}: We use the autoencoder model (AE)~\citet{vincent2010stacked} to get the features of graphs without label information. It is an unsupervised learning method, which learns the latent representations of connections in the graphs without considering the structural information.
	
	\item \textbf{CNN:} It is the convolutional model~\citet{krizhevsky2012imagenet} learns the structural information within small regions of the whole graph. We adopt one convolution layer and three fully-connected layer to be the classification module.
	
	\item \textbf{SDBN}: A model proposed in~\citet{wang2017structural}, which reorders the nodes in the graph first and then feeds the reordered graph into an augmented CNN. In this way, it not only learns the structural information but also tries to minimize the effect of the order constraint.
	
	
	\item \textbf{WL:}  WL~\cite{shervashidze2009fast} is a classic algorithm to do the graph  isomorphism test. For the graph classification, we computes the similarity scores for test graphs and train graph. The mean of all similarity scores between each test graph and train graphs will be used to do the classification.
	\item  \textbf{GCN:} GCN is proposed in~\cite{kipf2016semi} use the adjacent matrix to learn the implicit structure information in graphs. Here, we use two graph convolutional layers to learn node features and then take the all nodes features as the graph features. One fully-connected layer will be used as graph classification module. 
	 \item  \textbf{GIN:}  GIN is proposed in~\cite{xu2018powerful} can be used to do graph classification with node features. We adopt the same experimental setting as  GIN-0 stated in~\cite{xu2018powerful}. 
	 
	
	 
\end{itemize}
\vspace*{-10pt}

\begin{table*}[t]
	\vspace*{-20pt}
\caption{Classification Results of the Comparison Methods.}
\label{tab:classification_result}
\centering
{
	
	
	\begin{tabular}{lrccccccccc}
		\toprule
		\multicolumn{2}{l}{}&\multicolumn{9}{c}{Methods}\\
		\cmidrule{3-11}
		Dataset &Metric &Freq    &AE   &CNN   &SDBN  &WL &GCN  &GIN   &{\ourfast} &{\our}   \\
		\midrule
		\multirow{2}{*}{{HIV-fMRI}} &Acc. &54.3   &46.9 & 59.3 &66.5 &44.2     &58.3   &52.5    &70.5 &\textbf{73.4}\\
		&F1           &58.2  &35.5 &66.3 &66.7 &27.2   &56.4   &35.6   &69.9 &\textbf{72.2}\\
		\midrule
		\multirow{2}{*}{{HIV-DTI}} &Acc. &64.6   &62.4 &54.3 &65.9 &47.1     &57.7  &55.1    &60.1 &\textbf{67.5}\\
		&F1   &63.9 &0.0 &55.7 &65.6        &48.4    &54.4  &53.6   &61.9 &\textbf{68.3}\\
		\midrule
		
		\multirow{2}{*}{{BP-fMRI}} &Acc. &56.8   &53.6 &54.6 &64.8       &56.2   &60.7 &45.4   &62.3 &\textbf{64.9} \\
		&F1            &57.6 &69.5  &52.8 &63.7   &58.8  &61.2  &42.3   &63.2  &\textbf{69.7}\\
		\midrule
		
		
		\multirow{2}{*}{{MUTAG}} &Acc. &76.2   &50.0  &81.7  &54.0  &52.4   &63.5   &54.0  &\textbf{83.3} &\textbf{83.3}  \\
		&F1     &76.9   &66.7  &82.3 &66.7   &49.9  &61.9   &66.7  &\textbf{83.6} &83.0  \\
		\midrule
		
		\multirow{2}{*}{{PTC}} &Acc. &57.8   &50.0  &54.6  &50.0   &49.0    &49.0 &49.0  &53.0 &\textbf{59.9} \\
		&F1      &54.9  &\textbf{66.5}  &58.9  &\textbf{66.5}   &48.9    &48.9  &47.5   &55.8 &59.9\\
		\bottomrule

	\end{tabular}




}
\vspace*{-15pt}
\end{table*}



 \subsubsection{Experimental Setup and Evaluation Metrics}
\vspace*{-5pt}
  In our experiments, to make the results more reliable, we partition the datasets into 3 folds and then set the ratio of train/test according to , where two folds are treated as the training data and the remaining one is the testing data. We select top-100 features for Freq as stated in~\cite{wang2017structural} with a three layer MLP classifier, where the neuron numbers are 1024, 128, 2. For Auto-encoder, we apply the two-layer encoder and two-layer decoder. For the CNN, we apply the one convolutional layer with the size , a max-pooling layer with kernel size , one gating relu layer as activation layer and we set parameters in the classification module the same as Freq classifier.  For the SDBN, we set the architecture as follows: we use two layers of "convolution layer + max pooling layer + activation layer " and concatenate a fully connected layer with 100 neurons as well as an activation layer, where the parameters are the same as those in CNN. We also set the dropout rate in SDBN being 0.5 to avoid overfitting. For WL kernel, if the average similarity score for one test graph greater than 0.5, we assign the test graph positive label, otherwise, assign negative label. We follow the setting in ~\cite{kipf2016semi} and \cite{xu2018powerful} to do GCN and GIN-0. Here, to make a fair comparison, we will use the adjacency matrices as features (\ie, no node label information) for GCN and GIN. In the experiments, we set the kernel size  in the isomorphic layer for three datasets as 2, 4, 3, 4, 4 respectively, and then set the parameters in classification component the same as those in Freq classifier. In this experiment, we adopt Adam optimizer and the set the learning rate , and then we report the average results on balanced datasets. 


\vspace*{-8pt}
\subsection{Experimental Results}
\vspace*{-8pt}
In this section, we investigate the effectiveness of the learned subgraph-based graph feature representations for graphs. We adopt one isomorphic layer where the kernel size  and channel number  for HIV-fMRI, one isomorphic layer with , ,  and  for the HIV-DTI, BP-fMRI, MUTAG and PTC, respectively. The results are shown in Table~\ref{tab:classification_result}. From that table, we can observe that {\our} outperforms all other baseline methods on these all datasets.  We need to remark that  {\our} and {\ourfast} are very close on MUTAG, and {\our} has the best performance in total on PTC. Compared with Freq, the proposed method achieves a better performance without searching for all possible subgraphs manually. AE has almost the worst performance among all comparison methods. This is because the features learned from AE do not contain any structural information. For HIV-DTI, AE gets 0 in F1. This is because the dataset contains too many zeros, which makes the AE learns trivial features. Also, for PTC, its F1 is higher than other models, but the accuracy only get 50.0, which indicates AE actually have a bad performance since it cannot discriminate the classes of the instances (\ie, predicting all positive classes).  CNN performs better than AE but still worse than {\our}. The reason can be that it learns some structural information but fails to learn representative structural patterns. SDBN is designed for brain images, so it may not work for MUTAG and PTC.  One possible reason for WL got bad results is the isomorphism test is done on the whole graph, which may lead to erratic results. GCN performs better than GIN but worse than {\our}, showing that GCN can learn some sturctual information without node labels, but GIN cannot work with the adjacency matrix as input. {\ourfast} achieves the best scores on MUTAG and  second-best on HIV-fMRI, yet worse than several other methods on other datasets. This can be the approximation on  may impair the performance.  Comparing {\our} with AE, {\our} achieves better results. This means the structural information is more important than only connectivity information for the classification problem. If compared with CNN, the results also show the contribution of breaking the node-order in learning the subgraph templates. Similar to SDBN,  {\our} also finds the features from subgraphs, but {\our} gets better performance with more concise architecture. Contrasting with GCN and GIN, {\our} can maintain the explict subgraph structures in graph representations, while the GCN and GIN simply use the aggragation of the neighboring node features, losing the graph-level substructure infomation.
\begin{figure*}[t]
	\centering
			\vspace*{-20pt}
	\subfigure[\small{HIV-fMRI}]{\label{fig:hiv_fmri_k}
		
		\includegraphics[width=0.3\columnwidth]{./hiv_fmri_k.pdf}
	}
	\subfigure[\small{HIV-DTI}]{\label{fig:hiv_dti_k}
		\includegraphics[width=0.3\columnwidth]{.//hiv_dti_k.pdf}
	}
	\subfigure[\small{BP-fMRI}]{\label{fig:bp_fmri_k}
		\includegraphics[width=0.3\columnwidth]{./bp_fmri_k.pdf}
	}
	\subfigure[\small{MUTAG}]{\label{fig:bp_fmri_k}
		\includegraphics[width=0.3\columnwidth]{./mutag_k.pdf}
	}
	\subfigure[\small{PTC}]{\label{fig:bp_fmri_k}
		\includegraphics[width=0.3\columnwidth]{./ptc_k.pdf}
	}
\vspace{-10pt}
	\caption{Effectiveness of Different k}
	\vspace{-18pt}
	\label{fig:diff_k}
\end{figure*}
\vspace*{-11pt}
\subsection{ Parameter Analysis}
\vspace*{-5pt}
To further study the proposed method, we will discuss the effects of different kernel size and channel number in {\our}. The model convergence analysis will be provided in appendix.
\begin{itemize}
	\vspace*{-7pt}
	\item \textbf{Kernel Size}:
	We show the effectiveness of different  in Figure~\ref{fig:diff_k}. Based on the previous statement, parameter  can affect the final results since it controls the size of learned subgraph templates. To investigate the best kernel size for each dataset, we fix the channel number . As Figure~\ref{fig:diff_k} shows, different datasets have different appropriate kernel sizes. The best kernel sizes are 2, 4, 3, 4, 4 for the three datasets respectively. 
	\vspace*{-5pt}
	\item \textbf{Channel Number}:
	We also study the effectiveness of multiple channels (i.e., multiple templates in one layer). To discuss how the channel number influences the results, we choose the best kernel size for each dataset (i.e., 2, 4, 3, 4, 4 respectively). From all sub-figures in Figure~\ref{fig:diff_c}, we can see that the differences among the different channel numbers by using only one isomorphic layer. As shown in Figure~\ref{fig:diff_c},  {\our} achieves the best results by , respectively, which means the increase of the channel number can improve the performance, but more channels do not necessarily lead to better results. The reason could be the more templates we use,  the more complex our model would be. With such a complex model, it is easy to learn an overfitting model on train data, especially when the dataset is quite small. Thus, increasing the channel number can improve the performance but the effectiveness will still depend on the quality and the quantity of the dataset.
\end{itemize}



\begin{figure*}[t]
	\centering
	    \vspace*{-15pt}
	\subfigure[\small{HIV-fMRI}]{\label{fig:hiv_fmri_c}
		
		\includegraphics[width=0.3\columnwidth]{./hiv_fmri_c.pdf}
	}
	\subfigure[\small{HIV-DTI}]{\label{fig:hiv_dti_c}
		\includegraphics[width=0.3\columnwidth]{./hiv_dti_c.pdf}
	}
	\subfigure[\small{BP-fMRI}]{\label{fig:bp_fmri_c}
		\includegraphics[width=0.3\columnwidth]{./bp_fmri_c.pdf}
	}
		\subfigure[\small{MUTAG}]{\label{fig:mutag_c}
		\includegraphics[width=0.3\columnwidth]{./mutag_c.pdf}
	}
		\subfigure[\small{PTC}]{\label{fig:ptc_c}
		\includegraphics[width=0.3\columnwidth]{./ptc_c.pdf}
	}
	
\vspace{-10pt}
	\caption{Effectiveness of Different c}
		\vspace*{-15pt}
	\label{fig:diff_c}
\end{figure*}

\begin{figure*}[t]
	\centering
\subfigure[\small{Different k}]{\label{fig:time_k}
		
		\includegraphics[width=0.3\columnwidth]{./time_diff_k.pdf}
	}
	\subfigure[\small{Different c}]{\label{fig:time_c}
		\includegraphics[width=0.3\columnwidth]{./time_diff_c.pdf}
	}
	\subfigure[\small{{\our} \& {\ourfast}}]{\label{fig:cmp_time}
	\includegraphics[width=0.3\columnwidth]{./cmp_time.pdf}
}
\vspace*{-10pt}
	\caption{Time Complexity Study}
			\vspace*{-15pt}
	\label{fig:time}
\end{figure*}
\vspace*{-15pt}
\subsection{Time Complexity Study}
\vspace*{-5pt}
To study the efficiency of {\our} and {\ourfast}, we collect the actual running time on training model, which is shown in Figure~\ref{fig:time}.  In both Figures~\ref{fig:time_k} and~\ref{fig:time_c} \footnote{Since the PTC is a relative large dataset compared with the others, its running time is in different scale compared with the other datasets, which makes the time growth curve of other datasets not obvious. Thus, we don't show the results on PTC.}, the x-axis denotes its value for  or  and the y-axis denotes the time cost with different parameters.  From Figure~\ref{fig:time_k}, four lines show the same pattern. When the  increases, the time cost grows exponentially. This pattern can be directly explained by the size of the permutation matrix set. When we increase the kernel size by one, the number of corresponding permutation matrices grows exponentially. While changing , shown in Figure~\ref{fig:time_c}, it is easy to observe that those curves are basically linear with different slopes. This is also natural since whenever we add one channel, we only need to add a constant number of the permutation matrices. To study the efficiency of {\ourfast}, Figure~\ref{fig:cmp_time} shows the running times of {\our} and {\ourfast} on MUTAG. As it shows,  {\ourfast} use less time when the kernel size greater than , otherwise {\our} and {\our} will have little difference since the eigen decomposition has nearly the same time complexity as calculating all possible node permutaions. The results also verify the theoretical time complexity analysis in~\ref{subsec:discus}.



 
\vspace{-10pt}
\section{Conclusion}\label{sec:conclusion}
\vspace{-5pt}
In this paper, we proposed a novel graph neural network named {\our} to solve the graph classification problem. {\our} consists of two components: (1) isomorphic component, where a set of permutation matrices is used to break the randomness order posed by matrix representation for a bunch of templates and one min-pooling layer and one softmax layer are used to get the best isomorphic features, and (2) classification component, which contains three fully-connected layers.  We further discuss the two efficient variants of {\our} to accelerate the model. Next, we perform the experiments on five real-world datasets. The experimental results show the proposed method outperforms all comparison methods, which demonstrates the superiority of our proposed method. The experimental analysis on time complexity illustrates the efficiency of the {\ourfast}. 
\bibliography{reference}
\bibliographystyle{iclr2020_conference}

\newpage
\section{Appendix}\label{sec:appendix}


\subsection{Proof of Theorem 1}
Before giving the proof of Theorem~\ref{theo1}, we need to introduce Lemma~\ref{lemma1} first.

\begin{lemma}\label{lemma1}
If  and  are Hermitian matrices with eigenvalues  and  respectively, then  
\end{lemma}
Based on Lemma~\ref{lemma1}, we can derive the proof of Theorem~\ref{theo1} as follows.
\begin{proof}
    From Lemma~\ref{lemma1}, Equation~\ref{eq:t1} holds for any orthogonal matrix  since the eigenvalues of  are the same as those of~ .
	 
	On the other hand, if we use  in~\ref{eq:p}, we have
	10pt]
	&=|| \mb{S}  \mb{\Lambda}_{K_i} \mb{S} - \mb{\Lambda}_{M_{(s,t)}}||^2  \\ [10pt]
	&= ||\mb{\Lambda}_{K_i}  - \mb{\Lambda}_{M_{(s,t)}}||^2  \\ [10pt]
	&=\sum_{j=1}^n(\alpha_j-\beta_j)^2
	\end{array}
	
	where we use the equations that  for any orthogonal matrix  and  since  and are both orthogonal matrices and . 
\end{proof}

\subsection{Convergence Analysis}
The Figure~\ref{fig:convergence} shows the convergence trend of {\our} on five datasets, where the x-axis denotes the epoch number and the y-axis is the training loss, respectively. From these sub-figures, we can know that the proposed method can achieve a stable optimal solution within 50 iterations except for MUTAG (it needs almost 130 epochs to converge), which also illustrates our method would converge relatively fast.
\begin{figure*}[t]
	\vspace*{-20pt}
	\centering
	\subfigure[\small{HIV-fMRI}]{\label{fig:conv_hiv_fmri}
		
		\includegraphics[width=0.3\columnwidth]{./conv_hiv_fmri.pdf}
	}
	\subfigure[\small{HIV-DTI}]{\label{fig:conv_hiv_dti}
		\includegraphics[width=0.3\columnwidth]{./conv_hiv_dti.pdf}
	}
	\subfigure[\small{BP-fMRI}]{\label{fig:conv_bp_fmri}
		\includegraphics[width=0.3\columnwidth]{./conv_bp_fmri.pdf}
	}
	\subfigure[\small{MUTAG}]{\label{fig:conv_mutag}
		\includegraphics[width=0.3\columnwidth]{./conv_mutag.pdf}
	}
	\subfigure[\small{PTC}]{\label{fig:conv_ptc}
		\includegraphics[width=0.3\columnwidth]{./conv_ptc.pdf}
	}
\vspace{-10pt}
	\caption{Convergence Analysis}
	\vspace{-15pt}
	\label{fig:convergence}
\end{figure*}

%
 
\end{document}
