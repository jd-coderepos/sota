\documentclass{article}
\usepackage{spconf,amsmath,graphicx}
\usepackage{physics}
\usepackage{esvect}
\usepackage{floatrow}
\usepackage{acronym}
\usepackage[acronym]{glossaries}
\floatsetup[table]{capposition=top}
\newcommand{\etal}{\emph{et al.} }
\newcommand{\eg}{\emph{e.g.,} }
\newcommand{\ie}{\emph{i.e.} }

\def\x{{\mathbf x}}
\def\L{{\cal L}}
\newcommand{\fref}[1]{Figure \ref{#1}}		\newcommand{\eref}[1]{Eq. \eqref{#1}}

\newacronym{strf}{STRF}{spectro-temporal receptive field}
\newacronym{stmi}{STMI}{spectro-temporal modulation index}
\newacronym{stmr}{STMR}{spectro-temporal modulation response}
\newacronym{stme}{STME}{spectro-temporal modulation error}
\newacronym{se}{SE}{speech enhancement}
\newacronym{dnn}{DNN}{deep neural network}
\newacronym{tf}{TF}{time-frequency}
\newacronym{tfe}{TFE}{time-frequency error}
\newacronym{mse}{MSE}{mean-squared error}
\newacronym{stftm}{STFTM}{short-time Fourier transform magnitude}
\newacronym{sid}{SID}{speaker identification}
\newacronym{lps}{LPS}{log power spectra}
\newacronym{stft}{STFT}{short-time Fourier transform}
\newpage

\title{A Modulation-Domain Loss for Neural-Network-based Real-time Speech Enhancement}
\name{Tyler Vuong,
      Yangyang Xia,
      Richard M. Stern}
      
\address{ Department of Electrical and Computer Engineering, Carnegie Mellon University\\
          Language Technologies Institute, Carnegie Mellon University \\
       \texttt{tvuong@andrew.cmu.edu}, \texttt{raymondxia@cmu.edu}, \texttt{rms@cs.cmu.edu}\\   
}

\begin{document}

   Â© 2021 IEEE.  Personal use of this material is permitted.  Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works
 

\maketitle
\begin{abstract}
We describe a modulation-domain loss function for deep-learning-based speech enhancement systems. Learnable spectro-temporal receptive fields (STRFs) were adapted to optimize for a speaker identification task. The learned STRFs were then used to calculate a weighted \acrfull{mse} in the modulation domain for training a speech enhancement system. Experiments showed that adding the modulation-domain MSE to the MSE in the spectro-temporal domain substantially improved the objective prediction of speech quality and intelligibility for real-time speech enhancement systems without incurring additional computation during inference.
\end{abstract}
\begin{keywords}
Real-time speech enhancement, spectro-temporal receptive field, loss functions
\end{keywords}
\section{Introduction}
\label{sec:intro}
Supervised \gls{se} using deep neural networks (DNNs) has received tremendous attention in recent years. The availability of abundant amounts of training data and  advancements in \acrshort{dnn} architectures have resulted in systems that provide better performance than  the ideal binary mask \cite{luoConvTasNetSurpassingIdeal2019} -- a target that highly correlates with speech intelligibility \cite{loizouReasonsWhyCurrent2011,kryterValidationArticulationIndex1962}. The core design of a   DNN-based SE system involves  decisions that perform compensation in one feature domain and calculate the loss function in another. Despite the emerging time-domain compensation methods (\eg \cite{luoConvTasNetSurpassingIdeal2019}), predicting a time-varying gain function, or a \gls{tf} mask  \cite{wangTrainingTargetsSupervised2014}, has been the most popular and reliable approach.

Loss functions for supervised \gls{se} in the \gls{tf} domain have historically been   calculated in the time or frequency domain. However, most existing loss functions \cite{braunConsolidatedViewLoss2020} were motivated by statistically-optimal solutions \cite{ephraimSpeechEnhancementUsing1984,ephraimSpeechEnhancementUsing1985,limEnhancementBandwidthCompression1979} and do not necessarily correlate with perceptual quality or intelligibility of speech \cite{loizouSpeechEnhancementTheory2013}. More recently, perceptually-motivated loss functions have been proposed to optimize modified predictors of speech quality \cite{martin-donasDeepLearningLoss2018} and intelligibility \cite{zhaoPerceptuallyGuidedSpeech2018}. Interestingly, these methods did not show improvement over objective metrics for which the loss functions did not directly optimize, suggesting that there is room for improving their generalization ability.

Modulation is closely related to  speech intelligibility. The speech transmission index (STI)  measures the extent to which amplitude modulation of speech is preserved in degraded environments and is highly correlated with speech intelligibility \cite{steeneken1980physical,paytonMethodDetermineSpeech1999}.  The \gls{stmi} was subsequently proposed to account for joint spectro-temporal modulation \cite{elhilaliSpectrotemporalModulationIndex2003}. \gls{se} in the modulation domain has also been explored \cite{mesgaraniDenoisingDomainSpectrotemporal2007,mirbagheriNonlinearFilteringSpectrotemporal2010}. However, these methods assume that speech and noise are separable in the modulation domain. Moreover, they typically require a complete set of spectro-temporal receptive fields (STRFs)  in order to invert the processed modulation spectra back to the \gls{tf} domain.  This may  be computationally infeasible for real-time applications.

In this paper, we propose a simple \gls{mse}-based loss function in the spectro-temporal modulation domain for supervised \gls{se}. We call the loss \gls{stme} because of its close relation to template-based \gls{stmi} \cite{elhilaliSpectrotemporalModulationIndex2003}, which correlates well with speech intelligibility. The calculation of the \gls{stme} is based on a set of pre-selected modulation kernels, which had been shown to be critical for the accuracy of predicted speech intelligibility using speech stimuli \cite{paytonMethodDetermineSpeech1999}. Following our recent success in discriminating live speech from synthetic or broadcast speech using learnable \gls{strf} kernels \cite{vuong2020learnable}, we develop an automatic way to determine these kernels through an auxiliary \gls{sid} task. \gls{stme} is applicable to any \gls{dnn} system as long as the \gls{stftm} of the target and degraded speech are accessible when training the \gls{dnn}. Our proposed system's loss is easy to compute, does not incur additional computation during inference, and avoids lossy inversion, which is a problem with conventional modulation-domain SE approaches    \cite{mesgaraniDenoisingDomainSpectrotemporal2007}.

\textbf{Organization of this paper.}
In the next section, we introduce related work in supervised \gls{se} and the background of \gls{stmi}. We then describe our selection procedure for the \gls{strf}s and the loss function. Finally, we describe the evaluation procedure and discuss the experimental results.



\section{BACKGROUND}

\label{sec:prior}
In this section we briefly review supervised speech enhancement and the spectro-temporal modulation index.

\textbf{Supervised \gls{dnn}-based speech enhancement.}
We assume that the observed noisy speech contains clean speech corrupted by additive noise. This relationship in the \gls{stft} domain is described by 

where  and  represent the STFT at time frame  and frequency bin  of the observed noisy speech, clean speech and noise, respectively. Without loss of generality, we assume that a \gls{dnn} is trained to predict a magnitude gain, , from past and current information of degraded speech. The enhanced \gls{stftm} is obtained by element-wise multiplication of  the predicted gain by the noisy \gls{stftm},

A popular loss function that drives the learning for this \gls{dnn} is the \gls{mse} between the enhanced STFTM and the clean STFTM, or the \gls{tfe},

where  and  denote the vector representations of the clean STFTM and enhanced STFTM, respectively.

\textbf{Spectro-temporal modulation index.}
The spectro-temporal modulation index (STMI) is a measure of speech integrity in the modulation domain as viewed by a model of the auditory system \cite{elhilaliSpectrotemporalModulationIndex2003}. At the core of this model is a bank of \gls{strf}s that are believed to exist in the central auditory system and respond to a range of patterns of temporal and spectral modulation \cite{chiMultiresolutionSpectrotemporalAnalysis2005}.  Each \gls{strf} is a \gls{tf} template in which two seed functions control the temporal modulation (rate) and spectral modulation (scale) selectivity, respectively. The \gls{stmr} of a spectrographic representation of speech to a \gls{strf} is defined by the convolution of the two,

 where  is a frequency-integration function followed by a logarithmic compression that mimics the function of the early auditory system. Finally, the template-based \gls{stmi} \cite{elhilaliSpectrotemporalModulationIndex2003} is defined as

where  is typically integrated over time before being converted to the vector form. In previous implementations of  modulation-domain \gls{se}, compensation was performed directly on  \cite{mesgaraniDenoisingDomainSpectrotemporal2007,mirbagheriNonlinearFilteringSpectrotemporal2010}. In our method, we perform enhancement in the \gls{tf} domain and use a loss function in the modulation domain that is closely related to  for training the \gls{dnn}. 

It should be noted that the selection of meaningful modulation frequency becomes an issue when speech signals (instead of modulated noise) are used to calculate an estimate of speech intelligibility \cite{paytonMethodDetermineSpeech1999}. Previous work using \gls{strf}s typically performed dimensionality reduction on features extracted from densely-sampled \gls{strf}s \cite{mesgaraniDiscriminationSpeechNonspeech2006,meyerRobustnessSpectrotemporalFeatures2011,ravuriEasyDoesIt2012}. We learn the parameters of the \gls{strf}s through an auxiliary \gls{sid} task. We describe our own method next.

\section{Method}
In this section, we present the DNN system we used for speech enhancement and the calculation of our spectral-temporal modulation error loss.

\textbf{Speech enhancement system.}
We used the normalized \gls{lps} as the input feature.  The \gls{stft} is first obtained using a 20-millisecond Hamming window with 50\% overlap and a 512-point discrete Fourier transform.  Then we take the natural logarithm of the power of the \gls{stft} and normalize the \gls{lps} with frequency-dependent online normalization following \cite{xiaWeightedSpeechDistortion2020}.

To estimate the magnitude gain for each frame, we used a similar real-time network architecture to the one described in \cite{braunConsolidatedViewLoss2020}.  The network consists of a single fully connected (FC) layer followed by two stacked unidirectional Gated Recurrent Units (GRUs) and three more FC layers.  Rectified linear unit (ReLU) activation is used after each of the FC layers except the very last one where a sigmoid activation is used to bound the output magnitude gain to be between zero and one.  We obtain the enhanced waveform by multiplying the magnitude gain element-wise with the noisy STFTM and using the original noisy phase for reconstruction.  In total, the network contained roughly 2.8 million learnable parameters. 








\textbf{Tuning learnable \gls{strf}s on speaker identification.}
One central problem involving the construction of the loss function is the selection of modulation parameters that are relevant to speech intelligibility \cite{paytonMethodDetermineSpeech1999}. Previous work has shown that the \gls{stmr} is redundant \cite{mesgaraniDiscriminationSpeechNonspeech2006} and the possible values for those modulation parameters span a wide range \cite{meyerRobustnessSpectrotemporalFeatures2011,ravuriEasyDoesIt2012}. Following the success of our previous work on discriminating live speech from synthetic speech using learnable \gls{strf}s \cite{vuong2020learnable}, we  trained the \emph{STRFNet} system on \gls{sid} using the Librispeech \cite{panayotovLibrispeechASRCorpus2015} dataset
with artificially-added noise from Sound Bible\footnote{https://www.openslr.org/17/} to  learn the parameters of each Gabor-based \gls{strf} \cite{meyerRobustnessSpectrotemporalFeatures2011} automatically.  The \gls{sid} system was able to achieve an average of 95\% accuracy with 2484 speakers and signal-to-noise ratios (SNRs) ranging from 0 to 30 dB.  We then keep the learned \gls{strf}s fixed and utilize them for our loss.  We use 60 STRF kernels,  each with a time support of 300 milliseconds and a span of 20 channels on the Mel scale. This pipeline is depicted in the upper panel of \fref{fig:stme}.

\begin{figure}[htb]
\centering
\includegraphics[width = \textwidth]{figures/stme-flow.png}
\caption{Flow diagrams of the \gls{strf} tuning stage (top) and the \gls{stme} loss calculation stage (bottom).}
\label{fig:stme}
\vspace{-4mm}

\end{figure}

\textbf{Loss function.}
Given  \gls{strf}s, we define the response of the speech \gls{stftm} to the  \gls{strf} to be

where  is a frequency integration function using the Mel weighting and  denotes cross-correlation. The loss function is then

where no time integration is applied to obtain the vector form of \gls{stmr}.
In general, the \gls{stme} is strongly motivated by \gls{stmi} and is in fact very similar to the weighted distance term in the definition of  with a few differences. First, the \gls{stme} uses learned \gls{strf} kernels that are discriminatively trained to optimize for a \gls{sid} task. Second, the \gls{stmr} is calculated using cross-correlation instead of convolution. Third, the original auditory spectrogram \cite{chiMultiresolutionSpectrotemporalAnalysis2005} is approximated by the Mel-spectrogram and logarithmic compression. Finally, the \gls{stme} calculates a weighted \gls{mse} using instantaneous response. 

To train our enhancement system, we use the \gls{stme} loss in addition to the standard \gls{tfe} in \eref{eqn:mse_loss}. This is motivated by the observation that the \gls{stmr} is smooth and omits spectral details that are available in the \gls{tf} domain. We believe that the medium-time \gls{stme} loss complements the short-time \gls{tfe} for supervised \gls{se}.















A block diagram of the STME calculation is shown in the bottom panel of \fref{fig:stme}.  In the next section, we will describe the experimental setup used to evaluate the benefits of including the STME in the training loss.  We also assess the value of using automatically-learned Gabor-based STRF kernels compared to randomly-selected kernels.   

\section{Experimental Setup}

\textbf{Datasets.}
We used a small-scale and a large-scale dataset for evaluating the \gls{se} system. The small-scale dataset by Valentini \etal~\cite{valentini2016investigating} (VBD henceforth) contains 9.4 hours and 35 minutes of noisy speech in the training and test set, respectively. We downsampled the entire dataset to 16 kHz.  For the large-scale dataset, we used the Interspeech 2020 Deep Noise Suppression (DNS) dataset \cite{reddyINTERSPEECH2020Deep2020} with RIR responses provided by \cite{reddy2020icassp}.  
The DNS training set contains a total of 500 hours of noisy speech.  
For evaluation, the DNS dataset has two test sets named  and , which both contain 25 minutes of noisy speech.  In both datasets, we are also provided with the original clean speech that was used to artificially generate the noisy speech.  

We evaluated our system's capabilities to improve speaker verification performance in noisy conditions by using a modified version of the VoxCeleb1 test set \cite{nagraniVoxCelebLargeScaleSpeaker2017}.  The original test set contained 4874 speech pairs spoken by 40 unseen speakers.  We modified the VoxCeleb1 test set by randomly adding noise from the DNS test set at SNRs ranging from -6 to 6 dB.



\textbf{Training and evaluation procedure.}
To train our \gls{se} systems on the VBD dataset and DNS dataset, we randomly sampled 1-second noisy speech segments from the training data and 5-second noisy speech segments from the training data, respectively. All the \gls{se} systems were trained using the Adam optimizer with a learning rate of  and a batch size of 64 in PyTorch. For evaluation, we used the perceptual evaluation of speech quality (PESQ) \cite{rixPerceptualEvaluationSpeech2001}, scale-invariant signal-to-distortion ratio \cite{lerouxSDRHalfbakedWell2019}, and short-time objective intelligibility (STOI) \cite{taalShorttimeObjectiveIntelligibility2010} metrics.  

To evaluate our \gls{se} systems on speaker verification, we used a DNN-based speaker verification system \cite{chungDefenceMetricLearning2020} pretrained on VoxCeleb2 \cite{chungVoxCeleb2DeepSpeaker2018}, a dataset that contains over 1 million utterances and 6112 speakers. The verification system obtained an equal error rate (EER) of 2.2\% on the VoxCeleb1 test set, which is one of the lowest reported EER compared to any other method with a similar number of parameters \cite{chungDefenceMetricLearning2020}.  Although the system was not trained with artificial degradation, all the audio clips were extracted from YouTube which will naturally contain different acoustical conditions. To test if our \gls{se} system improves the performance of this strong speaker verification system in noisy conditions, we added noise from the DNS test set to the original VoxCeleb1 test set at SNRs ranging from -6 to 6 dB.  We evaluated the EER of the speaker verification system on clips enhanced by the SE system.



\textbf{Baseline systems.}
We evaluated three different baseline systems to illustrate the benefits of the additional STME loss.  Each of the baseline systems have the exact same network architecture, but they were each trained with different loss functions.  Our first baseline system, GRU(TFE), was trained only with the TFE loss.  To evaluate the benefits of the STME loss by itself, we trained a second baseline system, GRU(STME), using only the STME loss.  Our third baseline system, GRU(TFE+), was trained with both loss terms, although the parameters of the Gabor-based STRF kernels that were used to calculate the STME were randomly selected.  Specifically, the temporal and spectral modulation frequencies are uniformly sampled over  Hz and  cycles per channel, respectively.  This comparison allows us to assess the benefits of using automatically-learned Gabor-based STRF kernels to train the system, GRU(TFE+STME).

\section{Experimental Results and Discussion}In this section, we present and discuss results of our \gls{stme} loss on speech enhancement and speaker verification.

\begin{table}
\centering
\caption{Table summarizing the objective speech quality evaluation on the VBD test set}
\begin{tabular}{l|l|l|l}
Methods                        & PESQ~ & SI-SDR & STOI  \\
                               & (MOS) & (dB)   & (\%)  \\ 
\hline
Noisy                          & 1.97       & 8.5        & 92.1       \\
ERNN \cite{takeuchi2020real}                      & 2.54       &  ---      &  ---     \\
GRU(TFE)                       & 2.68       & \textbf{17.0}       &  93.3     \\
GRU(STME)                      & 2.78      & 14.4       &  93.1     \\
GRU(TFE+STME)         & 2.76       & 16.9        & 93.2       \\
GRU(TFE+STME)                & \textbf{2.82}       & \textbf{17.0}        & \textbf{93.8}     
\end{tabular}
\label{table:Valentini-results}
\vspace{-.5 cm}

\end{table}

\begin{table}
\scalebox{.87}{
\label{table:DNS-result}
\centering
\caption{Table summarizing the objective speech quality evaluation on the DNS   () test set}


\begin{tabular}{l|l|l|l}
Method                 & PESQ        & SI-SDR        & STOI         \\
                       & (MOS)       & (dB)          & (\%)         \\ 
\hline
Noisy                  & 1.58 (1.82) & 9.1 (9.0)   & 91.5 (86.6)  \\
DNS Baseline \cite{xiaWeightedSpeechDistortion2020} & 1.83 (1.52) & 12.5 (9.2)  & 90.6 (82.1)  \\
GRU(TFE)               & 2.27 (2.36) & 14.9 (13.2) & 94.2 (89.4)  \\
GRU(STME)              & 2.59 (2.64) & 12.4 (12.0) & 94.2 (90.1)  \\
GRU(TFE+STME)          & 2.57 (2.63) & \textbf{15.9 (14.5)} & 95.2 (90.6)  \\
GRU(TFE+STME)          & \textbf{2.71 (2.75)} & \textbf{15.9 (14.5)} & \textbf{95.5 (91.2)} 
\end{tabular}
}
\end{table}

\textbf{VBD results.}
In Table \ref{table:Valentini-results}, we show the objective evaluation of each SE system trained with different losses using the VBD dataset.  On a small dataset, our GRU(TFE+STME) outperformed the GRU(TFE) baseline in all the objective metrics.  Interestingly, different initialization of the STRF kernels in GRU(TFE+STME) resulted in similar improvements over the baseline TFE loss, but roughly 20\% of the time the random parameter selection resulted in a much worse performance.  This highlights the benefits of using automatically-learned Gabor-based STRF kernels over randomly-selected Gabor-based STRF kernels.    

\textbf{DNS results.}
The objective evaluation of our SE systems trained with different losses using the DNS   and  test set is summarized in Table 2.  Even with a large amount of training data, our  GRU(TFE+STME) loss function outperformed both the GRU(TFE) baseline and the provided challenge baseline \cite{xiaWeightedSpeechDistortion2020} in all the objective metrics.  Most notably, there is a significant improvement in PESQ which results in our system having a similar PESQ to the top system in the official DNS challenge \cite{isik2020poconet}.  We also evaluated the benefits of the \gls{stme} loss by itself, GRU(STME).  Curiously, training with only our the \gls{stme} loss provides a higher PESQ but much lower SI-SDR compared to training with only the TFE loss.  Nevertheless,  optimizing the combination of both losses  during training caused both the PESQ and SI-SDR scores to increase compared to training on each loss individually. This confirms our belief that the medium-time \gls{stme} loss is complemented by the short-time \gls{tfe} loss. As in the VBD experiments, the use of  automatically-learned Gabor-based STRF kernels provides a greater increase of PESQ scores compared to the use of randomly-selected Gabor-based STRF kernels.
\begin{figure}
\scalebox{.65}{
\centering
\includegraphics[width=5 in]{plots/final_asv_enhanced.pdf}
  \caption{Equal error rates on the VoxCeleb1 Test Set.}
\label{fig:EER_ASV}



  }
  \vspace{-5mm}
\end{figure}

\textbf{Speaker verification results.}
The performance of the speaker verification system with noise added at low SNRs is shown in Figure \ref{fig:EER_ASV}.  At low SNRs, the speaker verification system's performance starts to substantially degrade.  Our system GRU(TFE+STME) improved the EER by an average of 15.4\% relative and outperformed our baseline GRU(TFE) by an average of 5.5\% relative.  At higher SNRs, the verification system's performance quickly approached the state-of-the-art performance of 2.2\% and our enhancement systems did not provide any additional benefits. 
\section{Conclusions}
In this paper, we introduced a novel modulation-domain loss for training neural-network-based speech enhancement systems.  We showed that by adding   spectro-temporal modulation error to the standard time-frequency error during training, all three common objective speech quality metrics substantially improved on two different datasets.  Additionally, we demonstrated the value of utilizing automatically-learned Gabor-based STRF kernels over randomly-selected kernels.  We also showed that our speech enhancement system can improve a strong speaker verification system at low SNRs.  In the future, we plan on exploring deep-learning-based techniques to perform SE directly in the modulation domain.  We will also explore ways of directly optimizing the STRF parameters for speech enhancement.

\vfill\pagebreak

\bibliographystyle{IEEEtran}



\patchcmd{\thebibliography}
  {\settowidth}
  {\setlength{\itemsep}{0pt plus 0.1pt}\settowidth}
  {}{}
\apptocmd{\thebibliography}
  {\footnotesize}
  {}{}    
\bibliography{ICASSP2021}


\end{document}
