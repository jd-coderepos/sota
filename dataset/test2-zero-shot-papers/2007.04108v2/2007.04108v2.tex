

\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{amsmath,amssymb} \usepackage{color}
\usepackage{mypackages}

\def\myalgonamefirst{TRAS\@\xspace}
\def\myalgonamesecond{TRAST\@\xspace}
\def\myalgonamethird{TRASFUST\@\xspace}
\def\imagestripwidth{1\textwidth}
\def\vaImWidth{0.32\linewidth}
\def\vaImVisualResWidth{0.195\linewidth}
\def\samplesImWidth{0.33\linewidth}
\def\vspaceafterfigure{\vspace{-0.5em}}
\def\vspaceaftertable{\vspace{-0.6em}}
\def\vspaceafteralgo{\vspace{0em}}
\def\algoindent{\hspace{1em}}


\graphicspath{{./images/}{./figures/}}
\def\reals{\mathbb{R}}
\def\image{\mathbf{I}}
\def\images{\mathcal{I}}
\def\imagesetsize{n}
\def\dataset{\mathcal{D}}
\def\video{\mathcal{V}}
\def\episode{E}
\def\mdp{M}
\def\frame{F}
\def\demonstration{D}
\def\bbox{b}
\def\bboxgt{g}
\def\state{s}
\def\states{\mathcal{S}}
\def\action{a}
\def\actions{\mathcal{A}}
\def\reward{r}
\def\statevalue{v}
\def\counter{C}
\def\contextfactor{c}

\def\tkcf{\texttt{KCF}}
\def\tmdnet{\texttt{MDNet}}
\def\teco{\texttt{ECO}}
\def\tsiamrpn{\texttt{SiamRPN}}
\def\tatom{\texttt{ATOM}}
\def\tdimp{\texttt{DiMP}}
\def\tpool{\texttt{POOL}}
\def\tk{\teachers_{\texttt{K}}}
\def\tm{\teachers_{\texttt{M}}}
\def\te{\teachers_{\texttt{E}}}
\def\ts{\teachers_{\texttt{S}}}
\def\tp{\teachers_{\texttt{P}}}
\def\ta{\teachers_{\texttt{A}}}
\def\td{\teachers_{\texttt{D}}}

\def\IoU{\text{IoU}}

\def\dfiltcoeff{\beta}

\def\featuremapindex{c}
\def\numfeaturemaps{C}

\def\featfun{f}
\def\featfunpars{\mathcal{W}}

\def\mappingfun{\mathcal{H}}
\def\mappingfunout{\mathbf{h}}
\def\pyrpoolingfun{\phi}
\def\poolingconvfun{\mathcal{P}}
\def\poolingfun{\psi}
\def\poolingvec{\mathbf{p}}

\def\convweight{\mathbf{w}}

\def\numpyrconv{k}
\def\numpyrconvin{\numpyrconv^{-1}}
\def\numpyrconvout{\widehat{k}}
\def\numpyrstripes{s}
\def\pyrlevel{r}
\def\numpyrlevels{m}
\def\stripeheight{\delta}

\def\attentionweight{\mathbf{w}}

\def\loss{\mathcal{L}}
\def\classificationloss{\loss_{\mathrm{id}}}
\def\similarityloss{\loss_{\mathrm{sim}}}
\def\similaritylossmargin{\alpha}
\def\distillationloss{\loss_{\mathrm{dist}}}
\def\policyloss{\loss_{\pi}}
\def\valueloss{\loss_{\statevalue}}
\def\agent{\mathbf{a}}
\def\teacher{\mathbf{t}}
\def\teachers{\mathbf{T}}
\def\student{\mathbf{s}}
\def\studentparam{\mathbf{s}(\cdot \:|\: \theta)}
\def\weights{\theta}

\def\mixedlossweight{\lambda}

\def\mybn{\texttt{BN}\@\xspace}
\def\myrelu{\texttt{ReLU}\@\xspace}
\def\myleakyrelu{\texttt{Leaky ReLU}\@\xspace}
\def\myconv{\texttt{Conv}\@\xspace}
\def\myfc{\texttt{FC}\@\xspace}
\def\mydropout{\texttt{Dropout}\@\xspace}

\newcommand*{\smallcup}{\mathbin{\scalebox{0.5}{\ensuremath{\cup}}}}\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\orange}[1]{\textcolor{orange}{#1}}

\setlength{\textfloatsep}{7pt plus 1.0pt minus 2.0pt}
\setlength{\floatsep}{3pt plus 1.0pt minus 2.0pt}


\begin{document}
\pagestyle{headings}
\mainmatter

\def\ACCV20SubNumber{141}  

\title{Tracking-by-Trackers \\with a Distilled and Reinforced Model} \titlerunning{Tracking-by-Trackers with a Distilled and Reinforced Model}
\author{Matteo Dunnhofer \orcidID{0000-0002-1672-667X},
Niki Martinel \orcidID{0000-0002-6962-8643}, \\
Christian Micheloni \orcidID{0000-0003-4503-7483} \\
\footnotesize{\email{\{matteo.dunnhofer, niki.martinel, christian.micheloni\}@uniud.it}}
}



\authorrunning{M. Dunnhofer et al.}
\institute{Machine Learning and Perception Lab, University of Udine, Italy}


\maketitle

\begin{abstract}
Visual object tracking was generally tackled by reasoning independently on fast processing algorithms, accurate online adaptation methods, and fusion of trackers. 
In this paper, we unify such goals by proposing a novel tracking methodology that takes advantage of other visual trackers, offline and online. 
A compact student model is trained via the marriage of knowledge distillation and reinforcement learning. The first allows to transfer and compress tracking knowledge of other trackers. The second enables the learning of  evaluation measures which are then exploited online. After learning, the student can be ultimately used to build (i) a very fast single-shot tracker, (ii) a tracker with a simple and effective online adaptation mechanism, (iii) a tracker that performs fusion of other trackers.
Extensive validation shows that the proposed algorithms compete with real-time state-of-the-art trackers. \end{abstract}

\section{Introduction}
\label{sec:intro}
Visual object tracking corresponds to the persistent recognition and localization --by means of bounding boxes-- of a target object in consecutive video frames.
This problem comes with several different challenges including object occlusion and fast motion, light changes, and motion blur.
Additionally, real-time constraints are often posed by the many practical applications, such as video surveillance, behavior understanding, autonomous driving, and robotics.

In the past, the community has proposed solutions emphasizing different aspects of the problem. Processing speed was pursued by algorithms like correlation filters \cite{Bolme2010,KCF,DSST,Staple,Lukezic2018} or offline methods such as siamese convolutional neural networks (CNNs) \cite{GOTURN,RE3,SiamFC,SiamRPN,SiamRPNpp,DaSiam,Zhang2019}. 
Improved performance was attained by online target adaptation methods \cite{MDNet,RealTimeMDNet,ECO,ATOM,DiMP}.
Higher tracking accuracy and robustness were achieved by methods built on top of other trackers \cite{MEEM,Yoon2012,Wang2014,Bailer2014,HMMTxD}. All these characteristics belong to an optimal tracker but they were studied one independently from the other. The community currently lacks a general framework to tackle them jointly. In this view, a single model should be able to (i) track an object in a fast way, (ii) implement simple and effective online adaptation mechanisms, (iii) apply decision-making strategies to select tracker outputs.

It is a matter of fact that a large number of tracking algorithms has been produced so far, with different principles exploited. 
Preliminary solutions were based on mean shift algorithms \cite{Comanciu2000}, key-point \cite{Matrioska} or part-based methods \cite{LGT,OGT}, or SVM learning \cite{Struck}.
Later, correlation filters gained popularity thanks to their fast processing times \cite{Bolme2010,KCF,DSST,Staple,Lukezic2018}. 
Since more recently, CNNs have been exploited to extract efficient image features. This kind of representation has been included in deep regression networks \cite{GOTURN,RE3}, online tracking-by-detection methods \cite{MDNet,RealTimeMDNet}, solutions that treat visual tracking as a reinforcement learning (RL) problem \cite{Yun2017,Supancic2017,Choi2017,Ren2018,Chen2018,Dunnhofer2019}, CNN-based discriminative correlation filters \cite{CCOT,ECO,ATOM,DiMP}, and in siamese CNNs \cite{SiamFC,SiamRPN,SiamRPNpp,Zhang2019,SiamMask,Dunnhofer2020MedIA}. 
Other methods tried to take advantage of the output produced by multiple trackers \cite{MEEM,Yoon2012,Wang2014,Bailer2014,HMMTxD}.
Thus, one can imagine that different trackers incorporate different knowledge, and this may constitute a valuable resource to leverage during tracking. 

Lately, the knowledge distillation (KD) framework \cite{Hinton2014KD} was introduced in the deep learning panorama as paradigm for, among the many \cite{He2016ResNet,Tang2016,Li2017KDnoise,Phuong2019}, knowledge transferring between models \cite{Geras2015} and model compression \cite{Chen2017,Howard2017,Polino2018}.
The idea boils down into considering a student model and one or more teacher models to learn from. Teachers explicit their knowledge through demonstrations on a never seen before transfer set. Through specific loss functions, the student is set to learn a task by matching the teachers' output and the ground-truth labels.
As visual tracking requires fast and accurate methods, KD can be a valuable tool to transfer the tracking ability of more accurate teacher trackers to more compact and faster student ones.
However, the standard setup of KD does not provide methods to exploit teachers online, but just offline. 
This makes this methodology unsuitable for tracking, which has been shown to benefit from both offline and online methods \cite{ATOM,DiMP,ECO,Yun2017,Chen2018}.
In contrast to such an issue, RL techniques offer established methodologies to optimize not only policies but also policy evaluation functions \cite{Watkins1992,Konda2000,Sutton2000,Mnih2013,Mnih2016}, which are then used to extract decision strategies. Along with this, RL also gives the possibility to maximize arbitrary and non-differentiable performance measures, and thus more tracking oriented objectives can be defined.


For the aforementioned motivations, the contribution of this paper is a novel tracking methodology where a student model exploits off-the-shelf trackers offline and online (tracking-by-trackers). The student is first trained via an effective strategy that combines KD and RL. After that, the model's compressed knowledge can be used interchangeably depending on the application's needs.
We will show how to exploit the student in three setups which result in, respectively, (i) a fast tracker (\myalgonamefirst), (ii) a tracker with a simple online mechanism (\myalgonamesecond), and (iii) a tracker capable of expert tracker fusion (\myalgonamethird). 
Through extensive evaluation procedures, it will be demonstrated that each of the algorithms competes with the respective state-of-the-art class of trackers while performing in real-time.
 
\section{Related Work}

\subsubsection{Visual Tracking.} Here we review the trackers most related to ours. The network architecture implemented by the proposed student model takes inspiration from GOTURN \cite{GOTURN} and RE3 \cite{RE3}. These regression-based CNNs were shown to capture the target's motion while performing very fast. However, the learning strategy employed optimizes parameters just for coordinate difference. Moreover, great amount of data is needed to make such models achieve good accuracy. In contrast, our KD-RL-based method offers parameter optimization for overlap maximization and extracts previously acquired knowledge from other trackers requiring less labeled data.
Online adaptation methods like discriminative model learning \cite{TLD,MDNet,RealTimeMDNet} or discriminative correlation filters \cite{ECO,ATOM,DiMP} have been studied extensively to improve tracking accuracy. These procedures are time-consuming and require particular assumptions and careful design. We propose a simple online update strategy where an off-the-shelf tracker is used to correct the performance of the student model. Our method does not make any assumption on such tracker, and thus it can be freely selected to adapt to application needs.
Present fusion models exploit trackers in the form of discriminative trackers \cite{MEEM}, CNN feature layers \cite{Qi2016}, correlation filters \cite{Li2019} or out-of-the-box tracking algorithms \cite{Yoon2012,Wang2014,Bailer2014,HMMTxD}. However, such models work just online and do not take advantage of the great amount of offline knowledge that expert trackers can provide. Furthermore, they are not able to track objects without them. Our student model addresses these issues thanks to the decision making strategy learned via KD and RL.


\subsubsection{KD and RL.} We review the learning strategies most related to ours. KD techniques have been used for transferring knowledge between teacher and student models \cite{Bucila2006,Hinton2014KD}, where the supervised learning setting was employed more \cite{Geras2015,He2016ResNet,Tang2016,Li2017KDnoise} than the setup that uses RL \cite{Rusu2016,Parisotto2016}. 
In the context of computer vision, KD was employed for action recognition \cite{Garcia2018KDAR,Wank2019KDAR}, object detection \cite{Chen2017,Shmelkov2017}, semantic segmentation \cite{Liu2019semantic,He2019}, person re-identification \cite{Wu2019KDreid}. 
In the visual tracking panorama, KD was explored in \cite{Wang2019,Liu2019} to compress, CNN representations for correlation filter trackers and siamese network architectures, respectively. However, these works offer methods involving teachers specifically designed as correlation filter and siamese trackers, and so cannot be adapted to generic-approach visual trackers as we propose in this paper. Moreover, to the best of our knowledge, no method mixing KD and RL is currently present in the computer vision literature.
Our learning procedure is also related to the strategies that use deep RL to learn tracking policies \cite{Yun2017,Supancic2017,Ren2018,Chen2018,Meshgi2019}. Our formulation shares some characteristics with such methods in the markov decision process (MDP) definition, but our proposed learning algorithm is different as no present method leverages on teachers to learn the policy.
 
\section{Methodology}
The key point of this paper is to learn a simple and fast student model with versatile tracking abilities. KD is used for transferring the tracking knowledge of off-the-shelf trackers to a compressed model. 
However, as both offline and online strategies are necessary for tracking \cite{ATOM,DiMP,Yun2017,Chen2018}, we propose to augment the KD framework with an RL optimization objective. RL techniques deliver unified optimization strategies to directly maximize a desired performance measure (in our case the overlap between prediction and ground-truth bounding boxes) and to predict the expectation of such measure. We use the latter as base for an online evaluation and selection strategy. Put in other words, combining KD and RL lets the student model extract a tracking policy from teachers, improve it, and express its quality through an overlap-based objective.




\subsection{Preliminaries} 
Given a transfer set of videos , we consider the -th video

as a sequence of frames , where  is the space of RGB images. 
 Let  be the -th bounding box defining the coordinates of the top left corner, and the width and height of the rectangle that contains the target object.
At time , given the current frame , the goal of the tracker is to predict  that best fits the target in .
We formally consider the student model as 

that is a function which outputs the relative motion between  and , and the performance evaluation , when inputted with frame .
Similarly, we define the set of tracking teachers as

where each  is a function that, given a frame image, produces a bounding box estimate for that frame.

\begin{figure}[t]\centering
\begin{minipage}[b]{.49\textwidth}
\centering
\includegraphics[width=.75\columnwidth]{images/framework.jpg}
\caption{Scheme of the proposed KD-RL-based learning framework.  students interact independently with . After each  done, a copy  of the shared weights  is sent to each one. Every  steps each student send the computed gradients to apply an update on . The distilling students (highlighted by the orange dashed contour) extract knowledge from teachers  by optimizing . Autonomous students (circled with the blue dashed contour) learn an autonomous tracking policy by optimizing jointly  and .}
\label{fig:framework}
\end{minipage}\hfill
\begin{minipage}[b]{.49\textwidth}
\centering
\includegraphics[width=.9\columnwidth]{images/studentarch.jpg}
   \caption{The student architecture is composed by two branches of convolutional layers (gray boxes) with shared weights followed by, two fully-connected layers (orange boxes), an LSTM layer (in green) and two parallel fully connected layers for the prediction of  and   respectively.}
\label{fig:arch}
\includegraphics[width=.9\columnwidth]{images/trackers.jpg}
\caption{Visual representation showing how the student and teachers are employed in the proposed trackers at every frame . (a) represents \myalgonamefirst, (b) \myalgonamesecond, and (c) \myalgonamethird.}
\label{fig:trackers}
\end{minipage}
\end{figure}

\subsection{Visual Tracking as an MDP}
\label{sec:votmdp}
In our setting,  is treated as an artificial agent which interacts with an MDP defined over a video . 
The interaction happens through a temporal sequence of states , actions  and rewards . In the -th frame, the student is provided with the state  and outputs the continuous action  which consists in the relative motion of the target object, i.e. it indicates how its bounding box, which is known in frame , should move to enclose the target in the frame .  is rewarded by the measure of its quality . We refer this interaction process as the episode , which dynamics are defined by the MDP 
.

\subsubsection{States.} Every  is defined as a pair of image patches obtained by cropping  and  using .
Specifically, , where  crops the frames  within the area of the bounding box  that has the same center coordinates of  but which width and height are scaled by . By selecting , we can control the amount of additional image context information to be provided to the student. 

\subsubsection{Actions and State Transition.} Each  consists in a vector  which defines the relative horizontal and vertical translations (, respectively) and width and height scale variations (, respectively) that have to be applied to  to predict . The latter step is obtained through .\footnote{Please refer to Appendix \ref{sec:mdpaux} for the definition of .}
After performing , the student moves through  from  to  which is defined as the pair of cropped images obtained from  and  using .

\subsubsection{Reward.} The reward function expresses the quality of  taken at  and it is used to feedback the student. 
Our reward definition is based on the Intersection-over-Union (IoU) metric computed between  and the ground-truth bounding box, denoted as , i.e., 
At every interaction step , the reward is formally defined as

with
 that
floors to the closest  digit and shifts the input range from  to . 
 
\subsection{Learning Tracking from Teachers}
The student  is first trained in an offline stage. Through KD, knowledge is transferred from  to . By means of RL, such knowledge is improved and the ability of evaluating its quality is also acquired. All the gained knowledge will be used for online tracking.
We implement  as a parameterized function  that given  outputs at the same time the action  and state-value . In RL terms,  maintains representations of both the policy  and the state value   functions. 
The proposed learning framework, which is depicted in Figure \ref{fig:framework}, provides a single offline end-to-end learning stage. 
 students are distributed as parallel and independent learning agents. Each one owns a set of learnable weights  that are used to generate experience by interacting with . The obtained experience,
in the form of , is used to update asynchronously a shared set of weights . After ending , each student updates its  by copying the values of the currently available . The entire procedure is repeated until convergence. This learning architecture follows the recent trends in RL that make use of distributed algorithms to speed up the training phase \cite{Gorila,Mnih2016,IMPALA}. 
We devote half of the students, which we refer to as distilling students, in acquiring knowledge from the teachers' tracking policy. The other half, called autonomous students, learn to track by interacting with  autonomously. 

\subsubsection{Distilling Students.} Each distilling student interacts with  by observing states, performing actions and receiving rewards just as an autonomous student. However, to distill knowledge independently from the teachers' inner structure, we propose the student to learn from the actions of , which are executed in parallel. In particular,  is exploited every  steps with the following loss function

which is the L1 loss between the actions performed by the student and the actions  that the teacher would take to move the student's bounding box  into the teacher's prediction .\footnote{Please refer to Appendix \ref{sec:mdpaux} for the definition of .} At every ,  is selected as 
 
as we would like to learn always from the best teacher.
The absolute values are multiplied by . Each of these is computed along the interaction and determines the status in which  performed worse than  () or better () in terms of the rewards  and . The whole Eq. (\ref{eq:distloss}) is similar to what proposed in \cite{Chen2017} for KD from bounding-box predictions of object detectors. However, here we provide a temporal formulation of such objective and we swap the L2 loss with the L1, which was shown to work better for regression-based trackers \cite{GOTURN,RE3}.
By optimizing Eq. (\ref{eq:distloss}), the weights  are changed only if the student's performance is lower than the performance of the teacher. In this way, we make the teacher transferring its knowledge by suggesting actions only in bad performing cases. In the others, we let the student free to follow its current tracking policy since it is superior. 

\subsubsection{Autonomous Students.} The learning process performed by the autonomous students follows the standard RL method for continuous control \cite{SuttonBarto2018}. Each student interacts with  for a maximum of  steps. At each step , the students sample actions from a normal distribution , where the mean is defined as the student's predicted action, , and the standard deviation is obtained as  (which is the absolute value of the difference between the student's action and the action that obtains, by shifting , the ground-truth bounding box ). Intuitively,  shrinks when  is close to the ground-truth action , reducing the chance of choosing potential wrong actions when approaching the correct one. On the other hand, when  is distant from ,  spreads letting the student explore more. 
The students also predict  which is the cumulative reward that the student expects to receive from  to the end of the interaction. Since the proposed reward definition is a direct measure of the IoU occurring between the predicted and the ground-truth bounding boxes,  gives an estimate of the total amount of IoU that  expects to obtain from state  on wards. Thus, this function can be exploited as a future-performance evaluator. 
After  steps of interaction, the gradient to update the shared weights  is built as

where (\ref{eq:policyloss}) is the policy loss and 
(\ref{eq:valueloss}) is the value loss. These definitions follow the standard advantage actor-critic objective \cite{Sutton2000}. 

To further facilitate and improve the learning, a curriculum learning strategy \cite{Bengio2009} is built for each parallel student. During learning, the length of the interaction is increased as  performs better than . Details are given in Appendix \ref{sec:curriculum}.

\subsection{Student Architecture}
The architecture used to maintain the representation of both the policy  and the state value  functions, which is pictured in Figure \ref{fig:arch}, is simple and presents a structure similar to the one proposed in \cite{GOTURN,RE3}. 
The network gets as input two image patches that pass through two ResNet-18 based \cite{He2016ResNet} convolutional branches that share weights. The feature maps produced by the branches are first linearized, then concatenated together and finally fed to two consecutive fully connected layers with ReLU activations. After that, features are given to an LSTM \cite{Hochreiter1997LSTM} layer. Both the fully connected layers and the LSTM are composed of 512 neurons. The output of the LSTM is ultimately fed to two separate fully connected heads, one that outputs the action  and the other that outputs the value of the state .


\subsection{Tracking after Learning}
After the learning process, the student  is ready to be used for tracking. Here we describe three different ways in which  can be exploited:
\begin{enumerate}
    \item the student's learned policy  is used to predict bounding boxes  independently from the teachers. We call this setting \myalgonamefirst (TRAcking Student).
    \item the learned policy  and value function  are used to, respectively, predict  and evaluate  and  tracking behaviors, in order to correct the former's performance. We refer to this setup as \myalgonamesecond (TRAcking Student and Teacher).
    \item the learned state-value function  is used to evaluate the performance of the pool of teachers  in order to choose the best  and perform tracker fusion. We call this setup \myalgonamethird (TRAcking by Student FUSing Teachers).
\end{enumerate}

In the following, we provide more details about the three settings. For a better understanding, the setups are visualized in Figure \ref{fig:trackers}.

\subsubsection{\myalgonamefirst.} In this setting, each tracking sequence , with target object outlined by , is considered as  described in section \ref{sec:votmdp}. States  are extracted from frames , actions are performed by means of the student's learned policy  and are used to output the bounding boxes . 
This setup is fast as it requires just a forward pass through the network to obtain a bounding box prediction.

\subsubsection{\myalgonamesecond.} 
In this setup, the student makes use of the learned  to predict  and  to evaluate its running tracking quality and the one of  which is run in parallel. 
In particular, at each time step ,  and  are obtained as performance evaluation for  and  respectively. The teacher state is obtained as . By comparing the two expected returns, \myalgonamesecond decides if to output the student's or the teacher's bounding box. More formally, if  then  otherwise . This assignment has the side effect of correcting the tracking behaviour of the student as, at the successive time step, the previously known bounding box becomes the previous prediction of the teacher. 
Thus, the online adaption consists in a very simple procedure that evaluates 's performance to eventually pass control to it. Notice that, at every , the execution of  is independent from  as the second does not need the first to finish because the evaluations are done based on the predictions given at . Hence, the executions of the two can be put in parallel, with the overall speed of \myalgonamesecond resulting is the lowest between the one of  and . 

\subsubsection{\myalgonamethird.} 
\label{sec:trasfust}
In this tracking setup, just the student's learned state-value function  is exploited. At each step , teachers  are executed following their standard methodology. States  are obtained. The performance evaluation of the teachers is obtained through the student as . The output bounding box is selected as  by considering the teacher  that achieves the highest expected return, i.e. 
 
This procedure consists in fusing sequence-wise the predictions of  and, similarly as for \myalgonamesecond, the execution of teachers and student can be put in parallel. In such setting, the speed of \myalgonamethird results in the lowest between the ones of  and of each . 
 



















\begin{table*}[t]
\fontsize{5}{6}\selectfont
\centering
	\caption{Teacher-based statistics of the transfer set.}
	\label{tab:demostats}
\setlength\tabcolsep{.05cm}
\begin{tabular}{l | c c c | c c c | c c c | c c c | c c c }
		\toprule

		Teachers & \multicolumn{3}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{3}{c|}{} &  \multicolumn{3}{c|}{}  &  \multicolumn{3}{c}{}  \\

		 & \# traj & AO &  & \# traj & AO &   & \# traj & AO &  & \# traj & AO &  & \# traj & AO &  \\
		\midrule
		 & 1884 &	0.798 & 9225 & 1097 & 0.836 & 5349 & 439 & 0.873 & 2122 & 73 & 0.914 & 356 & 0 & 0.0 & 0  \\
		 & 1600 & 0.767 & 7859 & 781 & 0.808 & 3831 & 216 & 0.851 & 1052 & 18 & 0.898 & 86 & 0 & 0.0 & 0  \\
		 & 2754 & 0.808 & 13526 & 1659	& 0.843	& 8122	& 720 & 0.879 & 3507 & 160 & 0.915 & 773 & 1 & 0.954 & 4 \\
		 & 3913 & 0.829 & 19259 & 2646	& 0.854	& 12997	& 1447 & 0.878 & 7080 & 431 & 0.908 & 2097 & 9 & 0.947 & 42  \\
		 & 4519	& 0.840	& 22252	& 3092 & 0.863 & 15195 & 1698 & 0.887 & 8307 & 496 & 0.915 & 2414 & 10 & 0.948 & 46 \\
		\bottomrule		
\end{tabular}
\end{table*} 
\section{Experimental Results}

\subsection{Experimental Setup}

\subsubsection{Teachers.}
The tracking teachers selected for this work are KCF \cite{KCF}, MDNet \cite{MDNet}, ECO \cite{ECO}, SiamRPN \cite{SiamRPN}, ATOM \cite{ATOM}, and DiMP \cite{DiMP}, due to their established popularity in the visual tracking panorama. Moreover, since they tackle visual tracking by different approaches, they can provide knowledge of various quality. 
In experiments, we considered exploiting single teacher or a pool of teachers. In particular, the following sets of teachers were examined . 


\subsubsection{Transfer Set.}
The selected transfer set was the training set of GOT-10k dataset \cite{GOT10k}, due to its large scale. 
Just  were used for offline learning, as none of these was trained on this dataset. This is an important point because unbiased examples of the trackers' behavior should be exploited to train the student.
Moreover, predictions that exhibit meaningful knowledge should be retained. Therefore, we filtered out all the videos  which teacher predictions did not satisfy   for all . We considered  as minimum threshold for a prediction to be considered positive, and we then varied  among 0.6, 0.7, 0.8, 0.9 for more precise predictions.
To produce more training samples, videos, and filtered trajectories were split in five randomly indexed sequences of 32 frames and bounding boxes, similarly as done in \cite{RE3}.
In Table \ref{tab:demostats} a summary of  is presented. The number of positive trajectories, the average overlap (A0) \cite{GOT10k} on the transfer set, and the total number of sequences  are reported per teacher and per .

\subsubsection{Benchmarks and Performance Measures.}
We performed performance evaluations on the GOT-10k test set \cite{GOT10k}, UAV123 \cite{UAV123}, LaSOT \cite{LaSOT}, OTB-100 \cite{OTB} and VOT2019 \cite{VOT2019} datasets. These offer videos of various nature and difficulty, and are all popular benchmarks in the visual tracking community.
The evaluation protocol used for GOT-10k is the one-pass evaluation (OPE) \cite{OTB}, along with the metrics: AO, and success rates (SR) with overlap thresholds  and . For UAV123, LaSOT, and OTB-100 the OPE method was considered with the area-under-the-curve (AUC) of the success and precision plot, referred to as success score (SS) and precision scores (PS) respectively \cite{OTB}. Evaluation on VOT2019 is performed in terms of expected average overlap (EAO), accuracy (A), and robustness (R) \cite{VOT}.
Further details about the benchmarks are given in Appendix \ref{sec:benchmarks}.

\subsubsection{Implementation Details.}
The image crops of  were resized to  pixels and standardized by the mean and standard deviation calculated on the ImageNet dataset \cite{ImageNet}. The ResNet-18 weights were pre-trained for image classification on the same dataset \cite{ImageNet}. The image context factor  was set to . The training videos were processed in chunks of 32 frames. At test time, every 32 frames, the LSTM's hidden state is reset to the one obtained after the first student prediction (i.e. ), following \cite{RE3}.
Due to hardware constraints, a maximum of  training students were distributed on 4 NVIDIA TITAN V GPUs of a machine with an Intel Xeon E5-2690 v4 @ 2.60GHz CPU and 320 GB of RAM. 
The discount factor  was set to 1. The length of the interaction before an update was defined in  steps. 
The Radam optimizer~\cite{Radam} was employed and the learning rate for both distilling and autonomous students was set to . A weight decay of  was also added to  as regularization term. To control the magnitude of the gradients and stabilize learning,  was multiplied by .
The student was trained until the validation performance on the GOT-10k validation set stopped improving. Longest trainings took around 10 days.
The speed of the parallel setups of \myalgonamesecond and \myalgonamethird was computed by considering the speed of the slowest tracker (student or teacher) plus an overhead.
Code was implemented in Python and is available here\footnote{\url{https://github.com/dontfollowmeimcrazy/vot-kd-rl}}.
Source code publicly available was used to implement the teacher trackers. Default configurations were respected as much as possible. For a fair comparison, we report the results of such implementations, that have slightly different performance than stated in the original papers. 

\begin{table*}[t]
\fontsize{5}{6}\selectfont
\centering
	\caption{Performance of the proposed trackers. Results of removing some components of our methodology are also reported. Best values, per contribution, are highlighted in red.}
	\label{tab:ablation}
\setlength\tabcolsep{.08cm}
\begin{tabular}{ l | c c c | c c | c c | c c }
		\toprule
		& \multicolumn{3}{c|}{GOT-10k} & \multicolumn{2}{c|}{UAV123} &  \multicolumn{2}{c|}{LaSOT} & \multicolumn{2}{c}{OTB-100}  \\

		\multirow{-2}{*}{Contribution} & AO & SR & SR & SS & PS & SS & PS & SS & PS  \\
		\midrule
		\myalgonamefirst-GT & 0.444 & 0.495 & 0.286 & 0.483 & 0.616 & 0.331 & 0.271 & 0.438 & 0.581 \\
		\myalgonamefirst-KD-GT & 0.448 & 0.499 & 0.305 & 0.491 & 0.630 & 0.354 & 0.298 & 0.448 & 0.606 \\
		\myalgonamefirst-KD & 0.422 & 0.481 & 0.239 & 0.494 & 0.634 & 0.340 & 0.276 & 0.457 & 0.635 \\
		\myalgonamefirst-no-curr & 0.474 & 0.547 & 0.307 & 0.501 & 0.644 & 0.385 & 0.323 & 0.447 & 0.600 \\
		\myalgonamefirst & 0.484 & 0.556 & 0.326 & 0.515 & 0.655 & 0.386 & 0.330 & 0.481 & 0.644 \\
		\myalgonamesecond-no-curr & 0.530 & \tblbest{0.630} & \tblbest{0.347} & 0.602 & 0.770 & 0.484 & 0.464 & 0.595 & 0.794 \\
		\myalgonamesecond & \tblbest{0.531} & 0.626 & 0.345 & 0.603 & 0.773 & 0.490 & 0.470 & 0.604 & 0.818 \\
		\myalgonamethird-no-curr & 0.506 & 0.599 & 0.278 & 0.627 & 0.819 & 0.496 & 0.484 & \tblbest{0.665} & 0.879 \\
		\myalgonamethird & 0.519 & 0.616 & 0.287 & \tblbest{0.628} & \tblbest{0.823} & \tblbest{0.510} & \tblbest{0.505} & 0.660 & \tblbest{0.890} \\
		\bottomrule		
\end{tabular}
\end{table*} 
\begin{figure}[t]\begin{minipage}[t]{.49\textwidth}
\centering
\includegraphics[width=\columnwidth]{images/trastcontrol.jpg}
\caption{Visual example of how \myalgonamesecond relies effectively on the teacher, passing control to  and saving the simple student (\myalgonamefirst) from the drift.}
\label{fig:trastdecision}
\end{minipage}\hfill
\begin{minipage}[t]{.49\textwidth}
\centering
\includegraphics[width=.9\columnwidth]{images/vot2019ar.jpg}
\caption{Analysis of the accuracy (A) and robustness (R) on VOT2019 over different classes of tracking sequences. 
}
\label{fig:vot2019ar}
\end{minipage}
\end{figure}

\subsection{Results}
In the following sections, when not specified, the three tracker setups regard the student trained using  and , paired with  in \myalgonamesecond, and managing  in \myalgonamethird.

\subsubsection{General Remarks.}
In Table \ref{tab:ablation} the performance of \myalgonamefirst, \myalgonamesecond, \myalgonamethird are reported, while the performances of the teachers are presented in the first six rows of Table \ref{tab:sota}. \myalgonamefirst results in a very fast method with good accuracy. Combining KD and RL results in the best performance, outperforming the baselines that use for training just the ground-truth (\myalgonamefirst-GT), KD and ground-truth (\myalgonamefirst-KD-GT), and just KD (\myalgonamefirst-KD). We did not report the performance of  trained only by RL because convergence was not attained due to the large state and action spaces. 
Benefiting the teacher during tracking is an effective online procedure. Indeed, \myalgonamesecond improves \myalgonamefirst by 24\% on average, and a qualitative example of the ability to pass control to the teacher is given in Figure \ref{fig:trastdecision}.
The performance of \myalgonamethird confirms the student's evaluation ability. This is the most accurate and robust tracker thanks to the effective fusion of the underlined trackers. 
Overall, all three trackers show balanced performance across the benchmarks, thus demonstrating good generalization.
No use of the curriculum learning strategy (\myalgonamefirst-no-curr,  \myalgonamesecond-no-curr, \myalgonamethird-no-curr) slightly decreases the performance of all. 
In Figure \ref{fig:vot2019ar} the performance of the trackers is reported for different classes of sequences of VOT2019, while in Figure \ref{fig:qualitativeex} some qualitative examples are presented. \footnote{For more, please see \url{https://youtu.be/uKtQgPk3nCU}}
These results demonstrate the effectiveness of our methodology and that the proposed student model respects, respectively, the goals (i), (ii), (iii) introduced in Section \ref{sec:intro}.

\begin{table*}[t]
	\fontsize{5}{5.5}\selectfont
	\centering
	\caption{Performance of the proposed trackers while considering different teacher setups for training and tracking. Best results per tracker are highlighted in red, second-best in blue.}
	\label{tab:perteacher}
\setlength\tabcolsep{.08cm}
\begin{tabular}{ c | l | l | c c c | c c | c c | c c | c}
		\toprule
& Training & Tracking & \multicolumn{3}{c|}{GOT-10k} & \multicolumn{2}{c|}{UAV123} & \multicolumn{2}{c|}{LaSOT} &  \multicolumn{2}{c|}{OTB-100} & \multirow{2}{*}{FPS} \\
& Teachers & Teachers & AO & SR & SR & SS & PS  & SS  & PS & SS & PS  &  \\
		\midrule
		
		\scalebox{.75}{\parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\myalgonamefirst}}}} &  & - & 0.371 & 0.418 & 0.178 & 0.464 & 0.598 & 0.321 & 0.241 & 0.390 & 0.524 & \multirow{4}{*}{90} \\
		&  	& - & 0.414 & 0.473 & 0.214 & 0.462 & 0.606 & 0.336 & 0.262 & 0.390 & 0.545 & \\
		& 	& - & 0.422 & 0.484 & 0.232 & 0.507 & \tblsecondbest{0.652} & 0.357 & 0.286 & 0.422 & 0.567 & \\
		& 	& - & \tblsecondbest{0.441} & \tblsecondbest{0.499} & \tblsecondbest{0.290} & \tblbest{0.517} & 0.646 & \tblsecondbest{0.377} & \tblsecondbest{0.310} & \tblsecondbest{0.447} & \tblsecondbest{0.599} & \\
		& 		& - & \tblbest{0.484} & \tblbest{0.556} & \tblbest{0.326} & \tblsecondbest{0.515} & \tblbest{0.655} & \tblbest{0.386} & \tblbest{0.330} & \tblbest{0.481} & \tblbest{0.644} & \\
		
	    \midrule
		
		\scalebox{.75}{\parbox[t]{2mm}{\multirow{9}{*}{\rotatebox[origin=c]{90}{\myalgonamesecond}}}} &   &  		& 0.390 & 0.440 & 0.191 & 0.526 & 0.682 & 0.388 & 0.319 & 0.495 & 0.660 & 90 \\
		&   &    & 0.452 & 0.521	& 0.223 & 0.572	& 0.776 & 0.433 & 0.386 & 0.569	& 0.793 & 5 \\
		&  &   & 0.491	& 0.571	& 0.249	& 0.580	& 0.768	& 0.442	& 0.397	& 0.583	& 0.786 & 15 \\
		&  	&   & 0.532 &	0.632 & 0.354 & 0.605 & 0.779 & 0.485 & 0.457 & 0.601 & 0.806 & 40 \\
&   & 		& 0.469 & 0.541	& 0.297	& 0.562	& 0.727	& 0.422 & 0.376 & 0.560	& 0.760 & 90 \\
		&  & 		& 0.494	& 0.573	& 0.302	& 0.604	& 0.798	& 0.466 & 0.431 & 0.596 &	0.815 & 5 \\
		&  & 		& 0.521	& 0.607	& 0.307	& 0.606	& 0.795	& 0.456 & 0.419 & 0.608 & 0.822 & 15 \\
		&  & 	&   0.531	& 0.626	& 0.345	& 0.603	& 0.773	& 0.490 & 0.470 & 0.604 & 0.818 & 40 \\
&   & 		& \tblsecondbest{0.557} & \tblsecondbest{0.640}	& \tblsecondbest{0.393}	& \tblsecondbest{0.634}	& \tblsecondbest{0.823}	& \tblsecondbest{0.513} & \tblsecondbest{0.488} & \tblsecondbest{0.623}	& \tblsecondbest{0.838} & 20 \\
		&   & 		& \tblbest{0.604} & \tblbest{0.708}	& \tblbest{0.469}	& \tblbest{0.647}	& \tblbest{0.837}	& \tblbest{0.545} & \tblbest{0.524} & \tblbest{0.643}	& \tblbest{0.865} & 25 \\
		
		\midrule
		
		\scalebox{.75}{\parbox[t]{2mm}{\multirow{1}{*}{\rotatebox[origin=c]{90}{\myalgonamethird}}}} &  &  & 0.317	& 0.319 & 0.105	& 0.493 & 0.720	& 0.396 & 0.372	& 0.666 & \tblsecondbest{0.901} & 5 \\
		&  &  & 0.384	& 0.398 & 0.131	& 0.563 & 0.791	& 0.422 & 0.392	& \tblbest{0.701} & \tblbest{0.931} & 5 \\
		&  &  & \tblsecondbest{0.526}	& \tblsecondbest{0.624} & \tblsecondbest{0.305}	& \tblsecondbest{0.634} & 0.815	& 0.507 & 0.500	& 0.670 & 0.877 & 15 \\
		&  &  & \tblbest{0.617} & \tblbest{0.729}	& \tblbest{0.490}	& \tblbest{0.679} & \tblbest{0.873}	& \tblbest{0.576}	& \tblbest{0.574}	& \tblsecondbest{0.692} &	0.895 & 20 \\
		&  &  & 0.517	& 0.615 & 0.294	& 0.633 & \tblsecondbest{0.823}	& \tblsecondbest{0.513} & 0.504	& 0.682 & 0.897 & 5 \\
		 &  &  & 0.519 & 0.616	& 0.287	& 0.628 & \tblsecondbest{0.823}	& 0.510	& \tblsecondbest{0.505}	& 0.660 & 0.890 & 5 \\
		\bottomrule		
\end{tabular}
\end{table*} 
\subsubsection{Impact Of Teachers.}
In Table \ref{tab:perteacher} the performance of the proposed trackers in different student-teacher setups is reported.
The general trend of the three trackers reflects the increasing tracking capabilities of the teachers. Indeed, on every considered benchmark, the tracking ability of the student increases as a stronger teacher is employed.
For \myalgonamesecond, this is also proven by Figure \ref{fig:decisionsline} (a), where we show that better teachers are exploited more.
Moreover, using more than one teacher during training leads to superior tracking policies and to  better exploitation of them during tracking.
Although in general student models cannot outperform their teachers due to their simple and compressed architecture \cite{Cho2019}, \myalgonamefirst and \myalgonamesecond show such behavior on benchmarks where teachers are weak. 
Using two teachers during tracking is the best \myalgonamethird configuration, as in this setup it outperforms the best teacher by more than 2\% on all the considered benchmarks. 
When weaker teachers are added to the pool, the performance tends to decrease, suggesting a behavior similar to the one pointed out in \cite{Bailer2014}.
Part of the error committed by \myalgonamesecond and \myalgonamethird on benchmarks like OTB-100 and VOT2019 is explained by Figure \ref{fig:qualerror}. In situations of ambiguous ground-truth, such trackers make predictions that are qualitatively better but quantitatively worse.
\myalgonamesecond and \myalgonamethird show to be unbiased to the training teachers, as their capabilities generalize also to  and  which are not exploited during training.
\begin{table*}[t]
\fontsize{5}{5.5}\selectfont
\centering
	\caption{Results of the proposed trackers considering 's increasingly better predictions. Best values, per tracker, are highlighted in red, second-best in blue.}
	\label{tab:demofilt}
\setlength\tabcolsep{.08cm}
\begin{tabular}{l | l | c c c | c c | c c | c c }
		\toprule
		& & \multicolumn{3}{c|}{GOT-10k} & \multicolumn{2}{c|}{UAV123} & \multicolumn{2}{c|}{LaSOT} &  \multicolumn{2}{c}{OTB-100}  \\
		& \multirow{-2}{*}{Tracker} 	& AO & SR & SR & SS & PS  & SS  & PS & SS & PS \\
		\midrule
		
		 & \myalgonamefirst	& \tblbest{0.484} & \tblbest{0.556} & \tblbest{0.326} & \tblbest{0.515} & \tblbest{0.655} & \tblbest{0.386} & \tblbest{0.330} & \tblbest{0.481} & \tblbest{0.644} \\
		 & \myalgonamesecond	& \tblbest{0.532} & \tblbest{0.632} & \tblbest{0.354}	&  \tblbest{0.605} & \tblbest{0.779} & \tblbest{0.485} & \tblbest{0.457} & \tblsecondbest{0.601} & \tblsecondbest{0.806} \\
		 \multirow{-3}{*}{} & \myalgonamethird	& \tblbest{0.519}	& \tblbest{0.616} & 0.287 & 0.628 & 0.823 & 0.510 & \tblsecondbest{0.505} & 0.660 & 0.890 \\
		
		\midrule
		
		 & \myalgonamefirst	& \tblsecondbest{0.426} & \tblsecondbest{0.488} & \tblsecondbest{0.244} & \tblsecondbest{0.481} & \tblsecondbest{0.609} & \tblsecondbest{0.343} & \tblsecondbest{0.277} & \tblsecondbest{0.452} & \tblsecondbest{0.617} \\
		& \myalgonamesecond	& \tblsecondbest{0.518} & \tblsecondbest{0.616} & \tblsecondbest{0.326}	& \tblsecondbest{0.599} & \tblsecondbest{0.768} & 0.475 & 0.452 & \tblbest{0.608}	& \tblbest{0.809} \\
		\multirow{-3}{*}{} & \myalgonamethird	& \tblsecondbest{0.507}	& \tblsecondbest{0.599}	& \tblbest{0.295}	& \tblbest{0.639} & \tblbest{0.827}	& \tblbest{0.514}	& \tblbest{0.510}	& \tblbest{0.683} & \tblbest{0.901} \\
		
		\midrule
		
		 & \myalgonamefirst	& 0.404 & 0.449 & 0.231 & 0.430 & 0.552 & 0.334 & 0.260 & 0.390 & 0.522 \\
		& \myalgonamesecond	& 0.513 & 0.603 & 0.310	& 0.594 & 0.766 & \tblsecondbest{0.478} & \tblsecondbest{0.456} & 0.586 & 0.781 \\
		\multirow{-3}{*}{} & \myalgonamethird	& \tblsecondbest{0.507}	& \tblsecondbest{0.599}	& \tblsecondbest{0.289}	& \tblsecondbest{0.638} & \tblbest{0.827} & \tblsecondbest{0.513} & \tblsecondbest{0.505} & \tblsecondbest{0.675} & \tblsecondbest{0.894} \\
		
		\midrule
		
		 & \myalgonamefirst	& 0.326 & 0.344 & 0.155 & 0.387 & 0.489 & 0.243 & 0.170 & 0.323 & 0.414 \\
		& \myalgonamesecond	& 0.505 & 0.598 & 0.297	& 0.592 & 0.764 & 0.457 & 0.426 & 0.589 & 0.774 \\
		\multirow{-3}{*}{} & \myalgonamethird	& 0.494	& 0.575	& 0.260 & 0.624 & 0.815 & 0.494 & 0.482	& 0.672 & 0.888 \\
		
		\midrule
		
		 & \myalgonamefirst	& 0.140 & 0.070 & 0.014 & 0.064 & 0.045 & 0.086 & 0.019 & 0.132 & 0.104 \\
		& \myalgonamesecond	& 0.471 & 0.541 & 0.250	& 0.547 & 0.697 & 0.445 & 0.409 & 0.574	& 0.746 \\
		\multirow{-3}{*}{} & \myalgonamethird	& 0.403	& 0.425	& 0.169 & 0.534 & 0.743 & 0.401 & 0.374 & 0.626 & 0.836 \\
		
		\bottomrule		
\end{tabular}
\end{table*} In Table \ref{tab:demofilt} we present the performance of the proposed trackers while considering different quality of teacher actions. Increasing the quality, thus reducing the number of videos, results in decreasing the performance of all three trackers. The loss is not significant between  and , while considering more precise actions, \myalgonamefirst suffers majorly, suggesting that more data is a key factor for an autonomous tracking policy. Interestingly, \myalgonamesecond and \myalgonamethird are able to perform tracking even if the student is trained with limited training samples. The plot (b) in Figure \ref{fig:decisionsline} confirms that the student relies effectively to its teacher, as the latter's output is selected more often as  loses performance.

Running the student takes just 11ms on our machine. \myalgonamefirst performs at 90 FPS. The speed of \myalgonamesecond and \myalgonamethird depends on the chosen teacher and varies between 5 and 40 FPS, as shown in Table \ref{tab:perteacher}. In parallel setups, \myalgonamesecond and \myalgonamethird run in real-time if the teachers do so. 

\begin{figure}[t]\begin{minipage}[t]{.49\textwidth}
\centering
\includegraphics[width=\columnwidth]{images/decisions-line.jpg}
\caption{Per benchmark fractions of predictions attributed to  in the \myalgonamesecond setup.}
\label{fig:decisionsline}
\includegraphics[width=\columnwidth]{images/qualitative.jpg}
\caption{Qualitative examples of the proposed trackers.}
\label{fig:qualitativeex}
\end{minipage}\hfill
\begin{minipage}[t]{.49\textwidth}
\centering
\includegraphics[width=.7\columnwidth]{images/qualerror.jpg}
\caption{Behaviour of \myalgonamesecond and \myalgonamethird with ambiguous ground-truths. In the presented frames, \myalgonamesecond selects the bounding box predicted by the student, while \myalgonamethird to one given by . Those outputs are qualitative better but have much less  (quantified by the colored numbers) with respect to . This impacts the overall quantitative performance. }
\label{fig:qualerror}
\end{minipage}
\end{figure}


\subsubsection{State of the Art Comparison.}
In Table \ref{tab:sota} we report the results of the proposed trackers against the state-of-the-art. In the following comparisons, we consider the results of the best configurations proposed in the above analysis.

\myalgonamefirst outperforms GOTURN and RE3 which employ a similar DNN architecture but different learning strategies. On GOT-10k and LaSOT it also surpasses the recent GradNet and ROAM, and GCT on UAV123.
\myalgonamesecond outperforms ATOM and SiamCAR on GOT-10k, UAV123, LaSOT, while losing little performance to DiMP. The performance is better than RL-based trackers \cite{Yun2017,Chen2018,Ren2018} on UAV123 and comparable on OTB-100. 
Finally, \myalgonamethird outperforms all the trackers on all the benchmarks (where the pool  was used). Remarkable results are obtained on UAV123 and OTB-100, with SS of 0.679 and 0.701 and PS of 0.873 and 0.931, respectively. Large improvement is achieved over all the methodologies that include expert trackers in their methodology. 


\begin{table*}[t]
\fontsize{5}{6}\selectfont
\centering
	\caption{Performance of the proposed trackers (in the last block of rows) in comparison with the the state-of-the-art. First block of rows reports the performance of the selected teachers; second block shows generic-approach tracker performance; third presents trackers that exploit experts or perform fusion. Best results are highlighted in red, second-best in blue.}
	\label{tab:sota}
\setlength\tabcolsep{.08cm}
	\resizebox{\textwidth}{!}{
	\begin{tabular}{l | c c c |  c c |  c c | c c | c c c | c}
		\toprule
		
					& \multicolumn{3}{c|}{GOT-10k} & \multicolumn{2}{c|}{UAV123} & \multicolumn{2}{c|}{LaSOT} &  \multicolumn{2}{c|}{OTB-100} &  \multicolumn{3}{c|}{VOT2019} & \\
		\multirow{-2}{*}{Tracker} 	& AO & SR & SR & SS & PS  & SS  & PS & SS & PS & EAO & A & R & \multirow{-2}{*}{FPS} \\
		\midrule
		KCF \cite{KCF} & 0.203 & 0.177 & 0.065 & 0.331 & 0.503 & 0.178 & 0.166 & 0.477 &	0.693 & 0.110 & 0.441 & 1.279 & 105 \\
		MDNet \cite{MDNet} & 0.299& 0.303 & 0.099 & 0.489 & 0.718 & 0.397 & 0.373 & 0.673 & 0.909 & 0.151 & 0.507 & 0.782 & 5 \\
		ECO \cite{ECO} & 0.316 & 0.309	& 0.111	& 0.532	& 0.726	& 0.324	& 0.301	& 0.668	& 0.896 & 0.262 & 0.505 & 0.441 & 15 \\
		SiamRPN	\cite{SiamRPN} & 0.508	& 0.604	& 0.308	& 0.616	& 0.785	& 0.508	& 0.492	& 0.649 & 0.851 & 0.259 & 0.554 & 0.572 & 43 \\
		ATOM	\cite{ATOM} & 0.556	& 0.634	& 0.402	& 0.643	& 0.832 & 0.516	& 0.506	& 0.660 & 0.867 & \tblsecondbest{0.292} & \tblbest{0.603} & \tblsecondbest{0.411} & 20 \\
		DiMP	\cite{DiMP} & \tblsecondbest{0.611} & \tblsecondbest{0.717} & \tblbest{0.492}	& \tblsecondbest{0.653}	& \tblsecondbest{0.839}	& \tblsecondbest{0.570}	& \tblsecondbest{0.569} & 0.681 & 0.888 & \tblbest{0.379} & 0.594 & \tblbest{0.278} & 25 \\
		\midrule
		GOTURN	\cite{GOTURN} & 0.347	& 0.375	& 0.124	& 0.389	& 0.548	& 0.214	& 0.175	& 0.395 & 0.534 & - & - & - & 100 \\
		RE3	\cite{RE3} & -	& -	& -	& 0.514	& 0.667	& 0.325	& 0.301	& 0.464 & 0.582 & 0.152 & 0.458 & 0.940 & 150 \\
		ADNet	\cite{Yun2017} & -	& -	& -	& -	& -	& -	& -	& 0.646 & 0.880 & - & - & - & 3 \\
		ACT	\cite{Chen2018} & -	& -	& -	& 0.415	& 0.636	& -	& -	& 0.625 & 0.859 & - & - & - & 30 \\
		DRL-IS	\cite{Ren2018} & -	& -	& -	& -	& -	& -	& -	& 0.671 & 0.909 & - & - & - & 10 \\
		SiamRPN++	\cite{SiamRPNpp} & -	& -	& -	& 0.613	& 0.807	& 0.496	& -	& \tblsecondbest{0.696} & \tblsecondbest{0.914} & 0.285 & \tblsecondbest{0.599} & 0.482 & 35 \\
		GCT	\cite{GCT} & -	& -	& -	& 0.508	& 0.732	& -	& -	& 0.648 & 0.854 & - & - & - & 50 \\
		GradNet	\cite{GradNet} & -	& -	& -	& -	& -	& 0.365	& 0.351	& 0.639 & 0.861 & - & - & - & 80 \\
		SiamCAR	\cite{SiamCAR} & 0.569	& 0.670	& 0.415	& 0.614	& 0.760	& 0.507	& 0.510	& - & - & - & - & - & 52 \\
		ROAM	\cite{ROAM} & 0.436	& 0.466	& 0.164	& -	& -	& 0.368	& 0.390	& 0.681 & 0.908 & - & - & - & 13 \\
		\midrule
		MEEM	\cite{MEEM} & 0.253	& 0.235	& 0.068	& 0.392	& 0.627	& 0.280	& 0.224	& 0.566 & 0.830 & - & - & - & 10 \\
		HMMTxD	\cite{HMMTxD} & -	& -	& -	& -	& -	& -	& -	& - & - & 0.163 & 0.499 & 1.073 & - \\
		HDT	\cite{Qi2016} & -	& -	& -	& -	& -	& -	& -	& 0.562 & 0.844 & - & - & - & 10 \\
		Zhu et al. \cite{Zhu2018} & -	& -	& -	& -	& -	& -	& -	& 0.587 & 0.788 & - & - & - & 36 \\
		Li et al.	\cite{Li2019} & -	& -	& -	& -	& -	& -	& -	& 0.621 & 0.864 & - & - & - & 6 \\
		\midrule
		\myalgonamefirst			 & 0.484 & 0.556 & 0.326 & 0.515 & 0.655 & 0.386 & 0.330 & 0.481 & 0.644 & 0.131 & 0.400 & 1,020 & 90 \\
		\myalgonamesecond			 & 0.604 & 0.708 & 0.469	& 0.647 & 0.837 & 0.545 & 0.524 & 0.643	& 0.865 & 0.203 & 0.517 & 0.693  & 25 \\
		\myalgonamethird		& \tblbest{0.617}	& \tblbest{0.729} & \tblsecondbest{0.490}	& \tblbest{0.679} & \tblbest{0.873}	& \tblbest{0.576}	& \tblbest{0.574}	& \tblbest{0.701} & \tblbest{0.931} & 0.266 & 0.592 & 0.597 & 20 \\
		\bottomrule		
\end{tabular}
}
\end{table*} 

 
\section{Conclusions}
In this paper, a novel methodology for visual tracking is proposed. KD and RL are joined in a novel framework where off-the-shelf tracking algorithms are employed to compress knowledge into a CNN-based student model. After learning, the student can be exploited in three different tracking setups, \myalgonamefirst, \myalgonamesecond and \myalgonamethird, depending on application needs. An extensive validation shows that the proposed trackers \myalgonamefirst and \myalgonamesecond compete with the state-of-the-art, while \myalgonamethird outperforms recently published methods and fusion approaches. All trackers can run in real-time. 
\subsubsection{Acknowledgements.} This work is supported by the ACHIEVE-ITN project.

\bibliographystyle{splncs}
\bibliography{egbib}

\clearpage
\appendix

\begin{center}
Supplementary Material of \\
\Large
\textbf{Tracking-by-Trackers \\
with a Distilled and Reinforced Model}
\end{center}



\section{Methodology}

\subsection{MDP Auxiliary Functions}
\label{sec:mdpaux}

The function  used to obtain the bounding box  given  and the previous bounding box  is defined such that
.
The function  employed to obtain the expert action  given the teacher bounding boxes  is defined as
.

\subsection{Curriculum Learning Strategy}
\label{sec:curriculum}
A curriculum learning strategy \cite{Bengio2009} is designed to further facilitate and improve the student's learning. 
After terminating each , a success counter  for  is increased if  performs better than  in that interaction, i.e. if the first cumulative reward, received up to , is greater or equal to the one obtained by the second. In formal terms,  is updated if the following condition holds

The counter update is done by testing students that interact with  by exploiting . 
The terminal video index  is successively increased during the training procedure by a central process which checks if
. After each update of ,  is reset to zero. 
By this setup, we ensure that, at every increase of , students face a simpler learning problem where they are likely to succeed and in a shorter time, since they have already developed a tracking policy that, up to , is at least good as the one of . We found  to work well in practice.




\section{Experimental Setup}

\subsubsection{Benchmarks and Performance Measures.}
\label{sec:benchmarks}
In this subsection we offer more details about the benchmark datasets and the relative performance measures employed to validate our methodology.

\paragraph{GOT-10k Test Set.}
The GOT-10k \cite{GOT10k} test set is composed of 180 videos. Target objects belong to 84 different classes, and 32 forms of object motion are present. An interesting note is that, except for the class \emph{person}, there is no overlap between the object classes in the training and test splits. For the \emph{person} class, there is no overlap in the type of motion. 
The evaluation protocol proposed by the authors is the one-pass evaluation (OPE) \cite{OTB}, while the metrics used are the average overlap (AO) and the success rates (SR) with overlap thresholds  and . 

\paragraph{OTB-100.}
\label{sec:otb100res}
The OTB-100 \cite{OTB} benchmark is a set of 100 challenging videos and it is widely used in the tracking literature. The standard evaluation procedure for this dataset is the OPE method while the Area Under the Curve (AUC) of the success and precision plot, referred as success score (SS) and precision scores (PS) respectively, are utilized to quantify trackers' performance. 

\paragraph{UAV123.}
The UAV123  benchmark \cite{UAV123} proposes 123 videos that are inherently different from traditional visual tracking benchmarks like OTB and VOT, since it offers sequences acquired form low-altitude UAVs. 
To evaluate trackers, the standard OTB methodology \cite{OTB} is exploited.  
 
\paragraph{LaSOT.}
A performance evaluation was also performed on the test set of LaSOT benchmark \cite{LaSOT}. This dataset is composed of 280 videos, with a total of more than 650k frames and an average sequence length of 2500 frames, that is higher than the lengths of the videos contained in the aforementioned benchmarks. The same methodology and metrics used for the OTB \cite{OTB} experiments are employed.
 
\paragraph{VOT2019.}
The VOT benchmarks are datasets used in the annual VOT tracking competitions. These sets change year by year, introducing increasingly challenging tracking scenarios. 
We evaluated our trackers on the set of the VOT2019 challenge \cite{VOT2019}, which provides 60 highly challenging videos.
Within the framework used by the VOT committee, trackers are evaluated based on Expected Average Overlap (EAO), Accuracy (A) and Robustness (R)~\cite{VOT}. Differently from the OPE, the VOT evaluation protocol presents the automatic re-initialization of the tracker when the IoU between its estimated bounding box and the ground-truth becomes zero.



\section{Additional Results}

\subsection{Impact of Transfer Set}
We evaluated how performance change considering other sources of video data. By respecting the idea that unbiased demonstrations of the teachers should be employed, we used the training set of the LaSOT benchmark \cite{LaSOT}. This dataset is smaller than the training set of GOT-10k and contains 1120 videos with approximately 2.83M frames. After filtering the trajectories, we obtained the transfer set  which specification are given in Table \ref{tab:demostatslasot}.


\begin{table*}[t]
\fontsize{5}{6}\selectfont
\centering
	\caption{Teacher-based statistics of the LaSOT transfer set.}
	\label{tab:demostatslasot}
\setlength\tabcolsep{.05cm}
\begin{tabular}{l | c c c }
		\toprule
Teachers & \multicolumn{3}{c}{} \\
& \# traj & AO &  \\
		\midrule
		 & 16 & 0.835 & 80 \\
		 & 32 & 0.830 & 160 \\
		 & 44 & 0.817 & 220 \\
		 & 87 & 0.852 & 435 \\
		 & 106 & 0.856	& 530 \\
		\bottomrule		
\end{tabular}
\end{table*} 
The results are shown in Table \ref{tab:demofiltlasot}. The amount of training samples is lower than the amount obtained by filtering the GOT-10k transfer set with , and the proposed trackers present a behaviour that reflects the loss of data (as seen in Table \ref{tab:demofilt}). This experiment suggests that the quantity of data has more impact than the quality of data.
\begin{table*}[t]
\fontsize{5}{6}\selectfont
\centering
	\caption{Performance of the proposed trackers considering the training set of LaSOT as transfer set.}
	\label{tab:demofiltlasot}
\setlength\tabcolsep{.08cm}
\begin{tabular}{l | l | c c c | c c | c c | c c }
		\toprule
		& & \multicolumn{3}{c|}{GOT-10k} & \multicolumn{2}{c|}{UAV123} & \multicolumn{2}{c|}{LaSOT} &  \multicolumn{2}{c}{OTB-100}  \\
		
		& \multirow{-2}{*}{Tracker} 	& AO & SR & SR & SS & PS  & SS  & PS & SS & PS \\
		\midrule
		
		 & \myalgonamefirst	& 0.242 & 0.252 & 0.086 & 0.329 & 0.437 & 0.222 & 0.166 & 0.254 & 0.337 \\
		 & \myalgonamesecond	& 0.475 & 0.552 & 0.248	&  0.553 & 0.746 & 0.463 & 0.432 & 0.577 & 0.760 \\
		 \multirow{-3}{*}{} & \myalgonamethird	& 0.468	& 0.529 & 0.221 & 0.594 & 0.803 & 0.470 & 0.452 & 0.666 & 0.885 \\
		
		\bottomrule		
\end{tabular}
\end{table*} 

\subsection{Success and Precision Plots on OTB-100}
In Figures \ref{fig:otb2015succ} and \ref{fig:otb2015prec} the success plots and precision plots for different sequence categories of the OTB-100 benchmark are presented. 

\begin{figure}[!htbp]
\begin{center}
\includegraphics[width=.9\columnwidth]{images/otb2015succ.jpg}
\end{center}
   \caption{Success plots on OTB-100 presenting the performance of the proposed trackers and the teachers on tracking situations with: occlusion (OCC); background clutter (BC); out of view (OV); motion blur (MB); low resolution (LR); fast motion (FM).}
\label{fig:otb2015succ}
\end{figure}

\begin{figure}[!htbp]
\begin{center}
\includegraphics[width=.9\columnwidth]{images/otb2015prec.jpg}
\end{center}
   \caption{Precision plots on OTB-100 presenting the performance of the proposed trackers and the teachers on tracking situations with: occlusion (OCC); background clutter (BC); out of view (OV); motion blur (MB); low resolution (LR); fast motion (FM).}
\label{fig:otb2015prec}
\end{figure}


\subsection{Video}
At this link \url{https://youtu.be/uKtQgPk3nCU}, we provide a video showing the tracking abilities of our proposed trackers. For each video, the predictions of \myalgonamefirst, \myalgonamesecond and \myalgonamethird are shown. For \myalgonamesecond and \myalgonamethird, we report also the tracker which prediction was chosen as output proposes (with the term "CONTROLLING" next to the tracker's name). 
\end{document}
