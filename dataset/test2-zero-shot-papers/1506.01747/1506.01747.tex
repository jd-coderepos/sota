\documentclass{llncs}
\usepackage{llncsdoc}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{graphicx}


\bibliographystyle{plain}
\begin{document}


\title{Weighted Sampling Without Replacement from Data Streams}

\author{Vladimir Braverman\thanks{Johns Hopkins University, Department of Computer Science. Email: vova@cs.jhu.edu. This material is based upon work supported in part by the National Science Foundation under Grant No. 1447639, the Google Faculty Award and DARPA grant N660001-1-2-4014. Its contents are solely the responsibility of the authors and do not represent the official view of DARPA or the Department of Defense.} Rafail Ostrovsky\thanks{University of California Los Angeles, Department of Computer Science and Department
of Mathematics, Email: rafail@cs.ucla.edu. } Gregory Vorsanger\thanks{Johns Hopkins University, Department of Computer Science. gregvorsanger@jhu.edu. This material is based upon work supported in part by Raytheon BBN Technologies.}}

\institute{}

\maketitle






\begin{abstract}

Weighted sampling without replacement has proved to be a very important tool in designing new algorithms.
Efraimidis and Spirakis (IPL 2006) presented an algorithm for weighted sampling
without replacement from data streams.
Their algorithm works under the assumption of precise computations over the interval .
Cohen and Kaplan (VLDB 2008) used similar methods for their bottom-k
sketches.

Efraimidis and Spirakis ask as an open question whether using finite precision arithmetic impacts the accuracy of their algorithm. In this paper we show a method to avoid this problem by providing a precise reduction from k-sampling without replacement to k-sampling with replacement. We call the resulting method Cascade Sampling.
\end{abstract}

\section{Introduction}


\emph{Random sampling} is a fundamental tool that has many applications in
computer science (see e.g., Motwani and Raghavan \cite{randomized}, Knuth \cite{knuth}, Tille \cite{tille}, and Olken \cite{olken}). Random sampling methods are widely used is data stream processing
because of their simplicity and efficiency \cite{dlt,stream_operator,mg,dynamic,reservoir2,reservoir1}. In a stream, the size of the domain and the probability of sampling an element both change constantly; this makes the process of sampling non-trivial. We distinguish between
sampling \emph{with replacement}, where all samples are independent
(and thus can be repeated), and sampling
\emph{without replacement}, where repetitions are prohibited.

In particular, weighted sampling without replacement has proven to be a very important tool. In weighted sampling,  each element is given a weight, where the probability of an element being selected is based on its weight. 
In their work Efraimidis and Spirakis \cite{weighted_power} presented an algorithm for weighted sampling
without replacement. Cohen and Kaplan \cite{Cohen:2008:TEU:1453856.1453884} use similar methods for their bottom-k
sketches. While their preliminary implementation yielded promising results, Efraimidis and Spirakis \cite{weighted_power} state, as the main open problem of the paper, \emph{``However, the question if, and to what extent, the finite precision arithmetic affects the algorithms remains an open problem.''}

In this paper we continue this work and provide a new algorithm to avoid the issue of relying on finite precision arithmetic. With this result we show that precision loss is not required in order to sample without replacement. We accomplish this by providing a precise reduction from -sampling without replacement to -sampling with replacement, using a special case of -sampling with replacement, unit sampling (where =).  Additionally, we believe that in the future our method of expressing different random samples via reduction will provide a tool that allows further translation of other sampling methods into a more effective form for streams.




\subsection{Related Work}
Due to its fundamental nature, the problem of random sampling has received considerable attention
in the last few decades.

In 2005, Vitter \cite{reservoir} presented uniform sampling using a reservoir (with
and without replacement) over streams.
Further, the question of reductions between sampling methods has been addressed before.
For instance, Chaudhuri, Motwani and Narasayya \cite{joins} briefly discuss reductions for various sampling methods.
Cohen and Kaplan \cite{Cohen:2008:TEU:1453856.1453884} use a ``mimicking process'' in their papers, which is essentially a reduction
from sampling without replacement to sampling with replacement.

Chaudhuri, Motwani and Narasayya \cite{joins} use
the well-known method of ``over-sampling'', i.e. we sample the set independently
until  distinct elements are obtained. Clearly, this schema does not introduce any precision loss,
since unit sampling is used as a black-box.

Unfortunately, the amount of resources required to determine this information is a function of  the weight distribution for the data set, and thus can be arbitrarily large.

In particular, consider the case when there is an element with weight that is overwhelmingly larger than
the rest of the population. In this case, the number of repetitions found while sampling with replacement is significantly larger then .


Probably the first effective non-streaming solution for the weighted sampling without replacement problem was the algorithm
of Wong and Easton \cite{weighted_non_stream}. It is used by many other algorithms (see Olken \cite{olken} for the discussion).
For data streams, Efraimidis and Spirakis \cite{weighted_power} proposed an algorithm that is based on the ``exponent method''.
The algorithm requires precise computations of random keys , where .
The sample generated is composed of the  elements with maximal keys. Cohen and Kaplan \cite{Cohen:2008:TEU:1453856.1453884} used similar methods as a building block for their bottom-k
sketches. The bottom-k sketch is an effective construction that has been extensively used for various
applications including approximations of aggregative queries over data streams. As Cohen and Kaplan \cite{Cohen:2008:TEU:1453856.1453884} show,
these methods are very effective in practical applications and are superior to the sketches that are based on sampling with replacement.

While effective in practice, the algorithms of Efraimidis and Spirakis and Cohen and Kaplan
introduce a loss of accuracy, since their techniques require additional floating point arithmetic operations.



\subsection{Results}

In this paper we show that the tradeoff between precision and performance is not a necessary property of sampling without replacement from data streams and construct a precise streaming reduction from -sampling without replacement to -sampling with replacement. This result provides a practical improvement to the algorithms of Efraimidis and Spirakis in cases where high accuracy is required. 

Our method is yields a surprisingly simple algorithm, given the importance of sampling without replacement and the existence of many previous methods. We call this algorithm Cascade Sampling.
In particular, when used with the algorithm from \cite{joins} Cascade Sampling requires  memory, constant time per element and the same precision as in \cite{joins}. 

\subsection{Intuition}

Let  be \emph{any} algorithm that maintains a unit weighted sample from stream .
Similarly to the over-sampling method, we maintain instances of . Namely, we maintain  instances . However, we introduce the idea of \emph{stream modification}. That is, instead of applying  independently and symmetrically on , we apply  on the modified stream  that does not contain samples of  for .
In particular,  may process its input elements in an order \emph{different from the order of their arrival in }.
This simple but novel idea is sufficient to solve the problem.
In particular, we can claim that the input of  is a random set that precisely matches the definition of weighted sampling without replacement. Since we use  as a black box with only a constant number of auxiliary variables, specifically pointers,
the resulting schema is a precise reduction.





\section{Definitions}

An important building block of our algorithm is the concept of a unit sample, that is, the ability to sample a single element from a set. 

\begin{definition}
Let  be a finite set of elements and let  be a non negative function .
A random element  with values from  is a \emph{\textbf{unit weighted random sample}} if, for any ,
. Here .
\end{definition}


For an algorithm instantiating weighted unit sampling we provide Black-Box WR2 from \cite{joins}. Black-Box WR2 is a unit sample when . 

\begin{algorithm}[H]\label{j}
\caption{Black-Box WR2: Algorithm for Weighted Unit Sampling}

\begin{enumerate}
\item .
\item Initialize reservoir with length , .


\item For each tuple  in stream:
\begin{enumerate}
\item Get next tuple  with weight 
\item 
\item Set  with prob. 
\end{enumerate}


\item Return 
\end{enumerate}

\end{algorithm}

\begin{definition}
A \emph{\textbf{data stream}}  is an ordered, set of
elements, , that can be observed only
once. An algorithm  is a streaming sampling algorithm if  outputs a sample using a single pass over the data set.
\end{definition}
\begin{definition}
A set  is
called a \emph{\textbf{k-sample with replacement}} from  if  are independent random unit samples from .
\end{definition}

Another fundamental sampling method is \emph{weighted sampling without replacement}.
\begin{definition}
Let  be a finite set such that .
An ordered set  is called a \emph{\textbf{-sample without replacement}} from 
if  is a weighted unit sample from  and for any ,  is a weighted unit sample from .
\end{definition}
\begin{definition}
We say that there exists an a \emph{reduction} from a -sampling to a unit sampling if for
any unit sampling algorithm 
there exists a -sampling algorithm  that uses  as a black-box.
We say that the reduction is \emph{precise} if for any  that requires memory  and time :
\begin{enumerate}
\item  requires  memory and  time.
\item  only uses comparisons (in addition to using  as a black box).
\end{enumerate}
In other words,  does not introduce any precision loss.
\end{definition}
There exists a (trivial) precise reduction from weighted sampling with replacement to unit sampling.
In this paper we give the first precise streaming reduction for weighted sampling without replacement to unit sampling.


\section{Cascade Sampling}
Let  be a finite set such that  and let . Denote , and let  be a function.
Let  be a -sample without replacement from  with respect to .
Define an ordered sequence \footnote{Here the additional randomness is independent.} as follows:

For  define\footnote{Here  denotes the set difference, i.e. .}:

We will show that ; assuming that, let  be the single element from , i.e., .
Put  Define

\begin{lemma}\label{sddsfsdfsdf}
For all  the ordered set  is an  -sample without replacement from  with respect to .
\end{lemma}
\begin{proof}
We prove the lemma by induction on . For  the statement follows from direct computation and definitions.
Assuming that the lemma is correct for  we need to prove that

and for any :

To show  observe that  and .
By definition   and .

To show  fix  and ; it follows that  is fixed as well.
Denote  and ; it follows that .
For any fixed  we have

The case  is similar.
\end{proof}

\section{Precise Reduction and Resulting Algorithm}\label{sddssdsdfsdf}

Let  be an algorithm that maintains a unit weighted sample from . The algorithm from \cite{joins} is an example of  but our reduction works with \emph{any} algorithm for unit weighted sampling.
We construct an algorithm  such that  maintains a -sample without replacement.
Specifically, we maintain  instances of :  such that the input of  is a random substream of  that is selected in a special way. We denote the input stream for  as .
Let  be the sample produced by .
The critical observation is that our algorithm maintains the following invariant: at any moment . Thus, by definition, the weighted sample from  is the -th weighted sample from  when the samples are without replacement.

\begin{theorem}\label{lm:strem weighted}
Algorithm  maintains a weighted -sample without replacement from .
If  requires space  and time per element , then  requires  space and  time respectfully.
Thus, there exists a precise reduction from -sampling without replacement to a unit sampling.
\end{theorem}
\begin{proof}

Follows from the description of the algorithm (See Algorithm 2) and Lemma \ref{sddsfsdfsdf}.
\end{proof}


\begin{algorithm}[H]
\caption{Cascade Sampling}

\emph{Input: Data Stream },\\
\ \ \ \ \emph{ is an algorithm that maintains a unit weighted sample from },\\
\ \ \ \ \emph{ are independent instances of }\\
\emph{Output: Weighted -Sample Without Replacement  }
\begin{enumerate}
  \item For 
  \begin{enumerate}
  \item 
  \item For 
  \begin{enumerate}
  \item If () then set   (where  the current output of ).
  \item Feed  with 
  \item If  changes its value to , then set .
  \end{enumerate}
  \end{enumerate}
  \item Output 

\end{enumerate}

\end{algorithm}

Algorithm 2 provides a solution to the weighted -Sampling without replacement problem. To better demonstrate the algorithm, we show an example of updating a single unit sample inside of loop \emph{(b)} in Figure 1. In this example, unit sample  has currently sampled element  and unit sample  has currently sampled element , where  and  are elements that appeared previously in the stream.

\begin{figure}[h]\label{pic1}
\centering
\includegraphics[scale=0.5]{cascade_fig_1}\caption{Updating a Unit Sample}
\setlength{\belowcaptionskip}{-10pt}
\clearpage
\end{figure}

\subsection{Discussion}
There are several directions in which our algorithm can be improved. In particular, run time dependent on the number of samples is one issue for practical datasets with large k. We believe this can be improved by combining several sampling steps into a single step which will be useful for the cases when the element will not be sampled into any of the substreams. This will often be the cases with elements with small weights. Specifically, we ask if it is possible to reduce the total running time from  to .

Another interesting direction is applying this algorithm to weighted random sampling with a bounded number of replacements as shown in \cite{efraimidis2010weighted}. Finally, this method may also be interesting when applied to the Sliding Window Model \cite{our} and Streams with Deletions \cite{dynamic}.

We thank our anonymous reviewers for their helpful suggestions, particularly for suggesting interesting open problems for discussion.


\newpage

\bibliography{Bibliography}


\end{document}
