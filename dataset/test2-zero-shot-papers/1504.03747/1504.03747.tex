\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{usenix,epsfig,endnotes, multirow}
\usepackage[hyphens]{url}
\usepackage{cite}
\usepackage{pbox}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{titlesec}
\usepackage{lipsum}
\usepackage{caption}
\usepackage{subcaption}

\titlespacing{\section}{1pt}{\parskip}{-\parskip}
\titlespacing{\subsection}{1pt}{\parskip}{-\parskip}
\titlespacing{\subsubsection}{1pt}{\parskip}{-\parskip}

\newenvironment{packed_enum}{
\begin{enumerate}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{enumerate}}

\newenvironment{packed_item}{
\begin{itemize}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{itemize}}

\begin{document}

\date{}

\title{Android Permissions Remystified:\\A Field Study on Contextual Integrity}



\author{
{\rm Primal Wijesekera, Arjun Baokar, Ashkan Hosseini, Serge Egelman,}\\
{\rm David Wagner, and Konstantin Beznosov}\\
University of British Columbia, Vancouver, Canada,\\
\vspace{0.5em}
\{primal,beznosov\}@ece.ubc.ca\\
University of California, Berkeley, Berkeley, USA,\\
\{arjunbaokar,ashkan\}@berkeley.edu, \{egelman,daw\}@cs.berkeley.edu\\
} 

\maketitle




\subsection*{Abstract}
Due to the amount of data that smartphone applications can potentially access, platforms enforce permission systems that allow users to regulate how applications access protected resources. If users are asked to make security decisions too frequently and in benign situations, they may become habituated and approve all future requests without regard for the consequences. If they are asked to make too few security decisions, they may become concerned that the platform is revealing too much sensitive information. To explore this tradeoff, we instrumented the Android platform to collect data regarding how often and under what circumstances smartphone applications are accessing protected resources regulated by permissions. We performed a 36-person field study to explore the notion of ``contextual integrity,'' that is, how often are applications accessing protected resources when users are not expecting it? Based on our collection of 27 million data points and exit interviews with participants, we examine the situations in which users would like the ability to deny applications access to protected resources. We found out that at least 80\% of our participants would have preferred to prevent at least one permission request, and overall, they thought that over a third of requests were invasive and desired a mechanism to block them.

\section{Introduction}


Mobile platforms enforce permission models to regulate how applications access certain resources, such as users' personal information or sensor data (e.g., camera, GPS, etc.). For instance, Android prompts the user during application installation with a list of all the abilities that that application may use in the future; if the user is uncomfortable granting any of these requests, her only option is to discontinue installing the application~\cite{AndroidPermissions2}. On iOS, the user is prompted at runtime when an application requests any one of a handful of data types for the first time, such as location, address book contacts, or photos~\cite{OGrady2014}.

Research has shown that few people read the Android permission requests and even fewer comprehend them~\cite{Felt2012}. Another problem is habituation: on average, Android applications present the user with four permission requests during the installation process~\cite{Felt2011b}. While iOS users are likely to see far fewer permission requests than Android users, because there are fewer possible permissions and they are only displayed the first time the data is actually requested, it is not clear whether or not users are being prompted about access to data that they actually find concerning, or whether they would approve of subsequent requests~\cite{Felt2012c}.

Nissenbaum posited that the reason why most privacy models fail to predict violations is that they fail to consider contextual integrity~\cite{Nissenbaum2004}. That is, privacy violations occur when personal information is used in ways that defy users' expectations. We believe that this notion of ``privacy as contextual integrity'' can be applied to smartphone permission systems to yield more effective permissions by only prompting users when an application's access to sensitive data is likely to defy expectations. As a first step down this path, we examined how applications are currently accessing this data and then examined whether or not it complied with users' expectations.

We modified Android to log whenever an application accessed a resource that was protected by application permissions and then gave these modified smartphones to 36 participants who used them as their primary phones for one week. The purpose of this was to perform dynamic analysis to determine how often various applications are actually accessing protected resources under realistic circumstances. Afterwards, subjects returned to the laboratory to return the phones and complete exit surveys. We showed them various instances over the past week where applications had accessed certain types of data and asked whether those instances were expected, and whether they would have denied access, if given the opportunity. Participants stated a desire to block a third of the requests, and that their decision processes were governed by two related underlying factors: whether they had privacy concerns surrounding the specific data type and whether they understood why the application needed it.

We contribute the following:
\begin{packed_item}
\item To the best of our knowledge, we performed the first field study to quantify the permission usage by third party applications under realistic circumstances.
\item We show that our participants wanted to block access to protected resources a third of the time. This suggests that some requests should be granted by runtime consent dialogs, rather than the current all-or-nothing install-time approval approach.
\item We model participants' decisions and show how a runtime classifier may be able to determine when to confront users with permission decisions.
\end{packed_item}

\section{Related Work}

While users are required to approve Android application permission requests prior to installation, most users do not pay attention to these requests, and fewer fully comprehend them~\cite{Felt2012,Kelley2012}. In fact, studies have shown that even developers are not fully knowledgeable about permissions~\cite{Stevens2013}, and are given a lot of freedom when posting an application to the Google Play Store~\cite{Barrera2012}. Applications often do not follow the principle of least privilege, intentionally or unintentionally~\cite{Wei2012}. Other work has made suggestions on improving the Android permission model with better definitions and hierarchical breakdowns~\cite{Barrera2010}. Some researchers have experimented with adding fine-grained access control to the Android model~\cite{Bugiel2013}. Providing users with more privacy information and personal examples has been shown to help users in choosing applications with fewer permissions~\cite{Kelley2013, Harbach2014}. 

Previous work has examined the overuse of permissions by applications~\cite{Felt2011b, Gorla2014}, and attempted to identify malicious applications through their permission requests~\cite{Sarma2012}, or through natural language processing of application descriptions~\cite{Pandita2013}. Researchers have additionally developed static analysis tools to analyze Android permission specifications~\cite{Felt2011b, Au2012, Bodden2013}. Felt et al.\ created a permission map through static analysis of many Android applications. They found that roughly one-third of their tested applications were over-privileged. Our work complements this static analysis by applying dynamic analysis to permission usage. Other dynamic analysis has been applied to native (non-Java) APIs among third-party mobile markets~\cite{Spreitzenbarth2013}, whereas we apply it to the Java APIs available to developers in the Google Play Store.

Researchers examined user privacy expectations surrounding application permissions, and found that users were often surprised by the abilities of background applications to collect data~\cite{Jung2012, Thompson2013}. Their level of concern varied from annoyance to seeking retribution when presented with possible risks associated with permissions~\cite{Felt2012c}. Some studies employed crowdsourcing to create a privacy model based on user expectations~\cite{Lin2012}.

Researchers have designed systems to track or reduce privacy violations by recommending applications based on users' security concerns~\cite{Enck2010, Hornyack2011, Zhu2014, Gibler2012, Klieber2014, Xu2012, Almohri2014, Zhang2013}. Other tools dynamically block runtime permission requests~\cite{Shebaro2014}. Enck et al.\ found that a considerable number of applications transmitted location or other user data to third parties without requiring user consent~\cite{Enck2010}. Hornyack et al.'s AppFence system gave users the ability to deny data to applications or substitute fake data~\cite{Hornyack2011}. However, this broke application functionality for one-third of the applications tested.

Reducing the number of security decisions a user must make at install-time or run-time is likely to decrease habituation, and therefore, it is critical to identify {\it which} security decisions users should be asked to make. Based on this theory, Felt et al.\ created a decision tree to aid platform designers in determining the most appropriate permission-granting mechanism for a given resource (e.g., access to benign resources should be granted automatically, whereas access to dangerous resources should require approval)~\cite{Felt2012b}. They concluded that the majority of Android permissions can be automatically granted, but 16\% (corresponding to the 12 permissions in Table \ref{tbl:perm-list}) should be granted via runtime dialogs.

Nissenbaum's theory of contextual integrity can help us to analyze ``the appropriateness of a flow" in the context of permissions granted to Android applications~\cite{Nissenbaum2004}. There is ambiguity in defining when an application actually needs access to user data to run properly. It is quite easy to see why a location-sharing application would need access to GPS data, whereas that same request coming from a game like Angry Birds is less obvious. ``Contextual integrity is preserved if information flows according to contextual norms''~\cite{Nissenbaum2004}, however, the lack of thorough documentation on the Android permission model makes it easier for programmers to neglect these norms, whether intentionally or accidentally~\cite{Shklovski2014}. Deciding on whether an application is violating users' privacy can be quite complicated since ``the scope of privacy is wide-ranging''~\cite{Nissenbaum2004}. To that end, we performed dynamic analysis to measure how often (and under what circumstances) applications were accessing protected resources, whether this complied with users' expectations, as well as how often they might be prompted if we adopt Felt et al.'s proposal to require runtime user confirmation before accessing a subset of these resources~\cite{Felt2012b}.

\iffalse Stuff on Android / Mobile permissions:
Other mobile permissions stuff to look at:
\cite{Werthmann2013} \cite{Huang2014}
\cite{Balebako2013}
\cite{Livshits2013}
\cite{Hao2014}
\cite{Rosen2013}
\cite{Wang2013b}
\cite{Sellwood2013}

Contextual privacy:
\cite{Nissenbaum2004} --- main paper on this
\cite{Barkhuus2012} --- further discussion
\cite{Barth2006} --- formalizes some of the ideas behind contextual privacy
\cite{Ronen2013} --- applying contextual privacy to single sign-on
\cite{Egelman2013c} --- because people's expectations were more cynical than reality, they were willing to use Facebook Connect
\cite{Bilogrevic2013} --- created a classifier to infer sharing preferences
\cite{Shklovski2014} --- examined people's risk perceptions/privacy expectations when using mobile apps


\fi


\section{Methodology}

Our long-term research goal is to minimize habituation by only confronting users with {\it necessary} security decisions by not showing them permission requests that are either expected, reversible, or unconcerning. In this study, we explored the problem space in two parts: we instrumented Android so that we could collect actual usage data to understand how often access to various protected resources is requested by applications in practice, and then we surveyed our participants to understand the requests that they would not have granted, if given the option. This field study involved 36 participants over the course of one week of normal smartphone usage. In this section, we describe the log data that we collected, our recruitment procedure, and then our exit survey.







\subsection{Tracking Access to Sensitive Data}

In Android, when applications attempt to access protected resources (e.g., personal information, sensor data, etc.), the operating system checks to see whether or not the requesting application has been granted permission. We modified the Android platform to add a logging framework so that we could determine every time one of these resources was accessed by an application. Because our target device was a Samsung Nexus S smartphone, we modified Android 4.1.1 (Jellybean), which was the newest version of Android supported by our hardware.

\subsubsection{Data Collection Architecture}

Our goal was to collect as much data as possible surrounding each applications' access to protected resources, while minimizing our impact on system performance. Our data collection framework consisted of two main components: a series of ``producers'' that 
hooked various Android API calls and a ``consumer'' embedded in the main Android framework service that wrote the data to a log file and uploaded it to our collection server. 



We logged three kinds of permission requests. First, we logged function calls checked by {\tt checkPermission()} in the Android {\tt Context} implementation. Instrumenting the {\tt Context} implementation, instead of the {\tt ActivityManagerService} or {\tt PackageManager}, allowed us to also log the function name invoked by the user-space application. Next, we logged access to the {\tt ContentProvider} class, which verifies the read and write permissions of an application prior to it accessing structured data (e.g., contacts or calendars)~\cite{AndroidContentProviders}. Finally, we tracked permission checks during {\tt Intent} transmission, by instrumenting the {\tt ActivityManagerService} and {\tt BroadcastQueue}. {\tt Intents} allow an application to pass messages to another application when an activity is to be performed in that other application (e.g., opening a URL in the web browser)~\cite{AndroidIntents}.

We created a component called {\tt Producer}, which fetches the data from the above instrumented points and sends it back to the {\tt Consumer}, which is responsible for logging everything reported. {\tt Producers} are scattered across the Android Platform, since permission checks occur in multiple places. We placed the {\tt Producer} that fetched the most data in {\tt system\_server}, which recorded direct function calls to Android's Java API. For a majority of privileged function calls, when a user application invokes the function, it sends the request to {\tt system\_server} via {\tt Binder}. {\tt Binder} is the most prominent IPC mechanism implemented to communicate with the Android Platform (whereas {\tt Intents} communicate between applications). For requests that do not make IPC calls to the {\tt system\_server}, a {\tt Producer} is placed in the user application context (e.g., in the case of {\tt ContentProviders}).







The {\tt Consumer} class is responsible for logging data produced by each {\tt Producer}. Apart from this, the {\tt Consumer} also stores contextual information alongside the permission details. More details on the contextual data is given in Section \ref{sec:collection}. The {\tt Consumer} syncs data with the filesystem periodically to minimize impact on system performance. All log data is written to the internal storage of the Android Phone because the Android kernel is not allowed to write to external storage for security reasons. Although this protects our data from curious or careless users, it also limits our storage capacity. Thus, we compressed the log files once every two hours and upload them to our collection servers whenever the phone had an active Internet connection (the average uploaded and zipped log file was around 108KB and contained 9,000 events).



Due to the high volume of permission checks we encountered and our goal of keeping system performance at acceptable levels, we added logic so that the {\tt Consumer} can rate-limit itself. Specifically, if it has logged permission checks for a particular application/permission combination more than 10,000 times, it examines whether it did so while exceeding an average rate of 1 permission check every 2 seconds. If so, the {\tt Consumer} will only record 10\% of all future requests for this application/permission combination. When this rate-limiting is enabled, the {\tt Consumer} tracks these application/permission combinations and updates all the {\tt Producers} so that they start dropping these log entries. Finally, the {\tt Consumer} makes a note of whenever this occurs so that we can extrapolate the true number of permission checks that occurred.
 
 







\subsubsection{Data Collection}
\label{sec:collection}

We hooked the permission-checking APIs so that every time system code was called to check whether an application had been granted a particular permission, we logged the name of the permission being checked, the name of the calling application, the API method that resulted in the permission check, and various contextual data. Recall that permission checks not only occur when certain protected API methods are called, but also when protected {\tt Intents} and {\tt ContentProviders} are accessed. Thus, our logs differentiate between these three ways of accessing protected resources.



With regard to contextual data, in addition to timestamps, we collected the following types of data:

\begin{packed_item}
\item {\bf Visibility}---Whether the requesting application was visible to the user or not, which we categorized into four sub-categories: the application was running (a) as a service with no visibility to the user; (b) as a service, but interacted with the user via notifications or sounds; (c) as a foreground process, but was in the background due to multitasking; or (d) as a foreground process with which the user was interacting.
\item {\bf Screen Status}---Whether the screen was on/off.
\item {\bf Connectivity}---Whether the phone was connected to a WiFi network.
\item {\bf Location}---The user's last known coordinates. In order to preserve battery life, we collected cached location data, rather than directly querying the GPS.
\item {\bf View}---The UI elements in the requesting application that were exposed to the user at the time that a protected resource was accessed. Specifically, since the UI is built from an XML file, we recorded the name of the screen as defined in the DOM.
\item {\bf History}---A list of applications with which the user interacted prior to the requesting application.
\item {\bf Path}---When access to a {\tt ContentProvider} object was requested, the path to the specific content (e.g., photos, contacts, etc.).
\end{packed_item}




Felt et al.\ proposed that most Android permissions should require no {\it a priori} user approval, but 12 permissions (Table \ref{tbl:perm-list}) should be granted at runtime so that users have contextual information to infer why the data might be needed~\cite{Felt2012b}. Specifically, if the user is asked to grant a permission while using an application, she may have some understanding of why the application needs that permission based on what she was doing. We initially wanted to perform experience sampling by probabilistically questioning participants whenever any of these 12 permissions were checked~\cite{Larson1983}. Our goal was to survey participants about whether access to these resources was expected and whether it should proceed, but we were concerned that this would prime them to the security focus of our experiment, biasing their subsequent behaviors. Instead, we instrumented the phones to probabilistically take screenshots of what participants were doing when these 12 permissions were checked so that we could ask them about it during the exit survey. We used reservoir sampling to minimize storage and performance impacts, while also ensuring that the screenshots covered a broad set of applications and permissions~\cite{Vitter1985}.

\begin{table}[t]
\small
\center
\begin{tabular}{|l|p{4.3cm}|}
\hline
\textbf{Permission Type} & \textbf{Activity} \\ \hline
\begin{tabular}[c]{@{}l@{}}WRITE\_SYNC\_\\ SETTINGS\end{tabular} & Change sync settings for an application when the user is roaming \\ \hline
\begin{tabular}[c]{@{}l@{}}ACCESS\_WIFI\_\\ STATE\end{tabular} & View nearby SSIDs \\ \hline
INTERNET & Access Internet when the user is roaming \\ \hline
NFC & Communicate via NFC \\ \hline
\begin{tabular}[c]{@{}l@{}}READ\_HISTORY\_\\ BOOKMARKS\end{tabular} & Read users' browser history \\ \hline
\begin{tabular}[c]{@{}l@{}}ACCESS\_FINE\_\\ LOCATION\end{tabular} & Read the GPS location \\ \hline
\begin{tabular}[c]{@{}l@{}}ACCESS\_COARSE\_\\ LOCATION\end{tabular} & Read the network-inferred location (i.e., cell tower and/or WiFi) \\ \hline
\begin{tabular}[c]{@{}l@{}}LOCATION\_\\ HARDWARE\end{tabular} & Directly access GPS data \\ \hline
READ\_CALL\_LOG & Read call history \\ \hline
ADD\_VOICEMAIL & Read call history \\ \hline
READ\_SMS & Read sent/received/draft SMS \\ \hline
SEND\_SMS & Send SMS \\ \hline
\end{tabular}
\begin{flushleft}
\caption{The 12 permissions that Felt et al.\ recommend be granted via runtime dialogs~\cite{Felt2012b}. We randomly took screenshots when these permissions were requested by applications, and we asked about them in our exit survey.}
\label{tbl:perm-list}
\end{flushleft}
\end{table}


\begin{figure}[t]
\begin{subfigure}[h]{0.5\textwidth}
\centering
\includegraphics[scale=0.2]{screenshot.png}
\caption{Screenshot}
\label{fig_scrnshot}
\end{subfigure}
\begin{subfigure}[h]{0.5\textwidth}
\centering
\small
\begin{tabular}{|l|p{5cm}|}
\hline
\textbf{Name} & \textbf{Log Data} \\ \hline
Type & API\_FUNC \\ \hline
Permission & \pbox{20cm}{ACCESS\_WIFI\_STATE} \\ \hline
App\_Name & com.spotify.music \\ \hline
Timestamp & 1412888326273 \\ \hline
API Function & getScanResults() \\ \hline
Visibility & FALSE \\ \hline
Screen Status & SCREEN\_ON \\ \hline
Connectivity & NOT\_CONNECTED \\ \hline
Location & Lat 37.8735436 Long -122.2992491 - 1412538686641 (Time it was updated) \\ \hline
View & com.mobilityware.solitaire/.Solitaire \\ \hline
History &  \pbox{20cm}{com.android.phone/.InCallScreen \\ com.android.launcher/com.android.- \\ launcher2.Launcher \\ com.android.mms/ConversationList}   \\ \hline
Path & N/A \\ \hline
Screenshot & 898448929 \\ \hline
\end{tabular}
\caption{Corresponding log entry}
\label{tbl:smplLog}
\end{subfigure}
\caption{Screenshot (a) and corresponding log entry (b) captured during the experiment.}
\label{fig:example}
\end{figure}

Figure \ref{fig:example} shows an example screenshot captured during the study along with its corresponding
log entry. The user was playing the Solitaire game while Spotify requested a WiFi scan. Since
this function was of interest (Table \ref{tbl:perm-list}), our instrumentation took a screenshot.  Since Spotify was not the application the participant was
interacting with, its visibility is set to {\it false}. The history shows that prior to Spotify calling {\tt getScanResults()}, the user had viewed Solitaire, the call screen, the launcher, and the list of MMS conversations.

\subsection{Recruitment}

We placed an online recruitment advertisement on Craigslist in October of 2014, under the ``et cetera jobs'' 
section.\footnote{Approved by our IRB under protocol \#2013-02-4992} The title of the advertisement was ``Research Study on Android Smartphones,'' and it 
stated that the study was about how people interact with their smartphones. We made no mention of 
security or privacy. Those interested in participating were directed to an online consent form. Upon agreeing to the consent form, potential participants were directed to a screening application in the Google Play store. The screening application asked for information about each potential participant's age, 
gender, smartphone make and model. It also collected data on their phones' internal memory size and the installed applications. We screened out applicants who 
were under 18 years of age or used providers other than T-Mobile, since our experimental phones could not attain 3G speeds on other providers. We collected data on participants' installed applications so that we could pre-install their free applications on our experimental phones, prior to them visiting our laboratory. (We copied their paid applications from their phones, since we could not download those from Google Play ahead of time.)

We contacted participants who met our screening requirements by email to schedule a time for them to come and visit us to do the initial setup. Overall, 48 people showed up to our laboratory, and out of those 48 people, 40 qualified (8 had to be rejected because our screening application was unable to distinguish some Metro PCS users from T-Mobile users).  In the email we
noted that due to the space constraints of our experimental phones, we might not be able to install all the applications present on their existing phones, and therefore they needed to make a note of the ones that they planned to use during the next week. The initial setup took roughly 30 minutes and involved installing their existing SIM cards into our experimental phones, helping them set up their Google and other accounts, and making sure they had all the applications needed
for the coming week. We immediately compensated each participant with a \100 gift card for completing the study.

\subsection{Exit Survey}

When participants returned to our laboratory, they completed an exit survey. The 
exit survey software ran on a laptop in a private room so that it could ask questions about what they were doing on their phones during the course of the week without raising privacy concerns. We did not view their screenshots until participants gave us permission. The survey had three components:

\begin{packed_item}
\item {\bf Screenshots}---Our software displayed a screenshot taken during the course of the week when one of the 12 resources in Table \ref{tbl:perm-list} was accessed. Next to the screenshot (Figure \ref{fig:initial_screen}), we asked participants what they were doing on the phone when the screenshot was taken (open-ended). We also asked them to indicate which of several actions they believed the application was performing, chosen from a multiple-choice list of permissions presented in plain language (e.g., ``reading browser history,'' ``sending a SMS,'' etc.). After answering these two questions, they proceeded to a second page of questions (Figure \ref{fig:followup_screen}). We informed participants at the top of this page of the resource that the application had accessed when the screenshot was taken, and asked them to indicate how much they expected this using a 5-point Likert scale. Next, we asked, ``if you were given the choice, would you have prevented the app from accessing this data,'' and to explain why or why not. Finally, we asked for permission to view that particular screenshot. This phase of the exit survey was repeated for 10-15 different screenshots per participant, based on the number of screenshots saved by our reservoir sampling algorithm.

\item {\bf Locked Screens}---The second part of our survey involved questions about the same protected resources, however, this time these resources were accessed while device screens were off (i.e., when participants were not actively using their phones). Because there were no contextual cues (i.e., screenshots), we outright told participants which applications were accessing which resources and asked them multiple choice questions about whether they wanted to prevent this and the degree to which these behaviors were expected. They answered these two questions for up to 10 different requests, similarly chosen by our reservoir sampling algorithm to yield a breadth of application/permission combinations.

\item {\bf Personal Privacy Preferences}---Finally, in order to correlate participants' responses with their personal privacy preferences, they completed two privacy scales. Because of the numerous reliability problems with the often cited Westin index~\cite{Woodruff2014}, participants completed both Buchanan et al.'s Privacy Concerns Scale (PCS)~\cite{Buchanan2007} and Malhotra et al.'s Internet Users' Information Privacy Concerns (IUIPC) scale~\cite{Malhotra04}. We compared the average scores of both scales.
\end{packed_item}


\begin{figure}[t]
\centering
\begin{subfigure}[h]{0.5\textwidth}
\includegraphics[width=3.2in]{scnshotone.png}
\caption{On the first screen, participants answered questions to establish awareness of the permission request based on the screenshot.}
\label{fig:initial_screen}
\end{subfigure}
\quad
\begin{subfigure}[h]{0.5\textwidth}
\includegraphics[width=3.2in]{scnshottwo.png}
\caption{On the second screen, they saw the resource accessed, stated whether it was expected, and whether it should have been blocked.}
\label{fig:followup_screen}
\end{subfigure}
\begin{flushleft}
\caption{Exit Survey Interface}
\label{fig:three graphs}
\end{flushleft}
\end{figure}

After completing the exit survey, we re-entered the room, answered any remaining questions about the experiment, and then assisted the participant in transferring her SIM card back into her personal phone. Finally, we compensated each participant with a \\approx19\mu=3.26\mu=2.66$). They also stated a willingness to block almost half of the permission requests (49.6\% of 250) when not in use, compared to a third of the requests that occurred when using their phones (35.2\% of 423). However, neither of these differences was statistically significant.





\section{Modeling Users' Decisions}
\label{sec:regressions}


We constructed several statistical models to examine whether users' desire to block certain permission requests could be predicted using the contextual data that we collected. If such a relationship exists, a classifier could determine when to prompt users about potentially unexpected permission requests. Thus, the response variable in our models is the user's choice to block the given permission request or not. Our predictive variables consisted of the information that might be available at runtime: permission type, requesting application, and visibility of that application. We constructed several mixed effects binary logistic regression models to account for both inter-subject and intra-subject correlations.

\subsection{Model Selection}
In our mixed effects models, permission types and visibility of the requesting application were fixed effects, because all possible values for each variable existed in our data set. Visibility had two values: visible (the user is interacting with the application or has other contextual cues to know that it is running) and invisible. Permission types were categorized based on Table \ref{tbl:privacy_breakdown}. The application name and the participant ID were included as random effects, because our survey data did not have an exhaustive list of all possible applications a user could run, and the participant has a non-systematic effect on the data.





Table \ref{tbl:models} shows two goodness-of-fit metrics: the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC). Lower values for AIC and BIC represent better fit. Table \ref{tbl:models} shows the different parameters included in each model. We found no evidence of interaction effects and therefore did not include them. Visual inspection of residual plots of each model did not reveal obvious deviations from homoscedasticity or normality.

We initially included the phone's screen state as another variable. However, we found that creating two separate models based on the screen state resulted in better fit than using a single model that accounted for it as a fixed effect. When the screen was on, the model including application visibility and application name, while controlling for subject effects, offered the best fit. Here, fit improved once permission type was removed from the model, which shows that the decision to block a particular permission type changes based on contextual factors. When the screen is off, however, the effect of permission type was relatively stronger. Similarly, the strong subject effect in both models indicates that these decisions are highly nuanced from one user to the next. As a result, any classifier developed to automatically decide whether to block a permission at runtime (or prompt the user) will need to be tailored to that particular user's needs.

\begin{table}[t]
\centering
\small
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Predictors} & \textbf{AIC} & \textbf{BIC} &  \textbf{Screen State} \\ \hline
\hline
UserCode & 490.60 & 498.69 & Screen On \\ \hline
Application & 545.98 & 554.07 & Screen On \\ \hline
\begin{tabular}[c]{@{}l@{}}Application\\ UserCode\end{tabular} & 491.86 & 503.99 &  Screen On \\ \hline
\begin{tabular}[c]{@{}l@{}}Permission \\ Application\\ UserCode\end{tabular} & 494.69 & 527.05 &  Screen On \\ \hline
\textbf{\begin{tabular}[c]{@{}l@{}}Visibility \\Application\\ UserCode \end{tabular}} & \textbf{481.65} & \textbf{497.83}
 & \textbf{Screen On}\\ \hline
 \begin{tabular}[c]{@{}l@{}} Permission \\ Visibility \\ Application \\ UserCode \end{tabular} & 484.23 & 520.64 & Screen On \\ \hline
\hline

UserCode & 245.13 & 252.25 & Screen Off \\ \hline
Application & 349.38 & 356.50 &  Screen Off \\ \hline
\begin{tabular}[c]{@{}l@{}}Application\\ UserCode\end{tabular} & 238.84 & 249.52 &  Screen Off \\ \hline
\textbf{\begin{tabular}[c]{@{}l@{}}Permission  \\ Application \\ UserCode \end{tabular}} & \textbf{235.48} & \textbf{263.97} &  \textbf{Screen Off} \\ \hline





\end{tabular}
\begin{flushleft}
\caption{Goodness-of-fit metrics for various mixed effects logistic regression models on the exit survey data.}
\label{tbl:models}
\end{flushleft}
\end{table}





\subsection{Predicting User Reactions}

Using these two models, we built two classifiers to make decisions about whether to block any of the sensitive permission requests listed in Table \ref{tbl:privacy_breakdown}. These corresponded to 1.3M requests in our logs. We used our exit survey data as ground truth, randomly partitioning it into five groups, which we used for 5-fold cross-validation. We used four groups for training and one for evaluation. We iterated over the entire process 5 times and therefore report the averages over these 5 iterations.

We calculated the receiver operating characteristic (ROC) to capture the tradeoff between true-positive and false-positive rate. The quality of the classifier can be quantified with a single value by calculating the area under its ROC curve (AUC)~\cite{roc}. The closer the AUC gets to 1.0, the better the classifier is. The AUC for a random classifier is 0.5. When screens were on, the AUC was 0.7, which is 40\% better than the random baseline, making it a fair classifier. When screens were off, the AUC was 0.8, which is 60\% better than a random baseline, making it a good classifier. 

We ran the classifier on the log files we gathered from users and had it predict the portion of sensitive requests that users might want to block. When phone screens were on, the model predicted that participants would have blocked 35.29\% of sensitive requests, blocking an average of 645 requests daily. When screens were off, the classifier predicted 35.1\% of requests would have been blocked (1,143 requests per user/day). 










\section{Discussion}

We observed that in one week of standard smartphone usage, 80\% of our participants deemed at least one permission request as being inappropriate. This violates Nissenbaum's notion of ``privacy as contextual integrity'' because applications were performing actions that defied users' expectations~\cite{nissenbaum2009privacy}. Felt et al.\ posited that users may be able to better understand why permission requests are needed if some of these requests are granted via runtime consent dialogs, rather than Android's current install-time notification approach~\cite{Felt2012b}. By granting permissions at runtime, users will have additional contextual information; based on what they were doing at the time that resources are requested, they may have a better idea of why those resources are being requested. We make two primary contributions that system designers can use to make more usable permissions systems: we show that runtime notifications need to be tailored to individual users' needs and that platforms need to account for whether an application's access to protected resources is obvious to the user.

Based on the frequency with which potential runtime permissions are requested (Section \ref{sec:apps}), it is infeasible to prompt users each and every time. Doing so would quickly overwhelm them and lead to habituation. Yet at the same time, drawing user attention to the situations in which they are likely to be concerned will lead to greater control and awareness. Thus, the challenge is in automatically inferring {\it when} users are likely to find a permission request unexpected, and then only prompting them in these cases. Based on our data, we observed that participants' desires to block particular permissions were heavily influenced by two main factors: their understanding of the relevance of a permission request to the functionality of the requesting application and their individual privacy concerns.

Our models in Section \ref{sec:regressions} showed that individual characteristics greatly explain the variance between what different users deem appropriate, in terms of access to protected resources. While responses to privacy scales failed to explain these differences, this was not a surprise, as the disconnect between stated privacy preferences and behaviors is well-documented (e.g.,~\cite{Acquisti05}). This means that in order to accurately model user preferences, the system will need to learn what a specific user deems inappropriate over time. Thus, a feedback loop is likely needed: when devices are ``new,'' users will be required to provide more input surrounding permission requests, and then based on their responses, they will see fewer requests in the future. While we documented the frequency with which protected resources were accessed, future work is needed to quantify the varying contexts in which it can occur. Specifically, habituation could be drastically reduced (i.e., fewer runtime prompts) if users are only asked for permission for unique combinations of application, resource, and context. Future work is needed to quantify the varying contexts, as they are likely to be much more complex than ``foreground vs. background application'' or ``screen on vs. screen off.''

Beyond individual subject characteristics (i.e., personal preferences), participants based their decisions to block certain permission requests on the specific applications making the requests and whether they had contextual cues to indicate that the applications were running (and therefore needed the data to function). Future systems could take these factors into account when deciding whether or not to draw user attention to a particular request. For example, when an application that a user is not actively using requests access to a protected resource, she should be shown a runtime prompt. If she decides to grant that request, then that decision is likely to hold when she is actively using that same application (and therefore a subsequent prompt may not be needed). At a minimum, platforms need to treat permission requests from background applications differently than those originating from foreground applications. Similarly, applications running in the background should use passive indicators to communicate when they are accessing particular resources. Platforms can also be designed to make decisions about whether or not access to resources should be granted based on whether contextual cues are present, or at its most basic, whether the device screen is even on.

Finally, we built our models and analyzed our data within the framework of what resources our participants {\it believed} were necessary for applications to correctly function. Obviously, their perceptions may have been incorrect in many cases, such that if they better understood why a particular resource was necessary, they may have been more permissive. Thus, it is incumbent on developers to adequately communicate why particular requests are necessary, as this greatly impacts user notions of contextual integrity. Yet, no mechanisms in Android exist for developers to do this as part of the permission-granting process. For example, one could imagine requiring metadata to be provided that explains how each requested resource will be used, and then automatically integrating this information into permission requests. Tan et al.\ examined a similar feature on iOS that allows developers to include free-form text in runtime permission dialogs and observed that users were significantly more likely to grant requests that included this text~\cite{Tan2014}. Thus, we believe that including succinct explanations in these requests would go a long way towards preserving contextual integrity by promoting greater transparency.

In conclusion, we believe this study was instructive in showing the circumstances in which Android permission requests are made under real-world usage. While prior work has already established that most mobile permissions systems are failing users, we believe our study can benefit system designers by demonstrating several ways in which contextual integrity can be improved, thereby empowering users to make better security decisions.















{\footnotesize \bibliographystyle{acm}
\bibliography{biblio}

\appendix

\section{Invisible requests}
\label{app:invisible}

Following list shows the set of applications that have requested most number of permissions while executing invisibly to the user and the most requested permission types by each respective application.

\begin{packed_item}
\item {\it Facebook App}--- ACCESS NETWORK STATE, ACCESS FINE LOCATION, ACCESS WIFI STATE ,WAKE LOCK, INTERNET
\item {\it Google Location}---WAKE LOCK, ACCESS FINE LOCATION, GET ACCOUNTS, ACCESS COARSE LOCATION, ACCESS WIFI STATE
\item {\it Facebook Messenger}---ACCESS NETWORK STATE, ACCESS WIFI STATE, WAKE LOCK, READ PHONE STATE, INTERNET
\item {\it Taptu DJ}---ACCESS NETWORK STATE, INTERNET, NFC
\item {\it Google Maps}---ACCESS NETWORK STATE, GET ACCOUNTS, WAKE LOCK, ACCESS FINE LOCATION, INTERNET
\item {\it Google (Gapps)}---WAKE LOCK, ACCESS FINE LOCATION, AUTHENTICATE ACCOUNTS, ACCESS NETWORK STATE, ACCESS WIFI STATE
\item {\it Fouraquare}---ACCESS WIFI STATE, WAKE LOCK, ACCESS FINE LOCATION, INTERNET, ACCESS COARSE LOCATION
\item {\it Yahoo Weather}---ACCESS FINE LOCATION, ACCESS NETWORK STATE, INTERNET, ACCESS WIFI STATE, WRITE SYNC SETTINGS
\item {\it Devexpert Weather}---ACCESS NETWORK STATE, INTERNET, ACCESS FINE LOCATION, ACCESS COARSE LOCATION
\item {\it Tile Game(Umoni)}---ACCESS NETWORK STATE, WAKE LOCK, INTERNET, ACCESS WIFI STATE, WRITE SETTINGS
\end{packed_item}

Following is the most frequently requested permission type by applications while running invisibly to the user and the applications who requested the respective permission type most.

\begin{packed_item}
\item {\it ACCESS\_NETWORK\_STATE}--- Facebook App, Google Maps, Facebook Messenger, Google (Gapps), Taptu - DJ
\item {\it WAKE\_LOCK}---Google (Location), Google (Gapps), Google (GMS), Facebook App, GTalk.
\item {\it ACCESS\_FINE\_LOCATION}---Google (Location), Google (Gapps), Facebook App, Yahoo Weather, Rhapsody (Music)
\item {\it GET\_ACCOUNTS}---Google (Location), Google (Gapps), Google (Login), Google (GM), Google (Vending)
\item {\it ACCESS\_WIFI\_STATE}---Google (Location), Google (Gapps), Facebook App, Foursqaure, Facebook Messenger
\item {\it UPDATE\_DEVICE\_STATS}---Google (SystemUI), Google (Location), Google (Gapps)
\item {\it ACCESS\_COARSE\_LOCATION}---Google (Location), Google (Gapps), Google (News), Facebook App, Google Maps
\item {\it AUTHENTICATE\_ACCOUNTS}---Google (Gapps), Google (Login), Twitter, Yahoo Mail, Google (GMS)
\item {\it READ\_SYNC\_SETTINGS}---Google (GM), Google ( GMS ), android.process.acore, Google (Email), Google (Gapps)
\item {\it INTERNET}---Google (Vending), Google (Gapps), Google (GM), Facebook App, Google (Location)
\end{packed_item}

\newpage

\section{Permission Type Breakdown}
\label{app:prem}

This table lists the most frequently used permissions during the study period.

{\center
\small
\begin{tabular}{|l|l|}
\hline
\textbf{Permission Type} & \textbf{Requests} \\ \hline
ACCESS\_NETWORK\_STATE & 41077 \\ \hline
WAKE\_LOCK & 27030 \\ \hline
ACCESS\_FINE\_LOCATION & 7400 \\ \hline
GET\_ACCOUNTS & 4387 \\ \hline
UPDATE\_DEVICE\_STATS & 2873 \\ \hline
ACCESS\_WIFI\_STATE & 2092 \\ \hline
ACCESS\_COARSE\_LOCATION & 1468 \\ \hline
AUTHENTICATE\_ACCOUNTS & 1335 \\ \hline
READ\_SYNC\_SETTINGS & 836 \\ \hline
VIBRATE & 740 \\ \hline
INTERNET & 739 \\ \hline
READ\_SMS & 611 \\ \hline
READ\_PHONE\_STATE & 345 \\ \hline
STATUS\_BAR & 290 \\ \hline
WRITE\_SYNC\_SETTINGS & 206 \\ \hline
CHANGE\_COMPONENT\_ENABLED\_STATE & 197 \\ \hline
CHANGE\_WIFI\_STATE & 168 \\ \hline
READ\_CALENDAR & 166 \\ \hline
ACCOUNT\_MANAGER & 134 \\ \hline
ACCESS\_ALL\_DOWNLOADS & 127 \\ \hline
READ\_EXTERNAL\_STORAGE & 126 \\ \hline
USE\_CREDENTIALS & 101 \\ \hline
READ\_LOGS & 94 \\ \hline
WRITE\_SMS & 62 \\ \hline
WRITE\_CALENDAR & 60 \\ \hline
Bluetooth & 60 \\ \hline
CONNECTIVITY\_INTERNAL & 58 \\ \hline
STATUS\_BAR\_SERVICE & 57 \\ \hline
READ\_SYNC\_STATS & 56 \\ \hline
NFC & 51 \\ \hline
WRITE\_SETTINGS & 50 \\ \hline
WRITE\_EXTERNAL\_STORAGE & 48 \\ \hline
PACKAGE\_USAGE\_STATS & 28 \\ \hline
SUBSCRIBED\_FEEDS\_READ & 24 \\ \hline
\end{tabular}
}

\newpage

\section{User Application Breakdown}
\label{app:app_brekdown}

This table shows the applications that most frequently requested access to protected resources during the study period.
{\center
\small
\begin{tabular}{|l|l|}
\hline
\textbf{Application Name} & \textbf{Requests} \\ \hline
facebook.katana & 40041 \\ \hline
google.process.location & 32426 \\ \hline
facebook.orca & 24702 \\ \hline
taptu.streams & 15188 \\ \hline
google.android.apps.maps & 6501 \\ \hline
google.process.gapps & 5340 \\ \hline
yahoo.mobile.client.android.weather & 5505 \\ \hline
tumblr & 4251 \\ \hline
king.farmheroessaga & 3862 \\ \hline
joelapenna.foursquared & 3729 \\ \hline
telenav.app.android.scout\_us & 3335 \\ \hline
devexpert.weather & 2909 \\ \hline
ch.bitspin.timely & 2549 \\ \hline
umonistudio.tile & 2478 \\ \hline
king.candycrushsaga & 2448 \\ \hline
android.systemui & 2376 \\ \hline
bambuna.podcastaddict & 2087 \\ \hline
contapps.android & 1662 \\ \hline
handcent.nextsms & 1543 \\ \hline
foursquare.robin & 1408 \\ \hline
qisiemoji.inputmethod & 1384 \\ \hline
devian.tubemate.home & 1296 \\ \hline
google.android.gm & 1180 \\ \hline
lookout & 1158 \\ \hline
aol.mobile.aim & 1125 \\ \hline
google.android.music:main & 1114 \\ \hline
rhapsody & 1074 \\ \hline
google.android.gms & 1021 \\ \hline
\begin{tabular}[c]{@{}l@{}}yahoo.mobile.client.android.\\fantasyfootball \end{tabular}
& 886 \\ \hline
jb.gosms & 755 \\ \hline
google.android.googlequicksearchbox & 711 \\ \hline
android.mms & 695 \\ \hline
ideashower.readitlater.pro & 672 \\ \hline
android.inputmethod.latin & 672 \\ \hline
google.android.gsf.login & 618 \\ \hline
pandora.android & 611 \\ \hline
google.android.apps.plus & 582 \\ \hline
andrewshu.android.reddit & 569 \\ \hline
tunein.player & 556 \\ \hline
airkast.KNBRAM & 546 \\ \hline
twitter.android & 523 \\ \hline
android.vending & 486 \\ \hline
yahoo.mobile.client.android.mail & 480 \\ \hline
sg.gumi.bravefrontier & 473 \\ \hline
yahoo.mobile.client.android.yahoo & 458 \\ \hline
nuance.swype.trial & 402 \\ \hline
viber.voip & 392 \\ \hline
zynga.words & 388 \\ \hline
touchtype.swiftkey & 377 \\ \hline
\end{tabular}
}

\newpage

\section{Distribution of Requests}
The two graphs shows the distribution of number of requests per day per user and the distribution of request for a given day. First graph shows the distribution of requests through out a given day averaged across the data set and second graph shows the distribution of number requests across days for each user.

\begin{figure}[h]
\centering
\begin{subfigure}[h]{0.5\textwidth}
\includegraphics[scale=0.45]{total-perm-histo}
\end{subfigure}
\quad
\begin{subfigure}[h]{0.5\textwidth}
\includegraphics[scale=0.45]{day-histo}
\end{subfigure}
\begin{flushleft}
\end{flushleft}
\end{figure}
\end{document}
