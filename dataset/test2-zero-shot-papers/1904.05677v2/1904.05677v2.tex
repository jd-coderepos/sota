








\documentclass[10pt,journal,compsoc]{IEEEtran}




\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{color}
\usepackage{comment}
\usepackage{url}
\usepackage{xcolor}
\usepackage{tabu}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{wrapfig}
\usepackage{breakcites}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\def\Plus{\texttt{+}}

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother











\ifCLASSOPTIONcompsoc
\usepackage[nocompress]{cite}
\else
\usepackage{cite}
\fi






\ifCLASSINFOpdf
\else
\fi





















































\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
\title{Deep Back-Projection Networks for Single Image Super-resolution}


\author{Muhammad~Haris,
        Greg~Shakhnarovich,
        and~Norimichi~Ukita,~\IEEEmembership{Member,~IEEE}\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem M. Haris and N. Ukita are with Intelligent Information Media Lab, Toyota Technological Institute (TTI), Nagoya,
Japan, 468-8511.\protect\\
E-mail: muhammad.haris@bukalapak.com* and ukita@toyota-ti.ac.jp (*he is currently working at Bukalapak in Indonesia)
\IEEEcompsocthanksitem G. Shakhnarovich is with TTI at Chicago, US.
E-mail: greg@ttic.edu}\thanks{Manuscript received -; revised -.}}

















\IEEEtitleabstractindextext{\begin{abstract}
Previous feed-forward architectures of recently proposed deep
super-resolution networks learn the features of low-resolution
inputs and the non-linear mapping from those to a high-resolution
output. However, this approach does not fully address the mutual
dependencies of low- and high-resolution images. 
We propose Deep Back-Projection Networks (DBPN), 
the winner of two image super-resolution challenges (NTIRE2018 and PIRM2018),
that exploit iterative up- and down-sampling layers.
These layers are formed as a unit providing an error feedback mechanism for projection errors.
We construct mutually-connected up- and down-sampling
units each of which represents different types of low- and high-resolution components. 
We also show that extending this idea to demonstrate a new insight towards more efficient network design substantially, such as parameter sharing on the projection module and transition layer on projection step.
The experimental results yield superior results and in particular
establishing new state-of-the-art results across multiple data sets, especially for large scaling factors
such as .
\end{abstract}

\begin{IEEEkeywords}
Image super-resolution, deep cnn, back-projection, deep concatenation, large scale, recurrent, residual
\end{IEEEkeywords}}


\maketitle


\IEEEdisplaynontitleabstractindextext




\IEEEpeerreviewmaketitle

\IEEEraisesectionheading{\section{Introduction}
\label{sec:introduction}}

\IEEEPARstart{S}{ignificant} progress in deep neural network (DNN) for
vision~\cite{huang2017densely,he2015deep,denton2015deep,shrivastava2016learning,larsson2016fractalnet,radford2015unsupervised,IMKDB17}
has recently been propagating to the field of super-resolution (SR)~\cite{johnson2016perceptual,tao2017detail,sajjadi2018frame,Haris17,Kim_2016_VDSR,LapSRN,Tai-DRRN-2017,zhang2018image,park2018srfeat}. 

Single image SR (SISR) is an ill-posed inverse problem where the aim is to
recover a high-resolution (HR) image from a low-resolution (LR)
image. A currently typical approach is to construct an HR image by
learning non-linear LR-to-HR mapping, implemented as a DNN~\cite{dong2016image, dong2016accelerating, shi2016real, LapSRN, Kim_2016_VDSR, kim2016deeply, Tai-DRRN-2017} and non DNN~\cite{schulter2015fast,haris2017first,timofte2014a+,Haris17Sparse}. 
For DNN approach, the networks compute a sequence of feature maps from the LR image, culminating with
one or more upsampling layers to increase resolution and finally
construct the HR image. In contrast to this purely feed-forward
approach, the human visual system is believed to use a feedback connection
to simply guide the task for the relevant
results~\cite{felleman1991distributed, kravitz2013ventral,
  lamme2000distinct}. Perhaps hampered by lack of such feedback,
the current SR networks with only feed-forward connections have difficulty in representing the LR-to-HR relation, especially for large scaling factors.

On the other hand, feedback connections were used effectively by one
of the early SR algorithms, the iterative
back-projection~\cite{irani93}. It iteratively computes the
reconstruction error, then uses it to refine the HR image. 
Although it has been proven to improve the image quality,
results still suffers from ringing and chessboard
artifacts~\cite{dai2007bilateral}. Moreover, this method is sensitive to
choices of parameters such as the number of iterations and the blur
operator, leading to variability in results.





\begin{figure}[t!]
\centering
\includegraphics[width=8cm]{intro_new}
\caption{Super-resolution result on  enlargement. PSNR: LapSRN~\cite{LapSRN} (15.25 dB), EDSR~\cite{Lim_2017_CVPR_Workshops} (15.33 dB), and Ours~\cite{haris2018deep} (16.63 dB).}
\label{figure:intro}
\end{figure}

\begin{figure*}[t!]
\centering
\includegraphics[width=15cm]{multi_upsampling}\vspace{-1em}
\caption{Comparisons of Deep Network SR. (a) Predefined upsampling (e.g., SRCNN~\cite{dong2016image}, VDSR~\cite{Kim_2016_VDSR}, DRRN~\cite{Tai-DRRN-2017}) commonly uses the conventional interpolation, such as Bicubic, to upscale LR input images before entering the network. (b) Single upsampling (e.g., FSRCNN~\cite{dong2016accelerating}, ESPCN~\cite{shi2016real}) propagates the LR features, then construct the SR image at the last step. (c) Progressive upsampling uses a Laplacian pyramid network to gradually predict SR images~\cite{LapSRN}. (d) Iterative up- and down-sampling approach is proposed by our DBPN that exploit the mutually connected up- (blue box) and down-sampling (gold box) units to obtain numerous HR feature maps in different depths.}
\label{figure:multi_upsampling}
\end{figure*}

Inspired by~\cite{irani93}, we construct an end-to-end
trainable architecture based on the idea of iterative up- and
down-sampling layers: Deep Back-Projection Networks (DBPN). Our networks are not only able to remove the ringing and chessboard effect but also successfully perform large scaling factors, as shown
in~Fig.~\ref{figure:intro}. 
Furthermore, DBPN has been proven by winning SISR challenges. On NTIRE2018~\cite{timofte2018ntire}, DBPN is the  winner on track 8 Bicubic downscaling. On PIRM2018~\cite{pirm2018}, DBPN got  on Region 2,  on Region 1, and  on Region 3.



Our work provides the following contributions:

\noindent(1) \textbf{Iterative up- and down-sampling units}. 
Most of the existing methods extract the feature maps with the same size of LR input image, then these feature maps are finally upsampled to produce SR image directly or progressively.
 However, DBPN performed iterative up- and down- sampling layers so that it can capture features at each resolution that are helpful in recovering features at other resolution in a single framework. 
Our networks focus not only on generating variants of the HR features using the up-sampling unit but also on projecting it back to the LR spaces using the down-sampling unit. 
It is shown in~Fig.~\ref{figure:multi_upsampling} (d), alternating between up- (blue box) and down-sampling (gold box) units, which represent the mutual relation of LR and HR features. 
Each set of up- and down-sampling layers achieve data augmentation in the feature space to represent a certain set of features at LR and HR resolution. So, multiple sets of up- and down-sampling layers represent multiple feature maps that are useful for producing the SR image of an input LR image. That is, even if all features extracted by these layers are produced from the single input LR image, these features can be different from each other by training DBPN so that their variety helps to improve its output (i.e., SR image). The detailed explanation can be seen in Section~\ref{subsec:network architecture}.


\noindent(2) \textbf{Error feedback}. We propose an iterative
error-correcting feedback mechanism for SR, which calculates both up- and down-projection errors to guide the reconstruction for obtaining better results. 
Here, the projection errors are used to refine the initial features in early layers. 
The detailed explanation can be seen in Section~\ref{subsec:projection_unit}.

\noindent(3) \textbf{Deep concatenation}. Our networks represent different types of LR and HR components produced by each up- and down-sampling unit. 
This ability enables the networks to reconstruct the HR image using concatenation of the HR feature maps from all of the up-sampling units. 
Our reconstruction can directly use different types of HR feature maps from different depths without propagating them through the other layers as shown by the red arrows in~Fig.~\ref{figure:multi_upsampling} (d).




In this work, we make the following extensions to demonstrate a new insight towards more efficient network design substantially compare to our early results~\cite{haris2018deep}. 
This manuscript focuses on optimized DBPN architecture with advanced design methodologies. 
The detailed explanation can be seen in Section~\ref{subsec:recurrent_dbpn}. 
Based on this experiment, we found several technical contributions as follows.

\noindent(1) \textbf{Parameter sharing on projection module}. 
Due to a large amount of parameters, it is hard to train deeper DBPN. 
Based on our experiments, the deepest setting uses . 
However, using parameter sharing, we can avoid the increasing number of parameters, while widening the receptive field. 
The effectiveness of parameter sharing was shown in Table~\ref{tab:psnr_variant} where DBPN-R64-10 has better performance than D-DBPN while reducing the number of parameters by 10.

\noindent(2) \textbf{Transition layer on projection step}.
Inspired by dense connection network~\cite{huang2017densely}, we use transition layer to create multiple projection step. 
The last up- and down-projection units were used as transition layer. 
The last up-projection was used to produce final HR feature-maps for each iteration, and the last down-projection was used to produce the next input for the next iteration.
Table~\ref{tab:psnr_variant} shows that DBPN-RES-MR64-3 outperforms previous setting, D-DBPN, by a large margin. DBPN-RES-MR64-3 successfully improves D-DBPN performance by 0.7 dB on Urban100 without increasing the model parameter.




 \section{Related Work}
\label{sec:related}
\subsection{Image super-resolution using deep networks}
Deep Networks SR can be primarily divided into four types as shown in~Fig.~\ref{figure:multi_upsampling}.

(a) \textbf{Predefined upsampling} commonly uses interpolation as the upsampling operator to produce a middle resolution (MR) image. This scheme was proposed by SRCNN~\cite{dong2016image} to learn MR-to-HR non-linear mapping with simple convolutional layers as in non deep learning based approaches~\cite{timofte2014a+,schulter2015fast}. Later, the improved networks exploited residual learning~\cite{Kim_2016_VDSR,Tai-DRRN-2017} and recursive layers~\cite{kim2016deeply}. However, this approach has higher computation because the input is the MR image which has the same size as the HR image.

(b) \textbf{Single upsampling} offers a simple way to increase the resolution. This approach was firstly proposed by FSRCNN~\cite{dong2016accelerating} and ESPCN~\cite{shi2016real}. 
These methods have been proven effective to increase the resolution and replace predefined operators. 
Further improvements include residual network~\cite{Lim_2017_CVPR_Workshops}, dense connection~\cite{zhang2018residual}, and channel attention~\cite{zhang2018image}
However, they fail to learn complicated mapping of LR-to-HR image, especially on large scaling factors, due to limited feature maps from the LR image.
This problem opens the opportunities to propose the mutual relation from LR-to-HR image that can preserve HR components better.

(c) \textbf{Progressive upsampling} was recently proposed in
LapSRN~\cite{LapSRN}. It progressively reconstructs the multiple SR
images with different scales in one feed-forward network. For the sake
of simplification, we can say that this network is a stacked of single upsampling networks which only relies on limited LR feature maps. 
Due to this fact, LapSRN is outperformed even by our shallow networks especially for large scaling factors such as  in experimental results.

(d) \textbf{Iterative up- and down-sampling} is proposed by our networks~\cite{haris2018deep}. We focus on increasing the sampling rate of HR feature maps in different depths from iterative up- and down-sampling layers, then, distribute the tasks to calculate the reconstruction error on each unit. This scheme enables the networks to preserve the HR components by learning various up- and down-sampling operators while generating deeper features.

\subsection{Feedback networks} 
Rather than learning a non-linear mapping of input-to-target space in one step, the feedback networks compose the prediction process into multiple steps which allow the model to have a self-correcting procedure. Feedback procedure has been implemented in various computing tasks~\cite{carreira2016human,ross2011learning,tu2010auto,li2016iterative,zamir2016feedback, shrivastava2016contextual,lotter2016deep}.

In the context of human pose estimation, Carreira et al.~\cite{carreira2016human} proposed an iterative error feedback by
iteratively estimating and applying a correction to the current
estimation. PredNet~\cite{lotter2016deep} is an unsupervised recurrent
network to predictively code the future frames by recursively feeding
the predictions back into the model. For image segmentation, Li et
al.~\cite{li2016iterative} learn implicit shape priors and use them to
improve the prediction. However, to our knowledge, feedback procedures
have not been implemented to SR.

\subsection{Adversarial training}
Adversarial training, such as with Generative Adversarial Networks (GANs)~\cite{goodfellow2014generative} has been applied to various image reconstruction problems~\cite{ledig2016photo, sajjadi2016enhancenet, radford2015unsupervised, denton2015deep, johnson2016perceptual}. For the SR task, Johnson et al.~\cite{johnson2016perceptual} introduced perceptual losses based on high-level features extracted from pre-trained networks. Ledig et al.~\cite{ledig2016photo} proposed SRGAN which is considered as a single upsampling method. It proposed the natural image manifold that is able to create photo-realistic images by specifically formulating a loss function based on the euclidian distance between feature maps extracted from VGG19~\cite{simonyan2014very}. Our networks can be extended with the adversarial training. The detailed explanation is available in Section~\ref{sec:perceptual}.

\subsection{Back-projection} 
Back-projection~\cite{irani93} is an efficient iterative
procedure to minimize the reconstruction error. Previous studies have
proven the effectiveness of back-projection~\cite{zhao2017iterative,
  haris2017first, dong2009nonlocal, timofte2016seven}. Originally,
back-projection in SR was designed for the case with multiple LR inputs. However, given only one LR input image, the reconstruction procedure can be obtained by upsampling the LR image using multiple upsampling operators and calculate the reconstruction error iteratively~\cite{dai2007bilateral}. Timofte et al.~\cite{timofte2016seven} mentioned that back-projection could improve the quality of the SR images. Zhao et al.~\cite{zhao2017iterative} proposed a method to refine high-frequency texture details with an iterative projection process. However, the initialization which leads to an optimal solution remains unknown. Most of the previous studies involve constant and unlearned predefined parameters such as blur operator and number of iteration. 






 \section{Deep Back-Projection Networks} 
\label{sec:proposed}
Let  and  be HR and LR image with   and , respectively, where  and . The main building block of our proposed DBPN architecture is the
projection unit, which is trained (as part of the end-to-end training
of the SR system) to map either an LR feature map to an HR map
(up-projection), or an HR map to an LR map (down-projection). 

\subsection{Iterative back-projection}
\label{subsec:back_projection}
Back-projection is originally designed with multiple LR inputs~\cite{irani93}. Given only one LR input image, the iterative back-projection~\cite{dai2007bilateral} can be summarized as follows.



where  is the final SR image at the -th iteration, 
 is a constant back-projection kernel and  is a single blur filter, 
 and  are the up- and down-sampling operator, respectively.

The traditional back-projection relies on constant and unlearned predefined parameters such as single sampling filter and blur operator. 
To extend this algorithm, our proposal preserves the HR components by learned various up- and down-sampling operators and generates deeper features to construct numerous pair of LR-and-HR feature maps. 
We develop an end-to-end trainable architecture which focuses to guide the SR task using mutually connected up- and down-sampling layers to learn non-linear mutual relation of LR-to-HR components. 
The mutual relation between LR and HR components is constructed by creating iterative up- and down-projection layers where the up-projection unit generates HR feature maps, then the down-projection unit projects it back to the LR spaces as shown in~Fig.~\ref{figure:multi_upsampling} (d).

\subsection{Projection units}
\label{subsec:projection_unit}


The up-projection unit is defined as follows:

where * is the spatial convolution operator,  and  are, respectively, the up- and
down-sampling operator with scaling factor , and  are (de)convolutional layers at stage .

The up-projection unit, illustrated in the upper part of~Fig.~\ref{figure:projection_unit}, takes the previously computed LR feature map
 as input, and maps it to an (intermediate) HR map ;
then it attempts to map it back to LR map 
(``back-project''). The residual (difference)  between the observed LR map
 and the reconstructed  is mapped to HR again,
producing a new intermediate (residual) map ; the final output of the unit,
the HR map , is obtained by summing the two intermediate HR maps.

The down-projection unit, illustrated in the lower part of~Fig.~\ref{figure:projection_unit}, is defined very similarly, but now its job is
to map its input HR map  to the LR map .


We organize projection units in a series of \emph{stages}, alternating between  and .
These projection units can be understood as a self-correcting procedure which feeds a projection error to the sampling layer and iteratively changes the solution by feeding back the projection error.

\begin{figure}[t!]
\centering
\includegraphics[width=8cm]{projection_unit}\vspace{-1em}
\caption{Proposed up- and down-projection units in the DBPN. These units produce residual  between the initial features and the reconstructed features, then fuse it back by summing it to the initial features.}
\label{figure:projection_unit}
\end{figure}

The projection unit uses large sized filters such as  and . In the previous approaches, the use of large-sized filters is avoided because it can slow down the convergence speed and might produce sub-optimal results. 
However, the iterative up- and down-sampling units enable the mutual relation between LR and HR components.
These units also take benefit of large receptive fields to perform better performance especially on large scaling factor where the significant amount of pixels is needed.

\subsection{Network architecture}
\label{subsec:network architecture}
\begin{figure*}[t!]
\centering
\includegraphics[width=14cm]{proposed_network}\vspace{-1.5em}
\caption{An implementation of DBPN for super-resolution which exploits densely connected projection unit to encourage feature reuse.}
\label{figure:proposed_network}
\end{figure*}

The proposed DBPN is illustrated in~Fig.~\ref{figure:proposed_network}. It can be divided into three
parts: initial feature extraction, projection, and reconstruction, as
described below. Here, let \texttt{conv} be a convolutional layer,
where  is the filter size and  is the number of filters. 

\begin{enumerate}
\item \textbf{Initial feature extraction}. We construct initial LR
  feature-maps  from the input using \texttt{conv}. 
  Then \texttt{conv} is used to reduce the dimension from  to  before
  entering projection step where  is the number of filters used in the initial LR features extraction and  is the number of filters used in each projection unit.
\item \textbf{Back-projection stages}. Following initial feature
  extraction is a sequence of projection units, alternating between
  construction of LR and HR feature maps ( and ).
  Later, it further improves by dense connection where each unit has access to the outputs of all previous units (Section~\ref{subsec:dense_projection_unit}).

\item \textbf{Reconstruction}. Finally, the target HR image is
  reconstructed as  where  use \texttt{conv} as
  reconstruction and  refers to the concatenation of the feature-maps
  produced in each up-projection unit which called as deep concatenation.
\end{enumerate}

Due to the definitions of these building blocks, our network
architecture is modular. We can easily define and train networks with
different numbers of stages, controlling the depth. For a network with
 stages, we have the initial extraction stage (2 layers), and then
 up-projection units and  down-projection units, each with 3
layers, followed by the reconstruction (one more layer). 
However, for the dense projection unit, we add \texttt{conv} in each projection unit, except the first three units as mentioned in Section~\ref{subsec:dense_projection_unit}. \section{The Variants of DBPN}
\label{sec:variant}
In this section, we show how DBPN can be modified to apply the latest deep learning trends. 

\subsection{Dense projection units}
\label{subsec:dense_projection_unit}
The dense inter-layer connectivity pattern in DenseNets~\cite{huang2017densely} has been shown to 
alleviate the vanishing-gradient problem, produce improved features,
and encourage feature reuse. Inspired by this we propose to improve
DBPN, by introducing dense connections in the projection units called,
yielding Dense DBPN.

Unlike the original DenseNets, we avoid dropout and batch norm, which are not suitable for SR, because they remove the range flexibility of the features~\cite{Lim_2017_CVPR_Workshops}.
Instead, we use  convolution layer as the bottleneck layer for feature pooling and dimensional reduction~\cite{szegedy2015going,Haris17} before entering the projection unit.


In Dense DBPN, the input for each unit is the concatenation of the outputs from all previous units. Let the  and  be the input for dense up- and down-projection unit, respectively. They are generated using \texttt{conv} which is used to merge all previous outputs from each unit as shown in~Fig.~\ref{figure:D_DBPN}. This improvement enables us to generate the feature maps effectively, as shown in the experimental results.

\begin{figure}[t!]
\centering
\includegraphics[width=8.5cm]{D_DBPN}\vspace{-1em}
\caption{Proposed up- and down-projection unit in the Dense DBPN. The feature maps of all preceding units (i.e.,  and  in up- and down-projections units, respectively) are concatenated and used as inputs, and its own feature maps are used as inputs into all subsequent units.}
\label{figure:D_DBPN}
\end{figure}

\subsection{Recurrent DBPN}
\label{subsec:recurrent_dbpn}
Here, we propose recurrent DBPN which is able to reduce the number of parameters and widen the receptive field without increasing the model capacity.
In SISR, DRCN~\cite{kim2016deeply} proposed recursive layers without introducing new parameters for additional convolutions in the networks. 
Then, DRRN~\cite{Tai-DRRN-2017} improves residual networks by introducing both global and local residual learning using a very deep CNN model (up to 52-layers).
DBPN can also be treated as a recurrent network by sharing the projection units across the stages. We divided recurrent DBPN into two variants as mentioned below.

\noindent(a) \textbf{Parameter sharing on projection unit (DBPN-R)}. 
This variant uses only one up-projection unit and one down-projection unit which is shared across all stages without dense connection as shown in Fig.~\ref{figure:dbpnrec}. 

\noindent(b) \textbf{Transition layer on projection step (DBPN-MR)}. 
This variant uses multiple up- and down-projection units as shown in Fig.~\ref{figure:dbpniter}. 
The last up- and down-projection units were used as transition layer. 
Instead of taking the output from each up-projection unit, DBPN-MR takes the HR features only from the last up-projection unit, then concatenates the HR features from each iteration. 
Here, the output from the last down-projection unit is the input for the first up-projection layer on the next iteration.
Then, the last up-projection unit will receive the output of all previous down-projection units in the corresponding iteration.

\begin{figure}[t!]
\centering
\includegraphics[width=9cm]{dbpnrec}\vspace{-1.5em}
\caption{Recurrent DBPN with shared parameter (DBPN-R).}
\label{figure:dbpnrec}
\end{figure}

\begin{figure}[t!]
\centering
\includegraphics[width=9cm]{dbpniter}\vspace{-1.5em}
\caption{Recurrent DBPN with transition layer (DBPN-MR).}
\label{figure:dbpniter}
\end{figure}

\subsection{Residual DBPN}
\label{subsec:residual_dbpn}
Residual learning helps the network to converge faster and make the network have an easier job to produce only the difference between HR and interpolated LR image.
Initially, residual learning has been applied in SR by VDSR~\cite{Kim_2016_VDSR}.
Residual DBPN takes LR image as an input to reduce the computational time.
First, LR image is interpolated using Bicubic interpolation; then, at the last stage, the interpolated image is added to the reconstructed image to produce final SR image.

 \section{Experimental Results}
\label{sec:experiment}
\subsection{Implementation and training details}
In the proposed networks, the filter size in the projection unit is
various with respect to the scaling factor. For ,
we use  kernel with stride = 2 and pad by 2 pixels. 
Then,  use  kernel with stride = 4 and pad by 2 pixels. 
Finally, the  use  kernel with stride = 8 and pad by 2.\footnote{We found these settings to work well based on general intuition and preliminary experiments.}

We initialize the weights based on~\cite{he2015delving}. Here, standard deviation (std) is computed by  where ,  is the filter size, and  is the number of filters. For example, with  and , the std is . All convolutional and deconvolutional layers are followed by parametric rectified linear units (PReLUs), except the final reconstruction layer.

We trained all networks using images from DIV2K \cite{Agustsson_2017_CVPR_Workshops} with augmentation (scaling, rotating, flipping, and random cropping). To produce LR images, we downscale the HR images on particular scaling factors using Bicubic. We use batch size of 16 with size  for LR image, while HR image size corresponds to the scaling factors. The learning rate is initialized to  for all layers and decrease by a factor of 10 for every  iterations for total  iterations. We used Adam with momentum to  and trained with L1 Loss. All experiments were conducted using PyTorch 0.3.1 and Python 3.5 on NVIDIA TITAN X GPUs. The code is available in the internet.\footnote{The implementation is available \href{https://www.toyota-ti.ac.jp/Lab/Denshi/iim/members/muhammad.haris/projects/DBPN.html}{here}.}

\begin{table*}[t!]
\scriptsize
\caption{Model architecture of DBPN. "Feat0" and "Feat1" refer to first and second convolutional layer in the initial feature extraction stages. Note: () where  is filter size,  is number of filters,  is striding, and  is padding}
\centering
\label{tab:net_arc}
\begin{tabular}{*1l|*1c|*1c*1c*1c*1c*1c*1c*1c}
\hline
& \textbf{Scale} & \textbf{DBPN-SS}&\textbf{DBPN-S} & \textbf{DBPN-M}& \textbf{DBPN-L}& \textbf{D-DBPN-L}& \textbf{D-DBPN} & \textbf{DBPN}   \4pt]
\hline
Feat0&&(3,64,1,1)&(3,128,1,1)&(3,128,1,1)&(3,128,1,1)&(3,128,1,1)&(3,256,1,1)&(3,256,1,1)\4pt]
\hline
Reconstruction&&(1,1,1,0)&(1,1,1,0)&(1,1,1,0)&(1,1,1,0)&(1,1,1,0)&(3,3,1,1)&(3,3,1,1)\4pt]
\cline{2-9}
BP stages&&(8,18,4,2)&(8,32,4,2)&(8,32,4,2)&(8,32,4,2)&(8,32,4,2)&(8,64,4,2)&(8,64,4,2)\4pt]
\hline
&&106&337&779&1221&1230&5819&8811\4pt]
\cline{2-9}
&&421&1332&3101&4871&4879&23205&34026\4pt]
\hline
No. of stage ()&&2&2&4&6&6&7&10\4pt]
\hline
\end{tabular}
\end{table*}

\subsection{Model analysis}
There are six types of DBPN used for model analysis: DBPN-SS, DBPN-S, DBPN-M, DBPN-L, D-DBPN-L, D-DBPN, and DBPN.
The detailed architectures of those networks are shown in Table~\ref{tab:net_arc}.
Other methods, VDSR~\cite{Kim_2016_VDSR}, DRCN~\cite{kim2016deeply}, DRRN~\cite{Tai-DRRN-2017}, LapSRN~\cite{LapSRN}, was chosen due to the same nature in number of parameter.

\label{subsec:modelanalysis}
\begin{figure}[t!]
\centering
\includegraphics[width=8.5cm]{modular_comparison_4x}\vspace{-1.5em}
\caption{The depth analysis of DBPNs compare to other networks (VDSR~\cite{Kim_2016_VDSR}, DRCN~\cite{kim2016deeply}, DRRN~\cite{Tai-DRRN-2017}, LapSRN~\cite{LapSRN}) on Set5 dataset for 4 enlargement.}
\label{figure:modular_comparison_4x}
\end{figure} 

\noindent \textbf{Depth analysis}. To demonstrate the capability of our projection unit, we construct multiple networks: DBPN-S (), DBPN-M (), and DBPN-L (). In the feature extraction, we use  and . Then, we use  for the reconstruction. The input and output image are luminance only. 

The results on  enlargement are shown in~Fig.~\ref{figure:modular_comparison_4x}. 
From the first  iteration, our variants are outperformed VDSR.
Finally, starting from our shallow network, DBPN-S gives the higher PSNR than VDSR, DRCN, and LapSRN. 
DBPN-S uses only 12 convolutional layers with smaller number of filters than VDSR, DRCN, and LapSRN. At the best performance, DBPN-S can achieve  dB which better  dB,  dB,  dB than VDSR, DRCN, and LapSRN, respectively. DBPN-M shows performance improvement which better than all four existing methods (VDSR, DRCN, LapSRN, and DRRN). At the best performance, DBPN-M can achieve  dB which better  dB,  dB,  dB,  dB than VDSR, DRCN, LapSRN, and DRRN respectively. In total, DBPN-M uses 24 convolutional layers which has the same depth as LapSRN. Compare to DRRN (up to 52 convolutional layers), DBPN-M undeniable shows the effectiveness of our projection unit. Finally, DBPN-L outperforms all methods with  dB which better  dB,  dB,  dB,  dB than VDSR, DRCN, LapSRN, and DRRN, respectively.

The results of  enlargement are shown in~Fig.~\ref{figure:modular_comparison_8x}. 
Our networks outperform the existing networks for  enlargement, from the first  iteration, which clearly show the effectiveness of our proposed networks on large scaling factors. 
However, we found that there is no significant performance gain from each proposed network especially for DBPN-L and DBPN-M networks where the difference only  dB. 
\begin{figure}[t!]
\centering
\includegraphics[width=8.5cm]{modular_comparison_8x}\vspace{-1.5em}
\caption{The depth analysis of DBPN on Set5 dataset for 8 enlargement. DBPN-S (), DBPN-M (), and DBPN-L ()}
\label{figure:modular_comparison_8x}
\end{figure} 



\noindent \textbf{Number of parameters}. 
We show the tradeoff between performance
and number of network parameters from our networks and existing deep
network SR in~Fig.~\ref{figure:psnr_vs_param_4x}~and~\ref{figure:psnr_vs_param_8x}. 

For the sake of low computation for real-time processing, we construct DBPN-SS which is the lighter version of DBPN-S, . We use  and . However, the results outperform SRCNN, FSRCNN, and VDSR on both  and  enlargement. Moreover, DBPN-SS performs better than VDSR with  and  fewer parameters on  and  enlargement, respectively. 

DBPN-S has about  fewer parameters and higher PSNR than LapSRN on  enlargement. 
Finally, D-DBPN has about  fewer parameters, and
approximately the same PSNR, compared to EDSR on  enlargement. On the  enlargement, D-DBPN has about  fewer parameters with better PSNR compare to EDSR. This evidence show that our networks has the best trade-off between performance and number of parameter.
\begin{figure}[t!]
\centering
\includegraphics[width=8.5cm]{psnr_vs_param_4x}\vspace{-1.5em}
\caption{Performance vs number of parameters for  enlargement using Set5. The horizontal axis is log-scale.}
\label{figure:psnr_vs_param_4x}

\includegraphics[width=8.5cm]{psnr_vs_param_8x}\vspace{-1.5em}
\caption{Performance vs number of parameters for  enlargement using Set5. The horizontal axis is log-scale.}
\label{figure:psnr_vs_param_8x}
\end{figure} 



\noindent \textbf{Deep concatenation}. Each projection unit is used to
distribute the reconstruction step by constructing features
which represent different details of the HR components. Deep concatenation is also well-related with the number of  (back-projection stage), 
which shows more detailed features generated from the projection units will also increase the quality of the results. In~Fig.~\ref{figure:result_up_projection}, it is shown that each stage successfully generates diverse features to reconstruct SR image.

\begin{figure}[t!]
\centering
\includegraphics[width=8.5cm]{result_up_projection}
\caption{Sample of feature maps from up-projection units in D-DBPN where . Each feature has been enhanced using the same grayscale colormap. Zoom in for better visibility.}
\label{figure:result_up_projection}
\end{figure} 

\noindent \textbf{Error Feedback}.
As stated before, error feedback (EF) is used to guide the reconstruction in the early layer. Here, we analyze how error feedback can help for better reconstruction. 
We conduct experiments to see the effectiveness of error feedback procedure.
On the scenario without EF, we replace up- and down-projection unit with single up- (deconvolution) and down-sampling (convolution) layer.

We show PSNR of DBPN-S with EF and without EF in~Table~\ref{tab:error_feedback}. The result with EF has 0.53 dB and 0.26 dB better than without EF on Set5 and Set14, respectively. In~Fig.~\ref{figure:error_feedback}, we visually show how error feedback can construct better and sharper HR image especially in the white stripe pattern of the wing.

Moreover, the performance of DBPN-S without EF is interestingly 0.57 dB and 0.35 dB better than previous approaches such as SRCNN~\cite{dong2016image} and FSRCNN~\cite{dong2016accelerating}, respectively, on Set5. The results show the effectiveness of iterative up- and downsampling layers to demonstrate the LR-to-HR mutual dependency.

\begin{table}[t!]
\caption{Analysis of EF using DBPN-S on  enlargement. {\color{red}Red} indicates the best performance.}
\centering
\label{tab:error_feedback}
\begin{tabular}{*1l|*1c|*1c}
\hline
& \textbf{Set5} & \textbf{Set14} \\
\hline
SRCNN~\cite{dong2016image}&30.49&27.61\4pt]
\hline
Without EF&31.06&27.95\4pt]
\hline
\end{tabular}
\end{table}

We further analyze the effectiveness of EF comparing the same model size as shown in Table~\ref{tab:bp_analysis} using D-DBPN model. 
The better setting is to remove the subtraction (-) and addition (+) operations in the up-/down-projection unit. The results demonstrate the effectiveness of our EF module.

\begin{table}[t!]
\caption{Analysis of EF module on same model size (D-DBPN) on  enlargement. {\color{red}Red} indicates the best performance.}
\centering
\label{tab:bp_analysis}
\begin{tabular}{*1l|*1c|*1c}
\hline
& \textbf{D-DBPN-w/ EF} & \textbf{D-DBPN-w/o EF} \\
\hline
Set5&{\color{red}32.40/0.897}&32.17/0.894\4pt]
\hline
BSDS100&{\color{red}27.67/0.738}&27.56/0.736\4pt]
\hline
Manga109&{\color{red}30.89/0.913}&30.43/0.908\4pt]
\hline
8&4&2&{\color{red}32.47}&{\color{red}28.82}\4pt]
\hline
\end{tabular}
\end{table}

\noindent \textbf{Luminance vs RGB}
In D-DBPN, we change the input/output from luminance to RGB color channels. 
There is no significant improvement in the quality of the result as shown in~Table~\ref{tab:color}. 
However, for running time efficiency, constructing all channels simultaneously is faster than a separated process.
\begin{table}[t!]
\caption{Analysis of input/output color channel using DBPN-L. {\color{red}Red} indicates the best performance.}
\centering
\label{tab:color}
\begin{tabular}{*1l|*1c|*1c}
\hline
& \textbf{Set5} & \textbf{Set14} \\
\hline
RGB&{\color{red}31.88}&{\color{red}28.47}\4pt]
\hline
\end{tabular}
\end{table}



\subsection{Comparison of each DBPN variant}
\noindent \textbf{Dense connection}. We implement D-DBPN-L which is a dense connection of the  network to show how dense connection can improve the network's performance in all cases as shown in~Table~\ref{tab:dense}. On  enlargement, the dense network, D-DBPN-L, gains  dB and  dB higher than DBPN-L on the Set5 and Set14, respectively. On , the gaps are even larger. The D-DBPN-L has  dB and  dB higher that DBPN-L on the Set5 and Set14, respectively.
\begin{table}[t!]
\small
\caption{Comparison of the DBPN-L and D-DBPN-L on 4 and 8 enlargement. {\color{red}Red} indicates the best performance.}
\centering
\label{tab:dense}
\begin{tabular}{*1l*1c|*2c*2c}
\hline
 & &\multicolumn{2}{c}{Set5} & \multicolumn{2}{c}{Set14} \\         
Algorithm & Scale & PSNR&SSIM & PSNR&SSIM   \\
\hline
DBPN-L&4		&&&&\\
D-DBPN-L&4		&{\color{red}}&{\color{red}}&{\color{red}}&{\color{red}}\\
\hline
DBPN-L&8		&&&&\\
D-DBPN-L&8		&{\color{red}}&{\color{red}}&{\color{red}}&{\color{red}}\\
\hline
\end{tabular}
\end{table}

\noindent \textbf{Comparison across the variants}. We compare six DBPN variants: DBPN-R64-10, DBPN-R128-5, DBPN-MR64-3, DBPN-RES-MR64-3, DBPN-RES, and DBPN. 
First, DBPN, which was the winner of NTIRE2018~\cite{timofte2018ntire} and PIRM2018~\cite{pirm2018}, uses , , and  for the back-projection stages, and dense connection between projection units. In the reconstruction, we use . 
DBPN-R64-10 uses  with 10 iterations to produce 640 HR features as input of reconstruction layer.
DBPN-R128-5, uses  with 5 iterations, produces 640 HR features.
DBPN-MR64-3 has the same architecture with D-DBPN but the projection units are treated as recurrent network.
DBPN-RES-MR64-3 is DBPN-MR64-3 with residual learning. 
Last, DBPN-RES is DBPN with residual learning. 
All variants are trained with the same training setup.

The results are shown in Table~\ref{tab:psnr_variant}. It shows that all variants successfully have better performance than D-DBPN~\cite{haris2018deep}.
DBPN-R64-10 has the least parameter compare to other variants, which is suitable for mobile/real-time application. It can reduce  number of parameter compare to DBPN and maintain to get good performance.
We can see that increasing  can improve the performance of DBPN-R which is shown by DBPN-R128-5 compare to DBPN-R64-10.
However, better results is obtained by DBPN-MR64-3, especially on Urban100 and Manga109 test set compare to other variants.
It is also proven that residual learning can slightly improve the performance of DBPN.
Therefore, it is natural that we performed the combination of multiple stages recurrent and residual learning called DBPN-RES-MR64-3 which performs the best results and has lower parameter than DBPN.

\begin{table*}[t!]
\scriptsize
\caption{Quantitative evaluation of DBPN's variants on 4. {\color{red}Red} indicates the best performance. }
\centering
\label{tab:psnr_variant}
\begin{tabular}{*1l*1r*2c*2c*2c*2c*2c}
\hline
 & &\multicolumn{2}{c}{Set5} & \multicolumn{2}{c}{Set14}& \multicolumn{2}{c}{BSDS100}& \multicolumn{2}{c}{Urban100}&\multicolumn{2}{c}{Manga109} \\         
Method & \# Parameters () &  PSNR&SSIM & PSNR&SSIM & PSNR&SSIM & PSNR&SSIM & PSNR&SSIM\\
\hline
D-DBPN~\cite{haris2018deep} &10426					&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}\\
DBPN &15348					&&&&&&&&&&\\
DBPN-R64-10&{\color{red}1614}				&&&&&&&&&&\\
DBPN-R128-5&6349				&&&&&&&&&&\\
DBPN-MR64-3&10419				&&&&&&&&&&\\
DBPN-RES&15348				&&&&&&&&&&\\
DBPN-RES-MR64-3&10419				&{\color{red}}&{\color{red}}&{\color{red}}&{\color{red}}&{\color{red}}&{\color{red}}&{\color{red}}&{\color{red}}&{\color{red}}&{\color{red}}\\
\hline
\end{tabular}
\end{table*}

\subsection{Comparison with the-state-of-the-arts on SR}
\label{subsec:sota}
\begin{figure*}[t!]
\centering
\includegraphics[width=15cm]{4x_result}
\caption{Qualitative comparison of our models with other works on  super-resolution.}
\label{figure:4x_result}
\end{figure*}

To confirm the ability of the proposed network, we performed several experiments and analysis. We compare our network with 14 state-of-the-art SR algorithms: A+~\cite{timofte2014a+}, SRCNN~\cite{dong2016image}, FSRCNN~\cite{dong2016accelerating}, VDSR~\cite{Kim_2016_VDSR}, DRCN~\cite{kim2016deeply}, DRRN~\cite{Tai-DRRN-2017}, 
LapSRN~\cite{LapSRN}, MS-LapSRN~\cite{lai2018fast}, MSRN~\cite{li2018multi}, D-DBPN~\cite{haris2018deep}, EDSR~\cite{Lim_2017_CVPR_Workshops}, RDN~\cite{zhang2018residual}, RCAN~\cite{zhang2018image}, and SAN~\cite{dai2019second}. We carry out extensive experiments using 5 datasets: Set5~\cite{bevilacqua2012low}, Set14~\cite{zeyde2012single}, BSDS100~\cite{arbelaez2011contour}, Urban100~\cite{huang2015single} and Manga109~\cite{matsui2016sketch}. Each dataset has different characteristics. Set5, Set14 and BSDS100 consist of natural scenes; Urban100 contains urban scenes with details in different frequency bands; and Manga109 is a dataset of Japanese manga. 

Our final network, DBPN-RES-MR64-3, combines dense connection, recurrent network and residual learning to boost the performance of DBPN. It uses , , and  with 3 iteration. In the reconstruction, we use . RGB color channels are used for input and output image. It takes around 14 days to train\footnote{It takes around five days to train on PyTorch 1.0 and CUDA10.}.

PSNR and structural similarity (SSIM)~\cite{wang04}
were used to quantitatively evaluate the proposed method. Note that higher PSNR
and SSIM values indicate better quality. As used by existing networks,
all measurements used only the luminance channel (Y). For SR by factor
, we crop  pixels near image boundary before evaluation as in~\cite{Lim_2017_CVPR_Workshops, dong2016accelerating}. Some of the existing networks such as SRCNN, FSRCNN, VDSR, and EDSR did not perform  enlargement. To this end, we retrained the existing networks by using author's code with the recommended parameters. 

Figure~\ref{figure:4x_result} shows that EDSR tends to generate stronger edge than the ground truth and lead to misleading information in several cases. The result of EDSR shows the eyelashes were interpreted as a stripe pattern. Our result generates softer patterns which is subjectively closer to the ground truth. On the butterfly image, EDSR separates the white pattern and tends to construct regular pattern such ac circle and stripe, while D-DBPN constructs the same pattern as the ground truth. 

We show the quantitative results in the~Table~\ref{tab:psnr}. Our network outperforms the existing methods by a large margin in all scales except RCAN and SAN on . 
For , EDSR has  dB higher than D-DBPN but outperformed by DBPN-RES-MR64-3 with  dB margin on Urban100. Recent state-of-the-art, SAN~\cite{dai2019second} and RCAN~\cite{zhang2018image}, performs better results than our network on . 
However, on , our network has  dB higher than RCAN on Urban100. The biggest gap is shown on Manga109, our network has  dB higher than RCAN.

Our network shows its effectiveness on  enlargement which outperforms all of the existing methods by a large margin. Interesting results are shown on Manga109 dataset where D-DBPN obtains  dB which is  dB better than EDSR. 
While on the Urban100 dataset, D-DBPN achieves 23.25 which is only  dB better than EDSR. 
Our final network, DBPN-RES-MR64-3, outperforms all previous networks. 
DBPN-RES-MR64-3 is roughly  dB better than RCAN~\cite{zhang2018image} across multiple dataset. The biggest gap is on Manga109 where DBPN-RES-MR64-3 is  dB better than RCAN~\cite{zhang2018image}.
The overall results show that our networks perform better on fine-structures images especially manga characters, even though we do not use any animation images in the training.

The results of  enlargement are visually shown in~Fig.~\ref{figure:8x_result}. Qualitatively, our network is able to preserve the HR components better than other networks. 
For image ``img.png'', all of previous methods fail to recover the correct direction of the image textures, while ours produce more faithful results to the ground truth.
For image ``Hamlet.png'', other methods suffer from heavy blurring artifacts and fail to recover the details. While, our network successfully recovers the fined detail and produce the closest result to the ground truth.
It shows that our networks can successfully extract not only features but also create contextual information from the LR input to generate HR components in the case of large scaling factors, such as  enlargement.



\begin{figure*}[t!]
\centering
\includegraphics[width=14.5cm]{8x_result}
\caption{Qualitative comparison of our models with other works on  super-resolution.}
\label{figure:8x_result}
\end{figure*}

\begin{table*}[t!]
\scriptsize
\caption{Quantitative evaluation of state-of-the-art SR algorithms: average PSNR/SSIM for scale factors 2, 4, and 8. {\color{red}Red} indicates the best and {\color{blue}blue} indicates the second best performance. }
\centering
\label{tab:psnr}
\begin{tabular}{*1l*1c*1c*2c*2c*2c*2c*2c}
\hline
 & & &\multicolumn{2}{c}{Set5} & \multicolumn{2}{c}{Set14}& \multicolumn{2}{c}{BSDS100}& \multicolumn{2}{c}{Urban100}&\multicolumn{2}{c}{Manga109} \\         
Method & Scale &\# Parameters () & PSNR&SSIM & PSNR&SSIM & PSNR&SSIM & PSNR&SSIM & PSNR&SSIM  \\
\hline
Bicubic&2								&-&&&&&&&&&&\\
A+~\cite{timofte2014a+}&2				&-&&&&&&&&&&\\
SRCNN~\cite{dong2016image}&2			&0.05M&&&&&&&&&&\\
FSRCNN~\cite{dong2016accelerating}&2		&0.01M&&&&&&&&&&\\
VDSR~\cite{Kim_2016_VDSR}&2			&0.66M&&&&&&&&&&\\
DRCN~\cite{kim2016deeply}&2				&1.77M&&&&&&&&&&\\
DRRN~\cite{Tai-DRRN-2017}&2			&0.30M&&&&&&&&&&\\
LapSRN~\cite{LapSRN}&2				&0.81M&&&&&&&&&&\\
MS-LapSRN~\cite{lai2018fast}&2			&0.22M&&&&&&&&&&\\
MSRN~\cite{li2018multi}&2				&6.3M&&&&&&&&&&\\
D-DBPN~\cite{haris2018deep} &2			&5.82M&&&&&&&&&&\\
RDN~\cite{zhang2018residual}&2			&22.3M&&{\color{blue}}&&{\color{red}}&&{\color{blue}}&&&&{\color{blue}}\\
RCAN~\cite{zhang2018image}&2			&16M&{\color{blue}}&{\color{blue}}&{\color{red}}&{\color{red}}&{\color{blue}}&{\color{red}}&{\color{red}}&{\color{red}}&{\color{red}}&{\color{red}}\\
SAN~\cite{dai2019second}&2				&15.7M&{\color{red}}&{\color{red}}&&{\color{red}}&{\color{red}}&{\color{red}}&{\color{blue}}&{\color{blue}}&{\color{blue}}&{\color{red}}\\
DBPN-RES-MR64-3 &2					&5.82M&&&{\color{blue}}&{\color{red}}&&&&&&\\
\hline
Bicubic&4								&-&&&&&&&&&&\\
A+~\cite{timofte2014a+}&4				&-&&&&&&&&&&\\
SRCNN~\cite{dong2016image}&4			&0.05M&&&&&&&&&&\\
FSRCNN~\cite{dong2016accelerating}&4		&0.01M&&&&&&&&&&\\
VDSR~\cite{Kim_2016_VDSR}&4			&0.66M&&&&&&&&&&\\
DRCN~\cite{kim2016deeply}&4				&1.77M&&&&&&&&&&\\
DRRN~\cite{Tai-DRRN-2017}&4			&0.30M&&&&&&&&&&\\
LapSRN~\cite{LapSRN}&4				&0.81M&&&&&&&&&&\\
MS-LapSRN~\cite{lai2018fast}&4			&0.22M&&&&&&&&&&\\
MSRN~\cite{li2018multi}&4				&6.3M&&&&&&&&&&\\
D-DBPN~\cite{haris2018deep} &4			&10.2M&&&&&&&&&&\\
EDSR~\cite{Lim_2017_CVPR_Workshops}&4	&43.2M&&&&&&&&&&\\
RDN~\cite{zhang2018residual}&4			&22.3M&&{\color{blue}}&&&&&&&&\\
RCAN~\cite{zhang2018image}&4			&16M&&{\color{red}}&&{\color{blue}}&&{\color{blue}}&{\color{blue}}&{\color{blue}}&{\color{blue}}&{\color{blue}}\\
SAN~\cite{dai2019second}&4				&15.7M&{\color{blue}}&{\color{red}}&{\color{blue}}&{\color{blue}}&{\color{blue}}&&&&&{\color{blue}}\\
DBPN-RES-MR64-3 &4					&10.2M&{\color{red}}&{\color{blue}}&{\color{red}}&{\color{red}}&{\color{red}}&{\color{red}}&{\color{red}}&{\color{red}}&{\color{red}}&{\color{red}}\\
\hline
Bicubic&8								&-&&&&&&&&&&\\
A+~\cite{timofte2014a+}&8				&-&&&&&&&&&&\\
SRCNN~\cite{dong2016image}&8			&0.05M&&&&&&&&&&\\
FSRCNN~\cite{dong2016accelerating}&8		&0.01M&&&&&&&&&&\\
VDSR~\cite{Kim_2016_VDSR}&8			&0.66M&&&&&&&&&&\\
LapSRN~\cite{LapSRN}&8				&0.81M&&&&&&&&&&\\
MS-LapSRN~\cite{lai2018fast}&8			&0.22M&&&&&&&&&&\\
MSRN~\cite{li2018multi} &8				&6.3M&&&&&&&&&&\\
D-DBPN~\cite{haris2018deep} &8			&23.1M&&&&&&&&&&\\
EDSR~\cite{Lim_2017_CVPR_Workshops}&8	&43.2M&&&&&&&&&&\\
RCAN~\cite{zhang2018image}&8			&16M&{\color{blue}}&{\color{blue}}&{\color{blue}}&{\color{blue}}&{\color{blue}}&{\color{blue}}&{\color{blue}}&{\color{blue}}&{\color{blue}}&{\color{blue}}\\
SAN~\cite{dai2019second}&8				&15.7M&&&&&&&&&&\\
DBPN-RES-MR64-3 &8					&23.1M&{\color{red}}&{\color{red}}&{\color{red}}&{\color{red}}&{\color{red}}&{\color{red}}&{\color{red}}&{\color{red}}&{\color{red}}&{\color{red}}\\
\hline
\end{tabular}
\end{table*}

\subsection{Runtime Evaluation}
We present the runtime comparisons between our networks and three existing methods: VDSR~\cite{Kim_2016_VDSR}, DRRN~\cite{Tai-DRRN-2017}, and EDSR~\cite{Lim_2017_CVPR_Workshops}. The comparison must be done in fair settings. The runtime is calculated using python function \texttt{timeit} which encapsulating only \texttt{forward} function. For EDSR, we use original author code based on Torch and use \texttt{timer} function to obtain the runtime.

We evaluate each network using NVIDIA TITAN X GPU (12G Memory). The input image size is 64 64, then upscaled into 128 128 (2), 256 256 (4), and 512 512 (8). The results are the average of 10 times trials.

Table~\ref{tab:runtime} shows the runtime comparisons on 2, 4, and 8 enlargement. It shows that DBPN-SS and DBPN-S obtain the best and second best performance on 2, 4, and 8 enlargement. Compare to EDSR, D-DBPN shows its effectiveness by having faster runtime with comparable quality on 2 and 4 enlargement. On 8 enlargement, the gap is bigger. It shows that D-DBPN has better results with lower runtime than EDSR.

Noted that input for VDSR and DRRN is only luminance channel and need preprocessing to create middle-resolution image. So that, the runtime should be added by additional computation of interpolation computation on preprocessing.

\begin{table}[t!]
\caption{Runtime evaluation with input size 6464. {\color{red}Red} indicates the best and {\color{blue}blue} indicates the second best performance, * indicates the calculation using function timer in Torch, and N.A. indicates that the algorithm runs out of GPU memory.}
\centering
\label{tab:runtime}
\begin{tabular}{*1c|*1c|*1c|*1c}
\hline
& \textbf{} & \textbf{}& \textbf{} \\
& (128128) & (256256) & (512512) \\
\hline
VDSR~\cite{Kim_2016_VDSR}&0.022&0.032&0.068\4pt]
\hline
*EDSR~\cite{Lim_2017_CVPR_Workshops}&0.857&1.245&1.147\4pt]
\hline
DBPN-S&{\color{blue}0.013}&{\color{blue}0.020}&{\color{blue}0.038}\4pt]
\hline
DBPN-L&0.035&0.069&0.126\4pt]
\hline
DBPN-RES-MR64-3&0.171&0.227&0.339\
L_{G} = w_1*L_{mse} + w_2*L_{vgg} + w_3*L_{adv} + w_4*L_{style} 

  L_D = -\texttt{log} (D(I^{h})) - \texttt{log}(1-D(G(I^{l}))).

\texttt{Perceptual index} = 1/2((10-Ma)+NIQE).


The three regimes correspond to Region 1: RMSE  , Region 2:
 RMSE , and Region 3:  RMSE . We
select optimal parameter settings for each regime. 
This process yields
\begin{itemize}
\item Region 1 ()
\item Region 2 ()
\item Region 3 ()
\end{itemize}

Our method achieved  place on Region 2,  place on
Region 1, and  place on Region 3~\cite{pirm2018} as shown in Table~\ref{tab:results_perceptual}. In Region 3, it shows very competitive results where we got , however, it is noted that our method has the lowest RMSE among other top 5 performers which means the image has less distortion or hallucination w.r.t the original image.


\begin{figure*}[t!]
\centering
\includegraphics[width=16cm]{perceptual_x4}
\caption{Results of DBPN with perceptual loss compare with other methods.}
\label{figure:perceptual_img}
\end{figure*}

We show qualitative results from our method which is shown in Fig.~\ref{figure:perceptual_img}. 
It can be seen that there are significant improvement on high quality texture on each region compare to MSE-optimized SR image.
ESRGAN~\cite{wang2018esrgan}, the winner of PIRM2019 on Region 3, gets the best perceptual results among other methods.
However, our proposal contains less noise among other methods (the smallest RMSE) while maintaining good perceptual quality on Region 3. \section{Conclusion}
We have proposed Deep Back-Projection Networks for Single Image Super-resolution which is the winner of two single image SR challenge (NTIRE2018 and PIRM2018). Unlike the previous methods which predict the SR image in a feed-forward manner, our proposed networks focus to directly increase the SR features using multiple up- and down-sampling stages and feed the error predictions on each depth in the networks to revise the sampling results, then, accumulates the self-correcting features from each upsampling stage to create SR image. We use error feedbacks from the up- and down-scaling steps to guide the network to achieve a better result. The results show the effectiveness of the proposed network compares to other state-of-the-art methods. Moreover, our proposed network successfully outperforms other state-of-the-art methods on large scaling factors such as  enlargement. We also show that DBPN can be modified into several variants to follow the latest deep learning trends to improve its performance.


%
 








\ifCLASSOPTIONcompsoc
\section*{Acknowledgments}
\else
\section*{Acknowledgment}
\fi
This work was partly supported by JSPS KAKENHI Grant Number 19K12129
and by AFOSR Center of Excellence in Efficient and Robust Machine
Learning, Award FA9550-18-1-0166.


\ifCLASSOPTIONcaptionsoff
  \newpage
\fi







{\small
\bibliographystyle{IEEEtran}
\bibliography{egbib}
}



\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{muhammad_haris}}]{Muhammad Haris}
Muhammad Haris received S. Kom (Bachelor of Computer Science) from the Faculty of Computer Science, University of Indonesia, Depok, Indonesia, in 2009. Then, he received the M. Eng and Dr. Eng degree from Department of Intelligent Interaction Technologies, University of Tsukuba, Japan, in 2014 and 2017, respectively, under the supervision of Dr. Hajime Nobuhara. He worked as postdoctoral fellow in Intelligent Information Media Laboratory, Toyota Technological Institute with Prof. Norimichi Ukita. Currently, he is working as AI Research Manager at BukaLapak.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{greg_shakhnarovich}}]{Greg Shakhnarovich}
Greg Shakhnarovich has been faculty member at TTI-Chicago since
2008. He received his BSc degree in Computer Science and Mathematics from
the Hebrew University in Jerusalem, Israel, in 1994, and a MSc degree
in Computer Science from the Technion, Israel, in 2000. Prior to
joining TTIC Greg was a Postdoctoral Research Associate at Brown
University, collaborating with researchers at the Computer Science
Department and the Brain Sciences program there. Greg's research
interests lie broadly in computer vision and machine learning.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{norimichi_ukita}}]{Norimichi Ukita}
Norimichi Ukita is a professor at
the graduate school of engineering, Toyota Technological
Institute, Japan (TTI-J). He received
the B.E. and M.E. degrees in information engineering
from Okayama University, Japan, in
1996 and 1998, respectively, and the Ph.D degree
in Informatics from Kyoto University, Japan,
in 2001. After working for five years as an
assistant professor at NAIST, he became an
associate professor in 2007 and moved to TTIJ
in 2016. He was a research scientist of Precursory Research for
Embryonic Science and Technology, Japan Science and Technology
Agency (JST), during 2002 - 2006. He was a visiting research scientist
at Carnegie Mellon University during
2007-2009. He currently works also
at the Cybermedia center of Osaka University as a guest professor.
His main research interests are object detection/tracking and human
pose/shape estimation. He is a member of the IEEE.
\end{IEEEbiography}
\vfill











\end{document}
