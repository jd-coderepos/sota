\documentclass[letterpaper,11pt]{article}

\usepackage{makeidx}  

\usepackage{amsthm}
\usepackage[margin=1in,dvips]{geometry}

\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{float}
\usepackage{enumerate}
\usepackage{url}
\usepackage{subfig}
\usepackage[hidelinks]{hyperref}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{observation}[theorem]{Observation}

\newcommand{\tail}[2]{#1_{\overline{[#2]}}}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\inner}[1]{\langle#1\rangle}
\newcommand{\floor}[1]{\left\lfloor#1\right\rfloor}
\newcommand{\tnorm}[2]{\left \lVert#2\right \rVert_{#1}}
\newcommand{\norm}[2]{\lVert#2\rVert_{#1}}
\newcommand{\head}[2]{#1_{[#2]}}
\newcommand{\range}[3]{#1_{\overline{[#2]}\cap[#3]}}
\newcommand{\I}{\mathrm{I}}
\newcommand{\gaplinf}{\mathsf{Gap}\ell_{\infty}}
\newcommand{\gaplinfL}{\mathsf{LooseGap}\ell_{\infty}}
\newcommand{\multiLinf}{\mathsf{Multi}\ell_{\infty}}
\newcommand{\indlinf}{\mathsf{Ind}\ell_{\infty}}
\newcommand{\indlinfL}{\mathsf{LooseInd}\ell_{\infty}}
\newcommand{\IC}{\mathrm{IC}}
\newcommand{\wh}{\widehat}
\newcommand{\cc}{\mathcal{C}}

\newcommand{\PE}{E_p}
\newcommand{\KE}{E_k}



\makeatletter
\newcommand{\define}[3]{\@namedef{thm@#1}{#3}\@namedef{thmtypen@#1}{lemma}\newtheorem{thmtype@#1}[theorem]{#2}\newtheorem*{thmtypealt@#1}{#2~\ref{#1}}}
\newcommand{\declare}[1]{\begin{thmtype@#1} \label{#1}
    \@nameuse{thm@#1}
  \end{thmtype@#1}
}
\newcommand{\state}[1]{\begin{thmtypealt@#1}
    \@nameuse{thm@#1}
  \end{thmtypealt@#1}
}
\makeatother


 
{\makeatletter
 \gdef\xxxmark{\expandafter\ifx\csname @mpargs\endcsname\relax \expandafter\ifx\csname @captype\endcsname\relax \marginpar{xxx}\else
       xxx \fi
   \else
     xxx \fi}
 \gdef\xxx{\@ifnextchar[\xxx@lab\xxx@nolab}
 \long\gdef\xxx@lab[#1]#2{{\bf [\xxxmark #2 ---{\sc #1}]}}
 \long\gdef\xxx@nolab#1{{\bf [\xxxmark #1]}}
\long\gdef\xxx@lab[#1]#2{}\long\gdef\xxx@nolab#1{}}


\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\polylog}{poly\,log}
\DeclareMathOperator*{\mean}{mean}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator*{\median}{median}
\DeclareMathOperator*{\E}{\mathbb{E}}
\DeclareMathOperator*{\Var}{Var}
\DeclareMathOperator*{\Cov}{Cov}
\def\R{\mathbb{R}}
\def\Z{\mathbb{Z}}
\def\F{\mathcal{F}}
\def\Thetat{\widetilde{\Theta}}
\def\eps{\epsilon}

\makeatletter 
\newenvironment{proof*}[1][\proofname]{\par 
\normalfont \partopsep=\z@skip \topsep=\z@skip 
  \trivlist 
  \item[\hskip\labelsep 
        \itshape 
    #1\@addpunct{.}]\ignorespaces 
}{\endtrivlist\@endpefalse 
} 
\makeatother 

\pagestyle{plain}
\begin{document}


\begin{titlepage}
\title{Improved Concentration Bounds for
    Count-Sketch\thanks{This work began when both authors were funded by
      internships at Microsoft Research.  GM received further support
      from Hertz Foundation and National Science Foundation Fellowships,
      and EP received further support from a Simons Fellowship.}}

\date{}
\author{Gregory T. Minton\\MIT \and Eric Price\\MIT}
\maketitle


\begin{abstract}
  We present a refined analysis of the classic Count-Sketch streaming
  heavy hitters algorithm~\cite{CCF02}.  Count-Sketch uses  linear measurements of a vector  to give an estimate
   of .  The standard analysis shows that this estimate
   satisfies , where  is the vector
  containing all but the largest  coordinates of .  Our main
  result is that most of the coordinates of  have
  substantially less error than this upper bound; namely, for any , we show that each coordinate  satisfies

with probability , as long as the hash functions are
fully independent.  This subsumes the previous bound and is optimal
for all .  Using these improved point estimates, we prove a stronger
concentration result for set estimates by first analyzing the covariance
matrix and then using a median-of-median-of-medians argument to
bootstrap the failure probability bounds.  These results also give
improved results for  recovery of exactly -sparse estimates
 when  is drawn from a distribution with suitable decay, such
as a power law or lognormal.

We complement our results with simulations of Count-Sketch on a power
law distribution.  The empirical evidence indicates that our theoretical
bounds give a precise characterization of the algorithm's performance:
the asymptotics are correct and the associated constants are small.

Our proof shows that any symmetric random variable with finite variance
and positive Fourier transform concentrates around  at least as well as a
Gaussian.  This result, which may be of independent interest, gives good
concentration even when the noise does not converge to a Gaussian.
\end{abstract}

\thispagestyle{empty}
\end{titlepage}


\section{Introduction}

The \emph{heavy hitters} problem and the closely related \emph{sparse
  recovery} problem are two of the most fundamental problems in the
field of sketching and streaming
algorithms~\cite{CCF02,CM06,GI10,CH10,M05}.  The goal is to
efficiently identify and estimate the  largest coordinates of an
-dimensional vector using a linear sketch  of , where  has  rows.  The strongest commonly
used formal guarantee for the quality of such an estimate is the
 guarantee: this is a bound for the estimate  recovered from  which is of the form

where  denotes the vector obtained from  by replacing
its largest  coordinates with .

The classic approach for this problem is the Count-Sketch algorithm of
Charikar et al.~\cite{CCF02}, which uses 
measurements and satisfies~\eqref{e:linfl2} with 
probability.  It is simple, practical, and gives the best known
theoretical performance in many settings.  It also pioneered a
technique---hashing with random signs and estimating using
medians---that forms the basis for several subsequent works on sparse
recovery~\cite{GLPS,IPW11,HIKP12a,G12}.

\paragraph{Our result.} We show that, despite the popularity of
Count-Sketch, its performance has not been fully characterized and
understood. Specifically, we prove that the quality of the
approximation  given by Count-Sketch is better than the
standard bound~\eqref{e:linfl2} suggests.  While~\eqref{e:linfl2}
gives a bound on the \emph{worst-case} error of , we prove
that \emph{most} coordinates of  have asymptotically smaller
error than this worst case.

The Count-Sketch of a vector  using  rows of 
columns is defined as follows.  For , we choose hash functions  and .  The sketch is

which consists of  linear measurements.  The estimate  is given by

Setting  and ,~\cite{CCF02}
proves that~\eqref{e:linfl2} holds with  probability.
\iffalse
As per~\cite{CM06,GI10}, the  guarantee can be converted
into an  guarantee: if  contains the largest 
coordinates of , then

\fi



Our main result is the following strengthening of the analysis
in~\cite{CCF02} for the accuracy of the \emph{point estimates}
 resulting from Count-Sketch, assuming the hash functions
are fully random:

\define{thm:main}{Theorem}{Consider the estimate  of  from Count-Sketch using 
  rows and  columns, with fully random hash functions.  For
  any  and each index ,
  
}
\state{thm:main}

The standard analysis~\cite{CCF02} proves this bound in the special
case of ; one then gets~\eqref{e:linfl2} by setting  and applying a union bound.  We show in
Theorem~\ref{thm:lower} that our stronger result is optimal; it gives the
\emph{best possible} failure probability for all 
and all linear sketches.

Theorem~\ref{thm:main} shows that the \emph{average} squared error
of a set is  times the previously known bound, i.e., the bound coming
from \eqref{e:linfl2}.  We extend this in Theorem~\ref{thm:bettersets} to show
concentration when estimating a set of coordinates, so that the total squared error over
the set satisfies our improved bound \emph{with high probability}.

\paragraph{Implications.}
Often, one performs Count-Sketch in order to estimate the largest 
coordinates of .  In this case, the bound~\eqref{e:linfl2} gives an
optimal result for arbitrary vectors ~\cite{PW11} but not
necessarily for common distributions on .  A particularly important
distribution is the \emph{power law} or \emph{Zipfian} distribution,
which is the standard distribution to analyze for sparse
recovery~\cite{CCF02,CM05,CRT06,BCDH}.  Consider again Count-Sketch with  rows.  We show that if  follows the power law
 for some constant , then the error
in estimating the largest  coordinates is  times the
previously known bound with high probability (see Theorem~\ref{thm:l2}
for details).  The same result holds for other common distributions
such as lognormals or exponentials.

Previous work~\cite{P11} combined Count-Sketch with another sketch to
get the same bound as in Theorem~\ref{thm:l2}, but our result here applies
directly to the output of Count-Sketch.  This is important because
Count-Sketch is an algorithm that is used in practice, while chains of
algorithms are less likely to be used---especially because years later
we may discover that the original algorithm performed as well as the
chain!  For example, Google uses Count-Sketch to estimate the largest
 coordinates of  for their ``\texttt{top} table'', a core
language feature of their MapReduce programming language
Sawzall~\cite{PDGQ}.  Because many datasets Google encounters (for
example, the frequency of URLs on the web) are distributed as power
laws or lognormals~\cite{M04,BKM+,CM05}, Theorem~\ref{thm:l2} directly
applies to their setting.

\paragraph{Experiments.}
Finally, we complement our analysis with simulations of Count-Sketch
on a power law distribution.  These show that, unlike previous results,
Theorem~\ref{thm:main} and Theorem~\ref{thm:l2} correctly characterize
the asymptotic performance of point and top- estimates,
respectively.  Furthermore, the constants involved are small:
between  and .  We also find that Count-Sketch has
asymptotically less error than Count-Min, an alternative sketch
algorithm.

\paragraph{Limitations.}
Our analysis requires that the hash functions be fully random.
This is unfortunate because fully random hash functions would
take up more space than the sketch itself, but there are reasons
why this constraint is not too problematic.  One reason is that
Nisan's pseudorandom number generator~\cite{N92} lets us store the
hash
functions with only a  factor increase in space.  Then
if we wish to run Count-Sketch on multiple different vectors, we can reuse
the hash functions.
A second reason is that one expects bounded independence to suffice
as long as the vector  itself has sufficient entropy.  A result of
this form is known~\cite{MV10} when  is drawn at random from
a much larger domain.  For example, if  contains 
random coordinates out of , then~\cite{MV10} implies
near-uniformity with 4-wise independence.


\paragraph{Our Techniques}  
Our basic strategy is to translate the problem of bounding
Count-Sketch error into a problem of proving a strong concentration
result for a certain class of random variables.  This, in
turn, we solve by analyzing the Fourier transform of such variables.

In more detail, the argument proceeds as follows.  The error
 is, by definition, the median over rows of
error terms coming from the different coordinates which hash
to the same column as .  For each row, we separate the error term
into contributions from (i) the largest  coordinates 
and (ii) the remaining coordinates .
The error of type (i) is zero with constant probability, and
we bound the error of type (ii) with our concentration result.
We then get a bound on  by using Chernoff bounds
to conclude that if each of  symmetric random variables has a 
chance of being small, then the median has a  chance of being small.
This proves strong bounds for the error of point estimates;
we then analyze the pairwise dependence of said errors to
conclude a bound on the error of sets.

The concentration result we prove is a bound of the form , where  has variance 1 and is a sum
of independent random variables, each of which is symmetric and zero
with probability at least  (Corollary~\ref{cor:gen}).
Such a bound certainly holds in the limit as
 converges to a Gaussian, but we need it to be true before 
converges. To see why this is subtle, consider the sum of 
independent  variables.  The Berry-Ess\'een theorem
gives our bound for , but the bound is actually
false for  when  is odd.  When  is even,
we can pair up the variables to get  independent
 variables.  These variables are zero with 
probability, so our bound applies for arbitrarily small .
What distinguishes even  from odd ?

The key for our argument is that, for a symmetric random variable 
with at least  probability of being , the Fourier transform
of  is \emph{nonnegative}.
The Fourier transform of the triangle filter 
is also nonnegative.  We use the convolution theorem to translate
the expectation of the triangle filter into an integral in Fourier
space, and then use positivity to note that we can bound that
integral over all Fourier space by the integral over small
frequencies.  This we control directly by using the quadratic Taylor series
approximation to .  Because a lower bound on
the expectation of the triangle filter also gives a lower bound
on , this proves what we want.

The above techniques let us prove Theorem~\ref{thm:main}, which shows
that, for Count-Sketch with  rows and  columns, the
squared error in \emph{point estimates} of individual coordinates 
is exponentially distributed with mean , where 
is the previously known bound.

We generally want to estimate multiple coordinates at a time, though,
so we proceed to bound the average error over \emph{sets} of
coordinate estimates.  It follows easily from Theorem~\ref{thm:main}
that the average error is  in expectation; however, one
might expect to get this error with high probability, since averages
tend to concentrate as the size of the set grows.  Getting strong
concentration is difficult because the errors in different coordinates
are not independent.  To handle this, we resort to the following
approach.  Consider sets of size .  We first show that the error
coming from collisions with small coordinates can be replaced by
independent noise, and then we define a variant of Count-Sketch which
is pairwise independent.  By bounding the difference of regular
Count-Sketch and this pairwise independent variant, we get a bound on the
covariance matrix of the errors for each coordinate in our set.
We then apply Chebyshev's inequality, getting 
error with failure probability 
(Proposition~\ref{prp:sets}).  This bound is
nontrivial but falls well short of the ``high probability'' standard
of  failure probability for arbitrary constant .
Unfortunately, while a more refined bound on the covariance matrix
could improve the exponent, no approach based on Chebyshev's inequality
can prove better than a  failure probability.

However, there is a kludge that gives the  failure
probability we want.  Consider running  Count-Sketches in parallel
and taking the (coordinate-wise) median of the results of each Count-Sketch.
Some analysis shows that this boosts the failure probability from
 to the desired  (Corollary~\ref{c:hack}).
Our goal, though, is to analyze the simple Count-Sketch algorithm that
people actually use instead of this hackish variant.  Notice that the kludge uses the
same set of measurements as Count-Sketch with an  factor more rows,
but then performs recovery by estimating each coordinate as a median
(over chunks) of medians (within chunks), rather than Count-Sketch's direct
medians.  To complete the argument we show, via our ``Median Lemma''
(Lemma~\ref{l:median3}), that taking medians directly cannot be much worse
than computing the median of medians.  Thus true Count-Sketch also satisfies
the desired  bound on the failure probability
(Theorem~\ref{thm:bettersets}); in summary, the weak bound we get from
bounding the covariance matrix bootstraps into a better bound.



\section{Preliminaries}

\paragraph{Notation}
We use  to denote  and  to denote .

In the statement of Theorem \ref{thm:main},
 denotes the vector consisting of all but
the largest  coordinates of .  More generally,
we think of the coordinates of  as being sorted,
.
This is purely a notational convenience, possible because Count-Sketch is
invariant under permutation of coordinates.

Given a real-valued random variable , its
\emph{Fourier transform} is the function

In general  is complex-valued.  However, our random
variables are all symmetric; in this case  is
real-valued and equals .

\section{Concentration Lemmas}

The following is the key lemma for our proof.

\begin{lemma}\label{lemma:gen}
  Let  be a symmetric, real-valued random variable
  with variance , and suppose that its Fourier transform  is nonnegative.
  Then, for , .
\end{lemma}
\begin{proof}
Because  holds for all ,
we have

In particular,  for .
Let  be the triangle filter

and recall the Fourier transform relation

Using this relation and switching the order of integration,

The integrand is nonnegative, so we get a lower bound on 
by integrating only over the interval .  On this
interval we have  and,
because ,

is bounded below by its
value at .  Putting this together, we find that

For  we have .
Now noting that  completes
the proof.
\end{proof}

\begin{corollary} \label{cor:gen}
Let  be independent symmetric random variables such that
 for each .
Set  and .
For , .
\end{corollary}
\begin{proof}
For each , let .  The
Fourier transform of  is
.
Because , this is nonnegative.  Now  is a symmetric random
variable with nonnegative Fourier transform  and
with variance ; applying Lemma \ref{lemma:gen}
to it gives the desired bound.
\end{proof}



Note that Lemma \ref{lemma:gen} is not true without the
positivity assumption; in particular, as we observed in the
introduction, Corollary \ref{cor:gen} is not true when
 is small.  Indeed, it seems
intuitive that we get strong concentration
around  as a consequence of the large probability of each individual
variable being .  We also remark that there are analogs of Lemma
\ref{lemma:gen} and Corollary \ref{cor:gen} using only first moment
bounds.  The proof is nearly identical, so we omit it.

We also need the following lemma for concentration of medians.

\begin{lemma}\label{lemma:medians}
  Suppose  are independent symmetric random variables such
  that, for some , we have
   for all .  Then
  
\end{lemma}
\begin{proof}
  Let  denote the indicator for the event that .
  Because  is symmetric we have
  , so .
  The  are independent, so by a Chernoff bound we have that
  
  The same bound applies to the event that at least  of the 
  are less than , and if neither event occurs then the median is in
  the interval .
\end{proof}

\section{Count-Sketch}

\declare{thm:main}
\begin{proof}
  Fix .  For each row  and coordinate , define
  
  For each row , define
  
  Then, by definition,
  
  Each random variable  is symmetric, equals  with
  probability , and otherwise equals .
  Moreover, for each row , the random variables  are independent.
  Thus , so
  Corollary~\ref{cor:gen} shows that
  
  for all .
  Furthermore,  with probability at least ,
  i.e., with constant probability.
  Since  is independent of , this means that
  
  Therefore Lemma~\ref{lemma:medians} implies
  
  Setting  yields the desired result.
\end{proof}

\section{Concentration for Sets} \label{sec:sets}

Theorem~\ref{thm:main} shows that each individual error
 has a constant chance of being less than 
times the  bound.  One would reasonably suspect that
the average error over large sets would satisfy this bound with high
probability.  This is in fact true.  The following result is proven in
Appendix~\ref{app:sets}.



\define{prp:sets}{Proposition}{Fix a constant  and consider the estimate  of  from Count-Sketch
  using  rows and  columns, , for sufficiently large
  (depending on ) constant .  For any set  with ,
  
}
\declare{prp:sets}

The analysis leading to Proposition~\ref{prp:sets} is excessively lossy but, as we will see
presently, we can improve the resulting bound after the fact so that
the loss is only temporary.

\section{Improving the Probability Bound}
\label{sec:mediancubed}

To get a better bound on the failure probability than
Proposition~\ref{prp:sets}, we first consider the procedure of
running Count-Sketch a constant number of times in parallel
and taking the median of the resulting estimates.  Using
this procedure lets us improve the exponent in the failure
probability to any desired constant.

\begin{lemma} \label{l:medians}
Let  be vectors in  and let
 be the coordinate-wise median of
.  If at least a  fraction
of the variables  satisfy ,
then .
\end{lemma}
\begin{proof}
Choose  indices  satisfying ;
call these indices ``good''.  Fix a coordinate .
For at least  indices  we have 
and for at least  we have ;
thus (using the first group if  and the
second group if ) for at least  indices
 we have .  Of these, at least
 must also be good.  Hence

Summing over the coordinates  gives
i.
\end{proof}

We remark in passing that there is a generalization of
Lemma~\ref{l:medians} in which one replaces Euclidean
balls with convex, coordinate-wise symmetric sets.

\begin{lemma} \label{l:hack}
Suppose  are independent random variables
taking values in .  Let  be the random variable
obtained by taking the coordinate-wise median of
.  If 
for each , then .
\end{lemma}
\begin{proof}
  Let  denote the event that .  The
  probability that at least  of the  occur is at most
  .  Thus,
  with probability at least , at least a  fraction
  of the variables  satisfy .  When this holds
  we have  by Lemma~\ref{l:medians}.
\end{proof}

\begin{corollary} \label{c:hack} Fix a real constant 
  and a positive integer  and consider the
  estimate  of  coming from running  instances of
  Count-Sketch in parallel, each using  rows and  columns
  (for sufficiently large --- depending  and  --- constant ), and
  then taking the coordinate-wise median of the  resulting
  estimates.  Suppose .  For any set  with ,
  
\end{corollary}
\begin{proof}
  For  let  denote the estimate from
  the th instance of Count-Sketch.  Using Proposition~\ref{prp:sets}
  we can choose  such that
  
   We now get the desired result by applying Lemma \ref{l:hack} to
   the random variables .
\end{proof}

We now conclude the section by showing that the bound in
Corollary~\ref{c:hack} applies to Count-Sketch itself.
The key is the following combinatorial observation, which
can be summarized as ``the median of the median-of-medians is the median!''

\begin{lemma}[Median] \label{l:median3} Let
   be a list of  real numbers with 
  odd.  Consider the set  of all partitions  of  into blocks of size .  Then
  

\end{lemma}
\begin{proof}
  As medians depends only on the relative orderings, without loss of
  generality we may assume that the set  is symmetric about
   (e.g., take ).  Both sides of the
  desired equality are invariant under permutation of coordinates;
  hence they are both invariant under negation of the elements 
  and so are both zero.
\end{proof}

\begin{theorem} \label{thm:bettersets} Fix a constant , and
  consider the estimate  of  from Count-Sketch using 
  rows and  columns, for sufficiently large (depending on )
  constant .  Suppose .  For any set 
  with ,
  
\end{theorem}
\begin{proof}
  Let  be a partition of  into  blocks of
  size  and let  denote the estimate obtained by
  running Count-Sketch separately on each block and then taking the
  median of the results (as in Corollary~\ref{c:hack}).  Define

  and let  be the indicator for the event
  .
  Define

 By Corollary~\ref{c:hack}, we can choose the constant  so that
  .

  This holds for any partition .  Letting  denote
  the number of such partitions, we have ,
  and so  by Markov's
  inequality.  Suppose now (as happens with at least  probability)
  that .  Then, letting  be the 
  coordinate-wise median of  over all partitions ,
  we have  by
  Lemma~\ref{l:medians}.  But
   by the Median Lemma (Lemma~\ref{l:median3}).
  Putting this together, we have 
  with probability at least , which is exactly what we wanted.
\end{proof}

\section{Concentration for Compressible Signals}\label{sec:powerlaw}

One key application of Count-Sketch is to compute a table estimating
the largest  coordinates of ~\cite{PDGQ}.  Some questions arise
about the proper metric for evaluating such estimates.  For continuous
distributions, distinguishing the th and st largest
coordinates is both difficult and not very important.  We choose to
measure the ``distance to validity,'' meaning the distance from  to
the nearest  which has the same top  coordinates as .
That is, if  denotes the restriction of  to its  largest
components, then we denote the ``top- estimation error'' of
 by

The basic  guarantee~\eqref{e:linfl2} gives that,
with  and , Count-Sketch satisfies .  By~\cite{PW11}, this is
optimal on worst-case inputs .

However, real-world signals are not worst-case.  In fact, signals are
likely to be well approximated by power law or lognormal
distributions~\cite{M04,BKM+,CM05}, and sparsity is mainly useful
because such signals are, in fact, sparse~\cite{CRT06,BCDH}.

In this section we consider recovery of signals with suitable decay:
that is, signals where .  This condition is satisfied by any
power law distribution  with , which
is the range of  for which the distribution is sparse (in
); the condition is also satisfied by lognormal distributions
in the range for which they are sparse.

We show that, for such signals,  with high probability.  This gives
a factor of  improvement over the standard result.  The idea is that
while Theorem~\ref{thm:bettersets} only applies to fixed sets of
indices, on such distributions the largest  coordinates of 
will, with high probability, be among the largest  coordinates of
.  Hence we can apply Theorem~\ref{thm:bettersets} to that fixed
set of  coordinates.

\define{thm:l2}{Theorem}{Suppose  and fix a constant .  Let
   be the result of Count-Sketch using  rows
  and  columns, with fully random hash functions and
  constant factors depending on .  Define  as in~\eqref{e:KE}.  Then
  
  with  probability.
}
\declare{thm:l2}
\begin{proof}
  Let the number of columns be  for some constant .  By the
  standard Count-Sketch bound we have with 
  probability that .  Then for sufficiently large ,
  
  for all  and .

  Let  equal  over  and  over .
  Then by~\eqref{e:12} the top  coordinates of  and of 
  both lie among ; since  on this region, the top
   coordinates of the two are equal.  Hence
  
  But by Theorem~\ref{thm:bettersets},
  
  with probability at least .  Setting  large
  enough gives the result.
\end{proof}

\section{Lower Bound on Point Queries}\label{sec:lower}

The following is an application of the proof technique
of~\cite{PW11}, using Gaussian channel capacity to bound the number of
measurements required for a given error tolerance.

\define{thm:lower}{Theorem}{
  For any  and any distribution on  linear
  measurements of , there is some vector  and
  index  for which the estimate  of  satisfies
  
}
\declare{thm:lower}
\begin{proof}
  Suppose without loss of generality that  (by ignoring
  indices outside ) and that  is larger than some constant.
  Partition  into  blocks of size .  Set ,
  where  has a single random  in each
  block (so it is -sparse) and  for
  some constant  is i.i.d.\ Gaussian.

  Suppose that, in expectation over , 
  allows recovering  from  with
  
  for more than a  fraction of the coordinates .
  We will show that such an  must have  rows.  Yao's minimax principle then gives a lower bound
  for distributions on .  With this, the inability to increase  and 
  while preserving the number of rows gives the desired lower
  bound on failure probability.

  First, we show that .  Let  be the event
  that \eqref{e:goodestimate} holds for more than a 
  fraction of coordinates  and that .   holds with probability
   probability over .  Conditioned
  on , we have
  
  for a  fraction of the coordinates .  Thus, for
  , if we round  to the nearest integer we
  recover  with  in a  fraction of the
  coordinates; hence  over at least  of the blocks.
  We know that  has  bits of entropy in each block.  This
  means, conditioned on ,
  
  and hence 
  by the data processing inequality.  But since ,
  
  Second, we show that .  For each row
  ,  for .  We also have .  Hence  is an additive white Gaussian noise
  channel with signal-to-noise ratio
  
  By the Shannon-Hartley Theorem, this channel has capacity
  
  and thus, by linearity and independence of  (as in~\cite{PW11}),
  
  Combining~\eqref{e:Ilower} and \eqref{e:Iupper} gives .
\end{proof}

\section{Simulation}\label{sec:simul}

Theorems~\ref{thm:main} and~\ref{thm:l2} give asymptotic upper bounds
on the error of Count-Sketch estimates.  Theorem~\ref{thm:lower}
shows that there exists a distribution on inputs for which
Theorem~\ref{thm:main} gives the correct asymptotics.  However, this
does not show that the asymptotics are correct on common input
distributions, or that these asymptotics appear at practical input
sizes.

To address these questions, in this section we discuss empirical results demonstrating
that, on the most common model of input distributions,
\begin{itemize}
\item Theorems~\ref{thm:main} and~\ref{thm:l2} give the right asymptotics;
\item the constants involved are small; and
\item the estimates are better than those of Count-Min, an alternative
  estimation algorithm.
\end{itemize}

\subsection{Simulation Details}
We draw  from the Pareto (Type I) distribution with
parameter , chosen because Pareto distributions are common in large data sets and  is typical~\cite{CSN,M04}.
This distribution is given by

independently for each , where the scaling parameter

is chosen so that .  (Note that, for  and large , the
error in the approximation  is less than .)

We then perform Count-Sketch with  rows and  columns, for
various  and , to get estimates  of .  We will
analyze the distributions of point error and top- estimation error,
as distributions over  and the Count-Sketch.  Point error is
defined as

for a random coordinate .  For top- estimation error , we use the
definition~\eqref{e:KE} from \textsection\ref{sec:powerlaw}.

We will study the behavior of  and  for large  as a
function of , , and , in order to empirically verify the
following specific claims.

\begin{itemize}
\item (Theorem~\ref{thm:main}) After removing 
  probability mass, the point estimation error 
  has expectation
  
  and decays like a Gaussian:
  
\captionsetup{margin=1em}
\begin{figure}[H]
  \centering \subfloat[Distribution of  for various .  Note that it is nearly independent of  and
  .]{\label{f:ped}
    \includegraphics[width=0.5\textwidth]{figures/err-density.pdf}
  } \subfloat[Same as~(a), but with Count-Min added for comparison.
  Note that Count-Min has larger error than Count-Sketch.]{
    \label{f:pecm}
    \includegraphics[width=0.5\textwidth]{figures/err-cm.pdf}
  }
  \caption{Histograms of the point error }
  \label{fig:PE}
\end{figure}
\item (Theorem~\ref{thm:l2}) After removing 
  probability mass, the top- estimation error has expectation
  
  Furthermore, as  increases,  concentrates more strongly
  about its mean:
  
\end{itemize}

Our results are presented in the form of a series of figures.



\begin{figure}[H]
  \centering

  \subfloat[Distribution of  for
  multiple .  Once  is large enough, the distribution is fairly
  static.]{
    \includegraphics[width=0.5\textwidth]{figures/c-density.pdf}
  }
  \subfloat[Distribution of  for multiple .
  Once  is large enough, the distribution is fairly static.]{
    \includegraphics[width=0.5\textwidth]{figures/r-density.pdf}
  }\\
  \subfloat[{ as a function of .  Above some
  threshold (depending on ), the mean remains fixed at a constant.}]{
    \includegraphics[width=0.5\textwidth]{figures/c-mean.pdf}
  }
  \subfloat[{ as a function of . Above some
  threshold (depending on ), the mean remains fixed at a constant.}]{
    \includegraphics[width=0.5\textwidth]{figures/r-mean.pdf}
  }
  \caption{Experimental results for the top- error }
  \label{fig:KE}
\end{figure}
\begin{figure}[H]
  \centering

  \subfloat[Distribution of  for varying .
  For small , as  increases the distribution becomes narrower
  and remains roughly in place.  For larger , as  increases the
  distribution instead shifts to the right.]{
    \includegraphics[width=0.5\textwidth]{figures/k-density.pdf}
  }
  \subfloat[The variance of {}, as a
  distribution over .  It appears to be approximated well by .]{
    \includegraphics[width=0.5\textwidth]{figures/k-variance.pdf}
  }
  \caption{Experimental results for the variation of the top- error }
  \label{fig:KEvar}
\end{figure}


Figure~\ref{fig:PE}\subref{f:ped} shows the probability density
function of  for  and many different pairs .  We find that the PDFs all look fairly similar, and match a
Gaussian with constant standard deviation.  For comparison,
Figure~\ref{fig:PE}\subref{f:pecm} shows the equivalent error when
using the Count-Min sketch of Cormode and Muthukrishnan~\cite{CM04}.  We find that
Count-Min gives asymptotically higher error for the estimation of each
coordinate.


We study the distribution of  in Figure~\ref{fig:KE}.
Figures~\ref{fig:KE}(a) and \ref{fig:KE}(b) give the probability
density functions of  for various  and ,
respectively.  In both cases, we find that once  and  reach a
threshold, the distribution remains roughly constant---and has a
constant mean---as  and  increase beyond that
point. Figures~\ref{fig:KE}(c) and \ref{fig:KE}(d) show how 
changes as a function of  and , respectively.  As predicted, we
find that  has constant mean---so  scales
as ---after  and  are sufficiently large.  The
threshold above which  allows some
trade-off between  and .  At  and , we observe
that  is above the
threshold.

In Figure~\ref{fig:KEvar}, we consider how well  concentrates
about its mean as a function of .  In \ref{fig:KEvar}(a), we plot
the PDF of  for various values of .  We
observe that as  increases, the distribution becomes more tightly
distributed about its mean.  However, once  is large enough, our
chosen  is no longer above the threshold for , causing the
distribution of  to shift markedly to the right and
stop becoming more tightly distributed.  In \ref{fig:KEvar}(b), we plot the
variance of , as a function of .  We find that it is , which gives

This is the analog of Theorem~\ref{thm:bettersets}.



\bibliographystyle{alpha}
\bibliography{paper}

\appendix

\section{Proofs for Concentration of Sets} \label{app:sets}

In this section our aim is to prove Proposition~\ref{prp:sets}.
Our approach is to study the pairwise correlations between
errors in coordinates.  We do this in two parts.
We first define a variant of Count-Sketch,
which we call \emph{tail-independent modified (TIM) Count-Sketch}.
In TIM Count-Sketch, the error coming from collisions with small elements
(the ``tail error") is replaced by independent, uniform noise.   We then focus on two
fixed coordinates and define a further variant,
\emph{fully-independent modified (FIM) Count-Sketch}.  In FIM Count-Sketch,
the errors for our two coordinates of interest are fully independent.
We define these variants in such a way that
unmodified, TIM, and FIM Count-Sketch can all be sampled with the
same randomness, so that they may be compared simultaneously.
We then bound the covariance between coordinate errors in
TIM Count-Sketch by bounding the difference between TIM and
FIM, and use the resulting bound on the error of sets in TIM Count-Sketch
to conclude a bound on unmodified Count-Sketch.

As in the statement of Proposition~\ref{prp:sets}, let 
be a subset of indices with ; this is the set on which we
will study concentration.  In the first part of the argument we will work
instead with , the set of interest together with the set of
heavy hitters.  Let  be the number of columns
in our sketch, for sufficiently large constant .
Let  be the number of rows, and let  be
the vector we are sketching.  Finally, define
.

\begin{observation} \label{obs:resamplebigger}
Let  be a symmetric random variable and suppose  are such that, for all ,
.
(For instance, if  is a random variable to which Corollary~\ref{cor:gen} applies
and , then one can check that  suffices.)
Let  be a symmetric random variable which is uniform on
 with probability  and otherwise is .
Then 
for all .  It follows that we can sample
 and  in such a way that they always have the
same sign and satisfy .
\end{observation}

\begin{lemma} \label{lma:maketailbigger}
As in a row of Count-Sketch, randomly assign to each
 a column  and a sign .
For each column  let .
Let  be a subset of  columns.  There exist
i.i.d.\ symmetric random variables  with
the following properties:
(i)  is uniform on  with constant probability
and otherwise is ;
(ii)  and  have the same sign; and
(iii) .
\end{lemma}
\begin{proof}
Let , so that for each  we have
.
Recalling that  for sufficiently large , by
taking  we can ensure .

Consider the following alternative procedure for sampling the random variables :
for each , decide with probability  if , and if so
(1) assign  uniformly to one of the columns  in  and
(2) add to  a symmetric random variable which is  with probability
 and otherwise is .  (The variables  are not computed.)
In other words, for each  we double the probability of ,
but offset that by introducing a  probability that  contributes zero.
It is clear that this is equivalent to the original definition of , so we henceforth work with it.

Now condition on the column assignments .  Having done so,
the variables  are independent.  Moreover, each is a sum of independent,
symmetric random variables which are  with probability .
Thus Corollary \ref{cor:gen} applies.  Let  be the
standard deviation of  and let  be a set of i.i.d.\ random
variables distributed like the variable  in
Observation \ref{obs:resamplebigger}.  Then, by that observation,
we can sample  so that  and  have the same
sign and .

Removing the conditioning on column assignments,
the dependence between columns manifests in the random variables
 for .  Consider
the following procedure for sampling these variances.
\begin{enumerate}
\item Let  be the solution
to the equation .
\item For each , determine preliminary column
assignments by, for each column , deciding independently with probability  if 
is to be placed in column .  These preliminary assignments may have repetitions
and may not assign  to any column.
\item If a coordinate  is assigned to just one column, let that be .
If it is assigned to more than one column, then randomly choose one of those columns
to be .  If it is assigned to no columns, then we set , i.e.,
we effectively just ignore . 
\item Let .
\end{enumerate}
One can easily check that this is a valid way
of sampling.  The probability  is the solution
to the equation ;
since  for  it follows
that .

Let  be the variance that would have
been obtained from this procedure had we omitted
step 3, i.e., had we not corrected double-assignments.
Note that the random variables  are
i.i.d.\ and they satisfy .
In particular, we have .

The random variables  have expected value
.
Thus, for any , by Markov's inequality we have

Applying Observation~\ref{obs:resamplebigger} once more, we find
symmetric random variables  which are uniform on 
with constant probability, which have the same sign as  (and thus
the same sign as ), and which always satisfy .  These random variables have all the
desired properties.

Finally, we note that, as written, the random variables just constructed
depend on , in that  depends on .  However, we can remove this
dependence by simply choosing the largest  over all .  This gives
random variables which satisfy the same bounds but are agnostic about .
\end{proof}

In a moment we will define the tail-independent modified (TIM) Count-Sketch.
The main point of TIM Count-Sketch is to replace the actual contributions of the ``tail'' coordinates
 with the independent, uniform random variables in
Lemma~\ref{lma:maketailbigger}.
There is one additional difference, though:
for later use, we invent a notion of ``ghost coordinates" for TIM Count-Sketch.
We do this for the following reason.  In TIM Count-Sketch, when there is a collision
between two coordinates in , we will assign them each a fixed error
instead of using the tail noise.  Later, when we modify TIM Count-Sketch to achieve
full pairwise independence, it will be convenient to have a larger probability of
using the fixed error than we get from just collisions between coordinates in .  
The right probability for our uses lies somewhere between that which you get from
considering collisions amongst  coordinates and that which you get from
considering collisions amongst  coordinates.  To achieve
this intermediate probability, we fabricate  ``ghosts".
These are dummy coordinates whose only purpose is to (maybe) collide with
coordinates of  to force them to use the fixed error.  To allow us to tune
the probability of collision, each ghost may or may not be ``real", according to
i.i.d.\ Bernoulli random variables.  Thus the probability of a ghost colliding
with a fixed coordinate  is the probability of that ghost being real times
the probability that it is assigned the same column as .

We are now ready to give an actual definition.
Fix a bound  (later we will take ) and,
for each row, compute estimates as follows.
\begin{enumerate}
\item Assign signs  and columns  to the elements of .
\item Choose signs and columns for the elements of 
and, for the columns  occupied by elements of , let
 and  be as in Lemma~\ref{lma:maketailbigger}.
\item Fabricate  ghost coordinates and, for each, decide
independently with probability  if that ghost is real.
Random choose a column for each ghost that is real.
(The probability  will be chosen later.)
\item For each coordinate ,
\begin{enumerate}
\item Let  be the sum of 
over all  with  and .
\item Let  be the sign of 
(which would be the error in unmodified Count-Sketch).
\item If  is in the same column as  for some ,
 (i.e., if the sum defining  is not empty) or
if  is in the same column as a ghost, then return 
as the estimate.
\item Otherwise return 
as the estimate for  in this row.
\end{enumerate}
\end{enumerate}
The final estimate for each coordinate  is
the median of the estimates in each row.  (TIM Count-Sketch
only yields estimates for the coordinates in .)

The tail-independence modification can only worsen errors,
in the following sense.

\begin{observation} \label{obs:medianbigger}
Suppose  and  are sequences
such that, for each ,  and  have the same
sign and satisfy either  or .
Then  and  have the same sign and satisfy
either  or
.
\end{observation}

\define{lma:tics}{Lemma}{Let  be the estimate of  using
TIM Count-Sketch and let  be the estimate using unmodified
Count-Sketch.  For any subset  which is convex
and symmetric in each coordinate,
}
\declare{lma:tics}
\begin{proof}
Note that unmodified Count-Sketch can be run simultaneously
with TIM Count-Sketch, using the same randomness.
Consider a fixed row and a fixed coordinate .
Keeping the notation above, the error  arising from
TIM Count-Sketch is  if there is a
collision or  if not.  The error
 arising from unmodified Count-Sketch is .
Clearly  and  always
have the same sign and satisfy either
 or
.
The final errors in the estimates are medians of these row errors.
Thus, by Observation \ref{obs:medianbigger},

or else the TIM error is at least .
In other words, given this method of sampling, whenever
the error for TIM Count-Sketch is less than  we know
that the error for unmodified Count-Sketch is no bigger
than the error for TIM Count-Sketch.  This clearly proves
what we wanted.
\end{proof}

Now that we have arranged for independence of the tail
contributions, the only remaining dependence arises from collisions
amongst the elements of .  Fix two coordinates ;
we will bound the correlation between the errors in
these two coordinates.  Analogously to our analysis of
 in Lemma~\ref{lma:maketailbigger}, we can highlight
the dependence by first pretending collisions are independent
and then correcting double-collisions.  More precisely,
consider the following alternative mechanism for determining
collisions.
\begin{enumerate}
\item  Let 
denote the inverse of the monotone-increasing function .
\item For starters, declare that  and  do not collide.  (This may change later
in the procedure.)
\item For each element  of  (resp., each ghost)
and each of , independently decide with
probability  (resp., )
if  collides with .  Note that, because these decisions are independent,
there may well be double-collisions at this stage.
\item If any ghost or coordinate in  collides with both
 and , then resample everything according to
the correct distribution, conditioned on  and  colliding.
\end{enumerate}

Using this procedure, the event that  and  end up not colliding is
the same as the event that step 3 produced no double-collisions.  The
probability of this is

This is a monotone-decreasing function of .  Noting that
, we see that

and

By taking  to be a suitably large multiple of  we can
arrange for  to be at least .
By a simple calculus exercise, .
Thus there is a unique  such that
.  We now and henceforth set  to
this value of .

This is supposed to be an alternative, but equivalent,
method for determining collisions.  Before continuing, let
us check that it is indeed equivalent.  Our choice of
 guaranteed that the new mechanism has the
right probability of  and  colliding; moreover,
when they do collide, we explicitly sample according to
the correct distribution.  Thus, to demonstrate equivalence,
we need only to consider the case when  and  do not
collide.  Condition on this event and consider the
(conditional) probability of some other coordinate
 colliding
with .  Using the original sampling mechanism, this probability
is .  Using our alternative sampling mechanism, the probability is

The event that there is no double-collision is the intersection
of independent events for each element of 
and for each ghost.  Of these constituent events, only one is relevant
to the conditional probability we want to compute: the event that 
does not double-collide.  Thus our probability is

which is what we wanted.
One can similarly check that the probability of a ghost
collision is correct.  Thus, as claimed, our new scheme is
a valid way to sample the collision events.

We can now define our last variant of Count-Sketch,
fully-independent modified (FIM) Count-Sketch.  This only produces estimates for the two
coordinates  and .   For each row, the FIM
estimate is computed as follows.  We always specify
that  and  will not collide.
To determine which elements of 
and which ghosts collide with  and , we use
(1--3) of the ``alternative mechanism'' above.  We
omit step 4, so that a given coordinate may collide
with both  and .  Then, for each 
and for each coordinate colliding with , we choose a
random sign.  (In particular, if some coordinate is supposed to
collide with both  and , then it is associated with
two different, independent random signs.)  Using
these collision and sign data, we proceed
as in (4a--4d) of the description of TIM
Count-Sketch to get a row estimate.  As always, the final estimate is the median
of the row estimates.

In each row, the estimates for  and  produced
by FIM Count-Sketch are independent; thus the final
estimates are also independent.

FIM and, to a lesser extent, TIM Count-Sketch are quite a bit different
from unmodified Count-Sketch.
However, for a single coordinate they preserve many of the salient features.
In particular, all of the properties used in the proof of Theorem~\ref{thm:main}
still hold: rows are independent, the errors in each row are symmetric, and
in each row we have (i) with constant probability, there is no collision with
 and (ii) the error arising from collisions with  satisfies
the bound of Corollary~\ref{cor:gen}.  Thus, with the same proof as Theorem~\ref{thm:main},
we have the following bounds.

\begin{lemma} \label{lma:fimbound}
Fix  and consider the estimates 
and  of  from
TIM and FIM Count-Sketch, respectively.  For any  we have
 
\end{lemma}

\define{cor:timfimvariance}{Corollary}{Suppose .  Then

\centering}
\declare{cor:timfimvariance}

We can recover TIM Count-Sketch from FIM Count-Sketch by
resampling some of the rows.  More specifically, we
take any row in which some ghost or some element of 
collides with both  and  and resample that
row, conditioning on  and  colliding with
each other.  The errors for both  and  in
any such row are  in magnitude both before and
after resampling.  Moreover, for a fixed coordinate
 the signs of the errors before and
after resampling are independent and uniform.  (Note:
the signs of the errors for  and  after resampling
are not necessarily independent of each other.  We assume nothing about
their dependence in our argument.)

Let  be the estimate computed by
FIM Count-Sketch and let  be the
result of TIM Count-Sketch, recovered from FIM
Count-Sketch as above.  Let  be the number of
rows that have to be resampled to recover TIM Count-Sketch from FIM Count-Sketch.
Focusing for the moment on , define the errors

We expect  to be reasonably small, and thus we expect the change in moving from
FIM to TIM to be small.  In particular, we expect  and 
to be close.  More specifically, we shall prove the following proposition.

\define{prp:fimtim}{Proposition}{Suppose .
Then .}
\declare{prp:fimtim}

Since we just want to bound the expected value of a bounded random variable, small
probability events can be ignored.  Indeed, in general, if  is a
random variable bounded by  and  is an event with probability , then

Thus if we aim to prove  and we know , then it suffices to prove
.  Specializing to our case, because
 always holds, this demonstrates
that in order to prove Proposition~\ref{prp:fimtim}, we may condition away
from events that occur with probability .  We will refer to such
events as ``ignorable" and, as the name indicates, freely ignore them.

\begin{lemma} \label{lma:fimtimlargek}
If  then Proposition~\ref{prp:fimtim} holds.
\end{lemma}
\begin{proof}
The probability that a single row is not resampled is , and so

Because  and , .
Thus  is an ignorable event.  But of course if  then
.  This gives the desired bound.
\end{proof}

\begin{observation} \label{obs:whatisignorable}
Let  be arbitrary.  Any event that occurs with probability
 is ignorable.  Moreover, if we suppose that
, then any event that occurs with probability 
is ignorable.
\end{observation}

\begin{lemma} \label{lma:fimtimsmallk1}
Suppose .  Then, except for ignorable events, .
\end{lemma}
\begin{proof}
Each row is resampled with probability , and the resampling events for the  rows
are independent.  Thus, by the Chernoff bound 
(applied with , which in particular can be taken to be arbitrarily large),

This is ignorable by Observation~\ref{obs:whatisignorable}.
\end{proof}

Let  be the number of rows resampled
in which the error increases from  to , stays the same, and decreases with  to ,
respectively.  Thus .
The net effect of resampling is measured by .

\begin{lemma} \label{lma:binomial}
Fix a positive integer .  If
 is an  binomial random variable with , then
 (where the implied constant depends on ).
\end{lemma}
\begin{proof}
Let  be a random variable with
 and , and let
 where
 are i.i.d.\ with the same distribution as .  Then

where the coefficient  counts the number of ways to choose
an -tuple of elements of an -element set such that the most common element
occurs  times, the next most common element occurs  times, and so on.

We have , so the terms with  (for any ) all vanish.  Since ,
this leaves just the terms with .  Now for any , we have

and so


Now we can compute  by first choosing which  of the  elements occur
and then choosing how to arrange them.  The number of choices for the former is clearly
bounded by  and the number of choices for the latter is bounded by a function of  only.
Thus .  Moreover, the number of terms in the
second summation is bounded by a function of  only.  This leaves us with

Given that , the last term in the summation is dominant, giving
the desired bound.
\end{proof}

\begin{lemma} \label{lma:fimtimsmallk2}
Suppose .  Conditioning away from ignorable events, .
\end{lemma}
\begin{proof}
By reducing , we may assume without loss of generality that
.  Thus  is just the value of a discrete random walk
of length .  By Lemma~\ref{lma:binomial} the fourth
moment of such a random walk is .  Combining
this with Lemma~\ref{lma:fimtimsmallk1} gives the desired bound.
\end{proof}

\begin{lemma} \label{lma:fimtimsmallk3}
Suppose  and .  Conditioning away from ignorable events
and then conditioning on the value of ,
.
\end{lemma}
\begin{proof*}
First, suppose .  In this case ,
so the desired bound holds because both sides are .  Thus we may suppose ;
moreover, by symmetry we may assume .  In this case we have
.

By Lemma~\ref{lma:fimbound} we see that
; thus,
by Observation~\ref{obs:whatisignorable}, the corresponding
event is ignorable.  For the remainder of the proof, assume that it does not happen.

Condition for a moment on both the value of 
and on the set of  rows in which the error for FIM Count-Sketch
is above the median.  Pick one such row  and consider its error .
Before the conditioning, using the assumption ,
the distribution for  consisted of atoms at  and
 probability of being uniform in .  The net effect
of our conditioning is to simply condition on .
(By conditioning first on  having above-median error, we removed
the nontrivial dependence between  and .)
In particular, with  probability, .
(And, when this occurs,  is uniform in that interval.)
Applying a Chernoff bound to the 
such rows, we see that, with probability , there are
 rows in which the error lies in .
Since the failure probability is ignorable, we henceforth
assume that this holds.

(Note that the ignorable events we just conditioned away 
influence .  We conditioned on them first to remove that dependence.)

Fix  and consider the event .
Since the difference between TIM and FIM Count-Sketch is just replacing 
rows with error  by rows with error , this event is equivalent to FIM Count-Sketch
having fewer than  rows in which the error is in the interval
.
Given the above discussion, there are  rows in which the probability of
the error lying in that interval is .  More explicitly, for some
constants  and , there are  rows in which the probability of the error lying
in  is (at least) .  Let  be the total number of such rows.
Then  is a  binomial random variable.  For 
(i.e., for all  larger than a sufficiently large constant), we have .
In particular this implies , whence the th moment of
 is  by Lemma~\ref{lma:binomial}.  It also implies that
.
Thus, by Markov's inequality, \`a la Chebyshev's inequality,

For, say, , this is asymptotically .

Using integration by parts,

Writing

the first term is  because  is a constant.
The second is , which is
also .  Thus, as desired,

\end{proof*}

We are now ready to complete the

\begin{proof}[Proof of Proposition~\ref{prp:fimtim}]
After Lemma~\ref{lma:fimtimlargek} it only remains to handle the case .
Combining Lemmas \ref{lma:fimtimsmallk2} and \ref{lma:fimtimsmallk3}, we have
.  Applying
Lemma~\ref{lma:fimbound} and the union bound, we see that
.  Thus,

which gives the desired bound a fortiori.
\end{proof}

Using these lemmata, we can finally prove the desired result.

\state{prp:sets}
\begin{proof}
Throughout we condition on the error for each coordinate in 
being less than .  By Theorem~\ref{thm:main} and a union bound,
this occurs with probability ,
so this conditioning can be absorbed into the final bound.

In addition to unmodified Count-Sketch, consider running TIM Count-Sketch
with .  Define the convex, coordinate-wise symmetric set

Applying Lemma~\ref{lma:tics} with this set  shows that
unmodified Count-Sketch has at least as high of a probability
of its error lying in  as TIM Count-Sketch.  Thus it suffices
to prove the asserted probability bound
for the TIM estimate .

The error we are studying is ,
where .  By Corollary~\ref{cor:timfimvariance},
 for some constant .  Now, by Chebyshev's inequality, we have 

Thus we need to bound

For each  we have 
by Corollary~\ref{cor:timfimvariance}.  The covariance term is the harder part to control.
Fix two coordinates  and consider FIM Count-Sketch with
respect to those two coordinates.  For shorthand write  for 
() and define .  Then

The first term vanishes because, by construction,  and 
are independent.  We shall bound the remaining terms using the Cauchy-Schwarz inequality
.  

By Corollary~\ref{cor:timfimvariance} we have
.
By Proposition~\ref{prp:fimtim}, .
Thus

Substituting back into (\ref{eq:prpsets3}),

And finally, substituting this into (\ref{eq:prpsets2}), we get the desired bound.
\end{proof}

\end{document}
