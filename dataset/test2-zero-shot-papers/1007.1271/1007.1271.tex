\documentclass[11pt]{article}

\usepackage{amsmath,amssymb,epsfig,theorem,fullpage,boxedminipage}
\usepackage{url}
\usepackage{setspace}
\usepackage[ruled, noend]{algorithm2e}
\usepackage{appendix}
\usepackage[T1]{fontenc}

\setlength{\textheight}{9.75in}
\setlength{\textwidth}{6.5in}
\setlength{\abovecaptionskip}{0pt} 





\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{claim}{Claim}
\newtheorem{conjecture}{Conjecture}
\newtheorem{observation}{Observation}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{example}{Example}
\newcommand{\qed}{}
\newcommand{\rank}{\sigma}
\newcommand{\gain}{gain}
\newcommand{\MSVV}{MSVV}
\newcommand{\loss}{loss}
\newcommand{\eat}[1]{}
\newcommand{\greedy}{{\sc Greedy}}
\newcommand{\ranking}{{\sc Ranking}}
\newcommand{\pgreedy}{{\sc Perturbed-Greedy}}
\newenvironment{proof}{\noindent{\em Proof:}}{\hfill \qed \medskip}


\newcommand{\gnote}[1]{\marginpar{{\tiny { [Gagan: {#1} ] }}}}








\newenvironment{packed_enum}{
\vspace{-0.5ex}
\begin{enumerate}
  \setlength{\itemsep}{1pt}
}
{\end{enumerate}\vspace{-1ex}}


\newenvironment{packed_item}{
\vspace{-0.5ex}
\begin{itemize}
  \setlength{\itemsep}{1pt}
}
{\end{itemize}\vspace{-1ex}}



\newcommand{\score}{{\mbox score}}



\title{Online Vertex-Weighted Bipartite Matching and\\ Single-bid Budgeted Allocations}

\author{Gagan Aggarwal\thanks{Google Inc., Mountain View. Email: \texttt{gagana@google.com}} \and Gagan Goel\thanks{Georgia Institute of Technology. Email: \texttt{gagang@cc.gatech.edu}} \and Chinmay Karande\thanks{Georgia Institute of Technology. Email: \texttt{ckarande@cc.gatech.edu}. Work done while visiting Google.} \and Aranyak Mehta\thanks{Google Inc., Mountain View. Email:  \texttt{aranyak@google.com}}}

\date{}

\begin{document}
\maketitle
\thispagestyle{empty}
\begin{abstract}
We study the following vertex-weighted online bipartite matching
problem:  is a bipartite graph. The vertices in  have
weights and are known ahead of time, while the vertices in  arrive
online in an arbitrary order and have to be matched upon arrival. The
goal is to maximize the sum of weights of the matched vertices in
. When all the weights are equal, this reduces to the classic
\emph{online bipartite matching} problem for which Karp, Vazirani and
Vazirani gave an optimal -competitive
algorithm in their seminal work~\cite{KVV90}.

Our main result is an optimal -competitive
randomized algorithm for general vertex weights. We use \emph{random
  perturbations} of weights by appropriately chosen multiplicative
factors. Our solution constitutes the first known generalization of
the algorithm in~\cite{KVV90} in this model and
provides new insights into the role of randomization in online
allocation problems. It also effectively solves the problem of
\emph{online budgeted allocations} \cite{MSVV05} in the case when an
agent makes the same bid for any desired item, even if the bid is
comparable to his budget - complementing the results of \cite{MSVV05,
  BJN07} which apply when the bids are much smaller than the budgets.
\end{abstract}

\newpage
\setcounter{page}{1}

\section{Introduction}
\label{section:intro}
Online bipartite matching is a fundamental problem with numerous
applications such as matching candidates to jobs, ads to advertisers,
or boys to girls. A canonical result in online bipartite matching is
due to Karp, Vazirani and Vazirani~\cite{KVV90}, who gave an optimal
online algorithm for the unweighted case to maximize the {\em size} of
the matching. In their model, we are given a
bipartite graph . The vertices in  are known ahead of
time, while the vertices in  arrive one at a time online in an
arbitrary order. When a vertex in  arrives, the edges incident to
it are revealed and it can be matched to a neighboring vertex in 
that has not already been matched. A match once made cannot be
revoked. The goal is to maximize the number of matched vertices. 

However, in many real world scenarios, the value received from
matching a vertex might be different for different vertices: (1)
Advertisers in online display ad-campaigns are willing to pay a fixed
amount every time their graphic ad is shown on a website. By
specifying their targeting criteria, they can choose the set of
websites they are interested in. Each impression of an ad can be
thought of as matching the impression to the advertiser, collecting
revenue equal to the advertiser's bid. (2) Consider the sale of an
inventory of items such as cars. Buyers arrive in an online manner
looking to purchase one out of a specified set of items they are
interested in. The sale of an item generates revenue equal to the
price of the item. The goal in both these cases is to maximize the
total revenue.  With this background, we consider the following
problem:\\

\noindent\textsc{\textbf{Online vertex-weighted bipartite matching:}} The input
instance is a bipartite graph\\ , with
the vertices in  and their weights  known ahead of
time. Vertices in  arrive one at a time, online, revealing their
incident edges. An arriving vertex can be matched to an unmatched
neighbor upon arrival. Matches once made cannot be revoked later and a vertex
left unmatched upon arrival cannot be matched later. The goal is to
maximize the sum of the weights of the matched vertices in .\\

\noindent {\bf Connection to the online budgeted allocation problem:}
Apart from being a natural generalization of the online bipartite
matching problem, our vertex-weighted matching problem is closely
related to an important class of online problems. Mehta \textit{et al}
\cite{MSVV05} considered the following online version of maximum
budgeted allocation problem \cite{GKP01, LLN01} to model sponsored
search auctions: We have  agents and  items. Each agent 
specifies a monetary budget  and a bid  for each item
. Items arrive online, and must be immediately allocated to an
agent. If a set  of items is allocated to agent , then the agent
pays the minimum of  and . The objective is
to maximize the total revenue of the algorithm. An important and
unsolved restricted case of this problem is when all the non-zero bids
of an agent are equal, \textit{i.e.}  or 0 for all
. This case reduces to our vertex-weighted matching problem (For a
proof, refer to Appendix \ref{app7}).

For the general online budgeted allocation problem, no factor better
than  (achieved by a simple deterministic greedy
algorithm \cite{LLN01}) is yet known. The best known lower bound
stands at  due to the hardness result in \cite{KVV90}
for the case when all bids and budgets are equal to 1 - which is
equivalent to the unweighted online matching problem. The \emph{small
  bids} case - where  for all  and  - was solved
by \cite{MSVV05, BJN07} achieving the optimal 
deterministic competitive ratio. It was believed that handling
\emph{large bids} requires the use of randomization, as in
\cite{KVV90}. In particular, many attempts \cite{KV07, BM08, GM08} had
been made to simplify the analysis of the randomized algorithm in
\cite{KVV90}, but no generalization had been achieved.

Our solution to the vertex-weighted matching problem is a significant
step in this direction. Our algorithm generalizes that of \cite{KVV90}
and provides new insights into the role of randomization in these
solutions, as outlined in Section \ref{section:overview}. Finally,
our algorithm has interesting connections to the solution of
\cite{MSVV05} for the \emph{small bids} case - despite the fact that
the vertex-weighted matching problem is neither harder nor easier than
the \emph{small bids} case. This strongly suggests a possible unified
approach to the unrestricted online budgeted allocation problem. See
Section \ref{sec:implications} for details.

\subsection{Overview of the Result}
\label{section:overview}

\noindent{\bf Solution to the unweighted case}: To describe our result, it is instructive to start at the unweighted case ( for all ) and study its solution by \cite{KVV90}. Two natural approaches that match each arriving  to the an unmatched neighbor in  chosen (a) arbitrarily and (b) randomly, both fail to achieve competitive ratio better than . Their solution is an elegant randomized algorithm called \ranking~that works as follows: it begins by picking a \emph{uniformly random permutation} of the vertices in  (called the ``ranking'' of the vertices). Then, as a vertex in  arrives, it is matched to the highest-ranked unmatched neighbor. Surprisingly, this idea of using correlated randomness for all the arriving vertices achieves the optimal competitive ratio of .\\

How do we generalize \ranking~in presence of unrestricted weights ? The natural \greedy~algorithm which matches an arriving vertex to the highest-weighted unmatched neighbor, achieves a competitive ratio of  (see
Appendix \ref{app3} for a proof). No deterministic algorithm can do
better. While the optimality of \ranking~for unweighted matching suggests choosing random ranking permutations of , \ranking~itself can do as badly as factor  for some weighted instances.

The main challenge in solving this problem
is that a good algorithm must follow very different strategies
depending on the weights in the input instance. \greedy~and
\ranking~are both suboptimal for this problem, but both have ideas
which are essential to its solution. In particular, they perform well
on distinct classes of inputs, namely, \greedy~on highly skewed
weights and \ranking~on equal weights. The following observation about \ranking~helps us bridge the gap between these two approaches: Suppose we perturb each weight  identically and independently and then sort the vertices in the order of decreasing perturbed weights. When all the weights are equal, the resulting order happens to be a uniformly random permutation of  and thus, \ranking~on unweighted instances can be thought of as \greedy~on perturbed weights! We use this insight to construct our solution to the vertex-weighted matching problem. While the nature of perturbation used did not matter in the above discussion, we need a very specific perturbation procedure for general vertex-weights.


Our algorithm is defined below:

\begin{algorithm}[H]
\caption{{\sc Perturbed-Greedy}} For each , pick a number
 uniformly at random from .\\
Define the function .\\
\ForEach{arriving } { Match  to the unmatched neighbor
 with the highest value of . Break ties
consistently, say by vertex id. }
\end{algorithm}

\noindent{\bf Remarks}: It is not obvious, and indeed is remarkable in
our opinion, that it suffices to perturb each weight  completely
independently of other weights. In Appendix \ref{app6}, we provide
intuition as to why such is the case. Also, the particular form of the
function  is not a pre-conceived choice, but rather an artifact
of our analysis. This combined with the discussion in Section
\ref{sec:implications} seems to suggest that  is the `right'
perturbation function. We note that we can also choose the function
 to be , which keeps the algorithm and results
unchanged. Finally, we note that the multipliers  are
distributed according to the density function 
for . Therefore, we could have
equivalently stated our algorithm as: For each , choose a
random multiplier  from the
above distribution, and use  as the perturbed weight.

Our main result is the following theorem. The second part of
the theorem follows from the optimality of \ranking~for unweighted
matching \cite{KVV90}.
\begin{theorem}
\label{thm:main}
\pgreedy~achieves a competitive ratio of  for the
vertex-weighted online bipartite matching problem. No (randomized)
algorithm has a better competitive ratio.
\end{theorem}

In addition to the basic idea (from the proof of \ranking) of charging
unmatched vertices in some probabilistic events to matched vertices in
other events, our analysis needs to handle the new complexity
introduced due to the weights on vertices. At a very high level, just
like the algorithm, our analysis also manages to pull together the
essence of the analyses of both \greedy~and \ranking.

\subsection{Implications of the Result}
\label{sec:implications}

\noindent{\bf Finding the optimal distribution over permutations of
  :} Since \pgreedy~also chooses ranking orders through
randomization, we can interpret it as a non-uniform \ranking, where it
chooses permutations of  from the `optimal' distribution. But we
could have posed the following question, without the knowledge of our
algorithm: How do we find an optimal non-uniform distribution over
permutations of ? As a start, let us consider the case of  graphs. By exhaustive search over all  graphs, we can
figure out the best \ranking~like algorithm for  graphs
(Figure \ref{fig:twobytwo} in Appendix \ref{app9} shows the only two
potentially `hard' instances in  graphs). This algorithm
picks the permutation  with probability
 and the permutation  with
probability  (where ), and then proceeds to match to the highest neighbor. This
algorithm gives a factor of , which is minimized at , giving a factor of 
(in which case the algorithm is simply the same as \ranking).

An attempt to generalize this idea to larger graphs fails due to a
blow-up in complexity.  In general, we need a probability variable
 for every permutation  of . The expected weight
of the matching produced by the algorithm on a graph , is a linear
expression . Thus,
the optimal distribution over permutations is given by the optimal
solution of a linear program in the  variables. But this LP
has exponentially many variables (one per permutation) and constraints
(one per ``canonical graph instance''). Therefore, our algorithm can
be thought of as solving this extremely large LP through a very simple
process.\\

\noindent \textbf{General capacities / Matching  multiple
  times}: Consider the following generalization of the online
vertex-weighted bipartite matching problem: Apart from a weight ,
each vertex  has a capacity  such that  can be
matched to \emph{at most}  vertices in . The capacities allow
us to better model `budgets' in many practical situations,
\textit{e.g.}, in online advertising. Our algorithm easily handles
general capacities: For each , make  copies of  and
solve the resulting instance with unit capacities: It is easy to
verify that the solution is -approximate
in expectation for the original problem with capacities.\\

\noindent{\bf Online budgeted allocation :- The \emph{single bids}
  case vs. the \emph{small bids} case}: As noted earlier and proved in
Appendix \ref{app7}, the special case of the online budgeted
allocation problem with all the non-zero bids of an agent being equal
( or 0), reduces to our vertex-weighted matching
problem. Since each agent provides a single bid value for all items,
let us call this restriction the \emph{single bids} case.

\begin{corollary}
\pgreedy~achieves a competitive ratio of  for the \emph{single
  bids} case of the online budgeted allocation problem.
\end{corollary}

Note that the \emph{small bids} case () studied in
\cite{MSVV05, BJN07} does not reduce to or from the \emph{single bids}
case. Yet, as it turns out, \pgreedy~is equivalent to the algorithm of
\cite{MSVV05} - let us call it MSVV - on instances that belong to the
intersection of the two cases. When every agent has a \emph{single
  small bid} value, the problem corresponds to vertex-weighted
matching with large capacities  for every vertex . Recall that
we handle capacities on  by making  copies  of . For each of these copies, we choose a random
 uniformly and independently. In expectation, the
's are uniformly distributed in the interval . Also
observe that \pgreedy~will match  in the
increasing order of 's, if at all. Therefore, at any point in
the algorithm, if  is the unmatched copy of  with smallest
 (and consequently highest multiplier ) then
 is in expectation equal to the fraction of the capacity
 used up at that point. But \MSVV~uses exactly the scaling factor
 where  is the fraction of spent budget at any point. We
conclude that in expectation, \pgreedy~tends to \MSVV~as the
capacities grow large, in the single small bids case.

It is important to see that this phenomenon is not merely a
consequence of the common choice of function . In fact, the
function  is not a matter of choice at all - it is a by-product
of both analyses (Refer to the remark at the end of Section
\ref{sec:main-proof}). The fact that it happens to be the exact same
function seems to suggest that  is the `right'
function. Moreover, the analyses of the two algorithms do not imply
one-another. Our variables are about expected gains and losses over a
probability space, while the algorithm in~\cite{MSVV05} is purely
deterministic.

This smooth `interface' between the seemingly unrelated \emph{single bids} and \emph{small bids} cases hints towards the existence of a unified solution to the general online budgeted allocation problem.

\subsection{Other Related Work}\label{related_work}
Our problem is a special case of online bipartite matching with edge
weights, which has been studied extensively in the literature. With
general edge weights and vertices arriving in adversarial order, every
algorithm can be arbitrarily bad (see Appendix~\ref{app4}). There are two ways to
get around this hardness: (a) assume that vertices arrive in a random
order, and/or (b) assume some restriction on the edge weights.

When the vertices arrive in random order, it corresponds to a
generalization of the {\em secretary} problem to transversal
matroids~\cite{BIK07}. Dimitrov and Plaxton~\cite{DP08} study a
special case where the weight of an edge  depends only on the
vertex  -- this is similar to the problem we study, except that it
assumes a random arrival model (and assumes vertex weights on the {\em
  online} side). Korula and Pal~\cite{KP09} give an
-competitive algorithm for the problem with general edge
weights and for the general {\em secretary} problem on transversal
matroids.

If one does not assume random arrival order, every algorithm
can be arbitrarily bad with general edge weights or even with weights
on arriving vertices. ~\cite{KP93} introduce the assumption of edge
weights coming from a metric space and give an optimal deterministic
algorithm with a competitive factor of . As far as we
know, no better randomized algorithm is known for this problem.

Finally, there has been other recent work \cite{DH09,GM08,FMMM09}, although
not directly related to our results, which study online bipartite
matching and budgeted allocations in stochastic arrival settings.\\


\noindent{\bf Roadmap:} The rest of the paper is structured as follows: In
Section~\ref{sec:prelim} we set up the preliminaries and provide a
warm up analysis of a proof of \ranking~in the unweighted special
case. Section~\ref{sec:main-proof} contains the proof of
Theorem~\ref{thm:main}.




\section{Preliminaries}
\label{sec:prelim}
\subsection{Problem Statement}
\label{section:statement}
Consider an undirected bipartite graph . The vertices of
, which we will refer to as the \emph{offline} side, are known from
the start. We are also given a weight  for each vertex .
The vertices of , referred to as the \emph{online} side, arrive one
at a time (in an arbitrary order). When a vertex  arrives, all the
edges incident to it are revealed, and at this point, the vertex 
can be matched to one of its unmatched neighbors (irrevocably) or left
permanently unmatched. The goal is to maximize the sum of the weights
of matched vertices in .

Let permutation  represent the arrival order of vertices in 
and let  be the subset of matched vertices of  at the end. Then
for the input , the gain of the algorithm, denoted by , is .

We use competitive analysis to analyze the performance of an
algorithm.  Let  be an optimal (offline) matching, i.e. one
that maximizes the total gain for  (note that the optimal matching
depends only on , and is independent of ), and let
 be the total gain achieved by . Then the
competitive ratio of an algorithm is . Our goal is to devise
an online algorithm with a high competitive ratio.

\begin{definition}[] 
For a given , we will fix a particular optimal matching, and refer
to it as the optimal offline matching .
\end{definition}
 
\begin{definition}[]
Given a , its optimal offline matching  and a 
that is matched in , we define  as its partner in .
\end{definition}

\subsection{Warm-up: Analysis of {\sc Ranking} for Unweighted Online Bipartite Matching}
\label{section:KVV}

Recall that online bipartite matching is a special case of our problem
in which the weight of each vertex is , i.e.  for all .  \cite{KVV90} gave an elegant randomized algorithm for this
problem and showed that it achieves a competitive ratio of 
in expectation. In this section, we will re-prove this classical result
as a warm-up for the proof of the main result. The following proof is
based on those presented by \cite{BM08,GM08} previously.

\vspace{0.1in}

\begin{algorithm}[H]
\caption{{\sc Ranking}}
Choose a random permutation  of  uniformly from the space of all permutations.\\
\ForEach{arriving }
{
	Match  to the unmatched neighbor in  which appears earliest in .\\
}
\end{algorithm}

\begin{theorem}[~\cite{KVV90}]
\label{thm:kvv}
In expectation, the competitive ratio of {\sc Ranking} is at least .
\end{theorem}

In this warm-up exercise, we will simplify the analysis by making the following assumptions:  and  has a perfect matching. These two assumptions imply that  and that the optimal matching  is a perfect matching.

For any permutation , let {\sc Ranking}) denote the
matching produced by {\sc Ranking} when the randomly chosen
permutation happens to be .
For a permutation  of , we say that
a vertex  has rank . 
Consider the random variable 


\begin{definition}[, ]
 is defined as the set of all occurrences of matched vertices
  in the probability space.

Similarly,  is defined as the set of all occurrences of unmatched vertices
in the probability space.

\end{definition}

Let  be the probability that the vertex at rank  in 
is matched in {\sc Ranking}, over the random choice of
permutation . Then,  and
. The expected gain of the algorithm is
.


\begin{definition}[]
For any , let
 be the permutation obtained by removing  from 
and inserting it back into  at position .
\end{definition}

\begin{lemma}
\label{lemma1}
If the vertex  at rank  in  is unmatched by {\sc
  Ranking}(), then for every ,  is matched in
{\sc Ranking}() to a vertex  such that .
\end{lemma}
\begin{proof}
Refer to Lemma~\ref{lemma2} in the analysis of
\pgreedy~for the proof of a more general version of this statement.
\end{proof}

In other words, for every vertex that remains unmatched in some event
in the probability space, there are many matched vertices in many
different events in the space. In the remaining part of this section,
we quantify this effect by bounding , which is the probability
that the vertex at rank  in  (chosen randomly by {\sc
  Ranking}) is unmatched, in terms of some of the s.


\begin{definition}[Charging map ]
 is a map from bad events (where vertices remain unmatched) to good
  events (where vertices get matched).  For each , 
\end{definition}

In other words, let  be the vertex at rank  in . Then
 contains all , such that  can be
obtained from  by moving  to some position and  is the
rank of the vertex to which , the optimal partner of , is
matched in .

For every ,  implies
 for some . Therefore, 

\begin{claim}
\label{claim1}
If  and , then .
\end{claim}
\begin{proof}
Let  be the vertex in  at rank . Let  be the vertex
to which  is matched by {\sc Ranking}. Then it is clear
from the definition of the map  that , implying .
\end{proof}

The claim proves that for a fixed , the set-values  are disjoint for different . Therefore, 


Therefore, the probabilities 's obey the equation
 for all . Since any vertex with
rank 1 in any of the random permutations will be matched, . One can make simple arguments \cite{KVV90, BM08, GM08} to prove that
under these conditions, ,
thereby proving Theorem~\ref{thm:kvv}.


\section{Proof Of Theorem~\ref{thm:main}}
\label{sec:main-proof}

In this section, we will assume that  and that  has
a perfect matching. In Appendix \ref{app1} we will show how this
assumption can be removed.

Recall that our algorithm works as follows: For each , let
 be a number picked uniformly at random from  (and
independent of other vertices) Now, when the next vertex 
arrives, match it to the available neighbor  with the maximum value
of , where .

For ease of exposition, we will prove our result for a discrete
version of this algorithm. For every  we will choose a random
integer  uniformly from  where  is the
parameter of discretization. We will also replace the
function  by its discrete version . The discrete version of our
algorithm also matches each incoming vertex  to the available
neighbor  with the maximum value of . Notice
that  is a decreasing function, so  if . As , the discrete version tends to our
original algorithm.\\


We begin with some definitions, followed by an overview of the proof.

We will denote by , the set of these random choices. We will say that  is at
\emph{position}  in  if . As a matter of
notation, we will say that position  is \emph{lower} (resp. higher)
than  if  (resp. ).

\begin{definition}[ is matched in ]
We say that  is matched in  if our algorithm matches it
when the overall choice of random positions happens to be
. 
\end{definition}

Let  be the indicator variable denoting that the vertex
at position  is matched in . 

\begin{definition}[, ]
 is defined as the set of all occurrences of matched vertices in the probability space.

Similarly,  is defined as the set of all occurrences of unmatched vertices in the probability space.

\end{definition}

Let  be the \emph{expected gain} at , over the random choice
of . Then,


The expected gain of the algorithm is . Also note that the \emph{optimal gain} at any position
 is  since each vertex in 
appears at position  with probability  and is matched in the
optimal matching. Therefore,



\begin{definition}[]
For any ,  is obtained from  by
changing the position of  to , i.e.  and
 for all .
\end{definition}

\begin{observation}
For all  and , our algorithm
matches  to some  in .
\end{observation}
The above observation follows from Lemma~\ref{lemma2}. We'll use it
to define a map from bad events to good events as follows.

\begin{definition}[Charging Map ]
\label{def:map}
For every , define the set-valued
map 
\end{definition}

\begin{observation}
\label{obs1}
If , then .
\end{observation}


Now we are ready to give an overview of the proof.

\subsection*{Overview of the proof}
The key idea in the analysis of {\sc Ranking} in Section
\ref{section:KVV} was that we can bound the number of occurrences of
unmatched vertices - the \emph{bad} events - in the entire probability
space by a careful count of the matched vertices - the \emph{good}
events. The charging map  defined above is an attempt to do this.
We'll show in Lemma~\ref{lemma2} that if , then the scaled (by ) gain due to  in
 is no less than the scaled loss due to  in
. However,  may be higher or lower than , unlike {\sc
  Ranking} where . This implies that the bound is in terms
of events in , , which is very weak (as
many of the events in the union are not used). 

One idea is to bound the sum of losses incurred at all positions,
thereby using almost all the events in . However, if we do this,
then the charging map loses the disjointness property, i.e. if
 and  then 
value of both these occurrences is the same. Thus, each event in
 gets charged several times (in fact a non-uniform number
of times), again making the bound weak. To this end, we introduce the idea of {\em marginal loss}~(Definition \ref{def:margin}), which helps us define a disjoint map and get a tight bound.

Next, we formalize the above.

\subsection*{Formal proof}

We begin by proving an analogue of Lemma~\ref{lemma1}.

\begin{lemma}
\label{lemma2}
If the vertex  at position  in  is unmatched by our algorithm, then for every , the algorithm matches  in  to a vertex  such that .
\end{lemma}
\begin{proof} 
\noindent{\textbf{Case 1 ()}}: Let  be the order of arrival of vertices in
. Clearly,  will see the same choice of neighbors in
 as in , except the fact that the position of 
is higher in  than in . Since we did not match
 to  in ,  will retain its match from 
even in . Now assuming that  all match the
same vertex in  as they did in ,  will
see the same choice of neighbors in  as in  with
the exception of . Since  did not match  in 
either, it will retain the same neighbor in  and by
induction every vertex from , specifically  keeps the same
match in  as in . Since , we conclude .\\


\noindent{\textbf{Case 2 ()}}: For a vertex , let  and  be
the vertices to which  is matched in  and 
respectively, if such a match exists and null otherwise. Intuitively,
since , the scaling factor of  only
improves in this case, while that of any other vertex in  remains
the same. Therefore, we can expect  to be more likely to be matched
in  and the
 to
hold for all . In fact, something more specific is true. The
symmetric difference of the two matchings produced by the algorithm
for  and  is exactly one path starting at  that
looks like ,
where  appear in their order of arrival. In what
follows we prove this formally.



Let  be the
set of vertices in  with different matches in  and
. Index the members of  as  in the same
order as their arrival, \textit{i.e.}  arrives the earliest. For
simplicity, let  and .

We assert that the following invariant holds for :
Both  and  are unmatched in  when 
arrives and  matches , \textit{i.e.} .

For base case, observe that the choice of neighbors for  in
 is the same as in , except , which has moved
to a lower position. Since by definition  does not match  in
, . Now consider the situation when 
arrives. All the vertices arriving before  - with the exception
of  - have been matched to the same vertex in  as in
, and  has matched to , leaving  yet
unmatched. Let  and  be the sets
of unmatched neighbors of  in  and 
respectively \emph{at the moment} when  arrives. Then from above
arguments, . Since  was unmatched in , . Since , . This is only possible if . And hence the base case is true.

Now assume that the statement holds for  and consider the arrival
of . By induction hypothesis,  has been matched to  and
 have been matched to 
respectively. All the other vertices arriving before  that are
not in  have been matched to the same vertex in  as in
. Therefore,  is yet unmatched. Let 
and  be the sets of unmatched neighbors of 
in  and  respectively at the moment when 
arrives. Then from above arguments, . Since  was
unmatched in , . Given that , the
only possibility is . Hence the proof of the inductive
statement is complete.

If  then  and
the statement of the lemma clearly holds since . If , then  and

since . Now suppose  for some . Then  and by the invariant above,


Equation \eqref{eq11} follows from the fact that  was matched in
 to  when  was also unmatched. The fact that
only  changes its position between  and  leads us
to \eqref{eq12}. Finally, equation \eqref{eq13} follows from the fact
that  was matched to  in  when  was also
unmatched.
\end{proof}



Using the above lemma, we get the following easy observation.
\begin{observation}
\label{obsk}
For all , , 
contains  values.
\end{observation}


\textbf{Remark}: As noted in the overview, although Lemma \ref{lemma2}
looks very similar to Lemma \ref{lemma1}, it is not sufficient to get
the result, since the good events pointed to by Lemma \ref{lemma2} are
scattered among all positions  -- in contrast to
Lemma \ref{lemma1}, which pointed to only lower positions ,
giving too weak a bound. We try to fix this by combining the losses
from all . However we run into another difficulty in doing
so. While for any fixed , the maps  are disjoint
for all , but the maps for two occurrences in
different s may not be disjoint.
In fact, whenever some  is unmatched in  at position
, it will also remain unmatched in  for
, and the sets  and  will
be exactly the same! This situation is depicted in Figure \ref{fig:margins} in Appendix \ref{app8}.


This absence of disjointness again renders the bound too weak. To fix this, we carefully select a subset of bad events
from  such that their set-functions are indeed
disjoint, while at the same time, the total gain/loss can be easily
expressed in terms of the bad events in this subset.
   
\begin{definition}[Marginal loss events ] 
\label{def:margin}
Let , where .
\end{definition}

Informally,  consists of \emph{marginal} losses. If  is
unmatched at position  in , but matched at position 
in , then  (See Figure
\ref{fig:margins} in Appendix \ref{app8}). The following property can be proved using the same arguments
as in Case 1 in the proof of Lemma \ref{lemma2}.

\begin{observation}
\label{obs2}
For ,  is matched at  in  if and only if .
\end{observation}

\begin{definition}[Expected Marginal Loss ]

\end{definition}


\begin{claim}
\label{claim2}



\end{claim}
\begin{proof}
To prove equation \eqref{eq7}, we will fix a  and construct a
one-to-one map . Given
, let  be the lowest position of  such
that  remains unmatched in . By observation \ref{obs2},
 is unique for . We let . Clearly, . To prove that the map is one-to-one, suppose . Then by definition of ,  which is only possible if . Therefore, .

Lastly, observe that  maps an element of  corresponding to the
vertex  being unmatched, to an element of  corresponding to
the same vertex  being unmatched. From equation \eqref{eq8},



This proves equation \eqref{eq7}. Summing \eqref{eq7} for all , we get \eqref{eq6}.
\end{proof}

Now consider the same set-valued map  from Definition \ref{def:map}, but restricted only to the members of . We have:

\begin{claim}
\label{claim3}
For  and , if  and  then , 
and .
\end{claim}
\begin{proof}
If  is matched to  in  then by definition of , , implying . Therefore,  for some . But this implies
that  for some . This is only
possible for  since by definition, if  is unmatched in
 at , then there exists a unique  for which
. If , then  and .
\end{proof}

Armed with this disjointness property, we can now prove our main theorem. 

\begin{theorem}
As ,

\end{theorem}
\begin{proof}
Using Lemma~\ref{lemma2} and Observation~\ref{obsk}, we have for every
,



If we add the equation \eqref{eq10} for all  and for all , then using Claim \ref{claim3} and Observation \ref{obs1}, we arrive at



Equation \eqref{eq14} follows from \eqref{eq1} and
\eqref{eq9}. Equation \eqref{eq15} uses Claim \ref{claim2}.

We now rearrange terms to get 


When , observe that
 and
 as . Using
Claim~\ref{claim2},



Hence, as ,  

\noindent \textbf{Remark}: Observe that we substituted for  only after equation \eqref{eq18} - up until that point, any choice of a non-increasing function  would have carried the analysis through. In fact, the chosen form of  is a result of trying to reduce the left hand side of equation \eqref{eq18} to the expected total loss. To conclude, the `right' perturbation function is dictated by the analysis and not vice versa.

\end{proof}






\bibliographystyle{alpha}
\bibliography{weighted_matching}


\appendix

\section{The Reduction from Online Budgeted Allocation with Single Bids}
\label{app7}

In this section, we will show that the \emph{single bids} case of the online budgeted allocation problem reduces to online vertex-weighted bipartite matching. Let us first define these problems.\\

\noindent \textbf{\textsc{Online budgeted allocation}}: We have  agents and  items. Each agent  specifies a monetary budget  and a bid  for each item . Items arrive online, and must be immediately allocated to an agent. If a set  of items is allocated to agent , then the agent pays the minimum of  and  . The objective is to maximize the total revenue of the algorithm.\\

\noindent \textbf{\textsc{Single bids case}}: Any bid made by agent  can take only two values:  or 0. In other words, all the non-zero bids of an agent are equal.

\begin{claim}
Online budgeted allocation with single bids reduces to online vertex-weighted bipartite matching.
\end{claim}
\begin{proof}
Given an instance of online budgeted allocation where agent  has budget  and single bid value , we will construct an input instance  of online vertex-weighted bipartite matching. The set  consists of one vertex corresponding to every item. The set  will contain one or more vertices for every agent.


For every agent , let  be the largest integer such that  and let . Clearly, . We will construct a set  of  vertices, each with weight . In addition, if , then we will construct a vertex  with weight  and add it to . For all  and , the edge  if and only if agent  makes a non-zero bid on the item corresponding to .\\

\noindent (1) Given a solution to the budgeted allocation problem where a set  of items is allocated to agent , let us see how to construct a solution to the vertex-weighted matching problem with the same total value. 
\begin{itemize}
\item If agent  pays a total of , then we know that . Hence, for every item in , we will match the corresponding vertex in  to a vertex in . Let  be the set of vertices in  thus matched. We have: 

\item If agent  pays a total amount strictly less than , then we know that: (a) , (b)  and (3) agent  pays the budget . We can now choose any  items in  and match the corresponding vertices in  to the  vertices in . The sum of the weights of matched vertices in , .
\end{itemize}
Summing over all , the weight of the matching formed is equal to the total revenue of the budgeted allocation. Let  and  denote the values of the optimal solutions of the budgeted allocation and the vertex-weighted matching problems respectively. Then we conclude from the above discussion that: 


\noindent (2) Given a solution to the vertex-weighted matching problem where a set  of vertices is matched, let us see how to construct a solution to the budgeted allocation problem with at least the same total value. Let . For every  that is matched to a vertex in , we will allocate the corresponding item to agent . Let  be the set of items allocated to agent . 
\begin{itemize}
\item If , then agent  pays a total of  and we have:  
\item If on the other hand,  then agent  pays a total of  and we have: 
\end{itemize}
Summing over all , the total revenue of the budgeted allocation is at least the weight of the matching. Let  be the expected weight of the vertex-weighted matching constructed by \pgreedy~and  be the expected value of the budgeted allocation constructed using the above scheme. From the above discussion, we conclude:
Therefore, 
 

Here, equation \eqref{eq20} follows from the main result - Theorem \ref{thm:main} - and the last step uses equation \eqref{eq19}. This completes our proof.

\end{proof}

\section{Performance of {\sc Greedy} and {\sc Ranking}}
\label{app3}
With non-equal weights, it is clearly preferable to match vertices with larger weight. This
leads to the following natural algorithm.\\

\begin{algorithm}[H]
\caption{{\sc Greedy}}
\ForEach{arriving }
{
	Match  to the unmatched neighbor in  which maximizes  (breaking ties arbitrarily)\;
}
\end{algorithm}

It is not hard to show that {\sc Greedy} achieves a competitive ratio
of at least . 

\begin{lemma}
{\sc Greedy} achieves a competitive ratio of 1/2 in vertex-weighted online bipartite matching.
\end{lemma}
\begin{proof}
Consider an optimal offline matching, and a vertex  that is
matched in the optimal offline matching but not in the greedy
algorithm. Now look at a vertex  that is matched to the
vertex  in the optimal matching.  In \greedy,  must have been
matched to a vertex , s.t. , since  was
unmatched when  was being matched. So we'll charge the loss of
 to . Note that each  does not get charged more than
once -- it is charged only by the optimal partner of its partner in
the algorithm's matching. Thus the loss of the algorithm is no more
than the value of the matching output by the algorithm. Hence the
claim.
\end{proof}

In fact, this factor  is tight for {\sc Greedy} as shown by an instance consisting
  of many copies of the following gadget on four vertices, with  and . As , the
  competitive ratio of {\sc Greedy} tends to .

\begin{center}
\includegraphics[height=3cm]{fig1.eps}
\end{center}

Notice that this counter-example relies on weights being roughly
equal. We, however, know that {\sc Ranking} has an expected
competitive ratio of  when the weights are equal. On the
other hand, if the weights are very different, i.e.  is
large, in the above example, then {\sc Greedy} provides a good
competitive ratio. At the same time, if we exchanged the weights on
the two vertices in the example to be  and , then as  grows large, the expected competitive
ratio of {\sc Ranking} drops to  and on larger examples,
it can be as low as . To summarize, {\sc Greedy} tends to
perform well when the weights are highly skewed and {\sc Ranking}
performs well when the weights are roughly equal.

\section{Intuition Behind the Sufficiency of Independent Perturbations}
\label{app6}

Recall that our algorithm perturbs each weight  independent of the other weights. The fact that \pgreedy~achieves the best possible competitive ratio is a post-facto proof that such independence in perturbations is sufficient. Without the knowledge of our algorithm, one could reasonably believe that the vector of vertex-weights  - which is known offline - contains valuable information which can be exploited. In what follows we provide intuition as to why this is not the case.

Consider the two input instances in Figure \ref{fig:fig4}. Both the connected components in  have equal weights, and hence we know that \ranking~achieves the best possible competitive ratio on . Similarly, both connected components in  have highly skewed weights, suggesting \greedy~as the optimal algorithm. On the other hand, \ranking~and \greedy~are far from optimal on  and  respectively. Since two instances with identical values of vertex-weights require widely differing strategies, this exercise suggests that we may not be losing must information by perturbing weights independently. The optimality of our algorithm proves this suggestion.


\begin{figure}[h]
\begin{center}
\includegraphics[height=6cm]{fig4.eps}
\end{center}
\caption{\label{fig:fig4}Two instances with the same vertex-weights, but widely differing optimal strategies.}
\end{figure}

\section{Hard Instances in  Graphs}
\label{app9}

Figure \ref{fig:twobytwo} shows the only two potentially `hard' instances in  graphs. On all other instances, the optimal matching is found by any reasonable algorithm that leaves a vertex  unmatched only if all its neighbors are already matched.

\begin{figure}[h]
\begin{center}
\includegraphics[height=2cm]{fig2.eps}
\end{center}
\caption{\label{fig:twobytwo}Canonical examples for 22 graphs.}
\end{figure}


\newpage

\section{Marginal Loss Events}
\label{app8}

\begin{figure}[h]
\begin{center}
\includegraphics[height=3.5cm]{fig3.eps}
\end{center}
\caption{\label{fig:margins}Marginal Losses}
\end{figure}




\section{Graphs with Imperfect Matchings}
\label{app1}
In Section \ref{sec:main-proof}, we proved Theorem \ref{thm:main} for graphs  such that  and  has a perfect matching. We can remove these assumptions with just a few modifications to the definitions and equations involved in the proof. The algorithm remains unchanged, \textit{i.e.} we just use \pgreedy.  We will only outline these modifications and the rest of the proof follows easily. Let  be a maximum weight matching in  and  be the set of vertices in  matched by . Thus we know that .

Keeping the definition of  the same, we change the definition of  to:


The above redefinition conveys the fact that if a vertex  is \emph{not} matched by , then we no longer consider  being unmatched a bad event. Consequently, equation \eqref{eq8} changes to:

 which in turn yields following counterpart of equation \eqref{eq7}:



Let  be the version of \eqref{eq16} for . We then multiply  by  and sum over  to obtain a combined inequality (with ):



Equation \eqref{eq17} used the definition of . Combining equation \eqref{eq17} with \eqref{eq14}, we get:



as , since  and  as .

\section{A Lower Bound for Randomized Algorithms with Edge Weights}
\label{app4}


In this section, we will sketch the proof of a lower bound for the competitive ratio of a randomized algorithm, when the graph  has edge weights and our objective is to find a matching in  with maximum total weight of edges. Previous studies of this problem have only mentioned that no constant factor can be achieved when the vertices in  arrive in an online manner. However, we have not been able to find a proof of this lower bound for randomized algorithms in any literature. We prove the result when the algorithm is restricted to be scale-free. A scale-free algorithm in this context produces the exact same matching when all the edge weights are multiplied by the same factor.

Consider a graph  such that  contains just one vertex  and each vertex in  has an edge to  of weight . Fix  to be the order in which the vertices of  arrive online. By Yao's principle, it suffices for us to produce a probability distribution over  such that no deterministic algorithm can perform well in expectation. We will denote the vector of edge weights in the same order in which the corresponding vertices in  arrive, \textit{i.e.}  and so on. Consider the following  vectors of edge weights: For every ,  and so on, where  is a sufficiently large number. Suppose our input distribution chooses each one of these  vectors of edge weights with equal probability.

Clearly, regardless of the vector which is chosen, . Since an algorithm is assumed to be scale-free and online, it makes the exact same decisions after the arrival of first  vertices for each of the edge weight vectors , . Therefore, it cannot distinguish between  after just  steps. Hence, we can characterize any algorithm by the unique  such that it matches the 'th vertex in  with a positive weight edge.

Let  be any deterministic algorithm that matches the 'th incoming vertex with a positive weight edge to . Then the expected weight of the edge chosen by  is . Since  is large, this is at most , where  is some constant. Applying Yao's principle, we conclude that the competitive ratio of the best scale-free randomized algorithm for online bipartite matching with edge weights is . 

\end{document}
