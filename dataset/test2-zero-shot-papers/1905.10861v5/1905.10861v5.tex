\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subcaption}
\usepackage{multirow}



\usepackage[breaklinks=true,bookmarks=false,colorlinks]{hyperref}
\usepackage{cleveref}

\cvprfinalcopy 

\def\cvprPaperID{9} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\setcounter{page}{1}
\begin{document}

\title{Temporal Attentive Alignment for Video Domain Adaptation}

\author{Min-Hung Chen \hspace{3em}
Zsolt Kira \hspace{3em}
Ghassan AlRegib \\
Georgia Institute of Technology\\
{\tt\small \{cmhungsteve, zkira, alregib\}@gatech.edu}
}

\maketitle


\begin{abstract}
   Although various image-based domain adaptation (DA) techniques have been proposed in recent years, domain shift in videos is still not well-explored. Most previous works only evaluate performance on small-scale datasets which are saturated. Therefore, we first propose a larger-scale dataset with larger domain discrepancy: \textbf{UCF-HMDB}. Second, we investigate different DA integration methods for videos, and show that simultaneously aligning and learning temporal dynamics achieves effective alignment even without sophisticated DA methods. Finally, we propose \textbf{Temporal Attentive Adversarial Adaptation Network (TAN)}, which explicitly attends to the temporal dynamics using domain discrepancy for more effective domain alignment, achieving state-of-the-art performance on three video DA datasets. The code and data are released at \url{http://github.com/cmhungsteve/TA3N}.
\end{abstract}

\section{Introduction}

\begin{figure}[!t]
\centering
\includegraphics[width=0.475\textwidth]{figs/intro/intro_figure_basketball_attention_ver1.pdf}
\caption{An overview of proposed TAN for video DA. 
In addition to spatial discrepancy between frame images, videos also suffer from temporal discrepancy between sets of time-ordered frames that contain multiple local temporal dynamics with different contributions to the overall domain shift, as indicated by the thickness of green dashed arrows. 
Therefore, we propose to focus on aligning the temporal dynamics which have higher domain discrepancy using a learned attention mechanism to effectively align the temporal-embedded feature spaces for videos.
Here we use the action \textit{basketball} as the example.}
\label{fig:overview_video_DA}
\end{figure}

Unsupervised \textit{Domain adaptation (DA)}~\cite{csurka2017comprehensive} has been studied extensively in recent years to address the \textit{domain shift} problem~\cite{quionero2009dataset}, which means the models trained on source labeled datasets do not generalize well to target datasets and tasks, without access to any target labels.
While many DA approaches are able to diminish the distribution gap between source and target domains while learning discriminative deep features~\cite{ganin2015unsupervised, long2017deep, li2018adaptive, saito2018maximum}, most methods have been developed only for images and not videos.

Furthermore, unlike image-based DA work, 
there do not exist well-organized datasets to evaluate and benchmark the performance of DA algorithms for videos. 
The most common datasets are \textit{UCF-Olympic} and \textit{UCF-HMDB}~\cite{sultani2014human, xu2016dual, jamal2018deep}, which have only a few overlapping categories between source and target domains. 
This introduces limited domain discrepancy so that a deep CNN architecture can achieve nearly perfect performance even without any DA method (details in \Cref{sec:experimental_results} and \Cref{table:sota_ucf-olympic_ucf-hmdb-small}).
Therefore, we propose a larger-scale video DA dataset, \textit{UCF-HMDB}, by collecting all relevant and overlapping categories between UCF101~\cite{soomro2012ucf101} and HMDB51~\cite{kuehne2011hmdb}, leading to three times larger than previous two datasets, and contains larger domain discrepancy (details in \Cref{sec:experimental_results} and \Cref{table:sota_ucf-hmdb_full}). 

Videos can suffer from domain discrepancy along both the spatial and temporal directions, bringing the need of alignment for embedded feature spaces along both directions, as shown in \Cref{fig:overview_video_DA}. 
However, most DA approaches have not explicitly addressed the domain shift problem in the temporal direction.
Therefore, we first investigate different DA integration methods for video classification and 
propose \textit{Temporal Adversarial Adaptation Network (TAN)}, which simultaneously aligns and learns temporal dynamics, to effectively align domains spatio-temporally, outperforming other approaches which naively apply more sophisticated image-based DA methods for videos. 
To further achieve more effective temporal alignment, we propose to focus more on aligning those which have higher contribution to the overall domain shift, such as the local temporal features connected by thicker green arrows shown in \Cref{fig:overview_video_DA}.
Therefore, we propose \textbf{Temporal Attentive Adversarial Adaptation Network (TAN)} to explicitly attend to the temporal dynamics by taking into account the domain distribution discrepancy,
achieving state-of-the-art performance on all three investigated video DA datasets. (\href{http://github.com/cmhungsteve/TA3N}{GitHub})

\section{Technical Approach}
Here we introduce our baseline model (\Cref{sec:baseline}) and proposed methods for video DA (\Cref{sec:TAAN,sec:TAAAN}).

\subsection{Baseline Model} \label{sec:baseline}
Given the recent success of large-scale video classification using CNNs~\cite{karpathy2014large}, we build our baseline on such architectures, as shown in the lower part of Figure~\ref{fig:TemPooling_RevGrad}. 

\begin{figure}[!ht]
\centering
\includegraphics[scale=0.373]{figs/architecture/TemPooling_RevGrad_ver4_1_simple.pdf}
\caption{Baseline architecture (TemPooling) with the adversarial discriminators  and .  is the class prediction loss, and  and  are the domain losses. 
}
\label{fig:TemPooling_RevGrad}
\end{figure}

We first feed the 
frame-level feature representations extracted from ResNet~\cite{he2016deep} pre-trained on ImageNet into our model, 
which can be divided into two parts: 1) \textit{Spatial module }, 
which consists of multilayer perceptrons (MLP) that aims to convert the general-purpose feature vectors into task-driven feature vectors, where the task is video classification in this paper; 
2) \textit{Temporal module }, which aggregates the frame-level feature representations to form a single video-level feature representation for each video. Our baseline architecture conducts mean-pooling along the temporal direction in , so we note it as \textit{TemPooling}.

To address domain shift, we integrate TemPooling with \textit{Adversarial Discriminator }, which is the combination of a gradient reversal layer (GRL) and a domain classifier , inspired by \cite{ganin2015unsupervised}. Through adversarial training,  is optimized to discriminate data across domains, while the feature generator is optimized to gradually align the feature distributions across domains. 
 is integrated in two ways: 1) : show how directly applying image-based DA approaches can benefit video DA; 2) : indicate how DA on temporal-dynamics-encoded features benefits video DA. 

\subsection{Integration of Temporal Dynamics with DA} \label{sec:TAAN}
One main drawback of directly integrating image-based DA approaches into TemPooling
is that the relation between frames is missing. 
Therefore, we would like to address this question: \textit{Does the video DA problem benefit from encoding temporal dynamics into features?} 

Given the fact that humans can recognize actions by reasoning the observations across time, we propose the \textit{TemRelation} architecture by replacing the temporal pooling mechanism in  with the Temporal Relation module, which is modified from \cite{zhou2018temporalrelation}, as shown in Figure~\ref{fig:TemRelation_RevGrad_TransAttn}. 
Specifically, We fuse the time-ordered feature representations with MLPs into -frame relation , and then sum up all  into the final video representation to capture temporal relations at multiple time scales. 
To align and encode temporal dynamics simultaneously instead of solely modifying the Temporal module, we propose \textbf{Temporal Adversarial Adaptation Network (TAN)},
which integrates relation discriminators  \textit{inside} the Temporal module with corresponding -frame relations to properly align different temporal relations,  
outperforming those which are extended from sophisticated image-based DA approaches although TAN is adopted from a simpler DA method (details in \Cref{table:sota_ucf-hmdb_full}).

\subsection{Temporal Attentive Alignment for Videos} \label{sec:TAAAN}
Although aligning temporal features across domains benefits video DA, not all the features equally contribute to the overall domain shift. 
Therefore, we propose to focus more on aligning those which have larger domain discrepancy and assign them with 
larger attention weighting values, as shown in \Cref{fig:overview_domain_attention}.
The main question becomes: \textit{How to incorporate domain discrepancy for attention?} 

\begin{figure}[!t]
\centering
\includegraphics[width=0.475\textwidth]{figs/approach/approach_figure_basketball_attention_ver1.pdf}
\caption{The domain attention mechanism in TAN.
Thicker arrows corresponds to larger attention weights.
}
\label{fig:overview_domain_attention}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[width=0.475\textwidth]{figs/architecture/TemRelation_RevGrad_TransAttn_ver1_1_simple_v2.pdf}
\caption{The overall architecture of the proposed TAN. In the temporal relation module, time-ordered frames are used to generate -1 -frame relation feature representations .
The attentive entropy loss , which is calculated by domain entropy  and class entropy , aims to enhance the certainty of those videos that are more similar across domains. 
}
\label{fig:TemRelation_RevGrad_TransAttn}
\end{figure}

To address this, we propose \textbf{Temporal Attentive Adversarial Adaptation Network (TAN)}, as shown in \Cref{fig:TemRelation_RevGrad_TransAttn}, by introducing the \textit{domain attention} mechanism, which assigns higher domain attention values to -frame relation features with lower domain entropy, which correspond to higher domains discrepancy. Morevoer, we minimize the class entropy for the videos with low domain discrepancy to refine the classifier adaptation.

The overall loss of TAN can be expressed as follows:

where  is the total number of source data,  equals the number of all data.  represents the -th video. ,  and  is the trade-off weighting for each domain loss ,  and .  is the weighting for the attentive entropy loss . All the weightings are chosen via grid search. 

Our proposed TAN and TADA~\cite{wang2019transferable} both utilize entropy functions for attention but with different perspectives. TADA aims to focus on the foreground objects for image DA, while TAN aims to find important and discriminative parts of temporal dynamics to align for video DA.

\section{Experiments}
We evaluate on three datasets: UCF-Olympic, UCF-HMDB and UCF-HMDB, as shown in \Cref{table:dataset}. The adaptation setting is noted as ``Source  Target". 

\begin{table}[!t]
\centering
\scriptsize
    \begin{tabular}{c|c|c|c}
     & UCF-HMDB & UCF-Olympic & UCF-HMDB \\ \hline
    length (sec.) & 1 - 21 & 1 - 39 & 1 - 33 \\ \hline
    resolution & \multicolumn{3}{c}{UCF:  / Olympic: vary / HMDB: vary} \\ \hline
    class \# & 5 & 6 & 12 \\ \hline
    video \# & 1171 & 1145 & 3209 \\ \hline
    \end{tabular}
\caption{The comparison of the video DA datasets.}
\label{table:dataset}
\end{table}

\subsection{Datasets and Setup}
\noindent\textbf{UCF-Olympic} and \textbf{UCF-HMDB}:
First, we evaluate our approaches on two publicly used datasets, UCF-Olympic and UCF-HMDB, and compare with all other works that also evaluate on these two datasets~\cite{sultani2014human, xu2016dual, jamal2018deep}. 

\noindent\textbf{UCF-HMDB}:
To ensure large domain discrepancy to investigate DA approaches, 
we build \textbf{UCF-HMDB}, which is around 3 times larger than UCF-HMDB and UCF-Olympic, as shown in \Cref{table:dataset}. 
To compare with image-based DA approaches, we extend several state-of-the-art methods~~\cite{ganin2015unsupervised, long2017deep, li2018adaptive, saito2018maximum} for video DA with our TemPooling and TemRelation architectures, as shown in \Cref{table:sota_ucf-hmdb_full}. The difference between the ``Target only" and ``Source only" settings is the domain used for training. 

\subsection{Experimental Results and Discussion} \label{sec:experimental_results}
\noindent\textbf{UCF-Olympic} and \textbf{UCF-HMDB}:
In these two datasets, our approach outperforms all the previous methods,
as shown in Table~\ref{table:sota_ucf-olympic_ucf-hmdb-small} (e.g. at least 9\% absolute difference on ``U  H").
These results also show that the performance on these datasets is saturated. With a strong CNN as the backbone architecture, even our baseline architecture (TemPooling) can achieve high accuracy without any DA method.
This suggests that these two datasets are not enough to evaluate more sophisticated DA approaches, so larger-scale datasets for video DA are needed.

\begin{table}[!t]
\centering
\footnotesize
    \begin{tabular}{c|c|c|c|c}
    Source  Target & U  O & O  U & U  H & H  U \\ \hline
    W. Sultani et al.~\cite{sultani2014human} & 33.33 & 47.91 & 68.70 & 68.67 \\ 
    T. Xu et al. ~\cite{xu2016dual} & 87.00 & 75.00 & 82.00 & 82.00 \\ 
    AMLS (GFK)~\cite{jamal2018deep} & 84.65 & 86.44 & 89.53 & 95.36 \\ 
    AMLS (SA)~\cite{jamal2018deep} & 83.92 & 86.07 & 90.25 & 94.40 \\ 
    DAAA~\cite{jamal2018deep} & 91.60 & 89.96 & - & - \\ \hline
    TemPooling & 96.30 & 87.08 & 98.67 & 97.35 \\ 
    TemPooling + RevGrad~\cite{ganin2015unsupervised} & \textbf{98.15} & 90.00 & \textbf{99.33} & 98.41 \\ \hline\hline 
    Ours (TAN) & \textbf{98.15} & 91.67 & \textbf{99.33} & \textbf{99.47} \\ 
    Ours (TAN) & \textbf{98.15} & \textbf{92.92} & \textbf{99.33} & \textbf{99.47} \\ \hline
    \end{tabular}
\caption{The accuracy (\%) comparison on UCF-Olympic and UCF-HMDB (U: UCF, O: Olympic, H: HMDB). 
} 
\label{table:sota_ucf-olympic_ucf-hmdb-small}
\end{table}

\noindent\textbf{UCF-HMDB}:
It is worth noting that the ``Source only" accuracy on UCF-HMDB is much lower than UCF-HMDB, 
which implies much larger domain discrepancy in UCF-HMDB.
We now answer the question in Section~\ref{sec:TAAN}:
\textit{Does the video DA problem benefit from encoding temporal dynamics into features?} (details in \Cref{table:sota_ucf-hmdb_full})

For the same DA method, TemRelation outperforms TemPooling in most cases, especially for the gain value (e.g. on ``U  H", ``TemRelation+RevGrad" reaches 2.77\% absolute accuracy gain while ``TemPooling+RevGrad" only reaches 0.83\% gain).
By simultaneously aligning and encoding temporal dynamics, TAN 
outperforms other approaches which are extended from more sophisticated image-based DA methods. 
Finally, with the domain attention mechanism, our proposed \textbf{TAN} reaches 
78.33\%  (6.66\% gain) on ``U  H" and 81.79\% (7.88\% gain) on ``H  U", achieving state-of-the-art performance in terms of accuracy and gain, as shown in \Cref{table:sota_ucf-hmdb_full}.

\begin{table}[!t]
\centering
\scriptsize
    \begin{tabular}{c|c|c|c|c}
    S  T & \multicolumn{2}{c|}{UCF  HMDB} & \multicolumn{2}{c}{HMDB  UCF} \\ \hline
    Temporal & \multirow{2}{*}{TemPooling} & \multirow{2}{*}{TemRelation} & \multirow{2}{*}{TemPooling} & \multirow{2}{*}{TemRelation} \\ 
    Module &  &  &  &  \\ \hline
Target only & 80.56 (-) & 82.78 (-) & 92.12 (-) & 94.92 (-) \\ \hline
    Source only & 70.28 (-) & 71.67 (-) & 74.96 (-) & 73.91 (-) \\ 
    RevGrad~\cite{ganin2015unsupervised} & 71.11 (0.83) & 74.44 (2.77) & 75.13 (0.17) & 74.44 (1.05) \\ 
    JAN~\cite{long2017deep} & 71.39 (1.11) & 74.72 (3.05) & 80.04 (5.08) & 79.69 (5.79) \\ 
    AdaBN~\cite{li2018adaptive} & 75.56 (5.28) & 72.22 (0.55) & 76.36 (1.40) & 77.41 (3.51) \\ 
    MCD~\cite{saito2018maximum} & 71.67 (1.39) & 73.89 (2.22) & 76.18 (1.23) & 79.34 (5.44) \\ \hline
    Ours (TAN) & N/A & 77.22 (5.55) & N/A & 80.56 (6.66) \\ 
    Ours (TAN) & N/A & \textbf{78.33} (\textbf{6.66}) & N/A & \textbf{81.79} (\textbf{7.88}) \\ \hline
    \end{tabular}
\caption{The comparison of accuracy (\%) for other approaches on UCF-HMDB. The gain values are in ().  
}
\label{table:sota_ucf-hmdb_full}
\end{table}

\noindent\textbf{Integration of }.
Here we investigate how temporal modules affect each  without considering the attention mechanism.
For the TemRelation architecture,  integration outperforms  integration (averagely 0.58\% absolute gain improvement across two tasks), while the TemPooling does not show improvement, as shown in \Cref{table:experiments_dann_position}. This implies that TemRelation better encodes temporal dynamics than TemPooling. 
Finally, by combining all , the performance improves even more (4.20\% improvement).

\begin{table}[!t]
\centering
\scriptsize
    \begin{tabular}{c|c|c|c|c}
    S  T & \multicolumn{2}{c|}{UCF  HMDB} & \multicolumn{2}{c}{HMDB  UCF} \\ \hline
    Temporal & \multirow{2}{*}{TemPooling} & \multirow{2}{*}{TemRelation} & \multirow{2}{*}{TemPooling} & \multirow{2}{*}{TemRelation} \\ 
    Module &  &  &  &  \\ \hline
     & 71.11 (0.83) & 74.44 (2.77) & 75.13 (0.17) & 74.44 (1.05) \\
     & 71.11 (0.83) & 74.72 (3.05) & 75.13 (0.17) & 75.83 (1.93)  \\ \hline
    All  & 71.11 (0.83) & \textbf{77.22} (\textbf{5.55}) & 75.13 (0.17) & \textbf{80.56} (\textbf{6.66}) \\ \hline
    \end{tabular}
\caption{The full evaluation of accuracy (\%) for integrating  in different positions without the attention mechanism. 
}
\label{table:experiments_dann_position}
\end{table}

\noindent\textbf{Visualization of distribution}.
Figure~\ref{fig:tSNE} shows that TAN 
can group source data (blue dots) into denser clusters and generalize the distribution into the target domains (orange dots) as well, outperforming the baseline model. 

\begin{figure}[!t]
  \begin{subfigure}[b]{0.235\textwidth}
    \includegraphics[width=\textwidth]{figs/tSNE/hmdb_ucf-TemPooling-RevGrad_00_05_00.pdf}
    \caption{TemPooling + RevGrad~\cite{ganin2015unsupervised}}
    \label{fig:tSNE_TemPooling_G}
  \end{subfigure}
  \begin{subfigure}[b]{0.235\textwidth}
    \includegraphics[width=\textwidth]{figs/tSNE/hmdb_ucf-TemRelation-RevGrad_075_075_05-TransAttn-gamma_0003.pdf}
    \caption{TAN}
    \label{fig:tSNE_TAAAN}
  \end{subfigure}
\caption{The comparison of t-SNE visualization. The blue and orange dots represent source and target data. 
}
\label{fig:tSNE}
\end{figure}

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
