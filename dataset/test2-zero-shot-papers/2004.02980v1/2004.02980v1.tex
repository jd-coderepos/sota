\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{verbatim}            \usepackage{booktabs}
\usepackage{relsize}
\usepackage{scrextend}           \usepackage{multirow}
\usepackage{array}               \usepackage{makecell}            \usepackage{subcaption}          \usepackage{mathtools}           \usepackage{siunitx}             \usepackage{wrapfig}
\allowdisplaybreaks              \usepackage{float}
\usepackage{cite}
\usepackage{caption}
\captionsetup[table]{skip=0.1cm} 

\graphicspath{{fig/}}
\usepackage{bm}
\usepackage{paralist}

\newcommand{\example}{{e.g.}}
\newcommand{\thatIs}{{i.e.}}
\newcommand{\etcetera}{\textit{etc.}}

\newcommand{\bmu}{\boldsymbol{\mu}_{ij}}
\newcommand{\bmux}{\mu_{ijx}}
\newcommand{\bmuy}{\mu_{ijy}}
\newcommand{\bmuF}{\boldsymbol{\mu}_{Kj}}
\newcommand{\bmuFx}{{\mu_{Kj}}_{x}}
\newcommand{\bmuFy}{{\mu_{Kj}}_{y}}
\newcommand{\ground}{\mathbf{p}_j}
\newcommand{\groundx}{{p}_{jx}}
\newcommand{\groundy}{{p}_{jy}}
\newcommand{\Real}{\mathbb{R}}

\newcommand{\bSigmaNoSub}{\boldsymbol{\Sigma}}
\newcommand{\bSigma}{\boldsymbol{\Sigma}_{ij}}
\newcommand{\bSigmaF}{\boldsymbol{\Sigma}_{Kj}}
\newcommand{\bSigmaFxx}{{\Sigma_{Kj}}_{xx}}
\newcommand{\bSigmaFyy}{{\Sigma_{Kj}}_{yy}}
\newcommand{\bSigmaFxy}{{\Sigma_{Kj}}_{xy}}
\newcommand{\bSigmaInv}{\bm{\Sigma}_{ij}^{-1}}
\newcommand{\bh}{\bm{H}_{ij}}
\newcommand{\bL}{\bm{L}_{ij}}
\newcommand{\bLT}{\bm{L}_{ij}^{T}}
\newcommand{\eLone}{L_{ij1}^{}}
\newcommand{\eLtwo}{L_{ij2}^{}}
\newcommand{\eLthree}{L_{ij3}^{}}
\newcommand{\loss}{\mathcal{L}}
\newcommand{\visgd}{v_{j}}
\newcommand{\vispred}{\widehat{v}_{ij}}
\newcommand{\lonedistance}{\ell_1}
\newcommand{\ltwodistance}{\ell_2}

\newcommand{\threehundredW}{-W}
\newcommand{\threehundredLP}{W-LP-D}
\newcommand{\menpoTwoD}{Menpo D}
\newcommand{\cofwSixtyEight}{COFW-}
\newcommand{\aflwNineteen}{AFLW-}
\newcommand{\wflwNinetyEight}{WFLW-}
\newcommand{\ourdataset}{MERL-RAV}

\newcommand{\threehundredWHeading}{300-W}
\newcommand{\aflwNineteenHeading}{AFLW-19}
\newcommand{\ourdatasetHeading}{MERL-RAV}

\newcommand{\nmebox}{}
\newcommand{\nmeboxvis}{}
\newcommand{\nmediag}{}
\newcommand{\nmeocular}{}
\newcommand{\aucbox}{}
\newcommand{\aucocular}{}
\newcommand{\frocular}{}

\newcommand{\zerovec}{\bm{0}}

\newcommand{\first}[1]{}
\newcommand{\second}[1]{}
\newcommand{\firstkey}[1]{\textcolor{red}{\textbf{#1}}}
\newcommand{\secondkey}[1]{\textcolor{blue}{\textbf{#1}}}

\newcommand{\var}{{\rm var}}
\newcommand{\Tr}{^{\rm T}}
\newcommand{\vtrans}[2]{{#1}^{(#2)}}
\newcommand{\kron}{\otimes}
\newcommand{\schur}[2]{({#1} | {#2})}
\newcommand{\schurdet}[2]{\left| ({#1} | {#2}) \right|}
\newcommand{\had}{\circ}
\newcommand{\diag}{{\rm diag}}
\newcommand{\invdiag}{\diag^{-1}}
\newcommand{\rank}{{\rm rank}}
\newcommand{\nullsp}{{\rm null}}
\newcommand{\tr}{{\rm tr}}
\renewcommand{\vec}{{\rm vec}}
\newcommand{\vech}{{\rm vech}}
\renewcommand{\det}[1]{\left| #1 \right|}
\newcommand{\pdet}[1]{\left| #1 \right|_{+}}
\newcommand{\pinv}[1]{#1^{+}}
\newcommand{\erf}{{\rm erf}}
\newcommand{\hypergeom}[2]{{}_{#1}F_{#2}}

\renewcommand{\a}{{\bf a}}
\renewcommand{\b}{{\bf b}}
\renewcommand{\c}{{\bf c}}
\renewcommand{\d}{{\rm d}}  \newcommand{\e}{{\bf e}}
\newcommand{\f}{{\bf f}}
\newcommand{\g}{{\bf g}}
\newcommand{\h}{{\bf h}}
\renewcommand{\k}{{\bf k}}
\newcommand{\m}{{\bf m}}
\newcommand{\mb}{{\bf m}}
\newcommand{\n}{{\bf n}}
\renewcommand{\o}{{\bf o}}
\newcommand{\p}{{\bf p}}
\newcommand{\q}{{\bf q}}
\renewcommand{\r}{{\bf r}}
\newcommand{\s}{{\bf s}}
\renewcommand{\t}{{\bf t}}
\renewcommand{\u}{{\bf u}}
\renewcommand{\v}{{\bf v}}
\newcommand{\w}{{\bf w}}
\newcommand{\x}{{\bf x}}
\newcommand{\y}{{\bf y}}
\newcommand{\z}{{\bf z}}
\newcommand{\A}{{\bf A}}
\newcommand{\B}{{\bf B}}
\newcommand{\C}{{\bf C}}
\newcommand{\D}{{\bf D}}
\newcommand{\E}{{\bf E}}
\newcommand{\F}{{\bf F}}
\newcommand{\G}{{\bf G}}
\renewcommand{\H}{{\bf H}}
\newcommand{\I}{{\bf I}}
\newcommand{\J}{{\bf J}}
\newcommand{\K}{{\bf K}}
\renewcommand{\L}{{\bf L}}
\newcommand{\M}{{\bf M}}
\newcommand{\N}{\mathcal{N}}  \newcommand{\Dcal}{\mathcal{D}}  

\renewcommand{\O}{{\bf O}}
\renewcommand{\P}{{\mathcal{P}}}
\newcommand{\Q}{{\bf Q}}
\newcommand{\R}{{\bf R}}
\renewcommand{\S}{{\bf S}}
\newcommand{\T}{{\bf T}}
\newcommand{\U}{{\bf U}}
\newcommand{\V}{{\bf V}}
\newcommand{\W}{{\bf W}}
\newcommand{\X}{{\bf X}}
\newcommand{\Y}{{\bf Y}}
\newcommand{\Z}{{\bf Z}}

\newcommand{\bfLambda}{\boldsymbol{\Lambda}}

\newcommand{\bsigma}{\boldsymbol{\sigma}}
\newcommand{\balpha}{\boldsymbol{\alpha}}
\newcommand{\bpsi}{\boldsymbol{\psi}}
\newcommand{\bphi}{\boldsymbol{\phi}}
\newcommand{\boldeta}{\boldsymbol{\eta}}
\newcommand{\Beta}{\boldsymbol{\eta}}
\newcommand{\btau}{\boldsymbol{\tau}}
\newcommand{\bvarphi}{\boldsymbol{\varphi}}
\newcommand{\bzeta}{\boldsymbol{\zeta}}

\newcommand{\blambda}{\boldsymbol{\lambda}}
\newcommand{\bLambda}{\mathbf{\Lambda}}
\newcommand{\bOmega}{\mathbf{\Omega}}
\newcommand{\bomega}{\mathbf{\omega}}
\newcommand{\bPi}{\mathbf{\Pi}}

\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bpi}{\boldsymbol{\pi}}
\newcommand{\bxi}{\boldsymbol{\xi}}



\newcommand{\bgamma}{\boldsymbol{\gamma}}
\newcommand{\bGamma}{\mathbf{\Gamma}}

\newcommand{\1}{{\bf 1}}
\newcommand{\0}{{\bf 0}}



\newcommand{\bs}{\backslash}
\newcommand{\ben}{\begin{enumerate}}
\newcommand{\een}{\end{enumerate}}

 \newcommand{\notS}{{\backslash S}}
 \newcommand{\nots}{{\backslash s}}
 \newcommand{\noti}{{\backslash i}}
 \newcommand{\notj}{{\backslash j}}
 \newcommand{\nott}{\backslash t}
 \newcommand{\notone}{{\backslash 1}}
 \newcommand{\nottp}{\backslash t+1}


\newcommand{\notk}{{^{\backslash k}}}
\newcommand{\notij}{{^{\backslash i,j}}}
\newcommand{\notg}{{^{\backslash g}}}
\newcommand{\wnoti}{{_{\w}^{\backslash i}}}
\newcommand{\wnotg}{{_{\w}^{\backslash g}}}
\newcommand{\vnotij}{{_{\v}^{\backslash i,j}}}
\newcommand{\vnotg}{{_{\v}^{\backslash g}}}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\msgb}{m_{t \leftarrow t+1}}
\newcommand{\msgf}{m_{t \rightarrow t+1}}
\newcommand{\msgfp}{m_{t-1 \rightarrow t}}

\newcommand{\proj}[1]{{\rm proj}\negmedspace\left[#1\right]}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}

\newcommand{\dif}{\mathrm{d}}
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\norm}[1]{\lVert#1\rVert}

\newcommand{\EE}{\mathbb{E}}
\newcommand{\VV}{\mathbb{V}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\sbr}[1]{\left[#1\right]}
\newcommand{\rbr}[1]{\left(#1\right)}
\newcommand{\cmt}[1]{}
\newcommand{\dt}{\Delta t}

 
\usepackage{pifont}\newcommand{\cmark}{\checkmark}\newcommand{\xmark}{\ding{53}}

\usepackage{xcolor}
\newcommand{\abhinav}[1]{\textcolor{green}{\emph{{Abhinav:}~{#1}}}}
\newcommand{\tim}[1]{\textcolor{purple}{\emph{{Tim:}~{#1}}}}
\newcommand{\mike}[1]{\textcolor{red}{\emph{{Mike:}~{#1}}}}

\newcommand{\myMidRule}{\Xhline{1.5\arrayrulewidth}}
\newcommand{\myTopRule}{\Xhline{2\arrayrulewidth}}
\newcolumntype{t}{!{\vrule width 2\arrayrulewidth}}
\newcolumntype{m}{!{\vrule width 2.5\arrayrulewidth}}
\newcommand\scalemath[2]{\scalebox{#1}{\mbox{\ensuremath{\displaystyle #2}}}}

\usepackage{tikz} \usetikzlibrary{arrows}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{shapes.arrows}
\usetikzlibrary{fadings, shadows}
\usetikzlibrary{decorations.text}
\definecolor{my_green}{rgb}{0.0, 0.9, 0.24}
\definecolor{my_green_2}{rgb}{0.0, 0.4, 0.0}
\definecolor{my_violet}{rgb}{0.79, 0.40, 1}
\definecolor{my_blue}{rgb}{0.2, 0.6, 1}
\definecolor{my_yellow}{rgb}{0.9, 0.8, 0.54}
\definecolor{my_yellow_2}{rgb}{1, 0.75, 0.}
\definecolor{my_red}{rgb}{1,0,0}
\definecolor{my_purple}{rgb}{0.27,0.8, 0.8}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy

\def\cvprPaperID{7427} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

\setlength{\abovedisplayskip}{0.075cm}\setlength{\belowdisplayskip}{0.075cm}\setlength{\abovedisplayshortskip}{0.075cm}\setlength{\belowdisplayshortskip}{0.075cm}

\title{LUVLi Face Alignment: Estimating Landmarks'\\
Location, Uncertainty, and Visibility Likelihood}

\author{Abhinav Kumar\thanks{Equal Contributions} , Tim K. Marks\footnotemark[1]  , Wenxuan Mou\footnotemark[1] ,\\ Ye Wang, Michael Jones, Anoop Cherian, Toshiaki Koike-Akino, Xiaoming Liu, Chen Feng\\
{\tt\small abhinav3663@gmail.com, tmarks@merl.com, wenxuanmou@gmail.com, }\\
{\tt\small ywang, mjones, cherian, koike@merl.com, liuxm@cse.msu.edu, cfeng@nyu.edu }\\
{\footnotesize University of Utah, Mitsubishi Electric Research Labs (MERL),  University of Manchester, Michigan State University, New York University}
}


\maketitle
\thispagestyle{empty}

\begin{abstract}
Modern face alignment methods have become quite accurate at predicting the locations of facial landmarks, but they do not typically estimate the uncertainty of their predicted locations nor predict whether landmarks are visible. In this paper, we present a novel framework for jointly predicting landmark locations, associated uncertainties of these predicted locations, and landmark visibilities. We model these as mixed random variables and estimate them using a deep network trained with our proposed Location, Uncertainty, and Visibility Likelihood (LUVLi) loss. In addition, we release an entirely new labeling of a large face alignment dataset with over 19,000 face images in a full range of head poses. Each face is manually labeled with the ground-truth locations of 68 landmarks, with the additional information of whether each landmark is unoccluded, self-occluded (due to extreme head poses), or externally occluded. Not only does our joint estimation yield accurate estimates of the uncertainty of predicted landmark locations, but it also yields state-of-the-art estimates for the landmark locations themselves on multiple standard face alignment datasets. Our method's estimates of the uncertainty of predicted landmark locations could be used to automatically identify input images on which face alignment fails, which can be critical for downstream tasks.
\end{abstract}


\vspace{-0.5cm}
\section{Introduction}
    Modern methods for face alignment (facial landmark localization) perform quite well most of the time, but all of them fail some percentage of the time. Unfortunately, almost all of the state-of-the-art (SOTA) methods simply output predicted landmark locations, with no assessment of whether (or how much) downstream tasks should {\it trust} these landmark locations. This is concerning, as face alignment is a key pre-processing step in numerous safety-critical applications, including advanced driver assistance systems (ADAS), driver monitoring, and remote measurement of vital signs~\cite{nowara2018sparseppg}. As deep neural networks are notorious for producing overconfident predictions~\cite{guo2017calibration}, similar concerns have been raised for other neural network technologies~\cite{le2018uncertainty}, and they become even more acute in the era of adversarial machine learning where adversarial images may pose a great threat to a system~\cite{chen2019face}. However, previous work in face alignment (and landmark localization in general) has largely ignored the area of uncertainty estimation.
    
    To address this need, we propose a method to jointly estimate facial landmark locations and a parametric probability distribution representing the uncertainty of each estimated location. Our model also jointly estimates the visibility of landmarks, which predicts whether each landmark is occluded due to extreme head pose.

    \begin{figure}[!tb]
    	\centering
    	\begin{subfigure}{0.3\linewidth}
    		\includegraphics[height=\linewidth]{fig/qualitative/300W_test/all_with_all_landmarks/indoor_003.png}
    	\end{subfigure}
    	\hfill
    	\begin{subfigure}{0.3\linewidth}
    		\includegraphics[height=\linewidth]{fig/qualitative/300W_test/all_with_all_landmarks/outdoor_198.png}
    	\end{subfigure}\hfill
    	\begin{subfigure}{0.3\linewidth}
    		\includegraphics[height=\linewidth]{fig/qualitative/300W_test/all_with_all_landmarks/outdoor_174.png}
    	\end{subfigure}\caption{Results of our joint face alignment and uncertainty estimation on three test images. Ground-truth (green) and predicted (yellow) landmark locations are shown. The estimated uncertainty of the predicted location of each landmark is shown in blue (Error ellipse for Mahalanobis distance 1). Landmarks that are occluded (\example, by the hand in center image) tend to have larger uncertainty. 
    	}
    	\vspace{-0.25cm}
    	\label{fig:fig1}
    \end{figure}
    
    We find that the choice of methods for calculating mean and covariance is crucial. Landmark locations are best obtained using heatmaps, rather than by direct regression. To estimate landmark locations in a differentiable manner using heatmaps, we do not select the location of the maximum (argmax) of each landmark's heatmap, but instead propose to use the spatial mean of the positive elements of each heatmap. Unlike landmark locations, uncertainty distribution parameters are best obtained by direct regression rather than from heatmaps. To estimate the uncertainty of the predicted locations, we add a Cholesky Estimator Network (CEN) branch to estimate the covariance matrix of a multivariate Gaussian or Laplacian probability distribution. To estimate visibility of each landmark, we add a Visibility Estimator Network (VEN). We combine these estimates using a joint loss function that we call the Location, Uncertainty and Visibility Likelihood (LUVLi) loss. Our primary goal in designing this model was to estimate uncertainty in landmark localization. In the process, not only does our method yields accurate uncertainty estimation, but it also produces SOTA landmark localization results on several face alignment datasets.

    Uncertainty can be broadly classified into two categories~\cite{kendall2017uncertainties}: \emph{epistemic} uncertainty is related to a lack of knowledge about the model that generated the observed data, and \emph{aleatoric} uncertainty
    is related to the noise inherent in the observations, \example, sensor or labelling noise. The ground-truth landmark locations marked on an image by human labelers would vary across multiple labelings of an image by different human labelers (or even by the same human labeler). Furthermore, this variation will itself vary across different images and landmarks (\example, it will vary more for occluded landmarks and poorly lit images). The goal of our method is to estimate this aleatoric uncertainty.
    
    The fact that each image only has one ground-truth labeled location per landmark makes estimating this uncertainty distribution difficult, but not impossible. To do so, we use a parametric model for the uncertainty distribution. We train a neural network to estimate the parameters of the model for each landmark of each input face image so as to maximize the likelihood under the model of the ground-truth location of that landmark (summed across all landmarks of all training faces).

    The main contributions of this work are as follows:
    \begin{compactitem}
        \item This is the first work to introduce the concept of parametric uncertainty estimation for face alignment.
        \item We propose an end-to-end trainable model for the joint estimation of landmark location, uncertainty, and visibility likelihood (LUVLi), modeled as a mixed random variable.
        \item We compare our model using multivariate Gaussian and multivariate Laplacian probability distributions. 
        \item Our algorithm yields accurate uncertainty estimation and state-of-the-art landmark localization results on several face alignment datasets.
        \item We are releasing a new dataset with manual labels of the locations of  landmarks on over  face images in a wide variety of poses, where each landmark is also labeled with one of three visibility categories.
    \end{compactitem}



\section{Related Work}
\subsection{Face Alignment}
\vspace{-0.15cm}

    Early methods for face alignment were based on Active Shape Models (ASM) and Active Appearance Models (AAM)~\cite{cootes1995active, cootes2001active,sauer2011accurate,sung2009adaptive,tzimiropolous2013aam} as well as their  variations~\cite{romdhani1999multiview,cootes2002viewAAM,Asthana2011ICCV3DposeNorm,hu2004multicamAAM, liu2008discriminative,liu2010video}. Subsequently, direct regression methods became popular due to their excellent performance. Of these, tree-based regression methods~\cite{ren2014face,kazemi2014one,cao2014face,cootes2012robust,tuzel2008learning} proved particularly fast, and the subsequent cascaded regression methods~\cite{dollar2010cascaded,xiong2013supervised,asthana2014incremental,tzimiropoulos2015POCR,tuzel2016robust} improved accuracy.
    
    Recent approaches~\cite{Zhang2014eccvCFAN, toshev2014deeppose, zhu2015face, yang2017stacked, bulat2017far, wu2018look, tang2019towards, wang2019adaptive} are all based on deep learning and can be classified into two sub-categories: direct regression~\cite{toshev2014deeppose, carreira2016human} and heatmap-based approaches. The SOTA deep methods, \example, stacked hourglass networks~\cite{yang2017stacked, bulat2017far} and densely connected U-nets (DU-Net)~\cite{tang2019towards}, use a cascade of deep networks, originally developed for human body D pose estimation~\cite{newell2016stacked}. These models~\cite{newell2016stacked, bulat2017far, tang2018quantized, tang2019towards} are trained using the  distance between the predicted heatmap for each landmark and a proxy ground-truth heatmap that is generated by placing a symmetric Gaussian distribution with small fixed variance at the ground-truth landmark location.~\cite{li2019rethinking} uses a larger variance for early hourglasses and a smaller variance for later hourglasses.~\cite{wang2019adaptive} employs different variations of MSE for different pixels of the proxy ground-truth heatmap. Recent works also infer facial boundary maps to improve alignment~\cite{wu2018look,wang2019adaptive}. In heatmap-based methods,  landmarks are estimated by the argmax of each predicted heatmap. Indirect inference through a predicted heatmap offers several advantages over direct prediction~\cite{Belagiannis17}.
    
    \textbf{Disadvantages of Heatmap-Based Approaches.} These heatmap-based methods have at least two disadvantages. First, since the goal of training is to mimic a proxy ground-truth heatmap containing a fixed symmetric Gaussian, the predicted heatmaps are poorly suited to uncertainty prediction~\cite{kdnuncertain, chen2019face}. Second, they suffer from quantization errors since the heatmap's argmax is only determined to the nearest pixel~\cite{luvizon20182d,nibali2018numerical,tai2019towards}. To achieve sub-pixel localization for body pose estimation,~\cite{luvizon20182d} replaces the argmax with a spatial mean over the softmax. Alternatively, for sub-pixel localization in videos, \cite{tai2019towards} samples two additional points adjacent to the max of the heatmap to estimate a local peak.  
    
    \textbf{Landmark Regression with Uncertainty.}
    We have only found two other methods that estimate uncertainty of landmark regression, both developed concurrently with our approach. The first method~\cite{kdnuncertain, chen2019face} estimates face alignment uncertainty using a non-parametric approach: a kernel density network obtained by convolving the heatmaps with a fixed symmetric Gaussian kernel. The second~\cite{gundavarapu2019structured} performs body pose estimation with uncertainty using direct regression method (no heatmaps) to directly predict the mean and precision matrix of a Gaussian distribution.

    
\subsection{Uncertainty Estimation in Neural Networks} 
    \vspace{-0.2cm}\
    Uncertainty estimation broadly uses two types of approaches~\cite{le2018uncertainty}: sampling-based and sampling-free. Sampling-based methods include Bayesian neural networks~\cite{shridhar2019comprehensive}, Monte Carlo dropout~\cite{gal2016dropout}, and bootstrap ensembles~\cite{lakshminarayanan2017simple}. They rely on multiple evaluations of the input to estimate uncertainty~\cite{le2018uncertainty}, and bootstrap ensembles also need to store several sets of weights~\cite{ilg2018uncertainty}. Thus, sampling-based methods work for small D regression problems but might not be feasible for higher-dimensional problems~\cite{ilg2018uncertainty}. 

    Sampling-free methods produce two outputs, one for the estimate and the other for the uncertainty, and optimize Gaussian log-likelihood (GLL) instead of classification and regression losses~\cite{kendall2017uncertainties, lakshminarayanan2017simple,le2018uncertainty}.~\cite{lakshminarayanan2017simple} combines the benefits of sampling-free and sampling-based methods. 
    
    Recent object detection methods have used uncertainty estimation~\cite{le2018uncertainty, jiang2018acquisition, levi2019evaluating, he2019bounding, miller2019benchmarking, harakeh2019bayesod,atoum2017monocular}. Sampling-free methods~\cite{le2018uncertainty, levi2019evaluating, he2019bounding} jointly estimate the four parameters of the bounding box using Gaussian log-likelihood~\cite{levi2019evaluating}, Laplacian log-likelihood~\cite{le2018uncertainty}, or both~\cite{he2019bounding}. However, these methods assume the four parameters of the bounding box are independent (assume a diagonal covariance matrix). Sampling-based approaches use Monte Carlo dropout~\cite{miller2019benchmarking} and network ensembles~\cite{lakshminarayanan2017simple} for object detection.
    Uncertainty estimation has also been applied to pixelwise depth regression~\cite{kendall2017uncertainties}, optical flow~\cite{ilg2018uncertainty}, pedestrian detection~\cite{neumann2018relaxed,bhattacharyya2018long,bertoni2019monoloco} and D vehicle detection~\cite{feng2018towards}.
    


\section{Proposed Method}\label{sec:method}
\vspace{-0.15cm}
    Figure~\ref{fig:overview} shows an overview of our LUVLi Face Alignment. The input RGB face image is passed through a \mbox{DU-Net}~\cite{tang2019towards} architecture, to which we add three additional components branching from each U-net. The first new component is a \emph{mean estimator}, which computes the estimated location of each landmark as the weighted spatial mean of the positive elements of the corresponding heatmap. The second and the third new component, the \emph{Cholesky Estimator Network} (CEN) and the \emph{Visibility Estimator Network} (VEN), emerge from the bottleneck layer of each U-net. CEN and VEN weights are shared across all U-nets. The CEN estimates the Cholesky coefficients of the covariance matrix for each landmark location. The VEN estimates the probability of visibility of each landmark in the image,  meaning visible and  meaning not visible. For each U-net  and each landmark , the landmark's location estimate , estimated covariance matrix , and estimated visibility  are tied together by the LUVLi loss function , which enables end-to-end optimization of the entire framework.
    \begin{figure}[!tb]
        \centering
        \begin{tikzpicture}[scale=.4, every node/.style={scale=0.7}, every edge/.style={scale=0.7}]
\tikzset{vertex/.style = {shape=circle, draw=black!70, line width=0.06em, minimum size=1.4em}}
\tikzset{edge/.style = {-{Triangle[angle=60:.06cm 1]},> = latex'}}

    \node[inner sep=0pt] (input)      at (3.15,0)     {\includegraphics[width=2cm]{fig/qualitative/300W_test/original/indoor_003.png}};
    \node[inner sep=0pt] (prediction) at (18.82,-11.25)    {\includegraphics[width=1.7cm]{fig/qualitative/300W_test/all_with_all_landmarks/indoor_003.png}};
    
\draw [draw=black!70, fill= my_violet!90, line width=0.06em] 
       (5.15,1.8) node[]{}
    -- (5.15,-1.8) node[]{}
    -- (5.95,-1.3) node[]{}
    -- (5.95,1.3) node[]{}
    -- cycle;
\draw [draw=black!70, fill=white!55, line width=0.06em] 
    (6.1,1.3) node[]{}
    -- (6.1,-1.3) node[]{}
    -- (7.832,0) node[]{}
    -- (9.564,1.3) node[]{}
    -- (9.564,-1.3) node[]{}
    -- cycle;
    \draw [-{Triangle[angle=60:.1cm 1]}, draw=black!70, line width=0.06em, shorten <=1pt, shorten >=1pt, >=stealth] 
        (6.68, 0.84) node[]{} to [bend left= 70] (8.98, 0.84) node[]{};
    \draw [-{Triangle[angle=60:.1cm 1]}, draw=black!70, line width=0.06em, shorten <=1pt, shorten >=1pt, >=stealth] 
        (7.25, 0.42) node[]{} to [bend left= 70] (8.4, 0.42) node[]{};      
    
\draw [draw=black!70, fill= black!20, line width=0.06em] 
       (9.7,1.3) node[]{}
    -- (9.7,-1.3) node[]{}
    -- (10.05,-1.3) node[]{}
    -- (10.05,1.3) node[]{}
    -- cycle;
    \draw [draw=black!70, fill= my_yellow!90, line width=0.06em] 
       (8.9,-1.8) node[]{}
    -- (10.3,-1.8) node[]{}
    -- (10.3,-2.2) node[]{}
    -- (8.9,-2.2) node[]{}
    -- cycle;
    \draw [draw=black!70, fill= my_blue!75, line width=0.06em] 
       (7.53,-1.2) node[]{}
    -- (8.53,-1.2) node[]{}
    -- (8.33,-2.2) node[scale=0.6, pos=.25] {CEN~~~~~~~~~~}
    -- (7.73,-2.2) node[]{}
    -- cycle;
    \draw [draw=black!70, fill= my_purple!75, line width=0.06em] 
       (6.43,-1.2) node[]{}
    -- (7.43,-1.2) node[]{}
    -- (7.23,-2.2) node[scale=0.6, pos=.25] {VEN~~~~~~~~~~~~}
    -- (6.63,-2.2) node[]{}
    -- cycle;
    
\draw [-, draw=my_purple!75, line width=0.15em, shorten <=0.5pt, shorten >=0.5pt, >=stealth]
       (7.83,0) node[]{}
    -- (6.93,-1.2) node[]{};
    \draw [-{Triangle[angle=60:.1cm 1]}, draw=my_purple!75, line width=0.15em, shorten <=0.5pt, shorten >=0.5pt, >=stealth]
       (6.93,-2.2) node[]{}
    -- (6.93,-3.0) node[]{};
    \draw [-{Triangle[angle=60:.1cm 1]}, draw=black!40, line width=0.08em, shorten <=2pt, shorten >=2pt, >=stealth]
       (7.63,-1.4) node[]{}
    -- (6.08,-2.2) node[]{};
    
\draw [-{Triangle[angle=60:.1cm 1]}, draw=black!70, line width=0.08em, shorten <=2pt, shorten >=2pt, >=stealth]
       (8.83,-1.4) node[]{}
    -- (7.18,-2.2) node[]{};
    \draw [-, draw=my_blue!75, line width=0.15em, shorten <=0.5pt, shorten >=0.5pt, >=stealth]
       (7.83,0) node[]{}
    -- (7.93,-1.2) node[]{};
    \draw [-{Triangle[angle=60:.1cm 1]}, draw=my_blue!75, line width=0.15em, shorten <=0.5pt, shorten >=0.5pt, >=stealth]
       (7.98,-2.2) node[]{}
    -- (7.98,-3.0) node[]{};
    \draw [-, draw=my_yellow!90, line width=0.15em, shorten <=0.5pt, shorten >=0.5pt, >=stealth]
       (9.56,-1.3) node[]{}
    -- (9.56,-1.8) node[]{};
    \draw [-{Triangle[angle=60:.1cm 1]}, draw=my_yellow!90, line width=0.15em, shorten <=0.5pt, shorten >=0.5pt, >=stealth]
       (9.56,-2.2) node[]{}
    -- (9.56,-3.0) node[]{}; 
  
\draw[black,fill=my_red!45] (6.53,-3) rectangle (9.96,-4.5) node[pos=.5] {LUVLi};

	\node[vertex, fill=black!70, scale=0.15] (d030)  at (10.55,0) {};
	\node[vertex, fill=black!70, scale=0.15] (d031)  at (10.85,0)  {};
	\node[vertex, fill=black!70, scale=0.15] (d032)  at (11.15,0)  {};

    \draw [draw=black!70, fill= black!20, line width=0.06em] 
       (11.65,1.3) node[]{}
    -- (11.65,-1.3) node[]{}
    -- (12.00,-1.3) node[]{}
    -- (12.00,1.3) node[]{}
    -- cycle;

\draw [draw=black!70, fill=white!55, line width=0.06em] 
       (12.15,1.3) node[]{}
    -- (12.15,-1.3) node[]{}
    -- (13.902,0) node[]{}
    -- (15.614,1.3) node[]{}
    -- (15.614,-1.3) node[]{}
    -- cycle;
    \draw [-{Triangle[angle=60:.1cm 1]}, draw=black!70, line width=0.06em, shorten <=1pt, shorten >=1pt, >=stealth] 
        (12.73, 0.84) node[]{} to [bend left= 70] (15.03, 0.84) node[]{};
    \draw [-{Triangle[angle=60:.1cm 1]}, draw=black!70, line width=0.06em, shorten <=1pt, shorten >=1pt, >=stealth] 
        (13.31, 0.42) node[]{} to [bend left= 70] (14.45, 0.42) node[]{};    
    \draw [draw=black!70, fill= black!20, line width=0.06em] 
       (15.75,1.3) node[]{}
    -- (15.75,-1.3) node[]{}
    -- (16.1,-1.3) node[]{}
    -- (16.1,1.3) node[]{}
    -- cycle;
    
\draw [draw=black!70, fill= my_yellow!90, line width=0.06em] 
       (14.95,-1.8) node[]{}
    -- (16.35,-1.8) node[]{}
    -- (16.35,-2.2) node[]{}
    -- (14.95,-2.2) node[]{}
    -- cycle; 
    \draw [draw=black!70, fill= my_blue!75, line width=0.06em] 
       (13.55,-1.2) node[]{}
    -- (14.55,-1.2) node[]{}
    -- (14.35,-2.2) node[scale=0.6, pos=.25] {CEN~~~~~~~~~~}
    -- (13.75,-2.2) node[]{}
    -- cycle;
    \draw [draw=black!70, fill= my_purple!75, line width=0.06em] 
       (12.45,-1.2) node[]{}
    -- (13.45,-1.2) node[]{}
    -- (13.25,-2.2) node[scale=0.6, pos=.25] {VEN~~~~~~~~~~~~}
    -- (12.65,-2.2) node[]{}
    -- cycle;

\draw [-, draw=my_purple!75, line width=0.15em, shorten <=0.5pt, shorten >=0.5pt, >=stealth]
       (13.88,0) node[]{}
    -- (12.88,-1.2) node[]{};
    \draw [-{Triangle[angle=60:.1cm 1]}, draw=my_purple!75, line width=0.15em, shorten <=0.5pt, shorten >=0.5pt, >=stealth]
       (12.88,-2.2) node[]{}
    -- (12.88,-3.0) node[pos=0.5,scale= 0.9]{~~~~~~~~~~};
    \draw [-{Triangle[angle=60:.1cm 1]}, draw=black!40, line width=0.08em, shorten <=2pt, shorten >=2pt, >=stealth]
       (13.65,-1.4) node[]{}
    -- (12.1,-2.2) node[]{};

\draw [-{Triangle[angle=60:.1cm 1]}, draw=black!70, line width=0.08em, shorten <=2pt, shorten >=2pt, >=stealth]
       (14.85,-1.4) node[]{}
    -- (13.2,-2.2) node[]{};
    \draw [-, draw=my_blue!75, line width=0.15em, shorten <=0.5pt, shorten >=0.5pt, >=stealth]
       (13.88,0) node[]{}
    -- (14.05,-1.2) node[]{};
    \draw [-{Triangle[angle=60:.1cm 1]}, draw=my_blue!75, line width=0.15em, shorten <=0.5pt, shorten >=0.5pt, >=stealth]
       (14.05,-2.2) node[]{}
    -- (14.05,-3.0) node[pos=0.5,scale= 0.9]{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~};
    \draw [-, draw=my_yellow!90, line width=0.15em, shorten <=0.5pt, shorten >=0.5pt, >=stealth]
       (15.61,-1.3) node[]{}
    -- (15.61,-1.8) node[]{};
    \draw [-{Triangle[angle=60:.1cm 1]}, draw=my_yellow!90, line width=0.15em, shorten <=0.5pt, shorten >=0.5pt, >=stealth]
       (15.61,-2.2) node[]{}
    -- (15.61,-3.0) node[pos=0.5,scale= 0.9]{~~~~~~~~~~~~~~};     

\draw[black,fill=my_red!45] (12.68,-3) rectangle (16.01,-4.5) node[pos=.5] {LUVLi};     

	\node[vertex, fill=black!70, scale=0.15] (d030)  at (16.61,0) {};
	\node[vertex, fill=black!70, scale=0.15] (d031)  at (16.91,0)  {};
	\node[vertex, fill=black!70, scale=0.15] (d032)  at (17.21,0)  {};

    \draw [draw=black!70, fill= black!20, line width=0.06em] 
       (17.71,1.3) node[]{}
    -- (17.71,-1.3) node[]{}
    -- (18.06,-1.3) node[]{}
    -- (18.06,1.3) node[]{}
    -- cycle;

\draw [draw=black!70, fill=white!55, line width=0.06em] 
       (18.2,1.3) node[]{}
    -- (18.2,-1.3) node[]{}
    -- (19.922,0) node[]{}
    -- (21.664,1.3) node[]{}
    -- (21.664,-1.3) node[]{}
    -- cycle;
    \draw [-{Triangle[angle=60:.1cm 1]}, draw=black!70, line width=0.06em, shorten <=1pt, shorten >=1pt, >=stealth] 
        (18.78, 0.84) node[]{} to [bend left= 70] (21.08, 0.84) node[]{};
    \draw [-{Triangle[angle=60:.1cm 1]}, draw=black!70, line width=0.06em, shorten <=1pt, shorten >=1pt, >=stealth] 
        (19.35, 0.42) node[]{} to [bend left= 70] (20.5, 0.42) node[]{};
   
\draw [draw=black!70, fill= my_yellow!90, line width=0.06em] 
       (21.,-1.8) node[]{}
    -- (22.4,-1.8) node[]{}
    -- (22.4,-2.2) node[]{}
    -- (21.,-2.2) node[]{}
    -- cycle;    
    \draw [draw=black!70, fill= my_blue!75, line width=0.06em] 
       (19.6,-1.2) node[]{}
    -- (20.6,-1.2) node[]{}
    -- (20.4,-2.2) node[scale=0.6, pos=.25] {CEN~~~~~~~~~~~~}
    -- (19.8,-2.2) node[]{}
    -- cycle;
    \draw [draw=black!70, fill= my_purple!75, line width=0.06em] 
       (18.5,-1.2) node[]{}
    -- (19.5,-1.2) node[]{}
    -- (19.3,-2.2) node[scale=0.6, pos=.25] {VEN~~~~~~~~~~~~}
    -- (18.7,-2.2) node[]{}
    -- cycle;
    
\draw [-, draw=my_purple!75, line width=0.15em, shorten <=0.5pt, shorten >=0.5pt, >=stealth]
       (19.93,0) node[]{}
    -- (18.93,-1.2) node[]{};
    \draw [-{Triangle[angle=60:.1cm 1]}, draw=my_purple!75, line width=0.15em, shorten <=0.5pt, shorten >=0.5pt, >=stealth]
       (18.93,-2.2) node[]{}
    -- (18.93,-3.0) node[]{};
    \draw [-{Triangle[angle=60:.1cm 1]}, draw=black!40, line width=0.08em, shorten <=2pt, shorten >=2pt, >=stealth]
       (19.7,-1.4) node[]{}
    -- (18.15,-2.2) node[]{};
    
\draw [-{Triangle[angle=60:.1cm 1]}, draw=black!70, line width=0.08em, shorten <=2pt, shorten >=2pt, >=stealth]
       (20.9,-1.4) node[]{}
    -- (19.25,-2.2) node[]{};
    \draw [-, draw=my_blue!75, line width=0.15em, shorten <=0.5pt, shorten >=0.5pt, >=stealth]
       (19.93,0) node[]{}
    -- (20.1,-1.2) node[]{};
    \draw [-{Triangle[angle=60:.1cm 1]}, draw=my_blue!75, line width=0.15em, shorten <=0.5pt, shorten >=0.5pt, >=stealth]
       (20.1,-2.2) node[]{}
    -- (20.1,-3.0) node[]{};
    \draw [-, draw=my_yellow!90, line width=0.15em, shorten <=0.5pt, shorten >=0.5pt, >=stealth]
       (21.66,-1.3) node[]{}
    -- (21.66,-1.8) node[]{};
    \draw [-{Triangle[angle=60:.1cm 1]}, draw=my_yellow!90, line width=0.15em, shorten <=0.5pt, shorten >=0.5pt, >=stealth]
       (21.66,-2.2) node[]{}
    -- (21.66,-3.0) node[]{};     

\draw[black,fill=my_red!45] (18.63,-3) rectangle (22.03,-4.5) node[pos=.5] {LUVLi};


\node [draw=my_red!45, thick,inner sep= 3pt, minimum width= 9.4cm, minimum height= 5.52cm] (A2) at (14.3, -10.80) {};
\draw [draw=black!35, dashed, line width=0.06em, shorten <=0.5pt, shorten >=0.5pt, >=stealth]
           (12.68,-4.5) node[]{}
        -- (6.05,-5.8) node[]{};
    \draw [draw=black!35, dashed, line width=0.06em, shorten <=0.5pt, shorten >=0.5pt, >=stealth]
           (16.01,-4.5) node[]{}
        -- (22.61,-5.8) node[]{};  

\draw [-{Triangle[angle=60:.1cm 1]}, draw=my_blue!85, line width=0.15em, shorten <=0.5pt, shorten >=0.5pt, >=stealth]
       (10.5,-5.6) node[]{}
    -- (10.5,-8.3) node[]{};    
    \draw [draw=black!70, fill= my_blue!75, line width=0.06em] 
       (9,-6.1) rectangle (12,-7.5) node[pos=0.5, scale= 1.25]{~~};
       
    \draw [-{Triangle[angle=60:.1cm 1]}, draw=my_yellow!90, line width=0.15em, shorten <=0.5pt, shorten >=0.5pt, >=stealth]
       (18,-5.6) node[]{}
    -- (18,-8.3) node[]{};
    \draw [draw=black!70, fill= my_yellow!80, line width=0.06em] 
       (15,-6.1) rectangle (21,-7.5) node[pos=0.5, scale= 1.25]{Mean Estimator};

    \draw [-{Triangle[angle=60:.1cm 1]}, draw=my_purple!90, line width=0.15em, shorten <=0.5pt, shorten >=0.5pt, >=stealth]
       (7.2,-5.6) node[]{}
    -- (7.2,-8.1) node[]{};

\node [inner sep=1pt, scale= 1.25] at (7.2, -5.2)  {};
    \node [inner sep=1pt, scale= 1.25] at (10.5, -5.2) {};
    \node [inner sep=1pt, scale= 1.25] at (18, -5.2)   {};
    \node [inner sep=1pt, scale= 1.25] at (14.5, -14.5) {};
    \node [inner sep=1pt, scale= 1.25] at (11, -11) {Predictions};
    \node [inner sep=1pt, scale= 1.25] at (7.2, -8.5)  {};
    \node [inner sep=1pt, scale= 1.25] at (18.0, -8.5) {};
    \node [inner sep=1pt, scale= 1.25] at (10.3, -8.5) {};
    
\node [draw=black!70, inner sep= 3pt, line width=0.03em, minimum width= 2cm, minimum height= 2cm] (b2) at (15., -11) {};
    \node [draw=black!70, inner sep= 2pt, line width=0.03em, minimum width= 0.05cm, minimum height= 0.05cm] (b1) at (19.45, -12.4) {};
    
\draw [-, draw=black!70, , line width=0.03em, shorten <=2pt, shorten >=2pt, >=stealth]
       (19.45,-12.4) node[]{}
    -- (16.64,-9.12) node[]{};
    \draw [-, draw=black!70, , line width=0.03em, shorten <=2pt, shorten >=2pt, >=stealth]
       (19.5,-12.513) node[]{}
    -- (16.57,-12.76) node[]{};
\draw[rotate around={45:(15,-11)},my_blue!75, line width=0.07em] (15,-11) ellipse (0.7cm and 0.35cm);
	\draw[rotate around={45:(15,-11)},my_blue!75, line width=0.07em] (15,-11) ellipse (1cm and 0.5cm);
	\draw[rotate around={45:(15,-11)},my_blue!75, line width=0.07em] (15,-11) ellipse (1.4cm and 0.7cm);
	\draw[rotate around={45:(15,-11)},my_blue!75, line width=0.07em] (15,-11) ellipse (2.0cm and 1.cm);
\node[vertex, draw= my_yellow_2!80 , fill= my_yellow_2!80, scale=0.25] (l)  at (15,-11) {};
	\node[vertex, draw= my_green_2!80, fill= my_green_2!80, scale=0.25] (l)  at (13.7,-12) {};
	\node [inner sep=1pt,scale=1.25] at (15.2, -11.4)    {};
	\node [inner sep=1pt,scale=1.25] at (16.2, -12.2)  {};	
	\node [inner sep=1pt,scale=1.25] at (13.8, -11.6)  {};
\draw [-{Triangle[angle=60:.1cm 1]}, draw=black!70, densely dashed, line width=0.08em, shorten <=2pt, shorten >=2pt, >=stealth]
       (15,-11) node[]{}
    -- (16.8,-9.2) node[]{};
    \draw [-{Triangle[angle=60:.1cm 1]}, draw=black!70, densely dashed, line width=0.08em, shorten <=2pt, shorten >=2pt, >=stealth]
       (15,-11) node[]{}
    -- (13.9,-9.9) node[]{};

\end{tikzpicture}
         \vspace{-0.4cm}
        \caption{Overview of our LUVLi method. From each U-net of a DU-Net, we append a shared Cholesky Estimator Network (CEN) and Visibility Estimator Network (VEN) to the bottleneck layer and apply a mean estimator to the heatmap. The figure shows the joint estimation of location, uncertainty, and visibility of the landmarks performed for each U-net  and landmark . The landmark has ground-truth (labeled) location  and visibility .}
        \label{fig:overview}
        \vspace{-0.3cm}
    \end{figure}
    
    Rather than the argmax of the heatmap, we choose a mean estimator for the heatmap that is differentiable and enables sub-pixel accuracy: the weighted spatial mean of the heatmap's positive elements. Unlike the non-parametric model of~\cite{kdnuncertain, chen2019face}, our uncertainty prediction method is parametric: we directly estimate the parameters of a single multivariate Laplacian or Gaussian distribution. Furthermore, our method does not constrain the Laplacian or Gaussian covariance matrix to be diagonal.


\subsection{Mean Estimator}\label{sec:soft-argmax}
    \vspace{-0.15cm}
    Let  denote the value at pixel location  of the  landmark's heatmap from the  U-net. 
    The landmark's location estimate  is given by first post-processing the pixels of the heatmap  with a function , then taking the weighted spatial mean of the result (See~\eqref{eq:mean} in the supplementary material). We considered three different functions for : the ReLU function (eliminates the negative values), the softmax function (makes the mean estimator a soft-argmax of the heatmap~\cite{chapelle2010gradient, yi2016lift, luvizon20182d, dong2018supervision}), and a temperature-controlled softmax function (which, depending on the temperature setting, provides a continuum of softmax functions that range from a ``hard'' argmax to the uniform distribution). The ablation studies (Section~\ref{sec:results_ablation}) show that choosing  to be the ReLU function yields the simplest and best mean estimator.    


\subsection{LUVLi Loss} \label{sec:loss}
    \vspace{-0.15cm}
        Occluded landmarks, \example, landmarks on the far side of a profile-pose face, are common in real data. To explicitly represent visibility, we model the probability distributions of landmark locations using mixed random variables. For each landmark  in an image, we denote the ground-truth (labeled) visibility by the binary variable , where  denotes {\em visible}, and the ground-truth location by . By convention, if the landmark is not visible (), then , a special symbol indicating non-existence. Together, these variables are distributed according to an unknown distribution . The marginal Bernoulli distribution  captures the probability of visibility,  denotes the distribution of the landmark location when it is visible, and , where  denotes the PMF that assigns probability one to the symbol .
    
         After each U-net , we estimate the joint distribution of the visibility  and location  of each landmark  via
        
        where  is a Bernoulli distribution with
        
        where  is the predicted probability of visibility, and
        
        where  denotes the likelihood of the landmark being at location  given the estimated mean  and covariance .
        
        The LUVLi loss is the negative log-likelihood with respect to , as given by
        
        and thus minimizing the loss is equivalent to maximum likelihood estimation.

        The terms of~\eqref{eq:vloss} are a binary cross entropy plus  times the negative log-likelihood of  with respect to . This can be seen as an instance of multi-task learning~\cite{caruana1997multitask}, since we are predicting three things about each landmark: its location, uncertainty, and visibility. The first two terms on the right hand side of~\eqref{eq:vloss} can be seen as a classification loss for visibility, while the last term corresponds to a regression loss of location estimation.
        The sum of classification and regression losses is also widely used in object detection~\cite{jiao2019survey}.
        
        Minimization of negative log-likelihood also corresponds to minimizing KL-divergence, since
        
        where expectations are with respect to \mbox{}, and the entropy term  is constant with respect to the estimate .
        Further, since
        
        where  for brevity, minimizing the negative log-likelihood (LUVLi loss) is also equivalent to minimizing the combination of KL-divergences given by
        

\subsubsection{Models for Location Likelihood} \label{sec:loclikechoice}
    \vspace{-0.15cm}
        For the multivariate location distribution , we consider two different models: Gaussian and Laplacian.
        
        \textbf{Gaussian Likelihood.}
        The D Gaussian likelihood is:
        
Substituting~\eqref{eq:gauss_likelihood} into~\eqref{eq:vloss}, we have
        -0.6cm]
            &\quad  +\visgd\!\underbrace{\frac{1}{2}(\ground\!-\bmu\!)^T\bSigmaInv(\!\ground-\!\bmu\!)}_{T_2\vspace{-0.3cm}}.
            \label{eq:gauss_likelihood_loss}
        
            P(\z|\bmu,\! \bSigma) \! &= \!
            \frac{e^{-\sqrt{3(\z-\bmu)^T\bSigmaInv (\z-\bmu)}} }{\frac{2\pi}{3}\sqrt{\left| \bSigma \right|}}.
            \label{eq:lapl_simplified_likelihood}
        
            \loss_{ij}
            &= -(1\!-\!\visgd)\ln(1\!-\!\vispred)\!-\visgd\ln(\vispred) + \visgd\! \underbrace{\frac{1}{2}\!\log|\bSigma|}_{T_1}\!\nonumber \
In~\eqref{eq:lapl_simplified_loss},  is a scaled Mahalanobis distance, while  serves as a regularization or prior term that ensures that the Laplacian uncertainty distribution does not get too large. 
        
    Note that if  is the identity matrix and if all landmarks are assumed to be visible, then~\eqref{eq:gauss_likelihood_loss} simply reduces to the squared  distance, and~\eqref{eq:lapl_simplified_loss} just minimizes the  distance.
        
        
\subsection{Uncertainty and Visibility Estimation}\label{sec:CEN_VEN}
    \vspace{-0.15cm}
        Our proposed method uses heatmaps for estimating landmarks' locations, but not for estimating their uncertainty and visibility. We experimented with several methods for computing a covariance matrix directly from a heatmap, but none were accurate enough. We discuss this in Section~\ref{sec:300w}.
    
        \textbf{Cholesky Estimator Network (CEN).} We represent the uncertainty of each landmark location using a  covariance matrix , which is symmetric positive definite. The three degrees of freedom of  are captured by its Cholesky decomposition: a lower-triangular matrix  such that . To estimate the elements of , we append a Cholesky Estimator Network (CEN) to the bottleneck of each U-net. The CEN is a fully connected linear layer whose input is the bottleneck of the U-net  dimensions and output is an -dimensional vector, where  is the number of landmarks (\example, ). As the Cholesky decomposition  of a covariance matrix must have positive diagonal elements, we pass the corresponding entries of the output through an ELU activation function~\cite{clevert2015fast}, to which we add a constant to ensure the output is always positive (asymptote is negative -axis). 
        
        \textbf{Visibility Estimator Network (VEN).} To estimate the visibility of the landmark , we add another fully connected linear layer whose input is the bottleneck of the U-net  dimensions and output is an  -dimensional vector. This is passed through a sigmoid activation so the predicted visibility  is between  and . 

        The addition of these two fully connected layers only slightly increases the size of the original model. The loss for a single U-net is the averaged  across all the landmarks \,, and the total loss  for each input image is a weighted sum of the losses of all  of the U-nets:
        
        At test time, each landmark's mean and Cholesky coefficients are derived from the  (final) U-net. The covariance matrix is calculated from the Cholesky coefficients.


\vspace{-0.2cm}
\section{New Dataset: \ourdatasetHeading}
\vspace{-0.15cm}
    To promote future research in face alignment with uncertainty, we now introduce a new dataset with entirely new, manual labels of over  face images from the AFLW~\cite{koestinger2011annotated} dataset. In addition to landmark locations, every landmark is labeled with one of three visibility classes. We call the new dataset {\em MERL Reannotation of AFLW with Visibility} (\ourdataset).
    

\textbf{Visibility Classification.}
    Each landmark of every face is classified as either \emph{unoccluded}, \emph{self-occluded}, or \emph{externally occluded}, as illustrated in Figure~\ref{fig:sample}. \emph{Unoccluded} denotes landmarks that can be seen directly in the image, with no obstructions. \emph{Self-occluded} denotes landmarks that are occluded because of extreme head pose---they are occluded by another part of the face (\example, landmarks on the far side of a profile-view face). \emph{Externally occluded} denotes landmarks that are occluded by hair or an intervening object such as a cap, hand, microphone, or goggles. Human labelers are generally very bad at localizing self-occluded landmarks, so we do not provide ground-truth locations for these. We do provide ground-truth (labeled) locations for both unoccluded and externally occluded landmarks.


\textbf{Relationship to Visibility in LUVLi.} 
    In Section~\ref{sec:method}, visible landmarks () are landmarks for which ground-truth location information is available, while invisible landmarks () are landmarks for which no ground-truth location information is available (). Thus, invisible () in the model is equivalent to the \emph{self-occluded} landmarks in our dataset. In contrast, both \emph{unoccluded} and \emph{externally occluded} landmarks are considered visible () in our model. We choose this because human labelers are generally good at estimating the locations of externally occluded landmarks but poor at estimating the locations of self-occluded landmarks.
    
    
\textbf{Existing Datasets.}
    The most commonly used publicly available datasets for evaluation of D face alignment are summarized in Table~\ref{tab:dataset}. The \threehundredW~dataset~\cite{sagonas2013300, sagonas2016300, sagonas2013semi} uses a -landmark system that was originally used for Multi-PIE~\cite{gross2010multi}. \menpoTwoD~\cite{zafeiriou2017menpo, trigeorgis2016mnemonic, deng2019menpo} makes a hard distinction (denoted F/P) between nearly frontal faces (F) and profile faces (P). \menpoTwoD~uses the same landmarks as \threehundredW~for frontal faces, but for profile faces it uses a different set of  landmarks that do not all correspond to the  landmarks in the frontal images. \threehundredLP~\cite{zhu2016face,bulat2017far} is a synthetic dataset created by automatically reposing \threehundredW~faces, so it has a large number of labels, but they are noisy. The D model locations of self-occluded landmarks are projected onto the visible part of the face as if the face were transparent (denoted by T). The WFLW~\cite{wu2018look} and AFLW-~\cite{qian2019aggregation} datasets do not identify which landmarks are self-occluded, but instead label self-occluded landmarks as if they were located on the visible boundary of the noseless face. 
    
    \begin{table}[!t]
        \caption{Overview of face alignment datasets. [Key: \\Self Occ= Self-Occlusions, Ext Occ= External Occlusions]}
        \label{tab:dataset}
        \setlength{\tabcolsep}{0.075cm}
        \centering
        \footnotesize
        \begin{tabular}{lcccccc}
            \myTopRule
            \textbf{Dataset} & \textbf{\#train}  & \textbf{\#test} & \textbf{\#marks}  & \textbf{Profile} & \textbf{Self}  & \textbf{Ext}\\
            &&&  & \textbf{Images} & \textbf{Occ} & \textbf{Occ} \\
            \myTopRule
            COFW~\cite{burgos2013robust}                                                  &   &               &           & \xmark & \xmark & \cmark \\
            \cofwSixtyEight~\cite{ghiasi2015occlusion}                                    & -          &               &           & \xmark & \xmark & \cmark \\
            \threehundredW~\cite{sagonas2013300, sagonas2016300, sagonas2013semi}         &   &               &           & \xmark & \xmark & \xmark \\
            \menpoTwoD~\cite{zafeiriou2017menpo, trigeorgis2016mnemonic, deng2019menpo}   &   &           &        & \cmark & F/P    & \xmark \\ 
            \threehundredLP~\cite{zhu2016face}                                            &  & -                  &           & \cmark & T      & \xmark \\
            WFLW~\cite{wu2018look}                                                        &   &           &  & \cmark & \xmark & \xmark \\
            AFLW~\cite{koestinger2011annotated}                                           &  &  &           & \cmark & \cmark & \xmark \\
            \aflwNineteen~\cite{zhu2016unconstrained}                                     &  &  &           & \cmark & \xmark & \xmark \\
            AFLW-~\cite{qian2019aggregation}                                          &  &  &           & \cmark & \xmark & \xmark \\
            \bottomrule
            \ourdataset~(Ours)                                                            &  &           &           & \cmark & \cmark & \cmark \\
            \myTopRule
        \end{tabular}
        \vspace{-0.7cm}
    \end{table}
    
    
\textbf{Differences from Existing Datasets.}
    Our \ourdataset~dataset is the only one that labels every landmark using both types of occlusion (self-occlusion and external occlusion). Only one other dataset, AFLW, indicates which individual landmarks are self-occluded, but it has far fewer landmarks and does not label external occlusions. COFW and \cofwSixtyEight~indicate which landmarks are externally occluded but do not have self-occlusions. \menpoTwoD~categorizes faces as frontal or profile, but landmarks of the two classes are incompatible. Unlike \menpoTwoD, our dataset smoothly transitions from frontal to profile, with gradually more and more landmarks labeled as self-occluded.  
    
    Our dataset uses the widely adopted  landmarks used by \threehundredW, to allow for evaluation and cross-dataset comparison.  Since it uses images from AFLW, our dataset has pose variation up to  yaw and   pitch. Focusing on yaw, we group the images into five pose classes: frontal, left and right half-profile, and left and right profile. The train/test split is in the ratio of . Table~\ref{tab:dataset_statistics} provides the statistics of our \ourdataset~dataset. A sample image from the dataset is shown in Figure~\ref{fig:sample}. In the figure, unoccluded landmarks are green, externally occluded landmarks are red, and self-occluded landmarks are indicated by black circles in the face schematic on the right.
  
    \begin{table}[!tb]
        \begin{minipage}[b]{0.5\linewidth}
            \footnotesize
            \setlength{\tabcolsep}{0.075cm}
            \begin{tabular}{cccc}
                \myTopRule
                \textbf{Pose} & \textbf{Side} & \textbf{\#Train} & \textbf{\#Test}\\
                \myTopRule
                Frontal       & -             &            & \\
                \hline
                Half-         & Left half     &            & \\
                Profile       & Right half    &            & \\
                \hline
                \multirow{2}{*}{Profile} 
                              & Left          &            & \\
                              & Right         &            & \\
                \myTopRule
                Total         & -             &           & \\
                \myTopRule
            \end{tabular}
            \captionof{table}{Statistics of our new dataset for face alignment.}
            \label{tab:dataset_statistics}
        \end{minipage}
        \hfill
        \begin{minipage}[b]{0.44\linewidth}
            \centering
\includegraphics[trim=0 0 0 50, clip, width=1.85cm]{aflw_ours_sample2_face.png}
            \includegraphics[trim=0 0 0 0, clip, width=1.5cm]{fig/aflw_ours_sample2_schematic.png}
            \captionof{figure}{\textcolor{green}{\textbf{Unoccluded}}, \textcolor{my_red}{\textbf{externally occluded}}, and self-occluded landmarks.}
            \label{fig:sample}
        \end{minipage}
        \vspace{-0.5cm}
    \end{table}
    
    

\section{Experiments}\label{sec:exp}
\vspace{-0.15cm}
    Our experiments use the datasets \threehundredW~\cite{sagonas2013300, sagonas2016300, sagonas2013semi}, \threehundredLP~\cite{zhu2016face}, \menpoTwoD~\cite{zafeiriou2017menpo, trigeorgis2016mnemonic, deng2019menpo}, \cofwSixtyEight~\cite{burgos2013robust, ghiasi2015occlusion}, \aflwNineteen~\cite{koestinger2011annotated}, WFLW~\cite{wu2018look}, and our \ourdataset~dataset. Training and testing protocols are described in the supplementary material. On a  GB GeForce GTX Titan-X GPU, the inference time per image is  ms.

\textbf{Evaluation Metrics.}
    We use the standard metrics NME, AUC, and FR~\cite{tang2019towards, chen2019face, wang2019adaptive}. In each table, we report results using the same  metric adopted in respective baselines.
        
        \textit{Normalized Mean Error (NME).} The NME is defined as:
        
        where ,  and  respectively denote the visibility, ground-truth and predicted location of landmark  from the th (final) U-net. The factor of  is there because we cannot compute an error value for points without ground-truth location labels. Several variations of the normalizing term  are used.  \nmebox~\cite{zafeiriou2017menpo, bulat2017far, chen2019face} sets  to the geometric mean of the width and height of the ground-truth bounding box , while \nmeocular~\cite{sagonas2013300, kowalski2017deep, tang2019towards} sets  to the distance between the outer corners of the two eyes. If a ground-truth box is not provided, the tight bounding box of the landmarks is used~\cite{bulat2017far, chen2019face}. \nmediag~\cite{wu2018look, sun2019high} sets  as the diagonal of the bounding box. 
    
        \textit{Area Under the Curve (AUC).} To compute the AUC, the cumulative distribution of the fraction of test images whose  is less than or equal to the value on the horizontal axis is first plotted. The AUC for a test set is then computed as the area under that curve, up to the cutoff NME value.

        \textit{Failure Rate (FR).} FR refers to the percentage of images in the test set whose NME is larger than a certain threshold.


\subsection{\threehundredWHeading~Face Alignment}
    \label{sec:300w}
    \vspace{-0.15cm}
        We train on the \threehundredW~\cite{sagonas2013300, sagonas2016300, sagonas2013semi}, and test on \threehundredW, \menpoTwoD~\cite{zafeiriou2017menpo, trigeorgis2016mnemonic, deng2019menpo}, and \cofwSixtyEight~\cite{burgos2013robust, ghiasi2015occlusion}. Some of the models are pre-trained on the \threehundredLP~\cite{zhu2016face}.

\textbf{Data Splits and Evaluation Metrics.}\label{sec_split}
        There are two commonly used train/test splits for \threehundredW; we evaluate our method on both. \textit{Split :} The train set contains  images and full test set has  images~\cite{tang2019towards}. \textit{Split :} The train set includes  images and test set has  images~\cite{chen2019face}. The model trained on Split  is additionally evaluated on the  near-frontal training images of \menpoTwoD~and  test images of \cofwSixtyEight~\cite{chen2019face}. For Split , we use \nmeocular~\cite{tang2019towards, sun2019high, wang2019adaptive}. For Split , we use \nmebox~and  with   cutoff~\cite{bulat2017far, chen2019face}.

        \begin{table}[!tb]
            \caption{\nmeocular~on \threehundredW~Common, Challenge, and Full datasets (Split ). [Key: \firstkey{Best}, \secondkey{Second best}]}
            \label{tab:nme_split1}
            \centering
            \footnotesize
            \begin{tabular}{tcmccct}
                \myTopRule
                 & \multicolumn{3}{ct}{\nmeocular~}\0.05cm]
                                 & \threehundredW & Menpo & COFW & \threehundredW & Menpo & COFW\\ 
                \myTopRule
                SAN*~\cite{dong2018style}~in~\cite{chen2019face} &         &        &         &         &         & \\ 
                D-FAN*~\cite{bulat2017far}                    &         &        &         &         &         & \\
                KDN~\cite{kdnuncertain}                          &         &        & -             &         &         & - \\
                Softlabel*~\cite{chen2019face}                   &         &        &         &         &         &  \\
                KDN*~\cite{chen2019face}                         & \second{2.21} & \first{2.01} & \second{2.73} & \second{68.3} & \second{71.1} & \\
                \hline
                LUVLi (Ours)                                     &         &        &         & \second{68.3} &         & \second{60.8}\\LUVLi* (Ours)                                    & \first{2.10}  & \second{2.04}& \first{2.57}  & \first{70.2}  & \first{71.9}  & \first{63.4}\\\myTopRule
            \end{tabular}
            \vspace{-0.3cm}
        \end{table}
        
\textbf{Results: Localization and Cross-Dataset Evaluation.} 
        The face alignment results for \threehundredW~Split  and Split  are summarized in Table~\ref{tab:nme_split1} and~\ref{tab:nme_split2}, respectively. Table~\ref{tab:nme_split2} also shows the results of our model (trained on Split ) on the Menpo and \cofwSixtyEight~datasets, as in~\cite{bulat2017far, chen2019face}. The results in Table~\ref{tab:nme_split1} show that our LUVLi landmark localization is competitive with the SOTA methods on Split , usually one of the best two. Table~\ref{tab:nme_split2} shows that LUVLi significantly outperforms the SOTA on Split , performing best on  out of the  cases ( datasets   metrics). This is particularly impressive on \threehundredW~Split , because even though most of the other methods are pre-trained on the \threehundredLP~dataset (as was our best method, LUVLi*), our method without pre-training still outperforms the SOTA in  of  cases. Our method performs particularly well in the cross-dataset evaluation on the more challenging \cofwSixtyEight~dataset, which has multiple externally occluded landmarks.

        \begin{figure}[!tb]
            \centering
        	\begin{subfigure}{0.325\linewidth}
        		\includegraphics[width=\linewidth]{fig/resid_vs_pred_sigma_xx_lmark.png}
        		\vspace{-5mm}
        		\captionof{figure}{variance of }
        		\label{fig:res_vs_pred_x}
        	\end{subfigure}\hfill
        	\begin{subfigure}{0.325\linewidth}
        		\includegraphics[width=\linewidth]{fig/resid_vs_pred_sigma_yy_lmark.png}
        			\vspace{-5mm}
        		\captionof{figure}{variance of }
        		\label{fig:res_vs_pred_y}
        	\end{subfigure}
        	\hfill
        	\begin{subfigure}{0.325\linewidth}
        		\includegraphics[width=\linewidth]{fig/resid_vs_pred_sigma_xy_lmark.png}
        			\vspace{-5mm}
        		\captionof{figure}{covariance of }
        		\label{fig:res_vs_pred_xy}
        	\end{subfigure}
        	\vspace{-0.3cm}
        	\caption{Mean squared residual error vs.~predicted covariance matrix for all landmarks in \threehundredW~Test (Split ).}
        	\label{fig:res_vs_pred}
    	    \vspace{-0.3cm}
        \end{figure}

\textbf{Accuracy of Predicted Uncertainty.}
        To evaluate the accuracy of the predicted uncertainty covariance matrix, 
        
        we compare all three unique terms of this prediction with the statistics of the {\it residuals} (D error between the ground-truth location  and the predicted location ) of all landmarks in the test set. We explain how we do this for  in Figure~\ref{fig:res_vs_pred_x}. First, we bin every landmark of every test image according to the value of the predicted variance in the -direction . Each bin is represented by one point in the scatter plot. Averaging  across the  landmark points within each bin gives a single predicted   value (horizontal axis). We next compute the residuals in the -direction of all landmarks in the bin, and calculate the average of the squared residuals to obtain  for the bin. This mean squared residual error, , is plotted on the vertical axis. If our predicted uncertainties are accurate, this residual error, , should be roughly equal to the predicted uncertainty variance in the -direction (horizontal axis).
        
        Figure~\ref{fig:res_vs_pred} shows that all three terms of our method's predicted covariance matrices are highly predictive of the actual uncertainty: the mean squared residuals (error) are strongly proportional to the predicted covariance values, as evidenced by Pearson correlation coefficients of  and . However, decreasing  from 734 (plotted in Figure~\ref{fig:res_vs_pred}) to just 36 makes the correlation coefficients decrease to .
        Thus, the predicted uncertainties are excellent after averaging but may yet have room to improve.

\textbf{Uncertainty is Larger for Occluded Landmarks.} 
        The \cofwSixtyEight~\cite{ghiasi2015occlusion} test set annotates which landmarks are externally occluded. Similar to~\cite{chen2019face}, we use this to test uncertainty predictions of our model, where the square root of the determinant of the uncertainty covariance is a scalar measure of predicted uncertainty. We report the error, \nmebox, and average predicted uncertainty, , in Table~\ref{tab:nme_ext_normal_cofw}. We do not use any occlusion annotation from the dataset during training. Like~\cite{chen2019face}, we find that our model's predicted uncertainty is much larger for externally occluded landmarks than for unoccluded landmarks. Furthermore, our method's location estimates are more accurate (smaller \nmebox) than those of~\cite{chen2019face} for both occluded and unoccluded landmarks.
        \begin{table}[!tb]
            \caption{\nmebox~and uncertainty  on unoccluded and externally occluded landmarks of \cofwSixtyEight~dataset. [Key: \firstkey{Best}]}
            \label{tab:nme_ext_normal_cofw}
            \centering
            \footnotesize
            \setlength{\tabcolsep}{0.075cm}
            \begin{tabular}{|cmccmcc|}
                \myTopRule
                \addlinespace[0.01cm]
                    & \multicolumn{2}{cm}{Unoccluded} & \multicolumn{2}{c|}{Externally Occluded}\\
                    & \nmebox &  & \nmebox & \\
                \myTopRule
                Softlabel~\cite{chen2019face} &  &  &  &  \\
                KDN~\cite{chen2019face}       &  &  &  &  \\
                \hline
                LUVLi (Ours)                  & \first{2.15} &  & \first{4.00} & \\\myTopRule
            \end{tabular}
            \vspace{-0.2cm}
        \end{table}
        
\textbf{Heatmaps vs.~Direct Regression for Uncertainty.}
        We tried multiple approaches to estimate the uncertainty distribution from heatmaps, but none of these worked nearly as well as our direct regression using the CEN. We believe this is because in current heatmap-based networks, the resolution of the heatmap () is too low for accurate uncertainty estimation. This is demonstrated in Figure~\ref{fig:hist_covar_small_eigen_value}, which shows a histogram over all landmarks in \threehundredW~Test (Split ) of LUVLi's predicted covariance in the narrowest direction of the covariance ellipse (the smallest eigenvalue of the predicted covariance matrix). The figure shows that in most cases, the uncertainty ellipses are less wide than one heatmap pixel, which explains why heatmap-based methods are not able to accurately capture such small uncertainties.
        
        \begin{table}[!tb]
            \caption{NME and AUC on the \aflwNineteen~dataset (previous results are quoted from~\cite{sun2019high, chen2019face}).
            [Key: \firstkey{Best}, \secondkey{Second best}]}
            \label{tab:nme_auc_aflw19}
            \centering
            \footnotesize
            \setlength{\tabcolsep}{0.1cm}
            \begin{tabular}{tcmccmccm}
                \myTopRule
                \addlinespace[0.01cm]
                & \multicolumn{2}{ct}{\nmediag} & \nmebox & \aucbox \\ 
                & Full & Frontal & Full& Full\0.05cm]
                \myTopRule
            \end{tabular}
            \vspace{-0.5cm}
        \end{table}

    
\subsection{Ablation Studies}\label{sec:results_ablation}
    \vspace{-0.2cm}
    Table~\ref{tab:abl_1} compares modifications of our approach on Split . Table~\ref{tab:abl_1} shows that computing the loss only on the last U-net performs worse than computing loss on all U-nets, perhaps because of the vanishing gradient problem~\cite{wei2016convolutional}. Moreover, LUVLi's log-likelihood loss without visibility outperforms using MSE loss on the landmark locations (which is equivalent to setting all ). We also find that the loss with Laplacian likelihood~\eqref{eq:lapl_simplified_loss} outperforms the one with Gaussian likelihood~\eqref{eq:gauss_likelihood_loss}. Training from scratch is slightly inferior to first training the base DU-Net architecture before fine-tuning the full LUVLi network, consistent with previous observations that the model does not have strongly supervised pixel-wise gradients through the heatmap during training~\cite{nibali2018numerical}. Regarding the method for estimating the mean, using heatmaps is more effective than direct regression (Direct) from each U-net bottleneck, consistent with previous observations that neural networks have difficulty predicting continuous real values~\cite{Belagiannis17, nibali2018numerical}. As described in Section~\ref{sec:soft-argmax}, in addition to ReLU, we compared two other functions for : softmax, and a temperature-scaled softmax (-softmax). Results for temperature-scaled softmax and ReLU are essentially tied, but the former is more complicated and requires tuning a temperature parameter, so we chose ReLU for our LUVLi model. Finally, reducing the number of U-nets from 8 to 4 increases test speed by about  with minimal decrease in performance.

    \begin{table}[!tb]
        \caption{Ablation studies using our method trained on \threehundredLP~and then fine-tuned on \threehundredW~(Split ).}
        \label{tab:abl_1}
        \centering
        \footnotesize
        \setlength{\tabcolsep}{0.05cm}
        \begin{tabular}{tc|lmccmcct}
            \myTopRule
            \addlinespace[0.01cm]
            \multicolumn{2}{tcm}{\textbf{Change from LUVLi model:}} & \multicolumn{2}{cm}{\nmebox~} & \multicolumn{2}{ct}{\aucbox~}\12pt] Supplementary Material\
            \bmu = 
            \begin{bmatrix}
            \bmux\\ 
            \bmuy
            \end{bmatrix}
            = \dfrac{\mathlarger{\mathlarger{\sum}}\limits_{x,y} \sigma \bigl( \bh(x,y) \bigr)\begin{bmatrix}
            x\\
            y
            \end{bmatrix}}{\mathlarger{\mathlarger{\sum}}\limits_{x,y} \sigma \bigl( \bh(x,y) \bigr)}
            \label{eq:mean}
        
            \ground \sim P(\z|\bmuF,\! \bSigmaF) =
            \frac{e^{-\sqrt{3(\z-\bmuF)^T\bSigmaF^{-1} (\z-\bmuF)}} }{\frac{2\pi}{3}\sqrt{\left| \bSigmaF \right|}}.
            \label{eq:lapl_prediction}
        
            \ground' = \bSigmaF^{-0.5}(\ground - \bmuF).
            \label{eq:transformed_gt}
        
            \ground' \sim P(\z'|\zerovec, \I) =
            \frac{e^{-\sqrt{3\z'^T\z'}} }{{2\pi}/{3}}.
            \label{eq:lapl_prediction_transformed}
        
        \mathrm{NME^{\text{vis}}(\%)} = \frac{1}{\sum\limits_{j}\visgd}\sum_{j=1}^{N_p}\visgd \frac{{\|\ground - \bmuF\|}_2}{d} \times 100,
        \label{eq:nme_vis}
    
    If all of the facial landmarks are visible, then this reduces to our previous definition of NME~\eqref{eq:nme}.
    
    We define \nmeboxvis~as the special case of~ in which the normalization  is set to the geometric mean of the width and height of the ground-truth bounding box  as in ~\cite{zafeiriou2017menpo, bulat2017far, chen2019face}. Results for all head poses on \ourdataset~dataset using the metric \nmeboxvis~are shown in Table~\ref{tab:nme_aflw_ours_nme_vis}. We also repeat the \nmebox~numbers from Table~\ref{tab:nme_aflw_ours}. Clearly, the~\nmeboxvis~and \nmebox~numbers are very close for the frontal subsets but are different for half-profile and profile subsets. This is because half-profile and (especially) profile face images have fewer visible landmarks (more self-occluded landmarks), which causes the denominator in~\eqref{eq:nme_vis} to be smaller for these images.

    
\subsection{Additional Qualitative Results}
        In Figure~\ref{fig:more_qualitative}, we show example results on images from four datasets on which we tested.
        \begin{figure*}[!htb]
            \setlength{\tabcolsep}{0.0cm}
            \begin{tabular}{cccccc}
                \includegraphics[width=0.166\linewidth]{fig/qualitative/300W_test/all_with_all_landmarks/indoor_003.png} &
                \includegraphics[width=0.166\linewidth]{fig/qualitative/300W_test/all_with_all_landmarks/outdoor_198.png} &
                \includegraphics[width=0.166\linewidth]{fig/qualitative/300W_test/all_with_all_landmarks/outdoor_174.png} &
                \includegraphics[width=0.166\linewidth]{fig/qualitative/300W_test/all_with_all_landmarks/indoor_065.png} &
                \includegraphics[width=0.166\linewidth]{fig/qualitative/300W_test/all_with_all_landmarks/indoor_090.png} &
                \includegraphics[width=0.166\linewidth]{fig/qualitative/300W_test/all_with_all_landmarks/outdoor_279.png} \\
                \includegraphics[width=0.166\linewidth]{fig/qualitative/aflw_full/image13933.jpg} &
                \includegraphics[width=0.166\linewidth]{fig/qualitative/aflw_full/image28182.jpg} &
                \includegraphics[width=0.166\linewidth]{fig/qualitative/aflw_full/image26540.jpg} &
                \includegraphics[width=0.166\linewidth]{fig/qualitative/aflw_full/image25776.jpg} &
                \includegraphics[width=0.166\linewidth]{fig/qualitative/aflw_full/image14607.jpg} &
                \includegraphics[width=0.166\linewidth]{fig/qualitative/aflw_full/image26975.jpg}\\
                \includegraphics[width=0.166\linewidth]{fig/qualitative/wflw_full/31_Waiter_Waitress_Waiter_Waitress_31_608.jpg} &
                \includegraphics[width=0.166\linewidth]{fig/qualitative/wflw_full/13_Interview_Interview_On_Location_13_777.jpg} &
                \includegraphics[width=0.166\linewidth]{fig/qualitative/wflw_full/13_Interview_Interview_2_People_Visible_13_167.jpg} &
                \includegraphics[width=0.166\linewidth]{fig/qualitative/wflw_full/51_Dresses_wearingdress_51_917.jpg} &
                \includegraphics[width=0.166\linewidth]{fig/qualitative/wflw_full/52_Photographers_taketouristphotos_52_773.jpg} &
                \includegraphics[width=0.166\linewidth]{fig/qualitative/wflw_full/52_Photographers_photographertakingphoto_52_316.jpg}\\
                \includegraphics[width=0.166\linewidth]{fig/qualitative/aflw_ours/image00693.jpg} &
                \includegraphics[width=0.166\linewidth]{fig/qualitative/aflw_ours/image01574.jpg}&
                \includegraphics[width=0.166\linewidth]{fig/qualitative/aflw_ours/image33277_1.jpg} &
                \includegraphics[width=0.166\linewidth]{fig/qualitative/aflw_ours/image07216.jpg} &
                \includegraphics[width=0.166\linewidth]{fig/qualitative/aflw_ours/image57669.jpg} &
                \includegraphics[width=0.166\linewidth]{fig/qualitative/aflw_ours/image34077.jpg} \\
            \end{tabular}
            \caption{Results of our LUVLi face alignment on example face images from four face datasets. {\em Top row:} \threehundredW. {\em Second row:} \aflwNineteen. {\em Third row:} WFLW. {\em Bottom row:}  \ourdataset. Ground-truth (green) and predicted (yellow) landmark locations are shown. The estimated uncertainty of the predicted location of each landmark is shown in blue (Error ellipse for Mahalanobis distance 1). In the \ourdataset~images (bottom row), the predicted visibility of each landmark controls its transparency. In particular, the predicted locations of landmarks with predicted visibility close to zero (such the points on the far side of the profile face in the third image of the bottom row) are 100\% transparent (not shown).} 
            \label{fig:more_qualitative}
        \end{figure*}
 
    
\subsection{Video Demo of LUVLi}
    We include a short demo video of our LUVLi model that was trained on our new \ourdataset~dataset. The video  demonstrates our method's ability to predict landmarks' visibility (\thatIs, whether they are self-occluded) as well as their locations and uncertainty. We take a simple face video of a person turning his head from frontal to profile pose and run our method on each frame independently. Overlaid on each frame of video, we plot each estimated landmark location in yellow, and plot the predicted uncertainty as a blue ellipse. To indicate the predicted visibility of each landmark, we modulate the transparency of the landmark (of the yellow dot and blue ellipse). Landmarks whose predicted visibility is close to 1 are shown as fully opaque, while landmarks whose predicted visibility is close to zero are fully transparent (are not shown). Landmarks with intermediate predicted visibilities are shown  as partially transparent. 
    
    In the video, notice that as the face approaches the profile pose, points on the far edge of the face begin to disappear, because the method correctly predicts that they are not visible (are self-occluded) when the face is in profile pose.

    
\subsection{Examples from our \ourdatasetHeading~Dataset}
        Figure~\ref{fig:aflw_ours_samples} shows several sample images from our \ourdataset~dataset. The ground-truth labels are overlaid on the images. On each image, unoccluded landmarks are shown in green, externally occluded landmarks are shown in red, and self-occluded landmarks are indicated by black circles in the face schematic to the right of the image.
        
        \begin{figure*}[!htb]
            \begin{tabular}{c@{\hskip 0.05cm}cc@{\hskip 0.05cm}cc@{\hskip 0.05cm}c}
                \includegraphics[width=0.18\linewidth, height=0.20\linewidth]{fig/aflw_ours_sample/image66282_jpg_txt.png} &
                \includegraphics[width=0.15\linewidth, height=0.17\linewidth]{fig/aflw_ours_sample/schematic/image66282_jpg_txt.png} &
                \includegraphics[width=0.18\linewidth, height=0.20\linewidth]{fig/aflw_ours_sample/image59088_jpg_txt.png} &
                \includegraphics[width=0.15\linewidth, height=0.17\linewidth]{fig/aflw_ours_sample/schematic/image59088_jpg_txt.png} &
                \includegraphics[width=0.18\linewidth, height=0.20\linewidth]{fig/aflw_ours_sample/image42923_jpg_txt.jpg} &
                \includegraphics[width=0.15\linewidth, height=0.17\linewidth]{fig/aflw_ours_sample/schematic/image42923_jpg_txt.png} \\
                \includegraphics[width=0.18\linewidth, height=0.20\linewidth]{fig/aflw_ours_sample/image24875_jpg_txt.png} &
                \includegraphics[width=0.15\linewidth, height=0.17\linewidth]{fig/aflw_ours_sample/schematic/image24875_jpg_txt.png} &
                \includegraphics[width=0.18\linewidth, height=0.20\linewidth]{fig/aflw_ours_sample/image24988_jpg_txt.jpg} &
                \includegraphics[width=0.15\linewidth, height=0.17\linewidth]{fig/aflw_ours_sample/schematic/image24988_jpg_txt.png} &
                \includegraphics[width=0.18\linewidth, height=0.20\linewidth]{fig/aflw_ours_sample/image41775_jpg_txt.png} &
                \includegraphics[width=0.15\linewidth, height=0.17\linewidth]{fig/aflw_ours_sample/schematic/image41775_jpg_txt.png} \\
                \includegraphics[width=0.18\linewidth, height=0.20\linewidth]{fig/aflw_ours_sample/image00849_jpg_txt.jpg} &
                \includegraphics[width=0.15\linewidth, height=0.17\linewidth]{fig/aflw_ours_sample/schematic/image00849_jpg_txt.png} &
                \includegraphics[width=0.18\linewidth, height=0.20\linewidth]{fig/aflw_ours_sample/image13508_jpg_txt.png} &
                \includegraphics[width=0.15\linewidth, height=0.17\linewidth]{fig/aflw_ours_sample/schematic/image13508_jpg_txt.png} &
                \includegraphics[width=0.18\linewidth, height=0.20\linewidth]{fig/aflw_ours_sample/image08897_jpg_txt.png} &
                \includegraphics[width=0.15\linewidth, height=0.17\linewidth]{fig/aflw_ours_sample/schematic/image08897_jpg_txt.png} \\
                \includegraphics[width=0.18\linewidth, height=0.20\linewidth]{fig/aflw_ours_sample/image03939_jpg_txt.png} &
                \includegraphics[width=0.15\linewidth, height=0.17\linewidth]{fig/aflw_ours_sample/schematic/image03939_jpg_txt.png} &
                \includegraphics[width=0.18\linewidth, height=0.20\linewidth]{fig/aflw_ours_sample/image02713_jpg_txt.png} &
                \includegraphics[width=0.15\linewidth, height=0.17\linewidth]{fig/aflw_ours_sample/schematic/image02713_jpg_txt.png} &
                \includegraphics[width=0.18\linewidth, height=0.20\linewidth]{fig/aflw_ours_sample/image02606_jpg_txt.png} &
                \includegraphics[width=0.15\linewidth, height=0.17\linewidth]{fig/aflw_ours_sample/schematic/image02606_jpg_txt.png} \\
                \includegraphics[width=0.18\linewidth, height=0.20\linewidth]{fig/aflw_ours_sample/image14644_jpg_txt.png} &
                \includegraphics[width=0.15\linewidth, height=0.17\linewidth]{fig/aflw_ours_sample/schematic/image14644_jpg_txt.png} &
                \includegraphics[width=0.18\linewidth, height=0.20\linewidth]{fig/aflw_ours_sample/image00555_jpg_txt.png} &
                \includegraphics[width=0.15\linewidth, height=0.17\linewidth]{fig/aflw_ours_sample/schematic/image00555_jpg_txt.png} &
                \includegraphics[width=0.18\linewidth, height=0.20\linewidth]{fig/aflw_ours_sample/image00401_1_jpg_txt.png}&
                \includegraphics[width=0.15\linewidth, height=0.17\linewidth]{fig/aflw_ours_sample/schematic/image00401_1_jpg_txt.png} \\
            \end{tabular}
            \caption{Sample images from our \ourdataset~dataset with \textcolor{green}{\textbf{unoccluded}} landmarks shown in green, \textcolor{my_red}{\textbf{externally occluded}} landmarks shown in red, and self-occluded landmarks indicated by black circles in the face schematic on the right of each image.} 
            \label{fig:aflw_ours_samples}
        \end{figure*}



\section*{Acknowledgements}
    We would like to thank Lisha Chen from RPI for providing results from their method and Zhiqiang Tang and Shijie Geng from Rutgers University for providing their pre-trained models on \threehundredW~(Split ). We would also like to thank Adrian Bulat and Georgios Tzimiropoulos from the University of Nottingham for detailed discussions on getting bounding boxes for \threehundredW~(Split ). We also had very useful discussions with Peng Gao from Chinese University of Hong Kong on the loss functions and Moitreya Chatterjee from University of Illinois Urbana-Champaign. We are also grateful to Maitrey Mehta from the University of Utah who volunteered for the demo. We also acknowledge anonymous reviewers for their feedback that helped in shaping the final manuscript.
    


\end{document}