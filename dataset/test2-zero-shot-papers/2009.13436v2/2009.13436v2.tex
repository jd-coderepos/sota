\documentclass{article}
\usepackage{amsmath}
\usepackage{graphicx}


\usepackage[numbers]{natbib}




\usepackage[preprint]{paper}





\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage{subcaption}
\usepackage{footnote}
\makesavenoteenv{tabular}\makesavenoteenv{table}

\title{Learning to Detect Objects with a 1 Megapixel Event Camera}

\newcommand{\davide}[1]{{\color{red} #1}}
\newcommand{\etienneP}[1]{{\color{blue} #1}}
\newcommand{\amosS}[1]{{\color{ForestGreen} #1}}
\newcommand{\pierreDT}[1]{{\color{orange}  #1}}
\newcommand{\JonathanM}[1]{{\color{teal}  #1}}

\newcommand{\davidermk}[1]{{\color{red} {\bf dn: #1}}}
\newcommand{\etiennePrmk}[1]{{\color{blue} {\bf ep: #1}}}
\newcommand{\amosSrmk}[1]{{\color{ForestGreen} {\bf as: #1}}}
\newcommand{\pierreDTrmk}[1]{{\color{orange}  {\bf pdt: #1}}}
\newcommand{\JonathanMrmk}[1]{{\color{teal} {\bf jm: #1}}}
\newcommand{\faresMrmk}[1]{{\color{yellow} {\bf fm: #1}}}
\newcommand{\comment}[1]{}


\author{Etienne Perot\\
   PROPHESEE, Paris\\
   \texttt{eperot@prophesee.ai}\\
   \And
   Pierre de Tournemire\\
   PROPHESEE, Paris\\
   \texttt{pdetournemire@prophesee.ai}\\
   \AND
   Davide Nitti\\
   PROPHESEE, Paris\\
   \texttt{dnitti@prophesee.ai}\\
   \And
   Jonathan Masci\\
   NNAISENSE, Lugano\\
   \texttt{jonathan@nnaisense.com}\\
   \And
   Amos Sironi\\
   PROPHESEE, Paris\\
   \texttt{asironi@prophesee.ai}\\
}

\begin{document}

\maketitle


\begin{abstract}
    Event cameras encode visual
    information with high temporal precision, 
    low data-rate, and high-dynamic range. 
    Thanks to these characteristics, event cameras 
    are particularly suited for scenarios with high motion, 
    challenging lighting conditions and requiring low latency. 
    However, due to the novelty of the field, 
    the performance of event-based systems 
    on many vision tasks
    is still lower compared to conventional frame-based solutions.
    The main reasons for this performance gap are: the lower spatial resolution of event sensors, 
    compared to frame cameras; the lack of large-scale training datasets;
    the absence of well established deep learning architectures for event-based processing. 
    In this paper, we address all these problems in the context of an event-based object detection task.
    First, we publicly release the first high-resolution large-scale dataset for object detection.
    The dataset contains more than 14 hours recordings of a 1 megapixel event camera,
    in automotive scenarios, together with 25M bounding boxes of cars, pedestrians, and two-wheelers, 
    labeled at high frequency.
Second, we introduce a novel recurrent architecture for event-based detection
    and a temporal consistency loss for better-behaved training.
    The ability to compactly represent the sequence of events into the internal memory
    of the model is essential to achieve high accuracy. Our model outperforms by a 
    large margin feed-forward event-based architectures.
    Moreover, our method does not require any reconstruction 
    of intensity images from events, 
    showing that training directly from raw events is possible,
    more efficient, and more accurate than 
    passing through an intermediate intensity image. 
Experiments on the dataset introduced in this work, for which events
    and gray level images are available, show performance on par with that of
    highly tuned and studied frame-based detectors.


    \end{abstract}
     
\section{Introduction}





Event cameras~\cite{Lichtsteiner2008,posch2014retinomorphic,son20174,finateu20205} promise a paradigm shift in computer vision by
representing visual information in a fundamentally different way. 
Rather than encoding dynamic visual scenes with a sequence of still images, acquired at a fixed frame rate,
event cameras generate data in the form of a sparse and asynchronous events stream. 
Each event is represented by a tuple  corresponding to an illuminance 
change by a fixed relative amount, at pixel location  and time , with the polarity  indicating whether the illuminance was increasing or decreasing. Fig.~\ref{fig:intro} shows examples of data from an event camera in a driving scenario.

Since the camera does not rely on a global clock, 
but each pixel independently emits 
an event as soon as it detects an illuminance change,
the events stream has a very high temporal resolution, 
typically of the order of microseconds~\cite{Lichtsteiner2008}.
Moreover, due to a logarithmic pixel response characteristic, event cameras have a large dynamic range (often exceeding )~\cite{finateu20205}.
Thanks to these properties, event cameras are well suited for applications in which standard frame cameras are affected by
motion blur, pixel saturation, and high latency. 


Despite the remarkable properties of event cameras,
we are still at the dawn of event-based vision and 
their adoption in real systems is currently limited.
This implies scarce availability of algorithms, 
datasets, and tools to manipulate and process events.
Additionally, 
most of the available datasets have 
limited spatial resolution or they are not labeled, 
reducing the range of possible applications~\cite{de2020large,gallego2019event}.

To overcome these limitations, several works have focused on the reconstruction of gray-level information from an event stream~\cite{kim2008simultaneous,bardow2016simultaneous,munda2018real,rebecq2019high}. This approach is appealing since the reconstructed images can be fed to standard computer vision pipelines, leveraging more than 40 years of computer vision research.
In particular, it was shown~\cite{rebecq2019high} that all information required to reconstruct high-quality images is present in the event data.
However, passing through an intermediate intensity image comes at the price of adding considerable computational cost.
In this work, we show how to build an accurate event-based vision pipeline without the need of gray-level supervision.


\begin{figure*}[t]
    \centering
\includegraphics[width=1\textwidth]{figures/teaser1.pdf}
    \caption{Results of our event-camera detector on examples of the released 1Mpx Automotive Detection Dataset. Our method is able to accurately detect objects for a large variety of appearances, scenarios, and speeds. This makes it the first reliable event-based system on a large-scale vision task. Detected cars, pedestrians and two-wheelers are shown in yellow, blue and cyan boxes respectively.
    All figures in this work are best seen in electronic form.}
     \vspace{-4mm}
\label{fig:intro}
\end{figure*}



\comment{
\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{figures/intro_fig.png}
    \caption{Detecting objects with an event camera is an open computer vision problem. This is in part due to the lack of 
		large labeled datasets and effective machine learning architectures. One of the main challenges for event-based object detection 
		is to keep object persistence over time, even when no events are generated by the objects due to small motion.  
		As shown in the images above, our method is able to reliably detect objects over time also when they stop moving, 
		this thanks to a novel recurrent neural network architecture. We release and evaluate our approach on a
        new high resolution detection dataset, showing state-of-the-art performance and without the need of reconstructing 
        gray-level images.
		In the images above, we show input events accumulated over a fixed time interval, with overlayed object bounding boxes:
		White boxes correspond to ground truth, while cyan, red and green boxes are detections of cars, pedestrains and two-wheelers respectively, 
		returned by our method.
    \amosSrmk{Move this in results/method section and put here teaser of results. Times are made-up for now. Find an example video where another object passes in front of a detected box with no events}}
    \label{fig:intro}
\end{figure}
}
We target the problem of object detection in automotive scenarios, which is 
characterized by important objects dynamics and extreme lighting conditions. 
We make the following contributions to the field: 
First, we acquire and release the first large scale dataset for event-based 
object detection, with a high resolution (1280720) event camera~\cite{finateu20205}. 
We also define a fully automated labeling protocol, enabling fast and cheap dataset 
generation for event cameras. The dataset we release contains more than 
14 hours of driving recording, acquired in a large variety of scenarios. We also provide more 
than 25 million bounding boxes of cars, pedestrians and two-wheelers, labeled at 60Hz. 

Our second contribution is the introduction of a novel architecture for 
event-based object detection together with a new temporal consistency loss.
Recurrent layers are the core building block of our architecture, they introduce a fundamental memory mechanism 
needed to reach high accuracy with event data. At the same time, the temporal consistency loss helps to obtain more precise localization over time.
Fig.~\ref{fig:intro} shows some detections returned by our method on the released dataset.
We show that directly predicting the object locations is more efficient and more accurate than applying a detector on the gray-level images reconstructed with a state-of-the-art method~\cite{rebecq2019high}.
In particular, since we do not impose any bias coming from intensity image supervision,
we let the system learn the relevant features for the given task, which do not necessarily correspond to gray-level values.


Finally, we run extensive experiments on ours and another dataset 
for which gray-level images are also available, showing 
comparable accuracy to standard frame-based detectors and improved state-of-the-art results for event-based detection.
To the best of our knowledge, 
this is the first work showing an event-based system with on par 
performance to a frame-based one on a large vision task.
 
\section{Related Work}
\label{sec:related_work}
\vspace{-2mm}
Several machine learning architectures have been proposed for event cameras ~\cite{wang2019space,sekikawa2019eventnet,messikommer2020event}.
Some of these methods, such as Spiking Neural Networks~\cite{kasabov2013dynamic,lee2016training,shrestha2018slayer,tavanaei2019deep}, exploit the sparsity of the data and can be applied event by event, to preserve the temporal resolution of the events stream~\cite{lee2016training,cannici2019asynchronous,sironi2018hats,neil2016phased}. 
However, efficiently applying these methods to inputs with large event rate remains difficult. For these reasons, their efficacy has mainly been demonstrated on low-resolution classification tasks.




Alternative approaches map the events stream 
to a dense representation~\cite{alonso2019ev,maqueda2018event,zhu2019unsupervised,rebecq2019high}.
Once this representation is computed, 
it can be used as input to standard architectures.
Even if these methods lose some of the event's 
temporal resolution, they gain in terms of accuracy and scalability~\cite{zhu2018ev,maqueda2018event}.




Recently, the authors of~\cite{rebecq2019high} showed how to use a recurrent UNet~\cite{ronneberger2015u} to reconstruct high-quality gray-level images from event data. The results obtained with this method show the richness of the information contained in the events. 
However, reconstructing a gray-level image before applying a detection algorithm adds a further computational step, 
which is less efficient and less accurate than directly using the events, as we will show in our experiments.


Very few other works have focused directly on the task of event-based object detection. 
In~\cite{cannici2019asynchronous}, the authors propose a sparse convolutional network inspired by the YOLO network~\cite{redmon2016you}.
While in~\cite{li2017adaptive}, 
temporally pooled binary images from the event camera are fed to a faster-RCNN~\cite{ren2015faster}.
However, these methods have only been tested on simple sequences, with a few moving objects on
a static background. As we will see, feed-forward architectures are less accurate 
in more general scenarios. 


The lack of works on event-based object detection is also related to the scarce availability of large benchmarked datasets.
Despite the increasing effort of the community~\cite{orchard2015converting,amir2017low,binas2017ddd17,zhu2018multivehicle},
very few datasets provide ground-truth for object detection.
The authors of~\cite{miao2019neuromorphic} provide a pedestrian detection dataset. However, it is composed of only 12 sequences of 30 seconds.
Simulation~\cite{rebecq2018esim,gehrig2019video} is an alternative way to obtain large datasets.
Unfortunately, existing simulators use too simplified hardware models to accurately reproduce all the characteristics of event cameras.
Recently~\cite{de2020large} released an automotive 
dataset for detection. However, it is acquired with a low-resolution QVGA event camera 
and it contains low frequency labels (Hz). We believe instead that high-spatial resolution and
high-labeling frequency are crucial to properly evaluate an automotive detection pipeline. 





%
 
\section{Event-based Object Detection}
\label{sec:object_detection}
In this section, we first formalize the problem of object detection with an event camera,
then we introduce our method and the architecture used in our experiments.
\vspace{-2mm}
\subsection{Problem Formulation}
\label{subsec:formulation}
Let  be an input sequence of events, with 
 and  the spatial coordinates of the event, 
 the event's polarity and  its timestamp. 
We characterize objects by a set of bounding boxes 
,
where,  are the coordinates of the top left corner of the bounding box,
 its width and height,  the label object class, 
and  the time at which the object 
is present in the scene. 

A general event-based detector is given by a function , mapping  to 
.
Since we want our system to work in real time, we will assume that the output of a detector 
at time  will only depend on the past, i.e.\ on events generated before : 
,
where  outputs 
bounding boxes at time . In this work, we want to learn .
    
Applying the detector  at every incoming event is too expensive and often not required by the final 
applications, since the apparent motion of objects in the scene is typically much slower than the pixels response time. 
For this reason, we only apply the detector at fixed time intervals of size :

with \footnote{
Our method can also be applied on a fixed number of events.
For clarity, we only describe the fixed  case.
}.
However, a function  working an all past events  for every , 
would be computationally intractable, since the number of input events would indefinitely increase over time. 

A solution would be to consider at each step , only the events in the interval 
, as it is done for example in~\cite{zhu2018ev,maqueda2018event} for other event-based tasks. However, as we will see in Sec.~\ref{sec:experiments}, this approach leads to poor results for object detection.
This is mainly due to two reasons: first, it is hard to choose a single  (or a fixed number of events) working for objects having very different speeds and sizes, such as cars and pedestrians. Secondly, since events contain only relative change information, 
an event-based object detector must keep a memory of the past. 
In fact, when the apparent motion of an object is zero, it does not generate events anymore. 
Tracking objects using hard-coded rules is generally not accurate for edge cases such as reflections, moving shadows or object deformations. 

For these reasons, we decide to learn a memory mechanism  end-to-end, directly from the input events.
To use past events information while keeping computational cost tractable, 
we choose  such that 

where  is an internal state of our model encoding past information at time . For each , we define  by a recursive formula , with .
In the next sections, we describe the recurrent neural network architecture we propose to learn  and .
\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{figures/method_overview.pdf}
    \caption{Overview of the proposed architecture. Input events are used to build a tensor map  at every time step . Feed-forward convolutional layers extract low-level features from . Then, ConvLSTM layers extract high-level spatio-temporal patterns. Finally, multiscale features from the recurrent layers are passed to the output layers, to predict bounding box locations and class. 
Thanks to the memory of the ConvLSTM layers, temporal information is accumulated and preserved over time, allowing robust detections even when objects stop generating events in input.
}
    \label{fig:method_overview}
    \vspace{-4mm}
\end{figure}

\subsection{Method}
\label{sec:method}
In this section, we describe the recurrent architecture we use to learn the detector . In order to apply our model, we first preprocess the events to build a dense 
representation. More precisely, given input events  we compute a tensor map , with  the number of channels. 
We denote  in the following. Our method is not constrained to a particular  (cfr. Sec.~\ref{subsec:ablation_study}). 




To extract relevant features from the spatial component of the events,  is fed as input to a convolutional neural network ~\cite{krizhevsky2012imagenet,liu2016ssd}. In particular, we use
Squeeze-and-Excitation layers~\cite{hu2017squeezeandexcitation}, 
as they performed better in our experiments.
In addition, we want our architecture to contain a memory state to accumulate 
meaningful features over time and to remember the presence of objects, even when 
they stop generating events. 
For this, we use ConvLSTM layers~\cite{xingjian2015convolutional}, which have been 
successfully used to extract spatio-temporal information from 
data~\cite{finn2016unsupervised,liu2018mobile}.


\comment{We notice that we would like the internal state of our network to correspond to high-level and semantic features, changing slowly over time. By contrast, local or fast-changing features can be addressed via feed-forward layers. \JonathanMrmk{if hidden is smooth output FC cannot be jumpy unless unstable. careful here. do we need this sentence?}
For these reasons, in our network we first stack  feed-forward convolutional layers and then we introduce the ConvLSTM layers for the  consecutive ones (cfr. Fig.~\ref{fig:method_overview}). 
}
Our model first uses  feed-forward convolutional layers to extract high-level semantic features that are then fed to the remaining  ConvLSTM layers
(cfr. Fig.~\ref{fig:method_overview}). 
This is to reduce the computational complexity and memory footprint of the method due to 
recurrent layers operating on large feature maps, and more importantly to avoid the recurrent 
layers to model the dynamics of low-level features that is not necessary for the given task.
We denote this first part of the network \textit{feature extractor}.


The output of the feature extractor is fed to a bounding box  \textit{regression head}. In this work, we use Single Shot Detectors (SSD)~\cite{liu2016ssd}, since they are a good compromise between accuracy and computational time. However, our feature extractor could be used in combination with other detector families, such as two-stage detectors. 
Since we want to extract objects for a large range of scales, we feed features at 
different resolutions to the regression head. In practice, we use the feature map from
each of the recurrent layers.
A schematic representation of our architecture is provided in
Fig.~\ref{fig:method_overview}.









As typically done for object detection, to train the parameters of our network, we optimize a loss function composed of a regression term  for the box coordinates and 
a classification term  for the class.
We use smooth  loss~\cite{liu2016ssd}  for regression and the softmax focal loss~\cite{lin2017focal} for classification.
More precisely, for a set of  ground-truth bounding boxes at time , we encode their coordinates in a tensor  of size , as done in~\cite{liu2016ssd}, where  is the number of default boxes of the regression head matching a ground-truth box.
Let  be the output of the regression head, with  the tensor encoding the prediction for the above  default boxes and  the class probability distribution for all default boxes.
Then, the regression and classification terms of the loss are:

where  is the probability of the correct class .
We set the constant  to 2 and also adapt the unbalanced biases for softmax logits in the spirit of~\cite{lin2017focal}.
\comment{
Then, the regression term of the loss is:

where  is the smooth  loss~\cite{liu2016ssd} averaged over the components of the input tensors. The classification loss instead is given by:

where  is the probability of the correct class, estimated with the softmax, and the constant  is set to 2.
We also adapt the unbalanced biases for softmax logits in the spirit of ~\cite{lin2017focal}.
}
\begin{figure*}[t]
    \centering
      \begin{tabular}{cc}
        \includegraphics[width=0.45\textwidth]{figures/consistency_loss.pdf}&
        \includegraphics[width=0.45\textwidth]{figures/iou_tracks_timenorm2_.png}\\
        \vspace{-2mm}
        (a)&
        (b)
	\end{tabular}
    \caption{(a) Detail of the box regression heads. In order to regularize temporally our network, we introduce a secondary regression head, predicting, at time , the boxes  for time . We impose predictions corresponding to the same time step to be consistent. (b) IoU between ground truth tracks and predicted boxes over time. The consistency loss helps obtaining more precise boxes.}
    \vspace{-5mm}
\label{fig:consistency}
\end{figure*}
\subsubsection{Dual Regression Head and Temporal Consistency Loss}
\label{subsec:consistency}
To have temporally consistent detections, we would like the 
internal states of the recurrent layers to learn high-level features that are stable over long periods of time.
Even if ConvLSTM can, to some extent, learn slow-changing representations, we further improve detections consistency by introducing an auxiliary loss and an additional regression head trained to predict bounding boxes one time-step into the future. 
The idea is inspired by unsupervised learning methods such as word2vec~\cite{word2vec} and CPC~\cite{cpc}, 
which constraint the latent representation to preserve some domain structure. 
In our case, given that features are shared for both heads, we argue that this has the 
additional effect of inducing representations that account for object motion, something
which is currently used as regularization, but would require further analysis that goes
beyond the scope of the current work.



Given the input tensor  computed in the time interval , the two regression heads will output bounding boxes
 and , trying to match ground truth  and  respectively.
This dual regression mechanism is shown in Fig.~\ref{fig:consistency}(a). 

To train the two regression heads, we add to our loss an auxiliary regression term between  and . 
This term, when applied at every time step , indirectly constraints the output 's of the second head to be close to the predictions 's of the first 
head at the next time step, cfr. Fig.~\ref{fig:consistency}(a). 
However, since the two heads are independent, they could converge to different
solutions. Therefore, we further regularize training by adding another loss term
explicitly imposing  to be close to . In summary, the auxiliary loss is:

\comment{
\subsubsection*{Boxes decoding}  The additional head mirrors exactly the regression head (the same number of layers and the same number of parameters). For each anchor box in the first branch, there is a matching anchor box. So if a box  corresponding to the anchor  has a sufficient confidence level and has not been discarded by the NMS mechanism, the box  should represent the estimation of the box  at next timestamp. The coordinates () of  should be decoded in the same way as . Note that although we use this additional regression branch as an ancillary loss during training, it could be used during inference to predict the next box position.
\subsubsection*{speed loss}  In order to compute the loss for the secondary regression \JonathanMrmk{secondary is used only another time in the text. introduce and it will be clear i think} branch, the assignment must have been computed for the first one. We are then taking advantaged of the object IDs present in the ground truth. We replace the ground truth boxes at time  by the ground truth boxes at time  sharing the same ID. If there is no box of similar ID at time , the anchor doesn't contribute to the loss.

\subsubsection*{Temporal consistency loss}  We denote by  the output of the box regression head of our network and  the output of the secondary box regression head of our network.  represents the subtensor of  where only anchors matching with ground-truth boxes of object id \JonathanMrmk{object-id or object is?} present both at time  AND  . That is to say for an anchor to be part of  it needs to match with of box of id  and box of id  so that .
Then the loss is 

\bigskip
}
Then, the final loss we use during training is given by .
We minimize it during training using truncated backpropagation through time~\cite{werbos1990backpropagation}.


\comment{

\subsubsection{Events Input Representation}
\label{subsec:ev2dense}
\amosSrmk{To change with event cube?}
\JonathanMrmk{This section describes how the input map  is build.}
In this section, we describe out the input map  is built. 

Although we tried different representation proposed in the literature (cfr. Sec.~\ref{subsec:ev2dense}), we found that simple histograms of events are the best compromise in terms of computational cost and detection accuracy. 

An histogram of events , for events  is simply given by counting for each pixel and for each polarity, the number of events occurred in the time interval . More precisely,



In order to improve robustness to noise in the events, we clamp  to a maximum value. In particular, 
this makes our representation robust to \textit{crazy pixels} [REF ], sometimes also denoted as hot pixels [ref], which are pixels producing extremely high event rates, which does not correspond to real input signals. In our experiments, we set this maximum value to 20, even if we saw in practice little difference of performance in the range [0,255], allowing a quantization of the input layer.
Finally, after clamping, we normalize  between 0 and 1.





} 
\section{The 1 Megapixel Automotive Detection Dataset}
\label{sec:datasets}
\vspace{-2mm}
In this section, we describe an automated protocol to generate datasets for event cameras.
We apply this protocol to generate the detection dataset used in our experiments. 
However, our approach can be easily adapted 
to other computer vision tasks, such as face detection and 3D pose estimation. 
\vspace{-1mm}
\paragraph{Setup and Fully Automated Labeling Protocol}
The key component to obtaining automated labels is to do recordings with an event camera and a standard RGB camera
side by side. Labels are first extracted from the RGB camera and then transferred to the event camera pixel coordinates by using a geometric transformation.
In our work, we used the 1 megapixel event camera of~\cite{finateu20205} and a GoPro Hero6.
The two cameras were fixed on a rigid mount side by side, 
as close as possible to minimize parallax errors.
For both cameras, we used a large field of view:
~110 degrees for the event camera and ~120 degrees for the RGB camera. The video stream of the RGB camera is recorded at 4 megapixels and 60fps.
Once data are acquired from the setup, we perform the following label transfer:
1. Synchronize the time of the event and frame cameras; 2. Extract bounding boxes from the frame camera images; 3. Map the bounding box coordinates from the frame camera to the event camera.
The bounding boxes from the RGB video stream are obtained using a commercial automotive detector, outperforming freely available ones.
The software returns labels corresponding to pedestrians, two-wheelers, and cars. 
The time synchronization can be done using a physical connection between the cameras.
However, since this is not always possible, we also propose in the supplementary material an algorithmic way of synchronizing them.
Once the 2 signals are synchronized temporally, we need to find a geometric transformation mapping pixels from the RGB camera to the event camera.
Since the distance between the two cameras is small,
the spatial registration can be approximated by a homography. Both time synchronization and homography estimation can introduce some noise in the labels. 
Nonetheless, we observed time synchronization errors smaller than the discretization step  we use, and that
the homography assumption is good enough for our case, since objects encountered in automotive scenarios are relatively far compared to the cameras baseline.
We discuss more in depth failure cases of the labeling protocol in Sec.~\ref{subsec:failure_cases}.
More details can be also be found in the supplementary material.


\vspace{-3mm}
\paragraph{Recordings and Dataset Statistics}
Once the labeling protocol is defined, we can easily collect and label a large amount of data.
To this end, we mounted the event and frame cameras behind 
the windshield of a car. We asked a driver to drive in a variety of scenarios, including city,
highway, countryside, small villages, and suburbs. 
The data collection was conducted over several months, 
with a large variety of lighting and weather conditions during daytime.
At the end of the recording campaign, a total of 14.65 hours was obtained. 
We split them in 11.19 hours for training, 2.21 hours for validation, and 2.25 hours for testing.
The total number of bounding boxes is 25M.
More statistics can be found in the supplementary material, together with examples from the dataset.
To the best of our knowledge, the presented event-based dataset is the largest in terms of labels and classes.
Moreover, it is the only available high-resolution detection dataset for event cameras\footnote{Dataset available at: \href{https://www.prophesee.ai/category/dataset/}{prophesee.ai/category/dataset/}}.














 
\vspace{-3mm}
\section{Experiments}
\label{sec:experiments}
\vspace{-2mm}
In this section, we first evaluate the importance of the main components of our method in an ablation study. Then, we compare it against state-of-the-art detectors.
We consider the COCO metrics~\cite{lin2014microsoft}
and we report COCO mAP, as is it widely used for evaluating detection algorithms. Even if this metric is designed for frame-based data, we explain in the supplementary material how we extend it to event data.
Since labeling was done with a 4 Mpx camera, but the input events have lower resolution, in all our experiments, 
we filter boxes with diagonal smaller than 60 pixels. 
All networks are trained for 20 epochs using ADAM~\cite{kingma2014adam} and learning rate  with exponential decay of  every epoch. 
We then select the best model on the validation set and apply it to the test set to report the final mAP.
\footnote{Evaluation code at \href{https://github.com/prophesee-ai/prophesee-automotive-dataset-toolbox}{github.com/prophesee-ai/prophesee-automotive-dataset-toolbox}
}
\vspace{-2mm}
\subsection{Ablation Study}
\label{subsec:ablation_study}
As explained in Sec.~\ref{sec:method}, our network can take different representations as input. Here we compare the commonly used Histograms of events~\cite{moeys2016steering,maqueda2018event}, Time Surfaces~\cite{lagorce2016hots}, and Event Volumes~\cite{zhu2019unsupervised}. The results are given in Tab.~\ref{tab:ablation_study}. We see that Event Volume performs the best. Time Surface is  points less accurate than Event Volume, but more accurate than simple Histograms. We notice that we could also learn the input representation  together with the network. For example by combining it with~\cite{cannici2020matrix,tulyakov2019learning,gehrig2019end}. However, for efficiency reasons, we decided to use a predefined representation and introduce the memory mechanism in the deeper layers of the network, rather than at the pixel level.

In a second set of experiments, we show the importance of the internal memory of our network.
To do so, we train while constraining the internal state of the recurrent layers to zero. 
As we can see in Tab.~\ref{tab:ablation_study}, the performance drops by 12\%, showing that the memory of the network is fundamental to reach good accuracy.
Finally, we show the advantage of using the loss  of Sec.~\ref{subsec:consistency}.
When training with this term, mAP increases by  , and COCO   increases by  , Tab.~\ref{tab:ablation_study}. This shows the advantage of using , especially with regard to box precision. 
In order to better understand the impact of the loss  on the results, we compute the Intersection over Union (IoU) between 1000 ground truth tracks from the validation set and the predicted boxes. We normalize the duration of the tracks to obtain the average IoU. As shown in Fig.~\ref{fig:consistency}(b), with , IoU is higher for all tracks duration.   







\comment{
\amosSrmk{put details in suppl material}

\subsection{Evaluation Methodology}
\label{subsec:evaluation_methodology}
For our event-based detection task, ground-truth are given by
a set of bounding boxes at fixed frequency. 
If we assume that the detections have the same frequency as the ground truth,
accuracy computation is equivalent to evaluating frame-based detection algorithm.
If instead detections have a different rate than the ground truth,
or even if detections are asynchronous, we can easily reduce to the same frequency case, 
by restricting evaluation only at timestamps on which we have both detections and ground-truth
information, adding if necessary a small time tolerance.
In both cases we can therefore we can use standard detection metrics. 

For both the dataset we consider, ground-truth is annotated starting from frame-based images.
For this reason, there might be some ground-truth bounding boxes at the beginning of a sequence
for which no events have been yet generated. Since it would be impossible for
an algorithm to predict the presence of an object in this particular condition, 
we decide to ignore such boxes during both training and evaluation. 
More precisely, we ignore bounding boxes in the first 0.5 seconds of a sequence if they contain 
less than 10\% of pixels with events. 
Notice that, after this initial time interval, 
we consider all the boxes, even if they contain no events.

Similarly, since the frame-based camera of Sec.~\ref{sec:datasets}, used for annotation has larger resolution,
too far objects are not clearly distinguishable in the event camera.
For this reason, we ignore bounding boxes with diagonal size smaller then 60 pixels, 
during both training and evaluation.
}
\begin{figure*}[t]
    \centering
     \includegraphics[width=1\textwidth]{figures/results1_t.pdf}
    \caption{Detections on a 1 Mpx Dataset sequence. 
    From top to bottom: \textbf{Events-RetinaNet}, \textbf{E2Vid-RetinaNet} (with the input reconstructed images), and our method \textbf{RED}. 
    Thanks to the memory representation learned by our network, 
    \textbf{RED} can detect objects even when they stop generating events, as for example the stopped car on the right, even when occluded by the motorbike.}
\label{fig:gen4_results}
\end{figure*}
\begin{table}[htbp]
    \caption{Ablation study on the 1Mpx  Dataset. \textbf{Left}: mAP for different input representations (without consistency loss). \textbf{Right}: mAP and  without some components of our method, with Event Volume as input. "w/o memory" means forcing the internal state  to be zero, for all recurrent layers.}
    \vspace{1mm}
    \begin{subtable}{.5\linewidth}
      \centering
\tabcolsep=0.11cm
		\begin{tabular}{@{}ccc@{}}
			\toprule
			 Histogram & Time Surface & Event Volume\\             
			\hline
		     0.37  & 0.39   &   0.41\\
			\bottomrule
		\end{tabular}\end{subtable}\begin{subtable}{.5\linewidth}
      \centering
\tabcolsep=0.11cm
		\begin{tabular}{@{}ccc@{}}
			\toprule
			w/o memory & w/o  loss & memory +  loss\\             
			\hline
			0.29 | 0.26 &  0.41 | 0.40   & 0.43 | 0.44     \\    
			\bottomrule
		\end{tabular}\end{subtable} 
    \label{tab:ablation_study}
\end{table}
\subsection{Comparison with the State-of-the-art}
\label{subsec:results}
We now compare our method with the state-of-the-art on the 1 Mpx Detection Dataset and the Gen1 Detection Dataset~\cite{de2020large}, which is another automotive dataset acquired with a QVGA event camera~\cite{posch2010qvga}.

We denote our approach \textbf{RED} for Recurrent Event-camera Detector.
For these experiments, we consider Event Volumes of 50ms. Since there are not many available algorithms for event-based detection,
we use as a baseline a feed-forward architecture applied to the same input representation as ours,
thus emulating the approach of~\cite{miao2019neuromorphic} and~\cite{li2017adaptive}. 
We considered several architectures, leading to similar results. We report here those of RetinaNet~\cite{lin2017focal} with ResNet50~\cite{he2016deep} backbone and a feature pyramid scheme, since it gave the best results. We refer to this approach as \textbf{Events-RetinaNet}.
Then, we consider the method of~\cite{rebecq2019high}, 
which is currently the best method to reconstruct graylevel images from events and uses a recurrent Unet. For this, we use code and network publicly released by the authors. Then, we train the RetinaNet detector on these images.
We refer to this approach as \textbf{E2Vid-RetinaNet}. 
For all methods, before passing the input to the first convolutional layer of the detector, 
input height and width are downsampled by a factor 2.
For the Gen1 Detection Dataset, we report also results available from the literature~\cite{messikommer2020event}.


Finally, since the 1 Mpx Dataset was recorded together with a RGB camera, we can train a frame-based detector on these images. Since events do not contain color information, we first convert the RGB images to grayscale. Moreover, to have the same level of noise in the labels due to the automated labeling, we map frame camera pixels to the same resolution and FOV as the event camera.
In this way, we can have an estimation 
of how a grayscale detector would perform on our dataset.
Similarly, since the Gen1 Dataset was acquired using an event camera providing gray levels, we could run the RetinaNet detector on them.
We refer to this approach as \textbf{Gray-RetinaNet}.


The results we obtain are given in Tab.~\ref{tab:results}. We also report the number of parameters of the networks and the methods runtime, including both events preprocessing and detector inference, on a i7 CPU at 2.70GHz and a GTX980 GPU.  
Qualitative results are provided in Fig.~\ref{fig:gen4_results} and in the supplementary material.
From Fig.~\ref{fig:gen4_results}, we see in particular that our model continues detecting the car even when it does not generate events.
While \textbf{Events-RetinaNet} becomes unstable and \textbf{E2Vid-RetinaNet} begins oversmoothing the image, and thus loses the detection. 
As we can see from Tab.~\ref{tab:results}, our method outperforms all the other event-based ones by a large margin. On the 1Mpx dataset the images reconstructed by~\cite{rebecq2019high}  are of good quality and therefore \textbf{E2Vid-RetinaNet} is the second best method, even if 18\% points behind ours. Instead, on the Gen1 Dataset, the model of~\cite{rebecq2019high} does not generalize well and images are of lower quality. Therefore, on this dataset, \textbf{Events-RetinaNet} scores better. 
Our method reaches the same mAP as \textbf{Gray-RetinaNet} on the 1Mpx Dataset, making it the first event-camera detector with comparable precision to that of commonly used frame-camera detectors. 
If we also consider color, the RetinaNet mAP increases to 0.56, confirming that color information is usefull to increase the accuracy. Our method could benefit from color if acquired by the sensor, such as in~\cite{li2015design}.
On the Gen1 Dataset our method performs slightly worse, this is due to the higher level of noise of the QVGA sensor and also because the labels lower frequency makes training a recurrent model more difficult.
Finally, we observe that our method has less parameters than the others, it runs realtime and, on the 1Mpx Dataset, it is 21x faster than \textbf{E2Vid-RetinaNet}, which reconstructs intensity images.
\begin{table}[htpb]
\vspace{-3mm}
	\caption{Evaluation on the two automotive detection datasets.}
	\begin{center}
		\tabcolsep=0.11cm
		\begin{tabular}{l ccc |cc}
			\toprule
			& \multicolumn{3}{c}{1Mpx Detection Dataset} & \multicolumn{2}{c}{Gen1 Detection Dataset}\\
			\hline
			                                                                      & {mAP}  & runtime (ms) & params (M)  & {mAP}  & runtime (ms) \\
			\hline
\textbf{MatrixLSTM}~\cite{cannici2020matrix}                          & -      & -       & -           &   0.31     & - \\\textbf{SparseConv}~\cite{messikommer2020event}                       & -      & -       & -           &   0.15     & - \\
			\textbf{Events-RetinaNet}                                             & 0.18   & 44.05    & 32.8       &   0.34     & 18.29 \\
			\textbf{E2Vid-RetinaNet}                                              & 0.25   & 840.66   & 43.5       &   0.27     & 263.28 \\
			\textbf{RED} (ours)                                                   & \textbf{0.43}   & \textbf{39.33}    & \textbf{24.1}       &   0.40     &\textbf{16.70} \\
			\hline
			\textbf{Gray-RetinaNet}                                               & \textbf{0.43}   & 41.43    & 32.8        &  \textbf{0.44}    & 17.35 \\
			\bottomrule
			\multicolumn{3}{l}{\small{ Provided by the authors, using a pretrained YOLOv3.}}
		\end{tabular}\end{center}
 	\vspace{-4mm}
	\label{tab:results}
\end{table}
\begin{figure*}[t]
    \centering
      \begin{tabular}{@{}c@{}c@{}c@{}c}
        \includegraphics[width=0.25\textwidth]{figures/fail_fpgt.png}&
        \includegraphics[width=0.25\textwidth]{figures/fail_misaligment.png}&
        \includegraphics[width=0.25\textwidth]{figures/fail_doubledet.png}&
        \includegraphics[width=0.25\textwidth]{figures/fail_labeldet.png}
	\end{tabular}
    \caption{Labeling and detector failure cases. White boxes correspond to labels, colored boxes to detections. From left to right: Labels outlier, labels misalignment, double detection, label swap.}
    \vspace{-2mm}
\label{fig:failure_cases}
\end{figure*}
\vspace{-1mm}
\subsection{Failure Cases}
\label{subsec:failure_cases}
In this section, we discuss some of the failure cases of our network and of the automated labeling protocol of Sec.~\ref{sec:datasets}. In Fig.~\ref{fig:failure_cases}, we show the results of our detector together with the ground truth on some example sequences. In the ground truth we observe two types of errors: geometric errors and semantic errors. Geometric errors, such as misalignment between objects and bounding boxes are due to an imprecise temporal and spatial registration between the event and the frame cameras. Semantic errors, such as label swaps or erroneous boxes, are due to wrong detections of the frame based software used for labeling. Our detector can correct some of the geometric errors, if the errors in the box position are uniformly distribuited around the objects.
Semantic labels are harder to correct, and an outlier robust loss might be beneficial during training. In addition, we observe that our detector can produce double detections and is less accurate on small objects. 

\subsection{Generalization to Night Recordings and Other Event Cameras}
\label{subsec:night_results} 
We now study the generalization capabilities of our detector.
First, we focus on applying our detector, trained with daylight data only, on night recordings.
Since event cameras are invariant to absolute illuminance levels, an event-based detector should 
generalize better than a frame-based one.
To test this, we apply \textbf{RED} and \textbf{Gray-RetinaNet} detectors on
new recorded night sequences, captured using the event camera of 
Sec.~\ref{sec:datasets} and a HDR automotive camera.
We stress the fact that these networks have been trained exclusively on daylight data. Since the frame-based labeling software of Sec.~\ref{sec:datasets} is not accurate enough for night data, we report qualitative results in Fig.~\ref{fig:night} and in the appendix.
It can be observed that the accuracy of \textbf{Gray-RetinaNet} drops considerably.
This is due to the very different lighting and the higher level of motion blur
inherently present in night sequences. On the contrary, our method performs
well also in these conditions. 

In a second experiment, we test the generalization capability of our network when using different camera types as input.
Since there is no available 
dataset with object detection labels, 
we report qualitative results on the MVSEC dataset~\cite{zhu2018multivehicle}, which is an automotive dataset acquired with a DAVIS-346 camera.
For this purpose we use the model trained on the Gen1 Dataset, since it was acquired with an ATIS camera with similar resolution as the DAVIS. From Fig.~\ref{fig:davis_det}, we see that even if the model was trained on a different camera, it generalizes well also on the DAVIS sequences.
\begin{figure*}[t]
    \centering
      \begin{tabular}{@{}c@{}c@{}c}
        \includegraphics[width=0.33\textwidth]{figures/night_frames_4b.png}&
        \includegraphics[width=0.33\textwidth]{figures/night_frames_2.png}&
        \includegraphics[width=0.33\textwidth]{figures/night_frames_3.png}\
\mathcal{L}_s(B,B^*)&=\frac{1}{N}\sum_j
\mathcal{L}_{s}(B_j,B^*_j) \\
    \mathcal{L}_{s}(B_j,B^*_j) &=  
    \begin{cases}
    |B_j-B^*_j| - \frac{\beta}{2}  & \text{if } |B_j-B^*_j|\geq \beta \\
    \frac{1}{2 \beta} (B_j-B^*_j)^2 & \text{otherwise}
    \end{cases}

H_{p,x,y} = \min \left(1,\sum_{i, x_i=x, y_i=y, p_i=p} 1/m\right).

T_{p,j,x,y} &= \exp\left( \frac{ts_{p,x,y}-\max_{x,y}(ts_{p,x,y})}{\tau_j}\right),\\
ts_{p,x,y} &= \max_{i,x_i=x,y_i=y,p_i=p} t_i .

V_{t,p,x,y} &= \sum_{i, x_i=x, y_i=y, p_i=p} \max(0,1-|t-t_i^*|) \\
t_i^* &= (B-1) \frac{t_i-t_0}{t_I-t_1}

We omit the bilinear kernel for x, y as we restrict to build the volume at maximum resolution. We simply downsample after with a dense operator.  In the experiments, we generate Event Volumes of  bins and 2 polarities (for ON and OFF events). The input tensor shape used in the experiment is , where bins and polarity are combined in the first dimension. 



\section{Neural Network Architecture}
\label{sec:architecture}
The proposed neural network \textbf{RED} consists of a feature extractor that is fed to  bounding box regression heads.
The feedforward part of the proposed feature extractor uses Squeeze-Excite Layers, denoted SE and described in Tab.~\ref{tab:architecture2}. The ConvLSTM uses a BatchNorm with Conv Layer for input-to-hidden connection, and a plain Conv Layer for hidden-to-hidden connection. We run input-to-hidden connections (including the BatchNorm layer) in parallel for all timesteps of a batch. Detailed number of layers and parameters for each layer are given in Tab.~\ref{tab:architecture}.


\begin{table}[p]
	\caption{Architecture of the Feature Extractor.}
	\begin{center}
		\tabcolsep=0.11cm
		\begin{tabular}{lcccc}
			\toprule
			 & \textbf{Layer Type} & \textbf{Kernel size} & \textbf{Channels Out} & \textbf{Stride}\\
			\hline
            Layer1 & BNConvReLU & 7 & 32 & 2\\
            Layer2 & Squeeze-Excite & 3 & 64 & 2\\
            Layer3 & Squeeze-Excite & 3 & 64 & 2 \\ 
            Layer4 & ConvLSTM & 3 & 256 & 2\\
            Layer5 & ConvLSTM & 3 & 256 & 2\\
            Layer6 & ConvLSTM & 3 & 256 & 2\\
            Layer7 & ConvLSTM & 3 & 256 & 2\\
            Layer8 & ConvLSTM & 3 & 256 & 2\\
			\bottomrule
		\end{tabular}\end{center}
	\label{tab:architecture}
\end{table}

\begin{table}[htpb]
	\caption{Architecture of the Squeeze-Excitation (SE) Layers.}
	\begin{center}
		\tabcolsep=0.11cm
		\begin{tabular}{lcccc}
			\toprule
			 & \textbf{Layer Type} & \textbf{Kernel size} & \textbf{Channels Out} & \textbf{Stride}\\
			\hline
            Layer1 & BNConvReLU & 3 & 256 & 2\\
            Layer2 & BNConvReLU & 3 & 256 & 1\\
            Layer3 & BNConv & 3 & 256 & 2 \\ 
            Layer4 & GlobalAvgeragePooling & MxN & 256 & MxN\\
            Layer5 & DenseReLU & 1 & 64 & 1\\
            Layer6 & DenseSigmoid & 1 & 256 & 1\\
            Layer7 & Elementwise-Multiplication & 1 & 256 & 1\\
            Layer8 & Skip-Sum & 1 & 256 & 1\\
			\bottomrule
		\end{tabular}\end{center}
	\label{tab:architecture2}
\end{table}
\clearpage             

 
\small


\bibliographystyle{unsrtnat}
\bibliography{bibliography}
\end{document}
