\documentclass{article}

\usepackage{booktabs,multirow,array}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{units}
\usepackage{graphicx}
\usepackage{comment}
\usepackage{todonotes}
\usepackage{xcolor}
\usepackage{tikz}
\usetikzlibrary{shadows,arrows}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage[T1]{fontenc} 

\usepackage{placeins}

\usepackage{hyperref}

\usepackage{listings}
\usepackage{mathtools}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\usepackage{thmtools}
\usepackage{thm-restate}
\usepackage[algoruled, algo2e, vlined, titlenotnumbered]{algorithm2e}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

\parindent0pt

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Zn}{\mathbb{Z}^n}
\newcommand{\C}{\mathcal{C}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\NP}{\mbox{\slshape NP}}
\newcommand{\cF}{\mathcal{F}}
\renewcommand{\mid}{:}
\theoremstyle{plain}

\DeclareMathOperator*{\argmax}{arg\!\max}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corrolary}
\newtheorem{proposition}[theorem]{Proposition}

\theoremstyle{definition}
\newtheorem{defn}[theorem]{Definition}


 \author{Corinna Gottschalk \and Britta Peis} 
\title{Submodular Function Maximization over Distributive and Integer Lattices}


\pagestyle{plain}


\begin{document}
\maketitle

\begin{abstract}
\noindent
The problem of maximizing non-negative submodular functions has been studied extensively in the last few years. 
However, most papers consider submodular set functions. 
Recently, several advances have been made for submodular functions on the integer lattice. 
As a direct generalization of submodular set functions, a function  is submodular, if
 for all  where  and  denote 
element-wise minimum and maximum. 
The objective is finding a vector  maximizing . In this paper, we present a deterministic -approximation
using a framework inspired by \cite{DoubleGreedy}.
Moreover, we show that the analysis is tight and that other ideas used for approximating set functions cannot easily be extended. 

In contrast to set functions, submodularity on the integer lattice does not 
imply the so-called diminishing returns property. Assuming this property, it was shown that many results for set functions can
also be obtained for the integer lattice. 
In this paper, we consider a further generalization. Instead of the integer lattice, we consider a distributive lattice as the function domain
and assume the diminishing returns (DR) property.
On the one hand, we show that some approximation algorithms match the set functions setting. In particular, we can obtain a
-approximation for unconstrained maximization, a -approximation for monotone functions under a cardinality constraint and 
a -approximation for a poset matroid constraint. 
On the other hand, for a knapsack constraint, the problem becomes significantly harder: 
even for monotone DR-submodular functions, we show that there is no -approximation 
for every  under the assumption that  cannot be solved in time . 
\end{abstract}



\section{Introduction}
Recall that a set function  is called submodular if  for all . 
Optimization problems with submodular objective functions have received a lot of attention in recent years.
Submodular objectives are motivated by the principle of economy of scale, and thus find many applications in real-world problems.
Moreover, submodular functions play a major role in combinatorial optimization.
Several combinatorial optimization problems have some underlying submodular structure, for example, 
cuts in graphs and hypergraphs, or rank functions of matroids.

As a breakthrough result, the problem to find a subset  minimizing a submodular function 
has been shown to be solvable in strongly polynomial time in \cite{SubmodularMin1}.
In contrast, the corresponding maximization problem

for a nonnegative submodular function  is easily seen to be \NP-hard, as it contains,  for example,
\textsc{max cut} as a special case.
We refer to \eqref{USM} as \textsc{unconstrained submodular maximization (USM)}.

Recently, a generalization to submodular functions on a bounded subset of the integer lattice  has also been investigated.
For  let  denote  and  denote  
for . 
Then, a function   on a finite set  of the form 

is called submodular if 
 
Clearly, this captures submodular set functions since vectors with entries  and  can be seen as incidence vectors of 
sets and in that case  and  correspond to intersection and union.  is called \emph{bounded integer lattice}.
Submodular functions on the integer lattice have been studied before, for example, in discrete convex analysis, 
 -convex functions are of this type. We provide more details in ``Related Work''.  

 Submodularity for set functions is equivalent to the diminishing returns (DR) property: 
 For  we have , a very natural property in many problems.
 Submodularity on the integer lattice, however, is a weaker property in the sense that Diminishing Returns imply submodularity, but not the other way round.
 Soma and Yoshida \cite{SomaYoshi16} have investigated the setting of DR-submodular functions on the integer lattice. We will use the prefix ``DR'' to describe
 problems that consider DR-submodular functions. 
 
In addition to the integer lattice, we can also consider a more general structure: distributive lattices, that is, lattices where the meet- and join-operation 
 fulfill distributivity. We will provide definitions in Section \ref{sec:preliminaries}. 
 For now, it is important, that due to Birkhoff's theorem, we can represent
 any distributive lattice as the set of all ideals on some poset  and vice versa. 
 Form now on, we always assume that distributive lattices  are (implicitly) given by this poset.   

Maximizing a submodular function subject to some side constraints is also a well-understood problem. 
Typically, these constraints are matroid or knapsack constraints, with the special case of a cardinality constraint, which can be seen as a matroid constraint for 
the so called uniform matroid. 
In contrast to the unconstrained setting, considering monotone functions still yields interesting problems here. 

There are corresponding generalizations of matroids for both types of lattices presented above: 
On the integer lattice, we can consider integer polymatroids, on distributive lattices, there are poset-matroids. 
Hence, the problem of maximizing a submodular function subject to these constraints can be generalized as well. 
 
\subsection{Our results}
In this work, we present approximation algorithms for several constrained and unconstrained submodular maximization problems, 
in particular, we consider the following. 

\emph{USM on the integer lattice.}
Given a submodular function  on a bounded integer lattice , 
we show how to find a pseudopolynomial -approximation. 
Moreover, we show that the analysis is tight and discuss some difficulties for obtaining 
better approximation guarantees using existing techniques that have been applied successfully for set functions (). 
Note that the one-dimensional case where  already illustrates that obtaining an exact polynomial algorithm 
is not possible, since any function on  is submodular.  
A preliminary version of these results was published in \cite{mySubmax}.  

\emph{DR-USM on the distributive lattice.}
Given a DR-submodular function  on a distributive lattice , we show how to generalize the algorithm of 
Buchbinder et al.\ \cite{DoubleGreedy} for set functions to obtain a randomized -approximation. 

\emph{Cardinality and Poset Matroid Constraints on the distributive lattice.}
Given a monotone DR-submodular function  on a distributive lattice , 
we obtain a -approximation 
for maximization under a cardinality constraint and a -approximation under a poset matroid constraint.
 


\emph{Hardness on the distributive lattice.}
While the above results might suggest that submodular maximization on the distributive lattice is not 
fundamentally different from the integer lattice or even from set functions, this surprising result shows otherwise. 
Maximizing a monotone, DR-submodular function under a knapsack constraint is well understood and approximable in the previously mentioned cases.  
However, we show, that for the distributive lattice, there is no -approximation for every  
under the assumption that  cannot be solved in time .

\subsection{Related Work}\label{sec:related-results}
A comprehensive study on USM has been done by Feige, Mirrokni and Vondr\'ak in \cite{LocalSearch} 
who provide and analyze several constant approximation algorithms for USM.
In particular, they present a simple Local Search algorithm that yields a -approximation.
Using a noisy version of  as objective function, they could improve the performance guarantee to .
Feige et al.\ in \cite{LocalSearch} also showed that we cannot hope for a performance guarantee lower than .
They could prove that any  -approximation would require an 
exponential number of queries to the oracle. For symmetric submodular functions, they already show that the ratio is tight.
Subsequently, Oveis Gharan and Vondr\'ak \cite{USMImprovement1} and Feldmann, Naor and Schwartz \cite{USMImprovement2} 
improved the approximation ratio to 0.41 and 0.42 respectively and finally Buchbinder, Feldmann, Naor 
and Schwartz closed the gap and gave a randomzied -approximation for USM in \cite{DoubleGreedy}.
They also present a deterministic -approximation using a similar idea. 
Both algorithms use a ``Double Greedy'' framework that starts with two different sets and, 
for a fixed order of the elements, decides in each step which of the two sets should be modified using the given element.
Later, Buchbinder and Feldman showed in \cite{DerandomizedDoubleGreedy} how to 
derandomize this algorithm to obtain a deterministic -approximation.

For several special types of submodular functions,
better approximation ratios can be proved.
For example, Goemans and Williamson \cite{MaxCut1} 
provide an -approximation for \textsc{max cut} and a -approximation for \textsc{max dicut} based on semidefinite programming and 
Ageev and Sviridenko give a -approximation for \textsc{maximum facility location} \cite{FacLoc1}. 


Furthermore, USM has applications in marketing in social networks and revenue maximization \cite{SocialNetworks}. 
Submodular function maximization also plays a crucial role in algorithmic game theory. 
For example, Dugmi et al.\ use USM as well as constrained submodular maximization for analyzing auctions in \cite{GameTheory1} and 
Schulz and Uhan use USM as a subroutine to calculate the \emph{least core} in cooperative games \cite{GameTheory2}.

Since vectors with entries  and  can be seen as incidence vectors of sets, 
submodular functions on the integer lattice can be seen as a direct generalization of submodular set functions and in particular, all hardness results still apply. 
Moreover, the integer lattice can be interpreted as a specific distributive lattice that consists of disjoint chains. 
However, we remark that this correspondence is only pseudopolynomial, i.e.\ we need a pseudopolynomial number of 
elements to represent the integer lattice as a distributive lattice.  
Moreover, Soma and Yoshida observed that, for a DR-submodular function on the integer lattice, 
there is an equivalent submodular function on a Boolean lattice whose size is pseudopolynomially bounded (by  and the upper bound for the components).
We refer to a talk given at a Hausdorff Trimester Program which is also publicly available \cite{SomaBonnTalk}.
In particular, any approximation algorithm for submodular set functions yields a pseudopolynomial approximation 
of the same guarantee for DR-submodular functions on the integer lattice, where the running time depends on the bound . 
Thus, assuming DR-submodularity, the challenge on the integer lattice is developing polynomial algorithms.
In particular, there is a polynomial -approximation for DR-USM on the integer lattice \cite{SomaDR-SMBIL}. 
However, our hardness result suggests that no such easy reduction to submodular set functions exists for distributive lattices. 

In discrete convex analysis, several special cases of submodular functions on the integer lattice have been investigated. 
For example, -convex and -concave functions are submodular 
and -concave functions can be maximized in polynomial time \cite{Murota}.


As mentioned before, the problem of minimizing a submodular function is solvable in polynomial time. 
This is not only true for submodular set functions, but even for submodular functions on distributive lattices, 
or more precisely on ring families \cite{FujiDistrLattice}. On the other hand, nothing is known for maximization 
on the distributive lattice. 

A considerable amount of work deals with maximizing both monotone and non-monotone submodular functions subject to some side constraints. 
Most of these papers consider submodular set functions. The side constraints that are typically investigated are matroid and knapsack constraints 
and combinations of these. 
The first result in this area goes back to Fisher, Nemhauser and Wolsey, who considered monotone set functions and 
presented a -approximation for a cardinality constraint \cite{FisherGreedyI} 
and a -approximation for  matroid constraints \cite{FisherGreedyII}. 
Calinescu et al.\ improved this to  for one matroid constraint \cite{MonotoneMatroid1}. 
Nemhauser and Wolsey \cite{NemhauserHardnessSM} showed that any 
approximation better than  requires an exponential number of queries if  is accessed via a value oracle even for the case of a
cardinality constraint and monotone functions. 
Moreover, Feige showed that Max--Cover, which is a special case of maximizing a monotone submodular function subject to a cardinality constraint is hard
to approximate to within a factor of  for any fixed  unless . In contrast to the other complexity result, 
the submodular function is given by a closed form and not just accessed by an oracle.   


For monotone set functions, Sviridenko \cite{SviridenkoKnapsack} provided the first -approximation for a knapsack constraint. 
Since a cardinality constraint can also be interpreted as a knapsack constraint, the same hardness result applies.  

Inaba et al.\ \cite{Inaba:KnapsackIL} first considered constrained problems on the integer lattice and 
showed how to obtain a pseudopolynomial -approximation for the monotone knapsack case.
In \cite{SomaYoshi16}, Soma and Yoshida presented -approximations with 
polynomial running times depending on  for DR-submodular monotone functions and a cardinality, polymatroid or knapsack constraint.
Furthermore, they achieve the same approximation guarantee 
for submodular monotone functions and a cardinality constraint using an algorithm whose running time depends 
logarithmically on the ratio of the maximum function value and the minimum positive increase. 



In \cite{ContentionResolution}, Chekuri, Vondr\'ak and Zenklusen consider the very general problem 
of maximizing  a non-monotone submodular function on a down-monotone family of sets, 
which in particular contains the previously mentioned constraints. 
They also provide a concise overview over previous results in non-monotone maximization and we refer to
their paper for details. 
To the best of our knowledge, no results for non-monotone constrained maximization on the integer lattice exist. 

While our main interest in SMBIL is of a theoretical nature with the ultimate goal to gain a better understanding of submodular 
function maximization, there are applications where submodular functions on the integer lattice play a crucial role.
For example, Bolandnazar et al.\ (\cite{SubmodSupplyChains}) showed that Assemble-to-Order Systems for supply chain management
can be optimized by  minimizing a submodular function on the integer lattice. 
\cite{Inaba:KnapsackIL} discusses several applications for submodular maximization on the integer lattice, e.g.\ budget allocation and sensor placement. 
There is also an alternative way to interpret the bounded integer lattice: Let  denote the incidence vector of a multiset
where entries in  specify the multiplicity of individual elements. Then SMBIL can be seen as maximizing a submodular function
on multisets containing at most  copies of each element which gives rise to possible applications. This illustrates for example how sensor placement can be 
seen as an application of SMBIL by allowing the possibility to place multiple sensors at the same location. 

In Figure \ref{fig:SM-overview}, we provide a graphic summary of all problems considered in this paper and related problems.  
An arrow from one box  to box  indicates that problem   generalizes . 
We denote distributive, integer and Boolean lattices by DL, IL and Boolean respectively, where Boolean is equivalent to the the well-studied case of set functions. 
The diminishing returns property is denoted by DR and we remind the reader that every submodular set function is  DR. 
This is not given on integer or distributive lattices, 
where DR-submodular functions are a subclass of submodular functions. 
Those cases where we only have pseudopolynomial algorithms are marked as such by the additional tag "pseudo". 
Due to results on hardness of approximation for USM and (, Boolean), all approximation guarantees except for (poset matroid, DL, DR) and (SMBIL) are tight (up to ).


The paper is organized as follows: The necessary terminology is introduced in Section \ref{sec:preliminaries}. Then, we first investigate unconstrained maximization
on the integer lattice in Section \ref{sec:SMBIL-Algo}. In Section \ref{sec:SM-DR-DL-algos} we present algorithms for several 
maximization problems for DR-submodular functions on the distributive lattice and in Section \ref{sec:SM-DR-DL-hardness}, we present a
strong hardness result. 

\begin{figure}[tb]
  
 \tikzset{block/.style    = {draw, thick, rectangle, minimum height = 2.2em,
    minimum width = 3em}
}

\begin{subfigure}{\textwidth}
\begin{tikzpicture}[auto, thick, node distance=2cm, >=triangle 45]

      
 \draw node at (6.5, 0)[block] (USM) {USM:{ \cite{DoubleGreedy}}};
  \draw node at (3, 0)[block] (DR-SMBIL) {DR-SMBIL: \cite{SomaDR-SMBIL}};
  \draw node at (-0.9, 1)[block, fill = green] (SMBIL) {SMBIL:,pseudo};
  \draw node at (-0.5, -1)[block, fill = green] (DR-SMDL) {DR-SMDL:};
  \draw[->](DR-SMBIL) -- (USM);
  \draw[->](SMBIL) -- (DR-SMBIL);
 \draw[->](DR-SMDL) -- (DR-SMBIL);
 
 \end{tikzpicture}
 \caption{general submodular functions}
 \end{subfigure}

 \begin{subfigure}{\textwidth}
 \begin{tikzpicture}[auto, thick, node distance=2cm, >=triangle 45]

 		\draw [white] (0,4.6) circle (1mm);  
       \draw node at (4.5, 0)[block, align = left] (CardB) {, Boolean:\\  \cite{FisherGreedyI}};
       \draw node at (8.8, 0)[block, align = left] (MatB) {Matroid, Boolean:\\  \cite{MonotoneMatroid1}};
       \draw node at (0, 0)[block, align = left] (KnapB) {Knapsack, Boolean:\\ \cite{SviridenkoKnapsack}};
       
       \draw node at (4.5, 2)[block, align = left] (CardILD) {, IL, DR:\\  \cite{SomaYoshi16}};
       \draw node at (8.8, 2)[block, align = left] (MatILD) {int. polymatroid,\\ IL, DR: \cite{SomaYoshi16}};
       \draw node at (0, 2)[block, align = left] (KnapILD) {Knapsack, IL, DR:\\  \cite{SomaYoshi16}};
       
       \draw node at (4.5, 4)[block, align = left, fill = green] (CardDL) {, DL, DR:\\ };
       \draw node at (8.8, 4)[block, align = left, fill = green] (MatDL) {poset matroid,\\ DL, DR:};
       \draw node at (0, 4)[block, align = left, fill = green] (KnapDL) {Knapsack, DL, DR:\\ no constant};
       
       \draw node at (2, -2)[block, align = left] (KnapIL) {Knapsack, IL:\\ , pseudo \cite{Inaba:KnapsackIL}};
       \draw node at (6, -2)[block, align = left] (CardIL) {, IL:\\ , pseudo \cite{SomaYoshi16}};
       
       \draw[->](MatB) -- (CardB);
       \draw[->](KnapB) -- (CardB);
       \draw[->](CardILD) -- (CardB);
       
        \draw[->](KnapILD) -- (CardILD);
         \draw[->](MatILD) -- (CardILD);
          \draw[->](CardDL) -- (CardILD);
          
           \draw[->](MatDL) -- (CardDL);
              \draw[->](KnapDL) -- (CardDL);
              
              \draw[->](KnapDL) -- (KnapILD);
              \draw[->](KnapILD) -- (KnapB);
              \draw[->](MatDL) -- (MatILD);
              \draw[->](MatILD) -- (MatB);

	\draw[ ->] (KnapIL) to [out=60,in=30](KnapILD);
	\draw[->] (CardIL) to [out=60,in=30] (CardILD);
 \end{tikzpicture}
 \caption{monotone submodular functions}
 \end{subfigure}

\caption{Overview of all problems, those considered in this paper are shaded. 
}\label{fig:SM-overview}
\end{figure}



\section{Preliminaries}\label{sec:preliminaries}
For submodular set functions, it is \NP-hard to decide
whether there exists a set  such that , as mentioned by Feige et al.\ in \cite{LocalSearch}. 
Since our main goal is finding good approximation algorithms, 
throughout this paper, we assume that all submodular functions are nonnegative and given by a value oracle. 


For a vector  we denote by  the vector where all 
but the entry  remain the same and  is set to k.
As usual,  denotes the set  and  
denotes the vector where  and  for all .

\subsection{Submodular maximization on integer lattices.}
Given a bounded integer lattice  
and a submodular function 
, 
we consider the problem of maximizing  on :

We will refer to \eqref{SMBIL} as
\textsc{Submodular maximization on a bounded integer lattice (SMBIL)}.
For ease of notation, we will from now on assume that  and 
. 
Thus, we prove all results for a bounded integer lattice of the form , but 
all results in this paper can be easily generalized to any bounded integer lattice as defined above. 

As mentioned before, SMBIL generalizes USM, thus the hardness of approximation for USM holds.
The integer lattice with  underlying USM is also called the \emph{Boolean lattice}.
Directly generalizing the Diminishing Returns property for set functions, it can be stated for the integer lattice as in \cite{SomaYoshi16}: 
 for vectors  where  denotes the vector whose entries are  
everywhere except for component . 
They also remark that a function  is DR-submodular on the integer lattice  if and only if  is submodular and coordinate-wise concave, i.e.\
 for any  and  \cite{SomaYoshiSubmodularCover}.
Our definition of submodularity differs slightly from that of Soma at al.: while we only define a submodular function on , they define submodularity 
on , but restrict to vectors  with  for all optimization problems. These formulations are equivalent in the sense that 
any submodular function on  can be extended to a submodular function on . 




\subsection{Posets and Distributive Lattices}
For a partially ordered set (poset) , an \emph{antichain} 
is a set  of incomparable elements 
and a \emph{chain} is a set  where each pair of elements is comparable. 
An \emph{ideal} is a set  where  implies 
 for all . By  we denote the set of all ideals for . 

A \emph{lattice} is a poset  any two of whose elements  
have a \emph{meet} , i.e.\ a unique greatest common lower bound, and a \emph{join} , i.e.\ a unique least common upper bound. 
A lattice is called distributive, if meet and join satisfy distributivity. 
One example for such a lattice is the (bounded) integer lattice with component-wise minimum and maximum as join and meet.  
An element  of a lattice is called \emph{join-irreducible}, if it cannot be characterized as the join of two elements  .

For a poset ,  the pair  forms a distributive lattice with union and intersection as join and meet. 
Conversely, any distributive lattice is isomorphic to the lattice of ideals w.r.t.\
the induced poset on the join-irreducible elements of that lattice. This fact is known as Birkhoff's theorem \cite{Birkhoff}.
Therefore, we will assume throughout this paper that a distributive lattice is given in the form   for a poset . 
For the example of the bounded integer lattice of dimension , 
the corresponding lattice of ideals would be the set of ideals  on a poset  consisting of  disjoint chains of length . 
For a more extensive introduction to lattice theory, the reader is referred to \cite{Birkhoff}.

For a poset , we can consider the following generalization of SMBIL.
 
 where
  is the collection of all ideals in the poset  and  a nonnegative submodular function on .
 We refer to \eqref{SMDL} as \textsc{Submodular maximization on distributive lattices (SMDL)}.



The DR-property naturally generalizes to distributive lattices: 
 for  with  and  such that . 
This definition contains both the definition on set functions and on the integer lattice. 



 \subsection{Poset Matroids}
Integer polymatroids can be seen as a generalization of matroids to the integer lattice. 
These form special distributive lattices, as we saw above, and matroid structures can be extended further to distributive lattices as well. 
One way of doing so are poset matroids, and we are going to give a brief introduction here.
The main difference to ordinary matroids is that the ground set is a poset and matroid axioms only have to hold for ideals of this poset.
 \begin{defn}
 Given a partial order , a nonempty family  forms the independent sets of a poset matroid 
 if the following two properties hold:
 
 \begin{itemize}
\setlength{\itemindent}{1em}
  \item[(M1)] 
\item[(M2)]  with .
\end{itemize}
 \end{defn}
 
We remark that matroids are the special case of poset matroids where  consists only of pairwise incomparable elements and 
integer polymatroids correspond to the special case where  consists of disjoint chains. 
Therefore, by considering all subsets instead of the sets in  we obtain the usual matroid definition.

We use the usual matroid terminology for poset matroids, for example, sets in  are called independent and 
a basis in a poset matroid is a maximal independent set.

Poset matroids were introduced by Dunstan, Ingleton and Welsh~\cite{DIW1972} and are also known as \emph{distributive supermatroids}. 
Moreover, they are a special case of \emph{ordered matroids} as introduced by Faigle \cite{Faigle1984} and several other structures.
They share many structural properties with matroids, in particular, Faigle showed that, under a consistency assumption on the cost, 
it is possible to compute a minimum cost basis using a greedy algorithm \cite{Faigle1984}. 
Moreover, Barnabei et al.\ presented a strong base exchange property \cite{BarnabeiSymmExchange} and for two poset matroids on the same poset, 
finding a  common independent set of maximum cardinality is possible in polynomial time as shown by Tardos \cite{TardosPosetIntersection}. 
She also showed that the intersection problem is NP-hard for two poset matroids on posets with different partial orders.  


However, in contrast to polymatroids, as far as we know, there exists no polyhedral description for poset matroids.


It is important to realize that poset matroids are not just the intersection of independent sets of a matroid   and ideals of a poset on .
Consider for example the graphic matroid on the multigraph in Figure \ref{fig:posetMatroidDifference}.
Then the sets  and  are independent ideals,
but property  is not fulfilled.


\begin{figure}[tb]
\centering
\begin{tikzpicture}[node distance=3cm]
\tikzstyle{vertex}=[circle, draw,inner sep=1pt, minimum width=15pt]
\tikzstyle{edge} = [draw]
\node[vertex] (a) at (0,0) {};
\node[vertex] (b) at (2,0) {};
\node[vertex] (c) at (4,0) {};
\draw[edge, bend right] (a) edge node[below]{} (b);
\draw[edge, bend left] (a) edge node[above]{} (b);
\draw[edge, bend right] (b) edge node[below]{} (c);
\draw[edge, bend left] (b) edge node[above]{} (c);
\node[vertex] (e1) at (6,-1) {};
\node[vertex] (e3) at (6,1) {};
\node[vertex] (e2) at (8,-1) {};
\node[vertex] (e4) at (8,1) {};
\draw [->](e1) edge (e3);
\draw [->](e2) edge (e4);
\end{tikzpicture}
\caption{Multigraph and poset on the edges, illustrating that independent ideals in the graphic matroid do not form a poset matroid.}\label{fig:posetMatroidDifference}
\end{figure}






\section{Approximating SMBIL}\label{sec:SMBIL-Algo}
In this section, we consider unconstrained maximization of a general nonnegative submodular function on the integer lattice (SMBIL). In particular, 
we do not require DR-submodularity. 

We first present a -approximation for SMBIL which is inspired by \cite{DoubleGreedy}. 
Then, we show that our analysis is tight and discuss approaches for a randomized Double Greedy algorithm. 
``Double Greedy'' refers to the idea of starting with two vectors  and  and modifying them until both vectors are equal,
 while ensuring  never decreases.
In the beginning  and , i.e.\ initially,  (resp.\ ) is the unique minimal (maximal) element
in . 
Now, we traverse the components in a fixed order, say . For a given index , we change  and  
while maintaining  without decreasing the submodular function .
In particular, we find the best possible way to modify  and  while changing only component . We then choose the modification which yields
the biggest increase in  and set both vectors to that value. 

In the scenario where the integer lattice is bounded by 1, our Algorithm \ref{MyGreedy} coincides with the one in \cite{DoubleGreedy} 
when vectors are interpreted as characteristic vectors of sets. 

\begin{theorem}\label{ApproximationThm}
Let  be a nonnegative submodular function. 
Then Algorithm \ref{MyGreedy} is a -approximation for SMBIL and has running time 
 where  is the time for one call to the oracle representing . 
\end{theorem}

\subsection{Proof of Theorem \ref{ApproximationThm}}

The proof of Theorem \ref{ApproximationThm} relies upon two lemmas. 

\begin{algorithm2e}[htb]
\caption{Generalized Double Greedy for SMBIL}
\DontPrintSemicolon
\textbf{Input:} A bounded integer lattice defined by a bound  and a dimension , 
a nonnegative submodular function  on 
\;
\textbf{Output:} A vector \;
Set , \;
\For{ k = 1 \KwTo n}
{

  \;
  \;
  \If {}
  {
    Let  be the maximal number among those for which  is obtained.\; 
    , \;  
    , \;
  } 
  \Else
  {
    Let  be the minimal number among those for which  is obtained.\; 
    , \;  
    , \;
  }
\;  
}
\Return{ }\;
\label{MyGreedy}
\end{algorithm2e}

First, we show that Algorithm \ref{MyGreedy} really is a Greedy algorithm 
in the sense that  never decreases:
\begin{lemma}\label{IncreaseLemma}
Let  be a submodular function. Then for all 
and vectors ,  as in Algorithm \ref{MyGreedy} the following holds: 
 and . 
\end{lemma}


\begin{proof}
Let  be the component in which the vectors  and  will be changed in an iteration of the loop. 


By definition,  and  are nonnegative.
Suppose that  is changed first, i.e.\ , then  clearly, 
the lemma holds for  and  and .
Now we show that the change from  to  does not lead to a decrease in :

We have
 
 by submodularity of . 
 
 As , the above implies
 
Since  and , it follows that  as desired.  
  
  


The case where  can be shown in the same way.
\end{proof}




Let  denote a fixed optimal solution for SMBIL. 
In order to bound the value of a solution of Algorithm \ref{MyGreedy} with respect to the optimum, 
we define  analogous to \cite{DoubleGreedy}. 
Consequently,  and . 
In Lemma \ref{OptBoundLemma}, we show , 
i.e.\ the decrease in  (going from  to ) is bounded for each change of the vectors  or .

Then, as in \cite{DoubleGreedy}, Lemma \ref{OptBoundLemma} can be used to prove Theorem \ref{ApproximationThm}:




This is equivalent to  and the algorithm returns . 
Since the running time is obvious, 
this concludes the proof of Theorem \ref{ApproximationThm} except for Lemma \ref{OptBoundLemma}: 

\begin{lemma}\label{OptBoundLemma}

For a submodular function   the following holds in Algorithm \ref{MyGreedy} 
for all  where :

\hspace*{2 cm}  
\end{lemma}
\begin{proof}
Let us consider the case where   and let  be the index, where the vectors differ. 
 If  then  
 since  and thus .
 Since Lemma \ref{IncreaseLemma} implies that the right-hand side of the equation is nonnegative, the inequality holds. 
 
So we assume now that .
Since , we have , all other entries of  remain unchanged.
Moreover, , so .
Submodularity of  implies


Suppose that  , otherwise we are done.

Using this assumption and \eqref{eq1} yields
 

But we have .
This is true by design of the algorithm for the first change in an iteration of the loop. 
For the second change in an iteration, the other vector (in this case ) cannot improve further, so the claim holds as well. 

Setting  and \eqref{eq2} imply
 


But in the proof of Lemma \ref{IncreaseLemma}, we have shown that 
, so \eqref{eq2} is a contradiction.
 
The case  can be treated analogously and if neither case applies, then clearly 
 as well.  
\end{proof}



\subsection{The guarantee of  is tight}\label{subsec:TightExampleSec}
Since Algorithm \ref{MyGreedy} generalizes the deterministic algorithm in \cite{DoubleGreedy}, 
the tight example they provide also works for our algorithm. But our example has a few additional properties: 
First, even if we do not prescribe the order of the components in advance 
and instead choose the index that results in the biggest increase in  for each step, 
the algorithm does not yield a better solution (see Theorem \ref{AlgoTightThm} below) which is not true for 
 the example provided in \cite{DoubleGreedy}.
However, it is not possible to attain a -approximation by choosing a particular good order. 
This was shown by Huang and Borodin in \cite{BorodinMyopicBounds}. 
They examine classes of Double Greedy algorithms and provide an upper bound of 
 on approximation ratios achievable by those algorithms. 
These include in particular the algorithm described above. 
Moreover, we will show in the next subsection that our example also has implications when trying to generalize the 
-approximation presented in \cite{DoubleGreedy}.
\begin{theorem}
\label{AlgoTightThm}
 For an arbitrarily small constant  there exists a submodular function 
 for which Algorithm \ref{MyGreedy} provides only a -approximation.
 This result still holds if the components are ordered such that at any time the
 chosen component yields the biggest increase in .
\end{theorem}
\begin{figure}[htb]

	 \tikzstyle{vertex}=[circle, draw,
                        inner sep=1pt, minimum width=13pt]
	\tikzstyle{edge} = [draw,thick]
	\centering
\tikzstyle{vertex}=[circle, draw,
                        inner sep=1pt, minimum width=13pt]
	\tikzstyle{edge} = [draw,thick]
	
  	\centering
	\begin{tikzpicture}[scale = 0.8]
	\foreach \x/\y/\nr/\f/\set in {{0/0/0/0/\tiny }, {-2/1/1/1/\tiny }, {0/1/2/1/\tiny}, {2/1/3/1/\tiny}, 
				  {-5/3/4/\tiny /\tiny }, {-3/3/5/2/\tiny },
				  {-1/3/6/2/\tiny }, {1/3/7/2/\tiny }, 
				  {3/3/8/\tiny /\tiny }, {5/3/9/\tiny /\tiny }, 
				  {-6/5/10/2/\tiny }, {-4/5/11/\tiny /\tiny }, 
				  {-2/5/12/3/\tiny },{0/5/13/2/\tiny }, 
				  {2/5/14/\tiny /\tiny },{4/5/15/2/\tiny }, {6/5/16/2/\tiny }, 
				  {-5/7/17/2/\tiny }, {-3/7/18/\tiny /\tiny }, 
				  {-1/7/19/\tiny /\tiny }, 
				  {1/7/20/\tiny /\tiny }, {3/7/21/2/\tiny }, 
				  {5/7/22/2/\tiny }, {-2/9/23/1/\tiny }, {0/9/24/1/\tiny }, 
				  {2/9/25/1/\tiny }, {0/10/26/0/\tiny }}
	  {
	   \node[vertex](v\nr) at (\x, \y){\scriptsize \f};
	   \draw(\x + 0.25, \y) node[anchor = west]{\scriptsize \set};
	  }  
\foreach \i/\j in {{0/1}, {0/2}, {0/3}, {1/4}, {1/5}, {1/6}, {2/5}, {2/7}, {2/8}, {3/6}, {3/7}, {3/9}, 
			     {4/10}, {4/11},{5/10}, {5/12}, {5/13}, {6/11}, {6/12}, {6/15}, {7/12}, 
			     {7/14}, {7/16}, {8/13}, {8/14}, {9/15}, {9/16}, 
			     {10/17}, {10/18}, {11/17}, {11/19}, {12/17}, {12/21}, {12/22}, {13/18}, {13/21}, 
			     {14/20}, {14/21}, {15/19}, {15/22}, {16/20}, {16/22}, 
			     {17/23}, {17/24}, {18/23}, {19/24}, {20/25}, {21/23}, {21/25}, {22/24}, {22/25}, 
			     {23/26}, {24/26}, {25/26}}
 	      \draw (v\i) edge (v\j);

 	
	\end{tikzpicture}

\caption{Each node corresponds to a vector in , written next to it, with the value of  within the node. 
The picture can be read similar to Hasse diagrams of posets: 
There is a line connecting two vectors, if their -distance equals 1. If we think of the edges as being directed upwards, 
then the meet of two vectors is their biggest common predecessor, and their join the smallest common successor.}
\label{fig:TightExample}
\end{figure}


\begin{proof}
Consider the following submodular function . 
Let . Now we define 
 if  consists of either the entries  or  in any order. 
We set . For all other vectors, we set . 
It can be checked that  is indeed submodular. 
To give a better intuition, Figure \ref{fig:TightExample} illustrates this function. 
 


We analyze Algorithm \ref{MyGreedy} for this instance: 
Obviously, the optimum is the vector  of value 3, 
but Algorithm \ref{MyGreedy} terminates with a set of value : 

In the first iteration, the maximal possible gain in  is . 
So we we set  and . 
Next,  and thus  and . 

Therefore, we now have  and , both of value  
and the algorithm returns the vector  of value . 

Since the maximal possible gain in  is  for the first two indices independent of the order,
the same order could have been chosen by an algorithm that always processes the index which maximizes the gain in .
Moreover, the tie-braking rule for  
does not influence the value of the output here.
\end{proof}


While Algorithm \ref{MyGreedy} went ``too far'' when choosing the next entry, 
a variant of the algorithm that only changes the vectors by one at a time can be arbitrarily bad here. To see this, consider the example on  in Figure 
\ref{fig:BadLatticeOneGreedy}. If our algorithm was only allowed to add  to  or delete  from  and chose the better of these two options, 
the output could not be  which has an arbitrarily high value of . 
\begin{figure}[tb]

	 \tikzstyle{vertex}=[circle, draw,
                        inner sep=1pt, minimum width=12pt]
	\tikzstyle{edge} = [draw,thick]
	\centering
	\begin{tikzpicture}[scale = 0.8]
	\foreach \x/\y/\nr/\f in {{0/0/0/1}, {-1/1/1/1}, {1/1/2/2}, {-2/2/3/}, {0/2/4/1}, {2/2/5/3}, 
				  {-1/3/7/1}, {1/3/8/2}, {0/4/11/2}}
	   \node[vertex](v\nr) at (\x, \y){\scriptsize\f};
	    
	    
	   \draw (0,-0.2) node[anchor = north]{\scriptsize };
	   \draw (-1,0.8) node[anchor = north]{\scriptsize };
	   \draw (1,0.8) node[anchor = north]{\scriptsize };
	   \draw (-2, 1.8) node[anchor = north]{\scriptsize };
	   \draw (0, 1.8) node[anchor = north]{\scriptsize };
	   \draw (2, 1.8) node[anchor = north]{\scriptsize };
	   \draw (-1,2.8) node[anchor = north]{\scriptsize };
	   \draw (1,2.8) node[anchor = north]{\scriptsize };
	   \draw (0,3.8) node[anchor = north]{\scriptsize };
	 \foreach \i/\j in {{0/1}, {0/2}, {1/3}, {1/4}, {2/4}, {2/5}, {3/7}, {4/7}, {4/8}, {5/8},
			      {7/11},{8/11}}
	      \draw (v\i) edge (v\j);
	  \draw [white] (3,1) circle (1mm);    
	   
	\end{tikzpicture}
\caption{The values in the nodes give a submodular function for any .}
\label{fig:BadLatticeOneGreedy}
\end{figure}



\subsection{Difficulties in Designing Algorithms}\label{sec:BadRandom}
We already mentioned that submodularity is a much weaker property on the integer lattice than on set functions. 
This also makes it hard to generalize some algorithms without assuming additional properties like diminishing returns. 
Consider for example the one-dimensional integer lattice, on which any function is submodular. Therefore, we cannot expect 
running times that are better than pseudopolynomial for any algorithm with bounded approximation ratio, unless the function fulfills other properties as well.
This example also illustrates that some structural properties that are key to designing good algorithms on set functions do not hold any more. 
One such result is used in the local search approach presented in \cite{LocalSearch}. The authors rely on a generalization of a result about local optima, 
stating that the value of a local optimum  is at least as large as the value of all sub- and supersets. 
For the integer lattice, this statement is no longer true. 
Moreover, ``continuous Greedy algorithms'' that were used in various settings and also provided a better approximation ratio for USM,
rely on component-wise concavity along non-negative directions 
for the so-called multilinear relaxation. Since such a relaxation coincides with the function values on integer points, this property cannot be obtained 
without stronger assumptions like DR. 

As a generalization of the Double Greedy approach was possible, 
it seems reasonable to ask whether further results using similar techniques can also be generalized to SMBIL. 
For USM, \cite{DoubleGreedy} also presents a randomized ``Double Greedy'' algorithm which gives a guarantee of . 
They decide with probability proportional to the increase in  whether to add a given element to one set or delete it from the other. 
In the context of incidence vectors, this is equivalent to choosing whether entries  and  are set to  or . 
We show that a similar analysis cannot work if we adapt their idea to our algorithm.
We consider two natural strategies of generalizing the randomized algorithm above. One is the following:
For given vectors  and an index  we can consider all possibilities to increase  or decrease  such that
 remains true and choose one of these possibilities at random (again proportional to the increase in ). 
We will show this leads to arbitrarily bad solutions. 

The other alternative is more similar to our previous algorithm: We determine the best choice for  and  and then choose
between the two options with probability proportional to the -values, i.e.\ the maximal possible gain in .
While it is possible that this actually is a -approximation, 
we can so far only show that this randomized algorithm gives the same guarantee as the deterministic version.
Indeed, we will show that an analysis as in \cite{DoubleGreedy} which bounds the expected decrease of 
 in each step cannot prove a guarantee better than  by examining the example 
presented in Figure \ref{fig:TightExample}. 

So, while it might still be true that this randomized version of our algorithm actually is a -approximation 
(for example, the worst case expected value in the given example is  while the optimum is ), 
we would require an analysis that takes a more global view. 

We should note that for the Boolean lattice, both these approaches are identical and  correspond to the randomized algorithm in 
\cite{DoubleGreedy}. We now analyze both algorithms. 

\subsubsection{Randomized choice over all possible solutions}\label{subsec:RandBadAllPossible}

\begin{lemma}\label{AllRandChoicesBadLemma}
Consider the following randomized version of Algorithm 1: Assume an order of indices and 
consider index . While , randomly set either  or  where the choice is made over all 
 proportional to the increase in  if it is positive. 
If the increase in  is non-positive for all possibilities, we arbitrarily choose an option where the increase in  is , 
but  or  are changed.     
This algorithm for SMBIL can perform arbitrarily bad. 


\end{lemma}

\begin{proof}
First, we remark that in the case where the increase in  is non-positive everywhere, a choice as above exists.    
We now analyze the following instance: Given  , , 
 we define a submodular function  as follows

.
	
Our randomized algorithm starts with  and . 
We start with index 1. Until , we change entry  or  and choose a value such that  
at random proportional to the increase in . 
I.e., for the first step, we have  options where the increase in  is positive, 
all but one (setting ) will be taken with probability . 



After the first step, either  or . In the second case, if , the only options now to increase  are 
changing  or setting .  
Therefore, after two steps either  or  and depending on which of these holds, the return value will be  or . 
Thus, the probability for the algorithm to return a vector of value  is

We now show that this expression converges to one for , thus showing that 
the expected return value of the algorithm converges to .

To achieve that, it suffices to find a lower bound converging to one. After rewriting the above expression we obtain: 
 
For the last inequality, we used the fact that the partial sums of the harmonic series can be bounded by , 
the rest is simple arithmetic.  
\end{proof}
Note that variations like randomizing over combinations of values for  and  
and choosing proportional to the sum of the increases in  show a similar behavior. 

\subsubsection{Randomized choice over the best possible solutions}\label{subsec:RandBadBestOptions}

\begin{lemma}
Consider the following randomized version of Algorithm 1: Instead of deciding to change  or  depending on whether
, 
randomly change  or  proportional to  and .
Then adapt the other vector as before. 
For this algorithm, there is no constant  such that 
.
\end{lemma}

Unlike in \cite{DoubleGreedy}, where the above statement is true with ,
an analysis that bounds the expected decrease of  by the expected increase in  and  in each step cannot
yield an approximation factor better than . 

\begin{proof}
As before,  is defined as  for a fixed optimal solution .  
Consider the example presented in the previous section in Figure \ref{fig:TightExample}. 
No matter how we choose the first index ,  or  both with equal probability. 
Thus,  consists of the entries  or  and thus has value 2 in both scenarios which implies 

 and    
.

Note that the statement is true no matter how the order of indices is chosen.
\end{proof}







\section{DR-submodularity on Distributive Lattices: Algorithms}\label{sec:SM-DR-DL-algos}
In the rest of this paper, we consider more general domains, namely distributive lattices, but only DR-submodular functions. 
In this section, we examine several problems, that are well-understood for set functions and DR-submodular functions on the integer lattice 
and propose algorithms.
First, we investigate maximizing monotone functions subject to a poset matroid constraint. 
Moreover, we consider the special case of uniform poset matroids, 
i.e.\ a cardinality constraint. Then, we generalize the double greedy approach to the unconstrained case.    

\subsection{Monotone maximization subject to a poset matroid constraint}

Given a poset ,  let  be DR-submodular and monotone.
For a poset matroid , we consider the problem
 
We show a -approximation guarantee for a general poset matroid and 
a guarantee of  for a cardinality constraint, which is the special case of a uniform poset matroid. 
These guarantees were shown by Fisher, Nemhauser and Wolsey for matroid constraints on the Boolean lattice (\cite{FisherGreedyI},\cite{FisherGreedyII}) and
 the cardinality result is also presented in \cite{KrauseSurvey}.  
For cardinality constraints, this is tight in the sense that any algorithm evaluating  
a polynomial number of times cannot yield a better approximation guarantee than  \cite{FisherGreedyI}. 
There is also a -approximation for a matroid constraint (\cite{MonotoneMatroid1}) on the Boolean lattice 
and a -approximation for a polymatroid constraint on the integer lattice.
However, these are based on continuous greedy algorithms and rounding a vector in a downward closed polytope, 
and it is not clear how to generalize these techniques for distributive lattices as we have no appropriate polyhedral description of poset matroids.  

We start by describing and analyzing the algorithm for poset matroid constraints. 
Then, we give a better analysis for a cardinality constraint. 
The algorithm is a simple Greedy procedure: Starting from the empty set, 
we consider a minimal element among those not yet processed in the partial order
and choose the one that yields the biggest increase in , compared to the current set. 
If including this element would result in an independent set w.r.t.\ the poset matroid, 
we add it, otherwise, we discard the element. This procedure yields a linear extension  of the partial order. 
A formal description can be found in Algorithm \ref{Algo:SMMatroid}. 

\begin{algorithm2e}
\DontPrintSemicolon
\textbf{Input:} A poset matroid ,  DR-submodular and monotone\;
\textbf{Output:} A set \;


 Set \; 
 \For {t = 1 \KwTo n}{
    Select  \;
    \If{}
    {
      \;
    }
    \Return S\;
 }
 \caption{Greedy with poset matroid constraint.}\label{Algo:SMMatroid}
\end{algorithm2e}

\begin{theorem}\label{thm:monotoneDRDLApprox}
Algorithm \ref{Algo:SMMatroid} is a -approximation for monotone DR-submodular lattice maximization subject to a matroid constraint
and a -approximation for a cardinality constraint.    
\end{theorem}

For the proof, we will use a known lemma, which we restate here: 

\begin{lemma}[\cite{FisherGreedyII}]\label{lemma:FisherArithmetic}
 Let nonnegative numbers  for  with  for all  and 
  for  be given. 
 Then 
\end{lemma}

\begin{proof}[Proof of Theorem \ref{thm:monotoneDRDLApprox}]
Let  be the size of the set  returned by our Greedy algorithm.  
For ease of notation, we introduce  with  and . 
That means,  marks the progression of  throughout the algorithm. Let  denote a fixed optimal solution. Now we go through
the total ordering  and define  as the number of elements in  that we considered before reaching , 
starting from the last element added to . 

More formally, let  be the minimal index such that , for convenience we define .  
In other words,  are the elements considered by the Greedy algorithm at the point where  is set to  
and in particular . 
Then . 
We illustrate this definition in an example in Figure \ref{fig:DRMatroidExample}. 

\begin{figure}[tb]
\centering
\begin{tikzpicture}[scale = 0.7]
\tikzstyle{vertex}=[circle, draw,inner sep=1pt, minimum width=15pt]
\tikzstyle{edge} = [draw]
\node[vertex] (x1) at (0,0) {\scriptsize };
\node[vertex] (x4) at (3,0) {\scriptsize };
\node[vertex] (x6) at (5,0) {\scriptsize };
\node[vertex] (x2) at (-1,2) {\scriptsize };
\node[vertex] (x3) at (1,2) {\scriptsize };
\node[vertex] (x5) at (3,2) {\scriptsize };
\node[vertex] (x7) at (5,2) {\scriptsize };

\draw [->](x1) edge (x2);
\draw [->](x1) edge (x3);
\draw [->](x4) edge (x5);
\draw [->](x6) edge (x7);
  
 \end{tikzpicture}
\caption{Partial order with induced total order given by indices. Consider  and , then
 and .}\label{fig:DRMatroidExample}
 
\end{figure}


We start by showing that  fulfills the conditions of the above Lemma, that is  for . 
By construction of our Greedy algorithm,  is a maximal independent set in . 
Therefore, by poset matroid properties, it also has maximum size and any independent subset of  
contains at most  elements.
As the numbers  are just one way of counting the elements of  by grouping them into intervals on the given linear extension 
(w.l.o.g.\ ), we obtain
 
The last inequality holds as OPT is an independent set and thus  
 is independent as well. 

Next, we show that the differences  decrease as  increases. 
We distinguish two cases. 
If  and  are not comparable, we could have added  to , 
and thus,  must have been the better option at that moment. 
Note that  must be independent by poset matroid property , 
as  is independent. 
Therefore,  

 where the second inequality follows from DR-submodularity.  
 
Otherwise,   and  follows directly from DR-submodularity. 
Moreover, by monotonicity of ,  for all . 
Therefore, we can apply Lemma \ref{lemma:FisherArithmetic} for  and . 

Now, we consider only the elements of  that are not chosen by our algorithm, 
ordered as in the linear extension given by . 
Let , ordered the same way as the . 
We will partition this set into the subsets contributing to the different 
and to simplify notation we define these subsets as
. 

Now, we first use monotonicity of  and then rewrite.


Inequality  is a consequence of DR-submodularity and the Greedy property: 
Consider some . 
If , the inequality we use follows directly from DR-submodularity
as . 
Otherwise,  and  are incomparable and there is a minimal element  with  . By DR-submodularity, . 
But adding  maximized the increase in  and thus, this is at most   
(as  is not considered earlier in the algorithm). 

Inequality  is a direct consequence of Lemma \ref{lemma:FisherArithmetic} with  and  as defined above, 
and we already showed that the requirements for applying the lemma are fulfilled. 

This proves that our Algorithm \ref{Algo:SMMatroid} is a -approximation. 

If the poset matroid is the uniform matroid, the constraint is a cardinality constraint, i.e.\ . 
For that case,  we can use a similar argument to obtain a better bound: 
Consider some  and define  as before with  instead of , i.e.\ . 
Clearly, . 

Analogous to the previous inequalities, we can obtain 

As shown in \cite{KrauseSurvey}, this inequality directly implies that our Greedy algorithm 
is a -approximation. 
For the sake of completeness, we include the proof here as well. 

Define  and then rewrite the above inequality:
. We rearrange to obtain
. 
 It follows inductively that  for all . Moreover, 
 non-negativity of  implies  . Now we use the fact that
  for all , resulting in

Resubstituting   then yields  which implies the approximation guarantee. 
\end{proof}


 The same technique as in the proof of theorem \ref{thm:monotoneDRDLApprox} can also be used to generalize this result to a collection of  poset matroid
 constraints to obtain an approximation ratio of . 


\subsection{Unconstrained Maximization}
In this subsection, we again consider the randomized Double Greedy algorithm by Buchbinder et al.\ \cite{DoubleGreedy}. 
We will show that DR-submodularity seems to be a crucial property to obtain good approximation guarantees using that algorithm. 
In contrast to SMBIL, all proofs from \cite{DoubleGreedy} can easily be generalized to obtain a -approximation
for DR-SMDL. 
Note that this only results in a pseudopolynomial algorithm for the integer lattice. 
However, Soma and Yoshida showed how to adapt the Double Greedy to obtain a polynomial randomized -approximation
for DR-SMBIL \cite{SomaDR-SMBIL}. 
\begin{theorem}\label{thm:AlgoDR-SMDL}
 There is a randomized -approximation for DR-SMDL. 
\end{theorem}

We start by describing the modified algorithm, for a formal description see Algorithm \ref{algo:DLDoubleGreedy}. 
To avoid complicating it, read  in the algorithm.  
In contrast to the algorithm for SMBIL we presented in Section \ref{sec:SMBIL-Algo}, we will 
only add or delete one element at a time (which corresponds to adding or subtracting 1 for SMBIL). 
Given a distributive lattice  on a poset , we fix a linear extension of  to obtain a total ordering
 of . Starting from  and , 
we always consider a pair  consisting of the smallest available element in the ordering and a maximal
element  such that the pair is comparable in . Note that  is permitted.   
Then, we randomly choose to add  to  or delete  from  with probabilities proportional to the increase in  and 
remove the chosen element. This continues until . 
DR-submodularity is essential here.  Without it, the example presented in Figure \ref{fig:BadLatticeOneGreedy} 
with the linear extension  shows that Algorithm \ref{algo:DLDoubleGreedy} 
would perform arbitrarily badly even for the integer lattice. 

\begin{algorithm2e}
\DontPrintSemicolon
\textbf{Input:} A nonnegative DR-submodular function  on a distributive lattice \;
\textbf{Output:} A set \;

 , ,  a linear extension of , \; , \; \While {}{
 	\;
	 \;
	With probability  set ,  
	and find new . \;
	With complementary probability set , find new  for k.\;	
}
    \Return A\;
 \caption{Greedy with Matroid Constraint}\label{algo:DLDoubleGreedy}
\end{algorithm2e}

Adapting the arguments of \cite{DoubleGreedy} to our setting is straightforward. 
We start by showing that for at least one of the two options,  does not decrease.  
\begin{lemma}\label{lemma:DLpositivechange}
In every iteration of Algorithm \ref{algo:DLDoubleGreedy}, . 
\end{lemma}
Since  and , the above expression is just a reformulation of DR-submodularity and thus the Lemma follows. 
We fix an optimal solution  and let  and  be random variables denoting the sets  and  of elements after the i-th iteration. 
We show that for DR-submodular functions, we can get a good bound on the expected decrease in , where the random variable 
 is defined as . 

\begin{lemma}
For every iteration , we have . 
\end{lemma}
Then, as presented in Section \ref{sec:SMBIL-Algo}, by summing over all iterations and collapsing the resulting telescopic sum, 
we obtain the proof of Theorem \ref{thm:AlgoDR-SMDL}. 
It remains to show the lemma. 
\begin{proof}
The proof works completely analogous to \cite{DoubleGreedy}. We will give the details here and omit them for similar results. 
It suffices to prove the inequality conditioned on  for a set  consisting of elements
 and a corresponding series of pairs  when the probability that this
series of pairs leads to  is non-zero. We now fix such an event.
Then, , ,  and  become constants. 
By Lemma \ref{lemma:DLpositivechange},  or  must be nonnegative. 
Therefore, it suffices to consider the following cases. 

Suppose  and thus . Then,  and  and 
.
If , we have .   
If , then 


by DR-submodularity. 
The case where  is analogous.

Therefore, suppose  and . 
Then, 
						  

On the other hand, 


We still need to explain the final inequality. 
If , the second term on the left hand side equals zero. For the first term , we use DR-submodularity to obtain
. 

Otherwise,  as OPT is an ideal and thus, the first term is zero. Moreover, 
DR-submodularity implies   
.

Now the inequalities \eqref{eq:DLexpectedOPT} and \eqref{eq:DLexpectedSets} together with the fact that 
 yield the desired result. 
\end{proof}

The deterministic version from \cite{DoubleGreedy} which yields a -approximation can also be generalized to this case. 
In contrast to the non-DR-setting in Section \ref{sec:SMBIL-Algo}, the proofs from \cite{DoubleGreedy} by 
Buchbinder et al.\ can be directly generalized together with the ideas used in this section. Therefore, we will not present them here. 

As shown in \cite{DerandomizedDoubleGreedy}, there is also a deterministic -approximation based on the randomized algorithm. 
The rough idea is the following. They maintain a set of possible ``states'' of sets with corresponding probability. For example, 
in the beginning, there is exactly one possible state:  and .
In iteration , they consider element  and for each current state , they assign probabilities to adding  to  or deleting
it from . This way, they build the set of states for iteration . For example, after one iteration, 
the set of states would then be  with probability  and  with probability . 
The groundbreaking idea in this paper is how to assign these probabilities. There are two aspects to this. 
First, the set of states should stay small, in particular, doubling its size in every iteration is not an option. Second, the expected decrease in 
the objects  corresponding to states  should be small. 
Both these properties are achieved by determining the probabilities using a linear program. The first property follows from the fact 
that an extreme point of their linear program must have small support. The second property is enforced by inequalities in the LP. 

Their strategy can also be generalized to the DR-submodular setting on distributive lattices. But, there is one additional technical issue. 
In \cite{DerandomizedDoubleGreedy}, the element that is to be added or deleted is the same for all states in an iteration . 
For our setting, each state  also comes with its own pair . 
However, the objects  used for the analysis also depend on the state and for a fixed state we still obtain the necessary properties 
like .
Therefore, we can use DR-submodularity in the same way as \cite{DerandomizedDoubleGreedy} 
to obtain a deterministic -approximation for SMDL. Since the technical details are very similar to the ones already provided we will
not present them. 

It seems natural to ask whether the approach presented in Section \ref{sec:SMBIL-Algo} for SMBIL, i.e.\ for non-DR functions, 
could also be generalized to distributive lattices. 
This would mean choosing a best join-irreducible element of the lattice to add to  and a best complement of a meet-irreducible element to delete from . 
Which elements would be eligible to be chosen would be subject to discussion. 
But, using such a strategy, it is possible to end up in situations where there is no possibility of changing 
 or  as described above without decreasing their values w.r.t.\ . 

\section{DR-submodularity on Distributive Lattices: Hardness}\label{sec:SM-DR-DL-hardness}
As the previous algorithms maximizing DR-submodular functions on the distributive lattice were direct generalizations of algorithms for the Boolean lattice, 
one could assume that there is no significant structural difference. However, in this section we
consider the problem of maximizing a monotone DR-submodular function  on the distributive 
lattice subject to a knapsack constraint with binary knapsack weights. Formally, 


This problem is essentially as hard to approximate as the Densest -subhypergraph problem and thus, in particular there is no constant factor approximation
under some complexity assumptions. 
In contrast, for monotone functions on the Boolean lattice, there is a -approximation for one (arbitrary) knapsack constraint \cite{SviridenkoKnapsack} 
and a  -approximation(\cite{KulikMonotoneKnapsack}) for  Knapsack constraints. 
Moreover, \cite{SomaYoshi16} generalized these results to the integer lattice and obtained a guarantee of  for monotone 
DR-submodular functions and positive knapsack coefficients and a 
pseudopolynomial -approximation for monotone submodular functions 
\cite{Inaba:KnapsackIL}. Note that due to  monotonicity of , the result directly generalizes to nonnegative knapsack values. 


We start by introducing the densest -subhypergraph problem (DSH). Given a hypergraph  and , 
the goal is to choose  vertices such that the number of induced hyperedges is maximized. 
One special case is the densest -subgraph problem for which, although it is believed to be hard, 
it is only known that no PTAS exists if NP has no subexponential algorithms \cite{Khot04DkS}. 
However, Hajiaghayi at al.\ showed in \cite{HajiSubhypergraph} that DSH is hard to approximate within a factor of  
for some  under the assumption that 
, i.e.\  cannot be solved in time . 

\begin{theorem}
 \label{thm:SubmaxKnapsackHardness}
 If there is an -approximation for maximizing a monotone DR-submodular function on a distributive lattice subject to a --Knapsack constraint, 
 then there is a -approximation for DSH, where  and  denote the respective input sizes and 
  is positive and non-decreasing. 
\end{theorem}

\begin{proof}
Let ,  be the input of DSH.  
Our goal is the construction of a submodular maximization instance. 
We construct a poset  by introducing an element  for every vertex  and 
 elements  for every hyperedge . 
Slightly abusing notation, we are also going to denote by  the subset of elements in  corresponding to vertices in .  
Then,  if  in . 
Moreover,  which clearly is DR-submodular and monotone and . 
The knapsack constraint is given by asking that , i.e.\  and  for  and  otherwise. 


Any ideal  corresponds to the set of vertices  and some hyperedges induced by this set.
On the other hand, any induced subhypergraph in  can be represented (not uniquely) by an ideal. 
In particular, there is a one-to-one correspondence between induced subhypergraphs and ideals  where  is maximal. 

Let  be the result of an -approximation for our problem. Since the knapsack constraint only 
affects the elements in , we assume w.l.o.g.\ that  contains either all copies  for an edge  or none of them and that . 
Therefore,  corresponds to a feasible solution for DSH inducing  edges.
We can assume that . If not, we construct a solution fulfilling that easily.  

Let  denote the optimal value for DSH. Then, the optimal value for our submodular maximization instance is 
. 
Since  was obtained by an -approximation,  
. 
Combining the above, we obtain 

Therefore,
 and thus, we have a  approximation for DSH.  
\end{proof}

We could refine the constant of 2 in the previous analysis: 
For any constant  we can find a solution of DSH 
of value at least  by enumeration. (Unless the optimum is less than , then we solve DS optimally.)
If our algorithm performs worse, we take the solution of size  instead and therefore can assume 
. This leads to a -approximation in the above analysis.  


Using the above theorem we obtain: 

\begin{corollary}
The submodular maximization problem defined in \eqref{eq:KnapsackDL} is hard to approximate within a factor  
of  for every constant  under the assumption that 
 for every constant . 
\end{corollary}


\begin{proof}
We need to examine the hardness results for DHS from \cite{HajiSubhypergraph} a little closer. The authors reduce from a problem presented in \cite{FeigeKogan04}, the so-called 
Maximum Balanced Complete Bipartite Subgraph Problem. However, they omit the fact that the input graph for that problem is not just bipartite but balanced as well. 
This ensures that the complexity result for DSH is true even if the number of hyperedges is equal to the number of nodes. 
Moreover, \cite{FeigeKogan04} state their result for all constants , not just for some constant as cited and used in \cite{HajiSubhypergraph}, 
which is why we also use this more general formulation here.  

Therefore, we can now apply Theorem \ref{thm:SubmaxKnapsackHardness} for a hypergraph with   and use the same reduction as above. 
Therefore,  and thus
. Therefore, by setting , we obtain our result. 
\end{proof}





\bibliographystyle{plain}
\bibliography{SubMaxFullLiterature.bib}

\end{document}