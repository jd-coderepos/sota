\documentclass{article}





\usepackage[preprint, nonatbib]{neurips_2020}



\usepackage[numbers]{natbib}
\newcommand\Mycite[1]{\citeauthor{#1}~\cite{#1}}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage[colorlinks,citecolor=blue]{hyperref}      \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{lipsum,booktabs}
\usepackage{tabularx}



\newcommand{\comment}[1]{}

\def\methodname{Goal-Oriented Semantic Exploration}
\def\methodabbr{SemExp}

\title{Object Goal Navigation using \\
\methodname{}}



\makeatletter
\renewcommand*{\@fnsymbol}[1]{\ensuremath{\ifcase#1\or \dagger\or *\or \ddagger\or
    \mathsection\or \mathparagraph\or \|\or **\or \dagger\dagger
    \or \ddagger\ddagger \else\@ctrerr\fi}}
\makeatother

\author{
Devendra Singh Chaplot\textsuperscript{\textnormal{1}}\thanks{Correspondence: \texttt{chaplot@cs.cmu.edu}} ,
Dhiraj Gandhi\textsuperscript{\textnormal{2}},
Abhinav Gupta\textsuperscript{\textnormal{1,2}}\footnotemark[2] ,
Ruslan Salakhutdinov\textsuperscript{\textnormal{1}}\thanks{Equal Contribution}
\\
\textsuperscript{1}Carnegie Mellon University, \textsuperscript{2}Facebook AI Research \ DTS = max(||x_T - G||_2 - d_s, 0) 
where  is the L2 distance of the agent from the goal location at the end of the episode,  is the success threshold.

\vspace{-6pt}
\subsection{Baselines.}
\vspace{-8pt}
We use two end-to-end Reinforcement Learning (RL) methods as baselines:\\\vspace{-14pt}

\textbf{RGBD + RL:} A vanilla recurrent RL Policy initialized with ResNet18~\citep{he2016deep} backbone followed by a GRU adapted from ~\Mycite{savva2019habitat}. Agent pose and goal object category are passed through an embedding layer and append to the recurrent layer input.\\\vspace{-14pt}

\textbf{RGBD + Semantics + RL~\cite{mousavian2019visual}:} This baseline is adapted from~\Mycite{mousavian2019visual} who pass semantic segmentation and object detection predictions along with RGBD input to a recurrent RL policy. We use a pretrained Mask RCNN identical to the one used in the proposed model for semantic segmentation and object detection in this baseline. RGBD observations are encoded with a ResNet18 backbone visual encoder, and agent pose and goal object are encoded usinng embedding layers as described above. \\\vspace{-14pt}

Both the RL based baselines are trained with Proximal Policy Optimization~\cite{schulman2017proximal} using a dense reward of distance reduced to the nearest goal object. We design two more baselines based on goal-agnostic exploration methods combined with heuristic-based local goal-driven policy.\\\vspace{-14pt}

\textbf{Classical Mapping + FBE~\cite{yamauchi1997frontier}:} This baseline use classical robotics pipeline for mapping followed by classical frontier-based exploration (FBE)~\cite{yamauchi1997frontier} algorithm. We use a heuristic-based local policy using a pretrained Mask-RCNN. Whenever the Mask RCNN detects the goal object category, the local policy tries to go towards the object using an analytical planner. \\\vspace{-15pt}


\textbf{Active Neural SLAM~\cite{chaplot2020learning}:} In this baseline, we use an exploration policy trained to maximize coverage from~\cite{chaplot2020learning}, followed by the heuristic-based local policy as described above.

\setlength{\tabcolsep}{5.6pt}
\begin{table}[]
\caption{\small \textbf{Results.} Performance of \methodabbr{} as compared to the baselines on the Gibson and MP3D datasets.}
\vspace{-5pt}
\label{tab:results}
\centering
\begin{tabular}{@{}llccccccc@{}}
\toprule
                      &           & \multicolumn{3}{c}{Gibson}                       &           & \multicolumn{3}{c}{MP3D}                         \\
Method                &           & SPL            & Success           & DTS (m)           &           & SPL            & Success           & DTS (m)           \\ \midrule
Random                &           & 0.004          & 0.004          & 3.893          &           & 0.005          & 0.005          & 8.048          \\
RGBD + RL~\cite{savva2019habitat}             &           & 0.027          & 0.082          & 3.310          &           & 0.017          & 0.037          & 7.654          \\
RGBD + Semantics + RL~\cite{mousavian2019visual} &           & 0.049          & 0.159          & 3.203          &           & 0.015          & 0.031          & 7.612          \\
Classical Map + FBE~\cite{yamauchi1997frontier}   &           & 0.124          & 0.403          & 2.432          &           & 0.117          & 0.311          & 7.102          \\
Active Neural SLAM~\cite{chaplot2020learning}   &           & 0.145          & 0.446          & 2.275          &           & 0.119          & 0.321          & 7.056          \\
\methodabbr{}       &  & \textbf{0.199} & \textbf{0.544} & \textbf{1.723} &  & \textbf{0.144} & \textbf{0.360} & \textbf{6.733} \\ \bottomrule
\end{tabular}
\vspace{-10pt}
\end{table}

\vspace{-3pt}
\section{Results}
\vspace{-7pt}
We train all the baselines and the proposed model for 10 million frames and evaluate them on the Gibson and MP3D scenes in our test set separately. We run 200 evaluations episode per scene, leading to a total of 1000 episodes in Gibson (5 scenes) and 2000 episodes in MP3D (10 scenes, 1 scene did not contain any object of the 6 possible categories). Figure~\ref{fig:habitat_episode} visualizes an exmaple trajectory using the proposed \methodabbr{} showing the agent observations and predicted semantic map\footnote{See demo videos at \url{https://devendrachaplot.github.io/projects/semantic-exploration}}. The quantitative results are shown in Table~\ref{tab:results}. \methodabbr{} outperforms all the baselines by a considerable margin consistently across both the datasets (achieving a success rate 54.4\%/36.0\% on Gibson/MP3D vs 44.6\%/32.1\% for the Active Neural SLAM baseline) . The absolute numbers are higher on the Gibson set, as the scenes are comparatively smaller. The Distance to Success (DTS) threshold for Random in Table~\ref{tab:results} indicates the difficulty of the dataset. Interestingly, the baseline combining classical exploration with pretrained object detectors outperforms the end-to-end RL baselines. We observed that the training performance of the RL-based baselines was much higher indicating that they memorize the object locations and appearance in the training scenes and generalize poorly. The increase in performance of \methodabbr{} over the Active Neural SLAM baseline shows the importance of incorporating semantics and the goal object in exploration.

\setlength{\tabcolsep}{22pt}
\begin{table}[]
\caption{\small \textbf{Ablations and Error Analysis.} Table showing comparison of the proposed model, \methodabbr{}, with 2 ablations and with Ground Truth semantic segmentation on the Gibson dataset.}
\vspace{-5pt}
\label{tab:ablations}
\centering
\begin{tabular}{@{}lllll@{}}
\toprule
Method                                  &  & SPL   & Success & DTS (m) \\ \midrule
\methodabbr{} w.o. Semantic Map         &  & 0.165 & 0.488   & 2.084   \\
\methodabbr{} w.o. Goal Policy          &  & 0.148 & 0.450   & 2.315   \\ 
\methodabbr{}                           &  & 0.199 & 0.544   & 1.723   \\
\methodabbr{} w. GT SemSeg              &  & 0.457 & 0.731   & 1.089   \\ \bottomrule
\end{tabular}
\end{table}

\begin{figure*}[t]
\centering
\includegraphics[width=0.97\linewidth,height=\textheight,keepaspectratio]{images/ablation.pdf}
\vspace{-6pt}
\caption{Figure showing an example comparing the proposed model with (\textbf{top}) and without (\textbf{bottom}) Goal-Oriented Semantic Policy. Starting at the same location with the same goal object of `toilet', the proposed model with Goal-Oriented Policy can find the target object much faster than without Goal-Oriented Exploration.}
\label{fig:ablation}
\vspace{-7pt}
\end{figure*}


\subsection{Ablations and Error Analysis}
\vspace{-7pt}
To understand the importance of both the modules in \methodabbr{}, we consider two ablations:\\
\textbf{\methodabbr{} w.o. Semantic Map.} We replace the Semantic Map with the Obstacle-only Map. As opposed to the Active Neural SLAM baseline, the Goal-oriented Policy is still trained with distance reduced to the nearest object as the reward.\\
\textbf{\methodabbr{} w.o. Goal Policy.} We replace the Goal-driven policy with a goal-agnostic policy trained to maximize exploration coverage as in~\cite{chaplot2020learning}, but still trained with the semantic map as input.\\
The results in the top part of Table~\ref{tab:ablations} show the performance of these ablations. The performance of \methodabbr{} without the Goal-oriented policy is comparable to the Active Neural SLAM baseline, indicating that the Goal-oriented policy learns semantic priors for better exploration leading to more efficient navigation. Figure~\ref{fig:ablation} shows an qualitative example indicating the importance of the Goal-oriented Policy. The performance of \methodabbr{} without the Semantic Map also drops, but it is higher than the ablation without Goal Policy. This indicates that it is possible to learn some semantic priors with just the obstacle map without semantic channels.

The performance of the proposed model is still far from perfect. We would like to understand the error modes for future improvements. We observed two main sources of errors, semantic segmentation inaccuracies and inability to find the goal object. In order to quantify the effect of both the error modes, we evaluate the proposed model with ground truth semantic segmentation (see \textbf{\methodabbr{} w. GT SemSeg} in Table~\ref{tab:ablations}) using the `Semantic' sensor in Habitat simulator. This leads to a success rate of 73.1\% vs 54.4\%, which means around 19\% performance can be improved with better semantic segmentation. The rest of the ~27\% episodes are mostly cases where the goal object is not found, which can be improved with better semantic exploration.


\subsection{Result on Habitat Challenge}
\label{sec:challenge}
\vspace{-7pt}
\methodabbr{} also won the CVPR 2020 Habitat ObjectNav Challenge. The challenge task setup is identical to ours except the goal object categories. The challenge uses 21 object categories for the goal object. As these object categories are not overlapping with the COCO categories, we use DeepLabv3~\cite{chen2017rethinking} model for semantic segmentation. We predict the semantic segmentation and the semantic map with all 40 categories in the MP3D dataset including the 21 goal categories. We fine-tune the DeepLabv3 segmentation model by retraining the final layer to predict semantic segmentation for all 40 categories. This segmentation model is also trained with the map-based loss in addition to the first-person segmentation loss. The performance of the top 5 entries to the challenge are shown in Table~\ref{tab:challenge}. The proposed approach outperforms all the other entries with a success rate of 25.3\% as compared to 18.8\% for the second place entry. 


\iffalse
\begin{wrapfigure}{r}{0.45\textwidth}
\centering
\vspace{-15pt}
\includegraphics[width=0.45\textwidth]{images/habitat_leaderboard.pdf}
\vspace{-15pt}
\caption{\small \textbf{Results on CVPR 2020 Habitat Object Goal Navigation Challenge.} Our submission based on the \methodname{} model code-named `Arnold' is at top of the leaderboard at the time of submission.}
\label{fig:challenge}
\end{wrapfigure}
\fi

\iffalse
\setlength{\tabcolsep}{6.5pt}
\begin{table}[]
\caption{\small \textbf{Results on CVPR 2020 Habitat Object Goal Navigation Challenge.} Table showing the performance of top 5 entries on the challenge leaderboard. Our submission based on the \methodabbr{} model is at top of the leaderboard at the time of submission.}
\vspace{-5pt}
\label{tab:challenge}
\centering
\begin{tabular}{@{}llccccccc@{}}
\toprule
                                 &  & \multicolumn{3}{c}{Test-standard} &  & \multicolumn{3}{c}{Minival}    \\
Method                           &  & SPL    & Success  & Dist To Goal  &  & SPL   & Success & Dist To Goal \\ \midrule
SemExp                           &  & \textbf{0.071}  & \textbf{0.179}    & \textbf{8.818}         &  & \textbf{0.246} & \textbf{0.467}   & \textbf{3.334}        \\
Active Exploration               &  & 0.041  & 0.089    & 9.461         &  & 0.108 & 0.167   & 5.079        \\
DD-PPO~\cite{wijmans2019decentralized}                           &  & 0.021  & 0.062    & 9.316         &  & -     & -       & -            \\
Blue Ox                          &  & 0.017  & 0.060    & 8.903         &  & 0.083 & 0.133   & 4.254        \\
SRCB-robot-sudoer                &  & 0.002  & 0.004    & 10.276        &  & 0.124 & 0.233   & 4.848        \\
PPO RGBD~\cite{savva2019habitat}                         &  & -      & -        & -             &  & 0.000     & 0.000       & 6.055        \\
Random                           &  & 0.000  & 0.000    & 10.330        &  & 0.000     & 0.000       & 6.379        \\ \bottomrule
\end{tabular}
\end{table}
\fi

\setlength{\tabcolsep}{18pt}
\begin{table}[]
\caption{\small \textbf{Results on CVPR 2020 Habitat ObjectNav Challenge.} Table showing the performance of top 5 entries on the Test-Challenge dataset. Our submission based on the \methodabbr{} model won the challenge.}
\vspace{-5pt}
\label{tab:challenge}
\centering
\begin{tabular}{@{}llccc@{}}
\toprule
Team Name                        &  & SPL   & Success & Dist To Goal (m) \\ \midrule
	\textbf{SemExp}                  &  & \textbf{0.102} & \textbf{0.253}   & \textbf{6.328}            \\
SRCB-robot-sudoer                &  & 0.099 & 0.188   & 6.908            \\
Active Exploration (Pre-explore) &  & 0.046 & 0.126   & 7.336            \\
Black Sheep                      &  & 0.028 & 0.101   & 7.033            \\
Blue Ox                          &  & 0.021 & 0.069   & 7.233            \\ \bottomrule
\end{tabular}
\vspace{-7pt}
\end{table}

\begin{figure*}[t!]
\centering
\includegraphics[width=0.99\linewidth,height=\textheight,keepaspectratio]{images/real_world.pdf}
\vspace{-7pt}
\caption{\small \textbf{Real-world Transfer.} Figure showing an example trajectory of the \methodabbr{} model transferred to the real-world for the object goal `potted plant'. Sample images seen by the robot are shown on the top and the predicted semantic map is shown below. The long-term goal selected by the Goal-driven Policy is shown in blue.}
\label{fig:real_world_living}
\vspace{-7pt}
\end{figure*}

\vspace{-4pt}
\subsection{Real World Transfer}
\vspace{-7pt}
We used the Locobot hardware platform and PyRobot API~\citep{pyrobot2019} to deploy the trained policy in the real world. 
In Figure ~\ref{fig:real_world_living} we show an episode of the robot when it was provided `potted plant' as the object goal. The long-term goals sampled by the Goal-driven policy (shown by blue circles on the map) are often towards spaces where there are high chances of finding a potted plant. This indicates that it is learning to exploit the structure in the semantic map. Out of 20 trials in the real-world, our method succeeded in 13 episodes leading to a success rate of 65\%. 
End-to-end learning-based policies failed consistently in the real-world due to the visual domain gap between simulation environments and the real-world. Our model performs well in the real-world as (1) it is able to leverage Mask RCNN which is trained on real-world data and (2) the other learned modules (map denoising and the goal policy) work on top-down maps which are domain-agnostic. Our trials in the real-world also indicate that perfect pose and depth are not critical to the success of our model as it can be successfully transferred to the real-world where pose and depth are noisy. 

\section{Conclusion}
\vspace{-7pt}
In this paper, we presented a semantically-aware exploration model to tackle the object goal navigation task in large realistic environments. The proposed model makes two major improvements over prior methods, incorporating semantics in explicit episodic memory and learning goal-oriented semantic exploration policies. Our method achieves state-of-the-art performance on the object goal navigation task and won the CVPR2020 Habitat ObjectNav challenge. Ablation studies show that the proposed model learns semantic priors which lead to more efficient goal-driven navigation. Domain-agnostic module design led to successful transfer of our model to the real-world. We also analyze the error modes for our model and quantify the scope for improvement along two important dimensions (semantic mapping and goal-oriented exploration) in the future work. The proposed model can also be extended to tackle a sequence of object goals by utilizing the episodic map for more efficient navigation for subsequent goals.


\section*{Acknowledgements}
\vspace{-7pt}
This work was supported in part by the US Army W911NF1920104, IARPA D17PC00340, ONR Grant N000141812861, DARPA MCS and ONR Young Investigator. We would also like to acknowledge NVIDIA’s GPU support.

\textbf{Licenses for referenced datasets:}\\
Gibson: {\footnotesize \url{http://svl.stanford.edu/gibson2/assets/GDS_agreement.pdf}}\\
Matterport3D: {\footnotesize \url{http://kaldir.vc.in.tum.de/matterport/MP_TOS.pdf} \\ 



\bibliography{references}
\bibliographystyle{plainnat}

\end{document}
