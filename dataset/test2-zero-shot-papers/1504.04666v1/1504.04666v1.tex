\documentclass[11pt,letterpaper]{article}
\usepackage{naaclhlt2015}

\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{url}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mathrsfs}
\usepackage{tablefootnote}
\usepackage{subcaption}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\setlength\titlebox{6.5cm}    


\title{Unsupervised Dependency Parsing: Let's Use Supervised Parsers}

\author{Phong Le \and Willem Zuidema\\
Institute for Logic, Language, and Computation \\
University of Amsterdam, the Netherlands\\
{\tt \{p.le,zuidema\}@uva.nl } \\ }


\begin{document}

\maketitle

\begin{abstract}

  We present a self-training approach to unsupervised dependency parsing that
  reuses existing supervised and unsupervised parsing algorithms. Our
  approach, called `iterated reranking' (IR), starts
  with dependency trees generated by an unsupervised parser, and
  iteratively improves these trees using the richer probability models
  used in supervised parsing that are in turn trained on these trees. 
  Our system achieves 1.8\%
  accuracy higher than the state-of-the-part parser of Spitkovsky et
  al. (2013) on the WSJ corpus.

\end{abstract}


\section{Introduction}

Unsupervised dependency parsing and its supervised counterpart have
many characteristics in common: they take as input raw sentences,
produce dependency structures as output, and often use the same
evaluation metric (DDA, or UAS, the percentage of tokens for which the system
predicts the correct head). Unsurprisingly, there has been much more
research on supervised parsing -- producing a wealth of models,
datasets and training techniques -- than on unsupervised parsing,
which is more difficult, much less accurate and generally uses very
simple probability models.  Surprisingly, however, there have been no
reported attempts to reuse supervised approaches to tackle the
unsupervised parsing problem (an idea briefly mentioned in
\newcite{spitkovsky2010viterbiem}).

There are, nevertheless, two aspects of supervised parsers that we would 
like to exploit in an unsupervised setting. First, we can increase the model
expressiveness in order to capture more linguistic regularities. 
Many recent supervised parsers use third-order (or higher order) features
\cite{koo2010efficient,martins2013turning,le2014the} to 
reach state-of-the-art (SOTA) performance. 
In contrast, existing models for unsupervised parsing 
limit themselves to using simple features (e.g., 
conditioning on heads and valency variables) 
in order to reduce the computational cost, to identify 
consistent patterns in data \cite[page 23]{naseem2014linguistically}, 
and to avoid overfitting \cite{blunsom2010unsupervised}. 
Although this makes learning easier and more efficient, 
the disadvantage is that many useful linguistic regularities are
missed: an upper bound on the performance of such simple models -- estimated by using annotated data -- 
is 76.3\% on the WSJ corpus 
\cite{DBLP:conf/emnlp/SpitkovskyAJ13}, compared to over 93\% 
actual performance of the SOTA supervised parsers.

Second, we would like to make use of information available from lexical semantics, as in 
\newcite{bansal2014tailoring}, \newcite{le2014the}, and \newcite{chen2014}.
Lexical semantics is a source for handling rare 
words and syntactic ambiguities. For instance, if a parser 
can identify that ``he'' is a dependent of ``walks'' in 
the sentence ``He walks'', then, even if ``she'' and ``runs'' 
do not appear in the training data, the parser may still be able 
to recognize that ``she'' should be a dependent of ``runs'' 
in the sentence ``she runs''. Similarly, a parser can make use of the fact that 
``sauce'' and ``John'' have very different meanings to decide  
that they have different heads 
in the two phrases ``ate spaghetti with sauce'' and 
``ate spaghetti with John''. 

However, applying existing supervised parsing techniques to the task of unsupervised parsing is, unfortunately, not trivial. 
The reason is that those parsers are optimally designed for being trained on 
manually annotated data. 
If we use existing unsupervised training methods (like EM), learning could be easily 
misled by a large amount of ambiguity naturally embedded in unannotated 
training data.
Moreover, the computational cost could rapidly increase 
if the training algorithm is not designed properly.
To overcome these difficulties
we propose a framework, iterated reranking (IR), 
where existing supervised parsers are trained 
without the need of manually annotated data, starting with dependency trees provided by an existing unsupervised parser as initialiser.
Using this framework, we can employ the work of \newcite{le2014the}
to build a new system that outperforms the SOTA
unsupervised parser of \newcite{DBLP:conf/emnlp/SpitkovskyAJ13}
on the WSJ corpus. 

The contribution of this paper is twofold. First, we show 
the benefit of using lexical semantics for the unsupervised parsing
task. Second, our work is a bridge connecting the two research 
areasÂ± unsupervised parsing and its supervised counterpart. 
Before going to the next section, in order to avoid confusion introduced by names, 
it is worth noting that we use \textit{un-trained} existing supervised parsers
which will be trained on \textit{automatically annotated} treebanks.

\section{Related Work}
\label{section related work}

\subsection{Unsupervised Dependency Parsing}
\label{section related work - udp}

The first breakthrough was set by \newcite{DBLP:conf/acl/KleinM04}
with their dependency model with valence (DMV), the first 
model to outperform the right-branching baseline on the 
DDA metric: 43.2\% vs 33.6\% on sentences up to length 10 
in the WSJ corpus. Nine years later, \newcite{DBLP:conf/emnlp/SpitkovskyAJ13} 
achieved much higher DDAs: 72.0\% 
on sentences up to length 10, and 64.4\% on all sentences 
in section 23. During this period,  
many approaches have been proposed to attempt the challenge.

\newcite{naseem2011using}, \newcite{tu2012unambiguity}, 
\newcite{spitkovsky2012wabisabi}, \newcite{DBLP:conf/emnlp/SpitkovskyAJ13}, 
and \newcite{DBLP:conf/acl/MarecekS13}
employ extensions of the DMV but with different learning strategies. 
\newcite{naseem2011using} use semantic cues, which are event 
annotations from an out-of-domain annotated corpus, in their model 
during training. 
Relying on the fact that natural language grammars must be unambiguous 
in the sense that a sentence should have very few correct parses, 
\newcite{tu2012unambiguity} incorporate unambiguity regularisation 
to posterior probabilities. \newcite{spitkovsky2012wabisabi} bootstrap 
the learning by slicing up all input sentences at punctuation. 
\newcite{DBLP:conf/emnlp/SpitkovskyAJ13} propose a complete deterministic 
learning framework for breaking out of local optima using count transforms 
and model recombination. \newcite{DBLP:conf/acl/MarecekS13} make use 
of a large raw text corpus (e.g., Wikipedia) to estimate stop probabilities,
using the reducibility principle. 

Differing from those works, 
\newcite{bisk2012simple} rely on Combinatory Categorial Grammars with a 
small number of hand-crafted general linguistic principles; whereas 
\newcite{blunsom2010unsupervised} use Tree Substitution Grammars 
with a hierarchical non-parametric Pitman-Yor process prior 
biasing the learning to a small grammar. 

\subsection{Reranking}

Our work relies on reranking 
which is a technique widely used in (semi-)supervised parsing. 
Reranking requires two components: a -best parser and a reranker. 
Given a sentence, the parser generates a list of  best
candidates, the reranker then rescores those candidates and picks
the one that has the highest score. 
Reranking was first successfully applied to supervised constituent parsing
\cite{DBLP:conf/icml/Collins00,DBLP:conf/acl/CharniakJ05}. 
It was then employed in the supervised dependency parsing approaches 
of \newcite{sangati2009generative}, 
\newcite{hayashi2013efficient}, and \newcite{le2014the}.

Closest to our work is the work series on semi-supervised constituent 
parsing of McClosky and colleagues, e.g. \newcite{mcclosky2006effective}, 
using self-training.
They use a -best generative parser and a discriminative reranker 
to parse unannotated sentences, then add resulting parses to 
the training treebank and re-train the reranker. 
Different from their work, our work is for unsupervised dependency 
parsing, without manually annotated data, and uses iterated reranking
instead of single reranking. In addition, both two components, -best
parser and reranker, are re-trained after each iteration. 


\section{The IR Framework}
\label{section MPIR}

Existing training methods for the unsupervised dependency 
task, such as \newcite{blunsom2010unsupervised},
\newcite{gillenwater2011posterior}, and \newcite{tu2012unambiguity},
are hypothesis-oriented search with the EM algorithm 
or its variants: training is to move 
from a point which represents a model hypothesis to another point.
This approach is feasible for optimising models 
using simple features since existing dynamic programming 
algorithms can compute expectations, 
which are sums over all possible parses, 
or to find the best parse in the whole parse space with low complexities. 
However, the complexity increases rapidly if rich, complex 
features are used. 
One way to reduce the computational cost is to use approximation 
methods like sampling as in \newcite{blunsom2010unsupervised}. 

\subsection{Treebank-oriented Greedy Search}

Believing that the difficulty of using EM is from the fact that 
treebanks are `hidden', leading to the need of computing sum (or max) overall 
possible treebanks,  
we propose a greedy local search scheme based on another  
training philosophy: treebank-oriented search.
The key idea is to explicitly search for concrete treebanks which 
are used to train parsing models. 
This scheme thus allows supervised parsers to
be trained in an unsupervised parsing setting 
since there is a (automatically annotated) 
treebank at any time. 

Given  a set of raw sentences, the search 
space consists of all possible treebanks
 
where  is a dependency tree of sentence .
The target of search is the optimal treebank  that is 
as good as human annotations. Greedy search with this 
philosophy is as follows: starting at an initial point 
, we pick up a point  among its 
neighbours  such that 

where  is an objective function 
measuring the goodness of  (which may or may not be conditioned 
on ). We then continue this search until some stop 
criterion is satisfied. The crucial factor here is to define 
 and . 
Below are two special cases of this scheme.

\paragraph{Semi-supervised parsing using reranking} 
\cite{mcclosky2006effective}. This reranking 
is indeed one-step greedy local search. In this scenario, 
 is the Cartesian product of -best 
lists generated by a -best parser, and 
 is a reranker.

\paragraph{Unsupervised parsing with hard-EM}
\cite{spitkovsky2010viterbiem} 
In hard-EM, the target is to maximise 
the following objective function with respect to a parameter 
set 

where  is the set of all possible dependency structures of . 
The two EM steps are thus
\begin{itemize}
\item Step 1: 

\item Step 2: 

\end{itemize}
In this case,  is the whole treebank space and 
.


\subsection{Iterated Reranking}



We instantiate the greedy search scheme by iterated reranking
which requires two components: a -best parser , and a reranker . 
Firstly,  is used to train these two components, 
resulting in  and . The parser  then generates 
a set of lists of  candidates  (whose Cartesian 
product results in ) for the set of 
training sentences . The best candidates, according 
to reranker , are collected to form  for the 
next iteration. This process is halted when a pre-defined stop 
criterion is met.\footnote{
It is worth noting that, although  
has the size  where  is the number of sentences, reranking 
only needs to process  parses if these sentences 
are assumed to be independent.}

It is certain that we can, as in the work of 
\newcite{spitkovsky2010viterbiem} and many bootstrapping approaches, 
employ only parser . Reranking, however, brings us two benefits. 
First, it allows us to employ very expressive models like the -order 
generative model proposed by \newcite{le2014the}. Second, 
it embodies a similar idea to co-training
\cite{DBLP:conf/colt/BlumM98}:  and  play roles as two 
views of the data. 



\subsection{Multi-phase Iterated Reranking}



Training in machine learning often uses  
\textit{starting big} which is to use up all training data at 
the same time. However, \newcite{elman1993learning} suggests 
that in some cases, learning should start by training simple 
models on small data and then gradually increase the model
complexity and add more difficult data. This is called 
\textit{starting small}. 

In unsupervised dependency parsing, starting small is 
intuitive. For instance, given a set of long sentences, learning 
the fact that the head of a sentence is its main verb is 
difficult because a long sentence always contains many syntactic 
categories. It would be much easier if we start with only length-one 
sentences, e.g ``Look!'', since there is only one choice which is 
usually a verb. This training scheme was successfully applied 
by \newcite{SpitkovskyEtAl10} under the name: Baby Step.

We adopt starting small to construct the multi-phase iterated reranking 
(MPIR) framework.
In phase 0, a parser  with a simple model  is trained on 
a set of short sentences  as in traditional 
approaches. This parser is used to parse a larger set of sentences 
, resulting in 
.  is then used as 
the starting point for the iterated reranking in phase 1. 
We continue this process until phase  finishes, with 
 (). 
In general, we use the resulting reranker in the previous phase 
to generate the starting point for the iterated reranking in the 
current phase. 

\section{\newcite{le2014the}'s Reranker}
\label{section reranker}

\newcite{le2014the}'s reranker is an exception among supervised 
parsers because it employs an extremely \textit{expressive} model
whose features are -order\footnote{In fact, the order is finite but unbound.}. 
To overcome the problem of 
sparsity, they introduced the inside-outside recursive
neural network (IORNN) architecture that can estimate 
tree-generating models including those proposed by 
\newcite{eisner1996three} and \newcite{collins2003head}. 

\subsection{The -order Generative Model}

\newcite{le2014the}'s reranker employs the generative model proposed by 
\newcite{eisner1996three}. Intuitively, this model is top-down: starting 
with ROOT, we generate its left dependents and its right dependents. 
We then generate dependents for each ROOT's dependent. The generative 
process recursively continues until there is no dependent to generate. 
Formally, this model is described by the following formula
{\small

}
where  is the current head,  is the fragment of the dependency 
parse rooted at , and  is the context to generate .  
 are respectively 's left dependents and right dependents, 
plus  (End-Of-Children), a special token to inform that there are 
no more dependents to generate. Thus,  is the probability 
of generating the entire dependency structure .

Le and Zuidema's -order generative model is defined as 
Eisner's model in which the context  
to generate  contains \textit{all} of 's generated siblings, 
its ancestors and their siblings.
Because of very large fragments that contexts are allowed to hold, 
traditional count-based methods are impractical (even if we use smart 
smoothing techniques). They thus introduced the
IORNN architecture to estimate the model.


\subsection{Estimation with the IORNN}

\begin{figure}
    \centering
    \includegraphics[scale=0.35]{IORNN.png}
    \caption{Inside-Outside Recursive Neural Network (IORNN).
            Black/white rectangles correspond to inner/outer representations.}
    \label{figure iornn}
\end{figure}


An IORNN (Figure~\ref{figure iornn}) is a recursive neural network 
whose topology is a tree. What make this network different from 
traditional RNNs \cite{socher_learning_2010} is that each tree node 
 caries two vectors:  - the inner 
representation, represents the content of the phrase covered 
by the node, and  - the outer representation, 
represents the context around that phrase. In addition, information 
in an IORNN is allowed to flow not only bottom-up as in RNNs, but 
also top-down. That makes IORNNs a natural tool for estimating 
top-down tree-generating models. 

Applying the IORNN architecture to dependency parsing is straightforward, 
along the generative story of the -order generative model. 
First of all, the ``inside'' part of this IORNN is simpler than 
what is depicted in Figure~\ref{figure iornn}: the inner representation 
of a phrase is assumed to be the inner representation of its head. 
This approximation is plausible since the meaning of a phrase is often 
dominated by the meaning of its head. The inner representation at 
each node, in turn, is a function of a vector 
representation for the word (in our case, the word vectors are initially
borrowed from \newcite{collobert_natural_2011}), the POS-tag and
capitalisation feature.

Without loss of generality and ignoring directions for simplicity, 
they assume that the model is generating 
dependent  for node  conditioning on context  
which contains all of 's ancestors (including ) and theirs 
siblings, and all of previously generated 's sisters. 
Now there are two types of contexts: \textit{full} contexts of heads (e.g., )
whose dependents are being generated, and contexts to generate nodes
(e.g., ). 
Contexts of the first type are clearly represented by outer 
representations. Contexts of the other type are represented by 
\textit{partial outer representations}, denoted by .
Because the context to generate a node can be constructed recursively by 
combining the full context of its head and its previously generated sisters, 
they can compute  as a function of  and 
the inner representations of its previously generated sisters. 
On the top of , they put a softmax layer to estimate 
the probability . 

Training this IORNN is to minimise the cross entropy over all dependents. 
This objective function is indeed the negative log likelihood  
of training treebank . 


\subsection{The Reranker}
\label{subsection reranker}
Le and Zuidema's (generative) reranker is given by 

where  (Equation~\ref{equ gen}) is computed by the -order generative model 
which is estimated by an IORNN; 
and  is a -best list.



\section{Complete System}
\label{section system}

Our system is based on the multi-phase IR.
In general, any third-party parser for unsupervised 
dependency parsing can be used in phase 0, and any third-party 
parser that can generate -best lists can be used in the other 
phases. In our experiments, for phase 0, we choose the parser 
using an extension of the DMV model with stop-probability estimates
computed on a large corpus proposed by \newcite{DBLP:conf/acl/MarecekS13}. 
This system has a moderate performance\footnote{
\label{footnote mz}\newcite{DBLP:conf/acl/MarecekS13}
did not report any experimental result on the WSJ corpus. We use 
their source code at \url{http://ufal.mff.cuni.cz/udp}
with the setting presented in Section~\ref{subsection setting}. 
Because the parser does not provide the option to parse unseen 
sentences, we merge the training sentences (up to length 15) to all 
the test sentences to evaluate its performance. Note that 
this result is close to the DDA (55.4\%) that the authors reported on 
CoNLL 2007 English dataset, which is a portion of the WSJ corpus.}
on the WSJ corpus: 57.1\% vs the SOTA 64.4\% DDA of 
\newcite{DBLP:conf/emnlp/SpitkovskyAJ13}. For the other phases, 
we use the MSTParser\footnote{\url{http://sourceforge.net/projects/mstparser/}}
(with the second-order feature mode) \cite{DBLP:conf/eacl/McDonaldP06}.

Our system uses \newcite{le2014the}'s reranker (Section~\ref{subsection reranker}). 
It is worth noting that, in this case, each phase with iterated 
reranking could be seen as an approximation of hard-EM 
(see Equation~\ref{equation hardEM obj}) where 
the first step is replaced by 

In other words, instead of searching over the treebank space, 
the search is limited in a neighbour set  
generated by -best parser .

\subsection{Tuning Parser }
\label{subsection tuning parser}
Parser  trained on  defines neighbour set
 which is the Cartesian product
of the -best lists in . The position and shape 
of  is thus determined by two factors:  
how well  can fit , and . Intuitively, 
the lower the fitness is, the more 
goes far away from ; and the larger  is, 
the larger 
is. Moreover, the diversity of  is inversely 
proportional to the fitness. When the fitness decreases, 
patterns existing in the training treebank become less certain
to the parser, patterns that do not exist in the training treebank 
thus have more chances to appear in -best candidates. This 
leads to high diversity of .
We blindly set  in all of our experiments. 

With the MSTParser, there are two hyper-parameters: 
\texttt{iters}, the number of epochs, and 
\texttt{training-k}, the -best parse set size to 
create constraints during training. \texttt{training-k} 
is always 1 because constraints from -best parses with 
almost incorrect training parses are useless.

Because \texttt{iters} controls the fitness of the 
parser to training treebank , it, as pointed 
out above, determines the distance from 
to  and the diversity of the former. Therefore, if we 
want to encourage the local search to explore more distant areas, we 
should set \texttt{iters} low. In our experiments, 
we test two strategies: (i) MaxEnc, \texttt{iters} = 1, 
maximal encouragement, and (ii) MinEnc, \texttt{iters} = 10, 
minimal encouragement.


\subsection{Tuning Reranker }

Tuning the reranker  is to set values for \texttt{dim}, 
the dimensions of inner and outer representations, and 
\texttt{iters}, the number of epochs to train the IORNN. 
Because the -order model is very expressive and feed-forward 
neural networks are universal approximators 
\cite{cybenko1989approximation}, the reranker is capable of perfectly 
remembering all training parses. In order to avoid this, we set 
\texttt{dim} = 50, and set \texttt{iters} = 5 
for \textit{very} early stopping.


\subsection{Tuning multi-phase IR}

Because \newcite{DBLP:conf/acl/MarecekS13}'s parser does not distinguish 
training data from test data, we postulate . 
Our system has  phases such that  
contain all sentences up to length ,  ()
contains all sentences up to length , 
and  contains all sentences up to length 25.
Phase 1 halts after 100 iterations whereas all the following phases run 
with one iteration. Note that we force the local search in phase 1 to run 
intensively because we hypothesise that most of the important patterns for 
dependency parsing can be found within short sentences.


\section{Experiments}
\label{section experiments}

\subsection{Setting}
\label{subsection setting}

We use the Penn Treebank WSJ corpus: sections 02-21  
for training, and section 23 for testing. We then apply the standard
pre-processing\footnote{ 
\url{http://www.cs.famaf.unc.edu.ar/~francolq/en/proyectos/dmvccm}}
for unsupervised dependency parsing task \cite{DBLP:conf/acl/KleinM04}: 
we strip off all empty sub-trees, punctuation, and 
terminals (tagged \# and \0.1_\text{MST}_\text{MST}DDA_\textit{MaxEnc} - DDA_\textit{MinEnc}3\mathcal{S}^{(3)}DDA_\textit{MaxEnc} - DDA_\textit{MinEnc}3\mathcal{S}^{(3)}k\mathcal{S}^{(1)}\infty\mathcal{S}^{(1)}\le 15\le 15\ge 3\ge 7\infty\inftyP(x|\mathcal{C}^\infty(u))kk\inftyk\breve{s}\acute{c}$ for helpful discussion. 

\newpage
\bibliographystyle{naaclhlt2015}
\bibliography{ref}

\end{document}
