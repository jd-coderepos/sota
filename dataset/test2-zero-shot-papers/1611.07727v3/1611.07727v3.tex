\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{tabularx,ragged2e}
\usepackage{booktabs,calc}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{array}
\usepackage{float}
\usepackage{subcaption}
\restylefloat{table}
\usepackage{makecell}
\usepackage{color}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage[export]{adjustbox}
\mathchardef\mhyphen="2D
\usepackage[font={small},skip=0pt]{caption}

\usepackage{authblk}


\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}



\DeclareMathOperator*{\argmin}{argmin}

\newcommand{\fixmeu}[1]{\textcolor{green}{\xspace\textbf{U:}\xspace#1}\xspace}	\newcommand{\fixmea}[1]{\textcolor{blue}{\xspace\textbf{A:}\xspace#1}\xspace}		\newcommand{\fixmej}[1]{\textcolor{red}{\xspace\textbf{J:}\xspace#1}\xspace}		


\def\Fig{Fig\onedot} 
\def\Tab{Tab\onedot}
\def\Sec{Sec\onedot}

\cvprfinalcopy 

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ifcvprfinal\pagestyle{empty}\fi

\begin{document}
\newcommand\threecycles{\mathop{3\mhyphen \text{cycles}}}
\newcommand{\KU}{\underline{K}}
\newcommand{\KI}{\overline{K}}

\newcolumntype{Y}{>{\centering\arraybackslash}X}
\newcolumntype{s}{>{\hsize=.1\hsize}Y}
\newcolumntype{t}{>{\hsize=.2\hsize}Y}
\newcolumntype{u}{>{\hsize=.24\hsize}Y}



\title{PoseTrack: Joint Multi-Person Pose Estimation and Tracking}
\author[1]{\vspace{-6mm}Umar Iqbal}
\author[2]{Anton Milan}
\author[1]{Juergen Gall\vspace{-3mm}}
\affil[1]{Computer Vision Group, University of Bonn, Germany}
\affil[2]{Australian Centre for Visual Technologies, University of Adelaide, Australia\vspace{-3mm}}
\maketitle


\begin{abstract}
 In this work, we introduce the challenging problem of joint multi-person pose estimation and tracking of an unknown number of persons in unconstrained videos. Existing methods for multi-person pose estimation in images cannot be applied directly to this problem, since it also requires to solve the problem of person association over time in addition to the pose estimation for each person. We therefore propose a novel method that jointly models multi-person pose estimation and tracking in a single formulation. To this end, we represent body joint detections in a video by a spatio-temporal graph and solve an integer linear program to partition the graph into sub-graphs that correspond to plausible body pose trajectories for each person. The proposed approach implicitly handles occlusion and truncation of persons. Since the problem has not been addressed quantitatively in the literature, we introduce a challenging ``Multi-Person PoseTrack'' dataset, and also propose a completely unconstrained evaluation protocol that does not make any assumptions about the scale, size, location or the number of persons. Finally, we evaluate the proposed approach and several baseline methods on our new dataset. 

\end{abstract}

\section{Introduction}
Human pose estimation has long been motivated for its applications in understanding human interactions, activity recognition, video surveillance and sports video analytics. The field of human pose estimation in images has progressed remarkably over the past few years. The methods have advanced from pose estimation of single pre-localized persons \cite{pishchulin2016deepcut, carreira2015human, wei2016convolutional, hu2016bottom, insafutdinov2016deepercut, newell2016eccv, bulat2016human, rafi2016bmvc} to the more challenging and realistic case of multiple, potentially overlapping and truncated persons \cite{gkioxari2014using, chen2015parsing, pishchulin2016deepcut, insafutdinov2016deepercut, Iqbal_ECCVw2016}. 
Many applications, such as mentioned before, however, aim to analyze human body motion over time.     
While there exists a notable number of works that track the pose of a single person in a video \cite{park_iccv2011, cherian_cvpr2014, dong2015iccv, ramakrishna_cvpr2013, zuffi_iccv2013, jain_accv2014, Pfister15a, charles2016cvpr, georgia2016eccv, iqbal2017FG}, multi-person human pose estimation in unconstrained videos has not been addressed in the literature. 

In this work, we address the problem of tracking the poses of multiple persons in an unconstrained setting. This means that we have to deal with large pose and scale variations, fast motions, and a varying number of persons and visible body parts due to occlusion or truncation. In contrast to previous works, we aim to solve the association of each person across the video and the pose estimation together. To this end, we build upon the recent methods for multi-person pose estimation in images~\cite{pishchulin2016deepcut, insafutdinov2016deepercut, Iqbal_ECCVw2016} that build a spatial graph based on joint proposals to estimate the pose of multiple persons in an image. In particular, we cast the problem as an optimization of a densely connected spatio-temporal graph connecting body joint candidates spatially as well as temporally. The optimization problem is formulated as a constrained Integer Linear Program (ILP) whose feasible solution partitions the graph into valid body pose trajectories for any unknown number of persons. In this way, we can handle occlusion, truncation, and temporal association within a single formulation.   

\begin{figure}[t]
  \centering
    \includegraphics[width=0.49\linewidth]{annotation_sample1} 
    \hfill
    \includegraphics[width=0.49\linewidth]{annotation_sample2} 
  \caption{Example frames and annotations from the proposed Multi-Person PoseTrack dataset.   \vspace{-5mm}}
    \label{fig:annotation_examples}
\end{figure}

Since there exists no dataset that provides annotations to quantitatively evaluate joint multi-person pose estimation and tracking, we also propose a new challenging \emph{Multi-Person PoseTrack} dataset as a second contribution of the paper. The dataset provides detailed and dense annotations for multiple persons in each video, as shown in Fig.~\ref{fig:annotation_examples}, and introduces new challenges to the field of pose estimation in videos. In order to evaluate the pose estimation and tracking accuracy, we introduce a new protocol that also deals with occluded body joints. We quantify the proposed method in detail on the proposed dataset, and also report results for several baseline methods.
The source code, pre-trained models and the dataset are publicly available.\footnote{\url{http://pages.iai.uni-bonn.de/iqbal_umar/PoseTrack/}} 


\section{Related Work}

Single person pose estimation in images has seen a remarkable progress over the past few years \cite{toshev2014deeppose, pishchulin2016deepcut, carreira2015human, wei2016convolutional, hu2016bottom, insafutdinov2016deepercut, newell2016eccv, bulat2016human, rafi2016bmvc}. However, all these approaches assume that only a single person is visible in the image, and cannot handle realistic cases where several people appear in the scene, and interact with each other. In contrast to single person pose estimation, multi-person pose estimation introduces significantly more challenges, since the number of persons in an image is not known a priori. Moreover, it is natural that persons occlude each other during interactions, and may also become partially truncated to various degrees. Multi-person pose estimation has therefore gained much attention recently \cite{eichner2010we, sun2011articulated, pishchulin2012articulated, yang_tpami2014, Ladicky_2013_CVPR, gkioxari2014using, chen2015parsing, belagiannis2015tpami, pishchulin2016deepcut, insafutdinov2016deepercut, Iqbal_ECCVw2016}. Earlier methods in this direction follow a two-staged approach \cite{pishchulin2012articulated, gkioxari2014using, chen2015parsing} by first detecting the persons in an image followed by a human pose estimation technique for each person individually. Such approaches are, however, applicable only if people appear well separated and do not occlude each other. Moreover, most single person pose estimation methods always output a fixed number of body joints and do not account for occlusion and truncation, which often is the case in multi-person scenarios. Other approaches address the problem using tree structured graphical models \cite{yang_tpami2014, sun2011articulated, eichner2010we, Ladicky_2013_CVPR}. However, such models struggle to cope with large pose variations, and are shown to be significantly outperformed by more recent methods based on Convolutional Neural Networks \cite{pishchulin2016deepcut, insafutdinov2016deepercut}. For example, \cite{pishchulin2016deepcut} jointly estimate the pose of all persons visible in an image, while also handling occlusion and truncation. The approach has been further improved by stronger part detectors and efficient approximations~\cite{insafutdinov2016deepercut}. The approach in \cite{Iqbal_ECCVw2016} also proposes a simplification of \cite{pishchulin2016deepcut} by tackling the problem locally for each person. However, it still relies on a separate person detector. 

Single person pose estimation in videos has also been studied extensively in the literature \cite{park_iccv2011, cherian_cvpr2014, zuffi_iccv2013, ramakrishna_cvpr2013, zuffi_iccv2013,  jain_accv2014, dong2015iccv, Pfister15a, georgia2016eccv, iqbal2017FG}. These approaches mainly aim to improve pose estimation by utilizing temporal smoothing constraints \cite{park_iccv2011, cherian_cvpr2014, dong2015iccv, ramakrishna_cvpr2013, georgia2016eccv} and/or optical flow information \cite{zuffi_iccv2013, jain_accv2014, Pfister15a}, but they are not directly applicable to videos with multiple potentially occluding persons. 

In this work we focus on the challenging problem of joint multi-person pose estimation and data association across frames. While the problem has not been studied quantitatively in the literature\footnote{Contemporaneously with this work, the problem has also been studied in \cite{Insafutdinov:2017:CVPR}}, there exist early works towards the problem \cite{Izadinia:2012:ECCV, Andriluka:2008:CVPR}. These approaches, however, do not reason jointly about pose estimation and tracking, but rather focus on multi-person tracking alone. The methods follow a multi-staged strategy, \ie they first estimate body part locations for each person separately and subsequently leverage body part tracklets to facilitate person tracking. We on the other hand propose to simultaneously estimate the pose of multiple persons and track them over time. To this end, we build upon the recent progress on multi-person pose estimation in images \cite{pishchulin2016deepcut, insafutdinov2016deepercut, Iqbal_ECCVw2016} and propose a joint objective for both problems. 

Previous datasets used to benchmark pose estimation algorithms in-the-wild are summarized in Tab.~\ref{tab:datasets}. While there exists a number of datasets to evaluate single person pose estimation methods in videos, such as \eg, \mbox{J-HMDB}~\cite{Jhuang_iccv2013} and Penn-Action~\cite{zhang2013actemes}, none of the video datasets provides annotations to benchmark multi-person pose estimation and tracking at the same time. To allow for a quantitative evaluation of this problem, we therefore also introduce a new ``Multi-Person PoseTrack'' dataset which provides pose annotations for multiple persons in each video to measure pose estimation accuracy, and also provides a unique ID for each of the annotated persons to benchmark multi-person pose tracking. The proposed dataset introduces new challenges to the field of human pose estimation and tracking since it contains a large amount of appearance and  pose variations, body part occlusion and truncation, large scale variations,  fast camera and person movements, motion blur, and a sufficiently large number of persons per video.   

\begin{figure*}[t]
  \centering
    \begin{tabular}{c}
		\includegraphics[trim={0 2mm 0 2mm}, clip, width=0.85\linewidth]{15_detections_20} \\
  		\includegraphics[trim={0 2mm 0 2mm}, clip, width=0.85\linewidth]{15_spatio-temporal_edges_20} \\
	    \includegraphics[trim={0 2mm 0 2mm}, clip, width=0.85\linewidth]{15_final_20} \\
    \end{tabular}
  \caption{{\bf Top:} Body joint detection hypotheses shown for three frames. {\bf Middle:} Spatio-temporal graph with spatial edges (blue) and temporal edges for head (red) and neck (yellow). We only show a subset of the edges. {\bf Bottom:} Estimated poses for all persons in the video. Each color corresponds to a unique person identity. 
  }
    \label{fig:overview}
    \vspace{-2mm}
\end{figure*}


\section{Multi-Person Pose Tracking}
\label{sec:joint_pose_estimation_and_tracking}

Our method jointly solves the problem of multi-person pose estimation and tracking for all persons appearing in a video together. We first generate a set of joint detection candidates in each video as illustrated in Fig.~\ref{fig:overview}. From the detections, we build a graph consisting of spatial edges connecting the detections within a frame and temporal edges connecting detections of the same joint type over frames.   
We solve the problem using integer linear programming (ILP) whose feasible solution provides the pose estimate for each person in all video frames, and also performs person association across frames.
We first introduce the proposed method and discuss the proposed dataset for evaluation in Sec.~\ref{sed:data}.

\subsection{Spatio-Temporal Graph}\label{sec:graph}
Given a video sequence  containing an arbitrary number of persons, we generate a set of body joint detection candidates  where  is the set for frame~. Every detection  at location  in frame  belongs to a joint type . Additional details regarding the used detector will be provided in Sec.~\ref{sec:optimization_costs}.




For multi-person pose tracking, we aim to identify the joint hypotheses that belong to an individual person in the entire video. This can be formulated by a graph structure  where  is the set of nodes. The set of edges  consists of two types of edges, namely spatial edges  and temporal edges . The spatial edges correspond to the union of edges of a fully connected graph for each frame, \ie 

Note that these edges connect joint candidates independently of the associated joint type . The temporal edges connect only joint hypotheses of the same joint type over two different frames, \ie 

The temporal connections are not only modeled for neighboring frames, \ie , but we also take temporal relations up to  frames into account to handle short-term occlusion and missing detections. The graph structure is illustrated in Fig.~\ref{fig:overview}.                      


\subsection{Graph Partitioning}
\label{sec:graph-partitioning}
By removing edges and nodes from the graph \mbox{}, we obtain several partitions of the spatio-temporal graph and each partition corresponds to a tracked pose of an individual person. In order to solve the graph partitioning problem, we introduce the three binary vectors ,  , and . Each binary variable implies if a node or edge is removed, \ie  implies that the joint detection  is removed. Similarly,  with  implies that the spatial edge between the joint hypothesis  and  in frame  is removed while   
 with  implies that the temporal edge between the joint hypothesis  in frame  and  in frame  is removed.

A partitioning is obtained by minimizing the cost function  

This means that we search for a graph partitioning such that the cost of the remaining nodes and edges is minimal. The cost for a node  is defined by the unary term:  

where  corresponds to the probability of the joint hypothesis . Note that  is negative when  and detections with a high confidence are preferred since they reduce the cost function \eqref{eq:ilp_obj}. The cost for a spatial or temporal edge is defined similarly by

While  denotes the probability that two joint detections  and  in a frame  belong to the same person,  denotes the probability that two detections of a joint in frame  and  are the same. In Sec.~\ref{sec:optimization_costs} we will discuss how the probabilities , , and  are learned.  


In order to ensure that the feasible solutions of the objective \eqref{eq:ilp_obj} result in well defined body poses and valid pose tracks, we have to add additional constraints. The first set of constraints ensures that two joint hypotheses are associated to the same person () only if both detections are considered as valid, \ie,  and :  

The same holds for the temporal edges:


The second set of constraints are transitivity constraints in the spatial domain. Such transitivity constraints have been proposed for multi-person pose estimation in images~\cite{pishchulin2016deepcut, insafutdinov2016deepercut, Iqbal_ECCVw2016}. They enforce for any triplet of joint detection candidates  that if  and  are associated to one person and  and  are also associated to one person, \ie  and , then the edge  should also be added:  

An example of a transitivity constraint is illustrated in Fig.~\ref{fig:constraints-illua}. The transitivity constraints can be used to enforce that a human can have only one joint type , \eg only one head. Let  and  have the same joint type  while  belongs to another joint type . Without transitivity constraints connecting  and  with  
might result in a low cost. The transitivity constraints, however, enforce that the binary cost  is added. 
To prevent poses with multiple joints, we thus only have to ensure that the binary cost  is very high if . We discuss this more in detail in Sec.~\ref{sec:optimization_costs}. 

In contrast to previous work, we also have to ensure spatio-temporal consistency. Similar to the spatial transitivity constraints~\eqref{eq:cont:spatial-transitivity}, we can define temporal transitivity constraints:    


The last set of constraints are spatio-temporal constraints that ensure that the pose is consistent over time. We define two types of spatio-temporal constraints. The first type consists of a triplet of joint detection candidates  from two different frames  and . The constraints are defined as,

and enforce transitivity for two temporal edges and one spatial edge. 
The second type of spatio-temporal constraints are based on quadruples of joint detection candidates  from two different frames  and . The spatio-temporal constraints ensure that if  and  are temporally connected and  are spatially connected then the spatial edge  has to be added:  

An example of both types of spatio-temporal constraint can be seen in Fig.~\ref{fig:constraints-illuc} and Fig.~\ref{fig:constraints-illud}, respectively.


\begin{figure}[t]
\captionsetup[subfigure]{justification=centering}
\centering
\begin{subfigure}[t]{0.11\textwidth}
\includegraphics[scale=0.65, center]{cvpr2017_posetrack-figure0}
\caption{}
\label{fig:constraints-illua}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.11\textwidth}
\includegraphics[scale=0.65,center]{cvpr2017_posetrack-figure2}
\caption{}
\label{fig:constraints-illub}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.11\textwidth}
\includegraphics[scale=0.65,center]{cvpr2017_posetrack-figure3}
\caption{}
\label{fig:constraints-illuc}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.11\textwidth}
\includegraphics[scale=0.65,center]{cvpr2017_posetrack-figure1}
\caption{}
\label{fig:constraints-illud}
\end{subfigure}
\caption{
\emph{(a)} The spatial transitivity constraints~\eqref{eq:cont:spatial-transitivity} ensure that if the two joint hypotheses  and  are spatially connected to  (red edges) then the cost of the spatial edge between  and  (green edge) also has to be added. \emph{(b)} The temporal transitivity constraints~\eqref{eq:cont:temporal-transitivity} ensure transitivity for temporal edges (dashed). \emph{(c)} The spatio-temporal transitivity constraints~\eqref{eq:cont:spatio-temporal-transitivity} model transitivity for two temporal edges and one spatial edge. \emph{(d)} The spatio-temporal consistency constraints~\eqref{eq:cont:spatio-temporal-consistency} ensure that if two pairs of joint hypotheses  and  are temporally connected (dashed red edges) and  and  are spatially connected (solid red edge) then the cost of the spatial edge between  and  (solid green edge) also has to be added.
}
\label{fig:contraints}
\end{figure}

\subsection{Optimization}
We optimize the objective \eqref{eq:ilp_obj} with the branch-and-cut algorithm of the ILP solver Gurobi. To reduce the runtime for long sequences, we process the video batch-wise where each batch consists of  frames. For the first  frames, we build the spatio-temporal graph as discussed and optimize the objective \eqref{eq:ilp_obj}. We then continue to build a graph for the next  frames and add the previously selected nodes and edges to the graph, but fix them such that they cannot be removed anymore. Since the graph partitioning produces also small partitions, which usually correspond to clusters of false positive joint detections, we remove any partition that is shorter than 7 frames or has less than 6 nodes per frame on average. 

\vspace{-1mm}
\subsection{Potentials}
\label{sec:optimization_costs}
In order to compute the unaries  \eqref{eq:unary} and binaries  \eqref{eq:sbinary},\eqref{eq:tbinary}, we have to learn the probabilities , , and . 

The probability  is given by the confidence of the joint detector. As joint detector, we use the publicly available pre-trained CNN~\cite{insafutdinov2016deepercut} trained on the MPII Multi-Person Pose dataset \cite{pishchulin2016deepcut}. In contrast to~\cite{insafutdinov2016deepercut}, we do not assume that any scale information is given. We therefore apply the detector to an image pyramid with 4 scales . For each detection  located at , we compute a quadratic bounding box . We use  for the width and height. To reduce the number of detections, we remove all bounding boxes that have an intersection-over-union (IoU) ratio over 0.7 with another bounding box that has a higher detection confidence.   

The spatial probability  depends on the joint types  and  of the detections. If , we define \mbox{}. This means that a joint type  cannot be added multiple times to a person except if the detections are very close. If a partition includes detections of the same type in a single frame, the detections are merged by computing the weighted mean of the detections, where the weights are proportional to . If , we use the pre-trained binaries~\cite{insafutdinov2016deepercut} after a scale normalization.           

The temporal probability  should be high if two detections of the same joint type at different frames belong to the same person. To that end, we build on the idea recently used in multi-person tracking \cite{tang-2016} and compute dense correspondences between two frames using DeepMatching \cite{weinzaepfel:hal-00873592}. Let  and  be the sets of matched key-points inside the bounding boxes  and  and  and  the union and intersection of these two sets. We then form a feature vector by  where \mbox{}. We also append the feature vector with non-linear terms as done in \cite{tang-2016}. The mapping from the feature vector to the probability  is obtained by logistic regression. 


\newcommand{\squeezeCol}{\vspace{-0.1cm}}


\begin{table}
\centering
\scriptsize
\begin{tabularx}{\columnwidth}{Xssssu}
\toprule
Dataset & \rotatebox[origin=c]{90}{\parbox{1.5cm}{\centering \squeezeCol Video-labeled \\ poses \squeezeCol}} & \rotatebox[origin=c]{90}{multi-person} & \rotatebox[origin=c]{90}{\parbox{1.5cm}{\centering \squeezeCol Large scale \\ variation \squeezeCol}} & \rotatebox[origin=c]{90}{\parbox{1.5cm}{\centering \squeezeCol variable \\ skeleton size \squeezeCol}} & \rotatebox[origin=c]{90}{\# of persons}\\
\midrule
Leeds Sports \cite{Ever10} & &  & & & 2000 \\
MPII Pose \cite{andriluka_cvpr2014} &   &  & \checkmark & \checkmark & 40,522\\
We Are Family \cite{eichner2010we} &   & \checkmark &  &  & 3131\\
MPII Multi-Person Pose  \cite{pishchulin2016deepcut} &   & \checkmark &  \checkmark & \checkmark & 14,161\\
MS-COCO Keypoints \cite{lin2014microsoft} &  & \checkmark & \checkmark & \checkmark & 105,698 \\
\midrule
J-HMDB \cite{Jhuang_iccv2013} & \checkmark  & & \checkmark & \checkmark & 32,173\\
Penn-Action \cite{zhang2013actemes} & \checkmark & & \checkmark & & 159,633\\ 
VideoPose \cite{sap_cvpr2011} & \checkmark  & & & & 1286\\ 
Poses-in-the-wild \cite{cherian_cvpr2014}  & \checkmark & & & & 831\\ 
YouTube Pose \cite{charles2016cvpr} & \checkmark  & &  & & 5000\\ 
FYDP \cite{shen2014_eccv2014} & \checkmark  & & & & 1680\\ 
UYDP \cite{shen2014_eccv2014} & \checkmark  & & & & 2000\\ 
\midrule
\textbf{Multi-Person PoseTrack} & \checkmark & \checkmark & \checkmark & \checkmark & 16,219\\ 
\bottomrule
\end{tabularx}
\caption{A comparison of PoseTrack dataset with the existing related datasets for human pose estimation in images and videos.\vspace{-3mm}}
\label{tab:datasets}
\end{table}



\section{The Multi-Person PoseTrack Dataset}\label{sed:data}
In this section we introduce our new dataset for multi-person pose estimation in videos. The MPII Multi-Person Pose \cite{andriluka_cvpr2014} is currently one of the most popular benchmarks for multi-person pose estimation in images, and covers a wide range of activities. For each annotated image, the dataset also provides unlabeled video clips ranging 20 frames both forward and backward in time relative to that image. For our video dataset, we manually select a subset of all available videos that contain multiple persons and cover a wide variety of person-person or person-object interactions. Moreover, the selected videos are chosen to contain a large amount of body pose appearance and scale variation, as well as body part occlusion and truncation. The videos also contain severe body motion, \ie, people occlude each other, re-appear after complete occlusion, vary in scale across the video, and also significantly change their body pose. The number of visible persons and body parts may also vary during the video. 
The duration of all provided video clips is exactly 41 frames. To include longer and variable-length sequences, we downloaded the original raw video clips using the provided URLs and obtained an additional set of videos. To prevent an overlap with the existing data, we only considered sequences that are at least 150 frames apart from the training samples, and followed the same rationale as above to ensure diversity.

In total, we compiled a set of 60 videos with the number of frames per video ranging between 41 and 151. The number of persons ranges between 2 and 16 with an average of more than 5 persons per video sequence, totaling over 16,000 annotated poses. The person heights are between 100 and 1200 pixels. We split the dataset into a training and testing set with an equal number of videos.  



\subsection{Annotation}
As in~\cite{andriluka_cvpr2014}, we annotate 14 body joints and a rectangle enclosing the person's head. The latter is required to estimate the absolute scale which is used for evaluation. We assign a unique identity to every person appearing in the video. This person ID remains the same throughout the video until the person moves out of the field-of-view. Since we do not target person re-identification in this work, we assign a new ID if a person re-appears in the video.  We also provide occlusion flags for all body joints. A joint is marked occluded if it was in the field-of-view but became invisible due to an occlusion. Truncated joints, \ie those outside the image border limits, are not annotated, therefore, the number of joints per person varies across the dataset. Very small persons were zoomed in to a reasonable size to accurately perform the annotation. To ensure a high quality of the annotation, all annotations were performed by trained in-house workers, following a clearly defined protocol. An example annotation can be seen in Fig. \ref{fig:annotation_examples}.


\subsection{Experimental setup and evaluation metrics}
\label{sec:metrics}
Since the problem of simultaneous multi-person pose estimation and person tracking has not been quantitatively evaluated in the literature, we define a new evaluation protocol for this problem. To this end, we follow the best practices followed in both multi-person pose estimation \cite{pishchulin2016deepcut} and multi-target tracking \cite{milan2016mot16}. In order to evaluate whether a part is predicted correctly, we use the widely adopted PCKh (head-normalized probability of correct keypoint) metric \cite{andriluka_cvpr2014}, which considers a body joint to be correctly localized if the predicted location of the joint is within a certain threshold from the true location. Due to the large scale variation of people across videos and even within a frame, this threshold needs to be selected adaptively, based on the person's size. To that end,~\cite{andriluka_cvpr2014} propose to use 30\% of the head box diagonal.
We have found this threshold to be too relaxed because recent pose estimation approaches are capable of predicting the joint locations rather accurately. Therefore, we use a more strict evaluation with a 20\% threshold.

Given the joint localization threshold for each person, we compute two sets of evaluation metrics, one adopted from the multi-target tracking literature~\cite{Yang:2012:CVPR, Choi:2015:ICCV, milan2016mot16} to evaluate multi-person pose tracking, and one which is commonly used for evaluating multi-person pose estimation~\cite{pishchulin2016deepcut}.

\noindent\textbf{Tracking.}
To evaluate multi-person pose tracking, we consider each joint trajectory as one individual target,\footnote{Note that only joints of the same type are matched.} and compute multiple measures.
First, the CLEAR MOT metrics~\cite{Bernardin:2008:CLE} provide the tracking accuracy (MOTA) and tracking precision (MOTP). The former is derived from three types of error ratios: false positives, missed targets, and identity switches (IDs). These are linearly combined to produce a normalized accuracy where 100\% corresponds to zero errors. MOTP measures how precise each object, or in our case each body joint, has been localized \wrt the ground-truth.
Second, we report trajectory-based measures proposed in~\cite{Li:2009:CVPR}, that count the number of mostly tracked (MT) and mostly lost (ML) tracks. A track is considered mostly tracked if it has been recovered in at least 80\% of its length, and mostly lost if more than 80\% are not tracked.
For completeness, we also compute the number of times a ground-truth trajectory is fragmented (FM). 

\noindent\textbf{Pose.}
For measuring frame-wise multi-person pose accuracy, we use \emph{Mean Average Precision} (mAP) as is done in \cite{pishchulin2016deepcut}.
The protocol to evaluate multi-person pose estimation in \cite{pishchulin2016deepcut} assumes that the rough scale and location of a group of persons is known during testing \cite{pishchulin2016deepcut}, which is not the case in realistic scenarios, and in particular in videos. We therefore propose to make no assumption during testing and evaluate the predictions without rescaling or shifting them according to the ground-truth.

\noindent\textbf{Occlusion handling.}
Both of the aforementioned protocols to measure pose estimation and tracking accuracy do not consider occlusion during evaluation, and penalize if an occluded target that is annotated in the ground-truth is not correctly estimated \cite{milan2016mot16, pishchulin2016deepcut}. This, however, discourages methods that either detect occlusion and do not predict the occluded joints or approaches that predict the joint position even for occluded joints. We want to provide a fair comparison for both types of occlusion handling. We therefore extend both measures to incorporate occlusion information explicitly. To this end, we first assign each person to one of the ground-truth poses based on the PCKh measure as done in \cite{pishchulin2016deepcut}. For each matched person, we consider an occluded joint correctly estimated 
either if \emph{a)} it is predicted at the correct location despite being occluded,
or \emph{b)} it is not predicted at all. Otherwise, the prediction is considered as a false positive. 


\section{Experiments}
In this section we evaluate the proposed method for joint multi-person pose estimation and tracking on the newly introduced Multi-Person PoseTrack dataset. 

\begin{table}
\centering
\scriptsize
\begin{tabularx}{\columnwidth}{Xssssssstt}
\toprule
\textbf{Method} & \textbf{Rcll}  &  \textbf{Prcn} &  \textbf{MT}  &  \textbf{ML} &  \textbf{IDs}  &  \hspace*{-1mm} \textbf{FM}   & \hspace*{-3mm} \textbf{ MOTA} &   \hspace*{-1mm} \textbf{MOTP}   \\
 &  ~~ &  ~~ &   ~  &  ~ &  ~  &    ~~  &    &    \\
\midrule
\multicolumn{9}{c}{\textit{Impact of temporal connection density}} \\
\midrule
HT & 57.6 &  66.0 & 632 & 623 &  674 & 5080 &  27.2 &   56.1 \\ 
HT:N:S & 62.7 &  64.9 & 760 & 510 &  470 & 5557 &  28.2 &   55.8    \\ 
HT:N:S:H & 63.1 &  64.5 & 774 & 494 &  478 & 5564 &  27.8 &   55.7 \\ 
HT:W:A & 62.8 &  64.9  & 758 & 526 &  516 & 5458 &  28.2 &   55.8 \\   
\midrule
\multicolumn{9}{c}{\textit{Impact of the length of temporal connection ()}} \\  
\midrule
HT:N:S () & 62.7 &  64.9 & 760 & 510 &  470 & 5557 &  28.2 &   55.8  \\ 
HT:N:S () & 63.0 &  64.8 & 775 & 502 &  431 & 5629 &  28.2 &   55.7  \\ 
HT:N:S () & 62.8 &  64.7 & 763 & 508 &  381 & 5676 &  28.0 &   55.7  \\   
\midrule
\multicolumn{9}{c}{\textit{Impact of the constraints}} \\  
\midrule
All & 63.0 &  64.8 & 775 & 502 &  431 & 5629 &  28.2 &   55.7 \\ 
All  spat. transitivity & 22.2 &  76.0 & 115 & 1521 &   39 & 3947 &  15.1 &   58.0 \\ 
All  temp. transitivity & 60.3 &  65.1 & 712 & 544 &  268 & 5610 &  27.7 &   55.8 \\ 
All  spatio-temporal &  55.1 &  64.1 & 592 & 628 &  262 & 5444 &  23.9 &   55.7 \\  
\midrule
\multicolumn{9}{c}{\textit{Comparison with the Baselines}} \\  
\midrule
\bf Ours  & \textbf{63.0} &  \textbf{64.8}  & \textbf{775}  & \textbf{502} &  431 & 5629 &  \textbf{28.2} &   \textbf{55.7}  \\  
\multicolumn{9}{l}{BBox-Tracking \cite{tang-2016, ren2015faster}} \\  

\quad + LJPA \cite{Iqbal_ECCVw2016} & 58.8 &  64.8 & 716 & 646 &  \textbf{319} & \textbf{5026} &  26.6 &   53.5 \\ 
\quad + CPM \cite{wei2016convolutional} &  60.1 &  57.7 & 754 & 611 &  347 & 4969 &  15.6 &   53.4   \\ 
\bottomrule
\end{tabularx}
\caption{Quantitative evaluation of multi-person pose-tracking using common multi-object tracking metrics. Up and down arrows indicate whether higher or lower values for each metric are better. The first three blocks of the table present an ablative study on design choices \wrt joint selection, temporal edges, and constraints. The bottom part compares our final result with two strong baselines described in the text. HT:Head Top, N:Neck, S:Shoulders, W:Wrists, A:Ankles \vspace{-4mm}}
\label{tab:mot_results}
\end{table}

\subsection{Multi-Person Pose Tracking}
The results for multi-person pose tracking (MOT CLEAR metrics) are reported in \Tab~\ref{tab:mot_results}. To find the best setting, we first perform a series of experiments, investigating the influence of temporal connection density, temporal connection length, and inclusion of different constraint types.

We first examine the impact of different joint combinations for temporal connections.  Connecting only the Head Tops (HT) between frames results in a Multi-Object Tracking Accuracy (MOTA) of  with a recall and precision of  and , respectively. Adding Neck and Shoulder (HT:N:S) detections for temporal connections improves the MOTA score to , while also improving the recall from  to .  Adding more temporal connections also increases other metrics such as MT, ML, and also results in a lower number of ID switches (IDs) and fragments (FM). However, increasing the number of joints for temporal edges even further (HT:N:S:H) results in a slight decrease in performance. This is most likely due to the weaker DeepMatching correspondences between hip joints, which are difficult to match. When only the body extremities (HT:W:A) are used for temporal edges, we obtain a similar MOTA as for (HT:N:S), but slightly worse other tracking measures. Considering the MOTA performance and the complexity of our graph structure, we use (HT:N:S) as our default setting. 

Instead of considering only neighboring frames for temporal edges, we also evaluate the tracking performance while introducing longer-range temporal edges of up to  and  frames.
Adding temporal edges between detections that are at most three frames  apart improves the performance only slightly, whereas increasing the distance even further  worsens the performance. For the rest of our experiments we therefore set . 

To evaluate the proposed optimization objective \eqref{eq:ilp_obj} for joint multi-person pose estimation and tracking in more detail, we have quantified the impact of various kinds of constraints \eqref{eq:cont:consistency}-\eqref{eq:cont:spatio-temporal-consistency} enforced during the optimization. To this end, we remove one type of constraints at a time and solve the optimization problem. As shown in Tab.~\ref{tab:mot_results}, all types of constraints are important to achieve best performance, with the spatial transitivity constraints playing the most crucial role. This is expected since these constraints ensure that we obtain valid poses without multiple joint types assigned to one person.    
Temporal transitivity and spatio-temporal constraints also turn out to be important to obtain good results. Removing either of the two significantly decreases the recall, resulting in a drop in MOTA. 

Since we are the first to report results on the Multi-Person PoseTrack dataset, we also develop two baseline methods by using the existing approaches. For this, we rely on a state-of-the-art method for multi-person pose estimation in images \cite{Iqbal_ECCVw2016}. The approach uses a person detector \cite{ren2015faster} to first obtain person bounding box hypotheses, and then estimates the pose for each person independently. We extend it to videos as follows. We first generate person bounding boxes for all frames in the video using a state-of-the-art person detector (Faster R-CNN \cite{ren2015faster}), and perform person tracking using a state-of-the-art person tracker \cite{tang-2016} and train it on the training set of the Multi-Person PoseTrack Dataset. We also discard all tracks that are shorter than 7 frames. The final pose estimates are obtained by using the Local Joint-to-Person Association (LJPA) approach proposed by \cite{Iqbal_ECCVw2016} for each person track. We also report results when Convolutional Pose Machines (CPM) \cite{wei2016convolutional} are used instead. 
Since CPM does not account for joint occlusion and truncation, the MOTA score is significantly lower than for LJPA. LJPA~\cite{Iqbal_ECCVw2016} improves the performance, but remains inferior \wrt most measures compared to our proposed method. In particular, our method achieves the highest MOTA and MOTP scores. The former is due to a significantly higher recall, while the latter is a result of a more precise part localization. Interestingly, the person bounding-box tracking based baselines achieve a lower number of ID switches. We believe that this is primarily due to the powerful multi-target tracking approach~\cite{tang-2016}, which can handle person identities more robustly.


\begin{table}

\scriptsize
\begin{tabularx}{\columnwidth}{lssssssssss}
\toprule
\textbf{Method} & \textbf{Head} & \textbf{Sho} & \textbf{Elb} & \textbf{Wri} & \textbf{Hip} & \textbf{Knee}  &  \textbf{Ank} & \textbf{mAP}\\ \midrule
\multicolumn{9}{c}{\textit{Impact of the temporal connection density}} \\ 
\midrule
HT & 52.5  & 47.0  & 37.6  & 28.2  & 19.7  & 27.8 & 27.4 & 34.3 \\ 
HT:N:S & 56.1  & 51.3  & 42.1  & 31.2  & 22.0  & 31.6 & 31.3 & 37.9 \\ 
HT:N:S:H & 56.3  & 51.5  & 42.2  & 31.4  & 21.7  & 31.6 & 32.0 & 38.1 \\ 
HT:W:A & 56.0  & 51.2  & 42.2  & 31.6  & 21.6  & 31.2 & 31.7 & 37.9 \\ 
\midrule
\multicolumn{9}{c}{\textit{Impact of the length of temporal connection ()}} \\ 
\midrule
HT:N:S  & 56.1  & 51.3  & 42.1  & 31.2  & 22.0  & 31.6 & 31.3 & 37.9 \\ 
HT:N:S  & 56.5  & 51.6  & 42.3  & 31.4  & 22.0  & 31.9 & 31.6 & 38.2 \\ 
HT:N:S  & 56.2  & 51.3  & 41.8  & 31.1  & 22.0  & 31.4 & 31.5 & 37.9 \\  
\midrule
\multicolumn{9}{c}{\textit{Impact of the constraints}} \\ 
\midrule 
All & 56.5  & 51.6  & 42.3  & 31.4  & 22.0  & 31.9 & 31.6 & 38.2 \\ 
All  spat. transitivity & 7.8  & 10.1  & 7.2  & 4.6  & 2.7  & 4.9 & 5.9 & 6.2 \\ 
All  temp. transitivity & 50.5  & 46.8  & 37.5  & 27.6  & 20.3  & 30.1 & 28.7 & 34.5 \\ 
All  spatio-temporal & 42.3  & 40.8  & 32.8  & 24.3  & 17.0  & 25.3 & 22.4 & 29.3 \\ 
\midrule
\multicolumn{9}{c}{\textit{Comparison with the state-of-the-art}} \\  
\midrule
Ours & \textbf{56.5}  & 51.6  & \textbf{42.3}  & \textbf{31.4}  & 22.0  & \textbf{31.9} & \textbf{31.6} & \textbf{38.2} \\ 
\multicolumn{9}{l}{BBox-Detection \cite{ren2015faster}} \\  
\quad~ + LJPA \cite{Iqbal_ECCVw2016} & 50.5  & 49.3  & 38.3  & 33.0  & 21.7  & 29.6 & 29.2 & 35.9 \\ 
\quad~ + CPM \cite{wei2016convolutional} & 48.8  & 47.5  & 35.8  & 29.2  & 20.7  & 27.1 & 22.4 & 33.1 \\ 
DeeperCut \cite{insafutdinov2016deepercut} & 56.2  & \textbf{52.4}  & 40.1  & 30.0  & \textbf{22.8}  & 30.5 & 30.8 & 37.5 \\ 
\bottomrule
\end{tabularx}
\caption{Quantitative evaluation of multi-person pose estimation (mAP). HT:Head Top, N:Neck, S:Shoulders, W:Wrists, A:Ankles \vspace{-8mm}}
\label{tab:map_results}
\end{table}



\begin{figure*}[t]
\captionsetup[subfigure]{justification=centering}
\centering
\begin{subfigure}[t]{0.33\textwidth}
\includegraphics[scale=0.73, center]{plot_part_combinations_comparison}
\label{fig:contraints-a}
\end{subfigure}
\begin{subfigure}[t]{0.33\textwidth}
\includegraphics[scale=0.73,center]{plot_number_of_frame_connections}
\label{fig:contraints-b}
\end{subfigure}
\begin{subfigure}[t]{0.33\textwidth}
\includegraphics[scale=0.73,center]{plot_impact_of_constraints}
\label{fig:contraints-b}
\end{subfigure}
\vspace{-4mm}
\caption{\textbf{Left} Impact of the the temporal edge density. \textbf{Middle} Impact of the length of temporal edges. \textbf{Right} Impact of different constraint types.\vspace{-5mm}}
\label{fig:MOTAvsMAP}
\end{figure*}
 

\subsection{Frame-wise Multi-Person Pose Estimation}

The results for frame-wise multi-person pose estimation (mAP) are summarized in Tab.~\ref{tab:map_results}. Similar to the evaluation for pose tracking, we evaluate the impact of spatio-temporal connection density, length of temporal connections and the influence of different constraint types. Having connections only between Head Top (HT) detections results in a mAP of . As for pose tracking, introducing temporal connections for Neck and Shoulders (HT:N:S) results in a higher accuracy and improves the mAP from  to . The mAP elevates slightly more when we also incorporate connections for hip joints (HT:N:S:H). This is in contrast to pose tracking where MOTA dropped slightly when we also use connections for hip joints. As before, inclusion of edges between all detections that are in the range of  frames improves the performance, while increasing the distance further  starts to deteriorate the performance. A similar trend can also been seen for the impact of different types of constraints. The removal of spatial transitivity constraints results in a drastic decrease in pose estimation accuracy. Without temporal transitivity constraints or spatio-temporal constraints the pose estimation accuracy drops  by more than  and , respectively. This once again indicates that all types of constraints are essential to obtain better pose estimation and tracking performance. 

We also compare the proposed method with the state-of-the-art approaches for multi-person pose estimation in images. Similar to \cite{Iqbal_ECCVw2016}, we use Faster R-CNN \cite{ren2015faster} as person detector, and use the provided codes for LJPA \cite{Iqbal_ECCVw2016} and CPM \cite{wei2016convolutional} to process each bounding box detection independently. We can see that person bounding box based approaches significantly underperform as compared to the proposed method. We also compare with the state-of-the-art method DeeperCut \cite{insafutdinov2016deepercut}. The approach, however, requires the rough scale of the persons during testing. For this, we use the person detections obtained from \cite{ren2015faster} to compute the scale using the median scale of all detected persons.  


Our approach achieves a better performance than all other methods. Moreover, all these approaches require an additional person detector either to get the bounding boxes \cite{Iqbal_ECCVw2016, wei2016convolutional}, or the rough scale of the persons \cite{insafutdinov2016deepercut}. Our approach on the other hand does not require a separate person detector, and we perform joint detection across different scales, while also solving the person association problem across frames. 

We also visualize how multi-person pose estimation accuracy (mAP) relates with the multi-person tracking accuracy (MOTA)  in Fig.~\ref{fig:MOTAvsMAP}. Finally, Tab.~\ref{tab:computational_analysis} provides mean and median runtimes for constructing and solving the spatio-temporal graph along with the graph size for  frames over all test videos.  

\begin{table}[htbp!]
\vspace{-1mm}
\centering
\footnotesize
    \begin{tabularx}{\columnwidth}{@{}l *5{>{\centering\arraybackslash}X}@{}}
       \hline
      	&  Runtime (sec./frame)      & \# of nodes     	& \# of spatial edges  		& \# of temp. edges    \\ \hline
Mean  	& 14.7				 &			2084		&		65535		&  12903    		\\  
Median  &  4.2   			  &			1907		&		58164	 	&  8540					\\ \hline 
\end{tabularx}
\caption{Runtime and size of the spatio-temporal graph (, HT:N:S, ), measured on a single threaded 3.3GHz CPU . \vspace{-7mm}
}
\label{tab:computational_analysis}
\end{table}


\section{Conclusion}
\vspace{-1mm}
In this paper we have presented a novel approach to simultaneously perform multi-person pose estimation and tracking. We demonstrate that the problem can be formulated as a spatio-temporal graph which can be efficiently optimized using integer linear programming. We have also presented a challenging and diverse annotated dataset with a comprehensive evaluation protocol to analyze the algorithms for multi-person pose estimation and tracking. Following the evaluation protocol, the proposed method does not make any assumptions about the number, size, or location of the persons, and can perform pose estimation and tracking in completely unconstrained videos. Moreover, the method is able to perform pose estimation and tracking under severe occlusion and truncation. Experimental results on the proposed dataset demonstrate that our method outperforms other baseline methods. \vspace{-7mm} \\ \\ 

\noindent\textbf{Acknowledgments.} The authors are thankful to Chau Minh Triet, Andreas Doering, and Zain Umer Javaid for the help with annotating the dataset. The work has been financially supported by the DFG project GA 1927/5-1 (DFG Research Unit FOR 2535 Anticipating Human Behavior) and the ERC Starting Grant ARCA (677650).

{\small   
\bibliographystyle{ieee}
\bibliography{pose_bib,refs-short,anton-ref}
}

\end{document}
