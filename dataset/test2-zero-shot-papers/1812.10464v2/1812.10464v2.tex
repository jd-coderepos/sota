

\documentclass[11pt,a4paper]{article}
\usepackage{times,latexsym}
\usepackage{url}
\usepackage[T1]{fontenc}


\usepackage[acceptedWithA]{tacl2018v2}


\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{booktabs} \usepackage{multirow}
\usepackage{amsmath}
\DeclareMathOperator{\score}{score}
\DeclareMathOperator{\nn}{NN}
\DeclareMathOperator{\margin}{margin}

\newcommand{\NbLangTrain}{93 }
\newcommand{\NbLangTrainGen}{29 }
\newcommand{\NbLangTest}{112 }
\newcommand{\NbScript}{28 }
\newcommand{\bpeN}{50k }


\newcommand{\insertTabLanguages}{
  \begin{center}
  \begin{footnotesize}
  \addtolength{\tabcolsep}{-4pt}
  \begin{tabular}{lccccccccccccccccccccccc}
    \toprule
\bf  & \bf af & \bf am & \bf ar & \bf ay & \bf az & \bf be & \bf ber & \bf bg & \bf bn & \bf br & \bf bs & \bf ca & \bf cbk & \bf cs & \bf da & \bf de \\
\bf train sent. & 67k & 88k & 8.2M & 14k & 254k & 5k & 62k & 4.9M & 913k & 29k & 4.2M & 813k & 1k & 5.5M & 7.9M & 8.7M \\
\bf enxx err. & 11.20 & 60.71 & 8.30 & n/a & 44.10 & 31.20 & 29.80 & 4.50 & 10.80 & 83.50 & 3.95 & 4.00 & 24.20 & 3.10 & 3.90 & 0.90 \\
\bf xxen err. & 9.90 & 55.36 & 7.80 & n/a & 23.90 & 36.50 & 33.70 & 5.40 & 10.00 & 84.90 & 3.11 & 4.20 & 21.70 & 3.80 & 4.00 & 1.00 \\
\bf test sent. & 1000 & 168 & 1000 & -- & 1000 & 1000 & 1000 & 1000 & 1000 & 1000 & 354 & 1000 & 1000 & 1000 & 1000 & 1000 \\
\midrule
\bf  & \bf dtp & \bf dv & \bf el & \bf en & \bf eo & \bf es & \bf et & \bf eu & \bf fi & \bf fr & \bf ga & \bf gl & \bf ha & \bf he & \bf hi & \bf hr \\
\bf train sent. & 1k & 90k & 6.5M & 2.6M & 397k & 4.8M & 5.3M & 1.2M & 7.9M & 8.8M & 732 & 349k & 127k & 4.1M & 288k & 4.0M \\
\bf enxx err. & 92.10 & n/a & 5.30 & n/a & 2.70 & 1.90 & 3.20 & 5.70 & 3.70 & 4.40 & 93.80 & 4.60 & n/a & 8.10 & 5.80 & 2.80 \\
\bf xxen err. & 93.50 & n/a & 4.80 & n/a & 2.80 & 2.10 & 3.40 & 5.00 & 3.70 & 4.30 & 95.80 & 4.40 & n/a & 7.60 & 4.80 & 2.70 \\
\bf test sent. & 1000 & -- & 1000 & -- & 1000 & 1000 & 1000 & 1000 & 1000 & 1000 & 1000 & 1000 & -- & 1000 & 1000 & 1000 \\
\midrule
\bf  & \bf hu & \bf hy & \bf ia & \bf id & \bf ie & \bf io & \bf is & \bf it & \bf ja & \bf ka & \bf kab & \bf kk & \bf km & \bf ko & \bf ku & \bf kw \\
\bf train sent. & 5.3M & 6k & 9k & 4.3M & 3k & 3k & 2.0M & 8.3M & 3.2M & 296k & 15k & 4k & 625 & 1.4M & 50k & 2k \\
\bf enxx err. & 3.90 & 59.97 & 5.40 & 5.20 & 14.70 & 17.40 & 4.40 & 4.60 & 3.90 & 60.32 & 39.10 & 80.17 & 77.01 & 10.60 & 80.24 & 91.90 \\
\bf xxen err. & 4.00 & 67.79 & 4.10 & 5.80 & 12.80 & 15.20 & 4.40 & 4.80 & 5.40 & 67.83 & 44.70 & 82.61 & 81.72 & 11.50 & 85.37 & 93.20 \\
\bf test sent. & 1000 & 742 & 1000 & 1000 & 1000 & 1000 & 1000 & 1000 & 1000 & 746 & 1000 & 575 & 722 & 1000 & 410 & 1000 \\
\midrule
\bf  & \bf kzj & \bf la & \bf lfn & \bf lt & \bf lv & \bf mg & \bf mhr & \bf mk & \bf ml & \bf mr & \bf ms & \bf my & \bf nb & \bf nds & \bf nl & \bf oc \\
\bf train sent. & 560 & 19k & 2k & 3.2M & 2.0M & 355k & 1k & 4.2M & 373k & 31k & 2.9M & 2k & 4.1M & 12k & 8.4M & 3k \\
\bf enxx err. & 91.60 & 41.60 & 35.90 & 4.10 & 4.50 & n/a & 87.70 & 5.20 & 3.35 & 9.00 & 3.40 & n/a & 1.30 & 18.60 & 3.10 & 39.20 \\
\bf xxen err. & 94.10 & 41.50 & 35.10 & 3.40 & 4.70 & n/a & 91.50 & 5.40 & 2.91 & 8.00 & 3.80 & n/a & 1.10 & 15.60 & 4.30 & 38.40 \\
\bf test sent. & 1000 & 1000 & 1000 & 1000 & 1000 & -- & 1000 & 1000 & 687 & 1000 & 1000 & -- & 1000 & 1000 & 1000 & 1000 \\
\midrule
\bf  & \bf pl & \bf ps & \bf pt & \bf ro & \bf ru & \bf sd & \bf si & \bf sk & \bf sl & \bf so & \bf sq & \bf sr & \bf sv & \bf sw & \bf ta & \bf te \\
\bf train sent. & 5.5M & 4.9M & 8.3M & 4.9M & 9.3M & 91k & 796k & 5.2M & 5.2M & 85k & 3.2M & 4.0M & 7.8M & 173k & 42k & 33k \\
\bf enxx err. & 2.00 & 7.20 & 4.70 & 2.50 & 4.90 & n/a & n/a & 3.10 & 4.50 & n/a & 1.80 & 4.30 & 3.60 & 45.64 & 31.60 & 18.38 \\
\bf xxen err. & 2.40 & 6.00 & 4.90 & 2.70 & 5.90 & n/a & n/a & 3.70 & 3.77 & n/a & 2.30 & 5.00 & 3.20 & 39.23 & 29.64 & 22.22 \\
\bf test sent. & 1000 & 1000 & 1000 & 1000 & 1000 & -- & -- & 1000 & 823 & -- & 1000 & 1000 & 1000 & 390 & 307 & 234 \\
\midrule
\bf  & \bf tg & \bf th & \bf tl & \bf tr & \bf tt & \bf ug & \bf uk & \bf ur & \bf uz & \bf vi & \bf wuu & \bf yue & \bf zh \\
\bf train sent. & 124k & 4.1M & 36k & 5.7M & 119k & 88k & 1.4M & 746k & 118k & 4.0M & 2k & 4k & 8.3M \\
\bf enxx err. & n/a & 4.93 & 47.40 & 2.30 & 72.00 & 59.90 & 5.80 & 20.00 & 82.24 & 3.40 & 25.80 & 37.00 & 4.10 \\
\bf xxen err. & n/a & 4.20 & 51.50 & 2.60 & 65.70 & 49.60 & 5.10 & 16.20 & 80.37 & 3.00 & 25.20 & 38.90 & 5.00 \\
\bf test sent. & -- & 548 & 1000 & 1000 & 1000 & 1000 & 1000 & 1000 & 428 & 1000 & 1000 & 1000 & 1000 \\
  \bottomrule
  \end{tabular}
  \end{footnotesize}
  \end{center}
}


\newcommand{\insertTabTatoebaGen}{
  \begin{center}
  \begin{footnotesize}
  \addtolength{\tabcolsep}{-4pt}
  \begin{tabular}{lcccccccccccccccccccccc}
    \toprule
\bf  & \bf ang & \bf arq & \bf arz & \bf ast & \bf awa & \bf ceb & \bf ch & \bf csb & \bf cy & \bf dsb & \bf fo & \bf fy & \bf gd & \bf gsw & \bf hsb \\
\bf enxx err. & 58.96 & 58.62 & 31.24 & 12.60 & 63.20 & 81.67 & 64.23 & 54.55 & 89.74 & 48.64 & 28.24 & 46.24 & 95.66 & 52.99 & 42.44 \\
\bf xxen err. & 65.67 & 62.46 & 31.03 & 14.96 & 64.50 & 87.00 & 77.37 & 58.89 & 93.04 & 55.32 & 28.63 & 50.29 & 96.98 & 58.12 & 48.65 \\
\bf test sent. & 134 & 911 & 477 & 127 & 231 & 600 & 137 & 253 & 575 & 479 & 262 & 173 & 829 & 117 & 483 \\
\midrule
\bf  & \bf jv & \bf max & \bf mn & \bf nn & \bf nov & \bf orv & \bf pam & \bf pms & \bf swg & \bf tk & \bf tzl & \bf war & \bf xh & \bf yi \\
\bf enxx err. & 73.66 & 48.24 & 89.55 & 13.40 & 33.07 & 68.26 & 93.10 & 50.86 & 50.00 & 75.37 & 54.81 & 84.20 & 90.85 & 93.28 \\
\bf xxen err. & 80.49 & 50.00 & 94.09 & 10.00 & 35.02 & 75.45 & 95.00 & 49.90 & 58.04 & 83.25 & 55.77 & 88.60 & 92.25 & 95.40 \\
\bf test sent. & 205 & 284 & 440 & 1000 & 257 & 835 & 1000 & 525 & 112 & 203 & 104 & 1000 & 142 & 848 \\
  \bottomrule
  \end{tabular}
  \end{footnotesize}
  \end{center}
}


\newcommand{\insertTabXNLI}{
  \begin{center}
  \begin{footnotesize}
  \addtolength{\tabcolsep}{-3.5pt}
  \begin{tabular}{llclcccccccccccccc}
    \toprule
    & & \multirow{2}{*}{EN} & & \multicolumn{14}{c}{EN  XX} \\
    \cmidrule{5-18}
    & & & & fr & es & de & el & bg & ru & tr & ar & vi & th & zh & hi & sw & ur \\
    \midrule
    \multicolumn{16}{l}{\bf Zero-Shot Transfer, one NLI system for all languages:} \10pt]
      
    \toprule
    \multicolumn{16}{l}{\bf Translate test, one English NLI system:} \10pt]
    \multicolumn{16}{l}{\bf Translate train, separate NLI systems for each language:} \
    \score(x, y) = \margin (\cos(x, y), \\
    \sum_{z \in \nn_k(x)}{\frac{\cos(x, z)}{2k}} +  \sum_{z \in \nn_k(y)}{\frac{\cos(y, z)}{2k}})

where  and  are the source and target sentences, and  denotes the  nearest neighbors of  in the other language. The paper explores different margin functions, with \textit{ratio} () yielding the best results. This notion of margin is related to CSLS \citep{conneau2018word}.

We use this method to evaluate our sentence embeddings on the BUCC mining task \citep{zweigenbaum2017overview,zweigenbaum2018overview}, using exact same hyper-parameters as \citet{artetxe2018margin}. The task consists in extracting parallel sentences from a comparable corpus between English and four foreign languages: German, French, Russian and Chinese. The dataset consists of 150K to 1.2M sentences for each language, split into a sample, training and test set, with about 2--3\% of the sentences being parallel. As shown in Table \ref{tab:results_bucc}, our system establishes a new state-of-the-art for all language pairs with the exception of English-Chinese test. We also outperform \citet{artetxe2018margin} themselves, who use two separate models covering 4 languages each. Not only are our results better, but our model also covers many more languages, so it can potentially be used to mine bitext for any combination of the \NbLangTrain languages supported.


\subsection{Tatoeba: similarity search}
\label{subsec:tatoeba}

While XNLI, MLDoc and BUCC are well established benchmarks with comparative results available, they only cover a small subset of our \NbLangTrain languages. So as to better assess the performance of our model in all these languages, we introduce a new test set of similarity search for \NbLangTest languages based on the Tatoeba corpus. The dataset consists of up to 1,000 English-aligned sentence pairs for each language. Appendix~\ref{app:tatoeba} describes how the dataset was constructed in more details. Evaluation is done by finding the nearest neighbor for each sentence in the other language according to cosine similarity and computing the error rate.

We report our results in Table~\ref{tab:languages}. Contrasting these results with those of XNLI, one would assume that similarity error rates below 5\% are indicative of strong downstream performance.\footnote{We consider the average of enxx and xxen} This is the case for 37 languages, while there are 48 languages with an error rate below 10\% and 55 with less than 20\%. There are only 15 languages with error rates above 50\%. Additional result analysis is given in Appendix \ref{app:tatoeba_analysis}.

We believe that our competitive results for many low-resource languages are indicative of the benefits of joint training, which is also supported by our ablation results in Section \ref{subsec:ablation_languages}. In relation to that, Appendix~\ref{app:unseen} reports similarity search results for \NbLangTrainGen additional languages without any training data, showing that our encoder can also generalize to unseen languages to some extent as long as it was trained on related languages.


\section{Ablation experiments}
\label{sec:ablation}

In this section, we explore different variants of our approach and study the impact on the performance for all our evaluation tasks. We report average results across all languages. For XNLI, we also report the accuracy on English.


\subsection{Encoder depth}

Table~\ref{tab:results:depth} reports the performance on the different tasks for encoders with 1, 3 or 5 layers. We were not able to achieve good convergence with deeper models. It can be seen that all tasks benefit from deeper models, in particular XNLI and Tatoeba, suggesting that a single layer BiLSTM has not enough capacity to encode so many languages.

\begin{table}[t]
  \insertTabDepth
  \caption{Impact of the depth of the BiLSTM encoder.}
  \label{tab:results:depth}
\end{table}


\subsection{Multitask learning}

Multitask learning has been shown to be helpful to learn English sentence embeddings \cite{subramanian2018learning,cer2018universal}. The most important task in this approach is arguably NLI, so we explored adding an additional NLI objective to our system with different weighting schemes. As shown in Table \ref{tab:results:nli}, the NLI objective leads to a better performance on the English NLI test set, but this comes at the cost of a worse cross-lingual transfer performance in XNLI and Tatoeba. The effect in BUCC is negligible.

\begin{table}[t]
  \insertTabNLI
  \caption{Multitask training with an NLI objective and different weightings.}
  \label{tab:results:nli}
\end{table}


\subsection{Number of training languages}
\label{subsec:ablation_languages}

So as to better understand how our architecture scales to a large amount of languages, we train a separate model on a subset of 18 evaluation languages, and compare it to our main model trained on \NbLangTrain languages. We replaced the Tatoeba corpus with the WMT 2014 test set to evaluate the multilingual similarity error rate. This covers English, Czech, French, German and Spanish, so results between both models are directly comparable. As shown in Table~\ref{tab:results:nb_langs}, the full model equals or outperforms the one covering the evaluation languages only for all tasks but MLDoc. This suggests that the joint training also yields to overall better representations.

\begin{table}[t]
  \insertTabNLangs
  \caption{Comparison between training on \NbLangTrain languages and training on the 18 evaluation languages only.}
  \label{tab:results:nb_langs}
\end{table}


\section{Conclusions}
\label{sec:conclusions}

In this paper, we propose an architecture to learn multilingual fixed-length sentence embeddings for \NbLangTrain languages. We use a single language-agnostic \mbox{BiLSTM} encoder for all languages, which is trained on publicly available parallel corpora and applied to different downstream tasks without any fine-tuning. Our experiments on cross-lingual natural language inference (XNLI), cross-lingual document classification (MLDoc), and bitext mining (BUCC) confirm the effectiveness of our approach. We also introduce a new test set of multilingual similarity search in \NbLangTest languages, and show that our approach is competitive even for low-resource languages.
To the best of our knowledge, this is the first successful exploration of general purpose massively multilingual sentence representations.

In the future, we would like to explore alternative encoder architectures like self-attention \citep{vaswani2017attention}. We would also like to explore strategies to exploit monolingual data, such as using pre-trained word embeddings, back-translation \citep{sennrich2016improving,edunov2018understanding}, or other ideas from unsupervised MT \citep{artetxe2018unsupervised,lample2018phrase}. Finally, we would like to replace our language dependant pre-processing with a language agnostic approach like SentencePiece.\footnote{\url{https://github.com/google/sentencepiece}}

Our implementation, the pre-trained encoder and the multilingual test set are freely available at \url{https://github.com/facebookresearch/LASER}.


\newpage
\bibliography{tacl2018}
\bibliographystyle{acl_natbib}
\newpage


\appendix


\section{Training data}
\label{app:data}

Our training data consists of the combination of the following publicly available parallel corpora:
\begin{itemize}
    \item \textbf{Europarl}: 21 European languages. The size varies from 400k to 2M sentences depending on the language pair.
    \item \textbf{United Nations}: We use the first two million sentences in Arabic, Russian and Chinese.
    \item \textbf{OpenSubtitles2018:} A parallel corpus of movie subtitles in 57 languages. The corpus size varies from a few thousand sentences to more than 50 million. We keep at most 2 million entries for each language pair.
    \item \textbf{Global Voices:} News stories from the Global Voices website (38 languages). This is a rather small corpus with less than 100k sentence in most of the languages.
    \item \textbf{Tanzil:} Quran translations in 42 languages, average size of 135k sentences. The style and vocabulary is very different from news texts.
    \item \textbf{Tatoeba:} A community supported collection of English sentences and translations into more than 300 languages. We use this corpus to extract a separate test set of up to 1,000 sentences (see Appendix~\ref{app:tatoeba}). For languages with more than 1,000 entries, we use the remaining ones for training.
\end{itemize}

Using all these corpora would provide parallel data for more languages, but we decided to keep \NbLangTrain languages after discarding several constructed languages with little practical use (Klingon, Kotava, Lojban, Toki Pona and Volap√ºk). In our preliminary experiments, we observed that the domain of the training data played a key role in the performance of our sentence embeddings. Some tasks (BUCC, MLDoc) tend to perform better when the encoder is trained on long and formal sentences, whereas other tasks (XNLI, Tatoeba) benefit from training on shorter and more informal sentences. So as to obtain a good balance, we used at most two million sentences from OpenSubtitles, although more data is available for some languages. The size of the available training data varies largely for the considered languages (see Table~\ref{tab:languages}). This favours high-resource languages when the joint BPE vocabulary is created and the training of the joint encoder. In this work, we did not try to counter this effect by over-sampling low-resource languages.


\section{XNLI results for all language combinations}
\label{app:xnli_cross}

\begin{table*}[t!]
  \insertTabXNLIcross
  \caption{XNLI test accuracies for our approach when the premise and hypothesis are in different languages.}
  \label{tab:xnli_cross}
\end{table*}

\begin{table*}[t!]
  \insertTabTatoebaGen
  \caption{Performance on the Tatoeba test set for languages for which we have no training data.}
  \label{tab:languagesGen}
\end{table*}

Table~\ref{tab:xnli_cross} reports the accuracies of our system on the XNLI test set when the premises and hypothesis are in a different language. The numbers in the diagonal correspond to the main results reported in Table \ref{tab:results_xnli}. Our approach obtains strong results when combining different languages. We do not have evidence that distant languages perform considerably worse. Instead, the combined performance seems mostly bounded by the accuracy of the language that performs worst when used alone. For instance, Greek-Russian achieves very similar results to Bulgarian-Russian, two Slavic languages. Similarly, combing French with Chinese, two totally different languages, is only 1.5 points worse than French-Spanish, two very close languages.


\section{Tatoeba: dataset}
\label{app:tatoeba}

Tatoeba\footnote{\url{https://tatoeba.org/eng/}} is an open collection of English sentences and high-quality translations into more than 300 languages. The number of available translations is updated every Saturday. We downloaded the snapshot on November 19th 2018 and performed the following processing:
    1) removal of sentences containing ``@'' or ``http'', as emails and web addresses are not language specific;
    2) removal of sentences with less than three words, as they usually have little semantic information;
    3) removal of sentences that appear multiple times, either in the source or the target.

After filtering, we created test sets of up to 1,000 aligned sentences with English. This amount is available for 72 languages. Limiting the number of sentences to 500, we increase the coverage to 86 languages, and 112 languages with 100 parallel sentences. It should be stressed that, in general, the English sentences are not the same for different languages, so error rates are not directly comparable across languages.

\section{Tatoeba: result analysis}
\label{app:tatoeba_analysis}

In this section, we provide some analysis on the results given in Table \ref{tab:languages}. We have 48 languages with an error rate below 10\% and 55 with less than 20\%, respectively (English included). The languages with less than 20\% error belong to 20 different families and use 12 different scripts, and include 6 languages for which we have only small amounts of bitexts (less than 400k), namely Esperanto, Galician, Hindi, Interlingua, Malayam and Marathi, which presumably benefit from the joint training with other related languages.

Overall, we observe low similarity error rates on the Indo-Aryan languages, namely Hindi, Bengali, Marathi and Urdu. The performance on Berber languages (``ber'' and ``kab'') is remarkable, although we have less than 100k sentences to train them. This is a typical example of languages which are spoken by several millions of people, but for which the amount of written resources is very limited.  It is quite unlikely that we would be able to train a good sentence embedding with language specific corpora only, showing the benefit of joint training on many languages.

Only 15 languages have similarity error rates above 50\%. Four of them are low-resource languages with their own script and which are alone in their family (Amharic, Armenian, Khmer and Georgian), making it difficult to benefit from joint training. In any case, it is still remarkable that a language like Khmer performs much better than random with only 625 training examples. There are also several Turkic languages (Kazakh, Tatar, Uighur and Uzbek) and Celtic languages (Breton and Cornish) with high error rates. We plan to further investigate its cause and possible solutions in the future.


\section{Tatoeba: results for unseen languages}
\label{app:unseen}

We extend our experiments to \NbLangTrainGen languages without any training data (see Table~\ref{tab:languagesGen}). Many of them are recognized minority languages spoken in specific regions (e.g. Asturian, Faroese, Frisian, Kashubian, North Moluccan Malay, Piemontese, Swabian or Sorbian). All share some similarities, at various degrees, with other major languages that we cover, but also differ by their own grammar or specific vocabulary.  This enables our encoder to perform reasonably well, even if it did not see these languages during training.

\end{document}