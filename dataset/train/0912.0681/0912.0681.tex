
\documentclass[11pt]{article}
\usepackage{algorithm2e}
\usepackage{framed}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{url}
\usepackage{tabularx}

\addtolength{\textwidth}{1.4in}
\addtolength{\oddsidemargin}{-0.5in}
\addtolength{\evensidemargin}{-0.5in}
\addtolength{\topmargin}{-1.25in}
\addtolength{\textheight}{1.7in}



\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{conjecture}{Conjecture}
\newtheorem{claim}{Claim}
\newtheorem{observation}{Observation}
\newtheorem{fact}{Fact}
\newenvironment{proof}{\noindent {\em Proof:}}{\\\hspace*{\fill}\mbox{}}
\newcommand{\argmin}{\text{argmin}}

\usepackage{nicefrac}
\newcommand{\flatfrac}[2]{#1/#2}
\newcommand{\ffrac}{\flatfrac}
\newcommand{\nfrac}{\nicefrac}
\newcommand{\SDP}{{\sf SDP}\xspace}
\newcommand{\sdp}{{\sf SDP}}
\newcommand{\SPECTRAL}{{\sf SPECTRAL}\xspace}
\newcommand{\prog}{\mathsf{PROG}}
\newcommand{\Tr}{{\rm Tr}}
\newcommand{\grad}{\nabla}
\newcommand{\iprod}[1]{\langle #1\rangle}

\newcommand{\defeq}{\stackrel{\textup{def}}{=}}

\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\Norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\cutratio}[1]{\Phi(#1)}
\newcommand{\pagerank}[2]{\rho_{#1,#2}}
\newcommand{\edgenorm}[1]{\norm{#1}_D}
\newcommand{\edgesnorm}[1]{\snorm{#1}_D}
\newcommand{\normed}[2]{\frac{#1}{\edgenorm{#2}}}
\newcommand{\Supp}[1]{\text{Supp}(#1)}
\newcommand{\supp}{\mathrm{supp}}
\newcommand{\poly}{\mathrm{poly}}
\newcommand{\vol}{\mathrm{vol}}
\newcommand{\dist}{\mathrm{dist}}



\begin{document}



\title{A Local Spectral Method for Graphs: \\ 
with Applications to Improving Graph Partitions \\
and Exploring Data Graphs Locally}

\author{
Michael W. Mahoney
\thanks{
Department of Mathematics,
Stanford University,
Stanford, CA 94305.
{\tt mmahoney@cs.stanford.edu}.
}
\and
Lorenzo Orecchia
\thanks{
Computer Science Division, 
UC Berkeley,
Berkeley, CA, 94720.
{\tt orecchia@eecs.berkeley.edu}.
}
\and 
Nisheeth K. Vishnoi
\thanks{
Microsoft Research,
Bangalore, India.
{\tt nisheeth.vishnoi@gmail.com}.
}
}

\date{}
\maketitle




\vspace{3mm}
\begin{abstract}
The second eigenvalue of the Laplacian matrix and its associated eigenvector 
are fundamental features of an undirected graph, and as such they have found 
widespread use in scientific computing, machine learning, and data analysis.  
In many applications, however, graphs that arise have several \emph{local} 
regions of interest, and the second eigenvector will typically fail to 
provide information fine-tuned to each local region.  
In this paper, we introduce a locally-biased analogue of the second 
eigenvector, and we demonstrate its usefulness at highlighting local 
properties of data graphs in a semi-supervised manner.
To do so, we first view the second eigenvector as the solution to a 
constrained optimization problem, and we incorporate the local information 
as an additional constraint; 
we then characterize the optimal solution to this new problem and show that 
it can be interpreted as a generalization of a Personalized PageRank vector; 
and finally, as a consequence, we show that the solution can be computed in 
nearly-linear time. 
In addition, we show that this locally-biased vector can be used to compute 
an approximation to the best partition \emph{near} an input seed set in a 
manner analogous to the way in which the second eigenvector of the Laplacian 
can be used to obtain an approximation to the best partition in the entire 
input graph.
Such a primitive is useful for identifying and refining clusters locally, as 
it allows us to focus on a local region of interest in a semi-supervised 
manner.
Finally, we provide a detailed empirical evaluation of our method by showing 
how it can applied to finding locally-biased sparse cuts around an input 
vertex seed set in social and information networks.
\end{abstract}
\vspace{3mm}


\section{Introduction}
\label{sxn:intro}

Spectral methods are popular in machine learning, data analysis, and applied 
mathematics due to  their strong underlying theory and their good 
performance in a wide range of applications.
In the study of undirected graphs, in particular, spectral techniques play 
an important role, as many fundamental structural properties of a graph 
depend directly on spectral quantities associated with matrices representing 
the graph.
Two fundamental objects of study in this area are the second smallest 
eigenvalue of the graph Laplacian and its associated eigenvector.
These quantities determine many features of the graph, including the 
behavior of random walks and the presence of sparse cuts.
This relationship between the graph structure and an easily-computable 
quantity has been exploited in data clustering, community detection, image 
segmentation, parallel computing, and many other applications.

A potential drawback of using the second eigenvalue and its associated 
eigenvector is that they are inherently \emph{global} quantities, and thus 
they may not be sensitive to very \emph{local} information. 
For instance, a sparse cut in a graph may be poorly correlated with the 
second eigenvector (and even with all the eigenvectors of the Laplacian) 
and thus invisible to a method based only on eigenvector analysis. 
Similarly, based on domain knowledge one might have information about a 
specific target region in the graph, in which case one might be interested 
in finding clusters only near this prespecified local region, \emph{e.g.}, 
in a semi-supervised manner; but this local region might be essentially 
invisible to a method that uses only global eigenvectors.
For these and related reasons, standard global spectral techniques can have 
substantial difficulties in semi-supervised settings, where the goal is to 
learn more about a locally-biased target region of the graph.

In this paper, we provide a methodology to construct a locally-biased 
analogue of the second eigenvalue and its associated eigenvector, and we 
demonstrate both theoretically and empirically that this localized vector 
inherits many of the good properties of the global second eigenvector. 
Our approach is inspired by viewing the second eigenvector as the optimum of 
a constrained global quadratic optimization program. 
To model the localization step, we modify this program by adding a natural 
locality constraint.
This locality constraint requires that any feasible solution have sufficient 
correlation with the target region, which we assume is given as input in 
the form of a set of nodes or a distribution over vertices. 
The resulting optimization problem, which we name \textsf{LocalSpectral} and 
which is displayed in Figure~\ref{fig:spectral},
is the main object of our work. 

The main advantage of our formulation is that an optimal solution to 
\textsf{LocalSpectral} captures many of the same structural properties as 
the global eigenvector, except in a locally-biased setting. 
For example, as with the global optimization program, our locally-biased 
optimization program has an intuitive geometric interpretation.
Similarly, as with the global eigenvector, an optimal solution to 
\textsf{LocalSpectral} is efficiently computable. 
To show this, we characterize the optimal solutions of 
\textsf{LocalSpectral} and show that such a solution can be constructed in 
nearly-linear time by solving a system of linear equations.
In applications where the eigenvectors of the graph are pre-computed and 
only a small number of them are needed to describe the data, the optimal 
solution to our program can be obtained by performing a small number of 
inner product computations. 
Finally, the optimal solution to \textsf{LocalSpectral} can be used to 
derive bounds on the mixing time of random walks that start near the local 
target region as well as on the existence of sparse cuts near the 
locally-biased target region. 
In particular, it lower bounds the conductance of cuts as a function of how 
well-correlated they are with the seed vector.
This will allow us to exploit the analogy between global eigenvectors and 
our localized analogue to design an algorithm for discovering sparse cuts 
near an input seed set of vertices. 

In order to illustrate the empirical behavior of our method, we will 
describe its performance on the problem of finding locally-biased sparse 
cuts in real data graphs.
Subsequent to the dissemination of the initial technical report version of 
this paper, our methodology was applied to the problem of finding, given a 
small number of ``ground truth'' labels that correspond to known segments 
in an image, the segments in which those labels reside~\cite{MVM11}.
This computer vision application will be discussed briefly.
Then, we will describe in detail how our algorithm for discovering sparse 
cuts near an input seed set of vertices may be applied to the problem of 
exploring data graphs locally and to identifying locally-biased clusters and 
communities in a more difficult-to-visualize social network application.
In addition to illustrating the performance of the method in a practical 
application related to the one that initially motivated 
this work~\cite{LLDM08_communities_CONF,LLDM09_communities_IM,LLM10_communities_CONF}, 
this social graph application will illustrate how the various ``knobs'' of our method can 
be used in practice to explore the structure of data graphs in a locally-biased 
manner.

Recent theoretical work has focused on using spectral ideas to find good 
clusters nearby an input seed set of 
nodes~\cite{Spielman:2004,andersen06local,chung07_fourproofs}. 
These methods are based on running a number of local random walks around the 
seed set and using the resulting distributions to extract information about 
clusters in the graph.
Recent empirical work has used Personalized PageRank, a particular variant 
of a local random walk, to characterize very finely the clustering and 
community structure in a wide range of very large social and information 
networks~\cite{andersen06seed,LLDM08_communities_CONF,LLDM09_communities_IM,LLM10_communities_CONF}. 
In contrast with previous methods, our local spectral method is the first to 
be derived in a direct way from an explicit optimization problem inspired 
by the global spectral problem.  
Interestingly, our characterization also shows that optimal solutions to 
\textsf{LocalSpectral} are  generalizations of Personalized PageRank, 
providing an additional insight to why local random walk methods work well 
in practice.

In the next section, we will describe relevant background and notation; and 
then, in Section~\ref{sxn:optimize}, we will present our formulation of a 
locally-biased spectral optimization program, the solution of which will 
provide a locally-biased analogue of the second eigenvector of the graph 
Laplacian. 
Then, in Section~\ref{sxn:partition} we will describe how our method may be 
applied to identifying and refining locally-biased partitions in a graph; and
in Section~\ref{sxn:empirical} we will provide a detailed empirical 
evaluation of our algorithm.
Finally, in Section~\ref{sxn:discussion}, we will conclude with a discussion 
of our results in a broader~context.



\section{Background and Notation.} 
\label{sxn:background}

Let  be a connected undirected graph with  vertices and 
 edges, in which edge  has weight 
For a set of vertices  in a graph, the \emph{volume of } 
is , in which case the \emph{volume of the
graph } is . 
In the following,  will denote the 
adjacency matrix of , while  will denote 
the diagonal degree matrix of , \emph{i.e.},  
, the weighted degree of vertex .
The Laplacian of  is defined as . 
(This is also called the combinatorial Laplacian, in which case the
normalized Laplacian of  is .)

The Laplacian is the symmetric matrix having quadratic form 
, for . 
This implies that  is positive semidefinite and that the all-one vector 
 is the eigenvector corresponding to the smallest 
eigenvalue .
For a symmetric matrix , we will use  to denote that it is 
positive semi-definite.
Moreover, given two symmetric matrices  and , the expression 
 will mean .
Further, for two  matrices  and , we let  
denote . 
Finally, for a matrix  let  denote its (uniquely defined) 
Moore-Penrose pseudoinverse.

For two vectors , and the degree matrix  for a 
graph , we define the degree-weighted inner product as
.
Given a subset of vertices , we denote by  the indicator 
vector of  in  and by  the vector in  
having all entries set equal to .
We consider the following definition of the complete graph  on the 
vertex set : .
Note that this is not the standard complete graph, but a weighted version of 
it, where the weights depend on .
With this scaling we have .
Hence, the Laplacian of the complete graph defined in this manner becomes
.

In this paper, the \emph{conductance  of a cut } is 
.
A sparse cut, also called a good-conductance partition, is one for which 
 is small.
The \emph{conductance of the graph } is then
.
Note that the conductance of a set , or equivalently a cut , is often 
defined as . 
This notion is equivalent to that , in that the value 
 thereby obtained for the conductance of the graph  differs by 
no more than a factor of  times the constant , depending on 
which notion we use for the conductance of a~set.  



\section{The \textsf{LocalSpectral} Optimization Program}
\label{sxn:optimize}

In this section, we introduce the local spectral optimization program 
 as a strengthening of the usual
global spectral program .
To do so, we will augment  with a locality constraint
of the form , for a seed vector  and a correlation
parameter .
Both these programs are homogeneous quadratic programs, with optimization 
variable the vector , and thus any solution vector  
is essentially equivalent to  for the purpose of these optimizations. 
Hence, in the following we do not differentiate between  and , and we 
assume a suitable direction is chosen in each instance.


\subsection{Motivation for the Program}
\label{sxn:optimize-program}

Recall that the second eigenvalue  of the Laplacian  can 
be viewed as the optimum of the standard optimization problem 
 described in Figure~\ref{fig:spectral}. 
In matrix terminology, the corresponding optimal solution  is a 
generalized eigenvector of  with respect to . 
For our purposes, however, it is best to consider the geometric meaning of 
this optimization formulation. 
To do so, suppose we are operating in a vector space , where 
the th dimension is stretched by a factor of  so that the natural 
identity operator is  and the inner product between two vectors  and 
 is given by .
In this representation,  is seeking the vector 
 that is orthogonal to the all-one vector, lies on the 
unit sphere, and minimizes the Laplacian quadratic form. 
Note that such an optimum  may lie anywhere on the unit sphere.  

\begin{figure}
\begin{minipage}{0.5\textwidth}

\end{minipage}
\begin{minipage}{0.5\textwidth}

\end{minipage}
\caption{Global and local spectral optimization programs.
Left: The usual spectral program . 
Right: Our new locally-biased spectral program 
\textsf{LocalSpectral}.  
In both cases, the optimization variable is the vector .
}
\label{fig:spectral}
\end{figure}





Our goal here is to modify  to incorporate a bias 
towards a target region which we assume is given to us as an input vector .
We will assume (without loss of generality) that  is properly normalized 
and orthogonalized so that  and . 
While  can be a general unit vector orthogonal to , it may be helpful 
to think of  as the indicator vector of one or more vertices in , 
corresponding to the target region of the graph.
We obtain  from  
by requiring that a feasible solution also have a sufficiently large 
correlation with the vector . 
This is achieved by the addition of the constraint 
, which ensures that the projection of  onto 
the direction  is at least  in absolute value, where the 
parameter  is also an input parameter ranging between  and . 
Thus, we would like the solution to be well-connected with or to lie near 
the \emph{seed} vector . 
In particular, as displayed pictorially in Figure~\ref{fig:sphere},  must 
lie within the spherical cap centered at  that contains all vectors at an 
angle of at most  from . 
Thus, higher values of  demand a higher correlation with  and, 
hence, a stronger localization. 
Note that in the limit , the spherical cap constituting the 
feasible region of the program is guaranteed to include  and 
 is equivalent to . 
In the rest of this paper, we refer to  as the \emph{seed vector} and to 
 as the \emph{correlation parameter} for a given 
 optimization problem.  
Moreover, we denote the objective value of the program
 by the number . 

\begin{figure}[h]
\begin{center}
   \includegraphics[ scale=.17]{figcap.pdf}
   \end{center}
\caption{(Best seen in color.) Pictorial representation of the feasible 
regions of the optimization programs  and 
 that are defined in 
Figure~\ref{fig:spectral}.  See the text for a discussion.}
\label{fig:sphere}	
\end{figure}


\subsection{Characterization of the Optimal Solutions of \textsf{LocalSpectral}}
\label{sxn:optimize-theory}

Our first theorem is a characterization of the optimal solutions of 
\textsf{LocalSpectral}. 
Although \textsf{LocalSpectral} is a non-convex program (as, of course, is 
\textsf{Spectral}), the following theorem states that solutions to it can be 
expressed as the solution to a system of linear equations which has a 
natural interpretation.
The proof of this theorem (which may be found in 
Section~\ref{sxn:optimize-proofs-pagerank}) will involve a relaxation of the 
non-convex program \textsf{LocalSpectral} to a convex semidefinite program (SDP), 
\emph{i.e.}, the variables in the optimization program will be distributions 
over vectors rather than the vectors themselves.
For the statement of this theorem, recall that  denotes the (uniquely 
defined) Moore-Penrose pseudoinverse of the matrix .

\begin{theorem}[Solution Characterization]
\label{thm:pagerank}
Let  be a seed vector such that , 
, and , where  is the second 
generalized eigenvector of  with respect to .
In addition, let  be a correlation parameter, and let
 be an optimal solution to \textsf{LocalSpectral}.
Then, there exists some  and a 
 such that 

\end{theorem}

\noindent
There are several parameters (such as , , , and ) in 
the statement of Theorem~\ref{thm:pagerank}, and understanding their 
relationship is important:
 and  are the parameters of the program;  is a normalization 
factor that rescales the norm of the solution vector to be  (and that can 
be computed in linear time, given the solution vector); and  is 
implicitly defined by , , and~. 
The correct setting of  ensures that 
 \emph{i.e.}, that  is found 
exactly on the boundary of the feasible region.
At this point, it is important to notice the behavior of  and 
 as  changes. 
As  goes to ,  tends to  and  
approaches ;
conversely, as  goes to ,   goes to  and 
 tends towards , the global eigenvector.
We will discuss how to compute  and , given a specific 
, in Section~\ref{sxn:optimize-comp}.

Finally, we should note that there is a close connection between the 
solution vector  and the popular PageRank procedure. 
Recall that PageRank refers to a method to determine a global rank or global 
notion of importance for a node in a graph such as the web that is based on 
the link structure of the graph~\cite{BP98,LM04,berkhin05_pagerank}. 
There have been several extensions to the basic PageRank concept, including 
Topic-Sensitive PageRank~\cite{haveliwala03_topicpr}
and Personalized PageRank~\cite{JW03}. 
In the same way that PageRank can be viewed as a way to express the quality 
of a web page over the entire web, Personalized PageRank expresses a 
link-based measure of page quality around user-selected pages.
In particular, given a vector  and a 
\emph{teleportation} constant , the Personalized PageRank vector 
can be written as
~\cite{andersen06local}.
By setting   the optimal solution to \textsf{LocalSpectral} is proved to 
be a generalization of Personalized PageRank. 
In particular, this means that for high values of the correlation parameter 
, for which the corresponding  in Theorem~\ref{thm:pagerank}
is negative, the optimal solution to \textsf{LocalSpectral} takes the form 
of a Personalized PageRank vector. 
On the other hand, when  the optimal solution to 
\textsf{LocalSpectral} provides a smooth way of transitioning from the 
Personalized PageRank vector to the global second eigenvector~.


\subsection{Computation of the Optimal Solutions of \textsf{LocalSpectral}}
\label{sxn:optimize-comp}

In this section, we discuss how to compute efficiently an optimal solution
for , for a fixed choice of the 
parameters , , and . 
The following theorem is our main result.

\begin{theorem}[Solution Computation]
\label{thm:comp}
For any , a solution to \textsf{LocalSpectral} 
of value at most  can be 
computed in time 
 
using the Conjugate Gradient Method~\cite{GVL96}. 
Alternatively, such a solution can be computed in time 
 using the Spielman-Teng 
linear-equation solver~\cite{Spielman:2004}. 
\end{theorem}
\begin{proof}
By Theorem~\ref{thm:pagerank}, we know that the optimal solution  must be a unit-scaled version of 
 for an appropriate choice of 
Notice that, given a fixed  the task of computing  is equivalent to solving the system of linear equations
 for the unknown  This operation can be performed, up to accuracy  in time  
using the Conjugate Gradient Method, or  in time 
 using the Spielman-Teng linear-equation 
solver.
To find the correct setting of  it suffices to perform a binary search over the possible values of  in the interval  until  is sufficiently close to  
\end{proof}

\noindent
We should note that, depending on the application, other methods of 
computing a solution to  might be more 
appropriate.
In particular, if an eigenvector decomposition of  has been 
pre-computed, as is the case in certain machine learning and data analysis 
applications, then this computation can be modified as follows.
Given an eigenvector decomposition of  as 
, then
 must take the form

for the same choice of  and , as in Theorem~\ref{thm:pagerank}. 
Hence, given the eigenvector decomposition, each guess  of the 
binary search can be computed by expanding the above series, which requires 
a linear number of inner product computations.
While this may yield a worse running time than Theorem~\ref{thm:comp} in 
the worst case, in the case that the graph is well-approximated by a small 
number  of dominant eigenvectors, then the computation is reduced to 
only  straightforward inner product computations.


\subsection{Proof of Theorem~\ref{thm:pagerank}}
\label{sxn:optimize-proofs-pagerank}

We start with an outline of the proof.
Although the program \textsf{LocalSpectral}  is \emph{not} 
convex, it can be relaxed to the convex semidefinite program 
 of Figure~\ref{fig:sdp}. 
Then, one can observe that strong duality holds for this SDP relaxation. 
Using strong duality and the related complementary slackness conditions, one 
can argue that the primal  has a rank one unique optimal 
solution under the conditions of the theorem.
This implies that the optimal solution of  is the same 
as the optimal solution of \textsf{LocalSpectral}.
Moreover, combining this fact with the complementary slackness condition 
obtained from the dual  of Figure~\ref{fig:sdp}, one 
can derive that the optimal rank one solution is of the form promised by 
Theorem~\ref{thm:pagerank}.


\begin{figure}
\begin{minipage}{0.5\textwidth}
 
\end{minipage}
\begin{minipage}{.25\textwidth}

\end{minipage}
\caption{
Left:  Primal SDP relaxation of \textsf{LocalSpectral}:  
;  
for this primal, the optimization variable is  
such that  is symmetric and positive semidefinite.
Right: Dual SDP relaxation of \textsf{LocalSpectral}: 
;
for this dual, the optimization variables are . 
Recall that 
}
\label{fig:sdp}
\end{figure} 


Before proceeding with the details of the proof, we pause to make several 
points that should help to clarify our approach.
\begin{itemize}
\item
First, since it may seem to some readers to be unnecessarily complex to relax 
\textsf{LocalSpectral} as an SDP, we emphasize that the motivation for 
relaxing it in this way is that we would like to prove 
Theorem~\ref{thm:pagerank}.
To prove this theorem, we must understand the form of the optimal solutions 
to the non-convex program \textsf{LocalSpectral}. 
Thus, in order to overcome the non-convexity, we relax \textsf{LocalSpectral} 
to  (of Figure~\ref{fig:sdp}) by ``lifting'' the 
rank- condition implicit in \textsf{LocalSpectral}. 
Then, strong duality applies; and it implies a set of sufficient optimality 
conditions. 
By combining these conditions, we will be able to establish that an optimal 
solution  to  has rank , \emph{i.e.}, it has the 
form  for some vector ; and thus it yields an optimal solution 
to \textsf{LocalSpectral}, \emph{i.e.}, the vector~. 
\item
Second, in general, the value of a relaxation like  
may be strictly less than that of the original program 
(\textsf{LocalSpectral}, in this case). 
Our characterization and proof will imply that the relaxation is tight, 
\emph{i.e.}, that the optimum of  equals that of 
\textsf{LocalSpectral}.
The reason is that one can find a rank- optimal solution to 
, which then yields an optimal solution of the same 
value for \textsf{LocalSpectral}. 
Note that this also implies that strong duality holds for the non-convex 
\textsf{LocalSpectral}, although this observation is not needed for our 
proof.
\end{itemize}
\noindent
That is, although it may be possible to prove Theorem~\ref{thm:pagerank} in 
some other way that does not involve SDPs, we 
chose this proof since it is simple and intuitive and correct; and we 
note that Appendix B in the textbook of Boyd and Vandenberghe~\cite{Boyd04} 
proves a similar statement by the same SDP-based approach.




Returning to the details of the proof, we will 
proceed to prove the theorem by establishing a sequence of claims.
First, consider  and its dual  (as 
shown in Figure~\ref{fig:sdp}). 
The following claim uses the fact that, given  for 
, and for any matrix , we 
have that . 
In particular, , for any graph , and 
.

\begin{claim}
The primal  is a relaxation of the vector program 
\textsf{LocalSpectral}.
\end{claim}
\begin{proof} Consider a vector  that is a feasible solution to 
\textsf{LocalSpectral}, and note that  is a feasible 
solution to .
\end{proof}

\noindent
Next, we establish the strong duality of .
(Note that the feasibility conditions and
complementary slackness conditions
stated below
may not suffice to 
establish the optimality, in the absence of this claim; 
hence, without this claim, we could not prove the subsequent claims, which 
are needed to prove the theorem.)

\begin{claim}
Strong duality holds between  and .
\end{claim}
\begin{proof} Since  is convex, it suffices to verify that Slater's 
constraint qualification condition~\cite{Boyd04} is true for this primal SDP.
Consider .  
Then, .
\end{proof}

\noindent
Next, we use this result to establish the following two claims.
In particular, strong duality allows us to prove the following claim showing 
the KKT-conditions, \emph{i.e.}, the feasibility conditions and 
complementary slackness conditions stated below, suffice to establish
 optimality.

\begin{claim}   The following feasibility and complementary slackness conditions are 
sufficient for a primal-dual pair 
 to be an optimal solution.
The feasibility conditions are:

and the complementary slackness conditions are:

\end{claim}
\begin{proof}
This follows from the convexity of  and Slater's 
condition~\cite{Boyd04}.
\end{proof}


\begin{claim}
\label{claim:rankone}
These feasibility and complementary slackness conditions, coupled with the 
assumptions of the theorem, imply that   must be 
rank  and 
\end{claim}
\begin{proof}  Plugging in  in Equation~\eqref{F3}, we obtain that 

But  and  Hence,  
Suppose  As  it must be the case that  Hence, by Equation~\eqref{C3}, we must have  which implies that  {\em i.e.},  the optimum for \textsf{LocalSpectral} is the global eigenvector . This corresponds to a choice of  and  tending to infinity.

Otherwise, we may assume that  Hence, since  is connected and   has rank exactly  and kernel parallel to the vector  
From the complementary slackness condition \eqref{C3} we can deduce that the image of  is in the kernel of  
If  we have that  is a rank one matrix and, since  it reduces the rank of  by one precisely. If  then  must be  which is not possible  if  is feasible.
Hence, the rank of  must be exactly  As we may assume that  is in the kernel of ,  must be of rank one. 
This proves the claim. 
\end{proof}




\noindent
Now we complete the proof of the theorem.
From the claim it follows that,  where  satisfies the equation 

From the second complementary slackness condition, 
Equation~\eqref{C2},
and the fact that  we obtain that 
 
Thus, 
 as required. 


\section{Application to Partitioning Graphs Locally}
\label{sxn:partition}

In this section, we describe the application of \textsf{LocalSpectral} to 
finding locally-biased partitions in a graph, \emph{i.e.}, to finding 
sparse cuts around an input seed vertex set in the graph.
For simplicity, in this part of the paper, we let the instance graph  be 
unweighted. 


\subsection{Background on Global Spectral Algorithms for Partitioning Graphs}
\label{sxn:partition-background}

We start with a brief review of global spectral graph partitioning.
Recall that the basic global graph partitioning problem is: given as input 
a graph , find a set of nodes  to solve 

Spectral methods approximate the solution to this intractable global problem 
by solving the relaxed problem  presented in 
Figure~\ref{fig:spectral}.
To understand this optimization problem, recall that  counts the 
number of edges crossing the cut and that  encodes a variance 
constraint; thus, the goal of  is to minimize the 
number of edges crossing the cut subject to a given variance. 
Recall that for , we let  be a vector 
which is  for vertices in  and  otherwise.
Then for  a cut , if we define the vector 
, 
it can be checked that  satisfies the constraints of \textsf{Spectral} 
and has objective value . 
Thus, . 

Hence,  is a relaxation of the minimum conductance 
problem. 
Moreover, this program is a good relaxation in that a good cut can be 
recovered by considering a truncation, \emph{i.e.}, a sweep cut, of the
vector  that is the optimal solution to .
(That is, \emph{e.g.}, consider each of the  cuts defined by the vector 
, and return the cut with minimum conductance value.)
This is captured by the following celebrated result often referred to as 
Cheeger's Inequality.
\begin{theorem}[Cheeger's Inequality]
\label{thm:cheeger1}
For a connected graph , 
.
\end{theorem}

\noindent
Although there are many proofs known for this theorem (see, 
\emph{e.g.},~\cite{Chung:1997}), a particularly interesting proof was 
found by Mihail~\cite{Mihail}; this proof involves rounding any \emph{test 
vector} (rather than just the optimal vector), and it achieves the same 
guarantee as Cheeger's Inequality. 
\begin{theorem}[Sweep Cut Rounding]
\label{thm:cheeger2}
Let  be a vector such that . Then there is a  for which 
the set of vertices  
satisfies .
\end{theorem}
It is the form of Cheeger's Inequality provided by Theorem~\ref{thm:cheeger2}
that we will use below.


\subsection{Locally-Biased Spectral Graph Partitioning}
\label{sxn:partition-local}

Here, we will exploit the analogy between \textsf{Spectral} and 
\textsf{LocalSpectral} by applying the global approach just outlined to the 
following locally-biased graph partitioning problem: 
given as input a graph , an input node , and a positive integer
, find a set of nodes  achieving

That is, the problem is to 
find the best conductance set of nodes of volume no greater 
than  that contains the input node . 

As a first step, we show that we can choose the seed set and correlation parameters  and  such 
that  is a relaxation for this 
locally-biased graph partitioning problem.
\begin{lemma}
\label{lem:relaxation}
For , \textsf{LocalSpectral} is a relaxation 
of the problem of finding a minimum conductance cut  in  which 
contains the vertex  and is of volume at most~.  
In particular, .
\end{lemma}
\begin{proof}
If we let  in 
\textsf{LocalSpectral}, 
then , , and 
.
Moreover, we have that 
,
which establishes the lemma. 
\end{proof}

\noindent
Next, we can apply Theorem~\ref{thm:cheeger2} to the optimal solution for 
 and obtain a cut  whose 
conductance is quadratically close to the optimal value 
. 
By Lemma~\ref{lem:relaxation}, this implies that 
.
This argument proves the following theorem.
\begin{theorem}[Finding a Cut]
\label{thm:cut} 
Given an unweighted graph , a vertex  and a positive 
integer , we can find a cut in  of conductance at most 
 by computing a sweep cut of the optimal vector for 
. 
Moreover, this algorithm runs in nearly-linear time in the size of the graph.
\end{theorem}

\noindent
That is, this theorem states that we can perform a sweet cut over the vector 
that is the solution to  in order 
to obtain a locally-biased partition; and that this partition comes with 
quality-of-approximation guarantees analogous to that provided for the 
global problem  by Cheeger's inequality.

Our final theorem shows that the optimal value of \textsf{LocalSpectral}
also provides a lower bound on the conductance of \emph{other cuts}, as a function 
of how well-correlated they are with the input seed vector.
In particular, when the seed vector corresponds to a cut , this result
allows us to lower bound the conductance of an arbitrary cut , in terms
of the correlation between  and .
The proof of this theorem 
also uses in an essential manner the
duality properties that were used in the 
proof of Theorem~\ref{thm:pagerank}.

\begin{theorem}[Cut Improvement]
\label{thm:improve}
Let  be a  graph and  be such that
 where  is the degree matrix of 
In addition, let  be a correlation parameter.
Then, for all sets  such that
, we have that

\end{theorem}
\begin{proof}
It follows from Theorem \ref{thm:pagerank} that  is the 
same as the optimal value of  which, by strong 
duality, is the same as the optimal value of . 
Let  be the optimal dual values to 
 
Then, from the dual feasibility constraint 

it follows that 

Notice that since , it follows that 
.
Further, since  we obtain, if  that  

If on the other hand,  then

Note that strong duality was used here. 
\end{proof}

\noindent
Thus, although the relaxation guarantees of Lemma~\ref{lem:relaxation} only 
hold when the seed set is a single vertex, we can use 
Theorem~\ref{thm:improve} to consider the following problem: given a graph 
 and a cut  in the graph, find a cut of minimum conductance 
in  which is well-correlated with  or certify that there is none. 
Although one can imagine many applications of this primitive, the main 
application that motivated this work was to explore  clusters nearby or 
around a given \emph{seed set} of nodes in data graphs.  
This will be illustrated in our empirical evaluation 
in~Section~\ref{sxn:empirical}. 


\subsection{Our Geometric Notion of Correlation Between Cuts}
\label{sxn:partition-geometric}


Here we pause to make explicit the geometric notion of correlation between 
cuts (or partitions, or sets of nodes) that is used by \textsf{LocalSpectral}, 
and that has already been used in various guises in previous sections.
Given a cut  in a graph , a natural vector in 
 to associate with it is its characteristic vector, in which
case the correlation between a cut  and another cut 
 can be captured by the inner product of the characteristic 
vectors of the two cuts. 
A somewhat more refined vector to associate with a cut is the vector 
obtained after removing from the characteristic vector its projection along 
the all-ones vector.  
In that case, again, a notion of correlation is related to the inner product 
of two such vectors for two cuts. 
More precisely, given a set of nodes , or equivalently a cut 
, one can define the unit vector ~as

That is, 

which is exactly the vector defined in Section~\ref{sxn:partition-background}.
It is easy to check that this is well defined: one can replace  by 
 and the correlation remains the same with any other set. 
Moreover, several observations are immediate.
First, defined this way, it immediately follows that 
 and that . 
Thus,  for , where we denote by 
 the set of vectors ;
and  can be seen as an appropriately normalized version of the vector 
consisting of the uniform distribution over  minus the uniform 
distribution over .\footnote{Notice also that .  Thus, since 
we only consider quadratic functions of  we 
can consider both  and  to be representative vectors for 
the cut  }
Second, one can introduce the following measure of correlation between two 
sets of nodes, or equivalently between two cuts, say a cut  
and a cut :

The proofs of the following simple facts regarding  are omitted:
;
 if and only if  or ;
; and
.
Third, although we have described this notion of geometric correlation in 
terms of vectors of the form  that represent 
partitions , this correlation is clearly well-defined for other 
vectors  for which there is not such a simple 
interpretation in terms of cuts.
Indeed, in Section~\ref{sxn:optimize} we considered the case that  was 
an arbitrary vector in , while in the first part of 
Section~\ref{sxn:partition-local} we considered the case that  was the 
seed set of a single node.
In our empirical evaluation in Section~\ref{sxn:empirical}, we will 
consider both of these cases as well as the case that  encodes the 
correlation with cuts consisting of multiple nodes.


\section{Empirical Evaluation}
\label{sxn:empirical} 

In this section, we provide an empirical evaluation of \textsf{LocalSpectral} 
by illustrating its use at finding and evaluating locally-biased 
low-conductance cuts, \emph{i.e.}, sparse cuts or good clusters, around an input seed set of nodes in 
a data graph.
We start with a brief discussion of a very recent and pictorially-compelling 
application of our method to a computer vision problem; and then we discuss 
in detail how our method can be applied to identify clusters and communities 
in a more heterogeneous and more difficult-to-visualize social network 
application.

\subsection{Semi-Supervised Image Segmentation}

Subsequent to the initial dissemination of the technical report version of 
this paper, Maji, Vishnoi, and Malik~\cite{MVM11} applied our methodology to 
the problem of finding locally-biased cuts in a computer vision application.
Recall that image segmentation is the problem of partitioning a digital image 
into segments corresponding to significant objects and areas in the image. 
A standard approach consists in converting the image data into a similarity 
graph over the the pixels and applying a graph partitioning algorithm to 
identify relevant segments. 
In particular, spectral methods have been popular in this area since the 
work of Shi and Malik~\cite{ShiMalik00_NCut}, which used the second 
eigenvector of the graph to approximate the so-called normalized cut 
(which, recall, is an objective measure for image segmentation that is 
practically equivalent to conductance). 
However, a difficulty in applying the normalized cut method is that in many 
cases global eigenvectors may fail to capture important local segments of 
the image.
The reason for this is that they aggressively optimize a global objective 
function and thus they tend to combine multiple segments together; 
this is illustrated pictorially in the first row of Figure~\ref{fig:imseg}.

This difficulty can be overcome in a semi-supervised scenario by using 
our \textsf{LocalSpectral} method. 
Specifically, one often has a small number of ``ground truth'' labels that 
correspond to known segments, and one is interested in extracting and 
refining the segments in which those labels reside.
In this case, if one considers an input seed corresponding to a small number 
of pixels within a target object, then \textsf{LocalSpectral} can recover 
the corresponding segment with high precision.
This is illustrated in the second row of Figure~\ref{fig:imseg}. 
This computer vision application of our methodology was motivated by a 
preliminary version of this paper, and it was described in detail and evaluated against competing 
algorithms by Maji, Vishnoi, and Malik~\cite{MVM11}. 
In particular, they show that \textsf{LocalSpectral} achieves a performance superior to 
that of other semi-supervised segmentation algorithms~\cite{YS01,EOK07}; and 
they also show how \textsf{LocalSpectral} can be incorporated in an 
unsupervised segmentation pipeline by using as input seed distributions 
obtained by an object-detector algorithm~\cite{bmbm10}.

\begin{figure}[h]
\begin{center}
\includegraphics[ scale=1.20]{jaguar-cropped.pdf}
   \end{center}
\caption{The first row shows the input image and the three smallest eigenvectors of the Laplacian of the corresponding similarity graph computed using the intervening contour cue~\cite{MAFM08}.  Note that no sweep cut of these eigenvectors reveals the leopard.  The second row shows the results of \textsf{LocalSpectral} with a setting of  with the seed pixels highlighted by crosshairs.  Note how one can to recover the leopard by using a seed vector representing a set of only 4 pixels.  In addition, note how the first seed pixel allows us to capture the head of the animal, while the other seeds help reveal other parts of its body. }
\label{fig:imseg}	
\end{figure}


\subsection{Detecting Communities in Social Networks}

Finding local clusters and meaningful locally-biased communities is also of 
interest in the analysis of large social and information networks.  
A standard approach to finding clusters and communities in many network 
analysis applications is to formalize the idea of a good community with an 
``edge counting'' metric such as conductance or modularity and then to use a 
spectral relaxation to optimize it
approximately~\cite{newman2006finding,newman2006_ModularityPNAS}.
For many very large social and information networks, however, there simply do
not exist good large global clusters, but there do exist small
meaningful local clusters that may be thought of as being nearby 
prespecified seed sets of
nodes~\cite{LLDM08_communities_CONF,LLDM09_communities_IM,LLM10_communities_CONF}.
In these cases, a local version of the global spectral partitioning
problem is of interest, as was shown by Leskovec, Lang, and
Mahoney~\cite{LLM10_communities_CONF}.
Typical networks are very large and, due to their expander-like properties, 
are not 
easily-visualizable~\cite{LLDM08_communities_CONF,LLDM09_communities_IM}.
Thus, in order to illustrate the empirical behavior of our
\textsf{LocalSpectral} methodology in a ``real'' network application related 
to the one that motivated this 
work~\cite{LLDM08_communities_CONF,LLDM09_communities_IM,LLM10_communities_CONF}, 
we examined a small ``coauthorship network'' of scientists.
This network was previously used by Newman~\cite{newman2006finding} to study 
community structure in small social and information networks.


\begin{figure}[h] 
\begin{center}
\includegraphics[viewport= 0 0 1450 818, clip=true, scale=.27]{image/networknew.pdf} 
\end{center}
\caption{[Best viewed in color.]  The coauthorship network of Newman~\cite{newman2006finding}. This layout was obtained in the Pajek~\cite{Pajek03} visualization software, using the Kamada-Kawai method~\cite{Kamada89} on each component of a partition provided by \textsf{LocalCut} and tiling the layouts at the end.  Boxes show the two main global components of the network, which are displayed separately in subsequent~figures.}
\label{fig:network}
\end{figure}


The corresponding graph  is illustrated in Figure~\ref{fig:network} and
consists of  nodes and  edges, where each node represents an 
author and each unweighted edge represents a coauthorship relationship.
The spectral gap ; and a sweep cut of the eigenvector 
corresponding to this second eigenvalue yields the globally-optimal spectral 
cut separating the graph into two well-balanced partitions, corresponding to 
the left half and the right half of the network, as shown in 
Figure~\ref{fig:network}.
Our main empirical observations, described in detail in the remainder of 
this section, are the following.
\begin{itemize}
\item
First, we show how varying the teleportation parameter allows us to detect 
low-conductance cuts of different volumes that are locally-biased around a 
prespecified seed vertex; and how this information, aggregated over 
multiple choices of teleportation, can improve our understanding of the 
network structure in the neighborhood of the seed.
\item
Second, we demonstrate the more general usefulness of our definition of a 
\emph{generalized} Personalized PageRank vector (where the  
parameter in Eqn.~(\ref{eqn:xstar}) can be ) 
by displaying specific instances in which that vector is more effective than 
the usual Personalized PageRank (where only positive teleportation 
probabilities are allowed and thus where  must be negative).
We do this by detecting a wider range of low-conductance cuts at a given 
volume and by interpolating smoothly between very locally-biased solutions 
to \textsf{LocalSpectral} and the global solution provided by the 
\textsf{Spectral} program.
\item
Third, we demonstrate how our method can find low-conductance cuts that are 
well-correlated to more general input seed vectors by demonstrating an 
application to the detection of sparse peripheral regions, \emph{e.g.}, 
regions of the network that are well-correlated with low-degree nodes.
This suggests that our method may find applications in leveraging 
feature data, which are often associated with the vertices of a 
data graph, to find interesting and meaningful cuts.
\end{itemize}
We emphasize that the goal of this empirical evaluation is to illustrate 
how our proposed methodology can be applied in real applications; and thus 
we work with a relatively easy-to-visualize example of a small social graph.
This will allow us to illustrate how the ``knobs'' of our proposed method 
can be used in practice.
In particular, the goal is not to illustrate that our method or heuristic 
variants of it or other spectral-based methods scale to much larger 
graphs---this latter fact is by now 
well-established~\cite{andersen06seed,LLDM08_communities_CONF,LLDM09_communities_IM,LLM10_communities_CONF}.



\subsubsection{Algorithm Description and Implementation}

We refer to our cut-finding algorithm, which will be used to guide our 
empirical study of finding and evaluating cuts around an input seed set of 
nodes and which is a straightforward extension of the algorithm referred 
to in  Theorem~\ref{thm:cut}, as \textsf{LocalCut}.
In addition to the graph, the input parameters for \textsf{LocalCut} are a 
seed vector  (\emph{e.g.}, corresponding to a single vertex ), a 
teleportation parameter , and (optionally) a size factor .
Then, \textsf{LocalCut} performs the following steps.
\begin{itemize}
\item
First, compute the vector  of Eqn.~(\ref{eqn:xstar}) with seed  
and teleportation . 
\item
Second, either
perform a sweep of the vector , \emph{e.g.}, consider each of the
 cuts defined by the vector and return the the minimum conductance cut 
found along the sweep;
or 
consider only sweep cuts along the vector  of volume at most 
, 
where , 
that contain the input vertex , and return the 
minimum conductance cut among such cuts.
\end{itemize}

\noindent
By Theorem~\ref{thm:pagerank}, the vector computed in the first step of 
\textsf{LocalCut}, , is an optimal solution to 
\textsf{LocalSpectral} for some choice of 
.
(Indeed, by fixing the above parameters, the  parameter is fixed 
implicitly.)
Then, by Theorem~\ref{thm:cut}, when the vector  is rounded 
(to, \emph{e.g.}, ) by performing the sweep cut, provably-good 
approximations are guaranteed. 
In addition, when the seed vector corresponds to a single vertex , it
follows from Lemma~\ref{lem:relaxation} that  yields a lower 
bound to the conductance of cuts that contain  and have less than a 
certain volume .

Although the full sweep-cut rounding does not give a specific guarantee on 
the volume of the output cut, empirically we have found that it is often 
possible to find small low-conductance cuts in the range dictated by .
Thus, in our empirical evaluation, we also consider volume-constrained sweep 
cuts (which departs slightly from the theory but can be useful in practice).
That is, we also introduce a new input 
parameter, a \textit{size factor} , that regulates the maximum volume 
of the sweep cuts considered when  represents a single vertex.
In this case, \textsf{LocalCut} does not consider all  cuts defined by 
the vector , but instead it considers only sweep cuts of volume 
at most  that contain the vertex .
(Note that it is a simple consequence of our optimization characterization 
that the optimal vector has sweep cuts of volume at most  
containing .)
This new input parameter turns out to be extremely useful in exploring cuts 
at different sizes, as it neglects sweep cuts of low conductance at large 
volume and allows us to pick out more local cuts around the seed vertex.

In our first two sets of experiments, summarized in 
Sections~\ref{sec:teleport} and~\ref{sec:size}, we used single-vertex seed 
vectors, and we analyzed the effects of varying the parameters  and 
, as a function of the location of the seed vertex in the input graph.
In the last set of experiments, presented in Section~\ref{sec:multi}, we 
considered more general seed vectors, including both seed vectors that 
correspond to multiple nodes, \emph{i.e.}, to cuts or partitions in the 
graph, as well as seed vectors that do not have an obvious interpretation in 
terms of input cuts.
We implemented our code in a combination of MATLAB and C++, solving linear 
systems using the Stabilized Biconjugate Gradient Method~\cite{bicg92} 
provided in MATLAB 2006b. 
On this particular coauthorship network, and on a Dell PowerEdge 1950 
machine with 2.33 GHz and 16GB of RAM, the algorithm ran in less than a few 
seconds.



\subsubsection{Varying the Teleportation Parameter}
\label{sec:teleport}

Here, we evaluate the effect of varying the teleportation parameter 
, where recall .
Since it is known that large social and information networks are quite 
heterogeneous and exhibit a very strong ``nested core-periphery'' 
structure~\cite{LLDM08_communities_CONF,LLDM09_communities_IM,LLM10_communities_CONF}, 
we perform this evaluation by considering the behavior of \textsf{LocalCut} 
when applied to three types of seed nodes, examples of which are the 
highlighted vertices in Figure~\ref{fig:network}.
These three nodes were chosen to represent three different types of nodes
seen in larger networks: 
a \textit{periphery-like node}, which belongs to a lower-degree and less 
expander-like part of the graph, and which tends to be surrounded by 
lower-conductance cuts of small volume; 
a \textit{core-like node}, which belongs to a denser and higher-conductance 
or more expander-like part of the graph; and 
an \textit{intermediate node}, which belongs to a regime between the 
core-like and the periphery-like regions.

For each of the three representative seed nodes, we executed  runs of 
\textsf{LocalCut} with  and  varying by  increments. 
Figure~\ref{fig:teleportation} displays, for each of these three seeds, a 
plot of the conductance as a function of volume of the cuts found by each 
run of \textsf{LocalCut}. 
We refer to this type of plot as a \textit{local profile plot} since it is 
a specialization of the \textit{network community profile plot}~\cite{LLDM08_communities_CONF,LLDM09_communities_IM,LLM10_communities_CONF} 
to cuts around the specified seed vertex.
In addition, Figure~\ref{fig:teleportation} also plots several other 
quantities of interest:
first, the volume and conductance of the theoretical lower bound yielded by 
each run; 
second, the volume and conductance of the cuts defined by the shortest-path 
balls (in squares and numbered according to the length of the path) around 
each seed (which should and do provide a sanity-check upper bound); 
third, next to each of the plots, we present a color-coded image of 
representative cuts detected by \textsf{LocalCut}; and 
fourth, for each of the cuts illustrated on the left, a color-coded triangle 
and the numerical value of  is shown on the~right. 


\newcommand{\imscale}{.13}
\newcommand{\plscale}{.19}
\renewcommand{\imscale}{.2}
\renewcommand{\plscale}{.27}

\begin{figure}[] 
\subfigure[Selected cuts and profile plot for the \emph{core-like node}.]{
\includegraphics[viewport= 50 -90 818 818, clip=true, scale=\imscale]{image/core.pdf} 
\label{subfig:tc}
\hspace{.7in}
\includegraphics[ scale=\plscale]{plot/plot45.pdf}
}
\subfigure[Selected cuts and profiles plot for the \emph{intermediate node}.]
{
\includegraphics[viewport= 650 -90 1420 818, clip=true, scale=\imscale]{image/intermediate.pdf} 
\label{subfig:tb}
\hspace{.7in}
\includegraphics[ scale=\plscale]{plot/plot70.pdf}
}
\subfigure[Selected cuts and profile plot for the \emph{periphery-like node}.]{
\includegraphics[viewport= 50 -90 818 818, clip=true, scale=\imscale]{image/periphery.pdf}
\label{subfig:ta}
\hspace{.7in}
\includegraphics[ scale=\plscale]{plot/plot60.pdf}
}
\caption{[Best viewed in color.]  
Selected cuts and local profile plots for varying . 
The cuts on the left are displayed by assigning to each vertex a color 
corresponding to the smallest selected cut in which the vertex was included. 
Smaller cuts are darker, larger cuts are lighter; and the seed vertex is 
shown slightly larger. 
Each profile plot on the right shows results from  runs of 
\textsf{LocalCut}, with  and  decreasing in  increments 
starting at .
For each color-coded triangle, corresponding to a cut on the left,  
is also listed.
}
\label{fig:teleportation}
\end{figure}


Several points about the behavior of the \textsf{LocalCut} algorithm as a 
function of the location of the input seed node and that are illustrated in 
Figure~\ref{fig:teleportation} are worth emphasizing.
\begin{itemize}
\item
First, for the core-like node, whose profile plot is shown in 
Figure~\ref{subfig:tc}, the volume of the output cuts grows relatively 
smoothly as  is increased (\emph{i.e.}, as  is decreased). 
For small , \emph{e.g.},  or , the 
output cuts are forced to be small and hence display high conductance, as 
the region around the node is somewhat expander-like. 
By decreasing the teleportation, the conductance progressively decreases, 
as the rounding starts  to hit nodes in peripheral regions, whose inclusion only improves 
conductance (since it increases the cut volume without adding many 
additional cut edges).
In this case, this phenomena ends at  when a cut 
of conductance value close to that of the global optimum is~found. 
(After that, larger and slightly better conductance cuts can still be found, 
but, as discussed below, they require .)
\item
Second, a similar interpretation applies to the profile plot of the 
intermediate node, as shown in Figure~\ref{subfig:tb}. 
Here, however, the global component of the network containing the seed has 
smaller volume, around , and a very low conductance (again, requiring 
).
Thus, the profile plot \emph{jumps} from this cut to the much larger 
eigenvector sweep cut, as will be discussed below.
\item
Third, a more extreme case is that of the periphery-like node, whose profile 
plot is displayed in Figure~\ref{subfig:ta}. 
In this case, an initial increase in  does \emph{not} yield larger cuts.
This vertex is contained in a small-volume cut of low conductance, and thus
diffusion-based methods get ``stuck'' on the small side of the cut.
The only cuts of lower conductance in the network are those separating the 
global components, which can only be accessed when . 
Hence, the teleportation must be greatly decreased before the algorithm 
starts outputting cuts at larger volumes.
(As an aside, this behavior is also often seen with so-called ``whiskers'' 
in much larger social and information 
networks~\cite{LLDM08_communities_CONF,LLDM09_communities_IM,LLM10_communities_CONF}.) 
\end{itemize}


\noindent
In addition, several general points that are illustrated in 
Figure~\ref{fig:teleportation} are worth emphasizing about the 
behavior of our algorithm.
\begin{itemize}
\item
First, \textsf{LocalCut} found low-conductance cuts of different volumes 
around each seed vertex, outperforming the shortest-path algorithm (as it 
should) by a factor of roughly  in most cases. 
However, the results of \textsf{LocalCut} still lie away from the lower 
bound, which is also a factor of roughly  smaller at most volumes. 
\item
Second, consider the range of the teleportation parameter necessary for the 
\textsf{LocalCut} algorithm to discover the well-balanced globally-optimal 
spectral partition.
In all three cases, it was necessary to make  positive 
(\emph{i.e.},  negative) to detect the well-balanced global 
spectral cut.
Importantly, however, the quantitative details depend strongly on whether the
seed is core-like, intermediate, or periphery-like.
That is, by \emph{formally} allowing ``negative teleportation'' 
probabilities, which correspond to , the use of 
\emph{generalized} Personalized PageRank vectors as an exploratory tool is 
much stronger than the usual Personalized 
PageRank~\cite{andersen06local,andersen06seed}, in that it permits one to 
find a larger class of clusters, up to and including the global partition 
found by the solution to the global \textsf{Spectral} program.
Relatedly, it provides a smooth interpolation between Personalized PageRank
and the second eigenvector of the graph. 
Indeed, for , \textsf{LocalCut} 
outputs the same cut as the eigenvector sweep cut for all three seeds.
\item
Third, recall that, given a teleportation parameter , the rounding 
step selects the cut of smallest conductance along the sweep cut of the 
solution vector.
(Alternatively, if volume-constrained sweeps are considered, then it selects 
the cut of smallest conductance among sweep cuts of volume at most 
, where  is the lower bound obtained from the 
optimization program.)
In either case, increasing  can lead \textsf{LocalCut} 
to pick out larger cuts, but it does not \emph{guarantee} this will happen. 
In particular, due to the local topology of the graph, in many instances 
there may \emph{not} be a way of slightly increasing the volume of a cut 
while slightly decreasing its conductance. 
In those cases, \textsf{LocalCut} may output the same small sweep cut for a 
range of teleportation parameters until a much larger, much lower-conductance 
cut is then found.
The presence such horizontal and vertical \textit{jumps} in the local 
profile plot conveys useful information about the structure of the network 
in the neighborhood of the seed at different size scales, illustrating that 
the practice follows the theory quite well.
\end{itemize}


\subsubsection{Varying the Output-Size Parameter}
\label{sec:size}

Here, we evaluate the effect of varying the size factor , for a fixed 
choice of teleportation parameter .
(In the previous section,  was fixed at  and  was varied.)
We have observed that varying , like varying , tends to have the 
effect of producing low-conductance cuts of different volumes around the 
seed vertex. 
Moreover, it is possible to obtain low-conductance large-volume cuts, even 
at lower values of the teleportation parameter, by increasing  to a 
sufficiently large value.
This is illustrated in Figure~\ref{fig:sizecore}, which shows the result of 
varying  with the core-like node as the seed and . 
Figure~\ref{subfig:tc} illustrated that when  this setting only yielded 
a cut of volume close to  (see the red triangle with );
but the yellow crosses in Figure~\ref{fig:sizecore} illustrate that by 
allowing larger values of , better conductance cuts of larger volume can 
be~obtained. 

\begin{figure}[h]
\centering
\includegraphics[viewport= 50 -90 818 818, clip=true, scale=\imscale]{image/sizecore.pdf}
\hspace{.7in}
\includegraphics[ scale=\plscale]{plot/plotsize45.pdf}
\caption{[Best viewed in color.]  Selected cuts and local profile plots for varying  with the core-like node as the seed. The cuts are displayed by assigning to each vertex a color corresponding to the smallest selected cut in which the vertex was included. Smaller cuts are darker, larger are lighter. The seed vertex is shown larger. The profile plot shows results from  runs of \textsf{LocalCut}, with varying  and  . }
\label{fig:sizecore}
\end{figure}

While many of these cuts tend to have conductance slightly worse than the 
best found by varying the teleportation parameter, the observation that cuts 
of a wide range of volumes can be obtained with a single value of  
leaves open the possibility that there exists a single choice of 
teleportation parameter  that produces good low-conductance cuts at 
all volumes simply by varying~. 
(This would allow us to only solve a single optimization problem and still 
find cuts of different volumes.)
To address (and rule out) this possibility, we selected three choices of 
the teleportation parameter for each of the three seed nodes, and then we 
let  vary. 
The resulting output cuts for the core-like node as the seed are plotted 
(in blue, green, and yellow) in Figure~\ref{fig:sizecore}. 
(The plots for the other seeds are similar and are not displayed.)
Clearly, no single teleportation setting dominates the others: in 
particular, at volume  the lowest-conductance cut was produced with 
; at volume  it was produced with ; and at 
volume  with it was produced with . 
The highest choice of  performed marginally better overall, 
recording lowest conductance cuts at both small and large volumes. 
That being said, the results of all three settings roughly track each~other, 
and cuts of a wide range of volumes were able to be obtained by varying the 
size parameter .

These and other empirical results suggest that the best results are 
achieved when we vary both the teleportation parameter and the size factor. 
In addition, the use of multiple teleportation choices have the side-effect 
advantage of yielding multiple lower bounds at different volumes.


\subsubsection{Multiple Seeds and Correlation}
\label{sec:multi}

Here, we evaluate the behavior of \textsf{LocalCut} on more general seed 
vectors. 
We consider two examples---for the first example, there is an interpretation 
as a cut or partition consisting of multiple nodes; while the second example 
does not have any immediate interpretation in terms of cuts or partitions.

\begin{figure}[h] 
\subfigure[Seed set of four seed nodes.]{
\includegraphics[viewport= 50 0 818 818, clip=true, scale=\imscale]{image/mseedsperiphery.pdf}
\label{fig:mseeds0a}
}
\subfigure[A more general seed vector.]{
\includegraphics[viewport= 50 0 1250 818, clip=true, scale=\imscale]{image/mseeds-degbias.pdf}
\label{fig:mseeds0b}
}
\caption{[Best viewed in color.]  
Multiple seeds and correlation.
\ref{fig:mseeds0a} shows selected cuts for varying  with the seed 
vector corresponding to a subset of  vertices lying in the periphery-like
region of the network. 
\ref{fig:mseeds0b} shows selected cuts for varying  with the seed 
vertex equal to a normalized version of the degree vector. 
In both cases, the cuts are displayed by assigning to each vertex a color 
corresponding to the smallest selected cut in which the vertex was included. 
Smaller cuts are darker, larger are lighter.  
}
\label{fig:mseeds0}
\end{figure}




In our first example, we consider a seed vector representing a subset of 
four nodes, located in different peripheral branches of the left half of 
the global partition of the the network: 
see the four slightly larger (and darker) vertices in Figure~\ref{fig:mseeds0a}.
This is of interest since, depending on the size-scale at which one is 
interested, such sets of nodes can be thought of as either ``nearby'' or ``far 
apart.''
For example, when viewing the entire graph of  nodes, these 
four nodes are all close, in that they are all on the left side of the 
optimal global spectral partition; but when considering smaller clusters 
such as well-connected sets of  or  nodes, these four nodes are 
much farther apart. 
In Figure~\ref{fig:mseeds0a}, we display a selection of the cuts found by 
varying the teleportation, with . 
The smaller cuts tend to contain the branches in which each seed node is 
found, while larger cuts start to incorporate nearby branches. 
Not shown in the color-coding is that the optimal global spectral 
partition is eventually recovered.
Identifying peripheral areas that are well-separated from the rest of the 
graph is a useful primitive in studying the structure of social 
networks~\cite{LLDM08_communities_CONF,LLDM09_communities_IM,LLM10_communities_CONF}; 
and thus, this shows how \textsf{LocalCut} may be used in this context, when 
some periphery-like seed nodes of the graph are known.

In our second example, we consider a seed vector that represents a feature 
vector on the vertices but that does not have an interpretation in terms of 
cuts. 
In particular, we consider a seed vector that is a normalized version of the 
degree distribution vector. 
Since nodes that are periphery-like tend to have lower degree than those that
are core-like~\cite{LLDM08_communities_CONF,LLDM09_communities_IM,LLM10_communities_CONF},
this choice of seed vector biases \textsf{LocalCut} 
towards cuts that are well-correlated with periphery-like and low-degree 
vertices.
A selection of the cuts found on this seed vector when varying the 
teleportation with  is displayed in Figure~\ref{fig:mseeds0b}. 
These cuts partition the network naturally into three well-separated 
regions: a sparser periphery-like region in darker colors, a lighter-colored 
intermediate region, and a white dense core-like region, where higher-degree 
vertices tend to lie. 
Clearly, this approach could be applied more generally to find 
low-conductance cuts that are well-correlated with a known feature of the 
node vector.




\section{Discussion}
\label{sxn:discussion}

In this final section, we provide a brief discussion of our results in a 
broader context.

\paragraph{Relationship to local graph partitioning.} 
Recent theoretical work has focused on using spectral ideas to find good 
clusters nearby an input seed set of 
nodes~\cite{Spielman:2004,andersen06local,chung07_fourproofs}. 
In particular, local graph partitioning---roughly, the problem of finding a 
low-conductance cut in a graph in time depending only on the volume of the 
output cut---was introduced by Spielman and Teng~\cite{Spielman:2004}. 
They used random walk based methods; and they used this as a subroutine to 
give a nearly linear-time algorithm for outputting balanced cuts that match 
the Cheeger Inequality up to polylog factors. 
In our language, a local graph partitioning algorithm would start a random 
walk at a seed node, truncating the walk after a suitably chosen number of 
steps, and outputting the nodes visited by the walk. 
This result was improved  by Andersen, Chung and Lang~\cite{andersen06local} 
by performing a truncated Personalized PageRank computation. 
These and subsequent papers building on them were motivated by local graph 
partitioning~\cite{chung07_fourproofs}, but they do not address the problem 
of discovering cuts near general seed vectors, as do we, or of generalizing 
the second eigenvector of the Laplacian. 
Moreover, these approaches are more operationally-defined, while ours is 
axiomatic and optimization-based. 


\paragraph{Relationship to empirical work on community structure.}
Recent empirical work has used Personalized PageRank, a particular variant 
of a local random walk, to characterize very finely the clustering and 
community structure in a wide range of very large social and information 
networks~\cite{andersen06seed,LLDM08_communities_CONF,LLDM09_communities_IM,LLM10_communities_CONF}. 
In particular, Andersen and Lang used local spectral methods to identify 
communities in certain informatics graphs using an input set of nodes as a 
seed set~\cite{andersen06seed}.
Subsequently, Leskovec, Lang, Dasgupta, and Mahoney used related 
methods to characterize the small-scale and large-scale clustering and 
community structure in a wide range of large social and information 
networks~\cite{LLDM08_communities_CONF,LLDM09_communities_IM,LLM10_communities_CONF}. 
Our optimization program and empirical results suggest that this line of 
work can be extended to ask in a theoretically principled manner much more 
refined questions about graph structure near prespecified seed vectors. 


\paragraph{Relationship to cut-improvement algorithms.}
Many recently-popular algorithms for finding minimum-conductance cuts, 
such as those in~\cite{khandekar06_partitioning,OSVV08}, use as a crucial 
building block a primitive that takes as input a cut  and 
attempts to find a lower-conductance cut that is {\em well correlated} 
with . 
This primitive is referred to as a \emph{cut-improvement 
algorithm}~\cite{kevin04mqi,andersen08soda}, as its original purpose was 
limited to post-processing cuts output by other algorithms. 
Recently, cut-improvement algorithms have also been used to find low 
conductance cuts in specific regions of large graphs~\cite{LLM10_communities_CONF}. 
Given a notion of correlation between cuts, cut-improvement algorithms 
typically produce approximation guarantees of the following form:
for any cut  that is -correlated with the input 
cut, the cut output by the algorithm has conductance upper-bounded by a 
function of the conductance of  and .
This line of work has typically used flow-based techniques.
For example, Gallo, Grigoriadis and Tarjan~\cite{Gallo:1989} were the first 
to show that one can find a subset of an input set  with 
minimum conductance in polynomial time. 
Similarly, Lang and Rao~\cite{kevin04mqi} implement a closely related 
algorithm and demonstrate its effectiveness at refining cuts output by other 
methods. 
Finally, Andersen and Lang~\cite{andersen08soda} give a more general 
algorithm that uses a small number of single-commodity maximum-flows to find 
low-conductance cuts not only inside the input subset , but among all 
cuts which are well-correlated with . 
Viewed from this perspective, our work may be seen as a spectral analogue 
of these flow-based techniques, since Theorem~\ref{thm:improve} provides 
lower bounds on the conductance of other cuts as a function of how 
well-correlated they are with the seed vector.


\paragraph{Alternate interpretation of our main optimization program.}
There are a few interesting ways to view our local optimization problem 
of Figure~\ref{fig:spectral} which would like to point out here. 
Recall that \textsf{LocalSpectral} may be interpreted as augmenting the 
standard spectral optimization program with a constraint that the output 
cut be well-correlated with the input seed set.
To understand this program from the perspective of the dual, recall that 
the dual of \textsf{LocalSpectral} is given by the following.

where .
Alternatively, by subtracting the second constraint of 
\textsf{LocalSpectral} 
from the first constraint, it follows that

It can be shown that

where  is the -weighted complete graph on the vertex set .
Thus,
\textsf{LocalSpectral}
is clearly equivalent to

The dual of 
this program 
is given by the following.

From the perspective of this dual, this 
can be viewed as ``embedding'' a combination of a complete graph  and a
weighted combination of complete graphs on the sets  and , 
\emph{i.e.},  and .
Depending on the value of , the latter terms clearly discourage cuts 
that substantially cut into  or , thus encouraging partitions
that are well-correlated with the input cut .


\paragraph{Bounding the size of the output cut.}
Readers familiar with the spectral method may recall that given a graph with a small balanced cut, it is not possible, in general, to guarantee that the sweep cut procedure of Theorem~\ref{thm:cheeger2}  applied to the optimal of \textsf{Spectral} outputs  a balanced cut. One may have to iterate several times before one gets a balanced cut.  Our setting, building up on the spectral method,  also suffers from this;  we cannot hope, in general,  to bound the size of the output cut  (which is a sweep cut) in terms of the correlation parameter  
This was the reason for considering volume-constrained sweep cuts in our
empirical evaluation.


\begin{thebibliography}{10}

\bibitem{andersen06local}
R.~Andersen, F.R.K. Chung, and K.~Lang.
\newblock Local graph partitioning using {PageRank} vectors.
\newblock In {\em FOCS '06: Proceedings of the 47th Annual IEEE Symposium on
  Foundations of Computer Science}, pages 475--486, 2006.

\bibitem{andersen06seed}
R.~Andersen and K.~Lang.
\newblock Communities from seed sets.
\newblock In {\em WWW '06: Proceedings of the 15th International Conference on
  World Wide Web}, pages 223--232, 2006.

\bibitem{andersen08soda}
R.~Andersen and K.~Lang.
\newblock An algorithm for improving graph partitions.
\newblock In {\em SODA '08: Proceedings of the 19th ACM-SIAM Symposium on
  Discrete algorithms}, pages 651--660, 2008.

\bibitem{Pajek03}
V.~Batagelj and A.~Mrvar.
\newblock Pajek---analysis and visualization of large networks.
\newblock In {\em Proceedings of Graph Drawing}, pages 477--478, 2001.

\bibitem{berkhin05_pagerank}
P.~Berkhin.
\newblock A survey on {PageRank} computing.
\newblock {\em Internet Mathematics}, 2(1):73--120, 2005.

\bibitem{bmbm10}
L.~Bourdev, S.~Maji, T.~Brox, and J.~Malik.
\newblock Detecting people using mutually consistent poselet activations.
\newblock In {\em Proceedings of the 11th European Conference on Computer
  Vision}, 2010.

\bibitem{Boyd04}
S.~Boyd and L.~Vandenberghe.
\newblock {\em Convex Optimization}.
\newblock Cambridge University Press, Cambridge, UK, 2004.

\bibitem{BP98}
S.~Brin and L.~Page.
\newblock The anatomy of a large-scale hypertextual web search engine.
\newblock In {\em Proceedings of the 7th International Conference on World Wide
  Web}, pages 107--117, 1998.

\bibitem{Chung:1997}
F.R.K. Chung.
\newblock {\em Spectral graph theory}, volume~92 of {\em CBMS Regional
  Conference Series in Mathematics}.
\newblock American Mathematical Society, 1997.

\bibitem{chung07_fourproofs}
F.R.K Chung.
\newblock Four proofs of {C}heeger inequality and graph partition algorithms.
\newblock In {\em Proceedings of ICCM}, 2007.

\bibitem{EOK07}
A.~P. Eriksson, C.~Olsson, and F.~Kahl.
\newblock Normalized cuts revisited: A reformulation for segmentation with
  linear grouping constraints.
\newblock In {\em Proceedings of th 11th International Conference on Computer
  Vision}, pages 1--8, 2007.

\bibitem{Gallo:1989}
G.~Gallo, M.D. Grigoriadis, and R.E. Tarjan.
\newblock A fast parametric maximum flow algorithm and applications.
\newblock {\em SIAM Journal on Computing}, 18(1):30--55, 1989.

\bibitem{GVL96}
G.H. Golub and C.F.~Van Loan.
\newblock {\em Matrix Computations}.
\newblock Johns Hopkins University Press, Baltimore, 1996.

\bibitem{haveliwala03_topicpr}
T.H. Haveliwala.
\newblock Topic-sensitive {PageRank}: A context-sensitive ranking algorithm for
  web search.
\newblock {\em IEEE Transactions on Knowledge and Data Engineering},
  15(4):784--796, 2003.

\bibitem{JW03}
G.~Jeh and J.~Widom.
\newblock Scaling personalized web search.
\newblock In {\em WWW '03: Proceedings of the 12th International Conference on
  World Wide Web}, pages 271--279, 2003.

\bibitem{Kamada89}
T.~Kamada and S.~Kawai.
\newblock An algorithm for drawing general undirected graphs.
\newblock {\em Information Processing Letters}, 31(1):7--15, 1989.

\bibitem{khandekar06_partitioning}
R.~Khandekar, S.~Rao, and U.~Vazirani.
\newblock Graph partitioning using single commodity flows.
\newblock In {\em STOC '06: Proceedings of the 38th annual ACM Symposium on
  Theory of Computing}, pages 385--390, 2006.

\bibitem{kevin04mqi}
K.~Lang and S.~Rao.
\newblock A flow-based method for improving the expansion or conductance of
  graph cuts.
\newblock In {\em IPCO '04: Proceedings of the 10th International IPCO
  Conference on Integer Programming and Combinatorial Optimization}, pages
  325--337, 2004.

\bibitem{LM04}
A.~N. Langville and C.~D. Meyer.
\newblock Deeper inside {PageRank}.
\newblock {\em Internet Mathematics}, 1(3):335--380, 2004.

\bibitem{LLDM08_communities_CONF}
J.~Leskovec, K.J. Lang, A.~Dasgupta, and M.W. Mahoney.
\newblock Statistical properties of community structure in large social and
  information networks.
\newblock In {\em WWW '08: Proceedings of the 17th International Conference on
  World Wide Web}, pages 695--704, 2008.

\bibitem{LLDM09_communities_IM}
J.~Leskovec, K.J. Lang, A.~Dasgupta, and M.W. Mahoney.
\newblock Community structure in large networks: Natural cluster sizes and the
  absence of large well-defined clusters.
\newblock {\em Internet Mathematics}, 6(1):29--123, 2009.
\newblock Also available at: arXiv:0810.1355.

\bibitem{LLM10_communities_CONF}
J.~Leskovec, K.J. Lang, and M.W. Mahoney.
\newblock Empirical comparison of algorithms for network community detection.
\newblock In {\em WWW '10: Proceedings of the 19th International Conference on
  World Wide Web}, pages 631--640, 2010.

\bibitem{MAFM08}
M.~Maire, P.~Arbelaez, C.~Fowlkes, and J.~Malik.
\newblock Using contours to detect and localize junctions in natural images.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 1--8, 2008.

\bibitem{MVM11}
S.~Maji, N.~K. Vishnoi, and J.~Malik.
\newblock Biased normalized cuts.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 2057--2064, 2011.

\bibitem{Mihail}
M.~Mihail.
\newblock Conductance and convergence of {M}arkov chains---a combinatorial
  treatment of expanders.
\newblock In {\em Proceedings of the 30th Annual IEEE Symposium on Foundations
  of Computer Science}, pages 526--531, 1989.

\bibitem{newman2006finding}
M.E.J. Newman.
\newblock Finding community structure in networks using the eigenvectors of
  matrices.
\newblock {\em Physical Review E}, 74:036104, 2006.

\bibitem{newman2006_ModularityPNAS}
M.E.J. Newman.
\newblock Modularity and community structure in networks.
\newblock {\em Proceedings of the National Academy of Sciences of the United
  States of America}, 103(23):8577--8582, 2006.

\bibitem{OSVV08}
L.~Orecchia, L.~Schulman, U.V. Vazirani, and N.K. Vishnoi.
\newblock On partitioning graphs via single commodity flows.
\newblock In {\em Proceedings of the 40th Annual ACM Symposium on Theory of
  Computing}, pages 461--470, 2008.

\bibitem{ShiMalik00_NCut}
J.~Shi and J.~Malik.
\newblock Normalized cuts and image segmentation.
\newblock {\em IEEE Transcations of Pattern Analysis and Machine Intelligence},
  22(8):888--905, 2000.

\bibitem{Spielman:2004}
D.A. Spielman and S.-H. Teng.
\newblock Nearly-linear time algorithms for graph partitioning, graph
  sparsification, and solving linear systems.
\newblock In {\em STOC '04: Proceedings of the 36th annual ACM Symposium on
  Theory of Computing}, pages 81--90, 2004.

\bibitem{bicg92}
H.~A. van~der Vorst.
\newblock {Bi-CGSTAB}: a fast and smoothly converging variant of {Bi-CG} for
  the solution of nonsymmetric linear systems.
\newblock {\em SIAM J. Sci. and Stat. Comput.}, 13(2):631--644, 1992.

\bibitem{YS01}
S.~X. Yu and J.~Shi.
\newblock Grouping with bias.
\newblock In {\em Annual Advances in Neural Information Processing Systems 14:
  Proceedings of the 2001 Conference}, pages 1327--1334, 2002.

\end{thebibliography}

\end{document}
