\def\year{2019}\relax
\documentclass[letterpaper]{article} \usepackage{aaai19}  \usepackage{times}  \usepackage{helvet}  \usepackage{courier}  \usepackage{url}  \usepackage{graphicx}  



\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathabx}
\usepackage{multirow}
\usepackage{url}
\usepackage{hhline}
\usepackage{tikz,pgfplots}
\usepackage{xcolor}
\usetikzlibrary{arrows,intersections}
\usepackage{multirow}
\usepackage{pgfplots}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\usepackage{latexsym}

\usepackage{color}
\usepackage{multirow}
\usepackage{amsfonts}
\usepackage{float}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage[export]{adjustbox}\usepackage{adjustbox}
\usepackage{booktabs}
\usepackage{array}

\usepackage{pdfpages}
\usepackage{arydshln}

\usepackage{subcaption}
\usepackage{multirow}
\include{plots}
\definecolor{bblue}{HTML}{4F81BD}
\definecolor{rred}{HTML}{C0504D}
\definecolor{ggreen}{HTML}{9BBB59}
\definecolor{ppurple}{HTML}{9F4C7C}
\definecolor{Dark scarlet}{HTML}{560319}
\definecolor{Forest green}{HTML}{1E4D2B}

\frenchspacing  \setlength{\pdfpagewidth}{8.5in}  \setlength{\pdfpageheight}{11in}  

\newcounter{notecounter}
\newcommand{\enotesoff}{\long\gdef\enote##1##2{}}
\newcommand{\enoteson}{\long\gdef\enote##1##2{{
\stepcounter{notecounter}
{\large\bf
\hspace{1cm}\arabic{notecounter}  ##1: ##2
\hspace{1cm}}}}}
\enotesoff

\def\mathlinebreak{\
{\bf c}_w = f( \sum_{q \in Children(w)}  {\bf W}_{R_{(w,q)}} \cdot {\bf p}_q + {\bf b})  \ \mbox{and} \  {\bf p}_q  = [{\bf x}_q, {\bf c}_q] 

{\bf p}_{its} = [{\bf x}_{its}, {\bf c}\dnrm{LEAF}]

\begin{split}
{\bf p}_{president} & =  [{\bf x}_{president}, {\bf c}_{president} ]
\end{split}\\
\begin{split}
{\bf c}_{president} =& f({\bf W}_{poss} {\bf p}_{its} + {\bf b})
\end{split}

\begin{split}
{\bf p}_{named} & = [{\bf x}_{named}, {\bf c}_{named}]
\end{split}\\
\begin{split}
{\bf c}_{named} = & f({\bf W}_{dobj} {\bf p}_{president} + {\bf b})
\end{split}

{\bf h}_{{f}_{t}} = f({\bf V} \cdot {\bf i}_{t} + {\bf W} \cdot  {\bf h}_{{f}_{t-1}})\\
{\bf h}_{{b}_{t}} = f({\bf V} \cdot {\bf i}_{N-t+1} + {\bf W} \cdot {\bf h}_{{b}_{t+1}})\\
{\bf h}_{t} = f({\bf h}_{{f}_{t}} + {\bf h}_{{b}_{t}}+ {\bf W} \cdot {\bf h}_{t-1})
 \mbox{iDepNN-SDP}: {\bf i}_t \!=\! [{\bf x}_t, {\bf
L}_t] \ \ \ \ \ \ \mbox{iDepNN-ADP}: {\bf i}_t \!=\! [{\bf p}_t, {\bf L}_t] e_1/e_1e_2/e_2\le\le=\le\le1k\le\le\lekk$. 
Correspondingly in Table \ref{BioNLPstateoftheart2016}, iDepNN-ADP reports better  precision and balance between precision and recall, signifying its robustness to noise
 in handling inter-sentential relationships.  


{\bf iDepNN vs graphLSTM}: 
\citeauthor{peng2017cross} \shortcite{peng2017cross} focuses on general relation extraction framework using graphLSTM with challenges 
such as potential cycles in the document graph leading to
expensive model training and difficulties in convergence due
to loopy gradient backpropagation.  Therefore, they further
investigated different strategies to backpropagate
gradients. The graphLSTM introduces a number of parameters with a number of
edge types and thus, requires abundant supervision/training
data.  On other hand, our work introduces simple and robust
neural architectures (iDepNN-SDP and iDepNN-ADP), where the
iDepNN-ADP is a special case of document graph in form of a
parse tree spanning sentence boundaries. We offer a smooth
gradient backpropagation in the complete structure (e.g., 
in iDepNN-ADP via recurrent and recursive hidden
vectors) that is more efficient than graphLSTM due to non-cyclic
(i.e., tree) architecture.  We have also shown that
iDepNN-ADP is robust to false positives and maintains a
better balance in precision and recall than graphLSTM for
inter-sentential relationships
(Figure \ref{systemperformanceinsharedtask}).

\section{Conclusion}
\label{conclusion}
We have proposed to classify relations 
between entities within and across sentence boundaries 
by modeling the inter-sentential shortest and augmented dependency
paths within a novel neural network, 
named as inter-sentential Dependency-based Neural Network (iDepNN) that 
takes advantage of both recurrent and recursive neural networks  
to model the structures in the intra- and inter-sentential relationships. 
Experimental results on four datasets from  newswire and medical domains 
have demonstrated that iDepNN is robust to false positives, 
shows 
a better balance in precision and recall and 
achieves the state-of-the-art performance in extracting relationships within and across sentence boundaries. 
We also perform better than 11 teams participating  in the BioNLP shared task 2016. 





\section*{Acknowledgments}
We thank anonymous reviewers for their review comments. 
This research was supported by Bundeswirtschaftsministerium ({\tt bmwi.de}), grant 01MD15010A (Smart Data Web) 
at Siemens AG- CT Machine Intelligence, Munich Germany.

\small

\bibliography{aaai19}
\bibliographystyle{aaai19}

\includepdf[pages=-]{supplementary_material.pdf}

\end{document}
