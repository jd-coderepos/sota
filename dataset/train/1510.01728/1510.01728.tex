\documentclass[letterpaper,conference,onecolumn,11pt]{IEEEtran}
\IEEEoverridecommandlockouts                              \overrideIEEEmargins


\usepackage{srcltx}
\usepackage{color}
\usepackage{graphicx}
\usepackage{bm}
\usepackage[cmex10]{amsmath}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{amssymb}
\usepackage{subfigure}
\usepackage{multirow}
\linespread{1.5}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{example}{Example}
\newtheorem{assumption}{Assumption}
\newtheorem{property}{Property}
 \newtheorem{pf}{Proof}
\usepackage{amsfonts}
\def\qed{\hfill }



\begin{document}


\title{Learning-based Reduced Order Model Stabilization for Partial Differential Equations: Application to the Coupled Burgers Equation}
\author{Mouhacine Benosman, Boris Kramer, Petros Boufounos, Piyush Grover
\thanks{ M. Benosman
(m{\_}benosman@ieee.org) is with the Multimedia group of
Mitsubishi Electric Research Laboratories (MERL), Cambridge, MA
02139, USA. B. Kramer is with the Department of Aeronautics and
Astronautics,  Massachusetts Institute of Technology, Cambridge,
MA, 02139. P. Boufounos is with the Multimedia group of MERL. P.
Grover is with the Mechatronics group of MERL}}

\maketitle


\begin{abstract}
We present results on stabilization for reduced order models (ROM)
of partial differential equations using learning. Stabilization is
achieved via closure models for ROMs, where we use a model-free
extremum seeking (ES) dither-based algorithm to learn the best
closure models' parameters, for optimal ROM stabilization. We
first propose to auto-tune linear closure models using ES, and
then extend the results to a closure model combining linear and
nonlinear terms, for better stabilization performance. The coupled
Burgers' equation is employed as a test-bed for the proposed
tuning method.
\end{abstract}

\section{Introduction}
\label{intro} The problem of reducing a partial differential
equation (PDE) model to a system of finite dimensional ordinary
differential equations (ODE), has significant applications in
engineering and physics, where solving such PDE models is too time
consuming. Reducing the PDE model to a simpler representation,
without loosing the main characteristics of the original model,
such as stability and prediction precision, is appealing for any
real-time model-based computations. However, this problem remains
challenging, since model reduction can introduce stability loss
and prediction degradation. To remedy these problems, many methods
have been developed aiming at what is known as stable model
reduction.

In this paper, we focus on additive terms called {\em closure
models} and their application in reduced order model (ROM)
stabilization. We develop a learning-based method, applying
extremum-seeking (ES) methods to automatically tune the
coefficients of the closure models and obtain an optimal
stabilization of the ROM.

Our work extends some of the existing results in the field. For
instance, a reduced order modelling method is proposed in
\cite{CNTLDBDN13} for stable model reduction of Navier-Stokes flow
models. The authors propose stabilization by adding a nonlinear
viscosity stabilizing term to the reduced order model. The
coefficients of this term are identified using a variational
data-assimilation approach, based on solving a deterministic
optimization. In \cite{BDN13,B13}, a Lyapunov-based stable model
reduction is proposed for incompressible flows. The approach is
based on an iterative search of the projection modes satisfying a
local Lyapunov stability condition.

An example of stable model reduction for the Burger's equation
using closure models is explored in~\cite{SB14,SI13}. These
closure models modify some stability-enhancing coefficients of the
reduced order ODE model using either constant additive terms, such
as the constant eddy viscosity model, or time and space varying
terms, such as Smagorinsky models. The added terms' amplitudes are
tuned in such a way to stabilize the reduced order model. However,
such tuning is not always straightforward. Our work addresses this
issue and achieves optimal tuning using learning-based approaches.

This paper is organized as follows: Section \ref{prem} establishes our
notation and some necessary definitions. Section \ref{es-rom-stab}
introduces the problem of PDE model reduction and the closure
model-based stabilization, and presents the main result of this paper.
An example using the coupled Burgers' equation is treated in Section
\ref{Burgers-example}.  Finally, Section \ref{concl} provides some
discussion on our approach and concludes.

\section{Basic Notations and definitions}
\label{prem}
Throughout the paper we will use  to denote the Euclidean
vector norm; i.e., for  we have . The Kronecker delta function is defined as:
 and . We will
use  for the short notation of time derivative of ,
and  for the transpose of a vector . A function is said
analytic in a given set, if it admits a convergent Taylor series
approximation in some neighborhood of every point of the set. We
consider the Hilbert space , which is
the space of Lebesgue square integrable functions, i.e.,
, iff . We
define on  the inner product  and the associated norm
, as
, and , for
. A function  is in
 if for each ,
, and
.
Finally, in the remaining of this paper by stability we mean
stability of dynamical systems in the sense of Lagrange, e.g.,
\cite{haddad2008}.

\section{ES-based PDEs stable model reduction}
\label{es-rom-stab} We consider a stable dynamical system modelled
by a nonlinear partial differential equation of the form

where  is an infinite-dimension Hilbert space.
Solutions to this PDE can be obtained through numerical
discretization, using, e.g., finite elements, finite volumes,
finite differences etc. Unfortunately, these computations are
often very expensive and not suitable for online applications such
as analysis, prediction and control. However, solutions of the
original PDE often exhibit low rank representations in an
`optimal' basis~\cite{HLB98}. These representation can be
exploited to reduce the PDE to an ODE of significantly lower
order.

In particular, dimensionality reduction follows three steps: The first
step is to discretize the PDE using a finite number of basis
functions, such as piecewise linear or higher order polynomials or
splines. In this paper we use the well-established finite element
method (FEM), and refer the reader to the large literature, e.g.,
\cite{S97,f83} for details. We denote the approximation of the PDE
solution by , where  denotes the scalar
time variable, and  denotes the multidimensional space variable,
i.e.,  is scalar for a one dimensional space, a vector of two
elements in a two dimensional space, etc. We consider the
one-dimensional case, where  is a scalar in a finite interval,
chosen as  without loss of generality. By a standard
abuse of notation,  also denotes the
discretization of the spatial domain at equidistant space points, , for , and some spatial distance
. In this notation,  is an -dimensional vector.

The second step is to determine a set of `optimal,' for some
criterion, spatial basis vectors  for the
discretized problem, that are used to `best' approximate the
discretized PDE solution as

where  is the projection of  onto . Here,
 is a  matrix containing the basis vectors 
as column vectors. Note that the dimension , coming form the high
fidelity discretization of the PDE described above, is generally very
large, in contrast the dimension  of the optimal basis set.

The third step employs a Galerkin projection, a classical nonlinear
model reduction technique,  to obtain a ROM of the form

The function  is obtained
from the weak form of the original PDE, through the Galerkin
projection.

The main challenge in this approach lies in the selection of the
`optimal' basis matrix , and the criterion of optimality
used. There are many model reduction methods to find those basis
functions for nonlinear systems. For example some of the most used
methods are proper orthogonal decomposition (POD) \cite{KV02}, dynamic
mode decomposition (DMD) \cite{KGBBN15}, and reduced basis (RB)
\cite{veroy2005certified}.

\begin{remark}
    In the remainder, we present the idea of closure models in the
    framework of POD. However, the derivation it is not limited to a
    particular type of ROM. Indeed, closure model ideas can be applied
    to other ROMs, such as DMD.
\end{remark}


\subsection{Background on POD Basis Functions}
\label{basic_pod_chap3}
We briefly review the necessary steps for computing POD reduced order
models, described in detail in~\cite{KV02,HLB98}. Models based on POD
select basis functions that capture an maximal amount of energy of the
original system. In particular, the POD basis functions are computed
from a collection of snapshots from the dynamical system over a finite
time interval. These snapshots are usually obtained from a discretized
approximation of the PDE model. This approximation could be obtained
using a numerical method, such as FEM, or using direct measurements of
the system modeled by the PDE, if feasible. In this paper, the POD
basis is computed from snapshots of approximate numerical solutions of
the partial differential equation.

To this end, we compute a set of  snapshots of approximate
solutions as

where  is the selected number of FEM basis functions, and  are
time instances at which snapshots are recorded (do not have to be
uniform). Next, we define the \textit{correlation matrix}  with
elements

The normalized eigenvalues and eigenvectors of  are denoted by
 and , respectively. Note that the  are
also referred to as the \textit{POD eigenvalues}.

The  \textit{POD basis function} is given by

where , the number of retained POD basis
functions, depends on the application. An important property of the
POD basis functions is their orthonormality:

where  denotes the Kronecker delta function.

In this new basis, the solution of the PDE (\ref{general_PDE_chap3})
can then be approximated by

where  are the POD projection
coefficients (which play the role of the  in the ROM
(\ref{ROM1_chap3})).
To find the coefficients , the model (\ref{general_PDE_chap3}) is
projected on the -order POD space using a Galerkin
projection. In particular, both sides of equation (\ref{general_PDE_chap3})
are multiplied by the POD basis functions, where  is
substituted by , and then both sides are integrated over
the space interval . Using the orthonormality
of the POD basis (\ref{pod_ortho_chap3}) leads to an ODE of the form

Note, that the Galerkin projection preserves the structure of the nonlinearities of the original PDE.

\subsection{Closure models for ROM stabilization}\label{closure_models_Chap3}  We start with presenting the problem of stable model reduction in its general form, i.e., without specifying a
particular type of PDE. To this end, we highlight the dependence
of the general PDE  (\ref{general_PDE_chap3}), on a single
physical parameter  by

The parameter  is assumed to be critical for the stability and
accuracy of the model; changing the parameter can either make the
model unstable, or inaccurate for prediction. As an example, since we
are interested in fluid dynamical problems, we use  to denote a
viscosity coefficient. The corresponding reduced order POD model is
takes the form (\ref{pod_proj_chap3}) and (\ref{ROM2_chap3}):

As we explained earlier, the issue with this `simple' Galerkin POD ROM
(denoted POD ROM-G) is that the norm of  might become
unbounded over a finite time support, despite the fact that the
solution of (\ref{general_PDE2_chap3}) is bounded.

One of the main ideas behind the closure models approach is that the
viscosity coefficient  in (\ref{ROM3_chap3}) can be substituted
by a virtual viscosity coefficient , whose form is chosen to
stabilize the solutions of the POD ROM
(\ref{ROM3_chap3}). Furthermore, a penalty term  is
added to the original (POD) ROM-G, as follows

The term  is chosen depending on the structure of
 to stabilize the solutions of
(\ref{ROM41_chap3}). For instance, one can use the Cazemier penalty
model described in \cite{SI13}.

\subsection{Closure Model Examples}
The following closure models were introduced in \cite{WABI12,SI13}
for the case of Burgers' equations. We present them in a general
framework, since similar closure terms could be used on other PDE
models. These examples illustrate the principles behind closure
modelling, and motivate our proposed method. Throughout, 
denotes the total number of modes retained in the ROM. We recall
below some closure models proposed in the literature, which we
will use later to present the main result of this paper.
\subsubsection{Closure models with constant eddy
viscosity coefficients} Here we describe closure models which are
based on constant stabilizing eddy viscosity coefficients.

  {-\bf\it ROM-H model:} The first eddy
viscosity model, known as the Heisenberg ROM (ROM-H) is simply
given by the constant viscosity coefficient

where  is the nominal value of the viscosity coefficient in
(\ref{general_PDE2_chap3}), and  is the additional
constant term added to compensate for the damping effect of the
truncated modes.

{-\bf\it ROM-R model:} This model is a variation of the first one,
introduced in~\cite{R91}. In this model,  is dependent
on the mode index, and the viscosity coefficients for each mode
are given by

with  being the viscosity amplitude, and  the mode
index.

{-\bf\it ROM-RQ model:} This model proposed in \cite{SI13}, is a
quadratic version of the ROM-R, which we refer to as ROM-RQ. It is
given by the coefficients

where the variables are defined similarly to
(\ref{PODROMR_CHAP3}).

{-\bf\it ROM-RQ model:} This model proposed in \cite{SI13}, is a
root-square version of the ROM-R; we use ROM-RS to refer to it. It
is given by

where the coefficients are defined as in (\ref{PODROMR_CHAP3}).

{-\bf\it ROM-T model:} Known as spectral vanishing viscosity
model, is similar to the ROM-R in the sense that the amount of
induced damping changes as function of the mode index. This
concept has been introduced by Tadmor in \cite{T89}, and so these
closure models are referred to as ROM-T. These models are given by

where  denotes the mode index, and  is the index of
modes above which a nonzero damping is introduced.

{-\bf\it ROM-SK model:} Introduced by Sirisup and Karniadakis in
\cite{SK04}, falls into the class of vanishing viscosity models.
We use ROM-SK to refer to it; it is given by


{-\bf\it ROM-CLM model:} This model has been introduced in
\cite{C84,LM96}, and is given by

where  is the mode index, and
 are positive
gains (see \cite{KK00,C84} for some insight about their tuning).



\subsubsection{Closure models with time and space varying eddy
viscosity coefficients}

Several (time and/or space) varying viscosity terms have been proposed
in the literature. For instance, \cite{SI13} describes the Smagorinsky
nonlinear viscosity model. However, the model requires online
computation of some nonlinear closure terms at each time step, which
in general makes it computationally consuming. We report here the
nonlinear viscosity model presented in \cite{CNTLDBDN13}, which is
nonlinear and a function of the ROM state variables. This requires
explicit rewriting of the ROM model (\ref{ROM3_chap3}), to separate
the linear viscous term as follows

where  represents a constant viscosity
damping matrix, and the term  represents the remainder
of the ROM model, i.e., the part without damping. \\Based on
equation (\ref{ROM4_chap3}), we can write the nonlinear eddy
viscosity model denoted by , as

where  is the amplitude of the closure model, the
 are the diagonal elements of the matrix
, and  are defined as follows


where the  are the selected POD eigenvalues (as
defined in Section \ref{basic_pod_chap3}). Compared to the
previous closure models, the nonlinear term  does not
just act as a viscosity, but is rather added directly to the
right-hand side of the reduced order model (\ref{ROM4_chap3}), as
an additive stabilizing nonlinear term. The stabilizing effect has
been analyzed in \cite{CNTLDBDN13} based on the decrease over time
of an energy function along the trajectories of the ROM solutions,
i.e., a Lyapunov-type analysis.

All these closure models share several characteristics, including a
common challenge, among others~\cite{CNTLDBDN13,SB14}: the selection
and tuning of their free parameters, such as the closure models
amplitude . In the next section, we show how ES can be used
to auto-tune the closure models' free coefficients and optimize their
stabilizing effect.


\subsection{Main result: ES-based closure models auto-tuning}
\label{MES-closure-tuning_chap3}
As mentioned in~\cite{SB14}, the tuning of the closure model amplitude
is important to achieve an optimal stabilization of the ROM. To
achieve optimal stabilization, we use model-free ES optimization
algorithms to tune the coefficients of the closure models presented in
Section \ref{closure_models_Chap3}. The advantage of using ES is the
auto-tuning capability that such algorithms allow. Moreover, in
contrast to manual off-line tuning approaches, the use of ES allows us
to constantly tune the closure model, even in an online operation of
the system. Indeed, ES can be used off-line to tune the closure model,
but it can also be connected online to the real system to continuously
fine-tune the closure model coefficients, such as the amplitudes of
the closure models. Thus, the closure model can be valid for a longer
time interval compared to the classical closure models with constant
coefficients, which are usually tuned off-line over a fixed finite
time interval.

We start by defining a suitable learning cost function. The goal of
the learning (or tuning) is to enforce Lagrange stability of the ROM
model (\ref{ROM3_chap3}), and to ensure that the solutions of the ROM
(\ref{ROM3_chap3}) are close to the ones of the original PDE
(\ref{general_PDE2_chap3}). The later learning goal is important for
the accuracy of the solution. Model reduction works toward obtaining a
simplified ODE model which reproduces the solutions of the original
PDE (the real system) with much less computational burden, i.e., using
the lowest possible number of modes. However, for model reduction to
be useful, the solution should be accurate.

We define the learning cost as a positive definite function of the
norm of the error between the approximate solutions of
(\ref{general_PDE2_chap3}) and the ROM (\ref{ROM3_chap3}), as follows

where  denotes the learned parameter, and
 is a positive definite function of . Note that the
error  could be computed off-line using solutions of the ROM
(\ref{ROM3_chap3}), and approximate solutions of the PDE
(\ref{general_PDE2_chap3}). The error could be also computed online
where the  is obtained from solving the
model (\ref{ROM3_chap3}), but the  is obtained from real
measurements of the system at selected space points .

A more practical way of implementing the ES-based tuning of
, is to start with an off-line tuning of the closure
model. Then, the obtained ROM, i.e., the computed optimal value of
, is used in an online operation of the system, e.g.,
control and estimation. One can then fine-tune the ROM online by
continuously learning the best value of  at any give time
during the operation of the system.

To derive formal convergence results, we use some classical
assumptions of the solutions of the original PDE, and on the learning
cost function.
\begin{assumption}\label{pdestab_assumption1_chap3}
The solutions of the original PDE model
(\ref{general_PDE2_chap3}), are assumed to be in
, .
\end{assumption}
\begin{assumption} \label{robustmesass1_pdestab_chap3}
The cost function  in (\ref{Q_pde2_chap3}) has a local minimum
at .
\end{assumption}
\begin{assumption} \label{robustmesass2_pdestab_chap3}
The cost function  in (\ref{Q_pde2_chap3}) is analytic and its
variation with respect to  is bounded in the neighborhood of
, i.e., , where
 denotes a compact neighborhood of
.
\end{assumption}

Under these assumptions, the following lemma follows.
\begin{lemma}\label{pdestab_lemma1_chap3}
Consider the PDE (\ref{general_PDE2_chap3}), under Assumption
\ref{pdestab_assumption1_chap3}, together with its ROM model
(\ref{ROM3_chap3}), where the viscosity coefficient  is
substituted by . Let  take the form of any of
the closure models in (\ref{PODROMH_CHAP3}) to
(\ref{PODROMCLM_CHAP3}), where the closure model amplitude
 is tuned based on the following ES algorithm

where , ,  large enough,
and  is given by (\ref{Q_pde2_chap3}). Under Assumptions
\ref{robustmesass1_pdestab_chap3}, and
\ref{robustmesass2_pdestab_chap3}, the norm of the distance w.r.t.
the optimal value of ,
  admits the following bound

where , and the learning cost function approaches
its optimal value within the following upper-bound\

where .
\end{lemma}
\begin{pf}
Based on Assumptions \ref{robustmesass1_pdestab_chap3}, and
\ref{robustmesass2_pdestab_chap3}, the extremum seeking nonlinear
dynamics (\ref{pdestab_mes_1_chap3}), can be approximated by
linear averaged dynamics (using averaging approximation over time,
\cite[p. 435, Definition 1]{Rote00}). Furthermore, there exist , such that for all , the
solution of the averaged model  is locally
close to the solution of the original ES dynamics, and satisfies
(\cite[p. 436]{Rote00})

with . Moreover, since  is
analytic it can be approximated locally in 
with a quadratic polynomial, e.g., Taylor series up to second
order, which leads to (\cite[p. 437]{Rote00})

Based on the above, we can write

which implies

The cost function upper-bound is easily obtained from the previous
bound, using the fact that  is locally Lipschitz, with the
Lipschitz constant
.
 \hspace{+9cm}
\end{pf}

When the influence of the linear terms of the PDE are dominant, e.g.,
in short-time scales, closure models based on constant linear eddy
viscosity coefficients can be a good solution to stabilize ROMs and
preserve the intrinsic energy properties of the original PDE. However,
in many cases with nonlinear energy cascade, these closure models are
unrealistic; linear terms cannot recover the nonlinear energy terms
lost during the ROM computation. For this reason, many researchers
have tried to come up with nonlinear stabilizing terms for instable
ROMs. An example of such a nonlinear closure model is the one given by
equation (\ref{Smagorinsky_CHAP3}), and proposed in \cite{NETAL08}
based on finite-time thermodynamics (FTT) arguments and in
\cite{NMT011} based on scaling arguments.

Based on the above, we introduce here a combination of both linear and
nonlinear closure models. The combination of both models can lead to a
more efficient closure model. In particular, this combination can
efficiently handle linear energy terms, that are typically dominant
for small time scales and handle nonlinear energy terms, which are
typically more dominant for large time-scales and in some specific
PDEs/boundary conditions. Furthermore, we propose to auto-tune this
closure model using ES algorithms, which provides an automatic way to
select the appropriate term to amplify. It can be either the linear
part or the nonlinear part of the closure model, depending on the
present behavior of the system, e.g., depending on the test
conditions.  We summarize this result in the following Lemma.


\begin{lemma}\label{pdestab_lemma2_chap3}
Consider the PDE (\ref{general_PDE2_chap3}), under Assumption
\ref{pdestab_assumption1_chap3}, together with its stabilized ROM
model

 where the linear viscosity coefficient
 is substituted by  chosen from any of the
constant closure models (\ref{PODROMH_CHAP3}) to
(\ref{PODROMCLM_CHAP3}). The closure model amplitudes
 are tuned based on the following ES algorithm

where ,
,
 large enough, and  is given by
(\ref{Q_pde2_chap3}), with
. Under Assumptions
\ref{robustmesass1_pdestab_chap3}, and
\ref{robustmesass2_pdestab_chap3}, the norm of the vector of
distance w.r.t. the optimal values of ;
  admits the following bound

where , and the learning cost
function approaches its optimal value within the following
upper-bound

where
.
\end{lemma}
\begin{pf}
We will skip the proof for this Lemma, since it follows the same
steps as the proof of Lemma \ref{pdestab_lemma1_chap3}.
\end{pf}


\section{Example: The coupled Burgers' equation}
\label{Burgers-example}
As an example application of our approach, we consider the coupled
Burgers' equation, (e.g., see \cite{B_master_011}), of the form

where  represents the temperature,
 represents the velocity field,  is the
coefficient of the thermal expansion,  the heat diffusion
coefficient,  the viscosity (inverse of the Reynolds number
),  is the one dimensional space variable, ,
and  is the external forcing term such that . The boundary conditions are
imposed as

where  are positive constants, and  and 
denote left and right boundary, respectively. The initial conditions
are imposed as

and are specified below. Following a Galerkin projection onto the
subspace spanned by the POD basis functions, the coupled Burgers'
equation is reduced to a POD ROM with the following structure (e.g.,
see \cite{B_master_011})

where matrix  is due to the projection of the forcing term
, matrix  is due to the projection of the boundary
conditions, matrix  is due to the projection of the viscosity
damping term ,
matrix  is due to the projection of the thermal
coupling and the heat diffusion terms , and the
matrix  is due to the projection of the gradient-based terms
, and . The notations 
(),  (),
stand for the space basis functions and the time projection
coordinates, for the velocity and the temperature, respectively.
The terms  represent the mean values (over
time) of  and , respectively.

\subsection{Burgers equation
ES-based POD ROM stabilization} \subsubsection{Test 1} We first
test the stabilization performance of Lemma
\ref{pdestab_lemma1_chap3}. We test the auto-tuning results of the
ES learning algorithm when tuning the amplitudes of linear closure
models. More specifically, we test the performance of the
Heisenberg closure model given by (\ref{PODROMH_CHAP3}), when
applied in the context of Lemma \ref{pdestab_lemma1_chap3}.
 We consider the coupled Burgers'
equation (\ref{Burgers2_chap3}), with the parameters
,  the
trivial boundary conditions , a simulation
time-length  and zero forcing,  . We use  POD
modes for both variables (temperature and velocity). For the
choice of the initial conditions, we follow \cite{SI13}, where the
simplified Burgers' equation has been used in the context of POD
ROM stabilization. Indeed, in \cite{SI13} the authors propose two
types of initial conditions for the velocity variable, which led
to instability of the nominal POD ROM, i.e., the basic Galerkin
POD ROM (POD ROM-G) without any closure model. Accordingly, we
choose the following initial conditions:


We apply Lemma \ref{pdestab_lemma1_chap3}, with the Heisenberg
linear closure model given by (\ref{PODROMH_CHAP3}). The closure
model amplitude  is tuned using a discretized version of
the ES algorithm (\ref{pdestab_mes_1_chap3}), given by

 where , and  denotes the learning iterations index. We use the
 parameters' values: . The learning
cost function is chosen as

with  for this example. Moreover,
 define the
errors between the true model solution and the POD ROM solution
for temperature and velocity, respectively.\\We first show for
comparison purposes the true solutions, the high fidelity
(`truth') solutions obtained via the FEM. Equation
(\ref{Burgers2_chap3}) is discretized in space with piecewise
linear basis functions with 100 elements, and subsequently solved
with \texttt{ode45} in \texttt{Matlab}\textregistered. The true
solutions are reported in Figure
\ref{burgersstab_truesol_test1_chap3}. We then report in figure
\ref{burgersstab_lfromsol_test1_chap3}, the solutions of the POD
ROM-G (without learning). We can see clearly in these figures that
the POD ROM-G solutions are not as smooth as the true solutions,
particularly the velocity profile is very irregular, and the goal
of the closure model tuning is to smoothen both profiles, as much
as the closure model allows. For a clearer evaluation, we also
report the errors between the true solutions and the POD ROM-G
solutions, in Figure \ref{burgersstab_nome_test1_chap3}. Next, we
apply the ES-based closure model auto-tuning algorithm of Lemma
\ref{pdestab_lemma1_chap3}, and report the associated cost
function in Figure \ref{burgersstab_Q_test1lin_chap3}. We can
clearly see a decrease of the learning cost function over the
iterations, which means an improvement of the ROM prediction
performance. The corresponding profile of the linear closure model
amplitude over learning iterations is displayed in Figure
\ref{burgersstab_hatnue_test1lin_chap3}, which shows that it
reaches an optimal value around .  As
learning iterations proceed beyond the first  displayed here,
the values remain stable around the same points. Thus, for clarity
of presentation, only a zoom around the first  iterations is
displayed. Finally, the corresponding ROM solutions are shown in
Figure \ref{burgersstab_lromsol_test1lin_chap3}. For clearer
evaluation of the optimal closure model stabilizing effect on the
ROM, we also display in Figure \ref{burgersstab_le_test1lin_chap3}
the errors between the exact PDE solutions and the stabilized ROM
solutions.
\subsubsection{Test 2} We report here the case related to Lemma
\ref{pdestab_lemma2_chap3}, where we test the auto-tuning results
of ES on the combination of a linear constant viscosity closure
model and a nonlinear closure model. We apply Lemma
\ref{pdestab_lemma2_chap3}, with the Heisenberg linear closure
model given by (\ref{PODROMH_CHAP3}). The two closure model
amplitudes  and  are tuned using the discrete
version of the ES algorithm (\ref{pdestab_mes_2_chap3}), given by

where , and  is the number of
the learning iterations. We use the parameters' values:
, and a similar
cost function as in Test 1. We show the profile of the learning
cost function over the learning iterations in Figure
\ref{burgersstab_Q_test1_chap3}. We can see a quick decrease of
the cost function within the first  iterations. This means
that the ES manages to improve the overall solutions of the POD
ROM very fast. The associated profiles for the two closure models'
amplitudes learned values  and  are
reported in figures \ref{burgersstab_hatnue_test1_chap3}, and
\ref{burgersstab_hatnunl_test1_chap3}. We can see that even though
the cost function value drops quickly, the ES algorithm continues
to fine-tune the values of the parameters ,
 over the iterations, and they reach eventually
reach an optimal values of , and
. We also show the effect of the
learning on the POD ROM solutions in Figure
\ref{burgersstab_lromsol_test1_chap3}, and figure
\ref{burgersstab_le_test1_chap3}, which by comparison with figures
\ref{burgersstab_lfromsol_test1_chap3}, and
\ref{burgersstab_nome_test1_chap3}, show a clear improvement of
the POD ROM solutions with the ES tuning of the closure models'
amplitudes. We also notice an improvement of the ROM solutions
compared to the linear closure model stabilization test results of
figure  \ref{burgersstab_le_test1lin_chap3}. Specifically, we see
that the temperature error in the case of the closure model of
Lemma \ref{pdestab_lemma2_chap3}, is smaller than the one obtained
with the linear closure model of Lemma \ref{pdestab_lemma1_chap3}.
We do not currently have a formal proof. However, we believe that
the improvement is due to the fact that in the closure model of
Lemma \ref{pdestab_lemma2_chap3}, we are using both the
stabilizing effect of the linear viscosity closure model term and
the stabilizing effect of the nonlinear closure model term.


\begin{figure}\center
  \begin{minipage}{0.4\linewidth}
 \center\subfigure[True velocity profile]{
\includegraphics[width=1\linewidth]{truev_burgstab_test1.pdf}
\label{burgersstab_truev_test1_chap3}}
  \end{minipage}
  \hfill
  \begin{minipage}{0.4\linewidth}
   \center\subfigure[True temperature profile]{
\includegraphics[width=1\linewidth]{truet_burgstab_test1.pdf}
\label{burgersstab_truet_test1_chap3}}
  \end{minipage}
  \caption{True solutions of (\ref{Burgers2_chap3})}
  \label{burgersstab_truesol_test1_chap3}
\end{figure}


\begin{figure}\center
  \begin{minipage}{0.4\linewidth}
   \center\subfigure[Learning-free POD ROM velocity profile]{
\includegraphics[width=1\linewidth]{lfromv_burgstab_test1.pdf}
\label{burgersstab_lfromv_test1_chap3}}
  \end{minipage}
  \hfill
  \begin{minipage}{0.4\linewidth}
   \center\subfigure[Learning-free POD ROM temperature profile]{
\includegraphics[width=1\linewidth]{lfromt_burgstab_test1.pdf}
\label{burgersstab_lfromt_test1_chap3}}
  \end{minipage}
  \caption{Learning-free POD ROM solutions of (\ref{Burgers2_chap3})- No learning}
  \label{burgersstab_lfromsol_test1_chap3}
\end{figure}
\begin{figure}\center
  \begin{minipage}{0.4\linewidth}
   \center\subfigure[Error between the true velocity and the learning-free POD ROM velocity profile]{
\includegraphics[width=1\linewidth]{ev_nolear_burgstab_test1.pdf}
\label{burgersstab_nomv_test1_chap3}}
  \end{minipage}
  \hfill
  \begin{minipage}{0.4\linewidth}
   \center\subfigure[Error between the true temperature and the learning-free POD ROM temperature profile]{
\includegraphics[width=1\linewidth]{et_nolear_burgstab_test1.pdf}
\label{burgersstab_nomt_test1_chap3}}
  \end{minipage}
  \caption{Errors between the nominal POD ROM and the true solutions- No learning}
  \label{burgersstab_nome_test1_chap3}
\end{figure}


\begin{figure}\center
  \begin{minipage}{0.4\linewidth}
   \center\subfigure[Learning cost function vs. number of iterations]{
\includegraphics[width=1\linewidth]{Q_burgstab_test1lin.pdf}
\label{burgersstab_Q_test1lin_chap3}}
  \end{minipage}
  \hfill
  \begin{minipage}{0.4\linewidth}
   \center\subfigure[Learned parameter  vs. number of iterations]{
\includegraphics[width=1\linewidth]{nue_burgstab_test1lin.pdf}
\label{burgersstab_hatnue_test1lin_chap3}}
  \end{minipage}
  \caption{Learned parameters and learning cost function- Stabilization with learning- Linear closure model}
  \label{burgersstab_learningpara_test1lin_chap3}
\end{figure}
\begin{figure}\center
  \begin{minipage}{0.4\linewidth}
   \center\subfigure[Learning-based POD ROM velocity profile]{
\includegraphics[width=1\linewidth]{lromv_burgstab_test1lin.pdf}
\label{burgersstab_lromv_test1lin_chap3}}
  \end{minipage}
  \hfill
  \begin{minipage}{0.4\linewidth}
   \center\subfigure[Learning-based POD ROM temperature profile]{
\includegraphics[width=1\linewidth]{lromt_burgstab_test1lin.pdf}
\label{burgersstab_lromt_test1lin_chap3}}
  \end{minipage}
  \caption{Learning-based POD ROM solutions of (\ref{Burgers2_chap3})- Stabilization with learning- Linear closure model}
  \label{burgersstab_lromsol_test1lin_chap3}
\end{figure}
\begin{figure}\center
  \begin{minipage}{0.4\linewidth}
   \center\subfigure[Error between the true velocity and the learning-based POD ROM velocity profile]{
\includegraphics[width=1\linewidth]{ev_wlear_burgstab_test1lin.pdf}
\label{burgersstab_lev_test1lin_chap3}}
  \end{minipage}
  \hfill
  \begin{minipage}{0.4\linewidth}
\center \subfigure[Error between the true temperature and the
learning-based POD ROM temperature profile]{
\includegraphics[width=1\linewidth]{et_wlear_burgstab_test1lin.pdf}
\label{burgersstab_let_test1lin_chap3}}
  \end{minipage}
  \caption{Errors between the learning-based POD ROM and the true solutions- Stabilization with learning- Linear closure model}
  \label{burgersstab_le_test1lin_chap3}
\end{figure}
\begin{figure}\center
  \begin{minipage}{0.4\linewidth}
   \center\subfigure[Learning cost function vs. number of iterations]{
\includegraphics[width=1\linewidth]{Q_burgstab_test1.pdf}
\label{burgersstab_Q_test1_chap3}}
  \end{minipage}
  \hfill
  \begin{minipage}{0.4\linewidth}
   \center\subfigure[Learned parameter  vs. number of iterations]{
\includegraphics[width=1\linewidth]{nue_burgstab_test1.pdf}
\label{burgersstab_hatnue_test1_chap3}}
  \end{minipage}
  \center  \begin{minipage}{0.4\linewidth}
   \center\subfigure[Learned parameter  vs. number of iterations]{
\includegraphics[width=1\linewidth]{nunl_burgstab_test1.pdf}
\label{burgersstab_hatnunl_test1_chap3}}
  \end{minipage}
  \caption{Learned parameters and learning cost function- Stabilization with learning- Nonlinear closure model}
  \label{burgersstab_learningpara_test1_chap3}
\end{figure}
\begin{figure}\center
  \begin{minipage}{0.4\linewidth}
   \center\subfigure[Learning-based POD ROM velocity profile]{
\includegraphics[width=1\linewidth]{lromv_burgstab_test1.pdf}
\label{burgersstab_lromv_test1_chap3}}
  \end{minipage}
  \hfill
  \begin{minipage}{0.4\linewidth}
   \center\subfigure[Learning-based POD ROM temperature profile]{
\includegraphics[width=1\linewidth]{lromt_burgstab_test1.pdf}
\label{burgersstab_lromt_test1_chap3}}
  \end{minipage}
  \caption{Learning-based POD ROM solutions of (\ref{Burgers2_chap3})- Stabilization with learning- Nonlinear closure model}
  \label{burgersstab_lromsol_test1_chap3}
\end{figure}
\begin{figure}\center
  \begin{minipage}{0.4\linewidth}
   \center\subfigure[Error between the true velocity and the learning-based POD ROM velocity profile]{
\includegraphics[width=1\linewidth]{ev_wlear_burgstab_test1.pdf}
\label{burgersstab_lev_test1_chap3}}
  \end{minipage}
  \hfill
  \begin{minipage}{0.4\linewidth}
\center \subfigure[Error between the true temperature and the
learning-based POD ROM temperature profile]{
\includegraphics[width=1\linewidth]{et_wlear_burgstab_test1.pdf}
\label{burgersstab_let_test1_chap3}}
  \end{minipage}
  \caption{Errors between the learning-based POD ROM and the true solutions- Stabilization with learning- Nonlinear closure model}
  \label{burgersstab_le_test1_chap3}
\end{figure}





\newpage
\section{Conclusion and discussion}\label{concl}
In this work, we explore the problem of stabilization of reduced order
models for partial differential equations, focusing on the closure
model-based ROM stabilization approach. It is well known that tuning
the closure models' gains is an important part in obtaining good
stabilizing performances.  Thus, we propose a learning ES-based
auto-tuning method to optimally tune the gains of linear and nonlinear
closure models, and achieve an optimal stabilization of the ROM. We
validate our using the coupled Burgers' equation as an example,
demonstrating significant gains in error performance. The results are
encouraging. We defer to future publications verifying our approach on
more challenging higher dimensional cases. Our results also raise the
prospect of developing new nonlinear closure models, together with
their auto-tuning algorithms using extremum seeking, as well as other
machine learning techniques.

\bibliographystyle{IEEEtran}
\footnotesize{\bibliography{bibliov3}}



\end{document}
