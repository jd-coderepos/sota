

\documentclass[11pt,a4paper]{article}

\usepackage[nohyperref]{acl2017}

\usepackage{color}
\usepackage{times}
\usepackage{amsmath}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{url}
\usepackage{multirow}
\usepackage[normalem]{ulem}

\aclfinalcopy \def\aclpaperid{561} 



\newcommand\wacomment[1]{\textcolor{blue}{\textbf{[#1] --\textsc{Waleed}}}}
\newcommand\wadelete[1]{}
\newcommand\waadd[1]{#1}

\newcommand\mpcomment[1]{\textcolor{blue}{\textbf{[#1] --\textsc{Matt}}}}

\title{Semi-supervised sequence tagging with bidirectional language models}



\author{Matthew E. Peters, Waleed Ammar, Chandra Bhagavatula, Russell Power \\
    Allen Institute for Artificial Intelligence \\
    {\tt \{matthewp,waleeda,chandrab,russellp\}@allenai.org}
}



\date{}

\begin{document}
\maketitle
\begin{abstract}
Pre-trained word embeddings learned from unlabeled text
have become a standard component of neural network architectures for
NLP tasks.  However, in most cases, the recurrent
network that operates on word-level representations to produce context sensitive representations is trained on relatively little labeled data.
In this paper, we demonstrate a general semi-supervised approach for adding pre-trained context embeddings from bidirectional
language models to NLP systems and apply it to sequence labeling tasks.
We evaluate our model on two standard datasets for named entity recognition (NER) and chunking,
and in both cases achieve state of the art results, surpassing previous systems
that use other forms of transfer or joint learning with additional labeled data and task specific gazetteers.
\end{abstract}

\section{Introduction}

Due to their simplicity and efficacy, pre-trained word embedding have become ubiquitous in NLP systems.
Many prior studies have shown that they capture useful semantic and syntactic information
\citep{word2vec,Pennington2014GloveGV} and including them in NLP systems has been shown to be enormously helpful for a variety of downstream tasks \citep{NLPfromScratch:Collobert2011}.

However, in many NLP tasks it is essential to represent not just the meaning of a word, but also the word in context.  For example, in the two phrases ``A Central Bank spokesman'' and ``The Central African Republic'', the word  `Central' is used as part of both an Organization and Location.  Accordingly, current state of the art sequence tagging models typically include a bidirectional recurrent neural network (RNN) that encodes token sequences into a context sensitive representation before making token specific predictions \citep{yang-transfer-iclr07,Ma2016EndtoendSL,lample-EtAl:2016:N16-1,joint-many-iclr07}.

Although the token representation is initialized with pre-trained embeddings, the parameters of the bidirectional RNN are typically learned only on labeled data.
Previous work has explored methods for jointly learning the bidirectional RNN with supplemental labeled data from other tasks \citep[e.g.,][]{Sgaard2016DeepML,yang-transfer-iclr07}.








In this paper, we explore an alternate semi-supervised approach which does not require additional labeled data.
We use a neural language model (LM), pre-trained on a large, unlabeled corpus to compute an encoding of the context at each position in the sequence (hereafter an \textit{LM embedding}) and use it in the supervised sequence tagging model.
Since the LM embeddings are used to compute the probability of future words in a neural LM, they are likely to encode both the semantic and syntactic roles of words in context.


Our main contribution is to show that the context sensitive
representation captured in the LM embeddings is useful in the
supervised sequence tagging setting.
When we include the LM embeddings in our system overall performance increases from 90.87\% to 91.93\% $F_1$ for the CoNLL 2003 NER task, a more then 1\% absolute F1 increase,
and a substantial improvement over the previous state of the art.
We also establish
a new state of the art result (96.37\% $F_1$) for the CoNLL 2000 Chunking task.

As a secondary contribution, we show that using both forward and backward LM embeddings boosts performance over a forward only LM. We also demonstrate that domain specific pre-training is not necessary by applying a LM trained in the news domain to scientific papers.





\section{Language model augmented sequence taggers (TagLM)}


\subsection{Overview}
The main components in our language-model-augmented sequence tagger (TagLM) are illustrated in Fig.~\ref{fig:major_components}. 
After pre-training word embeddings and a neural LM on large, unlabeled corpora (Step 1), we extract the word and LM embeddings for every token in a given input sequence (Step 2)
and use them in the supervised sequence tagging model (Step 3).






\subsection{Baseline sequence tagging model}
\label{sec:baseline}
Our baseline sequence tagging model is a hierarchical neural tagging model, closely following a number of recent
studies \citep{Ma2016EndtoendSL,lample-EtAl:2016:N16-1,yang-transfer-iclr07,chiu-nichols-2016} (left side of Figure \ref{overview-figure}).

Given a sentence of tokens $(t_1, t_2, \ldots, t_N)$ it first forms a representation, $\mathbf{x}_k$, for each token
by concatenating a character based representation $\mathbf{c}_k$ with a token
embedding $\mathbf{w}_k$:
\begin{align}
\mathbf{c}_k & =  C(t_k; \mathbf{\theta}_c) \nonumber \\
\mathbf{w}_k & =  E(t_k; \mathbf{\theta}_w) \nonumber \\
\mathbf{x}_k & =  [\mathbf{c}_k; \mathbf{w}_k] \label{eqn:token_rep}
\end{align}
The character representation $\mathbf{c}_k$ captures morphological information and
is either a convolutional neural network (CNN) \citep{Ma2016EndtoendSL,chiu-nichols-2016} or RNN \citep{yang-transfer-iclr07,lample-EtAl:2016:N16-1}.  It is parameterized by $C(\cdot, \mathbf{\theta}_c)$ with parameters
$\mathbf{\theta}_c$. The token embeddings, $\mathbf{w}_k$, are obtained as a lookup
$E(\cdot, \mathbf{\theta}_w)$, initialized using pre-trained
word embeddings, and fine tuned during training \citep{NLPfromScratch:Collobert2011}.


\begin{figure}[t]
\begin{center}
\includegraphics[width=3in]{taglm_overview.pdf}
\end{center}
\caption{\label{fig:major_components} The main components in TagLM, our language-model-augmented sequence tagging system. The language model component (in orange) is used to augment the input token representation in a traditional sequence tagging models (in grey).}
\end{figure}

\begin{figure*}[t]
\begin{center}
\includegraphics[width=\textwidth]{overview_figure.pdf}
\end{center}
\caption{\label{overview-figure} Overview of TagLM, our language model augmented sequence tagging architecture.  The top level embeddings from a pre-trained bidirectional LM
are inserted in a stacked bidirectional RNN sequence tagging model.  See text for details.}
\end{figure*}



To learn a context sensitive representation, we employ multiple layers of bidirectional RNNs.
For each token position, $k$, the hidden state
$\mathbf{h}_{k,i}$ of RNN layer $i$ is formed
by concatenating the hidden states from the forward ($\overrightarrow{\mathbf{h}}_{k,i}$) and backward ($\overleftarrow{\mathbf{h}}_{k,i}$) RNNs.  As a result, the bidirectional RNN is able to use both past and future information
to make a prediction at token $k$.  More formally, for the first RNN layer that operates on $\mathbf{x}_k$ to output $\mathbf{h}_{k,1}$:
\begin{align}
\overrightarrow{\mathbf{h}}_{k,1} & = \overrightarrow{R}_1 (\mathbf{x}_k, \overrightarrow{\mathbf{h}}_{k-1,1}; \theta_{\overrightarrow{R}_1}) \nonumber \\
\overleftarrow{\mathbf{h}}_{k,1} & = \overleftarrow{R}_1 (\mathbf{x}_k, \overleftarrow{\mathbf{h}}_{k+1,1}; \theta_{\overleftarrow{R}_1}) \nonumber \\
\mathbf{h}_{k,1} & = [\overrightarrow{\mathbf{h}}_{k,1}; \overleftarrow{\mathbf{h}}_{k,1}] \label{eqn:rnn1}
\end{align}
The second RNN layer is similar and uses $\mathbf{h}_{k,1}$ to output $\mathbf{h}_{k,2}$.
In this paper, we use $L=2$ layers of RNNs in all experiments and parameterize $R_i$ as either
Gated Recurrent Units (GRU) \citep{GRU:Cho2014} or
Long Short-Term Memory units (LSTM) \citep{LSTM:Hochreiter1997} depending on the task.

Finally, the output of the final RNN layer $\mathbf{h}_{k,L}$ is used to predict a score for each possible
tag using a single dense layer.  Due to the dependencies between successive tags in our sequence labeling tasks
(e.g. using the BIOES labeling scheme, it is not possible for \texttt{I-PER} to follow \texttt{B-LOC}), it
is beneficial to model and decode each sentence jointly instead of independently predicting the label for each token.
Accordingly, we add another layer with parameters for each label bigram, computing the sentence conditional random field (CRF) loss
\citep{CRF:Lafferty2001} using the forward-backward algorithm at training time, and using the Viterbi algorithm to find the most likely tag sequence at test time, similar to \citet{NLPfromScratch:Collobert2011}.

\subsection{Bidirectional LM}
\label{sec:bidirectional_lm}
A language model computes the probability of a token sequence $(t_1, t_2, \ldots, t_N)$
\[
p(t_1, t_2, \ldots, t_N) = \prod_{k=1}^N p({t_k} \mid t_1, t_2, \ldots, t_{k-1}).
\]
Recent state of the art neural language models \citep{Jzefowicz2016ExploringTL}
use a similar architecture to our baseline sequence tagger 
where they pass a token representation (either
from a CNN over characters or as token embeddings) through multiple layers of LSTMs to embed
the history $(t_1, t_2, \ldots, t_k)$ into a fixed dimensional vector $\overrightarrow{\mathbf{h}}^{LM}_k$. This is the \textit{forward LM embedding} of the token at position $k$ and is the output of the top LSTM layer in the language model. 
Finally, the language model predicts the probability of token $t_{k+1}$ using
a softmax layer over words in the vocabulary.

The need to capture future context in the LM embeddings suggests it
is beneficial to also consider a \textit{backward} LM in additional
to the traditional \textit{forward} LM.  A backward LM predicts the previous token given the future context.  Given a sentence with $N$ tokens, it computes
\[
p(t_1, t_2, \ldots, t_N) = \prod_{k=1}^N p(t_k \mid t_{k+1}, t_{k+2}, \ldots, t_N).
\]
A backward LM can be implemented in an analogous way to a forward LM and produces the \textit{backward LM embedding} $\overleftarrow{\mathbf{h}}^{LM}_k$, for the sequence $(t_k, t_{k+1}, \ldots, t_N)$, the output embeddings of the top layer LSTM.


In our final system, after pre-training the forward and backward LMs separately, we remove the top layer
softmax and concatenate the forward and backward LM embeddings to form bidirectional LM embeddings, i.e., 
$\mathbf{h}^{LM}_k = [\overrightarrow{\mathbf{h}}^{LM}_k; \overleftarrow{\mathbf{h}}^{LM}_k]$.
Note that in our formulation, the forward and backward LMs are independent, without
any shared parameters.


\subsection{Combining LM with sequence model}
\label{sec:combining}

Our combined system, TagLM, uses the LM embeddings as additional inputs to the sequence tagging model.
In particular, we concatenate the LM embeddings $\mathbf{h}^{LM}$ with the output from one of the bidirectional RNN
layers in the sequence model.
In our experiments, we found that introducing the LM embeddings
at the output of the first layer performed the best.  More formally, we simply replace (\ref{eqn:rnn1}) with
\begin{equation}
\mathbf{h}_{k,1} = [\overrightarrow{\mathbf{h}}_{k,1}; \overleftarrow{\mathbf{h}}_{k,1}; \mathbf{h}_k^{LM}].  \label{eqn:lm_rnn1}
\end{equation}

There are alternate possibilities for adding the LM embeddings to the sequence model.  One possibility adds a non-linear mapping after the concatenation and before the second RNN (e.g. replacing (\ref{eqn:lm_rnn1}) with $f([\overrightarrow{\mathbf{h}}_{k,1}; \overleftarrow{\mathbf{h}}_{k,1}; \mathbf{h}_k^{LM}])$ where $f$ is a non-linear function). Another possibility introduces an attention-like mechanism that weights the all LM embeddings in a sentence before including them in the sequence model.  
Our initial results with the simple concatenation were encouraging so we did not explore these alternatives in this study, preferring to leave them for future work.






\section{Experiments}

We evaluate our approach on two well benchmarked sequence tagging tasks,
the CoNLL 2003 NER task \citep{CoNLL2003NER} and the CoNLL 2000 Chunking task
\citep{CoNLL2000Chunking}.
We report the official evaluation metric
(micro-averaged $F_1$).  In both cases, we use
the BIOES labeling scheme for the output tags,
following previous work which showed it outperforms other options
\citep[e.g.,][]{Ratinov2009DesignCA}.
Following \citet{chiu-nichols-2016}, we use the Senna word embeddings \citep{NLPfromScratch:Collobert2011} and pre-processed the text by
lowercasing all tokens and replacing all digits with 0. 

\paragraph{CoNLL 2003 NER.}
The CoNLL 2003 NER task consists of newswire from the Reuters RCV1 corpus tagged with four
different entity types (\texttt{PER}, \texttt{LOC}, \texttt{ORG}, \texttt{MISC}).  It includes
standard train, development and test sets.  Following previous work \citep{yang-transfer-iclr07,chiu-nichols-2016} we trained on both the train and development sets after tuning hyperparameters on the development set. 

The hyperparameters for our baseline model are similar to \citet{yang-transfer-iclr07}.
We use two bidirectional GRUs with 80 hidden units and 25 dimensional character embeddings for the token character
encoder.  The sequence layer uses two bidirectional GRUs with 300 hidden units each.  For regularization, we add
25\% dropout to the input of each GRU, but not to the recurrent connections.

\paragraph{CoNLL 2000 chunking.}
The CoNLL 2000 chunking task uses sections 15-18 from the Wall Street Journal corpus for training and
section 20 for testing.  It defines 11 syntactic chunk types (e.g., \texttt{NP}, \texttt{VP}, \texttt{ADJP})
in addition to \texttt{other}.  
We randomly sampled 1000 sentences from the training
set as a held-out development set.

The baseline sequence tagger uses 30 dimensional character embeddings and a CNN with 30 filters of width 3 characters followed
by a tanh non-linearity for the token character encoder.
The sequence layer uses two bidirectional LSTMs with 200 hidden units.  Following \citet{Ma2016EndtoendSL} we
added 50\% dropout to the character embeddings, the input to each LSTM layer (but not recurrent connections)
and to the output of the final LSTM layer.

\paragraph{Pre-trained language models.}
\label{sec:language_models}

The primary bidirectional LMs we used in this study were trained
on the 1B Word Benchmark \cite{Chelba2014OneBW}, a publicly available benchmark for large-scale language modeling.
The training split has approximately 800 million tokens, about a 4000X increase over the number training tokens in the CoNLL datasets.  
\citet{Jzefowicz2016ExploringTL} explored several model architectures and released their best single model and training recipes.
Following \citet{Sak2014LongSM}, they used linear projection layers at the output of each LSTM layer to reduce the computation time but still maintain a large LSTM state.
Their single best model took three weeks to train on 32 GPUs and achieved 30.0 test perplexity.  It uses a character CNN with 4096 filters for input, followed by two stacked LSTMs, each with 8192 hidden units and a 1024 dimensional projection layer.
We use \texttt{CNN-BIG-LSTM} to refer to this language model in our results.

In addition to \texttt{CNN-BIG-LSTM} from \citet{Jzefowicz2016ExploringTL},\footnote{\url{https://github.com/tensorflow/models/tree/master/lm_1b}} we used the same corpus to train two additional language models with fewer parameters: forward \texttt{LSTM-2048-512} and backward \texttt{LSTM-2048-512}.
Both language models use token embeddings as input to
a single layer LSTM with 2048 units and a 512 dimension projection layer.  
We closely followed the procedure outlined in \citet{Jzefowicz2016ExploringTL}, except we used synchronous parameter updates across four GPUs instead of asynchronous updates across 32 GPUs and ended training after 10 epochs.
The test set perplexities for our forward and backward \texttt{LSTM-2048-512} language models are 47.7 and 47.3, respectively.\footnote{Due to different implementations, the perplexity of the forward LM with similar configurations in \citet{Jzefowicz2016ExploringTL} is different (45.0 vs. 47.7).}

\begin{table}[t]
\begin{center}
\begin{tabular}{l|l}
\hline \hline
\bf Model & \bf $F_1 \pm$ std \\ \hline
\citet{chiu-nichols-2016} & $90.91 \pm 0.20$ \\
\citet{lample-EtAl:2016:N16-1} & $90.94$ \\
\citet{Ma2016EndtoendSL} & $91.37$ \\
\hline
Our baseline without LM   & $90.87 \pm 0.13$  \\
TagLM & $\mathbf{91.93 \pm 0.19}$ \\
\hline \hline
\end{tabular}
\end{center}
\caption{\label{2003-table} Test set $F_1$ comparison on CoNLL 2003 NER task,
using only CoNLL 2003 data and unlabeled text.}
\end{table}



\begin{table}[t]
\begin{center}
\begin{tabular}{l|l}
\hline \hline
\bf Model & \bf $F_1 \pm$ std \\ \hline
\citet{yang-transfer-iclr07} & $94.66$ \\
\citet{joint-many-iclr07} & $95.02$ \\
\citet{Sgaard2016DeepML} & $95.28$ \\
\hline
Our baseline without LM   & $95.00 \pm 0.08$  \\
TagLM & $\mathbf{96.37 \pm 0.05}$ \\
\hline \hline
\end{tabular}
\end{center}
\caption{\label{2000-table} Test set $F_1$ comparison on CoNLL 2000 Chunking task using only CoNLL 2000 data and unlabeled text.}
\end{table}



\begin{table*}[t]
\begin{center}
\begin{tabular}{l|p{2.4in}|l|l|l}
\hline \hline
          &                        & $F_1$ & $F_1$ & \\
\bf Model & \bf External resources & \bf Without & \bf With & $\Delta$ \\ \hline
\citet{yang-transfer-iclr07} & transfer from CoNLL 2000/PTB-POS & $91.2$ & $91.26$ & $+0.06$ \\
\citet{chiu-nichols-2016} & with gazetteers & $90.91$ & $91.62$ & $+0.71$ \\
\citet{NLPfromScratch:Collobert2011} & with gazetteers & $88.67$ & $89.59$ & $+0.92$ \\
\citet{luo-EtAl:2015:EMNLP2} & joint with entity linking & $89.9$ & $91.2$ & $\mathbf{+1.3}$ \\
\hline
Ours & no LM vs TagLM \textit{unlabeled data only} & $90.87$ & $\mathbf{91.93}$ & $+1.06$ \\
\hline \hline
\end{tabular}
\end{center}
\caption{\label{2003-table-aux} Improvements in test set $F_1$ in CoNLL 2003 NER when including
additional labeled data or task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).}
\end{table*}



\begin{table*}[t]
\begin{center}
\begin{tabular}{l|p{2.4in}|l|l|l}
\hline \hline
          &                        & $F_1$ & $F_1$ & \\
\bf Model & \bf External resources & \bf Without & \bf With & $\Delta$ \\ \hline
\citet{yang-transfer-iclr07} & transfer from CoNLL 2003/PTB-POS & $94.66$ & $95.41$ & $+0.75$ \\
\citet{joint-many-iclr07} & jointly trained with PTB-POS & $95.02$ & $95.77$ & $+0.75$ \\
\citet{Sgaard2016DeepML} & jointly trained with PTB-POS & $95.28$ & $95.56$ & $+0.28$ \\
\hline
Ours & no LM vs TagLM \textit{unlabeled data only} & $95.00$ & $\mathbf{96.37}$ & $\mathbf{+1.37}$ \\
\hline \hline
\end{tabular}
\end{center}
\caption{\label{2000-table-aux} Improvements in test set $F_1$ in CoNLL 2000 Chunking when including additional labeled data (except the case of TagLM where we do not use additional labeled data).}
\end{table*}




\paragraph{Training.}
All experiments use the Adam optimizer \cite{Kingma2014AdamAM} with gradient norms clipped at 5.0.
In all experiments, we fine tune the pre-trained Senna word embeddings but fix all weights in the
pre-trained language models.  In addition to explicit dropout regularization, we also use early
stopping to prevent over-fitting and use
the following process to determine when to stop training.
We first train with a constant learning rate $\alpha = 0.001$ on the training data and monitor the development set performance at each epoch.  Then, at the epoch with the highest development performance, we start a simple learning rate annealing schedule: decrease $\alpha$ an order of magnitude (i.e., divide by ten), train for five epochs, decrease
$\alpha$ an order of magnitude again, train for five more epochs and stop.

Following \citet{chiu-nichols-2016}, we train each final model configuration ten times with different random seeds and report the mean and standard deviation $F_1$.
It is important to estimate the variance of model performance since the test data sets are relatively small.

\begin{table}[t]
\begin{center}
\begin{tabular}{l|l}
\hline \hline
\bf Use LM embeddings at & \bf $F_1 \pm$ std \\ \hline
input to the first RNN layer & $91.55 \pm 0.21$ \\
output of the first RNN layer & $\mathbf{91.93 \pm 0.19}$ \\
output of the second RNN layer & $91.72 \pm 0.13$ \\
\hline \hline
\end{tabular}
\end{center}
\caption{\label{2003-table-lm-level} Comparison of CoNLL-2003 test set $F_1$ when the LM embeddings are included at different layers in the baseline tagger.
}
\end{table}


\begin{table*}[t]
\begin{center}
\begin{tabular}{l|l||c|c||l}
\hline \hline
\bf Forward language model &\bf Backward language model & \multicolumn{2}{c||}{\bf LM perplexity} & \bf $F_1 \pm$ std \\
&& Fwd & Bwd & \\ \hline
--- & --- & N/A & N/A & $90.87 \pm 0.13$ \\ \hline
$\texttt{LSTM-512-256}^*$ & $\texttt{LSTM-512-256}^*$ & 106.9 & 104.2 & $90.79 \pm 0.15$ \\ \hline
\texttt{LSTM-2048-512} & --- & 47.7 & N/A & $91.40 \pm 0.18$ \\
\texttt{LSTM-2048-512} & \texttt{LSTM-2048-512} & 47.7 & 47.3 & $91.62 \pm 0.23$ \\ \hline
\texttt{CNN-BIG-LSTM} & --- & 30.0 & N/A & $91.66 \pm 0.13$ \\
\texttt{CNN-BIG-LSTM} & \texttt{LSTM-2048-512} & 30.0 & 47.3 & $\mathbf{91.93 \pm 0.19}$ \\
\hline \hline
\end{tabular}
\end{center}
\caption{\label{2003-table-lm-size} Comparison of CoNLL-2003 test set $F_1$ for different language model combinations.  All language models were trained and evaluated on the 1B Word Benchmark, except $\texttt{LSTM-512-256}^*$ which was trained and evaluated on the standard splits of the NER CoNLL 2003 dataset.}
\end{table*}





\subsection{Overall system results}
\label{sec:overall_system_results}

Tables \ref{2003-table} and \ref{2000-table} compare results from TagLM with
previously published state of the art results without additional labeled data or task specific gazetteers.  Tables \ref{2003-table-aux} and \ref{2000-table-aux} compare results of TagLM to other systems that include additional labeled data or gazetteers.
In both tasks, TagLM establishes a new state of the art using bidirectional LMs (the forward \texttt{CNN-BIG-LSTM} and the backward \texttt{LSTM-2048-512}).

In the CoNLL 2003 NER task, our model scores 91.93
mean $F_1$, which is a statistically significant increase over the previous best result of 91.62 $\pm 0.33$ from \citet{chiu-nichols-2016} that used gazetteers (at 95\%, two-sided Welch t-test, $p=0.021$).  

In the CoNLL 2000 Chunking task, TagLM achieves 96.37 mean $F_1$, exceeding all previously published
results without additional labeled data by more then 1\% absolute $F_1$. 
The improvement over the previous best result of 95.77 in \citet{joint-many-iclr07} that jointly trains with Penn Treebank (PTB) POS tags is statistically significant at
95\% ($p < 0.001$ assuming standard deviation of $0.1$).

Importantly, the LM embeddings amounts to an average absolute improvement of 1.06 and 1.37 $F_1$ in the NER and Chunking tasks, respectively.

\paragraph{Adding external resources.}
Although we do not use external labeled data or gazetteers, we found that TagLM outperforms previous state of the art results in both tasks when external resources (labeled data or task specific gazetteers) are available.
Furthermore, Tables \ref{2003-table-aux} and \ref{2000-table-aux} show that, in most cases,
the improvements we obtain by adding LM embeddings are larger then the improvements previously
obtained by adding other forms of transfer or joint learning.
For example, \citet{yang-transfer-iclr07} noted an improvement of only 0.06 $F_1$ in the NER task when
transfer learning from both CoNLL 2000 chunks and PTB POS tags and \citet{chiu-nichols-2016} reported
an increase of 0.71 $F_1$ when adding gazetteers to their baseline.
In the Chunking task, previous work has reported from 0.28 to 0.75 improvement in $F_1$ when including supervised labels from
the PTB POS tags or CoNLL 2003
entities \citep{yang-transfer-iclr07,Sgaard2016DeepML,joint-many-iclr07}.

\subsection{Analysis}
To elucidate the characteristics of our LM augmented sequence tagger, we ran a number of
additional experiments  on the CoNLL 2003 NER task.

\paragraph{How to use LM embeddings?}
In this experiment, we concatenate the LM embeddings at different locations in the baseline sequence tagger.
In particular, we used the LM embeddings $\mathbf{h}_k^{LM}$ to:
\begin{itemize}
\item augment the \textit{input} of the \textit{first} RNN layer; i.e., \\
$\mathbf{x}_k  = [\mathbf{c}_k; \mathbf{w}_k; \mathbf{h}_k^{LM}]$,
\item augment
the \textit{output} of the \textit{first} RNN layer; i.e.,
$\mathbf{h}_{k,1} = [\overrightarrow{\mathbf{h}}_{k,1}; \overleftarrow{\mathbf{h}}_{k,1}; \mathbf{h}_k^{LM}]$,\footnote{This configuration the same as Eq.~\ref{eqn:lm_rnn1} in \S\ref{sec:combining}. It was reproduced here for convenience.} and
\item augment the \textit{output} of the \textit{second} RNN layer; i.e., $\mathbf{h}_{k,2}  = [\overrightarrow{\mathbf{h}}_{k,2}; \overleftarrow{\mathbf{h}}_{k,2}; \mathbf{h}_k^{LM}]$.
\end{itemize}

Table \ref{2003-table-lm-level} shows that the second alternative
performs best.
We speculate that the second RNN layer in the sequence tagging
model is able to capture interactions between task specific context as
expressed in the first RNN layer and general context as expressed in the LM
embeddings in a way that improves overall
system performance.  These results are consistent with \citet{Sgaard2016DeepML} who found that chunking performance was sensitive
to the level at which additional POS supervision was added.





\paragraph{Does it matter which language model to use?} In this experiment, we compare six different configurations of the forward and backward language models (including the baseline model which does not use any language models).
The results are reported in Table \ref{2003-table-lm-size}.

We find that adding backward LM embeddings consistently outperforms forward-only LM embeddings, with $F_1$ improvements between 0.22 and 0.27\%, even with the relatively small backward \texttt{LSTM-2048-512} LM.

LM size is important, and replacing the forward \texttt{LSTM-2048-512} with \texttt{CNN-BIG-LSTM} (test perplexities of 47.7 to 30.0 on 1B Word Benchmark) improves $F_1$ by 0.26 - 0.31\%, about as much as adding backward LM. Accordingly, we hypothesize (but have not
tested) that replacing the backward \texttt{LSTM-2048-512} with a backward LM analogous to the
\texttt{CNN-BIG-LSTM} would further improve performance.

To highlight the importance of including language models trained on a large scale data, we also experimented
with training a language model on just the CoNLL 2003 training and development data.  Due to the much smaller size
of this data set, we decreased the model size to 512 hidden units with a 256 dimension projection and
normalized tokens in the same manner as input to the sequence tagging model (lower-cased, with all digits replaced
with 0).  The test set perplexities for the forward and backward models (measured on the CoNLL 2003 test data) were 106.9 and 104.2, respectively.  Including embeddings from these language models
\textit{decreased} performance slightly compared to the baseline system without any LM.
This result supports the hypothesis that adding language models help because they learn composition functions (i.e., the RNN parameters in the language model) from much larger data compared to the composition functions in the baseline tagger, which are only learned from labeled data.

\paragraph{Importance of task specific RNN.}
To understand the importance of including a task specific sequence RNN
we ran an experiment that removed the task specific sequence RNN and used only the LM embeddings with a dense layer and CRF to predict output tags.  In this setup, performance was very low, 88.17 $F_1$, well below our baseline.
This result confirms that the RNNs in the baseline tagger encode essential information which is not encoded in the LM embeddings.
This is unsurprising since the RNNs in the baseline tagger are trained on labeled examples, unlike the RNN in the language model which is only trained on unlabeled examples.
Note that the LM weights are fixed in this experiment.

\paragraph{Dataset size.} \textit{A priori}, we expect the addition of LM embeddings to be most beneficial in cases
where the task specific annotated datasets are small.  To test this hypothesis, we replicated the setup from \citet{yang-transfer-iclr07}
that samples 1\% of the CoNLL 2003 training set and compared the performance of TagLM to our baseline without LM.
In this scenario, test $F_1$ increased 3.35\% (from 67.66 to 71.01\%) compared to an increase of 1.06\% $F_1$ for a similar comparison
with the full training dataset.  The analogous increases in \citet{yang-transfer-iclr07} are 3.97\% for cross-lingual
transfer from CoNLL 2002 Spanish NER and 6.28\% $F_1$ for transfer from PTB POS tags.  However, they found only a 0.06\% $F_1$
increase when using the full training data and transferring from both CoNLL 2000 chunks and PTB POS tags.
Taken together, this suggests that for very small labeled training sets, transferring from other tasks yields a large improvement, but this improvement almost disappears when the training data is large.
On the other hand, our approach is less dependent on the training set size and significantly improves performance even with larger training sets.

\paragraph{Number of parameters.}  Our TagLM formulation increases the
number of parameters in the second  RNN layer $R_2$ due to the increase
in the input dimension $\mathbf{h}_1$ if all other hyperparameters are held constant.
To confirm that this did not have a material impact on the results, we ran two additional experiments.
In the first, we trained a system without a LM but increased the second  RNN layer
hidden dimension so that number of parameters was the same as in TagLM.
In this case, performance \textit{decreased} slightly (by 0.15\% $F_1$) compared to the baseline model, indicating
that solely increasing parameters does not improve performance.  In the second experiment, we decreased
the hidden dimension of the second RNN layer in TagLM to give it the same number
of parameters as the baseline no LM model.  In this case, test $F_1$ \textit{increased} slightly
to $92.00 \pm 0.11$ indicating that the additional parameters in TagLM are slightly hurting performance.\footnote{A similar experiment
for the Chunking task did not improve $F_1$ so this conclusion is task dependent.} 

\paragraph{Does the LM transfer across domains?}
One artifact of our evaluation framework is that both the labeled data in the chunking and NER tasks
and the unlabeled text in the 1 Billion Word Benchmark used to train the bidirectional LMs are derived from news articles.
To test the sensitivity to the LM training domain, we also applied TagLM with a LM trained on news articles
to the SemEval 2017 Shared Task 10, ScienceIE.\footnote{\url{https://scienceie.github.io/}}
ScienceIE requires end-to-end
joint entity and relationship extraction from scientific publications across three diverse fields (computer science, material sciences, and physics) and defines three broad entity types (Task, Material and Process).
For this task, TagLM increased $F_1$ on the development set by 4.12\% (from 49.93 to
to 54.05\%) for entity extraction over our baseline without LM embeddings and it was a major component
in our winning submission to ScienceIE, Scenario 1 \citep{scienceie}.
We conclude that LM embeddings can improve the performance of a sequence tagger even when the data comes from a different domain.

\section{Related work}

\paragraph{Unlabeled data.}
TagLM was inspired by the widespread use of pre-trained word embeddings in supervised sequence tagging models. 
Besides pre-trained word embeddings, our method is most closely related to \citet{li:05}.
Instead of using a LM, \citet{li:05} uses a probabilistic generative model to infer context-sensitive latent variables for each token, which are then used as extra features in a supervised CRF tagger \citep{CRF:Lafferty2001}.
Other semi-supervised learning methods for structured prediction problems include co-training \citep{blum:98,pierce:01}, expectation maximization \citep{nigam:00,mohit:05}, structural learning \citep{ando:05} and maximum discriminant functions \citep{suzuki:07,suzuki:08}.
It is easy to combine TagLM with any of the above methods by including LM embeddings as additional features in the discriminative components of the model (except for expectation maximization).
A detailed discussion of semi-supervised learning methods in NLP can be found in \cite{sogaard:13}.

\citet{Melamud2016context2vecLG} learned a context encoder from unlabeled data with an objective function similar to a bi-directional LM and applied
it to several NLP tasks closely related to the unlabeled objective function: sentence completion, lexical substitution and word sense disambiguation.



LM embeddings are related to a class of methods \citep[e.g.,][]{Le2014DistributedRO,Kiros2015SkipThoughtV,Hill2016LearningDR} for learning sentence and document encoders from unlabeled data, which can be used for text classification and textual entailment among other tasks.  \citet{Dai2015SemisupervisedSL} pre-trained LSTMs using language models and sequence autoencoders then fine tuned the weights for classification tasks.  In contrast to our method that uses unlabeled data to learn token-in-context embeddings, all of these methods use unlabeled data to learn an encoder for an entire text sequence (sentence or document).

\paragraph{Neural language models.}
LMs have always been a critical component in statistical machine translation systems \citep{koehn:09}.
Recently, neural LMs \citep{bengio:03,mikolov:10} have also been integrated in neural machine translation systems \citep[e.g.,][]{kalchbrenner:13,devlin:14} to score candidate translations.
In contrast, TagLM uses neural LMs to encode words in the input sequence.

Unlike forward LMs, bidirectional LMs have received little prior attention.
Most similar to our formulation, \citet{Peris2015ABR} used a bidirectional
neural LM in a statistical machine translation system for instance selection.
They tied the input token embeddings and softmax weights in
the forward and backward directions, unlike our approach which uses
two distinct models without any shared parameters.
\citet{frinken:12} also used a bidirectional n-gram LM for
handwriting recognition.



\paragraph{Interpreting RNN states.}
Recently, there has been some interest in interpreting the activations of RNNs. \citet{Linzen2016AssessingTA} showed that single LSTM units
can learn to predict singular-plural distinctions.  \citet{Karpathy2015VisualizingAU} visualized character level LSTM states and showed that individual cells capture long-range dependencies such as line lengths, quotes and brackets.  Our work complements these studies by showing that LM states are useful for downstream tasks as a way of interpreting what they learn.

\paragraph{Other sequence tagging models.}
Current state of the art results in sequence tagging problems are based on bidirectional RNN models.
However, many other sequence tagging models have been proposed in the literature for this class of problems \citep[e.g.,][]{CRF:Lafferty2001,collins:02}. 
LM embeddings could also be used as additional features in other models, although it is not clear whether the model complexity would be sufficient to effectively make use of them.



\section{Conclusion}
In this paper, we proposed a simple and general semi-supervised method using pre-trained neural language models to augment token representations in sequence tagging models.
Our method significantly outperforms current state of the art models in two popular datasets for NER and Chunking. 
Our analysis shows that adding a backward LM in addition to traditional forward LMs consistently improves performance.
The proposed method is robust even when the LM is trained on unlabeled data from a different domain, or when the baseline model is trained on a large number of labeled examples.







\section*{Acknowledgments}
We thank Chris Dyer, Julia Hockenmaier, Jayant Krishnamurthy, Matt Gardner and Oren Etzioni for comments on earlier drafts that led to substantial improvements in the final version.  

\bibliography{transfer_lm_sequence_tagging}
\bibliographystyle{acl_natbib}


\end{document}
