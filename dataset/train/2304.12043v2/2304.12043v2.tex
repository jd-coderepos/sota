
\documentclass{article} \usepackage{iclr2023_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}


\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{bbding}
\usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{graphicx}
\usepackage{xspace} 
\usepackage{wrapfig}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{cite}
\usepackage{caption,subcaption}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{fontawesome}
\usepackage{soul}
\usepackage{color}
\title{MixPro: Data Augmentation with MaskMix and Progressive Attention Labeling for Vision Transformer}

\iclrfinalcopy
\author{Qihao Zhao, {Yangyu Huang} , Wei Hu\thanks{Corresponding author. huwei@buct.edu.cn}  , Fan Zhang\thanks{Fan Zhang is with the College of Information Science and Technology and the Interdisciplinary Research Center for Artificial Intelligence, Beijing University of Chemical Technology, Beijing 100029.} , Jun Liu\\
Beijing University of Chemical Technology, China
Microsoft Research Asia, China \\
Singapore University of Technology and Design, Singapore \\
\And
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}


\maketitle
\vspace{-20px}
\begin{abstract}
\label{secabs}

The recently proposed data augmentation TransMix employs attention labels to help visual transformers (ViT) achieve better robustness and performance. However, TransMix is deficient in two aspects: 1) The image cropping method of TransMix may not be suitable for ViTs. 2) At the early stage of training, the model produces unreliable attention maps. TransMix uses unreliable attention maps to compute mixed attention labels that can affect the model. To address the aforementioned issues, we propose MaskMix and Progressive Attention Labeling (PAL) in image and label space, respectively. In detail, from the perspective of image space, we design MaskMix, which mixes two images based on a patch-like grid mask. In particular, the size of each mask patch is adjustable and is a multiple of the image patch size, which ensures each image patch comes from only one image and contains more global contents. From the perspective of label space, we design PAL, which utilizes a progressive factor to dynamically re-weight the attention weights of the mixed attention label. Finally, we combine MaskMix and Progressive Attention Labeling as our new data augmentation method, named MixPro. The experimental results show that our method can improve various ViT-based models at scales on ImageNet classification (73.8\% top-1 accuracy based on DeiT-T for 300 epochs). After being pre-trained with MixPro on ImageNet, the ViT-based models also demonstrate better transferability to semantic segmentation, object detection, and instance segmentation. Furthermore, compared to TransMix, MixPro also shows stronger robustness on several benchmarks. The code is available at \url{https://github.com/fistyee/MixPro}. 
\end{abstract}

\vspace{-10px}
\section{Introduction}
\label{secIntro}
Transformers \citep{vaswani2017attention} have revolutionized the natural language processing (NLP) field and have recently inspired the emergence of transformer-style architectures in the computer vision (CV) field, such as Vision Transformer (ViT) \citep{2020ViTs}. These methods design with competitive results in numerous CV tasks like image classification \citep{deit, TokentoToken, pyramidViTs, swin, cait, ali2021xcit}, object detection \citep{fang2021youYOLOS, dai2021dynamicdetr, detr, zhu2020deformabledetr} and image segmentation \citep{strudel2021segmenter, pyramidViTs, swin}. Previous research has discovered that ViT-based networks are difficult to optimize and can easily overfit to training data with many images \citep{russakovsky2015imagenet}, resulting in a significant generalization gap in the test data. To improve the generalization and robustness of the model, the recent works \citep{2020ViTs, deit, TokentoToken, pyramidViTs, swin, cait, ali2021xcit} employ data augmentation \citep{zhang2017mixup} and regularization techniques \citep{labelsmoothing} during training. Among them, the mixup-base methods such as Mixup \citep{zhang2017mixup}, CutMix \citep{yun2019cutmix} and TransMix \citep{chen2021transmix} are implemented to improve the generalization and robustness of ViT-based networks. For the CutMix, patches are cut and pasted among training images where the ground truth labels are also mixed proportionally to the area of the patches. Furthermore, TransMix based on CutMix considers that not all pixels are created equal. Then it exploits ViT's attention map to re-assign the weight to the mixed label rather than applying the proportion of the cropped image area.

    
\begin{figure*}[t]
\centering
\includegraphics[width=1\columnwidth]{mixdataiclr.pdf}
\caption{ Comparison between TransMix \citep{chen2021transmix} (a) and our proposed MixPro (b). 1) For image space, TransMix shares the same cropped region with CutMix \citep{yun2019cutmix}, which results in patches containing different regions from the two images (patches colored red). Differently, as shown on the right of the figure, MixPro mixes patches using a patch-like mask. The size of the mask patches is the multiple of the image patches. This enables each patch of the mixed image to come from only one image (patches colored yellow and blue). 2) For label space, TransMix computes  by . In contrast, we propose progressive attention labeling that dynamically re-weights  and  using a progressive factor ().}

\label{figFramework}
\vspace{-20px}
\end{figure*}

Nevertheless, TransMix \citep{chen2021transmix} has the following drawbacks for vision transformer: (1) TransMix and CutMix share the same region-level cropped images as input. However, ViT-based models naturally have global receptive fields from self-attention \citep{2020ViTs}, so the region-based mixed images may provide insufficient image contents. (More details are in Sec. \ref{SecAbl}.) (2) As shown in Fig. \ref{figFramework} (a)TransMix provides cropped patches with sharp rectangular borders that are clearly distinguishable from the background (viewed as red patches). The ViT-based models may naturally be curious about the cropped patch and then pay attention to that patch, resulting in a basic weight of attention regardless of whether the patch contains useful information \citep{chen2021transmix}. (3) TransMix employs attention map to re-weight the confidence for the mixed targets. However, attention maps may not always be \textbf{reliable} during the training process. For example, at the beginning of the training, the model has no representation capability, and the attention maps gained are unreliable. In addition, it is possible to obtain difficult samples using massive data augmentation strategies, and the attention map is also unreliable. At this point, reassigning the mixed labels utilizing a low-confidence attention map will generate noisy mixed labels.

To this end, we propose a novel data augmentation method, \emph{MixPro}, to tackle the above issues from the perspective of image space and label space, respectively. Our approach is presented in Fig. \ref{figFramework} (b). In detail, from the perspective of image space, we designed MaskMix, which is inspired by the mask strategy of MAE \citep{he2022masked}. Our MaskMix replaces the masked patches of one image with visible patches of another image to create a mixed image. In particular, the scale of each mask patch is adjustable and is a multiple of the image patch size. In this way, each image patch comes from only one image (viewed as yellow and blue patches in the figure). In addition, the mask decomposition of pictures can take into account both region and global contents. From the perspective of label space, we designed Progressive Attention Labeling (PAL), which utilizes a progressive factor () to dynamically re-weight the attention weight of the mixed attention label. The progressive factor () provides an indirect measure of the confidence of the attention map for the mixed sample, to trade-off the attention proportional weight () and the area proportional weight (). In conclusion, we combine Mask\textbf{Mix} and \textbf{Pro}gressive Attention Labeling, namely MixPro, as our data augmentation strategy to improve the generalization and robustness of ViT-based models.

In experiments, we demonstrate extensive evaluations of MixPro on various ViT-based models and tasks. MixPro exhibits greater performance gains than TransMix for all listed ViT-based models. Notably, MixPro can further bring an improvement of 0.9\% for DeiT-S. In addition, we show that that the benefits can also be transferred to downstream tasks such as object detection, instance segmentation, and semantic segmentation if the model is first pre-trained on ImageNet using MixPro.
In terms of robustness, compared to TransMix, MixPro also shows stronger robustness on three different benchmarks.

Overall, we summarize our contributions as follows:
\begin{itemize}
    \item We propose a new data augmentation method, MixPro, to address the shortcomings of TransMix from the perspective of image space and label space, respectively.
    
    \item From the perspective of image space, MixPro ensures that each image patch comes from only one image and contains more global contents. From the perspective of label space, MixPro utilizes a progressive factor to dynamically re-weight the attention weight of the mixed attention label.
    
    \item In experiments, we demonstrate extensive evaluations of MixPro on various ViT-based models and downstream tasks. It boosts Deit-T achieving 73.8\% and Deit-S achieving 81.3\% on ImageNet-1K. Furthermore, compared to TransMix, MixPro also shows stronger robustness on three different benches.
\end{itemize}


\section{Related Work}
\label{subRel}

\textbf{Vision Transformers (ViTs).}
Transformers were initially proposed for sequence models such as machine translation \citep{vaswani2017attention}. Inspired by the success of transformers in NLP tasks,  Vision Transformer (ViT) \citep{2020ViTs} attempted to apply transformers for image classification by treating an image as a sequence of patches with excellent results compared to even state-of-the-art convolutional networks. DeiT \citep{deit} further extended ViT using a novel distillation approach and a powerful training recipe. Based on the success of ViT, numerous ViT-based models have emerged \citep{TokentoToken, pyramidViTs, swin, cait, ali2021xcit, yang2021focal}. PVT \citep{pyramidViTs} developed a progressive shrinking pyramid and a spatial-reduction attention layer to obtain high-resolution and multi-scale feature maps under limited computation/memory resources. XCIT \citep{ali2021xcit} built their models with cross-covariance attention as its core component and demonstrate the effectiveness and generality of our models on various computer vision tasks. Swin \citep{swin} proposed the shifted window-based self-attention and showed the model was effective and efficient on vision problems. These ViT-based models have achieved good results not only for classification tasks \citep{deng2009imagenet}, but also for object detection \citep{fang2021youYOLOS, dai2021dynamicdetr, detr, zhu2020deformabledetr} and image segmentation \citep{strudel2021segmenter, pyramidViTs, swin} tasks. However, these ViT-based models rely on MixUp-based data augmentation to enhance generalization in case of insufficient data.

\textbf{MixUp-based data augmentation.}
Mixup \citep{zhang2017mixup} was the first work to propose interpolation of two images and their labels to augment the training data. Subsequent variants of the Mixup appeared \citep{yun2019cutmix, chen2021transmix, verma2019manifold, baek2021gridmix, patchmix,kim2020puzzle, walawalkar2020attentive, uddin2020saliencymix}. They can be mainly divided into two groups: global image mixture (e.g. Manifold Mixup \citep{verma2019manifold}, GridMix \citep{baek2021gridmix}, TokenMix \citep{liu2022tokenmix}, and PatchMix \citep{patchmix}), and local/region image mixture (e.g.CutMix \citep{zhang2017mixup}, Puzzle-Mix \citep{kim2020puzzle}, Attentive-CutMix \citep{walawalkar2020attentive}, SaliencyMix \citep{uddin2020saliencymix}, and MixToken \citep{tolenlabeling}). Manifold Mixup \citep{verma2019manifold} proposed to train neural networks on interpolations of hidden representations. GridMix \citep{baek2021gridmix} was composed of two local constraints: local data augmentation by grid-based image mixing and local patch mapping constrained by patch-level labels. In CutMix \citep{yun2019cutmix}, patches are cut and pasted among training images where the ground truth labels are also mixed proportionally to the area of the patches. Attentive-CutMix \citep{walawalkar2020attentive} enhanced CutMix by choosing the most descriptive regions based on the intermediate attention maps from a feature extractor, which enables searching for the most discriminative parts in an image. MixToken \citep{tolenlabeling} is a modified version of CutMix operating on the tokens after patch embedding. Among them, MixUp and CutMix are most successful in improving the performance of ViTs \citep{deit}. Furthermore, TransMix \citep{chen2021transmix} based on CutMix continues to improve the generalization and robustness of ViT-based models by reassigning the ground truth labels with transformer attention guidance. TokenMix \citep{liu2022tokenmix} utilizes a pre-trained teacher model to generate attention maps for guiding the mixed labels.





All in all, our proposed MixPro differs significantly from the above MixUp-based methods.
First of all, CutMix, MixToken and TransMix focus on region contents, whereas MixPro mixes images with adjustable mask patches and may contain more global contents. Secondly, TransMix mixes image patches may generate noisy attention maps, while MixPro generates clean attention maps for mixing labels. The last and most important one is that, from the viewpoint of label space, TransMix neglects the fact that the model attention map during training is not always reliable. MixPro solves this issue by using progressive factors to dynamically recalculate the attention weight of the mixed attention label without a pre-trained model to generate it.

\section{Method}
\label{subMethod}

\subsection{Background}

\textbf{Multi-head self-Attention.} Multi-head self-Attention as introduced by ViTs \citep{2020ViTs}, firstly divides and embeds an image  to patch tokens , where  is the number of tokens, each of dimensionality . It aggregates the global information by a class token . Then ViTs operate on the patch embedding . For a Transformer with h attention heads and embedding z, the class attention can be formulated as follows:





where  are weight matrices for \textbf{q} and \textbf{k} .  is the map of attention from the class token to the patch tokens. For the final classifier, it is summarized as the most valuable patches. If there is more than one head in the attention, it is simply the average of all the heads in the attention.

\textbf{TransMix.} TransMix \citep{chen2021transmix} calculates  (the proportion for mixing two labels) with the attention map  and a binary mask  from CutMix \citep{yun2019cutmix}:



where  (  ) denotes the nearest-neighbor interpolation downsampling. It can convert the original \textbf{M} of  into  pixels.

\subsection{MixPro}

Our approach, MixPro, consists of MaskMix and progressive attention labeling to improve the performance and robustness of ViT-based models from the viewpoint of image space and label space, respectively. Next, we will describe these two methods in detail.

\textbf{MaskMix.} Let   denote a training image and let  be its corresponding label. ViT \citep{2020ViTs} regularly partitioned a high-resolution image  into  patches of a fixed size of   pixels () and then embeds these to tokens. In our method, we first divide the grid mask   into  patches, resulting in each mask patch region of size . In particular, to make each image patch come from only one image, we ensure that the size of mask patches () is a multiple of the image patch size (). Then we select  regions to mask, where 



and the value in the selected mask region is set to . In all our experiments, we follow CutMix \citep{yun2019cutmix} set  to 1, the  is sampled from the uniform beta distribution Beta(1,1). With the mask , for samples ,  and their corresponding labels , , we utilize the mask  to generate their mixing samples ():



where , binary mask filled with ones is represent as \textbf{1} and  is element-wise multiplication. In this way, as illustrated in Fig.\ref{figFramework} (b), image patches are aligned with mask patches when mixing images to ensure that each image patch comes from only one image.

\textbf{Progressive Attention Labeling.} However, the attention map  may not always be accurate for each mixed sample. At the early stage of the training, models have poor representational ability, and the attention map  may not be reliable for some difficult mixed samples. At this point, the mixed labels calculated with  would limit the performance of ViT.

Since blindly using attention maps to reassign mixed labels is not reliable. The focus is on designing a  progressive factor (), which could provide an indirect measure of the confidence of the attention map for the mixed sample, to trade-off the attention proportional weight () and the area proportional weight (). For neural networks with ground-truth distributions, cross-entropy measures the epistemic uncertainty of the model \citep{2017What}. The more certain the model is, the more reliable the attention map is obtained. There, we employ the cosine similarity of the model output to the ground-truth labels as a proxy of cross-entropy to measure whether a mixed sample can obtain a high-confidence attention map. We calculate the cosine similarity, i.e., 


where  is the model’s output softmax probability. Since both  and  are non-negative vectors, the range of  is . Ideally, the cosine similarity  closer to 1, the samples better the network fits the mixed sample and it can also produce a high-confidence attention map. Therefore, we use the cosine similarity as our progressive factor , 



and the  written as:
\vspace{-10px}


where  is the proportion to instead of  in Eq. (7) for mixing two labels. So far, we propose Progressive Attention Labeling (PAL), which employs the progressive factor () to dynamically re-weight the attention weight of the mixed attention label. 



\subsection{Discussion}
\begin{figure*}[!htb]
\centering
\includegraphics[width=1\columnwidth]{Topacc.pdf}

\caption{Visualization of progressive factor () and generalization analysis of our MixPro and TransMix. We employ Mixpro and TransMix based on Deit-S on ImageNet-1k for 300 epochs. Figures (a) and (b), demonstrate the visualization of the progressive factor (). Figures (c) and (d) depict their loss and top-1 validation error on ImageNet-1k. MixPro provides improved generalization compared to TransMix.}
\label{figError}
\vspace{-10px}
\end{figure*}
\textbf{Visualization of progressive factor ().}
Fig. \ref{figError} (a) indicates the trend of the mean progressive factor () of all samples with the epoch. We can observe that since the model will gradually fit the training samples, the progressive factor will grow slowly with epoch. Fig. \ref{figError} (b) indicates the value of the progressive factor in a mini-batch at epoch 300. We can find that since the model learns to different degrees for different samples, the progressive factors obtained by the model will be different, so our method can flexibly adjust the weight of the attention map in the mixed labels according to the progressive factors.


\textbf{Generalization analysis.} We compare and analyze the generalization of MixPro and TransMix. Fig. \ref{figError} (c) and (d) depict their loss and top-1 validation error on ImageNet-1k for 300 epochs based on Deit-S. We can notice that although MixPro has a larger loss in training compared to TransMix, it has a smaller top-1 error in verification. This demonstrates that Deit-S trained with MixPro can achieve better generalizability.

\section{Experiment}
\label{subExp}

In this section, we primarily evaluate MixPro for its effectiveness, transferability, and robustness. We first study the effectiveness of MixPro on ImageNet-1k classification in the Sec. \ref{subCla}. Next, we show the transfer ability of a MixPro pre-trained model when it is fine-tuned for downstream tasks (In the Sec. \ref{subTran}). In the Sec. \ref{secRob}, we also show that Mixpro can improve the model's robustness compared to TransMix.

\subsection{ImageNet Classification}
\label{subCla}

\begin{table*}[!htb]\footnotesize
\caption{Compared to TransMix, MixPro provides better performance on variety of models , e.g. DeiT, CaiT, PVT, XCiT, Swin on ImageNet-1k classification. All the baselines are reported in TransMix \citep{chen2021transmix}.}
\label{tabViT}
\begin{center}
\begin{tabular}[t]{c|cccp{2cm}p{2cm}}
\toprule
Models &  Params & \#FLOPs & Top-1 Acc(\%)  & Top-1 Acc(\%) +TransMix & Top-1 Acc(\%) +MixPro \\
\midrule
DeiT-T \citep{deit}     &5.7M  &1.6G &  72.2  & 72.6 & 73.8+\textcolor{blue}{(+1.2)}\\
PVT-T  \citep{pyramidViTs}     &13.2M & 1.9G & 75.1  & 75.5 & 76.7+\textcolor{blue}{(+1.2)} \\
XCiT-T \citep{ali2021xcit}     & 12M &2.3G & 79.4  & 80.1 & 81.2+\textcolor{blue}{(+1.1)} \\
CA-Swin-T \citep{swin}     &28.3M &4.2G &  81.6  & 81.8 & 82.8+\textcolor{blue}{(+1.0)} \\
\midrule
CaiT-XXS    &17.3M &3.8G &  79.1  & 79.8 & 80.6+\textcolor{blue}{(+0.8)} \\
DeiT-S \citep{deit}      &22.1M & 4.7G & 79.8  &80.7 & 81.3+\textcolor{blue}{(+0.6)} \\
PVT-S  \citep{pyramidViTs}      &24.5M & 3.8G &  79.8  & 80.5 & 81.2+\textcolor{blue}{(+0.7)} \\
XCiT-S  \citep{ali2021xcit}     &26M & 4.8G & 82.0  & 82.3 & 82.9+\textcolor{blue}{(+0.6)} \\
CA-Swin-S  \citep{swin}     &49.6M &8.5G & 82.8  & 83.2 & 83.7+\textcolor{blue}{(+0.5)} \\
\midrule
PVT-M  \citep{pyramidViTs}      &44.2M &6.7G &  81.2  & 82.1 & 82.7+\textcolor{blue}{(+0.6)} \\
PVT-L  \citep{pyramidViTs}      &61.4M & 9.8G & 81.7  & 82.4 & 82.9+\textcolor{blue}{(+0.5)} \\
XCiT-M  \citep{ali2021xcit}     &84M &16.2G &82.7     & 83.4 & 84.1+\textcolor{blue}{(+0.7)} \\
DeiT-B  \citep{deit}     & 86.6M &17.6G & 81.8  & 82.4 & 82.9+\textcolor{blue}{(+0.5)}\\
\midrule
XCiT-L \citep{ali2021xcit} & 189M &36.1G &  82.9  & 83.8 & 84.7+\textcolor{blue}{(+0.9)} \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-10px}
\end{table*}


\textbf{Implementation Details.} For image classification, we evaluate on ImageNet-1K \citep{deng2009imagenet}, which contains 1.28M training images and 50K validation images from 1,000 classes. We examined various baseline vision Transformer models using in TranMix \citep{chen2021transmix} including DeiT \citep{deit}, PVT \citep{pyramidViTs}, CaiT \citep{cait}, XCiT \citep{ali2021xcit}, CA-Swin \citep{swin,chen2021transmix}. Specifically, CA-swin replaces the last Swin block \citep{swin} with a block of classification attention (CA) with no parameter overhead. It allows TransMix and MixPro to be generalized to Swin. The training schemes will be slightly adjusted to the official papers’ implementation. These experiments' settings follow TransMix\citep{chen2021transmix}. We employ an AdamW  optimizer for 300 epochs expect that XCiT \citep{ali2021xcit} and CaiT \citep{cait} reported 400 epochs. Following the TransMix, we use a cosine decay learning rate scheduler and 20 epochs of linear warm-up. Similarly, a batch size of 1024, an initial learning rate of 0.001, and a weight decay of 0.05 are used. All baselines have already contained the carefully tuned regularization methods reported in TransMix \citep{chen2021transmix} except for repeated augmentation \citep{hoffer2020augment} and EMA \citep{polyak1992acceleration} which do not enhance performance. From the multi-head self-attention layer of the last transformer block, the attention map  in Eq. \ref{equattn} can be obtained as an intermediate output as in TransMix\citep{chen2021transmix}.

\subsubsection{Comparison with TransMix on a wide range of ViT-based model.}

As table \ref{tabViT} shows, MixPro can improve top-1 accuracy on ImageNet for all ViT-based models listed. Compared to TransMix, MixPro achieves better performance on all models. In particular, MixPro has better performance on models with fewer parameters. For example, MixPro reaches 73.8\% on Deit-T, which is 1.2\% higher than TransMix.

\vspace{-10px}
\subsubsection{Comparison with SOTA Mixup Variants}
\begin{table}[!htb]
\caption{Top1 accuracy, training speed (image/sec) and number of parameters compared to SOTA Mixup variants on ImageNet-1k for 300 epochs. For a fair comparison, all models listed are based on the DeiT-S training recipe. The training speed (image/sec) takes into account data and model forward and backward in training time. The main results are reported in TransMix \citep{chen2021transmix} and TokenMix \citep{liu2022tokenmix}. }
\label{tabmixup}
	\centering
	\begin{tabular}{c|ccp{1cm}p{2cm}}
    \toprule
    Method     & Backbone   & \#Params & Speed (im/sec) & top-1 Acc (\%) \\
    \midrule
    Baseline & \multirow{7}{*}{DeiT-S }   & 22M & 322 & 78.6     \\
    GridMix     &  ~ & 22M & 322 & 79.5 \textcolor{blue}{(+0.9)}    \\
    CutMix     &  ~ & 22M & 322 & 79.8 \textcolor{blue}{(+1.2)}    \\
    Attentive-CutMix    & ~ & 46M & 239 & 77.5 \textcolor{red}{(-1.1)}     \\
    SaliencyMix     & ~  & 22M & 314 & 79.2    \textcolor{blue}{(+0.6)}  \\
    Puzzle-Mix  &  ~  & 22M &  139 &  79.8   \textcolor{blue}{(+1.2)}   \\
    TransMix    & ~  & 22M & 322 & 80.7    \textcolor{blue}{(+2.1)}  \\
    TokenMix    & ~  & 22M & 322 & 80.8    \textcolor{blue}{(+2.2)}  \\
    \midrule
MixPro(Ours)     & ~ & 22M & 322 & \textbf{81.3}    \textcolor{blue}{(+2.7)}  \\
    \bottomrule
    
\end{tabular}
\vspace{-10px}
\end{table}

We compare many SOTA Mixup-based methods on ImageNet-1k \citep{deng2009imagenet} in this section. Following TransMix, we also train based on DeiT-S for a fair comparison. MixPro is measured in image per second (im/sec), training speed (i.e., training throughput), and takes into account data mixup \citep{zhang2017mixup}, model forward and backward in train-time in an average of five runs for images at 224x224 resolution with a batch size of 128 using a Tesla V100 graphics card. Table \ref{tabmixup} shows that MixPro outperforms all other Mixup-based methods.

\vspace{-5px}
\subsection{Transfer to Downstream Tasks}
\label{subTran}

We show that pre-trained models based on MixPro can be transferred to downstream tasks such as semantic segmentation, object detection, and instance segmentation. We observe the improvements over the vanilla pre-trained baselines as well as over TransMix.

\subsubsection{Semantic Segmentation} 

\begin{minipage}{\linewidth}\scriptsize

\begin{minipage}[t]{0.37\linewidth}
\centering
\makeatletter\def\@captype{table}
\begin{tabular}{p{0.8cm}p{0.8cm}p{0.9cm}p{0.3cm}p{0.3cm}}
    \toprule
    pretrained & Backbone     &  Decoder    &  mIoU & +MS\\
    \midrule
    ResNet101 &ResNet101 & Deeplabv3+      &   47.3 & 48.5    \\
    \midrule
    DeiT-S    &\multirow{3}{*}{DeiT-S}    &\multirow{3}{*}{Linear}          &  49.1 & 49.6    \\
    +TransMix    &~    & ~          &  49.7 & 50.3    \\
    +MixPro    &~    & ~          &  \textbf{50.3} & \textbf{50.9}   \\
    \midrule
    DeiT-S     &\multirow{3}{*}{DeiT-S} &\multirow{3}{*}{Segmenter}    &   49.7 & 50.5    \\
    +TransMix    &~    & ~       &  50.6 & 51.2    \\
    +MixPro    &~    & ~      &  \textbf{51.1} & \textbf{51.6}    \\
    \bottomrule
\end{tabular}
\caption{Overhead-free impact of MixPro on transferring to a downstream semantic segmentation task on the Pascal Context \citep{mottaghi2014pascal} dataset. (MS) denotes multi-scale testing. The best results are in bold.}
\label{segtable}
\end{minipage}
\hfill
\begin{minipage}[t]{0.6\linewidth}
\centering
\makeatletter\def\@captype{table}
\begin{tabular}{ccp{0.3cm}p{0.3cm}p{0.5cm}|p{0.3cm}p{0.3cm}p{0.3cm}}
    \toprule
    \multirow{2}{*}{Backbone}  &\multirow{2}{*}{\#Params} &\multicolumn{3}{c}{Object detection}  &\multicolumn{3}{c}{Instance segmentation}                       \\
    ~& ~&  &  &  &   &  &  \\
    \midrule
    ResNet50 &44.2M  &38.0  &58.6  &41.4  &34.4  &57.1  &36.7    \\
    ResNet101 & 63.2M  &40.4  &61.1  & 44.2  &36.4  & 57.7  &38.8    \\
    \midrule
    PVT-S &44.1M  &40.4  &62.9  &43.8  &37.8  &60.1  &40.3    \\
    TransMix-PVT-S &44.1M  &40.9  &63.8  &44.0  &38.4  &60.7  &41.3    \\
    MixPro-PVT-S &44.1M  &\textbf{41.4}  &\textbf{64.2}  &\textbf{44.4}  &\textbf{38.9}  &\textbf{61.1}  &\textbf{41.7}    \\
    \bottomrule
  \end{tabular}
\caption{Following TransMix \citep{chen2021transmix}, overhead-free transfer from MixPro to downstream object detection and instance segmentation with Mask R-CNN \citep{he2017mask} with PVT \citep{pyramidViTs} backbone on COCO val2017. The  denotes mask average precision for instance segmentation and  denotes bounding box average precision for object detection.}
\label{objecttable}
\end{minipage}
\end{minipage}

\textbf{Settings.} 
We decode the sequence of patch embeddings  into a segmentation map , where K is the count of classes. Following TransMix \citep{chen2021transmix}, we also employ two convolution-free decoders. One is the linear decoding, which is a point-by-point linear layer over the DeiT. We employ patch embedding  to generate patch-level logits. Then they are reshaped and bilinearly upsampled to the segmentation map . The second one is the segmenter decoder \citep{strudel2021segmenter}, which is a transformer-based decoder, namely the Mask Transformer. We also train and evaluate the models on the Pascal Context \citep{mottaghi2014pascal} dataset, which contains 4998 images with 59 semantic classes and a background class. In addition, the reported mean Intersection over Union (mIoU) is averaged over all classes as the main metric. The experiments are carried out using MMSegmentation \citep{contributors2020mmsegmentation}. All comparative experimental results come from TransMix \citep{chen2021transmix}.

\textbf{Results.} Table \ref{segtable} shows that the pre-trained DeiT-SLinear and DeiT-S-Segmenter of MixPro beat the pre-trained baselines of TransMix by 0.6\% and 0.5\% respectively. For multi-scale testing. MixPro also outperforms the TransMix pre-trained baselines of 0.6\% and 0.4\% mIoU.


\subsubsection{Object Detection and Instance Segmentation} 

\textbf{Settings.} Object detection and instance segmentation experiments are conducted on COCO 2017, which contains 118K images and evaluates 5K validation images. Following TransMix, we employ our method on PVT \citep{pyramidViTs} as the detection backbone since its pyramid features make it beneficial to object detection. Following TransMix \citep{chen2021transmix}, we adopt 1x training schedule (i.e., 12 epochs) to train the detector based on  mmDetection \citep{chen2019mmdetection} framework. 

\textbf{Results.} As indicated in Table \ref{objecttable}, we observe that on two downstream tasks, the results of MixPro's pre-trained backbone once again outperformed TransMix's pre-trained backbone, which enhanced 0.5\% box AP and 0.5\% mask AP, respectively. 


\subsection{Robustness Analysis}
\label{secRob}
We also compare MixPro with TransMix in terms of robustness and out-of-distribution performance.
    
\begin{figure*}[!htb]
\centering
\includegraphics[width=1\columnwidth]{robust.pdf}
\caption{Robustness against occlusion. The figure shows the robustness of DeiT-S against occlusion with different information loss ratios.}

\label{figrobust}
\end{figure*}
\subsubsection{Robustness to Occlusion} Naseer et al. \citep{naseer2021intriguing} investigate whether ViTs perform robustly in the presence of missing partial or substantial image content. In particular, ViTs divide an image 224×224×3 into  =256 patches on a 14x14x3 spatial grid. In the experiments, "patch dropping" means that the original image patches are replaced with patches with a 0 value. Following TransMix \citep{chen2021transmix}, We present classification accuracy on an ImageNet 1k validation set with three dropping settings. (1) Random patch dropping, where a subset of M patches is randomly selected and dropped. (2) Salient (foreground) Patch Dropping, which examines the robustness of ViTs to occlusions of highly salient regions. Naseer et al. \citep{naseer2021intriguing} thresholds on DINO's attention map to obtain salient patches, which are then dropped according to ratios. (3) Non-salient (background) segregation, which selects and segregates the least salient regions of an image using the above method.

\textbf{Results.} As demonstrated in Fig. \ref{figrobust}, DeiT-S with MixPro is superior to TransMix and vanilla DeiT-S on all occlusion levels.

\subsubsection{Natural Adversarial Example}
\begin{table}[!htb]
\caption{Robustness of DeiT-S against natural adversaries in ImageNet-A and out-of-distribution adversaries in ImageNet-O.}
\label{tabNat}
	\centering
	\begin{tabular}{ccccc}
    \toprule
      ~ & \multicolumn{3}{c}{Nat. Adversarial Example}  & Out-of-Dist \\
        Models &  Top1-Acc   &  Calib-Error  & AURRAC & AUPRC\\
    \midrule
     DeiT-S &19.1\% &32.0 \% & 23.8 \%& 20.9 \%    \\
     TransMix-DeiT-S &21.1\% &31.2 \% & 28.8 \%& 21.9 \%    \\
     MixPro-DeiT-S &\textbf{22.4}\% &\textbf{30.3} \% & \textbf{32.4} \%& \textbf{23.1} \%    \\
  
    \bottomrule
\end{tabular}
\end{table}

For these experiments, we use the ImageNet-A dataset \citep{hendrycks2021natural}, which is an adversarial collection of 7,500 unaltered, natural but "hard" images from the real world. These images are taken from a number of challenging scenarios, such as fog scenes and occlusion. For evaluating our method, following settings in TransMix \citep{chen2021transmix}, we evaluate methods on the top-1 accuracy, Calibration Error (CalibError) \citep{hendrycks2021natural} which judges how classifiers can reliably forecast their accuracy, and the Area Under the Response Rate Accuracy Curve (AURRAC) which is an uncertainty estimation metric.

\textbf{Results.} As demonstrated in Table \ref{tabNat}, MixPro-trained Deit-S outperforms TransMix-trained DeiT-S and vanilla DeiT-S on all metrics. MixPro lifts Top1-Acc on ImageNet-adversarial by 1.3\%. For AURRAC, MixPro-DeiT-S achieves 32.4\%, 3.6\% higher than TransMix-DeiT-S.
\subsubsection{Out-of-distribution Detection}
The ImageNet-O \citep{hendrycks2021natural} is an adversarial dataset designed to detect out-of-distribution. It is an adversarial collection of 2000 images from outside ImageNet-1K. The goal is to produce low confidence predictions from the anomalies of unforeseen classes. The metric is the area under the precision-recall curve (AUPRC). Table \ref{tabNat} shows that MixPro-trained DeiT-S outperforms TransMix-trained DeiT-S by 1.2\% AUPRC and outperforms DeiT-S by 2.2\% AUPRC.

 \vspace{-10px}

\section{Conclusion}
\label{subExp}
In this paper, we propose a new data augmentation method, MixPro. MixPro addresses the shortcomings of the current SOTA data augmentation method, TransMix, from the perspective of image and label space for the vision transformer. From the perspective of image space, we propose MaskMix which is a random mask strategy with adjustable scale. MaskMix ensures each image patch comes from only one image and contains more global contents. From the perspective of label space, we propose progressive attention labeling, which utilizes a progressive factor () to dynamically re-weight the attention weight of the mixed attention label. Experimental results show that compared with TransMix, our method brings an improvement of 1.2\%, 0.6\%, and 0.5\% for DeiT-T,  DeiT-S, and Deit-B on the imagenet, respectively. After being pre-trained with MixPro on ImageNet, the ViT-based models also demonstrate better transferability to three downstream tasks such as semantic segmentation, object detection, and instance segmentation . In addition, compared to TransMix, MixPro also shows stronger robustness on three different benchmarks. In the ablation study, we detail the effect of each proposed module, the effect of different scales of mask patches, the different strategies of the progressive factor, and so on.


\textbf{Acknowledgments and Disclosure of Funding}

This work was supported in part by the National Natural Science Foundation of China under Grant 62271034, and in part by the Fundamental Research Funds for the Central Universities under Grant XK2020-03.

\bibliography{iclr2023}
\bibliographystyle{iclr2023_conference}

\appendix
\section{Appendix}

\subsection{Ablation Study}
\label{SecAbl}

\textbf{The effect of our proposed module.} Table \ref{tabcom} demonstrated the effect of removing different components to provide insights into what makes MixPro successful. The experimental results are based on DeiT-S training for 300 epochs and Mixup \citep{zhang2017mixup} is default used. We can observe that combining MaskMix and PAL, our method MixPro can enhance 1.5\% compared to the standard setup (using Mixup and CutMix together), reaching 81.3\% top-1 accuracy.



\begin{table}[!htb]
\caption{Top1-accuracy is on imagenet-1K based on DeiT-S. We study the effect of removing different components to provide insights into what makes MixPro successful.}
\label{tabcom}
	\centering
	\begin{tabular}{ccccc|c}
    \toprule
     CutMix &TransMix & MaskMix   & PAL  & top-1 Acc (\%) \\
    \midrule
     \Checkmark & \XSolidBrush& \XSolidBrush    & \XSolidBrush  & 79.8        \\
     \Checkmark &  \Checkmark  & \XSolidBrush  & \XSolidBrush & 80.7 \textcolor{blue}{(+0.9)}    \\
     \XSolidBrush  & \Checkmark &  \Checkmark    & \XSolidBrush & 81.0 \textcolor{blue}{(+1.2)}    \\
     \Checkmark  & \Checkmark &  \XSolidBrush    & \Checkmark & 81.1 \textcolor{blue}{(+1.3)}    \\
      \XSolidBrush & \XSolidBrush  & \Checkmark    & \Checkmark  & 81.3 \textcolor{blue}{(+1.5)}     \\
    \bottomrule
\end{tabular}
\end{table}

\begin{figure*}[!htb]
\centering
\includegraphics[width=1\columnwidth]{random.pdf}
\caption{Illustration of different mask strategies.}
\label{figmask}
\end{figure*}



\textbf{The effect of different mask strategies.} Figure \ref{figmask} illustrates the different mask strategies. The region-based strategy is widely used in previous works \citep{yun2019cutmix,uddin2020saliencymix,cutout}. Block and random based strategies mainly used in self-supervised learning \citep{he2022masked, bao2021beit}. Our MixPro boosts random strategy on an adjustable scale. To better inspect the impact of mask strategies alone, in table \ref{tabmask}, we directly use  to generate mixed labels instead of PAL. Table \ref{tabmask} shows the different performances of the mask strategies. Block and random strategy results come from TokenMix \citep{liu2022tokenmix}. We can observe that the block and random mask strategy improves significantly compared to the region mask strategy. This also indicates that images with more mixed global content are more effective for the vision transformer. Furthermore, our proposed adjustable scale is necessary for the random mask strategy. Compared with block and random strategies, the mask patches with a 4x scale can improve the accuracy of Deit-T by 0.5\% and Deit-S by 0.2\%.


\begin{table}[!htb]
\caption{Ablation of mask strategy.}
\label{tabmask}
	\centering
	\begin{tabular}{ccccc}
    \toprule
     model & region &block & random   & random(4x scale)   \\
    \midrule
     Deit-T & 72.2    & 72.7  & 72.7  & \textbf{73.2}     \\
     Deit-S &  79.8  & 80.6  & 80.6 & \textbf{80.8}     \\

    \bottomrule
\end{tabular}
\end{table}

\textbf{The effect of different scales of mask patches.}
The scale of mask patches  is multiple of the scale of image patches.  There are several optional scales . The Fig. \ref{figBeta} (a) indicates that the evaluate results of MixPro with different scales based on DeiT-S on Imagenet-1K top-1 error. For all scales considered, MixPro improves upon the baseline (20.2\%) significantly. Moreover, it performs best when the scale is multiplied by 4. We still recommend adjusting the scale size more finely for different models to get better results.
\begin{figure*}[!htb]
\centering
\includegraphics[width=1\columnwidth]{beta.pdf}

\caption{Effect of different scales of mask patch and  on Imagenet-1K top-1 error with DeiT-S.}
\label{figBeta}
\end{figure*}

\textbf{The effect of different .}
In Fig. \ref{figBeta} (b), we demonstrate the top-1 error of MixPro under different Beta distribution, such as , on Imagenet-1K with DeiT-S. We can observe that  is equal to 1 when our model achieves the best performance.

\textbf{The different strategies of progressive factor .}
To facilitate the understanding of our proposed progressive attention labeling (PAL), we explore several different strategies to generate the progressive factor  evaluating on Imagenet-1K with DeiT-T. We employ several types of strategies: 1) Progress-relevant strategies adjust  with the number of training epochs, e.g., parabolic decay, etc. 2) Progress-irrelevant strategies include the equal weight. 3) A learnable parameter.

\textbf{How the boundary inside the patch influences ViTs? } TransMix introduces sharp rectangular borders even within an image patch (see Fig. \ref{figFramework}, red colored) that are clearly distinguishable from the background. These borders inside the patch may draw excessive attention to the model, because for vision transformers, a patch would form a semantic unit, and the computation of the attention map of the whole image is obtained by summing the attention of each image patch.

Our MaskMix is able to eliminate the influence of the internal borders of patches. By aligning the borders of the mask patches with the image patches, there thus will not be sharp rectangular borders within image patches. Here also design three approaches to explore the performance of our MaskMix: \textbf{A. Aligning mask borders to patch borders for TransMix. B. Misaligning the borders of mask patches with image patches for our MaskMix. C. Reducing the scale of each mask unit for our MaskMix to introduce more noises.} Experimental results are below:
\begin{table}[!htb]
\caption{Three approaches to exploring the performance of our MaskMix.}
\label{tabStra}
	\centering
	\begin{tabular}{c|ccccc}
    \toprule
     method  & TransMix & A & B & C & MaskMix(Ours) \\
    \midrule

    Top-1 Acc (\%)  &  72.6 & 73.1 & 73.1 &73.4 & \textbf{73.8}
    
\end{tabular}
\end{table}

We can observe that our MaskMix introduces more noisy patterns (such as B and C) lead to worse results. The TransMix eliminates the internal borders of the patches using the approach A and lifting Top-1 Acc by 0.5\%. This shows the improvements brought by the design in our MaskMix.

\textbf{Whether introducing more random patterns will further improve the performance?} We test three random patterns to evaluate the DeiT-T on ImageNet. The patterns and results are below:

\begin{table}[!htb]
\caption{Evaluated on more noisy patterns.}
\label{tabStra}
	\centering
	\begin{tabular}{lc}
    \toprule
     model  & results   \\
    \midrule

    Baseline   & 73.2   \\
    Random shuffling on the cropped patches	& 69.8\\
    Random shuffling on the patches of the canvas image & 69.7\\
    Both & 67.3\\
 
    \bottomrule
\end{tabular}
\end{table}
We can observe that damaging the original geometry of the image with these schemes leads to worse results, i.e., simply introducing random patterns (e.g., via simple random shuffling) does not bring performance improvement.

\begin{table}[!htb]
\caption{Ablation studies of different progressive factor strategies on Imagenet-1K with on DeiT-T.}
\label{tabStra}
	\centering
	\begin{tabular}{cc|c}
    \toprule
     strategy  &  & Top-1 Error (\%) \\
    \midrule

    Equal weight   & 0.5 & 27.3   \\
    
    Linear increment    &  & 27.1   \\
    
    Parabolic increment    &   & 26.9    \\
    Learnable parameter & - & 27.0  \\
    \midrule
    PAL (Ours)    & cosine distance  & \textbf{26.2}  \\
  
    \bottomrule
\end{tabular}
\end{table}

As illustrated in Table \ref{tabStra}, the progress-relevant strategies (i.e., linear increment, and parabolic increment) for generating  can yield better results than the progress-irrelevant strategy. The effects of progress-relevant strategies and learnable parameter strategy are equally. These observations prove our motivation that the model gets a low-confident attention map during the early training process, so employing the low-confident attention map for mixed labels may not be a good choice. Among these strategies, the one with the best performance for generating  is our proposed PAL.

\subsection{Visualization}
As shown in Fig. \ref{figvis}, we visualize images generated by our method and its corresponding attention map. We can observe that mixed images produced by MaskMix consist of multiple scattered and continuous areas in the image, which thus can capture broader parts of the object content of the original images from a global perspective. Furthermore, the attention maps of these mixed images also make sense, capturing the feature of objects in mixed images.
    
\begin{figure*}[t]
\centering
\includegraphics[width=1\columnwidth]{appendix.pdf}
\caption{Visualization. Top: Original images. Medium: Mixed images generated by our MaskMix. Bottom: Attention maps of mixed images.}
\label{figvis}
\end{figure*}


\subsection{Training Details on ImageNet-1K}
Our training receipt follows previous works \citep{deit, chen2021transmix}. The default setting is in Table \ref{tabImg}.

\begin{table}[!htb]
\caption{Training settings on ImageNet-1K.}

\label{tabImg}
	\centering
	\begin{tabular}{c|c}
    \toprule
     config   & value \\
    \midrule

    optimizer   & AdamW  \\
    
    learning rate    &  0.001 \\
    
    weight decay     & 0.05     \\
    
    batch size  & 1024  \\
    learning rate schedule  & cosine decay  \\
    warm-up epochs  & 20  \\
    training epochs  & 300  \\
    augmentation  & RandAug(9, 0.5)  \\
    label smoothing & 0.1\\
    drop path & 0.1\\
    MixUp & 0.8\\
    CutMix & 1\\
    MixPro & 1\\
    \bottomrule
\end{tabular}
\end{table}




\subsection{Pseudo-code}
Algorithm \ref{aa} provides the pseudo-code of MixPro in a pytorch-like style. It demonstrates that simply few lines of code can boost the performance in the plug-and-play manner.


\begin{algorithm}[H]

\caption{ Pseudo-code of MixPro in a PyTorch-like style.}
\textcolor[RGB]{122,197,205}{\# H, W: the height and width of the input image.}\\
\textcolor[RGB]{122,197,205}{\# h, w: the height and width of the attention map.}\\
\textcolor[RGB]{122,197,205}{\# M: 0/1 mask of MaskMix with shape (H,W).}\\
\textcolor[RGB]{122,197,205}{\# downsample: downsample from (H,W) to (h,w).}\\

for (x, y) in dataloader: \textcolor[RGB]{122,197,205}{\# load a mini-batch}\\

\begin{algorithmic}
\STATE         = Beta(,) \textcolor[RGB]{122,197,205}{\# Eq. (6)}\\
\STATE        M = (, ,)\\
\STATE         = sum(M)\\
\STATE         = x * M + x.flip(0) * (1-M) \textcolor[RGB]{122,197,205}{\# Eq. (7)}\\
\STATE        logits, A = model() \\

\STATE         =  * y + (1-) * y.flip(0) \textcolor[RGB]{122,197,205}{\# Eq. (7)}\\
\STATE         = (logits, ) \textcolor[RGB]{122,197,205}{\# Eq. (8)}\\

\STATE        M' = downsample(M)\\
\STATE         = matmul(A, M') \textcolor[RGB]{122,197,205}{\# Eq. (5)}\\
\STATE         =  *  + (1-) *   \textcolor[RGB]{122,197,205}{\# Eq. (9)}\\
\STATE         =   * y + (1-) * y.flip(0) \\

\STATE        CrossEntropyLoss(logits, ).backward()\\
 \end{algorithmic}
    

\label{aa}
\end{algorithm}

\end{document}
