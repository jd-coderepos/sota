\documentclass{article} \usepackage{iclr2019_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}



\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc} \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      

\usepackage{graphics}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amsmath,amsfonts,amssymb}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\usepackage{color}
\usepackage{multirow}
\usepackage{graphicx} 
\usepackage{subfigure}
\usepackage{algorithm}
\usepackage{algorithmic}

\newcommand{\rev}[1]{\color{red}#1 \color{black}}

\title{Marginalized Average Attentional Network for Weakly-Supervised Learning}



\author{Yuan Yuan\textsuperscript{12}, Yueming Lyu\textsuperscript{3}, Xi Shen\textsuperscript{4}, Ivor W. Tsang\textsuperscript{3} \& Dit-Yan Yeung\textsuperscript{1} \\\textsuperscript{1}Hong Kong University of Science and Technology, \textsuperscript{2}Alibaba Group \\\textsuperscript{3}University of Technology Sydney, \textsuperscript{4}Ecole des Ponts ParisTech }





\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}


\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
In weakly-supervised temporal action localization, previous works have failed to locate dense and integral regions for each entire action due to the overestimation of the most salient regions. To alleviate this issue, we propose a marginalized average attentional network (MAAN) to suppress the dominant response of the most salient regions in a principled manner. The MAAN employs a novel marginalized average aggregation (MAA) module and learns a set of latent discriminative probabilities in an end-to-end fashion. 
MAA samples multiple subsets from the video snippet features according to a set of latent discriminative probabilities and takes the expectation over all the averaged subset features.
Theoretically, we prove that the MAA module with learned latent discriminative probabilities successfully reduces the difference in responses between the most salient regions and the others.
Therefore, MAAN is able to generate better class activation sequences and identify dense and integral action regions in the videos. 
Moreover, we propose a fast algorithm to reduce the complexity of constructing MAA from $O(2^T)$ to $O(T^2)$. Extensive experiments on two large-scale video datasets show that our MAAN achieves a superior performance on weakly-supervised temporal action localization.


\end{abstract}



\section{Introduction}
Weakly-supervised temporal action localization has been of interest to the community recently. The setting is to train a model with solely video-level class labels, and to predict both the class and the temporal boundary of each action instance at the test time.
The major challenge in the weakly-supervised localization problem is to find the right way to express and infer the underlying location information with only the video-level class labels.
Traditionally, this is achieved by explicitly sampling several possible instances with different locations and durations~\citep{bilen2016weakly,kantorov2016contextlocnet,zhang2017co}. The instance-level classifiers would then be trained through multiple instances learning~\citep{cinbis2017weakly, yuan2017temporal} or curriculum learning~\citep{bengio2009curriculum}. 
However, the length of actions and videos varies too much such that the number of instance proposals for each video varies a lot and it can also be huge. As a result, traditional methods based on instance proposals become infeasible in many cases.


Recent research, however, has pivoted to acquire the location information by generating the class activation sequence (CAS) directly~\citep{nguyen2017weakly}, which produces the classification score sequence of being each action for each snippet over time. 
The CAS along the 1D temporal dimension for a video is inspired by the class activation map (CAM)~\citep{CAM, zhou2014object, pinheiro2015image, oquab2015object} in weakly-supervised object detection. 
The CAM-based models have shown that despite being trained on image-level labels, convolutional neural networks (CNNs) have the remarkable ability to localize objects. 
Similar to object detection, the basic idea behind CAS-based methods for action localization in the training is to sample the non-overlapping snippets from a video, then to aggregate the snippet-level features into a video-level feature, and finally to yield a video-level class prediction. 
During testing, the model generates a CAS for each class that identifies the discriminative action regions, and then applies a threshold
on the CAS to localize each action instance in terms of the start time and the end time.

In CAS-based methods, the feature aggregator that aggregates multiple snippet-level features into a video-level feature is the critical building block of weakly-supervised neural networks. A model's ability to capture the location information of an action is primarily determined by the design of the aggregators. While using the global average pooling over a full image or across the video snippets has shown great promise in identifying the discriminative regions~\citep{CAM, zhou2014object, pinheiro2015image, oquab2015object}, treating each pixel or snippet equally loses the opportunity to benefit from several more essential parts. Some recent works~\citep{nguyen2017weakly,zhu2017soft} have tried to learn attentional weights for different snippets to compute a weighted sum as the aggregated feature. However, they suffer from the weights being easily dominated by only a few most salient snippets.



In general, models trained with only video-level class labels tend to be easily responsive to small and sparse discriminative regions from the snippets of interest. This deviates from the objective of the localization task that is to locate dense and integral regions for each entire action.
To mitigate this gap and reduce the effect of the domination by the most salient regions, several heuristic tricks have been proposed to apply to existing models.
For example, ~\citep{wei2017object, zhang2018adversarial} attempt to heuristically erase the most salient regions predicted by the model which are currently being mined, and force the network to attend other salient regions in the remaining regions by forwarding the model several times. 
However, the heuristic multiple-run model is not end-to-end trainable. It is the ensemble of multiple-run mined regions but not the single model's own ability that learns the entire action regions. ``Hide-and-seek''\citep{singh2017hide} randomly masks out some regions of the input during training, enforcing the model to localize other salient regions when the most salient regions happen to be masked out. However, all the input regions are masked out with the same probability due to the uniform prior, and it is very likely that most of the time it is the background that is being masked out. A detailed discussion about related works can be found in Appendix~\ref{related_work}.

To this end, we propose the marginalized average attentional network (MAAN) to alleviate the issue raised by the domination of the most salient region in an end-to-end fashion for weakly-supervised action localization. Specifically, MAAN suppresses the action prediction response of the most salient regions by employing marginalized average aggregation (MAA) and learning the latent discriminative probability in a principled manner. 
Unlike the previous attentional pooling aggregator which calculates the weighted sum with attention weights, MAA first samples a subset of features according to their latent discriminative probabilities, and then calculates the average of these sampled features.  
Finally, MAA takes the expectation (marginalization) of the average aggregated subset features over all the possible subsets to achieve the final aggregation. As a result, MAA not only alleviates the domination by the most salient regions, but also maintains the scale of the aggregated feature within a reasonable range. 
We theoretically prove that, with the MAA, the learned latent discriminative probability indeed reduces the difference of response between the most salient regions and the others. Therefore, MAAN can identify more dense and integral regions for each action. Moreover, since enumerating all the possible subsets is exponentially expensive, we further propose a fast iterative algorithm to reduce the complexity of the expectation calculation procedure and provide a theoretical analysis.  
Furthermore, MAAN is easy to train in an end-to-end fashion since all the components of the network are differentiable. 
Extensive experiments on two large-scale video datasets show that MAAN consistently outperforms the baseline models and achieves superior performance on weakly-supervised temporal action localization. 

In summary, our main contributions include: (1) a novel end-to-end trainable marginalized average attentional network (MAAN) with a marginalized average aggregation (MAA) module in the weakly-supervised setting; (2) theoretical analysis of the properties of MAA and an explanation of the reasons MAAN alleviates the issue raised by the domination of the most salient regions; (3) a fast iterative algorithm that can effectively reduce the computational complexity of MAA; and (4) a superior performance on two benchmark video datasets, THUMOS14 and ActivityNet1.3, on the weakly-supervised temporal action localization.




















\section{Marginalized Average Attentional Network}
In this section, we describe our proposed MAAN for weakly-supervised temporal action localization.  We first derive the formulation of the feature aggregation module in MAAN as a MAA procedure in Sec.~\ref{MDA_define}. Then, we study the properties of MAA in Sec.~\ref{TwoPoposition}, and present our fast iterative computation algorithm for MAA construction in Sec.~\ref{FastComputation}. Finally, we describe our network architecture that incorporates MAA,
and introduce the corresponding inference process on weakly-supervised temporal action localization in Sec.~\ref{network_architecture}. 




\subsection{Marginalized Average Aggregation}~\label{MDA_define}

\vspace{-4mm}

  \begin{figure}[!t]
  	\begin{center}
  		\includegraphics[width=0.84\linewidth]{demonstration.pdf}
  		\caption{{An illustration of the weighted sum aggregation and the marginalized average aggregation.}}
  		\label{fig:demonstration}
  		\vspace{-4mm}
  	\end{center}
  \end{figure}




Let $\{{\bf{x}}_1,{\bf x}_2,\cdots {\bf{x}}_T\}$ denote the set of snippet-level features to be aggregated, where ${\bf x}_t \in  \mathbb{R}^m$ is the $ m $ dimensional feature representation extracted from a video snippet centered at time $ t $, and $ T $ is the total number of sampled video snippets. 
The conventional attentional weighted sum pooling aggregates the input snippet-level features into a video-level representation $\overline{\bf{x}}$. Denote the set of attentional weights corresponding to the snippet-level features as $\{\lambda_{1}, \lambda_{2}, \cdots \lambda_{T}\}$, where $\lambda_{t} $ is a scalar attentional weight for $ {\bf x}_t $. Then the aggregated video-level representation is given by
\begin{equation}
\label{x1}
\overline{\bf{x}} = \sum_{t=1}^{T}\lambda_{t}{\bf x}_t,
\end{equation}
as illustrated in Figure~\ref{fig:demonstration} (a). Different from the conventional aggregation mechanism, the proposed MAA module aggregates the features by firstly generating a set of binary indicators to determine whether a snippet should be sampled or not. The model then computes the average aggregation of these sampled snippet-level representations. Lastly, the model computes the expectation (marginalization) of the aggregated average feature for all the possible subsets, and obtains the proposed marginalized average aggregated feature.
Formally, in the proposed MAA module, we first define a set of probabilities $ \{p_{1}, p_{2}, \cdots p_{T}\}$, where each $ p_{t} \in [0,1] $ is a scalar corresponding to $ \bf x_{t} $, similar to the notation $ \lambda_{t} $ mentioned previously. We then sample a set of random variables $ \{z_{1}, z_{2}, \cdots z_{T}\}$, where $ z_{t} \sim Bernoulli(p_{t}) $, i.e., $z_{t} \in \{0,1\}$ with probability $ P(z_{t}=1) = p_{t}$. The sampled set is used to represent the subset selection of snippet-level features, in which $ z_{t}=1 $ indicates ${\bf x}_t $ is selected, otherwise not. Therefore, the average aggregation of the sampled subset of snipped-level representations is given by  $\overline{\bf{s}} = \sum\nolimits_{i = 1} ^T {{z_i}{{\bf{x}}_i}}/{\sum\nolimits_{i = 1}^T {{z_i}}}$ ,
and our proposed aggregated feature, defined as the expectation of all the possible subset-level average aggregated representations, is given by
\begin{equation}
\label{xbar}
\overline{\bf{x}} = \mathbb{E} [\overline{\bf{s}}]= \mathbb{E}\left[ \frac{\sum\nolimits_{i = 1} ^T {{z_i}{{\bf{x}}_i}}}{\sum\nolimits_{i = 1}^T {{z_i}}} \right],
\end{equation}
which is illustrated in Figure~\ref{fig:demonstration} (b).





\subsection{Partial order Preservation and Dominant Response Suppression}~\label{TwoPoposition}

Direct learning and prediction with the attention weights $\lambda$ in Eq.~(\ref{x1}) in weakly-supervised action localization leads to an over-response in the most salient regions.
The MAA in Eq.~(\ref{xbar}) has two properties that alleviate the domination effect of the most salient regions. First, the partial order preservation property, i.e., the latent discriminative probabilities preserve the partial order with respect to their attention weights. Second, the dominant response suppression property, i.e., the differences in the latent discriminative probabilities between the most salient items and others are smaller than the differences between their attention weights. The partial order preservation property guarantees that it does not mix up the action and non-action snippets by assigning a high latent discriminative probability to a snippet with low response. The dominant response suppression property reduces the dominant effect of the most salient regions and encourages the identification of dense and more integral action regions. Formally, we present the two properties in Proposition~\ref{Reduce} and Proposition~\ref{Re}, respectively. Detailed proofs can be found in Appendix~\ref{proof_reduce} and Appendix~\ref{proof_re} respectively. 






\begin{proposition}
\label{Reduce}

Let  $ z_{i} \sim Bernoulli(p_{i}) $  for $i \in \{1,...,T\}$. Then for $T \ge 2$, Eq.~(\ref{Weighted}) holds true, and ${p_i} \ge {p_j} \Leftrightarrow {c_i} \ge {c_j}  \Leftrightarrow {\lambda_i} \ge {\lambda_j}  $.  \begin{align}
\label{Weighted}
\mathbb{E}\left[ \frac{\sum\nolimits_{i = 1} ^T {{z_i}{{\bf{x}}_i}}}{\sum\nolimits_{i = 1}^T {{z_i}}} \right] =  \sum\nolimits_{i = 1} ^T {c_ip_i{\bf{x}}_i} = \sum\nolimits_{i =1}^{T}\lambda_{i}{\bf x}_i, \end{align}
where ${c_i} = \mathbb{E}\left[ {1/(1 + \sum\nolimits_{k = 1,k \ne i}^T {{z_k}} )} \right]$ and $\lambda_i = c_ip_i$ for $i \in \{1,...,T\}$.

\end{proposition}
Proposition~\ref{Reduce} shows that the latent discriminative probabilities $\{p_{i}\}$ preserve the partial order of the attention weights $\{\lambda_{i}\}$. This means that a large attention weight corresponds to a large discriminative probability, which guarantees that the latent discriminative probabilities preserve the ranking of the action prediction response.
Eq.~(\ref{Weighted}) can be seen as a factorization of the attention weight $\lambda_i$ into the multiplication of two components, $p_i$ and $c_i$, for $i  \in \{1,...,T\}$. $p_i$ is the latent discriminative probability related to the feature of snippet $ i $ itself. The factor $ c_i $ captures the contextual information of snippet $ i $ from the other snippets. This factorization can be considered to be introducing structural information into the aggregation. Factor $ c_{i} $ can be considered as performing a structural regularization for learning the latent discriminative probabilities $p_i$ for $i  \in \{1,...,T\}$, as well as for learning a more informative aggregation. 

\begin{proposition}
\label{Re}
Let  $ z_{i} \sim Bernoulli(p_{i}) $  for $i \in \{1,...,T\}$. Denote ${c_i} = \mathbb{E}\left[ {1/(1 + \sum\nolimits_{k = 1,k \ne i}^T {{z_k}} )} \right]$ and $\lambda_i = c_ip_i$  for $i \in \{1,...,T\}$.  Denote $\mathcal{I} = \left\{ {i\left| {{c_i} \ge 1/(\sum\nolimits_{t = 1}^T {{p_t}} )} \right.} \right\}$ as an index set. Then  $\mathcal{I} \ne \emptyset $  and for $\forall i  \in \mathcal{I}$, $\forall j \in \{1,...,T\} $   inequality~(\ref{gap}) holds true. \begin{align}
  \left| {\frac{{{p_i}}}{{\sum\nolimits_{t = 1}^T {{p_t}} }} - \frac{{{p_j}}}{{\sum\nolimits_{t = 1}^T {{p_t}} }}} \right| \le \left| {\frac{{{\lambda _i}}}{{\sum\nolimits_{t = 1}^T {{\lambda _t}} }} - \frac{{{\lambda _j}}}{{\sum\nolimits_{t = 1}^T {{\lambda _t}} }}} \right| \label{gap}  
\end{align}
\end{proposition}
The index set $\mathcal{I}$ can be viewed as the most salient features set.  Proposition~\ref{Re} shows that the difference between the normalized latent discriminative probabilities of the most salient regions and others is smaller than the difference between their attention weights. It means that the prediction for each snippet using the latent discriminative probability can reduce the gap between the most salient featuress and the others compared to conventional methods that are based on attention weights. Thus, MAAN suppresses the dominant responses of the most salient featuress and encourages it to identify dense and more integral action regions.  


Directly learning the attention weights $\lambda$ leans to an over response to the most salient region in weakly-supervised temporal localization. Namely, the attention weights for only a few snippets are too large and dominate the others, while attention weights for most of the other snippets that also belong to the true action are underestimated. Proposition~\ref{Re} shows that latent discriminative probabilities are able to reduce the gap between the most salient features and the others compared to the attention weights. Thus, by employing the latent discriminative probabilities for prediction instead of the attention weights, our method can alleviate the dominant effect of the most salient region in weakly-supervised temporal localization. 






\subsection{Recurrent Fast Computation} ~\label{FastComputation}

  \begin{figure}[!t]
  	\begin{center}
  		\includegraphics[width=0.5\linewidth]{matrix4.pdf}
  		\vspace{-3mm}
  		\caption{{The \textcolor{purple}{purple box} demonstrates the marginalized average aggregation module, where the inputs are $\{p_{i}\}_{i=1}^{4}$ and $ \{\bf{x}_{i}\}_{i=1}^{4}$ and the output is $ \bf{h}_{4} $. The two black boxes demonstrate the computation graphs of $q^t_i$ and $\bf{m}^{t}_{i}$, respectively. The black hollow point indicates its value is 0, while the value of the black solid point is non-zero. $q_{0}^{0}$ is initialized as 1.}}
  		\label{fig:graph}
  		\vspace{-2mm}
  	\end{center}
  \end{figure}
  \vspace{-2mm}

Given a video containing $ T $ snippet-level representations, there are $ 2^T $ possible configurations for the subset selection. Directly summing up all the $ 2^T $ configurations 
to calculate $ \overline{\bf{x}} $ has a complexity of $O(2^T)$ . In order to reduce the exponential complexity, we propose an iterative method to calculate $ \overline{\bf{x}} $ with $O(T^2)$ complexity. Let us denote the aggregated feature of $\{{\bf{x}}_1,{\bf x}_2,\cdots {\bf{x}}_t\}$ with length $ t $ as $ {{\bf{h}}_t} $, and denote ${\bf{Y}_t} = \sum\limits_{i = 1}^t {{z_i}{{\bf{x}}_i}} $ and ${Z_t} = \sum\limits_{i = 1}^t {{z_i}} $ for simplicity, then we have a set of
\begin{equation}
\label{ht}
{{\bf{h}}_t} = \mathbb{E}\left[ \frac{\sum\nolimits_{i = 1} ^t {{z_i}{{\bf{x}}_i}}}{\sum\nolimits_{i = 1}^t {{z_i}}} \right] = \mathbb{E}\left[ \frac{{{\bf{Y}_t}}}{{{Z_t}}} \right], t\in \{1,2,\cdots, T\}, 
\end{equation}
 and the aggregated feature of $ \{{\bf{x}}_1,{\bf x}_2,\cdots {\bf{x}}_T\}$ can be obtained as $ \overline{\bf{x}}  = {\bf{h}}_T $.  In Eq.~(\ref{ht}),  $Z_t$ is the summation of all the $z_{i}$, which indicates the number of elements selected in the subset. Although there are $2^t$ distinct configurations for $\{z_1,z_2,\cdots z_t\}$, it has only $t+1$ distinct values for $Z_t$, i.e. $0,1,\cdots, t$. Therefore, we can divide all the $2^t$ distinct configurations into $t+1$ groups, where the configurations sharing with the same $Z_t$ fall into the same group. Then the expectation $ {{\bf{h}}_t} $ can be calculated as the summation of the $t+1$ parts. That is, ${{\bf{h}}_t} = \mathbb{E}\left[ { \mathbb{E}\left[ {\left. {\frac{{{{\bf{Y}}_{\bf{t}}}}}{{{Z_t}}}} \right|{Z_t} = i} \right]} \right] = \sum\nolimits_{i = 0}^t {{\bf m}_i^t }$, where the $ {\bf{m}}^t_i $, indicating the $ i^{th} $ part of ${{\bf{h}}_t} $ for group $ Z_t=i $, is shown in Eq.~(\ref{m}).
\begin{align}
\label{m}
{\bf{m}}^t_i & = P\left({{Z_t=i }} \right) \mathbb{E} \left[  \left. {\frac{{{\bf{Y}_t}}}{{{Z_t}}} } \right|   {{{Z_t=i }} }   \right].
\end{align}
In order to calculate ${{\bf{h}}_{t+1}} = \sum\nolimits_{i = 0}^{t+1} {{\bf m}_i^{t+1} } $, given ${\bf{m}}_i^t\;,i \in \{0,\cdots, t\}$, we can calculate ${\bf{m}}_i^{t+1}, \; i \in \{0,1,\cdots, t+1\}$ recurrently. The key idea here is that ${\bf{m}}_i^{t + 1}$ comes from two cases: if $z_{t+1}=0$, then ${\bf{m}}_i^{t + 1}$ is the same as ${\bf{m}}_{i}^t$; if $z_{t+1}=1$, then ${\bf{m}}_i^{t + 1}$ is the weighted average of ${\bf{m}}_{i-1}^t$ and ${\bf{x}}_{t+1}$. The latter case is also related to the probability $ P\left({Z_t}  = i-1 \right) $. By denoting $q_{i-1}^t = P\left({Z_t}  = i-1 \right)$ for simplicity, we can obtain $ {\bf{m}}_i^{t+1} $ as a function of several elements:  
\begin{equation}
{\bf{m}}_i^{t+1} = f({\bf{m}}_{i-1}^t, {\bf{m}}_i^t, {\bf{x}}_{t+1}, p_{t+1}, q_{i-1}^t).
\end{equation}
Similarly, the computation of $q_i^{t+1} = P\left({Z_{t+1}}  = i \right)$ comes from two cases: the probability of selecting $ i-1 $ items from the first $ t $ items and selecting the ${(t+1)} ^{ th}$ item, i.e., $q_{i - 1}^t p_{t+1}$; and the probability of selecting $ i $ items all from the first $ t $ items and not selecting the ${(t+1)} ^{ th}$ item, i.e., $q_i^t \left( {1 - {p_{t + 1}}} \right)$. 
We derive the function of ${\bf{m}}_i^{t+1} $ and $  q_i^{t + 1}$ in Proposition~\ref{recurrent}. Detailed proofs can be found in Appendix~\ref{proof_recurrent}.
\begin{proposition}
\label{recurrent}
Let  $ z_{t} \sim Bernoulli(p_{t}) $ , ${Z_t} = \sum\limits_{i = 1}^t {{z_i}} $ and ${\bf{Y}_t} = \sum\limits_{i = 1}^t {{z_i}{{\bf{x}}_i}} $ for $t \in \{1,...,T\}$. Define ${\bf{m}}^t_i\;,i \in \{0,\cdots, t\} $ as Eq.~(\ref{m}) and $q_i^t = P\left({Z_t}  = i \right)$, then ${\bf{m}}_i^{t+1}\; i \in \{0,1,\cdots, t+1\}$ can be obtained recurrently by Eq.~(\ref{m22}) and Eq.~(\ref{q2}).  
\begin{align}
{\bf{m}}_i^{t + 1} & = {p_{t + 1}}\left( {{b_{i - 1}}{\bf{m}}_{i - 1}^t + (1 - {b_{i - 1}})q_{i - 1}^t{{\bf{x}}_{t + 1}}} \right) + (1 - {p_{t + 1}}){\bf{m}}_i^t, \label{m22} \\
q_i^{t + 1} & = {p_{t + 1}}q_{i - 1}^t + \left( {1 - {p_{t + 1}}} \right)q_i^t,\label{q2}
\end{align}
where $b_i = \frac{i}{i+1}$, $q_{-1}^t=0$, $q_{t+1}^t=0$, $ q_{0}^0=1$, ${\bf{m}}_0^t = {\bf{0}}$, and ${\bf{m}}_{t+1}^t = {\bf{0}}$.

\end{proposition}
Proposition~\ref{recurrent} provides a recurrent formula to calculate ${\bf{m}}^t_i $. With this recurrent formula, we calculate the aggregation ${\bf{h}}_T$ by iteratively calculating ${\bf{m}}^t_i$ from $i=1$ to $t$ and $t=1$ to $T$. Therefore, we can obtain the aggregated feature of $ \{{\bf{x}}_1,{\bf x}_2,\cdots {\bf{x}}_T\}$ as $ \overline{\bf{x}} =  {\bf{h}}_T = \sum\nolimits_{i = 0}^T {{\bf m}_i^T } $. The iterative computation procedure is summarized in Algorithm~\ref{MDA} in Appendix~\ref{maa_algo}. The time complexity is $ O(T^2) $.



With the fast iterative algorithm in Algorithm~\ref{MDA}, the MAA becomes practical for end-to-end training.
A demonstration of the computation graph for $ q_{i}^{t+1} $ in Eq.~(\ref{q2}) and ${\bf m}_{i}^{t+1} $ in Eq.~(\ref{m22}) is presented in the left and right-hand sides of Figure~\ref{fig:graph}, respectively. 
From Figure~\ref{fig:graph}, we can see clearly that, to compute ${\bf m}^3_2$ (the big black node on the right), it needs ${\bf m}^2_1$, ${\bf m}^2_2$, $\bf{x}_{3}$, $p_{3}$, and $q^2_1$.
The MAA can be easily implemented as a subnetwork for end-to-end training and can be used to replace the operation of other feature aggregators.









































































\begin{figure*}[t!]
\centering
\includegraphics[width=0.88\columnwidth]{T-CAM5}\vspace{-4mm}
\caption{Network architecture for the weakly-supervised action localization.}
\vspace{-4mm}
\label{fig:tcam}
\end{figure*}
\vspace{-2mm}


\subsection{Network Architecture and Temporal Action Localization}~\label{network_architecture}


\begin{figure*}[t!]
\centering
\includegraphics[width=0.53\columnwidth]{MAAN3.pdf}\vspace{-4mm}
\caption{The feature aggregators used in STPN and MAAN.}
\vspace{-2mm}
\label{Structure}
\end{figure*}
\vspace{-6mm}

\textbf{Network Architecture}: We now describe the network architecture that employs the MAA module described above for weakly-supervised temporal action localization. We start from a previous state-of-the-art base architecture, the sparse temporal pooling network (STPN) \citep{nguyen2017weakly}. As shown in Figure~\ref{fig:tcam}, it first divides the input video into several non-overlapped snippets and extracts the I3D~\citep{I3D} feature for each snippet. Each snippet-level feature is then fed to an attention module to generate an attention weight between 0 and 1. STPN then uses a feature aggregator to calculate a weighted sum of the snippet-level features with these class-agnostic attention weights to create a video-level representation, as shown on the left in Figure~\ref{Structure}. The video-level representation is then passed through an FC layer followed by a sigmoid layer to obtain class scores. Our MAAN uses the attention module to generate the latent discriminative probability $ p_{t} $ and replaces the feature aggregator from the weighted sum aggregation by the proposed marginalized average aggregation, which is demonstrated on the right in Figure~\ref{Structure}.


\textbf{Training with video-level class labels}: Formally, the model first performs aggregation of the snippet-level features (i.e. ${\bf{x}}_1,{\bf x}_2,\cdots {\bf{x}}_T$ ) to obtain the video-level representation $\bar {\bf{x}} $ ( $\bar {\bf{x}} = \mathbb{E}[{\sum\nolimits_{i = 1}^T {{z_i}} {{\bf{x}}_i}}/\sum\nolimits_{i=1}^T{{z_i}}] $). Then, it applies a logistic regression layer (FC layer + sigmoid) to output video-level classification prediction probability. Specifically, the prediction probability for class  $c \in \{1,2,\cdots C\}$ is parameterized as  $\sigma_j^c = \sigma({\bf{w}}_c^\top \overline{{\bf{x}}}_j)$, where $\overline{{\bf{x}}}_j$ is the aggregated feature for video $j \in \{1,...,N\}$. Suppose each video  $\overline{{\bf{x}}}_j$ is i.i.d and  each action class is independent from the other, the negative log-likelihood function (cross-entropy loss) is given as follows:
\begin{align}
\mathcal{L}({\bf{W} }) =  - \sum\limits_{j = 1}^N \sum\limits_{c = 1}^C   {\left( {y_j^c\log \sigma_j^c + (1 - y_j^c)\log (1 - \sigma_j^c)} \right)},
\end{align}
where $y^c_j \in \{0,1\}$ is the ground-truth video-level label for class $c$ happening in video $j$ and ${\bf{W}} =[{\bf{w}}_1,...,{\bf{w}}_C] $. 

\textbf{Temporal Action Localization}:
Let $s^c = {\bf{w}}_c^\top \overline{{\bf{x}}}$ be the video-level action prediction score, and $ \sigma( s^c) = \sigma({\bf{w}}_c^\top \overline{{\bf{x}}}) $ be the video-level action prediction probability. In STPN, as $\bar {\bf{x}} = \sum\nolimits_{t = 1}^T {{\lambda_t}{{\bf{x}}_t}}$,  the $s^c$ can be rewritten as:
\begin{align}
\label{sc}
   s^c  =  {\bf{w}}_c^\top \overline{\bf{x}}
    = \sum\nolimits_{t = 1}^T {{\lambda_t}{\bf{w}}_c^ \top {{\bf{x}}_t}},
 \end{align}
In STPN, the prediction score of snippet $t$ for action class c in a video is defined as:  
\begin{align}
\label{sct}
    s^c_t = {{\lambda_t}  \sigma( {\bf{w}}_c^ \top  {{\bf{x}}_t}}),
\end{align}
where $\sigma(\cdot)$ denotes the sigmoid function. In MAAN, as $\bar {\bf{x}} = \mathbb{E}[{\sum\nolimits_{i = 1}^T {{z_i}} {{\bf{x}}_i}}/\sum\nolimits_{i=1}^T{{z_i}}]$, according to Proposition~\ref{Reduce}, the $s^c$ can be rewritten as: \begin{align}
\label{sc}
   s^c  =  {\bf{w}}_c^\top \overline{\bf{x}} = {\bf{w}}_c^\top {\mathbb{E}[{\sum\nolimits_{i = 1}^T {{z_i}} {{\bf{x}}_i}}/\sum\nolimits_{i=1}^T{{z_i}}]}
    = \sum\nolimits_{t = 1}^T {{c_t}{p_t}{\bf{w}}_c^ \top {{\bf{x}}_t}}.
 \end{align}
The latent discriminative probability $p_t$ corresponds to the class-agnostic attention weight for snippet $ t $. According to Proposition~\ref{Reduce} and Proposition~\ref{Re}, $c_t$ does not relate to snippet $ t $, but captures the context of other snippets. $ {\bf{w}}_c $ corresponds to the class-specific weights for action class $ c $ for all the snippets, and $ {\bf{w}}_c^ \top {{\bf{x}}_t}$ indicates the relevance of snippet $ t $ to class $ c $. To generate temporal proposals, we compute the prediction score of snippet $ t $ belonging to action class $ c $ in a video as:
\begin{align}
\label{MAANscore}
    s^c_t = {{p_t} \sigma ( {\bf{w}}_c^ \top {{\bf{x}}_t}) }.
\end{align}
We denote the $ {\bf{s}}^{c} = (s^c_1, s^c_2, ..., s^c_T) \top $ as the class activation sequence (CAS) for class $ c $. Similar to STPN, the threshold is applied to the CAS for each class to extract the one-dimensional connected components to generate its temporal proposals. We then perform non-maximum suppression among temporal proposals of each class independently to remove highly overlapped detections.

Compared to STPN (Eq.~(\ref{sct})), MAAN (Eq.~(\ref{MAANscore})) employs the latent discriminative probability $p_t$ instead of directly using the attention weight $\lambda_t$ (equivalent to $ c_t p_t$) for prediction. Proposition~\ref{Re} suggests that MAAN can suppress the dominant response $s^c_t$ compared to STPN. Thus, MAAN is more likely to achieve a better performance in weakly-supervised temporal action localization.  









  




















 















\section{Experiments}
This section discusses the experiments on the weakly-supervised temporal action localization problem, which is our main focus. We have also extended our algorithm on addressing the weakly-supervised image object detection problem and the relevant experiments are presented in Appendix~\ref{exp_det}.

\subsection{Experimental Settings} 
\textbf{Datasets.} We evaluate MAAN on two popular action localization benchmark datasets, THUMOS14~\citep{THUMOS14} and ActivityNet1.3~\citep{ActivityNet}. \textbf{THUMOS14} contains 20 action classes for the temporal action localization task, which consists of 200 untrimmed videos (3,027 action instances) in the validation set and 212 untrimmed videos (3,358 action instances) in the test set. 
Following standard practice, we train the models on the validation set without using the temporal annotations and evaluate them on the test set. \textbf{ActivityNet1.3} is a large-scale video benchmark for action detection which covers a wide range of complex human activities. It provides samples from 200 activity classes with an average of 137 untrimmed videos per class and 1.41 activity instances per video, for a total of 849 video hours. This dataset contains 10,024 training videos, 4,926 validation videos and 5,044 test videos. In the experiments, we train the models on the training videos and test on the validation videos. \\
\textbf{Evaluation Metrics.} We follow the standard evaluation metric by reporting mean average precision (mAP) values at several different levels of intersection over union (IoU) thresholds. 
We use the benchmarking code provided by ActivityNet\footnote{\url{https://github.com/activitynet/ActivityNet/tree/master/Evaluation}} to evaluate the models. 

\vspace{-2mm}

\textbf{Implementation Details.}
We use two-stream I3D networks~\citep{I3D} pre-trained on the Kinetics dataset~\citep{kinetics} to extract the snippet-level feature vectors for each video. 
All the videos are divided into sets of non-overlapping video snippets. Each snippet contains 16 consecutive frames or optical flow maps. We input each 16 stacked RGB frames or flow maps into the I3D RGB or flow models to extract the corresponding 1024 dimensional feature vectors. Due to the various lengths of the videos, in the training, we uniformly divide each video into $ T $ non-overlapped segments, and randomly sample one snippet from each segment. Therefore, we sample $ T $ snippets for each video as the input of the model for training. We set $ T $ to $ 20 $ in our MAAN model. The attention module in Figure~\ref{fig:tcam} consists of an FC layer of $1024 \times 256$, a LeakyReLU layer, an FC layer of $256 \times 1$, and a sigmoid non-linear activation, to generate the latent discriminative probability $p_{t}$. We pass the aggregated video-level representation through an FC layer of $ 1024 \times C$ followed by a sigmoid activation to obtain class scores. We use the ADAM optimizer~\citep{adam} with an initial learning rate of $ 5\times 10^{-4} $ to optimize network parameters. At the test time, we first reject classes whose video-level probabilities are below $0.1$. We then forward all the snippets of the video to generate the CAS for the remaining classes. We generate the temporal proposals by cutting the CAS with a threshold $ th $. The combination ratio of two-stream modalities is set to $ 0.5 $ and $ 0.5 $. Our algorithm is implemented in PyTorch~\footnote{\url{https://github.com/pytorch/pytorch}}. We run all the experiments on a single NVIDIA Tesla M40 GPU with a 24 GB memory. 



\subsection{THUMOS14 dataset}
We first compare our MAAN model on the THUMOS14 dataset with several baseline models that use different feature aggregators in Figure~\ref{fig:tcam} to gain some basic understanding of the behavior of our proposed MAA. The descriptions of the four baseline models are listed below.


(1) \textbf{STPN.} It employs the weighed sum aggregation $ {\bar {\bf x} = \sum\nolimits_{t = 1}^T {{\lambda _t}{{\bf x}_t}} } $ to generate the video-level representation. (2) \textbf{Dropout.} It explicitly performs dropout sampling with dropout probability $p=0.5$ in STPN to obtain the video-level representation, $ {\bar {\bf x} = \sum\nolimits_{t = 1}^T {r_t{\lambda _t}{{\bf x}_t}} } $, $ r_t \sim Bernoulli(0.5)$. (3) \textbf{Normalization.} Denoted as ``Norm'' in the experiments, it utilizes the weighted average aggregation   $\bar {\bf x} = {\sum\nolimits_{t = 1}^T {{\lambda _t}{{\bf x}_t}} }/{{\sum\nolimits_{t = 1}^T {{\lambda _t}} }}$ for the video-level representation. (4) \textbf{SoftMax Normalization.} Denoted as ``SoftMaxNorm'' in the experiments, it applies the softmax function as the normalized weights to get the weighted average aggregated video-level feature,   $\bar {\bf x} = {\sum\nolimits_{t = 1}^T {e^{\lambda _t}{{\bf x}_t}} } /{{\sum\nolimits_{t = 1}^T {e^{\lambda _t}} }}$. 

\begin{table}[t]
\vspace{-4mm}
\label{tab:model_compare}
\centering
\begin{footnotesize}
\vspace{-3mm}
\caption{Comparison of the proposed MAAN with four baseline feature aggregators on the THUMOS14 test set. All values are reported in percentage. The last column is the classification mAP.}
\label{tab:model_compare}
\begin{tabular}{lcccccccccc}
\toprule
 \multirow{2}{*}{Methods} & \multicolumn{9}{c}{AP@IoU}   &    \multirow{2}{*}{Cls mAP} \\ 
                                                           & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.9 & \\ \midrule

STPN & 57.4  &  48.7 & 40.3  &  29.5 &  19.8 &  11.4  & 5.8  &  1.7   &  0.2 & 94.2 \\ 
Dropout & 53.4 &  44.9 & 35.4  & 25.0 &  16.2 &  8.7   & 4.3  & 1.3   & 0.1  & 92.4 \\ 
Norm & 48.0 & 39.9 & 30.5 & 20.9 & 12.3  & 5.7 &  2.4  &  0.6  &  0.1 & \textbf{95.2} \\  
SoftMaxNorm & 22.2 & 17.2 & 12.8 & 9.6 & 6.3  &  4.3  &  2.8 &  1.0  &  0.1 & 94.8 \\  
 MAAN & \textbf{59.8} & \textbf{50.8} & \textbf{41.1} & \textbf{30.6} & \textbf{20.3}  & \textbf{12.0}    & \textbf{6.9} & \textbf{2.6} & \textbf{0.2}  & 94.1 \\ \bottomrule
\end{tabular}
\end{footnotesize}
\end{table}


\vspace{-1mm}
We test all the models with the cutting threshold $ th $ as 0.2 of the max value of the CAS. We compare the detection average precision (\%) at IoU = [0.1 : 0.1 : 0.9] and the video-level classification mean average precision (\%) (denoted as Cls mAP) on the test set in Table~\ref{tab:model_compare}. From Table~\ref{tab:model_compare}, we can observe that although all the methods achieve a similar video-level classification mAP, their localization performances vary a lot. It shows that achieving a good video-level classification performance cannot guarantee obtaining a good snippet-level localization performance because the former only requires the correct prediction of the existence of an action, while the latter requires the correct prediction of both its existence and its duration and location. Moreover, Table~\ref{tab:model_compare} demonstrates that MAAN consistently outperforms all the baseline models at different levels of IoUs in the weakly-supervised temporal localization task. Both the ``Norm'' and ``SoftmaxNorm'' are the normalized weighted average aggregation. However, the ``SoftmaxNorm'' performs the worst, because the softmax function over-amplifies the weight of the most salient snippet. As a result, it tends to identify very few discriminative snippets and obtains sparse and non-integral localization.  The ``Norm'' also performs worse than our MAAN. It is the normalized weighted average over the snippet-level representation,  while MAAN can be considered as the normalized weighted average (expectation) over the subset-level representation. Therefore, MAAN encourages the identification of dense and integral action segments as compared to ``Norm'' which encourages the identification of only several discriminative snippets. MAAN works better than “Dropout” because “Dropout” randomly drops out the snippets with different attention weights by uniform probabilities. At each iteration, the scale of the aggregated feature varies a lot, however, MAAN samples with the learnable latent discriminative probability and conducts the expectation of keeping the scale of the aggregated feature stable. Compared to STPN, MAAN also achieves superior results. MAAN implicitly factorizes the attention weight into $c_tp_t$, where $p_t$ learns the latent discriminative probability of the current snippet, and $c_t$ captures the contextual information and regularizes the network to learn a more informative aggregation. The properties of MAA disallow the predicted class activation sequences to concentrate on the most salient regions. The quantitative results show the effectiveness of the MAA feature aggregator.




  \begin{figure}[!t]
  	\begin{center}
  		\includegraphics[width=0.9\linewidth]{Model_Compare7.pdf}
  		\vspace{-5mm}
  		\caption{{Visualization of the one-dimensional activation sequences on an example of the HammerThrow action in the test set of THUMOS14. The horizontal axis denotes the temporal dimension, which is normalized to [0, 1]. The first row of each model shows the ground-truth action segments. The second row demonstrates the predicted activation sequence for class HammerThrow.}}
  		\label{fig:Model_Compare}
  		\vspace{-6mm}
  	\end{center}
  \end{figure}

Figure~\ref{fig:Model_Compare} visualizes the one-dimensional CASs of the proposed MAAN and all the baseline models. The temporal CAS generated by MAAN can cover large and dense regions to obtain more accurate action segments. In the example in Figure~\ref{fig:Model_Compare}, MAAN can discover almost all the actions that are annotated in the ground-truth; however, the STPN have missed several action segments, and also tends to only output the more salient regions in each action segment. Other methods are much sparser compared to MAAN. 
The first row of Figure~\ref{fig:Model_Compare} shows several action segments in red and in green, corresponding to action segments that are relatively difficult and easy to be localized, respectively. We can see that all the easily-localized segments contain the whole person who is performing the ``HammerThrow'' action, while the difficultly-localized segments contain only a part of the person or the action. Our MAAN can successfully localize the easy segments as well as the difficult segments; however, all the other methods fail on the difficult ones. It shows that MAAN can identify several dense and integral action regions other than only the most discriminative region which is identified by the other methods. 

We also compare our model with the state-of-the-art action localization approaches on the THUMOS14 dataset. The numerical results are summarized in Table~\ref{tab:THUMOS14_state_of_the_art}. We include both fully and weakly-supervised learning, as in~\citep{nguyen2017weakly}.  As shown in Table~\ref{tab:THUMOS14_state_of_the_art}, our implemented STPN performs slightly better than the results reported in the original paper~\citep{nguyen2017weakly}. From Table~\ref{tab:THUMOS14_state_of_the_art}, our proposed MAAN outperforms the STPN and most of the existing weakly-supervised action localization approaches.  Furthermore, our model still presents competitive results compared with several recent fully-supervised approaches even when trained with only video-level labels.   









\begin{table}[t]
\centering
\begin{footnotesize}
\caption{Comparison of our algorithm to the previous approaches on THUMOS14 test set. AP (\%) is reported for different IoU thresholds. Both the fully-supervised and the weakly-supervised results are listed. (``UN'': using UntrimmedNet features, ``I3D'': using I3D features, ``ours'': our implementation.)}
\label{tab:THUMOS14_state_of_the_art}
\resizebox{\columnwidth}{!}{
\begin{tabular}{llccccccccc}
\toprule
\multirow{2}{*}{Supervision}       & \multirow{2}{*}{Methods} & \multicolumn{9}{c}{AP@IoU}      \\ 
                                   &                         & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.9 \\ \midrule
\multirow{8}{*}{\begin{tabular}[c]{@{}l@{}}Fully \\ Supervised\end{tabular}}
& Richard et al. \citep{richard2016temporal}          & 39.7 & 35.7 & 30.0 & 23.2 & 15.2 & -    & -    & -    & - \\ 
                                   & Shou et al. \citep{shou2016temporal}            & 47.7 & 43.5 & 36.3 & 28.7 & 19.0 & 10.3  &  5.3    & -    & -\\ 
                                   & Yeung et al. \citep{yeung2016end}           & 48.9 & 44.0 & 36.0 & 26.4 & 17.1 & -    & -    & -    & -\\  
                                   & Yuan et al.   \citep{yuan16}          & 51.4 & 42.6 & 33.6 & 26.1 & 18.8 & -    & -    & -    & -\\  
& Shou et al. \citep{cdc}            & -    & -    & 40.1 & 29.4 & 23.3 & 13.1    & 7.9    & -    & -\\ 
                                   & Yuan et al.  \citep{SMS17}           & 51.0 & 45.2 & 36.5 & 27.8 & 17.8 & -    & -    & -    & -\\  
                                   & Xu et al.   \citep{R-C3D}            & 54.5 & 51.5 & 44.8 & 35.6 & 28.9 & -    & -    & -    & -\\  
                                   & Zhao et al.  \citep{SSN}           & 66.0 & 59.4 & 51.9 & 41.0 & 29.8 & -    & -    & -    & -\\  
\midrule
\multirow{5}{*}{\begin{tabular}[c]{@{}l@{}}Weakly \\ Supervised\end{tabular}} & Wang et al. \citep{wang2017untrimmednets}             & 44.4 & 37.7 & 28.2 & 21.1 & 13.7 & -    & -    & -    & -\\ 
                                   & Singh \& Lee  \citep{singh2017hide}          & 36.4 & 27.8 & 19.5 & 12.7 & 6.8  & -    & -    & -    & -\\  
                                   & STPN~\citep{nguyen2017weakly} (UN)  & 45.3 & 38.8 & 31.1 & 23.5 & 16.2 & 9.8 & 5.1 & 2.0 & \textbf{0.3} \\
                                   & STPN~\citep{nguyen2017weakly} (I3D)       & 52.0 & 44.7 & 35.5 & 25.8 & 16.9 & 9.9 & 4.3    & 1.2   & 0.1\\ 
                                   &
                                STPN~\citep{nguyen2017weakly} (ours)  & 57.4  &  48.7 & 40.3  &  29.5 &  19.8 &  11.4  & 5.8  &  1.7   &  0.2  \\    
                                   & AutoLoc~\citep{shou2018autoloc} & - & - &  35.8 & 29.0 & \textbf{21.2} & \textbf{13.4} & 5.8 & - & - \\
                                   & \textbf{MAAN (ours)}  & \textbf{59.8} & \textbf{50.8} & \textbf{41.1} & \textbf{30.6} & 20.3  & 12.0   &  \textbf{6.9}  &  \textbf{2.6}  & 0.2 \\ \bottomrule
\end{tabular}
}
\end{footnotesize}
\vspace{-2mm}
\end{table}
\vspace{-2mm}

\begin{table}
\centering
\begin{footnotesize}
\caption{Comparison of our algorithm to the state-of-the-art approaches on ActivityNet1.3 validation set. AP (\%) is reported for different IoU threshold $\alpha$. (``ours'' means our implementation.) }
\label{tab:Anet_state_of_the_art}
\footnotesize
\begin{tabular}{llccc}
\toprule
\multirow{2}{*}{Supervision}       & \multirow{2}{*}{Methods} & \multicolumn{3}{c}{AP @ IoU} \\  
                                   &                          &  0.5     & 0.75      & 0.95   \\ \midrule
\multirow{4}{*}{Fully-supervised}  & Singh \& Cuzzolin~\citep{singh2016untrimmed}        & 34.5     & -        & -      \\
                                   & Wang \& Tao~\citep{Wang2016activitynet}              & 45.1     & 4.1      & 0.0    \\
                                   & Shou et al.~\citep{cdc}              & 45.3     & 26.0     & 0.2    \\
                                   & Xiong et al.~\citep{xiong2017pursuit}             & 39.1     & 23.5     & 5.5    \\ \midrule
\multirow{3}{*}{Weakly-supervised} & STPN~\citep{nguyen2017weakly}                     & 29.3     & 16.9     & 2.6    \\
                                   & STPN~\citep{nguyen2017weakly} (ours)             & 29.8    & 17.7    & 4.1   \\
                                   & \textbf{MAAN (ours)}                & \textbf{33.7}    & \textbf{21.9}    & \textbf{5.5}  \\ \bottomrule
\end{tabular}
\end{footnotesize}
\end{table}

\vspace{-1mm}

\subsection{ActivityNet1.3 dataset}
We train the MAAN model on the ActivityNet1.3 training set and compare our performance with the recent state-of-the-art approaches on the validation set in Table~\ref{tab:Anet_state_of_the_art}. The action segment in ActivityNet is usually much longer than that of THUMOS14 and occupies a larger percentage of a video. We use a set of thresholds, which are $ [0.2, 0.15, 0.1, 0.05] $ of the max value of the CAS, to generate the proposals from the one-dimensional CAS. As shown in Table~\ref{tab:Anet_state_of_the_art}, with the set of thresholds, our implemented STPN performs slightly better than the results reported in the original paper~\citep{nguyen2017weakly}. With the same threshold and experimental setting, our proposed MAAN model outperforms the STPN approach on the large-scale ActivityNet1.3. Similar to THUMOS14, our model also achieves good results that are close to some of the fully-supervised approaches.       




\section{Conclusion}





We have proposed the marginalized average attentional network (MAAN) for weakly-supervised temporal action localization. MAAN employs a novel marginalized average aggregation (MAA) operation to encourage the network to identify the dense and integral action segments and is trained in an end-to-end fashion. Theoretically, we have proved that MAA reduces the gap between the most discriminant regions in the video to the others, and thus MAAN generates better class activation sequences to infer the action locations. We have also proposed a fast algorithm to reduce the computation complexity of MAA. 
Our proposed MAAN achieves superior performance on both the THUMOS14 and the ActivityNet1.3 datasets on weakly-supervised temporal action localization tasks compared to current state-of-the-art methods.





\section{Acknowledgement}
We thank our anonymous reviewers for their helpful feedback and suggestions. Prof. Ivor W. Tsang was supported by ARC FT130100746, ARC LP150100671, and DP180100106. 


\bibliography{ms}
\bibliographystyle{iclr2019_conference_style}


\newpage

\newpage
\onecolumn


\title{Supplementary Material for  '' Marginalized Dropout Attention for Weakly Supervised Video Localization Feature Learning''}

\appendix


\section{Proof of Proposition \ref{Reduce}}\label{proof_reduce}
\subsection{Proof of Equation (\ref{Weighted})}
\begin{proof}
\begin{align}
\mathbb{E}\left[ \frac{\sum\nolimits_{i = 1} ^T {{z_i}{{\bf{x}}_i}}}{\sum\nolimits_{i = 1}^T {{z_i}}} \right] =  \sum\nolimits_{i = 1}^T {\mathbb{E}[{z_i}/\sum\nolimits_{i=1}^T{{z_i}} ]} {{\bf{x}}_i}.
\end{align}
In addition, 
\begin{align}
{\mathbb{E}[{z_i}/\sum\nolimits_{i=1}^T{{z_i}} ]}& = p_i \times   \mathbb{E}\left[ {1/(1 + \sum\nolimits_{k = 1,k \ne i}^T {{z_k}} )} \right]  + (1-p_i) \times 0 
 = p_ic_i.
\end{align}
Thus, we achieve  
\begin{align}
\mathbb{E}\left[ \frac{\sum\nolimits_{i = 1} ^T {{z_i}{{\bf{x}}_i}}}{\sum\nolimits_{i = 1}^T {{z_i}}} \right] = \sum\nolimits_{i = 1} ^T {c_ip_i{\bf{x}}_i}  = \sum\nolimits_{i = 1} ^T {\lambda_i{\bf{x}}_i}.
\end{align}



\end{proof}

\subsection{Proof of ${p_i} \ge {p_j} \Leftrightarrow {c_i} \ge {c_j}  \Leftrightarrow {\lambda_i} \ge {\lambda_j} $}

\begin{proof}
Denote $S_T =  \sum\nolimits_{k = 1,k \ne i, k \ne j}^T {{z_k}}  $, then we have 
\begin{align}
& {c_i} - {c_j}   = \mathbb{E}\left[ {1/(1 + \sum\nolimits_{k \ne i} {{z_k}} )} \right] - \mathbb{E}\left[ {1/(1 + \sum\nolimits_{k \ne j} {{z_k}} )} \right]\\
& = {p_j}\mathbb{E}\left[ {1/(2 + {S_T})} \right]  + (1 - {p_j})\mathbb{E}\left[ {1/(1 + {S_T})} \right]   - {p_i}\mathbb{E}\left[ {1/(2 + {S_T})} \right] - (1 - {p_i})\mathbb{E}\left[ {1/(1 + {S_T})} \right] \nonumber\\
& = ({p_i} - {p_j})\left(  \mathbb{E}\left[ {1/(1 + {S_T})} \right] -   {\mathbb{E}\left[ {1/(2 + {S_T})} \right] }     \right).
\end{align}
Since $\mathbb{E}\left[ {1/(1 + {S_T})} \right] -   {\mathbb{E}\left[ {1/(2 + {S_T})} \right] } >0$,  we achieve that ${p_i} \ge {p_j} \Leftrightarrow {c_i} \ge {c_j} $.
Since $\lambda_i = c_ip_i$ and $\lambda_j = c_jp_j$, and $c_i,c_j,p_i,p_j \ge 0$, it follows that ${p_i} \ge {p_j} \Leftrightarrow {\lambda_i} \ge {\lambda_j} $.

\end{proof}

\section{Proof of Proposition~\ref{Re}}\label{proof_re} 

\begin{proof}


$\sum\nolimits_{i = 1} ^T {c_ip_i}   =   \sum\nolimits_{i = 1}^T {\mathbb{E}[{z_i}/\sum\nolimits_{i=1}^T{{z_i}} ]} = \mathbb{E}\left[ (\sum\nolimits_{i=1}^T{{z_i}})/(\sum\nolimits_{i=1}^T{{z_i}}) \right]   =1$

When $p_1 = p_2 = \cdot \cdot  \cdot = p_T$, we have $\lambda_1 = \lambda_2 = \cdot \cdot  \cdot = \lambda_T$. Then inequality (\ref{gap}) trivially holds true.
Without loss of generality, assume $p_1 \ge p_2 \ge \cdot \cdot  \cdot \ge p_T$ and there exists a strict inequality.  Then $\exists k \in \{1,...,T-1 \}$ such that $c_i \ge 1/({\sum\nolimits_{t = 1}^T {{p_t}} })$ for $ 1 \le i \le k$ and $c_j \le 1/({\sum\nolimits_{t = 1}^T {{p_t}} })$ for   $ k < j \le T$. Otherwise, we obtain  $c_i \ge 1/({\sum\nolimits_{t = 1}^T {{p_t}} })$  or  $c_i \le 1/({\sum\nolimits_{t = 1}^T {{p_t}} })$ for $ 1 \le i \le T$ and there exists a strict inequality. It follows that    $ \sum\nolimits_{i=1}^T{c_ip_i} > 1$ or  $ \sum\nolimits_{i=1}^T{c_ip_i} < 1$, which  contradicts  $ \sum\nolimits_{i=1}^T{c_ip_i} = 1$.  Thus, we obtain the set  $\mathcal{I} \ne \emptyset  $.


Without loss of generality, 
for $ 1 \le i \le k$ and $i \le j \le T$ , we  have $c_i \ge 1/({\sum\nolimits_{t = 1}^T {{p_t}} })$ and $p_i \ge p_j$, then we obtain that $c_i \ge c_j$. It follows that 
\begin{align}
& {p_i}/(\sum\nolimits_{t = 1}^T {{p_t}} ) - {p_j}/(\sum\nolimits_{t = 1}^T {{p_t}} ) - \left( {{\lambda _i}/(\sum\nolimits_{t = 1}^T {{\lambda _t}} ) - {\lambda _j}/(\sum\nolimits_{t = 1}^T {{\lambda _t}} )} \right)\\
& = {p_i}/(\sum\nolimits_{t = 1}^T {{p_t}} ) - {p_j}/(\sum\nolimits_{t = 1}^T {{p_t}} ) - \left( {{c_i}{p_i} - {c_j}{p_j}} \right)\\
& = \left( {1/(\sum\nolimits_{t = 1}^T {{p_t}} ) - {c_i}} \right){p_i} - \left( {1/(\sum\nolimits_{t = 1}^T {{p_t}} ) - {c_j}} \right){p_j}\\
& \le \left( {1/(\sum\nolimits_{t = 1}^T {{p_t}} ) - {c_i}} \right){p_i} - \left( {1/(\sum\nolimits_{t = 1}^T {{p_t}} ) - {c_i}} \right){p_j}\\
& = \left( {1/(\sum\nolimits_{t = 1}^T {{p_t}} ) - {c_i}} \right)\left( {{p_i} - {p_j}} \right) \le 0.
\end{align}
















\end{proof}



\section{Proof of Proposition~\ref{recurrent}}\label{proof_recurrent}

\subsection{Computation of ${\bf h}_t$}

\begin{align}
{{\bf h}_t} = E[\frac{{{\bf{Y}_t}}}{{{Z_t}}}] & = \sum\limits_{{z_1},{z_2},...,{z_t}} {P\left( {{z_1},{z_2},\cdots{z_t}} \right)\frac{{\sum\nolimits_{j = 1}^t {{z_j}{{\bf x}_j}} }}{{\sum\nolimits_{j = 1}^t {{z_j}} }}}  \\
& =  \sum\nolimits_{i = 0}^t {\left( {\sum\limits_{{z_1},{z_2},\cdots {z_t}} {{\bf{1}}\left( {\sum\nolimits_{j = 1}^t {{z_j} = i} } \right)P\left( {{z_1},{z_2},...,{z_t}} \right)\frac{{\sum\nolimits_{j = 1}^t {{z_j}{{\bf x}_j}} }}{{\sum\nolimits_{j = 1}^t {{z_j}} }}} } \right)}  \label{eq12} \\
& = \sum\nolimits_{i = 0}^t {\sum\limits_{{z_1},{z_2},...,{z_t}} {{\bf{1}}\left( {\sum\nolimits_{j = 1}^t {{z_j} = i} } \right)P\left( {{z_1},{z_2},\cdots{z_t}} \right)\frac{{\sum\nolimits_{j = 1}^t {{z_j}{{\bf x}_j}} }}{i}} } \\
& = \sum\nolimits_{i = 0}^t {{\bf m}_i^t} \label{eq14},
\end{align}
where $\bf{1}(\cdot)$ denotes the indicator function.

We achieve Eq.~(\ref{eq12}) by partitioning the summation into $t+1$ groups . Terms belonging to group $i$ have ${\sum\nolimits_{j = 1}^t {{z_j}} } = i$.

Let ${\bf m}^t_i = {\sum\limits_{{z_1},{z_2},\cdots{z_t}} {{\bf{1}}\left( {\sum\nolimits_{j = 1}^t {{z_j} = i} } \right)P\left( {{z_1},{z_2},\cdots {z_t}} \right)\frac{{\sum\nolimits_{j = 1}^t {{z_j}{{\bf x}_j}} }}{i}} } $, and we achieve Eq.~(\ref{eq14}).


\subsection{Proof of Recurrent formula of $m^{t+1}_i$}
We now give the proof of the recurrent formula of Eq.~(\ref{eqm}) 
\begin{align}
\label{eqm}
{\bf m}_i^{t + 1} = {p_{t + 1}}\left( {{b_{i - 1}}{\bf m}_{i - 1}^t + (1 - {b_{i - 1}})q_{i - 1}^t{{\bf{x}}_{t + 1}}} \right) + (1 - {p_{t + 1}}){\bf m}_i^t.
\end{align}

\begin{proof}



\begin{align}
{\bf m}^{t+1}_i & = \sum\limits_{{z_1},{z_2},\cdots{z_t},{z_{t + 1}}} {{\bf{1}}\left( {\sum\nolimits_{j = 1}^{t + 1} {{z_j} = i} } \right)P\left( {{z_1},{z_2},\cdots{z_{t + 1}}} \right)\frac{{\sum\nolimits_{j = 1}^{t + 1} {{z_j}{{\bf x}_j}} }}{i}}  \\
& = \sum\limits_{{z_1},{z_2},\cdots{z_t},{z_{t + 1}}} {{\bf{1}}\left( {\sum\nolimits_{j = 1}^t {{z_j} + {z_{t + 1}} = i} } \right)P\left( {{z_1},{z_2},\cdots{z_t}} \right)P({z_{t + 1}})\frac{{\sum\nolimits_{j = 1}^t {{z_j}{{\bf x}_j} + {z_{t + 1}}{{\bf x}_{t + 1}}} }}{i}}   \\
& = \begin{array}{l}
\sum\limits_{{z_1},{z_2},\cdots{z_t}} {\left[ {{\bf{1}}\left( {\sum\nolimits_{j = 1}^t {{z_j} + 1 = i} } \right)P\left( {{z_1},{z_2},\cdots{z_t}} \right){p_{t + 1}}\frac{{\sum\nolimits_{j = 1}^t {{z_j}{{\bf x}_j} + {{\bf x}_{t + 1}}} }}{i}} \right]} \\
 + \sum\limits_{{z_1},{z_2},\cdots{z_t}} {{\bf{1}}\left( {\sum\nolimits_{j = 1}^t {{z_j} = i} } \right)P\left( {{z_1},{z_2},\cdots{z_t}} \right)(1 - {p_{t + 1}})\frac{{\sum\nolimits_{j = 1}^t {{z_j}{{\bf x}_j}} }}{i}} 
\end{array} \\
& = \begin{array}{l}
  \sum\limits_{{z_1},{z_2},\cdots{z_t}} {{\bf{1}}\left( {\sum\nolimits_{j = 1}^t {{z_j} + 1 = i} } \right)P\left( {{z_1},{z_2},\cdots{z_t}} \right){p_{t + 1}}\frac{{\sum\nolimits_{j = 1}^t {{z_j}{{\bf x}_j} + {{\bf x}_{t + 1}}} }}{i}} \\
 + (1 - {p_{t + 1}})\sum\limits_{{z_1},{z_2},\cdots{z_t}} {{\bf{1}}\left( {\sum\nolimits_{j = 1}^t {{z_j} = i} } \right)P\left( {{z_1},{z_2},\cdots{z_t}} \right)\frac{{\sum\nolimits_{j = 1}^t {{z_j}{{\bf x}_j}} }}{i}} 
\end{array}\\
& =  \begin{array}{l}
{p_{t + 1}}\sum\limits_{{z_1},{z_2}, \cdots {z_t}} {{\bf{1}}\left( {\sum\nolimits_{j = 1}^t {{z_j} = i - 1} } \right)P\left( {{z_1},{z_2}, \cdots {z_t}} \right)\frac{{i - 1}}{i}\frac{{\sum\nolimits_{j = 1}^t {{z_j}{{\bf{x}}_j} + {{\bf{x}}_{t + 1}}} }}{{i - 1}}} \\
 + (1 - {p_{t + 1}})m_i^t
\end{array} \\
  & = \begin{array}{l}
{p_{t + 1}}\sum\limits_{{z_1},{z_2}, \cdots {z_t}} {{\bf{1}}\left( {\sum\nolimits_{j = 1}^t {{z_j} = i - 1} } \right)P\left( {{z_1},{z_2}, \cdots {z_t}} \right)\left[ {\frac{{i - 1}}{i}\frac{{\sum\nolimits_{j = 1}^t {{z_j}{{\bf{x}}_j}} }}{{i - 1}} + \frac{{{{\bf{x}}_{t + 1}}}}{i}} \right]} \\
 + (1 - {p_{t + 1}}){\bf{m}}_i^t
\end{array} \\
  & =  \begin{array}{l}
{p_{t + 1}}\sum\limits_{{z_1},{z_2}, \cdots {z_t}} {{\bf{1}}\left( {\sum\nolimits_{j = 1}^t {{z_j} = i - 1} } \right)P\left( {{z_1},{z_2}, \cdots {z_t}} \right)\left[ {{b_{i - 1}}\frac{{\sum\nolimits_{j = 1}^t {{z_j}{{\bf{x}}_j}} }}{{i - 1}} + (1 - {b_{i - 1}}){{\bf{x}}_{t + 1}}} \right]} \\
 + (1 - {p_{t + 1}}){\bf{m}}_i^t
\end{array}
   \end{align}
Then, we have 
\begin{align}
{\bf m}^{t+1}_i & = \begin{array}{l}
{p_{t + 1}}{b_{i - 1}}\sum\limits_{{z_1},{z_2},\cdots{z_t}} {{\bf{1}}\left( {\sum\nolimits_{j = 1}^t {{z_j} = i - 1} } \right)P\left( {{z_1},{z_2},\cdots{z_t}} \right)} \frac{{\sum\nolimits_{j = 1}^t {{z_j}{{\bf x}_j}} }}{{i - 1}}\\  +
{p_{t + 1}}(1 - {b_{i - 1}})\sum\limits_{{z_1},{z_2},\cdots{z_t}} {{\bf{1}}\left( {\sum\nolimits_{j = 1}^t {{z_j} = i - 1} } \right)P\left( {{z_1},{z_2},\cdots{z_t}} \right)} {{\bf x}_{t + 1}} + (1 - {p_{t + 1}}) {\bf m}_i^t.
\end{array} \end{align}

Since  $q_{i-1}^t = P\left( {\sum\nolimits_{j = 1}^t {{z_j}}  = i-1} \right) = \sum\limits_{{z_1},{z_2},\cdots{z_t}} {{\bf{1}}\left( {\sum\nolimits_{j = 1}^t {{z_j} = i - 1} } \right)P\left( {{z_1},{z_2},\cdots{z_t}} \right)}$
we can achieve
\begin{align}
{\bf m}^{t+1}_i =  {p_{t + 1}}\left[ {{b_{i - 1}}{\bf m}_{i - 1}^t + (1 - {b_{i - 1}})q_{i - 1}^t{{\bf x}_{t + 1}}} \right] + (1 - {p_{t + 1}}){\bf m}_i^t.
\end{align}

\end{proof}

\subsection{Proof of recurrent formula of $q^{t+1}_i$}

We present the proof of Eq.~(\ref{Eqq})
\begin{align}
q_i^{t + 1} = {p_{t + 1}}q_{i - 1}^t + (1 - {p_{t + 1}})q_i^t
\label{Eqq}
\end{align}

\begin{proof}
\begin{align}
q_i^{t + 1} & = \sum\limits_{{z_1},{z_2},\cdots{z_t},{z_{t + 1}}} {{\bf{1}}\left( {\sum\nolimits_{j = 1}^{t + 1} {{z_j} = i} } \right)P\left( {{z_1},{z_2},\cdots{z_{t + 1}}} \right)} \\
& = \sum\limits_{{z_1},{z_2},\cdots{z_t},{z_{t + 1}}} {{\bf{1}}\left( {\sum\nolimits_{j = 1}^t {{z_j} + {z_{t + 1}} = i} } \right)P\left( {{z_1},{z_2},\cdots{z_t}} \right)P({z_{t + 1}})} \\ 
& = \sum\limits_{{z_1},{z_2},\cdots{z_t}} {{\bf{1}}\left( {\sum\nolimits_{j = 1}^t {{z_j} + 1 = i} } \right)P\left( {{z_1},{z_2},\cdots{z_t}} \right){p_{t + 1}}  } \\   & + \sum\limits_{{z_1},{z_2},\cdots{z_t}} {{\bf{1}}\left( {\sum\nolimits_{j = 1}^t {{z_j} = i} } \right)P\left( {{z_1},{z_2},\cdots{z_t}} \right)(1 - {p_{t + 1}})} \\
&  = {p_{t + 1}}\sum\limits_{{z_1},{z_2},\cdots{z_t}} {{\bf{1}}\left( {\sum\nolimits_{j = 1}^t {{z_j} = i - 1} } \right)P\left( {{z_1},{z_2},\cdots{z_t}} \right)}  + (1 - {p_{t + 1}})q_i^t \\
& = {p_{t + 1}}q_{i - 1}^t + (1 - {p_{t + 1}})q_i^t
\end{align}


\end{proof}


\section{Related Work}\label{related_work}

\textbf{Video Action Analysis.} 
Researchers have developed quite a few deep network models for video action analysis. Two-stream networks~\citep{simonyan2014two} and 3D convolutional neural networks (C3D)~\citep{tran2015learning} are popular solutions to learn video representations and these techniques, including their variations, are extensively used for video action analysis. Recently, a combination of two-stream networks and 3D convolutions, referred to as I3D~\citep{I3D}, was proposed as a generic video representation learning method, and served as an effective backbone network in various video analysis tasks such as recognition~\citep{wang2016temporal}, localization~\citep{shou2016temporal}, and weakly-supervised learning~\citep{wang2017untrimmednets}.

\textbf{Weakly-Supervised Temporal Action Localization.}\label{wstad} 
There are only a few approaches based on weakly-supervised learning that rely solely on video-level class labels to localize actions in the temporal domain. Wang et al.~\citep{wang2017untrimmednets} proposed a UntrimmedNet framework, where two softmax functions are applied across class labels and proposals to perform action classification and detect important temporal segments, respectively. However, using the softmax function across proposals may not be effective for identifying multiple instances. Singh et al.~\citep{singh2017hide} designed a Hide-and-Seek model to randomly hide some regions in a video during training and force the network to seek other relevant regions. However, the randomly hiding operation, as a data augmentation, cannot guarantee whether it is the action region or the background region that is hidden during training, especially when the dropout probabilities for all the regions are the same. Nguyen et al.~\citep{nguyen2017weakly} proposed a sparse temporal pooling network (STPN) to identify a sparse set of key segments associated with the actions through attention-based temporal pooling of video segments. However, the sparse constraint may force the network to focus on very few segments and lead to incomplete detection. In order to prevent the model from focusing only on the most salient regions, we are inspired to propose the MAAN model to explicitly take the expectation with respect to the average aggregated features of all the sampled subsets from the video. 




\textbf{Feature Aggregators.}
Learning discriminative localization representations with only video-level class labels requires the feature aggregation operation to turn multiple snippet-level representations into a video-level representation for classification. The feature aggregation mechanism is widely adopted in the deep learning literature and a variety of scenarios, for example, neural machine translation~\citep{machinetrans}, visual question answering~\citep{hermann2015teaching}, and so on. However, most of these cases belong to fully-supervised learning where the goal is to learn a model that attends the most relevant features given the supervision information corresponding to the task directly. Many variant feature aggregators have been proposed, ranging from non-parametric max pooling and average pooling, to parametric hard attention~\citep{gkioxari2015contextual}, soft attention~\citep{attention, sharma2015action}, second-order pooling~\citep{girdhar2017attentional, kong2017low}, structured attention~\citep{kim2017structured, mensch2018differentiableDP}, graph aggregators~\citep{zhang2018gaan, hamilton2017inductive}, and so on. Different from the fully-supervised setting where the feature aggregator is designed for the corresponding tasks, we develop a feature aggregator that is trained only with class labels, and then to be used to predict the dense action locations for test data. Different from the heuristic approaches~\citep{wei2017object, zhang2018adversarial} which can be considered as a kind of hard-code attention by erasing some regions with a hand-crafted threshold, we introduce the end-to-end differentiable marginalized average aggregation which incorporates learnable latent discriminative probabilities into the learning process.


\section{Marginalized Average Aggregation}\label{maa_algo}

\begin{algorithm}[h]
   \caption{Marginalized Average Aggregation}
   \label{MDA}
\begin{algorithmic}
   \STATE \textbf{Input:} Feature Representations  $\{{\bf{x}}_1,{\bf{x}}_2,\cdots{\bf{x}}_T \}$ , Sampling Probability $\{p_1,p_2,\cdots p_T\}$.
   \STATE \textbf{Output:} Aggregated Representation $\bf{\overline{x}}$
   \STATE Initialize ${\bf m}_0^0 = \bf{0}$, $q^0_0=1$, $b_i= \frac{i}{i+1}$;
   
   \FOR{$t=1$ {\bfseries to} $T$}
   \STATE Set  ${\bf{m}}_0^t = \bf{0}$, and     $q_{-1}^t=0$ and $q_{t+1}^t=0$;
   \FOR{$i=1$ {\bfseries to} $t$}
   \STATE $q_i^{t} = {p_{t }}q_{i - 1}^{t-1} + \left( {1 - {p_{t }}} \right)q_i^{t-1}$
   \STATE ${\bf{m}}_i^{t } = {p_{t }}\left( {{b_{i - 1}}{\bf{m}}_{i - 1}^{t-1} + (1 - {b_{i - 1}})q_{i - 1}^{t-1}{{\bf{x}}_{t }}} \right) + (1 - {p_{t }}){\bf{m}}_i^{t-1}$     
   \ENDFOR
   \ENDFOR
   \STATE Return $\overline{\bf{x}} = \sum\limits_{i = 0}^{T}  {{\bf{m}}_i^T} $
\end{algorithmic}

\end{algorithm}

\section{Experiments on Weakly-Supervised Image Object Localization}\label{exp_det}
\subsection{Models and Implementation Details}
We also evaluate the proposed model on the weakly-supervised object localization task. For weakly-supervised object localization, we are given a set of images in which each image is labeled only with its category label. The goal is to learn a model to predict both the category label as well as the bounding box for the objects in a new test image. 

Based on the model in ~\citep{CAM} (denoted as CAM model), we replace the global average pooling feature aggregator with other kinds of feature aggregator, such as the weighted sum pooling and the proposed MAA by extending the original 1D temporal version in temporal action localization into a 2D spatial version. We denote the model with weighted sum pooling as the weighted-CAM model. For the weighted-CAM model and the proposed MAAN model, we use an attention module to generate the attention weight $\lambda$ in STPN or the latent discriminative probability $ p $ in MAAN. The attention module consists of a 2D convolutional layer of kernel size $ 1 \times 1 $, stride 1 with 256 units, a LeakyReLU layer, a 2D convolutional layer of kernel size $ 1 \times 1 $, stride 1 with 1 unit, and a sigmoid non-linear activation. 


\subsection{Dataset and Evaluation Metric}
We evaluate the weakly-supervised localization accuracy of the proposed model on the CUB-200-2011 dataset~\citep{WahCUB_200_2011}.  The CUB-200-2011 dataset has 11,788 images of 200 categories with 5,994 images for training and 5,794 for testing. We leverage the localization metric suggested by \citep{russakovsky2015imagenet} for comparison. This metric computes the percentage of images that is misclassified or with bounding boxes with less than $50\%$ IoU with the groundtruth as the localization error.

\subsection{Comparisons}

\begin{table*}[t]
\centering
\caption{ Localization error on CUB-200-2011 test set}
\begin{tabular}{c|c|c}
\hline
Methods          & top1 err@IoU0.5 & top5 err@IoU0.5 \\ \hline
GoogLeNet-GAP (\citep{zhou2016cvpr})              & 59.00   & -           \\ \hline
weighted-CAM 4x4  & 58.51         & 51.73               \\ weighted-CAM 7x7  & 58.11          & 50.21              \\ 
MAAN 4x4        & 55.90         & 47.60               \\
MAAN 7x7          & 53.94          & 44.13               \\ \hline \end{tabular}
\label{localizationCub}
\end{table*}
We compare our MAA aggregator (MAAN) with the weighted sum pooling (weighted-CAM) and global average pooling (CAM \citep{zhou2016cvpr}). For MAAN and weighted-CAM, we pool the convolutional feature for aggregation into two different sizes, $ 4 \times 4 $ and $ 7 \times 7 $. 
We fix all other factors (e.g. network structure, hyper-parameters, optimizer), except for the feature aggregators to evaluate the models. 

\subsubsection{Qualitative Results}

The localization errors for different methods are presented in Table~\ref{localizationCub}, where the GoogLeNet-GAP is the CAM model. Our method outperforms GoogLeNet-GAP by $ 5.06\%$ in a Top-1 error. Meanwhile, MAAN achieves consistently lower localization error than weighted-CAM on the two learning schemes. It demonstrates that the proposed MAAN can improve the localization performance in the weakly-supervised setting. Moreover, both MAAN and weighted-CAM obtain smaller localization error when employing the $7 \times 7$ learning scheme than the $4 \times 4$ learning scheme. 





\subsubsection{Visualization}




\begin{figure*}[t!]
\centering
\includegraphics[width=1\columnwidth]{HeatMap.png}\caption{Comparison with the baseline methods. The proposed MAAN can locate larger object regions to improve localization performance (ground-truth bounding boxes are in red and the predicted ones are in green).}
\label{fig:loc}
\end{figure*}



Figure~\ref{fig:loc} visualizes the heat maps and localization bounding boxes obtained by all the compared methods. The object localization heat maps generated by the proposed MAAN can cover larger object regions and obtain more accurate bounding boxes. 















































 
\end{document}
