\documentclass[lettersize,journal]{IEEEtran}

\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}

\usepackage{enumerate}
\usepackage{booktabs}
\usepackage{microtype}
\usepackage{pifont}

\usepackage{xcolor}
\definecolor{brickred}{rgb}{0.8, 0.25, 0.33}
\definecolor{blueish}{rgb}{0.0, 0.3, 0.6}

\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,citecolor=blueish,linkcolor=brickred,bookmarks=false]{hyperref}

\hyphenation{GraphMLP}

\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}

\begin{document}

\title{GraphMLP: A Graph MLP-Like Architecture for 3D Human Pose Estimation}

\author{
Wenhao Li, Hong Liu, Tianyu Guo, Runwei Ding, and Hao Tang
\thanks{Wenhao Li, Hong Liu, Tianyu Guo, and Runwei Ding are with Key Laboratory of Machine Perception, Peking University, Shenzhen Graduate School, China. 
E-mail: \{wenhaoli,hongliu,dingrunwei\}@pku.edu.cn, levigty@stu.pku.edu.cn.}
\thanks{Hao Tang is with the Computer Vision Lab, ETH Zurich, Switzerland. 
E-mail: hao.tang@vision.ee.ethz.ch.}
\thanks{Corresponding author: Hong Liu.}
\thanks{This work is supported by the National Natural Science Foundation of China (No. 62073004), Basic and Applied Basic Research Foundation of Guangdong (No. 2020A1515110370), Shenzhen Fundamental Research Program (No. JCYJ20200109140410340, GXWD20201231165807007-20200807164903001).}
}

\markboth{}
{Li \MakeLowercase{\textit{et al.}}: 
GraphMLP: A Graph MLP-Like Architecture for 3D Human Pose Estimation}

\maketitle

\begin{abstract}
Modern multi-layer perceptron (MLP) models have shown competitive results in learning visual representations without self-attention. However, existing MLP models are not good at capturing local details and lack prior knowledge of human body configurations, which limits their modeling power for skeletal representation learning. To address these issues, we propose a simple yet effective graph-reinforced MLP-Like architecture, named GraphMLP, that combines MLPs and graph convolutional networks (GCNs) in a \textit{global-local-graphical} unified architecture for 3D human pose estimation. GraphMLP incorporates the graph structure of human bodies into an MLP model to meet the domain-specific demand of the 3D human pose, while allowing for both local and global spatial interactions. Furthermore, we propose to flexibly and efficiently extend the GraphMLP to the video domain and show that complex temporal dynamics can be effectively modeled in a simple way with negligible computational cost gains in the sequence length. To the best of our knowledge, this is the first MLP-Like architecture for 3D human pose estimation in a single frame and a video sequence. Extensive experiments show that the proposed GraphMLP achieves state-of-the-art performance on two datasets, \emph{i.e.}, Human3.6M and MPI-INF-3DHP. Code and models are available at \url{https://github.com/Vegetebird/GraphMLP}. 
\end{abstract}

\begin{IEEEkeywords}
3D Human Pose Estimation, Multi-Layer Perceptron, Graph Convolutional Network
\end{IEEEkeywords}

\section{Introduction}
\IEEEPARstart{3D} Human pose estimation from images is important in numerous applications, such as action recognition, motion capture, and augmented/virtual reality. 
Most existing works solve this task by using a 2D-to-3D pose lifting method, which takes graph-structured 2D joint coordinates detected by a 2D keypoint detector as input~\cite{simplebaseline,wei2019view,chen2021anatomy,hua2022weakly}. 
This is an inherently ambiguous problem since multiple valid 3D joint locations may correspond to the same 2D projection in the image space. 
However, it is practically solvable since 3D poses often lie on low-dimensional manifolds, which can provide important structural priors to mitigate the depth ambiguity~\cite{wang2014robust,ci2019optimizing}. 

\begin{figure}[tb]
  \centering
  \includegraphics[width=1.00\linewidth]{figure/compare.pdf}
  \caption
  {Performance comparison with MLP-Mixer~\cite{mlpmixer} and GCN~\cite{stgcn} on Human3.6M (a) and MPI-INF-3DHP (b) datasets. 
  The proposed GraphMLP absorbs the advantages of modern MLPs and GCNs to effectively learn skeletal representations, consistently outperforming each of them.  
  The evaluation metric is MPJPE (the lower the better). 
  }
  \label{fig:compare}
\end{figure}

Early works attempt to employ fully-connected networks (FCN)~\cite{simplebaseline} to lift the 2D joints into 3D space. 
However, the dense connection of FCN often results in overfitting and poor performance~\cite{zeng2021learning}. 
To address this problem, recent works consider that the skeleton of a human body can be naturally represented as the graph structure and utilize graph convolutional networks (GCNs) for this task~\cite{zhao2019semantic,wang2020motion,hu2021conditional}. 
Although GCN-based methods are effective in aggregating neighboring nodes to extract local features, they typically rely on learning relationships between human body joints utilizing first-order neighbors, while ignoring higher-order neighbors. 
However, global information between distant body joints is crucial for understanding overall body posture and movement patterns. For instance, the position of the hand relative to the foot can indicate whether the body is standing, sitting, or lying down. 
One way to capture global information is by stacking multiple GCN layers, which expands the model's receptive fields to cover the entire kinematic chain of a human.
However, this can be computationally expensive and may lead to valuable information loss in deeper layers. 

\begin{figure*}[tb]
\centering
\includegraphics[width=0.9\linewidth]{figure/overview.pdf}
\caption
{
Overview of the proposed GraphMLP architecture. 
The left illustrates the skeletal structure of the human body. 
The 2D joint inputs detected by a 2D pose estimator are sparse and graph-structured data.  
GraphMLP treats each 2D keypoint as an input token, linearly embeds each of them through the skeleton embedding, feeds the embedded tokens to GraphMLP layers, and finally performs regression on resulting features to predict the 3D pose via the prediction head. 
Each GraphMLP layer contains one spatial graph MLP (SG-MLP) and one channel graph MLP (CG-MLP). 
For easy illustration, we show the architecture using a single image as input. 
}
\label{fig:overview}
\end{figure*}

Recently, Transformers~\cite{transformer} with global receptive fields have shown promising advances on various vision tasks~\cite{vit,yuan2021temporal,liu2021swinnet,yang2021transformer}. 
Nevertheless, the strong performance of Transformers comes at a relatively high computational cost in the self-attention block whose complexity increases quadratically with the number of input sequences. 
Considering this, recent progress on modern multi-layer perceptron (MLP) models, in particular MLP-Mixer~\cite{mlpmixer}, provides new architectural designs in vision. 
The MLP model stacks only fully-connected layers without self-attention and consists of two types of blocks. 
The spatial MLP aggregates global information among tokens, and the channel MLP focuses on extracting features for each token. 
By stacking these two MLP blocks, it can be simply built with less inductive bias and achieves impressive performance in learning visual representations. 
This motivates us to explore an MLP-Like architecture for skeleton-based representation learning. 

However, there remain two critical challenges in adapting the MLP models from vision to skeleton:
\textbf{(i)} Despite their successes in vision, existing MLPs are less effective in modeling graph-structured data due to their simple connections among all nodes. 
Different from RGB images represented by highly dense pixels, skeleton inputs are inherently sparse and graph-structured data (see Fig.~\ref{fig:overview} (left)). 
Without incorporating the prior knowledge of human body configurations, the model is prone to learning spurious dependencies, which can lead to physically implausible poses. 
\textbf{(ii)} 
While such models are capable of capturing global interactions between distant body joints (such as the hand and foot) via their spatial MLPs, they may not be good at capturing local interactions due to the lack of careful designs for modeling relationships between adjacent joints (such as the head and neck). 
However, local information is also essential for 3D human pose estimation, as it can help the model to understand fine-grained movement details. 
For example, the movement of the head relative to the neck can indicate changes in gaze direction or subtle changes in posture. 

To overcome both limitations, we present GraphMLP, a graph-reinforced MLP-Like architecture for 3D human pose estimation, as depicted in Fig.~\ref{fig:overview}. 
Our GraphMLP is conceptually simple yet effective: it builds a strong collaboration between modern MLPs and GCNs to construct a \textit{global-local-graphical} unified architecture for learning better skeletal representations. 
Specifically, the GraphMLP mainly contains a stack of novel GraphMLP layers. 
Each layer consists of two Graph-MLP blocks where the spatial graph MLP (SG-MLP) and the channel graph MLP (CG-MLP) blocks are built by injecting GCNs into the spatial MLP and channel MLP, respectively. 
By combining MLPs and GCNs in a unified architecture, our GraphMLP is able to obtain the prior knowledge of human configurations encoded by the graphâ€™s connectivity and capture both local and global spatial interactions among body joints, hence yielding better performance.

Furthermore, we extend our GraphMLP from a single frame to a video sequence. 
Most existing video-based methods~\cite{videopose,poseformer,mhformer} typically model temporal information by treating each frame as a token or treating the time axis as a separated dimension. 
However, these methods often suffer from redundant computations that make little contribution to the final performance because nearby poses are similar~\cite{strided}.
Additionally, they are too computationally expensive to process long videos (\emph{e.g.}, 243 frames), thereby limiting their practical utility in real-world scenarios. 
To tackle these issues, we propose to utilize a simple and efficient video representation of pose sequences.  
This representation captures complex temporal dynamics by mixing temporal information in the feature channels and treating each joint as a token, offering negligible computational cost gains in the sequence length. 
It is also a unified and flexible representation that can accommodate arbitrary-length sequences (\emph{i.e.}, a single frame and variable-length videos). 

The proposed GraphMLP is evaluated on two challenging datasets, Human3.6M~\cite{ionescu2013human3} and MPI-INF-3DHP~\cite{mehta2017monocular}. 
Extensive experiments show the effectiveness and generalization ability of our approach, which advances the state-of-the-art performance for estimating 3D human poses from a single image.  
Its performance surpasses the previous best result~\cite{zou2021modulated} by 1.4  in mean per joint position error (MPJPE) on the Human3.6M dataset. 
Besides, as shown in Fig.~\ref{fig:compare}, it brings clear 6.6  and 6.9  improvements in MPJPE for the MLP model~\cite{mlpmixer} and the GCN model~\cite{stgcn} on the MPI-INF-3DHP dataset, respectively. 
Surprisingly, compared to video pose Transformer (\emph{e.g.}, PoseFormer~\cite{poseformer}), even with  fewer computational costs, our MLP-Like architecture achieves better performance. 

Overall, our main contributions are summarized as follows:
\begin{itemize}
\item We present, to the best of our knowledge, the first MLP-Like architecture called GraphMLP for 3D human pose estimation. 
It combines the advantages of modern MLPs and GCNs, including globality, locality, and connectivity. 
\item The novel SG-MLP and CG-MLP blocks are proposed to encode the graph structure of human bodies within MLPs to obtain domain-specific knowledge about the human body while enabling the model to capture both local and global interactions. 
\item  A simple and efficient video representation is further proposed to extend our GraphMLP to the video domain flexibly. 
This representation enables the model to effectively process arbitrary-length sequences with negligible computational cost gains. 
\item Extensive experiments demonstrate the effectiveness and generalization ability of the proposed GraphMLP, and show new state-of-the-art results on two challenging datasets, \emph{i.e.}, Human3.6M~\cite{ionescu2013human3} and MPI-INF-3DHP~\cite{mehta2017monocular}. 
\end{itemize}

\section{Related Work} 
\noindent \textbf{3D Human Pose Estimation.}
There are mainly two categories to estimate 3D human poses. 
The first category of methods directly regresses 3D human joints from RGB images~\cite{li20143d,pavlakos2017coarse,zhou2017towards}. 
The second category is the 2D-to-3D pose lifting method~\cite{simplebaseline,videopose,li2019generating}, which employs an off-the-shelf 2D pose detection as the front end and designs a 2D-to-3D lifting network using detected 2D poses as input. 
This lifting method can achieve state-of-the-art performance and has become the mainstream method due to its efficiency and effectiveness. 
For example, FCN~\cite{simplebaseline} shows that 3D poses can be regressed simply and effectively from 2D keypoints with fully-connected networks. 
TCN~\cite{videopose} extends the FCN to video by utilizing temporal convolutional networks to exploit temporal information from 2D pose sequences. 
Liu \emph{et al.}~\cite{liu2020attention} incorporate the attention mechanism into TCN to enhance the modeling of long-range temporal relationships across frames. 
SRNet~\cite{zeng2020srnet} proposes a split-and-recombine network that splits the human body joints into multiple local groups and recombines them with a low-dimensional global context. 

Since the physical skeleton topology can form a graph structure, recent progress has focused on employing graph convolutional networks (GCNs) to address the 2D-to-3D lifting problem. 
LCN~\cite{ci2019optimizing} introduces a locally connected network to improve
the representation capability of GCN. 
SemGCN~\cite{zhao2019semantic} allows the model to learn the semantic relationships among the human joints. 
MGCN~\cite{zou2021modulated} improves SemGCN by introducing a weight modulation and an affinity modulation. 

\noindent \textbf{Transformers in Vision.}
Recently, Transformer-based methods achieve excellent results on various computer vision tasks, such as image classification~\cite{vit,swin,wang2021pyramid}, object detection~\cite{dert,zhu2020deformable,zhao2022tracking}, and pose estimation~\cite{meshgraphormer,poseformer,strided,mhformer}. 
The seminal work of ViT~\cite{vit} divides an image into  patches and uses a pure Transformer encoder to extract visual features. 
Strided Transformer~\cite{strided} incorporates strided convolutions into Transformers to aggregate information from local contexts for video-based 3D human pose estimation. 
PoseFormer~\cite{poseformer} utilizes a pure Transformer-based architecture to model spatial and temporal relationships from videos. 
MHFormer~\cite{mhformer} proposes a Transformer-based framework with multiple plausible pose hypotheses. 
Mesh Graphormer~\cite{meshgraphormer} combines GCNs and attention layers in a serial order to capture local and global dependencies for human mesh reconstruction. 

Unlike~\cite{meshgraphormer}, we mainly investigate how to combine more efficient architectures (\emph{i.e.}, modern MLPs) and GCNs to construct a stronger architecture for 3D human pose estimation and adopt a parallel manner to make it possible to model local and global information at the same time. 
Moreover, different from previous video-based methods~\cite{poseformer,strided,mhformer} that treat each frame as a token for temporal modeling, we mix features across all frames and maintain each joint as a token, which makes the network to be economical and easy to train. 

\noindent \textbf{MLPs in Vision.}
Modern MLP models are proposed to reduce the inductive bias and computational cost by replacing the complex self-attentions of Transformers with spatial-wise linear layers~\cite{lian2021mlp,liu2021pay,ren2021cascaded,shi2022polyp}. 
MLP-Mixer~\cite{mlpmixer} firstly proposes an MLP-Like model, which is a simple network architecture containing only pure MLP layers. 
Compared with FCN~\cite{simplebaseline} (\emph{i.e.}, conventional MLPs), this architecture introduces some modern designs, \emph{e.g.}, layer normalization (LN)~\cite{ba2016layer}, GELU~\cite{hendrycks2016gaussian}, mixing spatial information. 
Moreover, ResMLP~\cite{touvron2021resmlp} proposes a purely MLP architecture with the Affine transformation. 
CycleMLP~\cite{chen2022cyclemlp} proposes a cycle fully-connected layer to aggregate spatial context information and deal with variable
input image scales. 
However, these modern MLP models have not yet been applied to 3D human pose estimation. 

Inspired by their successes in vision, we first attempt to explore how MLP-Like architectures can be used for 3D human pose estimation in non-Euclidean skeleton data. 
The difference between our approach and the existing MLPs is that we introduce the inductive bias of the physical skeleton topology by combining MLP models with GCNs, providing more physically plausible and accurate estimations. 
We further investigate applying MLP-Like architecture in video and design an efficient video representation, which is seldom studied. 

\begin{figure}[t]
\centering
\includegraphics[width=1.00\linewidth]{figure/graph.pdf}
\caption
{
\textbf{(a)} The human skeleton graph in physical and symmetrical connections. 
\textbf{(b)} The adjacency matrix used in the GCN blocks of GraphMLP. 
Different colors denote the different types of bone connections. 
}
\label{fig:graph}
\end{figure}

\section{Proposed GraphMLP}
Fig.~\ref{fig:overview} illustrates the overall architecture of the proposed GraphMLP. 
Our approach takes 2D joint locations  estimated by an off-the-shelf 2D pose detector as input and outputs predicted 3D poses , where  is the number of joints. 
The proposed GraphMLP architecture consists of a skeleton embedding module, a stack of  identical GraphMLP layers, and a prediction head module. 
The core operation of GraphMLP architecture is the GraphMLP layer, each of which has two parts: a spatial graph MLP (SG-MLP) block and a channel graph MLP (CG-MLP) block. 
Our GraphMLP has a similar architecture to the original MLP-Mixer~\cite{mlpmixer}, but we incorporate graph convolutional networks (GCNs) into the model to meet the domain-specific requirement of the 3D human pose estimation and learn the local and global interactions of human body joints. 

\subsection{Preliminary}
\label{sec:preliminary}
In this subsection, we briefly introduce the preliminaries in GCNs and modern MLPs. 

\subsubsection{Graph Convolutional Networks}
Let  denotes a graph where  is a set of  nodes and  is the adjacency matrix encoding the edges between the nodes. 
Given a  layer feature  with  dimensions, a generic GCN~\cite{kipf2016semi} layer, which is used to aggregate features from neighboring nodes, can be formulated as:

where  is the learnable weight matrix and  is the adjacency matrix with added self-connections. 
 is the identity matrix,  is a diagonal matrix, and . 

The design principle of the GCN block is similar to~\cite{stgcn}, but we adopt a simplified single-frame version that contains only one graph layer. 
For the GCN blocks of our GraphMLP, the nodes in the graph denote the joint locations of the human body in Fig.~\ref{fig:graph} (a), and the adjacency matrix represents the bone connections between every two joints for node information passing in Fig.~\ref{fig:graph} (b), \emph{e.g.}, the rectangle of 1st row and 2nd column denotes the connection between joint 0 and joint 1. 
In addition, the different types of bone connections use different kernel weights following~\cite{stgcn}. 

\subsubsection{Modern MLPs}
MLP-Mixer is the first modern MLP model proposed in~\cite{mlpmixer}. 
It is a simple and attention-free architecture that mainly consists of a spatial MLP and a channel MLP, as illustrated in Fig.~\ref{fig:layer} (a). 
The spatial MLP aims to transpose the spatial axis and channel axis of tokens to mix spatial information. Then the channel MLP processes tokens in the channel dimension to mix channel information. 
Let  be an input feature, an MLP-Mixer layer can be calculated as:

where  is layer normalization (LN)~\cite{ba2016layer} and  is the matrix transposition. 
Both spatial and channel MLPs contain two linear layers and a GELU~\cite{hendrycks2016gaussian} non-linearity in between. 

We adopt this MLP model as our baseline, which is similar to~\cite{mlpmixer}, but we transpose the tokens before LN in the spatial MLP (\emph{i.e.}, normalize tokens along the spatial dimension). 
However, such a simple MLP model neglects to extract fine-grained local details and lacks prior knowledge about the human configurations, which are perhaps the bottlenecks restricting the representation ability of MLP-Like architectures for learning skeletal representations. 

\subsection{Network Architecture}
In this work, we present a novel GraphMLP built upon the MLP-Mixer described in~\cite{mlpmixer} to overcome the aforementioned limitations of existing MLP models. 
Below we elaborate on each module used in GraphMLP and provide its detailed implementations. 

\subsubsection{Skeleton Embedding}
Raw input data are mapped to latent space via the skeleton embedding module. 
Given the input 2D pose  with  body joints, we treat each joint as an input token. 
These tokens are projected to the high-dimension token feature  by a linear layer, where  is the hidden size. 

\begin{figure}[tb]
  \centering
  \includegraphics[width=1.00\linewidth]{figure/layer.pdf}
  \caption
  {
    Comparison of MLP Layers. 
    \textbf{(a)} MLP-Mixer Layer~\cite{mlpmixer}. 
    \textbf{(b)} Our GraphMLP Layer.
    Compared with MLP-Mixer, our GraphMLP incorporates graph structural priors into the MLP model via GCN blocks. 
    The MLPs and GCNs are in a paralleled design to model both local and global interactions. 
  }
  \label{fig:layer}
\end{figure}

\subsubsection{GraphMLP Layer}
The existing MLP simply connects all nodes but does not take advantage of graph structures, making it less effective in handling graph-structured data. 
To tackle this issue, we introduce the GraphMLP layer that unifies globality, locality, and connectivity in a single layer. 
Compared with the original MLP-Mixer in Fig.~\ref{fig:layer} (a), the main difference of our GraphMLP layer (Fig.~\ref{fig:layer} (b)) is that we utilize GCNs for local feature communication. 
This modification retains the domain-specific knowledge of human body configurations, which induces an inductive bias enabling the GraphMLP to perform very well in skeletal representation learning. 

Specifically, our GraphMLP layer is composed of an SG-MLP and a CG-MLP. 
The SG-MLP and CG-MLP are built by injecting GCNs into the spatial MLP and channel MLP (mentioned in Sec.~\ref{sec:preliminary}), respectively. 
To be more specific, the SG-MLP contains a spatial MLP block and a GCN block. 
These blocks process token features in parallel, where the spatial MLP extracts features among tokens with a global receptive field, and the GCN block focuses on aggregating local information between neighboring joints. 
The CG-MLP has a similar architecture to SG-MLP but replaces the spatial MLP with the channel MLP and has no matrix transposition. 
Based on the above description, the MLP layers in Eq.~(\ref{equ:mlpmixer_spatial}) and Eq.~(\ref{equ:mlpmixer_channel}) are modified to process tokens as:


where  denotes the GCN block,  is the index of GraphMLP layers. 
Here  and  are the output features of the SG-MLP and the CG-MLP for block , respectively.  

\subsubsection{Prediction Head}
Different from~\cite{vit,mlpmixer} that use a classifier head to do classification, our prediction head performs regression with a linear layer. 
It is applied on the extracted features  of the last GraphMLP layer to predict the final 3D pose . 

\subsubsection{Loss Function}
To train our GraphMLP, we apply an -norm loss to calculate the difference between prediction and ground truth. 
The model is trained in an end-to-end fashion, and the -norm loss is defined as follows:

where  and  are the predicted and ground truth 3D coordinates of joint , respectively. 

\subsection{Extension in the Video Domain}
To extend our GraphMLP for capturing temporal information, we introduce a simple video representation that changes the skeleton embedding module in the original architecture.  
Specifically, given a 2D pose sequence  with  frames and  joints, we first concatenate features between the coordinates of - axis and all frames for each joint into , and then fed it into a linear layer  to map the tokens and get . 
Subsequently, each joint is treated as an input token and fed into the GraphMLP layers and prediction head module to output the 3D pose of the center frame . 
These processes of GraphMLP in the video domain are illustrated in Figure~\ref{fig:video}. 

This is also a unified and flexible representation strategy that can process arbitrary-length sequences, \emph{e.g.}, a single frame with  and variable-length videos with . 
Interestingly, we find that using a single linear layer to encode temporal information can achieve competitive performance without explicitly temporal modeling by treating each frame as a token. 
More importantly, this representation is efficient since the amount of  is small (\emph{e.g.}, 17 joints). 
The increased computational costs from single frame to video sequence are only in one linear layer that weights are  with linear computational complexity to sequence length, which can be neglected. 
Meanwhile, the computational overhead and the number of parameters in the GraphMLP layers and the prediction head module are the same for different input sequences. 

\begin{figure}[tb]
\centering
\includegraphics[width=0.55\linewidth]{figure/video.pdf}
\caption
{
  Illustration of the process of GraphMLP in the video domain. 
}
\label{fig:video}
\end{figure}

\subsection{Implementation Details}
In our implementation, the proposed GraphMLP is stacked by  GraphMLP layers with hidden size , the MLP dimension of CG-MLP, \textit{i.e.}, , and the MLP dimension of SG-MLP, \textit{i.e.}, . 
The whole framework is trained in an end-to-end fashion from scratch on a single NVIDIA RTX 2080 Ti GPU. 
The learning rate starts from 0.001 with a decay factor of 0.95 utilized each epoch and 0.5 utilized per 5 epochs. 
We follow the generic data augmentation (horizontal flip augmentation) in~\cite{videopose,stgcn,zou2021modulated}. 
Following~\cite{xu2021graph,zou2021modulated,zeng2021learning}, we use 2D joints detected by cascaded pyramid network (CPN)~\cite{chen2018cascaded} for Human3.6M and 2D joints provided by the dataset for MPI-INF-3DHP. 

\begin{table*}[tb]
  \centering
  \caption
  {
    Quantitative comparison with state-of-the-art single-frame methods on Human3.6M under Protocol \#1 and Protocol \#2. 
    Detected 2D keypoints are used as input.  
     - adopts the same refinement module as~\cite{stgcn,zou2021modulated}. 
  }
  \resizebox{\textwidth}{!}{
  \begin{tabular}{@{}l|ccccccccccccccc|c@{}}
  \toprule
  \textbf{Protocol \#1} & Dir. & Disc & Eat & Greet & Phone & Photo & Pose & Purch. & Sit & SitD. & Smoke & Wait & WalkD. & Walk & WalkT. & Avg.\\
  \midrule

  Martinez \emph{et al.}~\cite{simplebaseline} &51.8 &56.2 &58.1 &59.0 &69.5 &78.4 &55.2 &58.1 &74.0 &94.6 &62.3 &59.1 &65.1 &49.5 &52.4 &62.9 \\

  Fang \emph{et al.}~\cite{fang2018learning} & 50.1& 54.3& 57.0& 57.1& 66.6& 73.3& 53.4& 55.7& 72.8& 88.6& 60.3& 57.7& 62.7& 47.5& 50.6& 60.4 \\

  Yang \emph{et al.}~\cite{yang20183d} & 51.5 & 58.9 & 50.4 & 57.0 & 62.1 & 65.4 & 49.8 & 52.7 & 69.2 & 85.2 & 57.4 & 58.4 &\textbf{43.6} & 60.1 & 47.7 & 58.6 \\

  Ci \emph{et al.}~\cite{ci2019optimizing} &46.8 &52.3& \textbf{44.7}& 50.4& 52.9& 68.9& 49.6& 46.4& 60.2 &78.9& 51.2& 50.0& 54.8& 40.4& 43.3& 52.7 \\

  Liu \emph{et al.}~\cite{liu2020comprehensive} &46.3 &52.2 &47.3 &50.7 &55.5 &67.1 &49.2 &46.0 &60.4 &71.1 &51.5 &50.1 &54.5 &40.3 &43.7 &52.4 \\

  Xu \emph{et al.}~\cite{xu2021graph} &45.2 &49.9 &47.5 &50.9 &54.9 &66.1 &48.5 &46.3 &59.7 &71.5 &51.4 &48.6 &53.9 &39.9 &44.1 &51.9 \\

  Zhao \emph{et al.}~\cite{zhao2022graformer} &45.2 &50.8 &48.0 &50.0 &54.9 &65.0 &48.2 &47.1 &60.2 &70.0 &51.6 &48.7 &54.1 &39.7 &43.1 &51.8 \\

  Pavllo \emph{et al.}~\cite{videopose} &47.1& 50.6& 49.0& 51.8 &53.6 &61.4& 49.4 &47.4 &59.3 &67.4 &52.4& 49.5& 55.3& 39.5& 42.7& 51.8 \\

  Cai \emph{et al.}~\cite{stgcn} &46.5 &48.8 &47.6& 50.9& 52.9 &61.3 &48.3 &45.8 &59.2 &64.4& 51.2& 48.4& 53.5& 39.2& 41.2& 50.6\\

  Zeng \emph{et al.}~\cite{zeng2020srnet} &44.5& \textbf{48.2} &47.1 &\textbf{47.8} &51.2 &{56.8} &50.1& {45.6}& 59.9 &66.4 &52.1 &\textbf{45.3} &54.2 &39.1 &40.3 &49.9 \\

  Zou \emph{et al.}~\cite{zou2021modulated} &45.4 &49.2 &45.7 &49.4 &\textbf{50.4} &58.2 &47.9 &46.0 &57.5 &63.0 &49.7 &46.6 &52.2 &38.9 &40.8 &49.4 \\

  \midrule
  
  GraphMLP (Ours) &{45.4} &{50.2} &{45.8} &{49.2} &{51.6} &{57.9} &{47.3} &{44.9} &{56.9} &{61.0} &{49.5} &{46.9} &{53.2} &{37.8} &{39.9} &{49.2} \\

  GraphMLP (Ours) &\textbf{43.7} &{49.3} &{45.5} &{47.9} &{50.5} &\textbf{56.0} &\textbf{46.3} &\textbf{44.1} &\textbf{55.9} &\textbf{59.0} &\textbf{48.4} &{45.7} &{51.2} &\textbf{37.1} &\textbf{39.1} &\textbf{48.0} \\

  \toprule
  \textbf{Protocol \#2} & Dir. & Disc & Eat & Greet & Phone & Photo & Pose & Purch. & Sit & SitD. & Smoke & Wait & WalkD. & Walk & WalkT. & Avg. \\
  \midrule

  Martinez \emph{et al.}~\cite{simplebaseline} &39.5 &43.2 &46.4 &47.0 &51.0 &56.0 &41.4 &40.6 &56.5 &69.4 &49.2 &45.0 &49.5 &38.0 &43.1 &47.7 \\

  Fang \emph{et al.}~\cite{fang2018learning} &38.2 &41.7 &43.7 &44.9 &48.5 &55.3 &40.2 &38.2 &54.5 &64.4 &47.2 &44.3 &47.3 &36.7 &41.7 &45.7 \\

  Ci \emph{et al.}~\cite{ci2019optimizing} &36.9& 41.6& 38.0& 41.0& 41.9 &51.1 &38.2& 37.6& 49.1 &62.1 &43.1& 39.9& 43.5& 32.2& 37.0& 42.2 \\

  Liu \emph{et al.}~\cite{liu2020comprehensive} &35.9 &40.0 &38.0 &41.5 &42.5 &51.4 &37.8 &36.0 &48.6 &56.6 &41.8 &38.3 &42.7 &31.7 &36.2 &41.2 \\

  Cai \emph{et al.}~\cite{stgcn} &36.8 &38.7 &38.2 &41.7 &40.7 &46.8 &37.9 &35.6 &47.6 &51.7 &41.3 &36.8 &42.7 &31.0 &34.7 &40.2 \\

  Pavllo \emph{et al.}~\cite{videopose} &36.0 &38.7 &38.0 &41.7 &40.1 &45.9 &37.1 &35.4 &46.8 &53.4 &41.4 &36.9 &43.1 &30.3 &34.8 &40.0 \\

  Zeng \emph{et al.}~\cite{zeng2020srnet} &35.8 &39.2 &{36.6} &\textbf{36.9} &39.8 &45.1 &38.4 &36.9 &47.7 &54.4 &\textbf{38.6} &36.3 &\textbf{39.4} &30.3 &35.4 &39.4 \\

  Zou \emph{et al.}~\cite{zou2021modulated} &35.7 &38.6 &\textbf{36.3} &40.5 &\textbf{39.2} &44.5 &37.0 &35.4 &46.4 &51.2 &40.5 &35.6 &41.7 &30.7 &33.9 &39.1 \\
  
  \midrule
  GraphMLP (Ours) &\textbf{35.0} &{38.4} &{36.6} &{39.7} &{40.1} &{43.9} &{35.9} &{34.1} &{45.9} &{48.6} &{40.0} &{35.3} &{41.6} &{30.0} &\textbf{33.3} &{38.6} \\

  GraphMLP (Ours) &{35.1} &\textbf{38.2} &{36.5} &{39.8} &{39.8} &\textbf{43.5} &\textbf{35.7} &\textbf{34.0} &\textbf{45.6} &\textbf{47.6} &{39.8} &\textbf{35.1} &{41.1} &\textbf{30.0} &{33.4} &\textbf{38.4} \\

  \toprule
  \end{tabular}
  }
  \label{table:h36m}
\end{table*}

\section{Experiments}
In this section, we first introduce experimental settings for evaluation. 
Then, we compare the proposed GraphMLP with state-of-the-art methods. 
We also conduct detailed ablation studies on the importance of designs in our proposed approach. 

\subsection{Datasets and Evaluation Metrics}
\noindent \textbf{Human3.6M.} Human3.6M~\cite{ionescu2013human3} is the largest benchmark for 3D human pose estimation. 
It contains 3.6 million video frames captured by a motion capture system in an indoor environment, where 11 professional actors perform 15 actions such as greeting, phoning, and sitting. 
Following previous works~\cite{simplebaseline,xu2021graph,zeng2021learning}, our model is trained on five subjects (S1, S5, S6, S7, S8) and tested on two subjects (S9, S11). 
We report our performance using two evaluation metrics. 
One is the mean per joint position error (MPJPE), referred to as Protocol~\#1, which calculates the mean Euclidean distance in millimeters between the predicted and the ground truth joint coordinates. 
The other is the PA-MPJPE which measures the MPJPE after Procrustes analysis (PA)~\cite{gower1975generalized} and is referred to as Protocol~\#2. 

\noindent \textbf{MPI-INF-3DHP.}
MPI-INF-3DHP~\cite{mehta2017monocular} is a large-scale 3D human pose dataset containing both indoor and outdoor scenes. 
Its test set consists of three different scenes: studio with green screen (GS), studio without green screen (noGS), and outdoor scene (Outdoor).
Following previous works~\cite{ci2019optimizing,zeng2021learning,zou2021modulated}, we use Percentage of Correct Keypoint (PCK) with the threshold of 150  and Area Under Curve (AUC) for a range of PCK thresholds as evaluation metrics. 
To verify the generalization ability of our approach, we directly apply the model trained on Human3.6M to the test set of this dataset. 

\subsection{Comparison with State-of-the-Art Methods}
\noindent \textbf{Comparison with Single-frame Methods.}
Table~\ref{table:h36m} reports the performance comparison between our GraphMLP and previous state-of-the-art methods that take a single frame as input on Human3.6M. 
It can be seen that our approach reaches 49.2  in MPJPE and 38.6  in PA-MPJPE, which outperforms all previous methods. 
Note that some works~\cite{stgcn,zou2021modulated} adopt a pose refinement module to boost the performance further. 
Compared with them, GraphMLP achieves lower MPJPE even though we do not use the refinement module. 
Moreover, with this module, it achieves 48.0  in MPJPE, surpassing MGCN~\cite{zou2021modulated} by a large margin of 1.4  error reduction (relative 3\% improvements). 

Due to the uncertainty of 2D detections, we also report results using ground truth 2D keypoints as input to explore the upper bound of the proposed approach. 
As shown in Table~\ref{table:gt}, our GraphMLP obtains substantially better performance when given precise 2D joint information and attains state-of-the-art performance, which indicates its effectiveness. 

Table~\ref{table:3dhp} further compares our GraphMLP against previous state-of-the-art single-frame methods on cross-dataset scenarios. 
We only train our model on the Human3.6M dataset and test it on the MPI-INF-3DHP dataset. 
The results show that our approach obtains the best results in all scenes and all metrics, consistently surpassing other methods. 
This verifies the strong generalization ability of our approach to unseen scenarios. 

\begin{table*}[t]
  \centering
  \caption
  {
    Quantitative comparison with state-of-the-art single-frame methods on Human3.6M under Protocol \#1. 
    Ground truth 2D keypoints are used as input. 
  } 
  \resizebox{\textwidth}{!}{
  \begin{tabular}{@{}l|ccccccccccccccc|c@{}}
  \toprule
  \textbf{Protocol \#1} & Dir. & Disc & Eat & Greet & Phone & Photo & Pose & Purch. & Sit & SitD. & Smoke & Wait & WalkD. & Walk & WalkT. & Avg.\\
  \midrule

  Martinez \emph{et al.}~\cite{simplebaseline} &37.7 &44.4 &40.3 &42.1& 48.2& 54.9 &44.4 &42.1 &54.6& 58.0 &45.1 &46.4 &47.6 &36.4 &40.4& 45.5\\

  Zhao \emph{et al.}~\cite{zhao2019semantic} &37.8 &49.4 &37.6 &40.9 &45.1 &41.4 &40.1& 48.3& 50.1 &42.2& 53.5 &44.3 &40.5 &47.3& 39.0 &43.8 \\

  Cai \emph{et al.}~\cite{stgcn} &{33.4} &39.0 &33.8 &37.0 &38.1 &47.3 &39.5 &37.3 &43.2 &46.2 &37.7 &38.0 &38.6 &30.4 &32.1 &38.1\\

  Liu \emph{et al.}~\cite{liu2020comprehensive} &36.8 &40.3& 33.0 &36.3 &37.5 &45.0 &39.7 &34.9 &40.3 &47.7& 37.4& 38.5& 38.6& 29.6 &32.0 &37.8 \\

  Zou \emph{et al.}~\cite{zou2021modulated} &- &- &- &- &- &- &- &- &- &- &- &- &- &- &- &37.4 \\

  Zeng \emph{et al.}~\cite{zeng2020srnet} &35.9 &\textbf{36.7} &{29.3} &34.5 &36.0 &42.8 &37.7 &31.7 &40.1 &44.3 &35.8 &37.2 &36.2 &33.7 &34.0 &36.4  \\

  Ci \emph{et al.}~\cite{ci2019optimizing} &36.3& 38.8 &29.7& 37.8& {34.6}& 42.5& 39.8& 32.5& \textbf{36.2}&  39.5& {34.4}& 38.4& 38.2& 31.3& 34.2&36.3 \\

  Xu \emph{et al.}~\cite{xu2021graph} &35.8 &38.1 &31.0 &35.3 &35.8 &43.2 &{37.3} &{31.7} &38.4 &45.5 &35.4 &{36.7} & 36.8 &{27.9} &30.7 &35.8 \\

  Zhao \emph{et al.}~\cite{zhao2022graformer} &\textbf{32.0} &38.0 &30.4 &34.4 &34.7 &43.3 &\textbf{35.2} &\textbf{31.4} &38.0 &46.2 &34.2 &\textbf{35.7} &36.1 &\textbf{27.4} &30.6 &35.2 \\

  \midrule

  GraphMLP (Ours) &{32.2} &{38.2} &\textbf{29.3} &\textbf{33.4} &\textbf{33.5} &\textbf{38.1} &{38.2} &{31.7} &{37.3} &\textbf{38.5} &\textbf{34.2} &{36.1} &\textbf{35.5} &{28.0} &\textbf{29.3} &\textbf{34.2} \\


  \toprule
  \end{tabular}
  }
  \label{table:gt}
\end{table*}

\noindent \textbf{Comparison with Video-based Methods.}
As shown in Table~\ref{table:video}, our method achieves outstanding performance against video-based methods in both CPN and GT inputs. 
The proposed method surpasses our baseline model (\emph{i.e.}, MLP-Mixer~\cite{mlpmixer}) by a large margin of 4.6  (10\% improvements) with CPN inputs and 4.9  (14\% improvements) with GT inputs.  
These results further demonstrate the effectiveness of our GraphMLP, which combines MLPs and GCNs to learn better skeleton representations. 
Compared with the most related work, Poseformer~\cite{poseformer}, a self-attention based architecture, our GraphMLP, such a self-attention free architecture, improves the results from 31.3  to 30.3  with GT inputs (3\% improvements). 
Besides, our simple video representation, which uses a single linear layer to encode temporal information instead of being specifically designed for temporal enhancement like previous works~\cite{videopose,chen2021anatomy,poseformer}, is also capable of performing well. 
This indicates that our method can alleviate the issue of video redundancy by compressing the video information into a single vector, leading to impressive results. 

Table~\ref{table:poseformer} further reports the comparison of computational costs with Poseformer~\cite{poseformer} in different input frames. 
Surprisingly, our method requires only 22\% FLOPs (356M vs. 1625M) while achieving better performance. 
Note that \cite{poseformer} does not report the result of the 243-frame model since the training is difficult and time-consuming, where it requires 4874M FLOPs. 
In contrast, our method requires only  fewer FLOPs and offers negligible parameters and FLOPs gains in the sequence length, which allows it easy to deploy in real-time applications and has great potential for better results with a large input frame. 
These results show that our GraphMLP in video reaches competitive performance with fewer computational costs and can serve as a strong baseline for video-based 3D human pose estimation. 

\subsection{Ablation Study}
The large-scale ablation studies with 2D detected inputs on the Human3.6M dataset are conducted to investigate the effectiveness of our model (using the single-frame model). 

\begin{table}[tb]
  \centering
  \caption
  {
    Performance comparison with state-of-the-art single-frame methods on MPI-INF-3DHP. 
  }
  \setlength{\tabcolsep}{1.00mm} 
  \begin{tabular}{@{}l|cccccc@{}}
  \toprule
  Method &GS  &noGS  &Outdoor  &All PCK  &All AUC  \\

  \midrule
  Martinez \emph{et al.}~\cite{simplebaseline} &49.8 &42.5 &31.2 &42.5 &17.0 \\

  Mehta \emph{et al.}~\cite{mehta2017monocular} &70.8 &62.3 &58.5 &64.7 &31.7 \\

  Ci \emph{et al.}~\cite{ci2019optimizing} &74.8 &70.8 &77.3 &74.0 &36.7 \\

  Zhao \emph{et al.}~\cite{zhao2022graformer} &80.1 &77.9 &74.1 &79.0 &43.8 \\

  Liu \emph{et al.}~\cite{liu2020comprehensive} &77.6 &80.5 &80.1 &79.3 &47.6 \\

  Xu \emph{et al.}~\cite{xu2021graph} &81.5 &81.7 &75.2 &80.1 &45.8 \\

  Zeng \emph{et al.}~\cite{zeng2021learning} &- &- &84.6 &82.1 &46.2 \\

  Zou \emph{et al.}~\cite{zou2021modulated} &86.4 &86.0 &85.7 &86.1 &53.7 \\

  \midrule
  GraphMLP (Ours) &\textbf{87.3} &\textbf{87.1} &\textbf{86.3} &\textbf{87.0} &\textbf{54.3} \\

  \bottomrule
  \end{tabular}
  \label{table:3dhp}
\end{table}

\begin{table}[t]
\centering
\caption
{ 
Quantitative comparisons with video-based methods on Human3.6M under MPJPE.
CPN and GT denote the inputs of 2D poses detected by CPN and ground truth 2D poses, respectively. 
}
\setlength{\tabcolsep}{6.60mm} 
\begin{tabular}{l|cc}
\toprule [1pt]
Method &CPN &GT \\
\midrule [0.5pt]
ST-GCN~\cite{stgcn} () &48.8 &37.2 \\
TCN~\cite{videopose} () &46.8 &37.8 \\
Liu \emph{et al.}~\cite{liu2020attention} () &45.1 &34.7 \\
SRNet~\cite{zeng2020srnet} () &44.8 &32.0 \\
PoseFormer~\cite{poseformer} () &44.3 &31.3 \\
Anatomy3D~\cite{chen2021anatomy} () &44.1 &32.3 \\
\midrule [0.5pt]
Baseline () &48.4 &35.2 \\

GraphMLP (Ours, ) &\textbf{43.8} &\textbf{30.3} \\

\toprule [1pt]
\end{tabular}
\label{table:video}
\end{table}

\begin{table}[t]
\centering
\caption
{ 
  Comparison of parameters, FLOPs, and MPJPE  with PoseFormer in different input frames on Human3.6M. 
}  
\setlength{\tabcolsep}{0.60mm} 
\begin{tabular}{c|ccccc}
\toprule [1pt]
Model & &Param (M) &FLOPs (M) &MPJPE (mm) \\
\midrule [0.5pt]  
PoseFormer~\cite{poseformer} &27 &9.57 &541 &47.0 \\
PoseFormer~\cite{poseformer} &81 &9.60 &1625 &44.3 \\
PoseFormer~\cite{poseformer} &243 &9.69 &4874 &- \\
\midrule [0.5pt]

GraphMLP (Ours) &1 &9.49 &348 &49.2 & \\
GraphMLP (Ours) &27 &9.51 &349 &45.5 & \\
GraphMLP (Ours) &81 &9.57 &351 &44.5 & \\
GraphMLP (Ours) &243 &9.73 &356 &\textbf{43.8} & \\

\toprule [1pt]
\end{tabular}
\label{table:poseformer}
\end{table}

\begin{table}[t]
  \centering
  \caption
  {
    Ablation study on various configurations of our approach. 
     is the number of GraphMLP layers,  is the hidden size,  is the MLP dimension of CG-MLP, and  is the MLP dimension of SG-MLP. 
  }
  \setlength{\tabcolsep}{3.10mm}
  \begin{tabular}{cccccc}
    \toprule
    & & & & Params (M) & MPJPE ()  \\
    \midrule
    3& 512& 1024& 256& 9.49& \textbf{49.2} \\
    \midrule

    3& 512& 1024& 128& 9.47& 49.7 \\
    3& 512& 2048& 128& 12.62& 49.7 \\
    3& 512& 2048& 256& 12.64& 49.6 \\

    \midrule  

    2& 512& 1024& 256& 6.33& 50.2 \\
    4& 512& 1024& 256& 12.65& 50.0 \\
    5& 512& 1024& 256& 15.81& 50.3 \\
    6& 512& 1024& 256& 18.97& 50.4 \\

    \midrule

    3& 128& 256& 64& 0.60& 50.2 \\
    3& 256& 512& 128& 2.38& 50.1 \\
    3& 384& 768& 192& 5.35& 49.7 \\
    3& 768& 1536& 384 & 21.31& 49.4 \\
\toprule
\end{tabular}
\label{table:parameters}
\end{table}

\noindent \textbf{Model Configurations.}
We start our ablation studies by exploring the GraphMLP on different hyper-parameters. 
The results are shown in Table~\ref{table:parameters}. 
It can be observed that using the expanding ratio of 2 (, ) works better than the ratio value of 4 which is common in vision Transformers and MLPs. 
Increasing or reducing the number of GraphMLP layers  hurts performance, while using  performs best. 
The hidden size  is important to determine the modeling ability. 
While increasing the  from 128 to 512 (keeping the same MLP ratios), the MPJPE decreases from 50.2  to 49.2 . 
Meanwhile, the number of parameters increases from 0.60M to 9.49M. 
The performance saturates when  surpasses 512. 
Therefore, the optimal hyper-parameters for our model are , , , and , which are different from the original setting of MLP-Mixer~\cite{mlpmixer}. 
The reason for this may be the gap between vision and skeleton data, where the Human3.6M dataset is not diverse enough to train a large GraphMLP model. 

\noindent \textbf{Input 2D Detectors.}
A high-performance 2D detector is vital in achieving accurate 3D pose estimation. 
Table~\ref{table:detectors} reports the performance of our model with ground truth 2D joints, and detected 2D joints from Stack Hourglass (SH)~\cite{newell2016stacked}, Detectron~\cite{videopose}, CPN~\cite{chen2018cascaded}, and HRNet~\cite{hrnet}. 
We can observe that our approach can produce more accurate results with a better 2D detector and is effective on different 2D estimators. 

\begin{figure*}[t]
  \centering
  \includegraphics[width=1.00\linewidth]{figure/dataset.jpg}
  \caption
  {
    Qualitative comparison with MLP-Mixer~\cite{mlpmixer} and GCN~\cite{stgcn} for reconstructing 3D human poses on Human3.6M dataset. 
    Red arrows highlight wrong estimations. 
  }
  \label{fig:dataset}
\end{figure*}

\begin{table}[t]
  \centering
  \caption
  {
    Ablation study on different 2D detectors. 
  }
  \setlength{\tabcolsep}{4.50mm}
  \begin{tabular}{lcc}
  \toprule
  Method &2D Mean Error &MPJPE ()  \\
  \midrule
  SH~\cite{newell2016stacked} &9.03 &56.9 \\ 
  Detectron~\cite{videopose} &7.77  &54.5 \\
  CPN~\cite{chen2018cascaded} &6.67 &49.2 \\

  2D Ground Truth &0 &\textbf{34.2} \\
  \toprule
  \end{tabular}
  \label{table:detectors}
\end{table}

\begin{table}[t]
  \centering
  \caption
  {
    Ablation study on different network architectures. 
  }
  \setlength{\tabcolsep}{7.30mm}
  \begin{tabular}{lc}
  \toprule
  Method &MPJPE ()  \\
  \midrule
  FCN~\cite{simplebaseline} &53.5 \\
  GCN~\cite{stgcn} &51.3 \\
  Transformer~\cite{transformer} &52.7 \\
  MLP-Mixer~\cite{mlpmixer} &52.0 \\
  Mesh Graphormer~\cite{meshgraphormer} &51.3 \\
  \midrule
  Graph-Mixer& 50.4 \\
  Transformer-GCN& 51.5 \\
  GraphMLP &\textbf{49.2} \\

  \toprule
  \end{tabular}
  \label{table:architecture}
\end{table}

\begin{table}[t]
  \centering
  \caption
  {
    Ablation study on different design options of combining MLPs and GCNs. 
     means using spatial GCN block in SG-MLP. 
  }
  \setlength{\tabcolsep}{5.80mm}
  \begin{tabular}{lc}
  \toprule
  Method &MPJPE ()  \\
  \midrule
  Baseline &52.0 \\
  \midrule
  GCN Before Spatial MLP &51.6 \\
  GCN After Spatial MLP &50.6 \\
  GCN After Channel MLP &50.9 \\
  GCN and MLP in Parallel &50.2 \\
  GCN and MLP in Parallel &\textbf{49.2} \\
  \toprule
  \end{tabular}
  \label{table:design}
\end{table}

\begin{table}[t]
  \centering
  \caption
  {
    Ablation study on different components in GraphMLP. 
  }
  \setlength{\tabcolsep}{2.00mm}
  \begin{tabular}{l|cc|c}
  \toprule
  Method& SG-MLP& CG-MLP& MPJPE ()  \\
  \midrule
  Baseline &\xmark &
  \xmark &52.0 \\
  \midrule
  GraphMLP (SG-MLP) &\cmark &\xmark& 50.6 \\
  GraphMLP (CG-MLP) &\xmark &\cmark &50.5 \\
  GraphMLP &\cmark &\cmark &\textbf{49.2} \\

  \toprule
  \end{tabular}
  \label{table:components}
\end{table}

\noindent \textbf{Network Architectures.}
To clearly demonstrate the advantage of the proposed GraphMLP, we compare our approach with various baseline architectures. 
The `Graph-Mixer' is constructed by replacing the linear layers in MLP-Mixer with GCN layers, and the `Transformer-GCN' is built by replacing the spatial MLPs in GraphMLP with multi-head self-attention blocks and adding a position embedding module. 
To ensure a consistent and fair evaluation, the parameters of these architectures (\emph{e.g.}, the number of layers, hidden size) keep the same. 
As shown in Table~\ref{table:architecture}, our proposed approach consistently surpasses all other architectures. 
For example, our approach can improve the GCN-based model~\cite{stgcn} from 51.3  to 49.2 , resulting in relative 4.1\% improvements. 
It's worth noting that the MLP-Like model (`MLP-Mixer', `GraphMLP') outperforms the Transformer-based model (`Transformer', `Transformer-GCN'). 
This may be because a small number of tokens (\emph{e.g.}, 17 joints) are less effective for self-attention in learning long-range dependencies. 
Overall, these results confirm that the GraphMLP can serve as a new and strong baseline for 3D human pose estimation. 
The implementation details of these network architectures can be found in the supplementary. 

\noindent \textbf{Network Design Options.}
Our approach allows the model to learn strong structural priors of human joints by injecting GCNs into the baseline model (\emph{i.e.}, MLP-Mixer~\cite{mlpmixer}). 
We also explore the different combinations of MLPs and GCNs to find an optimal architecture. 
As shown in Table~\ref{table:design}, the location matters of GCN are studied by five different designs: 
(i) The GCN is placed before spatial MLP. 
(ii) The GCN is placed after spatial MLP. 
(iii) The GCN is placed after channel MLP. 
(iv) The GCN and MLP are in parallel but using a spatial GCN block (process tokens in spatial dimension) in SG-MLP. 
(v) The GCN and MLP are in parallel. 
The results show that all these designs can help the model produce more accurate 3D poses, and using GCN and MLP in parallel achieves the best estimation accuracy. 
The illustrations of these design options can be found in the supplementary. 

\noindent \textbf{Model Components.}
We also investigate the effectiveness of each component in our design. 
In Table~\ref{table:components}, the first row corresponds to the baseline model (\emph{i.e.}, MLP-Mixer~\cite{mlpmixer}) that does not use any GCNs in the model. 
The MPJPE is 52.0 . 
The rest of the rows show the results of replacing its spatial MLP with SG-MLP or channel MLP with CG-MLP by adding a GCN block into the baseline. 
It can be found that using our SG-MLP or CG-MLP can improve performance (50.6  and 50.5  respectively). 
When enabling both SG-MLP and CG-MLP, GraphMLP improves the performance over the baseline by a clear margin of 2.8  (relatively 5.4\% improvements), indicating that the proposed components are mutually reinforced to produce more accurate 3D poses. 
These results validate the effectiveness of our motivation: combining modern MLPs and GCNs in a unified architecture for better 3D human pose estimation. 

\begin{figure}[t]
\centering
\includegraphics[width=1.0\linewidth]{figure/wild.jpg}
\caption
{
  Qualitative comparison with MLP-Mixer~\cite{mlpmixer}, GCN~\cite{stgcn}, and MGCN~\cite{zou2021modulated} for reconstructing 3D human poses on challenging in-the-wild images. 
  Red arrows highlight wrong estimations. 
}
\label{fig:wild}
\end{figure}

\section{Qualitative Results}
Fig.~\ref{fig:dataset} presents the qualitative comparison among the proposed GraphMLP, MLP-Mixer~\cite{mlpmixer}, and GCN~\cite{stgcn} on Human3.6M dataset. 
Furthermore, Fig.~\ref{fig:wild} presents the qualitative comparison among our GraphMLP and three other methods, namely MLP-Mixer~\cite{mlpmixer}, GCN~\cite{stgcn}, and MGCN~\cite{zou2021modulated}, on more challenging in-the-wild images.
Note that these actions from in-the-wild images are rare or absent from the training set of Human3.6M. 
GraphMLP, benefiting from its globality, locality, and connectivity, performs better and is able to predict accurate and plausible 3D poses. 
It indicates both the effectiveness and generalization ability of our approach. 
However, there are still some failure cases, where our approach fails to produce accurate 3D human poses due to large 2D detection error, half body, rare poses, and heavy occlusion, as shown in Fig.~\ref{fig:fail}. 
More qualitative results can be found in the supplementary. 

\section{Conclusion}
In this paper, we propose a simple yet effective graph-reinforced MLP-Like architecture, termed GraphMLP, which represents the first use of modern MLP dedicated to 3D human pose estimation. 
Our GraphMLP inherits the advantages of both MLPs and GCNs, making it a \textit{global-local-graphical} unified architecture without self-attention. 
Extensive experiments show that the proposed GraphMLP achieves state-of-the-art performance and can serve as a strong baseline for 3D human pose estimation in both single frame and video sequence. 
We also show that complex temporal dynamics can be effectively modeled in a simple way without explicitly temporal modeling, and the proposed GraphMLP in video reaches competitive results with fewer computational costs. 

Although the proposed approach has shown promising results, there are still some failure cases since our approach highly depends on the quality of 2D detection results. 
In the future, we will extend our approach to multi-view inputs to improve its robustness. 
Moreover, as the MLPs and GCNs in our GraphMLP are straightforward, we look forward to combining more powerful MLPs or GCNs to further improve the performance. 

\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{figure/fail.jpg}
  \caption
  {
    Challenging scenarios where GraphMLP fails to produce accurate 3D human poses.
  }
  \label{fig:fail}
\end{figure}

\bibliographystyle{IEEEtran}
\bibliography{ref.bib}

\newpage
\section{Appendix}

This supplementary material contains the following details:
(1) Detailed description of multi-layer perceptrons (see Sec.~\ref{sec:mlp}). 
(2) Additional implementation details (see Sec.~\ref{sec:details}). 
(3) Additional ablation studies (see Sec.~\ref{sec:ablation}).
(4) Additional qualitative results (see Sec.~\ref{sec:qualitative}). 

\section{Multi-Layer Perceptrons}
\label{sec:mlp}
In Sec. III-A of our main manuscript, we give a brief description of the MLP-Mixer layer~\cite{mlpmixer} which is defined as below:


If considering more details about the spatial and channel MLPs, Eq.~\eqref{equ:mlpmixer_spatial_supp} and Eq.~\eqref{equ:mlpmixer_channel_supp} can be further defined as:

where  is the GELU activation function~\cite{hendrycks2016gaussian}.
 and  are the weights of two linear layers in the . 
 and  are the weights of two linear layers in the . 

For GraphMLP in video, LN is additionally applied after every fully-connected layer in , Eq.~\eqref{equ:mlpmixer_spatial_supp_1} is modified to:


\section{Additional Implementation Details}
\label{sec:details}
In Sec.~IV-C of our main manuscript, we conduct extensive ablation studies on different network architectures and model variants of our GraphMLP. 
Here, we provide implementation details of these models. 

\noindent \textbf{Network Architectures.}
In Table VIII of our main paper, we report the results of various network architectures. 
Here, we provide illustrations in Fig.~\ref{fig:supp_network} and implementation details as follows:
\begin{enumerate}[(a)]
  \item FCN: FCN~\cite{simplebaseline} is a conventional MLP whose building block contains a linear layer, followed by batch normalization, dropout, and a ReLU activation. 
  We follow their original implementation and use their code~\cite{code_baeline} to report the performance. 
  \item GCN: We remove the spatial MLP and the channel MLP in our SG-MLP and CG-MLP, respectively. 
  \item Transformer: We build it by using a standard Transformer encoder which is the same as ViT~\cite{vit}. 
  \item MLP-Mixer: It is our baseline model that has the same architecture as~\cite{mlpmixer}. We build it by replacing multi-head attention blocks with spatial MLPs and removing the position embedding module in the Transformer. 
  \item Mesh Graphormer: Mesh Graphormer~\cite{meshgraphormer} is the most relevant study to our approach that focuses on combining self-attentions and GCNs in a Transformer model for mesh reconstruction. 
  Instead, our GraphMLP focuses on combining modern MLPs and GCNs to construct a stronger architecture for 3D human pose estimation. 
  We follow their design to construct a model by adding a GCN block after the multi-head attention in the Transformer. 
  \item Graph-Mixer: We replace linear layers in MLP-Mixer with GCN layers. 
  \item Transformer-GCN: We replace spatial MLPs in GraphMLP with multi-head self-attention blocks and add a position embedding module before Transformer-GCN layers. 
  \item GraphMLP: It is our proposed approach. Please refer to Fig.~1 of our main paper. 
\end{enumerate}

\noindent \textbf{Network Design Options.}
In Fig.~\ref{fig:supp_design}, we graphically illustrate five different design options of the GraphMLP layer as mentioned in Table IX of our main paper. 
Fig.~\ref{fig:supp_design} (e) shows that we adopt the design of GCN and MLP in parallel but use a spatial GCN block in SG-MLP. 
The spatial GCN block processes tokens in the spatial dimension, which can be calculated as:


\noindent \textbf{Model Components.}
In Table X of our main paper, we investigate the effectiveness of each component in our design. 
Here, we provide the illustrations of these model variants in Fig.~\ref{fig:supp_component}. 

\section{Additional Ablation Studies}
\label{sec:ablation}
\noindent \textbf{Transposition Design Options.}
As mentioned in Sec. III-A of our main manuscript, we transpose the tokens before LN in the spatial MLP, and therefore the LN normalizes tokens along the spatial dimension. 
Here, we investigate the influence of transposition design options in Table~\ref{table:transposition}. 

The `Transposition Before LN' can be formulated as:


The `Transposition After LN' can be written as:


From Table~\ref{table:transposition}, the results show that performing transposition before LN brings more benefits in both MLP-Mixer and our GraphMLP models. 
Note that it is different from the original implementation of MLP-Mixer, which uses transposition after LN. 

\begin{table}[!t]
  \centering
  \scriptsize
  \caption
  {Ablation study on different design options of transposition. 
  }
  \setlength{\tabcolsep}{5.30mm}
  \begin{tabular}{lc}
  \toprule
  Method &MPJPE ()  \\
  \midrule
  MLP-Mixer, Transposition After LN &52.6 \\
  MLP-Mixer, Transposition Before LN &\textbf{52.0} \\

  \midrule
  GraphMLP, Transposition After LN &49.7 \\
  GraphMLP, Transposition Before LN &\textbf{49.2} \\

  \toprule
  \end{tabular}
  \label{table:transposition}
\end{table}

\section{Additional Qualitative Results}
\label{sec:qualitative}
Fig.~\ref{fig:supp_dataset} show qualitative results of the proposed GraphMLP on Human3.6M and MPI-INF-3DHP datasets. 
The Human3.6M is an indoor dataset (top three rows), and the test set of MPI-INF-3DHP contains three different scenes: studio with green screen (GS, fourth row), studio without green screen (noGS, fifth row), and outdoor scene (Outdoor, sixth row). 
Moreover, Fig.~\ref{fig:supp_wild} shows qualitative results on challenging in-the-wild images. 
We can observe that our approach is able to predict reliable and plausible 3D poses on these challenging cases. 

\begin{figure*}[b]
  \centering
  \includegraphics[width=1.0\linewidth]{figure/supp_network.pdf}
  \caption
  { 
    Different network architectures. 
  }
  \label{fig:supp_network}
\end{figure*}

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.60\linewidth]{figure/supp_design.pdf}
  \caption
  {
    Different design options of the GraphMLP layer. 
  }
  \label{fig:supp_design}
\end{figure*}

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.60\linewidth]{figure/supp_component.pdf}
  \caption
  {
    The model components we have studied for building our proposed GraphMLP. 
  }
  \label{fig:supp_component}
\end{figure*}

\begin{figure*}[htb]
  \centering
  \includegraphics[width=1.0\linewidth]{figure/supp_dataset.jpg}
  \caption
  {Visualization results of our approach for reconstructing 3D human poses on Human3.6M dataset (top three rows) and MPI-INF-3DHP dataset (bottom three rows). 
  }
  \label{fig:supp_dataset}
\end{figure*}

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.91\linewidth]{figure/supp_wild.jpg}
  \caption
  {Visualization results of the proposed GraphMLP for reconstructing 3D human poses on challenging in-the-wild images. 
  }
  \label{fig:supp_wild}
\end{figure*}

\end{document}
