\documentclass[sigconf]{acmart}
\usepackage{tabularx}
\usepackage{bm}

\AtBeginDocument{\providecommand\BibTeX{{\normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

\setcopyright{acmcopyright}
\copyrightyear{2021}
\acmYear{2021}
\acmDOI{10.1145/nnnnnnn.nnnnnnn}

\acmConference[ICAIL '21]{ICAIL '21: 18th International Conference on Artificial Intelligence and Law }{June 21--25, 2021}{ University of São Paulo}
\acmBooktitle{ICAIL '21: 18th International Conference on Artificial Intelligence and Law, June 21--25, 2021,  University of São Paulo}



\acmSubmissionID{163}



\begin{document}

\title[When Does Pretraining Help?]{When Does Pretraining Help? Assessing Self-Supervised Learning for Law and the CaseHOLD Dataset of 53,000+ Legal Holdings}

\author{Lucia Zheng}
\authornote{These authors contributed equally to this work.}
\email{zlucia@stanford.edu}
\affiliation{\institution{Stanford University}
 \city{Stanford}
 \state{California}
 \country{USA}
 }

\author{Neel Guha}
\authornotemark[1]
\email{nguha@stanford.edu}
\affiliation{\institution{Stanford University}
 \city{Stanford}
 \state{California}
 \country{USA}
 }
 
\author{Brandon R. Anderson}
\email{banderson@law.stanford.edu}
\affiliation{\institution{Stanford University}
 \city{Stanford}
 \state{California}
 \country{USA}
 }

\author{Peter Henderson}
\email{phend@stanford.edu}
\affiliation{\institution{Stanford University}
 \city{Stanford}
 \state{California}
 \country{USA}
 }
 
\author{Daniel E. Ho}
\email{dho@law.stanford.edu}
\affiliation{\institution{Stanford University}
 \city{Stanford}
 \state{California}
 \country{USA}
 }

\renewcommand{\shortauthors}{Zheng and Guha, et al.}

\begin{abstract}
While self-supervised learning has made rapid advances in natural language processing, it remains unclear when researchers should engage in resource-intensive domain-specific pretraining (domain pretraining). The law, puzzlingly, has yielded few documented instances of substantial gains to domain pretraining in spite of the fact that legal language is widely seen to be unique.  We hypothesize that these existing results stem from the fact that existing legal NLP tasks are too easy and fail to meet conditions for when domain pretraining can help.  To address this, we first present CaseHOLD (Case \underline{H}oldings \underline{O}n \underline{L}egal \underline{D}ecisions), a new dataset comprised of over 53,000+ multiple choice questions to identify the relevant holding of a cited case. This dataset presents a fundamental task to lawyers and is both legally meaningful and difficult from an NLP perspective (F1 of 0.4 with a BiLSTM baseline). Second, we assess performance gains on  CaseHOLD and existing legal NLP datasets.  While a Transformer architecture (BERT) pretrained on a general corpus (Google Books and Wikipedia) improves performance, domain pretraining (using corpus of 3.5M decisions across all courts in the U.S. that is larger than BERT's) with a custom legal vocabulary exhibits the most substantial performance gains with CaseHOLD (gain of 7.2\% on F1, representing a 12\% improvement on BERT) and consistent performance gains across two other legal tasks. Third, we show that domain pretraining may be warranted when the task exhibits sufficient similarity to the pretraining corpus: the level of performance increase in three legal tasks was directly tied to the domain specificity of the task. Our findings inform when researchers should engage resource-intensive pretraining and show that Transformer-based architectures, too, learn embeddings suggestive of distinct legal language.
\end{abstract}

\begin{CCSXML}
<ccs2012>
    <concept>
        <concept_id>10010405.10010455.10010458</concept_id>
        <concept_desc>Applied computing~Law</concept_desc>
        <concept_significance>500</concept_significance>
    </concept>
    <concept>
        <concept_id>10010147.10010178.10010179</concept_id>
        <concept_desc>Computing methodologies~Natural language processing</concept_desc>
        <concept_significance>500</concept_significance>
    </concept>
    <concept>
        <concept_id>10010147.10010257.10010293.10010294</concept_id>
        <concept_desc>Computing methodologies~Neural networks</concept_desc>
        <concept_significance>500</concept_significance>
    </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Applied computing~Law}
\ccsdesc[500]{Computing methodologies~Natural language processing}
\ccsdesc[500]{Computing methodologies~Neural networks}

\keywords{law, natural language processing, pretraining, benchmark dataset}



\maketitle

\section{Introduction}

How can rapid advances in Transformer-based architectures be leveraged to address problems in law?  One of the most significant advances in natural language processing (NLP) has been the advent of ``pretrained'' (or self-supervised) language models, starting with Google's BERT model \cite{devlin-etal-2019-bert}. Such models are pretrained on a large corpus of general texts --- Google Books and Wikipedia articles --- resulting in significant gains on a wide range of fine-tuning tasks with much smaller datasets and have inspired a wide range of applications and extensions \cite{liu2019roberta, rogers2021primer}. 

One of the emerging puzzles for law has been that while \emph{general} pretraining (on the Google Books and Wikipedia corpus) boosts performance on a range of legal tasks, there do not appear to be any meaningful gains from \emph{domain-specific} pretraining (domain pretraining) using a corpus of law. Numerous studies have attempted to apply comparable Transformer architectures to pretrain language models on law, but have found marginal or insignificant gains on a range of legal tasks~\cite{chalkidis-etal-2020-legal, Elwany2019, zhong2020does, zhong2020jec}.  These results would seem to challenge a fundamental tenet of the legal profession: that legal language is \emph{distinct} in vocabulary, semantics, and reasoning \cite{mellinkoff2004language,mertz2007language, tiersma1999legal}.  Indeed, a common refrain for the first year of U.S.\ legal education is that students should learn the ``language of law'': ``Thinking like a lawyer turns out to
depend in important ways on speaking (and reading, and writing) like a lawyer.'' \cite{mertz2007language}.

We hypothesize that the puzzling failure to find substantial gains from domain pretraining in law stem from the fact that existing finetuning tasks may be too easy and/or fail to correspond to the domain of the pretraining corpus task. We show that  existing legal NLP tasks (whether a sentence overrules a prior case and classification of contractual terms of service) are simple enough for naive baselines (BiLSTM) or BERT (without domain-specific pretraining) to achieve high performance.  Observed gains from domain pretraining are hence relatively small. Because U.S.\ law lacks any benchmark task that is comparable to the large, rich, and challenging datasets that have fueled the general field of NLP (e.g., SQuAD~\cite{Rajpurkar2016}, GLUE~\cite{Wang2018}, CoQA~\cite{reddy2019coqa}), we present a new dataset that simulates a fundamental task for lawyers: identifying the legal \emph{holding} of a case.  This CaseHOLD dataset (Case \underline{H}oldings \underline{o}n \underline{L}egal \underline{D}ecisions) provides 53,000+ multiple choice questions with prompts from a judicial decision and multiple potential holdings, one of which is correct, that could be cited.  We construct this dataset using the rules of case citation \cite{bluebook}, which allow us to match a proposition to a source through a comprehensive corpus of U.S.\ case law from 1965 to the present. Intuitively, we extract all legal citations and use the ``holding statement,'' often provided in parenthetical propositions accompanying U.S.\ legal citations, to match context to holding \cite{arredondo2017harvesting}.  For instance, the context in a legal case might be: 
\begin{quote}
In order to preserve a question for appellate review, the defendant must object stating the specific grounds for the ruling the party desired the court to make if the specific grounds were not apparent from the context. 
A general objection, when overruled, is ordinarily not adequate unless the evidence, considered as a whole, makes it clear that there is no purpose to be served from admitting the evidence.
\end{quote} 
This statement could be supported by the following legal citation, followed by the holding statement in parentheses:
\begin{quote}
State v. Perkins, 154 N.C. App. 148, 151-52 (2002) (holding that two general objections were insufficient to properly preserve the issue). 
\end{quote}
CaseHOLD extracts the context, legal citation, and holding statement and matches semantically similar, but inappropriate, holding propositions. This turns the identification of holding statements into a multiple choice task.  We show that this task is difficult for conventional NLP approaches (BiLSTM F1 = 0.4 and BERT F1 = 0.6), even though law students and lawyers are able to solve the task at high accuracy. We then show that there are substantial and statistically significant performance gains from domain pretraining with a custom vocabulary (which we call Legal-BERT), using all available case law from 1965 to the present (a 7.2\% gain in F1, representing a 12\% boost from BERT). We then experimentally assess conditions for gains from domain pretraining with CaseHOLD and find that the size of the finetuning task is the principal other determinant of gains to domain-specific pretraining.

The code, the CaseHOLD dataset, and the Legal-BERT models presented here can be found at: \url{https://github.com/reglab/casehold}.

Our paper informs how researchers should decide when to engage in data and resource-intensive pretraining. Such decisions pose an important tradeoff, as cost estimates for fully pretraining BERT can be upward of \t\pm 1.96 \times \text{standard error}0.910 \pm 0.0120.958 \pm 0.0050.958 \pm 0.0050.963 \pm 0.007\bm{0.974} \pm 0.0050.712 \pm 0.0200.722 \pm 0.0150.773 \pm 0.0190.750 \pm 0.018\bm{0.787} \pm 0.0130.399 \pm 0.0050.613 \pm 0.0050.623 \pm 0.0030.680 \pm 0.003\bm{0.695} \pm 0.003tp<0.001p<0.001p<0.00117.6\% \pm 3.73\pm 1.96 \times \text{standard error}xx =\pm1.96 \times \textbf{standard error}0.8\% \pm 0.1541M.  Deciding whether to pretrain itself can hence have  significant ethical, social, and environmental implications \cite{bender2021dangers}.  Our research suggests that many easy tasks in law may not require domain pretraining, but that gains are most likely when ground truth labels are scarce and the task is sufficiently in-domain. But because estimates of domain-specificity across tasks match our qualitative understanding, this heuristic can also be deployed to determine whether pretraining is worth it. The Custom Legal-BERT results on CaseHOLD suggest that for other difficult and high DS legal tasks, experimentation with custom, task relevant approaches, such as leveraging corpora from task-specific subdomains, building domain-specific model vocabularies, or applying tokenization/segmentation tailored to the characteristics of in-domain text, may yield substantial gains. Bender et al. \cite{bender2021dangers} discussed the significant environmental costs associated with training large language models and in particular, transferring an existing model to a new task or developing new models, which can multiply training costs by thousands of times, since these workflows require retraining to experiment with different model architectures and hyperparameters. DS provides a quick metric for future practitioners to evaluate when resource intensive experimentation on custom or new models may be warranted on other legal tasks. DS scores may also be readily extended to estimate the domain-specificity of tasks in other domains with existing pretrained models like SciBERT and BioBERT \cite{beltagy-etal-2019-scibert, lee_biobert_2019}.

In sum, we have shown that a new benchmark task, the CaseHOLD dataset, and a comprehensively pretrained Legal-BERT model illustrate the conditions for domain pretraining and suggests that language models, too, can embed what may be unique to legal language. 

\begin{acks}
    We thank Devshi Mehrotra and Amit Seru and for research assistance, Casetext for providing the Overruling dataset, Stanford's Institute for Human-Centered Artificial Intelligence for cloud computing support, and Pablo Arredondo, Urvashi Khandelwal, Chris Manning, and Javed Qadrud-Din for helpful comments. 
\end{acks}

















\bibliographystyle{ACM-Reference-Format}
\bibliography{legal-benchmarks}



\appendix
\section{Hyperparameters}
\begin{table}[htbp]
    \centering
    \caption{Hyperparameter settings for finetuning models on the three legal benchmark tasks.}
    \resizebox{\columnwidth}{!}{\begin{tabular}{ccccc}
         \toprule
         Task & Model & Batch Size &  Learning Rate & Epochs\\
         \midrule
         Overruling & Baseline & 16 & 5e-3 & 8 \\
         Overruling & BERT & 16 & 5e-6 & 3 \\
         Overruling & BERT (double) & 16 & 5e-6 & 3 \\
         Overruling & Legal-BERT & 16 & 1e-5 & 2 \\
         Overruling & Custom Legal-BERT & 16 & 5e-6 & 3 \\
         Terms of Service & Baseline & 32 & 9e-4 & 11 \\
         Terms of Service & BERT & 32 & 5e-6 & 3 \\
         Terms of Service & BERT (double) & 32 & 1e-5 & 3 \\
         Terms of Service & Legal-BERT & 32 & 1e-5 & 4 \\
         Terms of Service & Custom Legal-BERT & 16 & 1e-5 & 2 \\
         Citation & Baseline & 16 & 1e-4 & 5 \\
         Citation & BERT & 16 & 9e-6 & 3 \\
         Citation & BERT (double) & 16 & 9e-6 & 3 \\
         Citation & Legal-BERT & 16 & 5e-6 & 3 \\
         Citation & Custom Legal-BERT & 16 & 9e-6 & 3 \\
         \bottomrule
    \end{tabular}
    }
    \label{tab:hyperparameters}
\end{table}

\end{document}
