\section{Experiments}
\label{sec:experiments}
To experiment with our proposed approach, we train models using the LAION mutlimodal dataset (\S \ref{sec:exp-train_setup}), and evaluate on the MS-COCO image and caption generation tasks (\S \ref{sec:exp-eval_setup}).
We show that our retrieval-augmented model (\methodname) significantly improves both image and text  generation performance (\S \ref{sec:exp-main_result}). 
We then analyze the scaling laws and key design choices of our model (\S \ref{sec:exp-scaling_law}, \ref{sec:exp-method-design}).
Finally, \S \ref{sec:qualitative-results} presents qualitative results and capabilities of our model, such as knowledge intensive generation and in-context learning.



\subsection{Training setup} 
\label{sec:exp-train_setup}
\paragraph{Data.}
To train our model, we use LAION \cite{schuhmann2021laion}, an open-sourced dataset that consists of text-image pairs collected from the web.
Following the preprocessing step of Stable Diffusion \cite{rombach2022high}, we cleaned a subset of LAION\footnote{We filter out images with watermark probability above 0.5, unsafe probability above 0.5, or resolution below 256 \!$\times$\! 256.} and obtained 150M text-image pairs in total. Following CM3, we format each text-image pair as an HTML document, ``\texttt{<img alt=[text] src=[image]>}'', where \texttt{[image]} is a sequence of 1024 image tokens obtained by tokenizing the raw image using VQGAN \cite{esser2021taming, gafni2022make}.
These 150M documents are used as our model's final training data. 

We also use the same 150M documents for our external memory $\mathcal{M}$.


\paragraph{Implementation.}
In our retrieval module $R$, we use the off-the-shelf CLIP model (\texttt{ViT-L/14}) \cite{radford2021learning} for both the query and memory encoders $E_Q$ and $E_M$.
We use FAISS \cite{johnson2019billion} to index the external memory $\mathcal{M}$ (Flat Index) and perform MIPS-based retrieval.

For our generator $G$, we use a Transformer \cite{vaswani2017attention} of 2.7B parameters. The sequence length is 4096, which can take up to 3 documents. For each input document $x$, we retrieve $K \sim \operatorname{Uniform}(\{0,1,2\})$ documents and prepend them to $x$. At inference time, we may also retrieve and add $K\!>\!2$ documents via ensemble (see \S \ref{sec:result-oneshot}).\\[0.8mm]
The model is trained from scratch for five days on 256 A100 GPUs.
Our implementation is in PyTorch \citep{pytorch} using Metaseq \citep{zhang2022opt}. We use model parallelism over 4 GPUs and a batch size of 16 sequences per GPU. The optimization uses a linear learning rate decay with 1500 warmup steps, a peak learning rate of 1e-4, a gradient clipping of 1.0, and the Adam optimizer with $\beta_1=0.9$, $\beta_2=0.98$ \citep{kingma2015adam}.


\begin{table}[t]
\begin{center}
\setlength\tabcolsep{2pt}
\resizebox{0.49\textwidth}{!}{
\begin{tabular}{llcccccc@{}}
\toprule
&\multicolumn{1}{l}{\multirow{2}{*}{\textbf{Approach}}} && \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Model type}}} && \multicolumn{2}{c}{\textbf{MS-COCO FID ($\downarrow$)}}\\
\cmidrule{6-7}
&& && & Not finetuned & Finetuned \\
\midrule
&Retrieval Baseline  && -   &&  17.97 & -\\
\midrule
&KNN-Diffusion~\scalebox{0.8}{\cite{ashual2022knn}}\!  && Diffusion & & 16.66 & - &    \\
&Stable Diffusion~\scalebox{0.8}{\cite{rombach2022high}}\!\!\!  && Diffusion & & 12.63 & - &    \\
&GLIDE~\scalebox{0.8}{\cite{nichol2021glide}}  && Diffusion & & 12.24 & - &    \\
&DALL-E 2~\scalebox{0.8}{\cite{ramesh2022hierarchical}}  && Diffusion & & 10.39 & - &   \\
\midrule
&Imagen~\scalebox{0.8}{\cite{saharia2022photorealistic}}  && Diffusion & & {7.27} & - &  \\
&Re-Imagen~\scalebox{0.8}{\cite{reimagen}}  && Diffusion & & {6.88} & 5.25 &  \\
\midrule
&DALL-E (12B)~\scalebox{0.8}{\cite{ramesh2021zero}}  && \scalebox{0.95}[1]{Autoregressive}\!\!  & & $\sim$28 & - &    \\
&CogView (4B)~\scalebox{0.8}{\cite{ding2021cogview}}  && \scalebox{0.95}[1]{Autoregressive}\!\! & & 27.1 & - &  \\
&CogView2 (6B)~\scalebox{0.8}{\cite{ding2022cogview2}}  && \scalebox{0.95}[1]{Autoregressive}\!\! & & 24.0 & 17.7 &  \\
&Parti (20B)~\scalebox{0.8}{\cite{yu2022scaling}}  &&  \scalebox{0.95}[1]{Autoregressive}\!\!  && 7.23 & 3.22 \\
\midrule
&Vanilla CM3 &&  \scalebox{0.95}[1]{Autoregressive}\!\!  && 29.5 & - \\
&\textbf{\methodname (2.7B) (Ours)}  &&  \scalebox{0.95}[1]{Autoregressive}\!\!  && \textbf{15.7} & - \\
\bottomrule
\end{tabular}
}\vspace{-1mm}
\caption{\textbf{Caption-to-image generation performance on MS-COCO}.
Our retrieval-augmented CM3 significantly outperforms the baseline CM3 with no retrieval, as well as other models such as DALL-E (12B parameters). Moreover, our model achieves strong performance with much less training compute than existing models; see Figure \ref{fig:compute} for details.
}
\label{tbl:coco_image_results}
\end{center}\vspace{-2mm}
\end{table} \begin{figure}[!t]
    \centering
    \includegraphics[width=0.35\textwidth]{fig_compute_v1.pdf}
    \vspace{-2mm}
    \caption{\textbf{Image generation quality vs training compute} for our \methodname model and baseline models. $x$-axis is the amount of training compute used in terms of A100 GPU hours. $y$-axis is the MS-COCO FID score (the lower, the better).
    Our retrieval-augmented method achieves significantly better training efficiency than existing works under a similar autoregressive Transformer paradigm (e.g., CM3, DALL-E, Parti). 
    }
    \label{fig:compute}
\end{figure}


\paragraph{Baseline.} 
For our baseline, we train a vanilla CM3 with no retrieval augmentation, using the same model architecture, training data, and amount of compute, for {fair comparison}. Since \methodname's external memory consists of the same training data, the total information accessible to \methodname and vanilla CM3 are controlled to be the same.



\subsection{Evaluation setup}
\label{sec:exp-eval_setup}
For the main evaluation, we use the standard benchmark, MS-COCO  \cite{lin2014microsoft}, to evaluate both text-to-image and image-to-text generation. 
We evaluate our trained model with no further finetuning.

For text-to-image, following prior works \cite{ramesh2021zero, nichol2021glide}, we generate images for the MS-COCO validation set captions and measure the FID score \cite{heusel2017fid} against ground-truth images. To generate an image for each caption, we sample 10 images from the model and then take the top image based on the CLIP score \cite{radford2021learning} with respect to the input caption, as done in \citet{aghajanyan2022cm3}.

For image-to-text, following prior works \cite{alayrac2022flamingo}, we generate captions for the MS-COCO validation set images and measure the CIDEr score \cite{vedantam2015cider} against ground-truth captions.
To generate an caption for each image, we sample 32 captions from the model and take the top caption based on perplexity \citep{fried2022incoder}.



\begin{table}
\begin{center}
\scalebox{0.75}{
\begin{tabular}{lc}
\toprule
\textbf{Approach} & \textbf{CIDEr ($\uparrow$)} \\
\midrule
Retrieval Baseline & 84.1 \\
\midrule
DALL-E\textsuperscript{Small} \scalebox{0.8}{\cite{lucidrains-dalle}}   & 20.2  \\
ruDALL-E-XL \scalebox{0.8}{\cite{ru-dalle}}  & 38.7   \\
minDALL-E \scalebox{0.8}{\cite{kakaobrain2021minDALL-E}}       & 48.0  \\
X-LXMERT \scalebox{0.8}{\cite{cho2020x}}      & 55.8  \\
Parti \scalebox{0.8}{\cite{yu2022scaling}} & {83.9} \\
Flamingo (3B; 4-shot) \scalebox{0.8}{\cite{alayrac2022flamingo}} & {85} \\
Flamingo (80B; 4-shot) \scalebox{0.8}{\cite{alayrac2022flamingo}} & {103} \\
\midrule
Vanilla CM3 &  71.9  \\
\textbf{\methodname (2.7B) (Ours)} &  \textbf{89.1}  \\
\bottomrule
\end{tabular}}
\vspace{-1mm}
\caption{\textbf{Image-to-caption generation performance on MS-COCO} (with no finetuning).
Our retrieval-augmented CM3 significantly outperforms the baseline CM3 with no retrieval. Moreover, our model outperforms other strong models such as Parti (20B parameters) and Flamingo (3B; 4-shot), despite using just $\sim$3B parameters and 2-shot in-context examples.
\label{tbl:coco_caption_results}
}
\end{center}\vspace{-2mm}
\end{table}
 

\begin{figure*}[!t]
\hspace{-2mm}
    \includegraphics[width=1.03\textwidth]{fig_knowledge_intensive_v3.jpeg}\vspace{-1mm}
    \caption{\textbf{Text-to-image generation involving world knowledge.}
    Our retrieval-augmented model (RA-CM3) can generate correct images from entity-rich captions thanks to the access to retrieved images in the context. For example, \methodname's outputs faithfully capture the visual characteristics of various entities (e.g., the shape and painting of Ming Dynasty vase, the amount of Callanish standing stones).
    On the other hand, baseline models without retrieval capabilities (vanilla CM3, Stable Diffusion) tend to struggle, especially when the caption involves rare entities (e.g., ``Ming Dynasty vase'', ``Oriental Pearl tower'', ``Dragon and Tiger Pagodas'').
    }
    \label{fig:knowledge_intensive}
\end{figure*}


\subsection{Main results}
\label{sec:exp-main_result}
\paragraph{Caption-to-image generation.}

Table \ref{tbl:coco_image_results} shows the caption-to-image generation performance on MS-COCO. The metric is FID score, where lower is the better.
Our retrieval-augmented CM3 achieves an FID score of 16 without finetuning, significantly outperforming the baseline CM3 with no retrieval (FID 29) and other models such as DALL-E (FID 28), which is 3x bigger than our model. This suggests that retrieval augmentation provides significant help in generating higher-quality images.

To also factor in training efficiency, 
Figure \ref{fig:compute} visualizes the image generation performance ($y$-axis: FID score) vs the amount of compute used in model training ($x$-axis: normalized A100 GPU hours) for our \methodname model and baseline models. We find that existing models in the autoregressive Transformer paradigm follow a negatively sloped line in this chart (the blue dots and line in Figure \ref{fig:compute}). \methodname is located significantly below this line, i.e., obtaining a better FID with less training compute. This suggests that the proposed retrieval-augmented method achieves significantly better training efficiency than existing works.

Our intuition is that retrieval augmentation allows the model to focus on learning how to use the retrieved documents in the context rather than fitting all the documents into the parameters of the model, speeding up the training process.


\paragraph{Image-to-caption generation.}

Table \ref{tbl:coco_caption_results} shows the image-to-caption generation performance on MS-COCO, with no finetuning.
The metric is the CIDEr score, where the higher is the better.
Our retrieval-augmented CM3 achieves a CIDEr score of 89, significantly outperforming the baseline CM3 with no retrieval (CIDEr 72). Moreover, \methodname outperforms other strong models such as Parti (20B parameters) and Flamingo (3B; 4-shot), despite using just $\sim$3B parameters and 2-shot in-context examples.

These results confirm that our model can perform both image and text generation well, offering the first unified retrieval-augmented multimodal model (Table \ref{tbl:method_comparison}).




\subsection{Analysis}
We analyze the scaling laws of \methodname in \S \ref{sec:exp-scaling_law} and the key design choices of \methodname in \S \ref{sec:exp-method-design}.


\begin{figure}[!t]
\hspace{-3mm}
    \includegraphics[width=0.51\textwidth]{fig_rare_composition_v4.jpeg}\vspace{-1mm}
    \caption{\textbf{Text-to-image generation involving rare \textit{composition} of knowledge.}
    Our retrieval-augmented model (\methodname) can generate faithful images from captions that contain a rare or unseen composition of entities (e.g., ``French flag'' + ``moon'', ``Mount Rushmore'' + ``Japanese cherry'').
    On the other hand, baseline models without retrieval capabilities (vanilla CM3, Stable Diffusion) tend to struggle on these examples, e.g., generate a US flag instead of a French flag on the moon.
    }\vspace{-1mm}
    \label{fig:rare_composition}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.44\textwidth]{fig_image_infill.jpeg}
    \vspace{-2mm}
    \caption{\textbf{Our model can perform better image infilling.}
    Infilling an image requires world knowledge, e.g., to recover the masked patches of the above image, the model needs to know about skiing. While the vanilla CM3 (no retrieval) tends to simply infill legs, our \methodname (with retrieval) successfully recovers both legs and skis. 
    }\vspace{-1mm}
    \label{fig:image_infill}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.44\textwidth]{fig_image_edit.jpeg}
    \vspace{-2mm}
    \caption{\textbf{Our model can perform image editing.} 
    Instead of using retrieved examples in our \methodname's context (Figure \ref{fig:image_infill}), we can also intervene and manually specify the in-context examples to control image infilling. For instance, we can place an image of a person wearing a red jacket in the context to edit the black jacket in the original image to be red (Figure top). 
    }
    \vspace{1mm}
    \label{fig:image_edit}
\end{figure}

\begin{figure}[!t]
\hspace{-3mm}
    \includegraphics[width=0.51\textwidth]{fig_control_gen_v4.jpeg}\vspace{-3mm}
    \caption{\textbf{Controllable image generation.}
    Our \methodname model can control the style of caption-to-image generation by prepending demonstration examples in the generator's context. For instance, when generating an image of ``a house taken on an autumn day'' (Figure top), we can specify a concrete style by
    providing demonstration images (e.g., image of a triangular wooden house and image of orange autumn leaves background). Consequently, \methodname generates an image that follows the visual characteristics of these in-context images.
    }
    \label{fig:control_gen}
\end{figure}

\begin{figure}[!t]
    \centering
    \hspace{-2mm}\includegraphics[width=0.495\textwidth]{fig_one_shot_v2.jpg}
    \vspace{-0mm}
    \begin{center}
    \setlength\tabcolsep{6pt}
    \resizebox{0.32\textwidth}{!}{
    \begin{tabular}{lcccc}
    \toprule
    \multicolumn{1}{l}{\multirow{2}{*}{\textbf{Model}}} & \multicolumn{4}{c}{\textbf{$\boldsymbol{k}$-shot Accuracy}}\\
    \cmidrule{2-5}
    & $k\!=\!1$ & $2$ & $4$ & $8$ \\
    \midrule
    Baseline CM3 &  0.53 & 0.50 & 0.56 & 0.56\\
    \textbf{\methodname (Ours)}~~~~~  &  \textbf{0.78} & \textbf{0.79} & \textbf{0.86} & \textbf{0.9}\\
    \bottomrule
    \end{tabular}
    }
    \end{center}
    \vspace{0mm}
    \caption{\textbf{Our model performs one/few-shot image classification via in-context learning}. 
    To assess the in-context learning ability, we consider a binary image classification task with non-semantic labels (e.g., ``animal X'' and ``animal Y'' instead of ``dog'' and ``cat''). 
    For one-shot classification (Figure top), we feed into the model one pair of demonstration examples, followed by a test example (\texttt{[test\,\,image]}, ``animal\,\_''), for which we predict the probability of ``X'' and ``Y''. 
    For $k$-shot classification (Figure middle), we repeat the above procedure $k$ times, each using a different pair of demonstration examples, and take the average ensemble of the predicted probability (``X'' and ``Y'') across the $k$ passes.\\[1mm]
    The table (Figure bottom) shows the results of $k$-shot classification accuracy, with $k=1,2,4,8$. Across all $k$'s, our \methodname improves on the baseline CM3 by large margins. Increasing $k$ consistently improves accuracy for the $k$ values above.
    }
    \label{fig:image_one_shot}
\end{figure}

\section{Qualitative results}
\label{sec:qualitative-results}
We show novel qualitative capabilities of our \methodname, such as knowledge-intensive multimodal generation (\S \ref{sec:result-knowledge}) and multimodal in-context learning (\S \ref{sec:result-image_infill}, \ref{sec:result-controlled}, \ref{sec:result-oneshot}). 
While GPT-3 \cite{brown2020language} and Flamingo \cite{alayrac2022flamingo} showed in-context learning for text-to-text or image-to-text generation, we show that \methodname can do in-context learning for both text (\S \ref{sec:result-oneshot}) and image (\S \ref{sec:result-image_infill}, \ref{sec:result-controlled}) generation.


\subsection{Knowledge-intensive multimodal generation}
\label{sec:result-knowledge}
Because of the retrieval capability, \methodname is especially good at tasks that require world knowledge or composition of knowledge (knowledge-intensive generation).
Figure \ref{fig:knowledge_intensive}, \ref{fig:rare_composition} show example outputs from \methodname. For each caption, the output images were obtained by sampling 256 images from the model and then re-ranking them using the CLIP score with respect to the input caption. We then apply an off-the-shelf super-resolution tool \cite{rombach2022high}.


\textbf{World knowledge.} 
Figure \ref{fig:knowledge_intensive} shows model outputs for caption-to-image generation that involves world knowledge (e.g., specific entities).
We find that our \methodname model can generate correct images from entity-rich captions thanks to the access to retrieved images in the context. For example, \methodname's outputs faithfully capture the visual characteristics of various entities (e.g., the shape and painting of Ming Dynasty vase, the amount of Callanish standing stones).
On the other hand, baseline models without retrieval capabilities (vanilla CM3, Stable Diffusion) tend to struggle, especially when the caption involves rare entities (e.g., ``Ming Dynasty vase'', ``Oriental Pearl tower'', ``Dragon and Tiger Pagodas'').


\textbf{Composition of knowledge.} 
Figure \ref{fig:rare_composition} shows model outputs for caption-to-image generation that involves rare \textit{composition} of knowledge. 
We find that our retrieval-augmented model can generate faithful images from captions that contain a rare or unseen composition of entities (e.g., ``French flag'' + ``moon'', ``Mount Rushmore'' + ``Japanese cherry'').
On the other hand, baseline models without retrieval capabilities (vanilla CM3, Stable Diffusion) tend to struggle on these examples, e.g., generate a US flag instead of a French flag on the moon (Figure \ref{fig:rare_composition} top). This is likely because the US flag was the most common flag that co-occurred with the moon in the training data.





\subsection{Image infilling and editing}
\label{sec:result-image_infill}

Because our model builds on CM3, it can also perform \textit{infilling}.\footnote{To perform image infilling, the model is given ``\texttt{[unmasked part of image] <mask> [unmasked part of image]}'' as the prompt and generates the \texttt{<mask>} part as the completion. The final output image is constructed by plugging the generated output to the \texttt{<mask>} part of the input.} Figure \ref{fig:image_infill} shows that our \methodname can perform improved image infilling because of the retrieval capability. Infilling an image requires world knowledge, e.g., to recover the masked patches of the image in Figure \ref{fig:image_infill}, the model needs to know about skiing. While the vanilla CM3 (no retrieval) tends to simply infill legs, \methodname (with retrieval) successfully recovers both legs and skis. 

Moreover, instead of using retrieved examples in the \methodname context, we can also intervene and manually specify the in-context examples to control image infilling. Figure \ref{fig:image_edit} shows examples. For instance, we can place an image of a person wearing a red jacket in the context to edit the black jacket in the original image to be red (Figure \ref{fig:image_edit} top).


\subsection{Controlled image generation}
\label{sec:result-controlled}

Controlled generation---controlling the behavior of models in generation (e.g., style of outputs)---is a key problem in generative models \cite{keskar2019ctrl, li2019controllable}.

Our \methodname can control the style of caption-to-image generation by prepending demonstration examples in the generator's context (Figure \ref{fig:control_gen}).
For instance, when generating an image for ``Photo of a house taken on an autumn day'' (Figure \ref{fig:control_gen} top), we can specify a concrete style by providing demonstration images (e.g., an image of a triangular wooden house and an image of orange autumn leaves background). Consequently, \methodname can generate an image that actually follows the visual characteristics of these in-context images. 
This is a very useful capability because we can control generation not only via text (captions) but also via image demonstrations---especially helpful when some visual characteristics we want to specify might be difficult to express in text.

Moreover, the finding that \methodname can use in-context examples for controlled generation suggests that it has acquired a form of \textbf{multimodal in-context learning} ability. Our intuition is that because the \methodname generator has seen relevant multimodal documents prepended to the main document in context during retrieval-augmented training, it has learned how to use in-context examples effectively.



\subsection{One-shot and few-shot image classification}
\label{sec:result-oneshot}
So far we have seen \methodname's in-context learning behavior for image generation (\S \ref{sec:result-controlled}). Here we study its in-context learning ability for image-to-text generation, through one-shot and few-shot image classification.

Figure \ref{fig:image_one_shot} illustrates the experiment.
To assess the true in-context learning ability that factors out prior knowledge of the model, we consider a binary image classification task with non-semantic labels (e.g., ``animal X'' and ``animal Y'' instead of ``dog'' and ``cat''). Specifically, we use ImageNet \cite{deng2009imagenet} to construct such evaluation sets where each class (e.g., animal X or Y) contains the same number of test images (e.g., 100 images).
For one-shot classification (Figure \ref{fig:image_one_shot} top), we feed into the model one pair of demonstration examples (\texttt{[image\,X]}, ``animal X'', \texttt{[image\,Y]}, ``animal Y''), followed by a test example (\texttt{[test\,\,image]}, ``animal\,\_''), for which we predict the probability of ``X'' and ``Y''. 
For $k$-shot classification (Figure \ref{fig:image_one_shot} middle), we repeat the above procedure $k$ times, each using a different pair of demonstration examples, and take the average ensemble of the predicted probability (``X'' and ``Y'') across the $k$ passes.\footnote{An alternative way to use $k$-shot examples could be to prepend all the $k$ pairs of demonstrations directly in \methodname's context, but this would take a significant sequence length in Transformer and might not be easy to scale. 
We find that the ensemble-based method performs well empirically, and comes with benefits such as being more scalable (parallel runs of shorter-length passes) and principled (agnostic to the order of the $k$ examples).}

The table in Figure \ref{fig:image_one_shot} bottom shows the results of $k$-shot (binary) classification accuracy, with $k=1,2,4,8$.
Across all $k$'s, our \methodname obtains significantly improved accuracy over the baseline CM3, which were not trained with retrieved documents in context. In particular, \methodname already performs reasonably well at one-shot (0.78 accuracy at $k=1$). This result suggests that \methodname has acquired a strong in-context learning ability, especially given that we use non-semantic labels for image classes in this evaluation.
Moreover, we find that increasing $k$ consistently improves accuracy for the $k$ values above (0.90 accuracy at $k=8$). 
This observation suggests that ensemble is an effective method to increase the number of in-context examples to provide for the model.
