\section{Experiments}
\label{sec:experiments}
To experiment with our proposed approach, we train models using the LAION mutlimodal dataset (\S \ref{sec:exp-train_setup}), and evaluate on the MS-COCO image and caption generation tasks (\S \ref{sec:exp-eval_setup}).
We show that our retrieval-augmented model (\methodname) significantly improves both image and text  generation performance (\S \ref{sec:exp-main_result}). 
We then analyze the scaling laws and key design choices of our model (\S \ref{sec:exp-scaling_law}, \ref{sec:exp-method-design}).
Finally, \S \ref{sec:qualitative-results} presents qualitative results and capabilities of our model, such as knowledge intensive generation and in-context learning.



\subsection{Training setup} 
\label{sec:exp-train_setup}
\paragraph{Data.}
To train our model, we use LAION \cite{schuhmann2021laion}, an open-sourced dataset that consists of text-image pairs collected from the web.
Following the preprocessing step of Stable Diffusion \cite{rombach2022high}, we cleaned a subset of LAION\footnote{We filter out images with watermark probability above 0.5, unsafe probability above 0.5, or resolution below 256 \!\! 256.} and obtained 150M text-image pairs in total. Following CM3, we format each text-image pair as an HTML document, ``\texttt{<img alt=[text] src=[image]>}'', where \texttt{[image]} is a sequence of 1024 image tokens obtained by tokenizing the raw image using VQGAN \cite{esser2021taming, gafni2022make}.
These 150M documents are used as our model's final training data. 

We also use the same 150M documents for our external memory .


\paragraph{Implementation.}
In our retrieval module , we use the off-the-shelf CLIP model (\texttt{ViT-L/14}) \cite{radford2021learning} for both the query and memory encoders  and .
We use FAISS \cite{johnson2019billion} to index the external memory  (Flat Index) and perform MIPS-based retrieval.

For our generator , we use a Transformer \cite{vaswani2017attention} of 2.7B parameters. The sequence length is 4096, which can take up to 3 documents. For each input document , we retrieve  documents and prepend them to . At inference time, we may also retrieve and add  documents via ensemble (see \S \ref{sec:result-oneshot}).\1mm]
    The table (Figure bottom) shows the results of -shot classification accuracy, with . Across all 's, our \methodname improves on the baseline CM3 by large margins. Increasing  consistently improves accuracy for the  values above.
    }
    \label{fig:image_one_shot}
\end{figure}

\section{Qualitative results}
\label{sec:qualitative-results}
We show novel qualitative capabilities of our \methodname, such as knowledge-intensive multimodal generation (\S \ref{sec:result-knowledge}) and multimodal in-context learning (\S \ref{sec:result-image_infill}, \ref{sec:result-controlled}, \ref{sec:result-oneshot}). 
While GPT-3 \cite{brown2020language} and Flamingo \cite{alayrac2022flamingo} showed in-context learning for text-to-text or image-to-text generation, we show that \methodname can do in-context learning for both text (\S \ref{sec:result-oneshot}) and image (\S \ref{sec:result-image_infill}, \ref{sec:result-controlled}) generation.


\subsection{Knowledge-intensive multimodal generation}
\label{sec:result-knowledge}
Because of the retrieval capability, \methodname is especially good at tasks that require world knowledge or composition of knowledge (knowledge-intensive generation).
Figure \ref{fig:knowledge_intensive}, \ref{fig:rare_composition} show example outputs from \methodname. For each caption, the output images were obtained by sampling 256 images from the model and then re-ranking them using the CLIP score with respect to the input caption. We then apply an off-the-shelf super-resolution tool \cite{rombach2022high}.


\textbf{World knowledge.} 
Figure \ref{fig:knowledge_intensive} shows model outputs for caption-to-image generation that involves world knowledge (e.g., specific entities).
We find that our \methodname model can generate correct images from entity-rich captions thanks to the access to retrieved images in the context. For example, \methodname's outputs faithfully capture the visual characteristics of various entities (e.g., the shape and painting of Ming Dynasty vase, the amount of Callanish standing stones).
On the other hand, baseline models without retrieval capabilities (vanilla CM3, Stable Diffusion) tend to struggle, especially when the caption involves rare entities (e.g., ``Ming Dynasty vase'', ``Oriental Pearl tower'', ``Dragon and Tiger Pagodas'').


\textbf{Composition of knowledge.} 
Figure \ref{fig:rare_composition} shows model outputs for caption-to-image generation that involves rare \textit{composition} of knowledge. 
We find that our retrieval-augmented model can generate faithful images from captions that contain a rare or unseen composition of entities (e.g., ``French flag'' + ``moon'', ``Mount Rushmore'' + ``Japanese cherry'').
On the other hand, baseline models without retrieval capabilities (vanilla CM3, Stable Diffusion) tend to struggle on these examples, e.g., generate a US flag instead of a French flag on the moon (Figure \ref{fig:rare_composition} top). This is likely because the US flag was the most common flag that co-occurred with the moon in the training data.





\subsection{Image infilling and editing}
\label{sec:result-image_infill}

Because our model builds on CM3, it can also perform \textit{infilling}.\footnote{To perform image infilling, the model is given ``\texttt{[unmasked part of image] <mask> [unmasked part of image]}'' as the prompt and generates the \texttt{<mask>} part as the completion. The final output image is constructed by plugging the generated output to the \texttt{<mask>} part of the input.} Figure \ref{fig:image_infill} shows that our \methodname can perform improved image infilling because of the retrieval capability. Infilling an image requires world knowledge, e.g., to recover the masked patches of the image in Figure \ref{fig:image_infill}, the model needs to know about skiing. While the vanilla CM3 (no retrieval) tends to simply infill legs, \methodname (with retrieval) successfully recovers both legs and skis. 

Moreover, instead of using retrieved examples in the \methodname context, we can also intervene and manually specify the in-context examples to control image infilling. Figure \ref{fig:image_edit} shows examples. For instance, we can place an image of a person wearing a red jacket in the context to edit the black jacket in the original image to be red (Figure \ref{fig:image_edit} top).


\subsection{Controlled image generation}
\label{sec:result-controlled}

Controlled generation---controlling the behavior of models in generation (e.g., style of outputs)---is a key problem in generative models \cite{keskar2019ctrl, li2019controllable}.

Our \methodname can control the style of caption-to-image generation by prepending demonstration examples in the generator's context (Figure \ref{fig:control_gen}).
For instance, when generating an image for ``Photo of a house taken on an autumn day'' (Figure \ref{fig:control_gen} top), we can specify a concrete style by providing demonstration images (e.g., an image of a triangular wooden house and an image of orange autumn leaves background). Consequently, \methodname can generate an image that actually follows the visual characteristics of these in-context images. 
This is a very useful capability because we can control generation not only via text (captions) but also via image demonstrations---especially helpful when some visual characteristics we want to specify might be difficult to express in text.

Moreover, the finding that \methodname can use in-context examples for controlled generation suggests that it has acquired a form of \textbf{multimodal in-context learning} ability. Our intuition is that because the \methodname generator has seen relevant multimodal documents prepended to the main document in context during retrieval-augmented training, it has learned how to use in-context examples effectively.



\subsection{One-shot and few-shot image classification}
\label{sec:result-oneshot}
So far we have seen \methodname's in-context learning behavior for image generation (\S \ref{sec:result-controlled}). Here we study its in-context learning ability for image-to-text generation, through one-shot and few-shot image classification.

Figure \ref{fig:image_one_shot} illustrates the experiment.
To assess the true in-context learning ability that factors out prior knowledge of the model, we consider a binary image classification task with non-semantic labels (e.g., ``animal X'' and ``animal Y'' instead of ``dog'' and ``cat''). Specifically, we use ImageNet \cite{deng2009imagenet} to construct such evaluation sets where each class (e.g., animal X or Y) contains the same number of test images (e.g., 100 images).
For one-shot classification (Figure \ref{fig:image_one_shot} top), we feed into the model one pair of demonstration examples (\texttt{[image\,X]}, ``animal X'', \texttt{[image\,Y]}, ``animal Y''), followed by a test example (\texttt{[test\,\,image]}, ``animal\,\_''), for which we predict the probability of ``X'' and ``Y''. 
For -shot classification (Figure \ref{fig:image_one_shot} middle), we repeat the above procedure  times, each using a different pair of demonstration examples, and take the average ensemble of the predicted probability (``X'' and ``Y'') across the  passes.\footnote{An alternative way to use -shot examples could be to prepend all the  pairs of demonstrations directly in \methodname's context, but this would take a significant sequence length in Transformer and might not be easy to scale. 
We find that the ensemble-based method performs well empirically, and comes with benefits such as being more scalable (parallel runs of shorter-length passes) and principled (agnostic to the order of the  examples).}

The table in Figure \ref{fig:image_one_shot} bottom shows the results of -shot (binary) classification accuracy, with .
Across all 's, our \methodname obtains significantly improved accuracy over the baseline CM3, which were not trained with retrieved documents in context. In particular, \methodname already performs reasonably well at one-shot (0.78 accuracy at ). This result suggests that \methodname has acquired a strong in-context learning ability, especially given that we use non-semantic labels for image classes in this evaluation.
Moreover, we find that increasing  consistently improves accuracy for the  values above (0.90 accuracy at ). 
This observation suggests that ensemble is an effective method to increase the number of in-context examples to provide for the model.
