\documentclass{llncs}
\usepackage{amsmath,amssymb,mathrsfs}
\usepackage{color}
\usepackage{graphicx,subfigure}
\usepackage{indentfirst}
\usepackage{cite}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{xcolor}

\newcommand{\be}{}

\renewcommand{\appendix}{\par
\if@chapter@pp
\setcounter{chapter}{0}\setcounter{section}{0}\gdef\@chapapp{\appendixname}\gdef\thechapter{\@Alph\c@chapter}
\else
\setcounter{section}{0}\setcounter{subsection}{0}\gdef\thesection{Appendix \Alph{section}}
\fi
}

\spnewtheorem{subcase}{Subcase}[case]{\it}{}
\numberwithin{subcase}{case}
\begin{document}
\title{Combinations of Some Shop Scheduling Problems and the Shortest Path Problem: Complexity and Approximation Algorithms}

\author{Kameng Nip, Zhenbo Wang\thanks{Corresponding author. Department of Mathematical Sciences, Tsinghua University, Beijing, 100084, China. Email: zwang@math.tsinghua.edu.cn} \and Wenxun Xing}

\institute{Department of Mathematical Sciences, Tsinghua University, Beijing, 100084, China}
\maketitle
\begin{abstract}
We consider several combinatorial optimization problems which combine the classic shop scheduling problems, namely open shop scheduling or job shop scheduling, and the shortest path problem. The objective of the obtained problem is to select a subset of jobs that forms a feasible solution of the shortest path problem, and to execute the selected jobs on the open (or job) shop machines to minimize the makespan. We show that these problems are -hard even if the number of machines is two, and cannot be approximated within a factor less than 2 if the number of machines is an input unless . We present several approximation algorithms for these combination problems.

\noindent \textbf{Keywords.} approximation algorithm; combination of optimization problems; job shop; open shop; scheduling; shortest path.
\end{abstract}

\section{Introduction}
Combinatorial optimization has been developed for more than fifty years, and it involves many active subfields, e.g. network flows, scheduling, bin packing, etc. Usually these subfields were arisen by different applications or theoretical interests, and separately developed. The advancement of science and technology makes it possible to integrate manufacturing, service and management. At the same time, the decision-makers always need to deal with problems incurred by more than one combinatorial optimization problems.

Wang and Cui \cite{WC12} introduced a problem combining two classic combinatorial optimization problems, namely parallel machine scheduling and the vertex cover problem. The combination problem is to select a subset of jobs that forms a vertex cover and to schedule it on some identical parallel machines such that the makespan is minimized. They proposed a -approximation algorithm. This work also inspired the study of the combination of different combinatorial optimization problems.

Flow shop, open shop and job shop are three basic models of multi-stage scheduling problems. Nip and Wang \cite{NW13} studied a combination problem that combines two-machine flow shop scheduling and the shortest path problem. They argued that this problem is -hard, and proposed two approximation algorithms with worst-case ratio  and  respectively. Recently they extended the results to the case that the number of flow shop machines is arbitrary \cite{NW13_2}. One motivation of this problem is manufacturing rail racks. We need to build a railway between two cities. How should we choose a feasible path in a map, such that the corresponding rail tracks (jobs) can be manufactured on some flow shop machine as early as possible? It is convincing to change the flow shop environment into the other two well-known shop environments, i.e. open shop and job shop, as they also apply widely in the real world. This is the core motivation for this current work. In this paper, we mainly study two types of problems: the combination of open shop scheduling and the shortest path problem, and the combination of job shop scheduling and the shortest path problem.

The contributions of this paper are described as follows:
(1) we argue that these combination problems are -hard even if the number of machines is two, and if the number of machines is an input, these problems cannot be approximated within a factor of  unless ; (2) we present several approximation algorithms with performance ratio summarized as follows in which  is any constant and  is the maximum number of operations per job in job shop scheduling.
\begin{table}[h]\label{tab1}
\begin{center}
\begin{tabular}{|c|c|c|}
                  \hline
Number of Machines & Open Shop & Job Shop\\
                  \hline
                  2 & FPTAS & * \\
                   (fixed) & PTAS** & \\
                   (input) &   &  \\
                  \hline
\end{tabular}
\caption{Performance of our algorithms}
\end{center}
* Assume that each job has at most  operations.\\
** A -approximation algorithm is also proposed.
\end{table}

The rest of the paper is organized as follows. In Section \ref{sec_pre}, we give a formal definition of the combination problems stated above, and briefly review some related problems and algorithms will be used subsequently. In Section \ref{sec_com}, we study the computational complexity of these combination problems and give an inapproximability result when the number of machines is an input. Section \ref{sec_approx} provides several approximation algorithms for these problems. Some concluding remarks are provided in Section \ref{sec_end}.

\section{Preliminaries}\label{sec_pre}
\subsection{Problem Description}\label{sec_pd}
We first define the combination problems considered in this paper.

\begin{definition}
Given a directed graph  with two distinguished vertices , and  machines. Each arc  corresponds with a job . Each job  has several operations , , ,  (in the open shop,  and the order is arbitrary; in the job shop, the order is given as a chain). The processing times for  on machine  is . The  () problem is to find a  directed path  of , and to schedule the jobs of  on the open (job) shop machines to yield the minimum makespan over all , where  denotes the set of jobs corresponding to the arcs in .\label{d_comb}
\end{definition}

We denote the number of jobs (arcs) as , i.e. . Denote by  the  machines, and let  be the times of  needed to be processed on . Notice  in the open shop.

It is not difficult to see that the open (job) shop scheduling problem and the classic shortest path problem are special cases of our problems, and hence we say the considered problems are the combinations of the scheduling problems and the shortest path problem. We will show that the combination problems appear different aspects in computational complexity and algorithm design from the shop scheduling problems or the shortest path problem.

In this paper, we will use the results of some optimization problems that
have a similar structure with the classic shortest path problem. We introduce the generalized shortest path problem defined in \cite{NW13}, and extend it to  weights.

\begin{definition}
Given a directed graph  and two distinguished vertices  with . Each arc  is associated with  weights , and we define vector  for . The goal of our shortest path problem  is to find a  directed path  that minimizes , in which  is a given objective function and  contains the decision variables such that  if and only if .
\label{d_sp}
\end{definition}

For simplicity of notation, we denote  instead of  in the rest of the paper. Notice  is a generalization of various shortest path problems. For example, if we set  and , where  is the dot product, it is the classic shortest path problem. If , it is the min-max shortest path problem \cite{ABV06}.

\subsection{Review of Open Shop and Job Shop Scheduling}\label{sec_f2}
Gonzalez and Sahni \cite{Gonzalez1976} first gave a linear time optimal algorithm for . They also proved that  is -hard for , however whether it is strongly -hard is still an outstanding open problem. A feasible shop schedule is called dense when any machine is idle if and only if there is no job that could be processed on it. R{\'a}csm{\'a}ny (see B{\' a}r{\' a}ny and Fiala \cite{bt82}) observed that for any dense schedule, the makespan is at most twice of the optimal solution, that leads to a greedy algorithm. Sevastianov and Woeginger \cite{Sevastianov1998} presented a PTAS for fixed , which is obtained by dividing jobs into large jobs and small jobs. Their algorithm first  optimally schedules the large jobs, then fills the operations of the small jobs into the `gaps'. In this paper, we will use these algorithms, and refer to the GS algorithm, R{\'a}csm{\'a}ny algorithm and the SW algorithm respectively. We present the main results of these algorithms as follows.

\begin{theorem}[\cite{Gonzalez1976}]
The GS algorithm returns an optimal schedule for  in linear time such that

\label{th_o_gs}
\end{theorem}
\begin{theorem}[\cite{bt82,Shmoys1994}]
R{\'a}csm{\'a}ny algorithm returns a 2-approximation algorithm for  such that

where  is the last completed job and processed on .\label{th_o_racsmany}
\end{theorem}

\begin{theorem}[\cite{Sevastianov1998}]
The SW algorithm is a PTAS for .\label{th_o_sw}
\end{theorem}
For job shop schedule with an unlimited number of jobs, few polynomially solvable cases are known. One is , which can be solved by Jackson's rule \cite{Jackson56} that is an extension of Johnson's rule for  \cite{Johnson54}, where  means there are at most  operations per job. The idea is to divide the jobs into two sets according to the processing order of the jobs, and implement Johnson's rule for each job set, then combine the schedules. In fact, a slightly change may lead to -hard problems. For instance,  and  are -hard \cite{Lenstra1977},  and  are strongly -hard \cite{Lenstra1979}. For the general case , Shmoys, Stein and Wein \cite{Shmoys1994} constructed a randomized approximation algorithm with worst-case ratio , where  is the maximum number of operations per job. Schmidt, Siegel and Srinivasan \cite{Schmidt1995} obtained a deterministic algorithm with the same bound by derandomizing. We refer to it as the SSW-SSS algorithm. Moreover, for fixed , the best known approximation algorithm is also proposed in \cite{Shmoys1994} with an approximation factor , where  is an arbitrary constant. If  is a constant, the problem is denoted as  that admits a PTAS \cite{Jansen2003}. We list the main results mentioned above as follows.

\begin{theorem}[\cite{Jackson56}]
Jackson's rule solves  in  time. \label{th_j_jackson}
\end{theorem}

\begin{theorem}[\cite{Shmoys1994,Schmidt1995}]
The SSW-SSS algorithm solves  in polynomial time, and return a schedule with makespan
\label{th_SSW_SSS}
\end{theorem}

Furthermore, a well-known inapproximability result is that ,  and  cannot be approximated within  unless  \cite{Williamson97}. Recently, Mastrolilli and Svensson \cite{Mastrolilli:2011:HAF:2027216.2027218} showed that  cannot be approximated within  for  based on a stronger assumption than .

To conclude this subsection, we list some trivial bounds for a dense shop schedule. Denote by  the makespan of an arbitrary dense shop schedule with job set , and we have

\be
C_{max} \geq \max_{i\in\{1,\cdots,m\}}\left\{\sum_{J_j\in J}\mu_{ij}p_{ij}\right\},\label{eq_max}
\ee
and
\be
C_{max} \leq \sum_{J_j\in J}\sum^m_{i=1}\mu_{ij}p_{ij}.\label{eq_min}
\ee
For each job, we have
\be
C_{max} \geq \sum^m_{i=1}\mu_{ij}p_{ij},\qquad \forall J_j \in J.\label{eq_job}
\ee

\subsection{Review of Shortest Path Problems}\label{sec_sp}
It is well-known that Dijkstra algorithm solves the classic shortest path problem with nonnegative edge weights in  time \cite{DIJ59}.
We have mentioned the min-max shortest path problem, that is -hard even for , and Aissi, Bazgan and Vanderpooten proposed a FPTAS if  is a fixed number \cite{ABV06}. We refer to their algorithm as the ABV algorithm, which has the following result.

\begin{theorem}[\cite{ABV06}]\label{th_minmax}
Given , in a directed graph with  nonnegative weights on each arc, where  is a fixed number. The ABV algorithm finds a path  between two specific vertices satisfying 
for any path  between the two specified vertices, and the running time is .
\end{theorem}

In this paper, sometimes we need to find the min-mix shortest path among all the paths visiting some specified arcs if such a path exists. We propose a modified ABV algorithm for this problem in \ref{app_modABV}.

\section{Computational Complexity}\label{sec_com}
First, notice that  and  are special cases of the corresponding combination problems, thus the combination problem is not easier than its component optimization problems. On the other hand, we know  and  are polynomially solvable, but we can simply verify that the corresponding combination problems, say  and , are -hard by adopting the same reduction proposed in \cite{NW13} for the -hardness of . We summarize the results as Theorem \ref{comp_m}.

\begin{theorem}
Even if ,  is strongly -hard and  is -hard.  is -hard. \label{comp_m}
\end{theorem}

Now we consider the case where the number of machines  is part of the input. Williamson et al. showed that it is -hard to approximate ,  or  within a factor less than  by a reduction from the restricted versions of 3-SAT \cite{Williamson97}. They also showed that deciding if there is a scheduling of length at most 3 is in .
We show that for these problems combining with shortest path problem, deciding if there is a scheduling of length at most 1 is still -hard. Our proof is established by constructing a reduction from 3-Dimensional Matching (3DM) that is -complete \cite{GJ79}.

{\sc 3-Dimensional Matching}:\\
{\bf Instance:} Sets , , , and a family  of triples with  for . Assume that  without loss of generality.\\
{\bf Question:} Does  contains a matching, i.e. a subfamily  for which  and ?

\begin{theorem}
For , deciding if there is a scheduling of length at most  is -hard. \label{th_inapp}
\end{theorem}
\begin{proof}
Given an instance of 3DM, we construct an instance of  with  machines. For , machines ,  and  correspond to ,  and  respectively, and the remained  machines denoted by  are `dummy' machines. The graph has  vertices, denoted by . For , there are arcs ,  and corresponding to jobs . Let ,  and , be a -dimensional vector whose -th component corresponds to the processing time of  on . Let  be a -dimensional vector with 1 for the -th component and s for the others. Now we can define the processing times of the jobs:  if ;  if ;  if . Selecting jobs  implies that  is in the matching. Moreover for , there are  parallel arcs from  towards , corresponding to jobs  with processing times  respectively. Selecting those jobs implies that  is not in the matching. The objective is to find a path from  to  and to schedule the corresponding jobs (arcs) such that the makespan is at most , that completes the reduction. One example is shown in Figure \ref{fig_inapp_2}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=5in]{incomp.eps}\\
  \caption{An example of the reduction with .}\label{fig_inapp_2}
\end{figure}

It can be verified that a schedule with makespan at most 1 if and only if there is a matching for 3DM, and then the result follows. The details are deferred to the full version.
\qed
\end{proof}

Notice that the reduction in Theorem \ref{th_inapp} is also valid for  and , since each job in the reduction has only one nonzero processing time. Therefore we have the following result.
\begin{corollary}
,  and  do not admit an approximation algorithm with worst-case ratio less than , unless .
\end{corollary}

To our knowledge, the best known inapproximability results based on  for ,  and  are still . The corollary implies that the combination problems of the three shop scheduling problems and the shortest problem have stronger inapproximability results.

\section{Approximation Algorithms}\label{sec_approx}
\subsection{An Intuitive Algorithms for Arbitrary }\label{sec_alg_nat}
An intuitive algorithm was proposed for  in \cite{NW13}. The idea is to find the classic shortest path by setting the weight of an arc be the sum of processing times of its corresponding job, and then schedule the returned jobs by Johnson's rule. This simple idea can be extended to all the combination problems we considered, even if the number of machines is an input.

\begin{algorithm}[htb]
\caption{The SD algorithm for {\small ()}}
\label{alg_1}
\begin{algorithmic}[1]
\STATE Find the shortest path in  with weights  by Dijkstra algorithm. For the returned path , construct the job set .

\STATE Obtain a dense schedule for the jobs of  by an arbitrary open (job) shop scheduling algorithm. Let   be the returned job schedule and  the returned makespan, and denote the job set  by .
\RETURN ,  \AND .
\end{algorithmic}
\end{algorithm}

It is easy to show that Algorithm \ref{alg_1} is a -approximation algorithm,
by the bounds (\ref{eq_max}), (\ref{eq_min}), (\ref{eq_job}) and the fact that the returned path is the shortest path with respect to the single weight of each arc.



\subsection{A Unified Algorithms for Fixed }\label{sec_alg_im}
In \cite{NW13}, a -approximation algorithm was proposed for . The idea is to iteratively find a feasible path by the ABV algorithm with two weights for each arcs and schedule the corresponding jobs by Johnson's rule, and then adaptively modified the weights of arcs. We generalize this idea to solve the combination problems considered in this paper. We first propose a unified framework which denoted as UAR(, , ), where  is a polynomial time algorithm used for shop scheduling,  is a control parameter to decide the termination rule of the iterations and the jobs to be modified, and  is the number of machines.

\begin{algorithm}[htb]
\caption{Algorithm UAR(, , )}
\label{alg_uar}
\begin{algorithmic}[1]
\STATE Initially,, for  corresponding to .

\STATE Given , implement the ABV algorithm to obtain a feasible path  to , and construct the corresponding job set as .

\STATE Schedule the jobs of  by the algorithm , denote the returned makespan as , and the job schedule as .

\STATE , , , , .

\WHILE{ \AND there exists  in  satisfies }
    \FOR {all jobs satisfy  in }
        \STATE , .
    \ENDFOR
    \STATE Implement the ABV algorithm to obtain a feasible path  to , and construct the corresponding job set as .

    \STATE Schedule the jobs of  by the algorithm , denote the returned makespan as , and the job schedule as .

    \IF{}
        \STATE , , .
    \ENDIF
\ENDWHILE
\RETURN ,  \AND .
\end{algorithmic}
\end{algorithm}

By setting the appropriate scheduling algorithms and control parameters, we can derive algorithms for different combination problems. Notice that at most  jobs are modified in the UAR(, , ) algorithm, therefore the iterations execute at most  times. Since the scheduling algorithms for shop scheduling and the ABV algorithms are all polynomial time algorithms (for fixed  and ), we claim that the following algorithms based on UAR(, , ) are polynomial-time algorithms. We present the algorithms and their performance as follows and the detailed proofs are given in \ref{app_proof}.

We first apply the UAR(, , ) algorithm to  by setting  be the GS algorithm and , and refer to this algorithm as the GAR algorithm.

\begin{algorithm}[htb]
\caption{The GAR algorithm for }
\label{alg_gar}
\begin{algorithmic}[1]
\STATE Set ,  be the GS algorithm for  and  = 1.
\STATE Solve the problem by implementing UAR(, , ).
\end{algorithmic}
\end{algorithm}

\begin{theorem}
The GAR algorithm is a FPTAS for .  \label{th_GAR}
\end{theorem}

For  where  is fixed, based on UAR(, , ) and R{\'a}csm{\'a}ny algorithm, we obtain the following algorithm, referred to the RAR algorithm by considering an appropriate .

\begin{algorithm}[htb]
\caption{The RAR algorithm for }
\label{alg_rar}
\begin{algorithmic}[1]
\STATE Set  be R{\'a}csm{\'a}ny algorithm for  and .
\STATE Solve the problem by implementing UAR(, , ).
\end{algorithmic}
\end{algorithm}
\begin{theorem}
Given , the RAR algorithm is a -approximation algorithm for .  \label{th_RAR}
\end{theorem}

The framework also can be applied to the combination problem of job shop scheduling and the shortest path problem. For the combination of  and the shortest path problem, we obtain a -approximation algorithm by implementing Jackson's rule and setting  in the UAR(, , ) algorithm. We refer to this algorithm as the JJAR algorithm, and describe it in Algorithm \ref{alg_jjar}. Remind that all  in .

\begin{algorithm}[htb]
\caption{The JJAR algorithm for }
\label{alg_jjar}
\begin{algorithmic}[1]
\STATE Set ,  be Jackson's rule for  and .
\STATE Solve the problem by implementing UAR(, , ).
\end{algorithmic}
\end{algorithm}

Before studying the worst-case performance of the JJAR algorithm, we establish the following lemma. Let  () indicate the order that a job needs to be processed on  () first and then on  ().

\begin{lemma}
For , let  be the makespan returned by Jackson's rule. Suppose we change the processing order of all jobs to be  , and the processing times keep unchanged. Then schedule the jobs by Johnson's rule for , and denote the makespan as  . We have . \label{lemma_J2}
\end{lemma}

The proof of lemma \ref{lemma_J2} is also given in \ref{app_proof}.
\begin{theorem}
Given , the JJAR algorithm is a -approximation algorithm for .  \label{th_jjar}
\end{theorem}

Finally, we study the general case , where  is fixed. By theorem \ref{th_SSW_SSS}, we know that there exists , such that the SSW-SSS algorithm returns a schedule satisfies
\be
C'_{max} \leq\alpha\frac{\log^2(m\mu)}{\log{\log(m\mu)}}\left(\max_{i\in\{1,\cdots,m\}}{\sum_{J_j\in J'}\mu_{ij}p_{ij}}+ \max_{j\in J'}{\sum^{m}_{i=1}\mu_{ij}p_{ij}}\right).\label{eq:sjar_1}
\ee
The factor  is decided by choosing the probability of the randomized steps and the subsequent operations in the SSW-SSS algorithm \cite{Shmoys1994,Schmidt1995}, and its value can be obtained by complicated calculation. Assume we determine such value of . We can design an approximation algorithm with worst-case ratio  for . We refer to this algorithm as the SAR algorithm, and describe it in Algorithm \ref{alg_sar}.

\begin{algorithm}[htb]
\caption{The SAR for }
\label{alg_sar}
\begin{algorithmic}[1]
\STATE Set  be the SSW-SSS algorithm for  and .
\STATE Solve the problem by implementing UAR(, , ).
\end{algorithmic}
\end{algorithm}

\begin{theorem}
The SAR algorithm is an -approximation algorithm for .  \label{th_SAR}
\end{theorem}

However, we remind that the SAR algorithm relies on the assumption, that we can determine the constant  for the SSW-SSS algorithm. We can calculate it by following the details of the SSW-SSS algorithm, and in fact we can choose  large enough to guarantee the performance ratio of our algorithm.

\subsection{A PTAS for }\label{sec_alg_1+e_o}
In the previous subsection, we introduced a -approximation algorithm for  based on the UAR(, , ) algorithm. By a different approach, we propose a -approximation algorithm for any , i.e. a PTAS. We also iteratively find feasible solutions, but guarantee that one of the returned solutions has the same first -th largest jobs with an optimal solution where  is a given constant. Precisely speaking, we say job  is larger than job  if . To do this, we enumerate all size  subsets  of , and then iteratively modify the weights of the graph such that the jobs larger than any job in  will not be chosen. Then find a feasible solution which contains all the jobs in  corresponding to the modified graph, i.e., the corresponding path is constrained to visit all the arcs corresponding to  if such a path exists.

To find a feasible solution in each iteration, we adopt the modified ABV algorithm (see \ref{app_modABV}) to obtain a near optimal min-max shortest path among all the paths visiting the arcs corresponding to  if such a path exists. Then we schedule the selected jobs by the PTAS for  \cite{Sevastianov1998} which is denoted as the SW algorithm. We refer to our algorithm as the SAE algorithm, and describe it in Algorithm \ref{alg_sae}.

\begin{algorithm}[htb]
\caption{The SAE algorithm for }
\label{alg_sae}
\begin{algorithmic}[1]
\STATE Given , set .
\STATE Set , .
\STATE Initially, , for  corresponding to .
\FOR {all , with }\label{alg_sae_lfor}
\STATE , .
\STATE For jobs  with , set , .
\STATE Implement the modified ABV algorithm to obtain a feasible path  of  such that the returned path visits all the arcs corresponding to  if such a path exists. Construct the corresponding job set as .
\STATE Schedule the jobs of  by the SW algorithm.
\IF{}
        \STATE , , .
    \ENDIF
\ENDFOR\label{alg_sae_lendfor}
\RETURN , , .
\end{algorithmic}
\end{algorithm}

There are  distinct subsets , thus the iterations between line \ref{alg_sae_lfor} - line \ref{alg_sae_lendfor} run at most  times, that is a polynomial of  since  is a constant when  and  are fixed. Since the modified ABV algorithm is a FPTAS and the SW algorithm is a PTAS, the running time of each iteration is also bounded by the polynomial of  if  and  are fixed. It suffices to show that the SAE algorithm terminates in polynomial time. The following theorem indicates the SAE algorithm is a PTAS, and detailed proof can be found in \ref{app_SAE_proof}.

\begin{theorem}
The SAE algorithm is a PTAS for .\label{th_SAE}
\end{theorem}

\section{Conclusions}\label{sec_end}
This paper studies several problems combining two well-known combinatorial optimization problems. We show the hardness of the problems, and present
some approximation algorithms. It is interesting to find approximation algorithms with better worst-case ratios for  and . Moreover, it needs further study to close the gap between the 2-inapproximability results and the -approximation algorithms for  and . One can also consider other interesting combination of combinatorial optimization problems.

\section*{Acknowledgments}
This work has been supported by the Bilateral Scientific Cooperation Project BIL10/10 between Tsinghua University and KU Leuven.

\bibliographystyle{plain}
\bibliographystyle{splncs}

\begin{thebibliography}{10}
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\urlprefix}{URL }

\bibitem{ABV06}
Aissi, H., Bazgan, C., Vanderpooten, D.: Approximating min-max (regret)
  versions of some polynomial problems. In: Chen, D., Pardolos, P.M. (eds.)
  COCOON 2006, LNCS, vol. 4112, pp. 428--438. Springer, Heidelberg (2006)

\bibitem{bt82}
B{\' a}r{\' a}ny, I., Fiala, T.: T{\" o}bbg{\' e}pes {\" u}temez{\' e}si
  probl{\' e}m{\' a}k k{\" o}zel optim{\' a}lis megold{\' a}sa (in Hungarian).
  Szigma - Matematikai - K{\" o}zgazdas{\' a}gi Foly{\' o}irat  15,  177--191
  (1982)

\bibitem{DIJ59}
Dijkstra, E.W.: A note on two problems in connexion with graphs. Numerische
  Mathematik  1, 269--271 (1959)

\bibitem{GJ79}
Garey, M.R., Johnson, D.S.: Computers and Intractability: A Guide to the Theory
  of -completeness. Freeman, San Francisco (1979)

\bibitem{Gonzalez1976}
Gonzalez, T., Sahni, S.: Open shop scheduling to minimize finish time. Journal
  of the Association for Computing Machinery  23,  665--679 (1976)

\bibitem{Jackson56}
Jackson, J.R.: An extension of Johnson's results on job-lot scheduling. Naval
  Research Logistics Quarterly  3, 201--203 (1956)

\bibitem{Jansen2003}
Jansen, K., Solis-Oba, R., Sviridenko, M.: Makespan minimization in job shops:
  A linear time approximation scheme. SIAM Journal on Discrete Mathematics  16,
   288--300 (2003),

\bibitem{Johnson54}
 Johnson, S.M.: Optimal two- and three-stage production schedules with setup times
  included. Naval Research Logistics Quarterly, 1, 61--68 (1954).

\bibitem{Lenstra1977}
Lenstra, J.K., Kan, A.R., Brucker, P.: Complexity of machine scheduling
  problems. Annals of Operations Research  1,  343--362 (1977)

\bibitem{Lenstra1979}
Lenstra, J., Rinnooy~Kan, A.: Computational complexity of discrete optimization
  problems. Annals of Operations Research  4,  121--140 (1979),

\bibitem{Mastrolilli:2011:HAF:2027216.2027218}
Mastrolilli, M., Svensson, O.: Hardness of approximating flow and job shop
  scheduling problems. Journal of the Association for Computing Machinery  58,  20:1--20:32 (2011)

\bibitem{NW13}
Nip, K., Wang, Z.: Combination of two-machine flow shop scheduling and shortest
  path problems. In: Du, D.Z., Zhang, G. (eds.) COCOON 2013, LNCS, vol. 7936,
  pp. 680--687. Springer, Heidelberg (2013)

\bibitem{NW13_2}
Nip, K., Wang, Z.: Combination of flow shop scheduling and shortest path problem,
  working paper, Tsinghua University (2013)

\bibitem{Schmidt1995}
Schmidt, J.P., Siegel, A., Srinivasan, A.: Chernoff-hoeffding bounds for
  applications with limited independence. SIAM Journal on Discrete Mathematics
  8,  223--250 (1995)

\bibitem{Sevastianov1998}
Sevastianov, S.V., Woeginger, G.J.: Makespan minimization in open shops: A
  polynomial time approximation scheme. Mathematical Programming  82,  191--198
  (1998)

\bibitem{Shmoys1994}
Shmoys, D.B., Stein, C., Wein, J.: Improved approximation algorithms for shop
  scheduling problems. SIAM Journal on Computing  23,  617--632 (1994)

\bibitem{WC12}
Wang, Z., Cui, Z.: Combination of parallel machine scheduling and vertex cover.
  Theoretical Computer Science  460,  10--15 (2012)


\bibitem{Williamson97}
Williamson, D.P., Hall, L.A., Hoogeveen, J.A., Hurkens, C.A.J., Lenstra, J.K.,
  Sevast'janov, S.V., Shmoys, D.B.: Short shop schedules. Operations Research
  45,  288--294 (1997)
\end{thebibliography}

\newpage
\appendix
\section{The Modified ABV Algorithm}\label{app_modABV}
In this appendix, we give a modified version of the ABV algorithm \cite{ABV06}. The algorithm can determine a near optimal min-max shortest path among all the paths which are constrained to visit all arcs in a arc set  ( is a constant) if such a path exists. We propose a dynamic programming to solve this problem in pseudo-polynomial time.

We index the vertex set  as , where  is the starting point and  is the destination. We denote . For , let  be a set of -dimensional vectors, where  is the number of weights of each arc. A vector  with respect to a  path  with at most  arcs, satisfies that its first  components are the lengths of  from  to  with respect to different weights respectively, and each of the last  components is associated with an arc in  such that the -th component is 1 if  visits , and is 0 otherwise.

Notice that each of the first  components of  can be bounded by  and the other  components are binary, and thus the size (numbers of distinct vectors) of  is no more than . Let  be a -dimensional vector that is obtained by adding  zeros after , and  is the same with  except its -th component is 1. The dynamic programming is described in Algorithm \ref{alg_modABV}.

\begin{algorithm}[htb]
\caption{The Modified ABV Algorithm}
\label{alg_modABV}
\begin{algorithmic}[1]
\STATE .
\STATE ,  for .
\FOR {}
    \FOR {}
        \FOR {each  with }
            \FOR {each vector }
            \IF {, }
                \IF {}
                 \STATE .
                 \ELSE
                 \STATE .
                \ENDIF
            \ENDIF
            \ENDFOR
        \ENDFOR
    \ENDFOR
\ENDFOR
\RETURN the path  corresponding to a vector  such that  is the minimum among all such vectors if such a vector exists, and otherwise return an empty set.
\end{algorithmic}
\end{algorithm}

It is not difficult to see that Algorithm \ref{alg_modABV} returns an optimal solution in time , which is a pseudo-polynomial time algorithm. Based on the scaling technique (for example, see\cite{ABV06}), we can use Algorithm \ref{alg_modABV} to derive a FPTAS, such that given , it returns a path with the value at most  among all the paths  visiting arc set  if such a path exists.

\section{The Performance Analysis of Algorithms in Section \ref{sec_alg_im}}\label{app_proof}
We point out that the proofs of the worst-case performance of algorithms based on UAR(, , ) are quite similar. We give the detailed proof for the GAR algorithm, and describe the key ideas and main steps for the other results since they can be obtained by analogous arguments.

\begin{proof}[Theorem \ref{th_GAR}]
Let  and  be the makespan and the job set returned by the GAR algorithm respectively, and  and  the value of an optimal solution. We consider two cases.
\setcounter{case}{0}
\begin{case} 

It implies that there is at least one job in the optimal solution, say , such that  holds for a current schedule with makespan  during the execution. Notice that the schedule returned by the GAR algorithm is the best one among all current schedules, i.e. . It follows from (\ref{eq_job}) that
\be
C_{max}  \leq C'_{max} \leq p_{1j} + p_{2j} \leq C^*_{max}.
\ee
That is to say the GAR algorithm will return an optimal solution for this case.
\end{case}
\begin{case}


Consider the last current schedule during the execution of the GAR algorithm. We denote the corresponding job set and the makespan as  and  respectively.

In this case, we first argue that . Suppose that this is not the case, since , the weights of arcs corresponding to the jobs in  have not been revised. Hence we have . Moreover, by the assumption , we have . By Theorem \ref{th_minmax}, the solution returned by the ABV algorithm satisfies
 which leads to a contradiction.

Notice that each job in the last current schedule satisfies , since otherwise the algorithm will continue. Therefore, by Theorem \ref{th_o_gs}, the schedule returned by the GS algorithm satisfies
\be
C'_{max} = \max\left\{\sum_{J_j\in J'}p_{1j}, \sum_{J_j\in J'}p_{2j}\right\},\label{eq_go2ar_1}
\ee

Since , we know that all jobs  have not been revised. Thus,
it follows from (\ref{eq_max}), (\ref{eq_job}), (\ref{eq_go2ar_1}), Theorem~\ref{th_minmax} and the fact that the schedule returned by the GAR algorithm is the best one among all current schedules, we have

\end{case}

We have claimed that the algorithms based on UAR(, , ) are polynomial-time algorithms. Moreover, notice that the ABV algorithm is a FPTAS \cite{ABV06}, the GS algorithm runs in linear time, and the GAR algorithm implements the ABV algorithms and the GS algorithm at most  times, and we can claim that the GAR algorithm is a FPTAS for .
\qed
\end{proof}

In the following proofs of Theorems \ref{th_RAR}, \ref{th_jjar} and \ref{th_SAR}, we adopt the same notations as in the proof of Theorem \ref{th_GAR}, and also analyze the same two cases. Then we give the proof of the RAR algorithm for .

\begin{proof}[Theorem \ref{th_RAR}]
The argument for the first case is similar to that of Theorem \ref{th_GAR} by noticing that there is at least one job with  in both the optimal schedule and one current schedule, and it follows that .

For the second case, notice that by Theorem \ref{th_o_racsmany}, the makespan of the last current schedule returned by R{\'a}csm{\'a}ny algorithm satisfies , where  is the last completed job and processed on . Moreover, each jobs in  satisfies , as otherwise the algorithm will continue. By Theorem \ref{th_minmax} and a similar argument as in Theorem \ref{th_GAR}, it is not difficult to show that .
\qed
\end{proof}

Before analyzing the performance of , we first prove Lemma \ref{lemma_J2}.
\begin{proof}[Lemma \ref{lemma_J2}]
Denote  () as the set of jobs with processing order  () in the original job set. In the schedule returned by Jackson's rule for , let  be the makespan, and suppose that the total processing time of  on  is not less than that of  on . By  Jackson's rule, jobs in  are scheduled after jobs in  consecutively on , therefore no idle occurs on . We consider the following cases.
\setcounter{case}{0}
\begin{case}

It follows from (\ref{eq_max}) that .
\end{case}

\begin{case}

\begin{subcase}{no idle occurs on }

In this case, the processes on both machines are consecutive, it is straightforward to show that 
\end{subcase}
\begin{subcase}{idle occurs on }

Remember that Jackson's rule first schedules jobs in  and  by Johnson's rule respectively, denoted the two schedules as  and , and then combines the two schedules. Since Johnson's rule returns a permutation schedule, we can denote  and . Consider the job in , say , which starts the processing on  after the last idle on that machine. It is easy to see that  starts processing on  immediately after its completion on . Recall that no idle occurs on  by assumption, thus we have . Notice that  is obtained by Johnson's rule. We change all jobs' processing order as  and obtain a schedule by applying Johnson's rule to the revised jobs. Let the makespan of this schedule be . Since this schedule is also obtained by Johnson's rule, we know  are also scheduled before , whereas  are scheduled after  in this schedule. Therefore, we have , and it suffices to show that .
\end{subcase}
\end{case}

For the case where total processing time of  on  is less than that of  on , an analogous argument also yields .
\qed
\end{proof}

Now we can study the performance of the JJAR algorithm for .
\begin{proof}[Theorem \ref{th_jjar}]
The first case is similar to that of Theorem \ref{th_GAR} by noticing that there is at least one job with  in both the optimal schedule and one current schedule, it follows that .

For the second case, first by lemma \ref{lemma_J2} we have , where  is the makespan of the last current schedule, and  () is the makespan of the schedule obtained by changing the processing order of all jobs of  to be   and applying Johnson's rule. Assume that , and denote  as the critical job of the schedule with respect to . If , we have . Notice that all jobs in the last current schedule satisfy  in the JJAR algorithm, and we have . A similar argument as in Theorem \ref{th_GAR} shows that . The other situations can be verified by analogous arguments. Thus, the JJAR algorithm is -approximate.
\qed
\end{proof}

Finally, we give the proof of the SAR algorithm for .
\begin{proof}[Theorem \ref{th_SAR}]
The first case is analogous, and we can show that .

For the second case, all the jobs in  satisfy , as otherwise the algorithm will continue. Combining (\ref{eq:sjar_1}) and Theorem \ref{th_minmax}, by a similar argument as in Theorem \ref{th_GAR}, it is not difficult to show that . Thus, there exists an -approximation algorithm for this problem.
\qed
\end{proof}

\section{The Proof of Theorem \ref{th_SAE} in Section \ref{sec_alg_1+e_o}}\label{app_SAE_proof}
This appendix analyzes the performance of the SAE algorithm.
\begin{proof}[Theorem \ref{th_SAE}]
Remember that we have assumed  is a constant.

Consider the iteration that the subset  is exactly the first -th largest jobs of , and denote the makespan and the job set returned in this iteration as  and  respectively.

We now argue that the jobs in  are also the first -th largest jobs of . First, the modified ABV algorithm returns a path visiting the arcs corresponding to  if such a path exists. Since  is exactly the first -th largest jobs of , we have  and  following the analogous arguments in the proof of the algorithms based on UAR(, , ). Therefore  is exactly the set of first -th largest jobs of . Notice that  is the best one among all current schedules, and we have , so we only concern about  and the schedule returned in that iteration in the subsequent analysis.

Denote . In the SAE algorithm, given , we can use the modified ABV algorithm to return a path satisfying
\be
P'_{max} \leq \left(1 + \frac{\epsilon}{3}\right) \max_{i\in \{1, \cdots, m\}}\sum_{a_j\in J^*}p_{ij},
\ee
thus from (\ref{eq_max}) we have,
\be
P'_{max}\leq \left(1 + \frac{\epsilon}{3}\right)C^*_{max}. \label{eq_SAE_abv}
\ee

Now we study the schedule returned by the SW algorithm. Recall that jobs are divided into large jobs and small jobs\cite{Sevastianov1998}:

Furthermore, the operations of jobs in  are divided into two sets:


The value of  is determined by the inequalities
\be
\left(\frac{\epsilon}{m(3+\epsilon)}\right)^{2^{\frac{m(3+\epsilon)}{\epsilon}}} < \alpha \leq \frac{\epsilon}{m(3+\epsilon)}. \label{eq_SAE_alpha2}
\ee
and
\be
\sum_{O_{ij}\in O^1_S} p_{ij} \leq \frac{\epsilon}{3 + \epsilon} P'_{max}. \label{eq_SAE_alpha1}
\ee
We show that such  exists and can be found in polynomial time. Denote , , and  as the operations of  by setting . Thus we have  disjoint operation sets . Notice that the total processing time of all the operations is at most , thus there must be at least one  satisfying (\ref{eq_SAE_alpha1}), then we set  be such . Such  can be found in constant time for fixed  and .

Notice that the number of large jobs satisfy . Consequently, all the large jobs of  belongs to the job set , and thus belongs to . Moreover, notice that the SW algorithm first find an optimal schedule of  by trying any possible order, denote the makespan of this schedule as . Recall that all jobs in  also belong to , therefore we have
\be
C^L_{max} \leq C^*_{max}. \label{eq_sae_jn<C*}
\ee

The remaining analysis is based on the SW algorithm \cite{Sevastianov1998}.

Let  be the last completed machine, and  be the completion time of the last operation of jobs in  on .  and  are referred to the idle time on  during time intervals  and  respectively. The total processing time of all operations on  after time  is denoted by . The worst worst-case ratio is shown by considering several cases as follows.

\setcounter{case}{0}
\begin{case}{}

It implies that . By (), the algorithm returns an optimal schedule.
\end{case}

\begin{case}{}

Consider the last operation  on . By assumption, job  belongs to . Since  cannot be processed on any other idles on  after time , at these idles there must be some machine processing . By combining (\ref{eq_SAE_abv}) and (\ref{eq_SAE_alpha2}), we have
\be
T_t \leq \sum_{i \neq l}p_{ij} \leq (m-1)\alpha P'_{max} \leq \frac{\epsilon}{3 + \epsilon}P'_{max}\leq \frac{\epsilon}{3}C^*_{max}.\label{eq_sae_t}
\ee
Let  be the set of  processed on  after . We consider the following subcases.

\begin{subcase}{}

By (\ref{eq_SAE_alpha2}), (\ref{eq_SAE_alpha1}) and (\ref{eq_SAE_abv}), the total processing time on  after time  (the jobs are all in ) satisfies
\be
P_t \leq \sum_{O_{ij}\in O^1_S} p_{ij} + \sum_{O_{ij}\in O'} p_{ij} \leq \frac{\epsilon}{3 + \epsilon}P'_{max} + (m-1)\alpha^2P'_{max} \leq \frac{2}{3}\epsilon C^*_{max}. \label{eq_SAE_c2}
\ee

Thus, it follows from (\ref{eq_sae_jn<C*}), (\ref{eq_sae_t}) and (\ref{eq_SAE_c2}) that
\be
\begin{split}
C_{max}  \leq C'_{max}&  = t + P_t + T_t \leq C^L_{max} + P_t + T_t\\
&\leq C^*_{max} + \frac{2}{3}\epsilon C^*_{max} + \frac{\epsilon}{3}C^*_{max} = (1 + \epsilon)C^*_{max}.
\end{split}
\ee
\end{subcase}

\begin{subcase}{}

Notice that there are at most  idles (large jobs) before  on machine  in the schedule with respect to . On each idle, since there are at most  jobs being processed on the other machines and , the idle must smaller than , since otherwise some job will be scheduled on that idle. Therefore we have
\be
T_0 \leq \frac{m}{\alpha}\alpha^2P'_{max} \leq \frac{\epsilon}{3 + \epsilon}P'_{max} \leq \frac{\epsilon}{3}C^*_{max}. \label{eq_SAE_c1}
\ee
Since  can be written as , it follows from (\ref{eq_SAE_abv}), (\ref{eq_sae_t}) and (\ref{eq_SAE_c1}) that
\be
\begin{split}
C_{max}  \leq C'_{max}&  = \sum_{j\in J'}p_{lj} + T_0 + T_t \leq P'_{max} + T_0 + T_t\\
&\leq (1 + \frac{\epsilon}{3})C^*_{max} + \frac{2}{3}\epsilon C^*_{max} = (1 + \epsilon)C^*_{max}.
\end{split}
\ee

\end{subcase}
\end{case}
In conclusion, the SAE algorithm produces a schedule with makespan at most .
\qed
\end{proof}
\end{document}
