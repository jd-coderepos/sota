\section{Experiments}\label{sec:experiments}

\subsection{Datasets \& Evaluation Metrics}

\begin{table}[!t]
    \centering
    \small
    \resizebox{0.8\linewidth}{!}{
    \begin{tabular}{l|c|c|c|c|c}
        \hline 
        \multicolumn{2}{c|}{\texttt{Datasets}} & \texttt{\#S} & \texttt{POS} & \texttt{NEU} & \texttt{NEG} \\
        \hline
        \multirow{3}{*}{Lap14} & Train & 906 & 817 & 126 & 517 \\
         & Dev & 219 & 169 & 36 & 141 \\
         & Test & 328 & 364 & 63 & 116 \\
        \hline
        \multirow{3}{*}{14Res} & Train & 1266 & 1692 & 166 & 480 \\
         & Dev & 310 & 404 & 54 & 119 \\
         & Test & 492 & 773 & 66 & 155 \\
        \hline
        \multirow{3}{*}{15Res} & Train & 605 & 783 & 25 & 205 \\
         & Dev & 148 & 185 & 11 & 53 \\
         & Test & 322 & 317 & 25 & 143 \\
        \hline
        \multirow{3}{*}{16Res} & Train & 857 & 1015 & 50 & 329 \\
         & Dev & 210 & 252 & 11 & 76 \\
         & Test & 326 & 407 & 29 & 78 \\
        \hline
        
    \end{tabular}
    }
    \caption{ASTE-V2 dataset statistics. \#S denotes the no. of sentences, ‘POS’, ‘NEU’, and ‘NEG’ denote the no. of positive, neutral, and negative triplets respectively.}
    \label{tab:dataset_stats}
\end{table}

We evaluate \textit{CONTRASTE} on four ASTE datasets (\textbf{ASTE-Data-V2}) released by \citet{xu-etal-2020-position}, which includes one dataset from the \textit{laptop} domain and three datasets from the \textit{restaurant} domain. The statistics of all the datasets are shown in Table \ref{tab:dataset_stats}.

Following prior works, we report \textit{precision}, \textit{recall} and \textit{F1} scores to evaluate and compare methods on the ASTE task. 
A predicted triplet is considered correct if all its predicted elements exactly match with those of a ground-truth opinion triplet.


\subsection{Experimental Setup}
\label{subsec:exp:setup}

\subsubsection{Pre-Training}
For this, we combine the train data from all four ASTE-DATA-V2 datasets (Table \ref{tab:dataset_stats}) to prepare our \textbf{pre-training dataset}.
This results in a total of \textbf{5,039 train} data points (refer Section \ref{subsec:method:contra}) from 3,634 sentences.
Please note that we do not need test data in the pre-training phase.
Also, different from \citet{li-etal-2021-learning-implicit}, we \textbf{do not use any external data} for performing supervised contrastive pre-training.
Pre-trained \textit{t5-base}\footnote{https://huggingface.co/t5-base} was used to initialize the model weights. 
We (pre)train the T5 encoder-decoder framework for 14 epochs using AdamW optimizer \cite{AdamW} with a learning rate of 2e-7 and a batch size of 16.
The temperature parameter  was set to 0.07.


\subsubsection{Fine-Tuning}
\textit{CONTRASTE-MTL} contains around 222 million trainable parameters. 
For each of the datasets, we respectively fine-tune the pre-trained model weights for the downstream ASTE task using AdamW optimizer with a learning rate of 1e-4 for 14Res and 16Res, and 3e-4 for 15Res and Lap14. 
A batch size of 16 was used for all datasets. 
Following \citet{mrini-etal-2022-detection}, we optimize the auxiliary task weights,  (OTD), and  (TCE) for each dataset.
As shown in Fig. \ref{fig:alpha_beta_tuning_appen}, we start by optimizing  with  set to 0.4.
We then optimize  given the optimal  values.
One can visibly observe that the ASTE performance varies with changing task weights.
For each dataset, we obtain a different set of optimal  and  values based on the highest ASTE F\textsubscript{1} scores on the respective val sets.
Fig \ref{fig:alpha_beta_tuning_appen} shows the optimal weights on all for ASTE-Data-V2 datasets;  on Lap14,  on 14Res,  on 15Res,  and  on 16Res.


Each of our models was trained for 20 epochs and the model instance corresponding to the best val F1 score was used to evaluate the test set. 
We report the median scores over five runs of the experiments. 
Each pre-training epoch took 10 minutes and each fine-tuning epoch took 1 minute on \textit{15Res} and 2 minutes on the other three datasets. 
All our experiments were run on Tesla P100-PCIE 16GB GPU. 
We make our codes publicly available.\footnote{https://github.com/nitkannen/CONTRASTE/}

\subsection{Baselines}
We compare our proposed approach with several state-of-the-art ASTE baselines which can be broadly grouped into the following five categories: 
\begin{itemize}[nosep, leftmargin=*]
    \item \textbf{Pipeline:} CMLA, RINANTE \cite{dai-song-2019-neural} and Li-unified-R \cite{li2019unified} are pipeline methods to co-extract aspects and opinion terms. CMLA+, RINANTE+, Li-unified-R, and Peng-two-stage are improved versions proposed by \cite{Peng2020KnowingWH} to jointly extract the aspects, opinions, and corresponding sentiments. 

    \item \textbf{Tagging-based:} OTE-MTL \cite{zhang-etal-2020-multi-task} propose a multi-task framework to jointly extract the three sentiment terms. JET-BERT \cite{xu-etal-2020-position} is an end-to-end approach that proposes a novel position-aware tagging scheme. GTS-BERT \cite{wu-etal-2020-grid} models ASTE as a grid-tagging task. EMC-GCN \cite{chen-etal-2022-enhanced} proposes an Enhanced Multi-Channel GCN network to model the relation between words.

    \item \textbf{Span-based:} Span-ASTE \cite{xu-etal-2021-learning} considers the span-level interactions of targets and opinions while predicting the sentiment. SBSK-ASTE \cite{Feng2022} uses abundant syntax Knowledge to improve ASTE. Span-BiDir \cite{span-bidir-emnlp2022} is a recent approach that uses a span-level bidirectional network to utilize all possible spans for extracting tuples bidirectionally.

    \item \textbf{Generative:} PASTE \cite{mukherjee-etal-2021-paste} is a tagging-free decoding scheme using pointer networks for ASTE. Unified-BART-ABSA \cite{yan-etal-2021-unified} unifies all ABSA tasks with a generative framework using BART. GAS \cite{zhang-etal-2021-towards-generative} proposes a T5-based solution with linearized templated targets. PARAPHRASE \cite{zhang-etal-2021-aspect-sentiment} proposes to decode triplets as templated natural language paraphrases. 

    \item \textbf{MRC-based:} BMRC \cite{bmrc} proposes a novel method to solve ASTE as a Bidirectional Machine Reading Comprehension task. 

\end{itemize}

Among these, PARAPHRASE is closest to our fine-tuning approach, and one of the strongest baselines.
Next, we demonstrate the better advantage of our prompt-based templates over the ones used in PARAPHRASE to fine-tune the model for ASTE.

\subsection{PARAPHRASE vs. Our Templates}
\label{subsec:exp:template_compare}

\begin{table}[!t]
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{l|c|c|c|c}
        \hline 
        \textbf{Model} & \textbf{14Res} & \textbf{15Res} & \textbf{16Res} & \textbf{Lap14} \\
        \hline
        PARAPHRASE & 0.715 & 0.621 & 0.719 & 0.605 \\
        \hline
        ASTE-Base & 0.720 & 0.634 & 0.722 & 0.608 \\
        \hline
        \quad w/ SCL-Sent. & 0.722 & 0.645 & 0.724 & 0.611 \\
        \hline
        CONTRASTE-Base & \textbf{0.728} & \textbf{0.648} & \textbf{0.730} & \textbf{0.614} \\
        \hline    
    \end{tabular}
    }
    \caption{Test F\textsubscript{1} scores on ASTE. Comparing PARAPHRASE with our non-multi-task model variants.}
    \label{tab:para_contra_compare}
\end{table}

\textit{CONTRASTE-Base} refers to our \textit{base} (non-multi-task) model variant which is first pre-trained using our proposed pre-training strategy (refer Section \ref{subsec:method:contra}) and then fine-tuned for ASTE using our proposed templates (refer Table \ref{tab:template_examples}). 
We denote \textit{ASTE-Base} as the variant of \textit{CONTRASTE-Base} without the pre-training part. 
In essence \textit{ASTE-Base} is directly comparable to PARAPHRASE \cite{zhang-etal-2021-aspect-sentiment}, the difference being the templates used for fine-tuning. 
As previously discussed, the templates used in PARAPHRASE do not generalize well across ABSA tasks, and often lead to semantically meaningless or wrong targets (refer Table \ref{tab:template_examples}). 
Our prompt-based templates, on the other hand, can easily be extended across tasks, such as ACOS, as detailed in Section \ref{sec:analysis}. 
Fine-tuning the T5 encoder-decoder framework with our templates also results in better ASTE performance as reported in Table \ref{tab:para_contra_compare}.
One can observe that \textit{ASTE-Base} outperforms PARAPHRASE with overall \textbf{0.9\%} F\textsubscript{1} gains across all four ASTE benchmark datasets.
Detailed results for all compared models are reported in Table \ref{tab:main_res}.


\subsection{Comparison of Supervised Contrastive Pre-Training Approaches}
\label{subsec:exp:scl_compare}

\begin{figure}[!t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cc}
	\includegraphics[width=\linewidth]{figures/before_contra.png} &
	\includegraphics[width=\linewidth]{figures/after_contra.png} \\
	\textbf{(a) Before Contrastive Pre-training} & \textbf{(b) After Contrastive Pre-training} \\
	\end{tabular}
	}
	\caption{t-SNE visualization of decoder-generated [MASK] token embeddings from aspect-based prompts derived from \textit{15Res} val set. Our SCL objective encourages the decoder to produce discriminable representations of different sentiment polarities.}
	\label{fig:contra_adv}
\end{figure}

\begin{table*}
\centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|c|c|c|c|c|c|c|c|c|c|c|c}
        \hline 
        \multirow{2}{*}{\textbf{Model}} &
        \multicolumn{3}{c|}{\textbf{14Res}} &
        \multicolumn{3}{c|}{\textbf{15Res}} &
        \multicolumn{3}{c|}{\textbf{16Res}} &
        \multicolumn{3}{c}{\textbf{Lap14}} \\
        \cline{2-13}
        & \textbf{P.} & \textbf{R.} & \textbf{F\textsubscript{1}} & \textbf{P.} & \textbf{R.} & \textbf{F\textsubscript{1}}  & \textbf{P.} & \textbf{R.} & \textbf{F\textsubscript{1}}  
        & \textbf{P.} & \textbf{R.} & \textbf{F\textsubscript{1}}
        \\
        \hline
        
        CMLA+  & 0.392 & 0.471 & 0.428 & 0.346 & 0.398 & 0.370 & 0.413 & 0.421 & 0.417 & 0.301 & 0.369 & 0.332 \\
        
        RINANTE+  & 0.314 & 0.394 & 0.350 & 0.299 & 0.301 & 0.300 & 0.257 & 0.223 & 0.239 & 0.217 & 0.187 & 0.201 \\
        
        Li-unified-R  & 0.410 & 0.674 & 0.510 & 0.447 & 0.514 & 0.478 & 0.373 & 0.545 & 0.443 & 0.406 & 0.443 & 0.423 \\
        
        Peng-two-stage  & 0.432 & 0.637 & 0.515 & 0.481 & 0.575 & 0.523 & 0.470 & 0.642 & 0.542 & 0.374 & 0.504 & 0.429 \\
        
        ChatGPT  & 0.513 & 0.629 & 0.565 & 0.419 & 0.606 & 0.495 & 0.449 & 0.617 & 0.520 & 0.351 & 0.489 & 0.409 \\
        
        OTE-MTL & 0.630 & 0.551 & 0.587 & 0.579 & 0.427 & 0.489 & 0.603 & 0.534 & 0.565 & 0.492 & 0.405 & 0.451 \\
        
        JET-BERT  & 0.706 & 0.559 & 0.624 & 0.645 & 0.520 & 0.575 & 0.704 & 0.584 & 0.638 & 0.554 & 0.473 & 0.510 \\
        
        PASTE 
        & 0.648 & 0.638 & 0.643 & 0.583 & 0.567 & 0.575 & 0.655 & 0.644 & 0.650 & 0.550 & 0.516 & 0.532  \\
        
        GTS-BERT  & 0.680 & 0.676 & 0.678 & 0.627 & 0.555 & 0.589 & 0.654 & 0.680 & 0.667 & 0.561 & 0.530 & 0.545 \\
        
        GAS  & 0.650 & 0.695 & 0.672 & 0.561 & 0.618 & 0.588 & 0.661 & 0.687 & 0.674 & 0.571 & 0.540 & 0.555 
        \\
        
        Unified-BART-ABSA 
        & 0.655 & 0.650 & 0.653 & 0.591 & 0.594 & 0.593 & 0.666 & 0.687 & 0.676 & 0.614 & 0.562 & 0.587
        \\
        
        BMRC  & 0.756 & 0.618 & 0.680 & 0.685 & 0.534 & 0.601 & 0.712 & 0.611 & 0.658 & 0.706 & 0.490 & 0.578 \\
        
        EMC-GCN  & 0.712 & 0.724 & 0.718 & 0.615 & 0.625 & 0.619 & 0.656 & 0.713 & 0.683 & 0.617 & 0.563 & 0.588 \\
        
        Span-ASTE  & 0.729 & 0.709 & 0.719 & 0.622 & 0.645 & 0.633 & 0.695 & 0.712 & 0.703 & 0.634 & 0.558 & 0.594 \\
        
        PARAPHRASE 
        & 0.711 & 0.719 & 0.715 & 0.604 & 0.639 & 0.621 & 0.701 & 0.739 & 0.719 & 0.634 & 0.578 & 0.604 
        \\

        SBSK-ASTE  &  0.746 & 0.715 & 0.730 & 0.653 & 0.637 & 0.645 & 0.708 & 0.720 & 0.714 & 0.656 & 0.565 & 0.607 \\

        Span-BiDir  &  0.764 & 0.724 & \textbf{0.743} & 0.699 & 0.604 & \underline{0.648} & 0.716 & 0.726 & 0.721 & 0.657 & 0.599 & \underline{0.627} \\
        
        \hline
        
        ASTE-Base & 0.718 & 0.721 & 0.720 & 0.616 & 0.654 & 0.634 & 0.695 & 0.751 & 0.722 & 0.635 & 0.584 & 0.608 \\
        
        CONTRASTE-Base & 0.724 & 0.732 & 0.728 & 0.626 & 0.672 & \underline{0.648} & 0.721 & 0.739 & \underline{0.730} & 0.639 & 0.591 & 0.614 \\
        
        CONTRASTE-MTL
        & 0.736 & 0.744 & \underline{0.740} & 0.653 & 0.667 & \textbf{0.661} & 0.722 & 0.763 & \textbf{0.742} & 0.642 & 0.617 & \textbf{0.629} \\
        
        \hline
        
    \end{tabular}
    }
    \caption{Comparative results on the ASTE-Data-V2 \cite{xu-etal-2020-position}.  denotes that the results are retrieved from \citet{xu-etal-2020-position}.  ChatGPT results are obtained using 100-shot In Context Learning (ICL) prompts. The results for all other methods were reproduced using released codes and original parameters. The highest F1 scores on each dataset are highlighted in \textbf{bold}. The second highest F1 scores are \underline{underlined}.}
    
    \label{tab:main_res}
\end{table*}


As discussed in Section \ref{subsec:method:contra}, we design novel aspect-based prompts to pre-train our encoder-decoder framework by performing supervised contrastive learning (SCL) on decoder-generated \textit{aspect-aware} sentiment embeddings of [MASK] tokens. 
In order to qualitatively understand the effect of such pre-training on the aspect-based sentiment learning ability of our framework, we show in Fig. \ref{fig:contra_adv}, a t-SNE \cite{tsne} visualization of these embeddings on all the aspect-based prompts derived from all four ASTE-Data-V2 test sets. 
We observe that the \textit{positive}, and 
\textit{negative} sentiment embeddings are better clustered and more neatly separated from each other after pre-training. 
Thus, our SCL-based pre-training objective helps in improving the performance on ABSA tasks.

Next, we wanted to compare our pre-training strategy with the one used in existing ABSA works. While \citet{li-etal-2021-learning-implicit} uses SCL for pre-training, \citet{contra-cikm21}, and \citet{ke-etal-2021-classic} use it for fine-tuning their respective models. 
However, different from us, all of them apply SCL on \textit{sentence-level} sentiment representations of sentences. 
In order to replicate their methodology, we pre-train our encoder-decoder framework by applying SCL on mean-pooled representations of sentences from the final layer of encoder.
For this, we collected a total of \textbf{3358 data points} (sentences containing triplets with the same sentiment polarity) from 3,634 sentences combining all four train datasets.
Model weights were initialized with pre-trained \textit{t5-base}, and the framework was (pre)trained for 14 epochs using AdamW optimizer with a learning rate of 2e-5, batch size of 16, and .

We fine-tune \textit{ASTE-base} from this pre-trained checkpoint (settings discussed in Section \ref{subsec:exp:setup}) respectively for each of the datasets and report our results in Table \ref{tab:para_contra_compare} (row ASTE-Base w/ SCL-Sent.). 
We observe that CONTRASTE-Base outperforms ASTE-Base w/ SCL-Sent. with overall \textbf{0.6\%} improvement in F\textsubscript{1} scores.
This establishes the better suitability of performing supervised contrastive pre-training on \textit{aspect-centric} sentiment embeddings, since in ABSA, the sentiments are defined at an aspect level and not at the sentence level.



\subsection{Main Results}
\label{subsec:exp:mainres}

We report the comparison of our model variants with all considered baselines in Table \ref{tab:main_res}.
The majority of the baselines including OTE-MTL, JET-BERT, PASTE, GTS-BERT, BMRC, Span-ASTE, EMC-GCN, SBSK-ASTE, and Span-BiDir use BERT \cite{devlin-etal-2019-bert} as their backbone. 
Unified-BART-ABSA uses BART \cite{lewis-etal-2020-bart}.
GAS and PARAPHRASE are based on a T5 encoder-decoder framework and hence are most similar to our approach. 
PARAPHRASE is our strongest T5-based baseline.
SBSK-ASTE and Span-BiDir are very recent span-based techniques and they qualify to be our strongest two baselines.

We find that ASTE-Base, our non-multi-task non-pre-trained model outperforms PARAPHRASE with overall 0.9\% F\textsubscript{1} gains, however, fails to beat SBSK-ASTE and Span-BiDir.
This observation may be attributed to better feature learning strategies employed by these two methods.
We find that our proposed SCL-based pre-training approach hugely improves the performance of ASTE-Base, as CONTRASTE-Base outperforms SBSK-ASTE with overall 1.4\% F\textsubscript{1} gains, while performing comparably with Span-BiDir.
Finally, CONTRASTE-MTL comfortably outperforms PARAPHRASE with overall \textbf{4.4\%} F\textsubscript{1} gains, SBSK-ASTE with overall \textbf{2.8\%} F\textsubscript{1} gains, and Span-BiDir with overall \textbf{1.2\%} F\textsubscript{1} gains, to achieve \textbf{new state-of-the-art ASTE results}.

\begin{table}[!t]
    \small
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{ccc|c|c|c|c}
        \toprule 
        
        \textbf{CON} & \textbf{OTD} & \textbf{TCE} & \textbf{Triplet} & \textbf{Aspect} & \textbf{Opinion} & \textbf{Sentiment} \\
        \midrule
        
        \xmark & \xmark & \xmark & 0.671 & 0.820 & 0.815 & 0.753 \\
        \hline

        \cmark & \xmark & \xmark & 0.680 & 0.824 & 0.820 & 0.765 \\
        \hline
        
        \xmark & \cmark & \cmark & 0.678 & 0.827 & 0.827 & 0.762 \\
        \hline
        
        \cmark & \xmark & \cmark & 0.685 & 0.832 & 0.828 & 0.770 \\
        \hline
        
        \cmark & \cmark & \xmark & 0.682 & 0.836 & 0.842 & 0.776 \\
        \hline

        \cmark & \cmark & \cmark & 0.692 & 0.840 & 0.848 & 0.784 \\
        \bottomrule
        
    \end{tabular}
    }
    \caption{Ablation Results. We report F1 scores for \textit{Triplet}, \textit{Aspect}, and \textit{Opinion} predictions, and \textit{Sentiment} accuracies for correctly predicted tuples (asp, opin).}
    \label{tab:ablations}
    
\end{table}