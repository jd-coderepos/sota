

\documentclass[onecolumn,journal,letterpaper]{IEEEtran}
\usepackage{hyperref}
\pdfoutput=1

\usepackage{amssymb,amsmath,color,theorem}
\usepackage{graphicx}
\usepackage{algorithm,algorithmic,xspace}

\renewcommand{\natural}{{\mathbb{N}}}
\newcommand{\integer}{{\mathbb{Z}}}
\newcommand{\integernonnegative}{{\mathbb{Z}_{\geq0}}}
\renewcommand{\natural}{{\mathbb{Z}_{>0}}}
\newcommand{\naturalzero}{\mathbb{N}_0}
\renewcommand{\naturalzero}{\integernonnegative}

\newcommand{\irr}{{\mathbb{I}}}
\newcommand{\real}{{\mathbb{R}}}
\newcommand{\subscr}[2]{{#1}_{\textup{#2}}}
\newcommand{\union}{\cup}
\newcommand{\intersection}{\ensuremath{\operatorname{\cap}}}
\newcommand{\bigintersection}{\ensuremath{\operatorname{\bigcap}}}
\newcommand{\intersect}{\ensuremath{\operatorname{\cap}}}
\newcommand{\map}[3]{#1: #2 \rightarrow #3}
\newcommand{\setmap}[3]{#1: #2 \rightrightarrows #3}
\newcommand{\setdef}[2]{\{#1 \; | \; #2\}}
\newcommand{\setseq}[3]{\{#1\}_{#2}^{#3}}
\newcommand{\bigsetdef}[2]{\big\{#1 \; | \; #2\big\}}
\newcommand{\norm}[1]{\|#1\|}
\newcommand{\vers}{\operatorname{vers}}
\newcommand{\sign}{\operatorname{sign}}
\newcommand{\fl}[1]{\left\lfloor #1\right\rfloor}
\newcommand{\ceil}[1]{\left\lceil #1\right\rceil}
\newcommand{\diam}{\operatorname{diam}}
\newcommand{\radius}{\operatorname{radius}}

\newcommand{\alphabet}{\mathbb{A}}
\newcommand{\algofont}[1]{{\small\textsc{#1}}}

\newcommand{\e}{e}
\newcommand{\Enorm}[1]{\|#1\|_{2}}
\newcommand{\Infnorm}[1]{\|#1\|_{\infty}}

\newcommand{\umax}{\subscr{r}{ctr}}
\renewcommand{\umax}{\subscr{u}{max}}
\newcommand{\rcmm}{\subscr{r}{cmm}}
\newcommand{\vmax}{\subscr{v}{max}}

\newcommand{\xmin}{\subscr{x}{min}}
\newcommand{\xmax}{\subscr{x}{max}}
\newcommand{\ymin}{\subscr{y}{min}}
\newcommand{\ymax}{\subscr{y}{max}}


\newcommand{\Hoptimal}[1]{H^{[#1]}_{\textup{optimal}}}
\newcommand{\Hmeasured}[1]{H^{[#1]}_{\textup{measured}}}
\renewcommand{\Hmeasured}[1]{H^{[#1]}_{\textup{sensed}}}


\newcommand{\GG}{\mathcal{G}}
\newcommand{\cball}[2]{B(#2,#1)}


\newcommand{\Bigcball}[2]{B\Big(#2,#1\Big)}
\newcommand{\bigcball}[2]{B\big(#2,#1\big)}
\newcommand{\card}{\ensuremath{\operatorname{card}}}
\newcommand{\until}[1]{\{1,\dots,#1\}}
\newcommand{\fromto}[2]{\{#1,\dots,#2\}}
\newcommand{\diag}{\operatorname{diag}}
\renewcommand{\AA}{\mathcal{A}}
\newcommand{\CC}{\mathcal{C}}
\newcommand{\DD}{\mathcal{D}}
\newcommand{\EE}{\mathcal{E}}
\newcommand{\FF}{\mathcal{F}}
\newcommand{\MM}{\mathcal{M}}
\newcommand{\NN}{\mathcal{N}}
\newcommand{\PP}{\mathcal{P}}
\newcommand{\QQ}{\mathcal{Q}}
\newcommand{\RR}{\mathcal{R}}
\newcommand{\HH}{\mathcal{H}}
\newcommand{\TT}{\mathcal{T}}
\newcommand{\UU}{\mathcal{U}}
\newcommand{\VV}{\mathcal{V}}
\newcommand{\WW}{\mathcal{W}}
\newcommand{\XX}{\mathcal{X}}
\newcommand{\finite}{\mathbb{F}}

\newcommand{\zeroRd}{{0}_{\real^d}}
\newcommand{\oneRd}{{1}_{\real^d}}
\renewcommand{\zeroRd}{{0}_d}
\renewcommand{\oneRd}{{1}_d}
\newcommand{\oneRn}{{1}_n}
\newcommand{\vectorU}{U}

\newcommand{\system}{\mathcal{L}}
\newcommand{\abs}[1]{|#1|}
\newcommand{\subj}{\text{subj to}}
\newcommand{\subject}{\text{subject to}}
\newcommand{\maximize}{\text{maximize}}
\newcommand{\minimize}{\text{minimize}}
\newcommand{\half}{\frac{1}{2}}


\newcommand{\ViolTest}[2]{\textup{\texttt{Viol}}(#1, #2)}
\newcommand{\Basis}[2]{\textup{\texttt{Basis}}(#1, #2)}
\newcommand{\subexLP}{\textup{\texttt{SUBEX\_lp}}}
\renewcommand{\subexLP}{\textup{\texttt{Subex\_LP}}}
\newcommand{\NALPmap}{\mathcal{B}}



\newcommand{\outnbrs}{\subscr{\mathcal{N}}{out}}
\newcommand{\innbrs}{\subscr{\mathcal{N}}{in}}
\newcommand{\nbrs}{\mathcal{N}}

\newcommand{\dist}{\operatorname{dist}}

\newcommand{\formation}{form}
\newcommand{\lineformation}{lform}
\newcommand{\circleformation}{cform}

\newcommand{\bdrect}{\text{Rect}}

\newcommand{\fti}{\text{fti}}



\newcommand{\agent}{A}
\newcommand{\setofagents}{\mathcal{A}}
\newcommand{\true}{\textup{\texttt{true}}}
\newcommand{\false}{\textup{\texttt{false}}}

\newcommand{\network}[1][]{\Sigma_{\textup{#1}}}
\renewcommand{\network}[1][]{\mathcal{S}_{\textup{#1}}}
\newcommand{\supind}[2]{{#1}^{[#2]}}
\newcommand{\subind}[2]{{#1}_{#2}}
\newcommand{\FCC}[1][]{\mathcal{CC}_{\textup{#1}}}
\newcommand{\task}[1][]{\mathcal{T}_{\textup{#1}}}
\newcommand{\msgstandard}{\textup{msg}_{\textup{std}}}
\newcommand{\rdisk}{-\textup{disk}}
\newcommand{\nll}{\textup{\texttt{null}}\xspace}
\newcommand{\ctrl}{\textup{ctl}}
\newcommand{\msg}{\textup{msg}}
\newcommand{\msgs}{\textup{msgs}}
\newcommand{\stf}{\textup{stf}}
\newcommand{\dyn}{\textup{dyn}}
\newcommand{\myboolean}{\textup{\{\true, \false\}}}






\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{lemma}[theorem]{Lemma}
{\theorembodyfont{\rmfamily} \newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{remarks}[theorem]{Remarks}
\newtheorem{example}[theorem]{Example}
\newtheorem{algo}[theorem]{Algorithm}
\newtheorem{problem}[theorem]{Problem}}

\def\QEDclosed{\mbox{\rule[0pt]{1.3ex}{1.3ex}}} \def\QEDopen{{\setlength{\fboxsep}{0pt}\setlength{\fboxrule}{0.2pt}\fbox{\rule[0pt]{0pt}{1.3ex}\rule[0pt]{1.3ex}{0pt}}}}
\def\QED{\QEDclosed} 

\newcommand{\margin}[1]{\marginpar{\tiny\color{red} #1}}
\newcommand{\todo}[1]{\par\noindent{\raggedright\textsc{\color{red}#1}\par\marginpar{{\Large\color{red}}}}}

\newcommand\oprocendsymbol{\hbox{}}
\newcommand\oprocend{\relax\ifmmode\else\unskip\hfill\fi\oprocendsymbol}
\def\eqoprocend{\tag*{}}

\renewcommand{\theenumi}{(\roman{enumi})}
\renewcommand{\labelenumi}{\theenumi}


\renewcommand{\baselinestretch}{1.05}

\newcommand{\PiLP}{\Pi_{\textup{LP}}}

\graphicspath{{figs/}}


\begin{document}

\title{Distributed Abstract Optimization via Constraints Consensus: Theory
  and Applications\thanks{This material is based upon work supported in
    part by ARO MURI Award W911NF-05-1-0219, ONR Award N00014-07-1-0721,
    and NSF Award CNS-0834446. The research leading to these results has
    received funding from the European Community's Seventh Framework
    Programme (FP7/2007-2013) under grant agreement no. 224428 (CHAT
    Project). The authors would like to thank Dr.\ Colin Jones for helpful
    comments.
Early short versions of this work appeared
as~\cite{GN-FB:06d,GN-FB:06z,GN-FB:08x}: differences between these early
short versions and the current article include a much improved
comprehensive treatment, revised complete proofs for all statements, and
the Monte Carlo analysis.
}}

\author{Giuseppe Notarstefano\thanks{Giuseppe Notarstefano is with the
    Department of Engineering, University of Lecce, Via per Monteroni,
    73100 Lecce, Italy, \texttt{giuseppe.notarstefano@unile.it}}
  \quad\and\quad Francesco Bullo\thanks{Francesco Bullo is with the Center for Control,
    Dynamical Systems and Computation, University of California at Santa
    Barbara, CA 93106, USA, \texttt{bullo@engineering.ucsb.edu}} }

\maketitle


\begin{abstract}
  Distributed abstract programs are a novel class of distributed
  optimization problems where (i) the number of variables is much smaller
  than the number of constraints and (ii) each constraint is associated to
  a network node. Abstract optimization programs are a generalization of
  linear programs that captures numerous geometric optimization problems.
  We propose novel constraints consensus algorithms for distributed
  abstract programs: as each node iteratively identifies locally active
  constraints and exchanges them with its neighbors, the network computes
  the active constraints determining the global optimum.
The proposed algorithms are appropriate for networks with weak
  time-dependent connectivity requirements and tight memory constraints.
  We show how suitable target localization and formation control problems
  can be tackled via constraints consensus.
\end{abstract}

\begin{IEEEkeywords}
  Distributed optimization, linear programming, consensus algorithms,
  target localization, formation control.
\end{IEEEkeywords}

\section{Introduction}
This paper focuses on a class of distributed optimization problems and its
application to target localization and formation control.
Distributed optimization and computation have recently received widespread
attention in the context of distributed estimation in sensor networks,
distributed control of actuator networks and consensus algorithms. An early
established reference on distributed optimization is \cite{JNT-DPB-MA:86},
whereas a non-exhaustive set of recent references includes
\cite{NM-AJ:09,AN-AO-PAP:09,BJ-AS-MK-KHJ:08,DPP-MC:07}.
We consider a distributed version of abstract optimization problems.
Abstract optimization problems, sometimes referred to as abstract linear
programs or as LP-type programs, generalize linear programming and model a
variety of machine learning and geometric optimization problems. Examples
of geometric optimization problems include the smallest enclosing ball, the
smallest enclosing stripe and the smallest enclosing annulus problems.
Early references to abstract optimization problems
include~\cite{JM-MS-EW:96,BG:95,MG:95}.
In this paper we are interested in abstract optimization problems where the
number of constraints  is much greater than the number of constraints
 that identify the optimum solution (and where, therefore, there is
a large number of redundant constraints). For example, we are interested in
linear programs where  is much greater than the number of variables 
(in linear programs, ). Under this dimensionality assumption,
we consider distributed versions of abstract optimization programs, where
 is also the number of network nodes and where each constraint is
associated to a node. We consider processor networks described by
arbitrary, possibly time-dependent communication topologies and by
computing nodes with tight memory constraints. After presenting and
analyzing constraints consensus algorithms for distributed abstract
optimization, we apply them to target localization in sensor networks and
to formation control in robotic networks.

The relevant literature is vast; we organize it in three broad
areas. First, linear programming and its generalizations, including
abstract optimization, have received widespread attention in the
literature. For linear programs in a fixed number of variables subject to
 linear inequalities, the earliest algorithm with time complexity in
 is given in~\cite{NM:84}.  An efficient randomized algorithm is
proposed in~\cite{JM-MS-EW:96}, where a linear program in  variables
subject to  linear inequalities is solved in expected time ; the expectation is taken over the internal
randomizations executed by the algorithm.  An elegant survey on randomized
methods in linear programming and on abstract optimization
is~\cite{BG-EW:96}; see also~\cite{MG:95,BG-EW:01}.  The
survey~\cite{PKA-SS:01}, see also \cite{PKA-MS:98}, discusses the
application of abstract optimization to a number of geometric optimization
problems.  Regarding parallel computation approaches to linear programming,
linear programs with  linear inequalities can be solved~\cite{MA-NM:96}
by  parallel processors in time .  However, the
approach in~\cite{MA-NM:96}, see also references therein, is limited to
parallel random-access machines, where a shared memory is readable and
writable to all processors.  Other references on distributed linear
programming include~\cite{YB-JWB-DR:04,HD-HK:08}.



A second relevant literature area is distributed training of support vector
machines (SVMs).  A randomized parallel algorithm for SVM training is
proposed in~\cite{YL-VR:06} by using methods from abstract optimization and
by exploiting the idea of exchanging only active constraints. Along these
lines, \cite{YL-VR-LV:08} extends the algorithm to parallel computing over
strongly connected networks, \cite{JB-YD-JT-OW:08} contains a comprehensive
discussion of SVM training via abstract optimization, and
\cite{KF-BBL-PT:06} applies similar algorithmic ideas to wireless sensor
networks.  The algorithms in~\cite{YL-VR:06,YL-VR-LV:08}, independently
developed at the same time of our
works~\cite{GN-FB:06d,GN-FB:06z,GN-FB:08x}, differ from our constraint
consensus algorithm in the following ways.  First, the number of
constraints stored at the nodes grows at each iteration so that both the
memory and the local computation time at each node may be of order .
Second, our algorithm is proposed for general abstract optimization
problems and thus may be applied to a variety of application
domains. Third, our algorithm exploits a novel re-examination idea, is
shown to be correct for time-varying (jointly strongly connected) digraphs,
and features a distributed halting condition.

As third and final set of relevant references, we include a brief synopsis
of recent progress in target localization in sensor networks and formation
control in robotic networks.  The problem of target localization has been
widely investigated and recent interest has focused on sensors and wireless
networks; e.g., see the recent text~\cite{FZ-LG:04}. In this paper we take
a deterministic worst-case approach to localization, adopting the set
membership estimation technique proposed in~\cite{AG-AV:01}. A related
sensor selection problem for target tracking is studied in \cite{VI-RB:06}.
Regarding the literature on formation control for robotic networks, an
early reference on distributed algorithms and geometric patterns is
\cite{IS-MY:99}.  Regarding the rendezvous problem, that is, the problem of
gathering the robots at a common location, an early reference
is~\cite{HA-YO-IS-MY:99}.  The ``circle formation control'' problem, i.e.,
the problem of steering the robots to a circle formation, is discussed
in~\cite{XD-AK:02}.  The
references~\cite{ME-XH:01b,HGT-GJP-VK:04,JAM-MEB-BAF:04c} are based on,
respectively, control-Lyapunov functions, input-to-state stability and
cyclic pursuit.

The contributions of this paper are twofold.  First, we identify and study
distributed abstract programming as a novel class of distributed
optimization problems that are tractable and widely applicable.  We propose
a novel algorithmic methodology, termed \emph{constraints consensus}, to
solve these problems in networks with various connectivity and memory
constraints: as each node iteratively identifies locally active constraints
and exchanges them with its neighbors, the globally active constraints
determining the global optimum are collectively identified. A constraint
re-examination idea is the distinctive detail of our algorithmic design.
We propose three algorithms, a nominal one and two variations, to solve
abstract programs depending on topology, memory and computation
capabilities of the processor network. We formally establish various algorithm properties, including monotonicity,
finite-time convergence to consensus, and convergence to the
possibly-unique correct solution of the abstract program.  Moreover, we
provide a distributed halting conditions for the nominal algorithm.
We provide a conservative upper bound on the completion time of the nominal
algorithm and conjecture that the completion time depends linearly on 
(i.e., the number of constraints and the network dimension).  Next, we
evaluate the algorithm performance via a Monte Carlo probability-estimation
analysis and we substantiate our conjecture on stochastically-generated
sample problems. Sample problems are randomly generated by considering two
classes of linear programs, taken from~\cite{RS:87}, and three types of
graphs (line-graph, Erd\H{o}s-R\`enyi random graph and random geometric
graph).


As a second set of contributions, we illustrate how distributed abstract
programs are relevant for distributed target localization in sensor
networks and for formation control problems, such as the rendezvous problem
and the line or circle formation problems. Specifically, for the target
localization problem, we design a distributed algorithm to estimate a
convex polytope, specifically an axis-aligned bounding box, containing the
moving target.  Our proposed \emph{eight half-planes consensus algorithm}
combines (i) distributed linear programs to estimate the convex polytope at
a given instant and (ii) a set-membership recursion, consisting of
prediction and update steps, to dynamically track the region. We discuss
correctness and memory complexity of the distributed estimation algorithm.
Next, regarding formation control problems, we design a joint communication
and motion coordination scheme for a robotic networks model involving
range-based communication.  We consider formations characterized by the
geometric shapes of a point, a line, or a circle.  We solve these formation
control problems in a time-efficient distributed manner combining two
algorithmic ideas: (i) the robots implement a constraints consensus
algorithm to compute a common shape reachable in minimum-time, and (ii) the
network connectivity is maintained by means of an appropriate standard
connectivity-maintenance strategy. In the limit of vanishing robot displacement per communication round, our
proposed \emph{move-to-consensus-shape} strategy solves the optimal
formation control tasks.


\subsubsection*{Paper organization}
The paper is organized as follows.  Section~\ref{sec:LP-type problems}
introduces abstract optimization problems.
Section~\ref{sec:network-modeling} introduces network models.
Section~\ref{sec:network-ALP} contains the definition of distributed
abstract program and the constraints consensus
algorithms. Section~\ref{sec:computations} contains the Monte Carlo
analysis of the time-complexity of the constraints consensus
algorithm. Sections~\ref{sec:target-localization}
and~\ref{sec:mintime-formation} contain the application of the proposed
constraints consensus algorithms to target localization and formation
control.


\subsubsection*{Notation}
We let , , and  denote the natural
numbers, the non-negative integer numbers, and the positive real numbers,
respectively. For  and , we let
 denote the closed ball centered at  with radius , that
is, .  For
, we  and  denote the vectors in 
whose entries are all  and , respectively. Similarly, we let
 and  the vectors with  entries  and
, respectively.  For a finite set , we let  denote
its cardinality.  For two functions , we write
 (respectively, ) if there exist
 and  such that  for all  (respectively,  for all ).  Finally, we
introduce the convention that sets are allowed to contain multiple copies
of the same element.

Given  and , let  denote the
distance from  to , that is, .
For distinct  and , let  be
the line through  and .
In what follows, a set of distinct points
, , is in \emph{stripe-generic
  position} if, given any two ordered subsets  and
, either  or
.


\section{Abstract optimization}
\label{sec:LP-type problems}
In this section we present an abstract framework~\cite{PKA-SS:01,BG-EW:01}
that captures a wide class of optimization problems including linear
programming and various machine learning and geometric optimization
problems. Abstract optimization problems are also known as \emph{abstract
  linear programs}, \emph{generalized linear programs} or \emph{LP-type
  problems}.


\subsection{Problem setup and examples }
We consider optimization problems specified by a pair , where
 is a finite set, and  is a
function\footnote{Given a set , the set  is the set of all subsets
  of } with values in a linearly ordered set (); we assume
that  has a minimum value . The elements of  are called
\emph{constraints}, and for ,  is called the
\emph{value} of . Intuitively,  is the smallest value
attainable by a certain objective function while satisfying the constraints
of . An optimization problem of this sort is called an \emph{abstract
  optimization program} if the following two axioms are satisfied:
\begin{enumerate}
\item \emph{Monotonicity}: if , then ;
\item \emph{Locality}: if  with , then, for all ,
  
\end{enumerate}
A set  is \emph{minimal} if  for all
proper subsets  of .  A minimal set  with  is
a .  Given , a \emph{basis of } is a minimal
subset , such that .  A
constraint  is said to be \emph{violated} by , if .

A \emph{solution} of an abstract optimization program  is a
minimal set  with the property that .
The \emph{combinatorial dimension}  of  is the maximum
cardinality of any basis. Finally, an abstract program is called
\emph{basis regular} if, for any basis with  and any
constraint , every basis of  has the same
cardinality of . We now define two important primitive operations that
are useful to solve abstract optimization problems:
\begin{enumerate}
\item \emph{Violation test}: given a constraint  and a basis , it
  tests whether  is violated by ; we denote this primitive by
  ;
\item \emph{Basis computation}: given a constraint  and a basis , it
  computes a basis of ;  we denote this primitive by
  .
\end{enumerate}


\begin{example}[Abstract framework for linear programs]
  \label{ex:abstract-framework-for-LP}
  We recall from \cite{BG-EW:96} how to transcribe a linear program into an
  abstract optimization program.  A linear program (LP) in 
  is given by
  
  where  is the state dimension,  characterizes
  the linear cost function to minimize, and  and  describe  inequality constraints.  In order to
  transcribe the LP into an abstract program, we need to specify the
  constraint set  and the value  for each . The
  constraint set  is simply the set of half-spaces , where .  Defining
  the value function in order to satisfy the monotonicity and locality
  axioms is more delicate: if  and  is the
  minimum of  subject to the constraint set , then the locality
  axiom no longer holds (see Section~4 in \cite{BG-EW:96} for a
  counterexample).  A correct choice is as follows: let  be
  the set  with the \emph{lexicographical order},\footnote{In the
    lexicographic order on , we have  if
    and only if  or ( and ).} and define
  , where  is the (unique)
  \emph{lexicographically minimal} point  minimizing  over the
  constraint set , when it exists and is bounded. If the problem is
  infeasible (the intersection of the constraints in  is empty), then
  .  If the problem is unbounded (no lexicographically
  minimal point exists), then .  If  is finite, then
  a basis of  is a minimal subset of constraints  such that
  .  It is known~\cite{JM-MS-EW:96} that the abstract
  optimization program transcription of a feasible LP is basis regular and
  has combinatorial dimension . A constraint  is violated by 
  if and only if .  \oprocend
\end{example}




\begin{example}\textbf{\textup{(Abstract optimization problems in
    geometric optimization)}}
  \label{rem:examples}
  We present three useful geometric examples, illustrated in
  Figure~\ref{fig:geometric-alp}.
  \begin{enumerate}
  \item \emph{Smallest enclosing ball:} Given  distinct points
    in~, compute the center and radius of the ball of smallest
    volume containing all the points.  This problem is~\cite{JM-MS-EW:96}
    an abstract optimization program with combinatorial dimension~.


  \item \emph{Smallest enclosing stripe:}
Given  distinct points in~ in stripe-generic positions,
    compute the center and the width of the stripe of smallest width
    containing all the points. In the Appendix we prove (for the first time
    at the best of our knowledge) that this problem is an abstract
    optimization program with combinatorial dimension~.

  \item \emph{Smallest enclosing annulus:} Given  distinct points
    in~, compute the center and the two radiuses of the annulus of
    smallest area containing all the points.  This problem
    is~\cite{JM-MS-EW:96} an abstract optimization program with
    combinatorial dimension~.
  \end{enumerate}
  \begin{figure}[htbp]
    \centering
\includegraphics[height=.5\linewidth,angle=90]{ALP_examples}\caption{Smallest enclosing ball, stripe and annulus}
    \label{fig:geometric-alp}
  \end{figure}
  In all three examples, the violation test and the basis computation
  primitives amount to low-dimensional geometric problems and are more or
  less straightforward. Numerous other geometric optimization problems can
  be cast as abstract optimization programs as discussed
  in~\cite{JM-MS-EW:96,BG:95,BG-EW:96,PKA-SS:01} \oprocend
\end{example}

We end this section with a useful lemma, that is an immediate consequence
of locality, and a useful rare property.
\begin{lemma}
  \label{lemma:locality2} For any  and  subsets of ,
   if and only if there exists  such that
  .
\end{lemma}
\begin{proof}
  If there exists  such that , then by
  monotonicity . For the
  other implication, assume  for some ,
  and define  for . We may
  rewrite the assumption  as . If , then
  the locality axiom implies  and the thesis
  follows with . Otherwise, the same argument may be applied to
  . The recursion stops either when 
  (and the thesis follows with ) for some  or when
   (and the thesis follows with ).
\end{proof}


Next, given an abstract optimization program , let  denote
the basis of any .  An element  of  is
\emph{persistent} if  for all  containing .  An
abstract optimization program  is \emph{persistent} if all
elements of  are persistent. The persistence property is useful, as we
state in the following result.
\begin{lemma}
  \label{lemma:all-is-simple-if-persistent}
  Any persistent abstract optimization program  can be solved in a
  number of time steps equal to the dimension of .
\end{lemma}
\begin{proof}
  Let . Set  and
  then update  for . Because
  of persistency, each  is added to  once it is selected as
   and is not removed from  in subsequent basis computations.
\end{proof}
Unfortunately, the persistence property is rare.  Indeed, in
Figure~\ref{fig:LP_counter_example} we show an LP problem where the
persistency property does not hold. In fact, it can be easily noticed that
 is a basis for , but  is a
basis for . In other words  is not violated by
.  The lack of persistency complicates the design of
centralized and distributed solvers for abstract optimization problems. For
example, in network settings, flooding algorithms are not sufficient.

\begin{figure}[htbp]
  \centering
\includegraphics[width=.3\linewidth]{LP_counter_example}
  \caption{An LP problem that is not persistent.}
  \label{fig:LP_counter_example}
\end{figure}


\subsection{Randomized sub-exponential algorithm}
In the following sections we will assume that each node in the network
possesses a routine capable of solving small-dimensional abstract
optimization programs. For completeness' sake, this section reviews the
randomized algorithm proposed in \cite{JM-MS-EW:96}. This algorithm has
expected running time with linear dependence on the number of constraints,
whenever the combinatorial dimension  is fixed, and with
sub-exponential dependence on the ; these bounds are proven in
\cite{JM-MS-EW:96} for linear programs and in \cite{BG-EW:96} for general
abstract optimization programs.
The algorithm, called , has a recursive structure and is based on
the violation test and the basis computation primitives.  Given a set of
constraints  and a candidate basis , the algorithm is
stated as follows:
\begin{center}
\begin{minipage}[c]{.9\textwidth}
\textbf{function} 
\begin{algorithmic}[1]
\STATE \textbf{if} , \textbf{then} \textbf{return} 
\STATE \textbf{else}
\STATE \quad choose a random  and
compute 
\STATE \quad \textbf{if} {  (that is,  is violated by
  ),} \textbf{then}
\STATE \quad \quad compute  basis for 
\STATE \quad \quad \textbf{return} 
\STATE \quad \textbf{else} \textbf{return} 
\STATE \quad \textbf{endif}
\STATE \textbf{endif}
\end{algorithmic}
\end{minipage}
\end{center}

\noindent For the abstract optimization program , the routine is
invoked with , given any initial candidate basis .



\section{Network models}
\label{sec:network-modeling}
Following \cite{NAL:97}, we define a synchronous network system as a
``collection of computing elements located at nodes of a directed network
graph.''  We refer to computing elements are processors.


\subsection{Digraphs and connectivity}
We let  denote a directed graph (or digraph), where
 is the set of nodes and  is the set of
edges.  For each node  of , the number of edges going out from
(resp. coming into) node  is called \emph{out-degree}
(resp. \emph{in-degree}).
A digraph is \emph{strongly connected} if, for every pair of nodes , there exists a path of directed edges that
goes from  to . A digraph is \emph{weakly connected} if replacing all
its directed edges with undirected edges results in a connected undirected
graph. In a strongly connected digraph, the minimum number of edges between
node  and  is called the \emph{distance from  to } and is
denoted . The maximum  taken over all pairs 
is the \emph{diameter} and is denoted .  Finally, we consider
time-dependent digraphs of the form . The time-dependent digraph  is \emph{jointly strongly
  connected} if, for every , the digraph
 is strongly connected.

In a time-dependent digraph, the set of outgoing (incoming) neighbors of
node  at time  are the set of nodes to (from) which there are edges
from (to)  at time . They are denoted by  and
, respectively.


\subsection{Synchronous networks and distributed algorithms}
\label{subsec:networkmodel+distributedalgos}
A \emph{synchronous network} is a time-dependent digraph , where  is the set of \emph{identifiers} of
the processors, and the time-dependent set of edges 
describes communication among processors as follows:  is in
 if and only if processor  can communicate to
processor  at time .


For a synchronous network  with processors , a
\emph{distributed algorithm} consists of (1) the set , called the set of
\emph{processor states} , for all ; (2) the
set , called the \emph{message alphabet}, including the 
symbol; (3) the map , called the
\emph{message-generation function}; and (4) the map , called the \emph{state-transition function}.
The execution of the distributed algorithm by the network begins with all
processors in their start states.  The processors repeatedly perform the
following two actions. First, the th processor sends to each of its
outgoing neighbors in the communication graph a message (possibly the
 message) computed by applying the message-generation function to the
current value of .  After a negligible period of time, the
th processor computes the new value of its processor state
 by applying the state-transition function to the current
value of , and to the incoming messages (present in each
communication edge). The combination of the two actions is called a
\emph{communication round} or simply a round.

In this execution scheme we have assumed that each processor executes all
the calculations in one round. If it is not possible to upper bound the
execution-time of the algorithm, then one may consider a slightly different
network model that allows the state-transition function to be executed
across multiple rounds. When this happens, the message is generated by
using the processor state at the previous round.

The last aspect to consider is the \emph{algorithm halting}, that is a
situation such that the network (and therefore each processor) is in a idle
mode.  Such status can be used to indicate the achievement of a prescribed
task.  Formally we say that a distributed algorithm is in halting status if
the processor state is a fixed point for the state-transition function
(that becomes a self-loop) and no message (or equivalently the 
message) is generated at each node.


\section{Distributed abstract optimization}
\label{sec:network-ALP}
In this section we define distributed abstract programs, propose novel
distributed algorithms for their solutions and analyze their correctness.


\subsection{Problem statement}
Informally, a \emph{distributed abstract program} consists of three main
elements: a network, an abstract optimization program and a mechanism to
distribute the constraints of the abstract program among the nodes of the
network.
\begin{definition}
  A distributed abstract program is a tuple  consisting of
  \begin{enumerate}
  \item , a synchronous network;
  \item , an abstract program; and
  \item , a surjective map called
    \emph{constraint distribution map} that associates to each constraint
    one network node.
  \end{enumerate}
  If the map  is a bijection, we denote the distributed abstract
  program with the pair . A \emph{solution} of  is attained when all network processors have
  computed a solution to .
\end{definition}

\begin{remark}
  The most natural choice of constraint distribution map  is a
  bijection; in this case, (i) the network dimension is equal to the
  dimension of the abstract optimization program and (ii) precisely one
  constraint is assigned to each network node.  More complex distribution
  maps are interesting depending on the computation power and memory of the
  network processors.  In what follows, we typically assume  to
  be a bijection. \oprocend
\end{remark}



\subsection{Constraints consensus algorithms}
Here we propose three novel distributed algorithms that solve distributed
abstract programs.  First, we describe a distributed algorithm that
is well-suited for time-dependent networks whose nodes have bounded
computation time, memory and in-degree.  Equivalently, the algorithm is
applicable to networks with arbitrary in-degree, but also arbitrary
computation time and memory.
Then we describe two variations that deal with arbitrary in-degree versus
short computation time and small memory. The second version of the
algorithm is well-suited for time-dependent networks that have arbitrary
in-degree and bounded computation time, but also arbitrary memory (in the
sense that the number of stored messages may depend on the number of nodes
of the network).  The third algorithm considers the case of
time-independent networks with arbitrary in-degree and bounded computation
time and memory.

In all algorithms we consider a synchronous network  and an abstract
program  with  and with combinatorial
dimension .  We define a distributed abstract program by assuming
that constraints and nodes are in a one-to-one relationship, and we let
 be the constraint associated with network node . Here is an
informal description of our first algorithm.
\begin{quote}
  \emph{Constraints Consensus Algorithm}: Beside having access to the
  constraint , the th processor state contains a candidate basis
   consisting of  elements of .  The processor
  state  is initialized to  copies of . At each
  communication round, the processor performs the following tasks: (i) it
  transmits  to its out-neighbors and acquires from its
  in-neighbors their candidate bases; (ii) it solves an abstract optimization
  program with constraint set given by the union of: its constraint ,
  its candidate basis  and its in-neighbors' candidate
  bases; (iii) it updates  to be the solution of the
  abstract program computed at step (ii).
\end{quote}
For completeness' sake, the following table presents the algorithm in a way
that is compatible with the model given in
Section~\ref{subsec:networkmodel+distributedalgos}.  The 
algorithm is adopted as local solver for abstract optimization programs.


\bigskip \hrule width \linewidth \smallskip

\noindent\begin{minipage}{0.44\linewidth}\textbf{\texttt{Problem data:}}\end{minipage}\begin{minipage}{0.56\linewidth}\end{minipage}

\noindent\begin{minipage}{0.44\linewidth}\textbf{\texttt{Algorithm:}}\end{minipage}\begin{minipage}{0.56\linewidth}Constraints Consensus\end{minipage}

\noindent\begin{minipage}{0.44\linewidth}\textbf{\texttt{Message alphabet:}}\end{minipage}\begin{minipage}{0.56\linewidth}\end{minipage}

\noindent\begin{minipage}{0.44\linewidth}\textbf{\texttt{Processor state:}}\end{minipage}\begin{minipage}{0.56\linewidth} with
  \end{minipage}

\noindent\begin{minipage}{0.44\linewidth}\textbf{\texttt{Initialization:}}\end{minipage}\begin{minipage}{0.56\linewidth}\end{minipage}

\bigskip

\noindent\textbf{\texttt{function}}  
\begin{algorithmic}[1]
  \STATE \textbf{return} 
\end{algorithmic}

\medskip

\noindent\textbf{\texttt{function}}   \\
\emph{\% executed by node~, with }\
     \label{eq:globalopt}
     \bar{\phi} = \phi(\supind{B}{1}\union\cdots \union \supind{B}{n}).
   
     \label{eq:no-idea}
     \phi(\supind{B}{1}\union\cdots \union \supind{B}{k})
     = \bar{\phi},
   
     \label{eq:absurb}
     \phi(\supind{B}{1}\union\cdots \union \supind{B}{k}
     \union \supind{B}{k+1} )
     > \bar{\phi}.
   
     \label{eq:n+1greater-n}
     \phi(\supind{B}{1}\union\cdots \union \supind{B}{k} \union
     \supind{B}{k+1} ) >
     \phi(\supind{B}{1}\union\cdots \union \supind{B}{k}).
   
     \label{eq:three}
     \phi(\supind{B}{1}\union\cdots \union
     \supind{B}{k}\union\{g\}) > \phi(\supind{B}{1}\union\cdots \union
     \supind{B}{k}).
   
     \label{eq:four}
     \phi(\supind{B}{p}\union\{g\}) >  \phi(\supind{B}{p}).
   
     \label{eq:six}
     \phi(\supind{B}{p}) &\overset{\text{bases have converged}}{=}
     \phi(\supind{B}{p}\union \supind{B}{k+1})
\overset{\text{monotonicity}}{\geq}
     \phi(\supind{B}{p}\union \{g\})
\overset{\text{by equation}~\eqref{eq:four}}{>} \phi(\supind{B}{p}).
   
     \bar{\phi} = \phi(\supind{B}{1}\union\cdots\union\supind{B}{n})
     =\phi(H).
   
    \label{eq:monotonicity-(i,j)-t_t+1}
    \phi(\supind{B}{i}(t)) \leq \phi(\supind{B}{j}(t+1)),
  
    \phi(B) \geq \phi(\supind{B}{j}(\dist(i,j))) \geq \phi(B).
  
  \begin{split}
    \min  &\quad c^T x\\
    \subject &\quad A x \leq b,
  \end{split}
.75em] \hline
      \
    \hat{p}_N(\gamma) = \frac{1}{N}\sum_{i=1}^N \mathbb{I}_{J,\gamma}(\Delta^{(i)}).
  
    \label{bound:Chernoff}
    N \geq \frac{1}{2\epsilon^2}\log\frac{2}{\eta}.
    \label{eq:target_dyn}
  p(t + 1) = p(t) + v(t),

  \label{eq:set_memb_recurs}
  

  \label{eq:set_memb_recurs_approx}
  
 \label{eq:vtheta}
  \begin{split}
    \max   &\quad [\cos(\theta)\;\; \sin(\theta)] \cdot x\\
    \subject &\quad a_i^T x \leq b_i, \quad i\in\until{k}, \\
    & \quad \xmin\leq{x}\leq\xmax, \quad \ymin\leq{y}\leq\ymax.
  \end{split}

  {E}(t|t-1) &= \{  {h}_1(t|t-1),\dots,{h}_8(t|t-1)   \},
  \enspace \text{and}
  \\
  {E}(t|t)  &= \{  {h}_1(t|t),\dots,{h}_8(t|t)   \}.

  \{h_1(0|0),\dots,h_8(0|0)\}  = \PiLP\big(
  \{\supind{h}{1}(0),\dots,\supind{h}{n}(0)\} \big).

h_i(t+\tau|t) = \setdef{x\in\real^2}{\subind{a}{i}(t)^T
  x\leq\subind{b}{i}(t) + \subscr{v}{max} \tau}.

  \{h_1(t|t),\dots,h_8(t|t)\} = \PiLP(H(t)),

  H(t) = \{h_1(t|t-1),\dots,h_8(t|t-1)\}\union
  \{\supind{h}{1}(t),\dots,\supind{h}{n}(t)\}.
-1.9em]
\begin{algorithmic}[1]

  \STATE time-translate by an amount   all constraints
  constraints in     , , and
  

  \IF{a new measurement is taken at this time,}
  \STATE add it to ; drop oldest
  measurement from 
  \ENDIF

  \STATE set 

  \STATE \textbf{return} 
\end{algorithmic}

\smallskip \hrule width \linewidth \medskip

Finally, we collect some straightforward facts about this algorithm; we
omit the proof in the interest of brevity.

\begin{proposition}\textbf{\textup{(Properties of the eight half-planes consensus algorithm)}}
  Consider a connected network  of sensors that measure
  half-plane constraints and that implement the eight half-plane consensus
  algorithm.  Assume the target does not move, that is, set . The
  following statements hold:
  \begin{enumerate}
  \item the candidate optimal constraints at each node contain the target
    at each instant of time;
  \item the candidate optimal constraints at each node monotonically
    improve over time; and
  \item additionally, if each node makes at most  measurements in finite
    time, then the candidate optimal constraints at each node converge in
    finite time to the globally optimal  half-plane constraints.
  \end{enumerate}
\end{proposition}

Next, let us state some memory complexity bounds for the eight half-plane
consensus algorithm. Again, we adopt the convention that a memory unit is
the amount of memory required to store a constraint in .  Each node 
of the network requires  memory units in order to
implement the algorithm.
If we assume that number  of stored measurements is independent of 
and that the indegree of each node is bounded irrespectively of , then
the algorithm memory complexity is in . Vice-versa, in worst-case
graphs, the algorithm memory complexity is in .




\section{Application to formation control for robotic networks}
\label{sec:mintime-formation}

In this section we apply constraints consensus ideas to formation control
problems for networks of mobile robots.  We focus on formations with the
shapes of a point, a line, or a circle. (The problem of formation control
to a point is usually referred to as the rendezvous or gathering problem.)
We solve these formation control problems in a time-efficient manner via a
distributed algorithm regulating the communication among robots and the
motion of each robot.


\subsection{Model of robotic network}
We define a robotic network as follows. Each robot is equipped with a
processor and robots exchange information via a communication graph.
Therefore, the group of robots has the features of a synchronous network
and can implement distributed algorithms as defined in
Section~\ref{sec:network-modeling}. However, as compared with a synchronous
network, a robotic network has two distinctions: (i) robots control their
motion in space, and (ii) the communication graph among the robots depends
upon the robots positions, rather than time.

Specifically, the \emph{robotic network} evolves according to the following
discrete-time communication, computation and motion model. Each robot
 moves between rounds according to the first order
discrete-time integrator , where  and
.  At each discrete time instant, robots
at positions  communicate according to the disk graph
 defined as
follows: an edge , , belongs to
 if and only if
 for some .

A \emph{distributed algorithm for a robotic network} consists of (1) a
distributed algorithm for a synchronous network, that is, a processor
state, a message alphabet, a message-generation and a state-transition
function, as described in Section~\ref{sec:network-modeling}, (2) an
additional function, called the \emph{control function}, that determines
the robot motion, with the following domain and co-domain:

Additionally, we here allow the message generation and the state transition
to depend upon not only the processor state but also the robot position.
The state of the robotic network evolves as follows. First, at each
communication round , each processor  sends to its outgoing neighbors
a message computed by applying the message-generation function to the
current values of  and .  After a negligible
period of time, the th processor resets the value of its processor state
 by applying the state-transition function to the current
values of  and , and to the messages received
at time . Finally, the position  of the th robot at
time  is determined by applying the control function to the current
value of  and , and to the messages received
at time .


In formal terms, if  denotes the message vector received
at time  by agent  (with  being the message
received from agent ), then the evolution is determined by

with the convention that  if
.


\subsection{Formation tasks and related optimization problems}
\label{sec:mintime-formation_optimal-formation}
Numerous definitions of robot formation are considered in the multi-agent
literature. Here we consider a somehow specific situation.  Let
, , and  be the
set of points, lines and circles in the plane, respectively. We refer to
these three sets as the \emph{shape sets}.  We aim to lead all robots in a
network to a single element of one of the shape sets If  is a selected
shape set, the \emph{formation task} is achieved by the robotic network if
there exists a time  such that for all , all robots
 satisfy  for some element .
Specifically, the \emph{point-formation, or rendezvous task} requires all
connected robots to be at the same position, the \emph{line-formation task}
requires all connected robots to be on the same line, and the
\emph{circle-formation task} requires all connected robots to be on the
same circle.

We are interested in distributed algorithms that achieve such formation
tasks optimally with respect to a suitable cost function. For the
point-formation and line-formation tasks, we aim to minimize completion
time, i.e., the time required by all robots to reach a common shape.  For
the circle-formation task, we aim to minimize the product between the time
required to reach a common circle, and the diameter of the common circle.



\begin{remark}[Circle formation] For the circle-formation problem we do not
  select the completion time as cost function, because of the following
  reasons. The centralized version of the minimum time circle-formation
  problem is equivalent to finding the minimum-width annulus containing the
  point-set. For arbitrary data sets, the minimum-width annulus has
  arbitrarily large minimum radius and bears similarities with the solution
  to the smallest stripe problem.  For some configurations, all points are
  contained only in a small fraction of the minimum-width annulus; this is
  not the solution we envision when we consider moving robots in a circle
  formation.  Therefore, we consider, instead, the smallest-area
  annulus. This cost function penalizes both the difference of the radiuses
  of the annulus (width of the annulus) and their sum.  \oprocend
\end{remark}


The key property of the \emph{minimum-time point-formation task},
\emph{minimum-time line-formation task}, and \emph{optimum circle-formation
  task} is that their centralized versions are equivalent to finding the
smallest ball, stripe and annulus, respectively, enclosing the  agents'
initial positions. We state these equivalences in the following lemma
without proof.

\begin{lemma}[Optimal shapes from geometric optimization]
  Given a set of distinct points ,
  consider the three optimization problems:
  
  where  denotes the radius of the circle .  These three
  optimization problems are equivalent to the smallest enclosing ball, the
  smallest enclosing stripe (for points in stripe-generic position), and
  the smallest enclosing annulus problem, respectively.

  Therefore, they are abstract optimization problems with combinatorial
  dimension ,  and , respectively.
\end{lemma}

We conclude this section with some useful notation.  We let
 denote the point, the line or the
circle equidistant from the boundary of the smallest enclosing ball, stripe
or annulus, respectively.


\subsection{Connectivity assumption, objective and strategy}
We assume that the robotic network is connected at initial time, i.e., that
the graph  is connected, and we aim to achieve
the formation task while guaranteeing that the state-dependent
communication graph remains connected during the evolution.  The following
connectivity maintenance strategy was originally proposed
in~\cite{HA-YO-IS-MY:99} and a comprehensive discussion is
in~\cite{FB-JC-SM:09}.  The key idea is to restrict the allowable motion of
each robot so as to preserve the existing edges in the communication graph.
We present this idea in three steps.

First, in a network with communication edges , if agents
 and  are neighbors at time , then we require that
their positions at time  belong to

If all neighbors of agent  at time  are at locations
, then the (convex) \emph{constraint
  set} of agent  is


Second, given  and  in  and a convex closed set  with , we introduce the \emph{from-to-inside} function,
denoted by  and illustrated in Figure~\ref{fig:fti}, that computes
the point in the closed segment  which is at the same time closest
to  and inside . Formally,


\begin{figure}[h]
\begin{center}
\includegraphics[width=.28\linewidth]{figs/fti}\includegraphics[width=.28\linewidth]{figs/fti-2}\caption{Illustration of the  function; courtesy of the authors
    of~\cite{FB-JC-SM:09}.}
  \label{fig:fti}
\end{center}
\end{figure}

Third and final, we assume that, independent of whatever control algorithm
dictates the robots evolution, if  denote
the desired target positions of agent  at time , then we allow
robot  to move towards that location only so far as the constraint set
allows. This is encoded by:



\subsection{Move-to-consensus-shape strategy}
The minimum-time point-formation, minimum-time line-formation, and optimum
circle formation tasks appear intractable in their general form due to the
state-dependent communication constraints.  To attack these problem, we
search for an efficient strategy that converges to the optimal one when the
upper bound on the robot speed  goes to zero or, in other words,
when information transmission tends to be infinitely faster than motion.
To design such a strategy, we aim to reach consensus on the centralized
solution to the problem, i.e., the optimal shape, and use the solution as a
reference for the agents motion.

A simple sequential solution is as follows: first, the agents compute the
optimal shape via constraints consensus, and then, when consensus is
achieved, they move toward the closest point in the target shape (point,
line or circle).  An improved strategy allows concurrent execution of
constraints consensus and motion: while the constraints consensus algorithm
is running, each agent moves toward the estimated target position while
maintaining connectivity of the communication graph. We first provide an
informal description.

\begin{quote}
  \emph{Move-to-consensus-shape strategy}: The processor state at each
  robot  consists of a set  of  candidate optimal
  constraints and a binary variable .  The set
   is initialized to  and
   is initialized to . At each communication round,
  the processor performs the following tasks: (i) it transmits
   and  to its neighbors and acquires its
  neighbors' candidate constraints and their current position; (ii) it runs
  an instance of the constraints consensus algorithm for the geometric
  optimization program of interest (smallest enclosing ball, stripe or
  annulus); if the constraints consensus halting condition is satisfied, it
  sets  to ; (iii) it computes a robot target position
  based on the current estimate of the optimal shape; (iv) it moves the
  robot towards the target position while respect input constraint and, if
   is still zero, enforcing connectivity with its current
  neighbors.
\end{quote}

Next we give a pseudo-code description.  We let
.

\bigskip\bigskip
\bigskip \hrule width \linewidth \smallskip

\noindent\begin{minipage}{0.44\linewidth}\textbf{\texttt{Problem data:}}\end{minipage}\begin{minipage}[t]{0.56\linewidth} A robotic network and a shape set\end{minipage}

\noindent\begin{minipage}{0.44\linewidth}\textbf{\texttt{Algorithm:}}\end{minipage}\begin{minipage}{0.56\linewidth}Move-to-Consensus-Shape\end{minipage}

\noindent\begin{minipage}{0.44\linewidth}\textbf{\texttt{Message
      alphabet:}}\end{minipage}\begin{minipage}{0.56\linewidth}\end{minipage}

\noindent\begin{minipage}{0.44\linewidth}\textbf{\texttt{Processor
      state:}}\end{minipage}\begin{minipage}[t]{0.56\linewidth}
   with \\
\end{minipage}

\noindent\begin{minipage}{0.44\linewidth}\textbf{\texttt{Physical
      state:}}\end{minipage}\begin{minipage}[t]{0.56\linewidth}
  
\end{minipage}

\smallskip
\noindent\begin{minipage}{0.44\linewidth}\textbf{\texttt{Initialization:}}\end{minipage}\begin{minipage}[t]{0.56\linewidth}
  \\
  
\end{minipage}

\bigskip

\noindent\textbf{\texttt{function}}   \begin{algorithmic}[1]
  \STATE \textbf{return} 
\end{algorithmic}

\medskip

\noindent\textbf{\texttt{function}}   \\
\emph{\% executed by node~, with }\-1.9em]
\begin{algorithmic}[1]
  \STATE \\
  \STATE 
  \STATE \textbf{if} , \textbf{then}\\
  \\
  \textbf{else}  \quad \textbf{end if}
  \STATE \textbf{return} 
\end{algorithmic}

\smallskip \hrule width \linewidth \medskip

We refer the interested reader to~\cite{GN-FB:06d} for numerical simulation
results for the move-to-consensus-shape strategy.  Finally, we state the
correctness of the algorithm and omit the proof in the interest of brevity.

\begin{proposition}
  \textbf{\textup{(Properties of the move-to-consensus-shape algorithm)}}
  On a robotic network with communication graph  and
  bounded control inputs , the move-to-consensus-shape strategy
  achieves the desired formation control tasks.

  In the limit as , the move-to-consensus-shape strategy
  solves the optimal formation control tasks, i.e., the minimum-time
  point-formation, minimum-time line-formation, and optimum circle
  formation tasks.
\end{proposition}















\section{Conclusions}

We have introduced a novel class of distributed optimization problems and
proposed a simple and intuitive algorithmic approach. Additionally, we have
rigorously established the correctness of the proposed algorithms and
presented a thorough Monte Carlo analysis to substantiate our conjecture
that, for a broad variety of settings, the time complexity of our
algorithms is linear. Finally, we have discussed in detail two modern
applications: target localization in sensor networks and formation control
in robotic networks.

Promising avenues for further research include proving the
linear-time-complexity conjecture, as well as applying our algorithms to
(i) optimization problems for randomly switching graphs and gossip
communication, (ii) distributed machine learning
problems~\cite{JB-YD-JT-OW:08,YL-VR-LV:08,KF-BBL-PT:06}, and (iii) convex
quadratic programming~\cite{RG:99}. Additionally, it is of interest to
verify the performance of our proposed target localization and formation
control algorithms in experimental setups.

\appendix

\subsection*{Abstract framework for the smallest enclosing stripe problem}
In this appendix we consider the smallest enclosing stripe problem in
Example~\ref{rem:examples} for stripe-generic points and show that is
satisfies the abstract optimization axioms. We begin with some notation.
Let  be the set of constraints, i.e., the set of points in the plane for
which we aim to compute the smallest enclosing stripe. With a slight abuse
of notation,  denotes both a constraint and a point depending on
the context.  Let , , be the width of the smallest
stripe enclosing the points in . A constraint  is violated by the set
, i.e., , if the point  does not belong
to the smallest stripe enclosing .

First, note that three points are necessary, but in general not sufficient,
to uniquely identify the smallest enclosing stripe of a set  of three or
more points. Indeed, the boundary of the smallest stripe enclosing 
contains at least three points of , e.g., point  on one line and
points  on the other line. So, the smallest enclosing stripe
identifies the triplet . However, a simple geometric
argument shows that the converse is not true, i.e., three points are not
sufficient. Three points would be sufficient if the triplet was an ordered
set and uniquely identified a stripe.
To uniquely identify the smallest enclosing stripe for a set  of five or
more points, one needs to add to the triplet  another two
points of  that belong to the correctly stripe among the three stripes
determined by .  In summary, this discussion shows that
the combinatorial dimension is .


Second, we prove that the two axioms of abstract optimization are
satisfied.  As usual, monotonicity is trivially satisfied, thus we need to
prove locality.
Suppose, by contradiction, that locality does not hold. Therefore there
exist ,  and ,  and , with
 such that  and
.
Let  and  be the smallest stripes for  and , so that
 and
 are the ordered triplets of
points defining  and , respectively.
Since  is violated by  but not by , this means that  belongs to
 but not to . Therefore, the stripes  and  must be
different, i.e., .
Because the points are in stripe-generic positions, we know that
 and, therefore,
. This proves the contradiction.



\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}\typeout{** loaded for the language `#1'. Using the pattern for}\typeout{** the default language instead.}\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{GN-FB:06d}
G.~Notarstefano and F.~Bullo, ``Distributed consensus on enclosing shapes and
  minimum time rendezvous,'' in \emph{{IEEE} Conf. on Decision and Control},
  San Diego, CA, Dec. 2006, pp. 4295--4300.

\bibitem{GN-FB:06z}
------, ``Network abstract linear programming with application to minimum-time
  formation control,'' in \emph{{IEEE} Conf. on Decision and Control}, New
  Orleans, LA, Dec. 2007, pp. 927--932.

\bibitem{GN-FB:08x}
------, ``Network abstract linear programming with application to cooperative
  target localization,'' in \emph{Modelling, Estimation and Control of
  Networked Complex Systems}, ser. Understanding Complex Systems, A.~Chiuso,
  L.~Fortuna, M.~Frasca, L.~Schenato, and S.~Zampieri, Eds.\hskip 1em plus
  0.5em minus 0.4em\relax Springer, 2009, pp. 177--190.

\bibitem{JNT-DPB-MA:86}
J.~N. Tsitsiklis, D.~P. Bertsekas, and M.~Athans, ``Distributed asynchronous
  deterministic and stochastic gradient optimization algorithms,'' \emph{IEEE
  Transactions on Automatic Control}, vol.~31, no.~9, pp. 803--812, 1986.

\bibitem{NM-AJ:09}
N.~Motee and A.~Jadbabaie, ``Distributed multi-parametric quadratic
  programming,'' \emph{IEEE Transactions on Automatic Control}, vol.~54,
  no.~10, 2009.

\bibitem{AN-AO-PAP:09}
A.~Nedic, A.~Ozdaglar, and P.~A. Parrilo, ``Constrained consensus and
  optimization in multi-agent networks,'' \emph{IEEE Transactions on Automatic
  Control}, 2009, to appear.

\bibitem{BJ-AS-MK-KHJ:08}
B.~Johansson, A.~Speranzon, M.~Johansson, and K.~H. Johansson, ``On
  decentralized negotiation of optimal consensus,'' \emph{Automatica}, vol.~44,
  no.~4, pp. 1175--1179, 2008.

\bibitem{DPP-MC:07}
D.~P. Palomar and M.~Chiang, ``Alternative distributed algorithms for network
  utility maximization: Framework and applications,'' \emph{IEEE Transactions
  on Automatic Control}, vol.~52, no.~12, pp. 2254--2269, 2007.

\bibitem{JM-MS-EW:96}
J.~Matousek, M.~Sharir, and E.~Welzl, ``A subexponential bound for linear
  programming,'' \emph{Algorithmica}, vol.~16, no. 4/5, pp. 498--516, 1996.

\bibitem{BG:95}
B.~G{\"a}rtner, ``A subexponential algorithm for abstract optimization
  problems,'' \emph{SIAM Journal on Computing}, vol.~24, no.~5, pp. 1018--1035,
  1995.

\bibitem{MG:95}
M.~Goldwasser, ``A survey of linear programming in randomized subexponential
  time,'' \emph{SIGACT News}, vol.~26, no.~2, pp. 96--104, 1995.

\bibitem{NM:84}
N.~Megiddo, ``Linear programming in linear time when the dimension is fixed,''
  \emph{Journal of the Association for Computing Machinery}, vol.~31, no.~1,
  pp. 114--127, 1984.

\bibitem{BG-EW:96}
B.~G{\"a}rtner and E.~Welzl, ``Linear programming - randomization and abstract
  frameworks,'' in \emph{Symposium on Theoretical Aspects of Computer Science},
  ser. Lecture Notes in Computer Science, vol. 1046, 1996, pp. 669--687.

\bibitem{BG-EW:01}
------, ``A simple sampling lemma: {A}nalysis and applications in geometric
  optimization,'' \emph{Discrete \& Computational Geometry}, vol.~25, no.~4,
  pp. 569--590, 2001.

\bibitem{PKA-SS:01}
P.~K. Agarwal and S.~Sen, ``Randomized algorithms for geometric optimization
  problems,'' in \emph{Handbook of Randomization}, P.~Pardalos, S.~Rajasekaran,
  J.~Reif, and J.~Rolim, Eds.\hskip 1em plus 0.5em minus 0.4em\relax Kluwer
  Academic Publishers, 2001.

\bibitem{PKA-MS:98}
P.~K. Agarwal and M.~Sharir, ``Efficient algorithms for geometric
  optimization,'' \emph{ACM Computing Surveys}, vol.~30, no.~4, pp. 412--458,
  1998.

\bibitem{MA-NM:96}
M.~Ajtai and N.~Megiddo, ``A deterministic -time -processor algorithm for linear programming in fixed dimension,''
  \emph{SIAM Journal on Computing}, vol.~25, no.~6, pp. 1171--1195, 1996.

\bibitem{YB-JWB-DR:04}
Y.~Bartal, J.~W. Byers, and D.~Raz, ``Fast, distributed approximation
  algorithms for positive linear programming with applications to flow
  control,'' \emph{SIAM Journal on Computing}, vol.~33, no.~6, pp. 1261--1279,
  2004.

\bibitem{HD-HK:08}
H.~Dutta and H.~Kargupta, ``Distributed linear programming and resource
  management for data mining in distributed environments,'' in \emph{IEEE Int.
  Conference on Data Mining}, Pisa, Italy, Dec. 2008, pp. 543--552.

\bibitem{YL-VR:06}
Y.~Lu and V.~Roychowdhury, ``Parallel randomized support vector machine,'' in
  \emph{Advances in Knowledge Discovery and Data Mining (10th Pacific-Asia
  Conference, Singapore 2006)}, ser. Lecture Notes in Artificial Intelligence,
  W.~K. Ng, M.~Kitsuregawa, and J.~Li, Eds.\hskip 1em plus 0.5em minus
  0.4em\relax Springer, 2006, pp. 205--214.

\bibitem{YL-VR-LV:08}
Y.~Lu, V.~Roychowdhury, and L.~Vandenberghe, ``Distributed parallel support
  vector machines in strongly connected networks,'' \emph{IEEE Transactions on
  Neural Networks}, vol.~19, no.~7, pp. 1167--1178, 2008.

\bibitem{JB-YD-JT-OW:08}
J.~Balc{\'a}zar, Y.~Dai, J.~Tanaka, and O.~Watanabe, ``Provably fast training
  algorithms for support vector machines,'' \emph{Theory of Computing Systems},
  vol.~42, no.~4, pp. 568--595, 2008.

\bibitem{KF-BBL-PT:06}
K.~Flouri, B.~Beferull-Lozano, and P.~Tsakalides, ``Training a {SVM}-based
  classifier in distributed sensor networks,'' in \emph{European Signal
  Processing Conference}, Florence, Italy, Sep. 2006.

\bibitem{FZ-LG:04}
F.~Zhao and L.~Guibas, \emph{Wireless Sensor Networks: {A}n Information
  Processing Approach}.\hskip 1em plus 0.5em minus 0.4em\relax Morgan-Kaufmann,
  2004.

\bibitem{AG-AV:01}
A.~Garulli and A.~Vicino, ``Set membership localization of mobile robots via
  angle measurements,'' \emph{IEEE Transactions on Robotics and Automation},
  vol.~17, no.~4, pp. 450--463, 2001.

\bibitem{VI-RB:06}
V.~Isler and R.~Bajcsy, ``The sensor selection problem for bounded uncertainty
  sensing models,'' \emph{IEEE Transactions on Automation Sciences and
  Engineering}, vol.~3, no.~4, pp. 372--381, 2006.

\bibitem{IS-MY:99}
I.~Suzuki and M.~Yamashita, ``Distributed anonymous mobile robots: Formation of
  geometric patterns,'' \emph{SIAM Journal on Computing}, vol.~28, no.~4, pp.
  1347--1363, 1999.

\bibitem{HA-YO-IS-MY:99}
H.~Ando, Y.~Oasa, I.~Suzuki, and M.~Yamashita, ``Distributed memoryless point
  convergence algorithm for mobile robots with limited visibility,'' \emph{IEEE
  Transactions on Robotics and Automation}, vol.~15, no.~5, pp. 818--828, 1999.

\bibitem{XD-AK:02}
X.~Defago and A.~Konagaya, ``Circle formation for oblivious anonymous mobile
  robots with no common sense of orientation,'' in \emph{ACM Int. Workshop on
  Principles of Mobile Computing}, Toulouse, France, Oct. 2002, pp. 97--104.

\bibitem{ME-XH:01b}
M.~Egerstedt and X.~Hu, ``Formation constrained multi-agent control,''
  \emph{IEEE Transactions on Robotics and Automation}, vol.~17, no.~6, pp.
  947--951, 2001.

\bibitem{HGT-GJP-VK:04}
H.~G. Tanner, G.~J. Pappas, and V.~Kumar, ``Leader-to-formation stability,''
  \emph{IEEE Transactions on Robotics and Automation}, vol.~20, no.~3, pp.
  443--455, 2004.

\bibitem{JAM-MEB-BAF:04c}
J.~A. Marshall, M.~E. Broucke, and B.~A. Francis, ``Formations of vehicles in
  cyclic pursuit,'' \emph{IEEE Transactions on Automatic Control}, vol.~49,
  no.~11, pp. 1963--1974, 2004.

\bibitem{RS:87}
R.~Shamir, ``The efficiency of the simplex method: {A} survey,''
  \emph{Management Science}, vol.~33, no.~3, pp. 301--334, 1987.

\bibitem{NAL:97}
N.~A. Lynch, \emph{Distributed Algorithms}.\hskip 1em plus 0.5em minus
  0.4em\relax Morgan Kaufmann, 1997.

\bibitem{JMH:08}
J.~M. Hendrickx, ``Graphs and networks for the analysis of autonomous agent
  systems,'' Ph.D. dissertation, Universit{\'e} Catholique de Louvain, Belgium,
  Feb. 2008.

\bibitem{RA-ALB:02}
R.~Albert and A.-L. Barab{\'a}si, ``Statistical mechanics of complex
  networks,'' \emph{Reviews of Modern Physics}, vol.~74, no.~1, pp. 47--97,
  2002.

\bibitem{JRD-DGK-JWT:77}
J.~R. Dunham, D.~G. Kelly, and J.~W. Tolle, ``Some experimental results
  concerning the expected number of pivots for solving randomly generated
  linear programs,'' Operations Research and System Analysis Department,
  University of North Carolina at Chapel Hill, Tech. Rep. 77-16, 1977.

\bibitem{KHB:77}
K.~H. Borgwardt, ``{U}ntersuchungenzur {A}symptotik der mittleren {S}chrittzahl
  von {S}implexverfahren in der {L}inearen {O}ptimierung,'' \emph{Operations
  Research-Verfahren}, vol.~28, pp. 332--345, 1977.

\bibitem{MJT:91}
M.~J. Todd, ``Probabilistic models for linear programming,'' \emph{Mathematics
  of Operations Research}, vol.~16, no.~4, pp. 671--693, 1991.

\bibitem{RT-GC-FD:05}
R.~Tempo, G.~Calafiore, and F.~Dabbene, \emph{Randomized Algorithms for
  Analysis and Control of Uncertain Systems}.\hskip 1em plus 0.5em minus
  0.4em\relax Springer, 2005.

\bibitem{FB-JC-SM:09}
F.~Bullo, J.~Cort{\'e}s, and S.~Mart{\'\i}nez, \emph{Distributed Control of
  Robotic Networks}, ser. Applied Mathematics Series.\hskip 1em plus 0.5em
  minus 0.4em\relax Princeton University Press, 2009, available at
  http://www.coordinationbook.info.

\bibitem{RG:99}
R.~Goldbach, ``Some randomized algorithms for convex quadratic programming,''
  \emph{Applied Mathematics \& Optimization}, vol.~39, no.~1, pp. 121--142,
  1999.

\end{thebibliography}



\end{document}
