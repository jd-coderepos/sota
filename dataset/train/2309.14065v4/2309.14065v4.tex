

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  



\IEEEoverridecommandlockouts                              

\overrideIEEEmargins                                      \usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}
\graphicspath{{./Figs/}}
\usepackage[inkscapelatex=false]{svg}
\usepackage{multirow}
\usepackage{soul}
\usepackage{bbding}
\usepackage{lineno}
\usepackage{url}
\usepackage{hyperref}
\usepackage{xcolor}

\title{\LARGE \bf
 AsymFormer: Asymmetrical Cross-Modal Representation Learning for Mobile Platform Real-Time RGB-D Semantic Segmentation
}

 \author{Siqi Du\textsuperscript{},  Weixi Wang\textsuperscript{}, \emph{Member, IEEE}~, Renzhong Guo and Shengjun Tang\textsuperscript{*}, \emph{Member, IEEE}~\thanks{\textsuperscript{} indicates equal contribution.}\thanks{\textsuperscript{*} corresponding author.}\thanks{This work was supported by the Research Program of Shenzhen S and T Innovation Committee grant( (Grant No. JCYJ20210324093012033) and the Natural Science Foundation of Guangdong Province grant (Grant No. 2121A1515012574) and the National Natural Science Foundation of China (Project Nos. 71901147, 41971354, and 41971341), Shenzhen Key Laboratory of Digital Twin Technologies for Cities (Project No: ZDSYS20210623101800001)}\thanks{Siqi Du, Weixi Wan, Renzhong Guo, and Shengjun Tang is with Research Institute for Smart Cities, School of Architecture and Urban Planning, Shenzhen University}}




\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


\begin{abstract}







In the realm of robotic intelligence, achieving efficient and precise RGB-D semantic segmentation is a key cornerstone. State-of-the-art multimodal semantic segmentation methods, primarily rooted in symmetrical skeleton networks, find it challenging to harmonize computational efficiency and precision. In this work, we propose AsymFormer, a novel network for real-time RGB-D semantic segmentation, which targets the minimization of superfluous parameters by optimizing the distribution of computational resources and introduces an asymmetrical backbone to allow for the effective fusion of multimodal features. Furthermore, we explore techniques to bolster network accuracy by redefining feature selection and extracting multi-modal self-similarity features without a substantial increase in the parameter count, thereby ensuring real-time execution on robotic platforms. Additionally, a Local Attention-Guided Feature Selection (LAFS) module is used to selectively fuse features from different modalities by leveraging their dependencies. Subsequently, a Cross-Modal Attention-Guided Feature Correlation Embedding (CMA) module is introduced to further extract cross-modal representations. This method is evaluated on NYUv2 and SUNRGBD datasets, with AsymFormer demonstrating competitive results with 52.0\% mIoU on NYUv2 and 49.1\% mIoU on SUNRGBD. Notably, AsymFormer achieves an inference speed of 65 FPS and after implementing mixed precision quantization, it attains an impressive inference speed of 79 FPS on RTX3090. This significantly outperforms existing multi-modal methods, thereby demonstrating that AsymFormer can strike a balance between high accuracy and efficiency for RGB-D semantic segmentation. \href{https://github.com/Fourier7754/AsymFormer}{Code: \textcolor[RGB]{78,101,155}{https://github.com/Fourier7754/AsymFormer}}

\end{abstract}


\section{INTRODUCTION}
Real-time semantic segmentation methods are important for robots as they rely on obtaining semantic information in real-time to support navigation and task decision-making. Current Real-Time semantic segmentation networks achieve competitive results and reach over 100 FPS inference speed on simple outdoor environments (Cityscapes 19 classes) \cite{zhao2017pyramid,chen2017rethinking,yu2018bisenet,yu2021bisenet}. However, these networks perform poorly on complex indoor environments, such as NYU Depth v2 (40 classes) \cite{silberman2012indoor} and SUNRGBD (37 classes) \cite{song2015sun}. Several studies have explored how to improve indoor scene semantic segmentation performance by integrating RGB-D information \cite{cao2021shapeconv,liu2022cmx,seichter2021efficient,zhou2022frnet}, which is widely available from RGB-D sensors on indoor robots. RGB-D information consists of RGB (color, texture and shape) and Depth (boundaries and relative location) features, which are somewhat complementary \cite{cao2021shapeconv,liu2022cmx}. 



At present, RGB-D semantic segmentation methods have achieved state-of-the-art performance by employing dual-branch backbone for feature extraction and attention mechanisms for feature selection \cite{liu2022cmx}. However, there is a dearth of discourse on whether these operations can be executed with less redundancy and higher efficiency. Primarily, most methodologies utilize two symmetrical, computationally intensive backbones, which effectively doubles the overall computational complexity \cite{du2022pscnet}. Secondly, numerous methods concentrate on designing complex feature fusion modules to enhance accuracy, but do not exhibit significant advantages than some methods that employ simpler attention modules like ESANet \cite{seichter2021efficient}, which indicates that as more computational resources are invested, the incremental improvement in accuracy decreases. Lastly, the use of cross-attention modules to jointly utilize multi-modal features has been demonstrated to be effective in improving network accuracy \cite{liu2022cmx,zhou2022canet}. However, most methods do not discuss how to implement cross-attention more efficiently. Therefore, the reduction of redundancy in dual-branch networks and the construction of more efficient attention modules remain significant topics for discussion.


To address this issue, this study suggests a balanced approach between computational efficiency and feature richness in multi-modal representation learning. RGB images are found to contain more information than Depth images, thus it is not necessary to use the same size backbone for extracting Depth information. The performance improvements achieved by using attention mechanisms for various tasks come with minimal increase in computational cost. While existing modules like SE \cite{hu2018squeeze} are efficient for feature selection, they lack the ability to model correlations in spatial dimensions. Increasing parameters can facilitate spatial dimension feature selection, but careful design is needed to maintain computational efficiency. For feature extraction, the commonly used Multi-Head Self Attention (MHSA) \cite{dosovitskiy2020image} is effective for single-modal features despite its small parameter count. Extending this efficient feature extraction to multiple modalities is feasible, but consideration should be given to feature fusion while maintaining computational efficiency.



This paper introduces AsymFormer, a high-performance real-time network for RGB-D integration semantic segmentation that employs an asymmetrical backbone design. This includes a larger parameter backbone for RGB features and a smaller backbone for the Depth branch. Regarding framework selection, Vision Transformer are always performance better, but slower than same computational complexity CNNs due to a lack of hardware optimization \cite{li2022next}. In order to speed up the main branch, this paper employ a hardware-friendly CNN \cite{liu2022convnet} for the RGB branch and a Transformer \cite{xie2021segformer} with fewer parameters but similar performance for the Depth branch to further compress the parameters. This paper also discuss feature selection modeling and construct the Local Attention-Guided Feature Selection (LASF) module. This module computes the spatial-channel attention weights in parallel to enhance inference speed. Moreover, this paper establishes a novel approach to model spatial attention. This module can estimate the differences in features from different modalities by utilizing learnable channel weights and calculating spatial attention weights for each pixel. Additionally, a Cross-Modal Attention (CMA) module is introduced to embed cross-modal integration learning information into pixel-wise fused features. Finally, we employ a lightweight MLP-Decoder\cite{xie2021segformer} to decode semantic information from shallow features.



This paper evaluates AsymFormer on two classic indoor scene semantic segmentation datasets: NYUv2 and SUNRGBD. Meanwhile, the inference speed test is also preformed on Nvidia RTX 3090 platform. The AsymFormer achieves 52.0\% mIoU on NYUv2 and 49.1 mIoU on SUNRGBD, with 65 FPS inference speed. After performing mixed precision quantization on the model using the TensorRT framework, AsymFormer achieves an impressive inference speed of 79 FPS on RTX 3090, which significantly surpasses existing methods. Our experiments highlight AsymFormer's ability to acquire high accuracy and efficiency at the same time. Our main contributions are summarized as follows: \begin{itemize} \item[] We employed an asymmetric backbone that compressed the parameters of the Depth feature extraction branch, thus reducing redundancy.

\item[] We introduced the LAFS module for feature selection, utilizing learnable feature weights to calculate spatial attention weights.

\item[] We introduce a novel efficient cross-modal attention (CMA) for modeling of self-similarity in multi-modal features, validating its capability to enhance network accuracy with minimal additional model parameters.

\end{itemize}
 
\section{Related Works}
\textbf{RGB-D Representation Learning.} One of the earliest works on RGB-D semantic segmentation, FCN \cite{long2015fully}, treated RGB-D information as a single input and processed it with a single backbone. However, subsequent works have recognized the need to extract features from RGB and Depth information separately, as they have different properties. Therefore, most of them have adopted two symmetric backbones for RGB and Depth feature extraction \cite{hazirbas2017fusenet,jiang2018rednet,seichter2021efficient,seichter2022efficient,chen2020bi,liu2022cmx}. Further, the ACNet \cite{hu2019acnet} proposed a three-stream backbone to process RGB, Depth and Fusion features independently. Moreover, some works have considered the variability of RGB-D features and used asymmetric backbones for their feature extraction. For example, TSNet \cite{zhou2020tsnet} used ResNet \cite{he2016deep} for RGB feature extraction and VGG (without residual connections) \cite{simonyan2014very} for Depth feature extraction. The PSCNet \cite{du2022pscnet} also used an asymmetric backbone and reduced the redundant computational cost by cutting the parameters of the Depth branch.


\textbf{RGB-D feature fusion.} The design of RGB-D backbones for feature extraction has been mostly based on two or three stream architectures \cite{liu2022cmx}. However, the performance of different frameworks depends largely on how they fuse RGB and Depth features. Some early works, such as FuseNet\cite{hazirbas2017fusenet} and RedNet \cite{jiang2018rednet}, fused RGB and Depth feature maps pixel-wise in the backbone. Later works, such as ACNet \cite{hu2019acnet}, ESANet \cite{seichter2021efficient} and EMSANet\cite{seichter2022efficient}, proposed channel attention to select features from different channels, as RGB and Depth feature maps may not align well on the corresponding channels. PSCNet \cite{du2022pscnet} further extended channel attention to both spatial and channel directions and achieved better performance. Recently, more complex models have been proposed to exploit cross-modal information and select features for RGB-D fusion. For example, SAGate \cite{chen2020bi} proposed a gated attention mechanism that can leverage cross-modal information for feature selection. CANet \cite{zhou2022canet} extended non-local attention \cite{wang2018non} to cross-modal semantic information and achieved significant improvement. CMX \cite{liu2022cmx} extended SA-Gate to spatial and channel directions and proposed a novel cross-modal attention with global receptive field, which can enhance cross-modal complementarity using the similarity of two modalities. However, integrating cross-modal information and learning cross-modal similarity is still an open question in vision tasks.



\section{Method}

\begin{figure*}[htbp]
\vspace {+0.6em}
\centering
\includegraphics[width=0.85\textwidth]{overview_0915.pdf}
\caption{\label{fig:overview framework}Overview of AsymFormer.}
\vspace {-1.1em}
\end{figure*}


\subsection{Framework Overview}
This paper proposes AsymFormer, a novel network for RGB-D integration semantic segmentation. The network employs a ConvNext \cite{liu2022convnet} based backbone to extract features from RGB images and a Mix-Transformer \cite{xie2021segformer} based backbone to process RGB-D fused features. We introduced the LAFS module for feature selection, utilizing learnable feature weights to calculate spatial attention weights. We integrated modeling of self-similarity in multi-modal features, validating its capability to enhance network accuracy with minimal additional model parameters. The overall framework of AsymFormer is illustrated in Figure \ref{fig:overview framework}.


\subsection{Local Attention-Guided Feature Selection}
Attention mechanisms have been shown to be effective in selecting complementary features from RGB and Depth features, thus improving efficiency and performance\cite{hu2019acnet,yu2021bisenet,du2022pscnet,liu2022cmx}. Current spatial attention methods extract global information using pixel maximum or average values, neglecting the differences in channels from multiply modalities. In this work, we extend the previous spatial attention by proposing a more flexible spatial attention mechanism. The core design of LAFS is a trainable spatial attention weight. This weight is learned through an MLP in a similar manner to SE \cite{hu2018squeeze}.

In specific, Figure \ref{fig:overview framework} (b)-1 illustrates the details of LAFS. The LAFS receives input features of the concatenation of RGB feature  with dimension  and current layer Depth feature  with dimension .


For the computation of the attention weight, we first extract global information vectors  of each feature map by adaptive AvgPool. For the channel attention, we process the global information by a feed-forward network with a squeeze-and-excitation structure.

For the spatial attention, we process the global information  by another feed-forward network with a squeeze-and-excitation structure. The output vector is split into  and , which represent two different descriptions of pixel spatial-similarity.

Furthermore, the global spatial information  of feature map can be extracted by calculating the inner product similarity from  and the  (In computation, this is equivalent to the weighted sum of the features from each channel). Then, the spatial attention weight  is computed by sigmoid normalization.

Where, the results all divide  for preventing sigmoid overflow. Finally, the input features are selected by weight in spatial and channel  direction:




\subsection{Cross Modal Attention-Guided Feature Correlation Embedding}
In this paper, we introduce a novel cross-modal attention (CMA) module that incorporates cross-modal self-similarity information into pixel-wise fused features. Current MHSA \cite{vaswani2017attention} is limited to learning the self-similarity of a single modality, whereas in multi-modal processing, our goal is to jointly use information from multiple modalities for representation learning. To achieve this, CMA is proposed to learn the self-similarity of multiple modalities. The key insight of CMA is independently learning the self-similarity of two modalities and embeds the sum of the results into the fusion feature, which is different than standard MHSA in theoretical. Specifically, for each pixel  in feature map, we separately embed its RGB and Depth feature:

The cross-modal similarity of pixel  and other pixel in feature map are defined as:

Moreover, the CMA embeds cross-modal similarity into the fused feature  with same operation as\cite{vaswani2017attention,wang2018non,dosovitskiy2020image}. Following above idea, the CMA first embedding ,  and  independently for three modalities features. Then, we extend self-attention to the multimodal case, which contain two design: 1. Information mixing and redistribution of  and ; 2. Learning two different different representation subspaces for the same position. The overall framework of CMA is demonstrated in Figure 

\textbf{Independent linear embedding:} According to our definition, the ,  and  should be embedded independently. Assuming the input  feature has dimension ; the Enhanced Depth feature  has dimension ; and the previous fused feature  has dimension . We first normalize the RGB feature and Depth feature. Then, we use six independent convolution layers to generate self-attention keys and queries from RGB and Depth feature:
 
 
 
The dimensions of  and  are , where  is the down-sample rate and equals the convolution kernel-size and stride. This reduces the feature map resolution to . The dimensions of  and  are . The  and  is reshaped into dimension .

\textbf{Information mixing and redistribution:} We concatenate the cross-modal keys and queries to obtain the  and :
 
The first  channels of  and  contain RGB information. Similarly, the second half of the  and  channels contain Depth information. To obtain the ,  and ,  for integrating RGB-D information computing the two feature subspaces , , we perform a channel shuffle of  and  and split them into required vector:
 




\textbf{Representation subspaces learning:} The ,  are reshaped into dimension  and the ,  are reshaped into dimension . Then, the two representation subspaces  and  are computed by dot product of  and  respectively:
 
The  and  are embedded into  and  by dot product. The fused feature  is the concatenation of  and : 

Finally, the  is adapted into output channel  and added with residual connection .

\begin{table*}[ht]
\vspace {+0.6em}
\centering
\caption{\label{tab:ablation experiment}Ablation experiment results for different multi-modal feature fusion method.}
\begin{tabular}{c|cccc|ccc}
\hline \hline
\multirow{2}{*}{\textbf{Model}} & \multicolumn{4}{c|}{\textbf{Feature Fusion Method}} & \multicolumn{3}{c}{\textbf{Metric}} \\ \cline{2-8} 
 & \textbf{Cat} & \multicolumn{1}{c|}{\textbf{SE+MHSA-H2}} & \textbf{LAFS} & \textbf{CMA-H2} & \textbf{Params/M} & \textbf{mIoU(\%)} & \textbf{Inf.Speed(FPS)} \\ \hline
\multirow{2}{*}{\textbf{Baseline}} & \checkmark & \multicolumn{1}{c|}{} &  &  & 31.9 & 47.0 & 77.5 \\
 &  & \multicolumn{1}{c|}{\checkmark} &  &  & 32.6(+0.7M) & 49.9 (+2.9\%) & 65.7 \\ \hline
\multirow{3}{*}{\textbf{Ours}} &  & \multicolumn{1}{c|}{} & \checkmark &  & 32.4 (+0.5M) & 49.1 (+2.1\%) & 75.7 \\
 &  & \multicolumn{1}{c|}{} &  & \checkmark & 32.5 (+0.6M) & 49.6 (+2.6\%) & 67.4 \\
 &  & \multicolumn{1}{c|}{} & \checkmark & \checkmark & 33.0 (+1.1M) & 52.0 (+5.0\%) & 65.5 \\ \hline \hline
\end{tabular}
\vspace {-1.1em}
\end{table*}

\section{EXPERIMENT RESULTS AND ANALYSIS}
\subsection{Implementation Details}
To evaluate our Real-Time semantic segmentation network design, we conduct a series of experiments on two widely-used datasets: NYUv2\cite{silberman2012indoor}  (795 training and 654 testing RGB-D images) and SUNRGBD\cite{song2015sun} (5825 training and 5050 testing RGB-D images). 

We conduct the model training and testing on different platforms. For the training, we use Nvidia A100-40G GPU. For the evaluation and inference speed testing, we use Nvidia RTX 3090 GPU, Ubuntu 20.04, CUDA 12.0 and Pytorch 2.0.1. We apply data augmentation to all datasets by randomly flipping (p=0.5), srandom scales [1.0,2.0], random crop 480640 and random HSV. We adopt Mix Transformer\cite{xie2021segformer} and ConvNext\cite{liu2022convnet} backbones pretrained on ImageNet-1k\cite{deng2009imagenet}. In the augmented setting, both backbones are pretrained on ADE20k\cite{zhou2017scene}. For the CMA implementation, we set down-sample rate =1 in CMA-1 and =2 in CMA-2,3. The MLP-decoder in AsymFormer has the same structure as Segformer and an embedding dimension of 256. We choose AdamW optimizer with a weight decay of 0.01. The initial learning rate is  and we use a poly learning rate schedule  with a warm-up of 10 epochs. We train with a batch size of 8 for NYUv2 (500 epochs) and SUNRGBD (200 epochs). We employ cross-entropy as the loss function and do not use any auxiliary loss during training process. The evaluation metric is mean Intersection over Union (mIoU).


\subsection{Model Assessment}
This paper proposes two different models based on the AsymFormer framework with different backbones. The AsymFormer uses ConvNext-T\cite{liu2022convnet} for RGB representation learning and MiT-B0\cite{xie2021segformer} for fused feature processing. This model has 33.0 million parameters and 36.0 GFLOPs computational cost, and it can achieve 65 FPS inference speed on RTX 3090. Particularly, with mixed precision quantization, AsymFormer achieves a real-time inference speed of 79 FPS on the RTX 3090 GPU. It also achieves a real-time inference speed of 29.4 FPS on the Tesla T4 GPU, which has a more limited computation resources (65 TFLOPs FP16).


\subsection{Ablation Experiment}
We conduct a series of ablation experiments on NYUv2 dataset to evaluate the effectiveness of the LAFS and CMA module. We set two common feature fusion methods as our comparative baseline: \textbf{1. Cat:} This method directly concatenates two features and then uses convolution layers to adjust the channel numbers. Essentially, it is a pixel-wise fusion without feature selection. \textbf{2. SE+MHSA-H2:} This method combines the popular SE attention \cite{hu2018squeeze} and MHSA attention \cite{vaswani2017attention} for feature fusion. Here, SE is used for feature selection in the channel direction, while MHSA is employed for further feature extraction on the fused features. Specifically, we use an MHSA with 2 heads to align with CMA-H2.

In our experiments, the Cat fusion method, used as a baseline, achieved a segmentation accuracy of 47.0 mIoU and an inference speed of 77.5 FPS. When using LAFS for feature selection alone, we achieved a performance improvement of 2.1\% while sacrificing only 1.8 FPS of inference speed. This demonstrates that LAFS is a highly efficient feature selection method that provides performance gains without significantly impacting inference speed. In comparison to the other baseline, Cat+MHSA-H2, which resulted in an 11.8 FPS reduction in inference speed, only a 2.9\% improvement in segmentation accuracy was achieved. This further highlights the efficiency of LAFS.

Furthermore, we conducted experiments to investigate whether incorporating the mining of self-similarity within the two modalities after feature selection could further improve segmentation accuracy. When using CMA-H2 alone, we observed a 2.6\% improvement in segmentation accuracy but encountered a significant decrease in inference speed of 10.1 FPS. Compared to LAFS, CMA showed a more noticeable reduction in inference speed. In comparison to SE+MHSA-H2, CMA-H2 achieved similar accuracy, but it is important to note that no feature selection was performed in this case, indicating that relying solely on mining multi-modal self-similarity can achieve accuracy similar to the traditional feature fusion baseline SE+MHSA.

Finally, we combined LAFS with CMA (LAFS+CMA). Since LAFS had minimal impact on inference speed and served a different purpose than CMA, the network's inference speed only decreased by 2 FPS while achieving a significant improvement of 5.0\% compared to the baseline Cat. At this point, the inference speed of LAFS+CMA was similar to SE+MHSA, but with a 2.1\% performance improvement. This validates our experimental hypothesis: by re-modeling feature selection and mining cross-modal self-similarity, we can enhance the segmentation performance of the network without sacrificing inference speed compared to existing models. This demonstrates that we have indeed improved the efficiency of the network.



\subsection{Comparison With State-of-The-Arts}
\subsubsection{NYUv2 Comparison Results}
Table \ref{tab:NYUv2} presents the results of our network on the NYUv2 dataset with different backbone sizes and pretraining settings. Even without ImageNet-1k pretraining, which other methods use for a fair comparison, our method still achieves leading scores. Our Real-Time efficient method, AsymFormer achieves 52.0  mIoU, demonstrating competitive accuracy compared to those high-performance heavy designs. AsymFormer also has faster inference speed than other methods. For instance, AsymFormer outperforms PSCNet-T\cite{du2022pscnet} by 6.6\% mIoU and about 30\% inference speed improvement. Similarly, AsymFormer is two times faster than ESANet\cite{seichter2021efficient} with the same parameters and performance. In particular, AsymFormer has the two times smaller parameters than CMX-B2\cite{liu2022cmx}, but has about three times faster than CMX-B2 with a little performance degradation. In terms of semantic segmentation accuracy, AsymFormer does not show a significant disadvantage compared to the methods included in the comparison. Additionally, it demonstrates a substantial lead over other methods in terms of inference speed. This validates the effectiveness of our various efforts in reducing network redundancy parameters and improving inference speed.
\begin{table*}[htbp]
\vspace {+0.7em}
\centering
\caption{\label{tab:NYUv2}Comparison Results on NYUv2. * denotes ADE20k pretrain. The inference speed is tested on RTX 3090 platform, (480  640) inputs.}
\begin{tabular}{c|ccc|cc|c|c}
\hline \hline
\textbf{Method} & \textbf{Year} & \textbf{Backbone} & \textbf{Params/M} & \textbf{Pixel Acc. (\%)} & \textbf{mIoU (\%)} & \textbf{Inf Speed/FPS} & \textbf{Inf Speed (\%)} \\ \hline
PSCNet-T & 2022 & Res50 & 40 & - & 45.4 & 47 & 72.3\% \\
PSCNet-L & 2022 & Res50 & 52 & - & 46.2 & 30 & 46.2\% \\
ACNet & 2019 & Res50 & 118 & - & 48.3 & 28 & 43.1\% \\
RDFNet & 2017 & Res152 & 450 & 76 & 49.1 & 15 & 23.1\% \\
SA-Gate & 2021 & Res50 & 65 & 76.8 & 50.2 & 35 & 53.8\% \\
ESANet & 2022 & Res34-Nbt1D & 34 & - & 51.6 & 32 & 49.2\% \\
MSFNet & 2022 & Res50 & 49 & - & 52.2 & 25 & 38.5\% \\
FRNet & 2022 & Res34 & 86 & 77.6 & 53.6 & 39 & 60.0\% \\
PGDENet & 2022 & Res34 & 101 & 78.1 & 53.7 & 32 & 49.2\% \\
CMX-B2 & 2022 & Segformer-B2 & 67 & 78.7 & 54.1 & 24 & 36.9\% \\ \hline
AsymFormer & 2023 & B0+T & \textbf{33} & 77.0 & 52.0 & \textbf{65} & \textbf{100.0\%} \\ \hline
AsymFormer (FP16) & 2023 & B0+T & \textbf{33} & 77.0 & 52.0 & \textbf{79} & \textbf{121.5\%} \\ \hline \hline
\end{tabular}
\vspace {-0.7em}
\end{table*}

\subsubsection{SUNRGBD Comparison Results}
Table \ref{tab:SUNRGBD} reports the performance of AsymFormer on the SUNRGBD dataset. AsymFormer achieves competitive accuracy with 49.1 \% mIoU. Since the input image size is the same as NYUv2, all methods have the same inference speed as shown in Table \ref{tab:NYUv2}. However, the advantage of AsymFormer is not as significant as in NYUv2 experiment. For example, AsymFormer improves 1.8 mIoU over SA-Gate\cite{chen2020bi} in NYUv2 dataset (52.0\% vs 50.2\% mIoU), but decreases 0.3 mIoU in SUNRGBD dataset (49.1\% vs 49.4\% mIoU). A similar performance degradation can be observed in CMX-B2 result which also uses Transformer based backbone. We conjecture that this phenomenon may be caused by low quality depth images in SUNRGBD dataset. The aim of our research is not to construct a state-of-the-art method that has a marginal mIoU improvement over other methods, but to construct a method that has a better performance-speed balance and is more suitable for robot platform. Given that AsymFormer still has faster inference speed than other methods, we consider this performance acceptable for AsymFormer.

\begin{table}[htbp]
\centering
\setlength{\tabcolsep}{3mm}{
\caption{\label{tab:SUNRGBD}Comparison Results on SUNRGBD. \# denotes multi-scale test.}
\begin{tabular}{c|cc}
\hline \hline
\textbf{Method} & \textbf{Pixel Acc. (\%)} & \textbf{mIoU (\%)} \\ \hline 
RDFNet\cite{park2017rdfnet}          & 81.5                     & 47.7               \\
ESANet\cite{seichter2021efficient}         & -                        & 48.0                 \\
ACNet\cite{hu2019acnet}           & -                        & 48.1               \\
SA-Gate\cite{chen2020bi}         & 82.5                     & 49.4               \\
CMX-B2\cite{liu2022cmx}         & 82.8                     & 49.7               \\
MSFNet\cite{jiang2022multi}          & -                        & 50.3               \\
FRNet\cite{zhou2022frnet}           & 87.4                     & 51.8               \\
PGDENet\cite{zhou2022pgdenet}         & 87.7                     & 51.0                 \\\hline
AsymFormer   & 81.9                     & 49.1               \\\hline \hline
\end{tabular}}
\end{table}

\subsection{Visualization}
\textbf{1) LAFS Attention Map:} As shown in Figure \ref{fig:CBAM}, to demonstrate that LAFS performs better than CBAM \cite{woo2018cbam} in selecting features in the spatial dimension, we visualized the spatial attention weights of both methods. It can be observed that LAFS provides better coverage of informative regions in the image while maintaining consistency within objects and preserving the integrity of edges.

\begin{figure}[htbp]
\vspace {-0.9em}
\centering
\includegraphics[width=0.4\textwidth]{CBAM_SCC.png}
\caption{\label{fig:CBAM}Difference between CBAM and LAFS's spatial attention map.}
\vspace {-0.9em}
\end{figure}

\textbf{2) Semantic Segmentation Results:} The Figure \ref{fig:result} demonstrates the segmentation results of AsymFormer on the NYUv2 dataset. As observed, while maintaining a significantly faster inference speed compared to other methods, AsymFormer achieves comparable semantic segmentation accuracy to mainstream approaches.

\begin{figure}[htbp]
\vspace {+0.7em}
\centering
\includegraphics[width=0.4\textwidth]{visualization.pdf}
\caption{\label{fig:result}Visualization of AsymFormer Semantic Segmentation Results.}
\vspace {-1.1em}
\end{figure}


\section{CONCLUSIONS}
In this work, we proposed AsymFormer, which aims to construct a less-redundant real-time indoor scene understanding system. To enhance efficiency and reduce redundant parameters, we implemented the following improvement: 1. We employed an asymmetric backbone that compressed the parameters of the Depth feature extraction branch, thus reducing redundancy. 2. We introduced the LAFS module for feature selection, utilizing learnable feature weights to calculate spatial attention weights. This resulted in a significant performance improvement while almost maintaining the inference speed. 3. Expanding on feature selection, we integrated modeling of self-similarity in multi-modal features, validating its capability to enhance network accuracy with minimal additional model parameters. The experiments demonstrated that the AsymFormer surpassed existing SOTA methods in terms of speed, demonstrating our method achieved a balance between accuracy and speed. Moving forward, we will continue to optimize the modules and address issues such as self-supervised pre-training of the model, aiming for further improvements.

{\small
\bibliographystyle{IEEEtran}
\bibliography{ref}
}

\end{document}
