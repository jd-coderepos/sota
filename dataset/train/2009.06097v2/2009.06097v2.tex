


\section{Experiments}

\begin{table*}[t]
\centering
\begin{tabular}{lcccc}
\toprule
                           & Quasar-T & SearchQA  & NQ(long) & NQ(short) \\
                           & EM/F1     & EM/F1     & F1       & F1        \\
\midrule                
R3~\citep{wang2018r}                         & 35.3/41.7 & 49.0/55.3 & -        & -         \\
DECAPROP~\citep{tay2018densely} &38.6/46.9 & 62.2/70.8 &- &-\\
DS-QA~\citep{lin2018denoising}                      & 42.2/49.3 & 58.8/64.5 & -        & -         \\
Multi-passage BERT~\citep{wang2019multi}         & 51.1/59.1 & 65.1/70.7 & -        & -         \\
DrQA~\citep{chen2017reading}                       & 37.7/44.5 & 41.9/48.7 & 46.1     & 35.7      \\
DecAtt + DocReader~\citep{kwiatkowski2019natural}         & -         & -         & 54.8     & 31.4      \\
BERT$_{\text{joint}}$~\citep{alberti2019bert}                  & -         & -         & 64.7     & 52.7      \\
BERT$_{\text{wwm}}$ + SQuAD2~\citep{pan2019frustratingly} & -         & -         & 68.2    & 57.2       \\
RikiNet-RoBERTa~\citep{liu2020rikinet} & -         & -         & 75.3    & \textbf{59.3}       \\
\midrule
Sliding Window   & 52.9/62.8 & 65.8/73.2 & 75.3     & 56.4\\
Sparse Attention~\citep{sparsetransf}               & 52.1/62.0 & 64.7/71.7 & 74.5     & 56.1\\
Locality-Sensitive Hashing~\citep{reformer}       & 53.2/62.9 & 66.0/73.5 & 75.5     & 56.4\\
\midrule
Cluster-Former (\#C=64)        & 53.3/63.3 & 67.0/74.2 & 76.3     & 56.7\\
Cluster-Former (\#C=256)       & 53.6/63.5 & 67.5/74.5 & 76.3     & 56.7\\
Cluster-Former (\#C=512)       & \textbf{54.0}/\textbf{63.9} & \textbf{68.0}/\textbf{75.1} & \textbf{76.5}     & 57.1\\
\bottomrule
\end{tabular}
\caption{  Results on Quasar-T, SearchQA test sets and NQ dev set.  \#C: number of clusters.}
\label{tbl:qa}
\end{table*}

\begin{table*}
\centering
\setlength{\tabcolsep}{0.5em}
\begin{tabular}{lcccccc}
\toprule
               & \multicolumn{3}{c}{Long Answer}    & \multicolumn{3}{c}{Short Answer}   \\
               
               & F1            & Precision & Recall & F1            & Precision & Recall \\ \midrule
BigBird-ETC-large~\citep{zaheer2020big}      & 77.8          & 77.5      & \textbf{78.1}   & 57.9          & 63.7      & 53.0   \\
RikiNet~\citep{liu2020rikinet}        & 76.1          & 78.1      & 74.2   & \textbf{61.3} & \textbf{67.6}      & 56.1   \\ \midrule
Cluster-Former (Ours) & \textbf{78.0} & \textbf{78.5}      & 77.5   & 60.9          & 62.1      & \textbf{59.8}  \\
\bottomrule
\end{tabular}
\caption{Results on Natural Questions (NQ) leaderboard (test set). We show two published results here from over 40 submissions. Our model achieves No.1 for long answer and No.4 for short answer. }
\label{tbl:nq_results}
\end{table*} 
\subsection{Datasets}
We evaluate our proposed approach on 
multiple question answering benchmarks. 
The statistics of all the datasets are summarized in Table~\ref{tbl:sts}.
\begin{itemize}[itemsep=1pt,topsep=2pt,leftmargin=12pt]
\item
\textbf{Quasar-T\footnote{https://github.com/bdhingra/quasar}}~\citep{dhingra2017quasar}: The goal of this task is to answer open-domain questions from Trivia Challenge. All the passages harvested through information retrieval can be used to answer questions. The task requires the model to generate answers in phrases. The evaluation metric on this dataset is based on Exact Match and F1 score of the bag-of-words matching. 
Our evaluation tool\footnote{https://rajpurkar.github.io/SQuAD-explorer/} comes from the SQuAD dataset.
\item
\textbf{SearchQA\footnote{https://github.com/nyu-dl/dl4ir-searchQA}}~\citep{dunn2017searchqa}: The setting of this dataset is the same as Quasar-T, except that the questions are sourced from Jeopardy! instead.
\item
\textbf{Natural Questions\footnote{https://ai.google.com/research/NaturalQuestions}}~\citep{kwiatkowski2019natural}: This task aims to answer questions based on a given Wikipedia document, and has two settings.
($i$) Long answer: select a paragraph that can answer the question based on the Wikipedia document if any.
($ii$) Short answer: extract an answer phrase from the document if the document contains the answer.
As the given document may not contain answer, we can either predict an answer or predict no answer. 
The evaluation metric on this dataset is the F1 score, where
true positives are exactly correct answers, false positives are incorrect answer predictions, and false negatives are incorrect ``no answer" predictions.
As the test set is hidden, we split 5\% of the training set for validation, and use the original validation set for testing.
We use the official tool from the dataset to evaluate our models. We also submit our best  model to the leaderboard.
\end{itemize}


\subsection{Implementation Details}
All the models are trained on 8 Nvidia V100 GPUs. 
For clustering, we adopt ``Yinyang kmeans''~\citep{ding2015yinyang}\footnote{https://github.com/src-d/kmcuda} which takes less than 5 seconds for clustering in all our experiment settings.
We set the memory size for clustering  $M=100,000$ in Algorithm~\ref{alg:alg}.
Based on our experiments, it makes little difference for memory banks with 50k and 100k, update cycles with 1 iteration or half iteration.
We use cluster centroids that perform the best on the validation set for test set experiments.
As the cluster-centroid is offline computed, the inference time is the same as the sliding-window-based method.
We initialize our models with RoBERTa-large~\citep{roberta}.
As the number of position embeddings of RoBERTa is limited to 512, we cannot assign different position embeddings to all tokens.
Instead, we assign the same position embeddings to each chunked sequence.

The majority of our model is made up of Sliding-Window Layers, as the local information is essential for QA tasks. We adopt the proposed Cluster-Former Layer in layers 15 and 20 to further capture long-range information.
We set the sliding window size $l$ to 256, stride $m$ to 224, and change the number of clusters in \{64, 256, 512\} to analyze its impact on the final performance.
We prepend a special token to the beginning of all the given/retrieved paragraphs and directly concatenate all the paragraphs as the final context sequence.
Due to memory constraints, we set the max length to be 5000 during training and 10000 during inference.
During dataset finetuning, we use Adam~\citep{kingma2014adam} to optimize the model. We set warm-up updates to 2,220, maximal updates to 22,200, learning rate to $5\times10^{-5}$, and batch size to 160.
We tune the dropout rate from \{0.1, 0.15, 0.2\} for all the methods including baselines and report the best results.
The model converges in one day for all the QA datasets.

For Quasar-T and SearchQA, we predict the start and end positions of the answer.
For Natural Question, we first identify whether the question has short/long answers or not based on the mean values of the first hidden state of all the chunked sequences, $\frac{1}{K}\sum_{k=1}^{K}\mathbf{H}^{N}_k[0]$, where $K$ is the number of chunks and $N$ is the number of layers. 
If answerable, we rank all the candidates for long answer selection, and predict the start and end positions of short answers.
Our model submitted to Natural Question Leaderboard ensembled 3 models with 512 clusters, and only these models are firstly trained on SQuAD2.0 and then finetuned on Natural Question dataset.
\begin{table}
\setlength{\tabcolsep}{4.3pt}
\centering
\begin{tabular}{ccccc}
\toprule
   & 3         & 4         & 5         & 6         \\
 \midrule
8  & 55.7/65.0 & 55.6/64.4 & 54.7/64.3 & 55.4/64.6 \\
12 & 55.1/64.9 & 55.8/65.0 & \textbf{56.1/65.4} & 55.4/64.6 \\
16 & 55.6/65.0 & 55.2/64.7 & 55.1/64.6 & 54.8/64.1 \\
20 & 54.8/64.2 & 55.4/64.8 & 55.1/64.6 & -   \\     \bottomrule
\end{tabular}
\caption{Experiments on Quasar-T dev dataset. $a\in \{3, 4, 5, 6\}$ and $b\in \{8,12,16,20\}$, if the layer number $l\; \%\; a==0$ and $l >= b$, we set it as Cluster-Former Layer, otherwise Sliding Window Layer. }
\label{tbl:cf_layers}
\end{table}

\subsection{Baselines}
We compare our models with several strong baselines, including:


\textbf{R3}~\citep{wang2018r} proposes to use reinforcement learning to jointly train passage ranker and reader.
\textbf{DS-QA}~\citep{lin2018denoising} proposes to first use paragraph selection to filter the noisy data and then trained model on denoised data.
\textbf{Multi-passage BERT}~\citep{wang2019multi} proposes to filter the passages and then merge multiple useful passages into one sequence, which can be encoded by BERT.
\textbf{DrQA}~\citep{chen2017reading} makes use of attention mechanism across the question and the document for answer phrase extraction.
\textbf{DecAtt and DocReader}~\citep{kwiatkowski2019natural} is based on a pipeline approach that first uses a simpler model to select long answers and then a reading comprehension model to extract short answers from the long answers.
\textbf{BERT$_\text{joint}$}~\citep{alberti2019bert} jointly trains short and long answer extraction in a single model rather than using a pipeline approach.
\textbf{BERT$_\text{wwm}$+SQuAD2}~\citep{pan2019frustratingly} makes use of multi-task learning to further boost performance.
\textbf{RikiNet-RoBERTa}~\citep{liu2020rikinet} proposes a dynamic paragraph dual-attention reader and a multi-level
cascaded answer predictor.
\textbf{BigBird-ETC}~\citep{zaheer2020big} makes use of a sparse attention mechanism to encode long sequences.

We also re-implement several strong baselines which have not been applied to process long context in question answering tasks:


\begin{table}
\centering
\setlength{\tabcolsep}{4.3pt}
\begin{tabular}{lcc}
\toprule
                           & Wikitext & Enwik8\\
                           & ppl         & bpc    \\
\midrule

Sliding window       & 20.8        & 1.34   \\
Sparse Attention    & 20.5        & 1.29   \\
Locality-Sensitive Hashing   & 20.8        & 1.33   \\
\midrule
Cluster-Former (\#C=64)    & 20.5        & 1.28   \\
Cluster-Former (\#C=256)   & 20.3        & 1.24   \\
Cluster-Former (\#C=512)   & \textbf{20.2}        & \textbf{1.22}  \\
\bottomrule
\end{tabular}
\caption{Results on Language Modeling.  \#C: number of clusters;  Wikitext: Wikitext-103.}
\label{tbl:lm}
\end{table}

\begin{table*}[h]
\centering
\begin{tabular}{lp{13cm}}
\toprule
Question         & Where did the underground railroad start and finish ?\\
Context & The Underground Railroad by artist Charles T. Webber , 1893 Date Late 1700s - 1865 Location Northern United States with routes to Canada , Mexico ...\\
\midrule
Special token & \textless{}s\textgreater \textless{}s\textgreater \textless{}s\textgreater Island island in the colonies city\textless{}s\textgreater \textless{}s\textgreater \textless{}s\textgreater With in the in . 
\\
Time & did start and finish 1893 Date 1700 1865 Location Participants Outcome Deaths 19 1763  
\\
Stopwords & the the , the , , , , to , , , , the American runaway slaves of free states the , , , it to , a the 
\\
Entity & Canada Mexico Canada is applied Florida Spanish Railroad Railroad Railroad \\
\midrule
Positions         &   49, 50, 51, 52, 53, 54, 55, 115, 116, 168, 273, 394, ..., 
6022, 6040, 6042, 6060, 6094\\
\bottomrule
\end{tabular}
\caption{An example from Natural Question dataset. The rows in the middle section show the corresponding words of the clustered hidden states, and the bottom row shows the positions of the clustered hidden states. ``\textless{}s\textgreater"  refers to start token of long answer candidate.}
\label{tbl:cluster}
\end{table*}
\begin{itemize}\item
\textbf{Sliding Window}: The original method is fully made up of Sliding-Window Layers and can only attend to local information.
To make a fair comparison among different methods on long-range information collection, we replace several layers of this sliding window baseline with Sparse Attention, Locality-Sensitive Hashing, and Cluster-Former. 
\item
\textbf{Sparse Attention}~\citep{sparsetransf}: This method replaces several layers in the previous baseline by training a Transformer layer across sequences on pre-selected positions. We run this sparse Transformer on all the hidden states in the same position across sequences, so that the output of sparse Transformer can merge the information from different sequences.
\item
\textbf{Locality-Sensitive Hashing}~\citep{reformer}:  This method hashes hidden states into different buckets determined by randomly-initialized hashing vectors.
A Transformer layer is then applied across buckets to build Sparse Attention across the whole sequence.
Note that this method cannot be directly used for question answering without adding Sliding-Window layer, as our QA model is initialized by RoBERTa that only has 512 position embeddings.
\end{itemize}



\subsection{Experimental Results}

\paragraph{State-of-the-Art Results on QA} 
Table~\ref{tbl:qa} and \ref{tbl:nq_results} show that our proposed method outperforms several strong baselines, thanks to its ability to encode both local and global information.
Cluster-Former with 512 clusters achieves new state-of-the-art results on Quasar-T, SearchQA and Natural Question (long answer).

\paragraph{Effect of Cluster-Former}
We also test the ability of Cluster-Former on modeling long-range dependencies.
Note that Sparse Attention~\citep{sparsetransf} and Locality-Sensitive Hashing~\citep{reformer} have never been tested on question answering tasks with long context.
For fair comparison, we set the layers 15 and 20 as either Sparse Attention, Locality-Sensitive Hashing or our Cluster-Former, and the left layers are Sliding Window layers. 


As shown, Sparse Attention performs worse than our Cluster-Former.
The loss may come from the noise introduced by pre-selected positions, the corresponding words of which may not be related.
We set the number of hashing vectors in Locality-Sensitive Hashing (LSH) to 64, the same as the number of clusters in Cluster-Former.
LSH outperforms the baseline slightly on QA and consistently underperforms our Cluster-Former (\#C=64).
Overall, our Cluster-Former performs the best.

\paragraph{Effect of Number of Cluster Centroids} 
We also test the effect of different numbers of cluster centroids ($C$) on model performance. 
We observe that the model with 512 clusters works significantly better than the model with 64 clusters on most of the tasks.
However, for Natural Questions Long Answer setting, the improvement is marginal. As we mainly rely on the hidden state of special tokens ``$<$s$>$" for long answer selection, and the same tokens can be assigned into same chunk more easily even with a smaller number of clusters.


\paragraph{Selection of Cluster-Former Layers}
We also have an analysis on which layers are better used for Cluster-Former layer. As shown in Table~\ref{tbl:cf_layers}, we conduct a hyper-parameter search. And find that it can get better performance with at least one Cluster-Former layers in the middle layer (8-16). The worst results come from only one Cluster-Former layer in the layer of 22 or 23. 



\paragraph{Language Modeling}
Although we focus on QA tasks, to demonstrate the versatility of Cluster-Former, we conduct additional experiments on language modeling using the Wikitext-103~\citep{merity2016pointer} and Enwik8~\citep{mahoney2011large} benchmarks. 
All the models are trained from scratch.
We set the number of layers to 16, with 8 heads per layer.
Our Cluster-Former Layer is used in layers 11 and 15 as in QA models. 
We segment long input into short sequences of 3072 tokens, set sliding window size $l$ to 256, and stride $m$ to 128.
SGD is used for optimizing the models.
We set clip threshold of gradients to 0.1, warm-up updates to 16,000, maximal updates to 286,000, dropout rate to 0.3, learning rate to 0.1, and batch size to 16.
The model will converge in 3 days for all the LM datasets.
As shown in Table~\ref{tbl:lm}, Cluster-Former outperforms strong state-of-the-art baselines.


\subsection{Qualitative Analysis}

We perform qualitative analysis on how the hidden states are clustered,
by visualizing the corresponding words and positions of the hidden states in Table~\ref{tbl:cluster}.
From the first row, we can see that the special tokens ``$<$s$>$" tend to belong to the same cluster. 
Note that ``$<$s$>$" is the start token of each long answer candidate, and its hidden state is used for final long answer selection.
Therefore, Transformer on this cluster can compare across the candidates to make the final prediction.

We further observe that the same types of token are more likely to appear in the same cluster. 
For example, words from the second row to the forth row cover the topics of time, stopwords, and organization \& geopolitical entities.

Finally, we randomly sample a cluster and list the positions of clustered hidden states in the last row of the table. 
We find that states in long distance, such as the 50-th and 6060-th states (over 6000 tokens apart), can be in one cluster, which demonstrates the ability of Cluster-Former in detecting long-range dependencies. Further, we observe that states tend to cluster in phrases. 
For example, we see consecutive positions such as ``49, 50, 51, 52, 53, 54, 55", which likely results from the sliding-window encoding.

