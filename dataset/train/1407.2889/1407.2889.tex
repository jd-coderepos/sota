\documentclass{ws-ijait}
\PassOptionsToPackage{hyphens}{url}


\usepackage{listings}

\usepackage{pgfplots}
\usepgfplotslibrary{units}
\pgfplotsset{compat=newest} 

\usepackage{tikz}
\usetikzlibrary{patterns}
\usepackage{algorithm2e}

\begin{document}

\markboth{C. S. Kouzinopoulos, J.-A. M. Assael, Th. K. Pyrgiotis, K. G. Margaritis}
{A Hybrid Parallel Implementation of the Aho-Corasick and Wu-Manber Algorithms Using CUDA and MPI}

\catchline{}{}{}{}{}


\title{A Hybrid Parallel Implementation of the Aho-Corasick\\
and Wu-Manber Algorithms Using NVIDIA CUDA and MPI\\
Evaluated on a Biological Sequence Database}

\author{Charalampos S. Kouzinopoulos, John-Alexander M. Assael, Themistoklis K. Pyrgiotis, and
Konstantinos G. Margaritis}

\address{Parallel and Distributed Processing Laboratory\\
Department of Applied Informatics, University of Macedonia\\
156 Egnatia str., P.O. Box 1591, 54006 Thessaloniki, Greece\\
charalampos.kouzinopoulos@cern.ch, john.assael@wolfson.ox.ac.uk, t.pirgiot@gmail.com, kmarg@uom.gr}


\maketitle

\begin{history}
\received{(Day Month Year)}
\revised{(Day Month Year)}
\accepted{(Day Month Year)}
\end{history}

\begin{abstract}Multiple matching algorithms are used to locate the occurrences of patterns from a finite pattern set in a large input string. Aho-Corasick and Wu-Manber, two of the most well known algorithms for multiple matching require an increased computing power, particularly in cases where large-size datasets must be processed, as is common in computational biology applications. Over the past years, Graphics Processing Units (GPUs) have evolved to powerful parallel processors outperforming Central Processing Units (CPUs) in scientific calculations. Moreover, multiple GPUs can be used in parallel, forming hybrid computer cluster configurations to achieve an even higher processing throughput. This paper evaluates the speedup of the parallel implementation of the Aho-Corasick and Wu-Manber algorithms on a hybrid GPU cluster, when used to process a snapshot of the Expressed Sequence Tags of the human genome and for different problem parameters.\end{abstract}

\keywords{Multiple pattern matching; CUDA; MPI; Aho-Corasick; Wu-Manber; Biological sequence database; Expressed sequence tag}

\section{Introduction}
\label{sec:introduction}
The endless market demand for better and more realistic computer graphics evolved Graphics Processing Units into powerful and highly parallel multicore processors with enormous computational power. Moreover, their parallel nature facilitates the rapid execution of scientific calculations, outperforming in many cases traditional CPUs. Nowadays, various APIs have been introduced to enable the development and execution of General Purpose Computations on a GPU \cite{gpgpuorg} (GPGPU). One of the most widely known APIs for GPGPU is NVIDIA CUDA.\cite{CUDA_SDK}

Multiple pattern matching is a variant of the string matching problem. It is used to locate all the positions in an input string where one or more patterns from a finite pattern set occur. Computational Biology is a major area where the multiple pattern matching problem is applicable since many tasks require the location of nucleotide or Amino Acid sequence patterns in biological sequence databases. The multiple pattern matching problem can be defined as,\cite{Kouzinopoulos2011}:

\textit {\textbf{Definition}. Given an input string  of size  and a finite set of  patterns , where each  is a string  of size  over a finite character set , the alphabet size is denoted as  and the total size of all patterns as , the task is to find all occurrences of any of the patterns in the input string. More formally, for each  find all  where  such that for all  where  it holds that }

This paper presents a hybrid implementation of the Aho-Corasick \cite{Aho1975} (AC) and Wu-Manber \cite{Wu1994} (WM) multiple pattern matching algorithms on an MPI cluster using the CUDA API. The performance of both the sequential and the parallel implementations of the algorithm was evaluated on a homogeneous cluster of nodes for various problem parameters, including different pattern set and cluster node sizes. The data set used for the experiments of this paper consisted of a snapshot of the Expressed Sequence Tags (ESTs) and different sets of patterns.

The paper is organized as follows: Related work is presented in section \ref{sec:related-work}. Section \ref{sec:background} details the way the algorithms work. The GPU and MPI architectures and implementations are introduced in section \ref{sec:distrcudawm}. Experimental methodology and results are given in sections \ref{sec:experimental-methodology} and \ref{sec:experimental-results} respectively. Finally, the conclusions of this research are presented in section \ref{sec:conclusions}.

\section{Related Work}
\label{sec:related-work}

Several implementations of multiple pattern matching algorithms running on GPUs have been introduced during the last years, offering a substantial performance increase compared to their sequential versions. Some major fields of interest on these implementations include bioinformatics and intrusion detection systems.

In 2008, a group of researchers proposed a GPU version of the Wu-Manber multiple pattern matching algorithm.\cite{Huang2008} The algorithm was to be used in network intrusion detection, and was implemented under OpenGL in order to take the advantage of an NVIDIA GeForce 7600 GT card. The experiments proved to be two times faster than the existing optimized version that was used in Snort.\cite{SnortWeb}

Later in 2011,\cite{Hongjian2011} a group of researchers proposed an optimized version of the agrep algorithm,\cite{Wu1992} which is based on Wu-Manber using the CUDA API and taking advantage of a GeForce GTX285, for approximate nucleotide sequence matching. The performance of the implementation was evaluated for sequences of genomes, comparing an OpenMP implementation to the CUDA implementation of the algorithm, and proved to exhibit -fold and -fold performance speedups, for pattern sizes of  and  respectively.

Moreover, a modified version of the Wu-Manber algorithm for approximate matching was presented in 2011.\cite{tran2011} The implementation was simplified to run on a NVIDIA GeForce 480 using the OpenCL API, and managed to achieve -fold speedups. The next year, another group of researchers,\cite{pyrgiotis2012parallel} implemented the Wu-Manber algorithm using OpenCL and conducted an extended research and analysis on texts with different alphabet sizes, including biological sequences, and proved to have a x speedup, running the experiments on a NVIDIA GTX 280.

Finally in 2012, distributed hybrid implementations of several multiple pattern matching algorithms were presented,\cite{Kouzinopoulos2012} using MPI. One of the algorithms was Wu-Manber, and reached a maximum of x performance speedup. The comparison was between the single-threaded sequential CPU implementation and a multithreaded implementation running on  dual-core nodes.

The Aho-Corasick algorithm was implemented in \cite{Vasiliadis2008} using the CUDA API to perform network intrusion detection. The Aho-Corasick trie was represented by a two-dimensional \textit{state transition} array; each row of the array corresponded to a state of the trie, each column to a different character of the alphabet  while the cells of the array represented the \textit{next} state. The array was precomputed by the CPU and was then stored to the texture memory of the GPU. The input string (in the form of network packets) was stored in buffers allocated using \textit{pinned} host memory using a double buffering scheme and was copied to the global memory of the GPU as soon as each buffer was full. Since the input string was in the form of network packets, two different parallelization approaches were considered; with the first, each packet was processed by a different warp of threads while with the second, each packet was processed by a different thread. A similar implementation of the Aho-Corasick algorithm was used in \cite{Vasiliadis2010} to perform heavy-duty anti-malware operations.

The Parallel Failureless Aho-Corasick algorithm (PFAC), an interesting variant of the Aho-Corasick algorithm on a GPU was presented in \cite{Lin2010}. It is significantly different than the rest of the parallel implementations of the same algorithm in the sense that each character of the input stream was assigned to a different thread of the GPU. Since each thread needs only to examine if a pattern exists \textit{starting} at the specific character and no back-tracking takes place, the \textit{supply} function of Aho-Corasick was removed. The \textit{goto} function was mapped into a two-dimensional \textit{state transition} array. The \textit{state transition} array was then stored in shared memory by grouping the patterns based on their prefixes and distributing these groups into different multiprocessors.

In \cite{Tumeo2011}, the Aho-Corasick algorithm was implemented using the CUDA API. The Aho-Corasick trie was represented using a two-dimensional \textit{state transition} array that was precomputed on the CPU and stored to the texture memory of the GPU. Instead of storing a map of the final states in another array, the final states that corresponded to a complete pattern were flagged directly in the \textit{goto} array by reserving  bit. Bitwise operations were then used to check its value. The parallelization of the algorithm was achieved by assigning different characters of the input string to different threads of the GPU and letting them perform the matching by accessing the shared \textit{state transition} array.

The work presented in \cite{Zha2011} focused on the implementation of the Aho-Corasick algorithm on a GPU. The Aho-Corasick trie was precomputed on the CPU and was stored in the texture memory of the GPU but it is not clear the way it was represented. The input string was stored in the global memory of the GPU, partitioned to blocks and assigned to different threads in order to achieve parallelization. The threads were then responsible to process the input string and create an output array of the states of the Aho-Corasick trie that corresponded to each character position. The paper utilized a number of optimizations in order to further improve the performance of the algorithm implementation; casting the input string from \textit{unsigned char} to \textit{uint4} to ensure that each thread will read  input string characters from the global memory instead of . To further improve the bandwidth utilization, the accesses of multiple threads inside the same half-warp were coalesced by reading the required data to process an input string block to the shared memory of the device. Finally, to avoid shared memory bank conflicts, the threads of a half-warp were accessing memory from different banks.

An implementation of the Aho-Corasick algorithm was also presented in \cite{Hu2012} using the CUDA API. Similar to the methodology used in previously published research papers, the trie of the algorithm was represented using a two-dimensional \textit{state transition} array and was stored compressed in the texture memory of the GPU while the input string was stored in global memory. Kargus was introduced in \cite{Jamshed2012}, a software that utilizes the Aho-Corasick algorithm to perform intrusion detection on a hybrid system consisting of multi-core CPUs and heterogeneous GPUs. The trie of Aho-Corasick was represented in the GPU using a two-dimensional \textit{state transition} array. The \textit{state transition} array was created in the CPU and was stored in the GPU. The network packets that comprise the input string were stored in texture memory. To increase the utilization of the memory bandwidth of the GPU, the input string was cast to \textit{uint4} using a technique similar to \cite{Zha2011}.

In \cite{Pungila2012}, the Aho-Corasick and Commentz-Walter algorithms were used to perform virus scanning accelerated through a GPU. The Aho-Corasick and Commentz-Walter tries were represented in the GPU using stacks. The \textit{goto} and \textit{supply functions} were substituted with offsets, essentially serializing the trie in a continuous memory block. The stacks were precomputed in the CPU and were then transferred to the GPU. Parallelization was achieved using a data-parallel approach.

Finally, the Aho-Corasick algorithm was implemented in \cite{Tumeo2012} for a heterogeneous computer cluster with  NVIDIA Tesla S1070 boxes, each box being the equivalent of  C1060 GPUs, for a total of  GPUs.

To the best of our knowledge, this is the first time that the Aho-Corasick and Wu-Manber algorithms are implemented in parallel on a hybrid CUDA/MPI parallel cluster architecture.

\section{Background}
\label{sec:background}

\subsection{Aho-Corasick}

Aho-Corasick is an extension of the Knuth-Morris-Pratt algorithm for a set of patterns . It uses a deterministic finite state pattern matching machine; a rooted directed tree or \textit{trie} of  with a \textit{goto} function  and an additional \textit{supply} function . The \textit{goto} function maps a pair consisting of an existing state  and a symbol character into the next state. It is a generalization of the \textit{next} table or the \textit{success} link of the Knuth-Morris-Pratt algorithm for a set of patterns where a parent state can lead to its child states by  where  is a matching character. Each state of the trie is labeled after a single character of a pattern . If  denotes the label of the path between the initial state and a state , then  is also a prefix of one of the patterns. For each pattern  there is a state  such that . This state is marked as terminal and when visited during the search phase indicates that a complete match of  was found. The \textit{supply} function of Aho-Corasick is based on the \textit{supply} function of the Knuth-Morris-Pratt algorithm. It is used to visit a previous state of the automaton when there is no transition from the current state to a child state via the \textit{goto} function.

\begin{figure}[h]
\centering
{\epsfig{file = ACtransitiondiagram2.eps, width = 8.0cm}}
  \caption[The automaton of the Aho-Corasick algorithm]{The automaton of the Aho-Corasick algorithm for the pattern set ``AAC", ``AGT" and ``GTA" }
  \label{fig:ACtransitiondiagram}
\end{figure}

The \textit{goto} function and the \textit{supply} function are constructed during the preprocessing phase. To build the \textit{goto} function, the trie is depth-first traversed and extended for each character of the patterns from a finite pattern set  at the same time the outgoing transitions to each state are created. The \textit{supply} function is built in transversal order from the trie until it has been computed for all states. For each state , the \textit{supply} link can be determined based on the longest suffix of  that is also a prefix of any pattern from . Assume that for the parent state  of , . If  also has an outgoing transition to a state  by , then the \textit{supply} state of  can be set to . In any other case,  must be checked for a transition to a state by  and so on, until one such state is found or it is determined that no such state exists; in that case, the \textit{supply} state of  is set to the initial state.

\begin{algorithm}[H]

Function AC\_Preproc\_Goto (  )\\

create state 

\ForAll{  } {

	
}

\For{  } {

	; \\
	
	\While{  }{
	
		; \\
	}
	
	\For{ } {
		
		create state \\
		
		\ForAll{  } {

			

		}
		
		\\
	
		\\
	
		\\
	}
	
	\\
	Add terminal state on \\
}

\caption{The construction of the \textit{goto} function  of the Aho-Corasick automaton}
\label{compl:aho_corasick_preprocessing_goto}
\end{algorithm}

Let  be the longest suffix of the input string  that is also a prefix of any pattern . The character  located at position  of the input string is scanned next. If there is an outgoing transition from the current state  to another state  as indicated by the \textit{goto} function, then  is the new longest suffix of the input string at position  that is a prefix of one of the patterns. A match of a pattern exists in the input string if . If, on the other hand , then  is checked for an outgoing transition by . If  leads to a state  then . If  then  is considered and so on, until an outgoing transition by  is found or until the supply state of the initial state is reached; in that case, the search will start again from the initial state. The construction of the \textit{goto} function of the Aho-Corasick automaton is given in Algorithm~\ref{compl:aho_corasick_preprocessing_goto}, the computation of the \textit{supply} function is presented in Algorithm~\ref{compl:aho_corasick_preprocessing_supply} while the search phase of the Aho-Corasick algorithm is given in Algorithm~\ref{compl:aho_corasick_search}. The output function returns  for each terminal state  and is denoted as . A transition that does not point to a state is denoted as .

\begin{algorithm}[H]

Function AC\_Preproc\_Supply (  )\\

\ForAll{  } {


	\eIf{} {
		\\
	}{
	
		\\
	
	}
}

\ForAll{  trie states in transversal order } {

	\ForAll{  } {

		\\
		
		\If{} {

			\\
		
			\While{ }{
	
				\\
			}
			
			\\
		}
	}
}

\caption{The construction of the \textit{supply} function  of the Aho-Corasick automaton}
\label{compl:aho_corasick_preprocessing_supply}
\end{algorithm}

The \textit{goto} function can be implemented using any of the following data structures: an array of size , where each state has an outgoing transition for every character of the alphabet by precomputing all the transitions simulated by the \textit{supply} function \cite{Navarro2002}; a linked list that is space efficient but not time efficient; or a balanced search tree that is considered as a heavy-duty compromise and often not practical \cite{Dori2006}. The implementation used for the experiments of this paper was based on code from the Streamline system I/O software layer \cite{WEB03}. It uses a linked list for the \textit{supply} function and a linked list of arrays to represent the transitions of the \textit{goto} function with each cell of the arrays potentially containing a pointer to the next node. Each list node corresponds to a different state of the trie and has an array of size  with an outgoing transition for every character of . The trie of  can then be built for all  patterns in  time, with a total size of . The time to pass through a transition of the \textit{goto} function is  in the worst and average case, while the search phase has a cost of  in the worst and average case.

\begin{algorithm}[H]

Function AC\_Search (  )\\

\\
\For{ } {
	
	\While{  }{
			
		\\
	}
	
	\\
	
	\If{  is not empty }{
	
		report match at \\
	
	}
}

\caption{The search phase of the Aho-Corasick automaton}
\label{compl:aho_corasick_search}
\end{algorithm}

An example of a complete Aho-Corasick automaton for the pattern set ``AAC", ``AGT" and ``GTA" is presented in Figure~\ref{fig:ACtransitiondiagram}. Assume that the \textit{goto} function of the trie is already constructed and that the \textit{supply} function for states  has been computed. The \textit{supply} state of state  is determined next. State  is the parent state of state  since  and , therefore the \textit{goto} function of state  is considered next. Since  then  can be set to . If there was no outgoing transition from state  by  then  would be checked next for an outgoing transition to another state by  and so on, until one such state is found or is determined that no such state exists.


\subsection{Wu-Manber}

Wu-Manber is a generalization of the Horspool algorithm for multiple pattern matching. It scans the characters of the input string backwards for the occurrences of the patterns, shifting the search window to the right when a mismatch or a complete match occurs. To perform the shift, the \textit{bad character} shift function of the Horspool algorithm is used. The \textit{bad character} shift for a character  determines the safe number of shifts based on the position of the rightmost occurrence of  in \textit{any} pattern. The probability of  existing in one of the patterns increases with the size of the pattern set and is inversely proportional to the alphabet size and thus the maximum possible shift is decreased. To improve the efficiency of the algorithm, Wu-Manber considers the characters of the patterns and the input string as blocks of size  instead of single characters, essentially enlarging the alphabet size to .

\begin{figure}[h]
\centering
{\epsfig{file = wu_manber_border.eps, width = 9.0cm}}
  \caption{Comparing the suffix and prefix of the search window of the Wu-Manber algorithm}
  \label{fig:wu_manber_border}
\end{figure}

During the preprocessing phase, three tables are built from the patterns, the \textit{SHIFT}, \textit{HASH} and \textit{PREFIX} tables. \textit{SHIFT} is the equivalent of the \textit{bad character} shift of the Horspool algorithm for blocks of characters, generalized for multiple patterns. If  does not appear in \textit{any} pattern, the search window can be safely shifted by  positions to the right. Let  be the hash value of a block of  characters as determined by a hash function . Then, \textit{SHIFT[h]} is the distance of the rightmost occurrence of  to the end of \textit{any} pattern. The \textit{HASH} and \textit{PREFIX} tables are only used when the shift value stored in \textit{SHIFT[h]} is equal to . \textit{HASH[h]} contains an ordered list of pattern indices whose -character suffix has a hash value of . For each of these patterns, let  be the hash value of their -character prefix as determined by a hash function . The hash value  for each pattern  is stored in \textit{PREFIX[p]}. That way, a potential match of the -character suffix of a pattern can be verified first with the -character prefix of the pattern before comparing the patterns directly with the input string. The complexity of Wu-Manber was not given in the original paper, since hash functions  and  were not specified and the size of the \textit{SHIFT}, \textit{HASH} and \textit{PREFIX} tables was not given.\cite{Navarro2002} For the experiments of this paper, the algorithm was implemented with a block size of  and  while hash values  and  were calculated by shift and add; shifting the hash value to the left by \textit{bitshift} positions and then adding in the ASCII value of a pattern or input string character. The value of \textit{bitshift} was set to . Finally, the verification of the patterns to the input string was performed using the \textit{memcmp()} function of \textit{string.h}.

\begin{algorithm}[h]

Function WM\_Preproc (  )\\

Initialize all elements of  to \\

\For{ } {

	\For{ } {
		
		\\
		
		\\
			
		\\
			
		\If{  }{
			
			\\
			
			\\
			
			\\
		}
	}
}

\caption{The preprocessing phase of the Wu-Manber algorithm}
\label{compl:wu_manber_preproc}
\end{algorithm}

Assume that the search window is aligned with the input string at position  and that  is the hash value of the -character suffix of . Then the \textit{SHIFT} table is used to determine the number of safe shift positions. If \textit{SHIFT[h]}  then the search window is shifted by \textit{SHIFT[h]} positions. If, on the other hand, \textit{SHIFT[h]} , the suffix of the input string potentially matches the suffix of \textit{some} patterns of the pattern set and thus it must be determined if a complete match occurs at that position. The hash value  of the -character prefix of the input string starting at position  is then computed. For each pattern  with the same hash value  of its -character suffix, it is checked if \textit{PREFIX[p]} matches with . If both the prefix and the suffix of the search window match with the prefix and suffix of some , then the corresponding patterns are compared directly with the input string. The preprocessing phase of the Wu-Manber algorithm is detailed in Algorithm~\ref{compl:wu_manber_preproc} while the search phase is presented in Algorithm~\ref{compl:wu_manber_search}.

\begin{algorithm}[h]

Function WM\_Search (  )\\

\\

\While {  } {

	\\

	
	\uIf {  } {
		\\
	}
	\Else {
	
	
		\\
		
		\ForAll{ pattern indices  stored in  } {
		
			\If{  } {
		
				Verify the pattern corresponding to  directly against the input string\\
			}
		}
		
		\\
	}
}

\caption{The search phase of the Wu-Manber algorithm}
\label{compl:wu_manber_search}
\end{algorithm}

The complexity of Wu-Manber was not given in the original paper, since hash functions  and  were not specified and the size of the \textit{SHIFT}, \textit{HASH} and \textit{PREFIX} tables was not given.\cite{Navarro2002} For the experiments of this paper, the algorithm was implemented with a block size of  and  while hash values  and  were calculated by shift and add; shifting the hash value to the left by \textit{bitshift} positions and then adding in the ASCII value of a pattern or input string character. The value of \textit{bitshift} was set to . Finally, the verification of the patterns to the input string was performed using the \textit{memcmp()} function of \textit{string.h}. The cost of the implementation used in the experiments of this paper is as follows. To calculate the values of the \textit{SHIFT}, \textit{HASH} and \textit{PREFIX} tables during the preprocessing phase, the algorithm requires an  time. The space of Wu-Manber depends on the size of \textit{SHIFT}, \textit{HASH} and \textit{PREFIX}. The space needed for the \textit{SHIFT} table is . In the worst case there could be  patterns with the same hash value  or  for their -character suffix or -character prefix respectively, therefore \textit{HASH} and \textit{PREFIX} require a  space for a space complexity of .

In the worst case for the searching phase of the Wu-Manber algorithm, the input string and  characters of all  patterns consist of the same repeating character  with the character at position  of each pattern being different. The algorithm will then encounter a potential match on every position of the input string since \textit{SHIFT[h]} will constantly be . Therefore, as hash values  and  of the patterns will be identical, the  characters of all  patterns will be compared directly with the input string using the \textit{memcmp()} function. The worst case searching time of Wu-Manber is given in \cite{Chen2005} as . In \cite{Navarro2004} the lower bound for the average time complexity of exact multiple pattern matching algorithms is given as  and according to \cite{Chen2005} the searching phase of the Wu-Manber algorithm is optimal in the average case for a time complexity of . In \cite{Liu2005} the average time complexity of Wu-Manber was also estimated as .


\section{Distributed CUDA Implementation}
\label{sec:distrcudawm}

\subsection{GPU Architecture}
\label{sec:GPU-Architecture}

A GPU is a hardware device that acts as a separate co-processor to the host. It is based on a scalable array of multithreaded \textit{streaming multiprocessors} (SMs) that have a different design than CPU cores; they target lower clock rates; they support instruction-level parallelism but not branch prediction or speculative execution; and they have smaller caches \cite{Wilt2013}. Each SM consists of a number of stream processors (SPs), special function units for floating-point arithmetic operations and mathematical functions, one or more warp schedulers and an independent instruction decoder so they can run different instructions. The SPs, or CUDA Cores, are lightweight in-order processors that are used for arithmetic operations. Moreover, each SM has a number of resources including a -bit register file and a shared memory area that as detailed later in this section are distributed among the available threads and thread blocks respectively. On compute capability 1.x GPUs, SMs are grouped into \textit{Texture Processor Clusters} (TPCs) that contain additionally a texture unit and a texture memory cache. On compute capability 2.x and newer GPUs, the SMs are grouped into \textit{Graphics Processing Clusters} (GPCs).

The threads of a CUDA GPU are very lightweight comparing to threads of multicore CPU systems with a very small creation overhead involved. Although they are lightweight in the sense that they operate on small pieces of data, they are fully fledged in the conventional sense, each thread with its own stack, register file, program counter and local memory \cite{Halfhill2008}. The threads, that are identifiable through a unique thread ID using the \textit{threadIdx} variable are organized into \textit{thread blocks} and are then assigned to SMs, with each SM being capable of executing multiple thread blocks. The SMs divide the thread blocks into warps of threads that are queued for work. A warp typically consists of  threads, a half-warp consists of  threads while a quarter-warp consists of  threads. Threads are grouped into warps in a deterministic way; for warps with a size of  threads, the first warp will always contain threads with a thread ID between . Stream processors follow a MIMD model for warps since different warps can have different execution paths and process different data. Threads within the same warp though, follow a SIMD or as called by NVIDIA, Single Instruction, Multiple Thread (SIMT) model. The instruction decoder of an SM fetches a common, single instruction that will be executed by all the SPs at the same time, forming the basis of SIMT execution \cite{Lakshminarayana2010}. One instruction is fetched every fetch cycle per warp scheduler for a warp that is selected in a round robin fashion from among the active, ready warps of the SM. The instructions are then placed in a common issue queue from where they are dispatched for execution by the instruction dispatch units. The threads of a warp start at the same program address and execute concurrently a common instruction at every cycle. Individual threads can branch out and execute independently but this comes with a performance penalty. If threads of a warp diverge via a data-dependent conditional branch, such as an \textit{if-else} statement, the warp executes sequentially each branch path taken, disabling threads that are not on that path to ensure the correctness of the results \cite{CUDA_SDK}. The compiler inserts the reconvergence point using a field in the instruction encoding and also inserts an instruction before a diverging branch that provides the hardware with the location of the reconvergence point \cite{Papadopoulou2009}. Using this information, the threads converge back to the same execution path when all paths are complete. 

There are two sources of latency that affect the performance of a kernel, instruction latency and memory latency. Instruction latency is the number of cycles between issuing an arithmetic instruction and the processing of the instruction by the arithmetic pipeline of the GPU. Memory latency is the number of cycles needed to access a memory address. To hide both arithmetic and memory latency, there should be a number of warps \textit{resident} or in other words maintained on-chip during the entire lifetime of the warp on an SM, so that the SM can choose between them instructions to issue. The number of resident warps required to completely hide latency depends on the architecture of the device. A warp may not be ready to execute due to different factors; waiting on register dependencies; accessing off-chip memory; waiting on some synchronization point; or waiting to finish executing the previous instruction. At every instruction issue time, a \textit{warp scheduler} switches from one warp to another and switches contexts between threads. Because the execution context, including program counters and registers, for each warp processed by a multiprocessor is resident on the SMs, context switching is very fast. The resources of an SM are limited, therefore the \textit{occupancy}, or the number of threads and blocks that an SM can maintain, can be affected by different factors; the block size, the shared memory usage and the register usage. An SM can only support a few concurrent resident thread blocks (usually  or ), therefore it is important to use blocks with a sufficiently large block size in threads. The amount of shared memory per block and the number of registers per thread used by a kernel also affect significantly the occupancy of an SM. To ensure maximum occupancy, a kernel should use up to



\noindent
shared memory and 



\noindent
registers per thread, although as detailed in \cite{Volkov2010}, maximizing occupancy does not always result in a better performance.

GPUs have different memories, both on-chip and off-chip, each with its own advantages and limitations. Unlike the memory architecture of traditional computer systems where the compiler is mainly responsible for distributing data between an off-chip RAM and different levels of cache, the programmer of a GPU application must, in most cases, explicitly copy data between the memory areas of the device, trying to find a balance between the size of each memory area and the cost to access it in terms of clock cycles. Although this model offers the potential for high performance gains, it also comes with an increased coding complexity.

The fastest and at the same time more limited memory area of a GPU is the register file, a highly banked array of processor registers built out of dense SRAM arrays \cite{Gebhart2012}. As already discussed, each SM has its own register file. The threads of each SM use dedicated registers to store their register context in order to perform context switching very fast. To accommodate all the resident threads per SM, GPU devices have large register files, with their size depending on the compute capability of the device;  per SM for compute capability 1.0 and 1.1 devices,  per SM for compute capability 1.2 and 1.3 GPUs,  per SM for compute capability 2.0 devices and  for compute capability 3.0 and 3.5 devices.

Shared memory is a small on-chip per SM memory space that has a low-latency access cost and can also be used to bypass the coalescing requirements of the global memory. Because it is shared between all threads of a thread block, it is usually used for synchronization as well as for data exchange between them. To enable concurrent accesses to it, shared memory is divided into -bit memory banks. The maximum bandwidth of the shared memory is then utilized when a single memory request accesses one address from each different bank. If addresses from the same memory bank are accessed, the accesses are serialized to avoid conflicts with a significant degradation of the shared memory's bandwidth. Compute capability 1.0 to 1.3 GPUs have  of shared memory per SM while compute capability 2.0 and newer GPUs have  per SM.

Texture memory is a cached read-only memory space with a two-dimensional locality that is initialized host-side. Compute capability 1.x GPUs have an L1 texture cache per TPC and an L2 texture cache that is accessible by all SMs. It is often used to work around the coalescing requirements of the global memory and to increase the memory throughput of the kernel. The first time that an address of the texture memory is accessed, it is fetched from the global memory with the high latency that it entails. In that case though, the texture caches of the device are used to \textit{cache} the data, therefore minimizing the latency when cache hits occur. Unlike traditional CPU caches, the texture caches don't have coherency; writing data to texture memory either host- or device-side actually result in the invalidation of all caches.

The GPU has its own off-chip device memory, global memory. If data resides to \textit{pageable} host memory, a memory area that is usually allocated using \textit{malloc()}, it can be transferred to the GPU device explicitly before the launch of the kernel. Page-locked or \textit{pinned} memory, memory that always resides in physical memory as it cannot be paged out to disk, can also be allocated in host. Transferring of data between page-locked host memory and the global memory of the \textit{device} is performed concurrently with kernel execution using DMA to hide part of the latency involved. The global memory is accessible by all SMs using memory transactions of  and  bytes with a high latency, usually between  and  clock cycles.

Local memory is a special type of memory that is a cacheable part of the global memory of the GPU. It is used when the registers of an SM are spilled. This can occur due to register pressure; when for example the execution context in terms of registers of a thread is higher than the hardware limit of the device. The term ``local'' refers to the fact that each thread has its own private area where its execution context is spilled, resolved at compilation time by the NVCC compiler.

\begin{figure}[h]
\centering
{\epsfig{file = GT240.eps, width = 12cm}}
  \caption{The NVIDIA GT200 Architecture}
  \label{fig:GT240}
\end{figure}

The NVIDIA GT 240 is a compute capability  GPU from the GT200 series of NVIDIA's GeForce graphics processing units. It has  of GDDR3 global memory,  Graphics clock rate,  Processor clock tester rate and  memory clock rate. As shown in Figure~\ref{fig:GT240}, it consists of  SMs, with  SMs per TPC. Each SM has  SPs for a total of  SPs. The SMs have  of on-chip shared memory, and  -bit registers. Each thread block can have a maximum of  threads while each SM supports up to  active threads and up to  active blocks. Each thread can use between  registers when  threads are used per SM and  as a maximum per-thread register usage. Since every SM contains one warp scheduler, one instruction is issued per warp over  cycles. The latency of the arithmetic pipeline is  cycles, therefore it can be completely hidden by having  active warps at any one time. A request for any words within the same memory segment of the global memory for correctly aligned addresses is \textit{coalesced}, using one memory transaction per half-warp. The size of each global memory segment is  bytes for -byte words,  bytes for -byte words or  bytes for words of  and  bytes. The maximum memory throughput of the global memory can then be  bytes per transaction. GTX 240 contains two levels of texture caches; a  L1 cache within each TPC, partitioned in  caches and  L2 texture caches with a size of  each, visible to all SMs. The shared memory consists of  banks organized in such a way that successive -bit words are mapped into successive banks.\cite{CUDA_SDK} Each bank has a bandwidth of  bits over two clock cycles and therefore the bandwidth of the shared memory is  bytes over two cycles when all banks are accessed simultaneously. A request for shared memory addresses by a warp is split into two different requests, one for each half-warp.

\subsection{GPU Implementation}
\label{sec:GPU-Implementation}

The straightforward port of sequential applications to a GPU can often lead to significant speedups. The performance of the parallel implementations though can be improved even further when specific characteristics of the GPU architecture are taken into consideration. This section presents a basic data-parallel implementation strategy of the Aho-Corasick and Wu-Manber multiple pattern matching algorithms, analyzes the characteristics of the algorithm implementations that leverage the capabilities of the device, discusses the flaws that affect their performance and addresses them using different optimization techniques.
Table~\ref{tab:cuda_multi_notation} lists the notation that will be used for the rest of this section.

\begin{table}
\tbl{Implementation notation}
{\begin{tabular}{@{}ll@{}} \toprule
 & The number of thread blocks \\
 & The size in threads of each block \\
 & The unique ID of each thread of a block \\
 & The unique ID of each block\\
 & The size in characters of each chunk\\
 & The number of characters that each thread processes\\
 & The size in bytes of the shared memory per thread block\\ \botrule
\end{tabular}}
\label{tab:cuda_multi_notation}
\end{table}


\subsubsection{Parallelization strategy}
\label{sec:parallelizationstrategy}

To expose the parallelism of the multiple pattern matching algorithms, the following basic data-parallel implementation strategy was used. The preprocessing phase of the algorithms was performed sequentially on the host CPU. The input string and all preprocessing arrays were copied to the global memory of the device. The input string was subsequently partitioned into  character chunks, each with a size  of  characters. The chunks were then assigned to  thread blocks. Each chunk was further partitioned into  sub-chunks, that in turn were assigned to each of the  threads of a thread block. To ensure the correctness of the results,  overlapping characters were used per thread. Therefore, each thread processed  characters for a total of  additional characters. An integer array  with a size of  was used to store the number of matches per thread. To avoid extra coding complexity it is assumed that  is divisible by both  and . Since the character chunks have to overlap, the fewer possible thread blocks should be used to reduce the redundant characters as long as the maximum possible occupancy level is maintained per SM. 

Three tables were constructed during the preprocessing phase of the Aho-Corasick algorithm implementation. \textit{State\_transition} is a two-dimensional array where each row corresponds to a state of the trie, each column to a different character of the alphabet  and the cells of the array represent the next state. To ensure that alignment requirements are met on each row, \textit{state\_transition} was allocated as pitched linear device memory using the \textit{cudaMallocPitch()} function. \textit{State\_supply} and \textit{state\_final} are one-dimensional arrays, allocated using the \textit{cudaMalloc()} function. Each column of the arrays corresponds to a different state of the trie while the cells represent the supply state of a given state and the information whether that state is final or not respectively. Since the Aho-Corasick trie can have a maximum of  trie states, a value that for the experiments of this paper was typically equal to more than , each state was represented using a -byte integer. Algorithm~\ref{compl:cuda_AC_basic_implementation} presents the basic data-parallel strategy used for the implementation of the Aho-Corasick algorithm using the CUDA API. As can be seen from the pseudocode, there can be divergence among the execution paths of the threads when previous states are visited using the \textit{supply} function of the algorithm.



\begin{algorithm}[h]

\textbf{Algorithm} Basic Aho-Corasick\\

\\
\\
\\


\\

\For{  } {



		\While{  }{
			
			\\
		}
		
		\\
	
		\\
	
}

\caption{A basic parallel implementation of the Aho-Corasick algorithm}
\label{compl:cuda_AC_basic_implementation}
\end{algorithm}
 

The Wu-Manber algorithm uses the one-dimensional \textit{SHIFT} and the two-dimensional \textit{HASH} and \textit{PREFIX} tables. The space needed for the \textit{SHIFT} table is . Each row of the \textit{HASH} and \textit{PREFIX} represents a different pattern from the pattern set while the columns represent different hash values. In the worst case there could be  patterns with the same hash value  or  for their -character suffix or -character prefix respectively, therefore \textit{HASH} and \textit{PREFIX} require a  space each. The relevant data structures were copied to the global memory of the device with no modifications. The basic data-parallel strategy for the implementation of Wu-Manber is depicted in Algorithm~\ref{compl:cuda_WM_basic_implementation}.\\

\begin{algorithm}[h]

\textbf{Algorithm} Basic Wu-Manber\\

\\
\\
\\


\\

\While{ } {

	\\
	
	\uIf {  } {
		\\
	}
	\Else {
		
		\\
		
		\ForAll{ pattern indices  stored in  } {
		
			\If{  } {
		
				Verify the pattern corresponding to  directly against the input string\\
			}
		}
		
		\\
	}
}

\caption{A basic parallel implementation of the Wu-Manber algorithm}
\label{compl:cuda_WM_basic_implementation}
\end{algorithm}


\subsubsection{Implementation limitations and optimization techniques}
\label{subsec:implementation-limitations-optimizations}

As detailed in section \ref{sec:GPU-Architecture}, accesses to global memory for compute capability  GPUs by all threads of a half-warp are coalesced into a single memory transaction when all the requested words are within the same memory segment. The segment size is  bytes when -byte words are accessed,  bytes for -byte words and  bytes for words of  and  bytes. With the basic implementation strategy, each thread reads a single -byte character on each iteration of the search loop; in this case the memory segment has a size of  bytes. When , each thread accesses a word from a different memory segment of the global memory. This results to uncoalesced memory transactions, with one memory transaction for every access of a thread. Since the maximum memory throughput of the global memory is  bytes per transaction, the access pattern of the threads results in the utilization of only the  of the available bandwidth.

To work around the coalescing requirements of the global memory and increase the utilization of the memory bandwidth, it is important to change the memory access pattern by reading words from the same memory segment and subsequently store them in the shared memory of the device. This involves the partition of the input string into  chunks and the collective read of  characters from the global into the shared memory by all  threads of a thread block. For each  successive characters from the same segment then, only a single memory transaction will be used. This technique results in the utilization of the  of the global memory bandwidth, improved by a factor of . The threads can subsequently access the characters stored in shared memory in any order with a very low latency. Using the shared memory to increase the utilization of the memory bandwidth has two disadvantages. First, a total of  redundant characters are used that introduce significantly more work overhead when compared to the basic data-parallel implementation strategy. Second, using the shared memory effectively reduces the occupancy of the SMs. As the size of the shared memory for each SM of the GTX 240 GPU is , using the whole shared memory would reduce the occupancy to one thread block per SM. Partitioning the shared memory is not an efficient option since it would further increase the total work overhead.

The utilization of the global memory bandwidth can also increase when the threads read -byte words instead of single characters on every memory transaction. For that, the built-in  vector can be used, a C structure with members  and  that is derived from the basic integer type. This way, each thread accesses an -bit  word that corresponds to  characters of the input string with a single memory transaction while at the same time the memory segment size increases from  to  bytes. By having each thread read -bit  words from different memory segments results in the utilization of the  of the global memory bandwidth similar to the coalescing technique above. The input string array stored in global memory can be casted to  as follows:

\noindent\begin{minipage}{\textwidth}
\begin{lstlisting}
	
uint4 *uint4_text = reinterp_cast<uint4*>(d_text);

\end{lstlisting}
\end{minipage}\\

The two previous techniques can be combined; reading  successive -bit words or  bytes in the form of   vectors from global to shared memory can be done with just two memory transactions, fully utilizing the global memory bandwidth. The input string characters are then extracted from the uint4 vectors as retrieved from the global memory and are subsequently stored in shared memory on a character-by-character basis. To access the characters inside a uint4 vector, the vector can be recasted to :

\noindent\begin{minipage}{\textwidth}
\begin{lstlisting}	
uint4 uint4_var = uint4_text[i];

uchar4 uchar4_var0 = *reinterp_cast<uchar4*>(&uint4_var.x);
uchar4 uchar4_var1 = *reinterp_cast<uchar4*>(&uint4_var.y);
uchar4 uchar4_var2 = *reinterp_cast<uchar4*>(&uint4_var.z);
uchar4 uchar4_var3 = *reinterp_cast<uchar4*>(&uint4_var.w);
		
\end{lstlisting}
\end{minipage}\\

The drawback of casting input string characters to  vectors and recasting them to  vectors is that it can be expensive in terms of processing power.

The preprocessing arrays of the algorithm are relatively small in size while at the same time they are frequently accessed by the threads. Moreover, as a character-by-character verification of the patterns to the input string is required, the pattern set array is also often accessed. The performance of the parallel implementation of the algorithm should then benefit from the binding of the relevant arrays to the texture memory of the device. The texture reference was bound to the device memory using \textit{cudaBindTexture()} for one-dimensional arrays and \textit{cudaBindTexture2D()} for two-dimensional arrays allocated as pitched linear device memory. The textures were then accessed in-kernel using the \textit{tex1Dfetch()} and \textit{tex2D()} functions. Arrays accessed via textures not only take advantage of the texture caches to minimize the memory latency when cache hits occur but also bypass the coalescing requirements of the global memory. Moreover, the maximum size for an one-dimensional texture reference is  while the maximum size for two-dimensional texture references is  \textit{texels}.

\clearpage

The shared memory of the GTX 240 GPU consists of  memory banks numbered . The banks are organized in such a way that successive -bit words are mapped into successive banks with the  word being stored in bank . Bank conflicts occur when two or more threads of the same half-warp try to simultaneously access words  when . When the memory coalescence optimizations described above are used, it is challenging to avoid bank conflicts when the  characters of a  vector are successively stored to the shared memory and when are retrieved from shared memory by the threads of the same half-warp during the search phase. Storing the input string characters in shared memory results in a -way bank conflict. An alternative would be to cast each  vector to   vectors and store them in shared memory in a round-robin fashion:

\noindent\begin{minipage}{\textwidth}
\begin{lstlisting}
int tid16 = threadIdx.x 

if ( tid16 < 4 ) {
   uchar4_s_array[threadIdx.x * 4 + 0] = uchar4_var0;
   uchar4_s_array[threadIdx.x * 4 + 1] = uchar4_var1;
   uchar4_s_array[threadIdx.x * 4 + 2] = uchar4_var2;
   uchar4_s_array[threadIdx.x * 4 + 3] = uchar4_var3;
} else if ( tid16 < 8 ) {
   uchar4_s_array[threadIdx.x * 4 + 1] = uchar4_var1;
   uchar4_s_array[threadIdx.x * 4 + 2] = uchar4_var2;
   uchar4_s_array[threadIdx.x * 4 + 3] = uchar4_var3;
   uchar4_s_array[threadIdx.x * 4 + 0] = uchar4_var0;
} else if ( tid16 < 12 ) {
   uchar4_s_array[threadIdx.x * 4 + 2] = uchar4_var2;
   uchar4_s_array[threadIdx.x * 4 + 3] = uchar4_var3;
   uchar4_s_array[threadIdx.x * 4 + 0] = uchar4_var0;
   uchar4_s_array[threadIdx.x * 4 + 1] = uchar4_var1;
} else {
   uchar4_s_array[threadIdx.x * 4 + 3] = uchar4_var3;
   uchar4_s_array[threadIdx.x * 4 + 0] = uchar4_var0;
   uchar4_s_array[threadIdx.x * 4 + 1] = uchar4_var1;
   uchar4_s_array[threadIdx.x * 4 + 2] = uchar4_var2;
}		
\end{lstlisting}
\end{minipage}\\

This technique was not used since in practice the performance of the implementations did not improve. Although it was conflict-free when storing the vectors it resulted in a -way thread divergence that serialized accesses to shared memory, the same effect that the example code was trying to avoid. The modulo operator is very expensive when used inside CUDA kernels and had a significant impact in the performance of the implementations. Algorithms~\ref{compl:cuda_AC_optimized_implementation} and~\ref{compl:cuda_WM_optimized_implementation} depict the pseudocode of the optimized kernel of the Aho-Corasick and Wu-Manber algorithms respectively.  represents an array stored in the shared memory of the device with a size of  characters.

\begin{algorithm}[h]

\textbf{Algorithm} Optimized Aho-Corasick\\

\\
\\
\\
\\


\For {  } {

	\For {  } {
		\If {  } {
			read a \textit{uint4} vector from \\
			unpack the \textit{uint4} vector and store the  characters to \\
		}
	}
	Add  redundant characters to the end of the shared memory\\
	\textit{\_\_syncthreads()}\\
		
	\\
	
	\For {  \textbf{AND}  } {
		
			\While{  }{
			
				\\
			}
	
			\\
	}
	
	\textit{\_\_syncthreads()}\\
}
\\

\caption{An optimized parallel implementation of the Aho-Corasick algorithm}
\label{compl:cuda_AC_optimized_implementation}
\end{algorithm}

The , , , ,  and  arrays correspond to , , , ,  and  respectively when bound to the texture memory of the device. \\


\begin{algorithm}[h]

\textbf{Algorithm} Optimized Wu-Manber\\

\\
\\
\\
\\


\For {  } {

	\For {  } {
		\If {  } {
			read a \textit{uint4} vector from \\
			unpack the \textit{uint4} vector and store the  characters to \\
		}
	}
	Add  redundant characters to the end of the shared memory\\
	\textit{synchronize threads}\\

	\\

	\While{ } {

		\\
	
		\uIf {  } {
			\\
		}
		
		\Else {
	
			\\
		
			\ForAll{ pattern indices  stored in  } {
		
				\If{  } {
		
					Verify the pattern corresponding to  directly against the input string\\
				}
			}
		
			\\
		}
	}
}

\\

\caption{An optimized parallel implementation of the Wu-Manber algorithm}
\label{compl:cuda_WM_optimized_implementation}
\end{algorithm}

\clearpage

\subsection{MPI Architecture}
Message Passing Interface (MPI) is a standardized and portable message-passing system, developed by a group of researchers from academia and industry to function on a wide variety of parallel computers. The first steps were made in the beginning of the nineties, and version  of the interface was released in June 1994. MPI is considered the standard for High Performance Computing application development, on distributed memory architectures.\cite{Yang2011266}

MPI defines the semantics and the syntax of a core of library routines, useful to a wide range of developers writing portable message-passing programs. There are MPI bindings for many languages, including bindings for Fortran , C and C++. There are several implementations of the MPI interface, while many of them are totally free and are available to the public domain.

One of the most widely used implementations of MPI is MPICH.\cite{mpich} It supports efficiently different computation and communication platforms including commodity clusters (desktop systems, shared-memory systems, multicore architectures), high-speed networks, proprietary high-end computing systems (Blue Gene, Cray) and multiple operating systems (Windows, most flavors of UNIX including Linux and MacOSX).

MPI is commonly used to implement parallel applications for cluster systems, as it handles the communication and the synchronization between the nodes. However, passing data over the network is a time consuming operation, therefore applications should balance the communication time with the processing time. In order to achieve this balance, the ratio of computation to communication time must be maintained as high as possible.

\subsection{MPI Implementation}
The nodes of the cluster, on which the experiments were executed, were connected through a Gigabit switch. The dataset was shared among the nodes using the Network File System\cite{Callaghan2000} protocol (NFS). Hence, once it was copied to the file system it was subsequently available to all nodes, without the need to explicitly distribute it using the scatter operation of MPI.

The input string was split into chunks, each of them assigned to a different node of the cluster. The split was achieved by using two auxiliary variables, \textit{start} and \textit{stop}, indicating the beginning and the end of each biological sequence chunk. The chunk assignment took place using the value of the \textit{MPI\_Rank} function that returns the identification number of each node, the \textit{MPI\_Size} function that returns the total number of nodes available, the size  of the input string and the pattern size . The two auxiliary variables were calculated using the following formulas:



After the memory allocation of the strings, the CPU and the GPU functions were initiated on each node, returning the result count of each search. Finally, the count of the multiple pattern occurrences was gathered from each worker node with the master node calculating the total sum using the \textit{MPI\_Reduce} function of MPI.

\section{Experimental Methodology}
\label{sec:experimental-methodology}

The performance of the parallel implementations of the algorithms was evaluated by comparing the running time of the GPU and Distributed GPU implementations to that of the sequential version. The algorithm implementations were processed on a computer cluster with  homogeneous nodes, each with an Intel Core 2 Duo E8400 CPU, with two cores clocked at ,  L1 and  L2 Cache, and  of DDR2. Additionally, each node was equipped with a compute capability  NVIDIA GT 240 GPU, with  of GDDR3 global memory,  Graphics clock rate,  Processor clock tester rate and  memory clock rate. The cluster had a shared NFS disk space, while MPICH2 was used to handle all communication and synchronization operations, under Ubuntu Linux . The algorithm implementations were compiled using MPICH2 and CUDA , while the searching time of the sequential implementations was measured using the  function of the Message Passing Interface since it has a better resolution than the standard  function of \textit{time.h}. The searching time of the CUDA functions was measured using the CUDA event API.

For the experiments, a snapshot of the Expressed Sequence Tags (ESTs) from the February, 2009 assembly of the human genome was used, as produced by the Genome Reference Consortium and retrieved by the archives of the University of California Santa Cruz.\cite{ucsc2013} The snapshot, that was first converted to a one-dimensional input string and had any comments removed, consisted of  characters and had an alphabet of , the four nucleobases of the Deoxyribonucleic Acid (DNA).

The size of the data file was larger than the available size of \textit{global memory} of the GPU, thus it was impossible to load the input string on the device memory in one pass, and consequently to take advantage of the full computational power of the GPU. Host memory could be used, but by default, host memory allocations are pageable, while the GPU is unable to access data from pageable memory. Therefore, the data was allocated on pinned host memory and was then zero-copied to the GPU.

In order to simulate several demanding biological sequence searches, different multiple pattern sets were used. The sets were created from subsequences of the corresponding input string, consisting of ,  and  patterns, with each pattern having a size of  characters. The subsequences were chosen for at least  matches.

Finally, the source code that was used to run the experiments is available at: https://github.com/iassael/hybrid\_cuda\_aho\_wu, under GNU General Public License.

\section{Experimental Results}
\label{sec:experimental-results}

This section evaluates the performance of the Aho-Corasick and Wu-Manber algorithm implementations. As stated in section \ref{subsec:implementation-limitations-optimizations}, different optimization techniques were used for the implementation of the algorithms in order to take full advantage of the device hardware. Figure \ref{fig:acwm_GPU-optimizations} illustrates the execution times of each of the five GPU optimizations, running on a single node. For the execution, a set of  patterns was used, each with a pattern size of . Each implementation stage also incorporates the optimizations of the previous stages.

\begin{enumerate}
	
	\item The first stage of the implementation was unoptimized. The input string, the pattern set, the preprocessing arrays of the algorithms and the  array that holds the number of matches per thread, were stored in the global memory of the device. Each thread accessed input string characters directly from the global memory as needed and stored the number of matches directly to . At the end of the algorithms' search phase, the results array was transferred to the host memory and the total number of matches was calculated by the CPU. (GPU-U)
	
	\item The second stage of the implementation involved the binding of the preprocessing arrays and the pattern set array in the case of the Wu-Manber algorithm to the texture memory of the device. This optimization was not applied on the Aho-Corasick implementation as the preprocessing tables take more space than the supplied GPU texture memory for large pattern sets. (GPU-TC)
	
	\item In the third stage, the threads worked around the coalescing requirements of the global memory by collectively reading input string characters and storing them to the shared memory of the device. (GPU-CR)
	
	\item The fourth stage of the implementation was similar to the third but in addition, each thread read simultaneously  input string characters from the global memory by using a  vector. The characters were extracted from the vectors using  vectors and were subsequently stored to the shared memory. (GPU-R16)
	
	\item The fifth implementation stage involves the coalescing of the writes by the threads to the global memory of the device. (GPU-CW)
	
\end{enumerate}

As depicted in Figure \ref{fig:acwm_GPU-optimizations}, the final optimized parallel implementation of the Aho-Corasick and the Wu-Manber algorithms was x and x faster than their unoptimized version respectively. This indicates the significant performance increase that can be achieved when the implementations are customized to take advantage of the specific underlying hardware.

\begin{figure}[h]
\centering
\begin{tikzpicture}
	\begin{axis}[
		ybar,
		enlargelimits=0.15,		
		bar width=18pt,
		height=7.5cm,
		width=\textwidth,
		symbolic x coords={GPU-U, GPU-TC, GPU-CR, GPU-R16, GPU-CW},
		ylabel=Time (\textit{sec}),
              grid=both,
              nodes near coords,
		xtick=data
	]
	\addplot  +[
	black,
	fill=lightgray,
	] coordinates {
		(GPU-U,  95.5)
		(GPU-TC,  95.5)
		(GPU-CR,  62.1)
		(GPU-R16,  60.1)
		(GPU-CW,  57.3)
	};
	\addplot  +[
	black,
	fill=lightgray,
	postaction={
		pattern=north east lines
	}
	] coordinates {
		(GPU-U,  462.5)
		(GPU-TC,  467.7)
		(GPU-CR,  303.9)
		(GPU-R16,  302.9)
		(GPU-CW,  283.2)
	};
	\legend{AC\\WM\\}
	\end{axis}
\end{tikzpicture}
\caption{Aho-Corasick \& Wu-Manber GPU Optimizations}
\label{fig:acwm_GPU-optimizations}
\end{figure}



Figures \ref{fig:acwm_nodes1000} to \ref{fig:acwm_nodes16000} present the execution times of the searching phase of the parallel implementations for both the Aho-Corasick and Wu-Manber algorithms, including the time to distribute the data to the worker nodes of the cluster using the NFS protocol and the time to gather the results back to the master node using MPI. The pattern sets that were used, consisted of ,  and  patterns while at the same time,  to  cluster nodes were used.

\begin{figure}[h]
\centering
\begin{tikzpicture}
        \begin{axis}
	[
		height=7.5cm,
		width=\textwidth,
		grid=both,		
		xlabel=Number of processing nodes,
        	ylabel=Time (\textit{sec})
	]
	\addplot [color = gray, mark=triangle*] coordinates{
		(1, 26.08110621 ) (2, 13.0635242 ) (3, 9.679050247 ) (4, 6.522398536 ) (5, 5.289730038 ) (6, 4.375002936 ) (7, 3.768002724 ) (8, 3.285003587 ) (9, 3.078089243 ) (10, 2.637820736)
	};
	\addplot [color = black, mark=square*]  coordinates{
		(1, 38.795927 ) (2, 18.427936 ) (3, 12.827447 ) (4, 9.776997 ) (5, 7.893712 ) (6, 6.660501 ) (7, 5.776665 ) (8, 5.102328 ) (9, 4.61638 ) (10, 4.17171)
	};
	\addplot [color = darkgray, mark=diamond*]  coordinates{
		( 1 , 17.448042 )
( 2 , 12.46201 )
( 3 , 12.76181 )
( 4 , 11.3498 )
( 5 , 11.564684 )
( 6 , 10.714231 )
( 7 , 12.213895 )
( 8 , 11.790964 )
( 9 , 12.360423 )
( 10 , 14.63768 )
	};
            
	\legend{AC 1000 patterns\\WM 1000 patterns\\Network Read \& Gather\\}
        \end{axis}
\end{tikzpicture}
\caption{Aho-Corasick \& Wu-Manber Cluster's Nodes Used Comparison for  patterns}
\label{fig:acwm_nodes1000}
\end{figure}

\begin{figure}[h]
\centering
\begin{tikzpicture}
        \begin{axis}
	[
		height=7.5cm,
		width=\textwidth,
		grid=both,		
		xlabel=Number of processing nodes,
        	ylabel=Time (\textit{sec})
	]
	\addplot [color = gray, mark=triangle*] coordinates{
		( 1 , 55.59044781 ) ( 2 , 28.0574279 ) ( 3 , 18.58200313 ) ( 4 , 14.30901747 ) ( 5 , 11.27856157 ) ( 6 , 9.640428827 ) ( 7 , 8.130614679 ) ( 8 , 7.275712177 ) ( 9 , 6.515073469 ) ( 10 , 5.771265665 )
	};
	\addplot [color = black, mark=square*]  coordinates{
		(1, 283.156952 ) (2, 143.72614 ) (3, 96.301426 ) (4, 72.324056 ) (5, 57.709293 ) (6, 48.073995 ) (7, 41.292983 ) (8, 36.183308 ) (9, 32.162771 ) (10, 29.039478)
	};
	\addplot [color = darkgray, mark=diamond*]  coordinates{
		( 1 , 17.430959 )
( 2 , 12.308856 )
( 3 , 12.026645 )
( 4 , 11.384426 )
( 5 , 11.614029 )
( 6 , 10.704602 )
( 7 , 11.717291 )
( 8 , 11.666787 )
( 9 , 12.352483 )
( 10 , 14.565721 )
	};
            
	\legend{AC 8000 patterns\\WM 8000 patterns\\Network Read \& Gather\\}
        \end{axis}
\end{tikzpicture}
\caption{Aho-Corasick \& Wu-Manber Cluster's Nodes Used Comparison for  patterns}
\label{fig:acwm_nodes8000}
\end{figure}

\begin{figure}[h!]
\centering
\begin{tikzpicture}
        \begin{axis}
	[
		height=7.5cm,
		width=\textwidth,
		grid=both,		
		xlabel=Number of processing nodes,
        	ylabel=Time (\textit{sec})
	]
	\addplot [color = gray, mark=triangle*] coordinates{
		( 1 , 66.69782057 ) ( 2 , 33.73798385 ) ( 3 , 22.26686065 ) ( 4 , 17.38175948 ) ( 5 , 13.61955926 ) ( 6 , 11.73746916 ) ( 7 , 9.90550025 ) ( 8 , 8.95157835 ) ( 9 , 8.028906633 ) ( 10 , 7.107128268 )
	};
	\addplot [color = black, mark=square*]  coordinates{
		(1, 546.140773 ) (2, 275.746245 ) (3, 183.719287 ) (4, 138.484725 ) (5, 111.464913 ) (6, 91.972058 ) (7, 79.129625 ) (8, 69.206201 ) (9, 61.526889 ) (10, 55.257791)
	};
	\addplot [color = darkgray, mark=diamond*]  coordinates{
		( 1 , 17.422472 )
( 2 , 12.305245 )
( 3 , 12.026315 )
( 4 , 11.364924 )
( 5 , 11.53139 )
( 6 , 10.678401 )
( 7 , 11.700588 )
( 8 , 11.674269 )
( 9 , 12.306429 )
( 10 , 14.513983 )
	};
            
	\legend{AC 16000 patterns\\WM 16000 patterns\\Network Read \& Gather\\}
        \end{axis}
\end{tikzpicture}
\caption{Aho-Corasick \& Wu-Manber Cluster's Nodes Used Comparison for  patterns}
\label{fig:acwm_nodes16000}
\end{figure}

As presented in Figures \ref{fig:acwm_nodes1000} to \ref{fig:acwm_nodes16000}, parameters such as the number of the nodes used and the pattern set size affect significantly the execution time of the algorithm implementations. As the number of worker nodes increased, the total network communication time also increased. Moreover, for sets of  patterns, the network communication time was a large percentage of the total execution time. As can be observed in Figure \ref{fig:acwm_nodes1000}, this time overlaps the parallel searching time when using more than  nodes for Aho-Corasick and  or more nodes for Wu-Manber. Even for larger pattern sets, the network communication time is a significant proportion compared to the execution times of the implementations, especially in the case of the Aho-Corasick algorithm. As it can be seen in Figures \ref{fig:acwm_nodes8000} and \ref{fig:acwm_nodes16000}, the search execution time is approximately equal to the network communication time when using more than  nodes. On the other hand and for larger pattern sets, it consists a smaller proportion of the total execution time of the Wu-Manber algorithm, where the searching phase of the algorithm implementation is much more demanding.

It is noteworthy that for all the experiments, Aho-Corasick outperformed the Wu-Manber algorithm. For smaller pattern sets the difference was not significant, while for larger sets of patterns this difference became substantial. More specifically, for pattern sets of ,  and , Aho-Corasick was approximately ,  and  times faster than the Wu-Manber algorithm respectively.


\begin{table}[h]
\tbl{Comparison of execution times for  to  cluster nodes and for sets of  patterns }
{\begin{tabular}{@{}c |ccccc@{}} \toprule
Nodes & NFS & Load Files & AC GPU & WM GPU & Reduce\\
\colrule
1 & 5.04 & 12.40 & 26.08 & 38.80 & 0.00\\
2 & 6.10 & 6.36 & 13.06 & 18.43 & 0.00\\
3 & 7.85 & 4.91 & 9.68 & 12.83 & 0.00\\
4 & 7.51 & 3.13 & 6.52 & 9.78 & 0.71\\
5 & 7.07 & 2.50 & 5.29 & 7.89 & 2.00\\
6 & 6.09 & 2.14 & 4.38 & 6.66 & 2.49\\
7 & 6.99 & 2.31 & 3.77 & 5.78 & 2.91\\
8 & 6.96 & 1.68 & 3.29 & 5.10 & 3.15\\
9 & 7.56 & 1.45 & 3.08 & 4.62 & 3.36\\
10 & 8.91 & 1.37 & 2.64 & 4.17 & 4.35\\
\hline
\end{tabular}}
\begin{tabnote}
The displayed time values are in \textit{sec}, pattern size m=
\end{tabnote}
\label{tbl:comp-execution-1000}
\end{table}

Detailed average execution times of each part of the implemented algorithms, running on sets of , ,  patterns with a pattern size of , are given in Tables \ref{tbl:comp-execution-1000}, \ref{tbl:comp-execution-8000} and \ref{tbl:comp-execution-16000} respectively. These tables illustrate the difference between the proportion of the NFS file distribution time, the time to load the data and the time to reduce the results back to the master node using the MPI\_Reduce function, and the corresponding time to execute the searching phase of an algorithm for each different pattern set.

\begin{table}[h]
\tbl{Comparison of execution times for  to  cluster nodes and for sets of  patterns}
{\begin{tabular}{@{}c |ccccc@{}} \toprule
Nodes & NFS & Load Files & AC GPU & WM GPU & Reduce\\
\colrule
1 & 5.04 & 12.40 & 55.59 & 300.60 & 0.00\\
2 & 6.10 & 6.36 & 28.06 & 156.19 & 0.00\\
3 & 7.85 & 4.91 & 18.58 & 109.06 & 0.00\\
4 & 7.51 & 3.13 & 14.31 & 83.67 & 0.71\\
5 & 7.07 & 2.50 & 11.28 & 69.27 & 2.00\\
6 & 6.09 & 2.14 & 9.64 & 58.79 & 2.49\\
7 & 6.99 & 2.31 & 8.13 & 53.51 & 2.91\\
8 & 6.96 & 1.68 & 7.28 & 47.97 & 3.15\\
9 & 7.56 & 1.45 & 6.52 & 44.52 & 3.36\\
10 & 8.91 & 1.37 & 5.77 & 43.68 & 4.35\\
\hline
\end{tabular}}
\begin{tabnote}
The displayed time values are in \textit{sec}, pattern size m=
\end{tabnote}
\label{tbl:comp-execution-8000}
\end{table}


\begin{table}[h]
\tbl{Comparison of execution times for  to  cluster nodes and for sets of  patterns}
{\begin{tabular}{@{}c |ccccc@{}} \toprule
Nodes & NFS & Load Files & AC GPU & WM GPU & Reduce\\
\colrule
1 & 5.04 & 12.40 & 66.70 & 546.14 & 0.00\\
2 & 6.10 & 6.36 & 33.74 & 275.75 & 0.00\\
3 & 7.85 & 4.91 & 22.27 & 183.72 & 0.00\\
4 & 7.51 & 3.13 & 17.38 & 138.48 & 0.71\\
5 & 7.07 & 2.50 & 13.62 & 111.46 & 2.00\\
6 & 6.09 & 2.14 & 11.74 & 91.97 & 2.49\\
7 & 6.99 & 2.31 & 9.91 & 79.13 & 2.91\\
8 & 6.96 & 1.68 & 8.95 & 69.21 & 3.15\\
9 & 7.56 & 1.45 & 8.03 & 61.53 & 3.36\\
10 & 8.91 & 1.37 & 7.11 & 55.26 & 4.35\\
\hline
\end{tabular}}
\begin{tabnote}
Pattern size m=
\end{tabnote}
\label{tbl:comp-execution-16000}
\end{table}

The overall speedup of each of the GPU implementations, under the different pattern set sizes and running on multiple nodes is presented in Table \ref{tbl:overall-execution-speedup}. The speedup is calculated by the total execution time of the Aho-Corasick and Wu-Manber algorithms, as depicted in Tables \ref{tbl:comp-execution-1000} to \ref{tbl:comp-execution-16000}, compared to the corresponding single-node performance. It is worth noting, that for a pattern set size of  patterns, both algorithms exhibited a significant speedup with an upward trend when up to  nodes were used. For more than  nodes, the time of MPI\_Reduce had the tendency to increase proportionally to the number of nodes, affecting significantly the total execution time. Both algorithms exhibited a maximum speedup of  and  respectively, when they were executed on a cluster of  nodes. A similar trend was observed for pattern sets of , where the maximum speedup for both algorithms was exhibited using  cluster nodes, and was equal to  and  for AC and WM respectively. Finally, for pattern sets of , the speedup of the WM algorithm had a gradual increase by the number of cluster nodes, levelling at  when the algorithm was executed on all the available nodes. On the other hand, the AC algorithm had a maximum speedup of  when it was executed on  cluster nodes.





\begin{table}[h]
\tbl{Overall speedup for sets of ,  and  patterns}
{\begin{tabular}{@{}c | cc |cc|cc@{}} \toprule
Nodes & AC 1000 & WM 1000 & AC 8000 & WM 8000 & AC 16000 & WM 16000\\
\colrule
1 & 1.00x & 1.00x & 1.00x & 1.00x & 1.00x & 1.00x\\
2 & 1.71x & 1.82x & 1.80x & 1.89x & 1.82x & 1.96x\\
3 & 1.94x & 2.20x & 2.33x & 2.61x & 2.40x & 2.87x\\
4 & 2.44x & 2.66x & 2.85x & 3.35x & 2.93x & 3.76x\\
5 & 2.58x & 2.89x & 3.20x & 3.93x & 3.34x & 4.58x\\
6 & 2.88x & 3.24x & 3.59x & 4.58x & 3.75x & 5.49x\\
7 & 2.72x & 3.13x & 3.59x & 4.84x & 3.80x & 6.17x\\
8 & 2.89x & 3.33x & 3.83x & 5.32x & 4.06x & 6.96x\\
9 & 2.82x & 3.31x & 3.87x & 5.59x & 4.13x & 7.63x\\
10 & 2.52x & 2.99x & 3.58x & 5.45x & 3.87x & 8.06x\\
\hline
\end{tabular}}
\begin{tabnote}
The displayed time values are in \textit{sec}, pattern size m=
\end{tabnote}
\label{tbl:overall-execution-speedup}
\end{table}

 
 
 


\clearpage

\section{Conclusions}
\label{sec:conclusions}

This paper presented parallel implementations of two of the most well known multiple matching algorithms, Aho-Corasick and Wu-Manber, on a homogeneous cluster of nodes using the MPI and CUDA APIs. The performance of the algorithm implementations was evaluated when executed on an subsequence of the Expressed Sequence Tags of the human genome for different number of cluster nodes and pattern set sizes. The algorithm implementations were optimized in different steps in order to take advantage of the underlying GPU hardware. It was generally discussed that even low-end GPU cards could considerably facilitate demanding tasks, such as the processing of biological sequences. Finally, it was proven that a substantial performance increase can be achieved, when taking advantage of the combined power of the GPU nodes of a cluster.

The performance of the parallel algorithm implementations was evaluated over the corresponding time running on a single computer node, and was compared to the distributed implementation when executed on a GPU cluster of  nodes. 

Based on the results, it was determined that the searching time of the parallel algorithm implementations was between  and  over the sequential version for the Aho-Corasick algorithm, and between  and  for the Wu-Manber algorithm. It was also determined that the overall speedup for each algorithm was affected by the network communication time, proportionately to the number of nodes used. More specifically and for sets of  patterns, the overall speedup for both the Aho-Corasick and the Wu-Manber algorithm implementations increased when up to  cluster nodes were used, leveling at  and  respectively, with the maximum speedups of  and  being exhibited for  cluster nodes. For sets of  patterns, the maximum measured speedup was  for the Aho-Corasick algorithm implementation using  nodes, and  for the Wu-Manber algorithm implementation making use of all the available nodes of the cluster. Finally, for sets of  patterns, the equivalent speedups were  for Aho-Corasick using  nodes, and  for Wu-Manber using  nodes.

Although the Wu-Manber algorithm implementation exhibited a significant speedup in all cases, it was outperformed by the Aho-Corasick algorithm implementation for a pattern size of , especially in larger pattern sets. For a sequential execution of the algorithm implementations and for sets of ,  and  patterns, Aho-Corasick was ,  and  faster than Wu-Manber, and therefore it can be considered as a better choice for searching Biological Sequence Databases.

\bibliographystyle{ws-procs11x85}
\bibliography{cudampiwm}





\end{document}
