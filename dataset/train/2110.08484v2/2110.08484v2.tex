
\section{Results and Discussion}
In this section, we first discuss our main results on zero-shot and few-shot tasks and then answer the questions we raised: does prompt design matter in zero/few-shot learning?



\subsection{Experiment Details} 
For pre-training, we set batch size 1,280 and 800 for \method and \method, respectively and pre-train them with 30 epochs. 
We use learning rate 1e-4 with 5\% linear warmup.
For few-shot learning, we train models with 200 epochs, learning rate 5e-5 and 5\% linear warmup and choose the best checkpoint on the dev set.
For \method, we use ``question: \texttt{[Q]} answer \texttt{<text\_1>}'' (P3) as an input prompt and ``\texttt{<text\_1>} \texttt{[A]}'' as a target prompt for visual question answering, and ``an image of'' (Q3) as an input prompt for captioning, which show the best performance. 
We will study the effect of different prompts in Sec.~\ref{sec:exp:prompts}.
The sizes of of  and  are 16 on VQA and captioning tasks.
For miniImageNet, we use `This is \texttt{<text\_1>},'' and ``\texttt{<text\_1>} \texttt{[A]}'' as input and target prompts.
In this data, we test with \{1, 3, 5\}-shots per class. 


\subsection{Performance on Zero-shot Learning}
\label{sec:zero}
We evaluate the existing models in a zero-shot manner, in which models do not have access to any training data.
Tables~\ref{tab:zeroshotvqa} and \ref{tab:zeroshotcaption} show the results on VQA and captioning datasets, respectively.
First, \method\xspace with the hand-crafted prompt (P3) achieves better performance than other baselines on VQA datasets.
In particular, our \method significantly outperforms Frozen which is about 31 larger than ours.
Also, PICa based on GPT3~\cite{brown2020language} shows the best performance on OK-VQA. 
It is noticeable that our \method, the 246 smaller model, achieves the comparable result to PICa.
Compared to VL-T5 which is the same architecture as ours, \method improves VQAv2 performance by about 30\% point. 
As we will see in the later section, our pre-training objectives and the prompts boost the VQA performance. 
On NoCaps, SimVLM shows the best performance.
Our \method significantly improves the performance compared to VL-T5.
As we will see in the later section, our pre-training objectives and the prompts boost the VQA and captioning performance. 





\begin{table}[tb!]
\centering
\caption{\textbf{5-way miniImageNet results.} We evaluate \method\xspace in a generative manner. The shot represents the number of training examples per class.}
\resizebox{0.98\linewidth}{!}{
\begin{tabular}[t]{L{3cm}|C{1.5cm}|C{1.5cm}C{1.5cm}C{1.5cm}}
\toprule
    \textbf{Model} &\textbf{Model size}   &\textbf{1 shot }   &\textbf{3 shots}    &\textbf{5 shots}     \\
\midrule
Frozen   &7B & 14.5   & 34.7   & 33.8   \\
\midrule
\method (no prompt)  &224M   &48.0  &75.0    & 82.6   \\
\method  &224M   & 57.0   &78.0    & 84.2   \\
\method  &740M   & 57.1   & 78.3   & 84.4      \\
\midrule
AFHN  & - & 62.3   & -   & 78.1      \\
\bottomrule
\end{tabular}}
\label{tab:fewshotimage}
\end{table}

\subsection{Performance on Few-shot Learning}
\label{sec:few}




Tables~\ref{tab:fewshotvqa} and  \ref{tab:fewshotcaption} show the few-shot performance on VQA and captioning datasets.
Sizes of training and validation sets are 16 for \method, VL-T5, and Unified VLP; and Frozen and PICa use 4 and 16 in-context demonstration examples, respectively.


On VQAv2 and OK-VQA, PICa shows the best performance while our \method achieves the comparable result on VQAv2.
OK-VQA requires external knowledge to answer unlike other VQA datasets, so larger models and large pre-training data (prior knowledge) are necessary to improve.
Interestingly, \method, which is trained with 4 training examples, outperforms Frozen.
On captioning data, \method notably outperforms VL-T5 by 31.1\% point on NoCaps CIDEr.

Unified VLP slightly underperforms \method\xspace on Flickr30k captioning task.
We conjecture that their architecture is based on a encoder-decoder transfomer and it is pre-trained with a captioning task~\cite{zhou2020unified}.



\subsection{MiniImageNet}
Table~\ref{tab:fewshotimage} shows results on miniImageNet, where models must choose the correct class for each image.
We train and evaluate \method\xspace in an generative manner; the model must generate correct label text to get the credit.
\method\xspace significantly outperforms Frozen in all shots. Note that we train \method\xspace with a few training samples while Frozen uses them as in-context demonstration.
Interestingly, \method\xspace with a hand-crafted prompt improves performance a lot on the 1-shot case, while it marginally improves on the 5-shot case.













\begin{table}[tb!]
\centering
\caption{\textbf{Zero-shot results of hand-crafted prompts.} We test different input prompts in zero-shot predictions.
We use a CIDEr metric for Flickr30k.
Note that zero-shot setting does not require target prompts.
}
\resizebox{0.98\linewidth}{!}{
\begin{tabular}[t]{L{1.7cm}|C{1.8cm}C{1.7cm}C{1.7cm}C{1.7cm}}
\toprule
    &  \textbf{no prompt}  &  \textbf{P1}  &  \textbf{P2}  &  \textbf{P3}   \\
\midrule
\textbf{VQAv2} &3.7   &9.9   &19.0    &43.4    \\
\midrule
  &  \textbf{no prompt}  & \textbf{Q1}   & \textbf{Q2}  & \textbf{Q3}   \\
 \midrule
\textbf{Flickr30k} &9.6   &15.2   &25.6    &31.0 \\
\bottomrule
\end{tabular}}
\label{tab:promptvqa}
\end{table}






\subsection{Study of Prompt Design}
\label{sec:exp:prompts}



Here we examine the effect of different prompts on \method in Table~\ref{tab:promptvqa} and Figs.~\ref{fig:handpromptvqa}, \ref{fig:handpromptflickr}, and \ref{fig:prompt}.
We test the model on VQAv2 and Flickr30k datasets.



\subsubsection{Zero-shot Predictions}
Table~\ref{tab:promptvqa} shows the zero-shot performance on VQAv2 and Flickr30k.
We observe that zero-shot results are remarkably affected by input prompts on both datasets.
For input prompts, \texttt{<text\_1>} in P1 and P3 helps the zero-shot predictions significantly compared to ``no prompt'' and P2.
We conjecture that \texttt{<text\_1>} guides the model to predict masked spans similarly to MaskedLM, so it improves the performance.

On Flickr30k, we examine different word choices of prompts: ``a picture of'' (Q1), ``a photo of'' (Q2), and ``an image of'' (Q3).
For instance, using ``an image of'' outperforms using no prompt by 21.4 point.
It is noticeable that different word choices significantly affect the zero-shot results.

\begin{figure}[tb!]
    \centering
    \includegraphics[width=0.98\columnwidth]{figures/prompt.pdf}
    \caption{\textbf{VQAv2 results on noisy prompts.} We investigate different prompts on various training sizes. \method\xspace is trained with our best hand-crafted prompt (P3), irrelevant prompts, noisy tokens and random sentences. We list the prompt templates in Table~\ref{tab:prompts2} of appendix. We use ``\texttt{<text\_1>} \texttt{[A]}'' as our target prompt.}
    \label{fig:prompt}
\end{figure}


\begin{figure}[tb!]
    \centering
    \includegraphics[width=0.98\columnwidth]{figures/prompt_hand_flickr.pdf}
    \caption{\textbf{Flickr30k results on hand-crafted prompts.}   We investigate different hand-crafted prompts (Q1, Q2, and Q3) on various training sizes. 
}
    \label{fig:handpromptflickr}
\end{figure}


\begin{figure}[tb!]
    \centering
    \includegraphics[width=0.98\columnwidth]{figures/prompt_hand_vqa.pdf}
    \caption{\textbf{VQAv2 results on different target prompts.}  We investigate different target prompts with hand-crafted input prompts on various training sizes. 
}
    \label{fig:handpromptvqa}
\end{figure}


\subsubsection{Few-shot Predictions}
We study various input prompts including irrelevant prompts, noisy tokens, and random sentences on VQAv2 (Fig.~\ref{fig:prompt}).
First, noisy prompts and no prompt achieve near 0 accuracy on the zero-shot setting.
In few-shot predictions, \method\xspace with noisy prompts learns as quickly as hand-crafted prompts given larger data.
For example, our model with noisy prompts achieves comparable results to the best hand-crafted prompt.
Among all different types of noisy prompts, random sentences deteriorate performance the most.
This is because the random sentences come from captions in MS COCO, so the model might choose the answer from wrong captions not from images.
Interestingly, no prompt outperforms the other noisy prompts and even shows similar to or better than the hand-crafted prompt with larger training data.
We also observe a similar phenomenon on Flickr30k; no prompt performs similar to hand-crafted prompts in Fig.~\ref{fig:handpromptflickr}.

In addition, we explore two different target prompts, ``\texttt{<text\_1} \texttt{[A]}'' and ``\texttt{[A]}.''
We try to mimic the MaskedLM's target text format, so we add ``\texttt{<text\_1}'' to target prompt on VQA.
This might help the model's fast adaptation to a new task since they share the same target prompt.
In Fig.~\ref{fig:handpromptvqa}, we notice an interesting phenomenon; the target prompt ``\texttt{[A]}'' shows a larger variance than the other suggesting that introducing ``\texttt{<text\_1}'' helps the model quickly adapt to a new task.
However, both prompts show similar results given larger training data, e.g., 300.







































\begin{table}[tb!]
\centering
\caption{\textbf{Results on different pre-training objectives.} We test our pre-training objectives to investigate how it affects zero-shot and few-shot performance. We train \method with 16 training and validation examples.
}
\resizebox{0.98\linewidth}{!}{
\begin{tabular}[t]{L{4cm}|C{1.5cm}C{1.5cm}C{1.5cm}C{1.5cm}}
\toprule
    \textbf{Objective} &\textbf{VQAv2}     &\textbf{GQA}   &\textbf{Flickr30k CIDEr}\\
\midrule
\textbf{Zero-shot}  &   &   &   \\
MaskedLM   &42.4   &25.1   &4.6   \\
PrefixLM   &11.9   &6.7   &26.8  \\
MaskedLM + PrefixLM    &\textbf{43.4}   &\textbf{27.0}   &\textbf{31.0}   \\
\midrule
\textbf{Few-shot}  &   &   &   \\
MaskedLM   &46.0   &31.4   &18.5   \\
PrefixLM   &40.8   &27.6   &31.8  \\
MaskedLM + PrefixLM    &\textbf{48.2}   &\textbf{32.2}   &\textbf{32.6}   \\
\bottomrule
\end{tabular}}
\label{tab:obj}
\end{table}






\subsection{Pre-training Objectives}
\label{sec:exp:obj}



We investigate how pre-training objectives affect different tasks. 
We pre-train \method\xspace with different pre-training objectives: masked language modeling (MaskedLM) and prefix language modeling (PrefixLM).


In Table~\ref{tab:obj}, we observe that MaskedLM helps VQA tasks while PrefixLM helps captioning tasks in zero-shot and few-shot settings. 
We conjecture that MaskedLM is to predict spans, which is analogous to predict correct answers to questions, and PrefixLM is to generate the rest of the given prefix, which is similar to captioning tasks.
In other words, if the pre-training task is similar to the downstream tasks, then it will help performance further.
When pre-training with both objectives, they create a synergetic effect and thus improve cross-task generalization.






























