\chapter{Basic Concepts}
In this chapter we introduce the basic concepts and notation used through this thesis. Then, we present the data structures used to build our index. Finally, we present two self-indexes oriented to repetitive texts. All logarithms in this thesis will be in base 2 and we will assume \jeremy{that} $0\log 0 = 0$.
\section{Strings}
\label{sec:strings}
\begin{definition} A \emph{string} $T$ is a sequence of characters drawn from an alphabet $\Sigma$. The alphabet is an ordered and finite set of size $\left | \Sigma \right |=\sigma$. 
The $i$-th character of a string is represented as $T[i]$. The symbol $\varepsilon$ represents the empty string of length $0$.
\end{definition}

\begin{definition}
Given a string $T$, and positions $i$ and $j$, the \emph{substring} of $T$ starting at $i$ and ending at $j$ is defined as $T[i,j]=T[i]T[i+1]\ldots T[j]$. If $i>j$, then $T[i,j]=\varepsilon$.
\end{definition}

\begin{definition}
Let $T$ \diego{be} a string of length $n$. The \emph{prefixes} of $T$ are the strings $T[1,j], \forall\, 0\leq j \leq n$ and \djeremy{the}\jeremy{its} \emph{suffixes} \jeremy{are} the strings $T[i,n], \forall\, 1\leq i \leq n+1$.
\end{definition}

\begin{definition}
Let $T_1$, $T_2$ be strings of length $n_1$ and $n_2$\diego{, respectively}. We define the concatenation of these strings as  $T_1T_2=T_1[1]\ldots T_1[n_1]T_2[1]\ldots T_2[n_2]$.
\end{definition}

\begin{definition}
Given a string $T$ of length $n$, the reverse of $T$ is $T^{rev}=T[n]T[n-1]\ldots T[2]T[1]$.
\end{definition}
\begin{definition}
The \emph{lexicographic order} ($<$) between strings is defined as follows:
Let $a,b$ \diego{be} characters \diego{in $\Sigma$} and $X,Y$ \diego{be} strings \diego{over $\Sigma$}.
\begin{eqnarray*}
&\varepsilon < X, \, \forall X \neq \varepsilon& \\
&aX < bY\,\, \textit{if}\,\, a<b\, \vee \, ( a=b \wedge X<Y)&
\end{eqnarray*}
\end{definition}
\section{Search Problems}
\label{sec:queries}
\begin{definition}
Given a string $T$ and a pattern $P$ (a string of length $m$) both over an alphabet $\Sigma$, the \emph{occurrence positions} of $P$ in $T$ are defined as
$O = \lbrace 1 + |X|,\, \exists X,Y,\, T = XPY \rbrace$. 
\end{definition}

\begin{definition}
Given a string $T$ and a pattern $P$, the following search problems are of interest:
\begin{itemize}
\item $exists(P,T)$ returns true iff $P$ is in $T$, i.e., returns true iff $O\neq \emptyset$.
\item $count(P,T)$ counts the number of occurrences of $P$ in $T$, i.e., returns $occ = |O|$.
\item $locate(P,T)$  finds the occurrences \diego{of $P$ in $T$}, i.e., returns the set $O$ in some order.
\item $extract(T,l,r)$  extracts the text substring $T[l,r]$.\end{itemize}
\end{definition}

\begin{remark}
Note that $exists$ and $count$ can be answered after performing a $locate$ query.
\end{remark}
\section{Entropy}
\label{sec:entropy}
\begin{definition}
Let $T$ be a string of length $n$. The zero-th order empirical entropy is defined as
\begin{equation*}
H_0(T)= - \sum_{c\in \Sigma} \frac{n_c}{n} \log \frac{n_c}{n} 
\end{equation*}
where $n_c$ is the number of times the character $c$ appears in $T$, that is, $n_c/n$ is the empirical probability of appearance of character $c$. 
\end{definition}
It is worth noticing that the zero-th order entropy is invariant to permutations in the order of the text characters. The value $nH_0(T)$ is the least number of bits needed to represent $T$ using a compressor that gives each character a fixed encoding.

\begin{definition}
Let $T$ be a string of length $n$. The $k$-th order empirical entropy \cite{Man2001} is defined as
\begin{equation*}
H_k(T)= \sum_{S\in \Sigma^k} \frac{\left|T^S\right|}{n} H_0\left(T^S\right) 
\end{equation*}
where $T^S$ is the sequence composed of all characters preceded by string $S$ in $T$.
\end{definition}

The value $nH_k(T)$ is the least number of bits needed to represent $T$ using a compressor that encodes each character taking into account the $k$ preceding characters \diego{in $T$}. This value assumes the first $k$ characters are encoded for free, thus it gives a relevant lower bound only when $n\gg k$.

\djeremy{It can be proved that }$H_k$ is a decreasing function in $k$, that is, 
\begin{equation*}
0 \leq H_k(T) \leq H_{k-1}(T) \leq \ldots \leq H_1(T) \leq H_0(T) \leq \log \sigma.
\end{equation*}

The following lemma \jeremy{yields the ground to} show\djeremy{s} that the empirical entropy $H_k$ is not a good lower-bound measure for the compressibility of repetitive texts.
\begin{lemma}
\label{lemma:entropytt}
Let $T$ be a string of length $n$. For any $k\le n$ it holds $H_k(TT)\ge H_k(T)$. \end{lemma}
\begin{proof}
As new relevant contexts may have arisen in the concatenation $TT$, we denote by $C(T,k)$ the contexts of length $k$ present in $T$, and $C(TT,k)$ the contexts of $TT$. We have that $C(T,k) \subseteq C(TT,k)$. The number of new contexts in $TT$ is at most $k$.
For each $S \in C(T,k)$, we have $(TT)^{S} = T^{S}A^ST^{S}$, for some $A^S$ such that $|A^S|\leq k$. Then, 
\begin{eqnarray*}
H_k(TT)&=&\frac{1}{|TT|}\sum_{S\in C(TT,k)}|(TT)^S|H_0((TT)^S)\\
&\ge &\frac{1}{2|T|}\sum_{S\in C(T,k)}|T^SA^ST^S|H_0(T^SA^ST^S)\\
&\ge &\frac{1}{2|T|}\sum_{S\in C(T,k)}|T^ST^S|H_0(T^ST^S)\\
&=&\frac{1}{|T|}\sum_{S\in C(T,k)}|T^S|H_0(T^S)\\
&=&H_k(T).
\end{eqnarray*}
In the first step we used $C(T,k)\subseteq C(TT,k)$; in the second we used $|T|H_0(T)\le |TA|H_0(TA)$, for $T=T^ST^S$ and $A=A^S$ (since $|T^SA^ST^S| H_0(T^SA^ST^S) = $ \linebreak $|T^ST^SA^S| H_0(T^ST^SA^S)$); and in the third we used $H_0(TT)=H_0(T)$. The second property holds because
\begin{eqnarray*}
|TA|H_0(TA) & = & \sum_{c\in \Sigma} (n_c^T+n_c^A) \log \frac{n^T+n^A}{n_c^T+n_c^A} \\
& \ge & n^T \sum_{c\in \Sigma} \frac{n_c^T}{n^T} \log \frac{n^T+n^A}{n_c^T+n_c^A} \\
& \ge & n^T \sum_{c\in \Sigma} \frac{n_c^T}{n^T} \log \frac{n^T}{n_c^T}\\
& = & |T|H_0(T)
\end{eqnarray*}
where $n_c^X$ is the number of occurrences of character $c$ in string $X$, and $n^X=|X|$ for $X=T\, \text{or}\, A$. The last line is justified by the Gibbs inequality \cite{gibbs}.
\end{proof}
It follows that $|TT|H_k(TT)\ge2|T|H_k(T)$, that is, to encode $TT$ this model uses at least twice the space of the one used to encode $T$. An LZ77 encoding would need just one more phrase, as seen later.
\section{Encodings}
Most data structures need to represent symbols and numbers. Classic data structures use a fixed amount of space to store them, for example 1 byte for characters and 4 bytes for integers. Instead, compressed data structures aim to use the minimum possible space, thus they represent symbols using variable-length prefix-free codes or just using a fixed amount $b$ of bits, where $b$ is as small as possible. Table \ref{tab:coders} shows different encodings for the \ddiego{first 9} integers \diego{1,\ldots,9}, which we describe next.
\begin{description}
\item{\textbf{Unary Codes}} This representation is the simplest and serves as a basis for other coders. It represents a positive $n$ as $\texttt{1}^{n-1}\texttt{0}$, thus it uses exactly $n$ bits.
\item{\textbf{Gamma Codes}} It represents a positive $n$ by concatenating the length of its binary representation in unary and the binary representation of the symbol, omitting the most significant bit. The space is $2 \lfloor\log n\rfloor + 1$, $\lfloor\log n\rfloor + 1$ for the length and $\lfloor\log n\rfloor$ for the binary representation.   
\item{\textbf{Delta Codes}} This is an extension of $\gamma$-codes that works better on larger numbers. It represents the length of the binary representation of $n$ using $\gamma$-codes and then $n$ in binary without its most significant bit, thus using $\lbrace 2 \lfloor\log(\lfloor\log n\rfloor + 1)\rfloor + +1 \rbrace + \lfloor\log n\rfloor$ bits.
\item{\textbf{Vbyte Coding} \cite{vbyte}} It splits the $\lfloor \log(n + 1)\rfloor$ bits needed to represent $n$ into blocks of $b$ bits and stores each block in a chunk of $b + 1$ bits. The highest bit is 0 in the chunk holding the most significant bits of $n$, and 1 in the
rest of the chunks. For clarity we write the chunks from most to least significant, just like the binary representation of $n$. For example, if $n = 25 = 11001$ and $b = 3$, then we need two chunks and the representation is $0011 \cdot 1001$. Compared to an optimal encoding of $\lfloor \log(n + 1)\rfloor$ bits, this code loses one bit per $b$ bits of $n$, plus possibly an almost empty final chunk. Even when the best choice for $b$ is used, the total space achieved is still worse than $\delta$-encoding's performance. In exchange, Vbyte codes are very fast to decode.

\end{description}
\begin{table}
\centering
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Symbol & Unary Code & $\gamma$-Code & $\delta$-Code & Binary($b=4$)&Vbyte($b=2$)\\ \hline
\hline
1 & 0 & 0 & 0 & 0001 & 001\\ \hline
2 & 10 & 100 & 1000 & 0010 & 010\\ \hline
3 & 110 & 101 & 1001 & 0011 & 011\\ \hline
4 & 1110 & 11000 & 10100 & 0100 & 001100 \\ \hline
5 & 11110 & 11001 & 10101 & 0101 & 001101 \\ \hline
6 & 111110 & 11010 & 10110 & 0110 & 001110 \\ \hline
7 & 1111110 & 11011 & 10111 & 0111 & 001111\\ \hline
8 & 11111110 & 1110000 & 11000000 & 1000 & 001100100 \\ \hline
9 & 111111110 & 1110001 & 110000001 & 1001 & 001100101 \\ \hline
\end{tabular}
\caption{Example of different coders}
\label{tab:coders}
\end{table}
\subsection{Directly Addressable Codes}
\label{sec:dac}
\diego{In many cases} \ddiego{Many times} we need to store a set of numbers using the least possible space, yet providing fast random access to each element. Variable\diego{-}length codes complicate this task, as \ddiego{we need to store additional} \diego{they require storing, in addition,} pointers to sampled positions of the encoded sequence.

A simple solution that shows good performance in practice is the so\diego{-}called \nieves{\emph{Directly Addressable Codes} (DAC)} \ddiego{\emph{Reordered Vbytes}} \cite{DAC}, a variant of \emph{Vbytes} \cite{vbyte}.
They start with a sequence $C=C_1, \ldots, C_n$ of $n$ integers. Then they compute the Vbyte encoding of each number. The least significant blocks are stored contiguously in an array $A_1$, and the highest bits of the least significant chunks are stored in a bitmap $B_1$. The remaining chunks are organized in the same way in arrays $A_i$ and bitmaps $B_i$, storing contiguously the $i$-th chunks of the numbers that have them. Note that arrays $A_i$ store contiguously the bits $(i-1)\cdot b+1,\ldots,i\cdot b$ and bitmaps $B_i$ store whether a number has further chunks or not, hence the name \emph{Reordered Vbytes}.  


Figure \ref{fig:dac} shows an example of the \diego{resulting} structure. The first element is represented with two blocks, thus, $A_1[0] = C_{1,1}$, $A_2[0] = C_{1,2}$, $B_1[0] = \texttt{1}$ and $B_2[0] = \texttt{0}$.

\begin{figure}[ht]
\centering
\includegraphics[width=8cm]{img/basics/dac}
\caption{Example of Directly Addressable Codes structure}
\label{fig:dac}
\end{figure}



To access the element at position $i=i_1$ we \ddiego{see if} \diego{check whether} $B_1[i_1]$ is set. If it is not set, this is the last chunk and we already have the value $C[i]=A_1[i_1]$, otherwise we have to fetch the following chunks. In that case, we recompute the position as $i_2 = rank_{1}(B_1, i_1)$, where $rank_{1}(B_1,i_1)$ is the number of ones up to position $i_1$ on bitmap $B_1$ (see Section \ref{sec:bitmaps} for further details). If $B_2[i_2]$ is not set we are done with $C[i] = A_1[i_1] + A_2[i_2]\cdot2^b$, otherwise we set $i_3 = rank_1(B_2,i_2)$ and continue in the following levels. Accessing a random element takes $O(\log(M)/b)$ worst case time, where $M=\max C_i$. However, the access time is lower for elements with shorter codewords, which are usually the most frequent ones.

We will use the implementation of Susana Ladra\jeremy{\footnote{Universidade da Coruña, Spain. \texttt{sladra@udc.es}}} (available by personal request) in this thesis.
\section{Bitmaps}
\label{sec:bitmaps}
Let $B$ a binary sequence over $\Sigma=\lbrace\texttt{0},\texttt{1}\rbrace$ (\diego{a} bitmap) of length $n$ and assume it has $m$ ones. We are interested in solving the following operations:
\begin{itemize}
    \item \textbf{$rank_b(B,i)$:} How many $b$'s are up to position $i$ \jeremy{(included)}.
    \item \textbf{$select_b(B,i)$:} The position of the $i$-th $b$ bit.
\end{itemize}

\begin{figure}
\centering
\includegraphics[width=12cm]{img/basics/rankselect}
\caption{Example of rank and select}
\label{fig:rank_select}
\end{figure}
\begin{example}
\nieves{Figure \ref{fig:rank_select} shows an example of the operations rank and select. We show the values of both $rank_1(B,20)=11$ and $rank_0(B,20)=9$. Note that these two values add up to 20, since the former returns the number of ones up to position 20, and the latter the number of zeroes. Also, $access$ simply returns the bit stored at that position, in our case at position 20 there is a 1. Finally, we show the value of $select_1(B,11)=20$, which was expected since $access(B,20)=1$. The value of $select_0(B,9)$ is 19.}
\end{example}


Several solutions have been proposed to address this problem. The first solution able to solve both \diego{kinds of} queries in constant time uses $n+O(\frac{n\log\log n}{\log n})$ bits of space \cite{Cla96}. Raman, Raman and Rao\jeremy{'s solution} (RRR) \cite{RRR02} achieve\jeremy{s} $nH_0(B) + O(\frac{n\log\log n}{\log n})$ bits and answer the queries in constant time. Okanohara and Sadakane \cite{OS07} proposed several alternatives tailored to the case of small $m$ (sparse bitmaps): \texttt{esp}, \texttt{recrank}, \texttt{vcode}, and \texttt{sdarray}. Table \ref{tab:bitmap} shows the time and space complexities of these solutions. Note that the reported space\jeremy{s} include\djeremy{s} the representation of the bitmap.

\begin{table}
\begin{center}
\begin{tabular}{l|c|c|c}
Variant & Size & Rank & Select \\ \hline
Clark & $n + o(n)$ & $O(1)$ & $O(1)$ \\
RRR & $nH_0(B) + o(n)$ & $O(1)$ & $O(1)$ \\
esp & $nH_0(B) + o(n)$ & $O(1)$ & $O(1)$ \\
recrank & $1.44 m \log \frac{n}{m} + m + o(n)$ & $O\left(\log \frac{n}{m}\right)$ & $O\left(\log \frac{n}{m}\right)$\\
vcode & $m\log(n/\log^2 n)+o(n)$ & $O(\log^2 n)$ & $O(\log n)$  \\
sdarray & $m \log \frac{n}{m} + 2m + o(m)$& $O\left(\log \frac{n}{m} + \frac{\log^4 m}{\log n}\right)$ &  $O\left(\frac{\log^4 m}{\log n}\right)$ \end{tabular}
\caption{Complexities for binary rank and select}
\label{tab:bitmap}
\end{center}
\end{table}

\subsection{Practical Dense Bitmaps}
\label{sec:prac_bitmap}
The extra $o(n)$ space of theoretical solutions \cite{Cla96} is large in practice.
\djeremy{A solution with good results in practice and small space overhead (up to 5\%) is the one proposed by González \etal ~\cite{GGMNwea05}.}
\jeremy{González \etal ~\cite{GGMNwea05} proposed a solution with good results in practice and small space overhead (up to 5\%).}
This implementation is very simple, yet its practical performance is better than classical solutions. They store the plain bitmap in an array $B$ and have a table $R_s$ where they store $rank_1(B,i \cdot s)$, where $s=32k$, where $k$ is a parameter \jeremy{for the frequency of the sampling of the bit vector}. They use a function called \emph{popcount} that counts the number of \diego{\texttt{1} bits} \ddiego{ones} in \djeremy{an integer} \jeremy{a word} \jeremy{(4 bytes)}. \nieves{This operation can be solved bit by bit, but it is easy to improve it, using either bit parallelism or precomputed tables, requiring thus just a few operations.}
\nieves{They} solve the operations as follows ($rank_0$ and $select_0$ are obvious variations):


\begin{itemize}
\item $rank_1(B,i)$: They start in the last entry of $R_s$ that precedes $i$ ($R_s[\lfloor i/s \rfloor]$), and then sequentially scan the array $B$, popcounting \djeremy{in chunks of $w = 32$ bits}  \jeremy{consecutive words}, until reaching the desired position. The popcounting of the last word is done by first setting all bits after position $i$ to zero, which is done in constant time using a mask. Thus the time is $O(k)$. 
\item $select_1(B,i)$: They first binary search the $R_s$ table for the last position $p$ where $R_s[p]\le i$. Then they scan $B$ sequentially using \emph{popcount} looking for the word where the desired select position is. Finally they find the desired position in the word by \djeremy{first popcounting byte by byte and finally they} sequentially scan\jeremy{ning} the \djeremy{byte} \jeremy{word} bit by bit. Thus the time is $O(k+\log \frac{n}{k})$.
\end{itemize} 

We will use the implementation of Rodrigo González (available at \libcdsurl) in this thesis.

\subsection{Practical Sparse Bitmaps}
\label{sec:deltacodes}
When the bitmap is very sparse (i.e., the number of ones in the bitmap is very low) one practical solution is to $\delta$-encode the \ddiego{consecutive} distances between \diego{consecutive} ones.
Additionally we need to store absolute sample\djeremy{s} \jeremy{values} $select_1(B, i\cdot s)$ for a sampling step $s$, plus pointers to the corresponding positions in the $\delta$-encoded sequence. We solve the operations as follows:
\begin{itemize}
\item $select_1(B,i)$ is solved within $O(s)$ time by going to the last sampled position preceding $i$ and decoding the $\delta$-encoded sequence from there. 
\item $rank_1(B,i)$ is solved in time $O(s + \log \frac{m}{s})$. First, we binary search the samples looking for the last sampled position such that $select_1(B, \ell\cdot s) \le i$. Starting from that position we sequentially decode the bitmap and stop as soon as $select_1(B, p) \ge i$.
\item $access(B,i)$ is solved in time $O(s + \log \frac{m}{s})$ in a way similar to rank.
\end{itemize}
The space needed by the structure is $W + n/s(\lfloor\log m\rfloor +1 + \lfloor\log W \rfloor + 1)$, where $W$ is the number of bits needed to represent all the $\delta$-codes. In the worst case $W=2m \lfloor\log(\lfloor\log \frac{n}{m}\rfloor + 1)\rfloor + m\lfloor\log \frac{n}{m}\rfloor + m=m\log \frac{n}{m} + O(m\log\log \frac{n}{m})$.

This structure allows a space-time trade-off related to $s$ and also has the property that several operations cost $O(1)$ after solving others. For example, $select_1(B, p)$ and $select_1(B, p+1)$ cost $O(1)$ after solving $p \leftarrow rank_1(B, i)$.
\section{Wavelet Trees}
\label{sec:wavelettree}
A wavelet tree \cite{GGV03} is an elegant data structure that stores a sequence $S$ of $n$ symbols from an alphabet $\Sigma$ of size $\sigma$. 
This structure supports some basic queries and is easily extensible to support others.

We split the alphabet into two halves $L$ and $R$, so that the elements of $L$ are lexicographically smaller than those of $R$. Then, we create a bitmap $B$ of size $n$ setting $B[i]=\texttt{0}$ if the symbol at position $i$ belongs to $L$ and $B[i]=\texttt{1}$ otherwise.
This bitmap is stored at the root of the tree. \jeremy{Afterward}\djeremy{Then}, we extract from $S$ all symbols belonging to $L$, generating sequence $S_L$, and all symbols belonging to $R$, generating sequence $S_R$ (these sequences are not stored). \jeremy{Finally, we recursively generate}\djeremy{Then we generate recursively} the left subtree on $S_L$ and the right subtree on $S_R$. We continue until we get a sequence over a one-letter alphabet. \nieves{Figure \ref{fig:wavelettree} shows the wavelet tree for the example text \texttt{alabar\_a\_la\_alabarda}. Only the bitmaps (black color) are stored in the tree. The labels of the tree show (gray color) the subsets $L$ and $R$ and the strings over the bitmaps (gray color) show the conceptual subsequences $S_L$ and $S_R$.}

\begin{figure}[ht]
\begin{center}
\includegraphics{img/basics/wavelet_tree}
\end{center}
\caption[Example of a wavelet tree]{Example of a wavelet tree for the text \texttt{alabar\_a\_la\_alabarda}}
\label{fig:wavelettree}
\end{figure}

The resulting tree has $\sigma$ leaves, height $\lceil \log \sigma \rceil$, and $n$ bits per level. Thus the space occupancy is $n \log \sigma$ bits, plus $o(n\log \sigma)$ (more precisely, \djeremy{$O(\frac{n\log \sigma \log\log n}{\log n})=O(n\log\log \sigma)$} \jeremy{$O(\frac{n\log \sigma \log\log n}{\log n})$}) additional bits to support \emph{rank} and \emph{select} queries on the bitmaps. 

In the following we \dmine{will} explain how this structure supports the operations \emph{access}, \emph{rank} and \emph{select} on $S$. The last two operations are just a generalization for larger alphabets of those defined in Section \ref{sec:bitmaps}.

\begin{itemize}
\item \textbf{Access:} To retrieve the symbol $S[i]$ we look at $B[i]$ at the root. If it is a \texttt{0} we go to left subtree, otherwise to the right subtree. The new position is $i \leftarrow rank_0(B,i)$ if we go to the left and $i \leftarrow rank_1(B,i)$ if we go to the right. This procedure continues recursively until we reach \ddiego{the last level} \diego{a leaf}. The bits read in the path from the root to the leaf represent the symbol sought.

\item \textbf{Rank:} To count how many $c$'s are up to position $i$ we  go to the left if $c$ is in $L$ and otherwise to the right. The new position is $i \leftarrow rank_0(B,i)$ if we go to the left and $i \leftarrow rank_1(B,i)$ if we go to the right, where $B$ is the bitmap of the root. When we reach a leaf the answer is $i$.

\item \textbf{Select:} To find the $i$-th symbol $c$ we first go to the leaf corresponding to $c$ and then \ddiego{we} go upwards to the root. Let $B$ the bitmap of the parent. If the current node is a left child then the position at the parent is $i \leftarrow select_0(B,i)$, otherwise it is $i \leftarrow select_1(B,i)$. When we reach the root the answer is \diego{the current} $i$ \diego{value}.
\end{itemize}

The running time of these operations is $O(\log \sigma)$, \diego{since} \ddiego{given that} we use a bitmap supporting constant-time rank, select and access.

\begin{figure}
\centering
\includegraphics{img/basics/access}
\caption{Example of access in a wavelet tree}
\label{fig:wt_access}
\end{figure}
\begin{example}
\nieves{Figure \ref{fig:wt_access} shows an example of how we retrieve the 11th symbol of sequence $S$ ($access(S,11)=\texttt{a}$). First we access the bitmap of the root and see that at position 11 there is a 0. Hence we descend to the left. Then using $rank_0(B,11)=8$ we count how many zeroes are up to position 11. This value is our new position in the next level. Then we continue the process until we reach a leaf; in that case the symbol stored in that lead is the symbol sought, in our case an \texttt{`a'}.}
\end{example}

\begin{figure}
\centering
\includegraphics{img/basics/rank}
\caption{Example of rank in a wavelet tree}
\label{fig:wt_rank}
\end{figure}
\begin{example}
\nieves{Figure \ref{fig:wt_rank} shows step by step how we compute $rank_\texttt{l}(S,11)=2$. Since symbol \texttt{`l'} is mapped to a 1 we descend from the root to the right child. Using $rank_1(B,11)=3$ we count the number of ones up to that position. This is our new position in the next level. Then we continue the process until we reach a leaf. The value sought is the last value of rank, in our case 2.}
\end{example}

\begin{figure}
\centering
\includegraphics{img/basics/select}
\caption{Example of select in a wavelet tree}
\label{fig:wt_select}
\end{figure}
\begin{example}
\nieves{Figure \ref{fig:wt_select} shows an example of how to select the second \texttt{`b'} in the sequence $S$ ($select_\texttt{b}(S,2)$). First we descend to the leaf representing symbol \texttt{`b'}. Since that symbol was last mapped to a 1, we go to the parent and compute our new position as $select_1(B,2)=12$. In that level, \texttt{`b'} was mapped to a 0, so we go to the parent and the new position is $select_0(B,12)=16$, and that is the value sought.}
\end{example}

\subsection{Range Search}
\label{sec:range}
A direct application of wavelet trees is to answer range search queries \cite{MN07}. This method is very similar to the idea of Chazelle \cite{Cha88}.
\begin{definition}
Given a subset $R$ ($|R|=t$) of the discrete range \djeremy{$[1,n]\times[1,n]$} \jeremy{$[1,n]\times[1,\sigma]$}, a \emph{range query} returns the points $p \in R$ belonging to a range $[x_1,x_2]\times[y_1,y_2]$.
\end{definition}
\jeremy{An extension of the} \djeremy{The} wavelet tree supports range queries using $n+t\log n + o(n+t\log n)$ bits, counting the number of points \diego{within the range} in time $O(\log n)$ and reporting each occurrence in time $O(\log n)$. We will use a modified version of the implementation of Gonzalo Navarro\jeremy{\footnote{Check the LZ77-index source code (\indexurl) for the updated version}}.

We \djeremy{will} explain \jeremy{here} a simplified version \jeremy{for the case} in which there exists exactly one point for each value of $x$. We order the points of $R$ by their $x$ coordinate and create the sequence $S[1,n]$, such that for each $(x,y) \in R$, $S[x]=y$. Then we build the wavelet tree of $S$. 

\begin{figure}
\centering
\includegraphics[width=10cm]{img/basics/grid}
\caption{Example of 2-dimensional range query}
\label{fig:range_example}
\end{figure}

\begin{example}
Figure \ref{fig:range_example} shows a grid, with exactly one $y$ value for each value of $x$. The figure shows in yellow the range $[17,19]\times[9,18]$, containing two occurrences; in red and yellow, the range $[17,19]\times[1,21]$, containing 3 occurrences; and in green and yellow, the range $[1,21]\times[9,18]$, containing 10  occurrences.
\end{example}

\subsubsection{Projecting}
A range in $S$ represents a range along the $x$ coordinate and the splits made by the wavelet tree define ranges along the $y$ coordinate. Every time we descend to a child of a node we need to know where \ddiego{is} the range represented in that child \diego{is}. The operation of determining the range defined by a child, given the range of the parent, is called \emph{projecting}. Using rank we project a range downwards. Given a node with bitmap $B$ the left projection of $[x,x']$ is $[1+rank_0(B,x-1), rank_0(B,x')]$ and the right projection is $[1+rank_1(B,x-1), rank_1(B,x')]$. A range $[y,y']$ along the $y$ coordinate is projected to the left as $[y,\lfloor(y+y')/2\rfloor]$ and to the right as $[\lfloor(y+y')/2\rfloor +1,y']$.

\subsubsection{Counting}
We start from the root with the \diego{one-dimensional} \ddiego{linear} ranges $[x,x']=[x_1,x_2]$ and $[y,y']=[1,n]$ and project them in both subtrees. We do this recursively until:
\begin{enumerate}
    \item $[x,x']=\emptyset$;
    \item $[y,y'] \cap [y_1,y_2] = \emptyset$; or
    \item $[y,y'] \subseteq [y_1,y_2]$, in which case we add $x'-x+1$ to the total.
\end{enumerate}
As the interval $[y_1,y_2]$ is covered by $O(\log n)$ maximal wavelet tree nodes, the total time to count the occurrences is $O(\log n)$.

\begin{figure}
\centering
\includegraphics[width=10cm]{img/basics/2dimc}
\caption{Example of counting the occurrences in a 2-dimensional range query using a wavelet tree}
\label{fig:counting}
\end{figure}
\begin{example}
Figure \ref{fig:counting} shows the wavelet tree that represents the range of Figure \ref{fig:range_example}. The figure represents how to count the occurrences in the range $[17,19]\times[9,18]$. The figure shows in red how the range $[17,19]$ in the $x$ coordinate is projected downwards. The nodes below the blue line are those whose $y$ range is contained in the range $[9,18]$. Additionally, the nodes with a circle next to them are those in which the counting process ends. The blue circle represents rule number 1 (see above), the green one represents rule number 2 and finally rule number 3 is represented by the red circle. In our case, at each node marked with red, we report one occurrence, yielding a total of 2 occurrences. 
\end{example}
\subsubsection{Locating}
To locate the actual points we start from each node in which we were counting. If we want to know the $x$ coordinate we go up using select and if we want to know the $y$ coordinate we go down using rank. This operation takes $O(\log n)$ for each point \diego{located}.
\section{Permutations}
\label{sec:permutation}
A permutation is a bijection $\pi:[1,n]\rightarrow[1,n]$, and we are interested in computing efficiently both $\pi(i)$ and $\pi^{-1}(i)$ \jeremy{for any $1 \le i\le n$}. 
The permutation can be represented in a plain array using $n\log n$ bits, by storing $P=[\pi(1),\ldots,\pi(n)]$. This answers \djeremy{$\pi$}\jeremy{$\pi(i)$} in constant time. Solving \djeremy{$\pi^{-1}$}\jeremy{$\pi^{-1}(i)$} can be done by sequentially scanning $P$ for the position $j$ where $\pi(j)=i$. A more efficient solution \cite{MRRR03} is based on the cycles of a permutation. A cycle is a sequence $i,\pi(i),\pi^2(i), \ldots,\pi^k(i)$ such that $\pi^{k+1}(i)=i$. Every $i$ belongs to exactly one cycle. Then, to compute $\pi^{-1}$ we repeatedly apply $\pi$ over $i$, finding the element $e$ of the cycle such that $\pi(e)=i$. 
These solutions do not require any extra space to compute \djeremy{$\pi^{-1}$}\jeremy{$\pi^{-1}(i)$}, but they take $O(n)$ time in the worst case\dmine{ for computing $\pi^{-1}$}.

Representing the sequence $\pi[1,n]$ with a wavelet tree one \djeremy{could} \jeremy{can} answer both queries using $O(\log n)$ time and $n\log n + o(n \log n)$ bits of space. A faster solution \cite{MRRR03} is based on the cycles of the permutation. By introducing shortcuts in the cycles, it uses $(1 + \varepsilon)n \log n + O(n)$ bits and solves \djeremy{$\pi$}\jeremy{$\pi(i)$} in constant time and \djeremy{$\pi^{-1}$}\jeremy{$\pi^{-1}(i)$} in $O(1/\varepsilon)$ time, for any $\varepsilon>0$. 

We will use the implementation of \jeremy{Munro \etal's} \djeremy{permutations} \jeremy{shortcut technique} by Diego Arroyuelo\jeremy{\footnote{Yahoo! Research, Chile. \texttt{darroyue@dcc.uchile.cl}}}, available at \libcdsurl.
\section{Tree Representations}
\label{sec:dfuds}
A classical representation of a general tree of $n$ nodes requires $O(nw)$ bits of space, where $w \ge \log n$ is the bit length of a machine pointer. Typically only operations such as moving to the first child and to the next sibling, or to the $i$-th child, are supported in constant time. By further increasing the constant, some other simple operations are easily supported, such as moving to the parent, knowing the subtree size, or the depth of the node. However, the $\Omega(n \log n)$ bit space complexity is excessive in terms of information theory. The number of different general trees of $n$ nodes is $C_n \approx 4^n /n^{3/2}$, hence $\log C_n = 2n - \Theta(\log n)$ bits are sufficient to distinguish any one of them.

There are several succinct tree representations that use $2n+o(n)$ bits of space and answer most queries in constant time (see \jeremy{the review by Arroyuelo \etal} \cite{ACNSalenex10} for a detailed exposition); here we explain the DFUDS \cite{labeled_child} representation as this is the one that meets our requirements.

\begin{definition}
A sequence $S$ drawn from alphabet $\Sigma=\lbrace\texttt{0},\texttt{1}\rbrace$ is said to be \emph{balanced} if: (1) there are as many \texttt{0}s as \texttt{1}s and (2) at any position $i$ the number of zeroes to the left is greater or equal than the number of ones (i.e., $rank_\texttt{0}(S,i)\ge rank_\texttt{1}(S,i)$). Usually a balanced sequence is referred as \emph{balanced parentheses} by identifying \texttt{0} as `(' and \texttt{1} as `)', as the nesting of parentheses satisfies the above definition.
\end{definition}

The operations defined over a balanced sequence are: (1) \emph{findclose(S,i) (findopen(S,i))} finds the matching \texttt{1} (\texttt{0}) of the \texttt{0} (\texttt{1}) at position $i$, and (2) \emph{enclose(S,i)} is the position of tightest \texttt{0} enclosing node $i$.

\begin{definition}[\cite{labeled_child}]
The Depth-first unary degree sequence (DFUDS) is generated by a depth-first traversal of the tree, at each node appending the degree of the node in unary. Additionally a leading \texttt{1} is prepended to the sequence to make it balanced \jeremy{and allow the concatenation of several such encodings into a forest}. 
\end{definition}

The DFUDS sequence represents the topology of the tree using $2n$ bits. \jeremy{Tree nodes are identified in the DFUDS sequence according to their rank in the order given by the depth-first traversal (more precisely, the $i$-th node is identified by position $select_\texttt{1}(i)$ in the DFUDS encoding).}  \nieves{Figure \ref{fig:dfuds} shows the DFUDS bit-sequence for the example tree. The red 1 in the sequence is the preceding 1 added to make the sequence balanced. The green node is represented by the 10th 1 in the sequence, as it is the 10th node visited during a depth-first traversal. The blue sequence of five 1s and one 0 is the degree of the blue node.}

\begin{figure}[ht]
\begin{center}
\includegraphics{img/basics/dfuds}
\end{center}
\caption[Example of DFUDS representation]{Example of DFUDS representation}
\label{fig:dfuds}
\end{figure}

To solve the common operations over trees two data structures are built over the DFUDS sequence: a bitmap data structure supporting rank and select (Section \ref{sec:bitmaps}) and a data structure solving operations \emph{findclose}, \emph{findopen} and \emph{enclose} \cite{bp_jacobson,bp_munroraman,Nav09}. These structures allow one to compute the most common operations in constant time using $o(n)$ additional bits of space. Additionally, if we use labeled trees we need to store the labels of the edges in an array $chars$, using $n \log \sigma$ additional bits, where $\sigma$ is the labels' alphabet size. The label of the edge pointing to the $i$-th child of node $x$ is at \djeremy{$chars[rank_{\texttt{(}}(dfuds,x)+i]$} \jeremy{$chars[rank_{\texttt{1}}(dfuds,x)+i]$}. The operations we are interested in for this thesis are:
\begin{itemize}
\item \emph{degree($x$)}: number of children of node $x$.
\item \emph{isLeaf($x$)}: whether node $x$ is a leaf.
\item \emph{child($x$,$i$)}: $i$-th child of node $x$.
\item \emph{labeledChild($x$,$c$)}: child of node $x$ labeled by symbol $c$.
\item \emph{leftmostLeaf($x$)}: leftmost leaf of the subtree starting at node $x$.
\item \emph{rightmostLeaf($x$)}: rightmost leaf of the subtree starting at node $x$.
\item \emph{leafRank($x$)}: number of leaves to the left of node $x$. \item \emph{preorder($x$)}: preorder position of node $x$.
\end{itemize}
All these operations can be solved theoretically in constant time; however, in practice \emph{labeledChild} is solved by binary searching the \diego{labels of the} children\jeremy{, because it is much easier to implement and fast enough in practice}. To solve \emph{leftmostLeaf}, \emph{rightmostLeaf} and \emph{leafRank} we need to solve the queries $rank_{\texttt{00}}(i)$ and $select_{\texttt{00}}(i)$. $Rank_{\texttt{00}}(i)$ returns the number of occurrences of the substring \texttt{00} in the bitmap up to position $i$ and $select_{\texttt{00}}(i)$ returns the position $p$ of the $i$-th occurrence of the substring \texttt{00} in the bitmap. Solving these queries requires an additional data structure that uses $o(n)$ bits. It uses the same ideas as the one for solving rank and select for binary alphabets.

We will use a modified version of the implementation of Diego Arroyuelo available at \libcdsurl, adding support for leaf-related operations.
\section{Tries}
\label{sec:tries}
A \emph{trie} or digital tree is a data structure that stores a set of strings. It can find the elements of the set prefixed by a pattern in time proportional to the pattern length.
\begin{definition}
A \emph{trie} for a set $S$ of distinct strings is a tree where each node represents a distinct prefix in the set. The root node represents the empty
prefix $\varepsilon$. A node $v$ representing prefix $Y$ is a child of node $u$ representing prefix $X$ iff $Y = Xc$ for some character $c$, which labels the edge between $u$ and $v$.
\end{definition}
We suppose that all strings are ended by a special symbol \texttt{\$}, not present in the alphabet. We do this in order to ensure that no string $S_i$ is a prefix of some other string $S_j$. This property guarantees that the tree has exactly $|S|$ leaves. Figure \ref{fig:trie} shows an example of a trie.

\begin{figure}[ht]
\begin{center}
\includegraphics{img/basics/trie}
\end{center}
\caption[Example of a trie]{Example of a trie for the set $S=\lbrace\texttt{`alabar'}, \texttt{`a'}, \texttt{`la'}, \texttt{`alabarda'}\rbrace$.}
\label{fig:trie}
\end{figure}

A trie for the set $S=\lbrace S_1,\ldots,S_n\rbrace$ is easily built on $O(|S_1|+\ldots+|S_n|)$ time by successive insertions \diego{(assuming we can descend to any child in constant time)}. A pattern $P$ is searched for in the trie starting from the root and following the edges labeled with the characters of $P$. This takes a total time of $O(|P|)$.

A \emph{compact trie} is an alternative representation that reduces the space of the trie by collapsing unary nodes \ddiego{to} \diego{into} a single node and labeling the edge with the concatenation of all labels. A \emph{PATRICIA tree} \cite{patricia}, an alternative that uses even less space, just stores the first character of the label string and its length. This variant is used when the strings $S$ are \ddiego{separately available} \diego{available separately}, as not all information is stored in the edges. In this variant, after the search we need to check if the prefix found actually matches the pattern. For doing so, we have to extract the text corresponding to any string with the prefix found and compare it with the pattern. It they are equal then all leaves will be occurrences (i.e., strings prefixed with the pattern), otherwise none will be an occurrence. Figure \ref{fig:trie2} shows an example of this kind of trie.  

\begin{figure}[ht]
\begin{center}
\includegraphics{img/basics/trie2}
\end{center}
\caption[Example of a PATRICIA trie]{Example of a PATRICIA trie for the set $S=\lbrace\texttt{`alabar'}, \texttt{`a'}, \texttt{`la'}, \texttt{`alabarda'}\rbrace$. The values in parentheses are respectively the first character of the label and the length of the label.}
\label{fig:trie2}
\end{figure}


\begin{definition}
A \emph{suffix trie} is a trie composed of all the suffixes $T[i,n]$ of a given text $T[1,n]$. The leaves of the trie store the positions where the suffixes start.
\end{definition}

\section{Suffix Trees}
\label{sec:stree}
\begin{definition}[\cite{Wei73,McC76}]
A \emph{suffix tree} is a PATRICIA tree built over all the suffixes $T[i,n]$ of a given text $T[1,n]$. The leaves in the tree indicate the text positions where the corresponding suffixes start.
\end{definition}

Figure \ref{fig:sa} shows the suffix tree for the text \texttt{`alabar\_a\_la\_alabarda\$'}.
\begin{figure}[ht]
\begin{center}
\includegraphics{img/basics/suffixtree2}
\end{center}
\caption{\texorpdfstring{The suffix tree for the text \texttt{`alabar\_a\_la\_alabarda\$'}}{The suffix tree for the text 'alabar\_a\_la\_alabarda\$'}}
\label{fig:sa}
\end{figure}

The suffix tree can be built in $O(n)$ time using $O(n\log n)$ bits of space \cite{McC76,Ukk95}.

A suffix tree is able to find all the $occ$ occurrences of a pattern $P$ of length $m$ in time $O(m+occ)$, i.e., to solve the \emph{locate} query described in Section \ref{sec:queries}. After descending by the tree according to the characters of the pattern, we could be in three different cases: i) we reach a point in which there is no edge labeled with the current character of $P$, which means that the pattern does not occur in $T$; ii) we finish reading $P$ in an internal node (or in the middle of an edge), in which case the suffixes of the corresponding subtree are either all occurrences or none, therefore we only need to check if one of those suffixes matches the pattern $P$; iii) we end up in a leaf without consuming all the pattern, in which case at most one occurrence is found after checking the suffix with the pattern. As a subtree with $occ$ leaves has $O(occ)$ nodes\diego{,} the total time for reporting the occurrences is as stated above.

The suffix tree can solve the queries \emph{count} and \emph{exists} in $O(m)$ time. The process is similar to that of \emph{locate}. First we descend the tree according to the pattern. Then, we check if one of the suffixes of the subtree is a match. If it is a match the answer of \emph{count} is the number of leaves of the subtree \diego{(for which we need to store in each internal node the number of leaves that descend from it)}, otherwise it is zero.

\section{Suffix Arrays}
\label{sec:sarray}
\begin{definition}[\cite{MM93,sa_gonet}]
A \emph{suffix array} $A[1,n]$ is a permutation of the integer interval $[1,n]$, holding $T[A[i],n]<T[A[i+1],n]$ for all $1\le i < n$. In other words\diego{,} it is a permutation of the suffixes of the text such that the suffixes are lexicographically sorted.
\end{definition}

Figure \ref{fig:sarray} shows the suffix array for the text \texttt{`alabar\_a\_la\_alabarda\$'}. \jeremy{The character \texttt{\$} is the smallest one in lexicographical order.} The zone highlighted in gray represents those suffixes starting with \ddiego{an} \texttt{`a'}.
\begin{figure}[ht]
\begin{center}
\includegraphics{img/basics/suffixarray}
\end{center}
\caption{\texorpdfstring{The suffix array for the text \texttt{`alabar\_a\_la\_alabarda\$'}}{The suffix array for the text 'alabar\_a\_la\_alabarda\$'}}
\label{fig:sarray}
\end{figure}

Note that the suffix array could be computed by collecting the values at the leaves of the suffix tree. \ddiego{Several} \diego{However, several} methods exist that compute the suffix array in $O(n)$ or $O(n\log n)$ time\diego{,} using significantly less space. For a complete survey see \cite{sa_construction}.

The suffix array can solve \emph{locate} queries in $O(m\log n + occ)$ time, and \emph{count} and \emph{exists} queries in $O(m\log n)$ time. First, we search for the interval $A[sp_1, ep_1]$ of the suffixes starting with $P[1]$. This can be done via two binary searches on $A$. The first binary search determines the starting position $sp$ for the suffixes lexicographically larger than or equal to $P[1]$, and the second determines the ending position $ep$ for suffixes that start with $P[1]$. Then, we consider $P[2]$, narrowing the interval to $A[sp_2, ep_2]$, holding all suffixes starting with $P[1,2]$. This process continues until $P$ is fully consumed or the \ddiego{interval is empty} \diego{current interval becomes empty}. Note that this algorithm searches for the pattern from left to right. For each character of the pattern, we do two binary searches taking at most time $O(\log n)$, hence the total time is $O(m\log n)$. Then \emph{locate} reports all occurrences in $O(occ)$ time and the answer to \emph{count} is $ep_m-sp_m+1$. We can also directly search for the interval $A[sp,ep]$ where the suffixes start with the pattern $P$ using just two binary searches on $A$, which find the first  and last position where the suffixes start with $P$. Each comparison between the pattern and a suffix will take at most $O(m)$ time, hence the total running time is also $O(m\log n)$. Yet, this is faster in practice than the previous method and is what we use in this thesis.

\section{Backward Search}
\label{sec:bwt}
Backward search is an alternative method for finding the interval $[sp,ep]$ corresponding to a pattern $P$ in the suffix array. It searches for the pattern from right to left, and is based on the Burrows-Wheeler transform.

\begin{definition}[\cite{BW94}]
Given a text $T$ terminated with the special character $T[n]=\texttt{\$}$ smaller than all others, and its suffix array $A[1, n]$, the \emph{Burrows-Wheeler transform (BWT)} of $T$ is defined as $T^{bwt}[i] = T[A[i]-1]$, except when $A[i] = 1$, where $T^{bwt}[i]=T[n]$. In other words, the transformation is \jeremy{conceptually} built first by generating all the cyclic shifts of the text, then sorting them lexicographically, and finally taking the last character of each shift. \jeremy{In practice it can be built in linear time by building the suffix array first.}
\end{definition}
We can think of the sorted list of cyclic shift as a conceptual matrix $M[1,n][1,n]$. Figure \ref{fig:bwt} shows an example of how the BWT is computed for the text \linebreak
\texttt{`alabar\_a\_la\_alabarda\$'}. This transformation has the advantage of being easily compressed by local compressors \cite{Man2001}. It can be reversed as follows.
 
\begin{figure}[ht]
\begin{center}
\includegraphics[scale=1.2]{img/basics/bwt}
\end{center}
\caption{\texorpdfstring{The BWT of the text \texttt{`alabar\_a\_la\_alabarda\$'}}{The BWT of the text 'alabar\_a\_la\_alabarda\$'}}
\label{fig:bwt}
\end{figure}
\begin{definition}
The LF-mapping $LF(i)$ maps a position $i$ in the last column of $M$ ($L=T^{bwt}$) to its occurrence in the first column of $M$ ($F$).
\end{definition}

\begin{lemma}[\cite{FM05}]
\label{lemma:lfmap}
It holds
\begin{equation*}
LF(i) = C[c] + rank_{c}(T^{bwt},i)
\end{equation*}
where $c=T^{bwt}[i]$ and $C[c]$ is the number of symbols smaller than $c$ in $T$.
\end{lemma}

\begin{lemma}[\cite{BW94}]
The LF-mapping allows one to reverse the Burrows-Wheeler transform.
\end{lemma}
\begin{proof}
We know that $T[n]=\texttt{\$}$ and since $\texttt{\$}$ is the smallest symbol, $T[n]=F[1]=\texttt{\$}$ and thus $T[n-1]=L[1]=T^{bwt}[1]$. Using the LF-mapping we compute $i=LF(1)$; knowing that $T[n-1]$ is at $F[i]$, we have $T[n-2]=L[i]$, as $L[i]$ always precedes $F[i]$ in $T$. In general, it holds $T[n-k] = T^{bwt}[LF^{k-1}(1)]$.
\end{proof}

Given the close relation between the suffix array and the BWT, it is natural to expect that a search algorithm can work on top of the BWT. Such algorithm is called \emph{backward search} (BWS), and at each stage it narrows the interval $[sp_i,ep_i]$ of the suffix array in which the suffixes start with $P[i,m]$, starting from $i=m$ and ending with $i=1$. Narrowing the interval $A[sp,ep]$ with a new character $c$ is called a $BWS(sp,ep,c)$ step and it is done very similarly to the LF-mapping (Lemma \ref{lemma:lfmap}). BWS searches a pattern from right to left, opposite to the search on suffix arrays, that searches for a pattern from left to right. 

Figure \ref{alg:bws} shows the backward search algorithm. Lines 5-7 correspond to the BWS step. 
\begin{figure}[ht]
\renewcommand{\algorithmiccomment}[1]{/*#1*/}
\algsetup{linenodelimiter=,indent=0.8em}
\textbf{BWS}$(P)$
\begin{algorithmic}[1]
\STATE $i \leftarrow len(P)$
\STATE $sp \leftarrow 1$
\STATE $ep \leftarrow n$
\WHILE{$sp \le ep$ \textbf{and} $i \ge 1$}
    \STATE $c \leftarrow P[i]$
    \STATE $sp \leftarrow C[c] + rank_{c}(T^{bwt},sp-1)+1$ 
    \STATE $ep \leftarrow C[c] + rank_{c}(T^{bwt},ep)$ 
    \STATE $i \leftarrow i-1$
\ENDWHILE
\IF{$sp>ep$}
    \STATE \textbf{return} $\emptyset$
\ENDIF
\STATE \textbf{return} $(sp,ep)$
\end{algorithmic}
\caption{Backward Search algorithm (BWS)}
\label{alg:bws}
\end{figure}


\section{Lempel-Ziv Parsings and Repetitions}
\label{sec:lzparsings}
Lempel and Ziv proposed in the seventies a new compression method \cite{LZ76,ZL77,ZL78}. The basic idea is to replace a repeated portion of the text with a pointer to some previous occurrence of that portion. To find the repetitions they keep a dictionary representing all the portions that can be copied. Many variants of these algorithms exist \cite{LZSS,LZW,LZRW} which differ in the way they parse the text or the encoding they use.

The LZ77 \cite{ZL77} parsing is a dictionary-based compression scheme in which the dictionary used is the set of substrings of the preceding text. This definition allows it to get one of the best compression ratios for repetitive texts.

\begin{definition}[\cite{ZL77}]
\label{def:lz77}
The \emph{LZ77 parsing} of text $T[1,n]$ is a sequence $Z[1,n']$ of
\emph{phrases} such that $T = Z[1] Z[2] \ldots Z[n']$, built as follows. 
Assume we
have already processed $T[1,i-1]$ producing the sequence $Z[1,p-1]$. Then, we
find the longest prefix $T[i,i'-1]$ of $T[i,n]$ which occurs in
$T[1,i-1]$,\footnote{The original definition allows the source of
$T[i,i'-1]$ to extend beyond position $i-1$, but we ignore this feature in
this thesis.} set $Z[p] = T[i,i']$ and continue with $i = i'+1$. The
occurrence in $T[1,i-1]$ of the prefix $T[i,i'-1]$ is called the \emph{source} of 
the phrase $Z[p]$.
\end{definition}

Note that each phrase is composed of the content of a source, which can be the 
empty string $\varepsilon$, plus a trailing character. Note also that all 
phrases of the parsing are different, except possibly the last one. To avoid 
that case, a special character \texttt{\$} 
is appended at the end, $T[n]=\texttt{\$}$. 

Typically a phrase is represented as a triple
$Z[p]=(start,len,c)$, where $start$ is the start position of the source, $len$ is the length of the source and $c$ is the trailing character.

\begin{example}
\label{ex:lz77}
Let $T=\texttt{`alabar\_a\_la\_alabarda\$'}$; the LZ77 parsing is as follows:
\begin{equation*}
 \phrase{a}\phrase{l}\phrase{ab}\phrase{ar}\phrase{\_}\phrase{a\_}\phrase{la\_}\phrase{alabard}\phrase{a\$}
\end{equation*}
In this example the seventh phrase copies two characters starting at position 2 and has a trailing character `\texttt{\_}'.
\end{example}

One of the greatest  advantages  of this algorithm 
is the simple and fast scheme of decompression, opposed to the construction algorithm which is more complicated. Decompression runs in linear time by copying the source content referenced by each phrase and then the trailing character. However, random text extraction is not as easy.

The LZ78 \cite{ZL78} compression scheme is also dictionary-based. Its dictionary is the set of all phrases previously produced. Because of this definition of the dictionary the construction process is much simpler than that of LZ77.

\begin{definition}[\cite{ZL78}]
The {\em LZ78 parsing} of text $T[1,n]$ is a sequence $Z[1,n']$ of \emph{phrases} such that $T = Z[1] Z[2] \ldots Z[n']$, built as follows. 
Assume we have already processed $T[1,i-1]$ producing the sequence $Z[1,p-1]$. Then, we find the longest phrase $Z[j]$\diego{, for $j\leq p-1$,} that is a prefix of $T[i,n]$, set $Z[p] = Z[j]T[i+|Z[j]|]$ and continue with $i = i+|Z[j]|+1$.
\end{definition}

Typically a phrase is represented as $Z[p]=(j,c)$, where $j$ is the phrase number of the source and $c$ is the trailing character.

\begin{example}
\label{ex:lz78}
Let $T=\texttt{`alabar\_a\_la\_alabarda\$'}$; the LZ78 parsing is as follows:
\begin{equation*}
 \phrase{a}\phrase{l}\phrase{ab}\phrase{ar}\phrase{\_}\phrase{a\_}\phrase{la}\phrase{\_a}\phrase{lab}\phrase{ard}\phrase{a\$}
\end{equation*}
In this example the ninth phrase copies two characters starting at position 2 and has a trailing character `\texttt{b}'.
\end{example}


With respect to compression, both LZ77 and LZ78 converge to the entropy of stationary ergodic sources \cite{LZ76,ZL78}. It also converges below the empirical entropy (Section \ref{sec:entropy}), as detailed next.

\begin{definition}[\cite{KM99}]
A parsing algorithm is said to be {\em coarsely optimal} if its compression
ratio $\rho(T)$ differs from the $k$-th order empirical entropy $H_k(T)$ by a
quantity depending only on the length of the text and that goes to zero as the
length increases. That is, $\forall k \, \exists f_k, \lim_{n \rightarrow
\infty} f_k(n) = 0$, such that for every text $T$,
$
\rho(T) \le H_k(T)+f_k(|T|).
$
\end{definition}

\begin{theorem}[\cite{KM99,PWZ92}]
The LZ77 and LZ78 parsings are coarsely optimal.
\end{theorem}

As explained in Section \ref{sec:entropy}, however, converging to $H_k(T)$ is not sufficiently good for repetitive texts. Repetitive texts are originated in applications where many similar versions of one base text are generated (i.e., DNA sequences); or where successive versions, each one similar to the preceding one (i.e., wiki), are generated. Statistical compressors are not able to capture this characteristic, because they predict a symbol based only on a short previous context, and such statistics do not change when the text is replicated many times (see Section \ref{sec:entropy} for the relation between $H_k(T)$ and $H_k(TT)$). Compressors based on repetitions, such as Lempel-Ziv parsings or grammar based ones, do exploit this repetitiveness.
\section{Self-Indexes}
\begin{definition}
A \emph{self-index} \cite{NM07} is an index that uses space proportional to that of the compressed text and solves the queries \emph{locate} and \emph{extract}. As this kind of indexes can reproduce any text substring, they replace the original text. Additionally, some indexes provide more efficient ways of computing \emph{exists} and \emph{count} queries.
\end{definition}

There are several general-purpose self-indexes, however most of them \jeremy{do not achieve high compression for repetitive texts, as they are only able to compress up to the $k$-th order empirical entropy (Section \ref{sec:entropy}).}
\djeremy{are based on the $k$-th order empirical entropy model (Section \ref{sec:entropy}), thus they do not achieve high compression for repetitive texts.} 
Most are based on the BWT or suffix array (see \cite{NM07} for a complete survey).
In the last years some self-indexes oriented to repetitive texts have been proposed. We cover these now.

\subsection{Run-Length Compressed Suffix Arrays (RLCSAs)}
\label{sec:rlcsa}
The Run-Length Compressed Suffix Array (RLCSA) \cite{MNSV08} is based on the Compressed Suffix Array of Sadakane \cite{Sad03}. This is built around the so called $\Psi$ function. 
\begin{definition}[\cite{GV00}]
Let $A[1,\ldots,n]$ be the suffix array of a text $T$. Then $\Psi(i)$ is defined as 
\begin{equation*}
\Psi(i)=A^{-1}[(A[i]\!\!\!\!\mod n) + 1]
\end{equation*}
\end{definition}
The $\Psi$ function is the inverse of the LF mapping. $\Psi$ maps suffix $T[A[i],n]$ to suffix $T[A[i]+1,n]$, allowing one to scan the text from left to right. A \emph{run} in the $\Psi$ array is an interval $[a,b]$ for which it holds $\forall i \in [a,b-1],\,\Psi(i+1)=\Psi(i)+1$. 

In the RLCSA, \djeremy{they} \jeremy{one} run-length encodes the differences $\Psi[i]-\Psi[i-1]$ and store absolute samples of the array $\Psi$. This structure is very fast for \emph{count} and \emph{exists} queries. Its major drawback is the sampling it requires for \emph{locate} and \emph{extract} queries, as it takes $(n\log n)/s$ extra bits to achieve locating time $O(s)$, and time $O(s+r-l)$ for $extract(l,r)$, where $s$ is the sampling step.

The number of runs may be much smaller than $nH_k(T)$ (for example $runs(T)=runs(TT)$, whereas $|TT|H_k(TT)\ge 2|T|H_k(T)$ as shown in Section \ref{sec:entropy}). However, the difference between the number of runs and the number of phrases in an LZ77 parsing \cite{ZL77} may be a multiplicative factor as high as $\Theta(\sqrt{n})$.\footnote{Veli M\"{a}kinen, personal communication} For these reasons, the RLCSA seems to be an intermediate solution between LZ77 and empirical-entropy-based indexes. 

\subsection{Indexes based on sparse suffix arrays}
\label{sec:ku}
In this section we present two indexes \cite{KU96_sst,KU96} by K\"{a}rkk\"{a}inen and Ukkonen. Although these are not self-indexes, they set the ground for several self-indexes proposed later.

\begin{itemize}
\item First, they choose some indexing positions of the text. These can be evenly spaced points \cite{KU96_sst} or the points defined by a Lempel-Ziv parsing \cite{KU96}.
\item The suffixes starting at those points are indexed in a suffix trie, and the reversed prefixes in another trie.
\item The index in principle only allows one to find occurrences crossing an indexing point.
\item To find a pattern $P$ of length $m$, they partition it in all $m+1$ combinations of prefix and suffix.
\item For each partition, they search for the suffix in the suffix trie and for the prefix of the pattern in the reverse prefix trie.
\item The previous searches define a 2-dimensional range in a grid that relates each indexed text prefix (in lexicographic order) with the text suffix that follows (in lexicographic order). That is, related prefixes and suffixes are consecutive in the text.
\item A data structure supporting 2-dimensional range queries \cite{Cha88}, finds all pairs of related suffixes and prefixes, finding in this way the actual occurrences.
\item Additionally, using a Lempel-Ziv parsing they are able to find all the occurrences of the pattern. The occurrences are either found in the grid by the process described above (primary occurrences), or by considering the copies detected by the parsing (secondary occurrences), for which an additional method tracking the copies finds the remaining occurrences.
\end{itemize}

All following indexes can be thought as heirs of this general idea, which was improved by replacing or adding additional compact data structures to decrease the space usage. In most cases, the parsing was restricted only to LZ78 (Section \ref{sec:lz78_selfindexes}), since it simplifies the index, and in others to text grammars (SLPs, Section \ref{sec:slps}). In the following two subsections we list the results obtained in those cases. This thesis can also be thought as a heir of this fundamental scheme: For the first time compact data structures supporting the LZ77 parsing have been developed in this thesis, which show better performance on repetitive texts.

\subsection{LZ78-based Self-Indexes}
\label{sec:lz78_selfindexes}
In this section we present the space and running times of two indexes based on LZ78. Although they offer decent upper bounds and competitive performance on typical texts, experiments \cite{MNSV08} have demonstrated that LZ78 is too weak to profit from highly repetitive texts. There are other such self-indexes \cite{FM05}, not implemented as far as we know.
\subsubsection{Arroyuelo \etal's LZ-Index}
\label{sec:lz78index}
Navarro's LZ-Index \cite{Nav02} is the first self-index based on the LZ78 parsing \diego{using $O(nH_k(T))$ bits of space (it is also the first implemented in practice)}. It uses $4n'\log n'(1+o(1))$ bits and takes $O(m^3\log \sigma + (m+occ)\log n')$ time to \emph{locate} the $occ$ occurrences of a pattern of length $m$, where $\sigma$ is the size of the alphabet, and $n'$ is the number of phrases of the parsing.

Arroyuelo \diego{\etal} \ddiego{and Navarro} later improved the time and space of the index, achieving $(2+\epsilon)n'\log n'(1+o(1))$ bits and $O(m^2+(m+occ)\log n')$ locate time \ddiego{\cite{AN06}} \diego{\cite{ANSalgor10}}, or $(3+\epsilon)n'\log n'(1+o(1))$ bits and $O((m+occ)\log n')$ locate time \cite{AN07}.

\subsubsection{Russo and Oliveira's ILZI}
\label{sec:ilzi}
Russo and Oliveira present a self-index based on the so-called \emph{maximal parsing}, called ILZI \cite{ilzi}.
\begin{definition}[\cite{ilzi}]
Given a suffix trie $\mathcal{T}$ (of a set of strings), the \emph{$\mathcal{T}$-maximal parsing} of string $T$ is the sequence of nodes $v_1, \ldots , v_f$ such
that $T = v_1 \ldots v_f$ and, for every $j$, $v_j$ is the largest prefix of $v_j\ldots v_f$ that is a node of $T$.
\end{definition}

First, they compute the LZ78 parsing of $T^{rev}$, and then generate a suffix tree $\mathcal{T}_{78}$ over the set of the reverse phrases. Next they build the maximal parsing of $T$ using $\mathcal{T}_{78}$. \djeremy{They use this} \jeremy{This} parsing \djeremy{as it} improves the compression of LZ78, as shown by the following lemma. 

\begin{lemma}[\cite{ilzi}]
If the number of \ddiego{blocks} \diego{phrases} of the LZ78 parsing of $T$ is $n'$ then the $\mathcal{T}_{78}$-maximal parsing of $T$ has at most $n'$ \ddiego{blocks} \diego{phrases}.
\end{lemma}

Their index uses at most $5n'\log n'(1+o(1))$ bits and takes $O((m+occ)\log n')$ time to locate the $occ$ occurrences of a pattern of length $m$ ($n'$ is the number of blocks of the maximal parsing).
\subsection{Straight Line Programs (SLPs)}
\label{sec:slps}
Claude and Navarro \cite{CN09} propose\jeremy{d} a self-index based on \emph{straight-line programs} (SLPs). SLPs are grammars in which the rules are either $X_i \rightarrow \alpha \in \Sigma$ or $X_i \rightarrow X_lX_r, \, \text{for}\,\, l,r<i$. The LZ78 \cite{ZL78} parsing may produce an output exponentially larger than the smallest SLP. However, the LZ77 \cite{ZL77} parsing outperforms the smallest SLP \cite{CLLP+05}. On the other hand producing the smallest SLP is an NP-complete problem \cite{Rytter03,CLLP+05}. However, Rytter \cite{Rytter03} has shown how to generate in linear time a grammar using $O(\ell \log \ell)$ rules and height $O(\log \ell)$, where $\ell$ is the size of the LZ77 parsing. Again, SLPs are intermediate between LZ77 and other methods.

The index \cite{CN09} uses $n'\log n + O(n'\log n')$ bits of space, where $n'$ is the number of rules of the grammar. It solves $extract(l,r)$ in $O((r-l + h) \log n')$ time and \emph{locate} in $O((m(m + h) + h \cdot occ) \log n')$ time, where $h$ is the height of the derivation tree of the grammar and $m$ the length of the pattern.

Claude \etal ~\cite{CFMPNbibe10} evaluate\jeremy{d} a practical implementation using the grammar produced by Re-Pair \cite{LM00}. The results are competitive with the RLCSA only for extremely repetitive texts and short patterns.
