

\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} \usepackage{footnote}

\usepackage{hyperref}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{xspace}
\usepackage{array,multirow}


\renewcommand{\baselinestretch}{0.98}


\sloppy
\frenchspacing

\usepackage{xargs} \newcommand{\ent}{\ensuremath{e}}
\newcommand{\rel}{\ensuremath{r}}
\newcommand{\pth}{\ensuremath{p}}
\newcommand{\stpath}{\ensuremath{\textsf{st}(\pth)}}
\newcommand{\enpath}{\ensuremath{\textsf{en}(\pth)}}
\newcommand{\lenpath}{\ensuremath{\textsf{len}(\pth)}}
\newcommand{\typepath}{\ensuremath{\textsf{type}(\pth)}}

\newcommand{\swag}{\textsc{Swag}\xspace}
\newcommand{\hswag}{\textsc{HellaSwag}\xspace}
\newcommand{\bert}{\textsc{Bert}\xspace}
\newcommand{\roberta}{\textsc{Roberta}\xspace}
\newcommand{\bigb}{\textsc{BigBird}\xspace}
\newcommand{\spql}{\textsc{Sparql}\xspace}
\newcommand{\hitl}{\textsc{H-I-T-L}\xspace}
\newcommand{\webqsp}{\textsc{WebQuestionsSP}\xspace}
\newcommand{\cwq}{\textsc{ComplexWebQuestions}\xspace}
\newcommand{\cfq}{\textsc{CFQ}\xspace}
\newcommand{\grailqa}{\textsc{GrailQA}\xspace}
\newcommand{\fbqa}{\textsc{FreebaseQA}\xspace}
\newcommand{\gpt}{\textsc{Gpt-2}\xspace}
\newcommand{\ff}{\textsc{Family-Feud}\xspace}
\newcommand{\cbr}{\textsc{CBR}\xspace}
\newcommand{\fb}{FB122\xspace}
\newcommand{\nell}{NELL-995\xspace}
\newcommand{\wn}{WN18RR\xspace}
\newcommand{\grinch}{\textsc{Grinch}\xspace}
\newcommand{\alg}{\textsc{Cbr-kbqa}\xspace}

\usepackage{url}
\usepackage{ragged2e}
\newcommand{\ignore}[1]{}
\newcommand{\ensuretext}[1]{#1}
\newcommand{\clabcomment}[3]{\ensuretext{\textcolor{#3}{[#1 #2]}}}
\newcommand{\cjdmarker}{\ensuretext{\textcolor{green}{\ensuremath{^{\textsc{cj}}_{\textsc{d}}}}}}
\newcommand{\cjd}[1]{\clabcomment{\cjdmarker}{#1}{green}}
\newcommand{\raj}[1]{\clabcomment{Raj}{#1}{magenta}}
\newcommand{\nick}[1]{\clabcomment{Nick}{#1}{red}}
\newcommand{\ameya}[1]{\clabcomment{Ameya}{#1}{blue}}
\newcommand{\jy}[1]{\clabcomment{JY}{#1}{teal}}
\newcommand{\june}[1]{\clabcomment{June}{#1}{purple}}
 




\usepackage[accepted]{icml2021}

\icmltitlerunning{Case-based Reasoning for Natural Language Queries over Knowledge Bases}

\begin{document}

\twocolumn[
\icmltitle{Case-based Reasoning for Natural Language Queries over Knowledge Bases}





\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Rajarshi Das}{umass}
\icmlauthor{Manzil Zaheer}{goog}
\icmlauthor{Dung Thai}{umass}
\icmlauthor{Ameya Godbole}{umass}
\icmlauthor{Ethan Perez}{nyu}
\icmlauthor{Jay-Yoon Lee}{umass}
\icmlauthor{Lizhen Tan}{amz}
\icmlauthor{Lazaros Polymenakos}{amz}
\icmlauthor{Andrew McCallum}{umass}
\end{icmlauthorlist}

\icmlaffiliation{umass}{University of Massachusetts Amherst}
\icmlaffiliation{goog}{Google Research}
\icmlaffiliation{nyu}{New York University}
\icmlaffiliation{amz}{Amazon}

\icmlcorrespondingauthor{Rajarshi Das}{rajarshi@cs.umass.edu}
\icmlcorrespondingauthor{Ameya Godbole}{agodbole@cs.umass.edu}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]





\printAffiliationsAndNotice{}  

\begin{abstract}








It is often challenging for a system to solve a new complex problem from scratch, but much easier if the system can access other similar problems and description of their solutions --- a paradigm known as case-based reasoning (CBR).
We propose a neuro-symbolic CBR approach for question answering over large knowledge bases (\alg). While the idea of CBR is tempting, composing a solution from cases is nontrivial, when individual cases only contain partial logic to the full solution. To resolve this, \alg consists of two modules: a non-parametric memory that stores cases (question and logical forms) and a parametric model which can generate logical forms by retrieving relevant cases from memory. Through experiments, we show that \alg can effectively derive novel combination of relations not presented in case memory that is required to answer compositional questions. On several KBQA datasets that test compositional generalization, \alg achieves competitive performance. For example, on the challenging \cwq dataset, \alg outperforms the current state of the art by 11\% accuracy. Furthermore, we show that \alg is capable of using new cases \emph{without} any further training. Just by incorporating few human-labeled examples in the non-parametric case memory, \alg is able to successfully generate queries containing unseen KB relations.










\end{abstract}


\section{Introduction}
\label{sec:intro}
Humans often solve a new problem by recollecting and adapting the solution to multiple related problems that they have encountered in the past \cite{ross1984remindings,lancaster1987problem,schmidt1990cognitive}. In classical artificial intelligence (AI), case-based reasoning (CBR) pioneered by \citet{schank1982dynamic}, tries to incorporate such model of reasoning in AI systems \cite{kolodner1983maintaining,rissland1983examples,leake1996cbr}. A sketch of a CBR system \cite{aamodt1994case} comprises of the following modules ---  (i) a retrieval module, in which `cases' that are similar to the given problem are retrieved, (ii) a reuse module, in which the solutions of the retrieved cases are re-used to synthesize a new solution. Often, the newly derived solution can not be used directly (e.g. because of domain mismatch) and hence needs (iii) revision. Finally, successful solutions are (iv) retained in a case memory, to be reused later. 
 

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{images/fig1.pdf}
    \vspace{-5mm}
    \caption{Our CBR approach derives the logical form for a new query from the solutions of other retrieved queries. Here, the retrieved queries are about different entities but share relational similarity. \alg is capable of composing relations from different queries to answer a new query. }
    \label{fig:intro1}
    \vspace{-3mm}
\end{figure}


In its early days, the components of CBR were implemented with symbolic systems, which had their limitations.For example, finding similar cases and generating new solutions from them is a challenging task for a CBR system implemented with symbolic components. However, with the recent advancements in representation learning \cite{lecun2015deep}, the performance of ML systems have improved substantially on a range of practical tasks.  

Large symbolic KBs form the underlying data store for a wide variety of applications. Therefore, the ability to query information stored in the KB via a natural language interface is of practical importance. Current approaches for KBQA \cite{berant2013semantic,sun2019pullnet} do not leverage the power of CBR. Combined with the power of representations obtained from large pre-trained language models \citep[inter-alia]{devlin2018bert,liu2019roberta,t5}, we present a neuro-symbolic CBR approach for answering complex questions over a large symbolic KB (Figure~\ref{fig:intro1}).



Given a NL query, our model (\alg) first retrieves other similar cases (queries and its logical forms) from a case memory (e.g. training set). Next, \alg synthesizes a logical form for the given query by learning to reuse various components of the solutions of the retrieved cases (Figure~\ref{fig:intro1}). The newly generated logical form is then executed against the KB. However, often the generated logical form does not execute successfully. This can happen because one or more KB relations needed are never present in the retrieved cases (because of lack of good nearest neighbors) or because of the incompleteness of the knowledge graph (KG) \cite{min2013distant}.

To alleviate such cases, \alg has an additional revise step that \emph{aligns} the generated relations in the logical form to the query entities' local neighborhood in the KG. To achieve this, we take advantage of pre-trained relation embeddings from KB completion techniques (e.g. Trans-E \cite{bordes2013translating}) that learn the structure of the KG. We also experiment with another alignment model which takes advantage of the fact that similar KB relation often have overlapping terms in its surface form (e.g. \textsf{person.siblings} and \textsf{fictional\_character.siblings}) and hence the corresponding token embeddings are similar. 


\begin{figure}
    \centering
    \vspace{-3mm}
    \includegraphics[width=\columnwidth]{images/HITL_Experiment.pdf}
    \vspace{-5mm}
    \caption{Human-in-the-loop setting in which an expert point-fixes a model prediction on a real example by adding a simple case (query + LF containing only one relation) to the KNN index. The model got it wrong initially because no query with the relation (educational\_institution.colors) was present in the train set. To handle this unseen relation, an expert adds a relevant simple case to the full KNN-index and \alg retrieves this case and fixes the erroneous prediction without requiring any re-training.}
    \label{fig:hitl_fig}
    \vspace{-3mm}
\end{figure}

It has been shown black-box models do not generalize to questions requiring novel compositions especially if they are not seen at training time \cite{lake_baroni,loula2018rearranging}. We show that \alg is effective for complex compositional questions and we obtain new state-of-the art results on the challenging CWQ dataset \cite{Talmor2018TheWA} outperforming baselines by over 11\% points.


 It is important for ML systems to be \emph{debuggable} --- i.e. it should be easy to gain insights about what went wrong when a model outputs a erroneous prediction for a given input. Current models for question answering (QA) built on top of massive pre-trained language models (LMs)  \cite{liu2019roberta,t5} often function as black-boxes, offering very little insight when the model outputs a wrong answer. Currently, the popular approach to handle such cases is to re-train or fine-tune the model on new examples. This process is not only time-consuming and laborious but also many models often suffer from catastrophic forgetting \cite{hinton1987using,kirkpatrick2017overcoming}, making wrong predictions on examples which it previously predicted correctly. On the other hand, \alg is non-parametric and derives the logical form for a new query conditioned on other similar queries. Therefore, it is possible to fix model predictions by adding missing relevant cases to the case-memory (KNN-index) without requiring any retraining. We demonstrate this (Figure~\ref{fig:hitl_fig}) by showing that \alg can predict LFs of queries containing relations not seen during training when an expert (e.g. system administrator) adds few ``simple cases'' (containing only one relation) to the KNN index. We believe that such controllable properties are essential for models to be deployed in real-world settings and we hope that our work will inspire further research in this direction. It should also be noted there has been a lot of work which try to make neural models transparent and explainable \cite{ribeiro2016should}. \alg differs from them because in addition to making models explainable, it also allows predictions to be \emph{fixable}.


Recent works such as \textsc{Realm} \cite{guu2020realm} and \textsc{Rag} \cite{lewis2020retrieval} retrieve relevant paragraphs from a non-parametric memory for answering questions. Our \alg, in contrast, retrieves \emph{similar queries} w.r.t the input query and use their solution to derive a new solution for the query. \alg is also similar to the recent retrieve and edit framework \cite{hashimoto2018retrieve} for generating structured output. However, unlike us they condition on only a single retrieved example and hence is unlikely to be able to handle compositional questions. Moreover, unlike \alg, retrieve and edit does not have a component that can explicitly revise a generated output.

The contributions of our paper are as follows --- (a) We present a neural CBR approach for KBQA capable of generating complex logical forms conditioned on similar retrieved questions and their logical forms. (b) Since \alg explicitly learns to reuse cases, we show it is able to generalize to unseen relations at test time, when relevant cases are provided. (c) We also show the efficacy of our revise step of \alg which allows to correct generated output by aligning it to local neighborhood of the query entity. (d) Lastly, we show that \alg outperforms other competitive KBQA models, especially on datasets containing complex questions.

 
\section{Model}
\label{sec:model}
This section describes the implementation of various modules of \alg. In CBR, a case is defined as an abstract representation of a problem along with its solution. In our KBQA setting, a case is a natural language query paired with an executable logical form. As mentioned earlier, the practical importance of KBQA has led to the creation of an array of recent datasets \citep[inter-alia]{zelle1996learning,bordes2015large,su2016generating,yih2016value,zhong2017seq2sql,ngomo20189th,yu2018spider,Talmor2018TheWA}. In these datasets, a question is paired with an executable logical form such as \textsc{SPARQL}, \textsc{SQL}, S-expression or graph query. All of these forms have equal representational capacity and are interchangeable \cite{su2016generating}. Figure~\ref{fig:sparql_example} shows an example of two equivalent logical forms. For our experiments, we consider executable \textsc{SPARQL} programs as our logical form.

\textbf{Formal definition of task}: let  be a NL query and let  be a symbolic knowledge base that needs to be queried to retrieve an answer list  containing the answer(s) for . We also assume access to a training set  of NL queries and their corresponding logical forms where ,   represents NL query and its corresponding logical form, respectively. A logical form is an executable query containing entities, relations and free variables (Figure~\ref{fig:sparql_example}). \alg first retrieves  similar cases   from  (\S~\ref{sub:retrieval}). It then generates a intermediate logical form  by learning to reuse components of the logical forms of the retrieved cases (\S~\ref{sub:reuse}). Next, the logical form  is revised to output the final logical form  by aligning to the relations present in the neighborhood subgraph of the query entity to recover from any spurious relations generated in the reuse step (\S~\ref{sub:revise}). Finally,  is executed against  and the list of answer entities are returned. We evaluate our KBQA system by calculating the accuracy of the retrieved answer list w.r.t a held-out set of queries.


\subsection{Retrieve}
\label{sub:retrieval}
The retrieval module computes dense representation of the given query and uses it to retrieve other similar query representation from a training set. Inspired by the recent advances in neural dense passage retrieval \cite{das2019multi,karpukhin2020dense}, we use a \roberta-base encoder to encode each question independently. Also, we want to retrieve questions that have high relational similarity instead of questions which share the same entities (e.g. we prefer to score the query pair (Who is Justin Bieber's brother?, Who is Rihanna's brother?), higher than (Who is Justin Bieber's brother?, Who is Justin Bieber's father?)). To minimize the effect of any entity similarity, we use a named entity tagger\footnote{\url{https://cloud.google.com/natural-language}} to detect spans of entities and mask them with \textsc{[blank]} symbol with a probability . The entity masking strategy has previously been successfully used in learning entity-independent relational representations \cite{soares2019matching}. The similarity score between two queries is given by the inner product between their normalized vector representations (cosine similarity), where each representation, following standard practice \cite{guu2020realm}, is obtained from the encoding of the initial \textsc{[cls]} token of the query. 

\textbf{Fine-tuning question retriever}: In passage retrieval, training data is gathered via distant supervision in which passages containing the answer is marked as a positive example for training. Since in our setup, we need to retrieve similar questions, we use the available logical forms as a source of distant supervision. Specifically, a question pair is weighed by the amount of overlap it has in their corresponding logical queries. Following DPR \cite{karpukhin2020dense}, we sample in-batch negative examples and use a weighted negative log-likelihood loss where the weights are computed by the  score between the set of relations present in the corresponding logical forms. Concretely, let  denote all questions in a mini-batch. The loss function is:

Here,  denotes the vector representation of query 
 and . 
 is computed as the F overlap between relations in the logical pairs of q and q.
We pre-compute and cache the query representations of the training set . For query , we return the top- similar queries in  w.r.t  and pass it to the reuse module.


\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{images/sparql_example1.pdf}
    \vspace{-5mm}
    \caption{An example of a SPARQL logical form for a simple query and its equivalent graph-query.}
    \label{fig:sparql_example}
\end{figure}

\subsection{Reuse}
\label{sub:reuse}
The reuse step generates an intermediate logical form from the  cases that are fed to it as input from the retriever module. Pre-trained encoder-decoder transformer models such as \textsc{Bart} \cite{lewis-etal-2020-bart} and T5 \cite{t5} have enjoyed dramatic success on semantic parsing \cite{lin2018nl2bash,hwang2019comprehensive,shaw2020compositional,suhr2020exploring}. We take a similar approach in generating an intermediate logical form conditioned on the retrieved cases. However, one of the core limitation of transformer-based models is its quadratic dependency (in terms of memory), because of full-attention, which severely limits the sequence length it can operate on. For example, \textsc{Bart} and T5 only supports sequence length of 512 tokens in its encoder. Recall that for us, a case is a query from the train set and an executable \spql program, which can be arbitrarily long.   

\begin{figure*}
    \centering
    \includegraphics[width=2\columnwidth]{images/fig2_notations_JY.pdf}
    \vspace{-3mm}
    \caption{In the revise step of \alg, any generated relations which are not present for an entity are aligned to edges (that are similar above a threshold) present in the local KG neighborhood. Top figure shows a case where the generated relation belongs to a different domain (e.g. Demeter is a \textsf{fictional\_character}, however the generated edges were for \textsf{people} because of the retrieval). In the bottom figure, even though edges are in the same domain, generated edges could be missing because of incompleteness of the KB.}
    \label{fig:intro2}
\end{figure*}

To increase the number of cases we can utilize, we leverage a recently proposed sparse-attention transformer architecture --- \bigb \cite{zaheer2020big}. Instead of having each token attend to all input tokens as in the standard transformer, each token attends to only nearby tokens. Additionally, a small set of global tokens attend to all tokens in the input. This reduces the transformer's quadratic complexity to linear in terms of memory, and empirically, \bigb enables us to use many more cases.

\textbf{Description of input}: The input query  and cases  are concatenated on the encoder side. Specifically, , where \texttt{[SEP]} denotes the standard separator token. Each logical form also contain the KB entity id of each entities in the question (e.g. m.03\_r3 for Jamaica in Figure~\ref{fig:sparql_example}). We append the entity id after the surface form of the entity mention in the question string. For example, the query in Figure~\ref{fig:sparql_example} becomes "What do Jamaican m.03\_r3 people speak?".


Large deep neural networks usually benefit from ``good'' initialization points \citep{frankle2019lottery} and being able to utilize pre-trained weights is critical for our seq2seq models. 
However, our reuse step focuses more on \textit{copying} artifacts from the input, rather than \textit{memorizing and generating} the solution as in most pre-trained models. 
This change in training objective poses a challenge during fine-tuning. 
We conjecture that initial gradients from \textit{memorizing and generating} objective are fairly small compared to those of \textit{copying} when fine-tuning pre-trained weights. Therefore, some forms of loss regularization is needed to calibrate the two objectives. 
We introduce a regularization term that measures the Kullback–Leibler divergence (KLD) between prediction distributions (softmax layers) (1) when only the query  is presented (requires \textit{memorizing and generating}), and (2) when cases  are available (requires \textit{copying}).
Formally, let  be the seq2seq model, let  and  be the prediction distribution with and without cases, respectively. The following KLD term is added to the seq2seq cross-entropy loss

Intuitively, this term regularizes the prediction of  not to deviate too far away from that of the , especially in the beginning of the training process when the new information from cases  is presented to the network.



\subsection{Revise}
\label{sub:revise}
In the previous step, the model explicitly reuses the relations present in , nonetheless, there is no guarantee that the query relations in  will contain the relations required to answer the original query . 
This can happen when the domain of  and domain of cases in  are different even when  the relations are semantically similar.  For example, in figure~\ref{fig:intro2} (top), the input query  and ,  in cases are all asking information about siblings. However, `\textsf{person.sibling}' relation in the retrieved case is not directly applicable for the fictional character `Demeter’ in the query . 

Similarly, large KBs are very incomplete \cite{min2013distant}, so querying with a valid relation might require an edge that is missing in the KB (Figure~\ref{fig:intro2}, bottom) leading to intermediate logical forms which do not execute.

To alleviate this problem and to make the queries executable, we explicitly \emph{align} the generated relations with relations (edges) present in the local neighborhood of the query entity in the KG. We propose the following alignment models:

\textbf{Using pre-trained KB embeddings}: KB completion is a extensively studied research field \cite{nickel2011three,bordes2013translating,socher2013reasoning,Velickovic2018GraphAN,sun2019rotate} and several methods have been developed that learn low dimensional representation of relations such that similar relations are closer to each other in the embedding space. We take advantage of the pre-trained relations obtained from TransE \cite{bordes2013translating}, a widely used model for KB completion. For each predicted relation outgoing from / incoming to an entity, we find the most similar relation edge (in terms of cosine similarity) that exists in the KB for that entity and align with it. If the predicted edge exists in the KB, it trivially aligns with itself.

\textbf{Using similarity in surface forms}: Similar relations (even across domains) have overlap in their surface forms (e.g. `\textsf{siblings}' is common term in both `\textsf{person.siblings}' and `\textsf{fictional\_character.siblings}'). Therefore, word embeddings obtained by encoding these words should be similar. This observation has been successfully utilized in previous works \cite{toutanova-chen-2015-observed,hwang2019comprehensive}. We similarly encode the predicted relation and all the outgoing or incoming edges with \roberta-base model. Following standard practices, relation strings are prepended with a  token and the word pieces are encoder with the \roberta model and the output embeddings of the  token is considered as the relation representation. Like previous alignment model, similarity is computer w.r.t cosine similarity.

Our alignment is very simple and requires no learning. By aligning only single edges to existing edges in the KB, we make sure that we do not change the structure of the logical form generated in the re-use phase. We leave the exploration of learning to align single edges in the program to sequence of edges (paths) in the KB as future work. 


\subsection{Retain}
\label{sub:retain}
A CBR framework also stipulates an explicit retain step, in which derived solution of new cases that are solved successfully are stored. In classic CBR work, successful solutions were represented as domain specific symbolic rules. In our neural CBR framework, such rules are analogously stored in the parameters of each modules.

In the next section we show the efficacy of \alg on various benchmarks that test compositional generalization. We also show that \alg is able to reuse newly added cases containing unseen relations without any re-training. 

 
\section{Experiments}
\label{sec:experiments}
\begin{table*}[ht]
\centering
    \begin{tabular}{l c c c c}
    \toprule
    Model & Precision & Recall & F1 & Accuracy\\\midrule
    STAGG \cite{yih2016value} & 70.9 & \textbf{80.3} & 71.7 & 63.9\\
GraftNet \cite{sun2018open} & - & - & 66.4 (Hits@1) & - \\
    PullNet \cite{sun2019pullnet} & - & - & 68.1 (Hits@1) & - \\
    EmbedKGQA \cite{saxena2020improving} & - & - & 66.6 (Hits@1) & - \\\midrule
    T5-11B \cite{t5} &62.1 & 62.6& 61.5& 56.5\\
    T5-11B + Revise &63.6 & 64.3& 63.0& 57.7\\
    \alg (Ours) & \textbf{73.1} & 75.1 & \textbf{72.8} & \textbf{70.0}\\\bottomrule
    
    \end{tabular}
    \caption{Performance on the WebQSP dataset. GraftNet, PullNet and EmbedKGQA produces a ranking of KG entities hence evaluation is in Hits@k (see text for description). \alg significantly outperforms baseline models in the strict exact match accuracy metric.}
    \label{tab:webqsp_results}
\end{table*}

\subsection{Data}
For all our experiments, the underlying KB is the full Freebase KB containing over 45 million entities (nodes) and 3 billion facts (edges). \cite{fb}. We test \alg on three datasets --- \webqsp \cite{yih2016value}, \cwq \cite{Talmor2018TheWA} and Compositional Freebase Questions \cite{keysers2020measuring}. \webqsp (WebQSP) contains 4737 NL questions belonging to 56 domains covering 661 unique relations. Most questions need up to 2 hops of reasoning, where each hop is a KB edge. \cwq (CWQ) is generated by extending the WebQSP dataset with the goal of making it a more complex multi-hop dataset. There are four types of questions: composition (45\%), conjunction (45\%), comparative (5\%), and superlative (5\%). Answering these questions requires up to 4 hops of reasoning in the KB, making the dataset challenging. Compositional Freebase Questions (CFQ) is a recently proposed benchmark explicitly developed for measuring compositional generalization. For all the datasets above, the logical form (LF) for each NL question is a \textsc{Sparql} query that can be executed against the Freebase KB to obtain the answer entity.


\subsection{Model Hyperparameters}
The hyperparameters listed below are set by tuning on the valdation set for each dataset. WebQSP dataset does not contain a validation split, so we choose 300 training instance to form the validation set.\\
\textbf{Case Retriever}: We initialize our retriever with the pre-trained \roberta-base weights. We set  for \cwq and  for the remaining datasets. We set the initial learning rate to  and decay it linearly throughout training.

\textbf{Seq2Seq Generator}: We use a \bigb generator network with 6 encoding and 6 decoding sparse-attention layers, which we initialize with pre-trained \textsc{Bart}-base weights. We use =20 cases and decode with a beam size of 5 for decoding. We set the initial learning rate to  and decay it linearly throughout training.



\subsection{Entity Linking}
\label{sub:entity_linking}
The first step required to generate an executable LF for a NL query is to identify and link the entities present in the query. For our experiments, we use a combination of an off-the-shelf entity linker and a large mapping of mentions to surface forms. For the off-the-shelf linker, we use a recently proposed high precision entity linker \textsc{Elq} \cite{li2020efficient}. To further improve recall of our system, we first identify mention spans of entities in the question by tagging it with a NER\footnote{\url{https://cloud.google.com/natural-language}} system. Next, we link entities not linked by \textsc{Elq} by exact matching with surface form annotated in FACC1 project \cite{facc1}. Our entity linking results are shown in Table~\ref{tab:el_results}.
\begin{table}[]
    \centering
    \small
    \begin{tabular}{c c c c}
    \toprule
    Dataset &  Precision & Recall & F1\\\midrule
    WebQSP  & 0.761   & 0.819 & 0.789\\
    CWQ & 0.707 & 0.910 & 0.796\\\bottomrule
    \end{tabular}
    \caption{Entity linking performance on various datasets}
    \label{tab:el_results}
    \vspace{-1em}
\end{table}

\subsection{Retrieval Performance}
We compare the performance of our trained retriever with a \roberta-base model. We found that \roberta model even without any fine-tuning performs well at retrieval. However, fine-tuning \roberta with our distant supervision objective improved the overall recall, e.g., from 86.6\% to 90.4\% on \webqsp and from 94.8\% to 98.4\% on CFQ MCD1 split.

\subsection{KBQA Results}
\label{sub:main_results}
Table~\ref{tab:webqsp_results} reports results on WebQSP. All reported model except ours directly operate on the KB (e.g. traverse KB paths starting from the query entity) to generate the LF or the answer. As a result, models such as STAGG tend to enjoy much higher recall. On the other hand, much of our logical query is generated by reusing components of similar cases. Models such as GraftNet \cite{sun2018open} and PullNet \cite{sun2019pullnet} rank answer entities and return the top entity as answer (Hits@1 in table~\ref{tab:el_results}). This is undesirable for questions that have multiple entities as answers (e.g. ``Name all major cities in the U.S.?''). We also compare to T5 \cite{t5} a large pre-trained seq2seq model with over 11B parameters. T5 was fine-tuned on the query and LF pair. As show in Table~\ref{tab:webqsp_results}, \alg outperforms all other models significantly and improves on the strict exact-match accuracy by more than 6 points w.r.t. the best model.

\begin{table}[t]
\centering
\setlength{\tabcolsep}{4pt}
\begin{tabular}{l c c c c}
\toprule
Model & P & R & F1 & Acc \\
\midrule
KBQA-GST \cite{lan2019knowledge} & - & - & - & 39.4\\
QGG  \cite{lan2020query} & - & - & - & 44.1 \\
PullNet \cite{sun2019pullnet}& - & - & - & 45.9 \\
DynAS (Anonymous) & - & - & - & 50.0 \\
\midrule
T5-11B \cite{t5} & 55.2 & 55.4 & 54.6 & 52.4\\
T5-11B + Revise & 58.7 & 59.6& 58.2& 55.6\\
\alg (Ours) & \textbf{70.4} & \textbf{71.9} & \textbf{70.0} & \textbf{67.1} \\
\bottomrule
\end{tabular}
\caption{Performance on the CWQ dataset. The performance of models in the top section are taken from the official leaderboard. Questions in CWQ need complex reasoning and \alg outperforms all other existing models (including a massive fine-tuned T5 model for this task) by a significant margin.} 
\label{tab:cwq_results}
\end{table}


Table~\ref{tab:cwq_results} reports performance on CWQ\footnote{The result of our model in the official leaderboard (\url{https://www.tau-nlp.org/compwebq-leaderboard}) is higher (70.4 vs 67.1). This is because the official evaluation script assigns full score if any of the correct answer entities are returned even if there are multiple correct answers for a question. In the paper we report the stricter metric which only assigns when \emph{all} the answer entities are returned by the model}. As discussed earlier, CWQ contains complex multi-hop questions that require compositional reasoning. The top section reports performance of various baselines in the official leader-board.


\begin{table}[t]
    \centering
    \small
    \begin{tabular}{l c c c c}
    \toprule
        Model  & MCD1 & MCD2 & MCD3 & MCD-mean\\\midrule
         T5-11B & 72.9 & 69.2 & 62.0 & 67.7\\
         \alg & 87.9 & 61.3& 60.6 & 69.93\\
    \bottomrule
    \end{tabular}
    \caption{Accuracy on the CFQ dataset. Our CBR approach (with order of magnitude less parameters) outperforms a massive T5 model.}
    \label{tab:my_label}
\end{table}



Finally, we report results on CFQ in Table~\ref{tab:my_label}. The creators of CFQ propose to evaluate performance with exact string match accuracy between \spql programs is computed, which is quite conservative in what is counted as correct. For example, in a \spql query having a different (but consistent) name for a free variable (\textsf{x1} instead of \textsf{x}) does not change the semantics of the query. Also \spql is independent of the order of the relational triples. This is especially unfair to our model, which copies relations from other nearest neighbor queries and can copy relations in any order. As a remedy, we report results from evaluating the predicted (and gold) queries against a Freebase KB. We also recreate the results of the baseline T5-large model (which also improves w.r.t. the original results reported in \cite{furrer2020compositional}). With the exception of the MCD-2 split of the data, \alg performs comparably with the T5 model.\\
\textbf{Efficacy of the Revise step}:
Table~\ref{tab:revise} show that the revise step is useful for \alg on multiple datasets. We also show that the T5 model also benefits from the alignment in revise step with more than 3 points improvement in F1 score on the CWQ dataset. We find that TransE alignment outperforms \roberta based alignment, suggesting that graph structure information is more useful than surface form similarity for aligning relations. Moreover, relation names are usually short strings, so they do not provide enough context for LMs to form good representation.



\begin{table}[t]
    \centering
    \small
    \begin{tabular}{l c c}
    \toprule
        WebQSP &  Accuracy(\%) & \\
        \midrule
        \alg (before Revise) & 69.43 & --\\
        Revise (Roberta) & 69.49 & +0.06\\
        Revise (TransE) & \textbf{70.00} & \textbf{+0.57}\\
        \midrule
        CWQ &  Accuracy(\%) & \\
        \midrule
        \alg (before Revise) & 65.95 & --\\
        Revise (Roberta) & 66.32 & +0.37 \\
        Revise (TransE) & \textbf{67.11} & \textbf{+1.16}\\
    \bottomrule
    \end{tabular}
    \caption{Impacts of the revise step. We show that the revise step consistently improves the accuracy on WebQSP and CWQ, especially with the TransE pretrained embeddings.}
    \label{tab:revise}
\end{table}


\subsection{Applying Point-Fixes to Model Predictions without Re-Training}
\label{sub:analysis}
Modern NLP systems built on top of large LMs are used as black-boxes. Specifically, it does not provide users no control or insights to \emph{fix} an erroneous prediction. The current approach for fixing this practical problem is to re-train the model on the failed examples. However, this process is time-consuming and impractical for production settings. Moreover, it has been shown (and as we will empirically demonstrate) that this approach can also lead to catastrophic forgetting where the model forgets what it had learned before \cite{hinton1987using,mccloskey1989catastrophic,kirkpatrick2017overcoming,toneva2018empirical}.

On the contrary, we show that our \alg approach lets user inspect erroneous predictions and also allows them to perform point fixes \emph{without} re-training the model. 

\begin{table*}
    \centering
    \begin{tabular}{l c c c c}
    \toprule
    Scenario && Initial Set & Held-Out \\
    \midrule
    Transformer     && 59.6 & 0.0 & \\
    \quad + Fine-tune on additional cases only (100 steps/50sec) && 1.3 & 76.3 \\
    \quad + Fine-tune on additional cases and original data (300 steps/150sec) && 53.1 & 57.6 \\
    \midrule
    \alg (Ours) && 69.4 & 0.0 \\
    \quad + Adding additional cases to index (2 sec) && 69.4 & 70.6 \\
    \bottomrule
    \end{tabular}
    \caption{Robustness and controllability of our method against transformer. We can easily and quickly adopt to new relations given cases about it, whereas heavily parameterized transformer is finicky, not stable, and can undergo catastrophic forgetting when we try to add new relation information intro its parameters.}
    \label{tab:robust}
\end{table*}

\begin{table*}
    \centering
    \begin{tabular}{l l c c c c c c}
    \toprule
    Dataset & Scenario && Precision & Recall & F1 & Accuracy \\
    \midrule
    \multirow{2}{*}{WebQSP}& \alg (Ours) && 0.0 & 0.0 & 0.0 & 0.0 \\
    & \quad + Adding additional cases &&  36.54 & 38.59 & 36.39 & 32.89 \\
    \bottomrule
    \end{tabular}
    \caption{Results for \hitl experiment. After adding a few cases , we see that we can get the accuracy of OOV questions to improve considerably, without needing to re-train the model.}
    \label{tab:hitl_results_appendix}
\end{table*}


\subsubsection{Performance on Unseen Relations}
\label{subsub:unseen_rel}
We consider the case when the model generates a wrong LF for a given query. We create a controlled setup by removing all queries from the training set of WebQSP which contain the (people.person.education) relation. This led to a removal of 136 queries from the train set and ensured that the model failed to correctly answer the 86 queries (held-out) in the test set which contained the removed relation in its LF.

For this experiment, we compare to a black-box seq2seq \textsc{Bart} transformer model as our baseline. As shown in Table~\ref{tab:robust}, both baseline and \alg do not perform well without any relevant cases since a required KB relation was missing during training. Next, we add the 136 training instances back to the training set and recompute the KNN index. This process involves encoding the newly added NL queries and recomputing the KNN index, a process which is computationally much cheaper than re-training the model again. Row 5 in Table~\ref{tab:robust} shows the new result. On addition of the new cases, \alg can seamlessly use them and copy the unseen relation to predict the correct LF, reaching 70.6\% accuracy on the 86 held-out queries. 

In contrast, the baseline transformer must be fine-tuned on the new cases to handle the new relation, which is more computationally expensive than adding the cases to our index. Moreover, just fine-tuning on the new instances leads to \emph{catastrophic forgetting} as seen in row 2 of Table~\ref{tab:robust} where the baseline model's performance on the initial set decreases drastically. We find it necessary to carefully fine-tune the model on new examples alongside original training examples (in a 1:2 proportion). However, it still converges to a performance which is lower than its original performance and much lower than the performance of \alg.


\subsubsection{Human-in-the-Loop Experiment}
\label{subsub:hitl}
During error analysis, we realized that there are queries in the evaluation set of WebQSP that contain KB relations in their LFs which were never seen during training. That means model trained on the corresponding train set will never be able to predict the correct LF for the query because of the required unseen relation. 

To further showcase the flexibility of our model we conduct a human-in-the-loop experiment (Figure~\ref{fig:hitl_fig}) in which users add simple `cases' to point-fix erroneous predictions of \alg for those queries. A simple case is a NL query paired with a program which only contain one KB relation. Table~\ref{tab:hitl_appendix} (Appendix \ref{app:hitl}) shows some example of such cases. Because of the simple nature of the questions, these cases can be created manually (by a user who is knowledgeable about the KB schema) or automatically curated from data sources such as SimpleQuestions \cite{bordes2015large} which is a large collection of NL queries that can be a mapped to a single KB edge. Table~\ref{tab:hilt_stats_appendix} in Appendix \ref{app:hitl} shows various statistics of the missing relations and the number of cases added by humans and from SimpleQuestions. 

Table~\ref{tab:hitl_results_appendix} shows the result of this experiment. For WebQSP by adding a few cases, the performance of the model increases by 36 F1 without requiring any model re-training. Note unlike the previous controlled experiment in \S\ref{subsub:unseen_rel}, we only add around 3 - 4 cases for each unseen relation. The cases are added to the original KNN-index and to get a prediction right, the retriever module has to get the relation from the index and the reuse module has to learn to copy the unseen relation in the retrieved case (without any additional training). 

\subsection{Analysis}
\label{sub:analysis}

\begin{table}[]
\vspace{-1mm}
    \centering
    \small
    \begin{tabular}{c | c c | c c}\toprule
        \multirow{2}{*}{Data} & \multirow{2}{*}{\# Total Q} & \#\, Q that need & \multicolumn{2}{c}{\# Correct}\\
        & & comp. reasoning & T5 & CBR \\\midrule
        CWQ & 3531 & 639 & 205 & \textbf{270}\\
        CFQ & 11968 & 6541 & 3351 & \textbf{3886}\\\bottomrule
\end{tabular}
    \vspace{-2mm}
    \caption{\small Analysis of Compositional Reasoning. We compare the performance of models on questions that need \emph{novel combinations} of relations \emph{not seen} during training.}
    \vspace{-6mm}
    \label{tab:comp}
\end{table}

To further demonstrate CBR's ability to handle compositional questions, we analyze questions in the evaluation set which require \emph{novel combinations} of relations \emph{never seen} in the training set. This means, in order for our model to answer these questions correctly, it would have to retrieve relevant nearest neighbor (NN) questions from the training set and copy the required relations from  the logical form of \emph{multiple} NN queries. Table~\ref{tab:comp} shows that our model outperforms the competitive T5 baseline. Also as we saw in the last section, our model is able to quickly adapt to relations \emph{never seen} in the training set altogether by picking them up from newly added cases.


 

\section{Related Work}
\label{sec:related_work}
\textbf{Retrieval augmented LM architectures}: A growing class of models~\citep[e.g.,][]{guu2020realm,lewis2020retrieval} augments language models with a non-parametric memory, instead of solely relying on information stored in model parameters.
In contrast, our CBR approach retrieves \emph{similar queries}, instead of relevant supporting context, w.r.t the input query and use their solution (logical forms) to derive a new solution for the query. Recently, \cite{lewis2020question} also noted that the train set often contain similar questions w.r.t. the evaluation set, and concurrent work uses this insight to derive the answer to natural language questions using similar, retrieved questions~\cite{lewis2021paq}.
Our work develops a case-based reasoning approach for KBQA and is further capable of answering compositional questions from multiple simple questions.

\textbf{Retrieve and edit}: Our model shares similarities with the \textsc{retrieve-and-edit} framework \cite{hashimoto2018retrieve} which utilizes the case of the nearest-neighbor w.r.t input. 
Their ``edit'' step is similar to our ``reuse'' step, however, they simply rely on the sequence-to-sequence model to generate answer from the retrieved case without ``revise'' and ``retain'' steps. Furthermore, our ``reuse'' step brings in new challenges as parametric model have to compose one \spql query from multiple cases in contrast to \textsc{retrieve-and-edit} that only considers a single nearest case.

\textbf{K-NN based approach in other NLP applications}: Nearest neighbor models have been used in a number of NLP applications such as parts-of-speech tagging \cite{daelemans1996mbt} and morphological analysis \cite{bosch2007efficient}. \citet{wiseman-stratos-2019-label} achieved accurate sequence labeling by explicitly and only copying labels from retrieved neighbors. Another recent line of work use training examples at test time to improve language generation \cite{weston-etal-2018-retrieve,pandey-etal-2018-exemplar,cao-etal-2018-retrieve,peng-etal-2019-text}.
\citet{khandelwal2019generalization} also observed improvements in language modeling by utilizing explicit examples from past training data obtained via nearest neighbor search in a continuous vector space. However, unlike us, these work do not do explicit reasoning with subquestions.
\cite{hua2020retrieve} recently proposed a meta-learning approach which utilizes cases retrieved w.r.t. the similarity of the input. However, their main goal is to learn a better parametric model (retriever and generator) through neighboring cases rather than composing and fixing cases to generate answers at test time as we do.

\textbf{Case-based Reasoning for KB completion}: Recently, a CBR based approach was proposed by \citet{das2020simple}. Similar to our approach, they retrieve similar entities and then find reasoning paths from those entities. However, their approach does not handle complex natural language queries. Additionally, the logical forms handled by our model have much more expressive power than knowledge base paths.


\textbf{Question Decomposition}
One strategy to answer a compositional question is to first break it down into simpler subquestions, each of which can be viewed as a natural language program describing how to answer the question.
This approach has been shown to be effective as far back as IBM Watson~\cite{ferrucci2010building} to more recent systems for answering questions about text~\cite{das2019multi,min-etal-2019-multi,perez-etal-2020-unsupervised,wolfson2020break} or knowledge bases~\cite{Talmor2018TheWA}.
These prior studies do not leverage case-based reasoning when generating decompositions and thus may also benefit from similar techniques as proposed in our work.\\

\textbf{Program Synthesis}: Repairing / Revising generated programs has been studied in the field of program synthesis. For example, prior work repairs a program based on syntax of the underlying language \cite{le2017s3}, by generating sketches \cite{hua2018towards}. More recently, \citet{gupta2020synthesize} proposes a framework in which they use a program debugger to revise the program generated by a neural program synthesizer. However, none of these works take advantage of the similarity between semantic relations present in the knowledge base, and hence, unlike us, they do not use embeddings of similar relation to align relations. More generally, many prior efforts have employed neural models to generate \spql -like code for semantic parsing \cite{dong2016language,balog2016deepcoder,zhong2017seq2sql}, SQL queries over relational databases~\cite{zhongSeq2SQL2017}, program-structured neural network layouts~\cite{Andreas_2016_CVPR}, or even proofs for mathematical theorems~\cite{polu2020generative}.
Our work differs in our use of case-based reasoning, whose advantages we showcased earlier.

 
\section{Limitation of our Model and Future Work}
\label{sec:limitations}
To the best of our knowledge, our work is the first to propose a neuralized case-based reasoning approach for KBQA. We showed that our model is effective in handling complex compositional questions, but our work also has several limitations. First, our model relies on the availability of supervised logical forms such as \spql queries, which can be expensive to annotate at scale. In the future, we plan to explore ways to directly learn from question-answer pairs \cite{berant2013semantic,liang2016neural}. Even though, our CBR approach is modular and has several advantages, the retrieve and reuse components of our model are trained separately. In future, we plan to explore avenues for end to end learning for CBR. 
\section{Acknowledgments}
We thank Andrew Drozdov, Kalpesh Krishna, Subendhu Rongali and other members of the
UMass IESL and NLP groups for helpful discussion and feedback. RD and DT are funded in part by the Chan Zuckerberg Initiative under the project Scientific Knowledge Base Construction and in part by IBM Congitive Horizons Network (CHN).
EP is grateful for support from the NSF Graduate Research Fellowship and the Open Philanthropy AI Fellowship. The work reported here was performed in part by the Center for Data Science and the Center for Intelligent Information Retrieval, and in part using high performance computing equipment obtained under a grant from the Collaborative R\&D Fund managed by the Massachusetts Technology Collaborative.

\bibliography{example_paper}
\bibliographystyle{icml2021}


\newpage
\onecolumn
\appendix
\section{Details on Held-Out Experiments (\S\ref{sub:analysis})}
\label{sub:held_out_appendix}



In this section, we include more details about our held-out experiment described in section~\ref{sub:analysis}. The goal of this experiment is to show that our approach can \emph{generalize} to unseen relations without requiring \emph{any further training} of the model.
This is a relevant setting to explore, because real-world knowledge bases are often updated with new kinds of relations, and we would like KBQA systems that adapt to handle new information with minimal effort.

We explicitly hold-out all questions containing a particular relation from the datasets. Table~\ref{tab:held_out_stats} shows the relation type and the number of questions that are removed as a result of removing the relation.

\begin{table}[bht]
    \centering
    \begin{tabular}{c c c c} \toprule
         Dataset & Relation name & Train & Test\\\midrule
         WebQSP & people.person.education & 136 & 86 \\\bottomrule
\end{tabular}
    \caption{Relation type and the corresponding number of NL queries that are held-out.}
    \label{tab:held_out_stats}
\end{table}

Our baseline is a transformer-based seq2seq model which does not utilize similar cases. To be most fair, we use a similar \bigb architecture but only providing the encoder with the NL query (no retrieved cases) as input. Since there are no \spql queries given to the encoder, we do not do any hashing of relation strings since that would make learning impossible. 


Table~\ref{tab:robust} shows the results. First, the baseline model does worse on the original test set than our approach, indicating that the retrieved cases are helpful. When tested on the held-out relations as is, both our model and the baseline get 0\% accuracy (row1, row 4; col2) since the examples require generating an unseen relation. However, we show that if the held-out examples in the train set are added back to the KNN index (without any training), our model's performance significantly increases to 70.6\% (row 5, col2). 

To be fair to the baseline, we fine-tuned the baseline model with the held-out train set. This is a time-consuming process; on this small dataset, it took 25x longer to re-train when compared to just re-indexing for our model. On re-training, we found that the baseline model performed very well on the heldout dataset, even outperforming our model. However, when tested on the entire dataset, we found that the model exhibited \emph{catastrophic forgetting} \cite{kirkpatrick2017overcoming} (row 2, col 1) where the performance drastically fell from 59.6\% to 1.3\%. On further analysis, we found that the model was only predicting the held-out relation for all questions.

To overcome the catastrophic forgetting issue, we trained the model not only on the held-out data but also with some original training examples (we sample twice the number of held-out examples from the original data) . This is even more time-consuming (75x longer than just re-indexing). Here, we find that the amount of catastrophic forgetting is reduced to some extent, but the overall performance of the baseline model is lower for both the full and held-out sets.


\section{Details on Automated Case Collection and Human-in-the-Loop Experiments (\S\ref{sub:analysis})}
\label{app:hitl}
\begin{table}[b]
    \centering
    \begin{tabular}{c c c c c c} \toprule
    & & & \multicolumn{2}{ c }{Cases Added via}\\ \cline{4-5}
         Dataset & \# missing relations  & \# questions & \hitl & SimpleQuestions & Avg. \# cases per relation \\\midrule
WebQSP & 94 & 79 & 72 & 292 & 3.87 \\\bottomrule
    \end{tabular}
    \caption{Number of questions in the evaluation set that needs a relation which is not seen in the training set. Note that, there can be multiple relations in a question that might not be seen during training. The last two columns show the number of cases added both via human-in-the-loop (\hitl) annotation and automatically from SimpleQuestions dataset. }
    \label{tab:hilt_stats_appendix}
\end{table}

\begin{table}[]
    \centering
    \small
    \begin{tabular}{l l c}\toprule
        NL Query & \spql & Source\\\midrule
        What is the Mexican Peso called? & select ?x where \{ m.012ts8 finance.currency.currency\_code ?x .\} & Manual\\
        Who invented the telephone? & select ?x where \{ m.0g\_3r  base.argumentmaps.original\_idea.innovator ?x  .\} & Manual \\
        what area is wrvr broadcated in? & select ?x where \{ m.025z9rx broadcast.broadcast.area\_served ?x .\} & SQ\\
        Where are Siamese cats originally from? & select ?x where \{ m.012ts8 biology.animal\_breed.place\_of\_origin ?x .\} & Manual\\ \bottomrule   
\end{tabular}
    \caption{Examples of few added questions and their corresponding \spql queries. Notice that the \spql queries are very simple to create once we know the name of the missing relation. The source column indicate whether the question was manually created or automatically added from Simple Questions (SQ) dataset.}
    \label{tab:hitl_appendix}
\end{table}



The held-out experiment in the previous section (\S\ref{sub:held_out_appendix}) was synthetically designed by us to showcase the effectiveness of our approach on unseen relations. However, while conducting analysis, we also noticed that both WebQSP and CWQ actually have queries in the test set for which the required relations are never present in the training set. 

This gives us an opportunity to conduct real experiments to demonstrate the advantage of our model. To add more cases, we resort to a mix of automated data collection and human-in-the-loop strategy. For each of the missing relation, we first try to find NL queries present in the SimpleQuestions \cite{bordes2015large}. SimpleQuestions (SQ) is a large dataset containing more than 100K NL questions that are `simple' in nature --- i.e. each NL query maps to a single relation (fact) in the Freebase KB.  For each missing relation type, we try to find questions in the SQ dataset that can be mapped to the missing relation. However, even SQ has missing coverage in which case, we manually generate a question and its corresponding \spql query by reading the description of the relation. Table~\ref{tab:hilt_stats_appendix} shows the number of questions in the evaluation set which at least has a relation never seen during training and also the number of cases that has been added. For example, we\footnote{The H-I-T-L case addition was done by 2 graduate students in the lab. } were able to collect 292 questions from SQ and we manually created 72 questions for WebQSP. Overall, we add 3.87 new cases per query relation for WebQSP and 4.35 cases per relation for CWQ.

Table~\ref{tab:hitl_appendix} shows some example of cases added manually or from SQ. We look up entity ids for entities from the FACC1 alias table (\S\ref{sub:entity_linking}). Also note, that since we only add questions which are simple in nature, the corresponding \spql query can be easily constructed from the missing relation type and the entity id. 

Table~\ref{tab:hitl_results_appendix} show the results for our experiment. It is encouraging to see that only after adding 3 - 4 similar questions per query relation, our performance increases from 0\% to 32.89\% on WebQSP, without requiring any additional training. On CWQ, our relative improvement is lesser. On analysis, we found that even though the relations from the new cases were being used correctly, the \spql required for answering a CWQ query is more complex (e.g. with the use of operators such as order-by, filter, limit etc.). Since the cases we added are simple in nature where the \spql did not have such constructs, the generated queries also did not have them. We believe this can be fixed by adding cases with \spql queries containing similar construct and we leave it as future work.

In Table~\ref{tab:hitl_eye_candies_appendix}, we further show some predictions of the model after we added the cases for missing relations. In the first example, our model can pick up the missing relation (in \textbf{bold}) right away without any training/fine-tuning. In the second example, however, our model can only predict part of the \spql, which corresponds to \textit{Edgar Allan Poe}'s occupation. Our error analysis show that the problem lies in decomposition of the question and retrieve the other part -- \textit{earliest publication end date} rather than reuse and combine the relations. We leave this issue for future works.

\begin{table}[]
    \centering
    \begin{tabular}{c c c}\toprule
          & WebQSP & CWQ  \\\midrule
         Baseline (K = 0) & 67.2 & 65.8 \\
         \alg (K = 20) & \textbf{70.0} & \textbf{67.1} \\\bottomrule
    \end{tabular}
    \caption{Comparison with a baseline model that do not use cases. The numbers denote exact match accuracy.}
    \label{tab:ablation_results_k=0}
\end{table}


\begin{table*}
    \centering
    \begin{tabular}{l l}
    \toprule
    Question & What colors do the school where Donald Stanley Marshall is a grad student use?\\
    Predicted \spql: & \begin{minipage}[t]{0.75\columnwidth}\textsf{select ?x where \{\\\quad ?c ns:education.educational\_institution.students\_graduates ?k .\\\quad ?k ns:education.education.student ns:m.0\_y98vd .\\\quad \textbf{?c ns:education.educational\_institution.colors ?x} .\\ \}}\end{minipage}\\
    Ground-truth \spql: & Same as predicted.\\
    Predicted answers: & \textit{crimson} \\
    Ground-truth answers: & \textit{crimson} \\
    \midrule
    Questions: & What magazine with earliest publication end date did Edgar Allan Poe work for?\\
    Predicted \spql: & \begin{minipage}[t]{0.75\columnwidth}\textsf{select ?x where \{\\\quad \textbf{ns:m.02lt8 ns:book.periodical\_editor.periodicals\_edited ?y .}\\\quad \textbf{?y ns:book.erial\_tenure.periodical ?x .}\\ \}}\end{minipage}\\
    Ground-truth \spql: & \begin{minipage}[t]{0.75\columnwidth}\textsf{select ?x where \{\\\quad \textbf{ns:m.02lt8 ns:book.periodical\_editor.periodicals\_edited ?y .}\\\quad \textbf{?y ns:book.editorial\_tenure.periodical ?x .}\\\quad ?x ns:book.magazine.place\_of\_publication ?c .\\\quad ?c ns:book.place\_of\_publication\_period.to ?num .\\ \} order by ?num limit 1}\end{minipage}\\
    Predicted answers: & \textit{Graham's Magazine, Broadway Journal, Burton's Gentleman's Magazine}\\
    Ground-truth answers: & \textit{Burton's Gentleman's Magazine}\\
    \bottomrule
    \end{tabular}
    \caption{Compositional examples from the \hitl experiment.}
    \label{tab:hitl_eye_candies_appendix}
\end{table*}



\textbf{Importance of this result}: Through this experiment, we demonstrate two important properties of our model --- \emph{interpretability} and \emph{controllability}. Database schemas keep changing and new tables keep getting added to a corporate database. When our QA system gets a query wrong, by looking at the retrieved K-nearest neighbors, users can determine (interpretability) that the required relation is not present in the training set. By adding few cases for the new relations, they can query the DB for similar questions, without needing to train the QA system (controllability). Current black-box NLP models are not capable of doing such point-fixes and our experiment is an initial attempt towards building such systems. 


\begin{table}
    \centering
    \begin{tabular}{l l}\toprule
      Query: &  What was the title of the first book Charles Darwin wrote?\\
    Target \spql: & \textsf{select distinct ?x where \{ m.01lwx book.author.works\_written ?x . }\\  & \textsf{?x common.creative\_work.release\_date ?time\} order by xsd:datetime(?time) limit 1}\\\midrule
    KNN1 question: & What books did Lincoln write? \\
    KNN1 \spql: & \textsf{select distinct ?x where \{ m.0gzh book.author.works\_written ?x\}}\\\midrule
    KNN2 question: & What was the first album of Michael Jackson? \\
    KNN2 \spql: & \textsf{select distinct ?x where \{ m.09889g music.artist.album ?x .}\\
    & \textsf{?x common.creative\_work.release\_date ?time\} order by xsd:datetime(?time) limit 1}\\\bottomrule
    \end{tabular}
    \caption{A NL query shown with its target \spql program. Also shown are two nearest neighbor retrieved cases. Note, the target query can be generated by copying one relation from each of the retrieved cases (\textsf{book.author.works\_written} from KNN1 and \textsf{common.creative\_work.release\_date} from KNN2)}
    \label{tab:knn_eg_1}
\end{table}



\section{Ablation}
We also compare with a model with the same reuse component of \alg but is trained and tested without retrieving any cases from the case-memory. Table~\ref{tab:ablation_results_k=0} reports the result which shows that having cases is indeed helpful for correctly answering questions.




\section{Analysis of the Revise Step}

In the Revise step, we attempt to fix programs predicted by our Reuse step that did not execute on the knowledge base. The predicted program can be syntactically incorrect or enforce conditions that lead to an unsatisfiable query. In our work, we focus on predicted programs that can be fixed by aligning clauses to relations in the local neighborhood of query entities. Note that we only attempt to fix clauses that contain at least one grounded entity in the KB. This means there are clauses in the predicted program that we do not attempt to align during revision. As a result, not all programs with a successful alignment lead to the correct answer Table~\ref{tab:revision_stats}. We give examples of successful alignments Table~\ref{tab:revise_eye_candies_pos_appendix} as well as failed attempts at alignment Table~\ref{tab:revise_eye_candies_neg_appendix}.

\begin{table}[]
    \centering
    \begin{tabular}{l c c} \toprule
         Dataset & WebQSP & CWQ \\\hline
         Attempted revisions & 247 & 1128 \\
         Revised program covers more target clauses & 29 & 114 \\
         Revised program produces an answer & 13 & 27 \\
         Revised clauses match target & 9 & 41 \\\bottomrule
    \end{tabular}
    \caption{We report the outcomes of the revise step on various datasets. We attempt revision if the predicted program produces no answer. If the revision step aligns some but not all clauses, it is not guanranteed to produce the answer.}
    \label{tab:revision_stats}
\end{table}

\begin{table*}
    \centering
    \begin{tabular}{l l}
    \toprule
     & WebQSP\\
    \hline
    Question: & when did kaley cuoco m.03kxp7 join charmed m.01f3p\_ ?\\
    Predicted \spql: & \begin{minipage}[t]{0.75\columnwidth}\textsf{SELECT DISTINCT ?x WHERE \{\\\quad ns:m.03kxp7 ns:tv.tv\_character.appeared\_in\_tv\_program ?y .\\\quad ?y ns:tv.regular\_tv\_appearance.from ?x .\\\quad ?y ns:tv.regular\_tv\_appearance.series ns:m.01f3p\_ .\\ \}}\end{minipage}\\
    Ground-truth \spql: & \begin{minipage}[t]{0.75\columnwidth}\textsf{SELECT DISTINCT ?x WHERE \{\\\quad ns:m.03kxp7 ns:tv.tv\_actor.starring\_roles ?y .\\\quad ?y ns:tv.regular\_tv\_appearance.from ?x .\\\quad ?y ns:tv.regular\_tv\_appearance.s
eries ns:m.01f3p\_ .\\ \}}\end{minipage}\\
    Revised \spql: & \begin{minipage}[t]{0.75\columnwidth}\textsf{SELECT DISTINCT ?x WHERE \{\\\quad ns:m.03kxp7 \textbf{ns:tv.tv\_actor.starring\_roles} ?y .\\\quad ?y ns:tv.regular\_tv\_appearance.from ?x .\\\quad ?y ns:tv.regular\_tv\_appearance.s
eries ns:m.01f3p\_ .\\ \}}\end{minipage}\\
   
    \midrule
     & CWQ\\
    \hline
    Question: & \begin{minipage}[t]{0.70\columnwidth}What text in the religion which include Zhang Jue m.02gjv7 as a key figure is considered to be sacred m.02vt2rp ? \end{minipage}\\
    Predicted \spql: & \begin{minipage}[t]{0.75\columnwidth}\textsf{SELECT DISTINCT ?x WHERE \{\\\quad ?c ns:religion.religion.deities ns:m.02gjv7 .\\\quad ?c ns:religion.religion.texts ?x .\\\quad \ldots \textit{benign filters} \ldots \}\\\quad }\end{minipage}\\
    Ground-truth \spql: & \begin{minipage}[t]{0.75\columnwidth}\textsf{SELECT DISTINCT ?x WHERE \{\\\quad ?c ns:religion.religion.notable\_figures ns:m.02gjv7 .\\\qquad ?c ns:religion.religion.texts ?x .\}\\\quad }\end{minipage}\\
    Revised \spql: & \begin{minipage}[t]{0.75\columnwidth}\textsf{SELECT DISTINCT ?x WHERE \{\\\quad ?c \textbf{ns:religion.religion.notable\_figures} ns:m.02gjv7 .\\\quad ?c ns:religion.religion.texts ?x .\\\quad \ldots \textit{benign filters} \ldots \}\\\quad }\end{minipage}\\
    \midrule
    Question: & \begin{minipage}[t]{0.70\columnwidth} What is the mascot of the educational institution that has a sports team named the North Dakota State Bison m.0c5s26 ? \end{minipage}\\
    Predicted \spql: & \begin{minipage}[t]{0.75\columnwidth}\textsf{SELECT DISTINCT ?x WHERE \{\\\quad ?c ns:education.educational\_institution.sports\_teams ns:m.0c5s26 .\\\quad ?c ns:education.educational\_institution.mascot ?x .\\\quad\} }\end{minipage}\\
    Ground-truth \spql: & \begin{minipage}[t]{0.75\columnwidth}\textsf{SELECT DISTINCT ?x WHERE \{\\\quad ?c ns:education.educational\_institution.sports\_teams ns:m.0c41\_v .\\\quad ?c ns:education.educational\_institution.mascot ?x .\\\quad\} }\end{minipage}\\
    Revised \spql: & \begin{minipage}[t]{0.75\columnwidth}\textsf{SELECT DISTINCT ?x WHERE \{\\\quad ?c \textbf{ns:education.educational\_institution.athletics\_brand} ns:m.0c5s26 .\\\quad ?c ns:education.educational\_institution.mascot ?x .\\\quad\} }\end{minipage}\\
    Comments: & \begin{minipage}[t]{0.75\columnwidth} The entity linker has tagged the bison as a university symbol (m.0c5s26) rather than the Bison football team (m.0c41\_v). Alignment helps the model recover from this by picking the relation that connects the tagged entity to the university. \end{minipage}\\
    \bottomrule
    \end{tabular}
    \caption{Examples of successful alignment with TransE from the Revise stage.}
    \label{tab:revise_eye_candies_pos_appendix}
\end{table*}

\begin{table*}
    \centering
    \begin{tabular}{l l}
    \toprule
     & WebQSP\\
    \hline
    Question: & \begin{minipage}[t]{0.70\columnwidth} who is gimli m.0h34n 's father m.02pn7 in the hobbit m.0n4ck66 \end{minipage}\\
    Predicted \spql: & \begin{minipage}[t]{0.75\columnwidth}\textsf{SELECT DISTINCT ?x WHERE \{\\\quad ns:m.0h34n ns:people.person.parents ?x .\\\quad ?x ns:people.person.gender ns:m.02pn7 .\\\quad ?x ns:people.person.parents ?sk0 .\\ \} ORDER BY xsd:datetime(?sk0) LIMIT 1 }\end{minipage}\\
    Ground-truth \spql: & \begin{minipage}[t]{0.75\columnwidth}\textsf{SELECT DISTINCT ?x WHERE \{\\\quad ns:m.0h34n ns:fictional\_universe.fictional\_character.parents ?x .\\\quad ?x ns:fictional\_universe.fictional\_character.gender ns:m.05zppz .\\ \}} \end{minipage}\\
    Predicted \spql: & \begin{minipage}[t]{0.75\columnwidth}\textsf{SELECT DISTINCT ?x WHERE \{\\\quad ns:m.0h34n \textbf{ns:fictional\_universe.fictional\_character.parents} ?x .\\\quad ?x ns:people.person.gender ns:m.02pn7 .\\\quad ?x ns:people.person.parents ?sk0 .\\ \} ORDER BY xsd:datetime(?sk0) LIMIT 1 }\end{minipage}\\
    Comments: & \begin{minipage}[t]{0.75\columnwidth} In this example the prediction has an incorrect structure, so aligning an edge does not change the outcome. \end{minipage}\\
    \midrule
     & CWQ\\
    \hline
    Question: & \begin{minipage}[t]{0.70\columnwidth} What political leader runs the country where the Panama m.05qx1 nian Balboa m.0200cp is used? \end{minipage}\\
    Predicted \spql: & \begin{minipage}[t]{0.75\columnwidth}\textsf{SELECT DISTINCT ?x WHERE \{\\\quad ?c ns:location.country.currency\_formerly\_used ns:m.0200cp .\\\quad ?c ns:government.governmental\_jurisdiction.governing\_officials ?y .\\\quad
    ?y ns:government.government\_position\_held.office\_holder ?x .\\\quad \ldots \textit{benign filters} \ldots \}\\\quad }\end{minipage}\\
    Ground-truth \spql: & \begin{minipage}[t]{0.75\columnwidth}\textsf{SELECT DISTINCT ?x WHERE \{\\\quad ?c ns:location.country.currency\_used ns:m.0200cp .\\\qquad ?c ns:government.governmental\_jurisdiction.governing\_officials ?y .\\\quad ?y ns:government.government\_position\_held.office\_holder ?x .\\\quad \textbf{?y ns:government.government\_position\_held.office\_position\_or\_title ns:m.0m57hp6} .\\\quad \ldots \textit{benign filters} \ldots \}\\\quad }\end{minipage}\\
    Revised \spql: & \begin{minipage}[t]{0.75\columnwidth}\textsf{SELECT DISTINCT ?x WHERE \{\\\quad ?c \textbf{ns:location.country.currency\_used} ns:m.0200cp .\\\quad ?c ns:government.governmental\_jurisdiction.governing\_officials ?y .\\\quad
    ?y ns:government.government\_position\_held.office\_holder ?x .\\\quad \ldots \textit{benign filters} \ldots \}\\\quad }\end{minipage}\\
    Target Answers: & \{m.06zmv9x\} \\
    Revised Answers: & \{m.02y8\_r, m.06zmv9\}\\
    Comments: & \begin{minipage}[t]{0.70\columnwidth} The original prediction has missing clauses so alignment produces more answers than target program\end{minipage}\\
    \bottomrule
    \end{tabular}
    \caption{Examples of failed alignment with TransE from the Revise stage.}
    \label{tab:revise_eye_candies_neg_appendix}
\end{table*}



\section{Entity Linking Errors}
Identifying relevant entities is an important step in generating \spql queries that yield correct answers. Missing or incorrect entities can drastically change the \spql results and therefore, has a significant impact on the final scores. Our analysis shows that a large portion of errors are cascaded from entity linking. In WebQSP, 43.35\% of incorrect \spql queries are from missing or incorrect entities being copied. In ComplexWebQuestions, the corresponding number is 37.29\%. 
 
\end{document}
