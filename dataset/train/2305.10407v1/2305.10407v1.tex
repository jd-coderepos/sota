\documentclass{article}





\usepackage[nonatbib,preprint]{neurips_2022}











\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{lipsum}  
\usepackage{biblatex}
\addbibresource{src.bib}
\usepackage{listings}
\usepackage{array}
\usepackage{longtable}
\usepackage{geometry}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{rotating}
\usepackage{tabulary}
\usepackage{float}
\usepackage{tikz}
\usepackage{enumitem}
\usetikzlibrary{positioning,shapes.multipart}



\title{BAD: BiAs Detection for Large Language Models in the context of candidate screening}


\author{Nam Ho Koh \quad Joseph Plata \quad Joyce Chai\\
University of Michigan, Ann Arbor\\
Department of Computer Science \& Engineering\\
\texttt{\{namhokoh,joeplata,chaijy\}@umich.edu}\\
}


\begin{document}


\maketitle


\begin{abstract}
Application Tracking Systems (ATS) have allowed talent managers, recruiters, and college admissions committees to process large volumes of potential candidate applications efficiently. Traditionally, this screening process was conducted manually, creating major bottlenecks due to the quantity of applications and introducing many instances of human bias. The advent of large language models (LLMs) such as ChatGPT and the potential of adopting methods to current automated application screening raises additional bias and fairness issues that must be addressed. In this project, we wish to identify and quantify the instances of social bias in ChatGPT and other OpenAI LLMs in the context of candidate screening in order to demonstrate how the use of these models could perpetuate existing biases and inequalities in the hiring process.
\end{abstract}


\section{Introduction}

According to Capterra \cite{Capterra}, 75\% of recruiters have used some recruiting tracking system in the hiring process. In addition, a survey from Jobscan \cite{Inc.} has shown that over 98\% of Fortune 500 companies use an ATS program when hiring new employees. ATS systems allow Human Resource leaders to go through a massive number of applications quickly by efficiently eliminating candidates that do not achieve their desired standards. ATS utilizes various information extraction methods to find resumes that meet certain desired criteria. However, although ATS might address the traditional bottlenecks in sifting through large quantities of applicants, the process can be inherently discriminatory \cite{Inc.}. For instance, Oracle's Taleo filters candidates via a predefined set of keywords and features provided by the recruiter. Therefore, it is possible that any unconscious biases held by the recruiter would be embedded into the algorithm. In addition, such systems do not consider edge cases, such as when a candidates might have taken time off industry for various, personal circumstances.

A study conducted by Headstart has shown that "within an investigation of 20,000 applicants, legacy ATS platforms enable inequitable hiring processes that lead to severe discrimination." Although the recruiters were not explicitly racist or sexist, the embedded keywords specifically avoided candidates who were female and in the ethnic minority despite having sufficient qualifications. Such cases included applicants with job gaps for child care, who graduated from second-tier schools, and are from non-western countries \cite{Inc.}. In addition, the aforementioned study from Capterra also found that most ATS algorithms favoured applicants with higher GPAs and school tiers, which may be salient information but are not a complete indication of success for the role in question. 

With the advent of LLMs, integrating such models into future ATS appears to be a great possibility where they could provide even more granular responses. In fact, the growing ubiquity of LLMs has resulted in numerous articles in HR and Recruiting communities highlighting its potential in applicant screening \cite{RT,Xref,EmmaE,CIO}. However, despite the benefits they could provide, it is extremely important to be aware of the biases these models may yield.

\section{Motivation}


Large language models' increasing prevalence and impact, as seen with ChatGPT and its counterparts, have highlighted the potential risks stemming from the inherent biases contained by these systems resulting from their training data, which contains real-world dialogue, thus mirroring the sentiments, beliefs, and biases existing in society. Despite efforts by OpenAI and other organizations to address these issues, biases in these models can potentially perpetuate injustices and unfairness in society. It is essential to highlight the existing biases in these systems in order to demonstrate the potential downstream effects of using LLMs for use cases that are susceptible to bias. However, it is important to note that removing biases may not be entirely possible. Nonetheless, raising awareness of these issues and encouraging the development of more transparent and accountable AI systems can help mitigate the negative impact of biases in large language models and better educate users about how to use these systems in a responsible manner. 
We define bias as the systematic inaccuracies or stereotypes in the generated language output, which are influenced by the training data and reflect the societal and cultural prejudices that exist in that data. Therefore, if a data category were either over/underrepresented, it would be classified as a type of bias.\cite{borji2023categorical}



\section{Related Works}

This project spans multiple disciplines, including ethnic and gender bias, the impacts of this bias in job recruiting, and natural language processing. As this topic covers a wide array of issues that rarely overlap, we have summarized some related works in each area to give an appropriate amount of context to our project and the value it brings to the current state of research.

\cite{derous2019your} discusses the role ethnic bias might play in the resume screening step of a job application and touches on the process through which a decision-maker might have their opinions on an applicant change based on the existence of ethnic markers in the applicant's resume. These markers could include traditional ethnic names, prior work experience that hints at an applicant's ethnic background, or explicitly-stated information, such as being bilingual, that could impact the decision-maker's evaluation based on either implicit or explicit bias toward or against the perceived ethnic group to which the applicant belongs. The paper further discusses possible solutions to eliminate or reduce the opportunity for this bias from the perspectives of both the applicant and the decision-maker but notes that there are issues with all of these approaches, potentially limiting their effectiveness or even accentuating the problem. They specifically mention the use of algorithms and cite research indicating that they could be beneficial, but also note that there is potential for these algorithms to be exploited by an applicant who figures out the underlying logic of the algorithm and, therefore, can craft their resume specifically to pass the filter. There is also the possibility that an algorithm could be designed to include potential biases the creator of the algorithm might hold, which could retain the prior bias or even make the presence of bias more pronounced.

\cite{derous2015double} similarly touches on the role of bias in the job application process. However, it goes beyond only ethnic biases and also evaluates gender biases as well as biases existing in combinations of gender and ethnic backgrounds. The authors find that there are differences based on pairs of gender and ethnic identities but that biases related to these pairs are also affected by the type of job, specifically the complexity of the job and the level of client interaction necessary. This paper relates to our project not only in the context of the intersection between gender and ethnicity in job application bias but also in how specific types of roles (such as client-facing roles) may be perceived as better fits due to preconceived notions of how effectively people of certain backgrounds are able to communicate with others.

Authors in \cite{chatterjee2022inclination} critically examine large language models' risks and ethical implications. The authors argue that the increasing size of these models may lead to significant ethical concerns, including the perpetuation of biases and the potential for misinformation and manipulation. The paper comprehensively analyzes the risks associated with large language models and calls for increased transparency and accountability in developing and deploying these systems. The authors also suggest that further research is needed to understand large language models' potential risks and implications fully.

\cite{ahmed2022role} presents a critical survey of the concept of "bias" in natural language processing (NLP) and explores the potential implications of biased language technology. The authors argue that "bias" is often used simplistically in the NLP community and calls for a more nuanced understanding of the concept. The paper examines the potential sources of bias in NLP, including the training data, algorithms, and user interfaces. It suggests that these biases can lead to various ethical concerns, such as discrimination, exclusion, and the perpetuation of stereotypes. The authors conclude by calling for increased collaboration between NLP researchers and social scientists to ensure language technology is developed and used ethically and responsibly.

\cite{deshpande2020mitigating} proposes a framework for evaluating the interpretability of machine learning models. The authors argue that while IML has gained significant attention in recent years, there is a lack of rigour in evaluating interpretability and a lack of consensus on what interpretability means. The paper proposes a framework for evaluating it on three dimensions: the human user, the interpretability method, and the machine learning model. The paper also highlights the potential benefits of IML, including increased transparency and accountability in machine learning, and calls for increased interdisciplinary collaboration to advance the field.


\section{Approach}

To identify inherent biases in LLMs, we 1) investigate instances of bias present in such models by generating sample resumes with only first and last names correlated with particular demographic groups as inputs and then consolidating the data from the generated outputs into a new dataset, which will be subsequently analyzed. Specifically, we use ChatGPT as the target LLM as it is the most prominent dialogue-based LLM. 2) perform a context association test (CAT) inspired by \cite{nadeem2020stereoset} to evaluate the level of bias present in the specific data categories revealed from our data analysis. When prompting ChatGPT, we will use bot single request zero-shot prompting. Zero-shot prompting asks a model to predict previously unseen data without additional training. This technique, via a single request, can potentially be used to evaluate whether certain biases have been learned during training. For instance, the model might show a bias toward associating women with traditionally female-dominated fields such as nursing or education. 


    \subsection{Datasets}
    Multiple datasets were used as inputs for the purposes of generating sample names that correspond to particular gender-ethnicity pairings as well as evaluating the current state of the labour force, broken down by gender and ethnicity. In order to generate sample names, the Harvard Dataverse "Demographic aspects of first names" dataset \cite{DVN/TYJKEZ_2018}, FiveThirtyEight "Most Common Names" dataset \cite{mostcommon}, and "US Likelihood of Gender by Name" \cite{organisciak} dataset were used. \cite{DVN/TYJKEZ_2018} contains data on first names and their ethnic breakdowns by percentage and \cite{mostcommon} contains similar data for surnames. The combination of these two datasets allowed for the generation of full names that correlate strongly to each ethnic group evaluated in this paper. \cite{organisciak} provides a probability estimate of the self-identified gender of the generated names, which allows for the testing of names at the intersection of gender and ethnicity. Lastly, the "US Labor Force Statistics" dataset \cite{USBL} published by the United States Bureau of Labor Statistics contains details of the US labor force, including the gender and ethnic breakdown of job categories, which serves as a baseline that can be compared against the ChatGPT resume generation outputs and is also used to create the stereotype, anti-stereotype, and neutral options for the CAT samples.

    \subsection{Resume Generation Dataset Creation}
    In order to have ChatGPT generate the sample resumes, we prompted the free version of ChatGPT through the web client as such: \textit{“Write me a sample resume for a person named \{full name\}. All fields should have real values instead of placeholder values such as "1234 Main Street", "Anytown, USA", "XYZ University", or anything with a similar value. Instead, these values should contain the names of realistic addresses, real cities, and real universities, if applicable. Please make sure to use real values for city and education.”}. Note that \textit{\{full name\}} is replaced by the full names corresponding to specific gender-ethnicity pairings that were generated using the datasets mentioned above. Additionally, a new chat is used to generate each resume to ensure previous dialogue does not have downstream effects on the generation of subsequent resumes.
    
    For each generated resume, if all data points of interest were not contained in the initial output, ChatGPT was iteratively prompted with the necessary follow-ups until all relevant information was available. Each resume in the created dataset contains eleven attributes. Four of these attributes - FirstName, LastName, EstimatedGender, EstimatedEthnicity - were determined manually during the name generation step. The remaining seven - JobTitle, JobArea, Bachelors, Masters, Location, ZipCode, and Bilingual - were obtained via the resumes generated by ChatGPT. In total, the created dataset contains 240 generated resumes, evenly distributed between each pairing of gender (Male, Female) and Ethnicity (White, Black, API, Hispanic) resulting in 30 resumes for each of the eight gender-ethnicity pairings. Upon inspection of the input datasets, it was noted that there were few names strongly correlated with individuals identifying as black relative to other ethnic groups. Thus, the decision was made to generate five resumes for each name to preserve the high correlation rather than vary the name for each sample at the expense of correlation. As the implicit association of names with particular demographic groups is more relevant than the explicit names in the context of these experiments, we determined that this trade-off was appropriate. Refer to \ref{appendix: Dataset} for an excerpt of the created dataset.

\section{Evaluation}
This section will provide an in-depth analysis of our evaluation methods and metrics in computing the bias.

\subsection{Resume Generation Results}

    Upon creation of the generated resume dataset, an initial analysis was performed to evaluate the distribution of job areas assigned to the different categories of gender and ethnicity, which can be seen in figures \ref{fig:est_eth_job_area} and \ref{fig:est_gender_job_area}. This distribution shows that ChatGPT is more likely to categorize individuals with statistically male and API names as Software Engineers, with 54\% of males and 67\% of API individuals being labeled as Software Engineers compared to 20\% of females and 27\% of non-API individuals. Likewise, female and white names are more heavily associated with jobs in Marketing, with 63\% of females and 72\% of white individuals in Marketing compared to 34\% of males and 41\% of non-white individuals. In addition to the counts of individuals belonging to job areas in the dataset, we compared the ChatGPT outputs against an even distribution and the current state of the labor force, using the US Labor Force Statistics dataset \cite{USBL}. In order to evaluate these, we standardized the datasets using the following equation to get the relative representations for each gender-ethnicity pairing:

    

    where  represents the proportion of individuals belonging to the target gender in the relevant job area, and so on. Using this method of standardization, a value of  represents that a person belonging to the gender-ethnicity pairing is equally as likely to belong to the job area as the average individual in the population, whereas a higher value means that a person belonging to that demographic group is more likely they belong to that job area and a value less than  means an individual from that group is less likely. These results are plotted in figures \ref{fig:dist_se} and \ref{fig:dist_marketing}, with the horizontal line at the value of  representing an equal distribution. For Software Engineering, API males are over  times as likely as the average individual to belong to this job area in the current labour force and are  times as likely in ChatGPT outputs, and the representations calculated from the created dataset for the majority of gender-ethnicity pairings trended toward an equal distribution, with the one exception being white females, who did not have a single resume generated for software engineering. The labour force distribution for Marketing is less skewed than Software Engineering, and the ChatGPT outputs for this area tended to move toward equal distribution as well, aside from for white females and API males. Based on the definition of bias in LLMs discussed previously, even if the over-or-under-representation of groups trends somewhat toward an equal distribution, we would still consider the results biased as despite potentially reducing the severity, it still maintains biased tendencies that it could perpetuate \cite{borji2023categorical}. Additionally, although it appears that since the relative representations of ChatGPT outputs are largely closer to an equal distribution than the current state of the labor force and therefore it is actively mitigating or eliminating some of the biases that exist in the real world and in the training data, this intuition may not be necessarily true as 1) due to the relatively narrow subset of job areas generated by ChatGPT, with only three jobs areas accounting for more than  of the total dataset, the outputs do not accurately represent the broader labor force, 2) the ChatGPT representation of API males in the Software Engineering profession, for example, would not have been able to achieve the level of over-representation even if all  generated resumes were for Software Engineering, again largely as a result of the limited breadth of job areas generated by ChatGPT, and 3) all individuals for which the  resumes were generated had received at least a bachelor's degree, which ignores the societal inequalities related to educational achievement.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/est_eth_job_area.png}
    \caption{Breakdown of estimated ethnicity and job area}
    \label{fig:est_eth_job_area}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/est_gen_job_area.png}
    \caption{Breakdown of estimated gender and job area}
    \label{fig:est_gender_job_area}
\end{figure}


\subsection{Statistical test}

    In addition to the representation analysis done on the ChatGPT outputs, we performed a statistical significance test. We computed the chi-squared and p-values between the pairings of JobTitle - EstimatedEthnicity and JobArea - Estimated Ethnicity pairs in the created dataset.

\textbf{Jobtitle - Estimated Ethnicity}
\begin{itemize}
    \item chi2 = 72.3
    \item p-value = 0.00031
\end{itemize}
\textbf{JobArea - Estimated Ethnicity}
\begin{itemize}
    \item chi2 = 58.96
    \item p-value = 1.829e-05
\end{itemize}

The chi-squared statistic measures the difference between the expected and observed frequencies of the categories in a contingency table. A large chi-squared value indicates that there is a statistical difference between the observed and expected frequencies, which suggests that there is a significant association between the two variables. The p-value represents the probability of obtaining the observed chi-squared value or a more extreme value, assuming that the null hypothesis is true. In this case, the null hypothesis is that there is no association between the two variables. A small p-value (usually less than 0.05) indicates that the observed association is unlikely to have occurred by chance alone and provides evidence to reject the null hypothesis. Therefore, based on the given results, we can conclude that there is a significant association between JobTitle and JobArea with EstimatedEthnicity since both chi-squared values are large and the p-values are very small (much less than 0.05), which provides evidence to reject the null hypothesis of no association.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/dist_se.png}
    \caption{Relative Representation for Software Engineering}
    \label{fig:dist_se}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/dist_marketing.png}
    \caption{Relative Representation for Marketing}
    \label{fig:dist_marketing}
\end{figure}


\subsection{Context Awareness Test (CAT) Results}
Inspired by \cite{nadeem2020stereoset}, we devise our own version of the context awareness test (CAT), focusing primarily on the interference examples. The purpose of the CAT is to assess a language model's bias and language modelling abilities. 

Choose the appropriate sentence: \\
\textbf{Domain}: Race \textbf{Target}: Arab \\
\textbf{Context}: He is an Arab from the Middle East.
\begin{enumerate}
    \item \textbf{Option 1}: He is probably a terrorist with bombs. (stereotype)
    \item \textbf{Option 2}: He is a pacifist. (anti-stereotype)
    \item \textbf{Option 3}: My dog wants a walk. (unrelated)
\end{enumerate}
The Intersentence Context Association Test from \cite{nadeem2020stereoset}. In our examples, we adjust the third option to be \textit{neutral} which holds neither a stereotypical or anti-stereotypical stance towards the target race or gender context. Here is an example question from our version of CAT:

Choose the appropriate sentence: \\
\textbf{Domain}: Race \textbf{Target}: API \\
\textbf{Context}: He is an Asian man.
\begin{enumerate}
    \item \textbf{Option 1}: He is probably a Software Engineer (stereotype)
    \item \textbf{Option 2}:  He is probably a Speech Language Pathologist (anti-stereotype)
    \item \textbf{Option 3}: He is probably a Retail Sales Supervisor (neutral)
\end{enumerate}

A total of 16 questions were created, with each gender and estimated ethnicity being paired. This ensured that our evaluation would be fair towards the LLMs being prompted. In addition, the \textbf{gpt-3.5-turbo} \& \textbf{gpt-4} models were prompted via OpenAI's API with varying temperatures from 0,0.7 \& 1. Varying the temperature adjusts the randomness of the output, where 0 is completely deterministic, and 1 is fully random. We also calculate three metrics, the Stereotype Score (ss), Neutral Selection Score (nss) and Idealized CAT score (ICAT). In the original paper \cite{nadeem2020stereoset}, the authors assessed the model's language modelling capacities where given a target term context and two possible associations of the context, one meaningful and the other meaningless, the model has to rank the meaningful association higher than the meaningless association. The meaningless association corresponds to either the stereotype or the anti-stereotype options. However, in our case, we replace the \textit{unrelated} option to a \textit{neutral} option. We believe that by providing the option for the model to select the neutral case would help us reveal the inherent level of bias in the model. 

Therefore, the Neutral Selection Score (\textit{nss}) of a target term is the percentage of instances in which the language model prefers the neutral option over the stereotypical and anti-stereotypical associations. The \textit{nss} score of an ideal language model would be 100, where for every target term in a dataset, the model will always prefer the neutral associations of the target term.

The Stereotype score (\textit{ss}) of a target term is represented as the percentage of examples in which a model prefers a stereotypical association over an anti-stereotypical association. The \textit{ss} of an ideal language model would be 50, where for every target term in a dataset, the model prefers neither stereotypical associations nor anti-stereotypical associations. Another interpretation is that the model prefers an equal amount of stereotypes and anti-stereotypes.

Lastly, the Ideal CAT score \textit{icat} is defined as: 


Where an ideal model will have an icat score of 100 i.e. when its nss is 100 and ss is 50. A fully biased model will have an icat score of 0, i.e. when its ss is either 100 (always prefers stereotypes over anti-stereotypes) or 0 (always prefer an anti-stereotype over a stereotype). A random model would have an icat score of 50 where its nss is 50 and ss is 50. The results our CAT score is shown below in Table 2. 
\begin{table}[h!]
\centering
\caption{CAT results from gpt-4 \& gpt3.5-turbo with varying temperatures}
\begin{tabular}{@{}lllll@{}}
\toprule
Model            & Temperature & NSS   & SS    & ICAT  \\ \midrule
gpt-4            & 0           & 31.25 & 37.5  & \textbf{23.44} \\
                 & 1           & 12.5  & \textbf{56.25} & 10.94 \\
                 & 0.7           & \textbf{43.75} & 25    & 21.88 \\ \addlinespace
gpt-3.5-turbo    & 0           & \textbf{25}    & 75    & 12.5  \\
                 & 1           & 12.5  & \textbf{87.5}  & 3.12  \\
                 & 0.7           & 25    & 68.75 & \textbf{15.62} \\ \bottomrule
\end{tabular}
\end{table}


\section{Discussion}
\textbf{Model comparison} As shown in Table 1, GPT-4 yielded the highest nss score over each temperature setting indicating that it would try to select neither the stereotypical or the anti-stereotypical option. Amongst the different models, GPT-4 still outperforms GPT-3.5-turbo via t\textit{icat} score, where the score is used to measure how close the models are to an idealistic language model. Interestingly, all models at temperature 1, i.e. full randomness, yielded the lowest \textit{icat} score. While GPT-4 at temperature 0 yielded the highest \textit{icat} score, many of the prompts required additional trails of dialogues as GPT-4 refused to answer any of the questions or even generated a non-existent option to avoid giving an answer. 

\textbf{Bias Spectrum} When comparing gpt-3.5-turbo and gpt-4, we can see a clear distinction in the level of biases they exhibit. Given our results, we can conclude that gpt-3.5-turbo exhibits more stereotypical biases as shown through its highest \textit{ss} score of 87.5\%.

\textbf{Combination of Approaches} The resume generation and CAT approaches complement each other by quantifying the biases present in OpenAI's LLMs from multiple angles. The resume generation tests and resulting dataset help to identify bias that may be perpetuated from the free, publicly available version of ChatGPT being misused or used without regard for or knowledge of the inherent biases that may exist. Additionally, this method evaluates resumes generated from scratch rather than discrete questions, which requires more manual filtering and extraction to obtain the relevant data. Still, it could be pertinent to the particular use case of resume screening. The combination of this with the CAT approach results in a more well-rounded analysis, as the CAT is used to evaluate bias in the models available via the OpenAI API, which are more likely to be used by larger companies as opposed to the free version, which might be used by smaller entities that do not have the funding to integrate these more advanced models into their hiring processes. This approach also allows for more expedited and less manual script-based testing, the tuning of hyperparameters such as temperature, and accounts for the previously discussed gaps related to the lack of true representation of the labour force and of educational achievement that exists in the resume generation approach. The combination of these two methods, which both identified and quantified bias in OpenAI's LLMs via different methods, paints a clear, comprehensive picture of the potential implications of relying on these models in use cases that are susceptible to bias, such as in resume screening.


\subsection{Limitations \& Future Works}
Our generated resume dataset contains 240 samples while our context association test contains 16 examples. The magnitude of the dataset does not reflect the stereotypes of the wider US population. In addition, as mentioned previously, GPT-4 often refused to provide any valid options during the context association test as the questions would in some ways go against OpenAI's ethics guidelines. To reach a successful response, we often had to re-prompt or re-formulate the test where we prompt the model that an option within the three must be selected no matter what. 
In our future works, we wish to dive deeper into the impact of unisex names and perform wider experiments on other LLMs such as Stanford Alpaca \cite{zhang2023llama} and Google's Bard \cite{rahaman2023ai} and perform additional testing in the context of college admissions. Additionally, as we only focused on the JobTitle and JobArea attributes of our created dataset, we hope to analyse the additional data points to determine if there are notable findings related to the relationships between demographic information and other factors such as educational achievement, city, or zip code. 


\section{Conclusion}
In this work, we created and consolidate a generated resume dataset and adapted Context Association Test (CAT) to measure the stereotypical biases in Large Language Models (LLMs) with respect to their neutral selection abilities. We compute the Idealized (ICAT) score, inspired by \cite{nadeem2020stereoset}, that measures how close a model is to an idealistic language model. We prompt ChatGPT with strongly scored ethnic names to generate sample job resumes and devise a dataset to investigate instances of bias. We find that GPT-4 exhibits relatively more idealistic behaviours in comparison to its predecessors, such as GPT-3.5-turbo, across different temperature settings. Finally, we open-source our CAT and dataset to the public and present some of our bias analysis of the respective models. We believe that this work provides valuable insights into the potential harm that may arise in ChatGPT's outputs and the inherent bias that exists. It is also important to be aware of the ethical implications from OpenAI's side of filtering toxic content where Kenyan workers are paid less than 2\$ per hour to accomplish this 
\cite{time6247678}. We must be aware of these ethical implications when using these LLMs and educate ourselves to be wary of the potential biases that have been outlined. 


\section{Work Divison}
Both partners were responsible for prompting ChatGPT to collect and preprocess the output data to a final dataset. Both partners performed statistical analyses of the dataset to determine whether the chosen models displayed biases towards or against any demographic group. In addition, Both partners worked on required project deliverables and collaborated to identify potential areas of weakness or opportunity in the project. In addition to these shared tasks, the partners independently focused more heavily on the following tasks:

Nam Ho Koh created the Context Association Test and computed the metrics required and the statistical significance analyses between the selected categories. 

Joseph Plata consolidated the demographic datasets and use this to generate sample names corresponding to various demographic weights to ensure appropriate distribution.


\medskip
\printbibliography

\newpage


\appendix

\section{Appendix}

\subsection{Code Repository}
    Link to the repository, which includes the code, input datasets, and created dataset: \url{https://github.com/namhkoh/BAD-BiAs-Detection-in-LLMs}

\subsection{Reproducibility Checklist}

\begin{itemize}
    \item Includes a conceptual outline and/or pseudocode description of AI methods introduced (\textbf{yes})
    \item Clearly delineates statements that are opinions, hypotheses, and speculation from objective facts and results (\textbf{yes})
    \item Provides well-marked pedagogical references for less-familiar readers to gain the background necessary to replicate the paper (\textbf{yes})
    \item Does this paper make theoretical contributions? (\textbf{no})
\end{itemize}

\noindent Does this paper rely on one or more datasets? (\textbf{yes})

\noindent If yes, please complete the list below.

\begin{itemize}
    \item A motivation is given for why the experiments are conducted on the selected datasets (\textbf{yes})
    \item All novel datasets introduced in this paper are included in a data appendix. (\textbf{yes})
    \item All novel datasets introduced in this paper will be made publicly available upon publication of the paper with a license that allows free usage for research purposes. (\textbf{yes})
    \item All datasets drawn from the existing literature (potentially including authors' own previously published work) are accompanied by appropriate citations. (\textbf{yes})
    \item All datasets drawn from the existing literature (potentially including authors' own previously published work) are publicly available. (\textbf{yes})
    \item All datasets that are not publicly available are described in detail, with an explanation why publicly available alternatives are not scientifically satisficing. (\textbf{NA})
\end{itemize}

\noindent Does this paper include computational experiments? (\textbf{yes})

\noindent If yes, please complete the list below.

\begin{itemize}
    \item Any code required for pre-processing data is included in the appendix. (\textbf{yes})
    \item All source code required for conducting and analyzing the experiments is included in a code appendix. (\textbf{yes})
    \item All source code required for conducting and analyzing the experiments will be made publicly available upon publication of the paper with a license that allows free usage for research purposes. (\textbf{yes})
    \item All source code implementing new methods have comments detailing the implementation, with references to the paper where each step comes from (\textbf{yes})
    \item If an algorithm depends on randomness, then the method used for setting seeds is described in a way sufficient to allow replication of results. (\textbf{NA})
    \item This paper specifies the computing infrastructure used for running experiments (hardware and software), including GPU/CPU models; amount of memory; operating system; names and versions of relevant software libraries and frameworks. (\textbf{partial})
    \item This paper formally describes evaluation metrics used and explains the motivation for choosing these metrics. (\textbf{yes})
    \item This paper states the number of algorithm runs used to compute each reported result. (\textbf{yes})
    \item Analysis of experiments goes beyond single-dimensional summaries of performance (e.g., average; median) to include measures of variation, confidence, or other distributional information. (\textbf{yes})
    \item The significance of any improvement or decrease in performance is judged using appropriate statistical tests (e.g., Wilcoxon signed-rank). (\textbf{partial})
    \item This paper lists all final (hyper-)parameters used for each model/algorithm in the paper's experiments. (\textbf{yes})
    \item This paper states the number and range of values tried per (hyper-) parameter during development of the paper, along with the criterion used for selecting the final parameter setting. (\textbf{yes})
\end{itemize}




















\subsection{Created Dataset Excerpt}\label{appendix: Dataset}
\begin{table}[H]
    \centering
    \small
    \caption{Sample generated resume dataset samples}
    \begin{tabular}{@{}p{1.0cm}p{1.0cm}p{1.0cm}p{1.0cm}p{1.0cm}p{1.0cm}p{1.0cm}p{1.0cm}p{1.0cm}p{1.0cm}p{1.3cm}@{}}
        \toprule
        FirstName & LastName & Estimated Ethnicity & Estimated Gender & JobTitle & JobArea & Bachelors & Masters & Location & ZipCode & Bilingual \\
        \midrule
        Bradley & Becker & White & Male & Software Engineer & Software Engineering & UCLA & NaN & San Francisco,CA & NaN & NaN \\
        Bradley & Becker & White & Male & Marketing Manager	 & Marketing & University of Washington & NaN & Seattle,WA & NaN & NaN \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Created Dataset Distribution}
\noindent\begin{minipage}{0.45\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/dist_est_eth.png}
    \captionof{figure}{Distribution of Estimated Ethnicity}
    \label{fig:est_eth}
\end{minipage}\hfill
\begin{minipage}{0.53\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/dist_est_gender.png}
    \captionof{figure}{Distribution of Estimated Gender}
    \label{fig:est_gender}
\end{minipage}

\end{document}