\section{Experiments}
\label{sec:experiments}

\subsection{Datasets}
Since there are no public benchmarks for point cloud upsampling, we collect a dataset of 60 different models from the Visionair repository~\cite{timmurphy.org}, ranging from smooth non-rigid objects (\eg, Bunny) to steep rigid objects (\eg, Chair).
Among them, we randomly select 40 for training, and use the rest for testing\footnote{The complete object list can be found in the supplementary material.}.
We crop 100 patches for each training object, and we use  patches to train the network in total.
For testing objects, we use Monte-Carlo random sampling approach to sample 5000 points on each object as input.
To further demonstrate the generalization ability of our network, we directly test our well-trained network on the SHREC15~\cite{Lian2015} dataset, which contains 1200 shapes from 50 categories.
In detail, we randomly choose one model from each category for testing, considering that each category contains 24 similar objects in various poses. 
As for ModelNet40~\cite{wu20153d} and ShapeNet~\cite{chang2015shapenet}, we found it hard to extract patches from those objects due to the low mesh quality (\eg, holes, self-intersection, etc.).
Therefore, we use them for testing; see the supplementary material for the results.

\subsection{Implementation Details}
\label{sec:implementationdetails}
The default point number  of each patch is 4096, and the upsampling rate  is 4. Therefore, each input patch has 1024 points.
To avoid overfitting, we augment the data by randomly rotating, shifting and scaling the data.
We use 4 levels with grouping radii 0.05, 0.1, 0.2 and 0.3 in the point feature embedding component, and the dimension  of the restored feature is 64.  For details on other network architecture parameters, please see our supplementary material. 
Parameters  and  in repulsion loss are set as 5 and 0.03, respectively.
The balancing weights  and  are set as 0.01 and  = , respectively.
The implementation is based on TensorFlow\footnote{ \url{https://github.com/yulequan/PU-Net}}.
For the optimization, we train the network for 120 epoch using the Adam~\cite{kingma2014adam} algorithm with a minibatch size of 28 and a learning rate of 0.001.
Generally, the training took about 4.5h on the NVIDIA TITAN Xp GPU. 

\begin{figure}[!t]
	\centering
	\includegraphics[width=0.9\linewidth]{figures/metric}\vspace{1mm}
	\caption{Example point distributions with corresponding normalized uniformity coefficients (NUC) computed with .}
	\label{fig:metric}\vspace{-2mm}
\end{figure}



\subsection{Evaluation Metric}
\label{sec:metric}

To quantitatively evaluate the quality of the output point sets, we formulate two metrics to measure the deviation between the output points and the ground truth meshes, as well as the distribution uniformity of the output points.
For surface deviation, we find the closest point  on the mesh for each predicted point , and calculate the distance between them. 
Then we compute the mean and standard deviation over all the points as one of our metrics. 

As for the uniformity metric, we randomly put  equal-size disks on the object surface ( in our experiments) and calculate the standard deviation of the number of points inside the disks. We further normalize the density of each object and then compute the overall uniformity of the point sets over all the objects in the testing dataset.
Therefore, we define the \emph{normalized uniformity coefficient} (NUC) with disk area percentage  as:

where  is the number of points within the -th disk of the -th object,  is the total number of points on the -th object,  is the total number of test objects, and  is the percentage of the disk area over the total object surface area. 
Note that we use geodesic distance rather than Euclidean distance to form the disks.
Fig.~\ref{fig:metric} shows three different point distributions with their corresponding NUC values.
As we can see, the proposed NUC metric can effectively reveal the point set uniformity: the lower the UNC value, the more uniform the point set distribution is.



\subsection{Comparisons with Other Methods}
\label{sec:comparison}

\newcommand{\BE}[1]{{\textbf{#1}}}
\begin{table*}
	\caption{Quantitative comparison on our collected dataset.}
	\label{tab:comparision1}
	\centering
	\begin{center}
		\begin{tabular}{l|c|c|c|c|c|c||c|c||c} \toprule[1pt]
			\multirow{2}*{Method}& \multicolumn{6}{c||}{NUC with different } & \multicolumn{2}{c||}{Deviation ()}  & \multirow{2}*{Time (s)}\\
			\cline{2-9} 
			&0.2\%	&0.4\%	&0.6\%	&0.8\% 	&1.0\%	&1.2\%  	& mean  & std 	& \\ \hline \hline
			Input 		&0.316	&0.224	&0.185	&0.164	&0.150	&0.142		& -		&-		&-\\ \hline
			PointNet~\cite{qi2016pointnet}		&0.409	&0.334	&0.295	&0.270	&0.252	&0.239		&2.27	&3.18	&\BE{0.05}\\ \hline
			PointNet++~\cite{qi2017pointnet++}	&0.215	&0.178	&0.160	&0.150	&0.143	&0.139	&1.01	&0.83	&0.16\\ \hline
			PointNet++(MSG)~\cite{qi2017pointnet++}	&0.208	&0.169	&0.152	&0.143	&0.137	&0.134	&0.78	&0.61	&0.45\\ \hline
			PU-Net (Ours)			&\BE{0.174}&\BE{0.138}&\BE{0.122}&\BE{0.115}&\BE{0.112}&\BE{0.110}&\BE{0.63}	&\BE{0.53}	&0.15\\ \bottomrule[1pt]
		\end{tabular}
	\end{center}
	\vspace{-2.5mm}
\end{table*}

\begin{table*}
	\caption{Quantitative comparison on SHREC15 dataset.}
	\label{tab:comparision2}
	\centering
	\begin{center}
		\begin{tabular}{l|c|c|c|c|c|c||c|c||c} \toprule[1pt]
			\multirow{2}*{Method}& \multicolumn{6}{c||}{NUC with different } & \multicolumn{2}{c||}{Deviation ()}  & \multirow{2}*{Time (s)}\\
			\cline{2-9} 
			&0.2\%	&0.4\%	&0.6\%	&0.8\% 	&1.0\%	&1.2\%  	& mean  & std 	& \\ \hline \hline
			Input 								&0.315	&0.222	&0.184	&0.165	&0.153	&0.146		& -		&-		&-\\ \hline
			PointNet~\cite{qi2016pointnet}		&0.490	&0.405	&0.360	&0.330	&0.309	&0.293		&2.03	&2.94	&\BE{0.05}\\ \hline
			PointNet++~\cite{qi2017pointnet++}	&0.259	&0.218	&0.197	&0.185	&0.176	&0.170		&0.88	&0.75	&0.16\\ \hline
			PointNet++(MSG)~\cite{qi2017pointnet++}	&0.250	&0.199	&0.175	&0.161	&0.153	&0.148	&0.69	&0.59	&0.45\\ \hline
			PU-Net (Ours)								&\BE{0.219}	&\BE{0.173}	&\BE{0.154}	&\BE{0.144}	&\BE{0.140}	&\BE{0.137} 		&\BE{0.52}	&\BE{0.45}	&0.15\\ \bottomrule[1pt]	
		\end{tabular}
	\end{center}
	\vspace{-4.5mm}
\end{table*}

\begin{figure}[t]
	\centering
	\includegraphics[width=1.0\linewidth]{figures/ear.pdf}\vspace{1mm}
	\caption{Comparison with the EAR method~\cite{huang2013edge}.
We color-code all point clouds to show the deviation from the ground truth mesh.
For the EAR method, the radius of the Chair model is 0.1 and 0.182, while the radius of the Spider model is 0.106 and 0.155.}
	\label{fig:ear}
	\vspace{-2mm}
\end{figure}

\para{Comparison with an optimization-based method.} \
We compare our method with the Edge Aware Resampling (EAR) method~\cite{huang2013edge}, which is a state-of-art method for point cloud upsampling.
The results are shown in Fig.~\ref{fig:ear}, where the  Chair is from our collected testing dataset and the Spider is from SHREC15.
We color-code the point clouds to show the deviation from the ground truth meshes.
There are 1024 points in the input and we do a 4X upsampling.
Since EAR relies on the normal information, to be fair, we calculate the normal according to the ground truth mesh. 
We show two results of EAR with increasing radius, while setting other parameters to their default values.
As we can see, the radius parameter has a great influence on EAR's performance. 
For relatively small radius, the output has low surface deviation but the added points are not uniform, while more outliers are introduced if the radius is large. 
In contrast, our method can better balance the deviation and uniformity without the need to carefully tune the parameters. 


\para{Comparison with deep learning-based methods.} 
As far as we know, we are not aware of any deep learning-based method for point cloud upsampling, so we design some baseline methods for comparison. 
Since PointNet~\cite{qi2016pointnet} and PointNet++~\cite{qi2017pointnet++} are pioneers for 3D point cloud reasoning with deep learning techniques, we design the baselines based on them.
Specifically, we adopt the semantic segmentation network architecture for point feature embedding and use one set of convolutions for feature expansion. 
Note that we consider two versions of PointNet++: basic PointNet++ and PointNet++ with multi-scale grouping (MSG) for handling non-uniform sampling density; hence, we have three baselines in total, and we train them only with the reconstruction loss.
Please refer to the supplemental material for details of the baseline network architectures.
Although we modify the PointNet, PointNet++, and PointNet++(MSG) architectures for the upsampling problem, for convenience, we still call the baselines by their original names.

Tables~\ref{tab:comparision1} and~\ref{tab:comparision2} list the quantitative comparison results on our collected dataset and the SHREC15 dataset, respectively.
Note that measuring NUC with small  shows local distribution uniformity in small regions, while measuring NUC with large  shows more global uniformity.
Among the baselines, PointNet performs the worst, since it cannot capture local structure information. 
Compared with PointNet++, PointNet++(MSG) can slightly improve the uniformity due to the explicit multi-scale information grouping.  
However, it involves more parameters, thus significantly prolonging the training and testing time.
Overall, our PU-Net achieves the best performance with the lowest deviation from surface and the best distribution uniformity compared to the baselines, especially on the local uniformity.


Fig.~\ref{fig:visualcomp} shows results for visual comparison, where points are colored by their distance deviations from surface.
As we can see, the point clouds predicted by our method better match the underlying surface with lower deviations.  

\begin{figure*}[t]
	\centering
	\includegraphics[width=0.98\linewidth]{figures/deviation.pdf}\vspace{1mm}
	\caption{Visual comparison on samples from our collected dataset (top row) and SHREC (bottom row).
The colors on points (see color map) reveal the surface distance errors, where blue indicates low error and red indicates high error.}
	\label{fig:visualcomp}\vspace{-2mm}
\end{figure*}


\subsection{Architecture Design Analysis}
\label{sec:archtanalysis}

\para{Analyzing the feature expansion.} 
We compare our proposed feature expansion scheme with two interpolation-like schemes.  
The first one is similar to a naive point interpolation (denoted as \emph{interp1}). 
After extracting the point feature of each point, we combine its own features and the features from the nearest neighboring points to generate the upsampled features. 
The second one introduces more randomness (denoted as \emph{interp2}). 
Instead of using the features from the nearest neighbors, we use a radius based ball query to find the neighborhood and combine the features from these points to generate the upsampled features. 
We train these two networks with the reconstruction loss (also named as the EMD loss) and the results are listed in the top two rows of Table~\ref{tab:ablation}.
For fair comparison, we also train our network only with the EMD loss.
The results are shown in the fourth row.  
Comparing with these two interpolation-like schemes, our proposed scheme can generate more uniform outputs with comparable surface distance deviation.


\para{Comparing different loss functions.} 
As mentioned above, the EMD can better capture the object shape than CD.
Comparing their performance in Table~\ref{tab:ablation}, we can see that the EMD loss improves the output uniformity with low surface distance deviation when comparing with the CD loss, meaning that the EMD loss can better encourage the output points to lie on the underlying surface. 
Furthermore, by comparing the last two rows in Table~\ref{tab:ablation}, we can see that the repulsion loss can further improve the uniformity of the output. 

\begin{table}
	\centering
	\caption{Architecture design analysis on our collect dataset.}
	\label{tab:ablation}
	\resizebox{0.5\textwidth}{!}{
		\begin{tabular}{l|c|c|c|c||c|c} \toprule[1pt]
			\multirow{2}*{Methods}	& \multicolumn{4}{c||}{NUC with different } & \multicolumn{2}{c}{Deviation ()} \\
			\cline{2-7} 
			&0.4\%	&0.6\%	&0.8\% 	&1.0\%	 & mean	& std	\\ \hline \hline
			\emph{interp1}+EMD  	&0.153	&0.136	&0.126	&0.121		&0.67	&0.54	\\ \hline
			\emph{interp2}+EMD  	&0.144	&0.129	&0.122	&0.118	&0.71	&0.57	\\ \hline
			CD  				  	&0.185	&0.167	&0.154	&0.147	&0.87	&0.69	 \\ \hline
			EMD					  	&0.140	&0.126	&0.119	&0.116	&0.68	&0.58\\ \hline
			Ours			&\BE{0.138}&\BE{0.122}&\BE{0.115}&\BE{0.112}&\BE{0.63}	&\BE{0.53}\\ \bottomrule[1pt]	
		\end{tabular}
	}
	\vspace{-5.5mm}
\end{table}



\begin{figure*}[t]
	\centering
	\includegraphics[width=0.96\linewidth]{figures/SurfaceReconstruction.pdf}\vspace{1mm}
	\vspace{-2mm}
	\caption{Surface reconstruction results from the upsampled point clouds.}
	\label{fig:surfacereconstruction}
	\vspace{-3.5mm}
\end{figure*} 

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.99\linewidth]{figures/Iterative.pdf}\vspace{1mm}
	\vspace{-1mm}
	\caption{Results of iterative upsampling. We color-code points by the depth information. Blue points are closer to us.}
	\label{fig:iterative}
\end{figure}

\begin{figure*}[!t]
	\centering
	\includegraphics[width=13.5cm]{figures/noise_reconstruction}
	\caption{Surface reconstruction results from noisy input points.}
	\label{fig:noise}\vspace{-2mm}
\end{figure*}



\subsection{More Experiments}
\label{sec:moreresults}

\para{Surface reconstruction from upsampled point sets.} 
An important application of point cloud upsampling is to improve the surface reconstruction quality.
Hence, we compare the reconstruction results of different methods with the direct Poisson surface reconstruction method~\cite{poisson2006} provided in MeshLab~\cite{LocalChapterEvents:ItalChap:ItalianChapConf2008:129-136}; see Fig.~\ref{fig:surfacereconstruction}. 
We can observe that the reconstruction result from our method is the closest to the ground truth, while other methods either miss certain structures (\eg, the leg of the Horse) or overfill the hole.

\para{Results of iterative upsampling.}   
To study the ability of our network to handle varying number of input points, we design an iterative upsampling experiment, which takes the output of the previous iteration as the input of the next iteration.
Fig.~\ref{fig:iterative} shows the results.
The initial input point cloud has 1024 points and we increase fourfold in each iteration.
From the results, we can see that our network can produce reasonable results for different number of input points.
Furthermore, this iterative upsampling experiment also shows the anti-noise ability of our network to resist the accumulated errors introduced in the iterative upsampling.



\para{Results from noisy input point sets.}
Fig.~\ref{fig:noise} shows the surface reconstruction results from noisy point clouds (Gaussian noise of 0.5\% and 1\% of object bounding box diagonal), which demonstrate that our network facilitates the production of better surfaces even with noisy inputs.



\para{Results on real-scanned point clouds.}
Lastly, we evaluated the ability of our network to upsample real-scanned point clouds, which were downloaded from Visionair~\cite{timmurphy.org}.
In Fig.~\ref{fig:realscan}, the left-most column presents the real-scanned point clouds. 
Even though each real-scanned point cloud contains millions of points, the phenomenon of inhomogeneity still exists.
For better visualization, we cut small patches from the original point clouds and show the patches in the middle column.
We can observe that the real-scanned points tend to have line structures, while our network still has the ability to uniformly add points in the sparse regions.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\linewidth]{figures/RealScan.pdf}\vspace{1mm}
	\caption{Results on real-scanned point clouds (Screw nut \& Turtle). We color-code input patches and upsampling results to show the depth information. Blue points are closer to viewpoint.}
	\label{fig:realscan}\vspace{-2mm}
\end{figure}
