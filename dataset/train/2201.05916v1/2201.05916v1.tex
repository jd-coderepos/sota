\section{Experiments}

\begin{table}[t]
\centering
\caption{Evaluations on the Omniglot dataset. }
\label{table2}
\makebox[\linewidth]{
\setlength{\tabcolsep}{0.5em}
\renewcommand{\arraystretch}{0.60}
\begin{tabular}{lcccccc}
\toprule
Model & & Fine & \multicolumn{2}{c}{5-way accuracy} & \multicolumn{2}{c}{20-way accuracy} \\ 
&& Tune & 1-shot & 5-shot & 1-shot & 5-shot  \\ \hline
\textit{Conv. Siamese Nets} &\cite{koch2015siamese} & N &  &  &  &  \\
\textit{Conv. Siamese Nets} &\cite{koch2015siamese} & Y &  &  &  &  \\
\textit{Matching Net} &\cite{vinyals2016matching} & N &  &  &  &  \\
\textit{Prototypical Net} &\cite{snell2017prototypical} & N &  &  &  &  \\ 
\textit{MAML} & \cite{finn2017model} & Y &  &  &  &  \\ 
\textit{Relation Net} & \cite{sung2017learning} & N &  &  &  &  \\ 
\cdashline{1-2}
\textit{SoSN+PN} & & N &  &  & &  \\ 
\textit{MlSo+PN} & & N &  &  &  &  \\ 
\bottomrule
\end{tabular}}
\end{table}

\label{sec:exp}
Below we demonstrate the usefulness of our approach by evaluating it on the Omniglot \cite{lake_oneshot}, \textit{mini}-ImageNet \cite{vinyals2016matching} and \textit{tiered}-ImageNet \cite{ren18fewshotssl}  datasets, a recently proposed Open MIC dataset \cite{me_museum}, fine-grained  Flower102 \cite{Nilsback08}, CUB-200  \cite{WahCUB_200_2011} and Food-101 \cite{bossard14} datasets, and action recognition datasets such as HMDB51 \cite{Kuehne11}, UCF101 \cite{soomro2012ucf101} and \textit{mini}-MIT \cite{soomro2012ucf101}. We evaluate the performance of SoSN and MlSo in both supervised and unsupervised settings to demonstrate their superior performance compared to first-order representations in few-shot learning. 
We train our network with the Adam solver. The layer configurations of our SoSN model are shown in Figure \ref{fig:blocks}.  The results are compared against several state-of-the-art methods in one- and few-shot learning.


\begin{table*}[t]
\centering
\caption{Evaluations on the \textit{mini}-ImageNet dataset (5-way accuracy). {Asterisk `*' highlights that we converted the supervised Prototypical Net and the supervised Relation Net into the unsupervised contrastive pipelines, U-Prototypical Net and U-Relation Net, using the same mechanism (described in Section \ref{sec:un}) as the one applied for U-SoSN and U-MlSo.} }
\label{table3}
\makebox[\linewidth]{
\setlength{\tabcolsep}{2em}
\begin{tabular}{lcccc}
\toprule
\multicolumn{2}{c}{Model} & Backbone & 1-shot & 5-shot \\ \hline
& & & \multicolumn{2}{c}{Supervised Pipelines} \\
\midrule
\textit{Prototypical Net} & \cite{snell2017prototypical} & Conv-4 &  &  \\ 
\textit{MAML} & \cite{finn2017model} & Conv-4 &  &  \\ 
\textit{Relation Net} & \cite{sung2017learning} & Conv-4 &  &   \\ 
\textit{LwoF} & \cite{lowf} & WRN &   &  \\
\textit{GNN} & \cite{gnn} & Conv-4 &  &   \\ 
\textit{Reptile} & \cite{Nichol2018Reptile} & Conv-4 &   &   \\
\textit{Saliency Net} & \cite{zhang2019few} & Conv-4 &  &   \\
\textit{MAML++} & \cite{Antoniou2019Htmaml} & Conv-4 &  &   \\
\textit{TPN} & \cite{liu2018learning} & Conv-4 & 55.51 & 69.86 \\
\textit{Boosting} & \cite{gidaris2019boosting} & Conv-4 &  &  \\
\textit{TAML} & \cite{TAML} & Conv-4 &  & \\
\textit{KTN} & \cite{KTN} & Conv-4 &  & \\
\textit{ArL} & \cite{arl} & Conv-4 &  &  \\
\textit{TADAM} & \cite{tadam} & ResNet-12 &  &  \\
\textit{Variational FSL} & \cite{Zhang2019VariationalFL} & ResNet-12 &  &  \\
\textit{DeepEMD} & \cite{deepemd} & ResNet-12 &  &  \\
\textit{MetaOptNet} & \cite{metaopt}             & ResNet-18 &  &  \\
\textit{PN + SSL} & \cite{su2020does} & ResNet-18 & 58.40 & 76.60 \\
\textit{SimpleShot} & \cite{wang2019simpleshot} & ResNet-18 &  &   \\
\textit{FLAT} & \cite{FLAT} & WRN-28-10 &  & \\
\cdashline{1-2}
\textit{SoSN(+F)+PN} & Eq. \eqref{eq:concat_f} & Conv-4 &  &   \\
\textit{SoSN() (no PN)} &  Eq. \eqref{eq:concat_best} & Conv-4 &  &   \\
\textit{SoSN(+R)+PN} &  Eq. \eqref{eq:concat_r} & Conv-4 &  &   \\
\textit{SoSN()+PN} & Eq. \eqref{eq:concat_best} & Conv-4 &  &   \\
\textit{MlSo()+PN} & Eq. \eqref{eq:concat_best} & Conv-4 &  & \\
\cdashline{1-2}
\textit{SoSN()+PN} & Eq. \eqref{eq:concat_best} & ResNet-12 &  &   \\
\textit{MlSo()+PN} & Eq. \eqref{eq:concat_best} & ResNet-12 & {} & {}  \\
\textit{MlSo()+PN} & Eq. \eqref{eq:concat_best} & ResNet-18 & {} & {}  \\
\midrule
& & & \multicolumn{2}{c}{Unsupervised Pipelines} \\ \midrule
\multicolumn{2}{l}{\textit{Pixel (Cosine)}} & - &  &   \\
{\textit{BiGAN ()}} &  \cite{bigan} & - &  &   \\
\textit{BiGAN (cluster match)} & \cite{bigan} & - &  &   \\
\textit{DeepCluster ()}& \cite{deepcluster} & - &  &   \\
{\textit{DeepCluster (cluster match)}} & \cite{deepcluster} & - &  &   \\
{\textit{U-Prototypical Net}}\textsuperscript{*} & \cite{snell2017prototypical} & Conv-4 &   & \\
{\textit{U-Relation Net}}\textsuperscript{*} & \cite{sung2017learning} & Conv-4 &   & \\
\textit{UMTRA}& \cite{umtra} & Conv-4 &  &   \\
\textit{CACTUs} &\cite{cactu} & Conv-4 &  &   \\
\arrayrulecolor{black}
\cdashline{1-2}
\multicolumn{2}{l}{\textit{U-SoSN()+PN}} & Conv-4 &  &  \\
\multicolumn{2}{l}{\textit{U-MlSo+PN}} & Conv-4 & {} & {} \\
\bottomrule
\end{tabular}}
\end{table*}

\begin{table}[t]
\centering
\caption{Evaluations on the \textit{tiered}-ImageNet dataset (5-way accuracy) with the Conv-4 backbone.}
\label{table_tiered}
\makebox[\linewidth]{
\fontsize{8.5}{9}\selectfont
\begin{tabular}{lccc}
\toprule
Model && 1-shot & 5-shot \\ \hline
\textit{Incremental}&\cite{incremental}&  &  \\
\textit{Soft k-means}&\cite{ren18fewshotssl}&  &  \\
\textit{MAML}&\cite{finn2017model}&  &  \\
\textit{Reptile}&\cite{Nichol2018Reptile}&  &  \\
\textit{Prototypical Net} & \cite{snell2017prototypical} &  &  \\ 
\textit{Relation Net} & \cite{sung2017learning} &  &  \\
\textit{TPN} &\cite{liu2018learning} &  &  \\
\cdashline{1-2}
\textit{SoSN} & &  &  \\
\textit{MlSo} &&  &  \\
\midrule
& \multicolumn{2}{c}{Unsupervised Pipelines} \\ \midrule
\textit{U-Prototypical Net} & \cite{snell2017prototypical} &  &  \\
\textit{U-Relation Net} & \cite{sung2017learning} &  &  \\

\cdashline{1-2}
\textit{U-SoSN} & &  &  \\
\textit{U-MlSo} & &  &  \\ 
\bottomrule
\end{tabular}}
\end{table}

\comment{
\begin{table*}[t]
\centering
\caption{Ablations \wrt the number of abstraction levels  on the \textit{mini}-ImageNet dataset (5-way accuracy) with the Conv-4 backbone. }
\label{table_depth}
\makebox[\textwidth]{
\fontsize{8.5}{9}\selectfont
\begin{tabular}{l|ccc|ccc}
\toprule
Model & \multicolumn{3}{c|}{1-shot} & \multicolumn{3}{c}{5-shot} \\ \hline
\# of levels  & 2 & 3 & 4 & 2 & 3 & 4 \\ \hline
\textit{MlSo} &  &  &  &  &  &   \\
\textit{MlSo+VALD} &  &  &  &  &  &  \\
\textit{MlSo+GM} &  &  &  &  &  &  \\
\textit{MlSo+VALD+GM} &  &  &  &  &  & \\
\textit{MlSo+VALD+GM} &  &  &  &  &  & \\
\bottomrule
\end{tabular}}
\end{table*}}


\begin{table*}[t]
\centering
\caption{\hg Evaluations on the fine-grained classification datasets (5-way accuracy) with the Conv-4 backbone.}
\label{table5}
\makebox[\textwidth]{
\begin{tabular}{lccccccc}
\toprule
&& \multicolumn{2}{c}{CUB Birds} & \multicolumn{2}{c}{Stanford Dogs} & \multicolumn{2}{c}{Stanford Cars} \\
Model && 1-shot & 5-shot & 1-shot & 5-shot & 1-shot & 5-shot \\ \hline
\textit{\hg Matching Net} & \cite{vinyals2016matching} & \hg  & \hg  & \hg  & \hg  & \hg  & \hg  \\ 
\textit{\hg MAML} & \cite{finn2017model} & \hg  & \hg  & \hg  & \hg  & \hg  & \hg  \\ 
\textit{\hg Proto. Net} & \cite{snell2017prototypical} & \hg  & \hg  & \hg  & \hg  & \hg  & \hg   \\ 
\textit{\hg Relation Net} & \cite{sung2017learning} & \hg  & \hg  & \hg  & \hg  & \hg  & \hg   \\ 
\textit{\hg LRPABN} & \cite{huang2020low} & \hg  & \hg  & \hg  & \hg  & \hg  & \hg  \\
\textit{\hg MattML} & \cite{zhu2020multi} & \hg  & \hg  & \hg  & \hg  & \hg  & \hg  \\
{\textit{\hg SoSN}} & \cite{sosn} & \hg  & \hg  & \hg  & \hg  & \hg  & \hg  \\ 
\cdashline{1-8}
\multicolumn{2}{l}{\textit{\hg MlSo}} & \hg {} & \hg {} & \hg {} & \hg {} & \hg {} & \hg {}  \\ 
\midrule
\textit{\hg U-Proto. Net} & \cite{sung2017learning} & \hg  & \hg  & \hg  & \hg  & \hg  & \hg   \\ 
\textit{\hg U-Relation Net} & \cite{sung2017learning} &  \hg  & \hg  & \hg  & \hg  & \hg  & \hg   \\ 
{\textit{\hg U-SoSN}} & \cite{sosn} & \hg  & \hg  & \hg  & \hg  & \hg  & \hg  \\ 
\cdashline{1-8}
\multicolumn{2}{l}{\textit{\hg U-MlSo}} & \hg {} & \hg {} & \hg {} & \hg {} & \hg {} & \hg {} \\
\bottomrule
\end{tabular}}
\end{table*}

\comment{
\begin{table*}[t]
\centering
\caption{Evaluations on fine-grained classification datasets  (5-way accuracy) with the Conv-4 backbone.}
\label{table5}
\makebox[\textwidth]{
\begin{tabular}{lccccccc}
\toprule
&& \multicolumn{2}{c}{Flower102} & \multicolumn{2}{c}{CUB-200-2011} & \multicolumn{2}{c}{Food-101} \\
Model && 1-shot & 5-shot & 1-shot & 5-shot & 1-shot & 5-shot \\ \hline
\textit{Matching Net} & \cite{vinyals2016matching} &  &  &  &  &  &  \\ 
\textit{MAML} & \cite{finn2017model} &  &  &  &  &  &  \\ 
\textit{Prototypical Net} & \cite{snell2017prototypical} &  &  &  &  &  &   \\ 
\textit{Relation Net} &\cite{sung2017learning} &  &  &  &  &  &   \\ 
\textit{LRPABN} & \cite{huang2020low} &  &  &  &  &  &  \\
MattML & \cite{zhu2020multi} &  &  &  &  &  &  \\
{\textit{SoSN}} & \cite{sosn} &  &  &  &  &  &  \\ 
\cdashline{1-8}
\multicolumn{2}{l}{\textit{MlSo}} & {} & {} & {} & {} & {} & {}\\ 
\midrule
&& \multicolumn{6}{c}{Unsupervised Pipelines} \\ \midrule
\textit{U-RN}&\cite{sung2017learning} &  &  &  &  &  &  \\
{\textit{U-SoSN}} & \cite{sosn} &  &  &  &  &  &  \\ 
\cdashline{1-8}
\multicolumn{2}{l}{\textit{U-MlSo}} & {} & {} & {} & {} & {} & {} \\
\bottomrule
\end{tabular}}
\end{table*}
}

\begin{table*}[t]
\centering
\caption{\hg Evaluations on the action recognition benchmarks (5-way accuracy). To fairly compare our method with the prior works, we follow the same evaluation splits with \cite{arn}. For our baselines, we re-implement the Prototypical Net and the Relation Net both with the use of 3D convolutions.}
\label{table6}
\makebox[\textwidth]{
\begin{tabular}{lccccccccc}
\toprule
&& \multicolumn{2}{c}{HMDB51} & \multicolumn{2}{c}{UCF101} & \multicolumn{2}{c}{{\em mini}-MIT} & \multicolumn{2}{c}{Kinetics} \\
Model && 1-shot & 5-shot & 1-shot & 5-shot & 1-shot & 5-shot & 1-shot & 5-shot \\ \hline
\textit{\hg C3D PN} & \cite{snell2017prototypical} & \hg  &  & \hg  & \hg  & \hg  & \hg  & \hg 57.11 & \hg 77.92 \\ 
\textit{\hg C3D RN} & \cite{sung2017learning} & \hg  & \hg  & \hg  & \hg  & \hg  & \hg  & \hg 56.98 & \hg 77.83\\
{\textit{\hg C3D SoSN}} & \cite{sosn} & \hg  & \hg  & \hg  & \hg  & \hg  & \hg  & \hg 58.77 & \hg 79.02 \\
{\textit{\hg CMN}} & \cite{cmn} & \hg -& \hg -& \hg -& \hg -& \hg -& \hg -& \hg 60.50 & \hg 78.90 \\
{\textit{\hg ARN}} & \cite{arn} & \hg  & \hg  & \hg  & \hg  & \hg  & \hg  & \hg 63.70 & \hg 82.40 \\ 
\cdashline{1-10}
\multicolumn{2}{l}{\textit{\hg C3D MlSo}} & \hg  & \hg  & \hg  & \hg  & \hg  & \hg  & \hg  & \hg   \\
\bottomrule
\end{tabular}}
\end{table*}


{\begin{table*}[t]
\centering
\caption{Evaluations on the Open MIC dataset (Protocol I) (5-way 1-shot accuracy). }
\label{table_o1}
\makebox[\textwidth]{
\setlength{\tabcolsep}{0.4em}
\renewcommand{\arraystretch}{0.90}
\begin{tabular}{lccccccccccccc}
\toprule
Model & &  & & & &  && & & & & & \\
\hline
\it Matching Net& \cite{vinyals2016matching} &  &  &  &  &  &  &  &  &  &  &  & \\
\it MAML & \cite{finn2017model} &  &  &  &  &  &  &  &  &  &  &  & \\
\it Reptile & \cite{Nichol2018Reptile} &  &  &  &  &  &  &  &  &  &  &  & \\
\it Proto. Net& \cite{snell2017prototypical} &  &  &  &  &  &  &  &  &  &  &  & \\
\it Relation Net& \cite{sung2017learning} &  &  &  &  &  &  &  &  &  &  &  & \\
\it SoSN & \cite{sosn} &  &  &  &  &  &  &  &  &  &  &  & \\
\multicolumn{2}{l}{\it MlSo} &  &  &  &  &  &  &  &  &  &  &  & \\ 
\midrule
\it U-Relation Net & \cite{sung2017learning} &  &  &  &  &  &  &  &  &  &  &  & \\
\it U-SoSN & \cite{sosn} &  &  &  &  &  &  &  &  &  &  &  & \\
\multicolumn{2}{l}{\it U-MlSo} &  &  &  &  &  &  &  &  &  &  &  & \\
\bottomrule
\end{tabular}}
p1: shn+hon+clv, p2: clk+gls+scl, p3: sci+nat, p4: shx+rlc. Notation {\em xy} means training on exhibition {\em x} and testing on {\em y}.
\end{table*}
}

\subsection{Datasets}
Below we describe our setup, as well as the category recognition, fine-grained and action recognition datasets.

\vspace{0.05cm}
\noindent\textbf{Omniglot } \cite{lake_oneshot} consists of 1623 characters  from 50 alphabets. Samples in each class are drawn by 20 different people. The dataset is split into 1200 classes for training and 423 classes for testing. All images are resized to  pixels.

\vspace{0.05cm}
\noindent\textbf{\textit{mini}-ImageNet} \cite{vinyals2016matching} consists of 60000 RGB images from 100 classes, each class containing 600 samples. 
We follow the standard protocol \cite{vinyals2016matching} and use 80 classes for training (16 classes selected for validation) and remaining 20 classes for testing. We  use images of  pixels.



\vspace{0.05cm}
\noindent\textbf{\textit{tiered}-ImageNet} \cite{ren18fewshotssl} consists of 608 classes from ImageNet. We follow the protocol with 351 base classes, 96 validation classes and 160 novel test classes.

\vspace{0.05cm}
\noindent\textbf{Open MIC} stands for the Open Museum Identification Challenge (Open MIC) \cite{me_museum}, a recent dataset with photos of various exhibits \eg, paintings, timepieces, sculptures, glassware, relics, science exhibits, natural history pieces, ceramics, pottery, tools and indigenous crafts, captured within 10 museum exhibition spaces according to which this dataset is divided into 10 sub-problems. In total, Open MIC has 866 diverse classes and 1--20 images per class. The within-class images undergo various geometric and photometric distortions as the data was captured with wearable cameras. This makes Open MIC a perfect candidate for testing one-shot learning algorithms. We combine ({\em shn+hon+clv}), ({\em clk+gls+scl}), ({\em sci+nat}) and ({\em shx+rlc}) into sub-problems {\em p1}, , {\em p4}. We form 12 possible pairs in which sub-problem  is used for training and sub-problem  is used for testing (xy).

\vspace{0.05cm}
\comment{
\noindent\textbf{Flower102} \cite{Nilsback08} is a fine-grained category recognition dataset that contains 102 classes of various flowers. Each class consists of 40-258 images. We randomly select 80 classes for training and 22 classes for testing.

\vspace{0.05cm}
\noindent\textbf{Food-101} \cite{bossard14} consists of 101000 images in total and 1000 images per category. We choose 80 classes for training and 21 classes for testing.

}
{\hg
\vspace{0.05cm}
\noindent\textbf{Caltech-UCSD-Birds 200-2011 (CUB Birds)} \cite{WahCUB_200_2011} has 11788 images of 200 bird species. We follow the splits from  \cite{huang2020low}, that is, 130 classes are selected for training, 20 classes for validation and the remaining 50 categories for testing.

\vspace{0.05cm}
\noindent\textbf{Stanford Dogs} \cite{cars} has 17150 instances of 120 dogs classes where 70 classes are used for training, 20 classes for validation and the remaining 30 classes for testing, which is consistent with the protocol in \cite{huang2020low}.

\vspace{0.05cm}
\noindent\textbf{Stanford Cars} \cite{dogs} dataset has 16190 samples of 196 car categories. Following the protocol in \cite{huang2020low}, we use the 130/17/49 class splits for training, validation and testing, respectively.

\vspace{0.05cm}
\noindent\textbf{HMDB51} \cite{Kuehne11}, an action recognition dataset, contains 6849 clips divided into 51 action categories, each with a minimum of 101 clips. Following the protocol in \cite{arn}, 31  classes are selected for training, 10  classes are used for validating and the remaining 10  classes are used for testing. 

\vspace{0.05cm}
\noindent\textbf{UCF101} \cite{soomro2012ucf101} contains  action videos collected from YouTube. It has 13320 video clips and 101 action classes. Following the protocol in \cite{arn}, we pick 70 classes for training, 10 classes for validation and the remaining 21 classes for testing.

\vspace{0.05cm}
\noindent\textbf{\textit{mini}-MIT} \cite{monfortmoments} is a subset of the newly proposed large-scale Moments in Time dataset. This mini version contains 200 classes with 550 videos per class. Following the protocol in \cite{arn}, we select 120 classes for training, 40 classes for validation and the rest 40 classes for testing.
}

\subsection{Experimental Setup}
For the Omniglot dataset, we follow the setup in \cite{sung2017learning}. For {\em mini}-ImageNet, we use the  5-way 1-shot and  5-way 5-shot protocols. For every training and testing episodes, we randomly select 5 and 3 query samples per class, respectively. We compute an average over 600 episodes to obtain results. 
We use the initial learning rate of  and train the model with  episodes. For \textit{tiered}-ImageNet, we follow the  settings used for the {\em mini}-ImageNet dataset. 
For Open MIC, we mean-center images per sub-problem. We use the initial learning rate of  and  train the network with  episodes. 
For the fine-grained classification datasets, we evaluate our models on the 5-way 1-shot and  5-way 5-shot protocols. The numbers of support and query samples in each episode are the same as in the {\em mini}-ImageNet setting. We use the initial learning rate of  and train the model with  episodes. 
For the action recognition datasets, we randomly sample 50 frames per video along the temporal mode. We resize video frames to  pixels, which results in a lightweight model. We evaluate our algorithm on the 5-way 1-shot and  5-way 5-shot protocols on the three datasets detailed earlier. We adopt the hyper-parameter configuration used on the {\em mini}-ImageNet dataset.

\subsection{Evaluation Results}
Below we evaluate SoSN and its multi-level extension MlSo, and we compare them with state-of-the-art methods on datasets introduced above. 

\vspace{0.05cm}
\noindent\textbf{Omniglot.} Table \ref{table2} shows rather saturated results. We consider experiments on the Omniglot dataset as a sanity check to validate the performance of SoSN in the best default setting, that is, using the relationship descriptor from Eq. \eqref{eq:concat_best} and Power Normalization from Proposition \ref{pr:axmin}. {We note that the basic multi-level
 multi-scale variant of MlSo based on Eq. \eqref{eq:sosn22} outperforms SoSN.}


\vspace{0.05cm}
\noindent\textbf{{\em mini}-ImageNet.} Table \ref{table3} demonstrates that our method outperforms other approaches on both 1- and 5-shot evaluation protocols.
Firstly, we note that comparisons between various relation descriptors with/without PN are included and discussed as ablation studies in Section \ref{sec:abl}. 

For the image size of  and the 5-way 1-shot experiment, our best singe-level SoSN model achieves  higher accuracy than Relation Net \cite{sung2017learning}. Our best singe-level SoSN also outperforms Prototypical Net by  accuracy on the 5-way 1-shot protocol. Not shown in the table are results for SoSN trained with images of  pixels, in which case the accuracy scores on both protocols increase by  and , respectively. Such an accuracy gain demonstrates that SoSN benefits from large image sizes as second-order matrices are of higher rank for higher image resolutions due to the higher spatial resolution of feature maps compared with the low-resolution counterparts. Our similarity learning network works with variable resolutions of input images because SoSN operates on matrices whose size depends only on the number of output channels of encoding network.  

{Furthermore, our variant of MlSo with the ResNet-12 backbone, based on the inter-level matching strategy of the multi-level multi-scale feature representations by the Gate Module, achieved a significant gain between 6\% and 9\% accuracy over SoSN.} 
For unsupervised FSL, Table \ref{table3} shows that our U-SoSN and U-MlSo significantly outperform other unsupervised baselines (`{\em U-}' stands for the unsupervised FSL setting) \eg, U-Relation Net and U-Prototypical Net. { Compared to the U-SoSN model, the U-MlSo model  improves the top-1 accuracy by another 2\% and 3\% on the 1- and 5-shot protocols, respectively. }

{We  discuss the ablations on MlSo in Section \ref{sec:abl}. Firstly, we  discuss the results of the single-level SoSN model on more datasets. In the following tables, we drop `{\em ()+PN}' from our notations but this particular relation descriptor with PN is the best performing variant, thus it is used across the remaining experiments.}

\vspace{0.05cm}
\noindent\textbf{{\em tiered-}ImageNet.} Table \ref{table_tiered} shows the performance of our proposed methods on {\em tiered-}ImageNet. Our SoSN achieves  and  improvement in accuracy, compared with Relation Net for 1- and 5-shot protocols, respectively. { Moreover, MlSo with the Gate Module achieves  and  improvement in accuracy, compared with Relation Net for 1- and 5-shot protocols, respectively. The unsupervised variant, U-MlSo, yields  and  gain in accuracy over U-Relation Net for  1- and 5-shot protocols, respectively.
Our supervised and unsupervised FSL models outperform all other  FSL approaches based on the Conv-4 backbone.}

\vspace{0.05cm}
\noindent\textbf{Open MIC.} Table \ref{table_o1} demonstrates that our single-level SoSN model outperforms the Relation Net \cite{sung2017learning} for all train/test sub-problems of Protocol I. For the 5- and 20-way protocols, Relation Net scores  and  accuracy, respectively. In contrast, our single-level SoSN scores  and  accuracy, respectively. 

\begin{table}[t]
\centering
\caption{\small Ablation study on {\em mini}-ImageNet \wrt different modules of our pipeline  (5-way accuracy, the `Conv-4-64' backbone, 4 stages and 3 scales, and OT matching were used.)}
\label{table_ablation_3}
\makebox[\linewidth]{
\begin{tabular}{cccc|cc}
\toprule
 Baseline &  SB &  FM &  VALSD &  1-shot &  5-shot \\  \hline
 \checkmark & & & &  55.93 &  71.05 \\
 \checkmark &  \checkmark & & &  57.28 &  72.01 \\
 \checkmark &  \checkmark &  \checkmark & &  57.79 &  72.65 \\
 \checkmark &  \checkmark &  \checkmark &  \checkmark &  \bf 58.03 &  \bf 73.06 \\
\bottomrule
\end{tabular}}
\end{table}

\begin{table}[t]
\centering
\caption{\small  Ablation study on {\em mini}-ImageNet \wrt the number of abstraction levels and spatial scales (1-shot/5-shot accuracy, FM and VALSD were not used). }
\label{table_ablation_2}
\makebox[\linewidth]{
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{cc|ccc}
& & \multicolumn{3}{c}{ Levels} \\
& & 2 & 3 & 4 \\ \hline
\multirow{3}{0.8cm}{Scales} & 1 & \multicolumn{1}{|c}{ 55.24/70.85} & 55.66/70.88 & 55.93/71.05 \\
& 2 & \multicolumn{1}{|c}{55.95/70.77} & 56.19/71.13 & 56.63/71.72 \\
& 3 & \multicolumn{1}{|c}{56.67/71.81} & 57.03/71.98 & \bf 57.28/72.01 \\
\end{tabular}}
\end{table}

\begin{table}[t]
\centering
\caption{\small  Ablation study on {\em mini}-ImageNet \wrt the choice of matching algorithm and mode on the \textit{mini}-Imagenet dataset (1- and 5-shot accuracy). We used the `Conv-4-64' backbone. }
\label{table_ablation_1}
\makebox[\linewidth]{
\setlength{\tabcolsep}{0.6em}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{c|cccc|cccc}
& \multicolumn{4}{c}{ Intra-level Matching} & \multicolumn{4}{c}{ Inter-level Matching}\\
 shot &  CM &  GM &  OT &  GR &  CM &  GM &  OT &  GR \\ \hline
 1 &  57.49 &  57.83 &  \bf 58.03 &  57.79 &  56.87 &  57.72 &  57.91 &  56.91 \\ 
 5 &  72.21 &  72.58 &  \bf 73.06 &  72.49 &  71.59 &  72.43 &  72.36 &  71.77 \\ 
\end{tabular}}
\end{table}

\comment{
\begin{table*}[b]
\caption{Evaluations on the Open MIC dataset for Protocol II (asterisk  indicates splits with the number of classes such that ).}
\label{table_ablation_3}
\makebox[\textwidth]{
\setlength{\tabcolsep}{0.5em}
\renewcommand{\arraystretch}{1}
\fontsize{8.5}{9}\selectfont
\begin{tabular}{lcccccccccccc}
\toprule
Model &&  &  & & & &  && & & &  \\
\hline
Relation Net & & \multirow{2}{*}{5} &  &  &  &  &  &  &  &  &  &  \\
SoSN &&  &  &  &  &  &  &  &  &  &  &  \\
\hline
 Relation Net & & \multirow{2}{*}{20} &  &  &  &  &  &  &  &  &  &  \\
SoSN  &&  &  &  &  &  &  &  &  &  &  &  \\
\hline
Relation Net & & \multirow{2}{*}{30} &  &  &  &  &  &  &  &  &  &  \\
SoSN  &&  &  &  &  &  &  &  &  &  &  &   \\
\hline
SoSN && 45 &  &  &  &  &  &  &  &  &  &   \\
\hline
SoSN && 60 &  & -   &  &  &  & - & - &  &  &   \\
\hline
SoSN && 90 &  &  -  &  &  &  & - & - &  &  &   \\
\bottomrule
\end{tabular}}
\centering Training on source images and testing on target images for every exhibition, respectively.
\end{table*}
}



For the unsupervised pipelines, the U-SoSN and U-MlSo models achieve very promising results on  Open MIC considering no class-wise annotations were used during the training step. To demonstrate this point further, the performance of U-SoSN is better than that of the supervised Relation Net. { The U-MlSo model with the Gate Module further outperforms U-SoSN by up to 3\% accuracy, which is close to the performance of the supervised SoSN model. Specifically, U-MlSo outperformed supervised SoSN on 4 splits.} The above experiment shows that the unsupervised framework is especially effective in the scenario if (i) the number of training samples from the base classes is limited, and (ii) the problem is closely related to the image retrieval rather than the pure object category recognition. 

\vspace{0.05cm}
\noindent\textbf{\hg Fine-grained FSL.}
{\hg For the fine-grained datasets, our proposed models are evaluated on the CUB Birds \cite{WahCUB_200_2011}, Stanford Dogs \cite{dogs} and Cars \cite{cars} datasets given the 5-way 1-shot and  5-way 5-shot protocols. We follow the same training, validation and testing splits as provided in \cite{huang2020low}. Table \ref{table5} demonstrates that our models outperform the baseline models such as Relation Net \cite{sung2017learning}, LRPABN \cite{huang2020low} and MattML \cite{zhu2020multi}. For CUB Birds, our best MlSo with FM (OT) achieves  and  improvements in accuracy (the 1- and 5-shot protocols, respectively) compared to the 1-st order Relation Net. 
For the Stanford Dogs and Cars datasets, the overall improvements of our MlSo are more significant. To illustrate this point, MlSo outperforms Relation Net by up to  and  accuracy on Stanford Dogs, and  and  on Stanford Cars (the 1- and 5-shot protocols, respectively). Furthermore, our unsupervised FSL variants, U-SoSN and U-MlSo, attain even higher gains in accuracy which are often in the range of between 10\% and 25\% compared to U-Relation Net.}


\vspace{0.05cm}
\noindent\textbf{\hg Few-shot Action Recognition (FSAR).}
{\hg We conclude our evaluations on FSL Action Recognition, denoted as FSAR for short. To this end, results are obtained on the HMDB51 \cite{Kuehne11}, UCF101 \cite{soomro2012ucf101} and \textit{mini}-MIT \cite{monfortmoments} datasets. We follow the  evaluation protocols in \cite{arn}. As every action clip may contains hundreds of frames, we resize all frames to  pixels and downsample clips along the temporal mode to reduce the usage of the GPU memory and limit the computational footprint. Table \ref{table6} shows that our MlSo outperforms the C3D Relation Net, SoSN and CMN \cite{cmn} models. These results are consistent with our evaluations on the category
recognition and fine-grained datasets.}


\subsection{Ablation Studies}
\label{sec:abl}
Below we conduct ablations studies and provide discussions regarding several components of our pipeline.

\vspace{0.05cm}
\noindent\textbf{Relationship Descriptors and Power Normalization.}  Firstly, Table \ref{table3} shows that the use of PN brings  gain in accuracy over not using it. Not included in the tables, similar were our observations on the Open MIC dataset. Given the simplicity of PN, we have included it in our evaluations unless stated otherwise. 

Relationship descriptors from Table \ref{tab_methods} are evaluated in Table \ref{table3} according to which the single-level SoSN() model outperforms the single-level SoSN( +F) and SoSN(+R) models. We expect that averaging over  support descriptors from  support images, as in SoSN(),  removes the uncertainty in few-shot statistics, whereas the outer-products of support/query datapoints still enjoy the benefit of spatially-wise large convolutional feature maps (which helps form the robust second-order statistics). 

\vspace{0.05cm}
{ \noindent\textbf{Importance of Second-order Pooling}. Table \ref{table_tiered} ({\em tiered}-ImageNet) shows that Relation Net achieves  54.48\% and 71.32 \% accuracy on the 1- and 5-shot protocols, respectively. Relation Net can be considered identical with the SoSN model but it uses first-order pooling. In contrast, SoSN achieves 58.62 and 75.19 \% accuracy (1- and 5-shot protocols, respectively). The gain in accuracy can be attributed to the fact that SoSN is equipped with second-order pooling. 

Table \ref{table3} shows that on {\em mini}-ImageNet, Relation Net scores 50.44\% and 65.32\% accuracy on the 1- and 5-shot protocols, whereas SoSN scores  52.96\% and 68.63\% accuracy, respectively.

The similar trend can be observed on {\em mini}-ImageNet when aggregating over feature maps of images of  pixels. In such a  setting (not included in the tables as  pixels resolution is not a part of standard FSL protocol), Relation Net yields 54.01\% and 68.56\%  accuracy on the 1- and 5-shot protocols. In contrast, SoSN yields 57.74\% and 71.08\% accuracy, respectively.

SoP represents features of each image as second-order statistics which are invariant to the spatial order of features in feature maps, and the spatial size of these feature maps. Second-order statistics are also richer than first-order statistics, as indicated by gains in accuracy attained by SoSN in comparison with Relation Net. SoP is the most beneficial when feature representations provide many feature vectors for aggregation \eg, . In such a case, the autocorrelation matrices may be of full rank (rank-), thus capturing more statistical information in comparison to the first-order prototypes (equivalent of the rank- statistic).
}

\vspace{0.05cm}
{ \noindent\textbf{MlSo, SB, FM and VALSD. } Table \ref{table3} demonstrates on \textit{mini}-ImageNet that our MlSo outperforms SoSN and a larger number of prior works. The performance on the 5-way 1-shot and  5-way 5-shot protocols achieves the peak gain of  in accuracy  given 4 encoding levels and the Optimal Transport matching step, compared to the best single-level SoSN model.

Table \ref{table_ablation_3} shows that adding our Scale-wise Boosting (SB), the Feature Matching (FM) and the Visual Abstraction Level and Scale Discriminator (VALSD) yields  around  gain in accuracy over the baseline model. For the SB+FM case, we mean that the advanced abstraction level and spatial scale matching strategy is used, thus the loss in Eq. \eqref{eq:sosn_new1} is used for SB+FM, whereas the loss in Eq. \eqref{eq:sosn22} is used for SB alone.

Table  \ref{table_ablation_2} shows that using four level of visual abstraction and three spatial scales is the best, which is consistent with our claim that the levels of visual abstraction and spatial scales need to be taken into account in few-shot learning.

Table \ref{table_ablation_1} shows that the Intra-level Matching strategy is overall better than the Inter-level Matching strategy. This is consistent with our expectations as the levels of visual abstraction and spatial scales are quite complementary. Therefore, matching spatial scales irrespective of abstraction levels is a meaningful strategy. Finally, the Gate Module and the Optimal Transport are two best performing strategies,  followed by the GRaph matching (GR) strategy and the Cosine Matching (CM) strategy. We suspect that GR was somewhat suboptimal due to the small-size dense adjacency matrix in our problem rather than the large scale sparse adjacency matrix which would normally capture a complex topology of some large graph (node classification, \etc).  OT performed robust matching as it is designed to find an optimal transportation plan between different levels of abstraction and spatial scales of support-query pairs. Nonetheless, GM appears to provide the best matching trade-off, that is, GM is almost as good as OT in terms of accuracy, and it is faster than OT in terms of computational complexity (\ie, there is no need to solve any linear programs). 
Finally, Figure \ref{fig:cam} shows
the visualization of multiple levels of feature abstraction in our FEN. Across all visualizations and their Gate Module scores (GM), the coarse-to-fine levels of feature abstraction appear to be complementary with each other. 
} 

\ifdefined\arxiv
\newcommand{\PowH}{3.0cm}
\newcommand{\PowHB}{2.875cm}
\newcommand{\PowW}{3.65cm}
\else
\newcommand{\PowH}{3.2cm}
\newcommand{\PowHB}{3.4cm}
\newcommand{\PowW}{3cm}
\fi

\comment{
\begin{figure*}[t]\begin{subfigure}[t]{0.33\linewidth}
\centering\includegraphics[trim=0 0 0 0, clip=true, height=\PowH]{images/accv_pow1.pdf}\vspace{-0.2cm}
\caption{\label{fig:pow1}}
\end{subfigure}
\begin{subfigure}[t]{0.33\linewidth}
\centering\includegraphics[trim=0 0 0 0, clip=true, height=\PowH]{images/accv_pow2.pdf}\vspace{-0.2cm}
\caption{\label{fig:pow2}}
\end{subfigure}
\begin{subfigure}[t]{0.33\linewidth}
\centering\includegraphics[trim=0 0 0 0, clip=true, height=\PowH]{images/accv_pow3.pdf}\vspace{-0.2cm}
\caption{\label{fig:pow3}}
\end{subfigure}




\caption{MaxExp(), MaxExp, and SigmE are in Figure \ref{fig:pow1}. Note that MaxExp() extends MaxExp to negative values and is closely approximated by SigmE. Figure \ref{fig:pow2}: Derivatives of MaxExp(), MaxExp, and SigmE. Figure \ref{fig:pow3}: Smooth derivative of MaxExp() with the soft maximum.}
\label{fig:power-norms}
\end{figure*}
}

