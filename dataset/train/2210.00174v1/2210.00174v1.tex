
\section{Contributions on Code Quality}

To improve the code efficiency, we refactor and re-implement the data pipeline of the original official codebase to encourage modularity, compatibility and performance.

\noindent{\textbf{Modularity.}} We decouple the object category sampling, video sequence (instance) sampling, video frame image loading and tensor preparing from one deeper class into multiple independent shallow classes. These components can be used in plug-and-play and mix-and-match manners.

\noindent{\textbf{Compatibility.}} It is designed to be interoperable with other Pytorch standard domain-specific libraries (torchvision), and their highly-optimized modules and functions can be used in processing the ORBIT dataset.

\noindent{\textbf{Performance.}} To optimize I/O, we introduce multi-threading to reduce the latency of loading images from disk in each episode. Table~\ref{performance} shows the comparison of data loading speed, where our re-implemented codebase accelerates the data loading speed by 2.7 and 2.77 times.

\section{Experiment}

\subsection{Dataset and implementation details} 

\noindent{\textbf{Dataset.}} For this challenge, we evaluate our method on the ORBIT dataset which is designed for real-world few-shot object recognition~\cite{massiceti2021orbit}. It contains 3,822 videos of 486 objects collected by 67 users. Each user is asked to collect videos with the target object in isolation which is referred as \textit{clean} videos. They are also asked to take videos where the target object is mixed with multiple other objects. These videos are referred as \textit{clutter} videos. The goal of this challenge is to train a teachable object recognizer such that the model is personalized for each user using their \textit{clean} videos. The personalized model is then evaluated on the \textit{clutter} videos. Please refer \cite{massiceti2021orbit} for more details. In the concept of meta-learning scenario, the \textit{clean} videos are analogy as support set while the \textit{clutter} videos are query set. 

\noindent{\textbf{Network.}} We follow \cite{bronskill2021memory} to use EfficientNet-B0~\cite{tan2019efficientnet} pre-trained on ImageNet \cite{deng2009imagenet} as the feature extractor. A single layer transformer encoder as in FEAT~\cite{ye2020few} is used to further adapt the computed prototypes. 

\noindent{\textbf{Training and evaluation protocol.}} We leverage the concept of prototype-based meta-learning to train our network. A comprehensive survey on bi-level optimization can be found~\cite{chen2022gradient}. The embedding of the query videos corresponding to the same class is averaged as its prototypes. The query videos are classified by comparing the cosine similarity between the prototypes. We follow the episodic learning as in LITE~\cite{bronskill2021memory} to utilize large resolution frame patches to train the network. For a fair comparison, we use the same hyper-parameters and evaluation protocol as in~\cite{bronskill2021memory,massiceti2021orbit}. 


\begin{table}
\small
\centering
\setlength{\tabcolsep}{5pt}
\caption{\textbf{Training and testing speed for data loaders.} The baseline is the original ORBIT codebase. Testing speed is measured by preparing 300 videos from 17 users. Training speed is measured by preparing 100 episodes.} 
\begin{tabular}{lcccc}
\Xhline{1pt}
Method & \# workers & \# threads & Test speed & Train speed \\ 
\hline
Baseline & 4 & 1 & 233 & 2.61 \\
\Xhline{1pt}
 & 4 & 4 & 201 (1.15x) & 2.2 (1.18x) \\
Ours & 4 & 16 & 152 (1.53x) & 1.08 (2.41x) \\
 & 8 & 16 & 86 (2.7x) & 0.94 (2.77x) \\
 \Xhline{1pt}
\end{tabular}
\label{performance}
\end{table}

\section{Results}

\noindent{\textbf{Quantitative results.}} Table~\ref{results} shows the main results and the contributions of each component. We re-run the codebase of~\cite{massiceti2021orbit} for typical ProtoNet with default hyper-parameters, but its per frame accuracy dropped by 3\%. We re-implemented the codebase, and ours achieves similar results as reported by~\cite{bronskill2021memory}. With \textbf{Embedding adaptation} method, the generated prototypes for all categories are further enhanced by examining the relationships among them. The prototypes are pushed away from others and become more discriminative. Therefore, the accuracy increased by 2.87\%. \textbf{Uniform clip sampler} ensures the constant sampling rate and the higher temporal coverage for all videos to reduce the randomness of information gathering. In addition, due to the variable lengths of the videos, a uniform clip sampler ensures that the short videos are not over-sampled. Therefore, it improves an additional 1.52\%. Furthermore, due to the high correlation between consecutive video frames, some sampled video clips may convey repetitive information, especially with minor movements. Thus, the overall amount of discriminative data points is reduced. Enriching the data information by augmenting the video clip is an effective method. Hence, \textbf{Data augmentation} improves additional 0.88\%. To filter out the frames with less information (background only), the \textbf{Invalid frame detection} by edge detection and threshold is able to reduce such effect. Therefore, it further improves by 0.12\%. Overall, with full proposed components, our method outperforms the baseline by 5.39\%.

\begin{table}
\small
\centering
\setlength{\tabcolsep}{4pt}
\caption{\textbf{Main results and contribution from each component.}} 
\begin{tabular}{lcc}
\Xhline{1pt}
Method & Frame accuracy & Improvement\\ 
\hline
ProtoNet (copy from \cite{bronskill2021memory}) & 66.30  & - \\
ProtoNet (ORBIT code base) & 63.27 & -3.03\\
\Xhline{1pt}
ProtoNet (Ours) & 66.27 & -0.03 \\
+ Embedding adaptation & 69.17 & +2.87 \\
+ Uniform sampler & 70.69 & +4.39 \\
+ Data augmentation & 71.57& +5.27\\
+ Invalid frame detection & 71.69 & +5.39\\
 \Xhline{1pt}
\end{tabular}
\label{results}
\end{table}


\noindent{\textbf{Qualitative results.}} Fig.~\ref{fig:per_user} shows the per user performance accuracy. It is worth noting that the embedding adaptation and uniform sampling provide significant improvement for most of the users. With the integration of all proposed components, our method achieves the best accuracy for 11 users.

\begin{figure}[t]
    \centering
    \includegraphics[width =\linewidth]{figs/per_user.png}
    \caption{\textbf{Per user performance comparison.} Among 17 users, our method achieves the highest accuracy for the 11 users.} 
    \label{fig:per_user}
\end{figure}

\section{Conclusion and future work}

In this work, we proposed several improvements for the few-shot video object recognition task. The proposed components consist of embedding adaptation, uniform video sampling, and invalid frame detection. Our unified solution achieves the winner of the first challenge on the ORBIT dataset. Furthermore, we also contribute to refactoring and optimizing the original codebase to improve the productivity of other researchers. Future work includes the solution to tackle the domain shift between support and query videos which is quite common in real-world scenarios. 