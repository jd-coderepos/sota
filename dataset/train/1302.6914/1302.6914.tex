\documentclass{llncs}
\listfiles
\usepackage{amsmath,color}
\pagestyle{plain} 
\newcommand{\eg}{\textsl{e.g.}}
\newcommand{\ie}{\textsl{i.e.}}
\newcommand{\BigOh}[1]{O\!\left(#1\right)}
\newcommand{\LittleOh}[1]{o\!\left(#1\right)}
\newcommand{\BigOmega}[1]{\Omega\!\left(#1\right)}
\newcommand{\LittleOmega}[1]{\omega\!\left(#1\right)}
\newcommand{\BigTheta}[1]{\Theta\!\left(#1\right)}
\newcommand{\SU}[1]{\textsf{FF}_i\!\left(#1\right)}
\newcommand{\su}[1]{\textsf{FF}\!\left(#1\right)}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\E}{E}

\newcommand{\ji}[1]{\textcolor{red}{(JI Says: #1)}}
\newcommand{\pat}[1]{\textcolor{red}{(PM Says: #1)}}


\title{The Fresh-Finger Property \thanks{This research was partially supported by NSERC and MRI.}}
\author{John Howat\inst{1} \and John Iacono\inst{2} \and Pat Morin\inst{3}}
\institute{School of Computing\\Queen's University\\\email{howat@cs.queensu.ca}
\and Department of Computer Science and Engineering\\Polytechnic Institute of New York University\\\email{jiacono@poly.edu} \and
School of Computer Science\\Carleton University\\\email{morin@scs.carleton.ca}}


\begin{document}
\maketitle
\thispagestyle{plain}
\begin{abstract}
The \emph{unified property} roughly states that searching for an element is fast when the current access is close to a recent access. Here, \emph{close} refers to rank distance measured among all elements stored by the dictionary. We show that distance need not be measured this way: in fact, it is only necessary to consider a small working-set of elements to measure this rank distance. This results in a data structure with access time that is an improvement upon those offered by the unified property for many query sequences.
\end{abstract}



\section{Bounds for searching: notation and history}

Comparison-based searching is one of the most fundamental operations in computer science: given a set  of  totally ordered items, create a data structure that, given a query key , will return the largest key in  that is no larger than . This is called \emph{predecessor search}. We focus on the case where  is static, and thus can be assumed to be the integers from 1 to . We refer to a search that returns  as an \emph{access} to . Let  denote a sequence of accesses to be performed on a data structure, with  chosen to be sufficiently large to absorb any start-up costs.

Any comparison-based search data structure is, at its core, a method of choosing which comparisons to perform in order to execute an access. The data structure is essentially a way of encoding a comparison tree to execute each access---the data structure could do this in an explicit way like in a binary search tree, or in an implicit way as in the binary search algorithm. It has been long-known that information theory tells us that the worst-case time for an access must be , and that  can be achieved with data structures such as binary search on a sorted array.

But, worst-case analysis is not the end of the story, as one can design data structures that execute operations in  time if the operations have some kind of order to them. Thus, we can create data structures with access times that are functions of the access sequences themselves (or some distributional statistic of the access sequence)---these runtimes will still have  worst-case behavior, but will be faster over sequences that have certain desirable characteristics. We now review some runtime bounds that have been introduced, and the data structures whose runtimes have bounds (we will say a data structure has a bound to mean that its runtime can be bounded by the bound):

\paragraph{Static optimality bound.} If the number of searches in  to  is , then the runtime to search for  is .\footnote{In this paper,  .} Knuth showed how to achieve this bound if the -values are given in advance \cite{DBLP:journals/acta/Knuth71}.

\paragraph{Working-set bound.} Let  be the number of distinct items accessed since the last access to  in . A data structure has the working-set bound if an access to  takes time . The idea behind this is that if the accesses are restricted to a subset of  items, then the accesses will take time  rather than . It has been shown that the working-set bound implies the static optimality bound in the amortized sense. Splay trees \cite{DBLP:journals/jacm/SleatorT85} have the working-set bound in the amortized sense, while the working-set structure \cite{DBLP:conf/soda/Iacono01a} was designed to have this bound in the worst-case.

\paragraph{Queueish bound \cite{DBLP:journals/algorithmica/IaconoL05}.} The working-set bound requires that items which were accessed recently take less time than those that have not been accessed in a while. The queueish bound reverses this and states that the time to access  should be ; thus any structure with the queueish bound will execute the least recently accessed item in constant time. No dictionary is known to have the queueish bound and it remains open whether such a dictionary can exist; however, it was shown that there is a structure with a close-to-queueish bound of  amortized access time.

\paragraph{Dynamic finger bound \cite{DBLP:journals/siamcomp/ColeMSS00,DBLP:journals/siamcomp/Cole00}.} Let  be the number of keys between  and  in  (this is just  if  is the integers from  to ). The dynamic finger property says the cost to execute search  is . Level-linked trees \cite{DBLP:journals/iandc/HoffmanMRT86} have the dynamic finger property in the worst-case, and splay trees have the amortized dynamic finger property.

\paragraph{Unified bound \cite{DBLP:journals/tcs/BadoiuCDI07}.} The dynamic finger bound and the working-set bounds are the best known bounds on the runtime of splay trees, yet neither implies the other and neither is tight. For example in the sequence  the dynamic finger will give a bound of  per operation on average, while the working-set bound gives a bound of  per operation. In the sequence , the situation is reversed.  The unified bound was proposed as a natural combination of these two bounds;\footnote{In an unfortunate naming conflict, Sleator and Tarjan have a ``Unified Theorem'' for splay trees \cite[Theorem~5]{DBLP:journals/jacm/SleatorT85} and the bound in the Unified Theorem is also sometimes called the ``unified bound.''} 
an access is fast if it is close in key value to something that has been recently accessed. Formally, a data structure has the unified bound if accessing  takes time . This clearly implies the working-set bound (set ) and the dynamic finger bound (set ). A non-tree structure was presented with the unified bound, and it is conjectured that splay trees have the unified bound. A binary search tree (BST) structure with the unified bound plus an additive  is known \cite{DBLP:conf/wads/DerryberryS09}, and a BST structure with the unified bound was claimed \cite{dthesis}, but later declared to be buggy \cite{wrong}.

\noindent
There are several issues that are important when considering a bound:

\paragraph{Static vs.~dynamic.} If a search algorithm uses the same search tree for every access, it is said to be \emph{static}, while if the comparisons performed to execute a given search depend upon the previous searches performed it is said to be \emph{dynamic}. The static optimality bound is the best bound possible if the search algorithm generates the same comparison tree for every access.

\paragraph{Online.} A bound where the runtime bound to execute  is a function of the sequence  is said to be an online bound. All of the bounds listed above, except the static optimality, bound are online. The static optimality bound is not online because it is computed as a function of the frequency count over the entire length of a sequence.

\paragraph{Amortization.} For any operation there can be at most  different searches than can be done using at most  comparisons. Any bound that at any time that has  different searches perform only  comparisons for some value of  means that the bound can not hold in the worst case. None of the bounds above require amortization, and if a bound does require amortization, that is probably a sign that it is somehow unnatural.

\paragraph{Binary Search Tree model.} Wilber \cite{DBLP:journals/siamcomp/Wilber89} formalized the binary search tree model; in this model the data structure is a binary search tree which can be restructured through the use of rotations. The set of sequences which binary search trees can execute quickly seems to be a reasonable classifier of those sequences that we consider to be natural. Without a restriction to the BST model, given any single access sequence, it is possible to create a data structure that will execute the searches in that sequence quickly, and others slowly. This is not possible in the BST model as there are deterministic sequences such as the bit reversal permutation that can not be executed faster than  amortized time.

The class of BST data structures also have the possibility that there may exist an online BST data structure that can execute every access sequence asymptotically as fast as the best BST data structure \emph{for that sequence}. Such a structure would be called \emph{dynamically optimal}; no BSTs are known to be dynamically optimal although spay trees and Lucas' trees \cite{lucas} are conjectured to be. Blum et.~al. \cite{blum} gave a non-tree data structure that runs within a constant factor of comparisons of any BST data structure, but requires superpoloynomial time to decide which comparisons to perform. Tango trees \cite{DBLP:journals/siamcomp/DemaineHIP07} are a BST that execute every sequence within a  factor of the best possible binary search tree. Of the bounds described above, no BST can have the queueish property, it is conjectured that there is a BST with the unified property, and the rest of the bounds described above are achievable by BST data structures.

\section{Problems with the unified bound}

The unified bound is the best proposed bound for binary search trees, and seems to be a reasonable combination of temporal locality and locality in keyspace. However, we will show that the unified bound has a flawed view of keyspace, and propose a new bound that attempts to rectify this flaw.

Recall that the unified property roughly states than access is fast when the current access is close to a recent access. For example, consider the following access sequence (assume  is even and  divides ): 
(The exponentiation denotes that the sequence is repeated  times
to make a sequence of length .)
Observe that, except for the first two accesses in each cycle, every access is at distance one from the element accessed two accesses ago.  A dictionary with the unified
property would therefore perform this sequences in time at most

for an amortized cost of  per access.  

Next, consider the following access sequence:

where  is a multiple of ,  is a multiple of , and . For this sequence, the unified bound is useless: any element accessed less than  time units in the past is at distance at least  from the currently accessed element, so the cost of every access is .

On the other hand, the sequence  is not very different from .  Indeed  can be viewed as the sequence  over a larger set, , in which a  fraction of the elements are never accessed.  Intuitively, in a good data structure these irrelevant elements should ``fall out of the way'' in order to speed up accesses to the important elements (multiples of ).



The sequence  demonstrates the problem with the unified property: The distance function  simply measures the number of keys between  and . But, suppose some key values have not been accessed in a long time relative to  and , or in the extreme case, have never been accessed. Why should the number of such keys between  and  influence the runtime of accessing keys such as  and ?
Put simply, they should not. Data structures such as splay trees will have items that are never accessed ``percolate'' to to bottom of the structure, and the runtime of a splay tree with a subtree of never-accessed keys is identical to the runtime if the keys are not there. Thus we need a more nuanced  function that ``forgets'' keys that have not been accessed in a while when computing key distance.

In this paper, we will expand on the idea of counting only recently accessed elements towards the distance between elements stored by the dictionary. The remainder of the paper is organized in the following way. In Section~\ref{section:definitions}, we define a new, stronger version of the unified property. In Section~\ref{section:main}, we show how to construct a dictionary that has this new property. We conclude with possible directions for future research in Section~\ref{section:conclusion}.

\section{Defining the fresh-finger property}
\label{section:definitions}

In this section, we define a new, stronger version of the unified property that we term the \emph{fresh-finger property}. Recall that  is our access sequence. Define


One can think of  as the most recent time  has been queried in  before time . We then define

which is the \emph{working-set number} of  at time . We also define  to be the set of all elements  at time  such that , \ie, the set of all elements with working-set number at most  at time .  Next, we define  to be the rank distance between  and  in the set , \ie,

Finally, define


We are now ready to define the fresh-finger property. In terms of the preceding notation, the unified property states that the time to access the element  at time  is 

A first attempt at defining the fresh-finger property might be 

Equation~\eqref{eq:su-1} should be contrasted with the definition of the unified property defined by \eqref{eq:u}.  In \eqref{eq:su-1}, the rank distance between  and other elements is measured only with respect to the set , the set of elements that have been accessed since the last access to  (\ie, a set of \emph{fresh fingers}).  In \eqref{eq:u}, the rank distance is measured with respect to , the entire set of elements stored in the data structure.

Since , \eqref{eq:su-1} is certainly a stronger requirement.  Unfortunately, it is too strong, and there is no comparison-based data structure that can achieve this bound in the worst-case. To see this, consider the access sequence

where .  Let  (so that  represents the second access to ).  Then .  But then the rank difference, , between  and  is at most 1 and , so, according to \eqref{eq:su-1}, the time to accessing  is at most
But this is true for any , so for \emph{any} of the  choices for , \eqref{eq:su-1} requires that a data structure execute the access in constant time. This is not information-theoretically possible; accessing a randomly chosen  requires at least  comparisons in expectation.  

From the preceding discussion, we conclude that the set in which we measure rank distance should be expanded.  This leads to the following definition:

\begin{definition} A data structure has the fresh-finger property if its runtime for an access is bounded by

\end{definition}

Observe that the set  contains elements that have been accessed less recently than . These additional elements will allow us to support the rest of the access cost while respecting information-theoretic lower bounds. The intuition for expanding the set under consider in this manner is the fact that the data structure will consist of substructures that increase doubly-exponentially in size, and so by squaring the working-set number under consideration, we take advantage of elements in an adjacent substructure.

For brevity, we define

and


As an example, consider the following access sequence, where  is the element currently being accessed at the end of the sequence.


The original definition of the fresh-finger property uses , while the modified definition uses . This modified definition allows for  to contribute to the rank distance. This results in a query time that does not violate information-theoretic lower bounds, since it does not result in a situation where all  queries must be executed in constant time. 
Before presenting a data structure that (nearly) achieves the fresh-finger property, it is worth doing a sanity-check of this definition.  In particular, we confirm that there exists (distributions over) sequences  such that

is a lower-bound for accessing .

\begin{theorem}
  For all positive integers , , and , there exists a distribution, , over  such that, for any comparison-based dictionary data structure, , that stores , and an access sequence  drawn from 
\begin{enumerate}
 \item .
 \item the expected number of comparisons performed by  while accessing  is .
\end{enumerate}
\end{theorem}

\begin{proof}
  The sequence  is defined as  for  or  is selected
  uniformly at random from the set  for .  This
  choice of  immediately implies that 
  
  is a lower-bound on the expected number of comparisons performed by 
  while accessing the (randomly chosen values) .
  This establishes Part~2 of the result.

  On the the other hand, to establish Part~1, we have
  
  Thus,
  
  since .
\end{proof}



\section{Towards the fresh-finger property}
\label{section:main}

In this section, we describe a data structure that comes to within a small additive term of achieving the fresh-finger property.

\subsection{The data structure}
\label{section:main:datastructure}

The data structure consists of  finger search trees  as well as  accompanying queues . Recall that finger search trees can support insertions and deletions in  worst-case time (when provided with a pointer to the element to be deleted) and finger searches in  worst-case time, where  is the distance between the element being searched for and the supplied pointer into the data structure \cite{DBLP:journals/jcss/BrodalLMTT03}. 

The size of  is , except for  which has size . It follows that  is . We will maintain the invariant that  for all . The queue  contains exactly the same elements as  in the order they were inserted into . Pointers are maintained between elements in the queue and corresponding elements in the finger search tree.

To perform a search, we will perform finger searches for  in  until we find  for the first time (say, ). In , we use an arbitrary element as the starting finger for the search. In all other trees, we run two finger searches for  in parallel: one from the successor of the element found in the previous finger search tree, and one from the predecessor of the element found in the previous finger search tree. As soon as the first of these two searches terminates, we stop the other. 

To restructure the data structure after we have found , we must insert  into  (note that  is not present in any of these trees, since if it were, it would have already been found) and enqueue  in . At this point, we note that each of  and  are too big. We therefore dequeue the oldest element in each of  and delete the corresponding elements in .

\subsection{Analysis}
\label{section:main:analysis}

Recall that we are aiming for a running time of


Consider a search for  at time , and consider the element . Suppose  first appears in  and  first appears in . Because  first appears in , we have that . Therefore, . Similar reasoning shows , so that . We consider three cases, based on how  compares with :

If  (\ie,  appears no later than ),\footnote{In fact, this case can only occur when , since otherwise , and so , which contradicts the definition of .} then the running time follows easily:  has working-set number . The element  can thus be found in time , which is .




The more interesting case occurs when  (\ie,  appears after ).  In this case, the algorithm will reach the tree, , containing  in time 

The search in  finds both the predecessor and and successor,  and  of  in .  That is,

and  is not in the open interval .  In particular,
for any set  one of  or , say , has

Indeed, from this point onwards, every search in , for each ,  such that


The elements in  are all in , and so
the remaining searches in  therefore take a total of at most

time.




At last, the final search, in  is the expensive one, since the only
guarantee we have on the elements of  are that their working-set number
is at most .  Thus, the elements in  are a subset of the elements in  and the time to search in  is at most


In either case, the total search time thus far is at most 


At this point,  has been found and we must now adjust the data structure. First,  must be inserted in . Because we have a finger for  inside each of these structures, this takes total time . Enqueuing  in each of  also takes . The subsequent deletions and dequeueings of the oldest elements in  and  take a total of  time as well, since the dequeueing operation takes  time and provides a pointer to the node in the corresponding tree where the deletion must be performed. Therefore, all restructuring operations take time .

We therefore have

\begin{theorem}
\label{theorem:main}
There exists a static dictionary over the set  that supports querying element  in worst-case time

\end{theorem}
	
\section{Conclusion}
\label{section:conclusion}

In this paper, we defined a stronger version of the unified property and described a data structure that achieves it to within a small additive term. Instead of computing rank distance over the entire dictionary, we compute rank distance only within a working-set containing an element that is close to a recently-accessed element.

There are several possible directions for future research.

\begin{enumerate}
\item One can measure distances within the set  instead of the set  by changing how the substructures grow. Is it possible to reduce this further? For example, is it possible to measure distances within the set ?

\item We argued that it is not possible to measure within  in the \emph{worst case}. Is it possible to measure within this set in the amortized sense?

\item Can the additive term in Theorem~\ref{theorem:main} be reduced? It seems difficult to reduce this term below  using an approach similar to the one presented here, since elements must shift through at least this many substructures.

\item We have only considered the case where  is static. Is it possible to maintain the fresh-finger property while supporting insertions into and deletions from ?
\end{enumerate}



\bibliographystyle{splncs}
\bibliography{dblp,other}

\end{document}
