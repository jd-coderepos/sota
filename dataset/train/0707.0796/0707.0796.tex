

\documentclass[final, a4paper]{IEEEtran}
\usepackage{amsmath}
\usepackage{theorem}
\usepackage{amssymb}
\usepackage{times}
\usepackage{color}
\usepackage{epsfig, epsf}
\usepackage{cite,url}


\newcommand{\ls}[1]
   {\dimen0=\fontdimen6\the\font
    \lineskip=#1\dimen0
    \advance\lineskip.5\fontdimen5\the\font
    \advance\lineskip-\dimen0
    \lineskiplimit=.9\lineskip
    \baselineskip=\lineskip
    \advance\baselineskip\dimen0
    \normallineskip\lineskip
    \normallineskiplimit\lineskiplimit
    \normalbaselineskip\baselineskip
    \ignorespaces
   }

\newcommand{\insertfig}[4]{
\begin{figure}[tbh]
\centerline{\includegraphics[width=#1\columnwidth]{#2.eps}}
\vspace{-0.3cm}
\caption{#3}\label{#4}\end{figure}}

\newcommand{\insertpstex}[4]{\begin{figure}[h]
\centerline{\resizebox{#1}{!}{
\input #2.pstex_t}}
\caption{#3}
\label{#4}
\end{figure}}
\DeclareMathAlphabet{\mathsfbf}{OT1}{cmss}{sbc}{n}


\newcommand{\example}[2]{
\begin{center}
\parbox{0.95\columnwidth}{
\rule{0.95\columnwidth}{0.5mm}\\
\noindent {\bf Example~#1:}
#2\\
\rule{0.95\columnwidth}{0.5mm}
}
\end{center}
}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lem}{Lemma}[section]

\newcommand{\CC}{\mathbb{C}} \newcommand{\EE}{\mathop{\mathbb{E}}\limits} \newcommand{\PP}{\mathbb{P}} \newcommand{\RR}{\mathbb{R}} \newcommand{\ZZ}{\mathbb{Z}} \newcommand{\Herm}{^\dagger} 

\newcommand{\ee}{{\rm e}}
\newcommand{\jj}{{\rm j}}  \newcommand{\ii}{{\rm i}}  \newcommand{\dd}{{\rm\,d}} 

\newcommand{\erf}{{\rm erf}} 

\newcommand{\av}{{\bf a}}
\newcommand{\bv}{{\bf b}}
\newcommand{\cv}{{\bf c}}
\newcommand{\dv}{{\bf d}}
\newcommand{\ev}{{\bf e}}
\newcommand{\fv}{{\bf f}}
\newcommand{\gv}{{\bf g}}
\newcommand{\hv}{{\bf h}}
\newcommand{\iv}{{\bf i}}
\newcommand{\jv}{{\bf j}}
\newcommand{\kv}{{\bf k}}
\newcommand{\lv}{{\bf l}}
\newcommand{\mv}{{\bf m}}
\newcommand{\nv}{{\bf n}}
\newcommand{\ov}{{\bf o}}
\newcommand{\pv}{{\bf p}}
\newcommand{\qv}{{\bf q}}
\newcommand{\rv}{{\bf r}}
\newcommand{\sv}{{\bf s}}
\newcommand{\tv}{{\bf t}}
\newcommand{\uv}{{\bf u}}
\newcommand{\wv}{{\bf w}}
\newcommand{\vv}{{\bf v}}
\newcommand{\xv}{{\bf x}}
\newcommand{\yv}{{\bf y}}
\newcommand{\zv}{{\bf z}}
\newcommand{\zerov}{{\bf 0}}
\newcommand{\onev}{{\bf 1}}

\newcommand{\Am}{{\bf A}}
\newcommand{\Bm}{{\bf B}}
\newcommand{\Cm}{{\bf C}}
\newcommand{\Dm}{{\bf D}}
\newcommand{\Em}{{\bf E}}
\newcommand{\Fm}{{\bf F}}
\newcommand{\Gm}{{\bf G}}
\newcommand{\Hm}{{\bf H}}
\newcommand{\Id}{{\bf I}}
\newcommand{\Jm}{{\bf J}}
\newcommand{\Km}{{\bf K}}
\newcommand{\Lm}{{\bf L}}
\newcommand{\Mm}{{\bf M}}
\newcommand{\Nm}{{\bf N}}
\newcommand{\Om}{{\bf O}}
\newcommand{\Pm}{{\bf P}}
\newcommand{\Qm}{{\bf Q}}
\newcommand{\Rm}{{\bf R}}
\newcommand{\Sm}{{\bf S}}
\newcommand{\Tm}{{\bf T}}
\newcommand{\Um}{{\bf U}}
\newcommand{\Wm}{{\bf W}}
\newcommand{\Vm}{{\bf V}}
\newcommand{\Xm}{{\bf X}}
\newcommand{\Ym}{{\bf Y}}
\newcommand{\Zm}{{\bf Z}}

\def\Bmb{\bar{\Bm}}
\def\Hmb{\bar{\Hm}}
\def\Smb{\bar{\Sm}}

\def\Hmt{\widetilde{\Hm}}
\def\Wmt{\widetilde{\Wm}}
\def\Ymt{\widetilde{\Ym}}
\def\Zmt{\widetilde{\Zm}}

\def\Xmh{\widehat{\Xm}}
\def\Ymh{\widehat{\Ym}}
\def\Zmh{\widehat{\Zm}}

\def\yvt{\widetilde{\yv}}
\def\tvt{\tilde{\tv}}
\def\zvt{\widetilde{\zv}}

\def\xvh{{\hat{\xv}}}
\def\yvh{\widehat{\yv}}
\def\tvh{{\hat{\tv}}}

\def\Ht{\widetilde{H}}
\def\xh{\widehat{x}}
\def\alphah{\widehat{\alpha}}



\newcommand{\Ac}{{\cal A}}
\newcommand{\Bc}{{\cal B}}
\newcommand{\Cc}{{\cal C}}
\newcommand{\Dc}{{\cal D}}
\newcommand{\Ec}{{\cal E}}
\newcommand{\Fc}{{\cal F}}
\newcommand{\Gc}{{\cal G}}
\newcommand{\Hc}{{\cal H}}
\newcommand{\Ic}{{\cal I}}
\newcommand{\Jc}{{\cal J}}
\newcommand{\Kc}{{\cal K}}
\newcommand{\Lc}{{\cal L}}
\newcommand{\Mc}{{\cal M}}
\newcommand{\Nc}{{\cal N}}
\newcommand{\Oc}{{\cal O}}
\newcommand{\Pc}{{\cal P}}
\newcommand{\Qc}{{\cal Q}}
\newcommand{\Rc}{{\cal R}}
\newcommand{\Sc}{{\cal S}}
\newcommand{\Uc}{{\cal U}}
\newcommand{\Wc}{{\cal W}}
\newcommand{\Vc}{{\cal V}}
\newcommand{\Xc}{{\cal X}}
\newcommand{\Yc}{{\cal Y}}
\newcommand{\Zc}{{\cal Z}}

\newcommand{\alphav}{\hbox{\boldmath}}
\newcommand{\betav}{\hbox{\boldmath}}
\newcommand{\gammav}{\hbox{\boldmath}}
\newcommand{\deltav}{\boldsymbol{\delta}}
\newcommand{\etav}{\hbox{\boldmath}}
\newcommand{\lambdav}{\hbox{\boldmath}}
\newcommand{\epsilonv}{\hbox{\boldmath}}
\newcommand{\nuv}{\hbox{\boldmath}}
\newcommand{\muv}{\hbox{\boldmath}}
\newcommand{\zetav}{\hbox{\boldmath}}
\newcommand{\phiv}{\hbox{\boldmath}}
\newcommand{\psiv}{\hbox{\boldmath}}
\newcommand{\thetav}{\hbox{\boldmath}}
\newcommand{\tauv}{\hbox{\boldmath}}
\newcommand{\omegav}{\hbox{\boldmath}}
\newcommand{\xiv}{\hbox{\boldmath}}
\newcommand{\sigmav}{\hbox{\boldmath}}
\newcommand{\piv}{\hbox{\boldmath}}

\newcommand{\Gammam}{\hbox{\boldmath}}
\newcommand{\Lambdam}{\hbox{\boldmath}}
\newcommand{\Deltam}{\hbox{\boldmath}}
\newcommand{\Sigmam}{\hbox{\boldmath}}
\newcommand{\Phim}{\hbox{\boldmath}}
\newcommand{\Pim}{\hbox{\boldmath}}
\newcommand{\Psim}{\hbox{\boldmath}}
\newcommand{\Thetam}{\hbox{\boldmath}}
\newcommand{\Omegam}{\hbox{\boldmath}}
\newcommand{\Xim}{\hbox{\boldmath}}

\def\deltat{\widetilde{\delta}}
\def\sigmat{\widetilde{\sigma}}

\newcommand{\sinc}{{\hbox{sinc}}}
\newcommand{\diag}{{\hbox{diag}}}
\renewcommand{\det}{{\hbox{\rm det}}}
\def\toas{\stackrel{a.s.}{\longrightarrow}}
\def\trace{\mathsf{Tr}}
\def\vec{\mathrm{vec}}
\newcommand{\eqdef}{\ensuremath{\stackrel{\mbox{\upshape\tiny }}{=}}}
\def\ben{\begin{enumerate}}
\def\beq{}
\def\bit{\begin{itemize}}
\def\een{\end{enumerate}}
\def\eeq{}
\def\eit{\end{itemize}}
\def\dst{\displaystyle}
\def\non{\nonumber\\}
\def\argmax{\mathop{\mathrm{arg~max}}\limits}
\def\argmin{\mathop{\mathrm{arg~min}}\limits}
\def\union{\mathop{\cup}\limits}

\def\MSEav{{\rm MSE}_{\rm av}}
\def\MSEinf{{\rm MSE}_{\infty}}
\def\limbeta{\lim_{\substack{M,r\rightarrow +\infty \\\beta}}}
\def\limbetaomega{\lim_{\substack{M,r\rightarrow +\infty \\ \sigma_\delta\rightarrow 0
                         \\\beta, \omega}}}

\title{Performance of Linear Field Reconstruction\\ Techniques
with Noise and Uncertain Sensor Locations
}
\author{Alessandro Nordio, Carla-Fabiana Chiasserini, Emanuele Viterbo\\
Dipartimento di Elettronica, Politecnico di Torino\\
C. Duca degli Abruzzi 24, I-10129 Torino, Italy\\
Phone: +39 0115644183, Fax: +39 0115644099
E-mail: \tt{\{alessandro.nordio,carla.chiasserini,emanuele.viterbo\}@polito.it} \\
{\rm  Corresponding author}}

\begin{document}

\maketitle

\begin{abstract}
We consider a wireless sensor network, sampling a 
bandlimited field, described by a limited number of harmonics.
Sensor nodes are irregularly deployed over the area of
interest or subject to random motion; in addition sensors
measurements are affected by noise. Our goal is to obtain a high
quality reconstruction of the field, with the mean square error
(MSE) of the estimate as performance metric. In particular, we
analytically derive the performance of several
reconstruction/estimation techniques based on linear filtering. For
each technique, we obtain the MSE, as well as its
asymptotic expression in the case where the field
number of harmonics and the number of sensors grow
to infinity, while their ratio is kept constant.
Through numerical simulations, we show the validity of the
asymptotic analysis, even for a small number of sensors.
We provide some novel guidelines for the design of sensor networks when many parameters,
such as field bandwidth, number of sensors, reconstruction quality,
sensor motion characteristics, and noise level of the measures, have
to be traded off.
\end{abstract}

{\bf EDICS:}  DSP-RECO Signal reconstruction, DSP-SAMP Sampling,
SEN-APPL Applications of sensor networks,
SEN-FUSE Data fusion from multiple sensors,
SPC-PERF Performance analysis and bounds

\newpage
\section{Introduction}
Wireless sensor networks are often used for applications like environmental and traffic control,
habitat monitoring, or weather forecasts~\cite{Akyildiz02}, which
require to sample a physical phenomenon over an area of interest
(the sensor field).
In this paper, we consider a set of
sensors communicating with a sink node,
through either single- or multi-hop communications.
Each sensor locally samples the physical field, while the sink collecting all samples
is in charge of reconstructing the signal of interest.

We assume that initially sensors are either located at pre-defined positions, or, if randomly deployed over the
network area, their location
can be estimated at the sink node (see~\cite{Hightower01,Hu04,Moore04} for
a description of node location methods in sensor networks).
We do not deal with spatio-temporal correlation, but
consider a fixed time instant and focus on
the spatial sampling and reconstruction of the sensor field.
We note that, in general, sensors provide an irregular sampling of the observed phenomenon.
This may be due to various reasons: random deployment of the nodes,
environment characteristics that bias the network deployment,
sensors entering a sleep mode, inaccuracy in sensor positioning, or
nodes movement~\cite{Ganesan03}. In all these cases the sink has to
reconstruct the field from a collection of samples that are {\em
irregularly} spaced, different from the classical equally (or {\em regularly}) spaced sampling.

The problem of signal reconstruction from irregular samples has been
widely addressed in signal processing, where several efficient and
fast algorithms have been proposed to numerically reconstruct or
approximate a signal \cite{Feichtinger95,Grochenig99}. The problem
we address in this work, however, is different; the questions we
pose are:
\begin{quote}
{\em (i) How do noisy measures and inaccurate knowledge of
the sensor positions affect the quality of the reconstructed signal?}\\
{\em (ii) How can we trade off system parameters like measurement
noise, field bandwidth, signal reconstruction quality and number of
sensors?}
\end{quote}

To answer these questions we analyze two different models of the monitoring system that account for
the quality of the measurements performed by the sensors and differ in
the accuracy with which the sensor positions are known at the sink node.
More specifically, the model denoted as {\bf Model A} refers to the case
where sensors are fixed, the sink has perfect knowledge of the sensor positions,
but the sensor measurements are affected by error. In the second model, named {\bf Model B},
besides noisy measurements, we consider that the sensors position varies around an average
value, and only the average location of the nodes is known at the sink.
Examples where this model applies are observation systems using surface
buoys~\cite{AOSN}, underwater robots located at different depths~\cite{Bokser04,Cayirci06},
dropsondes or low-cost unmanned platforms, as in \cite{Majumdar06}.

For each of these models, we use as field
reconstruction techniques some linear filters that are commonly
employed in signal detection and estimation, and we evaluate the
mean square error of the resulting estimate.

We find that a key parameter for the network performance is the
ratio  of  the field number of harmonics to the number of sampling
sensors. In particular, there exists a value of this ratio,
beyond which the performance of all considered reconstruction
strategies degrade significantly, even for low values of noise level
and limited uncertainty on the sensor positions. To obtain an
acceptable reconstruction quality when  is large (i.e., the
number of available sensors is limited compared to the field
bandwidth), reconstruction techniques that exploit some knowledge of
the measurement noise and of the jitter in the sensors position must be employed.

We also carry out an asymptotic analysis of the system as the field
number of harmonics and the number of sensors grow to infinity, while their
ratio  is kept constant, and we show that this is an
effective tool to study the system performance, even when the number
of sensors is small. Finally, we find a lower bound to the mean
square error that can be achieved by any of the considered
techniques, both under Model A and Model B.

The remainder of the paper is organized as follows.
In Section~\ref{sec:system_model} we present our assumptions and
the system models under study. Section \ref{sec:related-work}
highlights our contribution with respect to previous work.
Section~\ref{sec:defs_perf} introduces the performance metrics and
provides some mathematical tools necessary for our study. Model A
and B are analyzed in Sections~\ref{sec:model_A}
and~\ref{sec:model_C}, respectively.
Finally, in Sections \ref{sec:summary} and \ref{sec:conclusions} we summarize our main results and
draw some conclusions.


\section{Assumptions and System Models}
\label{sec:system_model}

Let us consider a one-dimensional bandlimited field
 represented by  harmonics as

The field is observed within one period interval  and sampled by  sensors
placed at positions\footnote{Column vectors are denoted by bold lowercase
letters, matrices are denoted by bold upper case letters. The
 entry of the matrix  is denoted by . The
 identity matrix is denoted by , the generic
identity matrix is denoted by , and the conjugate transpose
operator is denoted by }
, , 
which are in general not equally spaced.
The signal samples are denoted by the column vector .
The field discrete spectrum is given by the  complex
vector . The complex numbers  represent amplitudes and phases
of the harmonics in . We can think of  as the approximate one-sided bandwidth of the field.


We assume that the entries of  are
i.i.d. uniformly distributed random variables in . The
extension to a multi-dimensional field can be easily obtained, as
discussed later in this section.

We define  as the ratio of the number of harmonics
which describe the field to the number of sensors, i.e., . This is an
important parameter in our analysis.
Note that the number of sensors  also corresponds to the sampling rate;
thus, the number  is the ratio of twice the field bandwidth
to the sampling rate (frequency).
In particular, in regular sampling theory, exact reconstruction is achieved for  and,
if a Nyquist regular sampling interval were used, we would have: .

We consider  to be known, and the random vector  to have zero mean and covariance matrix
, where  corresponds to the field
average power spectral density.

The value  of the field at positions  depends on the spectrum 
through the expression

where  is the 
{\em generalized Fourier matrix} defined as:

The dependence of the matrix  on the position vector 
is clearly indicated by its subscript.
When the samples are equally spaced in the interval ,
the matrix  is a unitary matrix
(i.e., ).
The above system model refers to a uni-dimensional
field where sensor positions are determined by a scalar variable.
However, the extension to the multi-dimensional case can still be
easily obtained since the relation between field spectrum and
samples in a band-limited multi-dimensional field can be expressed
in a matrix form similar to (\ref{eq:s}), where only the structure
of the matrix  differs.

Finally, we assume that sensor field measures are sent to a processing unit,
the so-called {\em sink} node, whose task is to provide an estimate
of the sensed field.
Since we focus on the reconstruction of the physical field,
we consider that sensor transmissions always reach successfully the sink node\footnote{Note that this is a fair assumption since, when ARQ or FEC techniques are used, the information either
is correctly retrieved at the sink or it is lost. The latter case corresponds to reduced value of 
}.



By relying on the assumptions discussed above, we study the following two systems.
\begin{itemize}
\item  {\bf Model A: Fixed sensors, perfect knowledge of the sensor positions, noisy measures}\\
In this model, sensors have a fixed position, given by the vector
 and known at the sink node, but each sensor provides a measure
of the field affected by additive noise with zero mean and variance
~\cite{Vuran04}.
The additive noise approximates the errors affecting the measurement procedure~\cite{Ergen06}.


The measures vector can therefore be written as:

where  is the true field and the zero mean noise vector is denoted by
, with covariance matrix .


\item {\bf Model B: Sensors with jittered positions and noisy measures}\\
In this case each sensor moves around an average position
 (), i.e., the sensor positions are given
by: , where:  and  is
the displacement of the sensors with respect to their average
location .  Note that our problem differs from
the well known problem of jittered sampling (see e.g.,
\cite{LiuStanley}), since we deal with irregular sample locations.
The displacements , , are modeled as
independent zero mean Gaussian random variables with variance
 and .  For convenience and
neglecting the edge effects, we consider  so that 
falls in the observation interval . The vector  of
measures is still given again by~(\ref{eq:p_fixed}). Also,  noise,
displacement, and field spectrum are assumed to be 
uncorrelated, hence , and the sink has perfect knowledge
of .
\end{itemize}

\section{Our contribution with respect to previous work}
\label{sec:related-work}

Given a network where sensors can enter a low-power operational
state (i.e., a sleep mode), the work in \cite{Perillo04}  presents an algorithm to determine which
sensor subsets should be selected to acquire data from an
area of interest and which nodes should remain inactive to
save energy.
A similar problem is addressed in \cite{Willett04}, where an adaptive sampling is
described, which
allows the central data-collector to vary the number of active
sensors, i.e., samples, according to the desired resolution level.
The optimal sensor density that minimizes the network energy consumption,
subject to constraints on the quality of the reconstructed signal and
network lifetime, is studied in \cite{Maleki05}.
Note that in our work we consider an irregular topology, which may be
caused by nodes moving into a sleep state; however we do not directly
address energy efficiency or scheduling of the node sleep/activity
periods.

In \cite{Kumar03}, the authors consider a uni-dimensional field, uniformly sampled at the
Nyquist frequency by low-precision sensors. The impact on the
field reconstruction accuracy of quantization errors
and node density is evaluated.
The effect of random error sources affecting the ADC, besides quantization,
is investigated in \cite{Ergen06}.
In our work we consider an additive noise that models errors due to the measurement
procedure as well as errors due to the ADC, but we do not specifically focus
on the latter issue.

The impact of medium access control (MAC) protocols on
the reconstruction of a signal field is investigated in \cite{Dong04}.
Both deterministic and random MAC schemes are considered,
and performance are derived as the number of received packets and
the experienced SNR vary.

Related to our work is also the literature on
spectral analysis~\cite{Maravic, Stoica}, which
deals with the problem of recovering the amplitude of sine waves
immersed in noise.
Note, however, that techniques such as MUSIC do not estimate phases; thus,
we do not compare with such techniques since our linear filtering reconstruction yields the
estimate of both amplitudes and phases.

The field reconstruction at the sink node with spatial and
temporal correlation among sensor measures is studied in
\cite{CristescuVetterli,Poor,Vuran04,Rachlin1}.
In particular, in~\cite{Rachlin1} the observed field is a discrete vector of
target positions and sensor observations are dependent. By modeling
the sensor network as a channel encoder and exploiting some concepts
from coding theory, the network capacity,
defined as the maximum ratio of target positions to number of sensors,
is studied as a function of noise, sensing function and sensor connections.
The paper by Dong and Tong~\cite{DongTong} focuses on signal reconstruction from possibly random
samples, as we do. However, two major issues make our work significantly
different from~\cite{DongTong}. Dong and Tong indeed assume that the exact
sensors locations are  known and that the central controller
always receives a sufficiently large number of samples. These assumptions allow
an interpolation method, which is used in  Dong and Tong's work,
to provide good performances. In our case, instead, even
in the asymptotic analysis, the ratio of the number of harmonics
to the number of samples is kept constant and, hence, interpolation
may be highly inefficient as we will show in the following.


The problem of reconstructing a band-limited signal from an
irregular set of samples at unknown locations is addressed
in~\cite{Marziliano00}. There the signal is oversampled by
irregularly spaced sensors; sensor positions are unknown but always
equal to an integer multiple of the sampling interval. Different
solution methods are proposed, and the conditions for which there
exist multiple solutions or a unique solution are discussed.
Differently from~\cite{Marziliano00}, we assume that the sink can
either acquire or estimate the sensor locations and that sensors are
randomly deployed over a finite interval.

Finally, in our previous work \cite{NordioChiasseriniViterbo} some conditions
on the irregular topology of the sensor network are identified, which allow for
a successful signal reconstruction, both under deterministic and random
node deployment. In particular, in \cite{NordioChiasseriniViterbo} the
spectrum estimate ,
computed by the sink, is obtained by applying to  the
Moore-Penrose pseudo-inverse of the matrix , i.e.,
.
The system model adopted in \cite{NordioChiasseriniViterbo} is ideal in the sense that
the reconstruction algorithm has perfect knowledge of the vector  and
neglects noisy measures: the failure in reconstruction (i.e. )
is only due to the bad conditioning of the matrix  in relation to the
finite machine precision.
In this work, instead, we propose to apply linear filters to the field reconstruction
and consider the following causes of quality degradation: {\em (i)}
noisy measures, and {\em (ii)} uncertainty at the sink on the sensors position.


\section{Preliminaries}
\label{sec:defs_perf}

Here we describe the techniques we use for field reconstruction,
and define the performance metrics employed for assessing
the effectiveness of these techniques on the quality of the reconstructed field.
Finally, we provide some mathematical tools necessary for the analysis of the
models under study.

\subsection{Reconstruction techniques}
Several reconstruction techniques have been proposed in the literature,
which  amount to the solution of a linear system
(see \cite{Feichtinger95,Grochenig99} and the references cited therein). A
widely used technique consists in processing the measures  by means of
a linear filter,
, which is an  matrix and is a function of the system parameters known
at the sink. In this case, the estimate of the field spectrum is given by:

The system model in~(\ref{eq:p_fixed}) is similar
the one employed in multiuser communications~\cite{Verdu_book} or multiple antennas
communications \cite{TulinoVerdu,BigTarTul}.
In those cases  is the received signal, the matrix  plays the role of
spreading matrix or channel matrix,  is the transmitted signal and  is
the channel noise.
By relying on the results obtained in those fields, for each system
model we propose and compare some reconstruction techniques
characterized by different matrices : the {\em matched filter
(MF)}, the {\em zero forcing (ZF)} filter and some {\em linear filters
minimizing the mean square error} (LMMSE)~\cite{Verdu_book}. In the
field of multiuser detection, the MF simply correlates the received
signal with the desired user's time reversed spreading waveform,
thus it does not take into account any other users in the system or
channel dynamics. The ZF filter counteracts multiuser interference
but it ignores the presence of channel noise. The LMMSE solution
minimizes the squared error between the received and transmitted
signals, thus accounting also for the channel noise; it becomes the
zero-forcing solution when no noise is present. Note that the
advantage of the MF with respect to the ZF and LMMSE filters is that
no matrix inversion is needed; while, between the ZF and the LMMSE
filter, clearly the best performance in terms of minimum square
error is given by the LMMSE, however the advantage of the ZF filter
is that it does not require any knowledge of the noise component
(see \cite{Verdu_book} for further details).

\subsection{Performance metrics}
\label{sec:MSE}
Given the spectrum estimate~(\ref{eq:hat_a}), the field can be reconstructed
as:

As a measure of the quality of the reconstruction, we consider
the {\em mean square error} (MSE) of the estimate of , which is given by:

We observe that computing MSE as above is equivalent to computing . 
Indeed, we have:

Therefore, in the following, for a given vector of sensor positions , we consider the MSE defined as:


where

is a  matrix,
the operator  averages with respect to all random variables of the model, and
 is the trace operator.
Also, in (\ref{eq:MSE}) we exploited the fact that, for any vector ,
we have: .


Next, we consider the vector  to be random.
In this case a more appropriate performance metric is the
{\em average} MSE, normalized to , i.e.,

where MSE is as in (\ref{eq:MSE}) and  averages over the realizations of .

When the parameters
 and  grow to infinity while the ratio  is kept constant, we
define the {\em asymptotic average} MSE as:

Our results will show later that  gives a very good approximation of MSE already for small
values of . This is a common feature of asymptotic analysis based on random matrices~\cite{TulinoVerdu}.

\subsection{Some mathematical tools}

\subsubsection{The functional }
\label{sec:free|probability}

Let us first consider an  Hermitian random matrix  and the functional:

Using~(\ref{eq:Psi}) and~(\ref{eq:MSEinf}), the asymptotic MSE can be written as:

In our analysis we use  the following results on the functional
. First, we notice that: . Secondly, we
can prove that, if  is an analytic function defined in  then\footnote{
Note the small abuse of notation
when using  for both scalar and matrix argument
}

where  is a random variable with the asymptotic eigenvalue distribution of .
The proof is given in Appendix~\ref{app:proof3}.


\subsubsection{A simple expression for }
As will be clear in Section~\ref{sec:model_C},
in the analysis of Model B many parameters are functions of
the matrix , where .
It is thus useful to derive an expression of  as a
function of , in order to separate the random part  of 
from the constant part . From~(\ref{eq:F}), the  entry of 
is defined as:

A useful expression of  in terms of  is given below.

\begin{lem}
\label{lemma:1}
For any vector  of size , let the  entry of the matrix
 be

for , and . Let the size  column vectors
, , and  be such that , then

where  is an  diagonal matrix, and
 is a  diagonal matrix with .
\end{lem}
\medskip
\noindent {\it Proof:} The proof is given in Appendix~\ref{app:proof}.

\section{Analysis of Model A}
\label{sec:model_A}

Here we consider the case where sensor positions are fixed and known
at the sink but the field estimates are degraded by noisy measures.
We analyze three different linear filters: the matched filter, the
zero forcing filter and the minimum mean square error filter
\cite{Verdu_book}.
In all cases, for any fixed , the filter matrix  is deterministic.
Thus, using (\ref{eq:p_fixed}),~(\ref{eq:hat_a}), and~(\ref{eq:Psi}) we obtain:

where

is the {\em signal-to-noise ratio on the measure}.
The MSE expression specialized to the different filters is given below.



\subsection{Matched filter}
As a first solution, we choose  as the filter matched to
. The MF is optimal  when the collected samples are equally
spaced,  that is when the rows of  are orthonormal
vectors and  is a unitary matrix (i.e.,
). Thus, we choose:

Recall that  depends on the position vector  that,
under Model A, coincides with the actual sensor positions.
Indeed, in the absence of noise and for equally spaced sensors, we have
the spectrum estimates perfectly match , i.e.,


By replacing (\ref{eq:A_B_MF}) in (\ref{eq:MSE_A}), we obtain the following
expression for :

where .
From the definition in~(\ref{eq:MSEinf-Psi}), the asymptotic MSE,
averaged over the random vector , is given by:

Notice that the second term on the right hand side reduces to  since .
Applying~(\ref{eq:g(X)}),  first with  and then with , we obtain:

where  is the random variable with probability density function (pdf)
, distributed as the asymptotic eigenvalues of .
 In~\cite{NordioChiasseriniViterbo} it is shown that, for any positive integer ,
 is a polynomial in  of degree .
In particular  and . We therefore obtain:




\subsection{ZF filter}
\label{sec:A_ZF}
The expression of the ZF filter for the system in~(\ref{eq:p_fixed}) is:

Notice that, by its definition, the ZF filter does not exploit any information on the
noise contribution (such as ). However,  this
reconstruction technique
takes into account the fact that the collected samples are not equally spaced and, hence,
that   is not a unitary matrix.

By using (\ref{eq:A_B_ZF}) in (\ref{eq:MSE_A}), the matrix  becomes:

Using the definition in~(\ref{eq:MSEinf-Psi}) and applying~(\ref{eq:g(X)})
with , the asymptotic MSE,
averaged over the random vector , can be written as:

We can make the following observations on the behavior of the :
\begin{itemize}
\item[{\em 1)}] Since  is a convex function, then .
In~\cite{NordioChiasseriniViterbo} it is shown that =1, thus it results: .

\item[{\em 2)}] We have:
 only for , with .
Indeed



In~\cite{NordioChiasseriniViterbo} it has been  empirically observed through Monte-Carlo simulation
that for :

where the exponent  is a decreasing function of  for
, and  for .
Given that, for any positive constant , we have:

where the integral in the right hand side (and
therefore~(\ref{eq:MSEinf_A_ZF})) does not diverge if and only if
, that is . This observation gives
us a fundamental limit to the minimum number of sensors required to
perform reliable reconstruction with the ZF filter.
\end{itemize}


\subsection{LMMSE linear filter}
\label{sec:A_LMMSE}
A more efficient solution is to employ the filter  that provides the
minimum MSE (LMMSE).
By assuming that the signal-to-noise ratio  is known to the sink and exploiting
this information for the filter design,
the expression of the LMMSE filter~\cite{Verdu_book} for
Model A in~(\ref{eq:p_fixed}) is given by:

We highlight that this reconstruction technique accounts for both the fact that
the collected samples are non-uniformly spaced and the presence of the measurement noise.

Substituting (\ref{eq:A_B_LMMSE}) in~(\ref{eq:MSE_A}), we obtain:

Using~(\ref{eq:g(X)}) with , the asymptotic MSE is:

Note that:

Also note that , since
. Given that the LMMSE filter provides the minimum
MSE, from~(\ref{eq:MSE_bound}) {\em it turns out that, for a given },
 {\em is a lower
bound for the performance of all linear reconstruction techniques}.


We summarize the main results of this section in Table~\ref{table:results-modelA}.

\begin{table}
\begin{center}
\caption{Results obtained under Model A \label{table:results-modelA}}
\begin{tabular}{|l||c|c|c|} \hline
              & \rule[-1.5mm]{0mm}{5mm} { MF} &  { ZF} & { LMMSE} \\ \hline \hline
    & &  \rule[-2mm]{0mm}{6mm}  &   \\ \hline
  &    &
              \rule[-2mm]{0mm}{6mm} 
              &  \\ \hline
    &  &
              \rule[-2mm]{0mm}{6mm} 
              &  \\ \hline
\end{tabular}
\end{center}
\end{table}


\subsection{Results}
In Figure~\ref{fig:MSE_model_A_0} we compare the average MSE obtained using
the MF, ZF, and LMMSE filters, when  varies and  (i.e.,
SNR\,dB). The points labeled by `` MF'', `` ZF'' and ``
LMMSE'' have been obtained generating  realizations of the
measures~(\ref{eq:p_fixed}) with , computing the estimates as
in~(\ref{eq:hat_a}) and averaging the square error .
These points are superimposed to the solid curves labeled by ``'',
representing the asymptotic MSE and obtained evaluating~(\ref{eq:MSEinf_A_MF}), (\ref{eq:MSEinf_A_ZF}) and
(\ref{eq:MSEinf_A_LMMSE}), respectively. Notice that computing closed
form expressions for  in~(\ref{eq:MSEinf_A_ZF}) and
 in~(\ref{eq:MSEinf_A_LMMSE}) is still
an open problem since a closed form expression of the distribution of  is unknown.
Thus, for a given , the value of these
asymptotic expressions have been obtained pseudo-analytically, averaging over the
eigenvalues  obtained by several realizations of the matrix , with 
which yields a very good approximation of the asymptotic case (see~\cite{NordioChiasseriniViterbo}).

We observe an excellent agreement between the asymptotic analysis
and the numerical results; this shows the validity of the asymptotic
analysis even for values of  as low as .
We also note that, for both the filters, higher values of MSE are obtained as 
increases. Finally, the LMMSE
filter provides the best performance, while the MSE of the ZF filter
shows a vertical asymptote for , in agreement
with the closed form analysis\footnote{The numerical results for the
ZF filter are highly unstable while approaching the asymptote, thus
they are shown only for }.

\insertfig{1.00}{MSE_model_A_0}{MSE obtained through the MF, ZF, and the LMMSE filters,
plotted versus , for , SNR\,dB (i.e., )}{fig:MSE_model_A_0}

\insertfig{1.00}{MSE_model_A_1}{MSE obtained with the MF, the ZF and the LMMSE filters,
plotted versus SNR, for  and }{fig:MSE_model_A_1}
\insertfig{1.00}{MSE_model_A_2}{MSE obtained through the LMMSE filter,
plotted versus SNR, for  and }{fig:MSE_model_A_2}

Figure~\ref{fig:MSE_model_A_1} shows the MSE versus SNR, for .
The behavior of the asymptotic MSE is represented by the curves labeled by
``'' while the average MSE obtained through numerical analysis is denoted by
the label ``''. The curves have been obtained using the same procedure as for
the results in Figure~\ref{fig:MSE_model_A_0}, using  for MSE and  for  computation.
Again, note the tight match between analytical and numerical results.
For all techniques, the MSE decreases as the SNR increases.
The MF however provides very poor performance, even for high SNR. In particular, as
SNR tends to infinity, it shows a horizontal asymptote with .

Besides linear filtering, another technique for estimating the spectrum  is based on
interpolation~\cite{DongTong}.
The idea is to interpolate the measures  to a regular sampling grid defined by the vector
 where , . The interpolated vector  is then multiplied
by the matrix . Notice that in this case  is unitary i.e.
, since  represents an equally spaced sampling.
In the figure the dashed line labeled ``Linear interp.'' shows the performance obtained using linear interpolation. The MSE has a horizontal asymptote for high SNR. While it outperforms
the MF, it clearly shows poor performance for high SNR, compared to ZF and LMMSE techniques.


Figure~\ref{fig:MSE_model_A_2} presents the performance of the LMMSE filter obtained
evaluating~(\ref{eq:MSEinf_A_LMMSE}) for different values of ,
as the SNR varies.
In agreement with the results presented in Figure~\ref{fig:MSE_model_A_0},
the performance of the LMMSE filter
degrades as  increases, while, as expected, it improves as
the SNR increases.

\medskip
\example{1}{{\it We need to estimate the number of sensors required
to sample a field with  harmonics. Each sensor provides
samples with \,dB.}

We choose to employ the LMMSE filter, which provides the best
performance.
Looking at Figure~\ref{fig:MSE_model_A_2}, if we allow an
 of , then we need , i.e.,  sensors.
By doubling the number of sensors (),  drops to .
}


\section{Analysis of Model B}
\label{sec:model_C}

Here we consider the case of sensors with jittered positions and average position,
, known at the sink node.
The true sensor location is: , where  is a random vector,
as defined in Section~\ref{sec:system_model}.
The reconstruction algorithm employs the matrix , which is a
function of the known average positions . For any given  and
, similarly to~(\ref{eq:MSE}), the MSE becomes:

where

where  represents the real part of the argument.

To proceed further we need to compute the averages over the
displacements , i.e.,  we need the expression of
 and
 as functions of
, whose derivation is given in
Appendix~\ref{app:averages}. We have:

and

where  is a  diagonal matrix with
, , where
 is the characteristic function of the
displacements. Under the assumption that  has a zero mean
Gaussian distribution we have , .



Using~(\ref{eq:EFx}) and~(\ref{eq:EFxFhx}) in~(\ref{eq:Psi_B}) we obtain:

where .

In the following, in the case of the LMMSE filter\footnote{Recall that
the MF and ZF techniques, by their definition,
do not require any information on   and }
we first consider that the variance
 of the sensor movement is unknown at the sink
and, hence, the sink  assumes the sensors to be fixed (i.e.,
), while running the reconstruction algorithm. Then,
we consider that  is known and the reconstruction
algorithm employs a filter that exploits such an information to
minimize the MSE (this case is referred to as ``LMMSE for known
''.)

Finally, we remark that, while in Model A the filters used for
signal reconstruction are functions of the matrix  with
 known to the sink, in Model B only the mean value of the
sensor positions  is known and, hence, the filters are
computed using  instead of  .

\subsection{Matched filter}
\label{sec:MF_B}
If the sink node employs the MF in~(\ref{eq:A_B_MF})
as function of  (i.e., ),
then, using~(\ref{eq:MSE_C2}), we obtain:

This result holds for strictly positive . Note that, for
 (no sensor motion), we have  and ;
thus ~(\ref{eq:MSE_C_MF}) reduces to~(\ref{eq:MSE_A_MF}).

Equation~(\ref{eq:MSE_C_MF}) refers to the MSE obtained with a given
vector ; we are now interested in deriving the asymptotic
expression for the MSE.
Note that (\ref{eq:MSE_C_MF}) is a function of both  and , and
contains terms of the form
 with  and ;
also the matrix  depends on  and , while the matrix
 depends on  and .
The definition of the asymptotic MSE in~(\ref{eq:MSEinf}) refers to
the case where the number of harmonics  and the number of sensors  grow
to infinity with constant ratio ; if this is directly applied
to~(\ref{eq:MSE_C_MF}), information losses may arise. Indeed, we
have:

and thus all terms depending on the matrix  would vanish
regardless of the value of . On the contrary, in a
realistic situation we expect to obtain high reconstruction quality
when the standard deviation of the motion () is
smaller than or comparable to the average sensor separation (),
and a significant degradation of the reconstruction quality when
 is much larger than the average sensor separation.
To distinguish such different conditions, we define the {\em
signal-to-noise ratio on the motion} as:

where .
We then redefine the asymptotic MSE as the limit of the average MSE
for , with constant  {\em and} constant
.
In this case,

where . Notice that  and
.
Also,  we have:


Using the new definition and~(\ref{eq:MSE_C_MF}), the asymptotic expression of the MSE
becomes:

Here we used the following facts:
\begin{itemize}
\item  since  is Hermitian and  is real and diagonal;
\item  for any square matrix  and ;
\item  for any positive integer  and .
This assumption holds only if  and  are asymptotically free~\cite{TulinoVerdu}.
Since asymptotical freeness is in general very hard to prove, we will simply verify the validity
of such assumption through numerical results.
\item  and  (see~\cite{NordioChiasseriniViterbo});
\end{itemize}
Equation~(\ref{eq:MSEinf_C_MF}) reduces to~(\ref{eq:MSEinf_A_MF2}) for ,
while it reduces to  for .




\subsection{ZF filter}

In this case the sink node employs the ZF filter in (\ref{eq:A_B_ZF}) but,
knowing only the average value of the sensor positions, the filter
results to be a function of : ,
and the matrix  can be written as:

We observe that, when  (no sensor motion), we have  and
, thus~(\ref{eq:MSE_C_ZF}) reduces to~(\ref{eq:MSE_A_ZF}).

Using (\ref{eq:limit_Mp}) and (\ref{eq:Psi-gamma}),  the asymptotic MSE is:

Equation~(\ref{eq:MSEinf_C_ZF}) reduces to~(\ref{eq:MSEinf_A_ZF}) for ,
while it reduces to  for .


\subsection{LMMSE filter neglecting   \label{subsec:neglecting-sigma_d}}

If the sink employs the filter in~(\ref{eq:A_B_LMMSE}) computed
using  (i.e., , where ), then the matrix 
in (\ref{eq:MSE_C2}) becomes:

For  (i.e.,  and ),
(\ref{eq:MSE_C_LMMSE}) reduces to~(\ref{eq:MSE_A_LMMSE}).

Using the properties described in Section~\ref{sec:MF_B} the asymptotic MSE is:

Equation~(\ref{eq:MSEinf_C_LMMSE}) reduces to~(\ref{eq:MSEinf_A_LMMSE}) for ,
while it becomes:   for .

\subsection{LMMSE filter for known  \label{subsec:known-sigma_d}}
We now consider the linear LMMSE filter optimized for the case where  is
known at the sink. We find the optimal 
minimizing ; that is, we null the derivative
of~(\ref{eq:MSE_B}) with respect to . We employ the following properties that hold
for any square matrix  \cite{MatrixBook}:

Then, we have:

Solving for , we obtain the expression of the LMMSE filter

Substituting~(\ref{eq:C_B_LMMSE_2}) into~(\ref{eq:MSE_C2}), we have:

In this case an explicit expression of  is hard to obtain.
However, we were able to find the following {\em lower bound} that
turns out to be very tight, as shown by the results presented in
the following section

where to derive the last expression we exploited (\ref{eq:limit_Mp}), (\ref{eq:Psi-gamma}),
(\ref{eq:g(X)}) and the fact that .


\insertfig{1.00}{MSE_model_B_0}{Performance of the ZF filter for  and , 
when  is neglected}{fig:MSE_model_B_0}
\insertfig{1.00}{MSE_model_B_1}{Performance of the LMMSE filter~(\ref{eq:A_B_LMMSE}) versus SNR, 
for  and , when  is neglected}{fig:MSE_model_B_1}
\insertfig{1.00}{MSE_model_B_2}{Performance of the LMMSE filter~(\ref{eq:C_B_LMMSE_2}) with perfect
knowledge of  versus SNR, for  and }{fig:MSE_model_B_2}
\insertfig{1.00}{MSE_model_B_3}{Performance comparison of the LMMSE filter neglecting
~(\ref{eq:A_B_LMMSE})
against the LMMSE filter with perfect knowledge of , as SNR varies and
for  and }{fig:MSE_model_B_3}
\insertfig{1.00}{MSE_model_B_4}{Performance of the LMMSE filter when 
is neglected as SNR varies, for SNR~dB, and  and }{fig:MSE_model_B_4}

\subsection{Results \label{subsec:C_numerical_results}}

We now show the performance of the filters analyzed under Model B.
Regarding the ZF filter (\ref{eq:A_B_ZF}),
Figure~\ref{fig:MSE_model_B_0} compares the asymptotic MSE evaluated
through~(\ref{eq:MSEinf_C_ZF}) (represented by solid lines and
labeled by ``'') against the average MSE (represented by
points and labeled by ``''). The  is obtained by
generating  realizations of the measures as
in~(\ref{eq:p_fixed}), with , computing the estimates as
in~(\ref{eq:hat_a}) and averaging the square error
. The MSE is shown in the log scale plotted
versus , for  and different values of .

Similarly, Figure~\ref{fig:MSE_model_B_1} presents the performance of the
LMMSE filter~(\ref{eq:A_B_LMMSE}). Here the curves labeled by ``'', generated through evaluation of~(\ref{eq:MSEinf_C_LMMSE}),
and the points in the plot, labeled by ``'', have been obtained as for Figure~\ref{fig:MSE_model_B_0}.

In both the plots the solid line labeled by ``SNR'' refers to the case
where , i.e. , and correspond to the performance provided by Model A under the same conditions.

 The excellent match between the asymptotic results and the
numerical simulation confirms the validity of the asymptotic analysis as an effective tool
to characterize the performance of the reconstruction techniques.

Also, comparing Figures~\ref{fig:MSE_model_B_0} and \ref{fig:MSE_model_B_1}, we observe that
the performances of the ZF and the LMMSE filters are similar for SNR~dB for
any value of , while, for lower SNR,
the LMMSE filter outperforms the ZF filter.

Figure~\ref{fig:MSE_model_B_2} compares the performance of the LMMSE filter~(\ref{eq:C_B_LMMSE_2}), which
has knowledge of , with its lower bound~(\ref{eq:MSEinf_C_LMMSE-D}) (dashed lines), as SNR varies.
We consider  and different values of . Notice that the lower bound is
very tight, especially for high values of .
The points in the plot, labeled by ``'' have been obtained as for Figure~\ref{fig:MSE_model_B_0}, using .
Here, as well as in Figure~\ref{fig:MSE_model_B_1}, the the case  (solid line) is shown, and
corresponds to the performance of the LMMSE filter for signal model A. Indeed, for  (i.e.,  and ), we have  and , and~(\ref{eq:MSE_C_LMMSE2}) simplifies to~(\ref{eq:MSE_A_LMMSE}).

Figure~\ref{fig:MSE_model_B_3} compares the performance of the LMMSE filter (\ref{eq:A_B_LMMSE}),
labeled by ``LMMSE'' (solid lines), and of
the LMMSE filter~(\ref{eq:C_B_LMMSE_2}), labeled by ``LMMSE '' (dashed lines),
for the same parameter setting as in Figure~\ref{fig:MSE_model_B_2}.
For the considered value of  (=0.2), the filter in (\ref{eq:C_B_LMMSE_2})
outperforms the simpler filter~(\ref{eq:A_B_LMMSE}) for any value of SNR and SNR,
but the performance gain is always negligible.

Figure~\ref{fig:MSE_model_B_4} shows the performance of the
LMMSE filter~(\ref{eq:A_B_LMMSE}) neglecting ,
obtained through evaluation of~(\ref{eq:MSEinf_C_LMMSE}) for
SNR\,dB (dashed lines) and SNR\,dB (solid lines), and
for .
While the  of the LMMSE filter~(\ref{eq:A_B_LMMSE}) always tends to 1 for
small values of  (i.e., large values of ),
for high  (i.e., low ) its behavior depends on .
Indeed the term  on the right hand side
of~(\ref{eq:MSEinf_C_LMMSE}) reduces to  for .
As explained in Section~\ref{sec:A_ZF},  diverges for
 and so the MSE (see the lines with  markers in the plot).
This behavior is more evident as  increases and the MSE is large,
for any SNR. These results, however, are of no interest from the application point
of view since a system characterized by such poor performance is not working.

\insertfig{1.00}{MSE_model_B_5}{Performance comparison of the LMMSE filter neglecting  against
the LMMSE filter with perfect knowledge of , as SNR varies, for SNR~dB, and }
{fig:MSE_model_B_5}

Finally, Figure~\ref{fig:MSE_model_B_5} compares
the performance of the LMMSE filter~(\ref{eq:C_B_LMMSE_2})
labeled by ``LMMSE '' (dashed lines) and the performance of the LMMSE filter~(\ref{eq:A_B_LMMSE}),
labeled by ``LMMSE'' (solid lines), for SNR\,dB and .

In general the filter~(\ref{eq:C_B_LMMSE_2}) performs always better than filter~(\ref{eq:A_B_LMMSE}).
In particular, for  the two filters show very similar performance, while,
when ,
the filter~(\ref{eq:C_B_LMMSE_2}) does not diverge for high SNR.
This is shown in Figure~\ref{fig:MSE_model_B_5},
where, for  and high values of ,
the advantage of exploiting the knowledge of 
becomes evident.

\medskip
\example{2}{{\it Consider  buoys  deployed in water and
equipped with sensors, which provide noisy measures with \,dB.
Buoys are moving but the variance  is unknown to the reconstruction algorithm.
We need to estimate the maximum number of harmonics of the field that the network can sample and reconstruct
with an average MSE lower than .
}

Since   is known to the reconstruction algorithm
while  is not, we employ the LMMSE filter in Sec.~\ref{subsec:neglecting-sigma_d}.
We have: . Looking at
Figure~\ref{fig:MSE_model_B_5}, we notice that, for \,dB, values of  lower than  can be
obtained only for . The maximum number of harmonics is
then .
} 

\medskip
\example{3}{ {\it Consider a  network of sensor with jittered positions
characterized by =0.4 and {\rm SNR}\,dB, and assume that
these values are known to the reconstruction algorithm. We want to
determine which type of sensor devices should be used  in order to
minimize the . In other words, we ask ourselves how accurate
the sensor measurements need to be (clearly, more expensive devices
provide a higher {\rm SNR}). }

Since SNR=10\,dB is known to the reconstruction algorithm, we
can employ the LMMSE filter given in (\ref{eq:C_B_LMMSE_2}). Looking
at Figure~\ref{fig:MSE_model_B_5}, we notice that the performance of
the filter for  shows a horizontal asymptote
corresponding to an average MSE of . Thus, an SNR\,dB is enough to achieve the best performance.
} 

\section{Summary of Results}
\label{sec:summary}

Our main results for the system models A and B are as follows.
\begin{description}
\item[{\bf Model A}] \hspace{0.7cm} (fixed sensors and noisy measures):
\begin{itemize}
\item  for a given , the MSE provided by any of the reconstruction techniques
is lower bounded by  and worsen with increasing
 (i.e., the ratio of the number of harmonics
 to the number of sampling sensors);
the MF in~(\ref{eq:A_B_MF}) is the only filter which
does not require matrix inversion,
however it provides poor performance in all of the considered cases;
\item the ZF filter
provides high quality performance only
for high  (namely, \,dB) and ;
\item the performance of the LMMSE filter, instead,
is good moderate values of  and .
\end{itemize}

\item[{\bf  Model B}] \hspace{0.7cm} (sensors with jittered positions and noisy measures):
\begin{itemize}
\item for a given  the MSE provided by any of the reconstruction techniques
is lower bounded by (\ref{eq:MSEinf_C_LMMSE-D});
\item the performance of all reconstruction techniques worsen with
increasing  and ;
\item the advantage of exploiting the knowledge of 
in the filter design is negligible for low  and low ,
while it is of fundamental importance to obtain a high
quality reconstruction for  and large values of .

\end{itemize}
\end{description}


\section{Conclusions\label{sec:conclusions}}
We addressed the problem of reconstructing band-limited fields from
measurements taken by irregularly deployed sensors, and we studied
the effects of noisy measures and jittered sensors positions on the reconstruction quality.
We analytically derived
the performance of several linear filters in terms of
the MSE of the field estimates. We also studied
the asymptotic MSE, obtained as the number of harmonics and the number of
sensors grow to infinity while their ratio  is kept constant.
We found that the asymptotic analysis is an effective tool to characterize the
performance of the reconstruction techniques
even for a small number of sensors, and we investigated the impact
that the parameter  has on the system performance.
In~\cite{NordioChiasseriniViterbo} we observed that random sampling without any type of noise
would require more than twice the sampling rate () of minimum regular sampling ()
to get a reliable reconstruction (without ill conditioning problems) with high probability.
The number of sensors further increases () compared to regular sampling when measurement
noise (model A) and sensors position jitter (model B) are present.

\begin{thebibliography}{99}
\bibitem{Akyildiz02} I.\,F. Akyildiz, W. Su, Y. Sankarasubramaniam, and E. Cayirci,
``Wireless sensor networks: A survey,'' {\em Computer Networks,} vol.\,38, no.\,4,
pp.\,393--422, 2002.
\bibitem{Hightower01}
J. Hightower, and G. Borriello,
``Location systems for ubiquitous computing,''
{\em IEEE Computer}, vol.\,34, no.\,8, pp.\,57--66,
Aug.\,2001.
\bibitem{Hu04}
L. Hu, and D. Evans,
``Localization for mobile sensor networks,''
{\em
ACM MobiCom 2004}, Philadelphia, PA, Sept.-Oct.\,2004.
\bibitem{Moore04}
D.~Moore, J. Leonard, D. Rus, and S. Teller,
``Robust distributed network localization with noisy range measurements,''
{\em 2nd ACM Conference on Embedded Networked Sensor Systems
(SenSys '04)}, Baltimore, MD, pp.\,50-61, Nov.\,2004.
\bibitem{Ganesan03} D. Ganesan, S. Ratnasamy, H. Wang, and D. Estrin,
``Coping with irregular spatio-temporal sampling in sensor networks,''
{\em 2nd Workshop on Hot Topics in Networks (HotNets-II)}, Cambridge,
MA, Nov.\,2003.

\bibitem{Feichtinger95}
H.\,G. Feichtinger, K. Gr\"ochenig, and T. Strohmer,
``Efficient numerical methods in non-uniform sampling theory,''
{\em Numerische Mathematik}, vol.\ 69, pp.\ 423--440, 1995.
\bibitem{Grochenig99} K. Gr\"{o}chenig,
 ``Irregular sampling, Toeplitz matrices, and the approximation of entire
functions of exponential type,'' {\em Mathematics of Computation,}
vol.\,68, no.\,226, pp.\,749--765, Apr.\,1999.


\bibitem{AOSN} The  Autonomous Ocean Sampling Network (AOSN) project,
\url{http://www.mbari.org/aosn/}

\bibitem{Bokser04} V. Bokser, C. Oberg, G.\,S. Sukhatme, and A.\,A.
Requicha, ``A small submarine robot for experiments in underwater sensor
networks,'' {\em IFAC - International Federation of Automatic Control
Symposium on Intelligent Autonomous Vehicles,} 2004.

\bibitem{Cayirci06} E. Cayirci, H. Tezcan, Y. Dogan, and V. Coskun,
``Wireless sensor networks for underwater surveillance systems,''
{\em Ad Hoc Networks,} vol.\,4, no.\,4, pp.\,431--446, July 2006.

\bibitem{Majumdar06}
S.\,J. Majumdar, S.\,D. Aberson, C.\,H. Bishop, R. Buizza, M.\,S. Peng, and C.\,A. Reynolds,
``A comparison of adaptive observing guidance for Atlantic tropical cyclones,'' {\em
27th Conference on Hurricanes and Tropical Meteorology}, Apr.\,2006.


\bibitem{Vuran04}
M.C. Vuran, \"O.B. Akan, and I.F. Akyildiz
``Spatio-temporal correlation: theory and applications for wireless sensor networks,''
{\em Computer Networks,} vol.\ 45, no.\ 3, pp.\ 245-259, June 2004.
\bibitem{Ergen06} S.\,C. Ergen, P. Varaiya, ``Effects of A-D conversion nonidealities on
distributed sampling in dense sensor networks,''
{\em 5th International Symposium on Information Processing in Sensor Networks (IPSN '06),}
Nashville, Tennessee, Apr.\,2006.
\bibitem{Perillo04} M. Perillo, Z. Ignjatovic, and W. Heinzelman, ``An
energy conservation method for wireless sensor networks employing a blue
noise spatial sampling technique,'' {\em 3rd International Symposium on
Information Processing in Sensor Networks (IPSN '04)}, Apr. 2004.
\bibitem{Willett04} R. Willett, A. Martin, and R. Nowak, ``Backcasting:
adaptive sampling for sensor networks,'' {\em 3rd International
Symposium on Information Processing in Sensor Networks (IPSN '04)},
Apr. 2004.
\bibitem{Maleki05} M. Maleki, and M. Pedram,
``QoM and lifetime-constrained random deployment of sensor networks
for minimum energy consumption,''
{\em 4th International Symposium on Information Processing in Sensor Networks (IPSN '05),}
Los Angeles, CA, Apr.\,2005.
\bibitem{Kumar03} P. Ishwar, A. Kumar, and K. Ramchandran, ``Distributed
sampling for dense sensor networks: a bit-conservation principle,''
{\em 3rd International Symposium on Information Processing in Sensor
Networks (IPSN '03)}, Apr. 2003.

\bibitem{Dong04} Z.\,Yang, M.\,Dong, L.\,Tong, and B.\,M. Sadler,
``MAC Protocols for optimal information retrieval pattern in sensor networks with mobile access,''
{\em EURASIP Journal on Wireless Communications and Networking},
vol.\,5, no.\,4, pp.\,493--504, September 2005.
\bibitem{CristescuVetterli} R. Cristescu, and M. Vetterli,
``On the optimal density for real-time data gathering of spatio-temporal
processes in sensor networks,''
{\em 4th International Symposium on Information Processing in Sensor Networks (IPSN '05),}
Los Angeles, CA, Apr.\,2005.
\bibitem{Poor} Y. Sung, L. Tong, and H.V. Poor,
``Sensor activation and scheduling for field detection in large sensor arrays'',
{\em 4th International Symposium on Information Processing in Sensor Networks (IPSN '05),}
Los Angeles, CA, Apr.\,2005.
\bibitem{Rachlin1} Y. Rachlin, R. Negi, P. Khosla,
``Sensing capacity for discrete sensor network applications,''
{\em 4th International Symposium on Information Processing in
Sensor Networks (IPSN'05),}  Los Angeles, CA, Apr.\,2005.
\bibitem{DongTong}
M.\,Dong, L.\,Tong, and B.M.\,Sadler,
``Impact of Data Retrieval Pattern on Homogeneous Signal Field Reconstruction in Dense Sensor Networks,''
{\em IEEE Transactions on Signal Processing,} vol.\,54, no.\,11, pp. 4352--4364, November 2006.
\bibitem{Marziliano00} P. Marziliano, and M. Vetterli, ``Reconstruction of
irregularly sampled discrete-time bandlimited signals with unknown sampling locations,''
{\em IEEE  Trans. on Signal Processing,} vol.\,48, no.\,12, pp.\,3462--3471, Dec.\,2000.
\bibitem{NordioChiasseriniViterbo}
A.\ Nordio, C-F.\ Chiasserini, and E.\ Viterbo,
``Bandlimited field reconstruction for wireless sensor networks,''
technical report,
\url{http://www.telematica.polito.it/~nordio/sensors/TIT_sub.pdf}.
\bibitem{Verdu_book} S.\,Verd\`u, {\em Multiuser detection,}
Cambridge Univ.\ Press, Cambridge, UK, 1998.
\bibitem{TulinoVerdu} A.\ Tulino, and S.\ Verd\'u, ``Random matrix theory and
wireless communications,'' {\em Foundations and Trends in Communications and
Information Theory}, vol.\,1, no.\,1, 2004.

\bibitem{BigTarTul} E.\,Biglieri, G.\,Taricco, and A.\,Tulino, ``Performance of
space--time codes for a large number of antennas,'' {\em IEEE Trans.\ Inform.\
Theory}, vol.\,48, no.\,7, pp.\,1794--1803, July 2002.
\bibitem{LiuStanley}
B.\,Liu and T.\,P.\,Stanley,
``Error Bounds for Jittered Sampling,''
{\it IEEE Trans. on Automatic Control,} vol.\,10, no.\,4, pp.\,449--454,\,October 1965.


\bibitem{MatrixBook} M. Brookes, {\em The matrix reference manual},
Imperial College, London, UK, 1998.

\bibitem{Maravic}
I.\,Maravic and M.\,Vetterli, ``Sampling and reconstruction of signals
with finite rate of innovation in the presence of noise,'' {\em IEEE
Transactions on Signal Processing,} vol.\,53, no. \,8, pp.\,2788-2805,
August 2005.

\bibitem{Stoica}
P.\,Stoica and R.\,Moses, {\em Introduction to Spectral Analysis,}
Upper Saddle River, NJ, Prentice-Hall, 2000.


\end{thebibliography}



\appendices

\section{Proof of~(\ref{eq:g(X)})}
\label{app:proof3}
Let  be an Hermitian matrix where  is an arbitrary random matrix.
Let  be an analytic function, defined for  that can be written as:
, with finite coefficients .
Considering that  is the -th moment of the asymptotic eigenvalue
distribution of , i.e., 
where  is the random variable distributed as the asymptotic eigenvalues of ,
and the continuity of the function , we have:



\section{Proof of Lemma~\ref{lemma:1}}
\label{app:proof}
Using~(\ref{eq:theorem_equation}) the -th entry of  is

which matches its definition.


\section{Computation of  and
 as functions of }
\label{app:averages}

We derive here the expressions of  and
 as functions of .
\paragraph{Computation of }
Using Lemma~\ref{lemma:1} we have

The average of  is given by:
,
where  is the  identity matrix and 
is the -th moment of . Hence,

where\footnote{Let  be a diagonal  matrix.
The exponential of , denoted by , is the diagonal matrix whose elements
are .}

is a  diagonal matrix and

is the {\em characteristic function} of the random variable , ,
sampled in .
In particular when  is a zero mean Gaussian random variable with
variance , we have:

and , .

\paragraph{Computation of }

where .
Now,

therefore

and


The first term of (\ref{eq:EFSF}) yields

The -th element of  is given by

which does not depend on . Thus,
,
and

Finally,

where

Therefore,

Concluding,

and, if  is real,






\end{document}
