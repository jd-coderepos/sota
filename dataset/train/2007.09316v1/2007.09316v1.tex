\section{Experiments}

\subsection{Datasets}
We evaluate our method on two public domain generalization benchmark datasets: \textbf{VLCS} and \textbf{PACS}. \textbf{VLCS}~\cite{fang2013unbiased} is a classic domain generalization benchmark for image classification, which includes five object categories from four domains (PASCAL VOC 2007, LabelMe, Caltech, and Sun datasets).
\textbf{PACS}~\cite{li2017deeper} is a recent domain generalization benchmark for object recognition with larger domain discrepancy. It consists of seven object categories from four domains (Photo, Art Paintings, Cartoon, and Sketches datasets) and the domain discrepancy among different datasets is more severe than VLCS, making it more challenging. 



\subsection{Network Architecture and Implementation Details}
Our framework is flexible and one can use different network backbones as the feature Encoder. 
We utilized a fully-connected layer with -dimensional output as the self-supervised auxiliary classification layer following the setting in \cite{carlucci2019domain} for a fair comparison. 
To enable the momentum metric learning, we further employed a fully-connected layer with  output channels following the Encoder part and added an L2 normalization layer to normalize the feature representation  of each sample.
The MuEncoder has the same network architecture as the Encoder, and the weight of MuEncoder was initialized with the same weight as Encoder.
We followed the previous works in the literature~\cite{balaji2018metareg,carlucci2019domain,li2018learning,dou2019domain} and employed the leave-one-domain-out cross-validation strategy to produce the experiment results, i.e., we take turns to choose each domain for testing, and train a network model with the remaining three domains.


We implemented our framework with the PyTorch library on one NVIDIA TITAN Xp GPU.
Our framework was optimized with the SGD optimizer. We totally trained 100 epochs, and the batch size was 128. 
The learning rate was set as 0.001 and decreased to 0.0001 after 80 epochs.
We empirically set the margin of the triplet loss as .
We also adopted the same on-the-fly data augmentation as JiGen~\cite{carlucci2019domain}, which includes random cropping, horizontal flipping, and jitter.


\if 0
We design proposed framework making it able to leverage over many possible backbones of convolutional deep architectures.
Indeed, one can supplement the existing classification layer with one new self-supervision auxiliary classification layer and one embedding extraction layer after the encoder backbone to realize intrinsic and extrinsic supervision, respectively.
The self-supervision auxiliary classification layer is a fully-connected layer following Encoder with -dimensional output.
While the embedding extraction layer consists of a fully-connected layer with  output neurons followed by a L2 normalization layer.
The MuEncoder has the same architecture as encoder with embedding extraction output and is initialized with the same weight as Encoder.
The model weights are initialized by pre-trained weights on IMAGENET \cite{deng2009imagenet}.
We deploy our method on one NVIDIA TITAN Xp 12 GB GPU.
Our framework is trained with SGD solver, 100 epochs, batch size 128.
The learning rate  of classification loss and auxiliary loss is set as 0.001, while the learning rate of triplet loss  is set as 0.003.
For the margin in triplet loss with -hard negative selector, we set it as  experimentally.
The  will decrease to 0.0001 after 80\% training epochs.
The data augmentation protocol is following JiGen \cite{carlucci2019domain}, with random cropping, horizontal flipping, jitter.
We dedicate the hyper-parameter selection and comparison in the following ablation study subsection.
\fi 


\if 0
\subsection{Compared Methods}
We compare our proposed method with the following ten previous methods.
\begin{itemize}
	\item \textbf{D-MTAE} \cite{ghifary2015domain} utilizes multi-task auto-encoders to learn robust features across domains.
	\item \textbf{CIDDG} \cite{li2018deep} is a conditional invariant adversarial network that can learn domain-invariant representations under distribution constraints.
	\item \textbf{CCSA} \cite{motiian2017unified} exploits
	a siamese network to learn a discriminative embedding subspace with distribution distances and similarities.
	\item \textbf{DBADG} \cite{li2017deeper} develops a low-rank parametrized CNN model for domain generalization and also presents a new benchmark dataset PACS.
	\item \textbf{MMD-AAE} \cite{li2018domain} aligns the distribution through an adversarial auto-encoder by Maximum Mean
	Discrepancy.
	\item \textbf{MLDG} \cite{li2018learning} is a meta-learning method by simulating train/test domain shift during training.
	\item \textbf{Epi-FCR} \cite{li2019episodic} is an episodic training method.
	\item \textbf{MetaReg} \cite{balaji2018metareg} is a regularization function applied in a meta-learning framework.
	\item \textbf{JiGen} \cite{carlucci2019domain} is a jigsaw puzzle solving method based on self-supervision. 
	\item \textbf{MASF} \cite{dou2019domain} is the latest a meta-learning based method with two complementary losses.
\end{itemize}
\fi 

\subsection{Results on VLCS Dataset}
We followed the same experiment setting in previous work~\cite{carlucci2019domain} to train and evaluate our method.
The extrinsic metric learning and intrinsic self-supervised learning was developed upon the ``FC7" features of AlexNet~\cite{NIPS2012_4824} pretrained on ImageNet~\cite{deng2009imagenet}.
We set the size of the memory bank as  and the number of negatives  in the triplet loss Eq.~\eqref{eq:triplet} as .
The hyper-parameters , , and  in total objective function Eq.~\eqref{eq:total} were set as , , and , respectively.
For our results, we report the average performance and standard deviation over three independent runs.


We compare our method with other nine previous state-of-the-art methods.
\textbf{D-MTAE} \cite{ghifary2015domain} utilized the multi-task auto-encoders to learn robust features across domains.
\textbf{CIDDG} \cite{li2018deep} was a conditional invariant adversarial network that learns the domain-invariant representations under distribution constraints.
\textbf{CCSA} \cite{motiian2017unified} exploited a Siamese network to learn a discriminative embedding subspace with distribution distances and similarities.
\textbf{DBADG} \cite{li2017deeper} developed a low-rank parametrized CNN model for domain generalization.
\textbf{MMD-AAE} \cite{li2018domain} aligned the distribution through an adversarial auto-encoder by Maximum Mean Discrepancy.
\textbf{MLDG} \cite{li2018learning} was a meta-learning method by simulating train/test domain shift during training.
\textbf{Epi-FCR} \cite{li2019episodic} was an episodic training method.
\textbf{JiGen} \cite{carlucci2019domain} solved a jigsaw puzzle auxiliary task based on self-supervision. 
\textbf{MASF} \cite{dou2019domain} employed a meta-learning based strategy with two complementary losses for encoder regularization.
Moreover, we include the \textbf{Within domain} performance of all the datasets as a comparison to reveal the performance drop due to domain discrepancy.
We trained \textbf{Within domain} using a supervised way with training and test images from the same domain.


The comparison results with the above methods are shown in Table~\ref{tab:results-VLCS}. 
It is observed that our EISNet achieves the best performance on both Caltech and Sun datasets and comparable results on PASCAL VOC and LabelMe datasets.
Overall, EISNet achieves an average accuracy of  over four domains, outperforming the previous state-of-the-art method \textbf{MASF}~\cite{dou2019domain}.
Our method also outperforms \textbf{JiGen}~\cite{carlucci2019domain} on three domains and achieves comparable results on the remaining PASCAL VOC domain, demonstrating that utilizing extrinsic relationship supervision can further improve the network generalization ability. 




\begin{table*} [!t]
	\centering
	\caption{Domain generalization results on \textbf{VLCS} dataset with object recognition accuracy (\%) using \textbf{AlexNet} backbone. 
		The top results are highlighted in \textbf{bold}.}
	\label{tab:results-VLCS}
	\resizebox{1.0\textwidth}{!}{
		\setlength\tabcolsep{1.5pt}
		\begin{tabular}{l|cccccccccc|c}
			\toprule[1pt]
			\textbf{Target} & \tabincell{c}{\textbf{Within }\\\textbf{domain}} & \tabincell{c}{\textbf{D-MTAE} \\ \cite{ghifary2015domain}} & \tabincell{c}{\textbf{CIDDG}\\ \cite{li2018deep}} & \tabincell{c}{\textbf{CCSA}\\ \cite{motiian2017unified}} & \tabincell{c}{\textbf{DBADG}\\ \cite{li2017deeper}}& 
			\tabincell{c}{\textbf{MMD-}\\ \textbf{AAE} \cite{li2018domain} }&  \tabincell{c}{\textbf{MLDG} \\ \cite{li2018learning}} & \tabincell{c}{\textbf{Epi-}\\ \textbf{FCR}  \cite{li2019episodic}} & \tabincell{c}{\textbf{JiGen}\\ \cite{carlucci2019domain}} &  \tabincell{c}{\textbf{MASF}\\ \cite{dou2019domain}} &  \tabincell{c}{\textbf{EISNet} \\ {(Ours)}} \\ \hline
			\T
			PASCAL & 82.07 & 63.90 & 64.38 & 67.10 & 69.99 & 67.70 & 67.7 & 67.1 &\textbf{70.62} & 69.14 & 69.830.48  \\ 
			
			\textbf{L}abelMe & 74.32 & 60.13 & 63.06 & 62.10 & 63.49 & 62.60 & 61.3 & 64.3 &60.90 & \textbf{64.90 }& 63.490.82   \\ 
			\textbf{C}altech & 100.0 & 89.05& 88.83 & 92.30 & 93.63 & 94.40 & 94.4 & 94.1 &96.93 & 94.78 &\textbf{97.33}0.36  \\     
			\textbf{S}un & 77.33 & 61.33 & 62.10 & 59.10 & 61.32 & 64.40 & 65.9 & 65.9 &64.30 & 67.64 &\textbf{68.02}0.81  \\  \hline \T
			\textbf{Average} & 83.43 & 68.60& 69.59 & 70.15 & 72.11 & 72.28 & 72.3 & 72.9 &73.19 & 74.11 &\textbf{74.67}  \\ 
			\toprule[1pt]
		\end{tabular}
	}
\end{table*}

\begin{table*} [!t]
	\centering
	\caption{Domain generalization results on \textbf{PACS} dataset with object recognition accuracy (\%) using \textbf{AlexNet} backbone. 
		The top results are highlighted in \textbf{bold}.
	}
	\label{tab:results-PACS-alex}
	\resizebox{1.0\textwidth}{!}{
		\setlength\tabcolsep{1.5pt}
		\begin{tabular}{l|ccccccccc|c}
			\toprule[1pt]
			\textbf{Target} & \tabincell{c}{\textbf{Within }\\\textbf{domain}} & \tabincell{c}{\textbf{D-MTAE} \\ \cite{ghifary2015domain}} & \tabincell{c}{\textbf{CIDDG}\\ \cite{li2018deep}}  & \tabincell{c}{\textbf{DBADG}\\ \cite{li2017deeper}}&  \tabincell{c}{\textbf{MLDG} \\ \cite{li2018learning}} & \tabincell{c}{\textbf{Epi-}\\ \textbf{FCR} 
				\cite{li2019episodic}} &
			\tabincell{c}{\textbf{MetaReg} \\\cite{balaji2018metareg}} & \tabincell{c}{\textbf{JiGen}\\ \cite{carlucci2019domain}} &  \tabincell{c}{\textbf{MASF}\\ \cite{dou2019domain}} & \tabincell{c}{\textbf{EISNet} \\ {(Ours)}}  \\ \hline
			\T
			\textbf{P}hoto & 97.80 & 91.12 & 78.65     & 89.50 & 88.00 & 86.1 & 91.07 & 89.00 &90.68  & \textbf{91.20}0.00  \\ 
			
			\textbf{A}rt painting & 90.36 & 60.27& 62.70 & 62.86 & 66.23 & 64.7 & 69.82 & 67.63 &70.35  & \textbf{70.38}0.37   \\ 
			
			\textbf{C}artoon & 93.31 & 58.65& 69.73 & 66.97 & 66.88 & 72.3 & 70.35 & 71.71 &\textbf{72.46} & 71.591.32  \\     
			
			\textbf{S}ketch & 93.88 & 47.68& 64.45 & 57.51 & 58.96 & 65.0 & 59.26 & 65.18&67.33 & \textbf{70.25}1.36   \\  \hline \T
			
			\textbf{Average} & 93.84 & 64.48& 68.88 & 69.21 & 70.01 & 72.0 & 72.62 &  73.38 & 75.21 &\textbf{75.86}    \\ 
			\toprule[1pt]
		\end{tabular}
	}
\end{table*}

\begin{table*} [!t]
	\centering
	\caption{Domain generalization results on \textbf{PACS} dataset with object recognition accuracy (\%) using \textbf{ResNet} backbones. 
		The top results are highlighted in \textbf{bold}.}
	\label{tab:results-PACS-resnet}
	\resizebox{1.0\textwidth}{!}{
		\setlength\tabcolsep{1.5pt}
		\begin{tabular}{l|ccc|ccc}
			\toprule[1pt]
			\multirow{2}{*}{\textbf{Target}} & \multicolumn{3}{c|}{\textbf{ResNet-18}} &
			\multicolumn{3}{c}{\textbf{ResNet-50}}\\
			\cline{2-7}  & \textbf{DeepAll} & \textbf{MASF} \cite{dou2019domain} & \tabincell{c}{\textbf{EISNet} {(Ours)}}  & \textbf{DeepAll} & \textbf{MASF} \cite{dou2019domain} & \tabincell{c}{\textbf{EISNet} {(Ours)}}  \T \B \\
			\hline
			\T
			\textbf{P}hoto & 94.25 & 94.99     &\textbf{95.93}0.06  & 94.83 & 95.01  & \textbf{97.11}0.40  \\ 
			
			\textbf{A}rt painting & 77.38& 80.29  &\textbf{81.89}0.88 & 81.47 &82.89  & \textbf{86.64}1.41   \\ 
			
			\textbf{C}artoon & 75.65& \textbf{77.17} & {76.44}0.31 & 78.61&80.49 & \textbf{81.53}0.64  \\     
			
			\textbf{S}ketch & 69.64&  71.69 & \textbf{74.33}1.37 & 69.69&72.29 & \textbf{78.07}1.43  \\  \hline \T
			
			\textbf{Average} & 79.23& 81.04& \textbf{82.15} &  81.15& 82.67&\textbf{85.84}   \\ 
			\toprule[1pt]
		\end{tabular}
	}
\end{table*}

\subsection{Results on PACS Dataset}


To show the effectiveness of our framework under different network backbones on PACS dataset, we evaluate our method with three different backbones: AlexNet, ResNet-18, and ResNet-50~\cite{He_2016_CVPR}.
The size of memory bank was set as  and  in the triplet loss Eq.~\eqref{eq:triplet} was set as .
The hyper-parameters in total objective function Eq.~\eqref{eq:total} were set as , , and  for , , and , respectively.
For our results, we also report the average performance and standard deviation over three independent runs.

Table~\ref{tab:results-PACS-alex} summarizes the experimental results developed with AlexNet backbone.
We compare our methods with eight other methods that achieved previous best results on this benchmark dataset.
\textbf{MetaReg} \cite{balaji2018metareg} utilized a novel classifier regularization in the meta-learning framework.
As we can observe from Table~\ref{tab:results-PACS-alex}, by simultaneously utilizing momentum metric learning and intrinsic self-supervision for images across different source domains, our method achieves the best performance on three datasets. 
Across all domains, our method achieves an average accuracy of , setting a new state-of-the-art performance.

We also compare our method with baseline method (DeepAll) and the state-of-the-art method \textbf{MASF}~\cite{dou2019domain} using ResNet-18 and ResNet-50 backbones in  Table~\ref{tab:results-PACS-resnet}. 
In the ResNet-50 experiment, we reduce the batch size to 64 to fit the limited GPU memory.
The DeepAll method is trained with all the source domains without any specific network design.
As shown in Table~\ref{tab:results-PACS-resnet}, our method consistently outperforms \textbf{MASF} about 1.11\% and 3.17\% on average accuracy with ResNet-18 and ResNet-50 backbone, respectively.
This indicates that our designed framework is very general and can be migrated to different network backbones.
Note that the improvement over \textbf{MASF} is more obvious with a deeper network backbone, showing that our proposed algorithm is more beneficial for domain generalization with deeper feature extractors.

\begin{figure}[!t]
	\centering
	\includegraphics[width=0.9\textwidth]{./ablation1.pdf}
	\caption{
		The performance of our method under different number of negative samples  and momentum update coefficient . 
}
	\label{fig:ablation1}
\end{figure}
\begin{table*} [!t]
	\centering
	\caption{Ablation study on key components of our method with the \textbf{PACS} dataset (\%). 
		The top results are highlighted in \textbf{bold}.}
	\label{tab:results-PACS-ablation}
{
		\setlength\tabcolsep{1.5pt}
		\begin{tabular}{cc|cccc|c}
			\toprule[1pt]
			\textbf{Extrinsic} & \textbf{Intrinsic}  & \textbf{P}hoto  & \textbf{A}rt painting &  \textbf{C}artoon &\textbf{S}ketch & \textbf{Average} \\
			\hline
			\T
			-&-&94.85&81.47&78.61&69.69& 81.15\\
			\checkmark &-&97.06&81.97&80.70&76.81&84.14 \\
			- &\checkmark&97.02&85.17&76.35&76.97&83.88 \\
			\checkmark     &\checkmark&\textbf{97.11}&\textbf{86.64}&\textbf{81.53}&\textbf{78.07}&\textbf{85.84} \\
			
			\toprule[1pt]
		\end{tabular}
	}
\end{table*}


\subsection{Analysis of Our Method}
We conduct extensive analysis of our method.  
Firstly, we investigate the effectiveness of extrinsic and intrinsic supervision using ResNet-50 backbone on \textbf{PACS} dataset, and the experimental results are illustrated in Table~\ref{tab:results-PACS-ablation}.
The \textbf{Extrinsic} supervision indicates that the momentum metric learning is used, while \textbf{Intrinsic} supervision denotes that the auxiliary self-supervision loss is optimized. 
The method without these two supervisions is the baseline model, which is the same with DeepAll results in Table~\ref{tab:results-PACS-resnet}.
From the results in Table~\ref{tab:results-PACS-ablation}, we observe that each supervision plays an important role in our framework.
Specifically, equipping the extrinsic supervision into the baseline model yields about 2.99\% average accuracy improvement.
Meanwhile, we also achieve 2.73\% average accuracy improvement over the baseline model by incorporating intrinsic self-supervision of the images.
By combing extrinsic and intrinsic supervision, performance is further improved across all settings, indicating these two supervisions are complementary. 





\begin{table*} [!t]
	\centering
	\caption{Comparison of our proposed -hard negative selector with original random selector and semi-hard negative selector.}
	\label{tab:results-ablation-selector}
{
		\setlength\tabcolsep{1.5pt}
		\begin{tabular}{c|ccc}
			\toprule[1pt]
			\T
			Selector & Random & Semi-hard & -hard \\ \hline \T
			Accuracy (\%) &65.38 &68.08&70.25   \\
			\toprule[1pt]
		\end{tabular}
	}
\end{table*}
\begin{table*} [!t]
	\centering
	\caption{Comparison among different memory bank size.}
	\label{tab:results-ablation-bank}
{
		\setlength\tabcolsep{1.5pt}
		\begin{tabular}{c|cccc}
			\toprule[1pt]
			\T
			Memory bank size  & 1024 & 512 & 256 & 128 \\ \hline
			No. of negatives  & 256 & 128 & 64 & 32\\ \hline
			\T 
			Accuracy (\%) &70.25 &68.80&68.24 & 67.93   \\
			\toprule[1pt]
		\end{tabular}
	}
\end{table*}


We then analyze five key components in our framework, that is a) the number of different negative samples  in momentum metric learning, b) the effectiveness of momentum update coefficient , c) the effectiveness of hard negative selector, d) the size of memory bank , and e) time cost.
All below comparison experiments are implemented with AlexNet backbone on the PACS benchmark.


\renewcommand{\theenumi}{\alph{enumi}}
\begin{enumerate}
	\item The number of negative samples  is a key parameter of our designed -hard negative selector in momentum metric learning. 
	We investigate the network performance under different options.
	We select six  values at different magnitudes, which are 1, 8, 64, 128, 256, and 512.
	The Sketch dataset results are shown in Fig.~\ref{fig:ablation1} (a), 
	We can observe that a large number of negative samples would lead to better results in general and the network generates the best result with . 
However, the performance drops drastically if we set , demonstrating that too large  will produce a burden on the metric distance calculation and make the network difficult to learn.


	\item The momentum update coefficient  is important to control the feature consistency among different batches of embedded features in the memory bank. 
	We show the accuracy with different momentum coefficient  in Fig.~\ref{fig:ablation1} (b).
	It is observed that the network performs well when  is relatively large, \ie, 0.999.
A small coefficient would degrade the network performance, suggesting that a slow updating MuEncoder is beneficial to the feature consistency.


\item To validate the effectiveness of -hard negative selector in our proposed metric learning, we compare our proposed -hard negative selector with original random triplet selector and semi-hard negative selector.
The Sketch dataset results are shown in Table~\ref{tab:results-ablation-selector}.
	Equipped with semi-hard negative selector, the accuracy improves .
	By selecting more negative pairs from the memory bank, we obtain the accuracy of , demonstrating the effectiveness of the proposed -hard negative selector.
	
	\item The size of memory bank  can be adjusted according to different tasks. Here, we show the results of four different settings with the number of negatives changing as well in Table~\ref{tab:results-ablation-bank}. 
In general, our method is able to generate better results with a large memory bank size and negative samples.
However, a too large memory bank will increase the burden to calculate the pair-wise distance in triplet loss.
	Therefore, we need to balance the accuracy and computation burden.
	\item Apart from the performance improvement over other methods, our method has much lower computation cost. Under the same server setting (one TITAN XP GPU) and AlexNet backbone, our method only takes 1.5 hours to train the network on PACS dataset, while the total training time of the state-of-the-art MASF is about 17 hours. Therefore, our method could save more than  time cost on training phase.
\end{enumerate}





\begin{figure}[!t]
	\centering
	\includegraphics[width=0.9\textwidth]{./tsne-results.pdf}
	\caption{
		t-SNE visualization on one target domain to show the discrimination of the network. (a) is the feature embedding extracted from the IMAGENET pre-trained network. (b) shows the feature embedding distributions extracted from our EISNet.
	}
	\label{fig:tsne}
\end{figure}

We also employ t-SNE~\cite{maaten2008visualizing} to analyze the feature level discrimination of our method and the visualization results are shown in Fig.~\ref{fig:tsne}.
Compared with the feature extracted from the ImageNet pre-trained network, the distance between different class clusters in our method becomes evident, indicating that equipped with our proposed extrinsic and intrinsic supervision, the model is able to learn more discriminative features among different object categories regardless domains.
