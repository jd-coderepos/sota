\documentclass{article}
\usepackage{spconf,amsmath,graphicx}
\usepackage{url}
\usepackage{subfigure}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{bbding}
\usepackage{array,multirow}
\def\x{{\mathbf x}}
\def\L{{\cal L}}

\title{Half Wavelet Attention on M-Net+ for Low-Light Image Enhancement}
\name{Chi-Mao Fan \qquad Tsung-Jung Liu \qquad Kuan-Hsien Liu}


\address{
\ninept Department of Electrical Engineering and Graduate Institute of Communication Engineering, National Chung Hsing University, Taiwan
\\
\ninept Department of Computer Science and Information Engineering, National Taichung University of Science and Technology, Taiwan
}
\begin{document}

\maketitle


\begin{abstract}
Low-Light Image Enhancement is a computer vision task which intensifies the dark images to appropriate brightness. It can also be seen as an ill-posed problem in image restoration domain. With the success of deep neural networks, the convolutional neural networks surpass the traditional algorithm-based methods and become the mainstream in the computer vision area. To advance the performance of enhancement algorithms, we propose an image enhancement network (HWMNet) based on an improved hierarchical model: M-Net+. Specifically, we use a half wavelet attention block on M-Net+ to enrich the features from wavelet domain. Furthermore, our HWMNet has competitive performance results on two image enhancement datasets in terms of quantitative metrics and visual quality. The source code and pretrained model are available at \url{https://github.com/FanChiMao/HWMNet.}
\end{abstract}

\begin{keywords}
Image enhancement, hierarchical M-Net, wavelet domain, attention mechanism
\end{keywords}


\section{Introduction}
Image enhancement is a low-level computer vision task which also plays an important role in the pre-process of high-level vision tasks such as image classification and object detection. Generally, image enhancement can also be considered as one kind of image restoration task, in which a low-quality image  could be represented as: 

where  is a high-quality or noise-free image,  denotes the degradation function and  means the additive noise. The image enhancement methods could be roughly classified to two types: algorithm-based and learning-based. Algorithm-based enhancement methods could also be summarized to three mainstream categories (i.e., histogram equalization-based, retinex-based, and dehazing-based methods). All of above methods are based on image priors which also could be called prior-based or model-based methods. More details are going to be discussed in the section of related work.

Recently, learning-based enhancement methods surpass conventional prior-based methods in terms of inference time and enhancement performance. Especially the appearance of convolution neural network (CNN) almost dominates the whole computer vision area, including image enhancement.

In this paper, we propose a hierarchical image enhancement model based on the existing M-Net architecture called M-Net+ to ameliorate the loss of spatial information caused from the sampling. Moreover, we use the proposed half wavelet attention block (HWAB) in our M-Net+ to gain the feature information in wavelet domain. For the reconstruction process, instead of using concatenation to fuse the feature maps with different resolutions, we adopt the selective kernel feature fusion (SKFF) \cite{01} to efficiently combine the features. Overall, the main contributions of this work can be summarized as follows: 

\begin{itemize}
  \item We propose the improved hierarchical architecture (M-Net+) for the task of low-light image enhancement.
  \item We propose the efficient feature extraction block called half wavelet attention block (HWAB) which can gain more diversified features in another domain.
\item We experiment on two low-light image enhancement datasets to demonstrate that our proposed model achieves state-of-the-art performance in image enhancement in terms of quantity and quality and even with less computational complexity.
\end{itemize}

\section{Related Work}


\begin{figure*}[!t]
\centering
	\includegraphics[width=17.5cm]{./figures/model/HWMNet.png}
	\caption{Proposed Half Wavelet attention M-Net (HWMNet) architecture. The source code and component structure of the model could be found in the provided URL indicated in the abstract. We set the initial channel in each resolution to 96 after  convolutions, and totally we have 4 layers in the proposed M-Net.}
	\label{HWMNet}
\end{figure*}

\subsection{Image enhancement}
As aforementioned, traditional image enhancement approaches are generally based on image priors or algorithms, such as histogram equalization-based \cite{adaptive, contrast}, retinex-based \cite{MSR} and dehazing-based \cite{DCP} methods. The enhancement performances of conventional prior-based methods are acceptable but always suffered from color distortions or produce over-enhanced images. Fortunately, the learning-based methods based on CNNs recently have been successfully applied to low-light image enhancement task \cite{2019ntire}. And they almost surpass the conventional prior-based enhancement methods \cite{ 01, kind, kind++, RUAS, LLFlow} with incredibly good performance.




\subsection{M-Net}
The M-Net architecture is first proposed for medical image segmentation \cite{mnet} which can be regarded as an improved hierarchical model architecture from U-Net \cite{unet}. Adiga \emph{et al.}\cite{fpd} use the same framework for fingerprint image denoising and also obtain good results. Compared with the traditional U-Net architecture, the M-Net adds additional gatepost feature paths in both encoder and decoder which the main objective is to enrich the contextual information in different image resolutions and try to ameliorate the spatial loss caused by sampling.



\section{Proposed Method}


\subsection{Half Wavelet attention M-Net (HWMNet)}
The proposed architecture of HWNNet for low-light image enhancement is shown in Fig. \ref{HWMNet}. We first use  weight sharing convolution in each resolution of low-light input image acquired by doing the bilinear down-sampling from original-resolution input. Each layer has the proposed half wavelet attention block (HWAB) to extract adequate and enriched semantic information. More details about HWAB will be illustrated in Section 3.3. In addition, we use pixel unshuffled \cite{RDN} method as our down-sampling module in the trunk feature path at the end of HWAB in the U-Net. Then, the feature maps are concatenated with previous shallow features from bilinear down-sampling and keep going as normal U-Net process. In this paper, we name this hierarchical network as M-Net+ and it will be discussed in section 3.2. 

We follow the recent enhancement method \cite{01}, and optimize our HWMNet end-to-end with the Charbonnier loss which is a variant L1 loss for low-light image enhancement and described as follows: 

\newcommand{\Lagr}{\mathcal{L}} 

where  denote the low-light images and ground-truth images, respectively.  is the batch size of training data,  is the number of feature channels,  and  are the size of images. The constant  in Eq. (\ref{eq2}) is empirically set to .



\subsection{M-Net+}
After analyzing the conventional M-Net \cite{mnet, fpd} that has been discussed in previous section, we think there are two major problems which make M-Net not a popular hierarchical network backbone in recent image restoration methods. First, for the encoder, they use max-pooling as down-sampling in both gatepost and U-Net feature paths. Since the restoration tasks are the pixel-wise vision tasks, it is inappropriate to use the down-sampling which will cause serious loss of spatial information. Second, for the decoder, because the additional gatepost feature paths are added, the reconstruction process (decoder) has a lot of features to fuse, which will increase the burden to the whole network. 

To focus on these two problems, we proposed the M-Net+ as our enhancement network backbone. The proposed HWMNet has two improvements: 1) More diversity and plentiful multi-scale cascading features. We utilize pixel unshuffle down-sampling in the U-Net path and bilinear down-sampling for the gatepost path, which makes the cascading features have more diversity. Furthermore, the design of the gatepost feature paths in the M-Net can ameliorate the loss of spatial information; 2) Using different feature fusion method SKFF \cite{01} to summarize the information in the decoder (reconstruction process). This improvement aims to solve high-dimensional cascading features in original M-Net, which makes the M-Net have large number of parameters and high computational complexity. 

\subsection{Half Wavelet Attention Block}
Fig.~\ref{HWAB} shows the architecture of the proposed Half Wavelet Attention Block (HWAB) which is modified from the dual-attention unit (DAU) \cite{01, cycleisp}. The DAU uses channel attention \cite{SENet} and spatial attention to extract features, and fuses them together to gain more enriched contextual information.


In the beginning of the HWAB, the input features  are divided into two parts  and  along the input channel, and they both . The main purpose of dividing the input features is to decrease the computation complexity and reserve the context information, where  is to keep the normal domain features which can be referred to HINet \cite{hinet}, while the other part of feature  will do the discrete wavelet transformation (DWT) to obtain the wavelet domain feature . Then,  will be passed to the DAU \cite{01, cycleisp} to get the weighted wavelet feature  and we perform the inverse wavelet transformation (IWT) to reshape  to the shape of  and become the weighted normal domain feature . After concatenating these two features ( and ), the residual feature  are obtained by passing these feature to  convolution layer and parameter ReLU layer. Finally, adding the shortcut features through  convolution to the residual feature , we get the output feature  which has the wavelet attention feature information.

\begin{figure}[!htbp] 
\centering
	\includegraphics[width=8.7cm]{./figures/model/HWAB_frame.png}
\centering
\caption{Illustration of the Half Wavelet Attention Block (HWAB) in our HWMNet.}
\label{HWAB}
\end{figure}





\section{Experiment}
\subsection{Experiment Setup}
\noindent\textbf{Implementation Details.} Our HWMNet is an end-to-end model and trained from scratch. The experiments conducted in this paper are implemented by PyTorch 1.8.0 with single NVIDIA GTX 1080Ti GPU. The network is trained on  patches only with a batch size of 2 for  iterations. For data augmentation, randomly horizontal and vertical flips are adopted. We use Adam optimizer with the initial learning rate of  and decrease to  by the consine annealing strategy. 


\noindent\textbf{Evaluation Metrics.} For the quantitative comparisons, we consider the Peak Signal-to-Noise Ratio (PSNR), Structural Similarity (SSIM) index and Learned Perceptual Image Patch Similarity (LPIPS) metric \cite{LPIPS}. Noted that the performance is better when both PSNR and SSIM values are higher. On the contrary, the LPIPS value is lower when the restoration performance is closer to human perception.

\begin{figure*}[!t] \centering
	\subfigure{
	\begin{minipage}{2.9cm}
	\includegraphics[width=2.8cm]{./figures/visual_LOL/low.png}
	\vspace*{-2.5mm}
	\caption*{\small{5.09 / 0.197 / 0.523}}
	\vspace*{-4mm}
	\caption*{\small{Low-light}}
	\end{minipage}
	\begin{minipage}{2.9cm}
	\includegraphics[width=2.8cm]{./figures/visual_LOL/Zero-DCE.png}
	\vspace*{-2.5mm}
	\caption*{\small{11.87 / 0.637 / 0.291}}
	\vspace*{-4mm}
	\caption*{\small{Zero-DCE \cite{zeroDCE}}}
	\end{minipage}
	\begin{minipage}{2.9cm}
	\includegraphics[width=2.8cm]{./figures/visual_LOL/LIME.png}
	\vspace*{-2.5mm}
	\caption*{\small{14.68 / 0.576 / 0.286}}
	\vspace*{-4mm}
	\caption*{\small{LIME \cite{lime}}}
	\end{minipage}
	\begin{minipage}{2.9cm}
	\includegraphics[width=2.8cm]{./figures/visual_LOL/RetinexNet.png}
	\vspace*{-2.5mm}
	\caption*{\small{17.78 / 0.492 / 0.430}}
	\vspace*{-4mm}
	\caption*{\small{RetinexNet \cite{retinexnet}}}
	\end{minipage}
	\begin{minipage}{2.9cm}
	\includegraphics[width=2.8cm]{./figures/visual_LOL/EnlightenGAN.png}
	\vspace*{-2.5mm}
	\caption*{\small{15.20 / 0.737 / 0.250}}
	\vspace*{-4mm}
	\caption*{\small{EnlightenGAN \cite{enlightengan}}}
	\end{minipage}
	\begin{minipage}{2.9cm}
	\includegraphics[width=2.8cm]{./figures/visual_LOL/DRBN.png}
	\vspace*{-2.5mm}
	\caption*{\small{17.72 / 0.896 / 0.121}}
	\vspace*{-4mm}
	\caption*{\small{DRBN \cite{DRBN}}}
	\end{minipage}
	}\vspace*{-2mm}
	\quad
	\subfigure{
	\begin{minipage}{2.9cm}
	\includegraphics[width=2.8cm]{./figures/visual_LOL/KinD.png}
	\vspace*{-2.5mm}
	\caption*{\small{10.94 / 0.717 / 0.300}}
	\vspace*{-4mm}
	\caption*{\small{KinD \cite{kind}}}
	\end{minipage}
	\begin{minipage}{2.9cm}
	\includegraphics[width=2.8cm]{./figures/visual_LOL/KinD++.png}
	\vspace*{-2.5mm}
	\caption*{\small{17.71 / 0.882 / 0.144}}
	\vspace*{-4mm}
	\caption*{\small{KinD++ \cite{kind++}}}
	\end{minipage}
	\begin{minipage}{2.9cm}
	\includegraphics[width=2.8cm]{./figures/visual_LOL/MIRNet.png}
	\vspace*{-2.5mm}
	\caption*{\small{\textbf{30.74} / 0.907 / 0.091}}
	\vspace*{-4mm}
	\caption*{\small{MIRNet \cite{01}}}
	\end{minipage}
	\begin{minipage}{2.9cm}
	\includegraphics[width=2.8cm]{./figures/visual_LOL/LLFlow.png}
	\vspace*{-2.5mm}
	\caption*{\small{19.33 / \underline{0.917} / \underline{0.086}}}
	\vspace*{-4mm}
	\caption*{\small{LLFlow \cite{LLFlow}}}
	\end{minipage}
	\begin{minipage}{2.9cm}
	\includegraphics[width=2.8cm]{./figures/visual_LOL/HWMNet.png}
	\vspace*{-2.5mm}
	\caption*{\small{\underline{28.34} / \textbf{0.928} / \textbf{0.076}}}
	\vspace*{-4mm}
	\caption*{\small{\textbf{HWMNet (Ours)}}}
	\end{minipage}
	\begin{minipage}{2.9cm}
	\includegraphics[width=2.8cm]{./figures/visual_LOL/high.png}
	\vspace*{-2.5mm}
	\caption*{\small{PSNR / SSIM / LPIPS}}
	\vspace*{-4mm}
	\caption*{\small{GT}}
	\end{minipage}
	}\vspace*{-4.5mm}  \caption{Visual comparisons for low-light image enhancement on the LOL dataset \cite{lol}. The best and second best scores are \textbf{highlighted} and \underline{underlined}, respectively.}
\label{LOL_figure}
\end{figure*}

\begin{table}[!b] \small
	\caption{Low-light image enhancement evaluation on the LOL dataset \cite{lol}. The best and second best scores are \textbf{highlighted} and \underline{underlined}, respectively.}
\vspace*{-4.5mm}  \label{LOL_table}
	\begin{center}
\setlength{\tabcolsep}{3.5mm}
	\begin{tabular}{l || ccc}
	\toprule[1.5 pt]
\multirow{2}{*}{\textbf{Methods}}&\multicolumn{3}{c}{Low-light dataset (LOL) \cite{lol}}\\
&PSNR &SSIM &LPIPS \\
	\midrule[1.5 pt]
MSR \cite{MSR}  					&13.17&0.48&-\\
Zero-DCE \cite{zeroDCE}			&14.86&0.54&0.33\\
LIME \cite{lime}           			&16.76&0.56&0.35\\
Retinex-Net \cite{retinexnet}		&16.77&0.56&0.47\\
EnlightenGAN \cite{enlightengan}  &17.48&0.65&0.32\\
RUAS \cite{RUAS}					&18.23&0.72&0.35\\
GLAD \cite{gladnet}  				&19.72&0.70&-\\
DRBN \cite{DRBN}					&20.13&0.83&0.16\\
KinD \cite{kind}  					&20.87&0.80&0.17\\
KinD++ \cite{kind++}  				&21.30&0.82&0.16\\
MIRNet \cite{01}					&24.14&0.83&0.13\\
LLFlow \cite{LLFlow}				&\textbf{25.19}&\textbf{0.93}&\textbf{0.11}\\
	\midrule[1.5 pt]
\textbf{HWMNet (Ours)}          	&\underline{24.24}&\underline{0.85}&\underline{0.12}\\
	\bottomrule[1.5pt]
	\end{tabular}
	\end{center}
\end{table}



\begin{table*}[!ht] \footnotesize
\caption{Low-light image enhancement evaluation on the MIT-FiveK dataset \cite{fivek}. The best and second best scores are \textbf{highlighted} and \underline{underlined}, respectively.}
\vspace*{-5mm}  \label{fiveK_table}
\begin{center}
\setlength{\tabcolsep}{4.2mm}
\begin{tabular}{l cccccccc}
\toprule[1.5 pt]
Method&HDRNet \cite{HDR}&W-Box \cite{wbox}&DR \cite{dr}&DPE \cite{DPE}&DeepUPE \cite{DeepUPE}&MIRNet \cite{01}&HWMNet (Ours)\\
\midrule[1.5 pt]
PSNR&21.96&18.57&20.97&22.15&23.04&\underline{23.73}&\textbf{24.44}\\
SSIM&0.866&0.701&0.841&0.850&0.893&\textbf{0.925}&\underline{0.914}\\
\bottomrule[1.5pt]
\end{tabular}
\end{center}
\end{table*}

\begin{figure}[!t] \footnotesize
\centering
	\subfigure{
	\begin{minipage}{2.8cm}
	\includegraphics[width=2.7cm]{./figures/visual_fiveK/low.png}
	\vspace*{-2.5mm}
	\caption*{5.09/0.197/0.523}
	\vspace*{-4mm}
	\caption*{Low-light}
	\end{minipage}
	\begin{minipage}{2.8cm}
	\includegraphics[width=2.7cm]{./figures/visual_fiveK/DPE.png}
	\vspace*{-2.5mm}
	\caption*{17.66/0.859/0.106}
	\vspace*{-4mm}
	\caption*{DPE \cite{DPE}}
	\end{minipage}
	\begin{minipage}{2.8cm}
	\includegraphics[width=2.7cm]{./figures/visual_fiveK/DeepUPE.png}
	\vspace*{-2.5mm}
	\caption*{25.06/0.898/0.087}
	\vspace*{-4mm}
	\caption*{DeepUPE \cite{DeepUPE}}
	\end{minipage}
	}\vspace*{-2mm}
	\quad
	\subfigure{
	\begin{minipage}{2.8cm}
	\includegraphics[width=2.7cm]{./figures/visual_fiveK/MIRNet.png}
	\vspace*{-2.5mm}
	\caption*{\textbf{31.07}/\underline{0.950}/\textbf{0.029}}
	\vspace*{-4mm}
	\caption*{MIRNet \cite{01}}
	\end{minipage}
	\begin{minipage}{2.8cm}
	\includegraphics[width=2.7cm]{./figures/visual_fiveK/HWMNet.png}
	\vspace*{-2.5mm}
	\caption*{\underline{26.51}/\textbf{0.964}/\underline{0.032}}
	\vspace*{-4mm}
	\caption*{\textbf{HWMNet (Ours)}}
	\end{minipage}
	\begin{minipage}{2.8cm}
	\includegraphics[width=2.7cm]{./figures/visual_fiveK/high.png}
	\vspace*{-2.5mm}
	\caption*{PSNR/SSIM/LPIPS}
	\vspace*{-4mm}
	\caption*{GT}
	\end{minipage}
	}\caption{Visual comparisons for low-light image enhancement on the MIT-Adobe FiveK dataset \cite{fivek}.}\label{fiveK_figure}
\end{figure}

\subsection{Experiment Datasets}
In this section, we will introduce two datasets for real-world low-light image enhancement. In the following experiments, we train separate models for different datasets. And the training, testing set details are illustrated below.

\noindent\textbf{LOL.} Low-light dataset \cite{lol} provides 485 and 15 images for training and testing, respectively. All the image sizes are , and each image pair in LOL dataset consists of a low-light image and its corresponding normal brightness ground-truth. In the experiments of LOL dataset, we randomly crop each training image to 10  patches. As for the validation set, we use center cropped  patch for each testing image. 

\noindent\textbf{MIT-Adobe FiveK.} To train another dataset, MIT-Adobe FiveK \cite{fivek} contains 5000 images including both indoor and outdoor scenes captured with single-lens reflex (SLR) cameras in different lighting conditions. We follow the same experimental setup as other image enhancement methods \cite{01} which consider the enhanced images of expert C (one of the  experts from A to E) as the ground-truth images. Furthermore, the first 4500 images are for training and the last 500 images are for testing. 




\subsection{Image Enhancement Performance}
\noindent\textbf{Evaluation on LOL.} In Table~\ref{LOL_table} and Fig.~\ref{LOL_figure}, we compare our HWMNet with the prior-based method (e.g., MSR \cite{MSR}) and other competitive enhancement learning-based methods on LOL dataset \cite{lol}. According to Table~\ref{LOL_table}, we could observe the proposed HWMNet achieves runner-up performance for all metrics (PSNR, SSIM and LPIPS) on LOL testing dataset, which means that our enhanced images are more perceptually faithful. Also, among the three top-performed methods (MIRNet \cite{01}, LLFlow \cite{LLFlow}, and our HWMNet), the floating-point operations per second (FLOPs) for MIRNet, LLFlow and HWMNet are 2.84T, 1.05T and 0.92T (tested on  color image, where T is ), respectively. This means the proposed HWMNet has the least computational complexity among the three best-performed models. We think it is attributed to the design of HWAB, where half of the features passing through the attention can decrease the computational complexity \cite{hinet}, and the design of wavelet attention could gain more semantic information and details in the training process to keep satisfactory results.


\noindent\textbf{Evaluation on MIT-Adobe FiveK.} As for the MIT-5K \cite{fivek}, the average performance scores on the testing set and comparisons of some enhanced image are shown in Table~\ref{fiveK_table} and Fig.~\ref{fiveK_figure}. It shows that the proposed HWMNet still achieves state-of-the-art performance on both PSNR and SSIM. 



\section{Conclusion}
In this paper, we present a restoration model HWMNet and achieve the competitive performances on low-light image enhancements. We improved the hierarchical model M-Net which is originally proposed for medical image segmentation. The novel design of M-Net+ has the advantage of enriching features with different resolutions. Moreover, the proposed HWAB focuses on the features of wavelet domain which can enrich the semantic information, too. In the future work, we are going to improve current model to handle different restoration tasks such as image denoising and deblurring.

\bibliographystyle{IEEEbib}
\fontsize{8.5pt}{8.5pt}
\selectfont
\bibliography{refs}

\end{document}
