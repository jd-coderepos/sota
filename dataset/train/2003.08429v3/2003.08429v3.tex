\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{comment}
\usepackage{amsmath,amssymb}
\usepackage{color}
\usepackage{siunitx}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{xcolor}
\usepackage{pgfplots}
\usepackage{floatrow}
\usepackage{multirow}
\usepackage{wrapfig,lipsum,booktabs}
\usepackage{subcaption}
\usepackage{sidecap}
\captionsetup{compatibility=false}
\captionsetup[subtable]{labelformat=simple, labelsep=space}
\usepackage[font=footnotesize,labelfont=bf]{caption}
\usepackage{url}

\renewcommand\thesubtable{(\alph{subtable})}

\usepackage{xspace}
\usepackage[export]{adjustbox}

\usetikzlibrary{arrows.meta}

\usepackage{bold-extra}

\newcommand*{\eg}{\emph{e.g.}\@\xspace}
\newcommand*{\ie}{\emph{i.e.}\@\xspace}
\newcommand*{\cf}{\emph{c.f.}\@\xspace}
\newcommand*{\etal}{\emph{et al.}\@\xspace}

\newcommand{\refsec}[1]{Sec.\,\ref{sec:#1}}
\newcommand{\refchap}[1]{Ch.\,\ref{chp:#1}}
\newcommand{\refpart}[1]{Part\,\ref{part:#1}}
\newcommand{\refequ}[1]{Eq.\,\ref{eq:#1}}
\newcommand{\reffig}[1]{Fig.\,\ref{fig:#1}}
\newcommand{\reftab}[1]{Tab.\,\ref{tab:#1}}
\newcommand{\parag}[1]{\vspace{0ex} \textit{#1}}

\newcommand{\todo}[1]{\textcolor{red}{\small Todo:\,#1}\PackageWarning{TODO:}{#1!}}

\makeatletter
\newcommand*{\etc}{\@ifnextchar{.}{etc}{etc.\@\xspace}}



\definecolor{ubpubColor}{rgb}{0.43, 0.5, 0.5}
\definecolor{backboneColor}{rgb}{0.423, 0.325, 0.365}
\definecolor{fpnColor}{rgb}{0.255, 0.498, 0.416}

\newcommand{\PAR}[1]{\vskip4pt \noindent {\bf #1~}}
\newcommand{\PARbegin}[1]{\noindent {\bf #1~}}
\newcommand{\TODO}[1]{\textcolor{red}{#1}}
\newcommand{\Francis}[1]{\textcolor{blue}{#1}}
\newcommand{\DOWE}[1]{\textcolor{green}{#1}}
\newcommand{\T}{\textcolor{red}{\textbf{T}}}
\newcommand{\REF}{\textcolor{blue}{\textbf{REF}}}
\newcommand{\UNPUB}[1]{\textcolor{ubpubColor}{#1}}
\newcommand{\lau}[1]{\textcolor{magenta}{\textbf{Laura: }{#1}}}
\newcommand{\sab}[1]{\textcolor{orange}{\textbf{Sabari: }{#1}}}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\newcommand{\RM}[1]{\textcolor{green}{#1}}

\newcommand*{\ra}{\textcolor{magenta}{\textbf{R1}}\@\xspace}
\newcommand*{\rb}{\textcolor{blue}{\textbf{R2}}\@\xspace}
\newcommand*{\rc}{\textcolor{orange}{\textbf{R3}}\@\xspace}



\setlength{\floatsep}{5pt plus2pt minus4pt}
\setlength{\textfloatsep}{5pt plus2pt minus4pt}
\setlength{\dblfloatsep}{5pt plus2pt minus4pt}
\setlength{\dbltextfloatsep}{5pt plus2pt minus4pt}





\newcommand{\known}{\textit{known} }
\newcommand{\unknown}{\textit{unknown} }
\newcommand{\matrixvar}[1]{\bm{\mathit{#1}}}
 
\begin{document}

\pagestyle{headings}
\mainmatter
\def\ECCVSubNumber{1299}

\newcommand\blfootnote[1]{\begingroup
  \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}

\title{STEm-Seg: Spatio-temporal Embeddings for Instance Segmentation in Videos}

\titlerunning{STEm-Seg}
\author{Ali Athar*\inst{1} \and
Sabarinath Mahadevan*\inst{1} \and
Aljo\u{s}a O\u{s}ep\inst{2}  \and
Laura Leal-Taix\'{e}\inst{2}  \and
Bastian Leibe\inst{1}
}
\authorrunning{Athar, Mahadevan~\etal}

\institute{RWTH Aachen University, Germany 
\email{\{athar,mahadevan,leibe\}@vision.rwth-aachen.de}\\ \and
Technical University of Munich, Germany\\
\email{\{aljosa.osep,leal.taixe\}@tum.de}}

\maketitle

\blfootnote{* Equal contribution}
\begin{abstract}



Existing methods for instance segmentation in videos typically involve multi-stage pipelines that follow the tracking-by-detection paradigm and model a video clip as a sequence of images. 
Multiple networks are used to detect objects in individual frames, and then associate these detections over time.
Hence, these methods are often 
non-end-to-end trainable and highly tailored to specific tasks. 
In this paper, we propose a different approach that is well-suited to a variety of tasks involving instance segmentation in videos.
In particular, we model a video clip as a single 3D spatio-temporal volume, and propose a novel approach that segments and tracks instances across space and time in a single stage. 
Our problem formulation is centered around the idea of spatio-temporal embeddings which are trained to cluster pixels belonging to a specific object instance over an entire video clip.
To this end, we introduce (i) novel mixing functions that enhance the feature representation of spatio-temporal embeddings, and (ii) a single-stage, proposal-free network that can reason about temporal context. Our network is trained end-to-end to learn spatio-temporal embeddings as well as parameters required to cluster these embeddings, thus simplifying inference.
Our method achieves state-of-the-art results across multiple datasets and tasks. Code and models are available at \url{https://github.com/sabarim/STEm-Seg}. 














\end{abstract} 
\section{Introduction}

The task of segmenting and tracking multiple objects in videos is becoming increasingly popular due to a surge in development of autonomous vehicles and robots that are able to perceive and accurately track surrounding objects.  
These advances are driven by the recent emergence of new datasets~\cite{Caelles19arXiv,Voigtlaender19CVPR,Yang19ICCV} containing videos with dense, per-pixel annotations of object instances.
The underlying task tackled in these datasets can be summarized as follows: given an input video containing multiple objects, each pixel has to be uniquely assigned to a specific object instance or to the background.

\begin{figure}[t]
\centering
  \rotatebox{90}{\resizebox{1.5cm}{!}{\parbox{3cm}{\center\footnotesize \vspace{-7px} Video Object\\Segmentation}}}
  \includegraphics[width=0.23\textwidth, frame]{figures/davis_1.png}\hspace{1px}\includegraphics[width=0.23\textwidth, frame]{figures/davis_2.png}\hspace{1px}\includegraphics[width=0.23\textwidth, frame]{figures/davis_3.png}\hspace{1px}\includegraphics[width=0.23\textwidth, frame]{figures/davis_4.png}\\
\rotatebox{90}{\resizebox{1.5cm}{!}{\parbox{3cm}{\center\footnotesize \vspace{-7px} Video Instance\\Segmentation}}}
  \includegraphics[width=0.23\textwidth, trim={330 200 0 0}, clip, frame]{figures/ytvis_tiles/2.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth, trim={330 200 0 0}, clip,frame]{figures/ytvis_tiles/7.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth, trim={330 200 0 0}, clip,frame]{figures/ytvis_tiles/11.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth, trim={330 200 0 0}, clip,frame]{figures/ytvis_tiles/18.jpg}\\
\rotatebox{90}{\resizebox{1.5cm}{!}{{\parbox{3cm}{\center\footnotesize \vspace{-7px} Multi-Object\\Tracking \& Segment.}}}}
  \includegraphics[width=0.23\textwidth, trim={50 0 520 0}, clip, frame]{figures/kittimots_tiles/80.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth, trim={50 0 520 0}, clip, frame]{figures/kittimots_tiles/110.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth, trim={50 0 520 0}, clip, frame]{figures/kittimots_tiles/139.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth, trim={50 0 520 0}, clip, frame]{figures/kittimots_tiles/176.jpg}\\
\raggedleft
\begin{tikzpicture}[node distance=2cm]
\node (A) at (2.75, 0) {};
\node (B) at (13.0, 0) {};
\draw[-{Stealth}, to path={-- (\tikztotarget)}](A) edge (B);
\node[text width=1cm] at (13.5,0){time};
\end{tikzpicture}
    \caption{Our method is applicable to multi-object segmentation tasks such as VOS \textit{(top)}, VIS \textit{(middle)} and MOTS \textit{(bottom)}.}
  \label{fig:teaser}
\end{figure}

State-of-the-art methods tackling these tasks~\cite{Voigtlaender19CVPR,Wang19ICCVW,Yang19ICCV} usually operate in a \textit{top-down} fashion and follow the \textit{tracking-by-detection} paradigm which is well-established in multi-object tracking (MOT)~\cite{ButtCollins13CVPR,Huang08ECCV,Leibe08TPAMI,Okuma04ECCV}. Such methods usually employ multiple networks to detect objects in individual images~\cite{He17ICCV}, associate them over consecutive frames, and resolve occlusions using learned appearance models.
Though these approaches yield high-quality results, they involve multiple networks, are computationally demanding, and not end-to-end trainable. 

Inspired by the Perceptual Grouping Theory~\cite{Palmer02}, 
we learn to segment object instances in videos in a \textit{bottom-up} fashion by leveraging \emph{spatio-temporal} embeddings. To this end, 
we propose an efficient, single-stage network that operates directly on a 3D spatio-temporal volume.
We train the embeddings in a category-agnostic setting, such that pixels belonging to the same object instance across the spatio-temporal volume are mapped to a single cluster in the embedding space.
This way, we can infer object instances by simply assigning pixels to their respective clusters.
Our method outperforms proposal-based methods for tasks involving pixel-precise tracking such as Unsupervised Video Object Segmentation (UVOS)~\cite{Caelles17CVPR,Caelles19arXiv}, Video Instance Segmentation (VIS)~\cite{Yang19ICCV}, and Multi-Object Tracking and Segmentation (MOTS)~\cite{Voigtlaender19CVPR}. 

To summarize, our contributions are the following: (i) We propose a unified approach for tasks involving instance segmentation in videos~\cite{Caelles19arXiv,Voigtlaender19CVPR,Yang19ICCV}. Our method performs consistently well under highly varied settings (see \reffig{teaser}) such as automotive driving scenes~\cite{Voigtlaender19CVPR}, semantically diverse YouTube videos~\cite{Yang19ICCV} and scenarios where object classes are not pre-defined~\cite{Caelles19arXiv}.
(ii) We propose using spatio-temporal embeddings for the aforementioned set of tasks. To this end, we propose a set of mixing functions (Sec.~\ref{sec:embedding_representation}) that improve performance by modifying the feature representation of these embeddings. Our method enables a simple inference procedure based on clustering within a 3D spatio-temporal volume, thus alleviating the need for external components for temporal association.
(iii) We propose a single-stage network architecture which is able to effectively incorporate temporal context and learn the spatio-temporal embeddings.
 

\section{Related Work}

\PAR{Image-level Instance Segmentation:}
Several existing methods for image-level instance segmentation, which is closely related to our task, operate in a \textit{top-down} fashion by using a Region Proposal Network to generate object proposals~\cite{Chen19ICCV,Pinheiro16ECCV,Pinheiro16NIPS,Ren15NIPS}, 
which are then classified and segmented~\cite{He17ICCV}.
Other methods operate in a \textit{bottom-up} fashion by grouping pixels belonging to the same object instance~\cite{Brabandere17CVPRW,Brabandere17arxiv,Kong18CVPR,Neven19CVPR,Newell17NIPS,Novotny18ECCV}.
Novotny~\etal~\cite{Novotny18ECCV} introduce an embedding mixing function to overcome appearance ambiguity, and predict a displacement field from the instance-specific center, which is similar to Hough voting based methods for object detection~\cite{Leibe08IJCV,wang20cvpr}. Recent methods for 3D object detection and instance segmentation also follow this trend~\cite{Engelmann20CVPR,jiang20cvpr,Qi19CVPR,Elich19GCPR,zhang2020spatial}.
Neven~\etal~\cite{Neven19CVPR} extend~\cite{Brabandere17CVPRW} by training a network to predict object centers and clustering bandwidths, thus alleviating the need for density-based clustering algorithms~\cite{Comaniciu02TPAMI,Ester96KDD,Mcinnes2017OSS}. This serves as a basis for our work.
However, in contrast to our approach, the aforementioned methods are only suitable for image-level segmentation. 

\PAR{Video Segmentation:}
Temporally consistent object segmentation in videos can benefit several other tasks such as action/activity recognition~\cite{Gkioxari15CVPR,Hou17ICCV}, video object detection~\cite{Kang17CVPR,Feichtenhofer17ICCV} and object discovery~\cite{Kwak15ICCV,Osep19ICRA,Wang14ECCV,Xiao16CVPR,Dave19arxiv,xie19cvpr}.
Several \textit{bottom-up} methods segment moving regions by grouping based on optical flow~\cite{Van13ICCV,Xu12CVPR} or point trajectories~\cite{Brox10ECCV,Ochs12CVPR,Palou13CVPR,xie19cvpr}. By contrast, our method segments and tracks both moving and static objects.
Other methods obtain video object proposals~\cite{Feichtenhofer17ICCV,Gkioxari15CVPR,Hou17ICCV,Kang17CVPR} based on cues such as actions~\cite{Gkioxari15CVPR,Hou17ICCV} or image-level objects of interest~\cite{Kang17CVPR,Osep19arxiv}. Feature representations are then learned for these proposals in order to perform classification. Instead, we propose a single-stage approach that localizes objects with pixel-level precision.

\PAR{Pixel-precise Tracking of Multiple Objects:}
Multi object tracking (MOT) has its roots in robotic perception~\cite{JainNagel79TPAMI,Paragios00TPAMI,Teichman11ICRA,Wren97TPAMI}. 
Although some works extend MOT with pixel-precise masks~\cite{Milan15CVPR,Osep18ICRA}, a much larger set of works can be found in the domain of Video Object Segmentation (VOS), which encompasses multiple tasks related to pixel-precise tracking. In the \textit{semi-supervised} variant of VOS~\cite{Caelles18arXiv}, the ground-truth masks of the objects which are meant to be tracked are given for the first frame. 
State-of-the-art approaches to this task~\cite{Caelles17CVPR,Ting17NIPS,Oh2019ICCV,Tokmakov17ICCV,Ventura19CVPR,Voigtlaender19CVPRFeelVOS,Wug18CVPR,Xu18ECCV,Yang18CVPR,Zeng19ICCV} usually involve online fine-tuning on the first frame masks and/or 
follow the \textit{object proposal generation and mask propagation} approach~\cite{Luiten18ACCV,Zeng19ICCV}.
Chen~\etal~\cite{Chen2018CVPR} tackle this problem by learning embeddings from the first-frame masks, and then associating pixels in the remaining frames. 

More relevant to our work is the task of \textit{unsupervised} VOS~\cite{Caelles19arXiv,Perazzi16CVPR}. 
Here, no ground truth information is provided at test-time, and the goal is to segment and track all dominant ``foreground" objects in the video clip. 
Current state-of-the-art methods for this task are either proposal-based~\cite{Zeng19ICCV,Zulfikar19CVPRW} or focus on foreground-background segmentation~\cite{Hou19BMVC,Jain17CVPR,SiamICRA2019,Song18ECCV,Tokmakov17ICCV,Ventura19CVPR,Yang19ICCVAnchorDiff,Koh17CVPR}. Li~\etal~\cite{Li18CVPR} propose an approach that groups pixel embeddings based on \textit{objectness} and optical flow cues. 
In contrast to ours, this method processes each frame separately, employs K-means clustering for object localization, and cannot separate different object instances. 
Wang~\etal~\cite{Wang19ICCV} employ an attentive graph neural network~\cite{Gori05IJCNN} and use differentiable message passing to propagate information across time; we compare our results to theirs in \refsec{saliency_seg}.

Recently, the task of Multi-Object Tracking and Segmentation (MOTS)~\cite{Voigtlaender19CVPR} and Video Instance Segmentation (VIS)~\cite{Yang19ICCV} were introduced. 
Voigtlaender~\etal~\cite{Voigtlaender19CVPR} extended the KITTI~\cite{Geiger12CVPR} and MOTChallenge~\cite{LealTaixe15arxiv,Milan16arxiv} datasets with pixel-precise annotations, and proposed a baseline method that adapts Mask R-CNN~\cite{He17ICCV} to associate object proposals across time. Hu~\etal~\cite{Hu19Arxiv} use a mask network to filter foreground embeddings and perform mean-shift clustering to generate object masks, which are then associated over time using a distance threshold unlike our single-stage end-to-end method. 

The YouTube-VIS~\cite{Yang19ICCV} contains a large number of YouTube videos with per-pixel annotations for a variety of object classes. Compared to MOTS, these videos contain fewer instances, but are significantly more diverse. 
In addition to adapting several existing methods~\cite{Bochinski17AVSS,Han16arxiv,Voigtlaender19CVPRFeelVOS,Wojke17ICIP,Yang18CVPR} for this task, Yang ~\etal~\cite{Yang19ICCV} also proposed their own method (MaskTrack-RCNN) which extends Mask R-CNN with additional cues for data association. 
Methods such as~\cite{Dong19ICCVW,Feng19ICCVW,Liu19ICCVW} also rely on object proposals and/or heuristic post-processing to associate objects over time, unlike our end-to-end bottom-up approach. 
\section{Our Method}

We tackle the task of instance segmentation in videos by modeling the video clip as a 3D spatio-temporal volume and using a network to learn an embedding for each pixel in that volume. This network is trained to push pixels belonging to different object instances towards different, non-overlapping clusters in the embedding space. 
This differs from most existing approaches, which first generate object detections per-frame, and then associate them over time. The following sections explain our problem formulation, the network architecture and loss functions employed, and the inference process.

\subsection{Problem Formulation}
\label{sec:problem_formulation}

As input, we assume a video clip with  frames of resolution . Let us denote the set of RGB pixels in this clip with  where . 
Assuming there are  object instances in this clip ( is unknown), our aim is to produce a segmentation that assigns each pixel in the clip to either the background, or to exactly one of these  instances.
We design a network that predicts video instance tubes by clustering pixels simultaneously across space and time based on a learned embedding function. 
Instead of learning just the embedding function and then using standard density-based clustering algorithms~\cite{Fukunaga1975TOIT,Lloyd1982TOIT,Mcinnes2017OSS} to obtain instance tubes, we take inspiration from Neven~\etal~\cite{Neven19CVPR} and design a network which estimates the cluster centers as well as their corresponding variances, thus enabling efficient inference.
Formally, our network can be viewed as a function that maps the set of pixels  to three outputs; (1) : an -dimensional embedding for each pixel, (2) : a positive variance for each pixel and each embedding dimension, and (3) : an object instance center heat-map.

\PAR{Instance representation:} The network is trained such that the embeddings belonging to the  instance in the video clip are modeled by a multivariate Gaussian distribution . 
Assuming that this instance comprises the set of pixel coordinates  with cardinality , we denote the embeddings and variances output by the network at these coordinates using 
,  
and 
, , respectively.
During training,  (\ie, the ground truth mask tube for instance ) is known. Using it, the mean  and covariance  of the distribution are computed by averaging over the per-pixel outputs:


This single distribution models all embeddings belonging to instance  across the entire clip (not individually for each frame).
We can now use the distribution  to compute the probability  of any embedding , anywhere in the input clip, of belonging to instance :

Using \refequ{probs} to compute , , we can obtain the set of pixels  comprising the predicted mask tube for instance  by thresholding the probabilities at 0.5:


\PAR{Training:} This way, the training objective can be formulated as one of learning the optimal parameters  and  which maximize the intersection-over-union (IoU) between the predicted and ground-truth mask tubes in the clip: 


that is, all pixels in the ground truth mask tube  should have probability larger than , and vice versa. A classification loss such as cross-entropy could be used to optimize the pixel probabilities. However, we employ the Lov\`{a}sz hinge loss~\cite{Berman18CVPR,Berman18CVPR2,Neven19CVPR,Yu2015ICML} (details in supplementary material), which is a differentiable, convex surrogate of the Jaccard index that directly optimizes the IoU. This formulation allows  and  to be implicitly learned by the network. 

Using Eqs.~\ref{eq:mean_var}-\ref{eq:optimization}, we can define and optimize the distribution for every instance (\ie ). Note that only a single forward pass of the network is required regardless of the number of instances in the clip.
This is in contrast to common approaches for Video Object Segmentation, which only process one instance at a time~\cite{Oh2019ICCV,Wug2018CVPR}.

We further remark that ours is a \textit{bottom-up approach} which detects and tracks objects in a single step, thus mitigating the inherent drawback of \textit{top-down} approaches that often require different networks/cues for single-image object detection and temporal association. A further advantage of our approach is that it can implicitly resolve occlusions, insofar as they occur within the clip. 

\subsection{Embedding Representation}
\label{sec:embedding_representation}

Under the formulation described in \refsec{problem_formulation}, the network can learn arbitrary representations for the embedding dimensions. However, it is also possible to fix the representation by using a mixing function  that modifies the embeddings  as follows: .

In~\cite{Neven19CVPR}, for the task of single-image instance segmentation, 2D embeddings were used () in conjunction with a spatial coordinate mixing function 
\footnote{This notation denotes element-wise addition between  and }.
With this setting, the embeddings could be interpreted as offsets to the  coordinates of their respective locations. The network thus learned to cluster the embeddings belonging to a given object towards some object-specific point on the image. It follows that the predicted variances could be interpreted as the network's estimate of the size of the object along the  and  axes. We postulate that the reason for this formulation yielding good results is that the  coordinates already serve as a good initial feature for instance separation; the network can then enhance this representation by producing offsets which further improve the clustering behavior. Furthermore, this can be done in an end-to-end trainable fashion which allows the network to adjust the clustering parameters, \ie, the Gaussian distribution parameters, for each instance. In general, it has been shown that imparting spatial coordinate information to CNNs can improve performance for a variety of tasks~\cite{Liu2018NIPS,Novotny18ECCV}.

Compared to single-image segmentation, the task of associating pixels across space and time in in videos poses additional challenges, \eg, camera ego-motion, occlusions, appearance/pose changes. To tackle these challenges we propose (and experimentally validate in \refsec{ablation}) the following extensions:

\PAR{Spatio-temporal coordinating mixing:} Since we operate on video clips instead of single images, a logical extension is to use 3D embeddings (\,=\,) with a spatio-temporal coordinate mixing function .

\PAR{Free dimensions:} In addition to the spatial (and temporal) coordinate dimensions, it can be beneficial to include extra dimensions whose representation is left for the network to decide. The motivation here is to improve instance clustering quality by allowing additional degrees of freedom in the embedding space. From here on, we shall refer to these extra embedding dimensions as \textit{free dimensions}. For example, if  with 2 spatial coordinate dimensions and 2 free dimensions, the mixing function is denoted as . 

There is, however, a caveat with free dimensions: since the spatial (and temporal) dimensions already achieve reasonable instance separation, the network may converge to a poor local minimum by producing very large variances for the free dimensions instead of learning a discriminative feature representation. Consequently, the free dimensions may end up offering no useful instance separation during inference. We circumvent this problem at the cost of introducing one extra hyper-parameter by setting the variances for the free dimensions to a fixed value .
We justify our formulation quantitatively using multiple datasets and different variants of the mixing function  in \refsec{ablation}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth,height=4cm]{figures/pipeline.pdf}
    \caption{For an input video clip, our network produces embeddings (), variances (), and instance center heat map ().  contains one peak per object for the entire spatio-temporal volume ( for the rider,  for the horse). ,  and ,  are the corresponding embeddings and variances at  and , respectively. These quantities are then used to define the Gaussian distribution for each object.}
    \label{fig:pipeline}
\end{figure}

\subsection{Inference}
\label{sec:inference}

Since the ground truth mask tube is not known during inference, it is not possible to obtain  and  using  \refequ{mean_var}. This is where the instance center heat map  comes into play. For each pixel , the value  in the heat map at this location gives us the probability of the embedding vector  at this location being an instance center. The sequential process of inferring object instances in a video clip is described in the following algorithm:

\begin{enumerate}
    \item Identify the coordinates of the most likely instance center .
\item Find the corresponding embedding vector  and variances .
\item Using  and , generate the 3D mask tube  for this instance by computing per-pixel probabilities using \refequ{probs}, and then thresholding them as in \refequ{thresh}.
\item Since the pixels in  have now been assigned to an instance, the embeddings, variances and heat map probabilities at these pixel locations are masked out and removed from further consideration:

    \item Repeat steps 1-4 until either , or the next highest probability in the heat map falls below some threshold.
\end{enumerate}

Even though this final clustering step (\reffig{pipeline}) depends on the number of instances in a clip, in practice, the application of \refequ{probs} and~\ref{eq:thresh} carries little computational overhead and its run-time is negligible compared to a forward pass.

\PAR{Video clip stitching:} Due to memory constraints, the clip length that can be input to the network is limited. In order to apply our framework to videos of arbitrary length, we split the input video into clips of length  with an overlap of  frames between consecutive clips. Linear assignment~\cite{Kuhn55NRLQ} is then used to associate the predicted tracklets in consecutive clips. The cost metric for this assignment is the IoU between tracks in overlapping frames. Our approach is currently \textit{near online} because, given a new frame, the delay until its output becomes available is at most .

\subsection{Losses}
Our model's loss function is a linear combination of three  terms:

\PAR{Embedding loss :} As mentioned in  \refsec{problem_formulation}, we use the Lov\`{a}sz hinge loss to optimize the IoU between the predicted and ground truth masks for a given instance. The embedding loss for the entire input clip is calculated as the mean of the Lov\`{a}sz hinge loss for all object instances in that clip.

\PAR{Variance smoothness loss :} To ensure that the variance values at every pixel belonging to an object are consistent, we employ a smoothness loss  similar to ~\cite{Neven19CVPR}. This regresses the variances  for instance  to be close to the average value of all the variances for that instance, \ie .

\PAR{Instance center heat map loss :} For all pixels belonging to instance , the corresponding outputs in the sigmoid activated heat map  are trained with an  regression loss to match the output of \refequ{probs}. The outputs for background pixels are regressed to 0. During inference, this enables us to sample the highest values from the heat map which corresponds to the peak of the learned Gaussian distributions for the object instances in an input volume, as explained in Sec~\ref{sec:inference}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth,height=4.5cm]{figures/network_architecture.pdf}
    \caption{The network has an encoder-decoder structure. GN: Group Normalization~\cite{Wu18ECCV}.}
    \label{fig:network_architecture}
\end{figure}

\subsection{Network Architecture}
\label{sec:network_architecture}

The network (\reffig{network_architecture}) consists of an encoder with two decoders. The first decoder outputs the embeddings  and variances , while the second outputs the instance center heat map . The encoder comprises a backbone with Feature Pyramid Network (FPN) that produces feature maps at 4 different scales for each image in the input clip. The feature maps at each scale are then stacked along the temporal dimension before being input to each of the decoders. 

Our decoder consists of 3D convolutions and pooling layers which first compress the feature maps along the temporal dimension before gradually expanding them back to the input size. The underlying idea here is to allow the network to learn temporal context in order to enable pixel association across space and time. To reduce the decoder's memory footprint and run-time, the large sized feature maps undergo a lower degree of temporal pooling (\ie, fewer convolution/normalization/pooling layers). We call our decoder a \textit{temporal squeeze-expand decoder} (abbreviated as TSE decoder). In \refsec{saliency_seg}, we experimentally validate our network's stand-alone ability to learn spatio-temporal context.

\subsection{Category Prediction}
\label{sec:category_classification}

Our task formulation is inherently category-agnostic. However, some datasets~\cite{Voigtlaender19CVPR,Yang19ICCV} require a category label for each predicted object track. For such cases we add an additional TSE decoder to the network that performs semantic segmentation for all pixels in the input clip, and that is trained using a standard cross-entropy loss. During inference, the logits for all pixels belonging to a given object instance are averaged, and the highest scoring category label is assigned. Note that this is merely a post-processing step; the instance clustering still happens in a category-agnostic manner. 
 
\section{Experimental Evaluation}

To demonstrate our method's effectiveness and generalization capability, we apply it to three different tasks and datasets involving pixel-precise segmentation and tracking of multiple objects in videos. 

\subsection{Training}
\label{sec:training}
For all experiments, we use a ResNet-101\,\cite{He16CVPR} backbone initialized with weights from 
Mask R-CNN\,\cite{He17ICCV} trained for image instance segmentation on COCO\,\cite{Lin14ECCV}. The temporal squeeze-expand decoders are initialized with random weights. The network is optimized end-to-end using SGD with momentum 0.9 and an initial learning rate of  which decays exponentially.

\PAR{Augmented Images.} 
Since the amount of publicly available video data with dense per-pixel annotations is limited, we utilize image instance segmentation datasets by synthesizing training clips from single images. We apply on-the-fly random affine transformations and motion blur and show that this technique is effective (\refsec{ablation}) even though such augmented image sequences have little visual resemblance to video clips.

\subsection{Benchmarks}
\label{sec:benchmarks}

\PARbegin{DAVIS Unsupervised:} DAVIS\,\cite{Caelles19arXiv} is a popular video object segmentation dataset with 90 videos (60 for training and 30 for validation) containing multiple moving objects of diverse categories. Several DAVIS benchmarks have been introduced over the years; we evaluate our method on the 2019 \textit{Unsupervised Video Object Segmentation} (UVOS) benchmark in which the salient ``foreground objects'' in each video have to be segmented and tracked.
The evaluation measures employed are (i) -score (the IoU between predicted and ground truth mask tubes), and (ii) -score (accuracy of predicted mask boundaries against ground truth). The mean of those measures, , serves as the final score.

\PAR{YouTube-VIS:} The YouTube Video Instance Segmentation (YT-VIS)\,\cite{Yang19ICCV} dataset contains  high quality YouTube videos with 131k object instances spanning  known categories. 
The task requires all objects belonging to the known category set to be segmented, tracked and assigned a category label.
The evaluation measures used for this task are an adaptation of the Average Precision () and Average Recall () metrics used for image instance segmentation. 
\PAR{KITTI-MOTS:} Multi-Object Tracking and Segmentation (MOTS) \cite{Voigtlaender19CVPR} extends the KITTI multi-object tracking dataset~\cite{Geiger12CVPR} with pixel-precise instance masks. 
It contains 21 videos (12 for training and 9 for validation) of driving scenes captured from a moving vehicle. The task here is to segment and track all \textit{car} and \textit{pedestrian} instances in the videos. The evaluation measures used are an extension of the CLEAR MOT measures\,\cite{Bernardin08JIVP} to account for pixel-precise tracking (details in \cite{Voigtlaender19CVPR}). We report these measures separately for each object class.

\subsection{Ablation Studies}

\PAR{Embedding formulation:}
We first ablate the impact of using different mixing functions for the embeddings (\refsec{embedding_representation}) on the DAVIS'19 \texttt{val} dataset with clip length \,=\,, and summarize the results in \reftab{ablation_embeddings}. 
Compared to the identity function baseline , imparting a spatial coordinate offset () improves the  from  to . 
Adding another embedding dimension with a temporal coordinate offset, as in , yields a further improvement to . 

Comparing the mixing function pairs (, ) where , and (, ) where , we note that having a free dimension is slightly better than having a temporal dimension since there is a difference of   for both pairs of functions. 
This is the case for both DAVIS'19 and YT-VIS,\footnote{\label{fnote:embedding_formulation}results for various  on YT-VIS and KITTI-MOTS are given in supplementary} where  yields the best results. For KITTI-MOTS however,\footnotemark[\value{footnote}] temporal and free dimensions yield roughly the same performance. Our intuitive explanation is that for DAVIS and YT-VIS, object instances normally persist throughout the video clip. 
Therefore, in contrast to the spatial coordinates which serve as a useful feature for instance separation, the temporal coordinate provides no useful separation cue, thus rendering the temporal embedding dimension less effective. 
On the other hand, using a free dimension enables the network to learn a more discriminative feature representation that can better aid in separating instances. 
By contrast, objects in KITTI-MOTS driving scenes undergo fast motion and often enter/exit the scene midway through a clip. Thus, the temporal dimension becomes useful for instance separation.

Having additional embedding dimensions is beneficial, but only up to a certain point (\ie, ). 
Beyond that, test-time performance drops, as can be seen by comparing  and . 
We conclude by noting that our proposed formulations for  improve performance on video-related tasks compared to existing formulations for single-image tasks\,\cite{Neven19CVPR,Novotny2018ECCV}. For further discussion and results we refer to the supplementary material.


\label{sec:ablation}
\begin{table}[t]
    \centering
    \footnotesize
    \begin{subtable}[t]{0.25\linewidth}
    {\vspace{-2mm}
                \begin{tabular}[t]{p{2.5cm} | c | c }
\toprule
             {\footnotesize Mixing Function} & \& &  \\
            \cmidrule(lr){1-3}
                             & 57.3  & 2\\
                              & 61.6       & 2\\
                               & 62.6       & 3\\
                               & 62.8       & 3\\
                              & 64.2       & 4\\
                               & 64.4      & 4\\
                              & 62.4      & 5\\
            \bottomrule
            \end{tabular}
            \caption{}
            \label{tab:ablation_embeddings}
        }
        \end{subtable}  
        \hfill
        \hspace{5mm}
        \begin{subtable}[t]{0.25\linewidth}
        {
        \vspace{-2mm}
        \begin{tabular}[t]{c | c}
            \toprule
            Clip Length (\textit{T}) &  \\
\cmidrule(lr){1-2}
  & 62.2 \\
                   & 64.4\\
                  & 64.7\\
                  & 63.1\\
                \\
                \\
                \\
            \bottomrule
            \end{tabular}
            \caption{}
            \label{tab:ablation_temporal}
        }
        \end{subtable}\hfill
        \begin{subtable}[t]{0.36\linewidth}{
\begin{tabular}[t]{lr}{\scriptsize\textbf{(c)}}&\begin{subtable}[t]{\linewidth}{\vspace{-2mm}\begin{tabular}[t]{P{2.5cm}|P{1.2cm}}
                            \toprule
                              Training Data &  \\
                            \cmidrule(lr){1-2}
                                            Images  & 57.1\\
                                            Video   & 60.7\\
                                            Images + Video  & 64.4\\
                            \bottomrule
                    \end{tabular}
\phantomsubcaption{}
                \label{tab:ablation_data}
            }
            \end{subtable}\\
            &\
    L_\text{total} = L_\text{emb} + L_\text{smooth} + L_\text{center},

    L_\text{smooth}^j = \frac{1}{|\boldsymbol{\mathcal{V}}_j |} ~ \sum\limits_{\boldsymbol{v} \in \boldsymbol{\mathcal{V}}_j} \left( \boldsymbol{v} - \boldsymbol{\bar{v}} \right)^2,

    L_\text{emb}(F) = \bar{\Delta_J}(h(F)),

where  is the Lov\`{a}sz extension of , and  is the hinge loss associated with a binary prediction.
Here we provide only a high-level description of the loss function. For a more detailed explanation of this loss we refer the reader to \cite{Berman18CVPR}.


\section{Implementation Details}
\label{sec:training_setup}

\PAR{Hardware:} We train our network using a batch size of 2 on a workstation with 2 Nvidia RTX TITAN GPUs and 64GB RAM. All inference experiments were performed on a workstation with a single Nvidia GTX 1080Ti GPU and 32GB RAM. 

\PAR{Training Schedule:} For all tasks, the network is trained using an SGD optimizer with an initial learning rate of . The learning rate is initially constant and then starts to decay exponentially after a certain number of iterations up to . The exact number of iterations varies for each setting as follows:

\begin{itemize}
    \item DAVIS'19 Unsupervised: 60k total iterations, decay begins after 20k iterations.
    \item YouTube-VIS: 150k total iterations, decay begins after 60k iterations.
    \item KITTI-MOTS: 100k total iterations, decay begins after 40k iterations.
\end{itemize}

\PAR{Image Augmented Sequences:} As mentioned in~\secTraining, we train our network on clips from actual video data in addition to sequences that have been synthesized from static images using random affine transformations and motion blur. Doing so allows us to utilize a large amount of publicly available image instance segmentation data (\eg, COCO~\cite{Lin14ECCV}, Pascal-VOC~\cite{Everingham10IJCV}, Mapillary Vistas~\cite{Neuhold17ICCV}) for training purposes. We experimentally verified the performance benefit of incorporating such data in~\secAblations. 

These augmentations were applied using the  \texttt{imgaug}~\footnote{https://github.com/aleju/imgaug} library, which, in addition to various transformations, also provides a built-in function that simulates image blur caused by camera motion. The affine transformations we apply consist of rotations in the range , translations of up to  of the image dimension along each axis, and scale variations in the range . We also apply small random offsets to the hue and saturation values of each image. All random transformations are independent of one another, \ie, we do not try to simulate consistent motion by sequentially applying the same transformation multiple times.

\PAR{Video Data Augmentation:} For training clips sampled from actual video data, random horizontal flipping is the only augmentation used. This is applied randomly to entire clips and not to individual frames within clips. 

\section{Baselines for DAVIS'19 Unsupervised}
\label{sec:davis19_baselines}

In~\secComparisonToSOTA we compared our method to two simple proposal-based baselines: optical flow tracker (\texttt{OF-Tracker}) and Re-ID tracker (\texttt{RI-Tracker}), in addition to other published methods on DAVIS'19 unsupervised benchmark. 
For both, we generate per-frame mask proposals  for all the objects in a video using a ResNet-101 based Mask R-CNN~\cite{He17ICCV}. 
To ensure a fair comparison with our approach, we train the Mask R-CNN jointly on YouTube-VIS~\cite{Yang19ICCV}, DAVIS'19~\cite{Caelles19arXiv}, and augmented images from COCO~\cite{Lin14ECCV}, as well as Pascal-VOC~\cite{Everingham10IJCV} dataset for 120k iterations. This network is initialized with weights from a model trained for image instance segmentation on COCO. We use SGD with a momentum of 0.9 and an initial learning rate of  with exponential decay.
The mask proposals generated by this re-trained Mask R-CNN network are then linked over time using optical flow and re-id for \texttt{OF-Tracker} and \texttt{RI-Tracker}, respectively.

\PAR{\texttt{OF-Tracker}:}
We use PWC-Net~\cite{Sun18CVPR} to generate optical flow for each subsequent pair of frames in the DAVIS'19 validation set. The optical flow is then used to warp  onto  for each frame pair  to generate a set of warped masks per-frame  for a video sequence. A simple linear assignment based on object overlap between the warped frame  and the proposal  is then used to associate the objects in the adjacent video frames. 
The associated object IDs are further propagated forward throughout the video sequence. 

\PAR{\texttt{RI-Tracker}:}
For the \texttt{RI-Tracker}, we train a re-id network with a ResNet-50~\cite{He16CVPR} backbone on the DAVIS'19 \cite{Caelles19arXiv} training set. The network is trained using a batch hard triplet loss~\cite{Hermans17ARXIV} on randomly selected triplets from a random video sequence for 25k iterations. This network is then used to generate re-id vectors for all the object proposals in , which are further associated over time using linear assignment based on the Euclidean distance between embedding vectors.


\section{Extended Ablations for Embedding Mixing Function} \label{extended_results_ytvis_mots}

In~\secAblations, we ablated the impact of using different mixing functions  that modify the embedding representation as discussed in ~\secEmbeddingRepresentation. In Tab.~1(a) of the main text, we reported the results of this ablation on the DAVIS'19 Unsupervised validation set. Here, we provide extended results of applying different  on the YouTube-VIS~\cite{Yang19ICCV} and KITTI-MOTS~\cite{Voigtlaender19CVPR} datasets in Tab.~\ref{tab:ablation_supplementary}. The results for the DAVIS'19 Unsupervised validation set have also been repeated for reference.
\begin{table}[t]
    \centering
    \footnotesize
    \vspace{-2mm}
                \begin{tabular}[t]{p{2.5cm} | c | c | c | c | c}
\toprule
            \multirow{2}{2.5cm}{{\footnotesize Mixing Function}} & \multirow{2}{0.5cm}{~~~~} & ~~DAVIS~~  & ~~YT-VIS~~ & \multicolumn{2}{c}{KITTI MOTS} \\
            \cmidrule(lr){3-6}
              && \& &  & sMOTSA (\textit{car}) & sMOTSA (\textit{pedestrian}) \\
            \cmidrule(lr){1-6}
                               & 2  & 61.6 & 30.5 & 64.2 & 41.1 \\
                              & 3   & 62.6  & 31.8 & 72.5 & \textbf{48.9} \\
                              & 3   & 62.8  & 32.6 & 71.8 & 42.2 \\
                              & 4  & 64.2  & 32.4 & 71.9 & 43.6 \\
                              & 4   & \textbf{64.4} & \textbf{34.6} & 73.2 & 47.3 \\
                             & 5 & 62.4   & 34.0 & \textbf{73.4} & 41.5 \\
            \bottomrule
            \end{tabular}
            \label{tab:ablation_embeddings_supplementary}
        \caption{Ablation studies on the Impact of different embedding mixing functions on DAVIS '19, YouTube-VIS (YT-VIS) and KITTI MOTS.}
        \label{tab:ablation_supplementary}
\end{table} 
It can be seen that both DAVIS'19 and YouTube-VIS give consistent results: for the same number of total embedding dimensions (), having a free dimension is more beneficial than having a temporal coordinate dimension. For KITTI-MOTS, however, the trend differs. In particular, we obtain similar performance with  ( and  sMOTSA on the \textit{car} and \textit{pedestrian} class, respectively) and  ( and  sMOTSA). In Tab.~4 (main text), we reported the results for  since the mean sMOTSA score for the two categories () is slightly better than that of  (). We attribute this difference in part to the fact that the temporal coordinate is a more useful feature for instance separation in KITTI-MOTS than in DAVIS'19 due to the fact that object instances undergo faster motion and often enter/exit the scene mid-way through a video clip. Furthermore, the performance trends for the \textit{car} and \textit{pedestrian} classes seem to follow different patterns, \eg, while  yields the highest sMOTSA for the \textit{car} class (), it is significantly lower for the \textit{pedestrian} class. 

\section{UnOVOST Training on KITTI-MOTS}

In~\secComparisonToSOTA, we reported the performance of UnOVOST~\cite{Zulfikar19CVPRW}, the highest-scoring workshop submission for the DAVIS'19 Unsupervised Challenge~\cite{Caelles19arXiv}, for the task of Multi-object Tracking and Segmentation (MOTS) using the KITTI-MOTS dataset~\cite{Voigtlaender19CVPR}.
We obtained the implementation from the authors~\cite{Zulfikar19CVPRW} and re-trained and tuned the model as follows:

\begin{itemize}
    \item We initialized a Mask R-CNN~\cite{He17ICCV} network with a ResNet-101~\cite{Szegedy17AAAI} backbone with weights from an off-the-shelf model trained for instance segmentation on the COCO dataset~\cite{Lin14ECCV}. We then altered the output layers to predict two categories, \ie. \textit{car} and \textit{pedestrian}, and trained the network for 60k iterations on Mapillary Vistas~\cite{Neuhold17ICCV} and KITTI-MOTS datasets. 
    The training data and the backbone is thus identical to the one used for our STEm-Seg network.
    
    \item We trained a ReID network on image instance crops from KITTI-MOTS using a triplet loss~\cite{Schroff15CVPR} and \textit{batch-hard sampling}~\cite{Hermans17ARXIV}.
\end{itemize}
The two most important hyper-parameters in UnOVOST are the IoU thresholds used for pruning object detections and for associating object detections based on optical flow, respectively. We performed a grid search for these two parameters on the KITTI-MOTS validation set in order to optimize the sMOTSA score. Our observation was that the UnOVOST framework is fairly insensitive to these parameters; however, the final scores on KITTI-MOTS are consistently low (see Tab. 4 in the paper). 
Qualitative analysis of the results showed that the ReID network frequently makes spurious associations. 
We postulate that this is because object instances in KITTI-MOTS frequently have similar appearances. This differs from the object instances in DAVIS whose appearances usually differ since they span a large variety of object classes. 



\section{Adaptation of TrackR-CNN to YouTube-VIS}
\label{sec:ytvis}

As discussed in \secComparisonToSOTA, we adapted the publicly available implementation~\footnote{https://github.com/VisualComputingInstitute/TrackR-CNN} of TrackR-CNN~\cite{Voigtlaender19CVPR} to the task of Video Instance Segmentation and evaluated it on the Youtube-VIS dataset~\cite{Yang19ICCV}. 
To this end, we initialized the parameters of the network, which overlap with Mask R-CNN~\cite{He17ICCV} with weights from a model trained for instance segmentation on COCO~\cite{Lin14ECCV} and Mapillary Vistas~\cite{Neuhold17ICCV}.

In the original implementation, a class-specific re-identification embedding head was used. This was feasible for KITTI-MOTS, where there are only two object classes. 
In YouTube-VIS, however, there are 40 object classes, and several occur infrequently in the dataset. Furthermore, video sequences are significantly shorter, and there are usually only 1--2 objects of the same class present in a video clip. For that reason, we adapted the TrackR-CNN architecture and kept a single ReID head that is shared among all object classes. We trained the network under this setting using a batch size of 8 images for 400k iterations and evaluated multiple intermediate checkpoints. Despite these efforts, the highest  score obtained was less than . 

A major performance bottleneck we identified is a low-resolution 14x14 RoI-Align~\cite{He17ICCV} layer used in TrackR-CNN that limit the memory usage to a reasonable level. This suffices for KITTI-MOTS, which contains small pedestrian instances and cars with simple shapes, but results in very coarse segmentation masks on the YouTube-VIS dataset which contains a diverse set of objects that cover a large area of the image. The  measure heavily penalizes such coarse segmentation as it is computed by taking the average over a set of IoU thresholds ranging from 0.5 to 0.95.



\section{Additional Qualitative Results}
\label{sec:qualitative}

In this section, we provide additional qualitative results on the validation split of all three datasets, DAVIS'19 \cite{Caelles19arXiv} in Fig. \ref{fig:qualitative_davis}, YouTube-VIS \cite{Yang19ICCV} in Fig. \ref{fig:qualitative_ytvis} and KITTI-MOTS \cite{Voigtlaender19CVPR} in Fig. \ref{fig:qualitative_mots}. As can be seen, our method can reliably segment and track a large variety of objects in diverse scenarios, and is fairly robust to scale changes and brief occlusions. 


\clearpage{}\begin{figure}[t]
\centering
  \includegraphics[width=0.23\textwidth, frame]{figures/supplementary/davis/pigs/00003.png}\hspace{1px}\includegraphics[width=0.23\textwidth, frame]{figures/supplementary/davis/pigs/00019.png}\hspace{1px}\includegraphics[width=0.23\textwidth, frame]{figures/supplementary/davis/pigs/00035.png}\hspace{1px}\includegraphics[width=0.23\textwidth, frame]{figures/supplementary/davis/pigs/00049.png}\\
  \includegraphics[width=0.23\textwidth, frame]{figures/supplementary/davis/camel/00001.png}\hspace{1px}\includegraphics[width=0.23\textwidth, frame]{figures/supplementary/davis/camel/00030.png}\hspace{1px}\includegraphics[width=0.23\textwidth, frame]{figures/supplementary/davis/camel/00051.png}\hspace{1px}\includegraphics[width=0.23\textwidth, frame]{figures/supplementary/davis/camel/00089.png}\\
  \includegraphics[width=0.23\textwidth, frame]{figures/supplementary/davis/dance-twirl/00000.png}\hspace{1px}\includegraphics[width=0.23\textwidth, frame]{figures/supplementary/davis/dance-twirl/00001.png}\hspace{1px}\includegraphics[width=0.23\textwidth, frame]{figures/supplementary/davis/dance-twirl/00002.png}\hspace{1px}\includegraphics[width=0.23\textwidth, frame]{figures/supplementary/davis/dance-twirl/00003.png}\\
  \includegraphics[width=0.23\textwidth, frame]{figures/supplementary/davis/horsejump-high/00003.png}\hspace{1px}\includegraphics[width=0.23\textwidth, frame]{figures/supplementary/davis/horsejump-high/00013.png}\hspace{1px}\includegraphics[width=0.23\textwidth, frame]{figures/supplementary/davis/horsejump-high/00024.png}\hspace{1px}\includegraphics[width=0.23\textwidth, frame]{figures/supplementary/davis/horsejump-high/00035.png}\\
  \includegraphics[width=0.23\textwidth, frame]{figures/supplementary/davis/motocross-jump/00006.png}\hspace{1px}\includegraphics[width=0.23\textwidth, frame]{figures/supplementary/davis/motocross-jump/00011.png}\hspace{1px}\includegraphics[width=0.23\textwidth, frame]{figures/supplementary/davis/motocross-jump/00028.png}\hspace{1px}\includegraphics[width=0.23\textwidth, frame]{figures/supplementary/davis/motocross-jump/00036.png}\\
  \includegraphics[width=0.23\textwidth, frame]{figures/supplementary/davis/goat/00000.png}\hspace{1px}\includegraphics[width=0.23\textwidth, frame]{figures/supplementary/davis/goat/00001.png}\hspace{1px}\includegraphics[width=0.23\textwidth, frame]{figures/supplementary/davis/goat/00002.png}\hspace{1px}\includegraphics[width=0.23\textwidth, frame]{figures/supplementary/davis/goat/00003.png}\\
  \includegraphics[width=0.23\textwidth, frame]{figures/supplementary/davis/soapbox/00000.png}\hspace{1px}\includegraphics[width=0.23\textwidth, frame]{figures/supplementary/davis/soapbox/00001.png}\hspace{1px}\includegraphics[width=0.23\textwidth, frame]{figures/supplementary/davis/soapbox/00002.png}\hspace{1px}\includegraphics[width=0.23\textwidth, frame]{figures/supplementary/davis/soapbox/00003.png}\\
  \includegraphics[width=0.23\textwidth, frame]{figures/supplementary/davis/bike-packing/00003.png}\hspace{1px}\includegraphics[width=0.23\textwidth, frame]{figures/supplementary/davis/bike-packing/00018.png}\hspace{1px}\includegraphics[width=0.23\textwidth, frame]{figures/supplementary/davis/bike-packing/00025.png}\hspace{1px}\includegraphics[width=0.23\textwidth, frame]{figures/supplementary/davis/bike-packing/00047.png}\\
\raggedleft
\begin{tikzpicture}[node distance=2cm]
\node (A) at (2.75, 0) {};
\node (B) at (13.0, 0) {};
\draw[-{Stealth}, to path={-- (\tikztotarget)}](A) edge (B);
\node[text width=1cm] at (13.5,0){time};
\end{tikzpicture}
\vspace{-2px}
  \caption{\textbf{Additional qualitative results on DAVIS'19}. STEm-Seg generates consistently good results under varied scenarios. E.g., in the \texttt{motocross-jump} sequence (\textit{fifth row}) it demonstrates robustness to a large change in scale. In the \texttt{bike-packing} sequence (\textit{bottom row})}, it is robust to sudden pose changes.
    \label{fig:qualitative_davis}
\end{figure}\clearpage{}

\clearpage{}\begin{figure}[t]
\centering
  \includegraphics[width=0.23\textwidth, trim={0 0 0 0}, clip, frame]{figures/supplementary/ytvis/139_skateboard_2/0.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth, trim={0 0 0 0}, clip,frame]{figures/supplementary/ytvis/139_skateboard_2/9.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth, trim={0 0 0 0}, clip,frame]{figures/supplementary/ytvis/139_skateboard_2/16.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth, trim={0 0 0 0}, clip,frame]{figures/supplementary/ytvis/139_skateboard_2/24.jpg}\\
  \includegraphics[width=0.23\textwidth, trim={0 0 0 0}, clip, frame]{figures/supplementary/ytvis/143_cat_lizard/4.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth, trim={0 0 0 0}, clip,frame]{figures/supplementary/ytvis/143_cat_lizard/13.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth, trim={0 0 0 0}, clip,frame]{figures/supplementary/ytvis/143_cat_lizard/17.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth, trim={0 0 0 0}, clip,frame]{figures/supplementary/ytvis/143_cat_lizard/25.jpg}\\
  \includegraphics[width=0.23\textwidth, trim={100 100 0 0}, clip, frame]{figures/supplementary/ytvis/211_surfing_3/0.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth, trim={100 100 0 0}, clip,frame]{figures/supplementary/ytvis/211_surfing_3/9.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth, trim={100 100 0 0}, clip,frame]{figures/supplementary/ytvis/211_surfing_3/14.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth, trim={100 100 0 0}, clip,frame]{figures/supplementary/ytvis/211_surfing_3/26.jpg}\\
  \includegraphics[width=0.23\textwidth, trim={0 0 0 0}, clip, frame]{figures/supplementary/ytvis/30_dog_child/1.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth, trim={0 0 0 0}, clip,frame]{figures/supplementary/ytvis/30_dog_child/5.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth, trim={0 0 0 0}, clip,frame]{figures/supplementary/ytvis/30_dog_child/13.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth, trim={0 0 0 0}, clip,frame]{figures/supplementary/ytvis/30_dog_child/19.jpg}\\
  \includegraphics[width=0.23\textwidth, trim={0 0 0 0}, clip, frame]{figures/supplementary/ytvis/40_zebras/0.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth, trim={0 0 0 0}, clip,frame]{figures/supplementary/ytvis/40_zebras/3.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth, trim={0 0 0 0}, clip,frame]{figures/supplementary/ytvis/40_zebras/7.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth, trim={0 0 0 0}, clip,frame]{figures/supplementary/ytvis/40_zebras/14.jpg}\\
  \includegraphics[width=0.23\textwidth, trim={0 0 0 0}, clip, frame]{figures/supplementary/ytvis/85_lizard_foot/5.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth, trim={0 0 0 0}, clip,frame]{figures/supplementary/ytvis/85_lizard_foot/9.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth, trim={0 0 0 0}, clip,frame]{figures/supplementary/ytvis/85_lizard_foot/24.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth, trim={0 0 0 0}, clip,frame]{figures/supplementary/ytvis/85_lizard_foot/35.jpg}\\
  \includegraphics[width=0.23\textwidth, trim={0 0 0 0}, clip, frame]{figures/supplementary/ytvis/223_bull_fight/1.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth, trim={0 0 0 0}, clip,frame]{figures/supplementary/ytvis/223_bull_fight/5.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth, trim={0 0 0 0}, clip,frame]{figures/supplementary/ytvis/223_bull_fight/11.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth, trim={0 0 0 0}, clip,frame]{figures/supplementary/ytvis/223_bull_fight/13.jpg}\\
  \includegraphics[width=0.23\textwidth, trim={0 0 0 0}, clip, frame]{figures/supplementary/ytvis/133_cat_hand/1.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth, trim={0 0 0 0}, clip,frame]{figures/supplementary/ytvis/133_cat_hand/4.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth, trim={0 0 0 0}, clip,frame]{figures/supplementary/ytvis/133_cat_hand/10.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth, trim={0 0 0 0}, clip,frame]{figures/supplementary/ytvis/133_cat_hand/14.jpg}\\
  \includegraphics[width=0.23\textwidth, trim={0 0 0 0}, clip, frame]{figures/supplementary/ytvis/161_tigers/0.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth, trim={0 0 0 0}, clip,frame]{figures/supplementary/ytvis/161_tigers/8.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth, trim={0 0 0 0}, clip,frame]{figures/supplementary/ytvis/161_tigers/21.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth, trim={0 0 0 0}, clip,frame]{figures/supplementary/ytvis/161_tigers/35.jpg}\\
  \includegraphics[width=0.23\textwidth, trim={0 0 0 0}, clip, frame]{figures/supplementary/ytvis/236_panda/0.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth, trim={0 0 0 0}, clip,frame]{figures/supplementary/ytvis/236_panda/5.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth, trim={0 0 0 0}, clip,frame]{figures/supplementary/ytvis/236_panda/8.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth, trim={0 0 0 0}, clip,frame]{figures/supplementary/ytvis/236_panda/12.jpg}\\
\raggedleft
\begin{tikzpicture}[node distance=2cm]
\node (A) at (2.75, 0) {};
\node (B) at (13.0, 0) {};
\draw[-{Stealth}, to path={-- (\tikztotarget)}](A) edge (B);
\node[text width=1cm] at (13.5,0){time};
\end{tikzpicture}
\vspace{-2px}
  \caption{\textbf{Additional qualitative results on YouTube-VIS (YT-VIS)}~\cite{Yang19ICCV}. Most of the semantically challenging animal categories are successfully segmented by STEm-Seg. It also captures some fine object details such as the skateboard (\textit{top row}) and the surfboard (\textit{third row}) well. }
   \label{fig:qualitative_ytvis}
\end{figure}
\clearpage


\clearpage{}

\clearpage{}\begin{figure}[t]
\centering

  \includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq2/5_25.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq2/10_47.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq2/13_59.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq2/15_66.jpg}\\ \includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq2/22_95.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq2/24_103.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq2/27_114.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq2/32_132.jpg}\\ \includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq6/10_47.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq6/12_53.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq6/13_58.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq6/15_66.jpg}\\
  \includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq6/18_76.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq6/20_82.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq6/21_89.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq6/25_107.jpg}\\
  \includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq6/32_131.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq6/32_135.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq6/34_140.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq6/37_152.jpg}\\
  \includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq7/11_49.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq7/15_64.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq7/16_71.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq7/17_75.jpg}\\ \includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq7/37_154.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq7/39_158.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq7/40_164.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq7/41_168.jpg}\\\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq7/51_207.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq7/52_214.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq7/54_223.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq7/58_236.jpg}\\\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq7/94_382.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq7/99_399.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq7/102_414.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq7/103_416.jpg}\\\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq7/112_452.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq7/113_457.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq7/114_463.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq7/119_477.jpg}\\\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq16/1_10.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq16/1_7.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq16/8_35.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq16/11_49.jpg}\\\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq8/24_102.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq8/62_253.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq8/64_261.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq8/85_340.jpg}\\
  \includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq13/3_17.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq13/4_21.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq13/6_27.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq13/8_39.jpg}\\\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq13/55_223.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq13/55_227.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq13/56_231.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq13/57_233.jpg}\\\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq13/60_246.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq13/61_247.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq13/62_253.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq13/64_260.jpg}\\\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq13/68_278.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq13/69_280.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq13/69_281.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq13/69_283.jpg}\\\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq13/70_287.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq13/71_287.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq13/74_298.jpg}\hspace{1px}\includegraphics[width=0.23\textwidth,frame]{figures/supplementary/kittimots/seq13/75_304.jpg}\\

\raggedleft
\begin{tikzpicture}[node distance=2cm]
\node (A) at (2.75, 0) {};
\node (B) at (13.0, 0) {};
\draw[-{Stealth}, to path={-- (\tikztotarget)}](A) edge (B);
\node[text width=1cm] at (13.5,0){time};
\end{tikzpicture}
\vspace{-2px}
  \caption{\textbf{Additional qualitative results on KITTI-MOTS.} Our method successfully tracks and segments cars and pedestrians in automotive scenarios, even when observed from a large distance (\textit{sixth row from the bottom}) and bridges occlusions (\textit{fifth row}).}
  
  \label{fig:qualitative_mots}
\end{figure}\clearpage{}

 \end{document}
