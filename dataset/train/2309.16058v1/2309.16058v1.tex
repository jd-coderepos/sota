\begin{thebibliography}{10}

\bibitem{tsimpoukelli2021multimodal}
M.~Tsimpoukelli, J.~L. Menick, S.~Cabi, S.~Eslami, O.~Vinyals, and F.~Hill,
  ``Multimodal few-shot learning with frozen language models,'' {\em Advances
  in Neural Information Processing Systems}, vol.~34, pp.~200--212, 2021.

\bibitem{flamingo}
J.-B. Alayrac, J.~Donahue, P.~Luc, A.~Miech, I.~Barr, Y.~Hasson, K.~Lenc,
  A.~Mensch, K.~Millican, M.~Reynolds, {\em et~al.}, ``Flamingo: a visual
  language model for few-shot learning,'' {\em Advances in Neural Information
  Processing Systems}, vol.~35, pp.~23716--23736, 2022.

\bibitem{blip2}
J.~Li, D.~Li, S.~Savarese, and S.~Hoi, ``Blip-2: Bootstrapping language-image
  pre-training with frozen image encoders and large language models,'' {\em
  arXiv preprint arXiv:2301.12597}, 2023.

\bibitem{OpenAI2023GPT4TR}
OpenAI, ``Gpt-4 technical report,'' {\em ArXiv}, vol.~abs/2303.08774, 2023.

\bibitem{idefics}
H.~Lauren√ßon, L.~Saulnier, L.~Tronchon, S.~Bekman, A.~Singh, A.~Lozhkov,
  T.~Wang, S.~Karamcheti, A.~M. Rush, D.~Kiela, M.~Cord, and V.~Sanh,
  ``Obelics: An open web-scale filtered dataset of interleaved image-text
  documents,'' 2023.

\bibitem{llama2}
H.~Touvron, L.~Martin, K.~Stone, P.~Albert, A.~Almahairi, Y.~Babaei,
  N.~Bashlykov, S.~Batra, P.~Bhargava, S.~Bhosale, {\em et~al.}, ``Llama 2:
  Open foundation and fine-tuned chat models,'' {\em arXiv preprint
  arXiv:2307.09288}, 2023.

\bibitem{GPT2}
A.~Radford, J.~Wu, R.~Child, D.~Luan, D.~Amodei, I.~Sutskever, {\em et~al.},
  ``Language models are unsupervised multitask learners,'' 2019.

\bibitem{flant5}
H.~W. Chung, L.~Hou, S.~Longpre, B.~Zoph, Y.~Tay, W.~Fedus, E.~Li, X.~Wang,
  M.~Dehghani, S.~Brahma, A.~Webson, S.~S. Gu, Z.~Dai, M.~Suzgun, X.~Chen,
  A.~Chowdhery, S.~Narang, G.~Mishra, A.~Yu, V.~Zhao, Y.~Huang, A.~Dai, H.~Yu,
  S.~Petrov, E.~H. Chi, J.~Dean, J.~Devlin, A.~Roberts, D.~Zhou, Q.~V. Le, and
  J.~Wei, ``Scaling instruction-finetuned language models,'' 2022.

\bibitem{gpt-j}
B.~Wang and A.~Komatsuzaki, ``{GPT-J-6B: A 6 Billion Parameter Autoregressive
  Language Model}.'' \url{https://github.com/kingoflolz/mesh-transformer-jax},
  May 2021.

\bibitem{opt}
S.~Zhang, S.~Roller, N.~Goyal, M.~Artetxe, M.~Chen, S.~Chen, C.~Dewan, M.~Diab,
  X.~Li, X.~V. Lin, {\em et~al.}, ``Opt: Open pre-trained transformer language
  models,'' {\em arXiv preprint arXiv:2205.01068}, 2022.

\bibitem{llama}
H.~Touvron, T.~Lavril, G.~Izacard, X.~Martinet, M.-A. Lachaux, T.~Lacroix,
  B.~Rozi{\`e}re, N.~Goyal, E.~Hambro, F.~Azhar, {\em et~al.}, ``Llama: Open
  and efficient foundation language models,'' {\em arXiv preprint
  arXiv:2302.13971}, 2023.

\bibitem{vicuna}
W.-L. Chiang, Z.~Li, Z.~Lin, Y.~Sheng, Z.~Wu, H.~Zhang, L.~Zheng, S.~Zhuang,
  Y.~Zhuang, J.~E. Gonzalez, I.~Stoica, and E.~P. Xing, ``Vicuna: An
  open-source chatbot impressing gpt-4 with 90\%* chatgpt quality,'' March
  2023.

\bibitem{xu2015show}
K.~Xu, J.~Ba, R.~Kiros, K.~Cho, A.~Courville, R.~Salakhudinov, R.~Zemel, and
  Y.~Bengio, ``Show, attend and tell: Neural image caption generation with
  visual attention,'' in {\em International conference on machine learning},
  pp.~2048--2057, PMLR, 2015.

\bibitem{vqa}
S.~Antol, A.~Agrawal, J.~Lu, M.~Mitchell, D.~Batra, C.~Lawrence~Zitnick, and
  D.~Parikh, ``{VQA}: Visual question answering,'' in {\em ICCV}, 2015.

\bibitem{visdial}
A.~Das, S.~Kottur, K.~Gupta, A.~Singh, D.~Yadav, J.~M. Moura, D.~Parikh, and
  D.~Batra, ``Visual dialog,'' in {\em CVPR}, 2017.

\bibitem{vision-language-navigation}
P.~Anderson, Q.~Wu, D.~Teney, J.~Bruce, M.~Johnson, N.~S{\"u}nderhauf, I.~Reid,
  S.~Gould, and A.~van~den Hengel, ``Vision-and-language navigation:
  Interpreting visually-grounded navigation instructions in real
  environments,'' in {\em Proceedings of the IEEE Conference on Computer Vision
  and Pattern Recognition}, pp.~3674--3683, 2018.

\bibitem{openflamingo}
A.~Awadalla, I.~Gao, J.~Gardner, J.~Hessel, Y.~Hanafy, W.~Zhu, K.~Marathe,
  Y.~Bitton, S.~Gadre, J.~Jitsev, S.~Kornblith, P.~W. Koh, G.~Ilharco,
  M.~Wortsman, and L.~Schmidt, ``Openflamingo,'' Mar. 2023.

\bibitem{palme}
D.~Driess, F.~Xia, M.~S.~M. Sajjadi, C.~Lynch, A.~Chowdhery, B.~Ichter,
  A.~Wahid, J.~Tompson, Q.~Vuong, T.~Yu, W.~Huang, Y.~Chebotar, P.~Sermanet,
  D.~Duckworth, S.~Levine, V.~Vanhoucke, K.~Hausman, M.~Toussaint, K.~Greff,
  A.~Zeng, I.~Mordatch, and P.~Florence, ``Palm-e: An embodied multimodal
  language model,'' in {\em arXiv preprint arXiv:2303.03378}, 2023.

\bibitem{instructblip}
W.~Dai, J.~Li, D.~Li, A.~M.~H. Tiong, J.~Zhao, W.~Wang, B.~Li, P.~Fung, and
  S.~Hoi, ``Instructblip: Towards general-purpose vision-language models with
  instruction tuning,'' 2023.

\bibitem{llava}
H.~Liu, C.~Li, Q.~Wu, and Y.~J. Lee, ``Visual instruction tuning,'' 2023.

\bibitem{minigpt4}
D.~Zhu, J.~Chen, X.~Shen, X.~Li, and M.~Elhoseiny, ``Minigpt-4: Enhancing
  vision-language understanding with advanced large language models,'' {\em
  arXiv preprint arXiv:2304.10592}, 2023.

\bibitem{otter}
B.~Li, Y.~Zhang, L.~Chen, J.~Wang, J.~Yang, and Z.~Liu, ``Otter: A multi-modal
  model with in-context instruction tuning,'' {\em arXiv preprint
  arXiv:2305.03726}, 2023.

\bibitem{mplug}
Q.~Ye, H.~Xu, G.~Xu, J.~Ye, M.~Yan, Y.~Zhou, J.~Wang, A.~Hu, P.~Shi, Y.~Shi,
  {\em et~al.}, ``mplug-owl: Modularization empowers large language models with
  multimodality,'' {\em arXiv preprint arXiv:2304.14178}, 2023.

\bibitem{multimodalGPT}
T.~Gong, C.~Lyu, S.~Zhang, Y.~Wang, M.~Zheng, Q.~Zhao, K.~Liu, W.~Zhang,
  P.~Luo, and K.~Chen, ``Multimodal-gpt: A vision and language model for
  dialogue with humans,'' {\em arXiv preprint arXiv:2305.04790}, 2023.

\bibitem{lamaadapterv2}
P.~Gao, J.~Han, R.~Zhang, Z.~Lin, S.~Geng, A.~Zhou, W.~Zhang, P.~Lu, C.~He,
  X.~Yue, H.~Li, and Y.~Qiao, ``Llama-adapter v2: Parameter-efficient visual
  instruction model,'' {\em arXiv preprint arXiv:2304.15010}, 2023.

\bibitem{zhang2023video}
H.~Zhang, X.~Li, and L.~Bing, ``Video-llama: An instruction-tuned audio-visual
  language model for video understanding,'' {\em arXiv preprint
  arXiv:2306.02858}, 2023.

\bibitem{su2023pandagpt}
Y.~Su, T.~Lan, H.~Li, J.~Xu, Y.~Wang, and D.~Cai, ``Pandagpt: One model to
  instruction-follow them all,'' {\em arXiv preprint arXiv:2305.16355}, 2023.

\bibitem{lyu2023macaw}
C.~Lyu, M.~Wu, L.~Wang, X.~Huang, B.~Liu, Z.~Du, S.~Shi, and Z.~Tu,
  ``Macaw-llm: Multi-modal language modeling with image, audio, video, and text
  integration,'' {\em arXiv preprint arXiv:2306.09093}, 2023.

\bibitem{m3it}
L.~Li, Y.~Yin, S.~Li, L.~Chen, P.~Wang, S.~Ren, M.~Li, Y.~Yang, J.~Xu, X.~Sun,
  {\em et~al.}, ``Mit: A large-scale dataset towards multi-modal multilingual
  instruction tuning,'' {\em arXiv preprint arXiv:2306.04387}, 2023.

\bibitem{clip}
A.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, G.~Goh, S.~Agarwal, G.~Sastry,
  A.~Askell, P.~Mishkin, J.~Clark, {\em et~al.}, ``Learning transferable visual
  models from natural language supervision,'' in {\em International Conference
  on Machine Learning (ICML)}, 2021.

\bibitem{laion}
C.~Schuhmann, R.~Beaumont, R.~Vencu, C.~Gordon, R.~Wightman, M.~Cherti,
  T.~Coombes, A.~Katta, C.~Mullis, M.~Wortsman, {\em et~al.}, ``Laion-5b: An
  open large-scale dataset for training next generation image-text models,''
  {\em Advances in Neural Information Processing Systems}, vol.~35,
  pp.~25278--25294, 2022.

\bibitem{clap}
Y.~Wu*, K.~Chen*, T.~Zhang*, Y.~Hui*, T.~Berg-Kirkpatrick, and S.~Dubnov,
  ``Large-scale contrastive language-audio pretraining with feature fusion and
  keyword-to-caption augmentation,'' in {\em IEEE International Conference on
  Acoustics, Speech and Signal Processing, ICASSP}, 2023.

\bibitem{imu2clip}
S.~Moon, A.~Madotto, Z.~Lin, A.~Dirafzoon, A.~Saraf, A.~Bearman, and
  B.~Damavandi, ``Imu2clip: Multimodal contrastive learning for imu motion
  sensors from egocentric videos and text,'' {\em arXiv preprint
  arXiv:2210.14395}, 2022.

\bibitem{laionfiltering}
F.~Radenovic, A.~Dubey, A.~Kadian, T.~Mihaylov, S.~Vandenhende, Y.~Patel,
  Y.~Wen, V.~Ramanathan, and D.~Mahajan, ``Filtering, distillation, and hard
  negatives for vision-language pre-training,'' in {\em Proceedings of the
  IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pp.~6967--6977, 2023.

\bibitem{audioset}
J.~F. Gemmeke, D.~P.~W. Ellis, D.~Freedman, A.~Jansen, W.~Lawrence, R.~C.
  Moore, M.~Plakal, and M.~Ritter, ``Audio set: An ontology and human-labeled
  dataset for audio events,'' in {\em Proc. IEEE ICASSP 2017}, (New Orleans,
  LA), 2017.

\bibitem{kim2019audiocaps}
C.~D. Kim, B.~Kim, H.~Lee, and G.~Kim, ``Audiocaps: Generating captions for
  audios in the wild,'' in {\em Proceedings of the 2019 Conference of the North
  American Chapter of the Association for Computational Linguistics: Human
  Language Technologies, Volume 1 (Long and Short Papers)}, pp.~119--132, 2019.

\bibitem{clotho}
K.~Drossos, S.~Lipping, and T.~Virtanen, ``Clotho: An audio captioning
  dataset,'' in {\em ICASSP 2020-2020 IEEE International Conference on
  Acoustics, Speech and Signal Processing (ICASSP)}, pp.~736--740, IEEE, 2020.

\bibitem{ego4d}
K.~Grauman, A.~Westbury, E.~Byrne, Z.~Chavis, A.~Furnari, R.~Girdhar,
  J.~Hamburger, H.~Jiang, M.~Liu, X.~Liu, M.~Martin, T.~Nagarajan,
  I.~Radosavovic, S.~K. Ramakrishnan, F.~Ryan, J.~Sharma, M.~Wray, M.~Xu, E.~Z.
  Xu, C.~Zhao, S.~Bansal, D.~Batra, V.~Cartillier, S.~Crane, T.~Do, M.~Doulaty,
  A.~Erapalli, C.~Feichtenhofer, A.~Fragomeni, Q.~Fu, C.~Fuegen,
  A.~Gebreselasie, C.~Gonzalez, J.~Hillis, X.~Huang, Y.~Huang, W.~Jia, W.~Khoo,
  J.~Kolar, S.~Kottur, A.~Kumar, F.~Landini, C.~Li, Y.~Li, Z.~Li, K.~Mangalam,
  R.~Modhugu, J.~Munro, T.~Murrell, T.~Nishiyasu, W.~Price, P.~R. Puentes,
  M.~Ramazanova, L.~Sari, K.~Somasundaram, A.~Southerland, Y.~Sugano, R.~Tao,
  M.~Vo, Y.~Wang, X.~Wu, T.~Yagi, Y.~Zhu, P.~Arbelaez, D.~Crandall, D.~Damen,
  G.~M. Farinella, B.~Ghanem, V.~K. Ithapu, C.~V. Jawahar, H.~Joo, K.~Kitani,
  H.~Li, R.~Newcombe, A.~Oliva, H.~S. Park, J.~M. Rehg, Y.~Sato, J.~Shi, M.~Z.
  Shou, A.~Torralba, L.~Torresani, M.~Yan, and J.~Malik, ``Ego4d: Around the
  {W}orld in 3,000 {H}ours of {E}gocentric {V}ideo,'' in {\em IEEE/CVF Computer
  Vision and Pattern Recognition (CVPR)}, 2022.

\bibitem{zhao2023pytorch}
Y.~Zhao, A.~Gu, R.~Varma, L.~Luo, C.-C. Huang, M.~Xu, L.~Wright,
  H.~Shojanazeri, M.~Ott, S.~Shleifer, A.~Desmaison, C.~Balioglu, P.~Damania,
  B.~Nguyen, G.~Chauhan, Y.~Hao, A.~Mathews, and S.~Li, ``Pytorch fsdp:
  Experiences on scaling fully sharded data parallel,'' 2023.

\bibitem{qlora}
T.~Dettmers, A.~Pagnoni, A.~Holtzman, and L.~Zettlemoyer, ``Qlora: Efficient
  finetuning of quantized llms,'' {\em arXiv preprint arXiv:2305.14314}, 2023.

\bibitem{lora}
E.~J. Hu, Y.~Shen, P.~Wallis, Z.~Allen-Zhu, Y.~Li, S.~Wang, L.~Wang, and
  W.~Chen, ``Lora: Low-rank adaptation of large language models,'' {\em arXiv
  preprint arXiv:2106.09685}, 2021.

\bibitem{cm3leon}
L.~Yu, B.~Shi, R.~Pasunuru, B.~Miller, O.~Golovneva, T.~Wang, A.~Babu, B.~Tang,
  B.~Karrer, S.~Sheynin, C.~Ross, A.~Polyak, R.~Howes, V.~Sharma, J.~Xu,
  U.~Singer, D.~Li, G.~Ghosh, Y.~Taigman, M.~Fazel-Zarandi, A.~Celikyilmaz,
  L.~Zettlemoyer, and A.~Aghajanyan, ``Scaling autoregressive multi-modal
  models: Pretraining and instruction tuning,'' 2023.

\bibitem{xu2021investigating}
X.~Xu, H.~Dinkel, M.~Wu, Z.~Xie, and K.~Yu, ``Investigating local and global
  information for automated audio captioning with transfer learning,'' in {\em
  ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, pp.~905--909, IEEE, 2021.

\bibitem{mei2021audio}
X.~Mei, X.~Liu, Q.~Huang, M.~D. Plumbley, and W.~Wang, ``Audio captioning
  transformer,'' {\em arXiv preprint arXiv:2107.09817}, 2021.

\bibitem{liu2022leveraging}
X.~Liu, X.~Mei, Q.~Huang, J.~Sun, J.~Zhao, H.~Liu, M.~D. Plumbley, V.~Kilic,
  and W.~Wang, ``Leveraging pre-trained bert for audio captioning,'' in {\em
  2022 30th European Signal Processing Conference (EUSIPCO)}, pp.~1145--1149,
  IEEE, 2022.

\bibitem{wang2022internvideo}
Y.~Wang, K.~Li, Y.~Li, Y.~He, B.~Huang, Z.~Zhao, H.~Zhang, J.~Xu, Y.~Liu,
  Z.~Wang, {\em et~al.}, ``Internvideo: General video foundation models via
  generative and discriminative learning,'' {\em arXiv preprint
  arXiv:2212.03191}, 2022.

\bibitem{li2023blip}
J.~Li, D.~Li, S.~Savarese, and S.~Hoi, ``Blip-2: Bootstrapping language-image
  pre-training with frozen image encoders and large language models,'' {\em
  arXiv preprint arXiv:2301.12597}, 2023.

\bibitem{mscoco}
T.-Y. Lin, M.~Maire, S.~Belongie, J.~Hays, P.~Perona, D.~Ramanan,
  P.~Doll{\'a}r, and C.~L. Zitnick, ``Microsoft coco: Common objects in
  context,'' in {\em ECCV}, 2014.

\bibitem{hatefulmeme}
D.~Kiela, H.~Firooz, A.~Mohan, V.~Goswami, A.~Singh, P.~Ringshia, and
  D.~Testuggine, ``The hateful memes challenge: Detecting hate speech in
  multimodal memes,'' {\em Advances in neural information processing systems},
  vol.~33, pp.~2611--2624, 2020.

\bibitem{TextVQA}
A.~Singh, V.~Natarjan, M.~Shah, Y.~Jiang, X.~Chen, D.~Parikh, and M.~Rohrbach,
  ``Towards vqa models that can read,'' in {\em Proceedings of the IEEE
  Conference on Computer Vision and Pattern Recognition}, pp.~8317--8326, 2019.

\bibitem{scienceqa}
P.~Lu, S.~Mishra, T.~Xia, L.~Qiu, K.-W. Chang, S.-C. Zhu, O.~Tafjord, P.~Clark,
  and A.~Kalyan, ``Learn to explain: Multimodal reasoning via thought chains
  for science question answering,'' {\em Advances in Neural Information
  Processing Systems}, vol.~35, pp.~2507--2521, 2022.

\bibitem{vizwiz}
D.~Gurari, Q.~Li, A.~J. Stangl, A.~Guo, C.~Lin, K.~Grauman, J.~Luo, and J.~P.
  Bigham, ``Vizwiz grand challenge: Answering visual questions from blind
  people,'' in {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.~3608--3617, 2018.

\bibitem{okvqa}
K.~Marino, M.~Rastegari, A.~Farhadi, and R.~Mottaghi, ``Ok-vqa: A visual
  question answering benchmark requiring external knowledge,'' in {\em
  Proceedings of the IEEE/cvf conference on computer vision and pattern
  recognition}, pp.~3195--3204, 2019.

\bibitem{dinov2}
M.~Oquab, T.~Darcet, T.~Moutakanni, H.~V. Vo, M.~Szafraniec, V.~Khalidov,
  P.~Fernandez, D.~Haziza, F.~Massa, A.~El-Nouby, R.~Howes, P.-Y. Huang, H.~Xu,
  V.~Sharma, S.-W. Li, W.~Galuba, M.~Rabbat, M.~Assran, N.~Ballas, G.~Synnaeve,
  I.~Misra, H.~Jegou, J.~Mairal, P.~Labatut, A.~Joulin, and P.~Bojanowski,
  ``Dinov2: Learning robust visual features without supervision,'' 2023.

\bibitem{li2020hero}
L.~Li, Y.-C. Chen, Y.~Cheng, Z.~Gan, L.~Yu, and J.~Liu, ``Hero: Hierarchical
  encoder for video+ language omni-representation pre-training,'' {\em arXiv
  preprint arXiv:2005.00200}, 2020.

\bibitem{wu2021star}
B.~Wu, S.~Yu, Z.~Chen, J.~B. Tenenbaum, and C.~Gan, ``Star: A benchmark for
  situated reasoning in real-world videos,'' in {\em Thirty-fifth Conference on
  Neural Information Processing Systems Datasets and Benchmarks Track (Round
  2)}, 2021.

\bibitem{xiao2021next}
J.~Xiao, X.~Shang, A.~Yao, and T.-S. Chua, ``Next-qa: Next phase of
  question-answering to explaining temporal actions,'' in {\em Proceedings of
  the IEEE/CVF conference on computer vision and pattern recognition},
  pp.~9777--9786, 2021.

\bibitem{yu2023self}
S.~Yu, J.~Cho, P.~Yadav, and M.~Bansal, ``Self-chained image-language model for
  video localization and question answering,'' {\em arXiv preprint
  arXiv:2305.06988}, 2023.

\bibitem{miech2019howto100m}
A.~Miech, D.~Zhukov, J.-B. Alayrac, M.~Tapaswi, I.~Laptev, and J.~Sivic,
  ``Howto100m: Learning a text-video embedding by watching hundred million
  narrated video clips,'' in {\em Proceedings of the IEEE/CVF international
  conference on computer vision}, pp.~2630--2640, 2019.

\bibitem{radosavovic2020designing}
I.~Radosavovic, R.~P. Kosaraju, R.~Girshick, K.~He, and P.~Doll√°r, ``Designing
  network design spaces,'' 2020.

\bibitem{roberta}
Y.~Liu, M.~Ott, N.~Goyal, J.~Du, M.~Joshi, D.~Chen, O.~Levy, M.~Lewis,
  L.~Zettlemoyer, and V.~Stoyanov, ``Roberta: A robustly optimized bert
  pretraining approach,'' {\em arXiv preprint arXiv:1907.11692}, 2019.

\bibitem{singer2022make}
U.~Singer, A.~Polyak, T.~Hayes, X.~Yin, J.~An, S.~Zhang, Q.~Hu, H.~Yang,
  O.~Ashual, O.~Gafni, {\em et~al.}, ``Make-a-video: Text-to-video generation
  without text-video data,'' {\em arXiv preprint arXiv:2209.14792}, 2022.

\bibitem{rlhf}
P.~F. Christiano, J.~Leike, T.~Brown, M.~Martic, S.~Legg, and D.~Amodei, ``Deep
  reinforcement learning from human preferences,'' {\em Advances in neural
  information processing systems}, vol.~30, 2017.

\bibitem{bai2022constitutional}
Y.~Bai, S.~Kadavath, S.~Kundu, A.~Askell, J.~Kernion, A.~Jones, A.~Chen,
  A.~Goldie, A.~Mirhoseini, C.~McKinnon, {\em et~al.}, ``Constitutional ai:
  Harmlessness from ai feedback,'' {\em arXiv preprint arXiv:2212.08073}, 2022.

\bibitem{dpo}
R.~Rafailov, A.~Sharma, E.~Mitchell, S.~Ermon, C.~D. Manning, and C.~Finn,
  ``Direct preference optimization: Your language model is secretly a reward
  model,'' {\em arXiv preprint arXiv:2305.18290}, 2023.

\bibitem{transformers}
T.~Wolf, L.~Debut, V.~Sanh, J.~Chaumond, C.~Delangue, A.~Moi, P.~Cistac,
  T.~Rault, R.~Louf, M.~Funtowicz, J.~Davison, S.~Shleifer, P.~von Platen,
  C.~Ma, Y.~Jernite, J.~Plu, C.~Xu, T.~L. Scao, S.~Gugger, M.~Drame, Q.~Lhoest,
  and A.~M. Rush, ``Transformers: State-of-the-art natural language
  processing,'' in {\em Proceedings of the 2020 Conference on Empirical Methods
  in Natural Language Processing: System Demonstrations}, Oct. 2020.

\bibitem{pytorch}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen,
  Z.~Lin, N.~Gimelshein, L.~Antiga, A.~Desmaison, A.~Kopf, E.~Yang, Z.~DeVito,
  M.~Raison, A.~Tejani, S.~Chilamkurthy, B.~Steiner, L.~Fang, J.~Bai, and
  S.~Chintala, ``Pytorch: An imperative style, high-performance deep learning
  library,'' in {\em Neural Information Processing Systems}, 2019.

\end{thebibliography}
