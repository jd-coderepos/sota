\documentclass[11pt,letter]{article}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{verbatim}
\usepackage{fullpage}
\usepackage[margin=.9in]{geometry}
\usepackage{framed}
\usepackage{algorithm,algpseudocode}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{complexity}
\newcommand{\pindent}{\setlength{\parindent}{18pt} \indent} \newcommand{\bdisp}{\begin{displaystyle}}
\newcommand{\edisp}{\end{displaystyle}}





\renewcommand{\Pr}{\mathbb{P}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Exp}{\mathbb{E}}
\newcommand{\from}{\leftarrow}

\renewcommand{\poly}{\mathrm{poly}}
\renewcommand{\polylog}{\mathrm{polylog}}

\newcommand{\Normal}{\mathcal{N}}
\newcommand{\Unif}{\mathrm{Unif}}

\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\PSet}{\mathcal{P}}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\Rational}{\mathbb{Q}}
\newcommand{\argmax}{\operatorname*{\mathrm{arg\,max}}}
\newcommand{\argmin}{\operatorname*{\mathrm{arg\,min}}}

\renewcommand{\d}{\mathrm{d}}
\newcommand{\Diff}[2][]{\frac{\d#1}{\d#2}}
\newcommand{\Grad}{\nabla}
\newcommand{\Del}[2][]{\frac{\partial#1}{\partial#2}}
\newcommand{\sgn}{\mathrm{sgn}}

\newcommand{\ferr}{\epsilon}
\newcommand{\perr}{\delta}
\newcommand{\safety}{s}


\newcounter{nTheorems}
\numberwithin{nTheorems}{section}

\newtheorem{theorem}[nTheorems]{Theorem}
\newtheorem{corollary}[nTheorems]{Corollary}
\newtheorem{conjecture}[nTheorems]{Conjecture}
\newtheorem{lemma}[nTheorems]{Lemma}
\newtheorem{proposition}[nTheorems]{Proposition}
\newtheorem*{proposition*}{Proposition}
\newtheorem*{lemma*}{Lemma}
\newtheorem{claim}[nTheorems]{Claim}

\newtheorem{definition}[nTheorems]{Definition}
\newtheorem{example}[nTheorems]{Example}

\newtheoremstyle{break}
  {\topsep}{\topsep}{}{}{\bfseries}{}{\newline}{}\theoremstyle{break}
\newtheorem{myalgorithm}{Algorithm}


\title{Optimizing Star-Convex Functions}
\author{
Jasper C.H. Lee
\quad
Paul Valiant\\ \ \\
Department of Computer Science\\
Brown University\\
\texttt{\{jasperchlee,paul\char`_valiant\}@brown.edu}
}

\begin{document}
\maketitle
\begin{abstract}
We introduce a polynomial time algorithm for optimizing the class of star-convex functions, under no Lipschitz or other smoothness assumptions whatsoever, and no restrictions except exponential boundedness on a region about the origin, and Lebesgue measurability. The algorithm's performance is polynomial in the requested number of digits of accuracy and the dimension of the search domain. This contrasts with the previous best known algorithm of Nesterov and Polyak which has exponential dependence on the number of digits of accuracy, but only  dependence on the dimension  (where  is the matrix multiplication exponent), and which further requires Lipschitz second differentiability of the function~\cite{Nesterov:2006}. Star-convex functions constitute a rich class of functions generalizing convex functions, including, for example: for any convex (or star-convex) functions , , with global minima , their \emph{power mean}  is star-convex, for \emph{any} real , defining powers via limits as appropriate. Star-convex functions arise as loss functions in non-convex machine learning contexts, including, for data points , parameter vector , and any real exponent , the loss function , significantly generalizing the well-studied \emph{convex} case where . Further, for \emph{any} function  on the surface of the unit sphere (including discontinuous, or pathological functions that have different behavior at rational vs. irrational angles), the star-convex function  extends the arbitrary behavior of  to the whole space.

Despite a long history of successful gradient-based optimization algorithms, star-convex optimization is a uniquely challenging regime because 1) gradients and/or subgradients often do not exist; and 2) even in cases when gradients exist, there are star-convex functions for which gradients provably provide no information about the location of the global optimum. We thus bypass the usual approach of relying on gradient oracles and introduce a new randomized cutting plane algorithm that relies only on function evaluations. Our algorithm essentially looks for structure at all scales, since, unlike with convex functions, star-convex functions do not necessarily display simpler behavior on smaller length scales. Thus, while our cutting plane algorithm refines a feasible region of exponentially decreasing volume by iteratively removing ``cuts", unlike for the standard convex case, the structure to efficiently discover such cuts may not be found within the feasible region: our novel star-convex cutting plane approach discovers cuts by sampling the function exponentially far outside the feasible region.

We emphasize that the class of star-convex functions we consider is as unrestricted as possible: the class of Lebesgue measurable star-convex functions has theoretical appeal, introducing to the domain of polynomial-time algorithms a huge class with many interesting pathologies.
We view our results as a step forward in understanding the scope of optimization techniques beyond the garden of convex optimization and local gradient-based methods.




\end{abstract}
\thispagestyle{empty}\setcounter{page}{0}
\newpage
\section{Introduction}


Optimization is one of the most influential ideas in computer science, central to many rapidly developing areas within computer science, and also one of the primary exports to other fields, including operations research, economics and finance, bioinformatics, and many design problems in engineering. Convex optimization, in particular, has produced many general and robust algorithmic frameworks that have each become fundamental tools in many different areas: linear programming has become a general modeling tool, its simple structure powering many algorithms and reductions; semidefinite programming is an area whose scope is rapidly expanding, yielding many of the best known approximation algorithms for optimizing constraint satisfaction problems~\cite{chlamtac-tulsiani}; convex optimization generalizes both of these and has introduced powerful optimization techniques including interior point and cutting plane methods. Our developing understanding of optimization has also led to new algorithmic design principles, which in turn leads to new insights into optimization. Recent progress in algorithmic graph theory has benefited enormously from the optimization perspective, as many recent results on max flow/min cut~\cite{Madry:2013,Kelner:2014,Lee:2014,Christiano:2011,Sherman:2013}, bipartite matching~\cite{Madry:2013}, and Laplacian solvers~\cite{Vishnoi:2012,Christiano:2011,Spielman:2004} have made breakthroughs that stem from developing a deeper understanding of the characteristics of convex optimization techniques in the context of graph theory. These successes motivate the quest for a deeper and broader understanding of optimization techniques: 1) to what degree can convex optimization techniques be extended to non-convex functions; 2) can we develop general new tools for tackling non-convex optimization problems; and 3) can new techniques from non-convex optimization yield new insights into convex optimization?

As a partial answer to the first question, gradient descent---perhaps the most natural optimization approach---has had enormous success recently in a variety of practically-motivated non-convex settings, sometimes with provable guarantees. The method and its variants are the de facto standard for training (deep) neural networks, a hot topic in high dimensional non-convex optimization, with many recent practical results (e.g.~\cite{Imagenet,AlphaGo}). Gradient descent can be thought of as a ``greedy" algorithm, which repeatedly chooses the most attractive direction from the local landscape. The efficacy of gradient descent algorithms relies on local assumptions about the function: for example, if the first derivative is Lipschitz (slowly varying), then one can take large downhill steps in the gradient direction without worrying that the function will change to going uphill along this direction. Thus the convergence of gradient descent algorithms typically depends on a Lipschitz parameter (or other smoothness measure), and conveniently does not depend on the dimension of the search space. Many of these algorithms converge to within  of a local optimum in time , with additional polynomial dependence on the Lipschitz constant or other appropriate smoothness guarantee~\cite{Nesterov:AGD,Hazan:2015}. In cases where all local optima are global optima, then one has global convergence guarantees.

While one intuitively expects gradient descent algorithms to always converge to a local minimum, in this paper we study the optimization of a natural generalization of convex functions where, despite the global optimum being the only stationary point/local minimum for every function in this class, provably no variant of gradient descent converges in polynomial time. The class of star-convex functions, which we define below, includes many functions of both practical and theoretical interest, both generalizing common families of convex functions to wider parameter regimes, and introducing new ``pathologies" not found in the convex case. We show, essentially, how to make a gradient-based cutting plane algorithm ``robust" to many new pathologies, including lack of gradients or subgradients, long narrow ridges and rapid oscillation in directions transverse to the global minimum, and, in fact, almost arbitrary discontinuities. This challenging setting gives new insights into what fundamentally enables cutting plane algorithms to work, which we view as progress towards answering questions 2 and 3 above.


\subsection{Star-convex functions}
This paper focuses on the optimization of \emph{star-convex} functions, a particular class of (typically) non-convex functions that includes convex functions as a special case.
We define these functions as follows, based on the definition in Nesterov and Polyak~\cite{Nesterov:2006}.

\begin{definition}[Star-convex functions]
A function  is \emph{star-convex} if there is a global minimum  such that for all  and ,

We call  the star center of .
For convenience, in the rest of the paper we shall refer to the minimum function value as .
\end{definition}

See Appendix~\ref{sec:examples} for a variety of basic constructions and examples of star-convex functions.

Intuitively, if we visualize the objective function as a landscape, star-convexity means that the global optimum is ``visible" from every point---there are no ``ridges" on the way to the global optimum, but there could be many ridges in transverse directions. Since the global optimum is always visible in a downhill direction from every point, gradient descent methods would \emph{seem} to be effective.
Counterintuitively, these methods all fail in general. In Section~\ref{sect:Hardness} we demonstrate that standard gradient descent and cutting plane methods fail in the absence of Lipschitz guarantees.

\begin{figure}
\centering
\begin{subfigure}[b]{.45\textwidth}
\centering
\includegraphics[width=5cm, keepaspectratio]{fig1a}
\caption{ defined on the unit circle}
\label{Fig:fig1a}
\end{subfigure}
\quad
\begin{subfigure}[b]{.45\textwidth}
\centering
\includegraphics[width=5cm, keepaspectratio]{fig1b}
\caption{Linear extension of  to }
\label{Fig:fig1b}
\end{subfigure}
\caption{An example star-convex function  defined by linearly extrapolating an \emph{arbitrary} positive function  defined on the unit circle (in white).}
\label{Fig:fig1}
\end{figure}


One broad class of star-convex functions that gives a good sense of the scope of this definition is constructed by the following process: 1) pick an arbitrary positive function  on the unit circle (see Figure~\ref{Fig:fig1a}) which may be discontinuous, rapidly oscillating, or otherwise badly behaved; and 2) linearly extend this function to a function  on the entire plane, about the value  (see Figure~\ref{Fig:fig1b}).

The name ``star-convex'' comes from the fact that each sublevel set (the set of  for which , for some ) is ``star-shaped".

At any point at which a gradient or subgradient exists, the star center (global optimum) lies in the halfspace opposite the (sub)gradient. However, even for the simple example of Figure~\ref{Fig:fig1}, gradients do not exist for angles  at which  is discontinuous, and subgradients do not exist at most angles, including those angles where  is a local maxima with respect to . Further, even for differentiable star-convex functions, gradients can be misleading, since a rapidly oscillating  implies that gradients typically point in the transverse direction, nearly orthogonal to the direction of the star center.

While it is often standard to design algorithms assuming one has access to an oracle that returns both function values and gradients, we instead only assume access to the function value: even in the case when gradients exist, it is unclear whether they are algorithmically helpful in our setting. (See Section~\ref{sect:Hardness} for details.) Indeed, we pose this as an open problem: under what assumptions (short of the Lipschitz guarantees on the second derivative of Nesterov and Polyak~\cite{Nesterov:2006}), can one meaningfully use a gradient oracle to optimize star-convex functions?

In this paper, we show that, assuming only Lebesgue measurability and an exponential bound on the function value within a large ball, our algorithm optimizes a star-convex function in  time where  is the desired accuracy in function value.

\begin{theorem}
\label{thm:InformalIntro} (Informal)
Given evaluation oracle access to a Lebesgue measurable star-convex function  and an error parameter , with the guarantee that  is within radius  of the origin, Algorithm~\ref{alg:Ellipsoid} returns an estimate  of the minimum function value such that  with high probability in time .
\end{theorem}

In previous work, Nesterov and Polyak~\cite{Nesterov:2006} introduced a new adaptation of Newton's method to find local minima in functions that are twice-differentiable, with Lipschitz continuity of their second derivatives. For the class of Lipschitz-twice-differentiable star-convex functions, they show that their algorithm converges to within  of the global optimum in time , using star convexity to lower bound the amount of progress in each of their optimization steps.

Our results are stronger in two significant senses: 1) as opposed to assuming Lipschitz twice-differentiability, we make \emph{no continuity assumptions whatsoever}, optimizing over an essentially arbitrary measurable function within the class of star-convex functions; 2) while the algorithm of~\cite{Nesterov:2006} requires exponential time to estimate the optimum to  digits of accuracy, our algorithm is \emph{polynomial in the requested number of digits of accuracy}.

The reader may note that the complexity of our algorithm does depend polynomially on the number of dimensions  of the search space, whereas the Nesterov-Polyak algorithm uses a number of calls to a Hessian oracle that is \emph{independent} of the dimension .
However, our dependency on the dimension  is necessary in the absence of Lipschitz guarantees, even for convex optimization~\cite{Nemirovskii:1983}.

We further point out an important distinction between \emph{star-convex} optimization and \emph{star-shaped} optimization.
A star-shaped function is defined similarly to a star-convex function, except the star center is allowed to be located away from the global minimum.
Certain \NP-hard optimization problems, including max-clique can be rephrased in terms of star-shaped optimization~\cite{MaxClique}, and the problem is in general impossible without continuity guarantees, for the global minimum may be hidden along a single ray from the star center that is discontinuous from the rest of the function. In this sense, our star-convex optimization algorithm narrowly avoids solving an \NP-hard optimization problem.

\subsection{Applications of Star-Convex Functions}

We begin with the following concrete example: the function  is convex for , yet is star-convex for \emph{all} real , positive or negative, taking limits as appropriate for . We generalize this example significantly below.

\emph{Empirical risk minimization} is a central technique in machine learning~\cite{Vapnik:2013}, where, given training data  with labels , we seek a hypothesis  so as to minimize  where  is a \emph{loss function} that describes the penalty for misprediction, on input our prediction  and the true answer . We take the hypothesis  to be a linear function (this setting includes kernel methods that preprocess  first and then apply a linear function). If the loss function  is convex, then finding the optimal hypothesis  is a convex optimization problem, which is widely used in practice: for exponent , we let  and thus aim to optimize the convex function 

In this paper we draw attention to the interesting regime of Equation~\ref{eq:ERM-p} where . This regime is not accessible with standard techniques. It is straightforward to verify, however, that the th power of the objective function of Equation~\ref{eq:ERM-p} is star-convex, and thus the main result of our paper yields an efficient optimization algorithm. Slightly more generally:

\begin{corollary}
  For a loss function  such that there exists a real number  for which  is a star-convex function of its first argument, the following empirical risk minimization problem can be optimized to accuracy  in time  by inputting its th power to the main algorithm of this paper:
  
\end{corollary}

(We note that negative  is a somewhat peculiar regime in the above corollary, where running a minimization algorithm on the th power of the loss would actually yield a global \emph{maximum} instead of minimum; we include this case here for completeness' sake.)

The regime where  has the structure that, when one is far from the right answer, small changes to the hypothesis do not significantly affect performance, but the closer one gets to the true parameters, the richer the landscape becomes. One of many interesting settings with this behavior is biological evolution, where ``fit" creatures have a rich landscape to traverse, while drastically unfit creatures fail to survive. A paper by one of the authors proposed star-convex optimization as a regime where evolutionary algorithms might be unexpectedly successful. This current work finds an affirmative answer to an open question raised there by showing the first polynomial time algorithm for optimizing this general class of functions. (The previous results by Nesterov and Polyak~\cite{Nesterov:2006} fail to apply to this setting because the objective function has regions that behave---for some parameter regimes---like , which is not Lipschitz.)

\begin{corollary}[Extending Theorem 4.5 of~\cite{Valiant:2014}]
There is a single mutation algorithm under which for any , defining the loss function , the class of constant-degree polynomials from  with bounded coefficients is evolvable with respect to all distributions over the radius  ball.
\end{corollary}



\subsection{Need for Novel Algorithmic Techniques}
\label{sect:Hardness}

Before we outline our approach to optimizing star-convex functions, we demonstrate the need for novel algorithmic approaches by explaining how a wide variety of standard techniques fail on this challenging class of functions. We start by explaining how simple star-convex functions such as  confound gradient descent, cutting plane and related approaches, and end with a pathological example with information-theoretic guarantees about its hardness to optimize, even given arbitrarily accurate first-order (gradient) oracle access.

Consider the star-convex function , which can be thought of essentially as . This function has unique global minimum (and star center) at the origin. Gradients of this function go to infinity as either  or  goes to 0, thus the function essentially has deep canyons along both the  and  axes. Intuitively, this function is challenging to optimize because, despite the origin lying in a ``downhill" direction from every point, gradient descent will typically converge to \emph{one} of the axes first, at which point the gradient has near-infinite magnitude in the direction of the nearest axis, and this direction is thus near-orthogonal to the direction of the origin. In short, many variants of gradient descent will have the following behavior: rapidly jump towards one of the two axes, and then fail to make significant further progress as the search point oscillates around that axis. This is in part a reflection of the fact that the gradient of  is \emph{not} Lipschitz, and in fact varies arbitrarily rapidly as  converges towards either axis; since these axes are ``canyons" in the search space, typical algorithms will spend a disproportionate amount of their time stumbling around this badly-behaved region.

There are many variants of gradient descent constructed to tackle optimization settings that are challenging for ``vanilla" gradient descent. One recent promising line of work concerns \emph{normalized} gradient descent~\cite{Hazan:2015}, which is designed to work well despite regions where the magnitude of the gradient is unusually small or large. However, the analysis techniques rely on the ``strict local quasi-convexity" property, which says that there is a global constant  such that the downhill halfspace induced by any gradient contains a ball of radius  about the global optimum; this property does not apply even to the simple function , where halfspaces cut arbitrarily close to the origin, the closer one gets to either the  or  axis.

As an alternative to gradient descent methods, one might consider a gradient-based cutting plane algorithm. Cutting plane methods work by iteratively cutting a large search space into ever-smaller regions until they converge to a tiny region guaranteed to contain the global optimum. Cutting plane algorithms, even for convex optimization, are known to typically lead to ill-conditioned behavior, where one dimension of the search region becomes much smaller than the overall diameter of the region, leading to bad numerical properties, difficulty finding and manipulating gradients, etc. These problems become even worse in our star-convex setting, even for the simple function : as the search space converges around one of the axes, the gradients become even more strongly oriented towards that axis, yielding cuts that make the already thin region even thinner, instead of cutting in a more productive transverse direction. Indeed, up to numerical precision, the computed gradients may be found to point exactly towards the nearest axis, making cuts in the transverse direction impossible, and thus halting the overall progress of the algorithm indefinitely.

Having seen simple examples showing how standard techniques cannot optimize star-convex functions, we now present a more sophisticated and pathological example which proves that, even with access to a first-order oracle and the assumptions of infinite differentiability and boundedness on a region around the origin, no \emph{deterministic} algorithm can efficiently optimize star-convex functions without Lipschitz guarantees, motivating the somewhat unusual randomized flavor of the algorithm of this paper.
In particular, we show that, for any deterministic polynomial time algorithm, there is an \emph{infinitely differentiable} star-convex function on  with a unique global minimum near the origin such that for all queried points 1) the returned function value is always 1 and 2) the returned gradient depends \emph{only} on the -coordinate.
The existence of such a function implies the inability of the algorithm to find the -coordinate of the star center.

\begin{example}\label{ex:deterministic}

Given a deterministic polynomial time optimization algorithm , we construct a star-convex function  that algorithm  fails to optimize.

We assume that  will only query the function inside some disk of radius  about the origin, for some (possibly exponentially large) .

Now, we first choose a -coordinate  (that  cannot specify exactly, for example an irrational number) for the star center; we shall choose  later.
Then, we simulate running the algorithm , assuming that in response to any query  that  makes to the function/gradient oracle, the return value is  and the gradient  is 0 in the  direction and  in the  direction.

Since  is a polynomial time algorithm, there must only be polynomially many queries before  terminates.
Therefore, we may choose  for the star center such that 1)  is not exponentially close to collinear with any pair of query points, and 2) the  coordinate of the star center, , is chosen to be significantly far from the  coordinates of all query points.

We thus construct a star-convex function  subject to the constraints that 1) it passes through all the points simulated above, with corresponding gradients, 2) it has star center at , and 3) it is infinitely differentiable, with each derivative being at most exponentially large in the degree of the derivative and the running time of . (Since no pair of query points are close to collinear with the star center, standard interpolation lets us construct  by first defining a smooth function  on the unit circle and linearly extending  to the entire plane, except for an exponentially small region about  that we make smooth instead of ``conically" shaped.)

Since the function and gradient oracles return zero information about the -coordinate of the star center, algorithm  clearly has a hopeless task optimizing this function.

\iffalse
We are now ready to extend  to the whole space, subject to the constraints imposed by the queries and that the star center is at .
Without loss of generality we now translate the space such that the star center is at the origin.
We use the functional form  if ,  if  and .
It is immediate that this form is always star-convex.
Observe that, since no pair of query points is collinear with the origin (star center), for each  there is only one query point constraining  (and ).
Under the above functional form, for  we have  and .
Therefore, for each query point  with , set  and .
There exists an infinitely differentiable extension of  to .
Therefore, for all query points with , we have constructed  such that  and .
Moreover,  is infinitely differentiable for all .
We can similarly construct , and since no query point is directly above the star center, we pick  arbitrarily, say .
Thus the function  is infinitely differentiable except when , which has measure 0.
Note that, since all query points  is at least  from the origin,  is at most  on the radius  circle in the original space.
Therefore, our boundedness assumption is also satisfied, thus giving us the required  that cannot be optimized by the candidate algorithm .
\fi


\end{example}


In summary, gradient information is very hard to use effectively in star-convex optimization, even for smooth functions. Our algorithm, outlined below, instead only queries the function value---not because we object to gradients, but rather because they do not seem to help.




\subsection{Our Approach}

Our overall approach is via the ellipsoid method, which repeatedly refines an ellipsoidal region containing the star center (global optimum) by iteratively computing ``cuts" that contain the star center, while significantly reducing the overall volume of the ellipsoid.

As mentioned in Section~\ref{sect:Hardness}, even for smooth functions with access to a gradient oracle, the cutting planes induced by the gradients may yield no significant progress in some directions.
Finding ``useful" cutting planes in the star-convex setting requires three novelties---see the introduction to Section~\ref{sec:cuts} for complete details. {\bf First}, the star-convex function may be discontinuous, without even subgradients defined at most points, so we instead rely on a sampling process involving the ``\emph{blurred logarithm}" of the objective function. The blurred logarithm mitigates both the (potentially) exponential range of the function, and the (potentially) arbitrary discontinuities in the domain. The blurred logarithm, furthermore, is differentiable, and sampling results let us estimate and use its derivatives in our algorithm. This technique is similar to that of randomized smoothing~\cite{Duchi:2012}. {\bf Second}, the negative gradient of the blurred logarithm might point away from the star center (global optimum)---despite all gradients of the unblurred function (when they exist) pointing towards the star center---because of the way blurring interacts with sharp wedge-shaped valleys. Addressing this requires an averaging technique that repeatedly samples Gaussians-in-Gaussians, until it detects sufficient conditions to conclude that the gradient may be used as a cut direction. {\bf Third} and finally, the usual cutting plane criterion---that the volume of the feasible region decreases exponentially---is no longer sufficient in the star-convex setting, as an ellipsoid in two coordinates  might get repeatedly cut in the  direction without any progress restricting the range of . This is provably not a concern in convex optimization. Our algorithm tackles this issue by ``locking" axes of the ellipsoid smaller than some threshold , and seeking cuts orthogonal to these axes; this orthogonal signal may be hidden by much larger derivatives in other directions, and hence requires new techniques to expose. The counterintuitive approach is that, in order to expose structure within an exponentially thin ellipsoid dimension we must search exponentially far outside the ellipsoid in this dimension.

\subsection{Stochastic Optimization}
Our approach extends easily to the stochastic optimization setting.
Here, the objective function is the expectation over a distribution of star-convex functions which share the same star center and optimum value.
For example, in the context of empirical risk minimization with a star-convex loss function  where we assume there is a true hypothesis  such that each labeled example is correctly predicted by , then for each example , the loss ---considered as a function of ---is star-convex, with global optimum 0 at . And the overall objective function is the expectation of this quantity, over random examples: , which is thus also star-convex with global optimum 0 at .

The optimization algorithm for this new stochastic setting is actually identical to the general star-convex optimization algorithm. Since star-convex functions can take arbitrarily unrelated values on nearby rays from the star center, our algorithm is already fully equipped to deal with the situation, and adding explicitly unrelated values to the model by stochastically sampling from a family of functions  makes the problem no harder. The algorithm, analysis, and convergence are unchanged.


\subsection{Outline}
In Section~\ref{sec:Statement} we introduce the oracle model by which we model access to star-convex functions. Because we address such a large class of potentially badly-behaved functions (including, for example, star-convex functions which, in polar coordinates, behave very differently depending on whether the angle is rational or irrational), we take great care to specify the input and output requirements of our optimization algorithm appropriately, in the spirit of Lov\'{a}sz~\cite{Lovasz:1987}.

In Section~\ref{sec:ellipsoid} we present the overall star-convex optimization algorithm, a cutting-plane method that at each stage tracks an ellipsoid that contains the star center (global optimum). The heart of the algorithm consists of finding appropriate cutting planes at each step, which we present in Section~\ref{sec:cuts}.

\begin{comment}
Convexity is a strong global guarantee about the shape of an objective function, while gradient-based algorithms typically only rely on local properties to make progress. This realization has led to a line of work showing how local guarantees (that are significantly weaker than convexity) can lead to efficient algorithms for finding \emph{local} optima. For example, the work of Ge et al.~\cite{Ge:2015} develops a variant of gradient descent for non-convex functions that satisfy the ``strict saddle" property, which says that points with 0 derivative (stationary points) that are \emph{not} local minima are allowed to exist, but if they exist they must have second derivative strictly bounded below 0 in some direction (see also the work of Dauphin et al.~\cite{Dauphin:2014}). Matrix completion is another important class of non-convex optimization problems, crucial to emerging techniques in machine learning~\cite{Koren:2009}. Recent algorithms for this problem rely on decomposing the overall non-convex optimization problem into related convex optimization (sub)problems~\cite{Candes:2009,Jain:2013}.

Stochastic approaches to optimization problems are becoming a crucial area of new optimization developments. Simulated annealing is a general and widely used approach to optimization that, unlike standard convex optimization algorithms, uses randomness to explore the search space. Randomized exploration can reveal structure of the search space that is otherwise hidden, and this intuition was used to great effect in the line of work on approximating the volume of convex bodies~\cite{Dyer:1991,Lovasz:2006}. Thus even classic convex optimization settings can be more profitably explored with unexpected randomized techniques.
A rather different application for randomization in a convex setting is as follows. In typical large-scale machine learning problems, the objective function to be minimized is the sum of a huge number of terms, each expressing the error of the hypothesis on an individual training example. Thus even a single evaluation of the objective function may be prohibitively expensive. Instead, the hugely influential ``stochastic gradient descent" algorithm~\cite{Bottou:2010} samples a subset of the training examples to estimate the objective function (and its gradient), thus introducing stochasticity into an otherwise deterministic approach.

In summary, new developments in optimization show much promise for inspiring new developments in algorithms, particularly in settings involving non-convex functions and stochastic techniques.

This paper presents the first polynomial time algorithm for globally optimizing the class of ``star-convex" functions~\cite{Nesterov:2006}, expanding the scope of cutting plane algorithms to a class of problems that is richer than convex optimization in several aspects, as well as introducing new randomized algorithms techniques to the area of optimization. As explained below, in contrast to standard optimizations settings, our results apply to a function class including examples that may be discontinuous everywhere (except the global optimum), may lack subgradients almost everywhere, and more generally, in  dimensions, can take values on the surface of an -sphere that match those of \emph{any} bounded, measurable function .

\subsection{Star-Convex Optimization}

This paper focuses on the optimization of a particular class of non-convex function, namely \emph{star-convex} functions.
We define these functions as follows, based on the definition in Nesterov and Polyak~\cite{Nesterov:2006}, and describe our main contribution in Theorem~\ref{thm:InformalIntro}.

\begin{definition}[Star-convex functions]
A function  is \emph{star-convex} if there is a global minimum  such that for all  and ,

We call  the star center of .
For convenience, in the rest of the paper we shall refer to the minimum function value as .
\end{definition}

We note that the definition of star-convex is unaffected by affine transformations of the domain, and arbitrary translations in the range.

A star-convex function  may have multiple global minima; Nesterov and Polyak~\cite{Nesterov:2006} require all of them to be star centers, but we only require one of them to be (and thus our definition is a slightly relaxed version of the definition in their work).

This notion is introduced by Nesterov and Polyak~\cite{Nesterov:2006} in the context of a new adaptation of Newton's method to find local minima in functions that are twice-differentiable, with Lipschitz continuity of their second derivatives. For the class of Lipschitz-twice-differentiable star-convex functions, their algorithm converges to within  of the global optimum in time .

Our results are stronger in two significant senses: 1) as opposed to assuming Lipschitz twice-differentiability, we make \emph{no continuity assumptions whatsoever}, optimizing over an essentially arbitrary measurable function within the class of star-convex functions; 2) while the algorithm of~\cite{Nesterov:2006} requires exponential time to estimate the optimum to  digits of accuracy, our algorithm is \emph{polynomial in the requested number of digits of accuracy}.


\begin{theorem}
\label{thm:InformalIntro} (Informal)
Given a Lebesgue measurable star-convex function  and an error parameter , with the guarantee that  is within radius  of the origin, Algorithm~\ref{alg:Ellipsoid} returns an estimate  of the minimum function value such that  with high probability in time .
\end{theorem}



\subsubsection{The Scope of Star-Convex Functions}\label{sec:examples}
In this section we introduce some basic building blocks to construct large families of star-convex functions, with the aim of demonstrating the richness of the definition. More technical examples of star-convex functions are introduced later in the paper to illustrate particular points.
\newpage
\medskip\noindent{\bf Basic Star-Convex Functions:}
\begin{itemize}
  \item[1.] Any convex function is star-convex.
  \item[2.] Even in 1 dimension, star-convex functions can be non-convex: 
  Or, for a Lipschitz twice-differentiable example,  (from Nesterov and Polyak~\cite{Nesterov:2006}).
  \item[3.] In  dimensions, take an \emph{arbitrary} positive function  on the unit sphere, and extend it to the origin in a way that is star-convex on each ray through the origin, for example, extending  linearly to define 
\end{itemize}


\medskip\noindent{\bf Ways to Combine Star-Convex Functions:}
Given any star-convex functions  that have star centers at , we can combine them to generate a new star convex function in the following ways:
\begin{itemize}
  \item[4.] A star-convex function can be shifted in the  and  senses so that its global optimum is at an arbitrary  and  value; in general, for any affine transformation  and real number , the function  is star-convex.
  \item[5.] For any positive power , the function  is star-convex.
  \item[6.] The sum  is star-convex.
  \item[7.] The product  is star-convex.
  \item[8.] The \emph{power mean}: for real number , the function  is star-convex, defining powers via limits as appropriate. (The case  corresponds to .)
\end{itemize}

\medskip\noindent{\bf Practical Examples of Star-Convex Function Classes:}

We combine the basic examples above to yield the following general classes of star-convex functions.

\begin{itemize}
  \item[9.] Sums of squares of monomials: the square of any monomial , with , will be star-convex about the origin, and thus sums of such terms will be star-convex despite typically being non-convex, for example . Sums-of-squares arise in many different contexts.
  \item[10.] More generally, any polynomial of  with nonnegative coefficients is star-convex.
  \item[11.] In the standard machine learning setting where one is trying to learn a parameter vector : for a given hypothesis , and each training example , the hypothesis gives an error  that is (typically) a convex function of its second argument, . Averaging these convex losses over all , via the power mean of exponent  leads to a star-convex loss, as a function of , with minimum value 0 at . The paper by one of the authors~\cite{Valiant:2014} discusses a class of such star-convex loss functions which have the form 
\end{itemize}
\end{comment}



\section{Problem Statement and Overview}
\label{sec:Statement}


Our aim here is to discuss algorithms for optimizing star-convex functions in the most general setting possible, and thus we must be careful about how the functions are specified to the algorithms.
In particular, we do not assume continuity, so functions with one behavior on the rational points and a separate behavior on irrational points are possible. Therefore, it is essential that our algorithms have access to the function values of  at inputs beyond the usual rational points expressible via standard computer number representations.
As motivation, see Example~\ref{ex:rationals}
which describes a star-convex function that has value 1 on essentially all rational points an algorithm is likely to query, despite having a rather different landscape on the irrational points, leading to a global minimum value of 0.

\begin{example}\label{ex:rationals}
We present the following example of a class of star-convex functions  in 2-dimensions parameterized by integers  such that  has global optimum at .
The class has the property that, if  and  are chosen randomly from an exponential range, then, except with exponentially small probability, any (probabilistic) polynomial time algorithm that accesses  only at rational points will learn nothing about the location of the global optimum.
It demonstrates the need for access beyond the rationals.


We define a star-convex function  parameterized by integers  that has a unique global optimum at .
We evaluate  via three cases:
\begin{enumerate}
  \item If the ray from  to  passes through no rational points, then let .
  \item Otherwise, if the ray from  to  passes through a rational point , then:
    \begin{enumerate}
    \item If  and  then let  as above.
    \item Else, let 
  \end{enumerate}
\end{enumerate}
We note that since  and  are linearly independent over the rationals, no ray from  passes through more than one rational point.
Hence the above three cases give a complete definition of .

The function  is star-convex since each of the three cases above apply on the entirety of any ray from the global optimum, and these cases each define a linear function on the ray.
The derivative of  along any of these rays is at most , since  is the closest that a point outside the integer square at  can get to the global optimum ; thus the function is linearly bounded in any finite region.
Further,  evaluated at any \emph{rational} point  will fall into Case 2, and thus, unless , the function  will return 1.

Thus, no algorithm that queries  only at rational points can efficiently optimize this class of functions, because unless the algorithm \emph{exactly} guesses the pair ---which was drawn from an exponentially large range---no information is gained (the value of  will always be 1 otherwise).
\end{example}


Since directly querying function values of general star-convex functions at rational points is so limiting, we instead introduce the notion of a \emph{weak sampling evaluation oracle} for a star-convex function, adapting the definition of a \emph{weak evaluation oracle} by Lov\'{a}sz~\cite{Lovasz:1987}.

\begin{definition}
\label{def:evaluation}
A \emph{weak sampling evaluation oracle} for a function  takes as inputs a point , a positive definite covariance matrix , and an error parameter . The oracle first chooses a random point , and returns a value  such that .
\end{definition}

The Gaussian sampling in Definition~\ref{def:evaluation} can be equivalently changed to choosing a random point in a ball of desired (small) radius, since any Gaussian distribution can be approximated to arbitrary precision (in the total variation sense) as the convolution of itself with a small enough ball, and vice versa. 

Because inputs and outputs to the oracle must be expressible in a polynomial number of digits, we consider ``well-guaranteed" star-convex functions (in analogy with Lov\'{a}sz~\cite{Lovasz:1987}), where the radius bound  and function bound  below should be interpreted as huge numbers (with polynomial numbers of digits), such as  and  respectively. Numerical accuracy issues in our analysis are analogous to those for the standard ellipsoid algorithm and we do not discuss them further.

\begin{definition}\label{def:boundedness}
  A weak sampling evaluation oracle for a function  is called \emph{well-guaranteed} if the oracle comes with two bounds,  and , such that 1) the global minimum of  is within distance  of the origin, and 2) within distance  of the origin, .
\end{definition}

Such sampling gets around the obstacles of Examples~\ref{ex:deterministic} and~\ref{ex:rationals} because even if the evaluation oracle is queried at a predictable rational point, the value it returns will represent the function evaluated at an unpredictable, (typically) irrational point nearby.

At this point, our notion of oracle access may \emph{seem} unnatural, in part because it allows access to the function at irrational points that are not computationally expressible.
However, there are two natural and widely used justifications for this approach, of different flavors.
First, the oracle represents a computational model of a mathematical abstraction (the underlying star-convex function), where even for mathematically pathological functions, we can actually in many cases implement simple code to emulate oracle access in the manner described above.
For example, it is in fact easy to implement a weak sampling evaluation oracle for the function of Example~\ref{ex:rationals}, where rays from the star center through rational points behave differently from the other rays.
Since the rational points have Lebesgue measure 0, the complexities introduced by case 2 of Example~\ref{ex:rationals} happen with probability 0, and thus we can write code to simulate a weak sampling evaluation oracle by only implementing the simpler case 1, evaluating  to the requested accuracy .




Second, setting oracle implementation issues aside, the model cleanly separates \emph{accessing} a potentially pathological function, from \emph{computing properties} of it, in this case, optimizing it. The more pathological a function is, the more surprising it is that any efficient automated technique can extract structure from it. Thus, in some sense, the unrealistic regime of pathological functions, for which we do not even know how to write code emulating oracle access, yields the most surprising regime for the success of the algorithmic results of this paper. This regime is the most insightful from a theory perspective almost exactly because it is the most unnatural and counterintuitive from a practical perspective.





Having established weak sampling evaluation oracles as input to our algorithms, we must now take the same care to define the form of their outputs.
The output of a traditional (convex) optimization problem is an  value such that  is close to the global minimum. For star-convex functions, accessed via a weak sampling evaluation oracle, one can instead ask for a small spherical region in which  remains close to its global optimum---given an input point  within distance  of the star center, star convexity and boundedness of  imply that  must be within value  of the global optimum (see the proof of Lemma~\ref{lem:TinyRadius}).

However, such an output requirement is too much to ask of a star-convex optimization algorithm: see Example~\ref{ex:output} for an example of a star-convex function for which there is a large region in which the distributions of  in all small (Gaussian) balls are indistinguishable from each other except with negligible probability, say . That is, no algorithm can hope to distinguish a small ball around the global optimum from a small ball anywhere else in this region, except by using close to  function evaluations. Instead, since on this entire region, the function is close to the global optimum except with  probability, the most natural option is for an optimization algorithm to return any portion of this region, namely a region where the function value is within  of optimal except with probability . This is how we define the output of a star-convex optimization problem: in analogy to the input convention, we ask optimization algorithms to return a Gaussian region, specified by a mean and covariance, on which the function is near-optimal on all but a  fraction of its points. (See Example~\ref{ex:output} for details.)





\begin{definition}\label{def:problem}
  The \emph{weak star-convex optimization problem} for a Lebesgue measurable star-convex function , parameterized by , is as follows.
Given a well-guaranteed weak sampling evaluation oracle for , return with probability at least  a Gaussian  such that .
\end{definition}

We now formally state our main result.
\begin{theorem}\label{thm:main}
Algorithm~\ref{alg:Ellipsoid} optimizes (in the sense of Definition~\ref{def:problem}) any Lebesgue measurable star-convex function  in time .
\end{theorem}

Observe that we require only Lebesgue measurability of our objective function, and we make no further continuity or differentiability assumptions.
The measurability is necessary to ensure that probabilities and expectations regarding the function are well-defined, and is essentially the weakest assumption possible for any probabilistic algorithm.
The minimality in our assumptions contrasts that of the work by Nesterov and Polyak, which assumes Lipschitz continuity in the second derivative~\cite{Nesterov:2006}.

It is mathematically interesting that the \emph{cardinality} of the set of star-convex functions which we optimize, even in the 2-dimensional case, equals the huge quantity , while the cardinality of the entire set of continuous functions---which is \NP-hard to optimize, and strictly contains most standard optimization settings---is only .

\section{Our Optimization Approach}\label{sec:ellipsoid}

In this section, we describe our general strategy of optimizing star-convex functions by an adaptation of the ellipsoid algorithm. Our algorithm, like the standard ellipsoid algorithm, looks for the global optimum inside an ellipsoid whose volume decreases by a fixed ratio for each in a series of iterations, via algorithmically discovered cuts that remove portions of the ellipsoid that are discovered to lie in the ``uphill" direction from the center of the ellipsoid.
In Section~\ref{sec:cuts} we will explore the properties of star-convex functions that will enable us to produce such cuts. However, it is crucial that, unlike for standard convex optimization, such cuts are not enough. Consider, for example, the possibility that for a star-convex function , a cutting plane oracle only ever returns cuts in the -direction, and never reduces the size of the ellipsoid in the  direction, a situation which provably cannot occur in the standard convex setting.

Thus, when constructing a cutting plane, our algorithm defines an exponentially small threshold  (defined in Definition~\ref{def:Parameters}), such that whenever a semi-principal axis of the ellipsoid is smaller than , we guarantee a cut orthogonal to this axis. In this section, we make use of the following characterization of our cutting plane algorithm, Algorithm~\ref{alg:SingleCut}, introduced and analyzed in Section~\ref{sec:cuts}:

\begin{proposition*}[See Proposition~\ref{prop:Alg1Correctness}]
With negligible probability of failure, given an ellipsoid containing the star center, centered at the origin with all semi-principal axes longer than  scaled to 1, Algorithm \ref{alg:SingleCut} either a) returns a Gaussian region  such that , or b) returns a direction , restricted to the subspace spanned by those ellipsoid semi-principal axes longer than , such that when normalized to a unit vector , the cut  contains the global minimum.
\end{proposition*}

Note that the above precondition on the ellipsoid input is satisfied by applying the appropriate affine transformation to the current ellipsoid before supplying it to Algorithm~\ref{alg:SingleCut}.
Also note that in case a) above, the returned Gaussian region is a solution to the overall optimization problem.


Given these guarantees about the behavior of the cutting plane algorithm, Algorithm~\ref{alg:SingleCut}, we now introduce our overall optimization algorithm, based on the ellipsoid method.

For the following algorithm, we define the number of iterations  according to a standard analysis of the multiplicative volume decrease of the ellipsoid method stated in Lemma~\ref{lem:Volume} in Appendix~\ref{ap:ellipsoid}.

\vspace{3mm}
\hspace*{-\parindent}\begin{minipage}{\linewidth}
\begin{framed}
\begin{myalgorithm}[Ellipsoid method]
\label{alg:Ellipsoid}

\noindent\textbf{Input}: A radius  ball centered at the origin which is guaranteed to contain the global minimum.\\
\textbf{Output}: \emph{Either} a) A Gaussian  such that  \emph{or} b) An ellipsoid  such that all function values in  are at most .
\begin{enumerate}
   \item Let ellipsoid  be the input radius  ball.
   \item For 
         \begin{enumerate}
            \renewcommand{\labelenumii}{\arabic{enumi}\alph{enumii}.}
            \item If all the axes of  are shorter than , then \textbf{Return}  and \textbf{Halt}.
            \item Otherwise, execute Algorithm~\ref{alg:SingleCut} with ellipsoid .
            \item If it returns a Gaussian, then \textbf{Return} this Gaussian and \textbf{Halt}.
            \item Otherwise, it returns a cut direction .
            Apply an affine transformation such that  becomes the unit ball centered at the origin and  points in the negative  direction.
            Use the construction in Lemma~\ref{lem:NewEllipsoid} (see appendix) to construct a new ellipsoid  that includes the intersection of ellipsoid  with the cut.
            \item If any semi-principal axis of the ellipsoid  is larger than  then apply Lemma~\ref{lem:resizing}, and if the center of the ellipsoid has distance  from the origin then apply Lemma~\ref{lem:recentering}, to yield an ellipsoid of smaller volume, containing the entire intersection of  with the ball of radius , that now has all semi-principal axes smaller than , and has center in the ball of radius .
          \end{enumerate}
\end{enumerate}
\end{myalgorithm}
\end{framed}
\end{minipage}
\vspace{3mm}

The analysis of Algorithm~\ref{alg:Ellipsoid} is a straightforward adaptation of standard techniques for analyzing the ellipsoid algorithm, bounding the decrease in volume of the feasible ellipsoid at each step, until either the algorithm returns an explicit Gaussian as the solution (in Step 2c), or terminates because the ellipsoid is contained in an exponentially small ball (in Step 2a). See Appendix~\ref{ap:ellipsoid} for full details.

\begin{lemma}
\label{lem:EllipsoidHalt}
Algorithm~\ref{alg:Ellipsoid} halts within  iterations either through Step 2a or Step 2c.
\end{lemma}



The following lemma shows that, if the star center (global optimum) is found to lie within a ball of exponentially small radius , then this entire ball has function value sufficiently close to the optimum that any point within the ball can be returned.

\begin{lemma}
\label{lem:TinyRadius}
Given an ellipsoid  that a) has its center within  of the origin, b) is contained within a ball of radius , and c) contains the star center , then  for all .
\end{lemma}

\begin{proof}
For any , let  be the intersection of the ray  with the sphere of radius  about the origin. The boundedness of  implies that , and that . Thus star-convexity yields that  since .
\end{proof}

Since the ellipsoid returned in Step 2a will always satisfy the preconditions of Lemma~\ref{lem:TinyRadius}, the entire ellipsoid has function value within  accuracy of the global minimum.
In practice, we can just return this region.
However, if we do wish to conform to our problem statement (Definition~\ref{def:problem}), we may simply return a Gaussian ball of sufficiently small radius and centered at the ellipsoid center, such that there is only negligible probability of sampling a point more than  from its center.

In summary, Lemmas~\ref{lem:EllipsoidHalt} and~\ref{lem:TinyRadius} imply that Algorithm~\ref{alg:Ellipsoid} optimizes a star-convex function in time , proving Theorem~\ref{thm:main}.

\section{Computing Cuts for Star-Convex Functions}
\label{sec:cuts}

The general goal of this section is to explain how to compute a single cut, in the context of our adapted ellipsoid algorithm explained in Section~\ref{sec:ellipsoid}.
Namely, given (weak sampling, in the sense of Definition~\ref{def:evaluation}) access to a star-convex function , and a bounding ellipsoid in which we know the global optimum lies, we present an algorithm (Algorithm~\ref{alg:SingleCut}) that will either a) return a cut passing close to the center of the ellipsoid and containing the global optimum, or b) directly return the answer to the overall optimization problem.

There are several obstacles to this kind of algorithm that we must overcome.
Star-convex functions may be very discontinuous, without any gradients or even subgradients defined at most points.
Furthermore, arbitrarily small changes in the input value  may produce exponentially large changes in the output of , for any  that is not already exponentially close to the global optimum.
This means that standard techniques to even approximate the function's shape do not remotely suffice in the context of star-convex functions.
Finally, considering again the example of linearly extending an arbitrary function of the unit sphere surface (see Figure~\ref{Fig:fig1} or item 3 in Appendix~\ref{sec:examples}), unlike regular convex functions, star-convex functions do not become ``locally flat" along thin dimensions of increasingly thin ellipsoids.
Thus, the ellipsoid algorithm in general might repeatedly cut certain dimensions while neglecting others.

The algorithm in this section (Algorithm~\ref{alg:SingleCut}) employs three key strategies that we intuitively explain now.

\begin{enumerate}
  \item We do not work directly with the star-convex function , but instead work with a \emph{blurred} version of its \emph{logarithm} (defined in Definition~\ref{def:BlurredLog}), which has a) well-defined derivatives, that b) we can estimate efficiently.
Given a badly-behaved (measurable) function , and the pdf of a Gaussian, , blurring  by the Gaussian produces the convolution , which has well-behaved derivatives since derivatives commute with convolution: namely, letting  stand for a derivative or second derivative, we have .
Since the derivatives of a Gaussian pdf  are each bounded, we can estimate derivatives of blurred versions of discontinuous functions by sampling.
Sampling bounds imply that if we have a random variable bounded to a range of size , then we can estimate its mean to within accuracy  by taking the average of  samples.
The logarithm function (after appropriate translation so that its inputs are positive), maps a star-convex function  to a much smaller range, which enables accurate sampling (in polynomial time).
Therefore, we can efficiently estimate derivatives of the blurred logarithm of .
  \item While intuitively, the negative gradient of (the blurred logarithm of)  points towards the global minimum, this signal might be overpowered by a confounding term in a different direction (see Lemma~\ref{lem:CutLB}), making the negative gradient point \emph{away} from the global optimum in some cases.
To combat this, our algorithm repeatedly estimates the gradient at points sampled from a distribution around the ellipsoid center, and for each gradient, estimates the confounding terms, returning the corresponding gradient only once the confounding terms are found to be small.
Lemma~\ref{lem:DoubleSampling} shows that the confounding terms are small \emph{in expectation}, so Markov's inequality shows that our strategy will rapidly succeed.
  \item The ellipsoid algorithm in general might repeatedly cut certain dimensions while neglecting others, and our algorithm must actively combat this.
If some axes of the ellipsoid ever become exponentially small, then we ``lock" them, meaning that we demand a cut orthogonal to those dimensions, thus maintaining a global lower bound on the length of any axis of the ellipsoid.
Combined with the standard ellipsoid algorithm guarantee that the volume of the ellipsoid decreases exponentially, this implies that the diameter of the ellipsoid can be made exponentially small in polynomial time, letting us conclude our optimization. Finding cuts under this new ``locking" guarantee, however, requires a new algorithmic technique.

We must take advantage of the fact that the ellipsoid is exponentially small along a certain direction  to somehow gain the new ability to produce a cut orthogonal to .
Convex functions become increasingly well-behaved on increasingly narrow regions, however, star-convex functions crucially do not (see item 3 in Section~\ref{sec:examples}).
Thus, as the thickness of the ellipsoid in direction  gets smaller, the structural properties of the function inside our ellipsoid \emph{do not} give us any new powers.
Strangely, we can take advantage of one new parameter regime that at first appears useless: relative to the thinness of the ellipsoid, we have exponentially much space in direction  \emph{outside} of the ellipsoid.
\end{enumerate}

To implement the intuition of Step 1 above, we define , a truncated and translated logarithm of the star-convex function , which maps the potentially exponentially large range of  to the (polynomial-sized) range , where  is defined below (Definition~\ref{def:Parameters}), and is slightly smaller than our function accuracy bound . In the below definition,  intuitively represents our estimate of the global optimum function value, and will record, essentially, the smallest function evaluation seen so far (see Algorithm~\ref{alg:SingleCut}).

\begin{definition}
\label{def:BlurredLog}
Given an objective function  with bound  when  and an offset value , we define the \emph{truncated logarithmic version of } to be

\end{definition}

While mapping to a small range,  nonetheless gives us a precise view of small changes in the function as we converge to the optimum. The next result shows that, if we ``blur"  by drawing  from a Gaussian distribution, then not only can we efficiently estimate the expected value of the ``blurred logarithm of ", we can also estimate the derivatives of this expectation with respect to changing either the mean or the variance of the Gaussian.

For an arbitrary bounded (measurable) function , the derivative of its expected value over a Gaussian of width  with respect to either a) moving the center of the Gaussian or b) changing its width , is bounded by . Thus we normalize the estimates below in terms of the product of the Gaussian width and the derivative, instead of estimating the derivative alone.



\begin{proposition}\label{prop:Sampling}

  Let  be a Gaussian with diagonal covariance matrix  consisting of elements .
  For an error bound  and a probability of error , we can estimate each of the following functions to within error  with probability at least  using  samples: 1) the expectation ; 2) the (scaled) derivative ; and 3) the derivative with respect to scaling . \emph{A fortiori}, these derivatives exist.
\end{proposition}
\begin{proof}

Chernoff bounds yield the first claim, since , so  samples suffice.

For the second claim, we note that the expectation can be rewritten as the integral over  of  times the probability density function of the normal distribution: . Thus the derivative of this expression with respect to the first coordinate  can be expressed by taking the derivative of the probability density function inside the integral, which ends up scaling it by the vector . Thus . The multiplier  is effectively bounded, because for real numbers , the probability  vanishes faster than . Thus we can pick  such that replacing the expression in the expectation, , by this expression clamped between  will change the expectation by less than . We can thus estimate the expectation of this clamped quantity to within error  with probability at least  via some sample of size , as desired.

The analysis for the third claim (the derivative with respect to ) is analogous: the derivative of the Gaussian probability density function with respect to  (as in the case of a univariate Gaussian of variance ) scales the probability density function by , and after scaling by , this expression measures the square of the number of standard deviations from the mean, and as above, the product  can be clamped to some polynomially-bounded interval  without changing the expectation by more than . The conclusion is analogous to the previous case.
\end{proof}
As mentioned in the previous section, our guiding aim for the ellipsoid method is to prevent any axis of the ellipsoid from getting too small.
Therefore, in order to treat axes differently depending on their length, we shall identify our basis as the unit vectors along the axes of the current ellipsoid and distinguish between axes that are smaller than  versus at least , where  is an exponentially small threshold for ``thinness", defined below in Definition~\ref{def:Parameters}.

\begin{definition}
\label{def:TopBottom}
  Given an ellipsoid, consider an orthonormal basis parallel to its axes. Each semi-principal axis of the ellipsoid whose length is less than , we call a ``thin dimension", and the rest are ``non-thin dimensions".
  Given a vector , we decompose it into  where  is non-zero only in the non-thin dimensions, and  is non-zero only in the thin dimensions.
Similarly, given the identity matrix , we decompose it into .
\end{definition}
We apply a scaling to the \emph{non-thin} dimensions so as to scale the non-thin semi-principal axes of the ellipsoid to unit vectors (making the ellipsoid a unit ball in the non-thin dimensions).
We keep the thin dimensions as they are.


We present again Proposition~\ref{prop:Alg1Correctness}, describing the guarantees on our cutting plane algorithm required by Algorithm~\ref{alg:Ellipsoid}, and then state the cutting plane algorithm, Algorithm~\ref{alg:SingleCut}.
Below, we make use of constants defined in Definition~\ref{def:Parameters}  that may be interpreted as follows:  is a polynomial number of mesh points;   is the mesh spacing;  is the minimum size of , a Gaussian width in the thin dimensions that is somewhat larger than , the size of the ellipsoid in the thin dimensions;  is a Gaussian width in the  (non-thin) dimensions, of inverse polynomial size;  is polynomially smaller than , and  is a polynomial quantity.  is a polynomial number of samples defined in the proof of Proposition~\ref{prop:Alg1Correctness}.
Moreover, the function  in the algorithm below is a sum of a probability and two derivatives of the blurred expectation of  (the truncated logarithm in Definition~\ref{def:BlurredLog}), and is defined in the statement of Lemma~\ref{lem:MarkovGadget}.


\begin{proposition}[Correctness of Algorithm~\ref{alg:SingleCut}]
\label{prop:Alg1Correctness}
With negligible probability of failure, Algorithm \ref{alg:SingleCut} either a) returns a Gaussian region  such that , or b) returns a direction , restricted to the  dimensions, such that when normalized to a unit vector , the cut  contains the global minimum.
\end{proposition}

\vspace{3mm}
\hspace*{-\parindent}\begin{minipage}{\linewidth}
\begin{framed}
\begin{myalgorithm}[Single cut with locked dimensions]
\label{alg:SingleCut}
Take an orthonormal basis for the ellipsoid, as in Definition~\ref{def:TopBottom}.
We apply an affine transformation so that a) the ellipsoid is centered at the origin, and b) the ellipsoid, when restricted to the  dimensions, is the unit ball.\\

\noindent\textbf{Input}: An ellipsoid containing the star center, under an affine transformation as above.\\
\textbf{Output}: \emph{Either} a) A cut direction  \emph{or} b) A Gaussian .

 \begin{enumerate}
 \item For each 
     \begin{enumerate}
     \item[1a.] Evaluate  at  samples from the Gaussian  of width  in the  dimensions and width  in the  dimensions (that is, ).
     \item[1b.] If at least  fraction of the evaluations are within  of the minimum evaluation (at this iteration ), then {\bf Return}  and {\bf Halt}.
     \end{enumerate}
 \item Otherwise, let  be the minimum of all samples in Step 1.
\item Repeatedly sample the following, estimating  to within  each time\vspace{-1mm} 
\begin{enumerate}\item[3a.]
    Accept the first pair  such that .
    \end{enumerate}
 \item {\bf Return} the gradient  \quad(the derivative as  changes, computed via Proposition~\ref{prop:Sampling}).
\end{enumerate}




\end{myalgorithm}
\end{framed}
\end{minipage}\vspace{3mm}

We note that a special case of the above algorithm is when there are no  (very thin, thinner than ) dimensions.
This applies, for example, at the beginning of the optimization process.





The rest of this section develops the mathematical analysis leading to the proof of Proposition~\ref{prop:Alg1Correctness}.


The following lemma (Lemma~\ref{lem:CutLB}) analyzes how  (the truncated logarithm of ) decreases as  moves towards the global optimum, or equivalently, how  increases as  moves away.
A crucial complicating factor is that we always average  over  drawn from a Gaussian, and moving the mean of a Gaussian away from the global optimum is not exactly the same thing as moving every point in the Gaussian away from the global optimum.
Lemma~\ref{lem:CutLB} expresses the effect of moving the mean away from the global optimum---restricted to the non-thin  dimensions---in terms of a positive probability, minus three confounding derivatives.
If we can show that the left hand side of the expression in the following lemma is positive, this means that a cut in the direction of the gradient of (a Gaussian-blurred)  in the  dimensions is guaranteed to contain the global minimum.
\begin{lemma}
\label{lem:CutLB}
For a star-convex function  with star center (global minimum) at the origin satisfying , and any mean , and variances ,

\end{lemma}

\begin{proof}
Let  be the function value at the global minimum . By the star-convexity of  with the origin as the star center,  for all , which implies , and thus . Thus the corresponding inequality holds for , provided,  and  is close enough to 1, so that  behaves like :


Consider the left hand side as a function ; rearranging to put the rightmost term on the left hand side, the inequality says that 

By Proposition~\ref{prop:Sampling},  has a derivative at , which equals  (by L'H\^{o}pital's rule), which by Equation~\ref{eq:prob} is thus greater than or equal to the probability .



Therefore

Rewriting the left hand side of the above inequality into the sum of the four derivatives in the lemma statement gives the result required.
\end{proof}

As explained above, if we are able to lower bound the left hand side of Lemma~\ref{lem:CutLB}, then we will be able to make a cut to the ellipsoid that contains the global optimum, while being perpendicular to the thin dimensions.
Therefore, we need to upper bound the three derivatives on the right hand side of the inequality of Lemma~\ref{lem:CutLB}.
Lemmas~\ref{lem:MovingInThinDimension}, \ref{lem:Isoperimetric}, and \ref{lem:SigmaTopExpectation} bound these three terms respectively.

In the following, we define the radius  to be slightly larger than  (see Definition~\ref{def:Parameters} below for details).
Lemma~\ref{lem:MovingInThinDimension} considers widths , in line with the operation of our Algorithm~\ref{alg:SingleCut}.

The next two lemmas directly bound the first two derivative terms on the right in the expression of Lemma~\ref{lem:CutLB}, via a direct calculation of how fast the average of an arbitrary function can change with respect to the Gaussian parameters.

\begin{lemma}
\label{lem:MovingInThinDimension}
For all  and all  and  we have:

\end{lemma}

\begin{proof}
Given ,  and , define  to be the probability measure of the distribution . We have

\end{proof}

\begin{lemma}
\label{lem:Isoperimetric}
For any  and  we have the following inequality:

where  is the number of  dimensions.
\end{lemma}

\begin{proof}
This proof is analogous to that of Lemma \ref{lem:MovingInThinDimension}, making use of Pinsker's inequality to bound the total variation distance between Gaussians of different variances, via their KL divergence.
\end{proof}

For our overall strategy of bounding each of the three terms in the right hand side of the inequality of Lemma~\ref{lem:CutLB}: the bound of Lemma~\ref{lem:MovingInThinDimension} we use as is, however the bound of Lemma~\ref{lem:Isoperimetric} is somewhat larger than 1, and needs to be improved (in order to ultimately compare it with ). We accomplish this with an averaging argument, where we sample the mean of our Gaussian from a somewhat larger Gaussian, which will effectively decrease the expectation of the left hand side of the expression in Lemma~\ref{lem:Isoperimetric} by the ratio of the variances of the two Gaussians. We analyze the effect of this ``double-sampling" process on the relevant quantities in Lemma~\ref{lem:DoubleSampling}.
\begin{lemma}
\label{lem:DoubleSampling}
For all  and  we have the following two equalities:

and
\end{lemma}

\begin{proof}
Because adding two Gaussian random variables produces a Gaussian random variable, the distribution

is equivalent to the distribution , which gives the first equality.
To prove the second equality, we move the derivative outside the expectation, and combine the two expectations into the equivalent expression 
Further observe that the derivatives of the  dependencies in the two expressions we are comparing are  and , which have ratio exactly , giving the second equality.
\end{proof}





Having bounded the first two ``confounding derivatives" from the right hand side of Lemma~\ref{lem:CutLB}, we now bound the third. We cannot directly bound this derivative, so we again employ an averaging argument. Intuitively, this term records how  increases as the width  increases; however, since  is bounded between  and , the derivative cannot stay large over a large range of . Crucially, we let  vary over a huge range outside the ellipsoid, between  and  (with  a polynomial factor, defined in Definition~\ref{def:Parameters}).
In order to give  a large enough range for the following bound to be meaningful,  (and hence , the threshold for ``thinness") has to be exponentially small, as specified below in Definition~\ref{def:Parameters}.
\begin{lemma}
\label{lem:SigmaTopExpectation}
For all ,

\end{lemma}

\begin{proof}
The probability density function of  is .
Let 
The following quantity thus equals the left hand side of the inequality in the lemma statement without the scaling factor :

since  and hence  is bounded between  and .
The lemma statement follows.
\end{proof}


Recalling that in Lemma~\ref{lem:MovingInThinDimension} we satisfactorily bounded the second term from the right hand side of the inequality of Lemma~\ref{lem:CutLB}, we now combine the results of the previous lemmas so that we may use Markov's inequality to bound the sum of the remaining three terms. The following lemma makes an assumption that the probability that  is bounded away from 1; in the context of our Algorithm~\ref{alg:SingleCut}, the case where this assumption is false turns out to imply that our algorithm has already successfully optimized the function---despite this algorithm being intended merely to seek another cut. This result is shown in Lemma~\ref{lem:Victory}.

\begin{lemma}
\label{lem:MarkovGadget}
Given  and , define an auxiliary function

Suppose we have  and  such that for all  we have .
Thus, letting  be the joint distribution of independent random variables  and , we have that

where

\end{lemma}

\begin{proof}
We bound the expectations of the individual terms in  when we draw .
For the first term, we make use of Lemma~\ref{lem:DoubleSampling} to simplify the double expectations.

The other two terms are bounded by Lemmas \ref{lem:Isoperimetric} (via Lemma~\ref{lem:DoubleSampling}) and \ref{lem:SigmaTopExpectation} respectively, yielding that . Having bounded the expectation, we now upper bound  so that we can apply Markov's inequality. The first term of  is a probability and hence is upper bounded by 1; the next two terms are each upper bounded via Lemma~\ref{lem:Isoperimetric} by . Thus .

Markov's inequality yields that .
\end{proof}

Using Lemma~\ref{lem:MarkovGadget} and the previous lemmas, we have successfully upper bounded the confounding derivatives and hence lower bounded the left hand side of Lemma~\ref{lem:CutLB}, but only when the probability term is bounded away from 0 for all .
As we mentioned above, in fact, if the probability term is too small for some , then it turns out we already have a Gaussian region that can be returned as the optimization output.
Intuitively, if  is ``flat'' (except on a small fraction of the points) on a large Gaussian region that is known to be near the global minimum (``near", relative to the size of the Gaussian), then the function value on this Gaussian is essentially the global minimum, and thus this Gaussian region may be returned as the overall answer to the optimization problem.
The next two lemmas formalize this result.

\begin{lemma}\label{lem:tail}
  Given a probability distribution on the positive real line with pdf proportional to , for  then any set with probability at least  under this distribution contains two points with ratio at least .
\end{lemma}
\begin{proof}

  The pdf has maximum at  value  (as can be found by differentiating the logarithm of ). We upper bound  by , since, taking the logarithm of both sides, the function value and first derivative match at  (the derivative is 0), while the second derivative of the (logarithm of the) right hand side equals  while the left hand side yields  which is smaller than  for all . Thus the integral of  from 0 to  is at most  times the integral of the Gaussian , which is at most .

  We correspondingly lower bound . We note the value  (specifying the  coordinate of the maximum of ) is bounded as . Thus for , Equation~\ref{eq:logf} yields the bound . Analogously to above this yields the lower bound that  for ---where the standard deviation of this Gaussian is now  instead of 1.

  Given this lower bound  for  and the upper bound  on the overall integral of , the probability of  lying in the interval  is at least  Similarly, the probability of  lying in the interval  is greater than .

  Thus, any set  of probability at least  on the distribution with pdf proportional to  must contain points from \emph{both} intervals  and . Hence  must contain two points with ratio at least .

  We conclude the lemma by bounding  in terms of . Let , which is at least  for . We have , which is easily seen to be at most  for . Thus  contains two points with ratio at least , as desired.


\end{proof}

\begin{lemma}
\label{lem:Victory}
Given a star-convex function  with global optimum at the origin, if for some location  we have  then
the function value at the global optimum, , satisfies .

\end{lemma}
\begin{proof}
  Consider a ray through the global minimum (at the origin), and let the ray be defined as all positive multiples of a unit vector . Consider the set  of scaling factors  such that . We note that if  contains two values  with some ratio  then by the star-convexity of  (on this ray), the global minimum of  must have value at least . Thus we want to show that there exists a direction  with a set  that contains two widely--spaced elements; namely, it is impossible for each  to only contain values within small ratio of each other.

  We note that we may express  in terms of the sets  as an integral in polar coordinates. Letting  denote the -dimensional sphere, this probability equals the ratio
  
  Consider those directions  with a positive component in the direction of ; this halfspace  comprises at least half the probability mass of the Gaussian , and thus, within this halfspace, . Suppose for the sake of contradiction that for each , defining   to be the component of  in the direction of  (where ), the set  does not contain any points of ratio at least . Thus by Lemma~\ref{lem:tail}, we have the bound . For each , with  as defined, the ratio between these integrals equals the ratio between the corresponding inner integrals of Equation~\ref{eq:integralRatio}, , yielding that the average value of this ratio over  in the halfspace  is at most . This contradicts the fact derived earlier that .

  Therefore there must exist a direction  such that  contains two points of ratio at least , where since , this ratio is thus at least . We then conclude .
\end{proof}

We now wish to combine Lemmas~\ref{lem:MarkovGadget} and~\ref{lem:Victory}, in the sense that we reason either one or the other applies.
The na\"{i}ve interpretation of the above would involve algorithmically checking the condition of Lemma~\ref{lem:Victory} for each and every Gaussian induced by all the  in the continuous (exponential) range as in Lemma~\ref{lem:MarkovGadget}, which is obviously impossible.
Therefore, we need to choose a mesh in that range such that the total variation distance between a Gaussian with a width  in the  dimensions and the closest Gaussian in the mesh is upper bounded by some quantity proportional to .
This allows us to reason that after sampling all points in the mesh, either Lemma~\ref{lem:Victory} applies, in which case we can conclude our optimization, or we can apply Lemma~\ref{lem:MarkovGadget} to produce a cut.
The following lemma shows that a geometric spacing in the mesh suffices, thus explaining Step 1 of Algorithm~\ref{alg:SingleCut}.

\begin{lemma}
\label{lem:Pinsker}
Given  and , let  be the probability measure of the distribution .
For , we have the inequalities

where  denotes the \emph{total variation distance} and  denotes the \emph{KL divergence}.
That is, if , then the difference in probabilities of event  happening under the two distributions is at most .
\end{lemma}

\begin{proof}
The first inequality is Pinsker's inequality.
A direct calculation from the standard expression for the KL-divergence of multivariate Gaussians shows the second inequality.
The third inequality is self-evident.
\end{proof}

We now have all the theoretical tools to reason about Algorithm~\ref{alg:SingleCut}.
In the following definition, we choose all the quantities we have used in the paper.
\begin{definition}\label{def:Parameters}

\end{definition}

Finally, we present the proof of Proposition~\ref{prop:Alg1Correctness} (restated below for convenience), which establishes the correctness of Algorithm~\ref{alg:SingleCut}.

\medskip\noindent{\bf Proposition~\ref{prop:Alg1Correctness}} (Correctness of Algorithm~\ref{alg:SingleCut}){\bf .} \emph{With negligible probability of failure, Algorithm \ref{alg:SingleCut} either a) returns a Gaussian region  such that , or b) returns a direction , restricted to the  dimensions, such that when normalized to a unit vector , the cut  contains the global minimum.
}




\begin{proof}
To keep the proof simpler, we now translate the coordinate system to place the \emph{global optimum} at the origin. If the probability error parameter , then make , which can only improve the results.



If the algorithm halts in Step 1b, this means that for the returned Gaussian ,  at least  fraction of the samples were within a range , denoting by  the smallest observed sample from . Chernoff bounds imply that (except with probability of failure ), the true probability  is at least , provided we take  samples.

Thus we apply Lemma~\ref{lem:Victory}, since , and conclude that , where we must now bound ``", which from the notation of Lemma~\ref{lem:Victory} means the ``distance between the global minimum and the center of the ellipsoid, measured by the number of standard deviations of Gaussian ". Specifically, in the  dimensions, the ellipsoid has radius 1 and the Gaussian  has radius ; in the  dimensions, the Gaussian has radius larger than the ellipsoid, so the distance in these directions is less than 1, which is certainly less than . Thus in total we bound . Plugging this bound into the conclusion of Lemma~\ref{lem:Victory}, where , we have , by the definition of . Thus, the returned Gaussian satisfies the desired properties of the output of our optimization algorithm: .

We now analyze the situation when the algorithm does \emph{not} halt in Step 1b. In this case, for each , the proportion of samples larger than  was observed to be at least . Since , the proportion larger than  must also be at least . As above, by Chernoff bounds, for each  (except with probability of failure ), the true probability  is at least , provided we take  samples.

Since as  ranges from  to , the Gaussians  vary exponentially in their width in the  dimensions, as , these widths form a fine (exponentially spaced) mesh over this entire region from  to . Thus, for \emph{any} , there is a an  for which the Gaussian  has width  such that . Thus for the Gaussian  we invoke Lemma~\ref{lem:Pinsker} to conclude that the difference in the probability of  between  and  is at most .

Thus for \emph{any} , letting , the probability  is at least . (Namely, given that the algorithm did not halt in Step 1b, we have a guarantee that holds over an \emph{exponentially} wide range of widths , despite only taking a polynomial () number of iterations to check, and a polynomial () number of samples from  per iteration.)


In order to apply Lemma~\ref{lem:MarkovGadget}, we need the tiny variant of the above claim, where instead of bounding the probability that , we instead need a bound on the probability that . However,  was chosen to be a truly huge number, such that we have the global guarantee of Definition~\ref{def:boundedness} that for all  within distance  of the origin, . We consider any case where our algorithm evaluates the function outside this ball to be a \emph{failure} of the algorithm. Because by the condition of Step 2e of Algorithm~\ref{alg:Ellipsoid}, the ellipsoid under consideration in Algorithm~\ref{alg:SingleCut} has semi-principal axes of length at most , and its center lies within  of the (original) origin, each point in the ellipsoid has distance at most  from the origin. Further, by construction, each Gaussian has standard deviation in the  dimensions bounded by the size of the ellipsoid over , and standard deviation in the  dimensions at most , where  is chosen so that, over the entire course of the algorithm, no Gaussian sample will ever be more than  standard deviations from its mean, except with negligible probability. Thus, except with negligible probability, all samples from the algorithm are in the region within  of the origin (as a very loose bound), and thus have function value . Having analyzed this negligible probability of failure, we assume for the rest of this proof that all function evaluations have magnitude less than , and condition all probabilities on the assumption that no failure has occurred.

Thus for any  we have , and
we may now invoke Lemma~\ref{lem:MarkovGadget}, in order to describe the properties of the function  at the center of Step 3. Using the parameters defined in Definition~\ref{def:Parameters}, the bound  of Lemma~\ref{lem:MarkovGadget} is found to equal . Lemma~\ref{lem:MarkovGadget} thus yields (with distribution  as defined in the lemma, and as used in Step 3 of Algorithm~\ref{alg:SingleCut}) that 

In Step 3, we estimate each of the three terms of  via Proposition~\ref{prop:Sampling}, and take enough samples to ensure that, except with negligible probability, our estimate of  is accurate to within . Thus for each sampled  such that , our estimate of  will be at least , and thus the condition in Step 3 will succeed.

Each iteration of Step 3 of the algorithm thus succeeds with probability at least , and Chernoff bounds imply that, except with failure probability , Step 3 will successfully terminate in  many iterations.

Given that our observed value of  when Step 3 terminates is at least  and our estimates are accurate to within , the true value of  must be at least .

We now show that the gradient  estimated in Step 4 has positive component in the direction away from the global optimum, which will enable us to make a cut. Lemma~\ref{lem:CutLB} shows that the component of the gradient at location  in the direction away from the global optimum is at least 

From the above bound  and the bound of Lemma~\ref{lem:MovingInThinDimension} the term we subtract from  is at most , we conclude that the component of the gradient in the direction away from the global optimum is at least .

We thus estimate the gradient in Step 4 by estimating the derivative in each of the dimensions in  to within error , which guarantees that the total gradient vector has component in the direction away from the global optimum at least . (As an implementation detail, we note that Proposition~\ref{prop:Sampling} computes a version of the derivative scaled by , to any desired accuracy , using samples scaling polynomially with . Thus for the unscaled derivative to have accuracy , the scaled derivative must have accuracy , which requires time polynomial to the inverse of this quantity. From the definition of  in Definition~\ref{def:Parameters}, its inverse is polynomial in the overall parameters of the algorithm.)

Thus our estimate of the gradient, normalized to a unit vector , defines a halfspace  that contains the global optimum. Further, since  came from a sample , where  (in Definition~\ref{def:Parameters}), and  was chosen so that no Gaussian sample will ever be more than  standard deviations from its mean, we have that . Thus , from which we conclude that the global optimum is contained in the halfspace , as desired.


\end{proof}




\bibliographystyle{plain}
\bibliography{STOC2016}
\newpage
\appendix
\section{The Scope of Star-Convex Functions}\label{sec:examples}
In this section we introduce some basic building blocks to construct large families of star-convex functions, with the aim of demonstrating the richness of the definition. More technical examples of star-convex functions are introduced in the rest of the paper to illustrate particular points (including Examples~\ref{ex:deterministic}, \ref{ex:rationals}, and Example~\ref{ex:output} below).

\medskip\noindent{\bf Basic Star-Convex Functions:}
\begin{itemize}
  \item[1.] Any convex function is star-convex.
  \item[2.] Even in 1 dimension, star-convex functions can be non-convex: 
  Or, for a Lipschitz twice-differentiable example,  (from Nesterov and Polyak~\cite{Nesterov:2006}).
  \item[3.] In  dimensions, take an \emph{arbitrary} positive function  on the unit sphere, and extend it to the origin in a way that is star-convex on each ray through the origin, for example, extending  linearly to define 
\end{itemize}


\medskip\noindent{\bf Ways to Combine Star-Convex Functions:}
Given any star-convex functions  that have star centers at , we can combine them to generate a new star convex function in the following ways:
\begin{itemize}
  \item[4.] A star-convex function can be shifted in the  and  senses so that its global optimum is at an arbitrary  and  value; in general, for any affine transformation  and real number , the function  is star-convex.
  \item[5.] For any positive power , the function  is star-convex.
  \item[6.] The sum  is star-convex.
  \item[7.] The product  is star-convex.
  \item[8.] The \emph{power mean}: for real number , the function  is star-convex, defining powers via limits as appropriate. (The case  corresponds to .)
\end{itemize}

\medskip\noindent{\bf Practical Examples of Star-Convex Function Classes:}

We combine the basic examples above to yield the following general classes of star-convex functions.

\begin{itemize}
  \item[9.] Sums of squares of monomials: the square of any monomial , with , will be star-convex about the origin, and thus sums of such terms will be star-convex despite typically being non-convex, for example . Sums-of-squares arise in many different contexts.
  \item[10.] More generally, any polynomial of  with nonnegative coefficients is star-convex.
  \item[11.] In the standard machine learning setting where one is trying to learn a parameter vector : for a given hypothesis , and each training example , the hypothesis gives an error  that is (typically) a convex function of its second argument, . Averaging these convex losses over all , via the power mean of exponent  leads to a star-convex loss, as a function of , with minimum value 0 at . The paper by one of the authors~\cite{Valiant:2014} discusses a class of such star-convex loss functions which have the form 
Note that, for , the loss function is convex.
\end{itemize}

\section{Example of a Pathological Star-Convex Function}\label{ap:examples}

See the discussion before Definition~\ref{def:problem}.

\begin{example}\label{ex:output}
We present a class of minimization problems where we argue that no algorithm will be able to return a (rational) point with objective function value near the global minimum, and instead, in order to be effective, an algorithm must return a region on which ``the function value is low with high probability".

  For a given  we define a class of minimization problems in the unit square, parameterized by three uniformly random real numbers . The star-convex function  will have unique global minimum at , and is defined as  where   denotes the fractional part of , and  denotes the angle of location  about the origin.

We first note that the ``if " condition is only possible if  is rational (since the input  must be rational), and hence this condition occurs with probability 0; this condition is part of the definition of  only to avoid making the entire line  a global minimum of . We thus ignore the  possibility in what follows.

Fixing , the probability with respect to the random choice of  that the evaluation of  falls into case 1 equals . Since  only affects  in case 1, if  is significantly larger than the runtime of the algorithm, then with high probability the algorithm will never observe an evaluation via case 1.

Given that the algorithm never observes an evaluation via case 1, and the only dependence of  on  is via case 1, the algorithm will have no information about , with high probability.

For any  that is -far from  (for some ), the function  will be at least , with probability at least  over choices of . Further, since regions of high function value appear every  radians about the origin, for any ball of radius  centered at  and \emph{any} , the the function  will take value at least  with probability close to  on a random point in the ball.

Thus, any optimization algorithm that runs in time  and outputs a set that can be decomposed into -radius balls can expect the function value on its output set to be low only with  probability. In short, we cannot expect any reasonable algorithm to return a set on which the function value is always near-optimal, or even near-optimal with probability 1. The best we can hope for is a polynomial relation between the error probability and the runtime, as we achieve in Theorem~\ref{thm:main}.

Letting the optimization algorithm specify output regions of double-exponentially small size does not help, since the set of angles for which case 1 applies could be replaced by the construction by Rudin~\cite{Rudin:1983} of a measurable set on , whose intersection with \emph{any} subinterval  has measure strictly between 0 and 
Furthermore, letting the optimization algorithm return rational points, instead of full-dimension sets, still does not help, since we could modify  in the style of Example~\ref{ex:rationals} so that its values on all the rationals with  coordinate more than  far from  are high.
\end{example}

\section{Details for Adapting the Ellipsoid Algorithm}\label{ap:ellipsoid}

In this section we fill in many standard adaptations of the ellipsoid algorithm omitted from the body of Section~\ref{sec:ellipsoid}, supporting the analysis of Algorithm~\ref{alg:Ellipsoid}.


In analogy with the standard ellipsoid algorithm, the following lemma provides a means to construct a new ellipsoid from the current ellipsoid and a cutting plane.
Such scenarios can always be affinely transformed into cutting an origin-centered unit ball along a basis direction.
\begin{lemma}
\label{lem:NewEllipsoid}
Consider the unit ball  in -dimensional space centered at the origin, and the half-space , where  denotes the coordinate of  in the  dimension.
Then the ellipsoid  contains the intersection , where

\end{lemma}

\begin{proof}
Consider an arbitrary point , then we have .
Thus, it suffices to show that the function

satisfies  for all .

The proof is simple.
The two roots of  are  and .
Also, , and since  is quadratic in , it must be the case that  for all .
Note that since,  for all , we have  for all .
\end{proof}

We also want to upper bound the volume of the new ellipsoid relative to the original one, in order to show a fixed ratio of volume decrease at each round.
\begin{lemma}
\label{lem:Volume}
The volume of the ellipsoid constructed in Lemma \ref{lem:NewEllipsoid} is less than  times that of the unit ball.
\end{lemma}

\begin{proof}
The ratio between the volumes is

\end{proof}

The next two lemmas demonstrate the straightforward constructions required by Step 2e in Algorithm~\ref{alg:Ellipsoid} to keep all semi-principal axes of the feasible ellipsoid bounded by , and keep the ellipsoid centered within distance  of the origin.

\begin{lemma}\label{lem:resizing}
  Let  be an ellipsoid in  consisting of the points  such that , namely with center  and semi-principal axis lengths specified by the vector . If the first  semi-principal axes have lengths  (for ), then replacing these axes with , multiplying the remaining axes () by , and replacing the first  elements of the center  with 0 yields an ellipsoid  defined by 
  that has smaller volume than  and contains the entire intersection of  with the unit ball.
\end{lemma}
\begin{proof}
  Let  be the -dimensional ellipsoid, defined as  restricted to dimensions , namely, points  satisfying , and let  be the -dimensional unit ball centered at the origin. Then the intersection of  with the -dimensional unit ball is contained in the cartesian product . We now show that  contains this cartesian product, by showing that points in this cartesian product satisfy Equation~\ref{eq:ellipsoid-E}.

  Consider a point . The first  coordinates of , corresponding to , satisfy , and thus contribute to Equation~\ref{eq:ellipsoid-E} at most . The remaining coordinates of , corresponding to , satisfy  and thus contribute to Equation~\ref{eq:ellipsoid-E} at most . Adding these two bounds yields that, for points , the left hand side of Equation~\ref{eq:ellipsoid-E} is at most , as desired, proving the lemma.
\end{proof}

\begin{lemma}\label{lem:recentering}
  The intersection of a ball  and an ellipsoid  is contained within the intersection of the ellipsoid and a version of the ball translated to be centered at the nearest point of  to the center of .
\end{lemma}

Lemma~\ref{lem:recentering}, interpreted under an affine transformation, and combined with Lemma~\ref{lem:resizing}, shows that we can run the ellipsoid method with every ellipsoid guaranteed to have axes bounded by , and always centered inside the ball of radius .





The following two lemmas give important invariants that the algorithm maintains throughout the iterations.

\begin{lemma}
The global minimum is contained in the ellipsoid  for all .
\end{lemma}

\begin{proof}
We prove by induction.
The base case is trivial.

For the inductive case, assume the lemma is true for the ellipsoid .
Apply an affine transformation such that the  is a unit ball.
By assumption, the cut  satisfies the property that, taking into account the affine transformation, the half-space  contains the global minimum.
Since the global minimum is also contained in  by assumption, it must be contained in  by Lemma \ref{lem:NewEllipsoid}.
\end{proof}

\begin{lemma}
\label{lem:AxesLB}
No semi-principal axis of any ellipsoid  is ever less than .
\end{lemma}

\begin{proof}
We prove by induction.
The base case is again trivial.

For the inductive case, assume the lemma is true for the ellipsoid .
For simplicity, when we refer to thin (less than ) and non-thin (at least ) directions, they are always with respect to the lengths of the axes of .
Observe that, by construction, all the thin directions in the axes of  are also axes directions of .
Also, by Step 2(c) in Algorithm \ref{alg:Ellipsoid},  always contain the center of .
Therefore, the semi-principal axes of  in the thin directions must be longer than that of , which is at least the quantity in the lemma statement by the induction hypothesis.
Now consider an arbitrary axis  of  that is not in any of the thin directions.
The direction of this axis is a linear combination of the non-thin directions in the axes of .
Therefore, the diameter of  in the direction of  is at least .
By construction of the cut, , and so
,
completing the proof of the lemma.
\end{proof}

We restate and prove Lemma~\ref{lem:EllipsoidHalt}, showing that Algorithm~\ref{alg:Ellipsoid} halts after the designated number of iterations.
\begin{lemma*}[Lemma~\ref{lem:EllipsoidHalt}]
Algorithm~\ref{alg:Ellipsoid} halts within  iterations either through Step 2a or Step 2c.
\end{lemma*}

\begin{proof}
By Lemma~\ref{lem:Volume} and induction, after  iterations, the volume of ellipsoid  is at most  times the volume of the input radius  ball.
Since, by Lemma~\ref{lem:AxesLB}, all its semi-principal axes have length at least , all the axes must also be of length at most .

Therefore, the algorithm will definitely halt in Step 2a of iteration , if it has not already done so in the previous iterations.
\end{proof}

\iffalse
\section{Details of the Proof of Proposition~\ref{prop:Alg1Correctness}}
\label{sec:SingleCutProof}

We first present the statement and proof of Proposition~\ref{prop:Sampling}.

\medskip\noindent{\bf Proposition~\ref{prop:Sampling}}
{\bf .} \emph{Let  be a Gaussian with diagonal covariance matrix  consisting of elements .
  For an error bound  and a probability of error , we can estimate each of the following functions to within error  with probability at least  using  samples: 1) the expectation ; 2) the (scaled) derivative ; and 3) the derivative with respect to scaling . \emph{A fortiori}, these derivatives exist.}

\begin{proof}[Proof of Proposition~\ref{prop:Sampling}]

Chernoff bounds yield the first claim, since , so  samples suffice.

For the second claim, we note that the expectation can be rewritten as the integral over  of  times the probability density function of the normal distribution: . Thus the derivative of this expression with respect to the first coordinate  can be expressed by taking the derivative of the probability density function inside the integral, which ends up scaling it by the vector . Thus . The multiplier  is effectively bounded, because for real numbers , the probability  vanishes faster than . Thus we can pick  such that replacing the expression in the expectation, , by this expression clamped between  will change the expectation by less than . We can thus estimate the expectation of this clamped quantity to within error  with probability at least  via some sample of size , as desired.

The analysis for the third claim (the derivative with respect to ) is analogous: the derivative of the Gaussian probability density function with respect to  (as in the case of a univariate Gaussian of variance ) scales the probability density function by , and after scaling by , this expression measures the square of the number of standard deviations from the mean, and as above, the product  can be clamped to some polynomially-bounded interval  without changing the expectation by more than . The conclusion is analogous to the previous case.
\end{proof}

This section develops the mathematical analysis needed, which ultimately leads to the proof of Proposition~\ref{prop:Alg1Correctness}.
We state again the proposition below.

\medskip\noindent{\bf Proposition~\ref{prop:Alg1Correctness}} (Correctness of Algorithm~\ref{alg:SingleCut}){\bf .} \emph{With negligible probability of failure, Algorithm \ref{alg:SingleCut} either a) returns a Gaussian region  such that , or b) returns a direction , restricted to the  dimensions, such that when normalized to a unit vector , the cut  contains the global minimum.
}

The following lemma (Lemma~\ref{lem:CutLB}) analyzes how  (the truncated logarithm of ) decreases as  moves towards the global optimum, or equivalently, how  increases as  moves away.
A crucial complicating factor is that we always average  over  drawn from a Gaussian, and moving the mean of a Gaussian away from the global optimum is not exactly the same thing as moving every point in the Gaussian away from the global optimum.
Lemma~\ref{lem:CutLB} expresses the effect of moving the mean away from the global optimum---restricted to the non-thin  dimensions---in terms of a positive probability, minus three confounding derivatives.
If we can show that the left hand side of the expression in the following lemma is positive, this means that a cut in the direction of the gradient of  in the  dimensions is guaranteed to contain the global minimum.
\begin{lemma}
\label{lem:CutLB}
For a star-convex function  with star center (global minimum) at the origin satisfying , and any mean , and variances ,

\end{lemma}

\begin{proof}
Let  be the function value at the global minimum . By the star-convexity of  with the origin as the star center,  for all , which implies , and thus . Thus the corresponding inequality holds for , provided,  and  is close enough to 1, so that  behaves like :


Consider the left hand side as a function ; rearranging to put the rightmost term on the left hand side, the inequality says that 

By Proposition~\ref{prop:Sampling},  has a derivative at , which equals  (by L'H\^{o}pital's rule), which by Equation~\ref{eq:prob} is thus greater than or equal to the probability .



Therefore

Rewriting the left hand side of the above inequality into the sum of the four derivatives in the lemma statement gives the result required.
\end{proof}

As explained above, if we are able to lower bound the left hand side of Lemma~\ref{lem:CutLB}, then we will be able to make a cut to the ellipsoid that contains the global optimum, while being perpendicular to the thin dimensions.
Therefore, we need to upper bound the three derivatives on the right in the expression of Lemma~\ref{lem:CutLB}.
Lemmas~\ref{lem:MovingInThinDimension}, \ref{lem:Isoperimetric}, and \ref{lem:SigmaTopExpectation} bound these three terms respectively.

In the following, we define the radius  to be slightly larger than  (see Definition~\ref{def:Parameters} below for details).
Lemma~\ref{lem:MovingInThinDimension} considers widths , in line with the operation of our Algorithm~\ref{alg:SingleCut}.

The next two lemmas directly bound the first two derivative terms on the right in the expression of Lemma~\ref{lem:CutLB}, via a direct calculation of how fast the average of an arbitrary function can change with respect to the Gaussian parameters.

\begin{lemma}
\label{lem:MovingInThinDimension}
For all  and all  and  we have:

\end{lemma}

\begin{proof}
Given ,  and , define  to be the probability measure of the distribution . We have

\end{proof}

\begin{lemma}
\label{lem:Isoperimetric}
For any  and  we have the following inequality:

where  is the number of  dimensions.
\end{lemma}

\begin{proof}
This proof is analogous to that of Lemma \ref{lem:MovingInThinDimension}, making use of Pinsker's inequality to bound the total variation distance between Gaussians of different variances, via their KL divergence.
\end{proof}

For our overall strategy of bounding each of the three terms in the right hand side of the inequality of Lemma~\ref{lem:CutLB}: the bound of Lemma~\ref{lem:MovingInThinDimension} we use as is, however the bound of Lemma~\ref{lem:Isoperimetric} is somewhat larger than 1, and needs to be improved (in order to ultimately compare it with ). We accomplish this with an averaging argument, where we sample the mean of our Gaussian from a somewhat larger Gaussian, which will effectively decrease the expectation of the left hand side of the expression in Lemma~\ref{lem:Isoperimetric} by the ratio of the variances of the two Gaussians. We analyze the effect of this ``double-sampling" process on the relevant quantities in Lemma~\ref{lem:DoubleSampling}.
\begin{lemma}
\label{lem:DoubleSampling}
For all  and  we have the following two equalities:

and
\end{lemma}

\begin{proof}
Because adding two Gaussian random variables produces a Gaussian random variable, the distribution

is equivalent to the distribution , which gives the first equality.
To prove the second equality, we move the derivative outside the expectation, and combine the two expectations into the equivalent expression 
Further observe that the derivatives of the  dependencies in the two expressions we are comparing are  and , which have ratio exactly , giving the second equality.
\end{proof}





Having bounded the first two ``confounding derivatives" from the right hand side of Lemma~\ref{lem:CutLB}, we now bound the third. We cannot directly bound this derivative, so we again employ an averaging argument. Intuitively, this term records how  increases as the width  increases; however, since  is bounded between  and , the derivative cannot stay large over a large range of . Crucially, we let  vary over a huge range outside the ellipsoid, between  and  (with  a polynomial factor, defined in Definition~\ref{def:Parameters}).
In order to give  a large enough range for the following bound to be meaningful,  (and hence , the threshold for ``thinness") has to be exponentially small, as specified below in Definition~\ref{def:Parameters}.
\begin{lemma}
\label{lem:SigmaTopExpectation}
For all ,

\end{lemma}

\begin{proof}
The probability density function of  is .
Let 
The following quantity thus equals the left hand side of the inequality in the lemma statement without the scaling factor :

since  and hence  is bounded between  and .
The lemma statement follows.
\end{proof}


Recalling that in Lemma~\ref{lem:MovingInThinDimension} we satisfactorily bounded the second term from the right hand side of the inequality of Lemma~\ref{lem:CutLB}, we now combine the results of the previous lemmas so that we may use Markov's inequality to bound the sum of the remaining three terms. The following lemma makes an assumption that the probability that  is bounded away from 1; in the context of our Algorithm~\ref{alg:SingleCut}, the case where this assumption is false turns out to imply that our algorithm has already successfully optimized the function---despite this algorithm being intended merely to seek another cut. This result is shown in Lemma~\ref{lem:Victory}.

\begin{lemma}
\label{lem:MarkovGadget}
Given  and , define an auxiliary function

Suppose we have  and  such that for all  we have .
Thus, letting  be the joint distribution of independent random variables  and , we have that

where

\end{lemma}

\begin{proof}
We bound the expectations of the individual terms in  when we draw .
For the first term, we make use of Lemma~\ref{lem:DoubleSampling} to simplify the double expectations.

The other two terms are bounded by Lemmas \ref{lem:Isoperimetric} (via Lemma~\ref{lem:DoubleSampling}) and \ref{lem:SigmaTopExpectation} respectively, yielding that . Having bounded the expectation, we now upper bound  so that we can apply Markov's inequality. The first term of  is a probability and hence is upper bounded by 1; the next two terms are each upper bounded via Lemma~\ref{lem:Isoperimetric} by . Thus .

Markov's inequality yields that .
\end{proof}

Using Lemma~\ref{lem:MarkovGadget} and the previous lemmas, we have successfully lower bounded the left hand side of Lemma~\ref{lem:CutLB}, but only when the probability term is bounded away from 0 for all s.
As we mentioned above, in fact, if the probability term is too small for some , then it turns out we already have a Gaussian region that can be returned as the optimization output.
Intuitively, it is because
The next two lemmas formalize this result.

\begin{lemma}\label{lem:tail}
  Given a probability distribution on the positive real line with pdf proportional to , for  then any set with probability at least  under this distribution contains two points with ratio at least .
\end{lemma}
\begin{proof}

  The pdf has maximum at  value  (as can be found by differentiating the logarithm of ). We upper bound  by , since, taking the logarithm of both sides, the function value and first derivative match at  (the derivative is 0), while the second derivative of the (logarithm of the) right hand side equals  while the left hand side yields  which is smaller than  for all . Thus the integral of  from 0 to  is at most  times the integral of the Gaussian , which is at most .

  We correspondingly lower bound . We note the value  (specifying the  coordinate of the maximum of ) is bounded as . Thus for , Equation~\ref{eq:logf} yields the bound . Analogously to above this yields the lower bound that  for ---where the standard deviation of this Gaussian is now  instead of 1.

  Given this lower bound  for  and the upper bound  on the overall integral of , the probability of  lying in the interval  is at least  Similarly, the probability of  lying in the interval  is greater than .

  Thus, any set  of probability at least  on the distribution with pdf proportional to  must contain points from \emph{both} intervals  and . Hence  must contain two points with ratio at least .

  We conclude the lemma by bounding  in terms of . Let , which is at least  for . We have , which is easily seen to be at most  for . Thus  contains two points with ratio at least , as desired.


\end{proof}

\begin{lemma}
\label{lem:Victory}
Given a star-convex function  with global optimum at the origin, if for some location  we have  then
the function value at the global optimum, , satisfies .

\end{lemma}
\begin{proof}
  Consider a ray through the global minimum (at the origin), and let the ray be defined as all positive multiples of a unit vector . Consider the set  of scaling factors  such that . We note that if  contains two values  with some ratio  then by the star-convexity of  (on this ray), the global minimum of  must have value at least . Thus we want to show that there exists a direction  with a set  that contains two widely--spaced elements; namely, it is impossible for each  to only contain values within small ratio of each other.

  We note that we may express  in terms of the sets  as an integral in polar coordinates. Letting  denote the -dimensional sphere, this probability equals the ratio
  
  Consider those directions  with a positive component in the direction of ; this halfspace  comprises at least half the probability mass of the Gaussian , and thus, within this halfspace, . Suppose for the sake of contradiction that for each , defining   to be the component of  in the direction of  (where ), the set  does not contain any points of ratio at least . Thus by Lemma~\ref{lem:tail}, we have the bound . For each , with  as defined, the ratio between these integrals equals the ratio between the corresponding inner integrals of Equation~\ref{eq:integralRatio}, , yielding that the average value of this ratio over  in the halfspace  is at most . This contradicts the fact derived earlier that .

  Therefore there must exist a direction  such that  contains two points of ratio at least , where since , this ratio is thus at least . We then conclude .
\end{proof}

We now wish to combine Lemmas~\ref{lem:MarkovGadget} and~\ref{lem:Victory}, in the sense that we reason either one or the other applies.
The na\"{i}ve interpretation of the above would involve algorithmically checking the condition of Lemma~\ref{lem:Victory} for each and every Gaussian induced by all the  in the continuous (exponential) range as in Lemma~\ref{lem:MarkovGadget}, which is obviously impossible.
Therefore, we need to choose a mesh in that range such that the total variation distance between a Gaussian with a width  in the  dimensions and the closest Gaussian in the mesh is upper bounded by some probability proportional to .
This allows us to reason that after sampling all points in the mesh, either Lemma~\ref{lem:Victory} applies, in which case we can conclude our optimization, or we can apply Lemma~\ref{lem:MarkovGadget} to produce a cut.
The following lemma shows that the spacing in the mesh is exponential in the total variation distance upper bound, thus explaining Step 1 of Algorithm~\ref{alg:SingleCut}.

\begin{lemma}
\label{lem:Pinsker}
Given  and , let  be the probability measure of the distribution .
For , we have the inequalities

where  denotes the \emph{total variation distance} and  denotes the \emph{KL divergence}.
That is, if , then the difference in probabilities of event  happening under the two distributions is at most .
\end{lemma}

\begin{proof}
The first inequality is Pinsker's inequality.
A direct calculation from the standard expression for the KL-divergence of multivariate Gaussians shows the second inequality.
The third inequality is self-evident.
\end{proof}

We now have all the theoretical tools to reason about Algorithm~\ref{alg:SingleCut}.
In the following definition, we choose all the quantities we have used in the paper.
\begin{definition}\label{def:Parameters}

\end{definition}

Finally, we present the proof of Proposition~\ref{prop:Alg1Correctness} (restated below for convenience), which establishes the correctness of Algorithm~\ref{alg:SingleCut}.

\medskip\noindent{\bf Proposition~\ref{prop:Alg1Correctness}} (Correctness of Algorithm~\ref{alg:SingleCut}){\bf .} \emph{With negligible probability of failure, Algorithm \ref{alg:SingleCut} either a) returns a Gaussian region  such that , or b) returns a direction , restricted to the  dimensions, such that when normalized to a unit vector , the cut  contains the global minimum.
}




\begin{proof}
To keep the proof simpler, we now translate the coordinate system to place the \emph{global optimum} at the origin. If the probability error parameter , then make , which can only improve the results.



If the algorithm halts in Step 1b, this means that for the returned Gaussian ,  at least  fraction of the samples were within a range , denoting by  the smallest observed sample from . Chernoff bounds imply that (except with probability of failure ), the true probability  is at least , provided we take  samples.

Thus we apply Lemma~\ref{lem:Victory}, since , and conclude that , where we must now bound ``", which from the notation of Lemma~\ref{lem:Victory} means the ``distance between the global minimum and the center of the ellipsoid, measured by the number of standard deviations of Gaussian ". Specifically, in the  dimensions, the ellipsoid has radius 1 and the Gaussian  has radius ; in the  dimensions, the Gaussian has radius larger than the ellipsoid, so the distance in these directions is less than 1, which is certainly less than . Thus in total we bound . Plugging this bound into the conclusion of Lemma~\ref{lem:Victory}, where , we have , by the definition of . Thus, the returned Gaussian satisfies the desired properties of the output of our optimization algorithm: .

We now analyze the situation when the algorithm does \emph{not} halt in Step 1b. In this case, for each , the proportion of samples larger than  was observed to be at least . Since , the proportion larger than  must also be at least . As above, by Chernoff bounds, for each  (except with probability of failure ), the true probability  is at least , provided we take  samples.

Since as  ranges from  to , the Gaussians  vary exponentially in their width in the  dimensions, as , these widths form a fine (exponentially spaced) mesh over this entire region from  to . Thus, for \emph{any} , there is a an  for which the Gaussian  has width  such that . Thus for the Gaussian  we invoke Lemma~\ref{lem:Pinsker} to conclude that the difference in the probability of  between  and  is at most .

Thus for \emph{any} , letting , the probability  is at least . (Namely, given that the algorithm did not halt in Step 1b, we have a guarantee that holds over an \emph{exponentially} wide range of widths , despite only taking a polynomial () number of iterations to check, and a polynomial () number of samples from  per iteration.)


In order to apply Lemma~\ref{lem:MarkovGadget}, we need the tiny variant of the above claim, where instead of bounding the probability that , we instead need a bound on the probability that . However,  was chosen to be a truly huge number, such that we have the global guarantee of Definition~\ref{def:boundedness} that for all  within distance  of the origin, . We consider any case where our algorithm evaluates the function outside this ball to be a \emph{failure} of the algorithm. Because by the condition of Step 2e of Algorithm~\ref{alg:Ellipsoid}, the ellipsoid under consideration in Algorithm~\ref{alg:SingleCut} has semi-principal axes of length at most , and its center lies within  of the (original) origin, each point in the ellipsoid has distance at most  from the origin. Further, by construction, each Gaussian has standard deviation in the  dimensions bounded by the size of the ellipsoid over , and standard deviation in the  dimensions at most , where  is chosen so that, over the entire course of the algorithm, no Gaussian sample will ever be more than  standard deviations from its mean, except with negligible probability. Thus, except with negligible probability, all samples from the algorithm are in the region within  of the origin (as a very loose bound), and thus have function value . Having analyzed this negligible probability of failure, we assume for the rest of this proof that all function evaluations have magnitude less than , and condition all probabilities on the assumption that no failure has occurred.

Thus for any  we have , and
we may now invoke Lemma~\ref{lem:MarkovGadget}, in order to describe the properties of the function  at the center of Step 3. Using the parameters defined in Definition~\ref{def:Parameters}, the bound  of Lemma~\ref{lem:MarkovGadget} is found to equal . Lemma~\ref{lem:MarkovGadget} thus yields (with distribution  as defined in the lemma, and as used in Step 3 of Algorithm~\ref{alg:SingleCut}) that 

In Step 3, we estimate each of the three terms of  via Proposition~\ref{prop:Sampling}, and take enough samples to ensure that, except with negligible probability, our estimate of  is accurate to within . Thus for each sampled  such that , our estimate of  will be at least , and thus the condition in Step 3 will succeed.

Each iteration of Step 3 of the algorithm thus succeeds with probability at least , and Chernoff bounds imply that, except with failure probability , Step 3 will successfully terminate in  many iterations.

Given that our observed value of  when Step 3 terminates is at least  and our estimates are accurate to within , the true value of  must be at least .

We now show that the gradient  estimated in Step 4 has positive component in the direction away from the global optimum, which will enable us to make a cut. Lemma~\ref{lem:CutLB} shows that the component of the gradient at location  in the direction away from the global optimum is at least 

From the above bound  and the bound of Lemma~\ref{lem:MovingInThinDimension} the term we subtract from  is at most , we conclude that the component of the gradient in the direction away from the global optimum is at least .

We thus estimate the gradient in Step 4 by estimating the derivative in each of the dimensions in  to within error , which guarantees that the total gradient vector has component in the direction away from the global optimum at least . (As an implementation detail, we note that Proposition~\ref{prop:Sampling} computes a version of the derivative scaled by , to any desired accuracy , using samples scaling polynomially with . Thus for the unscaled derivative to have accuracy , the scaled derivative must have accuracy , which requires time polynomial to the inverse of this quantity. From the definition of  in Definition~\ref{def:Parameters}, its inverse is polynomial in the overall parameters of the algorithm.)

Thus our estimate of the gradient, normalized to a unit vector , defines a halfspace  that contains the global optimum. Further, since  came from a sample , where  (in Definition~\ref{def:Parameters}), and  was chosen so that no Gaussian sample will ever be more than  standard deviations from its mean, we have that . Thus , from which we conclude that the global optimum is contained in the halfspace , as desired.


\end{proof}


\fi



\end{document}
