\def\year{2018}\relax
\documentclass[letterpaper]{article} \usepackage{aaai18}  \usepackage{times}  \usepackage{helvet}  \usepackage{courier}  \usepackage{url}  \usepackage{graphicx}  \usepackage{multirow}
\frenchspacing  \setlength{\pdfpagewidth}{8.5in}  \setlength{\pdfpageheight}{11in}  \pdfinfo{
/Title (Empower Sequence Labeling with Task-Aware Neural Language Model)
/Author (Liyuan Liu, Jingbo Shang, Xiang Ren, Frank F. Xu, Huan Gui, Jian Peng, Jiawei Han)}
\setcounter{secnumdepth}{0}

\newcommand{\nop}[1]{}

\usepackage{amsmath, amsfonts, amssymb, xspace, color,listings,enumitem,graphicx}

\newcommand{\our}{\textsc{LM-LSTM-CRF}\xspace}
\newcommand{\ournl}{\textsc{LM-LSTM-CRF\_NL}\xspace}
\newcommand{\ournh}{\textsc{LM-LSTM-CRF\_NH}\xspace}
\newcommand{\sbef}{}
\newcommand{\saft}{}

\def \myspace {\_}
\def \c {\mathbf{c}}
\def \d {\mathbf{d}}
\def \e {\mathbf{e}}
\def \f {\mathbf{f}}
\def \g {\mathbf{g}}
\def \h {\mathbf{h}}
\def \l {\mathbf{l}}
\def \m {\mathbf{m}}
\def \n {\mathbf{n}}
\def \p {\mathbf{p}}
\def \q {\mathbf{q}}
\def \r {\mathbf{r}}
\def \t {\mathbf{t}}
\def \u {\mathbf{u}}
\def \v {\mathbf{v}}
\def \w {\mathbf{w}}
\def \x {\mathbf{x}}
\def \y {\mathbf{y}}
\def \z {\mathbf{z}}
\def \A {\mathbf{A}}
\def \H {\mathbf{H}}
\def \I {\mathbf{I}}
\def \L {\mathbf{L}}
\def \P {\mathbf{P}}
\def \U {\mathbf{U}}
\def \W {\mathbf{W}}
\def \Y {\mathbf{Y}}
\def \Z {\mathbf{Z}}
\def \fl {\mathbf{f^{L}}}
\def \fw {\mathbf{f^{N}}}
\def \rl {\mathbf{r^{L}}}
\def \rw {\mathbf{r^{N}}}



\begin{document}

\title{Empower Sequence Labeling with Task-Aware Neural Language Model}


\author{
Liyuan Liu
Jingbo Shang
Xiang Ren 
Frank F. Xu
Huan Gui
Jian Peng
Jiawei Han
\
p(\hat{\y}|\x_i, \c_i) = \frac{ \prod_{j=1}^n \phi(\hat{y}_{j-1}, \hat{y}_j, \z_j) } { \sum_{\y' \in \Y(\Z)} \prod_{j=1}^n \phi(y'_{j-1}, y'_j, \z_j)}
\label{eqn:crf_prob}

\mathcal{J}_{CRF} = -\sum_i \log p(\y_i | \Z_i)
\label{eqn:crf}

\y^* = \arg \underset{\y \in \Y(\Z)}{{\max}} \; p(\y | \Z)
\label{eqn:decode}

p_f(x_1, ..., x_n) = \prod_{i=1}^N p_f(x_i| x_1, \dots, x_{i-1})

p_f(x_i| c_{0, \myspace}, \dots, c_{i-1, \myspace}) = \frac{\exp(\w_{x_i}^T \fw_{i-1})}{\sum_{\hat{x}_j} \exp(\w_{\hat{x}_j}^T \fw_{i-1})}

&p_r(x_1, ..., x_n) = \prod_{i=1}^N p_r(x_i| c_{i+1, \myspace}, \dots, c_{n, \myspace}) \\
\mbox{where }& p_r(x_i| c_{i+1, \myspace}, \dots, c_{n, \myspace}) = \frac{\exp(\w_{x_i}^T \rw_{i})}{\sum_{\hat{x}_j} \exp(\w_{\hat{x}_j}^T \rw_{i})}

\mathcal{J}_{LM} = -\sum_{i} \log p_f(\x_i) -\sum_{i} \log p_r(\x_i)
\label{eqn:lm}

\mathcal{J} = -\sum_{i} \Big( p(\y_i | \Z_i) + \lambda \big( \log p_f(\x_i) + \log p_r(\x_i) \big) \Big)
\label{eqn:obj}
^\ddagger\pm^\dagger^\ddagger\pm^\dagger^\ddagger\pm_1\dagger\ddagger4000X40962819210242048512_1_1_1_1\pm\pm\pm^\dagger^\dagger^\ddagger\pm\pm\dagger\ddagger_1_1^\star^\star_1_11/51/2_1\pm^\dagger^\dagger^\dagger^\dagger^\ddagger\pm^\dagger\pm^\dagger\pm^\dagger^\ddagger\pm_1\dagger\ddagger_1\cdot\cdot\cdot\cdot\cdot\cdot^\star^\star_1\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm_1200200\mathcal{J}_{CRF}\mathcal{J}_{CRF}\mathcal{J}_{LM}_1\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm$0.30 \\
\hline

\end{tabular}
}
\caption{Effect of language model and highway}\label{tbl:hw}
\end{table}

\noindent\textbf{Leveraging Additional Information.}
Integrating word-level and character-level knowledge has been proved to be helpful to sequence labeling tasks.
For example, word embeddings~\cite{mikolov2013distributed,pennington2014glove} can be utilized by co-training or pre-training strategies~\cite{2017arXiv170700166L,2016naacl}.
However, none of these models utilizes the character-level knowledge.
Although directly adopting character-level pre-trained language models could be helpful~\cite{peters2017semi}.
Such pre-trained knowledge is not task-specific and requires a larger neural network, external corpus, and longer training. 
Our model leverages both word-level and character-level knowledge through a co-training strategy, which leads to a concise, effective, and efficient neural network.
Besides, unlike other multi-task learning methods, our model has no reliance on any extra annotation~\cite{peters2017semi} or any knowledge base~\cite{shang2017automated}. 
Instead, it extracts knowledge from the self-contained order information.


\section{Conclusion}
\label{sect:con}

In this paper, we proposed a sequence labeling framework, \our, which effectively leverages the language model to extract character-level knowledge from the self-contained order information.
Highway layers are incorporated to overcome the discordance issue of the naive co-training
Benefited from the effectively captured such task-specific knowledge, we can build a much more concise model, thus yielding much better efficiency without loss of effectiveness (achieved the state-of-the-art on three benchmark datasets) . 
In the future, we plan to further extract and incorporate knowledge from other ``unsupervised'' learning principles and empower more sequence labeling tasks.


\section*{Acknowledgments}
\label{sect:ack}

We thank Junliang Guo, Cheng Cheng and all reviewers for comments on earlier drafts that led to substantial improvements in the final version.
Research was sponsored in part by the U.S. Army Research Lab. under Cooperative Agreement No. W911NF-09-2-0053 (NSCTA), National Science Foundation IIS 16-18481, IIS 17-04532, and IIS-17-41317, grant 1U54GM114838 awarded by NIGMS through funds provided by the trans-NIH Big Data to Knowledge (BD2K) initiative (www.bd2k.nih.gov), and Google PhD Fellowship. 
The views and conclusions contained in this document are those of the author(s) and should not be interpreted as representing the official policies of the U.S. Army Research Laboratory or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation hereon.


\bibliographystyle{aaai}
\bibliography{cited}
\end{document}
