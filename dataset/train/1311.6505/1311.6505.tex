The Generalized Minimum Residual method (GMRES) of Saad and Schultz
\cite{jje:saad1986gmres} is a Krylov subspace method for solving
large, sparse, possibly non-symmetric linear systems of the form
$\Mat{A}\Vect{x}=\Vect{b}$.
GMRES is based on the Arnoldi process \cite{mfh:arnoldi1951principle}, which
can also be used to approximate a matrix's eigenvalues and
eigenvectors.  GMRES has the convenient property that the residual
norm of the approximate solution at each iteration is monotonically
non-increasing, assuming correct arithmetic and storage.  Its use of
orthogonal projections and normalized (to length one) basis vectors
also has advantages, that we will discuss below.

We begin this section by explaining how to use properties of the
Arnoldi process to detect faults in an iteration of GMRES.  We then
apply the SDC models we developed above to show how to scale the
linear system in a way that enhances fault detection and bounds the
possible error of the major computational kernels.  We will show in
future work that these bounds by themselves do not suffice to bound
the solution error.  Nevertheless, they can, if one makes inexpensive
changes to how GMRES computes the solution update coefficients.

\subsection{Fault detection via projection coefficients}
\label{jje:sec:gmres:hessenberg}


The norms and inner products that occur in each iteration of the
Arnoldi process in GMRES have a bounded absolute value.  
The bound depends on the norm of the preconditioned matrix, which is
inexpensive to estimate.  We use this bound to detect faults in all the major
computational kernels in GMRES.



\begin{algorithm}
\caption{GMRES}
\label{jje:alg:gmres}
\begin{algorithmic}[1]
\Input{Linear system $\Mat{A}\Vect{x}=\Vect{b}$ and initial guess $\Vect{x}_0$}
\Output{Approximate solution $\Vect{x}_m$ for some $m \geq 0$}
\State{$\Vect{r}_0 := \Vect{b} - \Mat{A} \Vect{x}_0$}\Comment{Unpreconditioned
initial residual vector}
\State{$\beta := \TwoNorm{\Vect{r}_0}$, $\Vect{q}_1 := \Vect{r}_0 / \beta$}
\For{$j = 1, 2, \dots$ until convergence}
\label{jje:alg:gmres:arnoldi_start}
  \State{$\Vect{v}_{j+1} := \Mat{A} \Vect{q}_j$}\Comment{Apply the matrix $A$}
  \label{jje:alg:gmres:orthog_start_vec}
  \For{$i = 1, 2, \dots, j$}\Comment{Orthogonalize}
  \label{jje:alg:gmres:mgs_start}
    \State{$h_{i,j} := \Vect{q}_i \cdot \Vect{v}_{j+1}$}
    \label{jje:alg:gmres:hij}
    \State{$\Vect{v}_{j+1} := \Vect{v}_{j+1} - h_{i,j} \Vect{q}_i $}
    \label{jje:alg:gmres:wi}
  \EndFor
  \label{jje:alg:gmres:mgs_end}
  \State{$h_{j+1,j} := \TwoNorm{\Vect{v}_{j+1}}$}
  \label{jje:alg:gmres:hjp1}
\If{$h_{j+1,j} \approx 0$} \label{jje:alg:gmres:happyCheck}
       \State{Solution is
       $\Vect{x}_{j-1}$}\label{jje:alg:GMRES:happy_breakdown}\Comment{Happy
       breakdown}
       \State{\textbf{return}}
  \EndIf
\State{$\Vect{q}_{j+1} := \Vect{v}_{j+1} / h_{j+1,j}$}\Comment{New basis
  vector}
  \label{jje:alg:gmres:arnoldi_end}
  \State{$\Vect{y}_j := \argminy \TwoNorm{ \Mat{H}(\IndexRange{1}{j+1},
  \IndexRange{1}{j}) \Vect{y} - \beta
  \Vect{e}_1}$}
  \State{$\Vect{x}_j := \Vect{x}_0 + [\Vect{q}_1, \Vect{q}_2, \dots, \Vect{q}_j]
  \Vect{y}_j$}\Comment{Compute solution update}
\EndFor
\end{algorithmic}
\end{algorithm}





\subsection{Bounds on the Arnoldi Process}
\label{jje:sec:gmres:bounds}


We start our analysis by bounding the dot product which determines the
$i$-th upper Hessenberg entry, $h_{i,j}$ of the $j$-th Arnoldi
iteration. The Arnoldi process is expressed on
Lines~\ref{jje:alg:gmres:arnoldi_start}--\ref{jje:alg:gmres:arnoldi_end}
in Algorithm~\ref{jje:alg:gmres}.  At its core is an orthogonalization
kernel, which we have chosen to be the Modified Gram-Schmidt (MGS)
process.  Classical Gram-Schmidt or Householder transformations may
also be used.  As we will demonstrate, our bound is invariant of the
orthogonalization algorithm chosen.

The MGS process begins on Line~\ref{jje:alg:gmres:mgs_start} and completes on Line~\ref{jje:alg:gmres:mgs_end}.
To bound $h_{i,j}$ on Line~\ref{jje:alg:gmres:hij}, we exploit a property of
orthogonal projections. It is well known that linear transforms
utilizing orthogonal matrices are \emph{isometric}.  That is, they preserve the
length of the vectors. In $\Rn$, the dot product of a vector with a
unit-length vector is bounded by the length of the first vector.
This means that each $h_{i,j}$ entry is bounded by the length of
the vector that starts the orthogonalization process (the vector we wish to
make orthogonal).
To clarify what ``starts'' the orthogonalization process means, we step back from an algorithmic
formulation, and instead write the orthogonalization kernel as a mathematical
expression.  For clarity, we will use the Classical Gram-Schmidt expression:
\begin{equation}
\Vect{w} = \left[ \Mat{I} - \Mat{Q}^{\text{T}}\Mat{Q} \right]\Vect{u}
\label{jje:eq:classical_gs}
\end{equation}
In Eq~\eqref{jje:eq:classical_gs}, the vector $\Vect{u}$ is what ``starts''
the orthogonalization process, and $\Vect{w}$ is the resulting vector, which is
orthogonal to all vectors in $\Mat{Q}$, where $\Mat{Q} = \{\Vect{q}_1, \dotsc,
\Vect{q}_j\}$.

Returning to Algorithm~\ref{jje:alg:gmres}, what ``starts'' the
orthogonalization process is the vector resulting from
Line~\ref{jje:alg:gmres:orthog_start_vec}. If we can bound the length of this
vector, then we know the maximum absolute value that $h_{i,j}$ can take.
Since we want to bound the length of the resulting vector, we take the
induced $\ell^2$ norm, $\TwoNorm{\Vect{v}_{j+1}} =
\TwoNorm{\Mat{A}\Vect{q}_j}$,
\begin{equation}
\TwoNorm{\Vect{v}_{j+1}} \leq \TwoNorm{\Mat{A}}\TwoNorm{\Vect{q}_j}.
\label{jje:eq:H_bound}
\end{equation}

We can further reduce the bound, recognizing that the basis vector $\Vect{q}_j$
is a unit vector, i.e., $\TwoNorm{\Vect{q}} = 1$. We may deal with
$\TwoNorm{\Mat{A}}$ in several ways: 
\begin{enumerate}
  \item 
$\TwoNorm{\Mat{A}}$ is defined to be
the largest singular value, e.g., $\sigma_\mathrm{max} (\Mat{A})$, or
\item
the 2-norm
is bounded above by the Frobenius norm, which is likely cheaper to compute than
the largest singular value.
\end{enumerate}
 This leads us to an upper bound on \emph{all}
entries in the upper Hessenberg matrix
\begin{align}
|h_{i,j}| \leq \TwoNorm{\Mat{A}} \leq \FNorm{\Mat{A}}.
\label{jje:eq:gmres:bounds:hij}
\end{align}
The bound presented in Eq.~\eqref{jje:eq:gmres:bounds:hij} is crucial, as it
demonstrates that the upper Hessenberg entries are bounded entirely by the input
matrix. In Section~\ref{jje:sec:ftgmres}, we discuss Flexible GMRES
with GMRES (Algorithm~\ref{jje:alg:gmres}) as a preconditioner. In this
scenario, the bound presented is invariant for all applications of the
preconditioner, or, in other words, the bound depends \emph{only} on the input
matrix.
\subsection{Bound Application}


We have shown what the theoretical upper limit is for the values in the upper
Hessenberg. This essentially tells us what is \emph{theoretically possible}
inside the Arnoldi process. Using this approach to construct an
SDC detector is significant. By building a detection scheme in this way, we know
precisely what errors we can detect, and, more importantly, we know what is not
detectable. 

The important factor to keep in mind is that exactly how an error is
committed is irrelevant, the norm bounds allow us to filter out values
that are invalid by theory --- we either detect a
large error or commit a small error, and in Section~\ref{jje:sec:ftgmres} we
will demonstrate how restricting the magnitude of the error committed allows
Flexible GMRES to tolerate the error.



\subsection{Error Detection}


In the context of error detection, we can only detect an error that exceeds the
bound on the upper Hessenberg entry $h_{ij}$. To do this, we insert a
conditional between
Lines~\ref{jje:alg:gmres:hij}~and~\ref{jje:alg:gmres:wi} and 
Lines~\ref{jje:alg:gmres:hjp1}~and~\ref{jje:alg:gmres:happyCheck}
 and test
whether $\Abs{h_{ij}} \leq \FNorm{\Mat{A}}$. Should this condition be invalid, then we assume that we have committed an error 
at some point.






