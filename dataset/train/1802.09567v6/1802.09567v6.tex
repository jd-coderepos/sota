\section{Experimental Results}
\label{sec:results}

In this section, we conduct experiments to verify the effectiveness of the proposed \gls*{alpr} system. All the experiments were performed on a NVIDIA Titan XP GPU ($3$,$840$ CUDA cores and $12$ GB of RAM) using the Darknet framework~\cite{darknet13}.

We consider as correct only the detections with \gls*{iou}~$\geq$~$0.5$. This value was chosen based on previous works~\cite{montazzolli2017,li2017towards,yuan2017}. In addition, the following parameters were used for training the networks:~$80k$ iterations (max batches) and learning rate~=~[$1$\textsuperscript{-$3$},~$1$\textsuperscript{-$4$},~$1$\textsuperscript{-$5$}] with steps at $25k$ and $35k$ iterations.

Experiments were conducted in two datasets: \gls*{ssig} and \acrshort*{dataset}. We report the results obtained by the proposed system and compare with previous work and two commercial systems\footnote{OpenALPR and Sighthound systems have Cloud APIs available at \url{https://www.openalpr.com/cloud-api.html} and \url{https://www.sighthound.com/products/cloud}, respectively. The results presented here were obtained on January, 2018.}: Sighthound~\cite{masood2017sighthound} and OpenALPR\footnote{Although it has an open-source version, the commercial version uses different algorithms for OCR trained with larger datasets to improve accuracy.}~\cite{openalpr}. According to the authors, both are robust in the detection and recognition of Brazilian \glspl*{lp}. 

It is important to emphasize that although the commercial systems were not tuned for these datasets, they use much larger private datasets, which is a great advantage especially in \gls*{dl} approaches. 

In the OpenALPR system we choose which \gls*{lp}'s style we want to detect (i.e., Brazilian) and we do not need to make any changes. On the other hand, Sighthound uses a single model for \glspl*{lp} from different countries. Therefore, we made some adjustments in its prediction so that it fits the Brazilian \glspl*{lp} format, such as swapping $0$ by O and vice versa.

\subsection{Evaluation on the \gls*{ssig} Dataset}
\label{ssig:results_ssig}

The \gls*{ssig} dataset~\cite{goncalves2016benchmark} is composed of $2$,$000$ images of $101$ vehicles with the following annotations: the position of the vehicle's \gls*{lp}, its identification (e.g., ABC-1234) and each character's position.

The high resolution images ($1$,$920$ $\times$ $1$,$080$ pixels) were acquired with a static digital camera and are available in the \gls*{png} format. A sample frame of the dataset is shown in Fig.~\ref{fig:ssig_sample}.

\begin{figure}[!htb]
	\centering
 	\includegraphics[width=0.8\columnwidth]{imgs/ssig_sample_allowed.png}
	\caption{A sample frame of the \gls*{ssig} dataset. It should be noted that there are vehicles in the background that do not have annotations. The \glspl*{lp} were blurred due to privacy constraints.}
	\label{fig:ssig_sample}    
\end{figure}

The \gls*{ssig} dataset uses the following evaluation protocol: $40\%$ of the dataset to training, $20\%$ to validation and $40\%$ to test. According to the authors, this protocol was adopted because many character segmentation approaches do not require model estimation and a larger test set allows the reported results to be more statistically significant.

We report only the results obtained with the Fast-YOLO model in the vehicle and \gls*{lp} detection subsections, since it achieved impressive recall and precision rates in both tasks.

\subsubsection{Vehicle Detection}
Since the \gls*{ssig} dataset does not have vehicle annotations, we manually label the vehicle's bounding box on each image of the dataset. Another possible approach would be to train a vehicle detector using the large-scale CompCars dataset~\cite{yang2015}, but that way many vehicles (including those in the background) would also be detected. 

To perform the vehicle detection, we first evaluate different confidence thresholds. We started with confidence of $0.5$, however some vehicles were not detected. All $407$ vehicles in the validation set were successfully detected when the threshold was reduced to $0.25$. Based on that, we decided to use half of this value (i.e., $0.125$) in the test set to increase the chance that all vehicles are detected. With this threshold, we achieved a recall of $100\%$ and precision above $99\%$ (only $7$ false positives). 

\subsubsection{\gls*{lp} Detection}
Every vehicle in the validation set was well segmented with its \gls*{lp} completely within the predicted bounding box. Therefore, we use the vehicle patches without any margin to train the \gls*{lp} detection network. As expected, all \glspl*{lp} were correctly detected in both validation and test sets (recall and precision = $100\%$). 

\subsubsection{Character Segmentation}
A margin of $5\%$ (of the bounding box size) is required so each detected \gls*{lp} contains all its characters fully. Therefore, we double this value (i.e., $10\%$) in the test set and in the training of the character segmentation~\gls*{cnn}. 

We evaluated, in the validation set, the following confidence thresholds: $0.5$, $0.25$ and $0.1$, but the recall achieved was $99.89\%$, regardless. Therefore, we chose to use a lower threshold (i.e., $0.1$) in the test set to miss as few characters as possible. That way, we achieved $99.75\%$ ($5$,$614$/$5$,$628$) recall.

\subsubsection{Character Recognition}

The padding values that yielded the best recognition rates in the validation set were $2$ pixels for letters and $1$ pixel for digits. In addition, data augmentation with flipped characters only improved letter recognition, hampering digit recognition. We believe that a greater padding and data augmentation improve letter recognition because each class have far fewer training examples, compared to digits.

We first analyzed the results without temporal redundancy information. The proposed system achieved recognition rate of~$85.45\%$, recognizing all three letters and all four digits in~$86.32\%$ and~$98.63\%$ of the time, respectively. 

The results are greatly improved when taking advantage of temporal redundancy information. The final recognition rate is $93.53\%$, since the digits are correctly recognized in all vehicles and the letters in $93.53\%$ of them. This result is given based on the number of frames correctly recognized, thereby vehicles with more frames have greater weight in the final result.

The recognition rates accomplished by the proposed system were considerably better than those obtained in previous works ($81.8\%$~$\rightarrow$~$93.53\%$), as shown in Table \ref{tab:ssig_results}. As expected, the commercial systems have also achieved great recognition rates, but only the proposed system was able to recognize correctly at least $6$ of the $7$ characters in all \glspl*{lp}. 
This is particularly important since the \gls*{lp}'s identification can be combined with the vehicle's manufacturer/model~\cite{dlagnekov2005} or its appearance~\cite{goncalves2016} to further enhance the recognition.

\begin{table}[!htb]
	\centering
	\caption{Recognition rates obtained by the proposed \gls*{alpr} system, previous work and commercial systems in the \gls*{ssig} dataset.}
	\label{tab:ssig_results}
	{
		\renewcommand{\arraystretch}{1.1}
		\resizebox{\columnwidth}{!}{
		\begin{tabular}{@{}ccc@{}}
			\toprule
			ALPR & $\geq$ $6$ characters & All correct (vehicles)\\ \midrule
			Montazzolli and Jung~\cite{montazzolli2017} & $90.55\%$ & $63.18\%$ \\
			Sighthound~\cite{masood2017sighthound} & $89.05\%$ & $73.13\%$ \\
			Proposed & $99.38\%$ & $85.45\%$ \\
			OpenALPR~\cite{openalpr} & $92.66\%$ & $87.44\%$ \\ \midrule
			Gonçalves et al.~\cite{goncalves2016} (with redundancy) & $-$ &  $81.80\%$ ($32$/$40$) \\
			Sighthound (with redundancy) & $99.13\%$ & $89.80\%$ ($35$/$40$)\\
			OpenALPR (with redundancy) & $95.77\%$ & $93.03\%$ ($37$/$40$)\\
			\textbf{Proposed (with redundancy)} & $\textbf{100.00\%}$ & $\textbf{93.53\%}$ $\textbf{(37/40)}$ \\ \bottomrule
		\end{tabular}}
	}
\end{table}

According to our experiments, the great improvement in our system lies on separating the letter and digits recognition on two networks, so each one is tuned specifically for its task. Moreover, data augmentation was essential for letter recognition, since some classes (e.g., C, V) have less than $20$~training examples.

In Table~\ref{tab:ssig_fps}, we report the recall/accuracy rate achieved in each \gls*{alpr} stage separately, as well as the time required for the proposed system to perform each stage. The reported time is the average time spent processing all inputs in each stage, assuming that the network weights are already loaded.

\begin{table}[!htb]
	\centering
	\caption{Results obtained and the computational time required in each \gls*{alpr} stage in the \gls*{ssig} dataset. Recall stands for detection and segmentation, and Accuracy stands for recognition.}
	\label{tab:ssig_fps}
	\begin{tabular}{@{}cccc@{}}
		\toprule
		ALPR Stage & Recall/Accuracy & Time (ms) & \gls*{fps} \\ \midrule
		Vehicle Detection & $100.00\%$ & $4.0746$ & $245$ \\
		License Plate Detection & $100.00\%$ & $4.0654$ & $246$ \\
		Character Segmentation & $99.75\%$ & $1.6555$ & $604$ \\
		Character Recognition & $97.83\%$ & $1.6452$ $\times$ $7$ & $87$ \\ \midrule
		\gls*{alpr} (all correct) & $85.45\%$ & \multirow{2}{*}{$21.3119$} & \multirow{2}{*}{$47$} \\
		\gls*{alpr} (with redundancy) & $93.53\%$ & & \\ \bottomrule
	\end{tabular}
\end{table}

Since the same model is used for vehicle and \gls*{lp} detection, the time required for both stages is very similar. The same is true for character segmentation and recognition, but the latter is performed $7$ times (one time for each character). The average processing time for each frame was $21.31$ seconds, an average of $47$ \gls*{fps}.

Our system had no difficulty recognizing red \glspl*{lp}, even with less training examples. According to our experiments, this is due to the negative images used in the training of the character segmentation and recognition \glspl*{cnn}. 
Due to the agreement terms of the \gls*{ssig} dataset, we can not show qualitative results. 
Only a few \glspl*{lp} (all from the training set) can be shown for illustrations of publications.

\subsection{Evaluation on the \acrshort*{dataset} Dataset}
\label{dataset:results_ssig}

\subsubsection{Vehicle Detection}

We first evaluated the Fast-YOLO model, but the recognition rates achieved were not satisfactory. After evaluations with different confidence thresholds, the best recall rate achieved was $97.33\%$. This was expected since this dataset has greater variability in vehicle types and positions. 

We chose to use the YOLOv2 model for vehicle detection, despite its higher computational cost. We evaluated several confidence thresholds, being $0.125$ the best one, as in the \gls*{ssig} dataset. The recall and precision rates achieved were $100\%$ and~$99\%$, respectively. 
Fig.~\ref{fig:vehicle_detect_ufpr} shows a motorcycle and a car detected with the YOLOv2 model.

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.48\columnwidth]{imgs/track0143_15_.png}
	\includegraphics[width=0.48\columnwidth]{imgs/track0103_15_.png}
	\caption{Examples of the detection obtained with the YOLOv2 model.}
	\label{fig:vehicle_detect_ufpr}    
\end{figure}

\subsubsection{\gls*{lp} Detection}

We note that in more challenging images (usually of motorcycles), the vehicle's \gls*{lp} is not entirely within its predicted bounding box, requiring a small margin ($5\%$ in the validation set) so that the entire \gls*{lp} is completely within the predicted vehicle's bounding box. Therefore, we use a $10\%$ margin in the test set and in the training of the \gls*{lp} detection~\gls*{cnn}. 

The recognition rates obtained by both YOLO models were very similar (less than half a percent difference). Thus, we use the Fast-YOLO model for \gls*{lp} detection. The recall rate attained was $98.33\%$ ($1$,$770$/$1$,$800$). We were not able to detect the \gls*{lp} in just one vehicle (in its $30$ frames), because a false positive was predicted with greater confidence than the actual \gls*{lp}, as shown in Fig.~\ref{fig:dataset_lp_fp}.

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.9\columnwidth]{imgs/track0107_02__01__low.png}
	\caption{A sample frame from the \gls*{dataset} dataset where the actual \gls*{lp} was not predicted with the highest confidence. The predicted position and ground truth are outlined in red and green, respectively. The \gls*{lp} was blurred due to privacy constraints.}
	\label{fig:dataset_lp_fp}    
\end{figure}

We could use the character segmentation CNN to perform a post-processing in cases where more than one \gls*{lp} is detected, for example: evaluate on each detected \gls*{lp} if there are $7$ characters or consider only the \gls*{lp} where the characters’ confidence is greater. However, since the actual \gls*{lp} can be detected with very low confidence levels (i.e.,~$\leq$~$0.1$), many false negatives would have to be analyzed, increasing the overall computational cost of the system.

\subsubsection{Character Segmentation}

In the validation set, a margin of $10\%$ is required so each detected \gls*{lp} contains all its characters fully. We decided not to double the margin in the test set, as $20\%$ would add a considerable amount of noise and background in the \glspl*{lp} patches.

The recall obtained was $97.59\%$ when disregarding the \glspl*{lp} not detected in the previous stage and $95.97\%$ when considering the whole test set. We accomplished better results in the \gls*{ssig} dataset, but it is worth noting that our dataset has different \glspl*{lp} types and many of them are tilted.
Fig.~\ref{fig:dataset_cs_samples} depicts some \glspl*{lp} from different categories properly segmented, even when the \gls*{lp} is tilted or in presence of shadows.

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.32\columnwidth]{imgs/track0119_20_.png}
	\includegraphics[width=0.32\columnwidth]{imgs/track0122_29_.png}
	\includegraphics[width=0.32\columnwidth]{imgs/track0133_23_.png}
	\caption{\glspl*{lp} from different categories properly segmented.}
	\label{fig:dataset_cs_samples}    
\end{figure}

\subsubsection{Character Recognition}

The best results were obtained with $1$ pixel of padding and data augmentation, for both letters and digits. The proposed system achieved a recognition rate of $64.89\%$ when processing frames individually and $78.33\%$ ($47$/$60$ vehicles) with temporal redundancy. 

Despite the great results obtained in the previous dataset, both commercial systems did not achieve satisfactory results in the \acrshort*{dataset} dataset. 
Analyzing the results we noticed that a substantial part of the errors were in motorcycles images, highlighting this constraint in both systems.
This suggests that those systems are not so well trained for motorcycles.
OpenALPR performed better than Sighthound, attaining a recognition rate of $70\%$ when exploring temporal redundancy information. Table~\ref{tab:dataset_results} shows all results obtained in the \gls*{dataset} dataset.

\begin{table}[!htb]
	\centering
	\caption{Recognition rates obtained by the proposed \gls*{alpr} system and commercial systems in the \acrshort*{dataset} dataset.}
	\label{tab:dataset_results}
	{
		\renewcommand{\arraystretch}{1.1}
		\begin{tabular}{@{}ccc@{}}
			\toprule
			ALPR & $\geq$ $6$ characters & All correct (vehicles)\\ \midrule
			Sighthound~\cite{masood2017sighthound} & $62.50\%$ & $47.39\%$ \\ 
			OpenALPR~\cite{openalpr} & $54.72\%$ & $50.94\%$ \\
			Proposed & $87.33\%$ & $64.89\%$ \\ \midrule
			Sighthound (with redundancy) & $76.67\%$ & $56.67\%$ ($34$/$60$) \\
			OpenALPR (with redundancy) & $73.33\%$ & $70.00\%$ ($42$/$60$)\\ 
			\textbf{Proposed (with redundancy)} & $\textbf{88.33\%}$ & $\textbf{78.33\%}$ ($\textbf{47}$\textbf{/}$\textbf{60}$) \\ \bottomrule
		\end{tabular}
	}
\end{table}

We report the recall/accuracy rate achieved in each \gls*{alpr} stage separately in Table~\ref{tab:dataset_fps}, as well as the time required for the proposed system to perform each stage. The vehicle detection stage is more time-consuming in this dataset, as we use a larger \gls*{cnn} architecture (i.e., YOLOv2).

\begin{table}[!htb]
	\centering
	\caption{Results obtained and the computational time required in each stage in the \acrshort*{dataset} dataset. Recall stands for detection and segmentation, and Accuracy stands for recognition.}
	\label{tab:dataset_fps}
	\begin{tabular}{@{}cccc@{}}
		\toprule
		ALPR Stage & Recall/Accuracy & Time (ms) & \gls*{fps} \\ \midrule
		Vehicle Detection & $100.00\%$ & $11.1578$ & $90$ \\
		License Plate Detection & $98.33\%$ & $3.9292$ & $255$ \\
		Character Segmentation & $95.97\%$ & $1.6548$ & $604$ \\
		Character Recognition & $90.37\%$ & $1.6513$ $\times$ $7$ & $87$ \\ \midrule 
		\gls*{alpr} (all correct) & $64.89\%$& \multirow{2}{*}{$28.3011$} & \multirow{2}{*}{$35$} \\
		\gls*{alpr} (with redundancy) & $78.33\%$& & \\ \bottomrule
	\end{tabular}
\end{table}

It is worth noting that despite using a deeper \gls*{cnn} model in vehicle detection (i.e., YOLOv2), our system is still able to process images at $35$ \gls*{fps} (against $47$ \gls*{fps} using Fast-YOLO).  
This is sufficient for real-time usage, as commercial cameras generally record videos at $30$~\gls*{fps}.

Fig.~\ref{fig:result_dataset} illustrates some of the recognition results obtained by the proposed system in the \acrshort*{dataset} dataset. It is noteworthy that our system can generalize well and correctly recognize \glspl*{lp} under different lighting conditions. 

\begin{figure}[!htb]
	\vspace{-2mm} 
	\centering
	\captionsetup[subfigure]{labelformat=empty} 
	\subfloat[][A\textcolor{red}{UC}-1056]{
		\includegraphics[width=0.31\columnwidth]{imgs/result_samples_wrong/track0111_18__01_.png}}\subfloat[][BC\textcolor{red}{L}-9595]{
		\includegraphics[width=0.31\columnwidth]{imgs/result_samples_wrong/track0120_10__01_.png}}\subfloat[][AT\textcolor{red}{U}-4025]{
		\includegraphics[width=0.31\columnwidth]{imgs/result_samples_wrong/track0131_19__01_.png}} \\[-1ex]
		
	\subfloat[][AS\textcolor{red}{D}-9\textcolor{red}{7}43]{
		\includegraphics[width=0.31\columnwidth]{imgs/result_samples_wrong/track0123_18__01_.png}}\subfloat[][AU\textcolor{red}{S}-0936]{
		\includegraphics[width=0.31\columnwidth]{imgs/result_samples_wrong/track0138_29__01_.png}}\subfloat[][AX\textcolor{red}{B}-5\textcolor{red}{4}87]{
		\includegraphics[width=0.31\columnwidth]{imgs/result_samples_wrong/track0143_10__01_.png}} \\[-1ex]
		
	\subfloat[][AYL-2104]{
		\includegraphics[width=0.31\columnwidth]{imgs/result_samples_right/track0144_15__01_.png}}\subfloat[][AHB-1989]{
		\includegraphics[width=0.31\columnwidth]{imgs/result_samples_right/track0102_13__01_.png}}\subfloat[][AWX-9307]{
		\includegraphics[width=0.31\columnwidth]{imgs/result_samples_right/track0096_13__01_.png}} \\[-1ex]
		
	\subfloat[][ALJ-9348]{
		\includegraphics[width=0.31\columnwidth]{imgs/result_samples_right/track0130_17__01_.png}}\subfloat[][AKT-8174]{
		\includegraphics[width=0.31\columnwidth]{imgs/result_samples_right/track0148_05__01_.png}}\subfloat[][MCA-9954]{
		\includegraphics[width=0.31\columnwidth]{imgs/result_samples_right/track0122_14__01_.png}} \\[-1ex]
		
	\subfloat[][ABN-8528]{
		\includegraphics[width=0.31\columnwidth]{imgs/result_samples_right/track0106_17__01_.png}}\subfloat[][AZU-3476]{
		\includegraphics[width=0.31\columnwidth]{imgs/result_samples_right/track0121_30__01_.png}}\subfloat[][AWE-4633]{
		\includegraphics[width=0.31\columnwidth]{imgs/result_samples_right/track0141_28__01_.png}}

	\caption{Qualitative results obtained by the proposed \gls*{alpr} system in the \gls*{dataset} dataset. The first two rows shows examples of correctly detected and incorrectly recognized \glspl*{lp}, while the following rows show samples of \glspl*{lp} (from different categories) successfully recognized.}
	\label{fig:result_dataset}  
	\vspace{-2mm} 
\end{figure}