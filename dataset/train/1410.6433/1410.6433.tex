\documentclass{article}[11pt,letter]

 

\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage[cmex10]{amsmath}
\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{xspace}
\usepackage[symbol*]{footmisc}
\usepackage[hyperfootnotes=false,breaklinks,bookmarks=false]{hyperref}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{color}
\usepackage{authblk}
\usepackage{fullpage}
\usepackage{cite}
\usepackage{todonotes}

\usepackage{microtype}


\newtheorem{definition}{Definition}[section]
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{proposition}[definition]{Proposition}
\newtheorem{example}[definition]{Example}
\newtheorem{observation}[definition]{Observation}
\newtheorem{corollary}[definition]{Corollary}

\newcommand{\bigo}{\mathcal{O}}
\newcommand{\ie}{i.e.\xspace}
\newcommand{\eg}{e.g.\xspace}
\newcommand{\alg}{\mathcal{A}}
\newcommand{\dalg}{\mathcal{D}}
\newcommand{\head}{h}
\newcommand{\hash}{\Phi}
\newcommand{\divides}{\bigm|}

\newcommand{\poly}{\text{poly}}
\newcommand{\bhash}{\hash}
\newcommand{\midpalin}[1][n]{{\bf MID-PALIN}\xspace}
\newcommand{\palin}[1][n]{{\bf PALIN}\xspace}
\newcommand{\aerr}{\ensuremath{E}}
\newcommand{\etal}{et~al.}
\newcommand{\level}{\lambda}
\DeclareMathOperator{\per}{per}
\DeclareMathOperator{\lcm}{lcm}
\newcommand{\kpers}{\ensuremath{p}}
\newenvironment{mydesc}[1]{\begin{list}{}{\settowidth{\labelwidth}{#1}
    \setlength{\labelsep}{0.5cm}
    \setlength{\leftmargin}{\labelwidth}
    \addtolength{\leftmargin}{\labelsep}
    \setlength{\rightmargin}{0pt}
    \setlength{\parsep}{0.5ex plus0.2ex minus0.1ex}
    \setlength{\itemsep}{0ex plus0.2ex}
    }
  }
{\end{list}}
\newcommand{\myparagraph}[1]{{\vspace{0.1cm}\sffamily\large\bfseries\raggedright #1}}

\usepackage{thmtools}
\usepackage{thm-restate}

\usepackage{booktabs}
\usepackage{multirow}


\bibliographystyle{plain}





\begin{document}


\title{Tight tradeoffs for approximating palindromes in streams\footnote{up to a logarithmic factor.}}



\author[1]{Paweł Gawrychowski}
\author[2]{Przemysław Uznański}
\affil[1]{Institute of Informatics, University of Warsaw, Poland}
\affil[2]{Department of Computer Science, ETH Z\"{u}rich, Switzerland
}

\date{}

\maketitle

\begin{abstract}
We consider computing the longest palindrome in a text of length  in the streaming model, where
the characters arrive one-by-one, and we do not have random access to the input. While 
computing the answer exactly using sublinear memory is not possible in such a setting, one can still hope for a good approximation guarantee.

We focus on the two most natural variants, where we aim for either additive or multiplicative approximation
of the length of the longest palindrome. We first show that there is no point in considering
Las Vegas algorithms in such a setting, as they cannot achieve sublinear space complexity.
For Monte Carlo algorithms, we provide a lower bound of  bits for
approximating the answer with additive error , and   bits for 
approximating the answer with multiplicative error  for the binary alphabet.
Then, we construct a generic Monte Carlo algorithm, which by 
choosing the parameters appropriately achieves space complexity matching up to a logarithmic factor for
both variants.
This substantially improves the previous results by Berenbrink~\etal~(STACS~2014) and essentially settles
the space complexity.
\end{abstract}

\section{Introduction}
A recent trend in algorithms on strings is to develop efficient algorithms in the streaming model, where
characters arrive one-by-one, and we do not have random access to the input. The main goal is to minimize
the space complexity, \ie, avoid storing the already seen prefix of the text explicitly. One usually allows
randomization and requires that the answer should be correct with high probability.
We consider computing the longest palindrome in this model, where a palindrome is a fragment which
reads the same in both directions. This is one of the basic questions concerning regularities in texts
and it has been extensively studied in the classical non-streaming setting, see~\cite{Apostolico,GalilSeiferas,KMP,Manacher}
and the references therein. The notion of palindromes, but with a slightly different meaning, is
very important in
computational biology, where one considers strings over  and a palindrome is a sequence
equal to its reverse complement (a reverse complement reverses the sequences and interchanges
 with  and  with ); see~\cite{GawrychowskiSTACS} and the references therein for
a discussion of their algorithmic aspects. Our results generalize to biological palindromes
in a straightforward manner.

Computing the longest palindrome in the streaming model was recently considered by Berenbrink \etal~\cite{Berenbrink},
who developed tradeoffs between the bound on the error and the space complexity for additive and multiplicative variants of the problem, that is,
for approximating the length of the longest palindrome with either additive or a multiplicative error.
Their algorithms were Monte Carlo, \ie, returned the correct answer with high probability. They also proved
that any Las Vegas algorithm achieving additive error  must necessarily use  bits of memory,
which matches the space complexity of their solution up to a logarithmic factor in the  range,
but leaves at least two questions. Firstly, does the lower bound still hold for Monte Carlo algorithms? Secondly, 
what is the best possible space complexity when  in the additive variant,
and what about the multiplicative version? We answer all these questions.


\paragraph{Related work.} The most basic problem in algorithms on strings is pattern matching, where we want to detect an occurrence of a a pattern in a given text.
It is somewhat surprising that one can actually solve it using polylogarithmic space in the streaming model,
as proved by Porat and Porat~\cite{PoratStreaming}. A simpler solution was later given by Erg{\"u}n \etal~\cite{ErgunPeriodicity}, and Breslauer
and Galil~\cite{Breslauer}. Similar questions studied in such setting include multiple-pattern
matching~\cite{DictionaryStream}, approximate pattern matching~\cite{KMismatch}, and
parametrized pattern matching~\cite{ParametrizedStreaming}.

Pattern matching is also very closely related to detecting periodicities, and in fact Erg{\"u}n \etal~\cite{ErgunPeriodicity} also developed
an efficient algorithm for computing the smallest period, where  is a period of  if  for all . Also palindromes are closely connected to periodicities. Informally, two long
palindromes occurring close to each other imply a periodicity of the underlying fragment of the text
(to the best of our knowledge, this has been first explicitly stated by Apostolico \etal~\cite{Apostolico}).
Similar insights have been used to partition the text into the smallest number of palindromes~\cite{FiciGKK14,ISIBT14}
and recognizing the so-called  language~\cite{KosolobovRS15}.
At a very high level, the idea there is to consider longer and longer prefixes of the text and maintain a succinct description
of all palindromic suffixes of the current prefix. Naturally, our algorithm is based on the same high-level idea, but there
are multiple non-trivial technical difficulties stemming from the fact that we cannot provide random access to the already
seen part of the text, so we can only approximate such information.

\paragraph{Model.} We work in the streaming model and consider additive and multiplicative variant of the problem.
The model works as follows: we are first given the length of the text  and the bound on the desired error
 (in the additive variant) or  (in the multiplicative variant), then the characters
 arrive one-by-one. In the -th step we receive  and we are required to output a number , such that the length of the longest palindrome in
 is either between  and  (in the additive variant) or between  and 
(in the multiplicative variant). 
We have  bits of memory available, where we can store an arbitrary data.
It is important to remember that the procedure operates in steps corresponding
to the characters and we cannot retrieve an already seen character unless it has been stored in memory.

Now we are interested in the possible tradeoffs between  and the bound on the error.
We consider Las Vegas and Monte Carlo algorithms.
A Las Vegas algorithm always returns a correct answer, but its memory usage  is a random variable.
A Monte Carlo algorithm returns a correct answer with high probability, and its memory usage 
does not depend on the random choices, where high probability means , for arbitrarily large constant .


We assume that the memory consists of words of size  and
basic operations take  time on such words. Bounds on the space are expressed in such words unless stated otherwise.

\paragraph{Previous work.} The longest palindrome can be found in  time and space (cf. Manacher \cite{Manacher}).
Berenbrink \etal~\cite{Berenbrink} constructed a streaming algorithm
achieving additive error  using  space and  total time
for any ,
and a streaming algorithm guaranteeing multiplicative error  using 
space and  total time for any ,
both Monte Carlo.
They also proved that any Las Vegas algorithm with additive error  must necessarily use
 bits of space.

\paragraph{Our results.} We significantly improve on the previous results as follows and essentially settle the space complexity of the
problem in both variants (see Table~\ref{table:results} for summary).

Firstly, we prove that any Las Vegas algorithm approximating (in either variant) the length of the longest palindrome inside a text of length 
over an alphabet  must necessarily use  bits of memory (see Theorem~\ref{th:lv}).
Hence Las Vegas randomization is simply not the right model for this particular problem.
Then we move to Monte Carlo algorithms, and prove the following lower bounds on their space complexity:
\begin{itemize}
\item  bits to achieve additive error  with high probability if  (see Theorem~\ref{th:montecarlo_additive_lowerbound}),\footnote{This can be strengthened to , but for the sake of clarity we prefer to state a weaker bound, here and in subsequent similar places.}
\item  bits to achieve multiplicative error  with high probability if \  (see Theorem~\ref{th:montecarlo_multiplicative_lowerbound}).\footnote{Here  can be replaced by any constant larger than , and  can be strengthened to .}
\end{itemize}

\newcommand{\tablebgcolor}{}
\newcommand{\tablebgcolordark}{}
\newcommand{\cellbgcolor}{}
\newcommand{\cellbgcolordark}{}

\begin{table}[t!]

\begin{tabular}{p{0.44\columnwidth}p{0.51\columnwidth}}
	\toprule		
		\multicolumn{2}{c}{Las Vegas approximation}\\
		\midrule
		 &  \\
		\midrule
		\multicolumn{2}{c}{Monte Carlo additive approximation}\\
		\midrule
		 space,  time,  &  space,  time, \\
	
		\multicolumn{1}{c}{--} &  bits,  \\
		\midrule
		\multicolumn{2}{c}{Monte Carlo multiplicative approximation}\\
		\midrule
		 space,  time,  &  space,  time, \\

		\multicolumn{1}{c}{--} &  bits, \\
		\bottomrule
	\end{tabular}
	\caption{
    A comparison of previous (on the left, c.f. \cite{Berenbrink}) and our (on the right) results. Lower bounds are in bits, and upper bounds in words consisting of  bits.
\label{table:results}
}

\end{table}

Secondly, we construct a generic 
Monte Carlo approximation algorithm, which by adjusting the parameters appropriately matches our lower bounds up to a logarithmic multiplicative 
factor.
In more detail, our algorithm uses  words of space for any  in the additive variant (see Theorem~\ref{additive_scheme} and
Theorem~\ref{memory_efficient}) and  words of space for any 
in the multiplicative variant (see Theorem~\ref{multiplicative_scheme} and Theorem~\ref{memory_efficient}).\footnote{For 
small  this is , and for large  becomes .
Note that this does not contradict the lower bound, because  for .}
This essentially settles the space complexity of the problem, as it can be seen that our lower and upper bounds differ by at most a logarithmic factor
for any  and .
The time complexity of our algorithm is always  (see Theorem~\ref{thm:time_efficient}).

\paragraph{Overview of the methods.} As usual in the streaming model, we apply Karp-Rabin fingerprints. We store
such fingerprints for some carefully chosen prefixes of the already seen part of the text. Informally, these chosen
prefixes become more and more sparse as we move closer to the beginning, with the details depending on the
variant. We call such fingerprints of prefixes landmarks, and formalize this notion in Section~\ref{section:preliminaries}. Then, in Section~\ref{section:basic}, we present
a generic algorithm. The idea is that for every possible palindrome center we create a separate process
which maintains the corresponding palindromic radius (or, more precisely, its approximation).
By adjusting the parameters of the generic algorithm we are able to guarantee good bound on the error
in both variants.
For  or  there are only few landmarks and such generic algorithm
is already efficient enough when implemented naively. To implement it efficiently for smaller 
or , we need to avoid running processes which have already found
a mismatch, but maintaining such a list explicitly might take too much space. However, multiple sufficiently
long palindromes appearing close to each other imply periodicity of the corresponding fragment of the text, which
can be exploited to concisely describe the whole situation.
This insight (dating back to Apostolico \etal~\cite{Apostolico})
allows us to approximate the information about all active processes in logarithmic space as explained
in Section~\ref{section:time}. Then in Section~\ref{section:time2} we use it to avoid running all active processes after
reading every character.
Finally, in Section~\ref{section:lowerbounds} we apply the Yao's minimax principle to derive the
lower bounds. For Las Vegas algorithms, this is straightforward, but requires more work for Monte Carlo algorithms.



\paragraph{Comparison with previous work.}
The additive approximation algorithm proposed by Berenbrink \etal~\cite{Berenbrink} uses a flat structure of  fingerprints (called checkpoints).  The most recently seen  characters are stored explicitly,
and the information of all palindromes with larger radius is compressed using the periodicity lemma.
For multiplicative approximation, a sparse structure of checkpoints is used.
Our technical contribution is of several flavors. Firstly, we use a single generic construction 
for both variants of the problem. Secondly, in all variants we use a hierarchical structure of fingerprints,
with the fingerprints becoming more and more sparse as we move closer to the beginning.
This in particular allows us to avoid storing a long suffix explicitly in the
additive version. Thirdly, also the periodicity compression is applied in a hierarchical manner:
we maintain a partition of the text into segments with lengths exponentially increasing
with the distance from the most recently seen character and compress each such segment
separately. In previous work, a rigid partition into segments of length  was used.
Storing such rigid partition requires  space, which might be too much
when the allowed error is large. Working with segments of exponentially increasing lengths
allows us to decrease this additional memory usage to only , but
also makes the details more involved.




\section{Preliminaries}
\label{section:preliminaries}

For a word , we denote its length by , and its -th letter by  for any . Similarly,  denotes the fragment starting at the -th
and ending at the -th character, and  denotes the reversal, \ie, . The period  of  is the smallest natural number such that
 for all .
The well-known periodicity lemma~\cite{FineWilf} states that if  and  are periods of  and , then  is also a period
of .
We focus on detecting palindromes of even length (odd palindromes can be detected with the standard trick of duplicating every letter, see~\cite{Apostolico}).
The palindromic radius at  is the largest  such that .  is the center of
a~palindrome .

\paragraph{Karp-Rabin fingerprints.} We use the Karp-Rabin fingerprints~\cite{KR} to quickly check equality of long strings. We choose a large prime 
and draw  uniformly at random.Then define 
and define fingerprint  of a word  to consist of , , , , ,
which takes  space if .
The following operations take  time:
\begin{mydesc}{\bf erasing a suffix}
\item[\bf concatenation] given  and , find ,
\item[\bf erasing a prefix] given  and , find ,
\item[\bf erasing a suffix] given  and , find ,
\item[\bf reversal] given , find .
\end{mydesc}

The fingerprints allow us to check if two strings are the same. Formally, we assume that . Then,
to check if  we compare  and .
If  then , and if  while  then  with probability at most .
The latter situation is called a false positive. Because the running time of our algorithms will be always polynomial in , and we will
be operating on strings of length at most , by the union bound the probability of a false positive can be made 
for any  by choosing exponent in  large enough. When analyzing the correctness, we assume no false positives.

\paragraph{Landmarks.} Our algorithms stores some values .
The intuition is that after reading  we calculate
the fingerprint of the currently seen prefix and keep it available for some time.
A landmark is a position  such that  is currently stored.
If additionally  for some ,  is a landmark on
level , and if  is odd then  is a landmark strictly on level
.  is the set of landmarks on level 
and  is the set of all landmarks. Observe that knowing
 for all  is enough to calculate
 for any  in  time.
Technically,  depends on the current value of , and
`` is a landmark at '' means that  just after reading .

For each level of landmarks we fix its size . After reading , the
-th level consists of the  most recently seen positions of the form ,
that is,  for .
The sizes  are chosen differently depending on the desired approximation guarantee.
In all versions, the last level has number , and , while no restriction is put on .


For such choice of landmarks, after increasing  by one we need to add and remove at
most one landmark per level, which takes  time in total. The landmarks
on each level are kept in a random access array with cyclic addressing, thus using 
 space while allowing accesses and updates in  time.

\section{Space-efficient algorithm}
\label{section:basic}
We start with the basic algorithm. A proper choice of all  guarantees small additive or multiplicative error, but the time and
space complexity might be high. Nevertheless, the basic algorithm serves as a good starting point for developing first the space efficient version,
and then finally the time efficient solution.

The idea of the algorithm is that for every possible center  we create a process , which keeps on computing the corresponding
radius . We call a process alive if it has not found  such that
 yet, and dead otherwise. The process starts with  and then uses the landmarks to update the value
of  (and also the final answer) whenever possible.
To verify if  we need to check if  is a palindrome. This requires accessing
 and  to calculate  and then , which
are then compared to each other, see Fig.~\ref{fig:radius}.
We can simply maintain the current value of  but retrieving  is only possible when  is a landmark.
Therefore, the process  can update its  only when  is a landmark, and doing so will be referred to as running  using . If 
is a palindrome, we say that  succeeds, and otherwise fails.

\begin{figure}[t]
\includegraphics[width=\textwidth]{radius}
\caption{Checking if .}
\label{fig:radius}
\end{figure}

We would like to guarantee that running a process is a  time procedure, so we need to quickly check if 
is currently a landmark (and if so, access the stored ). This can be easily done by iterating through all possible levels, but we want a
faster method. We consider the last -th level separately in  time. For all lower levels, the values  are all the same. We compute
the largest power of  dividing , call it , then  cannot be a landmark on level larger than .
On the other hand, if  is a landmark on level , then it is also a landmark on level . Therefore, we only need to consider the
-th level. This allows us to run any process in  time, and furthermore the state of any 
can be fully described just by specifying its center  ( is not stored explicitly, unless mentioned otherwise).
Observe that even if a process is dead, there is no harm in running it again.

The basic algorithm simply runs all processes after reading the next  using appropriately defined landmarks (depending on variant
and desired error guarantee).
For  or , it needs  time to process .


\begin{restatable}{theorem}{restatablespeff}
\label{th:sp_eff}
The basic algorithm approximates the longest palindrome with additive error  using  time and words of memory
to process , and the longest palindrome with multiplicative error  using 
time and words of memory to process .
\end{restatable}

\begin{proof}
The algorithm runs, after reading every , the process  for every landmark .
For additive approximation, the landmarks are defined as in Theorem~\ref{additive_scheme} with .
For multiplicative approximation and ,
the landmarks are defined as in Theorem~\ref{multiplicative_scheme} with ,
but when  we slightly change the definition by choosing  and keeping as a landmark,
for every , the last position divisible by .
\end{proof}

An optimized version of the basic algorithm will be referred
to as a scheduling scheme. A scheduling scheme should guarantee that any alive  such that  is a landmark is run, unless we 
can either be sure that it would fail anyway, or there is another  such that  and  is also a landmark, and we can be sure
that it succeeds (in particular,  is still alive).

\begin{restatable}{theorem}{restatableadditivescheme}
\label{additive_scheme}
Any scheduling scheme with  approximates the longest palindrome with additive error
.
\end{restatable}


\begin{proof}
Consider an arbitrary palindrome . We will show that any scheduling scheme with  returns at least .
We can assume , then there exists  such that .
Therefore,  is permanently a landmark on level .  will be alive at , so by the properties of a scheduling
scheme we will run a process detecting a~palindrome with radius at least .
Thus any scheduling scheme with  approximates the longest palindrome with additive error .
\end{proof}

\begin{restatable}{theorem}{restatablemultiplicativescheme}
\label{multiplicative_scheme}
Any scheduling scheme with  for  approximates the longest palindrome with multiplicative 
error .
\end{restatable}

\begin{proof}
Consider an arbitrary palindrome . We will show that any such scheduling scheme returns at least .
Let  be the smallest integer such that . We have two cases.

\begin{description}
\item[] After reading , all  are landmarks on level 0, and  is one
of them because , so  or a longer palindrome is detected.

\item[] In the interval  there are at most   numbers divisible by ,
thus there exists  such that  was a landmark on level  after reading . As  is still alive
at , we will detect a palindrome of radius at least . In other words, we will approximate  with additive error .
However, since  was chosen to be minimal, we have that , so we can bound the multiplicative error from above by
, which is at most  for .
\end{description}
Therefore, any such scheduling scheme approximates the longest palindrome with multiplicative error .
\end{proof}

\begin{restatable}{lemma}{restatabledelay}
\label{lemma:delay}
If  for all , then for any  and for any  there is at least one  such that 
 is a landmark at .
\end{restatable}

\begin{proof}
Observe that there exists unique  such that . Because , after reading  there are two possibilities. If  then  is among the 12 last seen positions divisible by . If  then  is definitely a landmark anyway.
\end{proof}

\section{Maintaining the alive processes}
\label{section:time}

To implement a scheduling scheme, we want to know which processes are alive. Maintaining
them explicitly is too space-expensive, though.
Therefore, we will store a compressed approximate representation of all alive processes.
Intuitively, we will group together nearby alive processes using the notion
of a partition scheme described below. The representation will not be exact
in the sense that it might report some dead processes as alive (but then they will
have some additional properties).
Such information is not providing any speedup by itself yet,
but later in Section~\ref{section:time2} we will use it to implement any scheduling scheme efficiently.
In this section, we focus on maintaining the information.

\paragraph{Partition scheme.} We maintain a partition of  into disjoint segments stored in a linked list.
The length of every segment is a power of , their lengths are nonincreasing as one moves to the right, and there is 
such that we
have between  and  segments of length  for every , and between  and  segments of
length .  and  are constants to be specified later. After increasing  by one, a new segment of
length  appears, then we possibly take two adjacent segments of length  such that the
segment on their left (if any) is longer, and merge them into one segment of length . We call this a partition scheme, as there is some
flexibility as to when the merging happens.

\begin{restatable}{lemma}{restatablepartition}
\label{lemma:partition}
There is a partition scheme with  and , which guarantees that after adding a new segment of length  we can
merge in  time at most one pair of adjacent segments of length , such that there are  segments
of the same length  on their right.
\end{restatable}

\begin{proof}
This is a simple example of the recursive slow-down method of Kaplan and Tarjan~\cite{KaplanSlowdown}.
Let  be the lengths of the segments in the current partition, where .
We group together all segments with the same length, and denote the number of segments of length  by .
We will show how to maintain  for every  and ,
where  is the maximum length of a segment
in the current partition. To this end, we will keep the following invariant: if  then there exists  such that  
and . We call such partition valid.

We must show that, given a valid partition of , we
can construct in  time a valid partition of . We start with creating a new segment of length  and adding it to the
previous partition, which increases  by one. Now there are two cases.

\begin{description}
\item[] We merge two (leftmost) segments of length  into a segment of length , or in other words
we decrease  by two ( is now equal to 3) and increase  by one. Because the initial value of  was , the only way the invariant could have been
broken is that  was ,  and  for some . But then the new  becomes ,
and all  are now , so the invariant holds.
\item[] If there is  such that  and , then we merge the two
(leftmost) segments of length  into a segment of length , which decreases  by two and increases  by one.
As in the previous case, the only way the invariant could have been broken is that  was , 
and  for some . Then  becomes , and all  are now , so the invariant holds.
\end{description}

To implement the update, we group together all consecutive 's with the same value of . In other words, we store a list of lists of segments.
This allows us to find  from the second case in  time.
\end{proof}

Instead of storing every alive  explicitly, for every segment we group together all alive processes such that  lies inside.
We need the following result, which follows from a definition of a palindrome, see~\cite{Apostolico}.

\begin{lemma}
\label{lemma:periodicity}
If ,  and , then  is a period of .
\end{lemma}

The intuition is that in a segment of length  either there are at most  alive processes which can be kept explicitly,
or there are at least  of them and the whole segment is periodic with period at 
most . Hence for every segment we store either a sparse or a dense description, depending (roughly) on the periodicity of
the corresponding fragment.

\begin{description}
\item[Sparse description.] We explicitly store a list of all processes inside the segment, which can be potentially still alive.
We guarantee that there are at most  processes on that list, and that if a process is not on the list, it is surely dead. We do not guarantee
that all processes on the list are still alive, but whenever we run one of them and the check fails, we declare it dead and remove from the list.
The processes currently on the list are called relevant.

\begin{figure}[t]
\includegraphics[width=\textwidth]{dense}
\caption{Alive processes inside  with a dense description are of the form .}
\label{fig:dense}
\end{figure}

\item[Dense description.] We guarantee that there exists a word  such that  for which the whole segment 
of length 
is a subword of , see Fig.~\ref{fig:dense}. Denoting the segment by , this implies that 
and  has a palindromic subword of length . In such a case we store a~multiple of the period, denoted by ,
such that the only alive processes inside the segment are of the form  for , where  is an even palindrome fully within the segment.
(We do not require that all such processes are still alive.) We store  and , which is enough to run any relevant process
inside  in  time, where relevant means of the form .
No other process inside  can be alive.
\end{description}

We use Lemma~\ref{lemma:partition} to maintain a partition of  into segments. The description of every segment requires
just  space, making the total additional space complexity . 
After reading  we create a sparse
description of the new segment of length  and then need to merge at most one pair of adjacent segments. After having updated the
partition, we can simply run all relevant processes.
Therefore, now we need to show how to merge a pair of adjacent segments  and  of length
 as to obtain a new segment . If their descriptions are sparse, we merge the lists of  and  and either get at most  processes, which constitute a valid sparse description,
or at least  processes . The following observation follows from the Lemma~\ref{lemma:partition}.

\begin{observation}
\label{obs:lifespan}
When a segment of length  is being created, the number of already seen characters on its right is at most
.
When it is being destroyed, there are at least  of them.
\end{observation}

Hence any  could have been run everywhere in the interval ,
and by Lemma~\ref{lemma:delay} had at least one landmark available in that interval,
so its radius must be at least . Therefore, we have a list of  processes inside a segment of length
, all of which have radii at least  ( must be considered separately).
This suffices to construct a dense description by the following lemma.

\begin{restatable}{lemma}{restatablemerge}
\label{lemma:merge}
Given a list of  processes  inside a segment of length , such that their radii are all at least 
and no other process inside is alive, we can construct in  time a dense description.
\end{restatable}

\begin{proof}
We rearrange the processes so that  and define . Every  is a period of the segment
by Lemma~\ref{lemma:periodicity}. We claim that by the periodicity lemma also  is a period of the 
segment. This can be seen by the following reasoning: if the radii at  are all at least , ,  is a period of
 and  is a period of , then by the periodicity lemma  is a
period of  the whole . Then by induction  is a period
of , which contains the whole segment inside. Because 
and ,
 for at least one , so  and consequently  must be a multiple of .

Now we can construct a dense description. We compute  in  with  applications of the Euclidean algorithm,
and set .
Because  for every , all  are of the form . Furthermore, because , 
, so  is fully within the segment. Finally, we must argue that  is an even palindrome.
First observe that  lies fully within the segment, and consider two cases.
\begin{itemize}
\item If , then . Because the palindromic radius at  is at least ,
 is a palindrome.
\item If , then . Because the palindromic radius at  is at least ,  is a palindrome.\qedhere
\end{itemize}
\end{proof}

This settles the situation when both descriptions are sparse. Before we move to the remaining case, we need an additional tool. If a
description of a segment is dense, we maintain some additional information about the processes inside. Informally, we would like to know which
of them are still alive, but of course we cannot afford to explicitly maintain such information. We can only afford to store a short buffer, 
where we keep information about a few most recently run processes. Formally, the buffer is a list of processes  together with their
corresponding values of . We do not require that  is still alive, so it might have happened that it has been run again after reading 
with , but the more recent run was unsuccessful. The buffer is updated whenever we successfully run a process  inside the segment.
There either  was in the buffer, so we move it to the front and update the corresponding , and otherwise
we prepend it to the buffer together with the current , and if the length of the buffer is now  we remove the last element from there.
 Hence the buffer is of length at most . A less trivial consequence is as follows.

\begin{restatable}{lemma}{restatableconvergence}
\label{lemma:convergence}
If a segment with dense description of length  is being destroyed while at most  processes in its buffer have radii at least , 
then no other process inside the segment can be still alive.
\end{restatable}

\begin{proof}
By Observation~\ref{obs:lifespan} and how we process segments with dense descriptions, any  which might be still alive could have been run everywhere
in the interval , and by Lemma~\ref{lemma:delay} it had at least one
landmark available in that interval. Also, whenever we run any  inside the segment in the interval , and 
it succeeds,  is set to at least  (except when , but then there is just one process inside the segment, so the buffer
surely contains it). Therefore, if the buffer contains at most  processes with radii at least , any  such that 
is stored in the buffer, and no other process can be still alive.
\end{proof}

If at least one description is dense, by applying Lemma~\ref{lemma:convergence} to  (if its description is dense) or  (if its description is dense), we either get that one of these
segments contains at least  processes with radii at least  in its buffer, or we get a list of at most  potentially still alive processes
inside each segment.
In the latter case we concatenate the lists to get a list of at most  processes inside  such that all other processes
inside are dead. If the list contains at most  processes, we construct a sparse description of , and otherwise we apply Lemma~\ref{lemma:merge} to construct a dense description of  in  time.
In the former case we get a list of between  and  alive processes  inside . It might be the case that 
there are also some other processes inside the segment which are still alive, but they are not stored in the buffer of the corresponding segment. Nevertheless,
proceeding as in the proof of Lemma~\ref{lemma:merge} we can compute in  time  and  such that  is an even
palindrome fully within , all  are of the form , and  is a period of . This is not a valid 
dense description yet, as  or  (or both) might have dense descriptions, and we cannot guarantee that all alive processes 
there are of the form .

Consider the case when  has a dense description, meaning that we have  and  such  is an even palindrome fully within ,
all alive processes there are of the form , and  is a period of . If  there is nothing to do.
Otherwise, because the list  contains at least  processes inside  we have  and
by the periodicity lemma  is a period of . Then  must be actually a period of the whole . 
Now we claim that  can be, in fact, replaced by . This is because if a power of a word is a palindrome, the word itself must be a palindrome,
so  is an even palindrome.

The case when  has a dense description, or both  and  have dense descriptions, can be dealt with similarly.

\begin{theorem}
\label{memory_efficient}
For any scheduling scheme with  for all , descriptions of all segments in the current
partition of  can be maintained in  space and  time plus the
time to run all relevant processes.
\end{theorem}

\section{Time-efficient algorithm}
\label{section:time2}
The simulation from the previous section was space-efficient, but not time-efficient yet, because there might be segments with
dense descriptions and small periods, which in turn requires running many relevant processes. 
This is the only reason the time to process  might exceed , as merging at most one pair of segments and
running the processes in all segments with sparse descriptions takes just  time. In this section we show how to simulate
running all relevant processes in a segment with dense description in  time.

Consider a dense description of a segment . Recall that it consists of  and , such that  is an even palindrome
and  is a period of the whole segment, and we want to run
all processes  inside  of the form , where  is a landmark.
We can construct and run all relevant processes in  time each, but there might be many of them. However, there are
only two consequences of running such a : we might update the final answer, and we might also store it in the buffer (or move it to the front there).
Therefore, if we can guarantee that a particular  will fail anyway, we can avoid running it altogether. Similarly, if we can guarantee that many
processes  will succeed, it is enough to run just the  leftmost of them.
We will build on these observations to simulate running all processes of such form in a single segment with a dense description in  total time.
This is the most technical part, so we start with an overview.

\paragraph{Overview.}   We start with observing in Lemma~\ref{lem:few levels} that, when considering such a segment, just a constant number of \emph{associated landmark
levels} needs to be considered. Then we analyze which relevant processes inside a segment should be run because of a landmark on level .
After some basic arithmetical manipulation, we get a succinct description of all such values of . To avoid considering all of them, which
might be too costly, we apply two lemmas characterizing the structure of palindromes in a sufficiently periodic fragment of the text,
described in Lemma~\ref{lemma:structure1} and Lemma~\ref{lemma:structure2} (these observations go back to~\cite{Apostolico}, but we need
a slightly different formulation). To apply them, we need to compute how far the periodicity of a segment with a dense description continues
to the left and to the right. To this end, we relax the notion of landmarks, introducing the so-called \emph{ghost landmarks}, which allow us to
operate on a longer suffix of the already seen . Then, using the ghost landmarks, we binary search to compute how far
the periodicity extends, and apply the structural results to isolate at most  relevant processes, which should be run as to guarantee
the correctness. To achieve the final complexity of   to process , we precompute how far the
periodicity continues when creating the segment, and then maintain this information in  time.



\paragraph{Associated landmark levels.} Consider a segment . If, for some  inside ,  is a~landmark strictly strictly on
level  at , we say that  is a~landmark level associated to .

\begin{restatable}{lemma}{restatablefewlevels}
\label{lem:few levels}
There are at most  landmark levels associated to a single segment, and they can be all determined
 time.
\end{restatable}

\begin{proof}
Consider a segment  of length  and any  inside. By Observation~\ref{obs:lifespan}, when the segment is being created
by merging two segments of length  we have . Similarly, when the segment is being destroyed
by merging with an adjacent segment of length  to form a segment of length  we have . Consequently, we can bound , which is the number of already seen characters on the right of , as follows:

If  is a landmark strictly on level , then the number of already seen characters on its right belongs to
. Bounding the number of different
landmark levels associated to  requires counting  such that 
.
The condition translates into:

which is equivalent to . If , the number of already seen characters on
the right is at least , so the condition becomes .
All in all, there are at most  different possible values of .

Generating the landmark levels associated with a given segment can be done in  time by performing the above calculation.
\end{proof}

Due to the above Lemma~\ref{lem:few levels}, to achieve the claimed  time complexity for processing ,
we only need to show how to run all relevant processes inside a segment with a dense description using landmarks on a particular level
 associated to that segment in  time.


\paragraph{Relevant processes.} We need to consider all relevant processes , such that  is a landmark on level , implying that
. The condition is equivalent to:

which, denoting , is in turn equivalent to:

(unless  does not divide , when no  needs to be considered), so by computing the multiplicative inverse we finally get
a base solution to \eqref{eq:alfa1}:

and the general solution is:

Thus we also get the solution to the original equation:


Therefore, with a simple calculation we get a succinct description of all values of  which should be taken into the account.
Before we proceed further, let us comment on the complexity of the calculation. Since  is fixed, both values of

can be computed in  time when we create the segment and stored there.

The situation now is that we have a dense description of a segment, and want to run all processes  of the form \eqref{eq:gform}
inside the segment. Additionally, because we do not necessarily have all possible landmarks on level , just a few most recent,
we are interested only in sufficiently large . Observe, that we can analyze separately processes of the following two forms:

From now on we will only consider the former, as the whole reasoning still holds after replacing  by . We will also assume
that only  need to be considered, which can be  ensured by decreasing  by an appropriate multiple of
.

Because  is a period of the whole segment,  is its period as well, and furthermore we can assume that
, as otherwise there are just at most two relevant processes to run.
Intuitively, knowing how far the period extends to the left and to the right allows us to restrict the number of processes to run
by an argument based on the combinatorial properties of palindromes.
While computing how far the period extends exactly is not possible in our setting, it can be approximated quite well using the landmarks.
First, we need to introduce the notion of ghost landmarks.

\paragraph{Ghost landmarks.} For every level of landmarks , we store  most recently seen
landmarks on level . We call them ghost landmarks on level . All ghost landmarks can be maintained in the
same manner as the regular landmarks, so storing them does not change the complexity of our algorithm.

\begin{lemma}
\label{lem:using ghosts}
If  is a landmark level associated to a segment , then for any  inside  there exists at least one ghost landmark on
level  in .
\end{lemma}

\begin{proof}
Consider a segment  of length . By Observation~\ref{obs:lifespan}, the number of already seen characters on the right
of  when it is being created is at least . Let  be any position inside 
causing  to be associated to , \ie, , and denote
. Notice that  might be either smaller or larger than the current . Because  is a landmark
on level  at , we have that .

Now consider any  inside  and denote . Since  and  both belong to the same segment of length ,
. Applying Observation~\ref{obs:lifespan} again, we also get that
.

Thus the number of already seen characters on the right of  can be bounded as follows:

Because , we have , so
the above bound can be rewritten as:

where the last inequality holds because .
Since the leftmost ghost landmark on level  has at least  already seen characters on its right,
by the above calculation it must be on the left of  as claimed.
\end{proof}

Now going back to approximating how far the period extends to the left and to the right, we proceed as follows.
We choose  of length  starting at . Because we have adjusted  so that only  need to
be considered and , we can assume that  is fully within the segment.
We know that the whole segment can be covered by repeating  to the left and to the right (where, possibly, the last repetition is
a suffix or a prefix of , respectively), and would like to figure out how far we can continue that until we hit either a boundary of the already seen
, or a subword of length  which is different than . This can be approximated quite well using the ghost landmarks,
if  repeats at least twice.

\begin{lemma}
For any  such that , , and  contains at least one ghost landmark on level
, we can compute in  time 
such that  and either  or .
\label{lemma:extend}
\end{lemma}

\begin{proof}
By the assumption about ghost landmark on level , we can access any  with
 in  time.
Hence if we are lucky and , we can compute  for any 
in  time, which allows us to binary search for  in  time. In more
detail, to check if  we check if  is a~period of ,
which can be done by comparing  and
.

\begin{figure}[t]
\includegraphics[width=\textwidth]{extend}
\caption{A number of repetitions of  such that  implies that  is a~period of a
certain full fragment between two ghost landmarks.} 
\label{fig:extend}
\end{figure}

In the general case, let , where .
If  and , then  is a~period of ,
see Fig.~\ref{fig:extend}, where . In the other direction, if  is a~period of
 and , then . (The assumption that
 is crucial.) Hence we can determine the largest  such that
 is a~period of  in  time using ghost landmarks
on level , and then simply return , which guarantees .
\end{proof}

\begin{lemma}
For any  such that , , and  contains at least one ghost landmark on level
, we can compute in  time 
such that  and either  or .
\label{lemma:extend2}
\end{lemma}

\begin{proof}
The proof will be very similar to the proof of Lemma~\ref{lemma:extend}, except that we have to take into the account the fact that while there might
be many more repetitions of  to the left, we might not have enough ghost landmarks on level  to detect them.

Let , where .
If , then  is a period of ,
where . In the other direction, if  is a period of
, then .
So we only need to binary search for the largest  such that  is a period of
 and return .
The remaining difficulty is that  might lie too far on the left to be a ghost landmark on level ,
so the binary search needs to be slightly modified.
We first choose the largest  such that  is a ghost landmark on level . There
are two possibilities.
\begin{enumerate}
\item  is a period of , then the largest
 might exceed . But we can return , because then
, and the choice of  and the assumption, by Lemma~\ref{lem:using ghosts}, implies
,
so .
\item Otherwise, we binary search over all , and return .
\end{enumerate}
We can binary search for  in  time, so the total time is .
\end{proof}

We apply Lemma~\ref{lemma:extend} and Lemma~\ref{lemma:extend2} to
approximate how many times  repeats on its right and on its left in  with accuracy ,
assuming that  occurs at .
Notice that there might be many more repetitions to the left in the whole , but Lemma~\ref{lemma:extend2} does not
allow us to detect all of them.
Now the crucial insight is that even though we do now know the exact number of repetitions, we can iterate through the at most  possible combinations of
the number of of repetitions to the left and to the right right, and the additionally consider the possibility that there is only a single
occurrence of  in the segment. Hence we need to iterate through  possibilities in total.
For each such combination, we will restrict the number of processes which should be run, therefore by running the processes
determined for each of these combinations we will not lose the correctness.
Hence from now on we assume that we know the exact number of repetitions of  to the left and to the right.

We need the following two simple structural results, which allow us to bound the palindromic radius in a sufficiently periodic subword of the
text. A similar (in spirit) argument appeared already in~\cite{Apostolico}, but we need a slightly different formulation. We say that a palindrome
centered at  reaches  if .

\begin{lemma}
\label{lemma:structure1}
Consider  starting at position  in , where ,  is a palindrome, and .
For any , if the palindrome centered at  reaches  then .
\end{lemma}

\begin{proof}
Take any . For a palindrome centered at  to reach ,  must be at least
. But then either  or , which is a contradiction.
\end{proof}

\begin{lemma}
\label{lemma:structure2}
Consider  starting at position  in , where ,  is a palindrome, , and .
For any , the palindrome centered at  cannot reach .
Additionally, either all palindromes centered at  with  reach , or none of them do.
\end{lemma}

\begin{figure}[t]
\includegraphics[width=\textwidth]{structure2}
\caption{All palindromes centered at positions  with  reach  if  is a~prefix of .}
\label{fig:structure2}
\end{figure}

\begin{proof}
Take any . If , then because  the radius at  is too small
for the palindrome centered at  to reach .
Otherwise, let , where  because , see
Fig.~\ref{fig:structure2}. Now either  is not a prefix
of , and we actually get the situation from Lemma~\ref{lemma:structure1}, so only  can possibly correspond
to a palindrome reaching , or 
is a~prefix of , and for all  the palindrome centered at  reaches .
\end{proof}

Recall that we want to run all  inside the segment with , and our  starts at .
We know that  repeats  times to the left in  and  times to the right till the end of the
already seen . The actual number of repetitions of  to the left in the whole , denoted ,
might be larger than . By Lemma~\ref{lemma:structure1} and Lemma~\ref{lemma:structure2}, either all processes of the form
 with  will succeed, or just the one with
 will succeed. Because the size of the buffer is , we only need to ensure that the  leftmost
processes which will succeed are run.
To guarantee this, we run all processes of the form  for 
which are still inside the segment.
This is correct, as following two cases show.

\begin{enumerate}
\item The process  with  is on the left of the segment, so either all or none
processes of such form in the segment are alive.
\item The process  with  is inside the segment, so  cannot be too large.
More precisely, , and consequently .
\end{enumerate}

We run a constant number of processes, each of them in  time, but to ensure that every segment is processed in such
complexity, we also need to remove the binary search used to approximate how
many times  can be repeated to the left and to the right.

Recall that ,  starts at  and lies
fully within a segment , and furthermore  is a period of the whole . As mentioned before, we can also assume that ,
as otherwise there are at most two processes which might need to be run. We can compute how many times  can be repeated to
its left (or rather approximate this value as described in Lemma~\ref{lemma:extend2}) when the segment is created, as the result
does not depend on the current value of .
Similarly, we can compute how many times it can be repeated to the right when we create the segment, but here the important
difference is that we might continue till the very end of the current , \ie, the next copy of  might extend beyond the
current prefix . It can be seen that in such a case the next time we need to deal with the same segment, at most one
additional copy of  fits inside . This happens because the segment is relevant when , and
. Therefore, the number of times  repeats to the right can be maintained in  time.

\begin{theorem}
\label{thm:time_efficient}
Any scheduling scheme with  for all 
can be simulated using  additional space on the top of the
space taken by the landmarks and  time to process .
\end{theorem}

\section{Lower bounds}
\label{section:lowerbounds}

In this section we use Yao's minimax principle~\cite{Yao77} to prove
lower bounds on the space complexity of computing the largest radius of a palindrome in a word
of length  over an alphabet  in the streaming model. We denote this problem
by \palin.



\begin{theorem}[Yao's minimax principle for randomized algorithms]
\label{yao}
Let  be the set of inputs for a problem and  be the set of all deterministic algorithms solving it. Then,
for any  and , the cost of running  on  is denoted by .

Let  be the probability distribution over , and let  be an algorithm chosen at random according to . Let  be
the probability distribution over , and let  be an input chosen at random according to . Then
the worst-case expected cost of the randomized algorithm is at least as large as the cost of the best deterministic algorithm against the chosen
distribution on the inputs:

\end{theorem}

We use the above theorem for both Las Vegas and Monte Carlo algorithms. For Las Vegas algorithms, we consider only correct algorithms, and  is the
memory usage.
For Monte Carlo algorithms, we consider all algorithms (not necessarily correct) with memory usage not exceeding a certain threshold,
and  is the correctness indicator function, \ie,  if the algorithm is correct and  otherwise.

Our proofs will be based on appropriately chosen padding. The padding requires a constant number
of fresh characters.
If  is twice as large as the number of required fresh characters, we can still use half of
it to construct a difficult input instance, which does not affect the asymptotics. Otherwise,
we construct a difficult input instance over , then add enough new fresh characters
to facilitate the padding, and finally reduce the resulting larger alphabet to binary at the expense
of increasing the size of the input by a constant factor.

\begin{lemma}
\label{alphabetreduction}
For any alphabet  there exists a morphism  such that, for any ,  and, for any word ,  contains a palindrome
of length  if and only if  contains a palindrome of length .
\end{lemma}

\begin{proof}
We set:

Clearly  and, because every  is a palindrome, if  contains a palindrome
of length  then  contains a palindrome of length . Now assume that
 contains a palindrome of length , where . 
If  then we obtain that  should contain a palindrome of length , which always holds.
Otherwise, the palindrome contains  inside and we consider two cases.
\begin{enumerate}
\item The palindrome is centered inside . Then it corresponds to an odd palindrome of length
 in .
\item The palindrome maps some  to another . Then it corresponds to an even palindrome
of length  in .
\end{enumerate}
In either case, the claim holds.
\end{proof}

For the padding we will often use an infinite word , or more precisely
its prefixes of length , denoted . Here  and  should be understood as two characters
not belonging to the original alphabet, which is then reduced using the above lemma. The longest
palindrome inside  has radius . 


We first show that any Las Vegas approximation algorithm must necessarily use
 bits of memory in expectation in both variants, so Las Vegas
randomization is essentially useless here.
By Yao's minimax principle, it is enough to construct a distribution over the inputs, which is hard for
any deterministic algorithm using less memory. We restrict the inputs to a family of strings
of the form \, where \Sigma\nu(\aerr)\aerr\nu(\aerr) x \ are special characters not belonging to .
Let us look at the memory usage of  after having read . We say that  is "good" when the memory usage 
is at most  and "bad" otherwise.
Assume that  of all 's are good, then there are two strings
 such that the state of  after having read both  and  is exactly the same. Hence the behavior of  on
\ and \ is exactly the same. The former is a palindrome of radius , so  
must answer at least  , and consequently the latter also must contain a palindrome of radius at least .
A palindrome inside \ is either fully contained within
, ,  or it is a middle palindrome.
But the longest palindrome inside  is of length  (for  large enough)
and the longest palindrome inside  or  is of length , so
\
contains a middle palindrome of radius . This implies that , which is a contradiction.
Therefore, at least  of all 's are bad. But then the expected memory usage of  is at least
, which for  is  as claimed.

Now consider solving \palin with multiplicative error . 
An algorithm with multiplicative error  can also be considered as having additive error , so if the expected memory usage of
such an algorithm is  and  then we obtain
an algorithm with additive error  and expected memory usage , which we already know to be impossible.
\end{proof}

Now we move to Monte Carlo algorithms. We first consider exact algorithms solving \palin;
lower bounds on approximation algorithms will be then obtained by padding the input
appropriately. We introduce an auxiliary problem \midpalin, which is to compute radius
of the middle palindrome in a word of length  over an alphabet . We want to
show that solving \midpalin exactly with error probability smaller than  requires
 bits of space. By Yao's minimax principle,
it is enough to construct a distribution over the inputs, such that any deterministic algorithm
using less memory is not able to distinguish between inputs with different answers
reasonably often. This can be done by considering uniform distribution on inputs of the form
. Then amplification
(running multiple instances of an algorithm in parallel) gives us a lower bound on the space
complexity of any algorithm solving \midpalin exactly. The lower bound can be translated
to \palin by padding the input in the middle, so that the longest palindrome must be centered
in the middle.

\begin{restatable}{lemma}{restatablelowerboundexact}
\label{lowerbound:exact}
There exists a constant   such that any randomized Monte Carlo streaming algorithm
 solving \midpalin or \palin exactly with probability 
uses at least  bits of memory.
\end{restatable}


\begin{proof}
First we prove that if  is a Monte Carlo streaming algorithm solving \midpalin exactly using less than  bits
of memory, then its error probability is at least .

By Theorem~\ref{yao}, it is enough to construct probability distribution  over  such that for any deterministic
algorithm  using less than  bits of memory, the expected probability of error on a word chosen
according to  is at least .

Let . For any ,  and  we define:

Now  is the uniform distribution over all such .

Since there are  possible strings of length  and we assume that
 uses at most  bits,
we can 
partition at least half of these strings into pairs , such that  is in the same state after reading either  or . (If we choose an arbitrary maximal
matching of strings into pairs, at most half of possible strings will be left unpaired, that is one per each possible state of .)
Let  be longest common suffix of  and , so  and , where  are single characters.  Then
 returns the same answer on  and , even though the radius of the middle palindrome is exactly  in one of
them, and at least  in the other one. Therefore,  errs on at least one of these two inputs. Similarly, it errs on either
 or . Thus the error probability is at least . 

Now we can prove the lemma for \midpalin with a standard amplification trick.
Say that we have a~Monte Carlo streaming algorithm, which solves \midpalin exactly
with error probability  using  bits of memory. Then we can run its  instances simultaneously and return
the most frequently reported answer. The new algorithm needs  bits of memory and its error probability  satisfies:


Let us choose , for some constant .
Now we can prove the theorem. Assume that  uses less than  bits of memory. Then running  (which holds since )
instances of  in parallel requires less than  bits of memory. But then 
the error probability of the new algorithm is bounded from above by:

which we have already shown to be impossible.

The lower bound for \midpalin can be translated into a lower bound for solving \palin exactly
by padding the input so that the longest palindrome is centered in the middle.
Let  and
 be the input for \midpalin. We define:

Now if the radius of the middle palindrome in  is , then  contains a palindrome of radius
at least . In the other direction, any palindrome inside  of radius larger than  must be centered somewhere
in the middle block consisting of only zeroes and both ones are mapped to each other, so it must be
the middle palindrome.
Thus, the radius of the longest palindrome inside  is exactly , so we have reduced solving \midpalin to solving
\palin[2n+2]. We already know that solving \midpalin[n] with probability 
requires  bits of memory,
so solving \palin[2n+2] with probability  requires  bits of memory.
Notice that the reduction needs  additional bits of memory to count up to , but for large  this is
much smaller than the lower bound if we choose .
\end{proof}


To obtain a lower bound for Monte Carlo additive approximation, we observe that any algorithm
solving \palin with additive error  can be used to solve \palin[\frac{n-\aerr}{\aerr+1}] exactly
by inserting  zeroes between every two characters, in the very beginning, and in the very end.
However, this reduction requires  additional bits of memory for counting up to 
and cannot be used when the desired lower bound on the required number of bits
 is significantly smaller than . 
Therefore, we need a separate technical lemma which implies that either additive or multiplicative
approximation with error probability  requires  bits of space.


\begin{restatable}{lemma}{restatablehashinglb}
\label{hashing_lb}
Let  be any randomized Monte Carlo streaming algorithm solving \palin with additive error at most  or multiplicative error at most
  and error probability .
Then  uses  bits of memory.
\end{restatable}


\begin{proof}
By Theorem~\ref{yao}, it is enough to construct a probability distribution  over , such that for
any deterministic algorithm  using at most  bits of memory, the expected probability of error on a word chosen according
to  is .

Let .  For any , let .
Observe that if  then  contains a palindrome of radius , and otherwise the longest palindrome there has
radius at most , thus any algorithm with additive error of at most  or with a multiplicative error at most 
must be able to distinguish between these two cases (for  large enough).

Let  be an arbitrary family of words of length  such that , and let  be 
the uniform distribution on all words of the form , where  and  are chosen uniformly and independently from .
By a counting argument, we can create at least  pairs  of elements
from  such that the state of  is the same after having read  and . 
(If we create the pairs greedily, at most one such  per state of memory can be left unpaired, so at least  elements are paired.)
Thus,  cannot
distinguish between  and , and between  and , so its error probability must be at least
. Thus if , the error rate is at least  for  large enough, a contradicion.
\end{proof}

Combining the reduction with the technical lemma and 
taking into account that we are reducing to a problem with word length of ,
we obtain the following.

\begin{restatable}[Monte Carlo additive approximation]{theorem}{restatablemontecarloadditive}
\label{th:montecarlo_additive_lowerbound}
Let  be any randomized Monte Carlo streaming algorithm solving \palin with additive error 
with probability . If  then  uses
 bits of memory.
\end{restatable}

\begin{proof}
Define 

Because of Lemma~\ref{hashing_lb} and 
(which holds due to ),
it is enough to prove that  is a lower bound when 
 
Assume that there is a Monte Carlo streaming algorithm  solving \palin with additive error  using  
bits of memory and probability . 
Let  (the last inequality holds because  and because we can assume that ).  Given a word , we can simulate running 
on  to calculate  (using  additional bits of memory), and then return .
We call this new Monte Carlo streaming algorithm .
Recall that  reports the radius of the longest palindrome with additive error . Therefore, if the original word contains
a palindrome of radius , the new word contains a palindrome of radius , so  and 
will return at least . In the other direction, if  returns , then the new word contains a palindrome of radius .
If such palindrome is centered so that  is matched with  for some , then
it clearly corresponds to a palindrome of radius  in the original word. But otherwise every
 within the palindrome is matched with , so in fact the whole palindrome corresponds to a streak
of consecutive zeroes in the new word and can be extended to the left and to the right to start and end
with , so again it corresponds to a palindrome of radius  in the original word.
Therefore,  solves \palin[n'] exactly with probability
 and uses
 bits of memory. Observe that by Lemma~\ref{lowerbound:exact} we get a lower bound 

 (where the last inequality holds because of Eq.\eqref{foo}). Then, for large 
we obtain contradiction as follows

\end{proof}


Finally, we consider multiplicative approximation. Here we observe that any algorithm solving
\palin with multiplicative error  can be used  to solve \midpalin[2n'] exactly, where
, by separating the characters appropriately.
Intuitively, the padding is chosen so that the middle palindrome has the largest radius and
the larger the distance from the center the longer the separator inserted between two consecutive
characters of the original input. Again, we need  bits for a counter and hence need
to invoke a separate technical lemma when  is very large. After some calculations,
and taking into account that we are reducing to a problem with word length of 
we obtain the following.

\begin{restatable}[Monte Carlo multiplicative approximation]{theorem}{restatablemontecarlomultiplicative}
\label{th:montecarlo_multiplicative_lowerbound}
Let  be any randomized Monte Carlo streaming algorithm solving \palin with multiplicative error 
with probability . If  then  uses  bits of memory.
\end{restatable}


\begin{proof}
For  then the claimed lower bound reduces to  bits,
which obviously holds. Thus we can assume that .
Define 
First we argue that it is enough to prove that  uses
 bits of memory.
Since , we have that:

and consequently:

Finally, observe that:
 
because  for ,
and  for .
From \eqref{eq:sigma1} and \eqref{eq:2eps} we conclude that:


Because of Lemma~\ref{hashing_lb} and equations \eqref{eq:2eps} and \eqref{eq:sigma2},
it is enough to prove that  is a lower bound when
 
as otherwise .

Assume that there is a Monte Carlo streaming algorithm  solving \palin with multiplicative error 
with probability  using  bits of memory.
Let  be an input for \midpalin[2n']. We choose  so that . Then .
We choose  so that   for any .

(Observe that for  we have  and .) Finally we define:


If  contains a middle palindrome of radius exactly , then  contains a middle palindrome of radius  .
Also, based on the properties of , any non-middle centered palindrome in  has radius at most , which is less than  for  large 
enough. Since , value of  can be extracted from the answer of .
Thus, if  approximates the middle palindrome in  with multiplicative error  with probability 
using
 bits of memory, we can construct a new algorithm  solving \midpalin[2n'] exactly with
probability  using

bits of memory.
By Lemma~\ref{lowerbound:exact}  we get a lower bound

(where the last inequality holds because of~(\ref{foo2})). On the other hand, for large 

so~\eqref{eq:lb2} exceeds~\eqref{eq:ub2}, a contradiction.
\end{proof}

\newpage

\section*{Acknowledgments}

The first author is currently holding a post-doctoral position at Warsaw Center of Mathematics and Computer Science.
However, most of this work has been done when the first author was at Max-Planck-Institut f\"{u}r Informatik and
the second author at LIF, CNRS – Aix Marseille University (supported by the Labex Archimède and by the ANR project MACARON (ANR-13-JS02-0002)).

The authors would like to thank Tomasz Syposz for a suggestion which allowed them to simplify the algorithm.

\bibliography{main}

\end{document}
