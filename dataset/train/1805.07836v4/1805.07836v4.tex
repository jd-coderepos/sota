\documentclass{article}





\usepackage[final]{nips_2018}



\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      

\usepackage{amsmath,amssymb, amsthm}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{adjustbox}

\usepackage{booktabs}
\usepackage{multirow}
\captionsetup[table]{skip=10pt}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{natbib}

\renewcommand{\algorithmicrequire}{\textbf{Input }}
\renewcommand{\algorithmicensure}{\textbf{Output }}

\title{Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels}



\author{Anonymous \\
 \\
}
\author{
 Zhilu Zhang \qquad  Mert R. Sabuncu \\
 Electrical and Computer Engineering\\
 Meinig School of Biomedical Engineering \\
 Cornell University\\
 \texttt{zz452@cornell.edu, msabuncu@cornell.edu} \\
}
\begin{document}


\maketitle

\begin{abstract}
Deep neural networks (DNNs) have achieved tremendous success in a variety of applications across many disciplines. 
Yet, their superior performance comes with the expensive cost of requiring correctly annotated large-scale datasets. 
Moreover, due to DNNs' rich capacity, errors in training labels can hamper performance. 
To combat this problem, mean absolute error (MAE) has recently been proposed as a noise-robust alternative to the commonly-used categorical cross entropy (CCE) loss. 
However, as we show in this paper, MAE can perform poorly with DNNs and challenging datasets.
Here, we present a theoretically grounded set of noise-robust loss functions that can be seen as a generalization of MAE and CCE. 
Proposed loss functions can be readily applied with any existing DNN architecture and algorithm, while yielding good performance in a wide range of noisy label scenarios. 
We report results from experiments conducted with CIFAR-10, CIFAR-100 and FASHION-MNIST datasets and synthetically generated noisy labels. 
\end{abstract}

\section{Introduction}
The resurrection of neural networks in recent years, together with the recent emergence of large scale datasets, has enabled super-human performance on many classification tasks~\cite{krizhevsky2012imagenet, mnih2015human, noh2015learning}. 
However, supervised DNNs often require a large number of training samples to achieve a high level of performance. 
For instance, the ImageNet dataset~\cite{deng2009imagenet} has 3.2 million hand-annotated images. 
Although crowdsourcing platforms like Amazon Mechanical Turk have made large-scale annotation possible, some error during the labeling process is often inevitable, and mislabeled samples can impair the performance of models trained on these data. 
Indeed, the sheer capacity of DNNs to memorize massive data with completely randomly assigned labels~\cite{zhang2016understanding} proves their susceptibility to overfitting when trained with noisy labels. 
Hence, an algorithm that is robust against noisy labels for DNNs is needed to resolve the potential problem.
Furthermore, when examples are cheap and accurate annotations are expensive, it can be more beneficial to have datasets with more but noisier labels than less but more accurate labels \cite{khetan2017learning}. 


Classification with noisy labels is a widely studied topic~\cite{frenay2014classification}. 
Yet, relatively little attention is given to directly formulating a noise-robust loss function in the context of DNNs.
Our work is motivated by Ghosh et al. \cite{ghosh2017robust} who theoretically showed that mean absolute error (MAE) can be robust against noisy labels under certain assumptions.
However, as we demonstrate below, the robustness of MAE can concurrently cause increased difficulty in training, and lead to performance drop. This limitation is particularly evident when using DNNs on complicated datasets. 
To combat this drawback, we advocate the use of a more general class of noise-robust loss functions, which encompass both MAE and CCE. 
Compared to previous methods for DNNs, which often involve extra steps and algorithmic modifications, changing only the loss function requires minimal intervention to existing architectures and algorithms, and thus can be promptly applied. 
Furthermore, unlike most existing methods, the proposed loss functions work for \textit{both} closed-set and open-set noisy labels~\cite{wang2018iterative}.  
Open-set refers to the situation where samples associated with erroneous labels do not always belong to a ground truth class contained within the set of known classes in the training data. 
Conversely, closed-set means that all labels (erroneous and correct) come from a known set of labels present in the dataset.

The main contributions of this paper are two-fold.
First, we propose a novel generalization of CCE and present a theoretical analysis of proposed loss functions in the context of noisy labels. 
And second, we report a thorough empirical evaluation of the proposed loss functions using CIFAR-10, CIFAR-100 and FASHION-MNIST datasets, and demonstrate significant improvement in terms of classification accuracy over the baselines of MAE and CCE, under both closed-set and open-set noisy labels. 

The rest of the paper is organized as follows. Section~\ref{sec_2} discusses existing approaches to the problem. Section~\ref{sec_3} introduces our noise-robust loss functions. Section~\ref{sec_4} presents and analyzes the experiments and result. Finally, section~\ref{sec_5} concludes our paper. 

\section{Related Work}\label{sec_2}
Numerous methods have been proposed for learning with noisy labels with DNNs in recent years. Here, we briefly review the relevant literature. Firstly, Sukhbaatar and Fergus \cite{sukhbaatar2014learning} proposed accounting for noisy labels with a confusion matrix so that the cross entropy loss becomes

where  represents number of classes,  represents noisy labels,  represents the latent true labels and  is the 'th component of the confusion matrix. 
Usually, the real confusion matrix is unknown. 
Several methods have been proposed to estimate it \cite{goldberger2016training, hendrycks2018using, patrini2017making, jindal2016learning, han2018masking}.
Yet, accurate estimations can be hard to obtain. 
Even with the real confusion matrix, training with the above loss function might be suboptimal for DNNs. 
Assuming (1) a DNN with enough capacity to memorize the training set, and (2) a confusion matrix that is diagonally dominant, minimizing the cross entropy with confusion matrix is equivalent to minimizing the original CCE loss.
This is because the right hand side of Eq.~\ref{eq:1} is minimized when  for  and  otherwise,  .     
 
In the context of support vector machines, several theoretically motivated noise-robust loss functions like the ramp loss, the unhinged loss and the savage loss have been introduced \cite{brooks2011support,van2015learning,masnadi2009design}. More generally, Natarajan et al. \cite{natarajan2013learning} presented a way to modify any given surrogate loss function for binary classification to achieve noise-robustness. However, little attention is given to alternative noise robust loss functions for DNNs. Ghosh et al. \cite{ghosh2015making,ghosh2017robust} proved and empirically demonstrated that MAE is robust against noisy labels.
This paper can be seen as an extension and generalization of their work.
 
Another popular approach attempts at cleaning up noisy labels. 
Veit et al. \cite{veit2017learning} suggested using a label cleaning network in parallel with a classification network to achieve more noise-robust prediction. 
However, their method requires a small set of clean labels. 
Alternatively, one could gradually replace noisy labels by neural network predictions \cite{reed2014training,tanaka2018joint}. 
Rather than using predictions for training, Northcutt et al. \cite{northcutt2017learning} offered to prune the correct samples based on softmax outputs. 
As we demonstrate below, this is similar to one of our approaches. 
Instead of pruning the dataset once, our algorithm iteratively prunes the dataset while training until convergence. 

Other approaches include treating the true labels as a latent variable and the noisy labels as an observed variable so that EM-like algorithms can be used to learn true label distribution of the dataset \cite{xiao2015learning,khetan2017learning,vahdat2017toward}. 
Techniques to re-weight confident samples have also been proposed. 
Jiang et al. \cite{jiang2017mentornet} used a LSTM network on top of a classification model to learn the optimal weights on each sample, while Ren, et al. \cite{ren2018learning} used a small clean dataset and put more weights on noisy samples which have gradients closer to that of the clean dataset. 
In the context of binary classification, 
Liu et al. \cite{liu2016classification} derived an optimal importance weighting scheme for noise-robust classification. Our method can also be viewed as re-weighting individual samples; instead of explicitly obtaining weights, we use the softmax outputs at each iteration as the weightings. 
Lastly, Azadi et al. \cite{azadi2015auxiliary} proposed a regularizer that encourages the model to select reliable samples for noise-robustness. Another method that uses knowledge distillation for noisy labels has also been proposed~\cite{li2017learning}. Both of these methods also require a smaller clean dataset to work. 

\section{Generalized Cross Entropy Loss for Noise-Robust Classifications}\label{sec_3}
\subsection{Preliminaries}
We consider the problem of k-class classification. Let  be the feature space and  be the label space. In an ideal scenario, we are given a clean dataset , where each . A classifier is a function that maps input feature space to the label space . In this paper, we consider the common case where the function is a DNN with the softmax output layer.
For any loss function , the (empirical) risk of the classifier  is defined as 
, where the expectation is over the empirical distribution.
The most commonly used loss for classification is cross entropy. In this case, the risk becomes:

where  is the set of parameters of the classifier,  corresponds to the 'th element of one-hot encoded label of the sample ,  such that , and  denotes the 'th element of . Note that, , and , since the output layer is a softmax. The parameters of DNN can be optimized with empirical risk minimization.

We denote a dataset with label noise by  where 's are the noisy labels with respect to each sample such that . In this paper, we make the common assumption that noise is conditionally independent of inputs given the true labels so that 

In general, this noise is defined to be \textit{class dependent}. Noise is \textit{uniform} with noise rate , if  for , and  for . The risk of classifier with respect to noisy dataset is then defined as . 

Let  be the global minimizer of the risk . Then, the empirical risk minimization under loss function  is defined to be \textit{noise tolerant} \cite{manwani2013noise} if  is a global minimum of the noisy risk . 

A loss function is called \textit{symmetric} if, for some constant , 

The main contribution of Ghosh et al. \cite{ghosh2015making} is they proved that if loss function is \textit{symmetric} and , then under \textit{uniform} label noise, for any , . Hence,  is also the global minimizer for  and  is noise tolerant. Moreover, if , then  is also noise tolerant under class dependent noise. 

Being a \textit{nonsymmetric} and \textit{unbounded} loss function, CCE is  sensitive to label noise. On the contrary, MAE, as a \textit{symmetric} loss function, is noise robust. 
For DNNs with a softmax output layer, MAE can be computed as:

With this particular configuration of DNN, the proposed MAE loss is, up to a constant of proportionality, the same as the unhinged loss  \cite{van2015learning}. 

\subsection{ Loss for Classification}
In this section, we will argue that MAE has some drawbacks as a classification loss function for DNNs, which are normally trained on large scale datasets using stochastic gradient based techniques. Let's look at the gradient of the loss functions:

Thus, in CCE, samples with softmax outputs that are less congruent with provided labels, and hence smaller  or larger , are implicitly weighed more than samples with predictions that agree more with provided labels in the gradient update. This means that, when training with CCE, more emphasis is put on difficult samples.
This implicit weighting scheme is desirable for training with clean data, but can cause overfitting to noisy labels. 
Conversely, since the  term is absent in its gradient, MAE treats every sample equally, which makes it more robust to noisy labels. 
However, as we demonstrate empirically, this can lead to significantly longer training time before convergence. Moreover, without the implicit weighting scheme to focus on challenging samples, the stochasticity involved in the training process can make learning difficult. 
As a result, classification accuracy might suffer.

To demonstrate this, we conducted a simple experiment using ResNet \cite{he2016deep} optimized with the default setting of Adam \cite{kingma2014adam} on the CIFAR datasets \cite{krizhevsky2009learning}. 
Fig. \ref{cce_mae}(a) shows the test accuracy curve when trained with CCE and MAE respectively on CIFAR-10. 
As illustrated clearly, it took significantly longer to converge when trained with MAE. 
In agreement with our analysis, there was also a compromise in classification accuracy due to the increased difficulty of learning useful features. 
These adverse effects become much more severe when using a more difficult dataset, such as CIFAR-100 (see Fig.~\ref{cce_mae}(b)). 
Not only do we observe significantly slower convergence, but also a substantial drop in test accuracy when using MAE. 
In fact, the maximum test accuracy achieved after 2000 epochs, a long time after training using CCE has converged, was , while CCE achieved an higher accuracy of  after merely 7 epochs! Despite its theoretical noise-robustness, due to the shortcoming during training induced by its noise-robustness, we conclude that MAE is not suitable for DNNs with challenging datasets like ImageNet.
\begin{figure}
\centering
\begin{subfigure}{.31\textwidth}
\centering
\includegraphics[width=1.0\linewidth]{test_acc_cifar10_500epochs.png}
\label{fig:sfig2}
\caption{}
\end{subfigure}
\begin{subfigure}{.31\textwidth}
\centering
\includegraphics[width=1.0\linewidth]{test_acc_cifar100_2000epochs.png}
\label{fig:sfig2}
\caption{}
\end{subfigure}
\begin{subfigure}{.31\textwidth}
\centering
\includegraphics[width=1.0\linewidth]{average_softmax_lq_cce.png}
\label{fig:sfig2}
\caption{}
\end{subfigure}
\caption{(a), (b) Test accuracy against number of epochs for training with CCE (orange) and MAE (blue) loss on clean data with (a) CIFAR-10 and (b) CIFAR-100 datasets. (c) Average softmax prediction for correctly (solid) and wrongly (dashed) labeled training samples, for CCE (orange) and  (, blue) loss on CIFAR-10 with uniform noise ().}
\label{cce_mae}
\end{figure}

To exploit the benefits of both the noise-robustness provided by MAE and the implicit weighting scheme of CCE, we propose using the the negative Box-Cox transformation \cite{box1964analysis} as a loss function: 

where . Using L'Hôpital's rule, it can be shown that the proposed loss function is equivalent to CCE for , and becomes MAE/unhinged loss when . 
Hence, this loss is a generalization of CCE and MAE. 
Relatedly, Ferrari and Yang \cite{ferrari2010maximum} viewed the maximization of Eq.~\ref{eq:Lq} as a generalization of maximum likelihood and termed the loss function , which we also adopt.


Theoretically, for any input , the sum of  loss with respect to all classes is bounded by:

Using this bound and under uniform noise with , we can show (see Appendix) 

where ,  is the global minimizer of , and  is the global minimizer of . The larger the , the larger the constant , and the tighter the bound of Eq.~\ref{eq:bounds2}. 
In the extreme case of  (i.e., for MAE),  and . 
In other words, for  values approaching 1, the optimum of the noisy risk will yield a risk value (on the clean data) that is close to , which implies noise tolerance. 
It can also be shown that the difference  is bounded under class dependent noise, provided  and   (see Thm 2 in Appendix).

The compromise on noise-robustness when using  over MAE prompts an easier learning process. 
Let's look at the gradients of  loss to see this:

where    and . 
Thus, relative to CCE,  loss weighs each sample by an additional  so that less emphasis is put on samples with weak agreement between softmax outputs and the labels, which should improve robustness against noise. 
Relative to MAE, a weighting of  on each sample can facilitate learning by giving more attention to challenging datapoints with labels that do not agree with the softmax outputs. 
On one hand, larger  leads to a more noise-robust loss function. 
On the other hand, too large of a  can make optimization strenuous. 
Hence, as we will demonstrate empirically below, it is practically useful to set  between  and , where a tradeoff equilibrium is achieved between noise-robustness and better learning dynamics. 

\subsection{Truncated  Loss}
\label{trunc_loss}
Since a tighter bound in  would imply stronger noise tolerance, we propose the truncated  loss:

where , and . Note that, when , the truncated  loss becomes the normal  loss.  Assuming , the sum of truncated  loss with respect to all classes is bounded by (see Appendix):

where . It can be verified that the difference between upper and lower bounds for the truncated  loss, , is smaller than that for the  loss of Eq.~\ref{eq:bounds}, if 

As an example, when , the above inequality is satisfied for all  and . When , the inequality is satisfied for all q and . Since the derived bounds in Eq.~\ref{eq:bounds} and Eq.~\ref{eq:trunc_bound} are tight, introducing the threshold  can thus lead to a more noise tolerant loss function.

If the softmax output for the provided label is below a threshold, truncated  loss becomes a constant. 
Thus, the loss gradient is zero for that sample, and it does not contribute to learning dynamics. 
While Eq.~\ref{eq:trunc_bound} suggests that a larger threshold  leads to tighter bounds and hence more noise-robustness, too large of a threshold would precipitate too many discarded samples for training. 
Ideally, we would want the algorithm to train with all available clean data and ignore noisy labels. 
Thus the optimal choice of  would depend on the noise in the labels.
Hence,  can be treated as a (bounded) hyper-parameter and optimized.
In our experiments, we set  that yields a tighter bound for truncated  loss, and which we observed to work well empirically.


A potential problem arises when training directly with this loss function. When the threshold is relatively large (e.g.,  in a -class classification problem), at the beginning of the training phase, most of the softmax outputs can be significantly smaller than , resulting in a dramatic drop in the number of effective samples. 
Moreover, it is suboptimal to prune samples based on softmax values at the beginning of training. 
To circumvent the problem, observe that, by definition of the truncated  loss:

where  if  and  otherwise, and  represents the parameters of the classifier. 
Optimizing the above loss is the same as optimizing the following:

because for any , the optimal  is  if  and  if .
Hence, we can optimize the truncated  loss by optimizing the right hand side of Eq.~\ref{eq:trunc_obj}. If  is convex with respect to the parameters , optimizing Eq.~\ref{eq:trunc_obj} is a \textit{biconvex optimization} problem, and the alternative convex search (ACS) algorithm \cite{bazaraa2013nonlinear} can be used to find the global minimum. ACS iteratively optimizes  and  while keeping the other set of parameters fixed. 
Despite the high non-convexity of DNNs, we can apply ACS to find a local minimum. We refer to the update of  as "pruning". At every step of iteration, pruning can be carried out easily by computing  for all training samples. Only samples with  and  are kept for updating  during that iteration (and hence  ). The additional computational complexity from the pruning steps is negligible. Interestingly, the resulting algorithm is similar to that of self-paced learning \cite{kumar2010self}. 
\begin{algorithm}
\caption{ACS for Training with  Loss}\label{euclid}
\algorithmicrequire{Noisy dataset , total iterations , threshold }
\begin{algorithmic}
\State Initialize  
\State Update 
\While{}
\State Update  [Pruning Step]
\State Update 
\EndWhile
\end{algorithmic}
\algorithmicensure{}
\end{algorithm}

\section{Experiments}\label{sec_4}
The following setup applies to all of the experiments conducted. Noisy datasets were produced by artificially corrupting true labels.  of the training data was retained for validation. To realistically mimic a noisy dataset while justifiably analyzing the performance of the proposed loss function, only the training and validation data were contaminated, and test accuracies were computed with respect to true labels. A mini-batch size of 128 was used. All networks used ReLUs in the hidden layers and softmax layers at the output. All reported experiments were repeated five times with random initialization of neural network parameters and randomly generated noisy labels each time. We compared the proposed functions with CCE, MAE and also the confusion matrix-corrected CCE, as shown in Eq.~\ref{eq:1}. Following \cite{patrini2017making}, we term this "forward correction". All experiments were conducted with identical optimization procedures and architectures, changing only the loss functions.  

\subsection{Toward a Better Understanding of  Loss}
\label{Lq_exp}
To better grasp the behavior of  loss, we implemented different values of  and uniform noise at different noise levels, and trained ResNet-34 with the default setting of Adam on CIFAR-10. 
As shown in Fig.~\ref{Lq_diff_q}, when trained on clean dataset, increasing  not only slowed down the rate of convergence, but also lowered the classification accuracy. 
More interesting phenomena appeared when trained on noisy data. When CCE () was used, the classifier first learned predictive patterns, presumably from the noise-free labels, before overfitting strongly to the noisy labels, in agreement with Arpit et al.'s observations~\cite{arpit2017closer}. 
Training with increased  values delayed overfitting and attained higher classification accuracies. 
One interpretation of this behavior is that the classifier could learn more about predictive features before overfitting.
This interpretation is supported by our plot of the average softmax values with respect to the correctly and wrongly labeled samples on the training set for CCE and   loss, and with  uniform noise (Fig.~\ref{cce_mae}(c)). 
For CCE, the average softmax for wrongly labeled samples remained small at the beginning, but grew quickly when the model started overfitting. 
 loss, on the other hand, resulted in significantly smaller softmax values for wrongly labeled data.
This observation further serves as an empirical justification for the use of truncated  loss as described in section~\ref{trunc_loss}.

We also observed that there was a threshold of  beyond which overfitting never kicked in before convergence. 
When  for instance, training with  loss with  produced an overfitting-free training process. Empirically, we noted that, the noisier the data, the larger this threshold is. 
However, too large of a  hampers the classification accuracy, and thus a larger  is not always preferred. 
In general,  can be treated as a hyper-parameter that can be optimized, say via monitoring validation accuracy.
In remaining experiments, we used , which yielded a good compromise between fast convergence and noise robustness (no overfitting was observed for ). 
\begin{figure}
\centering
\begin{subfigure}{.31\textwidth}
\centering
\includegraphics[width=1.0\linewidth]{Lq_test_acc_noise0_0.png}
\caption{}
\end{subfigure}
\begin{subfigure}{.31\textwidth}
\centering
\includegraphics[width=1.0\linewidth]{Lq_test_acc_noise0_2.png}
\caption{}
\end{subfigure}
\begin{subfigure}{.31\textwidth}
\centering
\includegraphics[width=1.0\linewidth]{Lq_test_acc_noise0_6.png}
\caption{}
\end{subfigure}
\begin{subfigure}{.31\textwidth}
\centering
\includegraphics[width=1.0\linewidth]{Lq_val_loss_noise0_0.png}
\caption{}
\end{subfigure}
\begin{subfigure}{.31\textwidth}
\centering
\includegraphics[width=1.0\linewidth]{Lq_val_loss_noise0_2.png}
\caption{}
\end{subfigure}
\begin{subfigure}{.31\textwidth}
\centering
\includegraphics[width=1.0\linewidth]{Lq_val_loss_noise0_6.png}
\caption{}
\end{subfigure}
\caption{The test accuracy and validation loss against number of epochs for training with  loss at different values of . (a) and (d): ; (b) and (e): ; (c) and (f): . }
\label{Lq_diff_q}
\end{figure}
\begin{table}[ht]
\caption{Average test accuracy and standard deviation (5 runs) on experiments with closed-set noise. We report accuracies of the epoch where validation accuracy is maximum. Forward  and  represent forward correction with the true and estimated confusion matrices, respectively~\cite{patrini2017making}.  was used for all experiments with  loss and truncated  loss. Best 2 accuracies are \textbf{bold faced}.}
\centering
\begin{adjustbox}{width=1\textwidth}
\small
\begin{tabular}{c|c|cccc|cccc}
\hline
\multirow{3}{*}{Datasets} & \multirow{3}{*}{Loss Functions} & \multicolumn{4}{c}{Uniform Noise} & \multicolumn{4}{c}{Class Dependent Noise} \\ \cline{3-10} 
 &  & \multicolumn{4}{c}{Noise Rate } & \multicolumn{4}{c}{Noise Rate } \\
& & 0.2 & 0.4 & 0.6 & 0.8 & 0.1 & 0.2 & 0.3 & 0.4 \\ \hline \hline
\multirow{6}{*}{\begin{tabular}[c]{@{}c@{}}FASHION\\MNIST\\ \end{tabular}} & CCE &  &  &  &  &  &  &  &  \\
 & MAE &  &  &  &  &  &  &  &  \\
 & Forward  &  &  &  &  &  &  &  &  \\
 & Forward  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &   \\
& Trunc  &  &  &  &  &  &  &  &  \\\hline
\hline
\multirow{6}{*}{CIFAR-10} & CCE & 86.98  0.44 & 81.88  0.29 & 74.14  0.56 & 53.82  1.04 & 90.69  0.17 & 88.59  0.34 & 86.14  0.40 & 80.11 1.44 \\
 & MAE & 83.72  3.84 & 67.00  4.45 & 64.21  5.28 & 38.63  2.62 & 82.61  4.81 & 52.93  3.60 & 50.36  5.55 & 45.52  0.13 \\
 & Forward  & 88.63  0.14 & 85.07  0.29 & 79.12  0.64 &  &  &  &  &  \\
  & Forward  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  & 76.74 0.61 \\
& \begin{tabular}[c]{@{}c@{}}Trunc  \end{tabular} &  &  &  &  &   &  &  &  \\ \hline
\hline
\multirow{6}{*}{CIFAR-100} & CCE & 58.72  0.26 & 48.20  0.65 & 37.41  0.94 & 18.10  0.82 &  &  &  &  \\
 & MAE & 15.80  1.38 & 9.03  1.54 & 7.74  1.48 & 3.76  0.27 &  &  &  &  \\
 & Forward  & 63.16  0.37 & 54.65  0.88 & 44.62  0.82 & 24.83  0.71 &  &  &  &  \\
 & Forward  &  &  &  &  &  &  &  &  \\
 &  &  &  &  &  &  &  &  &  \\
& Trunc  &  &  &  &  &  &  &  &  
\\ \hline
\end{tabular}
\end{adjustbox}
\label{result_table}
\end{table}
\subsection{Datasets}
\textbf{CIFAR-10/CIFAR-100}:
ResNet-34 was used as the classifier optimized with the loss functions mentioned above. Per-pixel mean subtraction, horizontal random flip and  random crops after padding with 4 pixels on each side was performed as data preprocessing and augmentation. Following \cite{huang2016deep}, we used stochastic gradient descent (SGD) with  momentum, a weight decay of  and learning rate of , and divided it by 10 after 40 and 80 epochs (120 in total) for CIFAR-10, and after 80 and 120 (150 in total) for CIFAR-100. To ensure a fair comparison, the identical optimization scheme was used for truncated  loss. We trained with the entire dataset for the first 40 epochs for CIFAR-10 and 80 for CIFAR-100, and started pruning and training with the pruned dataset afterwards. Pruning was done every 10 epochs. To prevent overfitting, we used the model at the optimal epoch based on maximum validation accuracy for pruning. Uniform noise was generated by mapping a true label to a random label through uniform sampling. Following Patrini, et al. \cite{patrini2017making} class dependent noise was generated by mapping TRUCK  AUTOMOBILE, BIRD  AIRPLANE, DEER  HORSE, and CAT\  DOG with probability  for CIFAR-10. For CIFAR-100, we simulated class-dependent noise by flipping each class into the next circularly with probability .

We also tested noise-robustness of our loss function on open-set noise using CIFAR-10. For a direct comparison, we followed the identical setup as described in \cite{wang2018iterative}. For this experiment, the classifier was trained for only 100 epochs. We observed validation loss plateaued after about 10 epochs, and hence started pruning the data afterwards at 10-epoch intervals. The open-set noise was generated by using images from the CIFAR-100 dataset. A random CIFAR-10 label was assigned to these images. 

\textbf{FASHION-MNIST}: ResNet-18 was used. The identical data preprocessing, augmentation, and optimization  procedure as in CIFAR-10 was deployed for training. To generate a realistic class dependent noise, we used the t-SNE \cite{maaten2008visualizing} plot of the dataset to associated classes with similar embeddings, and mapped BOOT  SNEAKER , SNEAKER  SANDALS, PULLOVER  SHIRT, COAT  DRESS with probability . 
\begin{table}[]
\caption{Average test accuracy on experiments with CIFAR-10. We replicated the exact experimental setup as in \cite{wang2018iterative}. The reported accuracies are the average last epoch accuracies after training for 100 epochs. . CCE, Forward and method by Wang et al. are adapted for direct comparison.}
\centering
\begin{adjustbox}{width=1.0\textwidth}
\label{my-label}
\begin{tabular}{l|cccccc}
\hline
Noise type & \multicolumn{1}{l}{CCE \cite{wang2018iterative}} & \multicolumn{1}{l}{Forward \cite{wang2018iterative}} & \multicolumn{1}{l}{Wang, et al. \cite{wang2018iterative}} & MAE & \multicolumn{1}{l}{} & \multicolumn{1}{l}{Trunc } \\ \hline \hline
CIFAR-10 + CIFAR-100 (open-set noise) & 62.92 & 64.18 & 79.28 & 75.06 & 71.10 &  \\ \hline
CIFAR-10 (closed-set noise) & 62.38 & 77.81 & 78.15 & 74.31 & 64.79 &  \\ \hline
\end{tabular}
\end{adjustbox}
\label{open-set}
\end{table}
\subsection{Results and Discussion}
Experimental results with closed-set noise is summarized in Table \ref{result_table}. For uniform noise, proposed loss functions outperformed the baselines significantly, including forward correction with the ground truth confusion matrices. 
In agreement with our theoretical expectations, truncating the  loss enhanced results. 
For class dependent noise, in general Forward  offered the best performance, as it relied on the knowledge of the ground truth confusion matrix. 
Truncated  loss produced similar accuracies as Forward  for FASHION-MNIST and better results for CIFAR datasets, and outperformed the other baselines at most noise levels for all datasets.
While using  loss improved over baselines for CIFAR-100, no improvements were observed for FASHION-MNIST and CIFAR-10 datasets. 
We believe this is in part because very similar classes were grouped together for the confusion matrices and consquently the DNNs might falsely put high confidence on wrongly labeled samples. 

In general, classification accuracy for both uniform and class dependent noise would be further improved relative to baselines with optimized  and  values and more number of epochs.
Based on the experimental results, we believe the proposed approach would work well when correctly labeled data can be differentiated from wrongly labeled data based on softmax outputs, which is often the case with large-scale data and expressive models.
We also observed that MAE performed poorly for all datasets at all noise levels, presumably because DNNs like ResNet struggled to optimize with MAE loss, especially on challenging datasets such as CIFAR-100.

Table \ref{open-set} summarizes the results for open-set noise with CIFAR-10. Following Wang et al.~\cite{wang2018iterative}, we reported the last-epoch test accuracy after training for 100 epochs. 
We also repeated the closed-set noise experiment with their setup. Using  loss noticeably prevented overfitting, and using truncated  loss achieved better results than the state-of-the-art method for open-set noise reported in~\cite{wang2018iterative}. Moreover, our method is significantly easier to implement. Lastly, note that the poor performance of  loss compared to MAE is due to the fact that test accuracy reported here is long after the model started overfitting, since a shallow CNN without data augmentation was deployed for this experiment.

\section{Conclusion}\label{sec_5}
In conclusion, we proposed theoretically grounded and easy-to-use classes of noise-robust loss functions, the  loss and the truncated  loss, for classification with noisy labels that can be employed with any existing DNN algorithm. We empirically verified noise robustness on various datasets with both closed- and open-set noise scenarios.

\subsubsection*{Acknowledgments}
This work was supported by NIH R01 grants (R01LM012719 and R01AG053949), the NSF NeuroNex grant 1707312, and NSF CAREER grant (1748377).

\bibliography{reference}
\bibliographystyle{plain}

\newpage
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem*{remark}{Remark}

\section*{Appendix}
\begin{lemma}
, where  represents the  loss, and  represents the categorical cross entropy loss.  
\end{lemma}
\begin{proof}
from equation \ref{eq:Lq}, and using L'Hôpital's rule,

\end{proof}

\begin{lemma}
For any  and , the sum of  loss with respect to all classes is bounded by:

\end{lemma}
\begin{proof}
Observe that, since we have a softmax layer at the end,  for all , and . Now, since , we have , and . Hence, 

Moreover, since  for all  and , ,
and 

\end{proof}

\begin{theorem}
Under uniform noise with ,

and

where , ,  is the global minimizer of , and  is the global minimizer of .
\end{theorem}
\begin{proof}
Recall that for any softmax output , 

and since for uniform noise with noise rate ,  for , and  for , we have

 
Now, from Lemma 2, we have:

We can also write the inequality in terms of :

Thus, for ,  

or equivalently, 

where  and , since , and  is a minimizer of  . Lastly, since  is the minimizer of , we have that , or  . This completes the proof.
\end{proof}
\begin{remark}
Note that, when , , and  is also minimizer of risk under uniform noise. 
\end{remark}

\begin{theorem}
Under class dependent noise when , , , where , , and , if , then

where ,  is the global minimizer of , and  is the global minimizer of .
\end{theorem}
\begin{proof}
For class dependent noise, from Lemma 2, for any soft-max output function  we have

and 

Hence, 

Now, from our assumption that , we have . This is only satisfied iff  when , and  if . Hence,  . Moreover, by our assumption, we have . As a result, to derive a upper bound for the expression above, we need to maximize the second term. Note that by definition of the  loss,  , and hence the second term is maximized iff  . 
This implies that the maximum of the second term is non-positive, so we have 

Lastly, since  is the minimizer of , we have that . This completes the proof.
\end{proof}

\begin{lemma}
For any  and , assuming  where  represents the number of classes, the sum of truncated  loss with respect to all classes is bounded by:

where .
\end{lemma}
\begin{proof}
For the upper bound, by definition of truncated ,  for any  and . Hence, . 


For the lower bound, it can be verified that,   
where , with  and  is the number of elements in  with a value .  Note that since , : 

We can get a universal lower bound (that does not depend on ) by minimizing the above function with respect to . To do so, we treat  to be continuous. By definition of  loss, and recall that ,

We can verify using the second derivative test that the above objective function is convex. As a result, we can find the minimum by taking its derivative. Doing so, we find that  minimizes the above objective function. Hence, the lower bound is 

where .
\end{proof}
\begin{remark}
Using Lemma 3, we can prove that the proposed truncated loss leads to more noise robust training following the same arguments as in Theorem 1 and 2.  
\end{remark}



\end{document}