\documentclass[11pt]{article}
\usepackage{fullpage}
\textwidth 6.5in
\textheight 9in
\baselineskip 13.2pt

\usepackage{times}
\usepackage{amssymb,amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{todonotes}



\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{definition}[theorem]{Definition}

\newcommand{\sq}{\hbox{\rlap{}}}
\newcommand{\qed}{\hspace*{\fill}\sq}
\newenvironment{proof}{\noindent {\bf Proof.}\ }{\qed\par\vskip 4mm\par}
\newenvironment{proofof}[1]{\bigskip \noindent {\bf Proof of #1:}\quad }
{\qed\par\vskip 4mm\par}

\newcommand{\N}{\ensuremath{\mathbb{N}}}
\newcommand{\R}{\ensuremath{\mathbb{R}}}

\def\pred{{\rm pred}}
\def\succ{{\rm succ}}
\def\argmin{{\rm argmin}}
\def\argmax{{\rm argmax}}

\begin{document}
   

\date{}
\begin{titlepage}

\title{CONE-DHT: A distributed self-stabilizing algorithm for a heterogeneous storage system}

\author{Sebastian Kniesburges \\
   University of Paderborn \\
   seppel@upb.de \\
   \and
   Andreas Koutsopoulos \\
   University of Paderborn  \\
   koutsopo@mail.upb.de\\
   \and
   Christian Scheideler\\
   University of Paderborn  \\
   scheideler@mail.upb.de
   }

\maketitle \thispagestyle{empty}



\begin{abstract}
We consider the problem of managing a dynamic heterogeneous storage system in
a distributed way so that the amount of data assigned to a host in that system
is related to its capacity. Two central problems have to be solved for this:
(1) organizing the hosts in an overlay network with low degree and diameter so
that one can efficiently check the correct distribution of the data and route
between any two hosts, and (2) distributing the data among the hosts so that
the distribution respects the capacities of the hosts and can easily be
adapted as the set of hosts or their capacities change. We present distributed
protocols for these problems that are self-stabilizing and that do not need
any global knowledge about the system such as the number of nodes or the
overall capacity of the system. Prior to this work no solution was known
satisfying these properties.
\end{abstract}

\bigskip




\end{titlepage}


\section{Introduction}

In this paper we consider the problem of designing distributed protocols for a
dynamic heterogeneous storage system. Many solutions for distributed storage
systems have already been proposed in the literature. In the peer-to-peer
area, distributed hash tables (DHTs) have been the most popular choice. In a
DHT, data elements are mapped to hosts with the help of a hash function, and
the hosts are organized in an overlay network that is often of hypercubic
nature so that messages can be quickly exchanged between any two hosts. To be
able to react to dynamics in the set of hosts and their capacities, a
distributed storage system should support, on top of the usual data
operations, operations to join the system, to leave the system, and to change
the capacity of a host in the desired way. We present self-stabilizing
protocols that can handle all of these operations in an efficient way.


\subsection{Heterogeneous storage systems}

Many data management strategies have already been proposed for distributed
storage systems. If all hosts have the same capacity, then a well-known
approach called {\em consistent hashing} can be used to manage the data
\cite{conhash}. In consistent hashing, the data elements are hashed to points
in , and the hosts are mapped to disjoint intervals in , and a
host stores all data elements that are hashed to points in its interval. An
alternative strategy is to hash data elements and hosts to pseudo-random bit
strings and to store (indexing information about) a data element at the host
with longest prefix match \cite{Plaxton}. These strategies have been realized
in various DHTs including CAN \cite{RF+01:can}, Pastry \cite{RD01:pastry} and
Chord \cite{SM+01:chord}. However, all of these approaches assume hosts of
uniform capacity, despite the fact that in P2P systems the peers can be highly
heterogeneous.

In a heterogeneous setting, each host (or node)  has its specific capacity
 and the goal considered in this paper is to distribute the data among
the nodes so that node  stores a fraction of  of the data. The simplest solution would be to reduce the
heterogeneous to the homogeneous case by splitting a host of  times the
base capacity (e.g., the minimum capacity of a host) into  many virtual
hosts. Such a solution is not useful in general because the number of virtual
hosts would heavily depend on the capacity distribution, which can create a
large management overhead at the hosts. Nevertheless, the concept of virtual
hosts has been explored before (e.g., \cite{godfrey,rao, skewccc+}). In
\cite{godfrey} the main idea is not to place the virtual hosts belonging to a
real host randomly in the identifier space but in a restricted range to
achieve a low degree in the overlay network. However, they need an estimation
of the network size and a classification of nodes with high, average, and low
capacity. A similar approach is presented in \cite{skewccc+}.
Rao et al. \cite{rao} proposed some schemes also based on virtual servers, where the data is moved from heavy nodes to light nodes to balance the load after the data assignment, so and data movement is induced even without joining or leaving nodes.
In \cite{supernode} the authors organize the nodes into clusters, where a
super node (i.e., a node with large capacity) is supervising a cluster of
nodes with small capacities. Giakkoupis et al. \cite{Giakkoupis} present an
approach which focuses on homogeneous networks but also works for
heterogeneous one. However, updates can be costly.


Several solutions have been proposed in the literature that can manage
heterogeneous storage systems in a centralized way, i.e. they consider data
placement strategies for heterogeneous disks that are managed by a single
server ~\cite{rio,slicing,spread,diskarray,broadscale,redShare} or assume a
central server that handles the mapping of data elements to a set of hosts
~\cite{cone,brink2,brink}. We will only focus on the most relevant ones for
our approach. In \cite{brink} Brinkmann et al. introduced several criteria a
placement scheme needs to fulfill, like a faithful distribution, efficient
localization, and fast adaptation. They introduce two different data placement
strategies named SHARE and SIEVE that fulfill their criteria. To apply their
approach, the number of nodes and the overall capacity of the system must be
known. In \cite{redShare} redundancy is added to the SHARE strategy to allow a
fair and redundant data distribution, i.e. several copies of a data element
are stored such that no two copies are stored on the same host. Another
solution to handle redundancy in heterogeneous systems is proposed in
\cite{spread}, but also here the number of nodes and the overall capacity of
the system must be known. The only solution proposed so far where this is not
the case is the approach by Schindelhauer and Schomaker \cite{cone}, which we
call {\em cone hashing}. Their basic idea is to assign a distance function to
each host that scales with the capacity of the host. A data element is then
assigned to the host of minimum distance with respect to these distance
functions. We will extend their construction into a self-stabilizing DHT with
low degree and diameter that does not need any global information and that can
handle all operations in a stable system efficiently with high probability
(w.h.p.)\footnote{ I.e., a probability of  for any constant }.

\subsection{Self-Stabilization}

A central aspect of our self-stabilizing DHT is a self-stabilizing overlay
network that can be used to efficiently check the correct distribution of the
data among the hosts and that also allows efficient routing. There is a large
body of literature on how to efficiently maintain overlay networks, e.g.,
\cite{AS03:skip,AS04:hyperring,BK+04:Pagoda,RD01:pastry,HJ+03:skipnet,KSW05:self-repair,MNR02:viceroy,NW07:continous-discrete,RF+01:can,
SM+01:chord}. While many results are already known on how to keep an overlay
network in a legal state, far less is known about self-stabilizing overlay
networks. A self-stabilizing overlay network is a network that can recover its
topology from an arbitrary weakly connected state. The idea of
self-stabilization in distributed computing was introduced in a classical paper
by E.W. Dijkstra in 1974 \cite{D74:self-stab} in which he looked at the
problem of self-stabilization in a token ring. In order to recover certain
network topologies from any weakly connected state, researchers have started
with simple line and ring networks (e.g.
\cite{CF05:stab-ring,SR05:ring,ORS07:lin}). Over the years more and more
network topologies were considered
\cite{DK08,JRS+09:delaunay,JR+09:skip+,DT09,DT10,smallworld}. In
\cite{KKS11} the authors present a self-stabilizing algorithm for the Chord
DHT \cite{SM+01:chord}, which solves the uniform case, but the problem of
managing heterogeneous hosts in a DHT was left open, which is addressed in
this paper. To the best of our knowledge this is the first self-stabilizing
approach for a distributed heterogeneous storage system.

In this paper we present a self-stabilizing overlay network for a distributed
heterogeneous storage system based on the data assignment presented in
\cite{cone}.

\subsection{Model}

\subsubsection{Network model}

We assume an asynchronous message passing model for the CONE-DHT which is
related to the model presented in \cite{NNS11:corona} by Nor et al. The
overlay network consists of a static set  of  nodes or hosts. We further
assume \emph{fixed identifiers} (ids) for each node. These identifiers are
\emph{immutable} in the computation, we only allow identifiers to be compared,
stored and sent. In our model the identifiers are used as addresses, such that
by knowing the identifier of a node another node can send messages to this
node. The identifiers form a unique order. The communication between nodes is
realized by passing messages through channels. A node  can send a message
to  through the channel . We denote the channel  as the
union of all channels . We assume that the capacity of a channel is
unbounded and no messages are lost. Furthermore we assume that for a
transmission pair  the messages sent by  are received by  in the
same order as they are sent, i.e.  is a FIFO channel. Note that this
does not imply any order between messages from different sending nodes. For
the channel we assume \emph{eventual delivery} meaning that if there is a
state in the computation where there is a message in the channel  there
also is a later state where the message is not in the channel, but was
received by the process. We distinguish between the \emph{node state}, that is
given by the set of identifiers stored in the internal variables  can communicate with, and the \emph{channel state}, that is
given by all identifiers contained in messages in a channel . We
model the network by a directed graph . The set of edges
 describes the possible communication pairs.  consists of two subsets:
the \emph{explicit edges}  and the \emph{implicit edges}
, i.e.
. Moreover we define .

\subsubsection{Computational Model}
An action has the form . \emph{guard} is a
predicate that can be true or false. \emph{command} is a sequence of
statements that may perform computations or send messages to other nodes. We
introduce one special guard predicate  called the \emph{timer
predicate}, which is periodically true; i.e. according to an internal clock  becomes true after a number of clock cycles and is false  the other times,  and allows the nodes to perform
periodical actions. A second predicate is true if a message is received by a
node.
The \emph{program state} is defined by the node states and the channel
states of all nodes, i.e. the assignment of values to every variable of each
node and messages to every channel. We call the combination of the node
states of all nodes the \emph{node state of the system} and the combination
of the channel states of all nodes is called the \emph{channel state of the
system}.
An action is enabled in some state if its guard is true and disabled
otherwise.
A \emph{computation} is a sequence of states such that for each state 
the next state  is reached by executing an enabled action in .
By this definition, actions can not overlap and are executed atomically
giving a sequential order of the executions of actions. For the execution of
actions we assume \emph{weak fairness} meaning that if an action is enabled
in all but finitely many states of the computation then this action is
executed infinitely often.

We state the following requirements on our solution: \emph{Fair load
balancing}: every node with x\% of the available capacity gets x\% of the
data. \emph{Space efficiency}: Each node stores at most\\  information. \emph{Routing
efficiency}: There is a routing strategy that allows efficient routing in at
most  hops. \emph{Low degree}: The degree of each node is
limited by . Furthermore we require an algorithm that
builds the target network topology in a \emph{self-stabilizing} manner, i.e.,
any weakly connected network  is eventually transformed into a
network so that a (specified) subset of the explicit edges forms the target
network topology ({\em convergence}) and remains stable as long as no node
joins or leaves ({\em closure}).

\subsection{Our contribution}

We present a self-stabilizing algorithm that organizes a set of heterogeneous
nodes in an overlay network such that each data element can be efficiently
assigned to the node responsible for it. We use the scheme described in
\cite{cone} (which gives us good load balancing) as our data management scheme
and present a distributed protocol for the overlay network, which is efficient
in terms of message complexity and information storage and moreover works in a
self-stabilizing manner. The overlay network efficiently supports the basic
operations of a heterogeneous storage system, such as the joining or leaving
of a node, changing the capacity of a node, as well as searching, deleting and
inserting a data element. In fact we show the following main result:

\begin{theorem}\label{theo:main}
There is a self-stabilizing algorithm for maintaining a heterogeneous storage
system that achieves fair load-balancing, space efficiency and routing
efficiency, while each node has a degree of  w.h.p. The
data operations can be handled in  time in a stable system, and if
a node joins or leaves a stable system or changes its capacity, it takes at
most  structural changes, i.e., edges that are created
or deleted, until the system stabilizes again.
\end{theorem}

\subsection{Structure of the paper}

The paper is structured as follows: In Section ~\ref{cone-dht} we describe our target network and its properties.
In Section ~\ref{alg} we present our self-stabilizing protocol and prove that
it is correct. Finally, in Section ~\ref{ops} we describe the functionality of
the basic network operations.





\section{The \emph{CONE}-DHT}\label{cone-dht}


\subsection{The original CONE-Hashing}

Before we present our solution, we
first give some more details on the original CONE-Hashing \cite{cone} our
approach is based on. In \cite{cone} the authors present a centralized
solution for a heterogeneous storage system in which the nodes are of
different capacities. We denote the  capacity of a node  as . We use
a hash function  that assigns to each node a hash value. A
data element of the data set  is also hashed by a hash function . W.l.o.g. we assume that all hash values and capacities are
distinct.  According to \cite{cone} each node has a capacity function
, which determines which data is assigned to the node. A node is
\emph{responsible} for those elements  with , i.e.  is assigned to . We denote by  the \emph{responsibility range} of
  (see Figure ~\ref{example}). Note that  can consist of several intervals in .
In the original paper \cite{cone}, the authors considered two special cases
of capacity functions, one of linear form
 and of logarithmic form
. For these capacity
functions the following results were shown by the authors \cite{cone}:

\begin{theorem}\label{theo:coneOriginal1}
A data element  is assigned to a node  with probability
 for linear capacity functions
 and with probability  for logarithmic capacity functions . Thus in expectation fair load balancing can be achieved by using a logarithmic
capacity function .
\end{theorem}

The CONE-Hashing supports the following operations for a data element  or
a node :

\begin{itemize}\itemsep0.1pt
        \item \emph{Search(d)}: Returns the node  such that .
        \item \emph{Insert(d)}:  is assigned to  the node returned by .
        \item \emph{Delete(d)}:  is removed from the node returned by .
        \item \emph{Join(v)}: For all  the responsibility ranges
 are updated and data elements , with  are moved
to .
        \item \emph{Leave(v)}: For all  the responsibility ranges
 are updated and data elements  assigned to  are moved to nodes
 such that .
        \item \emph{CapacityChange(v)}: For all  the responsibility
ranges  are updated and data elements  not assigned to , but with  are moved to  while data elements  assigned to 
but with  are moved to nodes .
\end{itemize}

Moreover, the authors showed that the fragmentation is relatively small for the  logarithmic
capacity function, with each node having in expectation a logarithmic number of intervals it is responsible for.
In the case of the linear function, it can be shown that this number is only constant in expectation.

In \cite{cone} the authors further present a data structure to efficiently support the described
operations in a centralized approach. For their data structure they showed that
there is an algorithm that determines for a data element  the
corresponding node  with  in expected time . The used data structure has a size of  and the
joining, leaving and the capacity change of a node can be handled efficiently.


In the following we show that CONE-Hashing can also be realized by using a
distributed data structure. Further the following challenges have to be
solved. We need a suitable topology on the node set  that supports an
efficient determination of the responsibility ranges  for each node
 . The topology should also support an efficient \emph{Search(d)}
algorithm, i.e. for an \emph{Search(d)} query inserted at an arbitrary node
, the node  with  should be found. Furthermore a
\emph{Join(v)}, \emph{Leave(v)}, \emph{CapacityChange(v)} operation should
not lead to a high amount of data movements, (i.e. not more than the data
now assigned to  or no longer assigned to  should be moved,) or a high amount of structural changes ( i.e. changes in the topology built on
). All these challenges will be solved by our CONE-DHT.

\subsection{The \emph{CONE}-DHT}\label{cone}

In order to construct a heterogeneous storage network in the distributed case, we have to deal with the challenges mentioned above.
For that, we introduce the  \emph{CONE}-graph, which is an overlay network that, as we show, can support efficiently a heterogeneous storage system.

\subsubsection{The network layer}

We define the \emph{CONE} graph as a graph , with  being the hosts of our storage system.

For the determination of the edge set, we need following definitions, with respect to a node :

 \begin{itemize}\itemsep0.1pt
\item 
is the next node at the right of  with larger capacity, and we call it the first larger successor of .
Building upon this, we define recursively the i-th larger successor of  as:
, and the union of all larger successors as
 .

\item The first larger predecessor of  is defined as:
 
i.e. the next node at the left of  with larger capacity. The i-th larger predecessor of  is:
 , and the union of all larger predecessors as
 .

\item We also define the set of the smaller successors of , , as the set of all nodes , with , and the set of the smaller predecessors of ,  as the set of all nodes , such that .

\end{itemize}

Now we can define the edge-set of a node in .
 
\begin{definition}
 iff 
\end{definition}

We define also the neighborhood set of  as .
In other words,  maintains connections to each node , if there does not exist another node with larger capacity than  between  and  (see Figure ~\ref{example3}).
We will prove that this graph is sufficient for maintaining a heterogeneous storage network in a self-stabilizing manner and also that in this graph the degree is bounded logarithmically w.h.p..

\subsubsection{The data management layer}

We discussed above how the data is assigned to the different nodes. That is the assignment strategy we use for data in the \emph{CONE}-network.

In order to understand how the various data operations are realized in the network, we have to describe how each node maintains the knowledge about the data it
has, as well as the intervals it is responsible for.
It turns out that in order for a data item to be forwarded to the correct node, which is responsible for storing it, it suffices to contact the closest node (in terms of hash value) from the left to the data item's hash value.
That is because then, if the \emph{CONE} graph has been established, this node (for example node  in  Figure ~\ref{example}) is aware of the responsible node for this data item. We call the interval between  and the hash value of 's closest right node . We say that  is \emph{supervising} .
We show the following theorem.



\begin{theorem}\label{theo:responsibility}
In  a node  knows all the nodes v with .
\end{theorem}

\begin{proof}
We need to show that all these nodes  are in  . 
W.l.o.g. let us consider only the case of .
Indeed, there cannot be a node at the right of  () that has a responsible interval in 's supervising interval and that is not in  or .We will prove it by contradiction.
Let  be such a node. For  not to be in  or  there must be at least one node  larger (in terms of capacity) than , which is closer to  than  (). Then  it holds that , since  is increasing. Moreover, since  we have , so  dominates  for . And since , it cannot be that  is responsible for an interval in , since in that region  is dominated (at least) by .
This contradicts the hypothesis and the proof is completed.
\end{proof}

So, the nodes store their data in the following way. If a node  has a data item that falls into one of its responsible intervals, it stores in addition to this item a reference to the node   that is the closest from the left to this interval. Moreover, the subinterval  thinks it is responsible for (in which the data item falls) is also stored (as described in the next section, when the node's internal variables are presented).
 In case the data item is not stored at the correct node,  can resolve the conflict when contacted by .

Now we can discuss the functionality of the data operations.
A node has operations for inserting, deleting and searching a datum in the CONE-network.

Let us focus on  a data item. As shown above, it suffices to search for the left closest node to the data item's hash value.
We do this by using greedy routing. Greedy routing in the \emph{CONE}-network works as follows: If a search request wants to reach some
position  in , and the request is currently at node , then  forwards  to the node  in  that is closest to , until the closest node at the left of  is reached. Then this node will forward the request to the responsible node.
A more formal definition of the greedy routing follows:

\begin{definition}
The CONE Greedy routing strategy is defined as: If operation op is to be executed at position  in  and op is currently at node , then  forwards op to the node  such that  if  or  forwards op to the node  such that  if . If  and , then  and  forwards  to the node responsible for the subinterval containing pos. If  and  then  forwards op to  as  is in its supervised interval.
\end{definition}

 In that way we can route to the responsible node and then get an answer whether the data item is found or not, and so the searching is realized.
Note that the  of a data item can be realized in the same way, only that when the item is found, it is also deleted from the responsible node.
 an item follows a similar procedure, with the difference that when the responsible node is found, the data item is stored by it.


Moreover, the network handles efficiently structural operations, such as the joining and leaving of a node in the network, or the change of the capacity of
a node. Since this handling falls into the analysis of the self-stabilization algorithm, we will discuss the network operations in Section ~\ref{alg}, where we also formally analyze the algorithm.

It turns out that a single data or network operation (i.e greedy routing) can be realized in a logarithmic number of hops in the \emph{CONE}-network, and this happens due  to
the structural properties of the network, which we discuss in the next section, where we also show that the degree of the \emph{CONE}-network is logarithmic.

\subsection{Structural Properties of a Cone Network}

In this section we show that the degree of a node in a stable CONE-network is bounded by  w.h.p, and hence the information stored by each node (i.e the number of nodes which it maintains contact to, ) is bounded by  w.h.p..

First we show following lemma:

\begin{lemma}\label{log}
In a stable CONE network for each ,  and  in  w.h.p.
\end{lemma}

\begin{proof}
For an arbitrary  let  and let  be sorted by ids in ascending order, such that  for all . Furthermore, let  be the set of all nodes with larger ids and larger capacities than .
So, the determination of  is done by continuously choosing the correct  out of , when  are already chosen.
In this process, each time a  is determined, the number of nodes from which  can be chosen is getting smaller, since the nodes at the left of  as well as the nodes with smaller capacity than  can be excluded.
We call the choice of  \emph{good}, if , i.e. the number of remaining nodes in  is (at least) halved. Let . Since the id/position for each node is assigned uniformly at random, we can easily see that Pr[ is a good choice].
Then after a sequence of  choices that contains  good choices the remaining set  is the empty set. Thus there can not be more than  good choices in any sequence of choices.
So, what we have now is a random experiment, that is described by the random variable , that is equal to the number choices we must make, until we managed to have made  good ones. Then the random variable  follows the negative binomial distribution.
In order to bound the value of  from above we apply the following tail bound for negative binomially distributed random variables shown in \cite{NegBi}, derived by using a Chernoff bound:

\begin{claim}
Let  have the negative binomial distribution with parameters  and , i.e. with probability  there is a success and  is equal to the number of trials needed for  successes. Pick  and set . Then 
\end{claim}

We apply this claim with  and  and we pick . Then . Thus with probability at least ,  as  also .
\end{proof}

\begin{lemma}
In a stable CONE network for each ,  and  are  and  and  are  w.h.p..
\end{lemma}

\begin{proof}
W.l.o.g. we consider only  and  in the proof.
For each node  being in an interval  it holds . But since each node has (at most) one , the sum over all ,  must be (at most) . So we have , so    .
That means for a node , .

Now we consider the second part of the statement.
Let  be the direct right neighbor of , i.e. the first (from the left) node in  (). Then we can observe that every node in  (expect ) must be in . Let us assume a node  is in   but not in , then there must be another node  and , such that . But then  would be also in   instead of .
So, we contradicted this scenario. As a consequence  , but we already shown that  w.h.p., from which follows that  w.h.p..
\end{proof}

Combining the two lemmas we get the following theorem.

\begin{theorem}\label{theo:degree}
The degree of each node in a stable CONE network is  w.h.p.
\end{theorem}

Additionally to the nodes in , ,  and  that lead to the degree of  w.h.p. a node  only stores references about the closest nodes left to the intervals it is responsible for, where it actually stores data. A node  stores at most one reference and one interval for each data item. Thus the storage only has a logarithmic overhead for the topology information and the following theorem follows immediately.

\begin{theorem}\label{theo:storage}
In a stable CONE network each node stores at most  information w.h.p.
\end{theorem}

Once the CONE network  is set up, it can be used as an heterogeneous storage system supporting inserting, deleting and searching for data.
The CONE Greedy routing implies the following bound on the diameter:

\begin{lemma}\label{lem:routing}
CONE Greedy routing takes on a stable CONE network w.h.p. no more than a logarithmic number of steps, i.e. the diameter of a CONE network is  w.h.p..
\end{lemma}

\begin{proof}
This follows directly from Lemma \ref{log}, where we showed that each node  has w.h.p. a logarithmic number of nodes in , which means it has a logarithmic distance to the node with the greatest capacity, and vice versa, which means that the node with the greatest capacity has logarithmic distance to every node in the network. The proof for the CONE Greedy routing follows from a generalization of this observation. If an operation  with position  is currently at node  w.l.o.g. we assume , then  is forwarded at most  times w.h.p. (along nodes in ) to a node  such that   and further  times w.h.p. (along nodes in ) from  to .
\end{proof}

\begin{figure}[htb]
\caption{In this example, the case of the linear capacity functions is presented.
Concerning the intervals lying between  and ,
we can see by the coloring which interval is assigned to which node (the one having the lowest capacity function value at that interval). According to the \emph{CONE}-graph, 
must be aware of all these nodes ( and ).}

\includegraphics[scale=0.4]{example2}\label{example}
\end{figure}

\begin{figure}[htb]

\caption{In this example, the size of the capacity of a node is symbolized by the height of its green column (i.e. the larger the capacity the higher the column). So, for example in this case  is aware of  and . In fact,    }

\includegraphics[scale=0.4]{example3}\label{example3}
\end{figure}
\section{Self-Stabilization Process}\label{alg}

\subsection{Topological Self-stabilization}
We now formally describe the problem of topological self-stabilization. In topological self-stabilization the goal is to state a protocol \emph{P} that \emph{solves} an overlay problem \emph{OP} starting from an initial topology of the set \emph{IT}. A protocol is \emph{unconditionally} self-stabilizing if \emph{IT} contains every possible state. Analogously a protocol is \emph{conditionally} self-stabilizing if \emph{IT} contains only states that fulfill some conditions. For topological self-stabilization we assume that \emph{IT} contains any state as long as  is weakly connected, i.e. the combined knowledge of all nodes in this state covers the whole network, and there are no identifiers that don't belong to existing nodes in the network.
The set of target topologies defined in \emph{OP} is given by , i.e. the goal topologies of the overlay problem are only defined on explicit edges and  can be an arbitrary (even empty) set of edges. We also call the program states in \emph{OP} \emph{legal states}. We say a protocol \emph{P} that solves a problem \emph{OP} is topologically self-stabilizing if for \emph{P} \emph{convergence} and \emph{closure} can be shown. \emph{Convergence} means that \emph{P} started with any state in \emph{IT} reaches a legal state in \emph{OP}. \emph{Closure} means that \emph{P} started in a legal state in \emph{OP} maintains a legal state.
For a protocol \emph{P} we assume that there are no oracles available for the computation. In particular we assume there to be no \emph{connection oracle}, that can connect disconnected parts of the network, no \emph{identifier detector}, that can decide whether an identifier belongs to an existing node or not, and no \emph{legal state detector}, that can decide based on global knowledge whether the system is in a legal state or not.
With these assumptions our model complies with the \emph{compare-store-send} program model in \cite{NNS11:corona} in which protocols do not manipulate the internals of the nodes' identifiers. For our modified model the impossibility results of \cite{NNS11:corona} still hold such as Lemma 1 that states if the graph  is initially disconnected, then the graph is disconnected in every state of the computation. Furthermore Theorem 1 states that if the goal topology is a single component a program only solves the problem if the initial graph is weakly connected.



\subsection{Formal problem definition and notation}\label{problem}



Now we define the problem we solve in this paper in the previously introduced notation. We provide a protocol \emph{P} that solves the overlay problem \emph{CONE} and is topologically self-stabilizing.



In order to give a formal definition of the edges in  and in  we firstly describe which internal variables are stored in each node , i.e. which edges exist in :

\begin{itemize}\itemsep0.1pt

\item 

\item : The first node to the right with a larger capacity than 

\item 

\item : The first node to the left with a larger capacity than 

\item 

\item 

\item : the set of right neighbors that  communicates with. We assume that the nodes are stored in ascending order so that . If there are  nodes in  then . 

\item : the set of left neighbors that  communicates with. We assume that the nodes are stored in descending order so that  If there are  nodes in  then .

\item  the data set, containing all intervals , for which  is responsible and stores actual data .  
Additionally for each interval a reference  to the supervising node is stored
\end{itemize}


Additionally each node stores the following variables :
\begin{itemize}\itemsep0.1pt

\item : the timer predicate that is periodically true

\item : the interval between  and the successor of .  is supervising .

\item : the  message in  that now received by the node.

\end{itemize}

\begin{definition}\label{def:valid}
We define a valid state as an assignment of values to the internal variables of all nodes so that the definition of the variables is not violated, e.g.  contains no nodes  with  or  or  and  for any .
\end{definition}

Now we can describe the topologies in the initial states and in the legal stable state. Let  and let , such that for  the following conditions hold:

\begin{itemize}
  \item 
  \item  is in a valid state
  \item 
\end{itemize}

Note that we assume  to be a multiset, i.e in  an edge  might still exists, although  if e.g. .
Further note that, in case the network has stabilized to a \emph{CONE}-network, it holds for every node that  and .


\subsection{Algorithm}\label{app:alg}
In this section we give a description of the the distributed algorithm. The
algorithm is a protocol that each node executes based on its own node and
channel state. The protocol contains periodic actions that are executed if
the timer predicate  is true and actions that are executed if the node
receives a message .
In the periodic actions each node performs a consistency check of its
internal variables, i.e. are all variables valid according to Definition
~\ref{def:valid}. If some variables are invalid, the nodes causing this
invalidity are delegated. By \emph{delegation} we mean that node 
delegates a node  (resp. ) to the node  (resp. ) by a message
 to . The idea behind the delegation is to forward nodes closer to their correct position, so that the sorted list (and the \emph{CONE}-network) is formed.
In order for a node  to maintain valid lists (), it makes a periodic check of its lists with its neighbors in , where the lists are compared, so that inconsistencies
are repaired. Moreover a node checks whether  are valid and introduces to them their closest larger right/left neighbors (from 's perspective).
Unnecessary (for these lists) nodes are delegated.We show later in the analysis section that this process leads to the construction of the correct lists by each node and thus to the \emph{CONE}-network. 
Furthermore in the periodic actions each node introduces itself to its successor and predecessor  and  by a message . Also each pair of nodes in  and  with consecutive ids is introduced to each other.  also introduces the nodes  and  to each other by messages of type . By this a \emph{triangulation} is formed by edges
 (see Figure ~\ref{example3}).
To establish correct  and  lists in each node, a node  sends its
 (resp. ) list periodically to all nodes  in  (resp.
) by a message  (resp.
) to .
The last action a node periodically executes is to send a message to each
reference in  to check whether  is responsible for the data in the
corresponding interval  by sending a message
.

If the message predicate is true and  receives a message , the
action  performs depends on the type of the message. If  receives a
message   checks whether  has to be included in
it's internal variables , ,  or . If  doesn't
store ,  is delegated. If  receives a message
,  checks whether the ids in  has to be
included in it's internal variables , ,  or . If  doesn't store a node  in ,  is delegated. If  stores a node
 in  (resp. ) that is not in ,  is also delegated as
it also has to be in the list of  (resp. ). The remaining
messages are necessary for the data management.

If  receives a message  it checks whether 
is in  or  or has to be included, or delegates . Then 
checks whether  is in  and if  is responsible for . If
not,  sends a message  to  containing
a set of intervals in  that  is not responsible for and references
of the supervising nodes.
If  receives a message
 it forwards all data in intervals in
 to the corresponding references by a message
. If  receives such a message it checks whether
the data is in its supervised interval . If not  forwards the data
according to a greedy routing strategy, if  supervises the data it sends
a message  to the responsible node. If  receives
such a message it inserts the data, the interval and the corresponding
reference in . Note that no identifiers are ever deleted, but always
stored or delegated. This ensures the connectivity of the network.

In the following we give a description of the protocol executed by each node
in pseudo code.

\subsection{Pseudo code}



\begin{algorithm}[H]
Periodic actions including a consistency check, where all list  are checked and invalid nodes are delegated like in the ListUpdate/BuildTriangle operation. Furthermore each node sends its lists  to the  next smaller nodes and  to its direct left neighbor. Finally in the BuildTriangle()  introduces itself to its neighbors and neighbored nodes in  and  and  and  to each other and checks whether the information in  is still up to date.

\begin{algorithmic}
\State  \Comment{periodic actions}
\State Consistency check for , , , 
\ForAll{}
\State send m=(list-update,) to 
\EndFor
\ForAll{}
\State send m=(list-update,) to 
\EndFor
\State BuildTriangle()
\State CheckDataIntervals()
\end{algorithmic}
\end{algorithm}



\begin{algorithm}
\begin{algorithmic}

\Function{BuildTriangle}{node x}
\If{} \Comment{periodic introduction of nodes and  itself}
\ForAll{}
\State 
\State send m=(build-triangle,w) to  and  m'=(build-triangle,) to 
\EndFor
\ForAll{}
\State 
\State send m=(build-triangle,) to  and  m'=(build-triangle,) to 
\EndFor
\ForAll{}
\State send m=(build-triangle,) to 
\EndFor
\State send m=(build-triangle,) to  and  m'=(build-triangle,) to 
\Else \Comment{demand action by a received node id}
\If{}
\State send m=(buildtriangle,) to 
\State 
\ElsIf{}
\State send m=(buildtriangle,) to 
\State 
\ElsIf {}
\State calculate  out of  and 
\ForAll{}
\State 
\State send m=(build-triangle,) to 
\EndFor
\State 
\ElsIf {}
\State calculate  out of  and 
\ForAll{}
\State 
\State send m=(build-triangle,) to 
\EndFor
\State 
\EndIf
\EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}

\begin{algorithmic}
\Function{ListUpdate}{List}
\State  \Comment{candidates for }
\State  \Comment{candidates for }
\State  \Comment{candidates for }
\State  \Comment{candidates for }
\State calculate  out of  and  \Comment{calculate new lists and delegate all nodes not stored in the new lists}
\State 
\If{}
\ForAll{}
\State send m=(build-triangle,z) to 
\EndFor
\State 
\EndIf
\State calculate  out of  and 
\State 
\If {}
\ForAll{}
\State send m=(build-triangle,) to 
\EndFor
\State 
\EndIf
\State calculate  out of  and 
\ForAll{}
\State 
\State send m=(build-triangle,) to 
\EndFor
\State 
\State calculate  out of  and 
\ForAll{}
\State 
\State send m=(build-triangle,) to 
\EndFor
\State 
\EndFunction
\end{algorithmic}
\end{algorithm}




\begin{algorithm}
A node checks for each interval it is responsible for, if this is really the case.

\begin{algorithmic}
\Function{CheckDataIntervals}{}
\ForAll{u.DS[i]}
\State send m=(check-interval,,) to 
\EndFor
\EndFunction
\end{algorithmic}
\end{algorithm}


\begin{algorithm}
A node receiving a check-interval message, checks if the node which sent it is really responsible for the interval [a,b].

\begin{algorithmic}
\Function{CheckInterval}{[a,b],x}
\If{}
\State BuildTriangle(x)
\EndIf
\State IntervalSet := 
\State i:=1
\If{}
\State IntervalSet[i]= \Comment{The interval begins left of , so  can't be the supervising node for the whole interval}
\State IntervalSet[i].ref=
\State i:=i+1
\EndIf
\If{}
\State IntervalSet[i]= \Comment{The interval ends right of , so  can't be the supervising node for the whole interval}
\State IntervalSet[i].ref=
\State i:=i+1
\EndIf
\State           \Comment{ is  the subinterval of  for which  is responsible for}
\State [e,f]:=
\If{}
\State IntervalSet[i]= \Comment{ as the supervising node, knows other nodes responsible for parts of the interval}
\State IntervalSet[i].ref=u
\State i:=i+1
\EndIf
\If{}
\State IntervalSet[i]= \Comment{ as the supervising node, knows other nodes responsible for parts of the interval}
\State IntervalSet[i].ref=u
\EndIf
\State send m=(update-interval,IntervalSet) to 
\EndFunction
\end{algorithmic}
\end{algorithm}


\begin{algorithm}
By receiving an update-interval message, a node updates the lists of intervals which it is responsible for, and forwards the data in the
deleted intervals to another node, who is possibly responsible.

\begin{algorithmic}
\Function{UpdateInterval}{IntervalSet}
\ForAll {}
\ForAll{}
\ForAll {}
\State l:=
\State u.DS[l+1]=[e,f]
\State u.DS[l+1].ref=[c,d].ref \Comment{references are set to the new supervising node}
\State u.DS:=
\EndFor
\ForAll {}
\State send m=forward-data(data) to   \Comment{data  seems not to be responsible for or for that the reference changed is deleted}
\State delete(data)
\State BuildTriangle([a,b].ref) \Comment{references supervising nodes are forwarded to maintain connectivity}
\EndFor
\EndFor
\EndFor
\State UpdateDS() \Comment{Delete all intervals without data, forward the references of the deleted intervals, unite all consecutive intervals with the same reference}
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
By receiving a forward-data message, a node checks if it knows which node is responsible for the data it received, and sends a store-data message to it, in the other case it also forwards the data.

\begin{algorithmic}
\Function{ForwardData}{data}
\If {}
\If {}
\State send m=(forward-data(data)) to 
\Else
\State send m=(forward-data(data) to
\State 
\EndIf
\Else
\State send m=(store-data,data,,u) to 
\EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}
\begin{algorithm}
Storing the data received from the node supervising the corresponding interval.

\begin{algorithmic}
\Function{StoreData}{data,interval,x}
\If{}
\State u.DS[i].data := 
\State BuildTriangle(u.DS[i].ref)
\State u.DS[i].ref=x
\Else
\State l:=
\State u.DS[l+1]=interval
\State u.DS[l+1].ref=x
\State u.DS[l+1].data=data
\EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}


\begin{algorithm}
\begin{algorithmic}
\State  \Comment{demand actions depending on the received message}
\If{m=(list-update,List)}
\State ListUpdate(List)
\ElsIf{m=(build-triangle,x)}
\State BuildTriangle(x)
\ElsIf{m=(check-interval,[a,b],x)}
\State CheckInterval([a,b],x)
\ElsIf{m=(update-interval,IntervalSet)}
\State UpdateInterval(IntervalSet)
\ElsIf{m=(forward-data,data,boolean)}
\State ForwardData(data,boolean)
\ElsIf{m=(store-data,data,interval,x)}
\State StoreData(data,interval,x)
\EndIf

\end{algorithmic}
\end{algorithm}
\section{Correctness}
In this section we show the correctness of the presented algorithm. We do this by showing that by executing our algorithm any weakly connected network eventually converges to a CONE network and once a CONE network is formed it is maintained in every later state. We further show that in a CONE network the data is stored correctly.

\subsection{Convergence}
To show convergence we will divide the process of convergence into several phases, such that once one phase is completed its conditions will hold in every later program state. For our analysis we additionally define  as the set of edges at time . Analogous  and  are defined.
We show the following theorem. 

\begin{theorem}\label{theo:convergence}
If  at time  then eventually at a time  .
\end{theorem}

We divide the proof into 3 phases. First we show the preservation of the connectivity of the graph, then we show the convergence to the sorted list and eventually the convergence to the \emph{CONE}-network.

\subsubsection{Phase 1: Connectivity}
In the first phase we will show that the protocol keeps the network weakly connected and eventually forms a network that is connected by edges  such that  and edges  such that .

\begin{lemma}\label{lem:GConnectivity}
Any graph which is weakly connected due to edges in   stays weakly connected according to the given protocol, i.e if  is weakly connected then ,  is also weakly connected.
\end{lemma}

\begin{proof}
We show that for each existing edge  either the edge remains and  or a path connecting  exists. Obviously  and  stay weakly connected as long as an edge  exists. We therefore assume  and . If  is stored in an internal variable of  then there can be the following cases:

\begin{itemize}
  \item  at time , then  is delegated to  (resp. ) and  and  stay connected over the edges  and .
  \item  at time , then  is delegated to  (resp. ) and  and  stay connected.
  \item  at time  then x has received an interval-update message with a new reference for the data in  or the data is deleted. Then in both cases  delegates  to  (resp. ) and  and  stay connected.
\end{itemize}

If  is stored in an incoming message  in . When  is received then there can be the following cases:

\begin{itemize}
  \item  is in a list in a list-update message. Then either  is stored in a new list  or  is delegated to  (resp. ) and  and  stay connected.
  \item  is the node sending a check-interval message. Then either  is stored in  or delegated to  (resp. ) and  and  stay connected.
  \item  is a reference in an interval-update message, then either  is stored as a new reference for some data or if there is no corresponding data  is delegated to  (resp. ) and  and  stay connected.
  \item  is a reference in an store-data message, then  is stored as a new reference in .
  \item  is the id in a build-triangle message, then  is either stored in one of the lists  or  is delegated to  (resp. ) and  and  stay connected.
\end{itemize}
\end{proof}


\begin{definition}\label{def:TriangulationGraph}
Let  and   \newline  we then define the graph  as the \emph{triangulation graph}.
\end{definition}

\begin{lemma} \label{lem:TriConnectivity}
If  and  are connected in  at time  then they will be weakly connected at every time .
\end{lemma}

\begin{proof}
Again we will consider every possible edge  in  and show that  and  stay weakly connected.

If  then there can be the following cases:

\begin{itemize}
  \item  at time , then either  is delegated to  (resp. ) by a build-triangle message send to  (resp. ) and  and  are connected in  by  and , or  is stored in  and .
  \item  at time , then either  is delegated to  (resp. ) or  is also stored in  (resp. ) at time . By the same arguments as above  and  are connected in .
\end{itemize}

If  then . If  processes , then either  is stored in  and  or  is delegated to  (resp. ) and  and  are connected in .
\end{proof}

\begin{lemma}\label{lem:TriConnectivity2}
If  is weakly connected then eventually  will be weakly connected.
\end{lemma}

\begin{proof}
Again we consider every edge  in  and show that eventually  and  will be connected in . Note that we already showed in Lemma \ref{lem:TriConnectivity}, that nodes connected in  stay connected in . Therefore we only have to consider those edges in .

If  there can be the following case:

 at time  and  has received an interval-update message with a new reference for the data in  or the data is deleted. Then in both cases  delegates  to  (resp. ) and  and  are connected in . If  does not delegate , then  eventually sends an check-interval message to . Then either  or  delegates  and  and  are weakly connected in . If  is stored in an incoming message in  then there can be the following cases:

\begin{itemize}
  \item  is in a list in a list-update message. Then either  is stored in a new list  or  is delegated to  (resp. ) and  and  are weakly connected in .
  \item  is a reference in an interval-update message, then either  is stored as a new reference for some data or if there is no corresponding data  is delegated to  (resp. ) and  and  eventually are weakly connected in .
  \item  is a reference in an store-data message, then  is stored as a new reference in . And as already shown  and  are eventually weakly connected.
\end{itemize}
\end{proof}


Combining the Lemmas ~\ref{lem:GConnectivity}, ~\ref{lem:TriConnectivity} and ~\ref{lem:TriConnectivity2} leads to the following theorem:
\begin{theorem}\label{theo:phase1}
If  is weakly connected at time , then for some time   will be weakly connected at every time .
\end{theorem}

\subsubsection{Phase 2: Linearization}
For the rest of the analysis we assume that all variables of each node are valid according to definition ~\ref{def:valid}, i.e. we assume that each node has performed one consistency check. In this phase we show that eventually all nodes form a sorted list. We therefore define another subtopology

 with  and .

In the end  with  shall be formed.

\begin{theorem}\label{theo:phase2}
If  is weakly connected eventually  will be strongly connected and .
\end{theorem}
 
Before we can show the theorem we show some helping lemmas.


\begin{lemma}\label{lem:RConnectivity}
Eventually all nodes  and  (resp.  and ) with  will be connected and stay connected in every state after over nodes .
\end{lemma}

\begin{proof}
In the periodic action  executes , in which  introduces every pair of nodes  to each other. The connecting path only changes if w.l.o.g.  delegates , but then  can delegate  only to a node  with . By using this argument inductively  and  stay connected in every state after over nodes .
\end{proof}

\begin{lemma}\label{lem:Forward}
If  and  and  (resp. ) then eventually  with  (resp. ) and  and  are connected over nodes .
\end{lemma}


\begin{proof}
If  delegates  to a node  then obviously  and  and  are connected over .
In case  is not delegated  is stored in  or  (resp.  or ) or in a message . If  is stored in  and   and  then either  or . Eventually  is processed by  and either  is delegated, then there is another node  or  is stored in . Thus eventually  and another node  such that . From all such nodes  such that   introduces  to  and  with  and by the same arguments as above  and  stay connected over nodes .
If  and  the same analysis as for  can be applied. If  and  eventually  will receive the  list of . If  then  with  and by the same arguments as above  and  stay connected over nodes . Otherwise  sends a message  to  and again  with  and by the same arguments as above  and  stay connected over nodes .
If , then eventually  processes  and either stores  in  or  and we can apply one of the cases above or  is delegated.
\end{proof}

\begin{lemma}\label{lem:Mirror}
If  with  then eventually  with  and  and  are connected over nodes .
\end{lemma}

\begin{proof}
If  and  and  we can apply Lemma ~\ref{lem:Forward} and eventually  with  (resp. ) and  and  are connected over nodes   in every state after. Now if  with  we might again apply the lemma. Obviously we only can apply Lemma ~\ref{lem:Forward} a finite number of times until there is a node  such that  and there is no  with  and  and  are connected over nodes . Then either  or . If  then eventually  will introduce itself to  by a message , then  with  and  and  are connected over nodes  .
Otherwise as soon as  processes   is set to  and the same arguments as in the first case hold.
\end{proof}

Before we prove the theorem we introduce some additional definitions.

\begin{definition}
In the directed graph we define an \emph{undirected path}  as a sequence of edges  \newline ), such that . Let  and  then the \emph{range of a path}  is given by .
\end{definition}

Now we are ready to prove Theorem ~\ref{theo:phase2}.

\begin{proof}
Let  and  be a pair of nodes connected in ; i.e. w.o.l.g.  and . Then as  is weakly connected there is an undirected path connecting  and . Let  be such a path at time . We show that there is a path  with  that connects  and  weakly such that . Let  and  be the smallest and greatest node on the path  that limit the range of . Then  is connected to nodes  and . If  and . Then according to Lemma ~\ref{lem:Mirror} eventually  and  and  are connected over nodes  such that . The same holds for . Thus eventually  and  and  and  are connected over nodes  with  and  and  are connected over nodes . Then either  and we can construct another path connecting  and  over  and  with , and , otherwise  or . W.l.o.g. we assume . Then according to Lemma ~\ref{lem:Forward} eventually  and  and  are connected over nodes . Either  or also according to Lemma ~\ref{lem:Forward}   and  and  are connected over nodes . Note that according to the proof of Lemma ~\ref{lem:Forward}  and  have to be in  at the time the edge  resp.  is created. Then according to Lemma ~\ref{lem:RConnectivity}  and   are also connected to . Thus again we can construct another path connecting  and  over  and  with . The same arguments can be used symmetrically to show that  can be decreased. Thus eventually a connecting path can be found with a strict smaller range and by applying these arguments a finite number of times  and . Then if   otherwise  will eventually be processed and  is set to . By the same arguments eventually . As this holds for every pair ,  in , eventually .
\end{proof}

\subsubsection{Phase 3: From the sorted list to the CONE-network}

In this section we show that once the network has stabilized into a sorted list, it eventually also stabilizes into the legal cone-network, that means,
each node  maintains a correct set of neighbors, so the lists  maintain the correct nodes, so for example the list  maintains
the nodes in . For all the following lemmas and theorems in this section we assume .

We will first do the proof for the sets  and .
The following lemma will be helpful.

\begin{lemma}\label{induct1}
If every node at the right (with larger id) of a node  knows its correct closest larger right node  (stored in ), then for all nodes  which are in the correct right internal neighborhood  of , , it holds that  will eventually learn  (and store it in ).
\end{lemma}

\begin{proof}
We will prove it by induction over  (in ascending order of their  values).

\textbf{Induction basis:}  is the direct right neighbor of .  In that case  already knows , since we assumed the presence of the sorted list, and the statement holds.

\textbf{Inductive step:} If  knows the next node to the left of  (let this be ) which is in the right internal neighborhood of , then  eventually learns .
In this case,  is the closest larger right node of . That is because  must be larger (in terms of capacity) than , since else  would not
be in . By hypothesis,  knows about  (so ). So, when  conducts its periodic  call, it will introduce  and  to each other (as they are  and  respectively)
and  will learn about .
\end{proof}


Now we show that eventually all nodes learn their correct right larger-node lists.


\begin{lemma}\label{S^+}
Once the list has been established and , then eventually every node  learns its correct right larger-node list  (and stores it in ).
\end{lemma}

\begin{proof}
We will prove this by induction over the nodes  (in descending order of their  values).


\textbf{Induction basis:}  does not have any closest larger right node . The statement is obliviously true.


\textbf{Inductive step:} If every node at the right of  in the list, knows its correct right larger-node list, then eventually  will learn its correct right larger-node list .

From the induction hypothesis, every node at right of  knows its correct right larger-node list (so also its correct closest larger right node), so according to Lemma ~\ref{induct1},  will eventually learn its correct right internal neighborhood (and store it in ). Let  the node being the most right one in . It is obvious that the  closest larger right node of , , is also the closest larger right node of , since otherwise  would also be in . By the inductive hypothesis,  knows this node, and will introduce it to  by its periodic  call. So, once  learns  (and as a consequence  learns  after 's periodic  call,  will also send to  its right larger-node list (), through its periodic  message, which (together with ) is the correct right larger-node list of .
\end{proof}


\begin{lemma}\label{S^-}
If , then eventually every node  learns its correct right internal neighborhood  (and stores it in ).
\end{lemma}

\begin{proof}
By Lemma ~\ref{S^+}, there is a point where every node knows its right larger-node list  .
This is the hypothesis of Lemma ~\ref{induct1} for all nodes , so by using this lemma for every node  we derive the proof.
\end{proof}

\begin{theorem}\label{theo:phase3}
If  then eventually, every node  learns its correct internal neighborhood  , as well as its correct larger-node lists .
\end{theorem}

\begin{proof}
We already showed that for the right part of the neighborhood (for  and ) by lemmas ~\ref{S^+} and ~\ref{S^-}.
By symmetry (i.e. by using symmetric proofs for the left part) it also holds for  and .
\end{proof}



Combining Theorem ~\ref{theo:phase1}, Theorem ~\ref{theo:phase3} and Theorem ~\ref{theo:phase3} we can show that Theorem ~\ref{theo:convergence} holds, and by our protocol each weakly connected network converges to a CONE network.

\subsubsection{Closure and Correctness of the data structure}

We showed that from any initial state we eventually reach a state in which the network forms a correct CONE network. We now need to show that in this state the explicit edges remain stable and also that each node stores the data it is responsible for.

\begin{theorem}\label{theo:closure}
If  at time  then for  also .
\end{theorem}

\begin{proof}
The graph  only changes if the explicit edge set is changed. So if we assume that  at time  and for  also  then we added or deleted at least one explicit edge. Let  at time . W.l.o.g. we assume . Either  or . In both cases the edge is only deleted if  knows a node  with  and  as following from Theorem ~\ref{theo:phase3} all internal neighborhoods are correct in  there can not be such a node . By the same argument also no new edges are created. Thus  at time .
\end{proof}

So far we have shown that by our protocol eventually a CONE network is formed. It remains to show that also by our protocol eventually each node stored the data it is responsible for.

\begin{theorem}\label{theo:datastructure}
If  eventually each node stores exactly the data it is responsible for.
\end{theorem}

\begin{proof}
According to Theorem ~\ref{theo:responsibility} each node knows which node is responsible for parts of the interval it supervises. In our described algorithm each node  checks whether it is responsible for the data it currently stores by sending a message to the node  that  assumes to be supervising the corresponding interval. If  is supervising the interval and  is responsible for the data, then  simply keeps the data. If  is not supervising the data or  is not responsible for the data then  sends a reference to  with the id of anode that  assumes to be supervising the interval. Then  forwards the data to the new reference and does not store the data. By forwarding the data by Greedy Routing it eventually reaches a node supervising the corresponding interval, this node then tells the responsible node to store the data. Thus eventually all data is stored by nodes that are responsible for the data.
\end{proof}



\section{External Dynamics}\label{ops}

Concerning the network operations in the network, i.e. the joining of a new node, the leaving of a node and the capacity change of a node, we show the following:

\begin{theorem}\label{theo:dynam}
In case a node  joins a stable CONE network, or a node  leaves a stable CONE network 
or a node  in a stable CONE network changes its capacity, we show that in any of these three cases  structural changes in the explicit edge set are necessary to reach the new stable state.
\end{theorem}

We show the statement by considering the 3 cases separately.

\subsection{Joining of a node}

When a new node  enters the network, it does so by maintaining a connection to another node , which is already in the network.  is forwarded due to the  and  rules in the network until it reaches its right position, as it takes part in the linearization procedure.


\begin{theorem}
If a node  joins a stable CONE network  structural changes in the explicit edge set are necessary to reach the new stable state.
\end{theorem}

\begin{proof}
We show that there is at most a constant number of temporary edges, i.e. edges that are not in the stable state.
 stores  in its internal variables  or  as  is the only node  knows. In the periodic BuildTriangle()  sends a message to  containing its own id creating an implicit edge . Now there can be two cases: Either  is in ' lists , , ,  in a stable state then  stores 's identifier or  is not stored and delegated to another node  creating the implicit edge . Thus only explicit edges pointing to  are created that are in the stable state and only the explicit edge  is temporary. So far we have shown that according to Theorem ~\ref{theo:phase3} and ~\ref{theo:degree} at most  edges are created w.h.p. that point to  or start at . But by the join of  to the network also edges that have been in the stable state not longer exist in the new stable state. E.g. let  and  and  then  is not longer stored in  as soon as  is integrated in the network. According to ~\ref{theo:degree} there is at most  w.h.p. such nodes , as each node  has to store  in its lists, and also at most  w.h.p. nodes , as  has to store each  in its lists. Therefore there are at most  edges that have to be deleted.
\end{proof}



\subsection{Leaving of a node}

Once a node decides it wants to leave the network, it disconnects itself from its neighbors. In the case it is the node with the greatest capacity, it introduces its two direct neighbors to each other before doing so. In that way, connectivity is still guaranteed (at least in the stable state).

\begin{algorithm}
\begin{algorithmic}
\Function{Leave}{}
\ForAll{}
\State  send m=forward-data(data,false) to 
\State delegate the references in 
\EndFor
\If {}
\State send m=(buildtriangle, ) to 
\EndIf
\State delete all connections, leave network
\EndFunction
\end{algorithmic}
\end{algorithm}

After the leaving, the network must stabilize again.
This means that  and  must connect to each other. Lets consider . Since it won't have a direct right neighbor after the leaving of , the linearization process will take place again until  learns .

\begin{theorem}
If a node  leaves a stable CONE network  structural changes in the explicit edge set are necessary to reach the new stable state.
\end{theorem}

\begin{proof}
The proof is analogous to the proof in the case of a joining node. Obviously according to ~\ref{theo:degree} w.h.p.  edges are deleted that start at or point to the leaving node . By deleting  further edges have to be created. E.g. let  and  and  then  might now be stored in  or  and the edge  has to be created. Again according to ~\ref{theo:degree} there are w.h.p. at most  such nodes  and   such nodes , thus in total at most  edges have to be created.
\end{proof}





\subsection{Capacity Change}
If the capacity of a single node  in a stable CONE network decreases we can apply the same arguments as for the leaving of a node, as some nodes might now be responsible for intervals that  was responsible for. Additionally  might have to delete some ids in  and add ids in . If a node increases its capacity we can apply the same arguments as for the joining of a node, as some nodes might not longer be responsible for intervals that  is now responsible for. Additionally  might have to add some ids in  and delete ids in . Thus the following theorem follows.

\begin{theorem}
If a node  in stable CONE network changes its capacity  structural changes in the explicit edge set are necessary to reach the new stable state.
\end{theorem}




\section{Conclusion and Future Work}

We studied the problem of a self-stabilizing and heterogeneous overlay network and gave an algorithm of solving that problem, and by doing this we used an efficient network structure. We proved the correctness of our protocol, also concerning the functionality of the operations done in the network, data operations and node operations. This is the first attempt to present a self-stabilizing method for a heterogeneous overlay network and it works efficiently regarding the information stored in the hosts. Furthermore our solution provides a low degree, fair load balancing and polylogarithmic updates cost in case of joining or leaving nodes.
In the future we will try to also examine heterogeneous networks in the two-dimensional space and consider heterogeneity in other aspects than only the capacity, e.g. bandwidth, reliability or heterogeneity of the data elements. 






\newpage\begin{thebibliography}{10}
\bibliographystyle{plain}


\bibitem{D74:self-stab}
Edsger~W. Dijkstra.
\newblock {Self-stabilizing systems in spite of distributed control.}
\newblock {\em Commun. ACM}, 17:643-644, November 1974.

\bibitem{cone}
C. Schindelhauer, G. Schomaker.
\newblock Weighted distributed hash tables.
\newblock In {\em SPAA '05},Pages 218 - 227 , 2005.

\bibitem{Giakkoupis}
G. Giakkoupis, V. Hadzilacos.
\newblock A Self-Stabilization Process for Small-World Networks.
\newblock In {\em PODC '05}, 2005

\bibitem{brink}
A. Brinkmann, K. Salzwedel, and C. Scheideler.
\newblock Compact, adaptive placement schemes for
non-uniform distribution requirements.
\newblock In {\em SPAA '02}, Pages 53-62, 2002.

\bibitem{brink2}
A. Brinkmann, K. Salzwedel, and C. Scheideler.
\newblock Efficient, distributed data placement strategies for
storage area networks.
\newblock In {\em SPAA '00}, pages 119-128, 2000.

\bibitem{conhash}
D. Karger, E. Lehman, T. Leighton, M. Levine, D. Lewin, and R. Panigrahy.
\newblock Consistent hashing and
random trees: Distributed caching protocols for
relieving hot spots on the World Wide Web.
\newblock In {\em STOC '97}, Pages 654-663 , 1997.

\bibitem{RF+01:can}
S. Ratnasamy, P. Francis, M. Handley, R. Karp, and S. Shenker.
\newblock A scalable content-addressable network.
\newblock In {\em SIGCOMM}, pages 161--172, 2001.

\bibitem{RD01:pastry}
A. I.~T. Rowstron and P. Druschel.
\newblock Pastry: Scalable, decentralized object location, and routing for
  large-scale peer-to-peer systems.
\newblock In {\em Middleware}, pages 329--350, 2001


\bibitem{SM+01:chord}
I. Stoica, R. Morris, D. Karger, M. Frans Kaashoek and H. Balakrishnan.
\newblock Chord: A scalable peer-to-peer lookup service for internet
  applications.
\newblock In {\em SIGCOMM}, pages 149--160, 2001.



\bibitem{CF05:stab-ring}
C. Cramer and T. Fuhrmann.
\newblock Self-stabilizing ring networks on connected graphs.
\newblock In {\em Technical report}, University of Karlsruhe (TH),  Fakultaet fuer Informatik, 2005-5.


\bibitem{DK08}
S. Dolev and R. I. Kat.
\newblock HyperTree for self-stabilizing peer-to-peer systems.
\newblock In {\em Distributed Computing}, 20(5), pages 375--388, 2008.


\bibitem{DT09}
S. Dolev and N. Tzachar.
\newblock Empire of colonies: Self-stabilizing and self-organizing distributed
  algorithm.
\newblock {\em Theor. Comput. Sci.}, 410(6-7):514--532, 2009.

\bibitem{DT10}
S. Dolev and N. Tzachar.
\newblock Spanders: distributed spanning expanders.
\newblock In {\em SAC '10}, pages 1309--1314, 2010.

\bibitem{JR+09:skip+}
R. Jacob, A.~W. Richa, C. Scheideler, S. Schmid, and H. T{\"a}ubig.
\newblock A distributed polylogarithmic time algorithm for self-stabilizing
  skip graphs.
\newblock In {\em PODC '09}, pages 131--140, 2009.

\bibitem{JRS+09:delaunay}
R. Jacob, S. Ritscher, C. Scheideler, and S. Schmid.
\newblock A self-stabilizing and local delaunay graph construction.
\newblock In {\em ISAAC '09}, pages 771--780, 2009.

\bibitem{KKS11}
S. Kniesburges, A. Koutsopoulos, and C. Scheideler.
\newblock Re-chord: a self-stabilizing chord overlay network.
\newblock In {\em SPAA '11}, pages 235--244, 2011.


\bibitem{NNS11:corona}
R. Nor, M. Nesterenko, and C. Scheideler.
\newblock Corona: A stabilizing deterministic message-passing skip list.
\newblock In {\em SSS '11}, pages 356--370, 2011.

\bibitem{ORS07:lin}
M. Onus, A.~W. Richa, and C. Scheideler.
\newblock Linearization: Locally self-stabilizing sorting in graphs.
\newblock In {\em ALENEX '07} pages 99--108, 2007.




\bibitem{SR05:ring}
A. Shaker and D.~S. Reeves.
\newblock Self-stabilizing structured ring topology p2p systems.
\newblock In {\em P2P '05}, pages 39--46, 2005.

\bibitem{smallworld}
S. Kniesburges, A. Koutsopoulos, C. Scheideler.
\newblock A Self-Stabilization Process for Small-World Networks.
\newblock In {\em IPDPS '12}, Pages 1261-1271, 2012.

\bibitem{NegBi}
N. Harvey.
\newblock CPSC 536N: Randomized Algorithms, Lecture 3.
\newblock In {\em University of British Columbia}, Pages 5, 2011-12.



\bibitem{supernode}
H. Shena, Cheng-Zhong Xub.
\newblock Hash-based proximity clustering for efficient load balancing in heterogeneous DHT networks.
\newblock In {\em J. Parallel Distrib. Comput. 68},  686-702, 2008.

\bibitem{rao}
A. Rao, K. Lakshminarayanan, S. Surana, R. Karp and I. Stoica.
\newblock Load balancing in structured P2P systems.
\newblock In {\em IPTPS 03},  2003.





\bibitem{godfrey}
P. B. Godfrey and I. Stoica.
\newblock Heterogeneity and Load Balance in Distributed Hash Tables.
\newblock In {\em IEEE INFOCOM},  2005.

\bibitem{skewccc+}
M. Bienkowski, A. Brinkmann, M. Klonowski and M. Korzeniowski, Miroslaw.
\newblock SkewCCC+: a heterogeneous distributed hash table.
\newblock In {\em OPODIS'10}, pages 219-234, 2010.



\bibitem{rio}
J.~R. Santos and R. Muntz.
\newblock Performance Analysis of the RIO Multimedia Storage System with Heterogeneous Disk Configurations.
\newblock In {\em ACM Multimedia Conference}, pages 303-308, 1998.

\bibitem{slicing}
A. Miranda, S. Effert, Y. Kang, E.~L. Miller, A. Brinkmann and T. Cortes.
\newblock Reliable and randomized data distribution strategies for large scale storage systems.
\newblock In {\em HiPC '11}, pages 1-10, 2011.

\bibitem{broadscale}
S.-Y. Didi Yao, C. Shahabi, and R. Zimmermann.
\newblock BroadScale: Efficient scaling of heterogeneous storage systems.
\newblock In {\em Int. J. on Digital Libraries, vol. 6}, pages 98-111, 2006.


\bibitem{redShare}
A. Brinkmann, S. Effert, F. Meyer auf der Heide and C. Scheideler.
\newblock Dynamic and Redundant Data Placement.
\newblock In {\em ICDCS '07}, pp.29, 2007.

\bibitem{diskarray}
T. Cortes and J. Labarta.
\newblock Taking advantage of heterogeneity in disk arrays.
\newblock In {\em J. Parallel Distrib. Comput. 63}, pages 448-464, 2003.

\bibitem{spread}
M. Mense and C. Scheideler.
\newblock SPREAD: An adaptive scheme for redundant and fair storage in dynamic heterogeneous storage systems
\newblock In {\em SODA '08}, 2008.

\bibitem{AS03:skip}
J. Aspnes and G. Shah.
\newblock Skip graphs.
\newblock In {\em SODA '03}, pages 384--393, 2003.

\bibitem{AS04:hyperring}
B. Awerbuch and C. Scheideler.
\newblock The hyperring: a low-congestion deterministic data structure for
  distributed environments.
\newblock In {\em SODA '04}, pages 318--327, 2004.

\bibitem{BK+04:Pagoda}
A. Bhargava, K. Kothapalli, C. Riley, C. Scheideler, and M.
  Thober.
\newblock Pagoda: A dynamic overlay network for routing, data management, and
  multicasting.
\newblock In {\em SPAA '04}, pages 170-179, 2004.

\bibitem{HJ+03:skipnet}
N. J.~A. Harvey, M.~B. Jones, S. Saroiu, M. Theimer, and
  A. Wolman.
\newblock Skipnet: a scalable overlay network with practical locality
  properties.
\newblock In {\em USITS'03}, pages 9-9, 2003. 
  
\bibitem{KSW05:self-repair}
F. Kuhn, S. Schmid, and R. Wattenhofer.
\newblock A self-repairing peer-to-peer system resilient to dynamic adversarial
  churn.
\newblock In {\em IPTPS '05}, pages 13--23, 2005.

\bibitem{MNR02:viceroy}
D. Malkhi, M. Naor, and D. Ratajczak.
\newblock Viceroy: a scalable and dynamic emulation of the butterfly.
\newblock In {\em PODC '02}, pages 183-192, 2002.
  
\bibitem{NW07:continous-discrete}
M. Naor and U. Wieder.
\newblock Novel architectures for p2p applications: The continuous-discrete
  approach.
\newblock {\em ACM Transactions on Algorithms}, 3(3), 2007.


\bibitem{Plaxton}
C. G. Plaxton, R. Rajaraman, and A.~W. Richa.
\newblock Accessing nearby copies of replicated objects in a distributed
environment.
\newblock In {\em SPAA '97}, pages 311-320, 1997.

\end{thebibliography}









\end{document}