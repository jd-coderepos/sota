\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{color}
\usepackage{booktabs}

\usepackage{float}
\usepackage{caption}







\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}



\captionsetup[figure]{name={Figure}, labelsep=period}
\captionsetup[table]{name={Table}, labelsep=period}

\cvprfinalcopy 

\def\cvprPaperID{3959} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ifcvprfinal\pagestyle{empty}\fi
\begin{document}
\pagenumbering{gobble} 

\title{Modulating Image Restoration with Continual Levels \\via Adaptive Feature Modification Layers}

\author{Jingwen He\thanks{The first two authors are co-first authors. (e-mail: jw.he@siat.ac.cn; chao.dong@siat.ac.cn).} \quad Chao Dong\footnotemark[1] \quad Yu Qiao\thanks{Corresponding author (e-mail: yu.qiao@siat.ac.cn).}\\
	ShenZhen Key Lab of Computer Vision and Pattern Recognition, \, SIAT-SenseTime Joint Lab, \\ 
	Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, China\\
	The Chinese University of Hong Kong\\
}


\maketitle


\begin{abstract}

\vspace{-1em}
In image restoration tasks, like denoising and super-resolution, continual modulation of restoration levels is of great importance for real-world applications, but has failed most of existing deep learning based image restoration methods. Learning from discrete and fixed restoration levels, deep models cannot be easily generalized to data of continuous and unseen levels. This topic is rarely touched in literature, due to the difficulty of modulating well-trained models with certain hyper-parameters. We make a step forward by proposing a unified CNN framework that consists of few additional parameters than a single-level model yet could handle arbitrary restoration levels between a start and an end level. The additional module, namely AdaFM layer, performs channel-wise feature modification, and can adapt a model to another restoration level with high accuracy. By simply tweaking an interpolation coefficient, the intermediate model -- AdaFM-Net could generate smooth and continuous restoration effects without artifacts. Extensive experiments on three image restoration tasks demonstrate the effectiveness of both model training and modulation testing. Besides, we carefully investigate the properties of AdaFM layers, providing a detailed guidance on the usage of the proposed method.
\end{abstract}


\section{Introduction}


Deep learning methods have achieved great success in image restoration tasks, such as denoising, super-resolution, compression artifacts reduction, etc \cite{lim2017enhanced, ledig2017photo, dong2014learning, Dong_2015_ICCV, zhang2017beyond}. However, there still exists a large gap of restoration performance between research environment mand real-world applications. In this work, we focus on two main issues that prevent CNN based restoration methods from wide usages. 

First, the degradation levels of real-world images are generally continuous, such as JPEG quality  and . On the other hand, the deep restoration models are usually trained with discrete and fix levels (e.g., , ). Applying models with mismatched restoration levels tends to produce either over-sharpening or over-smoothed images, as shown in Figure~\ref{fig:problem}\footnote{DeJPEG is also known as JPEG deblocking and compression artifacts reduction.}. A straightforward solution is to train a sufficiently large model to handle all degradation levels. However, regardless of the computational burden, this general model is not optimal for each individual level. When we want to slightly adjust the output effects, we have to retrain a new model by refining the model structure, parameters or (and) loss functions, which is a tedious procedure with unpredictable results.

\begin{figure}[]
\vspace{-0.5em}
\centering
\includegraphics[scale=0.21]{imgs/problem/problem.pdf}
\vspace{-0.5em}
\captionsetup{font={small}}
\caption{Applying \textit{q10} or \textit{q80} DeJPEG model on images (LIVE1 \cite{1709988}) with degradation \textit{q30} tends to produce either over-sharpening (left) or over-smoothed (right) images.}
\vspace{-2em}
\label{fig:problem}
\end{figure}

Second, in industrial and commercial scenarios (e.g., human-interactive softwares), it is often necessary to consecutively modulate the restoration strength/effect to meet different requirements. For example, the users always expect a tool bar to flexibly adjust the restoration level, as shown in Figure \ref{fig:solution}. However, current deep models are trained on fixed degradation levels, and contain no hyper-parameters for users to change the final results. 

To fill in the gaps, our goal is to achieve arbitrary-level image restoration and continual model modulation in a unified CNN framework. More formally, the task is to deal with images of degradation levels between a ``start'' level and an ``end'' level in a user controllable manner. To facilitate practical usages, we should avoid building a very large model or model zoo, and prevent another training stage at test time. In other words, the solution should contain a small amount of additional parameters and allow continual tuning of parameters in testing. 

\begin{figure}[]
	\centering
	\includegraphics[scale=0.21]{imgs/problem/modulation_intuition.pdf}
	\vspace{-0.5em}
	\captionsetup{font={small}}
	\caption{We can modulate the tool bar to obtain continual restoration effect in DeJPEG, Super Resolution and Denoising.}
	\vspace{-2em}
	\label{fig:solution}
\end{figure}


This task is non-trivial and rarely studied in literature. Perhaps the most relevant topic to modifying the network outputs is arbitrary style transfer. Specifically, we can treat different levels of degradation as different kinds of styles. A representative approach is the Conditional Instance Normalization (IN) \cite{dumoulin2017learned}, which allows users to mix up different styles by tuning IN parameters. Nevertheless, image restoration has higher and finer request on the output image quality. Directly applying Conditional IN in image restoration could produce obvious and large-scale artifacts in the output image (see Figure \ref{fig:baselines}). Another similar concept is domain adaptation, which generally appears in high-level vision problems (e.g., image classification and object detection). It adapts/transfers the model trained on the source domain to the target domain. However, domain adaptation cannot easily generalize to unseen data, thus is not appropriate to address our problem.


In this work, we present a simple yet effective approach that for the first time enables consecutive modulation of the restoration strength with little computation cost. This approach stems from the observation that filters among networks of different restoration levels are similar at patterns while varying on scales and variances. Furthermore, the model outputs could change continuously by modulating the statistics of features/filters. The proposed framework is built upon a novel Adaptive Feature Modification (AdaFM) layer that modifies the middle-layer features with depth-wise convolution filters. In practice, we first train a standard restoration CNN for the start level, and then insert AdaFM layers and optimize it to the end level. After the training stage, we fix the CNN parameters, and interpolate the filters of AdaFM layers according to testing restoration level. By tuning a controlling coefficient (ranging from 0 to 1), we can interactively and consecutively manipulate the restoration results/effects. Note that we only need to train the CNN and AdaFM layers once, and no further training is required in the test time. 


To ensure the output quality, we demonstrate that the model with AdaFM layers achieves comparable performance to the single-level image restoration network in both start and end level. Then, we show that the modulated-network outputs are noise-free with consecutive restoration effects (see Figure \ref{fig:solution}). Besides, we also examine the properties of AdaFM layers –- complexity, range and direction, providing a detailed instruction on the usage of the proposed method. Notably, the added AdaFM layers contribute to less than 4\% parameters of the CNN model yet achieves excellent modulation performance. 


\section{Related Work}


The proposed Adaptive Feature Modification (AdaFM) layers are inspired by the recent normalization methods in deep CNNs, thus we give a brief review of these works. Normalization has been demonstrated effective in facilitating training very deep neural networks. 
The most representative method is batch normalization (BN) \cite{Ioffe2015BatchNA} that is proposed to address the problem of Internal Covariate Shift in the training process. In particular, BN layer normalizes the output of each neuron using the mean and variance of each batch calculated during the feed-forward process. 
Later on, Dmitry Ulyanov et al. \cite{DBLP:journals/corr/UlyanovVL16} achieved significant improvement in style transfer by replacing all the BN layers with their proposed instance normalization (IN) layers. The core idea is to normalize the features based on the statistics across the spatial dimensions of each sample instead of each batch. 
Recently, several alternative normalization methods have been proposed, such as instance weight normalization \cite{salimans2016weight}, layer normalization \cite{Ba2016LayerN}, group normalization (GN) \cite{wu2018group} and etc.
The spatial feature transformation (SFT) layer proposed by Wang~et al. \cite{wang2018recovering} further extends the normalization operation to a more general spatial-variant transformation. Specifically, they apply a feature spatial-wise transformation on the feature maps according to the semantic segmentation priors. This approach indeed helps generate more realistic textures compared with those popular GAN-based methods. We will compare the proposed AdaFM layer with BN and SFT layers in Section~\ref{sec:AdaFM}.

Furthermore, recent works show that BN and IN have the ability to adapt the model to a different domain with little computation cost.
Specifically, Li et al. \cite{Li2017RevisitingBN} propose AdaBN (Adaptive Batch Normalization) to alleviate domain shifts, and show that AdaBN is effective for domain adaptation task by re-computing the statistics of all BN layers across the network. Huang et al. \cite{huang2017arbitrary} show that instance normalization (IN) can perform as style normalization by aligning the mean and variance of content features with those style features. In such way, they realize arbitrary style transfer at test time. Moreover, Dumoulin et al. \cite{dumoulin2017learned} extended IN to enable multiple style transfer by learning different sets of parameters in normalization layers while the convolution parameters are shared. Our method is different from these works in that 1) the proposed AdaFM layer is independent of either batch or instance samples, 2) the filter size and position of AdaFM layers are flexible, indicating that AdaFM is beyond a normalization operation, 3) the interpolation property of AdaFM layers could achieve continual modulation of restoration levels, which has not been revealed before.



\section{Method}

\subsection{Problem Formulation.}
The problem of consecutive modulation of restoration levels can be formulated as follows.
Suppose we have a ``start'' restoration level --  and an ``end'' restoration level -- , the objective is to construct a deep network to handle images with arbitrary degradation level  (). 
Our solution pipeline consists of two stages -- \textit{model training} and \textit{modulation testing}. In model training, we train a basic model and an adaptive model that could deal with level  and , respectively. While in modulation testing, we propose a new network that can realize arbitrary restoration effects between level  and  by modulating certain hyper-parameters. In the following sections, we first show two important observations that inspire our method. Then we propose the AdaFM layer and compare it with BN \cite{Ioffe2015BatchNA} and SFT \cite{wang2018recovering}. At last, we describe how to use AdaFM layers in model training and modulation testing.

\subsection{Observation}

\textbf{Observation 1.} We find that the learned filters of restoration models trained with different restoration levels are pretty similar at visual patterns, but their weights have different statistics (e.g., mean and variance). An example is shown in Figure \ref{filters_noise}, the filter  of level  is like a 2-D Gaussian filter, then the corresponding filter  finetuned from level  to level  will also look like a Gaussian filter but with different mean and variance. 
\begin{figure}[t]
\centering
\includegraphics[scale=0.32]{imgs/filters/filters.pdf}
\captionsetup{font={small}}
\vspace{-0.5em}
\captionsetup{font={small}}
\caption{Filter visualization.}
\vspace{-1.5em}
\label{filters_noise}
\end{figure}
We use the Gaussian Denoising problem for illustration. The start level is noise level , and the end level is . We adopt a simple and standard CNN structure ARCNN \cite{Dong_2015_ICCV} to do the experiments. We first learn the model with noise level  and obtain ARCNN-15, then finetune the network on  to obtain ARCNN-50. The first layer filters of these two models are visualized in Figure \ref{filters_noise}. In the first glance, these filters look similar with only slight differences. Their mean cosine distance between the corresponding filters is 0.12, indicating that they are very close to each other.
To further reveal their relationship, we use a filter to bridge the corresponding filters. Specifically, each filter  in ARCNN-15 is convoluted with another filter  to approximate the corresponding filter  in ARCNN-50. According to the commutative law, we have , where  is convolution. Thus for each feature map , the parameters of  are optimized with 

\vspace{-1em}

\vspace{-1em}

The above operation is equivalent to adding a depth-wise convolution layer after each layer of ARCNN-15, and finetuning the added parameters on the   problem. When  is of size , it is equal to a scaling and shift operation, changing the mean and variance of the original filter. We use the PSNR gap between their network outputs to show the fitting error. From Table \ref{table:kernel_size}, we can see that the value of fitting error decreases when the filter size of  increases. The gap is already very small at , which demonstrates our primal assumption. The  filters are also visualized in Figure \ref{filters_noise}, where one can see the differences between  and . Similar experiments for super resolution and compression artifacts reduction are presented in the supplementary file. 

\begin{figure*}[t]
\centering
\vspace{-1em}
\includegraphics[scale=0.75]{imgs/arch/arch_process1_v8.pdf}
\includegraphics[scale=0.3]{imgs/arch/modulation_process_v6.PNG}
\vspace{-1em}
\captionsetup{font={small}}
\caption{The left part presents the basic model and the AdaFM-Net. The right part shows how AdaFM works in the adaptation process and the modulation testing.}
\vspace{-1.5em}
\label{arch}
\end{figure*}

\textbf{Observation 2.} We find that the network output could change continuously by modulating the statistics of features/filters. As the filter  is gradually updated by gradient descent, what if we control the updating process by interpolating the intermediate results? 
Specifically, we can obtain the intermediate filter  by the following function:

\vspace{-0.5em}

where  is an interpolation coefficient. When we modulate  gradually from 0 to 1,  will also change continuously from  to . After putting  back to the network, we find that the network output will also change continuously in visualization, as shown in Figure~\ref{fig:solution}. Detailed analysis can be found in Section~\ref{sec:test} and~\ref{sec:exp}.


\subsection{Adaptive Feature Modification}
\label{sec:AdaFM}
Inspired by the above observations, we propose a continual modulation method by introducing an Adaptive Feature Modification layer and the corresponding modulating strategy. The overall framework is depicted in Figure \ref{arch}. 

Our aim is to add another layer to manipulate the statistics of the filters, so that they could be adapted to another restoration level. As indicated in Observation 1, we can add a depth-wise convolution layer (or a group convolution layer with the group number equal to the number of feature maps) after each convolution layer and before the activation function (e.g., ReLU). We name the added layer as the Adaptive Feature Modification layer, which is formulated as
\vspace{-0.5em}

where  is the input feature map and  is the number of feature maps.  and  are the corresponding filter and bias, respectively. It is worth noting that  depends on the degradation level of input images. To further understand its behaviour, we compare the proposed layer with batch normalization (BN) \cite{Ioffe2015BatchNA} and spatial feature transformation (SFT) \cite{wang2018recovering} layers.


\textbf{Comparison with BN layer.} When we set the filter size of  to , the feature modification reduces to a normalization operation. Note that BN \cite{Ioffe2015BatchNA} is also put directly after the convolution layer. We compare it with BN as
\vspace{-0.5em}

where  are the mean and standard deviation of an input batch,  are affine parameters. The  AdaFM filter performs similar to BN without using the batch information. 
As a special case, we can also use BN to do feature modification and finetune  as , . Experiments show that using BN achieves almost the same results as the  AdaFM filter.

\textbf{Comparison with SFT layer.} When the filter size of  is as large as the feature map, it will perform spatial feature transform as SFT layer \cite{wang2018recovering}. The formulations are shown as 
\vspace{-0.5em}

where  are affine parameters. AdaFM and SFT layer share the same function, but different on the parameters. Specifically,  are calculated from another sub-network based on an additional prior, while  are directly learned with the network. 


\subsection{Model Training}
\label{sec:train}
In this subsection, we discuss how to utilize the proposed AdaFM layer for model training. The entire model, namely AdaFM-Net, consists of a basic network and the AdaFM layers. First, we train the basic network , which can be any standard CNN model, for the start restoration level . Then we insert AdaFM layers to  and form the AdaFM-Net . By fixing the parameters of , we optimize the parameters of AdaFM layers on the end level . Experiments demonstrate that by only finetuning the AdaFM layers, the model  could achieve comparable performance with a basic model  trained from scratch on level . As the AdaFM-Net is optimized from  to , we name this process as adaptation, and use \textit{adaptation accuracy} to denote its performance. Specifically, we can use the PSNR distance between PSNR of  and  as the measurement of adaptation accuracy.
There are three factors that affect the adaptation accuracy -- filter size, direction, and range.

(1) For filter size, a larger filter size or more parameters will lead to better adaptation accuracy. We try filter size from  to . From convergence curves shown in Figure~\ref{kernel_size_figure}, we find that  performs much better than  while  is only comparable to . Further increasing the filter size could not continuously improve the performance. 
(2) For direction, different restoration levels have different degrees of difficulty for the same network. Then should we modulate the model from an easy level to a hard level or the opposite direction? Experimentally, we find that from easy to hard is a better choice (see Section~\ref{sec:direction}). 
(3) For range, the smaller of the range/gap , the better the adaptation accuracy. For example, in super resolution problem, transferring the filters from  to  is easier than from  to . In Section~\ref{sec:exp}, we conduct numerous experiments to choose the best range for super-resolution, denoising and compression artifacts reduction. 


\subsection{Modulation testing}
\label{sec:test}
After the training process, we discuss how to modulate the AdaFM layers according to degradation level at test time. As the features remain the same after convolution with an identity filter, we initialize AdaFM layers with identity filters  and zero biases, which is regarded as the start point of AdaFM layers. Based on Observation 2, we can linearly interpolate the parameters of AdaFM layers as

where  are the filter and bias of the interpolated AdaFM layers,  is the interpolation coefficient determined by the degradation level of input image. After adding the interpolated AdaFM layers back to the basic network , we can get the AdaFM-Net  for a middle level . The effects of changing the coefficient  from 0 to 1 are shown in Figure \ref{fig:solution}, \ref{fig:baselines}, where the output effects change continuously along with . 

Interestingly, we find that the interpolated network could fairly deal with any restoration level  between level  and  by adjusting the coefficient , which behaves like a strength controller in traditional methods. Experimentally, we find that the relationship between the coefficient  and restoration level  can be formulated/approximated as a polynomial function:
\vspace{-1em}

where  is the order and  are coefficients. To fit this polynomial function, we need to determine at least  points – . Specially, the start point is  and the end point is . Furthermore, we require a test set with degraded images and ground truth to measure the adaptation accuracy. For a middle level  , we use the test images of level  as inputs. By adjusting the coefficient , the AdaFM-Net could generate a series of outputs. We select the  that achieves the highest PSNR (evaluated on the test set) as the best coefficient, recorded as  for . It is worth noticing that the modulation process and curve fitting require no additional training.

Extensive experiments show that the fitting curve varies a lot with ranges and problems. Take compression artifacts reduction as an example. If the range is small, such as JPEG quality from  to , then the fitting function is linear (order ) as shown in Figure \ref{fig:modulation}. On the other hand, if the range is large, such as from  to , then we have to use a  curve (order ) for approximation. Similar trend is observed for denoising and super resolution (see details in Section~\ref{sec:modulation} and the supplementary file).

As an alternative choice, we can also use the piece-wise linear function for approximation. Actually, when the range is small enough, the relationship between  and  is almost linear. We can train a set of AdaFM-Nets on middle levels . For a given level  (), we can use the coefficient  to interpolate the AdaFM-Nets between  and . This strategy needs to train and store more AdaFM-Nets on middle levels, but the adaptation accuracy is comparably higher due to the small range. 

\section{Experiments}
\label{sec:exp}

\subsection{Experimental Set-up}

\textbf{Training settings.} 
We use the DIV2K \cite{agustsson2017ntire} dataset for all the image restoration tasks. The training data is augmented by horizontal flipping and 90-degree rotations. Following SRResNet \cite{ledig2017photo}, the mini-batch size is set to 16 and the HR patch size is 96  96. The L1 loss \cite{wang2018esrgan} is adopted as the loss function. For model training, the initial learning rate is set to  and then decayed by a factor of 10 after   iterations. We adopt the Adam \cite{Kingma2015AdamAM} optimizer with , . All models are built on the PyTorch framework and trained with NVIDIA 1080Ti GPUs.


\textbf{The structure of basic model.}
Based on the widely used SRResNet and DnCNN \cite{zhang2017beyond}, the basic model  adopts a general CNN structure that consists of a pair of down-sampling (convolution with stride 2) and up-sampling (pixelshuffle \cite{shi2016real} with upscaling factor 2) layers, 16 residual blocks, and several convolution layers. Specifically, the filter number is 64 and the filter size is  for all convolution layers. The residual block contains two convolution layers and a ReLU activation layer. The middle features are processed in a low-resolution (1/4 of the input size) space, while the output size remains the same as the input size. For super-resolution, we can upsample the LR image to the HR image size as SRCNN \cite{dong2014learning}. As shown in Table~\ref{tabel:basic}, the basic model achieves better PSNR results than SRResNet, DnCNN and ARCNN on super-resolution, denoising and compression artifacts reduction, respectively. As stated in Section~\ref{sec:train} and \ref{sec:test}, the basic model is also trained on different levels (as the baseline) to evaluate the performance of AdaFM-Nets. 

\textbf{The position of AdaFM layers.}
As indicated in Section~\ref{sec:AdaFM}, we can insert the AdaFM layers after all convolution layers or just in the residual blocks (the same as BN and IN). Moreover, an alternative choice is to add AdaFM layers \textit{after} all activation layers. To evaluate the above three approaches, we conduct experiments for super resolution task  with filter size .
From the experimental results, we observe that adding AdaFM layers after activation is inferior to that before activation (32.00 dB, 31.84 dB evaluated on Set5 \cite{bevilacqua2012low}). The results of inserting AdaFM layers after all convolution layers and in the residual blocks make little difference (32.01 dB, 32.00 dB evaluated on Set5). To save computation, we insert AdaFM layers just in residual blocks before activation for all experiments.

\begin{table}[]
\small
\centering
\begin{tabular}{r|c|c}
\hline
\hline
Super resolution & SRResNet & basic model \\ 
\textbf{Set5}4& 32.05 & \textbf{32.13}\\
\hline
Denoising & DnCNN & basic model \\ 
\textbf{CBSD68} 15 & 33.89 & \textbf{34.10}\\
\hline
DeJPEG & ARCNN & basic model \\
\textbf{LIVE1}  & 29.13 & \textbf{29.55}\\
\hline
\end{tabular}
\captionsetup{font={small}}
\vspace{-1em}
\caption{Comparisons with the state-of-the-art methods in PSNR.}
\label{tabel:basic}
\end{table}

\begin{figure}[]
	\vspace{-1.5em}
	\centering
	\includegraphics[scale=0.44]{imgs/curve/sr_ksize.pdf}
\vspace{-1.5em}
	\captionsetup{font={small}}
	\caption{The performances of adaptation with different filter sizes of AdaFM layers in super resolution on Set5 dataset.}
	\vspace{-2em}
	\label{kernel_size_figure}
\end{figure}

\textbf{Complexity analysis.}
We calculate the parameters of the basic model and AdaFM layers. Following previous works, we exclude the number of biases that perform add operation in network. The total parameters in basic model include the parameters of 16 residual blocks, 4 convolution layers and a pixelshuffle layer. As we insert the AdaFM layers in residual blocks, the number of AdaFM layers is equal to the number of convolution layers in residual blocks. Thus there are  filters in AdaFM layers. When the filter size is , , , the number of parameters is 2048, 18432, 51200, respectively, accounting for 0.15\% 1.31\% 3.65\% of the total parameters in the basic model. Note that these numbers are even smaller than the parameter number of a single residual block (). Nevertheless, as AdaFM-Net is comparably larger than the basic model, we still need to verify whether it significantly improves the model capacity. In super resolution , we train an AdaFM-Net with AdaFM layers of a large filter size  from scratch. The PSNR value on DIV2K (30.39 dB) is almost the same as that of the basic model (30.37 dB), indicating that the performance is not influenced by AdaFM layers. We can safely use the basic model as baseline to test the AdaFM-Nets. In another perspective, this also demonstrates the effectiveness of the proposed strategy, which adapts the model to different restoration levels with little additional computation cost.


\begin{table}[]
\small
\centering
\begin{tabular}{r|p{1.8em}p{1.8em}p{1.8em}p{2em}|p{2.5em}}
\hline
\hline
 PSNR(dB)& 11 & 33 & 55 & 77 & baseline  \\ \hline
SR \; \quad \quad \quad \quad \textbf{Set5} & 31.42 & 31.88 & 32.00 & \textbf{32.03} & 32.13 \\

\textbf{DIV2K100} & 29.89 & 30.20 & 30.28 & \textbf{30.30} & 30.37 \\
 \hline
DeJPEG \quad\;\textbf{LIVE1} & 29.35 & 29.39 & 29.41 & \textbf{29.42} & 29.55 \\
\hline
 
Denoising \textbf{CBSD68} & 26.35 & 26.38 & 26.39 & \textbf{26.40} & 26.49 \\
 \hline
\end{tabular}
\vspace{-1em}
\captionsetup{font={small}}
\caption{The PSNR results of adaptation with different kernel sizes of AdaFM layers in three tasks.}
\vspace{-1em}
\label{table:kernel_size}
\end{table}


\begin{table}[]
	\small
	\centering
	\begin{tabular}{r|ccc}
		\hline
		\hline
		PSNR(dB) & AdaBN & Conditional IN &  AdaFM-Net\\ 
		\hline
		\textbf{Set5}\, & 34.04 & 33.53 & \textbf{34.34}\\
		 & 28.70 & 31.30 & \textbf{32.00} \\
		\hline
		\textbf{LIVE1}\, & 38.29 & 36.99 & \textbf{38.81}\\
		 & 27.61 & 28.89 & \textbf{29.35}\\
		\hline
		\textbf{CBSD68}  & 33.83 & 31.33 & \textbf{34.10}\\
		 & 19.68 & 24.15 & \textbf{26.35} \\
		\hline
	\end{tabular}
	\vspace{-1em}
	\captionsetup{font={small}}
	\caption{Comparisons with AdaBN \cite{Li2017RevisitingBN} and conditional IN \cite{dumoulin2017learned}}
	\vspace{-2em}
	\label{comparison}
\end{table}


\begin{table*}[t]\centering
\renewcommand{\arraystretch}{0.9}
\vspace{-1.5em}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{@{}rccccccccccc@{}}
\multicolumn{12}{c}{\textbf{Adaptation in Super Resolution}}\\\specialrule{0em}{0pt}{2pt}
\toprule
& \multicolumn{4}{c}{} & & \multicolumn{3}{c}{} &
& \multicolumn{2}{c}{}\\
\cmidrule{2-5} \cmidrule{7-9} \cmidrule{11-12}
&  & & &  & &  &  &  & &  & \\ \midrule
\textbf{Set5} & 34.34 & 32.13 & 30.26 & 28.74 && 32.13 & 30.26 & 28.74 && 34.34 & 37.84\\
AdaFM-Net & 33.98 & 32.00 & 30.16 & 28.73 && 31.66 & 29.98 & 28.61 && 34.11 & 37.11\\
\cmidrule{2-12}
PSNR distance  & 0.36 & \textbf{0.13} & \textbf{0.10} & \textbf{0.01} && 0.47 & 0.28 & \textbf{0.13} && 0.23 & 0.73\\\midrule
  
  
\textbf{DIV2K100} & 32.35 & 30.37 & 29.04 & 28.10 && 30.37 & 29.04 & 28.10 && 32.35 & 36.00\\
AdaFM-Net & 32.07 & 30.28 & 29.02 & 28.09 && 30.01 & 28,88 & 28.00 && 32.14 & 35.13\\
\cmidrule{2-12}
PSNR distance    & 0.28 & \textbf{0.09} & \textbf{0.02} & \textbf{0.01} && 0.36 & \textbf{0.16} & \textbf{0.10} && 0.21 & 0.87\\ 
\bottomrule
\end{tabular}
\vspace{-1em}
\caption{Adaptation results. The PSNR distances within 0.2 dB are shown in bold.}
\vspace{-1.5em}
\label{sr_range_direction}
\end{table*}




\subsection{Evaluation of Model Training}
In this section, we evaluate our proposed method on three image restoration tasks, super resolution, denoising, and compression artifacts reduction (JPEG Deblocking or DeJPEG). The basic settings are shown below. 

For super-resolution, we train our models in RGB channels and calculate the PSNR in y-channel on two widely used benchmark datasets -- Set5~\cite{bevilacqua2012low} and the test set of DIV2K~\cite{agustsson2017ntire}. We evaluate our methods on upscaling factors . All other settings remain the same as SRCNN~\cite{dong2014learning}. In denoising, we use Gaussian noise and consider 5 noise levels, i.e., . Following DnCNN~\cite{zhang2017beyond}, the models are trained with RGB channels and evaluated in RGB channels on CSBD68~\cite{roth2005fields} dataset. For DeJPEG, we use the JPEG quality  in MATLAB JPEG encoder. Similar as ARCNN~\cite{Dong_2015_ICCV}, our models are trained and tested in y channel only. LIVE1 \cite{1709988} dataset is used for evaluation.

\textbf{Filter Size.}
First, we need to determine the filter size of AdaFM layers for different problems. We denote the adaptation from the start level  to the end level  as . The basic model is trained on , and AdaFM-Net is tested on .

For the super resolution task , we compare the performance of AdaFM-Net with various filter sizes -- 11, 33, 55 and 77. The convergence curves on Set5 are plotted in Figure~\ref{kernel_size_figure} , and the quantitative results are presented in Table \ref{table:kernel_size}. In general, larger filters can achieve better performance. Notably, the PSNR gap between  and  is larger than 0.4 dB. However, this trend does not always hold when the filter size is expanded to 77. Therefore, we use the filter size 55 to conduct the following experiments for the super resolution tasks. 

Similar as in super resolution, we compare the performance with different filter sizes (11, 33, 55 and 77) for denoising task  and DeJPEG task . Results shown in Table \ref{table:kernel_size} indicate that in both two tasks, filter size 11 can already achieve excellent performance. The PSNR gap between  and  is less than 0.1 dB. Considering the computation cost, we use filter size  for all denoising and DeJPEG experiments.

\begin{table}[]
	\small
	\setlength{\tabcolsep}{5pt}
	\renewcommand{\arraystretch}{0.9}
	\begin{tabular}{@{}rccccc@{}}
\toprule
		& \multicolumn{4}{c}{} 
		& \multicolumn{1}{c}{}\\
\cmidrule(r{4pt}){2-5}  \cmidrule(l{4pt}){6-6}
		\textbf{\textit{DeJPEG}}
		& 8060 & 8040 & 8020 & 8010 &8010 \\ \midrule
\textbf{LIVE1} & 36.00  &34.34 &31.93 &  29.55 &  38.81\\
AdaFM-Net & 35.98 & 34.29 &31.81 & 29.35 &37.77\\
		\cmidrule{2-6}
distance & \textbf{0.02} &\textbf{ 0.05} &\textbf{ 0.12} & \textbf{0.20} &1.04 \\
		\bottomrule
		\\
		\specialrule{0em}{0pt}{-0.5em}
		\textit{\textbf{Denoising}}
		& 1525 & 1535 & 1550 & 1575 & 1575 \\ \midrule
\textbf{CBSD68} & 31.44 & 29.82 & 28.20 & 26.49 & 34.10\\
AdaFM-Net & 31.43 & 29.78 & 28.13 & 26.35 & 33.42\\
		\cmidrule{2-6}
distance & \textbf{0.01} & \textbf{0.04} & \textbf{0.07} & \textbf{0.14} & 0.68 \\
		\bottomrule
	\end{tabular}
	\vspace{-1em}
	\captionsetup{font={small}}
	\caption{Adaptation results of DeJPEG and Denoising. The PSNR distances within 0.2 dB are shown in bold.}
	\vspace{-2em}
	\label{jpeg_range_direction}
\end{table}


\textbf{Direction.}
\label{sec:direction}
The second step is to find the best adaptation direction. Before experiments, it is essential to clarify the way of measurement. For task , the baseline is the basic model trained on  with performance , and the AdaFM-Net is finetuned on  with performance . Then the PSNR distance  is used to evaluate the \textit{adaptation accuracy} of AdaFM-Net. In experience, 0.3 dB is regarded as a significant PSNR gap in image restoration. In other words, if the distance  exceeds 0.3 dB, then the adaptation is NOT well-suited for applications. 

We conduct three pairs of experiments -- super resolution task  and , denoising task  and , DeJPEG task  and . Results are shown in Table~\ref{sr_range_direction}, \ref{jpeg_range_direction}. In all three problems, the tasks with direction from easy to hard (i.e., , , ) achieve better adaptation results. Here, \textit{easy} and \textit{hard} refer to the difficulty of restoring the input images. For example, in DeJPEG, the PSNR distance of  is 0.2 dB, which is much lower than that of the inverse direction  -- 1.04 dB.



\textbf{Range.}
In this subsection, we investigate the influence of the adaptation range. Generally, by fixing the start level , we change the end level  and test the adaptation accuracy. 

Different from previous sections, we start discussion with denoising and DeJPEG, where the trend of range is more obvious. In denoising, we start with  and change the end level from  to . In DeJPEG, we start with  and change the end level from  to . The adaptation results are shown in Table~\ref{jpeg_range_direction}. It is observed that better adaptation accuracy is obtained with a smaller range.
In addition, the proposed AdaFM can easily handle very large range in either denoising or DeJPEG.

For super resolution, we find it hard to adapt the model across even 2 upscaling factors. For example, in Table~\ref{sr_range_direction}, the PSNR distance for the task  exceeds 0.3 dB for all three test sets, indicating that we should not further enlarge the range to 3. When fixing the range to be 1 and 2 upscaling factors, we change both the start and end level to see the change of results. From Table~\ref{sr_range_direction}, we can conclude that the adaptation is easier (lower PSNR distance) with a harder start level (e.g.,  is better than ).

\begin{figure*}
	\centering
	\vspace{-2em}
	\includegraphics[scale=0.27]{imgs/figure_last.pdf}
	\captionsetup{font={small}}
	\vspace{-1em}
	\caption{Left: Artifacts on the output images produced by AdaBN and conditional instance normalization. Right: Modulation testing in Denoising (CBSD68), DeJEPG (LIVE1) and Super Resolution (Set14 \cite{zeyde2010single}).}
	\vspace{-1.5em}
	\label{fig:baselines}
\end{figure*}

\vspace{-0.7em}
\subsubsection{Comparison with AdaBN and Conditional IN}
\quad We compare with state-of-the-art methods on super resolution task , denoising task  and DeJPEG task . To compare with AdaBN \cite{Li2017RevisitingBN}, we train a network with batch normalization after all convolutional layers in the residual blocks, and then change all the statistics in BN layers during testing. We also use conditional IN \cite{huang2017arbitrary} to handle different levels of restoration. The results are shown in Table \ref{comparison}.
It can be obviously observed that neither of the two methods can obtain reasonable super resolution, denoising and DeJPEG results. Therefore, they are not suitable for image restoration tasks. Qualitative comparisons are also shown in Figure~\ref{fig:baselines}, where we observe clear artifacts on their output images.



\subsection{Evaluation of Modulation Testing}
\label{sec:modulation}
In modulation testing, continuously manipulating the interpolation coefficient  could gradually change the output effect. If the input image is fixed, then the output image will become sharper or smoother with the increase of , as shown in Figure~\ref{fig:baselines}. On the other hand, we can choose different  to deal with different kinds of degraded images. As presented in Section~\ref{sec:test}, the coefficient  can be formulated as a polynomial function of restoration level  -- . In this subsection, we investigate the curving fitting with different ranges in DeJPEG problem. Similar investigations on super resolution and denoising problems can be found in supplementary file. 

We first investigate the DeJPEG task .
We select 6 middle levels --  -- between  and . Then for a given level , we use the test images of  as inputs, and adjust  to obtain different outputs of AdaFM-Net. After calculating the PSNR values on LIVE1 test set, we select the  that achieves the best PSNR as the best coefficient. For example, see the blue line in Figure~\ref{fig:modulation}, the best coefficient for level   and  are 0.14, 0.40, respectively. After we have obtained all middle points, we fit the curve by a cubic function:. Then for arbitrary levels between  and , we can use this function to predict its corresponding interpolation coefficient. If we test a smaller range, such as , then a simple straight line could fairly connect all middle points (see the orange line in Figure~\ref{fig:modulation}). In other words, the polynomial function is linear. This property holds for smaller ranges such as . 



To verify whether the interpolated image is of high quality, we use the PSNR distance on LIVE1 test set as the evaluation metric. Specifically, the basic model trained on level  is used as the baseline, and the PSNR distance is calculated between the PSNR of AdaFM-Net and that of a well-trained baseline model. The smaller of the PSNR distance the better of the adaptation/modulation accuracy. Figure~\ref{fig:modulation} illustrates the PSNR distances in two DeJPEG tasks. It is observed that all PSNR distances are below 0.2 dB, indicating that the output quality is good enough for practical usages. Further, the PNSR distances of the small-range task  is much lower than the large-range task . Thus modulation across smaller ranges achieves better performance. For higher request of modulation quality, we can decompose a large range to several small ranges, and train AdaFM-Nets for each sub-task. We can balance the performance and computation burden according to different applications.
\begin{figure}[h]
	\vspace{-1em}
\centering
	\includegraphics[scale=0.32]{imgs/jpeg_modulation/modulate_jpeg.pdf}
	\includegraphics[scale=0.32]{imgs/jpeg_modulation/modulate_jpeg_error.pdf}
\vspace{-1em}
	\captionsetup{font={small}}
	\caption{Top: the curve fitting with different ranges in DeJPEG problem; Bottom: the value of PSNR distance is annotated above each bar.}
	\label{fig:modulation}
	\vspace{-2em}
\end{figure}

\section{Conclusion}
We present a method that allows continual modulation of restoration levels in a single CNN for versatile and flexible image restoration. 
The core idea of our method is to handle images with arbitrary degradation levels with a single model, which consists of a basic model and a modulation layer -- AdaFM layer. We further propose the learning and modulating strategies of the AdaFM layers. In test time, the model can be adapted to any restoration level by directly adjusting the AdaFM layers without an additional training stage.

\textbf{Acknowledgements}. 
This work is partially supported by National Key Research and Development Program of China (2016YFC1400704), Shenzhen Research Program (JCYJ20170818164704758, JCYJ20150925163005055, CXB201104220032A), and Joint Lab of CAS-HK.


{\small
\bibliographystyle{ieee_fullname}
\begin{thebibliography}{10}\itemsep=-1pt
	
	\bibitem{agustsson2017ntire}
	Eirikur Agustsson and Radu Timofte.
	\newblock Ntire 2017 challenge on single image super-resolution: Dataset and
	study.
	
	\bibitem{Ba2016LayerN}
	Jimmy Ba, Ryan Kiros, and Geoffrey~E. Hinton.
	\newblock Layer normalization.
	\newblock {\em CoRR}, abs/1607.06450, 2016.
	
	\bibitem{bevilacqua2012low}
	Marco Bevilacqua, Aline Roumy, Christine Guillemot, and Marie~Line
	Alberi-Morel.
	\newblock Low-complexity single-image super-resolution based on nonnegative
	neighbor embedding.
	\newblock 2012.
	
	\bibitem{Dong_2015_ICCV}
	Chao Dong, Yubin Deng, Chen Change~Loy, and Xiaoou Tang.
	\newblock Compression artifacts reduction by a deep convolutional network.
	\newblock In {\em The IEEE International Conference on Computer Vision (ICCV)},
	December 2015.
	
	\bibitem{dong2014learning}
	Chao Dong, Chen~Change Loy, Kaiming He, and Xiaoou Tang.
	\newblock Learning a deep convolutional network for image super-resolution.
	\newblock In {\em European conference on computer vision}, pages 184--199.
	Springer, 2014.
	
	\bibitem{dumoulin2017learned}
	Vincent Dumoulin, Jonathon Shlens, and Manjunath Kudlur.
	\newblock A learned representation for artistic style.
	\newblock {\em Proc. of ICLR}, 2017.
	
	\bibitem{huang2017arbitrary}
	Xun Huang and Serge~J Belongie.
	\newblock Arbitrary style transfer in real-time with adaptive instance
	normalization.
	\newblock In {\em ICCV}, pages 1510--1519, 2017.
	
	\bibitem{Ioffe2015BatchNA}
	Sergey Ioffe and Christian Szegedy.
	\newblock Batch normalization: Accelerating deep network training by reducing
	internal covariate shift.
	\newblock In {\em ICML}, 2015.
	
	\bibitem{Kingma2015AdamAM}
	Diederik~P. Kingma and Jimmy Ba.
	\newblock Adam: A method for stochastic optimization.
	\newblock {\em CoRR}, abs/1412.6980, 2015.
	
	\bibitem{ledig2017photo}
	Christian Ledig, Lucas Theis, Ferenc Husz{\'a}r, Jose Caballero, Andrew
	Cunningham, Alejandro Acosta, Andrew~P Aitken, Alykhan Tejani, Johannes Totz,
	Zehan Wang, et~al.
	\newblock Photo-realistic single image super-resolution using a generative
	adversarial network.
	
	\bibitem{Li2017RevisitingBN}
	Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi Hou.
	\newblock Revisiting batch normalization for practical domain adaptation.
	\newblock {\em CoRR}, abs/1603.04779, 2017.
	
	\bibitem{lim2017enhanced}
	Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung~Mu Lee.
	\newblock Enhanced deep residual networks for single image super-resolution.
	\newblock In {\em The IEEE conference on computer vision and pattern
		recognition (CVPR) workshops}, volume~1, page~4, 2017.
	
	\bibitem{roth2005fields}
	Stefan Roth and Michael~J Black.
	\newblock Fields of experts: A framework for learning image priors.
	\newblock In {\em Computer Vision and Pattern Recognition, 2005. CVPR 2005.
		IEEE Computer Society Conference on}, volume~2, pages 860--867. IEEE, 2005.
	
	\bibitem{salimans2016weight}
	Tim Salimans and Diederik~P Kingma.
	\newblock Weight normalization: A simple reparameterization to accelerate
	training of deep neural networks.
	\newblock In {\em Advances in Neural Information Processing Systems}, pages
	901--909, 2016.
	
	\bibitem{1709988}
	H.~R. Sheikh, M.~F. Sabir, and A.~C. Bovik.
	\newblock A statistical evaluation of recent full reference image quality
	assessment algorithms.
	\newblock {\em IEEE Transactions on Image Processing}, 15(11):3440--3451, Nov
	2006.
	
	\bibitem{shi2016real}
	Wenzhe Shi, Jose Caballero, Ferenc Husz{\'a}r, Johannes Totz, Andrew~P Aitken,
	Rob Bishop, Daniel Rueckert, and Zehan Wang.
	\newblock Real-time single image and video super-resolution using an efficient
	sub-pixel convolutional neural network.
	\newblock In {\em Proceedings of the IEEE conference on computer vision and
		pattern recognition}, pages 1874--1883, 2016.
	
	\bibitem{DBLP:journals/corr/UlyanovVL16}
	Dmitry Ulyanov, Andrea Vedaldi, and Victor~S. Lempitsky.
	\newblock Instance normalization: The missing ingredient for fast stylization.
	\newblock {\em CoRR}, abs/1607.08022, 2016.
	
	\bibitem{wang2018recovering}
	Xintao Wang, Ke Yu, Chao Dong, and Chen Change~Loy.
	\newblock Recovering realistic texture in image super-resolution by deep
	spatial feature transform.
	\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
		Pattern Recognition}, pages 606--615, 2018.
	
	\bibitem{wang2018esrgan}
	Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and
	Chen~Change Loy.
	\newblock Esrgan: Enhanced super-resolution generative adversarial networks.
	\newblock In {\em European Conference on Computer Vision}, pages 63--79.
	Springer, 2018.
	
	\bibitem{wu2018group}
	Yuxin Wu and Kaiming He.
	\newblock Group normalization.
	\newblock In {\em Proceedings of the European Conference on Computer Vision
		(ECCV)}, pages 3--19, 2018.
	
	\bibitem{zeyde2010single}
	Roman Zeyde, Michael Elad, and Matan Protter.
	\newblock On single image scale-up using sparse-representations.
	\newblock In {\em International conference on curves and surfaces}, pages
	711--730. Springer, 2010.
	
	\bibitem{zhang2017beyond}
	Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang.
	\newblock Beyond a {Gaussian} denoiser: Residual learning of deep {CNN} for
	image denoising.
	\newblock {\em IEEE Transactions on Image Processing}, 26(7):3142--3155, 2017.
	
\end{thebibliography}

}

\end{document}
