\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{makecell}
\usepackage{mathrsfs}


\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy 


\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}



\begin{document}

\title{ELSD: Efficient Line Segment Detector and Descriptor}

\author{Haotian Zhang\\
Megvii Technology\\
{\tt\small zhanghaotian@megvii.com}
\and
Yicheng Luo\\
Megvii Technology\\
{\tt\small luoyicheng@megvii.com}
\and
Fangbo Qin\\
Institute of Automation, CAS\\
{\tt\small qinfangbo2013@ia.ac.cn}
\and
Yijia He\\
{\tt\small heyijia2016@gmail.com}
\and
Xiao Liu\\
Megvii Technology\\
{\tt\small liuxiao@megvii.com}
}

\maketitle
\ificcvfinal\thispagestyle{empty}\fi

\begin{abstract}
   We present the novel Efficient Line Segment Detector and Descriptor (ELSD) to simultaneously detect line segments and extract their descriptors in an image. Unlike the traditional pipelines that conduct detection and description separately, ELSD utilizes a shared feature extractor for both detection and description, to provide the essential line features to the higher-level tasks like SLAM and image matching in real time. First, we design the one-stage compact model, and propose to use the mid-point, angle and length as the minimal representation of line segment, which also guarantees the center-symmetry. The non-centerness suppression is proposed to filter out the fragmented line segments caused by lines' intersections. The fine offset prediction is designed to refine the mid-point localization. Second, the line descriptor branch is integrated with the detector branch, and the two branches are jointly trained in an end-to-end manner. In the experiments, the proposed ELSD achieves the state-of-the-art performance on the Wireframe dataset and YorkUrban dataset, in both accuracy and efficiency. The line description ability of ELSD also outperforms the previous works on the line matching task.
\end{abstract}

\section{Introduction}

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.9\linewidth]{fps.png}
\end{center}
   \caption{Inference speed (FPS) and accuracy () on Wireframe dataset.}
\label{FPS}
\end{figure}

Image representation is an essential issue for many computer vision tasks such as SLAM,  Structure-from-Motion (SfM), and image matching. Local point features \cite{superpoint,SIFT,orb} are widely used in these tasks, and recently the researchers have been exploring the usage of structural features for the better geometric representation\cite{plvio,planar_SLAM,RGB-D_SLAM_for_planar,Line_Correspondences,Line-Based_Map}. Line segments are the most widely seen structural features in man-made environments. The reliable extraction of line segments and the matching across frames are important for the aforementioned tasks.

Recently, the convolutional neural networks (CNN) based line segment detection models have significantly outperformed the traditional methods. The models\cite{HAWP,PPGNET,LCNN} consist of two stages.
They first detect junctions and then generate line segment proposals and finally feed the embedding of each line segment into a classifier. Although these two-stage methods can achieve high performance, their running speed cannot satisfy real-time applications. TP-LSD\cite{TP-LSD} first realizes the compact one-stage detection by introducing the Tri-points representation of line segment. However, TP-LSD predicts the two end-points separately and does not leverage the center-symmetric characteristics of the line segment. Thus, the predicted root-point might not be the mid-point of the two predicted end-points, and even the three points might be not co-linear. Moreover, the prediction of the root point is ambiguous especially when the lines intersect with each other so that many false root-points belonging to the fragmented line segments are detected.  Besides, TP-LSD does not differentiate hard and easy examples during training. Some hard root points of line segments may not be properly detected.  

Line segment descriptor is required to represent the line segment in a high-dimensional metric space, and the same line in two adjacent frames should be close in this metric space. There exist some CNN-based line descriptors\cite{WLD,DLD,LLD}. However, these line descriptors are designed individually, and not yet tightly coupled with the line segment detector. It is also time-consuming to execute detection and description separately. 



To this end, we propose ELSD that simultaneously predicts line segments and inferences line descriptors in an end-to-end fashion. 1) We introduce the one-stage architecture that utilizes the Center-Angle-Length (CAL) representation to vectorize a line segment. Our line detector consists of two module:  localization module and  regression module. 2) Since the mid-points might be ambiguous for detection when lines intersect, as shown in Figure \ref{centerness}b, we introduce the line-centerness to filter the false mid-points belonging to fragmented line segments and adopt modified focal loss\cite{Focal} to focus more on the mid-points of hard cases. 3) In the regression module, the geometric maps are predicted to provide the rotation angles and lengths. Moreover, we refine the position of the midpoints by predicting the fine offsets to compensate for the localization accuracy. 4) In the line descriptor branch, we obtain the descriptor of each predicted line segment by line pooling. The descriptor is trained learned by random homography-based self-supervision. The pipeline of ELSD is shown in Figure \ref{framework}.



In summary, the main contributions are as follows:


\begin{itemize}
\item We present a pipeline that simultaneously detects line segments and inferences line descriptors in an end-to-end fashion. To the best of our knowledge, this is the first work that unifies line detector and descriptor in a compact neural network. The major computation in the backbone is shared by the two tasks, and the two task branches can be jointly training, with negligible loss on detection performance.

\item We utilize the Center-Angle-Length (CAL) representation to encode a line segment that has only four parameters to predict. To overcome the detection ambiguity when lines intersect, we proposed the non-centerness suppression mechanism to remove the mid-points of fragmented line segments. The midpoint position is further refined by using the offset regression so that the line segment localization is more precise.

\item Our proposed ELSD obtains state-of-the-art performance in both accuracy and efficiency on the Wireframe and YorkUrban datasets. Moreover, the light version of our model achieves the speed of 107.5 FPS on a single GPU (RTX2080Ti) with comparable performance.




\end{itemize}


\section{Related Works}
\subsection{Line Segment Detection}
Deep learning-based line segment detection methods has attracted great attention due to the remarkable performances\cite{wireframe_cvpr18,TP-LSD,HAWP,LCNN,3Dwireframe}. 
AFM\cite{AFM} presented regional partition maps and attraction field maps of line segment maps, followed by a squeeze module to generate line segments. L-CNN\cite{LCNN} first proposed a two-stage pipeline for wireframe parser. It predicts junction map to generate line proposals and utilize the LoI-pooling to gather feature of the proposals. Then a line verification network classifies proposals and removes false lines. PPGNet\cite{PPGNET} used a graph formulation to represent the relation between junctions. HAWP\cite{HAWP} proposed a 4-D holistic attraction field map for generating line proposals and refine the proposals with junction heat maps. HT-HAWP\cite{HT-HAWP} combined Hough transform and HAWP model, obtaining excellent results in line segment detection. As the first one-stage line segment detector, TP-LSD\cite{TP-LSD} proposed a Tri-Points representation to encode line segments and predicted two endpoints of each line segment in an end-to-end manner. LETR\cite{LETR} applied transformers for line segment detection from coarse-to-fine grained. Our ELSD has a similar pipeline with TP-LSD. We encode a line segment by CAL representation, and can directly detect possible semantic line segments in the image without additional classification.

\subsection{Object Detection}
The recent surge of some keypoint-based object detectors has achieved remarkable performance. CornerNet\cite{CornerNet} formulated each object by a pair of corner keypoints and grouped all the detected corner keypoints to form the final detected bounding box, which requires more complicated post-processing.
CenterNet\cite{Centernet} models an object by the center point of its bounding box, and uses keypoint estimation method to find center points and regresses to its size. FCOS\cite{FCOS}  treats all the pixels with an object as candidate position and proposed center-ness to represent the importance of all the candidate positions. PolarNet\cite{polarnet} learns corner pairs based on polar coordinates and avoids the large variance of learned offsets in Cartesian coordinate. Such keypoint-based methods have good detection capabilities with a  fast speed and brief structure. Motivated by these, we proposed a new line segment representation and further designed a keypoint-based line segment detector.  

\begin{figure*}[t]
	\begin{center}
		\scalebox{0.8}{
\includegraphics[width=0.9\linewidth]{framework.png}
		}
	\end{center}
	\caption{Illustration of the architecture of our proposed ELSD. It consists of three components: backbone, line detector branch and line descriptor branch. See text for details.}
	\label{framework}
\end{figure*}


\subsection{Line Description}
Like descriptor-based keypoint matching\cite{superpoint,SIFT,orb}, line matching is also based on comparing the descriptors of the same line segments in two frames. MSLD\cite{MSLD} constructs the line descriptors by counting the mean and variance of the gradients of pixels in the neighbor region of a line segment. LBD\cite{LBD} proposes a line-band descriptor that computes gradient histograms over bands with more robustness and efficiency. Recently, some deep learning-based methods are used in learning line descriptors. LLD\cite{LLD} and DLD\cite{DLD} use the convolution neural network to learn the line descriptors and achieve remarkable performance. 




\section{Methods}
\subsection{Line Representation}
Line segments have two characteristics: 1) Due to the center-symmetry, the mid-point determines the location of the line segment, then the geometric feature is determined by the angle and length. 2) Since a line segment is straight, its direction can be consistently measured from a local part of it, which is easier to learn and requires a small receptive field. Therefore, we propose the Center-Angle-Length (CAL) representation to vectorize a line segment, which only has four parameters: 2D coordinates, rotation angle, and total length. In comparison, the Tri-points representation in TP-LSD\cite{TP-LSD} has six parameters to predict, which is redundant, and the prediction results might not satisfy the center-symmetry.








With angle , length , and center point , the two endpoints of the line segment are given by,

\begin{small}
.
\end{small}


\subsection{Overall Network Architecture}
As shown in Figure \ref{framework}, our proposed ELSD consists of a backbone, a line detector branch, and a line descriptor branch. Our backbone is a U-shape network that consists of an encoder and two decoder blocks. The backbone takes an image of size  as input and outputs the shared feature with a size of . After the backbone, the architecture splits into two parts: one for line detector and the other for line descriptor. The line detector branch can predict line segments from an image. We can further obtain line descriptors by feeding both shared feature and predicted line segments into the line descriptor branch. ELSD can produce line segments and further extract fixed dimensional descriptors of the line segments in a single forward pass. Moreover, unlike the traditional pipeline that first detects line segments, then computes line descriptors, ELSD shares most of the parameters between these two tasks, which reduces the computation cost and improves the compactness. 




\subsection{Line Detector Branch}
Our line detector branch takes the shared feature from the backbone as input and splits into two modules: 1) Localization module, which consists of a line-midpoint detection head and a line-centerness detection head. In Non-Centerness-Suppression (NCS), the two heads are combined to get a more accurate center detection; 2) Regression module, which contains a geometrics regression head and a fine offset regression head. The outputs of the regression module are a pair of geometrics maps that consists of  and a pair of fine offset maps. Finally, the outputs of two modules are combined together to generate the mid-points with two symmetrical endpoints as the line segment detection results.

\subsubsection{Localization Module}
Similar to TP-LSD\cite{TP-LSD}, we use a deformable convolution, two atrous convolution (dilation rate=2) and a standard convolution layers to obtain the adaptive spatial sampling and a large receptive field, to predict the mid-point map. Furthermore, we leverage the line-centerness, i.e. how close an on-line point lies to the mid-point, to distinct the mid-points of the entire lines and the fragmented lines. The line-centerness is calculated by,
\begin{small}

\end{small}
where  are the distances from a point on the line segment to the two end-points, respectively. Apparently,  equals 1 when the point is midpoint and decreases to 0 when the point approximates the end-points.  
\begin{figure}[t]
\begin{center}
\includegraphics[width=0.9\linewidth]{out.png}
\end{center}
   \caption{Illustration of Non-Centerness-Suppression (NCS). (b) and (c) show the predicted mid-point map and centerness map, respectively.}
\label{centerness}
\end{figure}

The line-centerness module has the same architecture as the localization module. 
Denote the predicted line-midpoint map and line-centerness map as  and , respectively. As shown in Figure \ref{centerness}, we propose the Non-Centerness Suppression (NCS) to filter false local midpoints belonging to fragmented line segments, and obtain a more accurate center confidence map , as given by,




The effectiveness of NCS is explained as follows. The midpoint detection is to obtain the exact positions but is prone to false detection caused by lines intersection. As shown in Figure \ref{centerness}, when a line segment is intersected with another line, its two endpoints and the intersection point form two shorter fragmented line segments. Although the mid-points of these fragmented line segments are not annotated as ground truths and are not expected to be detected, the detector tends to detect them because the fragmented line segments satisfy the definition of line segment. Differently, as visualized in Figure \ref{centerness}, line-centerness is not exact but provides a non-local distribution along the global line segment. The non-local distribution is more significant to inference and contains the global structure information of the potentially intersected lines. Namely, the midpoints can only mark a line segment without the awareness of the global structure, and the line-centerness map can further encode the global structure information with a non-local non-linear multi-peak 2D distribution. Therefore, the line mid-point map and line-centerness map are fused by Eq \ref{center_eq} to suppress the false detection and get the final mid-points. Thus the ambiguity problem met by TP-LSD is effectively alleviated.


\subsubsection{Regression Module} \label{polar_sec}
Our regression module consists of two heads: a fine offset regression head and a geometrics regression head. The fine offset regression head is used to predict the offset of the center caused by the downsampling ratio. The refined sub-pixel mid-point can be obtained by just add the corresponding offset to the position of the predicted mid-point. The geometrics regression head can predict angle and length with respect to the midpoint. Both of our regression heads contain two , a  convolutional layers, and a deconvolutional layer. The deconvolutional layer is used to restore the size of the output map to . We can index the related angle  and length  by the center position  on the output map. Then a line segment can be obtained by Eq \ref{polar_eq}.


We utilize the CAL representation rather than Cartesian coordinates representation because the angle belongs to the geometric attributes of the line segment itself. Since the angle information can be perceived from a local part of the line segment, it is easier and more precise to predict the angle than the coordinates. We have done experiments to compare CAL representation and Cartesian coordinate representation under the same settings in Section \ref{ablation_sec}.

\begin{figure}[t]
	\begin{center}
		\scalebox{1.1}{
\includegraphics[width=0.9\linewidth]{line_pooling.png}
		}
	\end{center}
	\caption{Illustration of Line Pooling. See text for more details.}
	\label{linepooling}
\end{figure}
\subsection{Line Descriptor Branch}
Given a set of line segments, the purpose of the line descriptor branch is to learn a fixed-length descriptor for each line segment, which is used to distinguish different line segments according to the distance between their descriptors. We first apply two   stride-1 convolution on the shared feature map from backbone. Then this intermediate feature map is resized to  by bilinear interpolation. The resulting feature map named dense descriptor map is used in the following Line Pooling.





\textbf{Line Pooling:} Similar to RoIPool\cite{roipool} and RoIAlign\cite{roialign} used in object detection, the Line Pooling is used to squeeze the rotated narrow ROI to a descriptor vector. As shown in Figure \ref{linepooling}, the RoI of a line segment is defined as a rotated bounding box centered at the line segment, with the same length and angle as the line segment. The width of the RoI is a hyperparameter that depends on the desired size of the receptive field. Then we crop a fixed-size line feature map by sampling from the dense descriptor map using bilinear interpolation. Assuming there exist  candidate line segments and each line feature map has the size of , in which  is the channel dimension of the dense descriptor map and  represent the height and width of the line feature map respectively. We further apply a  stride-1 depth-wise convolution as well as a stride- max pooling to the line feature map. Finally, the resulting feature vector is flattened and fed into a fully connected layer and then normalized, producing the final descriptor with a fixed length denoted as . 



\begin{figure}[]
\begin{center}
\centering
\includegraphics[width=\linewidth]{static_dynamic.png}
\end{center}
 \caption{Training framework. The random homography is used to realize self-supervised training. See text for details.}
    \label{training_fig}
\end{figure}


\textbf{Self-supervised learning:} Similar to \cite{superpoint}, we apply random homographies on an image to produce a paired image with different views of the same scene, assuming that planar scenes or distant scenes are common in the real environment. The homography transformation that we used is composed of a set of transformations such as translation, scale, rotation, and perspective distortion, covering most of the viewpoint change caused by camera motion. After applying random homography on the input image, we can obtain the exact image-to-image transformation. So we can label matched or unmatched line segments just by transforming the endpoint of the line segment from one image to another and checking whether the distance of two corresponding endpoints is close enough.\label{static_and_dynamic}

When training from scratch, inspired by the Line Sampling Module of L-CNN\cite{LCNN} that adopts static line sampler and dynamic line sampler to train the classifier, we use static line segments and dynamic line segments to train the descriptors. In the training stage, the static line segments are the annotated ground-truths, and the dynamic line segments are those predicted by the detection branch which changed as the model training proceeds. Because the line segment detection are not confident at the early training stage, we only use the detected line segments that are close enough to ground-truths as the dynamic line segments. 
Note that for training line descriptor branch, the proposed ELSD is trained on mini-batches of image pairs. We can obtain the ground truth correspondence of a pair of image's static line segments set during data preparation. The ground truth correspondence of a pair of dynamic line segments can be given by its closest static line segments. If the closest static line segments of a pair of dynamic line segments are matched, we then label this dynamic pair as a match and otherwise non-match. 
The whole training process of ELSD is shown in Figure \ref{training_fig}. 
To sum up, the training with the static line segments helps to cold-start the training of descriptors at the beginning. The training with dynamic line segments helps to couple the descriptor with the actual prediction of the detector.




\subsection{Loss Functions}
\subsubsection{Total Loss}
The total loss to train ELSD is composed of the line detector loss  and line descriptor loss . Note that the input of ELSD is a pair of images with random homographies, which have both ground truth line segments, as well as the ground truth correspondence of ground truth line segments and predicted line segments. This allows us to optimize the two losses simultaneously. Given a pair of image, , and the total loss can be represented as: 

We empirically set  in this work.
\subsubsection{Line Detector Loss}
In the training stage of line detector branch, the outputs of four heads include line midpoint map, line-centerness map, geometrics maps, and fine offset maps. The ground truth of these maps is generated from the raw line segments label. The total loss of line segment detection is shown in Eq \eqref{AllLoss}

where weights 

\textbf{Localization loss:} Given an image , for each ground truth midpoint  with continuous value,
we construct the midpoint confidence map  with four pixels near the midpoint by flooring and ceiling and we denote the selected pixels set by . The 2D Gaussian kernel exp) is then used to compute each confidence of the pixels in . Then we normalize these confidence by dividing the max value of . If the confidence of a pixel is assigned more than one time, we keep the max value of it. The overall process is described by,

Then we followed CornerNet\cite{CornerNet} to use a variant of focal loss:

where  and  are hyper-parameters and  is the number of midpoints in an image. We set  and .


 According to Eq \eqref{centerness_eq}, we can obtain ground truth centerness map. Then we use weighted Binary Cross Entropy (BCE) loss denoted as  to supervise the learning process of the centerness.

 
 
\textbf{Regression loss:} Suppose the ground truth angle, length is  and the corresponding predicted angle, length is . We use  loss and smooth  loss as the geometrics regression loss which is defined as


where . Besides, to recover the discretization error of midpoint coordinate caused by downsampling with ratio , we additionally predict the fine offset maps  for each midpoint. The offset is trained with  loss.
\begin{small} 

\end{small}
Note that only the midpoints where the confidence score of the ground truth equals 1 are involved in regression loss calculation.

\begin{table*}[t]
\resizebox{\textwidth}{32mm}{
\begin{tabular}{|l|c|c|ccc|cc|ccc|cc|c|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Method}} & \multicolumn{1}{|c|}{\multirow{2}{*}{ \makecell[l]{Input \\size}    }} & \multirow{2}{*}{Backbone} & \multicolumn{5}{c|}{Wireframe}     & \multicolumn{5}{c|}{YorkUrban}     & \multirow{2}{*}{FPS} \\ \cline{4-13}
\multicolumn{1}{|c|}{}       &\multicolumn{1}{|c|}{}     &                           & sAP & sAP & sAP & AP  & F   & sAP & sAP & sAP & AP  & F   &                      \\ \hline
LSD\cite{LSD}             & 320                             & /                         & /    & /     & /     & 55.2 & 62.5 & /    & /     & /     & 50.9 & 60.1 & 100                \\ \hline
AFM\cite{AFM}             & 320                              & U-Net                     & 18.5 & 24.4  & 27.5  & 69.2 & 77.2 & 7.3  & 9.4   & 11.1  & 48.2 & 63.3 & 12.8                 \\ \hline
DWP\cite{wireframe_cvpr18}             & 512                         & Hourglass                 & 3.7  & 5.1   & 5.9   & 67.8 & 72.2 & 1.5  & 2.1   & 2.6   & 51   & 61.6 & 2.2                 \\ \hline
LETR\cite{LETR}            & 512                         & ResNet101                 & /    & 65.2  & 67.7  & 86.3 & \textbf{83.3} & /    & 29.4  & 31.7  & 62.7 & 66.9 & /                    \\ \hline
TP-LSD-Lite\cite{TP-LSD}     & 320                         & ResNet34                  & 56.4 & 59.7  & /     & /    & 80.4 & 24.8 & 26.8  & /     & /    & \textbf{68.1} & 78.2                 \\ \hline
TP-LSD\cite{TP-LSD}          & 512                         & ResNet34                  & 57.6 & 57.2  & /     & /    & 80.6 & 27.6 & 27.7  & /     & /    & 67.2 & 18.1                 \\ \hline
L-CNN\cite{LCNN}           & 512                         & Hourglass                 & 58.9 & 62.9  & 64.9  & \makecell[l]{80.3 \\82.8} & \makecell[l]{76.9 \\81.3} & 24.3 & 26.4  & 27.5  &  \makecell[l]{58.5 \\59.6} & \makecell[l]{63.8 \\65.3} & 11.1                 \\ \hline
HT-HAWP\cite{HT-HAWP}         & 512                         & Hourglass                 & 62.9 & 66.6  & /     & /    & /    & 25   & 27.4  & /     & /    & /    & 8.9                    \\ \hline 
HAWP\cite{HAWP}            & 512                         & Hourglass                 & 62.5 & 66.5  & 68.2  & \makecell[l]{84.5 \\86.1} & \makecell[l]{80.3 \\83.1} & 26.1 & 28.5  & 29.7  & \makecell[l]{60.6 \\61.2} & \makecell[l]{64.8 \\66.3} & 32.1                 \\ \hline \hline
Ours-Lite         & 256                         & ResNet34                 & 57.4 & 63.1  & 65.5  & 85.6 & 80.2 &  24.3 & 27.4  & 29.3  & \textbf{63.2} & 63.3 & \textbf{107.5}                    \\ \hline
Ours-HG         & 512                         & Hourglass                 & 62.7 & 67.2  & 69.0  & 84.7 & 80.3 &  23.9 & 26.3  & 27.9  & 57.8 & 62.1 & 47                    \\ \hline
Ours-Res34      & 512                         & ResNet34                  & \textbf{64.3} & \textbf{68.9}  & \textbf{70.9}  & \makecell[l]{\textbf{87.2} \\87.3}  & \makecell[l]{82.3 \\83.1} & \textbf{27.6} & \textbf{30.2}  & \textbf{31.8}  & \makecell[l]{62.0 \\62.6} & \makecell[l]{63.6 \\64.8}     & 42.6                  \\ \hline
\end{tabular}
}
\caption{Comparison experiments on line segment detection. '/' means the values are not reported in the related paper. '' means the post-processing scheme proposed in L-CNN\cite{LCNN} is used.} 
\label{CompareWithLSD}
\end{table*}

\begin{figure*}[htbp]
\centering
\includegraphics[width=0.24\textwidth]{pic/sap10_wire.pdf}
\includegraphics[width=0.24\textwidth]{pic/apH_all_wire.pdf}
\includegraphics[width=0.24\textwidth]{pic/sap10_york.pdf}
\includegraphics[width=0.24\textwidth]{pic/apH_all_york.pdf}
\caption{PR curves of sAP and AP on Wireframe datasets (the left two figures) and YorkUrban datasets (the right two figures). The curve of our model is depicted in red. The results of DWP, AFM, and LSD on YorkUrban datasets are not displayed since they are slightly lower than the current methods.}
\label{PR}
\end{figure*}

\subsubsection{Line Descriptor Loss}
We utilize the triplet loss proposed in Facenet\cite{Triplet} to learn a line descriptor. Since the descriptors are regularized by  normalization, the cosine similarity of two descriptors can be represented as , where  are two descriptors. Given image pair  and their line segments set , let  be the -th line segment of image  and its corresponding descriptor,  be the descriptor of its matched line segment in image ,  be the descriptor of its unmatched line segment in image  with the maximal cosine similarity. Then the hard-negative triplet loss from image  to  can be represented as:
\begin{small}

\end{small}
where .  is the number of line segments in ,  is the margin that simultaneously enhances the  consistency of matched line segments and the discrepancy of unmatched line segments.
As mentioned in Section \ref{static_and_dynamic}, we have both static and dynamic line segments, so the overall loss of descriptor loss is:
\begin{small}

\end{small}
where  represent the dynamic and static descriptor loss according to Eq \eqref{triplet_loss}. We set  and  in this paper, where the  denotes the total epochs of entire training process and  denotes the current epoch. Briefly, we expect to rely more on static loss at the early training stage, and rely more on dynamic loss after the detector is well trained to adapt the descriptor to the actual detection result.
\section{Experiments}


\subsection{Experiment Setting}

\textbf{Implementation details:}
We use ResNet34\cite{ResNet} and optionally Hourglass Network\cite{Hourglass} and as the backbone, respectively. We conduct standard data augmentation for the training set, including horizontal/vertical flip and random rotate. Input images are resize to .  Our model is trained using ADAM\cite{Adam} optimizer with a total of 170 epochs on four NVIDIA RTX 2080Ti GPUs and an Inter Xeon Gold 6130 2.10 GHz CPU. The initial learning rate, weight decay, and batch size are set to , , and 16 respectively. The learning rate is divided by 10 at the 100th and 150th epoch.

\textbf{Datasets:}
We train and evaluate our model on Wireframe Dataset\cite{wireframe_cvpr18}, which contains 5000 images for training and 462 images for testing. We further evaluate on YorkUrban dataset\cite{York} with 102 test images from both indoor scenes and outdoor scenes to validate the generalization ability.

\textbf{Structural Average Precision Metric\cite{LCNN}:} The structural average precision (sAP) of the line segment is based on the L2-distance between the predicted end-points and the ground truths. The predicted line segments will be counted as 
True Positive (TP) if the distance is less than a certain threshold  and otherwise False Positives (FP). We set the threshold  and report the corresponding results, denote by sAP, sAP, sAP. For more details see \cite{LCNN}.



\textbf{Heatmap based Metric\cite{LCNN}:} Heatmap-based F-score and average precision,  and  are typical metrics used in wireframe parsing and line segment detection. We first convert the predicted line and ground truth line to two heatmaps by rasterizing the lines respectively. Then we can calculate the pixel-level precision and recall (PR) curves. Finally, we can compute  and  with the PR curves.

\subsection{Comparison Experiments on Line Detection}

We compare our proposed ELSD with line segment detection methods and wireframe parsing methods. Our model use ResNet34 as backbone and for a fair comparison with other methods, we also alter the backbone with Hourglass denote by Ours-HG. Ours-Lite is a faster version of our model. In Ours-Lite, we resize the input image to  and add a decoder in backbone. Therefore the outputs maps of each head is . Table \ref{CompareWithLSD} shows quantitative results based on sAP, , , and FPS of line segment detection. 

Ours-Res34 model achieves the best sAP on two datasets at a FPS of 42.6. It outperforms HAWP by 2.3\% and 1.8\% in msAP(mean of sAP) metric on Wireframe and YorkUrban respectively. Besides, when we replace the backbone with an Hourglass network(Ours-HG), it stills reaches a comparable sAP results on Wireframe. Since the HAWP and L-CNN are two stage methods, their inference speeds are limited. Moreover, their line segments rely on a pair of junctions, where junctions are usually local features that contain less global information. On the other hand, benefiting from more accurate midpoint detection and a more compact line representation method, our method is superior to TP-LSD. For further comparison, we evaluate the AP of mid-points similar to Junction AP proposed in L-CNN\cite{LCNN}. The mean AP of mid-point of ELSD is 2.9\% higher than TP-LSD, which means the mid-points are predicted more accurately in ELSD. 


In terms of the heatmap based metric, ELSD shows significant results in AP=87.2 on wireframe dataset and  achieves comparable results on F. Since our model predicts the line's angle, the angle prediction error of only one line segment could produce a lot of incorrect pixels, and but has the less influence to sAP. Therefore, the improvement of our model in pixel-based metric is not as obvious as that of sAP.

Our lightweight model can reach 107.5 FPS, which is 1.4  48.9 times faster than other learning-based methods while the accuracy drop is limited. We use Ours-Res34 as the representative model and depicted the precision and recall curves on both datasets in Figure \ref{PR}. Our ELSD outperforms other line segment detection methods especially in sAP metric on Wireframe dataset. Besides, ELSD achieved better generalization ability on YorkUrban dataset than other two-stage methods.



\subsection{Ablation Study for Line Detection}\label{ablation_sec}
We run ablation experiments on the Wireframe dataset, as reported in Table \ref{ablation study}.

\textbf{NCS:} NCS is to suppress the midpoints of fragmented line segments and remain the midpoints of entire segments. It improves the  from 0.680 to 0.689 according to No.3 and No.1 . 

\textbf{Descriptor:} The multi-task learning of detection and description leads to very small reduction on the detection accuracy  from 0.689 to 0.685, according to No.1 and No.2 . 

\textbf{Upsample:} For detecting line segments in real time, we use the shared feature map with 128 resolution, which is the same setting as L-CNN and HAWP. However, the prediction of the center in 128 resolution is much difficult than higher resolution. We solve this problem by upsampling the midpoint map, centerness map, geometrics maps and fine offset maps to 256 resolution. The  is thus improved from 0.658 to 0.689 according to No.4 and No.1. Since we only upsample once by bilinear interpolation or deconvolution, it has almost no extra cost on inference speed.

\textbf{Focal loss:} We use a variant focal loss instead of standard Binary Cross Entropy (BCE) loss for training the midpoint map. Since we treat the prediction of the midpoints as a binary classification problem, the focal loss that we used can have the ability to focus on the hard classified examples of midpoints. By introducing the focal loss, the  is improved from 0.660 to 0.689 according to No.5 and No.1 . 

\textbf{CAL:} The proposed CAL representation is compared to the Tri-points representation in TP-LSD. The  is improved from 0.679 to 0.689 according to No.6 and No.1 by replacing Tri-points with the CAL representation. This is because the Tri-points need to regress more parameters than the CAL representation (4 vs 2), and the angle is easier to learn than the displacements.

\begin{table}[]
\begin{center}
\scalebox{0.65}{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
No. & NCS & Upsample & Focal loss & CAL & Descriptor & sAP          & sAP         & sAP         \\ \hline
1   & \checkmark   & \checkmark        & \checkmark          & \checkmark     &            & \textbf{64.3} & \textbf{68.9} & \textbf{70.9} \\ \hline
2   & \checkmark   & \checkmark        & \checkmark          & \checkmark     & \checkmark          & 64.2          & 68.5          & 70.3          \\ \hline
3   &     & \checkmark        & \checkmark          & \checkmark     &            & 63.6          & 68.0          & 70.0          \\ \hline
4   & \checkmark   &          & \checkmark          & \checkmark     &            & 60.3          & 65.8          & 68.2          \\ \hline
5   & \checkmark   & \checkmark        &            & \checkmark     &            & 61.8          & 66.0          & 68.0          \\ \hline
6   & \checkmark   & \checkmark        & \checkmark          &       &            & 62.3          & 67.9          & 70.3          \\ \hline
7   &     &          &            &       &            & 58.0          & 62.8          & 64.8          \\ \hline
\end{tabular}
}
\end{center}
\caption{Ablation study of ELSD. See text for details.}
\label{ablation study}
\end{table}


\subsection{Comparison Experiments on Line Description}
To evaluate the line descriptor performance, we compare our method with LBD\cite{LBD} and LLD\cite{LLD}. The methods \cite{LJL,GNN} etc, are not involved to be compared, because they leverage the additional geometric characters of lines, other than local appearances. We test all of the algorithms on a subset of ScanNet dataset\cite{scannet} which is an RGB-D video dataset annotated with 3D camera poses. We select about 1000 image pairs with large viewpoint change, rotation change, and scale change for quantitative evaluation. We further compute the corresponding line descriptors of line segments detected by our model. We then obtain the ground truth line matches of the image pairs by checking if the reprojection error of corresponding lines less than a certain threshold.
We find the nearest neighbors to match across descriptors and perform cross-checking then we can get predicted corresponds of line segments.
We report the recall, precision and F-score to evaluate different descriptors. In our experiments, we use the OpenCV implementation of 72-dimensional LBD descriptors and the pre-trained model of LLD descriptors provided by the author. Meanwhile, our model is trained with setting the length of the descriptor to 256, 64 and 36 respectively.

The results are shown in Table \ref{desc_table}. Our descriptors outperform the LBD and LLD significantly, especially in Recall. 
The LBD descriptor is designed by the human priority that might not be the optimal solution. The LLD descriptor and the similar learning-based descriptors\cite{WLD,DLD} are trained with the line segments given by the line detectors such as Edlines\cite{edline}. However, there is a gap between those detected line segments and the annotated line segments in the datasets\cite{York,wireframe_cvpr18}.
In comparison, our descriptors cooperate well with our line detector since they share most of the parameters and representation, and their training is coupled, which can further reduce computation cost. The overall inference speed of ELSD (ResNet34 as backbone) with both line detector and line descriptor can achieve 38 FPS. Moreover, the 64-dimensional descriptor presents the same result as the 256-dimensional descriptor, and is better than the 36-dimensional descriptor in accuracy.
\begin{table}[]
\begin{center}
\scalebox{0.8}{
\begin{tabular}{lcccc}
\hline
\multicolumn{1}{c}{Methods} & Dimension & Precision(\%) & Recall(\%) & F-Score(\%) \\ \hline
LBD                         & 78        & 69.3          & 63.8       & 66.4        \\ \hline
LLD                         & 64        & 57.5          & 43.6       & 49.6        \\ \hline
WLD                         & 64        & 57.5          & 43.6       & 49.6        \\ \hline
Ours                        & 256       & 72.6          & \textbf{77.1}       & 74.7        \\
                            & 64        & \textbf{73.5}          & 76.2       & \textbf{74.8}        \\
                            & 36        & 72.2          & 75.3       & 73.7        \\ \hline
\end{tabular}
}
\end{center}
\caption{The precision, recall and F-Score for LBD, LLD, and Ours with different dimension.}
\label{desc_table}
\end{table}

\section{Conclusion}
This paper proposes a fast and accurate model ELSD that simultaneously detects line segments and their descriptors in a single forward pass, allowing share computation and representation in the two tasks. To detect line segments, We first utilize the Center-Angle-Length (CAL) representation to encode a line segment that fully exploits the geometric characters of lines. Furthermore, a centerness map is introduced to filter the false line segments by Non-Centerness-Suppression (NCS). Our proposed line detector achieves state-of-the-art performance on two benchmarks in both accuracy and efficiency. Moreover, our model also achieves real-time speed with a single GPU. The lite model can reach the high speed of 107.5 FPS while keeping a comparable performance, and thus is useful for many higher-level tasks such as SLAM and SfM that require high real-time performance.


\newpage
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
