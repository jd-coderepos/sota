

\documentclass[twoside, twocolumn]{article}

\usepackage[english]{babel}

\usepackage[sc]{mathpazo}

\usepackage[T1]{fontenc}
\linespread{1.05}

\usepackage{microtype}
\usepackage[margin=2cm, hmarginratio=1:1, top=32mm, columnsep=20pt]{geometry}
\usepackage[hang, small, labelfont=bf, up, textfont=it, up]{caption}
\usepackage{booktabs}
\usepackage{lettrine}

\usepackage{amsmath}

\usepackage{enumitem}
\setlist[itemize]{noitemsep}

\usepackage{abstract}
\renewcommand{\abstractnamefont}{\normalfont\bfseries}
\renewcommand{\abstracttextfont}{\normalfont\small\itshape}

\usepackage{titlesec}
\renewcommand\thesection{\Roman{section}}
\renewcommand\thesubsection{\roman{subsection}}
\titleformat{\section}[block]{\large\scshape\centering}{\thesection.}{1em}{}
\titleformat{\subsection}[block]{\large}{\thesubsection.}{1em}{}

\usepackage{fancyhdr}
\pagestyle{fancy}

\usepackage{titling}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{dblfloatfix}
\usepackage{floatrow}
\usepackage{multirow}
\floatsetup[table]{capposition=top}
\floatsetup[figure]{capposition=bottom}

\usepackage{graphicx}
\graphicspath{ {images/} }



\setlength{\droptitle}{-4\baselineskip}

\pretitle{\begin{center}\Huge\bfseries}
\posttitle{\end{center}}
\title{LIGAR: Lightweight General-purpose Action Recognition}
\author{
\textsc{Evgeny Izutov} \
\label{eq:segment_prob}
P_i=\frac{\sum_{k \in S_i} v_kI[v_k \geq 0] + 1}{\sum_t \sum_{k \in S_t} v_kI[v_k \geq 0] + 1}

\label{eq:score_update}
v_{k}^{new} = v_{k}^{old} + (I[\overline{y_i} = y_i] - I[\overline{y_i} \neq y_i])P_i, k \in S_i

\label{eq:lr_schedule}
\eta_t=\eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})(1 + cos\left(\left(\frac{T_{cur}}{T_{max}}\right)^{\alpha} \pi \right)


\noindent where  and  are the lower and upper bounds respectively for the learning rate.
 represents the current iteration and  is maximum number of iterations. The parameter 
allows us to control the fraction of high learning rate phase -- in our experiments we have fixed it to the  value.

Furthermore, it was found that training with the recently proposed Adaptive Gradient Clipping (AGC)~\cite{AGC} allows us
to not only speed up training but increase the model accuracy too. In all experiments below we set it enabled for our
network. The final network is trained on two GPUs by 12 clips per node with SGD optimizer and weight decay
regularization using the PyTorch framework~\cite{PyTorch}.



\begin{table*}[t]
\caption{The pivot table of different frame sampling strategies and test protocols. For each combination the Top-1
accuracy is reported.}
\label{table:ablation_samplers}
\centering
\begin{tabular}{l|cc|cc|cc}
\multirow{2}{*}{\textbf{Sampler}}& \multicolumn{2}{c|}{\textbf{UCF-101}}     & \multicolumn{2}{c|}{\textbf{ActivityNet-200}} & \multicolumn{2}{c}{\textbf{Jester-27}}    \\
                                 & \textbf{1 segment} & \textbf{10 segments} & \textbf{1 segment}   & \textbf{10 segments}   & \textbf{1 segment} & \textbf{10 segments} \\ \hline
sparse                           & 93.79              & 94.71                & 59.94                & 74.32                  & 95.25              & 95.68                \\
continuous                       & 93.63              & 94.85                & 64.19                & 75.27                  & 95.43              & 95.56
\end{tabular}
\end{table*}

\section{Experimental Results}

\subsection{Data}

\lettrine[nindent=0em,lines=3]{W}{}e conduct experiments on several commonly-used benchmarks for general action and
gesture recognition. The listed below datasets allow us to validate the proposed architecture in different scenarios:
trimmed and untrimmed general purpose action recognition and continuous hand gesture recognition. As it was described
early the first category checks the ability to learn the appearance-based action recognition while the last one -- the
ability to model the motion component of actions.

\begin{itemize}
\item \textbf{UCF-101}~\cite{UCF101} is a middle-size general-purpose action recognition dataset of trimmed action
videos, splitted on the 101 action categories. The total number of samples is 13320 and there are three train-val splits
provided. As many other researchers we report the results on the first split only to reduce the training time.
\item \textbf{ActivityNet-200}~\cite{ActivityNet} is a middle-size general-purpose action recognition dataset. Unlike
the previous dataset the source videos are untrimmed and split on 200 action categories. Note that the dataset is
designed to solve the temporal action detection/segmentation task mostly but we adopt it for action recognition purposes 
(see the next section for more details). The total number of available for downloading video instances is 17196.
\item \textbf{Jester-27}~\cite{Jester} is large-size dataset of labeled video clips that shows humans performing
pre-defined hand gestures. The dataset consists of 148092 unique video clips splitted on the 27 gesture categories.
\end{itemize}

\subsection{Evaluation Protocol}

\lettrine[nindent=0em,lines=3]{T}{}he main protocol to measure the performance of action recognition algorithms consists
of top-1 and top-5 accuracies. The last metric is used to flatten the perturbations in case of noisy labels in the
annotation. However as it was mentioned in the paper~\cite{ASLNet} the transition to ML-based heads allows us to measure
more noise resistant metric, like rank mAP (for more details see the reported above paper). In this paper we measure it
too.

Another question is related to the choice of a procedure for the clip selection from an input video during the testing
phase. Commonly used approach is to split the input video onto 10 temporal segments and apply a AR network for each
segment independently. Sometimes AR network is run for several crops inside each temporal segment. For the end user it
means that the reported performance metrics in terms of GFlops should be multiplied onto 30 (10 segments  3
crops). The mentioned method is suitable for the academic community (to show the best quality) but not for a business.
To report the fair performance-accuracy pairs we follow the reduced protocol -- single central spatio-temporal crop for
each input video (for comparison purposes we report the metrics for both protocols in the section~\ref{ablation_study}).
Note, the ActivityNet data is untrimmed, so the single temporal crop may not reflect the target class due to temporal
mismatch with ground-truth (see the section~\ref{noise_suppression}). So, the 10-temporal-crop protocol is considered as
a primary one for the ActivityNet benchmark.

One more possible difference in measurements is related to the frame sampling procedure. For the general-purpose
datasets it is reasonable to sample frames inside a segment uniformly to achieve the best temporal coverage.
Unfortantely, the discussed method is not suitable for the real-time applications (e.g. hand gesture recognition) which
process frames on-the-fly and does not have an access to the future frames. In the paper~\cite{ASLNet} the continues
protocol has been proposed -- sampling the frames during the train and test phases with a fixed frame-rate. In the paper
we follow the same continues protocol (the comparison between protocols can be found in the~\ref{ablation_study}
section).

\begin{table}[h]
\caption{The ablation study of the backbone initialization on the UCF-101 dataset.}
\label{table:ablation_init}
\centering
\begin{tabular}{l|ccc}
\textbf{Method}           & \textbf{Top-1} & \textbf{Top-5} & \textbf{mAP} \\ \hline
from scratch              & 64.05          & 87.97          & 57.33        \\
ImageNet (2D init)        & 71.90          & 90.67          & 77.74        \\
Kinetics (3D init)        & 92.81          & 98.76          & 96.88        \\
Kinetics+Ytb8M (3D init)  & 93.63          & 99.10          & 97.82   
\end{tabular}
\end{table}

\subsection{Ablation study} \label{ablation_study}

\lettrine[nindent=0em,lines=3]{B}{}elow we present the ablation study of the proposed framework in terms of the frame
sampling strategy (comparison between sparse and continues protocols during the inference), the pre-training method and
the influence of the introduced ACS module.

\begin{table*}[h]
\caption{The comparison of using ACS method on UCF-101 and ActivityNet-200 datasets. Note the metrics on the ActivityNet
         dataset is specified for the testing protocol with 10 temporal segments.}
\label{table:ablation_acs}
\centering
\begin{tabular}{l|ccc|ccc}
\multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Method}}} & \multicolumn{3}{c|}{\textbf{UCF-101}}               & \multicolumn{3}{c}{\textbf{ActivityNet-200}} \\
\multicolumn{1}{c|}{}                                 & \textbf{Top-1} & \textbf{Top-5} & \textbf{mAP} & \textbf{Top-1} & \textbf{Top-5} & \textbf{mAP} \\ \hline
w/o ACS                                               & 93.52          & 99.07          & 97.50        & 74.89          & 92.77          & 73.33        \\
w/ ACS                                                & 93.63          & 99.10          & 97.82        & 75.27          & 92.70          & 73.81            
\end{tabular}
\end{table*}

\textbf{Frame sampling strategy.} First, we would like to compare the frame sampling strategies. Depending on the target
use case there are two possible solutions described early: sparse sampling and fixed frame rate sampling (or just with
fixed temporal stride). The first strategy assumes the low impact of an ratio between a video and the network input
lengths but is not suitable for the live demo mode due to inability to see the future frames. Recently the sparse
sampling method~\cite{MoViNet} allowed the authors to show the magnificent performance with single temporal crop. Our
experiments (see the Table~\ref{table:ablation_samplers}) demonstrate that two-path network with separated global
context branch is able to reduce the impact of an sampling strategy. Moreover we see the improvement in case of single
crop for the ActivityNet dataset. In our opinion it is because the target clip with a valid action is significantly
shorter than the full video and as a result the sparse sampling strategy collects a scarce number of valid frames.
Generally speaking, the LIGAR framework allows us to close the question of the frame sampling strategy in case of live demo
applications.

Additionally, we have compared here the difference between testing protocols, especially the single- or multi-view video
prediction. As it is expected increasing the number of views per an video improves the accuracy metric due to smoothing
the impact of possible temporal mismatch between the unknown ground truth and the tested central temporal crop.
Unfortunately most of papers report the multi-view metrics to get the best results and it is not suitable for the live
demo scenario. Regarding the reported results the difference between the two testing protocols is comfortable with the
exception of ActivityNet dataset (the reason of that is described early).

\begin{table*}[ht]
\caption{Comparison the proposed LIGAR framework with SOTA solutions on UCF-101 and Jester-27 datasets. For fairness we
report the number of views for each measure if it is specified.}
\label{table:results}
\centering
\begin{tabular}{l|c|c|c|c|ccc|ccc}
\multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Name}}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Input\\ frames\end{tabular}}} & \multirow{2}{*}{\textbf{Views}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Single\\ GFLOPs\end{tabular}}} & \multirow{2}{*}{\textbf{MParams}} & \multicolumn{3}{c|}{\textbf{UCF-101}} & \multicolumn{3}{c}{\textbf{Jester-27}} \\
\multicolumn{1}{c|}{} &  &  &  &  & \textbf{Top-1} & \textbf{Top-5} & \textbf{mAP} & \textbf{Top1} & \textbf{Top-5} & \textbf{mAP} \\ \hline
R(2+1)D-BERT~\cite{Bert} & 64 & - & 152.97 & 66.67 & 98.69 & - & - & - & - & - \\
LGD-3D RGB~\cite{LGD} & 16 & 15 & - & - & 97.00 & - & - & - & - & - \\
PAN ResNet101~\cite{PAN} & 32 & 2 & 251.7 & - & - & - & - & 97.40 & 99.90 & - \\
STM~\cite{STM} & 16 & 10 & 66.5 & 22.4 & 96.20 & - & - & 96.70 & 99.90 & - \\
3D-MobileNetV2 1.0x~\cite{3DMob} & 16 & 10 & 0.45 & 3.12 & 81.60 & - & - & 94.59 & - & - \\ \hline
\multirow{2}{*}{LIGAR (our)} & \multirow{2}{*}{16} & 1 & \multirow{2}{*}{4.74} & \multirow{2}{*}{4.47} & 93.63 & 99.10 & 97.82 & 95.43 & 99.45 & 96.95 \\
 &  & 10 &  &  & 94.85 & 99.50 & 98.61 & 95.56 & 99.52 & 97.33
\end{tabular}
\end{table*}

\textbf{Network initialization.} Another important question is about the network initialization and noteworthiness of a
pre-training stage. Originally, pre-training is designed to transfer the knowledge from the big datasets to the small
one thereby reducing a harmful effect of over-fitting. For the 3D-based networks the initialization have two sources: 2D
initialization of a backbone only before the inflating procedure (see the I3D~\cite{I3D} paper for more details) and
direct 3D initialization of a full network. In the Table~\ref{table:ablation_init} we have summarized the potential
strategies of the network initialization. Note, the last line is different from the previous one by enabling the
mentioned early multi-head pre-training on the merged dataset. Overall, the model behavior reflects the intuition
behind it -- the method with more data during the pre-training stage exceeds the previous one in terms of all measured
metrics.

\textbf{Induced noise suppression.} The last question is related to the suppression of the described early induced label
noise. The main benchmark to measure the importance of the proposed Adaptive Clip Selection (ACS) module is ActivityNet
dataset. As it was mentioned before we do not follow the original ActivityNet protocol and measure the action
recognition metrics instead of localization one. Unfortunately we cannot measure the real performance of the model on
this dataset due to the lack of an accurate temporal annotation of target actions and it is expected that the real
quality is higher than it is announced. Nevertheless, we can see in the Table~\ref{table:ablation_acs} the improvement
over the baseline by using the proposed ACS module. Furthermore, the improvement is observed for the initially clean
dataset like UCF-101. In our opinion, it is because the impact of induced label noise is much stronger than it is
commonly believed even for the clean popular datasets.

\subsection{Comparison with the State-of-the-Arts}

\lettrine[nindent=0em,lines=3]{W}{}e further demonstrate the advances of our proposed LIGAR framework in comparison with
state-of-the-art methods for the general-purpose action recognition. For fair comparison all methods use the RGB
modality as an input. We report both results for possible testing protocols -- 1 and 10 temporal crops (spatial crops do
not benefit in our experiments). In the Table~\ref{table:results} we have collected the best solutions in term of
accuracy on two sufficiently different datasets.

\textbf{Results on UCF-101.} We first verify the appearance modeling ability on UCF-101. The model can achieve the
superior results compared with other methods which are computationally more expensive. For example the BERT-like
solution~\cite{Bert} is  times more expensive in term of GFLOPs but the accuracy drop is only  of top-1
metric. In case of equal testing protocol (10 views per video) the drop is slightly less. In comparison to the solution
with a similar computation budget~\cite{3DMob} the accuracy of the reported approach is significantly better.

\textbf{Results on Jester-27.} We also compare with other methods on Jester-27 to verify the model ability to make the
prediction according to the motion component. Comparing with results on general dataset the gap between heavy solutions
and the proposed one is even less. Specifically, we can observe that our proposed method drops less than 2 percentage in
comparison to the SOTA solution~\cite{PAN}. Like for the previous dataset the advantage in terms of a computation budget
even more than 53 times.



\section{Conclusion}

\lettrine[nindent=0em,lines=3]{T}{}his paper has presented the extension to the LGD framework for more efficient and
accurate solving an action recognition problem for a wide range of applications, like general (appearance-based) and
gesture (motion-based) recognition problems. Moreover, the paper proposed a novel clips selection module to tackle
the induced label noise issues. The described training pipeline allows us to train a robust DL-based solution which is able
to solve most of real-world action recognition problems in a fast and accurate manner. The reported results give us the
hope that DL-based solutions will continue advance on the rest vital challenges of a humanity.



\bibliographystyle{splncs}
\bibliography{egbib}



\end{document}
