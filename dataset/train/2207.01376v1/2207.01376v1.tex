\section{Experiments}
\label{sec:exp}
In this section, we evaluate TDM on standard fine-grained classification benchmarks. To verify high adaptability of TDM, we apply it to various existing methods: ProtoNet\cite{snell2017prototypical},  DSN\cite{simon2020adaptive}, CTX\cite{doersch2020crosstransformers}, and FRN\cite{wertheimer2021few}.
For a fair comparison, we reproduce each baseline model with the same hyperparameter and implementation details regardless of whether TDM is attached or not. 
In each table, $\dagger$ indicates the reproduced version of the baseline model. 
While TDM generally exploits the prototype\cite{snell2017prototypical} defined in \cref{prototype} for computing the intra and inter score, in CTX, it utilizes a query-aligned prototype\cite{doersch2020crosstransformers}.  

\subsection{Datasets}
\begin{table}[t]
    \centering
    {\small
		\begin{tabular}{l | c c c c}
		    \hlineB{2.5}
		    \multicolumn{1}{l}{\textbf{Dataset}} & \textbf{$C_{all}$} & \textbf{$C_{train}$} & \textbf{$C_{val}$} & \textbf{$C_{test}$} \\
		    \hlineB{2.5}
            \multicolumn{1}{l}{CUB-200-2011} & 200 & 100 & 50 & 50 \\
            \multicolumn{1}{l}{Aircraft} & 100 & 50 & 25 & 25 \\
            \multicolumn{1}{l}{meta-iNat} & 1135 & 908 & - & 227 \\
            \multicolumn{1}{l}{tiered meta-iNat} & 1135 & 781 & - & 354 \\
            \multicolumn{1}{l}{Stanford-Cars} & 196 & 130 & 17 & 49 \\
            \multicolumn{1}{l}{Stanford-Dogs} & 120 & 70 & 20 & 30 \\
            \multicolumn{1}{l}{Oxford-Pets} & 37 & 20 & 7 & 10 \\
            \hlineB{2.5}
		\end{tabular}
	}
    \vspace{-0.1cm}
	\caption{
	The splits of datasets. While $C_{all}$ is the number of total classes, $C_{train}$, $C_{val}$, $C_{test}$ are the number of training, validation, and test classes, respectively.
	The classes of subsets are disjoint.
	}
	\label{dataset_split}
	\vspace{-0.4cm}
\end{table}
We utilize seven benchmarks for few-shot classification: CUB-200-2011, Aircraft, meta-iNat, tiered meta-iNat, Stanford-Cars, Stanford-Dogs, and Oxford-Pets. 
The data split for each dataset is provided in \cref{dataset_split}.

\noindent\textbf{CUB-200-2011}\cite{wah2011caltech} is an image dataset with 11,788 photos of 200 bird species. 
This benchmark can be used in two ways: raw form\cite{chen2019closer} or preprocessed form by a human-annotated bounding box\cite{ye2020few, zhang2020deepemd}. 
For a fair comparison, we utilizes both settings. 
Following \cite{chen2019closer}, we split this benchmark and our split is the same with \cite{wertheimer2021few}.

\noindent\textbf {Aircraft}\cite{maji2013fine} contains 10,000 airplane images of 100 models. 
The main challenge of this benchmark is the similarity by the symbol of the airline. 
Although the types of aircraft are different, the symbol can be equivalent when they belong to the same airline, making it more difficult. 
Following \cite{wertheimer2021few}, we preprocess all images of this benchmark based on the bounding box and divide the dataset.

\noindent\textbf{meta-iNat}\cite{wertheimer2019few, van2018inaturalist} is a realistic, heavy-tailed benchmark for few-shot classification. 
It contains 1,135 animal species spanning 13 super categories, and the number of images in each class is imbalanced with a range between 50 and 1000 images.
We follow the dataset split introduced in \cite{wertheimer2019few}. 
While a full 227-way evaluation scheme, that each episode consists of all testing categories at once, is adopted in \cite{wertheimer2019few}, we employ a standard 5-way few-shot evaluation scheme following \cite{wertheimer2021few}.

\noindent\textbf{tiered meta-iNat}\cite{wertheimer2019few} is comprised of the same images with meta-iNat.
However, this benchmark divides the dataset by super categories.
The super categories of test dataset are Insects and Arachnids, while the super categories of train dataset are Plant, Bird, Mammal, and etc. 
Therefore, a large domain gap exists between the train and the test classes. 
As following \cite{wertheimer2021few}, we also employ a standard 5-way few-shot evaluation scheme in this benchmark.


\noindent\textbf{Stanford Cars}\cite{krause20133d} contains 16,185 images of 196 classes of cars. 
Classes are typically at the level of Year, Brand and Model name, e.g., 2012 Tesla Model S and 2012 BMW M3 coupe. 
This dataset was introduced by \cite{li2019revisiting} for the few-shot classification.
Likewise, we adopt the same data split with \cite{li2019revisiting}. 


\noindent\textbf{Stanford Dogs}\cite{khosla2011novel} is also introduced by \cite{li2019revisiting} for fine-grained few-shot classification. This dataset contains 20,580 images of 120 breeds of dogs around the world. Our split is the same with \cite{li2019revisiting}.

\noindent\textbf{Oxford Pets}\cite{parkhi2012cats} is comprised of 37 pet categories with roughly 200 images for each class. 
Since the lack of training images generally causes overfitting, the generalization capability is essential to attain high accuracy in this benchmark.
To our knowledge, this benchmark dataset has never been used for few-shot classification. 
Therefore, we randomly divide this dataset by referring to the split ratio of the other datasets.
We report the split information in the supplementary material.

\begin{table}[t]
    \centering
    {\small
		\begin{tabular}{l | c c c c}
		    \hlineB{2.5}
		    \multicolumn{1}{l}{\multirow{2}{*}{\textbf{Model}}} & \multicolumn{2}{c}{\textbf{Conv-4}} & \multicolumn{2}{c}{\textbf{ResNet-12}} \\
		    \multicolumn{1}{c}{}& \textbf{1-shot} & \textbf{5-shot} & \textbf{1-shot} & \textbf{5-shot} \\
		    \hlineB{2.5}
		    \multicolumn{1}{l}{MatchNet\cite{vinyals2016matching, ye2020few, zhang2020deepemd}} & 67.73 & 79.00 & 71.87 & 85.08 \\
		    \multicolumn{1}{l}{ProtoNet\cite{snell2017prototypical, ye2020few, zhang2020deepemd}} & 63.73 & 81.50 & 66.09 & 82.50 \\
		    \multicolumn{1}{l}{FEAT{$^{\ast}$}\cite{ye2020few}} & 68.87 & 82.90 & 73.27 & 85.77 \\
		    \multicolumn{1}{l}{DeepEMD\cite{zhang2020deepemd}} & - & - & 75.65 & 88.69 \\
		    \multicolumn{1}{l}{RENet\cite{kang2021relational}} & - & - & 79.49 & 91.11 \\
		    \hlineB{1.0}
            \multicolumn{1}{l}{ProtoNet{$^{\dagger}$}\cite{snell2017prototypical}} & 62.90 & 84.13 & 78.99 & 90.74 \\
            \multicolumn{1}{l}{~~~+ TDM} & 69.94 & 86.96 & 79.58 & 91.28 \\
            \hlineB{1.}
            \multicolumn{1}{l}{DSN{$^{\dagger}$}\cite{simon2020adaptive}} & 72.09 & 85.03 & 80.51 & 90.23 \\
            \multicolumn{1}{l}{~~~+ TDM} & 73.38 & 86.07 & 81.33 & 90.65 \\
                        \hlineB{1.}
            \multicolumn{1}{l}{CTX{$^{\dagger}$}\cite{doersch2020crosstransformers}} & 72.14 & 87.23 & 80.67 & 91.55 \\
            \multicolumn{1}{l}{~~~+ TDM} & \textbf{74.68} & 88.36 & 83.28 & 92.74 \\
                        \hlineB{1.}
            \multicolumn{1}{l}{FRN{$^{\dagger}$}\cite{wertheimer2021few}} & 73.24 & 88.33 & 83.16 & 92.42 \\
            \multicolumn{1}{l}{~~~+ TDM} & 74.39 & \textbf{88.89} & \textbf{83.36} & \textbf{92.80} \\
            \hlineB{2.5}
		\end{tabular}
	}
	\vspace{-0.1cm}
	\caption{Performance on CUB using bounding-box cropped images as input. ``$\ast$" denotes reproduced one in RENet. Confidence intervals for our implemented model are all below 0.23.}
	\label{CUB_cropped}
	\vspace{-0.4cm}
\end{table}
\subsection{Implementation Details}
\begingroup
\setlength{\tabcolsep}{4pt} \renewcommand{\arraystretch}{1.0} \begin{table}[t]
    \centering
    {\small
		\begin{tabular}{l | c c c}
		    \hlineB{2.5}
		    \multicolumn{1}{l}{\textbf{Model}} & \textbf{Backbone} & \textbf{1-shot} & \textbf{5-shot} \\
		    \hlineB{2.5}
            \multicolumn{1}{l}{Baseline{$^{\flat}$}\cite{chen2019closer}} & ResNet-18 & 65.51$\pm$0.87 & 82.85$\pm$0.55 \\
            \multicolumn{1}{l}{Baseline++{$^{\flat}$}\cite{chen2019closer}} & ResNet-18 & 67.02$\pm$0.90 & 83.58$\pm$0.54 \\
            \multicolumn{1}{l}{MatchNet{$^{\flat}$}\cite{chen2019closer, vinyals2016matching}} & ResNet-18 & 73.42$\pm$0.89 & 84.45$\pm$0.58 \\
            \multicolumn{1}{l}{ProtoNet{$^{\flat}$}\cite{chen2019closer, snell2017prototypical}} & ResNet-18 & 72.99$\pm$0.88 & 86.65$\pm$0.51 \\
            \multicolumn{1}{l}{MAML{$^{\flat}$}\cite{chen2019closer, finn2017model}} & ResNet-18 & 68.42$\pm$1.07 & 83.47$\pm$0.62 \\
            \multicolumn{1}{l}{RelatioNet{$^{\flat}$}\cite{chen2019closer, sung2018learning}} & ResNet-18 & 68.58$\pm$0.94 & 84.05$\pm$0.56 \\
            \multicolumn{1}{l}{S2M2{$^{\flat}$}\cite{mangla2020charting}} & ResNet-18 & 71.43$\pm$0.28 & 85.55$\pm$0.52 \\
            \multicolumn{1}{l}{Neg-Cosine{$^{\flat}$}\cite{liu2020negative}} & ResNet-18 & 72.66$\pm$0.85 & 89.40$\pm$0.43 \\
            \multicolumn{1}{l}{Afrasiyabi \textit{et al.}{$^{\flat}$}\cite{afrasiyabi2020associative}} & ResNet-18 & 74.22$\pm$1.09 & 88.65$\pm$0.55 \\
            \hlineB{1.}
            \multicolumn{1}{l}{ProtoNet{$^{\dagger}$}\cite{snell2017prototypical}} & ResNet-12 & 78.58$\pm$0.22 & 89.83$\pm$0.12 \\
            \multicolumn{1}{l}{~~~+ TDM} & ResNet-12 & 79.11$\pm$0.22 & 90.83$\pm$0.11\\
            \hlineB{1.}
            \multicolumn{1}{l}{DSN{$^{\dagger}$}\cite{simon2020adaptive}} & ResNet-12 & 80.47$\pm$0.20 & 89.92$\pm$0.12 \\
            \multicolumn{1}{l}{~~~+ TDM} & ResNet-12 & 80.58$\pm$0.20 & 89.95$\pm$0.12\\
            \hlineB{1.}
            \multicolumn{1}{l}{CTX{$^{\dagger}$}\cite{doersch2020crosstransformers}} & ResNet-12 & 80.95$\pm$0.21 & 91.54$\pm$0.11\\
            \multicolumn{1}{l}{~~~+ TDM} & ResNet-12 & 83.45$\pm$0.19 & 92.49$\pm$0.11\\
            \hlineB{1.}
            \multicolumn{1}{l}{FRN{$^{\dagger}$}\cite{wertheimer2021few}} & ResNet-12 & 83.54$\pm$0.19 & 92.96$\pm$0.10\\
            \multicolumn{1}{l}{~~~+ TDM} & ResNet-12 & \textbf{84.36$\pm$0.19} & \textbf{93.37$\pm$0.10}\\
            \hlineB{2.5}
		\end{tabular}
	}
	\vspace{-0.1cm}
	\caption{Performance on CUB using raw images as input. ``$\flat$" denotes larger backbones than \textit{ResNet-12}.}
	\label{CUB_raw}
	\vspace{-0.4cm}
\end{table}
\endgroup
\noindent\textbf{Architecture.} 
We adopt common protocols from recent few-shot classification works\cite{kim2019edge, chen2021pareto, zhao2021looking, hong2021reinforced, zhang2021rethinking}; we employ Conv-4 and ResNet-12.
While both backbone networks accept an image of size 84$\times$84, the size of feature maps is different according to the backbone network.
ResNet-12 yields a feature map with size 640$\times$5$\times$5 while Conv-4 offers 64$\times$5$\times$5 shape of the feature map.
For our proposed TDM, we additionally utilize fully-connected layer blocks where the size of blocks are proportional to the dimension of channels of the feature maps as described in \cref{fully_connectec_block}. 
\newline
\textbf{Training Details.} 
Following existing methods\cite{chen2019closer, wang2019simpleshot, ye2020few, zhang2020deepemd, wertheimer2021few}, we use standard data augmentation techniques including random crop, horizontal flip, and color jitter. 
The $\alpha, \beta$ in \cref{support_weight_vector}, \cref{task_weight_vector} are fixed to 0.5 and other parameters are adopted from \cite{wertheimer2021few} which is our baseline model. 
More details are described in the supplementary material.
To prevent overfitting, we add random noise between -0.2 and 0.2 to the task weight for each category from TDM.
\newline
\textbf{Evaluation Details.} 
For the N-way K-shot, we conduct few-shot classification on 10,000 randomly sampled episodes where it contains 16 queries per class. 
We report average classification accuracy with 95\% confidence intervals.
\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figure/add_dataset_1.pdf}
        \caption{Stanford Cars}
        \vspace{-0.3cm}
        \label{fig:y equals x}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figure/add_dataset_2.pdf}
        \caption{Stanford Dogs}
        \vspace{-0.3cm}
        \label{fig:three sin x}
    \end{subfigure}
    \hfill
        \begin{subfigure}[b]{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figure/add_dataset_3.pdf}
        \caption{Oxford Pets}
        \vspace{-0.3cm}
        \label{fig:three sin x}
    \end{subfigure}
    \caption{
    Accuracies on additional datasets. 
    The left and right graphs for each dataset show 1-shot accuracies and 5-shot accuracies, respectively.
    The left side of the graph reports the performance of baseline models, while the right side shows performance with TDM. The baseline methods are differentiated with colors.
    }
    \label{fig:additional_dataset}
    \vspace{-0.4cm}
\end{figure*}
%
 
\subsection{Comparison to Existing Methods}
\noindent\textbf{CUB-200-2011 results.} \cref{CUB_cropped} and \cref{CUB_raw} reports the results of TDM and baseline few-shot classification methods. 
Although some cases do not surpass confidence intervals, our TDM consistently improves the performance of baselines models in all cases, and achieves state-of-the-art scores regardless the depth of the backbone network.
Especially, TDM improves more than 7\% on ProtoNet in the 1-shot scenario of CUB with Conv-4. 
\newline
\textbf {Aircraft results.} 
As shown in \cref{aircraft}, TDM improves the performances of the baseline models in all cases. 
The increases are beyond the confidence intervals regardless of the type of baseline models and the number of labeled images. 
As a result, we attain top accuracy scores for all benchmarks by large margin. Specifically, TDM boosts the performance of CTX\cite{doersch2020crosstransformers} up to 7\% on the Conv-4 network.
These results demonstrates the effectiveness of TDM.
\newline
\textbf{meta-iNat results.} 
We validate the generalization ability of TDM in this benchmark. 
Meta-iNat is vulnerable to overfitting since the validation set does not exist.
However, as shown in \cref{inat}, TDM is not only robust to overfitting issues but also powerful in generalization capability.
Consequently, TDM shows consistent improvements over the baselines, particularly on ProtoNet where TDM assists them to retain competitive results compared to state-of-the-art methods.
\newline
\textbf{tiered meta-iNat results.} 
We further validate the generalization capability of TDM in a more difficult configuration where the super categories of train set and test set do not overlap. 
Specifically, TDM enhances the performance in most configurations and also accomplishes the best performance in the 1-shot scenario with FRN. 
For the slight decrease in a 5-shot scenario, $\lambda$, the learnable parameter in FRN, is responsible. 
In general, large $\lambda$ shows good performance when a domain gap exists but TDM restrains the $\lambda$ to be relatively small.
We think that this is because TDM assists the classifier to focus on discriminative channels\cite{wertheimer2021few}.
\newline
\textbf{Stanford Cars, Stanford Dogs and Oxford Pets results.} 
Unlike previous benchmarks, these datasets were not evaluated in our baseline models\cite{snell2017prototypical, simon2020adaptive, doersch2020crosstransformers ,wertheimer2021few}. 
To further validate the effectiveness of TDM, we additional conduct experiments on those fine-grained datasets with Conv-4.
As reported in \cref{fig:additional_dataset}, TDM is capable to improve the performance that the confidence interval does not overlap in all cases regardless of the baseline models.
In detail, TDM shows performance boosts in which their accuracies are 4.44 and 3.27 points higher than the baseline at the 1-shot and 5-shot scenarios, respectively.

Throughout the extensive experiments on seven benchmark datasets, we clearly validate the strength of TDM in fine-grained few-shot classification. 
To summarize, we improve the performance of the baseline models in all benchmarks datasets, achieving the state-of-the-art results except the one case: FRN on the tiered meta-iNat 5-shot scenario.

\begingroup
\setlength{\tabcolsep}{6pt} \renewcommand{\arraystretch}{1.0} \begin{table}[t]
    \centering
    {\small
		\begin{tabular}{l | c c c c}
		    \hlineB{2.5}
		    \multicolumn{1}{l}{\multirow{2}{*}{\textbf{Model}}} & \multicolumn{2}{c}{\textbf{Conv-4}} & \multicolumn{2}{c}{\textbf{ResNet-12}} \\
		    \multicolumn{1}{c}{} & \textbf{1-shot} & \textbf{5-shot} & \textbf{1-shot} & \textbf{5-shot} \\
		    \hlineB{2.5}
            \multicolumn{1}{l}{ProtoNet{$^{\dagger}$}\cite{snell2017prototypical}} & 47.37 & 68.96 & 67.28 & 83.21 \\
            \multicolumn{1}{l}{~~~+ TDM} & 50.55 & 71.12 & 69.12 & \textbf{84.77} \\
            \hlineB{1.}
            \multicolumn{1}{l}{DSN{$^{\dagger}$}\cite{simon2020adaptive}} & 52.22 & 68.75 & 70.23 & 
            83.05 \\
            \multicolumn{1}{l}{~~~+ TDM} & 53.77 & 69.56 & \textbf{71.57} & 83.65 \\
            \hlineB{1.}
            \multicolumn{1}{l}{CTX{$^{\dagger}$}\cite{doersch2020crosstransformers}} & 51.58 & 68.12 & 65.53 & 79.31 \\
            \multicolumn{1}{l}{~~~+ TDM} & \textbf{55.15} & 70.45 & 69.42 & 83.25 \\
            \hlineB{1.}
            \multicolumn{1}{l}{FRN{$^{\dagger}$}\cite{wertheimer2021few}} & 53.12 & 70.84 & 69.58 & 
            82.98 \\
            \multicolumn{1}{l}{~~~+ TDM} & 54.21 & \textbf{71.37} & 70.89 & 84.54 \\
            \hlineB{2.5}
		\end{tabular}
	}
	\vspace{-0.1cm}
	\caption{Performance on Aircraft. Confidence intervals for our implemented model are all below 0.25.}
	\label{aircraft}
	\vspace{-0.4cm}
\end{table}
\endgroup

\begingroup
\setlength{\tabcolsep}{6pt} \renewcommand{\arraystretch}{1.0} \begin{table}[t]
    \centering
    {\small
		\begin{tabular}{l | c c c c}
		    \hlineB{2.5}
		    \multicolumn{1}{l}{\multirow{2}{*}{\textbf{Model}}} & \multicolumn{2}{c}{\textbf{meta-iNat}} & \multicolumn{2}{c}{\textbf{tiered meta-iNat}} \\
		    \multicolumn{1}{l}{} & \textbf{1-shot} & \textbf{5-shot} & \textbf{1-shot} & \textbf{5-shot} \\
		    \hlineB{2.5}
            \multicolumn{1}{l}{ProtoNet{$^{\dagger}$}\cite{snell2017prototypical}} & 55.37 & 76.30 & 34.41 & 57.60 \\
            \multicolumn{1}{l}{~~~+ TDM} & 61.82 & 79.95 & 38.30 & 61.18 \\
            \hlineB{1.}
            \multicolumn{1}{l}{DSN{$^{\dagger}$}\cite{simon2020adaptive}} & 60.06 & 76.15 & 40.83 & 58.34 \\
            \multicolumn{1}{l}{~~~+ TDM} & 61.87 & 78.07 & 41.00 & 58.66 \\
            \hlineB{1.}
            \multicolumn{1}{l}{CTX{$^{\dagger}$}\cite{doersch2020crosstransformers}} & 60.80 & 78.57 & 42.24 & 60.54\\
            \multicolumn{1}{l}{~~~+ TDM} & 63.26 & 80.75 & 43.90 & 62.29 \\
            \hlineB{1.}
            \multicolumn{1}{l}{FRN{$^{\dagger}$}\cite{wertheimer2021few}} & 61.98 & 80.04 & 43.95 & \textbf{63.45} \\
            \multicolumn{1}{l}{~~~+ TDM} & \textbf{63.97} & \textbf{81.60} & \textbf{44.05} & 62.91 \\
            \hlineB{2.5}
		\end{tabular}
	}
	\vspace{-0.1cm}
	\caption{
	Performance on meta-iNat and tiered meta-iNat using Conv-4 backbones.
	Confidence intervals for our implemented model are all below 0.23.
	}
	\label{inat}
	\vspace{-0.1cm}
\end{table}
\endgroup
