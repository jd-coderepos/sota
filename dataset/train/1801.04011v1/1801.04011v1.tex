\section{Experiments}
\label{sec:experiments}


\subsection{Datasets}
We used several subsets of Imagenet~\cite{deng2009imagenet} for training and evaluation of our methods. We also evaluate a 
frequency- and spatial-domain diver-tracking algorithm on a video of scuba divers taken from YouTube\texttrademark\ 
\footnote{https://www.youtube.com/watch?v=QmRFmhILd5o}. Subsets of 
Imagenet
containing underwater images were selected for the training of CycleGAN, and manually separated into two classes based on visual
inspection. We let  be the set of underwater images with no distortion, and  be the set of underwater images with 
distortion. 
contained 6143 images, and  contained 1817 images. We then trained CycleGAN to learn the mapping , such 
that images
from  appeared to have come from . Finally, our image pairs for training data were generated by distorting all images in  
with
. Figure~\ref{fig:cgan_samples} shows sample training pairs. When comparing with CycleGAN, we used a test set of 56 images 
acquired from
Flickr\texttrademark .

\begin{figure}[!ht]
\centering
\footnotesize
\begin{tabular}{p{1.6cm} p{1.6cm} p{1.6cm} p{1.6cm}}
   Original & CycleGAN & \textbf{UGAN} & \textbf{UGAN-P} \\
   \includegraphics[width=0.7in]{1_original} &
   \includegraphics[width=0.7in]{1_cimg} &
   \includegraphics[width=0.7in]{1_u0img} &
   \includegraphics[width=0.7in]{1_u1img} \\ [-1ex]
   \includegraphics[width=0.7in]{1_oedges} &
   \includegraphics[width=0.7in]{1_cedges} &
   \includegraphics[width=0.7in]{1_u0edges} &
   \includegraphics[width=0.7in]{1_u1edges} \\

   \includegraphics[width=0.7in]{2_original} &
   \includegraphics[width=0.7in]{2_cimg} &
   \includegraphics[width=0.7in]{2_u0img} &
   \includegraphics[width=0.7in]{2_u1img} \\ [-1ex]
   \includegraphics[width=0.7in]{2_oedges} &
   \includegraphics[width=0.7in]{2_cedges} &
   \includegraphics[width=0.7in]{2_u0edges} &
   \includegraphics[width=0.7in]{2_u1edges} \\

   \includegraphics[width=0.7in]{3_original} &
   \includegraphics[width=0.7in]{3_cimg} &
   \includegraphics[width=0.7in]{3_u0img} &
   \includegraphics[width=0.7in]{3_u1img} \\ [-1ex]
   \includegraphics[width=0.7in]{3_oedges} &
   \includegraphics[width=0.7in]{3_cedges} &
   \includegraphics[width=0.7in]{3_u0edges} &
   \includegraphics[width=0.7in]{3_u1edges} \\
   
   \includegraphics[width=0.7in]{4_original} &
   \includegraphics[width=0.7in]{4_cimg} &
   \includegraphics[width=0.7in]{4_u0img} &
   \includegraphics[width=0.7in]{4_u1img} \\ [-1ex]
   \includegraphics[width=0.7in]{4_oedges} &
   \includegraphics[width=0.7in]{4_cedges} &
   \includegraphics[width=0.7in]{4_u0edges} &
   \includegraphics[width=0.7in]{4_u1edges} \\
\end{tabular}
\vspace{-2mm}
\caption{\small{Running the Canny Edge Detector on sample images. Both variants of UGAN contain less noise than CycleGAN,
and are closer in the image space to the original. For each pair, the top row is the input image, and bottom row
the result of the edge detector. The figure depicts four different sets of images, successively labeled A to D from top to 
bottom. See Table~\ref{tab:one}.}}
\label{fig:canny_samples}
\vspace{-5mm}
\end{figure}

\subsection{Evaluation}
We train UGAN and UGAN-P on the image pairs generated by CycleGAN, and evaluate on the images from the
test set, . Note that these images do not contain any ground truth, as they are original distorted images from
Imagenet. Images for training and testing are of size  and normalized between .
Figure ~\ref{fig:test_samples} shows samples from the test set. Notably, these images contain varying amounts of noise. Both UGAN 
and UGAN-P
are able to recover lost color information, as well as correct any color information this is present. 

While many of the distorted images contain a blue or green hue over the entire image space, that is not always the case.
In certain environments,
it is possible that objects close to the camera are undistorted with correct colors, while the background
of the image contains distortion. In these cases, we would like the network to only correct parts of the image that
appear distorted. The last row in Figure ~\ref{fig:test_samples} shows a sample of such an image. The orange of the clownfish is 
left
unchanged while the distorted sea anemone in the background has its color corrected.

For a quantitative evaluation we compare to CycleGAN, as it inherently learns an inverse mapping during the training of
. We first use the Canny edge detector
\cite{canny1986computational}, as this provides a color agnostic evaluation of the images in comparison to ground truth.
Second, we compare local image patches to provide sharpness metrics on our images. Lastly, we show how an existing
tracking algorithm for an underwater robot improves performance with generated images.






\begin{figure*}[t]
\centering
\begin{tabular}{p{3.7cm} p{3.7cm} p{3.7cm} p{3.7cm}}
  
   \qquad \: \, ~ Original & \qquad \: \: CycleGAN & \qquad \: \: \, \textbf{UGAN} & \qquad \: \, \textbf{UGAN-P} \\

   \includegraphics[width=1.5in]{flickr_cmp} &
   \includegraphics[width=1.5in]{cgan_cmp} &
   \includegraphics[width=1.5in]{ugan_cmp} &
   \includegraphics[width=1.5in]{uganp_cmp} \\ [-2ex]
   
\end{tabular}
\end{figure*}

\begin{figure*}[t]
\begin{tabular}{p{1.6cm} p{1.7cm} p{1.6cm} p{1.7cm} p{1.6cm} p{1.7cm} p{1.6cm} p{1.9cm} }
   
   \includegraphics[width=0.7in]{flickr_crop1} &
   \includegraphics[width=0.7in]{flickr_crop2} &
   \includegraphics[width=0.7in]{cgan_crop1} &
   \includegraphics[width=0.7in]{cgan_crop2} &
   \includegraphics[width=0.7in]{ugan_crop1} &
   \includegraphics[width=0.7in]{ugan_crop2} &
   \includegraphics[width=0.7in]{ugan_crop1} &
   \includegraphics[width=0.7in]{ugan_crop2} \\

   \includegraphics[width=0.7in]{flickr_crop3} &
   \includegraphics[width=0.7in]{flickr_crop4} &
   \includegraphics[width=0.7in]{cgan_crop3} &
   \includegraphics[width=0.7in]{cgan_crop4} &
   \includegraphics[width=0.7in]{ugan_crop3} &
   \includegraphics[width=0.7in]{ugan_crop4} &
   \includegraphics[width=0.7in]{ugan_crop3} &
   \includegraphics[width=0.7in]{ugan_crop4} \\

\end{tabular}
\caption{Local image patches extracted for quantitative comparisons, shown in Tables~\ref{fig:gdl_tbl} and~\ref{fig:mean_tbl}. 
Each patch was resized to , but shown enlarged for viewing ability.}
\label{fig:zoom}
\end{figure*}

\subsection{Comparison to CycleGAN}
It is important to note that during the process of learning a mapping , CycleGAN also learns a mapping . Here we give a comparison to our methods. We use the Canny edge detector \cite{canny1986computational} to provide 
a color agnostic evaluation of the images, as the original contain distorted colors and cannot be compared back to as ground 
truth. Due to the fact that restoring color information should not alter the overall structure of the image, we measure the 
distance in the image space between the edges found in the original and generated images. Figure ~\ref{fig:canny_samples} shows 
the original images and results from edge detection. Table~\ref{tab:one} provides the measurements from Figure 
~\ref{fig:canny_samples}, as well as the average over our entire Flickr\texttrademark\ dataset. Both UGAN and UGAN-P are 
consistently closer in the image space to the original than that of CycleGAN, suggesting noise due to blur. Next, we evaluate this 
noise explicitly.

We explore the artifacts of content loss, as seen in Figure ~\ref{fig:zoom}. In particular, we compare local statistics of the 
highlighted image patches, where each image patch is resized to . We use the GDL \cite{mathieu2015deep} from 
(\ref{gdl_eq}) as a sharpness measure. A lower GDL measure implies a smoother transition between pixels, as a noisy image would 
have large jumps in the image's gradient, leading to a higher score.\nostarnote{I THINK. I would double check, bottom of page 4: 
https://arxiv.org/pdf/1511.05440.pdf.} As seen in Table \ref{fig:gdl_tbl}, the GDL is lower for both UGAN and UGAN-P. 
Interestingly, UGAN consistently has a lower score than UGAN-P, despite UGAN-P explicitly accounting for this metric in the 
objective function. Reasoning for this is left for our future work.

Another metric we use to compare image patches are the mean and standard deviation of a patch. The standard deviation gives us a 
sense of blurriness because it defines how far the data deviates from the mean.\nostarnote{ A lower standard deviation means that 
the data is closer to the mean . not sure if we exactly need to say that because I assume the reviewers will know what it is.} In 
the case of images, this would suggest a blurring effect due to the data being more clustered toward one pixel value. Table 
\ref{fig:mean_tbl} shows the mean and standard deviations of the RGB values for the local image patches seen in Figure 
\ref{fig:zoom}. Despite qualitative evaluation showing our methods are much sharper, quantitatively they show only slight 
improvement. Other metrics such as entropy are left as future work.

\begin{table}
\centering
\footnotesize
\caption{Distances in image space}
\begin{tabular}{| c | c | c | c |}
   \hline
   Row/Method & CycleGAN & \textbf{UGAN} & \textbf{UGAN-P} \\ \hline
   A          & 116.45 & 85.71  & 86.15  \\ \hline
   B          & 114.49 & 97.92  & 101.01 \\ \hline
   C          & 120.84 & 96.53  & 97.57  \\ \hline
   D          & 129.27 & 108.90 & 110.50 \\ \hline
   Mean       & 111.60 & 94.91  & 96.51 \\ \hline
\end{tabular}
\label{tab:one}
\end{table}

\begin{table}[ht]
\footnotesize
\centering
\caption{Gradient Difference Loss Metrics}
\begin{tabular}{| c | c | c | c | }
   \hline
   Method/Patch & CycleGAN & \textbf{UGAN} & \textbf{UGAN-P} \\ \hline
   Red    & 11.53 & 9.39 & 10.93  \\ \hline
   Blue   & 7.52  & 4.83 &  5.50\\ \hline
   Green  & 4.15  & 3.18 & 3.25 \\ \hline
   Orange & 6.72  & 5.65 & 5.79 \\ \hline
\end{tabular}
\label{fig:gdl_tbl}
\end{table}

\begin{table*}[ht]
\centering
\caption{Mean and Standard Deviation Metrics}
\begin{tabular}{| c | c | c | c | c | }
   \hline
   Method/Patch & Original & CycleGAN & \textbf{UGAN} & \textbf{UGAN-P} \\ 
\hline
   Red & 0.43  0.23 & 0.42  0.22 & 0.44  0.23 & 0.45  0.25 \\ \hline
   Blue & 0.51  0.18 & 0.57  0.17 & 0.57  0.17 & 0.57  0.17 \\ \hline
   Green & 0.36  0.17 & 0.36  0.14 & 0.37  0.17 & 0.36  0.17 \\ \hline
   Orange & 0.3  0.15 & 0.25  0.12 & 0.26  0.13 & 0.27  0.14 \\ \hline
\end{tabular}
\label{fig:mean_tbl}
\end{table*}

\subsection{Diver Tracking using Frequency-Domain Detection}
We investigate the frequency-domain characteristics of the restored images through a case-study of periodic motion tracking in 
sequence of images. Particularly, we compared the performance of Mixed Domain Periodic Motion (MDPM)- tracker 
\cite{islam2017mixed} on a sequence of images of a diver swimming in  arbitrary directions. MDPM tracker is designed for 
underwater robots to follow scuba divers by   tracking distinct frequency-domain signatures (high-amplitude spectra at -Hz) 
pertaining to human swimming. Amplitude spectra in frequency-domain correspond to the periodic intensity variations in image-space 
over time, which is often eroded in noisy underwater images \cite{shkurti2017underwater}.

Fig. \ref{mdpmStuff} illustrates the improved performance of MDPM tracker on generated images compared to the real ones. 
Underwater images often fail to capture the true contrast in intensity values between foreground and background due to low 
visibility. The generated images seem to restore these eroded intensity variations to some extent, causing much improved positive 
detection (a 350\% increase in correct detections) for the MDPM tracker.


\subsection{Training Details and Inference Performance}
In all of our experiments, we use , , batch size of 32, and the Adam Optimizer 
\cite{kingma2014adam} with learning rate . Following WGAN-GP, the discriminator is updated  times for every update of the 
generator, where . For UGAN-P, we set  and . Our implementation was done using the 
Tensorflow library \cite{abadi2016tensorflow}. \footnote{Code is available at 
\url{https://github.com/cameronfabbri/Underwater-Color-Correction}} All networks were trained from scratch on a GTX 1080 for 100 
epochs. Inference on the GPU takes on average , which is about 72 Frames Per Second (FPS). On a CPU (Intel Core 
i7-5930K), inference takes on average , which is about 8 FPS. In both cases, the input images have dimensions 
. We find both of these measures acceptable for underwater tasks.

