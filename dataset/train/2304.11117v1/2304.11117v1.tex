\documentclass{article}


\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~(\ref{#1})}
\def\Eqref#1{Equation~(\ref{#1})}
\def\plaineqref#1{(\ref{#1})}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{spconf,amsmath,graphicx}
\usepackage{algpseudocode}\usepackage[ruled,vlined,linesnumbered]{algorithm2e}    
\usepackage{wrapfig}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{pifont}
\def\x{{\mathbf x}}
\def\L{{\cal L}}



\newcommand{\myparagraph}[1]{\noindent\textbf{#1}\hspace{.25cm}}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\DeclareMathOperator{\diag}{diag} 
\DeclareMathOperator{\tr}{tr} 

\title{A Vector Quantized Masked AutoEncoder for speech emotion recognition}
\name{Samir Sadok, Simon Leglaive, Renaud Séguier}
\address{CentraleSup\'elec, IETR UMR CNRS 6164, France}
\begin{document}
\ninept
\maketitle
\begin{abstract}
Recent years have seen remarkable progress in speech emotion recognition (SER), thanks to advances in deep learning techniques. However, the limited availability of labeled data remains a significant challenge in the field. Self-supervised learning has recently emerged as a promising solution to address this challenge. In this paper, we propose the vector quantized masked autoencoder for speech (VQ-MAE-S), a self-supervised model that is fine-tuned to recognize emotions from speech signals. The VQ-MAE-S model is based on a masked autoencoder (MAE) that operates in the discrete latent space of a vector quantized variational autoencoder. Experimental results show that the proposed VQ-MAE-S model, pre-trained on the VoxCeleb2 dataset and fine-tuned on emotional speech data, outperforms an MAE working on the  raw spectrogram representation and other state-of-the-art methods in SER.
\end{abstract}
\begin{keywords}
Self-supervised learning, masked autoencoder, vector-quantized variational autoencoder, speech emotion recognition. 
\end{keywords}
\section{Introduction}
\label{sec:intro}
Speech emotion recognition (SER) \cite{el2011survey} is a research area focused on automatically identifying emotions from speech signals. With the growth of technology and the increasing use of speech-based interfaces, there is a growing demand for systems that can accurately recognize emotions in speech. In recent years, deep learning methods have played a major role in improving SER performance \cite{akccay2020speech}. However, the scarcity and high cost of obtaining labeled speech data for emotion recognition pose a major difficulty. To address this challenge, researchers have shifted their focus towards self-supervised learning approaches \cite{wang2021fine, chen2021exploring}. In these approaches, models are pre-trained on a self-supervised task, such as predicting masked tokens in speech signals, and then fine-tuned on a smaller set of labeled data for the SER task \cite{gong2022ssast, pepino2021emotion}. This method has the advantage of scalability, as the self-supervised task can be trained on large amounts of unlabeled speech data, reducing the need for labeled data \cite{liu2022audio, zhang2022survey}. Self-supervised training has been successfully applied in the field of SER, and has demonstrated promising results by allowing the model to learn useful representations of speech signals for emotion recognition, even in scenarios where labeled data is limited \cite{pepino2021emotion, macary2021use}.

This article focuses on self-supervised SER with the masked autoencoder (MAE) approach \cite{he2022masked}. The MAE is an asymmetrical encoder-decoder architecture that relies on input masking \cite{he2022masked}. Originally developed in natural language processing (NLP) \cite{devlin2018bert}, the MAE approach has also been applied to image analysis using vision transformers (ViT) \cite{dosovitskiy2020image}. The MAE process involves dividing the input into non-overlapping patches, each represented by a token embedding. A large proportion of tokens are masked (usually 75\% for image modeling and 15\% for text modeling), and only the visible tokens are fed to the encoder. A lightweight decoder then reconstructs the image/text by combining the visible tokens from the encoder and learnable mask tokens. The cost function is applied only to the masked tokens. Recently, the MAE has been adapted for audio using 2D time-frequency representations such as the mel-spectrogram \cite{gong2022ssast, baade2022mae, xu2022masked}. However, using L1 or L2 losses for reconstruction can result in a blurred image or a noisy audio signal. As He et al. suggest \cite{he2022masked}, improving the quality of MAE predictions can potentially lead to better representations for downstream tasks. 

    This paper introduces the vector quantized MAE for speech (VQ-MAE-S), a self-supervised model designed for emotion detection in speech signals. VQ-MAE-S is an adapted version of the Audio-MAE model proposed in \cite{gong2022ssast, baade2022mae, xu2022masked}. Unlike Audio-MAE, VQ-MAE-S operates on the discrete latent representation of a vector-quantized variational autoencoder (VQ-VAE) \cite{van2017neural} instead of the spectrogram representation. The pre-training of the model is performed on the VoxCeleb2 dataset \cite{chung2018voxceleb2}, and fine-tuning is carried out on several standard emotional speech datasets. We conduct several experiments to study the impact of different model design choices (e.g., masking ratio, masking strategy, patch size, etc.). The experimental results demonstrate that the proposed VQ-MAE-S model yields improvements in SER performance compared to an MAE using raw spectrogram data and other state-of-the-art methods. The code and qualitative reconstruction results are available at \url{https://samsad35.github.io/VQ-MAE-Speech/}.

\section{Vector Quantized Masked Autoencoder for Speech}
\label{sec:methodology}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{images/overview.pdf}
    \caption{VQ-MAE-S model structure.}
    \label{fig:Overview}
\end{figure*}


This section presents the proposed self-supervised VQ-MAE-S model, which is represented in Figure~\ref{fig:Overview}. The model takes as input the power spectrogram of a speech signal, denoted by , where  and  correspond to the time and frequency dimensions. 

\subsection{Vector quantized variational autoencoder}

The proposed self-supervised approach builds upon the discrete latent representation of a VQ-VAE \cite{van2017neural} assumed to be pre-trained and frozen (more details on the pre-training are given in Section~\ref{sec:pretrain_setting}). The VQ-VAE encoder is used to obtain a compressed and quantized representation  of the input speech power spectrogram . Each entry of  corresponds to the index of a vector in the VQ-VAE codebook. The quantized representation  keeps the aspect of a time-frequency representation, as the VQ-VAE is designed to be fully convolutional on the frequency axis and to process spectrogram frames independently.  can thus be seen as a discrete and compressed representation of the speech power spectrogram , where compression occurs along the frequency axis (). As illustrated in Figure~\ref{fig:Overview} and discussed in the next subsections, the proposed self-supervised learning approach operates on this discrete and compressed representation before reconstruction with the VQ-VAE decoder.


\subsection{Tokens creation, masking, and embedding}
\label{sec:mask}

\myparagraph{Discrete index tokens} As illustrated in Figure~\ref{fig:Overview}, we build \emph{discrete index tokens} from the quantized representation  by dividing it into non-overlapping patches of size (). This leads to  and  patches horizontally and vertically, respectively. The representation  is then reshaped to  and used as input to the masking process. 

\myparagraph{Masking} We apply masking to the  time-frequency grid of discrete index tokens, each of which has a dimension of . The masked tokens are then replaced with a trainable vector. We explore three different types of patch-based masking, where tokens are masked randomly along the time-frequency, time, or frequency dimensions. Additionally, we test the frame masking strategy that involves masking the entire discrete frames of  (columns) instead of the patches. The effectiveness of these strategies is experimentally evaluated in the context of SER, including varying the ratio of masked tokens from \% to \%.

\myparagraph{Continuous embedding tokens} The discrete index tokens correspond to the indices obtained through the quantization step of the pretrained VQ-VAE encoder. Before being input to the VQ-MAE-S encoder, these discrete tokens are replaced with trainable continuous embedding vectors taken from a codebook of dimension , where  is the number of codes in the codebook, and  is the dimension of each code. This is simply achieved by replacing the  indices of a discrete token by the corresponding  vectors of dimension  in the codebook. The codebook is initialized by the pretrained VQ-VAE codebook and it can be held frozen or fine-tuned. After this embedding process (represented by the orange block in Figure\ref{fig:Overview}), the discrete index tokens in  are transformed into continuous embedding tokens in . In the experiments, we will investigate the effect of varying the embedding size  on the SER performance.

\subsection{VQ-MAE-S encoder, decoder, and loss function}



\myparagraph{Encoder} The VQ-MAE-S encoder, similar to the ViT architecture \cite{dosovitskiy2020image}, consists of a single Transformer encoder \cite{vaswani2017attention}. This encoder is a stack of  residual blocks that includes a self-attention layer, a normalization layer, and a Multi-layer Perceptron (MLP) block. Trained position embeddings are added for each token \cite{devlin2018bert}. As in \cite{he2022masked}, the encoder inputs are only the visible tokens; this is to learn a representation that relies on the context. In addition, we add a global trainable token  as in \cite{devlin2018bert}, which will be used for SER tasks. 
Since most of the tokens are masked and only the visible tokens are fed to the encoder, this resolves the quadratic complexity issue inherent in transformer models with respect to the sequence length \cite{he2022masked}. As a result, the number of attention blocks in the encoder can be increased without experiencing computational inefficiencies.

\myparagraph{Decoder} The VQ-MAE-S decoder takes in both visible and masked tokens along with an additional position embedding. It consists of attention blocks similar to the encoder, but with fewer consecutive blocks () compared to the encoder (). The decoder also includes a linear layer at the end that maps to the size of the VQ-VAE codebook. The output of this linear layer corresponds to the logits of the discrete index tokens. After applying a  operation, we obtain a reconstruction  of the indices  that were provided by the VQ-VAE encoder.

\myparagraph{Loss function} To train the VQ-MAE-S model, we minimize the cross-entropy loss applied only to the masked discrete index tokens:

where  and  represent the masked tokens in  and their reconstructions, respectively.



\begin{table*}[t]
\centering
\begin{minipage}{.45\linewidth}
      \centering
        \caption{Performance of VQ-MAE-S-\emph{12} on RAVDESS-Speech. Masking method: Random time-frequency masking (Patch-\emph{tf}), Ratio: 80\%, freeze refers to the freezing of the encoder of VQ-MAE-S.}
        \label{tab:finetuning}
            \begin{tabular}{c|ccc}
                Method          & Pre-train  & freeze  & Acc. \small{(\%)} \\ \hline
                VQ-MAE-S-\emph{12}        & \ding{55}               & \ding{55}             & 26.8  \\
                VQ-MAE-S-\emph{12}        & \checkmark             & \checkmark            & 57.6  \\
                VQ-MAE-S-\emph{12}        & \checkmark             & \ding{55}             & \textbf{76.7}  \\
                SpecMAE-\emph{12} & \checkmark            & \ding{55}             &   52.2            
            \end{tabular}
    \end{minipage} 
    \hfill
    \begin{minipage}{.5\linewidth}
      \caption{Performance of VQ-MAE-S-\emph{12} on RAVDESS-Speech for different masking strategies with a ratio of 80\%.}
      \label{tab:mask}
      \centering
        \begin{tabular}{c|cc}
                Masking method     & Acc. \small{(\%)} & f1-score \small{(\%)} \\ \hline
                Random frame masking (Frame) & \textbf{80.8} & \textbf{80.5}\\ 
                Random frequency masking (Patch-\emph{f})    & 65.7           &     65.0     \\
                Random time masking (Patch-\emph{t}) & 68.5           & 68.7         \\
                Random time-frequency masking  (Patch-\emph{tf}) & \textbf{76.7}           &     \textbf{75.9}         
            \end{tabular}
    \end{minipage}\hfill
    \begin{minipage}{.45\linewidth}
      \centering
        \caption{Performance of VQ-MAE-S-\emph{12} on RAVDESS-Speech for different continuous embedding sizes. Masking method: Random time-frequency masking (Patch-\emph{tf}); Ratio: 80\%}
        \label{tab:dim}
            \begin{tabular}{c|ccc}
                Token dim.     & Parm. \small{(M)} & Acc. \small{(\%)} & f1-score \small{(\%)}\\ \hline
                =160                & 5 & 70.9           &  70.1        \\
                =320                & 20 &\textbf{76.7}           &     \textbf{75.9}       \\
                =640                & 80 & 75.4           &   75.3     
            \end{tabular}
    \end{minipage} 
    \hfill
    \begin{minipage}{.5\linewidth}
      \centering
        \caption{Performance of VQ-MAE-S on RAVDESS-Speech for different encoder depths. Masking method: Random time-frequency masking (Patch-\emph{tf}); Ratio: 80\%.}
        \label{tab:depth}
            \begin{tabular}{c|ccc}
                Encoder Depth     & Parm. \small{(M)} & Acc. \small{(\%)} & f1-score \small{(\%)}\\ \hline
                VQ-MAE-S-\emph{6}  & 12 & 65.8           &    65.3      \\
                VQ-MAE-S-\emph{12} & 20 & 76.7           &    75.9         \\
                VQ-MAE-S-\emph{16} & 25 & \textbf{79.2}           &  \textbf{79.4}      \\
                VQ-MAE-S-\emph{20} & 30 & 76.8 &  76.2      
            \end{tabular}
    \end{minipage}

\end{table*}

\section{Experiments}
\subsection{Pre-training, fine-tuning, and evaluation}   

\subsubsection{Pre-training of VQ-MAE-Speech}
\label{sec:pretrain_setting}
To pre-train VQ-MAE-S, we used the VoxCeleb2 dataset \cite{chung2018voxceleb2}, which provides an extensive collection of audio speech data from open-source media, including speech segments corrupted by various real-world noises. We restricted our use of the dataset to a subset of around 1000 hours of audio-visual speech, including 2170 speakers.

The VQ-VAE model was trained on the same portion of the VoxCeleb2 dataset using short-time Fourier transform (STFT) power spectrograms (). The STFT is computed using a 64-ms Hann window (1024 samples) and a 70\% overlap, resulting in sequences of -dimensional discrete Fourier coefficients. The VQ-VAE architecture is symmetrical with respect to the encoder and the decoder, with three 1D convolution (encoder) or transposed convolution (decoder) layers on the frequency axis and a residual convolution layer. The model processes each frame independently with no time dependency. For each speech power spectrogram frame of size , the VQ-VAE compresses it into a discrete latent vector (a row of ) of size . 
The VQ-VAE codebook contains  codes of dimension . Such a low dimension is chosen to increase the use of the different codes in the codebook, as in \cite{yuvector}. 


We evaluate the impact of different encoder architectures on the performance of VQ-MAE-S. The architecture is denoted as VQ-MAE-S-\emph{n}, where \emph{n} refers to the number of attention blocks in the encoder.  The decoder is fixed at four attention blocks. Each self-attention layer in these blocks is subdivided into four heads. The model is trained using the AdamW optimizer \cite{loshchilov2017decoupled} with a cosine scheduler to adjust the learning rate, with a 100-epoch warm-up period. The parameters of the optimizer, similar to \cite{he2022masked}, are , , and \texttt{weight\_decay}. The base learning rate follows the linear scaling rule \cite{goyal2017accurate} . We distributed the pre-training of VQ-MAE-S on 4 NVIDIA HGX A100. As in \cite{xu2022masked}, no data augmentation is performed. For more information on the architecture, please refer to our publicly available implementation.

\subsubsection{SER fine-tuning details}

Only the encoder of the VQ-MAE-S model is fine-tuned for SER. We propose two approaches: The first one uses the global token  as input to a single linear layer, followed by a  operation to perform emotion classification.
The second approach, referred to as Query2Emo and inspired by \cite{liu2021query2label}, involves cross-attention between all sequence tokens as value/key and the emotion classes represented by trainable embedding as query. Query2Emo has a single attention block for both the encoder and decoder. For these two approaches, we use the AdamW optimizer \cite{loshchilov2017decoupled} with a cosine scheduler to adjust the learning rate and with a 40-epoch warm-up period. The parameters of the optimizer, similar to \cite{he2022masked}, are , , and \texttt{weight\_decay} . The base learning rate is \texttt{1e-4}. For the loss function, we adopt the asymmetric loss \cite{ridnik2021asymmetric} adapted for single labels instead of the cross entropy as it yields better results. 

\subsubsection{Emotional databases for fine-tuning and evaluation}

We fine-tune and evaluate the proposed approaches on four emotional speech audio databases.\\
\textbf{RAVDESS-Speech} \cite{livingstone2018ryerson}: This English database consists of 1440 audio files recorded by 24 professional actors and labeled with eight different emotional expressions (neutral, calm, happy, sad, angry, fearful, disgust, surprised). \\
\textbf{RAVDESS-Song} \cite{livingstone2018ryerson}: Same as the RAVDESS-Speech database, but utterances are sung \textit{a capella}. This database contains a total of 1012 audio files recorded by 23 actors and labeled with six emotions (neutral, calm, happy, sad, angry, and fearful). \\
\textbf{IEMOCAP} \cite{busso2008iemocap}: This database comprises approximately 12 hours of audio, annotated with several emotions, but only four emotions (neutral, happy, angry, and disgusted) have been retained to ensure a balanced distribution. It consists of dyadic sessions in which actors participate in improvisations or scripted scenarios.\\
\textbf{EMODB} \cite{burkhardt2005database}: The German EMODB database consists of 535 utterances spoken by ten professional speakers. It includes seven emotions (anger, boredom, anxiety, happiness, sadness, disgust, and neutral). 

For a fair comparison with the literature, we perform \emph{5}-fold cross-validation by separating the speakers' identity between the fine-tuning phase and the evaluation phase.

\begin{table*}[t]
\centering
\caption{Overall results (accuracy (\%) and f1-score (\%)) on the four evaluation databases.}
\label{tab:my-table}
\begin{tabular}{c|cc|cc|cc|cc}
\texttt{DATASET} & \multicolumn{2}{c|}{\texttt{RAVDESS-Speech}} & \multicolumn{2}{c|}{\texttt{RAVDESS-Song}} & \multicolumn{2}{c|}{\texttt{IEMOCAP}} & \multicolumn{2}{c}{\texttt{EMODB}} \\ \hline
Metrics & Accuracy           & f1-score          & Accuracy          & f1-score         & Accuracy           & f1-score           & Accuracy          & f1-score          \\ \hline
Self-attention audio \cite{chumachenko2022self}        & 58.3               &   -          & -              &     -       &      -         &      -        &        -      &          -     \\
SSAST \cite{gong2022ssast} (Patch-\emph{tf})        &  -      &   -          & -              &     -       &      54.3         &      -        &        -      &          -     \\
MAE-AST \cite{baade2022mae} (Patch-\emph{tf}) &  -      &   -          & -              &     -       &      58.6         &      -        &        -      &          - \\
SpecMAE-12 (Patch-\emph{tf})        &  52.2      &   52.0          & 54.5              &     53.9       &      46.7         &      45.9        &        57.2      & 57.0    \\ \hline
VQ-MAE-S-\emph{12}  (Patch-\emph{tf})      &   76.7             &      75.9       &        84.0     &    84.0        &    61.9           &   61.2           &    85.7          &      85.8       \\
VQ-MAE-S-\emph{12} (Patch-\emph{tf}) + Query2Emo        & 78.2               &   77.5          &       83.7       &      83.4      &    63.1           &   62.5           &      88.4        &  88.3           \\
VQ-MAE-S-\emph{12} (Frame)       & 80.8               &   80.5          &       84.2       &      84.3      &    65.2           &   64.9           &      87.0        &  87.0       \\  
VQ-MAE-S-\emph{12} (Frame) + Query2Emo      & \textbf{84.1}               &   \textbf{84.4}          &       \textbf{85.8}       &      \textbf{85.7}      &    \textbf{66.4}           &   \textbf{65.8}           &      \textbf{90.2}        &  \textbf{89.1}          
        
\end{tabular}
\end{table*}

\subsection{Performance on speech emotion recognition}
Table~\ref{tab:finetuning} highlights the importance of pre-training and fine-tuning the VQ-MAE-S-\emph{12} model for SER. Pre-training significantly improves the SER performance, with the accuracy increasing from \% to \% on the RAVDESS-Speech dataset. Fine-tuning the encoder is also crucial, as freezing the encoder results in a \% accuracy loss. We also compared our approach with SpecMAE-\emph{12}, an MAE model that directly uses speech power spectrogram patches and aims to reconstruct masked patches using L2 loss. Our approach outperforms SpecMAE-\emph{12} by \% in terms of accuracy, which clearly shows the benefit of working on the discrete representation of the VQ-VAE instead of the raw spectrogram representation for training the MAE.

Table~\ref{tab:my-table} compares the SER performance (accuracy and F1-score) of the proposed VQ-MAE-S model (with classification from the  token), its improved version VQ-MAE-S + Query2Emo, the SpecMAE-12 baseline, and three state-of-the-art methods: SSAST \cite{gong2022ssast}, MAE-AST \cite{baade2022mae}, and a supervised self-attention-based approach \cite{chumachenko2022self} on the four evaluation databases. Two configurations of masking are considered: random frame masking (Frame) and random time-frequency patch masking (Patch-\emph{tf}). The results indicate that the proposed models outperform all other methods across all databases.
For random time-frequency patch masking (Patch-\emph{tf}), VQ-MAE-S achieves \% better accuracy than SpecMAE, \% better accuracy than SSAST, and \% better accuracy than MAE-AST on the IEMOCAP dataset. The accuracy improvement over the supervised method on the RAVDESS-Speech database is of \%. Query2Emo also contributes to the SER performance, with a gain of \%, \%, and \% compared to VQ-MAE-S (Patch-\emph{tf}) alone on RAVDESS-Speech, IEMOCAP, and EMODB, respectively.

We conducted several experiments to assess the importance of several hyperparameters of the proposed VQ-MAE-S model, which are presented and discussed in the following paragraphs.

\myparagraph{Impact of the masking strategy} The choice of the masking strategy in MAE-based self-supervised models can have a significant impact on the performance of auxiliary tasks \cite{he2022masked}. As shown in Table~\ref{tab:mask}, which compares the performance of emotion recognition across four masking types discussed in Section~\ref{sec:mask}, random frame-based masking outperforms random patch-based masking. In particular, the frame-based approach achieves performance gains of \%, \%, and \% compared to random patch-based masking on the time, frequency, and time-frequency axes, respectively. These results are consistent with those reported in prior studies \cite{gong2022ssast, baade2022mae}, and are highlighted in Table~\ref{tab:my-table} for the four evaluated databases.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{images/ratio.pdf}
    \caption{Impact of the masking ratio for VQ-MAE-S-\emph{12}.}
    \label{fig:ratio}
\end{figure}
\myparagraph{Impact of the masking ratio} Figure \ref{fig:ratio} shows the relationship between masking ratio (x-axis) and SER accuracy (y-axis) for the patch (blue) and frame (red) masking using VQ-MAE-S. The results indicate that patch masking achieves maximum accuracy at around  masking ratio, after which performance declines. The trend can be attributed to the complexity of the task, where higher masking ratios improve representation learning and SER accuracy. In contrast, the performance of frame-based masking remains relatively stable. However, pushing the complexity too high (e.g., ) leads to a loss of relevant contextual information and decreased SER accuracy.


\myparagraph{Impact of the encoder depth}
The impact of the encoder depth on SER performance is presented in Table~\ref{tab:depth}, where the number of attention blocks is varied. The results show that, to some extent, increasing the number of blocks in the encoder leads to improved performance. When the number of blocks becomes very high (in this case, VQ-MAE-S-\emph{20}), there is no further improvement, and performance actually decreases (\% accuracy) compared to VQ-MAE-S-\emph{16}.

\myparagraph{Impact of the continuous embedding token size} The impact of the continuous embedding token size on the SER performance on the RAVDESS-Speech dataset is shown in Table~\ref{tab:dim}, by varying the dimension () of the codes in the dictionary. Although we did not conduct an extensive study on this parameter, we tested three exponentially increasing token sizes (). It is observed that using a size of  leads to an improvement in SER performance compared to sizes  and , with an increase of \% accuracy and \% accuracy, respectively.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{images/impact_t_d.pdf}
    \caption{Impact of the discrete index token size () on SER.}
    \label{fig:patch}
\end{figure}

\myparagraph{Impact of the discrete index token size}
Figure~\ref{fig:patch} illustrates the impact of the dimensions of the discrete index tokens ( and ) on SER performance on the RAVDESS-Speech dataset.  represents the discrete token size on the frequency axis, while  represents the discrete token size on the time axis. Our study reveals that both  and  significantly affect SER performance. Therefore, it is important to carefully select the values of (). Based on our experiment, we suggest fixing them to ().






\section{Conclusion}
\label{sec:conclusion}
VQ-MAE-S is a novel approach that uses a pre-trained VQ-VAE model to adapt the MAE for speech representation learning, using discrete tokens obtained from the quantization step of the VQ-VAE. Our method outperforms other MAE methods based on spectrograms or mel-spectrograms for SER on several standard datasets. Several experiments also highlighted the impact of VQ-MAE-S hyperparameters on SER performance. For future work, we plan to investigate other masking strategies to further improve SER performance, and we aim to combine the MAE with contrastive methods to improve the learned audio representation and achieve even better SER performance.
\clearpage









\bibliographystyle{IEEEbib}
\bibliography{strings}

\end{document}
