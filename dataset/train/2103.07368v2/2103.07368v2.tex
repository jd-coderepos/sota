\documentclass[journal]{IEEEtran}

\ifCLASSOPTIONcompsoc

  \usepackage[nocompress]{cite}
\else
  \usepackage{cite}
\fi


\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{textcomp}
\usepackage{multirow}
\usepackage{mathtools}
\usepackage{breqn}
\usepackage{kantlipsum}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{tabularx}
\usepackage{subfigure}

\usepackage[algo2e]{algorithm2e}
\usepackage{array}
\newcolumntype{x}[1]{>{\centering\arraybackslash\hspace{0pt}}p{#1}}
\newcolumntype{y}[1]{>{\lecentering\arraybackslash\hspace{0pt}}p{#1}}
\algnewcommand\algorithmicforeach{\textbf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}

\newcolumntype{Y}{>{\centering\arraybackslash}X}
\usepackage{caption}


\usepackage{listings}
\usepackage{xcolor}


\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    frame=single,
numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
}
\lstset{style=mystyle}

\usepackage{scalerel,stackengine}

\stackMath
\newcommand\reallywidehat[1]{\savestack{\tmpbox}{\stretchto{\scaleto{\scalerel*[\widthof{\ensuremath{#1}}]{\kern-.6pt\bigwedge\kern-.6pt}{\rule[-\textheight/2]{1ex}{\textheight}}}{\textheight}}{0.5ex}}\stackon[1pt]{#1}{\tmpbox}}
\parskip 1ex

\usepackage[utf8]{inputenc}


\DeclareMathOperator*{\mymax}{max}



\usepackage[breaklinks=true,bookmarks=false]{hyperref}



\def\iccvPaperID{****} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}



\begin{document}

\title{Information Maximization Clustering via Multi-View Self-Labelling}

\author{Foivos~Ntelemis,
        Yaochu~Jin, \emph{Fellow}, \emph{IEEE},
        Spencer~A.~Thomas
\thanks{This project is funded by an EPSRC industrial CASE award (number 17000013) and Department for Business, Energy and Industrial Strategy through the National Measurement System (122416). (\textit{Corresponding author: Yaochu Jin})}
\thanks{F. Ntelemis and Y. Jin are with the Department of Computer Science, University of Surrey, Guildford, GU2 7XH, United Kingdom. (Email: \{f.ntelemis; yaochu.jin\}@surrey.ac.uk)}
\thanks{S. A. Thomas is with the National Physical Laboratory, Teddington, TW11 0LW, United Kingdom. (Email: spencer.thomas@npl.co.uk)}
}


\maketitle


\begin{abstract}

Image clustering is a particularly challenging computer vision task, which aims to generate annotations without human supervision. Recent advances focus on the use of self-supervised learning strategies in image clustering, by first learning valuable semantics and then clustering the image representations. These multiple-phase algorithms, however, involve several hyper-parameters and transformation functions, and are computationally intensive. By extending the self-supervised approach, this work proposes a novel single-phase clustering method that simultaneously learns meaningful representations and assigns the corresponding annotations. This is achieved by integrating a discrete representation into the self-supervised paradigm through a classifier net. Specifically, the proposed clustering objective employs mutual information, and maximizes the dependency between the integrated discrete representation and a discrete probability distribution. The discrete probability distribution is derived through the self-supervised process by comparing the learnt latent representation with a set of trainable prototypes. To enhance the learning performance of the classifier, we jointly apply the mutual information across multi-crop views. Our empirical results show that the proposed framework outperforms state-of-the-art techniques with an average clustering accuracy of 89.1\%, 49.0\%, 83.1\% and 27.9\%, respectively, on the baseline datasets of CIFAR-10, CIFAR-100/20, STL10 and Tiny-ImageNet/200. Finally, the proposed method also demonstrates attractive robustness to parameter settings, and to a large number of classes, making it ready to be applicable to other datasets. The implementation of our method is available  \href{https://github.com/foiv0s/imc-swav-pub}{online}.

\end{abstract}



\begin{IEEEkeywords}
Deep neural models, mutual information maximization, unsupervised learning, self-supervised learning, clustering.
\end{IEEEkeywords}

\IEEEdisplaynontitleabstractindextext

\IEEEpeerreviewmaketitle

\section{Introduction}\label{sec:introduction}

Modern technologies such as Internet of Things and cloud computing have resulted in the collection and storage of a huge amount of data such as images and videos. With the help of such huge amount of data, deep supervised learning \cite{7780459,7298594} has achieved great success, provided that these data are labelled. Unfortunately, labelling such huge datasets is extremely laborious, and in many cases intractable. As a result, many image datasets are not fully utilized due to the lack of labels. In addition to the huge volumes, image data are usually characterized by a high dimensionality and multi-modal structure. Thus, the performance of traditional clustering approaches \cite{BEZDEK1984191,DBLP:conf/iccv/ComaniciuM99,Heller2005BayesianHC,Williams1999AMA,10.1007/978-3-642-33718-5_31} seriously deteriorates on such data  \cite{DBLP:conf/iccv/ChangWMXP17,Steinbach2004}. By contrast, deep unsupervised methods have demonstrated superiority and scalability in handling vision data \cite{NIPS2012_c399862d}, including representation learning \cite{caron2020unsupervised,pmlr-v119-chen20j,He_2020_CVPR} and clustering \cite{8237888,DBLP:conf/eccv/HanPPKC20,Ji_2019_ICCV,Ren2020DeepDI,10.1007/978-3-030-58607-2_16}.

Representation learning techniques are gaining popularity, since they can generate discriminative features by considering spatial properties, object shapes and photometric information without the requiring of human annotations. Recent studies \cite{caron2020unsupervised,pmlr-v119-chen20j,He_2020_CVPR,2018arXiv180703748V} have dramatically enhanced the learning capacity and thus minimized the performance gap between supervised and unsupervised learning tasks. This is achieved by proposing self-training objectives for representation learning, also called self-supervised methods.
In particular, contrastive learning strategies have widely been applied in self-supervised methods \cite{ caron2020unsupervised,pmlr-v119-chen20j,pmlr-v9-gutmann10a,2018arXiv180703748V}. They aim to increase the concordance of positive (similar) features of augmented views and decrease the discordance in negative (dissimilar) instances. Alternatively, feature comparisons are replaced in grouping techniques \cite{asano2020self, caron2018deep,caron2020unsupervised} by identifying sub-classes and assigning pseudo-labels to the relevant representations based on their similarities. These pseudo-labelling annotations are either derived through traditional clustering methods \cite{caron2018deep} (i.e., extracted features using a convolutional encoder are annotated through K-means clustering) or through an optimal transport plan \cite{asano2020self,caron2020unsupervised} that can ensure consistency and balance population. 
\par

Despite the remarkable success of grouping based self-supervised tasks, the pseudo-labels they generate have so far been evaluated for learning discriminative features only. Additionally, the deep clustering methods have not yet fully exploited the recently developed self-supervised algorithms. Some existing approaches \cite{Hu2017,Ji_2019_ICCV} implement a convolutional framework that introduces mutual information (MI) in the clustering objective during training. The application of MI has demonstrated to be beneficial; however, the performance of these methods degrades in tackling more challenging datasets. Most recently, self-supervised mechanisms were introduced to further improve clustering results on challenging datasets \cite{DBLP:conf/eccv/HanPPKC20,vangansbeke2020scan}, by firstly learning valuable properties of the training instances and then performing the clustering. Although the performance is enhanced, they often require multiple training phases whose clustering performance relies on the previous training phase and the effectiveness of each individual stage. Furthermore, many individual training phases require an additional set of augmentation operations and/or different objective functions. As a result, the final clustering accuracy heavily relies on an increasing number of hyper-parameters. Finally, multiple training phases often increase the computational complexity. 

This study aims to address the issues of multiple-phase clustering methods by extending the functionality of Swapping Assignments between multiple Views (SwAV) \cite{caron2020unsupervised}, a recently proposed grouping based self-supervised learning strategy that performs discriminative feature learning. Therefore, we propose an online single-phase clustering framework by integrating a deep classifier net into the SwAV framework to simultaneously learn the representations and assign the desired annotations. Specifically, the proposed clustering method exploits the semantic structure obtained through a discrete probabilistic distribution that is derived by comparing the latent representations with a set of trainable prototypes. This is accomplished by maximizing the mutual dependency between the discrete representation of the integrated classifier and the discrete probabilistic distribution derived by the prototypes. To further enhance the clustering performance, we adopt the multi-crop strategy as presented in \cite{caron2020unsupervised} and optimize the mutual information across multi-crop views. We call the proposed framework Information Maximization Clustering by Swapping Assignments between multiple Views (IMC-SwAV). Our empirical studies show that the proposed framework is highly competitive and outperforms a wide range of state-of-the-art methods. Additionally, it shows robustness to the number of prototypes and the number of clusters. Finally, we demonstrate that other grouping-based self-supervised learning methods such as SeLa \cite{asano2020self} can also be adopted, although the proposed single-phase clustering method is based on an extended SwAV. 

We highlight our contributions as follows: 
\begin{enumerate}
    \item A single-phase training framework is proposed by extending a feature learning strategy to simultaneously generated features and assign labels. This way, the proposed algorithm avoids extra hyper-parameters, and does not require additional transformation functions, nor extra training phases.
    \item A modified clustering objective based on mutual information that maximizes the dependency between an over-clustering discrete probabilistic distribution and the classifier's discrete representation. A multi-crop view strategy is adopted in optimizing the classifier to further enhance the classifier's performance.
    \item We demonstrate that our clustering strategy is not limited to a specific self-supervised learning strategy and can in principle be extended to any other grouping based self-supervised learning strategies within a single-phase training framework.
\end{enumerate}
The rest of the paper is organized as follows. In Section II, related work on clustering and representation learning are briefly reviewed. The proposed method, including the overall representation learning framework and a modified objective function for joint training on the basis of mutual information maximization is presented in Section III. Comparative experiments and ablation studies, along with the parameter settings and performance metrics, are described in Section IV. A summary of the proposed algorithm and future work are provided in Section V.    

\section{Related work}

\textbf{Embedding strategies} have emerged as an effective means to eliminate the necessity of the desired annotations in terms of disentangled feature learning. Previous approaches employ traditional frameworks, such as  autoencoders \cite{HintonSalakhutdinov2006b,Vincent2008} and deep belief networks \cite{10.1162/neco.2006.18.7.1527}. Generative models including variational autoencoders \cite{DBLP:conf/aistats/KhemakhemKMH20} and generative adversarial networks \cite{10.5555/2969033.2969125,DBLP:journals/corr/RadfordMC15} also became popular. This type of frameworks rely on two separate models,  one generating training instances from a latent space, and the other mapping the training samples into a latent domain. \par

\textbf{Self-supervised learning}, on the other hand, obtains visual representations using a single unit model. A range of training objectives have been proposed in literature: 1)  each training sample is treated as a unique class \cite{DBLP:journals/pami/DosovitskiyFSRB16}; 2) an image instance is partitioned in several patches and the encoder either attempts to solve the jigsaw puzzle \cite{10.1007/978-3-319-46466-4_5}, or 3) predicts the code of next image's patch \cite{2018arXiv180703748V}. Contrastive learning addresses feature extraction by comparing positive pairs (augmented views of similar instances). Many implementations \cite{caron2020unsupervised,pmlr-v119-chen20j,He_2020_CVPR,2018arXiv180703748V, DBLP:journals/ijcv/ZhaoXWWTL20} adopt variations of noise contrastive estimation (NCE) \cite{pmlr-v9-gutmann10a} loss function, that achieves a good representation by comparing positive pairs versus a large number of negative instances. To deal with the large negative sampling, momentum contrast (MoCo) \cite{He_2020_CVPR} has been suggested that compares representations between a momentum and a simple encoder, or in \cite{DBLP:journals/ijcv/ZhaoXWWTL20} through a memory bank for storing past representations. As another option, the grouping based strategies \cite{asano2020self,caron2018deep,caron2020unsupervised} assign the representations to a numerous surrogate sub-classes. A method presented in \cite{caron2018deep} employs a traditional K-means to cluster representations, whose performance, however, depends on network initialization, and an additional computational time is required for computing the centroids in each iteration. Some recently proposed algorithms \cite{asano2020self,caron2020unsupervised} take advantage of the efficient optimal transport plan, and introduce a self-labelling mechanism as target distribution which is computed via the Shinkhorn-Knopp \cite{NIPS2013_af21d0c9} algorithm. Nevertheless, these methods aim to extract features without producing cluster annotations.


\begin{figure*}[t]
    \centering
    \includegraphics[width=1.\textwidth]{figs/diagram.png}
    \caption{A diagram presents the framework's structure and a training instance , transformed twice through . Here  denotes the encoder model, and the comparable prototypes as .  indicates the introduced classification model implemented on top of the embedding output (diagram is designed via PlotNeuralNet \cite{haris_iqbal_2018_2526396}).}
    \label{fig:framework}
\end{figure*}

\par
\textbf{Image clustering} techniques group the corresponding representation basis to the defined priorities. Two main strategies are considered from different perspectives. A first group of studies \cite{DBLP:conf/iccv/ChangWMXP17,Ji_2019_ICCV, Yang2016JointUL,zhao2020deep} apply a defined clustering loss function to identify patterns within the training samples. A method called deep adaptive clustering (DAC) \cite{8237888} implements a convolutional net, and measures the cosine similarities of the generated features. The model is gradually trained to assign the most similar features to the same group index. Ji \textit{et al.} propose invariant information clustering (IIC) \cite{Ji_2019_ICCV} which includes the MI as a training objective to maximize the dependency between the categorical outputs of numerous augmented views. A similar algorithm is reported in \cite{Hu2017}, which suggests a variation of MI as the training objective for learning discrete representations regularized through virtual adversarial training. Nevertheless, the performance of the aforementioned single-phase training methods deteriorate on more challenging datasets. A second group of most recent methods is based on multiple sequential training phases. State-of-the-art performance has reported in \cite{vangansbeke2020scan}, where the model is initialized in the first phase by applying a contrastive learning objective. In the second phase, the  nearest features of each instance are measured and considered to belong to the same group, then the model is trained accordingly. Due to the mismatch prediction of the  nearest features, the method implements a third round of training, where the model learns from the most confident predictions. Likewise, a two-stage clustering is incorporated in \cite{eccv_han}, where the encoder parameters are firstly initialized through a self-supervised strategy. This pre-training phase significantly improves the performance by a large margin of the later clustering objective in the second stage. However, having multiple training stages will no doubt increase the computation time and introduce dependency on previous phases. In addition, each training phase usually involves its specific  hyper-parameters, transformation functions, optimizers as well as objective functions. By contrast, our proposed strategy is based on a single-phase training method, nevertheless, it demonstrates competitive performance comparable to the most recent multiple-phases strategies.


\section{The Proposed Method}
\label{sec:method}
Assume there is an unlabelled set of images denoted as , which holds a relation with a finite set of classes , where  is a given hyper-parameter equal to the number of classes. The goal of this work is to instantiate a convolutional model and map the described relation as , and  indicates the model's parameters. Figure \ref{fig:framework} presents the overall framework of the proposed IMC-SwAV, consisting of three main components: 1) a ConvNet encoder () that projects the training instances onto a latent space (); 2) the trainable prototype vectors  that are compared with the projected features to derive the computed distribution; and 3) an introduced classifier (), which maps the generated features to the corresponding discrete representation.

In the following, we begin with a brief description of a representation learning strategy based on an online self-labelling assignment method and its application to the optimization of an encoder model. We then present a joint training objective, which is a modified form of mutual information, with the aim to maximise the mutual dependency of the computed distribution obtained by the prototypes and the classifier's predictions.



\subsection{Unsupervised Representation Learning}

The encoder model adopted in this work , where  denotes the encoder's trainable parameters, aims to learn the important semantic information of the given set of data without supervision, while ignoring less valuable semantics such as background or noise. Hence, motivated by the recent state-of-the-art achievement of SwAV \cite{caron2020unsupervised}, we employ the SwAV contrastive learning strategy to impose the consistency between representations () obtained from augmented views of the same instances by comparing them to a set of prototype vectors . These prototypes are evaluated only for uniformly mapping the obtained representations by exchanging their predicted probabilistic distributions, respectively. This is achieved by minimizing the \textit{swapped} self-labelling training objective \cite{caron2020unsupervised}: 

where  denotes a latent representation derived by the encoder () of the  training instance, respectively,  is a collection of stochastic transformation functions applied on the original instance beforehand to obtain a transformed view , where  is the augmented view index. For example, representations obtained by two alternative views of the same instance () are indicated as  and  in Equation \ref{eq:swav objective}, as illustrated in Fig. \ref{fig:framework}.  and  are the swapped self-labelling target distributions of the two transformed views of the -th training instance, and  and  are the computed probabilistic distributions, respectively, also called ``codes", determined by comparing the corresponding representations with the trainable prototype vectors  as follows:

where  is a temperature parameter that controls the smoothness of the probabilistic Softmax output, and  indicates the number of prototypes. Lastly, we define  as the cosine similarity distance of both  normalized vectors, and  represents the  prototype vector. \par

To minimize the cross-entropy terms of the \textit{swapped} training objective (Equation \ref{eq:swav objective}), the computation of the exchanged pair distributions () for each augmented view is required. The self-labelling target distributions () prevent the assignment of trivial solutions, and enforce a uniform mapping to the prototypes. Ultimately, we accommodate the online computation as introduced in \cite{caron2020unsupervised} to derive the required distribution. Given a collection of feature vectors , with  being the size of the training mini-batch, and the prototype vectors , we aim to define a target distribution  to maximize the correlation between the generated feature vectors and the trainable prototypes by satisfying the aforementioned conditions as:


where  is a scaling parameter that regularizes the entropy term  by controlling the mapping of the distribution. In practice, the required target distribution can be efficiently optimized by iteratively employing the Sinkhorn-Knopp \cite{NIPS2013_af21d0c9} algorithm. Hence, the constraints of a uniform mapping and homogeneity are encouraged within the training mini-batch. The reader is referred to \cite{caron2020unsupervised} and \cite{NIPS2013_af21d0c9,peyre2020computational}, respectively, for a more detailed description of the SwAV  and Sinkhorn-Knopp algorithms.

\subsection{Joint Cluster Representations}

Thus far, the described encoder generates features without producing the desired annotation or discrete properties to the given  classes. Instead, the training set is mapped in domain . Our goal is to define a parametric classifier that assigns the generated representations into a given number of classes  such as , where  are parameters of classifier .\par

\begin{figure}[t]
    \centering
    \includegraphics[width=0.4\textwidth]{figs/jsd_pairwise.png}
    \caption{A scatter illustration of JSD pairwise distances between two probabilistic distributions  and  generated by a pre-trained encoder of the same 32 image instances, where different transformations applied in each instance.
    }
    \label{fig:scatter similarities}
\end{figure}
To this end, the encoder is optimized by assigning pairs of the same instances to their corresponding prototypes, while ensuring different instances, that are distinct for each prototype. We evaluate this argumentation by computing two distributions of a mini-batch denoted as  and  using a pre-trained encoder. Each distribution includes the same 32 instances from the training set, where each instance is alternated with the application of the transformation functions . We measure the pairwise similarity of the two probabilistic distributions by applying the Jensen–Shannon divergence (JSD) \cite{1365067}, a symmetric bounded distance measurement for evaluating two probability distributions. As presented in Fig. \ref{fig:scatter similarities}, the diagonal positions express the lower divergences with the mean value  as these transformed views are from the same original instances. On the other hand, the larger divergences are observed in the instances of different indices (off diagonals) with the mean value . Note that the upper bound of JSD is  and zero the lower bound. \par

Complementary to the above observation, we assume that the transformed views of an instance produce similar distribution in domain . We can also assume that the probabilistic output () in Eq. \ref{eq:u definition} and the corresponding classifier's outputs  holds a relationship since both are obtained from representations , hence defining a training objective that \textit{maximizes the mutual dependency} between these two distributions with respect to parameters . Then classifier  will map semantically similar instances in domain  to the same cluster in domain . Note that the distributions of  and  are a product of the Softmax output; hence, both belong to the probability simplex. Inspired by \cite{Ji_2019_ICCV}, we propose a measurement describing the amount of dependency between the two probabilistic outputs, which is the direct application of mutual information (MI) \cite{Cover:1991:EIT:129837}:

The MI expression is the relative entropy for measuring the divergence of the joint probability  and the product of two marginals. We modify Equation \ref{eq:mi} to define our clustering objective function with respect to parameters  of the classifier. In this work, we employ a deep net as the classifier  on top of the embedding layer () to jointly perform the training. We invert the corresponding expression into a minimization loss function, and optimizing the model using the stochastic gradient descent:



\begin{lstinputlisting}[language=Python,label={lst: psedocode},float=bp, caption=Pseudo-code written in Python presents the clustering objective based on Pytorch library \cite{NEURIPS2019_9015}.]{sup_includes/psedocode.py}
\end{lstinputlisting}

where ,  denotes the computed codes,  the classifier's prediction,  and  the two marginal terms. Here,  indicates the number of elements in the mini-batch and indices  and  denote the corresponding transformed views of the mini-batch, as illustrated in Fig. \ref{fig:framework}, where  indicates the total number of transformed views obtained from instances in the mini-batch. The scalar  denotes an introduced weight parameter of the entropy term , that  encourages the classifier to uniformly assign the predicted class indices and thus prevent degeneracy. The conditional entropy term is minimized by increasing the classifier's prediction confidence. In Listing \ref{lst: psedocode}, we provide the pseudo-code of the clustering objective function as described in Eq. \ref{eq:mi v2}, which returns a quantitative measurement of the dependency between the classifier's predictions () and the distribution (). The pseudo-code is written in Python and includes commands extended in the Pytorch library \cite{NEURIPS2019_9015}. The final training objective with respect to all parameters of the overall framework  is defined as follows: 

The internal computation of the IMC-SwAV is presented in Algorithm \ref{alg:framework} and the full training process, including all steps for the two transformed views, are given in Algorithm 2.



\SetKwInOut{Initialize}{Initialize}
\SetKwInOut{Generate}{Generate}

\begin{algorithm}[t]
\SetKwFunction{FMain}{Framework}
    \SetKwProg{Fn}{Function}{:}{}
    \Fn{\FMain{}}{
        \hspace{0.1mm}  apply transformation functions\\
        \hspace{0.1mm}  generate the representations\\
        \hspace{0.1mm}  compute the distribution \\
        \hspace{0.1mm}  classifier's output \\
        \hspace{0.1mm}   derived by  through Shinkhorn-Knopp \cite{NIPS2013_af21d0c9}\\
 \Return  ,   ,  
}
\textbf{End Function}


\caption{Framework method}
\label{alg:framework}
\end{algorithm}


\begin{algorithm}[t]
\KwData{}
\Initialize{}
\While{Convergence condition not satisfied}{
  \hspace{0.7mm}{ \textbullet \hspace{0.1mm} Sample minibatch } \\
  \hspace{1.mm}{ \Comment{First Views} }\\
  \hspace{0.7mm}{ \textbullet \hspace{0.1mm}  ,   ,    \FMain{} }\\ 
  \hspace{1.mm}{ \Comment{Second Views} }\\
  \hspace{0.7mm}{ \textbullet \hspace{0.1mm}  ,   ,    \FMain{}} \\ \\
   \hspace{1.mm}{ \Comment{Swap loss} }
 \\ 
\hspace{1.mm}{ \Comment{Clustering loss} }\\


\textbullet \hspace{0.1mm} Jointly update the framework's  parameters to minimize the combined training objective : \\

}
\caption{The proposed joint training algorithm.}
\label{alg:Process}
\end{algorithm}

In the following, we briefly discuss the reason for using mutual information in the objective function. Recall that the convergence of the SwAV \cite{caron2020unsupervised} framework is achieved by gradually dividing the full dataset into  partitions, where  is the number of defined prototypes, which is usually large. Thus, on average  instances are assigned to each partition. By maximizing the dependency between the distribution  and the classifier's prediction , the classifier converges by identifying partitions that hold similar semantics in  and grouping them to the same cluster in . The marginal entropy in Equation \ref{eq:mi v2} regularizes the classifier to ensure that each cluster is approximately assigned  elements. On the other hand, the classifier increases its confidence on its predictions by minimizing the conditional entropy. Despite its simplicity, the experimental results presented in Section \ref{sec:results} indicate that the proposed clustering objective can effectively leverage the derived distribution  to achieve high performance. \par

\subsection{Multi-Crop Strategy} 
In this work, we adopt the same multi-crop views training strategy as introduced in \cite{caron2020unsupervised} for a full exploitation of the proposed framework. Each training instance is transformed into a set of augmented views. The first two main transformed views are cropped in a negligible lower resolution of the original image, we later called this transformation as high resolution views. The additional mini-cropped views cover only a small part of the image with their resolution to be approximately the half of the original image size to reduce the convolutional operations and thus the time complexity. We named this transformation as low resolution views. During the training,  target distributions are computed only for the high resolution views, and used across all augmented views. We extend this process also to the proposed clustering objective (Eq. \ref{eq:mi v2}). Here, we compute pairwise the MI quantity based on all views; hence, if a mini-batch is transformed into two high and two low resolution views, so , resulting in 16 combinations in total. It is found that the multi-crop strategy, as demonstrated in our empirical studies, effectively enhances the model's prediction capability.


\section{Experimental Studies}
\label{sec:results}

We evaluate the proposed clustering method, IMC-SwAV, on  four challenging colour image datasets: 1) CIFAR-10, which contains 10 equally populated classes; 2) CIFAR-100 containing 100 classes with all elements being uniform distributed. These 100 classes are also grouped into 20 super-classes with each super class consisting of five classes. For convenience, we refer to the 20 super-classes as CIFAR-100/20; 3) STL10, which contains labels only for the 13000 images and the remaining unlabeled images are from various classes. The encoder is trained across the labeled and unlabeled instances, where the classifier is trained and evaluated only for the labeled instances; and 4) Tiny-ImageNet/200, which is a subset of ImageNet containing 200 classes downsampled to a lower resolution.  Table \ref{Dataset description} presents the details of each set, including the number of training and validation elements, the number of clusters, the image resolution, and the multi-crop ranges during the training. By ``2x28+4x18", we mean two high resolution views of crop size , and four low resolution views of crop size . 

The proposed algorithm is compared with a range of state-of-the-art visual clustering approaches in terms of unsupervised learning metrics. We further demonstrate the effectiveness of the proposed single-phase method on datasets with up to 200 classes. Ablation studies are also performed to evaluate the effects of different hyper-parameters to examine the role of the most important components of the proposed algorithm and the sensitivity to the key parameters.

\subsection{Metrics for Unsupervised Learning}

Three quantitative  metrics for supervised learning are adopted to measure the performance of IMC-SwAV: 1) Accuracy (ACC); 2) Normalized mutual information (NMI) \cite{vinh2009information}; and 3) Adjusted rand index (ARI). The ACC reports the accuracy by finding the best mapping between the predicted labels and the ground truth labels. The mapping matrix is found using the Hungarian algorithm \cite{doi:10.1002/nav.3800020109}. NMI measures the mutual information between the two distributions (the model's prediction and the ground truth), which is scaled between zero and one. ARI measures the similarity between the two particular distributions by comparing all possible pairs and measuring those assigned in the same cluster and those in different ones.

\begin{table}[!]
\caption{Descriptions of the Datasets}
\resizebox{\columnwidth}{!}{
\begin{tabular}{lcccccccccccc}
\hline
\textbf{Name} & \begin{tabular}{@{}c@{}}\textbf{Train.} \\ \textbf{No.}\end{tabular} & \begin{tabular}{@{}c@{}}\textbf{Val.} \\ \textbf{No.}\end{tabular} & \begin{tabular}{@{}c@{}}\textbf{Classes} \\ \textbf{()}\end{tabular}& \textbf{Res.} & \begin{tabular}{@{}c@{}}\textbf{Multi-}\\ \textbf{\textbf{crops}}\end{tabular} \\ \hline
\textbf{CIFAR-10} & 50000 & 10000 & 10 & 32x32& 
\begin{tabular}{@{}c@{}} 2x28+\\4x18 \end{tabular} \\
\begin{tabular}{@{}l@{}}\textbf{CIFAR-100}\\ \textbf{\textbf{CIFAR-100/20}}\end{tabular}& 50000& 10000 & \begin{tabular}{@{}c@{}} 100\\20\end{tabular} & 32x32 & \begin{tabular}{@{}c@{}} 2x28+\\4x18 \end{tabular} \\
\textbf{STL10} & 105000 & 8000 & 10 & 
96x96 & \begin{tabular}{@{}c@{}} 2x76+\\4x52 \end{tabular} \\ 
\textbf{Tiny-ImageNet} & 100000 & 10000 & 200 & 
64x64 & \begin{tabular}{@{}c@{}} 2x56+\\4x36 \end{tabular} \\\hline
\end{tabular}
}
\label{Dataset description}
\end{table}


\begin{table*}[t]
\caption{Comparative results on the three benchmarks. The top three methods in terms of the best results highlighted. Note that our method is evaluated for  independent runs across all datasets, and the average and best results are reported.}
\footnotesize
\begin{center}
\setlength\tabcolsep{1.5pt}
\begin{tabularx}{\textwidth}{l| Y Y Y|Y Y Y|Y Y Y|Y Y Y}
\hline

\multirow{2}{*}{\textbf{Method/Dataset}}& \multicolumn{3}{c}{\textbf{CIFAR-10}}  & \multicolumn{3}{c}{\textbf{CIFAR-100/20}}  & \multicolumn{3}{c}{\textbf{STL10}} & \multicolumn{3}{c}{\textbf{Tiny-ImageNet/200}}\\

\cline{2-13}
& ACC & NMI & ARI &  ACC & NMI & ARI &  ACC & NMI & ARI &  ACC & NMI & ARI \\ \hline

K-means & 22.9 & 8.7 & 4.9 & 13.0 & 8.4 & 2.8 & 19.2 & 12.5 & 6.1 & 2.5 & 6.5 & 0.5 \\

AE  \cite{NIPS2006_3048} & 31.4 & 23.9 & 16.9 & 16.5  &10.0 & 4.8 & 30.3&25.0 &16.1 & 4.1 & 13.1 & 0.7 \\

VAE \cite{Kingma2014} & 29.1 & 24.5 & 16.7 & 15.2 & 10.8 & 4.0 & 28.2 & 20.0 & 14.6 & 3.6 & 11.3 & 0.6\\

DCGAN \cite{DBLP:journals/corr/RadfordMC15} & 31.5 & 26.5 & 17.6 & 15.1& 12.0&4.5 &29.8 &21.0 &13.9 & 4.1 & 13.5 & 0.7\\

DEC \cite{pmlr-v48-xieb16} & 30.1 & 25.7 & 16.1 &  18.5&  13.6 & 5.0 & 35.9 & 27.6 & 18.6 & - & - & -\\

JULE \cite{Yang2016JointUL} & 27.2 & 19.2 & 13.8 & 13.7 & 10.3 & 3.3 & 27.7 &  18.2 & 16.4 & 3.3 & 10.2 & 0.6\\

ADC \cite{DBLP:conf/dagm/HausserPGAC18} & 32.5 & - & - & 16.0 & - & - & 53.0 & - & - & - & - & -\\

DAC \cite{DBLP:conf/iccv/ChangWMXP17} & 52.2 & 39.6 & 30.6 & 23.8 & 18.5 & 8.8 & 47.0 & 36.6 & 25.7 & 6.6 & 19.0 & 1.7 \\

IMSAT-DCGAN \cite{ntelemis2020image} & 70.0 & - & - & 32.4 & - & - & 58.7 & - & - & - & - & -\\

DDC \cite{2019arXiv190501681C} & 52.4 & 42.4 & 32.9 & - & - & - & 48.9 & 37.1 & 26.7 & - & - & -\\

DCCM \cite{DCCM} & 62.3 & 49.6 & 40.8 & 32.7 & 28.5 & 17.3 & 48.2 & 37.6 & 26.2 & \textbf{10.8} & 22.4 & 3.8\\

IIC \cite{Ji_2019_ICCV} & 61.7 & 51.1 & 41.1 & 25.7 & 22.5 & 11.7 & 59.6 & 49.6 & 39.7 & - & - & -\\

DCCS \cite{zhao2020deep} & 65.6 & 56.9 & 46.9 & - & - & - & 53.6 & 49.0 & 36.2 & - & - & -\\

PICA \cite{huang2020pica} & 69.6 & 59.1 & 51.2 & 33.7 & 31.0 & 17.1 & 71.3 & 61.1 & 53.1 & 9.8 & \textbf{27.7} & \textbf{4.0} \\

DRC \cite{zhong2020deep}  & 72.7 & 62.1 & 54.7 & 36.7 & 35.6 & 20.8 & 74.7 & 64.4 & 56.9 & - & - & -\\

EmbedUL \cite{DBLP:conf/eccv/HanPPKC20} & \textbf{81.0} & - & - & 35.3 & - & - & 66.5 & - & - & - & - & -\\

CC \cite{li2020contrastive} & 79.0 & \textbf{70.5} & \textbf{63.7} & \textbf{42.9} & \textbf{43.1} & \textbf{26.6} & \textbf{85.0} & \textbf{76.4} & \textbf{72.6} & \textbf{14.0} & \textbf{34.0} & \textbf{7.1}\\

SCAN \cite{vangansbeke2020scan} (Best) & \textbf{88.3} & \textbf{79.7} & \textbf{77.2} & \textbf{50.7 }& \textbf{48.6} & \textbf{33.3} & \textbf{80.9} & \textbf{69.8}  & \textbf{64.6} & - & - & -\\

\hline 

Supervised & 92.8 & - & - & 76.1  & - & - & 89.2 & - & - & 45.4 & - & -\\
SwAV\cite{caron2020unsupervised} + K-means & 78.4 & 67.5 & 61.3 & 40.1 & 47.0 & 10.6 & 74.9 & 70.5  & 54.0 & 18.2 & 46.8 & 5.2 \\

IMC-SwAV  (Avg) & \textbf{89.1}0.5 & \textbf{81.1}0.7 & \textbf{79.0}1.0 & \textbf{49.0}1.8 & \textbf{50.3}1.2 & \textbf{33.7}1.3 & \textbf{83.1}1.0 & \textbf{72.9}0.9 & \textbf{68.5}1.4 & \textbf{27.9}0.3 & \textbf{48.5}2.0 & \textbf{14.3}2.1 \\

IMC-SwAV  (Best) & \textbf{89.7} & \textbf{81.8} & \textbf{80.0} & \textbf{51.9} & \textbf{52.7} & \textbf{36.1} & \textbf{85.3} & \textbf{74.7}  & \textbf{71.6} & \textbf{28.2} & \textbf{52.6} & \textbf{14.6} \\

\hline
\end{tabularx}
\label{tab:Unsupervised Clustering}
\end{center}
\end{table*}

\subsection{Experimental Settings}

\subsubsection{Architecture Implementation} 
For fair comparisons, we adopt the same settings across all datasets as those given in \cite{caron2020unsupervised,pmlr-v119-chen20j}. Specifically, the proposed framework implements an encoder based on ResNet18 \cite{he2015deep} architecture. The classifier employs a multilayer perceptron net containing two hidden layers on top of the embedding layer. Similar to  \cite{caron2020unsupervised,pmlr-v119-chen20j}, we use a projection head to reduce the dimension of the embedding layer prior to the comparison with the prototypes. For the overall framework, the Adam optimizer \cite{DBLP:journals/corr/KingmaB14} is adopted for training. The learning rate is set to  with a warmup schedule in the first  training iterations. We use a decay learning rate of  in epochs  and a total of  epochs is run. The mini-batch size is set to  across all datasets except Tiny-ImageNet/200, for which the batch size was set  due to the large number of classes in this dataset. A  weight decay regularizer is used with a rate of . We apply an additional two-sided regularizer, in a similar form to that reported in \cite{NEURIPS2019_ddf35421,NEURIPS2020_f3ada80d}, to the logit outputs of the classifier's representation. Specifically, we penalize any absolute value higher than five prior to the Softmax activation function, hence preventing the classifier from making predictions with a high confidence level in the early stage of the training phase as:

where  is the logit outputs of the classifier,   is a weight parameter set to  across all experiments,  is the batch size, and  is the number of clusters.

\subsubsection{Hyper-Parameters Selection} 
The settings below are applied across all experiments. The temperature scalar  in Softmax smoothness is set to  as recommended in \cite{caron2020unsupervised}. Similarly, to prevent degeneracy, we set the hyperparameter  of the weighted entropy of Sinkhorn-Knopp to  and optimize it for three iterations. The implemented prototype vectors are set to  (except in the ablation studies). In the clustering objective, we set the weight factor of the marginal entropy  for all experiments.

\subsubsection{Transformation Functions} 
In this work, we follow the same transformation/augmentation scheme as presented in \cite{pmlr-v119-chen20j} across all experiments. Each instance is horizontally flipped at a probability of . All images are modified with color jittering at a rate: brightness , contrast , saturation , and hue . Color jitter is applied at a probability of . A probability of  is also used for a gray-scale instance transformation. The re-sizing rates are set to the range between  and , between  and , respectively, for the two main views (of high resolution) and four smaller views (of low resolution). The aspect ratio in both cases is set to . The corresponding crop sizes are given in Table \ref{Dataset description}.


\subsection{Comparative Results}

Table \ref{tab:Unsupervised Clustering} presents the results over  independent runs. To demonstrate the stability of the proposed IMC-SwAV, in each run the model's parameters are randomly initialized. The same process is followed across all datasets. We present the mean, standard deviation (STD), and the best result. In contrast to the previous clustering studies in which all instances are used for training and validation, we use a similar validation process as that in \cite{10.1007/978-3-030-58607-2_16}, where the validation results are derived from the validation data that has not been seen by the model. During the training, all components of the framework are jointly trained with training instances only. We list the results obtained by a broad range of  state-of-the-art unsupervised learning algorithms for comparison. It should be stressed that the ground truth is used only for computing the relevant metrics. To further demonstrate the effectiveness of our approach, we freeze the encoder parameters and train a single layer net on top of the embedding layer in a supervised manner. We present these supervised learning result to further demonstrate the capability of the proposed unsupervised clustering approach. Finally, we compare the clustering results of K-means by using the same features extracted by the SwAV method. We also report the best results of "SwAV + K-means'' over 20 initializations of K-means' centroids. \par

As presented in Table \ref{tab:Unsupervised Clustering}, the average performance of IMC-SwAV outperforms all state-of-the-art methods on CIFAR-10, CIFAR-100/20, and Tiny-ImageNet and produces competitive results on STL10, while the best results of our method are the best among all algorithms under comparison. Note that only the best results of all algorithms under comparison are reported in Table \ref{tab:Unsupervised Clustering}. IMC-SwAV performs clustering based on the online mode in the \textit{swap} training strategy without the requirement of a pre-trained model or the implementation of multi-phase strategies, thus providing an additional advantage in terms of simplicity and training time. We observe a large margin between the clustering performance of IMC-SwAV and SwAV + K-means. Since both methods are trained on the same features, this performance improvement can be attributed to the defined clustering objective and the jointly training proposed in this work. Furthermore, the increased number of parameters of the classifier has a low computational impact during the training, with the training time on a Nvidia Quadro RTX6000 GPU being increased by  only. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.4 \textwidth]{figs/confusion_matrix.png}
    \caption{The above confusion matrix showing the predictions and ground truth made by the proposed model on CIFAR-10 validation set. 
    }
   \label{fig:confusion matrix}
\end{figure}

In the following, we further demonstrate our model performance by evaluating the individual classes and instances.

\subsubsection{Individual class performance} 
To examine the performance of individual class accuracies on CIFAR-10, a confusion matrix with the predictions made by IMC-SwAV on the unseen validation set is presented in Fig. \ref{fig:confusion matrix}. Note that in contrast to the previous methods, none of the implemented components are trained on the validation set. Each class consists of  instances. In the figure,  -axis and  -axis indicates the model's predictions and the ground truth, respectively. From these results, it can be seen that IMC-SwAV shows a high accuracy on the majority of the classes. Mismatch inaccurate predictions are mainly observed between classes `Cat' and `Deer'.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.45 \textwidth]{figs/gradcam.png}
    \caption{This illustration is an interpretation of visual activation heatmap of accurate (positive - green frame) and inaccurate (negative - red frame) prediction made by our model on STL10.
    }
   \label{fig:gradcam}
\end{figure}

\subsubsection{Visual interpretation} 
We visualize the predictions made by IMC-SwAV on STL10 dataset in Fig. \ref{fig:gradcam}. The predictions are visualized via Grad-Cam \cite{8237336}. IMC-SwAV is able to find specific visual elements to achieve the successful predictions such as airplane wings or horse's body shape. On the other hand, from the visual heatmap layers of the negative predictions, we see that the model fails to recognize specific object elements. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.45 \textwidth]{figs/sample_size.png}
    \caption{Performance of IMC-SwAV made on CIFAR-10 for a limited number of training elements. x-axis presents the number of training samples (by thousand) per class.
    }
   \label{fig:sample size}
\end{figure}

\begin{table}[t]
\caption{CIFAR100 - Evaluation on 100 Classes}
\resizebox{\columnwidth}{!}{
\begin{tabular}{lcccc}
\hline
\textbf{} & \textbf{Top-1 ACC} & \textbf{Top-5 ACC}  & \textbf{NMI} & \textbf{ARI} \\ \hline
IMC-SwAV & 45.1 & 67.5 & 60.8 & 30.7 \\
SwAV + K-means & 30.2& 48.6 & 56.25 & 12.3 \\
\hline
\end{tabular}
}
\label{tab: cifar100}
\end{table}

\subsubsection{Influence of sample size}  
We vary the number of the training instances per class by an increasing interval of  per experiment within the range of , where 5000 is the maximum number of samples in each CIFAR-10 class (recall that CIFAR-10's classes are equally spread  with each class containing  element). The model is trained on the selected subset only, and the performance of validation set is illustrated in Fig. \ref{fig:sample size}. 
As expected, the model's precision is gradually decreasing when fewer training samples are considered. Notably, IMC-SwAV's performance remains stable when the sample size is larger than .

\subsubsection{Effectiveness to a large number of clusters} 
To further validate our method on a dataset with a large number of clusters, we conduct an experiment on the 100 sub-class of CIFAR-100 to  evaluate the clustering performance of IMC-SwAV. The features extracted by IMC-SwAV are directly compared with those obtained by the K-means algorithm. The metrics of Top-1 and Top-5 accuracy, as well as the rest unsupervised metrics are listed in Table \ref{tab: cifar100}. 
These results demonstrate that IMC-SwAV performs well on this highly demanding clustering task. The proposed algorithm achieves a large margin of  on Top-1 ACC and  on Top-5 ACC. 




\subsection{Ablation Studies}

In this subsection, we experiment with a variety of hyper-parameters to evaluate the effectiveness of the proposed IMC-SwAV. All parameters remain unchanged, as per the main settings, except for those to be studied. \par



\begin{figure}[t]
    \centering
    \includegraphics[width=0.45 \textwidth]{figs/prototypes.png}
    \caption{These diagrams plot the performance metrics on CIFAR-10 over different numbers of prototypes. From top to low: ACC, NMI and ARI. All figures report the number of prototypes in x-axis.
    }
   \label{fig:prototypes illustration}
\end{figure} 

\begin{table}[t]
\caption{IMC-SwAV with different ResNet architectures indicated by the method suffix}
\setlength\tabcolsep{3.5pt}
\begin{tabularx}{250.pt}{l|YYY|YYY|YYY}
\hline

\multirow{2}{*}{\begin{tabular}{@{}c@{}}\end{tabular}}& \multicolumn{3}{c}{\textbf{CIFAR-10}}  & \multicolumn{3}{c}{\textbf{CIFAR-100/20}}  & \multicolumn{3}{c}{\textbf{STL10}} \\

\cline{2-10}
& ACC & NMI & ARI &  ACC & NMI & ARI &  ACC & NMI & ARI \\ \hline

IMC-SwAV-18 (Avg) & 89.1  & 81.1  & 79.0 & 49.0 & 50.3 & 33.7 & 83.1  & 72.9  & 68.5 \\
IMC-SwAV-18 (Best) & 89.7 & 81.8 & 80.0 & 51.9 & 52.7 & 36.1 & 85.3 & 74.7 & 71.6 \\

IMC-SwAV-34 (Avg) & 89.5 & 81.7 & 79.9 & 50.2 & 51.2 & 34.6  & 84.5 & 74.8  & 70.9 \\
IMC-SwAV-34 (Best)  & 90.2 & 82.4 & 80.9 & 52.1 & 53.2 & 36.3 & 86.0 & 76.7  & 73.3 \\
IMC-SwAV-50 (Avg) & 91.0 & 83.8 & 82.6  & 51.2  & 52.6 & 35.6 &  86.3 & 77.3  & 73.8 \\
IMC-SwAV-50 (Best) & 91.4 & 84.1 & 82.9 & 52.7 & 54.0 & 37.0 & 87.1 & 77.9  & 75.0 \\
\hline
\end{tabularx}
\label{tab: effects resnet}
\end{table}




\subsubsection{Prototypes} 
We examine the performance of IMC-SwAV by varying the number of prototypes, , in the range of  while keeping all other parameters unchanged. Figure \ref{fig:prototypes illustration} shows the results from five independent runs of the unsupervised learning task. From these results, we conclude that IMC-SwAV shows robustness over different numbers of prototypes with the lowest mean value above . We also note that for  larger than , all three performance metrics become less sensitive to . Additionally, we note that our choice to set  to  prototypes in the main settings is not the optimum in terms of all metrics on CIFAR-10. However, in unsupervised learning, the ground truth is unknown and hence it is more realistic to compare the performance without using the optimal parameter setting.


\subsubsection{Experiment with different encoder architectures}  
In this subsection, we examine the performance of the proposed IMC-SwAV by varying the encoder's architecture by using different types of the ResNet \cite{he2015deep} models. Specifically, we compare the performance of the proposed method when ResNet34 (21.3M parameters) and ResNet50 (23.5M parameters), respectively, is adopted to replace  ResNet18 (11.1M parameters) used in the main experiments. All models are evaluated over five independent runs (the results of ResNet18 are averaged over 15 runs as in the main experiment) and the average and best recorded performances are reported for each model. The reported metrics are on the unseen test subset, and all frameworks are optimised with training subset only. The results are listed in Table \ref{tab: effects resnet}. As expected, larger models achieve slightly better performance in terms of all metrics. Note also that the discrepancies between different metrics of each model are minor, with the highest differences being observed difference on CIFAR-10. 


\subsubsection{Multiple Crops} 
We demonstrate the impact of the multiple crops strategy on the clustering performance on CIFAR-10. We change the number of low resolution cropped views with all other settings unchanged. First, a basic setting with two main views of the original image dimensions is evaluated. Then, we add two, four, and six additional low resolution views, respectively. The size of the two main views is decreased to  to reduce the computation time. The resizing ratio remains unchanged:  for the two main views, and  for the low resolution views. We report the mean and STD of the metrics over five independent experiments. Additionally, the accuracy of a single layer net, which is trained independently on generated features through a supervised mode, is also included. \par

\begin{table}[t]
\caption{Effectiveness of Multiple Crops Strategies}
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|ccccc}
\hline
\textbf{Crops} & \textbf{ACC} & \textbf{NMI} & \textbf{ARI} & \textbf{Sup.}& \begin{tabular}{@{}c@{}}\textbf{Train} \\ \textbf{Time}\end{tabular} \\ \hline
2x32 & 78.71.0 & 66.21.1 & 61.21.3 & 86.1 &   \\

2x28+2x18 & 87.21.1  & 78.61.4 & 75.82.0 & 92.1 &  \\

2x28+4x18 & 89.10.5  & 81.10.7 & 79.01.0 & 92.8 &  \\

2x28+6x18 & 89.40.6 & 81.20.9  & 79.01.8 & 93.2 &   \\
\hline
\end{tabular}
}
\label{tab: effects multiple}
\end{table}


\begin{figure}[t]
    \centering
    \includegraphics[width=0.45 \textwidth]{figs/training_epochs.png}
    \caption{Illustration presents the accuracy per epoch of each multi-crop implementation of Table \ref{tab: effects multiple}.
    }
   \label{fig:training epochs}
\end{figure}


\begin{figure}[t]
    \centering
    \includegraphics[width=0.45 \textwidth]{figs/view_sizes.png}
    \caption{Presentation with various combinations of high and low crop sizes.  represent different low resolution crop sizes and high resolution are associated with different colours and symbols. 
    }
   \label{fig:views resolution}
\end{figure}

Table \ref{tab: effects multiple} presents the results of the multiple crop experiments. From results, we notice that the additional low resolution views increase the performance in terms of all metrics, for both the unsupervised and supervised models. The last column presents the ratio of training time based on a basic training strategy of . The training accuracy over the iterations are presented in Fig. \ref{fig:training epochs} for each experiment. We note that the mini-crop strategy of ``" already reaches satisfactory results ( ACC) within  iterations, in comparison to the full experiment of a simple setup. Recall, the two-crop strategy holds an advantage by using the original resolution of  instead of .\par

\subsubsection{Combinations of crop sizes} 
To further examine the impact of multi-crop's strategy, we vary the resolution of crop sizes of main views (high) and smaller views (low) within the intervals of  and , respectively. All experiments are performed on CIFAR-10 based on the main setting. We also preserve the same crop ratios and the number of views (two main views and four smaller views). Since, the resizing ratio of the smaller view is set to , the difference between high and low resolutions does not exceed the absolute difference of the maximum  pixels and minimum four pixels, to keep proportional to the crop size of main view. Figure \ref{fig:views resolution} presents our findings in terms of ACC. The performance of IMC-SwAV slightly degrades in a very low resolution in combination with ``", probably because the model fails to capture sufficient semantic details. On the other hand, all combinations above the crop sizes of  and  exhibit a satisfactory accuracy above .



\subsubsection{Other representation learning strategies}  
In this section, we aim to demonstrate that our clustering strategy is not restricted to the SwAV \cite{caron2020unsupervised} framework and is also applicable to other self-supervised learning strategies. Here, we evaluate our single-phase clustering method that adopts SeLa \cite{asano2020self}, another grouping based self-supervised learning method, to replace SwAV. Similar to IMC-SwAV, the components of the encoder and the classifier are jointly trained in a single-phase mode. We compare the performance of the variant of our clustering method using Sela (called IMC-SeLa) with the K-means algorithm, and a supervised classifier net trained on same extracted features. The parameters of the encoder model are optimized with the self-supervised loss function of the adopted strategy only without using any additional loss functions or pre-trained models. Similar to the main experiments, the framework is optimized on training subset and the performance evaluation is made on the validation subset. Each method is performed for five independent runs and the best result is reported. For a fair comparison, the architecture is similar to the main experiment with the encoder being based on a ResNet18 network. \par

The comparative results are listed in Table \ref{tab: sela}, from which a performance degradation of all methods under comparison can be observed, implying that the extracted features are less separable. This can be easily validated by examining the results of the supervised linear classifier net (Supervised in the Tables) trained on the extracted features (for SwAV \cite{caron2020unsupervised} in Table \ref{tab:Unsupervised Clustering} and for SeLa \cite{asano2020self} in Table \ref{tab: sela}). This can also be confirmed by the K-means clustering approach on the same features extracted by the two self-supervised techniques. 

These results indicate that the final performance of the proposed framework also depends on the performance of the self-supervised learning algorithm it adopts. Nevertheless, IMC-SeLa still outperforms "SeLA + K-means'' at a large margin on CIFAR-10, CIFAR-100/20 and Tiny-ImageNet/200. 

\begin{table}[t]
\caption{Clustering strategy adopted to SeLA training}
\setlength\tabcolsep{3.5pt}
\begin{tabularx}{250.pt}{l|YYY|YYY|YYY}
\hline

\multirow{2}{*}{\begin{tabular}{@{}c@{}}\textbf{Method/}\\ \textbf{Dataset.}\end{tabular}}& \multicolumn{3}{c}{\textbf{CIFAR-10}}  & \multicolumn{3}{c}{\textbf{CIFAR-100/20}}  & \multicolumn{3}{c}{\textbf{Tiny-ImgNet200}} \\

\cline{2-10}
& ACC & NMI & ARI &  ACC & NMI & ARI &  ACC & NMI & ARI \\ \hline

IMC-SeLa & 74.5 & 65.2 & 59.1 & 39.4 & 40.7 & 25.1 & 19.1 & 45.2 & 7.9 \\
SeLA \cite{asano2020self} + Kmeans  & 64.0 & 56.4 & 47.3 & 34.7 & 34.3 & 18.0 & 17.1 & 43.2  & 6.7 \\
Supervised  & 87.1 & - & - & 66.6 & - & - & 34.2 & -  & - \\
\hline
\end{tabularx}
\label{tab: sela}
\end{table}

\section{Conclusion}

This study presents a single-phase framework for image clustering, called IMC-SwAV.  We introduce a modified mutual information to jointly train the framework by maximizing the dependency of a self-labelling assignment strategy and an implemented classifier. IMC-SwAV achieves highly competitive performance on challenging datasets compared to the state-of-the-art. In addition to the encouraging clustering performance, we demonstrate the robustness of the proposed method to the parameter settings and the possibility to adopt a different self-supervised learning technique. \par


Although the proposed model achieves significant capabilities in clustering, the multi-crop training strategy adopted in the framework slightly increases the time complexity. Additionally, we observe a dependency of its performance on the grouping based self-supervised learning method. Our future work will aim to remove the requirement of using the multi-crop views without deteriorating the performance of the overall framework. Moreover, we will examine the effectiveness of other grouping based self-supervised learning algorithms in the proposed single-phase training framework to make it available for a wider range of applications.


{\small
\bibliographystyle{ieee_fullname}
\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{NEURIPS2019_ddf35421}
Philip Bachman, R~Devon Hjelm, and William Buchwalter.
\newblock Learning representations by maximizing mutual information across
  views.
\newblock In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\textquotesingle
  Alch\'{e}-Buc, E. Fox, and R. Garnett, editors, {\em Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.

\bibitem{NIPS2006_3048}
Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle.
\newblock Greedy layer-wise training of deep networks.
\newblock In B. Sch\"{o}lkopf, J.~C. Platt, and T. Hoffman, editors, {\em
  Advances in Neural Information Processing Systems 19}, pages 153--160. MIT
  Press, 2007.

\bibitem{BEZDEK1984191}
James~C. Bezdek, Robert Ehrlich, and William Full.
\newblock Fcm: The fuzzy c-means clustering algorithm.
\newblock {\em Computers \& Geosciences}, 10(2):191 -- 203, 1984.

\bibitem{caron2018deep}
Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze.
\newblock Deep clustering for unsupervised learning of visual features.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, September 2018.

\bibitem{caron2020unsupervised}
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and
  Armand Joulin.
\newblock Unsupervised learning of visual features by contrasting cluster
  assignments.
\newblock In H. Larochelle, M. Ranzato, R. Hadsell, M.~F. Balcan, and H. Lin,
  editors, {\em Advances in Neural Information Processing Systems}, volume~33,
  pages 9912--9924. Curran Associates, Inc., 2020.

\bibitem{2019arXiv190501681C}
Jianlong {Chang}, Yiwen {Guo}, Lingfeng {Wang}, Gaofeng {Meng}, Shiming
  {Xiang}, and Chunhong {Pan}.
\newblock {Deep Discriminative Clustering Analysis}.
\newblock {\em arXiv e-prints}, page arXiv:1905.01681, May 2019.

\bibitem{DBLP:conf/iccv/ChangWMXP17}
Jianlong Chang, Lingfeng Wang, Gaofeng Meng, Shiming Xiang, and Chunhong Pan.
\newblock Deep adaptive image clustering.
\newblock In {\em {IEEE} International Conference on Computer Vision, {ICCV}
  2017, Venice, Italy, October 22-29, 2017}, pages 5880--5888. {IEEE} Computer
  Society, 2017.

\bibitem{8237888}
J. {Chang}, L. {Wang}, G. {Meng}, S. {Xiang}, and C. {Pan}.
\newblock Deep adaptive image clustering.
\newblock In {\em 2017 IEEE International Conference on Computer Vision
  (ICCV)}, pages 5880--5888, 2017.

\bibitem{pmlr-v119-chen20j}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In Hal~Daumé III and Aarti Singh, editors, {\em Proceedings of the
  37th International Conference on Machine Learning}, volume 119 of {\em
  Proceedings of Machine Learning Research}, pages 1597--1607, Virtual, 13--18
  Jul 2020. PMLR.

\bibitem{DBLP:conf/iccv/ComaniciuM99}
Dorin Comaniciu and Peter Meer.
\newblock Mean shift analysis and applications.
\newblock In {\em Proceedings of the International Conference on Computer
  Vision, Kerkyra, Corfu, Greece, September 20-25, 1999}, pages 1197--1203.
  {IEEE} Computer Society, 1999.

\bibitem{Cover:1991:EIT:129837}
Thomas~M. Cover and Joy~A. Thomas.
\newblock {\em Elements of Information Theory}.
\newblock Wiley-Interscience, New York, NY, USA, 1991.

\bibitem{NIPS2013_af21d0c9}
Marco Cuturi.
\newblock Sinkhorn distances: Lightspeed computation of optimal transport.
\newblock In C.~J.~C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.~Q.
  Weinberger, editors, {\em Advances in Neural Information Processing Systems},
  volume~26, pages 2292--2300. Curran Associates, Inc., 2013.

\bibitem{DBLP:journals/pami/DosovitskiyFSRB16}
Alexey Dosovitskiy, Philipp Fischer, Jost~Tobias Springenberg, Martin~A.
  Riedmiller, and Thomas Brox.
\newblock Discriminative unsupervised feature learning with exemplar
  convolutional neural networks.
\newblock {\em {IEEE} Trans. Pattern Anal. Mach. Intell.}, 38(9):1734--1747,
  2016.

\bibitem{1365067}
B. {Fuglede} and F. {Topsoe}.
\newblock Jensen-shannon divergence and hilbert space embedding.
\newblock In {\em International Symposium onInformation Theory, 2004. ISIT
  2004. Proceedings.}, pages 31--, 2004.

\bibitem{10.5555/2969033.2969125}
Ian~J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David
  Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In {\em Proceedings of the 27th International Conference on Neural
  Information Processing Systems - Volume 2}, NIPS'14, page 2672–2680,
  Cambridge, MA, USA, 2014. MIT Press.

\bibitem{NEURIPS2020_f3ada80d}
Jean-Bastien Grill, Florian Strub, Florent Altch\'{e}, Corentin Tallec, Pierre
  Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila~Pires, Zhaohan
  Guo, Mohammad Gheshlaghi~Azar, Bilal Piot, koray kavukcuoglu, Remi Munos, and
  Michal Valko.
\newblock Bootstrap your own latent - a new approach to self-supervised
  learning.
\newblock In H. Larochelle, M. Ranzato, R. Hadsell, M.~F. Balcan, and H. Lin,
  editors, {\em Advances in Neural Information Processing Systems}, volume~33,
  pages 21271--21284. Curran Associates, Inc., 2020.

\bibitem{pmlr-v9-gutmann10a}
Michael Gutmann and Aapo Hyvärinen.
\newblock Noise-contrastive estimation: A new estimation principle for
  unnormalized statistical models.
\newblock In Yee~Whye Teh and Mike Titterington, editors, {\em Proceedings of
  the Thirteenth International Conference on Artificial Intelligence and
  Statistics}, volume~9 of {\em Proceedings of Machine Learning Research},
  pages 297--304, Chia Laguna Resort, Sardinia, Italy, 13--15 May 2010. JMLR
  Workshop and Conference Proceedings.

\bibitem{DBLP:conf/eccv/HanPPKC20}
Sungwon Han, Sungwon Park, Sungkyu Park, Sundong Kim, and Meeyoung Cha.
\newblock Mitigating embedding and class assignment mismatch in unsupervised
  image classification.
\newblock In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan{-}Michael
  Frahm, editors, {\em Computer Vision - {ECCV} 2020 - 16th European
  Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part {XXIV}},
  volume 12369 of {\em Lecture Notes in Computer Science}, pages 768--784.
  Springer, 2020.

\bibitem{eccv_han}
Sungwon Han, Sungwon Park, Sungkyu Park, Sundong Kim, and Meeyoung Cha.
\newblock Mitigating embedding and class assignment mismatch in unsupervised
  image classification.
\newblock In {\em Computer Vision -- ECCV 2020}, 10 2020.

\bibitem{DBLP:conf/dagm/HausserPGAC18}
Philip H{\"{a}}usser, Johannes Plapp, Vladimir Golkov, Elie Aljalbout, and
  Daniel Cremers.
\newblock Associative deep clustering: Training a classification network with
  no labels.
\newblock In Thomas Brox, Andr{\'{e}}s Bruhn, and Mario Fritz, editors, {\em
  Pattern Recognition - 40th German Conference, {GCPR} 2018, Stuttgart,
  Germany, October 9-12, 2018, Proceedings}, volume 11269 of {\em Lecture Notes
  in Computer Science}, pages 18--32. Springer, 2018.

\bibitem{He_2020_CVPR}
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR)}, June 2020.

\bibitem{he2015deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition, 2015.

\bibitem{7780459}
K. {He}, X. {Zhang}, S. {Ren}, and J. {Sun}.
\newblock Deep residual learning for image recognition.
\newblock In {\em 2016 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 770--778, 2016.

\bibitem{Heller2005BayesianHC}
Katherine~A. Heller and Zoubin Ghahramani.
\newblock Bayesian hierarchical clustering.
\newblock In {\em ICML '05}, 2005.

\bibitem{10.1162/neco.2006.18.7.1527}
Geoffrey~E. Hinton, Simon Osindero, and Yee-Whye Teh.
\newblock A fast learning algorithm for deep belief nets.
\newblock {\em Neural Comput.}, 18(7):1527–1554, July 2006.

\bibitem{HintonSalakhutdinov2006b}
G~E Hinton and R~R Salakhutdinov.
\newblock Reducing the dimensionality of data with neural networks.
\newblock {\em Science}, 313(5786):504--507, July 2006.

\bibitem{Hu2017}
Weihua Hu, Takeru Miyato, Seiya Tokui, Eiichi Matsumoto, and Masashi Sugiyama.
\newblock Learning discrete representations via information maximizing
  self-augmented training.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning - Volume 70}, ICML'17, pages 1558--1567. JMLR.org, 2017.

\bibitem{haris_iqbal_2018_2526396}
Haris Iqbal.
\newblock Harisiqbal88/plotneuralnet v1.0.0, Dec. 2018.

\bibitem{Ji_2019_ICCV}
Xu Ji, Joao~F. Henriques, and Andrea Vedaldi.
\newblock Invariant information clustering for unsupervised image
  classification and segmentation.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision (ICCV)}, October 2019.

\bibitem{huang2020pica}
Shaogang~Gong Jiabo~Huang and Xiatian Zhu.
\newblock Deep semantic clustering by partition confidence maximisation.
\newblock In {\em Proceedings of IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, 2020.

\bibitem{DBLP:conf/aistats/KhemakhemKMH20}
Ilyes Khemakhem, Diederik~P. Kingma, Ricardo~Pio Monti, and Aapo
  Hyv{\"{a}}rinen.
\newblock Variational autoencoders and nonlinear {ICA:} {A} unifying framework.
\newblock In Silvia Chiappa and Roberto Calandra, editors, {\em The 23rd
  International Conference on Artificial Intelligence and Statistics, {AISTATS}
  2020, 26-28 August 2020, Online [Palermo, Sicily, Italy]}, volume 108 of {\em
  Proceedings of Machine Learning Research}, pages 2207--2217. {PMLR}, 2020.

\bibitem{DBLP:journals/corr/KingmaB14}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In Yoshua Bengio and Yann LeCun, editors, {\em 3rd International
  Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May
  7-9, 2015, Conference Track Proceedings}, 2015.

\bibitem{Kingma2014}
Diederik~P. Kingma and Max Welling.
\newblock {Auto-Encoding Variational Bayes}.
\newblock In {\em 2nd International Conference on Learning Representations,
  {ICLR} 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track
  Proceedings}, 2014.

\bibitem{NIPS2012_c399862d}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In F. Pereira, C.~J.~C. Burges, L. Bottou, and K.~Q. Weinberger,
  editors, {\em Advances in Neural Information Processing Systems}, volume~25,
  pages 1097--1105. Curran Associates, Inc., 2012.

\bibitem{doi:10.1002/nav.3800020109}
H.~W. Kuhn.
\newblock The hungarian method for the assignment problem.
\newblock {\em Naval Research Logistics Quarterly}, 2(1‐2):83--97, 1955.

\bibitem{li2020contrastive}
Yunfan Li, Peng Hu, Zitao Liu, Dezhong Peng, Joey~Tianyi Zhou, and Xi Peng.
\newblock Contrastive clustering.
\newblock {\em Proceedings of the AAAI Conference on Artificial Intelligence},
  35(10):8547--8555, May 2021.

\bibitem{10.1007/978-3-319-46466-4_5}
Mehdi Noroozi and Paolo Favaro.
\newblock Unsupervised learning of visual representations by solving jigsaw
  puzzles.
\newblock In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors,
  {\em Computer Vision -- ECCV 2016}, pages 69--84, Cham, 2016. Springer
  International Publishing.

\bibitem{ntelemis2020image}
Foivos Ntelemis, Yaochu Jin, and Spencer~A. Thomas.
\newblock Image clustering using an augmented generative adversarial network
  and information maximization.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  pages 1--14, 2021.

\bibitem{NEURIPS2019_9015}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
  Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
  Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith
  Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\' Alch\'{e}-Buc,
  E. Fox, and R. Garnett, editors, {\em Advances in Neural Information
  Processing Systems 32}, pages 8024--8035. Curran Associates, Inc., 2019.

\bibitem{peyre2020computational}
Gabriel Peyré and Marco Cuturi.
\newblock Computational optimal transport, 2020.

\bibitem{DBLP:journals/corr/RadfordMC15}
Alec Radford, Luke Metz, and Soumith Chintala.
\newblock Unsupervised representation learning with deep convolutional
  generative adversarial networks.
\newblock In Yoshua Bengio and Yann LeCun, editors, {\em 4th International
  Conference on Learning Representations, {ICLR} 2016, San Juan, Puerto Rico,
  May 2-4, 2016, Conference Track Proceedings}, 2016.

\bibitem{Ren2020DeepDI}
Yazhou Ren, Ni Wang, Mingxia Li, and Zenglin Xu.
\newblock Deep density-based image clustering.
\newblock {\em Knowledge-Based Systems}, 197:105841, 2020.

\bibitem{8237336}
R.~R. {Selvaraju}, M. {Cogswell}, A. {Das}, R. {Vedantam}, D. {Parikh}, and D.
  {Batra}.
\newblock Grad-cam: Visual explanations from deep networks via gradient-based
  localization.
\newblock In {\em 2017 IEEE International Conference on Computer Vision
  (ICCV)}, pages 618--626, 2017.

\bibitem{Steinbach2004}
Michael Steinbach, Levent Ert{\"o}z, and Vipin Kumar.
\newblock {\em The Challenges of Clustering High Dimensional Data}, pages
  273--309.
\newblock Springer Berlin Heidelberg, Berlin, Heidelberg, 2004.

\bibitem{7298594}
C. {Szegedy}, {Wei Liu}, {Yangqing Jia}, P. {Sermanet}, S. {Reed}, D.
  {Anguelov}, D. {Erhan}, V. {Vanhoucke}, and A. {Rabinovich}.
\newblock Going deeper with convolutions.
\newblock In {\em 2015 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 1--9, 2015.

\bibitem{2018arXiv180703748V}
Aaron {van den Oord}, Yazhe {Li}, and Oriol {Vinyals}.
\newblock {Representation Learning with Contrastive Predictive Coding}.
\newblock {\em arXiv e-prints}, page arXiv:1807.03748, July 2018.

\bibitem{10.1007/978-3-030-58607-2_16}
Wouter Van~Gansbeke, Simon Vandenhende, Stamatios Georgoulis, Marc Proesmans,
  and Luc Van~Gool.
\newblock Scan: Learning to classify images without labels.
\newblock In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm,
  editors, {\em Computer Vision -- ECCV 2020}, pages 268--285, Cham, 2020.
  Springer International Publishing.

\bibitem{vangansbeke2020scan}
Wouter Van~Gansbeke, Simon Vandenhende, Stamatios Georgoulis, Marc Proesmans,
  and Luc Van~Gool.
\newblock Scan: Learning to classify images without labels.
\newblock In {\em Proceedings of the European Conference on Computer Vision},
  2020.

\bibitem{Vincent2008}
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol.
\newblock Extracting and composing robust features with denoising autoencoders.
\newblock In {\em Proceedings of the 25th International Conference on Machine
  Learning}, ICML '08, page 1096–1103, New York, NY, USA, 2008. Association
  for Computing Machinery.

\bibitem{vinh2009information}
Nguyen~Xuan Vinh, Julien Epps, and James Bailey.
\newblock Information theoretic measures for clusterings comparison: is a
  correction for chance necessary?
\newblock In {\em ICML '09: Proceedings of the 26th Annual International
  Conference on Machine Learning}, pages 1073--1080, New York, NY, USA, 2009.
  ACM.

\bibitem{Williams1999AMA}
Christopher K.~I. Williams.
\newblock A mcmc approach to hierarchical mixture modelling.
\newblock In {\em NIPS}, 1999.

\bibitem{DCCM}
Jianlong Wu, Keyu Long, Fei Wang, Chen Qian, Cheng Li, Zhouchen Lin, and
  Hongbin Zha.
\newblock Deep comprehensive correlation mining for image clustering.
\newblock In {\em International Conference on Computer Vision}, 2019.

\bibitem{pmlr-v48-xieb16}
Junyuan Xie, Ross Girshick, and Ali Farhadi.
\newblock Unsupervised deep embedding for clustering analysis.
\newblock In Maria~Florina Balcan and Kilian~Q. Weinberger, editors, {\em
  Proceedings of The 33rd International Conference on Machine Learning},
  volume~48 of {\em Proceedings of Machine Learning Research}, pages 478--487,
  New York, New York, USA, 20--22 Jun 2016. PMLR.

\bibitem{Yang2016JointUL}
Jianwei Yang, Devi Parikh, and Dhruv Batra.
\newblock Joint unsupervised learning of deep representations and image
  clusters.
\newblock {\em 2016 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pages 5147--5156, 2016.

\bibitem{asano2020self}
Asano YM., Rupprecht C., and Vedaldi A.
\newblock Self-labelling via simultaneous clustering and representation
  learning.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{10.1007/978-3-642-33718-5_31}
Wei Zhang, Xiaogang Wang, Deli Zhao, and Xiaoou Tang.
\newblock Graph degree linkage: Agglomerative clustering on a directed graph.
\newblock In Andrew Fitzgibbon, Svetlana Lazebnik, Pietro Perona, Yoichi Sato,
  and Cordelia Schmid, editors, {\em Computer Vision -- ECCV 2012}, pages
  428--441, Berlin, Heidelberg, 2012. Springer Berlin Heidelberg.

\bibitem{zhao2020deep}
Junjie Zhao, Donghuan Lu, Kai Ma, Yu Zhang, and Yefeng Zheng.
\newblock Deep image clustering with category-style representation.
\newblock In {\em European Conference on Computer Vision (ECCV)}, 2020.

\bibitem{DBLP:journals/ijcv/ZhaoXWWTL20}
Yue Zhao, Yuanjun Xiong, Limin Wang, Zhirong Wu, Xiaoou Tang, and Dahua Lin.
\newblock Temporal action detection with structured segment networks.
\newblock {\em Int. J. Comput. Vis.}, 128(1):74--95, 2020.

\bibitem{zhong2020deep}
Huasong Zhong, Chong Chen, Zhongming Jin, and Xian-Sheng Hua.
\newblock Deep robust clustering by contrastive learning, 2020.

\end{thebibliography}
}

\end{document}
