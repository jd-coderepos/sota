
Neural architecture search (NAS) is a field to automatically explore the optimal architecture~\citep{DBLP:conf/iclr/ZophL17,DBLP:conf/iclr/BakerGNR17}. The search procedure can be divided into three components: search space, optimization approaches, architecture evaluation. Search space defines all network candidates that can be examined to produce the final
architecture. The optimization method dictates how to explore the search space. Architecture evaluation is responsible for helping the optimization method to evaluate the quality of architectures. Recent NAS approaches have been able to generate state-of-the-art models on downstream tasks~\cite{zoph2018learning,tan2019efficientnet}. 

However, despite good performance, NAS usually requires huge computations to find the optimal architecture, most of which are used for architecture evaluation. For example, \citet{DBLP:conf/iclr/ZophL17} use 800
GPUs for 28 days resulting in 22,400 GPU-hours. ~\citet{DBLP:conf/acl/StrubellGM19} find that a single neural architecture search solution can emit as much carbon as five cars in their lifetimes.  The major recent advance of NAS is to reduce the search costs. One research line focuses on search methods without architecture evaluation, like DART~\citep{DBLP:conf/iclr/LiuSY19,chu2020fair}.  While being simple, previous studies have shown these approaches suffer from well-known performance collapse due to an inevitable aggregation of skip connections~\citep{DBLP:conf/icml/YingKCR0H19,DBLP:conf/eccv/ChuZZL20}. Another line focuses on reducing architecture evaluation costs by using techniques like early-stopping~\citep{liang2019darts}, weight-sharing~\citep{pham2018efficient}, performance predicting~\cite{DBLP:conf/eccv/LiuZNSHLFYHM18}, and so on.  Compared to the original full-training solution, these techniques achieve promising speedup. However, if we consider a complicated search space with plenty of networks, they still require many computations.




In this work, we aim to explore a more challenging question: Can we evaluate architectures without training? 
Before answering this question, it is essential to figure out how architecture affects optimization. 
It is widely accepted that gradients, induced by neural networks, play a crucial role in optimization~\citep{DBLP:journals/tnn/BengioSF94,DBLP:journals/neco/HochreiterS97,DBLP:conf/icml/PascanuMB13}. Motivated by this common belief, we propose a bold hypothesis: gradients can be used as a coarse-grained proxy of downstream training to evaluate randomly-initialized architectures. To support the hypothesis, we conduct a comprehensive theoretical analysis and identify a key gradient feature, the Gram matrix of gradients (GM). For any neural networks, the F-norm of GM decides the upper bound of convergence rate. Higher F-norm is expected for a higher convergence rate.  Intuitively, GM can be regarded as a health index of gradients. Each element in GM is the dot product between any two gradient vectors. Vanishing gradients and ataxic gradients both get lower GM scores. Following the theoretical results, we propose to leverage GM  to evaluate randomly-initialized networks. To be specific, the mean of GM, short for MGM,  is adopted in implementation. We conduct experiments on CIFAR100 and calculate the Spearman correlation scores between MGM and key optimization metrics. Experiments show that MGM has good correlations with negative training loss and validation accuracy, with 0.53  () and 0.56  () coefficients, respectively\footnote{\url{https://www.statstutor.ac.uk/resources/uploaded/spearmans.pdf}}. Figure~\ref{fig:illu} illustrates the relation between MGM and validation accuracy. These results are strong evidence to support the hypothesis, demonstrating that MGM has good potential to be used as a coarse-grained architecture evaluation. 


\begin{figure}[t]
\centering
\begin{minipage}{.23\textwidth}
\centering
\includegraphics[width=\textwidth]{fig/fo_kernel.pdf}
\label{fig:prob1_6_2}
\end{minipage}
 \hfill
\begin{minipage}{0.23\textwidth}
\centering
\includegraphics[width=\linewidth]{fig/fo_kernel_group.pdf}
\label{fig:prob1_6_1}
\end{minipage}
\caption{An illustration of the relation between MGM and accuracy. We sample 200 architectures and rank them based on MGM scores. Smaller rank represents smaller MGM. Y-axis lists the validation accuracy. The right figure classifies architectures into four groups based on their MGM ranks. Each bar represents the average accuracy. The Spearman coefficient is 0.56  (), indicating good positive correlations.  }
\label{fig:illu}
\end{figure}








According to our hypothesis, we propose a gradient kernel based NAS solution, short for KNAS. We first select top- architectures with the largest MGM as candidates, which are then trained to choose the best one. In practice,  is usually set to be a very small value. Experiments show that the new approach achieves large speedups with competitive accuracies on NAS-Bench-201~\citep{DBLP:conf/iclr/Dong020}, a NAS benchmark dataset. Furthermore, the low search cost allows us to apply KNAS to diverse tasks. We find that the structures searched by KNAS outperform strong baseline RoBERTA-large on two text classification tasks.  
























The main contributions are summarized as follows:
\begin{itemize}

\item We propose a gradient kernel hypothesis to explore training-free architecture evaluation approaches. 


\item We find a practical gradient feature MGM to support the hypothesis.


\item Based on the hypothesis, we propose a green NAS solution KNAS that can find well-performing architectures with orders of magnitude faster than ``train-then-test'' NAS solutions. 
\end{itemize}


