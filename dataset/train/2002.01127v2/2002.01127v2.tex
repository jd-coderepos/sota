

As shown in the graphical model in Figure~\ref{fig:graphic_model}, our \method modifies the vanilla VAE model by introducing two independent latent variables  and , representing \textbf{template} latent variable and \textbf{content} latent variable respectively.  models the content information in the table, while  models the sentence template information. Target sentence  is generated by both content and template variables. The two latent variables are disentangled, which makes it possible to generate diverse and relevant sentences by sampling template variable and retraining the content variable.
Considering pairwise and raw data presented in Figure \ref{fig:example_intro}, their generation process for the content latent variable  is different. 


\begin{itemize}[leftmargin=2em, topsep=0pt]
    \item For a given table-text pair , the content is observable from table . As a result,  is assumed to be deterministic given table , whose prior is defined as a delta distribution . The marginal log-likelihood is:
    
    \item For raw text , the content is unobservable with the absence of table . As a result, the content latent variable  should be sampled from prior of Gaussian distribution . The marginal log-likelihood is:
    

\end{itemize}






















In order to make full use of both table-text pair data and raw text data, the above marginal log-likelihood should be optimized jointly:


Directly optimizing Equation \ref{eq:total_loss} is intractable. 
Following the idea of variational inference~\citep{kingma2013auto}, a variational posterior  is constructed as an inference model (dashed lines in Figure \ref{fig:graphic_model}) to approximate the true posterior. Instead of optimizing the marginal log-likelihood in Equation \ref{eq:total_loss}, we maximize the evidence lower bound~(). In Section \ref{sec:object_parallel} and \ref{sec:object_non-parallel}, the  of table-text pairwise data and raw text data are discussed, respectively.



\subsection{Learning from Table-text Pair Data}
\label{sec:object_parallel}
In this section, we will show the learning loss of table-text pair data. 
According to the aforementioned assumption, the content variable  is observable and follows a delta distribution centred in the hidden representation of the table . 




\noindent\textbf{ELBO objective}. \quad Assuming that the template variable  only relies on the template of target sentence, we introduce  as an approximation of the true posterior ,  


The  loss of Equation \ref{eq:mll_parallel} is written as  

The variational posterior  is assumed as a multivariate Gaussian distribution , while the prior  is taken as a normal distribution .

\noindent\textbf{Preserving-Template Loss}. \quad
Without any supervision, the ELBO loss alone does not guarantee to learn a good template representation space. Inspired by the work in style-transfer \citep{hzticml17_disent, disent_in_ST1, baoyu, disent_in_ST2_ACL19}, an auxiliary loss is introduced to embed the template information of sentences into template variable .

With table, we are able to roughly align the tokens in sentence with the records in the table. By replacing these tokens with a special token \textit{ent}, we can remove the content information from sentences and get the sketchy sentence template, denote as . We introduce the preserving-template loss  to ensure that the latent variable  only contains the information of the template.

where  is the length of the , and  denotes the parameters of the extra template generator. 
 is trained via parallel data. In practice, due to the insufficient amount of parallel data, template generator  may not be well-learned. However, experimental results show that this loss is sufficient to provide a guidance for learning a template space.












\subsection{Learning from Raw Text Data}
\label{sec:object_non-parallel}

Our model is able to make use of a large number of raw data without table since the content information of table could be obtained by the content latent variable.

\noindent\textbf{ELBO objective}. \quad
According to the definition of generative model in Equation~\ref{eq:mll_non-parallel}, the  of raw text data is

With the mean field approximation~\citep{xing2002generalized},  can be factorized as: .
We have:

In order to make use of template information contained in raw text data effectively, the parameters of generation network  and posterior network  are shared for pairwise and raw data. In decoding process, for raw text data, we use content variable  as the table embedding for the missing of table . 
Variational posterior for  is deployed as another multivariate Guassian . Both  and  are taken as normal distribution .

\noindent\textbf{Preserving-Content Loss.}\quad
In order to make the posterior  correctly infers the content information, the table-text pairs are used as the supervision to train the recognition network of . To this end, we add a preserving-content loss

where  is the embedding of table obtained by the table encoder. 
Minimizing  is also helpful to bridge the gap of  between pairwise (taking ) and raw training data (sampling from ).
Moreover, we find that the first term of  is equivalent to (1) make the mean of  closer to ; (2) minimize the trace of co-variance of . The second term serves as a regularization. Detailed explanations and proof are referred in supplementary materials. 
































\subsection{Mutual Information Loss}
\label{sec:mutual_info}
As introduced by previous works \citep{chen2016infogan,zhao2017infovae,zhao2018unsupervised}, adding mutual information term to  could  alleviate KL collapse effectively and improve the quality of variational posterior. Adding mutual information terms directly imposes the association of content and template latent variables with target sentences. 
Besides, theoretical proof\footnote{Proof can be found in Appendix \ref{app:elbo_pt_hinder}} and experimental results show that introducing mutual information bias is necessary in the presence of preserving-template loss .

As a result, in our work, the following mutual information term is added to objective










\subsection{Training Process}
The final loss of \method is made up of the  losses and extra losses:

 and  are hyperparameters with respect to auxiliary losses.

The training procedure is shown in Algorithm \ref{alg:train}.
The parameters of generation network  and posterior network  could be trained jointly by both table-text pair data and raw text data. In this way, a large number of raw text data can be used to enrich the generation diversity.

\begin{algorithm}[t]
\footnotesize
    \caption{Training procedure} \hspace*{0.02in} {\bf Input:} Model parameters \\
    \hspace*{0.49in}
    Table-text pair data ; raw text data ; \\
    \hspace*{0.02in} {\bf Procedure \textsc{Train}():}
    \begin{algorithmic}[1]
        \State \quad Update  by gradient descent on 
        \State \quad Update  by gradient descent on 
        \State \quad Update  by gradient descent on 
    \end{algorithmic}
    \label{alg:train}
\end{algorithm}
