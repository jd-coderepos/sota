

As shown in the graphical model in Figure~\ref{fig:graphic_model}, our \method modifies the vanilla VAE model by introducing two independent latent variables $\vz$ and $\vc$, representing \textbf{template} latent variable and \textbf{content} latent variable respectively. $\vc$ models the content information in the table, while $\vz$ models the sentence template information. Target sentence $y$ is generated by both content and template variables. The two latent variables are disentangled, which makes it possible to generate diverse and relevant sentences by sampling template variable and retraining the content variable.
Considering pairwise and raw data presented in Figure \ref{fig:example_intro}, their generation process for the content latent variable $\vc$ is different. 


\begin{itemize}[leftmargin=2em, topsep=0pt]
    \item For a given table-text pair $(x,y) \in \mathcal{D}_p$, the content is observable from table $x$. As a result, $\vc$ is assumed to be deterministic given table $x$, whose prior is defined as a delta distribution $p(c|x) = \delta(c=f_{\text{enc}}(x))$. The marginal log-likelihood is:
    \begin{equation}\label{eq:mll_parallel}
    \begin{split}
        \log p_\theta(y|x) & = \log \int_z \int_c p_\theta(y|x,z,c) p(z) p(c|x) \text{dc}\text{dz} \\
        & = \log \int_z p_\theta(y|x,z,c=f_{\text{enc}}(x)) p(z) \text{dz}, (x,y) \in \mathcal{D}_p.
    \end{split}
    \end{equation}
    \item For raw text $y \in \mathcal{D}_n$, the content is unobservable with the absence of table $x$. As a result, the content latent variable $c$ should be sampled from prior of Gaussian distribution $\mathcal{N}(0,I)$. The marginal log-likelihood is:
    \begin{equation}
        \log p_\theta(y) = \log \int_z \int_c p_\theta(y|z,c) p(z) p(c)\text{dcdz},  y \in \mathcal{D}_r.
        \label{eq:mll_non-parallel}
    \end{equation}

\end{itemize}






















In order to make full use of both table-text pair data and raw text data, the above marginal log-likelihood should be optimized jointly:
\begin{equation}
    \mathcal{L}(\theta) = \E_{(x,y)\sim \mathcal{D}_p} [\log p_\theta(y|x)] + \E_{y \sim \mathcal{D}_r} [\log p_\theta(y)].
    \label{eq:total_loss}
\end{equation}

Directly optimizing Equation \ref{eq:total_loss} is intractable. 
Following the idea of variational inference~\citep{kingma2013auto}, a variational posterior $q_\phi(\cdot)$ is constructed as an inference model (dashed lines in Figure \ref{fig:graphic_model}) to approximate the true posterior. Instead of optimizing the marginal log-likelihood in Equation \ref{eq:total_loss}, we maximize the evidence lower bound~($\ELBO$). In Section \ref{sec:object_parallel} and \ref{sec:object_non-parallel}, the $\ELBO$ of table-text pairwise data and raw text data are discussed, respectively.



\subsection{Learning from Table-text Pair Data}
\label{sec:object_parallel}
In this section, we will show the learning loss of table-text pair data. 
According to the aforementioned assumption, the content variable $\vc$ is observable and follows a delta distribution centred in the hidden representation of the table $x$. 




\noindent\textbf{ELBO objective}. \quad Assuming that the template variable $\vz$ only relies on the template of target sentence, we introduce $q_\phi(z|y)$ as an approximation of the true posterior $p(z|y,c,x)$,  


The $\ELBO$ loss of Equation \ref{eq:mll_parallel} is written as  
\begin{align*}
    \mathcal{L}_{\text{ELBO}_p}(x,y) =  - \E_{q_{\phi_z}(z|y)}\log p_\theta(y|z,c=f_{\text{enc}}(x),x) + \KL (q_{\phi_z}(z|y) \Vert p(z)), \quad (x,y) \in \mathcal{D}_p.
\end{align*}
The variational posterior $q_{\phi_z}(z|y)$ is assumed as a multivariate Gaussian distribution $\mathcal{N}(\mu_{\phi_z}(y), \Sigma_{\phi_z}(y))$, while the prior $p(z)$ is taken as a normal distribution $\mathcal{N}(0, I)$.

\noindent\textbf{Preserving-Template Loss}. \quad
Without any supervision, the ELBO loss alone does not guarantee to learn a good template representation space. Inspired by the work in style-transfer \citep{hzticml17_disent, disent_in_ST1, baoyu, disent_in_ST2_ACL19}, an auxiliary loss is introduced to embed the template information of sentences into template variable $\vz$.

With table, we are able to roughly align the tokens in sentence with the records in the table. By replacing these tokens with a special token \textit{$<$ent$>$}, we can remove the content information from sentences and get the sketchy sentence template, denote as $\Tilde{y}$. We introduce the preserving-template loss $\mathcal{L}_{\text{pt}}$ to ensure that the latent variable $z$ only contains the information of the template.
\begin{equation*}
    \mathcal{L}_{\text{pt}}(x,y,\Tilde{y}) = - \E_{q_{\phi_z}(z|y)}\log p_{\eta}(\Tilde{y}|z) = - \E_{q_{\phi_z}(z|y)} \sum_{t=1}^m \log p_{\eta}(\Tilde{y}_t|z, \Tilde{y}_{<t})
\end{equation*}
where $m$ is the length of the $\Tilde{y}$, and $\eta$ denotes the parameters of the extra template generator. 
$\mathcal{L}_{\text{pt}}$ is trained via parallel data. In practice, due to the insufficient amount of parallel data, template generator $p_\eta$ may not be well-learned. However, experimental results show that this loss is sufficient to provide a guidance for learning a template space.












\subsection{Learning from Raw Text Data}
\label{sec:object_non-parallel}

Our model is able to make use of a large number of raw data without table since the content information of table could be obtained by the content latent variable.

\noindent\textbf{ELBO objective}. \quad
According to the definition of generative model in Equation~\ref{eq:mll_non-parallel}, the $\ELBO$ of raw text data is
\begin{equation*}
    \log p_\theta(y) = \E_{q_\phi(z,c|y)}\log \frac{p_\theta(y,z,c)}{q_\phi(z,c | y)}, \quad y \in \mathcal{D}_r.
\end{equation*}
With the mean field approximation~\citep{xing2002generalized}, $q_\phi(z, c|x)$ can be factorized as: $q_\phi(z,c|y) = q_{\phi_z}(z|y) q_{\phi_c}(c|y)$.
We have:
\begin{align*}
    \mathcal{L}_{\text{ELBO}_r}(y) = & - \E_{q_{\phi_z}(z|y) q_{\phi_c}(c|y)}\log p_{\theta}(y|z,c)  \\
    & + \KL(q_{\phi_z}(z|y)||p(z)) + \KL(q_{\phi_c}(c|y)||p(c)), \quad y \in \mathcal{D}_r.
\end{align*}
In order to make use of template information contained in raw text data effectively, the parameters of generation network $p_{\theta}(y|z,c)$ and posterior network $q_{\phi_z}(z|y)$ are shared for pairwise and raw data. In decoding process, for raw text data, we use content variable $c$ as the table embedding for the missing of table $x$. 
Variational posterior for $c$ is deployed as another multivariate Guassian $q_{\phi_c}(c|y) = \mathcal{N}(\mu_{\phi_c}(y), \Sigma_{\phi_c}(y))$. Both $p(z)$ and $p(c)$ are taken as normal distribution $\mathcal{N}(0, I)$.

\noindent\textbf{Preserving-Content Loss.}\quad
In order to make the posterior $q_{\phi_c}(c|y)$ correctly infers the content information, the table-text pairs are used as the supervision to train the recognition network of $q_{\phi_c}(c|y)$. To this end, we add a preserving-content loss
\begin{equation*}
    \mathcal{L}_{\text{pc}}(x,y) = -\E_{q_{\phi_c}(c|y)}\Vert c-h \Vert^2 + \KL (q_{\phi_c}(c|y) || p(c)), \quad (x,y) \in \mathcal{D}_p,
\end{equation*}
where $h=f_{\text{enc}}(x)$ is the embedding of table obtained by the table encoder. 
Minimizing $\mathcal{L}_{\text{pc}}$ is also helpful to bridge the gap of $c$ between pairwise (taking $c=h$) and raw training data (sampling from $q_\phi(c|y)$).
Moreover, we find that the first term of $\mathcal{L}_{\text{pc}}$ is equivalent to (1) make the mean of $q_\phi(c|y)$ closer to $h$; (2) minimize the trace of co-variance of $q_\phi(c|y)$. The second term serves as a regularization. Detailed explanations and proof are referred in supplementary materials. 
































\subsection{Mutual Information Loss}
\label{sec:mutual_info}
As introduced by previous works \citep{chen2016infogan,zhao2017infovae,zhao2018unsupervised}, adding mutual information term to $\ELBO$ could  alleviate KL collapse effectively and improve the quality of variational posterior. Adding mutual information terms directly imposes the association of content and template latent variables with target sentences. 
Besides, theoretical proof\footnote{Proof can be found in Appendix \ref{app:elbo_pt_hinder}} and experimental results show that introducing mutual information bias is necessary in the presence of preserving-template loss $\mathcal{L}_{\text{pt}}(\vx^p,\vy^p)$.

As a result, in our work, the following mutual information term is added to objective
\begin{equation*}
    \mathcal{L}_{\text{MI}}(y) = - I(z,y) - I(c,y).
\end{equation*}









\subsection{Training Process}
The final loss of \method is made up of the $\ELBO$ losses and extra losses:
\begin{align*}
    \mathcal{L}_{tot}(x^p,y^p,y^r) & = \mathcal{L}_{\text{ELBO}_p}(x^p,y^p) + \mathcal{L}_{\text{ELBO}_r}(y^r) + \lambda_{\text{MI}}( \mathcal{L}_{\text{MI}}(y^p) + \mathcal{L}_{\text{MI}}(y^r))\\
    & + \lambda_{\text{pt}} \mathcal{L}_{\text{pt}}(x^p,y^p) + \lambda_{\text{pc}} \mathcal{L}_{\text{pc}}(x^p,y^p), \qquad (x^p, y^p) \in \mathcal{D}_p, y^r \in \mathcal{D}_r.
\end{align*}
$\lambda_{\text{MI}}, \lambda_{\text{pt}}$ and $\lambda_{\text{pc}}$ are hyperparameters with respect to auxiliary losses.

The training procedure is shown in Algorithm \ref{alg:train}.
The parameters of generation network $\theta$ and posterior network $\phi_{z,c}$ could be trained jointly by both table-text pair data and raw text data. In this way, a large number of raw text data can be used to enrich the generation diversity.

\begin{algorithm}[t]
\footnotesize
    \caption{Training procedure} \hspace*{0.02in} {\bf Input:} Model parameters $\phi_z, \phi_c, \theta, \eta$\\
    \hspace*{0.49in}
    Table-text pair data $\mathcal{D}_p = \{(\vx, \vy)_i\}_{i=1}^N$; raw text data $\mathcal{D}_r = \{\vy_j\}_{j=1}^M$; $M \gg N$\\
    \hspace*{0.02in} {\bf Procedure \textsc{Train}($\mathcal{D}_p, \mathcal{D}_r$):}
    \begin{algorithmic}[1]
        \State \quad Update $\phi_z,\phi_c,\theta,\eta$ by gradient descent on $\mathcal{L}_{\text{ELBO}_p} + \mathcal{L}_{\text{MI}} + \mathcal{L}_{\text{pt}}+ \mathcal{L}_{\text{pc}}$
        \State \quad Update $\phi_z,\phi_c,\theta$ by gradient descent on $\mathcal{L}_{\text{ELBO}_r} + \mathcal{L}_{\text{MI}}$
        \State \quad Update $\phi_z,\phi_c,\theta,\eta$ by gradient descent on $\mathcal{L}_{tot}$
    \end{algorithmic}
    \label{alg:train}
\end{algorithm}
