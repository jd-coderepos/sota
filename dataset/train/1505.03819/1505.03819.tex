\section{Experimental Performance Evaluation} \label{sec:results}
We conducted a series of experiments to compare the performance impact of 
co-processors on image analysis operations and to evaluate the 
scheduling algorithms for efficient execution on a computational cluster with 
co-processors. We used a distributed-memory Linux cluster, called Stampede, for this 
purpose. Stampede is one of the high performance computing resources provided by 
XSEDE\footnote{https://www.xsede.org/home}; the machine is hosted at Texas Advanced 
Computing Center\footnote{https://portal.xsede.org/tacc-stampede}. 
Each compute node of Stampede has dual socket Intel Xeon E5-2680 processors, an Intel 
Xeon Phi SE10P co-processor and 32GB main memory. The cluster also has 
128 nodes each with one SE10P and one NVIDIA K20 GPU. 
The hardware details of the Intel Phi and GPU used in our evaluation
are presented in Table~\ref{tab:devices}. All the nodes in the cluster 
are connected via Mellanox FDR InfiniBand switches. 

In all of the experiments the MIC based codes were executed using the offload mode. One MIC computing 
core was reserved for running the offload daemon. The MIC and multi-core CPU codes for the 
image analysis operations were implemented using C++ and OpenMP, while the GPU codes were 
developed using CUDA. We measured processing performance using a 
set of images which had been collected from The Cancer Genome Atlas repository for brain 
tumor studies~\cite{kong2013machine}. Each image is partitioned into 4K$\times$4K tiles, which 
can be processed concurrently for image analysis.

While we used a microscopy image analysis application for performance evaluation, we believe our 
findings will be applicable in other applications that have data access and processing 
characteristics similar to those of the microscopy image analysis application. These characteristics 
can be summarized as follows: (1) spatial data domain is partitioned into chunks (or tiles), and 
tiles are processed concurrently; (2) processing of each tile is expressed as a 
pipeline of operations; (3) the pipeline operations have a variety 
of data access and processing patterns (see segmentation and feature computation categories in 
Tables~\ref{tab:op-categories}~and~\ref{tab:op-data-complexity}), which lead to variable speedups on 
co-processors; (4) the variability in speedups also arise from the fact that some pipeline operations 
are data intensive while others are computationally more expensive. We observed similar 
processing characteristics in applications that analyze satellite imagery and that process telescope 
datasets. 

\begin{table}
\begin{center}
\caption{Hardware specifications of the MIC and GPU used in the performance experiments.} 
\begin{footnotesize}
\begin{tabular}{p{0.14\textwidth}p{0.2\textwidth}p{0.2\textwidth}} 
\hline
		& Intel Xeon Phi SE10P 	& NVIDIA K20 GPU  	\\ \hline \hline 
\# of cores	& 61 	 		& 2496		 	\\ \hline
Clock		& 1.09~GHz		& 0.706~GHz	\\ \hline
L1 Cache	& 32~KB			& 64~KB \\ \hline
L2 Cache	& 512~KB		& 768~KB\\ \hline
Memory		& 8~GB			& 5~GB\\ \hline
Bandwidth	& 352~GB/s		& 205~GB/s \\ \hline
\end{tabular}
\end{footnotesize}
\label{tab:devices}
\vspace*{-2ex}
\end{center}
\end{table}

  
\subsection{Performance and Scalability of Operations on MIC} \label{sec:opts-scalability}
Thread affinity (i.e., how process threads are mapped to the physical cores of a MIC) 
can affect the performance of operations running on a MIC. In this set of experiments 
we evaluate the performance impact of three thread affinity strategies 
(Figure~\ref{fig:affinity}) on the core operations described in Section~\ref{sec:app}. 
The first strategy, called 
{\em Compact}, binds threads to the next free thread context. That is, 
all four contexts in a physical MIC core are used before threads are placed
in the contexts of another core. The second strategy, called {\em Balanced}, 
allocates threads to new cores before all the contexts in the same core are 
used.  Threads are balanced among cores and subsequent thread IDs are assigned to
neighbor contexts. The last strategy, {\em Scatter}, also balances the computation, but
threads with subsequent IDs are assigned to different physical cores.

The experimental evaluation was performed using two operations with different 
data access and computation patterns: (1)~Morphological Open has a regular data
access pattern and low computation intensity; (2)~Distance Transform performs
irregular data accesses and has moderate computation intensity. The codes 
employed OpenMP static loop scheduling, because it led to better performance. 
\begin{figure}[htb!]
\begin{center}
        \includegraphics[width=0.96\textwidth]{thread_affinity2.png}
\vspace*{-1ex}
\caption{Thread affinity strategies. Example mappings of 8 threads to physical 
cores on a 4-way hyper-threaded device.}
\vspace*{-3ex}
\label{fig:affinity}
\end{center}
\end{figure}

As is shown in Figure~\ref{fig:opts-scalability}, the scalability of the operations 
varies significantly under different thread affinity strategies. In addition, speedup 
values peak when the number of threads is a multiple of the number of computation cores 
(60, 120, 180, and 240 threads). This is expected because executing the same number of 
threads as the number of cores reduces computational imbalance among the cores. 
Morphological Open and Distance Transform achieve the best performances with 
120 and 240 threads, respectively. The performance of Morphological Open decreases after
120 threads because it is a memory-intensive operation. As is reported by other 
researchers~\cite{McCalpin1995,DBLP:conf/ipps/SauleC12}, the peak memory bandwidth of 
MIC for regular data access (STREAM benchmark~\cite{McCalpin1995}) is
attained with 1 or 2 threads per core. If more cores are
used in data intensive operations, the bandwidth decreases because of  
overload on the memory subsystem.
\begin{figure}[htb!]
\begin{center}
	\subfigure[Morphological Open]{\label{fig:speedup-open}
        \includegraphics[width=0.46\textwidth]{open2}}
\subfigure[Distance Transform]{\label{fig:speedup-dist}
        \includegraphics[width=0.44\textwidth]{dist2}}
\vspace*{-1ex}
\caption{Scalability of Morphological Open and Distance Transform under three 
thread affinity strategies.}
\vspace*{-3ex}
\label{fig:opts-scalability}
\end{center}
\end{figure}

Distance Transform scales well until 240 threads are dispatched. The performance 
of this operation also depends on the memory bandwidth of the MIC. The 
difference is that this operation's data access pattern is irregular. To 
better understand the performance behavior of Distance Transform and because 
memory bandwidth figures for irregular data access are not provided in the 
hardware specifications, we created a micro-benchmark to measure the memory 
bandwidth of the MIC for random data accesses. The micro-benchmark program reads 
and writes data elements from/to random locations in a matrix in parallel. 
Locations to be accessed in the matrix are pre-computed and stored in a secondary 
array. Our experiments showed that the benchmark program scaled until 240 threads 
were executed. This finding correlates with the observed performance of Distance 
Transform.

The performances of the Scatter and Balanced thread affinity strategies were
similar for both operations. The only significant discrepancy among the
strategies occurred for Morphological Open and the Compact affinity strategy.
This is because Compact utilizes the 60 physical cores only when 240 threads
are dispatched. However, the Morphological Open achieves its best performance
when 120 threads are created (2-way hyperthreading) with 60 cores.

\subsection{Performance Analysis of Core Operations on MIC, GPU and CPU} \label{sec:comp-performance}
These experiments compare the speedups of core operations on MIC, GPU and CPU.
We computed the speedup values for an operation using the single-core CPU
execution as the baseline performance. We used single-core implementations from
well-known libraries and implementations published by other research groups
(see Section~\ref{sec:core-opts}). We have classified the core operations into three groups based on data access 
and computation patterns in order to better understand and characterize the 
speedup values obtained in the experiments. The groups are:
(1)~Operations with regular data access: RGB2Gray, Morphological Open, Color
Deconvolution, Pixel Stats, Gradient Stats, and Sobel Edge; (2)~Operations with
irregular data access: Morphological Reconstruction, FillHoles, and Distance
Transform; (3)~Operations that heavily use atomic instructions: Area
Threshold and Connected Component Labeling (CCL). We draw from the Roofline 
model proposed by Williams et al.~\cite{Williams:2009:RIV:1498765.1498785} 
in our evaluation. 
\subsubsection{{\bf Regular Operations.}}  
The performance of an operation in this class heavily depends on the memory 
bandwidth of a computing device. We measured the memory bandwidths of the 
GPU, CPU and MIC using the STREAM benchmark~\cite{McCalpin1995} which are 
148GB/s, 78GB/s (total bandwidth of two CPUs) and 160GB/s 
(one or two threads per core achieve similar performance), respectively. 
The peak computation capacities of the K20 GPU and MIC for double 
precision numbers are about 1~TFLOPS each, while 2 CPUs together 
achieve 345~MFLOPS.

The Morphological Open, RGB2Gray and Color Deconvolution operations are memory
bound with low arithmetic-instruction-to-byte ratio. Their speedups are low on
the CPU because they quickly saturate the memory bus. Their performances on the 
GPU are nearly 1.25$\times$ higher than on the MIC as is shown in 
Figure~\ref{fig:comp-performance}. The remaining 
regular operations (Pixel Stats, Gradient Stats and Sobel Edge) are compute
bound due to their higher computation intensity. These operations achieve
better scalability on all the devices. The GPU and the MIC result in similar 
performance improvements for these operations. Their execution times improve by 
about 1.9$\times$ over the multi-core CPU 
executions. This group of compute intensive operations 
achieve the best performance on the MIC using 120 threads. Using more threads 
does not improve performance because the MIC threads can launch a vector instruction 
every two cycles. 
\begin{figure*}[htb!]
\begin{center}
\includegraphics[width=\textwidth]{operations2}
\vspace*{-4ex}
\caption{Speedups of the core operations on multi-core CPU, MIC and GPU, 
using the single-core CPU versions as baseline. The number above each dash 
is the number of threads that led to the best performance on the MIC. The
number above the graph in each column is the percent contribution of the 
corresponding operation to the overall application execution time.}
\vspace*{-4ex}
\label{fig:comp-performance}
\end{center}
\end{figure*}
\subsubsection{{\bf Irregular Operations.}}
The operations with irregular data access patterns include
Morphological Reconstruction, FillHoles and Distance Transform. We measured 
the bandwidth of each device for random data accesses using the micro-benchmark 
program described in Section~\ref{sec:opts-scalability}. The program executed 
10 million random reads and writes on a 4K$\times$4K matrix of integer 
values. 
The results are presented in Table~\ref{tab:random-bandwidth}. The GPU has 
much higher bandwidth as compared to the other devices. The performance of 
the MIC is low, especially for writes. This is a result of heavy data traffic 
on the ring bus connection in the MIC which is necessary to maintain
consistency. 
\begin{table}[h!]
\caption{Device bandwidths for random data accesses (MB/s).}
\vspace*{-2ex}
\begin{center}
\begin{tabular}{l l l l}
\hline
   	& CPU	& MIC	&	GPU 	\\ \hline \hline
Reading	& 305	& 399	&	895	\\ \hline
Writing	& 74	& 16	&     	126	\\ \hline
\end{tabular}
\end{center}
\label{tab:random-bandwidth}
\vspace*{-2ex}
\end{table}

Given the micro-benchmark results, it is not surprising that 
Distance Transform is much faster (about two times faster)
on the GPU than on the MIC or CPU as is shown in 
Figure~\ref{fig:comp-performance}. All phases of this operation perform only
irregular data accesses with significantly more data reading than writing. Thus, 
the random data access bandwidths of the devices have significant impact on 
the operation's overall performance.
Morphological Reconstruction and Fill Holes, on the other hand, have different 
performance behaviors than Distance Transform, because their data access 
patterns are not purely irregular. These operations have two computation 
phases. They first perform regular raster-/anti-raster passes on data. Then, 
they carry out irregular computations that use a queue to execute propagations. 
As such, the operations benefit more from the MIC processor, which is efficient in their
regular phase. Additionally, they may execute the regular phase a
number of times before the second phase. This value was tuned in the experiments 
for each device. The regular phase was 
executed a large number of times on the MIC, before the second phase was started. 
Hence, for these operations, the MIC compares better with the GPU; the GPU is
only 1.33$\times$ faster than the MIC.
\subsubsection{Operations that Rely on Atomic Instructions.} 
Atomic instructions are necessary for correct execution in some operations; Area 
Threshold and CCL intensively use atomic instructions. We developed
another micro-benchmark to measure device bandwidths when atomic 
instructions are used. The benchmark program implements two scenarios. In the first 
scenario, a single variable is updated by all threads. This is a worst-case scenario 
because all threads access the same memory address and cause a lot of congestion on 
atomic instruction executions. In the second scenario, the elements of an array are 
updated such that an element is updated by one thread only; however, a thread may update 
multiple array elements and multiple array elements may be updated concurrently. The 
second scenario does not cause congestion on atomic instruction executions, although 
each thread executes an atomic instruction when accessing an array element. Less 
congestion is expected to yield better performance on all the devices. 
\begin{table}[h!]
\caption{Measured device bandwidths for the Atomic Add operation (Millions/sec).}
\vspace*{-2ex}
\begin{center}
\begin{tabular}{l l l l}
\hline
   		& CPU	& MIC		&	GPU 	\\ \hline \hline
Single Variable	& 134	& 55		&	693	\\ \hline
Array		& 2,200	& 906		&	38,630  \\ \hline
\end{tabular}
\end{center}
\label{tab:atomic}
\vspace*{-2ex}
\end{table}

The results of the benchmark runs are presented in Table~\ref{tab:atomic}. 
The bandwidth of the GPU is the highest in both of the scenarios. However, 
the GPU is also the
device that suffers the highest performance degradation from the second 
scenario to the first (worst-case) scenario. This large decrease in 
performance 
is a consequence of the fact that warps of threads on a GPU must
execute the same instructions. It is expected that all threads
executing in a warp will cause a conflict when updating the memory 
in the single variable case, and, consequently, threads will be 
serialized. This problem is
intensified in the GPU because it supports a much higher level of concurrency
than the other devices. The CPU is almost 2.4$\times$
faster than the MIC in both scenarios. The MIC relies on the use of
vectorized instructions for good performance, since it is equipped with
a simpler computing core. However, it does not support atomic vectorized
instructions, such as those proposed by Kumar et. al.~\cite{4556746} for other
multiprocessors. 

As is expected from the benchmark results, Area Threshold and CCL achieve higher 
performance on the GPU than on the other two devices, with executions on the CPU 
being faster than those on the MIC. 
\subsection{Cooperative Execution on Hybrid Machines}
\paragraph{{\bf Comparing Performances of Schedulers.}}
This experiment compares the performance of proposed and baseline schedulers.
The experiment uses 1~GPU, 1~MIC, and 14~CPU cores for
computation; the remaining 2~CPU cores are used to manage the co-processors. The
input data contains 800 4K$\times$4K image tiles. Because the decision space
may affect the performances of the schedulers, we varied the number of
coarse-grained application stages that were concurrently executed in a Worker. This value 
is referred to as the {\em window size}. The window size was varied from 16 (the number of
computing slots) to 70 -- values larger than 70 had no significant effect 
on performance. The
speedup and execution time estimates used by the schedulers were determined from 
previous runs. 
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{WindowVariation2.png}
\vspace{-2mm}
	\caption{Performance of the scheduling strategies when the number of 
	concurrent tasks assigned to a Worker (window size) is varied.}
	\label{fig:windowVariation}
\end{figure}

The execution times of the image analysis application using FCFS, PADAS, HEFT and 
PAMS are shown in Figure~\ref{fig:windowVariation}. FCFS and PADAS result in similar
execution times. Window size has little impact on their
performance. HEFT results in the highest execution times with
small window sizes, but performs better than 
FCFS and PADAS with window sizes larger than 45. HEFT performs better with larger
decision spaces because it is able to balance well the work among processors and
minimize miss-assignment of tasks.
The PAMS scheduler achieves the best performance among all the schedulers 
for all window sizes. It is at least 1.15$\times$ faster
than HEFT, but its gains are up to 1.39$\times$ for configurations with small
window sizes. This scheduler is able to perform very well even with small
window sizes and is less sensitive to window size. This is an important attribute
of a scheduler, because smaller window sizes in Workers 
reduces load imbalance among nodes on a distributed-memory machine.

To better understand the relative performance of the schedulers, we looked at the 
task scheduling computed by a scheduler with respect to the speedups of the operations 
on the devices and the percent 
contribution of the operations in the overall application execution time 
(Figure~\ref{fig:comp-performance}). 
\begin{figure*}[htb!]
\centering
\mbox{
    \subfigure[FCFS]{
    \includegraphics[width=0.45\linewidth]{fcfs.png}}
}
\mbox{
    \subfigure[PADAS]{
    \includegraphics[width=0.45\linewidth]{padas.png}}
}
\mbox{
    \subfigure[HEFT]{
	\label{fig:prof_heft}
    \includegraphics[width=0.45\linewidth]{heft.png}}
}
\mbox{
    \subfigure[PAMS]{
    \label{fig:pams}
    \includegraphics[width=0.45\linewidth]{pams.png}}
}
\vspace{-1ex}
\caption{Task assignment profile for each of the operations executed in the image 
analysis application under different scheduling strategies. A fixed window size of 
$80$ is used in all runs.} 
\vspace{-1ex}
\label{graf:distrib}
\end{figure*}
The percentage of operations executed by each computing device for a given 
scheduling strategy is presented in Figure~\ref{graf:distrib}. The number of
operations that FCFS assigns to each device is almost the same across 
all the operations. PADAS computes a
scheduling in which operations are preferably assigned to certain types of
processing devices. Nevertheless, it is not able to significantly improve on 
the performance of FCFS. 
PADAS fails when 
it assigns a substantial number of
Distance Transform tasks to CPU cores although Distance Transform is more efficient 
on co-processors.
The Sobel operation illustrates well what we believe to be the major problem with
this policy. Sobel achieves the highest speedups on the GPU, but it is also efficient on
the MIC. Because it is faster on the GPU, it is inserted in the queue from the 
GPU end. This significantly reduces the chances of the MIC getting 
selected for this operation. As a consequence, the MIC is used with other
operations for which it is less efficient.

The analysis of the HEFT scheduling (Figure~\ref{fig:prof_heft}) shows that
most of the tasks with short execution times were executed by the CPU cores, 
regardless of the speedup values. HEFT also assigns all of the
Distance Transform operations to the co-processors, but this operation is
2$\times$ more efficient on the GPU than on the MIC. Since no other device is able to
efficiently perform this computation, the GPU should be used with higher
percentages than what was observed in the experiments. Moreover, 
because this operation is
responsible for nearly 40\% of the whole application execution time, its
correct scheduling is critical for high performance. 

The profile of PAMS scheduling shows a better distribution of tasks. It 
schedules most of the Distance Transform operations on the GPU and 
utilizes the CPU cores for operations (such as Area Threshold and CCL) 
that do not attain high performance on the co-processors. It also schedules 
operations (such as ColorDevonv, FillHoles, Gradient, Pixel Stats and Sobel 
Edge) to the MIC that benefit from this device.
\paragraph{{\bf Evaluating Different Configurations of Hybrid System.}}
This section investigates how well the scheduling strategies perform when 
different combinations of CPU and co-processors are used as the hybrid 
system. In this work we looked at three hardware configurations: CPU-GPU 
(15 CPU cores and 1 GPU), CPU-MIC (15 CPU cores and 1 MIC) and CPU-GPU-MIC 
(14 CPU cores , 1 GPU, and 1 MIC). 
\begin{figure}[htb!]
	\centering
	\includegraphics[width=0.6\linewidth]{ProcVariation2.png}
	\vspace{-2mm}
	\caption{Application performance when application tasks are scheduled and 
	mapped to two or more computation devices.}
	\label{fig:procVariation}
	\vspace{-1mm}
\end{figure}

The experimental results presented in Figure~\ref{fig:procVariation} show
that FCFS has the worst performance in all hardware configurations. In the
scenarios with two device types (CPU-MIC and CPU-GPU), PADAS and PAMS
result in nearly the same application execution times. This is expected because these
schedulers will compute the same scheduling, since PAMS is reduced to PADAS
with an implementation using two queues instead of one. PADAS and PAMS
outperform HEFT in all of the configurations. The application executes faster in 
the configuration with three devices with PAMS. 
The configuration
with CPU-GPU is about 1.27$\times$ faster than the one with CPU-MIC. 
This is because of the fact that neither CPU nor MIC is efficient with
irregular patterns and atomic operations, which account for a large amount
of the application execution time. 
\paragraph{{\bf Sensitivity to Inaccurate Estimates.}} \label{sec:sensibility}
This section empirically evaluates the ability of the schedulers to perform well
in cases in which estimations of the metrics (execution time and speedup) they 
use in computing a schedule are inaccurate. To carry out these experiments, we
introduced errors in the execution times and, as a consequence, in the
speedups in a controlled manner. The amount of error was varied as 
$0\%$, $10\%$, $25\%$, $50\%$, $75\%$ and $100\%$ of the execution
times of the operations, with equal probability that the error was an increase 
or a decrease in execution time. 
\begin{figure}[htb!]
	\centering
	\includegraphics[width=0.8\linewidth]{ErrorVariation2.png}
	\vspace{-2mm}
	\caption{Performance of scheduling strategies as errors are 
	introduced in estimated execution times of operations.}
	\label{fig:errorVariation}
	\vspace{-2mm}
\end{figure}

As presented in Figure~\ref{fig:errorVariation}, the performances of PADAS and
PAMS are not significantly affected as the amount of error is varied. On the other
hand, HEFT, which uses execution times to calculate schedules, is strongly
impacted by the errors and is even less efficient than FCFS when the amount of 
error is higher 
than 50\%. The experimental results show that the scheduling
strategies based on the speedup metric are much less sensitive to inaccurate 
estimates and perform well even with large error in estimates. 
These schedulers use the input metric only to order tasks in a queue. The amount 
of error in estimated speedup has to be large enough to significantly alter the 
positions of tasks in the queue in order to adversely impact computed schedules 
and overall application performance. 
\paragraph{{\bf Scalability Results.}}
These experiments assessed the performance of the image analysis 
application on a distributed memory cluster. 
The scheduling strategies used the speedup values 
obtained in the experiments in Section~\ref{sec:comp-performance} (see 
Figure~\ref{fig:comp-performance}). 
Figure~\ref{fig:scalability} presents the performance results of both the
CPU-only version of the application (16-CPUs), which uses only the 16 CPU 
cores on each node, and the hybrid version, which uses the CPU, GPU and MIC 
in a coordinated manner via FCFS, HEFT or PAMS. The input data for this 
experiment consists of 6,379 4K$\times$4K image tiles. Both versions of the 
application achieved good scalability with efficiency higher than 90\%. 
The hybrid version with PAMS results in a performance improvement of 
about 2.2$\times$ on top of the CPU-only version. The performance 
improvement by PAMS on top of HEFT is maintained as the 
application scales.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{scalability}
	\vspace{-1ex}
	\caption{Strong-scaling results on a distributed-memory cluster in which each 
	node has two 8-core CPUs, one MIC, and one GPU.}
	\label{fig:scalability}
	\vspace{-2ex}
\end{figure}


We also executed the application on a large number of computing nodes equipped 
with two CPUs and one MIC only, since Stampede has more nodes with this configuration.
We experimented with five versions of the application:
CPU-only refers to the multi-core CPU version that uses all the 
CPU cores available but no co-processors; ~MIC-only uses only the MIC 
in each node to perform computations;
CPU-MIC uses all the CPU cores and the MIC in coordination and 
distributes tasks to devices in each node via FCFS, PAMS, or HEFT as
the scheduling strategy.  
\begin{figure}[htb!]
\begin{center}
	\includegraphics[width=0.7\textwidth]{weak-scaling2}\vspace*{-1ex}
	\caption{Weak scaling results on a distributed-memory cluster in which each node 
	has 	two 8-core CPUs and one MIC.}
	\vspace*{-2ex}
\label{fig:weak-scale-general}
\end{center}
\end{figure}

Figure~\ref{fig:weak-scale-general} shows the weak scaling performance of 
the application when the
dataset size and the number of nodes increase proportionally. For the
configuration with 192 nodes, the input data contains 68,284 4K$\times$4K image
tiles for a total of 3.27~TB of uncompressed data. The application
scaled well with about 84\% of efficiency on 192 nodes. The main factor limiting scalability is the
increasing cost of reading the input image tiles concurrently from storage as the
number of nodes (and processes) grows. The CPU-MIC versions resulted in improvements of 
up to 2.06$\times$ on top of the MIC-only version. 
The CPU-MIC PAMS version results in the lowest execution times and is 
on average 1.29$\times$ faster than the CPU-MIC FCFS version. 


