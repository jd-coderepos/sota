

\newcommand\meghal[1]{\textcolor{orange}{[Meghal: #1]}}
\newcommand\monika[1]{\textcolor{red}{[Monika: #1]}}


\documentclass{article}
\usepackage{spconf,amsmath,graphicx}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage[table,xcdraw]{xcolor}
\usepackage{amssymb}
\usepackage{comment}
\usepackage{siunitx}
\usepackage{enumitem}
\setlist{nosep, leftmargin=14pt}

\usepackage{mwe} 

\def\x{{\mathbf x}}
\def\L{{\cal L}}

\title{An Efficient Anchor-free Universal Lesion Detection in CT-Scans}
\name{Manu Sheoran, Meghal Dani, Monika Sharma, Lovekesh Vig\thanks{Authors contributed equally}}
\address{TCS Research, New Delhi, India}
\begin{document}
\maketitle
\begin{abstract}
Existing universal lesion detection (ULD) methods utilize compute-intensive anchor-based architectures which rely on predefined anchor boxes, resulting in unsatisfactory detection performance, especially in small and mid-sized lesions. Further, these default fixed anchor-sizes and ratios do not generalize well to different datasets. Therefore, we propose a robust one-stage anchor-free lesion detection network that can perform well across varying lesions sizes by exploiting the fact that the box predictions can be sorted for relevance based on their center rather than their overlap with the object. Furthermore, we demonstrate that the ULD can be improved by explicitly providing it the domain-specific information in the form of multi-intensity images generated using multiple HU windows, followed by self-attention based feature-fusion and backbone initialization using weights learned via self-supervision over CT-scans. We obtain comparable results to the state-of-the-art methods, achieving an overall sensitivity of  on the DeepLesion dataset, which comprises of approximately  CT-scans with lesions annotated across various body organs. 


\end{abstract}
\begin{keywords}
Universal Lesion Detection, CADe/x, Medical Image Analysis, One-stage Detector, CT-scans
\end{keywords}
\section{Introduction}
\label{sec:intro}
Computer-aided detection/diagnosis (CADe/x) using computed tomography (CT) images has evolved as an emerging field of research, thanks to the tremendous advancements of deep learning techniques in the area of computer vision~\cite{litjens2017survey, kidney, pulm_nodule}.
Cancer has been one of the most researched and prevalent diseases and the identification of lesions from CT-scans is an important step towards diagnosis. Due to the heterogeneous nature of lesions, their manual analysis and detection is a tedious and error-prone task that requires significant expert knowledge. In the past decade, many efforts have been made towards automated lesion detection but solutions were largely organ-specific focusing on detecting lesions in one of the organs such as liver, kidney, and lungs~\cite{kidney, pulm_nodule}. Recently, the focus has shifted towards developing a Universal Lesion Detector (ULD) which can identify lesions present in different organs from a patient's CT-scan~\cite{yan2019mulan, li2019mvp, 3dce, mla-net, anchorfree-rpn}. DeepLesion~\cite{yan2018deeplesion} is a multi-organ CT-scan dataset, which consists of  lesions annotated across various organs of the body, available publicly for bench-marking ULD techniques.

\vspace{-1mm}
Prior ULD methods~\cite{3dce, yan2019mulan, li2019mvp, retinanet_improv} have utilized neighboring slice information to provide 3D-context to the network and attention to provide better features for detection by enabling the network to focus on important regions of CT-scans. Yan et al.~\cite{yan2019mulan} proposed MULAN that fuses features from  input slices and jointly trains the network for lesion segmentation and tagging. Some of these works also incorporate novel negative mining techniques to remove false positives and improve the detection. Authors of MELD~\cite{meld} use  different datasets for training and further, use missing annotation matching (MAM) and negative region mining (NRM) for achieving state-of-the-art lesion detection performance on the DeepLesion test-set. We note that the previous ULD methods are anchor-boxe based such as Mask-RCNN~\cite{he2017mask} and FasterRCNN~\cite{ren2015faster}, etc. in which pre-defined fixed anchor-sizes and aspect-ratios are used. This makes it difficult to capture the heterogeneous sizes of lesions present in various body organs of different medical imaging datasets.
Moreover, the anchor-based methods are computationally heavy and quite slow as they require running the detection and classification modules multiple times. This prompted researchers to get rid of anchor-boxes in lesion detection networks~\cite{mla-net, anchorfree-rpn}. Zhang et al.~\cite{anchorfree-rpn} proposed a U-Net based anchor-free ULD network in which each feature map is attached with a detection head. In another work~\cite{mla-net}, authors proposed a compute-heavy multi-layer anchor-free MLANet based on hourglass network and center-to-corner transformation strategy for detecting varied sized lesions. However, both of these anchor-free ULD methods are not the state-of-the-art. 

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{pdf/dsa_model.pdf}
  \caption{\small{Overview of \emph{DSA-ULD} architecture. (a) An input , consisting of  CT-slices of a patient, is used for generating  multi-intensity images  with  different HU windows ( to ). Next, each image is passed through a shared convolutional feature extractor (having domain-specific (DS) weight initialization) with  FPN sub-levels ( to ) and we extract  feature maps  for each FPN level . (b) These feature maps at sub-level  are fused together using proposed self-attention module into a single feature map . Here , ,  represent dimensions for Value, Key, and Query matrix. Finally, detection from each sub-level is merged to get the final detection. 
  }}
  \label{fig:arch-dia}
  \vspace{-3mm}
\end{figure*}
\vspace{-1mm}

In this paper, we propose a robust and efficient one-stage anchor-free ULD network that performs at par with state-of-art ULD methods. The concept of a one-stage detector is based on centerness~\cite{tian2019fcos} of objects and predicts all bounding boxes in one go and hence, is computationally efficient and light for clinical deployment. Next, we demonstrate that the performance of automated diagnostic systems can be further improved by imparting extra domain knowledge to the deep networks explicitly. This domain driven information enables deep networks to mimic the diagnostic pattern of doctors by focusing on features or areas where they pay attention while making the final diagnosis. To this end, we provide multi-intensity images highlighting different organs of the body in CT-images using multiple HU windows, fuse the multiple features using self-attention and initialize the backbone of the detection network using medical imaging related weights learned via self-supervision on the DeepLesion~\cite{yan2018deeplesion} dataset. We call our proposed network \emph{DSA-ULD} (Domain-driven Self-attention based Anchor-free ULD).

To summarize, our contributions are as follows:
\begin{itemize}
    \item We propose a one-stage anchor-free ULD network which can efficiently detect lesions present in CT-scans across different organs.
    \item We demonstrate that the feeding of domain information to the deep networks helps to improve detection performance. We named our network \emph{DSA-ULD}: Domain-driven Self-attention based Anchor-free ULD.
    \item We evaluate DSA-ULD on the DeepLesion test-set and achieve better/comparable results with the state-of-the-art ULD methods.
\end{itemize}


\begin{figure*}[t]
  \centering
  \includegraphics[width=0.8\linewidth]{pdf/fcos_comparison.pdf}
  \vspace{-2mm}
  \caption{\small{Qualitative and Quantitative Comparison of sensitivity on DeepLesion test-set. Please note that ULD w/o DK represents the anchor-free ULD without domain-knowledge (DK) having  slices as input with only one HU window () and without attention-based feature fusion. D2-ULD is a custom-anchor based detectron2 network with DK. Here, BN, LNG, MDT, LVR, KDY, ABM, PLS and ST represent different organs such as bones, lungs, mediastinum, liver, kidney, abdomen, pelvis and soft-tissues, respectively. The green, magenta, and red color boxes represent ground-truth, true-positive (TP), and false-positive (FP) lesion detection, respectively.
  }}
  \label{fig:lesion_size}
  \vspace{-2mm}
\end{figure*}
\vspace{-3mm}


\section{Methodology}
\label{methods}
\vspace{-2mm}
Fig.~\ref{fig:arch-dia} shows our proposed DSA-ULD pipeline. The network takes a 3-channel image  (key slice with one inferior and superior slice) as input and generates  intensity-images  using  HU-windows  after pre-processing. Next, the features are extracted and fused into  using a novel feature-fusion strategy based on self-attention which are eventually fed to a detection head. We utilize self-supervised weights and anchor-free protocol to make DSA-ULD generic and robust. Now, we discuss the network in detail:  

\noindent \textbf{Feature Generation:} During manual analysis, a radiologist uses HU windowing to adjust CT intensity values to focus on organs/tissues of interest. Inspired by Masoudi et al.~\cite{masoudi2021quick}, we mimic the radiologist's behaviour in our ULD and
highlight multiple organs of interest with heuristically determined  HU windows~\cite{xue2012window}: , , ,  for bones, chest region including lungs \& mediastinum, abdomen including liver \& kidney, and soft-tissues, respectively. After windowing, 3-channel multi-intensity image  is passed as input to the ResNeXt-101 shared backbone with feature pyramid network (FPN)~\cite{lin2017featurefpn} based convolutional feature extractor. To further incorporate domain-information in our proposed ULD, we initialize the backbone of our feature extractor with weights learned via self-supervision on the DeepLesion dataset~\cite{grill2020bootstrap}.

\noindent \textbf{Attention Based Feature Fusion:} In order to fuse the feature maps  containing multi-organ information, one can simply apply a 2D convolution layer which operates only on a local neighborhood. However, recently Vision Transformers~\cite{ViT} have shown remarkable state-of-the-art results across various vision tasks by jointly attending to both spatial and feature sub-spaces with the use of multi-headed self-attention. Therefore, for efficient feature-fusion, we use self-attention which can capture global information across long range dependencies. At each FPN level ,  feature maps each having  channels are fed as input to the module. We also use a 2D convolution attention layer ( output channels) in parallel with the self-attention module ( output channels) to reduce the computational overhead. Subsequently, the outputs from the two parallel branches are concatenated to obtain the desired number of output channels . To reduce computation overhead for attention, we use  heads with the depth of Values matrix as . The dimensions per head for Keys and Values matrix are fixed at . This convolution augmented self-attention allows us to fuse features from different pyramid levels having different resolutions, resulting in robust detection of lesions of different sizes.

\noindent \textbf{Anchor-free One-Stage Detector:} Zhi et al.~\cite{tian2019fcos} proposed a fully convolutional one-stage (FCOS) detector which works on the principle of centerness to reduce the number of low-quality bounding box detections. The overall loss function is:
\vspace{-3mm}


 In a per-pixel prediction, for each location  in the feature map , classification score  is computed followed by regression prediction  for every positive location via an indicator function . Here,  and  are the classification focal loss and regression IoU loss for location ,  is the no. of positive samples,  is the balance weight, and,  and  are ground-truth labels for classification and regression, respectively. Apart from the two conventional detector heads (classification \& regression), there is a third crucial head for centerness. It is based on the idea that low-level regressed boxes that have a skewed feature location in terms of their location inside the box tend to hamper overall detection results. Thus, given the regression targets
 \&   for a location, the term centerness (as defined below) is trained with binary cross entropy (BCE) loss and added to the loss function defined in Eq.~\ref{eq:1} for the refined results.
\vspace{-2mm}



\section{Experiments and Results}
\label{sec:res}
\vspace{-3mm}
\textbf{Dataset and Metric:}
All the experiments are performed on standard DeepLesion dataset consisting of  lesions bounding-box instances on  axial key CT slices of  unique patients~\cite{yan2018deeplesion}. For a fair comparison, we conduct evaluation on the official test set , and report sensitivity at various false positives (FPs) per image levels.
\noindent \textbf{Implementation Details:} We utilize weights learned via self-supervised learning (SSL) for FPN backbone with ResNeXt-101 for all the comparison experiments. A -channel image input is generated by taking  slices of a patient's CT-scan (key slice with one superior and inferior neighboring slice). The pre-processing of images include clipping of black borders, windowing~\cite{xue2012window}, and resampling voxel space to  mm. We utilize transformations such as horizontal and vertical flips, resizing and translation to augment the training data.The models are trained on NVIDIA Tesla V100 GPUs having GB GPU-memory, with a batch size of . The model is trained until convergence using SGD optimizer with a learning rate and decay-factor of  and , respectively.

\noindent \textbf{Comparison with state-of-the-art:} Now, we present comparison results of ULD methods such as 3DCE~\cite{3dce}, improved RetinaNet~\cite{retinanet_improv}, Anchor-free RPN~\cite{anchorfree-rpn}, MLANet~\cite{mla-net}, MULAN~\cite{yan2019mulan} and Detectron2 based ULD in Table~\ref{tab:comp-sota}. Please note that D2-ULD is the network that is trained using anchor-based Detectron2 backbone in place of anchor-free FCOS and involves domain knowledge such as multi-intensity images, feature-fusion and custom-anchors relevant for lesion-sizes. We do not show a comparison with MELD~\cite{meld} as MELD is trained on  different datasets including DeepLesion and the comparison would not have been fair with our DSA-ULD which is trained on DeepLesion only. We still achieve comparable sensitivity of  similar to that of MELD (). This supports our claim that adding extra domain knowledge to the deep networks explicitly alleviates the need for large amounts of heterogeneous training data to learn robust features. As evident in Table~\ref{tab:comp-sota}, we outperform all the prior methods and achieve an average sensitivity of  with only a few slices of a patient's CT-scan. The average sensitivity is further improved to  after initializing the backbone of our DSA-ULD with weights learned using self-supervision on the DeepLesion dataset. From Table~\ref{tab:comp-sota}, we also observe that D2-ULD and DSA-ULD give comparable average sensitivity on DeepLesion where D2-ULD uses custom-anchors defined for lesion detection and DSA-ULD is anchor-free. Hence, it can be inferred that anchor-free ULD is more preferable as it does not require very heavy computation while giving equal detection performance.

\begin{table}[!ht]
\begin{center}
\resizebox{\columnwidth}{!}{\begin{tabular}{|l|c|l|c|c|c|c|c|}
\hline
\multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{(S,W)}}  & \multicolumn{4}{c|}{FP (\%)}  & \multirow{2}{*}{\textbf{Avg.}}
\\ \cline{3-6}
& &\textbf{0.5} & \textbf{1.0} & \textbf{2.0} & \textbf{4.0}&
\\\hline
3DCE~\cite{3dce} & (27,1)  & 62.48 & 73.37 & 80.70 & 85.65 & 75.55 \\
Anchor-Free RPN ~\cite{anchorfree-rpn} & (64,1)  & 68.73 & 77.10 & 83.54 & 88.12 & 79.37 \\
MLANet~\cite{mla-net} & (3,1)  & ---- & 77.10 & 83.0 & 88.30 & ---- \\
Improved RetinaNet~\cite{retinanet_improv} & (3,1)& 72.18 & 80.07 & 86.40 & 90.77 & 82.36 \\
MVP Net~\cite{li2019mvp} & (9,3)& 73.83 & 81.82 & 87.60 & 91.30 & 83.64 \\
MULAN (w/o tags)~\cite{yan2019mulan} & (27,1)  & 76.10 & 82.50 & 87.50 & 90.90 & 84.33 \\
MULAN (w/ tags)~\cite{yan2019mulan} & (27,1)  & 76.12 & 83.69 & 88.76 & 92.30 & 85.22 \\ \hline




D2-ULD (custom anchors) & (3,5) & 75.09 & 83.88 & 89.28   & 92.83  & 85.27   \\ 
D2-ULD + SSL  & (3,5) & 76.07 & 84.31 & \textbf{89.44}  & \textbf{92.94}  & 85.69   \\ \hline
(a) \emph{DSA-ULD}* & (3,5) & 77.38 & 84.06& 89.28 & 92.44 & 85.79 \\
(b)\textbf{+SSL} & \textbf{(3,5)} & \textbf{78.30} & \textbf{84.51} & 88.99 & 92.40 & \textbf{86.05}\\

\hline
\end{tabular}}
\vspace{-2mm}
\caption{\small{Sensitivity (\%) Comparison of \emph{DSA-ULD} with previous state-of-the-art methods on DeepLesion~\cite{yan2018deeplesion} test-set. (S, W) denote no. of slices and HU windows used in each experiment.
}}
\label{tab:comp-sota}
\end{center}
\vspace{-4mm}
\end{table}
\vspace{-0.5mm}

We also show a comparison of organ-wise average sensitivity, lesion-size wise sensitivity at FP = 4 and lesion-size wise average sensitivity of DSA-ULD with previous methods in Fig.~\ref{fig:lesion_size}(a),(b) and (c), respectively. It is clearly visible that we are able to improve the detection of very small-sized () lesions and sensitivity is improved across all the organs of the body. In addition to the above, we present qualitative comparison results (at FP=2) of DSA-ULD in Fig.~\ref{fig:lesion_size}(d) and demonstrate that false positives reduce drastically after the incorporation of domain knowledge in deep networks.






\vspace{-1mm}
\begin{table}[!ht]
\begin{center}

\setlength{\tabcolsep}{0.6\tabcolsep}\begin{tabular}{|c|c|l|c|c|c|}
\hline
\textbf{Sr. No.} &\textbf{HU windows}  &\textbf{Attention}  & \textbf{Avg. Sensitivity} 
\\ \hline
1 &1  &  & 82.29 \\ 
2 &3  &  & 83.71 \\
3 &5  &  & 84.61 \\
4 & \textbf{5}  & \textbf{\checkmark}  & \textbf{85.79} \\ 
\hline
\end{tabular}
\vspace{-2mm}
\caption{\small{Ablation studies and average sensitivity (\%) on introducing different no. of HU windows and attention based feature fusion in (\emph{DSA-ULD}) on the DeepLesion test-set.}}
\label{tab:ablation}
\end{center}
\vspace{-4mm}
\end{table}
\vspace{-1mm}

Now, we provide an ablation study on the effect of introducing different numbers of HU windows and attention based feature-fusion in our proposed network DSA-ULD. Table~\ref{tab:ablation} illustrates that when we use our 5 HU windows to generate a multi-intensity input image, we obtain a considerable boost in average sensitivity () as compared to using a single HU-window (). Following this, on applying self-attention based feature fusion, we obtain a further increment in average sensitivity (). 




\vspace{-4mm}
\section{Conclusion}
\label{sec:conclusion}
\vspace{-2mm}
We presented an anchor-free one-stage ULD network called DSA-ULD which is also augmented with explicit domain-driven information such as multi-intensity images, feature-fusion using self-attention and self-supervision techniques for efficient and robust lesion detection in CT-scans. We demonstrate that our proposed anchor-free DSA-ULD performs at par with anchor-based lesion detection methods on the DeepLesion test-set while being very simple and computationally-efficient. We also illustrate that the incorporation of domain knowledge in DSA-ULD removes the need of training on heterogeneous datasets. Going forward, we would like to propose a domain-adaptive ULD which can perform effectively on datasets coming from different scanners, domains and hospitals, etc.



\vspace{-2mm}
\small{
\bibliographystyle{IEEEbib}
\bibliography{refs.bib}
}

\end{document}
