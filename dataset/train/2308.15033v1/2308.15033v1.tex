\documentclass{article}


\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{lipsum}
\usepackage{fancyhdr}       \usepackage{graphicx}       \graphicspath{{media/}}     \usepackage{tabularx}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{caption}
\usepackage{subcaption}

\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 





  
\title{STEC: See-Through Transformer-based Encoder for CTR Prediction
}

\author{
 Serdarcan Dilbaz \\
  AI Enablement\\
  Huawei Türkiye R\&D Center\\
  Istanbul, Turkey \\
  \texttt{serdarcan.dilbaz@huawei.com} \\
   \And
 Hasan Saribas \\
  AI Enablement\\
  Huawei Türkiye R\&D Center\\
  Istanbul, Turkey \\
  \texttt{hasan.saribas1@huawei.com}
}

\begin{document}
\maketitle

\begin{abstract}
Click-Through Rate (CTR) prediction holds a pivotal place in online advertising and recommender systems since CTR prediction performance directly influences the overall satisfaction of the users and the revenue generated by companies. Even so, CTR prediction is still an active area of research since it involves accurately modelling the preferences of users based on sparse and high-dimensional features where the higher-order interactions of multiple features can lead to different outcomes.

Most CTR prediction models have relied on a single fusion and interaction learning strategy. The few CTR prediction models that have utilized multiple interaction modelling strategies have treated each interaction to be self-contained. In this paper, we propose a novel model named STEC that reaps the benefits of multiple interaction learning approaches in a single unified architecture. Additionally, our model introduces residual connections from different orders of interactions which boosts the performance by allowing lower level interactions to directly affect the predictions. Through extensive experiments on four real-world datasets, we demonstrate that STEC outperforms existing state-of-the-art approaches for CTR prediction thanks to its greater expressive capabilities.
\end{abstract}


\begin{figure}[t]
\centering
\includegraphics[width=0.5\columnwidth]{media/architecture.png}
\caption{The overall STEC architecture uses  stacked STEC blocks and fuses  group bilinear interactions from different levels to form a single CTR prediction.}
\label{fig:architecture}
\end{figure}


\section{Introduction}

Click-through rate (CTR) prediction is a crucial task in the fields of online advertising and recommender systems, as it aims to forecast the probability that users will engage with displayed ads or content on online platforms \cite{wu2019session, yu2020tagnn}. Despite its extensive application across a variety of domains, including web search, recommendation systems, and online advertising, CTR prediction remains a challenging task. This is due to numerous factors such as sparse and high-dimensional feature spaces, the need to capture complex higher-order feature interactions, imbalanced data distributions, temporal dynamics, and shifts in user behavior. Accurate CTR prediction is of significant importance not only for advertisers but also for online platforms seeking to optimize user engagement and revenue generation \cite{autoint2019}. By accurately measuring the likelihood of user engagement with displayed ads, CTR prediction models enable advertisers to effectively allocate resources and adapt their campaigns to target specific user segments. Simultaneously, by serving ads that take individual preferences into account, platforms can improve user experiences and increase the value of the overall ecosystem \cite{fignn2019, liu2015convolutional}.

Historically, traditional methodologies for click-through rate (CTR) prediction have relied on statistical techniques and manually designed features. Despite the complex, non-linear interactions present in user behavior and contextual variables, these methods have achieved some measure of success \cite{rendle2010factorization, mcmahan2013ad}. One such technique is Factorization Machines (FM), which have demonstrated effectiveness in capturing second-order feature interactions by computing the inner product of feature embeddings within each interaction \cite{rendle2010factorization}. This approach has proven successful in addressing the challenges posed by CTR prediction.

Recent advancements in deep learning models have led to significant achievements in various domains, including computer vision (CV) \cite{krizhevsky2012imagenet, simonyan2014very, szegedy2015going} and natural language processing (NLP) \cite{chai2019deep, goldberg2022neural}. Methods developed specifically for these domains have also contributed to solving problems in other fields. In the area of recommendation, numerous deep learning-based methods have primarily focused on more effectively capturing higher-order feature interactions \cite{guo2017deepfm, cheng2016wide, yu2020tagnn, lian2018xdeepfm}. After achieving notable results using these methods, approaches such as FiBiNET \cite{fibinet2019} and MaskNet \cite{masknet2021} have attempted to better determine the significance of embeddings using attention mechanisms such as the Squeeze-Excitation network (SENET) \cite{hu2018squeeze}. Following the successful application of attentional mechanisms in computer vision and natural language processing, attention-based methods such as AFM, AutoInt, and DESTINE have been developed for CTR prediction with promising results. However, methods like AutoInt and DESTINE have a high parameter count, which has made their applicability challenging.

One of the primary challenges in click-through rate (CTR) prediction is the effective modeling of feature interactions \cite{guo2017deepfm, masknet2021, yu2020tagnn}. FM \cite{rendle2010factorization} has proposed a successful method for capturing second-order feature interactions through the use of inner product calculations. DeepFM \cite{guo2017deepfm} combines an FM module with a deep neural network to achieve improved performance in the CTR domain. To model high-order feature interactions, xDeepFM \cite{lian2018xdeepfm} introduced the Compressed Interaction Network (CIN) layer, which utilizes outer-product calculations. Other models, such as HOFM \cite{blondel2016higher} and AFN \cite{cheng2020adaptive}, have also been used to extract high-order feature interactions. However, these models are not efficient in terms of time complexity. To address this issue, DeepIM \cite{yu2020tagnn} proposed the use of an IM layer instead of an FM layer to compute higher-order interactions using Newton’s identities for increased efficiency.

In this paper, we present a rigorous proof demonstrating that bilinear interactions can be derived from the commonly used scaled dot-product attention. Building on this insight, we introduce a novel model, STEC, that integrates multiple interaction learning approaches into a single, unified architecture by leveraging the bilinear interactions present in attention computations. Furthermore, our model incorporates residual connections among various orders of interactions, enhancing performance by enabling lower-level interactions to directly impact predictions. Extensive experiments on four real-world datasets demonstrate that STEC surpasses existing state-of-the-art methods for CTR prediction due to its superior expressive capabilities. An ablation study on the STEC architecture confirms the effectiveness of the novel concepts introduced.
Our main contributions can be listed as follows:
\begin{itemize}
  \item To the best of our knowledge, we are the first to expose and leverage the bilinear interaction hidden inside the scaled dot-product attention calculations to model higher-order feature interactions.
  \item Based on the proposed modified attention mechanism, we proposed the STEC architecture and conducted extensive experiments on 4 different public datasets; the proposed model achieved state-of-the-art scores on all of them except for the Criteo dataset, where it achieved comparable results.
\item As opposed to the existing approaches, STEC also introduces direct connections between different interaction levels and the output which  increases the performance compared to the traditional architecture where lower order interactions affect the prediction by only influencing higher order interactions.
\end{itemize}

\section{Related Works}

\subsection{Bilinear Interaction}

As previously discussed, a variety of interaction learning approaches have been used for CTR. Nevertheless, for the purposes of this paper, bilinear interaction is of particular importance. Models utilizing bilinear interaction have achieved notable results on numerous real-world datasets owing to the effectiveness of bilinear interaction layer at learning feature importance and fine-grained feature interactions \cite{fibinet2019, fibinetpp2022}.

\subsection{Attention Networks}

The attention mechanism, first introduced in deep learning as an enhancement to encoder-decoder-based neural machine translation systems in NLP \cite{attention2016}, has since been applied to a wide range of applications. Initially, attention mechanisms were applied to recurrent systems, but models that rely solely on attention have become more popular due to their parallelizability and faster training times \cite{vaswani2023attention}. Attention mechanisms and their variants have also been used in other fields, including computer vision and speech processing \cite{guo2022attention}.

In the field of sequential recommendation, where the goal is to predict a user’s next action based on their sequence of interactions, attention mechanisms have also been successfully applied since this task is already sequential in nature. Models such as DIN \cite{din2018}, DIEN \cite{dien2018}, BST \cite{bst2019}, DMIN \cite{dmin2020} have introduced novel attention mechanisms to achieve significant improvements in CTR performance.

Numerous studies have explored the use of attention as an interaction learning approach on user and item embeddings. For example, the AFM \cite{afm2017} model introduces attention to the FM mechanism to improve its performance. The AutoInt \cite{autoint2019} model utilizes the multi-head self-attention mechanism  with residual connections to explicitly model feature interactions. The DESTINE \cite{destine2021} model employs a similar structure to AutoInt but uses disentangled attention through whitening. The InterHAt \cite{interhat2020} model employs the Transformer architecture with multi-head attention and a hierarchical structure for learning feature interactions.

\section{Our Proposed Model}

\subsection{Feature Embedding}
In CTR prediction, input instances typically comprise three types of features: user, item, and context. These features may be either sparse categorical or numerical, and input instances are represented as the sparse vector  where  is the number of feature fields:

Due to the large number of unique values in the categorical features for CTR prediction, one-hot vectors are not a practical representation for input features. Instead, these features are embedded into a lower-dimensional space to enable CTR models to learn dense representations. For a categorical feature , the corresponding embedding is calculated as:     

where  is the embedding matrix for  unique feature values in the  dimensional embedding space. Unlike the shared embedding layer used in NLP tasks, a separate embedding layer is employed for each categorical feature.
Although numerical features are more amenable to being utilized in deep learning structures, these features still need to be embedded into the same dimensional space as the categorical features to allow for straightforward calculation of feature interactions. For a numerical feature  , the corresponding embedding is calculated as:

where  is scaled by the normalized numerical feature values.
For a given instance, when both categorical and numerical features undergo feature embedding,  is formed by stacking of embedding  from different feature fields. 

\subsection{STEC Block}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{media/Block.png}
         \caption{}
         \label{fig:stec_block}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{media/multihead.png}
         \caption{}
         \label{fig:multihead}
     \end{subfigure}
        \caption{(a) The STEC block builds on the Scaled Dot-Product Attention to accommodate concurrent bilinear interaction learning. The Hadamard Product and AvgPool in conjunction are equivalent to matrix multiplication. (b) The STEC Block is able to run multiple attention layers and learn bilinear interactions in parallel.}
        \label{fig:inner_blocks}
\end{figure}


The core component of the STEC architecture is the STEC block, which is designed to simultaneously extract bilinear interactions by modifying the existing scaled dot-product attention formulation.
For an input  , following the scaled dot-product attention formulation, the corresponding query and key matrices are given as: 

\noindent
\begin{tabularx}{\linewidth}{@{}XX@{}}

&

\end{tabularx}
where the learned projection matrices for the query and key are denoted as  respectively.
The scaled dot-product attention is given by:

The learnable projection matrices for the query and the key can be reduced to a single learnable parameter.

The first matrix multiplication in the equation above can be converted into an inner product:  

The remaining matrix multiplication can be expressed as a summation of a hadamard product over the model dimension  : 

where

The above equation for  is the identical formulation for the bilinear interaction used by the FiBiNet and FiBiNet++ models \cite{fibinet2019, fibinetpp2022}. Consequently, the STEC block alters the scaled dot-product attention structure to also learn the bilinear interaction between the query and key as seen in Figure \ref{fig:stec_block}.

Motivated by the success of multi-head attention, we also leverage the multi-head mechanism for the attention as well as the bilinear interactions as seen in Figure \ref{fig:multihead}. Grouping the bilinear interactions allows the model to jointly learn the different interaction subspaces at different positions.

\subsection{STEC Architecture}

The STEC architecture is inspired by the Transformer architecture where self-attention and position-wise feed-forward networks (FFN) are alternately stacked. Similarly, the STEC architecture interlaces multiple layers of STEC blocks and FFNs to perform CTR prediction, shown in Figure \ref{fig:architecture}. In contrast to the self-attention layer of the Transformer, the STEC layer produces two outputs simultaneously. The first output is mathematically identical to the self-attention described in the Transformer, while the second output corresponds to the bilinear interaction of the input. Bilinear interactions from STEC blocks at different levels are fused together and processed by a multilayer perceptron (MLP) to produce a final prediction.

\subsubsection{Position-wise Feed-Forward Networks}

The Transformer architecture employs FFNs between attention layers \cite{vaswani2023attention}. An FFN takes the hidden attention representation  at layer  and applies a two-layer MLP, typically using a rectified-linear activation function on the hidden layer. The FFN can be formally described as:



\subsubsection{Final Bilinear Interaction Layer}

As seen in Figure \ref{fig:stec_block}, The STEC blocks produce two outputs: an attention output and a bilinear interaction. Due to the stacked design of the architecture, the attention output from layer  is connected to the STEC block from layer  via a FFN layer. To utilize the attention output from the final STEC block, a standalone bilinear interaction layer is used, although it has been omitted from Figure \ref{fig:architecture} for brevity. As a result,  stacked layers produce  bilinear interactions.

\subsubsection{Concatenation Layer}

The STEC architecture combines learned interactions from multiple orders of interaction through the concatenation operation. Since the distributions of the interactions can be vary greatly, batch normalization is applied to interactions at each level before they are fused.



\section{Experimental Results}

In this section, we move forward to evaluate the effectiveness of our proposed approach by addressing the following research questions:

\begin{table}[h]
\centering
\begin{tabularx}{\columnwidth}{lX}
    \textbf{RQ1} & How does our proposed STEC architecture compared to the existing approaches in terms of performance?                                                  \\
    \textbf{RQ2} & Does the the STEC architecture compare to approaches that rely on attention in terms of performance as well as computational cost? \\
    \textbf{RQ3} & How does the design choices affect the performance of the proposed model? \\
\end{tabularx}
\end{table}


\begin{table}[h]
\centering
\caption{Summary Statistics of the four public CTR datasets}
\label{table:datasets}
\begin{tabular}{@{}l|c|c|c@{}}
\toprule
Dataset      & \#Instance & \#Fields & \#Features (Sparse) \\ \midrule
Criteo       &  46M         & 39        & 2.1M                   \\
Avazu        & 40M         & 23        & 1.5M                   \\
MovieLens    & 2M          & 7        & 90k                   \\
Frappe       & 289k          & 10        & 5.4k                   \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Experiment Setup}

\subsubsection{Datasets}

We evaluate our approach on four widely used public datasets for CTR prediction: Criteo\footnote{\url{http://labs.criteo.com/2013/12/download-terabyte-click-logs}}, Avazu\footnote{\url{https://www.kaggle.com/c/avazu-ctr-prediction/data}}, MovieLens\footnote{\url{https://grouplens.org/datasets/movielens/1m/}}, and Frappe\footnote{\url{https://www.baltrunas.info/context-aware}}. To eliminate any advantages that could be gained through random splits and preprocessing steps, we applied the same splits and preprocessing steps for all models based on \cite{cheng2020adaptive}.

\subsubsection{Training Objective}

The training objective for both our proposed approach and the baseline models was binary cross-entropy loss, as defined by Equation \ref{eq:binarycross} where , ,   represent the number of samples, the ground truth label, and the predicted label, respectively.



\subsubsection{Evaluation Metric}
In accordance with established CTR prediction conventions, we primarily evaluated the performance of our proposed approach and the baseline models using the Area Under the ROC Curve (AUC) metric. We reported our AUC results to four significant digits, as it is well established that even small increases in AUC can lead to significant improvements in CTR prediction and revenue \cite{cheng2016wide}.

Logloss is also widely used in binary classification to measure how closely the predicted probabilities match the corresponding ground truth values. We report this metric as well, as a smaller logloss indicates that the model was better able to model the probabilities.

\subsubsection{Baseline Models}

For baseline comparisons, we consider any model that has achieved competitive results on any of the four datasets. We grouped attention-based models separately to assess the effectiveness of our attention mechanism compared to existing models. Abbreviations for the models listed below were used whenever possible for brevity.

\begin{itemize}
  \item \textbf{IPNN}: Product-based Neural Networks \cite{pnn2016}
  \item \textbf{Wide\&Deep}: Wide\&Deep Learning \cite{cheng2016wide}
  \item \textbf{DCN}: Deep\&Cross Network \cite{deepcross2017}
  \item \textbf{DeepFM}: Deep Factorization Machine \cite{guo2017deepfm}
  \item \textbf{xDeepFM}: eXtreme Deep Factorization Machine \cite{lian2018xdeepfm}
  \item \textbf{FiBiNET}: Feature Importance and Bilinear feature Interaction NETwork \cite{fibinet2019}
  \item \textbf{AFN+}: Adaptive Factorization Network \cite{cheng2020adaptive}
  \item \textbf{DeepIM}: Deep Interaction Machine \cite{deepim2020}
  \item \textbf{AOANet}: Architecture and Operation Adaptive Network \cite{aoanet2020}
  \item \textbf{DCNV2}: Improved Deep\&Cross Network \cite{dcnv2}
  \item \textbf{EDCN}: Enhanced Deep\&Cross Network \cite{edcn2021}
  \item \textbf{MaskNet}: MaskNet \cite{masknet2021}
  \item \textbf{FinalMLP}: Enhanced Two-Stream MLP Model \cite{mao2023finalmlp}
  \item \textbf{AFM}: Attentional Factorization Machines \cite{afm2017}
  \item \textbf{InterHAt}: Interpretable Click-Through Rate Prediction through
Hierarchical Attention \cite{interhat2020}
  \item \textbf{AutoInt+}: Automatic Feature Interaction Learning via Self-Attentive Neural Network \cite{autoint2019}
  \item \textbf{DESTINE}: Disentangled Self-Attentive Neural Network \cite{destine2021}
\end{itemize}

\subsubsection{Implementation Details}

The implementation of our models is in PyTorch and compatible with the open-source CTR prediction library FuxiCTR \cite{barsctr}. We use Adam optimizer with a learning rate of 0.001 where the learning rate is reduced by a factor of 10 whenever the validation logloss stops improving. The mini-batch size for all experiment is 4096. We tune the hyper-parameters such as the number of layers, dimension size, and the number of heads model for each setting with grid search. We conduct our experiments on 2 NVIDIA A40 GPUs.

\begin{table}[t]
\caption{Performance comparisons on Avazu and Criteo datasets w.r.t. AUC and Logloss. The best results are in \textbf{bold} and the second best results are reported in \textit{italics}.}
\label{table:sota}
\resizebox{\columnwidth}{!}{
\begin{tabular}{llcccccc}
\hline
\multirow{2}{*}{Type}  & \multirow{2}{*}{Model} & \multicolumn{3}{c}{Avazu}   & \multicolumn{3}{c}{Criteo} \\ \cline{3-8}
                              &                        & AUC   & Impr.   & LogLoss   & AUC    & Impr.   & LogLoss   \\\hline \hline
\multirow{13}{*}{Base Models} & IPNN                   & 76.30 & 0.45\%  & 0.3676    & 81.37  & 0.07\%  & 0.4383     \\
                              & Wide\&Deep             & 76.49 & 0.20\%  & 0.3665    & 81.39  & 0.05\%  & 0.4380     \\
                              & DCN                    & 76.52 & 0.16\%  & 0.3664    & 81.38  & 0.06\%  & 0.4378     \\
                              & DeepFM                 & 76.48 & 0.21\%  & 0.3667    & 81.37  & 0.07\%  & 0.4381     \\ 
                              & xDeepFM                & 76.47 & 0.22\%  & 0.3671    & 81.39  & 0.05\%  & 0.4380     \\
                              & FiBiNET                & 76.45 & 0.25\%  & 0.3670    & 81.30  & 0.16\%  & 0.4388     \\
                              & AFN+                   & 76.40 & 0.35\%  & 0.3672    & 81.43  & 0.00\% & 0.4377     \\
                              & DeepIM                 & 76.45 & 0.25\%  & 0.3670    & 81.39  & 0.05\%  & 0.4380     \\
                              & AOANet                 & 76.54 & 0.13\%  & 0.3664    & 81.41  & 0.02\% & 0.4378     \\
                              & DCNV2                  & 76.56 & 0.10\%  & 0.3664    & 81.42  & 0.01\%  & 0.4378     \\
                              & EDCN                   & 76.52 & 0.16\%  & 0.3670    & \textit{81.47}  & 0.00\% & \textit{0.4373}     \\
                              & MaskNet                & 76.43 & 0.27\%  & 0.3674    & 81.39  & 0.05\%  & 0.4380     \\
                              & FinalMLP               & \textit{76.63} & 0.01\%  & \textit{0.3658}    & \textbf{81.48}  & -0.06\% & \textbf{0.4370}     \\ \hline
\multirow{5}{*}{\makecell{
Attention Based \\ Models}}   & AFM                    & 75.74 & 1.19\%  & 0.3705    & 80.44  & 1.23\%  & 0.4470     \\ 
                              & InterHAt               & 75.45 & 1.58\%  & 0.3721    & 80.74  & 0.85\%  & 0.4441     \\
                              & AutoInt+               & 75.45 & 0.25\%  & 0.3668    & 81.39  & 0.05\%  & 0.4379     \\
                              & DESTINE                & 76.61 & 0.04\%  & 0.3661    & 81.39  & 0.05\%  & 0.4380     \\
                              & STEC (Ours)            & \textbf{76.64} & Base    & \textbf{0.3658}    & 81.43  & Base    & 0.4379     \\ \hline

\end{tabular}}
\end{table}

\begin{table}[t]
\caption{Performance comparisons on Frappe and MovieLens datasets w.r.t. AUC and Logloss. The best results are in \textbf{bold} and the second best results are reported in \textit{italics}.}
\label{table:sota2}
\resizebox{\columnwidth}{!}{
\begin{tabular}{llcccccc}
\hline
\multirow{2}{*}{Type}  & \multirow{2}{*}{Model} & \multicolumn{3}{c}{Frappe}   & \multicolumn{3}{c}{MovieLens} \\ \cline{3-8}
                              &                        & AUC   & Impr.   & LogLoss   & AUC    & Impr.   & LogLoss   \\\hline \hline
\multirow{13}{*}{Base Models} & IPNN                   & 98.41 & 0.16\%  & 0.1540    & 96.99  & 0.13\%  & 0.2095     \\
                              & Wide\&Deep             & 98.41 & 0.16\%  & 0.1490    & 96.88  & 0.24\%  & 0.2161     \\
                              & DCN                    & 98.39 & 0.18\%  & 0.1491    & 96.87  & 0.25\%  & 0.2147     \\
                              & DeepFM                 & 98.45 & 0.12\%  & 0.1482    & 96.85  & 0.27\%  & 0.2130     \\ 
                              & xDeepFM                & 98.45 & 0.12\%  & 0.1466    & 96.97  & 0.15\%  & 0.2409     \\
                              & FiBiNET                & 98.32 & 0.25\%  & 0.1941    & 95.76  & 1.42\%  & 0.2518     \\
                              & AFN+                   & 98.26 & 0.32\%  & 0.2139    & 96.42  & 0.72\%  & 0.3030     \\
                              & DeepIM                 & 98.44 & 0.13\%  & 0.1490    & 96.93  & 0.19\%  & \textit{0.2099}     \\
                              & AOANet                 & 98.44 & 0.13\%  & \textbf{0.1424}    & 96.94  & 0.18\%  & 0.2105     \\
                              & DCNV2                  & 98.45 & 0.12\%  & 0.1491    & 96.91  & 0.21\%  & 0.2147     \\
                              & EDCN                   & 98.50 & 0.07\%  & 0.1547    & 96.71  & 0.42\%  & 0.2122     \\
                              & MaskNet                & 98.37 & 0.20\%  & 0.1696    & 96.72  & 0.41\%  & 0.2364     \\
                              & FinalMLP               & \textit{98.55} & 0.02\%  & 0.1484    & \textbf{97.20}  & -0.09\% & 0.2246     \\ \hline
\multirow{5}{*}{\makecell{
Attention Based \\ Models}}   & AFM                    & 96.97 & 1.65\%  & 0.2264    & 94.72  & 2.53\%  & 0.2653     \\ 
                              & InterHAt               & 97.41 & 1.19\%  & 0.1919    & 95.03  & 2.20\%  & 0.2540     \\
                              & AutoInt+               & 98.48 & 0.09\%  & 0.1490    & 96.92  & 0.20\%  & 0.2148     \\
                              & DESTINE                & 98.56 & 0.01\%  & 0.1434    & 96.94  & 0.18\%  & 0.2125     \\
                              & STEC (Ours)            & \textbf{98.57} & Base    & 0.1477    & \textit{97.12}  & Base    & \textbf{0.2016}     \\ \hline

\end{tabular}}
\end{table}

\subsection{Quantitative Results (RQ1)}

We obtained results for CTR models from the open-source CTR prediction library FuxiCTR whenever possible. Additional experiments for baseline models were conducted only if no previously reported results were available on BarsCTR \cite{barsctr}. The performances reported here and on BarsCTR were selected from runs with the best performance. We observed that the performance of most models may vary depending on the initial random seed choice, so we reported the best measured performance to provide a fair comparison. In subsequent sections, we analyze the effects of hyperparameter choices and discuss the impact of random initialization conditions by performing multiple runs.

Based on the results presented in Tables \ref{table:sota} and \ref{table:sota2}, we observed that some CTR models perform competitively on a select few datasets. For example, the IPNN model performs exceptionally well on the MovieLens dataset but lags in performance on other datasets. Some CTR models have been specifically tuned to maximize performance on certain datasets, resulting in significant deterioration when tested on different datasets.

A select few models perform competitively across all datasets. The STEC model demonstrates its flexible expressive abilities by performing well on all four public datasets. Models such as FinalMLP leverage MLPs with up to three hidden layers to perform CTR predictions, but our STEC model achieves state-of-the-art performance even when using a single linear layer to make predictions based on bilinear interactions. Overall, the STEC model performs on par with or better than state-of-the-art models while following a single-stream structure, and its expressiveness can be adjusted based on the dataset by modifying hyperparameters such as the number of layers, hidden dimension, or MLP structure.

The proposed method achieves the best results on the Avazu and Frappe datasets in terms of AUC metric by 76.64\% and 98.57\%, respectively, on the MovieLens dataset, it obtains the second best results in terms of the AUC metric, while outperforming this method with better logloss values. In the case of the Criteo dataset, the proposed method obtains comparable results with FinalMLP, EDCN, AFN+ models. Furthermore, the FinalMLP method involves an additional step compared to the others, as the user and item features need to specified beforehand. The proposed STEC architecture achieves equivalent results without any prior knowledge of the features.

\subsection{Comparison to Attention-based Models (RQ2)}

\begin{figure}[t]
\centering
\includegraphics[width=0.9\columnwidth]{media/auc_logloss_flop_plot.pdf}
\caption{STEC outperforms other attention-based models in terms of AUC and logloss with lower FLOPs.}
\label{fig:auc_logloss_flop_plot}
\end{figure}

It is crucial to compare STEC with other attention-based models, as attention is central to the STEC architecture. The number of floating-point operations (FLOPs) serves as a proxy for the computational complexity of deep learning models. As shown in Figure \ref{fig:auc_logloss_flop_plot}, aside from the AFM model, the number of FLOPs for attention-based models is several orders of magnitude larger than that of the STEC model. This is due to the fact that the STEC model performs attention calculations in lower-dimensional spaces and leverages intermediate calculation steps to improve its performance.

In terms of performance, the STEC model outperforms all attention-based models across all four datasets. Our experiments show that extracting intermediate bilinear interactions improves the performance of models that rely solely on attention. The AutoInt model, which relies only on stacked multi-head attention layers, is unable to perform competitively despite its significantly higher computational cost compared to our proposed model. In the next section, we analyze the effect of using intermediate outputs instead of final outputs and discuss other novel aspects of our model.

\subsection{Analysis (RQ3)}



In this section, we conduct ablation studies on the design of the STEC architecture to investigate the impact of various design choices on its performance. We consider several variations of the STEC architecture:

\begin{itemize}
  \item \textbf{STEC\textsubscript{BL}} replaces the STEC block with a parallel multi-head attention and bilinear interaction blocks.
  \item \textbf{STEC\textsubscript{NF}} removes the FFN from the STEC\textsubscript{BL} to make the overall structure resemble the AutoInt model.
  \item \textbf{STEC\textsubscript{LO}} only uses the bilinear interaction from the final STEC block by removing the bilinear interactions from intermediate layers.
  \item \textbf{STEC\textsubscript{F}} replaces the concatenation of bilinear interactions with addition.
\end{itemize}

\begin{table}[t]
\centering
\caption{Ablation studies on Avazu, Criteo, Frappe and MovieLens datasets in terms of AUC metric.}
\label{table:ablation}
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|c|c|c|c|c|c|c}
\hline
Model                  & Bilinear & Fusion    & FFN & Avazu          & Criteo         & Frappe         & MovieLens      \\ \hline
STEC\textsubscript{BL} & Exp      & Concat    & \checkmark   & 76.22          & 81.22          & 98.44          & 97.03          \\ \hline 
STEC\textsubscript{NF} & Exp      & Concat    & \textit{X}   & 76.56          & 81.25          & 98.35          & 96.89           \\ \hline
STEC\textsubscript{LO} & Imp      & Last Only & \checkmark   & 75.83          & 81.39          & 98.42          & 97.03           \\ \hline
STEC\textsubscript{F}  & Imp      & Add       & \checkmark   & 76.08          & 81.34          & 98.43          & 97.08           \\ \hline
STEC                   & Imp      & Concat    & \checkmark   & \textbf{76.64} & \textbf{81.43} & \textbf{98.57} & \textbf{97.12}  \\ \hline
\end{tabular}}
\end{table}

We use the STEC\textsubscript{LO} model to assess  the contribution of intermediate layers to overall performance. Unlike baseline click-through rate (CTR) models, which do not typically shorten the distance between intermediate representations and output, our approach allows lower-order interactions to directly affect prediction output without traversing through higher-order layers. Our results demonstrate that including intermediate bilinear interactions significantly improves performance across all settings.

We also compare various fusion strategies for bilinear interactions, including concatenation and addition. Our results show that concatenating information from varying complexities performs better than addition, although addition (STEC\textsubscript{F}) still outperforms using only the final interaction output (STEC\textsubscript{LO}). This further supports our argument for leveraging intermediate information sources.

Finally, we compare the performance of STEC\textsubscript{NF} and STEC\textsubscript{BL} to evaluate the impact of the FFN inside the attention block on performance. Our results show that for more challenging datasets such as Avazu and Criteo, the FFN can improve performance, while for smaller datasets such as Frappe and MovieLens, it may degrade performance by increasing model complexity.

\section{Conclusion and Future Work}

In conclusion, we propose a strong and versatile architecture called STEC, which extracts bilinear interaction from attention with minimal overhead to achieve state-of-the-art results. Our experiments demonstrate that our approach is not dataset-specific and performs well across a range of datasets. Furthermore, we show that introducing direct connections between different interaction levels and output improves performance by reducing the distance between lower-level interactions and output. Additionally, we demonstrate that reformulating existing blocks such as scaled dot-product attention can yield performance gains without incurring additional computational cost.

In future work, we plan to investigate whether current stacked architectures of CTR models can be modified to perform higher-order interaction calculations only when lower-order interactions are not sufficient for prediction. Our proposed architecture can potentially accommodate such a structure if the multilayer perceptron (MLP) on bilinear interactions is altered to contain the effect of each interaction within the same layer.

\bibliographystyle{unsrt}  
\bibliography{aaai24}  


\end{document}
