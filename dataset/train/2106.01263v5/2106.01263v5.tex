\section{Experiments}

\subsection{Experimental Setup} \label{setup}

We initialize our implementation with the BERT \citep{devlin2018bert} checkpoint provided by the Huggingface package\footnote{https://huggingface.co/models}.  We also test post-training \citep{whang2021response, han2021fine} on top of pre-trained BERT when the checkpoints are available. The post-trained checkpoints are provided by \citet{han2021fine}. As introduced in Section \ref{background}, the post-training strategy is a common technique to adapt the general pre-trained knowledge to the target domain. In practice, it continues the models' pre-training on domain-specific texts before fine-tuning them on downstream tasks to attain better performances. All the experiments are run on six NVIDIA A100-SXM4-40GB GPUs with CUDA 11.1. We use the Noam scheduler and the Adam optimizer with , , and weight decay . For experiments on the Ubuntu Corpus V2, we use a peak lr of 2e-4. As we want each dataset to reach the maximum batch size in training, their learning rates are also adjusted accordingly in Section \ref{results}. As for the loss function, we add masked language modeling (MLM) loss on top of the classification loss with the same weight coefficients. We use the average token embedding from each candidate as the input to the softmax function. Models are all run until they converge, measured by a validation set. 


\subsection{Dataset and Evaluation Metrics} \label{dataset}

In this section, we evaluate the proposed \textit{Uni-Encoder} across four standard datasets, i.e., PersonaChat~\citep{zhang2018personalizing},  Ubuntu Dialogue Corpus V1 \citep{lowe2015ubuntu}, Ubuntu Dialogue Corpus V2 \citep{lowe2017training}, and Douban Conversation Corpus \citep{wu2016sequential}.

\noindent\textbf{PersonaChat} \citep{zhang2018personalizing} is a crowd-sourced dataset with two-speaker talks conditioned on their given persona, containing short descriptions of characters they will imitate in the dialogue.

\noindent\textbf{Ubuntu Dialogue Corpus V1} \citep{lowe2015ubuntu} contains 1 million conversations about technical support for the Ubuntu system. We use the clean version proposed by \citet{xu2017incorporating}, which has numbers, URLs, and system paths replaced by special placeholders.

\noindent\textbf{Ubuntu Dialogue Corpus V2} \citep{lowe2017training} has several updates and bug fixes compared to V1. The major one is that the training, validation, and test sets are split into different periods. We choose this dataset to conduct a detailed study of \textit{Uni-Encoder} as it is the only dataset that \textit{Poly-Encoder} \citep{humeau2019poly} uses and has complete train/dev/test sets published.

\noindent\textbf{Douban Conversation Corpus} \citep{wu2016sequential} consists of web-crawled dyadic dialogs from a Chinese social networking website called Douban. Topics in this dataset are open-domain, and all the conversations are longer than two turns. Unlike other datasets where each context only has one proper response, the test set of Douban provides multiple proper responses.



\begin{table}[t]
\centering
\resizebox{0.96\linewidth}{!}{\begin{tabular}{@{}clccc@{}}
\toprule
\multicolumn{2}{c}{Dataset}                              & Train & Valid  & Test    \\ \midrule
\multirow{2}{*}{PersonaChat} & \multicolumn{1}{c}{Turns} & 65,719 & 7,801   & 7,512    \\
                             & Positive:Negative         & 1:19  & 1:19   & 1:19    \\ \midrule
\multirow{2}{*}{Ubuntu V1}   & \multicolumn{1}{c}{Pairs} & 1M    & 0.5M   & 0.5M    \\
                             & Positive:Negative         & 1:1   & 1:9    & 1:9     \\ \midrule
\multirow{2}{*}{Ubuntu V2}   & \multicolumn{1}{c}{Pairs} & 1M    & 195.6k & 189.2k  \\
                             & Positive:Negative         & 1:1   & 1:9    & 1:9     \\ \midrule
\multirow{2}{*}{Douban}      & \multicolumn{1}{c}{Pairs} & 1M    & 50k    & 6,670    \\
                             & Positive:Negative         & 1:1   & 1:1    & 1.2:8.8 \\ \bottomrule
\end{tabular}}
\caption{Statistics of four benchmark datasets.}
\label{tab:dataset}
\end{table}










\begin{table*}[th]
\small
\centering
{\begin{tabular}{@{}lcccccc@{}}
\toprule
\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Paradigm\end{tabular}} &
  \multirow{2}{*}{Setup} &
  \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Bs per GPU\end{tabular}} &
  \multicolumn{4}{c}{Ubuntu Corpus V2} \\ \cmidrule(l){4-7} 
                       &                                                                                   &   &  &  &  & MRR   \\ \midrule
(i) \ \ \ Uni-Encoder  & \begin{tabular}[c]{@{}c@{}}Arrow Attn\\ w/ Res Concat\end{tabular}                   & 8 & \textbf{0.859} & \textbf{0.938} & 0.990 & \textbf{0.915} \\ 
\midrule
(ii) \begin{tabular}[l]{@{}l@{}}  \ \ Uni-Encoder \\ \ w/o Repeated Position ID  \end{tabular}     & \begin{tabular}[c]{@{}c@{}} Arrow Attn\\ w/ Res Concat \end{tabular} & 8 & 0.837 & 0.933 & \textbf{0.992} & 0.903 \\

\midrule
(iii) \ Concat-Cross-Encoder  & \begin{tabular}[c]{@{}c@{}}Square Attn\\ w/ Res Concat\end{tabular}                   & 8 & 0.826 & 0.916 & 0.980 & 0.892 \\ \midrule
(iv) \ \ Cross-Encoder    & \begin{tabular}[c]{@{}c@{}}Square Attn\\ w/o Res Concat\end{tabular}                  & 5 & 0.844 & 0.930 & 0.987 & 0.905 \\ \midrule
(v) \ \ \ Bi-Encoder        & \begin{tabular}[c]{@{}c@{}}Diagonal Attn\\ w/ Res Concat\end{tabular}                   & 8 & 0.835 & 0.925 & 0.987 & 0.899 \\ \midrule
(vi) \ \ \ Poly-Encoder       & \begin{tabular}[c]{@{}c@{}}Light-Arrow Attn (360)\\ w/ Res Concat\end{tabular} & 8 & 0.844 & 0.929 & 0.989 & 0.906 \\ 
\bottomrule
\end{tabular}}
\caption{Comparisons between different paradigms implemented according to the setups described in Section \ref{attention_mechanism}. By replacing the attention mechanism in \textit{Uni-Encoder}, a unified framework can simulate different paradigms, which optimally controls all other training variables for fair comparisons. Please note the \textit{Cross-Encoder} (iii) cannot reach the same large batch size as the others as it is more memory-intensive. For \textit{Poly-Encoder}, we choose the best setting with 360 context codes.}
\label{tab:attn}

\vspace{-1mm}

\end{table*}




The statistics of four benchmark datasets are shown in Table \ref{tab:dataset}. They vary greatly in volume, language, and topic. During training, we recycle the other labels in the same batch as negative samples instead of using the pre-defined negative candidates in each dataset. Several metrics are used to evaluate our model following previous works. We use  to evaluate the model performance across four datasets. The mean reciprocal rank (MRR) metric is additionally calculated for PersonChat and Douban Conversation Corpus datasets. In the Douban Conversation Corpus, we also report the  and mean average precision (MAP) values because it contains multiple positive candidates for a given context. It is also noted that the proportion of the positive and negative samples of the validation set is significantly different from that of the test set in the Douban Conversation Corpus. To alleviate this discrepancy, we also utilize the in-batch negative labels in the validation stage to determine an appropriate checkpoint for inference. 






\begin{table*}[ht]
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{@{}lcccccccccc@{}}
\toprule
\multirow{2}{*}{Models} &
  \multicolumn{3}{c}{Ubuntu Corpus V2} &
   &
   &
   &
  \multicolumn{2}{c}{PersonaChat} &
  \multicolumn{2}{c}{} \\ \cmidrule(lr){2-4} \cmidrule(lr){8-9}
 &
   &
   &
   &
   &
  \multicolumn{1}{l}{} &
   &
   &
  MRR &
  \multicolumn{1}{l}{} &
   \\ \midrule
BERT \citep{devlin2018bert} &
  0.781 &
  0.890 &
  0.980 &
   &
  \multicolumn{1}{l}{} &
   &
  0.707 &
  0.808 &
  \multicolumn{1}{l}{} &
   \\
Poly-Encoder 360 \citep{humeau2019poly} &
  0.809 &
  - &
  0.981 &
  \multicolumn{1}{l}{} &
  \multicolumn{1}{l}{} &
  \multicolumn{1}{l}{} &
  - &
  - &
  \multicolumn{1}{l}{} &
  \multicolumn{1}{l}{} \\
SA-BERT \citep{gu2020speaker} &
  0.830 &
  0.919 &
  0.985 &
   &
  \multicolumn{1}{l}{} &
   &
  - &
  - &
  \multicolumn{1}{l}{} &
   \\
BERT-CRA \citep{gu2021partner} &
  - &
  - &
  - &
   &
  \multicolumn{1}{l}{} &
   &
  0.843 &
  0.903 &
  \multicolumn{1}{l}{} &
   \\ \midrule
Uni-Encoder (Ours) &
  \textbf{\begin{tabular}[c]{@{}c@{}}0.859\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}c@{}}0.938\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}c@{}}0.990\end{tabular}} &
   &
  \multicolumn{1}{l}{} &
   &
  \textbf{\begin{tabular}[c]{@{}c@{}}0.869\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}c@{}}0.922\end{tabular}} &
  \multicolumn{1}{l}{} &
   \\ \midrule
\multirow{2}{*}{} &
  \multicolumn{3}{c}{Ubuntu Corpus V1} &
   &
  \multicolumn{6}{c}{Douban Conversation Corpus} \\ \cmidrule(lr){2-4} \cmidrule(l){6-11} 
 &
   &
   &
   &
   &
  MAP &
  MRR &
   &
   &
   &
   \\ \midrule
BERT \citep{devlin2018bert} &
  0.808 &
  0.897 &
  0.975 &
   &
  0.591 &
  0.633 &
  0.454 &
  0.280 &
  0.470 &
  0.828 \\
SA-BERT \citep{gu2020speaker} &
  0.855 &
  0.928 &
  0.983 &
   &
  0.619 &
  0.659 &
  0.496 &
  0.313 &
  0.481 &
  0.847 \\
BERT-SL \citep{xu2020learning} &
  0.884 &
  0.946 &
  0.990 &
   &
  - &
  - &
  - &
  - &
  - &
  - \\
BERT+FGC \citep{li2021small} &
  0.829 &
  0.910 &
  0.980 &
   &
  0.614 &
  0.653 &
  0.495 &
  0.312 &
  0.495 &
  0.850 \\
UMS \citep{whang2021response} &
  0.843 &
  0.920 &
  0.982 &
   &
  0.597 &
  0.639 &
  0.466 &
  0.285 &
  0.471 &
  0.829 \\
MDFN \citep{liu2021filling} &
  0.866 &
  0.932 &
  0.984 &
  \multicolumn{1}{l}{} &
  0.624 &
  0.663 &
  0.498 &
  0.325 &
  0.511 &
  0.855 \\
SA-BERT+HCL \citep{su2020dialogue} &
  0.867 &
  0.940 &
  0.992 &
  \multicolumn{1}{l}{} &
  0.639 &
  0.681 &
  0.514 &
  \textbf{0.330} &
  0.531 &
  0.858 \\
UMS \citep{whang2021response} &
  0.875 &
  0.942 &
  0.988 &
  \multicolumn{1}{l}{} &
  0.625 &
  0.664 &
  0.499 &
  0.318 &
  0.482 &
  0.858 \\
BERT-UMS+FGC \citep{li2021small} &
  0.886 &
  0.948 &
  0.990 &
  \multicolumn{1}{l}{} &
  0.627 &
  0.670 &
  0.500 &
  0.326 &
  0.512 &
  0.869 \\
BERT-FP \citep{han2021fine} &
  0.911 &
  0.962 &
  \textbf{0.994} &
  \multicolumn{1}{l}{} &
  0.644 &
  0.680 &
  0.512 &
  0.324 &
  0.542 &
  \textbf{0.870} \\ \midrule
Uni-Encoder (Ours) &
  \begin{tabular}[c]{@{}c@{}}0.886\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}0.946\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}0.989\end{tabular} &
   &
  \begin{tabular}[c]{@{}c@{}}0.622\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}0.662\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}0.481\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}0.303\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}0.514\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}0.852\end{tabular} \\
Uni-Enc+BERT-FP (Ours) &
  \textbf{\begin{tabular}[c]{@{}c@{}}0.916\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}c@{}}0.965\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}c@{}}0.994\end{tabular}} &
  \multicolumn{1}{l}{} &
  \textbf{\begin{tabular}[c]{@{}c@{}}0.648\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}c@{}}0.688\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}c@{}}0.518\end{tabular}} &
  \begin{tabular}[c]{@{}c@{}}0.327\end{tabular} &
  \textbf{\begin{tabular}[c]{@{}c@{}}0.557\end{tabular}} &
  \begin{tabular}[c]{@{}c@{}}0.865\end{tabular} \\ \bottomrule
\end{tabular}}
\caption{Evaluations on four benchmark datasets. The models marked with  have been post-trained, and the others are fine-tuned based on the naive BERT \citep{devlin2018bert}.  denotes statistical significance with p-value  0.05.}
\label{tab:full}
\end{table*}


\subsection{Validating Our Design Choices}
\label{attn}

In this section, we will validate our two design choices through a set of controlled experiments. As described in Section \ref{inputs_to_ranking} and \ref{attention_mechanism}, we are able to simulate different paradigms by replacing the attention mechanism in \textit{Uni-Encoder} with some minor modifications. We thus conduct experiments in this unified framework to control all other variables and make the fairest comparisons. Note that the \textit{Cross-Encoder} (iii) has to repeatedly encode the same lengthy context with every candidate, resulting in high memory usage and smaller batch size (5 in our experiments). The experimental results are shown in Table \ref{tab:attn}. 

\paragraph{Why Repeating Position ID for Responses?}
Let us first compare the results in Row (i) vs. Row (ii), where the only difference is that Row (i) use the same set of position IDs for all responses while Row (ii) has unique position IDs. Uni-Encoder with repeated position ID has significantly better results. This observation confirms our hypothesis that our responses should be treated equally.  


\paragraph{Why using Full Attention Between Context and Responses?} If we compare the results of Row (i) with Row (v) and Row (vi), where the main differences lie in how much attention we have between context and responses, we can see that full attention can significantly boost performance. In fact, the more interaction (attention) they have, the better results they can get. Specifically, Poly-Encoder in Row(vi) has more interaction than Bi-Encoder in Row (v), and Uni-Encoder in Row (i) has more interaction than Poly-Encoder. These comparisons validate our design choices for full attention between context and responses. 


\paragraph{Why Avoiding Attention Among Responses?}

Comparing results in Row (i) and Row (iii), we can see that if we allow attention among responses, the performance drops significantly. This is easy to understand because if we allow attention among responses, it will be difficult for the ranker to distinguish them. 

\paragraph{Why Avoiding Recomputing the Context?}
It is easy to understand that if we recompute the lengthy context, the computational time increases dramatically, which we will measure quantitatively in Section \ref{speed}. Here we show another dimension of the consequence of recomputing the context. As shown in Row (iv), the repetitive computation of the context stops the \textit{Cross-Encoder} from having a large batch size because of the memory constraint. However, a good enough batch size, hence negative samples, is important for a multi-choice setting, as examined in \citet{humeau2019poly}. As a result, the performance of \textit{Cross-Encoder} (iv) is only on par with \textit{Poly-Encoder} (vi).



\subsection{Comparison with State-of-the-Art Methods} \label{results}


We compare \textit{Uni-Encoder} with the existing state-of-the-art methods in Table \ref{tab:full}. Noted that, different from the comparison in Table \ref{tab:attn}, the methods in Table \ref{tab:full} are not entirely comparable as they have different additional training tricks. And these tricks often have a high impact on the performance of these methods. The only message we want to deliver here is that \textit{Uni-Encoder} can achieve state-of-the-art performance even without some of these complex training tricks.



For Ubuntu Corpus V1 and Douban Conversation Corpus, we also employ the advanced post-training model from \citet{han2021fine} and list the results separately with  as it significantly affects the results and not all the methods use it.  



As shown in Table \ref{tab:full}, \textit{Uni-Encoder} achieves the best overall performance across all four benchmarks. For example, it improves the  value on PersonaChat, Ubuntu V1, and Ubuntu V2 datasets by , , and , respectively.


However, \textit{Uni-Encoder} only achieves the best results on the Douban Corpus on four of the six metrics. We conjecture that the positive example size discrepancy between the training set and test set is the reason for its poorer performance. In \textit{Uni-Encoder}, we have chosen the multi-choice setting, assuming there is only one positive response. This setting allows us to leverage response concatenation and in-batch negative training to separate the positive sample from negative examples. However, multiple positive candidates in Douban Corpus at inference time (but not in training) break this assumption and may confuse the network. Our future study will quantify the impact of this assumption. 



\textit{Uni-Encoder} also outperforms some of the more complex methods that rely on expensive training tricks, such as \citet{liu2021filling} adapted BiGRU to capture conversation-level representations, and \citet{su2020dialogue} leveraged hierarchical curriculum learning in their work. These approaches typically yield better outcomes, but at the expense of increased training budgets. In contrast, \textit{Uni-Encoder} only retains the MLM loss from pre-training and adds two extra tokens to distinguish between different speakers.





\begin{figure}[ht]

  \centering

  \includegraphics[width=\linewidth]{speed.png}

\caption{The inference time comparison for \textit{Uni-Encoder} and other paradigms on the Ubuntu V2 test set. Please note that \textit{Poly-Encoder} cannot pre-compute candidate embeddings in a generation-based dialogue system, so the results differ from those reported in \citet{humeau2019poly} on retrieval tasks.}
\label{tab:speed}

\end{figure}









\subsection{Lower Computational Cost} \label{speed}
In addition to the accuracy gain, we also see that \textit{Uni-Encoder} is computational efficiency compared to other paradigms. We test it on the Ubuntu V2 test set (189,200 contexts). The implementation of \textit{Cross-} and \textit{Poly-Encoder} follows the method proposed in \citet{humeau2019poly}.


Despite the fact that candidate pools in generation-based dialogue systems are typically small, we are interested in understanding the performance of \textit{Uni-Encoder} with enlarged pools. To this end, we vary the pool size from 10 and 20 to 50 and 100 for each context by randomly selecting additional candidates from the corpus. We then conducted all speed tests on a single NVIDIA A100-SXM4-40GB with CUDA 11.1. The batch size for each paradigm was maximized as much as possible.
The results are presented in Figure 2. \textit{Uni-Encoder} demonstrates  faster inference speed compared to \textit{Cross-Encoder} when the pool size is appropriate. As the pool size increases, the advantages of \textit{Uni-Encoder} become more pronounced. Compared with \textit{Poly-Encoder}, \textit{Uni-Encoder} exhibits a similar trend, with slightly better overall efficiency. Furthermore, we have also deployed \textit{Uni-Encoder} in a commercial psychotherapy chatbot to rank the responses generated by large language models (LLMs). It has shown to be even more advantageous in this real-world dialogue application, as it returns results with only one forward pass, thus reducing the latency caused by other factors such as data transfer.


















