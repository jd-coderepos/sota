\documentclass[letterpaper]{article}

\usepackage{include/aaai18}
\nocopyright
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{url}
\usepackage{graphicx}
\frenchspacing

\usepackage{subcaption} 
\usepackage{dblfloatfix}

\usepackage{booktabs}

\newcommand{\citet}[1]{\citeauthor{#1}~\shortcite{#1}}
\newcommand{\citep}{\cite}
\newcommand{\citealp}[1]{\citeauthor{#1}~\citeyear{#1}}

\usepackage{comment}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{siunitx}
\usepackage{wrapfig}
\usepackage{todonotes}
\usepackage{enumitem}
\usepackage{siunitx}
\usepackage{dsfont}
\usepackage{xcolor}
\usepackage{tablefootnote}
\usepackage[para,online,flushleft]{threeparttable}
\usepackage{multirow}
\usepackage{mathtools}






\newcommand{\choice}[2]{\left(\!\!\! \begin{array}{c} #1 \\ #2\end{array} 
\!\!\!\right)}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\defeq}{:=}
\newcommand{\real}{\mathbb{R}}


\newcommand{\given}{\|}
\newcommand{\indep}[2]{{#1} \perp {#2}}
\newcommand{\condindep}[3]{{#1} \perp {#2} | {#3}}
\newcommand{\condindepG}[3]{{#1} \perp_G {#2} | {#3}}
\newcommand{\condindepP}[3]{{#1} \perp_p {#2} | {#3}}
\newcommand{\depend}[2]{{#1} \not \perp {#2}}
\newcommand{\conddepend}[3]{{#1} \not \perp {#2} | {#3}}

\newcommand{\trans}[1]{{#1}^{T}}
\newcommand{\inv}[1]{{#1}^{-1}}

\newcommand{\ra}{\rightarrow}
\newcommand{\lra}{\leftrightarrow}
\newcommand{\Ra}{\Rightarrow}
\newcommand{\la}{\leftarrow}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\st}{\; \mathrm{s.t.} \;}
\newcommand{\size}{\mathrm{size}}
\newcommand{\trace}{\mathrm{trace}}

\newcommand{\pemp}{p_\mathrm{emp}}
\newcommand{\dom}{\mathrm{dom}}
\newcommand{\bel}{\mathrm{bel}}
\newcommand{\dsep}{\mathrm{dsep}}
\newcommand{\sep}{\mathrm{sep}}
\newcommand{\entails}{\models}
\newcommand{\range}{\mathrm{range}}
\newcommand{\myspan}{\mathrm{span}}
\newcommand{\nullspace}{\mathrm{nullspace}}
\newcommand{\adj}{\mathrm{adj}}
\newcommand{\pval}{\mathrm{pvalue}}
\newcommand{\NLL}{\mathrm{NLL}}


\newcommand{\betadist}{\mathrm{Beta}}
\newcommand{\Betadist}{\mathrm{Beta}}
\newcommand{\bernoulli}{\mathrm{Ber}}
\newcommand{\Ber}{\mathrm{Ber}}
\newcommand{\Binom}{\mathrm{Bin}}
\newcommand{\NegBinom}{\mathrm{NegBinom}}
\newcommand{\binomdist}{\mathrm{Bin}}
\newcommand{\cauchy}{\mathrm{Cauchy}}
\newcommand{\DE}{\mathrm{DE}}
\newcommand{\DP}{\mathrm{DP}}
\newcommand{\Dir}{\mathrm{Dir}}
\newcommand{\discrete}{\mathrm{Cat}}
\newcommand{\Discrete}{\discrete}
\newcommand{\expdist}{\mathrm{Exp}}
\newcommand{\expon}{\mathrm{Expon}}
\newcommand{\gammadist}{\mathrm{Ga}}
\newcommand{\Ga}{\mathrm{Ga}}
\newcommand{\GP}{\mathrm{GP}}
\newcommand{\GEM}{\mathrm{GEM}}
\newcommand{\gauss}{{\cal N}}
\newcommand{\erlang}{\mathrm{Erlang}}
\newcommand{\IG}{\mathrm{IG}}
\newcommand{\IGauss}{\mathrm{InvGauss}}
\newcommand{\IW}{\mathrm{IW}}
\newcommand{\Laplace}{\mathrm{Lap}}
\newcommand{\logisticdist}{\mathrm{Logistic}}
\newcommand{\Mu}{\mathrm{Mu}}
\newcommand{\Multi}{\mathrm{Mu}}
\newcommand{\NIX}{NI\chi^2}
\newcommand{\GIX}{NI\chi^2}
\newcommand{\NIG}{\mathrm{NIG}}
\newcommand{\GIG}{\mathrm{NIG}}
\newcommand{\NIW}{\mathrm{NIW}}
\newcommand{\GIW}{\mathrm{NIW}}
\newcommand{\MVNIW}{\mathrm{NIW}}
\newcommand{\NW}{\mathrm{NWI}}
\newcommand{\NWI}{\mathrm{NWI}}
\newcommand{\MVNIG}{\mathrm{NIG}}
\newcommand{\NGdist}{\mathrm{NG}}
\newcommand{\prob}{p}
\newcommand{\Poi}{\mathrm{Poi}}
\newcommand{\Student}{{\cal T}}
\newcommand{\student}{{\cal T}}
\newcommand{\Wishart}{\mathrm{Wi}}
\newcommand{\Wi}{\mathrm{Wi}}
\newcommand{\unif}{\mathrm{U}}
\newcommand{\etr}{\mathrm{etr}}


\newcommand{\loss}{\calL}
\newcommand{\mse}{\mathrm{mse}}
\newcommand{\pon}{\rho}
\newcommand{\lse}{\mathrm{lse}}
\newcommand{\softmax}{\calS}
\newcommand{\soft}{\mathrm{soft}}
\newcommand{\cond}{\mathrm{cond}}
\newcommand{\sign}{\mathrm{sign}}
\newcommand{\sgn}{\mathrm{sgn}}
\newcommand{\iid}{\mbox{iid}}
\newcommand{\mle}{\mbox{mle}}
\newcommand{\myiff}{\mbox{iff}}
\newcommand{\pd}{\mbox{pd}}
\newcommand{\pdf}{\mbox{pdf }}
\newcommand{\cdf}{\mbox{cdf}}
\newcommand{\pmf}{\mbox{pmf}}
\newcommand{\wrt}{\mbox{wrt}}
\newcommand{\matlab}{{\sc MATLAB}}
\newcommand{\NETLAB}{{\sc NETLAB}}
\newcommand{\MLABA}{\mbox{PMTK}}
\newcommand{\BLT}{\mbox{PMTK}}
\newcommand{\PMTK}{\mbox{PMTK}}
\newcommand{\mywp}{\mathrm{wp}}

\newcommand{\KLpq}[2]{\mathbb{KL}\left({#1}||{#2}\right)}
\newcommand{\KL}{\mathbb{KL}}
\newcommand{\MI}{\mathbb{I}}
\newcommand{\MIxy}[2]{\mathbb{I}\left({#1};{#2}\right)}
\newcommand{\MIxyz}[3]{\mathbb{I}\left({#1};{#2}|{#3}\right)}
\newcommand{\entrop}{\mathbb{H}}
\newcommand{\entropy}[1]{\mathbb{H}\left({#1}\right)}
\newcommand{\entropypq}[2]{\mathbb{H}\left({#1}, {#2}\right)}

\newcommand{\myvec}[1]{\mathbf{#1}}
\newcommand{\myvecsym}[1]{\boldsymbol{#1}}
\newcommand{\ind}[1]{\mathbb{I}(#1)}



\newcommand{\vzero}{\myvecsym{0}}
\newcommand{\vone}{\myvecsym{1}}

\newcommand{\valpha}{\myvecsym{\alpha}}
\newcommand{\vbeta}{\myvecsym{\beta}}
\newcommand{\vBeta}{\myvecsym{B}}
\newcommand{\vchi}{\myvecsym{\chi}}
\newcommand{\vdelta}{\myvecsym{\delta}}
\newcommand{\vDelta}{\myvecsym{\Delta}}
\newcommand{\vepsilon}{\myvecsym{\epsilon}}
\newcommand{\vell}{\myvecsym{\ell}}
\newcommand{\veta}{\myvecsym{\eta}}
\newcommand{\vgamma}{\myvecsym{\gamma}}
\newcommand{\vGamma}{\myvecsym{\Gamma}}
\newcommand{\vmu}{\myvecsym{\mu}}
\newcommand{\vmut}{\myvecsym{\tilde{\mu}}}
\newcommand{\vnu}{\myvecsym{\nu}}
\newcommand{\vkappa}{\myvecsym{\kappa}}
\newcommand{\vlambda}{\myvecsym{\lambda}}
\newcommand{\vLambda}{\myvecsym{\Lambda}}
\newcommand{\vLambdaBar}{\overline{\vLambda}}
\newcommand{\vomega}{\myvecsym{\omega}}
\newcommand{\vOmega}{\myvecsym{\Omega}}
\newcommand{\vphi}{\myvecsym{\phi}}
\newcommand{\vPhi}{\myvecsym{\Phi}}
\newcommand{\vpi}{\myvecsym{\pi}}
\newcommand{\vPi}{\myvecsym{\Pi}}
\newcommand{\vpsi}{\myvecsym{\psi}}
\newcommand{\vPsi}{\myvecsym{\Psi}}
\newcommand{\vtheta}{\myvecsym{\theta}}
\newcommand{\vthetat}{\myvecsym{\tilde{\theta}}}
\newcommand{\vTheta}{\myvecsym{\Theta}}
\newcommand{\vsigma}{\myvecsym{\sigma}}
\newcommand{\vSigma}{\myvecsym{\Sigma}}
\newcommand{\vSigmat}{\myvecsym{\tilde{\Sigma}}}
\newcommand{\vtau}{\myvecsym{\tau}}
\newcommand{\vxi}{\myvecsym{\xi}}

\newcommand{\vmuY}{\vb}
\newcommand{\vmuMu}{\vmu_{x}}
\newcommand{\vmuMuGivenY}{\vmu_{x|y}}
\newcommand{\vSigmaMu}{\vSigma_{x}}
\newcommand{\vSigmaMuInv}{\vSigma_{x}^{-1}}
\newcommand{\vSigmaMuGivenY}{\vSigma_{x|y}}
\newcommand{\vSigmaMuGivenYinv}{\vSigma_{x|y}^{-1}}
\newcommand{\vSigmaY}{\vSigma_{y}}
\newcommand{\vSigmaYinv}{\vSigma_{y}^{-1}}

\newcommand{\muY}{\mu_{y}}
\newcommand{\muMu}{\mu_{\mu}}
\newcommand{\muMuGivenY}{\mu_{\mu|y}}
\newcommand{\SigmaMu}{\Sigma_{\mu}}
\newcommand{\SigmaMuInv}{\Sigma_{\mu}^{-1}}
\newcommand{\SigmaMuGivenY}{\Sigma_{\mu|y}}
\newcommand{\SigmaMuGivenYinv}{\Sigma_{\mu|y}^{-1}}
\newcommand{\SigmaY}{\Sigma_{y}}
\newcommand{\SigmaYinv}{\Sigma_{y}^{-1}}

\newcommand{\hatf}{\hat{f}}
\newcommand{\haty}{\hat{y}}
\newcommand{\const}{\mathrm{const}}
\newcommand{\sigmoid}{\mathrm{sigm}}

\newcommand{\one}{(1)}
\newcommand{\two}{(2)}

\newcommand{\va}{\myvec{a}}
\newcommand{\vb}{\myvec{b}}
\newcommand{\vc}{\myvec{c}}
\newcommand{\vd}{\myvec{d}}
\newcommand{\ve}{\myvec{e}}
\newcommand{\vf}{\myvec{f}}
\newcommand{\vg}{\myvec{g}}
\newcommand{\vh}{\myvec{h}}
\newcommand{\vj}{\myvec{j}}
\newcommand{\vk}{\myvec{k}}
\newcommand{\vl}{\myvec{l}}
\newcommand{\vm}{\myvec{m}}
\newcommand{\vn}{\myvec{n}}
\newcommand{\vo}{\myvec{o}}
\newcommand{\vp}{\myvec{p}}
\newcommand{\vq}{\myvec{q}}
\newcommand{\vr}{\myvec{r}}
\newcommand{\vs}{\myvec{s}}
\newcommand{\vt}{\myvec{t}}
\newcommand{\vu}{\myvec{u}}
\newcommand{\vv}{\myvec{v}}
\newcommand{\vw}{\myvec{w}}
\newcommand{\vws}{\vw_s}
\newcommand{\vwt}{\myvec{\tilde{w}}}
\newcommand{\vWt}{\myvec{\tilde{W}}}
\newcommand{\vwh}{\hat{\vw}}
\newcommand{\vx}{\myvec{x}}
\newcommand{\vxt}{\myvec{\tilde{x}}}
\newcommand{\vy}{\myvec{y}}
\newcommand{\vyt}{\myvec{\tilde{y}}}
\newcommand{\vz}{\myvec{z}}

\newcommand{\vra}{\myvec{r}_a}
\newcommand{\vwa}{\myvec{w}_a}
\newcommand{\vXa}{\myvec{X}_a}


\newcommand{\vA}{\myvec{A}}
\newcommand{\vB}{\myvec{B}}
\newcommand{\vC}{\myvec{C}}
\newcommand{\vD}{\myvec{D}}
\newcommand{\vE}{\myvec{E}}
\newcommand{\vF}{\myvec{F}}
\newcommand{\vG}{\myvec{G}}
\newcommand{\vH}{\myvec{H}}
\newcommand{\vI}{\myvec{I}}
\newcommand{\vJ}{\myvec{J}}
\newcommand{\vK}{\myvec{K}}
\newcommand{\vL}{\myvec{L}}
\newcommand{\vM}{\myvec{M}}
\newcommand{\vMt}{\myvec{\tilde{M}}}
\newcommand{\vN}{\myvec{N}}
\newcommand{\vO}{\myvec{O}}
\newcommand{\vP}{\myvec{P}}
\newcommand{\vQ}{\myvec{Q}}
\newcommand{\vR}{\myvec{R}}
\newcommand{\vS}{\myvec{S}}
\newcommand{\vT}{\myvec{T}}
\newcommand{\vU}{\myvec{U}}
\newcommand{\vV}{\myvec{V}}
\newcommand{\vW}{\myvec{W}}
\newcommand{\vX}{\myvec{X}}
\newcommand{\vXs}{\vX_{s}}
\newcommand{\vXt}{\myvec{\tilde{X}}}
\newcommand{\vY}{\myvec{Y}}
\newcommand{\vZ}{\myvec{Z}}
\newcommand{\vZt}{\myvec{\tilde{Z}}}
\newcommand{\vzt}{\myvec{\tilde{z}}}

\newcommand{\vxtest}{\myvec{x}_*}
\newcommand{\vytest}{\myvec{y}_*}


\newcommand{\ftrue}{f_{true}}

\newcommand{\myprec}{\mathrm{prec}}
\newcommand{\precw}{\lambda_{w}} \newcommand{\precy}{\lambda_{y}} \newcommand{\fbar}{\overline{f}}
\newcommand{\xmybar}{\overline{x}}
\newcommand{\ybar}{\overline{y}}
\newcommand{\rbar}{\overline{r}}
\newcommand{\zbar}{\overline{z}}
\newcommand{\vAbar}{\overline{\vA}}
\newcommand{\vxbar}{\overline{\vx}}
\newcommand{\vXbar}{\overline{\vX}}
\newcommand{\vybar}{\overline{\vy}}
\newcommand{\vYbar}{\overline{\vY}}
\newcommand{\vzbar}{\overline{\vz}}
\newcommand{\vZbar}{\overline{\vZ}}
\newcommand{\xbar}{\overline{x}}
\newcommand{\wbar}{\overline{w}}
\newcommand{\Xbar}{\overline{X}}
\newcommand{\Ybar}{\overline{Y}}
\newcommand{\Gbar}{\overline{G}}
\newcommand{\Jbar}{\overline{J}}
\newcommand{\Lbar}{\overline{L}}
\newcommand{\Nbar}{\overline{N}}
\newcommand{\Qbar}{\overline{Q}}
\newcommand{\Tbar}{\overline{T}}
\newcommand{\Sbar}{\overline{S}}
\newcommand{\vSbar}{\overline{\vS}}
\newcommand{\Rbar}{\overline{R}}

\newcommand{\vtaubar}{\overline{\vtau}}
\newcommand{\vtbar}{\overline{\vt}}
\newcommand{\vsbar}{\overline{\vs}}
\newcommand{\mubar}{\overline{\mu}}
\newcommand{\phibar}{\overline{\phi}}


\newcommand{\htilde}{\tilde{h}}
\newcommand{\vhtilde}{\tilde{\vh}}
\newcommand{\Dtilde}{\tilde{D}}
\newcommand{\Ftilde}{\tilde{F}}
\newcommand{\wtilde}{\tilde{w}}
\newcommand{\ptilde}{\tilde{p}}
\newcommand{\pstar}{p^*}
\newcommand{\xtilde}{\tilde{x}}
\newcommand{\Xtilde}{\tilde{X}}
\newcommand{\ytilde}{\tilde{y}}
\newcommand{\Ytilde}{\tilde{Y}}
\newcommand{\vxtilde}{\tilde{\vx}}
\newcommand{\vytilde}{\tilde{\vy}}
\newcommand{\ztilde}{\tilde{\z}}
\newcommand{\vthetaMAP}{\hat{\vtheta}_{MAP}}
\newcommand{\vthetaS}{\vtheta^{(s)}}
\newcommand{\vthetahat}{\hat{\vtheta}}
\newcommand{\thetahat}{\hat{\theta}}
\newcommand{\thetabar}{\overline{\theta}}
\newcommand{\vthetabar}{\overline{\vtheta}}
\newcommand{\pibar}{\overline{\pi}}
\newcommand{\vpibar}{\overline{\vpi}}

\newcommand{\RSS}{\mathrm{RSS}}
\newcommand{\mydof}{\mathrm{dof}}



\newcommand{\vvec}{\mathrm{vec}}
\newcommand{\kron}{\otimes}
\newcommand{\dof}{\mathrm{dof}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\energy}{E}
\newcommand{\expectAngle}[1]{\langle #1 \rangle}
\newcommand{\expect}[1]{\mathbb{E}\left[ {#1} \right]}
\newcommand{\expectQ}[2]{\mathbb{E}_{{#2}} \left[ {#1} \right]}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\var}[1]{\mathrm{var}\left[{#1}\right]}
\newcommand{\std}[1]{\mathrm{std}\left[{#1}\right]}
\newcommand{\varQ}[2]{\mathrm{var}_{{#2}}\left[{#1}\right]}
\newcommand{\cov}[1]{\mathrm{cov}\left[{#1}\right]}
\newcommand{\corr}[1]{\mathrm{corr}\left[{#1}\right]}
\newcommand{\median}[1]{\mathrm{median}\left[{#1}\right]}




\newcommand{\sech}{\mathrm{sech}}
\newcommand{\kurt}{\mathrm{kurt}}
\newcommand{\proj}{\mathrm{proj}}
\newcommand{\myskew}{\mathrm{skew}}
\newcommand{\rank}{\mathrm{rank}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\blkdiag}{\mathrm{blkdiag}}
\newcommand{\bias}{\mathrm{bias}}
\newcommand{\union}{\cup}
\newcommand{\intersect}{\cap}


\newcommand{\myc}{c}
\newcommand{\myi}{i}
\newcommand{\myj}{j}
\newcommand{\myk}{k}
\newcommand{\myn}{n}
\newcommand{\myq}{q}
\newcommand{\mys}{s}
\newcommand{\myt}{t}



\newcommand{\kernelfn}{\kappa}

\newcommand{\Nsamples}{S}
\newcommand{\Ndata}{n}
\newcommand{\Ntrain}{n_{\mathrm{train}}}
\newcommand{\Ntest}{n_{\mathrm{test}}}
\newcommand{\Ndim}{d}
\newcommand{\Ndimx}{d_x}
\newcommand{\Ndimy}{d_y}
\newcommand{\Nhidden}{h}
\newcommand{\Noutdim}{d_y}
\newcommand{\Nlowdim}{l}
\newcommand{\Ndimlow}{l}
\newcommand{\Nstates}{K}
\newcommand{\Nfolds}{K}
\newcommand{\Npastates}{L}
\newcommand{\Nclasses}{C}
\newcommand{\Nclusters}{K}
\newcommand{\NclustersC}{C}
\newcommand{\Ntime}{T}
\newcommand{\Ntimes}{T}
\newcommand{\Niter}{T}
\newcommand{\Nnodes}{D}

\newcommand{\assign}{\leftarrow}







\newcommand{\ki}{i}
\newcommand{\kj}{j}
\newcommand{\kk}{k}
\newcommand{\kC}{C}
\newcommand{\kc}{c}

\newcommand{\supp}{\mathrm{supp}}
\newcommand{\query}{\calQ}
\newcommand{\vis}{\calE}
\newcommand{\nuisance}{\calN}
\newcommand{\hid}{\calH}

\newcommand{\advanced}{*}




\newcommand{\bbI}{\mathbb{I}}
\newcommand{\bbL}{\mathbb{L}}
\newcommand{\bbM}{\mathbb{M}}
\newcommand{\bbS}{\mathbb{S}}


\newcommand{\calA}{{\cal A}}
\newcommand{\calB}{{\cal B}}
\newcommand{\calC}{{\cal C}}
\newcommand{\calD}{{\cal D}}
\newcommand{\calDx}{{\cal D}_x}
\newcommand{\calE}{{\cal E}}
\newcommand{\cale}{{\cal e}}
\newcommand{\calF}{{\cal F}}
\newcommand{\calG}{{\cal G}}
\newcommand{\calH}{{\cal H}}
\newcommand{\calHX}{{\cal H}_X}
\newcommand{\calHy}{{\cal H}_y}
\newcommand{\calI}{{\cal I}}
\newcommand{\calK}{{\cal K}}
\newcommand{\calM}{{\cal M}}
\newcommand{\calN}{{\cal N}}
\newcommand{\caln}{{\cal n}}
\newcommand{\calNP}{{\cal NP}}
\newcommand{\calMp}{\calM^+}
\newcommand{\calMm}{\calM^-}
\newcommand{\calMo}{\calM^o}
\newcommand{\Ctest}{C_*}
\newcommand{\calL}{{\cal L}}
\newcommand{\calU}{{\cal U}}
\newcommand{\calP}{{\cal P}}
\newcommand{\calq}{{\cal q}}
\newcommand{\calQ}{{\cal Q}}
\newcommand{\calR}{{\cal R}}
\newcommand{\calS}{{\cal S}}
\newcommand{\calSstar}{\calS_*}
\newcommand{\calT}{{\cal T}}
\newcommand{\calV}{{\cal V}}
\newcommand{\calv}{{\cal v}}
\newcommand{\calX}{{\cal X}}
\newcommand{\calY}{{\cal Y}}

\newcommand{\Lone}{}
\newcommand{\Ltwo}{}

\newcommand{\score}{\mbox{score}}
\newcommand{\AIC}{\mbox{AIC}}
\newcommand{\BIC}{\mbox{BIC}}
\newcommand{\BICcost}{\mbox{BIC-cost}}
\newcommand{\scoreBIC}{\mbox{score-BIC}}
\newcommand{\scoreBICL}{\mbox{score-BIC-L1}}
\newcommand{\scoreL}{\mbox{score-L1}}

\newcommand{\ecoli}{\mbox{{\it E. coli}}}
\newcommand{\doPearl}{\mathrm{do}}
\newcommand{\data}{\calD}
\newcommand{\model}{\calM}
\newcommand{\dataTrain}{\calD_{\mathrm{train}}}
\newcommand{\dataTest}{\calD_{\mathrm{test}}}
\newcommand{\dataValid}{\calD_{\mathrm{valid}}}
\newcommand{\Xtrain}{\vX_{\mathrm{train}}}
\newcommand{\Xtest}{\vX_{\mathrm{test}}}
\newcommand{\futuredata}{\tilde{\calD}}
\newcommand{\algo}{\calA}
\newcommand{\fitAlgo}{\calF}
\newcommand{\predictAlgo}{\calP}
\newcommand{\err}{\mathrm{err}}
\newcommand{\logit}{\mathrm{logit}}

\newcommand{\nbd}{\mathrm{nbd}}
\newcommand{\nbr}{\mathrm{nbr}}
\newcommand{\anc}{\mathrm{anc}}
\newcommand{\desc}{\mathrm{desc}}
\newcommand{\pred}{\mathrm{pred}}
\newcommand{\mysucc}{\mathrm{suc}}
\newcommand{\nondesc}{\mathrm{nd}}
\newcommand{\pa}{\mathrm{pa}}
\newcommand{\parent}{\mathrm{pa}}
\newcommand{\copa}{\mathrm{copa}}
\newcommand{\ch}{\mathrm{ch}}
\newcommand{\mb}{\mathrm{mb}}
\newcommand{\connects}{\sim}
\newcommand{\nd}{\mathrm{nd}}
\newcommand{\bd}{\mathrm{bd}}
\newcommand{\cl}{\mathrm{cl}}



\newcommand{\be}{}
\newcommand{\bea}{}
\newcommand{\beaa}{}



\newcommand{\conv}[1]{\,\,\,\displaystyle{\operatorname*{\longrightarrow}^{\,_{#1}\,}}\,\,\,}
\newcommand{\dconv}{\conv{D}}
\newcommand{\pconv}{\conv{P}}
\newcommand{\asconv}{\conv{AS}}
\newcommand{\lpconv}[1]{\conv{L^{#1}}}

\DeclareMathAlphabet{\mathpzc}{OT1}{pzc}{m}{n}




\newcommand{\vfj}{\vf_j}
\newcommand{\vfk}{\vf_k}

\newcommand{\entropyBethe}{\mathbb{H}_{\mathrm{Bethe}}}
\newcommand{\entropyKikuchi}{\mathbb{H}_{\mathrm{Kikuchi}}}
\newcommand{\entropyEP}{\mathbb{H}_{\mathrm{ep}}}
\newcommand{\entropyConvex}{\mathbb{H}_{\mathrm{Convex}}}

\newcommand{\freeEnergyBethe}{F_{\mathrm{Bethe}}}
\newcommand{\freeEnergyKikuchi}{F_{\mathrm{Kikuchi}}}
\newcommand{\freeEnergyConvex}{F_{\mathrm{Convex}}}

\newcommand{\sigmaMle}{\hat{\sigma}^2_{mle}}
\newcommand{\sigmaUnb}{\hat{\sigma}^2_{unb}}


\newcommand{\keywordSpecial}[2]{{\bf #1}\index{keywords}{#2@#1}}
\newcommand{\bfidx}[1]{{\bf #1}}
\newcommand{\keywordDefSpecial}[2]{{\bf #1}\index{keywords}{#2@#1|bfidx}}




\newcommand{\keywordDef}[1]{{\emph{#1}}}

\newcommand\reffig[1]{Figure \ref{fig:#1}}
\newcommand\refsec[1]{Section \ref{sec:#1}}

\newcommand\commentnn[1]{\textcolor{blue}{NN: #1}}
\newcommand\commentjf[1]{\textcolor{green}{JF: #1}}
\newcommand\commentgf[1]{\textcolor{orange}{GF: #1}}
\newcommand\commentndf[1]{\textcolor{red}{NDF: #1}}
\newcommand\commentpk[1]{\textcolor{pink}{PK: #1}}
\newcommand\commentsw[1]{\textcolor{brown}{SW: #1}}

\newcommand{\theHalgorithm}{\arabic{algorithm}}
\newcommand{\xbest}{\mathbf{\vx}^{+}}
\newcommand{\xstar}{\mathbf{\vx}^{*}}
\newcommand{\ystar}{\mathbf{\vy}^{*}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\T}{^\intercal}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\thead}[1]{\mlticolumn{1}{c}{#1}}
\newcommand{\pluseq}{\mathrel{+}=}

\newcommand{\ex}[1]{{\mathbb E}\left[ #1 \right]}
\newcommand{\exc}[2]{{\mathbb E}\left[ #1 \,\middle \vert\, #2 \right]}
\newcommand{\exs}[2]{{\mathbb E_{#1}}\left[ #2 \right]}
\newcommand{\vars}[2]{{\mathbb V_{#1}}\left[ #2 \right]}
\newcommand{\excs}[3]{{\mathbb E_{#1}}\left[ #2 \,\middle \vert\, #3 \right]}

\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage[noend]{algpseudocode}
\usepackage{changepage}
\usepackage{bbm}

\newtheorem{lemma}{Lemma}



\providecommand{\citn}{{\color{red} \small \bf [cite]}}
\providecommand{\rev}[1]{{\color{blue}(revise) #1}}
\providecommand{\sw}[1]{{\color{red}SW: #1}}

\newcommand{\MLP}[1]{{\ensuremath{\text{MLP}(#1)}}}
\newcommand{\LSTM}[1]{{\ensuremath{\text{LSTM}(#1)}}}
\newcommand{\GRU}[1]{{\ensuremath{\text{GRU}(#1)}}}
\newcommand{\mysoftmax}[1]{{\ensuremath{\text{softmax}(#1)}}}

\makeatletter
\newcommand{\@BIBLABEL}{\@emptybiblabel}
\newcommand{\@emptybiblabel}[1]{}
\makeatother
\usepackage{hyperref}



\pdfinfo{
	/Title (Counterfactual Multi-Agent Policy Gradients)
	/Author (Jakob N. Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas 
	Nardelli, Shimon Whiteson)
	/Keywords (reinforcement learning, multi-agent, multi-agent systems, 
	actor-critic, policy gradient, deep learning)
}
\setcounter{secnumdepth}{1}

\begin{document} 
	
	\title{Counterfactual Multi-Agent Policy Gradients}
	\author{
		Jakob N. Foerster\\
		jakob.foerster@cs.ox.ac.uk\\
		\And
		Gregory Farquhar \\
		gregory.farquhar@cs.ox.ac.uk \\
		\AND
		Triantafyllos Afouras\\
		afourast@robots.ox.ac.uk\\
		\And
		Nantas Nardelli\\
		nantas@robots.ox.ac.uk\\
		\And
		Shimon Whiteson\\
		shimon.whiteson@cs.ox.ac.uk\\
		\AND
		\textnormal{University of Oxford, United Kingdom \quad 
		Equal contribution}
	}
	
	\maketitle

\begin{abstract}
\label{sec:abstract}
Many real-world problems, such as network packet routing  and the coordination of autonomous vehicles, are naturally modelled as cooperative multi-agent systems.  There is a great need for new reinforcement learning methods that can efficiently learn
decentralised policies for such systems.  To this end, we propose a new multi-agent actor-critic method called \emph{counterfactual multi-agent} (COMA) policy
gradients.  COMA uses a centralised critic to estimate the -function and
decentralised actors to optimise the agents' policies.  In addition, to address
the challenges of multi-agent credit assignment, it uses a \emph{counterfactual
baseline} that marginalises out a single agent's action, while keeping the other
agents' actions fixed. COMA also uses a critic representation that allows the
counterfactual baseline to be computed efficiently in a single forward pass. We
evaluate COMA in the testbed of \emph{StarCraft unit micromanagement}, using a
decentralised variant with significant partial observability. COMA significantly
improves average performance over other multi-agent actor-critic methods in this
setting,
and the best performing agents are competitive with  state-of-the-art centralised
controllers that get access to the full state.\end{abstract}

\section{Introduction}
\label{sec:intro}

Many complex \emph{reinforcement learning} (RL) problems such as the coordination of autonomous vehicles \citep{cao2013overview}, network packet delivery \citep{ye2015multi}, and distributed logistics \citep{ying2005multi} are naturally modelled as cooperative multi-agent systems.  However, RL methods designed for single agents typically fare poorly on such tasks, since the joint action space of the agents grows exponentially with the number of agents.  

To cope with such complexity, it is often necessary to resort to \emph{decentralised policies}, in which each agent selects its own action conditioned only on its local action-observation history.  Furthermore, partial observability and communication constraints during execution may necessitate the use of decentralised policies even when the joint action space is not prohibitively large.

Hence, there is a great need for new RL methods that can efficiently learn decentralised policies.  In some settings, the learning itself may also need to be decentralised.  However, in many cases, learning can take place in a simulator or a laboratory in which extra state information is available and agents can communicate freely.  This \emph{centralised training of decentralised policies} is a standard paradigm for multi-agent planning \citep{Oliehoek08JAIR,kraemer2016multi} and has recently been picked up by the deep RL community \citep{foerster2016learning,jorge2016learning}.  However, the question of how best to exploit the opportunity for centralised learning remains open.

Another crucial challenge is \emph{multi-agent credit assignment} 
\citep{chang2003all}: in cooperative settings, joint actions typically generate 
only global rewards, making it difficult for each agent to deduce its own 
contribution to the team's success.  Sometimes it is possible to design 
individual reward functions for each agent.  However, these rewards are not 
generally available in cooperative settings and often fail to encourage 
individual agents to sacrifice for the greater good. This often substantially 
impedes multi-agent learning in challenging tasks, even with relatively small 
numbers of agents.

In this paper, we propose a new multi-agent RL method called 
\emph{counterfactual multi-agent} (COMA) policy gradients, in order to address 
these issues.  COMA takes an \emph{actor-critic} \citep{konda2000actor} 
approach, in which the \emph{actor}, i.e., the policy, is trained by following 
a gradient estimated by a \emph{critic}.
COMA is based on three main ideas.  

First, COMA uses a centralised critic. The critic is only used during learning, 
while only the actor is needed during execution. Since learning is centralised, 
we can therefore use a centralised critic that conditions on the joint action 
and all available state information, while each agent's policy conditions only 
on its own action-observation history.

Second, COMA uses a \emph{counterfactual baseline}.  The idea is inspired by 
\emph{difference rewards} \citep{wolpert2002optimal,tumer2007distributed}, in 
which each agent 
learns from a shaped reward that compares the global reward to the reward 
received when that agent's action is replaced with a \emph{default action}.  
While difference rewards are a powerful way to perform multi-agent credit 
assignment, they require access to a simulator or estimated reward 
function, and in general it is unclear how to choose the default 
action. COMA addresses this by using the centralised critic to compute an agent-specific
\emph{advantage function} that compares the estimated return for the 
current joint action  to a counterfactual baseline that marginalises out a 
single 
agent's action, while keeping the other agents' actions fixed. This is similar 
to calculating an \emph{aristocrat utility} \citep{wolpert2002optimal}, but 
avoids the problem of a recursive interdependence between the policy and utility function 
because the expected contribution of the counterfactual baseline to 
the policy gradient is zero.
Hence, instead of relying on extra simulations, approximations, or assumptions 
regarding appropriate default actions, COMA computes a separate 
baseline for each agent that relies on the centralised critic to reason about
counterfactuals in which only that agent's action changes.

Third, COMA uses a critic representation that allows the counterfactual baseline to be computed efficiently.  In a single forward pass, it computes the -values for all the different actions of a given agent, conditioned on the actions of all the other agents.   Because a single centralised critic is used for all agents, all -values for all agents can be computed in a single batched forward pass.

We evaluate COMA in the testbed of \emph{StarCraft unit 
micromanagement}\footnote{StarCraft and its expansion StarCraft: Brood War are 
trademarks of Blizzard Entertainment\texttrademark.}, which has recently 
emerged as a challenging RL benchmark task with high stochasticity, a large 
state-action space, and delayed rewards. Previous works  
\citep{usunier2016episodic,peng2017multiagent} have made use of a centralised 
control policy that conditions on the entire state and can use powerful 
macro-actions, using StarCraft's built-in planner, that combine movement and 
attack actions. To produce a meaningfully decentralised benchmark that proves 
challenging for scenarios with even relatively few agents, we propose a variant 
that massively reduces each agent's field-of-view and removes access to these 
macro-actions.

Our empirical results on this new benchmark show that COMA can significantly 
improve performance over other multi-agent actor-critic methods, as well as 
ablated versions of COMA itself. In addition, COMA's  best agents are 
competitive with state-of-the-art centralised controllers that are given 
access to full state information and macro-actions.

\section{Related Work}
\label{sec:related}

Although multi-agent RL has been applied in a variety of settings~\citep{busoniu2008comprehensive,yang2004multiagent}, it has often been restricted to tabular methods and simple environments.
One exception is recent work in deep multi-agent RL, which can scale to high 
dimensional input and action spaces. \citet{tampuu2015multiagent} use a 
combination of DQN with independent 
-learning~\citep{tan1993multi,MASfoundations09} to learn how to play 
two-player pong. More recently the same method has been used 
by~\citet{leibo2017multi} to study the emergence of collaboration and defection 
in sequential social dilemmas.

Also related is work on the emergence of communication between agents, learned 
by gradient descent 
\citep{das2017learning,mordatch2017emergence,lazaridou2016multi,foerster2016learning,sukhbaatar2016learning}.
 In this line of work, passing gradients between agents during training and 
sharing parameters are two common ways to take advantage of centralised 
training. However, these methods do not allow for extra state information to be 
used during learning and do not address the multi-agent credit assignment 
problem.

\citet{guptacooperative} investigate actor-critic methods for decentralised execution with centralised training. However, in their methods both the actors and the critic condition on local, per-agent, observations and actions, and multi-agent credit assignment is addressed only with hand-crafted local rewards.

Most previous applications of RL to StarCraft micromanagement use a centralised 
controller, with access to the full state, and control of all units, although 
the 
architecture of the controllers exploits the multi-agent nature of the problem.
\citet{usunier2016episodic} use a \emph{greedy MDP}, which at each timestep
sequentially chooses actions for agents given all previous actions, in
combination with zero-order optimisation, while \citet{peng2017multiagent} use
an actor-critic method that relies on RNNs to exchange information between the
agents.

The closest to our problem setting is that of
\citet{foerster2017stabilising}, who also use a multi-agent representation and
decentralised policies. However, they focus on stabilising experience replay
while using DQN and do not make full use of the centralised training regime. As
they do not report on absolute win-rates we do not compare performance directly.
However, \citet{usunier2016episodic} address similar scenarios to our
experiments and implement a DQN baseline in a fully observable setting. In Section \ref{sec:results} we
therefore report our competitive performance against these state-of-the-art
baselines, while maintaining decentralised
control. \citet{omidshafiei2017deep} also address the stability of experience
replay in multi-agent settings, but assume a fully decentralised training
regime.

\cite{lowe2017multi} concurrently propose a multi-agent policy-gradient 
algorithm using centralised critics. Their approach does not address  
multi-agent credit assignment. Unlike our work, it learns a separate 
centralised critic for each agent and is applied to competitive 
environments with continuous action spaces. 

Our work builds directly off of the idea of \emph{difference rewards} 
\citep{wolpert2002optimal}.  The relationship of COMA to 
this line of work is discussed in Section \ref{sec:methods}.

\section{Background}
\label{sec:background}

We consider a fully cooperative multi-agent task that can be described as a 
stochastic game , defined by a tuple , in which  agents identified by  choose sequential actions. The environment has a true state . At each time step, each agent simultaneously chooses an action , 
forming a joint action  which induces a 
transition in the environment according to the state transition function 
. The 
agents all share the same reward function  and  is a discount factor.

We consider a partially observable setting, in which agents draw observations  according to the observation function . Each agent has an action-observation history , on which it conditions a stochastic policy . We denote joint quantities over agents in bold, and joint quantities over agents other than a given agent  with the superscript .

The discounted return is . The agents' joint policy induces a value function, i.e., an expectation over , , and an action-value function . The advantage function is given by .

Following previous work 
\citep{Oliehoek08JAIR,kraemer2016multi,foerster2016learning,jorge2016learning}, 
our problem setting allows centralised training but requires 
decentralised execution. This is a natural paradigm for a large set of 
multi-agent problems where training is carried out using a simulator with 
additional state information, but the agents must rely on local 
action-observation histories during execution. To condition on this full 
history, a deep RL agent may make use of a recurrent neural network 
\citep{hausknecht2015deep}, typically with a gated model such as LSTM 
\citep{hochreiter1997long} or GRU \citep{cho2014properties}.

In Section \ref{sec:methods}, we develop a new multi-agent policy gradient 
method for tackling this setting.  In the remainder of this section, we provide 
some background on single-agent policy gradient methods 
\citep{sutton1999policy}. Such methods optimise a single agent's policy, 
parameterised by , by performing gradient ascent on an estimate of 
the expected discounted total reward . Perhaps the simplest 
form of policy gradient is REINFORCE \citep{williams1992simple}, in which the 
gradient is:

In \emph{actor-critic} approaches 
\citep{sutton1999policy,konda2000actor,DBLP:journals/corr/SchulmanMLJA15},
 the \emph{actor}, i.e., the policy, is trained by following a gradient that 
depends on a \emph{critic}, which usually estimates a value function. In 
particular,  is replaced by any expression equivalent to , where   is a baseline designed to reduce variance 
\citep{weaver2001optimal}.  A common choice is , in which case 
 is replaced by .  Another option is to replace  with 
the \emph{temporal difference} (TD) error , 
which is an unbiased estimate of . In practice, the gradient must 
be estimated from trajectories sampled from the environment, and the 
(action-)value functions must be estimated with function approximators. 
Consequently, the bias and variance of the gradient estimate depends strongly 
on the exact choice of estimator \citep{konda2000actor}. 

In this paper, we  train critics  on-policy to estimate 
either  or , using a variant of TD() \citep{sutton1988learning} 
adapted for use with deep neural networks. TD() uses a mixture of 
-step returns . In particular, the critic parameters  
are updated by minibatch gradient descent to minimise the following loss:

where , 
and the -step returns  are calculated with bootstrapped values 
estimated by a \emph{target network} \citep{mnih2015human} with parameters 
copied periodically from .

\section{Methods}
\label{sec:methods}

In this section, we describe approaches for extending policy gradients to our multi-agent setting.

\subsection{Independent Actor-Critic}
The simplest way to apply policy gradients to multiple agents is to have each agent learn independently, with its own actor and critic, from its own action-observation history.  This is essentially the idea behind \emph{independent Q-learning} \citep{tan1993multi}, which is perhaps the most popular multi-agent learning algorithm, but with actor-critic in place of -learning.  Hence, we call this approach \emph{independent actor-critic} (IAC).  

In our implementation of IAC, we speed learning by sharing parameters among the 
agents, i.e., we learn only one actor and one critic, which are used by all 
agents. The agents can still behave differently because they receive different 
observations, including an agent-specific ID, and thus evolve different hidden 
states.  Learning remains independent in the sense that each agent's critic 
estimates only a local value function, i.e., one that conditions on , not 
. Though we are not aware of previous applications of this specific 
algorithm, we do not consider it a significant contribution but instead merely 
a baseline algorithm.

We consider two variants of IAC. In the first, each agent's critic estimates  and follows a gradient based on the TD error, as described in Section \ref{sec:background}.  In the second, each agent's critic estimates  and follows a gradient based on the advantage: , where . Independent learning is straightforward, but the lack of information sharing at training time makes it difficult to learn coordinated strategies that depend on interactions between multiple agents, or for an individual agent to estimate the contribution of its actions to the team's reward. 

\subsection{Counterfactual Multi-Agent Policy Gradients}
The difficulties discussed above arise because, beyond parameter sharing, IAC fails to exploit the fact that learning is centralised in our setting.  In this section, we propose \emph{counterfactual multi-agent} (COMA) policy gradients, which overcome this limitation.  Three main ideas underly COMA: 1) centralisation of the critic, 2) use of a counterfactual baseline, and 3) use of a critic representation that allows efficient evaluation of the baseline.  The remainder of this section describes these ideas.

First, COMA uses a centralised critic.  Note that in IAC, each actor   and each critic  or  conditions only on the agent's own action-observation history .  However, the critic is used only during learning and only the actor is needed during execution.  Since learning is centralised, we can therefore use a centralised critic that conditions on the true global state , if it is available, or the joint action-observation histories  otherwise. Each actor conditions on its own action-observation histories , with parameter sharing, as in IAC.  Figure \ref{fig:fig_1}a illustrates this setup.

\begin{figure*}[ht]
\centering
	\includegraphics[width=0.9\linewidth]{shared/joined_coma}
\caption{In (a), information flow between the decentralised actors, the environment and the centralised critic in COMA; red arrows and components are only required during centralised learning. In (b) and (c), architectures of the actor and critic. 
}
\label{fig:fig_1}
\end{figure*}

A naive way to use this centralised critic would be for each actor to follow a 
gradient based on the TD error estimated from this critic:



 However, such an approach fails to address a key credit assignment problem.  
 Because the TD error considers only global rewards, the gradient computed for 
 each actor does not explicitly reason about how that particular agent's 
 actions contribute to that global reward.  Since the other agents may be 
 exploring, the gradient for that agent becomes very noisy, particularly when 
 there are many agents.

Therefore, COMA uses a \emph{counterfactual baseline}.  The idea is inspired by 
\emph{difference rewards} \citep{wolpert2002optimal}, in which each agent 
learns from a shaped reward  that 
compares the global reward to the reward received when the action of agent  
is replaced with a \emph{default action} . Any action by agent  that 
improves  also improves the true global reward , because  does not depend on agent 's actions.

Difference rewards are a powerful way to perform multi-agent credit 
assignment.  However, they typically require access to a simulator in order to 
estimate .  When a simulator is already being used for 
learning, difference rewards increase the number of simulations that must be 
conducted, since each agent's difference reward requires a separate 
counterfactual simulation. \citet{proper2012modeling} and 
\citet{colby2015approximating} 
propose estimating difference rewards using function approximation rather than a 
simulator. However, this still requires a 
user-specified default action  that can be difficult to choose in many 
applications. In an actor-critic architecture, this approach would also
introduce an additional source of approximation error.

A key insight underlying COMA is that a centralised critic can be used to 
implement difference rewards in a way that avoids these problems.  COMA learns 
a centralised critic,  that estimates -values for the joint 
action  conditioned on the central state . For each agent  we can 
then compute an advantage function that compares the -value for the current 
action  to a counterfactual baseline that marginalises out , while 
keeping the other agents' actions  fixed:

Hence,  computes a separate baseline for each agent that uses the 
centralised critic to reason about counterfactuals in which only 's action 
changes, learned directly from agents' experiences instead of relying on extra 
simulations, a reward model, or a user-designed default action.

This advantage has the same form as the \emph{aristocrat utility} 
\citep{wolpert2002optimal}. However, optimising for an aristocrat utility using 
value-based methods creates a self-consistency problem because the policy and utility 
function depend recursively on each other. As a result, prior work focused
on difference evaluations using default states and actions. COMA is different because the
 counterfactual baseline's expected contribution to the gradient, as with other policy gradient baselines, is zero.  Thus, while
the baseline does depend on the policy, its expectation does not. Consequently, COMA can use 
this form of the advantage without creating a self-consistency problem.

While COMA's advantage function replaces potential extra simulations with 
evaluations of the critic, those evaluations may themselves be expensive if the 
critic is a deep neural network.  Furthermore, in a typical representation, the 
number of output nodes of such a network would equal , the 
size of the joint action space, making it impractical to train.  To address 
both these issues, COMA uses a critic representation that allows for efficient 
evaluation of the baseline. In particular, the actions of the other agents, 
, are part of the input to the network, which outputs a -value 
for each of agent 's actions, as shown in Figure~\ref{fig:fig_1}c. 
Consequently, the counterfactual advantage can be calculated efficiently by a 
single forward pass of the actor and critic, for each agent. Furthermore, the 
number of outputs is only  instead of (). 
While the network has a large input space that scales linearly in the number of 
agents and actions, deep neural networks can generalise well across such spaces.

In this paper, we focus on settings with discrete actions. However, COMA can be 
easily extended to continuous actions spaces by estimating the expectation in  
\eqref{eqn:advantage} with Monte Carlo samples or using functional forms that 
render it analytical, e.g., Gaussian policies and critic.

The following lemma establishes the convergence of COMA to a locally optimal policy.  The proof follows directly 
from the convergence of single-agent actor-critic algorithms 
\citep{sutton1999policy,konda2000actor}, and is subject to the same assumptions.

\begin{lemma}
For an actor-critic algorithm with a compatible TD(1) critic following a COMA 
policy gradient

at each iteration ,

\end{lemma}
\begin{proof}
The COMA gradient is given by

where  are the parameters of all actor policies, e.g. , and  is the counterfactual 
baseline defined in equation \ref{eqn:advantage}.

First consider the expected contribution of the this baseline :

where the expectation  is with respect to the state-action 
distribution 
induced by the joint policy . Now let  be the discounted 
ergodic state distribution as defined by \citet{sutton1999policy}:

Clearly, the per-agent baseline, although it reduces variance, does not change 
the expected gradient, and 
therefore does not affect the convergence of COMA. 

The remainder of the expected policy gradient is given by:

Writing the joint policy as a product of the independent actors:

yields the standard single-agent actor-critic policy gradient:


\citet{konda2000actor} prove that an actor-critic 
following this gradient converges to a local maximum of the expected return 
, given that:
\begin{enumerate}
	\item the policy  is differentiable,
	\item the update timescales for  and  are sufficiently slow, and 
	that 
	 is updated sufficiently slower than , and
	\item  uses a representation compatible with ,
\end{enumerate}
amongst several further assumptions. The parameterisation of the 
policy (i.e., the single-agent joint-action learner is decomposed into 
independent actors) is immaterial to convergence, as long as it remains 
differentiable. Note however that COMA's centralised critic is essential for 
this proof to hold.
\end{proof}

\section{Experimental Setup}
\label{sec:setting}

In this section, we describe the StarCraft problem to which we apply COMA, as
well as details of the state features, network architectures, training regimes,
and ablations.

\textbf{Decentralised StarCraft Micromanagement.} StarCraft is a rich
environment with stochastic dynamics that cannot be easily emulated. Many
simpler multi-agent settings, such as Predator-Prey \citep{tan1993multi} or
Packet World \citep{weyns2005packet}, by contrast, have full simulators with
controlled randomness that can be freely set to any state in order to perfectly
replay experiences. This makes it possible, though computationally expensive, to
compute difference rewards via extra simulations. In StarCraft, as in the real
world, this is not possible.

In this paper, we focus on the problem of \emph{micromanagement} in StarCraft,
which refers to the low-level control of individual units' positioning and
attack commands as they fight enemies. This task is naturally represented as a
multi-agent system, where each StarCraft unit is replaced by a decentralised
controller. We consider several scenarios with symmetric teams formed of: 3
marines (3m), 5 marines (5m), 5 wraiths (5w), or 2 dragoons with 3 zealots 
(2d\_3z). The enemy team is controlled by the StarCraft AI, which uses 
reasonable but suboptimal hand-crafted heuristics.

We allow the agents to choose from a set of discrete actions:
\texttt{move[direction]}, \texttt{attack[enemy\_id]}, \texttt{stop}, and
\texttt{noop}. In the StarCraft game, when a unit selects an attack action, it
first moves into attack range before firing, using the game's built-in
pathfinding to choose a route. These powerful \emph{attack-move} macro-actions
make the control problem considerably easier.


To create a more challenging benchmark that is meaningfully decentralised, we
impose a restricted field of view on the agents, equal to the firing range of
ranged units' weapons, shown in Figure~\ref{fig:setup}. This departure from the standard setup for
centralised StarCraft control has three effects.

\begin{figure}[h!]
    \begin{center}
        \includegraphics[width=1\linewidth]{shared/unites_start.pdf}
    \end{center}
    \caption{Starting position with example local field of view for the 2d\_3z map.}
    \label{fig:setup}
\end{figure}

First, it introduces significant partial observability. Second, it means units
can only attack when they are in range of enemies, removing access to the
StarCraft macro-actions. Third, agents cannot distinguish between enemies who
are dead and those who are out of range and so can issue invalid attack commands
at such enemies, which results in no action being taken. This substantially
increases the average size of the  action space, which in turn increases the
difficulty of both exploration and control.

Under these difficult conditions, scenarios with even relatively small numbers
of units become much harder to solve. As seen in Table \ref{tbl:test_in_domain},
we compare against a simple hand-coded heuristic that instructs the agents to
run forwards into range and then focus their fire, attacking each enemy in turn
until it dies. This heuristic achieves a 98\% win rate on 5m with a full field
of view, but only 66\% in our setting. To perform well in this task, the agents
must learn to cooperate by positioning properly and focussing their fire,
while remembering which enemy and ally units are alive or out of
view.

All agents receive the same global reward at each time step, equal to the sum of
damage inflicted on the opponent units minus half the damage taken. Killing an
opponent generates a reward of 10 points, and winning the game generates a
reward equal to the team's remaining total health plus 200. This damage-based
reward signal is comparable to that used by \citet{usunier2016episodic}. Unlike 
\cite{peng2017multiagent}, our approach does not require estimating local 
rewards.

 \begin{figure*}[t!]
 	\centering
 	\begin{subfigure}[b]{0.4\linewidth}
 		\includegraphics[width=\textwidth]{shared/3m_ef}
 		\caption{3m}
 	\end{subfigure}
  	\begin{subfigure}[b]{0.4\linewidth}
	\includegraphics[width=\textwidth]{shared/5m_ef}
 	\caption{5m}
 	\end{subfigure}
 	\begin{subfigure}[b]{0.4\linewidth}
	\includegraphics[width=\textwidth]{shared/5w_ef}
	\caption{5w}
	\end{subfigure}
 	\begin{subfigure}[b]{0.4\linewidth}
\includegraphics[width=\textwidth]{shared/2d_3z_ef}
	\caption{2d\_3z}

	\end{subfigure}
 	\caption{Win rates for COMA and competing algorithms on four different 
 		scenarios. COMA outperforms all baseline methods. Centralised critics 
 		also
 		clearly outperform their decentralised counterparts. The legend at the 
 		top applies across all plots.}
 	\label{fig:fig_2}
 \end{figure*}

\textbf{State Features.}
\label{ssec:features}
The actor and critic receive different input features, corresponding to local
observations and global state, respectively. Both include features for allies
and enemies. \emph{Units} can be either allies or enemies, while \emph{agents} are
the decentralised controllers that command ally units.

The local observations for every agent are drawn only from a circular subset of
the map centred on the unit it controls and include for each unit within this 
field of view:
\texttt{distance}, \texttt{relative x}, \texttt{relative
y}, \texttt{unit type} and \texttt{shield}.\footnote{After firing, a unit's 
\texttt{cooldown} is reset, and it must drop
before firing again. Shields absorb damage until they break, after which units
start losing health. Dragoons and zealots have shields but marines do not.}
All features are normalised by their maximum values. We do not include any 
information about the units' current target.

The global state representation consists of similar features, but for
all units on the map regardless of fields of view. Absolute distance is not 
included, and - locations are given relative to the centre of the map 
rather than to a particular agent. The global state also includes 
\texttt{health points} and \texttt{cooldown} for all agents. The representation 
fed
to the centralised -function critic is the concatenation of the global state 
representation with the 
local observation of the agent whose actions are being evaluated. Our 
centralised critic that estimates , 
and is therefore agent-agnostic, receives the global state concatenated with 
all agents' observations. The observations contain no new information but 
include the egocentric distances relative to that agent.

\textbf{Architecture \& Training.}
\label{ssec:architecture} 
The actor consists of 128-bit \emph{gated recurrent units} 
(GRUs)~\citep{cho2014properties} that use fully connected layers both to 
process the input and to produce the output values from the hidden state, 
. The IAC critics use extra output heads appended to the last layer of 
the actor network. Action probabilities are produced from the final layer, 
, via a bounded softmax distribution that lower-bounds the probability of 
any given action by :  . We anneal  linearly from  to  across 
 training episodes. The centralised critic is a feedforward network with 
multiple ReLU layers combined with fully connected layers.  Hyperparameters 
were coarsely tuned on the 5m scenario and then used for all other 
maps. We found that the most sensitive parameter was TD(), but settled 
on , which worked best for both COMA 
and our baselines. Our implementation uses 
TorchCraft~\citep{synnaeve2016torchcraft} and \mbox{Torch 7}~\citep{torch}. Pseudocode and
further details on the training procedure are in the 
appendix.

We experimented with critic architectures that are factored at the agent level and 
further exploit internal parameter sharing. However, we found that the 
bottleneck for scalability was not the centralisation of the critic, but rather 
the difficulty of multi-agent exploration.  Hence, we defer further 
investigation of factored COMA critics to future work.

\textbf{Ablations.}
We perform ablation experiments to validate three key elements of COMA. First, 
we test the importance of centralising the critic by comparing 
against two IAC variants, IAC- and IAC-. These critics take the same 
decentralised input as the actor, and share parameters with the actor network 
up to the final layer. IAC- then outputs  -values, one for 
each action, while IAC- outputs a single state-value. Note that we still 
share parameters between agents, using the egocentric observations and ID's as 
part of the input to allow different behaviours to emerge. The cooperative 
reward function is still shared by all agents.

Second, we test the significance of learning  instead of . The method 
\mbox{\emph{central-}} still uses a central state for the critic, but learns 
, and uses the TD error to estimate the advantage for policy gradient 
updates.

Third, we test the utility of our counterfactual baseline. The method 
\mbox{\emph{central-}} learns both  and  simultaneously and estimates 
the 
advantage as , replacing COMA's counterfactual baseline with .
All methods use the same architecture and training scheme for the actors, and 
all critics are trained with TD().

\begin{table*}[h!]
	\begin{center}
		\resizebox{\textwidth}{!}{
			\begin{tabular}{l ccccccc ccc}
				\toprule
				& \multicolumn{7}{c}{Local Field of View (FoV)} & 
				\multicolumn{3}{c}{Full FoV, Central Control} \\
				\cmidrule(lr){2-8} \cmidrule(lr){9-11}
				
				\multirow{2}{*}{map}  & \multirow{2}{*}{heur.}    & 
				\multirow{2}{*}{IAC-}  & \multirow{2}{*}{IAC-}  & 
				\multirow{2}{*}{cnt-} & \multirow{2}{*}{cnt-}  & 
				\multicolumn{2}{c}{COMA}  & \multirow{2}{*}{heur.} & 
				\multirow{2}{*}{DQN} & \multirow{2}{*}{GMEZO}  \\
				&   &  &    &   &  & mean & best  & & & \\
				
				
				\midrule
				3m        & 35   & 47 (3)   & 56 (6)    & 83 (3)   & 83 (5)   & 
				\textbf{87} (3)  &  98  &  74  &   - & -          \\
				5m        & 66   & 63 (2)   & 58 (3)    & 67 (5)   & 71 (9)   & 
				\textbf{81} (5)   &  95  &  98  &  99 & 100 \\
				5w        & 70   & 18 (5)   & 57 (5)     & 65 (3)   & 76 (1)   
				& 
				\textbf{82} (3)   &  98  &  82  &    70 & 
				74\footnotemark[3] \\
				2d\_3z    & \textbf{ 63}   & 27 (9)   & 19 (21)  & 36 (6)  & 
				39 (5)   
				&47  (5)  &  65  &  68  &    61 & 90 \\
				\bottomrule
				
			\end{tabular}
		}
		
	\end{center}
	\caption{Mean win percentage averaged across final 1000 evaluation episodes 
		for the different maps, for all methods and the hand-coded 
		heuristic in 
		the decentralised setting with a limited field of view. The 
		highest mean performances are in bold, while the values in 
		parentheses denote the 95\% confidence interval, for example 
		.
		Also shown, maximum win percentages for COMA (decentralised), in 
		comparison to the heuristic and published results (evaluated in the 
		centralised setting).}
	\label{tbl:test_in_domain}
	
\end{table*}


\section{Results}
\label{sec:results}

Figure \ref{fig:fig_2} shows average win rates as a function of episode
for each method and each StarCraft scenario. For each method, we conducted 35
independent trials and froze learning every 100 training episodes to evaluate
the learned policies across 200 episodes per method, plotting the average across
episodes and trials. Also shown is one standard deviation in performance.

The results show that COMA is superior to the IAC baselines in all scenarios. Interestingly, the IAC methods also eventually learn reasonable policies in
5m, although they need substantially more episodes to do so. This may seem
counterintuitive since in the IAC methods, the actor and critic networks share
parameters in their early layers (see Section
\ref{ssec:architecture}), which could be expected to speed learning. However, these 
results suggest that the improved accuracy of policy evaluation made 
possible by conditioning on the global state outweighs the overhead of training 
a separate network.

Furthermore, COMA strictly dominates \mbox{central-}, both in training 
speed and in final performance across all settings. This is a strong indicator 
that our counterfactual baseline is crucial when using a central -critic to 
train decentralised policies.

Learning a state-value function has the obvious advantage of not conditioning 
on the joint action. Still, we find that COMA outperforms the 
\mbox{central-} baseline in final performance. Furthermore, COMA typically achieves good 
policies faster, which is expected as COMA provides a shaped training signal. 
Training is also more stable  than \mbox{central-}, which is a consequence of the COMA gradient 
tending to zero as the policy becomes greedy. Overall, COMA is the best 
performing and most consistent method.

\citet{usunier2016episodic} report the performance of their best agents trained with their 
state-of-the-art centralised controller labelled GMEZO (greedy-MDP with 
episodic zero-order optimisation), and for a centralised DQN controller, both 
given a full field of view and access to attack-move macro-actions. These 
results are compared in Table~\ref{tbl:test_in_domain} against the best agents 
trained with COMA for each map. Clearly, in most settings these agents achieve 
performance comparable to the best published win rates despite being 
restricted to decentralised policies and local fields of view.

\footnotetext[3]{5w 
	DQN and GMEZO benchmark performances are of a policy 
	trained on a larger 
	map and tested on 5w}

\section{Conclusions \& Future Work}
\label{sec:conclusion}

This paper presented COMA policy gradients, a method that uses a centralised 
critic in order to estimate a counterfactual advantage for decentralised 
policies in mutli-agent RL. COMA addresses the challenges of multi-agent credit 
assignment by using a counterfactual baseline that marginalises out a single 
agent's action, while keeping the other agents' actions fixed. Our results in a 
decentralised \emph{StarCraft unit micromanagement} benchmark show that COMA 
significantly improves final performance and training speed over other 
multi-agent actor-critic methods and remains competitive with state-of-the-art 
centralised controllers under best-performance reporting. Future work will 
extend COMA to tackle scenarios with large numbers of agents, where centralised 
critics are more difficult to train and exploration is harder to coordinate. We 
also aim to develop more  sample-efficient variants that are practical for 
real-world applications such as self-driving cars.

\section*{Acknowledgements} 
This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement number 637713).  It was also supported by the Oxford-Google DeepMind Graduate Scholarship, the 
UK EPSRC CDT in Autonomous Intelligent Machines and Systems, and a generous 
grant from Microsoft for their Azure cloud computing services.
We would like to thank Nando de Freitas, Yannis Assael, and Brendan Shillingford
for helpful comments and discussion. We also thank Gabriel Synnaeve, Zeming Lin,
and the rest of the TorchCraft team at FAIR for their work on the interface.


\bibliography{starcomm,mrl}
\bibliographystyle{include/aaai}

\newpage
\onecolumn
\section*{Appendix}
\subsection*{Training Details and Hyperparameters}
Training is performed in batch mode, with a batch size of 30. Due to parameter 
sharing,  all agents can be processed in parallel, with each agent for each 
episode and time step occupying one batch entry. The training cycle progresses 
in three steps (completion of all three steps constitutes as one episode in our 
graphs):
1) \emph{collect data}: collect  episodes;
2) \emph{train critic}: for each time step, apply a gradient step to the 
feed-forward critic, starting at the end of the episode; and
3) \emph{train actor}: fully unroll the recurrent part of the actor, aggregate 
gradients in the backward pass across all time steps, and apply a gradient 
update. 

We use a target network for the critic, which updates every  training 
steps for the feed-forward centralised critics and every  steps for the 
recurrent IAC critics. The feed-forward critic receives more learning steps, 
since it performs a parameter update for each timestep. Both the actor and the 
critic networks are trained using RMS-prop with learning rate  and 
alpha , without weight decay.  We set gamma to  for all maps.

Although tuning the skip-frame in StarCraft can improve absolute 
performance \citep{peng2017multiagent}, we use a default value of 7, since the 
main focus is a relative evaluation between COMA and the baselines.


\subsection*{Algorithm}
\begin{algorithm}
	\caption{Counterfactual Multi-Agent (COMA) Policy Gradients}
	\label{alg:dic}
	\hskip -2em
	\begin{algorithmic} \State Initialise , 
		\For{each training episode }
		\State Empty buffer
		\For{ {\bfseries to} }
		\State  initial state, ,  for each 
		agent 
		\While{ terminal {\bfseries and} }
		\State 
		\For{each agent }
		\State  
		\State Sample  from  
		\EndFor
		\State Get reward  and next state     \EndWhile
		\State  Add episode to buffer
		\EndFor
		\State Collate episodes in buffer into single batch
		\For{ {\bfseries to} }  // from now processing all agents in 
		parallel via single batch
		\State Batch unroll RNN using states, actions and rewards
		\State Calculate TD() targets  using  
		
		\EndFor
		\For{ {\bfseries down to} } 
		\State 
		\State   // calculate critic gradient
		\State   // 
		update critic weights
		\State Every C steps  reset 
		
		\EndFor
		\For{ {\bfseries down to} }
		\State  // calculate COMA
		\State   // 
		accumulate actor gradients
		\EndFor
		\State	 // update actor weights
		\EndFor
	\end{algorithmic}
\end{algorithm}


\end{document} 
