\pdfoutput=1


\documentclass[11pt]{article}

\usepackage[final]{EACL2023}

\usepackage{times}
\usepackage{latexsym}
\usepackage{booktabs}
\usepackage[T1]{fontenc}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{subcaption}


\usepackage[utf8]{inputenc}

\usepackage{graphicx}
\usepackage{inconsolata}
\usepackage{amsmath}
\usepackage{tablefootnote}
\usepackage{gb4e}
\noautomath
\newcommand{\xmark}{\ding{55}}
\newcommand\decreasespace{\vspace{-5pt}}





\title{Can BERT eat RuCoLA? Topological Data Analysis to Explain}

\author{
    ~\textbf{Irina Proskurina\textsuperscript{1}}, 
    ~\textbf{Irina Piontkovskaya\textsuperscript{2}},
    ~\textbf{Ekaterina Artemova\textsuperscript{3}} \\
    \textsuperscript{1}Universit{\'e} de Lyon, Lyon 2, ERIC UR 3083, France\\ 
    \textsuperscript{2}Huawei Noahâ€™s Ark lab \\
    \textsuperscript{3}Center for Information and Language Processing (CIS), LMU Munich, Germany \\
    \textbf{Correspondence:} \href{mailto:Irina.Proskurina@univ-lyon2.fr}{Irina.Proskurina@univ-lyon2.fr}
}


\begin{document}
\maketitle
\begin{abstract}
This paper investigates how Transformer language models (LMs) fine-tuned for acceptability classification capture linguistic features. Our approach uses the best practices of topological data analysis (TDA) in NLP: we construct directed attention graphs from attention matrices, derive topological features from them, and feed them to linear classifiers. We introduce two novel features, chordality, and the matching number, and show that TDA-based classifiers outperform fine-tuning baselines. We experiment with two datasets, \textsc{CoLA} and \textsc{RuCoLA},\footnote{Arugula or rocket salad in English} in English and Russian, typologically different languages. 

On top of that, we propose several black-box introspection techniques aimed at detecting changes in the attention mode of the LMs during fine-tuning, defining the LM's prediction confidences, and associating individual heads with fine-grained grammar phenomena. 

Our results contribute to understanding the behavior of monolingual LMs in the acceptability classification task, provide insights into the functional roles of attention heads, and highlight the advantages of TDA-based approaches for analyzing LMs.
We release the code and the experimental results for further uptake.\footnote{\href{https://github.com/upunaprosk/la-tda}{https://github.com/upunaprosk/la-tda}}


\end{abstract}
\section{Introduction}
Language modelling with Transformer \cite{vaswani2017attention} has become a standard approach to acceptability judgements, providing results on par with the human baseline \cite{warstadt-etal-2019-neural}. 
The pre-trained encoders and BERT, in particular, were proven to have an advantage over other models, especially when judging the acceptability of sentences with long-distance dependencies \cite{Warstadt2019LinguisticAO}.  
Research examining linguistic knowledge of BERT-based language models (LMs) revealed that: (1) individual attention heads can store syntax, semantics or both kinds of linguistic information \cite{jo-myaeng-2020-roles,clark-etal-2019-bert}, (2) vertical, diagonal and block attention patterns could frequently repeat across the layers \cite{kovaleva-etal-2019-revealing}, and (3) fine-tuning affects the linguistic features encoding tending to lose some of the pre-trained model knowledge~\cite{miaschi-etal-2020-linguistic}. 
However, less attention has been paid to examining the grammatical knowledge of LMs in languages other than English. 
The existing work devoted to the cross-lingual probing showed that grammatical knowledge of Transformer LMs is adapted to the downstream language; in the case of Russian, the interpretation of results cannot be easily explained \cite{ravishankar-etal-2019-multilingual}. However, LMs are more insensitive towards granular perturbations when processing texts in languages with free word order, such as Russian \cite{taktasheva-etal-2021-shaking}.

In this paper, we probe the linguistic features captured by the Transformer LMs, fine-tuned for acceptability classification in Russian. 
Following recent advances in acceptability classification, we use the Russian corpus of linguistic acceptability (\textsc{RuCoLA})~\cite{mikhailov-etal-2022-rucola}, covering tense and word order violations, errors in the construction of subordinate clauses and indefinite pronoun usage, and other related grammatical phenomena.  
We provide an example of an unacceptable sentence from \textsc{RuCoLA} with a morphological violation in the pronoun usage: a possessive reflexive pronoun `svoj' (oneself's/own) instead of the 3rd person pronoun.
\begin{exe}
\ex \label{ex:intro_ex} * Eto byl pervyj chempionat mira v \textbf{svoej} kar'ere.
(``It was the first world championship in \textbf{own} career.'')
\end{exe}
Following the recently proposed Topological Data Analysis (TDA) based approach to the linguistic acceptability (LA) task \cite{cherniavskii-etal-2022-acceptability}, we construct directed attention graphs from attention matrices and then refer to the characteristics of the graphs as to the linguistic features learnt by the model. 
We extend the existing research on the acceptability classification task to the Russian language and show the advantages of the TDA-based approach to the task. 
Our main contributions are the following:~\emph{(i)}~we investigate the monolingual behaviour of LMs in acceptability classification tasks in the Russian and English languages, using a TDA-based approach, 
~\emph{(ii)}~we introduce new topological features and outperform previously established baselines,
~\emph{(iii)}~we suggest a new TDA-based approach for measuring the distance between pre-trained and fine-tuned LMs with large and base configurations. \emph{(iv)}~We determine the roles of attention heads in the context of LA tasks in Russian and English. 

Our initial hypothesis is that there is a difference in the structure of attention graphs between the languages, especially for the sentences with morphological, syntactic, and semantic violations. 
We analyze the relationship between models by comparing the features of the attention graphs.
To the best of our knowledge, our research is one of the first attempts to analyse the differences in monolingual LMs fine-tuned on acceptability classification corpora in Russian and English, using the TDA-based approach.







\section{Related Work}
\paragraph{Acceptability Classification.} First studies performed acceptability classification with statistical machine learning methods, rule-based systems, and context-free grammars~\cite{cherry-quirk-2008-discriminative,wagner2009judging,post-2011-judging}. Alternative approaches use threshold scoring functions to estimate the likelihood of a sentence \cite{lau-etal-2020-furiously}. Recent research has been centered on the ability of omnipresent Transformer LMs to judge acceptability \cite{wang-etal-2018-glue}, to probe for their grammar acquisition \cite{zhang-etal-2021-need}, and evaluate semantic correctness in language generation \cite{batra-etal-2021-building}. 
In this project, we develop acceptability classification methods and apply them to datasets in two different languages, English and Russian.


\paragraph{Topological Data Analysis (TDA) in NLP.}  Recent work uses TDA to explore the inner workings of LMs.  \citet{kushnareva-etal-2021-artificial} derive TDA features from attention maps to build artificial text detection. \citet{colombo-etal-2021-automatic} introduce \textsc{BaryScore}, an automatic evaluation metric for text generation that relies on Wasserstein distance and barycenters. \citet{chauhan2022bertops} develop a scoring function which captures the homology of the high-dimensional hidden representations, and is aimed at test accuracy prediction. 
We extend the set of persistent features proposed by \citet{cherniavskii-etal-2022-acceptability} for acceptability classification and conduct an extensive analysis of how the persistent features contribute to the classifier's performance.


\paragraph{How do LMs change via fine-tuning?} There have been two streams of studies of how fine-tuning affects the inner working of LM's: (i) what do sub-word representation capture and (ii) what are the functional roles of attention heads? The experimental techniques include similarity analysis between the weights of source and fine-tuned checkpoints \cite{clark-etal-2019-bert}, training probing classifiers \cite{durrani-etal-2021-transfer}, computing feature importance scores \cite{atanasova-etal-2020-diagnostic}, the dimensionality reduction of sub-word representations \cite{alammar-2021-ecco}. Findings help to improve fine-tuning procedures by modifying loss functions  \cite{elazar-etal-2021-measuring} and provide techniques for explaining LMs' predictions \cite{danilevsky-etal-2020-survey}. Our approach reveals the linguistic competence of attention heads by associating head-specific persistent features with fine-grained linguistic phenomena. 

\section{Methodology}
We follow \citealp{warstadt-etal-2019-neural} and treat the LA task as a supervised classification problem.
We fine-tune Transformer LMs to approximate the function that maps an input sentence to a target class: acceptable or unacceptable. 
\subsection{Extracted Features}\label{sec:tda_method}
Given an input text, we extract output attention matrices from Transformer LMs and follow \citealp{kushnareva-etal-2021-artificial} to compute three types of persistent features over them.



\begin{figure*}[h!]
\centering
\begin{subfigure}{0.3\textwidth}
    \includegraphics[width=0.8\textwidth]{images/ex_heatmap.pdf}
    \caption{}
    \label{fig:first}
\end{subfigure}
\hfill
\begin{subfigure}{0.3\textwidth}
    \includegraphics[width=1.1\textwidth]{images/ex_bipartite.pdf}
    \caption{}
    \label{fig:second}
\end{subfigure}
\hfill
\begin{subfigure}{0.36\textwidth}
    \includegraphics[width=1.1\textwidth]{images/ex_graph.pdf}
    \caption{}
    \label{fig:third}
\end{subfigure}

\caption{An example of an attention map (a) and the corresponding bipartite (b) and attention (c) graphs for the \textsc{CoLA} sentence \textit{``John sang beautifully''}.  The graphs are constructed with a threshold equal to 0.1.}
\label{fig:ex_features}
\end{figure*}







\textbf{Topological} features are properties of attention graphs.
We provide an example of an attention graph constructed upon the attention matrix in \autoref{fig:ex_features}.
An adjacency matrix of attention graph  is obtained from the attention matrix , using a pre-defined threshold : 

where  is an attention weight between tokens  and  and  is the number of tokens in the input sequence. Each token corresponds to a graph node.
Features of directed attention graphs include the number of strongly connected components, edges, simple cycles and average vertex degree. 
The properties of undirected graphs include the first two Betti numbers: the number of connected components and the number of simple cycles. 
We propose two new features of the undirected attention graphs: the matching number and the chordality. 
The matching number is the maximum matching size in the graph, i.e. the largest possible set of edges with no common nodes.

Consider an attention matrix depicted in \autoref{fig:first} and a simple undirected attention graph (\autoref{fig:third}) constructed based on the bipartite graph (\autoref{fig:second}) with a threshold of 0.1.
The matching number of that attention graph is equal to two.
One example of a maximum matching in that graph is a set of edges: \{(\textit{John} - \textit{sang}), (\textsc{[SEP]} - \textsc{[CLS]})\}. 
That matching is maximum because there are no more edges that are not incident to the already matched 4 nodes (tokens).
The chordality is a binary feature showing whether the attention graph is chordal; that is, whether the attention graph does not contain induced cycles of a length greater than~3.
For example, the plotted graph in \autoref{fig:third} is chordal because it does not contain induced cycles with more than 3 edges. 
If there were no dotted edges (chords) in the graph, there would be a cycle \textsc{[SEP]}-\textit{beautifully}-\textit{sang}-\textsc{[CLS]}-\textsc{[SEP]} of length 4, meaning that the graph is not chordal. 

We expect these novel features to express syntax phenomena of the input text. 
The chordality feature could carry information about subject-verb-object triplets. The maximum matching can correspond to matching sentence segments (subordinate clauses, adverbials, participles, introductory phrases, etc.). 

\textbf{Features derived from barcodes} include descriptive characteristics of -dimensional barcodes and reflect the survival (death and birth) of connected components and edges throughout the filtration. 

\textbf{Distance-to-pattern} features measure the distance between attention matrices and identity matrices of pre-defined attention patterns, such as attention to the first token \textsc{[CLS]} and to the last \textsc{[SEP]} of the sequence, attention to previous and next token and to punctuation marks \cite{clark-etal-2019-bert}.
We use a publicly available implementation to compute features.\footnote {\href{https://github.com/danchern97/tda4atd}{https://github.com/danchern97/tda4atd}}

\subsection{Experimental Framework}
\paragraph{Data}
We use two publicly available LA benchmarks in two typologically different languages: Russian (\textsc{RuCoLA}; \citealp{mikhailov-etal-2022-rucola}) and English (\textsc{CoLA}; \citealp{warstadt-etal-2019-neural}).
Both selected corpora consist of in- and out-of-domain data and contain sentences collected from linguistics publications; each is marked as acceptable or unacceptable. 
Unacceptable sentences are annotated with syntactic, morphological and semantic phenomena violated in them.
\textsc{RuCoLA}, in addition, covers synthetically generated data by generative LMs.
We provide examples of acceptable sentences from observed corpora (\ref{ex:cola_acc}, \ref{ex:rucola_acc}) along with sentences with semantic violations (\ref{ex:cola_unacc}, \ref{ex:rucola_unacc}).
\begin{exe}
\ex \begin{xlist}
    \ex\label{ex:cola_acc} The dog bit the cat.
    \ex\label{ex:cola_unacc} * The \textbf{soundly and furry} cat slept.
    \end{xlist}
\ex \begin{xlist}
    \ex\label{ex:rucola_acc} Koshki byli svyashchennymi zhivotnymi v Drevnem Egipte. 
     (``Cats were sacred animals in ancient Egypt.'')
    \ex\label{ex:rucola_unacc} * \textbf{Bliz} kresla na nebol'shom kovrike lezhala koshka.
    (``\textbf{Outside of} an armchair on a small rug a cat was lying.'')
    \end{xlist}
\end{exe}
\decreasespace
\autoref{tab:corpora-summary} (\autoref{sec:app_data}) reports statistics of the used corpora.
For per-category evaluation, we use \textsc{RuCoLA} error annotations, and for \textsc{CoLA}, we use minor grammatical phenomena annotations to group erroneous sentences. 
We provide more details in \autoref{tab:en-cola-groups} (\autoref{sec:app_data}). 

\paragraph{Models}
Our baseline model architectures, fine-tuning and evaluation scripts are taken from the Transformers library \cite{wolf-etal-2020-transformers}. 
We use the following case-sensitive monolingual Transformer LMs for the experiments: 
(1) base size En-BERT\footnote{\href{https://huggingface.co/bert-base-cased}{hf.co/bert-base-cased}}~\cite{devlin-etal-2019-bert} and Ru-BERT,\footnote{\href{https://huggingface.co/sberbank-ai/ruBert-base}{hf.co/sberbank-ai/ruBert-base}} (2) large size En-RoBERTa\footnote{\href{https://huggingface.co/roberta-large}{hf.co/roberta-large}}~\cite{liu2019roberta} and Ru-RoBERTa.\footnote{\href{https://huggingface.co/sberbank-ai/ruRoberta-large}{hf.co/sberbank-ai/ruRoberta-Large}}
To estimate the effect of fine-tuning, we compare two types of models: pre-trained LMs with frozen weights (frozen) and fine-tuned LMs on the training sets.
Transformer LMs are fine-tuned for 5 epochs on in-domain training data, with a batch size of 32 and an optimal set of hyper-parameters determined by the authors of the datasets.
To mitigate class imbalance, we use weighted cross-entropy loss.
We provide fine-tuning details in \autoref{tab:bert_hyperparameters} (\autoref{sec:app_data}). 
\paragraph{TDA Classifiers}
We extract a range of persistent (TDA) features listed in Section~\ref{sec:tda_method} from Transformer LMs and refer to them as training features fed to a linear classifier.  
We reduce the feature space dimensionality with principal component analysis (PCA).
Next, we train Logistic Regression classifiers with adjusted class weights on the reduced feature space.
We iterate over a range of inverse regularization parameter values  and the number of principal components . 
We choose the value 200 as the upper bound of the PC grid to ensure that the number of latent features is at least two times less than the size of the  in-domain development (IDD) or out-of-domain development (OODD) sets. 
We tune hyper-parameters to maximize the classifier performance on the IDD set.
We compare the performance of two feature sets, by reporting results of classifiers trained on (i) basic TDA features by \citealp{kushnareva-etal-2021-artificial} (dubbed as TDA) and  (ii) TDA features with two novel features added (dubbed as TDA. 
\subsection{Evaluation}
\label{sec:method_evaluation}
\paragraph{Performance Metrics}
Following \citealp{warstadt-etal-2019-neural}, we measure performance with Accuracy (Acc.) and Matthews Correlation Coefficient (MCC).
MCC is used as the main performance metric for finding hyperparameters, evaluating trained models, and adjusting the decision threshold.
\paragraph{Fine-tuning Effect} 

We estimate changes in attention weights between pre-trained and fine-tuned LMs with two methods. 
First, we follow \citealp{hao-etal-2020-investigating} and employ Jensen-Shannon (JS) divergence:

where  and  are fine-tuned and frozen models respectively,  is number of sentences,
 is a number of attention heads ( for base-configuration LMs,   for large LMs),
 is the number of tokens in the sentence , and  is an attention weight of attention head  at token  in model .

Second, we estimate the difference between attention graphs as an average correlation distance between the TDA features across attention heads:

where  is the number of features,  are values of the  feature , computed over attention matrix , extracted from the model .
\section{Results}

\begin{table}[t!]
\scriptsize
\centering
\newcommand{\hsp}{\hspace{4pt}}
\setlength{\tabcolsep}{2pt}
\begin{tabular}{@{}lcccc@{}} 
\toprule
\multirow{3}{*}{\textbf{Model}}  &
\multicolumn{2}{c}{\textbf{Fine-tuned LMs}} & \multicolumn{2}{c}{\textbf{Frozen LMs}} \p_i \sim \text{exp}(X_{0i}^TC^Tw + c),
where  are the input TDA features,  is the principal component matrix,  is a vector of PCs coefficients in the decision function, and  is the added bias.   is the feature contribution to prediction.




\begin{figure*}
\subsection{The Roles of Attention Heads} \label{subsec:head}
\centering
\includegraphics[width=0.8\textwidth]{images/rucola_heads.pdf}\\
\includegraphics[width=0.8\textwidth]{images/rucola_heads_disagr.pdf}\\
\medskip
\includegraphics[width=0.8\textwidth]{images/cola_heads.pdf}
\includegraphics[width=0.8\textwidth]{images/cola_heads_disagr.pdf}
\caption{Mean feature weights in TDA classifiers with respect to the dataset. TDA are extracted from fine-tuned Ru-BERT and En-BERT, respectively. 
Features of an \textit{agreeing head} contribute to correct prediction. Features of an \textit{disagreeing head} contribute to incorrect prediction. Brighter colors stand for higher mean feature weights.}
\label{fig:attn_heads}


\centering
\includegraphics[width=0.8\textwidth]{images/rucola_maps.pdf}\\
\medskip
\caption{Correlation coefficients between average vertex degree features and target labels for frozen and fine-tuned Ru-BERT by the threshold used to construct attention graph.}
\label{fig:feature_thr}
\end{figure*}
\end{document}
