\section{Experiments}

In this section, we start by presenting the downstream datasets (Sec.~\ref{subsec:datasets})
and the experiment protocol (Sec.~\ref{subsec:protocol}). Next, we report state-of-the-art results across the chosen suite of long-form video retrieval benchmarks (Sec.~\ref{subsec:results}). Then we perform investigation into the effectiveness of the simple weighted-mean aggregation and compare to alternative methods (Sec.\label{subsec:ablation}).

\subsection{Downstream Datasets}
\label{subsec:datasets}
We now describe the downstream text-to-video retrieval datasets our model is evaluated on, focused on those with long durations, as well as additionally a long video classification dataset.
\\
\noindent\textbf{MSR-VTT~\cite{xu2016msr}} benchmark contains 10K videos from YouTube with 5 captions per video, we trained on 9K videos and report results on the 1K-A~\cite{yu2018joint} test set, this dataset contains the shortest videos of which we evaluate on, averaging 15 seconds.
\\
\noindent\textbf{Condensed Movies Dataset (CMD)~\cite{bain2020condensed}} is a long-form text-video dataset consisting of 34K videos of movie scenes with an average duration of 132 seconds and corresponding high-level semantic textual descriptions. We train, validate and test on the recent challenge split~\cite{CMD_chall} of 32K, 2K, and 1K videos, respectively and compare to the Codalab leaderboard as well as evaluate results on other competing methods. Within a long movie scene there is a large variation among the events that occur and the information between shots, aggregating this over several minutes to match to a high-level semantic description requires a high degree of video understanding.
\\
\noindent\textbf{ActivityNet Captions~\cite{krishna2017dense}} consists of 20K videos from YouTube focused primarily on actions, annotated with 100K sentences. The training set consists of 10K videos, and we use the val1 set of videos to report results. With an average video duration of 180 seconds, these are the longest videos we evaluate on -- capturing a diverse range of events and actions within the two minutes. The captions consist of a sequence of descriptions of localised moments within the video. We employ the standard paragraph-to-video retrieval~\cite{Bain21} protocol when training and testing by concatenating the text sequences and evaluating on the whole long-form video.

\noindent\textbf{Charades~\cite{sigurdsson2016hollywood}} is a video classification dataset consisting of daily activities with an average duration of 30 seconds. The classification is multilabel and multiclass in that a video can contain multiple different actions at different times. This is a valuable setting for long-form video understanding since the action classes vary over the duration.

\subsection{Experiment Protocol}
\label{subsec:protocol}
We use CLIP ViT-B/16~\cite{radford2021learning} in all experiments and finetune the model end-to-end with the Adam optimizer~\cite{DBLP:journals/corr/KingmaB14} (learning rate set to 5e-7). Finetuning is done on two GPUs with a batch size of eight per GPU, and sample 16 frames from each video during training, randomly sampled within the video clip (we found this superior to the sub-segment sampling in~\cite{Bain21}). At test time 120 frames are sampled uniformly from the video, irrespective of their length, unless specified otherwise. For the text, words are dropped randomly during training with a probability of 10\%. For query-scoring, we use the  across all datasets to demonstrate the consistent performance (although optimal values might vary slightly between datasets). For the self and joint attention scoring methods, we use single-layer networks as was minimal difference in performance when using additional layers.
\subsection{Results}
\label{subsec:results}
\subsubsection{Comparison to the state of the art.}
We present the text-to-video retrieval results of the query-scoring method on MSR-VTT, ActivityNet and CondensedMovies in Tables~\ref{tab:msr-sota} and~\ref{tab:cmd-sota}. We achieve state-of-the art performance on all three datasets, significantly outperforming prior work aggregation methods using a CLIP backbone with millions of learned parameters. In contrast, our query-scoring method has only a single parameter. These results demonstrate the surprising effectiveness of \textit{weighted-mean} embeddings, and the limitations of current, more involved temporal aggregation methods. Query-scoring acts an improved baseline to proposed temporal aggregation methods.


\begin{table}[t]
\setlength{\tabcolsep}{5pt}
\scriptsize
\caption{\small{Comparison to state-of-the-art results on MSR-VTT 1k-A for text-to-video retrieval. The bottom section compares to methods using CLIP as the backbone image-text encoder, their different temporal aggregation methods (agg.), and the number of parameters learned for the aggregation. ``'' indicates an unknown value either due to no official public implementation or lack of reporting in the paper. \textbf{R@k:} Recall@K,\textbf{MedR:} Median Rank, \textbf{MnR:} Mean Rank.}}
\centering
\begin{tabular}{@{}rccrrrrr@{}}
\toprule
\multicolumn{3}{c}{\textbf{Method}}        & \multicolumn{1}{c}{\textbf{R@1}} & \multicolumn{1}{c}{\textbf{R@5}} & \multicolumn{1}{c}{\textbf{R@10}} & \multicolumn{1}{c}{\textbf{MedR}} & \multicolumn{1}{c}{\textbf{MnR}} \\ \midrule
\multicolumn{3}{c}{JSFusion~\cite{yu2018joint}}                                                                    & 10.2                             & 31.2                             & 43.2.4                              & 12                                &   -                               \\
\multicolumn{3}{c}{CE~\cite{Liu19a}}                                             & 20.9                             & 48.8                             & 62.4                              & 6                                 &   -                               \\
\multicolumn{3}{c}{MMT~\cite{gabeur2020multi}}                                                                    & 26.6                             & 57.1                             & 69.6                              & 4                                 &    -                              \\
\multicolumn{3}{c}{SSB~\cite{patrick2020support}}                                                                     & 30.1                             & 58.5                             & 69.3                              & 3                                 &  -                                \\
\multicolumn{3}{c}{TeachText~\cite{Croitoru21a}}                                                                     & 29.6                             & 61.6                             & 74.2                              & 3                                 &   -                               \\
 \multicolumn{3}{c}{Frozen~\cite{Bain21}}                                                                                  & 32.5                             & 61.5                             & 71.2                              & 3                                 &     -                             \\ \hline
\textbf{Method}  & \textbf{agg.}        & \textbf{\#agg. params} & &  &  &  &  \\ \midrule
{Clip4clip~\cite{Luo2021CLIP4Clip}}    & mean                 & 0                                                                   & 43.1                             & 70.4                             & 80.8                              & 2                                 & 16.2                             \\
{Clip4clip~\cite{Luo2021CLIP4Clip}}    & tightTransf          & 4M                                                                 & 40.2                             & 71.5                             & 70.5                              & 2                                 & 13.4                             \\
  {Clip4clip~\cite{Luo2021CLIP4Clip}}  & seqTransf            & 4M                                                                 & 44.5                             & 71.4                             & 81.6                              & 2                                 & 15.3                             \\
CAMoE~\cite{cheng2021improving}     & S.E attn.            & -                                                                 & 44.6                             & 72.6                             & 81.8                              & 2                                 & 13.3                             \\
Clip2Video~\cite{fang2021clip2video}   & TDB,TAB          & 19M                                                                & 45.6                             & 72.6                             & 81.7                              & 2                                 & 14.6                             \\ \midrule
\multicolumn{1}{c}{Ours}                     & Q-score              & 1                                     & \textbf{47.7}                    & \textbf{74.1}                    & \textbf{82.9}                     & 2                                 & \textbf{11.5}                    \\ \bottomrule
\end{tabular}
\label{tab:msr-sota}
\end{table}
 \begin{table}[t]
\centering
\scriptsize
\setlength{\tabcolsep}{3pt}
\caption{\small{Comparison to the start-of-the-art results on Condensed Movies and ActivityNet Challenge for text-to-video retrieval.}}
\begin{tabular}{@{}crrrrr|rrrrr@{}}
\toprule
                      & \multicolumn{5}{c|}{\textbf{Condensed Movies}}                                                                                                                                 & \multicolumn{5}{c}{\textbf{ActivityNet}}                                                                                                                                      \\
\textbf{Method}       & \multicolumn{1}{c}{\textbf{R@1}} & \multicolumn{1}{c}{\textbf{R@5}} & \multicolumn{1}{c}{\textbf{R@10}} & \multicolumn{1}{c}{\textbf{MdR}} & \multicolumn{1}{c|}{\textbf{MnR}} & \multicolumn{1}{c}{\textbf{R@1}} & \multicolumn{1}{c}{\textbf{R@5}} & \multicolumn{1}{c}{\textbf{R@10}} & \multicolumn{1}{c}{\textbf{MdR}} & \multicolumn{1}{c}{\textbf{MnR}} \\ \midrule
TeachText             & 12.1                             & 27.4                             & 37.5                              & -       & -                                 & 25.0                             & 58.7                             & -                                 & 4                              & -                                \\
Frozen                &  12.6                                &   28.4                               &  36.3                       &  25                      &   -   & 28.8                    & \multicolumn{1}{c}{-}   & 60.9                              & 3                              & -                                \\
Clip4clip (mean)      & 24.4                             & 48.2                             & 58.2                              & 6                              & 46.2                              & 40.5                             & 72.4                             & -                                 & 2                              & 7.4                              \\
Clip4clip (seqTransf) & 19.3                             & 44.4                             & 55.3                              & 9                              & 55.2                              & 40.5                             & 72.4                             & -                                 & 2                              & 7.5                              \\ \midrule
Ours (q-score)        & \textbf{27.0}                    & \textbf{52.3}                    & \textbf{61.2}                     & \textbf{5}                     & \textbf{41.2}                     & \textbf{44.0}                    & \textbf{74.9}                    & \textbf{86.1}                     & 2                              & \textbf{5.8}                     \\ \bottomrule
\end{tabular}
\label{tab:cmd-sota}
\end{table} 
We also compare to results on the long video classification dataset Charades in Tab.~\ref{tab:charades}. CLIP with query-scoring achieves the same performance as ActionCLIP~\cite{wang2021actionclip} which uses temporal modelling, averages predictions over 320 total frames, and a set of prompt templates. Query-scoring uses none of these yet offers similar performance, and outstanding improvements to CLIP4CLIP \textit{seqTransf} and the baseline of mean pooling the frame embeddings.

\noindent\textbf{Test-time normalisation}. Recent concurrent works achieve state of the art performance via CLIP with test-time normalisation such as QueryBank~\cite{cheng2021improving,bogolin2022cross} and dual softmax normalisation. The latter however requires access to all queries at test-time, which is not appropriate for real-world tasks. Both of these can be added to our method to achieve superior performance, but is not the focus of this work since it is separate to temporal modelling.

\begin{table}[t]
\setlength{\tabcolsep}{14pt}
\scriptsize
\centering
\caption{Multi-label classification results on the Charades dataset, where mAP is mean average precision. ActionCLIP results are computed by average 32 frame predictions over 10 (spatial) by 3 (temporal) views.}
\begin{tabular}{@{}clllr@{}}
\toprule
\multicolumn{1}{c}{Backbone}                                                                   & Aggregation         & Frame      & Finetune         & mAP  \\ \midrule

\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}CLIP\\ (ViT-B/16)\end{tabular}}                     & ActionCLIP~\cite{wang2021actionclip}          & 32 & \cmark & 44.6 \\
                                                                                               & Clip4clip & 32 & \cmark & 32.0      \\ \cline{2-5}
                                                                                               & Mean           & 32 & \cmark & 33.0   \\
                                                                                               & Q-scoring           & 32         & \cmark & \textbf{44.9} \\
                                                                                               & Temp. S-A           & 32         & \cmark & 36.3 \\
                                                                                               & Joint. S-A           & 32         & \cmark & 42.2 \\
                                                                                               
                                                                                               \midrule
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}CLIP\\ (ViT-B/16)\end{tabular}} & Mean                & 32         & \xmark           & 17.5 \\
                                                                         & Q-scoring           & 32         & \xmark          & \textbf{21.1} \\
\bottomrule
\end{tabular}
\label{tab:charades}
\end{table} 

\subsection{Ablation study}
\label{subsec:ablation}

\noindent\textbf{Alternative scoring methods.}
In Table~\ref{tab:attn_compare} we show the performance of alternative scoring methods, and that the performance is notably high across the board -- even without query information. This indicates that the strong baseline of weighted-mean of image-level CLIP embeddings is the best current approach of temporal modelling. The seemingly consistent boost no matter the scoring method indicates that the benefit is mainly afforded by restricting the aggregation  to linear weighted of the image embeddings. For ActivityNet, temporal self-attention scoring performs significantly better than the other scoring methods which is in contrast to CMD and MSR-VTT. One possible explanation for this is that ActivityNet captions are long paragraphs containing dense descriptions on the video by concatenating localised descriptions. Such a dense video description would be less useful when query-scoring since most frames relate to the query. This is unlike Charades where an action class might only correspond to a few seconds of a 30-second video amongst a sequence of other actions. Instead, the dense description setting of ActivityNet becomes a case of removing noisy frames, which we believe can be done without query-level information.

In contrast CMD, movies with long videos but extremely concise descriptions pertaining to a sequence of a few events in the video, affords the greatest benefits.

\begin{table}[t]
\centering
\caption{Comparison of the different proposed scoring methods on MSR-VTT, ActivityNet Captions and Condensed Movies test sets for text-to-video retrieval..}
\scriptsize
\begin{tabular}{@{}r|rrrr|rrrr|rrrr@{}}
\toprule
\multicolumn{1}{c|}{\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Scoring\\ Method\end{tabular}}}} & \multicolumn{4}{c|}{\textbf{MSR-VTT}}                                                                                                       & \multicolumn{4}{c|}{\textbf{Condensed Movies}}                                                                                              & \multicolumn{4}{c}{\textbf{ActivityNet Captions}}                                                                                          \\
\multicolumn{1}{c|}{}                                                                                   & \multicolumn{1}{c}{\textbf{R@1}} & \multicolumn{1}{c}{\textbf{R@5}} & \multicolumn{1}{c}{\textbf{R@10}} & \multicolumn{1}{c|}{\textbf{MnR}} & \multicolumn{1}{c}{\textbf{R@1}} & \multicolumn{1}{c}{\textbf{R@5}} & \multicolumn{1}{c}{\textbf{R@10}} & \multicolumn{1}{c|}{\textbf{MnR}} & \multicolumn{1}{c}{\textbf{R@1}} & \multicolumn{1}{c}{\textbf{R@5}} & \multicolumn{1}{c}{\textbf{R@10}} & \multicolumn{1}{c}{\textbf{MnR}} \\ \midrule
Baseline                                                                                                & 44.4                             & 71.6                             & 79.8                              & 12.8                              & 24.4                             & 48.2                             & 58.2                              & 46.2                              & 42.0                             & 73.1                             & 84.6                              & 7.4                              \\ \hline
Query                                                                                                   & \textbf{47.7}                    & \textbf{74.1}                    & 82.9                              & 11.5                              & \textbf{27.0}                    & \textbf{52.3}                    & \textbf{61.2}                     & \textbf{41.2}                     & 44.0                             & 74.9                             & 86.1                              & 5.8                              \\
Temporal Self-Attn.                                                                                     & 46.2                             & 71.4                             & 81.6                              & 12.9                              & 26.2                             & 51.9                             & 62.4                              & 41.5                              & \textbf{44.9}                    & \textbf{75.9}                    & \textbf{86.9}                     & \textbf{5.6}                     \\
Joint Attn.                                                                                             & 45.7                             & 73.8                             & \textbf{83.6}                     & \textbf{10.6}                     & 26.4                             & 51.1                             & 62.0                              & 43.0                              & 43.1                             & 74.1                             & 85.5                              & 6.7                              \\ \bottomrule
\end{tabular}
\label{tab:attn_compare}
\end{table} 

\begin{table}[t]
\centering
\scriptsize
\setlength{\tabcolsep}{9pt}
\caption{\small{Comparison of different frame aggregation methods and aggregation sources and their respective performance on zero-shot Condensed Movies, zero-shot MSR-VTT, and finetuned MSR-VTT text-to-video retrieval settings denoted by CMD ZS, MSR ZS and MSR FT repsectively. The reported value is the geometric mean of R@\{1,5,10\} to text-to-video retrieval performance. HParam denotes the hyperparameter selection for the chosen method.  Since training , training with  is not possible, so this setting is trained on  and evaluated on .}}
\begin{tabular}{@{}ccrrrrrr@{}}
\toprule
\multicolumn{1}{l}{}                                                               & \multicolumn{1}{l}{}            & \multicolumn{2}{c}{\textbf{CMD ZS}}                     & \multicolumn{2}{c}{\textbf{MSR ZS}}                     & \multicolumn{2}{c}{\textbf{MSR FT}}                     \\ \midrule
\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Aggregation\\ Method\end{tabular}}} & \multirow{2}{*}{\textbf{HParam}} & \multicolumn{2}{c}{Agg. source}                         & \multicolumn{2}{c}{Agg. source}                         & \multicolumn{2}{c}{Agg. source}                         \\ \cmidrule(l){3-8} 
                                                                                   &                                 & \multicolumn{1}{c}{Score} & \multicolumn{1}{c}{Feature} & \multicolumn{1}{c}{Score} & \multicolumn{1}{c}{Feature} & \multicolumn{1}{c}{Score} & \multicolumn{1}{c}{Feature} \\ \midrule
Mean-pooling                                                         &                                   &  27.6                  &  29.5                               & 48.3                             &  49.5                       & 60.5                           & 62.2                             \\ \hline
\multirow{4}{*}{Top-K}                                          & K=1                         & 28.8                  & 28.8                        &  48.8                       & 48.8                        & 63.3                          & 63.4                            \\
                                                                                   & K=8                       & 32.6                   & 33.3                        &  50.9                       & 50.3                          & 63.3                          & 64.5                            \\                    
& K=60                       & 30.0                   & 30.0                        &  51.2                     &  50.2                        &  61.9                         &  64.0                           \\ \midrule
\multirow{3}{*}{Query-scoring}                           & =0.01                    &  27.1                  & 30.6                        & 48.9                          & 49.0                     & 62.4                           & 64.4                            \\
                                                                                   & =0.1                      &  28.8                 & 30.9                        & 50.7                          & 50.5                      &  \textbf{63.8}                         & \textbf{65.4}                           \\
                                                                                   & =1.0                      &  27.7                 & 29.5                        &  48.5                        &  49.6                      & 60.1                          &  62.8                           \\ \bottomrule
\end{tabular}
\label{tab:scoring_compare}
\end{table} \noindent\textbf{Alternative aggregation methods.} Comparing different aggregation methods in Table~\ref{tab:scoring_compare}, we find that averaging over features is considerably better in both zero-shot and finetuning settings. Hard top-K seems to offer a comparable improvement over the baseline in the zero-shot setting, however we find soft scoring features for the finetune setting is most optimal. A smoother feature selection of frames seems to be more favourable for training. 

\noindent\textbf{Effect of number of frames.}
Unsurprisingly performance of both the baseline and query-scoring improves with increasing the number of input frames at test time, albeit with diminishing after ~1fps, shown in Figure~\ref{fig:test} (left). The improvement of query-scoring over the baseline also increases with the number of frames. Intuitively this makes sense, more input frames means there are more relevant frames to pick from and less relevant frames to ignore. This further motivates the use case of frame relevance scoring for long-form videos.

\begin{figure}[t]
\centering
\begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=.98\linewidth]{figures/num_frames_compare.png}
    \label{fig:cmd_num_frames}
\end{subfigure}\begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[width=.98\linewidth]{figures/temp_compare.png}
    \label{fig:cmd_temp_ablation}
\end{subfigure}
\caption{Downstream performance effects of varying the number of frames at test time (left) and the query-scoring temperature  (right), on the CMD test set for text-to-video retrieval.}
\label{fig:test}
\end{figure}

\noindent\textbf{Effect of scoring temperature.}
We find a temperature range between 0.05 and 0.15 is consistently optimal across long range datasets and tasks. Figure~\ref{test} (right) shows the effect of temperature on the CMD test set under both zero-shot and finetuning conditions. Zero-shot performance tends to be slightly better with smaller values of , which indicates the effect of  during the learning process. For extremely large values of , gradients only pass from the most similar text query which might cause drifting errors in the learning process (for the case where the initial similarity is wrong). \\

\noindent\textbf{Why are weighted-mean frame embeddings so effective?} \\

\noindent\textbf{a) Insufficient training data for learning new long-video text representations.} We find the relative performance boost of mean-weighted embeddings compared to more complex and learned temporal aggregations is reduced with the larger scale downstream datasets. This implies that with enough long video-text pairs, the more complex modelling attempts can outperform this simple baseline.

\noindent\textbf{b)The mean of frame embeddings captures distinct information.}
Given that CLIP embeddings are trained for the single image-text setting, it is surprising that taking the mean over many frames with vastly different content still performs well. For example, it is possible that the mean of embeddings from two semantically different frames maps to a semantically incorrect new space in the embeddings. To investigate whether this happens, we train both a linear classifier and a multi-layer perceptron (MLP) to classify between single-frame embeddings and mean embeddings from 16 frames sampled from a long video in CMD. We find that both are able to easily classify between these two, even with zero-shot embeddings (Table~\ref{tab:vmean_class}). These results suggest the mean-frame embeddings are mapped to entirely new locations in the embedding space, disjoint from the single-frame embeddings. This is encouraging since it suggests that CLIP can learn to capture multi-frame information within the 512 embeddings -- and hence the strong baseline out weighted-mean performing so well.


\begin{table}[t]
\centering
\setlength{\tabcolsep}{14pt}
\caption{Classification accuracy between normalised single-frame embeddings and 16-frame mean-pooled embeddings from CMD (training on the train set and testing on the test set). A single linear layer with high accuracy can discriminate between single-frame and 16-frame embeddings.}
\scriptsize
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{}  & \multicolumn{2}{c}{Test accuracy (\%)} \\ \cmidrule(l){2-3} 
Classifier & Zero-Shot         & Finetuned        \\ \midrule
Linear     & 89.0                   & 90.4               \\
MLP        & 97.4              & 98.2               \\ \bottomrule
\end{tabular}
\label{tab:vmean_class}
\end{table} 


\noindent\textbf{c) Query scoring during training improves single-frame representation.} The performance boost of query scoring after finetuning could be attributed to either (i) test time improvements by ignoring irrelevant improvements and/or (ii) improvements to the image-text level representation during training by contrastively learning on more semantically relevant frames. In order to investigate whether (ii) is true we evaluate on CMD test set retrieval in the 1-frame setting, to evaluate the single frame representation (Table~\ref{tab:vmean_class}). We find that query-scoring performance provides notable improvement to the image-text representation, indicating that such a method is valuable during the video-text learning process over the baseline of mean-pooling.
\begin{table}
\centering
\scriptsize
\setlength{\tabcolsep}{8pt}
\vspace{-0.5em}
\caption{Single-frame retrieval results on CMD test set. Finetuning with query scoring improves the image-level representation -- showing the effectiveness of constrastive learning on video-text pairs with relevance scoring on frames.}
\begin{tabular}{@{}crrrrr@{}}
\toprule
\textbf{Scoring Method} & \textbf{R@1}  & \textbf{R@5}  & \textbf{R@10} & \textbf{MedR} & \textbf{MnR} \\ \midrule
Mean  & 8.6          & 19.6       & 25.8        & 56.0  & 151 \\
Q-scoring & 9.0        & 21.1        & 27.3          & 52.5 &  148\\ \bottomrule
\end{tabular}
\vspace{-0.5em}
\label{tab:single-frame}
\end{table} 

\subsubsection{Are the frame scores semantically meaningful?} The notable improvement gains via frame scores implies some semantic relevance of the higher scoring frames. To investigate this we show qualitative results of the query scoring for CMD unseen videos. In Figure~\ref{fig:query_scoring_qual} we see that highest scoring frames are those with semantic similarity to the test, for example the frames containing the motorbike as well as the license registration of the character name in the title. Additionally, we see that the lowest scoring frames contain less information and have less relevance to the query. By utilising frame scoring during training, the constrastive loss is weighted less towards these irrelevant frames which could otherwise harm the representation.

\begin{figure}[!t]
\centering
\begin{subfigure}{0.95\textwidth}
    \centering
    \includegraphics[width=.98\linewidth]{figures/RxwUv1BcPwg.png}
    \label{fig:motorbike}
    \caption{``Blankman gives Kimberly a ride on his wacky motorcycle.''}
\end{subfigure}\\
\begin{subfigure}{0.95\textwidth}
    \centering
    \includegraphics[width=.98\linewidth]{figures/BtfFgfG6KIw.png}
    \label{fig:parents}
    \caption{``Mavis has an awkward lunch with her parents.''}
\end{subfigure}
\caption{\small{Visualisation of query-scoring from unseen videos in the Condensed Movies test set and their corresponding textual descriptions. The first row shows the weights assigned to each frame, with the maximum and minimum scores per segment marked and their corresponding frames in the middle and bottom rows respectively. The highest scoring frames in both (a) and (b) have high relevance to the text query and contain more information than the lowest scoring frames containing background frames or single person close-up shots.}}
\label{fig:query_scoring_qual}
\vspace{-0.8em}
\end{figure}


\section{Conclusion}
\vspace{-0.8em}
To conclude, we propose three simple ways to mean-weight frame embeddings from a joint image-text representation for long video retrieval and classification -- with and without query information, in doing so picking out the most salient frames. Our method provides a strong baseline, outperforming all prior works across four datasets, including attempts at more complicated temporal modelling. Our experiments uncover some insight into the benefits afforded by this highly constrained temporal aggregation and the challenges posed for more involved temporal modelling. Future work could look into tackling the lack of large-scale data needed to effectively learn effective long-form video representations. This could be done by employing self-supervised learning in addition to the scarcer textual supervision for long-form video data.
