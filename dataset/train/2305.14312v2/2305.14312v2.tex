
\pdfoutput=1

\documentclass[11pt]{article}

\usepackage{EMNLP2023}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{times, latexsym, microtype, inconsolata}

\usepackage{enumitem, graphicx, multirow, amsfonts, amsmath, verbatim, algorithm, algorithmicx, algpseudocode, booktabs, array, arydshln, times, epsfig, amssymb, pifont, capt-of, wrapfig, lipsum, makecell, tabulary, etoolbox, subcaption, xcolor}
\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}
\newcommand \blfootnote[1]{
    \begingroup
        \renewcommand \thefootnote{}\footnote{#1}
        \addtocounter{footnote}{-1}
        \vspace{-1ex}
    \endgroup
}

\title{Text-guided 3D Human Generation from 2D Collections}
\author{\textbf{Tsu-Jui Fu, Wenhan Xiong, Yixin Nie}\\\textbf{Jingyu Liu, Barlas OÄŸuz, William Yang Wang}\\ \\UC Santa Barbara~~Meta\\{\tt \small \{tsu-juifu, william\}@cs.ucsb.edu~~\{xwhan, ynie, jingyuliu, barlaso\}@meta.com}}

\begin{document}
\maketitle
\setlength{\abovedisplayskip}{5pt} \setlength{\belowdisplayskip}{5pt}

\begin{abstract}
3D human modeling has been widely used for engaging interaction in gaming, film, and animation. The customization of these characters is crucial for creativity and scalability, which highlights the importance of controllability. In this work, we introduce Text-guided 3D Human Generation (\texttt{T3H}), where a model is to generate a 3D human, guided by the fashion description. There are two goals: 1) the 3D human should render articulately, and 2) its outfit is controlled by the given text. To address this \texttt{T3H} task, we propose Compositional Cross-modal Human (CCH). CCH adopts cross-modal attention to fuse compositional human rendering with the extracted fashion semantics. Each human body part perceives relevant textual guidance as its visual patterns. We incorporate the human prior and semantic discrimination to enhance 3D geometry transformation and fine-grained consistency, enabling this to learn from 2D collections for data efficiency. We conduct evaluations on DeepFashion and SHHQ with diverse fashion attributes covering the shape, fabric, and color of upper and lower clothing. Extensive experiments demonstrate that CCH achieves superior results for \texttt{T3H} with high efficiency.\blfootnote{Project website:~\url{https://text-3dh.github.io}}
\end{abstract}

\begin{figure}[t]
\centering
    \includegraphics[width=\linewidth]{fig/fig_teaser.jpg}
    \vspace{-3.5ex}
    \caption{Text-guided 3D Human Generation (\texttt{T3H}).}
    \vspace{-2ex}
    \label{fig:teaser}
\end{figure}

\section{Introduction}
Our world is inherently three-dimensional, where this nature highlights the importance of 3D applications in various fields, including architecture, product design, and scientific simulation. The capability of 3D content generation helps bridge the gap between physical and virtual domains, providing an engaging interaction within digital media. Furthermore, realistic 3D humans have vast practical value, especially in gaming, film, and animation. Despite enhancing the user experience, the customization of the character is crucial for creativity and scalability. Language is the most direct way of communication. If a system follows the description and establishes the 3D human model, it will significantly improve controllability and meet the considerable demand.

We thus introduce Text-guided 3D Human Generation (\texttt{T3H}) to generate a 3D human with the customized outfit, guided via the fashion description. Previous works~\cite{Kolotouros2019human-recon,gao2022mps-nerf} depend on multi-view videos to learn 3D human modeling, but these data are difficult to obtain and are not language controllable. Text-to-3D~\cite{jain2022dream-field,poole2023dream-fusion} has shown attractive 3D generation results through the success of neural rendering~\cite{mildenhall2020nerf}. However, these methods apply iterative inference optimization by external guidance, which is inefficient for usage.

To tackle these above issues, we propose Compositional Cross-modal Human (CCH) to learn \texttt{T3H} from 2D collections. CCH divides the human body into different parts and employs individual volume rendering, inspired by EVA3D~\cite{hong2023eva3d}. We extract the fashion semantics from the description and adopt cross-modal attention to fuse body volumes with textual features, where each part can learn to perceive its correlated fashion patterns. To support various angles of view, CCH leverages the human prior~\cite{bogo2016smpl} to guide the geometry transformation for concrete human architecture. Then these compositional volumes can jointly render a 3D human with the desired fashion efficiently. The semantic discrimination further considers compositional distinguishment over each human part,  which improves the fine-grained alignment with its description through adversarial training.

We perform experiments on DeepFashion~\cite{liu2016deep-fashion,jiang2022text2human} and SHHQ~\cite{fu2022shhq}, which contain human images with diverse fashion descriptions. The patterns include various types of shapes (\textit{sleeveless}, \textit{medium short}, \textit{long}, etc.), fabrics (\textit{denim}, \textit{cotton}, \textit{furry}, etc.),  and colors (\textit{floral}, \textit{graphic}, \textit{pure color}, etc.) for the upper and lower clothing. To study the performance of \texttt{T3H}, we conduct a thorough evaluation from both visual and semantic aspects. We treat overall realism, geometry measure, and pose correctness to assess the quality of generated 3D humans. For the alignment with the assigned fashion, we apply text-visual relevance from CLIP and fine-grained accuracy by a trained fashion classifier.

The experiments indicate that language is necessary to make 3D human generation controllable. Our proposed CCH adopts cross-modal attention to fuse compositional neural rendering with textual fashion as 3D humans, and semantic discrimination further helps fine-grained consistency. In summary, our contributions are three-fold:
\begin{itemize}[noitemsep, leftmargin=*, topsep=1pt]
    \item We introduce \texttt{T3H} to control 3D human generation via fashion description, learning from collections of 2D images.
    \item We propose CCH to extract fashion semantics in the text and fuse with 3D rendering in one shot, leading to an effective and efficient \texttt{T3H}.
    \item Extensive experiments show that CCH exhibits sharp 3D humans with clear textual-related fashion patterns. We advocate that \texttt{T3H} can become a new field of vision-and-language research.
\end{itemize}

\begin{figure*}[t]
\centering
    \includegraphics[width=.95\linewidth]{fig/fig_cch.jpg}
    \vspace{-1ex}
    \caption{Compositional Cross-modal Human (CCH). CCH extracts fashion semantics from the description and adopts cross-modal attention in compositional body volumes for controllable 3D human rendering. The human prior (SMPL) provides robust geometry transformation, enabling CCH to learn from 2D collections for data efficiency. The semantic discrimination further helps find-grained consistency through adversarial training.}
    \vspace{-2ex}
    \label{fig:cch}
\end{figure*}

\section{Related Work}
\textbf{Text-guided Visual Generation.~}
Using human-understandable language to guide visual generation can enhance controllability and benefit creative visual design. Previous works built upon adversarial training~\cite{goodfellow2015gan,reed2016t2i} to produce images~\cite{xu2108attn-gan,el-nouby2019geneva,fu2020sscr,fu2022ldast} or videos~\cite{marwah2017att-video-gen,li2018t2v,fu2022lbve} conditioned on given descriptions. With sequential modeling from Transformer, vector quantization~\cite{esser2021taming} can generate high-quality visual content as discrete tokens~\cite{ramesh2021dalle,ding2021cog-view,fu2023tvc}. The denoising diffusion framework~\cite{ho2020diffusion,ramesh2022dalle2,saharia2022imagen,feng2023training-free} gains much attention as its diversity and scalability via large-scale text-visual pre-training. Beyond images and videos, 3D content creation is more challenging due to the increasing complexity of the depth dimension and spatial consistency. In this paper, we consider text-guided 3D human generation (\texttt{T3H}), which has vast applications in animated characters and virtual assistants.

\vspace{1ex} \noindent \textbf{3D Generation.~}
Different representations have been explored for 3D shapes, such as mesh~\cite{gao2019sdm-net,nash2020poly-gen,henderson2020mesh}, voxel grid~\cite{tatarchenko2017octree,li2017grass}, point cloud~\cite{li2018pc-gan,yang2019point-flow,luo2021surf-gen}, and implicit field~\cite{chen2019sdf,park2019deep-sdf,zheng2022sdf-style-gan}. Neural Radiance Field (NeRF)~\cite{mildenhall2020nerf,barron2022mip-nerf,muller2022instant-ngp} has shown remarkable results in novel view synthesis~\cite{schwarz2021graf,chan2021pi-gan,skorokhodov20233d-imagenet} and 3D reconstruction~\cite{yariv2021vol-sdf,zhang2021ners}. With the differentiable neural rendering, NeRF can be guided by various objectives. Text-to-3D draws appreciable attraction these days, which adopts external text-visual alignments~\cite{jain2022dream-field,mohammad2022clip-mesh,michel2022text2mesh,wang2022clip-nerf,hong2022avatar-clip} and pre-trained text-to-image~\cite{wang2022sjc,nam20223d-ldm, poole2023dream-fusion,metzer2023latent-nerf,tang2023make-it-3d,seo20233d-fuse}. However, existing methods take numerous iterations to optimize a NeRF model, which is time-consuming for practical usage. Our CCH learns to extract fashion semantics with NeRF rendering and incorporates the human prior for a concrete human body, achieving effective and efficient \texttt{T3H}.

\vspace{1ex} \noindent \textbf{3D Human Representation.~}
To reconstruct a 3D human, early works~\cite{collet2015human,guo2019relightables,su2020robust-fusion} count on off-the-shelf tools to predict the camera depth. As mitigating the costly hardware requirement, they estimate a 3D human texture~\cite{xu2021texformer,gomes2022uv} via the UV mapping~\cite{shysheya2019uv,yoon2021uv}. With the promising success of NeRF, recent works~\cite{peng2021neural-body,peng2021ani-nerf,su2021a-nerf} adopt volume rendering for 3D humans from multi-view videos~\cite{weng2022human-nerf,chen2022gp-nerf}. Since the data are difficult to collect, the 3D-aware generation~\cite{chan2022eg3d,gu2022style-nerf,noguchi2022enarf-gan} learns 3D modeling from the collection of human images~\cite{yang20223d-human-gan,hong2023eva3d}. In place of arbitrary outputs, we introduce the first controllable 3D human generation that also learns from a 2D collection, and the presented fashion patterns should align with the description.

\section{Text-guided 3D Human Generation}
\subsection{Task Definition}
We present text-guided 3D human generation (\texttt{T3H}) to create 3D humans via fashion descriptions. For data efficiency, a 2D collection = is provided, where  is the human image, and  is its fashion description. Our goal is to learn the neural rendering that maps  into an articulate 3D human with the fashion patterns of .

\subsection{Background}
Neural Radiance Field (\textbf{NeRF})~\cite{mildenhall2020nerf} defines implicit 3D as =. The query point  in the viewing direction  holds the emitted radiance  and the volume density . To get the RGB value  of certain rays , volume rendering is calculated along a ray  from the near bound  to the far bound :

where  stands for their accumulated transmittance. StyleSDF~\cite{or-ei2022style-sdf} then replaces  with single distance field (SDF)  for a better surface, where  and  is a learnable scalar that controls the tightness of the density around the surface boundary.

\vspace{1ex} \noindent \textbf{SMPL}~\cite{bogo2016smpl} defines the human body as , where  and  control its shape and pose. We consider Linear Blend Skinning (LBS) as the transformation from the canonical into the observation space for the point  to , where  is the scalar of the blend weight and  is the transformation matrix of the th joint. Inverse LBS transforms the observation back to the canonical space as a similar equation but with an inverted .

\subsection{Compositional Cross-modal Human}
Following EVA3D~\cite{hong2023eva3d}, we split the human body into 16 parts. As shown in Fig.~\ref{fig:cch}, each body part holds its own bounding box . To leverage the human prior for a target pose , we transform these pre-defined bounding boxes with SMPL's transformation matrices . Ray  is sampled for each pixel on the canvas. For a ray that intersects bounding boxes, we pick up its near and far bounds ( and ) and sample  points as follows: . We then transform these sampled points back to the canonical space with inverse LBS. For shape generalization, we consider not only pose transformation but also blend shapes ( and )~\cite{zheng2021pamir}.  contains  nearest vertices  of the target SMPL mesh for the sample point ray :

where  is the inverse weight of the vertex  and  is the transformation matrix. The  can be used for further volume rendering.

\vspace{1ex} \noindent \textbf{Cross-modal Attention.~}
During rendering, if the canonical point  with the viewing direction  is inside the th bounding box, it will be treated as:

where a linear mapping is applied to acquire preliminary features . To exhibit the desired fashion in the final rendering, we extract the word features by the text encoder as  from . We then fuse the textual features with  via cross-modal attention:

where  is the length of  and  is the learnable matrix. In this way, each point can learn to perceive relevant textual guidance for the th human body part and depict corresponding fashion patterns.

Each body part has its individual volume rendering , which consists of stacked multilayer perceptrons (MLPs) with the SIREN activation~\cite{sitzmann2020siren}. Since the point  may fall into multiple boxes , we follow EVA3D to apply the mixture function~\cite{lombardi2021mixture}:

where  and  are hyperparameters. With , we adopt Eq.~\ref{eq:nerf} to render the RGB value of ray . Through all sampled rays , we then have our final human rendering , where the overall process can be simplified as . To summarize, CCH leverages the human prior and adopts inverse LBS to acquire the canonical space for the target pose. The human body is divided into 16 parts, and each of them fuses its correlated fashion semantics via cross-modal attention. Finally, compositional bodies jointly render the target 3D human.

\begin{algorithm}[t]
    \begin{algorithmic}[1]
        \small
        \State : 2D collection of human images / fashion descriptions
        \\
        \State , : the generator / discriminator model for \texttt{T3H}
        \While{TRAIN\_CCH}
            \State ,   sampled human / description from 
            \State   estimated SMPL parameters of 
            \State   canonical points via inverse LBS \Comment{Eq.~\ref{eq:inverse-lbs}}
            \State   rendering features inside the th box \Comment{Eq.~\ref{eq:render-feature}}
            \State   extracted textual features of 
            \State CA  fusion via cross-modal attention \Comment{Eq.~\ref{eq:cross-modal-attention}}
            \State   mixture radiance / density \Comment{Eq.~\ref{eq:mixture}}
            \State   final rendering human \Comment{Eq.~\ref{eq:nerf}}
            \\
            \State   segmentation map of 
            \State   fashion map between  and  \Comment{Eq.~\ref{eq:fashion-map}}
            \State   adversarial loss from  \Comment{Eq.~\ref{eq:loss-adv}}
            \State ,   offset and derivation loss \Comment{Eq.~\ref{eq:loss-off}}
            \State   overall training loss \Comment{Eq.~\ref{eq:loss-all}}
            \State Update  by minimizing 
            \State Update  by maximizing 
        \EndWhile
    \end{algorithmic}
    \caption{Compositional Cross-modal Human}
    \label{algo:cch}
\end{algorithm}

\begin{table*}[t]
\centering \tablestyle{5pt}{1.1}
    \begin{tabular}{ccccccccccccc}
        \toprule
        ~ & ~ & \multicolumn{5}{c}{\textbf{DeepFashion}} & ~ & \multicolumn{5}{c}{\textbf{SHHQ}} \\
        \cmidrule{3-7} \cmidrule{9-13} Method & ~ & FID & Depth & PCK & CLIP-S & FA & ~ & FID & Depth & PCK & CLIP-S & FA \\
        \midrule
        Latent-NeRF & ~ & 69.654 & 0.0298 & 74.211 & 22.500 & 65.883 & ~ & 72.256 & 0.0381 & 73.401 & 22.210 & 67.427 \\
        TEXTure & ~ & 37.058 & 0.0165 & 86.354 & \underline{23.385} & \underline{67.508} & ~ & 48.618 & 0.0216 & 85.502 & \underline{24.456} & \underline{68.233} \\
        CLIP-O & ~ & \underline{25.488} & \underline{0.0133} & \underline{87.892} & 21.887 & 61.964 & ~ & \underline{34.212} & \textbf{0.0164} & \underline{87.312} & 21.401 & 66.808 \\
        CCH & ~ & \textbf{21.136} & \textbf{0.0121} & \textbf{88.355} & \textbf{25.023} & \textbf{72.038} & ~ & \textbf{32.858} & \underline{0.0165} & \textbf{87.624} & \textbf{27.855} & \textbf{76.194} \\
        \bottomrule
    \end{tabular}
    \vspace{-1.5ex}
    \caption{Overall results of pose-guided \texttt{T3H}.}
    \label{table:pose-guide}
    \vspace{-3ex}
\end{table*}

\vspace{1ex} \noindent \textbf{Semantic Discrimination.~}
With the SMPL prior, our CCH contains robust geometry transformation for humans and can learn from 2D images without actual 3D guidance. For a ground-truth , we parse the 2D human image as the segmentation  \cite{mmhuman3d},  which provides the reliable body architecture. To obtain its fashion map , we apply cross-modal attention between  and :

where  is the same dimension as ,  is the learnable attention matrix, and  perceives which human body part should showcase what fashion patterns. We concatenate the rendered human  (or the ground-truth ) with  and feed them into our discriminator  to perform binary classification:

In consequence,  can provide alignments of both the human pose and fashion semantics, which improves the fine-grained consistency of our CCH.

\vspace{1ex} \noindent \textbf{Learning of CCH.~}
We include the non-saturating loss with R1 regularization~\cite{mescheder2018non-saturating} for adversarial learning over the ground-truth :

Following EVA3D, we also append the minimum offset loss  to maintain a plausible human shape as the template mesh.  penalizes the derivation of delta SDFs to zero and makes the estimated SDF physically valid~\cite{gropp2020sdf}:

The learning process of our CCH is also illustrated as Algo.~\ref{algo:cch}, where the overall optimization can be:


\section{Experiemnts}
\subsection{Experimental Setup}
\textbf{Datasets.~}
We coduct experiments on DeepFashion~\cite{jiang2022text2human} and SHHQ~\cite{fu2022shhq} for \texttt{T3H}. DeepFashion contains 12K human images with upper and lower clothing descriptions. Since there are no annotations in SHHQ, we first fine-tune GIT~\cite{wang2022git} on DeepFashion and then label for 40K text-human pairs. We follow OpenPose~\cite{cao2019open-pose} and SMPLify-X~\cite{pavlakos2019smplify-x} to estimate the human keypoints and its SMPL parameters. The resolution is resized into 512x256 in our experiments. Note that all faces in datasets are blurred prior to training, and the model is not able to generate human faces.

\vspace{1ex} \noindent \textbf{Evaluation Metrics.~}
We apply metrics from both visual and semantic prospects. Following EVA3D, we adopt Frechet Inception Distance (\textbf{FID})~\cite{heusel2017fid} and \textbf{Depth}~\cite{ranftl2020depth} to calculate visual and geometry similarity, compared to the ground-truth image. We treat Percentage of Correct Keypoints (\textbf{PCK@0.5})~\cite{andriluka2014pck} as the correctness of the generated pose. To investigate the textual relevance of \texttt{T3H} results, we follow CLIP Score (\textbf{CLIP-S})~\cite{hessel2021clip-s} for the text-visual similarity. We fine-tune CLIP~\cite{radford2021clip} on DeepFashion for a more accurate alignment in this specific domain. To have the fine-grained evaluation, we train a fashion classifier on DeepFashion labels\footnote{There are six targets for FA, including the shape, fabric, and color of the upper and lower clothing. The fashion classifier has +95\% accuracy, which provides a precise evaluation.} and assess Fashion Accuracy (\textbf{FA}) of the generated human.

\vspace{1ex} \noindent \textbf{Baselines.~}
As a new task, we consider the following methods as the compared baselines.
\begin{itemize}[noitemsep, leftmargin=*, topsep=1pt]
    \item Latent-NeRF~\cite{metzer2023latent-nerf} brings NeRF to the latent space and guides its generation by the given object and a text-to-image prior.
    \item TEXTure~\cite{richardson2023texture} paints a 3D object from different viewpoints via leveraging the pre-trained depth-to-image diffusion model. 
    \item CLIP-O is inspired by AvatarCLIP~\cite{hong2022avatar-clip}, which customizes a human avatar from the description with CLIP text-visual alignment. We apply the guided loss to optimize a pre-trained EVA3D~\cite{hong2023eva3d} for faster inference.
    \item Texformer~\cite{xu2021texformer} estimates the human texture from an image. Text2Human~\cite{jiang2022text2human} predicts the target human image, and we treat Texformer to further build its 3D model.
\end{itemize}
For a fair comparison, all baselines are re-trained on face-blurred datasets and cannot produce identifiable human faces.

\vspace{1ex} \noindent \textbf{Implementation Detail.~}
We divide a human body into 16 parts and deploy individual StyleSDF~\cite{or-ei2022style-sdf} for each volume rendering, and two following MLPs then estimate SDF and RGB values. We adopt the same discriminator as StyleSDF over fashion maps to distinguish between fake rendered humans and real images. We sample =28 points for each ray and set (, ) to (4, 8) for mixture rendering. The text encoder is initialized from CLIP and subsequently trained with CCH. We treat Adam~\cite{kingma2015adam} with a batch size of 1, where the learning rates are 2e-5 for  and 2e-4 for . We apply visual augmentations by randomly panning, scaling, and rotating within small ranges. All trainings are done using PyTorch~\cite{paszke2017pytorch} on 8 NVIDIA A100 GPUs for 1M iterations.

\begin{table}[t]
\centering \tablestyle{5pt}{1.1}
    \begin{tabular}{cccccc}
        \toprule
        ~ & ~ & \multicolumn{3}{c}{\textbf{DeepFashion}} \\
        \cmidrule{3-5} Method & ~ & FID & CLIP-S & FA \\
        \midrule
        Texformer & ~ & 45.844 & \underline{20.546} & \underline{66.679} \\
        CLIP-O & ~ & \underline{25.579} & 20.112 & 61.298 \\
        CCH & ~ & \textbf{21.355} & \textbf{24.920} & \textbf{70.771} \\
        \bottomrule
    \end{tabular}
    \vspace{-1.5ex}
    \caption{Overall results of pose-free \texttt{T3H}.}
    \label{table:pose-free}
    \vspace{-1ex}
\end{table}

\begin{table}[t]
\centering \tablestyle{5pt}{1.1}
    \begin{tabular}{ccccccc}
        \toprule
        \multicolumn{3}{c}{\textbf{Ablation Settings}} & ~ & \multicolumn{3}{c}{\textbf{DeepFashion}} \\
        \cmidrule{1-3} \cmidrule{5-7} Text & CA & SD & ~ & FID & CLIP-S & FA \\
        \midrule
        \ding{55} & \ding{55} & \ding{55} & ~ & 25.671 & 9.632 & 36.634 \\
        \ding{51} & \ding{55} & \ding{55} & ~ & 24.624 & 21.079 & 69.173 \\
        \ding{51} & \ding{51} & \ding{55} & ~ & \underline{21.966} & \underline{24.103} & \underline{80.028} \\
        \ding{51} & \ding{51} & \ding{51} & ~ & \textbf{21.275} & \textbf{25.211} & \textbf{80.776} \\
        \bottomrule
    \end{tabular}
    \vspace{-1.5ex}
    \caption{Ablation study (256x128) of Cross-modal Attention (CA) and Semantic Discrimination (SD).}
    \label{table:ablation}
    \vspace{-3ex}
\end{table}

\subsection{Quantitative Results}
Table~\ref{table:pose-guide} shows the pose-guided \texttt{T3H} results on DeepFashion and SHHQ, where we feed the estimated human mesh as the input object into Latent-NeRF and TEXTure. Although Latent-NeRF can portray body shapes in multiple angles from its latent NeRF space, the rendering is clearly counterfeit (higher FID and Depth). For TEXTure, the human architecture is constructed well by the given mesh (higher PCK). However, the estimated texture is still spatially inconsistent and contains inevitable artifacts (still higher FID). From the semantic aspect, Latent-NeRF and TEXTure borrow those trained diffusion models and depict the assigned appearance in the description (higher CLIP-S than CLIP-O). CLIP-O relies on EVA3D to produce feasible 3D humans (lower FID). While the external CLIP loss attempts to guide the fashion, the global alignment is insufficient to demonstrate detailed patterns (lower FA). Without those above drawbacks, our CCH learns to extract fashion semantics along with the compositional human generation, leading to comprehensive superiority across all metrics.

A similar trend can be found on SHHQ. Latent-NeRF and TEXTure exhibit related fashion patterns but are hard to present realistic humans (higher FID and Depth). CLIP-O produces a sharp human body with the correct pose, but not the assigned fashion (lower CLIP-S and FA) by the inexplicit alignment from CLIP. Table~\ref{table:pose-free} presents the pose-free results. With the guided 2D image, Texformer contains the assigned clothing in the text (higher FA than CLIP-O). But the 3D reconstruction is unable to handle spatial rendering, resulting in low-quality humans (higher FID). With cross-modal attention and semantic discrimination, CCH exceeds baselines in both visual and textual relevance, making concrete human rendering with the corresponding fashion.

\begin{table}[t]
\centering \tablestyle{5pt}{1.1}
    \begin{tabular}{cccccc}
        \toprule
        ~ & ~ & ~ & \multicolumn{3}{c}{\textbf{DeepFashion}} \\
        \cmidrule{4-6} Train & FT. & ~ & R1 & R5 & R10 \\
        \midrule
        OpenAI-400M & \ding{55} & ~ & 4.2 & 20.4 & 28.0 \\
        LAION-2B & \ding{55} & ~ & \underline{13.4} & \underline{33.4} & \underline{46.4} \\
        LAION-2B & \ding{51} & ~ & \textbf{45.0} & \textbf{83.0} & \textbf{93.8} \\
        \bottomrule
    \end{tabular}
    \vspace{-1.5ex}
    \caption{Text-to-Fashion retrieval (sample 500 pairs) by CLIP with different fine-tunings (FT.).}
    \label{table:clip}
    \vspace{-1ex}
\end{table}

\begin{table}[t]
\centering \tablestyle{5pt}{1.1}
    \begin{tabular}{cccc}
        \toprule
        ~ & ~ & \multicolumn{2}{c}{\textbf{DeepFashion}} \\
        \cmidrule{3-4} Method & ~ & Quality & Relevance \\
        \midrule
        Latent-NeRF & ~ & 1.82 & 2.37 \\
        TEXTure & ~ & 2.38 & \underline{2.51} \\
        CLIP-O & ~ & \textbf{2.93} & 2.20 \\
        CCH & ~ & \underline{2.87} & \textbf{2.92} \\
        \bottomrule
    \end{tabular}
    \vspace{-1.5ex}
    \caption{Human evaluation for \texttt{T3H} with aspects of 3D quality and fashion relevance.}
    \label{table:human}
    \vspace{-3ex}
\end{table}

\begin{figure*}[t]
\centering
    \includegraphics[width=\linewidth]{fig/fig_pose-guide.jpg}
    \vspace{-3.5ex}
    \caption{Qualitative comparison of pose-guided \texttt{T3H}.}
    \vspace{-2ex}
    \label{fig:pose-guide}
\end{figure*}

\subsection{Ablation Study}
We study each component effect of CCH in Table~\ref{table:ablation}. Without the guided description, the model lacks the target fashion and results in a poor FA. This further highlights the importance of textual guidance for controllable human generation. When applying the traditional training~\cite{reed2016t2i}, conditional GAN is insufficient to extract fashion semantics for effective \texttt{T3H} (not good enough CLIP-S). On the other hand, our cross-modal attention constructs a better fusion between fashion patterns and volume rendering, facilitating a significant improvement in depicting the desired human appearance. Moreover, semantic discrimination benefits fine-grained alignment and leads to comprehensive advancement.

\vspace{1ex} \noindent \textbf{Fine-tune CLIP-S as Evaluator.~}
CLIP has shown promising text-visual alignment, which can calculate feature similarity between the generated human and the given text as CLIP-S~\cite{hessel2021clip-s}. Since our \texttt{T3H} is in a specific fashion domain, we consider the larger-scaled trained checkpoint from OpenCLIP~\cite{openclip} and fine-tune it as a more precise evaluator. Table~\ref{table:clip} presents text-to-fashion retrieval results, where a higher recall leads to a better alignment. Whether the original CLIP or OpenCLIP, both result in poor performance and is insufficient for our evaluation. By perceiving DeepFashion, fine-tuning helps bring reliable alignment and is treated as the final evaluator.

\begin{table}[t]
\centering \tablestyle{5pt}{1.1}
    \begin{tabular}{cccc}
        \toprule
        Method & ~ & Time (sec) & GPU (MB) \\
        \midrule
        Latent-NeRF & ~ & 755.7 & \underline{11250} \\
        TEXTure & ~ & \underline{103.7} & 12530 \\
        CLIP-O & ~ & 181.6 & 15988 \\
        CCH & ~ & \textbf{0.372} & \textbf{6258} \\
        \bottomrule
    \end{tabular}
    \vspace{-1.5ex}
    \caption{Time and GPU cost to perform \texttt{T3H}.}
    \label{table:efficiency}
    \vspace{-3ex}
\end{table}

\vspace{1ex} \noindent \textbf{Human Evaluation.~}
Apart from automatic metrics, we conduct the human evaluation with aspects of 3D quality and fashion relevance. We randomly sample 75 \texttt{T3H} results and consider MTurk\footnote{Amazon MTurk: \url{https://www.mturk.com}} to rank over baselines and our CCH. To avoid the potential ranking bias, we hire 3 MTurkers for each example. Table~\ref{table:human} shows the mean ranking score (from 1 to 4, the higher is the better). CLIP-O and CCH are built upon EVA3D, which provides an articulate human body for superior 3D quality. Even if Latent-NeRF and TEXTure take pre-trained diffusion models to acquire visual guidance, CCH exhibits more corresponding fashion via cross-modal fusion. This performance trend is similar to our evaluation, which supports the usage of CLIP-S and FA as metrics.

\vspace{1ex} \noindent \textbf{Inference Efficiency.~}
In addition to \texttt{T3H} quality, our CCH also contains a higher efficiency. Table~\ref{table:efficiency} shows the inference time and GPU cost on a single NVIDIA TITAN RTX. All baselines take more than 100 seconds since they require multiple iterations to optimize the 3D model from an external alignment. In contrast, we extract fashion semantics and carry out \texttt{T3H} in one shot. Without updating the model, we save the most GPU memory. In summary, CCH surpasses baselines on both quality and efficiency, leading to an effective and practical \texttt{T3H}.

\begin{figure}[t]
\centering
    \includegraphics[width=.79643765903\linewidth]{fig/fig_pose-free.jpg}
    \vspace{-0.5ex}
    \caption{Qualitative comparison of pose-free \texttt{T3H}.}
    \vspace{-3ex}
    \label{fig:pose-free}
\end{figure}

\begin{figure*}[t]
\centering
    \includegraphics[width=\linewidth]{fig/fig_animatable.jpg}
    \vspace{-3.5ex}
    \caption{Qualitative examples of animatable \texttt{T3H}, where the motion is also controlled by the text.}
    \vspace{-2ex}
    \label{fig:animatable}
\end{figure*}

\subsection{Qualitative Results.}
We demonstrate the qualitative comparison of pose-guided \texttt{T3H} in Fig.~\ref{fig:pose-guide}. Although Latent-NeRF can portray the 3D human based on the given mesh, it only presents inauthentic rendering. TEXTure generates concrete humans, but there are still obvious cracks and inconsistent textures from different angles of view. Moreover, both of them fail to capture ``\textit{three-point}'', where the rendered lower clothing is incorrectly depicted as long pants. Because CLIP provides an overall but inexplicit alignment to the description, CLIP-O is limited and exhibits vague ``\textit{denim}'' or ``\textit{long-sleeved}''. This observation further indicates the flaw of CLIP in detailed fashion patterns, even if it has been fine-tuned on the target dataset. In contrast, our CCH adopts cross-modal attention with NeRF, contributing to high-quality \texttt{T3H} with fine-grained fashion controllability. Fig.~\ref{fig:pose-free} shows the pose-free results. Texformer relies on a 2D image to estimate its 3D texture. Despite containing the assigned fashion, it is still restricted by the capability of 3D reconstruction, resulting in a low-resolution rendering. By learning text-to-3D directly, CCH can produce textual-related humans from random poses with clear visual patterns.

\begin{figure}[t]
\centering
    \includegraphics[width=.7625106022\linewidth]{fig/fig_pose-control.jpg}
    \vspace{-1ex}
    \caption{Qualitative examples of pose-control \texttt{T3H}.}
    \vspace{-3ex}
    \label{fig:pose-control}
\end{figure}

\vspace{1ex} \noindent \textbf{Pose-control \texttt{T3H}.~}
Since our CCH is generating 3D humans from given SMPL parameters, as illustrated in Fig.~\ref{fig:pose-control}, we can further control \texttt{T3H} with a specific pose. Different fashion descriptions make a human body present diverse appearances; different poses then guide the character to express rich body language. This flexibility in controlling appearance and pose allows for better practical customization.

\begin{figure}[t]
\centering
    \includegraphics[width=.82188295164\linewidth]{fig/fig_fa.jpg}
    \vspace{-1ex}
    \caption{Qualitative examples of our fashion classifier, which provides fine-grained labels for real/fake humans.}
    \vspace{-3ex}
    \label{fig:fa}
\end{figure}

\vspace{1ex} \noindent \textbf{Animatable \texttt{T3H}.~}
In addition to static poses, CCH can benefit from dynamic motions to achieve animatable \texttt{T3H}. Fig.~\ref{fig:animatable} adopts MotionDiffuse~\cite{zhang2022motion-diffuse} to create the assigned action also from the text and apply it to our produced 3D models. In this way, we prompt them to ``\textit{raise arms}'' or ``\textit{walk}'' for favorable dynamic scenarios.

\section{Conclusion}
We present text-guided 3D human generation (\texttt{T3H}) to create a 3D human by a fashion description. To learn this from 2D collections, we introduce Compositional Cross-modal Human (CCH). With cross-modal attention, CCH fuses compositional human rendering and textual semantics to build a concrete body architecture with the corresponding fashion. Experiments across various fashion attributes show that CCH effectively carries out \texttt{T3H} with high efficiency. We believe \texttt{T3H} helps advance a new field toward vision-and-language research.

\vspace{1ex} \noindent \textbf{Ethics Discussion and Limitation.~} Our work enhances the controllability of 3D human generation. To prevent identity leakage, we blur out faces prior to training and avoid risks similar to DeepFake~\cite{korshunov2018deep-fake}. Because we depend on SMPL parameters, an inaccurate estimation causes a distribution shift and quality degradation. For the datasets, they reveal narrow viewing angles, which results in visible artifacts of 3D consistency. 

\vspace{1ex} \noindent \textbf{Acknowledgements.~}
We appreciate the anonymous reviewers for constructive feedback. The research presented in this work was funded by Meta AI. The views expressed are those of the authors and do not reflect the official policy or position of the funding
agency.

\bibliography{anthology}
\bibliographystyle{acl_natbib}

\end{document}
