
\documentclass{article} \usepackage{iclr2024_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}


\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{pifont}

\usepackage{graphicx}
\usepackage{svg}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{amssymb}
\usepackage{subfig}
\usepackage{xcolor}
\definecolor{Gray}{gray}{0.92}
\definecolor{mygray}{RGB}{220,220,220}

\usepackage{wrapfig}
\usepackage{float}


\title{LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment}



\author{Bin Zhu\textsuperscript{1,2,*}, Bin Lin\textsuperscript{1,\thanks{Equal contribution.}}, Munan Ning\textsuperscript{1,4}, Yang Yan\textsuperscript{1}, JiaXi Cui\textsuperscript{1}, Hongfa Wang\textsuperscript{2}, Yatian Pang\textsuperscript{3}, \\ \textbf{Wenhao Jiang\textsuperscript{6}, Junwu Zhang\textsuperscript{1}, Zongwei Li\textsuperscript{2}, Wancai Zhang\textsuperscript{5}, Zhifeng Li\textsuperscript{2}, Wei Liu\textsuperscript{2}, Li Yuan\textsuperscript{1,4,\thanks{Corresponding author.}} }  \\
\textsuperscript{1}Peking University,
\textsuperscript{2}Tencent Data Platform,
\textsuperscript{3}National  University of Singapore, 
\textsuperscript{4}Pengcheng Lab,  \\ \textsuperscript{5}Nari Technology Development Limited Company,
\textsuperscript{6}Guangming Laboratory \\
}






\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}
\maketitle

\begin{abstract}



The video-language (VL) pretraining has achieved remarkable improvement in multiple downstream tasks.
However, the current VL pretraining framework is hard to extend to multiple modalities (N modalities, ) beyond vision and language. 
We thus propose \textbf{\textit{LanguageBind}}, taking the language as the bind across different modalities because the language modality is well-explored and contains rich semantics.
Specifically, we freeze the language encoder acquired by VL pretraining and then train encoders for other modalities with contrastive learning.
As a result, all modalities are mapped to a shared feature space, implementing multi-modal semantic alignment.
While LanguageBind ensures that we can extend VL modalities to N modalities, we also need a high-quality dataset with alignment data pairs centered on language. We thus propose \textbf{\textit{VIDAL-10M}} with 10 \textbf{\textit{M}}illion data with \textbf{\textit{V}}ideo, \textbf{\textit{I}}nfrared, \textbf{\textit{D}}epth, \textbf{\textit{A}}udio and their corresponding \textbf{\textit{L}}anguage. In our VIDAL-10M, all videos are from short video platforms with complete semantics rather than truncated segments from long videos, and all the video, depth, infrared, and audio modalities are aligned to their textual descriptions.
After pretraining on our dataset, we outperform CLIP4Clip by \textbf{3.7\%} R@1 on the MSR-VTT dataset with only \textbf{26\% of data} in the zero-shot video-text retrieval. Beyond this, our LanguageBind achieves superior performances on a broad range of \textbf{15 benchmarks} across video, audio, depth and infrared. Code address: \url{https://github.com/PKU-YuanGroup/LanguageBind}
\end{abstract}

\section{Introduction}

With the development of the Internet and smartphones, there has been a proliferation of video websites and apps (\textit{e.g.}, Youtube and TikTok), leading to a substantial increase number of videos~\citep{xue2022advancing}. Consequently, a set of video tasks have emerged, such as video search~\citep{smith1997image}, video recommendation~\citep{deldjoo2016content}, and video editing~\citep{casares2002simplifying, bonneel2014interactive}. To solve video understanding tasks, video-language pretraining has been employed by training foundation models by combining computer vision~\citep{he2016deep, dosovitskiy2020image} and natural language processing~\citep{vaswani2017attention}. These models can capture video semantics and solve downstream tasks~\citep{karpathy2014large, mithun2018learning}.




However, current VL pretraining frameworks are often limited to vision and language modalities.
The ImageBind~\citep{girdhar2023imagebind} introduces an indirect alignment method for multi-modal pretraining. It aligns other modalities to images, facilitating a comprehensive understanding of various modalities such as infrared~\citep{jia2021llvip}, depth~\citep{kim2022global}, audio~\citep{piczak2015esc}, and IMU~\citep{grauman2022ego4d}. In practical tasks such as zero-shot retrieval and classification as shown in Figure~\ref{fig:intro_fig}, the alignment with language modality is predominantly required for various modalities. While the indirect alignment of ImageBind may result in performance degradation, the LanguageBind method does not need images as intermediaries and facilitates straightforward expansion to additional modalities in downstream tasks.



In this paper, we propose the \textbf{\textit{LanguageBind}}, a language-based multi-modal pretraining framework that can extend video-language pretraining to multiple (N) modalities. As the language modality contains rich semantic information and is well-explored \citep{kenton2019bert, dai2019transformer}, we take it as the bind across different modalities. This process maps all modalities to a unified embedding space, enabling effective semantic alignment. To improve training efficiency, we employ Low-Rank Adaptation (LoRA)~\citep{hu2021lora} for fine-tuning, achieving impressive training results with minimal training iterations.


\begin{figure*}[t]
\centering
    \includegraphics[width=1.0\columnwidth]{images/intro_fig.pdf}     
\caption{\textbf{ImageBind vs. LanguageBind}. The ImageBind method relies on images as intermediaries, while the LanguageBind method dispenses with this requirement. LanguageBind directly aligns all modalities to the language space, thereby enhancing its applicability to downstream tasks. ``X'' represents all modalities except language, and ``c'' represents category.}
\label{fig:intro_fig}
\vspace{-0.4cm}
\end{figure*}

To further improve the modal integrity in pretraining and validate our LanguageBind, we introduce a dataset with five modalities, the \textbf{\textit{VIDAL-10M}}, which includes VL, IL (infrared-language), DL (depth-language), and AL (audio-language) data pairs. The videos of previous datasets are always truncated segments from long videos~\citep{miech2019howto100m,xue2022advancing}, resulting in fragmented semantics. To avoid this problem, we construct our video-text pairs from short videos with complete stories. To ensure the quality of the central language modality, we perform multi-view text generation and enhancement on VIDAL-10M.

\begin{wrapfigure}{r}{0.5\textwidth} 
\centering
    \includegraphics[width=0.5\columnwidth]{images/sota.pdf} 
    \caption{LanguageBind achieves superior performances on a broad range of \textbf{15 benchmarks} across video, audio, depth and infrared.} \label{fig:sota}
\end{wrapfigure}
The proposed LanguageBind ensures that we can extend vision-language to multiple (N) modalities, and our dataset VIDAL-10M benefits more downstream tasks beyond VL tasks, including video retrieval~\citep{luo2022clip4clip}, depth classification~\citep{cao2017estimating}, infrared classification~\citep{baffa2018convolutional} and audio classification~\citep{palanisamy2020rethinking}. As shown in Figure~\ref{fig:sota}, LanguageBind achieves superior performances on a broad range of 15 tasks. In text-to-video zero-shot retrieval based on CLIP-Large, LanguageBind achieves state-of-the-art (SOTA) performance on four datasets, surpassing InterVideo~\citep{wang2022internvideo} by 1.9\% on MSR-VTT~\citep{xu2016msr}, 8.8\% on MSVD~\citep{chen2011collecting}, 6.3\% on DiDeMo~\citep{anne2017localizing}, and 4.4\% on ActivityNet~\citep{caba2015activitynet}. For zero-shot classification on depth and infrared data, LanguageBind achieves a substantial performance advantage over ImageBind. LanguageBind attains top-1 accuracy of 87.2\% and 65.1\% on LLVIP and NYU-D, respectively, outperforming ImageBind by 23.8\% and 11.1\%. For zero-shot audio classification tasks, LanguageBind outperforms ImageBind with a 23.8\% higher top-1 accuracy on the ESC50 dataset.


We summarize our primary contributions as follows:
\begin{itemize}
\item We propose \textit{LanguageBind}, the langauge-based multi-modal pretraining approach. During the pretraining process, all modalities gradually align with the language modality through contrastive learning, and these modalities are unified within a shared embedding space. 
\item We introduce \textit{VIDAL-10M}, a large-scale five-modal video dataset, containing 10 million data pairs with aligned VL, IL, DL, and AL. To the best of our knowledge, \textit{VIDAL-10M} is the first large-scale video dataset with depth and infrared modalities.
\item Extensive experiments validate the effectiveness of our dataset and approach, achieving remarkable performance in video and other modality understanding tasks.
\end{itemize}


\section{Related work}

\paragraph{Multi-modal Pretraining}
Multi-modal pretraining begins with pretraining in vision and language. CLIP~\citep{radford2021learning} pioneered the alignment of images and texts on a large-scale dataset comprising 400 million samples, effectively establishing a bridge between the image and text domains. This alignment benefits a variety of downstream tasks, including zero-shot classification and image-text retrieval. CLIP can also be used as a foundation for alignment in other modalities.  For instance, CLIP4Clip \citep{luo2022clip4clip} aligns video with text, CLAP \citep{laionclap2023} aligns audio with text, and PointCLIP \citep{zhang2022pointclip} aligns point clouds with text.
Recent efforts have undertaken a comprehensive exploration of multi-modal alignment through pretraining. Augmenting the alignment process with additional modalities can enhance the model's robustness while maintaining its performance, as observed in VALOR \citep{chen2023valor} and VAST \citep{chen2023vast}. However, as the number of modalities increases, the training paradigm required to align them effectively undergoes significant changes. Meta-transformer \citep{zhang2023meta} accommodates 12 modalities and utilizes distinct tokenizers to harmonize the embedding space across modalities. ImageBind \citep{girdhar2023imagebind} expands multi-modal alignment pretraining to encompass six modalities but may not perform as well in language-related tasks due to indirect alignment. In our work, we propose LanguageBind, a direct alignment mechanism designed to align alternative modalities directly with the language modality, which has the highest information density. This direct alignment mechanism yields discernible improvements in downstream task performance.

\begin{wraptable}{r}{0.45\textwidth}
\small
\vspace{-0.4cm}
\setlength\tabcolsep{0.8mm}
\caption{Comparision of existing multi-modal datasets. VIDAL-10M is the currently first accessible multi-modal dataset including aligned VL, IL, DL, and AL data pairs.}
\label{tab:dataset}
\centering
\begin{tabular}{l|ccc}
\toprule
\textbf{Datasets}  & \textbf{Samples}   & \textbf{Modality} & \textbf{Year} \\
\midrule
HMDB-51   & 7K & V & 2011\\
UCF-101  & 13K   & V & 2012\\
\midrule
ActivityNet-200 & 20K   & VT & 2015 \\
WebVid-10M & 10.7M  & VT & 2021\\
HD-VILA-100M  & 100M    & VT & 2022\\
HowTo-100M   & 136M   & VT & 2019\\
\midrule
LLVIP    & 15k   & VI &   2021 \\
FLIR V1   & 10k  & VI &   2015 \\
FLIR V2  & 12k   & VI &   2015 \\
NYU-D  & 1.4k   & VD &   2012 \\
YouTube-8M & 6.1M & VAT & 2016\\
AVA & 58K & VAT & 2017 \\

\midrule
\textbf{VIDAL-10M (Ours)} & 10M    & VIDAL & 2023\\
\bottomrule
\end{tabular}
\end{wraptable}

\paragraph{Multi-modal Datasets} 
Multi-modal datasets serve as the foundation for multi-modal pretraining. Initially, these datasets only consisted of videos and their corresponding categories, as shown in Table~\ref{tab:dataset}. HMDB-51 \citep{kuehne2011hmdb} and UCF-101 \citep{soomro2012ucf101} are examples of such datasets, which contain truncated segments from long videos with manual annotation. However, creating these datasets required significant human effort, which limited their scalability and diversity. To address this issue, researchers turned their attention to the abundance of video-text resources available on the internet. Inspired by the success of image-text datasets \citep{sharma2018conceptual, changpinyo2021conceptual}, they used script-based programming \citep{schuldt2004recognizing,kong2019mmact, sigurdsson2018charades} to extract millions of video-text data pairs. However, acquiring data from modalities like infrared \citep{flirv1, flirv2} and depth \citep{silberman2012indoor}, which require specialized equipment and manual annotation, has been challenging. This has severely limited the scale of the data and its alignment with other modalities. Although existing work like ImageBind \citep{girdhar2023imagebind} has attempted to bind various image-paired datasets and achieve indirect semantic alignment between different modalities, this approach still faces issues of incomplete and indirect data alignment. Thus, there is an urgent need for multi-modal datasets with direct semantic aligned data pairs, especially for modalities with five or more types.


\begin{figure*}[htbp]
\centering
    \includegraphics[width=1.0\columnwidth]{images/languagebind.pdf}     
\caption{
\textbf{LanguageBind overview}. The language encoder parameters are frozen, while the multi-modal encoder parameters can be adjusted using the LoRA technique. By employing contrastive learning between language and other modalities, LanguageBind successfully achieved multimodal joint learning, thereby fostering semantic alignment across different modalities.
} \label{fig:languagebind_method}
\vspace{-0.4cm}
\end{figure*}



\section{Method}

In this section, we present LanguageBind, a multi-modal pretraining approach designed to align the semantics of different modalities and enhance cross-modal retrieval and zero-shot classification. As shown in Figure~\ref{fig:languagebind_method}, LanguageBind consists of three parts: (a) multi-modal encoders, (b) language encoder, and (c) multi-modal joint learning.

\subsection{Multi-modal Encoders}
\label{section: Multi-modal Encoders}
For other modalities besides language, we employ the 24-layer, 1024-dimensional vision transformer with a patch size of 14. The encoders are initialized from OpenCLIP-large \citep{ilharco_gabriel_2021_5143773}. Depth and infrared are treated as RGB images, which are replicated 3 times in the channel dimension to align with RGB images. Following ImageBind, audio data is transformed into spectrograms with a duration of 10 seconds (128 mel-bins) and we repeat and pad the spectrograms. For example, a 4-second spectrogram would be repeated twice and then padded with zero for an additional 2 seconds. Similarly, we also replicate it 3 times in the channel dimension. If the duration exceeds 10 seconds, we randomly sample three 10-second audio segments, each from the front 1/3, middle 1/3, and back 1/3 of the original audio, and finally stack them together.

\paragraph{Patch masking} To address the inefficiency in processing all tokens within the encoder, we divide the image into patches and take a small portion of patches by encoder mask , following MAE \citep{he2022masked}. Given a modality , where  represents the resolution of the original data, with  denoting the number of channels. We first transform it into patches using a patch embedding layer with non-overlapping filters. This operation produces patches denoted as  and  represents the resulting sequence length, where  represents the size of each patch. Subsequently, positional embedding is applied to the visible tokens, which are divided by encoder mask. The combined sequence  is defined as:
	
where  is a sequence of learnable position tokens, and  represents the position index at patches.
 
\paragraph{LoRA fine-tuning} We employ the LoRA technique ~\citep{hu2021lora} to accelerate fine-tuning. For a modality-agnostic encoder with a weight matrix , we maintain the weight matrix  frozen while learning a new weight matrix . For instance, in the case of the modality-agnostic encoder  and , the forward process can be represented as follows:

where , with  being the minimum of  and . It is important to highlight that both  and  possess the same input and output dimensions, facilitating their summation to produce the final output.
\paragraph{Modality extending} To extend the LanguageBind method to multiple (N) modalities, the first step involves processing the data into a sequence of tokens. Subsequently, the parameters are initialized from OpenCLIP. The encoder for different modalities is then trained through token masking and LoRA fine-tuning while keeping the language encoder frozen. Finally, this modality is aligned with the language feature space.
 
\subsection{Language Encoder and Multi-modal Joint Learning}
\label{section: Language Encoder and Multi-modal Joint Learning}
For the language encoder, we utilize a 12-layer transformer model with 768-dimensional and initialize it from OpenCLIP. For a given text, we initially employ a BPE tokenizer to segment words into relatively common subwords. Each subword corresponds to a unique token, and these tokens are embedded within a word embedding layer. Ultimately, the tokens are encoded by the language encoder to obtain a text logit , where  represents the length of the sequence. To ensure alignment across different modalities, we implement contrastive learning principles ~\citep{radford2021learning}. The objective of this approach is to increase the similarity of paired data, bringing them closer to the same semantic space, while minimizing the similarity of unpaired data. We utilize contrastive learning to bind individual modalities to language.

where  is the -th modality data and  is the -th text and their features are normalized.  and  are batch size and the temperature. The direct alignment of each modality  with language  enables us to significantly enhance zero-shot classification and retrieval tasks. 






















\section{The VIDAL-10M dataset}

In this section, we will describe how to construct our VIDAL-10M dataset, including 3 million pairs of video-language data, 3 million pairs of infrared-language data, 3 million pairs of depth-language data, and 1 million pairs of audio-language data. 
As shown in Figure~\ref{fig:framework}, the collection process consists of three main steps: visual search term database construction (Section~\ref{subsec:Visual search term database construction}), video and audio collection and filtering (Section~\ref{subsec: Video collection and filtering}), and modality generation and enhancement (Section~\ref{subsec:Modality generation and enhancement}).

\begin{figure*}[htbp]
\centering
\includegraphics[width=1.0\textwidth]{images/iclr-fivemodal-dataset-construction_crop.pdf}
\caption{\textbf{VIDAL-10M construction.} (a) Firstly, a search term database is generated by leveraging visually related datasets. (b) Subsequently, relevant videos and audios are collected from the internet and undergo a series of filtering processes. (c)  Lastly, we perform infrared and depth modality generation, as well as multi-view text generation and enhancement.}
\label{fig:framework}
\vspace{-0.4cm} 
\end{figure*}
\subsection{Visual search term database construction}
\label{subsec:Visual search term database construction}
To build a video dataset with rich visual concepts and diversity, we design a unique search term acquisition strategy. We leverage text data including labels and captions from various visual task datasets (YouTube-8M~\citep{abu2016youtube}, MSR-VTT~\citep{xu2016msr}, COCO~\citep{lin2014microsoft}, 
AVA~\citep{gu2018ava}, HMDB-51~\citep{kuehne2011hmdb}, ImageNet~\citep{deng2009imagenet}) to create a large-scale search term database with diversity and broad applicability. Then we filter these search terms based on their frequency and employ the NLTK toolkit for part-of-speech tagging, followed by tallying the occurrences of keywords (nouns and verbs). A balanced subset of 100,000 search items corresponding to these keywords is then extracted as the final search team database. 

\subsection{Video and Audio collection and filtering}
\label{subsec: Video collection and filtering}
During the data collection process, we utilize the aforementioned search terms to retrieve video-text pairs and audio-text pairs from relevant platforms, e.g. YouTube Shorts, and Freesound. Regarding video collection, in order to obtain short videos with high-quality textual descriptions, we implemented a filtering mechanism for the title and hashtags. Video samples with titles containing less than 2 words and without video hashtag labels are excluded from our dataset. Moreover, we removed irrelevant words and hashtags, such as "youtube", "fyp", "shorts", etc. Furthermore, to ensure a complete, consistent, and precise depiction of the event within a single full video, we decide to impose a duration limit of 20 seconds. Shorter videos tend to exhibit better scene coherence and event integrity and are more closely aligned with corresponding hashtags and title descriptions. Ultimately, we obtain a short video dataset that encompasses more specific rather than abstract content. Concerning audio collection, we rank the audio list on different audio platforms based on its similarity to the search terms. Additionally, we conduct filtering operations similar to those for videos, taking into account factors such as audio ratings, download counts, user comments, tags, and duration. This comprehensive approach allows us to curate and refine the audio and video content more effectively. 







\subsection{Modality generation and enhancement}
\label{subsec:Modality generation and enhancement}

\paragraph{Multi-view text generation and enhancement}
The language modality of VIDAL-10M consists of multi-view texts, including title, hashtags, keyframe captions, video captions, and enhanced captions. The detailed text generation and enhancement pipeline is illustrated in Figure~\ref{fig:multi-view text generation and enhancement}. Hashtags in VIDAL-10M are specifically designed to highlight the main subjects and actions depicted in the video. These hashtags serve as key indicators, emphasizing the focal points and dynamic elements of the video. However, hashtags alone may not fully capture the spatial information conveyed by the video frames. To address this limitation, we leverage the image captioning model OFA  \citep{wang2022unifying} to generate supplementary keyframe captions that enrich the spatial information at the keyframe level. These captions also contain local temporal information related to the video content, which is beneficial for visual-text pretraining. Besides spatial information, temporal information concealed within the video is equally significant, providing crucial insights into the progression and sequencing of events within the video. To further supplement the overall thematic and temporal information of the video, we employ the mPLUG-owl model \citep{ye2023mplug} to generate video captions based on the combination of video, title, and hashtags. By leveraging the title and hashtags as accurate video labels, we guide the mPLUG-owl model to generate captions that align with the video theme, reducing potential model bias to a certain extent. Furthermore, to extract valuable information from the generated video captions, we utilize the ChatGPT model to refine and enhance the textual description, thereby greatly improving the quality of the text. By incorporating the above text components, multi-view textual descriptions provide a comprehensive and detailed representation of the video content. 



\paragraph{Infrared and depth modality generation}

In the field of depth and infrared, creating modal datasets typically requires specialized equipment and human effort, resulting in limited data. Despite the success of large-scale pretraining models~\citep{radford2021learning,laionclap2023,luo2022clip4clip,chen2023vast} in NLP and CV, there remains a lack of large-scale data in this field. To address this challenge, we propose using advanced generative models specifically to construct a large-scale dataset of depth and infrared. The sRGB-TIR model~\citep{lee2023edge} is used for infrared modality generation and the GLPN model~\citep{kim2022global} for depth modality generation, generating depth and infrared from keyframes in our videos. While some limitations may exist, our collection of millions of video frames and corresponding texts with highly diverse semantics can significantly reduce the presence of model biases.



\section{Experiments and Results}
\label{sec:exp_and_res}
In this section, we evaluate the effectiveness of LanguageBind in various downstream tasks through different experiments. Firstly, LanguageBind's capability to align video and text is assessed using zero-shot video-text retrieval. Additionally, we use LanguageBind to enhance the performance of downstream tasks that involve depth, infrared images, and audios. Finally, we conduct ablation experiments to analyze the impact of different parameter configurations and text descriptions on LanguageBind's performance.






\subsection{Zero-shot retrieval in Video-Language} 



\paragraph{Comparison to prior methods} 
In the zero-shot video-text retrieval benchmark, we utilize ViT-L/14 as the video encoder and add temporal attention layers for fair comparison, which can be found in Appendix~\ref{appendix:pretraining_details}. According to the results presented in Table~\ref{tab:retrieval_languagebind}, the performance of LanguageBind exceeds that of VideoCoca~\citep{yan2022video} and OmniVL~\citep{wang2022omnivl} by 8.3\% and 8.0\% respectively on MSR-VTT. 
In comparison to the ImageBind model utilizing the Vit-Huge architecture, the LanguageBind model, employing the Vit-Large model, showcases superior experimental outcomes. Furthermore, compared to models based on CLIP-Large but using more training data, LanguageBind achieves state-of-the-art (SOTA) performance on four datasets, outperforming InterVideo~\citep{wang2022internvideo} by 1.9\% on MSR-VTT, 8.8\% on MSVD, 6.3\% on DiDeMo, and 4.4\% on ActivityNet. We also exceed TVTSv2~\citep{zeng2023tvtsv2} by 4.4\% and 3.2\% on MSR-VTT and DiDeMo, respectively. Moreover, we outperforms UMT-L~\citet{li2023unmasked} on all datasets. For a fair comparison of dataset validity, we use the Vit-B/32 model of CLIP4CLIP to conduct validation experiments using the 100K subset of VIDAL-10M and the 380k subset of HowTo100M. As shown in Table2, the VIDAL-100k outperforms the HT100M-380k on both MSRVTT and MSVD datasets, validating the effectiveness of our dataset.



\begin{table*}[htbp]
\small
\caption{\textbf{Zero-shot Video-Text retrieval performance} of LanguageBind across four datasets.  donates the result of full tuning.
}
\label{tab:retrieval_languagebind}
\centering
\setlength\tabcolsep{0.8mm}
\begin{tabular}{lc|ccc|ccc|ccc|ccc}
        \toprule
        \multirow{2}{*}{\bf{Method}} & \multirow{2}{*}{\bf{Dataset}} & \multicolumn{3}{c}{\bf{MSR-VTT}} & \multicolumn{3}{c}{\bf{MSVD}} & \multicolumn{3}{c}{\bf{DiDeMo}} & \multicolumn{3}{c}{\bf{ActivityNet}} \\
         &  & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 \\
        \midrule
        \multicolumn{14}{l}{\textit{Non-CLIP models}} \\
OmniVL & 14M & 34.6 & 58.4 & 66.6 & - & - & - & 33.3 & 58.7 & 68.5 & - & - & - \\
VideoCoCa & 100M & 34.3 & 57.8 & 67.0 & - & - & - & - & - & - & 34.5 & 63.2 & 76.6 \\
        \midrule
        \multicolumn{14}{l}{\textit{CLIP-H/14}} \\
        ImageBind & - & 36.8 & 61.8 & 70.0 & - & - & - & - & - & - & - & - & - \\
        \midrule
        \multicolumn{14}{l}{\textit{CLIP-L/14}} \\
        UMT & 5M & 33.3 & 58.1 & 66.7 & 44.4 & 73.3 & 82.4 & 34.0 & 60.4 & 68.7 & 31.9 & 69.2 & 72.0 \\
        TVTSv2 & 8.5M & 38.2 & 62.4 & 73.2 & - & - & - & 34.6 & 61.9 & 71.5 & - & - & - \\
        InternVideo & 12.8M & 40.7 & - & - & 43.4 & - & - & 31.5 & - & - & 30.7 & - & - \\
        \bf{LanguageBind} & \bf{3M} & \bf{42.6} & \bf{65.4} & \bf{75.5} & \bf{52.2} & \bf{79.4} & \bf{87.3} & \bf{37.8} & \bf{63.2} & \bf{73.4} & \bf{35.1} & \bf{63.4} & \bf{76.6} \\
        \bf{LanguageBind} & \bf{3M} & \bf{42.7} & \bf{67.1} & \bf{77.0} & \bf{53.5} & \bf{80.5} & \bf{87.5} & \bf{38.1} & \bf{65.0} & \bf{73.6} & \bf{36.9} & \bf{65.1} & \bf{77.2} \\
        
      \bottomrule
      \end{tabular}
\end{table*} 















\begin{table*}[htbp]
\small
\caption{ \textbf{Zero-shot Video-Text retrieval} of CLIP4Clip to verify the effectiveness of VIDAL-10M.
}
\label{tab:retrieval}
\centering
\setlength\tabcolsep{1.5mm}
\begin{tabular}{cl|cc|cccc}
        \toprule
        \textbf{Dataset} & \textbf{Method} & \textbf{Parameter} & \textbf{Source} &\textbf{R@1} & \textbf{R@5} & \textbf{R@10} & \textbf{MR} \\
        \midrule
\multirow{2}*{MSR-VTT} 
& CLIP4Clip  & 86M  & WIT400M, HT100M-380k  & 32.0 & 57.0 & 66.9 & 4.0 \\  
& CLIP4Clip & \textbf{86M} & WIT400M, \textbf{VIDAL-100k}   &  \textbf{35.7} & \textbf{60.8} & \textbf{71.5} & \textbf{3.0} \\
        \midrule
        
         \multirow{2}{*}{MSVD} 
& CLIP4Clip & 86M   & WIT400M, HT100M-380k & 38.5 & 66.9 & 76.8 & 2.0 \\  
& CLIP4Clip & \textbf{86M}  & WIT400M, \textbf{VIDAL-100k} & \textbf{42.0} & \textbf{70.0} & \textbf{79.2} & \textbf{2.0} \\  
        






      \bottomrule
      \end{tabular}
\end{table*} 



\subsection{Zero-shot in multiple modalities}
\label{sec:result_inf_dep}
\paragraph{Zero-shot X-Language classification} We compare our model with the recent state-of-the-art multi-modal pretraining models, OpenCLIP~\citep{ilharco_gabriel_2021_5143773} and ImageBind~\citep{girdhar2023imagebind} on multi-modal understanding ability tasks in Table~\ref{tab:depth_and_infrared}. 
For video zero-shot classification, we outperform ImageBind by 14.0\% with a smaller model on Kinetics-400~\citep{kay2017kinetics}, and we also report the results of multi-view/crop~\citep{simonyan2014two} on OpenCLIP for further comparison. For infrared, LanguageBind exhibits a noteworthy 23.8\% performance advantage over ImageBind on the LLVIP and outperforms OpenCLIP on all three datasets (LLVIP, FLIR V1, and V2). For depth images, our zero-shot results on NYU-D surpass ImageBind by a substantial margin of 11.1\% and outperform OpenCLIP by 19.7\%. For audio, we outperform ImageBind by 10.1\% on Audioset~\ dataset and 1.1\% on VGGSound dataset. We outperform ImageBind by a large margin of 23.9\% on the ESC-50 dataset.


\begin{table}[htbp]
\setlength\tabcolsep{1.2mm}
    \small
    \captionof{table}{\textbf{Zero-shot X-Language classification}. We report the mAP for Audioset Audio-only (AS-A)~\citep{gemmeke2017audio} and top-1 accuracy results for others.  donates the results of full tuning.}
\centering
    \begin{tabular}{l|c|cc|ccc|c|ccc}

        \toprule
        \multirow{2}{*}{\bf{Method}} & \multirow{2}{*}{\bf{Size}} & \multicolumn{2}{c|}{\bf{Video}} & \multicolumn{3}{c|}{\bf{Infrared}} & \multicolumn{1}{c|}{\bf{Depth}} & \multicolumn{3}{c}{\bf{Audio}} \\
         &  & K400 & K600 & LLVIP & FLIR V1  & FLIR V2 & NYU-D & AS-A & ESC-50 & VGGS \\
        \midrule
ImageBind & Huge & 50.0 & - & 63.4 & - & - & 54.0 & 17.6 & 66.9 & 27.8 \\
        OpenCLIP & Large & 60.7 & 59.0 & 82.2 & 81.2 & 42.6 & 45.4 & - & - & - \\
        \bf{LanguageBind} & \bf{Large} & \bf{64.0} & \bf{61.9} & \bf{87.2} & \bf{82.9} & \bf{48.0} & \bf{65.1} & \bf{27.7} & \bf{91.8} & \bf{28.9} \\
        \bf{LanguageBind} & \bf{Large} & - & - & - & - & - & -& \bf{30.0} & \bf{94.0} & \bf{38.6} \\
\bottomrule
        
    \end{tabular}
    \label{tab:depth_and_infrared}
\vspace{-0.2cm}
\end{table}




\paragraph{Zero-shot Audio-Language retrieval} 
\begin{wraptable}{r}{0.45\textwidth}
\small
\vspace{-0.4cm}
\setlength\tabcolsep{1.1mm}
\caption{\textbf{Zero-shot Audio-Language retrieval.}   donates the results of full tuning.}
\label{tab:audio_ret}
\centering
\begin{tabular}{l|cc|cc}
\toprule
    \multirow{2}{*}{\bf{Method}}  & \multicolumn{2}{c|}{\bf{Clotho}}   & \multicolumn{2}{c}{\bf{Audiocaps}} \\
    & R@1 & R@10 & R@1 & R@10 \\
    \midrule
    AVFIC   & 3.0 & 17.5 & 8.7 & 37.7 \\
    ImageBind   & 6.0 & 28.4 & 9.3 & 42.3 \\
    VALOR   & 8.4 & - & - & - \\
\textbf{LanguageBind}   & \bf{12.1} & \bf{44.0} & \bf{12.2} & \bf{53.2} \\
    \textbf{LanguageBind}   & \bf{16.7} & \bf{52.0} & \bf{19.7} & \bf{67.6} \\
    \bottomrule
\end{tabular}
\end{wraptable} We compare zero-shot text-to-audio retrieval performance on Clotho~\citep{font2013freesound} datasets and Audiocaps~\citep{kim2019audiocaps} dataset in Table~\ref{tab:audio_ret}. For the Clotho dataset, LanguageBind exhibits a significantly higher margin surpassing AVFIC~\citep{nagrani2022learning} and ImageBind by 9.1\% and 6.1\% respectively. 
Moreover, our LanguageBind model also surpasses the powerful baseline of VALOR~\citep{chen2023valor} by 3.7\%. The same trend is also observed in the Audiocaps dataset. LanguageBind outperformed AVFIC and ImageBind by 2.9\% and 5.5\%, respectively. Overall, LanguageBind significantly outperforms prior works on two benchmarks validating that it is an efficient way to align audio and language modalities.











\paragraph{Zero-shot multi-modal joint retrieval} In Table~\ref{tab:mm_lang_ret}, we conduct multi-modal joint retrieval to explore the complementarity of joint space. We report the R@1 scores on MSR-VTT and Place datasets, while reporting accuracy on other datasets. For MSR-VTT, we only evaluate using videos that include audio. Integrating audio embeddings for video-language retrieval further improves performance, increasing it from 41.4 to 42.0. Similar trends have been observed in other modalities, where each modality has the potential to enhance the performance when combined with other modalities. These results demonstrate that LanguageBind is capable of learning a more consistent feature space.

\paragraph{Emergent zero-shot retrieval} As shown in Table~\ref{tab:emergent}, we explore the zero-shot performance of emergency coverage in four datasets, including RGB images, audio, infrared, and depth. Due to the novelty of our approach, there are no "fair" baseline models for comparison. Nonetheless, we compare our results with ImageBind, which aligns with images directly. For example, we achieved R@1 scores of 10.6 and 10.0 on AVE~\citep{tian2018audio} and VGGS, respectively. On each benchmark, the performance of emergency zero-shot retrieval achieves significant gains, even approaching results obtained by incorporating textual features. These results suggest that LanguageBind aligns various modalities and implicitly transfers text supervision associated with specific modalities and tasks.

\begin{minipage}{\textwidth}

\begin{minipage}[t]{0.47\textwidth}
\makeatletter\def\@captype{table}

\small
\setlength\tabcolsep{1.5mm}
\caption{\textbf{Comparison of multi-modal language based retrieval.}  donates that it is not clear whether only videos with audio are included.  donates that dark nighttime images.}
\label{tab:mm_lang_ret}
\centering
\begin{tabular}{l|clcc}
\toprule
    \textbf{Dataset}  & \textbf{Method} & \textbf{Task}  & \textbf{Top-1} \\
    \midrule
    \multirow{4}{*}{MSR}  & \multirow{2}{*}{ImageBind} & VT  & 36.1 \\
      &  & A+VT  & 36.8 \tiny{(+0.7)} \\
      \cmidrule(r){3-4}
      & \multirow{2}{*}{Ours} & VT  & 41.4 \\
      &  & A+VT  & 42.0 \tiny{(+0.6)} \\
    \midrule
    \multirow{4}{*}{NYU}  & ImageBind & DT  & 54.0 \\
      \cmidrule(r){3-4}
      & \multirow{3}{*}{Ours} & DT & 65.1 \\
      &  & RGBT  & 76.0 \\
      &  & D+RGBT & 77.4 \tiny{(+1.4)} \\
    \midrule
    \multirow{2}{*}{LLVIP}  & \multirow{2}{*}{Ours} & RGBT & 62.4 \\
      &  & I+RGBT & 79.3 \tiny{(+16.9)} \\
\bottomrule
\end{tabular}






%
 \label{sample-table}
\end{minipage}
\hfill
\begin{minipage}[t]{0.51\textwidth}
\makeatletter\def\@captype{table}











\small
\setlength\tabcolsep{1.0mm}
\caption{\textbf{Comparison of emergent zero-shot retrieval.}  donates that we randomly select 10\% data to test.}
\label{tab:emergent}
\centering
\begin{tabular}{l|clccc}
\toprule
    \textbf{Dataset}  & \textbf{Method} & \textbf{Task} & \textbf{Emergent} & \textbf{R@1} \\
    \midrule
    \multirow{2}{*}{AVE}  & \multirow{1}{*}{Ours} & \multirow{2}{*}{RGBA} & \ding{52} & 10.6 \\
      & \multirow{1}{*}{\textcolor{gray}{ImageBind}} & & \textcolor{gray}{\ding{55}} & \textcolor{gray}{36.9} \\
    \midrule
    \multirow{2}{*}{VGGS}  & \multirow{1}{*}{Ours} & \multirow{2}{*}{RGBA} & \ding{52} & 10.0 \\
      & \multirow{1}{*}{\textcolor{gray}{ImageBind}} &  & \textcolor{gray}{\ding{55}} & \textcolor{gray}{28.7} \\
    \midrule
    \multirow{4}{*}{LLVIP}  & \multirow{4}{*}{Ours} & {RGBI} & \ding{52} & 7.5 \\
      &  & \textcolor{gray}{RGB+TI} & \textcolor{gray}{\ding{55}} & \textcolor{gray}{9.1} \\
      \cmidrule(r){3-5}
      &  & {IRGB} & \ding{52} & 9.3 \\
      &  & \textcolor{gray}{D+IRGB} & \textcolor{gray}{\ding{55}} & \textcolor{gray}{10.6} \\
    \midrule
    \multirow{4}{*}{NYU}  & \multirow{4}{*}{Ours} & {RGBD} & \ding{52} & 17.9 \\
      &  & \textcolor{gray}{RGB+TD} & \textcolor{gray}{\ding{55}} & \textcolor{gray}{18.3} \\
      \cmidrule(r){3-5}
      &  & {DRGB} & \ding{52} & 24.5 \\
      &  & \textcolor{gray}{D+TRGB} & \textcolor{gray}{\ding{55}} & \textcolor{gray}{25.7} \\
\bottomrule
\end{tabular}







%
 \label{sample-table}
\end{minipage}
\end{minipage}




\subsection{Training Loss and Architecture} 



\begin{table*}[h]
\caption{\textbf{Training loss and architecture} design decisions and their impact on zero-shot classification. Settings for results in Section~\ref{sec:result_inf_dep} highlighted in \colorbox{Gray}{gray}.}
\label{tab:ablate_hyperparameters}
\centering
\small
\setlength\tabcolsep{1.2mm}
    \subfloat[
	\label{tab:epoch}
	\textbf{Training epochs}
	]{
		\centering
		\begin{minipage}{0.3\linewidth}{\begin{center}
                    \begin{tabular}{lcc}
                        \toprule
                        Dataset & \colorbox{Gray}{1} & 5 \\ 
                        \midrule
                        NYU-D & \textbf{65.1} & 64.5 \\ 
                        LLVIP & \textbf{83.9} & 81.1 \\ 
                        FLIR V1 & 82.9 & \textbf{85.0} \\ 
                        FLIR V2 & \textbf{48.0} & 44.7 \\ 
                        \bottomrule
                    \end{tabular}
		\end{center}}\end{minipage}
	}
    \subfloat[
	\textbf{Training batch size}
	\label{tab:bs}
	]{
		\begin{minipage}{0.3\linewidth}{\begin{center}
                    \begin{tabular}{lccc}
                        \toprule
                        Dataset & 512 & \colorbox{Gray}{1k} & 2k \\ 
                        \midrule
                        NYU-D & 63.9 & \textbf{65.1} & 64.5 \\ 
                        LLVIP & 80.0 & \textbf{83.9} & 78.6 \\ 
                        FLIR V1 & 81.6 & 82.9 & \textbf{85.2} \\ 
                        FLIR V2 & 45.1 & \textbf{48.0} & 47.9 \\ 
                        \bottomrule
                    \end{tabular}
		\end{center}}\end{minipage}
	}
    \subfloat[
	\label{tab:init}
	\textbf{Training strategy}
	]{
		\centering
		\begin{minipage}{0.4\linewidth}{\begin{center}
                    \begin{tabular}{lccc}
                        \toprule
                         & Scratch & Full tuning & \colorbox{Gray}{LoRA} \\ 
                        \midrule
                        Time & 1.4h & 1.4h & \textbf{0.8h} \\ 
                        Mems & 278M & 278M & \textbf{132M} \\ 
LLVIP & 57.1 & \textbf{85.1} & 84.8 \\ 
                        FLIR V1 & 74.7 & 81.3 & \textbf{81.6} \\ 
                        FLIR V2 & 54.4 & 41.9 & \textbf{46.6} \\ 
                        ESC-50 & 86.8 & \textbf{88.9} & 87.4 \\ 
                        Clotho & 8.8 & 9.8 & \textbf{10.1} \\ 
                        \bottomrule
                    \end{tabular}
		\end{center}}\end{minipage}
	}
 
    \subfloat[
	\textbf{Rank of LoRA}
	\label{tab:LoRA}
	]{
		\begin{minipage}{0.3\linewidth}{\begin{center}
                    \begin{tabular}{lccc}
                        \toprule
                        Dataset & \colorbox{Gray}{2} & 4 & 8 \\ 
                        \midrule
                        NYU-D & \textbf{65.1} & 64.4 & 64.7 \\ 
                        LLVIP & \textbf{83.9} & 78.0 & - \\ 
                        FLIR V1 & \textbf{82.9} & 74.4 & - \\ 
                        FLIR V2 & \textbf{48.0} & 45.8 & - \\ 
                        \bottomrule
                    \end{tabular}
		\end{center}}\end{minipage}
	}
    \subfloat[
	\textbf{Temperature for loss}
	\label{tab:temp}
	]{
		\begin{minipage}{0.3\linewidth}{\begin{center}
                    \begin{tabular}{lccc}
                        \toprule
                        Dataset & \colorbox{Gray}{Learn} & 0.05 & 0.1 \\ 
                        \midrule
                        NYU-D & \textbf{65.1} & 63.0 & 62.7 \\ 
                        LLVIP & \textbf{83.9} & 81.8 & 83.1 \\ 
                        FLIR V1 & 82.9 & \textbf{83.3} & 80.3 \\ 
                        FLIR V2 & \textbf{48.0} & 45.0 & 43.2 \\ 
                        \bottomrule
                    \end{tabular}
		\end{center}}\end{minipage}
	}
    \subfloat[
	\textbf{Masked ratio}
	\label{tab:mask}
	]{
		\begin{minipage}{0.4\linewidth}{\begin{center}
                    \begin{tabular}{lcccc}
                        \toprule
                        Dataset & 0.0 & 0.3 & \colorbox{Gray}{0.5} & 0.7 \\ 
                        \midrule
                        NYU-D & - & 64.8 & \textbf{65.1} & 62.7 \\ 
                        LLVIP & 80.3 & 79.9 & \textbf{83.9} & 81.5 \\ 
                        FLIR V1 & 83.5 & \textbf{84.2} & 82.9 & 81.9 \\ 
                        FLIR V2 & 43.2 & 44.0 & \textbf{48.0} & 42.5 \\ 
                        \bottomrule
                    \end{tabular}
		\end{center}}\end{minipage}
	}
 
\end{table*}


Following ImageBind, we mainly focus on depth and infrared, which are visual and spatial modality. We report R@1 score for Clotho and Audiocaps datasets and top-1 accuracy for others.\\ \\
\textbf{Training epochs.} We conduct an experiment in Table~\ref{tab:epoch} to study the effect of training epochs which shows that the LoRA fine-tuning is highly effective. Although 3 epochs of training regimen yield superior accuracy, we chose to optimize for a single epoch, achieving a balance between performance and training cost.\\
\textbf{Training batch size.} In Table~\ref{tab:bs}, we evaluate the effect of batch size on representation learning. The experiments have shown that a larger batch size is not necessarily better. In fact, a batch size of 1,024 is the most optimal.\\
\textbf{Training strategy.} As indicated by Table~\ref{tab:init}, we compare three different strategies. Training from scratch exhibits the poorest performance, likely due to the lack of prior knowledge from CLIP pretraining. On the other hand, full tuning shows significant improvement compared to training from scratch. This highlights the positive impact of leveraging prior knowledge in the form of pre-trained weights. Meanwhile, the LoRA method stands out for its advantages in terms of time and memory cost. It requires less time and memory resources compared to full tuning. Furthermore, LoRA outperforms full tuning on multiple datasets such as LLVIP, FLIRv1, and Clotho. This indicates that LoRA is not only efficient but also effective in learning new knowledge specific to different domains while better retaining the previously acquired knowledge from the pre-trained OpenCLIP models.\\
\textbf{Rank of LoRA.} In our investigation, we examined prevalent rank configurations for LoRA, as detailed in Table~\ref{tab:LoRA}. We observe that smaller rank values lead to more significant performance improvements, whereas larger rank tends to decrease performance. This trend may be attributed to the potential overfitting of the model.\\
\textbf{Temperature for loss.} We scrutinize the impact of diverse temperature in Table~\ref{tab:temp}. We find that the learnable temperature initiated from 0.07 performs best, outperforming the fixed temperature strategy proposed by ImageBind.\\
\textbf{Masked ratio.} We explore the impact of different mask ratios in Table~\ref{tab:mask}. The results show that a mask ratio of 0.5 demonstrates the highest performance, requiring only a quarter of the computational resources, aligning with findings in FLIP \citep{li2023scaling}.




\section{Conclusion}
In this work, we propose the LanguageBind, a language-based semantic alignment method for multi-modal pretraining. We employ contrastive learning to establish modality semantic alignment between the language modality and all other modalities. To improve modal integrity, we also construct the first large-scale multi-modal dataset directly aligned to language modality, VIDAL-10M, comprising 10 million aligned VL, IL, DL, and AL pairs. Experimental results on the zero-shot VL, IL, DL, and AL understanding tasks demonstrate the capability of LanguageBind and the effectiveness of VIDAL-10M.





\section*{Reproducibility Statement}
\begin{enumerate}

\item  For LanguageBind approach details.
\begin{enumerate}
    \item  We provide a comprehensive overview of the multi-modal encoder, detailing its architecture and functionality in Section~\ref{section: Multi-modal Encoders}. \item  We outline the language encoder in Section~\ref{section: Language Encoder and Multi-modal Joint Learning}.
    \item We expound on the methodologies employed for multi-modal joint learning in Section~\ref{section: Language Encoder and Multi-modal Joint Learning}
\end{enumerate}

\item  For VIDAL-10M dataset construction details.
\begin{enumerate}
    \item  We describe the procedures employed to construct the search term database in Section~\ref{subsec:Visual search term database construction}. \item  We provide insights into the strategies used for collecting and filtering video and audio data within VIDAL-10M in Section~\ref{subsec: Video collection and filtering}.
    \item We elaborate on the generation of infrared and depth data, as well as the processes involved in multi-view text generation and enhancement in Section~\ref{subsec:Modality generation and enhancement}
    \item  We promise to release the VIDAL-10M dataset upon publication.
\end{enumerate}

\item  For setting details.
\begin{enumerate}
    \item  We describe in detail the training hyperparameters in Appendix~\ref{appendix:pretraining_details}. \item  We describe the setup of the downstream task dataset Appendix~\ref{appendix:downstream_data_details}. \end{enumerate}


\end{enumerate}














\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}

\clearpage































\appendix

\section*{Appendix}
\section{Statistics of VIDAL-10M Dataset}

\begin{figure*}[htbp]

\centering
    \includegraphics[width=1.0\textwidth]{images/iclr_dataset_sample.pdf}
    \caption{Examples of video-audio-text-depth-infrared pairs in VIDAL-10M, with the text components comprising hashtags, title, keyframe captions, and video caption. Examples are taken from 4 distinct clusters, corresponding to Sports, Pets \& Animals, News \& Politics, and Education.}
    \label{fig:overview}
\vspace{-0.4cm} 
\label{fig:samples of VIDAL-10M}
\end{figure*}

\begin{table*}[b]
\centering
\setlength\tabcolsep{1mm}
\renewcommand\arraystretch{1.1}
\caption{\label{table:text_data}Examples of textual descriptions from various datasets as search terms.}
\begin{tabular}{cllll}
\toprule

\multicolumn{1}{c}{\textbf{Dataset}}  & \multicolumn{1}{c}{\textbf{Search terms}}   \\ 

\toprule



\multicolumn{1}{c}{\multirow{2}{*}{YouTube-8M}}   & \multicolumn{4}{l}{How to make a delicious chocolate cake.} \\ 
\multicolumn{1}{c}{}                          & \multicolumn{4}{l}{Learn to dance salsa in 10 easy steps.}      \\
\multicolumn{1}{c}{}  & \multicolumn{4}{l}{\dots\dots}    \\ \hline

\multicolumn{1}{c}{\multirow{2}{*}{Howto100M}}   & \multicolumn{4}{l}{How to play chess.} \\ 
\multicolumn{1}{c}{}                          & \multicolumn{4}{l}{How to make pizza.}      \\
\multicolumn{1}{c}{}                  & \multicolumn{4}{l}{\dots\dots}    \\ \hline

\multicolumn{1}{c}{\multirow{2}{*}{ImageNet}} & \multicolumn{4}{l}{lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens, coon bear}                       \\ 
\multicolumn{1}{c}{}                          & \multicolumn{4}{l}{killer whale, killer, grampus, sea wolf, Orcinus orca, giant panda, panda, panda bear}                               \\ 
\multicolumn{1}{c}{}                  & \multicolumn{4}{l}{\dots\dots}    \\ \hline


\multicolumn{1}{c}{\multirow{2}{*}{COCO}}     & \multicolumn{4}{l}{ A small boat floating on a body of water with a city skyline in the background.}                                                \\ 
\multicolumn{1}{c}{}                          & \multicolumn{4}{l}{A man with a red helmet on a small moped on a dirt road.}           \\ 
\multicolumn{1}{c}{}                          & \multicolumn{4}{l}{\dots\dots}                               \\ \hline



\multicolumn{1}{c}{\multirow{1}{*}{Others}}     & \multicolumn{4}{l}{\dots\dots}                                                \\ \hline


\toprule
\end{tabular}
\vspace{-0.4cm}
\end{table*}



In order to build a video dataset with rich visual concepts and diversity, we develop a unique but simple search term acquisition strategy. This strategy involves obtaining search terms from various visual datasets (as shown in Table~\ref{table:text_data}). Subsequently, we use these search terms to gather videos from the YouTube Shorts platform, which has become a popular source for video data due to its abundance and diverse content. We collect videos in various categories, including sports, animals, nature, etc., resulting in a large and diverse dataset. Examples of video-audio-text-depth-infrared pairs in the VIDAL-10M dataset are shown in Figure~\ref{fig:samples of VIDAL-10M}. Moreover, to ensure data quality, we manually design a list of stop words that are filtered from our datasets. These words include terms such as "bts", "bmw", and "nfl", among others, that are not relevant to our research.

\begin{table}[htbp]
\centering
\setlength\tabcolsep{1mm}
\renewcommand\arraystretch{1.1}
    \caption{Stop words in our datasets.}
    \label{tab:filter}
    \begin{tabular}{|c|c|c|c|c|c|}
    \hline


viral & funny & love & fashion & subscribe & nature \\
\hline
asmr & motivation & fitness & art & satisfying & foryou \\
\hline
music & india & fun & bts & amazing & edit \\
\hline
life & roblox & vlog & minecraft & design & marvel \\
\hline
explore & dubai & foryoupage & comedy & outfit & ootd \\
\hline
share & indian & lol & creative & relaxing & tattoo \\
\hline
random & instagram & quotes & workout & sad & ideas \\
\hline
views & bgmi & yummy & respect & easy & usa \\
\hline
ronaldo & jawellery & memes & happy & nfl & song \\
\hline
mlb & reel & support & nba & wow & status \\
\hline
gree & meme & gameplay & top & blackpink & whatsappstatus \\
\hline
follow & homedecor & history & tutorial & bodybuilding & japan \\
\hline
interiordesign & freefire & stunt & foodie & animation & recipe \\
\hline
skills & tips & crazy & pov & editing & aesthetic \\
\hline
style & view & london & reaction & story & pubg \\
\hline
construction & challenge & healthy & bmw & uk & free \\
\hline
hairstyle & enjoy & motivational & messi & capcut & nailart \\
\hline
entertainment & fifa & attitude & europe & health & geography \\
\hline
gta & unboxing & adventure & whatsapp & fail & btsarny \\
\hline
god & inspiration & relatable & comment & tattoos & fy \\
\hline
highlights & amazon & illustration & fortnite & ntb & avaiation \\
\hline
interior & decor & travelvlog & canada & btsarmy & tranding \\
\hline
time & mtb & luxury & vlogs & picsart & reels \\
\hline
photoshoot & business & photography & ... & ... & ... \\
\hline


    \end{tabular}
\vspace{-0.4cm}
\end{table}

\paragraph{Video categories and duration}
Furthermore, we analyze the distribution of video categories with varying durations in our datasets, as illustrated in Figure~\ref{fig:cate_dist}. The normal distribution pattern observed in this analysis indicates that our dataset covers a wide range of concepts. Besides, we show the proportions of each category across different duration grades in the VIDAL-10M dataset in Figure~\ref{fig:dur-category}.



\begin{figure*}[htpb]\centerline{\includegraphics[width=1.0\textwidth]{images/text_gen.pdf}}
    \caption{\textbf{Multi-view text generation and enhancement pipline}. 
    We employ the OFA model to generate keyframe captions and input video, title and hashtags into the mPLUG-owl model to obtain video captions. The video captions are further refined using ChatGPT, resulting in the ChatGPT-mPLUG caption. The final multi-view textual description comprises these components. 
}  
    \label{fig:multi-view text generation and enhancement}
\end{figure*}





\begin{figure}

\centerline{\includegraphics[width=1\textwidth]{appendix/cate15_dist_data.pdf}}
  \caption{The number of 15 categories with different durations in our VIDAL-10M datasets. A wide range of concepts are covered.}  
  \label{fig:cate_dist}
\vspace{-1cm} 
\end{figure}

\begin{figure}[H]
  \centerline{\includegraphics[width=0.9\textwidth]{appendix/iclr-dur-category.pdf}}
  \caption{The statistical distribution of categories across the three duration grades in the VIDAL-10M dataset. The colors green, blue, and yellow represent video durations of 1-7, 8-14, and 15-20 s, respectively.}  
  \label{fig:dur-category}
\vspace{-0.4cm} 
\end{figure}


\paragraph{FPS, Aspect ratio and Resolution}

The first aspect examined in the dataset is the Frames Per Second (FPS) of the videos. FPS refers to the number of frames or images displayed per second in a video. The aspect ratio of a video represents the proportional relationship between its width and height dimensions. It is a critical factor in determining the visual presentation and viewing experience of the videos. The distribution of FPS and aspect ratios in Figure~\ref{fig:aspectratio_fps} provides insights into the smoothness and fluidity of the recorded content and sheds light on the various formats and orientations used. Video resolution refers to the number of pixels in each dimension that a video contains. It directly affects the clarity, sharpness, and level of detail in the visual content. Examining the distribution of resolutions (Figure~\ref{fig:height_width}) in the dataset provides an understanding of the available video quality and the technological capabilities of the recorded material.
\begin{figure}[htpb]
\centerline{\includegraphics[width=1\textwidth]{appendix/aspectratio_fps3.pdf}}
  \caption{The distribution of FPS (Frames Per Second) and aspect ratio in the videos of the VIDAL-10M dataset.}  
  \label{fig:aspectratio_fps}
\vspace{-0cm} 
\end{figure}

\begin{figure}[htpb]
\centerline{\includegraphics[width=0.75\textwidth]{appendix/height_width.pdf}}
  \caption{Height and width distribution of videos in VIDAL-10M dataset.}  
  \label{fig:height_width}
\vspace{-0cm} 
\end{figure}


\section{Pretraining details}
\label{appendix:pretraining_details}
In this section, we introduce our training configuration. 


\begin{table}[htbp]
\setlength\tabcolsep{2mm}
    \small
    \captionof{table}{Training setting.}
    \label{tab:setting}
    \centering
        \begin{tabular}{l|c|cccc}
             & CLIP4Clip & \multicolumn{4}{c}{LanguageBind} \\
            \midrule
             \multirow{1}{*}{Config} & Video & Video & Infrared & Depth & Audio \\
            \midrule
            Vision encoder & ViT-Base/32 & \multicolumn{4}{c}{ViT-Large/14} \\
            Optimizer & BertAdam& \multicolumn{4}{c}{AdamW} \\
            Optimizer Momentum &  & \multicolumn{4}{c}{} \\
            Epochs & 1 & 16 & 1 & 1 & 8 \\
            Learning rate & 1e-4 & 1e-4 & 1e-4 & 5e-4 & 5e-4 \\
            Coefficient learning rate & 1e-3 & 1 & 1e-3 & 1e-3 & 1e-3 \\
            Weight decay & 0.2 & \multicolumn{4}{c}{0.2} \\
            Batch size & 512 & 640 & 1024 & 1024 & 512 \\
            Warmup steps & 0 & 2000 & 200 & 200 & 2000 \\
            Temperature & learnable & \multicolumn{4}{c}{learnable} \\
            Learning rate schedule & cosine decay & \multicolumn{4}{c}{cosine decay} \\
Max words & 32 & \multicolumn{4}{c}{77} \\
            Max frames & 12 & 8 & - & - & - \\
Mask ratio & - & 0.3 & 0.5 & 0.5 & 0.3 \\
            LoRA rank & - & 16 & 2 & 2 & 16 \\
            LoRA alpha & - & \multicolumn{4}{c}{16} \\
            LoRA dropout & - & 0.0 & 0.1 & 0.1 & 0.1 \\
        \end{tabular}
\end{table}


\paragraph{Video-Language.} For the video-text retrieval based CLIP4Clip, we verify that the \textit{VIDAL-10M} dataset is highly aligned. We adopted the training framework of CLIP4Clip, and the model is initialized from ViT-B/32, and the rest of the parameters are the same as the default settings, except for 1 epoch and batch size of 512. For the video-text retrieval based LanguageBind, we add a temporal attention before each spatial attention following Aim~\citep{yang2023aim}. The temporal attention is initialized from the spatial attention and LoRA is applied only to the temporal attention. We add temporal position embedding before each temporal attention. We show the details of results as shown in Table~\ref{tab:retrieval_lb_details}. For zero-shot video classification, The text templates are sourced from OpenCLIP, with a modification consisting of the substitution of ``photo" with ``video" across all templates.


\paragraph{Depth-Language.} The model is initialized from OpenCLIP with a frozen language encoder. For each individual sample, we employ a random selection approach to extract either a depth image from the video sequence. Subsequently, we resize these frames to have a short edge length of 256 units, followed by a central cropping process to attain dimensions of 224224. Additionally, we tripled the number of channels in both the depth image. The text templates employed for zero-shot classification are sourced from OpenCLIP, with a modification consisting of the substitution of ``photo" with ``depth photo" across all templates. This alteration yields an approximate performance gain of 1\%. 

\paragraph{Infrared-Language.} Following depth-language, it is worth noting that the text templates corresponding to infrared images retain the ``photo" designation, as no discernible performance improvement is observed from this particular modification.
\paragraph{Audio-Language.} The data are preprocessed as in \ref{section: Multi-modal Encoders}. Unlike depth and infrared, spectrograms differ much from the domain of conventional visual images. Therefore, it is not easy to overfit during training, so we increase the training epoch and the rank of LoRA. Additionally, we replace ``the/a photo of" with ``the/a sound of" across all templates for audio zero-shot classification. 

\begin{table*}[htbp]
\small
\caption{ \textbf{Zero-shot Video-Text Retrieval Performance based LanguageBind (LoRA).} We show the details of results.}
\label{tab:retrieval_lb_details}
\centering
\setlength\tabcolsep{1.5mm}
\begin{tabular}{c|cccc|cccc}
        \toprule
         & \multicolumn{4}{c}{Text-to-Video} & \multicolumn{4}{c}{Video-to-Text} \\
        \textbf{Dataset}  &\textbf{R@1} & \textbf{R@5} & \textbf{R@10} & \textbf{MR}  &\textbf{R@1} & \textbf{R@5} & \textbf{R@10} & \textbf{MR} \\
        \midrule
        MSR-VTT  & 42.6 & 65.4 & 75.5 & 2.0 & 37.9 & 63.1 & 73.3 & 3.0 \\  
        MSVD  &  52.2 & 79.4 & 87.3 & 1.0 & 68.4 & 91.7 & 96.4 &  1.0\\  
        ActivityNet  & 35.1  & 63.4 & 76.6 & 3.0 & 32.3 & 62.2 & 74.5 & 3.0 \\  
        DiDeMo  &  37.8 & 63.2 & 73.4 & 3.0 & 37.6 & 63.7 & 73.3 &  3.0 \\  
      \bottomrule
      \end{tabular}
\end{table*} 

\begin{table*}[htbp]
\small
\caption{ \textbf{Zero-shot Video-Text Retrieval Performance based LanguageBind (full tuning).} We show the details of results.}
\label{tab:retrieval_lb_details}
\centering
\setlength\tabcolsep{1.5mm}
\begin{tabular}{c|cccc|cccc}
        \toprule
         & \multicolumn{4}{c}{Text-to-Video} & \multicolumn{4}{c}{Video-to-Text} \\
        \textbf{Dataset}  &\textbf{R@1} & \textbf{R@5} & \textbf{R@10} & \textbf{MR}  &\textbf{R@1} & \textbf{R@5} & \textbf{R@10} & \textbf{MR} \\
        \midrule
        MSR-VTT  & 42.7 & 67.1 & 77.0 & 2.0 & 39.7 & 63.9 & 73.8 & 3.0 \\  
        MSVD  & 53.5 & 80.5 & 87.5 & 1.0 & 68.1 & 89.5 & 96.0 & 1.0 \\ 
        ActivityNet  & 36.9 & 65.1 & 77.2 & 3.0 & 33.8 & 64.0 & 76.1 & 3.0 \\ 
        DiDeMo  & 38.1 & 65.0 & 73.6 & 2.0 & 38.4 & 63.0 & 72.6 & 3.0 \\ 
      \bottomrule
      \end{tabular}
\end{table*} 












\section{Downstream datasets}
\label{appendix:downstream_data_details}

\paragraph{Video-language.} We perform video-text retrieval experiments on 2 datasets. \textbf{(a) MSR-VTT} \citep{xu2016msr} comprises 10K YouTube videos, each paired by 200K captions. In our analysis, we present results based on the 1K-A test subset. \textbf{(b) MSVD} \citep{chen2011collecting} consists of about 120K sentences and reports results on test data (670 samples).
\paragraph{Infrared-language.} \textbf{(a) LLVIP} \citep{jia2021llvip} constitutes a dataset for pedestrian object detection within the infrared spectrum. Following ImageBind, we extracted all people from the images, designating all other objects as background elements. This process resulted in a dataset comprising 7,622 `background' classes and 7,954 `person' classes, which was subsequently employed for binary classification testing. \textbf{(b) FLIR v1} \citep{flirv1} offers comprehensive annotations for both thermal and visible spectrum frames. From the test data, we derived a dataset containing 11,696 images by extracting bounding boxes. This dataset encompasses 4 categories -- ['bicycle', 'car', 'dog', 'person']. \textbf{(c) FLIR v2} \citep{flirv2} includes 16,696 images after processing similarly, which were categorized into 12 classes -- ['bike', 'bus', 'car', 'hydrant', 'light', 'motor', 'other vehicle', 'person', 'sign', 'skateboard', 'stroller', 'truck'].
\paragraph{Depth-language.}  We use \textbf{NYU-v2 Depth-only (NYU-D)} \citep{silberman2012indoor} to validate by 654 test samples. Through preprocessing, we constrained the depth images to a maximum depth of 10 meters. Following ImageBind, we undertook a category reorganization process, resulting in a total of 10 scene categories.
\paragraph{Audio-language.} We validate the zero-shot classification capability with the \textbf{ESC-50} \citep{piczak2015esc} dataset, which has 2000 test audios, each uniquely labelled. For zero-shot retrieval, we use the \textbf{Clotho} \citep{font2013freesound} dataset. Each audio has 5 corresponding captions, so we use text-to-audio retrieval to validate the model performance. We perpare test data following ImageBind.


\section{License}
Unless explicitly noted otherwise, our released datasets are provided to users under the terms of the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License ("CC BY-NC-SA 4.0"), in conjunction with the additional terms outlined herein. The CC BY-NC-SA 4.0 license can be accessed at \url{https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode}. By downloading or utilizing our datasets from our website or other sources, you agree to adhere to the terms of CC BY-NC-SA 4.0, as well as the terms outlined in our dataset Terms. In the event of any conflict between the terms of CC BY-NC-SA 4.0 and our dataset Terms, the latter shall prevail.
We once again emphasize that this dataset is exclusively intended for non-commercial purposes, such as academic research, teaching, or scientific publications. We strictly prohibit any commercial use of the dataset or any derived works, including the sale of data or utilization of data for commercial gain.











\section{Ablation Study}
In this section, we conduct extensive experiments to investigate the impact of several factors. At first, we examine the effects of different enhanced textual inputs on downstream tasks.
Furthermore, we assess the impact of data volumes on pretraining. In addition, we explore various training strategies to enhance zero-shot classification. Finally, we conduct a meticulous analysis of model training configurations to ensure robust transferability.

\subsection{Impact of different text sources}

In Table~\ref{tab:inf_dep_text}, we conduct various experiments to explore how different text sources impact language modality. We verify the effectiveness of LanguageBind, trained with text from multiple sources, across various modalities. While some text sources yield good results, we discover that a single text source may not be universally suitable for all downstream tasks and datasets. In terms of video and depth modalities, the ChatGPT enhanced caption proves to be advantageous. For infrared images, the OFA performs best in the LLVIP dataset, while the raw caption achieves the highest accuracy in FLIR v1 and v2. That's why our VIDAL-10M provides multi-view textual descriptions, allowing for flexibility in selecting an appropriate text source that caters to diverse task requirements.






\begin{table*}[h]
    \small
    \captionof{table}{\textbf{Impact of different text sources.} We report the results of text-to-video R@1 for zero-shot retrieval and other datasets report top-1 accuracy. MSR-VTT results were tested on a 500K subset of VIDAL-10M. ``\textit{Raw} caption'' denotes the title \& hashtags.}
    \label{tab:inf_dep_text}
    \centering
        \begin{tabular}{c|c|cccc}
            \toprule
            Modality & Dataset & \textit{Raw} caption & \textit{OFA} caption & \textit{mPLUG} caption & \textit{ChatGPT-mPLUG} caption \\ 
            \midrule
            Video & MSR-VTT & 33.5 & 34.5 & 35.8 & \textbf{36.4} \\
            \midrule
            \multirow{3}{*}{Infrared} & LLVIP & 83.9 & \textbf{87.2} & 84.6 & 84.8 \\ 
            & FLIR V1 & \textbf{82.9} & 80.6 & 81.4 & 81.6 \\ 
            & FLIR V2 & \textbf{48.0} & 45.7 & 46.8 & 46.6 \\ 
            \midrule
            Depth & NYU-D & 61.5 & 62.1 & 63.9 & \textbf{65.1} \\ 
            \bottomrule
        \end{tabular}
\end{table*}



\subsection{Scaling the size of dataset}



\begin{wrapfigure}{r}{0.4\textwidth} 
\vspace{-0.2cm} 
\centering
    \includegraphics[width=0.4\columnwidth]{images/scaledata.pdf} 
    \caption{Scaling pretraining data size.} \label{fig:scaledata}
\vspace{-0.4cm} 
\end{wrapfigure}
We analyze the impact of different data amounts on MSR-VTT and report the R@1 score for zero-shot retrieval as shown in Figure~\ref{fig:scaledata}. Our findings indicate that an increase in data amount leads to significant improvement in recognition performance. 
Specifically, the performance of 3M ChatGPT-enhanced text surpasses that of 500k and 100k data by 0.9\% and 1.6\%, respectively. 

Furthermore, the trends observed in both video-to-text retrieval and text-to-video retrieval consistently demonstrate that the interaction between modalities plays a pivotal role in enhancing the learning process. Consequently, with the expansion of data size, the textual descriptions within the VIDAL-10M dataset align more closely with the video content and demonstrate increased scalability.
























\subsection{Training Loss and Architecture} 
Following ImageBind, we mainly focus on depth and infrared, which are visual and spatial modality. We report R@1 score for Clotho and Audiocaps datasets and top-1 accuracy for others.
\paragraph{Training epochs} 
We conduct an experiment in Table~\ref{tab:epoch} to study the effect of training epochs which shows that the LoRA fine-tuning is highly effective. Although 3 epochs of training regimen yield superior accuracy, we chose to optimize for a single epoch, achieving a balance between performance and training cost.

\paragraph{Training batch size} In Table~\ref{tab:bs}, we evaluate the effect of batch size on representation learning. The experiments have shown that a larger batch size is not necessarily better. In fact, a batch size of 1,024 is the most optimal.

\paragraph{Model initialization} As shown in Table~\ref{tab:init}, we compare three different initialization strategies. Training with LoRA is cheaper and more efficient compared to full fine-tuning or training from scratch. On average, LoRA require 132M memory per sample, while the other two methods require 278M, meaning LoRA only need half the GPU resources. Additionally, LoRA demonstrate competitive results compared to full fine-tuning, with an average epoch time that is only 57\% of the time required for full fine-tuning. Therefore, we adopt the efficient approach of using LoRA.

\paragraph{Rank of LoRA} In our investigation, we examined prevalent rank configurations for LoRA, as detailed in Table~\ref{tab:LoRA}. We observe that smaller rank values lead to more significant performance improvements, whereas larger rank tends to decrease performance. This trend may be attributed to the potential overfitting of the model.

\paragraph{Temperature for loss} We scrutinize the impact of diverse temperature in Table~\ref{tab:temp}. We find that the learnable temperature initiated from 0.07 performs best, outperforming the fixed temperature strategy proposed by ImageBind.

\paragraph{Masked ratio} We explore the impact of different mask ratios in Table~\ref{tab:mask}. The results show that a mask ratio of 0.5 demonstrates the highest performance, requiring only a quarter of the computational resources, aligning with findings in FLIP \citep{li2023scaling}.


\begin{table*}[h]
\caption{\textbf{Training loss and architecture} design decisions and their impact on zero-shot classification. Settings for results in Section~\ref{sec:result_inf_dep} highlighted in \colorbox{Gray}{gray}.}
\label{tab:ablate_hyperparameters}
\centering
\small
\setlength\tabcolsep{1.2mm}
    \subfloat[
	\label{tab:epoch}
	\textbf{Training epochs}
	]{
		\centering
		\begin{minipage}{0.3\linewidth}{\begin{center}
                    \begin{tabular}{lcc}
                        \toprule
                        Dataset & \colorbox{Gray}{1} & 5 \\ 
                        \midrule
                        NYU-D & \textbf{65.1} & 64.5 \\ 
                        LLVIP & \textbf{83.9} & 81.1 \\ 
                        FLIR V1 & 82.9 & \textbf{85.0} \\ 
                        FLIR V2 & \textbf{48.0} & 44.7 \\ 
                        \bottomrule
                    \end{tabular}
		\end{center}}\end{minipage}
	}
    \subfloat[
	\textbf{Training batch size}
	\label{tab:bs}
	]{
		\begin{minipage}{0.3\linewidth}{\begin{center}
                    \begin{tabular}{lccc}
                        \toprule
                        Dataset & 512 & \colorbox{Gray}{1k} & 2k \\ 
                        \midrule
                        NYU-D & 63.9 & \textbf{65.1} & 64.5 \\ 
                        LLVIP & 80.0 & \textbf{83.9} & 78.6 \\ 
                        FLIR V1 & 81.6 & 82.9 & \textbf{85.2} \\ 
                        FLIR V2 & 45.1 & \textbf{48.0} & 47.9 \\ 
                        \bottomrule
                    \end{tabular}
		\end{center}}\end{minipage}
	}
    \subfloat[
	\label{tab:init}
	\textbf{Model initialization}
	]{
		\centering
		\begin{minipage}{0.4\linewidth}{\begin{center}
                    \begin{tabular}{lccc}
                        \toprule
                         & Scratch & Full tuning & \colorbox{Gray}{LoRA} \\ 
                        \midrule
                        Time & 1.4h & 1.4h & \textbf{0.8h} \\ 
                        Mems & 278M & 278M & \textbf{132M} \\ 
                        NYU-D &  & \textbf{} & 65.1 \\ 
                        LLVIP & 74.7 & \textbf{84.6} & 83.9 \\ 
\bottomrule
                    \end{tabular}
		\end{center}}\end{minipage}
	}
 
    \subfloat[
	\textbf{Rank of LoRA}
	\label{tab:LoRA}
	]{
		\begin{minipage}{0.3\linewidth}{\begin{center}
                    \begin{tabular}{lccc}
                        \toprule
                        Dataset & \colorbox{Gray}{2} & 4 & 8 \\ 
                        \midrule
                        NYU-D & \textbf{65.1} & 64.4 & 64.7 \\ 
                        LLVIP & \textbf{83.9} & 78.0 & - \\ 
                        FLIR V1 & \textbf{82.9} & 74.4 & - \\ 
                        FLIR V2 & \textbf{48.0} & 45.8 & - \\ 
                        \bottomrule
                    \end{tabular}
		\end{center}}\end{minipage}
	}
    \subfloat[
	\textbf{Temperature for loss}
	\label{tab:temp}
	]{
		\begin{minipage}{0.3\linewidth}{\begin{center}
                    \begin{tabular}{lccc}
                        \toprule
                        Dataset & \colorbox{Gray}{Learn} & 0.05 & 0.1 \\ 
                        \midrule
                        NYU-D & \textbf{65.1} & 63.0 & 62.7 \\ 
                        LLVIP & \textbf{83.9} & 81.8 & 83.1 \\ 
                        FLIR V1 & 82.9 & \textbf{83.3} & 80.3 \\ 
                        FLIR V2 & \textbf{48.0} & 45.0 & 43.2 \\ 
                        \bottomrule
                    \end{tabular}
		\end{center}}\end{minipage}
	}
    \subfloat[
	\textbf{Masked ratio}
	\label{tab:mask}
	]{
		\begin{minipage}{0.4\linewidth}{\begin{center}
                    \begin{tabular}{lcccc}
                        \toprule
                        Dataset & 0.0 & 0.3 & \colorbox{Gray}{0.5} & 0.7 \\ 
                        \midrule
                        NYU-D & - & 64.8 & \textbf{65.1} & 62.7 \\ 
                        LLVIP & 80.3 & 79.9 & \textbf{83.9} & 81.5 \\ 
                        FLIR V1 & 83.5 & \textbf{84.2} & 82.9 & 81.9 \\ 
                        FLIR V2 & 43.2 & 44.0 & \textbf{48.0} & 42.5 \\ 
                        \bottomrule
                    \end{tabular}
		\end{center}}\end{minipage}
	}
 
\end{table*}

\end{document}
