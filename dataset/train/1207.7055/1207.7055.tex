\subsection{Relaxing barriers}
\label{subsec:barriers}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\columnwidth]{gnuplot/new/benefit_of_pipelining.eps}
  \caption{Predicted normalized makespan for optimized plans with various
  barrier configurations and different values of $\alpha$.
  All makespans are normalized relative to the optimal makespan for an
  all-global-barrier configuration.  Each bar represents the phase boundary at
  which the global barrier is relaxed to pipelining, with ``all'' corresponding
  to an all-pipelining configuration.
  \label{fig:benefit_of_pipelining}}
\end{figure}

Now we study the impact of relaxing barriers on the makespan predicted by our
model.
In particular, we focus on the impact of using pipelining vs. global barriers
at each phase boundary.
In Figure~\ref{fig:benefit_of_pipelining}, we show the normalized makespan for
a select set of different barrier configurations.
All predicted makespan values shown in the figure are normalized relative to
the optimal makespan derived for an all-global-barrier configuration, i.e., one
which has a global barrier at each phase boundary.
The bars in the figure show the effect of relaxing only a single global barrier
to pipelining at a time, at the push/map, the map/shuffle, and the
shuffle/reduce boundaries respectively, as well as  the all-pipelined
configuration, where all barriers are pipelined. We make two key observations.

\begin{myitemize}
  \item
  When phases are roughly ``balanced'' in terms of time taken, pipelining is
  most effective.
  This is because overlapping the execution of two balanced phases gives more
  opportunity for reducing their total execution time, compared to when one
  phase significantly dominates the other.
  For the parameters considered here, the phases are most closely balanced when
  $\alpha=1$, as can be seen from Figure~\ref{fig:benefit_of_multiphase}.
  Consequently, we see from Figure~\ref{fig:benefit_of_pipelining} that each
  barrier relaxation provides the greatest benefit when $\alpha=1$.

  \item
  Relaxing late-stage barriers---such as those between map and shuffle or
  between shuffle and reduce---is predicted to have a greater benefit than
  relaxing barriers between push and map stages.
  The reason is that our optimization is more constrained in data placement
  during the shuffle phase than the push phase due to the one-reducer-per-key
  constraint, and hence pipelining finds {\em more} opportunity to hide the
  latency of the shuffle phase with its adjoining computational phases (map or
  reduce).
  This phenomenon can also be observed from
  Figure~\ref{fig:benefit_of_multiphase}, where we see that the shuffle time is
  higher than the push time with our optimization (e2e multi), particularly for
  $\alpha$=1 and 10.
\end{myitemize}

To summarize, relaxing barriers is most useful when the execution phases are
roughly balanced, and for later phase boundaries, where there is more
opportunity for latency hiding over the optimal plan with global barriers.

\subsection{Distribution of network resources}
\label{subsec:network}
To evaluate the effect of distribution of network resources, we derive
optimized execution plans using our proposed (end-to-end multi-phase)
optimization algorithm for {\em all} of the network environments described in
Section~\ref{subsec:exptsetup}, starting from a relatively homogeneous
environment with a single data center, to an intra-continental setup consisting
of two data centers in the US, to globally-distributed setups with four or
eight data centers.
\begin{figure*}[htbp]
  \centering
  \subfigure[$\alpha$=0.1]{
    \includegraphics[width=0.65\columnwidth]{gnuplot/new/effect_of_environment_alpha_p1.eps}
  }
  \subfigure[$\alpha$=1]{
    \includegraphics[width=0.65\columnwidth]{gnuplot/new/effect_of_environment_alpha_1.eps}
  }
  \subfigure[$\alpha$=10]{
    \includegraphics[width=0.65\columnwidth]{gnuplot/new/effect_of_environment_alpha_10.eps}
  }
  \caption{A comparison of myopic and end-to-end optimization relative to the
  uniform baseline for different network environments and different values of
  $\alpha$.
  The makespans are normalized by computing the ratio of the actual makespan to
  the makespan of the corresponding uniform schedule.
  A global barrier is used between phases in all cases.}
  \label{fig:effect_of_environment}
\end{figure*}

Figure~\ref{fig:effect_of_environment} shows the makespan for the myopic
optimization that successively optimizes the execution times of the push phase
and the shuffle phase and the end-to-end optimization that optimizes makespan
by considering all phases at once.
Both the myopic and end-to-end optimizations are compared against a baseline
that performs uniform push and shuffle.

If the network environment is a single data center with relative homogeneity of
compute rates and network bandwidths, the uniform baseline does almost as well
as end-to-end optimization for all values of $\alpha$.
Note that this provides supporting evidence for why Hadoop's use of a largely
uniform schedule is quite effective in homogeneous environments.
Interestingly, some optimization is worse than none, as myopic optimization
does worse than uniform.
Intuitively, myopic optimization reacts to rectify small communication
imbalances, but this can in turn create larger computational imbalances among
the map and reduce tasks, resulting in a longer makespan.
This effect can be seen from the figure by the increased map and reduce times
for myopic for $\alpha$=0.1 and 10 respectively.

As the network environment becomes more diverse with more and more data
centers, both myopic and end-to-end optimizations start to perform better than
uniform, as uniform fails to account for the diversity of the environment.
As expected, end-to-end performs the best with makespans that are smaller by
82-87\% over uniform and by 65-82\% over myopic.

In summary, our results show that our optimization derives much greater
opportunity for improvement as the diversity and heterogeneity of the
environment increases, while reducing to a largely uniform placement for
tightly-coupled, homogeneous environments, where myopic optimization may
actually hurt performance.

\subsection{Comparing our optimization to Hadoop}
\label{subsec:hadoop}

We compare the results of our optimization against \textit{vanilla Hadoop},
which represents a typical unmodified Hadoop execution.

\subsubsection{Experimental setup}
For these experiments, we use the 8-node cluster with the emulated links
bandwidths of the distributed PlanetLab environment as described in
Section~\ref{subsec:validate}.
Each of our eight physical nodes hosts two map slots and one reduce slot, as
well as an HDFS datanode.
Each physical node also runs a simple TCP server to act as a data source.
We use our modified Hadoop implementation described in
Section~\ref{subsec:impl}, using largely default Hadoop configuration options,
aside from increasing \verb=io.sort.mb= to 200 and increasing the Java heap
size for worker processes to 800\unit{MB}.
We also set \newline\verb=dfs.replication= to 1 to prevent replication over
emulated slow links, which can have a pronounced adverse impact on performance
(See Section~\ref{subsec:repl}).
Since the emulated environment exhibits no inherent hierarchical structure,
there is no direct way to model it using Hadoop's rack-oriented model.
Therefore, for Hadoop's network topology configuration, we use the default that
models each node as being co-located in the same rack.

To focus our comparison on the efficacy of our optimized plans, we implement
the push phase for both vanilla Hadoop and our optimization using the same type
of \verb=InputSplit= as described in Section~\ref{subsec:impl}.
To provide vanilla Hadoop with a competitive baseline, we take advantage of
Hadoop's existing data-locality optimization, using the \verb=getLocations=
method of our \verb=InputSplit= to hint that Hadoop should push data from a
data source to the most local compute node.
In our testbed, since map tasks run on the same physical nodes as our data
sources, we hint that Hadoop should move data locally from the data source
server directly into the local HDFS data node.
This is logically identical to Hadoop's existing data-locality optimization,
but applied to the emulated wide-area setting.
In addition, we allow vanilla Hadoop to use its dynamic mechanisms such as
speculative task execution and non-local work stealing to avoid stragglers and
idle resources.

For our optimization, we provide our modified Hadoop implementation with the
exact plans produced by our optimization.
The optimized plan is derived using the model parameters for the underlying
platform and the application, and the model uses a G-P-L barrier configuration
at the consecutive phase boundaries to capture Hadoop's execution behavior.
In order to ensure that Hadoop strictly follows these plans, we turn on our
\verb=LocalOnly= configuration option (see Section~\ref{subsec:impl}).
Further, to understand the benefit of our offline execution plan, we turn off
Hadoop's dynamic mechanisms mentioned above for our optimization.
As a baseline, we also compare vanilla Hadoop and our optimization to a uniform
execution plan, which uniformly pushes and shuffles data to mappers and
reducers respectively.

\subsubsection{Applications}

We implement three MapReduce applications for this evaluation, which vary in
terms of their application characteristics, particularly, the expansion factor
$\alpha$:

\noindent{\em (1) Word Count.} This application takes as input a set of
documents and produces as output, for each term in the set, the number of
occurrences of that term.
Map tasks receive a plain text document and tokenize it, then count the number
of occurrences of each term.
For each term $t$, the mapper emits the key-value pair $(t, f(t))$ where $f(t)$
denotes the number of times term $t$ occurred.
We apply the in-mapper-combining pattern, described by Lin and
Dyer~\cite{lin2010ditp}.
Reducers receive key-value pairs of the form $(t, [f(t)])$ where $t$ is the
term and $[f(t)]$ denotes a list of all counts for that term.
The reducer simply sums up this list and emits as final output a tuple with $t$
as the key, and the sum of all counts as the value.
This application exhibits high aggregation; $\alpha=0.09$.
As inputs for this application, we use plain-text eBooks from Project
Gutenberg~\cite{gutenberg}.
The total input size is roughly 16.5\unit{GB}, spread across 48,000
eBooks.\footnote{Note that Project Gutenberg hosts fewer than 48,000 books at
the time of writing.  To reach this data size, we gather a large fraction of
the available books, and include each in our input data twice.}

\noindent{\em (2) Sessionization.} This application takes as input a collection
of Web server logs and produces as output, for each user, the sequence of
``sessions'' for that user.
At its core, this application is a large distributed sort.
Here the map function receives a single server log entry $v$, which it then
parses into a user identifier $id$ and timestamp $t$.
As intermediate data, mappers emit the composite key $(id, t)$ along with the
unchanged value $v$.
This application uses a custom \verb=SortComparator= to sort first by $id$,
then by $t$.
Intermediate key-value pairs are grouped using a custom
\verb=GroupingComparator= that groups only on the $id$; all log entries for a
single $id$ are then presented to a single call of the reduce function.
The system ensures that these values are delivered in sorted order by timestamp
$t$, and the reduce function simply determines the boundary between sessions
for that user by looking for sufficiently large gaps in this value.
There is no opportunity for aggregating (or expanding) intermediate results in
this application, as the mapper simply routes data to reducers.
Therefore $\alpha=1.0$.
We use a portion of the WorldCup98 trace~\cite{wc98} (roughly 5\unit{GB}
spanning 60 million log entries) as the input data for this application.

\noindent{\em (3) Full Inverted Index.} This application takes as input a
collection of documents, each represented as a pair of document identifier $id$
along with a sequence of word identifiers $w$.
It produces as output, for each word $w$, the complete list of documents in
which that word occurs, as well as the position within those documents.
The implementation is modeled after the example from Lin and
Dyer~\cite{lin2010ditp}.
This application also uses a custom \verb=SortingComparator= and
\verb=GroupingComparator= to rely as much as possible on the underlying
MapReduce system for its data movement.
This application expands the input data by adding additional information
regarding each term to the index, yielding $\alpha=1.88$.
As input, we use the same set of eBooks as for the Word Count application, but
preprocessed to remove stop words, and replace terms with an integer term
identifier; in essence a simple forward index.
This data again spans 48,000 books, but the more concise representation yields
a total input size of roughly 4\unit{GB}.

\subsubsection{Experimental results}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\columnwidth]{gnuplot/new/versus_hadoop.eps}
  \caption{Actual makespan for three sample applications on our local testbed.
  Error bars reflect 95\% confidence intervals.}
  \label{fig:versus_hadoop}
\end{figure}

Figure~\ref{fig:versus_hadoop} shows the results of our comparison for the
three applications.
Note that for Hadoop, since the shuffle phase is partially overlapped with the
map and reduce phases, we depict only three phases in each bar in the graph:
the push phase, the overlapped map/shuffle phases, and the overlapped
shuffle/reduce phases.
We see that across all applications, while vanilla Hadoop substantially
outperforms the uniform execution plan (by 68, 40, and 44\% for Word Count,
Sessionization, and Full Inverted Index respectively), Hadoop executing our
optimized plan achieves a further improvement of 36, 41, and 31\% over vanilla
Hadoop for the same applications.
Further, we see how vanilla Hadoop makes myopic decisions.
Hadoop reduces the push time substantially (by 92, 91, and 83\% for Word Count,
Sessionization, and Full Inverted Index, respectively) over uniform.
However, our optimization, while increasing the push time over vanilla Hadoop,
achieves more significant reduction in end-to-end makespan than vanilla Hadoop
by better optimizing the map, shuffle, and reduce phases.
Thus, overall, we find that Hadoop executing our offline end-to-end,
multi-phase optimal execution plan outperforms vanilla Hadoop using its dynamic
mechanisms.

\subsubsection{Enhancing optimized plan dynamically} 

Our optimization provides an offline execution plan based on information about
the underlying infrastructure available before the job begins execution.
As mentioned above, Hadoop provides two dynamic mechanisms to modify the
initial execution plan based on the observed runtime behavior of the network and
nodes:  (i) {\em speculative task execution}, where another copy of a straggler
task is launched on a different node; and (ii) {\em work stealing}, where a
node may request a non-local task if that node is idle.
Here, we evaluate the benefit of using dynamic mechanisms in addition to the
static execution plan derived from the results of our optimization.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\columnwidth]{gnuplot/new/effect_of_dynamic_mechanisms.eps}
  \caption{Effect of Hadoop's dynamic scheduling mechanisms applied atop our
  optimized static execution plan for three sample applications.
  Error bars reflect 95\% confidence intervals.
  \label{fig:effect_of_dynamic_mechanisms}}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\columnwidth]{gnuplot/new/effect_of_dynamic_mechanisms_atop_vanilla_hadoop.eps}
  \caption{Effect of Hadoop's dynamic scheduling mechanisms applied atop a
  competitive Hadoop baseline plan, one that pushes from each data source to
  the most local mapper, and distributes intermediate keys uniformly across all
  reducers.
  Error bars reflect 95\% confidence intervals.
  \label{fig:effect_of_dynamic_mechanisms_atop_vanilla_hadoop}}
\end{figure}

Figure~\ref{fig:effect_of_dynamic_mechanisms} shows the impact of enabling
these two dynamic mechanism on the performance of the optimized plan for each
of the three sample applications.
Additionally, Figure~\ref{fig:effect_of_dynamic_mechanisms_atop_vanilla_hadoop}
shows the impact of enabling these mechanisms atop a competitive Hadoop
baseline plan.
We find from these two figures that, applied on its own, speculation does not
statistically significantly degrade performance in any case, while it
significantly improves performance in one case.
On the other hand, the addition of work stealing to speculation never
statistically significantly improves performance over speculation alone,
while in some cases---specifically Word Count---stealing significantly
degrades performance.
In fact, there is only one case where the combination of speculation and
stealing is statistically significantly better than the static baseline: Full
Inverted Index with the Hadoop baseline.
The reason in this case is that the static plan myopically optimizes the push
phase, adversely impacting the much more dominant shuffle and reduce phases.
By enabling speculation and optionally stealing, however, Hadoop is able to
bypass the bottleneck network links and compute nodes by moving data over
faster links and placing tasks on faster nodes.

Although the dynamic mechanisms are helpful in such a case, they can also
degrade performance in other cases.
For example, the combination of speculation and stealing statistically
significantly worsens performance for two of the three applications when
applied atop an optimized plan, and for one of the three applications when
applied atop a Hadoop baseline plan.
For the optimized plans, this occurs because dynamic changes to the offline plan
can actually undermine the optimization.
After all, if the computed plan is optimal, then barring any changes to the
underlying infrastructure, no dynamic change could improve performance.
For the Hadoop baseline, the degradation occurs for the Word Count application,
for which the runtime is dominated by the push and map phases.
For such an application, Hadoop's myopic optimization is actually quite
effective, and dynamically deviating from this plan can yield significantly
worse performance as we see here.

Though it is not shown directly in the figures, it is noteworthy that the best
Hadoop performance is never statistically significantly better than the
performance with the optimized plan.
Hadoop's best performance relative to the optimized plan occurs for the Word
Count application, for which Hadoop with speculation (but not stealing) yields
a lower mean makespan than the optimized plan, but not statistically
significantly so.

These results overall show the strength of the statically enforced optimized
plan.
At the same time, they show how dynamic mechanisms can improve performance when
the initial plan is far from optimal, as is the case for the Full Inverted
Index application with the Hadoop baseline.
Such a situation could also arise if network or node conditions were to change
significantly, for example due to network congestion or changes in background
CPU load.
Developing dynamic mechanisms that improve performance in such cases without
adversely affecting the performance in other cases is an interesting direction
for future work.

\subsubsection{Impact of data replication across slow links}
\label{subsec:repl}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\columnwidth]{gnuplot/new/effect_of_replication.eps}
  \caption{Effect of HDFS file replication for three sample applications.
  Error bars reflect 95\% confidence intervals.
  \label{fig:effect_of_replication}}
\end{figure}

In our model, we restrict replication to be intra-cluster, to avoid sending
redundant data across slow wide-area links.
Figure~\ref{fig:effect_of_replication} shows the impact of wide-area
replication on the performance of vanilla Hadoop for each of the three
applications.
As the figure shows, increasing replication substantially increases the cost of
data push, as well as the cost of the reduce phase due to the need to
materialize final results to the distributed file system.
While higher replication also yields a reduction in compute time in the map
phase due to greater scheduling flexibility, this improvement is dwarfed by the
increased communication costs.
However, replication across clusters would be useful for achieving higher fault
tolerance against geographically localized faults.
Such replication may also provide other opportunities for performance
optimization; e.g., work stealing may be directed to local tasks to avoid
high-overhead data communication.
Enhancing our model to incorporate cross-cluster replication, or complementing
our execution with other techniques such as checkpointing for intermediate
data~\cite{ko10} is an interesting direction for future work.
