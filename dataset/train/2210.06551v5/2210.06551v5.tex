\begin{table*}[t]

\center
\small
\setlength{\tabcolsep}{5pt}
\resizebox{0.88\linewidth}{!}{
\begin{tabular}{l c | c c c c c c c c c c c c c c c c}
\thickhline 
Method &  & Dire. & Disc. & Eat & Greet & Phone & Photo & Pose & Purch. & Sit & SitD & Smoke & Wait & WalkD & Walk & WalkT & Avg \\
\hline 

Martinez \etal \cite{martinez_2017_3dbaseline} ICCV'17 & 1& 51.8 & 56.2 & 58.1 & 59.0 & 69.5 & 78.4 & 55.2 & 58.1 & 74.0 & 94.6 & 62.3 & 59.1 & 65.1 & 49.5 & 52.4 & 62.9 \\

Pavlakos \etal \cite{pavlakos2018ordinal} CVPR'18 & 1 & 48.5 & 54.4 & 54.4 & 52.0 & 59.4 & 65.3 & 49.9 & 52.9 & 65.8 & 71.1 & 56.6 & 52.9 & 60.9 & 44.7 & 47.8 & 56.2 \\

LCN \cite{ci2019optimizing} ICCV'19 & 1& 46.8 & 52.3 & 44.7 & 50.4 & 52.9 & 68.9 & 49.6 & 46.4 & 60.2 & 78.9 & 51.2 & 50.0 & 54.8 & 40.4 & 43.3 & 52.7 \\

Xu \etal~\cite{xu2021graph} CVPR'21   & 1&   45.2          & 49.9          & 47.5          & 50.9          & 54.9          & 66.1          & 48.5          & 46.3          & 59.7          & 71.5          & 51.4          & 48.6          & 53.9          & 39.9          & 44.1          & 51.9          \\
		

\hline 






VideoPose3D ~\cite{pavllo20193d} CVPR'19 & 243 & 45.2          & 46.7          & 43.3          & 45.6          & 48.1          & 55.1          & 44.6          & 44.3          & 57.3          & 65.8          & 47.1          & 44.0          & 49.0          & 32.8          & 33.9          & 46.8          \\

Cai \etal~\cite{cai2019exploiting} ICCV'19  & 7  & 44.6          & 47.4          & 45.6          & 48.8          & 50.8          & 59.0          & 47.2          & 43.9          & 57.9          & 61.9          & 49.7          & 46.6          & 51.3          & 37.1          & 39.4          & 48.8          \\

Yeh \etal~\cite{NEURIPS2019_1f88c7c5} NeurIPS'19                     & 243      & 44.8          & 46.1          & 43.3          & 46.4          & 49.0          & 55.2          & 44.6          & 44.0          & 58.3          & 62.7          & 47.1          & 43.9          & 48.6          & 32.7          & 33.3          & 46.7          \\

Liu \etal~\cite{Liu_2020_CVPR} CVPR'20   & 243      & 41.8          & 44.8          & 41.1          & 44.9          & 47.4          & 54.1          & 43.4          & 42.2          & 56.2          & 63.6          & 45.3          & 43.5          & 45.3          & 31.3          & 32.2          & 45.1          \\

  Cheng \etal~\cite{cheng20203d} AAAI'20         & 128      & \underline{36.2} & \underline{38.1} & 42.7 & 35.9 & \textbf{38.2} & \underline{45.7} & 36.8 & 42.0 & \textbf{45.9} & \textbf{51.3}          & 41.8 & 41.5 & 43.8 & 33.1 & 28.6    & 40.1      \\

 UGCN ~\cite{wang2020motion} ECCV'20                  & 96      & 38.2   & 41.0    & 45.9          & 39.7   & 41.4   & 51.4  & 41.6          & 41.4          & 52.0          & 57.4    & 41.8  & 44.4   & 41.6    & 33.1    & 30.0 &  42.6    \\

 PoseFormer ~\cite{zheng20213d} ICCV'21          & 81      & 41.5          & 44.8          & 39.8          & 42.5          & 46.5          & 51.6          & 42.1          & 42.0          & 53.3          & 60.7          & 45.5          & 43.3          & 46.1          & 31.8          & 32.2          & 44.3          \\

  Wehrbein \etal~\cite{WehRud2021} ICCV'21         & 200      & 38.5          & 42.5          &  39.9    & 41.7          & 46.5          & 51.6          &  39.9    & 40.8    & 49.5    & 56.8 & 45.3          & 46.4          & 46.8          & 37.8          & 40.4          & 44.3  \\

 MHFormer ~\cite{li2022mhformer} CVPR'22          & 351      & 39.2          & 43.1          &  40.1   & 40.9          & 44.9          & 51.2          & 40.6   & 41.3    & 53.5    & 60.3 & 43.7          & 41.1          & 43.8          & 29.8          & 30.6         & 43.0  \\

 MixSTE ~\cite{zhang2022mixste} CVPR'22         &        243    & 36.7 & 39.0 & \underline{36.5} & 39.4 & \underline{40.2} & \textbf{44.9} & 39.8 & 36.9 & 47.9 & 54.8          & \underline{39.6} & 37.8 & 39.3 & 29.7 & 30.6    & 39.8      \\

  P-STMO ~\cite{shan2022p} ECCV'22        &        243    & 38.4 & 42.1 & 39.8 & 40.2 & 45.2 & 48.9 & 40.4 & 38.3 & 53.8 & 57.3          & 43.9 & 41.6 & 42.2 & 29.3 & 29.3    & 42.1      \\
\hline

\rowcolor{mygray}
 Ours (scratch)     & 243 & 36.3 & 38.7 & 38.6 & \underline{33.6} & 42.1 & 50.1 & \underline{36.2} & \underline{35.7} & 50.1 & 56.6          & 41.3 & \underline{37.4} & \underline{37.7} & \underline{25.6} & \underline{26.5}    & \underline{39.2}      \\


\rowcolor{mygray}
 Ours (finetune)     & 243 & \textbf{36.1} & \textbf{37.5} & \textbf{35.8} & \textbf{32.1} & 40.3 & 46.3 & \textbf{36.1} & \textbf{35.3} & \underline{46.9} & \underline{53.9}          & \textbf{39.5} & \textbf{36.3} & \textbf{35.8} & \textbf{25.1} & \textbf{25.3}    & \textbf{37.5}      \\




















\thickhline
Method &  & Dire. & Disc. & Eat & Greet & Phone & Photo & Pose & Purch. & Sit & SitD & Smoke & Wait & WalkD & Walk & WalkT & Avg \\
\hline 

Martinez \etal \cite{martinez_2017_3dbaseline} ICCV'17 & 1& 37.7 & 44.4 & 40.3 & 42.1 & 48.2 & 54.9 & 44.4 & 42.1 & 54.6 & 58.0 & 45.1 & 46.4 & 47.6 & 36.4 & 40.4 & 45.5 \\

LCN \cite{ci2019optimizing} ICCV'19 & 1& 36.3 & 38.8 & 29.7 & 37.8 & 34.6 & 42.5 & 39.8 & 32.5 & 36.2 & 39.5 & 34.4 & 38.4 & 38.2 & 31.3 & 34.2 & 36.3 \\

Xu \etal\cite{xu2021graph} CVPR'21   & 1&   35.8          & 38.1          & 31.0          & 35.3          & 35.8          & 43.2          & 37.3          & 31.7          & 38.4          & 45.5          & 35.4          & 36.7          & 36.8          & 27.9          & 30.7          & 35.8          \\
		

\hline 

UGCN \cite{wang2020motion} ECCV'20             & 96      & 23.0   & 25.7    & 22.8          & 22.6   & 24.1   & 30.6  & 24.9          & 24.5          & 31.1          & 35.0    & 25.6  & 24.3   & 25.1    & 19.8    & 18.4 &  25.6    \\

 PoseFormer \cite{zheng20213d} ICCV'21      & 81      & 30.0          & 33.6          & 29.9          & 31.0          & 30.2          & 33.3          & 34.8          & 31.4          & 37.8          & 38.6          & 31.7          & 31.5          & 29.0          & 23.3          & 23.1          & 31.3          \\

 MHFormer \cite{li2022mhformer} CVPR'22       & 351      & 27.7          & 32.1          &  29.1   & 28.9          & 30.0          & 33.9          & 33.0   & 31.2    & 37.0    & 39.3 & 30.0          & 31.0          & 29.4          & 22.2          & 23.0         & 30.5  \\

 MixSTE \cite{zhang2022mixste}  CVPR'22    &        243    & 21.6 & 22.0 & 20.4 & 21.0 & 20.8 & 24.3 & 24.7 & 21.9 & 26.9 & 24.9          & 21.2 & 21.5 & 20.8 & 14.7 & 15.6    & 21.6      \\

  P-STMO ~\cite{shan2022p} ECCV'22        &        243    & 28.5 & 30.1 & 28.6 & 27.9  & 29.8 & 33.2 & 31.3 & 27.8 & 36.0 & 37.4 & 29.7 & 29.5 & 28.1 & 21.0 & 21.0 & 29.3      \\
\hline
\rowcolor{mygray}
 Ours (scratch)     & 243 & \underline{16.7} & \underline{19.9} & \underline{17.1} & \underline{16.5} & \underline{17.4} & \underline{18.8} & \underline{19.3} & \underline{20.5} & \underline{24.0} & \underline{22.1}          & \underline{18.6} & \textbf{16.8} & \underline{16.7} & \underline{10.8} & \underline{11.5}    & \underline{17.8}   \\


\rowcolor{mygray}
  Ours (finetune)   & 243 & \textbf{15.9} & \textbf{17.3} & \textbf{16.9} & \textbf{14.6} & \textbf{16.8} & \textbf{18.6} & \textbf{18.6} & \textbf{18.4} & \textbf{22.0} & \textbf{21.8}         & \textbf{17.3} & \underline{16.9} & \textbf{16.1} & \textbf{10.5} & \textbf{11.4}    & \textbf{16.9}      \\

\thickhline

Method &  & Dire. & Disc. & Eat & Greet & Phone & Photo & Pose & Purch. & Sit & SitD & Smoke & Wait & WalkD & Walk & WalkT & Avg \\
\hline 

VideoPose3D ~\cite{pavllo20193d} CVPR'19   & 243      & 3.0          & 3.1          & 2.2          & 3.4          & 2.3          & 2.7          & 2.7          & 3.1          & 2.1          & 2.9          & 2.3          & 2.4          & 3.7          & 3.1          & 2.8          & 2.8          \\

 PoseFormer \cite{zheng20213d} ICCV'21      & 81      & 3.2          & 3.4          & 2.6          & 3.6          & 2.6          & 3.0          & 2.9          & 3.2          & 2.6          & 3.3          & 2.7          & 2.7          & 3.8          & 3.2          & 2.9          & 3.1          \\

 MixSTE \cite{zhang2022mixste}  CVPR'22  &        243    & 2.5 & 2.7 & 1.9 & 2.8 & 1.9 & 2.2 & 2.3 & 2.6 & 1.6 & 2.2  & 1.9 & 2.0 & 3.1 & 2.6 & 2.2    & 2.3      \\
\hline
\rowcolor{mygray}
 Ours (scratch)  & 243 & \underline{1.8} & \underline{2.1} & \underline{1.5} & \underline{2.0} & \underline{1.5} & \underline{1.9} & \underline{1.8} & \underline{2.1} & \underline{1.2} & \underline{1.8}          & \underline{1.5} & \underline{1.4} & \underline{2.6} & \underline{2.0} & \underline{1.7}    & \underline{1.8}   \\


\rowcolor{mygray}
 Ours (finetune)  & 243 & \textbf{1.7} & \textbf{1.9} & \textbf{1.4} & \textbf{1.9} & \textbf{1.4} & \textbf{1.7} & \textbf{1.7} & \textbf{1.9} & \textbf{1.1} & \textbf{1.6}         & \textbf{1.4} & \textbf{1.3} & \textbf{2.4} & \textbf{1.9} & \textbf{1.6}    & \textbf{1.7}      \\


\thickhline

\end{tabular}}
\vspace{0.2cm}
\caption{\textbf{Quantitative comparison of 3D human pose estimation on Human3.6M.} 
(Top) MPJPE (mm) using detected 2D pose sequences. (Middle) MPJPE (mm) using GT 2D pose sequences. (Bottom) MPJVE (mm) using detected 2D pose sequences.  denotes the clip length used by the method. We select the best results reported by each work.  denotes using HRNet~\cite{sun2019deep} for 2D detection.  denotes implemented with a spatio-temporal Transformer design. The best and second-best results are highlighted in bold and
underlined formats. 
}
\label{tab:state_of_the_art_h36m}
\end{table*}





\subsection{Implementation}
\label{subsec:implementation}

We implement the proposed motion encoder DSTformer with depth , number of heads , feature size , embedding size . For pretraining, we use sequence length . The pretrained model could handle different input lengths thanks to the Transformer-based backbone. During finetuning, we set the backbone learning rate to be  of the new layer learning rate. We introduce the experiment datasets in the following sections respectively. Please refer to the appendix for more experimental details.


\subsection{Pretraining}
\label{Sec:exp-pretrain}
We collect diverse and realistic 3D human motion from two datasets, Human3.6M~\cite{h36m_pami} and AMASS~\cite{AMASS:2019}. Human3.6M~\cite{h36m_pami} is a commonly used indoor dataset for 3D human pose estimation which contains 3.6 million video frames of professional actors performing daily actions. Following previous works~\cite{martinez_2017_3dbaseline, pavllo20193d}, we use subjects 1, 5, 6, 7, 8 for training, and subjects 9, 11 for testing. AMASS~\cite{AMASS:2019} integrates most existing marker-based Mocap datasets~\cite{ACCAD, DanceDB:Aristidou:2019, BMLhandball, BMLrub, cmuWEB, dfaust:CVPR:2017, Eyes_Japan, ghorbani2020movi, chatzitofis2020human4d, HEva_Sigal:IJCV:10b, KIT_Dataset, MoSh_lopermahmoodetal2014, MPI_HDM05, PosePrior_Akhter:CVPR:2015, TCD_hands, DBLP:conf/bmvc/TrumbleGMHC17} and parameterizes them with a common representation. We do not use the images or 2D detection results of the two datasets during pretraining as Mocap datasets usually do not provide raw videos. Instead, we use orthographic projection to get the uncorrupted 2D skeletons. We further incorporate two in-the-wild RGB video datasets PoseTrack~\cite{PoseTrack} (annotated) and InstaVariety~\cite{humanMotionKanazawa19} (unannotated) for higher motion diversity. We align the body keypoint definitions with Human3.6M and calibrate the camera coordinates to pixel coordinates following~\cite{lcn-pami}. We randomly zero out  joints, and sample noises from a mixture of Gaussian and uniform distributions~\cite{chang2019poselifter}. We first train on 3D data only for  epochs, then train on both 3D data and 2D data for  epochs, following the curriculum learning practices~\cite{bengio2009icml, 9392296}.








\subsection{3D Pose Estimation}

We evaluate the 3D pose estimation performance on Human3.6M~\cite{h36m_pami} and report the mean per joint position error (MPJPE) in millimeters, which measures the average distance between the predicted joint positions and the GT after aligning the root joint. We also compute the mean per-joint velocity error (MPJVE) to evalute the temporal smoothness following previous works ~\cite{zheng20213d, zhang2022mixste}.
We use the Stacked Hourglass (SH) networks~\cite{newell2016stacked} to extract the 2D skeletons from videos, and finetune the entire network on Human3.6M~\cite{h36m_pami} training set. In addition, we train a separate model of the same architecture, but with random initialization rather than pretrained weights. As shown in Table~\ref{tab:state_of_the_art_h36m} (top), the model trained from scratch outperforms previous methods including other Transformer-based designs with spatio-temporal modeling. It shows the effectiveness of the proposed DSTformer in terms of learning 3D geometric structures and temporal dynamics. To further evaluate the upper bound of the models' capability, we compare the performance when using 2D GT pose sequences as input, which gets rid of the influence of different 2D detectors. As shown in Table~\ref{tab:state_of_the_art_h36m} (middle), our models significantly outperform all the previous approaches. Table~\ref{tab:state_of_the_art_h36m} (bottom) shows that both of our models also surpass previous works in terms of MPJVE, implying better temporal coherence. We attribute the performance advantage of our scratch model to the proposed DSTformer design. We include more comparisons and analysis to demonstrate the advantage of DSTformer with regard to other spatio-temporal architectures in Section~\ref{subsec:ablation} and supplementary materials. Additionally, our method achieves lower errors with the proposed pretraining stage. 






















    
    
    
    
    
    
    
    
    
    
    


    

\begin{table*}[t]

    \begin{minipage}[t]{0.49\linewidth}\vspace{0pt}\centering
    \begin{center}
    \setlength{\tabcolsep}{3pt}
    \resizebox{0.65\linewidth}{!}{
    \begin{tabular}{c|ccc}
    \thickhline 
    Method & X-Sub & X-View  \\
    \thickhline 
    ST-GCN~\cite{yan2018spatial} AAAI'18 & 81.5 & 88.3  \\
    
    
    
    2s-AGCN~\cite{2sagcn2019cvpr} CVPR'19 & 88.5 & 95.1   \\
    
    MS-G3D~\cite{liu2020disentangling} CVPR'20 & 91.5 & 96.2  \\
    
    Shift-GCN~\cite{cheng2020shiftgcn} CVPR'20 & 90.7 & 96.5  \\
    
    CrosSCLR ~\cite{li2021crossclr} CVPR'21 & 86.2 & 92.5  \\
    
    MCC (finetune)~\cite{Su_2021_ICCV} ICCV'21 & 89.7 & 96.3  \\
    
    SCC (finetune)~\cite{Yang_2021_ICCV} ICCV'21 & 88.0 & 94.9  \\
    
    
    UNIK (finetune) ~\cite{yang2021unik} BMVC'21 & 86.8 & 94.4  \\
    CTR-GCN ~\cite{chen2021channel} ICCV'21 & 92.4 & 96.8  \\
    PoseConv3D ~\cite{duanrevisiting} CVPR'22 & \textbf{93.1} & 95.7  \\
    \hline
    \rowcolor{mygray}
    Ours (scratch) & 87.7 & 94.1  \\
    \rowcolor{mygray}
    Ours (finetune) & 93.0 & \textbf{97.2}  \\
    
    \thickhline 
    \end{tabular}
    }
    \end{center}
    \label{tab:nturgbd}
    \end{minipage}
    \begin{minipage}[t]{0.49\linewidth}\vspace{0pt}\centering
    \setlength{\tabcolsep}{18pt}
    \resizebox{0.65\linewidth}{!}{
    \begin{tabular}{c|c}
    \thickhline 
    Method & Accuracy    \\
    \thickhline 
    ST-LSTM + AvgPool~\cite{liu2017skeleton} & 42.9   \\
    ST-LSTM + FC~\cite{liu2017global} & 42.1   \\
    ST-LSTM + Attention~\cite{liu2017global} & 41.0   \\
    APSR~\cite{liu2019ntu} & 45.3 \\
    TCN OneShot~\cite{Sabater_2021_CVPR} & 46.5 \\
    SL-DML~\cite{memmesheimer2021sl} & 50.9  \\
    Skeleton-DML~\cite{Memmesheimer_2022_WACV} & 54.2  \\
    \hline
    \rowcolor{mygray}
    Ours (scratch) & 61.0  \\
    \rowcolor{mygray}
    Ours (finetune) & \textbf{67.4}  \\
    
    \thickhline 
    \end{tabular}
    }
    \label{tab:ntu120_oneshot}
    \end{minipage}
    \caption{\textbf{Quantitative comparison of skeleton-based action recognition accuracy.} (Left) Cross-subject and cross-view recognition accuracy on NTU-RGB+D. All the methods are evaluated using only the ``joint'' modality with 1-clip sampling for the fairness of comparison. (Right) One-shot recognition accuracy on NTU-RGB+D-120. All results are top-1 accuracy (). 
    }
    \label{tab:action}
\end{table*}

\begin{table*}[ht]
\begin{center}
\setlength{\tabcolsep}{9pt}
\resizebox{0.8\linewidth}{!}{
\small
\begin{tabular}{l|ll|lll|llll}
\thickhline 
\multirow{2}{*}{Method} & \multirow{2}{*}{Input} & \multirow{2}{*}{} & \multicolumn{3}{c}{Human3.6M} & \multicolumn{3}{|c}{3DPW} \\
    \cmidrule[0.05em](lr){4-6} \cmidrule[0.05em](lr){7-9}
    & & & MPVE & MPJPE & PA-MPJPE & MPVE & MPJPE & PA-MPJPE \\
\thickhline 
HMR \cite{kanazawa2018end} CVPR'18 & image & 1 & - & 88.0 & 56.8 & - & 130.0 & 81.3  \\
 SPIN \cite{kolotouros2019learning} ICCV'19 & image & 1 & 82.3 & 59.4 & 39.3 & 129.1 & 100.9 & 59.1  \\
Pose2Mesh \cite{choi2020pose2mesh} ECCV'20 & 2D pose & 1 & 85.3 & 64.9 & 48.7 & 109.3 & 91.4 & 60.1 \\
I2L-MeshNet \cite{moon2020i2l} ECCV'20 & image & 1 & - & 55.7 & 41.7 & 110.1 & 93.2 & 58.6 \\
 HybrIK \cite{li2021hybrik} CVPR'21 & image & 1 & 58.1 & 47.4 & 30.1 & 82.4 & 71.3 & 41.9  \\
METRO \cite{lin2021end} CVPR'21 & image & 1 & - & 54.0 & 36.7 & 88.2 & 77.1 & 47.9 \\
Mesh Graphormer\cite{Lin_2021_ICCV} ICCV'21 & image & 1 & - & 51.2 & 34.5 & 87.7 & 74.7 & 45.6 \\
PARE \cite{Kocabas_2021_ICCV} ICCV'21 & image & 1 & - & - & - & 88.6 & 74.5 & 46.5 \\
ROMP \cite{sun2021monocular} ICCV'21 & image & 1 & - & - & - & 108.3 & 91.3 & 54.9 \\
PyMAF \cite{zhang2021pymaf} ICCV'21 & image & 1 & - & 57.7 & 40.5 & 110.1 & 92.8 & 58.9 \\
ProHMR \cite{Kolotouros_2021_ICCV} ICCV'21 & image & 1 & - & - & 41.2 & - & - & 59.8\\
OCHMR \cite{Khirodkar_2022_CVPR} CVPR'22 & image & 1 & - & - & - & 107.1 & 89.7 & 58.3 \\
3DCrowdNet \cite{Choi_2022_CVPR} CVPR'22 & image & 1 & - & - & - & 98.3 & 81.7 & 51.5 \\
CLIFF \cite{li2022cliff} ECCV'22 & image & 1 & - & 47.1 & 32.7 & 81.2 & 69.0 & 43.0 \\
FastMETRO \cite{cho2022FastMETRO} ECCV'22 & image & 1 & - & 52.2 & 33.7 & 84.1 & 73.5 & 44.6 \\
VisDB \cite{yao2022learning} ECCV'22 & image & 1 & - & 51.0 & 34.5 & 85.5 & 73.5 & 44.9 \\
\hline
TemporalContext\cite{arnab2019exploiting} CVPR'19 & video & 32  & - & 77.8 & 54.3  & - & - & 72.2 \\
HMMR \cite{humanMotionKanazawa19} CVPR'19 & video & 20  & - & - & 56.9 & 139.3 & 116.5 & 72.6 \\
DSD-SATN\cite{sun2019human} ICCV'19 & video & 9  & - & 59.1 & 42.4 & - & - & 69.5 \\
VIBE\cite{kocabas2020vibe} CVPR'20 & video & 16  & - & 65.6 & 41.4 & 99.1 & 82.9 & 51.9 \\
TCMR \cite{choi2021beyond} CVPR'21 & video & 16  & - & 62.3 & 41.1 & 102.9 & 86.5 & 52.7 \\
 MAED \cite{wan2021} ICCV'21 & video & 16  & 84.1 & 60.4 & 38.3 & 93.3 & 79.0 & 45.7 \\
MPS-Net \cite{WeiLin2022mpsnet} CVPR'22 & video & 16  & - & 69.4 & 47.4 & 99.7 & 84.3 & 52.1 \\
 PoseBERT \cite{baradel2022posebert} TPAMI'22 (+SPIN \cite{kolotouros2019learning}) & video & 16  & - & - & - & - & - & 57.3 \diff{ 2.3} \\
 SmoothNet \cite{zeng2022smoothnet} ECCV'22 (+SPIN \cite{kolotouros2019learning}) & video & 32  & - & 67.5 \diff{ 1.0} & 46.3 \diff{ 0.2} & - & 86.7 \diff{ 0.9} & 52.7 \diff{ 0.6} \\
\hline
\rowcolor{mygray}
Ours (scratch) & 2D motion & 16 & 75.7 & 62.8 & 41.0 & 99.1 & 85.5 & 50.2   \\

\rowcolor{mygray}
Ours (finetune) & 2D motion & 16 & 65.5 & 53.8 & 34.9 & 88.1 & 76.9 & 47.2  \\
\hline
\rowcolor{mygray}
Ours (finetune) + SPIN \cite{kolotouros2019learning} & video & 16  & 63.7 \diff{ 18.6} & 52.2 \diff{ 7.2} & 35.7 \diff{ 3.6} & 92.8 \diff{36.3} & 79.6 \diff{ 21.3} & 48.2 \diff{ 10.9}  \\

\rowcolor{mygray}
Ours (finetune) + MAED \cite{wan2021} & video & 16  & 66.8 \diff{ 17.3} & 54.8 \diff{ 5.6} & 36.4 \diff{ 1.9} & 84.4 \diff{ 8.9} & 72.3 \diff{ 6.7} & 42.3 \diff{ 3.4}  \\

\rowcolor{mygray}
Ours (finetune) + HybrIK \cite{li2021hybrik} & video & 16  & \textbf{52.6} \diff{ 5.5} & \textbf{43.1} \diff{ 4.3} & \textbf{27.8} \diff{ 2.3} & \textbf{79.4} \diff{ 3.0} & \textbf{68.8} \diff{ 2.5} & \textbf{40.6} \diff{ 1.3}  \\




\thickhline 
\end{tabular}
}
\end{center}
\caption{\textbf{Quantitative comparison of human mesh recovery on Human3.6M and 3DPW datasets.} 
 denotes the clip length used by the method.
 denotes the results obtained with official model weights. The rest are all officially reported results.
The gains in  correspond to different re-implemented SPIN \cite{kolotouros2019learning} results. 
}
\label{tab:mesh}
\vspace{-0.4cm}
\end{table*}


\subsection{Skeleton-based Action Recognition}
\label{Sec:experiment-action}
We further explore the possibility to learn action semantics with the pretrained human motion representations. We use the human action dataset NTU-RGB+D~\cite{shahroudy2016ntu} which contains 57K videos of 60 action classes, and we follow the data splits Cross-subject (X-Sub) and Cross-view (X-View). The dataset has an extended version, NTU-RGB+D-120~\cite{liu2019ntu}, which contains 114K videos of 120 action classes. We follow the suggested \emph{One-shot} action recognition protocol on NTU-RGB+D-120. For both datasets, we use HRNet~\cite{sun2019deep} to extract 2D skeletons following~\cite{duanrevisiting}. Similarly, we train a scratch model with random initialization for comparison. As Table~\ref{tab:action} (left) shows, our methods are comparable or superior to the state-of-the-art approaches. Notably, the pretraining stage accounts for a large performance gain.


Additionally, we delve into the one-shot setting which holds significant practical importance. Real-world applications often require fine-grained action recognition in specific domains such as education, sports, and healthcare. Unfortunately, the action classes in these scenarios are not typically defined in public datasets. As a result, only limited annotations for these novel action classes are available, making accurate recognition a challenging task. As proposed in ~\cite{liu2019ntu}, we report the results on the evaluation set of  novel classes using only  labeled video for each class. The auxiliary set contains the other  classes, and all samples of these classes can be used. We train the model on the auxiliary set using the supervised contrastive learning technique~\cite{SupContrast}. For a batch of auxiliary data, samples of the same class are pulled together, while samples of different classes are pushed away in the action embedding space. During the evaluation, we calculate the cosine distance between the test examples and the exemplars, and use 1-nearest neighbor to determine the class. Table~\ref{tab:action} (right) illustrates that the proposed models outperform state-of-the-art by a considerable margin. Moreover, it is noteworthy that our pretrained model achieves optimal performance with only 1-2 epochs of fine-tuning. Our results indicate that the pretraining stage is effective in learning a robust motion representation that generalizes well to novel downstream tasks, even with limited data annotations.


















\begin{figure*}[t]
  \centering
  \includegraphics[width=\linewidth]{figs/training_curves.pdf}
  \caption{\textbf{Learning curves of finetuning and training from scratch.}}
  \label{fig:training_curve}
\end{figure*}

\subsection{Human Mesh Recovery}
We conduct experiments on Human3.6M \cite{h36m_pami} and 3DPW \cite{vonMarcard2018} datasets and additionally add COCO \cite{lin2014microsoft} dataset during training following \cite{lin2021end, wan2021, kocabas2020vibe}. 
We keep the same training and test split for both datasets as in \cite{martinez_2017_3dbaseline} (Section \ref{Sec:exp-pretrain}) and \cite{kocabas2020vibe, wan2021, lin2021end}, respectively. Following the common practice \cite{kanazawa2018end, kolotouros2019convolutional, kocabas2020vibe, wan2021}, we report MPJPE (mm) and PA-MPJPE (mm) of  joints obtained by . PA-MPJPE calculates MPJPE after aligning with GT in translation, rotation, and scale. We further report the mean per vertex error (MPVE) (mm) of the mesh , which measures the average distance between the estimated and GT vertices after aligning the root joint. Note that most previous works \cite{kanazawa2018end, kolotouros2019learning, kocabas2020vibe, luo20203d, choi2021beyond, li2021hybrik, lin2021end, wan2021} use more datasets other than COCO~\cite{lin2014microsoft} during training, such as LSP~\cite{johnson2010clustered}, MPI-INF-3DHP~\cite{mehta2017monocular}, \etc, while we do not.
Table \ref{tab:mesh} demonstrates that our finetuned model delivers competitive results on both Human3.6M and 3DPW datasets, surpassing all the state-of-the-art \textit{video-based} methods, including MAED\cite{wan2021}, especially on the MPVE error. Nonetheless, we note that estimating full-body mesh from sparse 2D keypoints alone~\cite{bogo2016keep, choi2020pose2mesh} is an ill-posed problem because it lacks human shape information. In light of this, we propose a hybrid approach that leverages the strengths of both our framework (coherent motion) and RGB-based methods (accurate shape). We introduce a refiner module that can be easily integrated with existing image/video-based methods, similar to ~\cite{baradel2022posebert, zeng2022smoothnet}. Specifically, our refiner module is an MLP that takes the combination of our pretrained motion representations and an initial prediction, regressing a residual in joint rotations. Our approach effectively improves the state-of-the-art methods~\cite{kolotouros2019learning, wan2021, li2021hybrik} and achieves the lowest error to date.



















\begin{table}[t]

\center
\small
\resizebox{\linewidth}{!}{
\begin{tabular}{l  | c | c | c | c }
\thickhline 
{Backbone} & {MPJPE } & MPVE  & Accuracy  & Accuracy  \\
(frozen) & (3D pose)  & (mesh)  & (action x-view)  & (action 1-shot)  \\
\thickhline 

Random & 404.4mm & 114.4mm & 47.6\% & 46.8\%  \\
Pretrained & \textbf{40.3mm} & \textbf{72.1mm} & \textbf{87.3\%} & \textbf{60.7\%}  \\



\thickhline

\end{tabular}}
\vspace{0.1cm}
\caption{\textbf{Comparison of partial finetuning.} 
}
\label{tab:partial}
\vspace{-0.3cm}
\end{table}



\begin{table}[t]
\begin{center}
\resizebox{\linewidth}{!}{
\small
\begin{tabular}{cccc|ccc}
\thickhline 
\multirow{2}{*}{Pretrain} & \multirow{2}{*}{Noise} & \multirow{2}{*}{Mask} & \multirow{2}{*}{2D} & MPJPE & MPVE  &  Accuracy \\
& & & & (3D pose)  & (mesh)  & (action x-sub)   \\
\thickhline 


 - &     -      &    -        &    -   & 39.2mm & 75.7mm &  87.7\% \\

\checkmark &    -       &      -      &  -   & 38.8mm & 70.6mm &  89.4\% \\
\checkmark & \checkmark &    -       &   -  & 38.1mm & 68.4mm & 90.7\% \\
\checkmark & \checkmark & \checkmark &  -  & \textbf{37.4mm} & 67.8mm & 91.9\% \\
\checkmark & \checkmark & \checkmark &  \checkmark& 37.5mm & \textbf{65.5mm} & \textbf{93.0\%}  \\

\thickhline 
\end{tabular}
}
\end{center}
\caption{\textbf{Comparison of pretraining strategies.}}
\label{tab:ablation_pretrain}
\end{table}

\subsection{Ablation Studies}
\label{subsec:ablation}


\paragraph{Finetune vs. Scratch.} 
We compare the training progress of finetuning the pretrained model and training from scratch. As Figure ~\ref{fig:training_curve} shows, models initialized with pretrained weights demonstrate superior performance and faster convergence on all three tasks. This observation suggests that the pretrained model learns transferable knowledge about human motion, facilitating the learning of multiple downstream tasks.




\paragraph{Partial Finetuning.} 
In addition to end-to-end finetuning, we freeze the motion encoder backbone and only train the regression head for each downstream task. To verify the effectiveness of the pretrained motion representations, we compared the pretrained motion encoder with a randomly initialized motion encoder. We report results of 3D pose and mesh on Human3.6M, action on NTU-RGB+D and NTU-RGB+D-120 (same for the tables below). It can be seen in Table~\ref{tab:partial} that based on the frozen pretrained motion representations, our method still achieves competitive performance on multiple downstream tasks and shows a large improvement compared to the baseline. Pretraining and partial finetuning make it possible for all the downstream tasks to share the same backbone, significantly reducing computation overhead for applications requiring multi-task inference.





\paragraph{Pretraining Strategies.}
We evaluate how different pretraining strategies influence the performance of downstream tasks. Starting from the scratch baseline, we apply the proposed strategies one by one. As shown in Table~\ref{tab:ablation_pretrain}, a vanilla 2D-to-3D pretraining stage brings benefits to all the downstream tasks. Introducing corruptions additionally improves the learned motion embeddings. Unified pretraining with in-the-wild videos (\emph{w.} 2D) enjoys higher motion diversity, which further helps several downstream tasks.








\begin{table}[t]

\center
\small
\resizebox{\linewidth}{!}{
\begin{tabular}{l  | c | c | c | c }
\thickhline 
\multirow{2}{*}{Setting} & {MPJPE } & MPVE  & Accuracy  & Accuracy  \\
& (3D pose)  & (mesh)  & (action x-view)  & (action 1-shot)  \\
\thickhline 

TCN (scratch) & 50.1mm & 92.6mm & 91.5\% & 52.4\%  \\
TCN (finetune) & \textbf{47.9mm} & \textbf{86.3mm} & \textbf{92.8\%} & \textbf{59.9\%}  \\
\hline 
PoseFormer (scratch) & 44.8mm & 85.9mm & 94.2\% & 57.4\%  \\
PoseFormer (finetune) & \textbf{41.5mm} & \textbf{80.5mm} & \textbf{95.9\%} & \textbf{60.7\%}  \\



\thickhline

\end{tabular}}
\vspace{0.1cm}
\caption{\textbf{Comparison of different backbones.}
}
\label{tab:backbone}
\end{table}

\begin{table}[t]
\begin{center}
\setlength{\tabcolsep}{3pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|cccccc}
\thickhline
Arch. & (a)      &  (b)       &  (c)     &  (d) &  (e) & (f) \\
\multirow{2}{*}{Design} & \multirow{2}{*}{S-T}     &  \multirow{2}{*}{T-S}      &  \multirow{2}{*}{S + T}     &  \multirow{2}{*}{ST-MHSA} &  S-T + T-S & S-T + T-S \\
 &   &     &      &   &  (Average) & (Adaptive) \\
\thickhline
MPJPE  & 40.58     & 41.05 & 41.76       & 41.54 & 39.87 & \textbf{39.25}               \\
 \thickhline
\end{tabular}
}
\end{center}
\caption{\textbf{Comparison of model architecture variants.} All the methods are trained on Human3.6M from scratch over  runs and measured by MPJPE (mm) with mean and standard deviation.}
\label{tab:ablation_arch}
\end{table}



\paragraph{Pretraining with Different Backbones.} 
We further study the universality of the proposed pretraining approach. We replace the motion encoder backbone with two variants: TCN~\cite{pavllo20193d} and PoseFormer~\cite{zheng20213d}. The models are slightly modified to a \textit{seq2seq} version, while all the configurations for pretraining and finetuning are simply followed. Table~\ref{tab:backbone} shows that the proposed approach consistently benefits different backbone models on different tasks.








\paragraph{Model Architecture.} Finally, we study the design choices of DSTformer. From (a) to (f) in Table \ref{tab:ablation_arch}, we compare different structure designs of the basic Transformer module.  (a) and (b) are single-stream versions with different orders. (a) is conceptually similar to PoseFormer~\cite{zheng20213d}, MHFormer~\cite{li2022mhformer}, and MixSTE\cite{zhang2022mixste}. (c) limits each stream to either temporal or spatial modeling before fusion and is similar to MAED~\cite{wan2021}. (d) directly connects S-MHSA and T-MHSA without the MLP in between and is similar to the \textit{MSA-T} variant in MAED~\cite{wan2021}. (e) replaces the adaptive fusion with average pooling on two streams. (f) is the proposed DSTformer design. 
The result statistically confirms our design principles that both streams should be capable and meanwhile complementary, as introduced in Section~\ref{subsec_arch}. In addition, we find out that pairing each self-attention block with an MLP is crucial, as it could project the learned feature interactions and bring nonlinearity. In general, we design the model architecture for the 3D pose estimation task and apply it to all other tasks without additional adjustment. 


