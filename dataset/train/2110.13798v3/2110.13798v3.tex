
\section{Experiment}
In this section, we demonstrate the performance of our proposed \name\ in terms of effectiveness by comparing it with state-of-the-art deeper GNN methods and self-ablations.
In addition, we conduct a case study to show how the increasing number of layers influences the performance of deeper GNNs when the input graph has missing features.





\subsection{Experiment Setup}
\textbf{Data sets:} The Cora~\citep{DBLP:conf/icml/LuG03} data set is a citation network consisting of 5,429 edges and 2,708 scientific publications from 7 classes. The edge in the graph represents the citation of one paper by another. CiteSeer~\citep{DBLP:conf/icml/LuG03} data set consists of 3,327 scientific publications which could be categorized into 6 classes, and this citation network has 9,228 edges. PubMed~\citep{namata2012query} is a citation network consisting of 88,651 edges and 19,717 scientific publications from 3 classes. The Reddit~\citep{hamilton2017loyalty} data set is extracted from Reddit posts, which consists of 4,584 nodes and 19,460 edges. OGB-Arxiv~\citep{wang2020microsoft} is a citation network, which consists of 1,166,243 edges and 169,343 nodes from 40 classes.

\noindent\textbf{Baselines:} We compare the performance of our method with the following baselines including one vanilla GNN model and four state-of-the-art deeper GNN models: (1) GCN~\citep{DBLP:conf/iclr/KipfW17}: the vanilla graph convolutional network; (2) GCNII~\citep{DBLP:conf/icml/ChenWHDL20}: an extension of GCN with skip connections and additional identity matrices; (3) DGN~\citep{DBLP:conf/nips/Zhou0LZCH20}: the differentiable group normalization for GNNs to normalize nodes within the same group and separate nodes among different groups; (4) PairNorm~\citep{DBLP:conf/iclr/ZhaoA20}: a GNN normalization layer designed to prevent node representations from becoming too similar; (5) DropEdge~\citep{DBLP:conf/iclr/RongHXH20}: a GNN-agnostic framework that randomly removes a certain number of edges from the input graph at each training epoch; 
(6) 
\name-S: using the simplified WDG-ResNet in \name.


\noindent\textbf{Configurations}: In the experiments, we follow the splitting strategy used in~\citep{DBLP:conf/iclr/ZhaoA20} by randomly sampling 3\% of the nodes as the training samples, 10\% of the nodes as the validation samples, and the remaining 87\% as the test samples. We set the learning rate to be 0.001 and the optimizer is RMSProp, which is one variant of ADAGRAD~\citep{DBLP:journals/jmlr/DuchiHS11}. For fair comparison, we set the feature dimension of the hidden layer to be 50, the dropout rate to be 0.5, the weight decay rate to be 0.0005, and the total number of iterations to be 1500 for all methods. For \name~ and \name-S, we sample 10 instances and 5 neighbors for each class from the training set, $dist(\cdot)$ is the hamming distance, and $f(\cdot)$ is the cosine similarity measurement. The experiments are repeated 10 times if not otherwise specified.
All of the real-world data sets are publicly available. The experiments are performed on a Windows machine with a 16GB RTX 5000 GPU. 

\textbf{The detailed hyperparameters (e.g., $\lambda$ and $\alpha$) setting for the experimental results in each table could be found in Appendix~\ref{deeperGXX_parameter_setting}. Hyperparameter study and efficiency analysis could also be found in Appendix~\ref{deeperGXX_parameter_analysis} and ~\ref{deeperGXX_efficiency}, respectively.}


















\subsection{Experimental Analysis}
In this subsection, we evaluate the effectiveness of the proposed method on four benchmark data sets by comparing it with state-of-the-art methods. The base model for all methods we used in these experiments is graph convolutional neural network (GCN)~\citep{DBLP:conf/iclr/KipfW17}. For fair comparison, we set the numbers of hidden layers to be 60 for all methods and the dimension of the hidden layer to be 50. The experiments are repeated 5 times and we record the mean accuracy as well as the standard deviation in Table~\ref{TB:node_classification}. 

\begin{table*}[h]
\centering
\caption{Accuracy of node classification on four benchmark data sets with 60 hidden layers. GCN is used as the backbone for all methods.}
\vspace{-3mm}
\scalebox{0.95}
{
\begin{tabular}{|c|c|c|c|c|}
\hline Method & Cora  & CiteSeer & PubMed & Reddit \\
\hline
\hline  GCN                 & 0.2962 $\pm$ 0.0084 & 0.2125 $\pm$ 0.0140 & 0.4172 $\pm$ 0.0098 & 0.1140 $\pm$ 0.0136\\
\hline  PairNorm            & 0.6759 $\pm$ 0.0171 & 0.4817 $\pm$ 0.0197 & 0.7883 $\pm$ 0.0101 & 0.9017 $\pm$ 0.0241 \\
\hline  DropEdge            & 0.2911 $\pm$ 0.0122 & 0.2147 $\pm$ 0.0184 & 0.4162 $\pm$ 0.0208 & 0.1019 $\pm$ 0.0324 \\
\hline  GCNII               & 0.6076 $\pm$ 0.0050 & 0.5775 $\pm$ 0.0027 & 0.8188 $\pm$ 0.0030 & 0.6969 $\pm$ 0.0064  \\
\hline  DGN                 & 0.7022 $\pm$ 0.0079 & 0.4398 $\pm$ 0.0118 & 0.7843 $\pm$ 0.0032 & 0.5122 $\pm$ 0.0143\\
\hline  PairNorm + ResNet            & 0.7394 $\pm$ 0.0271 & 0.5544 $\pm$ 0.0166 & 0.7985 $\pm$ 0.0068 & 0.9385 $\pm$ 0.0102\\
\hline  DropEdge + ResNet            & 0.1696 $\pm$ 0.0271 & 0.1951 $\pm$ 0.0382 & 0.5798 $\pm$ 0.1501 & 0.0950 $\pm$ 0.0129\\
\hline  GCNII   + ResNet             & 0.7024 $\pm$ 0.0075 & 0.6051 $\pm$ 0.0062 & 0.8093 $\pm$ 0.0047 & 0.7538 $\pm$ 0.0095 \\
\hline  DGN   + ResNet               & 0.1543 $\pm$ 0.0004 & 0.2104 $\pm$ 0.0000 & 0.2086 $\pm$ 0.0000 & 0.1118 $\pm$ 0.0000\\
\hline  \name-S             & 0.8023 $\pm$ 0.0117 & 0.6544 $\pm$ 0.0099 & \textbf{0.8198 $\pm$ 0.0012} & 0.9693 $\pm$ 0.0036 \\
\hline  \name               & \textbf{0.8059 $\pm$ 0.0028} & \textbf{0.6655 $\pm$ 0.0117} & 0.8185 $\pm$ 0.0016 & \textbf{0.9721 $\pm$ 0.0011} \\
\hline
\end{tabular}}
\vspace{-3mm}
\label{TB:node_classification}
\end{table*}






In the first five rows of Table~\ref{TB:node_classification}, we observe that when we set 60 hidden layers as the reference, DropEdge has almost identical performance as the vanilla GCN. PairNorm, GCNII, and DGN increase the performance by more than 30\% in four data sets compared with GCN. The latent reason is that the over-smoothing problem is alleviated to some extent, since these three are deliberately proposed to deal with over-smoothing problem in deeper GNNs.
Besides, our proposed method (i.e., \name) and its simpler version (i.e., \name-S) outperform all of these baselines over four data sets.
Addition to addressing the over-smoothing problem, another part of our outperformance can be credited to our proposed residual connection.
To verify this conjecture, we further incorporate ResNet into PairNorm, DropEdge, DGN, GCNII, and DropEdge. Then, in the sixth to the eighth rows of Table~\ref{TB:node_classification}, we can observe (1) some baselines like PairNorm and GCNII suffer from the vanishing gradient (e.g., the performance of PairNorm increases by 6\% on Cora and 7\% on CiteSeer, while the performance of GCNII rises to 75.38\% on Reddit and 70.24\% on Cora). Also, although GCNII designs a ResNet-like architecture, adding ResNet to GCNII can still boost its performance on three data sets; (2) Compared with their "+ResNet" versions, our \name\ and \name-S still outperform, which implies that our designed residual connection indeed contributes to the outperformance, and we do the ablation study to quantify each component's contribution of \name\ in Section~\ref{deeperGXX_ablation}; (3) Not all deeper baselines need ResNet. For example, DropEdge+ResNet almost maintains the performance, and DGN+ResNet drops the performance.




In addition to four small data sets, we also examine the the node classification performance of \name\ on a large-scale graph called OGB-Arxiv shown in Figure~\ref{fig_base_model}a and Figure~\ref{fig_base_model}b. In this experiment, we fix the feature dimension of the hidden layer as 100, the total iteration is set as 3000 and GCN is chosen as the base model. Due to the memory limitation, we set the number of layers as 10 for all baselines methods in Figure~\ref{fig_base_model}b for fair comparison. By observation, we find that (1) in Figure~\ref{fig_base_model}a, the performance of \name\ increases as we increase the number of layers, which verifies our conjuncture that increasing the number of layers indeed leads to better performance in large graphs due to more information aggregated from neighbors; (2) comparing with PairNorm, \name\ further boosts the performance by more than 5.6\% on OGB-Arxiv data set in Figure~\ref{fig_base_model}b.



\begin{figure*}[h]
    \centering    
    \begin{subfigure}[t]{0.327\textwidth}
        \includegraphics[width=\textwidth]{Figures/ogb_with_layers.pdf}
        \captionsetup{justification=centering}
        \caption{~}
    \end{subfigure}
    \begin{subfigure}[t]{0.327\textwidth}
        \includegraphics[width=\textwidth]{Figures/ogb_effectiveness.jpg}
        \captionsetup{justification=centering}
        \caption{~}
    \end{subfigure}
    \begin{subfigure}[t]{0.327\textwidth}
        \includegraphics[width=\textwidth]{Figures/base_model.jpg}
        \captionsetup{justification=centering}
        \caption{~}
    \end{subfigure}
    \caption{(a) Accuracy of Deeper-GXX on OBG-Arxiv data set with different number of layers. (b) Performance (i.e., accuracy) comparison on OGB-Arxiv data set. (c) Accuracy of different base models with 60 hidden layers on four data sets.}
    \label{fig_base_model}
\end{figure*}

In Figure~\ref{fig_base_model}c, we show the performance of our proposed method with different base models (\eg, GAT~\citep{DBLP:conf/iclr/VelickovicCCRLB18} and SGC~\citep{DBLP:conf/icml/WuSZFYW19}). In the experiment, we set the numbers of the hidden layers as 60 for all methods and the dimension of the hidden layer as 50. The total number of training iteration is 1500. By observation, we find that both GAT and SGC suffer from vanishing gradient and over-smoothing when the architecture becomes deeper, and our proposed method \name~ greatly alleviates them and boosts the performance by 40\%-60\% on average over four data sets. Specifically, compared with the vanilla SGC, our \name\ boosts its performance by 43\% on the CiteSeer data set and more than 67\% on the Reddit data set.
























\subsection{Case Study: Missing feature scenario}
\label{Missing_feature_scenario}
\he{How come you did not mention this scenario in the introduction? This should be discussed together with the toy example as the motivation.}\lc{updated}
\emph{Why do we need to stack more layers of GNNs?} To answer this question, let us first imagine a scenario where some values of attributes are missing in the input graph. In this scenario, the shallow GNNs may not work well because GNNs could not collect useful information from the neighbors due to the massive missing values. However, if we increase the number of layers, GNNs are able to gather more information from the $k$-hop neighbors and capture latent knowledge to compensate for missing features. To verify this, we conduct the following experiment: we randomly mask $p\%$ attributes in Cora and CiteSeer data sets (i.e., setting the masked attributes to be 0), gradually increase the number of layers, and record the accuracy for each setting. In this case study, the number of layers is selected from $\{2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30, 40, 50, 60\}$, and the base model is GCN. For fair comparison, we add ResNet~\citep{DBLP:conf/cvpr/HeZRS16} if it can boost the baseline model's performance by avoiding the vanishing gradient issue. We repeat the experiments five times and record the mean accuracy and standard deviation. 


Table~\ref{TB:node_classification_missing_features} shows the performance of \name\ and various baselines with the optimal number of layers denoted as \#L, i.e., when the model achieves the best performance. By observation, we find that when the missing rate is 25\%, shallow GCN with ResNet has enough capability to achieve the best performance on both CiteSeer and Cora data sets. Compared with GCN, our proposed method further improves the performance by more than 3.83\% on the CiterSeer data set and 4.08\% on the Cora data set by stacking more layers. However, when we increase the missing rate to 50\% and 75\%, we observe that most methods tend to achieve the best performance by stacking more layers. Specifically, PairNorm achieves the best performance at 10 layers when 25\% features are missing, while it has the best performance at 40 layers when 75\% features are missing. A similar observation could also be found with GCNII on the Cora data set, DropEdge on CiteSeer data set as well as our proposed methods in both data sets.
Overall, the experimental results verify that the more features a data set is missing, the more layers GNNs need to be stacked to achieve better performance.
One possible explanation is that, when more features are missing, we need more neighbors aggregated for collecting their partial features to compensate for the missing such that stacking GNN layers works.


\begin{table*}[t]
\centering
\vspace{3mm}
\caption{Accuracy of node classification on two data sets by masking $p$ percent of node attributes. $\#L$ denotes the number of layers where a model achieves the best performance.}
\scalebox{0.86}{
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
\multicolumn{2}{|c|}{Node Feature Missing Rate} &  \multicolumn{2}{c|}{$p=25\%$} & \multicolumn{2}{c|}{$p=50\%$} & \multicolumn{2}{c|}{$p=75\%$} \\ \cline{1-8}
data set           & Method        &      Acc  & \#L&       Acc  & \#L &      Acc  & \#L   \\ \hline\hline
\multirow{5}{*}{Cora}           & GCN + ResNet        &  0.7503  $\pm$ 0.0101  &  7     &  0.7435 $\pm$ 0.0048    & 10    &  0.7226 $\pm$ 0.0099  &     10  \\ \cline{2-8} 
                                & PairNorm + ResNet   &  0.7529  $\pm$ 0.0129  &  10    &  0.7482 $\pm$ 0.0172    & 20    &  0.7262 $\pm$ 0.0178  &     40  \\ \cline{2-8}
                                & DropEdge + ResNet   &  0.7634  $\pm$ 0.0112  &  15    &  0.7611 $\pm$ 0.0102    & 20    &  0.7297 $\pm$ 0.0168  &     8   \\ \cline{2-8} 
                                & GCNII + ResNet      &  0.2667  $\pm$ 0.0063  &  25    &  0.3351 $\pm$ 0.0066    & 25    &  0.2914 $\pm$ 0.0106  &     40  \\ \cline{2-8} 
                                & DGN w/o ResNet     &  0.6850  $\pm$ 0.0184  &  30    &  0.6846 $\pm$ 0.0147    & 50    &  0.6717 $\pm$ 0.0156  &     25    \\ \cline{2-8} 
                                & \name-S             &  0.7872  $\pm$ 0.0128  & 15     &  0.7811 $\pm$ 0.0147    & 20    &  0.7586 $\pm$ 0.0121  &     60  \\ \cline{2-8} 
                                & \name               &  \textbf{0.7915  $\pm$ 0.0060}  & 10     &  \textbf{0.7848 $\pm$ 0.0043}    & 20    &  \textbf{0.7598 $\pm$ 0.0081}  &     60  \\\hline
\multirow{5}{*}{CiteSeer}       & GCN + ResNet        &  0.6141  $\pm$ 0.0080  & 4      &  0.5811 $\pm$ 0.0093    & 10    &  0.5149 $\pm$ 0.0173  &     9   \\ \cline{2-8} 
                                & PairNorm + ResNet   &  0.6184  $\pm$ 0.0087  & 8      &  0.5947 $\pm$ 0.0083    & 20    &  0.5176 $\pm$ 0.0075  &     10  \\ \cline{2-8}
                                & DropEdge + ResNet   &  0.6348  $\pm$ 0.0156  & 4      &  0.6083 $\pm$ 0.0128    & 6     &  0.5240 $\pm$ 0.0128  &     10  \\ \cline{2-8} 
                                & GCNII + ResNet      &  0.2453  $\pm$ 0.0045  & 40     &  0.2338 $\pm$ 0.0028    & 20    &  0.2403 $\pm$ 0.0046  &     25  \\ \cline{2-8} 
                                & DGN w/o ResNet      &  0.4560  $\pm$ 0.0162  & 20     &  0.4593 $\pm$ 0.0117    & 15    &  0.4498 $\pm$ 0.0292  &     15   \\ \cline{2-8}  
                                & \name-S             &  0.6508  $\pm$ 0.0060  &  10      &  0.6132 $\pm$ 0.0042   &  15    & 0.5544 $\pm$ 0.0138   &     20  \\\cline{2-8}
                                & \name               & \textbf{0.6524  $\pm$ 0.0087}  &  20    &  \textbf{0.6169 $\pm$ 0.0063}    & 60    & \textbf{0.5576 $\pm$ 0.0070}  &  50   \\  \hline
\end{tabular}}
\label{TB:node_classification_missing_features}
\vspace{-3mm}
\end{table*}


\begin{wraptable}{r}{0.5\textwidth}
\centering
\caption{Ablation Study on Cora Data Set}
\scalebox{1}
{
\begin{tabular}{|c|c|}
\hline Method & Accuracy \\
\hline
\hline  \name               & \textbf{0.8059 $\pm$ 0.0028}  \\
\hline  \name-S             & 0.8023 $\pm$ 0.0117 \\
\hline  \name-D             & 0.7498 $\pm$ 0.0139 \\
\hline  \name-T             & 0.7875 $\pm$ 0.0092 \\
\hline
\end{tabular}}
\label{DeeperGXX_ablation_study}
\end{wraptable}

\subsection{Ablation Study}
\label{deeperGXX_ablation}
In this subsection, we conduct the ablation study on Cora to show the effectiveness of WDG-ResNet and \layer\ in Table~\ref{DeeperGXX_ablation_study}. In this experiment, we fix the feature dimension of the hidden layer as 50, the total iteration is set as 3000, the number of layers is set as 60, the sampling batch size for Deeper-GXX is 10, and GCN is chosen as the base model. In Table~\ref{DeeperGXX_ablation_study}, \name-T removes TGCL loss, \name-D removes the weight decaying factor in WDG-ResNet and \name-S achieves the simplified WDG-ResNet in \name, which removes the similarity measure in WDG-ResNet.
In Table~\ref{DeeperGXX_ablation_study}, we have the following observations (1) comparing \name\ with \name-T, we find that \name\ boosts the performance by 1.84\% after adding TGCL loss, which demonstrates the effectiveness of TGCL to address over-smoothing issue; (2) \name~ outperforms \name-D by 5.61\%, which shows that \name\ could address the shading neighbors effect by adding the weight decaying factor; (3) comparing \name\ with \name-S, we verify that adding exponential cosine similarity measure $ e^{cos(\bm{H}^{(1)}, \bm{\tilde{H}}^{(l)})}$ could further improve the performance by extending the hypothesis space of neural networks.








