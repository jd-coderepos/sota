

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2021}
\usepackage{times}
\usepackage{latexsym}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{breakurl}
\renewcommand{\UrlFont}{\ttfamily\small}
\newcommand{\muhao}[1]{{\color{blue}(MC: #1)}}

\usepackage{microtype}

\aclfinalcopy 



\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{An Improved Baseline for Sentence-level Relation Extraction}

\author{Wenxuan Zhou \\
  University of Southern California \\
  \texttt{zhouwenx@usc.edu} \\\And
  Muhao Chen \\
  University of Southern California \\
  \texttt{muhaoche@usc.edu} \\}

\date{}

\begin{document}
\maketitle

\begin{abstract}
Sentence-level relation extraction (RE) aims at identifying the relationship between two entities in a sentence.
Many efforts have been devoted to this problem, while the best performing methods are still far from perfection.
In this report, we revisit two problems that affect the performance of existing RE models, namely \textsc{entity representation} and \textsc{noisy or ill-defined labels}.
Our improved baseline model, incorporated with entity representations with typed markers, achieves an  of  on TACRED, significantly outperforms previous SOTA methods.
Furthermore, the presented new baseline achieves an  of  on the refined Re-TACRED dataset, demonstrating that the pre-trained language models achieve unexpectedly high performance on this task.
We release our code\footnote{\burl{https://github.com/wzhouad/RE_improved_baseline}} to the community for future research.

\end{abstract}

\section{Introduction}

As one of the fundamental information extraction (IE) tasks,
relation extraction (RE) aims at identifying the relationship(s) between two entities in a given piece of text from a pre-defined set of relationships of interest.
For example, given the sentence ``Bill Gates founded Microsoft together with his friend Paul Allen in 1975'' and an entity pair (``Bill Gates'', ``Microsoft''), an RE system is expected to predict the relation to be \texttt{ORG:FOUNDED\_BY}.
On this task, SOTA models based on pre-trained language models \cite{devlin-etal-2019-bert} have gained significant success. 

Recent work on RE can be divided into two lines.
One focuses on injecting external knowledge into language models.
Methods of such, including ERNIE~\cite{zhang-etal-2019-ernie} and KnowBERT~\cite{peters-etal-2019-knowledge}, take pre-trained entity embedding as input to the transformer.
Similarly, K-Adapter~\cite{wang2020k} introduces a plug-in neural adaptor that injects factual and linguistic knowledge into the language model, and
LUKE~\cite{yamada-etal-2020-luke} extends the pre-training objective of masked language modeling to entities and proposes an entity-aware self-attention mechanism.
The other line of work focuses on fine-tuning pre-trained language models on entity-linked text using RE-oriented objectives.
Specifically, BERT-MTB~\cite{baldini-soares-etal-2019-matching} proposes a matching-the-blanks objective that decides whether two relation instances share the same entities.
Despite being extensively studied, the performance of existing RE classifiers is still far from perfection.
On the most commonly used benchmark TACRED~\cite{zhang-etal-2017-position}, the SOTA  result only increases from ~(BERT) to ~(LUKE) after applying pre-trained language models to this task.
It is unclear what building block is still missing in a satisfactory RE system. 

In this work, we discuss two obstacles that have hindered the performance of existing RE models.
First, the RE task provides a structured input of both the raw sentences and side information of the subject and object entities such as entity names, spans, and NER types.
In order to feed the structured input into pre-trained language models, one necessary step is to transform the structured inputs into plain text.
However, existing methods fall short of representing the entity information comprehensively in the text, leading to limited characterization of the entities.
Moreover, all splits of TACRED, including the training, development, and test set, contains a large portion of noisy or ill-defined labels, causing the model performance to be underestimated.
\citet{alt-etal-2020-tacred} relabeled the development and test set, and found that  of labels are incorrect.
\citet{stoica2021re} examined the definitions of pre-defined relationships and found a part of them to be ambiguous.
Based on their refined set of relationships, \citeauthor{stoica2021re} further re-annotated the TACRED dataset using an improved annotation strategy to ensure high-quality labels.
To this end, we propose an improved RE baseline, where we introduce the typed entity marker to sentence-level RE, which leads to promising improvement of performance over existing entity representation techniques.

We evaluate our improved baseline model on three different versions of TACRED: the original TACRED~\cite{zhang-etal-2017-position}, TACREV~\cite{alt-etal-2020-tacred}, and Re-TACRED~\cite{stoica2021re}.
Using RoBERTa~\cite{liu2019roberta} as the backbone, our improved baseline model achieves an  of  and  on the original TACRED and TACREV, respectively, significantly outperforming previous methods.
Particularly, our baseline model achieves an  of  on Re-TACRED.
It demonstrates that pre-trained language models achieve much better results than shown in previous work on the sentence-level RE task.
Moreover, we notice that the high noisy rate of TACRED makes the RE models more reliant on the side information of entities.
We suggest that Re-TACRED is a better evaluation benchmark for future work on sentence-level RE.


\section{Improved RE Baseline}

In this section, we first formally define the sentence-level RE task in Section~\ref{sec:task_definition}, and then present our model architecture and entity representation techniques in Section~\ref{sec:model_architecture} and Section~\ref{sec:entity_representation}.
Finally, we evaluate whether our model can generalize to unseen entities in Section~\ref{sec:unseen_entities}.

\subsection{Problem Definition}
\label{sec:task_definition}
In this paper, we focus on sentence-level RE.
Specifically, given a sentence  and an entity pair , the task of sentence-level RE is to predict the relationship  between  and  from , where  is a pre-defined set of relationships of interest, ,  are identified as the subject and object entity, respectively.
Entity pairs that do not express relation from  are labeled \textsc{NA}.

\begin{table*}[!t]
\centering
\scalebox{0.76}{
    \begin{tabular}{p{4.3cm}p{8.2cm}ccc}
         \toprule
         Method& Input Example& BERT& BERT& RoBERTa \\
         \midrule
         Entity mask& \texttt{[SUBJ-PERSON]} \text{was born in} \texttt{[OBJ-CITY]}\text{.}& 69.6& 70.6& 60.9 \\
         Entity marker& \texttt{[E1]} \text{Bill} \texttt{[/E1]} \text{was born in} \texttt{[E2]} \text{Seattle} \texttt{[/E2]}\text{.}& 68.4& 69.7& 70.7 \\
         Entity marker (punct)& \text{@ Bill @ was born in \# Seattle \#.}& 68.7& 69.8& 71.4 \\
         Typed entity marker& \texttt{S:PERSON} \text{Bill} \texttt{/S:PERSON} \text{was born in} \texttt{O:CITY} \text{Seattle} \texttt{/O:CITY}\text{.}& \textbf{71.5}& \textbf{72.9}& 71.0 \\
         Typed entity marker (punct)& \text{@ * person * Bill @ was born in \#  city  Seattle \#.}& 70.9& 72.7& \textbf{74.6} \\
         \bottomrule
    \end{tabular}}
    \caption{\textbf{Test  (in \%) of different entity representation techniques on TACRED.} For each technique, we also provide the processed input of an example sentence \textit{``Bill was born in Seattle''}. Typed entity marker (original and punct) significantly outperforms others.}
    \label{tab::representation}
\end{table*}

\subsection{Model Architecture}
\label{sec:model_architecture}
Our RE classifier is an extension of previous transformer-based~\cite{devlin-etal-2019-bert,liu2019roberta} RE models~\cite{baldini-soares-etal-2019-matching}.
Given the input sentence , we first mark the entity spans and entity types using techniques presented in Section~\ref{sec:entity_representation}, then feed the processed sentence into a pre-trained language model to get its contextual embedding.
Finally, we feed the hidden states of the subject and object entities in the language model's last layer, i.e.,  and , into the softmax classifier:

where ,  are model parameters.
In inference, the classifier returns the relationship with the maximum probability.

\subsection{Entity Representation}
\label{sec:entity_representation}
For sentence-level RE, the names, spans, and NER types of entities are provided as the input.
The RE models need to transform such side information of entities into text to allow it to be captured by the pre-trained language models.
Previous work~\cite{zhang-etal-2017-position,baldini-soares-etal-2019-matching,wang2020k} has proposed different entity representation techniques.
In this context, we conduct a comprehensive evaluation of the following techniques on TACRED:
\begin{itemize}[leftmargin=1em]
    \setlength\itemsep{0em}
    \item \textbf{Entity mask}~\cite{zhang-etal-2017-position}. This technique uses a special \texttt{[SUBJ-TYPE]} or \texttt{[OBJ-TYPE]} token to mask the subject and object entities in the original sentence, where \texttt{TYPE} is substituted with the corresponding entity type.
    This technique was originally proposed for the PA-LSTM model~\cite{zhang-etal-2017-position}, and was later adopted by transformer-based models such as SpanBERT~\cite{joshi-etal-2020-spanbert}.
    \citet{zhang-etal-2017-position} claim that this technique prevents RE models from over-fitting predictions to entity names.
    \item \textbf{Entity marker}~\cite{zhang-etal-2019-ernie,baldini-soares-etal-2019-matching}. This technique introduces special tokens \texttt{[E1]}, \texttt{[/E1]}, \texttt{[E2]}, \texttt{[/E2]} to enclose the spans of subject and object entities, and modify the sentence to ``\texttt{[E1]} \textsc{subj} \texttt{[/E1]} ... \texttt{[E2]} \textsc{obj} \texttt{[/E2]}'' \footnote{\textsc{subj} and \textsc{obj} are respectively the original token spans of subjet and object entities in the sentence.}.
    \item \textbf{Entity marker (punct)}~\cite{wang2020k,zhou2021atlop}. This technique is a variant of the previous technique that marks entity spans using punctuation.
    It modifies the original sentence to ``@ \textsc{subj} @ ... \# \textsc{obj} \#''.
    \item \textbf{Typed entity markers}~\cite{zhong2020frustratingly} This technique further adds the NER types into entity markers.
    It introduces special tokens \texttt{S:TYPE}, \texttt{/S:TYPE}, \texttt{O:TYPE}, \texttt{/O:TYPE}, where \texttt{TYPE} is the corresponding entity type given by NER, and modifies the sentence to ``\texttt{S:TYPE} \textsc{subj} \texttt{/S:TYPE} ... \texttt{O:TYPE} \textsc{obj} \texttt{/O:TYPE}''.
    \item \textbf{Typed entity marker (punct)} We propose a variant of the typed entity marker technique that marks the entity span and NER type of entities without introducing new special tokens.
    We enclose the subject and object entities with ``@'' and ``\#'', respectively.
    We also represent the subject and object entity types using plain text, which is prepended to the entity spans, and is enclosed by ``*'' for subjects or ``'' for objects.
    The modified sentence is ``@ * \textsc{subj-ner} * \textsc{subj} @ ... \#  \textsc{obj-ner}  \textsc{obj} \# ''.
\end{itemize}

The above techniques all seek to provide combined representations for different entity information.
Differently, the typed entity marker represents all of entity names, entity spans, and entity types, while entity marker and entity mask ignore entity types and entity names, respectively.

Table~\ref{tab::representation} shows the performance of different representation techniques.
For each technique, we also provide an example of the processed sentence.
We observe that: (1) Typed entity markers (original and punct) outperform other techniques by a notable margin.
Especially, the RoBERTa model achieves an  score of  using the typed entity marker (punct), which is significantly higher than the SOTA result of ~\cite{yamada-etal-2020-luke}.
It shows that all types of entity information are helpful to the RE task.
It also shows that keeping entity names in the input improves the generalization of RE models, contradicting the claim by \citet{zhang-etal-2017-position}.
(2) Although the original and punct versions of entity representation techniques represent the same types of entity information, they do make a difference in model performance.
Particularly, introducing special tokens deteriorates severely model performance on RoBERTa.
On RoBERTa, entity marker underperforms entity marker (punct) by , typed entity marker underperforms typed entity marker (punct) by , while entity mask gets a much worse result of .

\begin{table}[!t]
\centering
\scalebox{0.76}{
    \begin{tabular}{p{7.5cm}c}
         \toprule
         Model& Test  \\
         \midrule
         BERT + entity mask& 75.4 \\
         BERT + entity mask& 75.8 \\
         RoBERTa + entity mask& 69.7 \\
         \midrule
         BERT + typed entity marker& 75.9 \\
         BERT + typed entity marker& 76.8 \\
         RoBERTa + typed entity marker (punct)& 78.9 \\
         \bottomrule
    \end{tabular}}
    \caption{\textbf{Test  on the filtered test set of TACRED.}}
    \label{tab::filter}
\end{table}

\begin{table*}[!t]
\centering
\scalebox{0.81}{
    \begin{tabular}{p{7.5cm}ccc}
         \toprule
         \textbf{Model} & \multicolumn{1}{c}{\textbf{TACRED}} & \multicolumn{1}{c}{\textbf{TACREV}}& \multicolumn{1}{c}{\textbf{Re-TACRED}} \\
          &Test  & Test  & Test  \\
         \midrule
         \textit{Sequence-based Models} \\
         PA-LSTM~\cite{zhang-etal-2017-position}& 65.1& 73.3 & 79.4 \\
         C-GCN~\cite{zhang-etal-2018-graph}& 66.3& 74.6 &80.3 \\
         \midrule
         \textit{Transformer-based Models} \\
         BERT + entity marker& 68.4& 77.2& 87.7 \\
         BERT + entity marker& 69.7& 77.9& 89.2  \\
         RoBERTa + entity marker& 70.7& 81.2& 90.5\\
         SpanBERT~\cite{joshi-etal-2020-spanbert}& 70.8& 78.0& 85.3 \\
         KnowBERT~\cite{peters-etal-2019-knowledge}& 71.5& 79.3& - \\
         LUKE~\cite{yamada-etal-2020-luke}& 72.7& 80.6& 90.3 \\
         \midrule
         \textit{Improved RE baseline}\\
         BERT + typed entity marker& 71.5 & 79.3 & 87.9 \\
         BERT + typed entity marker & 72.9 & 81.3 & 89.7 \\
         RoBERTa + typed entity marker (punct)& \textbf{74.6}& \textbf{83.2}& \textbf{91.1} \\
         \bottomrule
    \end{tabular}}
    \caption{\textbf{ (in \%) on the original and revisited test set of TACRED.} * marks re-implemented results from \citet{alt-etal-2020-tacred}.  marks re-implemented results from \citet{stoica2021re}.  marks our re-implemented results. We report the median of  on 5 runs training using different random seeds. We fail to run the officially released code of KnowBERT because of environment errors.
    }
    \label{tab::tacred}
\end{table*}

\begin{table}[t]
    \centering
    \scalebox{0.74}{
    \begin{tabular}{p{2.4cm}cccc}
    \toprule
    \textbf{Dataset}& \# train& \# dev& \# test& \# classes\\
    \midrule
    TACRED& 68124& 22631& 15509& 42\\
    TACREV& 68124& 22631& 15509& 42\\
    Re-TACRED& 58465& 19584& 13418& 40\\
    \bottomrule
    \end{tabular}}
    \caption{\textbf{Statistics of different versions of TACRED.}}
    \label{tab:data_statistics}
\end{table}

\subsection{Inference with Unseen Entities}
\label{sec:unseen_entities}
Some previous work~\cite{zhang-etal-2018-graph} claims that entity names may leak superficial clues of the relation types, allowing heuristics to hack the benchmark.
They show that neural RE classifier can achieve high evaluation results only based on the subject and object entity names even without putting them in the context of the original sentence.
They also suggest that RE classifiers trained without entity masks may not generalize well to unseen entities.
However, as the provided NER types in RE datasets are usually coarse-grained, using entity masks may lead to the loss of entity information.
Using entity masks also contradicts existing methods of injecting entity knowledge into RE classifiers~\cite{zhang-etal-2019-ernie,peters-etal-2019-knowledge,wang2020k}.
If RE classifiers should not consider entity names, it is unreasonable to suppose that the RE classifiers can be improved by external knowledge graphs.

To evaluate whether the RE classifier can generalize to unseen entities, we propose a \textit{filtered} evaluation setting.
Specifically, we remove all test instances containing entities from the training set. This results in a \textit{filtered test set} of 4,599 instances that only contain sentences with entities that are unseen during training.
We present the evaluation results on the filtered test set in Table~\ref{tab::filter}.
Note that as the label distributions of the original and filtered test set are different, their results are not directly comparable.
Still, \emph{typed entity marker} consistently outperforms \emph{entity mask}, which shows that RE classifiers can learn from entity names and generalize to unseen entities.
Our finding is consistent with \citet{peng-etal-2020-learning}, whose work suggests that entity mentions can provide more information than entity types to improve the RE classifier.




\section{Experiments}
\subsection{Experimental Settings}
\smallskip
\noindent
\textbf{Datasets.} We evaluate our model on three different versions of TACRED: TACRED~\cite{zhang-etal-2017-position}, TACREV~\cite{alt-etal-2020-tacred}, and Re-TACRED~\cite{stoica2021re}.
The statistics of the three datasets are shown in Table~\ref{tab:data_statistics}.

\smallskip
\noindent
\textbf{Model Configurations}. Our model is implemented based on HuggingFace's Transformers~\cite{wolf-etal-2020-transformers}.
Most hyper-parameters are set following \citet{baldini-soares-etal-2019-matching}.
Specifically, our model is optimized with Adam~\cite{Kingma2015AdamAM} using the learning rate of  on BERT, and  on BERT and RoBERTa, with a linear warmup~\cite{Goyal2017AccurateLM} for the first  steps followed by a linear decay to 0.
We use a batch size of 64 for all models.
We fine-tune the model for 5 epochs on all datasets.
The best model checkpoint is selected based on dev .
For other compared methods, we rerun their officially released code using the recommended hyperparameters in their papers.

\subsection{Results}

The experimental results are shown in Table~\ref{tab::tacred}.
Using RoBERTa as the backbone, our improved RE baseline achieves the best results on all datasets.
However, we notice that on Re-TACRED, the gain from typed entity marker is much smaller compared to TACRED and TACREV, decreasing from  and  to  of .
This observation could be attributed to the high noise rate in the training set of TACRED, in which case introducing additional entity information can reduce the influence of noisy labels.
This shows that noisy labels in TACRED could make model predictions more dependent on the side information of entities such as NER types.
Therefore, we suggest future work on sentence-level RE to adopt Re-TACRED as the evaluation benchmark in order to avoid such bias.

\section{Conclusion}
In this report, we revisit two technical problems in existing sentence-level RE models, namely entity representation and noisy or ill-defined labels.
We propose an improved RE baseline model, which significantly outperforms existing RE models.
Especially, our model achieves an  score of  on the Re-TACRED dataset, showing that pre-trained language models already achieve satisfactory performance on this task.
We hope the proposed model can benefit future research in sentence-level RE.


\bibliographystyle{acl_natbib}
\bibliography{acl2021}


\end{document}
