\pdfoutput=1

\documentclass{article}













\usepackage[nonatbib,final]{neurips_2019}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      




\usepackage{epsfig}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\usepackage{booktabs,subcaption,dcolumn}

\newif\ifsubmit
\submitfalse

\definecolor{medGray}{RGB}{230,230,230}

\makeatletter
\newcommand{\figcaption}[1]{\def\@captype{figure}\caption{#1}}
\newcommand{\tblcaption}[1]{\def\@captype{table}\caption{#1}}
\makeatother

\ifsubmit
\newcommand\todo[1]{}
\newcommand\kwrite[1]{}
\newcommand\mwrite[1]{}
\newcommand\mnote[1]{}
\newcommand\ynote[1]{}
\newcommand\knote[1]{}
\else
\newcommand\todo[1]{\textcolor{red}{TODO: #1}}
\newcommand\mnote[1]{\textcolor{blue}{(MIYATO: #1)}}
\newcommand\mwrite[1]{\textcolor{green}{#1}}
\newcommand\kwrite[1]{\textcolor{brown}{#1}}
\newcommand\ynote[1]{\textcolor{blue}{(Yoshida: #1)}}
\newcommand\knote[1]{\textcolor{blue}{(Kataoka: #1)}}
\fi
\newcommand{\rmT}{{\rm T}}
\newcommand{\bbR}{\mathbb{R}}
\newcommand{\bmb}{{\bm b}}
\newcommand{\bms}{{\bm s}}
\newcommand{\bmu}{{\bm u}}
\newcommand{\bmv}{{\bm v}}
\newcommand{\bmw}{{\bm w}}
\newcommand{\bmx}{{\bm x}}
\newcommand{\bmh}{{\bm h}}
\newcommand{\bmy}{{\bm y}}
\newcommand{\bmz}{{\bm z}}
\newcommand{\bmm}{{\bm m}}
\newcommand{\bmSigma}{{\bm \Sigma}}
\newcommand{\bmLambda}{{\bm \Lambda}}
\newcommand{\bmxi}{{\bm \xi}}
\newcommand{\bmzero}{{\bm 0}}
\newcommand{\bmg}{{\bm g}}
\newcommand{\bmeta}{{\bm \eta}}
\newcommand{\bmdelta}{{\bm \delta}}
\newcommand{\bmgamma}{{\bm \gamma}}
\newcommand{\bmmu}{{\bm \mu}}
\newcommand{\bmphi}{{\bm \phi}}
\newcommand{\set}[1]{\{#1\}}
\newcommand{\E}{\mathop{\mathrm{E}}}

\graphicspath{{figures/}}

\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}

\title{Self-supervised GAN: Analysis and Improvement with Multi-class Minimax Game}





\author{Ngoc-Trung Tran, Viet-Hung Tran, Ngoc-Bao Nguyen, Linxiao Yang, Ngai-Man Cheung\\
  Singapore University of Technology and Design (SUTD)\\
  \\
  Corresponding author: Ngai-Man Cheung \texttt{<ngaiman\_cheung@sutd.edu.sg>}\\
}



\begin{document}

\maketitle

\begin{abstract}

Self-supervised (SS) learning is a powerful approach for representation learning using unlabeled data. Recently, it has been applied to Generative Adversarial Networks (GAN) training. Specifically, SS tasks were proposed to address the catastrophic forgetting issue in the GAN discriminator. In this work, we perform an in-depth analysis to understand how SS tasks interact with learning of generator. From the analysis, we identify issues of SS tasks which allow a severely mode-collapsed generator to excel the SS tasks. To address the issues, we propose new SS tasks based on a multi-class minimax game. The competition between our proposed SS tasks in the game encourages the generator to learn the data distribution and generate diverse samples. We provide both theoretical and empirical analysis to support that our proposed SS tasks have better convergence property. We conduct experiments to incorporate our proposed SS tasks into two different GAN baseline models. Our approach establishes state-of-the-art FID scores on CIFAR-10, CIFAR-100, STL-10, CelebA, Imagenet  and Stacked-MNIST datasets, outperforming existing works by considerable margins in some cases. Our unconditional GAN model approaches performance of conditional GAN {\em without} using labeled data.  Our code:  \url{https://github.com/tntrung/msgan}






\end{abstract}
 
\section{Introduction}

{\bf Generative Adversarial Networks (GAN).} GAN~\cite{goodfellow-nisp-2014} have become one of the most important methods to learn generative models. GAN has shown remarkable results in various tasks, such as: image generation \cite{karras-iclr-2018,brock-iclr-2018,karras-cvpr-2019}, image transformation \cite{isola-cvpr-2017, zhu-cvpr-2017}, super-resolution \cite{ledig-cvpr-2017}, text to image \cite{reed-arxiv-2016,zhang2-cvpr-2017}, anomaly detection \cite{schlegl-ipmi-2017,lim-icdm-2018}. The idea behind GAN is the mini-max game. It uses a binary classifier, so-called the discriminator, to distinguish the data (real) versus generated (fake) samples. 
The generator of GAN is trained to confuse the discriminator to classify the generated samples as the real ones. 
By having the generator and discriminator competing with each other in this adversarial process, they are able to improve themselves. The end goal is to have the generator capturing  
the data distribution. Although considerable improvement has been made for GAN under the conditional settings \cite{odena-icml-2017,zhang-arxiv-2018,brock-iclr-2018}, i.e., using ground-truth labels to support the learning, it is still very challenging with unconditional setup. Fundamentally, using only a single signal (real/fake) to guide the generator to learn the high-dimensional, complex data distribution is very challenging \cite{goodfellow-nips-2016,arjovsky-arxiv-2017a,che-arxiv-2016,chen-arxiv-2016,metz-arxiv-2016,salimans-nisp-2016}.


{\bf Self-supervised Learning.}
Self-supervised learning is an active research area \cite{doersch-cvpr-2015,pathak-cvpr-2016,zhang-eccv-2016,zhang1-cvpr-2017,noroozi-iccv-2017,gidaris-iclr-2018}. Self-supervised learning is a paradigm of unsupervised learning.
Self-supervised methods
encourage the classifier to learn better feature representation with {\em pseudo-labels}. In particular, these methods propose to learn image feature by training the model to recognize some geometric transformation that is applied to the image which the model receives as the input.
A simple-yet-powerful method proposed in \cite{gidaris-iclr-2018}
is to use image rotations by 0, 90, 180, 270 degrees as the geometric transformation.
The model is trained with the 4-way classification task of  recognizing one of the four rotations. This task is referred as 
the {\em self-supervised task}. This simple method is able to close the gap between supervised and unsupervised image classification \cite{gidaris-iclr-2018}. 

{\bf Self-supervised Learning for GAN.}
Recently, self-supervised learning has been applied to GAN training
\cite{chen-arxiv-2018,tran-arxiv-2019}.  These works propose 
auxiliary 
self-supervised classification tasks to assist the main GAN task (Figure~\ref{proposed_model}).
In particular, their objective functions for learning discriminator  and 
generator   are 
multi-task loss as shown in (\ref{gan_dis_obj_ss1}) and (\ref{gan_gen_obj_ss1}) respectively:

\vspace{-0.5cm}

\vspace{-0.5cm}

Here,  in (\ref{gan_original}) is the {\em GAN task}, which is the original value function proposed in Goodfellow et al.~\cite{goodfellow-nisp-2014}.
 is true data distribution,  is the distribution induced by the generator mapping.
 and  are the {\em self-supervised (SS) tasks} for discriminator and generator learning, respectively (details to be discussed).  is the classifier for the self-supervised task, e.g. rotation classifier 
as discussed \cite{gidaris-iclr-2018}.
Based on this framework, Chen et al.\cite{chen-arxiv-2018} apply self-supervised task to help discriminator counter catastrophic forgetting. Empirically, they have shown that self-supervised task enables discriminator to learn more stable and improved representation. Tran et al.~\cite{tran-arxiv-2019} propose to improve self-supervised learning with adversarial training. 

Despite the encouraging empirical results, in-depth analysis of the interaction between SS tasks ( and ) and GAN task () has not been done before.
On one hand, the application of SS task for {\em discriminator learning} is reasonable: the goal of discriminator is to classify real/fake image; an additional SS classification task  could assist feature learning and enhance the GAN task. On the other hand, the motivation and design of SS task for {\em generator learning} is rather subtle: the goal of generator learning is to capture the data distribution in , and it is unclear exactly how an additional SS classification task  could help.






{\bf In this work,} we conduct in-depth empirical and theoretical analysis to understand the interaction between self-supervised tasks ( and ) and learning of generator .
Interestingly, from our analysis,
we reveal issues of existing works. 
Specifically, 
the SS tasks of existing works have ``loophole'' that, during generator learning,  could exploit to maximize  without truly learning the data distribution. We show that analytically and empirically that a severely mode-collapsed generator can excel . 
To address this issue, we
propose new SS tasks based on a multi-class minimax game.
Our proposed new SS tasks of discriminator and generator 
compete with each other to reach the equilibrium point. Through this competition, our proposed SS tasks are able to support the GAN task better. Specifically, our analysis shows that {\em our proposed SS tasks enhance matching between  and  by leveraging the transformed samples used in the SS classification} (rotated images when 
\cite{gidaris-iclr-2018} is applied).
In addition, our design couples GAN task and SS task. To validate our design, 
we provide theoretical analysis on the convergence property of our proposed SS tasks. Training a GAN with our proposed self-supervised tasks based on multi-class minimax game significantly improves baseline models. Overall, our system  establishes state-of-the-art Fr{\'e}chet Inception Distance (FID) scores.
In summary, our contributions are:
\begin{itemize}
	\item We conduct in-depth empirical and theoretical analysis to understand the issues of self-supervised tasks in existing works.
	\item Based on the analysis, we propose new self-supervised tasks based on a multi-class minimax game.
	\item We conduct extensive experiments to validate our proposed self-supervised tasks.
\end{itemize}

\begin{figure}
  \centering
\includegraphics[scale=0.7]{original_proposal}
\caption{The model of (a) SSGAN \cite{chen-arxiv-2018} and (b) our approach. Here,  and  are the self-supervised value functions in training discriminator and generator, respectively, as proposed in \cite{chen-arxiv-2018}.  and  are the self-supervised value functions proposed in this work.}
  \label{proposed_model}
  \vspace{-0.4cm}
\end{figure}





 
\section{Related works}

While training GAN with conditional signals (e.g., ground-truth labels of classes) has made good progress \cite{odena-icml-2017,zhang-arxiv-2018,brock-iclr-2018}, training GAN in the unconditional setting is still very challenging. In the original GAN \cite{goodfellow-nisp-2014}, the single signal (real or fake) of samples is provided to train discriminator and the generator. With these signals, the generator or discriminator may fall into ill-pose settings, and they may get stuck at bad local minimums though still satisfying the signal constraints. To overcome the problems, many regularizations have been proposed. One of the most popular approaches is to enforce (towards) Lipschitz condition of the discriminator. These methods include weight-clipping \cite{arjovsky-arxiv-2017a}, gradient penalty constraints \cite{gulrajani-arxiv-2017,roth-nips-2017,kodali-arxiv-2017,petzka-arxiv-2017,liu-arxiv-2018} and spectral norm \cite{miyato-iclr-2018}. Constraining the discriminator mitigates gradients vanishing and avoids sharp decision boundary between the real and fake classes. 




Using Lipschitz constraints improve the stability of GAN. However, the challenging optimization problem still remains when using a single supervisory signal, similar to the  original GAN \cite{goodfellow-nisp-2014}. In particular, the learning of discriminator is highly dependent on generated samples. If the generator collapses to some particular modes of data distribution, it is only able to create samples around these modes. 
There is no competition to train the discriminator around other modes. As a result, the gradients of these modes may vanish, and it is impossible for the generator to model well the entire data distribution. Using additional supervisory signals helps the optimization process. For example, using self-supervised learning in the form of auto-encoder has been proposed. AAE \cite{makhzani-arxiv-2015} guides the generator towards resembling realistic samples. 
However, an issue with using auto-encoder is that pixel-wise reconstruction with -norm causes blurry artifacts. VAE/GAN \cite{larsen-arxiv-2015}, which combining  VAE \cite{kingma-arxiv-2013} and GAN, is an improved solution: while the discriminator of GAN enables the usage of feature-wise reconstruction to overcome the blur, the VAE constrains the generator to mitigate mode collapse. In ALI \cite{dumoulin-arxiv-2016} and BiGAN \cite{donahue-arxiv-2016}, they jointly train the data/latent samples in the GAN framework. InfoGAN \cite{chen-arxiv-2016} infers the disentangled latent representation by maximizing the mutual information. In \cite{tran-eccv-2018,tran-aaai-2018}, they  combine two different types of supervisory signals: real/fake signals and self-supervised signal in the form of auto-encoder.  In addition, Auto-encoder based methods, including \cite{larsen-arxiv-2015,tran-eccv-2018,tran-aaai-2018},  can be considered as an approach  to mitigate catastrophic forgetting because they  regularize the generator to resemble the real ones. It is similar to EWC \cite{kirkpatrick-2017-nas} or IS \cite{zenke-arxiv-2017} but the regularization is achieved via the output, not the parameter itself. Although using feature-wise distance in auto-encoder could reconstruct sharper images, it is  still challenging to produce very realistic detail of textures or shapes.

Several different types of supervisory signal have been proposed. Instead of using only one discriminator or generator, they propose ensemble models, such as multiple discriminators \cite{tu-nips-2017}, mixture of generators \cite{hoang-arxiv-2018,ghosh-cvpr-2018} or applying an attacker as a new player for GAN training \cite{liu-cvpr-2019}. Recently, training model with auxiliary self-supervised constraints \cite{chen-arxiv-2018,tran-arxiv-2019} via multi pseudo-classes \cite{gidaris-iclr-2018} helps improve stability of the  optimization process. This approach is appealing: it is simple to implement and does not require more parameters in the networks (except a small head for the classifier).
Recent work applies InfoMax principle to improve GAN \cite{kwotsin:2019}.  Variational Autoencoder is  another important approach to learn generative models \cite{kingma-arxiv-2013,Yang_2019_ICCV}.







 
\section{GAN with Auxiliary Self-Supervised tasks}

In \cite{chen-arxiv-2018}, self-supervised (SS) value function (also referred as ``self-supervised task'') was proposed for GAN \cite{goodfellow-nisp-2014} via image rotation prediction \cite{gidaris-iclr-2018}. 
In their work, they showed that the SS task was useful to mitigate catastrophic forgetting problem of GAN discriminator. The objectives of the discriminator and generator in \cite{chen-arxiv-2018} are shown in Eq. \ref{gan_dis_obj_ss} and \ref{gan_gen_obj_ss}. Essentially, the SS task of the discriminator (denoted by ) is to train the classifier  that maximizes the performance of predicting 
the rotation applied to the {\em real} samples.
Given this classifier , 
the SS task of the generator (denoted by ) is to train the generator  to produce {\em fake} samples for maximizing classification performance. The discriminator and classifier are the same (shared parameters), except the last layer in order to implement  two different heads: the last fully-connected layer which returns a one-dimensional output (real or fake) for the discriminator,  and the other which returns a -dimensional softmax of pseudo-classes for the classifier.  and  are constants.


Here, the GAN value function  
(also referred as ``GAN task'')
can be the original minimax GAN objective \cite{goodfellow-nisp-2014} or other improved versions.
 is the set of transformation,  is the -th transformation.
The rotation SS task proposed in \cite{gidaris-iclr-2018} is applied, and  are the 0, 90, 180, 270 degree image rotation, respectively.
 are the distributions of real and fake data samples, respectively.
 are the mixture distribution of {\em rotated} real and fake data samples (by ), respectively.
Let  be the -th softmax output of classifier , and we have . The models are shown in Fig. \ref{proposed_model}a. In \cite{chen-arxiv-2018}, empirical evidence of improvements has been provided.

Note that, the goal of  is to encourage the generator to produce realistic images. It is because classifier  is trained with real images and captures features that allow detection of rotation. However, the interaction of  with the GAN task  has not been adequately analyzed. 



 
\section{Analysis on Auxiliary Self-supervised Tasks}
\label{analysis_on_auxiliary_ss}

We analyze the SS tasks in \cite{chen-arxiv-2018} (Figure \ref{proposed_model}a).
We assume that all networks  have enough capacity \cite{goodfellow-nisp-2014}. Refer to the Appendix \ref{appendix_a} for full derivation. Let  and  be the optimal discriminator and optimal classifier respectively at an equilibrium point. We assume that we have an optimal  of the GAN task. We focus on  of SS task. Let  be the probability of sample  
under transformation by 
(Figure~\ref{mixture_of_distribution}).  denotes the probability  of data sample () or generated sample () respectively.

\begin{figure}
  \centering
  \includegraphics[scale=0.43]{mixture_of_distribution}
  \caption{The probability distribution . Here, samples from  are rotated by . The distribution of rotated sample is .
Some rotated samples resemble the original samples, e.g. those on the right of . On the other hand, for some image, there is no rotated image resembling it, e.g.  (). The generator can learn to generate these images e.g.  to achieve maximum of , without actually learning the entire .
  }
\label{mixture_of_distribution}
   \vspace{-0.3cm}
\end{figure}


\begin{proposition}
The optimal classifier  of Eq. \ref{gan_dis_obj_ss} is:

\label{prop_1}
\end{proposition}
\noindent \textit{Proof}. Refer to our proof in Appendix \ref{appendix_a} for optimal .


\begin{theorem}
Given optimal classifier  for SS task ,
at the equilibrium point, maximizing SS task  of Eq. \ref{gan_gen_obj_ss} is equal to maximizing:

\label{theorem_1}
\end{theorem}
\noindent \textit{Proof}. Refer to our proof in Appendix \ref{appendix_a}.\\

Theorem \ref{theorem_1} depicts learning of generator 
given the optimal : selecting  (hence ) to 
maximize .
As  is trained on real data,   encourages   to learn to generate realistic samples. However, we argue that  can maximize  without actually learning data distribution . 
{\em In particular, it is sufficient for  to  maximize  by  simply learning to produce images which rotated version is rare (near zero probability).} Some example images are shown in Figure~\ref{classifiion_theorem_1}a. Intuitively, for these images, rotation can be easily recognized.

The argument can be developed from Theorem \ref{theorem_1}. From (\ref{g_obj_jsd_1}),
it can be shown that
 ( and ).
One way for  to achieve the maximum is to generate 
such that 
 and 
.
For these , the maximum 
 is attained. Note that  corresponds to 0 degree rotation, i.e., no rotation.
Recall that 
 is the probability distribution of transformed data by .
Therefore the condition 
 and 
means that there is no other rotated image resembling  , or equivalently, rotated  does not resemble any other images (Figure~\ref{mixture_of_distribution}). Therefore, the generator can exploit this ``loophole'' to maximize  without actually learning the data distribution. 
In particular, even a mode-collapsed generator can achieve the maximum of  by generating such images.


{\bf Empirical evidence.}
Empirically, our experiments (in Appendix \ref{ss_task_generator_learning}) show that  the FID of the models when using 

is poor except for very small
.
We further illustrate this issue by a toy empirical example using CIFAR-10.
We augment the  training images  with transformation data  to train the classifier  to predict 
the rotation applied to .
This is the SS task of discriminator in 
Figure \ref{proposed_model}a.
Given this classifier , we simulate the SS task of generator learning as follows.
To simulate the output of a good generator  which generates diverse realistic samples, we choose the full test set of CIFAR-10 (10 classes) images and compute the cross-entropy loss, i.e. , when they are fed into . 
To simulate the output of a mode-collapsed generator , we select samples from {\em one} class, e.g. ``horse'', and compute the cross-entropy loss when they are fed into .
Fig. \ref{classifiion_theorem_1}b show that some  can outperform  and achieve a smaller . E.g. a  that produces {\em only} ``horse'' samples outperform  under .  This example illustrates that, while 
 may help the generator to create more realistic samples, it does not help the generator to prevent mode collapse. {\em In fact, as part of the multi-task loss
(see (\ref{gan_gen_obj_ss})),  would undermine the learning of synthesizing diverse samples in the GAN task .}

\begin{figure}
  \begin{minipage}[c]{0.26\textwidth}
          \begin{flushright}
          \includegraphics[width=3.51cm]{attack_images}
          \end{flushright}
  \end{minipage}
  \begin{minipage}[c]{0.70\textwidth}
          \begin{center}
          \includegraphics[width=10.2cm]{SStoyexample}
          \vspace{-0.8cm}
          \end{center}
          \begin{center}
          \includegraphics[width=10.2cm]{MStoyexample}
          \end{center}
  \end{minipage}
  \caption{(a) Left: Example images that achieve minimal loss (or maximal ). For these images, rotation can be easily recognized: an image with a 90 degree rotated horse is likely due to applying  rather than an original one. (b) Right (Top): the loss of original SS task, i.e.  computed over a good generator (red) and collapsed generators (green, yellow). 
  Some collapsed generators (e.g. one that generates only ``horse'') have smaller loss than the good generator under .
  (c) Right (Bottom): the loss of proposed MS task, , of a good generator (red) and collapsed generators (green). The good generator has the smallest loss under .
  }
  \label{classifiion_theorem_1}
\end{figure}




\section{Proposed method}
\label{proposed_method}

\subsection{Auxiliary Self-Supervised Tasks with Multi-class Minimax Game}

In this section, we propose improved SS tasks to address the issue (Fig. \ref{proposed_model}b).
Based on a multi-class minimax game, our classifier learns to distinguish the rotated samples from real data versus those from generated data. 
Our proposed SS tasks are  
 and  in (\ref{gan_dis_obj_ss_adv}) and (\ref{gan_gen_obj_ss_adv}) respectively.
Our discriminator objective is: 

Eq. \ref{gan_dis_obj_ss_adv} means that we simultaneously distinguish generated samples, as the -th class, from the rotated real sample classes. Here,  is the -th output for the fake class of classifier . 

While rotated real samples are fixed samples that help prevent the classifier (discriminator) from forgetting, the class  serves as the connecting point between generator and classifier, and the generator can directly challenge the classifier. Our technique resembles the original GAN by Goodfellow et al. \cite{goodfellow-nisp-2014}, but we generalize it for multi-class minimax game. Our generator objective is:

 and  form a multi-class minimax game.
Note that, when we mention multi-class minimax game (or multi-class adversarial training), we refer to the SS tasks. The game for GAN task is the original by Goodfellow et al. \cite{goodfellow-nisp-2014}.  

\subsubsection{Theoretical Analysis}

\begin{proposition}
For fixed generator , the optimal solution  under Eq. \ref{gan_dis_obj_ss_adv} is:
 
where  and  are probability of sample  in the mixture distributions  and  respectively.
\label{prop_2}
\end{proposition}

\noindent \textit{Proof}. Refer to our proof in Appendix \ref{appendix_a} for optimal .

\begin{theorem}
Given optimal classifier  obtained from multi-class minimax training , at the equilibrium point, maximizing  is equal to maximizing Eq. \ref{g_obj_jsd_2}:

\label{theorem_2}
\end{theorem}

\noindent \textit{Proof}. Refer to our proof in Appendix \ref{appendix_a}.


Note that proposed SS task objective (\ref{g_obj_jsd_2}) is different from the original SS task objective (\ref{g_obj_jsd_1}) with the KL divergence term.
Furthermore, note that , as rotation  is an affine transform and KL divergence is invariant under affine transform (our proof in Appendix \ref{appendix_a}).
Therefore, the improvement is clear: {\em Proposed SS tasks  work together to improve the matching of  and  by leveraging the rotated samples.} For a given , feedbacks are computed from not only   but also  via the rotated samples. Therefore,  has more feedbacks to improve . We investigate the improvement of our method on toy dataset as in Section \ref{analysis_on_auxiliary_ss}. The setup is the same, except that now we replace models/cost functions of  with our proposed ones  (the design of  and  are the same). The loss now is shown in Fig. \ref{classifiion_theorem_1}c. Comparing Fig. \ref{classifiion_theorem_1}c and Fig. \ref{classifiion_theorem_1}b, the improvement using our proposed model can be observed:  has the lowest loss under our proposed model. Note that, since optimizing KL divergence is not easy because it is asymmetric and could be biased to one direction \cite{tu-nips-2017}, in our implementation, we use a slightly modified version as described in the Appendix.  
\section{Experiments}

We measure the diversity and quality of generated samples via the Fr\'echet Inception Distance (FID) \cite{heusel-arxiv-2017}. FID is computed with 10K real samples and 5K generated samples exactly as in \cite{miyato-iclr-2018} if not precisely mentioned. 
We report the best FID attained in 300K iterations as in \cite{xiang-arxiv-2017, li-nips-2017, tran-eccv-2018, yazici-arxiv-2018}.
We integrate our proposed techniques into two baseline models (SSGAN \cite{chen-arxiv-2018} and Dist-GAN \cite{tran-eccv-2018}). We conduct experiments mainly on CIFAR-10 and STL-10 (resized into  as in \cite{miyato-iclr-2018}). We also provide additional experiments of CIFAR-100, Imagenet  and Stacked-MNIST.

For Dist-GAN \cite{tran-eccv-2018}, we evaluate three versions implemented with different network architectures: DCGAN architecture \cite{radford-arxiv-2015}, CNN architectures of SN-GAN \cite{miyato-iclr-2018} (referred as SN-GAN architecture) and ResNet architecture \cite{gulrajani-arxiv-2017}. We recall these network architectures in Appendix \ref{network-architecture}. We use ResNet architecture \cite{gulrajani-arxiv-2017} for experiments of CIFAR-100, Imagenet , and tiny K/4, K/2 architectures \cite{metz-arxiv-2016} for Stacked MNIST. We keep all parameters suggested in the original work and focus to understand the contribution of our proposed techniques. For SSGAN \cite{chen-arxiv-2018}, we use the ResNet architecture as implemented in the official code\footnote{https://github.com/google/compare\_gan}.

In our experiments, we use
{\bf SS}
to denote the original self-supervised tasks proposed in \cite{chen-arxiv-2018},  and we use {\bf MS} to denote our proposed self-supervised tasks ``Multi-class mini-max game based Self-supervised tasks". Details of the experimental setup and network parameters are discussed in Appendix \ref{appendix_b}.

We have conducted extensive experiments. Setup and results are discussed in Appendix \ref{appendix_b}.
In this section, we highlight the main results:
\begin{itemize}
  \item Comparison between {\bf SS} and our proposed {\bf MS} using the same baseline.
  \item Comparison between our proposed {\bf baseline + MS} and other state-of-the-art unconditional and conditional GAN. We emphasize that our proposed {\bf baseline + MS} is unconditional and does not use any label.
\end{itemize}




\subsection{Comparison between {\bf SS} and our proposed {\bf MS} using the same baseline}
\label{compare_ss_ms}
Results are shown in 
Fig. \ref{ss_d_g_finetuning_all} using Dist-GAN \cite{tran-eccv-2018} as the baseline.
For each experiment and for each approach ({\bf SS} or {\bf MS}), 
we obtain the best  and  using extensive search (see Appendix \ref{appendix_state_of_the_art} for details), and we use the best  and  in the comparison depicted in Fig. \ref{ss_d_g_finetuning_all}. 
In our experiments, we observe that Dist-GAN has stable convergence. Therefore, we use it in these experiments. As shown in Fig. \ref{ss_d_g_finetuning_all}, our proposed {\bf MS} outperforms the original {\bf SS} consistently.
More details can be found in Appendix \ref{appendix_state_of_the_art}. 










\begin{figure}
  \centering
\includegraphics[width=3.3cm,keepaspectratio]{cifar_sngan_d_finetuning_fid_best}
  \includegraphics[width=3.3cm,keepaspectratio]{cifar_resnet_g_finetuning_fid_best}
  \includegraphics[width=3.3cm,keepaspectratio]{stl_sngan_g_finetuning_fid_best}
  \includegraphics[width=3.3cm,keepaspectratio]{stl_resnet_g_finetuning_fid_best}
\caption{Compare {\bf SS} (original SS tasks proposed in \cite{chen-arxiv-2018}) and {\bf MS} (our proposed Multi-class mini-max game based Self-supervised tasks). The {\bf baseline} is Dist-GAN \cite{tran-eccv-2018}, implemented with {\bf SN-GAN} networks (CNN architectures in \cite{miyato-iclr-2018}) and {\bf ResNet}. Two datasets are used, CIFAR-10 and STL-10. For each experiment, we use the best  for the models, obtained through extensive search (Appendix \ref{appendix_state_of_the_art}). Note that  is the best for ``Baseline + SS'' in all experiments. The results suggest consistent improvement using our proposed self-supervised tasks.
}
  \label{ss_d_g_finetuning_all}
\end{figure}




















\subsection{Comparison between our proposed method with other state-of-the-art GAN}

Main results are shown in Table \ref{state_of_the_art}. Details of this comparison can be found in Appendix \ref{appendix_state_of_the_art}. The best   and  as in Figure \ref{ss_d_g_finetuning_all} are used in this comparison.
The best FID attained in 300K iterations are reported as in \cite{xiang-arxiv-2017, li-nips-2017, tran-eccv-2018, yazici-arxiv-2018}.
Note that SN-GAN method \cite{miyato-iclr-2018} attains the best FID at about 100K iterations with ResNet and it diverges afterward. Similar observation is also discussed in \cite{chen-arxiv-2018}. 

As shown in Table \ref{state_of_the_art}, our method (Dist-GAN + MS) consistently outperforms the baseline Dist-GAN and other state-of-the-art GAN. These results confirm the effectiveness of our proposed self-supervised tasks based on multi-class minimax game.

\begin{table}
  \small
  \caption{Comparison with other state-of-the-art GAN on CIFAR-10 and STL-10 datasets. 
  We report the best FID of the methods.
  Two network architectures are used: {\bf SN-GAN} networks (CNN architectures in \cite{miyato-iclr-2018}) and {\bf ResNet}.
  The FID scores are extracted from the respective papers when available.
  {\bf SS} denotes the original SS tasks proposed in \cite{chen-arxiv-2018}. {\bf MS} denotes our proposed self-supervised tasks. 
  `*': FID is computed with 10K-10K samples as in \cite{chen-arxiv-2018}. All compared GAN are unconditional, except SAGAN and BigGAN.
  SSGAN is SS-GAN in \cite{chen-arxiv-2018} but using the best parameters we have obtained. 
  In SSGAN + MS, we replace the original {\bf SS} in author's code with our proposed {\bf MS}.}

  \label{state_of_the_art}
  \centering
  \begin{tabular}{llllll}
    \toprule
    & \multicolumn{2}{c}{\textbf{SN-GAN}} & \multicolumn{2}{c}{\textbf{ResNet}} \\
    \cmidrule(r){2-3}  \cmidrule(r){4-6}
    \textbf{Methods}  & \textbf{CIFAR-10}     & \textbf{STL-10}     & \textbf{CIFAR-10}   & \textbf{STL-10}  & \textbf{CIFAR-10}\\
    \midrule
    GAN-GP \cite{miyato-iclr-2018}    			      & 37.7   & -      & - & - & - \\
    WGAN-GP \cite{miyato-iclr-2018}    			      & 40.2   & 55.1   & - & - & - \\
    SN-GAN \cite{miyato-iclr-2018}    			      & 25.5   & 43.2   & 21.70  .21 & 40.10  .50  & 19.73 \\
SS-GAN \cite{chen-arxiv-2018}                     & -      & -      & -               & -               & 15.65 \\
    Dist-GAN \cite{tran-eccv-2018}  			      & 22.95  & 36.19  & 17.61  .30 & 28.50  .49  & 13.01 \\
    GN-GAN \cite{tran-aaai-2018}                      & 21.70  & 30.80  & 16.47  .28 & - & - \\
    \hline
    SAGAN \cite{zhang-arxiv-2018} (cond.)             & -  & -  & 13.4 (best) & - & - \\
    BigGAN \cite{brock-iclr-2018} (cond.)             & -  & -  & 14.73       & - & - \\
    \hline
    SSGAN                                         & -  & -  & -  & -  & 20.47 \\
    \textbf{Ours(SSGAN + MS)}                     & -  & -  & -  & -  & 19.89 \\
    \hline

    Dist-GAN + SS                                     & 21.40  & 29.79 & 14.97  .29 & 27.98  .38 & 12.37 \\ 

    \textbf{Ours(Dist-GAN + MS)}                      & \textbf{18.88}  & \textbf{27.95} & \textbf{13.90  .22} & \textbf{27.10  .34} & \textbf{11.40} \\
    \bottomrule
  \end{tabular}
  \vspace{-0.4cm}
\end{table}


We have also extracted the FID reported in \cite{chen-arxiv-2018}, i.e. SSGAN with the original SS tasks proposed there. In  this case, we follow exactly their settings and compute FID using 10K real samples and 10K fake samples. Our model achieves better FID score than SSGAN with exactly the same ResNet architecture on CIFAR-10 dataset.  See results under the column CIFAR-10 in Table \ref{state_of_the_art}.

Note that we have tried to reproduce the results of SSGAN 
using its published code, but we were unable to achieve similar results as reported in the original paper \cite{chen-arxiv-2018}. 
We have performed extensive search and we use the obtained best parameter to report the results as 
SSGAN in Table \ref{state_of_the_art} (i.e., SSGAN uses the published code and the best parameters we obtained). 
We use this code and setup to 
compare 
{\bf SS}  and {\bf MS}, i.e. we replace the {\bf SS} code in the system with {\bf MS} code, and obtain 
``SSGAN + MS''. As shown in Table \ref{state_of_the_art}, our ``SSGAN + MS'' achieves better FID than  SSGAN. 
The improvement is consistent with Figure \ref{ss_d_g_finetuning_all} when Dist-GAN is used as the baseline.
More detailed experiments can be found in the Appendix. 
We have also compared SSGAN and our system (SSGAN + MS) on CelebA (). In this experiment, we use a small DCGAN architecture provided in the authors' code. Our proposed MS outperforms the original SS, with FID improved  
from  to . This experiment  again confirms the effectiveness of our proposed MS.


We conduct additional experiments on CIFAR-100 and ImageNet 3232 to compare \textbf{SS} and \textbf{MS} with Dist-GAN baseline. We use the same ResNet architecture as Section \ref{appendix_state_of_the_art} on CIFAR-10 for this study, and we use the best parameters  and  selected in Section \ref{appendix_state_of_the_art} for ResNet architecture. Experimental results in Table  \ref{new_experiments_cifar100_imagenet} show that our \textbf{MS} consistently outperform \textbf{SS} for  all benchmark datasets. For ImageNet 3232 we report the \textit{best} FID for \textbf{SS} because the model suffers serious mode collapse at the end of training. Our \textbf{MS} achieves the best performance at the end of training.

\begin{table}
    \small
    \centering
    \caption{Results on CIFAR-100 and ImageNet 3232. We use baseline model Dist-GAN with ResNet architecture. We  follow the same experiment setup as above. {\bf SS}: proposed in [4]; {\bf MS}: this work.}
    \begin{tabular}{ c  c  c }
    \toprule
    \textbf{Datasets} & \textbf{SS} & \textbf{MS} \\
    \hline
    CIFAR-100 (10K-5K FID)                    & 21.02 & 19.74  \\
ImageNet 3232 (10K-10K FID)       & 17.1 & 12.3 \\
    \bottomrule
    \end{tabular}
    \label{new_experiments_cifar100_imagenet}
    \vspace{-0.4cm}
\end{table}

We also evaluate the diversity of our generator on Stacked MNIST \cite{metz-arxiv-2016}. Each image of this dataset is synthesized by stacking any three random MNIST digits. We follow exactly the same experiment setup with tiny architectures ,  and evaluation protocol of \cite{metz-arxiv-2016}. We measure the quality of methods by the number of covered modes (higher is better) and KL divergence (lower is better). Refer to \cite{metz-arxiv-2016} for more details. Table. \ref{new_experiments_stacked_mnist} shows that our proposed \textbf{MS} outperforms \textbf{SS} for both mode number and KL divergence. Our approach  significantly outperforms state-of-the-art \cite{tran-eccv-2018,karras-iclr-2018}. The means and standard deviations of \textbf{MS} and \textbf{SS} are computed from eight runs (we re-train our GAN model from the scratch for each run). The results are reported with best  of :  for  architecture and  for  architecture. Similarly, best  of :  for  architecture and  for  architecture.

\begin{table}
    \centering
    \scriptsize
    \caption{Comparing to state-of-the-art methods on Stacked MNIST with tiny  and  architectures \cite{metz-arxiv-2016}. We also follow the same experiment setup of \cite{metz-arxiv-2016}. Baseline model: Dist-GAN. {\bf SS}: proposed in \cite{chen-arxiv-2018}; {\bf MS}: this work. Our method {\bf MS} achieves the best results for this dataset with both architectures, outperforming state-of-the-art \cite{tran-eccv-2018,karras-iclr-2018} by a significant margin.}
    \begin{tabular}{ c  c  c  c  c  c  c }
    \toprule
    \textbf{Arch} & \textbf{Unrolled GAN \cite{metz-arxiv-2016}} & \textbf{WGAN-GP \cite{gulrajani-arxiv-2017}} & \textbf{Dist-GAN \cite{tran-eccv-2018}} & \textbf{Pro-GAN \cite{karras-iclr-2018}} & \textbf{\cite{tran-eccv-2018}+SS} & \textbf{Ours(\cite{tran-eccv-2018}+MS)}\\ 
    \hline
    \textbf{K/4, \#} & 372.2  20.7 & 640.1  136.3 & 859.5  68.7 & 859.5  36.2 & 906.75  26.15 & 926.75  32.65 \\ 
    \textbf{K/4, KL} & 4.66  0.46 & 1.97  0.70 & 1.04  0.29 & 1.05  0.09 & 0.90  0.13 & 0.78  0.13\\
    \hline
    \textbf{K/2, \#} & 817.4  39.9 & 772.4  146.5 & 917.9  69.6 & 919.8  35.1 & 957.50  31.23 & 976.00  10.04\\ 
    \textbf{K/2, KL} & 1.43  0.12 & 1.35  0.55  & 1.06  0.23 & 0.82  0.13 & 0.61  0.15 & 0.52  0.07\\
    \bottomrule
    \end{tabular}
    \label{new_experiments_stacked_mnist}
    \vspace{-0.4cm}
\end{table}

Finally, in Table \ref{state_of_the_art}, we compare our FID to SAGAN \cite{zhang-arxiv-2018} (a state-of-the-art conditional GAN) and BigGAN \cite{brock-iclr-2018}. We perform the experiments under the same conditions using ResNet architecture on the  CIFAR-10 dataset. We report the best FID that SAGAN can achieve. As SAGAN paper does not have CIFAR-10 results \cite{zhang-arxiv-2018}, we run the published SAGAN code and select the best parameters to obtain the results for CIFAR-10. For BigGAN, we extract best FID from original paper. Although our method is unconditional, our best FID is very close  to that of these state-of-the-art conditional GAN. This validates the effectiveness of our design. 
Generated images using our system can be found in 
Figures \ref{cifar_resnet_samples} and \ref{stl_resnet_samples} of Appendix \ref{appendix_b}.



 
\section{Conclusion}

We provide theoretical and empirical analysis on auxiliary self-supervised task for GAN.
Our analysis reveals
the limitation of the existing work. To address the limitation, we propose multi-class minimax game based self-supervised tasks.
Our 
proposed self-supervised tasks leverage the rotated samples to provide better feedback in matching the data and generator distributions.
Our theoretical and empirical analysis support improved convergence of our design.
Our proposed SS tasks can be easily incorporated into existing GAN models. Experiment results suggest that they help boost the performance of baseline implemented with various network architectures on the CIFAR-10, CIFAR-100, STL-10, CelebA, Imagenet , and Stacked-MNIST datasets. The best version of our proposed method establishes state-of-the-art FID scores on all these benchmark datasets.

 





\pagebreak

\subsubsection*{Acknowledgements}

This work was  supported by ST Electronics and the National Research Foundation(NRF), Prime Minister's Office, Singapore under Corporate Laboratory @ University Scheme (Programme Title: STEE Infosec - SUTD Corporate Laboratory). This research was also supported by the National Research Foundation Singapore under its AI Singapore Programme [Award Number: AISG-100E-2018-005].
This research was also supported in part by the Energy Market Authority (EP award no. NRF2017EWT-EP003-061).
This project was  also supported by SUTD project PIE-SGP-AI-2018-01.
 
  
{\small
\bibliographystyle{plain} \bibliography{biblio}
}



\clearpage
\appendix

In our paper, we perform an in-depth analysis to understand how SS tasks interact with the learning of the generator. We analyze the issues of SS tasks and propose to improve it with a multi-class minimax game. 
In this Appendix section, we provide detail information about our proofs, discussion, ablation study, network parameters and network architectures of models.

\section{Appendix: Proofs for Sections \ref{analysis_on_auxiliary_ss} and \ref{proposed_method}}
\label{appendix_a}

\noindent \textbf{Proposition \ref{prop_1}} (Proof.)

Let  be the -th type of transformation, and let  be the distribution of the transformed real sample.
This section shows the proof for optimal .  is the -th soft-max output of , hence .  can be re-written as: 



where  is the probability that  belongs to class , which can be considered as the the -th output of ``ground-truth'' classifier on sample  we expect the classifier  to predict. Assume that  has first-order derivative with respective to . The optimal solution of  can be obtained via setting this derivative equal to zero:



For any , setting , and the value of optimal  has the following form:



Note that , according to Bayes' theorem , and  (the probability we apply the transformations  for sample  are equal), We finally obtain the optimal  from Eq. \ref{ck_solution}: . That concludes our proof.

\noindent \textbf{Theorem \ref{theorem_1}} (Proof.) Substitute  obtained above into :



Substitute   into (\ref{g_obj_1}) we have: 



That concludes our proof.
 


\noindent \textbf{Proposition \ref{prop_2}} (Proof.) Training self-supervised task  with minimax game is similar to previous objective, except the additional term of fake class as below:



Assume that  has first-order derivative with respective to . The optimal  can be derived via setting derivative of  equal to zero as follows:



Similar to above, for any , we have the derivative :



Setting , and we get optimal , :



With , we obtain the derivative of :



Setting , and finally we get optimal , :



Because , we finally obtain the optimal  from Eq. \ref{ck_solution_adv}: . That concludes the proof.

\noindent \textbf{Theorem \ref{theorem_2}} (Proof.) Substitute optimal  obtained above into  :



The first term can be written as:



With the note that  and . Moving the first term of Eq. \ref{g_obj_lower_bound} from the right side to left side, it concludes the proof.

\begin{theorem}
KL divergence is invariant to affine transform.
\end{theorem}

\textit{Proofs}. Let  be a random variable. 
is a distribution defined on . Let  be an affine
transform, i.e., ,
where  is a full rank matrix and .
Then for a random variable ,
, where 
is the Jacobian matrix, with its -th entry defined as:

Obviously, . Then we have
.

Let  and  are two distributions
defined on . Then let  and 
be the corresponding distributions defined on .
Then we have 
and .

Using the definition of the KL divergence between  and ,
we have:



As , then we have:



According to the property of multiple integral, we have:



It concludes our proof.

\begin{corollary}
KL divergence between real and fake distributions is equal to that of rotated real and rotated fake distributions by : 
\end{corollary}


Note that we apply the above theorem of invariance of KL, with  being  respectively, and image rotation  as the transform.

\subsection{Implementation}

Here, we discuss  details of our implementation. For the SS tasks, we follow the geometric transformation of \cite{gidaris-iclr-2018} to argument images and compute pseudo labels. It is simple yet effective and currently the state-of-the-art in self-supervised tasks. In particular, we train discriminator to recognize the 2D rotations which were applied to the input image. We rotate the input image with  rotations () and assign them the pseudo-labels from 1 to .

To implement our model, the GAN objectives for discriminator and generator can be the ones in original  GAN by Goodfellow et al. \cite{goodfellow-nisp-2014}, or other variants. In our work, we conduct experiments to show improvements with two baseline models: original SSGAN \cite{chen-arxiv-2018} and DistGAN \cite{tran-eccv-2018}.

We integrate SS tasks into Dist-GAN \cite{tran-eccv-2018} and conduct  study with this baseline. 
In our experiments, we observe that Dist-GAN has good convergence property and  this is important for our ablation study.







Second, in practice, achieving equilibrium point for optimal D, G, C is difficult. Therefore, inspired by \cite{tran-eccv-2018}, we propose the new generator objective to improve Eq. \ref{gan_gen_obj_ss_adv} as written in Eq. \ref{g_ss_maximum_entropy_loss}. It couples the convergence of  and  that allows the learning is more stable. Our intuition is that if generator distribution is similar to the real distribution, the classification performance on its transformed fake samples should be similar to that of those from real samples. Therefore, we propose to match the self-supervised tasks of real and fake samples to train the generator. In other words, if real and fake samples are from similar distributions, the same tasks applied for real and fake samples should have resulted in similar behaviors. In particular, given the cross-entropy loss computed on real samples, we train the generator to create samples that are able to match this loss. Here, we use -norm for the  and  is the objective of GAN task \cite{tran-eccv-2018}. In our implementation, we randomly select a geometric transformation  for each data sample when training the discriminator. And the same  are applied for generated samples when matching the self-supervised tasks to train the generator. 

%
 
For this objective of generator, similar to Eq. \ref{g_obj_lower_bound}, we have: 



The objective of Eq. \ref{g_ss_maximum_entropy_loss} can be re-written as:



 is the solution that minimizes Eq. \ref{g_obj_bound}. 
In practice, we found that this is stable. It is due to the stability of symmetric KL divergence (forward KL and inverse KL).




\section{Appendix: Experiments}
\label{appendix_b}



\begin{figure}
  \centering
\includegraphics[scale=0.74]{cifar_image_300000_real}
  \includegraphics[scale=0.74]{cifar_image_300000_real_argu}
  \includegraphics[scale=0.74]{cifar_resnet_image_300000_mixe_argu}
  \includegraphics[scale=0.74]{cifar_resnet_image_300000_fake}
  \caption{From left to right: Real samples, argument real samples by rotation, mixed argument real and fake samples, and generated images of CIFAR-10.}
  \label{cifar_resnet_samples}
\end{figure}

\begin{figure}
  \centering
\includegraphics[scale=0.50]{stl_image_300000_real}
  \includegraphics[scale=0.50]{stl_image_300000_real_argu}
  \includegraphics[scale=0.50]{stl_image_300000_mixe_argu}
  \includegraphics[scale=0.50]{stl_image_300000_fake}
  \caption{From left to right: Real samples, argument real samples by rotation, mixed argument real and fake samples, and generated images of STL-10.}
  \label{stl_resnet_samples}
\end{figure}

\subsection{Details of experiment setup}
\label{ablation_study_ld}

In our experiments, FID is computed every 10K iterations in training and visualized with the smoothening windows of 5. The latent dimension is  and mini-batch size is 64 for our all experiments. We visualize losses and FID scores in several figures. In these figures, the horizontal axis is the number of training iterations, and the vertical axis is either the loss and FID score. We compute the negative  discriminator/classifier value function for the visualization. We investigate the improvements of our proposed techniques on two baseline models:

Dist-GAN \cite{tran-eccv-2018}: We use Dist-GAN implemented with three network architectures: DCGAN, CNN in SN-GAN and ResNet.
We use standard ``log" loss for DCGAN architecture, and  with ``hinge" loss SN-GAN (the CNN network as in SN-GAN \cite{miyato-iclr-2018}) and ResNet architectures. We use ``hinge" loss for SN-GAN  and ResNet because it attains better performance than standard ``log" loss as shown in \cite{miyato-iclr-2018}. 
We train models using Adam optimizer with learning rate , ,  for DCGAN and SN-GAN architectures and ,  for ResNet architecture \cite{gulrajani-arxiv-2017}. {\bf If not precisely mentioned, it means Dist-GAN is used for the experiments.}

SSGAN: We were unable to reproduce results as reported in the original paper with this code\footnote{https://github.com/google/compare\_gan}, although we have followed the best parameter settings of the paper and communicated with authors of SSGAN regarding the issues. We achieve best results with another setting (spectral norm, ).
We use this setting as the baseline and compare to the one using our proposed SS tasks instead of the original SS tasks.


\subsection{Ablation study SS in Discriminator and Generator Learning for the original SS proposed in 
\cite{chen-arxiv-2018}}



\label{appendix_ablation_discriminator_learning}

In this experiment, we analyze original SS tasks proposed in \cite{chen-arxiv-2018} to understand the effect of self-supervised tasks.
We aim to provide empirical observation of how the  contributes to the discriminator via changing  with fixed . Experiments are on CIFAR-10 dataset using small DCGAN architecture. For implementation, they are integrated into the discriminator of the baseline model, Dist-GAN \cite{tran-eccv-2018} as mentioned above. Through the experiment, we confirm that the contribution of  is important in Dist-GAN model. We should set the  attain the good trade-off between GAN task and SS task because increasing  is not helpful. The SS task with  is good for Dist-GAN model, which is also discussed in \cite{chen-arxiv-2018} with SN-GAN model \cite{miyato-iclr-2018}. 



\begin{figure}
  \centering
\includegraphics[width=6.5cm,keepaspectratio]{cifar_dcgan_d_finetuning_ss_loss}
  \includegraphics[width=6.5cm,keepaspectratio]{cifar_dcgan_d_finetuning_gan_loss}
\includegraphics[width=6.5cm,keepaspectratio]{cifar_dcgan_ld_feature_quality}
  \includegraphics[width=6.5cm,keepaspectratio]{cifar_dcgan_d_finetuning_fid}
  \caption{The ablation study of SS task  as proposed in \cite{chen-arxiv-2018}. We analyze its effect via  fine-tuning, . (a) The discriminator losses of SS task, (b) The discriminator losses of GAN task, (c) the feature representation quality and (d) FID scores. With  for SS task, the model becomes seriously collapsed with FID > 100. Experiments are conducted with the baseline model, Dist-GAN. (Best view in color).}
  \label{ss_finetuning_dcgan}
\end{figure}
 
The results in Fig. \ref{ss_finetuning_dcgan} illustrate the effects of  to GAN with different values of . Fig. \ref{ss_finetuning_dcgan}a represents the losses of the SS task of the discriminator. It shows that in most cases, the larger  lead to faster  loss converges. However, when , the FID is not improved. We observe that once  is higher, the loss of GAN task is dominated by the SS tasks. When  is too high, (e.g., ), GAN loss is almost unchanged about first 10K iterations (Fig. \ref{ss_finetuning_dcgan}b) in early iterations and the model gets collapsed. 
This can be explained as follows.
When the discriminator improvement is slow due to the strong dominance of , the learning of the generator faster. This serious unbalance easily leads to the collapsed generator and the learning of generator gets stuck thereafter. When the GAN loss is strongly dominated by SS loss, the loss of GAN is saturated.

To understand deeper, we evaluate the representation qualities of the intermediate layers of the discriminator as in  \cite{chen-arxiv-2018} in this experiment. Given the above pre-trained discriminators, we compute features of train and test sets of CIFAR-10 via its last convolution layer. We evaluate the classification performance as training logistic regression on these features and measure with top-1 accuracy. We follow the experimental setup of parameters as in \cite{chen-arxiv-2018}. The result (Fig. \ref{ss_finetuning_dcgan}c) that as  >= 1.0, the accuracy is also similar, except for the case , the quality of feature is slightly worse but not too significant (although the GAN model is collapsed). It means increasing  does not necessarily improve the feature representation quality of the discriminator. 

Overall, 
with Dist-GAN as baseline, we observe that using the original SS tasks with  provide considerable improvement, and the results suggest that  should not be too high, but instead the one that provides a good trade-off between GAN and SS tasks. 


\subsubsection{SS task in Generator Learning}
\label{ss_task_generator_learning}


We continue to investigate the effects of  with fixed 
for the SS tasks proposed in 
\cite{chen-arxiv-2018}.
The experimental setup is similar to the previous one. The result represented in Fig. \ref{cifar_dcgan_dinetuning_g_d}a show that  still improves the baseline model, but higher than the case of . 
Note that 
\cite{chen-arxiv-2018} does not report result with .




\begin{figure}
  \centering
  \includegraphics[width=4.5cm,keepaspectratio]{ssgan_cifar_v1}
  \includegraphics[width=4.5cm,keepaspectratio]{theorem_1_cifar_dcgan_distgan_ss_ld_1_lg_finetuning}
  \includegraphics[width=4.5cm,keepaspectratio]{cifar_dcgan_ss_ld_1_lg_finetuning_best}
\caption{The ablation studies with (a) SSGAN using  and ours (SSGAN + MS) (b) Dist-GAN with  and  as fine-tuning , fixed . (c) Our model (Dist-GAN + MS) with  and  with  and . Experiments are with CIFAR-10 dataset. (Best view in color)}
  \label{cifar_dcgan_dinetuning_g_d}
\end{figure}



Following our discussion 
on  Theorem \ref{theorem_1}, applying  as proposed 
in 
\cite{chen-arxiv-2018}
does not support the matching between the generator and data distributions. From these experiments, we observe that the generator and discriminator are unable to reach optimal points, and using large  degrades the quality of GAN task, and even leads to mode collapse. For example, as   increases (eg. ), it seriously hurts the quality of GAN task of the generator. 


In addition, we verify with original code of SSGAN \cite{chen-arxiv-2018} on CIFAR-10 using our best setting mentioned above. Fig. \ref{cifar_dcgan_dinetuning_g_d}a confirms that with our best setting  and  achieve similar FID and increasing  degrades its performance, which is consistent to our analysis. In the same figure, when we use our proposed MS,  FID is improved.





\subsection{Ablation study (, ) with DCGAN for our proposed method}
\label{ablation_study_minimax_dcgan}

\begin{figure}
  \centering
  \includegraphics[width=6.5cm,keepaspectratio]{cifar_dcgan_ss_ld_1_lg_finetuning}
  \includegraphics[width=6.5cm,keepaspectratio]{cifar_dcgan_ss_lg_0dot1_ld_finetuning}
  \caption{Our model (Dist-GAN + MS) with (a) with fine-tuning , fixed . (b) fine-tuning , fixed . The baseline is Dist-GAN model, and we use DCGAN architecture. (Best view in color)}
  \label{cifar_dcgan_finetuning_g_d}
\end{figure}

We first change the  according  (Fig. \ref{cifar_dcgan_finetuning_g_d}a). With minimax game, the result suggests that  is the best for DCGAN architecture. Then, we seek  with this  as shown in Fig. \ref{cifar_dcgan_finetuning_g_d}b. Interestingly, now the best , which is higher than  of the original SS (the best with the original SS; Fig. \ref{ss_finetuning_dcgan}d). This suggests that using our proposed mini-max game based SS enable larger range of  with stable performance. 



\subsection{Ablation study of our proposed method with SN-GAN and ResNet architectures}
\label{appendix_state_of_the_art}

The detail of the ablation study of  and  for our proposed SS tasks using SN-GAN and ResNet architectures are shown in Fig. \ref{appendix_ss_d_g_finetuning_all}.


\begin{figure}
  \centering
\includegraphics[width=3.3cm,keepaspectratio]{cifar_sngan_g_finetuning_fid}
  \includegraphics[width=3.3cm,keepaspectratio]{cifar_resnet_g_finetuning_fid}
  \includegraphics[width=3.3cm,keepaspectratio]{stl_sngan_g_finetuning_fid}
  \includegraphics[width=3.3cm,keepaspectratio]{stl_resnet_g_finetuning_fid}  
  \includegraphics[width=3.3cm,keepaspectratio]{cifar_sngan_d_finetuning_fid}
  \includegraphics[width=3.3cm,keepaspectratio]{cifar_resnet_d_finetuning_fid}
  \includegraphics[width=3.3cm,keepaspectratio]{stl_sngan_d_finetuning_fid}
  \includegraphics[width=3.3cm,keepaspectratio]{stl_resnet_d_finetuning_fid}
\caption{Understanding the effects of MS tasks (our proposed self-supervised tasks), by  fine-tuning  (first row)  (second row) for CIFAR-10 and STL-10 with other architectures. From left to right: SN-GAN for CIFAR-10, ResNet for CIFAR-10, SN-GAN for STL-10 and ResNet for STL-10. SN-GAN architecture referred as CNN architectures used in SN-GAN \cite{miyato-iclr-2018}.}
  \label{appendix_ss_d_g_finetuning_all} 
\end{figure}













\section{Appendix: Network architectures}

\label{network-architecture}

\subsection{DCGAN architecture}

Our DCGAN architecture, which is used for ablation studies on CIFAR-10, are presented in Table. \ref{dcgan}.

\begin{table}[ht!]
	\caption{\label{dcgan}Our DCGAN architecture is similar to \cite{radford-arxiv-2015} but the smaller number of feature maps (D = 64) to be more efficient for our ablation study on CIFAR-10. The Encoder is the mirror of the Generator. Slopes of lReLU functions are set to .  is the uniform distribution.}
   	\centering
   	\scriptsize
    \begin{subtable}{.32\linewidth}
    	\centering
    	{\begin{tabular}{c}
			\toprule
			\midrule
		 	RGB image  \\
            \midrule
            55, stride=2 conv. 1  D ReLU\\
            \midrule
            55, stride=2 conv. BN 2  D ReLU\\        		 	
			\midrule
            55, stride=2 conv. BN 4  D ReLU\\            	
            \midrule
            55, stride=2 conv. BN 8  D ReLU\\    
            \midrule          
            dense  128 \\
            \midrule
			\bottomrule
		\end{tabular}}
        \caption{\label{tab:enc_dcgan}Encoder,  for CIFAR-10}
    \end{subtable}   	
    \begin{subtable}{.32\linewidth}
    	\centering
    	{\begin{tabular}{c}
			\toprule
			\midrule
		 	 \\	 	
           	\midrule
            dense  2  2  8  D  \\
            \midrule
            55, stride=2 deconv. BN 4  D ReLU\\
			\midrule
            55, stride=2 deconv. BN 2  D ReLU\\
            \midrule
            55, stride=2 deconv. BN 1  D ReLU\\
            \midrule
            55, stride=2 deconv. 3 Sigmoid\\	
            \midrule
			\bottomrule
		\end{tabular}}
        \caption{\label{tab:gen_dcgan}Generator for CIFAR-10}
    \end{subtable}
    \begin{subtable}{.34\linewidth}
    	\centering
    	{\begin{tabular}{c}
			\toprule
			\midrule
			RGB image  \\
            \midrule
            55, stride=2 conv. 1  D lReLU\\
            \midrule
            55, stride=2 conv. BN 2  D lReLU\\        		 	
			\midrule
            55, stride=2 conv. BN 4  D lReLU\\            	
            \midrule
            55, stride=2 conv. BN 8  D lReLU\\    
            \midrule
            dense  1, dense  5 (two heads) \\
            \midrule
			\bottomrule
		\end{tabular}}
        \caption{\label{tab:dis_dcgan}Discriminator,  for CIFAR-10. Two heads for the real/fake discriminator and multi-class classifier.}
    \end{subtable}
\end{table}


\subsection{SNGAN architecture}

Our SN-GAN architecture referred as CNN architectures of \cite{miyato-iclr-2018} for CIFAR-10 and STL-10 datasets are presented in Table. \ref{tab:sngan_models}.

\begin{table}[ht!]
	\caption{\label{tab:sngan_models} Encoder, generator, and discriminator of standard CNN architectures for CIFAR-10 and STL-10 used in our experiments. We use similar architectures as ones in \cite{miyato-iclr-2018}. The Encoder is the mirror of the Generator. Slopes of lReLU functions are set to .  is the uniform distribution.}
   	\centering
   	\scriptsize
    \begin{subtable}{.33\linewidth}
    	\centering
    	{\begin{tabular}{c}
			\toprule
			\midrule
		 	RGB image  \\
            \midrule
            33, stride=1 conv. 64\\	
            \midrule
            44, stride=2 conv. BN 128 ReLU\\            		 	
			\midrule
            44, stride=2 conv. BN 256 ReLU\\           	
            \midrule
            44, stride=2 conv. BN 512 ReLU\\  
            \midrule          
            dense  128 \\
            \midrule
			\bottomrule
		\end{tabular}}
        \caption{\label{tab:enc}Encoder,  for CIFAR-10, and  for STL-10}
    \end{subtable}   	
    \begin{subtable}{.33\linewidth}
    	\centering
    	{\begin{tabular}{c}
			\toprule
			\midrule
		 	 \\	 	
           	\midrule
            dense      512 \\
            \midrule
            44, stride=2 deconv. BN 256 ReLU\\
			\midrule
            44, stride=2 deconv. BN 128 ReLU\\
            \midrule
            44, stride=2 deconv. BN 64 ReLU\\
            \midrule
            33, stride=1 conv. 3 Sigmoid\\	
            \midrule
			\bottomrule
		\end{tabular}}
        \caption{\label{tab:gen}Generator,   for CIFAR-10, and  for STL-10}
    \end{subtable}
    \begin{subtable}{.32\linewidth}
    	\centering
    	{\begin{tabular}{c}
			\toprule
			\midrule
		 	RGB image  \\
			\midrule
            33, stride=1 conv 64 lReLU\\
            44, stride=2 conv 64 lReLU\\
            \midrule
            33, stride=1 conv 128 lReLU\\
            44, stride=2 conv 128 lReLU\\
            \midrule
            33, stride=1 conv 256 lReLU\\
            44, stride=2 conv 256 lReLU\\
            \midrule
            33, stride=1 conv. 512 lReLU\\
            \midrule
            dense  1, dense  5 (two heads)\\
			\bottomrule
		\end{tabular}}
        \caption{\label{tab:dis_deep}Discriminator,  for CIFAR-10, and  for STL-10. Two heads for the real/fake discriminator and multi-class classifier.}
    \end{subtable}
\end{table}

\subsection{ResNet architecture}

Our ResNet architectures for CIFAR-10 and STL-10 are presented in Table. \ref{tab:resnets_cifar10} and Table. \ref{tab:resnets_stl}.

\begin{figure}[ht!]
	\begin{tabular}{cc}
	     \scriptsize
        \begin{minipage}{1.\textwidth}
          \tblcaption{\label{tab:resnets_cifar10}ResNet architecture for CIFAR10 dataset. The Encoder is the mirror of the Generator. We use similar architectures and ResBlock to the ones used in \cite{miyato-iclr-2018}.  is the uniform distribution.}
          \centering
          \begin{subtable}{.33\textwidth}
                        \centering
                        {\begin{tabular}{c}
                            \toprule
                            \midrule
                            RGB image  \\
                            \midrule
                            33 stride=1, conv. 256\\ 
                            \midrule
                            ResBlock down 256\\                
                            \midrule
                            ResBlock down 256\\
                            \midrule
                            ResBlock down 256\\                                                                   
                            \midrule
                            dense  128 \\
                            \midrule
                            \bottomrule
                        \end{tabular}}
                        \caption{Encoder}
                    \end{subtable}
          \begin{subtable}{.33\textwidth}
              \centering
              {\begin{tabular}{c}
                  \toprule
                  \midrule
                   \\
                  \midrule
                  dense,  \\
                  \midrule
                  ResBlock up 256\\
                  \midrule
                  ResBlock up 256\\
                  \midrule
                  ResBlock up 256\\
                  \midrule
                  BN, ReLU, 33 conv, 3 Sigmoid\\
                  \midrule
                  \bottomrule
              \end{tabular}}
              \caption{Generator}
          \end{subtable}
          \begin{subtable}{.32\textwidth}
              \centering
              {\begin{tabular}{c}
                  \toprule
                  \midrule
                  RGB image  \\
                  \midrule
                  ResBlock down 128\\
                  \midrule
                  ResBlock down 128\\
                  \midrule
                  ResBlock 128\\
                  \midrule
                  ResBlock 128\\
                  \midrule
                  ReLU\\
                  \midrule
                  Global sum pooling\\
                  \midrule
                  dense  1, dense  5 (two heads)\\
                  \midrule
                  \bottomrule
              \end{tabular}}
              \caption{Discriminator. Two heads for the real/fake discriminator and multi-class classifier.}
          \end{subtable}
        \end{minipage}
    \end{tabular}
\end{figure}

\begin{table}[ht!]
          \caption{\label{tab:resnets_stl}ResNet architecture for STL-10 dataset. The Encoder is the mirror of the Generator. We use similar architectures and ResBlock to the ones used in \cite{miyato-iclr-2018}.  is the uniform distribution.}
          \centering
          \scriptsize
          \begin{subtable}{.33\textwidth}
                        \centering
                        {\begin{tabular}{c}
                            \toprule
                            \midrule
                            RGB image \\
                            \midrule
                            33 stride=1, conv. 64\\                            
                            \midrule
                            ResBlock down 128\\
                            \midrule 
                            ResBlock down 256\\
                            \midrule 
                            ResBlock down 512\\
                            \midrule                                 
                            dense  128 \\
                            \midrule
                            \bottomrule
                        \end{tabular}}
                        \caption{Encoder}
                    \end{subtable}
          \begin{subtable}{.33\textwidth}
              \centering
              {\begin{tabular}{c}
                  \toprule
                  \midrule
                   \\
                  \midrule
                  dense,  \\
                  \midrule
                  ResBlock up 256\\
                  \midrule
                  ResBlock up 128\\
                  \midrule
                  ResBlock up 64\\
                  \midrule
                  BN, ReLU, 33 conv, 3 Sigmoid\\
                  \midrule
                  \bottomrule
              \end{tabular}}
              \caption{Generator}
          \end{subtable}
          \begin{subtable}{.32\textwidth}
              \centering
              {\begin{tabular}{c}
                  \toprule
                  \midrule
                  RGB image  \\
                  \midrule
                  ResBlock down 64\\
                  \midrule
                  ResBlock down 128\\
                  \midrule
                  ResBlock down 256\\
                  \midrule
                  ResBlock down 512\\
                  \midrule
                  ResBlock 1024\\
                  \midrule
                  ReLU\\
                  \midrule
                  Global sum pooling\\
                  \midrule
                  dense  1, dense  5 (two heads)\\
                  \midrule
                  \bottomrule
              \end{tabular}}
              \caption{Discriminator. Two heads for the real/fake discriminator and multi-class classifier.}
          \end{subtable}
\end{table}
 
\end{document}
