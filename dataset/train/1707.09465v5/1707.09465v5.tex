

\iffalse
\begin{figure*}
\small
\centering
    \includegraphics[angle=90,origin=c,height=0.77\textheight,width=0.95\textwidth]{figure/v4-cropped}
    \vspace{-10pt}
  \caption{Qualitative semantic segmentation results on the Cityscapes dataset~\cite{ros_synthia_2016} (target domain). For each target image in the first column, we retrieve its nearest neighbor from the SYNTHIA~\cite{cordts_cityscapes_2016} dataset (source domain). The third column plots the label distributions due to the groundtruth pixel-wise semantic annotation, the predictions by the baseline network with no adaptation, and the inferred distribution by logistic regression. The last three columns are the segmentation results by the baseline network, our domain adaptation approach, and human annotators.} \label{fQualitative}
\end{figure*}
\fi

\begin{sidewaysfigure*}
\small
\centering
    \includegraphics[height=0.75\textheight,width=1.0\textwidth]{figure/v4-cropped}
    \vspace{-20pt}
  \caption{Qualitative semantic segmentation results on the Cityscapes dataset~\cite{ros_synthia_2016} (target domain). For each target image in the first column, we retrieve its nearest neighbor from the SYNTHIA~\cite{cordts_cityscapes_2016} dataset (source domain). The third column plots the label distributions due to the groundtruth pixel-wise semantic annotation, the predictions by the baseline network with no adaptation, and the inferred distribution by logistic regression. The last three columns are the segmentation results by the baseline network, our domain adaptation approach, and human annotators.} \label{fQualitative}
  \vspace{-15pt}
\end{sidewaysfigure*}

\section{Experiments} \label{sExperiments}

In this section, we describe the experimental setup and compare the results of our approach, its variations, and some existing baseline methods.

\subsection{Segmentation network and optimization}

In our experiments, we use FCN-8s~\cite{long_fully_2015} as our semantic segmentation network. We initialize its convolutional layers with VGG-19~\cite{SimonyanX14Very}, and then train it using the AdaDelta optimizer~\cite{zeiler_adadelta:_2012} with default parameters. Each mini-batch is comprised of five source images and five randomly chosen target images. When we train the baseline network with no adaptation, however, we try to use the largest possible mini-batch that includes 15 source images. The network is implemented in Keras~\cite{chollet_keras_2015} and Theano~\cite{al-rfou_theano:_2016}. We train different versions of the network on a single Tesla K40 GPU.


Unlike the existing deep domain adaptation methods~\cite{GaninICML15Unsupervised,GaninX15Domainadversarial,LongICML15Learning,hoffman_fcns_2016} which introduce regularization to the intermediate layers, we only revise the loss function over the output.  Hence, our curriculum domain adaptation can be readily applied to other segmentation networks (e.g.,~\cite{NohICCV15Learning,chen_semantic_2014}).


\subsection{Datasets and evaluation}
We use the publicly available \textbf{Cityscpaes}~\cite{cordts_cityscapes_2016} and \textbf{SYNTHIA}~\cite{ros_synthia_2016} datasets in our experiments. 

{Cityscapes} is a real-world, vehicle-egocentric image dataset collected in 50 cities in Germany and nearby countries. It provides four disjoint subsets: 2,993 training images, 503 validation image, 1,531 test images, and 20,021 auxiliary images. All the training, validation, and test images are accurately annotated with per pixel category labels, while the auxiliary set is coarsely labeled.  There are 34 distinct categories in the dataset.

{SYNTHIA}~\cite{ros_synthia_2016} is a large dataset of synthetic images and provides a particular subset, called SYNTHIA-RAND-CITYSCAPES, to pair with {Cityscapes}. This subset contains 9,400  images that are automatically annotated with 12 object categories,  one void class, and some unnamed classes. Note that the virtual city used to generate the synthetic images does not correspond to any of the real cities covered by {Cityscapes}. We abbreviate SYNTHIA-RAND-CITYSCAPES to {SYNTHIA} hereon.
\vspace{-10pt}

\paragraph{Domain idiosyncrasies.}
Although both datasets depict urban scenes, and {SYNTHIA} is created to be as photo-realistic as possible, they are mismatched domains in several ways. The most noticeable difference is probably the coarse-grained textures in SYNTHIA; very similar texture patterns repeat in a regular manner across different images. In contrast, the Cityscapes images are captured by high-quality dash-cameras. Another major distinction is the variability in view angles. Since Cityscapes images are recorded by the dash cameras mounted on a moving car, they are viewed from almost a constant angle that is about parallel to the ground.  More diverse view angles are employed by SYNTHIA --- it seems like some cameras are placed on the buildings that are significantly higher than a bus. Finally, some of the SYNTHIA images are severely shadowed by extreme lighting conditions, while we find no such conditions in the Cityscapes images. These combined factors, among others, make domain adaptation from SYNTHIA to Cityscapes a very challenging problem. 

Figure~\ref{fQualitative} shows some example images from both datasets. We pair each Cityscpaes image with its nearest neighbor in SYNTHIA, retrieved by the Inception-Resnet-v2 \cite{szegedy_inception-v4_2016} features. However, the cross-dataset nearest neighbors are visually very different from the query images, verifying the dramatic disparity  between the two domains.
\vspace{-10pt}





\paragraph{Experiment setup.}
Since our ultimate goal is to solve the semantic segmentation problem for real images of urban scenes, we take Cityscapes as the target domain and SYNTHIA as the source domain. The Cityscapes validation set is used as our test set. We split 500 images out of the Cityscpaes training set for the validation purpose (e.g., to monitor the convergence of the networks). In training,  we randomly sample mini-batches from both the images (and their labels) of SYNTHIA and the remaining images of Cityscapes yet with no labels. 

As in \cite{hoffman_fcns_2016}, we manually find 16 common classes between the two datasets: sky, building, road, sidewalk, fence, vegetation, pole, car, traffic sign, person, bicycle, motorcycle, traffic light, bus, wall, and rider. The last four are unnamed and yet labeled in SYNTHIA. 
\vspace{-10pt}

\paragraph{Evaluation.} We use the evaluation code released along with the Cityscapes dataset to evaluate our results. It calculates the PASCAL VOC intersection-over-union, i.e., 
~\cite{everingham_pascal_2015}, where  TP, FP, and FN are the numbers of true positive, false positive, and false
negative pixels, respectively, determined over the whole test set. Since we have to resize the images before feeding them to the segmentation network, we resize the output segmentation mask back to the original image size before running the evaluation against the groundtruth annotations.









\begin{table}
\centering
\caption{The  distances between the groundtruth label distributions and those predicted by different methods.}
\label{tLayoutmismatch}
\scalebox{0.9}{
\begin{tabular}{l|ccccc}
\hline
Method   & Uniform & NoAdapt & Src mean  & NN & \textbf{LR}\\
\hline
 Distance   & 1.13 & 0.65 & 0.44  & 0.33 & \textbf{0.27}\\
\hline
\end{tabular}
}
\vspace{-15pt}
\end{table}

\subsection{Results of inferring global label distributions}
Before presenting the final semantic segmentation results, we first compare the different approaches to inferring the global label distributions of the target images (cf.\ Section~\ref{subsLD}). We report the results on the held-out validation images of Cityscapes in this experiment, and then select the best method for the remaining experiments. 

In Table~\ref{tLayoutmismatch}, we compare the estimated label distributions with the groundtruth ones using the  distance, the smaller the better. We see that the baseline network (NoAdapt), which is directly learned from the source domain without any adaptation methods, outperforms the dumb uniform distribution (Uniform) and yet no other methods. This confirms that the baseline network gives rise to severely disproportionate predictions over the target domain. 

Another dumb prediction (Src mean), i.e., using the mean of  all label distributions over the source domain as the prediction for the target images, however, performs reasonably well. To some extent, this indicates the value of the simulated source domain for the semantic segmentation task of urban scenes. 

Finally, the nearest neighbors (NN) based method and the multinomial logistic regression (LR) (cf.\ Section~\ref{subsLD}) perform the best. We use the output of LR on the target domain in our remaining experiments.


\subsection{Comparison results}

\begin{table*}
    \centering
    \small
\caption{Comparison results for the semantic segmentation of the Cityscapes images~\cite{cordts_cityscapes_2016} by adapting from SYNTHIA~\cite{ros_synthia_2016}.}
\vspace{-3pt}
\label{Tresults}
\scalebox{0.92}{
\begin{tabular}{l||c||c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c}
\hline
\multirow{2}{*}[-2em]{ Method~~~~\%} & \multirow{2}{*}[-2em]{ IoU} & \multicolumn{16}{c}{Class-wise IoU}\\

 & &  \rot{bike} & \rot{fence} & \rot{wall} & \rot{t-sign} & \rot{pole} & \rot{mbike} & \rot{t-light} & \rot{sky} & \rot{bus} & \rot{rider} & \rot{veg} & \rot{bldg} & \rot{car} & \rot{person} & \rot{sidewalk} & \rot{road}\\
 \hline\hline
 NoAdapt~\cite{hoffman_fcns_2016}  & 17.4 & 0.0 & 0.0 & 1.2 & 7.2 & 15.1 & 0.1 & 0.0 & 66.8 & \underline{\textit{3.9}} & 1.5 & 30.3 & 29.7 & 47.3 & 51.1 & 17.7 & 6.4\\

FCN Wld~\cite{hoffman_fcns_2016} & \underline{\textit{20.2}} & \underline{\textit{0.6}} & \underline{\textit{0.0}} & \textbf{4.4} & \textbf{11.7} & \underline{\textit{20.3}} & \underline{\textit{0.2}} & \underline{\textit{0.1}} & \underline{\textit{68.7}} & 3.2 & \underline{\textit{3.8}} & \underline{\textit{42.3}} & \underline{\textit{30.8}} & \textbf{54.0} & \textbf{51.2} & \underline{\textit{19.6}} & \underline{\textit{11.5}}\\
\hline
NoAdapt & 22.0 & \textbf{18.0} & 0.5 & 0.8 & 5.3 & 21.5 & 0.5 & 8.0 & \underline{\textit{75.6}} & 4.5 & \textbf{9.0} & \underline{\textit{72.4}} & 59.6 & 23.6 & \underline{\textit{35.1}} & 11.2 & 5.6\\

\textbf{Ours (I)} & \underline{\textit{25.5}} & 16.7 & \textbf{0.8} & \underline{\textit{2.3}} & \underline{\textit{6.4}} & \textbf{21.7} & \textbf{1.0} & \textbf{9.9} & 59.6 & \underline{\textit{12.1}} & 7.9 & 70.2 & \underline{\textit{67.5}} & \underline{\textit{32.0}} & 29.3 & \underline{\textit{18.1}} & \underline{\textit{51.9}}\\
\hline
SP Lndmk & { 23.0} & { 0.0} & { 0.0} & { 0.0} & { 0.0} & { 0.0} & { 0.0} & { 0.0} & \textbf{ 83.1} & \textbf{ 26.1} & { 0.0} & { 73.1} & { 67.7} & { 41.1} & { 5.8} & { 10.6} & { 60.8}\\

SP & { 25.6} & { 0.0} & { 0.0} & { 0.0} & { 0.0} & { 0.0} & { 0.0} & { 0.0} & { 80.5} & { 22.1} & { 0.0} & { 71.9} & { 69.3} &  \underline{\textit{45.9}} & { 24.6} & { 19.8} & \textbf{ 75.0}\\

\textbf{Ours (SP)} & \underline{\textit{28.1}} & \underline{\textit{10.2}} & \underline{\textit{0.4}} & \underline{\textit{0.1}} & \underline{\textit{2.7}} & \underline{\textit{8.1}} & \underline{\textit{0.8}} & \underline{\textit{3.7}} & 68.7 & 21.4 & \underline{\textit{7.9}} & \underline{\textit{75.5}} & \underline{\textit{74.6}} & 42.9 & \underline{\textit{47.3}} & \underline{\textit{23.9}} & 61.8\\
\hline
\textbf{Ours (I+SP)} & \textbf{29.0} & 13.1 & 0.5 & 0.1 & 3.0 & 10.7 & 0.7 & 3.7 & 70.6 & 20.7 & 8.2 & \textbf{76.1} & \textbf{74.9} & 43.2 & 47.1 & \textbf{26.1} & 65.2\\
\hline
\end{tabular}
}
\vspace{-8pt}
\end{table*}

We report the final semantic segmentation results on the test data of the target domain in this section. We compare our approach to the following competing methods.


\begin{description}
  \setlength\itemsep{0.01in}

\item[No adaptation (NoAdapt).] We directly train the FCN-8s model on SYNTHIA without applying any domain adaptation methods. This is the most basic baseline for our experiments. 

\item[Superpixel classification (SP).]\label{baselineSP} Recall that we have trained a multi-class SVM using the dominant labels of the superpixels in the source domain. We then use them to classify the target superpixels. 

\item[Landmark superpixels (SP Lndmk).] Since we keep the top 60\% most confidently classified superpixels  as the landmarks to regularize our segmentation network during training (cf.\ Section~\ref{subsSP}), it is also interesting to examine the classification results of these superpixels. We run the evaluation after assigning the void class label to the other pixels of the images. 

In addition to the IoU, we have also evaluated the classification results of the superpixels by accuracy. We find that the classification accuracy is 71\% for all the superpixels of the target domain, while for the selected 60\% landmark superpixels, the classification accuracy is more than 88\%. 



\item[FCNs in the wild (FCN Wld).] Hoffman et al.'s work~\textbf{\cite{hoffman_fcns_2016}} is the only existing one addressing the same problem as ours, to the best of our knowledge. They introduce a pixel-level adversarial loss to the intermediate layers of the network and impose constraints to the network output. Their experimental setup is about identical to ours except that they do not specify which part of Cityscapes is considered as the test set. Nonetheless, we include their results for comparison to put our work in a better perspective. 



\end{description}


The comparison results are shown in Table~\ref{Tresults}. Immediately, we note that all our domain adaptation results are significantly better than those without adaptation (NoAdapt). 

We denote by (\textbf{Ours (I)}) the network trained using the global label distributions over the target images (and the labeled source images). Although one may wonder that the image-wise label distributions are too high-level to supervise the pixel-wise discriminative network, the gain is actually significant. They are able to correct some obvious errors of the baseline network, such as the disproportional predictions about road and sidewalk (cf.\ the results of \textbf{Ours (I)} vs.\ NoAdapt in the last two columns).


It is interesting to see that both superpixel classification-based segmentation results (SP and SP Lndmk) are also better than the baseline network (NoAdapt). The label distributions obtained over the landmark superpixels boost the segmentation network (\textbf{Ours (SP)}) to the mean IoU of 28.1\%, which is better than those by either superpixel classification or the baseline network individually. We have also tried to use the label distributions over all the superpixels to train the network, and observe little improvement over NoAdapt. This is probably because it is too forceful to regularize the network output at every single superpixel especially when the estimated label distributions are not accurate enough. 


The superpixel-based methods, including \textbf{Ours (SP)}, miss small objects such as fences, traffic lights (t-light), and traffic signs (t-sign), and instead are very accurate for categories like the sky, road, and building, that typically occupy larger image regions. On the contrary, the label distributions on the images give rise to a network (\textbf{Ours (I)}) that performs better on the small objects than \textbf{Ours (SP)}. In other words, they mutually complement to some extent. Re-training the network by using the label distributions over both global images and local landmark superpixels (\textbf{Ours (I+SP)}), we achieve the best semantic segmentation results on the target domain. In the future work, it is worth exploring other target properties, perhaps still in the form of label distributions, that handle the small objects well, in order to further complement the superpixel-level label distributions.
\vspace{-15pt}

\paragraph{Comparison with FCNs in the wild~\cite{hoffman_fcns_2016}.} 
Although we use the same segmentation network (FCN-8s) as~\cite{hoffman_fcns_2016},  our baseline results (NoAdapt) are better than those reported in~\cite{hoffman_fcns_2016}. This may be due to subtle differences in terms of implementation or experimental setup. Although our own baseline results are superior,  we gain larger improvements (7\%) over them than the performance gain of~\cite{hoffman_fcns_2016} (3\%) over the seemingly underperforming baseline network there. 
\vspace{-10pt}


\paragraph{Comparison with learning domain-invariant features.} At our first attempt to solve the domain adaptation problem for the semantic segmentation of urban scenes, we tried to learn domain invariant features following the deep domain adaptation methods~\cite{LongICML15Learning} for classification. In particular, we impose the maximum mean discrepancy~\cite{gretton2012kernel}  over the layer before the output. We name such network layer the feature layer. Since there are virtually three output layers in FCN-8s, we experiment with all the three feature layers correspondingly. We have also tested the domain adaptation by reversing the gradients of a domain classifier~\cite{GaninICML15Unsupervised}. However, none of these efforts lead to any noticeable gain over the baseline network so the results are omitted. 

















