\pdfoutput=1


\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{naaclhlt2019}
\usepackage{times}
\usepackage{latexsym}

\usepackage{url}

\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{comment}
\usepackage{tikz-dependency}
\usepackage{tikz}
\usepackage{tikz-qtree}
\usepackage[british]{babel}
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{hhline}
\usepackage{url}
\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage[noend]{algpseudocode}
\usepackage{booktabs}
\usepackage{proof}
\usepackage{paralist}
\usepackage{tikz-dependency}
\usepackage{tikz} 
\usepackage{graphicx, subfigure}
\usepackage{epstopdf}
\newcommand{\pending}[1]{\textcolor{red}{{#1}}}
\usepackage[latin1]{inputenc}


\usepackage{enumitem}
\setitemize{noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt}
\setenumerate{noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt}


\newcommand{\eat}[1]{}
\newcommand{\termdef}[1]{\textbf{#1}}
\newcommand{\size}[1]{\left| {#1} \right|}
\newcommand{\sep}{\,\mid\,}
\newcommand{\ep}{\varepsilon}
\newcommand{\emp}{\emptyset}
\newcommand{\substr}[2]{{#1}[{#2}]}
\newcommand{\numberset}{\mathbb}
\newcommand{\N}{\numberset{N}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\newcommand{\order}[1]{{\cal O}({#1})}
\newcommand*{\SET}[1]{\{#1\}}

\renewcommand{\labelenumi}{\rm{(\roman{enumi})}}
\renewcommand{\labelenumii}{\rm{(\alph{enumii})}}
\newcommand{\best}[1]{\textbf{#1}}
\makeatletter
\useshorthands{"}\defineshorthand{"-}{\nobreak-\bbl@allowhyphens}
\makeatother

\newcommand{\de}{\rightarrow}
\newcommand{\mytree}{t}
\newcommand{\loss}{{\cal L}}
\newcommand{\NT}{{\cal NT}}
\newcommand{\ploss}{\loss_c}

\newcommand{\tsystem}{P}
\newcommand{\stack}{\sigma}
\newcommand{\sizeL}{\ell}
\newcommand{\redbuffer}{{\buffer_R}}
\newcommand{\sizeR}{{|\redbuffer|}}
\newcommand{\stackel}[1]{\stack[{#1}]}
\newcommand{\buffer}{\beta}
\newcommand{\buffel}[1]{\buffer[{#1}]}
\newcommand{\stackbuf}{\gamma}
\newcommand{\stackbufi}[1]{\gamma[{{#1}}]}
\newcommand{\sizeLR}{{|\stackbuf|}}
\newcommand{\config}[3]{{({#1}, {#2}, {#3})}}
\newcommand{\transet}{{\cal T}}
\newcommand{\trans}{\vdash}
\newcommand{\translabel}{\tau}
\newcommand{\transname}[1]{\ensuremath{\mathsf{#1}}}
\newcommand{\dset}{{\cal D}}
\newcommand{\myitem}[3]{[{#1},{#2},{#3}]}
\newcommand{\titem}[2]{[\![{#1},{#2}]\!]}
\newcommand{\mytable}{{\cal T}}
\newcommand{\lanp}{\transname{la_2}}
\newcommand{\ranp}{\transname{ra_2}}
\newcommand{\oracle}{\ensuremath{\mathsf{oracle}}}
\newcommand{\valid}{\ensuremath{\mathsf{valid}}}
\newcommand{\emptystacksym}{\\bulletnn2n-1nO(n^2)nnnnw_1, \dots ,w_nw_ie_i\sh\rew_ipp \neq iw_pw_i\shw_pw_i \rightarrow w_pp = iw_i\re on 
the stack and, by applying 2-1 transitions, a dependency tree is built for the input 
in a top-down depth-first fashion, where multiple children of a same word are forced during training to be created in an inside-out manner. More in detail, for each parsing configuration , the decoder (implemented as a uni-directional LSTM) receives the encoder hidden state  of the word  on top of the stack to generate a \textit{decoder hidden state} . After that, , together with the sequence  of encoder hidden states from words still in the buffer plus , are used to compute the attention vector  as follows: 

As attention scoring function (), they adopt the biaffine attention mechanism described in \cite{Luong2015, DozatM16}. Finally, the attention vector  will be used to return the highest-scoring position 
and choose the next transition. 
The parsing process ends when 
only the root remains on the stack.

As extra high-order features, \citet{Ma18} add grandparent and sibling information, 
whose encoder hidden states are added to that of the word on top of the stack to generate the corresponding decoder hidden state .
They prove that these additions improve final accuracy, especially when children are attached in an inside-out fashion.

According to the authors, the original stack-pointer network is trained to maximize the likelihood of choosing the correct word for each possible top-down path from the root to 
a leaf.
More in detail, a dependency tree can be represented as a sequence of top-down paths , where each path  corresponds to a sequence of words , w_{i,1}, w_{i,2}, \dots, w_{i,l_i}P_\theta (y|x)yx\thetap_{<i}w_{i,j}jp_iw_{i,<j}p_iii\atw_iw_ppw_p \rightarrow w_iipw_innw_iiw_{i+1}na_tO(n)+O(n)=O(n)O(n^2)w_iw_is_ie_id_tnnl_1, \dots , l_{n}l_iw_iiw_h(w_i, w_h)P_\theta (y|x)w_hw_iil_{<i}^*^*^*^*\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm$0.09} \\ 
\hline
\multicolumn{5}{c}{}\\
\end{tabular}
\centering
\setlength{\abovecaptionskip}{4pt}
\caption{Parsing accuracy of the top-down and left-to-right pointer-network-based parsers on test datasets of twelve languages from UD. Best results for each language are shown in bold and, apart from the average UAS and LAS, we also report the corresponding standard deviation over 3 runs.}
\label{tab:ud}
\end{center}
\end{small}
\end{table}


In addition, in Table~\ref{tab:ud} we can see how, under the exactly same conditions, the left-to-right algorithm improves over the original top-down variant in nine out of twelve languages in terms of LAS, obtaining competitive results in the remaining three datasets. 

Finally, in spite of requiring a cycle-checking procedure, our approach proves to be twice as fast as the top-down alternative in decoding time, achieving, under the exact same conditions, a 23.08-sentences-per-second speed on the PTB-SD compared to 10.24 of the original system.\footnote{Please note that the implementation by \citet{Ma18}, also used by our novel approach, was not optimized 
for speed
and, therefore, the reported speeds are just intended for comparing algorithms implemented under the same framework, but not to be considered as the best speed that a pointer-network-based system can potentially achieve.}

\section{Related work}
There 
is previous work that proposes to implement dependency parsing by independently selecting the head of each word in a sentence, using neural networks.
In particular, \citet{Zhang17} make use of a BiLSTM-based neural architecture to 
compute the probability of attaching each word to one of the other input words, in a similar way as pointer networks do. 
During decoding, a post-processing step is needed to produce well-formed trees by means of a maximum spanning tree algorithm. 
Our approach does not need this post-processing, as cycles are forbidden during parsing instead, and achieves a higher accuracy thanks to the pointer network architecture and the use of information about previous dependencies.


Before \citet{Ma18} presented their top-down parser, \citet{Chorowski17} had already employed pointer networks \cite{Vinyals15} for dependency parsing. Concretely, they developed a pointer-network-based neural architecture with multitask learning able to perform pre-processing, tagging and dependency parsing exclusively by reading tokens from an input sentence, 
without needing
POS tags or pre-trained word embeddings. Like our approach, they also use the capabilities provided by pointer networks to undertake the parsing task as a simple process of attaching each word 
as dependent of another.
They also try to improve the network performance with POS tag prediction as auxiliary task and with different approaches to perform label prediction. 
They do not exclude cycles, neither by forbidding them at parsing time or by removing them by post-processing, as they report that their system produces parses with a negligible amount of cycles, even with greedy decoding (matching our observation for our own system, in our case with beam-search decoding).
Finally, the system developed by \citet{Chorowski17} is constrained to projective dependencies, while our approach can handle unrestricted non-projective structures.


\section{Conclusion}
We present a novel left-to-right dependency parser based on pointer networks. We follow the same neural network architecture as the stack-pointer-based approach developed by \citet{Ma18}, but just using a focus word index instead of a buffer and a stack. Apart from doubling their system's speed, our approach proves to be a competitive alternative on a variety of languages and achieves the best accuracy to date on the PTB-SD. 

The good performance of our algorithm can be explained by the shortening of the transition sequence length. In fact, it has been proved by several studies \cite{buffertrans, Qi2017, Fernandez18} that by reducing the number of applied transitions, the impact of error propagation is alleviated, yielding more accurate parsers.

Our system's source code is freely available at \url{https://github.com/danifg/Left2Right-Pointer-Parser}.

\section*{Acknowledgments}

This work has received funding from the European
Research Council (ERC), under the European
Union's Horizon 2020 research and innovation
programme (FASTPARSE, grant agreement No
714150), from 
MINECO (FFI2014-51978-C2-2-R, TIN2017-85160-C2-1-R)
and from Xunta de Galicia (ED431B 2017/01).


\bibliography{main,twoplanaracl,bibliography}
\bibliographystyle{acl_natbib}

\end{document}
