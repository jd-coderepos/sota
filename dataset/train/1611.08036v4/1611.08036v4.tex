\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathptmx} 
\usepackage[caption = false]{subfig}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}


\newcommand{\ck}[1]{ \color{red} CK: #1  \color{black}}

\cvprfinalcopy 

\def\cvprPaperID{2116 } \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\setcounter{page}{1}
\begin{document}

\title{Robotic Grasp Detection using Deep Convolutional Neural Networks}
\author{Sulabh Kumra$^1$$^2$ and Christopher Kanan$^1$
       \\
       $^1$Rochester Institute of Technology, Rochester, NY, USA\\
       $^2$Xerox Corporation, Webster, NY, USA\\
       \{sk2881, kanan\}@rit.edu\\ 
       }




\maketitle


\begin{abstract}
Deep learning has significantly advanced computer vision and natural language processing. While there have been some successes in robotics using deep learning, deep learning has not been widely adopted. In this paper, we present a novel robotic grasp detection system that predicts the best grasping pose of a parallel-plate robotic gripper for novel objects using the RGB-D image of the scene. The proposed model uses a  deep convolutional neural network to extract features from the scene and then uses a shallow convolutional neural network to predict the graspability of the object of interest for a specific position and orientation. Our multi-modal model achieved an accuracy of 88.96\% and runs at real-time speeds. This redefines the state-of-the-art for robotic grasp detection.




\end{abstract}

\section{Introduction}
Robotic grasping lags far behind human performance and is an unsolved problem in the field of robotics. When humans see novel objects, they instinctively know how they would grab them to pick them up. There has been a lot of work done related to robotic grasping and manipulation ~\cite{Ciocarlie2014,Redmon,saxena2008robotic}, but the problem of real-time grasp detection and planning is still a challenge. Even the current state-of-the-art grasp detection techniques fail to detect a potential grasp in real-time. The robotic grasping problem can be divided into three parts: grasp detection, trajectory planning, and execution. Grasp detection is essentially a visual recognition problem, where the robot uses sensors to gather information of the surroundings and detects graspable objects from the scene. 3D vision systems or RGB-D cameras are most commonly used to perceive the robot's environment. The key task is to predict potential grasps from sensor information and to map the pixel values to real world coordinates and orientation for a potential grasp. This is the most important step in performing a grasp as the subsequent steps are dependent on the coordinates calculated in this step. The calculated real world coordinates are then transformed to position and orientation for the robot's end-of-arm tooling (EOAT). An optimal trajectory for the robotic arm is then planned to reach the target grasp position. Subsequently, the planned trajectory for the robotic arm is  executed using either an open-loop or a closed loop controller. In contrast to an open-loop controller, a closed-loop controller receives a feedback from the vision system during the entire grasping task. A feedback based system is computationally more expensive and can drastically affect the performance and speed of the task. 

\begin{figure}
\begin{center}
\includegraphics[width=0.75\linewidth]{spoon.jpg}
\end{center}
   \caption{An example of grasp rectangle for a potential good grasp of a spoon. This is a five-dimensional grasp representation, where red lines represent parallel plates of gripper, blue lines correspond to the distance between parallel plates of the grippers before grasp is performed, (x,y) are the coordinates corresponding to the center of grasp rectangle and $\theta$ is the orientation of the grasp rectangle with respect to the horizontal axis}
\label{fig:rit_mug}
\end{figure}

In this paper, we target the problem of detecting a 'good grasp' from the RGB-D data of a scene. Figure \ref{fig:rit_mug} shows a five-dimensional grasp representation for a potential good grasp of a spoon. This five-dimensional representation gives the position and orientation of a parallel plate gripper before grasp is executed on an object. Although it is a simplification of the seven-dimensional grasp representation introduced by Jiang \etal ~\cite{jiang2011efficient}, Lenz \etal showed that a good five-dimensional grasp representation can be projected back to a seven-dimensional grasp representation and can be used by a robot to perform a grasp~\cite{lenz2015deep}. In addition to low computational cost, this reduction in dimension will allow us to detect grasps using 2-D images.

We introduce a novel approach to detecting good robotic grasps for parallel plate grippers using a five-dimensional representation. A pre-trained 50 layer deep convolutional neural network architecture is used to extract features from the input RGB image, followed by a binary classifier. A multi-modal grasp predictor is also presented, which consists of two independent ResNet-50 models running in parallel to extract features from a multi-modal (RGB and Depth) input image. The extracted features are given to a shallow convolutional neural network for predicting graspability. We explore different variations of our models and test them on the standard Cornell Grasp Dataset. Fig.~\ref{fig:dataset} shows a small sample of images from the Cornell Grasp Dataset. Our experiments show that the proposed architecture outperforms the current state-of-the-art results in terms of both accuracy and speed. 

\begin{figure}
\begin{center}
\includegraphics[width=1\linewidth]{collage.png}
\end{center}
   \caption{A small sample of the images from Cornell Grasp Dataset.}
\label{fig:dataset}
\end{figure}



\section{Background}
Recent developments in deep learning~\cite{cite-key} have enabled us to build systems that are loosely inspired by neural circuitry of the human brain. These techniques have been successfully used to solve longstanding problems in computer vision~\cite{NIPS2012_4824,russakovsky2015imagenet,Taigman_2014_CVPR} and natural language processing~\cite{sutskever2014sequence,cho2014learning,socher2011dynamic}. These results have inspired many researchers in the field of robotics to explore the applications of deep learning to solve some of the challenging problems in robotics. For example, robot localization is moving from using hand-engineered features~\cite{johns2014generative} to deep learning features~\cite{sunderhauf2015performance}, deep reinforcement learning is being used for end-to-end training for robotic arm control~\cite{levine2016end}, multi-view object recognition has achieved state-of-the-art performance by deep learning camera control~\cite{johns2016pairwise}, reinforcement learning has been used to learn dual-arm robot tasks~\cite{kumra2015dual}, and  autonomous driving has been tackled by using deep learning to estimate the affordances for driving~\cite{chen2015deepdriving}.

A major challenge with deep learning is that it needs a very large volume of training data, but large datasets with manually labeled images are unavailable for most robotics applications. In computer vision, transfer learning techniques are used to pre-train deep convolutional neural networks on some large dataset, e.g., ImageNet, which contains 1.2 million images with 1000 categories~\cite{imagenet_cvpr09}, before the network is trained on the target dataset ~\cite{yosinski2014transferable}. These pre-trained models are either used as an initialization or as a fixed feature extractor for the task of interest.

The most common approach for 2-D robotic grasp prediction is a sliding window detection framework. In this framework, a classifier is used to predict whether a small patch of the input image have a good potential grasp for an object. The classifier is applied to a number of patches on the image and the patches that get high scores are considered as good potential grasps. Lenz \etal used this technique with convolutional neural networks as a classifier and got an accuracy of 75 percent ~\cite{lenz2015deep}. A major drawback of their work was that it runs at a speed of 13.5 seconds per frame, which is is extremely slow for a robot to find where to move its grippers. In \cite{Redmon}, this method was accelerated by passing the entire image through the network at once, rather than passing several patches.

A significant amount of work has been done using 3-D simulations to find good grasps ~\cite{bohg2010learning,krainin2011autonomous,5509508,5649406}. These techniques are powerful, but most of them rely on a known 3-D model of the target object to calculate an appropriate grasp. However, general purpose robots should be able to grasp unfamiliar objects without object's 3-D model. Jincheng \etal shows that deep learning has the potential for 3-D object recognition and pose estimation, but their experiments only use five objects and the algorithm is computationally expensive~\cite{yu2013vision}. Recent work by Mahler \etal uses a cloud-based robotics approach to significantly reduce the number of samples required for robust grasp planning~\cite{7487342}. Johns \etal generated their training data by using a physics simulation and depth image simulation with 3D object meshes to learn grasp score which is more robust to gripper pose uncertainty~\cite{johns2016deep}.

We take a different approach. We treat the problem as a classification problem so that it can be solved by a single deep neural network architecture using RGB-D data. Moreover, instead of using AlexNet, as used in ~\cite{lenz2015deep}, ~\cite{7487517} and ~\cite{Redmon}, we use the current state-of-the-art deep CNN known as ResNet to solve this classification problem.




\section{Problem Formulation}
The robotic grasp detection problem can be formulated as finding a successful grasp configuration $g$ for a given image $I$ of an object. A five-dimensional grasp configuration $g$ is represented as:
\begin{equation}
g = f(x, y, h, w, \theta)
\end{equation}
where $(x,y)$ corresponds to the center of grasp rectangle, $h$ is the height of parallel plates, $w$ is the maximum distance between parallel plates and $\theta$ is the orientation of grasp rectangle with respect to the horizontal axis. $h$ and $w$ are usually fixed for a specific robot EOAT. An example of this representation is shown in figure \ref{fig:rit_mug}. 

We focus on planer grasps only as Lenz \etal showed that a five-dimensional grasp configuration can be projected back to a seven-dimensional configuration for execution on a real robot. Moreover, in order for a robot to predict the grasp configuration for an object, it should first recognize the object in the image. This means that this problem can be considered as an object detection and a grasp prediction problem.



\section{Approach}
Deep convolutional neural networks (DCNNs) have outperformed the previous state-of-the-art techniques to solve detection and classifications problems in computer vision. In this paper, we use DCNNs to detect the target object from the image and to predict a good grasp location and orientation. Since DCNNs are better at classification as compared to regression, we propose a single step classification technique instead of the two step approach used in \cite{lenz2015deep} and \cite{Redmon}. Moreover, we deter the computational cost of running a simple classifier many times on small patches of the input image by harnessing the extensive capacity of DCNNs to make global grasp predictions on complete RGB-D image of the object of interest.

Theoretically, a DCNN should have better performance with increased depth because increased depth means increased representational capacity. However, our current optimization method, SGD is not an ideal optimizer so in experiments researchers found that increased depth brought increased training error, which is not inline with the theory~\cite{he2015deep}. The increased training error indicates that the ultra-deep network is very hard to optimize. This means that identity map is very hard to obtain in a convolutional neural network by end-to-end training using SGD. Therefore, we use a ResNet model~\cite{he2015deep}, which reformulates the mapping function between layers, using the function given by eq.\eqref{eq:resnet}.

Similar to previous work, we assume that the input image contains only one graspable object and a single grasp has to be predicted for the object. The advantage of this assumption is that we can look at the complete image and make a global grasp prediction. This assumption may not be possible outside the experimental conditions and we would have to come up with a model that has to first divide the image into pieces, so each piece contains only one object.

\begin{figure}
\begin{center}
\includegraphics[width=0.85\linewidth]{resnet.jpg}
\end{center}
   \caption{Example of residual block in ResNet}
\label{fig:resnet}
\end{figure}

\subsection{Architecture}
Our model is much deeper compared to the previous approaches (e.g., \cite{lenz2015deep, 7487517, Redmon}). Instead of using a 8-layer AlexNet, we use ResNet-50, a 50-layer deep ResNet model, to solve this grasp detection problem. The ResNet architecture uses the simple concept of residual learning to overcome the challenge of learning an identity mapping. A standard feed-forward CNN is modified to incorporate skip connections that  bypass a few layers at a time. Each of these skip connections gives rise to a residual block, and the convolution layers predict a residual that is added to the block's input.  The key idea is to bypass the convolution layers and the non-linear activation layers in $k^{th}$ residual block, and let through only the identity of the input feature in the skip connection. Fig.~\ref{fig:resnet} shows an example of residual block with skip connections. The residual block is defined as:
\begin{equation}
\label{eq:resnet}
H_k = F(H_{k-1}, {W_k}) + H_{k-1}
\end{equation}
where, $H_{k-1}$ is the input to the residual block, $H_k$ is the output of the block, and $W_k$ are the weights learned for the mapping of function $F$. We encourage the readers to see ~\cite{he2015deep} for more details on the ResNet architecture.

\begin{figure*}[!th]
\begin{center}
\includegraphics[width=0.85\linewidth]{2dmodel.jpg}
\end{center}
   \caption{Complete architecture of our uni-modal grasp detector}
\label{fig:2dmodel}
\end{figure*}

We introduce two different architectures for robotic grasp prediction: uni-modal grasp predictor and multi-modal grasp predictor. A uni-modal grasp predictor is a 2D grasp predictor that uses only single modality (e.g., RGB) information from the input image to predict graspability, where as the multi-modal grasp predictor is a 3D Grasp Predictor that uses multi-modal (e.g., RGB and Depth) information. In the next two subsections, we discuss these two architectures in detail.


\subsection{Uni-modal Grasp Predictor}
Large-scale image classification datasets have only RGB images. Therefore, we can pre-train our deep convolutional neural networks with 3-channels only. Our uni-modal grasp predictor model is designed to detect grasp using only the RGB channels of the input image. Fig. \ref{fig:2dmodel} shows the complete architecture of our uni-modal grasp predictor. A ResNet-50 model that is pre-trained on ImageNet is used to extract features from the RGB channels of the image. For a baseline model, we use a linear SVM as classifier to predict the graspability of the object using the features extracted from the last hidden layer of ResNet-50. In our uni-modal grasp predictor, the last fully connected layer of ResNet-50 is replaced by two fully connected layers with 'ReLU' and 'Softmax' activation functions. A dropout layer is also added after the first fully connected layer to reduce over-fitting. We use stochastic gradient decent (SGD) to optimize our training loss.

The 3-channel image (RGB or RGD) is fed to the uni-modal grasp predictor, which uses the residual convolutional layers to extract features from the input image. Last fully connected layer is the output layer, which predicts the graspability of the object in the image. During training time, weights of convolutional layers in ResNet-50 are kept fixed and only the weights of last two fully connected layers are tuned. The weights of the last two layers are initialized using Xavier weight initialization.

\subsection{Multi-modal Grasp Predictor}
We introduce a multi-modal grasp predictor, inspired by the RGB-D object recognition approach introduced by Schwarz \etal \cite{schwarz2015rgb}. The multi-modal grasp predictor uses multi-modal (RGB-D) information from the raw images to predict the graspability. The raw RGB-D images are converted into two images. The first is a simple RGB image and other is a depth image converted into a 3-channel image. This depth to 3-channel conversion is done similar to a gray to RGB conversion. These two 3-channel images are then given as input to two independent pre-trained ResNet-50 models. The ResNet-50 models work as our feature extractors for both images. Features are extracted by each of these ResNet-50 models, similar to the 2D grasp predictor. The extracted features are then normalized using L2-normalization. The normalized features are concatenated together and sent to a shallow convolutional neural network. For baseline, we use a linear SVM classifier fit these features and predict the graspability of the object in the image.

For our multi-modal grasp predictor, we use the features from the second last layer of both the ResNet-50 networks and merge them to feed it into a shallow network with three fully connected layers. Fig.~\ref{fig:3dmodel} shows the complete architecture of our multi-modal grasp predictor. The first two fully connected layers use ReLU activation functions and the last layer uses a softmax layer to predict the graspability. We added a dropout layer after the first fully connected layer of the shallow network to reduce over-fitting.

Using two DCNNs in parallel enables us to extract features from both RGB and depth images. Therefore, the model can learn multimodal features from the RGB-D dataset. Weights of the two DCNNs are initialized using the pre-trained ResNet-50 model and the weights of the shallow network are initialized using Xavier weight initialization. 

\begin{figure*}[!th]
\begin{center}
\includegraphics[width=0.85\linewidth]{3dmodel.jpg}
\end{center}
   \caption{Complete architecture of our multi-modal grasp detector}
\label{fig:3dmodel}
\end{figure*}



\section{Experiments}
\subsection{Dataset}
For comparing the results with others, we test our architecture with Cornell Grasp Dataset. The dataset is available at \url{http://pr.cs.cornell.edu/grasping/rect_data/data.php}. This dataset consists of 885 images of 240 different objects. Each image has multiple grasp rectangles labeled as successful (positive) or failed (negative), specifically selected for parallel plate grippers. In total, there are 8019 labeled grasps with 5110 positive and 2909 negative grasps. Fig. \ref{fig:GraspCollage} shows the ground truth grasps using the rectangular metric for this dataset.

\begin{figure}[!h]
\begin{center}
\includegraphics[width=1.0\linewidth]{GraspCollage.jpg}
\end{center}
   \caption{Ground truth graps using rectangular metric for Cornell grasp dataset}
\label{fig:GraspCollage}
\end{figure}

Similar to previous works, we have used five-fold cross validation for all our experiments. The data is split in two different ways:  
\begin{enumerate}
\item \textbf{Image-wise split} \\
Image-wise splitting splits all the images in the dataset randomly. This is helpful to test how well did the network generalize to the objects it has seen before at a different position and orientation.
\item \textbf{Object-wise split} \\
Object-wise splitting splits all the object instances randomly and all images of an object are put in one validation set. This is helpful to test how well did the network generalize to objects it has not seen before.
\end{enumerate}

\begin{table*}[!th]
\begin{center}
\begin{tabular}{|l|l|c|c|}
\hline
\textbf{Authors} & \textbf{Algorithm} & \multicolumn{2}{|c|}{\textbf{Accuracy (\%)}} \\
\cline{3-4}
 & & Image-wise split & Object-wise split \\
\hline\hline
& Chance & 6.7 & 6.7 \\
Jiang \etal \cite{jiang2011efficient} & Fast Search & 60.5 & 58.3\\
Lenz \etal \cite{lenz2015deep} & SAE, struct. reg. two-stage & 73.9 & 75.6 \\
Redmon \etal \cite{Redmon} & AlexNet, MultiGrasp & 84.4 & 84.9 \\
Wang \etal \cite{wang2016robot} & Two-stage closed-loop, with penalty & 85.3 & - \\
\hline\hline
 & \textbf{Uni-modal Grasp Predictor} & & \\
 & ResNet-50, SVM - RGB (\textit{Baseline}) & 84.76 & 84.47 \\
 & ResNet-50, relu, softmax - RGB & \textbf{88.84} & 87.72 \\
Ours & ResNet-50, tanh, relu, softmax - RGD & 88.53 & 88.59 \\
 &  \textbf{Multi-Modal Grasp Predictor} &  & \\
 & ResNet-50x2, linear SVM - RGB-D & 86.44 & 84.47 \\
 & ResNet-50x2, relu,relu,softmax - RGB-D & 88.53 & \textbf{88.96} \\
\hline
\end{tabular}
\end{center}
\caption{Grasp Detection Accuracy on Cornell Grasp Dataset}
\label{tab:results}
\end{table*}

\subsection{Data Pre-processing}
We perform a minimal amount of data pre-processing before feeding it into the DCNN. The input to our DCNN is a patch around the grasp point, extracted from a training image. The patch is re-sized to 224x224, which is the input image size of the  ResNet-50 model. The depth image is re-scaled to range 0 to 255. There are some pixels in depth image that have a NaN value as they were occluded in the original stereo image. These pixels with NaN value were replaced by zeros.

\subsection{Pre-training}
Pre-training is necessary when the domain-specific data available is limited as in the Cornell grasp dataset. Therefore, ResNet-50 is first trained on ImageNet. We assume that most of the filters learned are not specific to the ImageNet dataset and only the layers near the top exhibit specificity for classifying 1000 categories. The DCNN will learn universal visual features by learning millions of parameters during this pre-training process. We then grab the features from the last layer and feed it to our shallow convolutional neural network. It is important to note that the ImageNet dataset has only RGB images and thus the network will learn RGB features only.

\subsection{Training}
For training and validation of our models we used Keras deep learning library, which is written in Python and running on top of Theano. Experiments were performed on a CUDA enabled NVIDIA GeForce GTX 645 GPU with Intel(R) Core(TM) i7-4770 CPU @ 3.40GHz. Although, GPUs are currently not an integral part of robotic systems, they are getting popular in vision based robotic systems because of the increased computational power.

To train our uni-modal grasp predictor, we used SGD to optimize the model with hyper parameters set as: learning rate = 0.001, decay = 1e-6, momentum = 0.9, mini-batch size = 32 and maximum number of epoch = 30. For the multi-modal grasp predictor, we used the following hyper parameters: learning rate = 0.0006, decay = 1e-6, momentum = 0.9, mini-batch size = 32 and maximum number of epoch = 50.

\subsection{Evaluation}
Prior works have used two different performance metrics for evaluating grasps on the Cornell grasp dataset: rectangle metric and point metric. The point grasp metric compares the distance between the center point of predicted grasp and the center point of all the ground truth grasps. A threshold is used to consider the success of grasp, but past work did not disclose these threshold values. Moreover, this metric does not consider the grasp angle, which is an essential parameter for grasping. The rectangle grasp metric consider complete grasp rectangle for evaluation and a grasp is considered to be a good grasp if the difference between the predicted grasp angle and the ground truth grasp angle is less than $30^{\circ}$, and the Jaccard similarity coefficient of the predicted grasp and ground truth grasp is more than $25\%$. Jaccard similarity coefficient or the Jaccard index measures similarity between the predicted grasp and ground truth grasp, and is defined as:
\begin{equation}
J(\widehat{\theta}, \theta) = \frac{| \widehat{\theta} \cap \theta|}{| \widehat{\theta} \cup \theta|}
\end{equation}
where $ \widehat{\theta}$ is the predicted grasp and $\theta$ is the ground truth grasp. As the rectangle metric is better at discriminating between good and bad grasp, we use this metric for our experiments.




\section{Results}
Table \ref{tab:results} shows a comparison of our results with the previous work for the rectangle metric grasp detection accuracy on Cornell RGB-D grasp dataset. Across the board, both of our models outperform the current state-of-the-art robotic grasp detection algorithms in terms of accuracy and speed. Results for the previous work are their self-reported scores. Tests were performed with image-wise split and object-wise split to test how well the network can generalize to different grasp features.

We present results of different versions of uni-modal and multi-modal grasp predictors. This is done by changing the information fed to the input channels. The RGB version of uni-modal grasp predictor uses only RGB channels of the input image. In RGD version, we replace the blue channel of the input image with the re-scaled depth information. The baseline model of uni-modal grasp predictor got an accuracy of 84.76\%. Our uni-modal grasp predictor model with ResNet-50 and softmax classifier with RGB data got an accuracy of 88.84\%. The same model with RGD data achieved an accuracy of 88.53\%.

The baseline multi-modal grasp predictor RGB-D data and got an accuracy of 86.44\%, which sets a new baseline for performance in RGB-D robotic grasp detection. Our multi-modal grasp predictor achieved an accuracy of 88.96\%, which is the new state-of-the-art performance for RGB-D robotic grasp detection. We also tried replacing the ResNet-50 model with a pre-trained VGG16 model. Although, it performed better than previous models, it did not perform better than our ResNet-50+Softmax model.

Fig.~\ref{fig:predict} shows the predicted grasps for the multi-modal grasp predictor. The true negative (Fig.~\ref{predict_b}) and false negative (Fig.~\ref{predict_d}) are the incorrect predictions. In Fig.~\ref{predict_b} we believe that the model failed to understand the depth features of the slipper strap, using which the grippers can grasp the slipper. Whereas, in Fig.~\ref{predict_d} the model failed to understand the orientation $\theta$ of the grasp rectangle with respect to the object. Other than some tricky examples such as these, the model predicts the graspability of different types of objects with high accuracy.

\begin{figure}[h]
\centering
\subfloat[True Positive]{\includegraphics[width = 1.5in]{TP.png} \label{predict_a}}
\subfloat[True Negative]{\includegraphics[width = 1.5in]{TN.png} \label{predict_b}} \\
\subfloat[False Positive]{\includegraphics[width = 1.5in]{FP.png} \label{predict_c}}
\subfloat[False Negative]{\includegraphics[width = 1.5in]{FN.png} \label{predict_d}}
\caption{Predicted grasps using multi-modal model}
\label{fig:predict}
\end{figure}

Table \ref{tab:speed} shows the grasp prediction speeds for our models and compares it with previous work. Both of our models are faster than previous methods. Our uni-modal grasp predictor runs 800 times faster than the two-stage SAE model by Lenz \etal. The main reason for this boost in speed is replacing the sliding window classifier based approach by a single pass model. We also used GPU hardware to accelerate computation and that can be another reason for faster computation.

\begin{table}
\begin{center}
\begin{tabular}{|l|c|}
\hline
\textbf{Method} & \textbf{Speed (fps)} \\
\hline
Lenz \etal \cite{lenz2015deep} & 0.02 \\
Redmon \etal \cite{Redmon} & 3.31 \\
Wang \etal \cite{wang2016robot} & 7.10\\
\hline
Uni-modal Grasp Predictor & 16.03 \\
Multi-modal Grasp Predictor & 9.71 \\
\hline
\end{tabular}
\end{center}
\caption{Grasp Prediction Speed}
\label{tab:speed}
\end{table}



\section{Discussion}
We show that deep convolutional neural networks can be used to predict the graspability of an object. Our network is 6 times deeper as compared to the previous work by Lenz \etal and we made an improvement of 14.94\% for image-wise split and 13.36\% for object-wise split. This shows that going deeper with network and having skip connections helps the model learn more features from the grasp dataset.

Our results show that high accuracy can be achieved with our multi-modal model and that it can be used to predict the graspability of the objects that the model has not seen before. The uni-modal model got the best accuracy when used with RGB data and image-wise split of dataset. Whereas, the multi-modal model performed the best with RGB-D data and object-wise split of the dataset.

Due to unavailability of a pre-trained ResNet model for depth data, both the ResNet-50 models used in the multi-modal model were pre-trained on ImageNet. This may not be the best model for the depth image as the model is only trained on RGB images and will not have depth specific features. In the future, we would like to pre-train the model on a large-scale RGB-D dataset and then use it to predict RGB-D grasps. Moreover, if we have a large-scale RGB-D grasp dataset, we can modify our uni-modal model to take a four channel input and predict grasps using all four channels. In this case, the input size for the network will be (224x224x4) and we can pass RGB as first three channels and depth as the fourth channel.



\section{Conclusion}
In this paper, we presented a novel multi-modal robotic grasp detection system that predicts the graspability of novel objects for a parallel plate robotic gripper using RGB-D images, along with a uni-modal model that uses RGB data only. We showed that DCNNs can be used in parallel to extract features from multi-modal inputs and can be used to predict graspability. Our models improved the state-of-the-art performance on the Cornell Grasping Dataset and run at real-time speeds.





{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
