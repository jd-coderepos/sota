\documentclass{llncs}
\usepackage{subfig}
\usepackage{amsfonts,amsmath,amstext,amssymb}

\usepackage{algorithm,algorithmicx}	\usepackage{algpseudocode}			
\usepackage{enumerate}
\usepackage{paralist}

\usepackage{todonotes}
\usepackage{epstopdf}
\usepackage{color}
\newcommand{\tod}[1]{\todo[inline]{#1}}
\newcommand{\fm}[1]{\todo[inline]{#1}}
\newcommand{\fmh}[2]{\pdfmarkupcomment[markup=Highlight,color=yellow,author=Fabien]{#1}{#2}}

\usepackage{pgf}
\usepackage{tikz}
\usetikzlibrary{arrows,automata}
\usepackage[all]{xy}

\usepackage{hyperref}


\begin{document}


\title{D-Iteration: diffusion approach for solving PageRank }
\titlerunning{D-Iteration: a diffusion approach for solving PageRank }  \author{Dohy Hong\inst{1} \and The Dang Huynh\inst{2}
\and Fabien Mathieu\inst{2}}
\authorrunning{Dohy Hong et al.} \institute{Samsung
\and
Alcatel-Lucent Bell Labs France}

\maketitle              
\begin{abstract}
In this paper we present a new method that can accelerate the computation of the PageRank importance vector. Our method, called D-Iteration (DI), is based on the decomposition of the matrix-vector product that can be seen as a fluid diffusion model and is potentially adapted to asynchronous implementation.
We give theoretical results about the convergence of our algorithm and we show through experimentations on a real Web graph that DI can improve the computation efficiency compared to other classical algorithm like Power Iteration, Gauss-Seidel or OPIC . 
\end{abstract}

\section{Introduction}






PageRank is a link analysis algorithm that has been initially introduced in \cite{BP99} and used by the Google Internet search engine. It assigns a numerical value to each element of a hyper-linked set of nodes, such as the World Wide Web. The algorithm may be applied to any collection of entities (nodes) that are linked through directional relationships. The numerical value assigned to each node called PageRank, is supposed to reflect the structural importance of nodes.

Although PageRank may today only be a small part of Google's ranking algorithm (the complete algorithm is obviously kept secret, but it seems to take into account hundreds of parameters, most of them been related to the user's profile), it stays appealing, especially in the research community, as it balances simplicity of definition, ranking efficiency and computational challenges.

Among these challenges are the growing size of the dataset (Web graphs can have tens of billions of nodes) and the dynamics ot the structure that requires frequent updates.

The complexity of computing the PageRank of a graph rapidly increases with the number of nodes, as it is equivalent to computing an eigenvector on some huge, matrix, and efficient and accurate methods to compute eigenvalues and eigenvectors of arbitrary matrices are in general a difficult problem. In the particular case of the PageRank equation, several specific solutions were proposed and analysed \cite{LM04,BM05} including the power method \cite{BP99} with adaptation \cite{ST03} or extrapolation \cite{TS03,CG03}, or the adaptive on-line method \cite{AP03}, etc.

In this paper we present some theoretical results of D-Iteration (DI): mathematical definition, convergence to the fixed point, measurement of the distance to the limit and solution for dynamic graph update issues. The results are validated on a real dataset.




The rest of the paper is organized as follows. Section~\ref{sec:the-pagerank-challenge} recalls the PageRank equation and exposes some of the main methods used to solve it. Section~\ref{sec:d-iteration-algorithm} formally defines the D-Iteration method and provides a few theoretical results on its convergence and use in a dynamic context.
Lastly, Section~\ref{sec:experiments} proposes a numerical evaluation of DI's performance and Section~\ref{sec:conclusion} concludes.


\section{The PageRank challenge}\label{sec:the-pagerank-challenge}


In this section, we shortly introduce the definition of PageRank along with some standard algorithms to compute it. For more detailed information, one may for example see \cite{LM04}.

\subsection{Definition}

The informal definition of PageRank is rather simple: it is an importance vector over Web pages such that important pages are referenced by important pages~\cite{BP98}.


More formally, let  be a weighted, oriented, graph. The size of  is  and  is the weight of edge .
 represents a set of nodes their (weighted, oriented) relationships. In \cite{BP98},  was a Web graph,  representing web pages and  hyperlinks, but the principle applies to most structured sets.

Let  be a  diffusion matrix defined by:




 is a left substochastic matrix, column  summing to  if node  has an outgoing edge,  if  is a dangling node. Note that:
\begin{itemize}
\item For unweighted graphs, the expression of  is simpler: for , we just have , where  is the out-degree of .
\item Some variants of PageRank require  to be stochastic.
For these variants, one usually pads the null columns of  with  (\emph{dangling nodes completion}).
\end{itemize}


 represents how importance flows from node to node. When it is stochastic, it represents the Markov chain over  implied by the edges . In that case, the PageRank can be defined as a stationary state of the Markov chain, that is a distribution  over  that verifies

Note that  is unique if  is strongly connected.



In practice, the following variant is used instead of \eqref{eq:pagerank-simple}:

\noindent where  is called \emph{damping} (often set to ), and  a \emph{default distribution} over  (often set to the uniform distribution).

If  is stochastic, the solution  of \eqref{eq:pagerank-pf} is a distribution, which corresponds to the Markov chain defined by: with probability , use the Markov chain induced by ; otherwise jump to a random node according to distribution .

Introducing parameters  and  has many advantages:
\begin{itemize}
\item It guarantees the existence and uniqueness of a solution for any substochastic matrix , without any assumption on .
\item It speeds up the PageRank computation (cf below).
\item Parameter  introduces some locality: influence of a node at distance  is reduced by a factor . This strengthens the impact of the local structure and mitigates the possibility of malicious PageRank alterations through techniques like links farm\cite{BM05}.
\item Parameter  allows to customize the PageRank. For instance, one can concentrate the default importance on pages known to talk about some given topic to create a topic-sensitive PageRank\cite{HT02}.
\end{itemize}


\label{sec:pagerank}

In the rest of the paper, unless stated otherwise, we focus on solving \eqref{eq:pagerank-pf}. Writing the solution is straightforward:


However, such a direct approach cannot be used due to the size of  that forbids an explicit computation of . Instead, one can use different iterative methods (cf \cite{LM04}). 

\subsection{Power Iteration}

The simplest approach is \textbf{Power Iteration (PI)}: starting from an initial guess vector , the stationary PageRank vector is iteratively computed using \eqref{eq:pagerank-pf}:

until the change between two consecutive vectors is negligible. It is straightforward that the error decays by a factor at least  at each iteration (hence one of the interests of introducing a damping factor). PI requires to maintain two vectors  and .



\subsection{Gauss-Seidel}

The \textbf{Gauss-Seidel (GS)} applied to PageRank consists in using the updated entries of  as they are computed. During an iteration round, entries  are computed from  to  using:


Thanks to the immediate update, one needs to maintain only one vector and the convergence is faster, typically by a factor 2 asymptotically. The main downside of the update mechanism is the necessity to access the entries in a round-robin fashion, which can cause problems in a distributed scenario.
Note that Gauss-Seidel belongs to a larger class of methods called Successive Overrelaxation (SOR), but other SOR variants are seldom used for Web PageRank computations\cite{SOR}.




\subsection{Online Page Importance Computation}
\label{sec:opic}

An algorithm very close to ours is the \textbf{Online Page Importance Computation (OPIC)} proposed in \cite{AP03}. Its core idea: most PageRank algorithms implicitly use a \emph{pull} approach, where the state of a node is updated according to the states of its incoming neighbors. By contrast, OPIC proposes a \emph{push} approach, where the state of a node is read and used to update the states of its outgoing neighbors. In details, OPIC focuses on solving \eqref{eq:pagerank-simple} for a modified graph , where  is a virtual \emph{zap} node and  is all possible edges between  and . This was introduced to make  stochastic and irreducible, allowing \eqref{eq:pagerank-simple} to admit a unique solution.

OPIC algorithm works as follows: initially, each node receives some amount of \emph{fluid} (a non-negative number) and a null history record. A scheduler, which can be associated to a web crawler, iterates among the nodes. When a node  is selected, its fluid  is, in order,
\begin{itemize}
\item credited to its history: ;
\item equally pushed to its neighbors: for all  that are outgoing neighbors of , ;
\item cleared:  if  has a loop,  otherwise.
\end{itemize}

It has been shown that as long as the scheduler is fair (i.e. any given node is selected infinitely often) then the history vector converges, up to normalization, to the desired solution \cite{AP03}.

The main advantage of OPIC is its flexibility. In particular, it is easy to adapt and incorporate to a continuous, possibly distributed, Web crawler, allowing to get a dynamic, lightweight, PageRank importance estimation. One drawback is that it is not designed to work with \eqref{eq:pagerank-pf}.




\subsection{Other Variants}

Due to lack of space, we only briefly describe a few other methods that have been proposed.

The Generalized Minimal Residual (GMRES) \cite{SS86} provides an approach using Arnoldi process to find the stationary vector in Krylov subspace. It is claimed to perform better than other algorithms in terms of iterations, but it requires more elementary computation steps per iteration.

Quadratic Extrapolation \cite{CG03} uses estimates of secondary eigenvectors of  to speed up the convergence of PI.

Lastly, a few methods have been proposed that take advantage of the locality of hyperlinks (most hyperlinks are internal to Web sites) to decompose PageRank computation into intra-site and extra-site values \cite{WD04,blockrank,flowrank}. These methods are especially adapted to distributed computation, but they usually only output an approximation of the solution of \eqref{eq:pagerank-pf}.

\section{D-Iteration Algorithm}\label{sec:d-iteration-algorithm}


Our proposal, D-Iteration, aims at solving Equation \eqref{eq:pagerank-pf} with an efficiency similar to Gauss-Seidel while keeping the scheduling flexibility offered by OPIC. This result in a fluid diffusion approach similar to OPIC with some damping added to the mix.

\subsection{Definition}

In the following, we assume that a deterministic or a random diffusion sequence  with  is given. We only require that the number of occurrences of each value of  in  is infinite (the scheduler  is fair).  is not obliged to be fixed in advance but can be adjusted on the fly as long as the fairness property stands.





Like with OPIC, we have to deal with two variable vectors at the same time: a \emph{fluid vector} , initially equal to  and a \emph{history vector} , initially null for all nodes.
When a node is selected, its current fluid value is added to its history, then a \emph{fraction } of its fluid is equally pushed to its neighbors and its fluid value is cleared.

Formally, the fluid vector  associated to the scheduler  is iteratively updated using:


where  is the standard basis vector corresponding to .

The second term in \eqref{eq:defF} represents the damped diffusion and the third term clears the local fluid (up to loops). Similarly to OPIC, an iteration reads one value,  and updates  and its outgoing neighbors. Note that F is always non-negative.

We also formally define the history vector :


By construction,  is non-decreasing with .

\subsection{Convergence}

The following Theorem states the convergence of the D-Iteration algorithm:

\begin{theorem}
\label{thm:conv}
For any fair sequence , the history vector  converges to the unique vector  such that :


Moreover, we have



\end{theorem}

\begin{proof}

We first prove the equality:


This is straightforward by induction:  \eqref{eq:HnFn} is true for ; assuming it is true for , we have



From the equation \ref{eq:HnFn}, we have:



Noticing that  is substochastic, we get 



All we need is to show that  tends to 0. Notice that the total available fluid is non-increasing. That being said, the fluid of a given node is non-decreasing until it is scheduled, and when it is, a quantity  of it is ``lost'' due to the damping (or more if it is a dangling node). Given these two observations, let us consider a time  and another time  such that all nodes have been scheduled at least once between  and  (this is always feasible thanks to the fairness assumption). For each node, its fluid at the time of its first scheduling after  is greater than its fluid at time , so we have . This is true for any  (including ) and concludes the proof.
\end{proof}



\subsection{Implicit completion}



\label{ss:distance}

Equation \eqref{eq:SL} is an equality if  is stochastic (for example thanks to dangling node completion), in which case we have  . 
One may want to solve \eqref{eq:SL} for a completed . However, if the completion follows the distribution , one observes that the result is just proportional to the one without completion, so this is just a matter of normalization.

It is more efficient  to perform the computation on a non-completed matrix (every time a dangling node is selected, all non-null entries of  are updated if  is completed). The question is: can we control the convergence to the solution of the completed matrix while running the algorithm on the original one?


To address this problem precisely, we count the total amount of fluid that has left the system when a diffusion was applied on a dangling node. We call this quantity  (at step  of the DI). To compensate this loss and emulate completion, a quantity  should have been added to the initial fluid, leading to  instead of . But then the fluid  would have produced after  steps a leak  on dangling nodes, which needs to be compensated\ldots 

In the end, the correction that is required to compensate the effect of dangling nodes on the residual fluid  consists in replacing the initial condition  by   such that:
\begin{center}

\end{center}

As ,  needs to be renormalized (multiplication) by  so that the exact  distance between  and the normalized  is equal to:


To summarize, we can run the algorithm on the original matrix using 
 as a stopping condition that guarantees the precision of the normalized result.


\subsection{Schedulers}

The actual performance of DI is directly related to the scheduler used. A simple scheduler, which we call \texttt{DI-cyc}, is a Round-Robin (RR) one, where a given permutation of nodes is repeated as long as necessary.

\begin{theorem}
\label{thm:rr}
For any Round-Robin scheduler , we have: 


\end{theorem}

The proof is a direct application of the proof of Theorem \ref{thm:conv} considering successive sequences of  steps.


Theorem \ref{thm:rr} ensures that D-Iteration performs at least as well as the PI method: in both cases, after a round where all nodes have been processed once, the error id reduced by at least .

While the bound can be tight for specific situations (for instance a clockwise scheduler applied to a counterclockwise-oriented cycle), it is conservative in the sense that it ignores that some of the fluid can be pushed multiple times during a sequence of  steps. For that reason, and keeping in mind that D-Iteration is a \emph{push} version of the Gauss-Seidel method, we expect that  \texttt{DI-cyc} will perform more like Gauss-Seidel in practice.


However, one strength of D-Iteration is the freedom in the choice of a scheduler. As the convergence is controlled by the remaining fluid , a good scheduler is one that makes the fluid vanish as quickly as possible. This disappearance is caused by two main parameters: the damping factor  and dangling nodes (cf above). Reminding that at time  a quantity  vanishes through damping, a natural greedy strategy would consist in selecting at each step .

The main drawback of such a strategy is the expensive searching cost. Therefore we propose a simple heuristic called \texttt{DI-argmax} as follows: we use a RR scheduler, but at each iteration, we run through the scheduler until we find a node that possesses a fluid greater or equal to the average fluid in the whole graph. The advantage of this method is that nodes with relatively low fluids will be skipped, avoiding unprofitable updates operation, with a searching cost lower than picking up the best node at each iteration.

\begin{theorem}
\label{thm:argmax}
Using \texttt{DI-argmax}, we have: 


\end{theorem}

The proof is immediate, as by construction we have .

Note that the Theorem proves the convergence of \texttt{DI-argmax}, which is not a fair scheduler: for instance, after some time, it will ignore the transient nodes of the graph, which eventually have no fluid left. We conjecture that \eqref{eq:argg} is not tight (tightness would require to always choose an average node) and that the actual convergence should be faster. 




\subsection{Update equation}

The existing iterative methods (such as Gauss-Seidel iteration, Power Iteration, ...) can naturally adapt the iteration when  (and thus ) is changed because they are in general independent of the initial condition (for any starting vector, the iterative scheme converges to the unique limit). The simplest way to adjust is to compute the PageRank of the new graph using the previous computation as starting vector.

This technique cannot be used in the case of DI so we need to provide an adapted result.


\begin{theorem}
Assume that after  diffusions, the DI algorithm has computed the values  for some matrix , and consider a new matrix  (typically an update of ). One can compute the unique solution of the equation  by running a new D-Iteration with starting parameters

\label{thm:update}
\end{theorem}

A few remarks on Theorem \ref{thm:update}:
\begin{itemize}
\item It implies that one can continue the diffusion process when  is regularly updated: we just need to inject in the system a fluid quantity equal to  and then change to the new matrix , keeping the same history. \item The precision of the result directly relates to the quantity of fluid left. Here the precision induced by  seems rather minimal, as the original fluid is only altered by the difference between the two matrices. In particular, if the difference  is small, the update should be quickly absorbed.
\item For sake of clarity, we assumed that the set of nodes is the same between  and , but the result can be extended to cope with variations of .
\item One can notice that this update may introduce negative fluids (this is indeed mandatory to correct values that are higher in  than in ), but this has no practical impact on computation.
\end{itemize}

\begin{proof}
Call  the asymptotic result of the new D-Iteration. We first use \eqref{eq:HnFn} on the reduced history  (Equation \eqref{eq:HnFn} requires that the history is initially empty):


Letting  go to  leads to 

which can be written



Equation \eqref{eq:HnFn} () concludes the proof.

\end{proof}








\section{Numerical evaluation}



\label{sec:experiments}
In this section, we compare the convergence speed of DI with other algorithms exposed in Section \ref{sec:the-pagerank-challenge}: Power Iteration, Gauss-Seidel, and OPIC.

The results shown in this paper are based on the 
\texttt{uk-06-07} Web graph, which is compressed using techniques in \cite{BRSLLP},\cite{BV03} and available at \cite{webgraph}. This is a crawl done for DELIS project \cite{DELISProj}, a one-year snapshot (from May 2006 to May 2007) of the \texttt{.uk} domain. It contains 133 million nodes and  5.5 billion links.

\subsection{Settings}

All the algorithms are tuned to solve Equation \eqref{eq:pagerank-pf} using  and .

In the case of OPIC, remind that the original version does not take into account of damping factor. In order be consistent with other algorithms, we emulate \eqref{eq:pagerank-pf} by running OPIC on the stochastic matrix 
 defined by:

Note that this emulation makes each diffusion rather costly as all entries need to be updated. It is only introduced to allow comparison with other methods, assuming all diffusions have the same cost, and should not be used in practice.

For DI, we used the two exposed variants, \texttt{DI-cyc} and \texttt{DI-argmax}. The same schedulers were used for OPIC, called 
\texttt{OPIC-cyc} and \texttt{OPIC-argmax}.
Remember that the fluid amount  is constant in OPIC, so the threshold that triggers diffusion in  is constant (it is the average fluid).
\begin{figure}
\begin{center}


\includegraphics[width=0.80\textwidth]{uk_06_07_OPIC_PI_GS_DI}
  
  \caption{Convergence to the stationary PageRank vector}
  \label{fig:exp}
 \end{center}
\end{figure}

\subsection{Results}

The results are shown in Figure \ref{fig:exp}.




The -axis counts the number of rounds required used by the algorithms. For pull methods like PI and GS, a round corresponds to updating the  entries. For push methods like DI and OPIC, a round corresponds to  elementary diffusion steps.



The -axis indicates the precision of the current vector  at  round compared to the real PageRank vector  (the distance to the limit or ). With DI, the precision can be directly deduced from . For other methods, a  approximation of  was precomputed using DI.

The OPIC methods perform well during the first few rounds, with a clear advantage of \texttt{OPIC-argmax} over \texttt{OPIC-cyc}. However, the convergence slows down really hard after. Note that OPIC remains interesting as its primary goal was to provide lightweight PageRank estimates. The results only state that OPIC should not be used for precise PageRank estimations

Power Iteration has asymptotically the smallest convergence speed after OPIC, but a good starting performance. As a result, it remains a good candidate for rough estimates (up to ), but other methods should be used if one needs to be more accurate.

The Gauss-Seidel method has the second best convergence speed, although it does not benefit from the quick start observed with PI. It still is a good candidate if one needs a simple and efficient way to compute a precise PageRank.

\texttt{DI-cyc} performs very similarly to Gauss-Seidel (although a little slower). This is in line with our interpretation that \texttt{DI-cyc} is a kind of \emph{push} version of Gauss-Seidel.

Lastly, \texttt{DI-argmax} clearly outperforms the other methods, being in par with \texttt{OPIC-argmax} after two rounds and achieving in 7 rounds the precision reached by Gauss-Seidel in 20 rounds. This shows that \texttt{DI-argmax} is a method of choice for all intended precisions, and suggests that Equation \eqref{eq:argg} really underestimates its actual performance.




\section{Conclusion}
\label{sec:conclusion}
In this paper, we proposed an algorithm based on a diffusion approach, to solve the PageRank equation. We demonstrated some theoretical results concerning the correctness (convergence), the precision measurement and update equations. Our algorithm shows its potential through experiments on real data in comparison with other classical pull (Power Iteration, Gauss-Seidel) and push (OPIC) methods. For future works, we plan to focus on refining the theoretical convergence results and on how to adapt and implement DI in a distributed manner. 
\bibliographystyle{splncs03}
\bibliography{trace}




\end{document}
