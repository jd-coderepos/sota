\documentclass[10pt,conference]{IEEEtran}
\usepackage{graphicx, graphics,epsfig}
\usepackage{color}
\usepackage[cmex10]{amsmath}
\usepackage{amssymb}
\usepackage{url}
\usepackage{amsfonts}
\usepackage{latexsym}
\usepackage{tabularx}
\usepackage{stmaryrd}
\usepackage{bbm}

\usepackage{subfigure,dsfont,comment,colortbl,cite}
\usepackage{algorithm}
\usepackage{algorithmic}


\usepackage{txfonts}

\pagestyle{empty}


\ifodd 1
\newcommand{\rev}[1]{{\color{red}#1}} \newcommand{\com}[1]{\textbf{\color{blue} (COMMENT: #1)}} \else
\newcommand{\rev}[1]{#1}
\newcommand{\com}[1]{}
\fi

\newcommand{\beq}   {}
\newcommand{\bea}   {}
\newcommand{\bda}   {}
\newcommand{\bdalign}   {}
\newcommand{\define}    {\equiv}
\newtheorem{theorem}{\bf Theorem}
\newtheorem{lemma}{\bf Lemma}
\newtheorem{proposition}{\bf Proposition}
\newtheorem{definition}[theorem]{\bf Definition}
\newcommand{\sfigurewidth}{0.85\textwidth}
\newcommand{\lfigurewidth}{1.0\textwidth}
\newcommand{\hfigurewidth}{0.75\textwidth}
\newcommand{\figurewidth}{0.5\textwidth}
\newcommand{\sfighalfcol}{0.4\textwidth}
\newcommand{\tfigurewidth}{0.16\textwidth}
\newcommand{\ffigurewidth}{0.27\textwidth}
\newcommand{\matlabfigurewidth}{0.33\textwidth}
\newcommand{\fffigurewidth}{0.25\textwidth}
\newcommand{\figurestarwidth}{0.7\linewidth}
\newcommand{\figureheight}{0.24\textwidth}
\newcommand{\spacewidth}{0.1\linewidth}
\newcommand{\widetfigurewidth}{0.23\textwidth}
\newcommand{\tablefigurewidth}{0.6\textwidth}

\newcommand{\server}{{\cal S}}



\title{
Optimal Distributed P2P Streaming under Node Degree Bounds
}

\author{{\large Shaoquan Zhang, Ziyu Shao, Minghua Chen, and Libin Jiang}
\\
Department of Information Engineering, The Chinese University of
Hong Kong
\\
Department of EECS, University of California, Berkeley
\\
Email: \{zsq008, zyshao6, minghua\}@ie.cuhk.edu.hk,\{libinj\}@caltech.edu }


\begin{document}
\maketitle

\begin{abstract}
We study the problem of maximizing the broadcast rate in peer-to-peer
(P2P) systems under \emph{node degree bounds}, i.e., the number of
neighbors a node can simultaneously connect to is upper-bounded. The
problem is critical for supporting high-quality video streaming in
P2P systems, and is challenging due to its combinatorial nature. In
this paper, we address this problem by providing the first distributed
solution that achieves near-optimal broadcast rate under arbitrary
node degree bounds, and over arbitrary overlay graph. It runs on individual
nodes and utilizes only the measurement from their one-hop neighbors,
making the solution easy to implement and adaptable to peer churn
and network dynamics. Our solution consists of two distributed algorithms
proposed in this paper that can be of independent interests: a network-coding
based broadcasting algorithm that optimizes the broadcast rate given
a topology, and a Markov-chain guided topology hopping algorithm that optimizes
the topology. Our distributed broadcasting algorithm achieves the
optimal broadcast rate over arbitrary P2P topology, while previously
proposed distributed algorithms obtain optimality only for P2P complete
graphs. We prove the optimality of our solution and its convergence
to a neighborhood around the optimal equilibrium under noisy measurements
or without time-scale separation assumptions. We demonstrate the effectiveness
of our solution in simulations using uplink bandwidth statistics of
Internet hosts.
\end{abstract}

\begin{table*}
\caption{Summary and comparison of previous work and this work for maximizing
P2P broadcast rate.}


\label{tab:summary} \begin{small}

\begin{centering}
\begin{tabular}{|c|c|c|c|c|}
\hline
References\emph{ }  & General\emph{ }  & Arbitrary Node\emph{ }  & Exact or\emph{ }  & Distributed \tabularnewline
 & Overlay Graph?  & Degree Bound?  & Optimality?  & Solution?\tabularnewline
\hline
\hline
Mutualcast \cite{all:Mutualcast:LPZ05} and the algorithms in \cite{massoulie2007rdb,all:P2PStreaming:KLR.07}  &   &   & \textbf{}  & \textbf{}\tabularnewline
\hline
Iterative in \cite{yicui06_optimal,sudipta2009lcclc}  & \textbf{}  &   & \textbf{}  & \tabularnewline
\hline
CoopNet/SplitStream \cite{castro2003shb,padmanabhan2002ccn}  &   & \textbf{}  &   & \tabularnewline
\hline
ZIGZAG~\cite{zigzag}, PRIME~\cite{magharei2009prime}  & \textbf{}  & \textbf{}  &   & \textbf{}\tabularnewline
\hline
Cluster-tree \cite{streaming_capacity.icdcs10}  &   & \textbf{}  & conditionally optimal   & \tabularnewline
\hline
\textbf{This paper }  & \textbf{}  & \textbf{}  & \textbf{}  & \textbf{}\tabularnewline
\hline
\end{tabular}
\par\end{centering}

\end{small}
\begin{quote}
\begin{scriptsize}  The Cluster-Tree algorithm is -optimal
with high probability if the node degree bound is .
\end{scriptsize}
\end{quote}

\end{table*}



\section{Introduction}

\label{sec:Intro}

Peer-to-peer (P2P) systems have provided a scalable and cost effective
way for streaming video in the past decade. Recent studies \cite{fenglili,AbeKirCig,wang2007r,zhang2005coolstreaming},
however, indicate that the practical performance of P2P streaming
systems can be far from their theoretical optimal.

There have been work studying the performance limit of P2P systems
to understand and unleash their potential. One focus is on the \emph{streaming
capacity} problem \cite{streaming_capacity.allerton09} in P2P live streaming systems
, i.e., maximizing the streaming rate subject to the peering and overlay topology constraints.
The problem is critical for supporting high-quality video, which is
determined by the streaming rate, in P2P live streaming systems. In this paper, we
focus on the broadcast scenario where all peers in the system are
receivers.

The case of unconstrained peering on top of a complete graph is well
studied, where the maximum broadcast rate is derived in several papers
\cite{all:P2PStreaming:KLR.07,all:Mutualcast:LPZ05,all:NetCodP2P:CYHF.06,massoulie2007rdb,chen2008ump}.
The case of unconstrained peering over general graph can also be addressed
by using a centralized solution\cite{sudipta2009lcclc}.

The streaming capacity problem becomes NP-Complete over general graph
with \emph{node degree bounds} \cite{streaming_capacity.icdcs10}.
Node degree is defined as the number of simultaneous active connections that a node maintains with its neighbors. Due to connection overhead costs, it is necessary
to limit the number of simultaneous connections a peer can maintain.
This naturally bounds the node degrees in P2P systems. For instance, in practical
systems such as PPLive \cite{all:pplive}, the total number of neighbors of
a node is usually bounded around 200, and the number
of active neighbors of a node is usually bounded by 10-15 \cite{streaming_capacity.allerton09}. In such large P2P systems with hundreds of thousands of peers, the system topology is not a complete graph.

There has been work studying this challenging problem of maximizing streaming rate under node degree bounds and over general P2P graph. SplitStream/CoopNet~\cite{castro2003shb,padmanabhan2002ccn},
ZIGZAG~\cite{zigzag}, PRIME~\cite{magharei2009prime} and most
practical systems (such as PPLive \cite{all:pplive} and UUSee \cite{all:uusee})
bound node degree but do not provide rate optimality guarantee. Recently,
the authors in \cite{streaming_capacity.icdcs10} proposed a centralized
Cluster-Tree algorithm that achieves near-optimal broadcast rate with
high probability over complete graph, under the assumption that the
node degree bound is at least logarithmic in the size of the network. A summary
and comparison of previous work and this work are in Table \ref{tab:summary}.

Despite of these exciting results, the following two important questions
remain open:
\begin{itemize}
\item What is the maximum broadcast rate under arbitrary node degree bounds,
and over general P2P overlay graph?
\item How to achieve the maximum broadcast rate in a \emph{distributed}
manner?
\end{itemize}
Systems running distributed algorithms, compared with those running
centralized algorithms, are more adaptable to peer churn and network
dynamics.

In this paper, we answer the above two questions and make the following
contributions:
\begin{itemize}
\item We provide the first distributed solution that achieves a broadcast
rate arbitrarily close to the optimal under arbitrary node degree
bounds, and over arbitrary overlay graph. Our solution runs on individual
nodes and utilizes only the information from their one-hop neighbors.
\end{itemize}
Our solution consists of the following two algorithms that can be
of independent interests.
\begin{itemize}
\item We propose a distributed broadcasting algorithm that achieves the
optimal broadcast rate over arbitrary overlay graph. Previous distributed
P2P broadcasting algorithms are optimal only for complete overlay
graph \cite{massoulie2007rdb,all:Mutualcast:LPZ05,all:P2PStreaming:KLR.07}.
Our algorithm is based on network coding and utilizes back-pressure
arguments.
\item We also propose a distributed algorithm that optimizes the topology.
In this algorithm, each node hops among their possible set of neighbors
towards the best peering configuration. Our algorithm is inspired
by a set of log-sum-exp approximation and Markov chain based arguments
expounded in \cite{MA:CLSC10}.
\item We prove the optimality of the overall solution. We also prove its
convergence to a neighborhood around the optimal equilibrium in the presence of
noisy measurements or without time-scale separation assumptions. We
demonstrate the effectiveness of our solution in simulations using
uplink bandwidth statistics of Internet hosts.
\end{itemize}


\section{Problem Formulation}

\label{sec:prob.formulation}


\subsection{Settings and Notations}

We model the P2P overlay network as a general directed graph , where
\emph{} denotes the set of nodes and \emph{} denotes the set
of links. Each link in the graph corresponds to a TCP/UDP
connection between two nodes. Let  denote the neighbor set of node  in the graph. Each node  is associated
with an upload capacity . We assume there is no constraint on the downloading
rate for each node . This assumption can be partly justified
by the empirical observation that as residential broadband
connections with asymmetric upload and download rates become increasingly dominant, bottlenecks
typically are at the uplinks of the access networks rather than
in the middle of the Internet.

As such, P2P networks have capacity limits on the nodes instead of links. This is different from traditional underlay networks where the capacity limits are on the links.

We focus on the single-source streaming scenario, i.e., a source
 broadcasts a continuous stream of contents to the entire network; we
denote its receiver set as .

We consider the peering constraints that each node has a degree bound
, i.e., it can only exchange streaming content
with up to a  number of neighbors \emph{simultaneously} due to connection overhead cost.
We allow different nodes to have different degree bounds.
Fig.~\ref{fig:pc} shows four sample peering configurations of a -node
network with node degree bound  for each node.

\begin{figure}[hbt!]
\centering
\subfigure[]{\includegraphics[width=0.17\textwidth]{peering_config1}}
\subfigure[]{\includegraphics[width=0.17\textwidth]{peering_config2}}\\
\subfigure[]{\includegraphics[width=0.17\textwidth]{peering_config3}}
\subfigure[]{\includegraphics[width=0.17\textwidth]{peering_config4}}
\caption{Peering configuration examples for a -node network with node degree bound  for each node.}
\label{fig:pc}
\end{figure}

Let  denote the set of all feasible peering configurations
over graph  under node degree bounds. Given a configuration , we obtain a
connected sub-graph of  that satisfies the node degree bound constraints. We denote this sub-graph as , where 
represents the set of links in this sub-graph. We denote 
as the set of node 's neighbors in this sub-graph. We have 
where  represents the size of a set.

\subsection{Problem Formulation and Our Approach}

For a configuration , let 
be the maximum achievable broadcast rate under , i.e., the highest rate at which
every node in the system can receive the streaming content simultaneously. The problem of
maximizing broadcast rate under node degree bounds can be
formulated as follows:


This problem is \emph{combinatorial} in nature which is known to be NP-complete \cite{streaming_capacity.icdcs10},
and there is no efficient approximate solution to the problem even
in a centralized manner.

In this paper, we address this problem by providing a distributed
solution. In particular, we first develop a distributed broadcasting
algorithm that can achieve  under arbitrary .
We then design a distributed algorithm that optimizes towards
the best peering configurations. They operate in tandem to achieve
a close-to-optimal broadcast rate under arbitrary node degree bounds,
and over arbitrary overlay graph. We elaborate on these two algorithms
in the following two sections.

\section{The Proposed Distributed Broadcasting Algorithm}

\label{sec:streaming_rate}


By exploiting network coding \cite{all:NetCod:ACLY00}, we design a back-pressure based distributed broadcasting algorithm. Back-pressure type algorithm is proposed initially in \cite{Tassiulas92backpressure}. This type of algorithms select a subset of queues in the system with the maximum back-pressures and serve these queues subject to resource constraints, where back-pressure is defined as the difference between the queue at the local node and that of its downstream nodes. Back-pressure algorithm design has found applications in many network resource allocation domains \cite{all:Backpressure:MAW96}, \cite{all:Backpressure:ESP05}, \cite{all:Backpressure:NMR05}. In this paper, we apply this method for the first time to design distributed P2P broadcasting algorithm. Our algorithm can achieve the maximum broadcast rate over arbitrary P2P topology.


\subsection{Routing vs. Network Coding}\label{subsec:routing-vs-coding}

In P2P systems, there are two approaches for broadcasting contents:
one is based on routing \cite{liu2008opportunities}, in which nodes only store and forward packets;
and the other is based on network coding \cite{all:NetCod:ACLY00}, \cite{liu2008opportunities},
in which a node is also allowed to mix information and output data
as functions of the data it received. Some commercial P2P systems
are built upon routing-based approach (e.g., PPLive \cite{all:pplive}),
and some are based on network coding (e.g., UUSee \cite{all:uusee,liu-uusee})\footnote{We refer interested readers to \cite{liu-uusee,hei2007msl}
for more details on performance of routing-based and network-coding-based
practical P2P systems. We focus on optimal distributed P2P broadcasting
algorithm design based on network coding in this paper. }. It is known that both routing and network coding approaches can
achieve optimal broadcast rate over arbitrary P2P graph \cite{chen2008ump,massoulie2007rdb}.
Compared to routing-based approach, the network-coding based approach introduces additional packet
header overhead for carrying coding coefficients (e.g.,  extra overhead according to \cite{chou2003practical}) and computation complexity for encoding and decoding (e.g., \cite{wang2007r,liu-uusee} discuss how to keep the complexity low). However, the network-coding based approach is robust to peer dynamics since there is no need
for constructing and maintaining the spanning trees. In this
section, we design a distributed broadcasting algorithm based on network
coding that is robust to dynamics. In Section \ref{sec:conclusion}, we will discuss how the overall problem can be solved by using centralized solutions when only routing is allowed.

\subsection{Network Coding Based Formulation}\label{subsec:nc-formulation}

According to the Max-Flow-Min-Cut theorem, a data transmission of rate
 between source  and a receiver  is \emph{feasible}
if and only if there exists a flow, denoted as ,
satisfying the following flow conservation constraints: 
 where  is the set of nodes
sending content to  under configuration , and 
is the set of nodes receiving content from  .

A powerful theorem established in \cite{all:NetCod:ACLY00} states
that a multicast or broadcast rate  from
 to a set of receivers is achievable if and only if  is feasible
for  and any receiver . This is
a strong result as it says that if the network can support a unicast
rate of  between  and any receiver assuming other receivers'
traffic is absent, then it can support a multicast rate of  to
all the receivers simultaneously. Such rate  can be achieved by every
node in the network performing network coding~\cite{all:NetCod:ACLY00}. Further, authors in~\cite{ho2009dynamic,chou2003practical}
show that it is sufficient to perform random linear network coding.

In random linear network coding, by independently and randomly choosing
a set of coding coefficients from a finite field, each node sends
out the coded packet as a linear combination of the
node's received packets. The combination information is
specified by a \emph{coefficient vector} in the packet header, which is
updated by applying the same linear transformations as to the data. When
one node receives a full set of linearly independent coded packets,
it can decode and recover the original packets. In this paper, we
focus on the distributed algorithm design. The discussions of
decoding probability and implementation details
can be found in~\cite{ho2009dynamic,chou2003practical}.

Under the setting of network coding, we can consider  as a {}``virtual''
information flow between  and . Multiple information flows
{}``piggyback'' together to transmit over the physical links. The
actual physical rate over a physical link is only the maximum rate
of individual information flows passing over it. Let  be the physical
flow rate over a link , then we have 
for all .

With the above understanding, we formulate the problem of maximizing
broadcast rate under configuration  as follows: 
 where  is a twice-differentiable strictly concave
utility function\footnote{It might seem unnecessary to involve a strictly concave utility
function in this formulation. The reason is that we later design a
primal-dual algorithm to solve the problem, and using a strictly concave
utility function can avoid its potential instability problem \cite{chen2008ump}.},  denotes the indicator function. The constraints
in (\ref{eq:MPC1}) describe the flow conservation requirements. The constraints in (\ref{eq:MPC2}) come from the piggybacking
property of information flows. The node upload capacity constraints
are in (\ref{eq:MPC3}). The problem  is a convex problem.
All feasible broadcast rates must satisfy the constraints in (\ref{eq:MPC1})-(\ref{eq:MPC3})
and are achievable by using random linear network coding.

\subsection{Algorithm Design via Lagrange Decomposition}

To proceed, we first relax the first set of constraints in \eqref{eq:MPC1}
in problem  to obtain a partial Lagrangian as follows:

 where  are Lagrange multipliers,
, and .

The strong duality holds for problem  since the Slater
conditions are satisfied \cite{Boyd04}. Therefore, we can solve problem  by finding the saddle points of .

Noticing that 
 and 
 we can find the saddle points of 
by solving the following problem successively in :



Given  and , we consider the following
scheduling subproblem on : 
 The above linear programming problem has a structure that allows
us to solve it distributedly. The first observation is that if
an optimal  is given, then an optimal 
can be obtained as follows: , 
 As such, it is sufficient to study the following problem in :

 where 
 denotes the aggregate back-pressure between two neighboring nodes
 and , and .

For any , let

be one of its neighbors with the maximum back-pressure (breaking ties
arbitrarily). Then one optimal solution for problem 
is as follows:
 and 
 Given  and , primal-dual
algorithms can be designed to adapt  and 
to pursue the desired optimal solution.

We summarize the above analysis into a distributed algorithm including
the following components:

\textbf{Primal-dual Rate Control: }we pursue the saddle point in 
and  simultaneously as follows: 
 where  and  are positive step sizes, and the function



\textbf{Neighbor Scheduling, Content Scheduling, and Network Coding:}
Every node  maintains a queue storing packets that are intended
for . Whenever a transmission opportunity arises, node  chooses
one neighbor  with the maximum back-pressure according to \eqref{eq:opt.neighbor}.

If , node  sends packets to  at rate
. Every output packet is constructed as follows. Node 
chooses one packet from the head of each queue of  if ,
and output one random linear combination of these heard-of-queue packets.
If otherwise  or there is no head-of-line packets
to code, node  does nothing.

We have the following observations.
\begin{itemize}
\item The Lagrangian variable  is proportional to the length
of queue storing packets that are intended for receiver . The
back-pressure  measures the aggregate difference in the queues
of all  between  and . The larger the back-pressure is, the
more desperate node  wants to receive data from .
\item Our algorithm can be implemented in a distributed manner. It only
requires nodes to exchange information with its one-hop neighbors,
and thus is robust to peer churn and system dynamics. When a new peer arrives,
it connects to a set of neighbors, assigned by the streaming server or trackers. Then the peer starts exchanging streaming data with them following the strategy defined by our algorithm. When a peer leaves, its neighbors are informed and then close the connections. For the network coding operation, theoretically we need to adjust the size of field where the coding coefficients are chosen to make sure of the decoding probability when the number of nodes changes \cite{all:article:HMKKESL04}, \cite{all:RandomNC:SET03}. While \cite{chou2003practical} and \cite{wang2007r} show that in practice the finite field  or  is enough to have a sufficiently high decoding probability. Therefore, only local
configuration changes corresponding to dynamics, which is easy to implement
compared to centralized algorithms where typically global information is needed for whole configuration change (e.g., spanning trees reconstruction in spanning tree based solutions).
\item Although our algorithm is designed for P2P broadcast scenarios, it
also works for P2P multicast scenarios where helper nodes exist. The
helper nodes simply also perform the operations described in
\eqref{eq:g_vu}-\eqref{eq:PD}. Our algorithm can be considered as
the extension of the algorithm in~\cite{ho2009dynamic} from
link-capacity-limited underlay networks to node-capacity-limited
overlay networks.
\end{itemize}

The following theorem characterize the convergence of the proposed algorithm.
\begin{theorem}\label{thm:converge}
The algorithm in \eqref{eq:g_vu}-\eqref{eq:PD} converges to the optimal solution of problem 
globally asymptotically in time.
\end{theorem}
The proof utilizes standard Lyapunov arguments and a Lyapunov function for primal-dual algorithm, similar to those used
in~\cite{chen2008ump}, \cite{all:mathematics.cong.ctrl}. The proof is relegated to Appendix \ref{sec:proof_converge}.

\textbf{Remark:} We derive our algorithm and prove its convergence based on a fluid model formulation.
It is also possible to obtain a similar back-pressure based distributed algorithm with
packet-level dynamics taken into account and prove its stability,  following
a set of Lyapunov drift arguments elaborated in \cite{georgiadis2006resource}.



\begin{comment}
\begin{itemize}
\item In our algorithm, problem \eqref{eq:solve-g} is solved through the
back-pressure scheduling in a way that the neighbor with the maximum
aggregate back-pressure is selected and the transmission rate is set
as the node's upload capacity. Network coding guarantees the transmission
rate can be achieved.
\item Compared to the classical back-pressure algorithm \cite{georgiadis2006resource},
our algorithm is derived through the method of Lagrangian decomposition.
By considering packet-level dynamics, we can obtain similar distributed
solutions. Also our back-pressure scheduling involves the neighbor
selection process uniquely. The uniqueness is determined by the feature
of P2P networks where node's upload capacity is the bottleneck.
\item For each node , it chops its time into slots. In each time
slot, node  selects one neighbor according to the neighbor scheduling
policy, and sends out one coded packet according to the content scheduling
policy at the rate of its upload capacity. Every node updates 
accordingly and the source updates the streaming rate . We remark
that the algorithm
\end{itemize}

\end{comment}

\section{The Proposed Distributed Topology Hopping Algorithm}

\label{sec:maf}

We recently proposed in \cite{MA:CLSC10} to use Markov chain as a principled approach in designing distributed algorithms for solving combinatorial network problems approximately. In particular, we show one can design distributed algorithms for a combinatorial network optimization problem in the following way. First, construct a special class of Markov chains with problem-specific steady-state distribution. Second, search for a Markov chain in this class that allows distributed implementation. If such Markov chain can be found, which is usually challenging and problem-specific, the distributed implementation directly yields a distributed algorithm for the problem.

In this paper, we follow the framework from \cite{MA:CLSC10} and design a distributed topology hopping algorithm for our problem \eqref{MWC}. There are two steps in designing
our algorithm under the Markov approximation framework \cite{MA:CLSC10}:
log-sum-exp approximation and constructing problem-specific Markov chains that allows distributed implementation.

\begin{comment}
The idea behind our design is to let the P2P system adapt peer configurations
to pursue good-performance topologies and stay in them for majority
of the time.
\end{comment}
{}


\subsection{Log-Sum-Exp Approximation}

First, the maximum broadcast rate can be approximated by a log-sum-exp
function as follows: 
 where  is a positive constant. Let  denote
the size of the set , then the approximation accuracy
is known as follows \cite{MA:CLSC10}: 


As  approaches infinity, the approximation gap approaches
zero. As discussed in \cite{MA:CLSC10}, however, usually  should not
take too large values as there are practical constraints or convergence
rate concerns in the algorithm design afterwards.

To better understand the log-sum-exp approximation, we associate with each configuration 
a probability . Consider the following problem

Its optimal value is  and is obtained by setting the probability corresponding to one of the best configurations
to be one and the rest probabilities to be zero. Hence, problem 
is equivalent to the original problem .

On the other hand, according to \cite{MA:CLSC10} we have the following
observations.
\begin{theorem}[cf. \cite{MA:CLSC10}] The optimal value of the following
optimization problem 
is given by .
The optimal solution of problem  is given by

 \end{theorem}

As such, by the log-sum-exp approximation in (\ref{appro1}),
we obtain an approximate version of the maximum broadcast rate problem
, off by an \emph{entropy }term .
If we can time-share among different configurations according
to the optimal solution  in \eqref{dist_no_error},
then we can solve the problem  approximately and obtain
a close-to-optimal broadcast rate.

\subsection{Markov Chain Guided Algorithm Design}

We design a Markov chain with a state space being the set of all feasible peering configurations  and has a stationary distribution as  in \eqref{dist_no_error}. We implement the Markov chain to guide the system to optimize the configuration. As the system hops among configurations, the Markov chain converges and the configurations are time-shared according to the desired distribution .

The key lies in designing such Markov chain that allows distributed
implementation. Since  in \eqref{dist_no_error}
is product-form, it suffices to focus on designing time-reversible Markov chains
\cite{MA:CLSC10}.

Let  be two states of Markov chain, and denote
 as the transition rate from state 
to . We have two degrees of freedom in designing a time-reversible
Markov chain:
\begin{itemize}
\item \textbf{The state space structure}: we can add or
cut direct transitions between any two states, given that the state
space remains connected and any two states are reachable from each
other.
\item \textbf{The transition rates}: we can explore various options
in designing , given that the detailed balance
equation is satisfied, i.e., 
Satisfying the above equations guarantees the designed Markov chain has the desired stationary distribution
as in \eqref{dist_no_error}.
\end{itemize}

Recall that for a node , the set of its neighbors under configuration
 is denoted by . We call node in  's in-use neighbor and node in  's not-in-use neighbor. For the ease of explanation,
we further define  as the set of all the node-pairs under , i.e., . Note we do not differentiate node pairs  and .
As an example, for the peering configuration  shown in Fig.~\ref{fig:pc}(b),  is given by .

In our Markov chain design, we first specify its state space structure
as follows: we set the transition rate  to be zero, unless  and  satisfy that
 or .
In other words, we only allow direct transitions between two configurations
if such transitions correspond to a single node adding a new node in its in-use
neighbor set or removing one in-use neighbor from its in-use neighbor set.

Second, given the state space structure of Markov chain, we design
the transition rates to favor distributed implementation
while satisfying the detailed balance equation in (\ref{eq:detailed.balance.eq}).

One possible option is to set  to be .
One way to implement this option is for every node to generate a timer according to its
\emph{measured} receiving rate and counts down accordingly. When the
timer expires, the dedicated node performs the neighbor swapping and resets its timer. As
simple as the implementation may sound, this option is expensive to implement. Once
the peering configuration changes, the system needs to notify all
the nodes to measure the new receiving rate and reset their timers
accordingly. It is not clear how to implement such system-wide notification
in a low-overhead manner.

In this paper, we design  and
 as follows:

and

where  is a constant. It is straightforward to verify that
detailed balance equation is satisfied. As will be clear in the next subsection, our choices of transition rates
do not require coordination or notification among peers in its implementation.


\subsection{Distributed Implementation\label{sub:Distributed-Implementation}}

One distributed implementation of our designed Markov chain is briefly
described as follows.
\begin{itemize}
\item \textbf{Initialization:} Each peer  randomly selects
neighbors from its neighbor list  under the node degree bound and builds connections with these selected neighbors.
\item \textbf{Step 1:} Let  denote the current configuration. Each node
 generates an exponentially distributed random number independently
with mean ,
and counts down according to this number.
\item \textbf{Step 2:} When the count-down expires, node  measures its
current receiving rate as an estimate of the broadcast rate .
Then with probability  node  goes to the \textbf{Step 2a};
with probability , node  goes to the \textbf{Step 2b};
    \begin{itemize}
        \item \textbf{Step 2a:} Node  randomly selects one in-use neighbor
        in  and removes it from . Under the new peering
        configuration , node  measures its receiving rate as an estimate of
        . With the estimates of  and , peer  stays in the
        new configuration  with probability , and switches back to  with probability . Node  then repeats \textbf{Step 1}.
        \item \textbf{Step 2b:} Node  randomly selects one not-in-use neighbor
        in . If the node degree of the selected not-in-use node is equal to
        the bound or 's node degree is equal to the bound, node  jumps back to
        \textbf{Step 1} immediately. Otherwise,
        node  adds this selected node into . Under the new peering
        configuration , node  measures its receiving rate as an estimate of
        . With the estimates of  and , peer  stays in the
        new configuration  with probability , and switches back to  with probability . Node  then repeats \textbf{Step 1}.
    \end{itemize}
\end{itemize}

It is straightforward to summarize the above implementation into a distributed algorithm that runs on individual nodes and utilizes only the measurement from their one-hop neighbors. The correctness of the implementation is shown as follows:
\begin{proposition}
\label{implement_mc} The implementation in fact realizes a time-reversible
Markov chain with stationary distribution in \eqref{dist_no_error}.
\end{proposition}
The proof is relegated to Appendix \ref{sec:proof_mc}.

\textbf{Remarks:} a) In \textbf{Step 1}, the generation of count-down
timers does not depend on the receiving rate, thus the system does
not need to notify the nodes about changes of peering configurations.
b) With the above implementation, the system hops towards configurations
with better broadcast rate probabilistically. For example, if ,
then the system will be more likely to stay in configuration 
than in , and vice versa. c) With large values of , the system
hops towards better configurations more greedily. However,
this may as well lead to the system getting trapped in locally optimal configurations.
Hence there is a trade-off to consider when setting the value of .
Moreover, the value of  also affects the convergence rate
of the time-reversible Markov chain to the desired stationary distribution.
It is worth future investigation to further understand the impact
of . d) In the presence of peer dynamics, our algorithm incurs only simple actions based on local information. When a new peer arrives, a neighbor set and a neighbor list are assigned
to it. The peer builds connections with the nodes in the neighbor set. Then the peer starts counting down as \textbf{Step 1} and follows the strategy of our algorithm. When a peer leaves, we just eliminate it from the neighbor list of its previous neighbors and end up connections.

\section{Convergence Properties of Overall Solution}

\label{sec:bp+ma}

\begin{algorithm}[hbt!]
\caption{Broadcasting Algorithm}
\label{alg:bp}
\begin{algorithmic}[1]
\STATE The following procedure runs on each individual node independently.
\STATE For the source  and each time slot,
\STATE 
\STATE For each node  and each time slot,
\STATE 
\FOR {}
   \FOR {for }
   \STATE {}
   \ENDFOR
   \IF {}
   \STATE {}
   \STATE {}
   \ENDIF
\ENDFOR
\IF {}
   \FOR {}
      \IF {}
      \STATE {}
      \ENDIF
   \ENDFOR
\ENDIF
\FOR {}
\STATE {}
\ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[hbt!]
\caption{Topology Hopping Algorithm}
\label{alg:ma}
\begin{algorithmic}[1]
\STATE The following procedure runs on each individual node independently. We focus on a particular node .
\STATE \textbf{procedure} Initialization
\begin{itemize}
\item Initialize , ; randomly connects to peers from  under the degree bound.
\item Generate a timer that follows exponential distribution with mean equal to  and begin counting down.
\end{itemize}
\STATE \textbf{end procedure}
\STATE {}
\STATE {When the timer expires, invoke the procedure Transition.}
\STATE \textbf{procedure} Transition
\STATE With probability ,\\
\STATE ;\\
\STATE randomly remove one in-use neighbor from ;\\
\STATE ;\\
\STATE    with probability\\
       ;\\
\STATE refresh the timer and begin counting down;\\
\STATE With probability ,\\
\STATE ;\\
\STATE randomly add one not-in-use neighbor  in\\
        to ;\\
\STATE \textbf{if}  or \\
\STATE refresh the timer and begin counting down;\\
\STATE \textbf{end if}\\
\STATE ;\\
\STATE    with probability\\
       ;\\
\STATE refresh the timer and begin counting down;\\
\STATE \textbf{end procedure}
\end{algorithmic}
\end{algorithm}

We have designed the distributed broadcasting algorithm in Section
\ref{sec:streaming_rate} and the Markov chain guided topology hopping
algorithm in Section \ref{sec:maf}. The pseudocodes of each algorithm
are shown in Algorithm~\ref{alg:bp} and Algorithm~\ref{alg:ma} respectively.
Both algorithms are simple to implement, run on each individual node,
and only require nodes to exchange information with their neighbors.

If the broadcasting algorithm converges instantaneously, i.e., time-scale
separation assumption holds, then we can obtain the accurate value
of  for any configuration . Transiting based
on the accurate , the designed Markov chain will converges
to the desired stationary distribution in \eqref{dist_no_error}.
Hence by operating these two algorithms in tandem, we obtain a close-to-optimal
broadcast rate under arbitrary node degree bounds, and over arbitrary
overlay graph. The optimality gap is characterized in (\ref{aa1}).

In practice, however, it is possible to obtain only an inaccurate
measurement or estimate of . These inaccuracies root in two
sources. One is the noisy measurements of the maximum broadcast rates
given the configuration. The other is the fast state transition of
Markov chain, i.e., the Markov chain transits before the underlying
broadcasting algorithm converges and thus it transits based on inaccurate
observations of the broadcast rates.

Consequently, the topology hopping Markov chain may \emph{not} converge
to the desired stationary distribution .
This observation motivates our following study on the convergence
of Markov chain in the presence of inaccurate transition rates.

For each configuration  with broadcast rate ,
we assume its corresponding inaccurate observed rate belongs to the
bounded region . 
is the inaccuracy bound and can be different for different .

For easy explanation of our approach, we further assume the observed
broadcast rate for configuration  only takes one of the following
 discrete values: 
 where  is a positive constant. Further, with probability
, the observed broadcast rate takes value
,  and
.

With the inaccurate observed broadcast rates, the topology hopping
behaves as follows. Suppose the current configuration is  and
the observed broadcast rate is ,
where . After some count-down process,
the system hops to a new configuration  and probes its broadcast
rate. In configuration , the broadcast rate is observed as .
The system stays in the new configuration  with probability 
and switches back to configuration  with probability 
By arguments similar to the proof of Proposition~, the transition
rate from configuration  with broadcast rate 
to configuration  with broadcast rate 
is given by 


We construct a Markov chain to capture and study the above topology
hopping behavior. In this Markov chain, a state is associated with
a configuration and an observed broadcast rate. Given any configuration
 and its corresponding , there are
 states in the extended Markov chain: .
Further, Given direct transitions between configuration  and 
in the original topology hopping Markov chain, there are direct transitions
between states  and 
()
in the corresponding new Markov chain. The corresponding transition
rates are shown as follows: 
 and 
where  and
. This new Markov chain
can be thought as an extended version of the original topology
hopping Markov chain. As an example, an extended Markov chain is
shown and explained in Fig. \ref{mc_inexact}.

\begin{figure}
\centering \includegraphics[scale=0.4]{mc2} \centering
\caption{An example of the original three-state topology hopping Markov chain and the extended Markov chain. M is the original topology
hopping Markov chain with accurate broadcast rates.
M' is the corresponding extended Markov chain with inaccurate broadcast
rate observations. For each configuration ,
the observed broadcast rate takes values , ,
 with probability  and 
respectively. The transition rates are assigned according to \eqref{tran_rate_mc1} and \eqref{tran_rate_mc2}.
}


\label{mc_inexact}
\end{figure}


The extended Markov chain has a unique stationary distribution since
it is irreducible and only has a finite number of states. We can study
the impact of inaccurate broadcast rates by comparing the stationary
configuration distribution of the new Markov chain and that
of the original topology hopping Markov chain.

We denote the stationary distribution of the \emph{states} in the
new Markov chain by 
We also denote 
as the stationary distribution of the \emph{configurations} in the
extended Markov chain. Given a configuration , there
are  states associated with  in the extended Markov
chain. We have 

Recall that the stationary distribution of the configurations for
the original topology hopping Markov chain is .
We use the total variance distance \cite{Diaconis91} to quantify
the difference between  and ,
as 

We have the following result: \begin{theorem} \label{error_impact}
Let , and .
The  are bounded
as follows: 
 Further, the optimality gap in broadcast rates  is bounded as below:

\end{theorem}
The proof is relegated to Appendix\ref{sec:error_impact}.

\textbf{Remarks:} a) The upper bound on
 shown in
\eqref{upper_bound} is general, as it is independent of the number
of configurations , the values of , and the
distributions of inaccurate observed rates
.
b) The upper bound on
 shown in
\eqref{upper_bound} decreases exponentially with the worst
inaccuracy bound  decreasing. c) It would be
interesting to explore a tighter upper bound on
 than the one in
\eqref{upper_bound}.

\section{Performance Evaluation}

\label{sec:simu}

We implement a packet-level simulator to our proposed solutions and use this simulator to evaluate the performance of our solutions.


\subsection{Settings}

In our simulations, time is chopped into slots of equal length, and we adopt three different settings. In Setting I, we set the total number of nodes to be , and assign the node upload capacities randomly according to the distribution
in Table \ref{tab:distrib}, which is obtained from the uplink bandwidth statistics of Internet hosts~\cite{vodprofitable}. We set the source's upload capacity to be  kbps; with this upload capacity, source is not the broadcast bottleneck~\cite{all:Mutualcast:LPZ05,all:P2PStreaming:KLR.07}.

Setting II is the same as Setting I, except we set the total number of nodes to be .

In Setting III, there are  different peering configurations as shown in Fig.~\ref{fig:4_config}. Every node has a unit capacity. Under configuration ,  and  the maximum broadcast rate is , and under configuration  the maximum broadcast rate is .

When running our network coding based broadcasting algorithm, we set the updating step size of  and  to be  and  respectively. These parameters are empirically chosen to obtain smooth algorithm updating and small errors.

In our simulations, we assign node degree bounds in the following two ways. The first is to set identical bound on each node's node degree. The second is to set degree bound proportional to the node's upload capacity. This is based on the empirical observations that nodes with high upload capacities usually have more system resource (e.g., memory and CPU power) than nodes with low upload capacities. With more system resource, nodes can maintain more concurrent connections, thus have larger node degree bounds. In our second degree bounds assignment, nodes set their node degree bounds proportional to the ratio between their upload capacities and  kbps. In particular, nodes with  kbps have a degree bound of , and nodes with  kbps have a degree bound of , etc.

We carry out two sets of simulations. First, we evaluate the performance of our distributed broadcasting algorithm under Setting I and II. Second, we evaluate the overall performance when we combine the topology hopping algorithm and the broadcasting algorithm under Setting I and III. In these two sets of simulations, we also compare the performance under the two degree bounds assignments explained in the previous paragraph.

\begin{table}
\caption{Peer upload capacity distribution}


\label{tab:distrib}

\centering{}\begin{tabular}{|c|c|c|c|c|c|}
\hline
Upload Capacity (kbps)  & 64  & 128  & 256  & 384  & 768 \tabularnewline
\hline
Fraction (\%)  & 2.8  & 14.3  & 4.3  & 23.3  & 55.3 \tabularnewline
\hline
\end{tabular}
\end{table}

\begin{figure}[hbt!]
\centering
\subfigure{\includegraphics[width=0.35\textwidth]{4_diff_config}}
\caption{Peering configurations under Setting III. For the ease of illustration, we only allow node  to add or remove neighbors between nodes  and . The rest nodes keep their neighbors fixed.}
\label{fig:4_config}
\end{figure}

\begin{figure*}[hbt!]
\centering \mbox{ \subfigure[]{\includegraphics[width=0.25\textwidth]{bp_packet_level_node10_bound3}\label{fig:bp.10.bound3}}
\subfigure[]{\includegraphics[width=0.25\textwidth]{bp_packet_level_node100_bound4}\label{fig:bp.bound3}}
\subfigure[]{\includegraphics[width=0.25\textwidth]{bp_packet_level_node100_bound10}\label{fig:bp.prop.bound}}
\subfigure[]{\includegraphics[width=0.25\textwidth]{bp_packet_level_cdf}\label{fig:degree.bound.comparison}}
} \\
 \caption{Broadcasting algorithm evaluations. a) The source
broadcast rate and average peer receiving rate under Setting II when degree bound is set to ; b) The source broadcast rate and average peer receiving rate under Setting I when degree bound is set to ; c) The source broadcast rate and average peer receiving rate under Setting I when degree bound is set to 10. d) This figure shows the impact of degree bound on the peer receiving rate under Setting I. The full-mesh rate is the maximum broadcast rate when the node degrees are unbounded~\cite{all:Mutualcast:LPZ05}.}
\label{fig:bp}
\end{figure*}

\begin{figure}[hbt!]
\centering
\subfigure[]{\includegraphics[width=0.24\textwidth]{ma_optimal_config_distrib}\label{fig:distrib.a}}
\subfigure[]{\includegraphics[width=0.24\textwidth]{ma_config_distrib}\label{fig:distrib.b}}
\caption{a) Optimal configuration distribution for different values of  under Setting III; b) Configuration distribution obtained by our algorithm for different values of  under Setting III.}
\end{figure}

\begin{figure*}[hbt!]
\centering \mbox{
\subfigure[]{\includegraphics[width=0.31\textwidth]{ma_packet_level_bound3_beta20}\label{fig:ma.a}}
\subfigure[]{\includegraphics[width=0.31\textwidth]{ma_packet_level_bound3_beta50}\label{fig:ma.b}}
\subfigure[]{\includegraphics[width=0.31\textwidth]{ma_packet_level_prop_beta20}\label{fig:ma.c}}
} \\
\caption{Evaluation of our overall solution which combines the topology hopping algorithm and the broadcasting algorithm. a) The average peer receiving rate when the node degree bound is  and  is ; b) The average peer receiving rate when the node degree bound is  and  is ; c) The average peer receiving rate when peer degree bound is proportional to its upload capacity and  is . The percentage of average receiving rate improvement of our overall algorithm against our broadcasting algorithm and the simple heuristic algorithm are shown in these three figures. For example, in (a),  means that the average receiving rate of our overall algorithm is  times of that of our broadcasting algorithm, and  means that the average receiving rate of our overall algorithm is  times of that of the simple heuristic algorithm.}
\end{figure*}


\subsection{Evaluation of the Proposed Broadcasting Algorithm }

In this simulation, we evaluate our distributed broadcasting algorithm
proposed in Section~\ref{sec:streaming_rate}. We randomly choose a sub-graph that satisfies the node degree bounds constraints, and run our algorithm over it. We evaluate three aspects of the proposed algorithm: 1) does it converge to optimal broadcast rate as expected from theoretical analysis? 2) How fast does it converge?
3) How would different values of degree bounds affect the maximum
broadcast rate? The results are summarized in Fig.~\ref{fig:bp}.

From Fig.~\ref{fig:bp}(a) and Fig.~\ref{fig:bp}(b), we see that our broadcasting algorithm converges. It converges faster in the small size network as shown in Fig.~\ref{fig:bp}(a) than in the large size network as shown in Fig.~\ref{fig:bp}(b). From  Fig.~\ref{fig:bp}(d), we also see the converged rate when the node degree bond is  is very close to a theoretical upper bound -- the optimal broadcast rate under no degree bounds computed according to~\cite{all:Mutualcast:LPZ05}, \cite{chen2008ump}, \cite{all:P2PStreaming:KLR.07}. This suggests that our algorithm converges to the optimal broadcast rate.

Under different degree bounds, the optimal broadcast rate varies. Fig.~\ref{fig:bp}(d) shows that the optimal broadcast rate increases when we increase the node degree bounds. We plot the CDF of peer receiving rates (after the broadcasting algorithm converges) for the case where degree bound is , , and proportional to the peer's upload capacity. It's seen that when the bound is , the obtained rate is close to the full-mesh rate, which suggests that we do not need a large degree bound to achieve close to the full-mesh rate. The obtained rate is also close to the full-mesh rate when degree bound is proportional to the peer's upload capacity.

\subsection{Evaluation of the Overall Solution}

Our overall solution, which combines the Markov chain guided topology hopping algorithm and
the back-pressure and network coding based broadcasting algorithm, achieves the near optimal broadcast rate
under arbitrary node degree bound and over arbitrary overlay graph.
To evaluate its performance, we generate a sub-graph randomly, run our algorithms on
every node, and evaluate the achieved broadcast rate.


The topology hopping algorithm runs on top of the broadcasting algorithm. Under given topology, the broadcasting algorithm achieves the optimal broadcast rate. Nodes swap neighbors based on their observed receiving rate, thus changing the topology from time to time. In the simulation, we run the broadcasting algorithm long enough so that it converges before the topology transits according to the Markov chain. This way, the overall algorithm converges to the close-to-optimal broadcast rate.

In all simulations, we compare our overall algorithm with our back-pressure and network coding based broadcasting algorithm to illustrate the benefit of topology hopping, and with a simple heuristic algorithm introduced below to illustrate the benefit of our overall solution. Remind that no existing works solve the problem of streaming-rate maximization under general node degree bounds and over arbitrary topology we studied in this paper.

The simple heuristic algorithm we compare our overall algorithm against is also composed of two parts: routing-based broadcasting algorithm and random topology hopping algorithm. In routing-based broadcasting algorithm, each peer evenly allocates its upload capacity to its neighbors. Given the topology and capacity allocation, a centralized routing strategy (e.g. spanning trees based solution) is used to achieve the best broadcast rate the system can support. Similarly, the random topology hopping algorithm runs on the top of the broadcasting algorithm. Every peer maintains a timer. When the timer of one peer expires, the peer randomly drops one active neighbor which is exchanging data with it, and then selects one random candidate from its feasible neighbor list and starts to exchange data with it. By doing so, we actually allow nodes running the simple scheme to have a node degree beyond the bounds. This relaxation gives the simple scheme more degree of freedom to optimize its performance. Overall, the topology changes randomly on the top under which peers use routing to exchange streaming data.

Our first observation is that our overall scheme converges to the solution that theory predicts. We carry out simulations under Setting III.  Under this setting the optimal broadcast rate is . The optimal configuration solution to problem  is calculated and shown in Fig.~\ref{fig:distrib.a} for different values of . We run the overall scheme for this specific case and show the empirical configuration distribution in Fig.~\ref{fig:distrib.b}. Comparing the distributions in Fig.~\ref{fig:distrib.a} and Fig.~\ref{fig:distrib.b}, we can see that the distribution obtained by our overall solution is very close to the optimal one. We also calculate the achieved broadcast rate under different values of . For  and , the broadcast rate is , , and  respectively. We see that with large , the achieved broadcast rate is close to the optimal value , as predicted by our analysis in Section~\ref{sec:maf}.

Next, we evaluate our overall solution under Setting I. In Fig.~\ref{fig:ma.a} and Fig.~\ref{fig:ma.b}, the broadcast rates obtained are  kbps and  kbps respectively. They are about  and  higher respectively than the broadcast rate  kbps achieved by running the broadcasting algorithm over a randomly chosen topology. This demonstrates the advantage of performing topology hopping to optimize the configuration, as compared to only randomly choosing topology.

By setting node degree bounds proportional to peers' upload capacity, nodes with higher upload capacity maintain more connections. From Fig.~\ref{fig:ma.c}, we observe that this flexibility offers a broadcast rate of  kbps. Although the additional gain of topology hopping is small under the specific P2P simulation settings (e.g., node uplink capacity distribution), we remark that our topology-hopping based algorithm is theoretically guaranteed to achieve close-to-optimal streaming rate under arbitrary node degree bounds and P2P settings, while the broadcasting algorithm with random topology selection has no performance guarantee. Moreover, in practical P2P streaming systems, the node degree bounds are typically small. For example, in PPLive, the node degree bounds are 15-20 \cite{streaming_capacity.allerton09}, while the size of the system (i.e., total number of peers that are simultaneously watching the same channel) is usually hundreds of thousands. Thus, we suspect we can see substantial gain of topology hopping if our algorithm is implemented in such system with small node degree bounds, as suggested by our simulation results under small node degree bounds.

From Fig.~\ref{fig:ma.a}, Fig.~\ref{fig:ma.b} and Fig.~\ref{fig:ma.c}, we observe that the average receiving rate of our overall algorithm is about 5.5-7 times higher than that of the simple algorithm respectively. And also we can see from Fig.~\ref{fig:ma.a} and Fig.~\ref{fig:ma.b}, our algorithm can achieve smoother streaming rate than the simple algorithm because our algorithm optimizes the topology hopping and stays in the optimal topology while the simple algorithm hops among topologies randomly and arbitrarily.

\section{Discussions and Future Work}

\label{sec:conclusion}

In this paper, we propose a distributed solution to achieve a
near-optimal broadcast rate under arbitrary node degree bounds, and
over arbitrary overlay graph. Our solution is distributed and
consists of two algorithms that can be of independent interests. The
first is a distributed broadcasting algorithm that optimizes the
broadcast rate given a P2P topology. It is derived from a network
coding based problem formulation and utilizes back-pressure
arguments. It can be considered as the extension of the algorithm
in~\cite{ho2009dynamic} from link-capacity-limited underlay networks
to node-capacity-limited overlay networks. The second algorithm is a
Markov chain guided hopping algorithm that optimizes the topology,
inspired by the Markov Approximation framework introduced
in~\cite{MA:CLSC10}.

Assuming the underlying broadcasting algorithm converges instantaneously,
the topology hopping algorithm converges to the optimal configuration
distribution. When the broadcasting algorithm does not converge fast enough,
the topology hopping Markov chain transits based on inaccurate observations of the maximum broadcast rates associated with the configurations.
We show that the topology hopping algorithm still converges, but to a sub-optimal configuration distribution. We characterize an upper bound on the total variance
distance between the optimal and sub-optimal configuration distributions, as well as an upper bound on the gap between the achieved and the optimal broadcast rates. We show that both bounds decreases exponentially as the bound on inaccuracy decreases.

Using uplink bandwidth statistics of Internet hosts, our simulations
validate the effectiveness of the proposed solutions, and demonstrate
the advantage of allowing node degree bounds to scale linearly with
their upload capacities.

In the scenarios where network coding is not allowed, we can formulate the broadcasting problem in Subsection \ref{subsec:nc-formulation} as a linear program to construct a feasible node capacity allocation so that the sum of rate of all spanning
trees is maximized \cite{streaming_capacity.allerton09}, which is solvable by centralized LP algorithms. Then we can design the overall algorithm in the following way. The overall algorithm is also composed of two separate algorithms: the spanning tree based broadcasting algorithm and the Markov chain guided hopping algorithm. The topology hopping algorithm is same as the one in Section \ref{sec:maf} which runs on the top of the broadcasting algorithm and guides the topology hopping. Compared to our distributed overall algorithm when network coding is applied, this algorithm is centralized making it unsuited for use in a dynamically changing systems.

Two interesting future directions are as follows. First,
the convergence rate of our solution is determined by the mixing time
of the topology-hopping Markov chain, which can be substantial for
large P2P systems. It is thus of great interest to explore the
design of topology-hopping Markov chains that mix fast and at the
same time allows distributed implementation. Second, while our algorithms adapt well to peer dynamics, our theoretical analysis is for static scenarios. How to extend the analysis to dynamic scenarios such as those observed in practical P2P systems \cite{wang2008stable} is another interesting future direction.

\section*{Acknowledgement}

This work was partially supported by the General Research Fund grants (Project No.
411008, 411209, 411010) and an Area of Excellence Grant (Project No.
AoE/E-02/08), all established under the University Grant Committee
of the Hong Kong SAR, China. This work was also partially supported
by two gift grants from Microsoft and Cisco.

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,ref,p2p}

\appendices
\section*{Appendix}

\subsection{Proof of Theorem \ref{thm:converge}} \label{sec:proof_converge}

We use the following Lyapunov function


where  are the saddle points
of \eqref{eq:partial.lag}.

By differentiating the Lyapunov function with respect to time we get




KKT conditions for  are shown as follows


and

where is the optimal solution of .

From equation \eqref{eq:kkt1}, we obtain


By using the above equation\eqref{eq:eq1}, we can transform the
terms in the inequality \eqref{eq:diff.inequality1} as follows


and



We use the above two equations \eqref{eq:eq2} and \eqref{eq:eq3} to
substitute the corresponding terms in the inequality
\eqref{eq:diff.inequality1} and then get




Next we check the value of \eqref{eq:bound1}, \eqref{eq:bound2},
\eqref{eq:bound3} respectively. First, the strict concavity of 
implies




Since  are optimal solutions, they
should satisfy the constraints of the problem . So we
have


Therefore,


Note that  is the solution of the following problem


which is equivalent to . Since is
also feasible, for \eqref{eq:bound2} we have




Now we focus on the term\eqref{eq:bound3}. According to \eqref{eq:kkt2},
the following equality holds.

Note that  is the solution of the following problem


So, 


Overall, we get




Let 
and .
Since 
and 
we have . Let  be the
largest invariant set in . By LaSalle\textquoteright{}s
invariance principle  converges to
the set  as . Since ,
as   satisfies


and
\begin{small}

\end{small}
Further, in , . To see this, if this is not
satisfied, then by \eqref{eq:PD} we can see  will not stay in
, which is contradicted with \eqref{eq:asym.eq}. This
concludes the proof.


\subsection{Proof of Proposition \ref{implement_mc}}
\label{sec:proof_mc}
By two conditions for state space structure of Markov chain, we know
that all configurations can reach each other within a finite number
of transitions, thus the constructed Markov chain is irreducible.
Further, it is a finite state ergodic Markov chain with a unique
stationary distribution. We now show that the stationary
distribution of the constructed Markov chain is indeed
\eqref{dist_no_error}.

Now we verify that under the implementation, the state transition
rate from  to  satisfies (\ref{eq:transition rate}).

In our Markov chain design, we only allow direct transitions between two configurations
if such transitions correspond to a single node adding a new neighbor
or removing a neighbor, i.e.,  or . We consider these two scenarios
separately in the following.

Let  denote the event that when the timer expires
the process will enter state  after leaving the current state
. The probability of this event is denoted by .

When ,
assuming ,
the event  can be divided into two disjoint events: the
event that node 's timer expires, then node  selects node
 to remove and remove it from its in-use neighbor set and the
event that node 's timer expires, then node  selects node
 to remove and remove it from its in-use neighbor set. Denote
these two events by  and 
. Let  be the event that node  selects node  and removes
it from its in-use neighbor set and  be the event that node
 selects node  and removes it from its in-use neighbor set.
Now we calculate the probability of 
and  respectively.


and

Therefore, we have


When ,
assuming ,
similarly we divide  into two disjoint events 
and . 
denotes the event that node 's timer expires, then node  selects
node  to add and add it in its in-use neighbor set. 
denotes the event that node 's timer expires, then node  selects
node  to add and add it in its in-use neighbor set. Let 
be the event that node  selects node  and adds it as one in-use
neighbor and  be the event that node  selects node 
and adds it as one in-use neighbor. Then we have


and

Therefore, we have


In our implementation, under configuration , peer  counts down
with rate .
Therefore, the rate of leaving the state  is
.
With the probability , the process jumps to state 
when leaving state . So, the transition rate from state  to
 is


With \eqref{dist_no_error}, we see that
, i.e., the detailed balance equations hold. Thus the
constructed Markov chain is time-reversible and its stationary
distribution is indeed \eqref{dist_no_error} according to Theorem
1.3 and Theorem 1.14 in \cite{Kelly79}.

\subsection{Proof of Theorem
\ref{error_impact}}\label{sec:error_impact}

We denote  as the original topology hopping Markov chain with
exact broadcast rates, and  as the corresponding extended Markov
chain with inaccurately observed broadcast rates. For the
convenience of expression, for all , we use  to represent the state
 in the extended Markov chain ,
and  to represent distribution of inaccurate observed
rates .

Therefore, given direct transitions between configuration  and
 in the original topology hopping Markov chain , there are
direct transitions between states  and  () in the
extended Markov chain . Following \eqref{tran_rate_mc1} and
\eqref{tran_rate_mc2}, we have the corresponding transition rates

and

where  and
.

Now we compute the stationary distribution of states for the
extended Markov chain . By detailed balance equation, we have


Then we have


Therefore,

and


Consider an arbitrary state  in the extended Markov chain
, where  and . Since
state space of  is connected, we can always find a path to
connect  and  through a series of adjacent states
, and . Therefore,


and by \eqref{ratio_mc} we have


Then


By \eqref{ratio_error} and \eqref{ratio_mc2}, we know that ,

and


On the other hand, we have


By \eqref{db1}, \eqref{db2} and \eqref{db3}, we obtain the
stationary distribution of states for the extended Markov chain 
as follows:


The stationary distribution of peer configurations in the extended
Markov chain  is the probability distribution of aggregate
states , i.e.,


Let 

Then we have


By \eqref{dist_no_error}, we know


Let


It is not hard to see that , so


The total variation distance

where .

By \eqref{bigdt}, we know .

Therefore, ,


Since  and ,

and

by \eqref{aver_coeff} we know that 

and


Then by \eqref{aver_coeff2}, we have . Therefore,


So by \eqref{difference}, we have ,


Then,


Therefore,


This concludes the proof.

\end{document}
