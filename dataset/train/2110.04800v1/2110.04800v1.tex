
\onecolumn
\begin{appendix}
\begin{center}

{\LARGE \textbf{Appendix}}
\vspace{2mm}

\end{center}

\section{Approach}
\subsection{Image Cropping}
\label{cropping}
The viewpoint  comprises the scale factor , 3D spatial rotation parameters , and 3D translation parameters . The original image  is cropped to its canonical view in 2D plane with viewpoint . The cropping is given by , where the transformation from  to  is formulated in the following.

Bilinear interpolation is used if  or  is not an integer.

\subsection{Weak Perspective Transformation}
\label{tsfm}
The 3D spatial rotation is represented by a rotation vector : the unit vector  is the axis of rotation, and the magnitude  is the rotation angle. The weak perspective transformation is used to project the world-coordinate facial shape  to image-coordinate , as formulated in


\subsection{Barycentric Coefficients}
\label{barycentric}
Given the vertices of a triangle () and its enclosing grid point  on image. The barycentric coefficients can be computed by

The barycenteric coefficients , , and  are in the range of  if the grid point  is in the triangle.

\subsection{Wrapping Function}
\label{wrapping}
The wrapping function  is defined as , where  is the index for the vertices of a 3D face.  and  are 3-dimensional vectors.  is the coordinates of shape in UV space from 3DMM \cite{blanz1999morphable}. Again, bilinear interpolation is used if  or  is not an integer.



\newpage
\section{Experiments}
\subsection{Network Architecture}
\label{netarch}
We use standard encoder networks
for viewpoint, shape and illumination predictions, and a network similar to U-Net~\cite{ronneberger2015u} for reflectance prediction. The detailed configurations are given in Table~\ref{view_illum_shape}. Parameter  is 7 for viewpoint network  and 9 for illumination network . Conv  denotes convoluitonal layer with kernel size of 3, where the stride and padding are 2 and 1, respectively. Each convolutional layer is followed by a Batch Normalization (BN) \cite{ioffe2015batch} layer and Rectified Linear Units (ReLU). Bilinear interpolation is adopted for the upsampling operation. Specifically, in Table \ref{view_illum_shape}, the layers in brackets are residual blocks. In Table \ref{reflectance}, we use shortcut to connect the feature maps of encoder and decoder, but different from U-Net, we use addition rather than concatenation to integrate information in the feature maps. For those encoder output shapes in brackets (\eg, ``[]''), the feature map will be added as a shortcut to the decoder feature map (also with the same brackets).

\begin{table}[ht]
	\centering
	\footnotesize
	\setlength{\abovecaptionskip}{3pt}
    \setlength{\belowcaptionskip}{-5pt}
	\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
	\begin{tabular}{c|c|c|c|c|c}
		\hline 
		\multicolumn{3}{c|}{Viewpoint \& Illumination Network} & \multicolumn{3}{c}{Shape Network} \\ \hline
		Layer & Act. & Output shape & Layer & Act. & Output shape \\ \hline
		Input & - &  & Input & - &  \\
		Conv  & BN + ReLU &  & Conv  & BN + ReLU &  \\
		Conv  & BN + ReLU &  & Conv  & BN + ReLU &  \\
    	Conv  & BN + ReLU &  & Conv  & BN + ReLU &  \\ 
    	Conv  & BN + ReLU &  & Conv  & BN + ReLU &  \\
        \multirow{3}{*}{} & \multirow{3}{*}{} & \multirow{3}{*}{} & \multirow{3}{*}{} & \multirow{3}{*}{} & \multirow{3}{*}{} \\
		  &  &  &  &  & \\
		  &  &  &  &  & \\
		Conv  & BN + ReLU &  & Conv  & BN + ReLU &  \\ 
		\multirow{3}{*}{} & \multirow{3}{*}{} & \multirow{3}{*}{} & \multirow{3}{*}{} & \multirow{3}{*}{} & \multirow{3}{*}{}\\
		  &  &  &  &  & \\ 
		  &  &  &  &  & \\ 
		Conv  & BN + ReLU &  & Conv  & BN + ReLU &  \\
	    Conv  & - &  & Conv  & - &  \\
		\hline
	\end{tabular}
	\renewcommand{\captionlabelfont}{\footnotesize}
	\caption{\footnotesize The detailed CNNs architectures of viewpoint, illumination, and shape networks.}\label{view_illum_shape}
\end{table}


\begin{table}[ht]
	\centering
	\footnotesize
	\setlength{\abovecaptionskip}{3pt}
    \setlength{\belowcaptionskip}{-5pt}
	\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
	\begin{tabular}{c|c|c|c|c|c}
		\hline 
		\multicolumn{6}{c}{Reflectance Network} \\
		\hline
		\multicolumn{3}{c|}{U-Net Encoder ()} & \multicolumn{3}{c}{U-Net Decoder ()} \\
		\hline
		Encoder Layer & Act. & Output shape & Decoder Layer & Act. & Output shape \\ \hline
		Input & - &  & Output & - &  \\
		- & - & - & Conv  & Tanh &  \\
		- & - & - & Conv  & BN + ReLU &  \\
		Conv  & BN + ReLU &  & Upsample () & - &  \\
		Conv  & BN + ReLU & [] & Conv  & BN + ReLU & [] \\
    	- & - & - & Conv  & BN + ReLU &  \\
    	Conv  & BN + ReLU &  & Upsample () & - &  \\
    	Conv  & BN + ReLU & [] & Conv  & BN + ReLU & [] \\
    	- & - & - & Conv  & BN + ReLU &  \\ 
    	Conv  & BN + ReLU &  & Upsample () & - &  \\ 
    	Conv  & BN + ReLU & [] & Conv  & BN + ReLU & [] \\
    	- & - & - & Conv  & BN + ReLU &  \\ 
    	Conv  & BN + ReLU &  & Upsample () & - &  \\ 
    	Conv  & BN + ReLU & [] & Conv  & BN + ReLU & [] \\
    	- & - & - & Conv  & BN + ReLU &  \\ 
    	Conv  & BN + ReLU &  & Upsample () & - &  \\ 
    	Conv  & BN + ReLU & [] & Conv  & BN + ReLU & [] \\
    	Conv  & BN + ReLU &  & Conv  & BN + ReLU &  \\ 
    	Conv  & BN + ReLU &  & Upsample () & - &  \\
    	\hline
	\end{tabular}
	\renewcommand{\captionlabelfont}{\footnotesize}
	\caption{\footnotesize The detailed CNNs architectures of reflectance networks. Note that, the layers in the decoder (from input to output) are listed from bottom to top.}\label{reflectance}
\end{table}


\subsection{More Ablation Studies}
\label{moreablation}
\begin{figure}[t]
  \centering
\renewcommand{\captionlabelfont}{\footnotesize}
  \includegraphics[width=6.5in]{figures/ablation2_v4_new.pdf}
  \caption{\footnotesize Ablations. (a) CEST with default settings. (b), (c) and (d) Averaged reflectance is used in training and the number of images from each video clips are 2, 4, and 8, respectively. {\color{gray}Errata: we mistakenly use the wrong image in the ICCV 2021 version. We have replaced it with the correct one in this version. Please refer to our latest arXiv version for up-to-date results.}}\label{fig:ablation2}
  \vspace{-0.1in}
\end{figure}

We perform more ablations for different settings of CEST. We explore the averaged representations, an approach adopted in \cite{tewari2019fml}, for reflectance consistency, where the averaged reflectance of a video clip is used to reconstruct the 3D face in each video frame. Here, we fix the size of minibatch, \ie 128, but vary the number of images from each video clip to 2, 4, and 8. Results are shown in Fig. \ref{fig:ablation2} (b), (c), and (d), respectively. As we can see, there are still some illumination in the reflectance, indicating that the averaged representation may not be a good strategy for learning disentangled facial parameters. 


\begin{figure}[t]
  \centering
\renewcommand{\captionlabelfont}{\footnotesize}
  \includegraphics[width=6.5in]{figures/ablation3_v4_small.pdf}
  \caption{\footnotesize Ablations. (a) CEST with default settings. (b) Reflectance consistency is applied to videos, not video clips.}\label{fig:ablation3}
  \vspace{-0.1in}
\end{figure}
Fig. \ref{fig:ablation3} shows the results from CEST trained with reflectance consistency across video. The performance is comparable to those from CEST trained with default setting (reflectance consistency across video clip). It shows that consistency constraint can be generalized to longer videos if the recording environments are not changed dramatically.

\subsection{More Qualitative Comparisons}
\label{morequalitative}
In this section, we show more comparisons to the state-of-art methods \cite{booth20173d,sela2017unrestricted,richardson2017learning,thies2016face2face}. Since there is no publicly available implementations for these methods, we compare to the results presented in their papers.
\begin{figure}[t]
  \centering
\renewcommand{\captionlabelfont}{\footnotesize}
  \includegraphics[width=6.5in]{figures/compared_to_250hz_v3_small.pdf}
  \caption{\footnotesize Comparisons to \cite{tewari2018self}. (a) and (c) Results from CEST. (b) and (d) Results from \cite{tewari2018self}.}\label{fig:250hz}
  \vspace{-0.1in}
\end{figure}

Overall, CEST produces more stable and reasonable geometries, detailed reflectances, and realistic  reconstructions of the 3D faces. As shown in Fig. \ref{fig:250hz} (a) (b), Fig. \ref{fig:mofa4}, Fig. \ref{fig:fml2}, and Fig. \ref{fig:fml3}, the facial shapes predicted by CEST are more accurate in facial expressions and lip closure. In addition, the predicted reflectances show more personal characteristics, but less remaining illumination, as illustrated in Fig. \ref{fig:mofa2} and Fig. \ref{fig:fml2}. Lastly, CEST yields faithful 3D reconstructions, capturing more details than the other methods (see Fig \ref{fig:mofa3} and Fig \ref{fig:mofa4}).


\begin{figure}[ht]
  \centering
   \setlength{\abovecaptionskip}{2pt}
   \setlength{\belowcaptionskip}{-5pt}
  \renewcommand{\captionlabelfont}{\footnotesize}
  \includegraphics[width=4.35in]{figures/compared_to_mofa2_v4_small.pdf}
  \caption{\footnotesize Comparisons to MoFA \cite{tewari2017mofa} and \cite{tuan2017regressing}.}\label{fig:mofa2}
  \vspace{-0.1in}
\end{figure}
\begin{figure}[ht]
  \centering
   \setlength{\abovecaptionskip}{2pt}
   \setlength{\belowcaptionskip}{-5pt}
  \renewcommand{\captionlabelfont}{\footnotesize}
  \includegraphics[width=4.3in]{figures/compared_to_mofa3_v4_small.pdf}
  \caption{\footnotesize Comparisons to MoFA \cite{tewari2017mofa} and \cite{thies2016face2face}.}\label{fig:mofa3}
  \vspace{-0.1in}
\end{figure}
\begin{figure}[ht]
  \centering
   \setlength{\abovecaptionskip}{2pt}
   \setlength{\belowcaptionskip}{-5pt}
  \renewcommand{\captionlabelfont}{\footnotesize}
  \includegraphics[width=4.3in]{figures/compared_to_mofa4_v4_small.pdf}
  \caption{\footnotesize Comparisons to MoFA \cite{tewari2017mofa} and \cite{richardson2017learning}. Our estimated shapes show more accurate expressions.}\label{fig:mofa4}
  \vspace{-0.1in}
\end{figure}
\begin{figure}[ht]
  \centering
   \setlength{\abovecaptionskip}{2pt}
   \setlength{\belowcaptionskip}{-5pt}
  \renewcommand{\captionlabelfont}{\footnotesize}
  \includegraphics[width=4.3in]{figures/compared_to_fml2_v4_small.pdf}
  \caption{\footnotesize We compare CEST to FML \cite{tewari2019fml} and \cite{booth20173d}.}\label{fig:fml2}
\end{figure}
\begin{figure}[ht]
  \centering
\renewcommand{\captionlabelfont}{\footnotesize}
  \includegraphics[width=5.5in]{figures/compared_to_fml3_v4_small.pdf}
  \caption{\footnotesize We compare the estimated shapes from CEST to those from \cite{richardson2017learning}, \cite{sela2017unrestricted}, \cite{tewari2017mofa}, \cite{tewari2019fml}, \cite{sanyal2019learning}, and \cite{shang2020self} (from left to right). Our estimated shapes are more stable and accurate.}\label{fig:fml3}
\end{figure}


\subsection{Challenging Cases}
We present some examples with dark skin in Fig. \ref{fig:hard_examples}. Although most people in the training set (VoxCeleb) are Caucasian, CEST still produces reasonable illumination and albedo for these examples. One limitation is that the reconstruction of the non-lambertian surface is inaccurate, e.g. eyes with unusual gaze directions.
\begin{figure}[ht]
  \centering
\renewcommand{\captionlabelfont}{\footnotesize}
  \includegraphics[width=6.5in]{figures/challenging_case_v2_small.pdf}
  \caption{\footnotesize Some challenging examples.}\label{fig:hard_examples}
\end{figure}


\subsection{Photometric Error}
\label{morequantitative}

We compare CEST, IEST, FML~\cite{tewari2019fml} and Garrido~\cite{garrido2016corrective} on overlay face reconstruction. To measure the quality of the overlay images, we compute the average photometric error (R,G,B pixel values are from 0 to 255) between the input face image and the overlay face image. We experiment on 1,000 images in CelebA dataset~\cite{liu2015deep}. Table \ref{overlay} shows that the conditional estimation is beneficial for reconstructing the 3D face, and the proposed CEST outperforms existing methods by a large margin.


\begin{table}[h]
\renewcommand{\captionlabelfont}{\footnotesize}
\setlength{\abovecaptionskip}{2pt}
\footnotesize
\centering
\begin{tabular}{l|c|c|c|c}
\hline
Method & \textbf{CEST} & IEST & FML \cite{tewari2019fml} & Garrido16 \cite{garrido2016corrective} \\
\hline
Photometric Error & \textbf{10.74} & 13.76 & 20.65 & 21.95 \\
\hline
\end{tabular}
\caption{\footnotesize Photometric errors obtained by different methods.}\label{overlay}
\end{table}

\end{appendix}