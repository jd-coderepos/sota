\section{experiment}
\begin{figure*}[ht]
  \centering
  \includegraphics[width=\textwidth]{fig/visualcomp2.pdf}
  \caption{Visual comparisons of recent point cloud completion methods~\cite{xie2020grnet,yu2021pointr,zhang2021view,zhou2022seedformer} on ShapeNet-ViPC~\cite{zhang2021view}. CSDN produces the most complete and detailed structures compared to its competitors.}
  \label{fig:visualcomp}
\end{figure*} 
\begin{figure*}[h]
  \centering
  \includegraphics[width=\textwidth]{fig/visnovel.pdf}
  \caption{Visual comparisons of recent point cloud completion methods~\cite{xie2020grnet,yu2021pointr,zhang2021view,wang2022pointattn} on unseen categories of ShapeNet-ViPC~\cite{zhang2021view}. CSDN produces the most complete and detailed structures compared to its competitors.}
  \label{fig:visnovel}
\end{figure*}  
\subsection{Datasets}
\subsubsection{Training Dataset}
We use the benchmark dataset ShapeNet-ViPC \cite{zhang2021view}, which is derived from ShapeNet \cite{chang2015shapenet}. 
It contains 13 categories and 38,328 objects, including airplane, bench, cabinet, car, chair, monitor, lamp, speaker, firearm, sofa, table, cellphone, and watercraft. The complete ground-truth point cloud is generated by uniformly sampling 2048 points on the mesh surface from ShapeNet \cite{chang2015shapenet}. Each object has a complete point cloud, 24 rendered images under 24 viewpoints, and 24 corresponding incomplete point clouds. The incomplete point cloud is generated from the corresponding viewpoint (with occlusion) and also contains 2048 points. The image data is rendered from 24 viewpoints as ShapeNetRendering in 3D-R2N2 \cite{choy20163d}. During training, the image viewpoint is randomly chosen for each training data pair and the point cloud will be aligned with the chosen image. For a fair comparison, we use the same training setting as in ViPC \cite{zhang2021view}, i.e., 80\% of the eight categories for training.

\subsubsection{Test Dataset}
The test dataset from ShapeNet-ViPC \cite{zhang2021view} consists of two parts: one contains the remaining 20\% of the 8 categories of objects for training; another one contains 4 categories that are not used during training. For each of the new categories, we randomly choose 200 pairs of point clouds and images. For a fair comparison, we follow the same setting as \cite{zhang2021view}, where images are randomly selected from the 24 viewpoints. Notably, the ShapeNet-ViPC dataset produces missing shapes by various kinds of occlusions (not limited to self-occlusion). Therefore, images from different viewpoints do not necessarily depict the missing parts. We test our approach when images and partial point clouds are produced under the same viewpoints. The results are reported in Section~\ref{viewEXP}.

Besides, we test some methods on partial point clouds from real-world LiDAR scans, the KITTI dataset~\cite{geiger2013vision} and RGB-D scans, the Scannet dataset~\cite{dai2017scannet}. Following the setting of \cite{yuan2018pcn}, we extract point clouds within the corresponding bounding boxes and then select point clouds having more than 2048 points.
\subsection{Implementation Details}
\label{detail}
In our implementation, the number of point surfaces in Shape Fusion is $k=4$ and each surface contains 512 points. The feature dimension is set to 1024, i.e., C = 1024. Thus, the generated coarse point cloud has 2048 points. For IPAdaIN, we set three layers for every surface. In the Local Refinement unit, the number of nearest neighbors is 16. In the Global Constraint unit, the size of feature maps used for projection are $56\times56$, $28\times28$, $14\times14$, and $7\times7$.

For the 3D encoder, we set a five-layer MLP. The feature dimensions are 64, 64, 64, 128, and 1024, respectively. For the 2D encoder, its architecture is shown in Figure \ref{fig:cnn}. 
Note that we reuse the last four feature maps in the global constraint unit and their dimensions are $56\times56$, $28\times28$, $14\times14$, and $7\times7$, respectively.
In the local refinement unit, the feature \emph{$F_{P}^{off}$} is produced by a three-layer MLP whose output dimensions are 32, 128, and 512, respectively. Res-MLP in the global constraint unit is implemented by a Con1d ResNet block \cite{he2016deep}. The final offset regression is implemented by four Conv1d layers whose output dimensions are 256, 128, 32, and 3, respectively. The coordinate offset is calculated through the \emph{tanh} activation function.

For quantitative evaluation, we use both the Chamfer Distance (CD) and F-Score as evaluation metrics. The whole network is trained end-to-end with a learning rate of $5 \times 10^{-5}$ for 50 epochs and the Adam optimizer. The learning rate also decayed by 0.1 for every 10 epochs. All the networks are implemented using PyTorch and trained on an NVIDIA RTX 3090 GPU.

\subsection{Result on ShapeNet-ViPC}
\label{subsec:comparison}
We compare our CSDN with ten state-of-the-art point cloud completion methods. They are Point-based, i.e., AtlasNet \cite{groueix2018papier}, PCN \cite{yuan2018pcn}, MSN \cite{liu2020morphing}, Transformer-based, i.e., PoinTr \cite{yu2021pointr}, Seedformer~\cite{zhou2022seedformer}, SDT~\cite{zhang2022point}, Folding-based, i.e., FoldingNet \cite{yang2018foldingnet}, TopNet \cite{tchapmi2019topnet}, GAN-based, i.e., PF-Net \cite{huang2020pf},  Convolution-based, i.e., GRNet \cite{xie2020grnet}, and Cross-modality-based, i.e., ViPC \cite{zhang2021view}. For AtlasNet, FoldingNet, PCN, and TopNet, we directly take the quantitative statistics from \cite{zhang2021view}. For the other methods, we retrain their models using the released codes on ShapeNet-ViPC. 
For a fair comparison, we uniformly downsample the results of PoinTr to 2048 points, the same as the output of the other methods. Among these methods, ViPC also exploits an additional image, while the others take only a partial point cloud as input.

\subsubsection{Qualitative Results on Known Categories}
Figure \ref{fig:visualcomp} shows the point clouds completed by the proposed CSDN and its competitors. 
For some cases of symmetric structures and small missing regions (e.g., plane and car), most methods behave well and generate complete shapes similar to the ground truths. Our CSDN recovers more clear overall shapes and more detailed structures than Seedformer, PoinTr, and ViPC, while GR-Net generates noisy points. For those challenging cases of complex shapes and large missing regions, like the table case, our CSDN generates the complete desktop and leg while the other methods only predict a coarse result. Compared to ViPC, 
our CSDN is better to recover latent geometric structures (e.g., holes in the desktop and the back of the chair).


\subsubsection{Quantitative Results on Known Categories}
We calculate both CD and F-Score on the 2,048 points of each object, as reported in Table \ref{tab:tab1} and Table \ref{tab:tab2}. It can be observed that our CSDN achieves almost the best results among all the competitors. It is worth noting that CSDN reduces the average CD value by 0.281 compared to the result of the second-ranked PoinTr \cite{yu2021pointr}. Also, CSDN outperforms ViPC \cite{zhang2021view} by a large margin of 22.3\%. Meanwhile, CSDN achieves the best results in all categories in terms of F-Score. 
    
\subsubsection{Results on Novel Categories}
We test different methods on four novel categories which are not used for training, to evaluate the category-agnostic ability. The quantitative results are reported in Table~\ref{tab:tab3} and Table~\ref{tab:tab4}. PoinTr~\cite{yu2021pointr} and PointAttN~\cite{wang2022pointattn} achieve competitive results similar to their results on known categories. ViPC~\cite{zhang2021view} struggles to generalize to new shapes, while our CSDN generalizes well to novel shapes that have not been seen during training.

We also present a visual comparison with other methods, including two Transformer-based methods PoinTr~\cite{yu2021pointr} and PointAttN~\cite{wang2022pointattn}, one Convolution-based method GR-Net~\cite{xie2020grnet}, and one Cross-modality-based method ViPC~\cite{zhang2021view}, as shown in Figure~\ref{fig:visnovel}. We can observe that the two Transformer-based methods cannot well recover the missing shapes for previously unseen categories, although they achieve competitive performance on quantitative results. Under the guidance of images, CSDN and ViPC both succeed in inferring the missing parts, and CSDN produces better details and fewer noisy points. This comparison further demonstrates that our method successfully exploits the complementary information provided by the images.

\begin{table}
\tiny
    \caption{Quantitative results on the Novel categories of ShapNet-ViPC using CD with 2,048 points. The best is highlighted in bold.}
    \renewcommand\arraystretch{1.2}
        \centering
        \label{tab:tab3}
        \footnotesize
        \normalsize
        \begin{tabular}{c|c|c|c|c|c}
        \hline
        \multirow{2}{*}{Methods}& 
        \multicolumn{5}{c}{Mean Chamfer Distance per point $\times 10^{-3}$} \cr\cline{2-6}
        & Avg & Bench & Monitor & Speaker & Phone \cr
        \hline
        \hline
                  PF-Net \cite{huang2020pf} & 5.011 & 3.684 & 5.304 & 7.663 & 3.392 \\
                  \hline
                  MSN \cite{liu2020morphing} & 4.684 & 2.613 & 4.818 & 8.259 & 3.047 \\
                  \hline
                  GRNet \cite{xie2020grnet} & 4.096 & 2.367 & 4.102 & 6.493 & 3.422 \\
                  \hline
                  PoinTr \cite{yu2021pointr} & 3.755 & 1.976 & 4.084 & 5.913 & 3.049 \\
                  \hline
                  ViPC \cite{zhang2021view} & 4.601 & 3.091 & 4.419 & 7.674 & 3.219 \\
                  \hline
                  PointAttN \cite{wang2022pointattn} & 3.674 & 2.135 & \textbf{3.741} & 5.973 & \textbf{2.848} \\
                  \hline
                  SDT \cite{zhang2022point} & 6.001 & 4.096 & 6.222 & 9.499 & 4.189 \\
                  \hline
                  Ours & \textbf{3.656} & \textbf{1.834} & 4.115 & \textbf{5.690} & 2.985 \\
                  \hline
        \hline
        \end{tabular}
    \end{table}
\begin{table}
\tiny
    \renewcommand\arraystretch{1.2}
        \centering
        \caption{Quantitative results on the Novel categories of ShapNet-ViPC using F-Score with 2,048 points. The best is highlighted in bold.}
        \label{tab:tab4}
        \footnotesize
        \normalsize
        \begin{tabular}{c|c|c|c|c|c}
        \hline
        \multirow{2}{*}{Methods}& 
        \multicolumn{5}{c}{F-Score@0.001} \cr\cline{2-6}
        & Avg & Bench & Monitor & Speaker & Phone \cr
        \hline
        \hline
                  PF-Net \cite{huang2020pf} & 0.468 & 0.584 & 0.433 & 0.319 & 0.534 \\
                  \hline
                  MSN \cite{liu2020morphing} & 0.533 & 0.706 & 0.527 & 0.291 & 0.607 \\
                  \hline
                  GRNet \cite{liu2020morphing} & 0.548 & 0.711 & 0.537 & 0.376 & 0.569 \\
                  \hline
                  PoinTr \cite{yu2021pointr} & 0.619 & 0.797 & \textbf{0.599} & 0.454 & 0.627 \\
                  \hline
                  ViPC \cite{zhang2021view} & 0.498 & 0.654 & 0.491 & 0.313 & 0.535 \\
                  \hline
                  PointAttN \cite{wang2022pointattn} & 0.605 & 0.764 & 0.591 & 0.428 & 0.637 \\
                  \hline
                  SDT \cite{zhang2022point} & 0.327 & 0.479 & 0.268 & 0.197 & 0.362 \\
                  \hline
                  Ours & \textbf{0.631} & \textbf{0.798} & 0.598 & \textbf{0.485} & \textbf{0.644} \\
                  \hline
        \hline
        \end{tabular}
\end{table}

\subsection{Evaluation on Real-world Scans}
We train our CSDN and two other methods on the car category of ShapeNet-ViPC to evaluate their performance on real-world scans. 
Ideally, a point cloud completion network trained on a cross-modal dataset should produce satisfactory results, since the cars and their corresponding real-world images cropped with 2D bounding boxes are provided. As reported in \cite{zhang2021view}, the real-to-synthetic domain gap led by rendered images in the training dataset makes ViPC not applicable to real-world point cloud completion. Thus we replace the input image with the rendered car image in ShapeNet-ViPC for ViPC~\cite{zhang2021view} and our method.  We also evaluate our method on the chairs and tables from the Scannet dataset~\cite{dai2017scannet}. Since the objects in Scannet are usually more complete and similar to those in the training datasets, we directly use the trained model in Section~\ref{subsec:comparison} without any fine-tuning. Figure~\ref{fig:kitti} shows that ViPC still cannot handle the domain gap problem and PoinTr generates coarse results due to the low resolution of the training dataset. In Figure~\ref{fig:scannet}, PoinTr fails to complete the missing parts, and ViPC cannot preserve the original inputs. In contrast, CSDN is capable of generating more plausible results on real-world scans.



\subsection{Ablation Study}
We first remove and change the main components  ablate CSDN. The ablation variants can be classified as ablation on Shape Fusion and Dual Refinement. Then, we ablate on the input modality and analyze the contribution of each modality.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{fig/kitti.pdf}
  \caption{Visual comparisons with two point cloud completion methods~\cite{zhang2021view,yu2021pointr} on the real-scanned point clouds (from KITTI~\cite{geiger2013vision}). CSDN produces the most complete and detailed structures.}
  \label{fig:kitti}
\end{figure}  
\begin{figure}[ht]
  \centering
  \includegraphics[width=\linewidth]{fig/scannet.pdf}
  \caption{Visual comparisons with two point cloud completion methods~\cite{zhang2021view,yu2021pointr} on the real-scanned point clouds from Scannet~\cite{dai2017scannet}. CSDN produces the most complete and detailed structures.}
  \label{fig:scannet}
\end{figure} 
\begin{figure*}[ht] 
  \centering
  \includegraphics[width=\textwidth]{fig/vis112.pdf}
  \caption{Visual comparisons between CSDN and its variants when the input images cannot depict the missing parts.}
  \label{fig:vis112}
\end{figure*}
\begin{figure*}[ht] 
  \centering
  \includegraphics[width=\textwidth]{fig/visualabla.pdf}
  \caption{Visual comparisons between CSDN and its variants.}
  \label{fig:visualabla}
\end{figure*}

\subsubsection{Ablation on Shape Fusion}
To analyze the contribution of IPAdaIN, the comparison between three variants on the Shape Fusion module is shown in Table~\ref{tab:ablationSF}. We replace IPAdaIN in the variant A with a regular instance normalization. Without the global information provided by images, A cannot guarantee the completeness of the final shape and typically generates point clouds with higher CD and lower F-Score. The variant B, switching the positions of \emph{$F_P$} and \emph{$F_I$}, also produces points with higher CD and lower F-Score. In the variant C, we only utilize the image feature for the folding operation. Reconstructing shapes directly from images like \cite{zhang2021view} leads to more performance degradation. It further reveals the superiority of IPAdaIN over the single-view reconstruction method in terms of feature fusion. Moreover, from the similar performance of A and C, as well as the coarse result of CSDN, we can conclude that IPAdaIN plays the core role in the proposed cross-modal fusion strategy.

\subsubsection{Ablation on Dual-Refinement}
Table~\ref{tab:ablationDR} shows the comparison between the variants of CSDN on Dual-Refinement. The variant D, without the Local Refinement unit, leads to a large performance drop and coarser results. The variant E, without the Global Constraint unit, typically has lower performance. It means both the point cloud and image contribute to the refinement step, but the point cloud plays a major role. To understand the contribution of the double-refinement and dual-branch design, in the variant F we replace the parallel structure of dual-refinement with a serial structure (i.e., $P_0$ is sent to Local Refinement for the first refinement, and the refined result is sent to Global Constraint for final results). 
In contrast to the Dual-Refinement module,  F only uses information from both modalities for separate refinement without exploiting complementary information from each modality. The performance drop of F demonstrates the superiority of the proposed two-branch architecture.
\begin{table}
\tiny
    \renewcommand\arraystretch{1.3}
        \centering
        \caption{Comparisons between CSDN and its variants on Shape Fusion.}
        \label{tab:ablationSF}
        \small
        \begin{tabular}{c|c|c|c|c}
        \hline
        \multirow{2}{*}{Methods}& \multicolumn{2}{c}{Known} & \multicolumn{2}{c}{Novel} \cr\cline{2-5} & CD $\times 10^{-3}$ & F-Score & CD $\times 10^{-3}$ & F-Score \cr
        \hline
        \hline
                  w/o IPAdaIN (A) & 3.658 & 0.648 & 4.528 & 0.602 \\
                  \hline
                  SW (B) & 2.647 & 0.681 & 3.765 & 0.625 \\
                  \hline
                  ImgFolding (C) & 4.174 & 0.631 & 5.142 & 0.584 \\
                  \hline
                  Coarse & 3.752 & 0.632 & 6.757 & 0.548 \\
                  \hline
                  Ours (CSDN) & \textbf{2.570} & \textbf{0.695} & \textbf{3.656} & \textbf{0.631} \\
                  \hline
        \hline
        \end{tabular}
\end{table}
\begin{table}
    \renewcommand\arraystretch{1.3}
        \centering
        \caption{Comparisons between CSDN and its variants on Dual-Refinement.}
        \label{tab:ablationDR}
        \small
        \begin{tabular}{c|c|c|c|c}
        \hline
        \multirow{2}{*}{Methods}& \multicolumn{2}{c}{Known} & \multicolumn{2}{c}{Novel} \cr\cline{2-5} & CD $\times 10^{-3}$ & F-Score & CD $\times 10^{-3}$ & F-Score \cr
        \hline
        \hline
                  w/o LG (D) & 3.428 & 0.648 & 6.284 & 0.561 \\
                  \hline
                  w/o GC (E) & 2.700 & 0.687 & 4.015 & 0.624 \\
                  \hline
                  serial (F) & 3.036 & 0.659 & 4.227 & 0.611 \\
                  \hline
                  Coarse & 3.752 & 0.632 & 6.757 & 0.548 \\
                  \hline
                  Ours (CSDN) & \textbf{2.570} & \textbf{0.695} & \textbf{3.656} & \textbf{0.631} \\
                  \hline
        \hline
        \end{tabular}
\end{table}
\subsubsection{Ablation of Input Modality}
This set of ablative experiments aims at exploring the role of point clouds and images in our method. First, we change our network into single-modal versions and analyze the contribution of different input modalities. Specifically, we ablate on the single-view images in the variant G, where both IPAdaIN and Global Constraint are removed. The variant H is a single-view reconstruction network based on CSDN. We replace the shape fusion module with a vanilla foldingnet that uses only image features to reconstruct a coarse point cloud and remove the Local Refinement unit in the dual-refinement module. Then, we analyze their performance from two aspects: (1) missing geometry completion, and (2) original structure preservation.

\textbf{Missing Geometry Completion}
Table \ref{tab:ablationM} shows that the variants G and H have varying degrees of performance loss. While G still has competitive results on both known and novel categories, H has a large-margin performance drop (156\% on known categories and 231\% on novel categories).
Besides, these results further demonstrate that our method is an image-guided point cloud completion network, where point clouds play the major role.

We demonstrate some cases when images cannot depict the missing parts in point clouds and visually compare the completion results of CSDN and its variants G and H in Figure~\ref{fig:vis112}. More results under such a condition can be found in Figure~\ref{fig:morevis}. 
We can see that G struggles to infer the missing parts. Meanwhile, the single-view reconstruction H can only reconstruct coarse shapes with a considerable number of noisy points. Both the variants cannot recover the structures that are unseen in images (e.g., the wheels of the car, the upper part of the chair legs and the table legs). 
In contrast, CSDN can generate smoother detailed structures that are missing in both the input point clouds and images, rather than just reconstructing geometric details from images. We credit such superiority to our feature fusion strategy.

In Figure~\ref{fig:visualabla}, we show one more visual comparison between the final and coarse results of CSDN and its variant G. It can be observed that without the input images our method struggles to infer the missing regions and the coarse point clouds generated by Shape Fusion are already reasonably complete compared to the corresponding partial inputs.

\textbf{Original Structure Preservation}
To evaluate how our network preserves the initial structure of the objects, we also compare partial matching~\cite{wen2021cycle4completion} value between $P_{in}$ and the generated result $P_{out}$, which is single-side Chamfer Distance defined as
\begin{equation}
    \mathcal{PM}(P_{in},P_{out})=\frac{1}{\lvert P_{in} \rvert}\sum_{x \in P_{in}}\min_{y \in P_{out}} \lvert \lvert x-y \rvert \rvert.
\end{equation}
Table~\ref{tab:ablationPM} shows that the variant H struggles to preserve the partial shape since it uses only input images. 
However, our cross-modal network still has a better shape-preserving ability than the variant G that inputs only the partial point cloud. The comparison demonstrates that our method fuses cross-modal features to assist completion without losing the original shape information contained in point clouds. In other words, our method can efficiently extract and fuse the complementary information from the two modalities, rather than just reconstructing shapes from images. 
It is noteworthy that CSDN preserves initial points better than ViPC~\cite{zhang2021view}, which could also be concluded from Figures~\ref{fig:visualcomp} and~\ref{fig:morevis}.

It is also worth noting that CSDN can preserve slender structures (e.g., the rear wing of the car) better than the two variants using a single modality input in Figure~\ref{fig:vis112}. To better reveal its capability in structure preservation, thin cross-sections of some models are demonstrated in Figure~\ref{fig:cut}. We can see that CSDN preserves the original surfaces well while the other two competitors~\cite{zhang2021view,yu2021pointr} generate more noisy points inside the object.

\begin{table}
\tiny
    \renewcommand\arraystretch{1.3}
        \centering
        \caption{Comparisons between CSDN and its variants on the input modality.}
        \label{tab:ablationM}
        \small
        \begin{tabular}{c|c|c|c|c}
        \hline
        \multirow{2}{*}{Methods}& \multicolumn{2}{c}{Known} & \multicolumn{2}{c}{Novel} \cr\cline{2-5} & CD $\times 10^{-3}$ & F-Score & CD $\times 10^{-3}$ & F-Score \cr
        \hline
        \hline
                  w/o Image (G) & 4.179 & 0.629 & 4.974 & 0.588 \\
                  \hline
                  w/o Point (H) & 6.583 & 0.411 & 12.103 & 0.268 \\
                  \hline
                  Coarse & 3.752 & 0.632 & 6.757 & 0.548 \\
                  \hline
                  Ours (CSDN) & \textbf{2.570} & \textbf{0.695} & \textbf{3.656} & \textbf{0.631} \\
                  \hline
        \hline
        \end{tabular}
\end{table}

\begin{table}
\tiny
    \renewcommand\arraystretch{1.2}
        \centering
        \caption{Comparisons of Partial Matching value. Lower value means a better preserving ability. G and H refer to the variants in Table~\ref{tab:ablationM}.}
        \label{tab:ablationPM}
        \small
        \begin{tabular}{c|c|c|c|c|c}
        \hline
        Methods & Ours & G & H  & ViPC~\cite{zhang2021view} & PointAttN \cite{wang2022pointattn}\\
        \hline
        PM $\times 10^{-3}$ & 0.277 & 0.358 & 3.038 & 0.854 & 0.424 \\
        \hline
        \hline
        \end{tabular}
\end{table}
\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{fig/cut.pdf}
  \caption{Visual comparisons of CSDN with \cite{zhang2021view,yu2021pointr} on structure preservation. Cross-sections of the completed point clouds are visualized. For each model, the 3D red rectangle in the first row indicates the position and orientation of the cross-section.}
  \label{fig:cut}
\end{figure} 
\begin{figure*}[h] 
  \centering
  \includegraphics[width=\textwidth]{fig/viewEXP.pdf}
  \caption{Images captured from different views often lead to slightly different completion results. A ``good" view that contains more complementary information for the input partial point clouds tends to produce the completion results with a lower CD value ($\times10^{-3}$).}
  \label{fig:viewEXP}
\end{figure*}
\begin{figure*}[ht]
  \centering
  \includegraphics[width=\textwidth]{fig/similar.pdf}
  \caption{Results of using similar but intrinsically different images to bind the partial point cloud for completion. The object's structures and colors affect the completion results. The partial point cloud with its original image can infer a completion result with the smallest error of CD ($\times10^{-3}$).}
  \label{fig:similar}
\end{figure*}
\begin{figure*}[!t] 
  \centering
  \includegraphics[width=\textwidth]{fig/failure.pdf}
  \caption{Failure cases. Similar to its competitors~\cite{zhang2021view,yu2021pointr,zhou2022seedformer}, our CSDN may generate poor completion results (the small structures cannot be reconstructed) when the partial point cloud lacks the main body and the assembled image has very low resolution. Please note that in such challenging cases, our CSDN can still recover the overall shape, which its competitors are not.}
  \label{fig:failure}
\end{figure*}
\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{fig/fea_eu_vis.pdf}
  \caption{Visualization of Euclidean distances between the target points (in blue) and other points in the feature space. For each set, Left: Input partial point cloud; Middle: Distance before IPAdaIN; Right: Distance after IPAdaIN. The target points are randomly selected in missing parts.}
  \label{fig:feaeuvis}
\end{figure} 
\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{fig/fea_vis.pdf}
  \caption{Visualization of feature distributions by t-SNE: $F_{before}$ (in blue), $F_{after}$ (in red) and $P_{coarse}$ (in yellow).}
  \label{fig:feavis}
\end{figure} 

\subsubsection{Ablation on Numbers of KNN}
We also conduct an ablation experiment on the number of nearest neighbors in the local refinement unit. $k$ is set to be 4, 8, 16, and 24, respectively. It is observed from Table \ref{tab:tab1} that CSDN has the best performance when $k=16$. 
However, we believe that this parameter needs to be fine-tuned for different datasets.
\begin{table}
\tiny
    \renewcommand\arraystretch{1.2}
        \centering
        \caption{Mean Chamfer distance with different point cloud resolutions. The first row denotes the number of input points.}
        \label{tab:resolution}
        \normalsize
        \begin{tabular}{c|c|c|c}
        \hline
        \multirow{2}{*}{Methods}& \multicolumn{3}{c}{Number of Input Points} \cr\cline{2-4} & 2048 & 1024 & 256 \cr
        \hline
        \hline
        Ours & 2.570 & 2.597 & 3.358  \\
        \hline
        PoinTr~\cite{yu2021pointr} & 2.851 & 2.882 & 4.255 \\
        \hline
        ViPC~\cite{zhang2021view} & 3.308 & 3.341 & 3.687 \\
        \hline
        \hline
        \end{tabular}
\end{table}
\begin{table}[ht]
    \tiny
    \renewcommand\arraystretch{1.2}
    \centering
    \caption{Quantitative comparisons with different KNN numbers.}
    \normalsize
    \begin{tabular}{c|c|c|c|c}
    \hline
    \multirow{2}{*}{Methods}& \multicolumn{2}{c}{Known} & \multicolumn{2}{c}{Novel} \cr\cline{2-5} & CD $\times 10^{-3}$ & F-Score & CD $\times 10^{-3}$ & F-Score \cr
    \hline
    \hline
              K = 4 & 2.643 & 0.691 & 4.061 & 0.625 \\
              \hline
              K = 8 & 2.636 & 0.688 & 4.053 & 0.622 \\
              \hline
              K = 16 & \textbf{2.570} & \textbf{0.695} & \textbf{3.656} & \textbf{0.631} \\
              \hline
              K = 24 & 2.656 & 0.685 & 4.116 & 0.624 \\
              \hline
    \hline
    \end{tabular}
    \label{tab:knn}
\end{table}
\begin{table*}
\tiny
    \renewcommand\arraystretch{1.2}
        \centering
        \caption{Standard deviations of CD on different-view images. The lower the value is,  the more view-independent the method is.}
        \label{tab:tab7}
        \normalsize
        \begin{tabular}{c|c|c|c|c|c|c|c|c|c}
        \hline
        {Methods}& 
        \multicolumn{9}{c}{Standard deviations of CD calculated on different images} \cr\cline{2-10}
        & Avg & Airplane & Cabinet & Car & Chair & Lamp & Sofa & Table & Watercraft \cr
        \hline
        \hline
                  ViPC \cite{zhang2021view} & 0.445 & 0.185 & 0.406 & 0.236 & 0.595 & 0.921 & 0.373 & 0.599 & 0.243 \\
                  \hline
                  Ours & \textbf{0.254} & \textbf{0.119} & \textbf{0.272} & \textbf{0.164} & \textbf{0.349} & \textbf{0.277} & \textbf{0.305} & \textbf{0.316} & \textbf{0.184} \\
                  \hline
        \hline
        \end{tabular}
\end{table*}
\begin{table}
    \renewcommand\arraystretch{1.3}
        \centering
        \caption{Results on different experiment settings. ``Different'' means images are randomly selected from 24 viewpoints. ``Same'' means images and partial point clouds are produced from the same viewpoints.}
        \label{tab:viewEXP}
        \small
        \begin{tabular}{c|c|c|c|c}
        \hline
        \multirow{2}{*}{Methods}& \multicolumn{2}{c}{Known} & \multicolumn{2}{c}{Novel} \cr\cline{2-5} & CD $\times 10^{-3}$ & F-Score & CD $\times 10^{-3}$ & F-Score \cr
        \hline
        \hline
                  Different & 2.570 & 0.695 & 3.656 & 0.631 \\
                  \hline
                  Same & 2.637 & 0.689 & 3.778 & 0.624 \\
                  \hline
        \hline
        \end{tabular}
\end{table}
\begin{table}
\tiny
    \renewcommand\arraystretch{1.2}
        \centering
        \caption{Comparisons on model sizes and performance.}
        \label{tab:size}
        \normalsize
        \begin{tabular}{c|c|c}
        \hline
        {Methods}&  Model size (Mib) & Performance (CD) \cr
        \hline
        \hline
                  ViPC \cite{zhang2021view} & 53.79 & 3.308 \\
                  \hline
                  PoinTr \cite{yu2021pointr} & 170.18 & 2.851 \\
                  \hline
                  Ours &  70.89 & 2.570 \\
                  \hline
        \hline
        \end{tabular}
\end{table}
\begin{table*}[h]
\centering
\caption{Summary of the limitations of our CSDN and its competitors.}
\renewcommand\arraystretch{1.2}
\small
\begin{tabularx}{16cm}{c|c}
\hline
Methods  & Limitation     \\ \hline \hline
     AltasNet\cite{groueix2018papier}
     & Rely on repeated reconstruction to represent a 3D shape as the union of several surface elements.
     \\ \hline 
     FoldingNet\cite{yang2018foldingnet}     
     & Overlook the local geometric characteristics.      \\ \hline
     PCN\cite{yuan2018pcn}
     & Only rely on a single global feature to generate points, hence incapable of synthesizing local details.                              \\ \hline 
     TopNet \cite{tchapmi2019topnet}                                         &  Results tend to have noisy points around the generated surface.
     \\ \hline
     PF-Net\cite{huang2020pf}                                                & Fail to recover local details to  certain extent.                   \\ \hline
     MSN\cite{liu2020morphing}
     & Lack of sufficient refinement and fail to generate fine-grained details.         
     \\ \hline  
     GRNet\cite{xie2020grnet}
     & \begin{tabular}[c]{@{}c@{}}It is subject to the resolution of the voxel representation,\\ which often leads to distortion in local complex structures.\end{tabular}
     \\ \hline
     PoinTr\cite{yu2021pointr}
     & The model has relatively more parameters owing to the transformer architecture.    
     \\ \hline
     ViPC\cite{zhang2021view}
     & \begin{tabular}[c]{@{}c@{}}Rely on the 3D reconstruction from a single image as well as an accurate 3D alignment.\\ The surface of the results is thus noisy.\end{tabular}
     \\ \hline
     Seedformer\cite{zhou2022seedformer}
     & The results are not evenly distributed.
     \\ \hline
     Ours 
     & Rely on the dual-domain training dataset. 
     \\ \hline \hline
\end{tabularx}\label{tab:limitations}
\end{table*}

\subsection{Impact of Point Cloud Resolution}
This section studies the stability of CSDN and two other competitors concerning point cloud resolution. 
To this end, we downsample the input point cloud to 1024 and 256 points respectively, and test the performance of the methods. From the results reported in Table~\ref{tab:resolution}, we observe that the performance drop of all three methods is small when the input is downsampled to 1024 points. This is because the point cloud can still convey the geometry of the partial shape. When the input is downsampled to 265 points, the performance of all methods degrades significantly. It is worth noting that the decline of ViPC (11.5\%) is smaller than that of CSDN (30.7\%) and PoinTr (49.2\%), which is due to that ViPC reconstructs points directly from images, while the input points play the major role in our method.

\subsection{Effect of Different-view Images}
\label{viewEXP}
This section studies the stability of CSDN when images can be collected from different views. Ideally, any good method of cross-modal point cloud completion should be view-independent. However, it is observed that images captured from different views may slightly affect the completion results. A ``good‚Äù view is considered to possess complementary information for the input partial point clouds. The images with a good view tend to produce the completion results with a lower CD value.

To validate the aforementioned statement, we randomly select some partial objects and test them with the individual images from all 24 views as input. As observed in Figure \ref{fig:viewEXP}, CSDN performs better on images that contain more information about the missing parts. 
For example, the image containing only the front or the end of a truck generates the results with the highest CD losses. 

To further demonstrate the stability of CSDN over the other cross-modal method, i.e. ViPC~\cite{zhang2021view}, we select 100 objects from each known category to evaluate how CSDN and ViPC perform with the images captured from different viewpoints. 
Specifically, we obtain 24 results for each incomplete object with the help of each image of the 24 different-view images.  We calculate the standard deviations for the 24 CD values to compare the stability of model performance, as shown in Table \ref{tab:tab7}.
The quantitative results clearly show that our CSDN  
is more view-independent than ViPC~\cite{zhang2021view} with a 42.9\% drop in the standard deviation.

In addition, we also test our method when images and the corresponding partial point clouds are acquired under the same viewpoints, with the results shown in Table~\ref{tab:viewEXP}. The negligible performance drop (2.6\% on the CD metric) indicates that CSDN is capable of extracting meaningful geometric information from images even when the images may not depict the missing parts of the objects. More visual comparisons can be found in Figure~\ref{fig:morevis}.

\subsection{Effect of Different Similar-images}
There may exist objects that have similar structures. Hence, we randomly select some models and replace the corresponding input images with similar images in the same category. 
Figure \ref{fig:similar} shows the results of one example. When inputting the pair of the original image and the partial point cloud, CSDN generates the completion result with a CD loss of $0.707 \times 10^{-3}$. 
Then, we replace the original image with seven images of the other models in ShapeNet-ViPC \cite{zhang2021view}. It is found that our method still works with similar images as input but produces results with a higher CD loss. Figure \ref{fig:similar} shows that the model performs better when the input image has similar structures and colors.
However, the last two images have nearly the same shape as the original image, but our method generates results with higher CD losses. We believe that it is caused by the remarkable differences in colors. Please note that we do not use such unpaired data during the training stage.
\subsection{Comparisons about Model Sizes}
We compare and report the model size of ViPC, PoinTr, and our CSDN in Table \ref{tab:size}. 
The comparison indicates that our CSDN has a slightly larger model size than ViPC but a much better performance, while PoinTr has the largest model size owing to its transformer architecture.

