\documentclass[10pt,nohyperref]{sigplanconf}

\toappear{Preprint produced \today\  for {\tt arXiv.org}.  Copyright \copyright\ 2015 Oracle.}




\usepackage{times}
\usepackage{graphicx} \usepackage{subfigure} 

\usepackage{natbib}

\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{algorithmicx}

\usepackage{latexsym}
\usepackage{color}
\usepackage{graphicx}

\multiply\textfloatsep by 1\divide\textfloatsep by 2
\multiply\dbltextfloatsep by 1\divide\dbltextfloatsep by 2


\algblockdefx{Begin}{End}{{\bf begin}}{{\bf end}}
\newcommand*\Let[2]{\State #1  #2}
\newcommand*\Assign[2]{\State #1  #2}
\newcommand*\LocalArray[1]{\State {\bf local array} #1}
\newcommand*\RegisterArray[1]{\State {\bf register array} #1}
\newcommand*\Bind[2]{\State {\bf let} #1  #2}
\newcommand*\PhantomBind[2]{\State{\setbox0\hbox{{\bf let} #1  (}\hbox to \wd0{\hss}#2}}
\newcommand*\Remark[1]{\State  #1}
\newcommand*\Var[1]{{\mathit{#1}}}
\newcommand*\Output{\;\mathbf{output}\;}

\newbox\proctwobox
\algblockdefx{ProcTwo}{EndProcedure}[2]{\global\setbox\proctwobox\hbox{{\bf procedure} {\sc #1}}\copy\proctwobox#2,}{{\bf end}}
\newcommand*\ProcTwoX[1]{\State\hbox{\hskip-\algorithmicindent\hskip\wd\proctwobox#1}}

\algblockdefx{CodeChunk}{End}[1]{{\bf code chunk} {#1}:}{{\bf end}}
\newcommand*\UseCodeChunk[1]{\State {#1}}





\newcommand{\theHalgorithm}{\arabic{algorithm}}






\algrenewcommand\algorithmicindent{0.8em}

\renewcommand\topfraction{0.8}





\begin{document} 


\conferenceinfo{no conference}{no place}
\copyrightyear{2015} 
\copyrightdata{no copyright data}
\doi{no doi}

\title{Using Butterfly-Patterned Partial Sums \\ to Optimize GPU Memory Accesses \\ for Drawing from Discrete Distributions}
\subtitle{}
\authorinfo{Guy L. Steele Jr.}{Oracle Labs}{guy.steele@oracle.com}
\authorinfo{Jean-Baptiste Tristan}{Oracle Labs}{jean.baptiste.tristan@oracle.com}

\maketitle




\begin{abstract} 
We describe a technique for drawing values from discrete distributions,
such as sampling from the random variables of a mixture model,
that avoids computing a complete table of partial
sums of the relative probabilities.  A table of alternate
(``butterfly-patterned'') form is faster to compute, making
better use of coalesced memory accesses.  From this table,
complete partial sums are computed on the fly during a binary search.
Measurements using an {\sc nvidia} Titan Black {\sc gpu} show that for a sufficiently large number
of clusters or topics (), this technique alone more than doubles
the speed of a latent Dirichlet allocation ({\sc LDA}) application already highly tuned for GPU execution.
\end{abstract} 

\keywords{\hskip 0pt plus 0.5em minus 0.5em
butterfly, coalesced memory access, discrete distribution,
GPU, graphics processing unit, latent Dirichlet allocation, LDA,
machine learning, multithreading,
memory bottleneck, parallel computing, random sampling, SIMD,
transposed memory access}



\section{Overview}
\label{sec:overview}

The successful use of Graphics Processing Units ({\sc gpu}s) to train neural
networks is a great example of how machine learning can benefit from
such massively parallel architecture. Generative
probabilistic modeling \cite{Blei}
and associated inference methods (such as Monte Carlo
methods) can also benefit. Indeed,
authors such as Suchard~{\it et~al.}~\cite{suchard_understanding_2010} and
Lee~{\it et~al.}~\cite{lee_utility_journal_2010} have pointed out that many algorithms of
interest are embarrassingly parallel. However, the potential for massively
parallel computation is only the first step toward full
use of {\sc gpu} capacity.  One bottleneck that such embarrassingly parallel
algorithms run into is related to memory bandwidth; one must
design key probabilistic primitives with such constraints in mind.

We address the important case wherein parallel threads must
draw from distinct discrete distributions in a {\sc simd} fashion.
This can arise when implementing any mixture model, and Latent Dirichlet Allocation ({\sc lda}) models in particular,
which are probabilistic mixture models used to discover abstract ``topics'' in a collection of documents (a \emph{corpus}) \cite{blei_latent_2003}. This model can be fitted (or ``trained'') in an unsupervised fashion using sampling methods \cite[chapter 11]{Bishop}\cite{griffiths_finding_2004}. Each document is modeled as a distribution  over topics, and each word in a document is assumed to be drawn from a distribution  of words. Understanding the methods described in this paper does not require a deep understanding of sampling algorithms for {\sc lda}. What is important is that each word in a corpus is associated with a so-called ``latent'' random variable \cite[chapter 9]{Bishop}, usually referred to as , that takes on one of  integer values, indicating a topic to which the word belongs.  Broadly speaking, the iterative training process works by tentatively choosing a topic (that is, sampling the random latent variable ) for a given word using relative probabilities calculated from  and , then updating  and  accordingly.

In this paper, we focus on the step that draws new  values from a finite domain,
given arrays of (typically floating-point) parameters  and ,
typically using the following four-step process for each new  value to be drawn:
\par\noindent{\bf 1.\quad}Use the parameters to construct a table of relative (unnormalized) probabilities for the possible choices for ,
where each relative probability is a product of some element of  and some element of .
\hfill\break{\bf 2.\quad}Normalize these table entries by dividing each entry by the sum of all entries.
\hfill\break{\bf 3.\quad}Let  be chosen uniformly at random from the real interval .
\hfill\break{\bf 4.\quad}Find the smallest index  such that the sum of all table entries at or below index  is larger than .
\par\noindent In practice, this sequence of steps may be optimized
by doing a bit of algebra and using a binary search:
\par\noindent{\bf 1.\quad}Use the parameters to construct a table of relative probabilities for the possible choices for the  value.
\hfill\break{\bf 2.\quad}Replace this table with its sum-prefix: each entry is replaced by the sum of itself and all earlier entries.  The last entry therefore becomes the sum of all the original entries.
\hfill\break{\bf 3.\quad}Let  be chosen uniformly at random from the real interval .
\hfill\break{\bf 4.\quad}Use a binary search to find the smallest index  such that the entry at index  is larger than  times the last entry.

When this algorithm is implemented in parallel on a {\sc simd} {\sc gpu}, an obvious approach is to assign the computation of each  value to a separate thread.
However, when the threads fetch their respective  values, the values to be fetched will likely reside at unrelated locations in memory, resulting in poor memory-fetch performance.  A standard trick is to have all the threads in a warp cooperate with each other (compare, for example, the storage of floating-point numbers as ``slicewise'' rather than ``fieldwise'' in the architecture of the Connection Machine Model CM-2, so that 32 1-bit processors cooperate on each of 32 clock cycles to fetch and store an entire 32-bit floating-point value that logically belongs to just one of the 32 processors~\cite{Johnsson-1989}).  Suppose there are  threads in a warp (a typical value is ), each working on a different document (and therefore using different discrete probability distributions), and suppose that at a certain step of the algorithm each thread needs to fetch  different  values.  The array of  values can be organized so that the  different  values needed by any one thread are stored in consecutive memory locations.  The trick is that each thread performs  fetches from the  table, as before, but instead of each thread fetching its own  values in all cases, on cycle  all the threads work together to fetch the  different  values needed by thread .  As a result, on each memory cycle all  values being fetched are in a contiguous region of memory,
allowing improved memory-fetch performance.

It is then necessary for the threads to exchange information among themselves so that the rest of the algorithm may be carried out, including the summation arithmetic.

The binary search does not access all entries in the prefix-sum table (in fact, for a table of size  it examines only about  entries).  Therefore it is not necessary to compute all entries of the prefix-sum table.  We present
an alternate technique that computes
a ``butterfly-patterned'' partial-sums table, using less computational and communication effort; a modified binary search then uses this table to compute, on the fly, entries that would have been in the original complete prefix-sum table.  This requires more work per table entry during the binary search, but because the search examines only a few table entries, the result is a dramatic net reduction in execution time.  This technique may be effective for collapsed LDA Gibbs samplers~\cite{yan_parallel_2009,lu_accelerating_2013,Augur}
as well as uncollapsed samplers, and may also be useful for GPU implementations of other algorithms \cite{zhao_same_2014}
whose inner loops sample from discrete distributions.

\section{Basic Algorithm}

For a Latent Dirichlet Allocation model of, for example, a set of documents to which we want to assign topics probablistically
using Gibbs sampling, let  be the number of documents,
 be the number of topics, and  be the size of the vocabulary, which is a set of distinct words.
Each document is a bag of words, each of which belongs to the vocabulary;
any given word can appear in any number of documents, and may appear any number of times in any single document.
The documents may be of different lengths.

We are interested in the phase of an uncollapsed Gibbs sampler that draws new  values, given  and  distributions.
Because no  value directly depends on any other  value in this formulation, new  values may all be computed independently
(and therefore in parallel to any extent desired).

We assume that we are given an  matrix  and a  matrix ; the elements of these matrices are
non-negative numbers, typically represented as floating-point values.
Row  of  (that is, ) is the (currently assumed) distribution of topics for document , that is,
the relative probabilities (weights) for each of the  possible topics to which the document might be assigned.
Note that columns of  are \emph{not} to be considered as distributions.
Similarly, column  of  (that is, ) is the (currently assumed) distribution of words for topic ,
that is, the weights with which the  possible words in the vocabulary are associated with the topic.
Note that rows of  are \emph{not} to be considered as distributions.
We organize  as rows and  as columns for engineering reasons:
we want the  entries obtained by ranging over all possible topics to be contiguous in memory
so as to take advantage of memory cache structure.

We also assume that we are given (i)~a length-
vector of nonnegative integers  such that  is the length of document , and (ii)~an  ragged array ,
by which we mean that for ,  is a vector of length .  (We use zero-based indexing throughout this document.)
Each element of  is less than  and may therefore be used as a first index for .


\begin{algorithm}[t]
\caption{Drawing new  values}\label{alg:basicdraw}
\begin{algorithmic}[1]
\ProcTwo{DrawZ}{}\ProcTwoX{}
   \LocalArray{}
   \ForAll{}
      \ForAll{}
         \Remark{Compute - products}
         \ForAll{}   \label{z1prodstart}
            \Assign{}{}
         \EndFor   \label{z1prodend}
         \Remark{Compute partials sums of the products}
         \Bind{}{}  \label{z1sumstart}
         \For{ from  through }
            \Assign{}{}
            \Assign{}{}
         \EndFor \label{z1sumend}
         \Bind{}{}
         \UseCodeChunk{search the table of partial sums}
         \Assign{}{} \label{z1searchend}
     \EndFor
   \EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}


Our goal, given , , , , , , and  and
assuming the use of a temporary  ragged work array  (which we will later optimize away), is to compute
all the elements for an  ragged array  as follows:
For all  such that  and for all  such that , do two things:
first, for all  such that , let ;
second, let  be a nonnegative integer less than , chosen randomly in such a way that the probability of choosing
the value  is  where
.
Thus,  is a relative (unnormalized) probability, and  is an absolute (normalized) probability.


Algorithm~\ref{alg:basicdraw} is a basic implementation of this process.
We remark that a ``{\bf let}'' statement creates a local binding of a scalar (single-valued) variable and gives it a value,
that a ``{\bf local array}'' declaration creates a local binding of an array variable
(containing an element value for each indexable position in the array), and that distinct iterations of a ``{\bf for}'' or ``{\bf for all}'' construct are understood to create
distinct and independent instantiations of such local variables for each iteration.  The iterations of ``{\bf for} \ldots\ from \ldots\ through \ldots''
are executed sequentially in a specific order; but the iterations of a ``{\bf for all}'' construct are intended to be computationally independent
and therefore may be executed in any order, or in parallel, or in any sequential-parallel combination.
We use angle brackets to indicate the use of a ``code chunk'' that is defined as a separate algorithm;
such a use indicates that the definition of the code chunk should be inserted at the use site, as if it were a C macro,
but surrounded by {\bf begin} and {\bf end} (this is a programming-language
technicality that ensures that the scope of any variable declared within the code chunk is confined to that code chunk).


The computation of the - products (lines~\hbox{\ref{z1prodstart}--\ref{z1prodend}} of Algorithm~\ref{alg:basicdraw}) is straightforward.
The computation of partial sums (lines~\hbox{\ref{z1sumstart}--\ref{z1sumend}}) is sequential; the variable 
accumulates the products, and successive values of  are stored into the array .
A random integer is chosen for  by choosing a random value uniformly from the range ,
scaling it by the final value of  (which has the same algorithmic effect as dividing each 
by that value, for all , to turn it into an absolute probability), and then searching the subarray 
to find the smallest entry that is larger than the scaled value (and if there are several such entries, all equal,
then the one with the smallest index is chosen); the index  of that entry is used as the desired
randomly chosen integer.   A simple linear search (Algorithm~\ref{alg:basiclinearsearch}) can do the job.
But because all elements of  and  are nonnegative,
the products in  are also nonnegative, and so each subarray  is monotonically nondecreasing;
that is, for all , , and , we have .
Therefore a binary search (Algorithm~\ref{alg:basicbinarysearch}) can be used instead, which is faster,
on average, for  sufficiently large
\cite[exercise 6.2.1-5]{KNUTH-VOLUME-3}.


\begin{algorithm}[t]
\caption{Simple linear search}\label{alg:basiclinearsearch}
\begin{algorithmic}[1]
\CodeChunk{search the table of partial sums}
         \Bind{}{random value chosen from }
         \Bind{}{}
        \While{ and }
            \Assign{}{}
         \EndWhile
\End
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[t]
\caption{Simple binary search}\label{alg:basicbinarysearch}
\begin{algorithmic}[1]
\CodeChunk{search the table of partial sums}
         \Bind{}{random value chosen from }
         \Bind{}{}
            \Bind{}{}
            \While{}
               \Bind{}{}
               \If{}
                  \Assign{}{}
               \Else
                  \Assign{}{}
               \EndIf
            \EndWhile
\End
\end{algorithmic}
\end{algorithm}

\section{Blocking and Transposition}

Anticipating certain characteristics of the hardware,
we make some commitments as to how the algorithm will be executed.  We assume that
arrays are laid out in row-major order (as they are when using C or {\sc cuda}).
Let  be a
machine-dependent constant (typically 16 or 32, but for now we do not require that  be a power of 2).
For purposes of illustration we assume  and also .
We divide the documents into groups of size  and assume that  is an exact multiple of .
(In an overall application, the set of documents can be padded with empty documents
so as to make  be an exact multiple of  without affecting the overall behavior of the algorithm on the ``real'' documents.)
We split the outermost loop of Algorithm~\ref{alg:basicdraw} (with index variable ) into two nested loops with
index variables  and , from which the equivalent value for  is then computed.
We commit to making the loop with index variable  sequential,
to treating the iterations of the loop on  as independent (and therefore possibly parallel),
and to treating the iterations of the loop on  as executed by a {\sc simd} ``thread warp'' of size , that is,
parallel and implicitly lock-step synchronized.
As a result, we view each of the  documents as being processed by a separate thread.
A benefit of making the loop on  sequential is that the array  can be made two-dimensional and non-ragged,
having size .  We fuse the loop that computes - products
with the loop that computes partial sums; this eliminates the need for the array ,
but instead (for reasons explained below) we retain  as a two-dimensional, non-ragged array of size 
that is used only when .  Within the loop controlling index variable  we declare a local work array 
of size  that will be used to exchange information by the  threads within a warp; our eventual intent
is that this array will reside in {\sc gpu} registers.
We cache values from the array  in a per-thread array  of length ,
anticipating that such cached values will reside in a faster memory and
be used repeatedly by the loop on .


\begin{algorithm}[t]
\caption{Drawing  values (transposed access)}\label{alg:transposedraw}
\begin{algorithmic}[1]
\ProcTwo{DrawZ}{}\ProcTwoX{}
  \LocalArray{}
   \ForAll{}
      \LocalArray{}
      \For{\hbox{\small\bf SIMD} }
         \Bind{}{}
         \LocalArray{}
         \UseCodeChunk{cache  values into }
         \Bind{}{}    \label{z6iloopstart}
          \While{}
            \Bind{}{}
            \UseCodeChunk{compute partial sums of - products}
            \Bind{}{}
            \UseCodeChunk{search the table of partial sums}
            \Assign{}{}
            \Assign{}{}
         \EndWhile     \label{z6iloopend}
   \EndFor
   \EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}


There is, however, a subtle problem with the loop controlling index variable : the upper bound  for the loop
variable may be different for different threads.  As a result, in the last iterations it may be that some threads
have ``gone to sleep'' because they reached their upper loop bound earlier than other threads in the warp.
This is undesirable because, as we shall see, we rely on all threads ``staying awake'' so that they can assist
each other.  Therefore, we rewrite the loop control to use a ``master index'' idiom and exploit the trick of allowing a thread
to perform its last iteration (with ) multiple times, which doesn't work for many algorithms
but is acceptable for {\sc lda} Gibbs.


The result of all these code transformations is Algorithm~\ref{alg:transposedraw},
which makes use of three code chunks: Algorithms~\ref{alg:transposecache}, \ref{alg:transposesum}, and either
Algorithm~\ref{alg:basiclinearsearch} or~\ref{alg:basicbinarysearch}.
Algorithms~\ref{alg:transposecache} and~\ref{alg:transposesum}, besides using {\sc simd} thread warps of size 
to process documents in groups of size , also process topics in blocks of size .  This allows the innermost loops to process ``little'' arrays
of size .  If  (the number of topics) is not a multiple of , then there will be a \emph{remnant} of size .
To make looping code slightly simpler, we put the remnant at the \emph{front} of each array, rather than at the end.
For  and , topics 0, 1, and 2 form the remnant; topics 3--10 form a block of length 8;
and topics 11--18 form a second block.  This organization of arrays into blocks
allows reduction of the cost of accessing data in main memory by performing \emph{transposed accesses}.



\begin{algorithm}[t]
\caption{Caching  values (transposed access)}\label{alg:transposecache}
\begin{algorithmic}[1]
\CodeChunk{cache  values into }
          \Bind{}{}
            \While{}   \label{z6cacheremnantloopstart}
               \Assign{}{}
               \Assign{}{}
            \EndWhile    \label{z6cacheremnantloopend}
            \While{}
               \For{ from  through }   \label{z6cacheblockloopstart}
                  \Remark{Next line uses transposed access to }
                  \Assign{}{}     \label{z6thetaaccess}
               \EndFor    \label{z6cacheblockloopend}
              \Assign{}{}
            \EndWhile
\End
\end{algorithmic}
\end{algorithm}

The simplest use of transposed memory access occurs in Algorithm~\ref{alg:transposecache}.
For every document, this algorithm fetches a  value for every topic.
The topics are regarded as divided into a leading remnant (if any) and then a sequence
of blocks of length .  The {\bf while} loop on lines~\hbox{\ref{z6cacheremnantloopstart}--\ref{z6cacheremnantloopend}}
handles the remnant, and then the following {\bf while} loop processes successive blocks.
On line~\ref{z6thetaaccess} within the inner loop,
note that the reference is to  rather than the expected 
(which would be the same as  because ).  The result is that when
the  threads of a {\sc simd} warp execute this code and all access  simultaneously, they access
 consecutive memory locations, which can typically be fetched by a hardware memory controller
much more efficiently than  memory locations separated by stride .
Another way to think about it is that on any given single iteration of the loop on
lines~\hbox{\ref{z6cacheblockloopstart}--\ref{z6cacheblockloopend}}
(which overall is designed to fetch one  block of  values)
instead of every thread in the warp fetching its th value from the  array,
all the threads work together to fetch all  values that are needed by thread  of the warp.
Each thread then stores what it has fetched into its local copy of the array .
The result is that each thread doesn't really have all the  data it needs to process its
document; pictorially, data in each  block has been \emph{transposed}.
The real story, however, is that memory for local arrays in a {\sc gpu} is not stored in the same way as memory for global arrays.
The global array  is laid out in row-major order in main memory like this:

\noindent
\hbox to \linewidth{\relax
\hsize=0.4\linewidth
\vbox{\unitlength=1 in \divide\unitlength by 16
\thinlines
\def\SQX{\vbox to 0pt{\vss\hbox to 0pt{\hss\vrule width 0.6\unitlength height 0.6\unitlength\hss}\vss}}\noindent
\begin{picture}(19,8)
\color[gray]{0}
\put(0,8){\line(1,0){19}}
\put(0,0){\line(1,0){19}}
\put(19,0){\line(0,1){8}}
\put(11,0){\line(0,1){8}}
\put(3,0){\line(0,1){8}}
\put(0,0){\line(0,1){8}}
\color[gray]{0.00}
\put(0.5,7.5){\SQX}
\put(1.5,7.5){\SQX}
\put(2.5,7.5){\SQX}
\put(3.5,7.5){\SQX}
\put(4.5,7.5){\SQX}
\put(5.5,7.5){\SQX}
\put(6.5,7.5){\SQX}
\put(7.5,7.5){\SQX}
\put(8.5,7.5){\SQX}
\put(9.5,7.5){\SQX}
\put(10.5,7.5){\SQX}
\put(11.5,7.5){\SQX}
\put(12.5,7.5){\SQX}
\put(13.5,7.5){\SQX}
\put(14.5,7.5){\SQX}
\put(15.5,7.5){\SQX}
\put(16.5,7.5){\SQX}
\put(17.5,7.5){\SQX}
\put(18.5,7.5){\SQX}
\color[gray]{0.12}
\put(0.5,6.5){\SQX}
\put(1.5,6.5){\SQX}
\put(2.5,6.5){\SQX}
\put(3.5,6.5){\SQX}
\put(4.5,6.5){\SQX}
\put(5.5,6.5){\SQX}
\put(6.5,6.5){\SQX}
\put(7.5,6.5){\SQX}
\put(8.5,6.5){\SQX}
\put(9.5,6.5){\SQX}
\put(10.5,6.5){\SQX}
\put(11.5,6.5){\SQX}
\put(12.5,6.5){\SQX}
\put(13.5,6.5){\SQX}
\put(14.5,6.5){\SQX}
\put(15.5,6.5){\SQX}
\put(16.5,6.5){\SQX}
\put(17.5,6.5){\SQX}
\put(18.5,6.5){\SQX}
\color[gray]{0.24}
\put(0.5,5.5){\SQX}
\put(1.5,5.5){\SQX}
\put(2.5,5.5){\SQX}
\put(3.5,5.5){\SQX}
\put(4.5,5.5){\SQX}
\put(5.5,5.5){\SQX}
\put(6.5,5.5){\SQX}
\put(7.5,5.5){\SQX}
\put(8.5,5.5){\SQX}
\put(9.5,5.5){\SQX}
\put(10.5,5.5){\SQX}
\put(11.5,5.5){\SQX}
\put(12.5,5.5){\SQX}
\put(13.5,5.5){\SQX}
\put(14.5,5.5){\SQX}
\put(15.5,5.5){\SQX}
\put(16.5,5.5){\SQX}
\put(17.5,5.5){\SQX}
\put(18.5,5.5){\SQX}
\color[gray]{0.36}
\put(0.5,4.5){\SQX}
\put(1.5,4.5){\SQX}
\put(2.5,4.5){\SQX}
\put(3.5,4.5){\SQX}
\put(4.5,4.5){\SQX}
\put(5.5,4.5){\SQX}
\put(6.5,4.5){\SQX}
\put(7.5,4.5){\SQX}
\put(8.5,4.5){\SQX}
\put(9.5,4.5){\SQX}
\put(10.5,4.5){\SQX}
\put(11.5,4.5){\SQX}
\put(12.5,4.5){\SQX}
\put(13.5,4.5){\SQX}
\put(14.5,4.5){\SQX}
\put(15.5,4.5){\SQX}
\put(16.5,4.5){\SQX}
\put(17.5,4.5){\SQX}
\put(18.5,4.5){\SQX}
\color[gray]{0.48}
\put(0.5,3.5){\SQX}
\put(1.5,3.5){\SQX}
\put(2.5,3.5){\SQX}
\put(3.5,3.5){\SQX}
\put(4.5,3.5){\SQX}
\put(5.5,3.5){\SQX}
\put(6.5,3.5){\SQX}
\put(7.5,3.5){\SQX}
\put(8.5,3.5){\SQX}
\put(9.5,3.5){\SQX}
\put(10.5,3.5){\SQX}
\put(11.5,3.5){\SQX}
\put(12.5,3.5){\SQX}
\put(13.5,3.5){\SQX}
\put(14.5,3.5){\SQX}
\put(15.5,3.5){\SQX}
\put(16.5,3.5){\SQX}
\put(17.5,3.5){\SQX}
\put(18.5,3.5){\SQX}
\color[gray]{0.60}
\put(0.5,2.5){\SQX}
\put(1.5,2.5){\SQX}
\put(2.5,2.5){\SQX}
\put(3.5,2.5){\SQX}
\put(4.5,2.5){\SQX}
\put(5.5,2.5){\SQX}
\put(6.5,2.5){\SQX}
\put(7.5,2.5){\SQX}
\put(8.5,2.5){\SQX}
\put(9.5,2.5){\SQX}
\put(10.5,2.5){\SQX}
\put(11.5,2.5){\SQX}
\put(12.5,2.5){\SQX}
\put(13.5,2.5){\SQX}
\put(14.5,2.5){\SQX}
\put(15.5,2.5){\SQX}
\put(16.5,2.5){\SQX}
\put(17.5,2.5){\SQX}
\put(18.5,2.5){\SQX}
\color[gray]{0.72}
\put(0.5,1.5){\SQX}
\put(1.5,1.5){\SQX}
\put(2.5,1.5){\SQX}
\put(3.5,1.5){\SQX}
\put(4.5,1.5){\SQX}
\put(5.5,1.5){\SQX}
\put(6.5,1.5){\SQX}
\put(7.5,1.5){\SQX}
\put(8.5,1.5){\SQX}
\put(9.5,1.5){\SQX}
\put(10.5,1.5){\SQX}
\put(11.5,1.5){\SQX}
\put(12.5,1.5){\SQX}
\put(13.5,1.5){\SQX}
\put(14.5,1.5){\SQX}
\put(15.5,1.5){\SQX}
\put(16.5,1.5){\SQX}
\put(17.5,1.5){\SQX}
\put(18.5,1.5){\SQX}
\color[gray]{0.84}
\put(0.5,0.5){\SQX}
\put(1.5,0.5){\SQX}
\put(2.5,0.5){\SQX}
\put(3.5,0.5){\SQX}
\put(4.5,0.5){\SQX}
\put(5.5,0.5){\SQX}
\put(6.5,0.5){\SQX}
\put(7.5,0.5){\SQX}
\put(8.5,0.5){\SQX}
\put(9.5,0.5){\SQX}
\put(10.5,0.5){\SQX}
\put(11.5,0.5){\SQX}
\put(12.5,0.5){\SQX}
\put(13.5,0.5){\SQX}
\put(14.5,0.5){\SQX}
\put(15.5,0.5){\SQX}
\put(16.5,0.5){\SQX}
\put(17.5,0.5){\SQX}
\put(18.5,0.5){\SQX}
\end{picture}\vskip5pt
}\hfil
{\hsize=0.6\linewidth
\vbox{\noindent\strut
where each row corresponds to a document and each column to a topic; but each of the local arrays is laid out like this:\strut}}}

\unskip\unskip
\hbox to \linewidth{\relax
\hsize=0.2\linewidth
\vbox{\unitlength=1 in \divide\unitlength by 16
\thinlines
\def\SQX{\vbox to 0pt{\vss\hbox to 0pt{\hss\vrule width 0.6\unitlength height 0.6\unitlength\hss}\vss}}\noindent
\begin{picture}(8,19)
\color[gray]{0}
\put(0,0){\line(0,1){19}}
\put(8,0){\line(0,1){19}}
\put(0,0){\line(1,0){8}}
\put(0,8){\line(1,0){8}}
\put(0,16){\line(1,0){8}}
\put(0,19){\line(1,0){8}}
\color[gray]{0.00}
\put(0.5,18.5){\SQX}
\put(0.5,17.5){\SQX}
\put(0.5,16.5){\SQX}
\color[gray]{0.00}\put(0.5,15.5){\SQX}
\color[gray]{0.12}\put(0.5,14.5){\SQX}
\color[gray]{0.24}\put(0.5,13.5){\SQX}
\color[gray]{0.36}\put(0.5,12.5){\SQX}
\color[gray]{0.48}\put(0.5,11.5){\SQX}
\color[gray]{0.60}\put(0.5,10.5){\SQX}
\color[gray]{0.72}\put(0.5,9.5){\SQX}
\color[gray]{0.84}\put(0.5,8.5){\SQX}
\color[gray]{0.00}\put(0.5,7.5){\SQX}
\color[gray]{0.12}\put(0.5,6.5){\SQX}
\color[gray]{0.24}\put(0.5,5.5){\SQX}
\color[gray]{0.36}\put(0.5,4.5){\SQX}
\color[gray]{0.48}\put(0.5,3.5){\SQX}
\color[gray]{0.60}\put(0.5,2.5){\SQX}
\color[gray]{0.72}\put(0.5,1.5){\SQX}
\color[gray]{0.84}\put(0.5,0.5){\SQX}
\color[gray]{0.12}
\put(1.5,18.5){\SQX}
\put(1.5,17.5){\SQX}
\put(1.5,16.5){\SQX}
\color[gray]{0.00}\put(1.5,15.5){\SQX}
\color[gray]{0.12}\put(1.5,14.5){\SQX}
\color[gray]{0.24}\put(1.5,13.5){\SQX}
\color[gray]{0.36}\put(1.5,12.5){\SQX}
\color[gray]{0.48}\put(1.5,11.5){\SQX}
\color[gray]{0.60}\put(1.5,10.5){\SQX}
\color[gray]{0.72}\put(1.5,9.5){\SQX}
\color[gray]{0.84}\put(1.5,8.5){\SQX}
\color[gray]{0.00}\put(1.5,7.5){\SQX}
\color[gray]{0.12}\put(1.5,6.5){\SQX}
\color[gray]{0.24}\put(1.5,5.5){\SQX}
\color[gray]{0.36}\put(1.5,4.5){\SQX}
\color[gray]{0.48}\put(1.5,3.5){\SQX}
\color[gray]{0.60}\put(1.5,2.5){\SQX}
\color[gray]{0.72}\put(1.5,1.5){\SQX}
\color[gray]{0.84}\put(1.5,0.5){\SQX}
\color[gray]{0.24}
\put(2.5,18.5){\SQX}
\put(2.5,17.5){\SQX}
\put(2.5,16.5){\SQX}
\color[gray]{0.00}\put(2.5,15.5){\SQX}
\color[gray]{0.12}\put(2.5,14.5){\SQX}
\color[gray]{0.24}\put(2.5,13.5){\SQX}
\color[gray]{0.36}\put(2.5,12.5){\SQX}
\color[gray]{0.48}\put(2.5,11.5){\SQX}
\color[gray]{0.60}\put(2.5,10.5){\SQX}
\color[gray]{0.72}\put(2.5,9.5){\SQX}
\color[gray]{0.84}\put(2.5,8.5){\SQX}
\color[gray]{0.00}\put(2.5,7.5){\SQX}
\color[gray]{0.12}\put(2.5,6.5){\SQX}
\color[gray]{0.24}\put(2.5,5.5){\SQX}
\color[gray]{0.36}\put(2.5,4.5){\SQX}
\color[gray]{0.48}\put(2.5,3.5){\SQX}
\color[gray]{0.60}\put(2.5,2.5){\SQX}
\color[gray]{0.72}\put(2.5,1.5){\SQX}
\color[gray]{0.84}\put(2.5,0.5){\SQX}
\color[gray]{0.36}
\put(3.5,18.5){\SQX}
\put(3.5,17.5){\SQX}
\put(3.5,16.5){\SQX}
\color[gray]{0.00}\put(3.5,15.5){\SQX}
\color[gray]{0.12}\put(3.5,14.5){\SQX}
\color[gray]{0.24}\put(3.5,13.5){\SQX}
\color[gray]{0.36}\put(3.5,12.5){\SQX}
\color[gray]{0.48}\put(3.5,11.5){\SQX}
\color[gray]{0.60}\put(3.5,10.5){\SQX}
\color[gray]{0.72}\put(3.5,9.5){\SQX}
\color[gray]{0.84}\put(3.5,8.5){\SQX}
\color[gray]{0.00}\put(3.5,7.5){\SQX}
\color[gray]{0.12}\put(3.5,6.5){\SQX}
\color[gray]{0.24}\put(3.5,5.5){\SQX}
\color[gray]{0.36}\put(3.5,4.5){\SQX}
\color[gray]{0.48}\put(3.5,3.5){\SQX}
\color[gray]{0.60}\put(3.5,2.5){\SQX}
\color[gray]{0.72}\put(3.5,1.5){\SQX}
\color[gray]{0.84}\put(3.5,0.5){\SQX}
\color[gray]{0.48}
\put(4.5,18.5){\SQX}
\put(4.5,17.5){\SQX}
\put(4.5,16.5){\SQX}
\color[gray]{0.00}\put(4.5,15.5){\SQX}
\color[gray]{0.12}\put(4.5,14.5){\SQX}
\color[gray]{0.24}\put(4.5,13.5){\SQX}
\color[gray]{0.36}\put(4.5,12.5){\SQX}
\color[gray]{0.48}\put(4.5,11.5){\SQX}
\color[gray]{0.60}\put(4.5,10.5){\SQX}
\color[gray]{0.72}\put(4.5,9.5){\SQX}
\color[gray]{0.84}\put(4.5,8.5){\SQX}
\color[gray]{0.00}\put(4.5,7.5){\SQX}
\color[gray]{0.12}\put(4.5,6.5){\SQX}
\color[gray]{0.24}\put(4.5,5.5){\SQX}
\color[gray]{0.36}\put(4.5,4.5){\SQX}
\color[gray]{0.48}\put(4.5,3.5){\SQX}
\color[gray]{0.60}\put(4.5,2.5){\SQX}
\color[gray]{0.72}\put(4.5,1.5){\SQX}
<\color[gray]{0.84}\put(4.5,0.5){\SQX}
\color[gray]{0.60}
\put(5.5,18.5){\SQX}
\put(5.5,17.5){\SQX}
\put(5.5,16.5){\SQX}
\color[gray]{0.00}\put(5.5,15.5){\SQX}
\color[gray]{0.12}\put(5.5,14.5){\SQX}
\color[gray]{0.24}\put(5.5,13.5){\SQX}
\color[gray]{0.36}\put(5.5,12.5){\SQX}
\color[gray]{0.48}\put(5.5,11.5){\SQX}
\color[gray]{0.60}\put(5.5,10.5){\SQX}
\color[gray]{0.72}\put(5.5,9.5){\SQX}
\color[gray]{0.84}\put(5.5,8.5){\SQX}
\color[gray]{0.00}\put(5.5,7.5){\SQX}
\color[gray]{0.12}\put(5.5,6.5){\SQX}
\color[gray]{0.24}\put(5.5,5.5){\SQX}
\color[gray]{0.36}\put(5.5,4.5){\SQX}
\color[gray]{0.48}\put(5.5,3.5){\SQX}
\color[gray]{0.60}\put(5.5,2.5){\SQX}
\color[gray]{0.72}\put(5.5,1.5){\SQX}
\color[gray]{0.84}\put(5.5,0.5){\SQX}
\color[gray]{0.72}
\put(6.5,18.5){\SQX}
\put(6.5,17.5){\SQX}
\put(6.5,16.5){\SQX}
\color[gray]{0.00}\put(6.5,15.5){\SQX}
\color[gray]{0.12}\put(6.5,14.5){\SQX}
\color[gray]{0.24}\put(6.5,13.5){\SQX}
\color[gray]{0.36}\put(6.5,12.5){\SQX}
\color[gray]{0.48}\put(6.5,11.5){\SQX}
\color[gray]{0.60}\put(6.5,10.5){\SQX}
\color[gray]{0.72}\put(6.5,9.5){\SQX}
\color[gray]{0.84}\put(6.5,8.5){\SQX}
\color[gray]{0.00}\put(6.5,7.5){\SQX}
\color[gray]{0.12}\put(6.5,6.5){\SQX}
\color[gray]{0.24}\put(6.5,5.5){\SQX}
\color[gray]{0.36}\put(6.5,4.5){\SQX}
\color[gray]{0.48}\put(6.5,3.5){\SQX}
\color[gray]{0.60}\put(6.5,2.5){\SQX}
\color[gray]{0.72}\put(6.5,1.5){\SQX}
\color[gray]{0.84}\put(6.5,0.5){\SQX}
\color[gray]{0.84}
\put(7.5,18.5){\SQX}
\put(7.5,17.5){\SQX}
\put(7.5,16.5){\SQX}
\color[gray]{0.00}\put(7.5,15.5){\SQX}
\color[gray]{0.12}\put(7.5,14.5){\SQX}
\color[gray]{0.24}\put(7.5,13.5){\SQX}
\color[gray]{0.36}\put(7.5,12.5){\SQX}
\color[gray]{0.48}\put(7.5,11.5){\SQX}
\color[gray]{0.60}\put(7.5,10.5){\SQX}
\color[gray]{0.72}\put(7.5,9.5){\SQX}
\color[gray]{0.84}\put(7.5,8.5){\SQX}
\color[gray]{0.00}\put(7.5,7.5){\SQX}
\color[gray]{0.12}\put(7.5,6.5){\SQX}
\color[gray]{0.24}\put(7.5,5.5){\SQX}
\color[gray]{0.36}\put(7.5,4.5){\SQX}
\color[gray]{0.48}\put(7.5,3.5){\SQX}
\color[gray]{0.60}\put(7.5,2.5){\SQX}
\color[gray]{0.72}\put(7.5,1.5){\SQX}
\color[gray]{0.84}\put(7.5,0.5){\SQX}
\end{picture}\vskip3pt
}\hfil
{\hsize=0.8\linewidth
\vbox{\noindent\strut\parfillskip=0pt
where each row corresponds to an array index and each column corresponds to a thread.  In each diagram,
the locations in a row are contiguous in memory; it is really the fact that we choose
to index  by topic and to assign each document to a thread
that causes ``transposition'' to occur.  In any case, Algorithm~\ref{alg:transposecache}
is coded so that every set of  simultaneous}}}

\vskip-\parskip\noindent
({\sc simd}) memory accesses refers
to  consecutive memory locations, so it runs much faster than a ``nontransposed'' version would;
and the result is that each thread ends up with data that other threads need.



\begin{algorithm}[t]
\caption{Compute partial sums (transposed access)}\label{alg:transposesum}
\begin{algorithmic}[1]
\CodeChunk{compute partial sums of - products}
               \Bind{}{}  \label{z6wordfetch}
               \ForAll{}    \label{z6broadcaststart}
                 \Assign{}{}    \Comment{Transposed access to }
               \EndFor     \label{z6broadcastend}
               \Bind{}{}
                 \Bind{}{}
                  \While{}  \label{z6remnantsumstart}
                      \Assign{}{}
                      \Assign{}{}
                      \Assign{}{}
                 \EndWhile   \label{z6remnantsumend}
                 \While{}
                    \For{ from  through }   \label{z6firstsumprodinnerloopstart}
                       \Remark{Next line uses transposed access to }
                       \Assign{}{}     \label{z6phiaccess}
                    \EndFor    \label{z6firstsumprodinnerloopend}
                    \For{ from  through }   \label{z6secondsumprodinnerloopstart}
                       \Remark{Next line uses transposed access to }
                       \Assign{}{}     \label{z6aaccess}
                       \Assign{}{}
                    \EndFor    \label{z6secondsumprodinnerloopend}
                    \Assign{}{}
                 \EndWhile
\End
\end{algorithmic}
\end{algorithm}



One possible remedy is to have the threads exchange data so that each thread has exactly the  values
it needs for the rest of the computation.  Instead, we compensate for the transposition of
 in Algorithm~\ref{alg:transposesum}.
The idea is to divide the array  into blocks (and possibly a remnant)
and perform transposed accesses to .  To do this, each thread needs to know what part of
the array  every other thread is interested in; this is done through the  local work array .
In line~\ref{z6wordfetch}, each thread figures out which word is the th word of its document and calls it ;
in lines~\hbox{\ref{z6broadcaststart}--\ref{z6broadcastend}} it then stores its value for  into every element of row 
of the array .
This is not an especially fast operation, but it pays for itself later on.
The loop in lines~\hbox{\ref{z6remnantsumstart}--\ref{z6remnantsumend}} computes - products and partial sums 
in the usual way (remember that the remnant in  is not transposed), but the loop in
lines~\hbox{\ref{z6firstsumprodinnerloopstart}--\ref{z6firstsumprodinnerloopend}}
processes a block to compute product values to store into the  array;
the access to  on line~\ref{z6phiaccess} is transposed (note that the accesses to 
and  are \emph{not} transposed; because they were constructed and stored in transposed form,
normal fetches cause their values to line up correctly with the  values obtained by a transposed fetch).
So this is pretty good; but in line~\ref{z6aaccess} we finally pay the piper: in order to have the finally computed partial sums 
reside in the correct thread for the binary search, it is necessary to perform a transposed access to 
on line~\ref{z6aaccess}; but  is a local array, so transposed accesses are bad rather than good,
and this occurs in an inner loop, so performance still suffers.



\begin{algorithm}[t]
\caption{Drawing new  values using a butterfly table}\label{alg:simddraw}
\begin{algorithmic}[1]
\ProcTwo{DrawZ}{}\ProcTwoX{}
\ForAll{}
      \For{\hbox{\small\bf SIMD} }
         \Bind{}{}
        \LocalArray{}  \label{z9localdecls}
         \RegisterArray{}  \label{z9registerdecls}
         \UseCodeChunk{cache  values into }
         \Bind{}{}
          \While{}
            \Bind{}{}
            \UseCodeChunk{{\sc SIMD} compute butterfly partial sums}
            \Bind{}{}
            \UseCodeChunk{{\sc SIMD} search butterfly partial sums}
            \Assign{}{}
            \Assign{}{}
         \EndWhile
     \EndFor
   \EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}




\section{Butterfly-patterned Partial Sums}


We can avoid the cost of the final transposition of  by not requiring the partial sums table 
for each thread to be entirely in the local memory of that thread.  Instead, we arrange for the
threads to ``help each other'' during the binary search, in much the same way that
each thread computes - products that are actually of interest to other threads.
Moreover, we avoid computing the entire set of partial sums; instead (this is our novel contribution) we compute a
``butterfly-patterned'' table of partial sums that is sufficient to reconstruct any
needed partial sum on the fly during the binary search process.  This makes each step of
the binary search process slower, but a binary search
of a block of  entries examines only  elements of the block.

Our final version is Algorithm~\ref{alg:simddraw}.  It is quite similar to Algorithm~\ref{alg:transposedraw},
but declares all local arrays in such a way as to be thread-local (and specifies that arrays  and 
are expected to reside in registers).  It uses the code chunk in Algorithm~\ref{alg:transposecache}
to cache  values in , and also uses two new code chunks:
Algorithm~\ref{alg:simdsum} creates a butterfly-patterned table of partial sums, and
Algorithm~\ref{alg:simdsearch} uses this table to perform the binary search.
For this algorithm to work properly,  must be a power of 2.


\begin{algorithm}[t]
\caption{Compute a butterfly-patterned table of sums}\label{alg:simdsum}
\begin{algorithmic}[1]
\CodeChunk{{\sc SIMD} compute butterfly partial sums}
             \Bind{}{}
               \ForAll{}
                 \Assign{}{}  \label{z9shuffle}
               \EndFor
               \Bind{}{}
                 \Bind{}{}
                  \While{}
                      \Assign{}{}
                      \Assign{}{}
                      \Assign{}{}
                 \EndWhile
                 \While{}
                    \For{ from  through }  \label{z9unrollk}
                       \Remark{Next line uses transposed access to }
                       \Assign{}{}
                    \EndFor
                    \For{ from  through }  \label{z9unrollb}
                       \Bind{}{}
                       \For{ from  through }  \label{z9unrolli}
                          \Bind{}{}
                          \Bind{}{(}
                          \PhantomBind{}{}
                          \PhantomBind{}{)}
                         \Bind{}{}  \label{z9shufflexor}
                          \If{}
                             \Assign{}{}
                          \EndIf
                          \Assign{}{}
                          \Assign{}{}
                       \EndFor
                    \EndFor
                    \Assign{}{}
                    \Assign{}{}
                    \Assign{}{}
                 \EndWhile
\End
\end{algorithmic}
\end{algorithm}




\newcommand\Z[3]{{\hbox{}}}

\newcommand\ZS[1]{{\setbox0=\hbox{}\setbox1=\hbox{}\relax
  \ifdim \wd0 > 1.5\wd1 \hbox{\hskip-1pt}\else \hbox{}\fi}}

\newcommand\ZB[3]{{\unitlength=1 in \divide\unitlength by 16
  \begin{picture}(3.3,3)
  \put(0,0){\hbox{\color[gray]{1}\vrule width 3.3\unitlength height 3\unitlength}}
  \put(1.65,1.5){\vbox to 0pt{\vss\hbox to 0pt{\hss{}\hss}\vss}}
  \put(0,0){\line(0,1){3}}
  \put(0,0){\line(1,0){3.3}}
  \put(3.3,0){\line(0,1){3}}
  \put(0,3){\line(1,0){3.3}}
 \end{picture}}}



To save space in our figures, we introduce a special abbreviation: .
(The variable  is a free parameter of this notation; when we use the notation, it will consistently refer to
a computation in one loop iteration during which  is unchanging.)  The large number  identifies a thread
that owns a document, and the subscript  and superscript  identify a subarray ; the symbol denotes the sum over that subarray.

For purposes of illustration, we shall (as before) assume  and ,
so that the remnant is of size  and there are two blocks of size ; we also assume .

\newdimen\patternhskip \patternhskip=0.7pt
\newcommand\PH{\hskip\patternhskip}
\begin{figure}
\renewcommand\arraystretch{1.2}   
\hbox to \linewidth{\relax
\begin{tabular}{|@{}l@{\PH}l@{\PH}l@{\PH}l@{\PH}l@{\PH}l@{\PH}l@{\PH}l@{}|}
\hline
\Z{0}{0}{0}&\Z{1}{0}{0}&\Z{2}{0}{0}&\Z{3}{0}{0}&\Z{4}{0}{0}&\Z{5}{0}{0}&\Z{6}{0}{0}&\Z{7}{0}{0}\\
\Z{0}{0}{1}&\Z{1}{0}{1}&\Z{2}{0}{1}&\Z{3}{0}{1}&\Z{4}{0}{1}&\Z{5}{0}{1}&\Z{6}{0}{1}&\Z{7}{0}{1}\\
\Z{0}{0}{2}&\Z{1}{0}{2}&\Z{2}{0}{2}&\Z{3}{0}{2}&\Z{4}{0}{2}&\Z{5}{0}{2}&\Z{6}{0}{2}&\Z{7}{0}{2}\\
\hline
\Z{0}{0}{3}&\Z{1}{0}{3}&\Z{2}{0}{3}&\Z{3}{0}{3}&\Z{4}{0}{3}&\Z{5}{0}{3}&\Z{6}{0}{3}&\Z{7}{0}{3}\\
\Z{0}{0}{4}&\Z{1}{0}{4}&\Z{2}{0}{4}&\Z{3}{0}{4}&\Z{4}{0}{4}&\Z{5}{0}{4}&\Z{6}{0}{4}&\Z{7}{0}{4}\\
\Z{0}{0}{5}&\Z{1}{0}{5}&\Z{2}{0}{5}&\Z{3}{0}{5}&\Z{4}{0}{5}&\Z{5}{0}{5}&\Z{6}{0}{5}&\Z{7}{0}{5}\\
\Z{0}{0}{6}&\Z{1}{0}{6}&\Z{2}{0}{6}&\Z{3}{0}{6}&\Z{4}{0}{6}&\Z{5}{0}{6}&\Z{6}{0}{6}&\Z{7}{0}{6}\\
\Z{0}{0}{7}&\Z{1}{0}{7}&\Z{2}{0}{7}&\Z{3}{0}{7}&\Z{4}{0}{7}&\Z{5}{0}{7}&\Z{6}{0}{7}&\Z{7}{0}{7}\\
\Z{0}{0}{8}&\Z{1}{0}{8}&\Z{2}{0}{8}&\Z{3}{0}{8}&\Z{4}{0}{8}&\Z{5}{0}{8}&\Z{6}{0}{8}&\Z{7}{0}{8}\\
\Z{0}{0}{9}&\Z{1}{0}{9}&\Z{2}{0}{9}&\Z{3}{0}{9}&\Z{4}{0}{9}&\Z{5}{0}{9}&\Z{6}{0}{9}&\Z{7}{0}{9}\\
\Z{0}{0}{10}&\Z{1}{0}{10}&\Z{2}{0}{10}&\Z{3}{0}{10}&\Z{4}{0}{10}&\Z{5}{0}{10}&\Z{6}{0}{10}&\Z{7}{0}{10}\\
\hline
\Z{0}{0}{11}&\Z{1}{0}{11}&\Z{2}{0}{11}&\Z{3}{0}{11}&\Z{4}{0}{11}&\Z{5}{0}{11}&\Z{6}{0}{11}&\Z{7}{0}{11}\\
\Z{0}{0}{12}&\Z{1}{0}{12}&\Z{2}{0}{12}&\Z{3}{0}{12}&\Z{4}{0}{12}&\Z{5}{0}{12}&\Z{6}{0}{12}&\Z{7}{0}{12}\\
\Z{0}{0}{13}&\Z{1}{0}{13}&\Z{2}{0}{13}&\Z{3}{0}{13}&\Z{4}{0}{13}&\Z{5}{0}{13}&\Z{6}{0}{13}&\Z{7}{0}{13}\\
\Z{0}{0}{14}&\Z{1}{0}{14}&\Z{2}{0}{14}&\Z{3}{0}{14}&\Z{4}{0}{14}&\Z{5}{0}{14}&\Z{6}{0}{14}&\Z{7}{0}{14}\\
\Z{0}{0}{15}&\Z{1}{0}{15}&\Z{2}{0}{15}&\Z{3}{0}{15}&\Z{4}{0}{15}&\Z{5}{0}{15}&\Z{6}{0}{15}&\Z{7}{0}{15}\\
\Z{0}{0}{16}&\Z{1}{0}{16}&\Z{2}{0}{16}&\Z{3}{0}{16}&\Z{4}{0}{16}&\Z{5}{0}{16}&\Z{6}{0}{16}&\Z{7}{0}{16}\\
\Z{0}{0}{17}&\Z{1}{0}{17}&\Z{2}{0}{17}&\Z{3}{0}{17}&\Z{4}{0}{17}&\Z{5}{0}{17}&\Z{6}{0}{17}&\Z{7}{0}{17}\\
\Z{0}{0}{18}&\Z{1}{0}{18}&\Z{2}{0}{18}&\Z{3}{0}{18}&\Z{4}{0}{18}&\Z{5}{0}{18}&\Z{6}{0}{18}&\Z{7}{0}{18}\\
\hline
\end{tabular}\hfil
\begin{tabular}{|@{}l@{\PH}l@{\PH}l@{\PH}l@{\PH}l@{\PH}l@{\PH}l@{\PH}l@{}|}
\hline
\Z{0}{0}{0}&\Z{1}{0}{0}&\Z{2}{0}{0}&\Z{3}{0}{0}&\Z{4}{0}{0}&\Z{5}{0}{0}&\Z{6}{0}{0}&\Z{7}{0}{0}\\
\Z{0}{0}{1}&\Z{1}{0}{1}&\Z{2}{0}{1}&\Z{3}{0}{1}&\Z{4}{0}{1}&\Z{5}{0}{1}&\Z{6}{0}{1}&\Z{7}{0}{1}\\
\Z{0}{0}{2}&\Z{1}{0}{2}&\Z{2}{0}{2}&\Z{3}{0}{2}&\Z{4}{0}{2}&\Z{5}{0}{2}&\Z{6}{0}{2}&\Z{7}{0}{2}\\
\hline
\Z{0}{3}{3}&\Z{1}{4}{4}&\Z{0}{5}{5}&\Z{1}{6}{6}&\Z{0}{7}{7}&\Z{1}{8}{8}&\Z{0}{9}{9}&\Z{1}{10}{10}\\ 
\Z{0}{3}{4}&\Z{1}{3}{4}&\Z{2}{5}{6}&\Z{3}{5}{6}&\Z{0}{7}{8}&\Z{1}{7}{8}&\Z{2}{9}{10}&\Z{3}{9}{10}\\  
\Z{2}{3}{3}&\Z{3}{4}{4}&\Z{2}{5}{5}&\Z{3}{6}{6}&\Z{2}{7}{7}&\Z{3}{8}{8}&\Z{2}{9}{9}&\Z{3}{10}{10}\\   
\Z{0}{3}{6}&\Z{1}{3}{6}&\Z{2}{3}{6}&\Z{3}{3}{6}&\Z{4}{7}{10}&\Z{5}{7}{10}&\Z{6}{7}{10}&\Z{7}{7}{10}\\    
\Z{4}{3}{3}&\Z{5}{4}{4}&\Z{4}{5}{5}&\Z{5}{6}{6}&\Z{4}{7}{7}&\Z{5}{8}{8}&\Z{4}{9}{9}&\Z{5}{10}{10}\\     
\Z{4}{3}{4}&\Z{5}{3}{4}&\Z{6}{5}{6}&\Z{7}{5}{6}&\Z{4}{7}{8}&\Z{5}{7}{8}&\Z{6}{9}{10}&\Z{7}{9}{10}\\      
\Z{6}{3}{3}&\Z{7}{4}{4}&\Z{6}{5}{5}&\Z{7}{6}{6}&\Z{6}{7}{7}&\Z{7}{8}{8}&\Z{6}{9}{9}&\Z{7}{10}{10}\\       
\Z{0}{0}{10}&\Z{1}{0}{10}&\Z{2}{0}{10}&\Z{3}{0}{10}&\Z{4}{0}{10}&\Z{5}{0}{10}&\Z{6}{0}{10}&\Z{7}{0}{10}\\        
\hline
\Z{0}{11}{11}&\Z{1}{12}{12}&\Z{0}{13}{13}&\Z{1}{14}{14}&\Z{0}{15}{15}&\Z{1}{16}{16}&\Z{0}{17}{17}&\Z{1}{18}{18}\\ 
\Z{0}{11}{12}&\Z{1}{11}{12}&\Z{2}{13}{14}&\Z{3}{13}{14}&\Z{0}{15}{16}&\Z{1}{15}{16}&\Z{2}{17}{18}&\Z{3}{17}{18}\\  
\Z{2}{11}{11}&\Z{3}{12}{12}&\Z{2}{13}{13}&\Z{3}{14}{14}&\Z{2}{15}{15}&\Z{3}{16}{16}&\Z{2}{17}{17}&\Z{3}{18}{18}\\   
\Z{0}{11}{14}&\Z{1}{11}{14}&\Z{2}{11}{14}&\Z{3}{11}{14}&\Z{4}{15}{18}&\Z{5}{15}{18}&\Z{6}{15}{18}&\Z{7}{15}{18}\\    
\Z{4}{11}{11}&\Z{5}{12}{12}&\Z{4}{13}{13}&\Z{5}{14}{14}&\Z{4}{15}{15}&\Z{5}{16}{16}&\Z{4}{17}{17}&\Z{5}{18}{18}\\     
\Z{4}{11}{12}&\Z{5}{11}{12}&\Z{6}{13}{14}&\Z{7}{13}{14}&\Z{4}{15}{16}&\Z{5}{15}{16}&\Z{6}{17}{18}&\Z{7}{17}{18}\\      
\Z{6}{11}{11}&\Z{7}{12}{12}&\Z{6}{13}{13}&\Z{7}{14}{14}&\Z{6}{15}{15}&\Z{7}{16}{16}&\Z{6}{17}{17}&\Z{7}{18}{18}\\       
\Z{0}{0}{18}&\Z{1}{0}{18}&\Z{2}{0}{18}&\Z{3}{0}{18}&\Z{4}{0}{18}&\Z{5}{0}{18}&\Z{6}{0}{18}&\Z{7}{0}{18}\\        
\hline
\end{tabular}}
\caption{On the left, a table of partial sums for 8 threads and 19 topics; on the right,
a butterfly-patterned table of partial sums, in which each thread holds data of interest
to other threads}
\label{fig:partialsums}
\end{figure}






For each , Algorithm~\ref{alg:transposedraw} uses Algorithm~\ref{alg:transposesum}
to compute, for each  and , .  For our example, this produces the  table shown
on the left-hand side of Figure~\ref{fig:partialsums}; each column corresponds to a warp thread
and contains all partial sums needed by that thread.
But Algorithm~\ref{alg:simddraw} uses Algorithm~\ref{alg:simdsum}
to more cheaply compute ``butterfly-patterned partial sums'' as shown
on the right-hand side of Figure~\ref{fig:partialsums}.  Not all data needed by a thread
is in the column for that thread; for example, some data needed by thread~2 appears in columns~0, 4, and~6.  Note that all the data
in the remnant and in the last line of each block \emph{is} in the column for the thread
to which it belongs---in fact, these entries are identical to those computed by Algorithm~\ref{alg:transposesum}.
The tables differ only in the first  rows of each block.

To see why this table can be called ``butterfly-patterned,'' consider the processing of a single block.
We regard a block of  as first being initialized from a block of -
products in the  local array~---which, recall, is stored in transposed form.
(To simplify this part of the illustration, we will assume that the rows of the block are numbered (indexed) from
 to ; it is as if the remnant has size zero and we are examining the first block.)

The ``butterfly'' portion of the algorithm sweeps over the array  in a specific order,
and at each step
operates on four entries within~ that are at the intersection of two rows whose indices differ by a power of~2
and two columns whose indices differ by that same power of~2.  Suppose the four values in those entries are
; they are replaced by
.
(It might seem more natural mathematically to replace
the four values with ,
and that strategy also leads to a working algorithm,
but on the {\sc nvidia} Titan {\sc gpu}, at least, that computation turns out to be noticeably more expensive,
for reasons related to the precise instruction sequences required---this butterfly-patterned computation
occurs in the innermost loop, where the inclusion of even one extra instruction can significantly decrease performance.)

\newcommand\replace[4]{\hbox{}}

\newcommand\diagramshrinktop{\vskip-5pt }
\newcommand\diagramshrink{\vskip-7pt }

Now, it must be admitted that if we examine the pattern in which data is transferred between rows and
columns during one of these replacement computations, we see that it is not the conventional ``'' butterfly design:
\begin{center}
\vbox{\diagramshrinktop\includegraphics[width=2.0in]{ButterflyTop.pdf}\diagramshrink}
\end{center}
To see more precisely why we use the phrase ``butterfly-patterned,'' it is helpful to consider a three-dimensional diagram
in which the vertical axis is time, the horizontal axis spans columns, and the front-to-back axis spans rows:
\begin{center}
\vbox{\diagramshrinktop\includegraphics[width=2.0in]{Butterfly3D.pdf}\diagramshrink}
\end{center}
The top plane of this diagram is labeled with the state of four entries before the replacement computation,
the bottom plane is labeled with the state of four entries after the replacement computation,
and the six arrows show the full pattern of transfer between the top plane and the bottom plane
(including data that remains in place, not moving in space but being carried forward in time).
The previous diagram is a vertical projection of this diagram (that is, what the 3-D diagram looks like when seen from above).


The next diagram is a front-to-back projection of this diagram (that is, what the 3-D diagram looks like when seen from the front):
\begin{center}
\vbox{\includegraphics[width=2.0in]{ButterflyFront.pdf}\diagramshrink}
\end{center}
and here we clearly see the standard ``'' butterfly pattern as data is carried forward in time within each column and also exchanged between the two columns.  Similarly, the next diagram is a horizontal projection of the 3-D diagram (that is, what the 3-D diagram looks like when seen from the side):
\begin{center}
\vbox{\diagramshrinktop\includegraphics[width=2.0in]{ButterflySide.pdf}\diagramshrink}
\end{center}
and once again we clearly see the standard ``'' butterfly pattern as data is carried forward in time within each row and also exchanged between the two rows.


We will use the symbol ``\replace{i}{j}{k}{l}'' to indicate application of this replacement computation to
rows  and  and columns  and ----that is, to the four entries at positions
, , , and .  In our example with , these replacement computations are performed:

\begin{center}\renewcommand\arraystretch{1.3}
\begin{tabular}{@{}llll@{}}
\hline
\replace{0}{1}{0}{1} & \replace{0}{1}{2}{3} & \replace{0}{1}{4}{5} & \replace{0}{1}{6}{7} \\
\replace{2}{3}{0}{1} & \replace{2}{3}{2}{3} & \replace{2}{3}{4}{5} & \replace{2}{3}{6}{7} \\
\replace{4}{5}{0}{1} & \replace{4}{5}{2}{3} & \replace{4}{5}{4}{5} & \replace{4}{5}{6}{7} \\
\replace{6}{7}{0}{1} & \replace{6}{7}{2}{3} & \replace{6}{7}{4}{5} & \replace{6}{7}{6}{7} \\
\hline
\replace{1}{3}{0}{2} & \replace{1}{3}{1}{3} & \replace{1}{3}{4}{6} & \replace{1}{3}{5}{7} \\
\replace{5}{7}{0}{2} & \replace{5}{7}{1}{3} & \replace{5}{7}{4}{6} & \replace{5}{7}{5}{7} \\
\hline
\replace{3}{7}{0}{4} & \replace{3}{7}{1}{5} & \replace{3}{7}{2}{6} & \replace{3}{7}{3}{7} \\
\hline
\end{tabular}
\end{center}
where replacements within a set between horizontal lines are independent
and therefore may be done sequentially or in parallel, but each set must be completed before
beginning computation of the replacements in the group below it.  Figure~\ref{fig:replacements}
shows the progress of this process with snapshots after each set of replacements is performed.
In the general case, there are  such sets of replacements.


\begin{algorithm}[t]
\caption{Searching within a butterfly-patterned table}\label{alg:simdsearch}
\begin{algorithmic}[1]
\CodeChunk{{\sc SIMD} search butterfly partial sums}
            \Bind{}{random value chosen from }
               \Bind{}{}
               \Bind{}{}
               \Bind{}{}
               \Bind{}{}
               \Remark{Binary search to find correct block of size }
               \While{}
                  \Bind{}{}
                  \If{}
                     \Assign{}{}
                  \Else
                     \Assign{}{}
                  \EndIf
               \EndWhile
               \Bind{}{}   \label{z9blockBase}
               \If{}
                 \UseCodeChunk{butterfly search one block}
               \EndIf
               \If{}
                \If{}
                 \Remark{Not in a block after all, so search remnant}
                 \For{ from  through }
                     \If{}
                        \Assign{}{}
                        \State {\bf break}
                     \EndIf
                  \EndFor
                \EndIf
               \EndIf
\End
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[t]
\caption{Butterfly search within one  block}\label{alg:simdsearchoneblock}
\begin{algorithmic}[1]
\CodeChunk{butterfly search one block}
               \Bind{}{(}
               \PhantomBind{}{}
               \PhantomBind{}{)}
               \Bind{}{}
               \Bind{}{}
               \Remark{Butterfly search within the block of size }
               \For{ from  through }  \label{z9treewalkstart}
                  \Bind{}{}
                  \Bind{}{}
                  \Bind{}{}
                  \For{ from  through }  \label{z9searchassistloopstart}
                     \Bind{}{}
                     \Bind{}{}
                     \Bind{}{}
                     \Bind{}{}  \label{z9searchassist}
                     \If{}
                        \Assign{}{}
                     \EndIf
                  \EndFor  \label{z9searchassistloopend}
                  \Bind{}{(}
                  \PhantomBind{}{}
                  \PhantomBind{}{)}
                  \If{}
                     \Assign{}{}
                     \Assign{}{}
                  \Else
                     \Assign{}{}
                     \Assign{}{}
                  \EndIf
               \EndFor  \label{z9treewalkend}
               \Assign{}{}
\End
\end{algorithmic}
\end{algorithm}



After all replacements have been done on a block of size ,
the entry in row  and column  contains the value  where
,
,
,
, and .
We use the symbol ``'' to indicate the bitwise negation (that is, {\sc not}) of an integer
represented in binary form; we also use the symbol ``'' to indicate the
the bitwise conjunction (that is, {\sc and}) of two binary integers, and
the symbol ``'' to indicate
the bitwise exclusive {\sc or} (that is, {\sc xor}) of two binary integers.

Algorithm~\ref{alg:simdsum} performs this computation.  The function  is used
in line~\ref{z9shuffle} to broadcast values from each thread of a warp to all the others,
and the function  is used
in line~\ref{z9shufflexor} to exchange values between pairs of threads whose thread numbers
differ by a power of~2.  These are precisely the {\sc cuda} intrinsic functions
{\tt \lower0.9ex\hbox{-}\lower0.9ex\hbox{-}shfl} and {\tt \lower0.9ex\hbox{-}\lower0.9ex\hbox{-}shfl\lower0.9ex\hbox{-}xor}
\cite{CUDA-handbook,CUDA-website-shuffle-functions}.








Within a butterfly-patterned block of partial sums,
Algorithm~\ref{alg:simdsearch} performs a binary search as follows.
The  value is computed exactly as in Algorithm~\ref{alg:basicdraw}, and a block to be searched is identified
by performing a binary search on the subarray consisting of just the last row of each block;
this identifies a specific block to search.  If , then there is at least one block,
and it is searched, but it is possible that the desired  value does not lie within that
block; in that case, the remnant is searched using a linear search.


\begin{figure*}
\renewcommand\arraystretch{1.2} \patternhskip=3pt   
\hbox to \textwidth{\relax
\begin{tabular}{|@{}l@{\PH}l@{\PH}l@{\PH}l@{\PH}l@{\PH}l@{\PH}l@{\PH}l@{}|}
\hline
\Z{0}{0}{0}&\Z{0}{1}{1}&\Z{0}{2}{2}&\Z{0}{3}{3}&\Z{0}{4}{4}&\Z{0}{5}{5}&\Z{0}{6}{6}&\Z{0}{7}{7}\\
\Z{1}{0}{0}&\Z{1}{1}{1}&\Z{1}{2}{2}&\Z{1}{3}{3}&\Z{1}{4}{4}&\Z{1}{5}{5}&\Z{1}{6}{6}&\Z{1}{7}{7}\\
\Z{2}{0}{0}&\Z{2}{1}{1}&\Z{2}{2}{2}&\Z{2}{3}{3}&\Z{2}{4}{4}&\Z{2}{5}{5}&\Z{2}{6}{6}&\Z{2}{7}{7}\\
\Z{3}{0}{0}&\Z{3}{1}{1}&\Z{3}{2}{2}&\Z{3}{3}{3}&\Z{3}{4}{4}&\Z{3}{5}{5}&\Z{3}{6}{6}&\Z{3}{7}{7}\\
\Z{4}{0}{0}&\Z{4}{1}{1}&\Z{4}{2}{2}&\Z{4}{3}{3}&\Z{4}{4}{4}&\Z{4}{5}{5}&\Z{4}{6}{6}&\Z{4}{7}{7}\\
\Z{5}{0}{0}&\Z{5}{1}{1}&\Z{5}{2}{2}&\Z{5}{3}{3}&\Z{5}{4}{4}&\Z{5}{5}{5}&\Z{5}{6}{6}&\Z{5}{7}{7}\\
\Z{6}{0}{0}&\Z{6}{1}{1}&\Z{6}{2}{2}&\Z{6}{3}{3}&\Z{6}{4}{4}&\Z{6}{5}{5}&\Z{6}{6}{6}&\Z{6}{7}{7}\\
\Z{7}{0}{0}&\Z{7}{1}{1}&\Z{7}{2}{2}&\Z{7}{3}{3}&\Z{7}{4}{4}&\Z{7}{5}{5}&\Z{7}{6}{6}&\Z{7}{7}{7}\\
\hline
\multicolumn{8}{c}{\small Transposed - products}
\end{tabular}\hfil
\begin{tabular}{|@{}l@{\PH}l@{\PH}l@{\PH}l@{\PH}l@{\PH}l@{\PH}l@{\PH}l@{}|}
\hline
\Z{0}{0}{0}&\Z{1}{1}{1}&\Z{0}{2}{2}&\Z{1}{3}{3}&\Z{0}{4}{4}&\Z{1}{5}{5}&\Z{0}{6}{6}&\Z{1}{7}{7}\\
\Z{0}{0}{1}&\Z{1}{0}{1}&\Z{0}{2}{3}&\Z{1}{2}{3}&\Z{0}{4}{5}&\Z{1}{4}{5}&\Z{0}{6}{7}&\Z{1}{6}{7}\\
\Z{2}{0}{0}&\Z{3}{1}{1}&\Z{2}{2}{2}&\Z{3}{3}{3}&\Z{2}{4}{4}&\Z{3}{5}{5}&\Z{2}{6}{6}&\Z{3}{7}{7}\\
\Z{2}{0}{1}&\Z{3}{0}{1}&\Z{2}{2}{3}&\Z{3}{2}{3}&\Z{2}{4}{5}&\Z{3}{4}{5}&\Z{2}{6}{7}&\Z{3}{6}{7}\\
\Z{4}{0}{0}&\Z{5}{1}{1}&\Z{4}{2}{2}&\Z{5}{3}{3}&\Z{4}{4}{4}&\Z{5}{5}{5}&\Z{4}{6}{6}&\Z{5}{7}{7}\\
\Z{4}{0}{1}&\Z{5}{0}{1}&\Z{4}{2}{3}&\Z{5}{2}{3}&\Z{4}{4}{5}&\Z{5}{4}{5}&\Z{4}{6}{7}&\Z{5}{6}{7}\\
\Z{6}{0}{0}&\Z{7}{1}{1}&\Z{6}{2}{2}&\Z{7}{3}{3}&\Z{6}{4}{4}&\Z{7}{5}{5}&\Z{6}{6}{6}&\Z{7}{7}{7}\\
\Z{6}{0}{1}&\Z{7}{0}{1}&\Z{6}{2}{3}&\Z{7}{2}{3}&\Z{6}{4}{5}&\Z{7}{4}{5}&\Z{6}{6}{7}&\Z{7}{6}{7}\\
\hline
\multicolumn{8}{c}{\small After first set}
\end{tabular}\hfil
\begin{tabular}{|@{}l@{\PH}l@{\PH}l@{\PH}l@{\PH}l@{\PH}l@{\PH}l@{\PH}l@{}|}
\hline
\Z{0}{0}{0}&\Z{1}{1}{1}&\Z{0}{2}{2}&\Z{1}{3}{3}&\Z{0}{4}{4}&\Z{1}{5}{5}&\Z{0}{6}{6}&\Z{1}{7}{7}\\
\Z{0}{0}{1}&\Z{1}{0}{1}&\Z{2}{2}{3}&\Z{3}{2}{3}&\Z{0}{4}{5}&\Z{1}{4}{5}&\Z{2}{6}{7}&\Z{3}{6}{7}\\
\Z{2}{0}{0}&\Z{3}{1}{1}&\Z{2}{2}{2}&\Z{3}{3}{3}&\Z{2}{4}{4}&\Z{3}{5}{5}&\Z{2}{6}{6}&\Z{3}{7}{7}\\
\Z{0}{0}{3}&\Z{1}{0}{3}&\Z{2}{0}{3}&\Z{3}{0}{3}&\Z{0}{4}{7}&\Z{1}{4}{7}&\Z{2}{4}{7}&\Z{3}{4}{7}\\
\Z{4}{0}{0}&\Z{5}{1}{1}&\Z{4}{2}{2}&\Z{5}{3}{3}&\Z{4}{4}{4}&\Z{5}{5}{5}&\Z{4}{6}{6}&\Z{5}{7}{7}\\
\Z{4}{0}{1}&\Z{5}{0}{1}&\Z{6}{2}{3}&\Z{7}{2}{3}&\Z{4}{4}{5}&\Z{5}{4}{5}&\Z{6}{6}{7}&\Z{7}{6}{7}\\
\Z{6}{0}{0}&\Z{7}{1}{1}&\Z{6}{2}{2}&\Z{7}{3}{3}&\Z{6}{4}{4}&\Z{7}{5}{5}&\Z{6}{6}{6}&\Z{7}{7}{7}\\
\Z{4}{0}{3}&\Z{5}{0}{3}&\Z{6}{0}{3}&\Z{7}{0}{3}&\Z{4}{4}{7}&\Z{5}{4}{7}&\Z{6}{4}{7}&\Z{7}{4}{7}\\
\hline
\multicolumn{8}{c}{\small After second set}
\end{tabular}\hfil
\begin{tabular}{|@{}l@{\PH}l@{\PH}l@{\PH}l@{\PH}l@{\PH}l@{\PH}l@{\PH}l@{}|}
\hline
\Z{0}{0}{0}&\Z{1}{1}{1}&\Z{0}{2}{2}&\Z{1}{3}{3}&\Z{0}{4}{4}&\Z{1}{5}{5}&\Z{0}{6}{6}&\Z{1}{7}{7}\\
\Z{0}{0}{1}&\Z{1}{0}{1}&\Z{2}{2}{3}&\Z{3}{2}{3}&\Z{0}{4}{5}&\Z{1}{4}{5}&\Z{2}{6}{7}&\Z{3}{6}{7}\\
\Z{2}{0}{0}&\Z{3}{1}{1}&\Z{2}{2}{2}&\Z{3}{3}{3}&\Z{2}{4}{4}&\Z{3}{5}{5}&\Z{2}{6}{6}&\Z{3}{7}{7}\\
\Z{0}{0}{3}&\Z{1}{0}{3}&\Z{2}{0}{3}&\Z{3}{0}{3}&\Z{4}{4}{7}&\Z{5}{4}{7}&\Z{6}{4}{7}&\Z{7}{4}{7}\\
\Z{4}{0}{0}&\Z{5}{1}{1}&\Z{4}{2}{2}&\Z{5}{3}{3}&\Z{4}{4}{4}&\Z{5}{5}{5}&\Z{4}{6}{6}&\Z{5}{7}{7}\\
\Z{4}{0}{1}&\Z{5}{0}{1}&\Z{6}{2}{3}&\Z{7}{2}{3}&\Z{4}{4}{5}&\Z{5}{4}{5}&\Z{6}{6}{7}&\Z{7}{6}{7}\\
\Z{6}{0}{0}&\Z{7}{1}{1}&\Z{6}{2}{2}&\Z{7}{3}{3}&\Z{6}{4}{4}&\Z{7}{5}{5}&\Z{6}{6}{6}&\Z{7}{7}{7}\\
\Z{0}{0}{7}&\Z{1}{0}{7}&\Z{2}{0}{7}&\Z{3}{0}{7}&\Z{4}{0}{7}&\Z{5}{0}{7}&\Z{6}{0}{7}&\Z{7}{0}{7}\\
\hline
\multicolumn{8}{c}{\small After third set}
\end{tabular}}
\caption{Generation of butterfly-patterned partial sums for a  block in three steps, each using a set of four-element replacement computations}
\label{fig:replacements}
\end{figure*}

In order to search within a block, Algorithm~\ref{alg:simdsearchoneblock} maintains
two additional state variables 
and .  An invariant is that if thread  has
indices  through  of a block still under consideration, then 
and .  In order to cut the search range in half, the binary search needs to
compare the  value to the midpoint value 
where ; in Algorithm~\ref{alg:basicbinarysearch}
this value is of course an entry in the  array, namely ,
but in Algorithm~\ref{alg:simdsearchoneblock} the midpoint value is \emph{calculated} by choosing an appropriate entry from
the butterfly-patterned ~array and then either adding it to  or subtracting it from .
Whether to add or subtract on iteration number  (where the  iterations are numbered starting from 0)
depends on whether bit  (counting from the right starting at 0) of the binary representation
of  is  or , respectively.  Depending on the result of the comparison of the midpoint value
with the  value,
the midpoint value is assigned to either  and , maintaining the invariant.
Also depending on the result of the comparison of the midpoint value
with the  value, a bit of a third state variable  (initially ) is updated.
When the binary search is complete, the correct index to select is computed from the value in .

A normal binary search effectively walks down a binary decision tree,
where each node of the tree is labeled with a (normal) partial sum; but Algorithm~\ref{alg:simdsearchoneblock} walks
down a tree that is labeled with entries from the butterfly-patterned table of partial sums.
For example, referring to the first seven rows of the upper block in the right-hand diagram in Figure~\ref{fig:partialsums},
the entries relevant to thread 5 form this tree:

\newcommand\centerit[1]{\vbox to 0pt{\vss\hbox to 0pt{\hss#1\hss}\vss}}

\vskip 1pt
\hbox to \linewidth{\relax
\hsize=0.43\linewidth
\vbox{\unitlength=1 in \divide\unitlength by 16\noindent
\begin{picture}(24,18)(1.5,0)
\put(6,9){\line(1,1){6}}
\put(18,9){\line(-1,1){6}}
\put(3,3){\line(1,2){3}}
\put(9,3){\line(-1,2){3}}
\put(15,3){\line(1,2){3}}
\put(21,3){\line(-1,2){3}}
\put(12,15){\centerit{\ZB{5}{7}{10}}}
\put(6,9){\centerit{\ZB{5}{3}{4}}}
\put(18,9){\centerit{\ZB{5}{7}{8}}}
\put(3,3){\centerit{\ZB{5}{4}{4}}}
\put(9,3){\centerit{\ZB{5}{6}{6}}}
\put(15,3){\centerit{\ZB{5}{8}{8}}}
\put(21,3){\centerit{\ZB{5}{10}{10}}}
\end{picture}
}\hfil
{\hsize=0.57\linewidth
\vbox{\noindent\strut
Starting with  and ,
lines~\hbox{\ref{z9treewalkstart}--\ref{z9treewalkend}} of Algorithm~\ref{alg:simdsearchoneblock}
can walk down the tree, at each node calculating a new midpoint
by adding the node label to  or subtracting it from  as appropriate.\strut}}}

\vskip-\parskip
The threads in a warp assist one another in fetching these tree nodes
using the loop in lines~\hbox{\ref{z9searchassistloopstart}--\ref{z9searchassistloopend}};
the function  effects this data transfer in line \ref{z9searchassist}.




\begin{figure}
\includegraphics[width=\linewidth]{ButterflyGraph.pdf}
\caption{Execution time ()}\label{fig:performance}
\end{figure}



\section{Evaluation}

We coded four versions of a complete {\sc lda} Gibbs-sampler topic-modeling algorithm in {\sc cuda} for an {\sc nvidia}
Titan Black {\sc gpu} ().  For each version we tested two variants, one using Algorithm~\ref{alg:basicdraw} (using the binary search
of Algorithm~\ref{alg:basicbinarysearch}) and one using Algorithm~\ref{alg:simddraw}.  All eight variants
were tested for speed using a Wikipedia-based
dataset with number of documents , vocabulary size , total number of words in corpus 
(therefore average document size ),
and maximum document size .  Each variant was measured using eight different values for the number of topics  (16, 48, 80, 112, 144, 176, 208, and 240),
in each case performing 100 sampling iterations and measuring the execution time of the entire application, not just the
part that draws  values.  Best performance requires unrolling three loops in Algorithm~\ref{alg:simdsum};
we had to manually unroll the loop that starts on line~\ref{z9unrollb}, and the {\sc cuda} compiler then automatically
unrolled the loops that start on lines~\ref{z9unrollk} and~\ref{z9unrolli}.
The performance results are shown in Figure~\ref{fig:performance}.
The butterfly variants are faster for .  For ,
for each of the four versions the butterfly variant is more than twice as fast.

\section{Related Work}

Because the computed probabilities are relative in our LDA application, it is necessary to compute all of them and then to compute,
if nothing else, their sum, so that the relative probabilities can be effectively normalized.  Therefore every method for
drawing from a discrete distribution represented by a set of relative probabilities
involves some amount of preprocessing before drawing from the distribution.
The various algorithms in the literature have differing tradeoffs according to what technique is used for preprocessing
what technique is used for drawing; some algorithms also accommodate incremental updating of the relative probabilities
by providing a technique for incremental preprocessing.

Instead of doing a binary search on the table of partial sums, one can instead (as Marsaglia~\cite{Marsaglia-1963} observes in passing)
construct a search tree using the principles of Huffman encoding~\cite{Huffman-1952} (independently rediscovered
by Zimmerman\cite{Zimmerman-1959}) to minimize the expected number of comparisons.  In either case
the complexity of the search is , but the optimized search may have a smaller constant,
obtained at the expense of a preprocessing step that must sort the relative probabilities and
therefore have complexity .

Walker~\cite{Walker-arbitrary-1974,Walker-TOMS-1977} describes what is now known as the ``alias'' method,
in which  relative probabilities are preprocessed into two additional tables  and  of length .  To draw a value
from the distribution, let  be a integer chosen uniformly at random from 
and let  be chosen uniformly at random from the real interval .  Then the value drawn from the
distribution is

Therefore, once the tables  and  have been produced, the complexity of drawing a value from the distribution is ,
assuming that the cost of an array access is .
Walker's method~\cite{Walker-TOMS-1977} for producing the tables  and  requires time ;
it is easy to reduce this to  by sorting the probabilities~\cite[exercise 3.4.1-7]{KNUTH-VOLUME-2}
and then using, say, priority heaps instead of a list for the intermediate data structure.
Either version heuristically attempts to minimize the probability of having to access the table .

Vose~\cite{Vose-1991} describes a preprocessing algorithm, with proof, that further reduces the preprocessing complexity
of the alias method to .  The tradeoff that permits this improvement is that the preprocessing algorithm makes
no attempt to minimize the probability of accessing the array .

Matias~{\it et~al.}~\cite{Matias-1993} describe a technique for preprocessing a set of relative probabilities into a set of trees,
after which a sequence of intermixed generate (draw) and update operations can be performed, where an update operation changes
just one of the relative probabilities; a single generate operation takes  expected time,
and a single update operation takes  amortized expected time.

Li~{\it et~al.}~\cite{li_reducing_14} describe a modified LDA topic modeling algorithm, which they call
Metropolis-Hastings-Walker sampling, that uses Walker's alias method but amortizes the cost of constructing
the table by drawing from the same table during multiple consecutive sampling iterations of a Metropolis-Hastings sampler;
their paper provides some justification for why it is acceptable to use a ``slightly stale'' alias table (their words)
for the purposes of this application.



\section{Conclusions}

The technique of constructing butterfly-patterned partial sums appears to be best suited
for situations where a SIMD processor is used to compute tables of relative probabilities for multiple discrete distributions,
each of which is then used just once to draw a single value, and where each thread, when computing its table,
must fetch data from a contiguous region of memory whose address is computed from other data.
The LDA application for which we developed the technique has these characteristics.
The technique uses transposed memory access in order to allow a SIMD memory controller
to touch only one or two cache lines on each fetch, then cheaply constructs a set of partial sums
that are just adequate to allow partial sums actually needed to be constructed on the fly
during the course of a binary search.
For a complete {\sc lda} Gibbs-sampler topic-modeling algorithm coded in {\sc cuda} for an {\sc nvidia}
Titan Black {\sc gpu} and already tuned as best we could for high performance, the butterfly-patterned
partial-sums technique further improves the speed of the overall application by at least a factor of 2
when the number of topics is greater than 200.






\bibliographystyle{plainurl}
\bibliography{butterfly}

\end{document} 
