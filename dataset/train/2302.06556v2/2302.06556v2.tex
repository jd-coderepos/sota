
\documentclass{article} \usepackage{iclr2023_conference, times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{siunitx}
\usepackage{graphicx}
\usepackage{soul}
\usepackage[hang]{subfigure}
\usepackage{lscape}
\usepackage[export]{adjustbox}
\usepackage[overload]{empheq}
\usepackage{wrapfig}
\usepackage{xcolor}
\usepackage{caption}
\usepackage[T1]{fontenc}
\definecolor{ao}{rgb}{0.0, 0.5, 0.0}
\definecolor{bluepigment}{rgb}{0.2, 0.2, 0.6}
\definecolor{coolblack}{rgb}{0.0, 0.18, 0.39}
\definecolor{codebrown}{rgb}{0.37,0.0,0.0}
\hypersetup{colorlinks,linkcolor={red},citecolor={codebrown},urlcolor={blue}} 

\newcommand{\kumar}[1]{\textcolor{red}{\textbf{Kumar:} #1}}
\newcommand{\formattedparagraph}[1]{\noindent \textbf{#1}}



\title{VA-DepthNet: A Variational Approach to Single Image Depth Prediction}




\author{Ce Liu~~~~Suryansh Kumar~~~~Shuhang Gu~~~~Radu Timofte~~~~Luc Van Gool \\ 
CVL ETH Z\"urich~~~UESTC China ~~~University of W\"urzburg~~~KU Leuven\\
\texttt{\{ce.liu, sukumar, vangool\}@vision.ee.ethz.ch}\\ ~~\texttt{shuhanggu@uestc.edu.cn, radu.timofte@uni-wuerzburg.de}
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \captionsetup[figure]{skip=0.0pt}
\captionsetup[table]{skip=0.0pt}
\addtolength{\parskip}{-0.1mm}
\pdfoutput=1

\begin{document}
\maketitle






\begin{abstract}
We introduce VA-DepthNet, a simple, effective, and accurate deep neural network approach for the single-image depth prediction (SIDP) problem. The proposed approach advocates using classical first-order variational constraints for this problem. While state-of-the-art deep neural network methods for SIDP learn the scene depth from images in a supervised setting, they often overlook the invaluable invariances and priors in the rigid scene space, such as the regularity of the scene. The paper's main contribution is to reveal the benefit of classical and well-founded variational constraints in the neural network design for the SIDP task. It is shown that imposing first-order variational constraints in the scene space together with popular encoder-decoder-based network architecture design provides excellent results for the supervised SIDP task. The imposed first-order variational constraint makes the network aware of the depth gradient in the scene space, i.e., regularity. The paper demonstrates the usefulness of the proposed approach via extensive evaluation and ablation analysis over several benchmark datasets, such as KITTI, NYU Depth V2, and SUN RGB-D. The VA-DepthNet at test time shows considerable improvements in depth prediction accuracy compared to the prior art and is accurate also at high-frequency regions in the scene space.  At the time of writing this paper, our method---labeled as VA-DepthNet, when tested on the KITTI depth-prediction evaluation set benchmarks, shows state-of-the-art results, and is the top-performing published approach\footnote{\href{https://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth\_prediction
}{kitti\_depth\_prediction\_benchmark}} \footnote{For official code refer \href{https://github.com/cnexah/VA-DepthNet}{here}}. 
\end{abstract}




\section{Introduction}
Over the last decade, neural networks have introduced a new prospect for the 3D computer vision field. It has led to significant progress on many long-standing problems in this field, such as multi-view stereo \citep{huang2018deepmvs,kaya2022uncertainty},  visual simultaneous localization and mapping \citep{teed2021droid}, novel view synthesis \citep{mildenhall2021nerf}, etc. Among several 3D vision problems, one of the challenging, if not impossible, to solve is the single-image depth prediction (SIDP) problem. SIDP is indeed ill-posed---in a strict geometric sense, presenting an extraordinary challenge to solve this inverse problem reliably. Moreover, since we do not have access to multi-view images, it is hard to constrain this problem via well-known geometric constraints \citep{longuet1981computer, nister2004efficient, furukawa2009accurate, kumar2019superpixel, kumar2017monocular}. Accordingly, the SIDP problem generally boils down to an ambitious fitting problem, to which deep learning provides a suitable way to predict an acceptable solution to this problem \citep{yuan2022new, yin2019enforcing}.

Impressive earlier methods use Markov Random Fields (MRF) to model monocular cues and the relation between several over-segmented image parts \citep{saxena2007learning, saxena2008make3d}. Nevertheless, with the recent surge in neural network architectures \citep{krizhevsky2012imagenet, simonyan2014very, he2016deep}, which has an extraordinary capability to perform complex regression, many current works use deep learning to solve SIDP and have demonstrated high-quality results \citep{yuan2022new, aich2021bidirectional, bhat2021adabins, eigen2014depth, fu2018deep, lee2019big, lee2021patch}. Popular recent methods for SIDP are mostly supervised. But even then, they are used less in real-world applications than geometric multiple view methods \citep{labbe2019rtab, muller2022instant}. Nonetheless, a good solution to SIDP is highly desirable in robotics \citep{yang2020d3vo}, virtual-reality \citep{hoiem2005automatic},  augmented reality \citep{du2020depthlab}, view synthesis \citep{hoiem2005automatic} and other related vision tasks \citep{liu2019dense}.





In this paper, we advocate that despite the supervised approach being encouraging, SIDP advancement should not wholly rely on the increase of dataset sizes. Instead, geometric cues and scene priors could help improve the SIDP results. Not that scene priors have not been studied to improve SIDP accuracy in the past. For instance, \citet{chen2016single} uses pairwise ordinal relations between points to learn scene depth. Alternatively, \citet{yin2019enforcing} uses surface normals as an auxiliary loss to improve performance. Other heuristic approaches, such as \citet{qi2018geonet}, jointly exploit the depth-to-normal relation to recover scene depth and surface normals. Yet, such state-of-the-art SIDP methods have limitations: for example, the approach in \citet{chen2016single} - using ordinal relation to learn depth - over-smooths the depth prediction results, thereby failing to preserve high-frequency surface details. Conversely, \citet{yin2019enforcing} relies on good depth map prediction from a deep network and the idea of virtual normal. The latter is computed by randomly sampling three non-collinear points with large distances. This is rather complex and heuristic in nature.  \citet{qi2018geonet} uses depth and normal consistency, which is good, yet it requires good depth map initialization. 

This brings us to the point that further generalization of the regression-based SIDP pipeline is required. As mentioned before, existing approaches in this direction have limitations and are complex. In this paper, we propose a simple approach that provides better depth accuracy and generalizes well across different scenes. To this end, we resort to the physics of variation \citep{mollenhoff2016sublabel, chambolle2010introduction} in the neural network design for better generalization of the SIDP network, which by the way, keeps the essence of affine invariance \citep{yin2019enforcing}. An image of a general scene---indoor or outdoor, has a lot of spatial regularity. And therefore, introducing a variational constraint provides a convenient way to ensure spatial regularity and to preserve information related to the scene discontinuities \citep{chambolle2010introduction}. Consequently, the proposed network is trained in a fully-supervised manner while encouraging the network to be mindful of the scene regularity where the variation in the depth gradient is large (cf. Sec.\ref{ssec:vlayer}). In simple terms, depth regression must be more than parameter fitting, and at some point, a mindful decision must be made---either by imaging features or by scene depth gradient variation, or both. As we demonstrate later in the paper, such an idea boosts the network's depth accuracy while preserving the high-frequency and low-frequency scene information (see Fig.\ref{fig:teaser2}).



Our neural network for SIDP disentangles the absolute scale from the metric depth. It models an unscaled depth map as the optimal solution to the pixel-level variational constraints via weighted first-order differences, respecting the neighboring pixel depth gradients. Compared to previous methods, the network's task has been shifted away from pixel-wise metric depth learning to learning the first-order differences of the scene, which alleviates the scale ambiguity and favors scene regularity. To realize that, we initially employ a neural network to predict the first-order differences of the depth map. Then, we construct the partial differential equations representing the variational constraints by reorganizing the differences into a large matrix, i.e., an over-determined system of equations. Further, the network learns a weight matrix to eliminate redundant equations that do not favor the introduced first-order difference constraint. Finally, the closed-form depth map solution is recovered via simple matrix operations.  

When tested on the KITTI  \citep{geiger2012we} and NYU Depth V2 \citep{silberman2012indoor} test sets, our method outperforms prior art depth prediction accuracy by a large margin. Moreover, our model pre-trained on NYU Depth V2 better generalizes to the SUN RGB-D test set. 




\begin{figure}[t]
\centering
\subfigure[Image]{
\begin{minipage}[b]{0.23\textwidth}
\includegraphics[width=0.9\textwidth, height=0.7\textwidth]{teaser2/image.pdf}
\end{minipage}
}
\subfigure[AdaBins]{
\begin{minipage}[b]{0.23\textwidth}
\includegraphics[width=0.9\textwidth, height=0.7\textwidth]{teaser2/adabins.pdf}
\end{minipage}
}
\subfigure[NeWCRFs]{
\begin{minipage}[b]{0.23\textwidth}
\includegraphics[width=0.9\textwidth, height=0.7\textwidth]{teaser2/newcrfs.pdf}
\end{minipage}
}
\subfigure[Ours]{
\begin{minipage}[b]{0.23\textwidth}
\includegraphics[width=0.9\textwidth, height=0.7\textwidth]{teaser2/dni.pdf}
\end{minipage}
}
\caption{\small Qualitative comparison of our method's depth result with recent state-of-the-art methods such as AdaBins~\citep{bhat2021adabins}, NeWCRFs~\citep{yuan2022new} on NYU Depth V2 test set~\citep{silberman2012indoor}. It can be observed that our method predicts high-frequency details better than other recent methods.}
\label{fig:teaser2}
\end{figure}

































\section{Prior Work}
Depth estimation is a longstanding task in computer vision. In this work, we focus on a fully-supervised, single-image approach, and therefore, we discuss prior art that directly relates to such approach. Broadly, we divide the popular supervised SIDP methods into three sub-categories.






\formattedparagraph{\textit{(i)} Depth Learning using Ranking or Ordinal Relation Constraint.} \citet{zoran2015learning} and \citet{chen2016single} argue that the ordinal relation between points is easier to learn than the metric depth. To this end, \citet{zoran2015learning} proposes constrained quadratic optimization while \citet{chen2016single} relies on the variation of the inception module to solve the problem. Later, \citet{xian2020structure} proposes structure-guided sampling strategies for point pairs to improve training efficiency. Recently, \citet{lienen2021plackett} elaborates on the use of listwise ranking method based on the Plackett-Luce model \citep{luce2012individual}.  The drawback of such approaches is that the ordinal relationship and ranking over smooth the depth solution making accurate metric depth recovery challenging.

\formattedparagraph{\textit{(ii)} Depth Learning using Surface Normal Constraint.}
\citet{hu2019revisiting} introduces normal loss in addition to the depth loss to overcome the distorted and blurry edges in the depth prediction. \citet{yin2019enforcing} proposes the concept of virtual normal to impose 3D scene constraint explicitly and to capture the long-range relations in the depth prediction. The long-range dependency in 3D is introduced via random sampling of three non-colinear points at a large distance from the virtual plane. Lately, \cite{long2021adaptive} proposes an adaptive strategy to compute the local patch surface normals at train time from a set of randomly sampled candidates and overlooks it during test time.

\formattedparagraph{\textit{(iii)} Depth Learning using other Heuristic Refinement Constraint.} There has been numerous works attempt to refine the depth prediction as a post-processing step. ~\citet{liu2015learning}, ~\citet{li2015cvpr} and ~\citet{yuan2022new} propose to utilize the Conditional Random Fields (CRF) to smooth the depth map. ~\citet{lee2019big} utilizes the planar assumptions to regularize the predicted depth map. ~\cite{qi2018geonet} adopts an auxiliary network to predict the surface normal, and then refine the predicted depth map following their proposed heuristic rules. There are mainly two problems with such approaches: Firstly, these approaches rely on a good depth map initialization. Secondly, the heuristic rules and the assumptions might result in over-smoothed depth values at objects boundaries.

Meanwhile, a few works, such as  \citet{ramamonjisoa2020predicting}, \citet{cheng2018depth}, \citet{li2017two}  were proposed in the past with similar inspirations.  \citet{ramamonjisoa2020predicting}, \citet{cheng2018depth} methods are generally motivated towards depth map refinement predicted from an off-the-shelf network. On the other hand, \citet{cheng2018depth} proposes to use an affinity matrix that aims to learn the relation between each pixel's depth value and its neighbors' depth values. However, the affinity matrix has no explicit supervision, which could lead to imprecise learning of neighboring relations providing inferior results.
On the contrary, our approach is mindful of imposing the first-order difference constraint leading to better performance. Earlier, \citet{li2017two} proposed two strategies for SIDP, i.e., fusion in an end-to-end network and fusion via optimization. The end-to-end strategy fuses the gradient and the depth map via convolution layers without any constraint on convolution weights, which may not be an apt choice for a depth regression problem such as SIDP. On the other hand, the fusion via optimization strategy is based on a non-differentiable strategy, leading to a non-end-to-end network loss function. Contrary to that, our method is well-constrained and performs quite well with a loss function that helps end-to-end learning of our proposed network. Not long ago, \citet{lee2019monocular} proposed to estimate relative depths between pairs of images and ordinary depths at a different scale. By exploiting the rank-1 property of the pairwise comparison matrix, it recovers the relative depth map. Later, relative and ordinary depths are decomposed and fused to recover the depth. On a slightly different note, \citet{lee2020multi} studies the effectiveness of various losses and how to combine them for better monocular depth prediction.


To sum up, our approach allows learning of confidence weight to select reliable gradient estimation in a fully differentiable manner. Further, it proffers the benefits of the variational approach to overcome the limitations of the existing state-of-the-art methods. More importantly, the proposed method can provide excellent depth prediction without making extra assumptions such as good depth initialization, piece-wise planar scene, and assumptions used by previous works mentioned above.



\begin{figure}[t]
\centering
\subfigure[t][First order depth variation along -axis.]{
\includegraphics[scale=0.6]{network/ill_dni101.pdf}
}
~~~\subfigure[t][Overall matrix by ordering the terms.]{
\includegraphics[scale=0.6]{network/ill_dni102.pdf}
}
\caption{\small \textbf{Illustration of the idea}. (a) Depth gradient constraint along  axis at location  in  matrix form. (b) Construction of the overall matrix formulation with constraints at all the pixel locations.}
\label{fig:dni}
\end{figure}

\section{Method}
In this section, we first describe our proposed variational constraint and then present the overall network architecture leading to the overall loss function.



\subsection{Variational Constraint}\label{ssec:vlayer}
Here we introduce our variational constraint and how it can be useful for depth estimation. Consider an unscaled depth map as , with  symbolizing the height and width, respectively. Assuming  and  as the gradient of  in the  and  axis, we write

Here,  and  subscript corresponds to the direction from left to right (-axis) and top to bottom of the image (-axis), respectively. Elaborating on this, we can write 




Suppose we augment Eq.(\ref{eq:gama_x}) expression for all ,  and . 
In that case, we will end up with an over-determined system with  equations in total. Given the predicted  and , we aim to recover the  unknown variables in .
However, some of the equations could be spurious and deteriorate the overall depth estimation result rather than improving it. As a result, we must be mindful about selecting the equation that respects the imposed first-order constraint and maintains the depth gradient to have a meaningful fitting for better generalization. To that end, we introduce confidence weight ,  for gradient along  direction. Consequently, we multiply the above two equations by the confidence weight term  and , respectively. On one hand, if the confidence is close to 1, the equation will have priority to be satisfied by the optimal . On the other hand, if the confidence is close to 0, we must ignore the equation. For better understanding, we illustrate the first-order difference and weighted matrix construction in Fig.\ref{fig:dni} (a) and Fig.\ref{fig:dni} (b).

Next, we reshape the , , , , and  into column vectors , , ,
, 
and , respectively. Organizing , we can write the overall expression in a compact matrix form  using simple algebra as follows




where  is the first-order difference operator. Specifically,  is a sparse matrix with only a few elements as 1 or -1. The  row of  provides the first-order difference operator for the  equation. The position of 1 and -1 indicates which pair of neighbors to be considered for the constraint. Fig.\ref{fig:dni} (b) provides a visual intuition about this matrix equation.

Eq.(\ref{eq:constraint}) can be utilized to recover  from the predicted , , , and . As alluded to above, we have more equations than unknowns, hence, we resort to recovering the optimal depth map  by minimizing the following equation:

Refer Appendix for the full derivation. The closed-form solution can be written as follows:

Denote  in Eq.(\ref{eq:analytic}), we write overall equation as . Next, we describe the overall network architecture.








\begin{figure}[t]
\centering
\includegraphics[width=0.9\linewidth]{network/dni_arch41.pdf}
\caption{\small \textbf{Overview of our framework.} Given an input image, first an encoder is employed to extract features. Then we predict the depth map by the V-layer. Next, we gradually upsample and refine the depth map. In the end, we recover the metric depth by the metric layer.} \label{fig:framwork}
\end{figure}



\subsection{Overall Network Architecture}\label{sec:network}
Our overall network architecture is composed of four main modules as follows.




\formattedparagraph{\textit{(a)} Encoder.} Given an input image, the encoder computes the hierarchical feature maps through a series of stages. To be precise, our encoder has four stages. Each stage contains transformer blocks \citep{liu2021swin}.  At the end of each stage, we collect the final feature map as the output of the encoder resulting in the encoded feature maps with strides 4, 8, 16, and 32, respectively. Our encoder module is inspired by \citet{liu2021swin}, a recent state-of-the-art transformer network design. We use it as our backbone by removing the final global pooling layer and fully connected layer.




\formattedparagraph{\textit{(b)} Variational Layer (V-Layer).} The goal of this layer is to compute a map from encoded feature maps to unscaled depth map, which adheres to the first-order variational constraint. As of V-layer, we feed the feature maps of strides 16 and 32 as input which is the output of the encoder. Since these features are at different resolutions, we up-sample the feature map of stride 32 to stride 16 via bi-linear interpolation and concatenate to construct , where  symbolizing the height, width, and the number of channels, respectively.
Note that  is not the same as the original resolution of the ground-truth depth and images. We use of two convolutional layers on  to predict the depth gradient and corresponding weight for each pixel as follows:

where,  denotes the convolutional layers with parameters . The predicted depth gradients  and  are observed to be more accurate at smooth surface than at boundaries. This brings us again to the point made above that we must take care of which first-order constraint must be included and discarded during regression. Using the Eq.(\ref{eq:network_prediction_eq}) prediction, we construct the variational constraint Eq.(\ref{eq:constraint}), and obtain the unscaled depth map following Eq.(\ref{eq:analytic}). The resulting depth map has a resolution of 1/16 to the original image, which is later upsampled to the appropriate dimension. 

To capture more scene features, we generate multiple channels (denoted as ) of  using Eq.(\ref{eq:network_prediction_eq}). As a result, we have a group of depth maps stacked along the channel dimension. For a feature map with spatial resolution , our V-layer has a complexity of . To overcome complexity issue, we perform V-layer operation on feature maps with stride 16 and then up-sample and refine the depth maps in the later stage. The V-layer pipeline is shown in Fig.\ref{fig:framwork}(b).








\formattedparagraph{\textit{(c)} Upsample and Refine.} This module upsamples and refines the input depth map via encoded features at a given depth map resolution. To this end, we perform refinement at three different resolutions in a hierarchical manner. Given the V-layer depth map at 1/16 resolution, we first refine the depth via encoded features at this resolution. Concretely, this refinement is done using the following set of operations. (1) concatenate the feature map and the depth map; (2) use one convolutional layer with ReLU activation to fuse the feature and depth information; and (3) predict refined feature and depth map via a convolutional layer. Later, the refined feature and depth map are upsampled and fed into 1/8 for later refinement using the same above set of operations. Finally, the exact is done at 1/4 resolution. Note that these steps are performed in a sequel. At the end of this module, we have a depth map of 1/4 of the actual resolution. The upsample and refine procedure are shown in Fig.\ref{fig:framwork}(c).













\formattedparagraph{\textit{(d)} Metric Layer.} 
We must infer the global scene scale and shift to recover the metric depth. For this, we perform global max pooling on the encoded feature map of stride 32. The resulting vector is fed into a stack of fully connected layers to regress the two scalars, i.e., one representing the scale and while other representing the shift. Using the feature map of stride 32 is motivated by the observation that we have a much richer global scene context using it than at higher depth resolution. It also provides a good compromise between computational complexity and accuracy.





\subsection{Loss Function}\label{sec:loss}

\formattedparagraph{Depth Loss.} It estimates the scale-invariant difference between the ground-truth depth and prediction at train time \citep{eigen2014depth}. The difference is computed by upsampling the predicted depth map to the same resolution as the ground truth via bi-linear interpolation. Denoting the predicted and ground-truth depth as ,  we compute the depth loss as follows 

Here,  is the number of positions with valid measurements and  is a hyper-parameter. Note that the above loss is used for valid measurements only.



\formattedparagraph{Variational Loss.} We define this loss using the output of  V-layer. Suppose the ground-truth depth map to be  and the predicted depth map for  channels as . Since the depth resolution is not same at this layer, we downsample the ground truth. It is observed via empirical study that low-resolution depth map in fact help capture the  first-order variational loss among distant neighbors. Accordingly, we downsample the  instead of upsamping .  We  downsample  denoted as  by random pooling operation, i.e., we randomly select a location where we have a valid measurement since ground-truth data may have pixels with no depth values. The coordinates of selected location in  and the corresponding depth value is put in   via bi-linear interpolation. We compute the variational loss as 

where  is the number of positions having valid measurements,  symbolises the first-order difference operator, and  refers to the convolutional layer. Here, we use the  layer to fuse  depth maps into a single depth map and also to compute its horizontal and vertical gradient.


\formattedparagraph{Total Loss.} We define the total loss as the sum of the depth loss and the variational loss i.e.,  , where  is the regularization parameter set to 0.1 for all our experiments.






\section{Experiments and Results}


\formattedparagraph{Implementation Details}
We implemented our method in PyTorch 1.7.1 (Python 3.8) with CUDA 11.0. The software is  evaluated on a computing machine with Quadro-RTX-6000 GPU.


\formattedparagraph{Datasets.} We performed experiments on three benchmark datasets namely NYU Depth V2 \citep{silberman2012indoor}, KITTI \citep{geiger2012we}, and SUN RGB-D \citep{song2015sun}. \textbf{\textit{(a)} NYU Depth V2} contains images with  resolution with depth values ranging from 0 to 10 meters.
We follow the train and test set split from ~\citet{lee2019big}, which contains 24,231 train images and 654 test images. \textbf{\textit{(b)} KITTI} contains images with  resolution where depth values range from 0 to 80 meters. The official split provides 42,949 train, 1,000 validation, and 500 test images. \citet{eigen2014depth} provides another train and test set split for this dataset which has 23,488 train and 697 test images. \textbf{\textit{(c)} SUN RGB-D} We preprocess its images to  resolution for consistency. The depth values range from 0 to 10 meters. We use the official test set (5050 images) for evaluation.



\begin{table}
\begin{center}
\scriptsize
\caption{\small Comparison with the state-of-the-art methods on the NYU test set~\citep{silberman2012indoor}. Please refer to Sec.\ref{sec:exp_sota} for details. }
\label{tab:nyu_sota}
\setlength\tabcolsep{5.5pt}
\begin{tabular*}{1.0\textwidth}{l@{\extracolsep{\fill}}ccccccc}
\hline
Method & Backbone & SILog  & Abs Rel & RMS & RMS log &    &  \\
\hline
GeoNet~\citep{qi2018geonet} & ResNet-50  & - & 0.128 & 0.569 & - & 0.834 & 0.960 \\
DORN~\citep{fu2018deep} & ResNet-101 & - & 0.115 & 0.509 & - & 0.828 & 0.965 \\
VNL~\citep{yin2019enforcing} & ResNeXt-101 &  - & 0.108 & 0.416 & - & 0.875 & 0.976 \\
TransDepth~\citep{yang2021transformer} & ViT-B  & - & 0.106 & 0.365 & - & 0.900 & 0.983 \\
ASN~\citep{long2021adaptive} & HRNet-48 & - & 0.101 & 0.377 & - & 0.890 & 0.982 \\
BTS~\citep{lee2019big} & DenseNet-161 & 11.533 & 0.110 & 0.392 & 0.142 & 0.885 & 0.978 \\
DPT-Hybird~\citep{ranftl2021vision} & ViT-B & - & 0.110 & 0.357 & - & 0.904 & 0.988 \\
AdaBins~\citep{bhat2021adabins} & EffNet-B5+ViT-mini &  10.570 & 0.103 & 0.364 & 0.131 & 0.903 & 0.983 \\
ASTrans~\citep{chang2021transformer} & ViT-B & 10.429 & 0.103 & 0.374 & 0.132 & 0.902 & 0.985 \\
NeWCRFs~\citep{yuan2022new} & Swin-L & 9.102 & 0.095 & 0.331 & 0.119 & 0.922 & \textbf{0.992} \\
\hline
\textbf{Ours} & Swin-L & \textbf{8.198}  & \textbf{0.086} & \textbf{0.304} & \textbf{0.108} & \textbf{0.937} & \textbf{0.992} \\
\textbf{\% Improvement} & &  \textcolor{ao}{{-9.93\%}} & \textcolor{ao}{{-9.47\%}} & \textcolor{ao}{{-8.16\%}} & \textcolor{ao}{{-9.24\%}} & \textcolor{ao}{{+1.63\%}} & \textcolor{ao}{{+0.00\%}}\\
\hline
\end{tabular*}
\end{center}
\end{table}



\begin{table}
\begin{center}
\scriptsize
\caption{\small Comparison with the state-of-the-art methods on the the KITTI official test set~\citep{geiger2012we}. We only list the results from the published methods. Please refer to Sec.\ref{sec:exp_sota} for details. }
\label{tab:kitti_off_sota}
\begin{tabular*}{1.0\textwidth}{l@{\extracolsep{\fill}}ccccc}
\hline
Method & Backbone & SILog & Abs Rel & Sq Rel & iRMS\\
\hline
DLE~\citep{liu2021deep} & ResNet-34 & 11.81 &   9.09 &  2.22 & 12.49\\
DORN~\citep{fu2018deep} &ResNet-101  & 11.80 &  8.93  &  2.19 & 13.22 \\
BTS~\citep{lee2019big} & DenseNet-161&  11.67 & 9.04 &  2.21 & 12.23 \\
BANet~\citep{aich2021bidirectional} & DenseNet-161 &  11.55 &  9.34 &  2.31 & 12.17 \\
PWA~\citep{lee2021patch} & ResNeXt-101 &  11.45 &   9.05 &  2.30 & 12.32 \\
ViP-DeepLab~\citep{qiao2021vip} &- & 10.80 &   8.94 &  2.19 & 11.77 \\
NeWCRFs~\citep{yuan2022new} & Swin-L & 10.39 & 8.37 & 1.83 & 11.03 \\
\hline
\textbf{Ours} & Swin-L &\textbf{9.84} &  \textbf{7.96} &  \textbf{1.66} & \textbf{10.44} \\
\textbf{\% Improvement} &  & \textcolor{ao}{{-5.29\%}} &  \textcolor{ao}{{-4.90\%}} &  \textcolor{ao}{{-9.29\%}} & \textcolor{ao}{{-5.35\%}} \\
\hline
\end{tabular*}
\end{center}
\end{table}

\formattedparagraph{Training Details.} We use \citep{liu2021swin} network as our backbone, which is pre-trained on ImageNet~\citep{deng2009imagenet}.
We use the Adam optimizer~\citep{kingma2014adam} without weight decay. 
We decrease the learning rate from  to  by the cosine annealing scheduler. To avoid over-fitting, we augment the images by horizontal flipping. For KITTI \citep{geiger2012we}, the model is trained for 10 epochs for the official split and 20 epochs for the Eigen split \citep{eigen2014depth}. For NYU Depth V2~\citep{silberman2012indoor}, the model is trained for 20 epochs.

\formattedparagraph{Evaluation Metrics.} We report statistical results on popular evaluation metrics such as square root of the Scale Invariant Logarithmic error (\textbf{SILog}), Relative Squared error (\textbf{Sq Rel}), Relative Absolute Error (\textbf{Abs Rel}), Root Mean Squared error (\textbf{RMS}), and threshold accuracy. Mathematical definition related to each one of them is provided in the Appendix.

\subsection{Comparison to State of the Art}
\label{sec:exp_sota}



Tab.(\ref{tab:nyu_sota}), Tab.(\ref{tab:kitti_off_sota}), Tab.(\ref{tab:eigen_sota}), and Tab.(\ref{tab:sun_sota}) provide statistical comparison results with the competing methods on NYU Depth V2, KITTI official split, KITTI Eigen split, and SUN RGB-D, respectively. Our proposed approach shows the best results for all the evaluation metrics. Particularly on the NYU test set, we reduce the SILog error from the previous best result, 9.102 to 8.198, and increase  from 0.922 to 0.937.  More qualitative results including V-layer output are presented in the Appendix.




For the SUN RGB-D test set, all competing models, including ours, are trained on the NYU Depth V2 training set \citep{silberman2012indoor} {\em without} fine-tuning on the SUN RGB-D. In addition, we align the predictions from all the models with the ground truth by a scale and shift following ~\citet{Ranftl2020}. Tab.(\ref{tab:sun_sota}) results show
our method's better generalization capability than other approaches. Extensive visual results are provided in the Appendix and supplementary video.



\begin{table}
\begin{center}
\scriptsize
\caption{\small Comparison with the state-of-the-art methods on the KITTI Eigen test set~\citep{eigen2014depth}. }
\label{tab:eigen_sota}
\setlength\tabcolsep{5.5pt}
\begin{tabular*}{1.0\textwidth}{l@{\extracolsep{\fill}}cccccccc}
\hline
Method & Backbone& SILog  & Abs Rel & RMS & RMS log &    &   \\
\hline
DORN~\citep{fu2018deep} &ResNet-101 & - & 0.072 & 0.273 & 0.120 & 0.932 & 0.984 \\
VNL~\citep{yin2019enforcing} & ResNeXt-101& - & 0.072 & 0.326 & 0.117 & 0.938 & 0.990 \\
TransDepth~\citep{yang2021transformer} & ViT-B & 8.930 & 0.064 & 0.275 & 0.098 & 0.956 & 0.994 \\
BTS~\citep{lee2019big} &DenseNet-161 & 8.933 & 0.060 & 0.280 & 0.096 & 0.955 & 0.993 \\
DPT-Hybird~\citep{ranftl2021vision} & ViT-B& - & 0.062 & 0.257 & - & 0.959 & 0.995 \\
AdaBins~\citep{bhat2021adabins} &EffNet-B5+ViT-mini & 8.022 & 0.058 & 0.236 & 0.089 & 0.964 & 0.995 \\
ASTrans~\citep{chang2021transformer} & ViT-B & 7.897 & 0.058 & 0.269 & 0.089 & 0.963 & 0.995 \\
NeWCRFs~\citep{yuan2022new} & Swin-L & 6.986 & 0.052 & 0.213 & 0.079 & 0.974 & \textbf{0.997} \\
\hline
\textbf{Ours} & Swin-L& \textbf{6.817}  & \textbf{0.050} & \textbf{0.209} & \textbf{0.076} & \textbf{0.977} & \textbf{0.997}\\
\textbf{\% Improvement} & & \textcolor{ao}{-2.42\%} & \textcolor{ao}{-3.85\%} & \textcolor{ao}{-1.88\%} & \textcolor{ao}{-3.80\%} & \textcolor{ao}{+0.03\%} & \textcolor{ao}{+0.00\%} \\
\hline
\end{tabular*}
\end{center}
\end{table}

\begin{table}
\begin{center}
\scriptsize
\caption{\small Comparison with AdaBins and NeWCRFs on SUN RGB-D test set. All methods are trained on NYU Depth V2 train set {without} fine-tuning on SUN RGB-D. }
\label{tab:sun_sota}
\begin{tabular*}{1.0\textwidth}{l@{\extracolsep{\fill}}cccccccc}
\hline
Method & Backbone &SILog  & Abs Rel & RMS & RMS log &    &  \\
\hline
AdaBins\citep{bhat2021adabins} & EffNet-B5+ViT-mini & 13.652 & 0.110 & 0.321 & 0.137 & 0.906 & 0.982\\
NeWCRFs~\citep{yuan2022new} & Swin-L & 13.695 & 0.105 & 0.322 & 0.138 & 0.920 & 0.980\\
\hline
\textbf{Ours} &Swin-L & \textbf{12.596}  & \textbf{0.094} & \textbf{0.299} & \textbf{0.127} & \textbf{0.929} & \textbf{0.983}\\
\textbf{\% Improvement} & &\textcolor{ao}{-7.73\%} & \textcolor{ao}{-10.48\%} & \textcolor{ao}{-6.85\%} & \textcolor{ao}{-7.30\%} & \textcolor{ao}{+0.98\%} & \textcolor{ao}{+0.10\%} \\
\hline
\end{tabular*}
\end{center}
\end{table}






\begin{table}
\begin{center}
\scriptsize
\caption{\small \textbf{Benefit of V-layer.} We replace the proposed V-layer with a single convolutional layer and a self-attention layer, and evaluate the accuracy of depth map predicted with and without subsequent refinements. }
\label{tab:ablation_vlayer}
\begin{tabular*}{1.0\textwidth}{l@{\extracolsep{\fill}}cccccccc}
\hline
Layer & Refine & SILog  & Abs Rel & RMS & RMS log  &   &  \\
\hline
\multirow{2}{*}{Convolution}& w/o & 8.830 & 0.090 & 0.325 & 0.114 & 0.927 & 0.990 \\
 & w/ & 8.688 & 0.089 & 0.317 & 0.113 & 0.928 & 0.991\\

\hline
\multirow{2}{*}{Self-Attention + PE} & w/o & 8.790 & 0.090 & 0.318 & 0.114 & 0.927 & 0.990\\
 & w/ & 8.595 & 0.089 & 0.316 & 0.112 & 0.929 & 0.991\\
\hline
\multirow{2}{*}{\textbf{V-Layer}}& w/o & 8.422 & 0.087 & 0.308 & 0.110 & 0.936 & 0.990\\
 & \textbf{w/} & \textbf{8.198} & \textbf{0.086} & \textbf{0.304} & \textbf{0.108} & \textbf{0.937} & \textbf{0.992}\\
\hline
\end{tabular*}
\end{center}
\end{table}

\subsection{Ablation Study} \label{sec:exp_aba}
All the ablation presented below is conducted on NYU Depth V2 test set \citep{silberman2012indoor}. 


\formattedparagraph{\textit{(i)} Effect of V-Layer.} To understand the benefit and outcome of our variational layer compared to other popular alternative layers in deep neural networks for this problem, we performed this ablation study. We replace our V-layer firstly with a convolutional layer and later with a self-attention layer. Tab.(\ref{tab:ablation_vlayer}) provides the depth prediction accuracy for this ablation. For each introduced layer in Tab.(\ref{tab:ablation_vlayer}), the first and second rows show the performance of the depth map predicted {\em with} (w) and {\em without} (w/o) subsequent refinements (cf. Sec.\ref{sec:network} (c)), respectively. For the self-attention layer, we follow the ViT \citep{dosovitskiy2020image} and set the patch size to be one as we use the feature map with stride 16. We also adopt the learnable position embedding (PE) with 128 dimensions. We set the number of heads to be 4 and the number of hidden units to be 512. As shown in Tab.(\ref{tab:ablation_vlayer}), our V-layer indeed helps improve the accuracy of depth prediction compared to other well-known layers. More experiments on KITTI and SUN RBG-D are provided in the Appendix.



\formattedparagraph{\textit{(ii)} Performance with Different Network Backbone.} We evaluate the effects of our V-layer with different types of network backbones. For this ablation, we use Swin-Large \citep{liu2021swin}, Swin-Small \citep{liu2021swin}, and ConvNeXt-Small \citep{liu2022convnet}. The SILog error is shown in Fig. \ref{fig:ablation_bone}. The results show that our V-layer improves the transformer and the convolutional network performance. An important observation is that our V-layer shows excellent improvements in depth prediction accuracy on weaker network backbones.





\formattedparagraph{\textit{(iii)} Performance with Change in the Value of .} For this ablation, we change the value of  in the V-layer and observe its effects (cf. Sec.\ref{sec:network} (b)). By increasing , we generate more channels of  and  which in-effect increases V-layer parameters. In the subsequent step, we expand the number of channels to 128 by a convolutional layer to use the  subsequent layers as they are. The results are shown in Tab.(\ref{tab:dni_channel}). For reference, we also present the result by replacing the V-layer with a convolutional layer in the first row in Tab.(\ref{tab:dni_channel}). By increasing , we reduce the SILog error, at the price of the speed (FPS). Yet, no real benefit is observed with  more than 16. 











\begin{wrapfigure}{rt}{0.34\textwidth}
\vspace{-20pt}
  \begin{center}
    \includegraphics[width=0.33\textwidth]{exp/abg_2.pdf}
    \vspace{-10pt}
    \caption{\scriptsize Evaluation on Swin-L, Swin-S, ConvNeXt-S \textbf{w/} and \textbf{w/o} the V-layer. }
    \label{fig:ablation_bone}
  \end{center}
  \vspace{-20pt}
\end{wrapfigure} 







\formattedparagraph{\textit{(iv)} Effect of Confidence Weight Matrix \& Difference Operator in V-Layer.} For this ablation, we study the network's depth prediction under four different settings. (a) without V-layer and replace it with convolutional layer (b) without the confidence weight matrix (c) with learnable difference operator and (d) our full model. The depth prediction accuracy observed under these settings is provided in Tab.(\ref{tab:dni_component}). Clearly, our full model has better accuracy. An important empirical observation, we made during this test is when we keep  learnable V-layer has more learnable parameters, the performance becomes worse than with fixed difference operator.



\begin{table}[!htb]
\scriptsize
\begin{minipage}{.47\linewidth}
\centering
\caption{\small Analysis of the number of feature groups. More groups reduce the SILog error.}\label{tab:dni_channel}
\resizebox{\linewidth}{!}{
\begin{tabular}
{lcccc}
\hline
& SILog & Abs Rel & RMS & FPS  \\
\hline
w/o V-layer &  8.688 & 0.089 & 0.317 & \textbf{9.343}  \\
1 &  8.456 & 0.088 & 0.310 & 8.175 \\
\textbf{16} & 8.198 & 0.086 & \textbf{0.304} & 7.032 \\
128 & \textbf{8.172} & \textbf{0.085} & 0.309 & 3.320\\
\hline
\end{tabular}
}
\end{minipage}\hspace{10pt}
\begin{minipage}{.47\linewidth}
\centering
\caption{\small Analysis of the confidence weight matrix  and the difference operator .}\label{tab:dni_component}
\scriptsize
\resizebox{\linewidth}{!}{
\begin{tabular}
{lccc}
\hline
 & SILog & Abs Rel & RMS  \\
\hline
(a) w/o V-layer&  8.688 & 0.089 & 0.317\\
(b) w/o  & 8.537 & 0.089 & 0.316 \\
(c) learnable  & 8.355 & 0.088 & 0.310 \\
(d) \textbf{full} & \textbf{8.198} & \textbf{0.086} & \textbf{0.304} \\
\hline
\end{tabular}
}
\end{minipage} 
\end{table}

\subsection{Network processing time \& Parameters}
We compared our method's inference time and the number of model parameters to the AdaBins~\citep{bhat2021adabins} and the NeWCRFs~\citep{yuan2022new}.
The inference time is measured on the NYU Depth V2 test set with batch size 1.  We have removed the ensemble tricks in AdaBins and NeWCRFs for an unbiased evaluation, resulting in a slight increase in SILog error as compared to Tab.(\ref{tab:nyu_sota}) statistics. As is shown in Tab.(\ref{tab:time}), our method is faster and better than AdaBins and NeWCRFs using Swin-Small backbone. With the same backbone as the NeWCRFs, i.e., Swin-Large, we achieve much better depth prediction results. Hence, our method with Swin-Small backbone provides a better balance between accuracy, speed and memory foot-print. 

\begin{table}[h]
\centering
\scriptsize
\caption{\small Comparison of the inference time and parameters to AdaBins and NeWCRFs on NYU Depth V2. We show our results using the Swin-Small and Swin-Large backbone.}
\begin{tabular}{c|cccc}
    \hline
        \multirow{2}{*}{} &  AdaBins
         & NeWCRFs & Ours & Ours \\
        & ~\citep{bhat2021adabins} & ~\citep{yuan2022new} & (Small)& (Large)\\
        \hline
        SILog Error  &  10.651 & 9.171 & \textcolor{blue}{\textbf{9.069}} & \textbf{8.198} \\
        \hline
        Speed (FPS)  & 5.638 & 10.551 & \textbf{11.891} & 7.032\\
        \hline
        Param (M)  &  \textbf{75} & 258 & \textcolor{blue}{\textbf{76}} & 249\\
    \hline
\end{tabular}\label{tab:time}
\end{table}

\section{Conclusion}
In conclusion, a simple and effective approach for inferring scene depth from a single image is introduced. The proposed SIDP approach is shown to better exploit the rigid scene prior, which is generally overlooked by the existing neural network-based methods. Our approach does not make explicit assumptions about the scene other than the scene gradient regularity, which holds for typical indoor or outdoor scenes. When tested on popular benchmark datasets, our method shows significantly better results than the prior art, both qualitatively and quantitatively.


\section{Acknowledgment}
This work was partly supported by ETH General Fund (OK), Chinese Scholarship Council (CSC), and The Alexander von Humboldt Foundation.











\bibliography{iclr2023_conference}
\bibliographystyle{iclr2023_conference}

\appendix
\section{Appendix}
\subsection{Training Details}
We implement our framework in PyTorch~\citep{paszke2019pytorch}. We adopt the Swin-Large~\citep{liu2021swin} as our backbone to conduct ablation experiments and compare with the state of the arts. And the backbone is pre-trained on ImageNet-22K~\citep{deng2009imagenet} by image classification. For training, we use the Adam optimizer~\citep{kingma2014adam} without weight decay. We decrease the learning rate from  to  by the cosine annealing scheduler. To avoid over-fitting, we augment the images by horizontal flipping. For both train and test, we keep the resolution of images to be  in KITTI~\citep{geiger2012we}, and  in both NYU Depth V2~\citep{silberman2012indoor} and SUN RGB-D~\citep{song2015sun}. In KITTI, the model is trained for 10 epochs for the official split and 20 epochs for the Eigen split~\citep{eigen2014depth}. In NYU Depth V2, the model is trained for 20 epochs. We set the batch size to be 4 and 8 respectively for ablation experiments and comparison to the state of the arts. 
\subsection{Evaluation Metrics}
\label{sec:evaluation_metrics}
Suppose the predicted and ground-truth depth to be  and , respectively, and the number of valid pixels to be . 
We follow the existing methods~\citep{yuan2022new} and utilize the following measures for quantitative evaluation: 
\begin{itemize}
\item square root of the Scale Invariant Logarithmic error (\textbf{SILog}): ,  where ; 
\item Relative Squared error (\textbf{Sq Rel}): ; 
\item Relative Absolute Error (\textbf{Abs Rel}): ; 
\item Root Mean Squared error (\textbf{RMS}): ; 
\item Root Mean Squared Logarithmic error (\textbf{RMS log}): ; 
\item threshold accuracy (): percentage of  s.t. .
\end{itemize}

\subsection{Derivation of the Variational Constraint}
In this part, we introduce the derivation of the variational constraint in more detail. Firstly, we present the detailed form of the difference operator , the confidence weight matrix , the difference vector  and the depth vector , when the depth map  is at resolution . Secondly we show the derivation of the optimal solution .  

\formattedparagraph{Overall Matrix Form.}
Suppose the unscaled depth map to be , the difference map to be  and , and the corresponding confidence weight map to be  and . We first reorganize the elements in , , ,  to construct the difference vector  and the confidence weight matrix : 


Next, we apply the difference operator :

on the depth vector :

Finally, we construct the following constraint equation:

In the above equation, each row represents a weighted constraint for the first-order difference along the -axis:

or along the -axis:

where  and  are the variational constraints for the first-order difference, while  and  are the confidence weights for the constraints. 

\formattedparagraph{Optimal Solution.}
Given  and , we search for the optimal depth vector  by minimizing the residual of Eq.(\ref{eq:app_matrix}):

Firstly, the objective function, , is convex with respect to . Because  is a convex function. A composition of  and the affine function , , is still convex. 

Secondly, the optimal solution of a convex function can be found at where the first derivative is zero. Thereby we obtain the final solution .
\subsection{Network Structure Details}
In this part, we introduce the detailed structure of our network. In general we set the kernel size of convolutional layers to be 3 unless otherwise stated.

\formattedparagraph{(a) Encoder.} We adopt the Swin-Large \citep{liu2021swin} as our backbone. The network first divides the image into patches each of size , and embeds each patch into a -dimensional vector. The above procedure is implemented by a convolutional layer with kernel size 4 and stride 4. Then there are 4  transformation stages to be applied to the embedded vector. The first, second, third and forth stages include  blocks, respectively. More specifically, each block will divide the feature map into non-overlapped windows of size  , and compute new features within each window following the transformer \citep{vaswani2017attention}. The feature channels in the 4 stages are 192, 384, 768 and 1536, respectively, and the number of heads are 6, 12, 24, and 48, respectively. In the end of each stage, there will be a downsampling operation to reduce the resolution of the feature map by 2. We collect the feature map before the downsampling operation as the output of the stage. In the end, the strides of output feature maps are 4, 8, 16, and 32 respectively. And the channels are 192, 384, 768, and 1536, respectively.

\formattedparagraph{(b) Upsampling 32->16.} Given the feature maps of strides 32 and 16 from \textbf{(a)}, we first upsample the feature map of stride 32 to stride 16 via bi-linear interpolation. Then, we concatenate the feature maps and obtain a new feature map of stride 16 and channels . We apply a convolutional layer that has 2304 input channels, 2304 output channels, and 4 groups, to fuse the information. We also append an instance normalization layer \citep{ulyanov2016instance} and a LeakyReLU activation function. Next, we apply another convolutional layer with  2304 input channels, 512 output channels to compress the feature channels. Again we append an instance normalization layer and the LeakyReLU activation function. In the end, we add a skip connection between the result feature map and the previous concatenated feature map by addition operation. The concatenated feature map will be transformed to consistent number of channels in advance by a convolutional layer with 2304 input channels and 512 output channels. The final feature map has stride 16 and 512 channels.

\formattedparagraph{(c) V-Layer.} The V-layer takes the output feature map from \textbf{(b)} as input. We first utilize a convolutional layer with 512 input channels and 512 output channels to transform the feature map into a more appropriate hidden space. The feature is transformed by the LeakyReLU activation function. Then we utilize another convolutional layer with 512 input channels and  output channels to predict the gradients along the horizontal and vertical axis, respectively. The number  represents that we predict  channels of horizontal gradients and  channels of vertical gradients respectively, where each channel is expected to capture different information in scenes. Similarly, we also use another convolutional layer with 512 input channels,  output channels to predict the corresponding confidence weight maps. The confidence weight will be transformed by the Sigmoid function. Then we reshape the predictions and compute the unscaled depth map for each pair of gradient and confidence weight following Eq.(\ref{eq:analytic}). The unscaled depth maps computed from all the pairs will be concatenated along the channel. Thereby we obtain a depth map with 16 channels and stride 16. We apply a group normalization on the depth map, where the 16 channels are viewed as a single group. In the end, we expand the channels of the depth map into 128 by a convolutional layer. Thereby the output of the V-layer is a depth map with 128 channels and stride 16.

\formattedparagraph{(d) Refine 16.}
We refine the feature map from \textbf{(b)} and the depth map from \textbf{(c)}. Specifically, we first concatenate the feature map with depth map, and obtain a new feature map with  channels. Then we employ a convolutional layer with 640 input channels and 640 output channels to fuse the information. The result feature map will be further transformed by the LeakyReLU activation function. Next a convolutional layer with 640 input channels and 128 output channels is applied to predict the refined depth map. Similarly, we also predict the refined feature map by a convolutional layer with 640 input channels and 512 output channels. 

\formattedparagraph{(e) Upsampling 16->8.} Here, we upsample the feature map from \textbf{(d)} to stride 8 via bi-linear interpolation. To recover the information of scenes, we concatenate with the feature map from the encoder that has the same stride. In such a way, we obtain a new feature map with  channels and stride 8. Similar to \textbf{(b)}, we apply a convolutional layer with 896 input channels, 896 output channels, and 4 groups to fuse the information. Then the feature map is transformed by the instance normalization layer and the LeakyReLU activation function. Next, we apply another convolutional layer with 896 input channels and 256 output channels to compress feature channels. Again, we apply the instance normalization layer and LeakyReLU activation function. Same as \textbf{(b)}, we add a skip connection between the result feature map and the previous concatenated feature map by addition operation. The concatenated feature map will be transformed to the consistent number of channels by a convolutional layer with 896 input channels and 256 output channels. Thereby the final feature map has stride 8 and 256 channels.

\formattedparagraph{(f) Refine 8.} We refine the depth map from \textbf{(d)} and the feature map from \textbf{(e)}. The procedure is the same as \textbf{(d)}, except for the number of channels in convolutional layers are adapted accordingly. Thereby we introduce the pipeline briefly. We first upsample the depth map from \textbf{(d)} into stride 8 to concatenate with the feature map. Thereby we obtain a new feature map with  channels. Then, we apply a convolutional layer with 384 input channels and 384 output channels to fuse the information. The feature map is then transformed by LeakyReLU activation function. Next, we utilize a convolutional layer with 384 input channels and 128 output channels to predict the refined depth map. Similarly, another convolutional layer with 384 input channels and  256 output channels is applied to predict the refined feature map.

\formattedparagraph{(g) Upsampling 8->4.} We upsample the feature map from \textbf{(f)} to stride 4. The procedure is the same as \textbf{(e)} except for the number of channels. More specifically, we first upsample the feature map and concatenate with the feature map from the encoder with consistent stride, obtaining a feature map with  channels and stride 4. Then a convolutional layer with 448 input channels, 448 output channels, and 4 groups is applied to fuse the information. Then the result feature map is transformed by the instance normalization and LeakyReLU activation function. Next, we apply another convolutional layer with 448 input channels and 64 output channels to compress the feature channels. Again, the instance normalization layer and LeakyReLU function is applied. In the end, we add a skip connection between the final feature map and the previous concatenated feature map.

\formattedparagraph{(h) Refine 4.} We refine the depth map from \textbf{(f)} and the feature map from \textbf{(g)} following the same pipeline as \textbf{(d)}. We first upsample the depth map to stride 4, and concatenate with the feature map, obtaining a new feature map with  channels and stride 4. Next, we apply a convolutional layer with 192 input channels and 192 output channels to fuse the information, and a LeakyReLU function to transform the feature map. In the end, we utilize another convolutional layer with 192 input channels and 128 output channels to predict the new depth map. The output is a depth map with 128 channels and stride 4.

\formattedparagraph{(i) Metric Layer.} We take the feature map with stride 32 from the encoder as input. We first apply global max pooling to compress the feature map into a vector with 1536 channels. Then we apply a fully-connected layer with 1536 input units and 384 output units to compress the channels. Next we apply a LeakyReLU function. In the end, we utilize another fully-connected layer with 384 input units and 2 output units to regress the scale and shift.


\formattedparagraph{(j) Final Prediction.} We take the output from \textbf{(d)},\textbf{(f)},  \textbf{(h)} and \textbf{(i)} as input. We upsample all the depth maps to stride 1 via bi-linear interpolation, and fuse the depth maps into a single depth map by the addition operation and a convolutional layer with 128 input channels and 1 output channel. In the end, we add the depth map with the shift and multiply with the scale from \textbf{(i)}, respectively.


\subsection{Statistical Evaluation With Other Methods}
In this section, we compare with
\citet{ramamonjisoa2020predicting}, \citet{cheng2018depth}, and \citet{li2017two} on NYU Depth V2 \citep{silberman2012indoor}. More specifically, we apply their methods to refine the final predicted depth map. The displacement field, the affinity matrix, and the depth gradient are predicted from the refined feature map, which is the output of the last refinement module. For \citet{cheng2018depth} we adopt their default hyper-parameters. The kernel size is 3, and the number of iterations is 24. \citet{li2017two} proposes the end-to-end fusion strategy and the optimization-based fusion strategy. For the end-to-end fusion strategy, we employ three convolutional layers with kernel size 5, and 16 channels. For the optimization-based fusion strategy, we use Adam optimizer. The learning rate is  and the number of iterations is 100. The results are shown in Tab. (\ref{tab:rebuttal_com}). Our method achieves better performance.

\begin{table}
\begin{center}
\scriptsize
\caption{\small Comparison with \citet{ramamonjisoa2020predicting}, \citet{cheng2018depth} and \citet{li2017two} on NYU Depth V2 test set \citep{silberman2012indoor}. }
\label{tab:rebuttal_com}
\begin{tabular*}{1.0\textwidth}{l@{\extracolsep{\fill}}ccccccc}
\hline
Method & SILog  & Abs Rel & RMS & RMS log &    &  \\
\hline
\citet{ramamonjisoa2020predicting} & 8.655 & 0.088 & 0.320 & 0.113 & 0.929 & 0.991\\
\citet{cheng2018depth} & 8.640 & 0.089 & 0.317 & 0.112 & 0.930 & 0.991\\
\citet{li2017two} (end-to-end) & 8.557 & 0.088 & 0.315 & 0.112 & 0.931 & 0.991\\
\citet{li2017two} (optimization) & 8.574 & 0.089 & 0.316 & 0.112 & 0.930 & 0.991\\
\hline
\textbf{Ours} & \textbf{8.198}  & \textbf{0.086} & \textbf{0.304} &\textbf{0.108} & \textbf{0.937} & \textbf{0.992}\\
\hline
\end{tabular*}
\end{center}
\end{table}


\subsection{Ablation on Resolution}
In this part, we evaluate the performance of the V-layer when applying on the feature map with stride 8. We achieve this by dividing the feature map into 4 rectangle sub-regions with equal area (top left, top right, bottom left and bottom right). We apply a V-layer on each sub-region. The predicted depth maps from the sub-regions will be stacked to recover the original resolution. The results are shown in Tab.(\ref{tab:rebuttal_res}). From stride 16 to 8, the improvement of accuracy is marginal. However, the inference speed (FPS) is significantly slow down.
\begin{table}
\begin{center}
\scriptsize
\caption{\small \textbf{Impact of resolution}. We evaluate the performance of V-layer when operating on feature maps of stride 16 and 8, respectively.}
\label{tab:rebuttal_res}
\begin{tabular*}{1.0\textwidth}{l@{\extracolsep{\fill}}cccccccc}
\hline
Stride & FPS & SILog  & Abs Rel & RMS & RMS log &    &  \\
\hline
16 & \textbf{7.032} & 8.198  & \textbf{0.086} & \textbf{0.304} &\textbf{0.108} & \textbf{0.937} & \textbf{0.992}\\
8 &2.597 & \textbf{8.149} & \textbf{0.086} & 0.306 & \textbf{0.108} & 0.936 & \textbf{0.992}\\
\hline
\end{tabular*}
\end{center}
\end{table}


\subsection{Ablation on KITTI and SUN RGB-D}
To further demonstrate the benefits of our V-layer on KITTI Eigen split~\citep{geiger2012we, eigen2014depth} and SUN RGB-D test set~\citep{song2015sun}, we present the accuracy of the predicted depth map when replacing the V-layer with a single convolutional layer. As shown in Tab.(\ref{tab:app_abl}), our V-layer can improve the depth map accuracy on both KITTI and SUN RGB-D. 
\begin{table}[hbt]
\begin{center}
\scriptsize
\caption{\small \textbf{Benefit of V-layer.} We replace the proposed V-layer with a single convolutional layer, and evaluate the predicted depth map accuracy.}
\label{tab:app_abl}
\begin{tabular*}{1.0\textwidth}{l@{\extracolsep{\fill}}cccccccc}
\hline
Dataset & Layer & SILog  & Abs Rel & RMS & RMS log &    &  \\
\hline
\multirow{2}{*}{KITTI} & Convolution & 6.996 & 0.052 & 0.217 & 0.079 & 0.975 & \textbf{0.997}\\
& \textbf{V-Layer} & \textbf{6.817}  & \textbf{0.050} & \textbf{0.209} & \textbf{0.076} & \textbf{0.977} & \textbf{0.997}\\
\hline
\multirow{2}{*}{SUN} & Convolution & 13.172 & 0.098 & 0.309 & 0.132 & 0.924 & 0.981\\
& \textbf{V-Layer} & \textbf{12.596}  & \textbf{0.094} & \textbf{0.299} & \textbf{0.127} & \textbf{0.929} & \textbf{0.983}\\
\hline
\end{tabular*}
\end{center}
\end{table}


\begin{figure}[t]
\centering
\subfigure[Image]{
\begin{minipage}[b]{0.228\textwidth}
\includegraphics[width=1\textwidth]{intermediate/bathroom_rgb_00045.pdf}\\
\includegraphics[width=1\textwidth]{intermediate/bathroom_rgb_00508.pdf}\\
\includegraphics[width=1\textwidth]{intermediate/bathroom_rgb_00509.pdf}\\
\includegraphics[width=1\textwidth]{intermediate/bathroom_rgb_00510.pdf}\\
\end{minipage}
}
\subfigure[Confidence Weight]{
\begin{minipage}[b]{0.23\textwidth}
\includegraphics[width=1\textwidth]{intermediate/0att.pdf}\\
\includegraphics[width=1\textwidth]{intermediate/3att.pdf}\\
\includegraphics[width=1\textwidth]{intermediate/4att.pdf}\\
\includegraphics[width=1\textwidth]{intermediate/5att.pdf}\\
\end{minipage}
}
\subfigure[Difference Map]{
\begin{minipage}[b]{0.23\textwidth}
\includegraphics[width=1\textwidth]{intermediate/0grad.pdf}\\
\includegraphics[width=1\textwidth]{intermediate/3grad.pdf}\\
\includegraphics[width=1\textwidth]{intermediate/4grad.pdf}\\
\includegraphics[width=1\textwidth]{intermediate/5grad.pdf}\\
\end{minipage}
}
\subfigure[Depth Map]{
\begin{minipage}[b]{0.23\textwidth}
\includegraphics[width=1\textwidth]{intermediate/0x.pdf}\\
\includegraphics[width=1\textwidth]{intermediate/3x.pdf}\\
\includegraphics[width=1\textwidth]{intermediate/4x.pdf}\\
\includegraphics[width=1\textwidth]{intermediate/5x.pdf}\\
\end{minipage}
}
\caption{\textbf{Visualization of V-layer Prediction.} We visualize the confidence weight , the difference map  and the depth map  from the V-layer when predicting on NYU Depth V2 test set.}
\label{fig:vis_inter}
\end{figure}

\subsection{More Generalization Experiments}

We further evaluated the generalization performance when training on KITTI and test on NYU, or training on NYU and test on KITTI. The results are shown in Tab.(\ref{tab:rebuttal_kitti2nyu}) and Tab.(\ref{tab:rebuttal_nyu2kitti}). Our method achieves better generalization performance in both settings.
\begin{table}
\begin{center}
\scriptsize
\caption{\small Comparison with AdaBins and NeWCRFs on NYU test set. All methods are trained on KITTI Eigen train set {without} fine-tuning on NYU. }
\label{tab:rebuttal_kitti2nyu}
\begin{tabular*}{1.0\textwidth}{l@{\extracolsep{\fill}}cccccccc}
\hline
Method & Backbone &SILog  & Abs Rel & RMS & RMS log &    &  \\
\hline
AdaBins\citep{bhat2021adabins} & EffNet-B5+ViT-mini & 28.147 & 0.251 & 0.753 & 0.286 & 0.614 & 0.867\\
NeWCRFs~\citep{yuan2022new} & Swin-L & 21.138 & 0.173 & 0.551 & 0.213 & 0.755 & 0.934\\
\hline
\textbf{Ours} &Swin-L & \textbf{18.090}  & \textbf{0.148} & \textbf{0.474} & \textbf{0.182} & \textbf{0.804} & \textbf{0.955}\\
\hline
\end{tabular*}
\end{center}
\end{table}


\begin{table}[h]
\begin{center}
\scriptsize
\caption{\small Comparison with AdaBins and NeWCRFs on KITTI Eigen test set. All methods are trained on NYU Depth V2 train set {without} fine-tuning on KITTI Eigen. }
\label{tab:rebuttal_nyu2kitti}
\begin{tabular*}{1.0\textwidth}{l@{\extracolsep{\fill}}cccccccc}
\hline
Method & Backbone &SILog  & Abs Rel & RMS & RMS log &    &  \\
\hline
AdaBins\citep{bhat2021adabins} & EffNet-B5+ViT-mini & 56.871 & 0.350 & 7.221 & 0.579 & 0.434 & 0.744\\
NeWCRFs~\citep{yuan2022new} & Swin-L & 54.460 & 0.268 & 6.246 & 0.550 & 0.512 & 0.833\\
\hline
\textbf{Ours} &Swin-L & \textbf{42.105}  & \textbf{0.221} & \textbf{5.360} & \textbf{0.426} & \textbf{0.598} & \textbf{0.888}\\
\hline
\end{tabular*}
\end{center}
\end{table}



\subsection{Sparse Ground-Truth Depth Map}
KITTI provides sparse LiDAR measurements as the ground truth (official ground truth has been inpainted to an extent). Yet, our method could work well in such cases where sparse LiDAR measurements are known. Our algorithm computes the depth map from the gradient predicted by the network in a differentiable way. Thereby, for such a case, when we apply sparse supervision on the computed depth map (note that the loss function is used for valid measurements only), the error signal from the loss function will be back-propagated to the predicted gradient to supervise the network to learn gradient clues. We demonstrated the effectiveness of our algorithm in Tab.(\ref{tab:kitti_off_sota}) and Tab.(\ref{tab:eigen_sota}) for such cases.

\subsection{More Visualization}
We visualize the confidence weight map , the difference map , and the depth map  from the V-layer in Fig.\ref{fig:vis_inter}. We observe that the depth value of a pixel shows correlation with respect to the image coordinates of the pixel. For example, in the last example in Fig.\ref{fig:vis_inter}, for different pixels at the door, the depth values are usually different but the first order difference are approximately the same. This observation shows that the difference map might be easier to predict than the depth map. 



\subsection{Qualitative Results}
\label{sec:qual_eigen}
We provide more qualitative results on KITTI Eigen split~\citep{eigen2014depth}, SUN RGB-D~\citep{song2015sun}, and NYU Depth V2~\citep{silberman2012indoor} in Fig.\ref{fig:qual_kitti},  Fig.\ref{fig:qual_sun} and Fig.\ref{fig:qual_nyu}, respectively. Our framework predicts more accurate shapes and preserves the high-frequency scene information.  


\begin{figure}[t]
\centering
\subfigure[]{
\begin{minipage}[b]{0.3\textwidth}
\includegraphics[width=1\textwidth]{quality_eigen/image/2011_09_26_2011_09_26_drive_0029_sync_image_02_data_0000000154_crop_rectange.png}\\
\includegraphics[width=1\textwidth]{quality_eigen/adabins/2011_09_26_drive_0029_sync_0000000154_crop_rectange.png}\\
\includegraphics[width=1\textwidth]{quality_eigen/newcrfs/2011_09_26_drive_0029_sync_0000000154_crop_rectange.png}\\
\includegraphics[width=1\textwidth]{quality_eigen/dni/2011_09_26_drive_0029_sync_0000000154_crop_rectange.png}\\
\end{minipage}
}
\subfigure[]{
\begin{minipage}[b]{0.3\textwidth}
\includegraphics[width=1\textwidth]{quality_eigen/image/2011_09_26_2011_09_26_drive_0036_sync_image_02_data_0000000096_crop_rectange.png}\\
\includegraphics[width=1\textwidth]{quality_eigen/adabins/2011_09_26_drive_0036_sync_0000000096_crop_rectange.png}\\
\includegraphics[width=1\textwidth]{quality_eigen/newcrfs/2011_09_26_drive_0036_sync_0000000096_crop_rectange.png}\\
\includegraphics[width=1\textwidth]{quality_eigen/dni/2011_09_26_drive_0036_sync_0000000096_crop_rectange.png}\\
\end{minipage}
}
\subfigure[]{
\begin{minipage}[b]{0.3\textwidth}
\includegraphics[width=1\textwidth]{quality_eigen/image/2011_09_26_2011_09_26_drive_0084_sync_image_02_data_0000000140_crop_rectange.png}\\
\includegraphics[width=1\textwidth]{quality_eigen/adabins/2011_09_26_drive_0084_sync_0000000140_crop_rectange.png}\\
\includegraphics[width=1\textwidth]{quality_eigen/newcrfs/2011_09_26_drive_0084_sync_0000000140_crop_rectange.png}\\
\includegraphics[width=1\textwidth]{quality_eigen/dni/2011_09_26_drive_0084_sync_0000000140_crop_rectange.png}\\
\end{minipage}
}

\subfigure[]{
\begin{minipage}[b]{0.3\textwidth}
\includegraphics[width=1\textwidth]{quality_eigen/image/2011_09_26_2011_09_26_drive_0093_sync_image_02_data_0000000321_crop_rectange.png}\\
\includegraphics[width=1\textwidth]{quality_eigen/adabins/2011_09_26_drive_0093_sync_0000000321_crop_rectange.png}\\
\includegraphics[width=1\textwidth]{quality_eigen/newcrfs/2011_09_26_drive_0093_sync_0000000321_crop_rectange.png}\\
\includegraphics[width=1\textwidth]{quality_eigen/dni/2011_09_26_drive_0093_sync_0000000321_crop_rectange.png}\\
\end{minipage}
}
\subfigure[]{
\begin{minipage}[b]{0.3\textwidth}
\includegraphics[width=1\textwidth]{quality_eigen/image/2011_09_30_2011_09_30_drive_0027_sync_image_02_data_0000000410_crop_rectange.png}\\
\includegraphics[width=1\textwidth]{quality_eigen/adabins/2011_09_30_drive_0027_sync_0000000410_crop_rectange.png}\\
\includegraphics[width=1\textwidth]{quality_eigen/newcrfs/2011_09_30_drive_0027_sync_0000000410_crop_rectange.png}\\
\includegraphics[width=1\textwidth]{quality_eigen/dni/2011_09_30_drive_0027_sync_0000000410_crop_rectange.png}\\
\end{minipage}
}
\subfigure[]{
\begin{minipage}[b]{0.3\textwidth}
\includegraphics[width=1\textwidth]{quality_eigen/image/2011_09_26_2011_09_26_drive_0096_sync_image_02_data_0000000190_crop_rectange.png}\\
\includegraphics[width=1\textwidth]{quality_eigen/adabins/2011_09_26_drive_0096_sync_0000000190_crop_rectange.png}\\
\includegraphics[width=1\textwidth]{quality_eigen/newcrfs/2011_09_26_drive_0096_sync_0000000190_crop_rectange.png}\\
\includegraphics[width=1\textwidth]{quality_eigen/dni/2011_09_26_drive_0096_sync_0000000190_crop_rectange.png}\\
\end{minipage}
}

\caption{Qualitative comparison on KITTI Eigen split~\citep{eigen2014depth}. For each column, from top to bottom we present the input image, the prediction from AdaBins~\citep{bhat2021adabins}, NeWCRFs~\citep{yuan2022new}, and our framework respectively.}
\label{fig:qual_kitti}
\end{figure}


\begin{figure}[t]
\centering
\subfigure[Image]{
\begin{minipage}[b]{0.23\textwidth}
\includegraphics[width=1\textwidth]{quality_sun/image/1.jpg}\\
\includegraphics[width=1\textwidth]{quality_sun/image/3.jpg}\\
\includegraphics[width=1\textwidth]{quality_sun/image/5.jpg}\\
\includegraphics[width=1\textwidth]{quality_sun/image/6.jpg}\\
\includegraphics[width=1\textwidth]{quality_sun/image/7.jpg}\\
\includegraphics[width=1\textwidth]{quality_sun/image/8.jpg}\\
\includegraphics[width=1\textwidth]{quality_sun/image/10.jpg}\\
\end{minipage}
}
\subfigure[AdaBins]{
\begin{minipage}[b]{0.23\textwidth}
\includegraphics[width=1\textwidth]{quality_sun/adabins/1.png}\\
\includegraphics[width=1\textwidth]{quality_sun/adabins/3.png}\\
\includegraphics[width=1\textwidth]{quality_sun/adabins/5.png}\\
\includegraphics[width=1\textwidth]{quality_sun/adabins/6.png}\\
\includegraphics[width=1\textwidth]{quality_sun/adabins/7.png}\\
\includegraphics[width=1\textwidth]{quality_sun/adabins/8.png}\\
\includegraphics[width=1\textwidth]{quality_sun/adabins/10.png}\\
\end{minipage}
}
\subfigure[NeWCRFs]{
\begin{minipage}[b]{0.23\textwidth}
\includegraphics[width=1\textwidth]{quality_sun/newcrfs/1.png}\\
\includegraphics[width=1\textwidth]{quality_sun/newcrfs/3.png}\\
\includegraphics[width=1\textwidth]{quality_sun/newcrfs/5.png}\\
\includegraphics[width=1\textwidth]{quality_sun/newcrfs/6.png}\\
\includegraphics[width=1\textwidth]{quality_sun/newcrfs/7.png}\\
\includegraphics[width=1\textwidth]{quality_sun/newcrfs/8.png}\\
\includegraphics[width=1\textwidth]{quality_sun/newcrfs/10.png}\\
\end{minipage}
}
\subfigure[Ours]{
\begin{minipage}[b]{0.23\textwidth}
\includegraphics[width=1\textwidth]{quality_sun/dni/1.png}\\
\includegraphics[width=1\textwidth]{quality_sun/dni/3.png}\\
\includegraphics[width=1\textwidth]{quality_sun/dni/5.png}\\
\includegraphics[width=1\textwidth]{quality_sun/dni/6.png}\\
\includegraphics[width=1\textwidth]{quality_sun/dni/7.png}\\
\includegraphics[width=1\textwidth]{quality_sun/dni/8.png}\\
\includegraphics[width=1\textwidth]{quality_sun/dni/10.png}\\
\end{minipage}
}
\caption{Qualitative comparison with AdaBins~\citep{bhat2021adabins}, NeWCRFs~\citep{yuan2022new} on SUN RGB-D test set~\citep{song2015sun}. All the models are pre-trained on NYU Depth V2~\citep{silberman2012indoor} training set.}
\label{fig:qual_sun}
\end{figure}

\begin{figure}[t]
\centering
\subfigure[Image]{
\begin{minipage}[b]{0.18\textwidth}
\includegraphics[width=1\textwidth, height=0.80\textwidth]{quality_nyu/img/bathroom_rgb_00689.pdf}\\
\includegraphics[width=1\textwidth, height=0.80\textwidth]{quality_nyu/img/bedroom_rgb_00532.pdf}\\
\includegraphics[width=1\textwidth, height=0.80\textwidth]{quality_nyu/img/bedroom_rgb_01193.pdf}\\
\includegraphics[width=1\textwidth, height=0.80\textwidth]{quality_nyu/img/home_office_rgb_00557.pdf}\\
\end{minipage}
}
\subfigure[AdaBins]{
\begin{minipage}[b]{0.18\textwidth}
\includegraphics[width=1\textwidth, height=0.80\textwidth]{quality_nyu/adabins/bathroom__rgb_00689.pdf}\\
\includegraphics[width=1\textwidth, height=0.80\textwidth]{quality_nyu/adabins/bedroom__rgb_00532.pdf}\\
\includegraphics[width=1\textwidth, height=0.80\textwidth]{quality_nyu/adabins/bedroom__rgb_01193.pdf}\\
\includegraphics[width=1\textwidth, height=0.80\textwidth]{quality_nyu/adabins/home_office__rgb_00557.pdf}\\
\end{minipage}
}
\subfigure[NeWCRFs]{
\begin{minipage}[b]{0.18\textwidth}
\includegraphics[width=1\textwidth, height=0.80\textwidth]{quality_nyu/newcrfs/bathroom_rgb_00689.pdf}\\
\includegraphics[width=1\textwidth, height=0.80\textwidth]{quality_nyu/newcrfs/bedroom_rgb_00532.pdf}\\
\includegraphics[width=1\textwidth, height=0.80\textwidth]{quality_nyu/newcrfs/bedroom_rgb_01193.pdf}\\
\includegraphics[width=1\textwidth, height=0.80\textwidth]{quality_nyu/newcrfs/home_office_rgb_00557.pdf}\\
\end{minipage}
}
\subfigure[Ours]{
\begin{minipage}[b]{0.18\textwidth}
\includegraphics[width=1\textwidth, height=0.80\textwidth]{quality_nyu/dni/bathroom_rgb_00689.pdf}\\
\includegraphics[width=1\textwidth, height=0.80\textwidth]{quality_nyu/dni/bedroom_rgb_00532.pdf}\\
\includegraphics[width=1\textwidth, height=0.80\textwidth]{quality_nyu/dni/bedroom_rgb_01193.pdf}\\
\includegraphics[width=1\textwidth, height=0.80\textwidth]{quality_nyu/dni/home_office_rgb_00557.pdf}\\
\end{minipage}
}
\subfigure[Ground Truth]{
\begin{minipage}[b]{0.18\textwidth}
\includegraphics[width=1\textwidth, height=0.80\textwidth]{quality_nyu/gt/bathroom__rgb_00689_30.pdf}\\
\includegraphics[width=1\textwidth, height=0.80\textwidth]{quality_nyu/gt/bedroom__rgb_00532_117.pdf}\\
\includegraphics[width=1\textwidth, height=0.80\textwidth]{quality_nyu/gt/bedroom__rgb_01193_246.pdf}\\
\includegraphics[width=1\textwidth, height=0.80\textwidth]{quality_nyu/gt/home_office__rgb_00557_366.pdf}\\
\end{minipage}
}
\caption{Qualitative comparison with AdaBins~\citep{bhat2021adabins}, NeWCRFs~\citep{yuan2022new} on NYU Depth V2 test set~\citep{silberman2012indoor}. The ground-truth depth map are in-painted for visualization.}
\label{fig:qual_nyu}
\end{figure}
\end{document}
