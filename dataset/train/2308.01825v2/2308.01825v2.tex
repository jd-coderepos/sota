
\documentclass{article} \usepackage{iclr2021_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 \usepackage{amsmath}        \usepackage{amsfonts}       \usepackage{amssymb}        \usepackage{mathrsfs}       \usepackage{bm}             \usepackage{amsthm}         \usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}


\title{Scaling Relationship on Learning Mathematical Reasoning with Large Language Models}



\author{Zheng Yuan\thanks{Contributed Equally.} ,  Hongyi Yuan\thanks{Work done during internships at Alibaba DAMO Academy.} ,  
Chengpeng Li,
Guanting Dong, Keming Lu \\
\textbf{Chuanqi Tan, Chang Zhou, Jingren Zhou} \\
Alibaba DAMO Academy \\
\texttt{\{yuanzheng.yuanzhen,yuanhongyi.yhy\}@alibaba-inc.com} \\
\texttt{\{lichengpeng.lcp,dongguanting.dgt,lukeming.lkm\}@alibaba-inc.com} \\
\texttt{\{chuanqi.tcq,ericzhou.zc,jingren.zhou\}@alibaba-inc.com} \\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
Mathematical reasoning is a challenging task for large language models (LLMs), while the scaling relationship of it with respect to LLM capacity is under-explored.
In this paper, we investigate how the pre-training loss, supervised data amount, and augmented data amount influence the reasoning performances of a supervised LLM.
We find that pre-training loss is a better indicator of the model's performance than the model's parameter count.
We apply supervised fine-tuning (SFT) with different amounts of supervised data and empirically find a log-linear relation between data amount and model performance, and we find better models improve less with enlarged supervised datasets.
To augment more data samples for improving model performances without any human effort, we propose to apply Rejection sampling Fine-Tuning (RFT).
RFT uses supervised models to generate and collect correct reasoning paths as augmented fine-tuning datasets.
We find with augmented samples containing more distinct reasoning paths, RFT improves mathematical reasoning performance more for LLMs.
We also find RFT brings more improvement for less performant LLMs.
Furthermore, we combine rejection samples from multiple models which push LLaMA-7B to an accuracy of 49.3\% on GSM8K which outperforms the supervised fine-tuning (SFT) accuracy of 35.9\% significantly.
We release our codes and rejection sampling augmented data in \url{https://github.com/OFA-Sys/gsm8k-ScRel}.
\end{abstract}

\section{Introduction}
Large language models (LLMs) \citep{anil2023palm,llama2,gpt4} have shown considerable abilities in various math reasoning tasks \citep{saxton2019analysing,gsm8k,lightman2023lets}.
It is of interest to understand, predict, and improve an LLM's math reasoning ability based on different pre-trained LLMs and supervised datasets.
With this knowledge, we can better decide the effort we put into improving the LLM or augmenting the dataset.
Many recent works are focusing on using different prompts \citep{Wei2022ChainOT,yao2023tree} or ensembling / reranking multiple times of inferences \citep{gsm8k,uesato2022solving,wang2023selfconsistency,lightman2023lets} to improve models' reasoning performances. 
While in-context learning (ICL) and performing multiple inferences can improve performance, it is computationally expensive and not suitable for online deployment scenarios. 
Therefore, we focus on the performance of the supervised LLMs with inference only once which is a setting closer to online deployment.

To this end, we empirically investigate the scaling relationship of factors that influence the math reasoning abilities of a supervised LLM, including pre-training losses, the amount of supervised data, and the amount of augmented data.
Firstly, we analyze the supervised fine-tuning (SFT) and ICL performance of LLMs.
We observe that the pre-training loss is approximately negatively linear correlated to the SFT and ICL accuracy in a given interval which is a better performance indicator than pre-trained model sizes or pre-trained token counts. 
Secondly, we analyze the relationship between SFT and different amounts of supervised data. 
We observe that the model performance has a log-linear relation versus the supervised data amount while the increase diminishes with the better pre-trained model. 
Thirdly, we want to leverage the model itself to generate more supervised data to reinforce its reasoning ability and analyze the scaling relationship of the augmented data amount.
We apply rejection sampling on SFT models to sample and select correct reasoning paths as augmented dataset \citep{uesato2022solving,zhu-etal-2023-solving}.
We use these augmented datasets to fine-tune base LLMs which would achieve better performances compared to SFT and we denote it as rejection sampling fine-tuning (RFT).
We find the key factor influencing RFT performance is the distinct reasoning path amount which can be increased by sampling more times or combing samples from multiple models.
We apply RFT on several pre-trained LLMs and show larger improvement on less performant models.
We discuss the reason RFT works is it provides multiple reasoning paths which makes LLMs have better reasoning generalization.
We also discuss that RFT is much cheaper than pre-training in computational resources while training an LLM with lower pre-training loss is the fundamental solution.

The key findings of this paper are shown in Figure~\ref{fig:head} and are summarized here:
\begin{itemize}
\item When the pre-training loss gets smaller (i.e. the pre-trained model gets better), the model reasoning performances of SFT and ICL increase linearly within a range. The SFT performance improves slower than ICL.
\item SFT improves in a log-linear manner with the increase of supervised data amount. The benefits of increasing data amount diminish as the pre-trained model gets better.
\item The model performance for RFT improves as the distinct reasoning path amount increases. The RFT performance improves slower than SFT.
\item The combination of rejection sampling samples from multiple models further enhances the RFT performance, resulting in an accuracy of 49.3 for LLaMA-7B (+13.4 compared to SFT), 50.3 for LLaMA2-7B (+8.7 compared to SFT), 52.1 for LLaMA-13B (+9.1 compared to SFT), and 55.4 for LLaMA2-13B (+5.4 compared to SFT).
\end{itemize}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{fig/head.png}
    \caption{The key findings of scaling relationship on learning math reasoning ability with LLMs.}
    \label{fig:head}
\end{figure}

\section{Related Works}

\paragraph{Learning Math Reasoning with LLMs} Recent research on LLMs has discovered the emergent ability to solve reasoning tasks beyond a certain model scale \citep{Wei2022EmergentAO}. Such reasoning abilities in LLMs can be elicited by fine-tuning, few-shot prompting, or zero-shot prompting  \citep{gsm8k,Wei2021FinetunedLM,nye2021work,Wei2022ChainOT,kojima2022large}. A large amount of research focuses on the reasoning tasks of math word problems (MWP), and methods are evaluated on the benchmarks spanning different levels of MWPs (\citet{koncel-kedziorski-etal-2016-mawps,patel-etal-2021-nlp,lan2021mwptoolkit,gsm8k,jie-etal-2022-learning,math401,fu2023chainofthought}, \textit{inter alia}). The core idea of improving the mathematical reasoning ability of LLMs is to aggregate various sampled reasoning paths during either fine-tuning or inference. 
\citet{gsm8k} trained and devised a reasoning path verifier to select the correct results during inference. \citet{wang2023selfconsistency} proposed to sample various reasoning paths during inference and then derive the final result by majority voting on the answers or through verifiers \citep{li-etal-2023-making}. 
Several works applied the idea of rejection sampling along with other techniques to filter the diverse sampled reasoning paths for fine-tuning data augmentation \citep{huang2022large,zelikman2022star,ni2023learning,zhu-etal-2023-solving}. 
Rejection sampling is a simple-yet-effective fine-tuning augmentation technique and is also used for LLM alignment with human preference \citep{bai2022constitutional,yuan2023rrhf,dong2023raft,llama2,song2023preference}. 
\citet{uesato2022solving} explored to use of reinforcement learning methods for improving the mathematical reasoning abilities of LLMs and they further discussed the difference between outcome-based and process-based reward modeling. Followed by \citet{lightman2023lets}, they collected large-scale process-based supervision signals through human annotation and verified that LLMs can benefit more from process-based reward modeling with human-annotated supervision than outcome-based reward modeling. There is also prior research that distilled the emergent reasoning ability of LLMs to small language models \citep{fu2023specializing,shridhar-etal-2023-distilling}.
Compared to previous works \citep{zelikman2022star,uesato2022solving,zhu-etal-2023-solving,ni2023learning}, we are using a simpler way of generating augmented samples without any trained process-level reward models and we are focusing on researching the scaling relationship between LLMs and math reasoning ability.

\paragraph{Scaling Laws of Large Language Models} It is important to understand and predict the performance gain as the language model scales up. \citet{scalinglaw} first investigated and derived a predictable relationship on how the number of model parameters and data sizes contribute to the loss over many orders of magnitudes. \citet{chinchilla} refined the scaling laws in \citep{scalinglaw} and found the scaling laws for computation-optimal training. \citet{muennighoff2023scaling} explored and extended the scaling laws under a data-constrained scenario. Besides investigating the scaling performance for pre-training, \citet{gao2022rmscaling} discussed the scaling laws for overparameterized reward models for alignment with human preference, and \citet{hernandez2021transferscaling} developed scaling laws for transferring performance from pre-trained models to downstream tasks.
\cite{henighan2020scaling,caballero2022broken} investigated scaling laws of math problems.
In this paper, we are investigating the scaling relationships of large language models on learning math word problems with pre-training losses, supervised data amount, and augmented data amount.

\section{The factors of math reasoning ability in Supervised LLM}

The target of this paper is to try to understand the performances of supervised LLMs in math reasoning. 
We expect a pre-trained LLM  to learn reasoning ability from a supervised reasoning dataset .
The dataset is defined by , where  is a question,  is a chain-of-thought reasoning path, and  is a numerical answer.
We perform supervised fine-tuning on dataset  to obtain an SFT model .
We use  to generate reasoning paths and answers in the test set by greedy decoding and report the accuracy (i.e.  or maj1@1) as our metric here.

\subsection{Model Accuracy vs. pre-training Loss}
Previous works state that the larger LLM shows better reasoning ability across the same series of models \citep{gpt3,chowdhery2022palm,llama,llama2}, and we find LLaMA outperforms GPT-3 which shows the model parameter counts should not be the only indicator of reasoning ability.
While LLMs have different architectures, model parameters, and pre-training token numbers, we find the pre-training loss is a stable performance indicator of the math reasoning ability and we use it to represent the model instead of using their model parameters and pre-training token numbers.

We analyze the SFT and ICL (8-shot) performance of GPT-3 \citep{gpt3}, LLaMA \citep{llama}, LLaMA2 \citep{llama2}, and GPT-4 \citep{gpt4}. 
The pre-training losses of these models are observed in their paper, we should notice that pre-training losses correspond to different pre-training datasets and different tokenizers which means they could not be compared strictly (and we cannot use it to do any sort of regression directly) while the tendency among these losses is still enlightening.
We use the results of GPT-3 fine-tuning from \citep{gsm8k} and we fine-tune LLaMA and LLaMA2 on the GSM8K training set (detailed in Appendix A.1).
For in-context learning, we use the results from LLaMA \citep{llama} and LLaMA2 \citep{llama2} paper.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{fig/model_performance.png}
    \caption{The performance of SFT (blue lines) and ICL (red lines) settings on GSM8K. GPT-4 states they use some part of the GSM8K data in pre-training, and suggest others consider its performance between SFT and ICL.}
    \label{fig:sft_full}
\end{figure}
In Figure~\ref{fig:sft_full}, we can find that:
\begin{itemize}
    \item The pre-training losses are approximately negatively linear correlated to the SFT and ICL accuracy during the given pre-training loss interval. 
    \item SFT outperforms ICL consistently, while the improvements diminish when the pre-training loss is lower.
\end{itemize}
The linear relation of SFT and ICL accuracy may only work in the given interval. The reasons are (1) the slope of ICL is steeper than SFT, while the SFT performance should be greater than ICL performance; (2) the accuracy can not bigger than 1 or smaller than 0. It should be using  instead of  as the dependent variable theoretically while we find an apparent linear relationship among pre-training loss and  and use  as the dependent variable.
LLaMA-2 7B(13B) can be viewed as an approximation of continue-training of LLaMA 7B(13B). As it trains longer, its ICL and SFT performance both improve without changing the parameter count.
From the observations, one effective way to improve reasoning ability is to train a better base model with lower pre-training loss (Pre-training is all you need!).
The models with lower pre-training loss improve less from the fine-tuning which may be due to the models having already obtained more reasoning abilities during pre-training and the supervised data can provide less signal to supervise them.





\subsection{Model Accuracy vs. Supervised Data Count}

Supervised fine-tuning does improve LLMs' reasoning ability, we want to know how the supervised data amount influences the model's improvement. 
We fine-tune LLaMA and LLaMA2 with  amount of the training set from GSM8K (detailed in Appendix A.2). We want to use this experiment to extrapolate the model performances if we have more supervised data. In Figure~\ref{fig:data_amount}, we plot the results of training with different amounts of supervised data. From this figure, we can observe that:
\begin{itemize}
    \item The model performance has a log-linear relation versus data amount. When the data amount doubles, the performance increases by a unit.
\item Better model needs more amount of data to outperform its ICL performance.
    \item Better model benefits less when supervised data amount doubles.
\end{itemize}
The log-linear relation is stable during  amount of the training data. From the observation, it is straightforward to enlarge the training dataset to improve the performance, especially for worse models.
For better models, it benefits less which echoes that better models have learned more reasoning ability during pre-training.



\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{fig/sft_data_amount.png}
    \caption{The performance of SFT with different amounts of supervised data on GSM8K.}
    \label{fig:data_amount}
\end{figure}

\subsection{Model Accuracy vs. Augmented Data Count}
Increasing the amount of math reasoning labeled data is difficult, especially proposing a new question. 
It is easy for a well-educated student to solve hundreds of math word problems per day, but it is very hard to come up with diverse and educational math problems.
So our direction changes to augment new data using existing resources.
We have tried augmenting new queries (detailed in Appendix D.1) and augmenting revisions (detailed in Appendix D.2).
These approaches have none to marginal improvements compared to SFT.
We find a simplified version of rejection sampling \citep{zhu-etal-2023-solving} is a naive and effective way to augment new reasoning paths and can improve the model performance. 
And we find the key factor influences fine-tuning on rejection sampling (RFT) augmented data is distinct reasoning path amount.
Combining rejection sampling samples from multiple models, we can further fine-tune a LLaMA-7B model to an accuracy of 49.3 (compared with SFT 35.9) and a LLaMA-13B model to an accuracy of 52.1 (compared with SFT 43.0).

\paragraph{Rejection Sampling Fine-tuning} The SFT model  obtains the ability to perform zero-shot chain-of-thought reasoning, and we use  to generate more correct reasoning paths  to supply the training dataset. 
For each , we generate  candidate reasoning paths and answers  with a temperature of 0.7 following \citep{gsm8k}.
We first filter out reasoning paths with wrong answers  or wrong calculations based on Python evaluation.
Each reasoning path contains a list of equations , and we select one reasoning path  for each distinct equation list as the augmented  data and remove other reasoning paths with the same list of equations to deduplicate similar reasoning paths.  
Different order of elements (e.g.  and ) or different order of equations (e.g.  and ) are considered different. 
It is helpful for models to know these orders can be exchanged and is hard for models to learn this with only one reasoning path each problem.
We define  as the augmented dataset. We fine-tune  on pre-trained LLM  to  as RFT, and we detail how we apply RFT in Appendix A.3.
We list the results of RFT with sampling  candidate reasoning paths on LLaMA and LLaMA-2 in Table~\ref{tab:rft-main}. For ICL, SFT, and RFT, we list the maj1@1 (accuracy) and maj1@100 (sample 100 times and calculate accuracy based on majority voting) as metrics.

\begin{table}[t]
    \small
    \centering
    \begin{tabular}{l|ccccccc}
  \hline
  Setting & 7B & 7B-2 & 13B & 13B-2  & 33B \\
  \hline
  Pretrain loss & 1.8& 1.75& 1.73& 1.68& 1.62 \\
  \hline
  ICL & 11.0/18.1 & 14.6/- & 17.8/29.3&28.7/-&35.6/53.1\\
  SFT &35.9/48.7& 41.6/55.4 & 43.0/55.2 & 50.0/61.7 & \textbf{54.6}/-& \\
  \hline
  RFT & \textbf{41.7/52.7} & \textbf{47.5/58.7} & \textbf{49.1/59.9} & \textbf{54.8/65.4}&54.5/-\\
  Correct paths per question & 53.3 & 60.8 & 62.5 & 71.6 & 88.7 \\
  Distinct paths per question & 5.25 & 5.19 & 5.26 & 5.29 & 2.78 \\
  \hline
    \end{tabular}
    \caption{The performance of RFT with  on GSM8K compared with SFT and ICL. Distinct path amount means distinct equation list amount here.}
    \label{tab:rft-main}
\end{table}

In the case of 7B and 13B models, RFT yields an approximate increase of 5 to 6 points in maj1@1 and about 4 points increase in maj1@100.
For 33B models, RFT does not improve performance compared to SFT.
The main reason comes from the augmented samples from rejection sampling. 
We can find that better models generate more correct reasoning paths per question.
For LLaMA-33B-SFT, it can generate an average of 88.7 correct paths per question. However, it overfits the training set and has difficulty generating more diverse paths on the training set questions. 
Rejection sampling with 33B is very time-consuming and we do not conduct a temperate grid search, we have tried using a larger temperate 1.0 for decoding LLaMA-33B-SFT models, it generates 82.4 correct paths and 4.77 distinct paths per question which is more diverse than using temperate 0.7 but still less diverse than 7B and 13B models. 
We admit there should be a temperate (or generation config) that can produce more distinct paths and generate good results for RFT in 33B and even larger models while it does need more computation resources for inference compared to sampling using 7B and 13B models.
We will show we can use 7B and 13B models only for rejection sampling to improve the 33B model.


\paragraph{Model Accuracy vs Rejection Sampling Data Count}



To understand the performance of RFT, we vary  among  and apply RFT. We also have another setting of  while not removing any reasoning paths denoted as \textit{no dedup}. We list the RFT results with different  on Figure~\ref{fig:rft_data_amount}.
Comparing using RFT with  and \textit{no dedup}, the performance is similar and shows that it is better to estimate RFT performance based on distinct reasoning path amount instead of RFT augmented sample counts.
Furthermore, using deduplication has better performances for 3 of 4 models and needs much less training time. 

When using , RFT outperforms SFT by 2 points stably.
For most data points, using larger  leads to better performances.
However, the merits of RFT are decreasing when doubling .
We calculate different paths per question for different  in Table~\ref{tab:rft_k_different_path}. We can see that the amount of different reasoning paths is not growing quickly along  growing. In Figure~\ref{fig:data_amount}, we know doubling training samples can have a linear performance improvement. Doubling reasoning paths should improve less than doubling training samples since obtaining different reasoning paths does not obtain any new questions. Therefore, doubling  leads to diminished performance improvements.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{fig/rft_data_amount.png}
    \caption{The performance of RFT with different amounts of sampling count  on GSM8K.}
    \label{fig:rft_data_amount}
\end{figure}

\begin{table}[t]
    \small
    \centering
    \begin{tabular}{l|ccccc}
  \hline
   & 7B & 7B-2 & 13B & 13B-2 & 33B\\
  \hline
  1 & 1.17 & 1.19 & 1.15 & 1.18 & 1.06  \\
  3 & 1.44 & 1.47 & 1.41 & 1.45 & 1.16  \\
  6 & 1.74 & 1.78 & 1.69 & 1.76 & 1.28 \\
  12 & 2.20 & 2.23 & 2.11 & 2.21 & 1.46 \\
  25 & 2.93 & 2.93 & 2.88 & 2.94 & 1.77 \\
  50 & 3.94 & 3.91 & 3.90 & 3.94 & 2.19\\
  100 & 5.25 & 5.19 & 5.26 & 5.29 & 2.78 \\
  \hline
  400 (U13B) & &&12.84 \\
  500 (U33B) & &&13.65 \\
  \hline
    \end{tabular}
    \caption{Different reasoning paths per question generated by different SFT models with different .}
    \label{tab:rft_k_different_path}
\end{table}



\paragraph{Combining rejection sampling samples from multiple models} 

The experiment results above demonstrate performance boosts in mathematical reasoning, benefitting from rejection sampling. Through case studies in \ref{different_path}, we show that rejection sampling can augment training data with reasoning paths of diverse calculation processes. 
However, the reasoning paths sampled from one single SFT model can be logically non-diverse. 
Therefore, we expect to further improve the mathematical reasoning performance by leveraging rejection sampled reasoning paths aggregated from different models. 
We denote two final datasets as  and , which are aggregated from rejection sampling different models  and , where U means models under a certain size, 7B/13B/33B means LLaMA-7B/13B/33B and 7B2/13B2 means LLaMA2-7B/13B.  means an aggregation process in which all the reasoning paths from different sets are first combined and then Algorithm \ref{alg:path_select} is applied to deduplicate the reasoning paths with the same calculation process regarding the equation forms and orders.



We can see, through the results visualized in Figure \ref{fig:rft_u13b}, that using the aggregated dataset  and  can lead to uniformly better performance than fine-tuning with datasets from a single model across different model sizes. RFT on these two augmented datasets  and  decreases the performance gaps among the same size models in SFT and RFT  which mean the combined augmented datasets provide enough reasoning supervision to fulfill the pre-training gap.
We can assume with sufficient supervised data amounts, the performance indicator should be the model size but not the pre-training losses. 

We have stated that it is expensive to apply RFT  on 33B models and it needs a temperate grid search to achieve an improvement compared to SFT. However fine-tuning on  has similar rejection sampling computational cost compared with sampling 100 times on 33B and achieve better performance.

Another phenomenon is including  in aggregation barely influences the performance. To give a more comprehensive analysis of the results, we calculate the average reasoning path number per question in Table \ref{tab:rft_k_different_path} and depict a Venn diagram to visualize the source of different reasoning paths shown in Figure \ref{fig:venn}. 
In Table \ref{tab:rft_k_different_path}, the average reasoning path numbers of  and  surpass those of a single model by large amounts, while  only have slightly more reasoning paths than  by 0.81. In the meanwhile, as shown in Figure \ref{fig:venn}, the models under and including the size of 13B can contribute unique reasoning paths of similar proportion in  around 15\%. However, only 6.5\% of the reasoning paths can be exclusively acquired from LLaMA-33B-SFT model. 
This shows that the SFT model of 33B can provide limited reasoning diversity when sampling the training questions. This finding is consistent with the results above in Table \ref{tab:rft-main}, indicating the 33B model (and possibly 65B and 70B models) can well memorize the human-annotated reasoning paths.

For 65B models, we find using  does not improve the performance compared to SFT. The reason can be better models benefit less from the supervised sample amounts while it has learnt more reasoning ability during pre-training.

Overall, we can come to the conclusion that (1) RFT improves the mathematical reasoning performance of (worse) LLMs through diverse reasoning paths from rejection sampling of the SFT models, and aggregating more diverse reasoning paths can improve the performance further. (2) Different SFT models can contribute reasoning paths with different calculation processes from rejection sampling, leading to more diverse training data for RFT, and LLMs of larger parameter sizes may degrade in generating diversified reasoning paths as a result of overfitting the training questions. There may be a generation config or training config for large enough LMs not to overfit on the training dataset while it is not trivial to find them. 

 \begin{figure}[t]
    \centering
\includegraphics[width=0.98\linewidth]{rft_u13b.png}
    \caption{The performance of RFT with rejection sampling samples from multiple models.}
    \label{fig:rft_u13b}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\linewidth]{fig/venn.png}
    \caption{The Venn diagram of the proportions of the reasoning calculation paths that each model provide to . For example, 15.5\% (in the yellow part) of the reasoning calculation paths in  can only be exclusively found in the rejection sampling results from LLaMA2-13B-SFT.}
    \label{fig:venn}
\end{figure}

\begin{table}[t]
    \small
    \centering
    \begin{tabular}{ll|cc}
  \hline
  Base Model & Training & maj1@1 & maj1@K* \\
  \hline
  \bf{Proprietary LLMs} &&& \\
  GPT-4 \citep{gpt4} & 5-shot ICL & 92.0 & - \\
GPT-3-175B \citep{gpt3} & SFT & 34.0 & - \\
  PaLM2 \citep{anil2023palm} & 8-shot ICL & 80.7 & 91.0@K=40 \\
  PaLM-540B \citep{chowdhery2022palm} & 8-shot ICL & 56.5 & 74.4@K=40 \\
  Chinchilla-70B \citep{uesato2022solving} & 5-shot ICL & 43.7 & 58.6@K=96  \\
  Chinchilla-70B & SFT & 58.9 & 77.7@K=96  \\
  \hline
  \bf{Open-sourced LLMs} &&& \\
GPT-Neo-2.7B \citep{gpt-neo} & FCS + PCS \citep{ni2023learning} & 19.5 & 41.4 \\
  GPT-J-6B \citep{gpt-j} & CoRE \citep{zhu-etal-2023-solving} & 34.9 & 63.2@K=40 \\
  ChatGLM2-6B \citep{zeng2022glm} & 8-shot ICL & 32.4	& - \\
  ChatGLM2-6B & Human Alignment & 28.1	& - \\
  ChatGLM2-12B & 8-shot ICL & 40.9	& - \\
  ChatGLM2-12B & Human Alignment & 38.1	& - \\
  InternLM-7B \citep{2023internlm} & 4-shot ICL & 31.2	& - \\
  InternLM-7B & Human Alignment & 34.5 \\
  LLaMA-7B & SFT & 35.9 & 48.7 \\
  \hline
  \bf{Our RFT on open-sourced LLMs} &&& \\
  LLaMA-7B & RFT-U13B & 49.3 & 61.8 \\
  LLaMA2-7B & RFT-U13B & 50.3 & 65.6 \\
  LLaMA-13B & RFT-U13B &  52.1 & 66.2 \\
  LLaMA2-13B & RFT-U13B &  55.4 & 69.1 \\
\hline
    \end{tabular}
    \caption{Compare GSM8K results with other baselines. RFT-U13B means models fine-tuned on . FCS and PCS represent fully-correct solutions and partially-correct solutions respectively. *K=100 if not specified.}
    \label{tab:compare}
\end{table}

\paragraph{Comparing to other baselines}

We compare our RFT results of training on  to several baselines and the results are detailed in Table \ref{tab:compare}. Although LLaMA and LLaMA2 are top-tier open-sourced LLMs \footnote{\url{https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard}}, their mathematical reasoning performances still lag behind the current proprietary LLMs which are of larger parameter scales, such as GPT-4 and PaLM2. Compared to results on open-resourced models, our results on LLaMA present better performance than two recent state-of-the-art reasoning augmentation methods. Our RFT method is simpler compared to CoRE, since RFT does not require training verifier models and decoding with Monte Carlo Tree Search (MCTS).
Compared to other open-sourced aligned language models, we can find that 7B models struggle at a level of 35 scores which are very similar to SFT performances of LLaMA-7B. We guess they use GSM8K during their pre-training phase following \citep{gpt4} or human alignment fine-tuning phase following \citep{alpaca-cot}.
Using our augmented dataset  to replace the original GSM8K can significantly boost their 7B models' performances.

\section{Discussion}

\subsection{Different distribution of reasoning paths}

\label{different_path}

\begin{figure}[t]
    \centering
    \small    \includegraphics[width=0.9\linewidth]{fig/diff_reason_path_number_hist.png}
    \caption{The histograms of question numbers solved with different numbers of unique reasoning calculation paths. We show the difference in question counts between SFT and RFT U13B in two cases where the numbers of unique reasoning calculation paths are 1 or more than 10.}
    \label{fig:reason-dist}
\end{figure}

In the aforementioned analysis of RFT training data, we observe that rejection sampling can augment the training question with diverse reasoning calculation paths. In this section, we investigate whether RFT models can learn to generate different reasoning paths to reach the correct answers.  
We fine-tune LLaMA and LLaMA2 of 7B and 13B on . During inference, we sample 100 different reasoning paths from each trained model for each test set question with a temperature of 0.7. For each question, we compute the number of different calculation processes presented in 100 sampled reasoning paths that lead to the correct answer and draw histograms with respect to test set questions. SFT and RFT models on self-sampled datasets (RFT k=100) are included for comparison.

As shown in Figure \ref{fig:reason-dist}, the models trained by RFT on  exhibit more question counts than the models trained by RFT k=100 and SFT on the larger numbers of unique calculation processes. There are more question counts for SFT models where all the sampled reasoning paths only correspond to one single calculation process and SFT models can barely generate more than 8 different calculation processes for a question. This analysis demonstrates that diverse reasoning calculation paths in training data can equip the LLMs with finding diverse reasoning logic for solving math problems. 


\subsection{Towards Excelsior Mathematical Reasoning}

From our findings, there are two main factors that can improve mathematical reasoning abilities given a preset amount of human-annotated samples, including:
(1) Pre-training the LLMs to lower losses;
(2) Augmenting fine-tuning with rejection sampling.
Through extensive experiments, we empirically verify the scaling relationships between the mathematical reasoning performance of LLM with both factors respectively. Out of the consideration of sustainable NLP, in this section, we investigate the possible computational resources required to extrapolate the mathematical performance of LLMs by both factors and discuss how to improve the performance more efficiently. 

We estimate the pre-training, SFT, RFT inference, and RFT FLOPs following \cite{scalinglaw} and GPU times in Table~\ref{tab:llamagpu} which is detailed in Appendix E.
We can find that the cost times of SFT () and RFT () are negligible compared to pre-training.
One can always use SFT and RFT to improve models' performance. 
However, it could be hard to use RFT to further boost performance. 
Since we need much more sampling counts (at an exponential level) to increase distinct reasoning paths and there exists an upper bound of distinct reasoning path amount for a given math reasoning question.

\begin{table}[t]
    \centering
    \small
    \resizebox{1\linewidth}{!}{
    \renewcommand{\arraystretch}{1.5}
    \begin{tabular}{l|ccccccc}
  \hline
  Model size & 7B & 7B-2& 13B&13B-2 & 33B& 65B & 70B\\
  \hline
  Pre-train FLOPs &  &  &  &  &&   & \\
  SFT FLOPs & \multicolumn{2}{c}{} &\multicolumn{2}{c}{} &  &  & \\
  RFT Inference FLOPs & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} &   &  &  \\
  RFT-U33B FLOPs & \multicolumn{2}{c}{} &\multicolumn{2}{c}{} &  &  & \\
  \hline
  Pre-train GPU hrs & 82k & 184k & 135k & 368k & 530k & 1022k & 1720k\\
  SFT GPU hrs & \multicolumn{2}{c}{0.6} & \multicolumn{2}{c}{4} &40 & 74 & 80 \\
  RFT Inference GPU hrs & \multicolumn{2}{c}{10} & \multicolumn{2}{c}{0.1k} &0.1k & 4.3k & 4.5k \\
  RFT-U33B GPU hrs & \multicolumn{2}{c}{9} & \multicolumn{2}{c}{62} &0.6k & 1k & 1.2k \\
  \hline
  ICL Accuracy & 11.0 & 14.6 & 17.8 & 28.7 & 35.6 & 50.9 & 56.8 \\
  SFT Accuracy & 35.9 & 41.6 & 43.0 & 50.0 & 54.6 & 59.3 & 63.2 \\
  RFT-U33B Accuracy &  49.1 & 51.2 & 51.4 & 55.3 & 57.9 &-&-\\
  \hline
    \end{tabular}}
    \caption{The statistics of FLOPs and GPU hours required for pre-training, SFT, RFT inference, and RFT. We take the pre-training GPU hours from \citet{llama,llama2}. The GPU hours for RFT inference are calculated for 7,473 train set questions and 100 samples per question. To make the best of GPUs and properly fit models into the GPU memory, we tune the inference batch size. For 33B, 65B, and 70B models, we use DeepSpeed ZeRO3 \citep{deepspeed} for distributed training. All the GPU hours are based on NVIDIA A100 80GB GPU. Note we use non-embedding parameters to compute FLOPs in our experiments.}
    \label{tab:llamagpu}
\end{table}

We assume that performance follows RFTSFTICL, from the findings in this paper we know the improvement speed follows RFTSFTICL. And if we have an omnipotent language model which has a pre-training loss that is the same as the corpus randomness, it could have RFT = SFT = ICL = 100.
Thus when you pre-train a better language model (i.e. smaller pre-training loss), your model's performance still follows RFTSFTICL but their performance gaps are diminishing.
Since you can obtain an RFT model without too much effort (compared to pre-training), then the most important thing we should do is to decrease the model's pre-training loss.
From LLaMA-7B to LLaMA2-7B, it needs to add  FLOPs to obtain a 2.1 improvement in the RFT-U33B setting with a 0.05 pre-training loss decrease.
From LLaMA-7B to LLaMA-13B, it adds  FLOPs to obtain a 2.3 improvement in the RFT-U33B setting with a 0.07 pre-training loss decrease.
While minimizing pre-training loss is expensive compared to SFT and RFT, we believe other abilities may follow a similar pattern and better pre-training can benefit all other tasks.

\section{Conclusions}
In this paper, we are investigating the scaling relationship in supervising math reasoning abilities with large language models.
We find the relationship between math performance and pre-training losses, supervised data amount, and distinct reasoning paths.
We find that better language models benefit less with SFT and RFT, and the most important thing is to pre-train a better language model towards excellent math reasoning abilities.

\section{Acknowledgement}
We would like to express our sincere appreciation to Tianhang Zhu, Runji Lin, Kai Dang, Keming Lu, Wei Wang, and Junyang Lin for their valuable insights and contributions to this paper.

\section{Limitations}
In this paper, we miss the following parts which are very important for building math reasoning abilities for LLMs and should be discussed in the revised version of this paper or future works.
\begin{itemize}
    \item RFT for 65B and 70B LLaMA models.
    \item Pre-training on the math-related corpus. This is obviously useful shown in \cite{lewkowycz2022solving}. While the pre-training loss obtained here cannot align with general domain pre-trained models' losses.
    \item We do not regress any scaling laws in this paper since many numbers are estimated and pre-training losses, ICL prompts and SFT settings of various models may not be aligned.
\end{itemize}





\bibliography{iclr2021_conference}
\bibliographystyle{iclr2021_conference}

\appendix
\section{Detailed experiment setting}
\subsection{SFT on GSM8K}
We fine-tune GSM8K with 3 epochs and a batch size of 128 on NVIDIA A100 GPUs. We use 8 GPUs for 7B and 13B models, 16 GPUs for 33B models, and 32 GPUs for 65B and 70B models during fine-tuning.
We use a peak learning rate of 2e-5 with a  learning rate warmup. We evaluate the results on the final epoch. We use greedy decode to calculate maj1@1 and decode with temperature 0.7 to calculate maj1@100.

\subsection{SFT on downsampled GSM8K}
We random downsample GSM8K dataset for fine-tuning. We find that using 3 epochs for little data will result in very poor results which are listed in Table~\ref{tab:detailed_results}. We search training epoch among  and evaluate the latest epoch. We report better test results among these two different epoch settings.

\subsection{Rejection Sampling Fine-tuning on GSM8K}
We use an SFT model  to sample on training dataset for  times with a temperature of 0.7. 
We extract the equation list in generated reasoning paths by finding  first,  removing all white spaces, and joining the equation string list by a special symbol to a string (called it get\_equation in our algorithm) for deduplication.
We select the reasoning paths by this algorithm:
\begin{algorithm}[H]
\SetAlgoLined
\KwData{Reasoning paths for question , }
\KwResult{Selected reasoning paths for question , }
Initialize selected reasoning paths, 

Initialize appeared equation set, 

\For{ in }{
\If{get\_equation() }{
.append(); \\
.update([\textit{get\_equation()}])
}
\Else{
find   s.t. \textit{get\_equation()} = \textit{get\_equation()};

\If{ Levenstein\_dist(, )   Levenstein\_dist(, )}{
 = ;
}
}
}
\caption{Reasoning Path Selection}
\label{alg:path_select}
\end{algorithm}
We are trying to find the most dissimilar reasoning paths based on Levenstein distances. The idea comes from we want diverse reasoning paths for better generalization.

\section{Detailed Results of SFT and RFT}
We list detailed results of SFT and RFT in Table~\ref{tab:detailed_results} and ~\ref{tab:detailed_results_100}.

\begin{table}[ht]
    \small
    \centering
    \begin{tabular}{l|cc|ccccccc}
  \hline
    Model & Data & Epoch & 7B & 7B-2 & 13B & 13B-2  & 33B & 65B & 70B-2 \\
  \hline
ICL-8shot & 0 & 0 & 11.0 &14.6 &17.8&28.7 & 35.6 & 50.9 & 56.8 \\
  \hline
  SFT & 1/32 & 96 & 9.5 & 10.1 & 8.6 & 17.1& 18.6 & 25.2 & 27.4\\
  SFT & 1/16 & 48 & 14.3 & 15.5 & 14.2 & 23.9 & 25.9 & 28.9 & 33.6\\
  SFT & 1/8 & 24 & 17.9 & 20.8 & 18.4 & 28.5 & 31.6 & 35.8 & 38.9\\
  SFT & 1/4 & 12 & 21.6 & 27.7 & 26.7 & 36.3& 38.4 & 45.6 &46.9\\
  SFT & 1/2 & 6 & 29.0 & 33.1 & 35.2 & 43.7& 48.6 & 50.5 & 57.5 \\
  \hline
  SFT & 1/32 & 3 & 7.8& 14.2 & 0.0 & 5.9 & 25.3 & 28.9 & 15.8\\
  SFT & 1/16 & 3 & 12.7& 16.2 & 7.4 & 27.7 & 29.2& 39.5 & 52.8 \\
  SFT & 1/8 & 3 & 16.5& 21.8 & 19.5 & 33.4 & 39.3& 46.0 & 57.8\\
  SFT & 1/4 & 3 & 22.7& 28.1 & 27.4 & 37.5 & 44.6 & 50.4 & 57.8\\
  SFT & 1/2 & 3 & 30.9& 34.6 & 36.1 & 45.3 & 50.8& 55.6 &61.0 \\
  SFT & 7.4K & 3 & 35.9& 41.6 & 43.0 & 50.0 & 54.6& 59.3 & 63.2 \\
\hline
  RFT no dedup & 1/32 &3 &37.5&-&-&-&-&-&-  \\
  RFT no dedup & 1/16 &3 &38.3&-&-&-&-&-&-  \\
  RFT no dedup & 1/8 &3 &41.1&-&-&-&-&-&-  \\
  RFT no dedup & 1/4 &3 &41.2&-&-&-&-&-&-  \\
  RFT no dedup & 1/2 &3 &43.9&-&-&-&-&-&-  \\
  RFT no dedup & 400K & 3 & 43.6 & 46.7 & 46.9 & 53.7&-&-&- \\
\hline
  RFT k=1 & 12K & 3 & 37.6 & 43.4 & 42.7 & 52.1 &-&-&-\\ 
  RFT k=3 & 15K & 3 & 39.0 & 45.3 & 45.2 & 51.9 &-&-&-\\ 
  RFT k=6 & 18K & 3 & 39.5 & 45.6 & 46.8 & 52.2 &-&-&-\\ 
  RFT k=12 & 22K & 3 & 41.6 & 45.3 & 48.0 & 53.1&-&-&-\\ 
  RFT k=25 & 28K & 3 & 40.9 & 46.5 & 46.0 & 52.6 &-&-&-\\ 
  RFT k=50 & 35K & 3 & 40.7 & 47.0 & 49.4 & 54.5  &-&-&-\\ 
  RFT k=100 & 47K & 3 & 41.7 & 47.5 & 49.1 & 54.8 & 54.5&-&- \\
  \hline
  RFT-U13B & 104K & 3 & 49.3 & 50.3 & 52.1 & 55.4 & 56.5 & 59.0&62.3\\
  RFT-U33B & 110K & 3 & 49.1 & 51.2 & 51.4 & 55.3 & 57.9&59.7&64.8\\
  \hline

    \end{tabular}
    \caption{Detailed numerical results in this paper, some experiments are still under running. We report maj1@1 (accuracy) in this table.}
    \label{tab:detailed_results}
\end{table}

\begin{table}[ht]
    \small
    \centering
    \begin{tabular}{l|ccccccc}
  \hline
  Setting & 7B & 7B-2 & 13B & 13B-2  & 33B & 65B & 70B-2 \\
  \hline
  ICL-8shot & 11.0/18.1 & 14.6/- & 17.8/29.3&28.7/-&35.6/53.1&50.9/69.7&56.8/-\\
  SFT &35.9/48.7& 41.6/55.4 & 43.0/55.2 & 50.0/61.7 & 54.6/72.6& 59.3/69.7&63.2/73.5  \\ RFT k=100& 41.7/52.7 & 47.5/58.7 & 49.1/59.9 & 54.8/65.4&54.5/-& -& -  \\
  RFT-U13B&49.3/61.8 & 50.3/65.6 & 52.1/66.2 & 55.4/69.1 & 56.5/- & 59.0/- & 62.3/- \\
  RFT-U33B& 49.1/61.6 & 51.2/64.1 & 51.4/66.3 & 55.3/69.1 & 57.9/71.3 & 59.7/- & 64.8/-  \\
  \hline
    \end{tabular}
    \caption{Detailed numerical results in this paper, some experiments are still under running. We report maj1@100 in this table.}
    \label{tab:detailed_results_100}
\end{table}


\section{Case Study of RFT}

\begin{table}[t]
    \centering
    \resizebox{1.0\textwidth}{!}{
    \begin{tabular}{p{0.1\textwidth}|p{0.9\textwidth}}
        \hline
        Query A & Weng earns \\textcolor{red}{\textlangle\textlangle12/60=0.2\textrangle\textrangle}0.2 an hour for the 50 minutes that she worked yesterday. Therefore, she earned \\textcolor{red}{\textlangle\textlangle0.2*50=10\textrangle\textrangle}10 for the 50 minutes that she worked yesterday.\#\#\#\# 10 \\
        \hline
        Path 2& Weng earns 12 / 60 = \\textcolor{red}{\textlangle\textlangle50*0.2=10\textrangle\textrangle}10 for the 50 minutes of babysitting. \#\#\#\# 10 \\
        \hline
        Path 3& Since there are 60 minutes in an hour, then Weng did 50/60 = \textcolor{red}{\textlangle\textlangle50/60=0.8333\textrangle\textrangle}0.8333 of an hour of babysitting. So, she earned \\textcolor{red}{\textlangle\textlangle12*0.8333=10\textrangle\textrangle}10. \#\#\#\# 10 \\
        \hline
        Path 4& She earns 12 * 50 / 60 = \\textcolor{red}{\textlangle\textlangle12*(50/60)=10\textrangle\textrangle}10. \#\#\#\# 10 \\
        \hline \hline 
        Query B & A local college is offering German lessons and currently has 8 students enrolled. Through advertising, 8 more became interested but a fourth of these dropped out within a day. 2 more got frustrated and left. The class then rallied to show how simple the course actually is and increased enrollment by 5 times the amount of students already enrolled in the class, but 2 had to drop it because of scheduling conflicts. After one last rally, 6 more people enrolled. As the days passed, half of the class eventually dropped, and half of the remaining students graduated. How many are still enrolled? \\
        \hline
        \hline
        Path 1& 8+8=\textcolor{red}{\textlangle\textlangle8+8=16\textrangle\textrangle}16 students are enrolled in the German class. 8*.25=\textcolor{red}{\textlangle\textlangle8*.25=2\textrangle\textrangle}2 dropped out from the new recruits. 2 more left, so the class had 16-2-2=\textcolor{red}{\textlangle\textlangle16-2-2=12\textrangle\textrangle}12 students left. There were 12*5+12=\textcolor{red}{\textlangle\textlangle12*5+12=72\textrangle\textrangle}72 total students in the class after the rally. Two people had to drop the class, so there were 72-2=\textcolor{red}{\textlangle\textlangle72-2=70\textrangle\textrangle}70 left. 70+6=\textcolor{red}{\textlangle\textlangle70+6=76\textrangle\textrangle}76 were enrolled after the final rally. 76/2=\textcolor{red}{\textlangle\textlangle76/2=38\textrangle\textrangle}38 are left after half the class dropped. 38/2=\textcolor{red}{\textlangle\textlangle38/2=19\textrangle\textrangle}19 are left after half the class graduated. \#\#\#\# 19\\
        \hline
        Path 2& 8+8=\textcolor{red}{\textlangle\textlangle8+8=16\textrangle\textrangle}16 students are enrolled after the first rally. 16/4=\textcolor{red}{\textlangle\textlangle16/4=4\textrangle\textrangle}4 dropped out. 16-4=\textcolor{red}{\textlangle\textlangle16-4=12\textrangle\textrangle}12 remain. 12*5+12=\textcolor{red}{\textlangle\textlangle12*5+12=72\textrangle\textrangle}72 after the second rally. 72-2=\textcolor{red}{\textlangle\textlangle72-2=70\textrangle\textrangle}70 after 2 more dropped. 70+6=\textcolor{red}{\textlangle\textlangle70+6=76\textrangle\textrangle}76 after 6 more enrolled. 76/2=\textcolor{red}{\textlangle\textlangle76/2=38\textrangle\textrangle}38 remain after half the class dropped. 38/2=\textcolor{red}{\textlangle\textlangle38/2=19\textrangle\textrangle}19 remain after half the class graduated. \#\#\#\# 19 \\
        \hline
        Path 3 & Advertising brought in 8+8=\textcolor{red}{\textlangle\textlangle8+8=16\textrangle\textrangle}16 students. Of these, 16/4=\textcolor{red}{\textlangle\textlangle16/4=4\textrangle\textrangle}4 dropped out. 16-4-2=\textcolor{red}{\textlangle\textlangle16-4-2=10\textrangle\textrangle}10 remained. Then 10*5=\textcolor{red}{\textlangle\textlangle10*5=50\textrangle\textrangle}50 more enrolled. This brought the class to 50+10=\textcolor{red}{\textlangle\textlangle50+10=60\textrangle\textrangle}60 students. Then 60+6=\textcolor{red}{\textlangle\textlangle60+6=66\textrangle\textrangle}66 enrolled. So there were 66+10=\textcolor{red}{\textlangle\textlangle66+10=76\textrangle\textrangle}76 students. Then 76/2=\textcolor{red}{\textlangle\textlangle76/2=38\textrangle\textrangle}38 dropped. So 76-38=\textcolor{red}{\textlangle\textlangle76-38=38\textrangle\textrangle}38 remained. Then 38/2=\textcolor{red}{\textlangle\textlangle38/2=19\textrangle\textrangle}19 graduated. So 38-19=\textcolor{red}{\textlangle\textlangle38-19=19\textrangle\textrangle}19 were left. \#\#\#\# 19 \\
        \hline
    \end{tabular}
    }
    \caption{Cases of generated reasoning paths with different reasoning complexity from rejection sampling for RFT. The calculations are highlighted in \textcolor{red}{red}.  }
    \label{tab:rft_case}
\end{table}

In this section, we present the cases of the training samples from rejection sampling. The case studies would shed light on how RFT potentially improves the mathematical reasoning performance of LLMs. The cases are shown in Table \ref{tab:rft_case}.
As aforementioned, RFT considers the reasoning paths with different calculation processes regarding equation forms or orders, leading to the correct answers. In the cases from Table \ref{tab:rft_case}, all the reasoning paths from RFT result in the correct answer of 10, while the calculation processes of reasoning are diverse. Path 1 and 2, as well as Path 4 and 5, are different in the equation forms as highlighted in red. Path 1 and 2 present a two-step calculation reasoning process while Path 4 and 5 alter to a one-step calculation reasoning process. 
The case demonstrates that rejection sampling can potentially provide more supervision signals that improve mathematical reasoning performance. The filtered reasoning paths sampled from LLMs themselves are of similar quality to the reasoning demonstrations from human annotations. 

\section{Preliminary Experiments}

\subsection{Self Query Augmentation}

Through our preliminary experiments and case studies, the errors made by the fine-tuned LLMs are partly attributed to the incorrect reasoning chains where LLMs mistakenly understand the context information or fail to consider all the information in the queries. Although such incorrect reasoning chains lead to wrong answers to the original queries, the reasoning chains themselves represent reasonable logic. For example, for the query \textit{Josh decides to try flipping a house. He buys a house for \50,000 in repairs.  This increased the value of the house by 150\%.  How much profit did he make?}, a fine-tuned LLaMA model predicts \textit{The value of the house increased by 80,000*.15=\92,000. So he made a profit of 92,000-80,000-50,000=\KK=1KKNN-1KNn_{ctx}n_{Q}, n_{R} N_{s}N_{s}n_{q}, n_{r} n_{q}\approx 66n_{r} \approx 130 $.






















\end{document}
