\pdfoutput=1
\documentclass[]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{authblk}
\usepackage{subfigure}
\usepackage{ctable}
\usepackage{multirow}
\usepackage{color}
\usepackage{booktabs}

\newcommand{\vecb}[1]{\mathbf{#1}}
\newcommand{\matb}[1]{\mathbf{#1}}
\newcommand{\R}{\mathbb{R}}

\begin{document}
\markboth{M. Romaszewski, P. G{\l}omb, P. Gawron}
{Natural hand gestures for human identification in a Human-Computer Interface}


\title{Natural hand gestures for human identification in a Human-Computer Interface}

\author{Micha{\l} Romaszewski}
\author{Przemys{\l}aw G{\l}omb}
\author{Piotr Gawron}
\affil{
Institute of Theoretical and Applied Informatics,Polish Academy of Sciences,
Ba\l{}tycka 5
\\
Gliwice, 44-100, Poland,
\\
\{michal,przemg,gawron\}@iitis.pl
}
\maketitle

\begin{abstract}
The goal of this work is the identification of humans based on motion data in
the form of natural hand gestures. In this paper, the identification problem is
formulated as classification with classes corresponding to persons' identities,
based on recorded signals of performed gestures. The identification performance
is examined with a database of twenty-two natural hand gestures recorded with
two types of hardware and three state-of-art classifiers: Linear Discrimination
Analysis (LDA), Support Vector machines (SVM) and k-Nearest Neighbour (k-NN).
Results show that natural hand gestures allow for an effective human
classification. 
\\
\\
\noindent Keywords:
gestures; biometrics; classification; human identification; LDA; k-NN; SVM 
\end{abstract}


\section{Introduction}
With a~widespread use of simple motion-tracking devices e.g. Nintendo Wii
Remote\texttrademark or accelerometer units in cell phones, the importance of
motion-based interfaces in Human-Computer Interaction (HCI) systems has become
unquestionable. Commercial success of early motion-capture devices led to the
development of more robust and versatile acquisition systems, both mechanical,
e.g. Cyberglove Systems Cyberglove\texttrademark,  Measurand
ShapeWrap\texttrademark, DGTech DG5VHand\texttrademark{} and optical  e.g.
Microsoft Kinect\texttrademark, Asus WAVI Xtion\texttrademark.  Also, the
interest in the analysis of a human motion itself 
\cite{McNeill:1992}, \cite{Quek:2002}, \cite{BergmannKopp:2010} 
has increased in the past few years.

While problems related to gesture recognition received much attention, an
interesting yet less explored problem is the task of recognising a human based
on his gestures. This problem has two main applications: the first one is the
creation of a gesture-based biometric authentication system, able to verify
access for authenticated users. The other task is related to personalisation of
applications with a motion component. In such scenario an effective classifier
is required to recognise between known users.

The goal of our experiment is to classify humans based on motion data in the
form of natural hand  gestures. Today's simple motion-based interfaces usually
limit users' options to a subset of artificial, well distinguishable gestures or
just detection of the presence of body motion. We argue that an interface should
be perceived by the users as natural and adapt to their needs. While modern motion-capture systems provide accurate recordings of human
body movement, creation of a HCI interface based on acquired data is not a
trivial task. Many popular gestures are ambiguous thus the meaning of a gesture
is usually not obvious for an observer and requires parsing of a complex
context. There are differences in body movement during the execution of a
particular gesture performed by different subjects or even in subsequent
repetitions by the same person. Some gestures may become unrecognisable with
respect to a particular capturing device, when important motion components are
unregistered, due to device limitations or its suboptimal calibration. We aim to
answer the question if high-dimensional hand motion data is distinctive enough
to provide a basis for personalisation component in a system with motion-based
interface.

In our works we concentrated on hand gestures, captured with two mechanical
motion-capture systems. Such approach allows to experiment with reliable
multi-source data, obtained directly from the device, without additional
processing. We used a gesture database of twenty two natural gestures performed
by a number of participants with varying execution speeds. The gesture database
is described in \cite{Glomb:2011}. We compare the effectiveness of three
established classifiers namely Linear Discrimination Analysis (LDA), Support
Vector machines (SVM) and k-Nearest Neighbour (k-NN). 

The following experiment scenarios are considered in this paper:
\begin{itemize}
\item Human recognition based on the performance of one selected gesture (e.g. 
`waving a hand',`grasping an object'). User must perform one specified gesture 
to be identified.
\item The scenario when instead of one selected gesture, a set of multiple 
gestures is used both for training and for testing. User must perform one of 
several gestures to be identified.
\item The scenario when different gestures are used for training and for 
testing of the classifier. User is identified based on one of several gestures, 
none of which were used for training the classifier.
\end{itemize}

The paper is organized as follows: Section 2 (Related work) presents a selection
of works on similar subjects, Section 3 (Method) describes the experiment,
results are presented in Section 4 (Results), along with authors' remarks on the
subject.

\section{Related work}
Existing approaches  to the creation of an HCI Interface that are based on
dynamic hand gestures can be categorized according to: the motion data gathering
method, feature selection, the pattern classification technique and the
domain of application.

Hand data gathering techniques can be divided into: device-based, where
mechanical or optical sensors are attached to a glove, allowing for measurement
of finger flex, hand position and acceleration, e.g.  \cite{Zhang:2009}, and
vision-based, when hands are tracked based on the data from optical sensors e.g.
\cite{Lahamy:2010}. A survey of glove-based systems for motion data
gathering, as well as their applications can be found in 
\cite{Dipietro:2008}, while \cite{Berman:2011} provides a comprehensive
analysis of the integration of various sensors into gesture recognition systems.

While non-invasive vision-based methods for gathering hand movement data are
popular, device-based techniques receive attention due to widespread use of
motion sensors in mobile devices. For example \cite{WandAccel:2006} presents
a high performance, two-stage recognition algorithm for acceleration signals,
that was adapted in Samsung cell phones.

Extracted features may describe not only the motion of hands but also their
estimated pose. A review of literature regarding hand pose estimation is
provided in \cite{Erol:2007}. Creation of a gesture model can be performed
using multiple approaches including Hidden Markov Models e.g. 
\cite{Mitra:2007} or Dynamic Bayesian Networks e.g. \cite{Xu:2006}. For
hand gesture recognition, application domains include: sign language recognition
e.g. \cite{Cooper_Visual:2011}, robotic and computer interaction e.g.
\cite{Lee:2006}, computer games e.g. \cite{Chan:2009} and virtual reality
applications e.g. \cite{Xu:2006}.

Relatively new application of HCI elements are biometric technologies aimed to
recognise a person based on their physiological or behavioural characteristic. A
survey of behavioural biometrics is provided in \cite{Yampolskiy:2008} where
authors examine types of features used to describe human behaviour as well as
compare accuracy rates for verification of users using different behavioural
biometric approaches. Simple gesture recognition may be applied for
authentication on mobile devices e.g. in \cite{Liu:2009} authors present a
study of light-weight user authentication system using an accelerometer while a
multi-touch gesture-based authentication system is presented in
\cite{Sae-Bae:2012}. Typically however, instead of hand motion more reliable
features like  hand layout \cite{Adan:2008} or body gait \cite{2003:Kale}
are employed. 

Despite their limitations, linear classifiers \cite{Koronacki:2005} proved to
produce good results for many applications, including face recognition
\cite{Wang:2004} and speech detection \cite{Martin:2001}. In
\cite{Rasamimanana:2006} LDA is used for the estimation of consistent
parameters to three model standard types of violin bow strokes. Authors show
that such gestures can be effectively presented in the bi-dimensional space.  In
\cite{Moiz:2011}, the LDA classifier was compared with neural networks (NN)
and focused time delay neural networks (TDNN) for gesture recognition based on
data from a 3-axis accelerometer. LDA gave similar results to the NN approach,
and the TDNN technique, though computationally more complex, achieved better
performance. An analysis of LDA and the PCA algorithm, with a discussion about
their performance for the purpose of object recognition is provided in
\cite{Martinez:2001}. SVM and k-NN classifiers were used in
\cite{Zhang:2006} for the purpose of visual category recognition. A
comparison of the effectiveness of these method is classification of human gait
patterns is provided in \cite{Sudha:2012}.

Thorough analysis of a gesture dataset used in the experiments, along with a
discussion on the benefits of naturality of HCI interface elements can be found
in \cite{Glomb:2012}. PCA analysis of the same dataset together with
visualization of eigengestures can be found in \cite{GawronEtAll:2011}.

\section{User identification using classification of natural gestures}
The general idea is to recognise a gesture performer. Experiment data consist of
data from `IITiS Gesture Database' that contains natural gestures performed by
multiple participants. Three classifiers will be used. PCA will be performed on
the data to reduce its dimensionality.

\subsection{Experiment data}

\begin{figure}[h]
 \centering
\subfigure[]{\includegraphics[scale=1.0]{gesture.jpg}}
\subfigure[]{\includegraphics[scale=1.0]{gesture2.jpg}}
  \caption{Scenes from recording of `IITiS Gesture Database'. Left DG5VHand glove, right CyberGlove/CyberForce system.}
  \label{fig:gestures_intro}
\end{figure}

A set of twenty-two natural hand gesture classes from `IITiS Gesture
Database'\footnote{The database can be downloaded from
http://gestures.iitis.pl/} \cite{Glomb:2011},
Tab.~\ref{id:table:gestureList}, was used in the experiments. Gestures used in
this experiments were recorded with two types of hardware (see Fig.
\ref{fig:gestures_intro}). First one was the DGTech 
DG5VHand\texttrademark{}\footnote{http://www.dg-tech.it/vhand} motion capture 
glove \cite{DG5VHand}, containing 5 finger bend sensors (resistance type),
and  three-axis accelerometer producing three acceleration and two orientation 
readings. Sampling frequency was approximately 33 Hz. The second one was
Cyberglove Systems CyberGlove\texttrademark{} 
\footnote{http://www.cyberglovesystems.com/products/cyberglove-ii/overview} 
with a CyberForce\texttrademark{} System for position and  orientation
measurement. The device produces 15 finger bend, three position  and four
orientation readings with a frequency of approximately 90 Hz.

\newcounter{gestureCounter}\stepcounter{gestureCounter}
\newcommand{\gc}{\arabic{gestureCounter}\stepcounter{gestureCounter}}
\ctable[
  star,
  cap = Gesture list,
  caption = The gesture list used in experiments. Source: 
  \cite{Glomb:2011} , 
  label = id:table:gestureList,
]{cllll}{
  \tnote{We use the terms `symbolic', `deictic', and `iconic' based on McNeill 
\& Levy \cite{McNeill:1992} classification, supplemented with a category of 
`manipulative' gestures (following \cite{Quek:2002})}
  \tnote[b]{Significant motion components: T-hand translation, R-hand rotation, 
F-individual finger movement}
  \tnote[c]{This gesture is usually accompanied with a specific object 
(deictic) reference}
}{\FL
& Name & Class\tmark & Motion\tmark[b] & Comments\ML
\gc & \emph{A-OK} & symbolic & F & common `okay' gesture\NN
\gc & \emph{Walking} & iconic & TF & fingers depict a walking person\NN
\gc & \emph{Cutting} & iconic & F & fingers portrait cutting a sheet of paper\NN
\gc & \emph{Showe away} & iconic & T & hand shoves avay imaginary object\NN
\gc & \emph{Point at self} & deictic & RF & finger points at the user\NN
\gc & \emph{Thumbs up} & symbolic & RF & classic `thumbs up' gesture\NN
\gc & \emph{Crazy} & symbolic & TRF & symbolizes `a crazy person'\NN
\gc & \emph{Knocking} & iconic & RF & finger in knocking motion\NN
\gc & \emph{Cutthroat} & symbolic & TR & common taunting gesture\NN
\gc & \emph{Money} & symbolic & F & popular `money' sign\NN
\gc & \emph{Thumbs down} & symbolic & RF & classic `thumbs down' gesture\NN
\gc & \emph{Doubting} & symbolic & F & popular flippant `I doubt'\NN
\gc & \emph{Continue} & iconic\tmark[c] & R & circular hand motion `continue', 
`go on'\NN
\gc & \emph{Speaking} & iconic & F & hand portraits a speaking mouth\NN
\gc & \emph{Hello} & symbolic\tmark[c] & R & greeting gesture, waving hand 
motion\NN
\gc & \emph{Grasping} & manipulative & TF & grasping an object\NN
\gc & \emph{Scaling} & manipulative & F & finger movement depicts size change\NN
\gc & \emph{Rotating} & manipulative & R & hand rotation depicts object 
rotation\NN
\gc & \emph{Come here} & symbolic\tmark[c] & F & fingers waving; `come here'\NN
\gc & \emph{Telephone} & symbolic & TRF & popular `phone' depiction\NN
\gc & \emph{Go away} & symbolic\tmark[c] & F & fingers waving; `go away'\NN
\gc & \emph{Relocate} & deictic & TF & `put that there'\LL
}

During the experiment, each participant was sitting at the table with the motion
capture glove on their right hand. Before the start of the experiment, the hand
of the participant was placed on the table in a fixed initial position. At the
command given by the operator sitting in front of the participant, the
participant performed the gestures. Each gesture was performed six times at
natural pace, two times at a rapid pace and two times at a slow pace. Gestures
number 2, 3, 7, 8, 10, 12, 13, 14, 15, 17, 18, 19, 21 are periodical and in
their case a single performance consisted of three periods. The termination of
data acquisition process was decided by the operator.

\subsection{Dataset exploration}

\begin{figure}[h]
 \centering
\subfigure[DG5VHand\texttrademark]{\includegraphics[scale=1]{scatter_dg5vhand_1}}
\subfigure[CyberGlove\texttrademark ]{\includegraphics[scale=1]{scatter_cyberglove_1}}
  \caption{Visualisation of data separability gestures dataset with use of LDA. The original data is projected on  canonical vectors.}
  \label{fig:lda_projection}
\end{figure}

Figure \ref{fig:lda_projection} presents the result of performing LDA (further described in subsection \ref{LDA}) on the
dataset: projection of the dataset on the first two components of 
 for both devices. It can be observed that many
gestures are linearly separable.  In the majority of visible gesture classes,
elements are centred around their respectable mean, with an almost uniform
variance. Potential conflicts for small number of gestures may be observed for
local regions of the projected data space.

\subsection{Data preprocessing}
\label{id:section:preprocessing}
A motion capture recording performed with a~device with  sensors
generates a time sequence of vectors . For the 
purpose of our work each recording was linearly interpolated and re-sampled to 
 samples, generating data matrices
, where  enumerates  recordings.
Then data matrices were normalized by computing the
t-statistics 

where ,  are mean and standard deviation for a given 
sensor  taken over all  recordings in the database.

Subsequently every matrix  for was vectorized row-by-row, so that
it was transformed into data vector 

belonging to , . 
These data vectors were organised into  (for DG5VHand) and  (for
Cyberglove) classes  corresponding to participants registered
with each device.
\subsection{PCA}
Principal Component Analysis \cite{Wall:2003} may be defined as follows. 
Let 

be the data matrix, where  are data vectors with zero
empirical mean. The associated covariance matrix is given by
. 
By performing eigenvalue decomposition of 
 such that 
eigenvalues   of  are ordered in 
descending order

one obtains the sequence of principal components 
 which are columns of 
 \cite{Wall:2003}.
One can form a feature vector  of dimension  by 
calculating 

\subsection{LDA} \label{LDA}
Linear Discriminant Analysis--thoroughly presented in
\cite{Koronacki:2005}--is a supervised, discriminative technique producing an
optimal linear classification function, which transforms the data from
 dimensional space
 into a lower-dimensional classification space .

Let the \emph{between-class scatter matrix}  be defined as follows

where  denotes mean of class means  i.e.	
, and  is the 
number of samples in class . 
Let \emph{within-class scatter matrix}  be

where  is the total number of the samples in all classes.

The eigenvectors of matrix  ordered by their respective
eigenvalues are called the canonical vectors. By selecting first  canonical
vectors and arranging them row by row as the projection matrix 
 any vector  can be projected onto a
lower-dimensional feature space . Using LDA one can effectively 
apply simple classifier e.g. for -class problem. A vector  is 
classified to class  if following inequality is observed 
for all .  denotes Euclidean norm.

Note that when the amount of available data is limited, LDA technique may result
in the matrix  that is singular.
In this case one can use Moore-Penrose pseudoinverse \cite{Tian:88}.
Matrix  is replaced by Moore-Penrose pseudoinverse matrix
 and canonical vectors are eigenvectors of the matrix
.

\subsection{-NN} \label{KNN}
The -Nearest Neighbour (-NN) method \cite{Hechenbichler:2004}
classifies the sample by assigning it to the  most frequently represented class
among k nearest samples. It may be described as follows. Let 

be a training set where  denotes class labels, and
, are feature vectors. For a nearest neighbour 
classification, given a new observation , first a nearest element 
 of a learning set is determined 

with Euclidean distance 

and resulting class label is .

Usually, instead of only one observation from ,  most similar elements are
considered. Therefore, counts of class labels for 
 are determined for each class

where  denotes Dirac delta.
The class label is determined as most common class present in the results


Note that in case of multiple classes or single class and even  there may be
a tie in the top class counts; in that case results may be dependent on data
order and behaviour of  implementation.

\subsection{SVM} \label{SVM}
Support Vector machines (SVM) presented in \cite{Byun:2002} are supervised
learning methods based on the principle of constructing a hyperplane separating
classes with the largest margin of separation between them. The margin is the
sum of distances from the hyperplane to closest data points of each class. These
points are called Support Vectors. SVMs  can be described as follows. Let 

be a set of linearly separable training samples where  denotes 
class labels. We assume the existence of a -dimensional hyperplane (
denotes dot product)

 separating  in .

The distance between separating hyperplanes satisfying  and  is 
. The optimal separating hyperplane can be found by 
minimising 

under the constraint

for all .

When the data is not linearly separable, a hyperplane that maximizes the margin
while minimizing a quantity proportional to the misclassification errors is
determined by introducing positive slack variables  in the equation
\ref{equation:svm:1}, wchich becomes:

and the equation (\ref{equation:svm:2}) is changed into:

where  is a penalty factor chosen by the user, that controls the trade off 
between the margin width and the misclassification errors. 

When the decision function is not a linear function of the data, an initial
mapping  of the data into a higher dimensional Euclidean space  is
performed as  and the linear classification problem
is formulated in the new space. The training algorithm then only depends on the
data through dot product in  of the form .  The Mercer's theorem \cite{Burges:1998} allows to
replace  by a positive definite
symmetric kernel function , e.g. Gaussian
radial-basis function , for .

\section{Results}
Our objective was to evaluate the performance of user identification based on
performed gestures. To this end, in our experiment class labels are assigned to
subsequent humans performing gestures (performers' ids were recorded during
database acquisition). Three experiment scenarios were investigated, differing
by the range of gestures used for recognition. The three  classification methods
described before were used, evaluated in two-stage -fold cross validation
scheme.

\subsection{Scenarios}
Three scenarios related to data labelling were prepared:
\begin{itemize}
\item Scenario A. Human classification using specific gesture. Each gesture was
treated as a separated case, and a classifier was created and verified using
samples from this particular gesture.
\item Scenario B. Human classification using a set of known gestures. Data from
multiple gestures was used in the experiment. Whenever the data was divided into
a teaching and testing subset, proportional amount of samples for each gesture
were present in both sets.
\item Scenario C. Human classification using separate gesture sets. In this
scenario the data from multiple gestures was used, similarly to Scenario B.
However, teaching subset was created using different gestures than a testing
subset.
\end{itemize}

\subsection{Experiments}

The three classifiers were used, with the following parameter ranges:
\begin{itemize}
\item LDA, with number of features ;
\item -NN, with number of neighbours ;
\item SVM, with Radial Basis Function (RBF) and 
.
\end{itemize} 

Common parameters values found by cross-validation: 
\begin{itemize}
\item LDA,  features
\item -NN,  neighbours for Scenarios B,C,  for Scenario C;
\item SVM, 
 .
\end{itemize} 

The parameter selection and classifier performance evaluation was performed by
splitting the available data into training and testing subset in two-stage
-fold cross validation (c.v.) scheme, with . Inner c.v. stage
corresponds to grid search parameter optimization and model selection. The outer
stage corresponds to final performance evaluation. The PCA was performed on the
whole data set before classifier training. The amount of principal components
was chosen empirically as .

\subsection{Results and discussion}

\begin{table}[h]
\centering
\begin{tabular}{ccccccc}
\toprule
Scenario&\multicolumn{6}{c}{Accuracy}\\ 
\cmidrule(r){2-7}
&\multicolumn{3}{c}{DG5VHand}&\multicolumn{3}{c}{CyberGlove}\\ 
\cmidrule(r){2-4}\cmidrule(r){5-7}
&LDA&k-NN&SVC&LDA&k-NN&SVC\\
\midrule
A&&&&&&\\
B&&&&&&\\
C&&&&&&\\
\bottomrule
\end{tabular} 
\caption{Classification accuracy (\%) for three considered scenarios.}
\label{id:table:mean_accuracy}
\end{table}

The accuracy of the classifiers for three discussed scenarios is presented in
Tab. \ref{id:table:mean_accuracy}. Confusion matrices for experiments B,C are
presented  on Figure \ref{fig:confusion_matrices}. High classification accuracy
can be observed for scenarios  and  when a classifier is provided with
training data for specific gesture. In scenario C, however, the accuracy
corresponds to a situation when a performer is recognised based on an unknown 
gesture. While the classification accuracy is lower than in previous scenarios,
it should be noted that the classifier was created using a limited amount of
high-dimensional data. The difference between the accuracy for both devices can
be explained by significantly higher precision of a CyberGlove device, where
hand position is captured using precise rig instead of an array of
accelerometers. 

\begin{table}[ht]
\centering
\begin{tabular}{c| c c c c}
& & LDA & -NN & SVC\\ \multirow{2}{*}{B}
&{\begin{sideways}\parbox{5cm}{\centering DG5VHand}\end{sideways}}
&\includegraphics[scale=0.6]{1_cm}
&\includegraphics[scale=0.6]{2_cm}
&\includegraphics[scale=0.6]{3_cm}\\ 
&{\begin{sideways}\parbox{5cm}{\centering CyberGlove}\end{sideways}}
&\includegraphics[scale=0.6]{4_cm}
&\includegraphics[scale=0.6]{5_cm}
&\includegraphics[scale=0.6]{6_cm}\\ 
\hline 
\multirow{2}{*}{C}
&{\begin{sideways}\parbox{5cm}{\centering DG5VHand}\end{sideways}}
&\includegraphics[scale=0.6]{7_cm}
&\includegraphics[scale=0.6]{8_cm}
&\includegraphics[scale=0.6]{9_cm}\\ 
&{\begin{sideways}\parbox{5cm}{\centering CyberGlove}\end{sideways}}
&\includegraphics[scale=0.6]{10_cm}
&\includegraphics[scale=0.6]{11_cm}
&\includegraphics[scale=0.6]{12_cm} 
\end{tabular}
\caption{Confusion matrices for scenarios B and C, all classifiers and both devices.}
\label{fig:confusion_matrices}
\end{table} 

Results of the experiment show that even linear classifiers can be successfully
employed for recognition of human performers based on their natural gestures.
Relatively high accuracy for experiment C indicates that the general
characteristics of a human natural body movement is highly discriminative, even
for different gesture patterns. While mechanical devices used in experiments
provide accurate measurements of body movements, they may be replaced by less
cumbersome data gathering device e.g. Microsoft Kinect\texttrademark. 

\section{Conclusion}
Experiments confirm that natural hand gestures are highly discriminative and
allow for an accurate classification of their performers. Applications of such
solution allow e.g. to personalise tools and interfaces to suit the needs of
their individual users.  However, a separate problem lies in the detection of
particular gesture performer based on general hand motion. Such task requires
deeper understanding of motion characteristics as well as identification of
individual features of human motion.

\section*{Acknowledgements}
The work of M. Romaszewski and P. G{\l}omb has been partially supported  by the
Polish Ministry of Science and Higher Education project NN516482340
``Experimental station for integration and presentation of 3D views''. Work by
P. Gawron was partially suported by  Polish Ministry of Science and Higher
Education project NN516405137 ``User interface based on natural gestures  for
exploration of virtual 3D spaces''. Open source machine learning library
scikit-learn \cite{scikit-learn} was used in experiments. We would like to
thank Z.~Pucha{\l}a and J. Miszczak for fruitful discussions.


\begin{thebibliography}{10}
\bibitem{Adan:2008}
M.~Ad{\'a}n, A.~Ad{\'a}n, A.S. V{\'a}zquez, and R~Torres.
\newblock {Biometric verification/identification based on hands natural
  layout}.
\newblock {\em Image and Vision Computing}, 26(4):451--465, 2008.

\bibitem{BergmannKopp:2010}
K.~Bergmann and S.~Kopp.
\newblock {Systematicity and Idiosyncrasy in Iconic Gesture Use: Empirical
  Analysis and Computational Modeling}.
\newblock In S.~Kopp and I.~Wachsmuth, editors, {\em {Gesture in Embodied
  Communication and Human-Computer Interaction}}, pages 182--194. Springer,
  2010.

\bibitem{Berman:2011}
S.~Berman and H.~Stern.
\newblock {Sensors for Gesture Recognition Systems}.
\newblock {\em Systems, Man, and Cybernetics, Part C: Applications and Reviews,
  IEEE Transactions on}, PP(99):1--14, 2011.

\bibitem{Sudha:2012}
R.~Bhavani and L.R Sudha.
\newblock {Performance Comparison of SVM and kNN in Automatic Classification of
  Human Gait Patterns}.
\newblock {\em International Journal of Computers}, 6:19--28, 2012.

\bibitem{Burges:1998}
C.J.C. Burges.
\newblock {A Tutorial on Support Vector Machines for Pattern Recognition}.
\newblock {\em Data Min. Knowl. Discov.}, 2(2):121--167, June 1998.

\bibitem{Byun:2002}
Hyeran Byun and Seong-Whan Lee.
\newblock Applications of support vector machines for pattern recognition: A
  survey.
\newblock In {\em Proceedings of the First International Workshop on Pattern
  Recognition with Support Vector Machines}, SVM '02, pages 213--236, London,
  UK, UK, 2002. Springer-Verlag.

\bibitem{Chan:2009}
A.T.S. Chan, Hong~Va Leong, and Shui~Hong Kong.
\newblock {Real-time tracking of hand gestures for interactive game design}.
\newblock In {\em {Industrial Electronics, 2009. ISIE 2009. IEEE International
  Symposium on}}, pages 98--103, july 2009.

\bibitem{WandAccel:2006}
S-J. Cho, E.~Choi, W-C. Bang, J.~Yang, J.~Sohn, D.Y. Kim, Y-B. Lee, and S.~Kim.
\newblock {Two-stage Recognition of Raw Acceleration Signals for 3-D
  Gesture-Understanding Cell Phones}.
\newblock In {\em {Tenth International Workshop on Frontiers in Handwriting
  Recognition}}, 2006.

\bibitem{Cooper_Visual:2011}
H.~Cooper, B.~Holt, and R.~Bowden.
\newblock {Sign Language Recognition}.
\newblock In Thomas~B. Moeslund, Adrian Hilton, Volker Kr{\"u}ger, and Leonid
  Sigal, editors, {\em {Visual Analysis of Humans: Looking at People}},
  chapter~27, pages 539 -- 562. Springer, October 2011.

\bibitem{DG5VHand}
{DG5 VHand 2.0 OEM Technical Datasheet}.
\newblock Technical report, DGTech Engineering Solutions, November 2007.
\newblock Release 1.1.

\bibitem{Dipietro:2008}
L.~Dipietro, A.M. Sabatini, and P.~Dario.
\newblock {A Survey of Glove-Based Systems and Their Applications}.
\newblock {\em Systems, Man, and Cybernetics, Part C: Applications and Reviews,
  IEEE Transactions on}, 38(4):461--482, july 2008.

\bibitem{Erol:2007}
A.~Erol, G.~Bebis, M.~Nicolescu, R.D. Boyle, and X.~Twombly.
\newblock {Vision-based hand pose estimation: A review}.
\newblock {\em Comput. Vis. Image Underst.}, 108(1-2):52--73, October 2007.

\bibitem{GawronEtAll:2011}
P.~Gawron, P.~G{\l}omb, J.A. Miszczak, and Z.~Pucha{\l}a.
\newblock {Eigengestures for Natural Human Computer Interface}.
\newblock In T.~Czach{\'o}rski, S.~Kozielski, and U.~Sta{\'n}czyk, editors,
  {\em {Man-Machine Interactions 2}}, volume 103 of {\em {Advances in
  Intelligent and Soft Computing}}, pages 49--56. Springer Berlin / Heidelberg,
  2011.

\bibitem{Glomb:2011}
P.~G{\l}omb, M.~Romaszewski, S.~Opozda, and A.~Sochan.
\newblock {Choosing and modeling hand gesture database for natural user
  interface}.
\newblock In {\em {Proc. of GW 2011: The 9th International Gesture Workshop
  Gesture in Embodied Communication and Human-Computer Interaction}}, 2011.

\bibitem{Glomb:2012}
P.~G\l{}omb, M.~Romaszewski, S.~Opozda, and A.~Sochan.
\newblock Choosing and modeling the hand gesture database for a natural user
  interface.
\newblock In E.~Efthimiou, G.~Kouroupetroglou, and S.-E. Fotinea, editors, {\em
  Gesture and Sign Language in Human-Computer Interaction and Embodied
  Communication}, volume 7206 of {\em Lecture Notes in Computer Science}, pages
  24--35. Springer Berlin Heidelberg, 2012.

\bibitem{Hechenbichler:2004}
K.~Hechenbichler and K.~Schliep.
\newblock {Weighted k-nearest-neighbor techniques and ordinal classification}.
\newblock Technical report, Ludwig-Maximilians-Universit\"at M\"unchen,
  Institut f\"ur Statistik, 2004.

\bibitem{2003:Kale}
A.~Kale, N.~Cuntoor, N.~Cuntoor, B.~Yegnanarayana, R.~Chellappa, and A.N.
  Rajagopalan.
\newblock {Gait Analysis for Human Identification}.
\newblock In {\em {LNCS}}, pages 706--714, 2003.

\bibitem{Koronacki:2005}
J.~Koronacki and J.~{\'C}wik.
\newblock {\em {Statistical learning systems (in Polish)}}.
\newblock Wydawnictwa Naukowo-Techniczne, Warsaw, Poland, 2005.

\bibitem{Lahamy:2010}
H.~Lahamy.
\newblock {Real-time hand gesture recognition using range cameras}.
\newblock In {\em {CGC10}}, page~54, 2010.

\bibitem{Lee:2006}
S.-W. Lee.
\newblock {Automatic gesture recognition for intelligent human-robot
  interaction}.
\newblock In {\em {Automatic Face and Gesture Recognition, 2006. FGR 2006. 7th
  International Conference on}}, pages 645--650, april 2006.

\bibitem{Liu:2009}
Jiayang Liu, Lin Zhong, Jehan Wickramasuriya, and Venu Vasudevan.
\newblock {uWave: Accelerometer-based personalized gesture recognition and its
  applications}.
\newblock {\em Pervasive Mob. Comput.}, 5(6):657--675, December 2009.

\bibitem{Martin:2001}
A.~Martin, D.~Charlet, and L.~Mauuary.
\newblock {Robust speech/non-speech detection using LDA applied to MFCC}.
\newblock {\em Acoustics, Speech, and Signal Processing, IEEE International
  Conference on}, 1:237--240, 2001.

\bibitem{Martinez:2001}
A.M. Martinez and A.C. Kak.
\newblock {{PCA} versus {LDA}}.
\newblock {\em Pattern Analysis and Machine Intelligence, IEEE Transactions
  on}, 23(2):228--233, feb 2001.

\bibitem{McNeill:1992}
D.~McNeill.
\newblock {\em {Hand and Mind: What Gestures Reveal about Thought}}.
\newblock The University of Chicago Press, 1992.

\bibitem{Mitra:2007}
S.~Mitra and T.~Acharya.
\newblock {Gesture Recognition: A Survey}.
\newblock {\em Systems, Man, and Cybernetics, Part C: Applications and Reviews,
  IEEE Transactions on}, 37(3):311--324, may 2007.

\bibitem{Moiz:2011}
F.~Moiz, P.~Natoo, R.~Derakhshani, and W.D. Leon-Salas.
\newblock {A comparative study of classification methods for gesture
  recognition using a 3-axis accelerometer}.
\newblock In {\em {Neural Networks (IJCNN), The 2011 International Joint
  Conference on}}, pages 2479--2486, 31 2011-aug. 5 2011.

\bibitem{scikit-learn}
F.~Pedregosa, G.~Varoquaux, A.~Gramfort, V.~Michel, B.~Thirion, O.~Grisel,
  M.~Blondel, P.~Prettenhofer, R.~Weiss, V.~Dubourg, J.~Vanderplas, A.~Passos,
  D.~Cournapeau, M.~Brucher, M.~Perrot, and E.~Duchesnay.
\newblock {Scikit-learn: Machine Learning in {P}ython}.
\newblock {\em Journal of Machine Learning Research}, 12:2825--2830, 2011.

\bibitem{Quek:2002}
F.~Quek, D.~McNeill, R.~Bryll, S.~Duncan, X-F. Ma, C.~Kirbas, K.E. McCullough,
  and R.~Ansari.
\newblock {Multimodal human discourse: gesture and speech}.
\newblock {\em ACM Trans. Comput.-Hum. Interact.}, 9:171--193, September 2002.

\bibitem{Rasamimanana:2006}
N.~Rasamimanana, E.~Fl{\'e}ty, and F.~Bevilacqua.
\newblock {Gesture Analysis of Violin Bow Strokes}.
\newblock In S.~Gibet, N.~Courty, and J.-F. Kamp, editors, {\em {Gesture in
  Human-Computer Interaction and Simulation}}, volume 3881 of {\em {Lecture
  Notes in Computer Science}}, pages 145--155. Springer Berlin / Heidelberg,
  2006.

\bibitem{Sae-Bae:2012}
N.~Sae-Bae, K.~Ahmed, K.~Isbister, and N.~Memon.
\newblock {Biometric-rich gestures: a novel approach to authentication on
  multi-touch devices}.
\newblock In {\em {Proceedings of the 2012 ACM annual conference on Human
  Factors in Computing Systems}}, {CHI '12}, pages 977--986, New York, NY, USA,
  2012. ACM.

\bibitem{Tian:88}
Q.~Tian, Y.~Fainman, and Sing~H. Lee.
\newblock {Comparison of statistical pattern-recognition algorithms for hybrid
  processing. II. Eigenvector-based algorithm}.
\newblock {\em J. Opt. Soc. Am. A}, 5(10):1670--1682, Oct 1988.

\bibitem{Wall:2003}
M.E. Wall, A.~Rechtsteiner, and L.M. Rocha.
\newblock {Singular Value Decomposition and Principal Component Analysis}.
\newblock In D.~P. Berrar, W.~Dubitzky, and M.~Granzow, editors, {\em {A
  Practical Approach to Microarray Data Analysis}}, chapter~5, pages 91--109.
  Kluwel, Norwell, M.A., March 2003.

\bibitem{Wang:2004}
X.~Wang and X.~Tang.
\newblock {Random sampling {LDA} for face recognition}.
\newblock In {\em {Computer Vision and Pattern Recognition, 2004. {CVPR} 2004.
  Proceedings of the 2004 {IEEE} {C}omputer {S}ociety {C}onference on}},
  volume~2, pages II--259--II--265 Vol.2, june-2 july 2004.

\bibitem{Xu:2006}
D.~Xu.
\newblock {A Neural Network Approach for Hand Gesture Recognition in Virtual
  Reality Driving Training System of SPG}.
\newblock In {\em {Pattern Recognition, 2006. ICPR 2006. 18th International
  Conference on}}, volume~3, pages 519--522, 0-0 2006.

\bibitem{Yampolskiy:2008}
R.V. Yampolskiy and V.~Govindaraju.
\newblock {Behavioural biometrics --- a survey and classification}.
\newblock {\em Int. J. Biometrics}, 1(1):81--113, June 2008.

\bibitem{Zhang:2006}
H.~Zhang, A.C. Berg, M.~Maire, and J.~Malik.
\newblock {SVM-KNN: Discriminative Nearest Neighbor Classification for Visual
  Category Recognition}.
\newblock In {\em {Proceedings of the 2006 IEEE Computer Society Conference on
  Computer Vision and Pattern Recognition - Volume 2}}, {CVPR '06}, pages
  2126--2136, Washington, DC, USA, 2006. IEEE Computer Society.

\bibitem{Zhang:2009}
X.~Zhang, X.~Chen, W.-H. Wang, J.-H. Yang, V.~Lantz, and K.-Q. Wang.
\newblock {Hand gesture recognition and virtual game control based on 3D
  accelerometer and EMG sensors}.
\newblock In {\em {Proceedings of the 14th international conference on
  Intelligent user interfaces}}, {IUI '09}, pages 401--406, New York, NY, USA,
  2009. ACM.
\end{thebibliography}

\end{document}
