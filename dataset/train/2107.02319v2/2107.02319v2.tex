\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{fancyhdr}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    
\chead{\rmfamily\fontsize{9}{30}\selectfont 
MICCAI 2019}
\renewcommand{\headrulewidth}{0pt}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{svg}
\usepackage{xspace}
\widowpenalty10000
\clubpenalty10000
\usepackage[acronym]{glossaries}
\makeglossaries
\usepackage{booktabs} 
\usepackage{url}
\usepackage{color}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{array}
\usepackage[nolist,nohyperlinks]{acronym}
\usepackage{footnote}
\makesavenoteenv{tabular}
\acrodef{NASNETMobile}{ Neural Architecture Search Network}
\acrodef{ReLU}{Rectified linear Unit}
\acrodef{mIoU}{mean Intersection over Union}
\acrodef{AUC}{Area Under Curve}


\cfoot{\rmfamily\fontsize{9}{0}\selectfont 978-1-7281-4387-3/19/\$31.00~\copyright~2019 IEEE}


\begin{document}

\title{Binary Segmentation Task at Robust Medical Instrument Segmentation (ROBUST-MIS) Challenge 2019}
\input{author.tex}


\maketitle
\thispagestyle{fancy}

\begin{abstract}
Segmentation of laparoscopic instruments plays an important role in intraoperative segmentation and tracking of minimally invasive instruments. With the increase of publicly available datasets, hardware and multimedia and medical community involved to tackle the task, surgical vision is gaining popularity for segmenting and tracking the instruments. For the propose of segmenting and tracking the instruments, the paper presents a solution for Robust Medical Instrument Segmentation (ROBUST-MIS) challenge, which is the part of Endoscopic vision challenge. The proposed method is an encoder-decoder network that utilize the combination of pre-trained model,\ac{NASNETMobile}~\cite{zoph2018learning} and ResNet50~\cite{he2016deep}) which utilizes the weight trained on ImageNet~\cite{deng2009imagenet}. At the decoder part, we make use of transpose convolution, Residual block, batch normalization, \ac{ReLU}, dilated convolution, squeeze and excitation block respectively. Based on the result produced with the available dataset, we can say that the proposed method produces good segmentation results on the instrument segmentation task. \\

\end{abstract}

\begin{IEEEkeywords}
Instrument segmentation, minimally invasive surgery, robustness, generalization
\end{IEEEkeywords}

\section{Introduction}
A safe laparoscopic surgery ideally requires minimal invasiveness. In recent years, the machine learning algorithm is commonly used to learn the shape and appearance of the images/videos if enough labeled training dataset is available. Instrument segmentation would benefit in robot-assisted surgery. Moreover, it would be helpful to track the position of the instrument during surgery and act as a second pair of eyes for the specialists. Therefore, a computer-assisted system could potentially improve the segmentation rate and aid in developing a clinically acceptable system. Toward achieving the segmentation goal; in this task, we perform binary instrument segmentation with the help of training dataset that is annotated by the medical experts. 

\begin{figure} []
    \centering
    \includegraphics[width=5cm, height=6cm]{figures/nasnet_mobile.png}
    \caption{Proposed architecture.}
    \label{fig:learning_curve}
\end{figure}
\begin{figure} 
    \centering
    \includegraphics[width=5cm, height=5cm]{figures/figure1.png}
    \caption{Decoder Block} 
    \label{fig:learning_curve}
\end{figure}
\begin{figure} []
    \centering
    \includegraphics[width=5cm, height=6cm]{figures/residual.png}
    \caption{Residual block}
    \label{fig:learning_curve}
\end{figure}



\section{Methods}
\subsection{Data Pre-processing}
The Robust Medical Instrument Segmentation (ROBUST-MIS) \footnote{https://robustmis2019.grand-challenge.org} provide the training datasets. The dataset contains a separate image and video files, out of which we trained the model on the image files. We aim at building a lightweight model. Therefore, we have made use of only image dataset for training rather than taking the complete video dataset. The original size of the image is 960$\times$540. We split the dataset into 80-20\%. We use 80\% of dataset for training and rest 20\% is used for validation. The training dataset augmented and resized. We perform augmentation such as center crop, random crop, horizontal flip, vertical flip, scale augmentation, cutout, gray-scale, etc. The validation dataset is not augmented and is directly resized into 512$\times$256. The total size of the training dataset is 57444 and validation dataset is 1196. 


\begin{figure}
    \centering
    \includegraphics[width=8cm, height=3cm]{figures/mask_294.jpg}\\
    \vspace{1mm}
    \includegraphics[width=8cm, height=3cm]{figures/mask_72.jpg}\\
     \vspace{1mm}
    \includegraphics[width=8cm, height=3cm]{figures/mask_77.jpg}
    \caption{The results produced by the proposed model. (a) input image, (b) ground truth, (c) predicted masks}
    \label{fig:results}
\end{figure}


\begin{table}[]
\caption{The table shows the evaluation results of the experiments done to determine the performance of the proposed model.}
    \label{table:results}
    \def\arraystretch{1.5}
     \setlength\tabcolsep{10pt}
    \par\bigskip
    \centering
\begin{tabular}{|l|l|}
\hline
\textbf{Trainable parameters} & \textbf{Values} \\ \hline
Number of trainable parameters & 32,039,819 \\ \hline
\end{tabular}
\end{table}      
\subsection{Encoder Explanation}
In the proposed encoder network, we have used two popular ImageNet~\cite{deng2009imagenet} classification model-\ac{NASNETMobile},and ResNet50. Both these model uses weights from pre-trained ImageNet~\cite{deng2009imagenet}. Therefore, in the encoder section, we make use of cascade network of NASNETMobile and Resnet50. As we are using two pre-trained encoder network, we are concatenating the feature maps from both the encoder to form a single skip connection feature map. The skip connection within the network provides the easy propagation of the information in the lower layer. The skip connection within the network provides a means of bridging the gap between the pre-trained feature maps and the network output.

\subsection{Decoder Explanation}
The decoder block uses Transposed Convolution, Residual block, ASPP, and Squeeze and Excite block. The input to the decoder is passed through the transposed convolution, which doubles the resolution of the feature maps.  We concatenate the output of transposed convolution with the skip connection feature maps from both the encoder. Now, the output is passed through the two residual blocks, which enhanced the learning, and that has the potential to solve the degradation problem. After that, we apply a batch normalization and a ReLU non-linearity. We use an ASPP block that uses diluted convolution. These diluted convolutions increased the field view of the feature maps.  Batch normalization and a   ReLU non-linearity follow the above step. Then we apply squeeze and excitation block, which helps feature maps to focus on most essential parts. At last, we apply a convolution layer with sigmoid activation function to get the binary mask. 


\subsection{Loss function}
We have used dice loss as the loss function.

\subsection{Hyperparameters}
Table shows the hyperparameters used in the proposed model. 
\begin{table}[h]
\caption{The table shows the evaluation results of the experiments done to determine the performance of the proposed model.}
    \label{table:results}
    \def\arraystretch{1.5}
     \setlength\tabcolsep{10pt}
    \par\bigskip
    \centering
\begin{tabular}{|l|l|}
\hline
\textbf{Hyperparameter} & \textbf{Values} \\ \hline
Batch size & 12 \\ \hline
Optimizer & SGD \\ \hline
Initial learning rate & 1e-4 \\ \hline
Number of epoch & 100 \\ \hline
\end{tabular}
\end{table}
\begin{table}[h]
 \caption{The table shows the evaluation results of the experiments done to determine the performance of the proposed model.}
    \label{table:results}
    \def\arraystretch{1.5}
     \setlength\tabcolsep{10pt}
    \par\bigskip
    \centering
  \begin{tabular}{|l|l|l|l|l|}
\hline
\textbf{Method } & \textbf{Dice} & \textbf{mIOU} & \textbf{Recall} & \textbf{Precision}  \\ \hline
NasmobResNet & 0.8401 & 0.5792 & 0.7474   & 0.8903 \\ \hline
\end{tabular}
\end{table} 
\section{Results}
For the binary segmentation task, we used the method explained above and segmented instruments from the tissue. Figure \ref{fig:results} shows the input image frame, segmentation mask, and its corresponding output generated by the proposed deep learning model. To calculate the performance of the model, we have used the parametric metrics such as the soerensen-dice coefficient, \ac{mIoU}, accuracy, recall, precision, and \ac{AUC}. Table \ref{table:results} shows the results for the binary segmentation tasks. By incorporating a cascade of the pre-trained network at the down-sampling section and using the multiple advanced block in decoder section we were able to achieve a high performance. 

\section{Discussion}
We have tested the dataset with the different deep learning model. The results with the other methods were also promising. However, dice coefficient is the performance metrics for the competition. The proposed Rasnet model was chosen based on this criteria. The results of some of the cases were quite interesting because as a normal eye it is very difficult to see the instrument where as the machine learning models detected the model very easily. This example can be seen in the Figure 5.Here, we have also showed the worst performing cases where it was difficult for the model to predict.The examples are shown in figure. While observing the ground truth of the input images carefully, we found the some times the ground truth of the few images are good but not perfect. This might also be a small region why our model could not perform well with all the images. But we believe that this reason might be because of the annotation tool used to create the segmentation mask.  Some  of the input images used in the task are blurry. This might even be difficult for human to annotate. 

\section{Conclusion}
We proposed a novel method for instrument segmentation. The proposed method obtained promising results on the available dataset. We believe that further training the model for higher number of epoch and hyperparameter tuning could increase the performance same model. In future, we will apply post-processing steps  such as conditional random field could improve the results. 
\section*{Acknowledgment}
The project is funded by Simula Research Laboratory and partially funded by PRIVATON project. 

\bibliographystyle{IEEEtran}
\bibliography{references} 
\end{document}
