\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{fancyhdr}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    
\chead{\rmfamily\fontsize{9}{30}\selectfont 
MICCAI 2019}
\renewcommand{\headrulewidth}{0pt}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{svg}
\usepackage{xspace}
\widowpenalty10000
\clubpenalty10000
\usepackage[acronym]{glossaries}
\makeglossaries
\usepackage{booktabs} 
\usepackage{url}
\usepackage{color}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{array}
\usepackage[nolist,nohyperlinks]{acronym}
\usepackage{footnote}
\makesavenoteenv{tabular}
\acrodef{NASNETMobile}{ Neural Architecture Search Network}
\acrodef{ReLU}{Rectified linear Unit}
\acrodef{mIoU}{mean Intersection over Union}
\acrodef{AUC}{Area Under Curve}


\cfoot{\rmfamily\fontsize{9}{0}\selectfont 978-1-7281-4387-3/19/\\times\times$256. The total size of the training dataset is 57444 and validation dataset is 1196. 


\begin{figure}
    \centering
    \includegraphics[width=8cm, height=3cm]{figures/mask_294.jpg}\\
    \vspace{1mm}
    \includegraphics[width=8cm, height=3cm]{figures/mask_72.jpg}\\
     \vspace{1mm}
    \includegraphics[width=8cm, height=3cm]{figures/mask_77.jpg}
    \caption{The results produced by the proposed model. (a) input image, (b) ground truth, (c) predicted masks}
    \label{fig:results}
\end{figure}


\begin{table}[]
\caption{The table shows the evaluation results of the experiments done to determine the performance of the proposed model.}
    \label{table:results}
    \def\arraystretch{1.5}
     \setlength\tabcolsep{10pt}
    \par\bigskip
    \centering
\begin{tabular}{|l|l|}
\hline
\textbf{Trainable parameters} & \textbf{Values} \\ \hline
Number of trainable parameters & 32,039,819 \\ \hline
\end{tabular}
\end{table}      
\subsection{Encoder Explanation}
In the proposed encoder network, we have used two popular ImageNet~\cite{deng2009imagenet} classification model-\ac{NASNETMobile},and ResNet50. Both these model uses weights from pre-trained ImageNet~\cite{deng2009imagenet}. Therefore, in the encoder section, we make use of cascade network of NASNETMobile and Resnet50. As we are using two pre-trained encoder network, we are concatenating the feature maps from both the encoder to form a single skip connection feature map. The skip connection within the network provides the easy propagation of the information in the lower layer. The skip connection within the network provides a means of bridging the gap between the pre-trained feature maps and the network output.

\subsection{Decoder Explanation}
The decoder block uses Transposed Convolution, Residual block, ASPP, and Squeeze and Excite block. The input to the decoder is passed through the transposed convolution, which doubles the resolution of the feature maps.  We concatenate the output of transposed convolution with the skip connection feature maps from both the encoder. Now, the output is passed through the two residual blocks, which enhanced the learning, and that has the potential to solve the degradation problem. After that, we apply a batch normalization and a ReLU non-linearity. We use an ASPP block that uses diluted convolution. These diluted convolutions increased the field view of the feature maps.  Batch normalization and a   ReLU non-linearity follow the above step. Then we apply squeeze and excitation block, which helps feature maps to focus on most essential parts. At last, we apply a convolution layer with sigmoid activation function to get the binary mask. 


\subsection{Loss function}
We have used dice loss as the loss function.

\subsection{Hyperparameters}
Table shows the hyperparameters used in the proposed model. 
\begin{table}[h]
\caption{The table shows the evaluation results of the experiments done to determine the performance of the proposed model.}
    \label{table:results}
    \def\arraystretch{1.5}
     \setlength\tabcolsep{10pt}
    \par\bigskip
    \centering
\begin{tabular}{|l|l|}
\hline
\textbf{Hyperparameter} & \textbf{Values} \\ \hline
Batch size & 12 \\ \hline
Optimizer & SGD \\ \hline
Initial learning rate & 1e-4 \\ \hline
Number of epoch & 100 \\ \hline
\end{tabular}
\end{table}
\begin{table}[h]
 \caption{The table shows the evaluation results of the experiments done to determine the performance of the proposed model.}
    \label{table:results}
    \def\arraystretch{1.5}
     \setlength\tabcolsep{10pt}
    \par\bigskip
    \centering
  \begin{tabular}{|l|l|l|l|l|}
\hline
\textbf{Method } & \textbf{Dice} & \textbf{mIOU} & \textbf{Recall} & \textbf{Precision}  \\ \hline
NasmobResNet & 0.8401 & 0.5792 & 0.7474   & 0.8903 \\ \hline
\end{tabular}
\end{table} 
\section{Results}
For the binary segmentation task, we used the method explained above and segmented instruments from the tissue. Figure \ref{fig:results} shows the input image frame, segmentation mask, and its corresponding output generated by the proposed deep learning model. To calculate the performance of the model, we have used the parametric metrics such as the soerensen-dice coefficient, \ac{mIoU}, accuracy, recall, precision, and \ac{AUC}. Table \ref{table:results} shows the results for the binary segmentation tasks. By incorporating a cascade of the pre-trained network at the down-sampling section and using the multiple advanced block in decoder section we were able to achieve a high performance. 

\section{Discussion}
We have tested the dataset with the different deep learning model. The results with the other methods were also promising. However, dice coefficient is the performance metrics for the competition. The proposed Rasnet model was chosen based on this criteria. The results of some of the cases were quite interesting because as a normal eye it is very difficult to see the instrument where as the machine learning models detected the model very easily. This example can be seen in the Figure 5.Here, we have also showed the worst performing cases where it was difficult for the model to predict.The examples are shown in figure. While observing the ground truth of the input images carefully, we found the some times the ground truth of the few images are good but not perfect. This might also be a small region why our model could not perform well with all the images. But we believe that this reason might be because of the annotation tool used to create the segmentation mask.  Some  of the input images used in the task are blurry. This might even be difficult for human to annotate. 

\section{Conclusion}
We proposed a novel method for instrument segmentation. The proposed method obtained promising results on the available dataset. We believe that further training the model for higher number of epoch and hyperparameter tuning could increase the performance same model. In future, we will apply post-processing steps  such as conditional random field could improve the results. 
\section*{Acknowledgment}
The project is funded by Simula Research Laboratory and partially funded by PRIVATON project. 

\bibliographystyle{IEEEtran}
\bibliography{references} 
\end{document}
