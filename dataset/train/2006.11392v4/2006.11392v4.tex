\documentclass[runningheads]{llncs}
\usepackage[rgb,x11names]{xcolor}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{subfig}
\usepackage{hyperref}
\hypersetup{colorlinks=true,citecolor=blue}
\usepackage{graphicx}
\usepackage[]{algorithm2e}
\usepackage{mathrsfs}
\usepackage{bbding}
\usepackage{rotating}
\usepackage{multirow}
\usepackage{marginnote}
\usepackage{overpic}
\usepackage{colortbl}
\usepackage[labelfont=bf]{caption}
\usepackage{bbding}

\newcommand{\secref}[1]{ \ref{#1}}
\newcommand{\fdp}[1]{{\textcolor{blue}{#1}}}
\newcommand{\figref}[1]{Fig. \ref{#1}}
\newcommand{\tabref}[1]{Tab. \ref{#1}}
\newcommand{\supp}[1]{\textcolor{magenta}{#1}}

\definecolor{mygray}{gray}{.92}
\def\etal{{\em et al.}}
\def\ie{\emph{i.e.}}
\def\eg{\emph{e.g.}}
\def\etc{\emph{etc}}
\def\etal{{\em et al.~}}

\def\ourmodel{\textit{PraNet}}

\makeatletter
\newcommand\figcaption{\def\@captype{figure}\caption}
\newcommand\tabcaption{\def\@captype{table}\caption}
\makeatother



\newcommand\blfootnote[1]
{\begingroup
	\renewcommand\thefootnote{}
	\footnotetext{#1}\endgroup
}


\usepackage{graphicx}



\graphicspath{{./Imgs/}}
\DeclareGraphicsExtensions{.jpg,.pdf,.png}

\begin{document}
\title{PraNet: Parallel Reverse Attention Network \\for Polyp Segmentation}



\author{Deng-Ping Fan\inst{1} \and
Ge-Peng Ji\inst{2} \and
Tao Zhou\inst{1}   \and 
Geng Chen\inst{1}  \and  \\
Huazhu Fu\inst{1}  \Envelope \and
Jianbing Shen\inst{1} \Envelope \and
Ling Shao          \inst{3,1}
}



\institute{ Inception Institute of Artificial Intelligence, Abu Dhabi, UAE.\\
 School of Computer Science, Wuhan University, Hubei, China. \\
 Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE. \\
{\tt \{huazhu.fu, jianbing.shen\}@inceptioniai.org}\\
{\tt \href{https://github.com/DengPingFan/PraNet}{https://github.com/DengPingFan/PraNet}}
}

\authorrunning{D.-P. Fan et al.}

\titlerunning{PraNet: Parallel Reverse Attention Network for Polyp Segmentation}



\maketitle              \begin{abstract}
Colonoscopy is an effective technique for detecting colorectal polyps, which are highly related to colorectal cancer.
In clinical practice, segmenting polyps from colonoscopy images is of great importance since it provides valuable information for diagnosis and surgery.
However, accurate polyp segmentation is a challenging task, for two major reasons: (i) the same type of polyps has a diversity of size, color and texture; and (ii) the boundary between a polyp and its surrounding mucosa is not sharp. 
To address these challenges, we propose a parallel reverse attention network (\ourmodel) for accurate polyp segmentation in colonoscopy images.
Specifically, we first aggregate the features in high-level layers using a parallel partial decoder (PPD). Based on the combined feature, we then generate a global map as the initial \textit{\textbf{guidance area}} for the following components.
In addition, we mine the \textit{\textbf{boundary cues}} using the reverse attention (RA) module, which is able to establish the relationship between areas and boundary cues.
Thanks to the recurrent cooperation mechanism between areas and boundaries, our \ourmodel~is capable of calibrating some misaligned predictions, improving the segmentation accuracy.
Quantitative and qualitative evaluations on five challenging datasets across six metrics show that our \ourmodel~improves the segmentation accuracy significantly, and presents a number of advantages in terms of generalizability, and real-time segmentation efficiency (\textbf{50fps}). 


\keywords{Colonoscopy \and Polyp segmentation \and Colorectal cancer}
\end{abstract}
\section{Introduction}
Colorectal cancer (CRC) is the third most common type of cancer around the world \cite{silva2014toward}. Therefore, preventing CRC by screening tests and removal of preneoplastic lesions (colorectal adenomas) is very critical and has become a worldwide public health priority.
Colonoscopy is an effective technique for CRC screening and prevention since it can provide the location and appearance information of colorectal polyps, enabling doctors to remove these before they develop into CRC. A number of studies have shown that early colonoscopy has contributed to a 30\% decline in the incidence of CRC \cite{haggar2009colorectal}.
Thus, in a clinical setting, accurate polyp segmentation is of great importance. It is a challenging task, however, due to two major reasons. First, the polyps often vary in appearance, \eg, size, color and texture, even if they are of the same type. Second, in colonoscopy images, the boundary between a polyp and its surrounding mucosa is usually blurred and lacks the intense contrast required for segmentation approaches. These issues result in the inaccurate segmentation of polyps, and sometimes even cause the missing detection of polyps.
Therefore, an automatic and accurate polyp segmentation approach capable of detecting all possible polyps at an early stage is of great significance in the prevention of CRC \cite{jia2019wireless}.







Among the various polyp segmentation methods, 
the early learning-based methods rely on extracted hand-crafted features \cite{mamonov2014automated,tajbakhsh2015automated}, such as color, texture, shape, appearance, or a combination of these features. These methods are usually trained a classifier to distinguish a polyp from its surroundings.  
However, these models often suffer from a high miss-detection rate. The main reason is that the representation capability of hand-crafted features is quite limited when it comes to dealing with the high intra-class variations of polyps and low inter-class variations between polyps and hard mimics \cite{yu2016integrating}. 
Recently, numerous deep learning based methods have been developed for polyp segmentation~\cite{yu2016integrating,zhang2018polyp}. Although progress has been made by these methods, they only detect polyps using a bounding boxes, thus failing to locate accurate boundaries of polyps. To address this issue, Brandao \etal \cite{brandao2017fully} employed an FCN with a pre-trained model to identify and segment polyps. Akbari \etal \cite{akbari2018polyp} utilized a modified version of FCN to improve the accuracy of polyp segmentation.
Inspired by the success of the U-Net~\cite{ronneberger2015u} applied in biomedical image segmentation, U-Net++~\cite{zhou2018unetplus} and ResUNet++~\cite{jha2019resunetplus} were employed for polyp segmentation and obtained promising performance. These methods focus on segmenting the whole area of the polyp, but they ignore the area-boundary constraint, which is very critical for enhancing the segmentation performance. To this end, Psi-Net~\cite{murugesan2019psi} utilized area and boundary information simultaneously in polyp segmentation, but the relationship between the area and boundary was not fully captured. Besides, Fang \etal \cite{fang2019selective} proposed a three-step selective feature aggregation network with area and boundary constraints for polyp segmentation. This method \textit{explicitly} considers the dependency between areas and boundaries and obtains good results with additional edge supervision; however, it is time-consuming (20 hours) and easily corrupted with over-fitting.







In this paper, we propose a novel deep neural network, called \textbf{P}arallel \textbf{R}everse \textbf{A}ttention \textbf{Net}work (\textbf{\ourmodel}), for the polyp segmentation task.
Our motivation stems from the fact that, during polyp annotation, clinicians first roughly locate a polyp and then accurately extract its silhouette mask according to the local features. We therefore argue that the area and boundary are two key characteristics that distinguish normal tissues and polyps.
Different from \cite{fang2019selective}, we first predict coarse areas and then \textit{implicitly} model the boundaries by means of reverse attention.
There are three advantages to this strategy, including better learning ability, improved generalization capability, and higher training efficiency. Please refer to our experiments (\secref{sec:Experiments}) for more details.
In a nutshell, our contributions are threefold. 
(1) We present a novel deep neural network for real-time and accurate polyp segmentation. By aggregating features in high-level layers using a parallel partial decoder (PPD), the combined feature takes contextual information and generates a global map as the initial \textit{\textbf{guidance area}} for the subsequent steps. To further mine the \textit{\textbf{boundary cues}}, we leverage a set of recurrent reverse attention (RA) modules to establish the relationship between areas and boundary cues. Due to this  recurrent cooperation mechanism between areas and boundaries, our model is capable of calibrating some misaligned predictions. 
(2) We introduce several novel evaluation metrics for polyp segmentation and present a comprehensive benchmark for existing SOTA models that are publicly available.
(3) Extensive experiments demonstrate that the proposed \ourmodel~outperforms most cutting-edge models and advances the SOTAs by a large margin, on five challenging datasets, with real-time inference and shorter training time. 







\begin{figure*}[t!]
	\centering
\begin{overpic}[width=\textwidth]{framework-final-min}
    \end{overpic}
	\caption{Overview of the proposed \ourmodel,~which consists of three reverse attention modules with a parallel partial decoder connection. See \secref{sec:Methods} for details.}
    \label{fig:Framework}
\end{figure*}

\section{Method}\label{sec:Methods}
\figref{fig:Framework} shows our \ourmodel, which utilizes a  parallel partial decoder to generate the high-level semantic global map and a set of reverse attention modules for accurate polyp segmentation from the colonoscopy images. Each component will be elaborated as follows. 

\subsection{Feature Aggregating via Parallel Partial Decoder}\label{sec:PPD}
Current popular medical image segmentation networks usually rely on a U-Net~\cite{ronneberger2015u} or a U-Net like network (\eg, U-Net++~\cite{zhou2018unetplus}, ResUNet~\cite{zhang2018road}, \etc). 
These models are essentially encoder-decoder frameworks, which typically aggregate \textit{all} multi-level features extracted from CNNs. As demonstrated by Wu~\etal\cite{wu2019cascaded}, compared with high-level features, low-level features demand more computational resources due to their larger spatial resolutions, but contribute less to performance.
Motivated by this observation, we propose to aggregate high-level features with a \textbf{\textit{parallel partial decoder}} component. More specifically, for an input polyp image  with size , five levels of features  with resolution  can be extracted from Res2Net-based~\cite{pami20Res2net} backbone network.
Then, we divide  features into low-level features  and high-level features . We introduce the partial decoder ~\cite{wu2019cascaded}, a new SOTA decoder component, to aggregate the high-level features with a paralleled connection. The partial decoder feature is computed by , and we can obtain a global map .

\subsection{Reverse Attention Module}
In a clinical setting, doctors first roughly locate the polyp region, and then carefully inspect local tissues to accurately label the polyp.
As discussed in \secref{sec:PPD}, our global map  is derived from the deepest CNN layer, which can only capture a relatively rough location of the polyp tissues, without structural details (see \figref{fig:Framework}).
To address this issue, we propose a principle strategy to progressively mine discriminative polyp regions through an erasing foreground object manner~\cite{wei2017object,chen2018reverse}. Instead of aggregating features from all levels like in~\cite{chen2018reverse,Gu2019,ETNet2019,AGNet}, we propose to adaptively learn the \textit{\textbf{reverse attention}} in three parallel high-level features. In other words, our architecture can sequentially mine complementary 
regions and details by erasing the existing estimated polyp regions from high-level side-output features, where the existing estimation is up-sampled from the deeper layer.

Specifically, we obtain the output reverse attention features  by multiplying  (element-wise ) the high-level side-output feature  by a reverse attention weight , as below:

The reverse attention weight  is de-facto for salient object detection in the computer vision community~\cite{chen2018reverse,zhang2020bilateral}, and can be formulated as:

where  denotes an up-sampling operation,  is the Sigmoid function, and  is a reverse operation subtracting the input from matrix , in which all the elements are . 
\figref{fig:Framework} (RA) shows the details of this process.
It is worth noting that the erasing strategy driven by reverse attention can eventually refine the imprecise and coarse estimation into an accurate and complete prediction map. 

\subsection{Learning Process and Implementation Details.}

\noindent\textbf{Loss Function.} Our loss function is defined as , where  and  represent the weighted IoU loss and binary cross entropy (BCE) loss for the global restriction and local (pixel-level) restriction. Different from the standard IoU loss, which has been widely adopted in segmentation tasks, the weighted IoU loss increases the weights of hard pixels 
to highlight their importance. In addition, compared with the standard BCE loss,  pays more attention to hard pixels rather than assigning all pixels equal weights. The definitions of these losses are the same as in~\cite{qin2019basnet,wei2019f3net} and their effectiveness has been validated in the field of salient object detection. Here, we adopt deep supervision for the three side-outputs (\ie, , , and ) and the global map . Each map is up-sampled (\eg, ) to the same size as the ground-truth map . Thus the total loss for the proposed \ourmodel~can be formulated as:
. \\

\noindent\textbf{Implementation Details.}
We implement our model in PyTorch, which is accelerated by an NVIDIA TITAN RTX GPU. All the inputs are uniformly resized to  and employ a multi-scale training strategy  rather than data augmentation. We employ the Adam optimization algorithm to optimize the overall parameters with a learning rate of . The whole network is trained in an end-to-end manner, which takes 32 minutes to converge  over 20 epochs with a batch size of 16. Our final prediction map  is generated by  after a sigmoid operation.




\section{Experiments}\label{sec:Experiments}

\subsection{Experiments on Polyp Segmentation}\label{sec:CmpSOTAS}
In this section, we compare our \ourmodel~with existing methods in terms of learning ability, generalization capability, complexity, and qualitative results. \\

\noindent\textbf{Datasets and Baselines.} Experiments are conducted on five polyp segmentation datasets: ETIS~\cite{silva2014toward},
CVC-ClinicDB/CVC-612~\cite{bernal2015wm}, CVC-ColonDB~\cite{tajbakhsh2015automated}, 
EndoScene~\cite{vazquez2017benchmark}, and Kvasir~\cite{jha2020kvasir}. 
The first four are standard benchmarks, and the last one is the largest-scale challenging dataset, recently released. We compare our \ourmodel~with four SOTA medical image segmentation methods: U-Net~\cite{ronneberger2015u}, U-Net++~\cite{zhou2018unetplus}, ResUNet-mod~\cite{zhang2018road}, and ResUNet++~\cite{jha2019resunetplus}. We also report the cutting edge polyp segmentation model, \ie, SFA~\cite{fang2019selective}. 
The segment results of SFA are generated by the released code with default settings. \\

\noindent\textbf{Training Settings and Metrics.} Unless otherwise noted, we follow the same training settings as in~\cite{jha2019resunetplus}, \ie, the images from Kvasir, and CVC-ClinicDB are randomly split into 80\% for training, 10\% for validation, and 10\% for testing.  
We employ two metrics (\ie, mean Dice and mean IoU) for quantitative evaluation, similar to~\cite{jha2019resunetplus,jha2020kvasir}. To provide deeper insight into the model performance, we further introduce four other metrics which are widely used in the field of object detection~\cite{fan2020camouflaged,Zhang2020UCNet,fan2018salient,fu2020jl,zhao2019contrast,zhao2019egnet}. The weighted Dice metric  is used to amend the ``Equal-importance flaw'' in Dice. The MAE metric is utilized to evaluate the pixel-level accuracy. To evaluate pixel-level and global-level similarity, we adopt the recently released enhanced-alignment metric ~\cite{Fan2018Enhanced}. Since  and MAE are based on a pixel-wise evaluation system and ignore structural similarities, ~\cite{fan2017structure} is adopted to assess the similarity between predictions and ground-truths. The evaluation toolbox is available at {\tt \href{https://github.com/DengPingFan/PraNet}{https://github.com/DengPingFan/PraNet}}.
\\ 





\begin{table*}[t!]
  \centering
  \scriptsize
  \renewcommand{\arraystretch}{1.1}
  \setlength\tabcolsep{5pt}
  \caption{Quantitative results on Kvasir~\cite{jha2020kvasir} and  CVC-612~\cite{bernal2015wm} datasets.
  `n/a' denotes that the results are not available. `' represents evaluation scores from~\cite{jha2019resunetplus}.
}\label{tab:Kvasir-CVC612}
  \begin{tabular}{cr||cccccccc}
  \hline
\rowcolor{mygray}
   &Methods & mean Dice & mean IoU  &   & & & MAE\\
  \hline
\multirow{6}{*}{\begin{sideways}Kvasir\end{sideways}} & 
U-Net (MICCAI'15)~\cite{ronneberger2015u} & 0.818 & 0.746 & 0.794 & 0.858 & 0.893 & 0.055 \\
  &U-Net++ (TMI'19)~\cite{zhou2018unetplus} & 0.821  & 0.743 & 0.808 & 0.862 & 0.910 & 0.048 \\
& ResUNet-mod~\cite{zhang2018road} & 0.791 &  n/a  & n/a & n/a & n/a & n/a \\
  & ResUNet++~\cite{jha2019resunetplus} & 0.813 & 0.793 & n/a & n/a & n/a & n/a\\
  & SFA (MICCAI'19)~\cite{fang2019selective} & 0.723 & 0.611 & 0.670 & 0.782 & 0.849 & 0.075\\
  & \textbf{\ourmodel~(Ours)} & \textbf{0.898} & \textbf{0.840} & \textbf{0.885} & \textbf{0.915} & \textbf{0.948} & \textbf{0.030}\\
\hline
  \hline
  \multirow{6}{*}{\begin{sideways}CVC-612\end{sideways}} & 
U-Net (MICCAI'15)~\cite{ronneberger2015u} & 0.823 & 0.755 & 0.811 & 0.889 & 0.954 & 0.019 \\
  &U-Net++ (TMI'19)~\cite{zhou2018unetplus} & 0.794  & 0.729 & 0.785 & 0.873 & 0.931 & 0.022\\
  &ResUNet-mod~\cite{zhang2018road} & 0.779 & n/a & n/a & n/a & n/a & n/a \\
  &ResUNet++~\cite{jha2019resunetplus} & 0.796  & 0.796 & n/a & n/a & n/a & n/a\\
  &SFA (MICCAI'19)~\cite{fang2019selective} & 0.700 & 0.607 & 0.647 & 0.793 & 0.885 & 0.042\\
  &\textbf{\ourmodel~(Ours)} & \textbf{0.899} & \textbf{0.849} & \textbf{0.896} & \textbf{0.936} & \textbf{0.979}  & \textbf{0.009}\\
  \hline
  \end{tabular}
\end{table*}

\noindent\textbf{Learning Ability.} In this section, we conduct two experiments to validate our model's learning ability on two \textit{seen} datasets, \ie, Kvasir and CVC-612.
\textit{Kvasir} is a recently released challenging dataset that contains 1,000 images selected from a sub-class (polyp class) of the Kvasir dataset~\cite{pogorelov2017kvasir}. 
\textit{CVC-ClinicDB}, also called \textit{CVC-612}, includes 612 open-access images from 31 colonoscopy clips.
As shown in \tabref{tab:Kvasir-CVC612}, our \ourmodel~outperforms all SOTAs by a large margin (mean Dice: about ), across both datasets, in all metrics. 
This suggests that our model has a strong learning ability to effectively segment polyps. \\

\noindent\textbf{Generalization Capability.} We conduct three experiments to test the model's generalizability. The three \textit{unseen} datasets have their own challenging situations and properties. \textit{CVC-ColonDB} is a small-scale database which contains 380 images from 15 short colonoscopy sequences. All images are used as our testing set.
\textit{ETIS} is an early established dataset which has 196 polyp images for early diagnosis of colorectal cancer.
\textit{EndoScene} is a combination of CVC-612 and CVC300. We follow Fang~\etal~\cite{fang2019selective} and split it into training, validation, and testing subsets. We only use the testing set of EndoScene-CVC300 in this experiment, since part of CVC-612 may be seen in the training stage. 
\ourmodel~ again outperforms existing classical medical segmentation baselines (\ie, U-Net, U-Net++), as well as SFA, with significant improvements (see \tabref{tab:Generalizability})
on all three unseen datasets. One notable finding is that SFA drops dramatically on these unseen datasets, partially demonstrating that the model generalizability is poor. \\




\begin{table*}[t!]
  \centering
  \scriptsize
  \renewcommand{\arraystretch}{1.1}
  \setlength\tabcolsep{5pt}
  \caption{ Quantitative results on CVC-ColonDB~\cite{tajbakhsh2015automated}, ETIS~\cite{silva2014toward}, and {\tt test set} (CVC-T) of EndoScene~\cite{vazquez2017benchmark} datasets. SFA~\cite{fang2019selective} results are generated using the released code.
  }\label{tab:Generalizability}
  \begin{tabular}{cr||cccccccc}
  \hline
  \rowcolor{mygray}
  &Methods & mean Dice & mean IoU  &   & & & MAE\\
  \hline
  \multirow{4}{*}{\begin{sideways}ColonDB\end{sideways}} & 
  U-Net(MICCAI'15)~\cite{ronneberger2015u}  & 0.512 & 0.444 & 0.498 & 0.712 & 0.776 & 0.061 \\
  &U-Net++(TMI'19)~\cite{zhou2018unetplus} & 0.483 & 0.410 & 0.467 & 0.691 & 0.760 & 0.064 \\
&SFA (MICCAI'19)~\cite{fang2019selective} & 0.469 & 0.347 & 0.379 & 0.634 & 0.765 & 0.094\\
&\textbf{\ourmodel~(Ours)} & \textbf{0.709} & \textbf{0.640} & \textbf{0.696} & \textbf{0.819} & \textbf{0.869} & \textbf{0.045}\\
  \hline
  \hline
  \multirow{4}{*}{\begin{sideways}ETIS\end{sideways}} &
  U-Net (MICCAI'15)~\cite{ronneberger2015u}  & 0.398 & 0.335 & 0.366 & 0.684 & 0.740 & 0.036 \\
  &U-Net++ (TMI'19)~\cite{zhou2018unetplus}  & 0.401 & 0.344 & 0.390 & 0.683 & 0.776 & 0.035 \\
&SFA (MICCAI'19)~\cite{fang2019selective} & 0.297 & 0.217 & 0.231 & 0.557 & 0.633 & 0.109\\
&\textbf{\ourmodel~(Ours)}  & \textbf{0.628} & \textbf{0.567} & \textbf{0.600} & \textbf{0.794}  & \textbf{0.841} & \textbf{0.031}\\
  \hline
  \hline
  \multirow{4}{*}{\begin{sideways}CVC-T\end{sideways}} &
  U-Net (MICCAI'15)~\cite{ronneberger2015u}  & 0.710 & 0.627 & 0.684 & 0.843 & 0.876 & 0.022\\
  &U-Net++ (TMI'19)~\cite{zhou2018unetplus}   & 0.707 & 0.624 & 0.687 & 0.839 & 0.898 & 0.018\\
&SFA (MICCAI'19)~\cite{fang2019selective} & 0.467 & 0.329 & 0.341 & 0.640 & 0.817 & 0.065\\
&\textbf{\ourmodel~(Ours)}  & \textbf{0.871} & \textbf{0.797} & \textbf{0.843} & \textbf{0.925} & \textbf{0.972} & \textbf{0.010} \\
  \hline
\end{tabular}
\end{table*}

\begin{table*}[t!]
  \centering
  \scriptsize
  \renewcommand{\arraystretch}{1.1}
  \setlength\tabcolsep{5.0pt}
  \caption{Training and inference analysis (same platform) on CVC-ClinicDB~\cite{bernal2015wm} dataset. We record the \#epochs when the model converges. Lr = learning rate.
  }\label{tab:Params}
  \begin{tabular}{cr||ccrccc}
  \hline
  \rowcolor{mygray}
   & Methods & Epoch & Lr & Training & Inference& mean Dice \\
\hline
  \multirow{4}{*}{\begin{sideways} CVC-612\end{sideways}} &
  U-Net (MICCAI'15)~\cite{ronneberger2015u} & 30 & 3e-4 & 40 minutes& 8fps & 0.823 \\
  & U-Net++ (TMI'19)~\cite{zhou2018unetplus} & 30 & 3e-4 & 45 minutes& 7fps & 0.794 \\
& SFA (MICCAI'19)~\cite{fang2019selective} & 500 & 1e-2 & 20 hours & 40fps & 0.700 \\
&\textbf{\ourmodel~(Ours)} & \textbf{20} & \textbf{1e-4} & \textbf{30 minutes} & \textbf{50fps}  & \textbf{0.899} \\
  \hline
  \end{tabular}
\end{table*}

\noindent\textbf{Qualitative Results.} In \figref{fig:Results}, we provide the polyp segmentation results of our \ourmodel~on the Kvasir {\tt test set}. Our model can precisely locate and segment the polyp tissues in many challenging cases, such as varied size, homogeneous regions, different kinds of texture, \etc.\\

\noindent\textbf{Training and Inference Analysis.} In \tabref{tab:Params}, we present the training time, and inference time of \ourmodel~and current SOTA approaches. 
The running times of all compared models are tested on an Intel i9-9820X CPU and a TITAN RTX GPU with 24GB memory.
As shown, our model achieves convergence with only 20 epochs (0.5 hours) of training. One reason is that the parallel structure of our \ourmodel~provides a short connection way to back-propagate the loss to the early layer in the decoder path (red flow of map in \figref{fig:Framework}). Moreover, the side-outputs also relieve the vanishing gradient problem and guide the early layer training. Note that our \ourmodel~runs at a real-time speed of 50fps for a 352352 input, which guarantees our method can be implemented in colonoscopy video.



 

\subsection{Ablation Study}\label{sec:ablation}
In this section, we test each component of our \ourmodel~on the \textit{seen} and \textit{unseen} datasets to provide deeper insight into our model. \\
\noindent\textbf{Effectiveness of PPD.} 
We investigate the importance of the cascaded mechanism (parallel partial decoder, PPD). From \tabref{tab:Ablation}, we observe that No.2 (backbone + PPD) outperforms No.1 (backbone), clearly showing that the cascaded mechanism is necessary for increasing performance. Note that our PPD is only deployed on the high-level features, which greatly reduces the training time (See \tabref{tab:Params}, \textit{Inference} = 50fps) of the model. \\
\noindent\textbf{Effectiveness of RA.} We further investigate the contribution of the reverse attention. The results are listed in the first and third column of \tabref{tab:Ablation}. We observe that No.3 improves the backbone (No.1) performance on the CVC-612, increasing the mean Dice from 0.747 to 0.888 and the structure measure  from 0.735 to 0.912. These improvements suggest that introducing reverse attention component can enable our model to accurately distinguish true polyp tissues. \\
\noindent\textbf{Effectiveness of PPD \& RA.} To assess the combination of the PPD and RA modules, we test the performance of No.4 (PPD + RA + Backbone). As shown in \tabref{tab:Ablation}, our \ourmodel~(No.4) is generally better than other settings (No.1No.3). 
In addition, \ourmodel~outperforms four SOTA models on all datasets tested, with significant improvements (5\%), making it a robust, unified architecture that can help promote future research in polyp segmentation. 

 

\begin{table}[t]
\centering
\scriptsize
\renewcommand{\arraystretch}{1.1}
\renewcommand{\tabcolsep}{3pt}
\caption{Ablation study for \ourmodel~on the CVC-612 and CVC300 datasets.
}\label{tab:Ablation}
\begin{tabular}{r||ccc|ccc}
 \hline
 \rowcolor{mygray}
 \multicolumn{1}{r||}{Settings}  & \multicolumn{3}{c|}{CVC-612 (\textit{\textbf{seen}})} &\multicolumn{3}{c}{CVC300 (\textit{\textbf{unseen}})}   \\
 \rowcolor{mygray}
 &  mean Dice  &  mean IoU  &    &  mean Dice  &  mean IoU  &   \\
 \hline
 Backbone (No.1)            & 0.747 & 0.668 & 0.735 & 0.726 & 0.631 & 0.670 \\
 PPD + Backbone (No.2)      & 0.865 & 0.798 & 0.902 & 0.824 & 0.734 & 0.893\\
 RA + Backbone (No.3)       & 0.888 & 0.845 & 0.912 & 0.871 & \textbf{0.800} & 0.888 \\
 PPD + RA + Backbone (No.4) & \textbf{0.899} & \textbf{0.849} & \textbf{0.936} & \textbf{0.871} & 0.797 & \textbf{0.925} \\
 \hline
\end{tabular}
\end{table}



\begin{figure*}[t!]
	\centering
	\begin{overpic}[width=\textwidth]{Results}
    \end{overpic}
\caption{Qualitative results of different methods. 
}
    \label{fig:Results}
\end{figure*}

\section{Conclusion}\label{conclusion}
We have presented a novel architecture, \ourmodel, for automatically segmenting polyps from colonoscopy images.
Extensive experiments demonstrated that \ourmodel~consistently outperforms all state-of-the-art approaches by a large margin (5\%) across five challenging datasets. Furthermore, \ourmodel~achieves a very high accuracy (mean Dice = 0.898 on Kvasir dataset) without any pre-/post-processing. Another advantage is that \ourmodel~is universal and flexible, meaning that more effective modules can be added to further improve the accuracy.
Compared with current top-ranked SFA models, \ourmodel~can achieve strong learning, generalization ability, and real-time segmentation efficiency. We hope this study will offer the community an opportunity to explore more powerful models on the related topics such as lung infection segmentation~\cite{fan2020inf}/classification~\cite{wu2020jcs}, or even on the upstream task, \etc.















\bibliographystyle{splncs03}
\bibliography{PolypSegmentation}

\end{document}
