\section{Experiments}
We evaluate our approach summarized in \figref{fig:OurApproach} on the dataset of the Pascal VOC 2012 challenge~\cite{pascal-voc-2012}. The task is semantic image segmentation of 21 object classes (including background). The original dataset contains $1464$ training, $1449$ validation and $1456$ test images. In addition to this data we make use of the annotations provided by Hariharan \etal~\cite{HariharanICCV2011}, resulting in a total of $10582$ training instances. The reported performance is measured using the intersection-over-union metric. Note that we conduct our tests on the 1449 validation set images which were neither used during training nor for fine-tuning. 

\begin{figure}
\centering
\begin{tabular}{cc}
\includegraphics[width=6cm]{ValOverIterationsUnary.pdf}&
\includegraphics[width=6cm]{JointTrainingPerformance.pdf}\\
(a)&(b)\\
\end{tabular}
\caption{(a) Validation set performance over the number of iterations when fine-tuning the unary parameters only. (b) Validation set performance over the number of iterations when fine-tuning all parameters.}
\label{fig:ValOverIterationsUnaryValOverIterationsJoint}
\end{figure}

\subsection{Model}
Our model setup follows~\cite{ChenARXIV2015b}, \ie, we employ the 16 layer DeepNet model~\cite{SimonyanARXIV2014}. Just like~\cite{ChenARXIV2015b} we first convert the fully connected layers into convolutions as first discussed in~\cite{GuistiICIP2013,SermanetICLR2014}. This is useful since we are not interested in a single variable output prediction, but rather aim at learning probability masks. To obtain a larger probability mask we skip downsampling during the last two max-pooling operations. To take into account the skipped downsampling during subsequent convolutions we  employ the `\`{a} trous (with hole) algorithm'~\cite{Mallat1999}. It takes care of the fact that data is stored in an interleaved way, \ie, in our case convolutions sub-sample the input data by a factor of two or four respectively. To adapt to the 21 object classes we also replace the top layer of the DeepNet model to yield 21 classes for each pixel.

Similar to~\cite{ChenARXIV2015b} we assume the input size of our network to be of dimension $306\times 306$ which results in a $40\times 40$ sized spatial output of the DeepNet which is in our case an \emph{intermediate} result however.

Contrasting~\cite{ChenARXIV2015b}, we jointly optimize for both unary and CRF parameters using the algorithm presented in \figref{fig:OurApproach}. To this end, given images downsampled to a size of $306\times 306$, our algorithm first performs a forward pass through the convolutional DeepNet to obtain the $40\times40\times21$ sized class probability maps in an \emph{intermediate} stage. These intermediate class probability maps are directly  up-sampled to the original image dimension using a bi-linear interpolation layer. This yields the actual output of our augmented DeepNet network defining the scoring function $F(x,\hat y,w)$. Note that the number $N$ of variables $\hat y = (\hat y_1, \ldots, \hat y_N)\in\Y$ is therefore equal to the number of pixels of the original image.

For the second step of our algorithm we perform 5 iterations of mean field updates to compute the marginals $q_{(x,y),i}(\hat y_i)$ of the fully connected CRF. Those are then compared to the original groundtruth image segmentations, using as our loss function the sum of cross-entropy terms, \ie, the log-likelihood loss, as specified in \equref{eq:LogLikelihoodLoss}. In the third step we back-track through the marginals to obtain a gradient of the loss function. Afterwards we back-propagate the derivatives \wrt the unary term through both the bi-linear interpolation and the 16-layer convolutional network. The shape and compatibility parameters of the CRF, detailed below, are updated directly.





It was shown independently by many authors~\cite{SimonyanARXIV2014,ChenARXIV2015}, that successively increasing the number of parameters during training typically yields better performance due to better initialization of larger models. We therefore train our model in two stages. First, we assume no pairwise connections to be present, \ie, we fine-tune the weights obtained from the DeepNet ImageNet model~\cite{SimonyanARXIV2014,ILSVRCarxiv14} to the Pascal dataset~\cite{pascal-voc-2012}. Standard parameter settings for a momentum of $0.9$, a weight decay of $0.0005$ and learning rates of $0.01$ and $0.001$ for the top and all other layers  are employed respectively. Due to the 12GB memory restrictions on the Tesla K40 GPU we use a mini-batch size of 20 images.

\begin{table}
\centering
\setlength\tabcolsep{1pt}
\begin{tabular}{c}
\begin{tabular}{|c||c|c|c|c|c|c|c|c|c|c|c|}\hline
Data&bkg&areo&bike&bird&boat&bottle&bus&car&cat&chair&cow\\\hline\hline
Valid. & 90.461 & 77.455& 30.355& 76.564& 60.735& 65.075& 81.261& 74.958& 81.505& 23.367& 66.279\\\hline
Train &90.159& 76.314& 64.450& 78.677& 68.224& 68.044& 84.491& 80.274& 86.347& 44.567& 79.987\\\hline
\end{tabular}\\
\\
\begin{tabular}{|c||c|c|c|c|c|c|c|c|c|c||c||c|}\hline
Data&table&dog&horse&mbike&person&plant&sheep&sofa&train&tv&Our mean&\cite{ChenARXIV2015b}\\\hline
Valid. & 52.219& 70.624& 66.660& 65.725& 72.913& 42.174& 73.452& 43.412& 71.738& 58.322& {\bf 64.060}&63.74\\\hline
Train & 62.710& 82.987& 76.729& 76.523& 75.399& 63.863& 79.937& 55.146& 80.699& 70.164& 73.604&-\\\hline
\end{tabular}
\end{tabular}
\vspace{-0.2cm}
\caption{Performance of our approach for individual classes. In the last two columns of the lower panel we compare our mean to the recently presented baseline by Chen \etal~\cite{ChenARXIV2015b}. }
\label{tab:ResultsBreakDown}
\end{table}





In a second stage we jointly train the convolutional network parameters as well as the compatibility and shape parameters of the dense CRF arising from the pairwise functions
\be
f_{ij}(\hat y_i,\hat y_j, x,w) = \mu(\hat y_i,\hat y_j)\sum_{m=1}^2 w_m k^{(m)}(\hat f_i^{(m)}(x) - \hat f_j^{(m)}(x)).
\label{eq:OurPariwise}
\ee
Hereby, we employ the Potts potential $\mu(y_i,y_j) = \llbracket y_i = y_j\rrbracket$ and the Gaussian kernels given by
$$
k^{(m)} = \exp\left(-\frac{1}{2}(f_i^{(m)} - f_i^{(m)})^\top \Sigma_m^{-1}(\hat f_i^{(m)} - \hat f_i^{(m)})\right).
$$
As indicated in \equref{eq:OurPariwise}, we use $M=2$ kernels, both with diagonal covariance matrix $\Sigma_m$. One containing as features $\hat f_i(x)$ the two-dimensional pixel positions,  the other one containing as features the two dimensional pixel positions as well as the three color channels. Hence we obtain a total of nine parameters, \ie, two compatibility parameters $w_1$ and $w_2$ and $2+5 = 7$  kernel shape parameters for the diagonal covariance matrices $\Sigma_m$.


\begin{figure}
\centering
\newlength{\imgwidth}
\setlength{\imgwidth}{1.6cm}
\begin{tabular}{ccc|ccc}
\includegraphics[width=\imgwidth]{2009_001314.jpg}&\includegraphics[width=\imgwidth]{GT_2009_001314.png}&\includegraphics[width=\imgwidth]{2009_001314.png}&
\includegraphics[width=\imgwidth]{2009_002604.jpg}&\includegraphics[width=\imgwidth]{GT_2009_002604.png}&\includegraphics[width=\imgwidth]{2009_002604.png}\\
\includegraphics[width=\imgwidth]{2009_002320.jpg}&\includegraphics[width=\imgwidth]{GT_2009_002320.png}&\includegraphics[width=\imgwidth]{2009_002320.png}&
\includegraphics[width=\imgwidth]{2010_000836.jpg}&\includegraphics[width=\imgwidth]{GT_2010_000836.png}&\includegraphics[width=\imgwidth]{2010_000836.png}\\
\includegraphics[width=\imgwidth]{2010_001995.jpg}&\includegraphics[width=\imgwidth]{GT_2010_001995.png}&\includegraphics[width=\imgwidth]{2010_001995.png}&
\includegraphics[width=\imgwidth]{2010_001448.jpg}&\includegraphics[width=\imgwidth]{GT_2010_001448.png}&\includegraphics[width=\imgwidth]{2010_001448.png}\\
\includegraphics[width=\imgwidth]{2007_001289.jpg}&\includegraphics[width=\imgwidth]{GT_2007_001289.png}&\includegraphics[width=\imgwidth]{2007_001289.png}&
\includegraphics[width=\imgwidth]{2007_005915.jpg}&\includegraphics[width=\imgwidth]{GT_2007_005915.png}&\includegraphics[width=\imgwidth]{2007_005915.png}\\
\includegraphics[width=\imgwidth]{2011_000807.jpg}&\includegraphics[width=\imgwidth]{GT_2011_000807.png}&\includegraphics[width=\imgwidth]{2011_000807.png}&
\includegraphics[width=\imgwidth]{2008_001546.jpg}&\includegraphics[width=\imgwidth]{GT_2008_001546.png}&\includegraphics[width=\imgwidth]{2008_001546.png}\\
\includegraphics[width=\imgwidth]{2007_002094.jpg}&\includegraphics[width=\imgwidth]{GT_2007_002094.png}&\includegraphics[width=\imgwidth]{2007_002094.png}&
\includegraphics[width=\imgwidth]{2008_002588.jpg}&\includegraphics[width=\imgwidth]{GT_2008_002588.png}&\includegraphics[width=\imgwidth]{2008_002588.png}\\
\includegraphics[width=\imgwidth]{2011_002993.jpg}&\includegraphics[width=\imgwidth]{GT_2011_002993.png}&\includegraphics[width=\imgwidth]{2011_002993.png}&
\includegraphics[width=\imgwidth]{2007_008430.jpg}&\includegraphics[width=\imgwidth]{GT_2007_008430.png}&\includegraphics[width=\imgwidth]{2007_008430.png}\\
\end{tabular}
\caption{Visual results of good predictions.}
\label{fig:VisualResultsGood}
\end{figure}


\subsection{Results}
As mentioned before, all our results were computed on the validation set of the Pascal VOC dataset. This part of the data was neither used for training nor for fine-tuning. 

{\bfseries Unary performance:} We first investigate the performance of the first training stage of the proposed approach, \ie, fine-tuning of the 16 layer DeepNet parameters on the Pascal VOC data. The validation set accuracy is plotted over the number of iterations in \figref{fig:ValOverIterationsUnaryValOverIterationsJoint}~(a). We observe the performance to peak at around 4000 iterations with a mean intersection over union measure of $61.476\%$. The result reported by~\cite{ChenARXIV2015b} for this experiment is $59.80\%$, \ie, we outperform their unary model by $1.5\%$. 



{\bfseries Joint training:} Next we illustrate the performance of the second step, \ie, joint training of both convolutional network parameters and CRF compatibility and shape parameters. In \figref{fig:ValOverIterationsUnaryValOverIterationsJoint}~(b) we indicate the best obtained unary performance from the first step and visualize the validation and training set performance over the number of iterations. We observe the results to peak quickly after around $20$ iterations and remain largely stable thereafter. 





{\bfseries Details: } In \tabref{tab:ResultsBreakDown} we provide the training and test set accuracies for the 21 individual classes. We observe the `bike' and `chair' class to be particularly difficult. For both categories the validation set performance is roughly half of the training set accuracy.

{\bfseries Comparison to baseline: } As provided in \tabref{tab:ResultsBreakDown}, the peak validation set performance of our approach is $64.060\%$, which slightly outperforms the separate training result of $63.74\%$ reported by Chen \etal~\cite{ChenARXIV2015b}.





{\bfseries Visual results: } We illustrate visual results of our approach in \figref{fig:VisualResultsGood}. Our method successfully segments the object if the images are clearly apparent. Noisy images and objects with many variations pose challenges to the presented approach as visualized in \figref{fig:FailureCases}. Also, we observe our learnt parameters to generally over-smooth results while being noisy on the boundaries. 















