\documentclass{article}

\PassOptionsToPackage{numbers, compress}{natbib}






\usepackage[final]{neurips_2022}



\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{amsmath}
\usepackage{algorithm} 
\usepackage{algpseudocode} 
\usepackage{bbm}
\usepackage{pifont}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{caption}\usepackage{subcaption}
\usepackage{multirow}

\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}



\author{Yue~Hu \hspace{1.75cm} Shaoheng~Fang \hspace{1.75cm} Zixing Lei \\
  Cooperative Medianet Innovation Center, Shanghai Jiao Tong University \\
  \texttt{\{18671129361, shfang, chezacarss\}@sjtu.edu.cn} \\
  \AND
  Yiqi~Zhong \\
University of Southern California \\
  \texttt{yiqizhon@usc.edu} \\
  \And
  Siheng ~Chen\thanks{Corresponding author} \\
  Shanghai Jiao Tong University,  Shanghai AI Laboratory \\
  \texttt{sihengc@sjtu.edu.cn}
}
\newcommand{\highlightChange}{\color{red}}
\def\HC{\highlightChange}
\newcommand{\Note}[1]{{\color{blue} \bf \small [NOTE: #1]}}
\usepackage{xcolor}

\begin{document}

\title{ Where2comm: Communication-Efficient \\ Collaborative Perception via Spatial Confidence Maps}
\maketitle

\begin{abstract}
Multi-agent collaborative perception could significantly upgrade the perception performance by enabling agents to share complementary information with each other through communication. It inevitably results in a fundamental trade-off between perception performance and communication bandwidth. To tackle this bottleneck issue, we propose a spatial confidence map, which reflects the spatial heterogeneity of perceptual information. It empowers agents to only share spatially sparse, yet perceptually critical information, contributing to where to communicate. Based on this novel spatial confidence map, we propose \texttt{Where2comm}, a communication-efficient collaborative perception framework. \texttt{Where2comm} has two distinct advantages: i) it considers pragmatic compression and uses less communication to achieve higher perception performance by focusing on perceptually critical areas; and ii) it can handle varying communication bandwidth by dynamically adjusting spatial areas involved in communication. To evaluate~\texttt{Where2comm}, we consider 3D object detection in both real-world and simulation scenarios with two modalities (camera/LiDAR) and two agent types (cars/drones) on four datasets: OPV2V, V2X-Sim, DAIR-V2X, and our original CoPerception-UAVs.~\texttt{Where2comm} consistently outperforms previous methods; for example, it achieves more than $100,000 \times$ lower communication volume and still outperforms DiscoNet and V2X-ViT on OPV2V. Our code is available at~\url{https://github.com/MediaBrain-SJTU/where2comm}.
\vspace{-2mm}









\end{abstract} \vspace{-3mm}
\section{Introduction}\label{intro}
\vspace{-2mm}





Collaborative perception enables multiple agents to share complementary perceptual information with each other, promoting more holistic perception. It provides a new direction to fundamentally overcome a number of inevitable limitations of single-agent perception, such as occlusion and long-range issues. Related methods and systems are desperately needed in a broad range of real-world applications, such as vehicle-to-everything-communication-aided autonomous driving~\cite{v2vnet,disconet,Chen20213DPC},  multi-robot warehouse automation system~\cite{li2020mechanism,zaccaria2021multi} and multi-UAVs (unmanned aerial vehicles) for search and rescue~\cite{scherer2015autonomous,alotaibi2019lsar,DVDET}. To realize collaborative perception, recent works have contributed high-quality datasets~\cite{V2XSim,OPV2V,dair} and effective collaboration methods~\cite{when2com,who2com,disconet,zhou2022multi,ArnoldDT:22,SyncNet,Li2022mrsc, CoBEVT,UQ4CP}.
\vspace{-1mm}

In this emerging field, the current biggest challenge is how to optimize the trade-off between perception performance and communication bandwidth. Communication systems in real-world scenarios are always constrained that they can hardly afford huge communication consumption in real-time, such as passing complete raw observations or a large volume of features. Therefore, we cannot solely promote the perception performance without evaluating the expense of every bit of precious communication bandwidth. To achieve a better performance and bandwidth trade-off, previous works put forth solutions from several perspectives. For example, When2com~\cite{when2com} considers a handshake mechanism which selects the most relevant collaborators; V2VNet~\cite{v2vnet} considers end-to-end-learning-based source coding; and DiscoNet~\cite{disconet} uses 1D convolution to compress message. However, all previous works make a plausible assumption: once two agents collaborate, they are obligated to share perceptual information of all spatial areas \textit{equally}. This unnecessary assumption can hugely waste the bandwidth as a large proportion of spatial areas may contain irrelevant information for perception task. Figure~\ref{fig:CoPerceptionScene} illustrates such a spatial heterogeneity of perceptual information.





\begin{figure}[!t]
    \includegraphics[width=\linewidth]{figures/Intro_WithLiDAR/Intro.png}
\vspace{-7mm}
  \caption{Collaborative perception could contribute to safety-critical scenarios, where the white car and the red car may collide due to occlusion. This collision could be avoided when the blue car can share a message about the red car's position. Such a message is spatially sparse, yet perceptually critical. Considering the precious communication bandwidth, each agent needs to speak to the point!}
  \label{fig:CoPerceptionScene}
  \vspace{-5mm}
\end{figure}








To fill this gap, we consider a novel spatial-confidence-aware communication strategy. The core idea is to enable a spatial confidence map for each agent, where each element reflects the perceptually critical level of a corresponding spatial area. Based on this map, agents decide which spatial area (where) to communicate about. That is, each agent offers spatially sparse, yet critical features to support other agents, and meanwhile requests complementary information from others through multi-round communication to perform efficient and mutually beneficial collaboration.


Following this strategy, we propose~\texttt{Where2comm}, a novel communication-efficient multi-agent collaborative perception framework with the guidance of spatial confidence maps; see Fig.~\ref{fig:system}.~\texttt{Where2comm} includes three key modules: i) a spatial confidence generator, which produces a spatial confidence map to indicate perceptually critical areas; ii) a spatial confidence-aware communication module, which leverages the spatial confidence map to decide \textit{where} to communicate via novel message packing, and \textit{who} to communicate via novel communication graph construction; and iii) a spatial confidence-aware message fusion module, which uses novel confidence-aware multi-head attention to fuse all messages received from other agents, upgrading the feature map for each agent.


\texttt{Where2comm} has two distinct advantages. First, it promotes pragmatic compression at the feature level and uses less communication to achieve higher perception performance by focusing on perceptually critical areas. Second, it adapts to various communication bandwidths and communication rounds, while previous models only handle one predefined communication bandwidth and a fixed number of communication rounds. To evaluate~\texttt{Where2comm}, we consider the collaborative 3D object detection task on four datasets: DAIR-V2X~\cite{dair}, V2X-Sim~\cite{V2XSim}, OPV2V~\cite{OPV2V} and our original dataset CoPerception-UAVs. Our experiments cover both real-world and simulation scenarios, two types of agents (cars and drones) and sensors (LiDAR and cameras). Results show that i) the proposed~\texttt{Where2comm} consistently and significantly outperforms previous works in the performance-bandwidth trade-off across multiple datasets and modalities; and ii)~\texttt{Where2comm} achieves better trade-off when the communication round increases.
 \vspace{-3mm}
\section{Related Works}\label{related_works}
\vspace{-3mm}

\textbf{Multi-agent communication.}
The communication strategy in multi-agent systems has been widely studied~\cite{singh2018learning}. Early works~\cite{tan1993multi,qureshi2008smart,li2010auction} often use predefined protocols or heuristics to decide how agents communicate with each other. However, it is difficult to generalize those methods to complex tasks. Recent works, thus, explore learning-based methods for complex scenarios. For example, CommNet~\cite{sukhbaatar2016learning} learns continuous communication in the multi-agent system. Vain~\cite{hoshen2017vain} adopts the attention mechanism to help agents selectively fuse the information from others. Most of these previous works consider decision-making tasks and adopt reinforcement learning due to the lack of explicit supervision. In this work, we focus on the perception task. Based on direct perception supervision, we apply supervised learning to optimize the communication strategy in both trade-off perception ability and communication cost. 

\begin{table}[!t]
\centering
\caption{Major components comparisons of collaborative perception systems.}
\setlength\tabcolsep{3pt}
\begin{scriptsize}
\begin{tabular}{lllll}
\hline
Method & Venue &  Message packing & Communication graph construction   & Message fusion \\ \hline
When2com~\cite{when2com}  & CVPR 2020 & Full feature map  &  Handshake-based sparse graph  & Attention per-agent\\
V2VNet~\cite{v2vnet}  & ECCV 2020 & Full feature map & Fully connected graph    & Average per-agent              \\
DiscoNet~\cite{disconet} & NeurIPS 2021 & Full feature map  & Fully connected graph & MLP-based attention per-location  \\
V2X-ViT~\cite{xu2022v2x}  & ECCV 2022 & Full feature map  & Fully connected graph & Self-attention per-location  \\
\hline
\multirow{2}{*}{Where2comm} & \multirow{2}{*}{NeurIPS~2022} &  \emph{Confidence}-aware sparse &\emph{Confidence}-aware~sparse~graph  & \emph{Confidence}-aware multi-head \\ 
&  & feature map  + request map  & & attention per-location
\\ \hline
\end{tabular}
\end{scriptsize}
\label{tab:comparison}
\vspace{-5mm}
\end{table}

\textbf{Collaborative perception.}
As a recent application of multi-agent communication systems to perception tasks, collaborative perception is still immature. To support this area of research, there is a surge of high-quality datasets (e.g., V2X-Sim~\cite{V2XSim}, OpenV2V~\cite{OPV2V}, Comap\cite{comap} and DAIR-V2X\cite{dair}), as well as collaboration methods aimed for better performance-bandwidth trade-off (see comparisons in Table~\ref{tab:comparison}). When2com~\cite{when2com} proposes a handshake communication mechanism to decide \textit{when} to communicate and create sparse communication graph. V2VNet~\cite{v2vnet} proposes multi-round message passing based on graph neural networks to achieve better perception and prediction performance. DiscoNet~\cite{disconet} adopts knowledge distillation to take the advantage of both early and intermediate collaboration. OPV2V~\cite{OPV2V} proposes a graph-based attentive intermediate fusion to improve perception performances. V2X-ViT~\cite{xu2022v2x} introduces a novel heterogeneous multi-agent attention module to fuse information across heterogeneous agents. In this work, we leverage the proposed spatial confidence map to promote more compact messages, more sparse communication graphs, and more comprehensive fusion, resulting in efficient and effective collaboration. 
%
 \vspace{-3mm}
\section{Problem Formulation}
\label{sec:ProblemForm}
\vspace{-2mm}
Consider $N$ agents in the scene. Let $\mathcal{X}_i$ and $\mathcal{Y}_i$ be the observation and the perception supervision of the $i$th agent, respectively. The objective of collaborative perception is to achieve the maximized perception performance of all agents as a function of the total communication budge $B$ and communication round $K$; that is,
\vspace{-1mm}
\begin{equation*}
    {\xi}_{\Phi}(B, K) = \underset{\theta,\mathcal{P}}{\arg \max}~\sum_{i=1}^{N} 
    g \left(\Phi_{\theta} \left(\mathcal{X}_i,\{\mathcal{P}_{i\rightarrow j}^{(K)}\}_{j=1}^N \right), \mathcal{Y}_i  \right),~
    ~{\rm s.t.~} \sum_{k=1}^{K} \sum_{i=1}^{N}|\mathcal{P}_{i\rightarrow j}^{(k)}| \leq B,
\vspace{-1mm}
\end{equation*}
where $g(\cdot,\cdot)$ is the perception evaluation metric, $\Phi$ is the perception network with trainable parameter $\theta$, and $\mathcal{P}_{i\rightarrow j}^{(k)}$ is the message transmitted from the $i$th agent to the $j$th agent at the $k$th communication round. Note that i) when $B=K=0$, there is no collaboration and ${\xi}_{\Phi}(0, 0)$ reflects the single-agent perception performance; ii) through optimizing the communication strategy and the network parameter, collaborative perception should perform well consistently at any communication bandwidth or round; and iii) we consider multi-round communication, where each agent serves as both a supporter (offering message to help others) and a requester (requesting messages from others).



In this work, we consider the perception task of 3D object detection and present three contributions: i) we make communication more efficient by designing compact messages and sparse communication graphs; ii) we boost the perception performance by implementing more comprehensive message fusion; iii) we enable the overall system to adapt to varying communication conditions by dynamically adjusting where and who to communicate.



\vspace{-2mm}
\section{\texttt{Where2comm}: Spatial Confidence-Aware Collaborative Perception System}
\vspace{-2mm}

This section presents~\texttt{Where2comm}, a multi-round, multi-modality, multi-agent collaborative perception framework based on a spatial-confidence-aware communication strategy; see the overview in Fig.~\ref{fig:system}. \texttt{Where2comm} includes an observation encoder, a spatial confidence generator, the spatial confidence-aware communication module, the spatial confidence-aware message fusion module and a detection decoder. Among five modules, the proposed spatial confidence generator generates the spatial confidence map. Based on this spatial confidence map, the proposed spatial confidence-aware communication generates compact messages and sparse communication graphs to save communication bandwidth; and the proposed spatial confidence-aware message fusion module leverages informative spatial confidence priors to achieve better aggregation; also see an algorithmic summary in Algorithm 1 and the optimization-oriented design rationale in Section 7.3 in Appendix.


\begin{figure}[!t]
    \centering
\includegraphics[width=1.0\linewidth]{figures/System/system-final.png}
\caption{System overview. In \texttt{Where2comm}, spatial confidence generator enables the awareness of spatial heterogeneous of perceptual information, spatial confidence-aware communication enables efficient communication, and spatial confidence-aware message fusion boosts the performance.}
    \label{fig:system}
    \vspace{-5mm}
\end{figure}

\vspace{-2mm}
\subsection{Observation encoder}
\label{sec:Encoder}
\vspace{-2mm}
The observation encoder extracts feature maps from the sensor data.~\texttt{Where2comm} accepts single/multi-modality inputs, such as RGB images and 3D point clouds. This work adopts the feature representations in bird's eye view (BEV), where all agents project their individual perceptual information to the same global coordinate system, avoiding complex coordinate transformations and supporting better shared cross-agent collaboration. For the $i$th agent, given its input $\mathcal{X}_i$, the feature map is 
$ 
\mathcal{F}_i^{(0)} = \Phi_{\rm enc}(\mathcal{X}_i) \in \mathbb{R}^{H \times W \times D},$
where $\Phi_{\rm enc}(\cdot)$ is the encoder, the superscript $0$ reflects that the feature is obtained before communication and $H,W,D$ are its height, weight and channel. All agents share the same BEV coordinate system. For the image input, $\Phi_{\rm enc}(\cdot)$ is followed by a warping function that transforms the extracted feature from front-view to BEV. For 3D point cloud input, we discretize 3D points as a BEV map and $\Phi_{\rm enc}(\cdot)$ extracts features in BEV. The extracted feature map is output to the spatial confidence generator and the message fusion module.


\vspace{-2mm}
\subsection{Spatial confidence generator}
\label{sec:Generator}
\vspace{-2mm}
The spatial confidence generator generates a spatial confidence map from the feature map of each agent. The spatial confidence map reflects the perceptually critical level of various spatial areas. Intuitively, for object detection task, the areas that contain objects are more critical than background areas. During collaboration, areas with objects could help recover the miss-detected objects due to the limited view; and background areas could be omitted to save the precious bandwidth. So we represent the spatial confidence map with the detection confidence map, where the area with high perceptually critical level is the area that contains an object with a high confidence score.

To implement, we use a detection decoder structure to produce the detection confidence map. Given the feature map at the $k$th communication round, $\mathcal{F}_i^{(k)}$, the corresponding spatial confidence map is 
\vspace{-1mm}
\begin{equation}
\label{eq:generator}
    \mathbf{C}_i^{(k)}  =  \Phi_{\rm generator}(\mathcal{F}_i^{(k)})  \in [0,1]^{H \times W},
\end{equation}
where the generator $\Phi_{\rm generator}(\cdot)$ follows a detection decoder. Since we consider multi-round collaboration, \texttt{Where2comm} iteratively updates the feature map by aggregating information from other agents. Once $\mathcal{F}_i^{(k)}$ is obtained,~\eqref{eq:generator} is triggered to reflect the perceptually critical level at each spatial location. The proposed spatial confidence map answers a crucial question that was ignored by previous works: for each agent, information at which spatial area is worth sharing with others. By answering this, it provides a solid base for efficient communication and effective message fusion.

\vspace{-2mm}
\subsection{Spatial confidence-aware communication}
\label{sec:Comm}
\vspace{-2mm}
With the guidance of spatial confidence maps, the proposed communication module packs compact messages with spatially sparse feature maps and transmits messages through a sparsely-connected communication graph. Most existing collaboration perception systems~\cite{v2vnet,disconet,xu2022v2x} considers full feature maps in the messages and fully-connected communication graphs. To reduce the communication bandwidth without affecting perception, we leverage the spatial confidence map to select the most informative spatial areas in the feature map (where to communicate) and decide the most beneficial collaboration partners (who to communicate).

\textbf{Message packing.} Message packing determines what information should be included in the to-be-sent message. The proposed message includes: i) a request map that indicates at which spatial areas the agent needs to know more; and ii) a spatially sparse, yet perceptually critical feature map.

The request map of the $i$th agent is $\mathbf{R}_{i}^{(k)} = 1 - \mathbf{C}_{i}^{(k)} \in \mathbb{R}^{H\times W}$, negatively correlated with the spatial confidence map. The intuition is, for the locations with low confidence score, an agent is hard to tell if there is really no objects or it is just caused by the limited information (e.g. occlusion). Thus, the low confidence score indicates there could be missing information at that location. Requesting information at these locations from other agents could improve the current agent's detection accuracy.

The spatially sparse feature map are selected based on each agent's spatial confidence map and the received request maps from others. Specifically, a binary selection matrix is used to represent each location is selected or not, where $1$ denotes selected, and $0$ elsewhere. For the message sent from the $i$th agent to the $j$th agent at the $k$th communication round, the binary selection matrix is
\begin{equation}
\vspace{-1mm}
\label{eq:selector}
\mathbf{M}_{i\rightarrow j}^{(k)}=\left\{
\begin{array}{lll}
\Phi_{\rm select}(\mathbf{C}_{i}^{(k)})\in \{0, 1\}^{H\times W}, &\quad k=0;\\
\Phi_{\rm select}(\mathbf{C}_{i}^{(k)}\odot \mathbf{R}_j^{(k-1)} ), \in \{0, 1\}^{H\times W},  &\quad k>0;
\vspace{-1mm}
\end{array} \right.
\end{equation}
where $\odot$ is the element-wise multiplication, $\mathbf{R}_j^{(k-1)}$ is the request map from the $j$th agent received at the previous round, $\Phi_{\rm select}(\cdot)$ is the selection function which targets to select the most critical areas conditioned on the input matrix, which represents the critical level at the certain spatial location. We implement $\Phi_{\rm select}(\cdot)$ by selecting the locations where the largest elements at in the given input matrix conditioned on the bandwidth limit; optionally, a Gaussian filter could be applied to filter out the outliers and introduce some context. In the initial communication round, each agent selects the most critical areas from its own perspective as the request maps from other agents are not available yet; in the subsequent rounds, each agent also takes the partner's request into account, enabling more targeted communication. Then, the selected feature map is obtained as
$
    \mathcal{Z}_{i\rightarrow j}^{(k)}=\mathbf{M}_{i\rightarrow j}^{(k)}\odot \mathcal{F}_i^{(k)} \in \mathbb{R}^{H\times W \times D},
$
which provides spatially sparse, yet perceptually critical information. 

Overall, the message sent from the $i$th agent to the $j$th agent at the $k$th communication round is $\mathcal{P}_{i\rightarrow j}^{(k)}=(\mathbf{R}_{i}^{(k)}, \mathcal{Z}_{i\rightarrow j}^{(k)} )$. Note that i) $\mathbf{R}_{i}^{(k)}$ provides spatial priors to request complementary information for the $i$th agent's need in the next round; the feature map $ \mathcal{Z}_{i\rightarrow j}^{(k)}$ provides supportive information for the $i$th agent's need in the this round. They together enable mutually beneficial collaboration; ii) since  $\mathcal{Z}_{i\rightarrow j}^{(k)}$ is sparse, we only transmit non-zero features and corresponding indices, leading to low communication cost; and iii) the sparsity of $\mathcal{Z}_{i\rightarrow j}^{(k)}$ is determined by the binary selection matrix, which dynamically allocates the communication budget at various spatial areas based on their perceptual critical level, adapting to various communication conditions. 




\textbf{Communication graph construction.} Communication graph construction targets to identify when and who to communicate to avoid unnecessary communication that wastes the bandwidth. Most previous works~\cite{v2vnet,disconet,OPV2V} consider fully-connected communication graphs. When2com~\cite{when2com} proposes a handshake mechanism, which uses similar global features to match partners. This is hard to interpret because two agents, which have similar global features, do not necessarily need information from each other. Different from all previous works, we provide an explicit design rationale: the necessity of communication between the $i$th and the $j$th agents is simply measured by the overlap between the information that the $i$th agent has and the information that the $j$th agent needs. With the help of the spatial confidence map and the request map, we construct a more interpretable communication graph.

For the initial communication round, every agent in the system is not aware of other agents yet. To activate the collaboration, we construct a fully-connected communication graph. Every agent will broadcast its message to the rest of the system. For the subsequent communication rounds, we examine if the communication between agent $i$ and agent $j$ is necessary based on the maximum value of the binary selection matrix $\mathbf{M}_{i\rightarrow j}^{(k)}$, i.e. if there is at least one patch is activated, then we regard the connection is necessary. Formally, let $\mathbf{A}^{(k)}$ be the adjacency matrix of the communication graph at the $k$th communication round, whose $(i,j)$th element is \vspace{-1mm}
\begin{equation*}
 \mathbf{A}_{i,j}^{(k)}=\left\{
\begin{array}{lll}
1, &\quad k=0; \\
{\rm max}_{h \in \{0,1,..,H-1\},w \in \{0,1,...,W-1\}}~ \left( \mathbf{M}_{i\rightarrow j}^{(k)} \right)_{h,w} \in \{0, 1\}, &\quad k>0;
\end{array} \right.
\end{equation*}
where $h, w$ index the spatial area, reflecting message passing from the $i$th agent to the $j$th agent.  Given this sparse communication graph, agents can exchange messages with selected partners.



\vspace{-2mm}
\subsection{Spatial confidence-aware message fusion}
\label{sec:Fusion}
\vspace{-2mm}
Spatial confidence-aware message fusion targets to augment the feature of each agent by aggregating the received messages from the other agents. To achieve this, we adopt a transformer architecture, which leverages multi-head attention to fuse the corresponding features from multiple agents at each individual spatial location. The key technical design is to include the spatial confidence maps of all the agents to promote cross-agent attention learning. The intuition is that, the spatial confidence map could explicitly reflect the perceptually critical level, providing a useful prior for attention learning.

Specifically, for the $i$th agent, after receiving the $j$th agent's message $\mathcal{P}_{j\rightarrow i}^{(k)}$, it could unpack to retrieve the feature map $\mathcal{Z}^{(k)}_{j\rightarrow i}$ and the spatial confidence map $\mathbf{C}^{(k)}_j = 1 - \mathbf{R}^{(k)}_j$. We also include the ego feature map in fusion and denote $\mathcal{Z}^{(k)}_{i\rightarrow i} = \mathcal{F}_{i}^{(k)}$ to make the formulation simple and consistent, where $\mathcal{Z}^{(k)}_{i\rightarrow i}$ might not be sparse. To fuse the features from the $j$th agent at the $k$th communication round, the cross-agent/ego attention weight for the $i$th agent is
\begin{equation}
    \label{eq:attention}
    \mathbf{W}_{j\rightarrow i}^{(k)} = {\rm MHA}_{\rm W}\left(\mathcal{F}_{i}^{(k)}, \mathcal{Z}_{j\rightarrow i}^{(k)}, \mathcal{Z}_{j\rightarrow i}^{(k)}\right) \odot  \mathbf{C}^{(k)}_j \in \mathbb{R}^{H\times W},
\end{equation}
where ${\rm MHA}_{\rm W}(\cdot)$ is a multi-head attention applied at each individual spatial location, which outputs the scaled dot-product attention weight. Note that i) the proposed spatial confidence maps contributes to the attention weight, as the features with higher perceptually critical level are more preferred in the feature aggregation; ii) the cross-agent attention weight models the collaboration strength with a $H\times W$ spatial resolution, leading to more flexible information fusion at various spatial regions. Then, the feature map of the $i$th agent after fusing the messages in the $k$th communication round is
\begin{align*}
\vspace{-4mm}
    \mathcal{F}_{i}^{(k+1)} &= {\rm FFN} \left( \sum_{j\in  \mathcal{N}_i\bigcup \{i\} } {\mathbf{W}}_{j\rightarrow i}^{(k)} \odot \mathcal{Z}_{j\rightarrow i}^{(k)}   \right) \in \mathbb{R}^{H\times W\times D},
\vspace{-4mm}
\end{align*}
where ${\rm FFN}(\cdot)$ is the feed-forward network and $\mathcal{N}_i$ is the neighbors of the $i$th agent defined in the communication graph $\mathbf{A}^{(k)}$. The fused feature $\mathcal{F}_{i}^{(k+1)}$ would serve as the $i$th agent's feature in the $(k+1)$th round. In the final round, we output $\mathcal{F}_{i}^{(k+1)}$ to the detection decoder to generate detections.

\textbf{Sensor positional encoding.} Sensor positional encoding represents the physical distance between each agent's sensor and its observation.  It adopts a standard positional encoding function conditioned on the sensing distance and feature dimension. The features are summed up with the positional encoding of each location before inputting to the transformer.

Compared to existing fusion modules that do not use attention mechanism~\cite{v2vnet} or only use agent-level attentions~\cite{when2com}, the per-location attention mechanism adopted by the proposed fusion emphasizes the location-specific feature interactions. It  makes the feature fusion more targeted. Compared to the methods that also use the per-location attention-based fusion module\cite{disconet,OPV2V,xu2022v2x}, the proposed fusion module leverages multi-head attention with two extra priors, including spatial confidence map and sensing distances. Both assist attention learning to prefer high quality and critical features.

\vspace{-2mm}
\subsection{Detection decoder}
\label{sec:Decoder}
\vspace{-2mm}
The detection decoder decodes features into objects, including class and regression output. 
Given the feature map at the $k$th communication round $\mathcal{F}_i^{(k)}$, the detection decoder $\Phi_{\rm dec}(\cdot)$ generate the detections of $i$th agent by
$
     \widehat{\mathcal{O}}_i^{(k)}  =  \Phi_{\rm dec}(\mathcal{F}_i^{(k)}) \in \mathbb{R}^{H \times W \times 7},
$
where each location of $\widehat{\mathcal{O}}_i^{(k)}$ represents a rotated box with class $(c,x,y,h,w, \cos\alpha, \sin\alpha)$, denoting class confidence, position, size and angle. The objects are the final output of the proposed collaborative perception system. Note that $\widehat{\mathcal{O}}_i^{(0)}$ denotes the detections without collaboration.

\vspace{-2mm}
\subsection{Training details and loss functions}
\vspace{-2mm}
To train the overall system, we supervise two tasks: spatial confidence generation and object detection at each round. As mentioned before, the functionality of the spatial confidence generator is the same as the classification in the detection decoder. To promote parameter efficiency, our spatial confidence generator reuses the parameters of the detection decoder. For the multi-round settings, each round is supervised with one detection loss, the overall loss is
$
    L = \sum_{k=0}^K\sum_i^{N} L_{\rm det} \left(\widehat{\mathcal{O}}_i^{(k)},\mathcal{O}_i \right),
$
where $\mathcal{O}_i$ is the $i$th agent's ground-truth objects, $L_{\rm det}$ is the detection loss~~\cite{zhou2019objects}.


\textbf{Training strategy for multi-round setting.} To adapt to multi-round communication and dynamic bandwidth, we train the model under various communication settings with curriculum learning strategy~\cite{curriculum}. We first gradually increase the communication bandwidth and round; and then, randomly sample bandwidth and round to promote robustness. Through this training strategy, a single model can perform well at various communication conditions.
%
 \vspace{-2mm}
\section{Experimental Results}
\vspace{-2mm}


\begin{figure}[!t]
  \centering
\begin{subfigure}{0.49\linewidth}
    \includegraphics[width=0.9\linewidth]{figures/UAV_Performance/SOTA_Performance_AP0.50_vs_Commcost.png}
    \caption{CoPerception-UAVs}
    \label{fig:UAV_SOTA}
  \end{subfigure}
\begin{subfigure}{0.49\linewidth}
    \includegraphics[width=0.9\linewidth]{figures/OPV2V_Performance/SOTA_Performance_AP0.50_vs_Commcost.png}
    \caption{OPV2V}
    \label{fig:OPV2V_SOTA}
  \end{subfigure}
\begin{subfigure}{0.49\linewidth}
    \includegraphics[width=0.9\linewidth]{figures/V2X-Sim_Detections/CommGraph_AP0.50_vs_Commcost.png}
    \caption{V2X-Sim}
    \label{fig:V2X_SOTA}
    \end{subfigure}
\begin{subfigure}{0.49\linewidth}
    \includegraphics[width=0.9\linewidth]{figures/DAIR_V2X/SOTA_Performance_AP0.50_vs_Commcost.png}
    \caption{DAIR-V2X}
    \label{fig:DAIR_SOTA}
    \end{subfigure}
  \vspace{-1mm}
  \caption{\texttt{Where2comm} achieves consistently superior performance-bandwidth trade-off on all the three collaborative perception datasets, e.g, \texttt{Where2comm} achieves \emph{5,000} times less communication volume and still outperforms When2com on CoPerception-UAVs dataset. The entire red curve comes from a single~\texttt{Where2comm} model evaluated at varying bandwidths.}
  \label{fig:SOTA}
  \vspace{-3mm}
\end{figure}


Our experiments covers four datasets, both real-world and simulation scenarios, two types of agents (cars and drones) and two types of sensors (LiDAR and cameras). Specifically, we conduct camera-only 3D object detection in the setting of V2X-communication aided autonomous driving on OPV2V dataset~\cite{OPV2V}, camera-only 3D object detection in the setting of drone swarm on the proposed CoPerception-UAVs dataset, and LiDAR-based 3D object detection on DAIR-V2X dataset~\cite{dair} and V2X-Sim dataset~\cite{V2XSim}. The detection results are evaluated by Average Precision (AP) at Intersection-over-Union (IoU) threshold of $0.50$ and $0.70$. The communication results count the message size by byte in log scale with base $2$. To compare communication results straightforward and fair, we do not consider any extra data/feature/model compression.

\vspace{-2mm}
\subsection{Datasets and experimental settings}
\label{sec:ExpDetails}
\vspace{-2mm}
\textbf{OPV2V.}
OPV2V~\cite{OPV2V} is a vehicle-to-vehicle collaborative perception dataset, co-simulated by OpenCDA~\cite{OPV2V} and Carla~\cite{carla}. It includes $12$K frames of 3D point clouds and RGB images with $230$K annotated 3D boxes. The perception range is 40m$\times$40m. For camera-only 3D object detection task on OPV2V, we implement the detector following CADDN~\cite{CaDDN}. The input front-view image size is $(416, 160)$. The front-view input feature map is transformed to BEV with resolution $0.5$m/pixel.


\textbf{V2X-Sim.}
V2X-Sim~\cite{V2XSim} is a vehicle-to-everything collaborative perception dataset, co-simulated by SUMO~\cite{sumo} and Carla, including 10K frames of 3D LiDAR  point clouds and 501K 3D boxes. The perception range is 64m$\times$64m. For LiDAR-based 3D object detection task, our detector follows MotionNet~\cite{Wu2020MotionNetJP}. We discretize 3D points into a BEV map with size  $(256, 256, 13)$ and the resolution is $0.4$m/pixel in length and width, $0.25$m in height.


\textbf{CoPerception-UAVs.}
To enrich the collaborative perception datasets, we consider the swarm of unmanned aerial vehicles (UAV) and propose a UAV-swarm-based collaborative perception dataset: CoPerception-UAVs, co-simulated by AirSim~\cite{Airsim} and Carla~\cite{carla}, including 131.9K aerial images and 1.94M 3D boxes. The perception range is 200m$\times$350m. For the camera-only 3D object detection task on CoPerception-UAVs, our detector follows DVDET~\cite{DVDET}. The input aerial image size is $(800, 450)$. The aerial-view input feature map is transformed to BEV with the resolution of $0.25$m/pixel, and the size is $(192,352)$; see more details in Appendix.


\textbf{DAIR-V2X.} DAIR-V2X~\cite{dair} is the only public \textbf{real-world} collaborative perception dataset. Each sample contains two agents: a vehicle and an infrastructure, with 3D annotations. The perception range is 201.6m$\times$80m.  Originally DAIR-V2X does not label objects outside the camera's view, we relabel all objects to cover 360-degree detection range. We complement several intermediate fusion-based baselines on DAIR-V2X to comprehensively validate our method on real data. For LiDAR-based 3D object detection task, our detector follows PointPillar~\cite{PointPillar}. We represent the field of view into a BEV map with size $(200, 504, 64)$ and the resolution is $0.4$m/pixel in length and width.




\vspace{-2mm}
\subsection{Quantitative evaluation}
\vspace{-2mm}




\textbf{Benchmark comparison.} Fig.~\ref{fig:SOTA} compares the proposed \texttt{Where2comm} with the previous methods in terms of the trade-off between detection performance (AP@IoU=0.50) and communication bandwidth; also see exact values in Table 3 of Appendix. We consider single-agent detection without collaboration ($\widehat{\mathcal{O}}_i^{(0)}$), When2com~\cite{when2com}, V2VNet~\cite{v2vnet}, DiscoNet~\cite{disconet}, V2X-ViT~\cite{xu2022v2x} and late fusion, where agents directly exchange the detected 3D boxes. The red curve comes from a single~\texttt{Where2comm} model evaluated at varying bandwidths. We see that the proposed \texttt{Where2comm}: i) achieves a far-more superior perception-communication trade-off across all the communication bandwidth choices and various collaborative perception tasks, including camera-only 3D object detection from aerial view and car front view, and LiDAR-based 3D object detection; ii) achieves significant improvements over previous state-of-the-arts on both real-world (DAIR-V2X) and simulation scenarios, improves the SOTA performance by 7.7\% on DAIR-V2X, 6.62\% on CoPerception-UAVs, 25.81\% on OPV2V, 1.9\% on V2X-Sim; iii) achieves the same detection performance of previous state-of-the-arts with extremely less communication volume: 5128 times less on CoPerception-UAVs, more than 100K times less on OPV2V, 55 times less on V2X-Sim, 105 times less on DAIR-V2X.




\begin{figure}[!t]
  \centering
\begin{subfigure}{0.32\linewidth}
    \includegraphics[width=0.85\linewidth]{figures/UAV_Performance/Multiround_AP0.50_vs_Commcost.png}
    \vspace{-1mm}
    \caption{CoPerception-UAVs}
    \label{fig:UAV_MultiRound}
  \end{subfigure}
\begin{subfigure}{0.32\linewidth}
    \includegraphics[width=0.85\linewidth]{figures/OPV2V_Performance/Multiround_AP0.50_vs_Commcost.png}
    \vspace{-1mm}
    \caption{OPV2V}
    \label{fig:OPV2V_MultiRound}
  \end{subfigure}
\begin{subfigure}{0.32\linewidth}
    \includegraphics[width=0.85\linewidth]{figures/V2X-Sim_Detections/CommGraph_AP0.50_vs_Commcost_round.png}
    \vspace{-1mm}
    \caption{V2X-Sim}
    \label{fig:V2X_MultiRound}
    \end{subfigure}
  \vspace{-2mm}
  \caption{More communication rounds continuously improve performance-bandwidth trade-off.}
  \label{fig:multiround}
  \vspace{-5mm}
\end{figure}



\textbf{Multi-round evaluation.} Fig.~\ref{fig:multiround} presents the performances of~\texttt{Where2comm} at communication rounds ranging from 1 to 3. Each curve comes from a single~\texttt{Where2comm} model with a certain communication round evaluated at varying bandwidths. Results show that 1 communication round is good, more rounds are even better. Multi-round communication steadily improves the performance-bandwidth trade-off across all three datasets, reflecting its effectiveness and robustness. This encourages the agents to actively collaborate without worrying the performance degradation. This also validates that~\texttt{Where2comm} can well work at various communication bandwidths and rounds.



\begin{figure}
    \centering
    \centering
  \begin{subfigure}{0.32\linewidth}
    \includegraphics[width=0.95\linewidth]{figures/LocalizationError/LocalizationError_UAV.png}
\caption{CoPerception-UAVs}
    \label{fig:UAV}
  \end{subfigure}
\begin{subfigure}{0.32\linewidth}
    \includegraphics[width=0.95\linewidth]{figures/LocalizationError/LocalizationError_OPV2V.png}
\caption{OPV2V}
    \label{fig:OPV2V}
  \end{subfigure}
  \begin{subfigure}{0.32\linewidth}
    \includegraphics[width=0.95\linewidth]{figures/LocalizationError/LocalizationError_V2X.png}
\caption{V2X-Sim}
    \label{fig:V2X}
  \end{subfigure}
  \vspace{-1mm}
    \caption{Robustness to localization error. Gaussian noise with zero mean and varying std is introduced. \textit{Where2comm} consistently outperforms previous SOTAs and No Collaboration.}
    \label{fig:loc_noise}
    \vspace{-4mm}
\end{figure}

\textbf{Robustness to localization noise.}
We follow the localization noise setting in V2VNet and V2X-ViT (Gaussian noise with a mean of 0m and a standard deviation of 0m-0.6m) and conduct experiments on all the three datasets to validate the robustness against realistic localization noise. \textit{Where2comm} is more robust to the localization noise than previous SOTAs. Fig.~\ref{fig:loc_noise} shows the detection performances as a function of localization noise level in CoPerception-UAVs, OPV2V and V2X-Sim datasets, respectively We see: i) overall the collaborative perception performance degrades with the increasing localization noise, while \textit{where2comm} outperforms previous SOTAs (When2com, V2VNet,DiscoNet) under all the localization noise. ii) \textit{where2comm} keeps being superior to \textit{No~Collaboration} while V2VNet fails when noise is over 0.4m and DiscoNet fails when noise is over 0.5m on CoPerception-UAVs. The reasons are: i) the powerful transformer architecture in fusion module attentively select the most suitable collaborative feature; ii) the spatial confidence map helps filter out noisy features, these two designs work together to mitigate noise localization distortion effects.



\begin{figure}[!t]

  \centering
\begin{subfigure}{0.18\linewidth}
    \includegraphics[width=0.99\linewidth]{figures/UAV_Pair/1_Image.png}
    \vspace{-5mm}
    \caption{$\mathcal{X}_1$ in BEV}
    \label{fig:UAV_BEVImage1}
  \end{subfigure}
\begin{subfigure}{0.18\linewidth}
    \includegraphics[width=0.99\linewidth]{figures/UAV_Pair/1_ConfidenceMap.png}
    \vspace{-5mm}
    \caption{$\mathbf{C}_1^{(0)}$}
    \label{fig:UAV_ConfMap1}
  \end{subfigure}
  \begin{subfigure}{0.18\linewidth}
    \includegraphics[width=0.99\linewidth]{figures/UAV_Pair/1_SelectionMatrix.png}
    \vspace{-5mm}
    \caption{$\mathbf{M}_{1\rightarrow 2}^{(0)}$}
    \label{fig:UAV_SelectMat1}
  \end{subfigure}
  \begin{subfigure}{0.18\linewidth}
    \includegraphics[width=0.99\linewidth]{figures/UAV_Pair/1to1_AttentionWeight.png}
    \vspace{-5mm}
    \caption{$\mathbf{W}_{1\rightarrow 1}^{(0)}$}
    \label{fig:UAV_AttenWeight1to1}
  \end{subfigure}
\begin{subfigure}{0.18\linewidth}
    \includegraphics[width=0.99\linewidth]{figures/UAV_Pair/1_DetectionSingle.png}
    \vspace{-5mm}
    \caption{$\widehat{\mathcal{O}}_{1}^{(0)}$}
    \label{fig:UAV_DetectionBeforeComm}
  \end{subfigure}
  \begin{subfigure}{0.04\linewidth}
    \includegraphics[width=0.99\linewidth]{figures/UAV_Weights/ColorBar.png}
\end{subfigure}
  \centering
  \begin{subfigure}{0.18\linewidth}
    \includegraphics[width=0.99\linewidth]{figures/UAV_Pair/2_Image.png}
    \vspace{-5mm}
    \caption{$\mathcal{X}_2$ in BEV}
    \label{fig:UAV_BEVImage2}
  \end{subfigure}
\begin{subfigure}{0.18\linewidth}
    \includegraphics[width=0.99\linewidth]{figures/UAV_Pair/2_RequestMap.png}
    \vspace{-5mm}
    \caption{$\mathbf{R}_2^{(0)}$}
    \label{fig:UAV_RequestMap2}
  \end{subfigure}
  \begin{subfigure}{0.18\linewidth}
    \includegraphics[width=0.99\linewidth]{figures/UAV_Pair/2_Feature.png}
    \vspace{-5mm}
    \caption{$\mathcal{Z}_{2\rightarrow 1}^{(0)}$}
    \label{fig:UAV_Message2}
  \end{subfigure}
  \begin{subfigure}{0.18\linewidth}
    \includegraphics[width=0.99\linewidth]{figures/UAV_Pair/2to1_AttentionWeight.png}
    \vspace{-5mm}
    \caption{$\mathbf{W}_{2\rightarrow 1}^{(0)}$}
    \label{fig:UAV_AttenWeight2to1}
  \end{subfigure}
\begin{subfigure}{0.18\linewidth}
    \includegraphics[width=0.99\linewidth]{figures/UAV_Pair/1_Detection.png}
    \vspace{-5mm}
    \caption{$\widehat{\mathcal{O}}_{1}^{(1)}$}
    \label{fig:UAV_DetectionAfterComm}
  \end{subfigure}
  \begin{subfigure}{0.04\linewidth}
    \includegraphics[width=0.99\linewidth]{figures/UAV_Weights/ColorBar.png}
\end{subfigure}
\vspace{-2mm}
  \caption{Visualization of collaboration between Drone 1 and Drone 2 on CoPerception-UAVs dataset, including spatial confidence map ($\mathbf{C}_1^{(0)}$), selection matrix ($\mathbf{M}_{1\rightarrow2}^{(0)}$), message ($\{\mathbf{R}_2^{(0)},\mathcal{Z}_{2\rightarrow1}^{(0)}\}$) in the communication module, attention weight in the fusion module ($\mathbf{W}_{1\rightarrow1}^{(0)}$,$\mathbf{W}_{2\rightarrow1}^{(0)}$), and Drone 1's detection results before ($\widehat{\mathcal{O}}_{1}^{(0)}$) and after ($\widehat{\mathcal{O}}_{1}^{(1)}$) collaboration. \textcolor{green}{Green} and \textcolor{red}{red} boxes denote ground-truth and detection, respectively. The objects occluded by a tall building can be detected through transmitting spatially sparse, yet perceptually critical message. }
  \label{fig:UAV_Weights}
  \vspace{-4mm}
\end{figure}

\vspace{-2mm}
\subsection{Qualitative evaluation}
\vspace{-2mm}

\textbf{Visualization of spatial confidence map.} Fig.~\ref{fig:UAV_Weights} illustrates how~\texttt{Where2comm} is empowered by the proposed spatial confidence map. In the scene, Drone 1's view is occluded by a tall building. With Drone 2's help, Drone 1 is able to detect through occlusion. Fig.~\ref{fig:UAV_Weights} (a-d) shows Drone 1's observation, spatial confidence map~\eqref{eq:generator}, binary selection matrix~\eqref{eq:selector}, and ego attention weight~\eqref{eq:attention}. Fig.~\ref{fig:UAV_Weights} (f-h) shows Drone 2's observation and message sent to Drone 1, including the request map (opposite of confidence map) and the sparse feature map, achieving efficient communication. Fig.~\ref{fig:UAV_Weights} (i) shows the attention weight for Drone 1 to fuse Drone 2's messages, which is sparse, yet highlights the objects' positions. Fig.~\ref{fig:UAV_Weights} (e) and (j) compares the detection results before and after the collaboration with Drone 2. We see that the proposed spatial confidence map contributes to spatially sparse, yet perceptually critical message, which effectively helps Drone 1 detect occluded objects.






\textbf{Visualization of detection results.} Fig.~\ref{fig:dairdetections} shows that compared to~\textit{No Collaboration},~\textit{When2com} and~\textit{DiscoNet},~\texttt{Where2comm} is able to achieves more complete and accurate detection results. The reason is that~\textit{When2com} employs a scalar to denote the agent-to-agent attention, which cannot distinguish which spatial area is more informative;~\textit{DiscoNet} employs a MLP-based fusion weight learning, which cannot well capture the complex collaboration attention; while \texttt{Where2comm} can zoom in to critical spatial areas in a cell-level resolution and leverage  the spatial confidence map and sensing distances as priors to achieve more comprehensive fusion.









\begin{figure}
    \centering
  \begin{subfigure}{0.24\linewidth}
    \includegraphics[width=0.99\linewidth]{figures/DAIR_V2X/single_bev_00019.png}
\caption{No Collaboration}
    \label{fig:no_comm}
  \end{subfigure}
\begin{subfigure}{0.24\linewidth}
    \includegraphics[width=0.99\linewidth]{figures/DAIR_V2X/when2com_bev_00019.png}
\caption{When2com}
    \label{fig:when2com}
  \end{subfigure}
  \begin{subfigure}{0.24\linewidth}
    \includegraphics[width=0.99\linewidth]{figures/DAIR_V2X/disconet_bev_00019.png}
    \caption{DiscoNet}
    \label{fig:disconet}
  \end{subfigure}
  \begin{subfigure}{0.24\linewidth}
    \includegraphics[width=0.99\linewidth]{figures/DAIR_V2X/where2comm_bev_00019.png}
    \caption{Where2comm}
    \label{fig:where2comm}
  \end{subfigure}
  \vspace{-2mm}
    \caption{\texttt{Where2comm} qualitatively outperforms When2com and DiscoNet in DAIR-V2X dataset. \textcolor{green}{Green} and \textcolor{red}{red} boxes denote ground-truth and detection, respectively. \textcolor{yellow}{Yellow} and \textcolor{blue}{blue} denote the point clouds collected from vehicle and infrastructure, respectively. }
    \label{fig:dairdetections}
    \vspace{-4mm}
\end{figure}














\begin{minipage}{0.38\textwidth}
     \includegraphics[width=1.0\linewidth]{figures/UAV_Performance/GaussianSmooth_AP0.50_vs_Commcost_04.png}
\captionof{figure}{Selection matrix ablation study. Applying Gaussian filter improves performance.}
  \label{fig:uav_gaussian}
\end{minipage}
\hspace{2mm}
\begin{minipage}{0.6\textwidth}
\setlength\tabcolsep{3pt}
\renewcommand{\arraystretch}{1.3}
\captionof{table}{Fusion component ablation study. \label{tab:fusion}
Multi-head attention (MHA), sensor positional encoding (SPE) and spatial confidence map (SCM) all improves the performances. Results are reported in AP@0.50/AP@0.70.}
\begin{footnotesize}
    \centering
    \begin{tabular}{ccc|ccc}
\hline
MHA    & SPE  & SCM      & OPV2V & CoPerception-UAVs & V2X-Sim     \\ \hline
&           &                        & 34.96/13.92  & 63.48/44.23  & 51.2/45.7 \\
\checkmark &            &            & 38.75/13.28  & 63.99/44.46  & 57.3/50.8 \\
\checkmark & \checkmark &            & 39.82/16.43  & 64.34/46.86  & 59.1/52.0   \\
\checkmark & \checkmark & \checkmark & \textbf{47.30/19.30}  & \textbf{64.83/47.62}  & \textbf{59.1/52.2}   \\ \hline
\end{tabular}
\end{footnotesize}
\end{minipage}

\vspace{-1mm}
\subsection{Ablation studies}
\vspace{-2mm}
\textbf{Effect of Gaussian filter in perceptually critical area selection.} Fig.~\ref{fig:uav_gaussian} compares two versions of the selection matrix~\eqref{eq:selector} with and without Gaussian filter. We see that applying Gaussian filter improves the overall performance. The reason is that: i) Gaussian filter could help filter out the outliers in the input map, selecting more robust critical regions; ii) it considers the context, benefiting the independent feature selection at each certain location by providing more information.

\textbf{Effect of components in spatial confidence-aware message fusion.} Tab.~\ref{tab:fusion} assesses the effectiveness of the proposed fusion with two priors. We see that: i) per-location multi-head attention (MHA) outperforms the vanilla attention by 10.84\% on OPV2V on AP@0.50, because MHA leverages information from multiple heads, better capturing cross-agent attention; and ii) As two informative priors, both sensing position encoding (SPE) and spatial confidence map (SCM) can consistently improve the performance. Especially, the version with all three designs improves the detection performance by 22.06\% on OPV2V on AP@0.50.
 \vspace{-3mm}
\section{Conclusion and limitation}
\vspace{-3mm}
We propose \texttt{Where2comm}, a novel communication-efficient collaborative perception framework. The core idea is to exploit a spatial confidence map at each agent to promote pragmatic compression, assisting agents to decide what to communicate with whom, and whose information to aggregate. Each agent offers spatially sparse, yet perceptually critical features to support other agents; meanwhile, requests complementary information from others in multi-round communication. Comprehensive experiments covering multi-type agents and multi-modality inputs show that \texttt{Where2comm} achieves far superior trade-off between perception performance and communication bandwidth.

\textbf{Limitation and future work.} 
The current work focuses on perceptually critical spatial areas. In future, we plan to expand a similar idea to the temporal dimension and determine critical time stamps. More cost will be reduced by exploring when to communicate. We also expect that more methods on pragmatic compression and emergent communication could be applied to collaborative perception.

\textbf{Acknowledgment.}
This research is partially supported by the National Key R\&D Program of China under Grant 2021ZD0112801, National Natural Science Foundation of China under Grant 62171276, the Science and Technology Commission of Shanghai Municipal under Grant 21511100900, CCF-DiDi GAIA Research Collaboration Plan 202112 and CALT Grant 2021-01. 
\clearpage
{\small
\bibliographystyle{unsrt}
\bibliography{ref}
}
\clearpage
\section*{Checklist}





\begin{enumerate}


\item For all authors...
\begin{enumerate}
  \item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
    \answerYes{}
  \item Did you describe the limitations of your work?
    \answerYes{}
  \item Did you discuss any potential negative societal impacts of your work?
    \answerNA{}
  \item Have you read the ethics review guidelines and ensured that your paper conforms to them?
    \answerYes{}
\end{enumerate}


\item If you are including theoretical results...
\begin{enumerate}
  \item Did you state the full set of assumptions of all theoretical results?
    \answerNA{}
        \item Did you include complete proofs of all theoretical results?
    \answerNA{}
\end{enumerate}


\item If you ran experiments...
\begin{enumerate}
  \item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?
    \answerYes{See Section~\ref{sec:ExpDetails} and the supplemental material.}
  \item Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?
    \answerYes{See Section~\ref{sec:ExpDetails} and the supplemental material.}
        \item Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?
    \answerNo{We have not repeated experiments many times to get error bars since experiments of 3d object detection on large scale datasets is time-consuming.}
        \item Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?
    \answerYes{See Section~\ref{sec:ExpDetails} and the supplemental material.}
\end{enumerate}


\item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
\begin{enumerate}
  \item If your work uses existing assets, did you cite the creators?
    \answerYes{See Section~\ref{sec:ExpDetails}.}
  \item Did you mention the license of the assets?
    \answerYes{See Section~\ref{sec:ExpDetails}.}
  \item Did you include any new assets either in the supplemental material or as a URL?
    \answerYes{See Section~\ref{sec:ExpDetails} and the supplemental material.}
  \item Did you discuss whether and how consent was obtained from people whose data you're using/curating?
    \answerNA{We generate the data using the open-sourced tool and the owners consent to all the public use for research purposes.}
  \item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?
    \answerNA{ Our data is synthesized using the open-sourced tool, so there is no real-world personally identifiable information, nor offensive content.}
\end{enumerate}


\item If you used crowdsourcing or conducted research with human subjects...
\begin{enumerate}
  \item Did you include the full text of instructions given to participants and screenshots, if applicable?
    \answerNA{}
  \item Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?
    \answerNA{}
  \item Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?
    \answerNA{}
\end{enumerate}


\end{enumerate} 
\clearpage
\begin{algorithm}[!t]
	\caption{Multi-round spatial confidence-aware collaborative perception system} 
	\begin{algorithmic}[1]
	    \State Define $N$ as the number of agents , $K$ as communication round
	    \State {\color{blue} { \#~Initialization}}
	    \For {$i=1,2,\ldots, N$,}
	        \State $\mathcal{F}_{i}^{(0)}=\Phi_{\rm enc}(\mathcal{X}_i) \in \mathbb{R}^{H\times W \times D}$
	        \Comment{Extract intermediate feature}
	    \EndFor
	    
	    \For {$k=0,1,\ldots, K-1$,}
	        \For {$i=1,2,\ldots, N$,}  {\color{blue} { \#~ Each agent is computing individually}} 
	            \State $\mathbf{C}_{i}^{(k)} = \Phi_{\rm generator}(\mathcal{F}_{i}^{(k)})\in \mathbb{R}^{H\times W}$ 
	            \Comment{Generate spatial confidence map} 
	            \For {$j=1,2,\ldots, N$,}
	                \State {\color{blue} { \#~Message packing}} 
	                \State $\mathbf{R}_{i}^{(k)}=1 - \mathbf{C}_i^{(k)}\in \mathbb{R}^{H\times W}$ \Comment{Pack request map}
	                \If {$k=0$}
    	                \State $\mathbf{M}_{i\rightarrow j}^{(k)}=\Phi_{\rm select}(\mathbf{C}_{i}^{(k)})\in \{0, 1\}^{H\times W}$ 
    	                \Comment{Select critical areas}
    	            \Else
    	                \State $\mathbf{M}_{i\rightarrow j}^{(k)}=\Phi_{\rm select}(\mathbf{C}_{i}^{(k)}\odot \mathbf{R}_{j}^{(k-1)} ) \in \{0, 1\}^{H\times W}$ 
	                    \Comment{Select requested areas}
	                \EndIf
	                \State $\mathcal{Z}_{i\rightarrow j}^{(k)}=\mathbf{M}_{i\rightarrow j}^{(k)}\odot \mathcal{F}_i^{(k)} \in \mathbb{R}^{H\times W \times D}$
\Comment{Pack spatially sparse features}
	                \State {\color{blue} { \#~Communication graph learning }}
	                \If {$k=0$}
    	                \State $\mathbf{A}_{i\rightarrow j}^{(k)}=1$ 
    	                \Comment{Broadcast critical features and request}
    	            \Else
    	                \State $\mathbf{A}_{i\rightarrow j}^{(k)}={\rm max}_{h,w}~ \left( \mathbf{M}_{i\rightarrow j}^{(k)} \right)_{h,w} \in \{0, 1\}$
    	                \Comment{Communicate only when necessary}
    	            \EndIf
    	        \EndFor
    	        \State {\color{blue} { \#~Communication}} \State Send $\mathcal{P}_{i\rightarrow j} = \left(\mathcal{Z}_{i\rightarrow j}^{(k)},\mathbf{R}_i^{(k)} \right)$ to other agents
    	        \State Receive $\{\mathcal{P}_{j\rightarrow i}=\left(\mathcal{Z}_{j\rightarrow i}^{(k)},\mathbf{R}_j^{(k)}\right), j \neq i \}$ from other agents
    	        \State {\color{blue} { \#~Message fusion}}
	            \State $\mathcal{F}_{i}^{(k+1)}=f_{\rm fuse} \left(\mathcal{F}_{i}^{(k)},\{(\mathcal{Z}_{j\rightarrow i}^{(k)},\mathbf{R}_{j}^{(k)}), j=1,2,...,N\}\right)\in \mathbb{R}^{H\times W\times D}$ 
	       \EndFor
	       \State Store $\mathcal{F}_{i}^{(k+1)}$ and $\{\mathbf{R}_{j}^{(k)},j \neq i\}$ for the next round
	    \EndFor
    \State $\mathcal{O}_{i}^{(K)}=\Phi_{\rm dec}(\mathcal{F}_{i}^{(K)})$
	        \Comment{Output the final detections}
	\end{algorithmic} 
	\label{alg:system}
\end{algorithm}

\section{Appendix}

\subsection{Highlights of our contribution}
To sum up, our contributions are:

$\bullet$ We propose a novel fine-grained spatial-aware communication strategy, where each agent can decide where to communicate and pack messages only related to the most perceptually critical spatial areas. This strategy not only enables more precise support for other agents, but also more targeted request from other agents in multi-round communication.

$\bullet$ We propose~\texttt{Where2comm}, a novel collaborative perception framework based on the spatial-aware communication strategy. With the guidance of the proposed spatial confidence map,~\texttt{Where2comm} leverages novel message packing and communication graph learning to  achieve lower communication bandwidth, and adopts confidence-aware multi-head attention to reach better perception performance.

$\bullet$ We conduct extensive experiments to validate~\texttt{Where2comm} achieves state-of-the-art performance-bandwidth trade-off on multiple challenging real/simulated datasets across views and modalities. 


\subsection{Detailed information about the system pipeline}
Alg.~\ref{alg:system} presents the pipeline of our multi-round spatial confidence-aware collaborative perception system.


\subsection{Detailed information about the optimization problem of collaborative perception}
The constrained optimization in Sec.3 is the mathematical formation of collaborative perception. It is hard to obtain the global optimum due to hard constrains and non-differentialability of binary variables. Therefore, the proposed Where2comm essentially introduces an auxiliary variable and decomposes the original problem into two sub-optimization problems, each one of which is easy to solve.

To understand the details, let us consider a setting of fixed communication bandwidth and communication round, $K=1,B=[B_1]$. Then, the optimization is
\vspace{-1mm}
\begin{equation*}
    \underset{\theta,\mathcal{P}}{\max}~\sum_{i=1}^{N} 
    g \left(\Phi_{\theta} \left(\mathcal{X}_i,\{\mathcal{P}_{i\rightarrow j} \}_{j=1}^N \right), \mathcal{Y}_i  \right),~
    ~{\rm s.t.~} \sum_{i,j=1}^{N}|\mathcal{P}_{i\rightarrow j}| \leq B_1.
\end{equation*}
Since there is only one round, we do not consider the request map, then, the message sent from the $i$th agent to the $j$th agent is 
$\mathcal{P}_{i\rightarrow j}
= \mathcal{Z}_{i\rightarrow j} 
= \mathbf{M}_{i\rightarrow j} \odot \mathcal{F}_i,$ whose spatial sparsity is determined by the binary selection mask $\mathbf{M}_{i\rightarrow j}$. Note that $\mathbf{M}_{i\rightarrow j}$ determines where to communicate, and is the key of the proposed Where2comm. Then, the original optimization is equivalent to 
\begin{equation}
\label{eq:optimization_equivalent}
    \underset{\theta,\mathbf{M}}{\max}~\sum_{i=1}^{N} 
    g \left(\Phi_{\theta} \left(\mathcal{X}_i,\{\mathbf{M}_{i\rightarrow j}\}_{j=1}^N 
    \right), \mathcal{Y}_i  \right),~
    ~{\rm s.t.~} \sum_{i=1}^{N}\sum_{j=1,j\neq i}^{N}|\mathbf{M}_{i\rightarrow j}| \leq b_1, \mathbf{M}_{i\rightarrow j}\in\{0,1\}^{H\times W},
\end{equation}
where $\mathcal{F}_i$ can attribute to the network $\Phi_{\theta}(\cdot)$ and input data $\mathcal{X}_i$ and $b_1 = B_1/D$ with $D$ the channel number of $\mathcal{F}_i$. Due to the binary constrains, it is hard to optimize~\eqref{eq:optimization_equivalent} directly. Instead, we decompose~\eqref{eq:optimization_equivalent} into two sub-optimization problems and optimize the binary selection matrix  $\mathbf{M}_{i\rightarrow j}$ and the network parameters $\theta$ once at a time: i) obtain a feasible binary selection matrix $\mathbf{M}_{i\rightarrow j}$ by optimizing a proxy constrained problem; ii) given the feasible binary selection matrix $\mathbf{M}_{i\rightarrow j}$, optimize the perception network parameter $\theta$. The constraint is satisfied in i) and the perception goal is achieved in ii). Specifically, two sub-optimization problems are

$\bullet $ \textbf{Obtain  a feasible binary selection matrix $\mathbf{M}_{i\rightarrow j}$.} This essentially optimizes where to allocate the communication bandwidth. Intuitively, the spatial confidence reflects the perceptually critical level, so that those spatial regions with higher spatial confidence will provide more critical information to help the partners and should have a higher priority be selected.

Following this spirit, we consider a proxy constrained problem as follows,
\begin{equation*}
    \underset{\mathbf{M}}{\max}~\sum_{i=1}^{N}\sum_{j=1,j\neq i}^{N}  
    \mathbf{M}_{i\rightarrow j} \odot \mathbf{C}_i,~
    ~{\rm s.t.~}  \sum_{i=1}^{N}\sum_{j=1,j\neq i}^{N}|\mathbf{M}_{i\rightarrow j}| \leq b_1, \mathbf{M}_{i\rightarrow j}\in\{0,1\}^{H\times W},
\end{equation*}
where $\mathbf{C}_i$ is the spatial confidence map. Note that i) even this optimization problem has hard constraints and non-differentialability of binary variables, it has an analytical solution that naturally satisfies all the constraints in~\eqref{eq:optimization_equivalent}; and ii) even we cannot solve the original objective, this proxy objective still carries the similar idea to promote better, yet more compact perception. This solution is obtained by selecting those spatial regions whose corresponding elements in $\mathbf{M}$ rank top-$b_1$. The detailed steps of \textbf{selection function} are: i) arrange the elements in the input matrix in descending order; ii) given the communication budget constrain, decide the total number ($b_1$) of communication regions; iii) set the spatial regions of $\mathbf{M}$, where elements rank in top-$b_1$ as the $1$ and $0$ verses.


$\bullet $ \textbf{Given the feasible binary selection matrix, optimize the network parameter $\theta$.}  This essentially optimizes the perception performance. The sub-problem is
\begin{equation*}
    \underset{\theta}{\max}~\sum_{i=1}^{N} 
    g \left(\Phi_{\theta} \left(\mathcal{X}_i,\{\mathbf{M}_{i\rightarrow j}\}_{j=1}^N 
    \right), \mathcal{Y}_i  \right).
\end{equation*}
This can be solved by standard~\textbf{supervised learning}. For example, the perception evaluation metric $g(\cdot)$ can be evaluated by the  detection loss calculated between detections and the ground-truth and the detection loss is optimized with an Adam optimizer. We thus get the optimized perception network parameter $\theta$. Note that this sub-problem does not involve any constraints and is thus easy to optimize.





\subsection{Detailed information about the module design}

\textbf{Observation encoder.} Here we elaborate on the warping functions for the monocular camera, where the depth is unknown and estimated. Instead of directly projecting 2D features to flat ground space, we first lift them to 3D voxel space and then collapse them to the BEV. This design considers all the possible depths/altitudes, introducing flexibility in the projection, and mitigating the distortion effect caused by information loss in imaging. The detailed steps are: {\bf 1)} Categorical Depth Distribution Network (CaDDN~\cite{CaDDN}), which is a recent and effective method to warp image feature to BEV feature, is applied to estimate the depth distribution for each image feature point. {\bf 2)} Each feature point is wrapped from the 2D image space to the 3D physic space according to the known camera parameters. {\bf 3)} The 3D voxel features are flattened to BEV features. Briefly, the warping function is unfolded as follows: for each image feature point locates at $(u,v)$, given the estimated categorical depth $d_i$, and the known camera projection matrix $\mathbf{P}\in\mathbb{R}^{3\times4}$, 3D physical space coordinates $[x,y,z]^T$ is calculated conditioned on the image feature coordinates $[u,v,d_i]^T$ based on the projection function:  $[u,v,d_i]^T=\mathbf{P}\cdot[x,y,z,1]^T$.


\begin{figure}
    \centering
\includegraphics[width=0.8\linewidth]{figures/System/packing.png}
    \caption{Spatial confidence-aware message packing module. $\odot$ denotes point-wise multiplication, $\ominus$ denotes point-wise minus by a matrix with the same shape as the input and filled with $1$. Best viewed in color. Grey denotes the location being filled with zeros for the binary selection matrix $\mathbf{M}_{i\rightarrow j}^{(k)}$ and the feature map $\mathcal{Z}_{i\rightarrow j}^{(k)}$.}
    \label{fig:packing}
\end{figure}


\begin{figure}
    \centering
\includegraphics[width=1.0\linewidth]{figures/Communication_graph.pdf}
    \caption{Spatial confidence-aware communication graph construction module. We spatially decouple the full feature map, and could flexibly involve the informative spatial areas in the communication. This \textit{Spatial-decouple partially connected} communication could further flexibly prune irrelevant connections per-location and is more bandwidth-efficient.}
    \label{fig:communication_graph}
\end{figure}


\textbf{Spatial confidence-aware message packing.} Fig.~\ref{fig:packing} presents the detail about the spatial confidence-aware message packing module. For the message from agent $i$ to agent $j$ at $k$th communication round, the module takes the spatial confidence map $\mathbf{C}_{i}^{(k)}$ of agent $i$ and the request map $\mathbf{R}_{j}^{(k-1)}$ of agent $j$ as input, and outputs the message $\mathcal{P}_{i\rightarrow j}^{(k)}$ including the masked feature map $\mathcal{Z}_{i\rightarrow j}^{(k)}$ and the request map of agent $i$.




\textbf{Spatial confidence-aware communication graph construction.} Fig.~\ref{fig:communication_graph} presents the comparisons on the communication graph with previous works. \textit{Fully connected} versus \textit{agent-level partially connected} versus ours \textit{spatial-decouple partially connected} communication. \textit{Fully connected} communication results in a large amount of bandwidth usage, growing on the order of $O(N^2)$, where N is the number of agents in a network. \textit{Agent-level partially connected} communication prune irrelevant connections between agents while may erroneously sever the information connection. \textit{Spatial-decouple partially connected} communication could further flexibly prune irrelevant connections per-location and can substantially reduce the overall network complexity.



\begin{figure}
    \centering
\includegraphics[width=0.6\linewidth]{figures/System/fusion.png}
    \caption{Spatial confidence-aware message fusion module. Each agent attentively augments the features with the received messages at each location. And the per-location multi-head attention are separately operated at each location in parallel, it takes the features and the corresponding confidence scores as input, and outputs the augmented features.}
    \label{fig:fusion}
\end{figure}

\textbf{Spatial confidence-aware message fusion.} Fig.~\ref{fig:fusion} presents the detail about the spatial confidence-aware message fusion module. Given the received messages $\{\mathcal{P}_{j\rightarrow i}^{(k)}, j\in \mathcal{N}_i\}$, each agent $i$ attentively augments the features with the received messages at each location. And the request map $\mathbf{R}_{j}^{(k)}$ in the received message is firstly decoded to the confidence map $\mathbf{C}_{j}^{(k)}$ via a point-wise minus. Then the per-location multi-head attention are separately operated at each location in parallel, it takes the features and the corresponding confidence scores as input, and outputs the augmented features. 

\textbf{Sensor positional encoding.} Sensor positional encoding is conditioned on the physical distance between the known sensor coordinates and each BEV gird's coordinate in the 3D physic space. It is introduced to provide spatial prior, as the smaller the sensing distance is, the clear the observation would be. Mathematically, similar to the position encoding in~\cite{vaswani2017attention}, our sensor positional encoding is given by 
$
SPE_{(dis, 2p)} = sin(dis/10000^{2p/D}),
SPE_{(dis, 2p+1)} = cos(dis/10000^{2p/D})
$
where $dis$ is the physical distance, $p$ is the dimension, $D$ is the total channel dimension of the BEV feature map, $sin$ and $cos$ denote the sine and cosine functions.

\subsection{Detailed information about experimental settings}
\textbf{Implementation details.} For camera-only 3D object detection task on OPV2V, we implement the detector following CADDN~\cite{CaDDN}. The model is trained $100$ epoch with initial learning rate of $1$e-$3$, and decay by 0.1 at epoch 80. For LiDAR-based 3D object detection task, our detector follows MotionNet~\cite{Wu2020MotionNetJP}. We train $120$ epoch with learning rate $1$e-$3$. For the camera-only 3D object detection task on CoPerception-UAVs, our detector follows the CenterNet~\cite{zhou2019objects} with DLA-34~\cite{yu2018dla} backbone. The model is trained $140$ epoch with learning rate $5$e-$4$. 



\begin{table}[!t]
\setlength\tabcolsep{2pt}
\caption{Overall performance on CoPerception-UAVs, OPV2V, V2X-Sim and DAIR-V2X. Comm denotes the communication volume calculated with Equation~\eqref{comm}.}
\begin{small}
\centering
\begin{tabular}{l|cc|cc|cc|cc}
\hline
Dataset                              & \multicolumn{2}{c|}{CoPerception-UAVs}          & \multicolumn{2}{c|}{OPV2V}                      & \multicolumn{2}{c|}{V2X-Sim1.0}          & \multicolumn{2}{c}{DAIR-V2X} \\ \hline
Method/Metric                        & Comm  & AP@0.50/0.70 & Comm  & AP@0.50/0.70 & Comm  & AP@0.50 & Comm   & AP@0.50/0.70  \\ \hline
No Collaboration                              & 0.00     & 57.67/29.52   & 0.00     & 22.65/9.09    & 0.00     &45.80   & 0.00      & 50.03/43.57    \\
Late Fusion                          & 15.77 & 53.12/37.88   & 11.87 & 8.24/3.84    & 8.83 &46.70   & 11.45  & 53.12/37.88    \\
When2com                             & 28.37 & 61.63/33.55   & 22.28 & 19.69/8.29    & 20.00 &46.70  & 22.62  & 51.12/36.17    \\
V2VNet                               & 29.95 & 59.82/33.14   & 23.87 & 37.47/14.67   & 21.58 &55.30   & 24.21  & 56.01/42.25    \\
V2X-ViT                              & 28.37 & 59.12/41.57   & 22.28 & 39.82/16.43   & 20.00 &57.30   & 22.62  & 54.26/43.35    \\
DiscoNet                             & 28.37 & 59.74/29.71   & 22.28 & 36.00/12.50   & 20.00 &58.00   & 22.62  & 54.29/44.88    
\\\hline
\multirow{9}{*}{\textbf{Where2comm}} &11.76 & 60.19/34.94    &5.67 & 40.11/15.36     &6.70 & 47.60   &11.40 & 50.98/39.11    \\
&14.27 & 60.23/34.93    &15.49 & 42.15/16.09    &8.29 & 49.10   &15.58 & 51.01/39.10    \\
&15.73 & 61.30/35.29    &16.13 & 43.37/16.84    &9.52 & 50.60   &17.03 & 53.53/40.70    \\
&17.96 & 63.04/36.10    &17.04 & 44.07/17.15    &10.41 & 51.80  &17.53 & 55.84/42.44    \\
&19.04 & 63.94/37.16    &17.86 & 44.68/17.77    &11.10 & 54.20  &18.19 & 58.46/44.46    \\
&21.62 & 65.10/38.98    &18.43 & 45.23/18.02    &12.27 & 56.60  &20.56 & 63.54/48.78    \\
&23.33 & 65.32/39.25    &18.92 & 46.04/18.23    &12.80 & 57.00  &21.78 & 63.76/48.94    \\
&25.31 & 65.46/39.27    &18.92 & 46.04/18.23    &13.98 & 58.90  &22.35 & 63.71/48.89    \\
&28.48 & 65.71/39.38    &22.71 & 47.14/19.07    &20.00 &59.10    &22.62 & 63.71/48.93    \\\hline 
\end{tabular}
\end{small}
\label{tab:performance}
\vspace{-5mm}
\end{table}



\begin{minipage}{0.41\textwidth}
     \includegraphics[width=0.95\linewidth]{figures/V2X-Sim2/SOTA_Performance_AP0.50_vs_Commcost.png}
     \vspace{-1mm}
  \captionof{figure}{Where2comm achieves consistently superior performance-bandwidth trade-off on V2X-Sim2.0~\cite{V2XSim}.}
  \label{fig:v2xsim2}
\end{minipage}
\hspace{1mm}
\begin{minipage}{0.58\textwidth}
\centering
\setlength\tabcolsep{2pt}
\captionof{table}{Overall performance on V2X-Sim2.0~\cite{V2XSim}. Comm denotes the communication volume calculated with Equation(~\ref{comm}). Metric AP@(0.50/0.70) is used.}
\begin{scriptsize}
\centering
\begin{tabular}{l|cc|l|cc}
\hline
Method/Metric                        & Comm  & AP@0.50/0.70  &Method  & Comm  & AP@0.50/0.70\\ \hline
No Collaboration                     & 0.00     & 65.93/51.79 &\multirow{7}{10pt}{\textbf{Where\\2comm}} &13.84 & 75.72/65.13 \\
Late Fusion                          & 14.84 & 72.33/62.12  &&17.47 & 79.14/67.05 \\
When2com                             & 26.04 & 62.15/49.42  &&19.84 & 81.69/70.79 \\
V2VNet                               & 27.62 & 80.80/71.22  &&21.98 & 81.94/72.10 \\
V2X-ViT                              & 26.04 & 78.73/63.17  &&24.13 & 82.99/73.05 \\
DiscoNet                             & 26.04 & 69.73/55.12  &&25.93 & 83.77/74.09\\
 \hline
\end{tabular}
\end{scriptsize}
\label{tab:v2xsim2}
\end{minipage}


\textbf{Inference strategy in multi-round setting.} For the single-round communication, all the communication budget are used in this broadcast communication round. For the two-round communication, a small bandwidth (about 20\%) is allocated to activate the collaboration; for the next round, the remained relatively large (about 80\%) bandwidth is allocated to transmit the targeted information to meet agents' request. For more than two rounds communication  setting, we strategically allocate communication budget across multiple communication rounds. For the initial broadcast round, a small bandwidth (about 20\%) is allocated to activate the collaboration; for the next round, a relatively large (about 60\%) bandwidth is allocated to transmit the targeted information to meet agents' request; then, the bandwidth is gradually reduced, accounting for the communication degradation with the increasing rounds. 


\textbf{Communication volume.} Our communication volume is the same as DiscoNet~\cite{disconet}, the only difference is that our log base is $2$, while it is $10$, so our number is about $3.32$ times theirs. The base $2$ is chosen to align with the metric bit/byte, this is, communication volume counts the message size by byte in log scale with base $2$. Mathematically for the selected sparse feature map
$
    \mathcal{Z}_{i\rightarrow j}^{(k)}=\mathbf{M}_{i\rightarrow j}^{(k)}\odot \mathcal{F}_i^{(k)} \in \mathbb{R}^{H\times W \times D}
$, the communication volume is
\begin{equation}
    \text{log}_2\left(|\mathbf{M}_{i\rightarrow j}^{(k)}| \times D \times 32 / 8\right)\label{comm},
\end{equation}
where $|\cdot|$ denotes the L0 norm counting the non-zero elements in the binary selection matrix, this is, the total spatial girds need to be transmitted, and for each feature point $D$ denotes the channel dimension, $32$ is multiplied as float32 data type is used to represent each number, $8$ is divided as the metric byte is used. 



\subsection{Benchmarks}
We conduct extensive experiments on all the available collaborative perception benchmarks. Tab.~\ref{tab:performance} presents the overall performance on the four datasets, CoPerception-UAVs, OPV2V~\cite{OPV2V}, V2X-Sim1.0~\cite{disconet} and DAIR-V2X~\cite{dair}. And we further benchmark the updated V2X-Sim2.0~\cite{V2XSim} in Fig.~\ref{fig:v2xsim2} and Tab.~\ref{tab:v2xsim2}. For this LiDAR-based 3D object detection task, our detector follows PointPillar~\cite{PointPillar}. We see that \texttt{where2comm} consistently achieves significant improvements over previous methods on all the benchmarks.

\begin{figure}[!t]

  \centering
    \begin{subfigure}{0.22\linewidth}
    \includegraphics[width=0.99\linewidth]{figures/OPV2V_WeightsPair/c1.png}
    \vspace{-5mm}
    \caption{$\mathbf{C}_1^{(0)}$}
    \label{fig:UAV_ConfMap1}
  \end{subfigure}
\begin{subfigure}{0.22\linewidth}
    \includegraphics[width=0.99\linewidth]{figures/OPV2V_WeightsPair/m1-2.png}
    \vspace{-5mm}
    \caption{$\mathbf{M}_{1\rightarrow 2}^{(0)}$}
    \label{fig:UAV_SelectMat1}
  \end{subfigure}
  \begin{subfigure}{0.22\linewidth}
    \includegraphics[width=0.99\linewidth]{figures/OPV2V_WeightsPair/w1-1.png}
    \vspace{-5mm}
    \caption{$\mathbf{W}_{1\rightarrow 1}^{(0)}$}
    \label{fig:UAV_AttenWeight1to1}
  \end{subfigure}
  \begin{subfigure}{0.22\linewidth}
    \includegraphics[width=0.99\linewidth]{figures/OPV2V_WeightsPair/o1-0.png}
    \vspace{-5mm}
    \caption{$\widehat{\mathcal{O}}_{1}^{(0)}$}
    \label{fig:UAV_Detections}
  \end{subfigure}
  \begin{subfigure}{0.07\linewidth}
    \includegraphics[width=0.99\linewidth]{figures/UAV_Weights/ColorBar.png}
  \end{subfigure}
  \centering
  \begin{subfigure}{0.22\linewidth}
    \includegraphics[width=0.99\linewidth]{figures/OPV2V_WeightsPair/r2.png}
    \vspace{-5mm}
    \caption{$\mathbf{R}_2^{(0)}$}
    \label{fig:UAV_RequestMap2}
  \end{subfigure}
\begin{subfigure}{0.22\linewidth}
    \includegraphics[width=0.99\linewidth]{figures/OPV2V_WeightsPair/z2-1.png}
    \vspace{-5mm}
    \caption{$\mathcal{Z}_{2\rightarrow 1}^{(0)}$}
    \label{fig:UAV_Message2}
  \end{subfigure}
  \begin{subfigure}{0.22\linewidth}
    \includegraphics[width=0.99\linewidth]{figures/OPV2V_WeightsPair/w2-1.png}
    \vspace{-5mm}
    \caption{$\mathbf{W}_{2\rightarrow 1}^{(0)}$}
    \label{fig:UAV_AttenWeight2to1}
  \end{subfigure}
  \begin{subfigure}{0.22\linewidth}
    \includegraphics[width=0.99\linewidth]{figures/OPV2V_WeightsPair/o1-1.png}
    \vspace{-5mm}
    \caption{$\widehat{\mathcal{O}}_{1}^{(1)}$}
    \label{fig:UAV_Detections}
  \end{subfigure}
  \begin{subfigure}{0.07\linewidth}
    \includegraphics[width=0.99\linewidth]{figures/UAV_Weights/ColorBar.png}
    \end{subfigure}
\vspace{-2mm}
  \caption{Visualization of collaboration between Vehicle 1 and Vehicle 2 on OPV2V dataset, including spatial confidence map ($\mathbf{C}_1^{(0)}$), selection matrix ($\mathbf{M}_{1\rightarrow2}^{(0)}$), message ($\{\mathbf{R}_2^{(0)},\mathcal{Z}_{2\rightarrow1}^{(0)}\}$) in the communication module, attention weight in the fusion module ($\mathbf{W}_{1\rightarrow1}^{(0)}$,$\mathbf{W}_{2\rightarrow1}^{(0)}$), and Vehicle 1's detection results before ($\widehat{\mathcal{O}}_{1}^{(0)}$) and after ($\widehat{\mathcal{O}}_{1}^{(1)}$) collaboration. \textcolor{green}{Green} and \textcolor{red}{red} boxes denote ground-truth and detection, respectively. The objects occluded can be detected through transmitting spatially sparse, yet perceptually critical message. }
  \label{fig:OPV2V_Weights}
  \vspace{-4mm}
\end{figure}

\begin{figure}[!t]
  \centering
\begin{subfigure}{0.18\linewidth}
    \includegraphics[width=0.99\linewidth]{figures/V2X-Sim_WeightsPair/1.png}
    \vspace{-5mm}
    \caption{$\mathcal{X}_1$ in BEV}
    \label{fig:UAV_BEVImage1}
  \end{subfigure}
\begin{subfigure}{0.18\linewidth}
    \includegraphics[width=0.99\linewidth]{figures/V2X-Sim_WeightsPair/2.png}
    \vspace{-5mm}
    \caption{$\mathbf{C}_1^{(0)}$}
    \label{fig:UAV_ConfMap1}
  \end{subfigure}
  \begin{subfigure}{0.18\linewidth}
    \includegraphics[width=0.99\linewidth]{figures/V2X-Sim_WeightsPair/3.png}
    \vspace{-5mm}
    \caption{$\mathbf{M}_{1\rightarrow 2}^{(0)}$}
    \label{fig:UAV_SelectMat1}
  \end{subfigure}
  \begin{subfigure}{0.18\linewidth}
    \includegraphics[width=0.99\linewidth]{figures/V2X-Sim_WeightsPair/4.png}
    \vspace{-5mm}
    \caption{$\mathbf{W}_{1\rightarrow 1}^{(0)}$}
    \label{fig:UAV_AttenWeight1to1}
  \end{subfigure}
\begin{subfigure}{0.18\linewidth}
    \includegraphics[width=0.99\linewidth]{figures/V2X-Sim_WeightsPair/5.png}
    \vspace{-5mm}
    \caption{$\widehat{\mathcal{O}}_{1}^{(0)}$}
    \label{fig:UAV_DetectionBeforeComm}
  \end{subfigure}
  \begin{subfigure}{0.06\linewidth}
    \includegraphics[width=0.99\linewidth]{figures/UAV_Weights/ColorBar.png}
\end{subfigure}
  \centering
  \begin{subfigure}{0.18\linewidth}
    \includegraphics[width=0.99\linewidth]{figures/V2X-Sim_WeightsPair/6.png}
    \vspace{-5mm}
    \caption{$\mathcal{X}_2$ in BEV}
    \label{fig:UAV_BEVImage2}
  \end{subfigure}
\begin{subfigure}{0.18\linewidth}
    \includegraphics[width=0.99\linewidth]{figures/V2X-Sim_WeightsPair/7.png}
    \vspace{-5mm}
    \caption{$\mathbf{R}_2^{(0)}$}
    \label{fig:UAV_RequestMap2}
  \end{subfigure}
  \begin{subfigure}{0.18\linewidth}
    \includegraphics[width=0.99\linewidth]{figures/V2X-Sim_WeightsPair/8.png}
    \vspace{-5mm}
    \caption{$\mathcal{Z}_{2\rightarrow 1}^{(0)}$}
    \label{fig:UAV_Message2}
  \end{subfigure}
  \begin{subfigure}{0.18\linewidth}
    \includegraphics[width=0.99\linewidth]{figures/V2X-Sim_WeightsPair/9.png}
    \vspace{-5mm}
    \caption{$\mathbf{W}_{2\rightarrow 1}^{(0)}$}
    \label{fig:UAV_AttenWeight2to1}
  \end{subfigure}
\begin{subfigure}{0.18\linewidth}
    \includegraphics[width=0.99\linewidth]{figures/V2X-Sim_WeightsPair/10.png}
    \vspace{-5mm}
    \caption{$\widehat{\mathcal{O}}_{1}^{(1)}$}
    \label{fig:UAV_DetectionAfterComm}
  \end{subfigure}
  \begin{subfigure}{0.06\linewidth}
    \includegraphics[width=0.99\linewidth]{figures/UAV_Weights/ColorBar.png}
\end{subfigure}
\vspace{-2mm}
  \caption{Visualization of collaboration between Vehicle 1 and Vehicle 2 on V2X-Sim dataset, including spatial confidence map ($\mathbf{C}_1^{(0)}$), selection matrix ($\mathbf{M}_{1\rightarrow2}^{(0)}$), message ($\{\mathbf{R}_2^{(0)},\mathcal{Z}_{2\rightarrow1}^{(0)}\}$) in the communication module, attention weight in the fusion module ($\mathbf{W}_{1\rightarrow1}^{(0)}$,$\mathbf{W}_{2\rightarrow1}^{(0)}$), and Drone 1's detection results before ($\widehat{\mathcal{O}}_{1}^{(0)}$) and after ($\widehat{\mathcal{O}}_{1}^{(1)}$) collaboration. \textcolor{green}{Green} and \textcolor{red}{red} boxes denote ground-truth and detection, respectively. The objects occluded by a tall building can be detected through transmitting spatially sparse, yet perceptually critical message. }
  \label{fig:V2X-Sim_Weights}
  \vspace{-4mm}
\end{figure}

\begin{figure}
\centering
  \begin{subfigure}{0.05\linewidth}
    \includegraphics[width=\linewidth]{figures/detection_compound/title.pdf}
  \end{subfigure}
  \hspace{-3mm}
  \begin{subfigure}{0.23\linewidth}
    \includegraphics[width=\linewidth]{figures/detection_compound/row1.pdf}
    \caption{No Collaboration}
    \label{fig:opv2v_cam_result_no_comm}
  \end{subfigure}
\begin{subfigure}{0.23\linewidth}
    \includegraphics[width=\linewidth]{figures/detection_compound/row2.pdf}
    \caption{When2com}
    \label{fig:opv2v_cam_result_disconet}
  \end{subfigure}
  \begin{subfigure}{0.23\linewidth}
    \includegraphics[width=\linewidth]{figures/detection_compound/row3.pdf}
    \caption{DiscoNet}
    \label{fig:opv2v_cam_result_v2vnet}
  \end{subfigure}
\begin{subfigure}{0.23\linewidth}    \includegraphics[width=\linewidth]{figures/detection_compound/row4.pdf}
    \caption{Where2comm}
    \label{fig:opv2v_cam_result_where2comm}
  \end{subfigure}
  
\vspace{-2mm}
  \caption{\texttt{Where2comm} qualitatively outperforms the state-of-the-art methods in CoPerception-UAVs, V2X-Sim and OPV2V datasets. \textcolor{green}{Green} and \textcolor{red}{red} boxes denote ground-truth and detection, respectively.}
  \label{fig:detections}
  \vspace{-5mm}
\end{figure}

\subsection{Visualization}
\textbf{Visualization of collaboration in OPV2V and V2X-Sim.} Fig.~\ref{fig:OPV2V_Weights} and Fig.~\ref{fig:V2X-Sim_Weights} illustrates how~\texttt{Where2comm} is empowered by the proposed spatial confidence map on OPV2V and V2X-Sim dataset. In the scene, with Vehicle 2's help, Vehicle 1 is able to detect the missed objects in the single view. Fig.~\ref{fig:OPV2V_Weights} (a-d) shows Vehicle 1's spatial confidence map, binary selection matrix, ego attention weight, and the detection results by its own observation. Fig.~\ref{fig:OPV2V_Weights} (e-f) shows Vehicle 2's message sent to Drone 1, including the request map (opposite of confidence map) and the sparse feature map, achieving efficient communication. Fig.~\ref{fig:OPV2V_Weights} (g) shows the attention weight for Vehicle 1 to fuse Vehicle 2's messages, which is sparse, yet highlights the objects' positions. Fig.~\ref{fig:OPV2V_Weights} (d) and (h) compares the detection results before and after the collaboration with Vehicle 2. We see that the proposed spatial confidence map contributes to spatially sparse, yet perceptually critical message, which effectively helps Vehicle 1 detect occluded objects.

\textbf{Visualization of detection results.} Fig.~\ref{fig:detections} shows that \texttt{Where2comm} qualitatively outperforms the state-of-the-art methods in CoPerception-UAVs, V2X-Sim and OPV2V datasets. 












\begin{figure}[!t]
    \centering
    \centering
  \begin{subfigure}{0.45\linewidth}
    \includegraphics[width=0.9\linewidth]{figures/OPV2V_Performance/2Round_AP0.50_vs_Commcost.png}
\caption{2-rounds communication.}
    \label{fig:UAV_Image}
  \end{subfigure}
\begin{subfigure}{0.45\linewidth}
    \includegraphics[width=0.9\linewidth]{figures/OPV2V_Performance/3Round_AP0.50_vs_Commcost.png}
\caption{3-rounds communication. }
    \label{fig:UAV_ConfMap}
  \end{subfigure}
    \caption{Bandwidth allocation ablation study in multi-round communication. (a-b) shows the perception performance and communication bandwidth trade-offs for 2- and 3-round communication using different bandwidth allocation strategies on the OPV2V dataset. The legend shows the bandwidth ratio from the initial communication round to the entire communication round. Allocating more bandwidth in the second and subsequent communication rounds achieves a better performance-bandwidth trade-off than allocating all bandwidth in the initial communication round.}
    \label{fig:bandwidth_allocation}
\end{figure}


\subsection{Ablation on bandwidth allocation}
Fig.~\ref{fig:bandwidth_allocation} shows the bandwidth allocation ablation study in multi-round communication setting. We see that allocating more bandwidth in the second and subsequent communication rounds achieves a better performance-bandwidth trade-off than allocating all bandwidth in the initial communication round, and the gain is stable for different bandwidth allocation strategies. The reason is that multi-round communication employs a request map in the second and subsequent communication rounds to denote the spatial area where each agent needs more information, which enables more targeted and efficient communication. 




\subsection{Discussion on the realistic limitations}
There are many challenges in a collaborative perception system. In this work, we focus on the biggest challenge in current collaborative perception systems; that is, the trade-off between communication bandwidth and perception performance. This challenge has been actively addressed in previous works~\cite{who2com,when2com,v2vnet,disconet}. Because collaborative perception is enabled and also severely limited by the communication capacity, which is critically reflected in the highly dynamic and limited bandwidth in real-world communication systems. \textit{Where2comm} flexibly adapts to various communication bandwidths, achieving superior performance-bandwidth trade-off.


Here we further discuss other realistic limitations, assess the robustness of our system and future improvements to be done. \\
$\bullet$ For other realistic communication issues such as \textbf{latency}, \textit{where2comm} communicates strategically when necessary, rather than all the time or everywhere, to reduce the possibility of encountering communication problems. In addition, a prediction module could be integrated to estimate the missed or delayed frames according to the historically received frames. And by focusing on the informative spatial regions, \textit{where2comm} can reduce the estimation difficulty. \\
$\bullet$ For the \textbf{time synchronisation} issue, by using the powerful transformer architecture-based fusion module, \textit{where2comm} can attentively augment the features with the received asynchronous features from other agents. In addition, \textit{where2comm} can introduce positional encoding conditioned on delay time and easily extend to global multi-head attention to further reduce the effects of time synchronization.\\
$\bullet$ For the \textbf{noisy localization} issue, \textit{where2comm} exchanges the intermediate features among agents, which has a relatively low spatial resolution, thus is relatively robust to noisy pose. In addition, \textit{where2comm} can easily extend to a deformable transformer architecture like~\cite{DeformableDETR} to further alleviate the feature distortion caused by the noisy localization.\\
$\bullet$ For the \textbf{attack} issue, by focusing on specific spatial regions and attentively fusing the received features from other agents, \textit{where2comm} is relatively less likely to be attacked. \\
$\bullet$ For the \textbf{data availability}, \textit{where2comm} works on both RGB and point cloud modalities, and is sensor friendly, so it can be deployed on cheap camera sensors and lidar sensors.

\begin{figure}[t]
  \centering
\includegraphics[width=1\linewidth]{figures/Dataset/overview.png}
\vspace{-3mm}
   \caption{As an important component of the UAV swarm, collaborative perception  could fundamentally resolve various reception-field restrictions in the traditional single-agent perception.}
   \label{fig:overview}
    \vspace{-5mm}
\end{figure}


\subsection{CoPerception-UAVs dataset details}
CoPerception-UAVs dataset collects data from drones, see Fig.~\ref{fig:overview}. As the rapid development of an unmanned aerial vehicle (UAV) significantly enhances human’s ability to perceive the world from an aerial perspective. UAV-based systems have been widely used in numerous applications, including search and rescue, security and surveillance, photography, geographical mapping, as well as traffic monitoring. Through collaboration, UAV swarm can further distribute multiple tasks and achieve higher flexibility, stronger robustness, and a larger perception range, leading to significant advantages in harsh and complex environments. Unfortunately, collaborative perception mainly focuses on the vehicles and ignores the UAV literature. To \textbf{provide more diverse views and challenging benchmark for the collaborative perception community}, here we present the first comprehensive large-scale collaborative perception dataset for UAV swarm so far.



\begin{figure}[t]
  \centering
  \vspace{2mm}
\includegraphics[width=1\linewidth]{figures/Dataset/formation.png}
\vspace{-5mm}
   \caption{Two types of UAV swarm formation. The left shows the discipline formation mode, where the swarm keeps a static array and the right shows the dynamic formation mode, where each UAV navigates independently in the scene.}
   \label{fig:arrangement}
\vspace{-6mm}
\end{figure}

Since building a dataset in the real world is too expensive and laborious, in this initial version,  we consider a virtual dataset based on the co-simulation of AirSim~\cite{Airsim} and Carla~\cite{carla}, where AirSim simulates the UAV swarms and Carla simulates the complex background scenes and dynamic foreground objects. In the simulation, we consider that the UAV swarm is flying over diverse simulated scenes at various altitudes. Each UAV has a sensing device to collect RGB images, a computation device to perceive the environment with a perception model, and a communication device to transmit perception information among UAVs. In this setting, the UAV swarm is able to achieve 2D/3D object detection, pixel-wise or bird's-eye-view (BEV) semantic segmentation in a collaborative manner. Our dataset consists of 131.9k synchronous images collected from 5 coordinated UAVs flying at 3 altitudes over 3 simulated towns with 2 types of swarm formation. To enable the model training and testing, each image is fully annotated with the pixel-wise semantic segmentation labels, 2D bounding boxes of vehicles, as well as 3D bounding boxes on the ground and the semantic mask from BEV view. This benchmark can enable the evaluation of collaborative perception methods on the important perception tasks: 2D/3D/BEV object detection and semantic segmentation. The dataset details as unfolded as follows.

\textbf{Data collection.} Our proposed dataset is collected by the co-simulation of CARLA~\cite{carla} and AirSim\cite{Airsim} (both under MIT license). We use CARLA to generate complex simulation scenes and traffic flow; and use AirSim to simulate UAV swarm flying in the scene. The flight route of UAVs is controlled by AirSim and sample data are collected randomly at about 4-second intervals.

\textbf{Map creation.} The simulation scenes, including the road layout, static objects, and traffic flow, are created based on CARLA~\cite{carla} simulation. We take three open-source maps (\emph{town4} to \emph{town6}) provided by CARLA as the basic road layouts, which are the three largest maps in scale. To increase the complexity and diversity of the scenes and make the perception tasks more challenging, we customize the original maps, adding and replacing various buildings, vegetation, roadblocks, barriers, and other static objects with various assets provided by CARLA.

\textbf{Traffic flow creation.} Moving vehicles in the scene are managed through CARLA. Hundreds of vehicles are spawned in each scene by script \emph{spawn\_npc.py} provided by CARLA. The initial location and motion trajectory of each vehicle is determined by the map's road layout.

\textbf{Sensor setup.}
Each UAV is equipped with 5 RGB cameras in 5 directions and 5 semantic cameras collecting semantic ground truth for RGB cameras. The cameras include a bird's eye view camera and four cameras facing forward, backward, right, and left with a pitch degree of $-45^\circ$. Each camera has an FoV of $90^\circ$ and the resolution is $800 \times 450$. On each UAV, all the cameras are fixed and their internal relative position and rotation degree are invariable. The translation (x, y, z) and rotation (w, x, y, z in quaternion) of each camera in both global and ego coordinates are recorded during data collection. With such a sensor setting, a UAV at the height of $40m$ can mostly cover an area of $200m \times 200m$.



\textbf{Formation flying.}
The UAV swarm moves and executes tasks in the three-dimensional space, where the situation could be much more complex than those of vehicles or roadside units. In our proposed dataset, we take into consideration two main factors that may affect the perception and collaboration patterns of UAV swarms: flight formation and altitude. Each UAV swarm consists of 5 UAVs. We arrange two types of formation modes for a UAV swarm: discipline mode, where all 5 UAVs keeps a consistent and relatively static array, and dynamic mode, where each UAV navigates independently in the scene; see Fig. \ref{fig:arrangement}. The former simulates the situation where the swarm of UAVs is executing a same specific task such as exploring an unknown area, search and rescue; while the latter simulates the monitoring and patrolling tasks in the city. 


\begin{figure*}[]
    \centering
    \vspace{2mm}
\begin{subfigure}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/Dataset/sample/CAM_FRONT_id_0.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/Dataset/sample/CAM_FRONT_id_1.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/Dataset/sample/CAM_FRONT_id_2.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/Dataset/sample/CAM_FRONT_id_3.png}
    \end{subfigure}
    
    \centering
\begin{subfigure}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/Dataset/sample/CAM_FRONT_id_0_4.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/Dataset/sample/CAM_FRONT_id_1_4.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/Dataset/sample/CAM_FRONT_id_2_4.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/Dataset/sample/CAM_FRONT_id_3_4.png}
    \end{subfigure}
    
    
    \centering
\begin{subfigure}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/Dataset/sample/CAM_FRONT_id_0_seg.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/Dataset/sample/CAM_FRONT_id_1_seg.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/Dataset/sample/CAM_FRONT_id_2_seg.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/Dataset/sample/CAM_FRONT_id_3_seg.png}
    \end{subfigure}
    
    \centering
\begin{subfigure}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/Dataset/sample/CAM_FRONT_id_0_bev.jpg}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/Dataset/sample/CAM_FRONT_id_1_bev.jpg}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/Dataset/sample/CAM_FRONT_id_2_bev.jpg}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/Dataset/sample/CAM_FRONT_id_3_bev.jpg}
    \end{subfigure}
\caption{Data and annotations of one sample. From top to bottom: RGB image, image with 3D bounding boxes, image with  semantic labels, and BEV map with semantic labels. From left to right are from different cameras equipped on four UAVs.}
\label{fig:sample example}
\vspace{-4mm}
\end{figure*}

Fully-annotated data are provided in our proposed dataset, including synchronous images with pixel-wise semantic labels, 2D \& 3D bounding boxes of vehicles, and BEV semantic map; see ~\ref{fig:sample example}.

\textbf{Camera data.} We collect synchronous images from all cameras on 5 UAVs, which is 25 images in a sample. Camera intrinsics and extrinsics in global coordinate are provided to support coordinate transformation across various UAVs. In total, 123.8K images are collected for the discipline swarm mode and 8.1K for the dynamic swarm mode.

\textbf{Bounding boxes.} During data collection, 3D bounding boxes of vehicles are recorded at the same moment with images, including location (x, y, z), rotation (w, x, y, z in quaternion) in the global coordinate and their length, width and height. The location (x, y, z) is the center of the bounding box. Then we provide 2D bounding boxes by projecting the 3D bounding boxes to the image perspective plane of each camera, resulting in 1.94M 3D bounding boxes and 3.6M 2D bounding boxes in total.

\textbf{Data usage.} In total, CoPerception-UAVs has 131.9K aerial images and 1.94M 3D boxes. We randomly split the samples into train/validation/test, resulting 91,175/19,500/20,250 images, and 1,316,536/303,888/319,576 3D bounding boxes. The dataset is organized in a similar way with the widely-used autonomous driving dataset, nuScenes~\cite{nuscenes2019}; so it can be used directly with the well-established nuScenes-devkit.














%
 





\end{document}