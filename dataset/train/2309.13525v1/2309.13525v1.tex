\section{Experiments}
\label{sec:experiments} \textbf{Real-to-artistic generalization.} We conduct our experiments based on 4 datasets: Pascal-VOC (VOC) \cite{pascal-voc-2012,pascal-voc-2007}, Clipart1k \cite{inoue2018cross}, Watercolor2k \cite{inoue2018cross}, Comic2k \cite{inoue2018cross} to assess generalizability on artistic and style domain shifts. VOC contains 20 categories of common objects from 16,551 natural images, Clipart consists of 20 categories of 3,165 objects from 1000 instances, and Watercolor and Comic contain 6 categories of 3,315 and 6,389 objects from 2,000 images, respectively. Experiments are conducted in three settings: (1) VOC and Clipart, (2) VOC and Watercolor, and (3) VOC and Comic are used as the source domains, in which only VOC is considered a labeled domain. In each setting, the remaining datasets are considered unseen targets to assess the model's generalizability across multiple domains (Table~\ref{tab:DG-voc}). We extend our model to DA and show the results in Table~\ref{tab:DA-voc}.

\textbf{Adverse weather adaptation.} We experiment on Cityscapes \cite{Cordts2016Cityscapes} (City), Foggy-Cityscapes \cite{Cordts2016Cityscapes} (Foggy), and BDD100k \cite{bdd100k} (Bdd) to evaluate generalizability across different time and weather conditions. City features 3,025 urban images from 50 cities, Foggy simulates fog on the City, and Bdd comprises 10,000 images of various weather conditions and times. All datasets contain 8 categories, but we ignore the ``train'' category on Bdd due to its low instance count in the validation set \cite{lin2021domain}. As Foggy is a simulated based on City, we use them as the source domains without the need for style transfer (City labeled and Foggy unlabeled), while Bdd is an unseen domain in the domain generalization task summarized in Table~\ref{tab:DG-city}. Note we do not use the labels on the Foggy data throughout training. Following DIDN \cite{lin2021domain}, since most previous works are on weather adaptation, we perform adaptation from City to Foggy and extensively compare against baselines and related works as shown in Table~\ref{tab:uda-c2fc}.

\textbf{Baselines.}
\label{sec:baselines}
Faster-RCNN \cite{ren2015faster} (F-RCNN) with ImageNet \cite{deng2009imagenet} and RegionCLIP (R-CLIP) are labeled source-only baselines, trained on VOC for real-to-artistic generalization/adaptation (Table~\ref{tab:DG-voc} / Table \ref{tab:DA-voc}) and City for adverse weather generalization/adaptation (Table~\ref{tab:DG-city} / Table~\ref{tab:uda-c2fc}). For a fair comparison, we only compare against models using ResNet50 backbone as RegionCLIP \cite{zhong2022regionclip} does not have ResNet101 pre-trained weights available. For real-to-artistic generalization, we further train Adaptive-MT
\cite{li2022cross}, a state-of-the-art (sota) DA with ResNet50 backbone. We define \textit{Direct Visual Alignment} (DVA) and \textit{Caption Pseudo Labeling} (Caption-PL) as additional baselines in the ablation (Table~\ref{tab:DG-voc} and Table~\ref{tab:DA-voc} for DG and DA in real-to-artistic, respectively). DVA applies the descriptive consistency loss in the visual space without using . Caption-PL uses pseudo labels obtained by applying ClipCap on the original image to compute image-captioning loss over tokens for the stylized image. IRG \cite{vs2022instance} is only reported for the (VOC and Clipart) setting 
as performance/pre-trained weights were reported only in this setting. 


\textbf{Implementation details.} PyTorch \cite{NEURIPS2019_9015} and Detectron2 \cite{wu2019detectron2} are used for development. We adopt RegionCLIP's ResNet50 \cite{he2016deep} as the object detector backbone. Following ClipCAP\footnote{\url{https://github.com/rmokady/CLIP_prefix_caption}},  comprises 8 multi-head self-attention layers, each with 8 heads. ClipCAP is pre-trained on COCO-captions \cite{chen2015microsoft,lin2014microsoft} using frozen RegionCLIP as the image encoder and GPT-2 as the language decoder, with only the mapper () being updated. Projection layer  (after  in Fig.~\ref{fig:model}) has output dimension of 256. For all experiments, we use SGD with 0.002 initial lr, a linear scheduler, and a batch size of 8, except 12 for the RegionCLIP baseline. Experiments use 4 NVIDIA Quadro RTX 5000 with 10k burn-up and 20k joint-training iterations. The burn-up stage follows RegionCLIP \cite{zhong2022regionclip}, and SSDG experiments continue from this checkpoint.  in  is 1. We evaluate mAP with a 0.5 threshold. For realistic evaluation, following \cite{cct,oliver2018realistic}, we avoided intensive hyperparameter search and manually chose reasonable parameters that resulted in stable training. However, a comprehensive search is expected to improve the performance.
\subsection{Results}

\begin{table}[!tp]
\scriptsize
\centering
  \captionsetup{skip=0pt, position=above} \caption{\textbf{Real-to-artistic generalizations}. Numbers in parentheses show the difference from R-CLIP. Max  shows maximum improvement over two target domains compared to F-RCNN. We report mAP (). / denote DA methods/labeled source-only methods.
}
  \label{tab:DG-voc}
  \begin{tabular}{@{\hspace{3pt}}llllllll@{\hspace{1pt}}} 
\hline
\multirow{2}{*}{{Method}}&
    \multicolumn{2}{l}{{VOC\&Clip \textrightarrow Water,Com}}  & 
    \multicolumn{2}{l}{{VOC\&Water\textrightarrow Clip,Com}} &
    \multicolumn{2}{l}{{VOC\&Com\textrightarrow Clip,Water}}&
    \multirow{2}{*}{Max  }\\  
\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule{6-7}
& Watercolor&Comic&Clipart&Comic&Clipart&Watercolor&\\
\hline
\multicolumn{1}{l|}{F-RCNN \cite{ren2015faster}} &41.2&17.9&24.1&17.9&24.1&41.2&\multicolumn{1}{|l}{-}\\
\multicolumn{1}{l|}{R-CLIP \cite{zhong2022regionclip}}  &44.7&34.2&33.9&34.2&33.9&44.7&\multicolumn{1}{|@{\hspace{3pt}}l@{\hspace{3pt}}}{16.3/16.3/9.8}\\
\hline
\multicolumn{1}{l|}{Adaptive MT \cite{li2022cross}} &40.6 \tiny{\textcolor{red}{(-4.1)}}&22.2 \tiny{\textcolor{red}{(-12.0)}}&29.0 \tiny{\textcolor{red}{(-4.9)}}&24.3 \tiny{\textcolor{red}{(-9.9)}}&25.7 \tiny{\textcolor{red}{(-8.2)}}&42.3 \tiny{\textcolor{red}{(-2.4)}}&\multicolumn{1}{|@{\hspace{3pt}}l@{\hspace{0pt}}}{4.3/6.4/1.6}\\
\multicolumn{1}{l|}{IRG \cite{vs2022instance}} &48.1 \tiny{\textcolor{blue}{(+3.4)}}&25.9 \tiny{\textcolor{red}{(-8.3)}}&-&-&-&-&\multicolumn{1}{|@{\hspace{3pt}}l@{\hspace{3pt}}}{8.0/-/-}\\
\hline
\multicolumn{1}{l|}{DVA} &45.6 \tiny{\textcolor{blue}{(+0.9)}}&38.1 \tiny{\textcolor{blue}{(+3.9)}}&
32.6 \tiny{\textcolor{red}{(-1.3)}} &34.2 \tiny{\textcolor{blue}{(+0.0)}}&35.9 \tiny{\textcolor{blue}{(+2.0)}}&45.9 \tiny{\textcolor{blue}{(+1.2)}}&\multicolumn{1}{|@{\hspace{3pt}}l@{\hspace{0pt}}}{20.2/16.3/11.8}\\
\multicolumn{1}{l|}{Caption-PL} &45.0 \tiny{\textcolor{blue}{(+0.3)}}&36.4 \tiny{\textcolor{blue}{(+2.2)}}&30.1 \tiny{\textcolor{red}{(-3.8)}}&30.3 \tiny{\textcolor{red}{(-3.9)}}&34.7 \tiny{\textcolor{blue}{(+0.8)}}&42.1 \tiny{\textcolor{red}{(-2.6)}}&\multicolumn{1}{|@{\hspace{3pt}}l@{\hspace{0pt}}}{18.5/12.4/10.6}\\

\multicolumn{1}{l|}{\textbf{Ours}}  &\textbf{49.8} \tiny{\textcolor{blue}{(+5.1)}}&\textbf{45.9} \tiny{\textcolor{blue}{(+11.7)}}&\textbf{38.7} \tiny{\textcolor{blue}{(+4.8)}}&\textbf{43.5} \tiny{\textcolor{blue}{(+9.3)}}&\textbf{39.8} \tiny{\textcolor{blue}{(+5.9)}}&\textbf{49.4} \tiny{\textcolor{blue}{(+4.7)}}&\multicolumn{1}{|@{\hspace{3pt}}l@{\hspace{3pt}}}{\textbf{28.0}/\textbf{25.6}/\textbf{15.7}}\\
\hline
\end{tabular}
\end{table}




\begin{table}[!tp]\centering

\scriptsize
\captionsetup{skip=0pt, position=above} 

\caption{\textbf{Adverse weather generalization.} \emph{City, Foggy \textrightarrow \, Bdd}  in }
\label{tab:DG-city}
\begin{tabular}{l@{\hspace{3pt}}|lllllll@{\hspace{2pt}}|@{\hspace{3pt}}l@{\hspace{3pt}}}
\hline
{Method}  &{prsn} &{rider} &{car} &{truck} &{bus} &{motor} 
&{bike} & {mAP} \\
\hline
F-RCNN \cite{ren2015faster} & 27.9& 27.5& 43.1& 16.6& 15.1&5.6 &21.0 &19.6\\
\multirow{8}{*}{}R-CLIP \cite{zhong2022regionclip}& 40.6 \tiny{\textcolor{blue}{(+12.7)}}& 31.3 \tiny{\textcolor{blue}{(+3.8)}}& 47.9 \tiny{\textcolor{blue}{(+4.8)}}& 16.8 \tiny{\textcolor{blue}{(+0.2)}}& 12.0 \tiny{\textcolor{red}{(-3.1)}} &11.2 \tiny{\textcolor{blue}{(+5.6)}} &23.2 \tiny{\textcolor{blue}{(+2.2)}} &26.1 \tiny{\textcolor{blue}{(+6.5)}} \\
\hline
 DIDN \cite{lin2021domain}  \tiny(ICCV'21)  &34.5 \tiny{\textcolor{blue}{(+6.6)}}&30.4 \tiny{\textcolor{blue}{(+2.9)}}&44.2 \tiny{\textcolor{blue}{(+1.1)}} &\textbf{21.2} \tiny{\textcolor{blue}{(+4.6)}}&\textbf{19.0} \tiny{\textcolor{blue}{(+3.9)}}&9.2 \tiny{\textcolor{blue}{(+3.6)}}&22.8 \tiny{\textcolor{blue}{(+1.8)}} & 22.7 \tiny{\textcolor{blue}{(+3.1)}}\\

\textbf{Ours}  & \textbf{41.4} \tiny{\textcolor{blue}{(+13.5)}}& \textbf{31.7} \tiny{\textcolor{blue}{(+4.2)}}& \textbf{49.8} \tiny{\textcolor{blue}{(+6.7)}}& 18.1 \tiny{\textcolor{blue}{(+1.5)}}& 11.4 \tiny{\textcolor{red}{(-3.7)}}& \textbf{12.4} \tiny{\textcolor{blue}{(+6.8)}}& \textbf{25.6} \tiny{\textcolor{blue}{(+4.6)}}&\textbf{27.1} \tiny{\textcolor{blue}{(+7.5)}}\\
\hline
\end{tabular}
\end{table}

\textbf{Domain generalization.} 
 Table~\ref{tab:DG-voc} showcases the generalizability of our proposed method on real-to-artistic tasks. The table is divided into three sections according to the unlabeled domain (i.e., Clipart, Watercolor, and Comic). Our proposed method consistently enhances R-CLIP generalizability and achieves the best result compared to the DA baselines by a large margin. For example, it achieves an  and  improvement on Comic when Clipart and Watercolor is the unlabeled source domain, respectively. We also observe that baselines perform better on Watercolor, which is intuitive as Watercolor is the closest domain to VOC. Likewise, our proposed approach outperforms DIDN on \emph{City, Foggy  Bdd} by  (Table~\ref{tab:DG-city}). These results demonstrate the effectiveness of language-guided feature alignment and the proposed consistency learning method for developing a generalizable object detector. 
 

\begin{table}[!tp]\centering
\scriptsize
\captionsetup{skip=0pt, position=above} \caption{\textbf{Adverse weather adaptation.}\emph{ City\textrightarrow Foggy} in mAP (),  from \cite{he2022cross}.}
\label{tab:uda-c2fc}
\begin{tabular}{l|@{\hspace{3pt}}l@{\hspace{2pt}}|cccccccc|l}
\hline
 &{Method}  &{prsn} &{rider} &{car} &{truck} &{bus} &{train} &{motor} 
&{bike} & {mAP} \\
\hline
\multirow{2}{*}{-}&F-RCNN \cite{ren2015faster}& 36.9 & 36.1 & 44.5.6& 21.7& 32.3&9.2 & 21.5& 32.4&28.3 \textcolor{red}{\tiny{(-20.8)}} \\
&R-CLIP \cite{zhong2022regionclip} &46.5 &51.8 &57.6 &27.3 &45.1 & 19.7& 34.8& 50.2 &\textbf{41.6} \textcolor{red}{\tiny{(-7.5)}} \\
\hline
 \multirow{9}{*}{DA}&SW-DA \cite{saito2019strong} \tiny(CVPR'19) &31.8 & 44.3&48.9 &21.0 &43.8 &28.0&28.9 &35.8& 35.3\\
&D\&Match \cite{kim2019diversify} \tiny(CVPR'19)  &31.8 &40.5 &51.0 &20.9  &41.8 &34.3 &26.6 &32.4 & 34.9\\
 &SC-DA \cite{zhu2019adapting} \tiny(CVPR'19)&33.8&42.1&52.1&26.8&42.5&26.5&29.2&34.4&35.9\\
 &MTOR \cite{cai2019exploring} \tiny(CVPR'19)  &30.6 &41.4 &44.0 &21.9  &38.6 &40.6 &28.3 &35.6 & 35.1\\
 &AFAN \cite{wang2021afan} \tiny{(TIP'21)}&42.5&44.6&57.0&26.4&48.0&28.3&33.2&37.1&39.6\\
&GPA \cite{xu2020cross} \tiny(CVPR'20)  & 32.9&46.7 &54.1 &24.7 &45.7 &41.1 &32.4 &38.7 & 39.5\\ 
 &ViSGA \cite{Rezaeianaran_2021_ICCV} \tiny{(ICCV'21)}&38.8&45.9&57.2&29.9&50.2&\textbf{51.9}&31.9&40.9&43.3\\
 &SFA \cite{wang2021exploring} \tiny{(acmmm'21)}&46.5&48.6&62.6&25.1&46.2&29.4&28.3&44.0&41.3\\
&DSS\cite{wang2021domain} \tiny{(CVPR'21)}&\textbf{50.9}&\textbf{57.6}&61.1&\textbf{35.4}&50.9&36.6&38.4&51.1&47.8\\




&TTD+FPN \cite{he2022cross} \tiny(CVPR'22) & 50.7 & 53.7 & \textbf{68.2} & 35.1 & 53.0 & 45.1 & 38.9 &49.1 & \textbf{49.2}\\
 &IRG\tablefootnote{Source-free domain adaptation} \cite{vs2022instance} \tiny(CVPR'23)  & 37.4& 45.2& 51.9& 24.4& 39.6& 25.2& 31.5 &41.6 &  37.1\\
 \hline
 \multirow{2}{*}{DG}&DIDN \cite{lin2021domain} \tiny(ICCV'21)  &38.3 &44.4 &51.8 &28.7 &53.3 &34.7 &32.4 & 40.4&40.5 \textcolor{red}{\tiny{(-8.6)}} \\
&\textbf{Ours}  &50.5 &55.1 &66.9 &35.0 &\textbf{56.2} &33.5 & \textbf{41.0}& \textbf{54.3}&\textbf{49.1} \\
\hline
\end{tabular}
\end{table}
\textbf{Extension to domain adaptation.} Our method can naturally extend to domain adaptation as it requires only one labeled source domain. The labeled and unlabeled source domains in the domain generalization task serve as source and target domains, respectively. Table~\ref{tab:uda-c2fc} presents the result of \emph{} adaptation against baselines and sota DA methods, while real-to-artistic adaptation can be found in the Table~\ref{tab:DA-voc}. As illustrated in Table~\ref{tab:uda-c2fc}, our method improves F-RCNN and R-CLIP by  and .
It not only substantially outperforms DIDN (sota DGOD on this setting) by  but also achieves comparable results to the existing DA works. While CDDMSL is slightly under-performed compared to TTD by only , contrary to DA methods, it is not designed to adapt to a particular domain.

\subsection{Ablation \& Discussion}
\label{sec:ablation}
\begin{table*}[!tp]
\scriptsize
\captionsetup{skip=0pt, position=above} 
\caption{(a) \textbf{Real-to-artistic adaptation}. VOC is labeled source domain. (b) \textbf{Ablation study}. \emph{VOC, Clipart~~Watercolor, Comic (DG), Clipart (DA)}. Results in mAP (\%).}
\begin{subtable}[b]{0.455\linewidth}
\captionsetup{skip=0pt, position=above} 

\begin{tabular}
{|l@{\hspace{3pt}}|c|c@{\hspace{3pt}}|c@{\hspace{0pt}}|} 
\hline
\multirow{2}{*}{Method}& \multicolumn{3}{c|}{Target Domain}\\

 &  \multicolumn{1}{c|}{{Clipart}} & \multicolumn{1}{c|}{{Watercolor}} &\multicolumn{1}{c|}{{Comic}}\\
\hline

F-RCNN \cite{ren2015faster}  &24.1&41.2&17.9 \\
R-CLIP \cite{zhong2022regionclip} &  33.3&44.7&34.2 \\
\hline
Adaptive MT \cite{li2022cross} &30.5&43.7&23.4\\
IRG \cite{vs2022instance}&31.5&\textbf{53.0}&- \\
\hline
 DVA &36.6&43.9&35.9\\
 Caption-PL &35.2&44.2&34.2\\
    \bfseries Ours & \bfseries40.4&49.7&\bfseries46.3\\

\hline
 \end{tabular}
 \caption{Domain adaptation on real-to-artistic}
    \label{tab:DA-voc}
\end{subtable}
\hfill
\begin{subtable}[b]{0.56\linewidth}
\centering
    \begin{tabular}{|@{\hspace{2pt}}l@{\hspace{3pt}}l@{\hspace{3pt}}l@{\hspace{3pt}}l@{\hspace{3pt}}|@{\hspace{3pt}}c@{\hspace{3pt}}|c@{\hspace{3pt}}c@{\hspace{3pt}}|}
        \hline
        &&&  &DA & \multicolumn{2}{c|}{DG}\\
        R-CLIP init. &  &   &  & Clipart & Watercolor&Comic \\
        \hline
        &&&  &24.1&41.2&17.9\\
        &&&& 32.3&44.7&34.2\\
        \hline
        & &&& 32.3&41.7&35.1 \\
        & && & 34.6&45.0&35.4 \\
        & &&&35.1&44.2&35.7 \\
        & & && \textbf{40.4}&\textbf{49.8}&\textbf{45.9} \\
\hline
\end{tabular}
        \caption{Effectiveness of each component}
        \label{tab:ablation_b}
    \end{subtable}
    \vspace{-10.0pt}
\end{table*}

We conduct an extensive ablation study by examining five aspects of CDDMSL: (i) Impact of vision-language pre-training (Sec.~\ref{sec:supervised}), (ii) Effectiveness of language space alignment compared to visual-space alignment (Sec.~\ref{sec:CDDMSL}), (iii) Benefit of feature-space objective compared to token-space objective, (iv) Effectiveness of multi-scale learning (Sec~\ref{sec:CDDMSL}), (v) Contribution of KD regularization (Sec.~\ref{sec:KD}). Table~\ref{tab:ablation_b} exhibits the effectiveness of each component.

(i) \textbf{Vision-language pre-training.} According to Table~\ref{tab:DG-voc}, R-CLIP solely notably improves the generalizability of F-RCNN (pre-trained on Imagenet) by , , and  on Clipart, Watercolor, and Comic, respectively. Additionally, we observe  gain on \emph{City,Foggy  Bdd} (Table~\ref{tab:DG-city}), supporting the benefit of vision-language pre-training. 
Despite the improvement, vision-language pre-training does not guarantee optimal generalizability as it lacks explicit optimization for adaptation to a new domain, and the model may still overfit to domain-specific features during fine-tuning. Hence, we further improve R-CLIP initialization by incorporating descriptive consistency learning as described in Sec.~\ref{sec:CDDMSL}.

(ii) \textbf{Language space vs. visual space alignment.}
    To better understand the effectiveness of enforcing domain-invariant learning through language space, we compare our model with the Direct Visual Alignment baseline (row 5 in Table~\ref{tab:DG-voc} and Table~\ref{tab:DA-voc}) under an identical setting. In all settings, our (last row) substantially exceeds DVA in DA and DG tasks. 

(iii) \textbf{Feature-space vs. token-space.} Comparing Caption-PL (row 4) with our approach in Table~\ref{tab:DG-voc} and Table~\ref{tab:DA-voc} indicates the superiority of enforcing consistency in the feature space. Ours significantly improves Caption-PL in all settings and tasks. Token-space approaches may degrade the performance by focusing on unnecessary non-informative tokens. 

(iv) \textbf{Effectiveness of Multi-Scale Learning.} Table~\ref{tab:ablation_b} shows instance-level is slightly more effective than image-level, but combined delivers the best result. This underscores the importance of mitigating bias and distribution shifts at both levels in object detection.

(v) \textbf{KD regularization's contribution.} Finally, adding a KD regularizer results in the best performance (compare last two rows in Table~\ref{tab:ablation_b}), implying that KD regularization helps the model avoid trivial solutions and produce semantically consistent descriptions.





\begin{table}[!tp]
\scriptsize
\centering
\captionsetup{skip=0pt, position=above} \caption{\textbf{Visualization.} \emph{City, Foggy Bdd} \& \emph{VOC, Clipart Comic, Watercolor}.}
\label{Fig:Vis_weather}
\begin{tabular}{@{\hspace{0pt}}c@{\hspace{3pt}}c@{\hspace{2pt}}c@{\hspace{2pt}}}




\raisebox{5.5\height}{\textbf{Night}}& \includegraphics[width=0.32\textwidth]{images/pred_comp/adverse-weather/bdd/baseline/img_b84f4f83-c6842bd6.jpg.jpg} &
\includegraphics[width=0.32\textwidth,]{images/pred_comp/adverse-weather/bdd/ours/img_b84f4f83-c6842bd6.jpg.jpg} \\
\raisebox{5.5\height}{\textbf{Rainy}}& \includegraphics[width=0.32\textwidth]{images/pred_comp/adverse-weather/bdd/baseline/img_c836f138-d9b4f7a1.jpg.jpg} &
\includegraphics[width=0.32\textwidth]{images/pred_comp/adverse-weather/bdd/ours/img_c836f138-d9b4f7a1.jpg.jpg} \\
\raisebox{5.5\height}{\textbf{Comic}} & \includegraphics[width=0.29\textwidth,height=0.175\textwidth]{images/pred_comp/real-to-artistic/comic/baseline/img_33497847.jpg} & 
\includegraphics[width=0.29\textwidth,height=0.175\textwidth]{images/pred_comp/real-to-artistic/comic/ours/img_33497847.jpg}  \\
\raisebox{5.5\height}{\textbf{Watercolor}}& \includegraphics[width=0.32\textwidth,height=0.18\textwidth]{images/pred_comp/real-to-artistic/watercolor/baseline/img_155004325.jpg} &
\includegraphics[width=0.32\textwidth,height=0.18\textwidth]{images/pred_comp/real-to-artistic/watercolor/ours/img_155004325.jpg}\\
& \textbf{R-CLIP} & \textbf{Ours}\\
\end{tabular}
\end{table}

\textbf{Visualization.} Table~\ref{Fig:Vis_weather} shows qualitative detection results from the R-CLIP baseline and CDDMSL. R-CLIP yields more false negatives, particularly in challenging conditions like rain and detecting distant small objects. Our multi-scale feature alignment improves object-background differentiation, leading to superior detection on the target domain.    Additionally, CDDMSL ensures more accurate recognition by retaining semantically crucial information.