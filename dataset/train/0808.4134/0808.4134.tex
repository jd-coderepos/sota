\documentclass[11pt]{article}
\usepackage{amsmath,amsfonts,amsthm,amssymb,ifthen,epsfig,subfigure,caption}

\def\thesubfigure{}

\newboolean{@laptop}
\newboolean{@todoon}
\setboolean{@laptop}{true}
\setboolean{@todoon}{true}
 
\ifthenelse{\boolean{@laptop}}{}{\usepackage{mathbbol}}

\def\todo#1{\ifthenelse{\boolean{@todoon}}{\marginpar{\textit{#1}}}{}}

\textheight 8.5in
\topmargin -0.2in
\oddsidemargin 0.2in
\textwidth 6.3in



\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{warning}[theorem]{Warning}
















 
\newcommand{\Furedi}{F{\"u}redi}
\newcommand{\Komlos}{Koml{\'o}s}

\newcommand{\sparsify}{\ensuremath{\mathtt{Sparsify}}}
\newcommand{\sparsifytwo}{\ensuremath{\mathtt{Sparsify2}}}
\newcommand{\unwtedsparsify}{\ensuremath{\mathtt{UnwtedSparsify}}}
\newcommand{\boundedsparsify}{\ensuremath{\mathtt{BoundedSparsify}}}

\newcommand{\sample}{\ensuremath{\mathtt{Sample}}}
\newcommand{\approxcut}{\ensuremath{\mathtt{ApproxCut}}}
\newcommand{\approxdecomp}{\ensuremath{\mathtt{ApproxDecomp}}}
\newcommand{\partition}{\ensuremath{\mathtt{Partition}}}
\newcommand{\partitiontwo}{\ensuremath{\mathtt{Partition2}}}
\newcommand{\partsample}{\ensuremath{\mathtt{PartitionAndSample}}}



\newcommand{\setj}[2]{S_{#1} (#2)}
\newcommand{\lamj}[2]{\lambda_{#1} (#2)}



\newcounter{bean}

\newenvironment{tightlist}{\begin{list}{}{\usecounter{bean}\setlength{\topsep}{0em}
  \setlength{\parsep}{0em}
  \setlength{\partopsep}{0em}
  \setlength{\itemsep}{0em}
}}{\end{list}}


\def\Span#1{\textbf{Span}\left(#1  \right)}

\newcommand\zzero{\boldsymbol{\mathit{0}}}


\def\calW{\mathcal{W}}
\def\calL{\mathcal{L}}

\def\util{\tilde{u}}
\def\vtil{\tilde{v}}
\def\Htil{\widetilde{H}}
\def\Itil{\widetilde{I}}
\def\Dtil{\widetilde{D}}
\def\dtil{\tilde{d}}
\def\Ktil{\widetilde{K}}
\def\ktil{\tilde{k}}
\def\Khat{\widehat{K}}
\def\khat{\hat{k}}
\def\Ctil{\widetilde{C}}
\def\Chat{\widehat{C}}
\def\ctil{\tilde{c}}
\def\Gtil{\widetilde{G}}
\def\Etil{\widetilde{E}}
\def\Ftil{\widetilde{F}}
\def\Ltil{\widetilde{L}}
\def\ftil{\tilde{f}}
\def\etil{\tilde{e}}
\def\Atil{\widetilde{A}}
\def\Ahat{\widehat{A}}
\def\ahat{\hat{a}}
\def\That{\widehat{T}}
\def\Ehat{\widehat{E}}
\def\atil{\tilde{a}}
\def\Ghat{\widehat{G}}
\def\Btil{\widetilde{B}}
\def\btil{\tilde{b}}
\def\htil{\tilde{h}}
\def\wtil{\tilde{w}}
\def\ytil{\tilde{y}}
\def\fhat{\hat{f}}
\def\Fhat{\widehat{F}}
\def\mhat{\hat{m}}
\def\Stil{\widetilde{S}}

\def\ztil{\tilde{z}}


\def\supp#1#2{\sigma_{f} (#1, #2)}
\def\lap#1{\mathcal{L} (#1)}


\def\vs#1#2#3{#1_{#2},\ldots , #1_{#3}}


\def\round#1{\left[#1 \right]_{\epsilon }}

\def\have#1#2#3{\frac{1}{2}\Big(H_{#1} (#2-2\phi #3) + H_{#1} (#2+2\phi #3) \Big)}
\def\haveb#1#2{\frac{1}{2}\Big(H_{#1} (#2-2\phi \bar{#2}) + H_{#1} (#2+2\phi \bar{#2}) \Big)}

\def\jt{\tilde{j}}
\def\pt{\tilde{p}}
\def\pit{\tilde{\pi}}
\def\rhot{\tilde{\rho}}

\def\phat{\hat{p}}
\def\epshat{\hat{\epsilon}}


\def\softO#1{\widetilde{\mathcal{O}} \left( #1 \right)}
\def\bigO#1{\widetilde{\mathcal{O}} \left( #1 \right)}


\def\form#1#2{#1^{T} #2}

\def\calP{{\cal P}}
\def\calQ{{\cal Q}}
\def\calR{{\cal R}}

\def\myPhiSym{\ifthenelse{\boolean{@laptop}}{\varphi}
{\mathbb{\Phi}}
}


\def\trace#1{\mathrm{Tr} \left(#1 \right)}

\def\calC{{\cal C}}
\def\calW{{\cal W}}
\def\bdry#1#2{\partial_{#1}\left(#2\right)}
\def\myphi#1#2{\myPhiSym_{#1}\left(#2\right)}
\def\graphmyphi#1{\myPhiSym_{#1}}
\def\cutsize#1{\textbf{cut-size}\left(#1\right)}

\def\balance#1{\textbf{bal}\left(#1\right)}
\def\level#1{\textbf{level}\left(#1  \right)}

\def\pleq{\preccurlyeq}
\def\pgeq{\succcurlyeq}

\def\edge#1{\textrm{edges}\left(#1  \right)}
\def\degree#1#2{\textrm{deg}_{#1}\left(#2  \right)}
\def\wdegree#1#2{\textrm{wdeg}_{#1}\left(#2  \right)}
\def\bridge#1#2{\textrm{bridge}\left(#1, #2  \right)}
\def\metaGraph#1#2#3{\textrm{metaGraph}\left(#1, #2 ,#3 \right)}

\def\edg#1{\textbf{(}#1 \textbf{)}}

\def\stretch#1#2{\textrm{stretch}_{#1} (#2)}
\def\weight#1{\textrm{weight} (#1)}
\def\res#1{\textrm{resistance} (#1)}

\def\union{\cup}
\def\intersect{\cap}
\def\Union{\bigcup}
\def\Intersect{\bigcap}

\def\defeq{\stackrel{\mathrm{def}}{=}}

\newcommand\xx{\boldsymbol{\mathit{x}}}

\newcommand\yy{\boldsymbol{\mathit{y}}}
\newcommand\zz{\boldsymbol{\mathit{z}}}
\newcommand\rr{\boldsymbol{\mathit{r}}}
\newcommand\bb{\boldsymbol{\mathit{b}}}

\newcommand\cc{\boldsymbol{\mathit{c}}}

\def\prob#1#2{\Pr_{#1}\left[ #2 \right]}
\def\expec#1#2{\mbox{\bf E}_{#1}\left[ #2 \right]}
\newcommand{\E}{\mbox{{\bf E}}}

\def\norm#1{\left\| #1 \right\|}
\def\onenorm#1{\left\| #1 \right\|_{1}}
\def\infnorm#1{\left\| #1 \right\|_{\infty }}
\def\fnorm#1{\left\| #1 \right\|_{F}}
\def\setof#1{\left\{#1  \right\}}
\def\sizeof#1{\left|#1  \right|}

\def\dist#1#2{\mbox{{\bf dist}}\left(#1, #2 \right)}
\def\diff#1{\, d #1 \,}

\def\setminus{-}



\def\pos#1{\mathcal{H} (#1)}

\newcommand\brho{\bar{\rho}}



\def\bvec#1{{\mbox{\boldmath }}}
\def\origin{{\mbox{\boldmath }}}

\def\pleq{\preccurlyeq}
\def\pgeq{\succcurlyeq}

\def\abs#1{\left|#1  \right|}
\def\intersect{\cap}

\newcommand{\ceiling}[1]{\lceil#1\rceil}
\newcommand{\floor}[1]{\lfloor#1\rfloor}



\def\wdilation#1#2{\textbf{wd}_{#1}\left(#2  \right)}
\def\wcong#1#2{\textbf{wc}_{#1}\left(#2  \right)}

\def\path#1#2{\textbf{path}_{#1}\left(#2  \right)}

\def\bigO#1{O\left(#1  \right)}

\def\xtil{\tilde{x}}
\def\setof#1{\left\{#1  \right\}}
\def\abs#1{\left|#1  \right|}
\def\vol#1{\mathrm{Vol}\left(#1  \right)}



\def\lap#1{\mathcal{L} (#1)}

\newcommand\xxt{\boldsymbol{\mathit{\tilde{x}}}}

\newdimen\pIR
\pIR= -131072sp
\newcommand\StevesR{{\rm I\kern\pIR R}}
\def\Reals#1{\StevesR^{#1}}


\def\tA{\tilde{A}}
\def\tS{\tilde{S}}
\def\tB{\tilde{B}}
\def\tL{\tilde{L}}
\def\td{\tilde{d}}

\def\tP{\tilde{P}}
\def\tR{\tilde{R}}

\def\wedge#1#2#3{\left(\setof{#1,#2},#3 \right)}

\def\conduc#1#2{\Phi_{#1}\left(#2  \right)}
\def\conducin#1#2{\Phi^{G}_{#1}\left(#2  \right)}

\def\Conduc#1{\Phi_{#1}}
\def\Conducin#1{\Phi^{G}_{#1}}

\def\blowup#1#2{\textrm{blow-up}_{#1}\left( #2 \right)}

\def\preDelta#1{{}^{\delta}\!{#1}}
\begin{document}

\title{Spectral Sparsification of Graphs\thanks{
This paper is the second in a sequence of three papers expanding
  on material that appeared first under the title
  ``Nearly-linear time algorithms for graph partitioning, 
    graph sparsification, and solving linear systems''~\cite{SpielmanTengPrecon}.
The first paper,
``A Local Clustering Algorithm for Massive Graphs and its Application to Nearly-Linear Time Graph Partitioning''~\cite{SpielmanTengCuts}
 contains graph partitioning algorithms that are used to construct the sparsifiers in this paper.
The third paper, ``Nearly-Linear Time Algorithms for Preconditioning and Solving Symmetric, Diagonally Dominant Linear Systems''~\cite{SpielmanTengLinsolve} contains the results
  on solving linear equations and approximating eigenvalues and eigenvectors.
\vskip 0.01in
This material is based upon work supported by the National Science Foundation 
  under Grant Nos. 0325630, 0324914, 0634957, 0635102 and 0707522.
Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.
\vskip 0.01in
Shang-Hua Teng wrote part of this paper while at MSR-NE lab and Boston
University.}
}

\author{
Daniel A. Spielman\\
Department of Computer Science\\
Program in Applied Mathematics\\
Yale University
\and
Shang-Hua Teng\\
Department of Computer Science\\
Viterbi School of Engineering\\
University of Southern California}

\maketitle

\begin{abstract}
We introduce a new notion of graph sparsification based on 
  spectral similarity of graph Laplacians:
 spectral sparsification requires that the Laplacian quadratic form
  of the sparsifier approximate that
  of the original.
This is equivalent to saying that the Laplacian of the sparsifier
  is a good preconditioner for the Laplacian of the original.

We prove that every graph has a spectral sparsifier of nearly-linear size.
Moreover, we present an algorithm that produces spectral sparsifiers
   in time , where  is the number of edges in the
   original graph and  is some absolute constant.
This construction is a key component of a nearly-linear time algorithm
  for solving linear equations in diagonally-dominant matrices.

Our sparsification algorithm makes use of a nearly-linear time algorithm for graph
  partitioning that satisfies a strong guarantee:
 if the partition it outputs is very unbalanced, then the larger part is contained in
  a subgraph of high conductance.
\end{abstract}

\newpage


\section{Introduction}
Graph sparsification is the task of approximating a graph by a sparse graph,
  and is often useful in the design of efficient approximation algorithms.
Several notions of graph sparsification have been proposed.
For example, Chew \cite{PaulChew}
  was motivated by  proximity problems in computational geometry
  to introduce graph spanners.
Spanners are defined in terms of the {\em distance similarity}
  of two graphs:
A spanner is a sparse graph in which the shortest-path distance 
  between every pair of vertices is approximately the same in the original graph
  as in the spanner.
Motivated by cut problems, Benczur and Karger~\cite{BenczurKarger} introduced
  a notion of sparsification that requires that for every set of vertices,
  the weight of the edges
  leaving that set should be approximately the same 
  in the original graph as in the sparsifier.

Motivated by problems in numerical linear algebra and spectral graph theory,
  we introduce a new notion of sparsification 
  that we call \textit{spectral sparsification}.
A spectral sparsifier is a subgraph of the original whose Laplacian
  quadratic form is approximately the same as that of the original graph
  on all real vector inputs.
The Laplacian matrix\footnote{For more information on the Laplacian matrix of a graph, we refer the
  reader to one of~\cite{BollobasMGT,Mohar91Laplacian,GodsilRoyle,Chung}.} of a weighted graph , where  is the weight of edge , is defined by

It is better understood by its quadratic form, which on 
  takes the value


We say that  is a { \em -spectral approximation}
   of  if for all 



Our notion of sparsification captures the {\em spectral similarity}
  between a graph and its sparsifiers.
It is  a stronger notion than the
  cut sparsification of Benczur and Karger:
 the cut-sparsifiers constructed by Benczur and Karger~\cite{BenczurKarger}
  are only required to satisfy these inequalities for all .
In Section~\ref{sec:examples} we present an example demonstrating that these notions
  of approximation are in fact different.


Our main result is
  that every weighted graph has a spectral sparsifier with
   edges that can be computed in
   time,
 where we recall that
  
  means , for some constant .
In particular, we prove that for every weighted graph  and every
  , there is a re-weighted subgraph of  with  edges
  that is a  approximation of .
Moreover, we show how to  find such
  a subgraph in  time, where  and .
The constants and powers of logarithms
  hidden in the -notation
  in the statement of our results are quite large.
Our goal in this paper is not to produce sparsifiers with optimal parameters, but
  rather just to prove
  that spectral sparsifiers with a nearly-linear number of edges exist and
  that they can be found in nearly-linear time.

Our sparsification algorithm makes use of a nearly-linear time
  graph partitioning algorithm, \approxcut ,  that we develop in Section~\ref{sec:approxCut} and which may be of independent interest.
On input a target conductance , \approxcut \ always outputs a set of vertices
  of conductance less than .
With high probability, if the set it outputs is small then its complement
 is contained in a subgraph of conductance at least .

\section{The Bigger Picture}

This paper arose in our efforts to design nearly-linear time algorithms
  for solving diagonally-dominant linear systems, and is the second in
  a sequence of three papers on the topic.
In the first paper~\cite{SpielmanTengCuts}, we develop fast routines
  for partitioning graphs, which we then use in our algorithms for building
  sparsifiers.
In the last paper~\cite{SpielmanTengLinsolve}, we show how to use
  sparsifiers to build preconditioners for diagonally-dominant matrices
  and thereby solve linear equations in such matrices in nearly-linear time.
Koutis, Miller and Peng~\cite{KMP} have recently developed an algorithm for
  solving such systems 
  of linear equations in time 
  that does not rely upon the sparsifiers of the present paper.

The quality of a preconditioner is measured by the relative condition number,
  which for the Laplacian matrices of a graph  and its sparsifier  is

So, if  is a -spectral approximation of  then
  .
This means that
  an iterative solver such as the Preconditioned Conjugate Gradient~\cite{Axelsson}
 can solve a linear system in the Laplacian of
   to accuracy  by solving 
  linear systems in
   and performing as many multiplications by .
As a linear system in a matrix with  non-zero entries may be solved
  in time  by using the Conjugate Gradient as a
  direct method~\cite[Theorem~28.3]{TrefethenBau},
 the use of the sparsifiers in this paper alone provides an
  algorithm for solving linear systems in  to -accuracy in
  time , which is nearly optimal when
  the Laplacian matrix has  non-zero entries.
In our paper on solving linear equations~\cite{SpielmanTengLinsolve}, we show
  how to get the time bound down to ,
  where  is the number of non-zero entries in .





\section{Outline}
In Section~\ref{sec:background}, we present technical background required
  for this paper, and maybe even for the rest of this outline.
In Section~\ref{sec:examples}, we present three examples of graphs and
  their sparsifiers.
These examples help motivate key elements of our construction.


There are three components to our algorithm for sparsifying graphs.
The first is a random sampling procedure.
In Section~\ref{sec:sampling}, we prove that this procedure produces
  good spectral sparsifiers for graphs of high conductance.
So that we may reduce the problem of sparsifying arbitrary graphs
  to that of sparsifying graphs of high conductance, we require
  a fast algorithm for partitioning a graph into parts of high conductance
  without removing too many edges.
In Section~\ref{sec:decomp}, we first prove that such partitions exist,
  and use them to prove the existence of spectral sparsifiers for
  all unweighted graphs.
In Section~\ref{sec:approxCut}, we then build on tools from~\cite{SpielmanTengCuts}
  to develop a graph partitioning procedure that suffices.
We use this procedure in Section~\ref{sec:unweighted} to construct
  a nearly-linear time algorithm
  for sparsifying unweighted graphs.
We show how to use this algorithm to sparsify
  weighted graphs in Section~\ref{sec:weighted}.

We conclude in Section~\ref{sec:conclusion} by surveying recent improvements
  that have been made in both sparsification and in the
  partitioning routines on which the present paper depends.

\section{Background and Notation}\label{sec:background}
By  we always mean the logarithm base , and we denote the
  natural logarithm by .

As we spend this paper studying spectral approximations, we will say
  ``-approximation'' instead of 
  ``-spectral approximation'' wherever it won't create
  confusion.

We may express \eqref{eqn:approxForm} more compactly by employing the notation
   to mean

Inequality  is then equivalent to

We will overload notation by writing  for graphs  and 
  to mean .

For two graphs  and , we write

to indicate the graph whose Laplacian is .
That is, the weight of every edge in  is the sum of the weights of the
  corresponding edges in  and .
We will use this notation even if  and  have different vertex sets.
For example,
  if their vertex sets are disjoint, then their sum is simply
  the disjoint union of the graphs.
It is immediate that  and  imply

In many portions of this paper, we will consider vertex-induced subgraphs of graphs.
\textit{When we take subgraphs, we always preserve the identity of vertices.}
This enables us to sum inequalities on the different subgraphs to say something
  about the original.

For an unweighted graph , we will let  denote the degree
  of vertex .
For  and  disjoint subsets of , we let  denote the set of edges
  in  connecting one vertex of  with one vertex of .
We let  denote the subgraph of  induced on the vertices in :
  the graph with vertex set  containing the edges of  between
  vertices in .

For , we define .
Observe that  if  has  edges.
The conductance of a set of vertices , written
  , is often defined by

The conductance of  is then given by


The conductance of a graph is related  to the smallest non-zero
  eigenvalue of its Laplacian matrix,
  but is even more strongly related to the
  smallest non-zero eigenvalue of its Normalized Laplacian
  matrix (see~\cite{Chung}), whose definition we now recall.
Let  be the diagonal matrix whose -th diagonal is .
The Normalized Laplacian of the graph , written ,
  is defined by

It is well-known that both  and  are positive semi-definite matrices,
  with smallest eigenvalue zero.
The eigenvalue zero has multiplicity one if an only if the graph  is connected, in which case
  the eigenvector of  with eigenvalue zero is the constant vector
  (see~\cite[page 269]{BollobasMGT}, or derive from \eqref{eqn:quadraticForm}).


Our analysis exploits a discreet version of
  Cheeger's inequality\cite{Cheeger} (see~\cite{Chung,JerrumSinclair,DiaconisStrook}), which relates the smallest non-zero
  eigenvalue of , written , to the conductance of .
\begin{theorem}[Cheeger's Inequality]\label{thm:cheeger}

\end{theorem}

\section{A few examples}\label{sec:examples}

\subsection{Example 1: Complete Graph}
\begin{figure}[h]
\centering
\subfigure[: The complete graph on  vertices]{\epsfig{figure=fig1a,height=1.7in}}
\quad
\subfigure[: A -approximation of ]{\epsfig{figure=fig1b,height=1.7in}}
\end{figure}

We first consider what a sparsifier of the complete graph should look like.
Let  be the complete graph on  vertices.
All non-zero eigenvalues of  equal .
So, for every unit vector  orthogonal to the all-1s vector,

From Cheeger's inequality, one may prove that graphs with constant conductance, called expanders,
  have a similar property.
Spectrally speaking, the best of them
  are the Ramanujan graphs~\cite{LPS,Margulis}, which are -regular
  graphs
  all of whose non-zero Laplacian eigenvalues lie between 
   and .
So, if we let  be a Ramanujan graph in which every edge has been given weight ,
  then for every unit vector  orthogonal to the all-1s vector,

Thus,  is a -approximation of .

\subsection{Example 2: Joined Complete Graphs}

\begin{figure}[h]
\centering
\subfigure[: Two complete graphs joined by an edge.]{\epsfig{figure=fig2a,height=1.2in}} \quad
\subfigure[: A good approximation of .  Thicker edges indicate edges of weight ]{\epsfig{figure=fig2b,height=1.2in}}
\end{figure}


Next, consider a graph on  vertices obtained by joining two complete graphs
  on  vertices
  by a single edge, .
Let  and  be the vertex sets of the two complete graphs.
We claim that a good sparsifier for  may be obtained by setting  to be
  the edge  with weight 1, plus  times a Ramanujan graph on each vertex set.
To prove this, let  and  denote the complete graphs on  and ,
  and let  denote the graph just consisting of the edge .
Similarly, let  and  denote  times a Ramanujan graph on each
  vertex set, and let .
Recalling the addition we defined on graphs, we have

We already know that for
,
and  

As , we have

The other inequality follows by similar reasoning.
This example demonstrates both the utility of using edges with different weights, even
  when sparsifying unweighted graphs, and how we can combine sparsifiers
  of subgraphs to sparsify an entire graph.
Also observe that every sparsifier of  must contain the edge , while no other
  edge is particularly important.


\subsection{Example 3: Distinguishing cut sparsifiers from spectral sparsifiers}


\begin{figure}[h]
\centering
\subfigure[:  sets of  vertices arranged in a ring and
  connected by complete bipartite graphs, plus one edge across.]{\epsfig{figure=fig3a,height=2in}} \quad
\subfigure[: A good cut sparsifier of , but a poor spectral sparsifier]{\epsfig{figure=fig3b,height=2in}}
\end{figure}



Our last example will demonstrate the difference between our notion of sparsification
  and that of Benczur and Karger.
We will describe graphs  and  for which  is not a -approximation of  for any small
  , but it is a very good sparsifier of 
  under the definition considered by Benczur and Karger.
The vertex set  will be , where  is even.
The graph  will consist of  complete bipartite graphs, connecting all pairs of
  vertices  and  where .
The graph  will be identical to the graph , except that it will have one additional
  edge  from vertex  to vertex .
As the minimum cut of  has size , and  only differs by one edge,
   is a -approximation of  in the notion considered by Benczur and Karger.
To show that  is a poor spectral approximation of , consider the vector 
  given by

One can verify that

So,  inequality \eqref{eqn:approxForm}
  is not satisfied for any  less than .


\section{Sampling Graphs}\label{sec:sampling}
In this section, we show that if a graph has high conductance,
  then it may be sparsified by a simple random sampling procedure.
The sampling procedure involves assigning a probability
   to each edge , and then selecting edge  to be
  in the graph  with probability .
When edge  is chosen to be in the graph, we multiply its weight by
  .
As the graph is undirected, we implicitly assume that .
Let  denote the adjacency matrix of the original graph , and 
  the adjacency matrix of the sampled graph .
This procedure guarantees that

Sampling procedures of this form were examined by Benczur and Karger~\cite{BenczurKarger}
  and Achlioptas and McSherry~\cite{AchlioptasMcSherry}.
Achlioptas and McSherry analyze the approximation obtained by such a procedure
  through a bound on the norm of a random matrix
  of \Furedi \ and \Komlos~\cite{FurediKomlos}.
As their bound does not suffice for our purposes, we tighten it by
  refining the analysis of \Furedi \  and \Komlos.

If  is going to be a sparsifier for , then we must be sure
  that every vertex in  has edges attached to it.
We guarantee this by requiring that, for some parameter ,

The parameter  controls the number of edges we expect to find
  in the graph, and will be set to at least 
  to ensure that every vertex has an attached edge.

We will show that if  has high conductance and
 \eqref{eqn:delta} is satisfied for a sufficiently large ,
 then  will be a good sparsifier of  with high probability.
The actual theorem that we prove is slightly more complicated,
  as it considers the case where we only apply the sampling
  on a subgraph of .

\begin{theorem}[Sampling High-Conductance Graphs]\label{thm:sampling}
Let  and
  let  be an unweighted graph  whose smallest non-zero normalized Laplacian
  eigenvalue is at least .
Let  be a subset of the vertices of , let  be the edges in ,
  and let  be the rest of the edges.
Let

and let .
Then, with probability at least ,
\begin{itemize}
\item [(S.1)]  is a -approximation of , and
\item [(S.2)] The number of edges in  is at most

\end{itemize}
\end{theorem}


\vskip 0.2in
\noindent
\fbox{
\begin{minipage}{6in}
\noindent 
\begin{enumerate}
\item [1.] Set .
\item [2.] Set .
\item [3.] For every edge  in , set
  .
\item [4.] For every edge  in , with probability
   put an edge of weight  between vertices 
  into .
\end{enumerate}
\end{minipage}
}
\vskip 0.2in



Let  be the diagonal matrix of degrees of vertices of .
To prove Theorem~\ref{thm:sampling}, we  establish that
  the 2-norm of  is probably small\footnote{Recall that the 2-norm of a symmetric matrix is the largest absolute value of
its eigenvalues.}, and then apply the following lemma.

\begin{lemma}\label{lem:normSmallApprox}
Let  be the Laplacian matrix of a connected graph ,  be the Laplacian of ,
  and let  be the diagonal matrix of degrees of .
If
\begin{itemize}
\item [1.] , and
\item [2.] ,
\end{itemize}
then  is a -approximation of  for

\end{lemma}
\begin{proof}
Let  be any vector and let .
By assumption,  is connected and so
  the nullspace of the normalized Laplacian 
  is spanned by .
Let  be the projection of  orthogonal to
  , so

We compute

We may similarly show that

The lemma follows from these inequalities.
\end{proof}

Let  be the adjacency matrix of  and let  be the adjacency matrix
  of .
For each edge ,


To prove Theorem~\ref{thm:sampling}, we will observe that

where  is the diagonal matrix of the diagonal entries of .
It will be easy to bound the second of these terms, so we defer that part of the proof
  to the end of the section.
A bound on the first term comes from the following lemma.

\begin{lemma}[Random Subgraph]\label{lem:sampling}
For all even integers ,

\end{lemma}

Our proof of this lemma applies a modification of techniques introduced
  by \Furedi \ and \Komlos~\cite{FurediKomlos} (See also the paper by
  Vu~\cite{Vu} that corrects some bugs in their work).
However, they consider the eigenvalues of random graphs in which
  every edge can appear.
Some interesting modifications are required to make an argument such
  as ours work when downsampling a graph that may already be sparse.
We remark that without too much work one can
  generalize Theorem~\ref{thm:sampling} so that it applies
  to weighted graphs.

\begin{proof}[Proof of Lemma~\ref{lem:sampling}]
To simplify notation, define

so for each edge ,


Note that  has the same eigenvalues
  as .
So, it suffices to bound the absolute values of the eigenvalues of .
Rather than trying to upper bound the eigenvalues of 
  directly, we will upper bound a power of 's trace.
As the trace of a matrix is the sum of its eigenvalues,
   is an upper bound
  on the th power of every eigenvalue of ,
 for every even power .

Lemma~\ref{lem:trace} implies that, for even ,

Applying Markov's inequality, we obtain

Recalling that
  the eigenvalues of  are the -th powers of the eigenvalues of ,
 and
  taking -th roots, we conclude

\end{proof}


\begin{lemma}\label{lem:trace}
For even ,

\end{lemma}
\begin{proof}
Recall that the  entry of  satisfies

Taking expectations, we obtain

We will now describe a way of coding every sequence
  
  that could
  possibly contribute to the sum.
Of course, any sequence containing a consecutive pair
   for
  which  is always zero
  will contribute zero to the sum.
So, for a sequence to have a non-zero contribution, each
  consecutive pair  must be an edge in the graph .
Thus, we can identify every sequence with non-zero contribution with a walk
  on the graph  from vertex  to vertex .


The first idea in our analysis is to observe that most of the
  terms in this sum are zero.
The reason is that, for all  and 

As  is independent of every
  term in  other than ,
  we see that the term

corresponding to
  , will be zero
  unless each edge  appears at least
  twice (in either direction).


We now describe a method for coding all walks
  in which each edges appears at least twice.
We set  to be the set of time steps 
  at which the edge between  and 
  does not appear earlier in the walk (in either direction).
Note that  is always an element of .
We then let  denote the map from
  , indicating for each
  time step not in  the time step in which
  the edge traversed first appeared (regardless of
  in which direction it is traversed).
Note that we need only consider the cases in which , as
  otherwise some edge appears only once in the walk.
To finish our description of a walk, we need
  a map

indicating the vertex encountered at each time .

For example, for the walk

we get



Using ,  and , we can inductively reconstruct
  the sequence  by the rules
\begin{itemize}
\item if , ,
\item if , and ,
  then , and
\item if , and ,
  then .
\end{itemize}
If ,
  then the tuple  does not properly code
  a walk on the graph of .
We will call  a \textit{valid assignment} for  and  if
  the above rules do produce a walk on the graph of  from 
  to .

We have



Each of the terms

is independent of the others,
  and involves a product of the terms
  
  and
  .
In Lemma~\ref{lem:edgeExpec}, we will prove that

which implies

To bound the sum of products on the right hand-side of
  \eqref{eqn:simpleSample2}, fix  and  and consider
  the following random process for generating a valid 
  and corresponding walk:
  go through the elements of  in order.
For each ,
  pick  to be a random neighbor of the st vertex in the walk.
If possible, continue the walk according to  until it reaches the next
  step in .
If the process produces a valid , return it.
Otherwise, return nothing.
The probability that any particular valid  will be returned by this
  process is

So,


As there are at most
  at most  choices for ,
  and at most  choices for ,
  we may combine inequalities \eqref{eqn:simpleSample2} and \eqref{eqn:simpleSample3} 
  with
  \eqref{eqn:sampling1} to obtain

The lemma now follows from

\end{proof}


\begin{claim}\label{clm:magnitude}

\end{claim}
\begin{proof}
If , then .
If not, then we have .
With probability ,

On the other hand, with probability ,

As  in this case, we have established
  .
\end{proof}

\begin{lemma}\label{lem:edgeExpec}
For all edges  and integers  and ,

\end{lemma}
\begin{proof}
First, if , then .
Second, if , .
So, we may restrict our attention to the case where  and ,
  which by \eqref{eqn:delta} implies .
Claim~\ref{clm:magnitude} tells us that for ,

A similar statement may be made for .
So, it suffices to prove the lemma in the case
  .


As  and
  , we have

In the case , , we finish the proof by

and in the case ,  by

\end{proof}

This finishes the proofs of Lemmas~\ref{lem:trace} and~\ref{lem:sampling}.
We now turn to the last ingredient we will need for the proof of Theorem~\ref{thm:sampling},
  a bound on the norm of the difference of the degree matrices.


\begin{lemma}\label{lem:sampledDegrees}
Let  be a graph and let  be obtained by sampling  with probabilities
   that satisfy \eqref{eqn:delta}.
Let  be the diagonal matrix
  of degrees of , and let  be the diagonal matrix of weighed degrees
  of .
Then,

\end{lemma}
\begin{proof}
Let  be the weighted degree of vertex  in .
As  and  are diagonal matrices,

As the expectation of  is  and
   is a sum of  random variables
  each of which is always  or some value less than ,
  we may apply the variant of the Chernoff bound given in
  Theorem~\ref{thm:chernoff}
  to show that

The lemma now follows by taking a union bound over .
\end{proof}



We use the following variant of the Chernoff bound from~\cite{Raghavan}.
\begin{theorem}[Chernoff Bound]\label{thm:chernoff}
Let  all lie in  and 
 let  be independent random variables such
  that  equals  with probability 
  and  with probability .
Let  and
  .
Then,

For ,
  both of these probabilities are at most
  .
\end{theorem}
We remark that Raghavan~\cite{Raghavan} proved this theorem with ;
  the extension to general  follows by re-scaling.


\begin{proof}[Proof of Theorem~\ref{thm:sampling}]
Let  be the Laplacian of ,  be its adjacency matrix, and 
  its diagonal matrix of degrees.
Let ,  and  be the corresponding matrices for .
The matrices  and  only differ on rows and columns indexed by .
So, if we let  denote the submatrix of  with rows and
  columns in , we have

Applying Lemma~\ref{lem:sampling} to the first of these terms,
  while observing

  we find

Applying Lemma~\ref{lem:sampledDegrees} to the second term,
  we find

Thus, with probability at least ,

in which case Lemma~\ref{lem:normSmallApprox} tells us that
   is a -approximation of 
  for

using
  .

Finally, we use Theorem~\ref{thm:chernoff} to bound the number of edges in .
For each edge  in , let  be the indicator random variable for
  the event that edge  is chosen to appear in .
Using  to denote the degree of vertex  in , we have

One may similarly show that .
Applying Theorem~\ref{thm:chernoff} with  (note that here  is the
parameter in the statement of Theorem~\ref{thm:chernoff}), we obtain



\end{proof}


\section{Graph Decompositions}\label{sec:decomp}
In this section, we prove
  that every graph can be decomposed into components of high conductance,
  with a relatively small number of edges bridging the components.
A similar result was obtained independently by Trevisan~\cite{Trevisan}.
We prove this result for three reasons: first, it enables us to quickly establish
  the existence of good spectral sparsifiers.
Second, our algorithm for building sparsifiers requires a graph
  decomposition routine which is inspired by the computationally
  infeasible routine presented in this section\footnote{The routine \texttt{idealDecomp} is infeasible because it requires
  the solution of an NP-hard problem in step 2.
We could construct sparsifiers from a routine that approximately satisfies
  the guarantees of \texttt{idealDecomp}, such as the clustering algorithm of
  Kannan, Vempala and Vetta~\cite{KannanVempalaVetta}.
However, their routine could take  quadratic time, which is too slow
  for our purposes.
}.
Finally, the analysis of our algorithm relies upon Lemma~\ref{lem:certificate}, which
  occupies most of this section.
Throughout this section, we will consider an unweighted graph ,
  with .
In the construction  of a decomposition of , we will be concerned with
  vertex-induced subgraphs of .
However, \textit{when measuring the conductance and volumes of vertices in
  these vertex-induced subgraphs, we will continue to measure the
  volume according to the degrees of vertices in the original graph.}
For clarity, 
  we define the boundary of a vertex set  with respect to another
  vertex set  to be

we define the conductance of a set  in the subgraph induced
  by  to be

and we define

For convenience, we define  and, for ,
  .

We introduce the notation  to denote the graph  to which self-loops
  have been added so that every vertex in  has the same degree as in .
For 

Because  measures volume by degrees in 
  and those degrees are higher than in ,

So, when we prove lower bounds on , we obtain lower
  bounds on .

\subsection{Spectral Decomposition}

We define a \emph{decomposition} of  to be a partition of  into sets
  , for some .
We say that a decomposition is 
  a \emph{-decomposition} if
   for all .
We define the boundary of a decomposition, written
   to be the set of edges between different
  vertex sets in the partition:

We say that a decomposition  is a
  \emph{-spectral decomposition}
  if the smallest non-zero normalized Laplacian eigenvalue of
   is at least , for all .
By Cheeger's inequality (Theorem \ref{thm:cheeger}),
  every -decomposition is a -spectral decomposition.

\begin{theorem}\label{thm:graphDecompExist}
Let  be a graph and let .
Then,  has a -decomposition with
.
\end{theorem}


\subsection{Existence of spectral sparsifiers}\label{sec:existence}
Before proving Theorem \ref{thm:graphDecompExist},
  we first quickly explain how to use Theorem~\ref{thm:graphDecompExist}
  to prove that spectral sparsifiers exist.
Given any graph , apply the theorem to find a decomposition of the graph
  into components of conductance , with at most half of the
  original edges bridging components.
Because this decomposition is a -spectral decomposition,
by Theorem~\ref{thm:sampling} we may sparsify the graph induced on
  each component by random sampling.
The average degree in the sparsifier for each component will be
  .
It remains to sparsify the edges bridging components.
If only  edges bridge components, then we do not need to sparsify
  them further.
If more edges bridge components, we sparsify them recursively.
That is, we treat those edges as a graph in their own right,
  decompose that graph, sample the edges induced in its components,
  and so on.
As each of these recursive steps reduces the number of edges remaining
  by at least a factor of two,
  at most a logarithmic number of recursive steps will be required, and thus the
  average degree of the sparsifier will be at most .
The above process also establishes the following decomposition theorem.







Recently, Batson, Spielman and Srivastava~\cite{BatsonSpielmanSrivastava}
  have shown that -spectral sparsifiers with 
  edges exist.

\subsection{The Proof of Theorem \ref{thm:graphDecompExist}}

Theorem \ref{thm:graphDecompExist} is not algorithmic.
It follows quickly from
  the following lemma, which says that if the largest set with conductance less
  than  is small,
  then the graph induced on the complement has conductance almost .
This lemma is the key component in our proof
  of Theorem~\ref{thm:graphDecompExist}, and its analog for 
  approximate sparsest cuts (Theorem \ref{thm:ApproxCut})
  is the key to our algorithm.

\begin{lemma}[Sparsest Cuts as Certificates]\label{lem:certificate}
Let  be a graph and let .
Let  and let  be a set
  maximizing  among those
  satisfying
\begin{enumerate}
\item [(C.1)]  , and
\item [(C.2)] .
\end{enumerate}
If  for ,
  then

\end{lemma}
\begin{proof}
Let  be a set of maximum size that satisfies  and ,
  let

and
  assume by way of contradiction that
  
Then, there exists a set
  
  such that


Let .
We will prove

and ,
  contradicting the maximality of .

We begin by observing that


We divide the rest of our proof into two cases, depending on whether
  or not .
First, consider the case in which
  .
In this case,  provides a contradiction to the maximality of
  , as ,
  and

which implies


In the case ,
  we will prove that the set  contradicts the maximality of .
First, we show

which implies
   because we assume .
To prove \eqref{eqn:certificate2}, compute


To upper bound the conductance of , we compute


So,

by our choice of .
\end{proof}

We will prove Theorem~\ref{thm:graphDecompExist} by proving that
  the following procedure produces the required decomposition.


\vskip 0.2in
\noindent
\fbox{
\begin{minipage}{6in}
Set .\\
Note that we initially call this algorithm with .
\\

\noindent 
\begin{enumerate}

\item [1.] If , then return .  Otherwise, proceed.

\item [2.] Let  be the subset of  maximizing  satisfying (C.1) and (C.2).

\item [3.] If , return the decomposition
   ,


\item [4.] else, return the decomposition
  .
\end{enumerate}
\end{minipage}
}
\vskip 0.2in

\begin{proof}[Proof of Theorem~\ref{thm:graphDecompExist}]
To see that the recursive procedure terminates, recall that we have defined
   when .

Let  be the output of .
Lemma~\ref{lem:certificate} implies that
   for each .

To bound the number of edges in ,
  note that the depth of the recursion is at most 
  and that at most a  fraction of the edges are added to
   at each level of the recursion.
So, 

\end{proof}



\section{Approximate Sparsest Cuts}\label{sec:approxCut}

Unfortunately, it is NP-hard to compute sparsest cuts.
So, we cannot directly
  apply Lemma~\ref{lem:certificate} in the design of our algorithm.
Instead, we will apply a nearly-linear time algorithm, \approxcut,
  that computes approximate sparsest cuts that satisfy an analog of
  Lemma~\ref{lem:certificate}, stated in Theorem~\ref{thm:ApproxCut}.
Whereas in Lemma~\ref{lem:certificate} we proved that if the largest sparse cut
  is small then its
  complement has high conductance,
  here we prove that if the cut output by \approxcut\ is small, then
  its complement is contained in a subgraph of high conductance.

The algorithm \approxcut \ works by repeatedly calling a routine
  for approximating sparsest cuts, \partition, from~\cite{SpielmanTengCuts}.
On input a graph that contains a sparse cut, with high probability
  the algorithm \partition \ either finds a large cut or a cut that has high overlap
  with the sparse cut.
We have not been able to find a way to quickly
   use an algorithm satisfying such a guarantee
  to certify that the complement of a small cut has high conductance.
Kannan, Vempala and Vetta~\cite{KannanVempalaVetta} showed that if we applied
  such an algorithm until it could not find any more cuts then we
  could obtain such a guarantee.
However, such a procedure could require quadratic time, which it too
  slow for our purposes.

\begin{theorem}[\approxcut]\label{thm:ApproxCut}
Let  and let  be a graph with  edges.
Let  be the output of .
Then
\begin{itemize}
\item [(A.1)] ,
\item [(A.2)] If  then , and
\item [(A.3)] With probability at least , either
\begin{itemize}
\item [(A.3.a)] , or
\item [(A.3.b)] there exists a set  for which
  , where

for some absolute constant .
\end{itemize}
\end{itemize}
Moreover, the expected running time of \approxcut\  is 
  .
\end{theorem}

The code for \approxcut\ follows.
It relies on a routine called \partitiontwo\, which in turn relies on a routine called
  \partition\ from~\cite{SpielmanTengCuts}.
While one could easily combine the routines \approxcut \ and \partitiontwo ,
  their separation simplifies our analysis.
The algorithm \partitiontwo \ is very simple: it just calls 
  \partition \ repeatedly and collects the cuts it produces until they
  contain at least  of the volume of the graph or until it has
  made enough calls.
The algorithm \approxcut \ is similar: it calls \partitiontwo \ in the
  same way that \partitiontwo \ calls \partition .
  
\vskip 0.2in
\noindent
\fbox{
\begin{minipage}{6in}
\noindent ,
  where  is a graph, .
\begin{enumerate}
\item [(0)] Set  and .

\item [(1)] Set  and .
\item [(2)] While  and
,
\begin{enumerate}
\item [(a)] Set .
\item [(b)] Set 
\item [(c)] Set .
\end{enumerate}
\item [(3)] Set .
\end{enumerate}
\end{minipage}
}
\vskip 0.2in

\subsection{Partitioning in Nearly-Linear-Time}

\vskip 0.2in
\noindent
\fbox{
\begin{minipage}{6in}
\noindent ,
where  is a graph,  and .
\begin{enumerate}
\item [(0)] Set  and .  Set .

\item [(1)] While  and
,
\begin{enumerate}
\item [(a)] Set .
\item [(b)] Set 
\item [(c)] Set .
\end{enumerate}
\item [(2)] Set .
\end{enumerate}
\end{minipage}
}
\vskip 0.2in

The algorithm \partition\ from~\cite{SpielmanTengCuts},
  satisfies the following theorem (see \cite[Theorem 3.2]{SpielmanTengCuts})

\begin{theorem}[\partition]\label{thm:Partition}
Let  be the output of ,
  where  is a graph and .
Then
\begin{itemize}
\item [(P.1)] ,
\item [(P.2)] If  then , and
\item [(P.3)] For some absolute constant  and

for \textbf{every} set  satisfying

with probability at least  either
\begin{itemize}
\item [(P.3.a)] , or
\item [(P.3.b)] .
\end{itemize}
\end{itemize}
Moreover, the expected running time of \partition \ is 
  .
\end{theorem}
If either  or  occur for a set
   satisfying \eqref{eqn:P3}, we say that \partition \
 \textbf{succeeds} for .
Otherwise, we say that it \textbf{fails}.

One can view condition  in Theorem~\ref{thm:ApproxCut} as reversing
  the quantifiers in condition  in Theorem~\ref{thm:Partition}.
Theorem~\ref{thm:Partition} says that for every set  of low conductance
  there is a good probability that a substantial portion of  is removed.
On the other hand, Theorem~\ref{thm:ApproxCut} says that with high probability
  all sets of low conductance will be removed.

The algorithm \partitiontwo \ satisfies a guarantee similar to that of \partition,
  but it strengthens condition .

\begin{lemma}[\partitiontwo]\label{lem:partition2}
Let  be the output of ,
  where  is a graph,  and .
Then
\begin{itemize}
\item [(Q.1)] ,
\item [(Q.2)] If  then , and
\item [(Q.3)] For \textbf{every} set 
  satisfying

  with probability at least ,
  either
\begin{itemize}
\item [(Q.3.a)] , or
\item [(Q.3.b)] ,
  where .
\end{itemize}
\end{itemize}
Moreover, the expected running time of \partitiontwo \ is
  .
\end{lemma}
If either  or  occur
  for a set  satisfying \eqref{eqn:Q3},
  we say that \partitiontwo \
 \textbf{succeeds} for .
Otherwise, we say that it \textbf{fails}.

The proof of this lemma is routine, given Theorem~\ref{thm:Partition}.

\begin{proof}
Let  be such that .
To prove (Q.1),
  let .
As , .
By ,
  , so


To establish (Q.2),
  we first compute

So, if  ,
  then .
On the other hand, we established above that
  , from which
  it follows that

So,


To prove (Q.3),
  let  be a set satisfying \eqref{eqn:Q3}, and
  let .
From Theorem~\ref{thm:Partition}, we know that with probability at least
  ,

We need to prove that with probability at least ,
  either 
  or
  .
If neither of these inequalities hold, then

where we recall .
So, there must exist a  for which .
If  satisfied condition \eqref{eqn:Q3} 
  in  this would imply that
  \partition \ failed for .
We already know this is unlikely for .
To show it is unlikely for , we prove that  does
  satisfy condition \eqref{eqn:Q3} 
  in .
Assuming \eqref{eqn:part2half},

where the third equality follows from the assumption 
   and the last
  inequality follows from the definition
  .
So,  satisfies conditions \eqref{eqn:P3} 
  with , but \partition \ fails for .
As there are at most  sets , this happens for one of them with probability at most
  .

Finally, the bound on the expected running time of \partitiontwo \
  is immediate from the bound on the running time of \partition.
\end{proof}

\subsection{Proof of Theorem~\ref{thm:ApproxCut}}
The rest of this section is devoted to the proof of Theorem~\ref{thm:ApproxCut}, with all but one
  line devoted to part (A.3).
Our goal is to prove the existence of a set of vertices  of high conductance
  that contains all the vertices not cut out by \approxcut .
We will construct this set  in stages.
Recall that  is the set of vertices
  that are not removed by the first  cuts.
In stage , we will express , a superset of , as a set of high
  conductance  plus some vertices in .
We will show that in each stage the volume of the vertices that are
  not in the set of high conductance shrinks by at least a factor of 2.

We do this by letting  be the biggest set of conductance at most
   in , where  is a factor  smaller
  than the conductance of .
We then show that at least a  fraction of the volume
  of  lies outside  and thus inside .
From Lemma~\ref{lem:certificate} we know that 
  has high conductance.
We will use Lemma~\ref{lem:partition2} to show that at most an 
  fraction of  appears in .
So, the volume of  that remains inside  will be at most
  half the volume of  that is not in .
We then set , and proceed
  with our induction.
Eventually, we will arrive at an  for which either  
  has high conductance or enough volume has been removed from .

\begin{figure}[h]
\centering
\subfigure[The subsets of .
Not drawn to scale.]{\epsfig{figure=wi,width=2in}} \qquad
\subfigure[The shaded portion is .
It equals , and so can be viewed as the
union of the set of vertices maintained by the algorithm with the
high-conductance set we know exists.]{\epsfig{figure=wip1,width=2in}}
\end{figure}

Formally, we set 

We then construct sets ,  and  by the following
  inductive procedure.

\begin{itemize}

\item [1.] Set .

\item [2.] While  and  is defined,

\begin{itemize}

\item [a.] If  contains a set 
 such that

set  to be such a set of maximum size.

If , 
  stop the procedure and leave  undefined.

If there is no such set, set , set ,
  stop the procedure and leave  undefined.



\item [b.] Set .
\item [c.] Set .

\item [d.] Set .

\item [e.] Set .

\item [f.] Set .



\end{itemize}

\item [3.] Set  where  is the last index for which  is defined.
\end{itemize}

Note that there may be many choices for a set .
Once a choice is made, it must be fixed for the rest of the procedure so that
  we can reason about it using Lemma~\ref{lem:partition2}.

We will prove that if some set  has volume
  greater than , then with high probability \approxcut \
  will return a large cut , and hence part (A.3.a) is satisfied.
Thus, we will be mainly concerned with the case in which this does not happen.
In this case, we will prove that  is not too much less than ,
and so the set  has high conductance.
If the procedure stops because  is empty, then 
  is the set of high conductance we seek.
We will prove that for some  probably either  is empty, 
   or .

\begin{claim}\label{clm:VinsideW}
For all  such that  is defined,

\end{claim}
\begin{proof}
We prove this by induction on .
For , we know that .
As  and the algorithm ensures ,

Thus,

\end{proof}


\begin{claim}\label{clm:uiExpander}
For all  such that  is defined

\end{claim}
\begin{proof}
Follows immediately from  Lemma~\ref{lem:certificate} and
  the definitions of  and .
\end{proof}


\begin{lemma}\label{lem:ac1}
If
\begin{itemize}
\item [(a)] , and
\item [(b)] , then
\end{itemize}
  then

\end{lemma}
\begin{proof}
This lemma follows easily from the definitions of the sets 
  ,  and .
As  and ,

So, we may apply Claim~\ref{clm:uiExpander} 
  to show

On the other hand, 

Combining these two inequalities yields

and

As

we may conclude

\end{proof}

We now show that if at most an  fraction of each  appears in
  , then the sets  shrink to the point of vanishing.

\begin{lemma}\label{lem:ac2}
If all defined  and  satisfy
\begin{itemize}
\item [(a)] , 
\item [(b)] , and
\item [(c)] ,
\end{itemize}
then
for all  for which  is defined,

and

In particular, the set  is empty if it is defined.
\end{lemma}
\begin{proof}
Lemma~\ref{lem:ac1} tells us that

Combining this inequality with  yields

Similarly, we may conclude from Lemma~\ref{lem:ac1} that

which when combined with  yields

from which the second part of the lemma follows.

For  to be defined, we must have ;
  so,

We conclude that the set  must be empty if it is defined.
\end{proof}

This geometric shrinking of the volumes of the sets  allows
  us to prove a lower bound on .


\begin{lemma}\label{lem:ac3}
Under the conditions of Lemma~\ref{lem:ac2},

for some absolute constant .
\end{lemma}
\begin{proof}
We have

As  and , we have

To analyze the other product, we apply Lemma~\ref{lem:ac2} to prove

and so

Thus,

for some constant .
\end{proof}

To prove that condition  of Lemma~\ref{lem:ac2} is probably satisfied,
  we will consider two cases.
First, if 
  then  is trivially satisfied as .
On the other hand, if ,
  then we will show that  satisfies conditions \eqref{eqn:Q3}
  in , and so 
  with high probability the
  cut  made by 
  \partitiontwo \ removes enough of .

\begin{lemma}\label{lem:ac4}
If
\begin{itemize}
\item [(a)] , 
\item [(b)] , and
\item [(c)] ,
\end{itemize}
then

where .
If, in addition 

then

\end{lemma}
\begin{proof}
By Claim~\ref{clm:ac0},

Set .
Assumption  tells us that .
As , 

The last part of the lemma is trivial.
\end{proof}

\begin{claim}\label{clm:ac0}

\end{claim}
\begin{proof}

\end{proof}

We now show that if ,
  then in the th iteration \partition2 \ will probably
  remove a large portion of the graph.
If 
  we will argue that  satisfies condition
  \eqref{eqn:Q3} in .
Otherwise, will argue that  does.

\begin{lemma}\label{lem:ac5}
If
\begin{itemize}
\item [(a)] ,
\item [(b)] , and
\item [(c)] , 
\end{itemize}
 then

Moreover, if 
then

\end{lemma}
\begin{proof}
We first lower-bound the volume of the 
  intersection of  with  by

We then apply Claim~\ref{clm:ac0} to show

The last part of the lemma follows from 
   
  and .  
\end{proof}

\begin{lemma}\label{lem:ac6}
If
\begin{itemize}
\item [(a)]   and
\item [(b)] ,
\end{itemize}
 then

Moreover, if 
then

\end{lemma}
\begin{proof}
As 
   and
 ,

So, by Claim~\ref{clm:ac0},

The last part now follows from

and .
\end{proof}



\begin{proof}[Proof of Theorem~\ref{thm:ApproxCut}]
The proofs of (A.1) and (A.2) are similar to the proofs of (Q.1) and (Q.2).

To prove (A.3), 
  we will assume that for each set  that satisfies conditions
  \eqref{eqn:Q3} in  the call to \partition2 \ succeeds
 and that the same holds for all sets  that satisfy conditions
  \eqref{eqn:Q3} in .
As this assumption involves at most  sets, by Lemma~\ref{lem:partition2} 
  it holds with probability at least .

If there is an  for which ,
  then 
  and condition  is satisfied.
So, we assume that  for the rest of the proof.

Observe that the algorithm \approxcut \ calls \partition2 with 

and  that 

So, if  and

then  satisfies the conditions \eqref{eqn:Q3} in .

If there is an  for which ,
  then by Lemmas~\ref{lem:ac5} and~\ref{lem:ac6} either 
  or  satisfies conditions
  \eqref{eqn:Q3} in 
 and the success of the call to \partition2 implies


So, for the rest of the proof we may assume
  .
In this case we may show that

as follows.
If  then
  \eqref{eqn:ac} trivially holds.
Otherwise, Lemma~\ref{lem:ac4} tells us that 
  satisfies conditions \eqref{eqn:Q3} in  and
  that the success of the call to \partition2 guarantees \eqref{eqn:ac}.

We may now apply Lemma~\ref{lem:ac2} to show that  is empty if it
  is defined. 
So, there is an  for which  and by Claim~\ref{clm:uiExpander}  and
  Lemma~\ref{lem:ac3} 

as , the set 
  satisfies (A.3.b).
\end{proof}

\section{Sparsifying Unweighted Graphs}\label{sec:unweighted}
We now show how to use the algorithms \approxcut\ and \sample\
  to sparsify unweighted graphs.
More precisely, we treat every edge in an unweighted graph as an
  edge of weight .
The algorithm \unwtedsparsify \ 
  follows the outline described in Section~\ref{sec:existence}.
Its main subroutine \partsample \ calls \approxcut \ to partition the graph.
Whenever \approxcut \ returns a small cut, we know that the complement
  is contained in a subgraph of large conductance.
In this case, \partsample \ calls \sample \ to sparsify the large part.
Whenever the cut returned by \approxcut \ is large, \partsample \
  recursively acts on the cut and its complement so that it eventually
  partitions and samples both.
The output of \partsample \ is 
  the result of running \sample \ on the graphs
  induced on the 
  vertex sets of a decomposition of the original graph.
The main routine \unwtedsparsify \ calls \partsample \ and then
  acts recursively to sparsify the edges that go between the parts
  of the decomposition produced by \partsample .



\vskip 0.2in
\noindent
\fbox{
\begin{minipage}{6in}
\noindent 
\begin{enumerate}
\item [1.] If , return 
 (where  is set in the proof of Lemma~\ref{lem:partsample}).

\item [2.] Set ,
, and
 .

\item [3.] Set .

\item [4.] Let  be the vertex sets of
  , respectively, and let
   be the graph with vertex set  and edge set
  .

\item [5.] Set .

\item [6.] Set .
\end{enumerate}

\noindent 
\begin{enumerate}
\item [0.] Set , where  is defined in \eqref{eqn:f4}.
\item [1.] Set .
\item [2.] If , return
  .

\item [3.] Else, if 
\begin{itemize}
\item [a.] Set 
\item [b.] Return .
\end{itemize}

\item [4.] Else,
\begin{itemize}
\item [a.] Set .
\item [b.] Set .
\item [c.] Return .
\end{itemize}
\end{enumerate}
\end{minipage}
}
\vskip 0.2in


\begin{lemma}[\partsample]\label{lem:partsample}
Let  be a graph.
Let  be the output
  of .
Let  be the vertex sets of
  , respectively, and let
   be the graph with vertex set  and edge set
  .


Then,
\begin{itemize}
\item [(PS.1)]
.
\end{itemize}
With probability at least ,
\begin{itemize}
\item [(PS.2)]
  the graph

is a  approximation of , and
\item [(PS.3)]
the total number of edges in 
  is at most , for some 
  absolute constant .
\end{itemize}

\end{lemma}
\begin{proof}
We first observe that whenever the algorithm calls itself recursively, the volume
  of the graph in the recursive call is at most  of the volume of the
  input graph.
So, the recursion depth of the algorithm is at most .
Property  is a consequence of part  of Theorem~\ref{thm:ApproxCut}
  and this bound on the recursion depth.


We will assume for the rest of the analysis that
\begin{enumerate}
\item [1.] for every call to \sample \ in line 2,
   is a  approximation of  and the number
  of edges in  satisfies (S.2),

\item [2.] for every call to \sample \ in line 3a,
   is a
   approximation of   and the number
  of edges in  satisfies (S.2), and
\item [3.] For every call to \approxcut \ in line 1
  for which the set  returned satisfies ,
  there exists a set  containing  for which ,
  where  was defined in \eqref{eqn:f4}.
\end{enumerate}
First observe that at most  calls are made to \sample \ and \approxcut \
  during the course of the algorithm.
By Theorem~\ref{thm:ApproxCut}, the probability that assumption  fails is at
  most .
If assumption  never fails, 
  we may apply Theorem~\ref{thm:sampling} to prove that assumptions 1 and 2
  probably hold, as follows.
Consider a subgraph  on which \sample \ is called, using 
  if \sample \ is called on line 2.
Assumption 3 tells us that there is a set 
  for which .
Theorem~\ref{thm:cheeger} tells us that
  the smallest non-zero normalized Laplacian eigenvalue of  is at least
  , where  is set in line .
Treating  as the input graph, and , 
  we may apply Theorem~\ref{thm:sampling} to show that
  assumptions  and  fail with probability at most  each.
Thus, all three assumptions hold with probability at least .

Property , and the existence of the constant ,
   is a consequence of assumptions  and .
Using these assumptions, we will now establish  by induction on
  the depth of the recursion.
For a graph  on which \partsample \ is called, let  be the maximum
  depth of recursive calls of the algorithm on , let
  be output of \partsample \ on ,
  and let  be the vertex sets of
  , respectively.
We will prove by induction on  that




We base our induction on the case in which the algorithm does not call itself,
  in which case it returns
the output of \sample \ in line , and
  the assertion follows from
assumption 1.

Let  be the set of vertices returned by \approxcut.
If , then .
We first consider the case in which
  .
In this case, let
  ,
 let  
  be the graphs returned by
  the recursive call to  on ,
  and let  be the vertex sets of .
Let  be the graph on vertex set  with edges .
We may assume by way of induction that

is a -approximation of .
We then have

One may similarly prove

establishing \eqref{eqn:partsparse} for .

We now consider the case in which
  .
In this case, let
   and .
Let  be the vertex sets of 
  and let 
  be the vertex sets of .
By our inductive hypothesis, we may assume that

  is a -approximation of  and that

  is a -approximation of .
These two assumptions immediately imply that

is a -approximation of ,
  establishing \eqref{eqn:partsparse} in the second case.

As the recursion depth of this algorithm is bounded by
  , we have established property .
\end{proof}


\begin{lemma}[\unwtedsparsify]\label{lem:unwtsparsify}
For  and an unweighted graph  with  vertices, let
 be the output of .
Then,
\begin{itemize}
\item [(U.1)] The edges of  are a subset of the edges of ; and
\end{itemize}
 with probability at least ,
\begin{itemize}
\item [(U.2)]  is a -approximation of , and
\item [(U.3)]  has at most 
  edges, for some constant .
\end{itemize}
\end{lemma}
Moreover, the expected running time of \unwtedsparsify \ is 
  .

\begin{proof}
From , we know that the depth of the recursion of
  \unwtedsparsify \ on  is at most .
So, with probability at least

properties  and  hold for the output of
  \partsample \ every time it is called by \unwtedsparsify.
For the rest of the proof, we assume that this is the case.

Claim  follows immediately from  and the bound on the
  recursion depth of \unwtedsparsify .
We prove claim  by induction on the recursion depth.
In particular, we prove that if \unwtedsparsify \ makes  recursive
  calls to itself on graph , then the graph  returned is
  a  approximation of .
We base the induction in the case where \unwtedsparsify \ makes no
  recursive calls to itself, in which case it returns at line 1
  with a -approximation.

For , we assume for induction that  is a
  -approximation of .
By the assumption that  holds, we know that
  
  is a

approximation of , as 
  (here, we apply the inequality ).
By following the arithmetic in the proof of Lemma~\ref{lem:partsample},
  we may prove that

  is a  approximation of .

To finish, we observe that

for .

Claim  follows from the observation that the set of edges of
  the graph output by \sample \ is a subset of the set of edges of its input.

To bound the expected running time of \unwtedsparsify,
  observe that the bound on the recursion depth of
  \partsample \ implies that its expected running time is at most
   times the expected running time
  of \approxcut \ with ,
  plus the time required to make the calls
  to sample, which is at most .

Another multiplicative factor of  comes from the
  logarithmic number of times that \unwtedsparsify \ can call
  itself during the recursion.
\end{proof}

\section{Sparsifying Weighted Graphs}\label{sec:weighted}
In this section, we show how to sparsify graphs whose edges have
  arbitrary weights.
We begin by showing how to sparsify weighted graphs
  whose edge weights are integers in the range
  .
One may also think of this as sparsifying a multigraph.
This first result will follow simply from the algorithm for
  sparsifying unweighted graphs, at a cost of a  factor
   in the number of edges in the sparsifier.

We then explain the obstacle to sparsifying arbitrarily weighted graphs
  and how we overcome it.
We end the section by proving that it is possible to modify our construction
  of sparsifiers
  so that for every node the total blow-up in weight of the edges attached to
  it is bounded.

\subsection{Bounded Weights}

We recall that we treat an unweighted graph as a graph in which every edge has
  weight 1, and for clarity we often refer to such a graph as a
  \textit{weight-1 graph}.
Our algorithm for sparsifying graphs with weights in
   works by constructing
   weight-1 graphs  and then expressing
   as a sum of .
Each edge of  appears in the graphs  for which the th   
  bit of the binary expansion of the weight of the edge is .
We sparsify the graphs  independently, and then sum the results.

\vskip 0.2in
\noindent
\fbox{
\begin{minipage}{6in}
\noindent , 
  has integral weights in .
\begin{enumerate}
\item [1.] Decompose  as

where each  is a weight-1 graph.

\item [2.] For each , set .
\item [3.] Return .
\end{enumerate}
\end{minipage}
}
\vskip 0.2in

\begin{lemma}[\boundedsparsify]\label{lem:boundedsparsify}
For  and a graph  with integral weights
  and with  vertices, let
 be the output of .
Let  be the maximum weight of an edge in .
Then,
\begin{itemize}
\item [(B.1)] The edges of  are a subset of the edges of ; and,
\end{itemize}
with probability at least ,
\begin{itemize}
\item [(B.2)]  is a -approximation of , and
\item [(B.3)]  has at most 
  edges.
\end{itemize}
\end{lemma}
Moreover, the expected running time of \boundedsparsify  \ is 
  .
\begin{proof}
Immediate from Lemma~\ref{lem:unwtsparsify}.
\end{proof}


\subsection{Coping with Arbitrary Weights: Graph Contraction}\label{}

When faced with an arbitrary weighted graph, we will
  first approximate the weight of every edge by the sum of a
  few powers of two.
However, if the weights are arbitrary many different powers
  of two could be required, and we could not construct a sparsifier
  by treating each power of two separately as we did in
  \boundedsparsify .
To get around this problem, we observe that when we are considering
  edges of a given weight, we can assume that all edges of much greater
  weight have been contracted.
We formalize this idea in Lemma~\ref{lem:pullback}.

By exploiting this idea, we are able to
  sparsify arbitrary weighted graphs with at most a -factor
  more edges than employed in \boundedsparsify \ when .
Our technique is inspired by how Benczur and Karger~\cite{BenczurKarger}
  built cut sparsifiers for weighted graphs out of
  cut sparsifiers for unweighted graphs.


Given a weighted graph  and a partition  of V,
  we define the \textit{map} of the partition to be the function

for which  if .
We define the \textit{contraction} of  under  to be the weighted graph
  , where  consists of edges of the form
   for , and where the weight of edge
   is

We do not include self-loops in the contraction, so
  edges  for which  do not appear in the contraction.

Given a weighted graph ,
  we say that 
  is a \textit{pullback} of  under  if
\begin{itemize}
\item [1.]   is the contraction of  under , and
\item [2.]  for every edge
  ,  contains exactly one edge 
  for which  and .
\end{itemize}

In the following lemma, we consider a graph in which each of the vertex
  sets  are connected by edges of high weight while
  all the edges that go between these sets have low weight.
We show that one can sparsify the low-weight edges by taking a pullback
  of an approximation of the contraction of the graph.

\begin{lemma}[Pullback]\label{lem:pullback}
Let  be a weighted graph, let 
  be a partition of , and let  be the map of the partition.
Set ,
  ,
  , and
  .
For some  let
  be a pullback under  of a
  -approximation of the contraction of  under .
Assuming that ,
\begin{itemize}
\item [1.] each set of vertices  is connected by edges in ,
\item [2.] every edge in  has weight at least , and
\item [3.] every edge in  has weight 1.
\end{itemize}
Then,  is an -approximation of ,
  for

\end{lemma}

Our proof of Lemma~\ref{lem:pullback} uses the following lemma bounding
  how well a path preconditions an edge.
It is an example of a Poincar{\'e} inequality~\cite{DiaconisStrook},
  and it may be derived from  the
  Rank-One Support Lemma of~\cite{SupportTheory},
  the Congestion-Dilation Lemma of~\cite{SupportGraph},
  or the Path Lemma of~\cite{SpielmanTengLinsolve}.
We include a proof for convenience.
\begin{lemma}\label{lem:Path}
Let  be an edge of weight ,
  and let  consist of a path from  to  in which
  the edges on the path have weights
  .
Then,

\end{lemma}
\begin{proof}
Name the vertices on the path  through  with vertex 
  replacing  and vertex  replacing .
Let  denote the weight of edge .
We need to prove that for every vector ,

For  set .
The Cauchy-Schwarz inequality now tells us that

as required.
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lem:pullback}]
Let  be the contraction of  under ,
  and let  be the -approximation of 
  for which  is a pullback.

We begin the proof by choosing an arbitrary vertex  in each set .
Now, let  be the weighted
  graph on vertex set 
  isomorphic to  under the map ,
 and let  be the analogous graph for .
Our analysis will go through an examination of the graphs

The lemma is a consequence of the following three statements,
  which we will prove momentarily:
\begin{itemize}
\item [(a)]  is a -approximation of .
\item [(b)]  is a -approximation of .
\item [(c)]  is a -approximation of .
\end{itemize}
To prove claim (a), consider any edge .
As , the graph
   contains a path from
   to  and a path from  to .
The sum of the lengths of these paths is at most , and each
  edge on each path has weight at least .
So, if we let  denote an edge of weight  from
   to ,
  then Lemma~\ref{lem:Path} tells us that

and

As there are fewer than  edges in , we may
 sum \eqref{eqn:pullback1n} over all of them 
  to establish

So,

as .
The inequality

and thus part , may be established by similarly summing
  over inequality \eqref{eqn:pullback2}.

Part  is immediate from the facts that
    is a -approximation
  of , that  and .

Part  is very similar to part .
We first note that the sum of the weights of edges in 
  is at most  times the sum of the weights of edges
  in , and so is at most .
Now, for each edge  in  of weight , there is
  a corresponding edge  of weight 
  in .
Let  denote the edge  of weight  and let  denote
  the edge  of weight .
As in the proof of part , we have

and

Summing these inequalities over all edges in ,
  adding  to each side, and recalling
   and ,
  we establish part .
\end{proof}

We now state the algorithm \sparsify .
For simplicity of exposition, 
  we assume that the weights of edges in its input are all
  at most .
However, this is not a restriction as one can scale down
   the weights of any graph to satisfy this requirement, apply \sparsify ,
  and then scale back up.

The algorithm \sparsify \ first replaces each weight  with
  its truncation to its few most significant bits, .
The resulting modified graph is called .
As  is very close to , little is lost by this substitution.
As in \boundedsparsify ,  is represented as a sum of graphs 
   where each  is a weight-1 graph.
Because the weight of every edge in  only has a few bits,
  each edge only appears in a few of the graphs .

Our first instinct would be to sparsify each of the graphs  individually.
However, this could result in too many edges as sparsifying produces
  a graph whose number of edges is proportional to its number of vertices,
  and the sum over  of the number of vertices in each  could be large.
To get around this problem, we contract all edges of much higher weight
  before sparsifying.
In particular, the algorithm \sparsify \
  partitions the vertices into components that are connected by
  edges of much higher weight.
It then replaces each  with a pullback of a
  sparsifier of the contraction of  under this partition.
In Lemma~\ref{lem:sparsifyNumClusters} we prove that the sum over  of the
  number of vertices in the contraction of each  will only be
  a small multiple of .

\noindent
\fbox{
\begin{minipage}{6in}
\noindent ,
where  and  for all .
\begin{enumerate}

\item [0.] Set ,
  ,
  ,
  , and
  .


\item [1.] For each edge ,
\begin{enumerate}
\item [a.] choose
   so that ,
\item [b.] let  be the largest integer such that
  , (and note )
\item [c.] set .
\end{enumerate}

\item [2.] Let , and
  express

where in each graph  all edges have weight ,
  and each edge appears in at most 
  of these graphs.

\item [3.]
  Let  be the edge set of .
  Let .
  For each , let 
  be the connected components of  under .
For , set . 


\item [4.] For each  for which  is non-empty,
\begin{enumerate}
\item [a.] Let  be the set of vertices attached to edges
  in .

\item [b.]
  Let  be the sets of form
   that are non-empty
  and have an edge of  on their boundary,
  (that is, the interesting components of  after contracting edges
  in ).  Let .

\item [c.] Let  be the map of partition ,
  and let  be the contraction of  under .

\item [d.] .
\item [e.] Let  be a pullback of  under  whose
  edges are a subset of .

\end{enumerate}
\item [5.] Return .
\end{enumerate}
\end{minipage}
}
\vskip 0.125in


\begin{lemma}\label{lem:sparsifyNumClusters}
Let  denote the number of clusters described by \sparsify \
  at step 4b.
Then,

\end{lemma}
\begin{proof}
Let  denote the number of connected components
  in the graph .
Each cluster  has at least one edge of 
  leaving it.
As each pair of components under  that are joined by
  an edge of  appear in the same component under ,

As the number of clusters never goes negative and is initially at
  most , we may conclude

\end{proof}


\begin{theorem}[\sparsify]\label{thm:sparsify}
For ,  and a weighted graph 
  and with  vertices in which every edge has weight at most 1.
Let  be the output of .
\begin{itemize}
\item [(X.1)] The edges of  are a subset of the edges of ; and
\end{itemize}
with probability at least ,
\begin{itemize}
\item [(X.2)]  is a -approximation of , and
\item [(X.3)]  has at most
  
  edges, for some constant .
\end{itemize}
Moreover, the expected running time of \sparsify  \ is 
  .
\end{theorem}
\begin{proof}
To establish property , it suffices to show that step 4e can
  actually be implemented.
That is, we need to know that all edges in  can be pulled
  back to edges of .
This follows from  and the fact that  is a contraction
  of .

We now establish that the graph  is a -approximation
  of .
We will then spend the rest of the proof establishing that 
  approximates .
As the weight of every edge in  is less than the corresponding
  weight in , we have .
On the other hand, for every edge , ,
  so , and
   is a -approximation of .

From Lemma~\ref{lem:sparsifyNumClusters}, we know that there are at most
   values of  for which ,
  and so \boundedsparsify \ is called at most  times.
Thus, with probability at least , the output returned by every call
  to \boundedsparsify \ satisfies properties  and , and accordingly
  we will assume that these properties are satisfied for the rest of the proof.

As each edge set  has at most  edges, the weight
  of every edge in graph  is an integer between  and .
So, by property , the number of edges in  , and therefore
  in , is at most

Applying Lemma~\ref{lem:sparsifyNumClusters}, we may prove that the number of edges
  in  is at most

for some constant ,
  thereby establishing .

To establish ,
 define for \textit{every}  the weight-1 graph
  , and observe that

We may apply  and Lemma~\ref{lem:pullback} to show that

is a -approximation of .
Summing over  while multiplying the th term by , we conclude that

is a -approximation of

Setting

we have proved that 
is a -approximation of
,
  and by so
  Proposition~\ref{pro:sparsifyCalc} below,

is a

approximation of .
Property  now follows from the facts that
    is a -approximation of , and

for .

To bound the expected running time of \sparsify,
  we observe that the time of the computation is dominated
  by the calls to \boundedsparsify \ and the time
  required to actually form the graphs .
The sets  may be maintained using
  union-find~\cite{Tarjan}, and so incur a cost of at most 
  over the course of the algorithm.
Each graph  may be formed by determining the component
  of each of its edges, at a cost of .
So, the time to form the graphs  can be bounded by

This is dominated by our upper bound on
  the time required in the calls to \boundedsparsify,
  which is

\end{proof}

\begin{proposition}\label{pro:sparsifyCalc}
If  and
  is a -approximation of ,
  then
 is a -approximation of .
\end{proposition}
\begin{proof}
We have

which implies



On the other hand,

under the conditions .
\end{proof}


\subsection{Bounding Blow-Up}\label{ssec:blowup}
When we approximate a graph  by a graph
   with ,
  we define the \textit{blow-up} of an edge 
  by

Similarly, we define the blow-up of a vertex  to be

The algorithm in~\cite{SpielmanTengLinsolve} for solving linear equations
  requires sparsifiers in which every vertex has bounded blow-up.
While the sparsifiers output by \unwtedsparsify \ and \boundedsparsify \
  satisfy this condition with high probability,
  the sparsifiers output by \sparsify  \ do not.
The reason is that nodes of low degree can become part of clusters
   with many edges of  on their boundary.
These clusters can become vertices of high degree in the contraction by ,
  and so can become attached to edges of high blow-up when they are sparsified.

This problem may be solved by making two modifications to \sparsify .
First, we sub-divide the clusters  so all the vertices in each
  cluster have approximately the same degree, and so that
  the degree of every vertex in  is at most four times the degree
  of the vertices that map to it.
Then,
  we set  to be a \textit{random} pullback of 
  whose edges are a subset of .
That is, for each edge  we pull it back to a randomly
  chosen edge  for which  and .
In this way we may guarantee with high probability that no vertex
  has high blow-up.
We now describe the corresponding algorithm \sparsifytwo \ by just listing
  the lines that differ from \sparsify.


\vskip 0.2in
\noindent
\fbox{
\begin{minipage}{6in}
\noindent ,
where  has all edge-weights at most .
\begin{enumerate}

\item [4a.]
  Let  be the set of vertices in  with
  degrees in .
  Let  be the set of vertices attached to edges
  in .
  Let  be the set of vertices in .


\item [4b.] For each ,
  let  be the sets of form
   that are non-empty
  and have an edge of  on their boundary.
  Let .
  For each set
   that has
  more than 
  edges of  on its boundary,
  sub-divide the set until each part has between  and 
  edges on its boundary.
  [We will give a procedure to do the subdivision in the paragraph
immediately after this algorithm].
  Let 
  be the resulting collection of sets.

\item [4c.] Let  be the map of partition
  of  by the sets ,
  and let  be the contraction of  under .

\item [4e.] 
Let .
Let  be a random pullback of  under  whose
  edges are a subset of .

\end{enumerate}
\end{minipage}
}
\vskip 0.2in

We should establish that it is possible to sub-divide the clusters
  as claimed in step 4b.
To see this, recall that each vertex in a set 
  has degree at most .
So, if we greedily pull off vertices one by one to form a new set, each time we
  move a vertex the boundary of the new set will increase by at most
   and the boundary of the old set will decrease by at most
  .
Thus, at the point when the size of the boundary of the new set
  first exceeds , the size of the boundary of the old set must
  be at least .
So, one can perform the subdivision in step 4b by a naive greedy algorithm.

\begin{theorem}[\sparsifytwo]\label{thm:sparsifytwo}
For ,  and a weighted graph 
  with  vertices, let
 be the output of .
Then,
\begin{itemize}
\item [(Y.1)] the edges of  are a subset of the edges of ; and,
\end{itemize}
with probability at least ,
\begin{itemize}
\item [(Y.2)]  is a -approximation of , and
\item [(Y.3)]  has at most
  
  edges, for some constant ,
\item [(Y.4)] every vertex has blow-up at most .
\end{itemize}
Moreover, the expected running time of \sparsifytwo  \ is 
  .
\end{theorem}

\begin{proof}
To prove , we must bound the number of clusters,
  , produced
  in the modified step 4b.
From Lemma~\ref{lem:sparsifyNumClusters}, we know that

To bound , let  denote the set
  of edges in  leaving a set of vertices .
Let  be the set of  for which 
  was created by subdivision,
and recall that for all
  ,

So,

and

As vertices in  have at most  edges
  and
  each edge of  only appears in at most  sets ,

Combining \eqref{eqn:sparsify2a} with \eqref{eqn:sparsify2b}
  and \eqref{eqn:sumki}, we get

and so

for some constant .
By now applying the analysis from the proof of Theorem~\ref{thm:sparsify},
  we may prove that  and  hold with probability
  at least .
Of course, property  always holds.

To prove property ,
  we note that the blow-up of a vertex  is the sum
  of  times the the blow-up of each of its edges.
We prove in Lemma~\ref{lem:expectedBlowUp} that the expectation of this
  sum is , and in Lemma~\ref{lem:maxBlowUp} that each
  term is bounded by

If the variables were independent, we could apply Theorem~\ref{thm:chernoff}
  to prove it is unlikely that
   has blow-up greater than .

However, the variables are not independent.
The blow-up of edges output by \boundedsparsify \ are independent.
But, the choice of a random pullback at line 4e introduces correlations
  in the blow-up of edges.
Fortunately, the blow-up of edges attached to  have a negative association
  (as may be proved by Proposition~8 and Lemma~9 of
  Dubhashi and Ranjan~\cite{DubhashiRanjan}).
Thus, by Proposition~7 of~\cite{DubhashiRanjan}, we may still apply
  Theorem~\ref{thm:chernoff}, with  and  to show that the

Applying a union bound over the vertices , we see that 
  hold with probability at least .

The analysis of the running time of \sparsifytwo \ is similar to
  the analysis of \sparsify, except for the work required to sub-divide
  sets in step 4b, which we now analyze.
Each time a vertex is removed from a set 
  during the subdivision, the work required
  by a reasonable implementation is proportional to the degree
  of that vertex in graph .
So, the work required to perform all the subdivisions over the course
  of the algorithm is at most

As

whenever we subdivide , we have

Now,  by \eqref{eqn:sparsify2b}

Thus,

The stated bound on the expected running time of \sparsifytwo \ follows.
\end{proof}


\begin{lemma}\label{lem:expectedBlowUp}
Let 
  be the graph output by \sparsifytwo \ on input .
Then, for every ,
\end{lemma}
\vspace{-0.2in}


\begin{proof}
We first observe that

 holds for the graph  output
  by \sample \, as it takes a weight-1 graph as input, selects a
  probability  for each edge, and includes it at weight 
  with probability .
As \unwtedsparsify  \ merely partitions its input into edge-disjoint
  subgraphs and then applies \sample \ to some of them, \eqref{eqn:eqblowup}
  holds for the output of \unwtedsparsify \ as well.

To show that \eqref{eqn:eqblowup} holds for the graph output by
  \boundedsparsify \, for each edge  and for each  set

We have

For the graph  returned on line 2 of \boundedsparsify, let
  .
We have established that

So,

establishing \eqref{eqn:eqblowup} for the output of \boundedsparsify .

Applying similar reasoning, we may establish \eqref{eqn:blowup} for
  the output of \sparsifytwo \ by proving that for each edge  in
  each weight-1 graph ,
  the expected blow-up of  in  is at most .
If  is not on the boundary of a set ,
  then  will not appear in  and so its blow-up will be zero.
If  is on the boundary, then let  denote the number
  of edges  for which  and .
If we let  and ,
  then .

Now, let  be the edge  in .
We know that .
If  appears in , then the probability that edge  is chosen
  in the random pullback is .
As  has weight , we find

\end{proof}


\begin{lemma}\label{lem:maxBlowUp}
Let 
  be the graph output by \sparsifytwo \ on input .
Then, for every ,
\end{lemma}
\vspace{-0.2in}


\begin{proof}
As in the proof of the previous lemma, we work our way though the algorithms
  one-by-one.
The graph produced by the  algorithm \sample \
  has blow-up at most 
  for every edge .
As \unwtedsparsify \ only calls \sample \ on subgraphs of its input graph,
  a similar guaranteed holds for the output of \unwtedsparsify.
In fact, as \unwtedsparsify \ calls \sample \ with ,
  every edge output by \unwtedsparsify \ actually has blow-up less than

As \boundedsparsify \ merely calls \unwtedsparsify \ on a collection of
  graphs that sum to , the same bound holds on the blow-up of the
  graph output by \boundedsparsify  .

To bound the blow-up of edges in the graph output by \sparsifytwo  ,
  note that for every  and every vertex  in a graph ,
  the vertices  of the original graph that map to  under
   satisfy

where  refers to the degree of vertex  in the original graph
  and  is the degree of vertex  in graph .
So, the blow-up of every edge  satisfies


We now measure the blow-up of edges relative to  instead of
  , which can only over-estimate their blow-up.
The lemma then follows from


\end{proof}

\section{Final Remarks}\label{sec:conclusion}

Since the initial announcement \cite{SpielmanTengPrecon} 
  of our results, significant improvements 
  have been made in spectral sparsification. 
Spielman and Srivastava~\cite{SpielmanSrivastava} have
  proved that spectral sparsifiers with  edges
  exist, and may be found in time  where
   is the ratio of the largest weight to the smallest 
  weight of an edge in the input graph.
Their nearly-linear time algorithm relies upon the solution of a logarithmic
  number of linear systems in diagonally-dominant matrices.
Until recently, the only nearly-linear time algorithm for solving such systems
  was the algorithm in~\cite{SpielmanTengLinsolve}, which relied upon the constructions
  in this paper.
Recently, Koutis, Miller and Peng~\cite{KMP} have developed a faster
  algorithm that does not rely on the sparsifier construction of the present paper.
Their algorithm finds -approximate solutions to Laplacian linear systems
  in time .
One may remove the dependence on  in the running time of the
  algorithm of~\cite{SpielmanSrivastava} through the procedure
  described in Section~\ref{sec:weighted} of this paper.
Batson, Spielman and Srivastava~\cite{BatsonSpielmanSrivastava}
  have shown that sparsifiers with  edges exist, and
  present a polynomial-time algorithm that finds these sparsifiers.
It is our hope that sparsifiers with so few edges may also be found
  in nearly-linear time.

Andersen, Chung and Lang~\cite{AndersenChungLang} 
  and Andersen and Peres~\cite{AndersenPeres} have
  improved upon some of the core algorithms we presented in~\cite{SpielmanTengCuts}
  and in particular have improved upon the algorithm \partition \ upon which
  we based \approxcut.
The algorithm of Andersen and Peres~\cite{AndersenPeres} is both significantly
  faster and saves a factor of  in the conductance of the set it
  outputs.
In particular, it satisfies guarantee  with the term
   in place of our function .


\bibliographystyle{alpha}
\bibliography{precon}

\end{document}
