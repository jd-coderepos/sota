\documentclass{article}



\PassOptionsToPackage{numbers,square}{natbib}

\usepackage[final]{nips_2017}





\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{amsmath}
\usepackage{subfiles}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{subfiles}


\newcommand\blfootnote[1]{\begingroup
  \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}

\newcommand\todo[1]{\textcolor{red}{[[#1]]}}
\newcommand\mc[1]{\mathcal{#1}}
\newcommand\concat[3]{\left[#1 \parallel_#3 #2\right]}
\newcommand\cs[3]{ConvStep_{#1,#2}(#3)}
\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}


\title{Depthwise Separable Convolutions for Neural Machine Translation}



\author{
  {\L}ukasz Kaiser\thanks{All authors contributed equally and are ordered randomly.}\\
  Google Brain\\
  \texttt{lukaszkaiser@google.com} \\
  \And
  Aidan N. Gomez\samethanks[1]\hspace{1.7mm}\thanks{Work performed while at Google Brain.}\\
  University of Toronto\\
  \texttt{aidan@cs.toronto.edu} \\
  \And
  Fran\c{c}ois Chollet\samethanks[1]\\
  Google Brain\\
  \texttt{fchollet@google.com} \\
}

\begin{document}

\maketitle

\begin{abstract}
Depthwise separable convolutions reduce the number of parameters and computation used in convolutional operations while increasing representational efficiency.
They have been shown to be successful in image classification models, both in obtaining better models than previously possible for a given parameter count (the Xception architecture) and considerably reducing the number of parameters required to perform at a given level (the MobileNets family of architectures). Recently, convolutional sequence-to-sequence networks have been applied to machine translation tasks with good results. In this work, we study how depthwise separable convolutions can be applied to neural machine translation. We introduce a new architecture inspired by Xception and ByteNet, called SliceNet, which enables a significant reduction of the parameter count and amount of computation needed to obtain results like ByteNet, and, with a similar parameter count, achieves new state-of-the-art results.
In addition to showing that depthwise separable convolutions perform well for machine translation, we investigate the architectural changes that they enable: we observe that thanks to depthwise separability, we can increase the length of convolution windows, removing the need for filter dilation. We also introduce a new "super-separable" convolution operation that further reduces the number of parameters and computational cost for obtaining state-of-the-art results.
\blfootnote{Code available at \url{https://github.com/tensorflow/tensor2tensor}}
\end{abstract}

\section{Introduction}

In recent years, sequence-to-sequence recurrent neural networks (RNNs) with long short-term memory (LSTM) cells \citep{hochreiter1997}
have proven successful at many natural language processing (NLP) tasks, including machine translation
\citep{sutskever14,bahdanau2014neural,cho2014learning}. In fact, the results they yielded have been so good that
the gap between human translations and machine translations has narrowed significantly \cite{GNMT} and LSTM-based recurrent
neural networks have become standard in natural language processing.

Even more recently, auto-regressive convolutional models have proven highly effective when applied to audio \citep{wavenet2016}, image \citep{pixelcnn2016} and text generation \citep{bytenet2016}. Their success on sequence data in particular rivals or surpasses that of previous recurrent models \citep{bytenet2016,fbpaper}. Convolutions provide the means for efficient non-local referencing across time without the need for the fully sequential processing of RNNs. However, a major critique of such models is their computational complexity and large parameter count. These are the principal concerns addressed within this work: inspired by the efficiency of depthwise separable convolutions demonstrated in the domain of vision, in particular the Xception architecture \citep{xception2016} and MobileNets \citep{mobilenets2017}, we generalize these techniques and apply them to the language domain, with great success.


\section{Our contribution}

We present a new convolutional sequence-to-sequence architecture, dubbed SliceNet, and apply it to machine translation tasks, achieving state-of-the-art results. Our architecture features two key ideas:

\begin{itemize}
    \item Inspired by the Xception network \cite{xception2016}, our model is a stack of depthwise separable convolution layers with residual connections. Such an architecture has been previously shown to perform well for image classification. We also experimented with using grouped convolutions (or "sub-separable convolutions") and add even more separation with our new super-separable convolutions.

    \item We do away with filter dilation in our architecture, after exploring the trade-off between filter dilation and larger convolution windows. Filter dilation was previously a key component of successful 1D convolutional architectures for sequence-to-sequence tasks, such as ByteNet \citep{bytenet2016} and WaveNet \citep{wavenet2016}, but we obtain
    better results without dilation thanks to separability.

\end{itemize}


\subsection{Separable convolutions and grouped convolutions}

The depthwise separable convolution operation can be understood as related to both grouped convolutions and the "inception modules" used by the Inception family of convolutional network architectures, a connection explored in Xception \citep{xception2016}. It consists of a depthwise convolution, i.e. a spatial convolution performed independently over every channel of an input, followed by a pointwise convolution, i.e. a regular convolution with 1x1 windows, projecting the channels computed by the depthwise convolution onto a new channel space. The depthwise separable convolution operation should not be confused with spatially separable convolutions, which are also often called “separable convolutions” in the image processing community.

Their mathematical formulation is as follow (we use  to denote the element-wise product):



Thus, the fundamental idea behind depthwise separable convolutions is to replace the feature learning operated by regular convolutions over a joint "space-cross-channels realm" into two simpler steps, a spatial feature learning step, and a channel combination step. This is a powerful simplification under the oft-verified assumption that the 2D or 3D inputs that convolutions operate on will feature both fairly independent channels and highly correlated spatial locations.

A deep neural network forms a chain of differentiable feature learning modules, structured as a discrete set of units, each trained to learn a particular feature. These units are subsequently composed and combined, gradually learning higher and higher levels of feature abstraction with increasing depth. Of significance is the availability of dedicated feature pathways that are merged together later in the network; this is one property enabled by depthwise separable convolutions, which define independent feature pathways that are later merged. In contrast, regular convolutional layers break this creed by learning filters that must simultaneously perform the extraction of spatial features and their merger into channel dimensions; an inefficient and ineffective use of parameters.

Grouped convolutions (or "sub-separable convolutions") are an intermediary step between regular convolutions and depthwise separable convolutions. They consist in splitting the channels of an input into several non-overlapping segments (or "groups"), performing a regular spatial convolution over each segment independently, then concatenating the resulting feature maps along the channel axis.

Depthwise separable convolutions have been previously shown in Xception \citep{xception2016} to allow for image classification models that outperform similar architectures with the same number of parameters, by making more efficient use of the parameters available for representation learning. In MobileNets \citep{mobilenets2017}, depthwise separable convolutions allowed to create very small image classification models (e.g. 4.2M parameters for 1.0 MobileNet-224) that retained much of the capabilities of architectures that are far larger (e.g. 138M parameters for VGG16), again, by making more efficient use of parameters.

The theoretical justifications for replacing regular convolution with depthwise separable convolution, as well as the strong gains achieved in practice by such architectures, are a significant motivation for applying them to 1D sequence-to-sequence models.

The key gains from separability can be seen when comparing the number of parameters (which in this case corresponds to the computational cost too) of separable convolutions, group convolutions, and regular convolutions. Assume we have  channels and filters (often  or more) and a receptive field of size  (often  but we will use  upto ). The number of parameters for a regular convolution, separable convolution, and group convolution with  groups is:


\subsection{Super-separable convolutions}

As can be seen above, the size (and cost) of a separable convolution with  channels and a receptive field of size  is .
When  is small compared to  (as is usuallty the case) the term  dominates, which raises the question how it could be reduced.
We use the idea from group convolutions and the recent separable-LSTM paper \cite{factlstm} to further reduce this size by factoring the final  convolution, and we call the result a \emph{super-separable} convolution.

We define a super-separable convolution (noted ) with  groups as follows. Applied to a tensor , we first split  on the depth dimension into  groups, then apply a separable convolution to each group separately, and then concatenate the results on the depth dimension.

where  is  split on the depth axis and  for  are the parameters of each separable convolution. Since each  is of size  and each  is of size , the final size
of a super-separable convolution is .
Parameter counts (or computational budget per position) for all convolution types are summarized in Table~\ref{table:conv_compare}.

\begin{table}
  \centering
  \begin{tabular}{lc}
  \toprule
    Convolution type & Parameters and approximate floating point operations per position \\
  \midrule
    Non-separable       &  \\
    Fully-separable     &  \\
    -Sub-separable   &  \\
    -Super-separable &  \\
  \bottomrule
  \end{tabular} \ReLUSepConvhGB 
    LN(x) = \frac{G}{\sigma(x)}(x-\mu(x)) + B \qquad
    \sigma(x) = \sqrt{\frac{1}{h}\sum^h_i (x_i - \mu(x))^2} \qquad
    \mu(x) = \frac{1}{h}\sum^h_i x_i,
 \cs{d}{s}{W, x} = LN(SepConv_{d,s}(W, ReLU(x))). 
    hidden1(x) &= \cs{1}{1}{W^{3 \times 1}_{h1}, x}\\
    hidden2(x) &= x + \cs{1}{1}{W^{3 \times 1}_{h2}, hidden1(x)}\\
    hidden3(x) &= \cs{1}{1}{W^{15 \times 1}_{h3}, hidden2(x)}\\
    hidden4(x) &= x + \cs{8}{1}{W^{15 \times 1}_{h4}, hidden3(x)}\\
    ConvModule(x) &= \begin{cases}
                Dropout(hidden4(x), 0.5) & \text{during training}\\
                hidden4(x) & \text{otherwise}
              \end{cases}
kConvModules^ksource[m, depth]target[n, depth]
    Attend(source, target) = \frac{1}{\sqrt{depth}} \cdot Softmax(target \cdot source^{T}) \cdot source\\
[k, depth]
    timing{(t, 2d)}   &= \sin(t / 10000^{2d/depth}) \\
    timing{(t, 2d+1)} &= \cos(t / 10000^{2d/depth})

    attention1(x) &= \cs{1}{1}{W^{5 \times 1}_{a1}, x + timing}\\
    attention(source, target) &= Attend(source, \cs{4}{1}{W^{5 \times 1}_{a1}, attention1(target)})
abd^{\text{th}}\concat{a}{b}{d}
    mix_i &= IOMixer(InputEncoder(inputs),\\
                    &\qquad\qquad\qquad OutputEmbedding(outputs_{<i}))\\
    outputs &= Decoder(mix) \\
    \\
    InputEncoder(x) &= ConvModule^6(x + timing)\\
    IOMixer(i, o) &= \cs{1}{1}{W^{3 \times 1}, \concat{attention(i, o)}{o}{2}}\\
    AttnConvModule(x, source) &= ConvModule(x) + attention(source, x)\\
    Decoder(x) &= AttnConvModule^4(x, InputEncoder(inputs))\\
4mm]
  \caption{Performance on WMT EN-DE after 250k gradient descent steps.}\label{tab:ablations}
  \label{table:acc_and_size}
\end{table}

We evaluate all models on the WMT English to German translation task and use newstest2013 evaluation set for this purpose.
For two best large models, we also provide results on the standard test set, newstest2014, to compare with other works.
For tokenization, we use subword units, and follow the same tokenization process as Sennrich \cite{SennrichHB15}.
All of our experiments are implemented using the TensorFlow framework \cite{tensorflow}.
A comparison of our different models in terms of parameter count and Negative Log Perplexity
as well as per-token Accuracy on our task are provided in Table~\ref{table:acc_and_size}.
The parameter count (and computation cost) of the different types of convolution operations used
was already presented in Table~\ref{table:conv_compare}.
Our experimental results allow us to draw the following conclusions:

\begin{itemize}
    \item Depthwise separable convolutions are strictly superior to regular convolutions in a ByteNet-like architecture, resulting in models that are more accurate while requiring fewer parameters and being computationally cheaper to train and run.
    \item Using sub-separable convolutions with groups of size 16 instead of full depthwise separable convolutions results in a performance dip, which may indicate that higher separability (i.e. groups as small as possible, tending to full depthwise separable convolutions) is preferable in this setup, this further confirming the advantages of depthwise separable convolutions.
    \item The need for dilation can be completely removed by using correspondingly larger convolution windows, which is made computationally tractable by the use of depthwise separable convolutions.
    \item The newly-introduced super-separable convolution operation seems to offer an incremental performance improvement.
\end{itemize}

Finally, we run two larger models with a design based on the conclusions drawn from our first round of experiments: a SliceNet model which uses depthwise separable convolutions and a SliceNet model which uses super-separable convolutions, with significantly higher feature depth in both cases. We achieve state-of-the-art results, as shown in Table~\ref{table:large_models}, where we also include
previously reported results for comparison. For getting the BLEU, we used a beam-search decoder with a beam size of  and
a length penalty tuned on the evaluation set (newstest2013).

\begin{table}
  \centering
  \begin{tabular}{lclcccc}
    \toprule
    Model & BLEU (newstest14) \\
    \midrule
    SliceNet (Full, 2048)      & 25.5 \\
    SliceNet (Super 2/3, 3072) & {\bf 26.1} \\
    \midrule
    ByteNet  \cite{bytenet2016}         & 23.8 \\
    GNMT  \cite{GNMT}                   & 24.6 \\
    ConvS2S  \cite{fbpaper}             & 25.1 \\
    GNMT+Mixture of Experts  \cite{moe}  & 26.0 \\
  \bottomrule
  \end{tabular}\4mm]
  \caption{Performance of our larger models compared to
    best published results.}
  \label{table:large_models}
\end{table}


\subsection{Conclusions}

In this work, we introduced a new convolutional architecture for sequence-to-sequence tasks, called SliceNet, based on the use of depthwise separable convolutions. We showed how this architecture achieves results beating not only ByteNet but also the previous
state-of-the-art, while using over two times less (non-embedding) parameters and floating point operations than the ByteNet architecture.

Additionally, we have shown that filter dilation, previously thought to be a key component of successful convolutional sequence-to-sequence architectures, was not a requirement. The use of depthwise separable convolutions makes much larger convolution window sizes possible, and we found that we could achieve the best results by using larger windows instead of dilated filters. We have also introduced a new type of depthwise separable convolution, the super-separable convolution, which shows incremental performance improvements over depthwise separable convolutions.

Our work is one more point on a significant trendline started with Xception and MobileNets, that indicates that in any convolutional model, whether for 1D or 2D data, it is possible to replace convolutions with depthwise separable convolutions and obtain a model that is simultaneously cheaper to run, smaller, and performs a few percentage points better. This trend is backed by both solid theoretical foundations and strong experimental results. We expect our current work to play a significant role in affirming and accelerating this trend, and we hope to see depthwise separable convolutions replace regular convolutions for an increasing number of use cases in the future.

\begin{thebibliography}{24}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2015)Abadi, Agarwal, Barham, Brevdo, Chen, Citro,
  Corrado, Davis, Dean, Devin, Ghemawat, Goodfellow, Harp, Irving, Isard, Jia,
  Jozefowicz, Kaiser, Kudlur, Levenberg, Mané, Monga, Moore, Murray, Olah,
  Schuster, Shlens, Steiner, Sutskever, Talwar, Tucker, Vanhoucke, Vasudevan,
  Viégas, Vinyals, Warden, Wattenberg, Wicke, Yu, and Zheng]{tensorflow}
Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig
  Citro, Greg Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay
  Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard,
  Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
  Levenberg, Dan Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah,
  Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar,
  Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol
  Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang
  Zheng.
\newblock Tensorflow: Large-scale machine learning on heterogeneous distributed
  systems, 2015.
\newblock URL \url{http://download.tensorflow.org/paper/whitepaper2015.pdf}.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{layernorm2016}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Bahdanau et~al.(2014)Bahdanau, Cho, and Bengio]{bahdanau2014neural}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock \emph{CoRR}, abs/1409.0473, 2014.
\newblock URL \url{http://arxiv.org/abs/1409.0473}.

\bibitem[Cho et~al.(2014{\natexlab{a}})Cho, van Merrienboer, Bahdanau, and
  Bengio]{gru2014}
K.~Cho, B.~van Merrienboer, D.~Bahdanau, and Y.~Bengio.
\newblock On the properties of neural machine translation: Encoder-decoder
  approaches.
\newblock \emph{CoRR}, abs/1409.1259, 2014{\natexlab{a}}.

\bibitem[Cho et~al.(2014{\natexlab{b}})Cho, van Merrienboer, Gulcehre,
  Bougares, Schwenk, and Bengio]{cho2014learning}
Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger
  Schwenk, and Yoshua Bengio.
\newblock Learning phrase representations using rnn encoder-decoder for
  statistical machine translation.
\newblock \emph{CoRR}, abs/1406.1078, 2014{\natexlab{b}}.
\newblock URL \url{http://arxiv.org/abs/1406.1078}.

\bibitem[Chollet(2016)]{xception2016}
Fran{\c{c}}ois Chollet.
\newblock Xception: Deep learning with depthwise separable convolutions.
\newblock \emph{arXiv preprint arXiv:1610.02357}, 2016.

\bibitem[Gehring et~al.(2017)Gehring, Auli, Grangier, Yarats, and
  Dauphin]{fbpaper}
Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann~N Dauphin.
\newblock Convolutional sequence to sequence learning.
\newblock \emph{arXiv preprint arXiv:1705.03122}, 2017.

\bibitem[Hochreiter and Schmidhuber(1997)]{hochreiter1997}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Howard et~al.(2017)Howard, Zhu, Chen, Kalenichenko, Wang, Weyand,
  Andreetto, and Adam]{mobilenets2017}
Andrew~G Howard, Menglong Zhu, Bo~Chen, Dmitry Kalenichenko, Weijun Wang,
  Tobias Weyand, Marco Andreetto, and Hartwig Adam.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision
  applications.
\newblock \emph{arXiv preprint arXiv:1704.04861}, 2017.

\bibitem[Kalchbrenner and Blunsom(2013)]{KalchbrennerB13}
Nal Kalchbrenner and Phil Blunsom.
\newblock Recurrent continuous translation models.
\newblock In \emph{Proceedings {EMNLP} 2013}, pages 1700--1709, 2013.
\newblock URL \url{http://nal.co/papers/KalchbrennerBlunsom_EMNLP13}.

\bibitem[Kalchbrenner et~al.(2016)Kalchbrenner, Espeholt, Simonyan, Oord,
  Graves, and Kavukcuoglu]{bytenet2016}
Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van~den Oord, Alex
  Graves, and Koray Kavukcuoglu.
\newblock Neural machine translation in linear time.
\newblock \emph{arXiv preprint arXiv:1610.10099}, 2016.

\bibitem[Kuchaiev and Ginsburg(2017)]{factlstm}
Oleksii Kuchaiev and Boris Ginsburg.
\newblock Factorization tricks for {LSTM} networks.
\newblock \emph{CoRR}, abs/1703.10722, 2017.
\newblock URL \url{http://arxiv.org/abs/1703.10722}.

\bibitem[{\L}ukasz~Kaiser(2016)]{extendedngpu}
Samy~Bengio {\L}ukasz~Kaiser.
\newblock Can active memory replace attention?
\newblock In \emph{Advances in Neural Information Processing Systems,
  ({NIPS})}, 2016.

\bibitem[Meng et~al.(2015)Meng, Lu, Wang, Li, Jiang, and Liu]{MengLWLJL15}
Fandong Meng, Zhengdong Lu, Mingxuan Wang, Hang Li, Wenbin Jiang, and Qun Liu.
\newblock Encoding source language with convolutional neural network for
  machine translation.
\newblock In \emph{ACL}, pages 20--30, 2015.

\bibitem[Odena et~al.(2016)Odena, Dumoulin, and Olah]{odena2016deconvolution}
Augustus Odena, Vincent Dumoulin, and Chris Olah.
\newblock Deconvolution and checkerboard artifacts.
\newblock \emph{Distill}, 1\penalty0 (10):\penalty0 e3, 2016.

\bibitem[Sennrich et~al.(2015)Sennrich, Haddow, and Birch]{SennrichHB15}
Rico Sennrich, Barry Haddow, and Alexandra Birch.
\newblock Neural machine translation of rare words with subword units.
\newblock \emph{CoRR}, 2015.

\bibitem[Shazeer et~al.(2017)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton,
  and Dean]{moe}
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le,
  Geoffrey Hinton, and Jeff Dean.
\newblock Outrageously large neural networks: The sparsely-gated
  mixture-of-experts layer.
\newblock \emph{arXiv preprint arXiv:1701.06538}, 2017.

\bibitem[Sifre and Mallat(2013)]{sifre2013}
Laurent Sifre and St{\'{e}}phane Mallat.
\newblock Rotation, scaling and deformation invariant scattering for texture
  discrimination.
\newblock In \emph{2013 {IEEE} Conference on Computer Vision and Pattern
  Recognition, Portland, OR, USA, June 23-28, 2013}, pages 1233--1240, 2013.

\bibitem[Sutskever et~al.(2014)Sutskever, Vinyals, and Le]{sutskever14}
Ilya Sutskever, Oriol Vinyals, and Quoc~VV Le.
\newblock Sequence to sequence learning with neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3104--3112, 2014.
\newblock URL \url{http://arxiv.org/abs/1409.3215}.

\bibitem[van~den Oord et~al.(2016{\natexlab{a}})van~den Oord, Dieleman, Zen,
  Simonyan, Vinyals, Graves, Kalchbrenner, Senior, and
  Kavukcuoglu]{wavenet2016}
A{\"a}ron van~den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol
  Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu.
\newblock Wavenet: A generative model for raw audio.
\newblock \emph{CoRR abs/1609.03499}, 2016{\natexlab{a}}.

\bibitem[van~den Oord et~al.(2016{\natexlab{b}})van~den Oord, Kalchbrenner,
  Espeholt, Vinyals, Graves, et~al.]{pixelcnn2016}
Aaron van~den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex
  Graves, et~al.
\newblock Conditional image generation with pixelcnn decoders.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4790--4798, 2016{\natexlab{b}}.

\bibitem[Vanhoucke(2014)]{vanhoucke-iclr14}
Vincent Vanhoucke.
\newblock Learning visual representations at scale.
\newblock ICLR, 2014.

\bibitem[Wu et~al.(2016)Wu, Schuster, Chen, Le, Norouzi, Macherey, Krikun, Cao,
  Gao, Macherey, Klingner, Shah, Johnson, Liu, Kaiser, Gouws, Kato, Kudo,
  Kazawa, Stevens, Kurian, Patil, Wang, Young, Smith, Riesa, Rudnick, Vinyals,
  Corrado, Hughes, and Dean]{GNMT}
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc~V. Le, Mohammad Norouzi, Wolfgang
  Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner,
  Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws,
  Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian,
  Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick,
  Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean.
\newblock Google's neural machine translation system: Bridging the gap between
  human and machine translation.
\newblock \emph{CoRR}, abs/1609.08144, 2016.
\newblock URL \url{http://arxiv.org/abs/1609.08144}.

\bibitem[Yu and Koltun(2015)]{dilatedconv2015}
Fisher Yu and Vladlen Koltun.
\newblock Multi-scale context aggregation by dilated convolutions.
\newblock \emph{arXiv preprint arXiv:1511.07122}, 2015.

\end{thebibliography}


\end{document}
