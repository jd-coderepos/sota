\documentclass{sig-alternate}

\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{alltt, amssymb}



\def\Q {\ensuremath{\mathbb{Q}}}
\def\N {\mathbb{N}}
\def\R {\ensuremath{\mathbb{R}}}
\def\Z {\ensuremath{\mathbb{Z}}}
\def\F {\ensuremath{\mathbf{F}}}
\def\H {\ensuremath{\mathbb{H}}}
\def\K {\ensuremath{\mathbb{K}}}
\def\Kbar {\ensuremath{\overline{\mathbb{K}}}}
\def\A {\ensuremath{\mathsf{A}}}

\def\M{\ensuremath{\mathsf{M}}}
\def\T{\ensuremath{\mathsf{T}}}

\def\car {\ensuremath{{\rm char}}}
\def\val {\ensuremath{{\rm val}}}
\def\ct {\ensuremath{{\rm lc}}}
\def\tt {\ensuremath{{\rm lt}}}
\def\op {\ensuremath{\mathbf{O}}}
\def\o {\ensuremath{\mathsf{o}}}

\def\mymod {\ensuremath{\mathsf{mod}}}

\def\dom {\ensuremath{\mathsf{dom}}}
\def\g {\ensuremath{\mathsf{G}}}
\def\diag{\ensuremath{\mathsf{d}}}
\def\eiag{\ensuremath{\mathsf{e}}}
\def\fiag{\ensuremath{\mathsf{f}}}

\def\Lg {\ensuremath{\mathsf{Log}}}
\def\lg {\ensuremath{\mathsf{L}}}

\def\Xp {\ensuremath{\mathsf{Exp}}}
\def\xp {\ensuremath{\mathsf{E}}}

\def\shift {\ensuremath{\mathsf{A}}}
\def\Shift {\ensuremath{\mathsf{Shift}}}

\def\scale {\ensuremath{\mathsf{M}}}
\def\Scale {\ensuremath{\mathsf{Scale}}}

\def\power {\ensuremath{\mathsf{P}}}
\def\Power {\ensuremath{\mathsf{Power}}}

\def\root {\ensuremath{\mathsf{R}}}
\def\Root {\ensuremath{\mathsf{Split}}}

\def\Split {\ensuremath{\mathsf{Split}}}
\def\LinComb {\ensuremath{\mathsf{Comb}}}            
\def\Diag{\ensuremath{\mathsf{\Delta}}}

\def\Rev {\ensuremath{\mathsf{Rev}}}
\def\inv {\ensuremath{\mathsf{Inv}}}

\def\mul {\ensuremath{\mathsf{Mul}}}
\def\Eval {\ensuremath{\mathsf{Eval}}}

\def\ZZ {\ensuremath{\mathbb{Z}}}
\def\J {\ensuremath{\mathbb{J}}}
\def\CC {\ensuremath{\mathbb{M}}}
\def\C {\ensuremath{\mathbb{C}}}
\def\V {\ensuremath{\mathbb{V}}}
\def\W {\ensuremath{\mathbb{W}}}
\def\B {\ensuremath{\mathbb{B}}}
\def\D {\ensuremath{\mathbb{D}}}
\def\L {\ensuremath{\mathbb{L}}}
\def\U {\ensuremath{\mathbb{U}}}
\def\I {\ensuremath{\mathbb{I}}}

\def\myproof{\noindent{\sc Proof.}~}
\def\foorp{\hfill}

\def\gathen#1{{#1}}
\def\hoeven#1{{#1}}

\newtheorem{Def}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{Coro}{Corollary}
\newtheorem{Prop}{Proposition}
\newtheorem{PropDef}{Proposition - Definition}
\newtheorem{Lemma}{Lemma}
\newtheorem{Hyp}{Assumption}

\title{Power Series Composition and Change of Basis}
\numberofauthors{3}
\author{
\alignauthor Alin Bostan\\
\affaddr{Algorithms Project}\\
\affaddr{INRIA Rocquencourt}\\ 
\affaddr{France}\\
\affaddr{Alin.Bostan@inria.fr} 
\alignauthor Bruno Salvy\\
\affaddr{Algorithms Project}\\
\affaddr{INRIA Rocquencourt}\\ 
\affaddr{France}\\
\affaddr{Bruno.Salvy@inria.fr} 
\alignauthor {\'E}ric Schost\\
\affaddr{ORCCA and CSD}\\
\affaddr{University of Western Ontario}\\
\affaddr{London, ON, Canada}\\
\affaddr{eschost@uwo.ca} 
}

\begin{document}

\conferenceinfo{ISSAC'08,} {July 20--23, 2008, Hagenberg, Austria.}  
\CopyrightYear{2008} 
\crdata{978-1-59593-904-3/08/07}

\maketitle
\begin{abstract}
Efficient algorithms are known for many operations on truncated power
series (multiplication, powering, exponential, \dots). Composition is
a more complex task. We isolate a large class of power series for
which composition can be performed efficiently. We deduce fast
algorithms for converting polynomials between various bases, including
Euler, Bernoulli, Fibonacci, and the orthogonal Laguerre, Hermite,
Jacobi, Krawtchouk, Meixner and Meixner-Pollaczek.
\end{abstract}



\vspace{1mm}
 \noindent
 {\bf Categories and Subject Descriptors:} \\
\noindent I.1.2 [{\bf Computing Methodologies}]:{~} Symbolic and Algebraic
  Manipulation -- \emph{Algebraic Algorithms}
 
 \vspace{1mm}
 \noindent
 {\bf General Terms:} Algorithms, Theory
 
 \vspace{1mm}
 \noindent
 {\bf Keywords:} Fast algorithms, transposed algorithms, basis conversion, orthogonal polynomials.




\section{Introduction}\label{sec:intro}

Through the Fast Fourier Transform, fast polynomial multiplication has
been the key to devising efficient algorithms for polynomials and
power series. Using techniques such as Newton iteration or
divide-and-conquer, many problems have received satisfactory
solutions: polynomial evaluation and interpolation, power series
exponentiation, logarithm, \dots~can be performed in quasi-linear
time.

In this article, we discuss two questions for which such fast
algorithms are not known: power series composition and change of basis
for polynomials. We isolate special cases, including most
common families of orthogonal polynomials, for which our algorithms
reach quasi-optimal complexity.

\smallskip\noindent{\bf Composition.} Given a power series  with
coefficients in a field , we first consider the map of
evaluation at 

Here,  is the -dimensional -vector space of
polynomials of degree less than . We note~ for 
.

To study this problem, as usual, we denote by  a
\emph{multiplication time} function, such that polynomials of degree
less than  can be multiplied in  operations in~. We
impose the usual super-linearity conditions
of~\cite[Chap.~8]{GaGe99}. Using Fast Fourier Transform algorithms,
 can be taken in  over fields with suitable roots
of unity, and  in
general~\cite{ScSt71,CaKa91}.

If , the best known algorithm, due to Brent and Kung,
uses~ operations in~~\cite{BrKu78}; in
small characteristic, a quasi-linear algorithm is
known~\cite{Bernstein98b}. There are however special cases of power
series~ with faster algorithms: evaluation at  takes
linear time; evaluation at  requires \emph{no} arithmetic
operation. A non-trivial example is , which takes time
 when the base field has characteristic zero or large
enough~\cite{AhStUl75}. Brent and Kung~\cite{BrKu78} also showed how
to obtain a cost in  when  is a polynomial; this
was extended by van der Hoeven~\cite{Hoeven02} to the case where 
is algebraic over . In \S\ref{sec:compose}, we prove that
evaluation at  and at  can also be performed
in  operations over fields of characteristic zero or
larger than~.

Using associativity of composition and the linearity of the
map~, we show in \S\ref{sec:compose} how to use these
special cases as building blocks, to obtain fast evaluation algorithms
for a large class of power series.  This idea was first used by
Pan~\cite{Pan98}, who applied it to functions of the form
. Our extensions cover further examples
such as~ or , for which we improve 
the previously known costs.

\smallskip\noindent{\bf Bivariate problems.}  Our results on the cost
of evaluation (and of the transposed operation) are applied in
\S\ref{sec:basis} to special cases of a more general composition,
reminiscent of umbral operations~\cite{Roman05}. Given a
\emph{bivariate} power series , we
consider the linear map

For instance, with 
this is the map~ seen before.  For general , the
conversion takes quadratic time (one needs  coefficients for
). Hence, better algorithms can only been found for structured
cases; in \S\ref{sec:basis}, we isolate a large family of bivariate
series~ for which we can provide such fast algorithms.
This approach follows Frumkin's~\cite{Frumkin95}, which was specific
to Legendre polynomials.

\smallskip\noindent{\bf Change of basis.} Our framework captures in
particular the generating series of many classical polynomial
families, for which it yields at once conversion algorithms between
the monomial and polynomial bases, in both directions.

Thus, we obtain in \S\ref{sec:applications} change of basis
algorithms of cost only  for all of Jacobi, Laguerre and
Hermite orthogonal polynomials, as well as Euler, Bernoulli, and Mott
polynomials (see Table~\ref{Fig:M1}). These algorithms are derived
in a uniform manner from our composition algorithms; they improve upon
the existing results, of cost  or  at best (see below for historical comments).

We also obtain  conversion algorithms for a large
class of Sheffer sequences~\cite[Chap.~2]{Roman05}, including
actuarial polynomials, Poisson-Charlier polynomials and Meix\-ner
polynomials (see Table~\ref{Fig:M2}).

\smallskip\noindent{\bf Transposition.} A key aspect of our results is
their heavy use of transposed algorithms. Introduced under this name
by Kaltofen and Shoup, the \emph{transposition principle} is an
algorithmic theorem with the following content: given an algorithm
that performs an  matrix-vector product ,
one can deduce an algorithm with the same complexity, up to 
operations, and that performs the transposed matrix-vector product . In other words, this relates the cost of computing a
-linear map  to that of computing the transposed map
.

For the transposition principle to apply, some restrictions must be
imposed on the computational model: we require that only linear
operations in the coefficients of  are performed (all our
algorithms satisfy this assumption). See~\cite{BuClSh97} for a precise
statement, Kaltofen's ``open problem''~\cite{Kaltofen00} for further
comments and~\cite{BoLeSc03} for a systematic review of some classical
algorithms from this viewpoint.

To make the design of transposed algorithms transparent, we choose as
much as possible to describe our algorithms in a ``functional''
manner. Most of our questions boil down to computing linear maps
; expressing algorithms as a factorization of
these maps into simpler ones makes their transposition
straightforward. In particular, this leads us to systematically
indicate the dimensions of the source (and often target) space as
a subscript.

\smallskip\noindent{\bf Previous work.} The question of efficient
change of basis has naturally attracted a lot of attention, so that
fast algorithms are already known in many cases.

Gerhard~\cite{Gerhard00} provides  conversion
algori\-thms between the falling factorial basis and the monomial basis:
we recover this as a special case.  The general case of Newton
interpolation is discussed in~\cite[p. 67]{BiPa94} and
developed in~\cite{BoSc05}. The algorithms have cost  as well.

More generally, if  is a sequence of polynomials satisfying a
recurrence relation of fixed order (such as an orthogonal family), the
conversion from  to the monomial basis  can also be
computed in  operations: an algorithm is given
in~\cite{PoStTa98}, and an algorithm for the transpose problem is
in~\cite{DrHeRo97}. Both operate on real or complex arguments, but the
ideas extend to more general situations.  Alternative algorithms,
based on structured matrices techniques, are given
in~\cite{Heinig01}. They perform conversions in both directions in
cost .

The overlap with our results is only partial: not all families
satisfying a fixed order recurrence relation fit in our framework;
conversely, our method applies to families which do not necessarily
satisfy such recurrences (the work-in-progress~\cite{BoSaSc08}
specifically addresses conversion algorithms for orthogonal polynomials).

Besides, special algorithms are known for converting between
particular families, such as Chebyshev, Legendre and
B\'ezier~\cite{LiZh98,BaPe04}, with however a quadratic
cost. Floating-point algorithms are known as well, of cost~ for
conversion from Legendre to Chebyshev bases~\cite{AlRo91} and  for conversions between Gegenbauer bases~\cite{Keiner07},
but the results are approximate. Approximate conversions for the
Hermite basis are discussed in~\cite{LeRoCh07}, with cost .

\smallskip\noindent{\bf Note on the base field.} For the sake of
simplicity, in all that follows, the base field is supposed to have
characteristic~0. All results actually hold more generally, for fields
whose characteristic is sufficiently large with respect to the target
precision of the computation. However, completely explicit estimates
would make our statements cumbersome.



\section{Composition}\label{sec:compose}
Associativity of composition can be read both ways: in the identity
,  is either composed on the left of~
or on the right of~.  In this section, we discuss the consequences
of this remark.  We first isolate a class of operators~ for which
both left and right composition can be computed fast. Most results are
known; we introduce two new ones, regarding exponentials and logarithms.
Using these as building blocks, we then define \emph{composition
sequences}, which enable us to obtain more complex functions by
iterated compositions. We finally discuss the cost of the map
 and of its inverse for such functions, showing how to
reduce it to  or .



\subsection{Basic Subroutines}\label{ssec:basic}
We now describe a few basic subroutines that are the building
blocks in the rest of this article.
   
\smallskip\noindent{\bf Left operations on power series.}  In
Table~\ref{tab:leftcomp}, we list basic composition operators,
defined on various subsets of .
Explicitly, any such operator  is defined on a \emph{domain}
, given in the third column. Its action on a power
series~ is given in the second column, and the cost of
computing~ is given in the last column. 

\begin{table}[!!!h]
\centering
\begin{tabular}{l@{\hspace{-0em}}c@{\hspace{-0em}}c@{\hspace{0em}}c}
{\hspace{-0.8em}}Operator&Action&Domain&Cost\\
\hline
{\hspace{-0.8em}} (add)&&&1\\
{\hspace{-0.8em}} (mul)&&&\\
{\hspace{-0.8em}} (power)&&&\hspace{-1ex}\\  
{\hspace{-0.8em}} (root)&&{~}&\\
{\hspace{-0.8em}} (inverse)&&&\\
{\hspace{-0.8em}} (exp.)&{~}&&\\
{\hspace{-0.8em}} (log.)&&&\\       
\hline
\end{tabular}
\caption{Basic Operations on Power Series\label{tab:leftcomp}}
\end{table}

Some comments are in order. For addition and multiplication, we take
 and  in . To lift indeterminacies, the value
of  is defined as the unique power series with
leading term~ whose~th power is~; observe that to
compute , we need  modulo
 as input. Finally, we choose to subtract~1 to the
exponential so as to make it the inverse of the logarithm.
All complexity results are known; they are obtained by Newton
iteration~\cite{Brent75}.

\smallskip\noindent{\bf Right operations on polynomials.}  In
Table~\ref{tab:rightcomp}, we describe a few basic linear maps
on~ (observe that the dimension  of the source is
mentioned as a subscript). Their action on a polynomial

is described in the third column. In the case of powering, it is
assumed that~. Here and in what follows, we  freely
identify  and , through the isomorphism


\begin{table}[!!!t]
\centering
\begin{tabular}{l@{\hspace{0.5em}}c@{\hspace{0.5em}}c@{\hspace{0.5em}}c}
{\hspace{-0.8em}}Name&Notation&Action&Cost\\ 
\hline
{\hspace{-0.8em}}Powering&&&0\\
{\hspace{-0.8em}}Reversal&&&0\\
{\hspace{-0.8em}}Mod&&&0\\
{\hspace{-0.8em}}Scale&&&\\
{\hspace{-0.8em}}Diagonal&&&\\
{\hspace{-0.8em}}Multiply&~~&~&\\
{\hspace{-0.8em}}Shift&&&~\\
\hline
\end{tabular}
\caption{Basic Operations on Polynomials\label{tab:rightcomp}}
\vspace{-2ex}
\end{table} 

\noindent All of the cost estimates are straightforward, except for
the shift, which, in characteristic~0, can be deduced from the other
ones by the factorization~\cite{AhStUl75}:

where  is the polynomial .
We continue with some equally simple operators, whose description
however requires some more detail. For , any polynomial 
in  can be uniquely written as

Inspecting degrees, one sees that if  is in , then  is in , with    

This leads us to define the map  
 It uses no arithmetic operation.
We also use linear combination with polynomial coefficients.
Given polynomials  in , we
denote by
 the map sending
 to

It can be computed in~ operations.
Finally, we extend our set of subroutines on polynomials with the
following new results on the evaluation at  and .
\begin{Prop}\label{Prop:xp} The maps

can be computed in  arithmetic operations.  
\end{Prop}
\myproof We start by truncating  modulo , since
 After shifting by , we
are left with the question of evaluating a polynomial in  at
. Writing its matrix shows that this map factors
as  where 
is the map
 To summarize,
we have obtained that
 Using fast
transposed evaluation~\cite{CaKaLa89,BoLeSc03},  can
thus be computed in  operations.  Inverting these
computations leads to the factorization
 where 
is interpolation at . Using algorithms for transpose
interpolation~\cite{KaLa88,BoLeSc03}, this operation can be done in
time .  \foorp



\subsection{Associativity Rules}\label{ssec:ar}

For each basic power series operation in Table~\ref{tab:leftcomp}, we
now express  in terms of simpler operations; we
call these descriptions \emph{associativity rules}. We write them in a
formal manner: this formalism is the key to automatically design
complex composition algorithms, and makes it straightforward to obtain
\emph{transposed} associativity rules, required in the next section.
Most of these rules are straightforward; care has to be taken
regarding truncation, though.

\smallskip\noindent {\bf Scaling, Shift and Powering.}
\numberwithin{equation}{section}




\noindent {\bf Inversion.}  From  and writing , we get



\smallskip\noindent {\bf Root taking.} For  and  in ,
if , one has . We deduce the following rule, where the
indices~ are defined in Equation~\eqref{eq:ni}.
1mm]
A_0,\dots,A_{k-1} = \Root_{m,k}(A)\notag\
\vskip-4mm


\noindent {\bf Exponential and Logarithm.}






\subsection{Composition sequences}\label{ssec:comp}
We now describe more complex evaluations schemes, obtained by
composing the former basic ones. 
\begin{Def}Let  be the set of actions
from Table~\ref{tab:leftcomp}. A sequence  with
entries in  is \emph{defined at a series}  if  is
in , and for ,  is in
. It is a \emph{composition sequence} if it is defined at
; in this case,  \emph{computes} the power series
, with  and ; it
\emph{outputs} .
\end{Def}

\smallskip\noindent{\bf Examples.}  As mentioned in~\cite{Pan98}, the
rational series , with ,
decomposes as
 This shows that  is output by the
composition sequence .
A more complex example is 

which shows that  is output by the composition
sequence

Finally, consider 
. Using 

we get the composition sequence


\smallskip\noindent{\bf Computing the associated power series.} Our
main algorithm requires truncations of the series 
associated to a composition sequence. The next lemma discusses the
cost of their computation. In all complexity estimates, \emph{the
composition sequence  is fixed}; hence, our estimates hide a
dependency in  in their constant factors.

\begin{Lemma}\label{Lemma:0}
  If  is a composition sequence that computes
  power series , one can compute all  in
  time .
\end{Lemma}
\myproof All operators in  preserve the precision, except for
root-taking, since the operator  loses 
terms of precision. For , define  if
 has the form , 
otherwise, and define  and inductively . Starting the computations with , we iteratively
compute  from .

Inspecting the list of possible cases, one sees that computing 
always takes time . For powering, this estimate is
valid because we disregard the dependency in : otherwise, terms of
the form  would appear. For the same reason, 
is in , as is the total cost, obtained by summing over all
. \foorp

\smallskip\noindent{\bf Composition using composition sequences.}  We
now study the cost of computing the map , assuming that
 is output by a composition sequence . The cost
depends on the operations in . To keep simple expressions, we
distinguish two cases: if  contains no operation
 or , we let ; otherwise, .

\begin{theorem}[Composition]\label{Prop:0}
 Let  be a composition sequence that outputs a
 series . Given , one can compute the map
  in time .
\end{theorem}
\myproof We follow the algorithm of Figure~\ref{Fig:1}. The main
function first computes the sequence  modulo ,
using a subroutine  that follows
Lemma~\ref{Lemma:0}. The cost  of this operation is in
. Then, we call the auxiliary 
function.

\begin{figure}[t]
\begin{center}
\fbox{
\begin{minipage}{5 cm}
\begin{tabbing}
\= \quad\quad\quad\quad\quad\quad\= \quad \= \quad \kill
\1mm]
\>  \\
\> {\sf return} \Eval{\sf \_aux}
\end{tabbing}
\end{minipage}
}
\end{center}
\caption{Algorithm .}
\label{Fig:1}
\end{figure}

On input , this latter function computes
. This is done recursively, applying the
appropriate associativity rule~\eqref{AR:1} to~\eqref{AR:7}; the
pseudo-code uses a C-like \texttt{switch} construct to find the
matching case. Even if the initial polynomial  is in ,
this may not be the case for the arguments passed to the next calls;
hence the need for the extra parameter . For root-taking, the
subroutine \textsf{FindDegrees} computes the quantities  of
Eq.~\eqref{eq:ni}.

Since we write the complexity as a function of , the cost analysis
is simple: even if several recursive calls are generated ( for
th root-taking), their total number is still . Similarly, the
degree of the argument  passed through the recursive calls may
grow, but only like . 

Two kinds of operations contribute to the cost: precomputations of
 (for ) or of
 for , and
linear operations on : shifting, scaling, multiplication \dots The
former take , since the exponents involved are in
. The latter operations take  if no  or 
operation is performed, and  otherwise. This
concludes the proof. \foorp




\subsection{Inverse map}  

The map  is invertible if and only if 
(hereafter,  is the derivative of ). We discuss here the
computation of the inverse map.
\begin{theorem}[Inverse]\label{Prop:1}x
  Let  be a composition sequence that outputs  with . One can compute the map
  in time .
\end{theorem}
\myproof If  is the power series 
with ,  is the \emph{valuation} of ,
 its \emph{leading coefficient} and  its \emph{leading term}.  We also introduce an equivalence
relation on power series:  if 
and~. The proof of the next lemma is
immediate by case inspection.
\begin{Lemma}For  in
, if  and  is in , then  is in
 and .
\end{Lemma}


\smallskip\noindent{\bf Series tangent to the identity.}  We prove the
proposition in two steps. For series of the form ,
it suffices to ``reverse'' step-by-step the computation sequence for
. The following lemma is crucial.
\begin{Lemma}\label{Lemma:val}
  Let  be in , with , and let
   be a sequence defined at . Then  is a
  composition sequence.
\end{Lemma}
\myproof We have to prove that  is defined at , \emph{i.e.},
that all of  are well-defined. This
follows by applying the previous lemma inductively. \foorp

\smallskip\noindent We can now work on the inversion property
proper. Let thus  be a computation sequence,
that computes  and outputs , with . We define operations  through the following
table (note that we reverse the order of the operations):


\begin{Lemma}
 The sequence  is a
 composition sequence and outputs a series  such that
 .
\end{Lemma}
\myproof One sees by induction that for all ,  is in  and . This shows that the sequence
 is defined at  and that .  From Lemma~\ref{Lemma:val}, we deduce that 
is defined at .  Letting  be the output of ,
the previous equality gives , which concludes the
proof. \foorp

\smallskip\noindent Since , and in view of
Theorem~\ref{Prop:0}, the next lemma concludes the proof of
Theorem~\ref{Prop:1} in the current case.
\begin{Lemma}\label{lemma:rev}
With  and  as above, the map  is
the inverse of .
\end{Lemma}
\myproof Let  be in  and let , so that
, with . Evaluating at , we get
, since .  \foorp

\smallskip\noindent{\bf General case.} Lemma~\ref{lemma:rev} fails
when . We can however reduce the general case to that where
. Let us write , with ,
and define , so that .
If  is a composition sequence for , then  is a composition sequence for , and we have . Thus, by the previous point,
we can use this composition sequence to compute the map
 in time .  From the equality

we deduce 
 
Since scaling and shifting induce only an extra 
arithmetic operations, this finishes the proof of Theorem~\ref{Prop:1}.



\section{Change of Basis}  \label{sec:basis}

This section applies our results on composition to \emph{change of
basis} algorithms, between the monomial basis  and various
families of polynomials , with , for which we
reach quasi-linear complexity. As an intermediate step, we present a
bivariate evaluation algorithm.



\subsection{Main Theorem}

Let  be the bivariate power series

Associated with , we consider the map
 The matrix of this map is .
The following theorem shows that for a large class of series , the
operation  and its inverse can be performed
efficiently. The proof relies on a transposition
argument, given in~\S\ref{ssec:transposition}.\begin{theorem}[Main theorem]\label{Prop:3}
Let  be such that
\begin{itemize}
\item  and  are given by composition sequences  and ;
\item ,  and  can be computed modulo  in time ;
\item  and  are non-zero;
\item  all coefficients of 
  are non-zero.
\end{itemize}
Then the series
 is
well-defi\-ned. Besides, one can compute the map  and
its inverse in time .
\end{theorem}
\myproof Write ,
 Since
, we have that either
 for , or  for . 
Thus, the coefficient
 of  is well-defined and
 These
coefficients are those of a product of three matrices, the middle one
being diagonal; we deduce the factorization
 The assumptions on ,  and  further imply
that the map  is invertible, of inverse
 By
Theorems~\ref{Prop:0} and~\ref{Prop:1}, as well as
Theorem~\ref{Prop:4} stated below,  and its
inverse can thus be evaluated in time
. Now, from the identity , we deduce that
 Our assumptions on  and  make this
map invertible, and
 
with  and . The extra
costs induced by the computation of , , their inverses, and
the truncated products fit in .
\foorp



\subsection{Change of Basis}

To conclude, we consider polynomials  in ,
with , with generating series defined in terms of series  as in
Theorem~\ref{Prop:3} by
 
\begin{Coro}
   Under the above assumptions, one can \sloppy perform the change of basis
   from  to , and conversely, in
   time .
\end{Coro}
A surprisingly large amount of classical polynomials fits into this
framework (see next section). An important special case is provided
by Sheffer sequences~\cite[Chap.~2]{Roman05},
whose exponential generating function has the form
 Examples
include the actuarial, Laguerre, Meixner and Pois\-son-Charlier
polynomials, and the Bernoulli polynomials of the second kind (see
Tables~\ref{Fig:M1} and \ref{Fig:M2}). In this case, if  is output
by the composition sequence  and  can be computed modulo
 in time , one can perform the change of basis from
 to , and conversely, in time
.







\subsection{Transposed evaluation}   \label{ssec:transposition}

The following completes the proof of Theorem~\ref{Prop:3}.
\begin{theorem}[Transposition]\label{Prop:4}
 Let  be a composition sequence that outputs . Given , one can compute the map
  in time .
\end{theorem}


\begin{figure}[t]
\begin{center}
\fbox{
\begin{minipage}{5 cm}
\begin{tabbing}
\= \quad\quad\quad\quad\quad\quad \= \quad \= \quad \kill
\1mm]
\>  \\
\> {\sf return} 
\end{tabbing}
\end{minipage}
}
\end{center}
\caption{Algorithm .}
\label{Fig:2}
\end{figure}


\myproof This result follows directly from the transposition
principle. However, we give an explicit construction of the transposed
map  in Figure~\ref{Fig:2}. Non-linear
precomputations are left unchanged. The terminal case  is
dealt with by noting that the transpose of  is
.  To conclude, it suffices to give transposed
associativity rules for our basic operators. The formal approach we
use to write our algorithms pays off now, as it makes this
transposition process automatic.

Recall that our algorithms deal with polynomials. The dual of
 can be identified with  itself: to a -linear
form  over , one associates . Hence, transposed versions of algorithms acting on
polynomials are seen to act on polynomials as well.  Remark also that
diagonal operators are their own transpose.

\smallskip\noindent{\bf Multiplication.} In~\cite{BoLeSc03},
following~\cite{HaQuZi04}, details of the transposed versions of
plain, Karatsuba and FFT multiplications are given, with a cost
matching that of the direct product. Without relying on such
techniques, by writing down the multiplication matrix, one sees that
 is
 if  has degree . Using standard multiplication
algorithms, this formulation leads to slower algorithms than those
of~\cite{BoLeSc03}. However, in our usage cases, ,  and  are
of the same order of magnitude, and only a constant factor is lost.

\smallskip\noindent {\bf Scale.} The operator  is
diagonal; through transposition, the associativity rule becomes:


\smallskip\noindent {\bf Shift.} The transposed map  of the
reversal operator coincides with  itself, since this operator
is symmetric. By transposing the identity for
, we deduce

This algorithm for the transpose operation, though not described as
such, was already given in~\cite{Gerhard00}. This yields:

                                                      
\smallskip\noindent {\bf Powering.}  The dual map 
maps  to  (with the notation
of \S\ref{ssec:basic}). We deduce:

 
\smallskip\noindent {\bf Inversion.}  The transposed version of the
rule for  is


\smallskip\noindent {\bf Root taking.}
Considering its matrix, one sees that 
 maps  to  Besides, since the map  is
the direct sum of the maps

its transpose
 sends  to 
 
Putting this together gives the transposed associativity rule
1mm]
  A_0,\dots,A_{k-1} = \LinComb^t_{n}(A,h_0,\dots,h_{k-1})\1mm] 
 \Eval^t_{m,n}(A,\root_{k,\alpha,r}(g)) = \Root^t_{m,k}(B_0,\dots,B_{k-1})\notag\

\noindent{\bf Exponential and Logarithm.}  From the proof of
Proposition~\ref{Prop:xp}, we deduce the transposed map of  and their associativity rules




\section{Applications}\label{sec:applications}

Many generating functions of classical families of polynomials fit
into our framework. To obtain conversion algorithms, it is sufficient
to find suitable composition sequences. Table~\ref{Fig:M1} lists
families of polynomials for which conversions can be done in time
 with our method (see e.g.~\cite{Roman05,AnAsRo99} for more
on these classical families). In Table~\ref{Fig:M2}, a similar list is
given, leading to conversions of cost~; most of these
entries are actually Sheffer sequences. Many other families can be
obtained as special cases (e.g., Gegenbauer, Legendre, Chebyshev,
Mittag-Leffler, etc). 

The entry marked by  is from~\cite{Gerhard00}; the entries
marked by  are orthogonal polynomials, for which one
conversion (from the orthogonal to the monomial basis) is already
mentioned with the same complexity in~\cite{DrHeRo97,PoStTa98}.

In all cases, the pre-multiplier  depends on  only and
can be computed at precision  in time ; all our functions
 can be expanded at precision  in time . Regarding the
functions  and , most entries are easy to check; the only
explanations needed concern some series . Rational functions are
covered by the first example of \S\ref{ssec:comp}; the second example
of \S\ref{ssec:comp} deals with Jacobi polynomials and Spread
polynomials; the last example of \S\ref{ssec:comp} shows how to handle
functions with logarithms.  For Fibonacci polynomials, the function
 satisfies   From this, we deduce the sequence
for :
 For
 Mott polynomials the series  can be
 rewritten as
 This yields the composition
sequence


\begin{table*}[!!!t]

\vskip-3ex
\caption{Polynomials with conversion in }\label{Fig:M1}

\vskip-1ex

\vskip-3ex\caption{Polynomials with conversion in }\label{Fig:M2}
\end{table*}






\section{Experiments}

We implemented the algorithms for change of basis using
NTL~\cite{Shoup95}; the experiments are done for coefficients defined
modulo a 40 bit prime, using the \verb+ZZ_p+ NTL class (our algorithms
still work for degrees small with respect to the characteristic). All
timings reported here are obtained on a Pentium M, 1.73 Ghz, with 1 GB
memory.

Our implementation follows directly the presentation of the former
sections. We use the transposed multiplication implementation
of~\cite{BoLeSc03}. The Newton iteration for inverse is built-in in
NTL; we use the standard Newton iteration for square
root~\cite{Brent75}. Exponentials are computed using the algorithm
of~\cite{HaZi04}. Powers are computed through exponential and
logarithm~\cite{Brent75}, except when the arguments are binomials,
when faster formulas for binomial series are used. For evaluation and
interpolation at , and their transposes, we use the
implementation of~\cite{BoLeSc03}.

We use the Jacobi and Mittag-Leffler orthogonal polynomials (a special
case of Meixner polynomials, with  and ), with the
composition sequences of \S\ref{ssec:comp}. Our algorithm has cost
 for the former and  for the latter.  We
compare this to the naive approach of quadratic cost in
Figure~\ref{Fig:jacobi} and~\ref{Fig:mittag}, respectively. Timings
are given for the conversion from orthogonal to monomial bases; those
for the inverse conversion are similar.
\begin{figure}[!!!!!!!!!h]
\centerline{\includegraphics[scale=0.5]{jacobi}}
\vspace{-3ex}
\caption{{\small Jacobi polynomials.}}
\label{Fig:jacobi}
\end{figure}
\vspace{-5ex}
\begin{figure}[!!!!!!!!!h]
\centerline{\includegraphics[scale=0.5]{mittagleffler}}
\vspace{-3ex}
\caption{{\small Mittag-Leffler polynomials.}}
\label{Fig:mittag}
\end{figure}

Our algorithm performs better than the quadratic one. The crossover
points lie between 100 and 200; this large value is due to the
constant hidden in our big-Oh estimates: in both cases, there is a
contribution of about , plus an additional  for
Mittag-Leffler.





\section{Discussion}

This article provides a flexible framework for generating new families
of conversion algorithms: it suffices to add new composition operators
to Table~\ref{tab:leftcomp} and provide the corresponding
associativity rules.  Still, several questions need further
investigation. Several of the composition sequences we use are
non-trivial: this raises in particular the questions of characterizing
what functions can be computed by a composition sequence, and of
determining such sequences algorithmically.  Besides, the costs of our
algorithms are measured only in terms of arithmetic operations; the
questions of numerical stability (for floating-point computations) or
of coefficient size (when working over ) require further work.

\smallskip\noindent{\bf Acknowledgments.} We thank ANR Gecko, the
joint Inria-Microsoft Research Lab and NSERC for financial support.

\scriptsize
\begin{thebibliography}{10}

\bibitem{AhStUl75}
A.~V. Aho, K.~Steiglitz, and J.~D. Ullman.
\newblock Evaluating polynomials at fixed sets of points.
\newblock {\em SIAM J. Comp.}, 4(4):533--539, 1975.

\bibitem{AlRo91}
B.~K. Alpert and V.~Rokhlin.
\newblock A fast algorithm for the evaluation of {L}egendre expansions.
\newblock {\em SIAM J. Sci. Statist. Comp.}, 12(1):158--179, 1991.

\bibitem{AnAsRo99}
G.~Andrews, R.~Askey, and R.~Roy.
\newblock {\em Special functions}.
\newblock Cambridge University Press, 1999.

\bibitem{BaPe04}
R.~Barrio and J.~Pe{\~n}a.
\newblock Basis conversions among univariate polynomial representations.
\newblock {\em C. R. Math. Acad. Sci. Paris}, 339(4):293--298, 2004.

\bibitem{Bernstein98b}
D.~J. Bernstein.
\newblock Composing power series over a finite ring in essentially linear time.
\newblock {\em J. Symb. Comp.}, 26(3):339--341, 1998.

\bibitem{BiPa94}
D.~Bini and V.~Y. Pan.
\newblock {\em Polynomial and matrix computations. {V}ol. 1}.
\newblock Birkh{\"a}user Boston Inc., 1994.

\bibitem{BoLeSc03}
A.~Bostan, G.~Lecerf, and {\'E}.~Schost.
\newblock Tellegen's principle into practice.
\newblock In {\em ISSAC'03}, pages 37--44. ACM, 2003.

\bibitem{BoSaSc08}
A.~Bostan, B.~Salvy, and {\'E}.~Schost.
\newblock Fast algorithms for orthogonal polynomials.
\newblock In preparation.

\bibitem{BoSc05}
A.~Bostan and {\'E}.~Schost.
\newblock Polynomial evaluation and interpolation on special sets of points.
\newblock {\em J. Complexity}, 21(4):420--446, 2005.

\bibitem{Brent75}
R.~P. Brent.
\newblock Multiple-precision zero-finding methods and the complexity of
  elementary function evaluation.
\newblock In {\em Analytic Computational Complexity}, pages 151--176. Acad.
  Press, 1975.

\bibitem{BrKu78}
R.~P. Brent and H.~T. Kung.
\newblock Fast algorithms for manipulating formal power series.
\newblock {\em J.~ACM}, 25(4):581--595, 1978.

\bibitem{BuClSh97}
P.~B{\"u}rgisser, M.~Clausen, and A.~Shokrollahi.
\newblock {\em Algebraic complexity theory}, volume 315 of {\em GMW}.
\newblock Springer--Verlag, 1997.

\bibitem{CaKaLa89}
J.~Canny, E.~Kaltofen, and Y.~Lakshman.
\newblock Solving systems of non-linear polynomial equations faster.
\newblock In {\em ISSAC'89}, pages 121--128. ACM, 1989.

\bibitem{CaKa91}
D.~G. Cantor and E.~Kaltofen.
\newblock On fast multiplication of polynomials over arbitrary algebras.
\newblock {\em Acta Inform.}, 28(7):693--701, 1991.

\bibitem{DrHeRo97}
J.~R. Driscoll, J.~D.~M.~Healy, and D.~N. Rockmore.
\newblock Fast discrete polynomial transforms with applications to data
  analysis for distance transitive graphs.
\newblock {\em SIAM J. Comp.}, 26(4):1066--1099, 1997.

\bibitem{Frumkin95}
M.~Frumkin.
\newblock A fast algorithm for expansion over spherical harmonics.
\newblock {\em Appl. Algebra Engrg. Comm. Comp.}, 6(6):333--343, 1995.

\bibitem{GaGe99}
J.~g. Gathen and J.~Gerhard.
\newblock {\em Modern computer algebra}.
\newblock Cambridge University Press, 1999.

\bibitem{Gerhard00}
J.~Gerhard.
\newblock Modular algorithms for polynomial basis conversion and greatest
  factorial factorization.
\newblock In {\em RWCA'00}, pages 125--141, 2000.

\bibitem{HaQuZi04}
G.~Hanrot, M.~Quercia, and P.~Zimmermann.
\newblock The {M}iddle {P}roduct {A}lgorithm,~{I}.
\newblock {\em Appl. Algebra Engrg. Comm. Comp.}, 14(6):415--438, 2004.

\bibitem{HaZi04}
G.~Hanrot and P.~Zimmermann.
\newblock Newton iteration revisited.
\newblock \texttt{http://www.loria.fr/~zimmerma/papers}, 2002.

\bibitem{Heinig01}
G.~Heinig.
\newblock Fast and superfast algorithms for {H}ankel-like matrices related to
  orthogonal polynomials.
\newblock In {\em NAA'00}, volume 1988 of {\em LNCS}, pages 361--380.
  Springer-Verlag, 2001.

\bibitem{Hoeven02}
J.~g. Hoeven.
\newblock Relax, but don't be too lazy.
\newblock {\em J. Symb. Comput.}, 34(6):479--542, 2002.

\bibitem{Kaltofen00}
E.~Kaltofen.
\newblock Challenges of symbolic computation: my favorite open problems.
\newblock {\em J. Symb. Comp.}, 29(6):891--919, 2000.

\bibitem{KaLa88}
E.~Kaltofen and Y.~Lakshman.
\newblock Improved sparse multivariate polynomial interpolation algorithms.
\newblock In {\em ISSAC'88}, volume 358 of {\em LNCS}, pages 467--474. Springer
  Verlag, 1989.

\bibitem{Keiner07}
J.~Keiner.
\newblock Computing with expansions in {G}egenbauer polynomials.
\newblock Preprint AMR07/10, U. New South Wales, 2007.

\bibitem{LeRoCh07}
G.~Leibon, D.~Rockmore, and G.~Chirikjian.
\newblock A fast {H}ermite transform with applications to protein structure
  determination.
\newblock In {\em SNC'07}, pages 117--124, New York, NY, USA, 2007. ACM.

\bibitem{LiZh98}
Y.-M. Li and X.-Y. Zhang.
\newblock Basis conversion among {B}\'ezier, {T}chebyshev and {L}egendre.
\newblock {\em Comput. Aided Geom. Design}, 15(6):637--642, 1998.

\bibitem{Pan98}
V.~Y. Pan.
\newblock New fast algorithms for polynomial interpolation and evaluation on
  the {C}hebyshev node set.
\newblock {\em Computers and Mathematics with Applications}, 35(3):125--129,
  1998.

\bibitem{PoStTa98}
D.~Potts, G.~Steidl, and M.~Tasche.
\newblock Fast algorithms for discrete polynomial transforms.
\newblock {\em Math. Comp.}, 67(224):1577--1590, 1998.

\bibitem{Roman05}
S.~Roman.
\newblock {\em The umbral calculus}.
\newblock Dover publications, 2005.

\bibitem{ScSt71}
A.~Sch{\"o}nhage and V.~Strassen.
\newblock {S}chnelle {M}ultiplikation gro\ss er {Z}ahlen.
\newblock {\em Computing}, 7:281--292, 1971.

\bibitem{Shoup95}
V.~Shoup.
\newblock A new polynomial factorization algorithm and its implementation.
\newblock {\em J. Symb. Comp.}, 20(4):363--397, 1995.

\end{thebibliography}

\end{document}
