\documentclass{article}







\usepackage[final]{nips_2018}



\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{tabu,multirow}
\usepackage{pbox}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{subfigure}
\usepackage[normalem]{ulem}


\hypersetup{pdftitle={},
  pdfauthor={},
  pdfkeywords={},
  bookmarksnumbered,
  pdfstartview={FitH},
  colorlinks,
urlcolor=black,
  filecolor=black,
  linkcolor=black,
  breaklinks=true,
}
\pagenumbering{arabic}

\graphicspath{ {figs/} }

\usepackage[dvipsnames]{xcolor}

\newcommand{\oursshort}[0]{DA-Net\xspace}
\newcommand{\ours}[0]{Double Attention Networks\xspace}



\usepackage{xspace}

\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}
\def\eg{e.g\onedot} \def\Eg{E.g\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{cf\onedot} \def\Cf{Cf\onedot}
\def\etc{etc\onedot} \def\vs{vs\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et~al}\onedot} \def\iid{i.i.d\onedot}
\def\Fig{Fig\onedot} \def\Eqn{Eqn\onedot} \def\Sec{Sec\onedot}
\def\vs{vs\onedot}
\def\ul{\underline}
\makeatother
 

\title{-Nets: Double Attention Networks}




\author{
  Yunpeng Chen\thanks{Part of the work is done during internship at Facebook Research.} \\
  National University of Singapore\\
  \texttt{chenyunpeng@u.nus.edu} \\
  \And
  Yannis Kalantidis \\
  Facebook Research\\
  \texttt{yannisk@fb.com} \\
  \And
  Jianshu Li \\
  National University of Singapore\\
  \texttt{jianshu@u.nus.edu} \\
  \And
  Shuicheng Yan \\
  Qihoo 360 AI Institute \\
  National University of Singapore\\
  \texttt{eleyans@nus.edu.sg} \\
  \And
  Jiashi Feng \\
  National University of Singapore\\
  \texttt{elefjia@nus.edu.sg} \\
}
\newcommand{\nospace}{\vspace{-4mm}}
\newcommand{\head}[1]{{\smallskip\noindent\bf{#1}}}
\newcommand{\alert}[1]{{\color{red}{#1}}}




\newcommand{\one}{\mathbbm{1}}
\newcommand{\bbF}{\mathbb{F}}
\newcommand{\bbH}{\mathbb{H}}
\newcommand{\expect}{\mathbb{E}}
\newcommand{\project}{\mathbb{P}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\tran}{^{\mathrm{T}}}
\newcommand{\diff}{\mathrm{d}}
\newcommand{\prob}{\mathrm{Pr}}
\newcommand{\binomial}{\mathrm{Bi}}
\newcommand{\weibull}{\mathrm{Wb}}
\newcommand{\normal}{\mathcal{N}}
\newcommand{\wishart}{\mathcal{W}}
\newcommand{\zcol}{\mathbf{0}}
\newcommand{\zrow}{\zcol\tran}
\newcommand{\otherwise}{\mathrm{otherwise}}

\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cZ}{\mathcal{Z}}

\newcommand{\va}{\mathbf{a}}
\newcommand{\vb}{\mathbf{b}}
\newcommand{\vc}{\mathbf{c}}
\newcommand{\vd}{\mathbf{d}}
\newcommand{\ve}{\mathbf{e}}
\newcommand{\vf}{\mathbf{f}}
\newcommand{\vg}{\mathbf{g}}
\newcommand{\vh}{\mathbf{h}}
\newcommand{\vi}{\mathbf{i}}
\newcommand{\vj}{\mathbf{j}}
\newcommand{\vk}{\mathbf{k}}
\newcommand{\vl}{\mathbf{l}}
\newcommand{\vm}{\mathbf{m}}
\newcommand{\vn}{\mathbf{n}}
\newcommand{\vo}{\mathbf{o}}
\newcommand{\vp}{\mathbf{p}}
\newcommand{\vq}{\mathbf{q}}
\newcommand{\vr}{\mathbf{r}}
\newcommand{\vt}{\mathbf{t}}
\newcommand{\vu}{\mathbf{u}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\vw}{\mathbf{w}}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vy}{\mathbf{y}}
\newcommand{\vz}{\mathbf{z}}

\newcommand{\vA}{\mathbf{A}}
\newcommand{\vB}{\mathbf{B}}
\newcommand{\vC}{\mathbf{C}}
\newcommand{\vD}{\mathbf{D}}
\newcommand{\vF}{\mathbf{F}}
\newcommand{\vG}{\mathbf{G}}
\newcommand{\vI}{\mathbf{I}}
\newcommand{\vM}{\mathbf{M}}
\newcommand{\vS}{\mathbf{S}}
\newcommand{\vW}{\mathbf{W}}
\newcommand{\vX}{\mathbf{X}}
\newcommand{\vzero}{\mathbf{0}}

\newcommand{\rLambda}{\mathrm{\Lambda}}
\newcommand{\rSigma}{\mathrm{\Sigma}}

\newcommand{\vmu}{\bm{\mu}}
\newcommand{\vpi}{\bm{\pi}}
\newcommand{\vLambda}{\bm{\rLambda}}
\newcommand{\vSigma}{\bm{\rSigma}}
\newcommand{\indicator}{\mathbbm{1}}



\usepackage{xspace}
\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}
\def\eg{e.g\onedot} \def\Eg{E.g\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{cf\onedot} \def\Cf{Cf\onedot}
\def\etc{etc\onedot} \def\vs{vs\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et~al}\onedot} \def\iid{i.i.d\onedot}
\def\Fig{Fig\onedot} \def\Eqn{Eqn\onedot} \def\Sec{Sec\onedot}
\def\vs{vs\onedot}
\makeatother
 

\begin{document}


\maketitle

\begin{abstract}
Learning to capture long-range relations is fundamental to image/video recognition. Existing CNN models generally rely on increasing depth to model such relations which is highly inefficient. In this work, we propose the ``double attention block'', a novel component that aggregates and propagates informative global features from the entire spatio-temporal space of input images/videos, enabling  subsequent convolution layers to access features from the entire space efficiently. The component is designed with a double attention mechanism in two steps, where the first step gathers features from the entire space into a compact set through second-order attention pooling and the second step adaptively selects and distributes features to each location via another attention. The proposed double attention block is easy to adopt and can be plugged into existing deep neural networks conveniently. We conduct extensive ablation studies and experiments on both image and video recognition tasks for evaluating its performance. On the image recognition task, a ResNet-50 equipped with our double attention blocks outperforms a much larger ResNet-152 architecture on ImageNet-1k dataset with over  less the number of parameters and less FLOPs. On the action recognition task, our proposed model achieves the state-of-the-art results on the Kinetics and UCF-101 datasets with significantly higher efficiency than recent works. 
\end{abstract}
 
\section{Introduction}
\label{sec:intro}

Deep Convolutional Neural Networks (CNNs) have been successfully applied in image and video understanding during the past few years. 
Many new network topologies have been developed to alleviate optimization difficulties~\cite{he2016deep,he2016identity} and increase the learning capacities~\cite{xie2017aggregated,chen2017dual}, which benefit recognition performance for both images~\cite{girshick2015fast,chen2016deeplab} and videos~\cite{tran2017closer} significantly.


However, CNNs are inherently limited by their convolution operators which are dedicated to capturing local features and relations, \emph{e.g.} from a  region, and are inefficient in modeling long-range interdependencies. 
Though stacking multiple convolution operators can enlarge the receptive field,  it also comes with a number of unfavorable issues in practice. First, stacking multiple operators makes the model unnecessarily deep and large, resulting in higher computation and memory cost as well as increased over-fitting risks. Second, features far away from a specific location have to pass through a stack of layers before 
affecting the location for both forward propagation and backward propagation, increasing the optimization difficulties during the training.
Third, the features visible to a distant location are actually ``delayed'' ones from several layers behind, 
causing inefficient reasoning. Though some recent works~\cite{hu2017,wang17non} can partially alleviate the above issues, they are either non-flexible~\cite{hu2017} or computationally expensive~\cite{wang17non}.

In this work, we aim to overcome these limitations by introducing a new network component that enables a convolution layer to sense the entire spatio-temporal space\footnote{Here by ``space'' we mean the entire feature maps of an input frame and the complete spatio-temporal features from a video sequence.} from its adjacent layer immediately.
The core idea is to first \textit{gather} key features from the entire space into a compact set and then \textit{distribute} them to each location adaptively, so that the subsequent convolution layers can sense features from the entire space even without a large receptive filed. We develop a generic function for such purpose and implement it with an efficient \textit{double attention} mechanism. The first second-order attention pooling operation selectively gathers key features from the entire space, while the second adopts another attention mechanism to adaptively distribute a subset of key features that are helpful to complement each spatio-temporal location for high-level tasks. 
We denote our proposed double-attention block as -block and its resultant network as  -Net. 

The double-attention block is related to a number of recent works, including the Squeeze-and-Excitation Networks~\cite{hu2017}, covariance pooling~\cite{li2017second}, the Non-local Neural Networks~\cite{wang17non} and the Transformer architecture of~\cite{vaswani2017attention}. However, compared with these existing works, it  enjoys several unique advantages:
Its first attention operation implicitly computes second-order statistics of pooled features and can capture complex appearance and motion correlations that cannot be captured by the global average pooling used in SENet~\cite{hu2017}. Its second attention operation \textit{adaptively} allocates features from a compact bag, which is more efficient than exhaustively correlating the features from all the locations with every specific location as in~\cite{wang17non,vaswani2017attention}. Extensive experiments on image and video recognition tasks clearly validate the above advantages of our proposed method.

We summarize our contributions as follows:
\begin{itemize}
\item We propose a generic formulation for capturing long-range feature interdependencies via universal gathering and distribution functions. 
\item We propose the double attention block for gathering and distributing long-range features, an efficient architecture that captures second-order feature statistics and makes adaptive feature assignment. The block can model long-range interdependencies with a low computational and memory footprint and at the same time boost image/video recognition performance significantly. 

\item We investigate the effect of our proposed -Net with extensive ablation studies and prove its superior performance through comparison with the state-of-the-arts on a number of public benchmarks for both image recognition and video action recognition tasks, including ImageNet-1k, Kinetics and UCF-101.
\end{itemize}

The rest of the paper is organized as follows. We first motivate and present our approach in Section~\ref{sec:method}, where we also discuss the relation of our approach to recent works. We then evaluate and report results in Section~\ref{sec:experiments} and conclude the paper with Section~\ref{sec:conclusions}.
 
\section{Method}
\label{sec:method}

Convolutional operators are designed to focus on local neighborhoods and therefore fail to ``sense'' the entire spatial and/or temporal space, \emph{e.g.} the entire input frame or one location across multiple frames. A CNN model thus usually employs multiple convolution layers (or recurrent units~\cite{donahue2015long,ng2015beyond}) in order to capture global aspects of the input. Meanwhile, self-attentive and correlation operators like second-order pooling have been recently shown to work well in a wide range of tasks~\cite{vaswani2017attention,li2017second,lin2015bilinear}. 
In this section we present a component capable of gathering and distributing global features to each spatial-temporal location of the input, helping subsequent convolution layers sense the entire space immediately and capture complex relations. We first formally describe this desired component by providing a generic formulation and then introduce our double attention block, a highly efficient instantiation of such a component. We finally discuss the relation of our approach to other recent related approaches.

Let  denote the input tensor for a spatio-temporal (3D) convolutional layer, where  denotes the number of channels,  denotes the temporal dimension\footnote{For a spatial (2D) convolution, \emph{i.e.} when the input is an image, .} and ,  are the spatial dimensions of the input frames. For every spatio-temporal input location  with local feature , let us define

to be the output of an operator that first \textit{gathers} features in the entire space
and then \textit{distributes} them back to each input location , taking into account the local feature  of that location.
Specifically,  adaptively aggregates features from the entire input space, and  distributes the gathered information to each location , conditioned on the local feature vector . 

 
\begin{figure}[t]
\centering
\resizebox{1.0\columnwidth}{!}{
	\includegraphics[]{fig1_main_motivations.pdf}
}
\caption{Illustration of the double-attention mechanism. (a) An example on a single frame input for explaining the idea of our double attention method, where the set of global featues is computed only once and then shared by all locations.
Meanwhile, each location  will generate its own attention vector based on the need of its local feature  to select a desired subset of global features that is helpful to complement current location and form the feature . (b) The double attention operation on a three dimensional input array . The first attention step is shown on the top and produces a set of global features. At location , the second attention step generates the new local feature , as shown at the bottom.}
\label{fig:AA-operation}
\end{figure}


The idea of gathering and distributing information is motivated by the squeeze-and-excitation network (SENet)~\cite{hu2017}.
Eqn.~\eqref{eqn:gather-and-distr}, however, presents it in a more general form that leads to some interesting insights and optimizations. In~\cite{hu2017}, global average pooling is used in the gathering process, while the resulted single global feature is distributed to all locations, ignoring different needs  across locations. Seeing these shortcomings, we introduce this genetic formulation and propose the \textit{Double Attention block}, where global information is first gathered by second-order  attention pooling (instead of first-order average pooling), and the gathered global features are adaptively distributed conditioned on the need of current local feature , by a second attention mechanism. In this way, more complex global relations can be captured by a compact set of features and each location can receive its customized global information that is complementary to the exiting local features, facilitating learning more complex relations. 
The proposed component  is illustrated in Figure~\ref{fig:AA-operation} (a). At below, we first describe its architecture in details and then discuss some instantiations and its connections to other recent related approaches.


\subsection{The First Attention Step: Feature Gathering}
\label{subsec:first_attention}
A recent work \cite{lin2015bilinear} used bilinear pooling to capture second-order statistics of features and generate  global representations. Compared with the conventional average and max pooling which only compute first-order statistics,  bilinear pooling can capture and preserve complex relations better. Concretely, bilinear pooling gives a sum pooling of second-order features from the \emph{outer product} of all the feature vector pairs  within two input feature maps  and :

where  and  . In CNNs,  and  can be the feature maps from the same layer, \emph{i.e.} , or from two different layers, \emph{i.e.}  and , with parameters  and .


By introducing the output variable   of the bilinear pooling and rewriting the second feature   as  where each  is a -dimensional row vector, we can reformulate  Eqn.~\eqref{eqn:bilienar-pooling-org} as

Eqn.~\eqref{eqn:first-att-pooling_vec} gives a new perspective on the bilinear pooling result: instead of just computing second-order statistics, the output of bilinear pooling  is actually a bag of visual primitives, where each primitive  is calculated by gathering local features weighted by  . 
This inspires us to develop a new attention-based feature gathering operation. We further apply a  onto  to ensure , \emph{i.e.} a valid attention weighting vector, which gives following \emph{second-order attention pooling} process: 

The first row in Figure~\ref{fig:AA-operation} (b) shows the second-order attention pooling that corresponds to Eqn.~\eqref{eqn:att_pool}, where both  and  are outputs of two different convolution layers transforming the input . In implementation, we let  and . The second-order attention pooling offers an effective way to gather key features: it captures the global features, \emph{e.g.} texture and lighting, when  is densely attended on all locations; and it captures the existence of specific semantic, \emph{e.g.} an object and parts, when  is sparsely attended on a specific region. We note that similar understandings were presented in \cite{girdhar2017attentional}, in which they proposed a rank-1 approximation of a bilinear pooling operation associated with a fully connected classifier. However, in our work, we propose to apply attention pooling to gather visual primitives at different locations into a bag of global descriptors using  attention map and do not apply any low-rank constraint.


\subsection{The Second Attention Step: Feature Distribution}

The next step after gathering features from the entire space is to distribute them to each  location of the input, such that the subsequent convolution layer can sense the global information even with a small convolutional kernel. 

Instead of distributing the same summarized global features to all locations like SENet~\cite{hu2017}, we propose to get more flexibility by distributing an adaptive bag of visual primitives based on the need of feature  at each location. In this way, each location can select features that are complementary to the current feature which can make the training easier and help capture more complex relations. This is achieved by selecting a subset of feature vectors from  with soft attention:

Eqn.~\eqref{eqn:second-att-pooling_vec} formulates the proposed soft attention for feature selection. In our implementation, we apply  the  function to normalize   into the one with unit sum, which is found to give better convergence. The second row in Figure~\ref{fig:AA-operation} (b) shows the above feature selection step. Similar to the way we generate the attention map, the set of attention weight vectors is also generated by a convolution layer follow by a  normalizer, \emph{i.e.}  where  contains parameters for this layer.


\subsection{The Double Attention Block}
\label{subsec:double_att}
We combine the above two attention steps to form our proposed \emph{double-attention block}, with its computation graph in deep neural networks is given in Figure~\ref{fig:Implement_A^2_block}. 
To formulate the double attention operation, we substitute Eqn.~\eqref{eqn:att_pool} and Eqn.~\eqref{eqn:second-att-pooling_vec} into Eqn.~\eqref{eqn:gather-and-distr} and obtain


\begin{figure}
\centering
\resizebox{0.75\columnwidth}{!}{
    \includegraphics[]{fig2_computation_graph.pdf}
}
\caption{The computational graph of the proposed double attention block. All convolution kernel size is . We insert this double attention block to existing convolutional neural network, \emph{e.g.} residual networks~\cite{he2016deep}, to form the -Net.}
\label{fig:Implement_A^2_block}
\end{figure}

Figure~\ref{fig:AA-operation} (b) shows the combined double attention operation and Figure~\ref{fig:Implement_A^2_block} shows the corresponding computational graph, where the feature arrays ,  and  are generated by three different convolution layers operating on the input feature array  followed by  normalization if necessary. The output result  is given by conducting two matrix multiplications with necessary reshape and transpose operations. Here, an additional convolution layer is added at the end to expand the number of channels for the output , such that it can be encoded back to the input  via element-wise addition. During the training process, gradient of the loss function can be easily computed using  auto-gradient~\cite{chen2015mxnet,paszke2017pytorch} with the chain rule.

There are two different ways to implement the computational graph of Eqn.~\eqref{eqn:A^2_combined}. One is to use the left association as given in Eqn.~\eqref{eqn:A^2_combined} with computation graph is shown in Figure~\ref{fig:Implement_A^2_block}. The other is to conduct the right association, as formulate below:

We note these two different associations are mathematically equivalent and thus will produce the same output. However, they have different computational cost and memory consumption. The computational complexity of the second matrix multiplication  in ``left association'' in Eqn.~\eqref{eqn:A^2_combined} is , while  ``right association''  in Eqn.~\eqref{eqn:A^2_final_right} has complexity of . As for the memory cost\footnote{All values are stored in 32-bit float.},  storing the output of the results of the first matrix multiplication costs  MB and MB for the left and right associations  respectively. In practice, an input data array  with   frames  and  channel size can easily cost more than GB memory when adopting the right association, much more expensive than  MB cost of the left association. In this case, left association is also more computationally efficient than the right one. Therefore, for common cases where , we suggest implementation in Eqn.~\eqref{eqn:A^2_combined} with left  association. 



\subsection{Discussion}
It is interesting to observe that the implementation in Eqn.~\eqref{eqn:A^2_final_right} with right association can be further explained by the recent NL-Net~\cite{wang17non}, where the first multiplication captures pair-wise relations between local features and gives an output relation matrix in . The resulted relation matrix is then applied to linearly combine the transformed features  into the output feature . The difference is apparent in the design of the pair-wise relation function, where we propose a new relation function, \emph{i.e.}  rather than using the Embedded Gaussian formulation~\cite{vaswani2017attention} to capture the pair-wise relations. Meanwhile, as discussed above, any such a method practically suffers from high computational and memory costs, and relies on the some subsampling tricks to reduce the cost which may potentially hurts the accuracy. Since NL-Net is the current state-of-the-art for video recognition tasks and also closely related, we directly compare and extensively discuss performance between the two in the Experiments section. The results clearly show that our proposed method not only outperforms NL-Net, but does so with higher efficiency and accuracy.
As the Embedded Gaussian NL-Net formulation that we compare in the experiments is mathematically equivalent to the self-attention formulation of~\cite{vaswani2017attention}, conclusions/comparisons to NL-Net extend to the transformer networks as well.
 






 
\section{Experiments}
\label{sec:experiments}

In this section, we first conduct extensive ablation studies to evaluate the proposed -Nets on the Kinetics~\cite{kay2017kinetics} video recognition dataset and compare it with the state-of-the-art NL-Net~\cite{wang17non}. Then we conduct more experiments using deeper and wider neural networks on both image recognition  and video recognition tasks and compare it with state-of-the-art methods. 
 
\subsection{Implementation Details}

\paragraph{Backbone CNN}
We use the residual network~\cite{he2016identity} as our backbone CNN for all experiments. Table~\ref{tab:backbone_cnns} shows architecture details of the backbone CNNs for  video recognition tasks, where we use {ResNet-26} for all ablation studies and {ResNet-29} as one of the baseline methods. The computational cost is measured by FLOPs, \emph{i.e.} floating-point multiplication-adds, and the model complexity is measured by \#Params, \emph{i.e.} total number of trained parameters. The {ResNet-50} is almost  deeper and wider than the {ResNet-26} and thus only  used for last several experiments when comparing with the state-of-the-art methods. For the image recognition task, we use the same {ResNet-50} but without the temporal dimension for both the input/output data and convolution kernels.

\begin{table*}[t]
  \tiny
  \centering
  \setlength\tabcolsep{3pt}
  \caption{Three backbone Residual Networks for the video tasks. The input size for ResNet-26 and ResNet-29 are 16112112, while the input size for ResNet-50 is 8224224. We follow~\cite{wang17non} and set  for ResNet-50 in last three stages and decrease the temporal size to reduce computational cost.}
  \resizebox{\columnwidth}{!}{
    \begin{tabular}{c|c|c|c||c|c}
    \toprule
    stage & ResNet-26 & ResNet-29 & output & ResNet-50 & output
    \\ \midrule conv1 
    & 355, 16, stride (1,2,2)
    & 355, 16, stride (1,2,2)
    & 165656
    & \makecell{355, 32, stride (1,2,2) \\ max pooling, stride (1,2,2)}
    & 85656
    \\ \midrule conv2
	& \times\times\times\times\times\times
    & \times\times\times\times\times\times
    & 85656
    & \times\times\times\times\times\times
    & 85656
    \\ \midrule conv3 
    &  \times\times\times\times\times\times 
    &  \times\times\times\times\times\times
    & 82828
    &  \times\timesk\times\times\times\times
    & 42828
    \\ \midrule conv4 
    &  \times\times\times\times\times\times
    &  \times\times\times\times\times\times
    & 81414
    &  \times\timesk\times\times\times\times
    & 41414
    \\ \midrule conv5 
	&  \times\times\times\times\times\times 
	&  \times\times\times\times\times\times 
    & 877
    &  \times\timesk\times\times\times\times 
    & 477
    \\ \midrule & \pbox{20cm}{global average pool, fc, softmax} 
    & \pbox{20cm}{global average pool, fc, softmax} 
    & 111
    & \pbox{20cm}{global average pool, fc, softmax} 
    & 111
    \\ \midrule (\#Params, FLOPs)
    &  (7.0 M, 8.3 G)
    &  (7.6 M, 9.2 G)
    &  &  (33.4 M, 31.3 G)
    &  \\ \bottomrule \end{tabular}
  }
  \label{tab:backbone_cnns}
\end{table*}


\paragraph{Training and Testing Settings}
We use MXNet~\cite{chen2015mxnet} to experiment on the image classification task, and PyTorch~\cite{paszke2017pytorch} on video classification tasks. For  image classification, we report standard single model single  center crop validation accuracy,  following ~\cite{he2016deep,he2016identity}. For experiments on video datasets, we report both single clip accuracy and video accuracy.
All experiments are conducted using a distributed K80 GPU cluster and the networks are optimized by synchronized SGD. Code and trained models will be released on GitHub soon.


\subsection{Ablation Studies}

For the ablation studies on Kinetics~\cite{carreira2017quo}, we use 32 GPUs per experiment with a total batch size of 512 training from scratch. All networks take 16 frames with resolution  as input. The base learning rate is set to  and is reduced with a factor of  at the k-th, k-th iterations, and terminated at the k-th iteration. We set the number of output channels for three convolution layers ,  and  to be  of the number of input channels. Note that sub-sampling trick is not adopted for all methods for fair comparison.

\vspace{-2mm}
\paragraph{Single Block}
Table~\ref{tab:single_block} shows the results when only one extra block is added to the backbone network. The block is placed after the second residual unit of a certain stage. As can be seen  from the last three rows, our proposed -block constantly improves the performance compared with both the baseline ResNet-26 and the deeper ResNet-29. Notably the  extra cost is very little. We also find that  the performance gain from placing  -block on top layers is more significant than placing it at  lower layers. This may be because the top layers give  more  semantically abstract representations that are  suitable for extracting global visual primitives. Comparatively, the Nonlocal Network~\cite{wang17non} shows less accuracy gain and more computational cost than ours. Since the computational cost for Nonlocal Network is increased quadratically on bottom stage, we are even unable to finish the training when the block is placed at Conv2.

\begin{table}[t]
\caption{Comparisons between single nonlocal block~\cite{wang17non} and single double attention block on the Kinetics dataset. The performance of  vanilla residual networks without extra block is shown in the top row. 
}
\centering
\resizebox{\columnwidth}{!}{
  \begin{tabular}{>{\centering}p{3cm}>{\centering}p{2cm}c|cc|cc|c}
  \toprule
  Model                                            &  + 1 Block  & \#Params   &   FLOPs   &  FLOPs &   Clip 1   &  Clip1 &  Video1  \\
  \midrule
  ResNet-26                                        &    None     & 7.043 M    & ~~8.3 G   &   	  --       &    50.4 \%    &          --       &    60.7 \%  \\
  ResNet-29                                        &    None     & 7.620 M    & ~~9.2 G   & 	~~900 M    &    50.8 \%    &         +0.5 \%   &    61.6 \%  \\
  \midrule
  \multirow{3}{*}{ResNet-26 + NL~\cite{wang17non}} &  Conv2   & 7.061 M    &  49.0 G   & 	40.69 G    &     --        &          --       &     --      \\
                                                   &  Conv3   & 7.112 M    &  13.7 G   &    ~~5.45 G    &    51.5 \%    &         +1.1 \%   &    62.0 \%  \\
                                                   &  Conv4   & 7.312 M    & ~~9.3 G   &    ~~1.04 G    &    51.7 \%    &         +1.3 \%   &    62.3 \%  \\
  \midrule
  \multirow{3}{*}{ResNet-26 + }               &  Conv2   & 7.061 M    & ~~8.7 G   & 	~~463 M    &    51.2 \%    &         +0.8 \%   &    61.8 \%  \\
                                                   &  Conv3   & 7.112 M    & ~~8.7 G   &     ~~463 M    &    51.9 \%    &         +1.5 \%   &    62.0 \%  \\
                                                   &  Conv4   & 7.312 M    & ~~8.7 G   & 	~~463 M    &    \textbf{52.3} \%    &         \textbf{+1.9} \%   &    \textbf{62.6} \%  \\
  \bottomrule
\end{tabular}
}
\label{tab:single_block}
\end{table}


\vspace{-2mm}
\paragraph{Multiple Blocks}
Table~\ref{tab:multiple_block} shows the performance gain when multiple blocks are added to the backbone networks. As can be seen from the results, our proposed -Net monotonically improves the accuracy when more blocks are added and costs less \#FLOPs compared with its competitor. We also find that adding blocks to different stages can lead to more significant accuracy gain than adding all blocks to the same stage.


\begin{table}[t]
\caption{Comparisons between performance from  multiple nonlocal blocks~\cite{wang17non} and multiple double attention blocks on Kinetics dataset. We report both top-1 clips accuracy and top-1 video accuracy for all the methods. The vanilla residual networks without extra blocks are shown in the top row.}
\centering
\resizebox{\columnwidth}{!}{
  \begin{tabular}{>{\centering}p{3cm}>{\centering}p{2cm}c|cc|cc|c}
  \toprule
  Model                                            &  +N Blocks      & \#Params   &  FLOPs    &  FLOPs &  Clip 1  &  Clip1 & Video 1  \\
  \midrule
  ResNet-26                                        &     None        & 7.043 M    & ~~8.3 G   &      --         &   50.4 \%   &  	      --      &   60.7 \%   \\
  ResNet-29                                        &     None        & 7.620 M    & ~~9.2 G   &    ~~900 M      &   50.8 \%   &  	    +0.5 \%   &   61.6 \%   \\
  \midrule
  \multirow{3}{*}{ResNet-26 + NL~\cite{wang17non}} & 1  Conv4     & 7.312 M    & ~~9.3 G   &   ~~1.04 G      &   51.7 \%   &  	    +1.3 \%   &   62.3 \%   \\
                                                   & 2  Conv4     & 7.581 M    &  10.4 G   &   ~~2.08 G      &   52.0 \%   &  	    +1.6 \%   &   62.9 \%   \\
                                                   & 4  Conv3\&4  & 7.719 M    &  21.3 G   &    12.97 G      &   52.4 \%   &  	    +2.0 \%   &   62.8 \%   \\
  \midrule
  \multirow{3}{*}{ResNet-26 + }               & 1  Conv4     & 7.312 M    & ~~8.7 G   &    ~~463 M      &   52.3 \%   &  	    +1.9 \%   &   62.6 \%   \\
                                                   & 2  Conv4     & 7.581 M    & ~~9.2 G   &    ~~925 M      &   52.5 \%   &  	    +2.1 \%   &   63.1 \%   \\
                                                   & 4  Conv3\&4  & 7.719 M    &  10.1 G   &     1.85 G      &   \textbf{53.0} \%   &  	    \textbf{+2.6} \%   &   \textbf{63.5} \%   \\
  \bottomrule
\end{tabular}
}
\label{tab:multiple_block}
\end{table}


\subsection{Experiments on Image Recognition}

We evaluate the proposed -Net on ImageNet-1k~\cite{krizhevsky2012imagenet} image classification dataset, which contains more than 1.2 million high resolution images in  categories. Our implementation is based on the code released by \cite{chen2017dual} using  GPUs with a batch size of . The base learning rate is set to  and decreases with a factor of  when training accuracy is saturated. 

\begin{table}
\parbox{0.4\linewidth}{
    \caption{Comparison with state-of-the-arts on ImageNet-1k.}
    \centering
    \resizebox{0.4\columnwidth}{!}{
      \begin{tabular}{cc|c|c}
      \toprule
      Model                                            &    Backbone    &    Top-1     &    Top-5     \\
      \midrule
      \multirow{2}{*}{ResNet~\cite{he2016deep}}        &   ResNet-50    &    75.3 \%   &    92.2 \%   \\
                                                       &   ResNet-152   &    77.0 \%   &    93.3 \%   \\
      \midrule                                                   
      SENet~\cite{hu2017}                              &   ResNet-50    &    76.7 \%   &    93.4 \%   \\
      \midrule
      -Net					          		       &   ResNet-50    &    \textbf{77.0 }\%   &    \textbf{93.5} \%   \\
      \bottomrule
    \end{tabular}
    }
    \label{tab:imnet-1k}
}
\hfill
\parbox{0.58\linewidth}{
	\caption{Comparisons with state-of-the-arts results on Kinetics. Only RGB information is used for input.}
    \resizebox{0.58\columnwidth}{!}{
      \begin{tabular}{ccc|c|c}
      \toprule
      Model                       			&  \#Frames  &    FLOPs		&	Video 1  &  Video 5 	\\
      \midrule
	  ConvNet+LSTM~\cite{carreira2017quo}	& 	--	     & 	    --		&	  63.3 \%	&	   --	    \\
      \midrule
      I3D~\cite{carreira2017quo}  			&      64    &   107.9 G	&	  71.1 \%   &     89.3 \%   \\
	  R(2+1)D~\cite{tran2017closer} 		&      32    &   152.4 G	&	  72.0 \%  	&     90.0 \%   \\
      \midrule
      -Net      			  			&       8    &    40.8 G	& \textbf{74.6} \% & \textbf{91.5} \%  \\
      \bottomrule
    \end{tabular}
    }
    \label{tab:kinetics-sota}
}
\end{table}

As can be seen from Table~\ref{tab:imnet-1k}, a ResNet-50 equipped with 5 extra -blocks at Conv3 and Conv4 outperforms a much larger ResNet-152 architecture. We note that the -blocks embedded ResNet-50 is also over 40\% more efficient than ResNet-152 and only costs  GFLOPs and  M parameters. Compared with the SENet~\cite{hu2017}, the -Net also achieves better accuracy which proves the effectiveness of the proposed double attention mechanism.


\subsection{Experiment Results on Video Recognition}

In this subsection, we evaluate the proposed method on learning video representations. We consider the scenario where static image features are pretrained but motion features are learned from scratch by training a model on the large-scale Kinetics~\cite{carreira2017quo} dataset, and the scenario where well-trained motion features are transfered to small-scale UCF-101~\cite{soomro2012ucf101} dataset.


\paragraph{Learning Motion from Scratch on Kinetics}
We use ResNet-50 pretrained on ImageNet and add 5 randomly initialized -blocks to build the 3D convolutional network. The corresponding backbone is shown in Table~\ref{tab:backbone_cnns}. The network takes 8 frames (sampling stride: 8) as input and is trained for  iterations with a total batch size of  using  GPUs. The initial learning rate is set to  and decreased in a stepwise manner when training accuracy is saturated. The final result is shown in Table~\ref{tab:kinetics-sota}. Compared with the state-of-the-art I3D~\cite{carreira2017quo} and R(2+1)D~\cite{tran2017closer}, our proposed model shows higher accuracy even with a less number of sampled frames, which once again confirms the superiority of the proposed double-attention mechanism.


\paragraph{Transfer the Learned Feature to UCF-101}
The UCF-101 contains about  videos from 101 action categories and has three train/test splits. The training set of UCF-101 is several times smaller than the Kinetics dataset and we use it to evaluate the generality and robustness of the features learned by our model pre-trained on Kinetics. The network is trained with a base learning rate of  which is decreased for three times with a factor , using 8 GPUs with a batch size of 104 clips and tested with  input resolution on single scale. Table~\ref{tab:ucf-hmdb_sota} shows results of our proposed model and comparison with state-of-the-arts. Consistent with above results, the -Net achieves leading performance with significantly lower computational cost. This shows that the features learned by -Net are robust and can be effectively transfered to new dataset in very low cost compared with existing methods. 


\begin{table}[t]
\setlength{\tabcolsep}{10pt}
\centering
\caption{Comparisons with state-of-the-arts results on UCF-101. The averaged Top-1 video accuracy on three train/test splits is reported.}
\resizebox{1\columnwidth}{!}{
  \begin{tabular}{>{\centering}p{5cm}|>{\centering}p{3cm}|>{\centering}p{2.5cm}|c}
  \midrule
  Method     							&    Backbone   &     FLOPs     &  ~~~~Video 1~~~~	\\
  \midrule
  C3D~\cite{tran2015learning}        	&      VGG      &     38.5 G    &     82.3 \%   	\\
  Res3D~\cite{tran2017convnet}	   		& 	ResNet-18   &     19.3 G    &     85.8 \%   	\\
  I3D-RGB~\cite{carreira2017quo}	   	& 	Inception   &    107.9 G    &     95.6 \%   	\\
  R(2+1)D-RGB~\cite{tran2017closer}  	& 	ResNet-34   &    152.4 G    &     96.8 \%   	\\
  \midrule
  -Net                 			& 	ResNet-50   &     41.6 G 	&     96.4 \%  	    \\
  \bottomrule
  \end{tabular}
}
\label{tab:ucf-hmdb_sota}
\end{table}






 
\section{Conclusions}
\label{sec:conclusions}
In this work, we proposed a double attention mechanism for deep CNNs to overcome the limitation of local convolution operations. The proposed double attention method effectively captures the global information  and distributes it to every location in a two-step attention manner. We well formulated   the proposed method and instantiated it as an light-weight block that can be easily inserted into to existing CNNs with little computational overhead. Extensive ablation studies and experiments on a number of benchmark datasets, including ImageNet-1k, Kinetics and UCF-101, confirmed the effectiveness of the proposed -Net on both 2D image recognition tasks and 3D video recognition tasks. In the future, we want to explore integrating the double attention in recent compact network architectures~\cite{sandler2018inverted, ma2018shufflenet, chen2018multifiber}, to leverage the expressiveness of the proposed method for smaller, mobile-friendly models.
 


\bibliographystyle{plain}
\bibliography{reference}

\end{document}
