

\documentclass{article}


\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} 

\newcommand{\s}[1]{{\color{magenta} [SE: {#1}]}}
\newcommand{\ag}[1]{{\color{blue} [AG: {#1}]}}

\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}
 
\usepackage{tikz,tkz-graph}
\usetikzlibrary{
  graphs,
  graphs.standard,
  positioning,chains,fit,shapes,calc
}

\newcommand{\name}{Graphite}

\usepackage{booktabs}       \usepackage{nicefrac}       

\usepackage{amsmath,amsfonts,bbm,comment,mathtools,subcaption,amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{property}[theorem]{Property} 


\usepackage[accepted]{icml2019}

\icmltitlerunning{Graphite: Iterative Generative Modeling of Graphs}

\begin{document}

\twocolumn[
\icmltitle{Graphite: Iterative Generative Modeling of Graphs}



\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Aditya Grover}{sta}
\icmlauthor{Aaron Zweig}{sta}
\icmlauthor{Stefano Ermon}{sta}
\end{icmlauthorlist}


\icmlaffiliation{sta}{Department of Computer Science, Stanford University, USA}

\icmlcorrespondingauthor{Aditya Grover}{adityag@cs.stanford.edu}


\icmlkeywords{Machine Learning, Unsupervised Learning, Graphs, Generative Modeling, Deep Learning}

\vskip 0.3in
]
\printAffiliationsAndNotice{}

\begin{abstract}
  Graphs are a fundamental abstraction for modeling relational data. 
  However, graphs are discrete and combinatorial in nature, and learning representations suitable for machine learning tasks poses statistical and computational challenges. 
  In this work, we propose \textit{\name{}}, an algorithmic framework for unsupervised learning of representations over nodes in large graphs using deep latent variable generative models. Our model parameterizes variational autoencoders (VAE) with graph neural networks, and uses a novel iterative graph refinement strategy inspired by low-rank approximations for decoding.
On a wide variety of synthetic and benchmark datasets, \name{} outperforms competing approaches for the tasks of density estimation, link prediction, and node classification. 
Finally, we derive a theoretical connection between message passing in graph neural networks and mean-field variational inference.
\end{abstract}


\section{Introduction}

Latent variable generative modeling is an effective approach for unsupervised representation learning of high-dimensional data~\citep{loehlin1998latent}. In recent years, representations learned by latent variable models parameterized by deep neural networks have shown impressive performance on many tasks such as semi-supervised learning and structured prediction~\citep{kingma2014semi,sohn2015learning}. However, these successes have been largely restricted to specific data modalities such as images and speech. In particular, it is challenging to apply current deep generative models for large scale graph-structured data which arise in a wide variety of domains in physical sciences, information sciences, and social sciences. 

To effectively model the relational structure of large graphs for deep learning, prior works have proposed to use \textit{graph neural networks}~\citep{gori2005new,scarselli2009graph,bruna2013spectral}. A graph neural network learns node-level representations by parameterizing an iterative message passing procedure between nodes and their neighbors.  
The tasks which have benefited from graph neural networks, including semi-supervised learning~\citep{kipf2016semi} and few shot learning~\citep{garcia2017few}, involve \textit{encoding} an input graph to a final output representation (such as the labels associated with the nodes). The inverse problem of learning to \textit{decode} a hidden representation into a graph, as in the case of a latent variable generative model, is a pressing challenge
that we address in this work. 

We propose \textit{\name{}}, a latent variable generative model 
for graphs based on variational autoencoding~\citep{kingma2013auto}. Specifically, we learn a directed model expressing a joint distribution over the entries of adjacency matrix of graphs 
and latent feature vectors for every node.
Our framework uses graph neural networks for inference (encoding) and generation (decoding).  
While the encoding is straightforward and can use any existing graph neural network, the decoding of these latent features to reconstruct the original graph is done using a multi-layer iterative procedure. The procedure starts with an initial reconstruction based on the inferred latent features, and iteratively refines the reconstructed graph via a message passing operation. 
The iterative refinement can be efficiently implemented using graph neural networks. 
In addition to the \name{} model, we also contribute to the theoretical understanding of graph neural networks by deriving equivalences between message passing in graph neural networks with mean-field inference in latent variable models via kernel embeddings~\citep{smola2007hilbert,dai2016discriminative}, formalizing what has thus far has been largely speculated empirically to the best of our knowledge~\citep{yoon2018inference}.

In contrast to recent works focussing on generation of small graphs \textit{e.g.}, molecules
\citep{you2018graphrnn,li2018learning}, the \name{} framework is particularly suited for representation learning on large graphs. Such representations are useful for several downstream tasks. In particular, we demonstrate that representations learned via \name{} outperform competing approaches for graph representation learning empirically for the tasks of density estimation (over entire graphs), link prediction, and semi-supervised node classification on synthetic and benchmark datasets. 

 \section{Preliminaries}
Throughout this work, we assume that all probability distributions admit absolutely continuous densities on a suitable reference measure. 
Consider a weighted undirected graph  where  and  denote index sets of nodes and edges respectively.
Additionally, we denote the (optional) feature matrix associated with the graph as  for an -dimensional signal associated with each node, for \textit{e.g.}, these could refer to the user attributes in a social network.
We represent the graph structure using a symmetric adjacency matrix  where  and the entries  denote the weight of the edge between node  and .

\subsection{Weisfeiler-Lehman algorithm}
The 
Weisfeiler-Lehman (WL) algorithm~\citep{weisfeiler1968reduction,douglas2011weisfeiler} is a heuristic test of graph isomorphism between any two graphs  and . The algorithm proceeds in iterations. 
Before the first iteration, we label every node in  and  with a scalar \textit{isomorphism invariant} initialization (\textit{e.g.}, node degrees). That is, if  and  are assumed to be isomorphic, then an isomorphism invariant initialization is one where the matching nodes establishing the isomorphism in  and  have the same labels (a.k.a. messages).
Let  denote the vector of initializations 
for the nodes in the graph at iteration .
 At every iteration , we perform a relabelling of nodes in  and  based on a message passing update rule:

where  denotes the adjacency matrix of the corresponding graph and  is any suitable hash function \textit{e.g.}, a non-linear activation. Hence, the message 
for every node is computed as a hashed sum of the messages from the neighboring nodes (since  only if  and  are neighbors). We repeat the process for a specified number of iterations, or until convergence. If the label sets for the nodes in  and  are equal (which can be checked using sorting in  time), then the algorithm declares the two graphs  and  to be isomorphic. 

The ``-dim" WL algorithm extends the 1-dim algorithm above by simultaneously passing messages of length  (each initialized with some isomorphism invariant scheme).
A positive test for isomorphism requires equality in all  dimensions for nodes in  and  after the termination of message passing. 
This algorithmic test is a heuristic which guarantees no false negatives but can give false positives wherein two non-isomorphic graphs can be falsely declared isomorphic. 
Empirically, the test has been shown to fail on some regular graphs but gives excellent performance on real-world graphs~\citep{shervashidze2011weisfeiler}.

\subsection{Graph neural networks}
Intuitively, the WL algorithm encodes the structure of the graph in the form of messages at every node.
Graph neural networks (GNN) build on this observation and parameterize an unfolding of the iterative message passing procedure which we describe next.


A GNN consists of many layers, indexed by  with each layer associated with an activation  and a dimensionality . In addition to the input graph , every layer  of the GNN takes as input the activations from the previous layer , a family of linear transformations , and a matrix of learnable weight parameters  and optional bias parameters . Recursively, the layer wise propagation rule in a GNN is given by:

with the base cases  and .
Here,  is the feature dimensionality.
If there are no explicit node features, we set  (identity) and .  
Several variants of graph neural networks have been proposed in prior work. For instance, graph convolutional networks (GCN) \cite{kipf2016semi} instantiate graph neural networks with a permutation equivariant propagation rule:

where  is the symmetric diagonalization of  given the diagonal degree matrix  (\textit{i.e.}, ), and same base cases as before.
Comparing the above with the WL update rule in Eq.~\eqref{eq:wl_mp}, we can see that the activations for every layer in a GCN are computed via parameterized, scaled activations (messages) of the previous layer being propagated over the graph, with the hash function implicitly specified using an activation function . 

Our framework is agnostic to  instantiations of message passing rule of a graph neural network in Eq.~\eqref{eq:gnn_mp}, and we use graph convolutional networks for experimental validation due to the permutation equivariance property. For brevity, we denote the output  for the final layer of a multi-layer graph neural network with input adjacency matrix , node feature matrix , and parameters  as , with appropriate activation functions and linear transformations applied at each hidden layer of the network.
 \section{Generative Modeling via \name{}}\label{sec:framework}


For generative modeling of graphs, we are interested in learning a parameterized distribution over adjacency matrices . In this work, we restrict ourselves to modeling graph \textit{structure} only, and any additional information in the form of node features  is incorporated as conditioning evidence.

\begin{figure}[t]
\centering
\includegraphics[width=0.45\textwidth]{images/lvm.png}
\caption{Latent variable model for \name{}. Observed evidence variables in gray.}\label{fig:lvm}
\end{figure}
In \name{}, we adopt a latent variable approach for modeling the generative process. That is, we introduce latent variable vectors  and evidence feature vectors 
 for each node  along with an observed variable for each pair of nodes . Unless necessary, we use a succinct representation , , and  for the variables henceforth. 
The conditional independencies between the variables can be summarized in the directed graphical model (using plate notation) in Figure~\ref{fig:lvm}. We can learn the model parameters  by maximizing the marginal likelihood of the observed adjacency matrix conditioned on :


Here,  is a fixed prior distribution over the latent features of every node \textit{e.g.}, isotropic Gaussian.
If we have multiple graphs in our dataset, we maximize the expected log-likelihoods over all the corresponding adjacency matrices. We can obtain a tractable, stochastic evidence lower bound (ELBO) to the above objective by
introducing a variational posterior  with parameters : 


The lower bound is tight when the variational posterior  matches the true posterior  and hence maximizing the above objective optimizes for the parameters that define the best approximation to the true posterior within the variational family~\citep{blei2017variational}.
We now discuss parameterizations for specifying   (\textit{i.e.}, encoder) and  (\textit{i.e.}, decoder).


\paragraph{Encoding using forward message passing.}

Typically we use the mean field approximation for defining the variational family and hence:

Additionally, we would like to make distributional assumptions on each variational marginal density  such that it is reparameterizable and easy-to-sample, such that the gradients w.r.t.  have low variance~\citep{kingma2013auto}. In \name{}, we assume isotropic Gaussian variational marginals with diagonal covariance.
The parameters for the variational marginals  are specified using a graph neural network: 

where  and  denote the vector of means and standard deviations for the variational marginals  and  are the full set of variational parameters.

\paragraph{Decoding using reverse message passing.} For specifying the observation model , we cannot directly use a graph neural network since we do not have an input graph for message passing. To sidestep this issue, we propose an iterative two-step approach that alternates between defining an intermediate graph and then gradually refining this graph through message passing.
Formally, given a latent matrix  and an input feature matrix , we iterate over the following sequence of operations:

where the second argument to the GNN is a concatenation of   and . The first step constructs an intermediate weighted graph  by applying an inner product of  with itself and adding an additional constant of 1 to ensure entries are non-negative. 
And the second step performs a pass through a parameterized graph neural network. We can repeat the above sequence to gradually refine the feature matrix . The final distribution over graph parameters is obtained using an inner product step on  akin to Eq.~\eqref{eq:int_graph_decoding}, where  is determined via the network architecture.
For efficient sampling, we assume the observation model factorizes:

The distribution over the individual edges can be expressed as a Bernoulli or Gaussian distribution for unweighted and real-valued edges respectively. E.g., the edge probabilities for an unweighted graph are given as .




\begin{table*}[t]
  \caption{Mean reconstruction errors and negative log-likelihood estimates (in nats) for autoencoders and variational autoencoders respectively on test instances from six different generative families. Lower is better.
  }
    \vspace{0.05in}
  \label{table-elbo}
  \centering
  \resizebox{\textwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|}
    \toprule
	& Erdos-Renyi & Ego & Regular & Geometric & Power Law & Barabasi-Albert \\
    \midrule
    GAE & 221.79  7.58 & 197.3  1.99 & 198.5  4.78 & 514.26  41.58 & 519.44  36.30 & 236.29  15.13\\
        \name{}-AE & \textbf{195.56}  1.49 & \textbf{182.79}  1.45 & \textbf{191.41}  1.99 & \textbf{181.14}  4.48 & \textbf{201.22}  2.42 & \textbf{192.38}  1.61\\
    \midrule
    VGAE & 273.82  0.07 & 273.76  0.06 & 275.29  0.08 & 274.09  0.06 & 278.86  0.12 & 274.4  0.08\\
    \name{}-VAE & \textbf{270.22}  0.15 & \textbf{270.70}  0.32 & \textbf{266.54}  0.12 & \textbf{269.71}  0.08 & \textbf{263.92}  0.14 & \textbf{268.73}  0.09\\

    \bottomrule
  \end{tabular}}
\end{table*}

\subsection{Scalable learning \& inference in \name{}} 

For representation learning of large graphs, we require the encoding and decoding steps  in \name{} to be computationally efficient. On the surface, the decoding step involves inner products of potentially dense matrices , which is an  operation. 
Here,  is the dimension of the per-node latent vectors  used to define . 

For any intermediate decoding step as in Eq.~\eqref{eq:int_graph_decoding}, we propose to offset this expensive computation by using the associativity property of matrix multiplications for the message passing step in Eq.~\eqref{eq:refine_graph_dec}. For notational brevity, consider the simplified graph propagation rule for a GNN:

where  is defined in Eq.~\eqref{eq:int_graph_decoding}. 

Instead of directly taking an inner product of  with itself, we note that the subsequent operation involves another matrix multiplication and hence, we can perform right multiplication instead.
If  and  denote the size of the layers  and  respectively, then the time complexity of propagation based on right multiplication is given by . 

The above trick sidesteps the quadratic  complexity for decoding in the intermediate layers without any loss in statistical accuracy. The final layer however still involves an inner product with respect to  between potentially dense matrices. However, since the edges are generated independently, we can approximate the loss objective by performing a Monte Carlo evaluation of the reconstructed adjacency matrix parameters in Eq.~\eqref{eq:dec_mf}. By adaptively choosing the number of entries for Monte Carlo approximation, we can trade-off statistical accuracy for computational budget.  
\section{Experimental Evaluation}\label{sec:exps}

We evaluate \name{} on tasks involving entire graphs, nodes, and edges. 
We consider two variants of our proposed framework: the \textit{\name{}-VAE}, which corresponds to a directed latent variable model as described in Section~\ref{sec:framework} and \textit{\name{}-AE}, which corresponds to an autoencoder trained to minimize the error in reconstructing an input adjacency matrix. For unweighted graphs (\textit{i.e.}, ), the reconstruction terms in the objectives for both \name{}-VAE and \name{}-AE minimize the negative cross entropy between the input and reconstructed adjacency matrices. For weighted graphs, we use the mean squared error. Additional hyperparameter details are described in Appendix~\ref{app:expt}.


\subsection{Reconstruction \& density estimation}

In the first set of tasks, we evaluate learning in \name{} based on held-out reconstruction losses and log-likelihoods estimated by the learned \name{}-VAE and \name{}-AE models respectively on a collection of graphs with varying sizes. In direct contrast to modalities such as images, graphs cannot be straightforwardly reduced to a fixed number of vertices for input to a graph convolutional network. One simplifying modification taken by \citet{bojchevski2018netgan} is to consider only the largest connected component for evaluating and optimizing the objective, which we appeal to as well.  Thus by setting the dimensions of  to a maximum number of vertices, \name{} can be used for inference tasks over entire graphs with potentially smaller sizes by considering only the largest connected component.

We create datasets from six graph families with fixed, known generative processes: the Erdos-Renyi, ego-nets, random regular graphs,  random geometric graphs, random Power Law Tree and Barabasi-Albert. For each family, 300 graph instances were sampled with each instance having  nodes and evenly split into train/validation/test instances. As a benchmark comparison, we compare against the Graph Autoencoder/Variational Graph Autoencoder (GAE/VGAE)~\citep{kipf2016variational}. The GAE/VGAE models consist of an encoding procedure similar to \name{}. However, the decoder has no learnable parameters and reconstruction is done solely through an inner product operation (such as the one in Eq.~\eqref{eq:int_graph_decoding}). 

The mean reconstruction errors and the negative log-likelihood results on a test set of instances are shown in Table~\ref{table-elbo}. Both \name{}-AE and \name{}-VAE outperform AE and VGAE significantly on these tasks, indicating the usefulness of learned decoders in \name{}. 


\begin{table}[t]
\centering
  \caption{Citation network statistics}
  \label{table-stats}
  \vspace{0.05in}
  \centering
  \begin{tabular}{|c|c|c|c|c|}
    \toprule
	& Nodes & Edges & Node Features & Labels \\
    \midrule
    Cora & 2708 & 5429 & 1433 & 7\\
    Citeseer & 3327 & 4732 & 3703 & 6\\
    Pubmed & 19717 & 44338 & 500 & 3\\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table*}[t]
  \caption{Area Under the ROC Curve (AUC) for link prediction (* denotes dataset with features). Higher is better.
  }
  \label{table-auc}
   \vspace{0.05in}
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|}
    \toprule
	& Cora &  Citeseer & Pubmed & Cora* & Citeseer* & Pubmed* \\
    \midrule
    SC & 89.9  0.20 & 91.5  0.17& \textbf{94.9}  0.04 & - & - & -\\
    DeepWalk & 85.0  0.17& 88.6  0.15& 91.5  0.04& - & - & -\\
    node2vec & 85.6  0.15& 89.4  0.14& 91.9  0.04 & - & - & -\\
    GAE & 90.2  0.16& 92.0  0.14& 92.5  0.06& 93.9  0.11& 94.9  0.13& 96.8  0.04\\
    VGAE & 90.1  0.15& 92.0  0.17& 92.3  0.06 & 94.1  0.11& 96.7  0.08& 95.5  0.13\\
    \midrule
    \name{}-AE & 91.0  0.15 & 92.6  0.16& 94.5  0.05& 94.2  0.13& 96.2  0.10& \textbf{97.8}  0.03 \\
    \name-VAE & \textbf{91.5}  0.15 & \textbf{93.5}  0.13 & 94.6  0.04& \textbf{94.7}  0.11 & \textbf{97.3}  0.06 & 97.4  0.04\\
    \bottomrule
  \end{tabular}
\end{table*}

\normalsize
\begin{table*}[t]
  \caption{Average Precision (AP) scores for link prediction (* denotes dataset with features). Higher is better.}
  \label{table-ap}
  \centering
   \vspace{0.05in}
  \begin{tabular}{|c|c|c|c|c|c|c|}
    \toprule
	& Cora & Citeseer & Pubmed & Cora* & Citeseer* & Pubmed* \\
    \midrule
    SC & 92.8  0.12 & 94.4  0.11& \textbf{96.0}  0.03 & - & - & -\\
    DeepWalk & 86.6  0.17& 90.3  0.12& 91.9  0.05& - & - & -\\
    node2vec & 87.5  0.14& 91.3  0.13& 92.3  0.05 & - & - & -\\
    GAE & 92.4  0.12& 94.0  0.12& 94.3  0.5& 94.3  0.12& 94.8  0.15& 96.8  0.04\\
    VGAE & 92.3  0.12& 94.2  0.12& 94.2  0.04& 94.6  0.11& 97.0  0.08& 95.5  0.12\\
    \midrule
\name-AE & 92.8  0.13& 94.1  0.14& 95.7  0.06& 94.5  0.14& 96.1  0.12& \textbf{97.7}  0.03 \\
\name-VAE & \textbf{93.2}  0.13 & \textbf{95.0}  0.10 & \textbf{96.0}  0.03 & \textbf{94.9}  0.13 & \textbf{97.4}  0.06 & 97.4  0.04\\

    \bottomrule
  \end{tabular}
\end{table*}

\normalsize

\begin{figure}[ht]
\centering
\begin{subfigure}[b]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{images/tsne_ae}
\caption{Graphite-AE}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{images/tsne_vae}
\caption{Graphite-VAE}
\end{subfigure}
\caption{t-SNE embeddings of the latent feature vectors for the Cora dataset. Colors denote labels.}\label{fig:tsne_clustering} 
\vspace{-0.05in}
\end{figure}


\subsection{Link prediction}

The task of link prediction is to predict whether an edge exists between a pair of nodes~\citep{loehlin1998latent}. Even though \name{} learns a distribution over graphs, it can be used for predictive tasks within a \textit{single} graph. In order to do so, we learn a model for a random, connected training subgraph of the true graph. For validation and testing, we add a balanced set of positive and negative (false) edges to the original graph and evaluate the model performance based on the reconstruction probabilities assigned to the validation and test edges (similar to \textit{denoising} of the input graph). In our experiments, we held out a set of  edges for validation,  edges for testing, and train all models on the remaining subgraph. Additionally, the validation and testing sets also each contain an equal number of non-edges.

 \paragraph{Datasets.} We compared across standard benchmark citation network datasets: Cora, Citeseer, and Pubmed with papers as nodes and citations as edges~\citep{sen2008networks}. 
 The node-level features correspond to the text attributes in the papers.
The dataset statistics are summarized in Table~\ref{table-stats}.

\paragraph{Baselines and evaluation metrics.} We evaluate performance based on the Area Under the ROC Curve (AUC) and Average Precision (AP) metrics. We evaluated \name{}-VAE and \name{}-AE against the following baselines: Spectral Clustering (SC)~\citep{tang2011leveraging}, DeepWalk~\citep{perozzi2014deepwalk}, node2vec~\citep{grover2016node2vec}, and GAE/VGAE~\citep{kipf2016variational}. SC, DeepWalk, and node2vec do not provide the ability to incorporate node features while learning embeddings, and hence we evaluate them only on the featureless datasets. 



\paragraph{Results.} The AUC and AP results (along with standard errors) are shown in Table~\ref{table-auc} and Table~\ref{table-ap} respectively averaged over 50 random train/validation/test splits. 
On both metrics, \name{}-VAE gives the best performance overall. \name{}-AE also gives good results, generally outperforming its closest competitor GAE.  




\paragraph{Qualitative evaluation.} We  visualize the embeddings learned by \name{} and given by a 2D t-SNE projection~\citep{maaten2008visualizing} of the latent feature vectors (given as rows for  with ) on the Cora dataset in Figure~\ref{fig:tsne_clustering}. Even without any access to label information for the nodes during training, the name{} models are able to cluster the nodes (papers) as per their labels (paper categories).



\subsection{Semi-supervised node classification}


Given labels for a subset of nodes in an underlying graph, the goal of this task is to predict the labels for the remaining nodes. We consider a \textit{transductive} setting, where we have access to the test nodes (without their labels) during training.  

Closest approach to \name{} for this task is a supervised graph convolutional network (GCN) trained end-to-end. We consider an extension of this baseline, wherein we augment the GCN objective with the \name{} objective and a hyperparameter to control the relative importance of the two terms in the combined objective. The parameters  for the encoder are shared across these two objectives, with an additional GCN layer for mapping the encoder output to softmax probabilities over the requisite number of classes. All parameters are learned jointly.

\paragraph{Results.} The classification accuracy of the semi-supervised models is given in Table~\ref{table-acc}. We find that \name{}-hybrid outperforms the competing models on all datasets and in particular the GCN approach which is the closest baseline. Recent work in Graph Attention Networks shows that extending GCN by incoporating attention can boost performance on this task~\citep{velickovic2018graph}. 
Using GATs in place of GCNs for parameterizing \name{} could yield similar performance boost in future work.



\begin{table}[t]
\centering
  \caption{Classification accuracies (* denotes dataset with features). Baseline numbers from~\citet{kipf2016semi}.}
  \label{table-acc}
  \centering
\vspace{0.05in}
  \begin{tabular}{|c|c|c|c|}
    \toprule
	& Cora* & Citeseer* & Pubmed* \\
    \midrule
    SemiEmb & 59.0 & 59.6 & 71.1 \\
    DeepWalk & 67.2 & 43.2 & 65.3 \\
    ICA & 75.1 & 69.1 & 73.9 \\
    Planetoid & 75.7 & 64.7 & 77.2 \\
    GCN & 81.5 & 70.3 & 79.0\\ \midrule
    Graphite & \textbf{82.1}  0.06 & \textbf{71.0}  0.07 & \textbf{79.3}  0.03\\
    \bottomrule
  \end{tabular}
\end{table}
 
\section{ Theoretical Analysis}\label{sec:interpret}

In this section, we derive a theoretical connection between message passing in graph neural networks and approximate inference in related undirected graphical models. 

\subsection{Kernel embeddings}
We first provide a brief background on kernel embeddings.
A kernel defines a notion of similarity between pairs of objects~\citep{scholkopf2002learning,shawe2004kernel}.
Let  be the kernel function defined over a space of objects, say . 
With every kernel function , we have an associated feature map  where  is a potentially infinite dimensional feature space.


Kernel methods can be used to specify embeddings of \textit{distributions} of arbitrary objects~\citep{smola2007hilbert,gretton2007kernel}. 
Formally, we denote these functional mappings as  where  specifies the space of all distributions on . 
These mappings, referred to as kernel embeddings of distributions, are defined as:

for any .
We are particularly interested in kernels with feature maps  that define injective embeddings, \textit{i.e.}, for any pair of distributions  and , we have  if . 
For injective embeddings, 
we can compute functionals of any distribution by directly applying a corresponding function on its embedding. Formally, for every function ,  and injective embedding , there exists a function  such that:

Informally, we can see that the operator  can be defined as the composition of  with the inverse of .


\subsection{Connections with mean-field inference}

\begin{figure}[t]
\centering
\begin{subfigure}[b]{0.45\textwidth}
\centering
\begin{tikzpicture}
 \node[circle, blue, text=black] (a) at (0.5,1) [draw, minimum width=0.5cm,minimum height=0.5cm] {2};
 \node[circle, blue, text=black] (b) at (1.5,1) [draw, minimum width=0.5cm,minimum height=0.5cm] {3};
 \node[circle, blue, text=black] (c) at (1,2) [draw, minimum width=0.5cm,minimum height=0.5cm] {1};
 \foreach \from/\to in {a/c, c/b}
\draw [-, blue] (\from) -- (\to);
    \end{tikzpicture}
\caption{Input graph with edge set .}\label{fig:lvm_enc_graph}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
\centering
\begin{tikzpicture}
\node[circle, black, text=black] (d) at (0,1) [draw, minimum width=0.15cm,minimum height=0.15cm]{};
 \node[circle, blue, text=black] (a) at (1,1) [draw, minimum width=0.15cm,minimum height=0.15cm] {};
 \node[circle, black, text=black] (e) at (4,1) [draw, minimum width=0.15cm,minimum height=0.15cm]{};
 \node[circle, blue, text=black] (b) at (3,1) [draw, minimum width=0.15cm,minimum height=0.15cm] {};
  \node[circle, black, text=black] (f) at (2,3) [draw, minimum width=0.05cm,minimum height=0.05cm]{};
 \node[circle, blue, text=black] (c) at (2,2) [draw, minimum width=0.05cm,minimum height=0.05cm] {};
  \node[circle, black, text=black] (g) at (1,2.5) [draw, minimum width=0.35cm,minimum height=0.35cm]
  {};
  \node[circle, black, text=black] (h) at (2,0.5) [draw, minimum width=0.35cm,minimum height=0.35cm]{};
  \node[circle, black, text=black] (i) at (3,2.5) [draw, minimum width=0.35cm,minimum height=0.35cm]{};
 \foreach \from/\to in {a/c, c/b}
\draw [-, blue] (\from) -- (\to);
 \foreach \from/\to in {d/a, e/b, f/c}
\draw [-, black] (\from) -- (\to);
\foreach \from/\to in {g/a, g/c, h/a, h/b, i/b, i/c}
\draw [-, black] (\from) -- (\to);
    \end{tikzpicture}
\caption{Latent variable model  satisfying Property~\ref{thm:imap} with  .}\label{fig:lvm_enc_model}
\end{subfigure}
\caption{Interpreting message passing in Graph Neural Networks via Kernel Embeddings and Mean-field inference}\label{fig:lvm_enc}
\end{figure}

Locality preference for representational learning is a key inductive bias for graphs. We formulate this using an (undirected) graphical model  over ,  , and .
As in a GNN, we assume that  and  are observed and specify conditional independence structure in 
a
conditional distribution over the latent variables, denoted as . We are particularly interested in models that satisfy the following property.
\begin{property}\label{thm:imap}
The edge set  defined by the adjacency matrix   is an undirected I-map for the 
distribution .
\end{property}

In words, the above property implies that according to the conditional distribution over , any individual   is independent of all other  
when conditioned on , , and the neighboring latent variables of node  as determined by the edge set . See Figure~\ref{fig:lvm_enc} for an illustration. 

A mean-field (MF) approximation for   approximates the conditional distribution   as:

where  denotes the set of parameters for the -th variational 
marginal.
These parameters are optimized by minimizing the KL-divergence between the variational 
and the true 
conditional distributions:


Using standard variational arguments~\citep{wainwright2008graphical}, we know that the optimal variational marginals  assume the following functional form:

where  denotes the neighbors of  in  
and  is a function determined by the fixed point equations that depends on the potentials associated with . 
Importantly, the above functional form suggests that the optimal marginals in mean field inference are locally consistent that they are only a function of the neighboring marginals.
An iterative algorithm for mean-field inference is to perform message passing over the underlying graph until convergence. With an appropriate initialization at , the updated marginals at iteration  are given as:


We will sidestep deriving , and instead use the kernel embeddings of the variational marginals to directly reason in the embedding space. 
That is, we assume we have an injective embedding for each marginal  given by  for some feature map  
and directly use the equivalence established in Eq.~\eqref{eq:kernel_op} iteratively. For mean-field inference via message passing as in Eq.~\eqref{eq:marginal_mp}, this gives us the following recursive expression for iteratively updating the embeddings at iteration :

with an appropriate base case for  . 
We then have the following result:

\begin{theorem}\label{thm:gnn_imap}
Let  be any undirected latent variable model such that the conditional distribution  expressed by the model satisfies Property~\ref{thm:imap}. 

Then there exists a choice of , , 
, 
and  such that for all , the GNN propagation rule in Eq.~\eqref{eq:gnn_mp} is computationally equivalent to updating  via a first order approximation of Eq.~\eqref{eq:rec_mu}.  
\end{theorem}
\begin{proof}
See Appendix~\ref{app:enc}.
\end{proof}

While  and  are typically fixed beforehand, the parameters , 
and  are directly learned from data in practice. Hence we have shown that a GNN is a good model for computation with respect to latent variable models that attempt to capture inductive biases relevant to graphs, \textit{i.e.}, ones where the latent feature vector for every node is conditionally independent from everything else given the feature vectors of its neighbors (and , ). Note that such a graphical model would satisfy Property~\ref{thm:imap} but is in general different from the posterior specified by the one in Figure~\ref{fig:lvm}. However if the true (but unknown) posterior on the latent variables for the model proposed in Figure~\ref{fig:lvm} could be expressed as an equivalent model satisfying the desired property, then Theorem~\ref{thm:gnn_imap} indeed suggests the use of GNNs for parameterizing variational posteriors, as we do so in the case of \name{}.
 \section{Discussion \& Related Work}\label{sec:related}


Our framework effectively marries \textit{probabilistic modeling} and  \textit{representation learning} on graphs. We review some of the dominant prior works in these fields below.

\paragraph{Probabilistic modeling of graphs.}
The earliest \textit{probabilistic models of graphs} proposed to generate graphs by creating an edge between any pair of nodes with a constant probability~\citep{erdos1959random}. Several alternatives have been proposed since; \textit{e.g.}, the small-world model generates graphs that exhibit local clustering~\citep{watts1998collective}, the Barabasi-Albert models preferential attachment wherein high-degree nodes are likely to form edges with newly added nodes~\citep{barabasi1999random}, the stochastic block model is based on inter and intra community linkages~\citep{holland1983stochastic} etc.
 We direct the interested reader to prominent surveys on this topic~\citep{newman2003structure,mitzenmacher2004brief,chakrabarti2006graph}.

\paragraph{Representation learning on graphs.}
For \textit{representation learning on graphs}, there are broadly three kinds of approaches: matrix factorization, random walk based approaches, and graph neural networks. We include a brief discussion on the first two kinds in Appendix~\ref{app:related} and refer the reader to \citet{hamilton2017representation} for a recent survey. 


Graph neural networks, a collective term for networks that operate over graphs using message passing, have shown success on several downstream applications, e.g., ~\citep{duvenaud2015molecular,li2015gated,kearnes2016molecular,kipf2016semi,hamilton2017inductive} and the references therein. \citet{gilmer2017neural} provides a comprehensive characterization of these networks in the message passing setup. We used Graph Convolution Networks, partly to provide a direct comparison with GAE/VGAE and leave the exploration of other GNN variants for future work.

\paragraph{Latent variable models for graphs.} Hierarchical Bayesian models parameterized by deep neural networks have been recently proposed for graphs~\citep{hu2017deep,wang2017relational}. Besides being restricted to single graphs, these models are limited since inference requires running expensive Markov chains~\citep{hu2017deep} or are task-specific~\citep{wang2017relational}. \citet{johnson2017transitions} and \citet{kipf2018neural} generate graphs as latent representations learned directly from data. In contrast, we are interested in modeling observed (and not latent) relational structure. Finally, there has been a fair share of recent work for generation of special kinds of graphs, such as parsed trees of source code~\citep{maddison2014structured} and SMILES representations  for molecules~\citep{olivecrona2017denovo}. 

 Several deep generative models for graphs have recently been proposed. Amongst adversarial generation approaches, \citet{wang2017graphgan} and \citet{bojchevski2018netgan} model local graph neighborhoods and random walks on graphs respectively. \citet{li2018learning} and \citet{you2018graphrnn} model graphs as sequences and generate graphs via autoregressive procedures. Adversarial and autoregressive approaches are successful at generating graphs, but do not directly allow for inferring latent variables via encoders. Latent variable generative models have also been proposed for generating small molecular graphs~\citep{jin2018junction,samanta2018designing,simonovsky2018graphvae}. These methods involve an expensive decoding procedure that limits scaling to large graphs. Finally, closest to our framework is the GAE/VGAE approach \citep{kipf2016variational} discussed in Section~\ref{sec:exps}. \citet{pan2018adversarially} extends this approach with an adversarial regularization framework but retain the inner product decoder. Our work proposes a novel multi-step decoding mechanism based on graph refinement.
 
 
 \section{Conclusion \& Future Work}
We proposed \name{}, a scalable deep generative model for graphs based on variational autoencoding. The encoders and decoders in \name{} are parameterized by graph neural networks that propagate information locally on a graph. 
Our proposed decoder performs a multi-layer iterative decoding comprising of alternate inner product operations and message passing on the intermediate graph.

Current generative models for graphs are not permutation-invariant and are learned by feeding graphs with a fixed or heuristic ordering of nodes.
This is an exciting challenge for future work, which could potentially be resolved by incorporate graph representations robust to permutation invariances~\citep{verma2017hunt} or modeling distributions over permutations of node orderings via recent approaches such as NeuralSort~\citep{grover2019stochastic}.
Extending \name{} for modeling richer graphical structure such as heterogeneous and time-varying graphs, as well as integrating domain knowledge within \name{} decoders for applications in generative design and synthesis \textit{e.g.}, molecules, programs, and parse trees is another interesting future direction. 

Finally, our theoretical results in Section~\ref{sec:interpret} suggest that a principled design of layerwise propagation rules in graph neural networks inspired by additional message passing inference schemes~\citep{dai2016discriminative,gilmer2017neural} is another avenue for future research. 

 \section*{Acknowledgements}
 This research has been supported by Siemens, a Future of Life Institute grant, NSF grants (\#1651565, \#1522054, \#1733686), ONR (N00014-19-1-2145), AFOSR (FA9550-19-1-0024), and an Amazon AWS Machine Learning Grant. AG is supported by a Microsoft Research Ph.D. fellowship and a Stanford Data Science Scholarship. We would like to thank Daniel Levy for helpful comments on early drafts. 

\bibliography{refs}
\bibliographystyle{icml2019}

\clearpage
\appendix
\section*{Appendices}

\section{Proof of Theorem~\ref{thm:gnn_imap}}\label{app:enc}
\begin{proof}
For simplicity, we state the proof for a single variational marginal embedding  and consider that  for all  are unidimensional. 

Let us denote  to be the vector of neighboring kernel embeddings at iteration  such that the -th entry of  corresponds to  if  and zero otherwise.  Hence, we can rewrite Eq.~\eqref{eq:rec_mu} as:

where we have overloaded  to now denote a function that takes as argument an -dimensional vector of marginal embeddings.

Assuming that the function  is differentiable, a first-order Taylor expansion of Eq.~\eqref{eq:rec_mu_n} around the origin  is given by:



Since every marginal density is unidimensional, we now consider a GNN with a single activation per node in every layer, ,   for all . 
 This also implies that the bias can be expressed as an  -dimensional vector, \textit{i.e.},  and we have a single weight parameter .
For a single entry of , we can specify Eq.~\eqref{eq:gnn_mp} component-wise as:

where  denotes the -th row of  and is non-zero only for entries corresponding to the neighbors of node .

Now, consider the following instantiation of Eq.~\eqref{eq:gnn_mp_comp}:
\begin{itemize}
\item  (identity function)
\item 
\item A family of  transformations  where 
\item 
\item .
\end{itemize}

With the above substitutions, we can equate the first order approximation in Eq.~\eqref{eq:rec_mu_n} to the GNN message passing rule in Eq.~\eqref{eq:gnn_mp_comp}, thus completing the proof. With vectorized notation and use of matrix calculus in Eqs.~(\ref{eq:rec_mu_n}-\ref{eq:gnn_mp_comp}), the derivation above also applies to entire vectors of variational marginal embeddings with arbitrary dimensions.
\end{proof}















\section{Experiment Specifications}\label{app:expt}




\subsection{Link prediction}




We used the SC implementation from \citep{pedregosa2011scikit} and public implementations for others made available by the authors. For SC, we used a dimension size of . For DeepWalk and node2vec which uses a skipgram like objective on random walks from the graph, we used the same dimension size and default settings used in \citep{perozzi2014deepwalk} and \citep{grover2016node2vec} respectively of  random walks of length  per node and a context size of . For node2vec, we searched over the random walk bias parameters using a grid search in  as prescribed in the original work. For GAE and VGAE, we used the same architecture as VGAE and Adam optimizer with learning rate of . 

For Graphite-AE and Graphite-VAE, we used an architecture of 32-32 units for the encoder and 16-32-16 units for the decoder (two rounds of iterative decoding before a final inner product). The model is trained using the Adam optimizer~\citep{kingma2013auto} with a learning rate of . All activations were RELUs.The dropout rate (for edges) and  were tuned as hyperparameters on the validation set to optimize the AUC, whereas traditional dropout was set to 0 for all datasets. Additionally, we trained every model for  iterations and used the model checkpoint with the best validation loss for testing.  Scores are reported as an average of 50 runs with different train/validation/test splits (with the requirement that the training graph necessarily be connected).

For \name{}, we observed that using a form of skip connections to define a linear combination of the initial embedding  and the final embedding  is particularly useful. The skip connection consists of a tunable hyperparameter  controlling the relative weights of the embeddings.
The final embedding of \name{} is a function of the initial embedding  and the last induced embedding .  We consider two functions to aggregate them into a final embedding.  That is,  and , which correspond to a convex combination of two embeddings, and an incremental update to the initial embedding in a given direction, respectively. Note that in either case, GAE and VGAE reduce to a special case of \name{}, using only a single inner-product decoder (\textit{i.e.}, ).  On Cora and Pubmed final embeddings were derived through convex combination, on Citeseer through incremental update.


\paragraph{Scalability.} We experimented with learning VGAE and \name{} models by subsampling  random entries for Monte Carlo evaluation of the objective at each iteration. The corresponding AUC scores are shown in Table~\ref{table-scale}. The results suggest that \name{} can effectively scale to large graphs without significant loss in accuracy. 
The AUC results trained with edge subsampling as we vary the subsampling coefficient  are shown in Figure~\ref{fig:scalability}.


\begin{table}[t]
\centering
  \caption{AUC scores for link prediction with Monte Carlo subsampling during training. Higher is better.}
  \label{table-scale}
  \vspace{0.05in}
  \centering
  \begin{tabular}{|c|c|c|c|}
    \toprule
	& Cora & Citeseer & Pubmed \\
    \midrule
    VGAE & 89.6 & 92.2 & 92.3\\
    Graphite & \textbf{90.5} & \textbf{92.5} & \textbf{93.1}\\
    \bottomrule
  \end{tabular}
\end{table}

 


\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{images/scalable}
\caption{AUC score of VGAE and Graphite with subsampled edges on the Cora dataset.}\label{fig:scalability} 
\end{figure}

\subsection{Semi-supervised node classification}

We report the baseline results for  SemiEmb~\citep{weston2008embedding},  DeepWalk~\citep{perozzi2014deepwalk}, ICA~\citep{lu2003classification} and  Planetoid~\citep{yang2016embeddings} as specified in~\citep{kipf2016semi}.  GCN uses a 32-16 architecture with ReLu activations and early stopping after  epochs without increasing validation accuracy.  The \name{} model uses the same architecture as in link prediction (with no edge dropout). The parameters of the posterior distributions are concatenated with node features to predict the final output.  The parameters are learned using the Adam optimizer~\citep{kingma2013auto} with a learning rate of . All accuracies are taken as an average of 100 runs.

\subsection{Density estimation}
To accommodate for input graphs of different sizes, we learn a model architecture specified for the maximum possible nodes (\textit{i.e.},  in this case). While feeding in smaller graphs, we simply add dummy nodes disconnected from the rest of the graph. The dummy nodes have no influence on the gradient updates for the parameters affecting the latent or observed variables involving nodes in the true graph.
For the experiments on density estimation, we pick a graph family, then train and validate on graphs sampled exclusively from that family.  We consider graphs with nodes ranging between 10 and 20 nodes belonging to the following graph families :
\begin{itemize}
\item Erdos-Renyi~\citep{erdos1959random}: each edge independently sampled with probability 
\item Ego Network: a random Erdos-Renyi graph with all nodes neighbors of one randomly chosen node
\item Random Regular: uniformly random regular graph with degree 
\item Random Geometric: graph induced by uniformly random points in unit square with edges between points at euclidean distance less than 
\item Random Power Tree: Tree generated by randomly swapping elements from a degree distribution to satisfy a power law distribution for 
\item Barabasi-Albert ~\citep{barabasi1999random}: Preferential attachment graph generation with attachment edge count 
\end{itemize}

We use convex combinations over three successively induced embeddings.  Scores are reported over an average of 50 runs.  Additionally, a two-layer neural net is applied to the initially sampled embedding  before being fed to the inner product decoder for GAE and VGAE, or being fed to the iterations of Eqs.~\eqref{eq:int_graph_decoding} and \eqref{eq:refine_graph_dec} for both Graphite-AE and Graphite-VAE.


  \section{Additional Related Work}\label{app:related}
  \textbf{Factorization based approaches}, such as Laplacian Eigenmaps~\citep{belkin2002laplacian} and IsoMaps~\citep{saxena2004non}, operate on a matrix representation of the graph, such as the adjacency matrix or the graph Laplacian. These approaches are closely related to dimensionality reduction and can be computationally expensive for large graphs. 

\textbf{Random-walk methods} are based on variations of the skip-gram objective~\citep{mikolov2013distributed} and learn representations by linearizing the graph through random walks. These methods, in particular DeepWalk~\citep{perozzi2014deepwalk}, LINE~\citep{tang2015line}, and node2vec~\citep{grover2016node2vec}, learn general-purpose unsupervised representations that have been shown to give excellent performance for  semi-supervised node classification and link prediction. Planetoid~\citep{yang2016embeddings} learn representations based on a similar objective specifically for semi-supervised node classification by explicitly accounting for the available label information during learning.
 \end{document}
