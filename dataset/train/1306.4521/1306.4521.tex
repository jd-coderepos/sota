We performed extensive experimentation studying the performance of
these algorithms on various input types and on many different
multicore architectures. In addition to measuring the running time of
these algorithms, we used {\tt papi} library and the Linux perfctr kernel module to
read the hardware performance counters and measure cache misses, DRAM
accesses, TLB misses, branch mispredictions, number of instructions
etc. This section summarizes the key findings of
our experiments.

Our query points were generated uniformly at random inside the
grid of size . To elicit the
asymptotic worst case performance of point
location algorithms, we focus on long segments, whose length is chosen
uniformly at random between  and  and are at a random -coordinate.

\textbf{Configuration.} We ran our implementation on the following multicore systems:
\begin{enumerate}
\item
\label{item:intel}
A system with a single 4-core 2.66 GHz Intel Core i7-920 processor and a total
of 12.3GB RAM. Each core can run 2 threads due to hyperthreading. The
processor has an L3 cache of size 8192 KB that is shared
among all 4 cores. The L2 cache of 256 KB is only shared among pairs of cores.
\item
\label{item:amd}
A system with -core 1.9 GHz AMD Opteron 6168 processors 
and total of 264 GB of RAM.  Each core contains a private L2 cache of 512
KB and groups of 6 cores share an L3 cache of 5118KB. Thus, each processor
contains two L3 caches of combined size of just over 10MB.
\item
\label{item:bli}
A system with 2 x 16-core 2.6 GHz AMD Opteron 6282 SE processors and
total of 96 GB RAM. Each core has its private L2 cache while the L3
cache is shared between 16 cores. The L2 cache size is 2 MB and L3 cache size is 16 MB. 
\end{enumerate}

All configurations run Linux kernels and the codebase was
compiled using g++-2.4 compiler and -O3 flag.

\textbf{Spatio-temporal locality in our setting}.
The cache line size for all cache levels on all 3 systems is 64 bytes. Since our objects take 32 bytes
of space, it appears that each cache line can hold only two objects.
Therefore, at a first glance it is not clear if I/O efficient
algorithm can utilize the spatial locality for any improvement in runtime. However, we
observed that given an array that is too large to fit in cache and which contains
our 32-byte objects, it takes 4-5 times faster to access the objects
sequentially rather than performing access in random locations. This
observation can be explained by the fact that the memory system prefetches 2-3
cache lines when performing a sequential scan. Thus, during sequential scan the
prefetcher amplifies the size of the cache line by the number of lines being
prefetched.\footnote{For this experiment, the array must contain the actual
objects and not just pointers to the objects, which could be allocated anywhere
in memory.}

Another benefit of performing -way distribution sweeping is that it
allows us to utilize temporal locality by reducing the number of
recursive calls. In particular,  is chosen as  and
the number of recursive levels is . Given limit of RAM size
on our systems and the large size of L3 cache, it appears from our experiments
that  is set to  on configuration 1 and 2, resulting in a single recursive level
dedicated to (sequential) distribution (with the recursive base case performing
plane sweep on chunks that fit in L3 cache). On configuration 3, it
requires two recursive calls. The various trade-offs involved in
selecting the correct values of parameters  and  and the effect
of these parameters on the actual run-time of our PEM implementation
are described in Appendix~\ref{sec:param_km}.

\begin{figure*}[tb]
\center
\includegraphics[width=.3\textwidth, angle=270]{AMD_running_time_M53312_wo_sort_nolog_per_ele.eps}
\hspace{1cm}
\includegraphics[width=.3\textwidth, angle=270]{i7_running_time_wo_sort_nolog_per_ele.eps}
\caption{Runtimes on the configuration~\ref{item:amd} (left) and
  configuration~\ref{item:intel} (right) per element. The plots exclude the times to perform initial
sorting of inputs by the -coordinate for distribution sweeping and
-coordinate for the plane sweep.}
\label{fig:runtimes}
\end{figure*}

\textbf{Random access vs. I/O-efficient algorithms.} 
 Figure~\ref{fig:runtimes} shows the absolute running times for the plane sweep
and (parallel) distribution sweeping algorithms. One can see improvements in
runtimes with the increase in the number of processors used. Also note the
difference in the slopes in the graphs of the plane sweep algorithm compared to
distribution sweeping algorithms. This is due to larger asymptotic number of
cache misses of the plane sweep algorithm. 

Figure~\ref{fig:relative_linesweep} demonstrates this difference better. It
shows the speedup of the sequential and parallel distribution sweeping
algorithms relative to the plane sweep algorithm for long segments. In this
figure one can see the effects of cache-efficiency on runtimes.  It clearly
shows that the I/O-efficient algorithms outperform the plane sweep algorithm as
the input sizes increase. Recall our discussion that for the parameters of our
systems  and the I/O complexity of the distribution sweeping algorithm
is . This explains the non-linear
asymptotic speedup over plane sweep algorithm (with I/O complexity of
) as a function of the input size.

\begin{figure*}[!htb]
\center
\includegraphics[width=.3\textwidth, angle=270]{AMD_relative_running_time_M53312_wo_sort.eps}
\hspace{1cm}
\includegraphics[width=.3\textwidth, angle=270]{i7_relative_running_time_M64000_wo_sort.eps}
\caption{Speedup of the distribution sweeping algorithms relative to the plane
sweep algorithm on the configuration~\ref{item:amd} (left) and configuration~\ref{item:intel} (right). The
plots exclude the times to perform initial sorting of inputs by the
-coordinate for distribution sweeping and -coordinate for the plane sweep.}
\label{fig:relative_linesweep}
\end{figure*}

\begin{figure*}[!htb]
\center
\includegraphics[width=.3\textwidth, angle=270]{AMD_relative_running_time_M53312_seq_io_wo_sort.eps}
\hspace{1cm}
\includegraphics[width=.3\textwidth, angle=270]{i7_relative_running_time_M64000_seq_io_wo_sort.eps}
\caption{Speedup of the parallel distribution sweeping algorithms relative to the sequential distribution sweeping
algorithm on configuration~\ref{item:amd} (left) and configuration~\ref{item:intel} (right) systems. The plots exclude the times to perform initial sorting of inputs by the -coordinate.}
\label{fig:relative_sequential}
\end{figure*}

Figure~\ref{fig:relative_sequential} shows the speedup that parallel
distribution sweeping algorithm achieves relative to the sequential
distribution sweeping algorithm. 

\begin{figure*}[!htb]
\center
\includegraphics[width=0.3\textwidth, angle=270]{BLI_running_time_pram_pem_wo_sort_nolog_per_ele.eps}
\hspace{1cm}
\includegraphics[width=0.3\textwidth, angle=270]{AMD_DRAM_access_combined_with_pram.eps}
\caption{Comparison of PEM and PRAM algorithms on 16 cores of
  configuration~\ref{item:bli} is shown in the left figure. Running time and DRAM
  traffic for long segments on 12 cores of configuration~\ref{item:amd}
  in the right.}
\label{fig:dram_access}
\end{figure*}

\textbf{PRAM vs. PEM performance.} Figure~\ref{fig:dram_access} (left) shows the comparative
performance of the various algorithms on configuration~\ref{item:bli}. We observe
that the PRAM implementation is significantly slower than the PEM
algorithm. For instance, with 51.2 million segments and the same
number of queries, PRAM implementation takes 96 seconds with 16
cores, while the PEM implementation only requires 30 seconds with the
same number of cores (excluding the time for loading the input and
sorting it, which is 18 seconds for both implementations). This is
largely accounted for by the fact that the PRAM implementation makes
poor use of temporal locality and thus, has larger number of recursive
levels. In each recursive level, it scans all the segments and query
points, increasing the DRAM accesses significantly. 

\textbf{DRAM Accesses and Cache Misses.} 
We could not find a reliable way to measure only L3 cache misses: the {\tt
papi} library does not support measurement of shared cache events, while the
hardware counters for LLC (Last Level Cache) counters returned suspiciously
similar results to L2 cache misses.  Instead we measured the total traffic to
DRAM using {\tt perf} tool.  Figure~\ref{fig:dram_access} (right) shows a clear
correlation between the total DRAM traffic and running times. It is interesting
to note that although our algorithms are designed in simple 2-level cache
model, they minimize the total traffic to DRAM, in spite of complex nature of
modern memory systems.

\textbf{Random, short, medium and long segments.}
We refer the reader to
Appendix~\ref{sec:long-short} for the relative behavior of the
different point location 
algorithms on different segment types.

