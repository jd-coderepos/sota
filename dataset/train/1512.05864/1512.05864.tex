\documentclass{llncs}
\usepackage{latexsym}
\usepackage[utf8]{inputenc}

\usepackage{times}
\usepackage{graphicx}

\usepackage{amsmath,amsfonts,mathrsfs,amssymb,mathtools}
\usepackage{bbold}
\usepackage{stmaryrd}
\usepackage{color}

\usepackage{url}


\usepackage[caption=false]{subfig}




\usepackage{varwidth}





\renewenvironment{proof}{\paragraph{Proof} }{\hfill\qed}
\renewcommand{\paragraph}[1]{\noindent\textit{#1}.}






\newcommand{\BigO}[1]{\ensuremath{\operatorname{\mathcal{O}}\bigl(#1\bigr)}}

\begin{document}








\title{Optimization of Tree Modes for Parallel Hash Functions: A Case Study}

\author{Kevin Atighehchi\inst{1} \and Robert Rolland\inst{2}}

\institute{Aix Marseille Univ, CNRS, LIF, Marseille, France\\
\email{kevin.atighehchi@univ-amu.fr}\\
\and
Aix Marseille Univ, CNRS, I2M, Marseille, France\\
\email{robert.rolland@acrypta.fr}}

\maketitle



\begin{abstract} 
This paper focuses on parallel hash functions based on tree modes of operation for an inner Variable-Input-Length function. 
This inner function can be 
either a single-block-length (SBL) and \emph{prefix-free} MD hash function, or a sponge-based hash function. 
We discuss the various forms of optimality that 
can be obtained
when designing parallel hash functions based on trees where all leaves have the same depth.
The first result is a scheme which optimizes 
the tree topology in order to decrease the running time. 
Then, without affecting the optimal running time 
we show that we can slightly change the corresponding tree topology so as 
to minimize the number of required processors as well.
Consequently, the resulting scheme decreases in the first place the running 
time and in the second place the number of required processors. 
\end{abstract}

\smallskip
\noindent \textbf{Keywords.} Hash functions, Hash tree, Merkle tree, Parallel algorithms





\section{Introduction}\label{sec:intro}


A mode of operation for hashing is an algorithm iterating (operating)
an underlying function over parts of a message, under a particular composition method, 
in order to compute a digest. 
A hash function is obtained by applying such a mode to a concrete underlying function;
we then
say that the former is constructed on top of the latter.
Usually, when the purpose is to process messages of arbitrary length, the underlying function may be a fixed-input-length (FIL) compression function, 
a block cipher or a permutation. 
However, there may also be an interest in using a sequential (or serial) hash function as underlying function, in order to add to it other features, 
like coarse-grained parallelism.
The resulting hash function must satisfy the usual properties of pre-image resistance (given a digest value, it is hard to find 
any pre-image producing this digest value),
second pre-image resistance (given a message , it is hard to find a second message  which produces the same digest value),
and collision resistance (it is hard to find two distinct messages which produce the same digest value).
A sequential hash function can only use 
Instruction-Level Parallelism (ILP) and SIMD instructions \cite{GK12a,GK12b}.
A cryptographic hash function has numerous applications, the 
main one is
its use
in a signature algorithm to compress a message before signing it.

The most well known sequential hashing mode is the Merkle-Damgård \cite{Dam90,Mer79} construction
which can only take advantage of the fine-grained parallelism of the operated compression function.
If such a low-level "primitive" can benefit from the Instruction-Level Parallelism, by using also
SIMD instructions, the outer algorithm iterating this building block could benefit from a coarse-grained 
parallelism. This parallelism can be employed in multithreaded implementations.
Suppose that we have a collision-free compression function taking as input
a fixed-size data, . By using a balanced binary tree structure, 
Merkle and Damgård \cite{Dam90,Mer80} show that we can extend the domain 
of this function so that the new outer function, denoted
, has an arbitrary sized domain and is still collision-resistant.
Note that if the function  is a sequential hash function, the purpose of this tree structure is merely the addition 
of coarse-grained parallelism.



A construction using a balanced binary tree allows simultaneous processing of multiple parts of data 
at a same level of the tree, 
reducing the running time to hash the message from  to  
if we have  processors \cite{Dam90,Mer80}. If we want to further reduce
the amount of resources involved, we can use one of the following rescheduling techniques:
\begin{itemize}
 \item Each processor is assigned the processing of a subtree (in the data structure sense) having  leaves. 
 There are approximatively  such subtrees. 
The processing of the remaining ancestor nodes, at each remaining level of the tree, is distributed as fairly as possible
between the processors. An example is depicted in Figure \ref{resch_technique}.

 \item An alternative solution is, at each level of the tree, to distribute as fairly as possible the node computations among
 processors.
\end{itemize}

\begin{figure}[htbp]
\begin{center}
\includegraphics{illustration_rescheduling_conv_xfig.pdf}
\caption{Example of the computation of the root node in  time using  processors. The message
to hash is of size . In Phase 1, the computation of each hash subtree containing  leaves is assigned to each processor. The first 
subtree is assigned to processor , the second one to processor , the third one to processor  and the last one to processor . A fine-grained allocation
is then performed in Phase~2.}
\label{resch_technique}
\end{center}
\end{figure}

The number of processors is then reduced by a factor  and
the asymptotic running time is conserved (with, nevertheless, a multiplicative factor 2).
In this paper we are not interested in tradeoffs between
the amount of used resources and the running time but instead we study optimal algorithms in finite distance.
More precisely, we determine the hash tree structures which give the best concrete (parallel) time complexity for finite message lengths.


A tree structure is notably used in parallel hashing modes of Skein \cite{FLSWBKCW09}, BLAKE2~\cite{ANWW13} or MD6 \cite{RABCDEKKLRSSSTY08}. 
To give some examples, Skein uses a tree whose topology is controlled by the user thanks to three parameters:
the arity of base level nodes which is a power of two; the arity of other inner nodes,
which is also a power of two, and a last parameter limiting the height of the tree.
MD6 uses a full (but not necessarily perfect) quaternary tree, in the sense that an inner node has always four children.
Some fictive leaves or nodes padded with 0 are added so that a rightmost node has the correct number of children. 
Like Skein, MD6 offers a parameter which serves to limit the height of the tree.


Some proposals \cite{SS01,SS02,PS03} consider that a tree covering all the message blocks
is not a good thing, because the number of processors should not grow with the size of the message.
For instance, the domain extension parallel algorithm from Sarkar et al. \cite{SS01,SS02,PS03} uses a perfect binary tree
of processors, of fixed size. 
This perfect binary tree of compression/hash function calls can be seen as a big compression function, sequentially iterated
over large parts of the message. 
In other words only the nodes computations performed in the tree can be done in parallel.
The number of usable processors is a system parameter chosen by the issuer of the cryptographic form when hashing the message.
The value of this parameter has to be reused by the recipients, for instance when verifying a signature. 
Thus, this one limits the scalability and the potential speedup. 
In this paper we consider that the scalability and the potential speedup 
should be independent of the characteristics (configuration) of the transmitting computer.



Bertoni \textit{et al.} \cite{BDPV09,BDPV14_Sak} give sufficient conditions for a tree-based hash function to ensure its indifferentiability
from a random oracle. They propose several tree hashing modes for different usages.
For example we can make use of a tree of height 2, defined in the following way: we divide the message in as many parts (of roughly equal size) 
as there are processors so that each processor hashes each part, 
and then the concatenation of all the results is sequentially hashed by one processor. 
To divide the message in parts of roughly equal size, the algorithm needs to know in advance the size of the message.
Bertoni \textit{et al.} propose also a variant
which still makes use of a tree having two levels and a fixed number of processors, but this one interleaves 
the blocks of the message. This interleaving offers a number of advantages, as it allows an efficient parallel hashing of a streamed message, 
a fairly equal distribution of the data processed by each processor in the first level of tree (without prior knowledge of the message size), 
and a correct alignment of the data in the processors' registers. This kind of solution is suitable for multithreaded and SIMD implementations \cite{Gue14}.
In this paper we study theoretically optimal speedups, and, 
as a consequence, the message to hash is supposed to be already available.




Our concern in this paper is with hash tree modes using an underlying variable-input-length (VIL)
function that needs  invocations of a lower level primitive to process a message of  blocks, 
where a block and the hash output have the same size.
To make such a complexity concrete,
we choose to use a single-block-length (SBL) hash function as underlying function 
and to focus on the \emph{prefix-free} Merkle-Damg{\aa}rd construction from Coron \textit{et al.} \cite{CDMP05}. We make this choice for two reasons: 
first, we need a hash function whose mode of operation is proven indifferentiable from a random oracle (when its underlying primitive is assumed to be ideal).
Second, assuming that we have applied the \emph{prefix-free} encoding \cite{CDMP05} 
and another encoding \cite{BDPV09,BDPV14_Sak} to identify the type of input in the tree, it is possible to precompute a constant number of hash states,
making the aforementioned complexity possible.
Note that, even if we take this construction as an example, the possible use of a sponge-based function 
will be discussed.
In this work, we aim to show that we can improve
the performance of a hash tree mode of operation by reworking the tree-structured circuit topology.
While we focus on the case of trees having all their leaves at the same depth,
we are interested in minimizing the depth (parallel time)
of the circuit and the width (number of processors involved). This kind of work has been done for parallel 
exponentiation in finite fields \cite{Sti90,Gat91,AMV88a,LKPC05,WLLC06} where the multiplication operator is both associative and commutative.
In the case of parallel hashing, the considered operator can be a FIL (Fixed-Input Length) compression function. This is quite different since 
we do not have these two properties and we need to cope with other problems (the space consumption of a padding rule, a length encoding, 
or other information bits).
To the best of our knowledge, it is the first time that the problem of optimizing hash trees is addressed. The main interest of this paper is 
the methodology provided.
The results can be presented as follows:
\begin{itemize}
 \item The first result is an algorithm which optimizes 
the tree topology in order to decrease the depth. We first show that a node arity greater than  is not possible and then we prove
that we can construct such an optimal tree using exclusively levels of arity  and .
 \item Without affecting this optimal depth, 
we show that we can change the corresponding tree topology in order to decrease the width as much as possible. 
In particular, we show that for some message lengths , the width can be decreased to .
\item We also provide an algorithm which optimizes the number of processors 
 at each step of the hash computation. We prove that eleven tree topologies are possible.
 \item With the assumption that the message size is Pareto-distributed, we estimate the relative frequency of each tree topology 
 using the Monte Carlo method.
 \item Finally, we show that by using a SBL hash function as underlying function and by assuming a constant number of precomputed values, 
 these optimisations can be applied safely.
\end{itemize}



Suppose that the processing of one block of the message by the underlying function costs one unit of time. 
A binary tree is not necessarily the structure which gives the best running time. Figure \ref{Arbre_exemple} shows two different tree topologies for
hashing a 6-block message. 
The binary tree depicted in (\ref{fig:sub1}) gives a (parallel) running time of  units while the rightmost one with a different arity 
at each level, depicted in (\ref{fig:sub2}), gives a running time of 5 units. Furthermore, one may note that for messages of length less that 5 blocks, 
the use of the topology (\ref{fig:sub1}) has no utility compared to a purely sequential mode (\textit{i.e.} a completely degenerated binary tree).


\begin{figure}[h]
\centering
\subfloat[Non optimal tree]
  {
  \centering
  \includegraphics[scale=0.43]{6_leaves_tree_nonoptimal.pdf}
\label{fig:sub1}
  }
\qquad\qquad
\subfloat[Optimal tree]
  {
  \centering
  \includegraphics[scale=0.43]{6_leaves_tree_optimal.pdf}
\label{fig:sub2}
  }
\caption{Tree hashing with a 6-block message. The hash tree on the left requires 2 units of time to process each level, while the one on the right requires 
3 units of time to process the base level and 2 units of time to process the root node.}
\label{Arbre_exemple}
\end{figure}



In what follows, we suppose the use of variable-input-length (VIL) compression functions (or hash functions)
having a domain space   for  and a fixed length range space .
We also assume that such a function has an ideal computational cost of  units when compressing  blocks of size  bits.
In other words, if we consider a tree of calls of
this function, the computation of a node having  children (\emph{i.e.}  blocks) has a cost of  units.
Such a computational cost is realist. For instance, the UBI transformation function\footnote{UBI stands for Unique Block Iteration, the 
sequential operating mode used in Skein. The UBI transformation function refers to the application of this mode to the underlying tweakable
compression function, itself based on the tweakable block cipher Threefish.
}
used in the hash function family Skein \cite{FLSWBKCW09} performs  calls to the tweakable block cipher Threefish
to compress a data of length  blocks. Assuming a hash tree of height  and  the arity of level  (for ), we define the parallel 
running time to obtain the root node value as being .

The paper is organized in the following way. In Section 2 we give background information and definitions.
In Section 3, we first describe the approach to minimize the running time of a hash function. Then, we give an algorithm to
construct a hash tree topology which achieves the same optimal running time while requiring an optimal number of processors.
We also show that we can optimize the number of processors at each step of the hash computation. This leads to eleven possible
tree topologies, whose probability distribution is empirically analyzed in Section \ref{prob_dist}.
We propose in Section 5
a concrete tree-based hash function that safely implements these optimizations.
Finally, in Section 5,  we conclude the paper. 

\section{Preliminaries}\label{subsec:backg}

\subsection{Tree structures}


Throughout this paper, 
we use the 
convention\footnote{This corresponds to the convention used to describe Merkle trees. 
} that a node is the result of
a function called on a data composed of the node's children.
A node value (or chaining value) then corresponds to an image by such a function and a child of this node can be either
an other image or a message block.
We call a base level node a node at level  pointing to the leaves representing message data blocks. The leaves (or leaf nodes) 
are then at level~. Then, a tree of nodes of height  has  levels.
We define the arity of a level in the tree as being the greatest node arity in this level.

A -ary tree is a tree where the nodes are of arity at most . For instance, a tree with only one node of arity  is said to be a -ary tree. 
A full -ary tree is a tree where all nodes have exactly  children.
A perfect -ary tree is a full -ary tree where all leaves have the same depth.

\begin{figure}[htb]
\centering
\subfloat[Ternary tree, or -aries tree]
  {
  \centering
  \includegraphics[scale=0.50]{ternary_tree_example.pdf}
\label{perf1}
  }
\vspace{0.75cm}

\subfloat[Full -aries tree]
  {
  \centering
  \includegraphics[scale=0.50]{full_23_tree_example.pdf}
\label{perf1}
  }
\vspace{0.75cm}

\subfloat[Full and perfect ternary tree, also seen as a full and perfect -aries tree]
  {
  \centering
  \includegraphics[scale=0.50]{full_tree_example.pdf}
\label{perf2}
  }
\caption{Examples of trees}
\label{Tree_examples}
\end{figure}


\begin{sloppypar}
We also define other "refined" types of tree. 
We say that a tree is of arities  (we can call it a -aries tree) if 
it has  levels (not counting level ) whose nodes at the first level are
of arity at most , nodes at level  are of arity at most , and so on. We say that such a tree is full 
if all nodes at the first level have exactly  children, all nodes at level  have exactly  children, and so on. 
As before, we say that such a tree is perfect if it is full and if all the leaves are at the same depth.
Some examples are depicted in Figure~\ref{Tree_examples}.
\end{sloppypar}

A tree of nodes has a corresponding tree of -inputs having one less level. For instance, the simple tree of nodes of height 1 and arity 3 has 2 levels and 4 nodes: 
one root node and 3 children which are the message blocks. Its corresponding tree of -inputs has a single level containing a single -input. This -input consists
of the concatenation of the message blocks and some meta-information bits. This representation is further defined in the following subsection \ref{three_conditions}.

\subsection{Security of tree-based hash functions}\label{three_conditions}

In this section (and in Subsection~\ref{sec_section}), we represent a hash tree as a tree of -input,
assuming that a single inner function  is operated in the outer hash function, denoted~.
This is the representation adopted in~\cite{BDPV09,BDPV14_Suf} to prove the desired security properties.
We recall that a tree of -inputs has one less level compared to a tree of nodes. 

A -input is a finite 
sequence of bits from the following elements: message bits, chaining value bits (\emph{i.e.} bits coming from a -image), and frame bits
(bits which are fully determined by the hash algorithm and the message size). In a tree of -inputs, there are pointers from children to their corresponding parent. 
When a chaining value is present in a formatted -input, it is
pointed by another -input which is considered as its children. Each -input has an associated index which locates it in the tree.
In addition, we need to define, for a tree  of -inputs, its corresponding \emph{tree template}  which has the same topology, where the corresponding -inputs
have the same lengths and the frame bits match the corresponding bits in , but where message and chaining value bits are not valuated. 
Thus, a tree template is fully determined by the message size 
and the parameters of the tree mode algorithm. This tree template
is used by the tree hash mode to instantiate the tree of -inputs, by valuating progressively message bits and chaining value bits.


A tree  of -inputs is said to be \emph{compliant} with a tree hash mode  if the latter can produce a tree of -inputs whose corresponding tree template
is compatible with it (its topology, the frame bits and the sizes of its -inputs match those of ). A tree  of -inputs is said to be 
\emph{final-subtree-compliant} with  if the latter can produce a tree of -inputs whose proper subtree (\textit{i.e.} contaning at least the root -input) 
has a corresponding tree template with which  is compatible.


Bertoni \textit{et al.}~\cite{BDPV09,BDPV14_Suf}
give some guidelines to design correctly a tree hash mode  operating an inner hash (or compression) function . 
They define three sufficient conditions which ensure that the constructed hash function , which makes use of 
an ideal hash (or compression) function ,
is indifferentiable from an ideal hash function. Besides, they propose to use particular frame bits in order to meet these conditions.
We refer to \cite{BDPV09,BDPV14_Suf} for the detailed definitions, and we give here a short description for each of them:
\begin{itemize}
 \item \textit{message-completeness:} 
 suppose we have a tree of -inputs produced by the tree hash mode. There is an algorithm  which, among the bits in the tree, 
 uniquely determines the message. 
 This requires that each message bit is processed at least once by . The message can be reconstructed correctly if, given the sequence of bits of a -input, 
 we can identify those which are message bits, and we are able to say what their positions in the message are. 
Generally, only the end of the message is problematic. To cope with this, dedicated frame bits can be used such as a reversible 
 padding\footnote{Since a hash function processes an entire number of blocks (whose size  depends on the underlying primitive), 
 a reversible padding is an efficient way of revealing the end of the message. This consists in applying to the message , whatever its length, 
 a function  which returns a bit-string of length a multiple of .
 Such a padding has to be reversible, \emph{i.e.} there is a function  such that  for all messages .
 A well-known technique consists in appending the bit "" to the end of the message, followed by the minimum number of bits "", so that the total bit-length of the 
 padded message is a multiple of .} for the message or a coding of the message length.
 The running time of  should be linear in the total number of bits in the tree.
\item \emph{tree-decodability:} 
intuitively, given a tree T of -inputs generated by , it is impossible to extract a final proper subtree T' of T 
which could have legitimately been generated by . 
In other words, given such a subset of -inputs, we are able to say whether there is a missing input or not. More formally,
 this property is satisfied by  if there are no trees of -inputs which are both \emph{compliant} and \emph{final-subtree-compliant} with it, and there is a decoding
 algorithm  that can parse the tree progressively on subtrees, starting from the
root -input, to retrieve frame bits, chaining value bits and message bits unambiguously. Also, when terminating, 
must decide if the tree is \emph{compliant}, \emph{final-subtree-compliant}, or \emph{incompliant} with~. 
The running time of  should be linear in the total number of bits in the tree.
\item \textit{final--input separability:}  
 whatever the tree  of -inputs generated by , we can distinguish between a root -input and any other -input.
Such a property is useful to prevent length extension attacks. 
 One straightforward way to fulfil this property is by means of domain separation between this final (root) -input and other -inputs, \emph{e.g.} 
 by augmenting them with a frame bit identifying them as such.
\end{itemize}



These conditions ensure that no weaknesses are introduced on top of
the risk of collisions in the inner function. For instance, with \emph{tree-decodability}, an inner
 collision in the tree is impossible without a collision for the inner function.
Andreeva \textit{et al.} have shown in 
\cite{AMP10,AMP10isc}
that a hash function indifferentiable from a random oracle
satisfies the usual security notions, up to a certain degree, such as pre-image and second pre-image resistance, 
collision resistance and multicollisions resistance.

\subsection{Definition of the inner hash function}

For our inner function, the hash functions based on the Merkle-Damg{\aa}rd construction, such as MD5, SHA-1 or SHA-2, have to be discarded. 
First, these functions cannot emulate a random oracle and we need this property to construct a tree-based hash function, 
constructed on top of it, which is still indifferentiable from a random oracle \cite{BDPV14_Suf}. Second, for efficiency purposes,
we want the inner function to have a running time linear in the number of blocks of the message. When MD-strengthened padding
is applied, the size of the message is appended to its end. This makes it difficult to obtain a running time perfectly linear in the number 
of blocks. As we will see, the \emph{prefix-free} Merkle-Damg{\aa}rd construction from Coron \emph{et al.} \cite{CDMP05} is a solution to both these problems.

Our inner hash function, based on the \emph{prefix-free} MD construction, iterates a SBL (Single-Block-Length) compression function, 
\emph{i.e.} whose output length is the same as the
message block length. We use the compression function
 based on a 
-bit block cipher with a -bit key, such as the Davies-Meyer compression function:  where  
is the previous hash state and  is a block of the message. Such a hash function \cite{CDMP05}, denoted , consists in applying the 
plain MD construction to a prefix-free encoding of the message input.~\\

\noindent\fbox{\begin{varwidth}{\dimexpr\linewidth-2\fboxsep-2\fboxrule\relax}
\paragraph{{\boldmath\textbf{The considered inner function }}}~\\
INPUT: message .~\\
OUTPUT: hash value.
\begin{enumerate}
 \item The message  is padded with  where  is the minimum number (possibly zero) of bits  such that its bitlength 
 is a multiple of .
 \item The -bits encoding of the number of blocks is prepended to the message (prefix-free encoding step).
 \item The message is parsed into  blocks , ,...,  of size  bits.
 \item The plain MD mode is applied on these  blocks.
 \begin{itemize}
  \item[] Let  (or a fixed IV value).
  \item[] For  to  do .
  \item[] Return .
 \end{itemize}
\end{enumerate}
\end{varwidth}
}~\\

At first sight, due to the padding and the prefix-free encoding, this hash function requires  calls to the underlying compression function 
to process a message of ~bits. In fact, the node arities of our tree topologies can be upper-bounded by a constant. Thus, the first block of
the prefix-free encoding of an input involved in our trees has a constant number of possible values, and their possible corresponding hash states 
 can be precomputed. Assuming a constant number of precomputed hash states, the running time of this hash function is then reduced to 
calls to the underlying primitive. Hence, it is explained in the subsection above that before applying the function~, domain
separation bits have to be added to the input. Bertoni \emph{et al.} \cite{BDPV14_Suf} have stated that 2 bits is sufficient. 
The number of distinct 
domain separation codes can then be considered small.
For domain separation purposes, we choose to prepend a code of  bits to the message 
so that there is no extra bits  due to the padding operation. With this second ``large'' encoding, the first bit of 
the message is at the end of the second block. Then, the running time is still  when the possible values of
 are precomputed. Since the number of distinct domain separation codes is small and that only one bit of the message is in the second block, 
we can precompute the possible values of  (instead of )
so as to reduce the running time of  to  units of time. Further details will be provided in Section \ref{sec_section2}.
~\\

A hash function that performs one call to a SBL compression function to process one block of the (padded) message is also called a SBL hash function.~\\



\paragraph{On the use of other SBL compression functions}
Chang \emph{et al.} \cite{CLNY06} have extended the work of Coron \emph{et al.} by checking if other 
hash functions using this prefix-free MD construction are still indifferentiable from an RO.
It turns out that sixteen of the PGV (Preneel Govaerts Vandewalle \cite{PGV93}) compression functions yield sound hash functions.
Again, these hash functions divide the message in blocks of the same size than the digest.~\\

\paragraph{On the use of a sponge-based hash function as inner function}
The function  can be a sponge-based hash function~\cite{BerDaePeeVan08} if the message block and output sizes are equal.
This constraint can be fulfilled by setting the rate (also denoted ) of this sponge function to the output size. Let us suppose that we use
a function having the same padding rule than Keccak~\cite{BerDaePeeVan13}. This padding rule consists in adding to the end of the message 
the bitstring  where  is
the minimum number of bits  such that the padded message has a size a multiple of . 
Hence, we choose to use  bits for the prepended domain separation codes so that the padded message ends with the two bits~.
With this large encoding, the first two bits of the message are at the end of the first block. Taking into account these overheads, the running time of
 is  to process a message of  blocks.
Some precomputations can be done to reduce the running time of  to  units of time. 
Further details will be provided in Section \ref{sec_section2}.~\\

\paragraph{About the padding overhead in other designs} The UBI transformation function of Skein~\cite{FLSWBKCW09} is collision resistant and 
requires exactly  calls to the tweakable
block-cipher Threefish to process a message of  blocks. Indeed, when the message size is already a multiple of , a flag in the tweak indicates whether
or not the message is padded. Such a design choice is also present in the BLAKE2~\cite{ANWW13} hash function.

\subsection{Parallel computation model}

We make the assumption that the number of processors is equal to the number of nodes at the first level of the tree. Once the nodes have been computed 
at a given level, the processors are reused to process the next (upper) level.

We use the classic PRAM (Parallel Random Access Machine) model of computation~\cite{GiRy88},
assuming the strategy EREW (Exclusive Read Exclusive Write). When dealing with hash trees, this model can indeed 
be restricted to this strategy: we do not need that two processors write simultaneously into the same memory location, nor that a same data
can be read simultaneously by two or more processors. In the context of parallel hashing, 
it serves \textit{a priori} no purpose to process twice a same message block or chaining value.


Let us denote by  the list of nodes at level . 
Given the definitions of a level arity and of our inner hash function, 
the parallel running time to process a hash tree of height  is equal to 

where the function  returns the arity of a node.
The \textit{total work} to process a hash tree is equal to

In other words, this quantity corresponds to the running time when it is executed 
sequentially (\textit{i.e.} by a single processor).

\section{Optimization of hash trees for parallel computing}\label{sec:optim}

\subsection{Minimizing the running time}\label{subsec:runni}

In order to optimize the running time of a tree mode, we make a certain degree of flexibility 
on the choices of node arities. 
We can note immediately that allowing different node arities in a same level of the tree provides no efficiency gains. Worse, 
the running time may be less interesting since a tree level processing running time is bounded by the running time to process the node 
having the highest arity. 
This observation suggests that, in order to hope a reduction of the tree processing running time, node arities at the same level need to be set 
to the same value\footnote{Except maybe the rightmost node which may be of smaller arity.} while allowing arities to vary from one level to another.
Therefore our strategy allows a different arity at each level of the tree.

Let us denote  the block-length of a message. The problem is to find a tree height  and integer arities , , ...,  
such that  is minimized. When constructing a hash tree having its leaves at the same depth,
we seek integers , , ...,  such that .
Since we have 

for (strictly) positive integers ,
any solution to the problem must necessarily satisfy the following constraints:


Note that if a solution  does not satisfy the second contraint in~(\ref{contraintes}), this means that a better solution
exists. A solution to this problem is a multiset of arities.
First, we show that, in a non-asymptotic setting, 
a perfect ternary tree 
comes closer to optimality 
than a perfect binary tree.
Then we examine the case of trees having different arities at each level.



First of all, we can start by considering the  and  (for ) as real numbers. Thus, 
we have to minimize the summation of  subject to the constraint that their product is . 
We know that the minimum is reached when the 
are equal to the same number, which we will denote . So we have
, that is . We must now determine
 so that  is minimized.
The calculation of a derivative shows that this minimum is reached for ,
which implies . 
Consequently, we can wonder what the best solution is between a perfect binary tree and a perfect ternary tree.
The comparison of these two cases is done in Appendix \ref{comp} and shows that
beyond a certain message size  (), a perfect ternary tree gives a better running time than a perfect binary tree.
In fact, as the present general study shows, a tree having different level arities can give better results.





Let us remind that node arities are not allowed to vary in a same level (same stage) of the tree.
A level of the tree is said to be of arity  when all nodes at this level are of arity at most~.
Given an optimal tree (in the sense of the running time) for hashing, we can ask what the possible arities are for its levels. 
We have the following Theorem:

\begin{theorem}\label{arites}
For a hash tree whose running time is optimal, the following statements hold: 
\begin{itemize}
 \item It can be comprised of levels of arity , , , or . Higher arities are not possible.
 \item It can be constructed using only levels of arity  and .
\end{itemize}
\end{theorem}

\begin{proof}
We prove these two assertions separately:
\begin{itemize}
 \item We first show that levels of arity  with  lead to trees having a suboptimal running time.
Indeed, any node of arity  can be replaced by a tree of arity  having a better running time. 
We simply have to note that  for all , meaning that
a -ary tree of height  can be advantageously replaced by a binary tree of height . 
In contrast, for all nodes of arity  with  and for all  
we have . 
Finally, a node of arity  can be replaced by a -aries tree, since ,
thereby reducing the running time to  units. 
 \item As regards the second assertion, a node of arity  can be replaced by a tree of arities , since . 
This transformation does not
change the running time since . Finally, a node of arity  can be replaced by a binary tree of height  for a running time
which is still unchanged.
\end{itemize}
\end{proof}

An optimal tree has not necessarily a single topology. Firstly, a solution satisfying constraints
(\ref{contraintes}) can be defined as a multiset of arities since we can permute them.
For instance, suppose a tree has three levels with the first level of arity , the second one of arity  and the last one (that is, the root node) 
of arity . We can permute these arities so that the first level is of arity  and the latter two levels of arity . 
If this new tree has the same running time, its topology has however changed.
Secondly, we can find examples where different multisets of arities lead to trees of optimal running time. For instance, if we consider a -block message, 
the multisets of arities ,  and  allow the construction of trees having the optimal running time (see 
Figure~\ref{Same_running_time_different_trees}).
We can, however, construct optimal trees by restricting the set of possible arities. We have the following theorem:


\begin{figure}[t!]
\centering
\subfloat[-aries tree]
  {
  \centering
  \includegraphics[scale=0.6]{Example_7_blocks.pdf}
\label{perf1}
  }
~~~~~~~~~~
\subfloat[-aries tree]
  {
  \centering
  \includegraphics[scale=0.6]{Examplebis_7_blocks.pdf}
\label{perf1}
  }


\vspace{0.75cm}
\subfloat[-aries tree]
  {
  \centering
  \includegraphics[scale=0.6]{Exampleter_7_blocks.pdf}
\label{perf2}
  }
\caption{Different topologies for a -block message}
\label{Same_running_time_different_trees}
\end{figure}

\begin{theorem}\label{min_running_time_3_cases}
Let a message of length  blocks and let  be the lowest integer such that . Let us note  the value which minimizes
the product  under the constraint .  There exists an optimal tree (in the sense of optimal running time) which has  levels 
of arity  and  levels of arity . More precisely, we can state the followings:
\begin{itemize}
 \item If  then a ternary hash tree is optimal for a running time of .
 \item If  then an optimal hash tree has  levels of arity  and one level of arity ,
for a running time of .
 \item Otherwise , and then an optimal hash tree has  levels of arity  and  levels of
arity , for a running time of .
\end{itemize}
Such an optimal tree maximizes the number of levels of arity .
\end{theorem}

\begin{proof}
According to Theorem \ref{arites}, a hash tree whose running time is optimal can be constructed using only levels of arity  and levels of arity .
We still need to find out their numbers.
If we have at least  levels of arity  then we can replace these  levels by  levels of arity  ().
The running time to process  levels of arity  or  levels of arity  is . 
Therefore, it is always possible to construct optimal trees with maximum  levels of arity . 
Let  be such that .
From the parallel running time standpoint, it is preferable to trade a level of arity  for a level of arity .
This means that the sought solution corresponds to the highest value  
 such that . The three assertions follow immediately.
\end{proof}



~\\
To determine the level arities of an optimal tree, we apply the following algorithm:~\\



\noindent\fbox{\begin{varwidth}{\dimexpr\linewidth-2\fboxsep-2\fboxrule\relax}
\paragraph{\textbf{Algorithm 1}} ~\\
INPUT: a message length .~\\
OUTPUT: a multiset of arities minimizing the running time.
\begin{itemize}
 \item We first compute  
and then . 
 \item We return a multiset which consists of  first levels of arity  and  last levels of arity .
\end{itemize}
\end{varwidth}}

~\\
\paragraph{\textbf{Examples}} For messages of lengths  and  blocks respectively, Algorithm 1 returns the multisets 
of arities ,  and  respectively.
The number of processors is not optimized here. This aspect is addressed in the following section. ~\\




The result can be either a \emph{perfect} tree where the number of leaves is greater than the message length (the tree is said to be \emph{perfect} since, on the one hand, 
nodes at a same level are all of same arity, and, on the other hand, all the leaves are at the same depth), or a \emph{truncated} tree since it 
is possible to prune some right branches to remove this surplus of leaves.
In the rest of the paper, we refer to a truncated -aries tree to speak about a tree having a number of leaves equal 
to the message length and where the nodes of the base level are of arity at most , nodes at the second level are of arity at most  and so on.~\\

As a last remark, since the hash function must be deterministic, the multiset of arities must also be chosen deterministically 
as a function of the message size. For instance, we can arrange in descending order the elements of the multiset of arities. 
The solution to the problem of minimizing the running time is then uniquely determined as an ordered multiset.


\paragraph{\textbf{Performance improvements}}
We have seen that for a message of 6 blocks (see Figure~\ref{Arbre_exemple}), the performance gain of an optimal tree compared to a binary tree is 20\%.
Figure~\ref{perf1} shows the running times of an optimal tree and a binary tree as functions of the message size varying from  to  blocks. 
Figure~\ref{perf2} shows the speed gain obtained with an optimal tree. 
The gain in time (or speedup gain) 
is computed as  where  is the running time of a binary tree and  the running time of an optimal tree. 
As we can see, the gain differs from one message size to another. The gain can be greater than 30\% for very short messages
but decreases quickly, to cap at 10\%. 
As regards the message size, although the diagram
does not cover a sufficiently long range, one can note a slight downward slope. 
\begin{figure}[htb]
\centering
\subfloat[Running time of an optimal tree (shown in blue) compared to a binary tree (in black)]
  {
  \centering
  \includegraphics[scale=0.31]{Temps_execution_Comparaison_eng2.png}
\label{perf1}
  }


\subfloat[Speed gain of an optimal tree compared to a binary tree]
  {
  \centering
  \includegraphics[scale=0.31]{Pourcentage_gain_execution_eng6.png}
\label{perf2}
  }
\caption{Performance comparison between an optimal tree and a binary tree}
\label{Gains_execution}
\end{figure}

\subsection{Minimizing the number of processors}\label{subsec:proce}

In this section we look into how to minimize the number of required processors to obtain the optimal running time.
We have two cases to study, the trees having all leaves at the same depth and the others. 
We fully treat the first case and we make a few observations regarding the second type of tree, 
which we intuitively sense to further reduce the number of required processors.~\\

At the outset, one may be interested in the maximum possible number of levels of arity  or . We have the following Lemma:

\begin{lemma}\label{numb_ar_4_and_5}
In a tree having an optimal running time the following statements hold:
\begin{itemize}
 \item There can be up to  level of arity ; 
 \item There can be up to  levels of arity .
\end{itemize}
\end{lemma}

\begin{sloppypar}
\begin{proof}
We prove these two assertions separately:
\begin{itemize}
 \item Suppose that the tree has  levels of arity . We 
replace these  levels by  levels of arity  since .
The running time is improved since . 
We can then state that  levels of arity  lead to a tree having a sub-optimal running time. 
 \item Now, let us look for a pair of minimum integers  satisfying  and . 
The first pair which satisfies these constraints is  and . We can then replace
 levels of arity  by  levels of arity  in order to decrease the running time. 
\end{itemize}
\end{proof}
\end{sloppypar}


\begin{sloppypar}
We have seen that it is possible to construct a tree optimizing the running time by using only levels of arity  and .
In what follows, we 
show how 
to deduce an optimal tree minimizing the number of involved processors.
Let us suppose that level arities ,  ...,  are noted in (no strictly) decreasing order so that  
is the arity of the base level and  the arity of the last level, 
\textit{i.e.} the arity of the root node. 
The trees optimizing the running time, defined above, are not necessarily full in the sense that a rightmost node at a given level can be of arity 
strictly lower than the arity of this level. First, we note that for the trees constructed with Algorithm 1, the number of required processors
is equal to  in the best case, and equal to  when there are only levels of arity . Moreover, 
according to Theorem \ref{arites} 
we know that a level arity cannot be greater than . This means that in the best case, after optimization, the number of required processors 
could be reduced to . Thus, we could in the best case decrease the number of processors 
by a factor of about .
\end{sloppypar}


Given an optimal tree for the running time, the intent is to increase the arity of the first level (base level) while decreasing arities 
of the following levels so that the sum of the level arities remains constant and their product remains greater than or equal to .
To solve this problem we propose in Appendix \ref{red_number_processors} two solutions (Algorithm 2a or 2b). However, as will be discussed below,
we can further optimize hash trees.

According to Theorem \ref{arites}, a level arity of a tree minimizing the running time cannot exceed . 
Thus, Algorithm 2a (or Algorithm 2b) of Appendix \ref{red_number_processors} allows us to substitute any sub-multiset  for 
another one, denoted , where the sum of arities remains the same, 
and by trying to increase the arity of the base level up to .
Consider, for instance, a message of size  blocks. With such a message size, Algorithm 1 returns the multiset of arities 
which defines a tree structure involving  processors.
By applying Algorithm 2, we obtain the multiset  which reduces the number of involved processors to  
while leaving the running time unchanged.
~\\

\noindent
\textbf{What if the arity of each level is increased? As much as possible?}
We just saw 
that we can increase the arity of the first level.
It would also be preferable to increase the arity of each level of the tree in order to free up the highest number of processors 
at each step of the computation. An example is depicted in Figure \ref{Gain_proc}.
\begin{figure}[h]
  \centering
  \resizebox{1\textwidth}{!}{
\includegraphics{Gains_processeurs_conv_xfig.pdf}
 }
 \caption{Two trees compressing a 20-block message, optimized both for the running time and the number of involved processors. 
 Both trees require 4 processors.
 Nevertheless, we note that the right tree is the best choice. Indeed, the one on the left needs 4 processors during 5 units of time,
 then 2 processors during 2 units of time, and finally one processor during 2 units of time. The one on the right needs 4 processors during 5 units of time and then one processor during 4 units 
 of time.}
\label{Gain_proc}
\end{figure}

While we propose an iterative algorithm in Appendix \ref{red_number_processors} to construct an optimal tree maximizing the arity of each level, we also 
enumerate all possible cases in the following Theorem:

\begin{theorem}\label{min_running_time_max_arities}
For any integer  there is an unique ordered multiset 
of  arities ,  arities ,  arities  and  arities 
such that the corresponding tree covers a message size , has a minimal running time
and has first  as large as possible, then  as large as possible, and then 
 as large as possible.
More precisely, if  is the lowest integer such that , 
this ordered multiset is such that:

where the number  is at least  in the first case and can be  in the other cases. 
\end{theorem}

\begin{proof}
Let us start from the 3 cases of Theorem \ref{min_running_time_3_cases} which maximize the number of levels of arity 3. 
For a given message length , we consider the corresponding optimal tree (in the sense of the running time). 
We denote by  the initial number of levels of arity  and by  the initial (maximized) number of levels of arity . 
We want to transform this tree in order to increase the arity of each level as much as possible, while leaving the running time unchanged.
According to Lemma \ref{numb_ar_4_and_5}, there can be one level of arity 5 and up to six levels of arity 4.
Since we want to maximize the number of levels of arity 4 after having maximized the number of levels of arity 5, 
there cannot be more than one level of arity 2. Thus, ,  
and , meaning there shall be at most 28 cases.
Note that among these 28 cases, many may not be valid solutions.
The aim is to transform the initial product  into
a product  where  and  are respectively the number of levels of arity  and the number of levels of arity  
that we have transformed, and , ,  the number of levels of arity ,
,  respectively.
For each triple  with ,  
and , we can verify that there is a solution  with  an integer in 
and  a positive integer such that . We remark that  can be rewritten  if ,  if , and  if . 
Thus, all but one integers are produced.
We can also verify that this solution is unique. Indeed, let us suppose a second solution . Since , we have
, meaning that  divides . This is impossible, unless . Such a solution must satisfy  , 
that is

According to Theorem \ref{min_running_time_3_cases}, we have . Consequently, if we have 


this solution does not meet the constraint (\ref{eqn_to_verify}), and then cannot exist. Among the 28 cases, we observe that 13 of them are not valid solutions.
Thus, we have 15 solutions, denoted , for which we compute and sort the values  so that we can establish
their domains of validity. We then obtain the fifteen following cases:



In accordance with Theorem \ref{min_running_time_3_cases}, we have this grouping: 
\begin{itemize}
 \item Group I consists of the five cases ensuring a running time of ;
 \item Group II consists of the six cases ensuring a running time of ;
 \item Group III consists of the four cases ensuring a running time of .
\end{itemize}
Now, we have to optimize the arities. In the first group, we delete the last case since it decreases  compared to the immediately preceding case. 
For the same reasons, we delete in the second group the fourth and sixth cases. Again, in the last group, we need to delete the last case. Overall, 11 cases are deduced
by intersecting the intervals of validity.
\end{proof}

\begin{remark}
The number of cases is lower when . Their determinations are let to the reader.
\end{remark}

\begin{remark}\label{local_optimality_rem}
Let us consider a tree which is optimal in the sense of the theorem \ref{min_running_time_max_arities}. If we extract a final subtree by deleting 
one or several lower levels (at the bottom), the resulting tree is still optimal. Indeed, let us suppose
that the original tree has height  and has  leaves (for  message blocks). If we delete the  lower levels, the resulting tree 
has  leaves and is already optimal for a number  of blocks.
If this form of local optimality does not exist, we can further optimize the original tree. Indeed, let us suppose that a -aries 
tree covers the number of blocks  and improves either the running time or the number of processors (in the sense of Theorem~\ref{min_running_time_max_arities}), 
compared to the -aries tree. This means that a -aries tree is a better choice 
to process  leaves.
\end{remark}

Let us now consider trees having a number of leaves equal to the message length. 
Having a multiset of arities arranged in descending order, that we denote \break ,
the number of nodes of level  is . One important thing is the number of nodes 
of the base level. We have the following Corollary:


\begin{corollary}\label{numb_proc}
Let the message size be  and let  be the lowest integer such that . 
The number of processors required to process
such a message is:
\begin{itemize}
 \item  if ,
 \item  if ,
 
 
 \item  if .
\end{itemize}

\end{corollary}

\begin{proof}
These results follow immediately from Theorem \ref{min_running_time_max_arities}.
\end{proof}
~\\





An other important thing is the minimization of the \textit{total work} done by the hash tree algorithm for the processing of a single message. 
Since we are interested in hash trees having an optimal running time for a given message size , 
we apply Theorem \ref{min_running_time_3_cases} or \ref{min_running_time_max_arities} to retrieve a topology.
For a perfect -aries tree constructed thanks to this theorem, the \textit{total work} is:

We notice that  for (strictly) 
positive integers . Consequently, for a -aries truncated tree constructed thanks to this theorem, the \textit{total work} is:


This quantity is necessarily greater than or equal to . Regarding truncated trees minimizing the running time, 
Theorem \ref{min_running_time_max_arities} indicates a topology which minimizes the \emph{total work}, by first choosing  as large as possible, 
then choosing  as large as possible, and so on.

\begin{remark}
 Decreasing the total work consists in decreasing as much as possible the number of nodes (apart from the number of leaves). We have to
 check if the multisets provided by Theorem \ref{min_running_time_max_arities} are preferable to others.
 Let us suppose that, for a given message size , we have a (non-ordered) multiset minimizing
 the running time.
 Let us also suppose  that at least one order of this multiset minimizes the total work.
We can show that, among all the possible orders, this is the one represented in decreasing order
 which minimizes the total work. 
 We denote such a solution by  with .
 Indeed, for a given message size  and for any random permutation  of the indices , we have 
  for all . Thus, summing left sides and right sides from the 
 inequalities, we have .
 When this ordered multiset cannot be derived from Theorem~\ref{min_running_time_max_arities}, we show that the transformations performed in its proof
 can further decrease the total work. We recall that the composition of five types of transformation 
 can lead to the eleven cases of this theorem. These transformations change the following pairs of arities , , ,  and  
 into , , ,  and  respectively.
 It is sufficient to show that each of these transformations can reduce the total work:
 \begin{itemize}
  \item Case : We indeed have .
  \item Case : We indeed have .
  \item Case : Since  
  and , we can show that
  
  for . As regards the other values of , it appears that this inequality does not hold
  for , but we remark that this message length is not concerned by such a transformation.
  \item Case : By the same reasoning, we can show that 
   for  and .
  We then remark that a message of length  or  blocks cannot be covered by a -aries tree.
  \item Case : Again, we can show that 
   for .
  We remark that a length of  cannot be covered by a -aries tree.
 \end{itemize}

\end{remark}

\section{About the distribution of cases}\label{prob_dist}

For the purpose of minimizing the number of processors at each step of the computation, we apply Theorem~\ref{min_running_time_max_arities}. 
There are 11 possible cases and we would like to estimate their distribution. We then perform an empirical estimation with a Monte Carlo simulation.
Numerous papers have analysed the distribution of file sizes, and more particularly the sizes of files transferred accross a network. 
It is shown that the average size of transferred files is about 10KB \cite{TMW97,WAWB05,BC98}, and we observe that
the Pareto distribution
is the predominantly used model \cite{BC98,Dow05,WAWB05,HSBA2003} to fit the datasets, with a \emph{shape} parameter  generally estimated between  and ,
and a \emph{location} parameter  estimated by hand or as a function of the average file size (using the mean formula). For this simulation, 
we use the Pareto model with  and 
 bytes in order to generate a sample of  byte sizes.
We initialize 11 counters  for . 
With the assumption of a -byte block size, we perform the following operations for each generated :
\begin{enumerate}
 \item We compute ;
 \item We check which one of the following cases is satisfied:
 Case~:~;
 Case~:~;
 Case~:~;
 Case~:~;
 Case~:~;
 Case~:~;
 Case~:~;
 Case~:~;
 Case~:~;
 Case~:~;
 Case~:~;~\\
 The index of the satisfied case is denoted ;
 \item We increment .
\end{enumerate}
The relative frequency of the -th case is then  for .
These estimations are made using R software \cite{RCTEAM} with the VGAM library \cite{Yee10}.
According to Corollary~\ref{numb_proc},
it follows that the number of required processors is  with probability 
of about , 
with probability of about , and  with 
probability of about .
The relative proportions of each individual case are depicted in Figure \ref{barplot_11_cases}.
\begin{figure}[h]
 \centering
 \includegraphics[scale=0.45]{barplot_11_cases_Pareto.pdf}
 \caption{Proportions for the eleven cases of Theorem \ref{min_running_time_max_arities}. The bars are drawn in decreasing order of frequency.
 The notation Tr(,,...,) stands for a tree having arities , , ...,  from the base level to the root node.}
 \label{barplot_11_cases}
\end{figure}


























\section{Applying our optimizations safely}\label{sec_section2}

Suppose that we have 4 different inner functions , ,  and  with the following properties:
\begin{itemize}
 \item  for  and    .
 \item Without precomputation, they have the same running time of  units of
time when compressing  message blocks of size  bits.
  \item With a constant number of precomputations, they have the same running time of  units of time when
  compressing  message blocks.
\item They behave like independent random oracles.
\end{itemize}
In the hash tree construction we propose, we use  to compute base level nodes,  to compute inner nodes, 
to compute the root node. If the tree is of height one, there is only one node computed using . In order
to simulate four independent functions, we use the same inner function  but with domain separation. Indeed, since  behaves like a random oracle,
by construction the functions , ,  and  
behave like independent random oracles.
~\\

Since  is based on the \emph{prefix-free} MD construction, the first block encodes the number of blocks of the message, comprised between  and . 
Due to the domain separation codings,
the second block contains the bitstring  where the 1-bit values  and  depend on the type of processed input, 
and  is the first bit of the input. Overall, we have to precompute 16 hash states (resulting from the processing of these two blocks) in order to
obtain an inner function having a running time equals to the node arity.

\paragraph{On the precomputations costs induced by the use of a sponge function}
The first block merely consists of the bitstring  where the 1-bit values  and  are as above,
and  and  are the first two bits of the message input. Again, we have to precompute 32 hash states (resulting from the processing of only one block) 
in order to obtain an inner function having a running time equal to the node arity.

\subsection{An example of hash function}
Given a message , a hash tree mode could be the following:
\begin{enumerate}
 \item Whatever the message bit-length is, we append to  a bit "1" and the minimum number of bits "0" so that the total bit-length is a multiple
 of . The new message is denoted  and its total number of blocks of size  bits is , where
  is the bitlength of . 
\item We apply Theorem \ref{min_running_time_max_arities} on  to retrieve the height 
  of the tree and an ordered multiset  of arities , , ...,  (arranged in decreasing order).
 \emph{This step requires the computation of , which can be considered free.}
 \item If , we compute and return the hash value . Otherwise, we go to the following step.
 \item We first split  into blocks , , ...,  where: () ; ()~all blocks but the last one are
 bits long and the last block is between  and  bits long.
 Then, we compute the message
 
 \item If  we go to step 6. Otherwise, for  to , we perform the following operations:
 \begin{enumerate}
  \item We split  into blocks , , ...,  where: ()~; 
  ()~all blocks but the last one are  bits long and the last block is between  and  bits long.
  \item We compute the message 
  
 \end{enumerate}
  \item We compute and return the hash value .


\end{enumerate}

\subsection{Security}\label{sec_section}



Since our mode can be rewritten as if it was using only ,
it suffices to check whether the three conditions (seen in Section \ref{three_conditions}) are satisfied to prove soundness.~\\

\noindent
We first need to describe some rules regarding our mode:

\paragraph{Rule 0} The root -input has a prepended code  or . 

\paragraph{Rule 1} A -input with a prepended code  has children having a prepended code  or . 

\paragraph{Rule 2} A -input with a prepended code  or  has no children.

\paragraph{Rule 3} A -input with a prepended code 00 has children having a prepended code  or .

\paragraph{Rule A} A -input must be -bit long with an integer .

\paragraph{Rule B} At a same level of the tree, the number of chaining values is the same for all the -inputs, except for the righmost one where 
 this number may be smaller.
 
\paragraph{Rule C} At a same level of the tree, prepended codes are the same for all the -inputs.~\\

\noindent
Note that the satisfaction of the rules 0, 1, 2, 3 and C imply that the leaves are at the same depth. So, we do not need to define a rule to express this.~\\

By construction our mode
is final--input separable. Our mode is trivially message-complete since it processes all message bits. Indeed, 
having the valuated tree of -inputs produced by , the algorithm  reaches directly the base level -inputs and recovers 
message blocks by discarding the frame bits, whether they serve as padding purpose (in the rightmost -input) or for identifying the type of -input.  
This algorithm runs in linear time in the number of bits in the tree.
Finally, our mode is also tree-decodable. Thanks to domain separation between base level -inputs and other -inputs, 
we cannot find a tree 
which is both \emph{compliant} and \emph{final-subtree-compliant}.
Given only one -input, the prepended coding allows its content to be recognized correctly. We can then construct a decoding algorithm  which runs 
in  phases: 
Phase  starts from the root -input and fully determines the tree structure by recursively decoding each -input. The size of a -input determines the 
number of its children. This phase terminates with the ``correct'' state C0 if all the visited -inputs respect the rules defined above.
This phase terminates with the ``incorrect'' state C1 if one of the rule 0, 2, A, B or C is not respected. 
If Rule 1 is not respected it terminates with the ``incorrect'' state C2. Otherwise, it terminates with the ``incorrect'' state C3.
Phase  examines the properties of 
the decoded tree by taking into account the termination state of the first phase. The details of Phase 2 are the followings:
\begin{enumerate}
\item If the state is C0, it runs the  algorithm in order to check the message size. If for the corresponding number  of blocks, 
 Theorem \ref{min_running_time_max_arities} indicates a topology which differs from
 the one that Phase 1 has just decoded, then it returns ``incompliant''. Otherwise, it returns ``compliant''.

\item If the state is C1, it returns ``incompliant''.
 \item If the state is C2, the following examinations are made:
 \begin{enumerate}
  \item If the tree seems incomplete with a single -input, it checks, after having discarded the prepended code, the number of blocks of size  bits.
  If there are 2, 3 or 4 blocks, then it returns ``final-subtree-compliant''. Otherwise, it returns ``incompliant''.
  \item Otherwise, the coding is incompatible with the mode, and it returns ``incompliant''.
 \end{enumerate}
 \item If the state is C3, this means that only the rule 3 is not respected. The following examinations are then made:
 \begin{enumerate}
  \item If there is a -input with a prepended code 00 which has a child with a prepended code not equal to 10, then it returns ``incompliant''.
\item Otherwise, there is at least one missing -input. We note  the maximum number of -inputs visited on a path in this proper final subtree. 
The algorithm has to check if its topology is consistent with Theorem\footnote{Meaning that it has to detect if this final subtree can be extracted 
  from an optimal tree, in the sense of this theorem.}~\ref{min_running_time_max_arities}. 
  First, it establishes a system of contraints regarding the arity of each level.  If there is only one -input at a level which is not the root, 
  and if it is the righmost one\footnote{A -input is the righmost one in the complete tree if we see that it does not have a right sibling, when looking at the 
  retrieved topology by Phase 1.} in the complete tree, then the number of (message or chaining) blocks it contains defines a lower bound for the arity 
  of this level. Otherwise, the arity for this level has a single possible value and the constraint is an equality. Having these  constraints, it checks in 
  Theorem \ref{min_running_time_max_arities}
  which cases (among the eleven) can satisfy these contraints. We denote by  the list of compatible cases and for each case  in  we denote by  the arity 
  of the level . For each case  in , the algorithm performs the following operations:
  \begin{enumerate}
   \item It completes this final subtree until its leaves are at the same depth . It performs this 
   by (virtually) creating the missing -inputs with a maximal arity (\emph{i.e.}, even if a missing -input at level  is the rightmost one 
   in the complete tree, it chooses its arity to be exactly ).
   \item It counts the number  of blocks covered by the completed subtree. If  is in the domain of validity of the case , then it returns ``final-subtree-compliant''.
  \end{enumerate}
  A this point, no cases in  are suitable. This phase 2 finally returns ``incompliant''.
 \end{enumerate}


\end{enumerate}

\noindent
The total running time of  is linear in the number of bits in the tree.



\begin{remark}If prepending 2 bits is sufficient \cite{BDPV14_Suf} for soundness, we could have used a reduction to the Sakura coding \cite{BDPV14_Sak} 
where meta-information bitstrings are longer. 
Using Sakura coding allows any tree-based hash function
to be automatically indifferentiable from a random oracle, without the need of further proofs.
According to the Sakura ABNF grammar \cite{BDPV14_Sak}, the number of chaining values (\textit{i.e.} the number of children of a -input) 
is also coded in the formatted input to .  
In our context and using this coding, the number of ways to format the input to  (depending on 
its location in the tree topology) is at least~10. A Sakura coding bit 
expresses the fact that 
the input is or is not the last one (for the computation of the root hash). Another bit indicates whether the input contains a message block
or a certain number of chaining values (whose number, 2, 3, 4 or 5, is also coded at the beginning of the input). 
Overall, this corresponds to 10 ways to format an input in our 
tree topology. This larger encoding and the fact to have meta-information bits at the beginning (for prepended bits) 
and at the end of the input (for appended bits) both complicates our construction and increases the number of precomputed hash states.
\end{remark}

\begin{remark}
 Yet another solution is to use different IVs (Initial Values) instead of particular frame bits, as suggested in \cite{BDPV14_Suf,luk13}. We could use a \emph{free-IV} hash function,
 like the \emph{suffix-free-prefix-free} hash function from Bagheri \emph{et al.} \cite{BGKZ12}. The distinction between the -inputs would be done by using 4 distinct 
 IVs: \texttt{BL\_IV} for base level -inputs, \texttt{I\_IV} for inner -inputs, \texttt{F\_IV} for the root -input and \texttt{SN\_IV} for a tree reduced to
 a single -input.
\end{remark}



\section{Conclusion}

In this paper, we focused on trees having their leaves at the same depth. 
We have shown, for a given message length, 
how to construct a hash tree minimizing the running time. 
In particular, we have shown how to minimize the
number of processors allowing such a running time. 
The proposed construction makes use of a prepended 
coding for each input to the inner function in order to satisfy 
the three conditions of Bertoni \textit{et al.} \cite{BDPV14_Suf}.
Besides, our tree topologies could also be used in substitution of
the tree hash mode of Skein, provided that the tweaks to 
the UBI mode are carefully chosen for each node.




\bibliographystyle{plain}
\bibliography{trees}

\newpage

\appendix

\section{Comparison between a perfect binary tree and a perfect ternary tree}\label{comp}
Let  an integer.
Let  the lowest integer such that  and  the lowest integer such that .
We assume that we use a perfect binary (or ternary) tree as in the original Merkle (and Damg{\aa}rd) hash tree mode,
\emph{i.e.} the message is padded to obtain a message size which is a power of  (or ).
The problem is to compare  and .

Any  can be uniquely written

where  is an integer such that . Then

If  then  else .

\subsection{The case }
In this case 

Then 

where .
We must compare 
 with , namely

or

As  is bounded by  and , for  sufficiently large
we have .
More precisely if  then , meaning that a perfect ternary tree gives a better running time than a perfect binary tree.
When , we compute the  values 

and we look at the sign of the result: 
\begin{itemize}
\item For  (), a perfect binary tree and a perfect ternary tree give the same result ().
\item For ,  a perfect ternary tree is better ().
\item For , a perfect binary tree is better ().
\end{itemize}

\subsection{The case }
In this case  and 

We must compare  to .
But:

and 

As , for  sufficiently large
we have .
More precisely for  then , meaning that a perfect ternary tree gives a better running time than a 
perfect binary tree. For any  and any 
such that  we must compute the sign of

As  is an increasing function of , it is sufficient to determine for any 
the value of  where the sign changes. This can be done by dichotomy. Results are in Table~\ref{Compar_perfect_binary_ternary}.

\begin{table}

\caption{Comparison between a perfect binary tree and a perfect ternary tree. If  a perfect ternary tree has a better running time. 
If  the two trees give the same running time. Otherwise a perfect binary tree is better.}
\label{Compar_perfect_binary_ternary}
\end{table}

\section{Algorithms for reducing the number of processors}\label{red_number_processors}

\subsection{Reducing the number of processors at the base level}

We propose two (different) algorithms to construct an optimal tree (in the sense of the running time) which covers exactly  blocks (the tree is not necessarily perfect)
and increases as much as possible the arity of the base level.
The first solution consists to check if there exists an optimal tree having a level of arity  or .~\\

\noindent\fbox{\begin{varwidth}{\dimexpr\linewidth-2\fboxsep-2\fboxrule\relax}
\begin{sloppypar}
\paragraph{\textbf{Algorithm 2a}} ~\\
INPUTS: a message length  and a multiset of arities
(arranged in descending order) minimizing the running time, denoted  .~\\
OUTPUT: a multiset of arities (still sorted in descending order)
minimizing the number of processors while leaving unchanged the running time.~\\
Let  the optimal running time for a message of size , \emph{i.e.} the sum of arities of .
The algorithm proceeds as follows: 
\begin{enumerate}
 \item Use Algorithm 1 to construct a tree for a message length  and denote by  the corresponding ordered multiset of arities.
 If  then return the multiset , otherwise go to the following step.
 \item Use Algorithm 1 to construct a tree for a message length  and denote by  the corresponding ordered multiset of arities.
 If  then return the multiset , otherwise go to the following step.
 \item Return  (which cannot be further optimized).
\end{enumerate}
\end{sloppypar}
\end{varwidth}
}

\newpage


The second approach uses the following hints:~\\

\paragraph{\textbf{Hints}} Let us note that if , then . Moreover, if  then . This suggests that
a product of several numbers, where the sum is constant, is maximized when these numbers are as close together as possible.
In order to decrease the product of arities as slowly as possible we 
use the fact that if  
we have
.~\\



\noindent\fbox{\begin{varwidth}{\dimexpr\linewidth-2\fboxsep-2\fboxrule\relax}
\begin{sloppypar}
\paragraph{\textbf{Algorithm 2b}} ~\\
INPUTS: a message length  and a multiset of arities
(arranged in descending order) minimizing the running time, denoted  .~\\
OUTPUT: a multiset of arities (still sorted in descending order)
minimizing the number of processors while leaving unchanged the running time.~\\
The algorithm proceeds as follows: 
\begin{enumerate}
 \item We start by replacing in  each pair of arities  by an arity  (leaving possibly only one arity  in ). 
 We sort  in descending order.
 \item We repeat at most twice the following routine to determine the solution:
 \begin{itemize}
  \item Case : we return .
  \item Case :
    \begin{itemize}
      \item Case : we return .
      \item Case : if  then , otherwise we return .
      \item Case : we return .
      \item Case : if  then . We return .
    \end{itemize}
  \item Case :
    \begin{itemize}
      \item Case : we return .
      \item Case : if  then we perform the following operations: 
      ()~we add  to  and we subtract  to ; ()~we replace a possible pair of arities  by an arity ; 
      ()~we reorder~. If either the check fails or  then we return .
    \end{itemize}
 \end{itemize}
\end{enumerate}
\end{sloppypar}
\end{varwidth}
}

\newpage

\subsection{Reducing the number of processors at all the levels}

The following algorithm uses Algorithm 1 and 2 in order to compute a multiset of arities (sorted in descending order) 
minimizing the running time and the required number of processors at each step of the computation.~\\

\noindent\fbox{\begin{varwidth}{\dimexpr\linewidth-2\fboxsep-2\fboxrule\relax}
\paragraph{\textbf{Algorithm 3}} ~\\
INPUT: a message length .~\\
OUTPUT: an ordered multiset of arities
minimizing the running time and the required number of processors at each step of the computation.~\\
Let  be the multiset of arities returned by Algorithm 1. 
We then use Algorithm 2 with a message of length  and 
the multiset  to compute the multiset of arities . 
The rest of the algorithm proceeds iteratively as follows:
\begin{sloppypar}
\begin{enumerate}
 \item 
We apply Algorithm 2 on inputs  and  to 
compute the multiset . We set .
 \item As long as one of the following termination conditions is not met, namely
 ~; ()~the highest number of levels of arity  
has been reached (see Lemma \ref{numb_ar_4_and_5}); or ~, 
we set  and apply Algorithm 2 with the inputs  and 
 to compute the multiset 
.
 
\end{enumerate}
The resulting multiset of arities 
minimizes the number of required processors at each step of the computation.
\end{sloppypar}
\end{varwidth}
}



\end{document}
