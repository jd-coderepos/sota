\section{Experiments}












\subsection{Setup}

\subsubsection{Datasets and benchmarks}
\textbf{Objects365 dataset}~\cite{objects365}
is a large-scale, high-quality dataset for object detection. We use this dataset to conduct the modulated pre-training of our MQ-Det models.
It contains 0.66 million images of 365 object categories, with 30 million bounding boxes, which are more diverse and fine-grained than those in other popular datasets such as COCO~\cite{coco} and Pascal VOC~\cite{everingham2010pascal}. Objects365 is a widely-used pre-training dataset in previous foundamental detection models~\cite{glip,glipv2,groundingdino,omdet,florence,yao2023detclipv2}.


\textbf{LVIS benchmark}~\cite{lvis}
is a challenging dataset for long-tail objects. It contains 1,203 categories for evaluation, with many rare categories that scarcely exist in the pre-training datasets. Therefore, 
we use LVIS in the downstream task to evaluate the finetuning-free transferability. We report on MiniVal containing 5,000 images introduced in MDETR~\cite{mdetr} as well as the full validation set v1.0. 



\textbf{ODinW benchmark}~\cite{elevater} (Object Detection in the Wild) is a more challenging benchmark for evaluating model performance under real-world scenarios. For example, Aquarium requires locating fine-grained fish species, and Pothole concerns detecting holes on the road. It collects more than 35 datasets for evaluation.
There are two commonly used versions of the ODinW benchmark, \emph{i.e.}, ODinW-13 and ODinW-35, where ODinW-13 is a subset of ODinW-35 and contains much less noisy data.  The results on both benchmarks are reported for comprehensive comparison. We demonstrate that the fine-grained visual details in MQ-Det facilitate transfer to such diverse and challenging tasks.


\subsubsection{Implementation details}
\label{sec:implement_detail}
We conduct extensive experiments on three settings: an open-set setting on finetuning-free LVIS~\cite{lvis} and ODinW~\cite{elevater} benchmarks, a few-shot setting and a close-set full-shot setting on ODinW. 
To demonstrate the plug-and-play versatility of our approach, we apply MQ-Det on two typical SoTA language-queried object detectors, GLIP~\cite{glip} and GroundingDINO~\cite{groundingdino}, and obtain our multi-modal queried models \textbf{MQ-GLIP} and \textbf{MQ-GroundingDINO}, respectively. We incorporate our GCP modules into the last 6 layers of the frozen text encoder.
We conduct modulated pre-training of our models on the Objects365 dataset~\cite{objects365} for only one epoch using 8 NVIDIA V100 GPUs. The finetuning process under few/full-shot settings completely follows the baseline method GLIP, where vision queries are extracted from the few/full-shot training set.
We also evaluate our method in a \textit{finetuning-free} setting, namely, users can detect their customized objects through textual descriptions, visual exemplars, or both without any fine-tuning. During finetuning-free evaluation, we extract 5 instances as vision queries for each category from the downstream training set without any finetuning.



\begin{table}[t]
\centering
\caption{Finetuning-free detection on the LVIS benchmark.  denotes supervised approaches. The training time is tested on one V100 GPU. We present the number of vision queries during evaluation.}
\label{tab:lvis_zeroshot}
\resizebox{\linewidth}{!}{
\setlength{\tabcolsep}{1.0mm}{
\begin{tabular}{lc|cc|c|c|cccc|cccc}
\toprule
\multicolumn{1}{l}{\multirow{2}{*}{Model}} & \multirow{2}{*}{Backbone} & \multirow{2}{*}{Pre-Train Data} & Data     & Training Time &  \#Vision & \multicolumn{4}{c|}{MiniVal (\%)}                                                                           & \multicolumn{4}{c}{Val v1.0 (\%)}                                                                         \\
                       &                           &                                 &    Size   & (V100 days)   &  Query    &  &  &  &  &  &  &  &   \\
\midrule
MDETR~\cite{mdetr}                                     & RN101                    & GoldG,RefC             &   0.9M     &   400       &    0 & 24.2           & 20.9                       & 24.9                       & 24.3                       &    22.5   &       7.4              &         22.7          &          25.0         \\
Mask R-CNN~\cite{maskrcnn}                             & RN101                    &  -             &    -     &    -  &      0    & 33.3           & 26.3                       & 34.0                       & 33.9                       &        -            &        -             &          -          &    -    \\
Supervised-RFS~\cite{lvis}                             & RN50                    &  -             &    -     &      -    & 0                       & -                       & -       &    -                  & -          &    25.4   &        12.3            &        24.3             &          32.4           \\
\midrule
GLIP-T (B)~\cite{glip}                                     & Swin-T                    & O365             &  0.66M     &       300    &    0  & 17.8          & 13.5                       & 12.8                       & 22.2                      & 11.3         & 4.2                       & 7.6                       & 18.6                         \\
GLIP-T~\cite{glip}                                     & Swin-T                    & O365,GoldG,CC4M             &  5.5M    &  480          &    0    & 26.0          & 20.8                       & 21.4                       & 31.0                        & 17.2          & 10.1                       & 12.5                       & 25.5                       \\
GLIPv2-T~\cite{glipv2}                                     & Swin-T                    & O365,GoldG,CC4M             &  5.5M     &    -  &     0      & 29.0          &          -            &             -         &           -           &           -           &          -            &            -           &     -     \\
GroundingDINO-T~\cite{groundingdino}                                       & Swin-T                    & O365,GoldG,Cap4M      & 5.5M     &    -  &       0     &      25.7    &        15.2            &        21.9               &      30.9                 &            -             &       -                   &          -                &     -        \\
GLIP-L~\cite{glip}                                     & Swin-L                    & FourODs,GoldG,Cap24M             &   27.5M      &    600   &   0  & 37.3          & 28.2                       & 34.3                       & 41.5                       & 26.9          & 17.1                       & 23.3                       & 35.4                       \\
GroundingDINO-L~\cite{groundingdino}                                       & Swin-L                    & O365,OI,GoldG,Cap4M,COCO,RefC  &   15.8M   &    -  &      0      &       33.9    &      22.2               &           30.7           &                38.8        &          -                &           -               &           -               &      -        \\
\midrule
\rowcolor{Tabcolor} MQ-GLIP-T-Img                                       & Swin-T                    & O365\tablefootnote{\label{note}Modulating upon pretrained models indirectly utilizes their pre-training data.}                    &   0.66M   &    10        &  5  &   17.6   &        12.0          &          14.5          &        21.2          &            12.4             &          8.9             &          9.2              &      18.3      \\
\rowcolor{Tabcolor} MQ-GLIP-T-Txt                                       & Swin-T                    & O365\footref{note}                    &   0.66M   & 10     &   0          &   26.0     &        20.8           &      21.4              &         31.0        &         17.2               &          10.1              &           12.5             &     25.5       \\
\midrule
\rowcolor{Tabcolor} MQ-GroundingDINO-T                                       & Swin-T                    & O365\footref{note}             &    0.66M      &  10    &    5      &     30.2       &          21.7            &       26.2                &         35.2               &         22.1             &      12.9                  &           17.4             &       31.4    \\
\rowcolor{Tabcolor} MQ-GLIP-T                                       & Swin-T                    & O365\footref{note}                    &   0.66M  & 10   &      5          &     30.4     &   21.0                   &   27.5                     &        34.6             &             22.6          &          15.4             &            18.4           &    30.4       \\
\rowcolor{Tabcolor2} MQ-GLIP-L                                       & Swin-L                    & O365\footref{note}                    &     0.66M    &      22&     5     &    \textbf{43.4}      &   \textbf{34.5}                    &              \textbf{41.2}          &         \textbf{46.9}             &       \textbf{34.7}        &           \textbf{26.9}              &          \textbf{32.0}              &      \textbf{41.3}      \\
\bottomrule
\end{tabular}
}
}
\end{table}

























\subsection{MQ-Det helps low-shot object detection}
\label{sec:main_exp}
\subsubsection{Multi-modal queried detection without finetuning}

We evaluate the model’s ability to recognize rare and diverse objects on both LVIS and ODinW in a \textit{finetuning-free} setting, where users can detect their customized objects through textual descriptions, visual exemplars, or both without any fine-tuning.
Each category is provided with both language and vision queries.
Tab.\,\ref{tab:lvis_zeroshot} shows the results on LVIS. 
Overall, MQ-Det demonstrates \textbf{strong finetuning-free transferability} with impressive \textbf{efficiency on both data usage and training time}. 
MQ-GLIP-L surpasses the current SoTA by a large margin with simply 5 visual exemplars provided, reaching \% AP on Val v1.0 (+7.8\% over GLIP-L), which verifies the superiority of multi-modal queries over single-modal queries. 
Meanwhile, MQ-Det demonstrates good training efficiency, \emph{e.g.}, MQ-GLIP-T only requires additional 2\% training time and 12\% data usage when modulated upon GLIP-T. 


Additionally, we ﬁnd three contributing factors: 
\textbf{(i)} Strong generalization comes from combination of textual breadth and visual granularity.
In Tab.\,\ref{tab:lvis_zeroshot}, MQ-GLIP-T-Img replaces all text queries with [MASK] tokens and solely utilizes vision queries to predict objects, achieving 17.6\% AP. This confirms that our low-cost modulated pre-training strategy introduces sufficient visual cues.
Augmenting generalizable language with visual details, MQ-GLIP-T outperforms the text-only model MQ-GLIP-T-Txt by a large margin.
\textbf{(ii)} Efficiency on data usage and training time benefits from frozen pre-trained detection models. Our approach preserves the generalizable knowledge in the frozen pre-trained models and thus only needs little additional cost for further modulation. This is also verified in Tab.\,\ref{tab:ablation}\,(c).
\textbf{(iii)} Our simple plug-and-play architecture design enables wide-range applications, since most language-queried detectors share the similar parallel architecture design as GLIP and GroundingDINO, \emph{e.g.}, MDETR~\cite{mdetr}, OWL-ViT~\cite{owl-vit}, and DetCLIP~\cite{yao2023detclipv2} in Tab.\,\ref{tab:odinw}. 






\begin{table}[t]
\centering
\caption{Results on the ODinW benchmark. 
The few-shot performance is evaluated by 3-shot~\cite{groundingdino,glip}.}
\label{tab:odinw}
\resizebox{\linewidth}{!}{
\setlength{\tabcolsep}{0.8mm}{
\begin{tabular}{lcc|c|cc|cc}
\toprule 
\multirow{2}{*}{Model} & \multirow{2}{*}{Language Query} & \multirow{2}{*}{Vision Query} & \multirow{2}{*}{Backbone} & \multirow{2}{*}{Pre-train Data}  & Data & ODinW-35    &  ODinW-13                                  \\
                       &                                 &                               &                           &                          & Size       &    &  \\
\midrule
\multicolumn{7}{c}{\textit{Finetuning-free Setting}}                                                                                                                                                                                \\
\midrule
MDETR~\cite{mdetr}     & \Checkmark                    & \XSolidBrush                & ENB5~\cite{efficientnet}  & GoldG,RefC           &     0.9M       &       10.7                              &  25.1 \\
OWL-ViT~\cite{owl-vit} & \Checkmark                    & \Checkmark                  & ViT L/14(CLIP)            & O365, VG                  &   0.8M   &       18.8                                &  40.9\\
GLIP-T~\cite{glip}     & \Checkmark                    & \XSolidBrush                & Swin-T                    & O365,GoldG,Cap4M        &    5.5M  &       18.7                              &   41.9\\
GLIP-L~\cite{glip}     & \Checkmark                    & \XSolidBrush                & Swin-L                    & FourODs,GoldG,Cap24M        &   27.5M  &       22.6                              &  51.0\\
OmDet~\cite{omdet}     & \Checkmark                    & \XSolidBrush                & ConvNeXt-B                & COCO,O365,LVIS,PhraseCut     &  1.8M   &       16.0                & 43.6\\
GLIPv2-T~\cite{glipv2} & \Checkmark                    & \XSolidBrush                & Swin-T                    & O365,GoldG,Cap4M          &   5.5M     &       22.3                &  50.7\\
DetCLIP~\cite{yao2023detclipv2}  & \Checkmark          & \XSolidBrush                & Swin-T                    & O365,GoldG,YFCC1M             &  2.4M     &             -               &  43.3\\
GroundingDINO-T~\cite{groundingdino}& \Checkmark       & \XSolidBrush                & Swin-T                    & O365,GoldG,Cap4M           &   5.5M    &       21.7                          &  49.8\\
\midrule
 \rowcolor{Tabcolor} MQ-GroundingDINO-T      & \Checkmark                    & \Checkmark                  & Swin-T                    & O365\footref{note}                   &    0.66M        &      22.5                        &   50.9\\
\rowcolor{Tabcolor} MQ-GLIP-T               & \Checkmark                    & \Checkmark                  & Swin-T                    & O365\footref{note}                  &    0.66M        &       20.8                           &      45.6\\
\rowcolor{Tabcolor} MQ-GLIP-L               & \Checkmark                    & \Checkmark                  & Swin-L                    & O365\footref{note}                   &    0.66M     &      \textbf{23.9}                          &   \textbf{54.1}\\
\midrule
\multicolumn{7}{c}{\textit{Few-Shot Setting}}                                                                                                                                                                                 \\
\midrule
DyHead-T~\cite{dynamichead}& \XSolidBrush                  & \XSolidBrush                & Swin-T                    & O365                      &   0.66M     &      37.5                         &     43.1 \\
GLIP-T~\cite{glip}         & \Checkmark                    & \XSolidBrush                & Swin-T                    & O365,GoldG,Cap4M          &   5.5M     &      38.9                           &   50.7  \\
DINO-Swin-T~\cite{dino_det}& \XSolidBrush                  & \XSolidBrush                & Swin-T                    & O365                      &   0.66M       &      41.2                       &   49.0  \\
OmDet~\cite{omdet}         & \Checkmark                    & \XSolidBrush                & ConvNeXt-B                & COCO,O365,LVIS,PhraseCut   &     1.8M  &      42.4                       &  48.5\\
\midrule
\rowcolor{Tabcolor} MQ-GLIP-T                  & \Checkmark                    & \Checkmark                  & Swin-T                    & O365\footref{note}                      &    0.66M     &        \textbf{43.0}                          &   \textbf{57.0}\\
\midrule
\multicolumn{7}{c}{\textit{Full-Shot Setting}}                                                                                                                                                                                \\
\midrule
GLIP-T~\cite{glip}         & \Checkmark                    & \XSolidBrush                & Swin-T                    & O365,GoldG,Cap4M            &   5.5M    &      62.6                           &   61.9\\
DyHead-T~\cite{dynamichead}& \XSolidBrush                  & \XSolidBrush                & Swin-T                    & O365                        &   0.66M   &      63.2                               &   58.7\\
DINO-Swin-T~\cite{dino_det}& \XSolidBrush                  & \XSolidBrush                & Swin-T                    & O365                        &  0.66M     &      66.7                             &  - \\
OmDet~\cite{omdet}         & \Checkmark                    & \XSolidBrush                & ConvNeXt-B                & COCO,O365,LVIS,PhraseCut      &    1.8M  &      67.1                          & 65.3 \\
DINO-Swin-L~\cite{dino_det}& \XSolidBrush                  & \XSolidBrush                & Swin-L                    & O365                   &     0.66M     &      68.8                             &  67.3\\
\midrule
\rowcolor{Tabcolor} MQ-GLIP-T                 & \Checkmark                    & \Checkmark                  & Swin-T                    & O365\footref{note}                 &     0.66M       &          64.8                        &   62.5 \\
\bottomrule
\end{tabular}
}
}
\end{table}



\begin{table}[t]
\centering
\caption{Ablation results. We evaluate finetuning-free performance on LVIS and ODinW. Both average and median values on 13 datasets of ODinW are reported. Each row in this ablation study table should be compared to the baseline MQ-GLIP-T reported at the top of the table.}
\label{tab:ablation}
\resizebox{\linewidth}{!}{ 
\begin{tabular}{cllc >{\centering\arraybackslash}p{0.1cm} c|cccc|cc}
\toprule
& Ablated                        & Ablated                       & MQ-GLIP-T        &    \multirow{2}{*}{}       & Changed                     & \multicolumn{4}{c|}{LVIS MiniVal (\%)}                                       & \multicolumn{2}{c}{ODinW-13 (\%)}                 \\
& Setting                        & Details                       & Original Value           &                & Value                       &  &  &  &  &  &  \\ \midrule
\multicolumn{6}{c|}{\g{Original GLIP-T model}}                                                                                             &     \g{26.0}      &      \g{20.8}         &     \g{21.4}          &    \g{31.0}           &      \g{41.9}           &        \g{39.3}          \\ \midrule
\multicolumn{6}{c|}{\textbf{MQ-GLIP-T model}}                                                                                             &     \textbf{30.4}      &      \textbf{21.0}         &     \textbf{27.5}          &    \textbf{34.6}           &      \textbf{45.6}           &        \textbf{48.8}          \\ \midrule
\multirow{2}{*}{\textbf{(a)}} & \multirow{2}{*}{Mask Strategy} & \multirow{2}{*}{Mask Rate}    & 40\%                   &      & 0\%                           &    26.9       &       20.1       &      23.4         &     31.2         &              42.8    &       46.8          \\
            &                   &                               & 40\%                      &   & 80\%                         &   28.9       &       20.4         &      24.2         &      34.6         &        45.0         &     48.3           \\ \midrule

            
\multirow{4}{*}{\textbf{(b)}} & \multirow{4}{*}{Gate}          & Enable                    & \Checkmark  && \XSolidBrush   &      27.1     &               19.4    &        24.4        &        31.1        &        44.1        &      47.1            \\ \cmidrule{3-6}
                &               & \multirow{2}{*}{Architecture} & MLP                        & & Scaler                      &    
                29.3     &       18.3         &        26.3        &       34.0        &        43.4            &         46.0       
                \\
                &               &                               & MLP                       &  & Linear                      &     30.0      &      19.7         &    26.9          &         34.5      &      44.4         &         47.3        \\ \cmidrule{3-6}
                &               & Input                         &                         & & cat()                  &    29.2      &       17.8        &    26.0           &      34.2       &        44.8        &      48.2          \\ \midrule
\multirow{2}{*}{\textbf{(c)}}  &Freezing                       & Full Model                    & \Checkmark   & & \XSolidBrush &     24.4       &     28.6         &       21.1         &      17.2         &        38.6        &    45.4                 \\ 
\cmidrule{3-6}
 & Detector                       & Text Encoder                  & \Checkmark   & & \XSolidBrush &   26.6   & 17.3   &       23.0        &    31.5           &        42.7         &       46.6          \\ 


\bottomrule
\end{tabular}
}
\end{table}




\subsubsection{MQ-Det as a strong few-shot learner}
The experiments on ODinW with respect to finetuning-free, few-shot and full-shot settings are listed in Tab.\,\ref{tab:odinw}, where MQ-Det shows robust performance on this challenging benchmark. We observe that most large-scale fundamental detection models only support language as category queries. With additional vision queries, MQ-GLIP-T averagely improves GLIP-T by +3.7\% AP and +6.3\% AP in the finetuning-free and few-shot settings respectively on ODinW-13, thus exhibiting the superiority of the rich description granularity of vision queries in our approach. 
Meanwhile, MQ-GLIP-T sets a new record on the ODinW-35 few-shot setting with 43.0\% AP. Fig.\,\ref{fig:fewshot} demonstrates few-shot performance with respect to different amounts of training data. We also notice that MQ-GLIP-T makes limited gains over GLIP-T with sufficient training data in the full-shot setting (\emph{e.g.}, +0.6\% AP on ODinW-13). The reason is that the learning procedure, with sufficient training data, degenerates to a traditional close-set detection problem, where the model just represents categories as index regardless of textual or visual descriptions.



\subsection{Ablation studies}
\label{sec:ablation}

\textbf{Importance of the masked language prediction strategy.}
\label{sec:exp_mask}
We propose our masked language prediction strategy to address the learning inertia problem described in Sec.\,\ref{sec:pre-training}. Tab.\,\ref{tab:ablation}\,(a) verifies the effect of our pre-training strategy.
We observe that our approach holds a great performance degradation (-3.5\% AP) on LVIS with the mask rate of 0.  MQ-GLIP-T with the mask rate of 0 only improves GLIP-T by +0.9\% AP on LVIS. 
This indicates that our multi-modal queried MQ-GLIP may lose its effect and degenerate to language-queried GLIP without the  masked language prediction strategy. 
Meanwhile, a too large mask rate (\emph{e.g.}, 80\%) may lead to overfitting on the vision queries and losing attention to semantic-rich language queries, thus impairing performance. 



\begin{figure}[t]
\centering
\begin{minipage}[t]{0.36\textwidth}
\centering
\includegraphics[width=\textwidth]{imgs/fewshot.pdf}
\caption{Average precision (\%) on ODinW-13, from finetuning-free to full-shot data. MQ-Det clearly improves GLIP on data efficiency.}
\label{fig:fewshot}
\end{minipage}
\hspace{1.5mm}
\begin{minipage}[t]{0.6\textwidth}
\centering
\includegraphics[width=\textwidth]{imgs/vis.pdf}
\caption{Finetuning-free detection with different vision queries. The language queries of 1,203 LVIS categories are always provided. Similar boxes are located while vision queries with better quality provide more accurate predictions.}
\label{fig:vision_query}
\end{minipage}
\end{figure}



\textbf{The role of vision queries}.
MQ-Det provides users a flexible way to construct customized vision queries.
We use one vision query for each category of LVIS in MQ-GLIP-T, and investigate the effect of different query qualities as shown in Fig.\,\ref{fig:vision_query}. The vision queries are selected based on the following criteria: \textbf{(i)} Positive queries (in red) contain complete information about the target objects, which significantly improve detection. \textbf{(ii)} Hard positive queries (in brown) only provide partial object information and sometimes include additional noise, such as the example of the boy and car. Despite the additional noise, MQ-Det can filter it out and still help improve the detection accuracy to some extent, identifying at least one object correctly. \textbf{(iii)} Negative queries (in green) contain too much noise and do not benefit detection, while \textbf{(iv)} ``no vision queries'' (in blue) is used as a baseline. Finally, we also use \textbf{(v)} mixed queries (in purple) that combine the three types of vision queries. The results show the robustness of MQ-Det, namely, our approach can select relevant information from miscellaneous vision queries to boost detection.




\textbf{Architecture design}.
We design a plug-and-play GCP module so that visual details can be smoothly interleaved with highly-semantic textual cues. Particularly, in GCP, the conditional gating mechanism on each interleaved category token in Eqn.\,(\ref{eqn:gate}) ensures smooth model initialization from the frozen model and stable modulating according to different quality of visual cues. As shown in the first two rows of Tab.\,\ref{tab:ablation}\,(b), roughly adding visual cues to each text token, or equally scaling them using the same learnable gating scalar for all vision queries, can result in sub-optimal performance (-3.3\% AP and -1.1\% AP on LVIS, respectively). When using a simple gating design consisting of one linear layer over different vision queries, we obtain a slight performance drop compared to the non-linear MLP design, indicating content-aware gating mechanism is necessary and more parametric design helps analyze the quality of vision queries. In order to encourage the gate to balance the vision query  and the text query  in Eqn.\,(\ref{eqn:gate}), we combine the two query types as a joint input (the last row of Tab.\,\ref{tab:ablation}\,(b)). However, we discover that this task rises the learning difficulty for a simple gate design.


\textbf{Freezing pre-trained detectors}.
Tab.\,\ref{tab:ablation}\,(c) shows the necessity of modulating on the frozen models to introduce vision queries into the language-queried detectors.
Either modulating the whole pre-trained detector or only modulating the text encoder with the limited detection data inevitably lose the generalization capability of the initial language-queried model, leading to  a significant performance decrease (-6.0\% AP and  -3.8\% AP on LVIS respectively).
Moreover,  training without freezing the detector requires almost twice the time consumption, not to mention the additional memory cost.















































