\documentclass{llncs}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,mathpazo}

\usepackage[colorlinks=true,allcolors=blue,breaklinks,draft=false]{hyperref} \usepackage[rightcaption]{sidecap}
\usepackage{booktabs}
\usepackage[all]{xy}

\usepackage{etex}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage{stmaryrd}

\clearpage{}


\renewcommand{\l}{\langle}
\renewcommand{\r}{\rangle}

\newcommand{\smt}[1]{\texttt{#1}}

\newcommand{\pair}[2]{\mbox{}}
\newcommand{\tuple}[1]{\mbox{}}
\newcommand{\powerset}[1]{\mbox{}}
\newcommand{\powfin}[1]{\mbox{}}
\newcommand{\set}[1]{\mbox{}}
\newcommand{\alt}{\mathrel{|}}
\newcommand{\ent}{\vdash}
\newcommand{\imp}{\Rightarrow}
\newcommand{\bimp}{\Leftrightarrow}
\newcommand{\deq}{\triangleq}
\newcommand{\ptag}[1]{\tag*{\{#1\}}}

\newcommand{\oto}{\mathrel{\relbar\joinrel\circ}}     \newcommand{\ofrom}{\mathrel{\relcirc\joinrel\relbar}}   \newcommand{\x}{\times}
\newcommand{\os}{\oslash}
\newcommand{\oa}{\oplus}
\newcommand{\ox}{\otimes}
\newcommand{\bigox}{\bigotimes}
\newcommand{\bigoa}{\bigoplus}
\newcommand{\tadj}{\dashv}        \newcommand{\girpar}{\bindnasrempa}

\newcommand{\reals}{\mathbb{R}}
\newcommand{\rats}{\mathbb{Q}}
\newcommand{\ints}{\mathbb{Z}}
\newcommand{\nats}{\mathbb{N}}
\newcommand{\nif}{\mathb{N}^{\infty}}

\newcommand{\may}[1]{\mbox{}}
\newcommand{\must}[1]{\mbox{}}
\newcommand{\semantics}[1]{\mbox{}}

\newcommand{\pre}{\sqsubseteq}
\newcommand{\ua}[1]{\mbox{}}    
\newcommand{\da}[1]{\mbox{}}    
\newcommand{\dirsup}{\bigsqcup}
\newcommand{\lub}{\bigvee}
\newcommand{\glb}{\bigwedge}
\newcommand{\stepfn}[2]{\mbox{}}
\newcommand{\dda}{\mathord{\mbox{\makebox[0pt][l]{\raisebox{-.4ex}
                           {}}}}}
\newcommand{\dua}{\mathord{\mbox{\makebox[0pt][l]{\raisebox{.4ex}
                           {}}}}}

\newcommand{\maclane}{Mac\thinspace Lane}


\newcommand{\bA}{\mathbf{A}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bC}{\mathbf{C}}
\newcommand{\bD}{\mathbf{D}}
\newcommand{\bE}{\mathbf{E}}
\newcommand{\bF}{\mathbf{F}}
\newcommand{\bG}{\mathbf{G}}
\newcommand{\bH}{\mathbf{H}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bJ}{\mathbf{J}}
\newcommand{\bK}{\mathbf{K}}
\newcommand{\bL}{\mathbf{L}}
\newcommand{\bM}{\mathbf{M}}
\newcommand{\bN}{\mathbf{N}}
\newcommand{\bO}{\mathbf{O}}
\newcommand{\bP}{\mathbf{P}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bR}{\mathbf{R}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bZ}{\mathbf{Z}}

\newcommand{\sA}{\mathsf{A}}
\newcommand{\sB}{\mathsf{B}}
\newcommand{\sC}{\mathsf{C}}
\newcommand{\sD}{\mathsf{D}}
\newcommand{\sE}{\mathsf{E}}
\newcommand{\sF}{\mathsf{F}}
\newcommand{\sG}{\mathsf{G}}
\newcommand{\sH}{\mathsf{H}}
\newcommand{\sI}{\mathsf{I}}
\newcommand{\sJ}{\mathsf{J}}
\newcommand{\sK}{\mathsf{K}}
\newcommand{\sL}{\mathsf{L}}
\newcommand{\sM}{\mathsf{M}}
\newcommand{\sN}{\mathsf{N}}
\newcommand{\sO}{\mathsf{O}}
\newcommand{\sP}{\mathsf{P}}
\newcommand{\sQ}{\mathsf{Q}}
\newcommand{\sR}{\mathsf{R}}
\newcommand{\sS}{\mathsf{S}}
\newcommand{\sT}{\mathsf{T}}
\newcommand{\sU}{\mathsf{U}}
\newcommand{\sV}{\mathsf{V}}
\newcommand{\sW}{\mathsf{W}}
\newcommand{\sX}{\mathsf{X}}
\newcommand{\sY}{\mathsf{Y}}
\newcommand{\sZ}{\mathsf{Z}}

\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cZ}{\mathcal{Z}}

\newcommand{\mfrA}{\mathfrak{A}}
\newcommand{\mfrB}{\mathfrak{B}}
\newcommand{\mfrC}{\mathfrak{C}}
\newcommand{\mfrD}{\mathfrak{D}}
\newcommand{\mfrE}{\mathfrak{E}}
\newcommand{\mfrF}{\mathfrak{F}}
\newcommand{\mfrG}{\mathfrak{G}}
\newcommand{\mfrH}{\mathfrak{H}}
\newcommand{\mfrI}{\mathfrak{I}}
\newcommand{\mfrJ}{\mathfrak{J}}
\newcommand{\mfrK}{\mathfrak{K}}
\newcommand{\mfrL}{\mathfrak{L}}
\newcommand{\mfrM}{\mathfrak{M}}
\newcommand{\mfrN}{\mathfrak{N}}
\newcommand{\mfrO}{\mathfrak{O}}
\newcommand{\mfrP}{\mathfrak{P}}
\newcommand{\mfrQ}{\mathfrak{Q}}
\newcommand{\mfrR}{\mathfrak{R}}
\newcommand{\mfrS}{\mathfrak{S}}
\newcommand{\mfrT}{\mathfrak{T}}
\newcommand{\mfrU}{\mathfrak{U}}
\newcommand{\mfrV}{\mathfrak{V}}
\newcommand{\mfrW}{\mathfrak{W}}
\newcommand{\mfrX}{\mathfrak{X}}
\newcommand{\mfrY}{\mathfrak{Y}}
\newcommand{\mfrZ}{\mathfrak{Z}}

\newcommand{\bbA}{\mathbb{A}}
\newcommand{\bbB}{\mathbb{B}}
\newcommand{\bbC}{\mathbb{C}}
\newcommand{\bbD}{\mathbb{D}}
\newcommand{\bbE}{\mathbb{E}}
\newcommand{\bbF}{\mathbb{F}}
\newcommand{\bbG}{\mathbb{G}}
\newcommand{\bbH}{\mathbb{H}}
\newcommand{\bbI}{\mathbb{I}}
\newcommand{\bbJ}{\mathbb{J}}
\newcommand{\bbK}{\mathbb{K}}
\newcommand{\bbL}{\mathbb{L}}
\newcommand{\bbM}{\mathbb{M}}
\newcommand{\bbN}{\mathbb{N}}
\newcommand{\bbO}{\mathbb{O}}
\newcommand{\bbP}{\mathbb{P}}
\newcommand{\bbQ}{\mathbb{Q}}
\newcommand{\bbR}{\mathbb{R}}
\newcommand{\bbS}{\mathbb{S}}
\newcommand{\bbT}{\mathbb{T}}
\newcommand{\bbU}{\mathbb{U}}
\newcommand{\bbV}{\mathbb{V}}
\newcommand{\bbW}{\mathbb{W}}
\newcommand{\bbX}{\mathbb{X}}
\newcommand{\bbY}{\mathbb{Y}}
\newcommand{\bbZ}{\mathbb{Z}}

\newcommand{\defn}[1]{\textbf{#1}}

\newcommand{\decision}[3]{\begin{list}{}{
\setlength{\parsep}{0pt}
\setlength{\itemsep}{2pt}
\setlength{\topsep}{\itemsep}
\setlength{\partopsep}{\itemsep}
}
\item
{\underline{\textsc{#1}}}
\item
{\textbf{INPUT:} #2}
\item
{\textbf{QUESTION:} #3}
\end{list}
}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}

\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\clearpage{}

\renewcommand{\sectionautorefname}{\S}
\renewcommand{\subsectionautorefname}{\S}

\title{On partial order semantics for SAT/SMT-based symbolic encodings of weak memory concurrency\thanks{This work is funded by a gift from Intel Corporation for research on Effective Validation of Firmware and the ERC project ERC 280053.}}
\author{Alex Horn \and Daniel Kroening}

\institute{University of Oxford}

\begin{document}

\maketitle
\begin{abstract}
Concurrent systems are notoriously difficult to analyze, and technological advances such as weak memory architectures greatly compound this problem.
This has renewed interest in partial order semantics as a theoretical foundation for formal verification techniques.
Among these, symbolic techniques have been shown to be particularly effective at finding concurrency-related bugs because they can leverage highly optimized decision procedures such as SAT/SMT solvers.
This paper gives new fundamental results on partial order semantics for SAT/SMT-based symbolic encodings of weak memory concurrency.
In particular, we give the theoretical basis for a decision procedure that can handle a fragment of concurrent programs endowed with least fixed point operators.
In addition, we show that a certain partial order semantics of relaxed sequential consistency is equivalent to the conjunction of three extensively studied weak memory axioms by Alglave et al.
An important consequence of this equivalence is an asymptotically smaller symbolic encoding for bounded model checking which has only a quadratic number of partial order constraints compared to the state-of-the-art cubic-size encoding.
\end{abstract}

\section{Introduction}
\vspace{-0.3em}
\label{section:introduction}

Concurrent systems are notoriously difficult to analyze, and technological advances such as weak memory architectures as well as highly available distributed services greatly compound this problem. This has renewed interest in partial order concurrency semantics as a theoretical foundation for formal verification techniques. Among these, \emph{symbolic techniques} have been shown to be particularly effective at finding concurrency-related bugs because they can leverage highly optimized decision procedures such as SAT/SMT solvers. This paper studies partial order semantics from the perspective of SAT/SMT-based symbolic encodings of weak memory concurrency.

Given the diverse range of partial order concurrency semantics, we link our study to a recently developed unifying theory of concurrency by Tony Hoare et al.~\cite{HMSW2011}. This theory is known as \emph{Concurrent Kleene Algebra} (CKA) which is an algebraic concurrency semantics based on quantales, a special case of the fundamental algebraic structure of idempotent semirings. Based on quantales, CKA combines the familiar laws of the sequential program operator () with a new operator for concurrent program composition (). A distinguishing feature of CKA is its exchange law  that describes how sequential and concurrent composition operators can be interchanged. Intuitively, since the binary relation  denotes program refinement, the exchange law expresses a divide-and-conquer mechanism for how concurrency may be sequentially implemented on a machine. The exchange law, together with a uniform treatment of programs and their specifications, is key to unifying existing theories of concurrency~\cite{HvS2014}. CKA provides such a unifying theory~\cite{HvS2012,HvS2014} that has practical relevance on proving program correctness, e.g. using rely/guarantee reasoning~\cite{HMSW2011}. Conversely, however, pure algebra cannot refute that a program is correct or that certain properties about every program always hold~\cite{HvS2012,HvS2014,HvSMSVZOH2014}. This is problematic for theoretical reasons but also in practice because todays software complexity requires a diverse set of program analysis tools that range from proof assistants to automated testing. The solution is to accompany CKA with a mathematical model which satisfies its laws so that we can \emph{prove} as well as \emph{disprove} properties about programs.

One such well-known model-theoretical foundation for CKA is Pratt's~\cite{P1986} and Gischer's~\cite{G1988} partial order model of computation that is constructed from \emph{labelled partially ordered multisets} (pomsets). Pomsets generalize the concept of a string in finite automata theory by relaxing the total ordering of the occurrence of letters within a string to a partial order. For example,  denotes a pomset that consists of two unordered events that are both labelled with the letter . By partially ordering events, pomsets form an integral part of the extensive theoretical literature on so-called `true concurrency', e.g.~\cite{P1966,L1978,G1981,NPW1981,P1986,G1988}, in which pomsets strictly generalize Mazurkiewicz traces~\cite{BK1992}, and prime event structures~\cite{NPW1981} are pomsets enriched with a conflict relation subject to certain conditions. From an algorithmic point of view, the complexity of the \emph{pomset language membership} (PLM) problem is NP-complete, whereas the pomset language containment (PLC) problem is -complete~\cite{FKL1993}.

Importantly, these aforementioned theoretical results only apply to star-free pomset languages (without fixed point operators). In fact, the decidability of the equational theory of the pomset language closed under least fixed point, sequential and concurrent composition operators (but without the exchange law) has been only most recently established~\cite{LS2014}; its complexity remains an open problem~\cite{LS2014}. Yet another open problem is the decidability of this equational theory together with the exchange law~\cite{LS2014}. In addition, it is still unclear how theoretical results about pomsets may be applicable to formal techniques for finding concurrency-related bugs. In fact, it is not even clear how insights about pomsets may be combined with most recently studied language-specific or hardware-specific concurrency semantics, e.g.~\cite{SSONM2010,SVZJS2011,BOSSW2011,AMSS2012}.

These gaps are motivation to reinvestigate pomsets from an algorithmic perspective. In particular, our work connects pomsets to a SAT/SMT-based bounded model checking technique~\cite{AKT2013} where shared memory concurrency is symbolically encoded as partial orders. To make this connection, we adopt pomsets as \emph{partial strings} (Definition~\ref{def:partial-string}) that are ordered by a refinement relation (Definition~\ref{def:partial-string-isomorphism}) based on \'{E}sik's notion of \emph{monotonic bijective morphisms}~\cite{E2002}. Our partial-string model then follows from the standard Hoare powerdomain construction where sets of partial strings are downward-closed with respect to monotonic bijective morphism (Definition \ref{def:program}). The relevance of this formalization for the modelling of weak memory concurrency (including data races) is explained through several examples. Our main contributions are as follows:
\begin{enumerate}
\item We give the theoretical basis for a decision procedure that can handle a fragment of \emph{concurrent programs endowed with least fixed point operators} (Theorem~\ref{theorem:program-reduction}). This is accomplished by exploiting a form of periodicity, thereby giving a mechanism for reducing a countably infinite number of events to a finite number. This result particularly caters to partial order encoding techniques that can currently only encode a finite number of events due to the deliberate restriction to quantifier-free first-order logic, e.g.~\cite{AKT2013}.
\item We then interpret a particular form of weak memory in terms of certain downward-closed sets of partial strings (Definition~\ref{def:SC-relaxed-program}), and show that our interpretation is equivalent to the conjunction of three fundamental weak memory axioms (Theorem~\ref{theorem:SC-relaxed-equivalence}), namely `write coherence', `from-read' and `global read-from'~\cite{AMSS2012}. Since all three axioms underpin extensive experimental research into weak memory architectures~\cite{AMSS2011}, \emph{Theorem~\ref{theorem:SC-relaxed-equivalence} gives denotational partial order semantics a new practical dimension}.
\item Finally, we prove that there exists an \emph{asymptotically smaller quantifier-free first-order logic formula} that has only  partial order constraints (Theorem~\ref{theorem:smaller-fr}) compared to the state-of-the-art  partial order encoding for bounded model checking~\cite{AKT2013} where  is the maximal number of reads and writes on the same shared memory address. This is significant because  can be prohibitively large when concurrent programs frequently share data.
\end{enumerate}

The rest of this paper is organized into three parts. First, we recall familiar concepts on partial-string theory (\autoref{section:partial-string-theory}) on which the rest of this paper is based. We then prove a least fixed point reduction result (\autoref{label:least-fixed-point-reduction}). Finally, we characterize a particular form of relaxed sequential consistency in terms of three weak memory axioms by Alglave et al. (\autoref{section:SC-relaxed}).

\section{Partial-string theory}
\vspace{-0.3em}
\label{section:partial-string-theory}

In this section, we adapt an axiomatic model of computation that uses partial orders to describe the semantics of concurrent systems. For this, we recall familiar concepts (Definition~\ref{def:partial-string},~\ref{def:partial-string-composition},~\ref{def:partial-string-isomorphism}~and~\ref{def:program}) that underpin our mathematical model of CKA (Theorem~\ref{theorem:program-algebra}). This model is the basis for subsequent results in~\autoref{label:least-fixed-point-reduction}~and~\autoref{section:SC-relaxed}.

\begin{definition}[Partial string]
\label{def:partial-string}
Denote with  a nonempty set of \defn{events}. Let  be an \defn{alphabet}. A \defn{partial string}  is a triple  where  is a subset of ,  is a function that maps each event in  to an alphabet symbol in , and  is a partial order on . Two partial strings  and  are said to be \defn{disjoint} whenever . A partial string  is called \defn{empty} whenever . Denote with  the set of all \defn{finite partial strings}  whose event set  is finite.
\end{definition}

Each event in the universe  should be thought of as an occurrence of a computational step, whereas letters in  describe the computational effect of events. Typically, we denote a partial string by , or letters from  through . In essence, a partial string  is a partially-ordered set  equipped with a labelling function . A partial string is therefore the same as a \emph{labelled partial order} (lpo), see also Remark~\ref{remark:partial-string}. We draw finite partial strings in  as inverted Hasse diagrams (e.g. Fig.~\ref{fig:partial-string-example}), where the ordering between events may be interpreted as a happens-before relation~\cite{L1978}, a fundamental notion in distributed systems and formal verification of concurrent systems, e.g.~\cite{BOSSW2011,AMSS2012}. We remark the obvious fact that the empty partial string is unique under component-wise equality.

\begin{example}
In the partial string in Fig.~\ref{fig:partial-string-example},  happens-before , whereas both  and  happen concurrently because neither  nor .
\end{example}

\begin{SCfigure}[100][t]
\xymatrix@C=1.2em@R=0.9em{
  e_0            & e_2            \\
  e_1\ar@{<-}[u] & e_3\ar@{<-}[u]
}
\caption{
A partial string  with events  and the labelling function  satisfying the following: , ,  and .
}
\label{fig:partial-string-example}
\end{SCfigure}

We abstractly describe the control flow in concurrent systems by adopting the sequential and concurrent operators on labelled partial orders~\cite{G1981,P1986,G1988,E2002,HA2014}.

\begin{definition}[Partial string operators]
\label{def:partial-string-composition}
Let  and  be disjoint partial strings. Let  and  be their \defn{concurrent} and \defn{sequential composition}, respectively, where  such that, for all events  in , the following holds:
\begin{itemize}
\item ,
\item ,
\item 
\end{itemize}
\end{definition}

For simplicity, we assume that partial strings can be always made disjoint by renaming events if necessary. But this assumption could be avoided by using coproducts, a form of constructive disjoint union~\cite{HA2014}. When clear from the context, we construct partial strings directly from the labels in .

\begin{example}
If we ignore labels for now and let  for all  be four partial strings which each consist of a single event , then  corresponds to a partial string that is isomorphic to the one shown in Fig.~\ref{fig:partial-string-example}.
\end{example}

To formalize the set of all possible happens-before relations of a concurrent system, we rely on \'{E}sik's notion of monotonic bijective morphism~\cite{E2002}:

\begin{definition}[Partial string refinement]
\label{def:partial-string-isomorphism}
Let  and  be partial strings such that  and . A \defn{monotonic bijective morphism} from  to , written , is a bijective function  from  to  such that, for all events , , and if , then . Then  \defn{refines} , written , if there exists a monotonic bijective morphism  from  to .
\end{definition}

\begin{remark}
\label{remark:partial-string}
Partial words~\cite{G1981} and pomsets~\cite{P1986,G1988} are defined in terms of isomorphism classes of lpos. Unlike lpos in pomsets, however, we study partial strings in terms of monotonic bijective morphisms~\cite{E2002} because isomorphisms are about sameness whereas the exchange law on partial strings is an inequation~\cite{HA2014}.
\end{remark}

\begin{SCfigure}[100][t]
\xy
  \xymatrix "M"@C=1.2em@R=1em{
    e_0\ar@{.>}@/^1pc/[rrr]             & e_2\ar@{.>}@/^1pc/[rrr]             &              & e'_0             & e'_2           \\
    e_1\ar@{<-}[u]\ar@{.>}@/^-1pc/[rrr] & e_3\ar@{<-}[u]\ar@{.>}@/^-1pc/[rrr] &   \ar@{~}[u] & e'_1\ar@{<-}[u] & e'_3\ar@{<-}[u]\ar@{<-}[lu] \\
&&&&\\
  }
\POS"M3,1"."M3,2"!C*\frm{_\}},+D*++!U\txt{}
    ,"M3,4"."M3,5"!C*\frm{_\}},+D*++!U\txt{}
\endxy
\caption{Two partial strings  and  such that  provided all the labels are preserved, e.g. .}
\label{fig:sqsubseteq-partial-string}
\end{SCfigure}

The purpose of Definition~\ref{def:partial-string-isomorphism} is to disregard the identity of events but retain the notion of `subsumption', cf.~\cite{G1988}. The intuition is that  orders partial strings according to their determinism. In other words,  for partial strings  and  implies that all events ordered in  have the same order in .

\begin{example}
\label{example:sqsubseteq-partial-string}
Fig.~\ref{fig:sqsubseteq-partial-string} shows a monotonic bijective morphism from a partial string as given in Fig.~\ref{fig:partial-string-example} to an -shaped partial string that is almost identical to the one in Fig.~\ref{fig:partial-string-example} except that it has an additional partial order constraint, giving its  shape. One well-known fact about -shaped partial strings is that they cannot be constructed as  or  under any labelling~\cite{P1986}. However, this is not a problem for our study, as will become clear after Definition~\ref{def:program}.
\end{example}

Our notion of partial string refinement is particularly appealing for symbolic techniques of concurrency because the monotonic bijective morphism can be directly encoded as a first-order logic formula modulo the theory of uninterpreted functions. Such a symbolic partial order encoding would be fully justified from a computational complexity perspective, as shown next.

\begin{proposition}
\label{proposition:psr-NP-complete}
Let  and  be finite partial strings in . The \defn{partial string refinement} (PSR) problem --- i.e. whether  --- is NP-complete.
\end{proposition}
\begin{proof}
Clearly \textsc{PSR} is in NP. The NP-hardness proof proceeds by reduction from the PLM problem~\cite{FKL1993}. Let  be the set of strings, i.e. the set of finite partial strings  such that  is a total order (for all ,  or ). Given a finite partial string , let  be the set of all strings which refine ; equivalently, . So  denotes the same as  in~\cite[Definition 2.2]{FKL1993}.

Let  be a string in  and  be a pomset over the alphabet . By Remark~\ref{remark:partial-string}, fix  to be a partial string in . Thus  refines  if and only if  is a member of . Since this membership problem is NP-hard~\cite[Theorem 4.1]{FKL1993}, it follows that the \textsc{PSR} problem is NP-hard. So the \textsc{PSR} problem is NP-complete. \qed
\end{proof}

Note that a single partial string is not enough to model mutually exclusive (nondeterministic) control flow. To see this, consider a simple (possibly sequential) system such as \texttt{if * then P else Q} where \texttt{*} denotes nondeterministic choice. If the semantics of a program was a single partial string, then we need to find exactly one partial string that represents the fact that \texttt{P} executes or \texttt{Q} executes, but never both. To model this, rather than using a conflict relation~\cite{NPW1981}, we resort to the simpler Hoare powerdomain construction where we lift sequential and concurrent composition operators to \emph{sets} of partial strings. But since we are aiming (similar to Gischer~\cite{G1988}) at an \emph{over-approximation of concurrent systems}, these sets are downward closed with respect to our partial string refinement ordering from Definition~\ref{def:partial-string-isomorphism}. Additional benefits of using the downward closure include that program refinement then coincides with familiar set inclusion and the ease with which later the Kleene star operators can be defined.

\begin{definition}[Program]
\label{def:program}
A \defn{program} is a downward-closed set of finite partial strings with respect to ; equivalently  is a program whenever  where . Denote with  the family of all programs.
\end{definition}

Since we only consider systems that terminate, each partial string  in a program  is finite. We reemphasize that the downward closure of such a set  can be thought of as an over-approximation of all possible happens-before relations in a concurrent system whose instructions are ordered according to the partial strings in . Later on (\autoref{section:SC-relaxed}) we make the downward closure of partial strings more precise to model a certain kind of relaxed sequential consistency.

\begin{example}
\label{example:program}
Recall that -shaped partial strings cannot be constructed as  or  under any labelling~\cite{P1986}. Yet, by downward-closure of programs, such partial strings are included in the over-approximation of all the happens-before relations exhibited by a concurrent system. In particular, according to Example~\ref{example:sqsubseteq-partial-string}, the downward-closure of the set containing the partial string in Fig.~\ref{fig:partial-string-example} includes (among many others) the -shaped partial string shown on the right in Fig.~\ref{fig:sqsubseteq-partial-string}. In fact, we shall see in~\autoref{section:SC-relaxed} that this particular -shaped partial string corresponds to a data race in the concurrent system shown in Fig.~\ref{fig:intro-example-racy}.
\end{example}

It is standard~\cite{G1988,HA2014} to define  and  where  is the (unique) empty partial string. Clearly  and  form programs in the sense of Definition~\ref{def:program}. For the next theorem, we lift the two partial string operators (Definition~\ref{def:partial-string-composition}) to programs in the standard way:

\begin{definition}[Bow tie]
\label{def:program-operator}
Given two partial strings  and , denote with  either concurrent or sequential composition of  and . For all programs  in  and partial string operators ,  where  and  are called \defn{concurrent} and \defn{sequential program composition}, respectively.
\end{definition}

By denoting programs as sets of partial strings, we can now define Kleene star operators  and  for iterative concurrent and sequential program composition, respectively, as least fixed points () using set union () as the binary join operator that we interpret as the nondeterministic choice of two programs. We remark that this is fundamentally different from the pomsets recursion operators in ultra-metric spaces~\cite{BW1990}. The next theorem could be then summarized as saying that the resulting structure of programs, written , is a partial order model of an algebraic concurrency semantics that satisfies the CKA laws~\cite{HMSW2011}. Since CKA is an exemplar of the universal laws of programming~\cite{HvS2014}, we base the rest of this paper on our partial order model of CKA.

\begin{theorem}
\label{theorem:program-algebra}
The structure  is a complete lattice, ordered by subset inclusion (i.e.  exactly if ), such that  and  form unital quantales over  where  satisfies the following:

\end{theorem}
\begin{proof}
The details are in the accompanying technical report of this paper~\cite{HA2014}.
\end{proof}

By Theorem~\ref{theorem:program-algebra}, it makes sense to call  in structure  the \defn{-identity program} where  is a placeholder for either  or . In the sequel, we call the binary relation  on  the \defn{program refinement relation}.

\section{Least fixed point reduction}
\vspace{-0.3em}
\label{label:least-fixed-point-reduction}

This section is about the least fixed point operators  and . Henceforth, we shall denote these by . We show that under a certain finiteness condition (Definition~\ref{def:program-elementary}) the program refinement problem  can be reduced to a bounded number of program refinement problems without least fixed points (Theorem~\ref{theorem:program-reduction}). To prove this, we start by inductively defining the notion of iteratively composing a program with itself under .

\begin{definition}[-iterated--program-composition]
\label{def:program-n-iterated-composition}
Let  be the set of \defn{non-negative integers}. For all programs  in  and non-negative integers  in ,  is the -identity program and .
\end{definition}

Clearly  is the limit of its approximations in the following sense:

\begin{proposition}
\label{proposition:program-least-fixed-point-as-n-iterated-composition}
For every program  in , .
\end{proposition}


\begin{definition}[Elementary program]
\label{def:program-elementary}
A program  in  is called \defn{elementary} if  is the downward-closed set with respect to  of some finite and nonempty set  of finite partial strings, i.e. . The set of elementary programs is denoted by .
\end{definition}

An elementary program therefore could be seen as a machine-representable program generated from a finite and nonempty set of finite partial strings. This finiteness restriction makes the notion of elementary programs a suitable candidate for the study of decision procedures. To make this precise, we define the following unary partial string operator:

\begin{definition}[-repeated- partial string operator]
\label{def:partial-string-dot-coproduct}
For every non-negative integer  in ,  is the empty partial string and .
\end{definition}

Intuitively,  is a partial string that consists of  copies of a partial string , each combined by the partial string operator . This is formalized as follows:

\begin{proposition}
Let  be a non-negative integer. Define  and . For every partial string ,  is isomorphic to  where  such that, for all  and , the following holds:
\begin{itemize}
\item if `' is `', then  exactly if  and ,
\item if `' is `', then  exactly if  or ( and ),
\item .
\end{itemize}
\end{proposition}

\begin{definition}[Partial string size]
\label{def:partial-string-size}
The \defn{size} of a finite partial string , denoted by , is the cardinality of its event set .
\end{definition}

For example, the partial string in Fig.~\ref{fig:partial-string-example} has size four. It is obvious that the size of finite partial strings is non-decreasing under the -repeated- partial string operator from Definition~\ref{def:partial-string-dot-coproduct} whenever . This simple fact is important for the next step towards our least fixed point reduction result in Theorem~\ref{theorem:program-reduction}:

\begin{proposition}[Elementary least fixed point pre-reduction]
\label{proposition:program-forward-finite-reduction}
For all elementary programs  and  in , if the -identity program  is not in  and , then  where  such that  and  is the size of the largest and smallest partial strings in  and , respectively.
\end{proposition}
\begin{proof}
Assume . Let  be a finite partial string. We can assume  because . By assumption, . By Proposition~\ref{proposition:program-least-fixed-point-as-n-iterated-composition}, there exists  such that . Fix  to be the smallest such non-negative integer. Show  (the fraction is well-defined because  and  are nonempty and ). By downward closure and definition of  in terms of a one-to-one correspondence, it suffices to consider that  is one of a (not necessarily unique) longest partial strings in , i.e.  for all ; equivalently, . If , set , satisfying  and  as required. Otherwise, since the size of partial strings in a program can never decrease under the -iterated program composition operator  when , it suffices to consider the case  for some shortest partial string  in . Since  is the Cartesian product of  and , it follows . Since  and , . By definition , proving . \qed
\end{proof}

Equivalently, if there exists a partial string  in  such that  for all non-negative integers  between zero and , then . Since we are interested in decision procedures for program refinement checking, we need to show that the converse of Proposition~\ref{proposition:program-forward-finite-reduction} also holds. Towards this end, we prove the following left  elimination rule:

\begin{proposition}
\label{proposition:program-eliminate-left-lfp}
For every program  and   in ,  exactly if .
\end{proposition}
\begin{proof}
Assume . By Proposition~\ref{proposition:program-least-fixed-point-as-n-iterated-composition}, . By transitivity of  in , . Conversely, assume . Let . By induction on , . Thus, by Proposition~\ref{proposition:program-least-fixed-point-as-n-iterated-composition} and distributivity of  over least upper bounds in , , i.e.  is idempotent. This, in turn, implies that  is a closure operator. Therefore, by monotonicity, , proving that  is equivalent to . \qed
\end{proof}

\begin{theorem}[Elementary least fixed point reduction]
\label{theorem:program-reduction}
For all elementary programs  and  in , if the -identity program  is not in , then  is equivalent to  where  such that  and  is the size of the largest and smallest partial strings in  and , respectively.
\end{theorem}
\begin{proof}
By Proposition~\ref{proposition:program-eliminate-left-lfp}, it remains to show that  is equivalent to  where . The forward and backward implication follow from Proposition~\ref{proposition:program-forward-finite-reduction}~and~\ref{proposition:program-least-fixed-point-as-n-iterated-composition}, respectively. \qed
\end{proof}

From Theorem~\ref{theorem:program-reduction} follows immediately that  is decidable for all elementary programs  and  in  because there exists an algorithm that could iteratively make  calls to another decision procedure to check whether  for all  and  where . However, by Proposition~\ref{proposition:psr-NP-complete}, each iteration in such an algorithm would have to solve an NP-complete subproblem. But this high complexity is expected since the PLC problem is -complete~\cite{FKL1993}.

\begin{corollary}
For all elementary programs  and  in , if  for all  and , then  is equivalent to .
\end{corollary}

We next move on to enriching our model of computation to accommodate a certain kind of relaxed sequential consistency.

\section{Relaxed sequential consistency}
\vspace{-0.3em}
\label{section:SC-relaxed}

For efficiency reasons, all modern computer architectures implement some form of weak memory model rather than sequential consistency~\cite{L1979}. A defining characteristic of weak memory architectures is that they violate interleaving semantics unless specific instructions are used to restore sequential consistency. This section fixes a particular interpretation of weak memory and studies the mathematical properties of the resulting partial order semantics. For this, we separate memory accesses into synchronizing and non-synchronizing ones, akin to~\cite{GLLGGH1990}. A synchronized store is called a \emph{release}, whereas a synchronized load is called an \emph{acquire}. The intuition behind release/acquire is that prior writes made to other memory locations by the thread executing the release become visible in the thread that performs the corresponding acquire. Crucially, the particular form of release/acquire semantics that we formalize here is shown to be equivalent to the conjunction of three weak memory axioms (Theorem~\ref{theorem:SC-relaxed-equivalence}), namely `write coherence', `from-read' and `global read-from'~\cite{AMSS2012}. Subsequently, we look at one important ramification of this equivalence on \emph{bounded model checking} (BMC) techniques for finding concurrency-related bugs (Theorem~\ref{theorem:smaller-fr}).

We start by defining the alphabet that we use for identifying events that denote synchronizing and non-synchronizing memory accesses.

\newcommand{\memorylocation}{\langle\textit{ADDRESS}\rangle}
\newcommand{\register}{\langle\textit{REG}\rangle}
\newcommand{\loadmemoryorder}{\langle\textit{LOAD}\rangle}
\newcommand{\storememoryorder}{\langle\textit{STORE}\rangle}
\newcommand{\bit}{\langle\textit{BIT}\rangle}

\begin{definition}[Memory access alphabet]
\label{def:memory-access-alphabet}
Define ,  and . Let  and  be disjoint sets of \defn{memory locations} and \defn{registers}, respectively. Let  and . Define the set of \defn{load} and \defn{store} labels, respectively:
\label{def:alphabet}

Let  be the \defn{memory access alphabet}. Given ,  and , we write  for the label  in ; similarly,  is shorthand for the label  in .

Let  be a partial string and  be an event in . Then  is called a \defn{load} or \defn{store} if its label, , is in  or , respectively. A load or store event  is a \defn{non-synchronizing memory access} if ; otherwise, it is a \defn{synchronizing memory access}. Let  be a memory location. An \defn{acquire on } is an event  such that  for some . Similarly, a \defn{release on } is an event  labelled by  for some . A \defn{release} and \defn{acquire} is a release and acquire on some memory location, respectively.
\end{definition}

\begin{SCfigure}[100][t]
\begin{tabular}{@{}l@{\hspace{2mm}} || @{\hspace{2mm}}l@{}}
\multicolumn{1}{c}{Thread }    & \multicolumn{1}{c}{Thread }       \\
\midrule
  \,\texttt{:=}\, & \,\texttt{:=}\,\texttt{1}    \\
  \,\texttt{:=}\,    & \,\texttt{:=}\,\texttt{1}
\end{tabular}
\caption{A concurrent system  consisting of two threads. The memory accesses on memory locations  are synchronized, whereas those on  are not.}
\label{fig:intro-example-racy}
\end{SCfigure}

\begin{example}
\label{example:memory-access}
Fig.~\ref{fig:intro-example-racy} shows the syntax of a program that consists of two threads  and . This concurrent system can be directly modelled by the partial string shown in Fig.~\ref{fig:partial-string-example} where memory location  is accessed through acquire and release, whereas memory location  is accessed through non-synchronizing loads and stores (shortly, we shall see that this leads to a data race).
\end{example}

Given Definition~\ref{def:memory-access-alphabet}, we are now ready to refine our earlier conservative over-approximation of the happens-before relations (Definition~\ref{def:program}) to get a particular form of release/acquire semantics. For this, we restrict the downward closure of programs  in , in the sense of Definition~\ref{def:program}, by requiring all partial strings in  to satisfy the following partial ordering constraints:

\begin{definition}[SC-relaxed program]
\label{def:SC-relaxed-program}
A program  is called \defn{SC-relaxed} if, for all  and partial string  in , the set of release events on  is totally ordered by  and, for every acquire  and release  on ,  or .
\end{definition}

Henceforth, we denote loads and stores by  and , respectively. If  and  are release events that modify the same memory location, either  happens-before , or vice versa.  If  is an acquire and  is a release on the same memory location, either  happens-before  or  happens-before . Importantly, however, two acquire events  and  on the same memory location may still happen concurrently in the sense that neither  happens-before  nor  happens-before , in the same way non-synchronizing memory accesses are generally unordered.

\begin{example}
\label{example:memory-access-with-data-race}
Example~\ref{example:program}~and~\ref{example:memory-access} illustrate the SC-relaxed semantics of the concurrent system in Fig.~\ref{fig:intro-example-racy}. In particular, the -shaped partial string in Fig.~\ref{fig:sqsubseteq-partial-string} corresponds to a data race in  because the non-synchronizing memory accesses on memory location  happen concurrently. To see this, it may help to consider the interleaving  where both memory accesses on location  are unordered through the happens-before relation because there is no release instruction separating  from . One way of fixing this data race is by changing thread  to . Since CKA supports non-deterministic choice with the  binary operator (recall Theorem~\ref{theorem:program-algebra}), it would not be difficult to give semantics to such conditional checks, particularly if we introduce `assume' labels into the alphabet in Definition~\ref{def:memory-access-alphabet}.
\end{example}

We ultimately want to show that the conjunction of three existing weak memory axioms as studied in~\cite{AMSS2012} fully characterizes our particular interpretation of relaxed sequential consistency, thereby paving the way for Theorem~\ref{theorem:smaller-fr}. For this, we recall the following memory axioms which can be thought of as relations on loads and stores on the same memory location:

\begin{definition}[Memory axioms]
\label{def:memory-axioms}
Let  be a partial string in . The \defn{read-from} function, denoted by , is defined to map every load to a store on the same memory location. A load  \defn{synchronizes-with} a store  if  implies . \defn{Write-coherence} means that all stores  on the same memory location are totally ordered by . The \defn{from-read axiom} holds whenever, for all loads  and stores  on the same memory location, if  and , then .
\end{definition}

By definition, the read-from function is total on all loads. The synchronizes-with axiom says that if a load reads-from a store (necessarily on the same memory location), then the store happens-before the load. This is also known as the global read-from axiom~\cite{AMSS2012}. Write-coherence, in turn, ensures that all stores on the same memory location are totally ordered. This corresponds to the fact that ``all writes to the same location are serialized in some order and are performed in that order with respect to any processor''~\cite{GLLGGH1990}. Note that this is different from the modification order (`mo') on atomics in C++14~\cite{CPP14} because `mo' is generally not a subset of the happens-before relation. The from-read axiom~\cite{AMSS2012} requires that, for all loads  and two different stores  on the same location, if  reads-from  and  happens-before , then  happens-before .We start by deriving from these three memory axioms the notion of SC-relaxed programs.


\begin{proposition}[SC-relaxed consistency]
\label{proposition:SC-relaxed-consistency}
For all  in , if, for each partial string  in , the synchronizes-with, write-coherence and from-read axioms hold on all release and acquire events in  on the same memory location, then  is an SC-relaxed program.
\end{proposition}
\begin{proof}
Let  be a memory location,  be an acquire on  and  be a release on . By write-coherence on release/acquire events, it remains to show  or . Since the read-from function is total,  for some release  on . By the synchronizes-with axiom, . We therefore assume . By write-coherence,  or . The former implies  by the from-read axiom, whereas the latter implies  by transitivity. This proves, by case analysis, that  is an SC-relaxed program. \qed
\end{proof}

We need to prove some form of converse of the previous implication in order to characterize SC-relaxed semantics in terms of the three aforementioned weak memory axioms. For this purpose, we define the following:

\begin{definition}[Read consistency]
\label{def:read-consistency}
Let  be a memory location and  be a finite partial string in . For all loads  on , define the following set of store events: . The read-from function  is said to satisfy \defn{weak read consistency} whenever, for all loads  and stores  on memory location , the least upper bound  exists, and  implies ; \defn{strong read consistency} implies .
\end{definition}

By the next proposition, a natural sufficient condition for the existence of the least upper bound  is the finiteness of the partial strings in  and the total ordering of all stores on the same memory location from which the load  reads, i.e. write coherence. This could be generalized to well-ordered sets.

\begin{proposition}[Weak read consistency existence]
\label{proposition:weak-read-consistency-existence}
For all partial strings  in , write coherence on memory location  implies that  exists for all loads  on .
\end{proposition}

We remark that  if ; alternatively, to avoid that  is empty, we could require that programs are always constructed such that their partial strings have minimal store events that initialize all memory locations.

\begin{proposition}[Weak read consistency equivalence]
\label{proposition:read-consistency-characterization}
Write coherence implies that weak read consistency is equivalent to the following: for all loads  and stores  on memory location , if  and , then .
\end{proposition}
\begin{proof}
By write coherence,  exists, and  because  by assumption  and Definition~\ref{def:read-consistency}. By assumption of weak read consistency, . From transitivity follows .

Conversely, assume . Let  be a store on  such that . Thus, by hypothesis, . Since  is arbitrary,  is an upper bound. Since the least upper bound is well-defined by write coherence, . \qed
\end{proof}

Weak read consistency therefore says that if a load  reads from a store  and another store  on the same memory location happens before , then  happens before . This implies the next proposition. 

\begin{proposition}[From-read equivalence]
\label{proposition:read-consistency-equivalence}
For all SC-relaxed programs in , weak read consistency with respect to release/acquire events is equivalent to the from-read axiom with respect to release/acquire events.
\end{proposition}


We can characterize strong read consistency as follows:

\begin{proposition}[Strong read consistency equivalence]
\label{proposition:strong-read-consistency-equivalence}
Strong read consistency is equivalent to weak read consistency and the synchronizes-with axiom.
\end{proposition}
\begin{proof}
Let  be a partial string in . Let  be a load and  be a store on the same memory location. The forward implication is immediate from .

Conversely, assume . By synchronizes-with, , whence . By definition of least upper bound, . Since , by hypothesis, and  is antisymmetric, we conclude . \qed
\end{proof}

\begin{theorem}[SC-relaxed equivalence]
\label{theorem:SC-relaxed-equivalence}
For every program  in ,  is SC-relaxed where, for all partial strings  in  and acquire events  in , , if and only if the synchronizes-with, write-coherence and from-read axioms hold for all  in  with respect to all release/acquire events in  on the same memory location.
\end{theorem}
\begin{proof}
Assume  is an SC-relaxed program according to Definition~\ref{def:SC-relaxed-program}. Let  be a partial string in  and  be an acquire in the set of events . By Proposition~\ref{proposition:weak-read-consistency-existence},  exists. Assume . Since  is arbitrary, this is equivalent to assuming strong read consistency. Since release events are totally ordered in , by assumption, it remains to show that the synchronizes-with and from-read axioms hold. This follows from Proposition~\ref{proposition:strong-read-consistency-equivalence}~and~\ref{proposition:read-consistency-equivalence}, respectively.

Conversely, assume the three weak memory axioms hold on  with respect to all release/acquire events in  on the same memory location. By Proposition~\ref{proposition:SC-relaxed-consistency},  is an SC-relaxed program. Therefore, by Proposition~\ref{proposition:read-consistency-equivalence}~and~\ref{proposition:strong-read-consistency-equivalence}, , proving the equivalence. \qed
\end{proof}

While the state-of-the-art weak memory encoding is cubic in size~\cite{AKT2013}, the previous theorem has as immediate consequence that there exists an asymptotically smaller weak memory encoding with only a quadratic number of partial order constraints.

\begin{theorem}[Quadratic-size weak memory encoding]
\label{theorem:smaller-fr}
There exists a quantifier-free first-order logic formula that has a quadratic number of partial order constraints and is equisatisfiable to the cubic-size encoding given in~\cite{AKT2013}.
\end{theorem}

\begin{proof}
Instead of instantiating the three universally quantified events in the from-read axiom, symbolically encode the least upper bound of weak read consistency. This can be accomplished with a new symbolic variable for every acquire event. It is easy to see that this reduces the cubic number of partial order constraints to a quadratic number. \qed
\end{proof}

In short, the asymptotic reduction in the number of partial order constraints is due to a new symbolic encoding for how values are being overwritten in memory: the current cubic-size formula~\cite{AKT2013} encodes the from-read axiom (Definition~\ref{def:memory-axioms}), whereas the proposed quadratic-size formula encodes a certain least upper bound (Definition~\ref{def:read-consistency}). We reemphasize that this formulation is in terms of release/acquire events rather than machine-specific accesses as in~\cite{AKT2013}. The construction of the quadratic-size encoding, therefore, is generally only applicable if we can translate the machine-specific reads and writes in a shared memory program to acquire and release events, respectively. This may require the program to be data race free, as illustrated in Example~\ref{example:memory-access-with-data-race}.

Furthermore, as mentioned in the introduction of this section, the primary application of Theorem~\ref{theorem:smaller-fr} is in the context of BMC. Recall that BMC assumes that all loops in the shared memory program under scrutiny have been unrolled (the same restriction as in~\cite{AKT2013}). This makes it possible to symbolically encode branch conditions, thereby alleviating the need to explicitly enumerate each finite partial string in an elementary program.

\section{Concluding remarks}
\vspace{-0.3em}
\label{section:concl}

This paper has studied a partial order model of computation that satisfies the axioms of a unifying algebraic concurrency semantics by Hoare et al. By further restricting the partial string semantics, we obtained a relaxed sequential consistency semantics which was shown to be equivalent to the conjunction of three weak memory axioms by Alglave et al. This allowed us to prove the existence of an equisatisfiable but asymptotically smaller weak memory encoding that has only a quadratic number of partial order constraints compared to the state-of-the-art cubic-size encoding. In upcoming work, we will experimentally compare both encodings in the context of bounded model checking using SMT solvers. As future theoretical work, it would be interesting to study the relationship between categorical models of partial string theory and event structures.

\paragraph{Acknowledgements.} We would like to thank Tony Hoare and Stephan van Staden for their valuable comments on an early draft of this paper, and we thank Jade Alglave, C\'{e}sar Rodr\'{i}guez, Michael Tautschnig, Peter Schrammel, Marcelo Sousa, Bj\"{o}rn Wachter and John Wickerson for invaluable discussions.

\bibliographystyle{splncs}
\bibliography{paper}

\end{document}
