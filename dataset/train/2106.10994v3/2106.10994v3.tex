\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage{amsmath}
\PassOptionsToPackage{numbers,compress}{natbib}
\usepackage[final]{neurips_2021}
\usepackage{booktabs}

\usepackage{amsthm}
\usepackage{xcolor}
\usepackage{bm}

\usepackage{graphicx}
\usepackage{subfigure}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         

\usepackage{makecell}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{constraint}{Constraint}[section]
\newcommand{\xu}[1]{{\color{red} xu: #1}}
\newcommand{\he}[1]{{\color{red}#1}}




\title{BernNet: Learning Arbitrary Graph Spectral Filters via Bernstein Approximation}


\author{Mingguo He \\
  Renmin University of China\\
  \texttt{mingguo@ruc.edu.cn}
  \And
  Zhewei Wei\footnotemark[1] \\
  Renmin University of China\\
  \texttt{zhewei@ruc.edu.cn}
  \AND
  Zengfeng Huang\\
  Fudan University\\
  \texttt{huangzf@fudan.edu.cn}
  \And
  Hongteng Xu\footnotemark[1]\\
  Renmin University of China\\
  \texttt{hongtengxu@ruc.edu.cn}
}

\begin{document}

\maketitle
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{Zhewei Wei and Hongteng Xu are the corresponding authors. Work partially done at Gaoling School of Artificial Intelligence, Beijing Key Laboratory of Big Data Management and Analysis Methods, MOE Key Lab of Data Engineering and Knowledge Engineering, Renmin University of China, and Pazhou Lab, Guangzhou, 510330, China.} 

\begin{abstract}
Many representative graph neural networks, , GPR-GNN and ChebNet, approximate graph convolutions with graph spectral filters. 
However, existing work either applies predefined filter weights or learns them without necessary constraints, which may lead to oversimplified or ill-posed filters. 
To overcome these issues, we propose , a novel graph neural network with theoretical support that provides a simple but effective scheme for designing and learning arbitrary graph spectral filters.
In particular, for any filter over the normalized Laplacian spectrum of a graph, our BernNet estimates it by an order- Bernstein polynomial approximation and designs its spectral property by setting the coefficients of the Bernstein basis. 
Moreover, we can learn the coefficients (and the corresponding filter weights) based on observed graphs and their associated signals and thus achieve the BernNet specialized for the data. 
Our experiments demonstrate that BernNet can learn arbitrary spectral filters, including complicated band-rejection and comb filters, and it achieves superior performance in real-world graph modeling tasks. Code is available at \url{https://github.com/ivam-he/BernNet}.
\end{abstract}


\section{Introduction}\label{sec_intro}
Graph neural networks (GNNs) have received extensive attention from researchers due to their excellent performance on various graph learning tasks such as social analysis~\cite{qiu2018deepinf,li2019encoding,tong2019leveraging}, drug discovery~\cite{jiang2021could,rathi2019practical}, traffic forecasting~\cite{li2017diffusion,bogaerts2020graph,cui2019traffic}, recommendation system~\cite{ying2018graph,wu2019session} and computer vision~\cite{zhao2019semantic,chen2019multi}. 
Recent studies suggest that many popular GNNs operate as polynomial graph spectral filters~\cite{Chebnet,kipf2016semi,chien2021GPR-GNN,levie2018cayleynets,bianchi2021ARMA,xu2018graphWavelet}. 
Specifically, we denote an undirected graph with node set  and edge set  as , whose adjacency matrix is . 
Given a signal  on the graph, where  is the number of nodes, we can formulate its graph spectral filtering operation as , 's are the filter weights,   is the symmetric normalized Laplacian matrix of , and  is the diagonal degree matrix of . Another equivalent polynomial filtering operation is , where  is the normalized adjacency matrix and 's are the filter weights.



We can broadly categorize the GNNs applying the above filtering operation into two classes, depending on whether they design the filter weights or learn them based on observed graphs. 
Some representative models in these two classes are shown below.

\begin{itemize}
    \item \textbf{The GNNs driven by designing filters:} GCN~\cite{kipf2016semi} uses a simplified first-order Chebyshev polynomial, which is proven to be a low-pass filter~\cite{balcilar2021analyzing,wu2019sgc,xu2020graphheat,zhu2021interpreting}. 
APPNP~\cite{appnp} utilizes Personalized PageRank (PPR) to set the filter weights and achieves a low-pass filter as well~\cite{klicpera2019diffusion,zhu2021interpreting}. 
    GNN-LF/HF~\cite{zhu2021interpreting} designs filter weights from the perspective of graph optimization functions, which can simulate high- and low-pass filters. 
\item \textbf{The GNNs driven by learning filters:}
    ChebNet~\cite{Chebnet} approximates the filtering operation with Chebyshev polynomials, and learns a filter via trainable weights of the Chebyshev basis.  
    GPR-GNN~\cite{chien2021GPR-GNN} learns a polynomial filter by directly performing gradient descent on the filter weights, which can derive high- or low-pass filters. ARMA~\cite{bianchi2021ARMA} learns a rational filter via the family of Auto-Regressive Moving Average filters~\cite{narang2013signal}. 
\end{itemize}




Although the above GNNs achieve some encouraging results in various graph modeling tasks, they still suffer from two major drawbacks. 
Firstly, most existing methods focus on designing or learning simple filters (, low- and/or high-pass filters), while real-world applications often require much more complex filters such as band-rejection and comb filters. 
To the best of our knowledge, none of the existing work supports designing arbitrary interpretable spectral filters. The GNNs driven by learning filters can learn arbitrary filters in theory, but they cannot intuitively show what filters they have learned. In other words, their interpretability is poor. For example, GPR-GNN~\cite{chien2021GPR-GNN} learns the filter weights 's but only proves a small subset of the learnt weight sequences corresponds to low- or high-pass filters.
Secondly, the GNNs often design their filters empirically or learn the filter weights without any necessary constraints. 
As a result, their filter weights often have poor  controllability. 
For example, GNN-LF/HF~\cite{zhu2021interpreting} designs its filters with a complex and non-intuitive polynomial with difficult-to-tune hyperparameters. The multi-layer GCN/SGC~\cite{kipf2016semi,wu2019sgc} leads to ``ill-posed'' filters (, those deriving negative spectral responses).
Additionally, the filters learned by GPR-GNN~\cite{chien2021GPR-GNN} or ChebNet~\cite{Chebnet}  have a chance to be ill-posed as well.




\begin{figure}
    \centering
    \includegraphics[height=4cm]{bernnet_scheme.pdf}
    \vspace{-3mm}
    \caption{An illustration of the proposed BernNet.}
    \label{fig:scheme}
\end{figure}

To overcome the above issues, we propose a novel graph neural network called \textit{BernNet}, which provides an effective algorithmic framework for designing and learning arbitrary graph spectral filters.
As illustrated in Figure~\ref{fig:scheme}, for an arbitrary spectral filter  over the spectrum of the symmetric normalized Laplacian , our BernNet approximates  by a -order Bernstein polynomial approximation, , . 
The non-negative coefficients  of the Bernstein basis  work as the model parameter, which can be interpreted as ,  (, the filter values uniformly sampled from ). 
By designing or learning the 's, we can obtain various spectral filters, whose filtering operation can be formulated as , where  is the graph signal. 
We further demonstrate the rationality of our BernNet from the perspective of graph optimization --- any valid polynomial filers, , those polynomial functions mapping  to , can always be expressed by our BernNet, and accordingly, the filters learned by our BernNet are always valid.
Finally, we conduct experiments to demonstrate that 1) BernNet can learn arbitrary graph spectral filters (e.g., band-rejection, comb, low-band-pass, etc.), and 2) BernNet achieves superior performance on real-world datasets.




 
 






\section{BernNet}
\subsection{Bernstein approximation of spectral filters}
\label{ber:filter}
Given an arbitrary filter function  , let  denote the eigendecomposition of the  symmetric normalized Laplacian matrix , where  is the matrix of eigenvectors and  is the diagonal matrix of eigenvalues. We use 

to denote a spectral filter on graph signal . 
The key of our work is approximate  (or, equivalently, ).
For this purpose, we leverage the Bernstein basis and Bernstein polynomial approximation defined below. 

\begin{definition}[~\cite{farouki2012bernstein}]~\textbf{(Bernstein polynomial approximation)}
\label{def:bernstein}
Given an arbitrary continuous function  on , the Bernstein polynomial approximation (of order ) for  is defined as

Here, for ,  is the -th Bernstein base, and  is the function value at , which works as the coefficient of . 
\end{definition}
 
\begin{lemma}[~\cite{farouki2012bernstein}]
\label{lemma_bern}  Given an arbitrary continuous function  on , let  denote the Bernstein approximation of  as defined in Equation~\eqref{eqn:bernstein_new}. We have  as .
\end{lemma}

For the filter function , we let  and , so that the Bernstein polynomial approximation becomes applicable, where  and  for .
Consequently, we can approximate  by , and Lemma~\ref{lemma_bern} ensures that  as .


Replacing  with , we approximate the spectral filter  in Equation~\eqref{eq:eigen} as  and derive the proposed BernNet. 
In particular, given a graph signal , the convolutional operator of our BernNet is defined as follows: 

where each coefficient  can be either set to  to approximate a predetermined filter , or learnt from the graph structure and signal in an end-to-end fashion. 
As a natural extension of Lemma~\ref{lemma_bern}, our BernNet owns the following proposition.
\begin{proposition}
\label{thm:main}
For an arbitrary continuous filter function , by setting , the  in Equation~\eqref{eqn:bernnet} satisfies  as .
\end{proposition}

\begin{proof}
According to the above derivation, we have
, and Lemma~\ref{lemma_bern} ensures that  as  and .

Consequently, we have

as  and .

\end{proof}



\subsection{Realizing existing filters with BernNet.}
As shown in Proposition~\ref{thm:main}, our BernNet can approximate arbitrary continuous spectral filters with sufficient precision. 
Below we give some representative examples of how our BernNet exactly realizes existing filters that are commonly used in GNNs.
\begin{itemize}
    \item {\bf All-pass filter .} 
    We set  for , and the approximation  is exactly the same with .
    Accordingly, our BernNet becomes an identity matrix, which realizes the all-pass filter perfectly.


\item {\bf Linear low-pass filter .}  
    \
    We set  for  and obtain . 
    The BernNet becomes
    , which achieves the linear low-pass filter exactly. 
    Note that  is also the same as the graph convolutional network (GCN) before renormalization~\cite{kipf2016semi}.


    \item {\bf Linear high-pass filter .} 
    Similarly, we can set  for  to get a perfect approximation , and the BernNet becomes .
\end{itemize}

Note that even for those non-continuous spectral filters, , the impulse low/high/band-pass filters, our BernNet can also provide good approximations (with sufficient large ).
\begin{itemize}
    \item {\bf Impulse low-pass filter .}\footnote[2]{The impulse function  if , otherwise }
    We set  and  for , and .
    Accordingly, the BernNet becomes , deriving an -layer linear low-pass filter.
\item {\bf Impulse high-pass filter .} 
    We set  and  for , and . 
    The BernNet becomes , , an -layer linear high-pass filter.
\item {\bf Impulse band-pass filter .} 
    Similarly, we set  and  for , and .
    The BernNet becomes , which can be explained as stacking a -layer linear low-pass filter and a -layer linear high-pass filter.
    Obviously,  should be an even number in this case.
\end{itemize}




Table~\ref{tab:filters} summarizes the design of the BernNet for the filters above. 
We can find that an appealing advantage of our BernNet is that its coefficients are highly correlated with the spectral property of the target filter. 
In particular, we can determine to pass or reject the spectral signal with  by using a large or small  because each Bernstein base  corresponds to a ``bump'' located at . 
This property provides useful guidance when designing filters, which enhances the interpretability of our BernNet. 

\begin{table}[t]
\caption{Realizing commonly used filters with BernNet.}\label{tab:filters}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}lllll@{}}
\toprule
Filter types
&Filter   
& for  
&Bernstein approximation  
&BernNet
\\ 
\midrule
All-pass          
&1
&  
&1 
&
\\
Linear low-pass   
&  
&  
&   
&
\\
Linear high-pass  
&  
&  
&  
&
\\
Impulse low-pass  
&
& and other   
&   
&
\\
Impulse high-pass 
&
& and other   
&   
&
\\
Impulse band-pass 
& 
& and other   
& 
&
\\ 
\bottomrule
\end{tabular}}
\end{table}


\subsection{Learning complex filters with BernNet}
\label{le:comp_filter}
Besides designing the above typical filters, our BernNet can express more complex filters, such as band-pass, band-rejection, comb, low-band-pass filters, .
Moreover, given the graph signals before and after applying such filters (, the 's and the corresponding 's), our BernNet can learn their approximations in an end-to-end manner. 
Specifically, given the pairs , we learn the coefficients  of the BernNet by gradient descent. 
More implementation details can be found at the experimental section below. 
Figure~\ref{fig:filters_bern} illustrates the four complex filters and the approximations we learned (The low-band pass filter is , where  when , otherwise ).
In general, our BernNet can learn a smoothed approximation of these complex filters, and the approximation precision improves with the increase of the order . 
Note that although the BernNet cannot pinpoint the exact peaks of the comb filter or drop to 0 for the valleys of comb or low-band-pass filters due to the limitation of , it still significantly outperforms other GNNs for learning such complex filters.






\begin{figure}[t]
    \centering
   \subfigure[Band-pass]{
   \includegraphics[width=33mm]{bp.pdf}
   }
   \hspace{-2mm}
   \subfigure[Band-rejection]{
   \includegraphics[width=33mm]{bj.pdf}
   }
   \hspace{-2mm}
   \subfigure[Comb]{
   \includegraphics[width=33mm]{comb.pdf}
   }
   \hspace{-2mm}
   \subfigure[Low-band-pass]{
   \includegraphics[width=33mm]{lb.pdf}
   }
   \vspace{-3mm}
    \caption{Illustrations of four complex filters and their approximations learnt by BernNet.}
    \label{fig:filters_bern}
\end{figure}


\section{BernNet in the Lens of Graph Optimization}
In this section, we motivate BernNet from the perspective of graph optimization. In particular, we show that any polynomial filter that attempts to approximate a valid filter has to take the form of BernNet. 
\subsection{A generalized graph optimization problem}
Given a -dimensional graph signal , we consider a generalized graph optimization problem

where  is a trade-off parameter,  denotes the propagated representation of the input graph signal , and  denotes an energy function of , determining the rate of propagation~\cite{spielman2019spectral}. 
Generally,  operates on the spectral of , and we have .




We can model the polynomial filtering operation of existing GNNs with the optimal solution of Equation~\eqref{energyfunction}. For example, if we set , then the optimization function~\eqref{energyfunction} becomes , a well-known convex graph optimization function proposed by Zhou et al.~\cite{zhouLearing}.   takes the minimum when the derivative ,
which solves to 

By taking a suffix sum , we obtain the polynomial filtering operation for APPNP~\cite{appnp}. Zhu et al.~\cite{zhu2021interpreting} further show that GCN~\cite{kipf2016semi}, DAGNN~\cite{liu2020DAGNN}, and JKNet~\cite{xu2018jknet} can be interpreted by the optimization function~\eqref{energyfunction} with .  





The generalized form of Equation~\eqref{energyfunction} allows us to simulate more complex polynomial filtering operation. For example, let  and , a heat kernel with  as the temperature parameter. Then  takes the minimum when the derivative ,
which solves to 

By taking a suffix sum , we obtain the polynomial filtering operation for the heat kernal based GNN such as GDC~\cite{klicpera2019diffusion} and GraphHeat ~\cite{xu2020graphheat}.

\subsection{Non-negative constraint on polynomial filters}
A natural question is that, does an arbitrary energy function  correspond to a valid or ill-posed spectral filter? Conversely, does any polynomial filtering operation  correspond to the optimal solution of the optimization function~\eqref{energyfunction} for some energy function ? 

As it turns out, there is a ``minimum requirement'' for the energy function ;   has to be {\bf positive semidefinite}. In particular, if  is not positive semidefinite, then the optimization function  is not convex, and  the solution to
 may corresponds to a saddle point. 
Furthermore, without the positive semidefinite constraint on ,  may goes to  as we set  to be a multiple of the eigenvector corresponding to the negative eigenvalue. 

\textbf{Non-negative polynomial filters.}
Given a positive semidefinite energy function , we now consider how the corresponding polynomial filtering operation  should look like. 
Recall that we assume . By the positive semidefinite constraint, we have  for . Since the objective function  is convex, it takes the minimum when . Accordingly, the optimum  can be derived as 

Let   denote the exact spectral filter, and   denote a polynomial approximation  of  (e.g. the suffix sum of 's taylor expansion). Since  when , we have  for . Consequently, it is natural to assume the polynomial filter  also satisfies .
\begin{constraint}
\label{constraint}
Assuming the  energy function  is positive semidefinite, a polynomial filter  approximating the optimal solution to~Equation~\eqref{energyfunction} has to satisfy 

\end{constraint}



While Constraint~\ref{constraint} seems to be simple and intuitive, some of the existing GNN may not satisfies this constraint. For example, GCN uses , which corresponds to a polynomial filter  that takes negative value  when , violating Constraint~\ref{constraint}. As shown in~\cite{wu2019sgc}, the renormalization trick  shrinks the spectral and thus reliefs the problem. However,  may still take negative value as the maximum eigenvalue of  is still larger than . 

\subsection{Non-negative polynomials and Bernstein basis}
Constraint~\ref{constraint} motivates us to design polynomial filters  such that  when  The  part is trivial, as we can always rescale each  by a factor of . The   part, however, requires more elaboration. Note that we can not simply set  for each , since it is shown in~\cite{chien2021GPR-GNN} that such polynomials only correspond to  low-pass filters. 

As it turns out, the Bernstein basis has the following nice property: a polynomial that is non-negative on a certain interval can always be expressed as a non-negative linear combination of Bernstein basis. Specifically, we have the following lemma.

\begin{lemma}[\cite{powers2000polynomials}]
\label{lem:non-negative}
Assume a polynomial  satisfies  for . 
Then there exists a sequence of non-negative coefficients , , such that 

\end{lemma}
Lemma~\ref{lem:non-negative} suggests that to approximate a valid filter, the polynomial filter  has to be a non-negative linear combination of Bernstein basis. 
Specifically, by setting , the filter  that satisfies  for  can be expressed as 

Consequently, any valid polynomial filter that approximate the optimal solution of~\eqref{energyfunction} with positive semidefinite energy function  has to take the following form: .
This observation motivates our BernNet from the perspective of graph optimization --- any valid polynomial filers, , the , can always be expressed by BernNet, and accordingly, the filters learned by our BernNet are always valid.





\section{Related Work}
Graph neural networks (GNNs) can be broadly divided into spectral-based GNNs and spatial-based GNNs~\cite{wu2020comprehensive}. 

Spectral-based GNNs design spectral graph filters in the spectral domain. 
ChebNet~\cite{Chebnet} uses Chebyshev polynomial to approximate a filter. 
GCN~\cite{kipf2016semi} simplifies the Chebyshev filter with the first-order approximation. 
GraphHeat~\cite{xu2020graphheat} uses heat kernel to design a graph filter. 
APPNP~\cite{appnp} utilizes Personalized PageRank (PPR) to set the filter weights.  
GPR-GNN~\cite{chien2021GPR-GNN}  learns the polynomial filters via gradient descent on the polynomial coefficients. 
ARMA~\cite{bianchi2021ARMA} learns a rational filter via the family of Auto-Regressive Moving Average filters~\cite{narang2013signal}. 
AdaGNN~\cite{adagnn} learns simple filters across multiple layers with a single parameter for each feature channel at each layer.
As aforementioned, these methods mainly focus on designing low- or high-pass filters or learning filters without any constraints, which may lead to misspecified even ill-posed filters.


On the other hand, spatial-based GNNs directly propagate and aggregate graph information in the spatial domain. 
From this perspective, GCN~\cite{kipf2016semi} can be explained as the aggregation of the one-hop neighbor information on the graph. 
GAT~\cite{gat} uses the attention mechanism to learn aggregation weights. 
Recently, Balcilar et al.~\cite{balcilar2021analyzing} bridge the gap between spectral-based and spatial-based GNNs and unify GNNs in the same framework. 
Their work shows that the GNNs can be interpreted as sophisticated data-driven filters. 
This motivates the design of the proposed BernNet, which can learn arbitrary non-negative spectral filters from real-world graph signals.



\section{Experiments}
\label{sec:exp}
In this section, we conduct experiments to evaluate BernNet's capability to learn arbitrary filters as well as the performance of BernNet on real datasets. All the experiments are conducted on a machine with an NVIDIA TITAN V GPU (12GB memory), Intel Xeon CPU (2.20 GHz), and 512GB of RAM.



\begin{figure}[t]
    \centering
   \subfigure[Original]{
   \includegraphics[width=22mm]{img_10.pdf}
   }
   \hspace{-3mm}
   \subfigure[Low-pass]{
   \includegraphics[width=22mm]{low_10.pdf}
   }
   \hspace{-3mm}
   \subfigure[High-pass]{
   \includegraphics[width=22mm]{high_10.pdf}
   }
   \hspace{-3mm}
   \subfigure[Band-pass]{
   \includegraphics[width=22mm]{band_10.pdf}
   }
   \hspace{-3mm}
   \subfigure[Band-rejection]{
   \includegraphics[width=22mm]{rejection_10.pdf}
   }
   \hspace{-3mm}
   \subfigure[Comb]{
   \includegraphics[width=22mm]{comb_10.pdf}
   }
\vspace{-2mm}
    \caption{A input image and the filtering results.}
    \label{fig:img_filter}
\end{figure}


\begin{table}[t]
\centering
\small
\caption{Average sum of squared error and  score in parentheses.}
\label{image_res}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}lccccc@{}}
\toprule
& Low-pass &High-pass & Band-pass  & Band-rejection &Comb    \\\cmidrule(l){2-6}
  &  & &           & &  \\ \midrule
GCN       &3.4799(.9872) &67.6635(.2364) &25.8755(.1148) &21.0747(.9438) &50.5120(.2977)              \\
GAT       &2.3574(.9905) &21.9618(.7529) &14.4326(.4823) & 12.6384(.9652) &23.1813(.6957)              \\
GPR-GNN   &0.4169(.9984)  &0.0943(.9986)  &3.5121(.8551)  &3.7917(.9905) &4.6549(.9311)               \\
ARMA      &1.8478(.9932)  &1.8632(.9793)  &7.6922(.7098)  &8.2732(.9782) &15.1214(.7975)              \\
ChebNet  &0.8220(.9973)  &0.7867(.9903)   & 2.2722(.9104)   & 2.5296(.9934)   & 4.0735(.9447)                \\
BernNet   &\textbf{0.0314(.9999)}  &\textbf{0.0113(.9999)}  & \textbf{0.0411(.9984)}  & \textbf{0.9313(.9973)}   &\textbf{0.9982(.9868)}           \\
\bottomrule
\end{tabular}}
\end{table}

\subsection{Learning filters from the signal}
We conduct an empirical analysis on 50 real images with the resolution of 100×100 from the Image Processing Toolbox in Matlab. 
We conduct independent experiments on these 50 images and report the average of the evaluation index.
Following the experimental setting in~\cite{balcilar2021analyzing}, we regard each image as a 2D regular 4-neighborhood grid graph. The graph structure translates to an  adjacency matrix while the pixel intensity translates to a -dimensional signal vector. 




For each of the 50 images, we apply 5 different filters (low-pass, high-pass, band-pass, band-rejection and comb) to the spectral domain of its signal. The formula of each filter is shown in Table ~\ref{image_res}. Recall that applying a low-pass filter  to the spectral domain  means applying 
 to the graph signal. 
Figure~\ref{fig:img_filter} shows the one of the input image and the corresponding filtering results.


In this task, we use the original graph signal as the input and the filtering signal to supervise the training process. The goal is to minimize the square error between output and the filtering signal by learning the correct filter. We evaluate BernNet against  five popular GNN models: GCN~\cite{kipf2016semi}, GAT~\cite{gat}, GPR-GNN~\cite{chien2021GPR-GNN}, ARMA~\cite{bianchi2021ARMA} and ChebNet~\cite{Chebnet}. To ensure fairness, we use two convolutional units and a linear output layer for all models. We train all models with approximately 2k trainable parameters and tune the hidden units to ensure they have similar parameters. Following ~\cite{balcilar2021analyzing}, we discard any regularization or dropout and simply force the GNN to learn the input-output relation. For all models, we set the maximum number of epochs to 2000 and stop the training if the loss does not drop for 100 consecutive times and use Adam optimization with a 0.01 learning rate without decay. Models do not use the position information of the picture pixels. We use a mask to cover the edge nodes of the picture, so the problem can be regarded as a simple regression problem. 
For BernNet, we use a two-layer model, with each layer sharing the same set of  for  and set . For GPR-GNN, we use the officially released code (see the supplementary materials for URL and commit numbers) and set the order of polynomial filter . Other baseline models are based on Pytorch Geometric implementation~\cite{fey2019fast}. The more detailed experiments setting can be found in the Appendix. 


Table ~\ref{image_res} shows the average of the sum of squared error (lower the better) and the  scores (higher the better).
We first observe that GCN and GAT can only handle low-pass filters, which concurs with the theoretical analysis in~\cite{balcilar2021analyzing}. GPR-GNN, ARMA and ChebNet can learn different filters from the signals. However, BernNet consistently outperformed these models by a large margin on all tasks in terms of both metrics. We attribute this quality to BernNet's ability to tune the coefficients 's, which directly correspond to the uniformly sampled filter values.












\subsection{Node classification on real-world datasets}
\label{5.2}
We now evaluate the performance of BernNet  against the competitors on real-world datasets. Following~\cite{chien2021GPR-GNN}, we include three citation graph Cora, CiteSeer and PubMed~\cite{sen2008collective,yang2016revisiting}, and the Amazon co-purchase graph Computers and Photo~\cite{mcauley2015image}. As shown in~\cite{chien2021GPR-GNN} these 5 datasets are homophilic graphs on which the connected nodes tend to share the same label. We also include the Wikipedia graph Chameleon and Squirrel~\cite{musae}, the Actor co-occurrence graph, and webpage graphs Texas and Cornell from WebKB\footnote[3]{\url{http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-11/www/wwkb/}}~\cite{pei2020geom}. These 5 datasets are heterophilic datasets on which connected nodes tend to have different labels.
We summarize the statistics of these datasets in Table~\ref{datasets}.



\begin{table}[t]
\centering
\caption{Dataset statistics.}
\label{datasets}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}lcccccccccc@{}}
\toprule
         & Cora & CiteSeer & PubMed & Computers & Photo  & Chameleon & Squirrel & Actor & Texas & Cornell \\ \midrule
Nodes    & 2708 & 3327     & 19717  & 13752     & 7650   & 2277      & 5201     & 7600  & 183   & 183     \\
Edges    & 5278 & 4552     & 44324  & 245861    & 119081 & 31371     & 198353   & 26659 & 279   & 277     \\
Features & 1433 & 3703     & 500    & 767       & 745    & 2325      & 2089     & 932   & 1703  & 1703    \\
Classes  & 7    & 6        & 5      & 10        & 8      & 5         & 5        & 5     & 5     & 5       \\ \bottomrule
\end{tabular}}
\end{table}

Following~\cite{chien2021GPR-GNN}, we perform full-supervised node classification task with each model, where we randomly split the node set into train/validation/test set with ratio 60\%/20\%/20\%. For fairness, we generate 10 random splits by random seeds and evaluate all models on the same splits, and report the average metric for each model. 



We compare BernNet  with 6 baseline models: MLP, GCN~\cite{kipf2016semi}, GAT~\cite{gat}, APPNP~\cite{appnp}, ChebNet~\cite{Chebnet}, and GPR-GNN~\cite{chien2021GPR-GNN}.  For GPR-GNN, we use the officially released code (see the supplementary materials for URL and commit numbers) and set the order of polynomial filter . For other models, we use the corresponding Pytorch Geometric library implementations~\cite{fey2019fast}. For BernNet, we use the following propagation process:

where  is
a 2-layer MLP with 64 hidden units on the feature matrix . 
Note that this propagation process is almost identical to that of APPNP or GPR-GNN. 
The only difference is that we substitute the Generalized PageRank polynomial with Bernstein polynomial. We set the  and use different learning rate and dropout for the linear layer and the propagation layer. For all models, we optimal leaning rate over  and weight decay . More detailed experimental settings are discussed in Appendix.


We use the micro-F1 score with a 95\% confidence interval as the evaluation metric. 
The relevant results are summarized in Table~\ref{acc_res}. 
Boldface letters indicate the best result for the given confidence interval. 
We observe that BernNet provides the best results on seven out of the ten datasets. 
On the other three datasets, BernNet also achieves competitive results against SOTA methods. 



\begin{figure}[t]
    \centering
    \hspace{-2mm}
   \subfigure[Chameleon]{
   \includegraphics[width=33mm]{Chameleon.pdf}
   }
   \hspace{-2mm}
   \subfigure[Actor]{
   \includegraphics[width=33mm]{Actor.pdf}
   }
   \hspace{-2mm}
   \subfigure[Squirrel]{
   \includegraphics[width=33mm]{Squirrel.pdf}
   }
   \hspace{-2mm}
   \subfigure[Texas]{
   \includegraphics[width=33mm]{Texas.pdf}
   }
    \caption{Filters learnt from real-world datasets by BernNet.}
    \label{fig:filters_realdata}
\end{figure}


\begin{table}[t]
\centering
\small
\caption{Results on real world benchmark datasets: Mean accuracy (\%) ± 95\% confidence interval.}
\label{acc_res}
\resizebox{\textwidth}{!}{
\begin{tabular}{l c c c c c c c}
\toprule
& GCN & GAT & APPNP & MLP & ChebNet & GPR-GNN & BernNet \\ 
\hline
Cora    
&87.14 &88.03  &88.14 &76.96 &86.67 &\textbf{88.57} &88.52\\

CiteSeer 
&79.86 &\textbf{80.52} &80.47 &76.58 &79.11 &80.12 &80.09\\

PubMed   
&86.74 &87.04 &88.12 &85.94 &87.95 &88.46  &\textbf{88.48}\\

Computers 
&83.32 &83.32 &85.32 &82.85 &87.54 &86.85  &\textbf{87.64}\\

Photo 
&88.26 &90.94 &88.51 &84.72  &93.77 &\textbf{93.85} &93.63 \\

Chameleon 
&59.61 &63.13  &51.84 &46.85  &59.28  &67.28 &\textbf{68.29}\\

Actor
 &33.23  &33.93  &39.66 &40.19  &37.61  &39.92 &\textbf{41.79}\\

Squirrel
&46.78 &44.49  &34.71 &31.03 &40.55 &50.15  &\textbf{51.35}\\

Texas
&77.38 &80.82   &90.98   &91.45  &86.22 &92.95 &\textbf{93.12} \\

Cornell
&65.90 &78.21 &91.81  &90.82  &83.93  &91.37 &\textbf{92.13}\\

\bottomrule
\end{tabular}}
\end{table}


More interestingly, this experiment also shows BernNet can learn complex filters from real-world datasets with only the supervision of node labels.  Figure~\ref{fig:filters_realdata} plots some of the filters BernNet learnt in the training process. On Actor, BernNet learns an all-pass-alike filter, which concurs with the fact that MLP outperforms all other baselines on this dataset. On Chameleon and Squirrel, BernNet learns two comb-alike filters. Given that BernNet outperforms all competitors by at least 1\% on these two datasets, it may suggest that comb-alike filters are necessary for Chameleon and Squirrel. Figure~\ref{fig:coe_real_all_1} shows the Coefficients  learnt from real-world datasets by BernNet. When comparing Figures ~\ref{fig:filters_realdata} and ~\ref{fig:coe_real_all_1}, we observe that the curves of filters and curves of coefficients are almost the same. This is because BernNet's coefficients are highly correlated with the spectral property of the target filter, which indicates BernNet Bernnet has strong interpretability.






\begin{figure}[t]
    \centering
   \hspace{-2mm}
   \subfigure[Chameleon]{
   \includegraphics[width=33mm]{Chameleon_w.pdf}
   }
   \hspace{-2mm}
   \subfigure[Actor]{
   \includegraphics[width=33mm]{Actor_w.pdf}
   }
   \hspace{-2mm}
   \subfigure[Squirrel]{
   \includegraphics[width=33mm]{Squirrel_w.pdf}
   }
   \hspace{-2mm}
   \subfigure[Texas]{
   \includegraphics[width=33mm]{Texas_w.pdf}
   }
    \caption{Coefficients  learnt from real-world datasets by BernNet.}
    \label{fig:coe_real_all_1}
\end{figure}
Finally, we present the train time for each method in Table ~\ref{time_complexity}. BernNet is slower than other methods due to its quadratic dependence on the degree . However, compared to the SOTA method GPR-GNN, the margin is generally less than , which is often acceptable in practice. In theory, both ChebNet~\cite{Chebnet} and GPR-GNN~\cite{chien2021GPR-GNN} are linear time complexity related to propagation step , but BernNet is quadratic time complexity related to . Delgado et al.~\cite{delgado2003linear} show that Bernstein approximation can be evaluated in linear time related to  using the corner cutting algorithm. 
However, BernNet can not use this algorithm directly, because we need to multiply signal . How to convert BernNet to linear complexity will be a problem worth studying in the future. 



\begin{table}[t]
\centering
\small
\caption{Average running time per epoch (ms)/average total running time (s).}
\label{time_complexity}
\resizebox{\textwidth}{!}{
\begin{tabular}{l c c c c c c c}
\toprule
& GCN & GAT & APPNP & MLP & ChebNet & GPR-GNN & BernNet \\ 
\hline
Cora &4.59/1.62 &9.56/2.03 &7.16/2.32 &3.06/0.93 &6.25/1.76 &9.94/2.21 &19.71/5.47\\

CiteSeer &4.63/1.95  &9.93/2.21 &7.79/2.77 &2.95/1.09 &8.28/2.56 &11.16/2.37 &22.36/6.32\\


PubMed &5.12/1.87 &16.16/3.41 &8.21/2.63  &2.91/1.61 &18.04/3.03 &10.45/2.81  &22.02/8.19\\

Computers &5.72/2.52  &30.91/7.85 &9.19/3.48 &3.47/1.31 &20.64/9.64 &16.05/4.38 &28.83/8.69\\

Photo &5.08/2.63 &19.97/5.41 &8.69/4.18  &3.67/1.66 &13.25/7.02 &13.96/3.94  &24.69/7.37\\

Chameleon &4.93/0.99 &13.11/2.66 &7.93/1.62 &3.14/0.63 &10.92/2.25 &10.93/2.41 &22.54/4.75 \\

Actor &5.43/1.09  &11.94/2.45 &8.46/1.71 &3.82/0.77 &7.99/1.62 &11.57/2.35 &23.34/5.81\\

Squirrel &5.61/1.13 &22.76/4.91 &8.01/1.61  &3.41/0.69 &38.12/7.78 &9.87/5.56 &25.58/9.23\\

Texas &4.58/0.92  &9.65/1.96 &7.83/1.63 &3.19/0.65 &6.51/1.34  &10.45/2.16 &23.35/4.81\\

Cornell  &4.83/0.97  &9.79/1.99 &8.23/1.68 &3.25/0.66 &5.85/1.22 &9.86/2.05 &22.23/5.26 \\

\bottomrule
\end{tabular}}
\end{table}


\section{Conclusion} 
This paper proposes {\em BernNet}, a graph neural network that provides a simple and intuitive mechanism for designing and learning an arbitrary spectral filter via  Bernstein polynomial approximation. Compared to previous methods, BernNet can approximate complex filters such as band-rejection and comb filters, and can provide better interpretability. Furthermore, the polynomial filters designed and learned by BernNet are always valid. Experiments show that BernNet outperforms SOTA methods in terms of effectiveness on both synthetic and real-world datasets. For future work, an interesting direction is to improve the efficiency of BernNet.

\section*{Broader Impact}
\label{sec:impact}
The proposed BernNet algorithm addresses the challenge of designing and learning arbitrary spectral filters on graphs. We consider this algorithm a general technical and theoretical contribution, without any foreseeable specific impacts. For applications in bioinformatics, computer vision, and natural language processing, applying the BernNet algorithm may improve the performance of existing GNN models. We leave the exploration of other potential impacts to future work.


\section*{Acknowledgments and Disclosure of Funding}
Zhewei Wei was supported in part by National Natural Science Foundation of China (No. 61972401, No. 61932001 and No. 61832017), by Beijing Outstanding Young Scientist Program NO. BJJWZYJH012019100020098, by Alibaba Group through Alibaba Innovative Research Program, and by CCF-Baidu Open the Fund (NO.2021PP15002000). 
Zengfeng Huang was supported by National Natural Science Foundation of China Grant No. 61802069, and by Shanghai Science and Technology Commission Grant No. 17JC1420200.
Hongteng Xu was supported by Tencent AI Lab Rhino-Bird Joint Research Program.
This work is supported by China Unicom Innovation Ecological Cooperation Plan and by Intelligent Social Governance Platform, Major Innovation  Planning Interdisciplinary Platform for the “Double-First Class” Initiative, Renmin University of China. We also wish to acknowledge the support provided and contribution made by Public Policy and Decision-making Research Lab of Renmin University of China.

\newpage
\bibliographystyle{plain}
\bibliography{ref}


\newpage

\appendix


\section{Additional experimental details}
\subsection{Learning filters from the signal (Section 5.1)}
For all models, we use two convolutional layers and a linear output layer that projects the final node representation onto the single output for each node. We train all models with approximately 2k trainable parameters and tune the hidden units to ensure they have similar parameters. We discard any regularization or dropout and simply force the GNN to learn the input-output relation. We stop the training if the loss does not drop for 100 consecutive epochs with a maximum limit of 2000 epochs and use Adam optimization with a 0.01 learning rate without decay. For GPR-GNN, we use the officially released code (URL and commit number in Table~\ref{url_commit}), and other baseline models are based on Pytorch Geometric implementation~\cite{fey2019fast}.

For GCN, we set the hidden units to 32 and set the linear units to 64. For GAT, the first layer has 4 attention heads and each head has 8 hidden; the second layer has 8 attention heads and each head has 8 hidden. And we set the linear units to 64. For ARMA, we set the ARMA layer and ARMA stacks to 1, and set the hidden units to 32 and set the linear units to 64. For ChebNet, we use 3 steps propagation for each layer with 32 hidden units and set the linear units to 64. For GPR-GNN, we set the hidden units to 32 with 10 steps propagation and set the linear units to 64, and use the random initialization for the GPR part. For BernNet, we use same set of  for  in each layer and set . 



\begin{table}[ht]
    \centering
    \caption{URL and commit number of GPR-GNN codes}
    \label{url_commit}
    \begin{tabular}{@{}lcc@{}}
    \toprule
         & URL &Commit  \\ \midrule
         GRP-GNN&\url{https://github.com/jianhao2016/GPRGNN} &2507f10\\ \bottomrule
    \end{tabular}
\end{table}
Regarding the analysis experiment in section~\ref{le:comp_filter}, BenNet's settings are the same as above and the image used is Figure~\ref{fig:img_filter}.



\subsection{Node classification on real-world datasets (Section 5.2)}
In this experiment, we also  use the officially released code (URL and commit number in Table~\ref{url_commit}) for GPR-GNN and Pytorch Geometric implementation~\cite{fey2019fast} for other models. For all models, we use two convolutional layers and use early stopping 200 with a maximum of 1000 epochs for all datasets. We use the Adam optimizer to train the models and optimal leaning rate over  and weight decay . 

\begin{table}[t]
\centering
\caption{ Hyperparameters for BernNet on real-world datasets.}
\label{hy:bern_real}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}lcccccccc@{}}
\toprule
    Datasets&\makecell[c]{Linear layer\\learning rate} &\makecell[c]{Propagation layer\\learning rate}  &\makecell[c]{Hidden\\dimension} &\makecell[c]{Propagation layer\\dropout}
    &\makecell[c]{Linear layer\\dropout} & &\makecell[c]{Weight\\decay}   \\ \midrule
Cora  &0.01 &0.01  &64  &0.0 &0.5 &10  &0.0005\\

CiteSeer &0.01   &0.01  &64  &0.5   &0.5 &10  &0.0005\\

PubMed &0.01   &0.01  &64  &0.0   &0.5  &10  &0.0\\

Computers &0.01   &0.05  &64  &0.6   &0.5  &10  &0.0005\\

Photo &0.01   &0.01  &64  &0.5   &0.5  &10  &0.0005\\

Chameleon &0.05   &0.01  &64  &0.7   &0.5 &10   &0.0\\
Actor &0.05   &0.01  &64  &0.9   &0.5 &10   &0.0\\
Squirrel &0.05   &0.01  &64  &0.6   &0.5  &10  &0.0\\
Texas  &0.05  &0.002  &64  &0.5   &0.5 &10   &0.0005\\
Cornell  &0.05   &0.001  &64  &0.5   &0.5 &10   &0.0005\\
\bottomrule
\end{tabular}}
\end{table}

For GCN, we use 64 hidden units for each GCN convolutional layer. For MLP, we use the 2-layer full connected network with 64 hidden units. For GAT, we make the first layer have 8 attention heads and each heads have 8 hidden units, the second layer have 1 attention head and 64 hidden units. For APPNP, we use 2-layer MLP with 64 hidden units and set the propagation steps  to be 10. We search the optimal  within . For ChebNet, we set the propagation steps  to be 2 and use 32 hidden units for each layer, which the number of equivalent hidden units is 64 for this case. For GPR-GNN, we use 2-layer MLP with 64 hidden units and set the propagation steps  to be 10, and use PPR initialization with  for the GPR weights. For BernNet, we use 2-layer MLP with 64 hidden units and set the order , and we optimize the learning rate for the linear layer and the propagation layer. We fix the dropout rate for the convolutional layer or the linear layer to be 0.5 for all models, and optimize the dropout rate for the propagation layer for GPR-GNN and BernNet. The Table~\ref{hy:bern_real} shows the hyperparameters of BernNet on real-world datasets.




Figure~\ref{fig:filters_real_all} plots the filters learnt from real-world datasets by BernNet. Besides some special filters discussed in section~\ref{5.2}, we find that BernNet learns a low-pass filter on Cora and CiteSeer which concurs the analysis in ~\cite{balcilar2021analyzing}. Figure~\ref{fig:coe_real_all} shows the Coefficients  learnt from real-world datasets by BernNet. We observe that the curves of filters and curves of coefficients are almost the same, this is because BernNet's coefficients are highly correlated with the spectral property of the target filter which indicates BernNet Bernnet has strong interpretability.

\begin{figure}[ht]
    \centering
    \hspace{-2mm}
   \subfigure[Cora]{
   \includegraphics[width=33mm]{Cora.pdf}
   }
   \hspace{-2mm}
   \subfigure[CiteSeer]{
   \includegraphics[width=33mm]{Citeseer.pdf}
   }
   \hspace{-2mm}
   \subfigure[PubMed]{
   \includegraphics[width=33mm]{Pubmed.pdf}
   }
   \hspace{-2mm}
   \subfigure[Computers]{
   \includegraphics[width=33mm]{computers.pdf}
   }
   \hspace{-2mm}
   \subfigure[Photo]{
   \includegraphics[width=33mm]{photo.pdf}
   }
   \hspace{-2mm}
   \subfigure[Chameleon]{
   \includegraphics[width=33mm]{Chameleon.pdf}
   }
   \hspace{-2mm}
   \subfigure[Actor]{
   \includegraphics[width=33mm]{Actor.pdf}
   }
   \hspace{-2mm}
   \subfigure[Squirrel]{
   \includegraphics[width=33mm]{Squirrel.pdf}
   }
   \hspace{-2mm}
   \subfigure[Texas]{
   \includegraphics[width=33mm]{Texas.pdf}
   }
   \hspace{-2mm}
   \subfigure[Cornell]{
   \includegraphics[width=33mm]{cornell.pdf}
   }
    \caption{Filters learnt from real-world datasets by BernNet.}
    \label{fig:filters_real_all}
\end{figure}


\begin{figure}[t]
    \centering
    \hspace{-2mm}
   \subfigure[Cora]{
   \includegraphics[width=33mm]{Cora_w.pdf}
   }
   \hspace{-2mm}
   \subfigure[CiteSeer]{
   \includegraphics[width=33mm]{Citeseer_w.pdf}
   }
   \hspace{-2mm}
   \subfigure[PubMed]{
   \includegraphics[width=33mm]{Pubmed_w.pdf}
   }
   \hspace{-2mm}
   \subfigure[Computers]{
   \includegraphics[width=33mm]{computers_w.pdf}
   }
   \hspace{-2mm}
   \subfigure[Photo]{
   \includegraphics[width=33mm]{photo_w.pdf}
   }
   \hspace{-2mm}
   \subfigure[Chameleon]{
   \includegraphics[width=33mm]{Chameleon_w.pdf}
   }
   \hspace{-2mm}
   \subfigure[Actor]{
   \includegraphics[width=33mm]{Actor_w.pdf}
   }
   \hspace{-2mm}
   \subfigure[Squirrel]{
   \includegraphics[width=33mm]{Squirrel_w.pdf}
   }
   \hspace{-2mm}
   \subfigure[Texas]{
   \includegraphics[width=33mm]{Texas_w.pdf}
   }
   \hspace{-2mm}
   \subfigure[Cornell]{
   \includegraphics[width=33mm]{cornell_w.pdf}
   }
    \caption{Coefficients  learnt from real-world datasets by BernNet.}
    \label{fig:coe_real_all}
\end{figure}

\end{document}
