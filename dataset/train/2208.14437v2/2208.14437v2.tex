
\documentclass{article} \usepackage{iclr2023_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}

\usepackage{dsfont}
\usepackage{graphicx} \usepackage{xcolor}
\usepackage{csquotes}
\usepackage{amsmath,amssymb}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{comment}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{xspace}
\usepackage{array}
\usepackage{bm}
\usepackage{placeins}
\usepackage{soul}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{comment}
\usepackage{ctable}
\usepackage{multirow}
\usepackage{booktabs} 
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsthm}
\usepackage{xspace}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{xspace}
\usepackage{array}
\usepackage[group-separator={,}]{siunitx}
\usepackage{bm}
\usepackage{bbm}
\usepackage{placeins}
\usepackage{soul}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{multibib}
\usepackage{wrapfig}
\usepackage{makecell}
\usepackage{xcolor,colortbl}
\usepackage{dictsym}
\definecolor{Gray}{gray}{0.90}
\captionsetup[figure]{font=small,labelsep=period}
\captionsetup[table]{font=small,labelsep=period}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=magenta,
    citecolor=blue,
}



\def\eg{\emph{e.g.}} \def\Eg{\emph{E.g.}}
\def\ie{\emph{i.e.}} \def\Ie{\emph{I.e.}}
\def\vs{\emph{vs.}}
\def\etc{\emph{etc.}}
\def\etal{\emph{et al.}}


\newcommand{\oldnew}[2]{#2}
\newcommand{\chk}{{\centering\checkmark}}


\title{MapTR: Structured Modeling and Learning for Online Vectorized HD Map Construction}



\author{\quad\quad\quad{Bencheng Liao} \textsuperscript{1, 2, 3}\footnotemark[1] \quad
{Shaoyu Chen} \textsuperscript{2, 3} \footnotemark[1] \quad
 {Xinggang Wang} \textsuperscript{2} \footnotemark[2] \\
\textbf{\quad\quad\quad{Tianheng Cheng} \textsuperscript{2, 3}  \quad
{Qian Zhang} \textsuperscript{3} \quad {Wenyu Liu}\textsuperscript{2} \quad {Chang Huang}}\textsuperscript{3}
\\
\\
\quad\quad\quad \textsuperscript{1} Institute of Artificial Intelligence, Huazhong University of Science \& Technology \\
\quad\quad\quad \textsuperscript{2} School of EIC, Huazhong University of Science \& Technology \\
\quad\quad\quad \textsuperscript{3} Horizon Robotics \\
\texttt{\quad\quad\quad \{bcliao,shaoyuchen,xgwang,thch,liuwy\}@hust.edu.cn} \\
\texttt{\quad\quad\quad \{qian01.zhang, chang.huang\}@horizon.ai}
}





\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
High-definition (HD) map provides abundant and precise environmental information of the driving scene, serving as a fundamental and indispensable component for planning in autonomous driving system.
We present MapTR, a structured end-to-end Transformer for efficient online vectorized HD map construction. We propose a unified permutation-equivalent modeling approach,
\ie, modeling map element as a point set with a group of equivalent permutations, which accurately describes the shape of map element and stabilizes the learning process. We design a hierarchical query embedding scheme to flexibly encode structured map information and perform hierarchical bipartite matching for map element learning. 
MapTR achieves the best performance and efficiency with only camera input among existing vectorized map construction approaches on nuScenes dataset. In particular, MapTR-nano runs at real-time inference speed ($25.1$ FPS) on RTX 3090, $8\times$ faster than the existing state-of-the-art camera-based method while achieving $5.0$ higher mAP. Even compared with the existing state-of-the-art multi-modality method, MapTR-nano achieves $0.7$ higher mAP 
, and MapTR-tiny achieves $13.5$ higher mAP and $3\times$ faster inference speed. Abundant qualitative results show that MapTR maintains stable and robust map construction quality in complex and various driving scenes. MapTR is of great application value in autonomous driving. Code and more demos are available at \url{https://github.com/hustvl/MapTR}.
\end{abstract}


\section{Introduction}
High-definition (HD)  map is the high-precision  map specifically designed for autonomous driving, composed of instance-level vectorized representation of  map elements (pedestrian crossing, lane divider, road boundaries, \etc). HD map contains rich semantic information of road topology and traffic rules, which is vital for the navigation of self-driving vehicle. 

Conventionally HD map is constructed offline with SLAM-based methods~\citep{loam,legoloam,liosam}, incurring complicated pipeline and high maintaining cost. Recently, online HD map construction has attracted ever-increasing interests, which constructs map around ego-vehicle at runtime with vehicle-mounted sensors, getting rid of offline human efforts.


\begin{figure}[t!]
    \begin{center}
    \includegraphics[width=\linewidth]{fig/main_visualizations.pdf}
    \end{center}
    \vspace*{-0.3cm}
    \caption{MapTR maintains stable and robust vectorized HD map construction quality in complex and various driving scenes.}
    \label{fig:visualization}
    \vspace*{-0.8cm}
\end{figure}

Early works~\citep{chen2022persformer,lstr,can2021structured} leverage line-shape priors to perceive open-shape lanes based on the front-view image. They are restricted to single-view perception and can not cope with other map elements with arbitrary shapes.
With the development of bird's eye view (BEV) representation learning,
recent works~\citep{gkt,cvt,fiery,bevformer} predict rasterized map by performing BEV semantic segmentation. 
However, the rasterized map lacks vectorized instance-level information, such as the lane structure,  which is important for the downstream tasks (\eg, motion prediction and planning). 
To construct vectorized HD map, HDMapNet~\citep{hdmapnet} groups pixel-wise segmentation results, which requires complicated and time-consuming post-processing.   VectorMapNet~\citep{vectormapnet} represents each map element as a point sequence. It adopts a cascaded coarse-to-fine framework  and utilizes auto-regressive decoder to predict points sequentially, leading to long inference time.

Current online vectorized HD map construction methods are restricted by the efficiency and 
not applicable in real-time scenarios.
Recently, DETR~\citep{detr} employs a simple and efficient encoder-decoder Transformer architecture and realizes end-to-end object detection.  


It is natural to ask a question: \textit{Can we design a DETR-like paradigm for efficient end-to-end vectorized HD map construction?} We show that the answer is affirmative with our proposed  \textbf{Map} \textbf{TR}ansformer (MapTR).






Different from object detection in which objects can be easily geometrically abstracted as bounding box, vectorized map elements have more dynamic shapes.
To accurately describe map elements,
we propose a novel unified modeling method. 
We model each map element as a point set with a group of equivalent permutations.
The point set determines the position of the map element. 
And the permutation group includes all the possible organization sequences of the point set corresponding to the same geometrical shape, avoiding the ambiguity of shape.

Based on the permutation-equivalent modeling, we design a structured framework which takes as input images of vehicle-mounted cameras and outputs vectorized HD map.
We streamline the online vectorized HD map construction as a parallel regression problem.  
Hierarchical query embeddings are proposed to flexibly encode instance-level and point-level information. All instances and all points of instance are simultaneously predicted with a unified Transformer structure. And   the training pipeline is formulated as a hierarchical set prediction task, where we perform hierarchical bipartite matching to assign instances and points in turn. And we supervise the geometrical shape in both point and edge levels with the proposed point2point loss and edge direction loss.

With all the proposed designs, we present MapTR, an efficient end-to-end online vectorized HD map construction method with unified modeling and  architecture.
MapTR achieves the best performance and efficiency among existing vectorized map construction approaches on nuScenes~\citep{nuscenes} dataset. In particular, MapTR-nano runs at real-time inference speed ($25.1$ FPS) on RTX 3090, $8\times$ faster than the existing state-of-the-art camera-based method while achieving $5.0$ higher mAP.
Even compared with the existing state-of-the-art multi-modality method, MapTR-nano achieves $0.7$ higher mAP and $8\times$ faster inference speed, and MapTR-tiny achieves $13.5$ higher mAP and $3\times$ faster inference speed.
As the visualization shows (Fig.~\ref{fig:visualization}), MapTR maintains stable and robust map construction quality in complex and various driving scenes.

Our contributions can be summarized as follows:
\begin{itemize}
    \item We propose a unified permutation-equivalent modeling approach for map elements, \ie, modeling map element as a point set with a group of equivalent permutations, which accurately describes the shape of map element and stabilizes the learning process.
\item  Based on the novel modeling, we present MapTR, a structured end-to-end framework for efficient online vectorized HD map construction.
    We design a hierarchical query embedding scheme to flexibly encode instance-level and point-level information, perform hierarchical bipartite matching for map element learning,  and supervise the geometrical shape in both point and edge levels with the proposed point2point loss and edge direction loss.
    \item  MapTR is the first real-time and SOTA vectorized HD map construction approach with stable and robust performance in complex and various driving scenes.
\end{itemize}


\section{Related Work}
\paragraph{HD Map Construction.}
Recently, with the development of 2D-to-BEV methods~\citep{Ma2022VisionCentricBP},
HD map construction is formulated as a segmentation problem based on  surround-view image data captured by vehicle-mounted cameras. \cite{gkt,cvt,fiery,bevformer,lss,polarbev} generate rasterized map by performing BEV semantic segmentation.
To build vectorized HD map, HDMapNet~\citep{hdmapnet}  groups pixel-wise semantic segmentation results with heuristic and time-consuming post-processing to generate instances. 
VectorMapNet~\citep{vectormapnet} serves as the first end-to-end framework, which adopts a two-stage coarse-to-fine framework and utilizes auto-regressive decoder to predict points sequentially, leading to long inference time and the ambiguity about permutation.
Different from VectorMapNet, MapTR introduces novel and unified modeling for map element, solving the ambiguity and stabilizing the learning process. And MapTR builds a structured and parallel one-stage framework with much higher efficiency. 




\paragraph{Lane Detection.}
Lane detection can be viewed as a sub task of HD map construction, which focuses on detecting lane elements in the road scenes. Since most datasets of lane detection only provide single view annotations and focus on open-shape elements, related methods are restricted to single view.
LaneATT~\citep{tabelini2021keep} utilizes anchor-based deep lane detection model to achieve good trade-off between accuracy and efficiency. LSTR~\citep{lstr} adopts the Transformer architecture to directly output parameters of a lane shape model. GANet~\citep{wang2022keypoint} formulates lane detection as a keypoint estimation and association problem and takes a bottom-up design.  \cite{feng2022rethinking} proposes parametric Bezier curve-based method for lane detection. Instead of detecting lane in the 2D image coordinate, \cite{garnett20193d} proposes 3D-LaneNet which performs 3D lane detection in BEV. STSU~\citep{can2021structured} represents lanes as a directed graph in BEV coordinates and adopts curve-based Bezier method to predict lanes from monocular camera image. Persformer~\citep{chen2022persformer} provides better BEV feature representation and optimizes anchor design to unify 2D and 3D lane detection simultaneously.
Instead of only detecting lanes in the limited single view,  MapTR can perceive various kinds of map elements of $360^\circ$ horizontal FOV, with a unified modeling and learning framework.

\paragraph{Contour-based Instance Segmentation.}
Another line of work related to MapTR is contour-based 2D instance segmentation~\citep{Zhu_2022_CVPR, polarmask, xu2019explicit,liu2021dance}. These methods  reformulate 2D instance segmentation as object contour prediction task, and estimate the image coordinates of the contour vertices. CurveGCN~\citep{ling2019fast} utilizes Graph Convolution Networks to predict polygonal boundaries. 
\cite{Lazarow_2022_CVPR,PolyTransform,building_seg,peng2020deep} rely on intermediate representations and adopt a two-stage paradigm, \ie, the first stage performs segmentation / detection to generate vertices and the second stage converts vertices to polygons.
These works model contours of 2D instance masks as polygons. Their modeling methods cannot cope with line-shape map elements and are not applicable for map construction. Differently, MapTR is tailored for HD map construction and models various kinds of map elements in a unified manner. Besides, MapTR does not rely on intermediate representations and has an efficient and compact pipeline.


\section{MapTR}
\subsection{Permutation-equivalent Modeling \label{sec:modeling}}


\begin{figure}[]
    \begin{center}
    \includegraphics[width=\linewidth]{fig/example.pdf}
    \end{center}
    \vspace*{-0.7cm}
    \hspace*{1.5cm} { (a) Polyline} \hspace*{5.0cm} { (b) Polygon} 
    \vspace*{-0.2cm}
    \caption{Typical cases for illustrating the ambiguity of map element about start point and direction.
    (a) Polyline: for the lane divider between two opposite lanes, defining its direction is difficult. Both endpoints of the lane divider can be regarded as the start point and the point set can be organized in two directions.
    (b) Polygon: for the pedestrian crossing,
    each point of the polygon can be regarded as the start point, and the polygon can be connected in two opposite directions (counter-clockwise and clockwise).}
    \label{fig:ambiguity-case}
    \vspace*{-0.5cm}
\end{figure}




MapTR aims at modeling and learning the HD map in a unified manner.
HD map is a collection of vectorized static map elements, including pedestrian crossing, lane divider, road boundarie, \etc{}
For structured modeling,  MapTR geometrically abstracts map elements as closed shape (like pedestrian crossing) and open shape (like lane divider). 
Through sampling points sequentially along the shape boundary, closed-shape element is discretized into polygon while open-shape element is discretized into polyline. 

Preliminarily,  both polygon and polyline can be represented as an ordered point set ${V}^{F}= [v_0, v_1, \dots, v_{N_v-1}]$ (see Fig.~\ref{fig:modeling} (Vanilla)). $N_v$ denotes the number of points. However, the permutation of the point set is not explicitly defined and not unique. There exist many equivalent permutations for polygon and polyline.
For example, as illustrated in Fig.~\ref{fig:ambiguity-case} (a), for the lane divider (polyline) between two opposite lanes, defining its direction is difficult. Both endpoints of the lane divider can be regarded as the start point and the point set can be organized in two directions.
In Fig.~\ref{fig:ambiguity-case} (b), for the pedestrian crossing (polygon), the point set can be organized in two opposite directions (counter-clockwise and clockwise). And circularly changing the permutation of point set has no influence on the geometrical shape of the polygon.
Imposing a fixed permutation to the point set as supervision is not rational. The imposed fixed permutation contradicts with other equivalent permutations, hampering the learning process.




To bridge this gap, MapTR models each map element with $\mathcal{V}=(V,\Gamma)$. 
$V = \{v_{j}\}_{j=0}^{N_v-1}$ denotes the point set  of the map element ($N_v$ is the number of points). 
$\Gamma = \{\gamma^{k}\}$  denotes a group of equivalent permutations of the point set $V$, covering all the possible organization sequences.


\begin{figure}[]
    \begin{center}
    \includegraphics[width=\linewidth]{fig/modeling.pdf}
    \end{center}
    \vspace*{-0.3cm}
    \caption{Illustration of permutation-equivalent modeling of MapTR. Map elements are geometrically abstracted and discretized into polylines and polygons.
    MapTR models each map element  with $(V, \Gamma)$ (a point set $V$ and a group of equivalent permutations $\Gamma$), avoiding the  ambiguity and stabilizing the learning process.}
    \label{fig:modeling}
    \vspace*{-0.5cm}
\end{figure}

Specifically, for polyline element (see Fig.~\ref{fig:modeling} (left)),  $\Gamma$ includes $2$ kinds of equivalent permutations:
\begin{equation}
\begin{aligned}
&\Gamma_{\rm{polyline}} = \{\gamma^{0}, \gamma^{1} \}
\begin{cases}
&\gamma^{0}(j) = j \mod N_v,\\
&\gamma^{1}(j) = (N_v - 1) - j \mod N_v.\\
\end{cases}
\end{aligned}
\end{equation}

For polygon element (see Fig.~\ref{fig:modeling} (right)),  $\Gamma$ includes $2\times N_v$ kinds of equivalent permutations:
\begin{equation}
\begin{aligned}
&\Gamma_{\rm{polygon}} = \{\gamma^{0}, \dots, \gamma^{2 \times N_v-1}\}
\begin{cases}
&\gamma^{0}(j) = j \mod N_v,\\
&\gamma^{1}(j) = (N_v - 1) - j \mod N_v,\\
&\gamma^{2}(j) = (j + 1) \mod N_v,\\
&\gamma^{3}(j) = (N_v - 1) - (j + 1) \mod N_v,\\
&...\\
&\gamma^{2 \times N_v - 2}(j) = (j + N_v - 1) \mod N_v,\\
&\gamma^{2 \times N_v - 1}(j) = (N_v - 1) - (j + N_v - 1) \mod N_v.
\end{cases}
\end{aligned}
\end{equation}




By introducing the conception of equivalent permutations, MapTR models map elements in a unified manner and addresses the ambiguity issue. 
MapTR further introduces hierarchical bipartite matching (see Sec.~\ref{matching} and Sec.~\ref{sec:e2e-training}) for map element learning, and designs a structured encoder-decoder Transformer architecture to efficiently predict map elements (see  Sec.~\ref{sec:architecture}). 

\subsection{Hierarchical Matching\label{matching}}
MapTR parallelly infers a fixed-size set of $N$ map elements in a single pass, following the end-to-end paradigm of DETR~\citep{detr,yolos}. $N$ is set to be larger than the typical number of map elements in a scene. Let's denote the set of $N$ predicted map elements by $\hat{Y} = \{\hat{y}_i \}_{i=0}^{N-1}$. 
The set of ground-truth (GT) map elements is padded with $\varnothing$ (no object) to form a set with size $N$, denoted by $Y=\{y_i\}_{i=0}^{N-1}$.
$y_i=(c_i, V_i, \Gamma_i)$, where $c_i$, $V_i$ and $\Gamma_i$ are respectively the target class label, point set and permutation group of GT map element $y_i$. $\hat{y}_i=(\hat{p}_i, \hat{V}_i)$, where $\hat{p}_i$ and $\hat{V}_i$ are respectively the predicted classification score and predicted point set.
To achieve structured map element modeling and learning,
MapTR introduces hierarchical bipartite matching, \ie,  performing instance-level matching and point-level matching in order.


\paragraph{Instance-level Matching.\label{sec:ins-lvl-matching}}

First, we need to find an optimal instance-level label assignment $\hat{\pi}$ between predicted map elements $\{\hat{y}_i \}$ and GT map elements $\{y_i \}$.  $\hat{\pi}$ is a permutation of $N$ elements ($\hat{\pi} \in \Pi_N$) with the lowest instance-level matching cost:
\begin{equation}
\begin{gathered}
\hat{\pi} = \argmin_{\pi \in \Pi_N} \sum_{i=0}^{N-1} \mathcal{L}_{\rm ins\_match} (\hat{y}_{\pi(i)}, y_i). \label{eq:argmin_inst_loss}
\end{gathered}
\end{equation}

$\mathcal{L}_{\rm ins\_match}(\hat{y}_{\pi(i)}, y_i)$ is a pair-wise matching cost between prediction $\hat{y}_{\pi(i)}$ and GT $y_i$, which  considers both the class label of map element and the position of point set:
\begin{equation}
\begin{gathered}
\mathcal{L}_{\rm ins\_match}(\hat{y}_{{\pi}(i)},  y_i ) =
\mathcal{L}_{\rm{Focal}}(\hat{p}_{{\pi}(i)}, c_i) + \mathcal{L}_{\rm position}(\hat{V}_{\pi(i)}, V_i).
\end{gathered}
\end{equation}
$\mathcal{L}_{\rm{Focal}}(\hat{p}_{{\pi}(i)}, c_i)$ is the class matching cost term, defined as the Focal Loss~\citep{focal} between predicted classification score $\hat{p}_{{\pi}(i)}$ and target class label $c_i$.
$\mathcal{L}_{\rm position}(\hat{V}_{\pi(i)}, V_i)$ is the position matching cost term, which reflects the position correlation between the predicted point set $\hat{V}_{\pi(i)}$ and the GT point set $V_i$ (refer to Sec.~\ref{sec:more_ablation} for more details). 
Hungarian algorithm is utilized to find the optimal instance-level assignment $\hat{\pi}$ following DETR.


\paragraph{Point-level Matching.}
After instance-level matching,  each predicted map element $\hat{y}_{\hat{\pi}(i)}$  is assigned with a GT map element $y_i$.
Then for each predicted instance assigned with positive labels ($c_i\neq\varnothing$), we perform point-level matching
to find an optimal point2point assignment $\hat{\gamma} \in \Gamma$ between predicted point set $\hat{V}_{\hat{\pi}(i)}$ and GT point set $V_{i}$. $\hat{\gamma}$ is selected among the predefined permutation group ${\Gamma}$ and with the lowest point-level matching cost:
\begin{equation}
\begin{gathered}
\hat{\gamma} = \argmin_{\gamma \in {\Gamma}} \sum_{j=0}^{N_v-1} 
D_{\rm{Manhattan}}(\hat{v}_{j}, v_{ \gamma(j)}). \label{eq:argmin_pts_loss}
\end{gathered}
\end{equation}
$D_{\rm{Manhattan}}(\hat{v}_{j}, v_{ \gamma(j)})$ is the Manhattan distance between the $j$-th point of the predicted point set $\hat{V}$ and the $\gamma(j)$-th point of the GT point set $V$. 


\subsection{Training Loss
\label{sec:e2e-training}} 
MapTR is trained based on the optimal instance-level and point-level assignment ($\hat{\pi}$ and $\{\hat{\gamma_i}\}$).
The loss function is composed of three parts, classification loss, point2point loss and edge direction loss:
\begin{equation}
\begin{gathered}
   \mathcal{L} = \lambda \mathcal{L}_{\rm{cls}} + \alpha \mathcal{L}_{\rm{p2p}} + \beta \mathcal{L}_{\rm{dir}},
\end{gathered}    
\end{equation}
where $\lambda$, $\alpha$ and $\beta$ are the weights for balancing different loss terms.


\paragraph{Classification Loss.}
With  the  instance-level  optimal matching result $\hat{\pi}$,
each predicted map element is assigned with a class label \oldnew{(or 'no object' $\varnothing$)}{}. The classification loss is a Focal Loss term formulated as:
\begin{equation}
\begin{gathered}
\mathcal{L}_{\rm{cls}} = \sum_{i=0}^{N-1} \mathcal{L}_{\rm{Focal}}(\hat{p}_{\hat{\pi}(i)}, c_i).
\end{gathered}
\end{equation}



\paragraph{Point2point Loss.}
Point2point loss supervises the position of each predicted point.
For each GT instance with index $i$,
according to  the  point-level optimal matching result $\hat{\gamma}_i$,
each predicted point $\hat{v}_{\hat{\pi}(i),j}$ is assigned with a GT point $v_{i, \hat{\gamma}_i(j)}$. 
The point2point loss is defined  as the Manhattan distance computed between each assigned point pair:

\begin{equation}
\begin{gathered}
    \mathcal{L}_{\rm{p2p}} = \sum_{i=0}^{N-1}   \mathbbm{1}_{\{c_i\neq \varnothing\}}    \sum_{j=0}^{N_v-1} D_{\rm {Manhattan}}(\hat{v}_{\hat{\pi}(i),j},  v_{i, \hat{\gamma}_i(j)}).
\end{gathered}    
\end{equation}

\paragraph{Edge Direction Loss.}
Point2point loss only supervises the node point of polyline and polygon, not considering the edge (the connecting line between adjacent points).
For accurately representing map elements, the direction of the edge is
important.
Thus, we further design edge direction loss to
supervise the geometrical shape in the higher edge level.
Specifically, we consider the  cosine similarity of the paired predicted edge $\boldsymbol{\hat{e}_{\hat{\pi}(i),j}}$ and GT edge $\boldsymbol{e_{i, \hat{\gamma}_i(j)}}$:
  
\begin{equation}
\begin{gathered}
    \mathcal{L}_{\rm{dir}} = - \sum_{i=0}^{N-1}   \mathbbm{1}_{\{c_i\neq \varnothing\}}  
    \sum_{j=0}^{N_v-1}  {\rm cosine\_similarity}(\boldsymbol{\hat{e}_{\hat{\pi}(i),j}}, \boldsymbol{e_{i, \hat{\gamma}_i(j)}}),\\
    \boldsymbol{\hat{e}_{\hat{\pi}(i),j}}=\hat{v}_{\hat{\pi}(i),j}  - \hat{v}_{\hat{\pi}(i),(j+1) \rm{mod} N_v},\\
    \boldsymbol{e_{i, \hat{\gamma}_i(j)}}=v_{i, \hat{\gamma}_i(j)} - v_{i, \hat{\gamma}_i(j+1) \rm{mod} N_v}.
\end{gathered}    
\end{equation}


\begin{figure}[]
    \centering
    \includegraphics[width=\linewidth]{fig/framework.pdf}
    \caption{The overall architecture of MapTR. MapTR adopts an encoder-decoder paradigm. The map encoder transforms sensor input to a unified BEV representation. The map decoder adopts a hierarchical query embedding scheme to explicitly encode  map elements and performs hierarchical matching based on the permutation-equivalent modeling. MapTR is fully end-to-end. The pipeline is highly structured, compact and efficient.}
    \label{fig:framework}
    \vspace*{-0.5cm}
\end{figure}



\subsection{Architecture \label{sec:architecture}}
MapTR designs an encoder-decoder paradigm. The overall architecture is depicted in Fig.~\ref{fig:framework}.

\paragraph{Input Modality.}
MapTR takes surround-view images of vehicle-mounted cameras as input.
MapTR is also compatible with other vehicle-mounted sensors (\eg, LiDAR and RADAR).
Extending MapTR to multi-modality data is straightforward and trivial.
And thanks to the rational permutation-equivalent modeling, even with only camera input, MapTR significantly outperforms other methods with multi-modality input.

\paragraph{Map Encoder.}
The map encoder of MapTR extracts features from images of multiple vehicle-mounted cameras and transforms the features into a unified feature representation, \ie, BEV representation.
Given multi-view images $\mathcal{I}=\{I_1, \ldots, I_K\} $,
we leverage a conventional backbone to generate multi-view feature maps $\mathcal{F}=\{F_1, \ldots, F_K\} $.
Then 2D image features $\mathcal{F}$ are transformed to BEV features  $\mathcal{B} \in \mathbb{R}^{H\times W \times C}$.
By default, we adopt GKT~\citep{gkt} as the basic 2D-to-BEV transformation module, considering its easy-to-deploy property and high efficiency. MapTR is compatible with other transformation methods and maintains stable performance, \eg, CVT~\citep{cvt}, LSS~\citep{lss,liu2022bevfusion,bevdepth,bevdet}, Deformable Attention~\citep{bevformer,deformdetr} and IPM~\citep{ipm}. Ablation studies are presented in Tab.~\ref{tab:2dtobev}.

\paragraph{Map Decoder.}
We propose a hierarchical query embedding scheme to explicitly encode each map element. Specifically, we define a set of instance-level queries $\{ q^{\rm{ins}}_i\}_{i=0}^{N-1}$ and a set of point-level queries $\{q^{\rm{pt}}_j\}_{j=0}^{N_v-1}$ shared by all instances. 
Each map element (with index $i$) corresponds to a set of hierarchical queries $\{q^{\rm{hie}}_{ij}\}_{j=0}^{N_v-1}$.
The hierarchical query of $j$-th point of $i$-th map element is formulated as:
\begin{equation}
\begin{gathered}
    q^{\rm{hie}}_{ij} =  q^{\rm{ins}}_i + q^{\rm{pt}}_j.
\end{gathered}
\end{equation}


The map decoder contains several cascaded decoder layers which  update the hierarchical queries iteratively.
In each decoder layer, we adopt MHSA to make hierarchical queries exchange information with each other (both inter-instance and intra-instance).
We then adopt Deformable Attention~\citep{deformdetr} to make hierarchical queries  interact with BEV features, inspired by BEVFormer~\citep{bevformer}. 
Each query $q^{\rm{hie}}_{ij}$ predicts the 2-dimension normalized BEV coordinate $(x_{ij},y_{ij})$ of the reference point $p_{ij}$. We then sample BEV features around the reference points and update queries.

Map elements are usually with irregular shapes and require long-range context. Each map element corresponds to a set of reference points $\{p_{ij}\}_{j=0}^{N_v-1}$ with flexible and dynamic distribution.
The reference points $\{p_{ij}\}_{j=0}^{N_v-1}$ can adapt to the arbitrary shape of map element and capture informative context for map element learning.


The prediction head of MapTR is simple, consisting of  a classification branch and  a point regression branch.
The classification branch predicts instance class score. The point regression branch predicts the positions of the point sets $\hat{V}$. For each map element, it outputs a $2N_v$-dimension vector, which represents normalized BEV coordinates  of the $N_v$ points.




\section{Experiments}


\paragraph{Dataset and Metric.}
We evaluate  MapTR on the popular nuScenes~\citep{nuscenes} dataset, which contains 1000 scenes of roughly 20s duration each. Key samples are annotated at $2$Hz. Each sample has RGB images from $6$ cameras and covers $360^\circ$ horizontal FOV of the ego-vehicle. 
Following the previous methods ~\citep{hdmapnet,vectormapnet}, three kinds of map elements are chosen for fair evaluation -- pedestrian crossing, lane divider, and road boundary. 
The perception ranges are $[-15.0m, 15.0m]$ for the $X$-axis and $[-30.0m, 30.0m]$ for the $Y$-axis. And we adopt average precision (AP) to evaluate the map construction quality.
Chamfer distance $D_{Chamfer}$ is used to determine whether the prediction and GT are matched or not.
We calculate the $\rm{AP}_{\tau}$ under several $D_{Chamfer}$ thresholds ($\tau \in T, T=\{0.5, 1.0, 1.5\}$), and then average across all thresholds as the final AP metric:

\begin{equation}
\rm{AP} = \frac{1}{|T|} \sum_{\tau \in T} \rm{AP}_{\tau}.
\end{equation}



\paragraph{Implementation Details.}
MapTR is trained with $8$ NVIDIA GeForce RTX 3090 GPUs.
We adopt AdamW \citep{adamw} optimizer and cosine annealing schedule. 
For MapTR-tiny, we adopt ResNet50~\citep{resnet} as the backbone. We train MapTR-tiny with a total batch size of $32$ (containig 6 view images). All ablation studies are based on MapTR-tiny trained with $24$ epochs. MapTR-nano is designed for real-time applications.  We adopt ResNet18 as the backbone. More details are provided in Appendix~\ref{sec:more_details}.



\subsection{Comparisons with State-of-the-Art Methods}
In Tab.~\ref{tab:main-result}, we compare MapTR with state-of-the-art methods. 
MapTR-nano runs at real-time inference speed ($25.1$ FPS) on RTX 3090, $8\times$ faster than the existing state-of-the-art camera-based method (VectorMapNet-C) while achieving $5.0$ higher mAP. 
Even compared with the existing state-of-the-art multi-modality method, MapTR-nano achieves $0.7$ higher mAP and $8\times$ faster inference speed, and MapTR-tiny achieves $13.5$ higher mAP and $3\times$ faster inference speed. MapTR is also a fast converging method, which demonstrate advanced performance with 24-epoch schedule.

\begin{table}[ht]
\begin{center}
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{lccc|cccc|c}
\hline
\rowcolor{Gray}
Method & Modality & Backbone & Epochs & AP$_{\textit{ped}}$ & AP$_{\textit{divider}}$ & AP$_{\textit{boundary}}$ & mAP & FPS \\
\toprule
HDMapNet & C & Effi-B0 & 30 & 14.4 & 21.7 & 33.0 & 23.0 & 0.8 \\
HDMapNet & L & PointPillars & 30 & 10.4 & 24.1 & 37.9 & 24.1& 1.0\\
HDMapNet & C \& L & Effi-B0 \& PointPillars & 30 & 16.3 & 29.6 & 46.7 & 31.0 & 0.5\\
\midrule
VectorMapNet & C & R50& 110 & 36.1 & 47.3 & 39.3 & 40.9 &2.9\\
VectorMapNet & L & PointPillars & 110 & 25.7 & 37.6 & 38.6 & 34.0 &-\\
VectorMapNet & C \& L & R50 \& PointPillars & 110 & 37.6 & 50.5 & 47.5 & 45.2 &-\\
\midrule
MapTR-nano & C & R18 & 110 & 39.6 & 49.9 & 48.2 & 45.9 &  \textbf{25.1}\\
MapTR-tiny & C &R50 & \textbf{24} & 46.3 & 51.5 & 53.1 & 50.3& 11.2\\
MapTR-tiny & C & R50 &110 & \textbf{56.2} & \textbf{59.8}  & \textbf{60.1} &\textbf{58.7} &11.2\\
\bottomrule
\end{tabular}
}
\end{center}
\vspace*{-0.45cm}
\caption{Comparisons with state-of-the-art methods~\citep{vectormapnet,hdmapnet} on nuScenes \texttt{val} set.  ``C'' and ``L'' respectively denotes camera and LiDAR. ``Effi-B0'' and ``PointPillars'' respectively correspond to \cite{efficientnet} and \cite{pointpillars}.
The APs of other methods are taken from the paper of VectorMapNet. The FPS of VectorMapNet-C is provided by its authors and measured on RTX 3090. Other FPSs are measured on the same machine with RTX 3090. ``-'' means that the corresponding results are not available.
Even with only camera input, MapTR-tiny significantly outperforms multi-modality counterparts ($+13.5$ mAP). MapTR-nano achieves SOTA camera-based performance and runs at $25.1$ FPS,  realizing real-time vectorized map construction for the first time.}
\label{tab:main-result}
\vspace*{-0.35cm}
\end{table}



\subsection{Ablation Study}
\label{sec:ablation}

To validate the effectiveness of different designs, we conduct ablation experiments on nuScenes \texttt{val} set. More ablation studies are in Appendix~\ref{sec:more_ablation}.

\paragraph{Effectiveness of Permutation-equivalent Modeling.}
In Tab.~\ref{tab:abla-modeling}, we provide ablation experiments to validate the effectiveness of the proposed permutation-equivalent modeling. Compared with vanilla modeling method which imposes a unique permutation to the point set, permutation-equivalent modeling solves the ambiguity of map element and brings an improvement of $5.9$ mAP.
For pedestrian crossing, the improvement even reaches $11.9$ AP, proving the superiority in modeling polygon elements. We also visualize the learning process in Fig.~\ref{fig:stablization} to show the stabilization of the proposed modeling.

\begin{table}[ht]
\begin{center}
\begin{tabular}{l|ccc|c}
\hline
\rowcolor{Gray}
Modeling method & AP$_{\textit{ped}}$ & AP$_{\textit{divider}}$ & AP$_{\textit{boundary}}$ & mAP\\
\toprule
Fixed-order $V^F$ w/ ambiguity   &34.4&48.1&50.7&44.4 \\
Permutation-equivalent $(V,\Gamma)$ w/o ambiguity &\cellcolor{blue!10}46.3&\cellcolor{blue!10}51.5&\cellcolor{blue!10}53.1&\cellcolor{blue!10}50.3\\
\bottomrule
\end{tabular}
\end{center}
\vspace*{-0.45cm}
\caption{Ablations about modeling method. Vanilla modeling method imposes a unique permutation to the point set, leading to ambiguity. MapTR introduces permutation-equivalent modeling to avoid the ambiguity, which stabilizes the learning process and significantly improves performance ( $+5.9$ mAP).}
\label{tab:abla-modeling}
\vspace*{-0.35cm}
\end{table}


\paragraph{Effectiveness of Edge Direction Loss.}

\begin{wraptable}{r}{0.46\linewidth}
\vspace{-1.0em}
\centering
\scalebox{0.8}{
\begin{tabular}{l|ccc|c}
\hline
\rowcolor{Gray}
$\beta$ & AP$_{\textit{ped}}$ & AP$_{\textit{divider}}$ & AP$_{\textit{boundary}}$ &mAP\\
\toprule
$0$ & 41.4 & 51.3 & 51.9 & 48.2 \\
$3e^{-3}$ & 44.8 & 50.4 & 52.1 & 49.1\\
$5e^{-3}$ &\cellcolor{blue!10}46.3&\cellcolor{blue!10}51.5&\cellcolor{blue!10}53.1&\cellcolor{blue!10}50.3 \\
$1e^{-2}$ &41.9 & 50.9 & 52.0 & 48.3 \\
\bottomrule
\end{tabular}
}
\vspace*{-0.25cm}
\caption{Ablations about the weight $\beta$ of edge direction loss.}
\label{tab:dir-weight}
\vspace{-3em}
\end{wraptable}
Ablations about the weight of edge direction loss are presented in Tab.~\ref{tab:dir-weight}. $\beta = 0$ means that we do not use edge direction loss. $\beta = 5e^{-3}$ corresponds to appropriate supervision and is adopted as the default setting.
\vspace{1em}


\paragraph{2D-to-BEV Transformation.}

\begin{wraptable}{r}{0.46\linewidth}
\vspace{-1.5em}
\centering
\scalebox{0.9}{
\begin{tabular}{l|ccc}
\hline
\rowcolor{Gray}
Method & mAP & FPS & Param. \\
\toprule
IPM        & 46.2 & 11.7 & 35.7M \\
LSS &  49.5& 10.0 & 37.1M\\
Deform. Atten.  & 49.7 & 11.2 &36.0M \\
GKT & \cellcolor{blue!10}50.3 & \cellcolor{blue!10}11.2 &\cellcolor{blue!10}35.9M\\
\bottomrule
\end{tabular}
}
\vspace*{-0.25cm}
\caption{Ablations about 2D-to-BEV transformation methods.  MapTR is compatible with various 2D-to-BEV methods and achieves stable performance.}
\label{tab:2dtobev}
\vspace{-1em}
\end{wraptable}
\vspace{-1em}\vspace{-\parskip}
In Tab.~\ref{tab:2dtobev}, we ablate on the 2D-to-BEV transformation methods  (\eg, IPM~\citep{ipm}, LSS~\citep{liu2022bevfusion,lss}, Deformable Attention~\citep{bevformer} and GKT~\citep{gkt}). We use an optimized implementation of LSS~\citep{liu2022bevfusion}. And for fair comparison with IPM and LSS, GKT and Deformable Attention both adopt one-layer configuration.
Experiments show MapTR is compatible with various 2D-to-BEV methods and achieves stable performance. We adopt GKT as the default configuration of MapTR, considering its easy-to-deploy property and high efficiency.



\vspace*{-0.15cm}
\subsection{Qualitative Visualization}
We show the predicted vectorized HD map results of complex and various driving scenes in Fig.~\ref{fig:visualization}. MapTR maintains stable and impressive results. More qualitative results are provided in Appendix~\ref{sec:more_vis}. We also provide videos (in the supplementary materials) to show the robustness.




\section{Conclusion}
MapTR is a structured end-to-end framework for efficient online vectorized HD map construction, which  adopts a simple encoder-decoder Transformer architecture and hierarchical bipartite matching to perform map element learning based on the proposed permutation-equivalent modeling.
Extensive experiments show that the proposed method can precisely perceive  map elements of arbitrary shape in the challenging nuScenes dataset. We hope MapTR can serve as a basic module of self-driving system and boost the development of downstream tasks (\eg, motion prediction and planning).


\section*{Acknowledgment}
This work was in part supported by NSFC (No. 62276108). We would like to thank Yicheng Liu for his guidance on evaluation and constructive discussions.




\bibliography{iclr2023_conference}
\bibliographystyle{iclr2023_conference}

\newpage

\appendix
{\LARGE  Appendix}

\section{Implementation Details}
\label{sec:more_details}
This section provides more implementation details of the method and experiments.

\paragraph{Data Augmentation.}
The resolution of source images is $1600\times900$. For MapTR-nano, we resize the source images with $0.2$ ratio. For MapTR-tiny, we resize the source images with $0.5$ ratio. Color jitter is used by default.


\paragraph{Model Setting.}
For all experiments, $\lambda$ is set to $2$, $\alpha$ is set to $5$, and $\beta$ is set to $5e^{-3}$ during training. For MapTR-tiny, we set the number of instance-level queries and point-level queries to 50 and 20 respectively.
And  we set the size of each BEV grid to $0.3m$ and stack $6$ transformer decoder layers. We train MapTR-tiny with a total batch size of $32$ (containig 6 view images), a learning rate of $6e^{-4}$, learning rate multiplier of the backbone is 0.1. All ablation studies are based on MapTR-tiny trained with $24$ epochs.
For MapTR-nano, we set the number of instance-level queries and point-level queries to 100 and 20 respectively. And we set the size of each BEV grid to $0.75m$ and stack $2$ transformer decoder layers. We train MapTR-nano with 110 epochs, a total batch size of 192, a learning rate of $4e^{-3}$, learning rate multiplier of the backbone is 0.1. We employ GKT \citep{gkt} as the default 2D-to-BEV module  of MapTR.

\paragraph{Dataset Preprocessing.}
We process the map annotations following \cite{vectormapnet, hdmapnet}. Map elements in the perception ranges of ego-vehicle are extracted as ground-truth map elements. By default, The perception ranges are $[-15.0m, 15.0m]$ for the $X$-axis and $[-30.0m, 30.0m]$ for the $Y$-axis.

\section{Ablation Study}
\label{sec:more_ablation}

\begin{figure}[h!]
    \begin{center}
    \includegraphics[width=\linewidth]{fig/overall_comparison.png}
    \end{center}
    \vspace*{-0.3cm}
    \caption{Convergence curves of permutation modeling methods .}
    \label{fig:stablization}
\end{figure}


\paragraph{Point Number.}
Ablations about the number of points for modeling each map element are presented in Tab.~\ref{tab:point-number}. Too few points can not describe the complex geometrical shape of the map element. Too many points affect the efficiency. We adopt $20$ points as the default setting of MapTR.

\begin{table}[ht]
\begin{center}
\begin{tabular}{c|ccc|cc}
\hline
\rowcolor{Gray}
Pt. num. & AP$_{\textit{ped}}$ & AP$_{\textit{divider}}$ & AP$_{\textit{boundary}}$ &mAP & FPS  \\
\toprule
10        &42.5&51.3&50.1& 48.0  & 12.3 \\
20  &\cellcolor{blue!10}46.3&\cellcolor{blue!10}51.5&\cellcolor{blue!10}53.1&\cellcolor{blue!10}50.3&\cellcolor{blue!10}11.2\\
40 &44.7&52.4&52.9  &50.0 & 10.8\\
\bottomrule
\end{tabular}
\end{center}
\vspace*{-0.45cm}
\caption{Ablations about the number of points for modeling each map element.}
\label{tab:point-number}
\vspace*{-0.35cm}
\end{table}


\paragraph{Element Number.}
Ablations about the number of map elements are presented in Tab.~\ref{tab:element-number}. We adopt $50$ as the default number of map elements for MapTR-tiny.

\begin{table}[ht]
\begin{center}
\begin{tabular}{c|ccc|cc}
\hline
\rowcolor{Gray}
Ele. num. & AP$_{\textit{ped}}$ & AP$_{\textit{divider}}$ & AP$_{\textit{boundary}}$ &mAP & FPS  \\
\toprule
25        &36.3&43.8&44.7& 41.6  & 11.4 \\
50  &\cellcolor{blue!10}46.3&\cellcolor{blue!10}51.5&\cellcolor{blue!10}53.1&\cellcolor{blue!10}50.3&\cellcolor{blue!10}11.2\\
75 &48.2&53.1&55.3  &52.2 & 11.1\\
\bottomrule
\end{tabular}
\end{center}
\vspace*{-0.45cm}
\caption{Ablations about the number of map elements.}
\label{tab:element-number}
\vspace*{-0.35cm}
\end{table}

\paragraph{Decoder Layer Number.}
Ablations about the layer number of map decoder are presented in Tab.~\ref{tab:layer-number}. The map construction performance improves with more layers, but gets saturated when the layer number reaches $6$. 


\begin{table}[ht]
\begin{center}
\begin{tabular}{c|ccc|cc}
\hline
\rowcolor{Gray}
Layer num. & AP$_{\textit{ped}}$ & AP$_{\textit{divider}}$ & AP$_{\textit{boundary}}$ &mAP & FPS  \\
\toprule
1  &20.8 & 30.2 & 36.3& 29.1 & 15.2 \\
2  &36.0 & 43.1 & 48.0 & 42.4 & 14.2\\
3 &38.2 &44.1 & 49.5&  44.0 & 13.5\\
6 &\cellcolor{blue!10}46.3&\cellcolor{blue!10}51.5&\cellcolor{blue!10}53.1&\cellcolor{blue!10}50.3&\cellcolor{blue!10}11.2\\
8 &39.6 & 51.9 & 51.2& 47.6 & 10.6\\
\bottomrule
\end{tabular}
\end{center}
\vspace*{-0.45cm}
\caption{Ablations about the number of decoder. }
\label{tab:layer-number}
\vspace*{-0.35cm}
\end{table}


\paragraph{Position Matching Cost.}
As mentioned in Sec.~\ref{sec:ins-lvl-matching}, we adopt the position matching cost term $\mathcal{L}_{\rm position}(\hat{V}_{\pi(i)}, V_i)$ in instance-level matching, for reflecting the position correlation between the predicted point set $\hat{V}_{\pi(i)}$ and the GT point set $V_i$. 
In Tab.~\ref{tab:position-matching-cost}, we compare two kinds of cost design. \ie, Chamfer distance cost and point2point cost. 
Point2point cost is similar to the point-level matching cost. Specifically, we find the best point2point assignment, and sum the Manhattan distance of all point pairs as the  position matching cost of two point sets.
The experiments show point2point cost is better than Chamfer distance cost.
\begin{table}[ht!]
\begin{center}
\begin{tabular}{l|ccc|c}
\hline
\rowcolor{Gray}
Position matching cost& AP$_{\textit{ped}}$ & AP$_{\textit{divider}}$ & AP$_{\textit{boundary}}$ &mAP\\
\toprule
Chamfer distance cost & 40.3 & 53.8 & 48.5& 47.5 \\
Point2point cost  &\cellcolor{blue!10}46.3&\cellcolor{blue!10}51.5&\cellcolor{blue!10}53.1&\cellcolor{blue!10}50.3\\
\bottomrule
\end{tabular}
\end{center}
\vspace*{-0.45cm}
\caption{Ablations about the position matching cost term.}
\label{tab:position-matching-cost}
\vspace*{-0.35cm}
\end{table}

\paragraph{Swin Transformer Backbones.} Ablations about the Swin Transformer backbones \citep{swin} are presented in Tab.~\ref{tab:swin-backbones}. 

\begin{table}[h!]
\begin{center}
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{ll|cccc|rc}
\hline
\rowcolor{Gray}
Method  & Backbone  & AP$_{\textit{ped}}$ & AP$_{\textit{divider}}$ & AP$_{\textit{boundary}}$ & mAP & FPS & Param. \\
\toprule
MapTR-tiny  &R50        & \cellcolor{blue!10}46.3 & \cellcolor{blue!10}51.5 & \cellcolor{blue!10}53.1 & \cellcolor{blue!10}50.3& \cellcolor{blue!10}11.2 & \cellcolor{blue!10}35.9M\\
MapTR-tiny  & Swin-tiny & 45.2 & 52.7 & 52.3 & 50.1& 9.1 & 39.9M\\
MapTR-small & Swin-small & 50.2 & 55.4 & 57.3 & 54.3 & 7.3 & 61.2M \\
MapTR-base & Swin-base  & 50.6 & 58.7 & 58.4 & 55.9 & 6.1 & 99.2M \\
\bottomrule
\end{tabular}
}
\end{center}
\vspace*{-0.45cm}
\caption{Ablations about Swin Transformer backbones.}
\label{tab:swin-backbones}
\vspace*{-0.35cm}
\end{table}


\paragraph{Modality.} Multi-sensor perception is crucial for the safety of autonomous vehicles, and MapTR is compatible with other vehicle-mounted sensors like LiDAR. As illustrated in Tab.~\ref{tab:modality}, with the schedule of only 24 epochs, multi-modality MapTR significantly outperform previous state-of-the-art result by 17.3 mAP while being $2\times$ faster.

\begin{table}[ht!]
\begin{center}

\begin{tabular}{lcc|cccc|c}
\hline
\rowcolor{Gray}
Method & Modality & Epochs & AP$_{\textit{ped}}$ & AP$_{\textit{divider}}$ & AP$_{\textit{boundary}}$ & mAP & FPS \\
\toprule
HDMapNet & C \& L & 30 & 16.3 & 29.6 & 46.7 & 31.0 & 0.5\\
VectorMapNet & C \& L  & 110 & 37.6 & 50.5 & 47.5 & 45.2 & $<$2.9\\
\midrule
MapTR-tiny & C & 24 & \cellcolor{blue!10}46.3 & \cellcolor{blue!10}51.5 & \cellcolor{blue!10}53.1 & \cellcolor{blue!10}50.3& \cellcolor{blue!10}11.2\\
MapTR-tiny & L  &24 & 48.5 & 53.7  & 64.7 &55.6 &7.2\\
MapTR-tiny & C \& L  &24 & \textbf{55.9} & \textbf{62.3}  & \textbf{69.3} & \textbf{62.5} &5.8\\
\bottomrule
\end{tabular}

\end{center}
\vspace*{-0.45cm}
\caption{Ablations about the modality.}
\label{tab:modality}
\vspace*{-0.35cm}
\end{table}

\paragraph{Robustness to the camera deviation.} In real applications, the camera intrinsics are usually accurate and change little, but the camera extrinsics may be inaccurate due to the shift of camera position, calibration error, \etc~To validate the robustness, we traverse the validation sets and randomly generate noise for each sample. We respectively add translation and rotation deviation of different degrees.
Note that we add noise to all cameras and all coordinates. 
And the noise is subject to normal distribution. There exists extremely large deviation in some samples, which affect the performance a lot. As illustrated in Tab.~\ref{tab:trans} and Tab.~\ref{tab:rot}, when the standard deviation of  $\Delta_x, \Delta_y, \Delta_z$ is  $0.1m$ or the standard deviation of $\theta_x, \theta_y, \theta_z$ is  $0.02rad$, MapTR still keeps comparable performance.

\begin{table}[t!]
\begin{center}
\begin{tabular}{l|cccccc}
\hline
\rowcolor{Gray}
 &  \multicolumn{5}{c}{  $\sigma_1 (m)$} \\
\rowcolor{Gray}
\multirow{-2}{*}{Method} &  0 &  0.05 &  0.1 &  0.5 &  1.0   \\
\toprule
MapTR-tiny & \cellcolor{blue!10}50.3& 49.9&49.0&34.0&17.0\\
\bottomrule
\end{tabular}
\end{center}
\vspace*{-0.45cm}
\caption{Robustness to the translation deviation of camera. The metric is mAP. $\sigma_1$ is the standard deviation of  $\Delta_x, \Delta_y, \Delta_z$.}
\label{tab:trans}
\vspace*{-0.35cm}
\end{table}

\begin{table}[t!]
\begin{center}
\begin{tabular}{l|cccccc}
\hline
\rowcolor{Gray}
 &  \multicolumn{5}{c}{  $\sigma_2 (rad)$} \\
\rowcolor{Gray}
\multirow{-2}{*}{Method} & 0 &  0.005 &  0.01 &  0.02 &  0.05   \\
\toprule
MapTR-tiny & \cellcolor{blue!10}50.3& 49.4& 47.5 & 42.0 & 24.7\\
\bottomrule
\end{tabular}
\end{center}
\vspace*{-0.45cm}
\caption{Robustness to the rotation deviation of camera. The metric is mAP. $\sigma_2$ is the standard deviation of $\theta_x, \theta_y, \theta_z$.}
\label{tab:rot}
\vspace*{-0.35cm}
\end{table}

\paragraph{Detailed running time.} To have a deeper understanding on the efficiency of MapTR, we present the detailed running time of each component in MapTR-tiny with only multi-camera input in Tab.~\ref{tab:runtime}.

\begin{table}[ht!]
\begin{center}
\begin{tabular}{ccc}
\hline
\rowcolor{Gray}
Component & Runtime (ms) & Proportion \\
\toprule
Backbone & 55.5 & 62.1\%\\
2D-to-BEV module (GKT) & 12.3 & 13.8\%\\
Map decoder & 21.5 & 24.1\% \\
\hline
Total& 89.3 & 100 \%\\
\bottomrule
\end{tabular}
\end{center}
\vspace*{-0.45cm}
\caption{Detailed running time for each component in MapTR-tiny on a RTX 3090.}
\label{tab:runtime}
\vspace*{-0.35cm}
\end{table}
\section{Qualitative Visualization}
\label{sec:more_vis}
We visualize map construction results of MapTR under various weather conditions and challenging road environment on nuScenes \texttt{val} set. As shown in Fig.~\ref{fig:sunny-cloudy}, Fig.~\ref{fig:rainy} and Fig.~\ref{fig:night}, MapTR maintains stable and impressive results. Video results are provided in the supplementary materials.

\begin{figure}[t!]
    \begin{center}
    \includegraphics[width=\linewidth]{fig/sunny-cloudy.pdf}
    \end{center}
    \vspace*{-0.3cm}
    \caption{Visualization under sunny and cloudy weather.}
    \label{fig:sunny-cloudy}
\end{figure}

\begin{figure}[t!]
    \begin{center}
    \includegraphics[width=\linewidth]{fig/rainy.pdf}
    \end{center}
    \vspace*{-0.3cm}
    \caption{Visualization under rainy weather.}
    \label{fig:rainy}
\end{figure}

\begin{figure}[t!]
    \begin{center}
    \includegraphics[width=\linewidth]{fig/night.pdf}
    \end{center}
    \vspace*{-0.3cm}
    \caption{Visualization at night.}
    \label{fig:night}
\end{figure}

\end{document}
