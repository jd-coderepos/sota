

\section{Experiments}
\subsection{Datasets}
\noindent{\bf Training Dataset.}
We train our models on the FFHQ dataset~\cite{karras2019style}, which contains 70,000 high-quality (HQ) images, and all images are resized to  for training.
To form training pairs, we synthesize LQ images  from the HQ counterparts  with the following degradation model~\cite{li2020blind, wang2021towards, yang2021gan}: 

where the HQ image  is first convolved with a Gaussian kernel , followed by a downsampling of scale . After that, additive Gaussian noise  is added to the images, and then JPEG compression with quality factor  is applied. Finally, the LQ image is resized back to .
We randomly sample , , , and  from , , , and , respectively.


\noindent{\bf Testing Datasets.}
We evaluate our method on a synthetic dataset CelebA-Test and three real-world datasets: LFW-Test, WebPhoto-Test, and our proposed WIDER-Test.
CelebA-Test contains 3,000 images selected from the CelebA-HQ dataset~\cite{karras2018progressive}, where LQ images are synthesized under the same degradation range as our training settings.
The three real-world datasets respectively contain three different degrees of degradation, \ie, mild (LFW-Test), medium (WebPhoto-Test), and heavy (WIDER-Test).
LFW-Test consists of the first image of each person in LFW dataset~\cite{huang2008labeled}, containing 1,711 images.
WebPhoto-Test~\cite{wang2021towards} consists of 407 low-quality faces collected from the Internet. 
Our WIDER-Test comprises 970 severely degraded face images from the WIDER Face dataset~\cite{yang2016wider}, providing a more challenging dataset to evaluate the generalizability and robustness of blind face restoration methods. 
\subsection{\bf Experimental Settings and Metrics}
\label{sec:settings}
\noindent{\bf Settings.} 
We represent a face image of  as a  code sequence.
For all stages of training, we use the Adam~\cite{kingma2014adam} optimizer with a batch size of 16.
We set the learning rate to  for stages I and II, and adopt a smaller learning rate of  for stage III.
The three stages are trained with 1.5M, 200K, and 20K iterations, respectively.
Our method is implemented with the PyTorch framework and trained using four NVIDIA Tesla V100 GPUs.


\noindent{\bf Metrics.}
For the evaluation on CelebA-Test with ground truth, we adopt PSNR, SSIM, and LPIPS~\cite{zhang2018unreasonable} as metrics.
We also evaluate the identity preservation using the cosine similarity of features from ArcFace network~\cite{deng2019arcface}, denoted as IDS.
For the evaluation on real-world datasets without ground truth, we employ the widely-used non-reference perceptual metrics: FID~\cite{heusel2017gans} and MUSIQ (KonIQ)~\cite{ke2021musiq}.
\subsection{Comparisons with State-of-the-Art Methods}
\begin{figure*}[t]
	\vspace{-1mm}
	\centering
	\includegraphics[width=\textwidth]{Figs/results_celeb.pdf}
	\vspace{-5mm}
	\caption{Qualitative comparison on the CelebA-Test. Even though input faces are severely degraded, our CodeFormer produces high-quality faces with faithful details. (\textbf{Zoom in for details})}
	\label{fig:celeb_compare}
	\vspace{-1mm}
\end{figure*}
\begin{figure*}[t]
 	\vspace{-1mm}
	\centering
	\includegraphics[width=\textwidth]{Figs/results_real.pdf}
	\vspace{-5mm}
	\caption{Qualitative comparison on real-world faces. Our CodeFormer is able to restore high-quality faces, showing robustness to the heavy degradation. (\textbf{Zoom in for details})}
	\label{fig:real_compare}
	\vspace{-3mm}
\end{figure*}
We compare the proposed CodeFormer with state-of-the-art methods, including PULSE~\cite{menon2020pulse}, DFDNet~\cite{li2020blind}, PSFRGAN~\cite{chen2021progressive}, GLEAN~\cite{chan2022glean}, GFP-GAN~\cite{wang2021towards}, and GPEN~\cite{yang2021gan}. We conduct extensive comparisons on both synthetic and real-world datasets. 
\begin{table}[b]
	\centering
	\vspace{-1mm}
	\begin{minipage}[c]{0.465\textwidth}
		\small
		\centering
		\vspace{-3mm}
		\renewcommand{\arraystretch}{1.2}
		\captionof{table}{Quantitative comparison on the \textbf{CelebA-Test}. \red{Red} and \blue{blue} indicate the best and the second best performance, respectively. The result of Code GT is the upper bound of  CodeFormer.}
		\vspace{1mm}
		\label{tab:celeba_blind}
		\tabcolsep=0.1cm
		\scalebox{0.665}{
\begin{tabular}{c|ccc|c|cc}
				\toprule
				Methods       & LPIPS  & FID   &MUSIQ    & IDS   & PSNR  & SSIM  \\
				\midrule 
				Input  & 0.712& 295.73 & 15.16 & 0.32 & 21.53 & \red{0.623}\\
				PULSE~\cite{menon2020pulse}  & 0.406 &72.94 & 67.39 & 0.30& 21.38 & 0.571\\
				DFDNet~\cite{li2020blind} & 0.466 &85.15 & 57.00 & 0.42 & 21.24 & 0.562\\
				PSFRGAN~\cite{chen2021progressive} & 0.395 &62.05 & 65.93 & 0.43 & 20.91 &0.549\\ 
				GLEAN~\cite{chan2022glean} & 0.371 & 59.87  & 61.59 & 0.51 & \blue{21.59} &0.598\\
				GFP-GAN~\cite{wang2021towards}& 0.391 & \red{58.36} & 67.84 & 0.42 &20.37 &0.545\\
				GPEN~\cite{yang2021gan}& \blue{0.349} & \blue{59.70} & \blue{71.53} & \blue{0.54} & 21.26 & 0.565\\
				\textbf{CodeFormer (ours)}  & \red{0.299} & {60.62} & \red{73.79} & \red{0.60} & \red{22.18} & \blue{0.610} \\ 
				\midrule
				Code GT   & 0.124      & 54.31 & 71.94   & 0.89    & 25.43   & 0.749  \\
				GT        & 0      & 51.40 & 72.02  & 1    &     & 1  \\
				\bottomrule
		\end{tabular}}
	\end{minipage}
	\hspace{2mm}
	\begin{minipage}[c]{0.505\textwidth}
		\small
		\centering
		\vspace{-3mm}
		\renewcommand{\arraystretch}{1.2}
		\captionof{table}{Quantitative comparison on the \textit{real-world} \textbf{LFW-Test}, \textbf{WebPhoto-Test}, and \textbf{WIDER-Test}. \red{Red} and \blue{blue} indicate the best and the second best performance, respectively. }
		\vspace{1mm}
		\label{tab:real_test}
		\tabcolsep=0.1cm
		\scalebox{0.69}{
\begin{tabular}{c|cc|cc|cc}
				\toprule
				Dataset       & \multicolumn{2}{c|}{\textbf{LFW-Test}} & \multicolumn{2}{c|}{\textbf{WebPhoto-Test}}  &
				\multicolumn{2}{c}{\textbf{WIDER-Test}} \\ 
				Degradation       & \multicolumn{2}{c|}{mild} & \multicolumn{2}{c|}{medium}  &
				\multicolumn{2}{c}{heavy} \\ 
				Methods       & FID  & MUSIQ  & FID  & MUSIQ   &FID  & MUSIQ   \\ 
				\midrule
				Input     & 137.56 & 25.05 &  170.11  & 19.24 & 202.06 & 15.57\\
				PULSE~\cite{menon2020pulse}&   64.86 & 66.98  &     {86.45}     &    66.57 & 73.59 & {65.36}\\
				DFDNet~\cite{li2020blind} &  62.57  &  {67.95} &      100.68     &     63.81 & 57.84& 59.34 \\
				PSFRGAN~\cite{chen2021progressive} &  \blue{51.89} & 69.21 &    88.45     &    67.09  & 51.16 & 67.27\\ 
				GLEAN~\cite{chan2022glean} & 53.49& 66.48 & 105.63& 61.30 &47.11 & 60.68\\ 
				GFP-GAN~\cite{wang2021towards} & \red{49.96} &  {68.95}   &      {87.35}    &   {68.04}   &\blue{40.59} & {68.26}\\ 
				GPEN~\cite{yang2021gan} & 57.58 & \red{73.59} &\blue{81.77} & \red{73.41} & 46.99 & \red{72.36}\\
				\textbf{CodeFormer (ours)} & {52.02} &  \blue{71.43}    &     \red{78.87}     &  \blue{70.51} &\red{39.06}  &  \blue{69.31}\\ 
				\bottomrule
		\end{tabular}}
	\end{minipage}
\end{table}


\noindent{\bf Evaluation on Synthetic Dataset.}
We first show the quantitative comparison on the \text{CelebA-Test} in Table~\ref{tab:celeba_blind}. In terms of the image quality metrics LPIPS, FID, and MUSIQ, our CodeFormer achieves the best scores than existing methods. Besides, it also faithfully preserves the identity, reflected by the highest IDS score and PSNR. 
Additionally, we present the qualitative comparison in Fig.~\ref{fig:celeb_compare}. The compared methods fail to produce pleasant restoration results, e.g., DFDNet~\cite{li2020blind}, PSFRGAN~\cite{chen2021progressive}, GFP-GAN~\cite{wang2021towards}, and GPEN~\cite{yang2021gan} introduce obvious artifacts and GLEAN~\cite{chan2022glean} produces over-smoothed results that lack facial details. Moreover, all compared methods are unable to preserve the identity.
Thanks to the expressive codebook prior and global modeling, CodeFormer not only produces high-quality faces but also preserves the identity well, even when inputs are highly degraded.


\noindent{\bf Evaluation on Real-world Datasets.}
As presented in Table~\ref{tab:real_test}, our CodeFormer  achieves comparable perceptual quality of FID score with the compared methods on the real-world testing datasets with mild and medium degradation, and the best score on the testing dataset with heavy degradation.  Although PULSE \cite{menon2020pulse} also obtains good perceptual MUSIQ score, it cannot preserve the identity of input images, as the identity score of IDS and visual results respectively suggested in  Table~\ref{tab:celeba_blind} and  Fig.~\ref{fig:real_compare}.
From the visual comparisons in Fig.~\ref{fig:real_compare}, it is observed that our method shows exceptional robustness to the real heavy degradation and produces most visually pleasing results. Notably, CodeFormer successfully preserves the identity and produces natural results with rich details. 


\subsection{Ablation Studies}


\noindent{\bf Effectiveness of Codebook Space.}
We first investigate the effectiveness of the codebook space. As shown in Exp.~(a) of Table~\ref{tab:ablation}, removing the codebook (\ie, directly feeding the encoder features  to the decoder) results in worse LPIPS and IDS scores. The results suggest that the discrete space of codebook is the key to ensure the robustness and effectiveness of our model.


\noindent{\bf Superiority of Transformer-based Code Prediction.}
To verify the superiority of our Transformer-based code prediction for codebook lookup, we compare it with two different solutions, \ie, nearest-neighbour (NN) matching, \ie, Exp.~(b), and a CNN-based code prediction module, \ie, Exp.~(c),
\begin{wrapfigure}{r}{0.448\textwidth}
	\vspace{-9pt}
	\centering
	\includegraphics[width=\linewidth]{Figs/nn_codeformer.pdf}
	\vspace{-16pt}
	\caption{\small{
			Qualitative comparisons of different codebook lookup methods.
	}}\label{fig:nn_transformer}
	\vspace{-15pt}
\end{wrapfigure}
that adopts a Linear layer for prediction following encoder~.
As shown in Table~\ref{tab:ablation}, the comparison of Exps.~(b) and (c) indicates that adopting code prediction for codebook lookup is more effective than NN feature matching.
However, the local nature of convolution operation of CNNs restricts its modeling capability for long code sequence prediction. In comparison to the pure CNN-based method, \ie, Exp.~(c), our Transformer-based solution produces better-fidelity results in terms of LPIPS and IDS scores, as well as higher accuracy of code prediction under all degradation degrees, as shown in Fig.~\ref{fig:code_accuracy}. Besides, the superiority of CodeFormer is also demonstrated in visual comparisons shown in Fig.~\ref{fig:nn_transformer} and Fig.~\ref{fig:results_inpainting}.


\noindent{\bf Importance of Fixed Decoder.}
Unlike the large dictionary (3.2G) in DFDNet~\cite{li2020blind}, which aims to store a massive quantity of facial details, we deliberately adopt a compact codebook  with  and  that only keeps the essential codes for face restoration, which then activate the detailed cues stored in the pre-trained decoder. Thus, the codebook must be used alongside the decoder to fully unleash its potential. To vindicate our design,  we conduct two studies: 1) fixing both codebook and decoder, \ie,~Exp.~(g), and 2) fixing codebook but fine-tuning decoder, \ie,~Exp.~(e). 
Table~\ref{tab:ablation} shows fine-tuning decoder deteriorates the performance, validating our statement. This is because fine-tuning the decoder destroys the learned prior that is held by the pre-trained codebook and decoder, resulting in suboptimal performance. Therefore, we keep the decoder fixed in our method. 
\begin{table}[t]
\begin{minipage}{0.62\linewidth}
		\centering
		\small
		\tabcolsep=0.1cm
		\scalebox{0.745}{
			\begin{tabular}{c|ccc|cc|cc}
				\toprule
				& \multicolumn{3}{c|}{Networks} & \multicolumn{2}{c|}{Code Lookup}  &
				\multicolumn{2}{c}{Metrics} \\ 
				Exp.       & Codebook & Transformer  & Fix Decoder  & NN & Code Pred. & LPIPS  & IDS \\ 
				\midrule
				(a) & & & \checkmark & & & 0.420 & 0.47 \\
				(b) & \checkmark & & \checkmark & \checkmark &  & 0.397 &  0.51 \\
				(c) & \checkmark & & \checkmark & & \checkmark & 0.351 & 0.55   \\
				(e) & \checkmark & \checkmark & & & \checkmark & 0.379 & 0.52\\
				\midrule
				(f) (ours, w=1) & \checkmark & \checkmark & \checkmark & & \checkmark & \textbf{0.297} & \textbf{0.60} \\
				(g) (ours, w=0) & \checkmark & \checkmark & \checkmark & & \checkmark & {0.307} & 0.58 \\
				\bottomrule
		\end{tabular}}
		\vspace{1mm}
		\captionof{table}{\small Ablation studies of variant networks and code lookup methods on the CelebA-Test. Removing `Codebook' means the network is a general encoder-decoder structure. `' is an adjustable coefficient of CFT modules that controls the information flow from encoder. }
		\label{tab:ablation}
\end{minipage}
\hspace{2.5mm}
\begin{minipage}{0.35\linewidth}
		\vspace{-0.6mm}
		\begin{subfigure}[h]{\linewidth}
			\centering
			\includegraphics[width=\textwidth]{Figs/code_acc_curve.pdf}
		\end{subfigure}
		\vspace{0.5mm}
		\captionof{figure}{\small Curve comparison on code sequence prediction accuracy.}
		\label{fig:code_accuracy}
	\end{minipage}
	\vspace{-6mm}
\end{table}


\noindent{\bf Flexibility of Controllable Feature Transformation Module.}
Considering the diverse degradation in real-world LQ face images, we provide a controllable feature transformation module (CFT) to allow a flexible trade-off between quality and fidelity. As shown in Fig.~\ref{fig:control}, a smaller  tends to produce a high-quality result while a larger  improves the fidelity. 
While such a flexibility is rarely explored in previous work, here we show that it is an appealing strategy to improves the adaptiveness of our method for different scenarios.
As shown in Table~\ref{tab:ablation}, Exp.~(f), \ie, setting the coefficient  to  increases the reconstruction and identity scores but decreases the visual quality. 
In this work, we trade between the quality and fidelity, and set the coefficient  to  by default. 
\begin{figure*}[t]
\centering
	\includegraphics[width=\textwidth]{Figs/controllable.pdf}
	\vspace{-4mm}
	\caption{CFT module is capable to generate continuous transitions between image quality and fidelity.}
	\label{fig:control}
	\vspace{-4mm}
\end{figure*}
\subsection{Running time}
We compare the running time of state-of-the-art methods~\cite{menon2020pulse, li2020blind, chen2021progressive, chan2021glean, wang2021towards, yang2021gan} and the proposed CodeFormer.
All existing methods are evaluated on  face images using their publicly available code.
As shown in Table~\ref{tab:time}, the proposed CodeFormer has a similar running time as PSFRGAN~\cite{chen2021progressive} and GPEN~\cite{yang2021gan} that can infer one image within 0.1s. 
Meanwhile, our method achieves the best performance in terms of  LPIPS on the Celeb-Test dataset. 
\begin{table}[ht]
	\vspace{-1mm}
	\captionof{table}{Running time of different networks. All methods are evaluated on  input images using an NVIDIA Tesla V100 GPU. `\underline{\quad}' indicates the running time is less than 0.1s per test image.}
	\footnotesize
	\centering
	\vspace{1mm}
	\smallskip
	\resizebox{\textwidth}{!}{
		\begin{tabular}{c c c c c c c c}		
			\toprule
			& PULSE~\cite{menon2020pulse} & DFDNet~\cite{li2020blind}& PSFRGAN~\cite{chen2021progressive} & GLEAN~\cite{chan2021glean}& GFP-GAN~\cite{wang2021towards} & GPEN~\cite{yang2021gan} & \textbf{CodeFormer (Ours)}\\  
			\midrule
			Time (sec) & 48.955 & 0.179 & \underline{0.065} & 0.132 & 0.126 & \textbf{\underline{0.055}} & \underline{0.070} \\
			LPIPS & 0.406 & 0.466 & 0.395 & 0.371 & 0.391 & 0.349 & \textbf{0.299}  \\
			\bottomrule
		\end{tabular}
	}
	\vspace{-1mm}
	\label{tab:time}
\end{table}
\subsection{Extensions}
\noindent{\bf  Face Color Enhancement.}
We finetune our model on face color enhancement using the same color augmentations (random color jitter and grayscale conversion) as GFP-GAN  (v1)~\cite{wang2021towards}.
We compare our method with GFP-GAN (v1)~\cite{wang2021towards} on the real-world old photos (from CelebChild-Test dataset~\cite{wang2021towards}) with color loss. 
The proposed CodeFormer generates high-quality face images with more natural color and faithful details.
\begin{figure*}[ht]
\centering
	\includegraphics[width=\textwidth]{Figs/results_color_enhancement.pdf}
	\vspace{-4mm}
	\caption{Visual comparison of face color enhancement on the real-world old face photos.}
	\label{fig:results_color_enhancement}
	\vspace{-2mm}
\end{figure*}


\noindent{\bf Face Inpainting.}
The proposed Codeformer can be easily extended to face inpainting, and it shows great performance even in large mask ratios.
To build training pairs, we use a publicly available script~\cite{yang2021gan} to randomly draw irregular polyline masks for generating masked faces. 
We compare our method with two state-of-the-art face inpainting methods CTSDG~\cite{guo2021image} and GPEN~\cite{yang2021gan}, as well as Nearest-Neighbor matching for codebook lookup. 
As shown in Fig.~\ref{fig:results_inpainting}, CTSDG and GPEN struggle in cases with large masks.
Using Nearest-Neighbor matching within our framework roughly reconstructs the face structures, but it also fails in restoring complete visual parts such as the glasses and the eyes.
In contrast, our CodeFormer generates high-quality natural faces without strokes and artifacts.
\begin{figure*}[ht]
\centering
	\includegraphics[width=\textwidth]{Figs/results_inpainting.pdf}
	\vspace{-4mm}
	\caption{Visual comparison with state-of-the-art face inpainting methods on the challenging cases.}
	\label{fig:results_inpainting}
	\vspace{-2mm}
\end{figure*}
\subsection{Limitation}
\label{sec:limitation}
Our method is built on a pre-trained autoencoder with a codebook. Thus, the capability and expressiveness of the autoencoder could affect the performance of our method. 1) Though the identity inconsistency issue is significantly relieved by the Transformer's global modeling, inconsistency still exists in some rare visual parts such as accessories, where the current codebook space cannot seamlessly represent the image space. Using multiple scales in the codebook space to explore more fine-grained visual quantization may be a solution. 2) Although CodeFormer exhibits great robustness in most cases, when it comes to side faces, CodeFormer offers limited superiority to other methods and also cannot produce good results, as failure cases shown in Fig.~\ref{fig:failure_case}. This is expected because there are only few side faces in the FFHQ training dataset, thus, the codebook is unable to learn sufficient codes for this case, leading to less effectiveness in both reconstruction and restoration. 


\begin{figure*}[ht]
\centering
	\includegraphics[width=\textwidth]{Figs/failure_case.pdf}
	\vspace{-4mm}
	\caption{Failure cases tend to occur on side faces, which could be caused by the limited number of side face images in the training dataset of FFHQ.}
	\label{fig:failure_case}
	\vspace{-2mm}
\end{figure*}
%