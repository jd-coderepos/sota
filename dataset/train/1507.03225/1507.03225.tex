\documentclass[prodmode,acmtalg]{acmsmall} 

\usepackage[ruled]{algorithm2e}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}

\acmVolume{0}
\acmNumber{0}
\acmArticle{0}
\acmYear{0}
\acmMonth{0}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{MnSymbol}
\usepackage{ifsym}
\usepackage[utf8]{inputenc}
\usepackage{latexsym}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{microtype}




\newcommand{\Osymbol}{{\mathcal O}}
\newcommand{\BO}[1]{\Osymbol\left(#1\right)}
\newcommand{\TO}[1]{\tilde{\Osymbol}\left(#1\right)}
\newcommand{\SO}[1]{{o}\left(#1\right)}
\newcommand{\BT}[1]{{\Theta}\left(#1\right)}
\newcommand{\BOM}[1]{\Omega\left(#1\right)}
\newcommand{\floor}[1]{\lfloor{#1}\rfloor}
\newcommand{\ceil}[1]{\lceil{#1}\rceil}
\newcommand{\E}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\V}[1]{\text{Var}\left(#1\right)}
\renewcommand{\Pr}[1]{\text{Pr}\left[#1\right]}


\doi{0000001.0000001}

\issn{1234-56789}


\begin{document}



\markboth{R. Pagh}{CoveringLSH: Locality-sensitive Hashing without False Negatives}

\title{CoveringLSH: Locality-sensitive Hashing without False Negatives}
\author{RASMUS PAGH
\affil{IT University of Copenhagen}}

\begin{abstract}
We consider a new construction of locality-sensitive hash functions for Hamming space that is \emph{covering} in the sense that is it guaranteed to produce a collision for every pair of vectors within a given radius . 
The construction is \emph{efficient} in the sense that the expected number of hash collisions between vectors at distance~, for a given , comes close to that of the best possible data independent LSH without the covering guarantee, namely, the seminal LSH construction of Indyk and Motwani (STOC '98).
The efficiency of the new construction essentially \emph{matches} their bound when the search radius is not too large --- e.g., when , where  is the number of points in the data set, and when  where  is an integer constant.
In general, it differs by at most a factor  in the exponent of the time bounds. 
As a consequence, LSH-based similarity search in Hamming space can avoid the problem of false negatives at little or no cost in efficiency.
\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10003752.10003809.10010055.10010060</concept_id>
<concept_desc>Theory of computation~Nearest neighbor algorithms</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10003752.10003809.10010031.10010033</concept_id>
<concept_desc>Theory of computation~Sorting and searching</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Theory of computation~Nearest neighbor algorithms}
\ccsdesc[300]{Theory of computation~Sorting and searching}




\keywords{Similarity search, high-dimensional, locality-sensitive hashing, recall}

\acmformat{Rasmus Pagh, 2016. CoveringLSH: Locality-sensitive Hashing without False Negatives.}

\begin{bottomstuff}
The research leading to these results has received funding from the European Research Council under the European Unionâ€™s 7th Framework Programme (FP7/2007-2013) / ERC grant agreement no.~614331.
\end{bottomstuff}



\maketitle


\section{Introduction}

Similarity search in high dimensions has been a subject of intense research for the last decades in several research communities including theory of computation, databases, machine learning, and information retrieval.
In this paper we consider nearest neighbor search in Hamming space, where the task is to find a vector in a preprocessed set  that has minimum Hamming distance to a query vector .

It is known that efficient data structures for this problem, i.e., whose query and preprocessing time does not increase exponentially with~, would disprove the strong exponential time hypothesis~\cite{williams2005new,DBLP:conf/focs/AlmanW15}.
For this reason the algorithms community has studied the problem of finding a \emph{-approximate} nearest neighbor, i.e., a point whose distance to  is bounded by  times the distance to a nearest neighbor, where  is a user-specified parameter.
If the exact nearest neighbor is sought, the approximation factor  can be seen as a bound on the relative distance between the nearest and the second nearest neighbor.
All existing -approximate nearest neighbor data structures that have been rigorously analyzed have one or more of the following drawbacks:
\begin{enumerate}
	\item Worst case query time linear in the number of points in the data set, or
	\item Worst case query time that grows exponentially with , or
	\item Multiplicative space overhead that grows exponentially with , or
	\item Lack of unconditional guarantee to return a nearest neighbor (or -approximate nearest neighbor).
\end{enumerate}
Arguably, the data structures that come closest to overcoming these drawbacks are based on locality-sensitive hashing (LSH).
For many metrics, including the Hamming metric discussed in this paper, LSH yields sublinear query time (even for ) and space usage that is polynomial in  and linear in the number of dimensions~\cite{Indyk1998,gionis1999similarity}.
If the approximation factor  is larger than a certain constant (currently known to be at most~) the space can even be made , still with sublinear query time~\cite{panigrahy2006entropy,kapralov2015smooth,DBLP:journals/corr/Laarhoven15a}.

However, these methods come with a Monte Carlo-type guarantee:
A -approximate nearest neighbor is returned only \emph{with high probability}, and there is no efficient way of detecting if the computed result is incorrect.
This means that they do not overcome the 4th drawback above.

\medskip

\paragraph{Contribution}
In this paper we investigate the possibility of Las Vegas-type guarantees for (-approximate) nearest neighbor search in Hamming space.
Traditional LSH schemes pick the sequence of hash functions independently, 
which inherently implies that we can only hope for high probability bounds.
Extending and improving results by~\cite{greene1994multi} and~\cite{Arasu_VLDB06} we show that in Hamming space, by suitably correlating hash functions we can ``cover'' all possible positions of~ differences and thus eliminate false negatives, while achieving performance bounds comparable to those of traditional LSH methods.
By known reductions~\cite{DBLP:conf/stoc/Indyk07} this implies Las Vegas-type guarantees also for  and  metrics.
Since our methods are based on combinatorial objects called \emph{coverings} we refer to the approach as \emph{CoveringLSH}.

\medskip

Let  denote the Hamming distance between vectors  and .
Our results imply the following theorem on similarity search (specifically -approximate near neighbor search) in a standard unit cost (word RAM) model:

\begin{theorem}\label{thm:RAM}
Given ,  and , we can construct a data structure such that for  and a value  bounded by
		 		 
	the following holds:
		\begin{itemize}
		\item On query   the data structure is guaranteed to return  with  if there exists  with .
		 \item The expected query time is , where~ is the word length.
		 \item The size of the data structure is  bits.
		\end{itemize}
\end{theorem}

\medskip

Our techniques, like traditional LSH, extend to efficiently solve other variants of similarity search.
For example, we can: 1) handle nearest neighbor search without knowing a bound on the distance to the nearest neighbor, 2) return \emph{all} near neighbors instead of just one, and 3) achieve high probability bounds on query time rather than just an expected time bound. 

When  the performance of our data structure matches that of classical LSH with constant probability of a false negative~\cite{Indyk1998,gionis1999similarity}, so  is the multiplicative overhead compared to classical LSH.
In fact, \cite{o2014optimal} showed that the exponent of  in query time is optimal for methods based on (data independent) LSH.


\subsection{Notation}

For a set  and function  we let .
We use  and  to denote vectors of all 0s and 1s, respectively. 
For  we use  and  to denote bit-wise conjunction and disjunction, respectively, and  to denote the bitwise exclusive-or.
Let .
We use  to denote the Hamming weight of a vector~, and 

to denote the Hamming distance between  and .
For  let  be an upper bound on the time required to produce a representation of the nozero entries of a vector in  in a standard (word RAM) model~\cite{word-RAM}.
Observe that in general  depends on the representation of vectors (e.g., bit vectors for dense vectors, or sparse representations if  is much larger than the largest Hamming weight).
For bit vectors we have  if we assume the ability to count the number of 1s in a word in constant time\footnote{This is true on modern computers using the {\sc popcnt} instruction, and implementable with table lookups if . 
If only a minimal instruction set is available it is possible to get  by a folklore recursive construction, see e.g.~\cite[Lemma 3.2]{hagerup2001deterministic}.}, and this is where the term  in Theorem~\ref{thm:RAM} comes from.
We use ``'' to refer to the integer in  whose difference from  is divisible by .
Finally, let  denote , i.e., the dot product of  and .




\section{Background and related work}

Given  the problem of searching for a vector in  within Hamming distance~ from a given query vector  was introduced by Minsky and Papert as the \emph{approximate dictionary} problem~\cite{minsky1987perceptrons}.
The generalization to arbitrary spaces is now known as the \emph{near neighbor} problem (or sometimes as \emph{point location in balls}).
It is known that a solution to the approximate near neighbor problem for fixed  (known before query time) implies a solution to the nearest neighbor problem with comparable performance~\cite{Indyk1998,HarPeled2012}.
In our case this is somewhat simpler to see, so we give the argument for completeness.
Two reductions are of interest, depending on the size of .
If  is small we can obtain a nearest neighbor data structure by having a data structure for every radius , at a cost of factor  in space and  in query time.
Alternatively, if  is large we can restrict the set of radii to the  radii of the form .
This decreases the approximation factor needed for the near neighbor data structures by a factor , which can be done with no asymptotic cost in the data structures we consider.
For this reason, in the following we focus on the near neighbor problem in Hamming space where  is assumed to be known when the data structure is created.


\subsection{Deterministic algorithms}


For simplicity we will restrict attention to the case .
A baseline is the \emph{brute force} algorithm that looks up all  bit vectors of Hamming distance at most~ from .
The time usage is at least , assuming , so this method is not attractive unless~ is quite small.
The dependence on  was reduced by~\cite{Cole:2004:DMI:1007352.1007374} who achieve query time  and space .
Again, because of the exponential dependence on~ this method is interesting only for small values of .



\subsection{Randomized filtering with false negatives}\label{sec:lsh}

In a seminal paper~\cite{Indyk1998}, Indyk and Motwani presented 
a randomized solution to the \emph{-approximate} near neighbor problem where the search stops as soon as a vector within distance  from  is found.
Their technique can also be used to solve the approximate dictionary problem, but the time will then depend on the number of points at distance between  and  that we inspect.
Their data structure, like all LSH methods for Hamming space we consider in this paper, uses a set of functions from a \emph{Hamming projection} family:

where .
The vectors in  will be referred to as \emph{bit masks}.
Given a query , the idea is to iterate through all functions  and identify collisions  for , e.g.~using a hash table.
This procedure \emph{covers} a query  if at least one collision is produced when there exists  with , and it is \emph{efficient} if the number of hash function evaluations and collisions with  is not too large.
The procedure can be thought of as a randomized \emph{filter} that attempts to catch data items of interest while filtering away data items that are not even close to being interesting.
The \emph{filtering efficiency} with respect to vectors  and  is the expected number of collisions  summed over all functions  , with expectation taken over any randomness in the choice of~.
We can argue that without loss of generality it can be assumed that the filtering efficiency depends only on  and not on the location of the differences.
To see this, using an idea from~\cite{Arasu_VLDB06}, consider replacing each  by a vector  defined by , where  is a random permutation used for all vectors in .
This does not affect distances, and means that collision probabilities will depend solely on , , and the Hamming weights of vectors in .

\medskip

\paragraph{Remark} If vectors in  are sparse it is beneficial to work with a sparse representation of the input and output of functions in , and indeed this is what is done by Indyk and Motwani who consider functions that concatenate a suitable number of 1-bit samples from .
However, we find it convenient to work with -dimensional vectors, with the understanding that a sparse representation can be used if  is large. 

\medskip

\paragraph{Classical Hamming LSH}
Indyk and Motwani use a collection 

where  is a set of uniformly random and independent -dimensional vectors.
Each vector  encodes a sequence of  samples from , and  is the projection vector that selects the sampled bits.
That is,  if and only if  for some . 
By choosing  appropriately we can achieve a trade-off that balances the size of  (i.e., the number of hash functions) with the expected number of collisions at distance~.
It turns out that  suffices to achieve collision probability  at distance~ while keeping the expected total number of collisions with ``far'' vectors (at distance  or more) linear in~.

\medskip

\paragraph{Newer developments}
In a recent advance of~\cite{DBLP:conf/stoc/AndoniR15}, extending preliminary ideas from~\cite{andoni2014beyond}, it was shown how \emph{data dependent} LSH can achieve the same guarantee with a smaller family (having  space usage and evaluation time).
Specifically, it suffices to check collisions of  hash values, where .
We will not attempt to generalize the new method to the data dependent setting, though that is certainly an interesting possible extension.

In a surprising development, it was recently shown~\cite{DBLP:conf/focs/AlmanW15} that even with no approximation of distances () it is possible to obtain truly sublinear time per query if: 1)  \emph{and}, 2) we are concerned with the answers to a \emph{batch} of  queries.


\subsection{Filtering methods without false negatives}\label{sec:nofalseneg}

The literature on filtering methods for Hamming distance that do not introduce false negatives, but still yield formal guarantees, is relatively small. 
As in section~\ref{sec:lsh} the previous results can be stated in the form of Hamming projection families (\ref{eq:hamming-projection}).
We consider constructions of sets  that ensure collision for every pair of vectors at distance at most~, while at the same time achieving nontrivial filtering efficiency for larger distances.

Choosing error probability  in the construction of Indyk and Motwani, we see that there must exist a set  of size  that works for every choice of  mismatching coordinates, i.e., ensures collision under some  for all pairs of vectors within distance . In particular we have . However, this existence argument is of little help to design an algorithm, and hence we will be interested in \emph{explicit} constructions of LSH families without false negatives.\footnote{\cite{DBLP:conf/soda/Indyk00} sketched a way to \emph{verify} that a random family contains a colliding function for every pair of vectors within distance , but unfortunately the construction is incorrect~\cite{IndykPersonalCommunication2015}.}

Kuzjurin has given such explicit constructions of ``covering'' vectors~\cite{kuzjurin2000explicit} but in general the bounds achieved are far from what is possible existentially~\cite{Kuzjurin:1995:DAG:204515.204520}.
Independently,~\cite{greene1994multi} linked the question of similarity search without false negatives to the TurÃ¡n problem in extremal graph theory.
While optimal TurÃ¡n numbers are not known in general, Greene et al.~construct a family  (based on \emph{corrector hypergraphs}) that will incur few collisions with \emph{random vectors}, i.e., vectors at distance about  from the query point.\footnote{It appears that Theorem 3 of~\cite{greene1994multi} does not follow from the calculations of the paper --- a factor of about 4 is missing in the exponent of space and time bounds~\cite{ParnasPersonalCommunication}.}
\cite{gordon1995new} presented near-optimal coverings for certain parameters based on finite geometries --- in section~\ref{sec:small-radius} we will use their construction to achieve good data structures for small~.

\cite{Arasu_VLDB06} give a construction that is able to achieve, for example,  filtering efficiency for approximation factor  with .
Observe that there is no dependence on  in these bounds, which is crucial for high-dimensional (sparse) data.
The technique of~\cite{Arasu_VLDB06} allows a range of trade-offs between  and the filtering efficiency, determined by parameters  and .
No theoretical analysis is made of how close to~ the filtering efficiency can be made for a given , but it seems difficult to significantly improve the constant~7.5 mentioned above.

Independently of the work of~\cite{Arasu_VLDB06}, ``lossless'' methods for near neighbor search have been studied in the contexts of approximate pattern matching~\cite{kucherov2005multiseed} and computer vision~\cite{norouzi2012fast}.
The analytical part of these papers differs from our setting by focusing on filtering efficiency for random vectors, which means that differences between a data vector and the query appear in random locations. 
In particular there is no need to permute the dimensions as described in section~\ref{sec:lsh}.
Such schemes aimed at random (or more generally ``high entropy'') data become efficient when there are few vectors within distance  of a query point.
Another variation of the scheme of~\cite{Arasu_VLDB06} recently appeared in~\cite{DBLP:journals/pvldb/DengLWF15}



\section{Basic construction}\label{sec:basic}

Our basic CoveringLSH construction is a Hamming projection family of the form (\ref{eq:hamming-projection}).
We start by observing the following simple property of Hamming projection families:

\begin{lemma}\label{lem:xor}
For every , every , and all  we have  if and only if .
\end{lemma}
\begin{proof}
Let  be the vector such that .
We have  if and only if . 
Since  the claim follows. 
\end{proof}

Thus, to make sure all pairs of vectors within distance  collide for some function, we need our family to have the property (implicit in the work of~\cite{Arasu_VLDB06}) that every vector with 1s in  bit positions is mapped to zero by some function, i.e., the set of 1s is ``covered'' by zeros in a vector from .

\begin{definition}\label{def:covering}
For , the Hamming projection family  is \emph{-covering} if for every  with , there exists  such that .
The family is said to have \emph{weight}  if  for every .
\end{definition}

A trivial -covering family uses .
We are interested in -covering families that have a nonzero weight chosen to make collisions rare among vectors that are not close.
Vectors in our \emph{basic} -covering family, which aims at weight around 1/2, will be indexed by nonzero vectors in .
The family depends on a function  that maps bit positions to bit vectors of length~. 
(We remark that if  and  is the function that maps an integer to its binary representation, our construction is identical to known coverings based on finite geometry~\cite{gordon1995new}; however we give an elementary presentation that does not require knowledge of finite geometry.)
Define a family of bit vectors  by

where  is the dot product of vectors  and .
We will consider the family of all such vectors with nonzero :

Figure~\ref{fig:hadamard7} shows the family  for  and  equal to the binary representation of~.

\begin{figure}[t]
	\begin{center}
\includegraphics{hadamard7.pdf}
\caption{The collection  corresponding to nonzero vectors of the Hadamard code of message length~3. The resulting Hamming projection family  , see (\ref{eq:hamming-projection}), is -covering since for every pair of columns there exists a row with 0s in these columns. It has weight  since there are four 1s in each row.
Every row covers 3 of the 21 pairs of columns, so no smaller -covering family of weight  exists.}\label{fig:hadamard7}
	\end{center}
\end{figure}

\begin{lemma}\label{lemma:covering1}
For every , the Hamming projection family  is -covering.
\end{lemma}
\begin{proof}
Let  satisfy  and consider  as defined in (\ref{def:A_v}).
It is clear that whenever  we have  (recall that ).
To consider  for  let , where elements are interpreted as -dimensional vectors over the field .
The span of  has dimension at most , and since the space is -dimensional there exists a vector  that is orthogonal to .
In particular  for all .
In turn, this means that , as desired. 
\end{proof}

If the values of the function  are ``balanced'' over nonzero vectors the family  has weight close to~ for . 
More precisely we have:
\begin{lemma}\label{lemma:weight}
Suppose  for each  and . Then  has weight at least .
\end{lemma}
\begin{proof}
It must be shown that  for each nonzero vector .
Note that  has a dot product of~1 with a set  of exactly  vectors (namely the nontrivial coset of 's orthogonal complement).
For each  the we have  for all .
Thus the number of 1s in  is:
  
\end{proof}

\medskip

\paragraph{Comment on optimality}
We note that the size  is close to the smallest possible for an -covering families with weight around .
To see this, observe that  possible sets of errors need to be covered, and each hash function can cover at most  such sets.
This means that the number of hash functions needed is at least 

which is within a factor of~2 from the upper bound. 

\medskip

Lemmas~\ref{lemma:covering1} and~\ref{lemma:weight} leave open the choice of mapping~.
We will analyze the setting where  maps to values chosen uniformly and independently from . 
In this setting the condition of Lemma~\ref{lemma:weight} will in general not be satisfied, but it turns out that it suffices for  to have balance in an expected sense.
We can relate collision probabilities to Hamming distances as follows:

\begin{theorem}\label{thm:basic}
	For all  and for random ,
	\begin{enumerate}
\item If  then .
\item .
\end{enumerate}
\end{theorem}
\begin{proof}
Let .
For the first part we have .
Lemma~\ref{lemma:covering1} states that there exists 
 such that .
By Lemma~\ref{lem:xor} this implies .

To show the second part we fix .
Now consider , defined in~(\ref{def:A_v}), and the corresponding function .
For  we have  if and only if .
Since  is random and  the  values are independent and random, so the probability that  for all  is .
By linearity of expectation, summing over  choices of  the claim follows. 
\end{proof}

\paragraph{Comments} A few remarks on Theorem~\ref{thm:basic} (that can be skipped if the reader wishes to proceed to the algorithmic results):
\begin{itemize}
\item The vectors in  can be seen as samples from a \emph{Hadamard code} consisting of  vectors of dimension , where bit  of vector  is defined by , again interpreting the integers  and  as vectors in .
Nonzero Hadamard codewords have Hamming weight and minimum distance .
However, it does not seem that error-correcting ability in general yields nontrivial -covering families.

\item The construction can be improved by changing  to map to  and/or requiring the function values of  to be \emph{balanced} such that the number of bit positions mapping to each vector in  is roughly the same.
This gives an improvement when  but is not significant when  is much smaller or much larger than .
To keep the exposition simple we do not analyze this variant.
\item At first glance it appears that the ability to avoid collision for CoveringLSH (``filtering'') is not significant when .
However, we observe that for similarity search in Hamming space it can be assumed without loss of generality that either all distances from the query point are even or all distances are odd.
This can be achieved by splitting the data set into two parts, having even and odd Hamming weight, respectively, and handling them separately.
For a given query  and radius~ we then perform a search in each part, one with radius~ and one with radius  (in the part of data where distance  to  is not possible).
This reduces the expected number of collisions at distance  to at most . 
\end{itemize}


\paragraph{Nearest neighbor} 

Above we have assumed that the search radius  was given in advance, but it turns out that CoveringLSH supports also supports finding the nearest neighbor, under the condition that the distance is at most~.
To see this, consider the subfamily of  indexed by vectors of the form , where  for some , then collision is guaranteed up to distance .
That is, we can search for a nearest neighbor at an unknown distance in a natural way, by letting  map randomly to  and choosing  as the binary representation of  (or alternatively, the vectors in a Gray code for ).
In either case Theorem~\ref{thm:basic} implies the invariant that the nearest neighbor has distance at least , where  is interpreted as an integer.
This means that when a point  at distance at most  is found, we can stop after finishing iteration  and return  as a -approximate nearest neighbor.
Figure~\ref{fig:code} gives pseudocode for data structure construction and nearest neighbor queries using CoveringLSH.\footnote{A corresponding Python implementation is available on github, {\tt https://github.com/rasmus-pagh/coveringLSH}.}


\begin{figure}[t]
	\hrule
\begin{minipage}{.5\linewidth}
	\begin{tabbing}
	  xx\=xx\=xx\=xx\=xx\=xx\=xx\=\kill\\
	  {\bf procedure} {\sc InitializeCovering}\+\\
	    {\bf for}  {\bf do} \\
	    {\bf for}  to  {\bf do} \+\\
			\\
	    	{\bf for}  {\bf do}
				 mod \-\\
	  {\bf end for}\-\\
	  {\bf end}\\
	  \\
	  {\bf function} {\sc BuildDataStructure}\+\\
	  	\\
{\bf for} ,  {\bf do}\+\\
		      \-\\
		{\bf return} \-\\
	  {\bf end}\\
	\end{tabbing}
	\end{minipage}
	\begin{minipage}{.5\linewidth}
	\begin{tabbing}
	  xx\=xx\=xx\=xx\=xx\=xx\=xx\=\kill
	  \+\+\\
	  {\bf function} {\sc NearestNeighbor}\+\\
	  	\\ 
	  	\\ 
	    {\bf for}  to  {\bf do} \+\\
			{\bf for}  {\bf do}\+\\
		  		{\bf if}  {\bf then}\+\\
					\\
					\-\\
				{\bf end if}\-\\
			{\bf end for}\\
			{\bf if}  {\bf then return} \-\\
		{\bf end for}\\
		{\bf return} \-\\
	  {\bf end}
	\end{tabbing}
	\vspace{2.5mm}
	\end{minipage}
	\hrule
\caption{Pseudocode for constructing (left) and querying (right) a nearest neighbor data structure on a set  as described in section~\ref{sec:randomdata}.
Parameter  controls the largest radius for which a nearest neighbor is returned.
This is the simplest instantiation of CoveringLSH --- it works well on high-entropy data where there are few points within distance  of a query point.
In this setting, given a query point , the expected search time for finding a nearest neighbor  is . If only a -approxiate nearest neighbor is sought the condition  should be changed to . \smallskip\newline
{\bf Notation:} The function  returns a random element from a given set.
The inner product  can be computed by a bitwise conjunction followed by counting the number of bits set ({\sc popcnt}).
 is used to denote the information associated with key  in the dictionary  that is the main part of the data structure; if  is not a key in  then . 
The function call  typecasts an integer to a bit vector of dimension .
Finally,  denotes the Hamming distance between  and . 
\smallskip\newline
{\bf Other comments:} 
Vectors are stored  times in , but may be represented as references to a single occurrence in memory to achieve better space complexity for large~.
The global dictionary , which contains a covering independent of the set , must be initialized by {\sc InitializeCovering} before {\sc BuildDataStructure} is called.
Note that the function  is not stored, as it is not needed after constructing the covering.}
\label{fig:code}
\end{figure}


\subsection{Approximation factor }\label{sec:randomdata}

We first consider a case in which the method above directly gives a strong result, namely when the threshold  for being an approximate near neighbor equals .
Such a threshold may be appropriate for high-entropy data sets of dimension  where most distances tend to be large (see~\cite{kucherov2005multiseed,norouzi2012fast} for discussion of such settings).
In this case Theorem~\ref{thm:basic} implies efficient -approximate near neighbor search in expected time , where  bounds the time to compute the Hamming distance between query vector  and a vector . This matches the asymptotic time complexity of~\cite{Indyk1998}.

To show this bound observe that the expected total number of collisions , summed over all  and  with , is at most .
This means that computing  for each  and computing the distance to the vectors that are not within distance  but collide with  under some  can be done in expected time .
The expected bound can be supplemented by a high probability bound as follows: Restart the search in a new data structure if the expected time is exceeded by a factor of~2. Use  data structures and resort to brute force if this fails, which happens with polynomially small probability in~.

What we have bounded is in fact performance on a \emph{worst case} data set in which most data points are just above the threshold for being a -approximate near neighbor.
In general the amount of time needed for a search will depend on the distribution of distances between~ and data points, and may be significantly lower.

The space required is  words plus the space required to store the vectors in~, again matching the bound of Indyk and Motwani.
In a straightforward implementation we need additional space  to store the function , but if  is large (for sets of sparse vectors) we may reduce this by only storing  if there exists  with .
With this modification, storing  does not change the asymptotic space usage.
For dense vectors it may be more desirable to explicitly store the set of covering vectors  rather than the function , and indeed this is the approach taken in the pseudocode.


\medskip

\paragraph{Example} Suppose we have a set  of  vectors from  and wish to search for a vector at distance at most  from a query vector .
A brute-force search within radius~ would take much more time than linear search, so we settle for -approximate similarity search.
Vectors at distance larger than  have collision probability at most  under each of the  functions in , so in expectation there will be less than  hash collisions between  and vectors in . 
The time to answer a query is bounded by the time to compute  hash values for  and inspect the hash collisions.

It is instructive to compare to the family  of Indyk and Motwani, described in section~\ref{sec:lsh}, with the same performance parameters ( hash evaluations, collision probability  at distance ).
A simple computation shows that for  samples we get the desired collision probability, and collision probability  at distance~.
This means that the probability of a false negative by not producing a hash collision for a point at distance  is .
So the risk of a false negative is nontrivial given the same time and space requirements as our ``covering'' LSH scheme. 







\section{Construction for large distances}\label{sec:large}

Our basic construction is only efficient when  has the ``right'' size (not too small, not too large). 
We now generalize the construction to arbitrary values of , , and , with a focus on efficiency for large distances. 
In a nutshell:
\begin{itemize}
\item For an arbitrary choice of  (even much larger than ) we can achieve performance that differs from classical LSH by a factor of  in the exponent.\item We can match the exponent of classical LSH for the -approximate near neighbor problem whenever  is (close to) integer.
\end{itemize}
\noindent
We still use a Hamming projection family (\ref{eq:hamming-projection}), changing only the set  of bit masks used.
Our data structure will depend on parameters  and , i.e., these can not be specified as part of a query.
Without loss of generality we assume that  is integer.

\medskip

\paragraph{Intuition}
When  we need to increase the average number of 1s in the bit masks to reduce collision probabilities.
The increase should happen in a correlated fashion in order to maintain the guarantee of collision at distance~.
The main idea is to increase the fraction of 1s from  to , for , by essentially repeating the sampling from the Hadamard code  times and selecting those positions where at least one sample hits a~1. 

On the other hand, when  we need to decrease the average number of 1s in the bit masks to increase collision probabilities. This is done using a refinement of the partitioning method of~\cite{Arasu_VLDB06} which distributes the dimensions across partitions in a balanced way.
The reason this step does not introduce false negatives is that for each data point  there will always exist a partition in which the distance between query  and~ is at most the average across partitions. 
An example is shown in figure~\ref{fig:hadamard2x7}.

\medskip

\begin{figure}[t]
	\begin{center}
\includegraphics{coveringMatrix14.pdf}
\caption{The collection  containing two copies of the collection  from figure~\ref{fig:hadamard7}, one for each half of the dimensions. The resulting Hamming projection family  , see (\ref{eq:hamming-projection}), is -covering since for set of 5 columns there exists a row with 0s in these columns. It has weight  since there are four 1s in each row.
Every row covers  sets of 5 columns, so a lower bound on the size of a -covering collection of weight  is .}\label{fig:hadamard2x7}
	\end{center}
\end{figure}


We use  to denote, respectively, the number of partitions and the number of partitions to which each dimension belongs.
Observe that if we distribute  copies of  ``mismatching'' dimensions across  partitions, there will always exist a partition with at most  mismatches.
Let  denote the set of intervals in  of length , where intervals are considered modulo~ (i.e., with wraparound).
We will use two random functions,


to define a family of bit vectors , indexed by vectors  and . 
We define a family of bit vectors  by

where  is the preimage of  under  represented as a vector in  (that is,  if and only if ), and  is the dot product of vectors  and~.
We will consider the family of all such vectors with nonzero :

Note that the size of  is .


\begin{lemma}\label{lemma:covering2}
For every choice of , and every choice of functions  and  as defined above, the Hamming projection family  is -covering.
\end{lemma}
\begin{proof}
Let  satisfy . 
We must argue that there exists a vector  and  such that , i.e., by (\ref{def:A_vk})

We let , breaking ties arbitrarily.
Informally,  is the partition with the smallest number of 1s in .
Note that  so by the pigeonhole principle, .
Now consider the ``problematic'' set  of positions of 1s in , and the set of vectors that  associates with it:

The span of  has dimension at most .
This means that there must exist  that is orthogonal to all vectors in .
In particular this implies that for each  we have  is false, as desired. 
\end{proof}

We are now ready to show the following extension of Theorem~\ref{thm:basic}:
\begin{theorem}\label{thm:main}
	For random  and , for every  and :
	\begin{enumerate}
\item .
\item .
\end{enumerate}
\end{theorem}
\begin{proof}
    By Lemma~\ref{lem:xor} we have  if and only if  where .
	So the first part of the theorem is a consequence of Lemma~\ref{lemma:covering2}.
	For the second part consider a particular vector , where  is nonzero, and the corresponding hash value .
	We argue that over the random choice of  and  we have, for each :

The second equality uses independence of the vectors  and , and that for each  we have .
	Observe also that  depends only on  and .
	Since function values of~ and  are independent, so are the values 

This means that the probability of having  for all  where  is a product of probabilities from~(\ref{eq:zeroprob}):

The second part of the theorem follows by linearity of expectation, summing over the 
vectors in . 
\end{proof}


\subsection{Choice of parameters}

The expected time complexity of -approximate near neighbor search with radius~ is bounded by the size  of the hash family plus the expected number  of hash collisions between the query~ and vectors  that are not -approximate near neighbors. Define

where the expectation is over the choice of family .
Choosing parameters , , and  in Theorem~\ref{thm:main} in order to get a family  that minimizes  is nontrivial.
Ideally we would like to balance the two costs, but integrality of the parameters means that there are ``jumps'' in the possible sizes and filtering efficiencies of
. Figure~\ref{fig:numerical-optimization-low-radius} shows bounds achieved by numerically selecting the best parameters in different settings. We give a theoretical analysis of some choices of interest below. In the most general case the strategy is to reduce to a set of subproblems that hit the ``sweet spot'' of the method, i.e., where  and  can be made equal. 

\begin{figure*}[t]
	\begin{center}
	\includegraphics[width=\textwidth]{coveringLSH-vs-classical-LSH2.pdf}\\
	\bigskip
	\includegraphics[width=\textwidth]{coveringLSH-vs-classical-LSH3.pdf}
	\caption{Expected number of memory accesses for different similarity search methods for finding a vector within Hamming distance~ of a query vector~. The plots are for  and , respectively, and are for a worst-case data set where all points have distance~ from~, i.e., there exists no -approximate near neighbor for an approximation factor . The bound for exhaustive search in a Hamming ball of radius  optimistically assumes that the number of dimensions is~, which is smallest possible for a data set of size  (for  this number is so large that it is not even shown).
	Two bounds are shown for the classical LSH method of Indyk and Motwani: A small fixed false negative probability of 1\%, and a false negative probability of .
	The latter is what is needed to ensure no false negatives in a sequence of  searches.
	The bound for CoveringLSH in the case  uses a single partition (), while for  multiple partitions are used.}\label{fig:numerical-optimization-low-radius}
	\end{center}
\end{figure*}


\begin{corollary}\label{cor:params}
	For every  there exist explicit, randomized -covering Hamming projection families , 
	such that for every :
	\begin{enumerate}
		\item  and 
		.
		\item If , for , then  and .
		\item If  then  and 
		.
	\end{enumerate}
\end{corollary}
\begin{proof}
		We let  with  and . Then 

Summing over  the second part of Theorem~\ref{thm:main} yields: 


\medskip
		
		For the second bound on  we notice that the factor  is caused by the rounding in the definition of~, which can cause  to jump by a factor . When  is integer we instead get a factor .
		
\medskip
		
		Finally, we let  with , , and . The size of  is bounded by 
.
Again, by Theorem~\ref{thm:main} and summing over :

where the second inequality follows from the fact that  when . 
\end{proof}




\section{Construction for small distances}\label{sec:small-radius}

In this section we present a different generalization of the basic construction of Section~\ref{sec:basic} that is more efficient for small distances, , than the construction of Section~\ref{sec:large}.
The existence of \emph{asymptotically} good near neighbor data structures for small distances is not a big surprise: For  it is known how to achieve query time ~\cite{Cole:2004:DMI:1007352.1007374}, even with .
In practice this will most likely no faster than linear search for realistic values of~ except when  is a small constant.
In contrast we seek a method that has reasonable constant factors and may be useful in practice.

The idea behind the generalization is to consider vectors and dot products modulo~ for some prime .
This corresponds to using finite geometry coverings over the field of size ~\cite{gordon1995new}, but like in Section~\ref{sec:basic} we make an elementary presentation without explicitly referring to finite geometry.
Vectors in the -covering family, which aims at weight around , will be indexed by nonzero vectors in .
Generalizing the setting of Section~\ref{sec:basic}, the family depends on a function  that maps bit positions to vectors of length~. 
Define a family of bit vectors ,  by

for all , where  is the dot product of vectors  and .
We will consider the family of all such vectors with nonzero :




\begin{lemma}\label{lemma:covering3}
For every , the Hamming projection family  is -covering.
\end{lemma}
\begin{proof}
	Identical to the proof of Lemma~\ref{lemma:covering1}. The only difference is that we consider the field  of size~.
\end{proof}



Next, we relate collision probabilities to Hamming distances as follows:
\begin{theorem}\label{thm:finitegeometry}
	For all  and for random ,
	\begin{enumerate}
\item If  then .
\item .
\end{enumerate}
\end{theorem}
\begin{proof}
The proof is completely analogous to that of Theorem~\ref{thm:basic}. 
The first part follows from Lemma~\ref{lemma:covering3}.
For the second part we use that  for each  and that we are summing over  values of .
\end{proof}

Now suppose that  and let  be the smallest prime number such that , or in other words the smallest prime . We refer to the family  with this choice of  as , and note that .

By the second part of Theorem~\ref{thm:finitegeometry} the expected total number of collisions , summed over all  and  with , is at most .
This means that computing  for each  and computing the distance to the vectors that are not within distance  but collide with  under some  can be done in expected time .

What remains is to bound  in terms of  and~.
According to results on prime gaps (see e.g.~\cite{dudek2014explicit} and its references)
there exists a prime between every pair of cubes  and  for  larger than an explicit constant.
We will use the slightly weaker upper bound , which holds for .
If  exceeds a certain constant, since  is the smallest such prime, choosing  we have .
By our upper bound on  we have .
Using  we have



\paragraph{Improvement for small }

To asymptotically improve this bound for small  we observe that without loss of generality we can assume that : 
If this is not the case move to vectors of dimension  by repeating all vectors  times, where  is the largest integer with . 
This increases all distances by a factor exactly , and increases  by at most a factor .
Then we have:

That is, the expected time usage of  matches the asymptotic time complexity of~\cite{Indyk1998} up to a polylogarithmic factor.


\paragraph{Comments}

In principle, we could combine the construction of this section with partitioning to achieve improved results for some parameter choices.
However, it appears difficult to use this for improved bounds in general, so we have chosen to not go in that direction.
The constant 3 in the upper bound on  comes from bounds on the maximum gap between primes. A proof of CramÃ©r's conjecture on the size of prime gaps would imply that 3 can be replaced by any constant larger than~1, which in turn would lead to a smaller exponent in the polylogarithmic overhead.




\section{Proof of Theorem~\ref{thm:RAM}}

The data structure will choose either  or  of Corollary~\ref{cor:params}, or  of section~\ref{sec:small-radius} with size bounded in (\ref{eq:smallbound}), depending on which  minimizes . The term  comes from part (3) of Corollary~\ref{cor:params} and the inequality .

The resulting space usage is  bits, representing buckets by list of pointers to an array of all vectors in~.
Also observe that the expected query time is bounded by .
\qed



\section{Conclusion and open problems}

We have seen that, at least in Hamming space, LSH-based similarity search can be implemented to avoid the problem of false negatives at little or no cost in efficiency compared to conventional LSH-based methods.
The methods presented are simple enough that they may be practical.
An obvious open problem is to completely close the gap, or show that a certain loss of efficiency is necessary (the non-constructive bound in section~\ref{sec:nofalseneg} shows that the gap is at most a factor ).

It is of interest to investigate the possible time-space trade-offs.
CoveringLSH uses superlinear space and employs a data independent family of functions. 
Is it possible to achieve covering guarantees in linear or near-linear space?
Can data structures with very fast queries and polynomial space usage match the performance achievable with false negatives~\cite{DBLP:journals/corr/Laarhoven15a}?

Another interesting question is what results are possible in this direction for other spaces and distance measures, e.g., , , or .
For example, a more practical alternative to the reduction of~\cite{DBLP:conf/stoc/Indyk07} for handling  and  would be interesting.

Finally, CoveringLSH is data independent. Is it possible to improve performance by using data dependent techniques?


\medskip

{\bf Acknowledgements.} The author would like to thank: Ilya Razenshteyn for useful comments; Thomas Dybdal Ahle, Ugo Vaccaro, and Annalisa De Bonis for providing references to work on explicit covering designs; Piotr Indyk for information on reduction from  and  metrics to the Hamming metric; members of the Scalable Similarity Search project for many rewarding discussions of this material.

\newpage

\bibliographystyle{ACM-Reference-Format-Journals}
\bibliography{coveringLSH}

\end{document}
