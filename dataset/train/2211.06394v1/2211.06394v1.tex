\documentclass[3p, preprint, times]{elsarticle}
\graphicspath{ {./figures/} }
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{verbatim} \usepackage{apalike}
\usepackage{mathrsfs,amsmath}
\restylefloat{figure}
\restylefloat{table}
\hypersetup{colorlinks}
\usepackage{color}
\usepackage{array}
\usepackage{algpseudocode}
\usepackage{algorithm}

\usepackage{booktabs}
\usepackage{multirow}

\bibliographystyle{model5-names}\biboptions{authoryear}
\DeclareMathOperator*{\argmax}{argmax}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\begin{document}
\begin{frontmatter}

\journal{Neurocomputing}

\title{STAR: \\ A Session-Based Time-Aware Recommender System}


\author[label1]{Reza Yeganegi}
\ead{yeganegi.reza@ut.ac.ir}

\author[label1]{Saman Haratizadeh\corref{cor1}}
\ead{haratizadeh@ut.ac.ir}


\cortext[cor1]{Corresponding author}
\address[label1]{Faculty of New Sciences and Technologies, University of Tehran, Tehran, Iran}


\begin{abstract}

Session-Based Recommenders (SBRs) aim to predict users' next preferences regard to their previous interactions in sessions while there is no historical information about them. Modern SBRs utilize deep neural networks to map users' current interest(s) during an ongoing session to a  latent space so that their next preference can be predicted. Although state-of-art SBR models achieve satisfactory results, most focus on studying the sequence of events inside sessions while ignoring temporal details of those events. In this paper, we examine the potential of session temporal information in enhancing the performance of SBRs, conceivably by reflecting the momentary interests of anonymous users or their mindset shifts during sessions. We propose the STAR framework, which utilizes the time intervals between events within sessions to construct more informative representations for items and sessions. Our mechanism revises session representation by embedding time intervals without employing discretization. Empirical results on Yoochoose and Diginetica datasets show that the suggested method outperforms the state-of-the-art baseline models in Recall and MRR criteria.

\end{abstract}

\begin{keyword}
Session-Based Recommendation \sep Time-Aware Recommender Systems \sep Recurrent Neural Networks \sep Representation Learning
\end{keyword}

\end{frontmatter}

\label{sec:Introduction}

A Session-based recommender system aims to predict the users' next item based on their previous interacted items in sessions. Such a system has two characteristics that distinguish it from other recommender systems: 1) The lack of users' identity information and 2) the significance of short-term preferences \cite{wang2019survey}.

Since the only information source is the interaction data in an ongoing session, SBRs suffer from users' identity unavailability. Thus, user-item matrix-analysis models cannot be used directly in this class of recommender systems. Furthermore, compared to a wide range of recommendation models that employ all historical users' intentions, SBR models recommend items by focusing on users' interactions in the current session. 

In recent years deep learning approaches have been one of the most potent methods. These models usually concentrate on encoding a session into a vector defined as a session representation resulting from the recommendation errors. Seminal studies on deep learning models \cite{zhang2014sequential,hidasi2016session} tried to design SBRs with different types of deep neural networks to outperform the conventional models. These models are extended by using techniques like data augmentation \cite{tan2016improved}, attention mechanism \cite{li2017neural,liu2018stamp}, combining the conventional models with deep learning models \cite{jannach2017recurrent}, and feature engineering \cite{wu2019session}.

One aspect of the session-based data that has been investigated rarely is the role of time intervals between user-item interactions in reflecting users' mindset during sessions. Intuitively, those time intervals may be interpreted in several ways: the importance of an item, distraction of a user, a change in a user mindset, an unintentional click, etc. As a case in point, please see the session presented in Fig.~\ref{fig:Time_Interval}. The time intervals between items  and  are considerable; therefore, the user has probably spent much time on item . This observation can indicate his/her interest, which increases the chance of revisiting this item in the session. On the other hand, a relatively short interaction time (e.g., ) may show a lack of interest.

However, it is clear that, usually, there is no simple explanation for such an observation. For example, a substantial time interval between interactions may indicate a temporary user distraction or a technical problem while visiting the web page. Therefore, we probably need comprehensive models to combine the time interval data and other available information in a session to extract the user's interests.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=0.7\textwidth]{Time_Interval.png}\\
        \caption{Time Intervals Between Interactions}
        \label{fig:Time_Interval}
    \end{center}
\end{figure}

In this paper, we present the STAR framework (Session-based and Time-Aware Recommender system) that employs the sequence of items observed in a session, as well as the time intervals between them, to construct a vector representation for a session, which accurately reflects the interest of the user at the moment and then uses this representation to predict his/her subsequent interest. We also utilize a technique in which there is no need for any discretization method of time interval values. In this system, time intervals are one of the main components that directly influence outputs. 

The main contributions of this work are summarized as follows:

\begin{enumerate}
    \item We introduce an approach to using time intervals between user-item interactions in a session to learn better behavioral patterns and make more accurate recommendations.

	\item We propose an approach to embed time intervals between items and utilize it to adjust each item's effect upon the aggregated session representation. 
	
	\item We suggest a method that uses time interval information to construct temporally sub-sessions in order to embed items based on their co-occurrence.
	
\end{enumerate}

The rest of this paper is organized as follows: In Section (\ref{sec:background}), we will review the existing session-based recommender systems. Then in section (\ref{sec:proposedModel}), the STAR framework and the training steps will be described. The experimental results will be reported, and our model will be compared to the state-of-the-art baselines in section (\ref{sec:results}). Finally, in section (\ref{sec:conclusion}), we will conclude the paper and suggest future works.

\section{Related Works}
\label{sec:background}

\textit{Conventional approaches:} Conventional session-based recommender systems usually analyze a part of sessions to infer the next step recommendations, namely K-Nearest Neighbor (KNN) based approaches, which are simple but effective \cite{ludewig2018evaluation}. Their primary idea is to find the similarity between the current session and previous ones in train data in various ways. For example, some of them seek the similarity between the last items of the sessions (Item-based KNN) \cite{hidasi2016session} or the entire events inside the sessions (Session-based KNN) \cite{jannach2017recurrent}.

Markov chain models are other types of conventional SBRs that utilize the Markov Decision Process (MDP). They consider sessions as Markov chains and endeavor to predict the next item based on transition probability between items. Whether the transition probability is calculated based on direct observations or latent spaces, Markov models can be divided into basic Markov chain-based approaches and latent Markov embedding-based approaches. \cite{zhang2007efficient} combined first and second-order Markov transitions to assemble a more accurate recommender model in the Web page domain. \cite{chen2012playlist} presented Logistic Markov Embedding (LME) to represent songs as one (or multiple) point(s) in Euclidean space so that Euclidean distance between songs reflects the transition probabilities between songs.

A well-known class of recommender systems includes methods that build latent representations by user and item connections. These methods aim to find patterns between users and items but are powerless for SBRs with anonymous users. However, with a few modifications, they can be applied to SBRs. \cite{rendle2010factorizing} proposed Factorized Personalized Markov Chains (FPMC) for the next basket recommendation. By limiting the basket size to one and considering the current session as the basket history, FPMC can be directly applied to SBRs \cite{ludewig2018evaluation}. Factored Item Similarity Model (FISM) is another model based on item-item factorization \cite{kabbur2013fism}. \cite{ludewig2018evaluation} used FISM with a few modifications from the movie recommendations domain to SBRs. Factorized Sequential Prediction with Item SImilarity modeL (FOSSIL) \cite{he2016fusing} combined FISM and FPMC and introduced another Latent representation-based approach.

\textit{Deep Learning approaches:} In recent years applying neural networks, especially RNNs, to improve the performance of SBRs has gained significant attention. The main idea of these methods is to learn different levels of behavioral patterns. \cite{hidasi2016session} offered GRU4Rec, which operates RNNs and suggests the session parallel mini-batch method with negative sampling to feed sessions online into a GRU. GRU4Rec achieved remarkable results as opposed to Conventional SBRs. GRU4Rec generates a representation for each session which is the last hidden state of the GRU. Finally, this representation lists a set of items as a recommendation. They also introduced pairwise ranking loss functions, leading to better results than cross-entropy loss. Subsequent attempts have been made to increase the accuracy of GRU4Rec. \cite{tan2016improved} improved GRU4Rec using data augmentation techniques. \cite{jannach2017recurrent} presented a heuristics-based nearest neighbor (KNN) scheme for sessions by combining RNN and KNN. The power of modeling the short-term preferences of SBRs can be used to improve the accuracy of the other types of recommenders when the users' IDs are available. \cite{zhao2019leveraging} utilized RNN to model short-term preferences and MF for long-term interests (that change very slowly) with the Generative Adversarial Network (GAN) in the movie recommendation domain.

NARM \cite{li2017neural} and STAMP \cite{liu2018stamp} were designed to model short-term and long-term users' preferences simultaneously in sessions. They argue that models which have been proposed merely based on the last hidden of RNN do not distinguish between items priority in sessions. However, a user may change his/her preferences during a session (short-term preference), so the importance of items for users may change. NARM and STAMP use attention mechanisms to adjust items' importance in sessions focusing on the last observed item and devising a new representation vector indicating the user's short-term preference. Finally, their model recommends items by combining the session representation vector (long-term preference vector) and the short-term preference vector. The difference between these two models is in considering the importance of vectors representing short-term and long-term preferences. In NARM, short-term and long-term preference vectors are concatenated, and the item scores are calculated by vectors passing through a linear layer. While in STAMP, the trilinear product of short-term and long-term vectors is used. While The principle of NARM and STAMP methods is to model the connection between the final item and the previous ones, the SR-GNN model \cite{wu2019session} is developed on the assumption that a significant part of information can be discovered by discovering more connections among all a session's items. Considering this idea, the SR-GNN uses a graphical representation of the session and a GNN-based approach to analyze it. 

\textit{Time Aware approaches:} Although these developed SBR deep models have achieved remarkable results, they all focus on extracting patterns from the sessions by scrutinizing the events observed during sessions. One of the first attempts to incorporate time information in SBR models is STAN \cite{garg2019sequence}. It examined the effect of three characteristics: the time interval between the current session and neighborhood sessions, the position of the item in neighborhood sessions, and the position of the item in the current session. K nearest neighbors of a session are determined using the cosine similarity between the item occurrence vectors of the session and the past ones. 

Some models have applied auxiliary information, including time intervals, to improve the performance of sequential recommendations. Multivariate Hawkes Process Embedding with attention (MHPE-a) \cite{wang2021sequential} maps each item and user entity to an embedding space. Then it obtains the probability that item  will occur by user  at time  using a Multivariate Hawkes Process model and an attention mechanism. TiSASRec \cite{li2020time} employs absolute positions of items and time intervals between them in a user's sequence. For each user, a personalized time interval matrix is made, in which every matrix entity indicates a scaled time interval between two items. Then this matrix is used to adjust user and item embeddings. TIEN \cite{li2020deep} is a Click-Through Rate (CTR) prediction model introducing time-aware item behaviors in addition to user behaviors. The item behavior for an item is a sequence of users who interact with this item in chronological order. These item behaviors are adjusted by calculating the weight of each user using time intervals. However, since these models are designed for situations where user identifications are available, and user embedding is one of their main components, they cannot be directly applied for session based recommendation.


\section{Proposed Method: STAR}\label{sec:proposedModel}

In the STAR model, the learning process is performed in two main steps. First, we use the GloVe method \cite{pennington2014glove} to obtain initial item embeddings based on the co-occurrences of items in sub-session groups. Second, we fine-tune item embeddings, adjust them using time intervals between items, generate session representations, and predict the following items.

\begin{figure*}[t]
    \begin{center}
        \includegraphics[width=1\textwidth]{STAR_Framework.png}
        \caption{STAR Framework: A recommendation is generated by six modules in the STAR Framework. Module 1: Feature Extraction, Module 2: Session Encoding, Module 3: Weight Calculation,
        Module 4: Time Attention, Module 5: Self Attention, Module 6: Preference Detection.}
        \label{fig:STAR_framwork}
    
    \end{center}
\end{figure*}


\subsection{Problem Definition and Notation}
Suppose a typical user interacts with a set of items from steps  to , one item at each step. An SBR aims to predict the following item which this user will probably interact with at step  based on her/his history of interactions (viewing items) from steps  to . Here we give a formulation of this problem:

Let  be the set of all items. Session , which , is a sequence of items in chronological order from  that a user has interacted with. Referring to (\ref{eq:score}), given a user interaction history in session , our model (M) aims to predict the interaction probability of all   as the next preference of the user.




\begin{table}[!h]
\caption{Notation}
\label{tab:notation}
\centering
\begin{tabular}{ll}
\hline
Notation             & Description                                  \\ \hline
                  & Item set                                     \\
                  & A sample session of an anonymous user        \\
n                    & Number of items in                        \\
d                    & Latent vector dimension                      \\
                  & One-hot vector                               \\
                  & Embedding vector.                            \\
 & The time interval between value\hh^{'}h^{''}\thetaI_{t_k}X^{I}_{t_{k}}I_{t_k}E^{I}_{t_{k}}X^{I}_{t_{k}}\mathcal{W}^{I} \in \mathbb{R}^{n*d}\Delta T_{k-1,k}\Delta T_{k,k+1}\Delta T_{k-1,k}\Delta T_{k,k+1}{hh}{mm}{ss}\{X^{B_h}_{t_k},X^{B_m}_{t_k},X^{B_s}_{t_k}\}\{X^{A_h}_{t_k},X^{A_m}_{t_k},X^{A_s}_{t_k}\}{hh}^B_{t_k}{mm}^B_{t_k}{ss}^B_{t_k}{hh}^A_{t_k}{mm}^A_{t_k}{ss}^A_{t_k}\{{W}^{B_h} \in \mathbb{R}^{24*d},{W}^{B_m} \in \mathbb{R}^{60*d}, {W}^{B_s} \in \mathbb{R}^{60*d}\}\{{W}^{A_h} \in \mathbb{R}^{24*d},{W}^{A_m} \in \mathbb{R}^{60*d}, {W}^{A_s} \in \mathbb{R}^{60*d}\}I_{t_k}kE^{B}_{t_1}E^{A}_{t_k}\Delta T_{k,k+1}=182I_{t_k}E^{A}_{t_k}\Delta T_{k,k+1}E^{I}_{t_k}I_{t_k}h_{k-1}h^{'}_{k-1}t_{k-1}h_{k}h^{'}_{k}t_kh^{'}_{k}h_{k}t_kAW^{A}_{t_k}AW^{B}_{t_k}E^{A}_{t_k}E^{B}_{t_k}W^{1}, W^{2} \in \mathbb{R}^{3d*d}b^{1}, b^{2} \in \mathbb{R}^{d}\sigmaAW^{A}_{t_k}AW^{B}_{t_k}h^{''}_{k}k^{th}h^{A}_{k}h^{B}_{k}\odoth^{A}_{k}h^{B}_{k}h^{A}_{k}h^{A}_{1}h^{A}_{m}h^{B}_{k}h^{B}_{1}h^{B}_{m}\{ \alpha^{A_1}_k, \alpha^{A_m}_k, \alpha^{B_1}_k, \alpha^{B_m}_k \}\{h^{A}_{m}h^{B}_{m}\}\{AP^{A}_{1}, AP^{A}_{m}, AP^{B}_{1}, AP^{B}_{m}\}hAP\mathcal{W}^3 \in \mathbb{R}^{6d*d}b^{3} \in \mathbb{R}^{d}z^{'}\mathcal{W}^{I}b^{4} \in \mathbb{R}^{n}\frac{1}{4}\frac{1}{64}mm-1Recall@kMrr@kRecall@kTP@kFN@kMRR@kMRR@krank_i\{0.01, 0.001, 0.0001\}l\theta\{\frac{1}{3}l,\frac{1}{2}l,l,2l,3l\}\{0.1, 0.2, 0.3\}\{64, 128, 256, 512, 1024\}\thetaRecall@20MRR@k20\frac{1}{4}\{AP^{A}_{1}, AP^{A}_{m}, AP^{B}_{1}, AP^{B}_{m}\}AW^{A}_{t_k}AW^{B}_{t_k}$ weight vectors are excluded from the framework, and solely the Self Attention module is applied to unadjusted session representations.
\end{itemize}

It can be observed in Table (\ref{tab:effects}) that eliminating any of attention modules from STAR decreases the performance. The Self Attention has a more important role in the algorithm's accuracy as the MRR criterion is lower in STAR\_V1 than STAR\_V2. This clarifies that the model tends to learn more accurate behavioral patterns by considering the users' main purpose in sessions. The results also imply that the Self Attention and Time Attention Modules are complimentary. This observation can indicate that session representations without time interval adjustment are not sufficiently authentic since Self Attention is applied to less-tuned session representation vectors. Furthermore, employing Time Attention without considering Self Attention which is adopted to capture the user's primary goal, leads to sub-optimal predictions.   

\section{Conclusion}\label{sec:conclusion}
In this paper, we presented an approach for using temporal information available in the session data to enhance the recommendation quality. We used the time feature to identify the discontinuities in the course of events in sessions. We embedded items based on their co-occurrence in the resulting sub-session groups, which may reflect the users' momentary interests. We also introduced an approach to embedding time intervals between the sessions' events that are used to adjust the weights of the items in the aggregated vector representation of sessions. Our experimental results indicated that the temporal information of sessions' events has the potential to help us build models that efficiently reflect the users' interests during their sessions. In this study, we focused on analyzing the temporal information inside sessions; however, as a subject for future research, it may be helpful to consider exploring temporal relations among events happening in different sessions to embed items and sessions so that they reflect the effect of trends and popular items as well.


\bibliography{STAR}
\end{document}
