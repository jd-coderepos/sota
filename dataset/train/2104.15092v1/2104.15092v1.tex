

\documentclass[final]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{bbm}

\usepackage{color}
\usepackage{balance}


\newcommand{\argmin}{\mathop{\mathrm{argmin\,}}}


\def\mathbi#1{\textbf{\em #1}}


\newcommand{\methodname}{Faster Meta Update Strategy}
\newcommand{\methodnameabbre}{FaMUS}
\newcommand{\modulename}{Layer-wise Gradient Sampling Module}
\newcommand{\modulenameabbre}{LGSM}

\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}




\pagestyle{empty} 

\begin{document}

\title{Faster Meta Update Strategy for Noise-Robust Deep Learning}

\author{Youjiang Xu \hspace{12pt} Linchao Zhu \hspace{12pt} Lu Jiang \hspace{12pt}  Yi Yang\\
Baidu Research ReLER, University of Technology Sydney Google Research\\
{\tt\small youjiangxu@gmail.com, lujiang@google.com, \{linchao.zhu, yi.yang\}@uts.edu.au}
}

\maketitle
\thispagestyle{empty} 






\begin{abstract}
It has been shown that deep neural networks are prone to overfitting on biased training data. Towards addressing this issue, meta-learning employs a meta model for correcting the training bias. 
Despite the promising performances, super slow training is currently the bottleneck in the meta learning approaches.
In this paper, we introduce a novel \methodname\ (\methodnameabbre) to replace the most expensive step in the meta gradient computation with a faster layer-wise approximation. We empirically find that \methodnameabbre\xspace yields not only a reasonably accurate but also a low-variance approximation of the meta gradient.
We conduct extensive experiments to verify the proposed method on two tasks. We show our method is able to save two-thirds of the training time while still maintaining the comparable or achieving even better generalization performance. In particular, our method achieves the state-of-the-art performance on
both synthetic and realistic noisy labels, and obtains promising performance on long-tailed recognition on standard benchmarks. Code are released at {\small \url{https://github.com/youjiangxu/FaMUS}}.

\end{abstract} 



\begin{figure}[th]
\begin{subfigure}{.45\textwidth}
\centering
\includegraphics[width=0.95\linewidth]{./figs/fig1.pdf}
\footnotesize\caption{Top-1 Accuracy vs. Training time}
\end{subfigure}
\begin{subfigure}{.23\textwidth}
\includegraphics[width=1.0\linewidth]{./figs/time_period.pdf}
\footnotesize\caption{Cost in One Iteration}
\end{subfigure}
\begin{subfigure}{.23\textwidth}
\includegraphics[width=1.0\linewidth]{./figs/gradient_variation.pdf}
\footnotesize\caption{Gradient Variance}
\end{subfigure}
\caption{(a) Top-1 accuracy vs. Training time (in hours) on the WebVision dataset~\cite{li2017webvision}. We apply our method on the MW-Net model~\cite{shu2019meta} and train them using the identical hardware platform of four NVIDIA V100 GPUs. (b) The average GPU running time (in seconds) of each step in MW-Net per training iteration. Inception-ResNet V2 is used as the backbone. (c) The meta gradient during the training process. The solid line denotes the mean and the shaded region show the standard deviation.}
\label{fig:motivation}
\end{figure}


\section{Introduction}
Deep neural networks (DNNs) have achieved impressive results in various computer vision applications such as image classification~\cite{krizhevsky2012imagenet,he2016deep}, object detection~\cite{ren2016faster,redmon2016you,liu2016ssd}, and semantic segmentation~\cite{he2017mask}. 
A notable issue is that DNNs are prone to memorizing the training data~\cite{zhang2016understanding, tanaka2018joint}, aggravating training set bias such as noisy training labels~\cite{zhang2016understanding} or imbalanced class distributions~\cite{he2009learning,zhu2020inflated}. This significantly degrades the generalization capabilities and results in skewed classifiers or degenerated feature representations.

Numerous works have been proposed to tackle this issue (\eg~\cite{jiang2017mentornet, han2018co, ren2018learning, li2020dividemix, lin2017focal}). Among them, meta-learning~\cite{ren2018learning, shu2019meta, wang2020training} has recently emerged as an effective framework to mitigate the training data bias. In a nutshell, it employs a meta-model to correct bias by providing a more precise estimation of the training data. The meta-model is updated by stochastic gradient descent using the \emph{meta gradient} (or the high-order gradient) computed on a small proportion of validation data that is assumed available during training\footnote{The extra validation dataset is not a requirement in meta-learning. As in our experiments, we can use a subset of pseudo-labeled training data as the validation data. In this case, no extra labels or data are used.}.
Recently, meta-learning approaches such as L2R~\cite{ren2018learning}, MW-Net~\cite{shu2019meta}, and MLC~\cite{wang2020training} have shown superior performance on several public benchmarks such as CIFAR~\cite{krizhevsky2009learning}, WebVision~\cite{li2017webvision}, and Clothing1M~\cite{xiao2015learning}.

Despite the promising empirical results~\cite{vyas2020learning,shu2020meta}, slow training is currently the bottleneck that prevents meta-learning from being applied in many applications.
The training time of the meta-learning model is approximately 37 times more than the regular DNN training time. For instance, it could take 4 days with 4 NVIDIA V100 GPUs to train MW-Net~\cite{shu2019meta} on a mini subset of WebVision~\cite{li2017webvision,jiang2017mentornet} of only 50K images.


To understand why the meta-learning approaches are computationally intensive,
we may divide the training into three stages: Virtual-Train, Meta-Train, and Actual-Train~\cite{wang2020training}, where each stage consists of a forward and a backward step. Figure~\ref{fig:motivation}(b) summarizes the GPU time for each stage using a representative meta-learning model called MW-Net~\cite{shu2019meta}. We find more than 80\% of the total computation comes from the Meta-Train backward step in which the \emph{meta gradient} is computed with respect to the loss on the validation data. In this step, the meta gradient is back-propagated through every layer of the network all the way back to the meta-model to update its parameters. Since the regular training does not have such a step, this overhead cost rapidly becomes significant as the number of layers grows in the deep networks.

In this work, we aim at improving the training efficiency of meta-learning while maintaining the generalization capability. We propose a new Meta-Train step, named \methodname\space (\methodnameabbre), to efficiently compute the meta gradient. The plausibility of our method relies on the important finding that the total meta gradient can be reasonably approximated by the meta gradient accumulated from only a few network layers. As a result, instead of accumulating meta gradients from all layers in the Meta-Train step, we design a gradient sampler that is learned to decide, whether or not, to aggregate the meta gradient for each layer. 
When the learnable gradient sampler is turned off, the meta gradient computation is hence circumvented for the corresponding layer. This saves a considerable amount of computation especially when the gradient samplers for lower layers are turned off. 


More importantly, we find the meta gradient yielded by the \methodnameabbre\space has lower variance. Figure~\ref{fig:motivation}(c) shows the total meta gradient of the ground-truth (blue curve) and the approximation by the \methodnameabbre\space (red curve). It shows that our approximation is reasonably close to the mean but has a much lower variance. We hypothesize this is because the \methodnameabbre\space learns to select a small number of most informative layers which hence reduces the noisy or redundant signals in the meta gradient. As shown in~\cite{neelakantan2015adding, miller2017reducing}, reduction in gradient variance results in faster and more stable optimization. We observe similar results in our experiments where our method is able to improve the generalization performance of the recent meta-learning methods on noisy training data. 

We conduct extensive experiments to verify the efficiency and efficacy of the proposed method. We demonstrate two benefits of our method in overcoming corrupted training labels. First, it speeds up the recent meta-learning methods~\cite{ren2018learning, shu2019meta, wang2020training} by at least three times while maintaining the comparable or even better generalization performance. For example, Figure~\ref{fig:motivation}(a) shows a faster and better convergence when we applied our method on the MW-Net model. Second, our method achieves new state-of-the-art performance on multiple benchmarks for both synthetic label noise and realistic label noise, including the challenging CNWL benchmark~\cite{jiang2020beyond}.
The comparison is fair as our meta-model is learned without using any extra data. In addition, we also validate our method on the long-tailed recognition task. On the long-tailed CIFAR dataset~\cite{cui2019class}, our method yields competitive performance compared to the recent strong baseline methods.

The contributions of this paper are three-fold. (1) We propose a new \methodname\space to efficiently learn to approximate the meta gradient, which halves two-thirds of the training time of the recent meta-learning methods~\cite{ren2018learning, shu2019meta, wang2020training}.
(2) We empirically show our approach reduces the variance of the meta gradient and improves the generalization performance of the meta-learning model. 
(3) Our method achieves state-of-the-art performance on several benchmarks with noisy labels. 
 
\section{Related Work}

\noindent\textbf{Corrupted/Noisy training labels.} 
Numerous methods have been recently proposed to learn robust deep networks that can overcome corrupted or noisy training labels. These methods address this problem from a variety of directions. For example, several works~\cite{goldberger2016training, patrini2017making, xia2019anchor, tanaka2018joint, yi2019probabilistic, arazo2019unsupervised, wang2020training} modeled the noise distribution or the transition matrix to correct noisy training samples.
Other approaches tried to reduce the weights assigned to noisy samples~\cite{malach2017decoupling, jiang2017mentornet, jiang2020beyond, su2017pose, shu2019meta, yang2019snapshot}. Another effective strategy is to directly identify the clean samples and only select them to train the models~\cite{han2018co, yu2019does, northcutt2019confident, pleiss2020identifying, li2020dividemix, wei2020combating}. Other contributions in this direction include data augmentation~\cite{zhang2017mixup, liang2020simaug, cheng2020advaug}, semi-supervised learning~\cite{hendrycks2018using, vahdat2017toward, li2020dividemix, zhang2020distilling}, \etc. 

Among them, meta-learning~\cite{ren2018learning, shu2019meta, li2019learning, wang2020training} has recently emerged as an effective framework for addressing the noisy labels. These methods all learn a meta-model from clean validation examples but differ in the specific ways to correct the biased training labels. For example, L2R~\cite{ren2018learning} directly adjusts the weight for each example. MLNT~\cite{li2019learning} simulates regular training with synthetic noisy labels. MW-Net~\cite{shu2019meta} learns an explicit weighting function. MLC~\cite{wang2020training} estimates the noise transition matrix.

This paper aims at improving the training efficiency of the meta-learning models. The results show our method not only significantly reduces the training time of three recent meta-learning approaches but also improves their robustness to noisy labels on several standard benchmarks.


\noindent\textbf{Long-tailed recognition.} Long-tailed recognition has been an active research field in computer vision~\cite{chawla2002smote, drummond2003c4, han2005borderline, shen2016relay, mahajan2018exploring, yin2019feature, liu2019large, khan2017cost, cui2019class, cao2019learning, Jamal_2020_CVPR, zhou2020bbn,zhu2020inflated}. 
For example, \cite{chawla2002smote, han2005borderline} aimed to increase the number of minority classes by oversampling, while Drummond~\etal~\cite{drummond2003c4} solved this problem by reducing the number of data in majority classes. Some recent studies~\cite{shen2016relay, mahajan2018exploring} proposed to balance the number of data for each class. \cite{yin2019feature, liu2019large} applied the knowledge learned from the head classes to the tail. 
\cite{khan2017cost, cui2019class, cao2019learning, zhou2020bbn} aimed to manipulate the loss on the class-level based on the data distribution. 

Meta-learning based methods~\cite{ren2018learning, shu2019meta, Jamal_2020_CVPR} have recently achieved promising results on the long-tailed recognition task, in which the meta-model is learned to assign larger weights to the examples of the long-tailed classes. Similar to the noisy labels, meta-learning suffers from slow training speed~\cite{ren2018learning, shu2019meta, wang2020training, Jamal_2020_CVPR}. We show our method improves the efficiency and accuracy of the meta-learning methods on the long-tailed recognition task, and achieves competitive performance compared with recent strong baselines.

 

\begin{figure*}[t]
\centering
\includegraphics[width=0.94\linewidth]{./figs/framework.pdf}
\caption{Illustration of the proposed method. We propose a new Meta-Train step, named \methodname\space (\ie, the red line \textcircled{4}), which learns a gradient sampler (denoted as ) to aggregate the meta gradient for each layer. In this figure, the meta gradients from the -th and -th layers would be aggregated to compute  and used to update the meta-model .}
\vspace{-2mm}
\label{fig:framework}
\end{figure*}

\section{Preliminary on Meta-learning}\label{sec:preliminary}

In this section, we briefly introduce the preliminary on meta-learning methods~\cite{ren2018learning, shu2019meta} that learn robust deep neural networks from noisy labels by reweighting the training data.
We follow the notation in the MW-Net~\cite{shu2019meta} model using corrupted labels as an example. Alternative formulation can be found in~\cite{ren2018learning,wang2020training,vyas2020learning}.

Let  be a noisy training set of  examples, 
where  is the -th training image and  is its one-hot label over  classes.
Consider a deep neural network (DNN) as the base model  with  denoting its parameters.
Generally, we can derive the optimal parameter  by minimizing the softmax cross-entropy loss  over the training data, where  is the prediction of the DNN and  is the given label for the input image .


In the meta-learning methods~\cite{ren2018learning, shu2019meta}, there is an out-of-sample validation set , where  denote the -th example.  is the size of  and . The extra validation dataset is not always required in meta-learning. See the discussion in Section~\ref{sec:exp_sota_noisy}. 

The meta-learning method employs a meta-model (\eg, instanced by a multilayer perceptron network (MLP) with only one hidden layer~\cite{shu2019meta}) to learn a weight for each training example. Let  denote the meta-model, parametrized by , which maps a loss to a weight scalar. A meta-model can be regarded as a learnable derivation of the self-paced function in SPCL~\cite{jiang2015self}.
Let  be the loss for the -th example in . The optimal parameter  can be obtained by computing the weighted loss: 

where  is the generated weight for the -th training example.

The meta-model is optimized by minimizing the validation loss:

where  is the loss for the -th example in the validation set.


Solving Eq.~\eqref{eq:obj_classifer} and Eq.~\eqref{eq:obj_meta_model} by alternating minimization is intractable for mini-batch gradient descent. Alternatively, an online optimization method is used instead which comprises three steps: Virtual-Train, Meta-Train, and Actual-Train~\cite{wei2020combating}. 

Consider the -th iteration. Given a training mini-batch  and a validation mini-batch ,  and  stand for the number of the examples in the mini-batch. 
For the Virtual-Train, an one-step ``virtually'' updated DNN can be derived by:

where  is the learning rate for the DNN.  is the parameter of the base DNN at the current iteration. This step is called Virtual-Train because  will not be used to update the parameter of the base DNN. 

Then for the Meta-Train, with the latest , the meta-model is updated by:

Similarly,  is the learning rate for the meta-model.  is the parameter of the updated meta-model.
Notice that  is called meta gradient, which is expensive to compute. More details will be discussed in Section~\ref{sec:slow_training}. 


Finally, in the last step (Actual-Train), the updated meta-model  is used to update the base DNN model using:

where  is the weight for the -th example computed by the latest meta-model. This step is called Actual-Train because  will be used to actually update the parameter of base DNN. Therefore,  becomes the  in Eq.~\eqref{eq:virtual_train} in the -th iteration. 


\section{\methodname}
In this section, we introduce a \methodname\space(\methodnameabbre) to efficiently approximate the total meta gradients by a layer-wise meta gradient sampling procedure. 
Figure~\ref{fig:framework} presents the overall training process, where the red line indicates the proposed method. Specifically, we learn a gradient sampler to decide, whether or not, to aggregate the meta gradient for each layer.
In the following, 
we first explain how the meta gradient can be calculated in a layer-wise fashion in Section~\ref{sec:slow_training}.
Next, we detail the gradient sampler in Section~\ref{sec:LGSM} and the final objective for the meta-model in Section~\ref{sec:overall_objective}. 
The full algorithm for \methodname\space is shown in the supplementary materials. 



\subsection{Layer-wise meta gradient computation}\label{sec:slow_training}
In this section, we discuss the meta gradient computation and show how it can be calculated in a layer-wise fashion.
For notational convenience, we simplify Eq.~\eqref{eq:meta_train} as:

where  denotes the meta gradient, which has shown to be computational intensive in recent studies~\cite{ren2018learning, shu2019meta, wang2020training}. 



Without loss of generality, suppose that the base DNN has  layers denoted as , where  represents the parameter for the -th layer.

We rewrite the computation of meta gradient using the chain rule:

where  is the dot product between 
the gradient from the -th validation loss \wrt 
and the gradient from the -th training loss \wrt .
Intuitively,  can be viewed as the similarity between the -th training example and the -th validation example according to the -th layer of the base network. 
The derivation of Eq.~\eqref{eq:gradient_accumulate} can be found in the supplementary materials.


Two observations can be drawn from Eq.~\eqref{eq:gradient_accumulate}. First, it explains the slow Meta-Train step in meta-learning, \ie computing the meta gradient involves enumerating all training examples, all validation examples, and all layers. Second, it shows that the meta gradient can be calculated by first computing the gradient  within each individual layer and then aggregating the values together. This finding lays a foundation for the proposed layer-wise meta gradient approximation.



\subsection{Layer-wise gradient sampler}\label{sec:LGSM}

We propose to approximate the total meta gradient by aggregating meta gradients sampled from a few layers. We learn a gradient sampler to accumulate the meta gradient for each layer, and formulate the gradient sampler, denoted as , as follow:

The output to the gradient sampler is the discrete activation status .

The input of the gradient sampler is the average gradient  obtained from the Virtual-Train backward step.
To be more specific, suppose the gradient tensor for the convolutional kernel has the shape  where  and  are the output/input dimensions;  and  are the kernel sizes. The  operator averages the gradient tensor across all except the first dimensions while leaving the bias term unchanged. Therefore, the dimension for .
The  performs a similar operation for the fully connected layer by setting .

For efficiency, we adopt a lightweight design for the gradient sampler and implement it by two fully-connected (FC) layers: FC and FC, where the first layer FC is followed by a PReLU layer and FC by the ``Gumbel-softmax'' operator~\cite{jang2016categorical}. The hidden size of the fully connected layer is fixed to 128 for all experiments.


Applying the gradient sampler to all layers gives:
{\small

}where  is the indicator function.
As shown in Eq.~\eqref{eq:our_meta_gradient}, the meta gradient for the -th layer is accumulated only if the gradient sampler is turned on (\ie ).

Finally, we replace  in Eq.~\eqref{eq:simple_meta_train} with   to update the meta-model. 



\subsection{Training objective for meta-model}\label{sec:overall_objective}
The proposed gradient samplers are jointly optimized with the meta-model. In addition to the cross-entropy loss described in  in Eq.~\eqref{eq:meta_train}, we incorporate two auxiliary losses to facilitate learning the gradient samplers.

The first loss is designed to prevent the gradient samplers from activating too many layers. We introduce a loss  regularizing the output of the gradient samplers:

where  is the expected number of layers to be activated. 

Moreover, we add another loss (denoted as ) to facilitate learning the meta-model:

where  is the average gradient from training loss discussed in Eq.~\eqref{eq:rl}. Likewise  is the average gradient from the validation loss, \ie . This loss term  captures the prior knowledge that the distance between validation and training gradient should be close. Notice that we only compute the gradients at the last layer  for efficiency.


Finally, the total loss to update the meta-model:

where  is the standard cross-entropy loss in Eq.~\eqref{eq:meta_train}. 
 and  are hyperparameters. We will examine the effectiveness of these loss terms in the ablation study.

 
\section{Experiments}
We conduct extensive experiments on the noisy labeled data to verify the efficiency and effectiveness of our method for learning robust DNN models. Specifically, we show our method improves the efficiency and generalization performance of the meta-learning methods in Section~\ref{sec:comp_meta_methods}. Section~\ref{sec:ablation_study} presents ablation studies to verify our design choices.
Section~\ref{sec:exp_sota_noisy} compares with the state-of-the-art results on synthetic and realistic noisy labels. In addition, we also experiment on the long-tailed recognition task in Section~\ref{sec:exp_longtail}.
The implementation details and more experimental results are presented in the supplementary materials.


\begin{table}[t]
	\centering
	\footnotesize
	\begin{tabular}{|l|p{1.0cm}|p{0.4cm}p{0.4cm}p{0.4cm}|p{0.4cm}p{0.4cm}p{0.4cm}|}
	\hline
	\multirow{2}{*}{Method}  & \multirow{2}{*}{Time (ms)} & \multicolumn{3}{|c|}{CIFAR-10} & \multicolumn{3}{|c|}{CIFAR-100} \\
	\cline{3-8}
	& & 20\% & 40\% & 60\% & 20\% & 40\% & 60\% \\
	\hline
	\small{MW-Net~\cite{shu2019meta}} & 933 & 91.9 & 89.6 & 84.5 & 73.1 & 68.1 & 61.7  \\
	\hline
	\textbf{+FaMUS} & 284(\textcolor{red}{3.3x}) & 92.9 & 90.5 & 85.8 & 73.6 & 69.4 & 62.9  \\
	\hline\hline
	L2R~\cite{ren2018learning}& 839 & 90.5 & 86.9 & 82.2 & 69.3 & 62.8 & 50.8  \\
	\hline
	\textbf{+FaMUS} & 244(\textcolor{red}{3.4x}) & 91.3 & 87.6 & 82.8 & 70.7 &	65.5 & 51.6  \\
	\hline
	\end{tabular}
	\vspace{-2mm}
	\caption{Comparison with MW-Net and L2R on CIFAR-10 and CIFAR-100. Percentage numbers represent the noise rate. ``Time (ms)'' denotes the average running time per training iteration on a single NVIDIA V100 GPU. 
	}\label{tab:comp_mwn_and_l2r}
\end{table} 

\iftrue
\begin{table}[t]
	\centering
	\small
	\begin{tabular}{|l|l|cccc|}
		\hline
		Method & Time (ms) & 10\% & 20\% & 30\% & 40\% \\
		\hline
		MLC~\cite{wang2020training} & 265 & 85.23 & 84.28 & 82.10 & 79.89 \\ 
		\hline
		\textbf{+FaMUS} & 84(\textcolor{red}{3.1x}) & 87.28 & 85.00 & 82.65 & 80.41 \\
		\hline
	\end{tabular}
	\vspace{-2mm}
	\caption{Comparison with MLC on CIFAR-10 with four different noise rates: \{10\%, 20\%, 30\%, 40\%\}. ``Time (ms)'' denotes the average running time per training iteration on a single NVIDIA V100 GPU.}\label{tab:cifar10_mlc_uniform}
	\vspace{-5mm}
\end{table}
\fi 


\subsection{Comparison with meta-learning methods}\label{sec:comp_meta_methods}

This subsection shows our method improves the efficiency and generalization performance of three meta-learning methods: L2R~\cite{ren2018learning}, MW-Net~\cite{shu2019meta}, and MLC~\cite{wang2020training}. 

\textbf{Setups.} We apply our method to three meta-learning methods using their official code and train them under the same settings as reported in their papers~\cite{ren2018learning, shu2019meta, wang2020training}. This includes using the same clean validation set to learn the meta-model. The experiments are conducted on the standard CIFAR~\cite{krizhevsky2009learning} benchmarks. Following~\cite{wang2020training}, we use the \textit{symmetric} label noise in which a percentage of true labels are randomly replaced with all possible labels, and report the best peak accuracy which is the maximum accuracy on the clean test set during training. 

\textbf{Implementation details.} The proposed gradient samplers are jointly optimized with the meta-model by SGD with a momentum of 0.9. The learning rate is fixed as 0.1 throughout the training. 
 and  are both set to 0.1.  is set to 4.


Table~\ref{tab:comp_mwn_and_l2r} and Table~\ref{tab:cifar10_mlc_uniform} show the results on the CIFAR datasets, where ``Time'' column lists the average running time (in millisecond) per training iteration on a single NVIDIA V100 GPU. It shows that our method accelerates the training time of the three meta-learning methods~\cite{ren2018learning, shu2019meta, wang2020training} by at least 3 times. More importantly, our method improves their generalization performance across all noise rates. To understand the training dynamics, we compare three methods: the best baseline MW-Net~\cite{shu2019meta}, the MW-Net with our method, and the Random MW-Net in which each layer is randomly sampled to compute the meta gradient.
Figure~\ref{fig:test_acc} shows the training curves on the CIFAR-100 dataset with 60\% noise. We observe that our method (MW-Net + FaMUS) has the lowest test loss and the highest accuracy throughout the training.


\begin{figure}[t]
\begin{subfigure}{.23\textwidth}
\includegraphics[width=1.0\linewidth]{./figs/test_acc_vs_iters.pdf}
\footnotesize\caption{Top-1 Acc vs. Training Step}
\end{subfigure}
\begin{subfigure}{.23\textwidth}
\includegraphics[width=1.0\linewidth]{./figs/test_loss_vs_iters.pdf}
\footnotesize\caption{Test loss vs. Training Step}
\end{subfigure}
\vspace{-2mm}
\caption{Test curves under CIFAR-100 with 60\% noise.}
\label{fig:test_acc}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.95\linewidth]{./figs/weight_variation.pdf}
\vspace{-2mm}
\caption{Variance of the meta gradient produced by different methods during the training. All models are trained on the CIFAR-100 dataset with 60\% noise.}
\label{fig:weight_variation}
\end{figure}


\begin{figure}[t]
\centering
\begin{subfigure}{.23\textwidth}
\includegraphics[width=1.0\linewidth]{./figs/merged_clean_weight_dist_6.pdf}
\end{subfigure}
\begin{subfigure}{.23\textwidth}
\includegraphics[width=1.0\linewidth]{./figs/merged_clean_weight_dist_20.pdf}
\end{subfigure}
\vspace{-2mm}
\caption{Weight distribution over the clean examples in the \textit{6K} (left) and \textit{20K} (right) training step. All models are trained on the CIFAR-100 dataset with 60\% noise.}
\label{fig:weight_distutions}
\vspace{-6mm}
\end{figure}




We hypothesize that this benefit is related to the low variance in the meta gradient learned by our method. For example, Figure~\ref{fig:weight_variation} visualizes the variance of the meta gradient during training and shows our method yields a low-variance approximation of the meta gradient. These results suggest that \methodnameabbre\space can learn to select a small number of most informative layers to compute the meta gradient, which hence reduces the noisy learning signals in the corrupted training data. This observation agrees with the finding in~\cite{neelakantan2015adding, miller2017reducing} that reduction in gradient variance results in faster and more stable optimization.
To substantiate this hypothesis, we examine the meta-models by plotting their weight distribution on the clean examples in Figure~\ref{fig:weight_distutions}. We find that in both the early (\textit{6K} step) and late training stages (\textit{20K} step), the meta-models learned by our method tend to assign larger weights to more clean examples.
The results in Table~\ref{tab:comp_mwn_and_l2r}, Table~\ref{tab:cifar10_mlc_uniform}, and  Figure~\ref{fig:test_acc} demonstrate that our method improves the efficiency and generalization performance of the meta-learning methods. More results can be found in the Appendix C.

\begin{table}[t]
	\centering
	\small
	\begin{tabular}{|ccc|c|c|c|}
		\hline
		 &  &  & Time (ms) &CIFAR-10 & CIFAR-100 \\
		\hline
		\checkmark &  &                      & 933  & 84.5  & 61.7 \\
		\hline
		\checkmark &          & \checkmark   & 446  & 85.2 & 62.2 \\
		\hline
		\checkmark & \checkmark &            & 275  & 85.3 & 62.0   \\
		\hline
		\checkmark & \checkmark & \checkmark & 284   & 85.8 & 62.9 \\
		\hline
	\end{tabular}
	\vspace{-1mm}
	\caption{Accuracy vs. Training Time on CIFAR-10 and CIFAR-100 with 60\% noise. 
	``Time (ms)'' denotes the average running time per training iteration on a single NVIDIA V100 GPU. 
	}\label{tab:ablation_loss_function}
\end{table}
 


\begin{table}[t]
	\centering
	\small
	\begin{tabular}{|l|l|c|c|}
		\hline
		\multicolumn{2}{|c|}{Model} & Time (ms) & ACC   \\
		\hline
        \multicolumn{2}{|c|}{MW-Net~\cite{shu2019meta}} & 933 & 61.7  \\
		\hline
		\multirow{3}{*}{ Pre-specified Block} &  = 4  & 718 & 60.8  \\
                                           &  = 8  & 569 & 61.9  \\
                                           &  = 12 & 326 & 61.4  \\
        \hline
        \multirow{3}{*}{ Random Layers} &  = 4  & 764 & 61.2  \\
                                       &  = 8  & 826 & 61.6  \\
                                       &  = 16 & 899 & 62.1  \\
		\hline\hline
		\multicolumn{2}{|c|}{\textbf{FaMUS} ()} & \textbf{284} & \textbf{62.9}  \\  
		\hline
	\end{tabular}
	\vspace{-1mm}
	\caption{Comparison of sampling strategies on CIFAR-100 with 60\% noise.  is the index of the residual block.  is the number of randomly selected layers. Our method samples from about 4 layers by setting  in Eq.~\eqref{eq:l_r}. ``Time (ms)'' denotes the average running time per training iteration on a single NVIDIA V100 GPU.
	}\label{tab:ablation_diff_accumulation}
	\vspace{-5mm}
\end{table}
 

\subsection{Ablation study}\label{sec:ablation_study}
We conduct the ablation studies using the MW-Net method with the WideResNet-28-10 backbone model~\cite{zagoruyko2016wide}.

\textbf{Loss function.} Table~\ref{tab:ablation_loss_function} analyzes the impact of the auxiliary loss components in Eq.~\eqref{eq:final_loss} on the CIFAR datasets with 60\% noise. ``'' denotes the loss of the base meta-learning model (MW-Net).
We find that adding ``'' significantly reduces the training time since it limits the number of layers to be activated. 
When incorporating both ``'' and ``'', our method achieves the best result and improves both the efficiency and accuracy of the base MW-Net model.



\textbf{Sampling strategy.} To verify the design of the proposed gradient sampler, we compare with two predefined sampling strategies: \textit{pre-specified block} and \textit{random layers}. In the Pre-specified Block, we select a residual block (indexed by  and ), which consists of two convolutional layers and two batch normalization layers, to compute the meta gradients. In the Random Layers, we uniformly select  layers to compute the meta gradients.

Table~\ref{tab:ablation_diff_accumulation} shows the comparison on the CIFAR-100 dataset with 60\% noise rate. For the Pre-specified Block, we find that computing the meta gradient from the top residual block () is the most efficient way, which is about x faster than using the bottom block (). 
As for the Random Layers, the accuracy is improved as the number of layers  increases, while the running time shows a different trend.
Our method outperforms all the compared methods both in efficiency and accuracy, suggesting the necessity of the proposed gradient sampler. 

\begin{table}[t]
	\centering
	\small
	\begin{tabular}{|lr|cc|cc|}
		\hline
		\multicolumn{2}{|c|}{\multirow{2}{*}{Method}}  & \multicolumn{2}{|c|}{CIFAR-10} & \multicolumn{2}{|c|}{CIFAR-100} \\
		\cline{3-6}
		      &  & 40\% & 60\% & 40\% & 60\% \\
		\hline

        \multicolumn{2}{|l|}{Co-teaching~\cite{han2018co}} & 74.81 & 73.06 & 46.20 & 35.67 \\  
        \hline
        \multicolumn{2}{|l|}{L2R~\cite{ren2018learning}}& 86.92 & 82.24 & 62.81 & 50.81 \\
        \hline
        \multicolumn{2}{|l|}{MW-Net~\cite{shu2019meta}} & 89.60 & 84.49 & 68.11 & 61.71 \\
        \hline
        \multicolumn{2}{|l|}{MentorNet~\cite{jiang2017mentornet}} & 91.20 & 74.20 & 66.80 & 58.80 \\
        \hline
        \multicolumn{2}{|l|}{Mixup~\cite{zhang2017mixup}} & 91.50 & 86.80 & 66.80 & 58.80 \\
        \hline
\multicolumn{2}{|l|}{M-correction~\cite{arazo2019unsupervised}} & 92.80 & 90.30 & 70.10 & 59.50 \\
        \hline
\multicolumn{2}{|l|}{MentorMix~\cite{jiang2020beyond}} & 94.20 & 91.30 & 71.30 & 64.60 \\
        \hline
\multicolumn{2}{|l|}{DivideMix~\cite{li2020dividemix}} & 94.90 & 94.30 & 75.20 & 72.00 \\
		\hline\hline
        \multicolumn{2}{|l|}{\textbf{Ours}} &  \textbf{95.37} & \textbf{94.97} & \textbf{75.91} & \textbf{73.58}  \\
                   &   &  {\footnotesize 0.15} & {\footnotesize 0.11} & {\footnotesize 0.19} & {\footnotesize 0.28}  \\
		\hline
	\end{tabular}
	\vspace{-1mm}
	\caption{Comparison with the state-of-the-art on CIFAR-10 and CIFAR-100 with 40\% and 60\% noise rates.  denotes the results are reported by~\cite{jiang2020beyond}.}
	\label{tab:cifar_mwn}
	\vspace{-5mm}
\end{table}
 

\subsection{Comparison to state-of-the-art}\label{sec:exp_sota_noisy}

This subsection compares our method with the state-of-the-art robust learning methods in overcoming both \textit{synthetic} and \textit{realistic noisy labels}.

\textbf{Datasets.} For the realistic noisy labels, we employ three datasets: (mini) WebVision 1.0~\cite{li2017webvision},  Clothing1M~\cite{xiao2015learning}, and \textit{Controlled Noisy Web Labels} (CNWL)~\cite{jiang2020beyond}. \textbf{WebVision} contains 2.4 million images with noisy labels categorized into the same 1,000 classes as in the ImageNet ILSVRC12. Following the previous works~\cite{jiang2017mentornet, chen2019understanding}, we use the first 50 classes of the Google image subset as the training data. 
\textbf{Clothing1M} has 1 million noisy labeled clothing images crawled from online shopping websites. \textbf{CNWL} is a recent benchmark of controlled label noise from the web. Uniquely, it allows for comparing methods on various rates of realistic label noises. We use the Red Mini-ImageNet set~\cite{vinyals2016matching} that consists of 50K images from 100 classes for training and 5K images for testing. 

\textbf{Implementation details.} To fairly compare with the the-state-of-art, we use a subset of pseudo labeled training data as the meta-learning validation set. Inspired by~\cite{li2020dividemix}, we employ the Gaussian Mixture Model (GMM) to divide the training data into a pseudo-clean and a pseudo-noisy label set. 
By doing so, no extra clean labels nor data are used to train the meta-model. We find using the pseudo validation set notably improves the performance because the pseudo validation set is much larger than the clean validation set used in the meta-learning method~\cite{shu2019meta}. 
More discussions can be found in the Appendix E.


For experiments on CIFAR-10, CIFAR-100, and CNWL, we employ the PreAct ResNet-18~\cite{he2016identity} as the base DNN. For experiments on Clothing1M and WebVision, we use ResNet-50~\cite{he2016deep} and Inception-ResNet V2~\cite{szegedy2016inception}, respectively. 

\begin{table}[t]
	\centering
	\small
	\begin{tabular}{|l|c|c|c|c|}
		\hline
		\multirow{2}{*}{Method} & \multicolumn{2}{|c|}{WebVision} & \multicolumn{2}{|c|}{ILSVRC12}\\
		                        \cline{2-5}
		                        & top1 & top5 & top1 & top5 \\
        \hline		                        
	    F-correction~\cite{patrini2017making} & 61.12 & 82.68 & 57.39 & 82.36 \\
        \hline
        Decoupling~\cite{malach2017decoupling} & 62.54 & 84.74 & 58.26 & 82.26 \\
        \hline
        D2L~\cite{ma2018dimensionality} & 62.68 & 84.00 & 57.80 & 81.36 \\
        \hline
        MentorNet~\cite{jiang2017mentornet} & 63.00 & 81.40 & 57.80 & 79.92 \\
        \hline
        Co-teaching~\cite{han2018co} & 63.58 & 85.20  & 61.48 & 84.70 \\
        \hline
        Iterative-CV~\cite{chen2019understanding} & 65.24 & 85.34 & 61.60 & 84.98 \\
        \hline
		MW-Net~\cite{shu2019meta} & 74.52 & 88.89 & 72.60 & 88.80 \\
		\hline
        MentorMix~\cite{jiang2020beyond} & 76.00 & 90.20 & 72.90 & 91.10 \\ 
		\hline
        DivideMix~\cite{li2020dividemix} & 77.32 & 91.64 &	75.20 & 90.84 \\

		\hline\hline
        \textbf{Ours} & \textbf{79.40} & \textbf{92.80} & \textbf{77.00} & \textbf{92.76} \\
		\hline
	\end{tabular}
	\vspace{-1mm}
	\caption{Comparison with the state-of-the-art on (mini) WebVision dataset. Numbers denote top-1 (top-5) accuracy on the validation set of WebVision and ImageNet ILSVRC12.
	}\label{tab:webvision}
\end{table}
 

\begin{table}[t]
	\centering
	\small
	\begin{tabular}{|l|cccc|c|}
		\hline
		Method & 20\% & 40\% & 60\% & 80\% & Mean\\
		\hline
		Cross-entropy & 47.36 & 42.70 & 37.30 & 29.76 & 39.28 \\
		\hline
		Mixup~\cite{zhang2017mixup} & 49.10 & 46.40 & 40.58 & 33.58 & 42.41 \\
		\hline
		DivideMix~\cite{li2020dividemix} & 50.96 & 46.72 & 43.14 & 34.50 & 43.83 \\ 
		\hline
		MentorMix~\cite{jiang2020beyond} & 51.02 & 47.14 & 43.80 & 33.46 & 43.85 \\
		\hline\hline
		\textbf{Ours (FaMUS)} & \textbf{51.42} & \textbf{48.06} & \textbf{45.10} & \textbf{35.50} & \textbf{45.02} \\
		\hline
	\end{tabular}
	\vspace{-1mm}
	\caption{Results on Controlled Noisy Web Labels~\cite{jiang2020beyond}. 
	}\label{tab:cnwl}
	\vspace{-6mm}
\end{table}
 


\begin{table*}[ht]
	\centering
	\small
	\begin{tabular}{|l|c|c|c|c|c|c|c|c|}
		\hline
		\multirow{2}{*}{Method} & \multicolumn{4}{c|}{Long-Tailed CIFAR-10} & \multicolumn{4}{c|}{Long-Tailed CIFAR-100} \\
		\cline{2-9}
		 & 100 & 50 & 20 & 10 & 100 & 50 & 20 & 10 \\
		\hline
		CE loss & 70.36 & 74.81 & 82.23 & 86.39 & 38.32 & 43.85 & 51.14 & 55.71 \\
		\hline
		Focal Loss~\cite{lin2017focal} & 70.38 & 76.71 & 82.76 & 86.66 & 38.41 & 44.32 & 51.95 & 55.78 \\
		\hline
		CB Focal~\cite{cui2019class} & 74.57 & 79.27 & 84.36 & 87.49 & 39.60 & 45.32 & 52.59 & 57.99 \\
		
		\hline
		LDAM-DRW~\cite{cao2019learning}  & 77.03 & - & - & 88.16 & 44.70 & - & - & \underline{59.59} \\
		\hline
		BBN~\cite{zhou2020bbn} & 79.82 & 82.18 & - & 88.32 & 42.56 & 47.02 & - & 59.12 \\
		\hline\hline
		
		L2R~\cite{ren2018learning} with CE loss & 74.16 & 78.93 & 82.12 & 85.19 & 40.23 & 44.44 & 51.64 & 53.73 \\
 		\hline
		MW-Net~\cite{shu2019meta} with CE loss & 75.21 & 80.06 & 84.94 & 87.84 & 42.09 & 46.74 & 54.37 & 58.46 \\ \hline                       		                        
		\cite{Jamal_2020_CVPR} with CE loss  &    76.41 & 80.51 & \underline{86.46} & \underline{88.85} &  43.35 & 48.53 & 55.62 & 59.58 \\
        \hline
		\cite{Jamal_2020_CVPR} with LDAM  & \underline{80.00} & 82.34 & 84.37 & 87.40 & 44.08 & 49.16 & 52.38 & 58.00 \\
		\hline
        \textbf{MW-Net with CE loss + FaMUS} & 79.30 & \underline{83.15} & \textbf{87.15} & \textbf{89.39} & \underline{45.60}	 & \underline{49.56} & \textbf{56.22} & \textbf{60.42} \\
		\hline
		\textbf{MW-Net with LDAM loss + FaMUS} & \textbf{80.96} & \textbf{83.32} & 86.24 & 87.90 & \textbf{46.03} & \textbf{49.93} & \underline{55.95}  & 59.03 \\
		\hline
	\end{tabular}\vspace{-3mm}
	\caption{Top-1 test accuracy of ResNet-32 on the long-tailed CIFAR-10 and CIFAR-100 with four imbalanced factors . Methods in the bottom block use extra clean data. The best performance is in \textbf{bold} and the second best is \underline{underscored}.  denotes the results are reported by~\cite{cao2019learning}. }\label{tab:long_tailed_cifar}
	\vspace{-2mm}
\end{table*}
 

\textbf{Baselines.} We briefly introduce the baselines:
(1) \textbf{Co-teaching}~\cite{han2018co}, \textbf{Decoupling}~\cite{malach2017decoupling}, and \textbf{JoCoR}~\cite{wei2020combating} train two networks to improve each other.
(2) \textbf{F-correction}~\cite{patrini2017making} estimates the noise transition matrix to correct the loss function. 
(3) \textbf{D2L}~\cite{ma2018dimensionality} learns to monitor the dimensionality of subspaces and adapts the loss functions accordingly. 
(4) \textbf{Iterative-CV}~\cite{chen2019understanding} iteratively increases the number of the selected samples to train the networks. 
(5) \textbf{MentorNet}~\cite{jiang2017mentornet} is an example-weighting method based on curriculum learning. \textbf{MentorMix}~\cite{jiang2020beyond} further combines the MentorNet with the Mixup~\cite{zhang2017mixup}. 
(6) \textbf{DivideMix}~\cite{li2020dividemix} addresses the corrupted labels in a semi-supervised learning fashion.
(7) \textbf{M-correction}~\cite{arazo2019unsupervised} estimates the probability of a sample being mislabelled and then corrects the loss accordingly.





\subsubsection{Results on synthetic noisy labels}
Table~\ref{tab:cifar_mwn} shows the results on the CIFAR-10 and CIFAR-100 datasets with symmetric label noises. 
For the compared methods, we directly cite the reported numbers in their papers except for MW-Net~\cite{shu2019meta} and L2R~\cite{ren2018learning} where we report the reproduced results. For our method, we report the average and standard deviation of over three training trials using different random seeds.
The gains over baseline methods are statistically significant at the p-value level of 0.05, according to the one-tailed t-test.
These results illustrate the effectiveness of our method on the synthetic noisy labels.




\subsubsection{Results on realistic noisy labels}

Table~\ref{tab:webvision} shows the results on the WebVision dataset. As shown, our method consistently outperforms the baselines, achieving the best accuracy on the validation sets of WebVision and ImageNet. 
In particular, our method performs favorably against very recent methods such as MentorMix~\cite{jiang2020beyond} and DivideMix~\cite{li2020dividemix} in the top-1 accuracy.


Table~\ref{tab:cnwl} shows the results on the CNWL dataset. We implement several strong baselines using their official codes released on the CIFAR-100 dataset, \eg, 
MentorMix~\cite{jiang2020beyond}.
Note that in order to use their implementation, we downsample the images of the CNWL Mini-ImageNet dataset from 84x84 to 32x32. This results in new benchmark numbers to compare our baseline methods, and supplements \cite{jiang2020beyond}'s results on 32x32 images.
More details are discussed in the Appendix E.
Table~\ref{tab:cnwl} shows that our method outperforms all baseline methods on the realistic web noisy labels. The result is notable because 1) it verifies our method on the challenging CNWL dataset; 2) it demonstrates our consistent improvement across all noise rates as a useful and robust feature since the underlying noise rate is often unknown in practice.

We also apply our method on the Clothing1M dataset, and achieve 74.4\% in top-1 accuracy without using extra clean data, which is comparable to recently published methods. 
The results on the above three datasets demonstrate that our method trained with a noisy validation set is effective for addressing the realistic noisy labels.




\subsection{Long-tailed recognition task}

\label{sec:exp_longtail}

In addition to the noisy training label problem, we also evaluate our method on the long-tailed recognition task.

\textbf{Datasets and implementation details.} Four imbalanced factors  are applied on the long-tailed CIFAR-10 and CIFAR-100~\cite{cui2019class}. 
The number of training samples for each class is randomly removed by
, where  indicates the class index,  is the original number of the training samples for the -th class, and . The imbalanced factor is the ratio between the largest and the smallest class. Following~\cite{shu2019meta, Jamal_2020_CVPR}, we do not change the test set and select ten training images per class as the clean validation set. Our method is implemented on the MW-Net model~\cite{shu2019meta} with the ResNet-32 backbone~\cite{he2016deep}.


From Table~\ref{tab:long_tailed_cifar}, we find our method consistently outperforms previous meta-learning based methods~\cite{ren2018learning, shu2019meta, Jamal_2020_CVPR}. 
Moreover, our method accelerates the training of the meta-learning model MW-Net by 2.9 times.
It is noteworthy that even compared to the very recent approaches (\eg, improved L2R~\cite{Jamal_2020_CVPR}), our method still obtains a reasonable performance gain, which illustrates the effectiveness of our method on the long-tailed recognition task.
 
\section{Conclusion}
In this paper, we discuss a novel \methodname\space(\methodnameabbre) to efficiently approximate the meta gradients by a layer-wise meta gradient sampling fashion.
We empirically show that our method yields not only an accurate but also a low-variance approximation of the meta gradient. The experimental results demonstrate that \methodnameabbre\space is able to reduce two-thirds of the training time of the meta-learning methods, while achieving a better generalization performance. Our method yields the state-of-the-art performance to address the noisy label problem, and obtains competitive performance on the long-tailed recognition task.

We find meta-model training is considerably influenced by the quantity and quality of the pseudo-clean label set. Future research in this area may include improving the robustness on limited validation data or low-quality pseudo validation data, in addition to further closing the gap in training time.
 

\clearpage


{\small
\balance
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
\balance
}

\end{document}
