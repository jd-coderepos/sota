\begingroup
\setlength{\tabcolsep}{3.7pt} \renewcommand{\arraystretch}{1.1} \begin{table*}[t!]
	\centering
	\caption{AUROC scores for distinguishing in- and out-distribution data for image classification. The model is trained with CIFAR-10 dataset and evaluated on both CIFAR-10 and each OOD dataset. `' indicates our reproduced version based on official implementations to unify the backbone network and training protocols. All experiments are averaged over five runs and `' denotes the standard deviation. `FS' and `IN' stand for the number of forwarded samples to train each model and ImageNet, respectively. 
}
    	\label{table_OOD}
	{\scriptsize
\begin{tabular}{l| c c c c c c | c}
		    \hlineB{2.5}
Method & SVHN & LSUN & IN & LSUN (FIX) & IN (FIX) & CIFAR-100 & FS \\
			\hlineB{2.5}
            Cross Entropy & 84.6 & 90.9 & 87.8 & 84.3 & 85.3 & 83.5 & 5M\\
 			Cutmix~\cite{yun2019cutmix} & 75.5 & 92.5 & 92.1 & 86.2 & 84.3 & 80.9 & 5M\\
 			SLA+SD~\cite{lee2020self} & 89.1 & 90.7 & 89.8 & 82.9 & 86.0 & 83.6 & 20M \\
			Rotations~\cite{hendrycks2019using} & 96.1 & 97.3 & 96.9 & 91.0 & 91.8 & 89.1 & 25M \\
\hline
     		SupCLR~\cite{khosla2020supervised} & 97.3 & 92.8 & 91.4 & 91.6 & 90.5 & 88.6 & 70M \\
 			CSI~\cite{tack2020csi} & 96.5 & 96.3 & 96.2 & 92.1 & 92.4 & 90.5 & 280M \\
 			\hline
 			LoRot-I  & 92.6 & \underline{98.6} & \underline{98.0} & 94.4 & 93.6 & 90.1 & 5M \\
            LoRot-E  & 94.4 & \textbf{98.7} & \textbf{98.1} & 94.1 & 93.1 & \underline{90.6} & 5M \\
            \hline
 			CSI+LoRot-I & \textbf{97.7} & 98.3 & \underline{98.0} & \textbf{95.7} & \textbf{95.6} & \textbf{93.8} & 280M \\
            CSI+LoRot-E & \underline{97.5} & 98.0 & 97.8 & \underline{95.5} & \underline{95.4} & \textbf{93.8} & 280M \\
            \hlineB{2.5}
        	
		\end{tabular}
	}
\end{table*}
\section{Experiments}
\label{section:Experiments}
We first examine the robustness of LoRot in Sec.~\ref{subsection:Robustness} and validate the generalization capability in Sec.~\ref{subsection:Classification}. 
Unless otherwise mentioned, LoRot is applied to supervised baseline with cross-entropy loss. 
For baselines, we compare rotation~\cite{gidaris2018unsupervised}, the most popular pretext task in supervise domain, and previous works that adopted self-supervision in supervised domains (Rotations~\cite{hendrycks2019using}, SLA+SD~\cite{lee2020self}, and SSP~\cite{yang2020rethinking}).
We also compare SOTA methods between benchmarks and show that contrastive learning is a complementary method, not our baseline.
For other pretext tasks, we claim these are neither our baseline nor better than our baselines since our baselines are modified versions for supervised domain on top of existing pretext tasks.
Throughout this section, we use bolds and underlines to represent the best and the second best scores.
Furthermore, as LoRot is robust to , we set  to 0.1 for all experiments and further explore the effects of  in the supplementary.


\subsection{Robustness}
\label{subsection:Robustness}





\subsubsection{Out-Of-Distribution Detection}
\label{subsection:OODExperiments}
 is to assess the model's uncertainty against unknown data. 
It is essential when deploying the model in real-world systems since DNNs are vulnerable to shortcut learning~\cite{geirhos2020shortcut, nguyen2015deep, hendrycks2016baseline}.
For the experiment, we train the model with CIFAR-10 ~\cite{krizhevsky2009learning} which we call it in-distribution dataset. 
Then, we use SVHN~\cite{netzer2011reading}, resized ImageNet and LSUN~\cite{liang2017enhancing}, fixed versions of ImageNet and LSUN~\cite{tack2020csi}, and CIFAR-100~\cite{krizhevsky2009learning} as out-of-distribution datasets.
We compare our method against the previous SOTA works on OOD detection~\cite{hendrycks2019using, tack2020csi} as well as approaches that utilize a rotation technique~\cite{lee2020self} and regional modification~\cite{yun2019cutmix} to enhance the robustness.
Unlike the previous SOTA works that require huge costs either at the training~\cite{tack2020csi} or inference time~\cite{hendrycks2019using} to yield their best performance, LoRot does not require huge costs neither at the training nor inference time.
Indeed, we only take 3.6\% of training time compared to \cite{tack2020csi} and 50\% of inference time compared to \cite{hendrycks2019using} (Measured with 2 Quadro RTX 8000).
Still, we acquired significant gains on detecting OOD samples. For fair comparison, we use the ResNet18~\cite{he2016deep} following the previous SOTA work~\cite{tack2020csi} to unify the benchmarks. 


As we can see in Tab.~\ref{table_OOD}, our proposed LoRot outperforms the state-of-the-art methods on five benchmarks. 
To measure the performance of LoRot, we utilize the KL-divergence between the softmax predictions and the uniform distribution as in \cite{hendrycks2018deep, hendrycks2019using}.
However, we use the softmax predictions for SLA+SD~\cite{lee2020self} and CutMix~\cite{yun2019cutmix} as the softmax results fit better with their methods. 
The results for SupCLR~\cite{khosla2020supervised} and CSI~\cite{tack2020csi} are from its paper and we further report the performances of LoRot when applied to contrastive approach, CSI.
Interestingly, we observe that the AUROC score of CutMix~\cite{yun2019cutmix} degrades on harder benchmarks in OOD detection.
We conjecture that the label smoothing effect of CutMix could degrade their robustness to unseen samples in harder benchmarks.
In contrast, LoRot consistently improves the baselines by large margin
(including 11\%p and 4\%p improvement on LSUN(FIX) dataset to cross-entropy and CSI, respectively).
For the slightly low performance on SVHN, we think that it is because there is no difference when a small patch is rotated against a plain background of the SVHN dataset.
Thus, we believe LoRot-E is better when images are composed of a simple background and, otherwise both approaches would work fine.


























\begingroup
\setlength{\tabcolsep}{7.0pt} \renewcommand{\arraystretch}{0.8} \begin{table*}[t]
    \centering
    {
        \caption{Imbalanced classification accuracy (\%) on CIFAR-10/100. We add LoRot and other self-supervised approaches on LDAM-DRW and compare the gains. 
}
    \label{table_IM0.01}
\small
        \begin{tabular}{l | c c c | c c c}
            \hlineB{2.5}
Imbalance Ratio & 0.01 & 0.02 & 0.05 & 0.01 & 0.02 & 0.05  \\
            \hlineB{2.5}
        
            LDAM-DRW~\cite{cao2019learning} & 77.03 & 80.94 & 85.46 & 42.04 & 46.15 & 53.25\\
            \hline
            + Rot (DA) &  &  &  &  &  &  \\
            + Rot (MT) &  &  &  &  &  &  \\
            + Rot (PT) &  &  &  &  &  &  \\
            + SSP~\cite{yang2020rethinking} &  &  & - &  &  & -\\
            + SLA+SD~\cite{lee2020self} &  & - & - &  & - & - \\
            \hline
+ LoRot-I&  &  &  &  &  & \\
            + LoRot-E&  &  &  &  &  & \\
            
\hlineB{2.5}
        \end{tabular}
    }
\end{table*}
\endgroup
\subsubsection{Imbalanced Classification.}
\label{subsection:ImbExperiments}
Following \cite{cao2019learning}, we use CIFAR to design imbalanced scenarios. 
To make imbalanced set,  is multiplied to define the sample numbers for each class as  where  and  are the class index and the number of the original train set. 
 and  denote imbalance ratio and number of classes, respectively.
Then, we measure the accuracy using the original test set. 
As the baseline, we deploy LDAM-DRW~\cite{cao2019learning} and follow experimental configurations from them.
Meanwhile, to compare ours with other self-supervision techniques, 
we also report the results of Rotation~\cite{gidaris2018unsupervised}, SLA+SD~\cite{lee2020self}, and SSP~\cite{yang2020rethinking}. 
To be specific, we apply rotation in the form of DA, MT, and PT as described in Fig.~\ref{figure_CAM}. SSP~\cite{yang2020rethinking} is the method of pre-training the network with self-supervised learning.




In Tab.~\ref{table_IM0.01}, we show LoRot has clear complementary effects and consistently improves the SOTA model by a large gain of up to +4.44\%p (10.56\%) in the highly imbalanced scenario in CIFAR100.
As an analysis, the classifier might not learn the discriminative parts for specific classes only with a few examples in an imbalanced setting since the classifier has a bias towards a small number of samples for such categories.
However, LoRot alleviates this issue since LoRot complements the classifier by discovering sub-discriminative features.
More results with the fully supervised baseline are in the supplementary report.



\subsubsection{Adversarial Perturbations.}
\label{subsection:AAExperiments}
Substantial efforts were put into improving DNN's robustness~\cite{madry2017towards, athalye2018obfuscated, dong2020benchmarking} to compensate for the vulnerability against adversarial noise~\cite{szegedy2013intriguing}.
For the evaluation, we adopt the PGD training~\cite{madry2017towards} as the baseline following the settings from Rotations~\cite{hendrycks2019using}. 
We conduct experiments on CIFAR10 against  perturbations with  set to 8/255. We adversarially train the network with 10-step adversaries and use 20-step and 100-step adversaries. We set the  to 2/255 for 10, 20-step and 0.3/255 for 100-step as in \cite{madry2017towards, hendrycks2019using}.
Tab.~\ref{table_AA} shows the results of LoRot along with the rotation task under the same codebase. Using LoRot led the network to be robust with the increase in PGD attacks by large improvement compared to the baselines. Note that the tradeoff between accuracy and robustness against adversarial noise is very natural~\cite{zhang2019theoretically}.



\begingroup
\setlength{\tabcolsep}{6.0pt} \renewcommand{\arraystretch}{0.7} \begin{table}[t]
    \centering
    {
    	\caption{Classification accuracy (\%) against the adversarial attack on CIFAR10. The results show that our model outperforms the baselines in 20-step PGD and 100-step PGD with less degradation of the accuracy for the clean dataset.}
	\label{table_AA}
		\begin{tabular}{l | c c c}
		    \hlineB{2.5}
		    \multicolumn{1}{l|}{Method} & Clean & 20-step & 100-step \\
		    \hlineB{2.5}
            Baseline & 95.3 & 0.0 & 0.0 \\
            Adv. Training & 83.4 & 46.5 & 46.5 \\
+ Rotations~\cite{hendrycks2019using} & 82.8 & 49.3 & 49.2 \\
            \hline
            + LoRot-I (\textbf{Ours}) & 82.1 & \underline{52.7} & \underline{52.6}\\
            + LoRot-E (\textbf{Ours}) & 82.6 & \textbf{52.8} & \textbf{52.8}\\
            \hlineB{2.5}
		\end{tabular}
	}
\end{table}
\endgroup


\begingroup
\setlength{\tabcolsep}{6.0pt} \renewcommand{\arraystretch}{0.7} \begin{table}[t]
    \centering
    {
\caption{Top-1 and Top-5 Classification accuracy (\%) on ImageNet. Numbers in the parenthesis are the baseline accuracy.}
    \label{table_Sup_CIFAR}
        \begin{tabular}{l | c| c c}
		    \hlineB{2.5}
			Method & Backbone & Top-1 & Top-5 \\
			\hlineB{2.5}
			Baseline & ResNet50 & 76.32 & 92.95 \\
			+ Rot(DA) & ResNet50 & 76.42 & 93.06 \\
			+ Rot(MT) & ResNet50 & 76.68 & 93.10 \\
		    + Rot(SS) & ResNet50 & 76.79 & 93.16 \\
			
			SLA+SD~\cite{lee2020self} & ResNet50 &   & - \\
            \hline
			LoRot-I (\textbf{Ours}) & ResNet50 & 77.71 & 93.60 \\
			LoRot-E (\textbf{Ours}) & ResNet50 & \underline{77.72} & \underline{93.65}\\
			\hline
			Cutout~\cite{devries2017improved} & ResNet50 & 77.07 & 93.34 \\
			CutMix~\cite{yun2019cutmix} & ResNet50 & \textbf{78.60} & \textbf{94.08} \\
            \hlineB{2.5}
		\end{tabular}
	}
\end{table}
\endgroup

\subsection{Generalization Capability}
\label{subsection:Classification}
\subsubsection{Image Classification.}
To validate LoRot's benefits in terms of the generalization capability, we evaluate on ImageNet~\cite{deng2009imagenet} and CIFAR datasets.
We compare ours with rotation~\cite{gidaris2018unsupervised} in multiple forms, SLA+SD~\cite{lee2020self}, and patch-based augmentations~\cite{devries2017improved, yun2019cutmix}.
SLA+SD augments the class label by applying rotation and utilizes self-distillation to yield a similar output to ensemble results at inference time.
Tab.~\ref{table_Sup_CIFAR} shows that LoRot clearly achieves the best performance among the methods utilizing rotation. 
Furthermore, we newly spotlight the potential of self-supervision in the perspective of generalization capability in that the gap between LoRot and popular patch-based augmentation, CutMix, is less than 1\% on ImageNet while achieving robustness multifariously.














We further apply our LoRot with data augmentation techniques and contrastive learning. 
Particularly, we test with AutoAugment~\cite{cubuk2019autoaugment}, RandAugment~\cite{cubuk2020randaugment}, and Mixup~\cite{zhang2017mixup} on ImageNet~\cite{deng2009imagenet} and SupCLR~\cite{khosla2020supervised} of contrastive learning on CIFAR datasets. 
The results in Tab.~\ref{table_ImageNet_Aug} show the consistent trend of the performance gain with three data-augmentation methods without a large number of additional parameters () and extra training time ().
Interestingly, we notice that Mixup~\cite{zhang2017mixup} better fits to LoRot-I while Auto- and Rand-Augment are better with LoRot-E.
In the viewpoint of LoRot-E, we speculate that this is because Auto- and Rand-Augment provide the randomness to the grid layout which results in more diverse inputs while Mixup causes a large modification to the image when used with LoRot-E.
Note that LoRot's limitation is that it does not bring surplus benefits to CutMix~\cite{yun2019cutmix}~() since LoRot and patch-based augmentations may modify overlapped region and interrupt each other.


Contrastive Learning has achieved promising results for both unsupervised~\cite{chen2020simple, caron2020unsupervised} and supervised learning~\cite{khosla2020supervised}. 
As it is shown that relation-based and transform-based methods are complementary in PIRL~\cite{misra2020self}, we also examine it in terms of supervised domain. 
We report the performance of SupCLR~\cite{khosla2020supervised} both from its paper and our reproduced version in Tab.~\ref{table_Sup_CIFAR_complementary}. 
We first applied rotation~\cite{gidaris2018unsupervised} with two different strategies: MT and PT.
However, applying rotation with MT provoked the decline in the performance as mentioned in contrastive learning~\cite{chen2020learning} that rotation as augmentation degrades the discriminative performance. 
As is, using rotation only for self-supervised loss (PT) was not very efficient either, in that it requires twice more computational cost to yield insignificant increase.
On the contrary, LoRot benefits additive effects to contrastive learning by enriching the representation vectors that are to be pushed or pulled between other samples.


\begingroup
\setlength{\tabcolsep}{6pt} \renewcommand{\arraystretch}{0.8} \begin{table}[t]
    \centering
    {
    	\caption{Additive benefits of LoRot with augmentation methods on ImageNet classification (\%). LoRot shows a consistent trend of performance gains.}
	\label{table_ImageNet_Aug}
	    \begin{tabular}{l | c | c c | c c|  c c}
		    \hlineB{2.5}
		    \multirow{2}{*}{Method} & \multirow{2}{*}{Backbone} & \multicolumn{2}{c|}{-} & \multicolumn{2}{c|}{+LoRot-I}& \multicolumn{2}{c}{+LoRot-E} \\ & & Top-1 & Top-5& Top-1 & Top-5& Top-1 & Top-5   \\
			\hlineB{2.5}
			Mixup~\cite{zhang2017mixup} & ResNet50 &77.58 & 93.60 &\textbf{78.36} & \textbf{94.15} &\underline{78.18} & \underline{94.05}\\
			\hline
			AutoAug~\cite{cubuk2019autoaugment} & ResNet50 &77.60 & 93.80 &\underline{78.09} & \underline{93.76} &\textbf{78.22} & \textbf{93.86} \\
			\hline
			RandAug~\cite{cubuk2020randaugment} & ResNet50 &77.52 & 93.47 &\underline{78.12} & \underline{93.84} &\textbf{78.24} & \textbf{93.95} \\
\hlineB{2.5}
		\end{tabular}
	}
\end{table}
\endgroup
\begingroup
\setlength{\tabcolsep}{7pt} \renewcommand{\arraystretch}{0.7} \begin{table}[t]
    \centering
    {
    	\caption{Additive benefits of LoRot with contrastive learning on CIFAR-10/100 classification and OOD detection. `' indicates the number taken from the paper~\cite{khosla2020supervised} using batch size of 1024. The rest of the results were reproduced with batch size of 512 due to lack of GPU memory. OOD scores are measured with the trained model on CIFAR-10 and averaged over the datasets in Table.~\ref{table_OOD}. All results are averaged on three trials.}
	\label{table_Sup_CIFAR_complementary}
		\begin{tabular}{l | c c c c c}
		    \hlineB{2.5}
			Method & CIFAR10 & CIFAR100 & OOD  \\
			\hlineB{2.5}
			\textcolor{gray}{SupCLR~\cite{khosla2020supervised}} &\textcolor{gray}{96.0} & \textcolor{gray}{76.5} & \textcolor{gray}{N/A}  \\
			SupCLR~\cite{khosla2020supervised} & 95.75 & 76.52 &  96.98\\
			+ Rotation (MT) & 94.24 & 71.80 & 96.28  \\
			+ Rotation (PT) & 96.07 & 76.73 & 96.90 \\
			\hline
			+ LoRot-I & \textbf{96.79} & \textbf{78.78} & \textbf{97.95} \\
			+ LoRot-E & \underline{96.73} & \underline{78.77} & \underline{97.92}\\
\hlineB{2.5}
		\end{tabular}
	}
\end{table}
\endgroup















\subsubsection{Localization and Transfer Learning}
\label{subsection:highlevelunderstanding}
 are important criteria to evaluate the model's localization capability. 
For these experiments, we used our pretrained model yielded from Tab.~\ref{table_Sup_CIFAR}.
Briefly, for weakly supervised object localization, the model needs to localize the object when only given with class labels.
Thus, the model is required to not only find the class-descriptive clues but also understand the image.
Tab.~\ref{table_wsol} demonstrates that LoRot better guides the model to focus on salient regions. Particularly, we observe that LoRot-E, explicitly having the localization task, leads to better localization capability. For evaluation, we use CAM~\cite{zhou2016learning} following ACOL~\cite{zhang2018adversarial} and Co-mixup~\cite{kim2021co}. As ACOL searched for threshold for CAM results between 0.5 to 0.9, we report all these results. 
\begin{table}[!t]
    \centering
    \small
    \begin{minipage}[t!]{0.54\linewidth}\setlength{\tabcolsep}{3pt} \renewcommand{\arraystretch}{0.7} \centering
        	{\small
            \caption{Weakly Supervised Object Localization accuracy (\%) on ImageNet.}
        	\label{table_wsol}
            \begin{tabular}{l | c c c c c}
    		    \hlineB{2.5}
                Threshold & 0.5 & 0.6 & 0.7 & 0.8 & 0.9 \\
    			\hlineB{2.5}
    			Baseline & 46.72 & 31.55 & 14.49 & 4.22 & 1.91 \\
    			CutMix & 47.39 & 30.24 & 13.86 & 4.57 & \underline{2.03} \\
    			LoRot-I & \underline{49.73} & \underline{35.49} & \underline{17.21} & \underline{5.08} & \underline{2.03} \\
    			LoRot-E & \textbf{50.24} & \textbf{36.07} & \textbf{17.81} & \textbf{5.49} & \textbf{2.12} \\
                \hlineB{2.5}
    		\end{tabular}
        	}
    \end{minipage}\hfill \begin{minipage}[t!]{0.44\linewidth}
    \setlength{\tabcolsep}{4pt} \renewcommand{\arraystretch}{0.7} \centering
    	{\small
        \caption{AP (\%) of object detection and instance segmentation models initialized with each pretrained method.}
    	\label{table_transferlearning}
		\begin{tabular}{l | c c}
		    \hlineB{2.5}
		    Pretrained & RetinaNet & SOLOv2 \\\hline
			Baseline & 33.8 & 33.7 \\
			LoRot-I & \textbf{35.3} & \textbf{34.5} \\
			LoRot-E & \underline{35.2} & \underline{34.4} \\
            \hlineB{2.5}
		\end{tabular}
    	}
    \end{minipage}\end{table}


Object detection and instance segmentation are another tasks that require precise localization capability of the model. 
Indeed, backbones are commonly initialized with ImageNet pretrained weights to deal with the lack of labeled train data. 
Thus, we examine whether pretrained models trained with LoRot yield any benefits. 
For evaluation, we employ Retinanet~\cite{lin2017focal} and SOLOv2~\cite{wang2020solov2} for each task and use COCO 2017 dataset~\cite{lin2014microsoft} for experiments. Tab.~\ref{table_transferlearning} shows our findings: pretrained models with LoRot consistently outperform standard models. 







