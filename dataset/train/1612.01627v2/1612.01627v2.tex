

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2017}
\usepackage{times}
\usepackage{latexsym}
\usepackage{times}
\usepackage{stfloats}
\usepackage{url}
\usepackage{latexsym}
\usepackage{times}
\usepackage{float}
\usepackage{url}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{fixltx2e}
\usepackage{amsmath}
\usepackage{verbatim}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{pifont}\usepackage{amssymb}\usepackage{array}
\usepackage{fp}
\usepackage{makecell}
\usepackage{flushend}
\usepackage{cases}
\usepackage{color}
\usepackage{bbm}
\newcommand{\thickhline}{\noalign{\hrule height 1pt}}
\def\aclpaperid{37} 


\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}




\aclfinalcopy
\newcommand\BibTeX{B{\sc ib}\TeX}

\title{Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-Based Chatbots}

\author{
	Yu Wu, Wei Wu,	Chen Xing, Zhoujun Li\thanks{~~~Corresponding Author}~, Ming Zhou~~~~\\
	State Key Lab of Software Development Environment, Beihang University, Beijing, China\\
	College of Computer and Control Engineering, Nankai University, Tianjin, China\\
	~~~~Microsoft Research, Beijing, China\\
	\{wuyu,lizj\}@buaa.edu.cn \{wuwei,v-chxing,mingzhou\}@microsoft.com 
}
\date{}


\begin{document}
	\maketitle
	\begin{abstract}
		We study response selection for multi-turn conversation in retrieval-based chatbots. Existing work either concatenates utterances in context or matches a response with a highly abstract context vector finally, which may lose relationships among utterances or important contextual information. We propose a sequential matching network (SMN) to address both problems. SMN first matches a response with each utterance in the context on multiple levels of granularity, and distills important matching information from each pair as a vector with convolution and pooling operations. The vectors are then accumulated in a chronological order  through a recurrent neural network (RNN) which models relationships among utterances. The final matching score is calculated with the hidden states of the RNN. An empirical study on two public data sets shows that SMN can significantly outperform state-of-the-art methods for response selection in multi-turn conversation.
	\end{abstract}
	
	\section{Introduction}
Conversational agents include task-oriented dialog systems and non-task-oriented chatbots. Dialog systems focus on helping people complete specific tasks in vertical domains \cite{young2010hidden}, while chatbots aim to naturally and meaningfully converse with humans on open domain topics \cite{ritter2011data}. Existing work on building chatbots includes generation -based methods and retrieval-based methods. Retrieval based chatbots enjoy the advantage of informative and fluent responses, because they select a proper response for the current conversation from a repository with response selection algorithms.  While most existing work on retrieval-based chatbots studies response selection for single-turn conversation \cite{wang2013dataset} which only considers the last input message, we consider the problem in a multi-turn scenario. In a chatbot, multi-turn response selection takes a message and utterances in its previous turns as input and selects a response that is natural and relevant to the whole context.
	
\begin{table}[!t]
		
		\label{example1} \small 
		\centering	
			\scalebox{0.8}{
		\begin{tabular}{l|l}
			\hline
		   &\textbf{Context} \\ \thickhline
			utterance 1& \emph{Human}: How are you doing? \\ \hline
		utterance 2&	\emph{ChatBot}: I am going to \textbf{hold a drum class} in Shanghai. \\ &Anyone wants to join? The location is near Lujiazui.\\ \hline
utterance 3&	\emph{Human}: Interesting! Do you have coaches who\\& can help me practice \textbf{drum}? \\ \hline
		utterance 4&	\emph{ChatBot}: Of course. \\ \hline
		utterance 5&	\emph{Human}: Can I have a free first lesson?\\  \thickhline
		&	\textbf{Response Candidates} \\ \hline
		response 1&	 Sure. Have you ever played drum before? \checkmark  \\ \hline
		response 2&	 What lessons do you want? \xmark \\ \hline
			
\end{tabular}
	}
		\caption{An example of multi-turn conversation}	\vspace{-5mm}
	\end{table}
	The key to response selection lies in input-response matching. Different from single-turn conversation, multi-turn conversation requires matching between a response and a conversation context in which one needs to consider not only the matching between the response and the input message but also  matching between responses and utterances in previous turns.  The challenges of the task include (1) how to identify important information (words, phrases, and sentences) in context, which is crucial to selecting a proper response and leveraging relevant information in matching; and (2) how to model relationships among the utterances in the context. Table \ref{example1} illustrates the challenges with an example.  First, ``hold a drum class'' and ``drum'' in context are very important. Without them, one may find responses relevant to the message (i.e., the fifth utterance of the context) but nonsense in the context (e.g., ``what lessons do you want?'').  Second, the message highly depends on the second utterance in the context, and the order of the utterances matters in response selection: exchanging the third utterance and the fifth utterance may lead to different responses. Existing work, however, either ignores relationships among utterances when concatenating them together \cite{lowe2015ubuntu}, or loses important information in context in the process of converting the whole context to a vector without enough supervision from responses (e.g., by a hierarchical RNN \cite{zhou2016multi}).     
	
	We propose a sequential matching network (SMN), a new context based matching model that can tackle both challenges in an end-to-end way. The reason that existing models lose important information in the context is that they first represent the whole context as a vector and then match the context vector with a response vector. Thus, responses in these models connect with the context until the final step in matching. To avoid information loss, SMN matches a response with each utterance in the context at the beginning and encodes important information in each pair into a matching vector. The matching vectors are then accumulated in the utterances' temporal order to model their relationships. The final matching degree is computed with the accumulation of the matching vectors. Specifically, for each utterance-response pair, the model constructs a word-word similarity matrix and a sequence-sequence similarity matrix by the word embeddings and the hidden states of a recurrent neural network with gated recurrent units (GRU) \cite{chung2014empirical} respectively. The two matrices capture important matching information in the pair on a word level and a segment (word subsequence) level respectively, and the information is distilled and fused as a matching vector through an alternation of convolution and pooling operations on the matrices. By this means, important information from multiple levels of granularity in context is recognized under sufficient supervision from the response and carried into matching with minimal loss. The matching vectors are then uploaded to another GRU to form a matching score for the context and the response. The GRU accumulates the pair matching in its hidden states in the chronological order of the utterances in context. It models relationships and dependencies among the utterances in a matching fashion and has the utterance order supervise the accumulation of pair matching. The matching degree of the context and the response is computed by a logit model with the hidden states of the GRU. SMN extends the powerful ``2D'' matching paradigm in text pair matching for single-turn conversation to context based matching for multi-turn conversation, and enjoys the advantage of both important information in utterance-response pairs and relationships among utterances being sufficiently preserved and leveraged in matching. 
	
	
	We test our model on the Ubuntu dialogue corpus \cite{lowe2015ubuntu} which is a large scale publicly available English data set for research in multi-turn conversation. The results show that our model can significantly outperform state-of-the-art methods, and  improvement to the best baseline model on R@1 is over \%. In addition to the Ubuntu corpus, we create a human-labeled Chinese data set, namely the Douban Conversation Corpus, and test our model on it. In contrast to the Ubuntu corpus in which data is collected from a specific domain and negative candidates are randomly sampled, conversations in this data come from the open domain, and response candidates in this data set are collected from a retrieval engine and labeled by three human judges. On this data, our model improves the best baseline model by \%  on R@1 and \% on P@1. As far as we know, Douban Conversation Corpus is the first human-labeled data set for multi-turn response selection and could be a good complement to the Ubuntu corpus. We have released Douban Conversation Corups and our source code at \url{https://github.com/MarkWuNLP/MultiTurnResponseSelection} 



	
	Our contributions in this paper are three-folds: (1) the proposal of a new context based matching model for multi-turn response selection in retrieval-based chatbots; (2) the publication of a large human-labeled data set to research communities; (3) empirical verification of the effectiveness of the model on public data sets.
	
	
	
	\section{Related Work}


	Recently, building a chatbot with data driven approaches \cite{ritter2011data,ji2014information} has drawn significant attention. Existing work along this line includes retrieval-based methods \cite{hu2014convolutional,ji2014information,wang2015syntax,DBLP:conf/sigir/YanSW16,DBLP:journals/corr/WuWLZ16,zhou2016multi,wu2016ranking} and generation-based methods \cite{DBLP:conf/acl/ShangLL15,serban2015building,vinyals2015neural,li2015diversity,li2016persona,xing2016topic,serban2016multiresolution}. Our work is a retrieval-based method, in which we study context-based response selection. 
	
	Early studies of retrieval-based chatbots focus on response selection for single-turn conversation \cite{wang2013dataset,ji2014information,wang2015syntax,DBLP:journals/corr/WuWLZ16}.
	Recently, researchers have begun to pay attention to multi-turn conversation. For example, Lowe et al. \shortcite{lowe2015ubuntu} match a response with the literal concatenation of context utterances. Yan et al. \shortcite{DBLP:conf/sigir/YanSW16} concatenate context utterances with the input message as reformulated queries and perform matching with a deep neural network architecture. Zhou et al. \shortcite{zhou2016multi} improve multi-turn response selection with a multi-view model including an utterance view and a word view. Our model is different in that it matches a response with each utterance at first and accumulates matching information instead of sentences by a GRU, thus useful information for matching can be sufficiently retained. 
	
	
	\section{Sequential Matching Network}
\subsection{Problem Formalization}\label{probform}
	Suppose that we have a data set , where  represents a conversation context with  as utterances.   is a response candidate and  denotes a label.  means  is a proper response for , otherwise .  Our goal is to learn a matching model  with . For any context-response pair ,  measures the matching degree between  and . 
	
	\subsection{Model Overview}
	\begin{figure*}[t]	
		\begin{center}
			\includegraphics[width=14cm,height=5.5cm]{arch2.pdf}	
		\end{center}
		\vspace{-3mm}
		\caption{Architecture of SMN}\label{fig:arch}\vspace{-3mm}
	\end{figure*}
	We propose a sequential matching network (SMN) to model . Figure \ref{fig:arch} gives the architecture. SMN first decomposes context-response matching into several utterance-response pair matching and then all pairs matching are accumulated as a context based matching through a recurrent neural network. SMN consists of three layers. The first layer matches a response candidate with each utterance in the context on a word level and a segment level, and important matching information from the two levels is distilled by convolution, pooling and encoded in a matching vector. The matching vectors are then fed into the second layer where they are accumulated in the hidden states of a recurrent neural network with GRU following the chronological order of the utterances in the context. The third layer calculates the final matching score with the hidden states of the second layer.
	
	SMN enjoys several advantages over existing models. First, a response candidate can match each utterance in the context at the very beginning, thus matching information in every utterance-response pair can be sufficiently extracted and carried to the final matching score with minimal loss. Second, information extraction from each utterance is conducted on different levels of granularity and under sufficient supervision from the response, thus semantic structures that are useful for response selection in each utterance can be well identified and extracted. Third, matching and utterance relationships are coupled rather than separately modeled, thus utterance relationships (e.g., order), as a kind of knowledge, can supervise the formation of the matching score.  
	
	By taking utterance relationships into account, SMN extends the ``2D'' matching that has proven effective in text pair matching for single-turn response selection to sequential ``2D'' matching for context based matching in response selection for multi-turn conversation. In the following sections, we will describe details of the three layers. 
	
	\subsection{Utterance-Response Matching}\label{multi-channel}
	Given an utterance  in a context  and a response candidate , the model looks up an embedding table and represents  and  as  and  respectively, where  are the embeddings of the -th word of  and  respectively. 
	  and    are then used to construct a word-word similarity matrix    and a sequence-sequence similarity matrix    which are two input channels of a convolutional neural network (CNN). The CNN distills important matching information from the matrices and encodes the information into a matching vector . 
	
	Specifically, , the -th element of  is defined by
	
	 models the matching between  and  on a word level. 
	
	To construct , we first employ a GRU to transform  and  to hidden vectors. Suppose that  are the hidden vectors of , then ,  is defined by 
	
	\small	 
	
	\normalsize	 	 
	where ,  and  are an update gate and a reset gate respectively,  is a sigmoid function, and , , , , , are parameters. Similarly, we have  as the hidden vectors of . 
	Then, ,  the -th element of  is defined by
		 
	where  is a linear transformation. , GRU models the sequential relationship and the dependency among words up to position  and encodes the text segment until the -th word to a hidden vector. Therefore,  models the matching between  and  on a segment level.    
	
	 and  are then processed by a CNN to form . , CNN regards  as an input channel, and alternates convolution and max-pooling operations. Suppose that  denotes the output of feature maps of type- on layer-, where , . On the convolution layer, we employ a 2D convolution operation with a window size , and define  as
	
	where  is a ReLU,  and  are parameters, and  is the number of feature maps on the -th layer. A max pooling operation follows a convolution operation and can be formulated as 
	
	where  and  are the width and the height of the 2D pooling respectively. The output of the final feature maps are concatenated and mapped to a low dimensional space with a linear transformation as the matching vector .
	
	According to Equation (\ref{M1Element}), (\ref{M2Element}), (\ref{Conv}), and (\ref{Pool}), we can see that by learning word embedding and parameters of GRU from training data, words or segments in an utterance that are useful for recognizing the appropriateness of a response may have high similarity with some words or segments in the response and result in high value areas in the similarity matrices. These areas will be transformed and selected by convolution and pooling operations and carry important information in the utterance to the matching vector. This is how our model identifies important information in context and leverage it in matching under the supervision of the response. We consider multiple channels because we want to capture important matching information on multiple levels of granularity of text.    
	
	\subsection{Matching Accumulation}\label{match_acum}
	Suppose that  is the output of the first layer (corresponding to  pairs), at the second layer, a GRU takes  as an input and encodes the matching sequence into its hidden states  with a detailed parameterization similar to Equation (\ref{gru}). This layer has two functions: (1) it models  the dependency and the temporal relationship of utterances in the context; (2) it leverages the temporal relationship to supervise the accumulation of the pair matching as a context based matching.  Moreover, from Equation (\ref{gru}), we can see that the reset gate (i.e., ) and the update gate (i.e., ) control how much information from the previous hidden state and the current input flows to the current hidden state, thus important matching vectors (corresponding to important utterances) can be accumulated while noise in the vectors can be filtered out. 
	
	
	\subsection{Matching Prediction and Learning}
	With , we define  as
		where  and  are parameters.
	We consider three parameterizations for : (1) only the last hidden state is used. Then . (2) the hidden states are linearly combined. Then, , where . (3) we follow \cite{yang2016hierarchical} and employ an attention mechanism to combine the hidden states. Then,  is defined as 
	
	
\begin{small}
	
	\end{small}where  and  are parameters.  and  are the -th matching vector and the final hidden state of the -th utterance respectively.  is a virtual context vector which is randomly initialized and jointly learned in training.
	
	Both (2) and (3) aim to learn weights for  from training data and highlight the effect of important matching vectors in the final matching. The difference is that weights in (2) are static, because the weights are totally determined by the positions of utterances, while weights in (3) are dynamically computed by the matching vectors and utterance vectors. We denote our model with the three parameterizations of  as SMN, SMN, and SMN, and empirically compare them in experiments.
	
	
	We learn  by minimizing cross entropy with . Let  denote the parameters of SMN, then the objective function  of learning can be formulated as  
	


	
	
	\section{Response Candidate Retrieval} \label{candidate_retrieval}
	In practice, a retrieval-based chatbot, to apply the matching approach to the response selection, one needs to retrieve a number of response candidates from an index beforehand. While candidate retrieval is not the focus of the paper, it is an important step in a real system. In this work, we exploit a heuristic method to obtain response candidates from the index. Given a message  with  utterances in its previous turns, we extract the top  keywords from  based on their tf-idf  scores\footnote{Tf is word frequency in the context, while idf is calculated using the entire index.} and expand  with the keywords. Then we send the expanded message to the index and retrieve response candidates using the inline retrieval algorithm of the index. Finally, we use  to re-rank the candidates and return the top one as a response to the context.  
	
	\section{Experiments}
	We tested our model on a publicly available English data set and a Chinese data set published with this paper.
	
	\subsection{Ubuntu Corpus}
	The English data set is the Ubuntu Corpus \cite{lowe2015ubuntu} which contains multi-turn dialogues collected from chat logs of the Ubuntu Forum. The data set consists of  million context-response pairs for training,  million pairs for validation, and  million pairs for testing. Positive responses are true responses from humans, and negative ones are randomly sampled. The ratio of the positive and the negative is 1:1 in training, and 1:9 in validation and testing. We used the copy shared by Xu et al. \shortcite{xu2016incorporating} \footnote{\url{https://www.dropbox.com/s/2fdn26rj6h9bpvl/ubuntu data.zip?dl=0}} 
	in which numbers, urls, and paths are replaced by special placeholders. We followed \cite{lowe2015ubuntu} and employed recall at position  in  candidates () as evaluation metrics.
	\subsection{Douban Conversation Corpus}
The Ubuntu Corpus is a domain specific data set, and response candidates are obtained from negative sampling without human judgment. To further verify the efficacy of our model, we created a new data set with open domain conversations, called the Douban Conversation Corpus. Response candidates in the test set of the Douban Conversation Corpus are collected following the procedure of a retrieval-based chatbot and are labeled by human judges. It simulates the real scenario of a retrieval-based chatbot. We publish it to research communities to facilitate the research of multi-turn response selection.
	
Specifically, we crawled  million dyadic dialogues (conversation between two persons) longer than  turns from Douban group\footnote{\url{https://www.douban.com/group}} which is a popular social networking service in China. We randomly sampled  million dialogues for creating a training set,  thousand dialouges for creating a validation set, and  dialogues for creating a test set, and made sure that there is no overlap between the three sets. For each dialogue in training and validation,  we took the last turn as a positive response for the previous turns as a context and randomly sampled another response from the  million data as a negative response. There are  million context-response pairs in the training set and  thousand pairs in the validation set. 
	
	To create the test set,  we  first crawled  million post-reply pairs from Sina Weibo\footnote{\url{http://weibo.com/}} which is the largest microblogging service in China and indexed the pairs with Lucene\footnote{\url{https://lucenenet.apache.org/}}. We took the last turn of each Douban dyadic dialogue in the test set as a message, retrieved  response candidates from the index following the method in Section \ref{candidate_retrieval}, and finally formed a test set with  context-response pairs. We recruited three labelers to judge 
	if a candidate is a proper response to the context. A proper response means the response can naturally reply to the message given the whole context. Each pair received three labels and the majority of the labels were taken as the final decision. Table \ref{dataset} gives the statistics of the three sets. Note that the Fleiss' kappa \cite{fleiss1971measuring} of the labeling is , which indicates that the three labelers reached a relatively high agreement.  
	
	Besides s, we also followed the convention of information retrieval and employed mean average precision (MAP) \cite{baeza1999modern}, mean reciprocal rank (MRR) \cite{voorhees1999trec}, and precision at position 1 (P@1) as evaluation metrics. We did not calculate R@1 because in Douban corpus one context could have more than one correct responses, and we have to randomly sample one for R@1, which may bring bias to evaluation. When using the labeled set, we removed conversations with all negative responses or all positive responses, as models make no difference with them. There are  context-response pairs left in the test set.  
	
	
	
	
	\begin{table}
		\small
			\scalebox{0.9}{
		\centering
		\begin{tabular}{c|c|c|c}
			\thickhline
			&train&val&test\\ \hline
			 context-response pairs &1M&50k & 10k\\ \hline
			  candidates per context & 2&2&10\\ \hline
			  positive candidates per context  &1&1& 1.18\\ \hline
			Min.   turns per context  & 3&3&3\\ \hline
			Max.   turns per context   & 98&91&45\\\hline
			Avg.  turns per context  & 6.69&6.75&6.45\\\hline
			Avg.   words per utterance &18.56&18.50 & 20.74\\
			\thickhline
		\end{tabular}}			\vspace{-2mm}
		\caption{Statistics of Douban Conversation Corpus \label{dataset}}	
		\vspace{-8mm}
	\end{table}
	\subsection{Baseline}
	\begin{table*}[t]
		\small
		\centering
		\begin{tabular}{l|c|c|c|c|c|c|c|c|c|c}
			\thickhline &   \multicolumn{4}{c|}{\textbf{Ubuntu Corpus}}    &        \multicolumn{6}{c}{\textbf{Douban Conversation Corpus}}        \\ \hline
			&  R@1      &  R@1 &  R@2&  R@5 &MAP&MRR&P@1&  R@1 & R@2  & R@5\\ \hline
TF-IDF  & 0.659 & 0.410 & 0.545 & 0.708 & 0.331 &0.359 &0.180 & 0.096&0.172& 0.405 \\ 
			RNN  & 0.768 & 0.403 & 0.547 & 0.819 & 0.390 &0.422 &0.208&0.118&0.223&0.589\\ 
			CNN & 0.848 & 0.549 & 0.684 & 0.896 & 0.417 &0.440 &0.226&0.121&0.252&0.647\\ 
LSTM & 0.901 & 0.638 & 0.784 & 0.949 & 0.485 & 0.527 &0.320&0.187&0.343&0.720\\ 
			BiLSTM & 0.895 & 0.630 & 0.780 & 0.944 &0.479&0.514&0.313&0.184&0.330&0.716\\ \hline
			Multi-View  & 0.908 & 0.662 & 0.801 & 0.951 &0.505&0.543&0.342&0.202&0.350&0.729\\ 
DL2R  & 0.899& 0.626 & 0.783 & 0.944&0.488&0.527&0.330&0.193&0.342&0.705 \\ \hline
			MV-LSTM & 0.906& 0.653 & 0.804 & 0.946& 0.498 & 0.538 & 0.348 &0.202&0.351&0.710 \\ 
			Match-LSTM & 0.904& 0.653 & 0.799 & 0.944& 0.500& 0.537 & 0.345&0.202&0.348&0.720  \\ 
			Attentive-LSTM & 0.903& 0.633 & 0.789 & 0.943& 0.495& 0.523 & 0.331&0.192&0.328&0.718  \\ 
			Multi-Channel & 0.904& 0.656 & 0.809 & 0.942& 0.506 & 0.543 & 0.349 &0.203&0.351&0.709 \\  
			Multi-Channel& 0.714& 0.368 & 0.497 & 0.745& 0.476 & 0.515 & 0.317 &0.179&0.335&0.691\\ \hline
SMN & \textbf{0.923} & \textbf{0.723} & \textbf{0.842} & \textbf{0.956} &\textbf{0.526}&\textbf{0.571}&\textbf{0.393}&\textbf{0.236}&\textbf{0.387}&0.729\\
			SMN & \textbf{0.927} & \textbf{0.725} & \textbf{0.838} & \textbf{0.962} &\textbf{0.523}&\textbf{0.572}& \textbf{0.387}&\textbf{0.228}&\textbf{0.387}&0.734\\
			SMN & \textbf{0.926} & \textbf{0.726} & \textbf{0.847} & \textbf{0.961} &\textbf{0.529}&\textbf{0.569}&\textbf{0.397}&\textbf{0.233}&\textbf{0.396}&0.724\\
			\thickhline
		\end{tabular}

		\caption{Evaluation results on the two data sets. Numbers in bold mean that the improvement is statistically significant compared with the best baseline. 
		}		\label{exp:response} \vspace{-5mm}	
	\end{table*}
	We considered the following baselines:
	
	\textbf{Basic models}: models in \cite{lowe2015ubuntu} and \cite{kadlec2015improved} including TF-IDF, RNN, CNN, LSTM and BiLSTM. 
	
	\textbf{Multi-view}: the model proposed by Zhou et al. \shortcite{zhou2016multi} that utilizes a hierarchical recurrent neural network to model utterance relationships. 

	\textbf{Deep learning to respond (DL2R)}: the model proposed by Yan et al. \shortcite{DBLP:conf/sigir/YanSW16} that reformulates the message with other utterances in the context.
	
	\textbf{Advanced single-turn matching models}: since  BiLSTM does not represent the state-of-the-art matching model, we concatenated the utterances in a context and matched the long text with a response candidate using more powerful models including MV-LSTM \cite{wan2016match} (2D matching), Match-LSTM \cite{wang2015learning}, Attentive-LSTM \cite{tan2015lstm} (two attention based models), and Multi-Channel which is described in Section \ref{multi-channel}. Multi-Channel is a simple version of our model without considering utterance relationships. We also appended the top 5 tf-idf words in context to the input message, and computed the score between the expanded message and a response with Multi-Channel, denoted as Multi-Channel.
	
	\subsection{Parameter Tuning}
	For baseline models, if their results are available in existing literature (e.g., those on the Ubuntu corpus), we just copied the numbers, otherwise we implemented the models following the settings in the literatures. All models were implemented using Theano  \cite{2016arXiv160502688short}. Word embeddings were initialized by the results of word2vec \cite{mikolov2013distributed} which ran on the training data, and the dimensionality of word vectors is . For Multi-Channel and layer one of our model, we set the dimensionality of the hidden states of GRU as . We tuned the window size of convolution and pooling in  and chose   finally. The number of feature maps is .  In layer two, we set the dimensionality of matching vectors and the hidden states of GRU as . The parameters were updated by stochastic gradient descent with Adam algorithm \cite{kingma2014adam} on a single Tesla K80 GPU. The initial learning rate is , and the parameters of Adam,  and  are  and  respectively. We employed early-stopping as a regularization strategy. Models were trained in mini-batches with a batch size of , and the maximum utterance length is . We set the maximum context length (i.e., number of utterances) as , because the performance of models does not improve on contexts longer than 10 (details are shown in the Section \ref{analysis}). We padded zeros if the  number of utterances in a context is less than , otherwise we kept the last  utterances. 
	\begin{table*}[h!] \vspace{-2mm} \small
		
		\centering
		\begin{tabular}{l|c|c|c|c|c|c|c|c|c|c}
			\thickhline &   \multicolumn{4}{c|}{\textbf{Ubuntu Corpus}}    &        \multicolumn{6}{c}{\textbf{Douban Conversation Corpus}}        \\ \hline
			&  R@1      &  R@1 &  R@2&  R@5 &MAP&MRR&P@1 &  R@1 &  R@2&  R@5  \\ \hline
Replace & 0.905 & 0.661 & 0.799 & 0.950 & 0.503 &0.541 &0.343&0.201&0.364&0.729\\ 
			Replace & 0.918 & 0.716 & 0.832 & 0.954 & 0.522&0.565 &0.376&0.220&0.385&0.727\\  \hline
			Only  & 0.919 & 0.704 & 0.832 & 0.955 & 0.518 &0.562 &0.370&0.228&0.371&0.737\\ 
			Only & 0.921 & 0.715 & 0.836 & 0.956 & 0.521 & 0.565 &0.382&0.232&0.380&0.734\\ \hline				
			SMN & 0.923 & 0.723 & 0.842 & 0.956 &0.526&0.571&0.393&0.236&0.387&0.729\\ \hline
		\end{tabular}
		\caption{Evaluation results of model ablation. \label{exp:discuss}}	
	\end{table*}
	\begin{figure*}[!h]\vspace{-2mm}
		\centering
		\subfigure[ of  and ]{
			\label{fig:w1_u1} \includegraphics[width=4cm,height=3cm]{sim1.pdf}}
		\subfigure[ of  and ]{
			\label{fig:w1_u3} \includegraphics[width=3.8cm,height=3cm]{sim2.pdf}}
		\subfigure[Update gate]{
			\label{fig:update}
\includegraphics[width=4cm,height=3cm]{update_gate.pdf}}
\vspace{-4mm}
		\caption{Model visualization. Darker areas mean larger value.}
		\label{fig:compareall} \vspace{-4mm}
	\end{figure*}
	\vspace{-2mm}
	
	\subsection{Evaluation Results}
	Table \ref{exp:response} shows the evaluation results on the two data sets.  Our models outperform baselines greatly in terms of all metrics on both data sets, with the improvements being statistically significant (t-test with -value , except  on Douban Corpus). Even the state-of-the-art single-turn matching models perform much worse than our models. The results demonstrate that one cannot neglect utterance relationships and simply perform multi-turn response selection by concatenating utterances together. Our models achieve significant improvements over Multi-View, which justified our ``matching first'' strategy.  DL2R is worse than our models, indicating that utterance reformulation with heuristic rules is not a good method for utilizing context information. s are low on the Douban Corpus as there are multiple correct candidates for a context (e.g., if there are  correct responses, then the maximum  is ). 
	SMN is only slightly better than  SMN and SMN. The reason might be that the GRU can select useful signals from the matching sequence and accumulate them in the final state with its gate mechanism, thus the efficacy of an attention mechanism is not obvious for the task at hand.
	\vspace{-2mm} 	
	\subsection{Further Analysis}\label{analysis}
	\textbf{Visualization}: we visualize the similarity matrices and the gates of GRU in layer two using an example from the Ubuntu corpus to further clarify how our model identifies important information in the context and how it selects important matching vectors with the gate mechanism of GRU as described in Section \ref{multi-channel} and Section \ref{match_acum}. The example is \emph{ : how can unzip many rar (  for example ) files at once; : sure you can do that in bash; : okay how? : are the files all in the same directory? : yes they all are; : then the command glebihan should extract them all from/to that directory}. It is from the test set and our model successfully ranked the correct response to the top position. Due to space limitation, we only visualized ,  and  the update gate (i.e. ) in Figure \ref{fig:compareall}. We can see that in  important words including ``unzip'', ``rar'', ``files'' are recognized and carried to matching by ``command'', ``extract'', and ``directory'' in ,  while  is almost useless and thus little information is extracted from it.  is crucial to response selection and nearly all information from  and  flows to the hidden state of GRU, while other utterances are less informative and the corresponding gates are almost ``closed'' to keep the information from  and  until the final state.   
	
\textbf{Model ablation}: we investigate the effect of different parts of SMN by removing them one by one from SMN, shown in Table \ref{exp:discuss}. First, replacing the multi-channel ``2D'' matching with a neural tensor network (NTN) \cite{socher2013reasoning} (denoted as Replace) makes the performance drop dramatically. This is because NTN only matches a pair by an utterance vector and a response vector and loses important information in the pair. Together with the visualization, we can conclude that ``2D'' matching plays a key role in the ``matching first'' strategy as it captures the important matching information in each pair with minimal loss. Second, the performance drops slightly when replacing the GRU for matching accumulation with a multi-layer perceptron (denoted as Replace). This indicates that utterance relationships are useful. Finally, we left only one channel in matching and found that  is a little more powerful than  and we achieve the best results with both of them (except on  on the Douban Corpus).  
	
	\textbf{Performance across context length}: we study how our model (SMN) performs across the length of contexts. Figure \ref{length} shows the comparison on MAP in different length intervals on the Douban corpus. Our model consistently performs better than the baselines, and when contexts become longer, the gap becomes larger. The results demonstrate that our model can well capture the dependencies, especially long dependencies, among utterances in contexts.\begin{figure}[!h] \vspace{-2mm} \centering 
			\includegraphics[width=6cm,height=3.2cm]{across_length.pdf}	\vspace{-3mm}			\caption{Comparison across context length}	\label{length} \vspace{-3mm}	
		\end{figure}
	
\textbf{Maximum context length}: we investigate the influence of maximum context length for SMN. Figure  \ref{fig:max_smn_length} shows the performance of SMN on Ubuntu Corpus and Douban Corpus with respect to maximum context length. From Figure  \ref{fig:max_smn_length}, we find that performance improves significantly when the maximum context length is lower than 5, and becomes stable after the context length reaches 10. This indicates that context information is important for multi-turn response selection, and we can set the maximum context length as 10 to balance effectiveness and efficiency. 
	\begin{figure*}[h]
		\centering
		
	\subfigure[Ubuntu Corpus]{
\includegraphics[width=6cm,height=4cm]{ubuntu_max.pdf}}
		\subfigure[Douban Conversation Corpus]{
\includegraphics[width=6cm,height=4cm]{douban_max.pdf}}
		\caption{Performance of SMN across maximum context length}		\label{fig:max_smn_length}
	\end{figure*}

	\textbf{Error analysis}: although SMN outperforms baseline methods on the two data sets, there are still several problems that cannot be handled perfectly.
	
	(1) Logical consistency. SMN models the context and response on the semantic level, but pays little attention to logical consistency. This leads to several DSATs in the Douban Corpus. For example, given a context \emph{\{a: Does anyone know Newton jogging shoes? b: 100 RMB on Taobao. a: I know that. I do not want to buy it because that is a fake which is made in Qingdao ,b: Is it the only reason you do not want to buy it? \}}, SMN gives a large score to the response  \emph{\{ It is not a fake. I just worry about the date of manufacture\}}. The response is inconsistent with the context on logic, as it claims that the jogging shoes are not fake. In the future, we shall explore the logic consistency problem in retrieval-based chatbots.
	
	(2) No correct candidates after retrieval. In the experiment, we prepared 1000 contexts for testing, but only 667 contexts have correct candidates after candidate response retrieval. This indicates that there is still room for candidate retrieval components to improve, and only expanding the input message with several keywords in context may not be a perfect approach for candidate retrieval. In the future, we will consider advanced methods for retrieving candidates.
	
	\section{Conclusion and Future Work}
	We present a new context based model for multi-turn response selection in retrieval-based chatbots. Experiment results on open data sets show that the model can significantly outperform the state-of-the-art methods. Besides, we publish the first human-labeled multi-turn response selection data set to research communities. In the future, we shall study how to model logical consistency of responses and improve candidate retrieval.
	
	
	
		\section{Acknowledgment}
	We appreciate valuable comments provided by anonymous reviewers and our discussions with Zhao Yan.
	This work was supported by the National Natural Science Foundation of China (Grand Nos. 61672081, U1636211, 61370126), Beijing Advanced Innovation Center for Imaging Technology (No.BAICIT-2016001), National High Technology Research and Development Program of China (No.2015AA016004), and the Fund of the State Key Laboratory of Software Development Environment (No.SKLSDE-2015ZX-16).
\bibliographystyle{acl_natbib}
	\bibliography{acl2017}


\end{document}
