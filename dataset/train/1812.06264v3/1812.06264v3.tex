\begin{figure*}[t]
\centering
\footnotesize
\begin{tabular}{@{}c @{\hskip 0.05in} c @{\hskip 0.05in} c @{\hskip 0.05in} c @{\hskip 0.05in} c @{\hskip 0.05in} c@{}}

\includegraphics[width=0.16\linewidth]{assets_jpg/flow_things/lv0_0.jpg}
&
\includegraphics[width=0.16\linewidth]{assets_jpg/flow_things/lv0_1.jpg}
&
\includegraphics[width=0.16\linewidth]{assets_jpg/flow_things/lv0_2.jpg}
&
\includegraphics[width=0.16\linewidth]{assets_jpg/flow_things/lv0_3.jpg}
&
\includegraphics[width=0.16\linewidth]{assets_jpg/flow_things/lv0_4.jpg}
&
\includegraphics[width=0.16\linewidth]{assets_jpg/flow_things/lv0_5.jpg}
 \\
\includegraphics[width=0.16\linewidth]{assets_jpg/flow_things/lv1_0.jpg}
&
\includegraphics[width=0.16\linewidth]{assets_jpg/flow_things/lv1_1.jpg}
&
\includegraphics[width=0.16\linewidth]{assets_jpg/flow_things/lv1_2.jpg}
&
\includegraphics[width=0.16\linewidth]{assets_jpg/flow_things/lv1_3.jpg}
&
\includegraphics[width=0.16\linewidth]{assets_jpg/flow_things/lv1_4.jpg}
&
\includegraphics[width=0.16\linewidth]{assets_jpg/flow_things/lv1_5.jpg}
 \\
\includegraphics[width=0.16\linewidth]{assets_jpg/flow_things/lv2_0.jpg}
&
\includegraphics[width=0.16\linewidth]{assets_jpg/flow_things/lv2_1.jpg}
&
\includegraphics[width=0.16\linewidth]{assets_jpg/flow_things/lv2_2.jpg}
&
\includegraphics[width=0.16\linewidth]{assets_jpg/flow_things/lv2_3.jpg}
&
\includegraphics[width=0.16\linewidth]{assets_jpg/flow_things/lv2_4.jpg}
&
\includegraphics[width=0.16\linewidth]{assets_jpg/flow_things/lv2_5.jpg}
 \\
\includegraphics[width=0.16\linewidth]{assets_jpg/flow_things/lv3_0.jpg}
&
\includegraphics[width=0.16\linewidth]{assets_jpg/flow_things/lv3_1.jpg}
&
\includegraphics[width=0.16\linewidth]{assets_jpg/flow_things/lv3_2.jpg}
&
\includegraphics[width=0.16\linewidth]{assets_jpg/flow_things/lv3_3.jpg}
&
\includegraphics[width=0.16\linewidth]{assets_jpg/flow_things/lv3_4.jpg}
&
\includegraphics[width=0.16\linewidth]{assets_jpg/flow_things/lv3_5.jpg}
 \\
\includegraphics[width=0.16\linewidth]{assets_jpg/flow_things/lv4_0.jpg}
&
\includegraphics[width=0.16\linewidth]{assets_jpg/flow_things/lv4_1.jpg}
&
\includegraphics[width=0.16\linewidth]{assets_jpg/flow_things/lv4_2.jpg}
&
\includegraphics[width=0.16\linewidth]{assets_jpg/flow_things/lv4_3.jpg}
&
\includegraphics[width=0.16\linewidth]{assets_jpg/flow_things/lv4_4.jpg}
&
\includegraphics[width=0.16\linewidth]{assets_jpg/flow_things/lv4_5.jpg}
 \\
Image 1 & Image 2 & Ground Truth & Prediction & Error Map & Confidence Map
\end{tabular}
\caption{Qualitative multi-scale flow result on the validation set of FlyingThings3D dataset. Bilinearly downsampled raw images, coarser level flows, error maps and confidence maps are enlarged via nearest neighbor upsampling for visualization purpose. Our network gives precise predictions in most regions, while occasionally presents confusion in occluded regions and disappearing parts.}
\label{fig::things_flow}
\end{figure*}

\modelname provides hierarchically decomposed match densities. It can be used for different tasks, such as stereo matching and optical flow. The probability of point estimates can be used as uncertainty estimation. 
It is hard to evaluate the quality of the learned distribution directly, but we can investigate its performance being applied to these specific tasks.

\subsection{Implementation Details}
\paragraph{Network Variants.}
We can apply our models to stereo matching and optical flow. 
The networks are called \stereomodelname and \flowmodelname.
The two variants differ slightly: we adopt 1D correlation for \stereomodelname and 2D correlation for \flowmodelname. 
The correlation range is always 4 for both tasks at different levels, which is consistent with the size of match density support. 
Since we treat stereo matching as 1D flow estimation, we clip the positive values in converted point estimates at each level for \stereomodelname. 
The pyramid level is set to  for \flowmodelname and  for \stereomodelname based on experiment results.

\paragraph{Module Details.}
We select DLA-34-Up~\cite{yu2018deep} as our pyramid feature extractor, because it can achieve competitive semantic segmentation accuracy on small datasets with much less computation than the deeper alternatives.
The features at the coarsest level are  downsampled.
The density decoder  consists of two residual blocks plus one aggregation node~\cite{he2016identity, yu2018deep}, except for the last level when it is fulfilled via a dilated convolutional network~\cite{yu2015multi} as a context module.
We adopt batch normalization~\cite{ioffe2015batch} in all of our models to stabilize the training. 
Predictions are upsampled from the lowest level with highest output resolution to full resolution during evaluation.

\paragraph{Training Details.}
We train our models on  GPUs without synchronized batch normalization.
The weights of pyramid feature extractor are initialized from the ImageNet pretrained model.
The network is optimized by Adam~\cite{kingma2014adam}, where , . 
For all of our pretraining experiments on synthetic datasets, models are trained for  epochs, and the learning rate is decayed by  every  epochs after  epochs for  times in total.
As for data augmentation, besides random cropping, we adopt random resizing and color perturbation~\cite{Meister:2018:UUL} during the fine-tuning stage, and introduce random flipping for optical flow experiments. 
The dense and sparse annotations, as supervision at different scales, are bilinearly downsampled and average pooled from the ground-truth map respectively. 
In this section, unless otherwise stated, confidence maps are obtained through aggregating the probabilities within  of the last level match density, and uncertainty maps are the opposite.

\begin{table}[tbh]
\small
\centering
\setlength{\tabcolsep}{1pt}
\begin{tabular*}{1.0\linewidth}{r | c  c | c  c  c | p{0.8cm}<{\centering} }
\toprule
& \multicolumn{2}{c|}{KITTI 2012} &  \multicolumn{3}{c|}{KITTI 2015} & Time \\
\midrule
Methods & Out-Noc & Out-All & D1-bg & D1-fg & D1-all & (s) \\
\midrule
SPS-st~\cite{sps} & 3.39 & 4.41 & 3.84 & 12.67 & 5.31 & 2.00 \\
Displets v2~\cite{displet} & 2.37 & 3.09 & 3.00 & 5.56 & 3.43 & 265 \\
MC-CNN-acrt~\cite{mccnn} & 2.43 & 3.63 & 2.89 & 8.88 & 3.88 & 67.0 \\
SGM-Net~\cite{sgmnet} & 2.29 & 3.50 & 2.66 & 8.64 & 3.66 & 67.0 \\
L-ResMatch~\cite{lresmatch} & 2.27 & 3.40 & 2.72 & 6.95 & 3.42 & 48.0\\
GC-Net~\cite{gcnet} & 1.77 & 2.30 & 2.21 & 6.16 & 2.87 & 0.90 \\
EdgeStereo~\cite{edgestereo} & 1.73 & 2.18 & 2.27 & 4.18 & 2.59 & 0.27 \\
PDSNet~\cite{tulyakov2018practical} & 1.92 & 2.53 & 2.29 & 4.05 & 2.58 & 0.50 \\
PSMNet~\cite{chang2018pyramid} & 1.49 & 1.89 & 1.86 & 4.62 & 2.32 & 0.41 \\
SegStereo~\cite{segstereo} & 1.68 & 2.03 & 1.88 & 4.07 & 2.25 & 0.60 \\
\midrule
\stereomodelname~(Ours) & \bf{1.40} & \bf{1.80} & \bf{1.70} & \bf{3.63} & \bf{2.02} & \bf{0.14} \\
\bottomrule
\end{tabular*}
\vspace{0.5ex}
\caption{Stereo matching results on KITTI test set. All of the numbers denote percentages of disparity outliers. The official leaderboard ranks methods according to ``Out-Noc'' for KITTI 2012 and ``D1-all'' for KITTI 2015.}
\vspace{-4ex}
\label{tab:stereo_kitti}
\end{table}

\begin{figure}[t]
\centering
\footnotesize
\begin{tabular}{@{}c @{\hskip 0.025in} c @{\hskip 0.025in} c @{\hskip 0.025in} c@{}}

\includegraphics[width=0.4\linewidth]{assets_jpg/kitti_stereo/img-7.jpg}
&
\includegraphics[width=0.4\linewidth]{assets_jpg/kitti_stereo/img-1.jpg}
&
\includegraphics[width=0.137\linewidth]{assets_jpg/kitti_stereo/img-5.jpg}
&
\rotatebox{90}{GCNet}
 \\
\includegraphics[width=0.4\linewidth]{assets_jpg/kitti_stereo/img-3.jpg}
&
\includegraphics[width=0.4\linewidth]{assets_jpg/kitti_stereo/img-4.jpg}
&
\includegraphics[width=0.137\linewidth]{assets_jpg/kitti_stereo/img-6.jpg}
&
\rotatebox{90}{~~Ours}
 \\  \hline \vspace{-2.3mm} \\
\includegraphics[width=0.4\linewidth]{assets_jpg/kitti_stereo/img-12.jpg}
&
\includegraphics[width=0.4\linewidth]{assets_jpg/kitti_stereo/img-8.jpg}
&
\includegraphics[width=0.137\linewidth]{assets_jpg/kitti_stereo/img-10.jpg}
&
\rotatebox{90}{GCNet}
 \\
 \includegraphics[width=0.4\linewidth]{assets_jpg/kitti_stereo/img-2.jpg}
&
\includegraphics[width=0.4\linewidth]{assets_jpg/kitti_stereo/img-9.jpg}
&
\includegraphics[width=0.137\linewidth]{assets_jpg/kitti_stereo/img-11.jpg}
&
\rotatebox{90}{~~Ours}
 \\  \hline \vspace{-2.3mm} \\
\includegraphics[width=0.4\linewidth]{assets_jpg/kitti_stereo/img-16.jpg}
&
\includegraphics[width=0.4\linewidth]{assets_jpg/kitti_stereo/img-14.jpg}
&
\includegraphics[width=0.137\linewidth]{assets_jpg/kitti_stereo/img-17.jpg}
&
\rotatebox{90}{GCNet}
 \\
\includegraphics[width=0.4\linewidth]{assets_jpg/kitti_stereo/img-13.jpg}
&
\includegraphics[width=0.4\linewidth]{assets_jpg/kitti_stereo/img-15.jpg}
&
\includegraphics[width=0.137\linewidth]{assets_jpg/kitti_stereo/img-18.jpg}
&
\rotatebox{90}{~~Ours}
 \\
\end{tabular}
\vspace{-1ex}
\caption{Example stereo error maps on KITTI 2015 test set. We contrast our method with GC-Net~\cite{gcnet}. Orange corresponds to erroneous prediction. This figure is best viewed in color.}
\label{fig::kitti_stereo_2015}
\end{figure}

\subsection{Stereo Matching}
To evaluate the performance of our \stereomodelname model, we benchmark our result on the KITTI stereo dataset~\cite{geiger2013vision}. Due to the limited amount of training data in KITTI, we pretrain our model on the FlyingThings3D dataset~\cite{dispnet}.

\minisection{FlyingThings3D.} 
We use the FlyingThings3D dataset as training data. 
Following the training protocol of the original FlowNet2 model~\cite{ilg2017flownet}, we use a subset of the dataset which omits some extremely hard samples.
We train our model with a batch size of  and an initial learning rate of . 
The image crop size is .
Qualitative examples, as well as the confidence maps, are shown in Fig.~\ref{fig::things_stereo}. We find low confidence correlates well with prediction errors, which generally occurs at boundaries and occlusions.

\begin{table}[tbh]
\small
\centering
\setlength{\tabcolsep}{5pt}
\begin{tabular*}{1.0\linewidth}{r | c  c | c  c | c}
\toprule
& \multicolumn{2}{c|}{Training} &  \multicolumn{2}{c|}{Test} & Time \\
\midrule
Methods & Clean & Final & Clean & Final & (s) \\
\midrule
PatchBatch~\cite{patchbatch} & - & - & 5.79 & 6.78 & 50.0 \\
EpicFlow~\cite{epic} & - & - & 4.12 & 6.29 & 15.0 \\
CPM-flow~\cite{cpm} & - & - & 3.56 & 5.96 & 4.30 \\
FullFlow~\cite{full} & - & 3.60 & \bf{2.71} & 5.90 & 240 \\
FlowFields~\cite{bailer2015flow} & - & - & 3.75 & 5.81 & 28.0 \\
MRFlow~\cite{mrflow} & 1.83 & 3.59 & 2.53 & 5.38 & 480 \\
FlowFieldsCNN~\cite{flowfieldscnn} & - & - & 3.78 & 5.36 & 23.0 \\
DCFlow~\cite{xu2017accurate} & - & - & 3.54 & 5.12 & 8.60 \\
SpyNet-ft~\cite{ranjan2017optical} & (3.17) & (4.32) & 6.64 & 8.36 & 0.16 \\
FlowNet2~\cite{ilg2017flownet} & 2.02 & 3.14 & 3.96 & 6.02 & 0.12 \\
FlowNet2-ft~\cite{ilg2017flownet} & \bf{(1.45)} & (2.01) & 4.16 & 5.74 & 0.12 \\
LiteFlowNet~\cite{hui2018liteflownet} & 2.52 & 4.05 & - & - & 0.09 \\
LiteFlowNet-ft~\cite{hui2018liteflownet} & (1.64) & (2.23) & 4.86 & 6.09 & 0.09 \\
PWC-Net~\cite{sun2018pwc} & 2.55 & 3.93 & - & - & \bf{0.03} \\
PWC-Net-ft~\cite{sun2018pwc} & (2.02) & (2.08) & 4.39 & 5.04 & \bf{0.03} \\
\midrule
\flowmodelname (Ours) & 3.84 & 8.77 & - & - & 0.08 \\
\flowmodelname-ft (Ours) & (1.70) & \bf{(1.17)} & 4.79 & \bf{4.67} & 0.08 \\
\bottomrule
\end{tabular*}
\vspace{0.5ex}
\caption{Average EPE results on MPI Sintel dataset. ``-ft'' means finetuning on the Sintel training set and numbers in the parenthesis are results on data the method has been trained on.}
\label{tab:flow_sintel}
\end{table}

\minisection{KITTI.} During fine-tuning stage, we leverage the available  image pairs from KITTI 2012 \& 2015 as training data. 
Training is performed for  epochs, with batch size  and image crop size . 
The initial learning rate is  and decayed by  at the  and the  epoch. 

As shown in Tab.~\ref{tab:stereo_kitti}, our method achieves the lowest percentages of disparity outliers in both non-occluded (Out-Noc) and total regions (Out-All), background (D1-bg) and foreground regions (D1-fg), among all of the competitive baselines on both datasets. 
We also hold the lowest inference time for processing a standard KITTI stereo pair. Note that we do not leverage the entire Scene Flow dataset~\cite{dispnet} for training as~\cite{chang2018pyramid, gcnet}, nor do we utilize additional semantic or edge cues as in~\cite{edgestereo, segstereo}.
Qualitative comparisons are shown in Fig.~\ref{fig::kitti_stereo_2015}. Our method exhibits better performance in regions with complex and ambiguous textures. This indicates the effectiveness of hierarchical match density learning based on pyramid feature representations, which exhibits robustness to local noise. 

\subsection{Optical Flow}
We pretrain our \flowmodelname on synthetic data from FlyingChairs~\cite{dosovitskiy2015flownet} and FlyingThings3D~\cite{ilg2017flownet}, then investigate the effectiveness of our model on established optical flow benchmarks including MPI Sintel~\cite{sintel} and KITTI~\cite{geiger2013vision}.

\minisection{FlyingChairs.} We train our network on FlyingChairs with batch size  and initial learning rate . Images are randomly resized and cropped to  patches. We find larger crop size can improve the network performance.

\begin{table}[tbh]
\small
\centering
\setlength{\tabcolsep}{1pt}
\begin{tabular*}{1.0\linewidth}{r | c  c  c | c  c  c}
\toprule
& \multicolumn{3}{c|}{KITTI 2012} &  \multicolumn{3}{c}{KITTI 2015} \\
\midrule
Methods & AEPE & AEPE & F1-Noc & AEPE & F1-all & F1-all\\
 & \emph{train} & \emph{test} & \emph{test} & \emph{train} & \emph{train} & \emph{test} \\
\midrule
EpicFlow~\cite{epic} & - & 3.8 & 7.88\%  &  - & - & 26.29\% \\
FullFlow~\cite{full} & - & - & - & - & - & 23.37\% \\
PatchBatch~\cite{patchbatch} & - & 3.3 & 5.29\% & - & - & 21.07\% \\
FlowFields~\cite{bailer2015flow} & - & - & - & - & - & 19.80\% \\
DCFlow~\cite{xu2017accurate} & - & - & - & - & 15.09\% & 14.83\% \\
MirrorFlow~\cite{mirror} & - & 2.6 & 4.38\% & - & 9.93\% & 10.29\% \\
PRSM~\cite{prsm} & - & \bf{1.0} & 2.46\% & - & - & 6.68\% \\
SpyNet-ft~\cite{ranjan2017optical} & (4.13) & 4.7 & 12.31\% & - & - & 35.07\% \\
FlowNet2~\cite{ilg2017flownet} & 4.09 & - & - & 10.06 & 30.37\% & - \\
FlowNet2-ft~\cite{ilg2017flownet} & (1.28) & 1.8 & 4.82\% & (2.30) & (8.61\%) & 10.41\% \\
LiteFlowNet~\cite{hui2018liteflownet} & 4.25 & - & - & 10.46 & 29.30\% & -\\
LiteFlowNet~\cite{hui2018liteflownet} & (1.26) & 1.7 & - & (2.16) & (8.16\%) & 10.24\% \\
PWC-Net~\cite{sun2018pwc} & 4.14 & - & - & 10.35 & 33.67\% & - \\
PWC-Net-ft~\cite{sun2018pwc} & (1.45) & 1.7 & 4.22\% & (2.16) & (9.80\%) & 9.60\% \\
\midrule
\flowmodelname (Ours) & 4.65 & - & - & 13.17 & 23.99\% & - \\
\flowmodelname-ft (Ours) & \bf{(0.81)} & 1.4 & \bf{2.26\%} & \bf{(1.31)} & \bf{(4.10\%)} & \bf{6.55\%} \\
\bottomrule
\end{tabular*}
\vspace{0.5ex}
\caption{Optical flow results on KITTI dataset. ``-ft'' means fine-tuning on the KITTI training set. Numbers in parenthesis are results on data the network has been trained on.}
\label{tab:flow_kitti}
\end{table}

\begin{figure}[t]
\centering
\footnotesize
\begin{tabular}{@{}c @{\hskip 0.025in} c @{\hskip 0.025in} c @{\hskip 0.025in} c@{}}

\includegraphics[width=0.4\linewidth]{assets_jpg/kitti_flow/img/img-3.jpg}
&
\includegraphics[width=0.4\linewidth]{assets_jpg/kitti_flow/img/img-1.jpg}
&
\includegraphics[width=0.139\linewidth]{assets_jpg/kitti_flow/img/img-10.jpg}
&
\rotatebox{90}{~~PWC}
 \\
\includegraphics[width=0.4\linewidth]{assets_jpg/kitti_flow/img/img-16.jpg}
&
\includegraphics[width=0.4\linewidth]{assets_jpg/kitti_flow/img/img-2.jpg}
&
\includegraphics[width=0.139\linewidth]{assets_jpg/kitti_flow/img/img-11.jpg}
&
\rotatebox{90}{~~Ours}
 \\ \hline \vspace{-2.3mm} \\ 
\includegraphics[width=0.4\linewidth]{assets_jpg/kitti_flow/img/img-6.jpg}
&
\includegraphics[width=0.4\linewidth]{assets_jpg/kitti_flow/img/img-4.jpg}
&
\includegraphics[width=0.139\linewidth]{assets_jpg/kitti_flow/img/img-12.jpg}
&
\rotatebox{90}{~~PWC}
 \\
 \includegraphics[width=0.4\linewidth]{assets_jpg/kitti_flow/img/img-18.jpg}
&
\includegraphics[width=0.4\linewidth]{assets_jpg/kitti_flow/img/img-5.jpg}
&
\includegraphics[width=0.139\linewidth]{assets_jpg/kitti_flow/img/img-13.jpg}
&
\rotatebox{90}{~~Ours}
 \\  \hline \vspace{-2.3mm} \\
\includegraphics[width=0.4\linewidth]{assets_jpg/kitti_flow/img/img-9.jpg}
&
\includegraphics[width=0.4\linewidth]{assets_jpg/kitti_flow/img/img-7.jpg}
&
\includegraphics[width=0.139\linewidth]{assets_jpg/kitti_flow/img/img-14.jpg}
&
\rotatebox{90}{~~PWC}
 \\
\includegraphics[width=0.4\linewidth]{assets_jpg/kitti_flow/img/img-17.jpg}
&
\includegraphics[width=0.4\linewidth]{assets_jpg/kitti_flow/img/img-8.jpg}
&
\includegraphics[width=0.139\linewidth]{assets_jpg/kitti_flow/img/img-15.jpg}
&
\rotatebox{90}{~~Ours}
 \\
\end{tabular}
\vspace{-1ex}
\caption{Example flow error maps on KITTI 2015 test set. We compare our method with PWC-Net~\cite{sun2018pwc}. Orange corresponds to erroneous prediction. This figure is best viewed in color.} 
\label{fig::kitti_flow_2015}
\end{figure}


\minisection{FlyingThings3D.}
We further fine-tune the model on the FlyingThings3D data, the same subset in our stereo matching experiments, with batch size , learning rate  and image crop size .
We visualize examples of multi-scale predictions in Fig.~\ref{fig::things_flow}. The results indicate that our model is able to progressively refine the prediction from coarse to fine scales. Though we adopt the discrete distribution, our model can still capture very detailed displacements.

\minisection{MPI Sintel.} Finally, we fine-tune our model on MPI Sintel~\cite{sintel} for  epochs with batch size  and image crop size . 
The initial learning rate is  and decayed by  at the  and the  epoch.  
Though the dataset provides training data of different subsets (\emph{clean} \& \emph{final} passes), we only adopt the \emph{final} pass as training data rendered with motion blur, defocus blur, and atmospheric effects. As shown in Tab.~\ref{tab:flow_sintel}, we can obtain the lowest average EPE in the \emph{final} pass, and compelling results on the \emph{clean} pass, though our model does not see the \emph{clean} pass data during training.
In the model generalization experiment, 
our pretrained \flowmodelname estimates the flow accurately near the occlusion boundary, resulting in the lowest outlier percentage on KITTI (see the ``\flowmodelname (Ours)'' entry in Tab.~\ref{tab:flow_kitti}). 
The metric of EPE emphasizes large motion error. This influence makes our pretrained \flowmodelname achieve higher EPE on MPI Sintel (see the ``\flowmodelname (Ours)'' entry in Tab.~\ref{tab:flow_sintel}). 

\minisection{KITTI.}
Alternatively, we can finetune our pretrained model on KITTI dataset. 
We follow the configurations of our stereo experiment. Tab.~\ref{tab:flow_kitti} summarize the results. 
We can obtain the lowest F1-Noc on KITTI 2012 test set and the lowest F1-all on KITTI 2015 test set. 
At the time of writing, \flowmodelname outperforms all two-frame optical flow methods by large margins on both KITTI 2012 \& 2015. 
It even surpasses some competitive scene flow methods such as PRSM~\cite{prsm}, which use additional stereo data. 
This reveals the suitability of our probabilistic method in challenging real-world cases. We show qualitative comparisons against PWC-Net in Fig.~\ref{fig::kitti_flow_2015}. Our method appear to have advantages in estimating many thin structures.

\begin{figure}[t]
\centering
\small
\begin{tabular}{@{}c @{\hskip 0.04in} c@{}}
\multicolumn{2}{c}{\footnotesize Input Image}  \\
\includegraphics[width=0.49\linewidth]{assets_jpg/kitti_confidence/img1_new.jpg}
&
\includegraphics[width=0.49\linewidth]{assets_jpg/kitti_confidence/img2_new.jpg}
 \\
\multicolumn{2}{c}{\footnotesize Confidence Map} \\
\includegraphics[width=0.49\linewidth]{assets_jpg/kitti_confidence/conf_1_new.jpg}
&
\includegraphics[width=0.49\linewidth]{assets_jpg/kitti_confidence/conf_2_new.jpg}
 \\
\multicolumn{2}{c}{\footnotesize Error Map} \\
\includegraphics[width=0.49\linewidth]{assets_jpg/kitti_confidence/error_1_new.jpg}
&
\includegraphics[width=0.49\linewidth]{assets_jpg/kitti_confidence/error_2_new.jpg}
\end{tabular}
\vspace{-1ex}
\caption{Example confidence maps of our predictions and error maps \wrt ground-truth. In confidence maps, white colors mean confident predictions while dark colors denote uncertain ones. In the error maps, warmer colors indicate inaccurate predictions.}
\label{fig::uncertain}
\end{figure}

\subsection{Uncertainty Estimation}
We also conduct quantitative analysis of uncertainty estimation. 
We compute the log likelihoods of our network predictions and compare \flowmodelname with probabilistic flow networks~\cite{gast2018lightweight}. 
FlowNetDropOut uses variational Gaussian dropout layers~\cite{kingma2015variational}. While FlowNetProbOut replaces deterministic outputs with probabilistic output layers. FlowNetADF propagates uncertainty through the entire network using ADF. 
During the evaluation, we recover the full match density through composing the multi-scale match densities. 
This can be achieved through iteratively sampling from coarse to fine, and we assume a discrete non-uniform distribution for sampling outside  (see Sec.~\ref{sec::convert}). 
As shown in Tab.~\ref{tab:flow_uncertainty}, \flowmodelname achieves the best average log likelihoods against all of the baselines.

\begin{table}[tbh]
\small
\centering
\setlength{\tabcolsep}{5pt}
\begin{tabular*}{1.0\linewidth}{r | c c c }
\toprule
Methods & Sintel clean & Sintel final & Chairs \\
\midrule
FlowNetDropOut~\cite{gast2018lightweight} & -7.106 & -10.820 & -6.176 \\
FlowNetProbOut~\cite{gast2018lightweight} & -6.888 & -7.621 & -3.591 \\
FlowNetADF~\cite{gast2018lightweight} & -3.878 & -4.186 & -3.348 \\
\midrule
HDF(Ours) & \bf{-1.487} & \bf{-1.821} & \bf{-0.872} \\
\bottomrule
\end{tabular*}
\caption{Average log likelihoods of probabilistic flow methods on MPI Sintel training set and FlyingChairs test set.}
\vspace{-1ex}
\label{tab:flow_uncertainty}
\end{table}

\begin{table}[tbh]
\small
\centering
\setlength{\tabcolsep}{8.5pt}
\begin{tabular*}{1.0\linewidth}{c | c | c  c | c  c}
\toprule
\multirow{2}{*}{Classes} & \multirow{2}{*}{Methods} & \multicolumn{2}{c|}{Noc} &  \multicolumn{2}{c}{All} \\
 & & IoU & Acc & IoU & Acc\\
\midrule
\multirow{2}{*}{Outlier} & Consistency & 17.5 & \bf{64.9} & 23.3 & \bf{81.9} \\
& Ours & \bf{37.6} & 57.8 & \bf{44.1} & 76.4 \\
\midrule
\multirow{2}{*}{Inlier}& Consistency & 84.2 & 85.8 & 75.6 & 76.9 \\
& Ours & \bf{96.1} & \bf{97.8} & \bf{91.8} & \bf{93.7} \\
\midrule
\multirow{2}{*}{Mean}& Consistency & 50.9 & 75.4 & 49.5 & 79.4 \\
& Ours & \bf{66.9} & \bf{77.8} & \bf{68.0} & \bf{85.1} \\
\bottomrule
\end{tabular*}
\vspace{0.5ex}
\caption{Classification result of inlier and outlier predictions on KITTI 2015 training set. Noc denotes evaluation only in the non-occluded area, while All denotes evaluation in the overall region.}
\vspace{-1ex}
\label{tab:uncer_error}
\end{table}

Furthermore, we measure the reliability of network prediction based on uncertainty. We treat predictions with uncertainty greater than a certain threshold () as outliers and compare such criterion with the forward-backward consistency check~\cite{yin2018geonet} which is popularly adopted for point estimate. Both methods use the same \flowmodelname model for inference. As shown in Tab.~\ref{tab:uncer_error}, our uncertainty estimation gives the highest mean IoU and mean accuracy in both non-occluded and overall regions. 
Fig.~\ref{fig::uncertain} presents visualization of the confidence and error maps.
We can observe the positive correlation between our estimated uncertainty and prediction error.
