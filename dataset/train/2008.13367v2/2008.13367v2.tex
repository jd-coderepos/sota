\section{Experiments}
\paragraph{Dataset and Evaluation Metrics.}
We evaluate the VFNet on the challenging MS COCO 2017 benchmark~\cite{COCO}. Following the common practice~\cite{fasterRCNN, FCOS, retinaNet, ATSS}, we train detectors on the \texttt{train2017} split, report ablation results on the \texttt{val2017} split and compare with other detectors on the \texttt{test-dev} split by uploading the results to the evaluation server. We adopt the standard COCO-style Average Precision (AP) as the evaluation metrics. 

\vspace{-4mm}
\paragraph{Implementation and Training Details.} 
We implement the VFNet with MMDetection~\cite{mmdetection}. Unless specified, we adopt the default hyper-parameters used in MMDetection. The initial learning rate is set as 0.01 and we employ the linear warming up policy~\cite{1hour} to start the training where the warm-up ratio is set as 0.1. We use 8 V100 GPUs for training with a total batch size of 16 (2 images per GPU) in both ablation studies and performance comparison.

For ablation studies on the \texttt{val2017}, the ResNet-50~\cite{ResNet} is used as the backbone network and 1x training schedule (12 epochs)~\cite{mmdetection} is adopted. Input images are resized to a maximum scale of 1333$\times$800, without changing the aspect ratio. Only random horizontal image flipping is used for data augmentation. 

For performance comparison with the state-of-the-art on the \texttt{test-dev}, we train the VFNet with different backbone networks, including those ones with deformable convolution layers~\cite{DCN, DCNv2} (denoted as DCN) inserted. When DCN is used in the backbone, we also insert it into the last layers before the star deformable convolution in the VFNet head.  2x (24 epochs) training scheme and multi-scale training (MSTrain) are adopted, where a maximum image scale for each iteration is randomly selected from a scale range. In fact, we apply two image scale ranges in experiments. For fair comparison with the baseline, we use the scale range 1333$\times$[640:800]; out of curiosity, we also experiment with a wider scale range 1333$\times$[480:960]. Note that even MSTrain is employed, we keep the maximum image scale to 1333$\times$800 in inference, although a bigger scale performs slightly better (about 0.4 AP gain with 1333$\times$900 scale).

\vspace{-4mm}
\paragraph{Inference Details.} 
In inference, we forward the input image which is resized to a maximum scale of 1333$\times$800 through the network and obtain estimated bounding boxes with corresponding IACSs. We first filter out those bounding boxes with $p_{max} \leq$  0.05 and select at most 1k top-scoring detections per FPN level. Then, the selected detections are merged and redundant detections are removed by NMS with a threshold of 0.6 to yield the final results.

\begin{table}[t]
    \begin{center}
        \begin{tabular}{K{2.5em} K{2.5em} K{5.5em} | K{3em} K{3em} K{3em} }
            \hline
            $\gamma$ & $\alpha$ & q weighting & AP &  AP$_{50}$ &  AP$_{75}$\\
            \hline
             1.0 & 0.50 & \checkmark & 41.2  & 59.2  & 44.7  \\ 
             1.5 & 0.75 & \checkmark & 41.5  & 59.7  & 45.1  \\
             2.0 & 0.75 & \checkmark & \textbf{41.6}  & 59.5  & 45.0  \\
             2.0 & 0.75 &            & 41.2  & 59.1  & 44.4  \\
             2.5 & 1.25 & \checkmark & 41.5  & 59.4  & 45.2  \\ 
             3.0 & 1.00 & \checkmark & 41.3  & 59.0  & 44.7  \\ 
            \hline
        \end{tabular}
    \end{center}
    \vspace{-5mm}
\caption{Peformance of the VFNet when changing the hyper-parameters ($\alpha$, $\gamma$) of the varifocal loss. q weighting means weighting the loss of the positive example with the learning target q.  }
\label{table:VFL}
\vspace{-0mm}
\end{table}


\begin{table}[t]
    \begin{center}
        \begin{tabular}{ K{2.5em} K{2.5em} K{4.5em} | K{3em} K{3em} K{3em} }
            \hline
             VFL  & Star Dconv & BBox Refinement & AP &  AP$_{50}$ &  AP$_{75}$\\
            \hline
                        &   &  & 39.0 & 57.7 & 41.8 \\
             \checkmark &   &  & 40.1 & 58.5 & 43.4 \\ 
             \checkmark & \checkmark &  & 40.7  & 59.0  & 44.0  \\ 
             \checkmark & \checkmark & \checkmark & \textbf{41.6}  & 59.5  & 45.0  \\ 
            \hline
            \multicolumn{3}{c|}{FCOS+ATSS} & 39.2 & 57.3 & 42.4 \\
            \hline
        \end{tabular}
    \end{center}
    \vspace{-5mm}
\caption{Individual contribution of the components in our method. The first row represents the results of the raw VFNet trained with the focal loss~\cite{retinaNet}.}
\label{table:ablation}
\vspace{-2mm}
\end{table}

\begin{table*}[tpb]
    \begin{center}
        \begin{tabular}{ K{11.0em} | K{8.0em} |K{2.5em} | K{4.2em} K{4.2em} K{4.2em} | K{4.2em} K{4.2em} K{4.2em} } 
            \hline
             Method  & Backbone & FPS & AP &  AP$_{50}$ &  AP$_{75}$ & AP$_{S}$ & AP$_{M}$ & AP$_{L}$ \\
            \hline
             Anchor-based multi-stage: &    &  & &  &  &  &\\ 
             Faster R-CNN~\cite{fasterRCNN} & X-101 & & 40.3 & 62.7 & 44.0 & 24.4 & 43.7 & 49.8  \\
             Libra R-CNN~\cite{libraRCNN} & R-101 & & 41.1 & 62.1 & 44.7 & 23.4 & 43.7 & 52.5  \\
             Mask R-CNN~\cite{maskRCNN} & X-101 &  & 41.4 & 63.4 & 45.2 & 24.5 & 44.9 & 51.8  \\
             R-FCN~\cite{RFCN} & R-101 &  & 41.4 & 63.4 & 45.2 & 24.5 & 44.9 & 51.8  \\
             TridentNet~\cite{TridentNet} & R-101  &  & 42.7 & 63.6 & 46.5 & 23.9 & 46.6 & 56.6  \\
             Cascade R-CNN~\cite{cascadeRCNN} & R-101 &  & 42.8 & 62.1 & 46.3 & 23.7 & 45.5 & 55.2  \\
             SNIP~\cite{SNIP} & R-101 &  & 43.4 & 65.5 & 48.4 & 27.2 & 46.5 & 54.9  \\
             
             
             \hline
             Anchor-based one-stage: &    &  &  &  &  &\\
             SSD512~\cite{SSD} & R-101 &  & 31.2 & 50.4 & 33.3 & 10.2 & 34.5 & 49.8 \\
             YOLOv3~\cite{YOLOv3} & DarkNet-53 &  & 33.0 & 57.9 & 34.4 & 18.3 & 35.4 & 41.9  \\
             DSSD513~\cite{DSSD} & R-101 &  & 33.2 & 53.3 & 35.2 & 13.0 & 35.4 & 51.1 \\
             RefineDet~\cite{refineDet} & R-101 &  & 36.4 & 57.5 & 39.5 & 16.6 & 39.9 & 51.4 \\
             RetinaNet~\cite{retinaNet} & R-101 &  & 39.1 & 59.1 & 42.3 & 21.8 & 42.7 & 50.2 \\
             FreeAnchor~\cite{freeAnchor} & R-101 &  & 43.1 & 62.2 & 46.4 & 24.5 & 46.1 & 54.8 \\
             GFL~\cite{GFL} & R-101-DCN &  & 47.3 & 66.3 & 51.4 & 28.0 & 51.1 & 59.2  \\
             GFL~\cite{GFL} & X-101-32x4d-DCN &  & 48.2 & 67.4 & 52.6 & 29.2 & 51.7 & 60.2  \\
             
            EfficientDet-D6~\cite{EfficientDet} & B6 & 5.3$^{\dag}$ & 51.7 & 71.2 & 56.0 & 34.1 & 55.2 & 64.1  \\
            EfficientDet-D7~\cite{EfficientDet} & B6 & 3.8$^{\dag}$ & 52.2 & 71.4 & 56.3 & 34.8 & 55.5 & 64.6  \\
             
             \hline
             Anchor-free key-point: &    &  &  &  &  &\\ 
             ExtremeNet~\cite{extremeNet} & Hourglass-104 &  & 40.2 & 55.5 & 43.2 & 20.4 & 43.2 & 53.1  \\
             CornerNet~\cite{cornerNet} & Hourglass-104 &  & 40.5 & 56.5 & 43.1 & 19.4 & 42.7 & 53.9  \\
             Grid R-CNN~\cite{gridRCNN} & X-101 &  & 43.2 & 63.0 & 46.6 & 25.1 & 46.5 & 55.2  \\
             CenterNet~\cite{cornerNet} & Hourglass-104 &  & 44.9 & 62.4 & 48.1 & 25.6 & 47.4 & 57.4  \\
             RepPoints~\cite{repPoints} & R-101-DCN &  & 45.0 & 66.1 & 49.0 & 26.6 & 48.6 & 57.5  \\
             
             \hline
             Anchor-free one-stage: &    &  &  &  &  &\\
             FoveaBox~\cite{foveaBox} & X-101 &  & 42.1 & 61.9 & 45.2 & 24.9 & 46.8 & 55.6  \\
             FSAF~\cite{FSAF} & X-101-64x4d &  & 42.9 & 63.8 & 46.3 & 26.6 & 46.2 & 52.7  \\
             FCOS~\cite{FCOS} & R-101 &   & 43.0 & 61.7 & 46.3 & 26.0 & 46.8 & 55.0  \\
             SAPD~\cite{SAPD} & R-101 &   & 43.5 & 63.6 & 46.5 & 24.9 & 46.8 & 54.6  \\
             SAPD~\cite{SAPD} & R-101-DCN &  & 46.0 & 65.9 & 49.6 & 26.3 & 49.2 & 59.6  \\
             


             
             
             
             \hline
             Baseline: &    &  &  &  &  &\\
             ATSS~\cite{ATSS} & R-101 & 17.5 & 43.6 & 62.1 & 47.4 & 26.1 & 47.0 & 53.6 \\
             ATSS~\cite{ATSS} & X-101-64x4d & 8.9 & 45.6 & 64.6 & 49.7 & 28.5 & 48.9 & 55.6 \\
             ATSS~\cite{ATSS} & R-101-DCN & 13.7 & 46.3 & 64.7 & 50.4 & 27.7 & 49.8 & 58.4 \\
             ATSS~\cite{ATSS} & X-101-64x4d-DCN & 6.9 & 47.7 & 66.5 & 51.9 & 29.7 & 50.8 & 59.4 \\
            
            \hline
             Ours: &  &    &  &  &  &  &\\
             VFNet & R-50 & 19.3 & 44.3/44.8 & 62.5/63.1 & 48.1/48.7 & 26.7/27.2 & 47.3/48.1 & 54.3/54.8 \\
             VFNet & R-101 & 15.6 & 46.0/46.7 & 64.2/64.9 & 50.0/50.8 & 27.5/28.4 & 49.4/50.2 & 56.9/57.6 \\
             VFNet & X-101-32x4d & 13.1 & 46.7/47.6 & 65.2/66.1 & 50.8/51.8 & 28.3/29.4 & 50.1/50.9 & 57.3/58.4 \\
             VFNet & X-101-64x4d & 9.2 & 47.4/48.5 & 65.8/67.0 & 51.5/52.6 & 29.5/30.1 & 50.7/51.7 & 58.1/59.7 \\
             VFNet & R2-101~\cite{Res2Net} & 13.0 & 48.4/49.3 & 66.9/67.6 & 52.6/53.5 & 30.3/30.5 & 52.0/53.1 & 59.2/60.5 \\
             VFNet & R-50-DCN & 16.3 & 47.3/48.0 & 65.6/66.4 & 51.4/52.3 & 28.4/29.0 & 50.3/51.2 & 59.4/60.4 \\
             VFNet & R-101-DCN & 12.6 & 48.4/49.2 & 66.7/67.5 & 52.6/53.7 & 28.9/29.7 & 51.7/52.6 & 61.0/62.4 \\
             VFNet & X-101-32x4d-DCN & 10.1 & 49.2/50.0 & 67.8/68.5 & 53.6/54.4 & 30.0/30.4 & 52.6/53.2 & 62.1/62.9 \\
             VFNet & X-101-64x4d-DCN & 6.7 & 49.9/50.8 & 68.5/69.3 & 54.3/55.3 & 30.7/31.6 & 53.1/54.2 & 62.8/64.4 \\
             VFNet & R2-101-DCN~\cite{Res2Net} & 10.3 & 50.4/51.3 & 68.9/69.7 & 54.7/55.8 & 31.2/31.9 & 53.7/54.7 & 63.3/64.4 \\
             
             VFNet-X-800 & R2-101-DCN~\cite{Res2Net} & 8.0 & 53.7 & 71.6 & 58.7 & 34.4 & 57.5 & \textbf{67.5} \\
             VFNet-X-1200 & R2-101-DCN~\cite{Res2Net} & 4.2 & \textbf{55.1} & \textbf{73.0} & \textbf{60.1} & \textbf{37.4} & \textbf{58.2} & 67.0 \\
            
            \hline
        \end{tabular}
    \end{center}
    \vspace{-5mm}
\caption{Performance (single-model single-scale) comparison with state-of-the-art detectors on MS COCO \texttt{test-dev}. VFNet consistently outperforms the strong baseline ATSS by $\sim$2.0 AP. Our best model VFNet-X-1200 reaches 55.1 AP, achieving the new stat-of-the-art. 'R': ResNet. 'X': ResNeXt. 'R2': Res2Net. 'DCN': Deformable convolution network. '/' separates results of the MSTrain image scale range 1333$\times$[640:800] / 1333$\times$[480:960]. FPSs with $^{\dag}$ are from papers.}
\label{table:sota}
\end{table*}
\begin{table}[t]
    \begin{center}
        \begin{tabular}{ K{12em}  | K{3em} K{3em} K{3em} }
            \hline
             Method  & AP &  AP$_{50}$ &  AP$_{75}$ \\
            \hline
             RetinaNet~\cite{retinaNet} + FL  & 36.5 & 55.5 & 38.8  \\ 
             RetinaNet~\cite{retinaNet} + GFL & 37.3 & 56.4 & 40.0  \\ 
             RetinaNet~\cite{retinaNet} + VFL & \textbf{37.4} & 56.5 & 40.2  \\ 
             \hline
             FoveaBox~\cite{foveaBox} + FL  & 36.3 & 56.3 & 38.3  \\ 
             FoveaBox~\cite{foveaBox} + GFL & 36.9 & 56.0 & 39.7  \\ 
             FoveaBox~\cite{foveaBox} + VFL & \textbf{37.2} & 56.2 & 39.8  \\
             \hline
             RepPoints~\cite{repPoints} + FL  & 38.3 & 59.2 & 41.1  \\ 
             RepPoints~\cite{repPoints} + GFL & 39.2 & 59.8 & 42.5 \\ 
             RepPoints~\cite{repPoints} + VFL & \textbf{39.7} & 59.8 & 43.1  \\ 
             \hline
             ATSS~\cite{ATSS} + FL & 39.3 & 57.5 & 42.5 \\
             ATSS~\cite{ATSS} + GFL & 39.8 & 57.7 & 43.2 \\
             ATSS~\cite{ATSS} + VFL & \textbf{40.2} & 58.2 & 44.0 \\
             \hline
             VFNet + FL  & 40.0 & 58.0 & 43.2 \\ 
             VFNet + GFL & 41.1 & 58.9 & 42.2 \\ 
             VFNet + VFL & \textbf{41.6} & 59.5 & 45.0 \\ 
            \hline
        \end{tabular}
    \end{center}
    \vspace{-5mm}
\caption{Comparison of performances when applying the focal loss (FL)~\cite{retinaNet}, the generalized focal loss (GFL)~\cite{GFL} and our varifocal loss (VFL) to existing popular dense object detectors and our VFNet.}
\label{table:existing}
\vspace{-4mm}
\end{table}

\vspace{0.3mm}
\subsection{Ablation Study}
\subsubsection{Varifocal Loss}
We first investigate the effect of the hyper-parameters of the varifocal loss on the detection performance. There are two hyper-parameters: \textbf{$\alpha$} for balancing the losses between positive examples and negative examples, and \textbf{$\gamma$} for down-weighting the losses of the easy negative examples. 
We show the performance of the VFNet in Table~\ref{table:VFL} when varying $\alpha$ from 0.5 to 1.5 and $\gamma$ from 1.0 to 3.0 (only the results obtained with optimal $\alpha$  are shown). It shows that similar results above 41.2 AP are achieved and our varifocal loss is quite robust to different sets of ($\alpha$, $\gamma$). Among those, \textbf{$\alpha$ = 0.75} and \textbf{$\gamma$ = 2.0} work best (41.6 AP), and we adopt these two values for all the following experiments.

We also investigate the effect of weighting the loss of the positive example with the training target q, termed as \textit{q weighting}. The fourth row in Table~\ref{table:VFL} shows the performance of the optimal set of ($\alpha$, $\gamma$) without q weighting and 0.4 AP drop is observed (41.2 AP v.s 41.6 AP). This confirms the positive effect of q weighting.

\vspace{-3mm}
\subsubsection{Individual Component Contribution}

We study the impact of the individual component of our method and results are shown in Table~\ref{table:ablation}. The first row shows the performance of the raw VFNet (FCOS+ATSS without centerness branch) trained with the focal loss and 39.0 AP is acquired. Replacing the focal loss with our varifocal loss, the performance is improved to 40.1 AP, which is 0.9 AP higher than the FCOS+ATSS. By adding the star-shaped representation and bounding box refinement modules, the performance is further boosted to 40.7 AP and 41.6 AP respectively. These results verify the effectiveness of the three modules in our VFNet. 



\subsection{Comparison with State-of-the-Art}
We compare our VFNet with other detectors on the COCO \texttt{test-dev}. We select the ATSS~\cite{ATSS} as our baseline since it has similar performance to the FCOS+ATSS.

Table~\ref{table:sota} presents the results. Compared with the strong baseline ATSS, our VFNet achieves $\sim$2.0 AP gaps with different backbones, \eg 46.0 AP vs. 43.6 AP with the ResNet-101 backbone. This validates the contributions of our method. Compared to the concurrent work, the GFL~\cite{GFL} (whose MSTrain scale range is 1333x[480:800]), our VFNet is consistently better than it by a considerable margin. Meanwhile, our model trained with Res2Net-101-DCN~\cite{Res2Net} achieves a single-model single-scale AP of 51.3, surpassing almost all recent state-of-the-art detectors.

We also report the inference speed of VFNet in terms of frame per second (FPS) on a Nvidia V100 GPU. Since it is difficult to get the speed of all the listed detectors under exactly same settings, we only compare VFNet with the baseline ATSS. 
It can be seen that our VFNet is very efficient, \eg achieving 44.8 AP at 19.3 FPS, and only incurs small additional computation overhead compared to the baseline.


\vspace{-0.5mm}
\subsection{VarifocalNet-X}
To push the envelope of VFNet, We also implement some extensions to the original VFNet. This version of VFNet is called VFNet-X and those extensions include: 

\noindent \textbf{PAFPN.} We replace the FPN with the PAFPN~\cite{PAFPN}, and apply the DCN and group normalization (GN)~\cite{GN} in it.

\noindent \textbf{More and Wider Conv Layers.} We stack 4 convolution layers in the detection head, instead of 3 layers in the original VFNet, and increase the original 256 feature channels to 384 channels. 

\noindent \textbf{RandomCrop and Cutout.} We employ the random crop and cutout~\cite{cutout} as additional data augmentation methods.

\noindent \textbf{Wider MSTrain Scale Range and Longer Training.} We adopt a wider MSTrain scale range, from 750$\times$500 to 2100$\times$1400, and initially train the VFNet-X for 41 epochs.

\noindent \textbf{SWA.} We apply the technique of stochastic weight averaging (SWA)~\cite{SWA} in training the VFNet-X, which brings 1.2 AP gain. Specifically, after the initial 41-epoch training of VFNet-X, we further train it for another 18 epochs using a cyclic learning rate schedule and then simply average those 18 checkpoints as our final model.


The performance of VFNet-X on COCO \texttt{test-dev} is shown in the last rows of Table~\ref{table:sota}. When the inference scale 1333$\times$800 and soft-NMS\cite{SoftNMS} are adopted, VFNet-X-800 achieves 53.7 AP, while simply increasing the image scale to 1800$\times$1200, VFNet-X-1200 reaches a new state-of-the-art \textbf{55.1} AP, surpassing prior detectors by a large margin. 
 Qualitative detection examples of applying this model to the COCO \texttt{test-dev} can be found in Figure~\ref{fig:example}.



\vspace{-0.5mm}
\subsection{Generality and Superiority of Varifocal Loss}
\vspace{-0.5mm}
To verify the generality of our varifocal loss, we apply it to some existing popular dense object detectors, including RetinaNet~\cite{retinaNet}, FoveaBox~\cite{foveaBox}, RepPoints~\cite{repPoints} and ATSS~\cite{ATSS}, and evaluate the performance on the \texttt{val2017}. We simply replace the focal loss (FL)~\cite{retinaNet} used in these detectors (ResNet-50 backbone) with our varifocal loss for training. We also train them with the generalized focal loss (GFL) for comparison. 

Table~\ref{table:existing} shows the results.
We can see that our varifocal loss improves RetinaNet, FoveaBox and ATSS consistently by 0.9 AP. For RepPoints, the gain increases to 1.4 AP. This shows that our varifocal loss can easily bring considerable performance boost to existing dense object detectors. Compared to the GFL, our varifocal loss performs better than it in all cases, evidencing the superiority of our varifocal loss. 

Additionally, we train our VFNet with the FL and GFL for further comparison. Results are shown in the last section of Table~\ref{table:existing} and the consistent advantage of our varifocal loss over the FL and GFL can be observed.

\vspace{-2mm}

\begin{figure*}[tbp]
	\centering
	\begin{tabular}{cc}
		\includegraphics[width=0.48\textwidth]{examples/13601.png}&
		\includegraphics[width=0.48\textwidth]{examples/386688.png}\\
		\includegraphics[width=0.48\textwidth]{examples/222219.png}&
		\includegraphics[width=0.48\textwidth]{examples/435995.png}\\
		\includegraphics[width=0.48\textwidth]{examples/540552.png}&
		\includegraphics[width=0.48\textwidth]{examples/308940.png}\\
	\end{tabular}
	\caption{Detection examples of applying our best model on COCO \texttt{test-dev}. The score threshold for visualization is 0.3.
	}\vspace{-2mm}
	\label{fig:example}
\end{figure*}








    
