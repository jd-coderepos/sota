
\documentclass{article} \usepackage{iclr2022_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[export]{adjustbox}\usepackage{multirow}
\usepackage{soul,color}
\soulregister\cite7
\soulregister\citep7
\soulregister\ref7

\title{Unsupervised Semantic Segmentation \\ by Distilling Feature Correspondences}



\author{Mark Hamilton \\
MIT, Microsoft\\
\texttt{markth@mit.edu} \\
\And
Zhoutong Zhang \\
MIT \\
\And
Bharath Hariharan \\
Cornell University \\
\AND
Noah Snavely \\
Cornell University, Google \\
\And 
William T. Freeman \\
MIT, Google \\
}




\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\zt}[1]{{\color{green}{zt:#1}}}
\iclrfinalcopy \begin{document}


\maketitle
\vspace{-.2in}


\begin{abstract}
Unsupervised semantic segmentation aims to discover and localize semantically meaningful categories within image corpora without any form of annotation. To solve this task, algorithms must produce features for every pixel that are both semantically meaningful and compact enough to form distinct clusters. Unlike previous works which achieve this with a single end-to-end framework, we propose to separate feature learning from cluster compactification. Empirically, we show that current unsupervised feature learning frameworks already generate dense features whose correlations are semantically consistent. This observation motivates us to design STEGO (\textbf{S}elf-supervised \textbf{T}ransformer with \textbf{E}nergy-based \textbf{G}raph \textbf{O}ptimization), a novel framework that distills unsupervised features into high-quality discrete semantic labels. At the core of STEGO is a novel contrastive loss function that encourages features to form compact clusters while preserving their relationships across the corpora.
STEGO yields a significant improvement over the prior state of the art, on both the CocoStuff (\textbf{+14 mIoU}) and Cityscapes (\textbf{+9 mIoU}) semantic segmentation challenges.  



\end{abstract}


\section{Introduction}

Semantic segmentation is the process of classifying each individual pixel of an image into a known ontology. Though semantic segmentation models can detect and delineate objects at a much finer granularity than classification or object detection systems, these systems are hindered by the difficulties of creating labelled training data. In particular, segmenting an image can take over $100\times$ more effort for a human annotator than classifying or drawing bounding boxes \citep{zlateski2018importance}. Furthermore, in complex domains such as medicine, biology, or astrophysics, ground-truth segmentation labels may be unknown, ill-defined, or require considerable domain-expertise to provide \citep{yu2018methods}. 

Recently, several works introduced semantic segmentation systems that could learn from weaker forms of labels such as classes, tags, bounding boxes, scribbles, or point annotations \citep{ren2020ufo,pan2021weakly,liu2020leveraging,bilen_benenson}. However, comparatively few works take up the challenge of semantic segmentation without \textit{any} form of human supervision or motion cues. Attempts such as Independent Information Clustering (IIC) \citep{iic} and PiCIE~\citep{Cho2021PiCIEUS} aim to learn semantically meaningful features through transformation equivariance, while imposing a clustering step to improve the compactness of the learned features.


\begin{figure}[h]
 \centering
 \includegraphics[width=\linewidth]{figures/cocostuff27_results.jpg}
  \vspace{-.3in}
  \caption{Unsupervised semantic segmentation predictions on the CocoStuff \citep{cocostuff} 27 class segmentation challenge. Our method, STEGO, does not use labels to discover and segment consistent objects. Unlike the prior state of the art, PiCIE \citep{Cho2021PiCIEUS}, STEGO's predictions are consistent, detailed, and do not omit key objects.}
\label{fig:teaser}
\end{figure} 
In contrast to these previous methods, we utilize pre-trained features from unsupervised feature learning frameworks and focus on distilling them into a compact and discrete structure while preserving their relationships across the image corpora. This is motivated by the observation that correlations between unsupervised features, such as ones learned by DINO \citep{dino}, are already semantically consistent, both within the same image and across image collections. 


As a result, we introduce STEGO (\textbf{S}elf-supervised \textbf{T}ransformer with \textbf{E}nergy-based \textbf{G}raph \textbf{O}ptimization), which is capable of jointly discovering and segmenting objects without human supervision. STEGO distills pretrained unsupervised visual features into semantic clusters using a novel contrastive loss. STEGO dramatically improves over prior art and is a considerable step towards closing the gap with supervised segmentation systems. We include a short video detailing the work at {\color{blue}\url{https://aka.ms/stego-video}}. Specifically, we make the following contributions:
\vspace{-.1in}
\begin{itemize}
\setlength{\itemsep}{0pt}
\item Show that unsupervised deep network features have correlation patterns that are largely consistent with true semantic labels.
\item Introduce STEGO, a novel transformer-based architecture for unsupervised semantic segmentation.
\item Demonstrate that STEGO achieves state of the art performance on both the CocoStuff (\textbf{+14 mIoU}) and Cityscapes (\textbf{+9 mIoU}) segmentation challenges.
\item Justify STEGO's design with an ablation study on the CocoStuff dataset.
\end{itemize}


\section{Related Work}


\paragraph{Self-supervised Visual Feature Learning}
Learning meaningful visual features without human annotations is a longstanding goal of computer vision. Approaches to this problem often optimize a surrogate task, such as denoising~\citep{vincent2008extracting}, inpainting~\citep{pathakCVPR16context}, jigsaw puzzles, colorization~\citep{zhang2017split}, rotation prediction~\citep{gidaris2018unsupervised}, and most recently, contrastive learning over multiple augmentations \citep{deepinfomax, simclrv2, simclrv2, mocov2, oord2018representation}. Contrastive learning approaches, whose performance surpass all other surrogate tasks, assume visual features are invariant under a certain set of image augmentation operations. These approaches maximize feature similarities between an image and its augmentations, while minimizing similarity between negative samples, which are usually randomly sampled images. Some notable examples of positive pairs include temporally adjacent images in videos \citep{oord2018representation}, image augmentations \citep{simclrv2,mocov2}, and local crops of a single image \citep{deepinfomax}. Many works highlight the importance of large numbers of negative samples during training. To this end \cite{wu2018unsupervised} propose keeping a memory bank of negative samples and \cite{mocov2} propose momentum updates that can efficiently simulate large negative batch sizes. Recently some works have aimed to produce spatially dense feature maps as opposed to a single global vector per image. In this vein, VADeR \citep{pinheiro2020unsupervised} contrasts local per-pixel features based on random compositions of image transformations that induce known correspondences among pixels which act as positive pairs for contrastive training. Instead of trying to learn visual features and clustering from scratch, STEGO treats pretrained self-supervised features as input and is agnostic to the underlying feature extractor. This makes it easy to integrate future advances in self-supervised feature learning into STEGO.



\paragraph{Unsupervised Semantic Segmentation}
Many unsupervised semantic segmentation approaches use techniques from self-supervised feature learning. IIC \citep{iic} maximizes mutual information of patch-level cluster assignments between an image and its augmentations. Contrastive Clustering \citep{contrastive-clustering}, and SCAN \citep{scan} improve on IIC's image clustering results with supervision from negative samples and nearest neighbors but do not attempt semantic segmentation. PiCIE \citep{Cho2021PiCIEUS} improves on IIC's semantic segmentation results by using invariance to photometric effects and equivariance to geometric transformations as an inductive bias. In PiCIE, a network minimizes the distance between features under different transformations, where the distance is defined by an in-the-loop k-means clustering process. SegSort \citep{segsort} adopts a different approach. First, SegSort learns good features using superpixels as proxy segmentation maps, then uses Expectation-Maximization to iteratively refine segments over a spherical embedding space. In a similar vein, MaskContrast \citep{maskcontrast} achieves promising results on PascalVOC by first using an off-the-shelf saliency model to generate a binary mask for each image. MaskContrast then contrasts learned features within and across the saliency masks. In contrast, our method focuses refining existing pretrained self-supervised visual features to distill their correspondence information and encourage cluster formation. This is similar to the work of \cite{collins2018deep} who show that low rank factorization of deep network features can be useful for unsupervised co-segmentation. We are not aware of any previous work that achieves the goal of high-quality, pixel-level unsupervised semantic segmentation on large scale datasets with diverse images.


\paragraph{Visual Transformers}
Convolutional neural networks (CNNs) have long been state of the art for many computer vision tasks, but the nature of the convolution operator makes it hard to model long-range interactions. To circumvent such shortcomings, \cite{wang2018non, zhang2019self} use self-attention operations within a CNN to model long range interactions. Transformers \citep{vaswani2017attention}, or purely self-attentive networks, have made significant progress in NLP and have recently been used for many computer vision tasks~\citep{dosovitskiy2020image, touvron2021training, Ranftl2021, dino}.
Visual Transformers (ViT)~\citep{vaswani2017attention} apply self-attention mechanisms to image patches and positional embeddings in order to generate features and predictions. Several modifications of ViT have been proposed to improve supervised learning, unsupervised learning, multi-scale processing, and dense predictions. In particular, DINO~\citep{dino} uses a ViT within a self-supervised learning framework that performs self-distillation with exponential moving average updates. \cite{dino} show that DINO's class-attention can produce localized and semantically meaningful salient object segmentations. Our work shows that DINO's features not only detect salient objects but can be used to extract dense and semantically meaningful correspondences between images. In STEGO, we refine the features of this pre-trained backbone to yield semantic segmentation predictions when clustered. We focus on DINO's embeddings because of their quality but note that STEGO can work with any deep network features.

\section{Methods}

\subsection{Feature Correspondences Predict Class Co-Occurrence}
\label{sec:correspondence}





Recent progress in self-supervised visual feature learning has yielded methods with powerful and semantically relevant features that improve a variety of downstream tasks. Though most works aim to generate a single vector for an image, many works show that intermediate dense features are semantically relevant \citep{hamilton2021model,collins2018deep,cam}. To use this information, we focus on the ``correlation volume'' \citep{raft} between the dense feature maps. For convolutional or transformer architectures, these dense feature maps can be the activation map of a specific layer. Additionally, the Q, K or V matrices in transformers can also serve as candidate features, though we find these attention tensors do not perform as well in practice. More formally, let $f\in \mathbb{R}^{CHW}, g \in \mathbb{R}^{CIJ}$ be the feature tensors for two different images where $C$ represents the channel dimension and $(H,W), (I,J)$ represent spatial dimensions. We form the feature correspondence tensor:
\begin{equation}
F_{hwij} := \sum_c \frac{f_{chw}}{|f_{hw}|} \frac{g_{cij}}{|g_{ij}|}, 
\label{eqn:correspondence}
\end{equation}
whose entries represent the cosine similarity between the feature at spatial position $(h,w)$ of feature tensor $f$ and position $(i,j)$ of feature tensor $g$. In the special case where $f=g$ these correspondences measure the similarity between two regions of the same image. We note that this quantity appears often as the ``cost-volume'' within the optical flow literature, and \cite{hamilton2021model} show this acts a higher-order generalization of Class Activation Maps \citep{cam} for contrastive architectures and visual search engines. By examining slices of the correspondence tensor, $F$, at a given $(h,w)$ we are able to visualize how two images relate according the featurizer. For example, Figure \ref{fig:correspondence} shows how three different points from the source image (shown in blue, red, and green) are in correspondence with relevant semantic areas within the image and its K-nearest neighbors with respect to the DINO~\citep{dino} as the feature extractor.

This feature correspondence tensor not only allows us to visualize image correspondences but is strongly correlated with the true label co-occurrence tensor. In particular, we can form the ground truth label co-occurrence tensor given a pair of ground-truth semantic segmentation labels $k \in \mathcal{C}^{HW}, l \in \mathcal{C}^{IJ}$ where $\mathcal{C}$ represents the set of possible classes:
$$
    L_{hwij} :=  \begin{cases} 
        1, & \text{if } l_{hw}=k_{ij}\\
        0, & \text{if } l_{hw} \neq k_{ij} \\
        \end{cases} 
$$
By examining how well the feature correspondences, $F$, predict the ground-truth label co-occurrences, $L$, we can measure how compatible the features are with the semantic segmentation labels. More specifically we treat the feature correspondences as a probability logit and compute the average precision when used as a classifier for $L$. This approach not only acts as a quick diagnostic tool to determine the efficacy of features, but also allows us to compare with other forms of supervision such as the fully connected Conditional Random Field (CRF) \citep{fullcrf}, which uses correspondences between pixels to refine low-resolution label predictions. In Figure \ref{fig:pr} we plot precision-recall curves for the DINO backbone, the MoCoV2 backbone, the CRF Kernel, and our trained STEGO architecture. Interestingly, we find that DINO is already a spectacular predictor of label co-occurrence within the Coco stuff dataset despite \textbf{never seeing the labels}. In particular, DINO recalls $50\%$ of true label co-occurrences with a precision of $90\%$ and significantly outperforms both MoCoV2 feature correspondences and the CRF kernel. One curious note is that our final trained model is a better label predictor than the supervisory signal it learns from. We attribute this to the distillation process discussed in Section \ref{sec:distillation} which amplifies this supervisory signal and drives consistency across the entire dataset. Finally, we stress that our comparison to ground truth labels within this section is solely to provide intuition about the quality of feature correspondences as a supervisory signal. \textbf{We do not use the ground truth labels to tune any parameters of STEGO.}


\begin{figure}[t]
\centering
\begin{minipage}{.60\textwidth}
  \centering
    \includegraphics[width=\linewidth]{figures/dino_correspondence.jpg}
    \vspace{-.2in}
    \captionof{figure}{Feature correspondences from DINO. Correspondences between the source image (left) and the target images (middle and right) are plotted over the target images in the respective color of the source point (crosses in the left image). Feature correspondences can highlight key aspects of shared semantics within a single image (middle) and across similar images such as KNNs (right) }
  \label{fig:correspondence}
\end{minipage}\hfill
\begin{minipage}{.38\textwidth}
  \centering
  \includegraphics[width=.85\linewidth]{figures/stego_pr.jpg}
  \vspace{-.1in}
  \captionof{figure}{Precision recall curves show that feature self-correspondences strongly predict true label co-occurrence. DINO outperforms MoCoV2 and a CRF kernel, which shows its power as an unsupervised learning signal. }
  \label{fig:pr}
\end{minipage}
\end{figure} 

\subsection{Distilling Feature Correspondences}
\label{sec:distillation} 

In Section \ref{sec:correspondence} we have shown that feature correspondences have the potential to be a quality learning signal for unsupervised segmentation. In this section we explore how to harness this signal to create pixel-wise embeddings that, when clustered, yield a quality semantic segmentation. In particular, we seek to learn a low-dimensional embedding that ``distills'' the feature correspondences. To achieve this aim, we draw inspiration from the CRF which uses an undirected graphical model to refine noisy or low-resolution class predictions by aligning them with edges and color-correlated regions in the original image. 

More formally, let $\mathcal{N}: \mathbb{R}^{C'H'W'} \to \mathbb{R}^{CHW}$ represent a deep network backbone, which maps an image $x$ with $C'$ channels and spatial dimensions $(H',W')$ to a feature tensor $f$ with $C$ channels and spatial dimensions $(H,W)$. In this work, we keep this backbone network frozen and focus on training a light-weight segmentation head $\mathcal{S}: \mathbb{R}^{CHW} \to \mathbb{R}^{KHW}$, that maps our feature space to a code space of dimension $K$, where $K<C$. The goal of $\mathcal{S}$ is to learn a nonlinear projection, $\mathcal{S}(f) =: s  \in \mathbb{R}^{KHW}$, that forms compact clusters and amplifies the correlation patterns of $f$.

To build our loss function let $f$ and $g$ be two feature tensors from a pair of images $x$, and $y$ and let $s := \mathcal{S}(f) \in \mathbb{R}^{CHW}$ and $t := \mathcal{S}(g)  \in \mathbb{R}^{CIJ}$ be their respective segmentation features. Next, using Equation \ref{eqn:correspondence} we compute a feature correlation tensor $F \in R^{HWIJ}$ from $f$ and $g$ and a segmentation correlation tensor $S \in R^{HWIJ}$ from $s$ and $t$. Our loss function aims to push the entries of $s$ and $t$ together if there is a significant coupling between two corresponding entries of $f$ and $g$. As shown in Figure \ref{fig:arch}, we can achieve this with a simple element-wise multiplication of the tensors $F$ and $S$:
\begin{equation}
\label{eqn-simple_coor}
    \mathcal{L}_{\mathit{simple-corr}}(x,y,b) := - \sum_{hwij} (F_{hwij} - b) S_{hwij}
\end{equation}
Where $b$ is a hyper-parameter which adds uniform ``negative pressure'' to the equation to prevent collapse. Minimizing $\mathcal{L}$ with respect to $S$ encourages elements of $S$ to be large when elements of $F-b$ are positive and small when elements of $F-b$ are negative. More explicitly, because the elements of $F$ and $S$ are cosine similarities, this exerts an attractive or repulsive force on pairs of segmentation features with strength proportional to their feature correspondences. We note that the elements of $S$ are not just encouraged to \textit{equal} the elements of $F$ but rather to push to total anti-alignment $(-1)$ or alignment $(1)$ depending on the sign of $F-b$. 

In practice, we found that $\mathcal{L}_{\mathit{simple-corr}}$ is sometimes unstable and does not provide enough learning signal to drive the optimization. Empirically, we found that optimizing the segmentation features towards total anti-alignment when the corresponding features do not correlate leads to instability, likely because this increases co-linearity. Therefore, we optimize weakly-correlated segmentation features to be orthogonal instead. This can be efficiently achieved by clamping the segmentation correspondence, $S$, at $0$, which dramatically improved the optimization stability.

Additionally, we encountered challenges when balancing the learning signal for small objects which have concentrated correlation patterns. In these cases, $F_{hwij}-b$ is negative in most locations, and the loss drives the features to diverge instead of aggregate. To make the optimization more balanced, we introduce a \textbf{S}patial \textbf{C}entering operation on the feature correspondences:
\begin{align}
    F^{SC}_{hwij} &:= F_{hwij} - \frac{1}{IJ}\sum_{i'j'}F_{hwi'j'}.
\end{align}

Together with the zero clamping, our final correlation loss is defined as:
\begin{equation}
    \mathcal{L}_{corr}(x,y,b) := - \sum_{hwij} (F^{SC}_{hwij} - b) max(S_{hwij},0).
    \label{eq:corrloss}
\end{equation}
We demonstrate the positive effect of both the aforementioned ``0-Clamp'' and ``SC'' modifications in the ablation study of Table \ref{table:ablation}.

\begin{figure}[t]
 \centering
 \includegraphics[width=\linewidth]{figures/stego.pdf}
  \vspace{-.2in}
  \caption{High-level overview of the STEGO architecture at train and prediction steps. Grey boxes represent three different instantiations of the main correspondence distillation loss which is used to train the segmentation head. }
\label{fig:arch}
\end{figure} 
\subsection{STEGO Architecture}
\label{sec:architecture}



STEGO uses three instantiations of the correspondence loss of Equation \ref{eq:corrloss} to train a segmentation head to distill feature relationships between an image and itself, its K-Nearest Neighbors (KNNs), and random other images. The self and KNN correspondence losses primarily provide positive, attractive, signal and random image pairs tend to provide negative, repulsive, signal. We illustrate this and other major architecture components of STEGO in Figure \ref{fig:arch}. 

STEGO is made up of a frozen backbone that serves as a source of learning feedback, and as an input to the segmentation head for predicting distilled features. This segmentation head is a simple feed forward network with ReLU activations \citep{relu}. In contrast to other works, our method does not re-train or fine-tune the backbone. This makes our method very efficient to train: it only takes less than 2 hours on a single NVIDIA V100 GPU card. 






We first use our backbone to extract global image features by global average pooling (GAP) our spatial features: $GAP(f)$. We then construct a lookup table of each image's K-Nearest Neighbors according to cosine similarity in the backbone's feature space. Each training minibatch consists of a collection of random images $x$ and random nearest neighbors $x^{knn}$. In our experiments we sample $x^{knn}$ randomly from each image's top $7$ KNNs. We also sample random images, $x^{rand}$, by shuffling $x$ and ensuring that no image matched with itself. STEGO's full loss is:
\begin{equation}
\label{eq:full-loss}
    \mathcal{L} = \lambda_{self} \mathcal{L}_{corr}(x, x, b_{self}) + \lambda_{knn} \mathcal{L}_{corr}(x, x^{knn}, b_{knn}) + \lambda_{rand} \mathcal{L}_{corr}(x, x^{rand}, b_{rand})
\end{equation}
Where the $\lambda$'s and the $b$'s control the balance of the learning signals and the ratio of positive to negative pressure respectively. In practice, we found that a ratio of $\lambda_{self} \approx \lambda_{rand} \approx 2 \lambda_{knn}$ worked well. The $b$ parameters tended to be dataset and network specific, but we aimed to keep the system in a rough balance between positive and negative forces. More specifically we tuned the $b$s to keep mean KNN feature similarity at $\approx0.3$ and mean random similarity at $\approx0.0$. 

Many images within the CocoStuff and Cityscapes datasets are cluttered with small objects that are hard to resolve at a feature resolution of $(40,40)$. To better handle small objects and maintain fast training times we five-crop training images prior to learning KNNs. This not only allows the network to look at closer details of the images, but also improves the quality of the KNNs. More specifically, global image embeddings are computed for each crop. This allows the network to resolve finer details and yields five times as many images to find close matching KNNs from. Five-cropping improved both our Cityscapes results and CocoStuff segmentations, and we detail this in Table \ref{table:ablation}. 

The final components of our architecture are the clustering and CRF refinement step. Due to the feature distillation process, STEGO's segmentation features tend to form clear clusters. We apply a cosine distance based minibatch K-Means algorithm \citep{kmeans} to extract these clusters and compute concrete class assignments from STEGO's continuous features. After clustering, we refine these labels with a CRF to improve their spatial resolution further. 


\begin{table}[t]
\centering
\captionof{table}{Comparison of unsupervised segmentation architectures on 27 class CocoStuff validation set. STEGO significantly outperforms prior art in both unsupervised clustering and linear-probe style metrics.}
 \vspace{-.2in}
\begin{tabular}{c|cc|cc}
\textbf{}                           & \multicolumn{2}{c|}{\textbf{Unsupervised}} & \multicolumn{2}{c}{\textbf{Linear Probe}} \\
\textbf{Model}                      & \textbf{Accuracy}      & \textbf{mIoU}     & \textbf{Accuracy}     & \textbf{mIoU}     \\ \hline
ResNet50 \citep{he2016deep}               & 24.6                   & 8.9               & 41.3                  & 10.2              \\
MoCoV2 \citep{mocov2}                 & 25.2                   & 10.4              & 44.4                  & 13.2              \\
DINO \citep{dino}                   & 30.5                   & 9.6               & 66.8                  & 29.4              \\
Deep Cluster   \citep{deep-cluster} & 19.9                   & -                 & -                     & -                 \\
SIFT \citep{SIFT}                   & 20.2                   & -                 & -                     & -                 \\
\cite{doersh15}                    & 23.1                   & -                 & -                     & -                 \\
\cite{isola16}                     & 24.3                   & -                 & -                     & -                 \\
AC \citep{ac}                       & 30.8                   & -                 & -                     & -                 \\
InMARS \citep{inmars}               & 31.0                   & -                 & -                     & -                 \\
IIC \citep{iic}                     & 21.8                   & 6.7               & 44.5                  & 8.4               \\
MDC \citep{Cho2021PiCIEUS}          & 32.2                   & 9.8               & 48.6                  & 13.3              \\
PiCIE \citep{Cho2021PiCIEUS}        & 48.1                   & 13.8              & 54.2                  & 13.9              \\
PiCIE + H   \citep{Cho2021PiCIEUS}  & 50.0                   & 14.4              & 54.8                  & 14.8              \\
\textbf{STEGO (Ours)}               & \textbf{56.9}          & \textbf{28.2}     & \textbf{76.1}         & \textbf{41.0}    
\end{tabular}
  \label{table:cocostuff_results}
\end{table} 
\subsection{Relation to Potts Models and Energy-based Graph Optimization}
\label{sec:ebm}
\renewcommand{\R}{\mathbb{R}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\V}{\mathcal{V}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\F}{\mathcal{F}}
\renewcommand{\P}{\mathbb{P}}

Equation \ref{eq:corrloss} can be viewed in the context of Potts models or continuous Ising models from statistical physics \citep{potts1952some,baker1979continuous}. We briefly overview this connection, and point interested readers to Section \ref{sec:additional_ebm} for a more detailed discussion. To build the general Ising model, let $\G = (\V, w)$ be a fully connected, weighted, and undirected graph on $|\V|$ vertices. In our applications we take $\V$ to be the set of pixels in the training dataset. Let $w: \V \times \V \to \R$ represent an edge weighting function. Let $\phi: \V \to \C$ be a vertex valued function mapping into a generic code space $\C$ such as the probability simplex over cluster labels $\mathcal{P}(L)$, or the $K$-dimensional continuous feature space $\R^K$. The function $\phi$ can be a parameterized neural network, or a simple lookup table that assigns a code to each graph node. Finally, we define a compatibility function $\mu: \C \times \C \to \R$ that measures the cost of comparing two codes. We can now define the following graph energy functional:
\begin{equation}
    \label{eq:energy}
    E(\phi) := \sum_{v_i, v_j \in \V} w(v_i, v_j) \mu(\phi(v_i), \phi(v_j))
\end{equation}
Constructing the Boltzmann Distribution \citep{hinton2002training} yields a normalized distribution over the function space $\Phi$:
\begin{equation}
    \label{eq:boltzman}
    p(\phi | w, \mu ) = \frac{\exp(-E(\phi))}{\int_\Phi \exp(-E(\phi')) d\phi'}
\end{equation}
In general, sampling from this probability distribution is difficult because of the often-intractable normalization factor. However, it is easier to compute the maximum likelihood estimate (MLE),
$\argmax_{\phi \in \Phi} p(\phi | w, \mu )$. In particular, if $\Phi$ is a smoothly parameterized space of functions and $\phi$ and $\mu$ are differentiable functions, one can compute the MLE using stochastic gradient descent (SGD) with highly-optimized automatic differentiation frameworks \citep{pytorch,tensorflow}. In Section \ref{sec:additional_ebm} of the supplement we prove that the finding the MLE of Equation \ref{eq:boltzman} is equivalent to minimizing the loss of Equation \ref{eq:corrloss} when $|V|$ is the set of pixels in our image training set, $\phi  = \mathcal{S} \circ \mathcal{N}$, $w$ is the cosine distance between features, and $\mu$ is cosine distance. Like STEGO, the CRF is also a Potts model, and we use this connection to re-purpose the STEGO loss function to create continuous, minibatch, and unsupervised variants of the CRF. We detail this exploration in Section \ref{sec:crf} of the Supplement. 



\section{Experiments}
\vspace{-.1in}

We evaluate STEGO on standard semantic segmentation datasets and compare with current state-of-the-art. We then justify different design choices of STEGO through ablation studies. Additional details on datasets, model hyperparameters, hardware, and other implementation details can be found in Section \ref{sec:implementation-details} of the Supplement.

\subsection{Evaluation Details}
\label{sec:evaluation}
\vspace{-.1in}
\paragraph{Datasets}
Following \cite{Cho2021PiCIEUS}, we evaluate STEGO on the $27$ mid-level classes of the CocoStuff class hierarchy and on the $27$ classes of Cityscapes. Like prior art, we first resize images to $320$ pixels along the minor axis followed by a $(320 \times 320)$ center crops of each validation image. We use mean intersection over union (mIoU) and Accuracy for evaluation metrics. Our CocoStuff evaluation setting originated in \cite{iic} and is common in the literature. Our Cityscapes evaluation setting is adopted from \cite{Cho2021PiCIEUS}. The latter is newer and more challenging, and thus fewer baselines are available. Finally we also compare on the Potsdam-3 setting fro \cite{iic} in Section \ref{sec:potsdam} of the Appendix.
\vspace{-.1in}
\paragraph{Linear Probe} The first way we evaluate the quality of the distilled segmentation features is through transfer learning effectiveness. As in \cite{maskcontrast,Cho2021PiCIEUS,chen2020improved}, we train a linear projection from segmentation features to class labels using the cross entropy loss. This loss solely evaluates feature quality and is not part of the STEGO training process.
\vspace{-.3in}
\paragraph{Clustering} Unlike the linear probe, the clustering step does not have access to ground truth supervised labels. As in prior art, we use a Hungarian matching algorithm to align our unlabeled clusters and the ground truth labels for evaluation and visualization purposes. This measures how consistent the predicted semantic segments are with the ground truth labels and is invariant to permutations of the predicted class labels.








\begin{figure}[t]
\centering
\begin{minipage}{.58\textwidth}
  \centering
 \includegraphics[width=\linewidth]{figures/cityscapes_results.jpg}
 \vspace{-.3in}
   \captionof{figure}{Comparison of ground truth labels (middle row) and cluster probe predictions for STEGO (bottom row) for images from the Cityscapes dataset.}
    \label{fig:cityscapes_preds}
\end{minipage}\hfill
\begin{minipage}{.4\textwidth}
  \centering
    \includegraphics[width=\linewidth]{figures/cocostuff_cm.png}
     \vspace{-.3in}
    \captionof{figure}{Confusion matrix of STEGO cluster probe predictions on CocoStuff. Classes after the ``vehicle'' class are ``stuff'' and classes before are ``things''. Rows are normalized to sum to 1.}
  \label{fig:cocostuff_cm}
\end{minipage}
\end{figure} 

\subsection{Results}
\vspace{-.05in}

We summarize our main results on the 27 classes of CocoStuff in Table \ref{table:cocostuff_results}. STEGO significantly outperforms the prior state of the art, PiCIE, on both linear probe and clustering (Unsupervised) metrics. In particular, STEGO improves by \textbf{+14} unsupervised mIoU, \textbf{+6.9} unsupervised accuracy, \textbf{+26} linear probe mIoU, and \textbf{+21} linear probe accuracy compared to the next best baseline. In Table \ref{table:cityscapes_results}, we find a similarly large improvement of \textbf{+8.7} unsupervised mIoU and \textbf{+7.7} unsupervised accuracy on the Cityscapes validation set. These two experiments demonstrate that even though we do not fine-tune the backbone for these datasets, DINO's self-supervised weights on ImageNet \citep{imagenet} are enough to simultaneously solve both settings. STEGO also outperforms simply clustering the features from unmodified DINO, MoCoV2, and ImageNet supervised ResNet50 backbones. This demonstrates the benefits of training a segmentation head to distill feature correspondences. 

We show some example segmentations from STEGO and our baseline PiCIE on the CocoStuff dataset in Figure \ref{fig:teaser}. We include additional examples and failure cases in Sections \ref{sec:additional-examples} and \ref{sec:failure-cases}. We note that STEGO is significantly better at resolving fine-grained details within the images such as the legs of horses in the third image from the left column of Figure \ref{fig:teaser}, and the individual birds in the right-most column. Though the PiCIE baseline uses a feature pyramid network to output high resolution predictions, the network does not attune to fine grained details, potentially demonstrating the limitations of the sparse training signal induced by data augmentations alone. In contrast, STEGO's predictions capture small objects and fine details. In part, this can be attributed to DINO backbone's higher resolution features, the 5-crop training described in \ref{sec:architecture}, and the CRF post-processing which helps to align the predictions to image edges. We show qualitative results on the Cityscapes dataset in Figure \ref{fig:cityscapes_preds}. STEGO successfully identifies people, street, sidewalk, cars, and street signs with high detail and fidelity. We note that prior works did not publish pretrained models or linear probe results on Cityscapes so we exclude this information from Table \ref{table:cityscapes_results} and Figure \ref{fig:cityscapes_preds}.

To better understand the predictions and failures of STEGO, we include confusion matrices for CocoStuff (Figure \ref{fig:cocostuff_cm}) and Cityscapes (Figure \ref{fig:cityscapes_cm} of the Supplement). Some salient STEGO errors include confusing the ``food'' category from the CocoStuff ``things'', and the ``food'' category from CocoStuff ``stuff''. STEGO also does not properly separate ``ceilings'' from ``walls'', and lacks consistent segmentations for classes such as ``indoor'', ``accessory'', ``rawmaterial'' and ``textile''. These errors also draw our attention to the challenges of evaluating unsupervised segmentation methods: \textit{label ontologies can be arbitrary}. In these circumstances the divisions between classes are not well defined and it is hard to imagine a system that can segment the results consistently without additional information. In these regimes, the linear probe provides a more important barometer for quality because the limited supervision can help disambiguate these cases. Nevertheless, we feel that there is still considerable progress to be made on the purely unsupervised benchmark, and that even with the improvements of STEGO there is still a measurable performance gap with supervised systems.


\setlength{\tabcolsep}{3pt}

\newcommand*\rot{\rotatebox{90}}

\begin{figure}[t]
\begin{minipage}{\textwidth}
\centering
\begin{minipage}{.58\textwidth}
  \captionof{table}{Architecture ablation study on the CocoStuff Dataset (27 Classes).}
\begin{tabular}{ccccc|cc|cc}
\multirow{2}{*}{\textbf{Arch.}} & \rot{\kern-.7em\multirow{2}{*}{\textbf{0-Clamp}}} & \rot{\multirow{2}{*}{\kern-.7em\textbf{5-Crop}}} & \rot{\multirow{2}{*}{\kern-.7em\textbf{SC}}} & \rot{\kern-.7em\multirow{2}{*}{\textbf{CRF}}} & \multicolumn{2}{c|}{\textbf{Unsup.}} & \multicolumn{2}{c}{\textbf{Linear Probe}} \\
                                   &                                   &                                  &                                     &                               & \textbf{Acc.}        & \textbf{mIoU}       & \textbf{Acc.}       & \textbf{mIoU}       \\ \hline
MoCoV2                           & \checkmark                        &                                  &                                     &                               & 48.4                 & 20.8                & 70.7                & 26.5                \\
ViT-S                          &                                   &                                  &                                     &                               & 34.2                 & 7.3                 & 54.9                & 15.6                \\
ViT-S                          & \checkmark                        &                                  &                                     &                               & 44.3                 & 21.3                & 70.9                & 36.8                \\
ViT-S                          & \checkmark                        & \checkmark                       &                                     &                               & 47.6                 & 23.4                & 72.2                & 36.8                \\
ViT-S                          & \checkmark                        & \checkmark                       & \checkmark                          &                               & 47.7                 & 24.0                & 72.9                & 38.4                \\
ViT-S                          & \checkmark                        & \checkmark                       & \checkmark                          & \checkmark                    & 48.3                 & 24.5                & 74.4                & 38.3                \\
ViT-B                           & \checkmark                        & \checkmark                       & \checkmark                          & \textbf{}                     & 54.8                 & 26.8                & 74.3                & 39.5                \\
ViT-B                           & \checkmark                        & \checkmark                       & \checkmark                          & \checkmark                    & \textbf{56.9}        & \textbf{28.2}       & \textbf{76.1}       & \textbf{41.0}      
\end{tabular}
  \label{table:ablation}

\end{minipage}\hfill
\begin{minipage}{.42\textwidth}
  \centering
  \captionof{table}{Results on the Cityscapes Dataset (27 Classes). STEGO improves significantly over all baselines in both accuracy and mIoU.}
    \begin{tabular}{c|cc}
                                 & \multicolumn{2}{c}{\textbf{Unsup.}} \\
    \textbf{Model}               & \textbf{Acc.}     & \textbf{mIoU}     \\ \hline
    IIC \citep{iic}              & 47.9                  & 6.4               \\
    MDC \citep{Cho2021PiCIEUS}   & 40.7                  & 7.1               \\
    PiCIE \citep{Cho2021PiCIEUS} & 65.5                  & 12.3              \\
    \textbf{STEGO (Ours)}        & \textbf{73.2}         & \textbf{21.0}    
    \end{tabular}
  \label{table:cityscapes_results}
\end{minipage}
\end{minipage}
\end{figure}
 
\subsection{Ablation Study}
\vspace{-.05in}

To understand the impact of STEGO's architectural components we perform an ablation analysis on the CocoStuff dataset, and report the results in Table \ref{table:ablation}. We examine the effect of using several different backbones in STEGO including MoCoV2, the ViT-Small, and ViT-Base architectures of DINO. We find that ViT-Base is the best feature extractor of the group and leads by a significant margin both in terms of accuracy and mIoU. We also evaluate the several loss function and architecture decisions described in Section \ref{sec:architecture}. In particular, we explore clamping the segmentation feature correspondence tensor at $0$ to prevent the negative pressure from introducing co-linearity (0-Clamp), five-cropping the dataset prior to mining KNNs to improve the resolution of the learning signal (5-Crop), spatially centering the feature correspondence tensor to improve resolution of small objects (SC), and Conditional Random Field post-processing to refine predictions (CRF). We find that these modifications improve both the cluster and linear probe evaluation metrics. 

\vspace{-.1in}
\section{Conclusion}
\vspace{-.1in}

We have found that modern self-supervised visual backbones can be refined to yield state of the art unsupervised semantic segmentation methods. We have motivated this architecture by showing that correspondences between deep features are directly correlated with ground truth label co-occurrence. We take advantage of this strong, yet entirely unsupervised, learning signal by introducing a novel contrastive loss that ``distills'' the correspondences between features. Our system, STEGO, produces low rank representations that cluster into accurate semantic segmentation predictions. We connect STEGO's loss to CRF inference by showing it is equivalent to MLE in Potts models over the entire collection of pixels in our dataset. We show STEGO yields a significant improvement over the prior state of the art, on both the CocoStuff (\textbf{+14 mIoU}) and Cityscapes (\textbf{+9 mIoU}) semantic segmentation challenges. Finally, we justify the architectural decisions of STEGO with an ablation study on the CocoStuff dataset. 


\subsubsection*{Acknowledgments}

We would like to thank Karen Hamilton for proofreading the work and Siddhartha Sen for sponsoring access to the Microsoft Research compute infrastructure. We also thank Jang Hyun Cho for helping us run and evaluate the PiCIE baseline. We thank Kavital Bala, Vincent Sitzmann, Marc Bosch, Desalegn Delelegn, Cody Champion, and Markus Weimer for their helpful commentary on the work.

This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No. 2021323067. Any opinion, findings, and conclusions or recommendations expressed in this material are those of the authors(s) and do not necessarily reflect the views of the National Science Foundation. This research is based upon work supported in part by the Office of the Director of National Intelligence (Intelligence Advanced Research Projects Activity) via 2021-20111000006. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, or the U S Government. The US Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein. This work is supported by the National Science Foundation under Cooperative Agreement PHY-2019786 (The NSF AI Institute for Artificial Intelligence and Fundamental Interactions, http://iaifi.org/)




\bibliography{iclr2022_conference}
\bibliographystyle{iclr2022_conference}

\newpage

\appendix
\section{Appendix}

\subsection{Video and Code}

We include a short video description of our work at {\color{blue} \url{https://aka.ms/stego-video}}.

We also provide training and evaluation code at {\color{blue} \url{https://aka.ms/stego-code}}


\subsection{Additional Results on the Potsdam-3 Dataset}
\label{sec:potsdam}

In addition to our evaluations in Section \ref{sec:evaluation} we compare STEGO to prior art on the Potsdam 3-class aerial image segmentation task presented in \cite{iic}. In Table \ref{table:potsdam-results} We find that STEGO is able to achieve $+12\%$ accuracy compared to the previous state of the art, IIC. We show example qualitative results in Figure \ref{fig:potsdam-results}.

\begin{table}[h]
\centering
\caption{Additional results on the Potsdam-3 aerial image segmentation challenge}
\begin{tabular}{c|c}
\textbf{Model}        & \textbf{Unsup. Acc.} \\ \hline
Random CNN  \citep{iic}          & 38.2                 \\
K-Means  \citep{pedregosa2011scikit}             & 45.7                 \\
SIFT    \citep{SIFT}              & 38.2                 \\
\cite{doersh15}              & 49.6                 \\
\cite{isola16}                 & 63.9                 \\
Deep Cluster  \citep{deep-cluster}         & 41.7                 \\
IIC \citep{iic}                   & 65.1                 \\
\textbf{STEGO (Ours)} & \textbf{77.0}       
\end{tabular}
\label{tab:potsdam-results}
\end{table}


 \begin{figure}[h]
 \centering
 \includegraphics[width=.8\linewidth]{figures/potsdam-results.jpg}
  \caption{Qualitative comparison of STEGO segmentation results on the Potsdam-3 segmentation challenge.}
\label{fig:potsdam-results}
\end{figure}

\newpage

\subsection{Additional Ablation Study}

In addition to the ablation study of Table \ref{table:ablation}, we investigate the effect of each major architectural decision in isolation. We find that in most metrics, removing each architectural component hurts performance.


\begin{table}[h]
\centering
  \caption{Additional architecture ablation study on the CocoStuff Dataset (27 Classes).}

\begin{tabular}{cccccccc|cc|cc}
\textbf{}         & \textbf{\rot{\kern-1.1em 0-Clamp}} & \textbf{\rot{\kern-1.1em 5-Crop}} & \textbf{\rot{\kern-1.1em Pointwise}} & \textbf{\rot{\kern-1.1em CRF}} & \textbf{\rot{\kern-1.1em Self-Loss}} & \textbf{\rot{\kern-1.1em KNN-Loss}} & \textbf{\rot{\kern-1.1em Rand-Loss}} & \multicolumn{2}{c|}{\textbf{Unsupervised}} & \multicolumn{2}{c}{\textbf{Linear Probe}} \\
\textbf{Backbone} &                        &                       &                          &                    &                          &                         &                          & \textbf{Acc.}        & \textbf{mIoU}       & \textbf{Acc.}       & \textbf{mIoU}       \\ \hline
ViT-Small         & \checkmark             & \checkmark            & \checkmark               & \checkmark         & \checkmark               & \checkmark              & \checkmark               & \textbf{48.3}        & \textbf{24.5}       & \textbf{74.4}       & 38.3                \\
MoCoV2          & \checkmark             & \checkmark            & \checkmark               & \checkmark         & \checkmark               & \checkmark              & \checkmark               & 43.1                 & 19.6                & 65.9                & 26.0                \\
ViT-Small         &                        & \checkmark            & \checkmark               & \checkmark         & \checkmark               & \checkmark              & \checkmark               & 42.8                 & 10.3                & 59.3                & 19.3                \\
ViT-Small         & \checkmark             &                       & \checkmark               & \checkmark         & \checkmark               & \checkmark              & \checkmark               & 48.0                 & 23.1                & 73.9                & \textbf{38.9}       \\
ViT-Small         & \checkmark             & \checkmark            &                          & \checkmark         & \checkmark               & \checkmark              & \checkmark               & 50.2                 & 22.3                & 73.7                & 37.7                \\
ViT-Small         & \checkmark             & \checkmark            & \checkmark               &                    & \checkmark               & \checkmark              & \checkmark               & 47.7                 & 24.0                & 72.9                & 38.4                \\
ViT-Small         & \checkmark             & \checkmark            & \checkmark               & \checkmark         &                          & \checkmark              & \checkmark               & 43.0                 & 20.2                & 73.0                & 36.2                \\
ViT-Small         & \checkmark             & \checkmark            & \checkmark               & \checkmark         & \checkmark               &                         & \checkmark               & 47.0                 & 22.2                & 74.0                & 37.7                \\
ViT-Small         & \checkmark             & \checkmark            & \checkmark               & \checkmark         & \checkmark               & \checkmark              &                          & 39.8                 & 12.8                & 65.5                & 29.9               
\end{tabular}

\end{table}

\newpage

\subsection{Additional Qualitative Results}
\label{sec:additional-examples}

\begin{figure}[h]
 \centering
 \includegraphics[width=\linewidth]{figures/cocostuff27_results_additional.jpg}
  \caption{Additional unsupervised semantic segmentation predictions on the CocoStuff 27 class segmentation challenge using STEGO (Ours) and the prior state of the art, PiCIE. Images are not curated.}
\label{fig:additional_results}
\end{figure}


\newpage 
\subsection{Failure Cases}
 \label{sec:failure-cases}
 
 Unsupervised Segmentation is prone to a variety of issues. We include some of the following to segmentations to demonstrate cases where STEGO breaks down. In the first column of Figure \ref{fig:failure_cases} we can see that STEGO improperly segments ground from trees and backgrounds. In the second column we see that STEGO makes an understandable error and assigns the barn floor to the ``outdoor'' class and the barn wall to the ``building'' class. In the third column STEGO misses the boundary between wall and ceiling. The fourth column demonstrates the challenge between food (thing) and food (stuff) characterization. Interestingly PiCIE makes the same type of error both here, and in the barn case. The last column shows an example of STEGO missing a human in the lower left. In this image it is challenging to spot the person, probably because it is grayscale.
 
 \begin{figure}[h]
 \centering
 \includegraphics[width=.8\linewidth]{figures/cocostuff27_results_failures.jpg}
  \caption{STEGO failure cases. }
\label{fig:failure_cases}
\end{figure}


\newpage

\subsection{Feature Correspondences Predict STEGO's Errors}

Section \ref{sec:correspondence} demonstrates how unsupervised feature correspondences serve as an excellent proxy for the true label co-occurrence information. In this section we explore how and where DINO's feature correspondences systematically differ from the ground truth labels, and show that these insights allow us to directly predict STEGO's final confusion matrix. 

More specifically we consider the setting of Section \ref{sec:correspondence}. Instead of computing precision-recall curves from our feature correspondence scores we can instead threshold these scores, select the strongest couplings between the images, and evaluate whether these couplings are between objects of the same class or objects of different classes. In particular, Figure \ref{fig:feature_correspondence_cm} shows a confusion matrix capturing how well DINO feature correspondences between images and their K-Nearest Neighbors align with the ground truth label ontology in the CocoStuff27 dataset. We find that that this analysis predicts many of the areas where the final STEGO architecture fails. In particular, we can see that DINO conflates the ``Food (things)'' and ``Food (stuff)'' and this error also appears in STEGO's confusion matrix in Figure \ref{fig:cocostuff_cm_2}. Likewise both visualizations show confusion between ``appliance'' and ``furniture'', ``window'' and ``wall'', and several other common errors.

This analysis demonstrates that many of STEGO's errors originate from the structure of the DINO features used to train STEGO as opposed to other aspects of the architecture. However we note that the question of whether whether this is an issue with the DINO features, or due to ambiguities in the CocoStuff label ontology is still outstanding. Finally we note that this analysis is able to predict the results of a fully-trained STEGO architecture, and could be used as a way to select better backbones without having to training STEGO.

 \begin{figure}[h]
 \centering
 \includegraphics[width=.7\linewidth]{figures/feature_correspondence_cm.jpg}
  \caption{Normalized matrix of predicted label co-occurrences between an Images and KNNs. This analysis shows where our unsupervised supervisory signal, the DINO feature correspondences, fails to align with the CocoStuff27 label ontology.} 
\label{fig:feature_correspondence_cm}
\end{figure}

\newpage

\subsection{Higher Resolution Confusion Matrices}
 \label{sec:high-res-cm}

\begin{figure}[h]
\centering
  \includegraphics[width=.65\linewidth]{figures/cityscapes_cm.png}
  \captionof{figure}{Confusion Matrix for Cityscapes predictions}
  \label{fig:cityscapes_cm}
\end{figure}

\begin{figure}[h]
\centering
  \includegraphics[width=.65\linewidth]{figures/cocostuff_cm.png}
  \captionof{figure}{Confusion Matrix for CocoStuff predictions}
  \label{fig:cocostuff_cm_2}
\end{figure}

\newpage

\subsection{Relationship with Graph Energy Minimization}
\label{sec:additional_ebm}

In section \ref{sec:ebm} we briefly mention that STEGO's feature correlation distillation loss defined in Equation \ref{eq:corrloss} can be seen as a particular case of Maximum Likelihood (ML) estimation on a undirected graphical model or Ising model. In this section we demonstrate this connection in greater detail using the formalism defined in \ref{sec:ebm}. In particular, we recall the energy for a Potts model:

\begin{equation}
    E(\phi) := \sum_{v_i, v_j \in \V} w(v_i, v_j) \mu(\phi(v_i), \phi(v_j))
\end{equation}

We then construct the Boltzmann Distribution \citep{hinton2002training} yields a normalized distribution over the function space $\Phi$:

\begin{equation}
    p(\phi | w, \mu ) = \frac{\exp(-E(\phi))}{\int_\Phi \exp(-E(\phi')) d\phi'}
\end{equation}

In general, sampling from this probability distribution is difficult because of the often-intractable normalization factor. However, it is easier to compute the maximum likelihood estimate (MLE):

\begin{equation}
\argmax_{\phi \in \Phi} p(\phi | w, \mu ) = \argmax_{\phi \in \Phi} \frac{1}{Z} \exp(-E(\phi))
\end{equation}

Where $Z$ is the unknown constant normalization factor. Simplifying the right-hand side yields: 

\begin{equation}
\argmax_{\phi \in \Phi} p(\phi | w, \mu ) = \argmin_{\phi \in \Phi} E(\phi) = \argmin_{\phi \in \Phi} \sum_{v_i, v_j \in \V} w(v_i, v_j) \mu(\phi(v_i), \phi(v_j))
\end{equation}

We are now in the position to connect this to the STEGO loss function. First, we take our nodes $\V$ to be the set of all spatial locations across our entire dataset of images. For concreteness we can represent $v \in \V$ by the tuple $(n, h, w)$ where $h,w$ represent height and width $n$ represents the image number. We now let $\phi(v_i)$ be the output of the segmentation head, $s_{v_i}$, at the image and spatial location $v_i$. Using cosine distance, $d_{cos}(x,y) = 1- \frac{x}{|x|}\frac{y}{|y|}$  as the compatibility function, $\mu$, yields the following:

\begin{equation}
     = \argmin_{\mathcal{S}} \sum_{v_i, v_j \in \V} -w(v_i, v_j) \frac{s_{v_i}}{|s_{v_i}|}\frac{s_{v_j}}{|s_{v_j}|}
\end{equation}

Wherte the argmin now ranges over the parameters of the segmentation head $\mathcal{S}$. We can now observe that the sum over all pairs $v_{i},v_{j} \in \V$ can be written as a sum over pairs of images $x,y \in X$ and pairs of spatial locations $(h,w), (i,j)$ where we note that $(i,j)$ in this context refers to the spatial coordinates of image $y$ as in \ref{sec:correspondence} and not the indices of the vertices. 

\begin{equation}
     = \argmin_{\mathcal{S}} \sum_{x,y \in X} \sum_{hwij} -W(x,y)_{hwij} S(x,y)_{hwij}
\end{equation}

Where we define $S(x,y)$ to be the segmentation feature correlation tensor for images $x,y$ as defined in Section \ref{sec:distillation}. Finally letting $W(x,y)_{hwij} = F_{hwij} - b$ we recover our loss:

\begin{equation}
     \argmax_{\phi \in \Phi} p(\phi | w, \mu )  = \argmin_{\mathcal{S}} \sum_{x,y \in X} \mathcal{L}_{\mathit{simple-corr}}(x,y,b)
\end{equation}

Finally we note that in practice we approximate the minimization using minibatch SGD, and our inclusion of KNN and Self-correspondence distillation changes the weight function $w$, but does not change its functional form. 

Switching to the ML formulation of this problem allows us to solve this optimization for $\phi$ by gradient descent on the parameters of the segmentation head, $\mathcal{S}$, and makes this computationally tractable. For large image datasets that can contain millions of high-resolution images, the induced graph can contain billions of image locations. Other graph embedding and clustering approaches such as Spectral methods require solving for eigenvalues of the graph Laplacian, which can take $O(|\V|^3)$ time \citep{yan2009fast}. More recent attempts to accelerate Spectral clustering such as \citep{yan2009fast} and \citep{han2017mini} further assume a ``Nonparametric'' structure on the function $\phi$, where a separate cluster assignment is learned for each vertex. This assumption of a ``nonparametric'' function $\phi$ can be undesirable as one cannot cluster or embed new data without recomputing the entire clustering. In contrast, STEGO's backbone and segmentation head act as a parametric form for the function $\phi$ allowing the approach to output predictions for novel images.

\subsection{Continuous, Unsupervised, and Mini-batch CRF}
\label{sec:crf}

\begin{figure}[h]
    \centering
    \includegraphics[width=.7\linewidth]{figures/continuous_crf.jpg}
    \caption{Unsupervised CRF solutions for discrete (middle) and continuous (right) code spaces. In the discrete case we mark the boundaries between classes, in the continuous case we visualize the top 3 dimensions of the code space.}
    \label{fig:crf}
\end{figure}

Fully connected Gaussian Conditional Random Fields (CRFs) \citep{crf} are an extremely popular addition to semantic segmentation architectures. The CRF has the ability to improve initial predictions of locations, and can ``sharpen'' predictions to make them consistent with edges and areas with consistent color in the original image. CRF post-processing for refining supervised and weakly supervised semantic segmentation predictions is ubiquitous in the literature \citep{crf,chen2014semantic,long2015fully,liu2020leveraging,interpixel}. Recently, new connections between CRF message passing and convolutional networks have allowed CRFs to be embedded into existing models \citep{chen2017rethinking,teichmann2018convolutional} and trained jointly for better performance. By connecting the STEGO correspondence distillation loss to the energy of an undirected model on image pixels we can use the same minibatch MLE strategy to estimate other similar graphical models. For example, in the fully connected Gaussian edge potential CRF, one forms a pairwise potential function potential function for the pixels of a single image:

\begin{equation}
    \label{eq:crf}
    w_{crf}(v_i,v_j) = a \exp \left( - \frac{|p_i - p_j|^2}{2 \theta_\alpha^2} - \frac{|I_i - I_j|^2}{2 \theta_\beta^2} \right) + b \exp \left(  - \frac{|p_i - p_j|^2}{2 \theta_\gamma^2} \right)
\end{equation}

Where $p_i$ represent the pixel coordinates associated with node $v_i$ and $I_i$ represents pixel colors associated with node $v_i$. The parameters $a,b,\theta_\alpha,\theta_\beta,\theta_\gamma$ are hyperparameters and control the behavior of the model. These parameters balance the effect of long- and short-range color similarities against smoothness. The CRF directly learns a pixel-wise array of probabilistic class assignments over $k$ labels corresponding to the probability simplex code space $\C = \mathcal{P}(l)$ and a non-parametric clustering function $f$. For a compatibility function $\mu$ the CRF chooses the Potts Model \citep{potts1952some}: $\mu_{potts}(\phi(v_i), \phi(v_j)) := \P(\phi(v_i) \neq \phi(v_j))$. 

With this setting of the weights and compatibility function, we directly recover the binary potentials of the fully connected Gaussian edge potential CRF \citep{fullcrf}. We can also add the unary potentials which are often the outputs of another model. However, for our analysis we explore the case without unary potentials which yields an ``unsupervised'' variant of the CRF. However, without external unary potential terms, the strictly positive similarity kernel encourages the maximum likelihood estimator (MLE) of the graph to be the constant function. To rectify this, we can add small negative constant, $-b$, to the weight tensor to push unrelated pixels apart. This negative force is the direct analogue of the negative pressure hyper-parameter in STEGO and can be interpreted through the lens of negative sampling  \citep{mikolov2013distributed}. This negative shift also appears in the word2vec and graph2vec embedding techniques \citep{narayanan2017graph2vec,levy2014neural}. Our shifted CRF potential encourages natural clusters to form that respect the structure of the potentials that capture similarities in pixel colors and locations. In the discrete case, solutions to this equation resemble superpixel algorithms such as SLIC \citep{zhang2015slic}. Additionally lifting this to the continuous code space and provide a natural continuous generalization of superpixels and seems to avoid challenging local minima. We illustrate these solutions to just the unsupervised CRF potential in Figure \ref{fig:crf}. Finally, we note that the second term of Equation \ref{eq:crf}, referred to as the smoothness kernel, matches IIC's notion of local class consistency. However, we found that adding these CRF terms to the self-correspondence loss of STEGO did not improve performance.

\newpage

\subsection{Implementation Details}
 \label{sec:implementation-details}

\paragraph{Model} STEGO uses the ``ViT-Base'' architecture of DINO pre-trained on ImageNet. This backbone was trained using self-supervision without access to ground-truth labels. We use the ``teacher'' weights when creating our backbone. We take the final layer of spatially varying features and apply a small amount ($p=0.1$) of channel-wise dropout \citep{dropout} before using them throughout the architecture during training. Our segmentation head consists of a linear network and a two-layer ReLU MLP added together and outputs a $70$ dimensional vector. We use the Adam optimizer \citep{adam} with a learning rate of $0.0005$ and a batch size of $32$. To make our losses resolution independent we sample 121 random spatial locations in the source and target implementations and use grid sampling \citep{stn} to sample features from the backbone and segmentation heads. Our cluster probe is trained alongside the STEGO architecture using a minibatch k-means loss where closeness is measured by cosine distance. Cluster and linear probes are trained with separate Adam optimizers using a learning rate of $.005$

\paragraph{Datasets} We use the training and validation sets of Cocostuff described first in \cite{iic} and used throughout the literature including in \cite{Cho2021PiCIEUS}. We note that the validation set used in \cite{iic} is a subset of the full CocoStuff validation set and we use this validation subset to be consistent with prior benchmarks. We note that using the full validation set does not change results significantly. When five-cropping images we use a target size of $(.5h,.5w)$ for each crop where $h,w$ are the original image height and width. Training images are then scaled to have minor axis equal to $224$ and are then center cropped to $(224,224)$, validation images are first scaled to $320$ then are center cropped to $(320,320)$. All image resizing uses bilinear interpolation and resizing of target tensors for evaluation uses nearest neighbor interpolation.

\paragraph{CRF} We use PyDenseCRF \citep{fullcrf} with 10 iterations with parameters $a=4, b=3, \theta_\alpha=67, \theta_\beta=3, \theta_\gamma=1$ as written in Section \ref{sec:crf}.

\paragraph{Compute} All experiments use PyTorch \citep{pytorch} v1.7 pre-trained models, on an Ubuntu 16.04 Azure NV24 Virtual Machine with Python 3.6. Experiments use PyTorch Lightning for distributed and multi-gpu training when necessary \citep{pytorch-lightning}.

\paragraph{Hyperparameters}
We use the following hyperparameters for our results in Tables \ref{table:cocostuff_results} and \ref{table:cityscapes_results}:


\begin{table}[h]
\caption{Hyperparameters used in STEGO}
\centering
\begin{tabular}{ccc}
\textbf{Parameter} & \textbf{Cityscapes} & \textbf{CocoStuff} \\ \hline
$\lambda_{rand}$   & 0.91                & 0.15               \\
$\lambda_{knn}$    & 0.58                & 1.00               \\
$\lambda_{self}$   & 1.00                & 0.10               \\
$b_{rand}$         & 0.31                & 1.00               \\
$b_{knn}$          & 0.18                & 0.20               \\
$b_{self}$         & 0.46                & 0.12              
\end{tabular}
\end{table}

\newpage

\subsection{A Heuristic for Setting Hyper-parameters}

Setting hyperparameters without cross-validation on ground truth data can be difficult and this is an outstanding challenges with the STEGO architecture that we hope can be solved in future work. Nevertheless we have identified some key intuition to guide manual hyperparameter tuning. More specifically, we find that the most important factor affecting performance is the balance of positive and negative forces. Too much negative feedback and vectors will all push apart and clusters will not form well, too much positive feedback and the system will tend towards a small number of clusters. To debug this balance, we found it useful to visualize the distribution of feature correspondence similarities as a function of training step as shown in Figure \ref{fig:hyperparameter-intuition}. A balanced system (Orange distribution) will tend towards a bi-modal distribution with peaks at alignment 1 or orthogonality at 0. This bi-modal structure is indicative that there is some clustering within images, but that not everything is assigned to the same cluster. Pink and blue distributions show too much positive and negative signal respectively. We find that given a reasonable balance of the $\lambda$'s, this balance can be achieved by tuning the $b$s to achieve the desired balance.

\begin{figure}[h]
    \centering
    \includegraphics[width=.7\linewidth]{figures/hyperparameter_intuition.jpg}
    \caption{Distributions of feature correspondences between an image and itself across three different hyper-parameter settings. The orange curve and distribution shows a proper balance between attractive and repulsive forces allowing some pairs features to cluster together (the peak at 1) and other pairs of features to orthogonalize (the peak at 0)}
    \label{fig:hyperparameter-intuition}
\end{figure}

\newpage

\subsection{A note on 5-Crop Nearest Neighbors}

We found that pre-processing the dataset by 5-cropping images was a simple and effective way to improve the spatial resolution of STEGO and the quality of K-Nearest Neighbors. We consider each resulting 5-crop as a separate image when computing KNNs and patches from the same image are valid KNNs. Figure \ref{fig:5crop} shows the distribution of these self-matches for the CocoStuff dataset. We note that the majority of patches do not have any nearest neighbors from the same image.



\begin{figure}[h]
    \centering
    \includegraphics[width=.7\linewidth]{figures/5crop_analysis.png}
    \caption{Number of patches from the same image found within each patch's 7 nearest neighbors}
    \label{fig:5crop}
\end{figure}

\end{document}
