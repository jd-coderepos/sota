

\section{Experiments}

In this section, we evaluate the effectiveness of ThunderNet on PASCAL VOC \cite{everingham2010pascal} and COCO \cite{lin2014microsoft} benchmarks.
Then we conduct ablation studies to evaluate our design.

\begin{table*}[!t]
\scriptsize
\centering
\begin{tabular}{p{11em}|P{10em}|P{8em}|P{6em}|P{4em}}
Model & Backbone & Input & MFLOPs & mAP \\ \hlineB{2.5}
YOLOv2 \cite{redmon2017yolo9000} & Darknet-19 &  & 17400 & 76.8 \\
SSD300* \cite{liu2016ssd} & VGG-16 &  & 31750 & 77.5 \\
SSD321 \cite{fu2017dssd} & ResNet-101 &  & 15400 & 77.1 \\
DSSD321 \cite{fu2017dssd} & ResNet-101 + FPN &  & 21200 & 78.6 \\
R-FCN \cite{dai2016r} & ResNet-50 &  & 58900 & 77.4 \\ \hline
Tiny-YOLO \cite{redmon2017yolo9000} & Tiny Darknet &  & 3490 & 57.1 \\
D-YOLO \cite{mehta2018object} & Tiny Darknet &  & 2090 & 67.6 \\
MobileNet-SSD \cite{wang2018pelee} & MobileNet &  & 1150 & 68.0 \\
Pelee \cite{wang2018pelee} & PeleeNet &  & 1210 & 70.9 \\
Tiny-DSOD \cite{li2018tiny} & DDB-Net + D-FPN &  & 1060 & 72.1 \\ \hline
ThunderNet (\emph{ours}) & SNet49 &  & \textbf{250} & 70.1 \\
ThunderNet (\emph{ours}) & SNet146 &  & \textbf{461} & \textbf{75.1} \\
ThunderNet (\emph{ours}) & SNet535 &  & 1287 & \textbf{78.6} \\
\end{tabular}
\vspace{3pt}
\caption{
Evaluation results on VOC 2007 test.
ThunderNet surpasses competing models with significantly less computational cost.
}
\label{table:results-voc}
\end{table*}

\begin{table*}[!t]
\setlength{\tabcolsep}{10pt}
\centering
\scriptsize
\begin{tabular}{l|c|c|c|ccc}
Model & Backbone & Input & MFLOPs & AP & AP & AP \\ \hlineB{2.5}
YOLOv2 \cite{redmon2017yolo9000} & Darknet-19 &  & 17500 & 21.6 & 44.0 & 19.2 \\
SSD300* \cite{liu2016ssd} & VGG-16 &  & 35200 & 25.1 & 43.1 & 25.8 \\ 
SSD321 \cite{fu2017dssd} & ResNet-101 &  & 16700 & 28.0 & 45.4 & 29.3 \\
DSSD321 \cite{fu2017dssd} & ResNet-101 + FPN &  & 22300 & 28.0 & 46.1 & 29.2 \\
Light-Head R-CNN \cite{ma2018shufflenet} & ShuffleNetV2* &  & 5650 & 23.7 & - & - \\ \hline
MobileNet-SSD \cite{howard2017mobilenets} & MobileNet &  & 1200 & 19.3 & - & - \\
MobileNet-SSDLite \cite{sandler2018mobilenetv2} & MobileNet &  & 1300 & 22.2 & - & - \\
MobileNetV2-SSDLite \cite{sandler2018mobilenetv2} & MobileNetV2 &  & 800 & 22.1 & - & - \\
Pelee \cite{wang2018pelee} & PeleeNet &  & 1290 & 22.4 & 38.3 & 22.9 \\
Tiny-DSOD \cite{li2018tiny} & DDB-Net + D-FPN &  & 1120 & 23.2 & 40.4 & 22.8 \\ \hline
ThunderNet (\emph{ours}) & SNet49 &  & \textbf{262} & 19.2 & 33.7 & 19.7 \\
ThunderNet (\emph{ours}) & SNet146 &  & \textbf{473} & \textbf{23.7} & 40.3 & \textbf{24.6} \\
ThunderNet (\emph{ours}) & SNet535 &  & 1300 & \textbf{28.1} & \textbf{46.2} & \textbf{29.6} \\
\end{tabular}
\vspace{3pt}
\caption{
Evaluation results on COCO test-dev.
ThunderNet with SNet49 achieves MobileNet-SSD level accuracy with 22\% of the FLOPs.
ThunderNet with SNet146 achieves superior accuracy to prior lightweight one-stage detectors with merely 40\% of the FLOPs.
ThunderNet with SNet535 rivals large detectors with significantly less computational cost.
}
\label{table:results-coco}
\end{table*}

\subsection{Implementation Details}
\label{section:implementation-details}

Our detectors are trained end-to-end on 4 GPUs using synchronized SGD with a weight decay of 0.0001 and a momentum of 0.9.
The batch size is set to 16 images per GPU.
Each image has 2000/200 RoIs for training/testing.
For efficiency, the input resolution of 320320 pixels is used instead of 600 or 800 pixels in common large two-stage detectors.
Multi-scale training with \{240, 320, 480\} pixels is adopted.
As the input resolution is small, we use heavy data augmentation \cite{liu2016ssd}.
The networks are trained for 62.5K iterations on VOC dataset and 375K iterations on COCO dataset.
The learning rate starts from 0.01 and decays by a factor of 0.1 at 50\% and 75\% of the total iterations.
Online hard example mining \cite{shrivastava2016training} is adopted and Soft-NMS \cite{bodla2017soft} is used for post-processing.
Cross-GPU Batch Normalization (CGBN) \cite{peng2018megdet} is used to learn batch normalization statistics.

\subsection{Results on PASCAL VOC}

PASCAL VOC dataset consists of natural images drawn from 20 classes.
The networks are trained on the union set of VOC 2007 trainval and VOC 2012 trainval, and we report single-model results on VOC 2007 test.
The results are exhibited in Table~\ref{table:results-voc}.

ThunderNet surpasses prior state-of-the-art lightweight one-stage detectors.
ThunderNet with SNet49 outperforms MobileNet-SSD with merely 21\% of the FLOPs, while the SNet146-based model surpasses Tiny-DSOD by 2.9 mAP with about 43\% of the FLOPs.
Moreover, ThunderNet with SNet146 performs better than Tiny-DSOD by 6.5 mAP under similar computational cost.

Furthermore, ThunderNet achieves superior results to state-of-the-art large object detectors such as YOLOv2 \cite{redmon2017yolo9000}, SSD300* \cite{liu2016ssd}, SSD321 \cite{liu2016ssd} and R-FCN \cite{dai2016r}, and is on a par with DSSD321 \cite{fu2017dssd}, but reduces the computational cost by orders of magnitude.
We note that the backbone of ThunderNet is significantly weaker and smaller than the large detectors.
It demonstrates that ThunderNet achieves a much better trade-off between accuracy and efficiency.

\subsection{Results on MS COCO}

MS COCO dataset consists of natural images from 80 object categories.
Following common practice \cite{lin2017feature,li2017light}, we use trainval35k for training, minival for validation, and report single-model results on test-dev.

As shown in Table~\ref{table:results-coco}, ThunderNet with SNet49 achieves MobileNet-SSD level accuracy with 22\% of the FLOPs.
ThunderNet with SNet146 surpasses MobileNet-SSD \cite{howard2017mobilenets}, MobileNet-SSDLite \cite{sandler2018mobilenetv2}, and Pelee \cite{wang2018pelee} with less than 40\% of the computational cost.
It is noteworthy that our approach achieves considerably better AP, which suggests our model performs better in localization.
This is consistent with our initial motivation to design two-stage real-time detectors.
Compared with Tiny-DSOD \cite{li2018tiny}, ThunderNet achieves better AP but worse AP with 42\% of the FLOPs.
We conjecture that deep supervision and feature pyramid in Tiny-DSOD contribute to better classification accuracy.
However, ThunderNet is still better in localization.

ThunderNet with SNet535 achieves significantly better detection accuracy under comparable computational cost.
As shown in Table~\ref{table:results-coco}, ThunderNet surpasses other one-stage counterparts by at least 4.8 AP, 5.8 AP and 6.7 AP.
The gap in AP is larger than the gap in AP, which means our model provides more accurate bounding boxes than other detectors.
This further demonstrates that two-stage detectors are prior to one-stage detectors in real-time detection task.
Fig.~\ref{figure:coco-visualization} visualizes several examples on COCO test-dev.



\begin{figure}[!t]
\centering
\includegraphics[width=0.47\textwidth]{figures/images/visualize.png}
\caption{Examples visualization on COCO test-dev.}
\label{figure:coco-visualization}
\end{figure} 
We also compare ThunderNet with large one-stage detectors.
ThunderNet with SNet146 surpasses YOLOv2 \cite{redmon2017yolo9000} with 37 fewer FLOPs.
And ThunderNet with SNet535 significantly outperforms YOLOv2 and SSD300 \cite{liu2016ssd}, and rivals SSD321 \cite{fu2017dssd} and DSSD321 \cite{fu2017dssd}.
It suggests that ThunderNet is not only efficient but highly accurate.

\subsection{Ablation Experiments}

\subsubsection{Input Resolution}
\label{section:ablation-study-input-resolution}

We first explore the relationship between the input resolution and the backbone.
Table~\ref{table:input-resolution} reveals that large backbones with small images and small backbones with large images are both not optimal.
There is a trade-off between the two factors.
On the one hand, small images lead to low-resolution feature maps and induce severe loss of detail features.
It is hard to be remedied by simply increasing the capacity of the backbones.
On the other hand, small backbones are too weak to encode sufficient information from large images.
The backbone and the input images should match for a better balance between the representation ability and the resolution of the feature maps.

\vspace{-5pt}
\subsubsection{Backbone Networks}
\label{section:ablation-study-backbone-networks}

We then evaluate the design of the backbones.
SNet146 and SNet49 are used as the baselines.
SNet146 achieves 32.5\% top-1 error on ImageNet classification and 23.6 AP on COCO test-dev (Table~\ref{table:ablation-study-backbone-networks}(a)), while SNet49 achieves 39.7\% top-1 error and 19.1 AP (Table~\ref{table:ablation-study-backbone-networks}(e)).

\vspace{-12pt}
\paragraph{55 Depthwise Convolutions.}

We evaluate the effectiveness of 55 depthwise convolutions on SNet146.
We first replace all 55 depthwise convolutions with 33 depthwise convolutions.
For fair comparison, the channels from Stage2 to Stage4 are slightly increased to maintain the computational cost unchanged.
This model performs worse on both image classification (by 0.2\%) and object detection (by 0.9 AP) (Table~\ref{table:ablation-study-backbone-networks}(b)).
Compared with 33 depthwise convolutions, 55 depthwise convolutions considerably increase the receptive fields, which helps in both tasks.

We then add another 33 depthwise convolution before the first 11 convolution in all building blocks as in ShuffleNetV2* \cite{ma2018shufflenet}.
The number of channels is kept unchanged as the baseline.
This model is comparable on image classification, but slightly worse on object detection (by 0.3 AP) (Table~\ref{table:ablation-study-backbone-networks}(c)).
As this model and SNet146 have the same receptive fields theoretically, we conjecture that 55 depthwise convolutions can provide larger valid receptive fields, which is especially crucial in object detection.

\vspace{-12pt}
\paragraph{Early-stage and Late-stage Features.}

To investigate the trade-off between early-stage and late-stage features, we first add a 1024-channel Conv5 in SNet146.
The channels in the early stages are reduced accordingly.
This model slightly improves the top-1 error, but reduces AP by 0.4 (Table~\ref{table:ablation-study-backbone-networks}(d)).
A wide Conv5 generates more discriminative features, which improves the classification accuracy.
However, object detection focuses on both classification and localization.
Increasing the channels in early stages encodes more detail information, which is beneficial for localization.

For SNet49, we first remove Conv5 in SNet49 and increase the channels from Stage2 to Stage4.
Table~\ref{table:ablation-study-backbone-networks}(f) shows that both the classification and the detection performance suffer from severe degradation.
Removing Conv5 cuts the output channels of the backbone by half, which hinders the model from learning adequate information.

We then extend Conv5 to 1024 channels as in the original ShuffleNetV2.
The early-stage channels are compressed to maintain the same overall computational cost.
This model surpasses SNet49 on image classification by 0.8\%, but performs worse on object detection (Table~\ref{table:ablation-study-backbone-networks}(g)).
By leveraging a wide Conv5, this model benefits from more high-level features in image classification.
However, it suffers from the lack of low-level features in object detection.
It further demonstrates the differences between image classification and object detection.

\begin{table}[!t]
\setlength{\tabcolsep}{10pt}
\centering
\scriptsize
\begin{tabular}{l|c|c|P{3em}}
Backbone & Input & MFLOPs & AP \\ \hlineB{2.5}
SNet49 &  & 262 & \textbf{19.2} \\
SNet146 &  & 267 & 18.7 \\
SNet535 &  & 265 & 13.2 \\ \hline
SNet49 &  & 506 & 22.0 \\
SNet146 &  & 473 & \textbf{23.7} \\
SNet535 &  & 512 & 20.2 \\
\end{tabular}
\vspace{3pt}
\caption{Evaluation of different input resolutions on COCO test-dev. Large backbones with small images and small backbones with large images are both not optimal.}
\label{table:input-resolution}
\end{table}

\begin{table}[!t]
\scriptsize
\centering
\begin{tabular}{c@{\hskip 2pt}l|c|c|P{3em}}
\multicolumn{2}{l|}{Backbone} & MFLOPs & Top-1 Err. & AP \\ \hlineB{2.5}
(a) & SNet146 & 146 & 32.5 & \textbf{23.7} \\
(b) & SNet146 + 33 DWConv & 145 & 32.7 & 22.7 \\
(c) & SNet146 + double 33 DWConv & 143 & 32.4 & 23.3 \\
(d) & SNet146 + 1024-d Conv5 & 147 & \textbf{32.3} & 23.2 \\ \hline
(e) & SNet49 & 49 & 39.7 & \textbf{19.2} \\
(f) & SNet49 + No Conv5 & 49 & 40.8 & 18.2 \\
(g) & SNet49 + 1024-d Conv5 & 49 & \textbf{38.9} & 18.8 \\
\end{tabular}
\vspace{3pt}
\caption{
Evaluation of different backbones on ImageNet classification and COCO test-dev.
\textbf{DWConv}: depthwise convolution.
}
\label{table:ablation-study-backbone-networks}
\end{table}

\begin{table}[!t]
\setlength{\tabcolsep}{10pt}
\centering
\scriptsize
\begin{tabular}{l|c|c|P{3em}}
Backbone & MFLOPs & Top-1 Err. & AP \\ \hlineB{2.5}
ShuffleNetV1 \cite{zhang2018shufflenet} & 137 & 34.8 & 20.8 \\
ShuffleNetV2 \cite{ma2018shufflenet} & 147 & \textbf{31.4} & 22.7 \\
ShuffleNetV2* \cite{ma2018shufflenet} & 145 & 32.2 & 23.2 \\
Xception \cite{chollet2017xception} & 145 & 34.1 & 23.0 \\
MobileNetV2 \cite{sandler2018mobilenetv2} & 145 & 32.9 & 22.7 \\ \hline
SNet146 & 146 & 32.5 & \textbf{23.7} \\
\end{tabular}
\vspace{3pt}
\caption{Evaluation of lightweight backbones on COCO test-dev.
SNet146 achieves better detection results though the classification accuracy is lower.}
\label{table:backbones}
\end{table}

\vspace{-12pt}
\paragraph{Comparison with Lightweight Backbones.}

We further compare SNet with other lightweight backbones in ThunderNet framework (Table~\ref{table:backbones}).
SNet146 surpasses Xception \cite{chollet2017xception}, MobileNetV2 \cite{sandler2018mobilenetv2}, and ShuffleNetV1/V2/V2* \cite{zhang2018shufflenet,ma2018shufflenet} on object detection under similar FLOPs.
These results further demonstrate the effectiveness of our design.

\vspace{-5pt}
\subsubsection{Detection Part}
\label{section:ablation-study-detection-part}

We also investigate the effectiveness of the design of the detection part in ThunderNet.
Table~\ref{table:ablation-study-architecture-modules} describes the comparison of the model variants in the experiments.

\vspace{-12pt}
\paragraph{Baseline.}

We choose a compressed Light-Head R-CNN \cite{li2017light} with SNet146 as the baseline.
 is upsampled by  to obtain the same downsampling rate.
 and  are then squeezed to 245 channels and sent to RPN and RoI warping respectively.
We use a 256-channel 33 convolution in RPN and a 2048-d \emph{fc} layer in R-CNN subnet.
This model requires 703 MFLOPs and achieves 21.9 AP (Table~\ref{table:ablation-study-architecture-modules}(a)).
Besides, we would mention that multi-scale training, CGBN \cite{peng2018megdet}, and Soft-NMS \cite{bodla2017soft} gradually improve the baseline by 1.4 AP (from 20.5 to 21.9 AP).

\vspace{-12pt}
\paragraph{RPN and R-CNN subnet.}

We first replace the 33 convolution in RPN with a 55 depthwise convolution and a 11 convolution.
The number of output channels remains unchanged.
This design reduces the computational cost by 28\% without harming the accuracy (Table~\ref{table:ablation-study-architecture-modules}(b)).
We then halve the number of outputs of the \emph{fc} layer in R-CNN subnet to 1024, which achieves a further 13\% compression on the FLOPs with a marginal decrease of 0.2 AP. (Table~\ref{table:ablation-study-architecture-modules}(c)).
These results demonstrate that heavy RPN and R-CNN subnet introduce great redundancy for lightweight detectors.
More details will be discussed in Sec.~\ref{section:large-backbone-or-heavy-head}.

\vspace{-12pt}
\paragraph{Context Enhancement Module.}

\begin{table}[!t]
\scriptsize
\setlength{\tabcolsep}{4pt}
\centering
\begin{tabular}{cccccc|ccc|c}
 & BL & SRPN & SRCN & CEM & SAM & AP & AP & AP & MFLOPs \\ \hlineB{2.5}
(a) & \checkmark & & & & & 21.9 & 37.6 & 22.5 & 714 \\
(b) & & \checkmark & & & & 21.8 & 37.5 & 22.4 & 516 \\
(c) & & \checkmark & \checkmark & & & 21.6 & 37.4 & 22.2 & \textbf{448} \\
(d) & & \checkmark & \checkmark & \checkmark & & 23.3 & 39.9 & 24.0 & 449 \\
(e) & & \checkmark & \checkmark & & \checkmark & 23.0 & 39.0 & 24.0 & 473 \\
(f) & & \checkmark & \checkmark & \checkmark & \checkmark & \textbf{23.7} & \textbf{40.3} & \textbf{24.6} & 473 \\
\end{tabular}
\vspace{3pt}
\caption{
Ablation studies on the detection part on COCO test-dev.
We use a compressed Light-Head R-CNN with SNet146 as the baseline (BL), and gradually add small RPN (SRPN), small R-CNN (SRCN), Context Enhancement Module (CEM) and Spatial Attention Module (SAM) for ablation studies.
}
\label{table:ablation-study-architecture-modules}
\end{table}

We then insert Context Enhancement Module (CEM) after the backbone.
The output feature map of CEM is used for both RPN and RoI warping.
CEM achieves thorough improvements of 1.7 AP, 2.5 AP and 1.8 AP with negligible increase on FLOPs (Table~\ref{table:ablation-study-architecture-modules}(d)).
The combination of the multi-scale feature maps introduces semantic and context information of different levels, which improves the representation ability.

\vspace{-12pt}
\paragraph{Spatial Attention Module.}



\begin{figure}[!t]
\centering
\includegraphics[width=0.47\textwidth]{figures/images/attention-map.pdf}
\caption{Visualization of the feature map before RoI warping. Spatial Attention Module (SAM) enhances the features in the foreground regions and weakens those in the background regions.}
\label{figure:attention-feature-map}
\end{figure} 
Adopting Spatial Attention Module (SAM) without CEM (Table~\ref{table:ablation-study-architecture-modules}(e)) improves AP by 1.4 with merely 5\% extra computational cost compared with Table~\ref{table:ablation-study-architecture-modules}(c).
Fig.~\ref{figure:attention-feature-map} visualizes the feature maps before RoI warping in Table~\ref{table:ablation-study-architecture-modules}(c) and Table~\ref{table:ablation-study-architecture-modules}(e).
It is clear that SAM effectively refines the feature distribution with foreground feature enhanced and background features weakened.

At last, we adopt both CEM and SAM to compose the complete ThunderNet (Table~\ref{table:ablation-study-architecture-modules}(f)).
This setting improves AP by 1.8, AP by 2.7, and AP by 2.1 over the baseline while reducing the computational cost by 34\%.
These results have demonstrated the effectiveness of our design.

\vspace{-5pt}
\subsubsection{Balance between Backbone and Detection Head}
\label{section:large-backbone-or-heavy-head}

We further explore the relationship between the backbone and the detection head.
Two models are used in the experiments: a \emph{large-backbone-small-head} (LBSH) model and a \emph{small-backbone-large-head} (SBLH) model.
The LBSH model is ThunderNet with SNet146.
The SBLH model uses SNet49 and a heavier head:  is 10, and a 2048-d \emph{fc} layer is used in R-CNN subnet.
As shown in Table~\ref{table:ablation-study-light-heavy-head}, the LBSH model outperforms the SBLH one by 3.4 AP even with less FLOPs.
It suggests that \emph{the large-backbone-small-head design is better than the small-backbone-large-head design for lightweight two-stage detectors}.
We conjecture that the capability of the backbone and the detection head should match.
In the small-backbone-large-head design, the features from the backbone are relatively weak, which makes the powerful detection head redundant.

\begin{table}[!t]
\scriptsize
\centering
\begin{tabular}{l|c|c|c|c|c}
Model & Backbone & RPN & Head & Total & AP \\ \hlineB{2.5}
large-backbone-small-head & 338 & 43 & 92 & 473 & 23.6 \\
small-backbone-large-head & 154 & 70 & 286 & 510 & 20.2 \\
\end{tabular}
\vspace{3pt}
\caption{
MFLOPs and AP of different detection head designs on COCO test-dev.
The large-backbone-small-head model outperforms the small-backbone-large-head model with less FLOPs.
}
\label{table:ablation-study-light-heavy-head}
\end{table}

\begin{table}[!t]
\setlength{\tabcolsep}{10pt}
\centering
\scriptsize
\begin{tabular}{l|c|c|c}
Model & ARM & CPU & GPU \\ \hlineB{2.5}
Thunder w/ SNet49 & 24.1 & 47.3 & 267 \\
Thunder w/ SNet146 & 13.8 & 32.3 & 248 \\
Thunder w/ SNet535 & 5.8 & 15.3 & 214 \\
\end{tabular}
\vspace{3pt}
\caption{
Inference speed in fps on Snapdragon 845 (ARM), Xeon E5-2682v4 (CPU) and GeForce 1080Ti (GPU).
}
\label{table:inference-speed}
\end{table}

\subsection{Inference Speed}
\label{section:inference-speed}

At last, we evaluate the inference speed of ThunderNet on Snapdragon 845 (ARM), Xeon E5-2682v4 (CPU) and GeForce 1080Ti (GPU).
On ARM and CPU, the inference is executed with a \emph{single thread}.
The batch normalization layers are merged with the preceding convolutions for faster inference speed.
The results are shown in Table~\ref{table:inference-speed}.
ThunderNet with SNet49 achieves real-time detection on both ARM and CPU at 24.1 and 47.3 fps, respectively.
To the best of our knowledge, this is the \emph{first} real-time detector and the \emph{fastest} single-thread speed on ARM platforms ever reported.
ThunderNet with SNet146 runs at 13.8 fps on ARM and runs in real-time on CPU at 32.3 fps.
All three models run at over 200 fps on GPU.
These results suggest that ThunderNet is highly efficient in real-world applications.
