

\documentclass[nohyperref]{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} \usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{tablefootnote}
\usepackage{amssymb}
\usepackage[T1]{fontenc}
\usepackage{xspace}
\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{inconsolata}

\usepackage{hyperref}

\newcommand*{\xdash}[1][3em]{\rule{1cm}{0.15mm}{}}

\newcommand{\theHalgorithm}{\arabic{algorithm}}

\newcommand{\interalia}[1]{\citep[\emph{inter alia}]{#1}}
\newcommand{\ensuretext}[1]{#1}
\newcommand{\marker}[2]{\ensuremath{^{\textsc{#1}}_{\textsc{#2}}}}
\newcommand{\arkcomment}[3]{\ensuretext{\textcolor{#3}{[#1 #2]}}}

\newcommand{\nascomment}[1]{\arkcomment{\marker{NA}{S}}{#1}{blue}}
\newcommand{\weijia}[1]{\arkcomment{\marker{W}{S}}{#1}{red}}
\newcommand{\scott}[1]{\arkcomment{\marker{S}{Y}}{#1}{violet}}
\newcommand{\sewon}[1]{\arkcomment{\marker{S}{M}}{#1}{purple}}
\newcommand{\michi}[1]{\arkcomment{\marker{M}{Y}}{#1}{cyan}}
\newcommand{\luke}[1]{\arkcomment{\marker{L}{Z}}{#1}{brown}}

\newcommand{\model}{\textsc{RePlug}\xspace}
\newcommand{\tunemodel}{\textsc{RePlug LSR}\xspace}

\newcommand{\ours}{\textsc{Instructor}\xspace}







\usepackage[accepted]{icml2022}

\usepackage{amsmath}
\usepackage{tabularx}

\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}




\begin{document}

\twocolumn[
\icmltitle{\model: Retrieval-Augmented Black-Box Language Models}





\icmlsetsymbol{equal}{*}
\icmlsetsymbol{thanks}{*}


\begin{icmlauthorlist}
\icmlauthor{Weijia Shi,\!}{uw,thanks}
\icmlauthor{Sewon Min,\!}{uw}
\icmlauthor{Michihiro Yasunaga,\!}{su}
\icmlauthor{Minjoon Seo,\!}{kaist}
\icmlauthor{Rich James,\!}{fb}
\icmlauthor{Mike Lewis,\!}{fb}
\icmlauthor{Luke Zettlemoyer\!}{uw,fb}
\icmlauthor{Wen-tau Yih}{fb}
\end{icmlauthorlist}

\icmlaffiliation{su}{Stanford University}
\icmlaffiliation{fb}{Meta AI}
\icmlaffiliation{uw}{University of Washington}
\icmlaffiliation{kaist}{KAIST }


\icmlcorrespondingauthor{Weijia Shi}{swj0419@uw.edu}



\vskip 0.3in
]





\printAffiliationsAndNotice{}  


\begin{abstract}
We introduce \model, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross attention mechanisms to encode the retrieved text, \model simply prepends retrieved documents to the input for the frozen black-box LM. 
This simple design can be easily applied to any existing retrieval and language models. 
Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. 
Our experiments demonstrate that \model with the tuned retriever 
significantly improves the performance of GPT-3 (175B) on language modeling by 6.3\%, as well as the performance of Codex on five-shot MMLU by 5.1\%. 



\end{abstract}
    

\section{Introduction}




















Large language models (LLMs) such as GPT-3~\cite{NEURIPS2020_1457c0d6} and Codex~\cite{DBLP:journals/corr/abs-2107-03374}, 
have demonstrated impressive performance on a wide range of language tasks.
These models are typically trained on very large datasets and store a substantial amount of world or domain knowledge implicitly in their parameters. However, they are also prone to hallucination and cannot represent the full long tail of knowledge from the training corpus. Retrieval-augmented language models~\cite{Khandelwal2020Generalization, borgeaud2022improving, 
izacard2022few, yasunaga2022retrieval}, in contrast, can retrieve knowledge from an external datastore when needed, potentially reducing hallucination and increasing coverage. 
Previous approaches of retrieval-augmented language models require access to the internal LM representations (e.g., to train the model~\cite{borgeaud2022improving, izacard2022few} or to index the datastore~\cite{Khandelwal2020Generalization}), and are thus difficult to be applied to very large LMs. 
In addition, many best-in-class LLMs can only be accessed through APIs. Internal representations of such models are not exposed and fine-tuning is not supported.




\begin{figure}[]
\centering 
\includegraphics[scale=0.455]{fig/intro.png}
\caption{
{Different from previous retrieval-augmented approaches~\cite{borgeaud2022improving} that enhance a language model with retrieval by updating the LM's parameters, \model treats the language model as a black box and augments it with a frozen or tunable retriever.} 
This black-box assumption makes \model applicable to large LMs (i.e., >100B parameters), which are often served via APIs.
} \label{fig:intro}
\end{figure}

In this work, we introduce \model (\textbf{Re}trieve and \textbf{Plug}), a new retrieval-augmented LM framework where the language model is viewed as a black box and the retrieval component is added as a tuneable plug-and-play module.
Given an input context, \model first retrieves relevant documents from an external corpus using an \textit{off-the-shelf} retrieval model. The retrieved documents are prepended to the input context and fed into the black-box LM to make the final prediction. Because the LM context length limits the number of documents that can be prepended, we also introduce a new ensemble scheme that encodes the retrieved documents in parallel with the same black-box LM, allowing us to easily trade compute for accuracy. 
As shown in \autoref{fig:intro}, \model is extremely flexible and can be used with any existing black-box LM and retrieval model. 
 

 


 We also introduce \tunemodel (\model with \textbf{L}M-\textbf{S}upervised \textbf{R}etrieval), a training scheme that can further improve the initial retrieval model in \model with supervision signals from a black-box language model. 
 The key idea is to adapt the retriever to the LM, which is in contrast to prior work~\cite{borgeaud2022improving} that adapts language models to the retriever. 
We use a training objective which prefers retrieving documents that improve language model perplexity, while treating the LM as a frozen, black-box scoring function. 




















Our experiments show that \model can improve the performance of diverse black-box LMs on both language modeling 
and downstream tasks, including MMLU~\cite{hendrycks2021measuring} and open-domain QA~\cite{kwiatkowski-etal-2019-natural, joshi-etal-2017-triviaqa}. 
For instance, \model can improve Codex (175B) performance on MMLU by 4.5\%, achieving comparable results to the 540B, instruction-finetuned Flan-PaLM.
Furthermore, tuning the retriever with our training scheme (i.e., \tunemodel) leads to additional improvements, including up to 6.3\% increase in GPT-3 175B language modeling.
To the best of our knowledge, our work is the first to show the benefits of retrieval to large LMs (>100B model parameters), for both reducing LM perplexity and and improving in-context learning performance. We summarize our contributions as follows:
\begin{itemize}
\item We introduce \model (\S \ref{section:inference}), the first retrieval-augmented language modeling framework for enhancing large black-box language models with retrieval. 
\item We propose a training scheme (\S \ref{section:tunemodel}) to further adapt an off-the-shelf retrieval model to the LM, using the language modeling scores as supervision signals, resulting in improved retrieval quality.

\item Evaluations on language modeling (\S \ref{section:experiments}), open-domain QA and MMLU demonstrate that \model can improve the performance of various language models such as GPT, OPT and BLOOM, including very large models with up to 175B parameters. 

\end{itemize}
























 



%
 \section{Background and Related Work}
\begin{figure*}
    \centering
    \includegraphics[scale=0.55]{fig/inference.png}
    \caption{\textbf{\model at inference} (\S \ref{section:inference}). Given an input context, \model first retrieves a small set of relevant documents from an external corpus using a retriever (\S \ref{section:retrieve} \textit{Document Retrieval}). Then it prepends each document separately to the input context and ensembles output probabilities from different passes (\S \ref{section:fusion}  \textit{Input Reformulation}).
    }
    \label{fig:inference}
\end{figure*}









\paragraph{Black-box Language Models}





Large language models (i.e., >100B), such as GPT-3~\cite{NEURIPS2020_1457c0d6}, Codex~\cite{DBLP:journals/corr/abs-2107-03374}, and Yuan 1.0~\cite{wu2021yuan}, are not open-sourced due to commercial considerations and are only available as black-box APIs, through which users can send queries and receive responses. 
On the other hand, even open sourced language models such as OPT-175B~\cite{zhang2022democratizing} and BLOOM-176B~\cite{scao2022bloom} require significant computational resources to run and finetune locally. 
For example, finetuning BLOOM-176B requires 72 A100 GPUs (80GB memory, \x\mathcal{D}=\{d_1...d_m\}xxdd \in D\textbf{E}(d)dx\textbf{E}(x)kxd \in Dkxxkk\mathcal{D}' \subset \mathcal{D}kxd \in \mathcal{D}'xkxk\mathcal{D}'y\circ\lambda(d,x)dxkk\mathcal{D}' \subset \mathcal{D}\mathcal{D}xd\gamma\mathcal{D}\mathcal{D}'P_{LM}(y \mid d, x)yxdd_id\betaxy\mathcal{B}TxyDDx^\dag^\dag^\dag_{cc}$\tablefootnote{\citet{si2022prompting} augment Codex with concatenation of 10 documents retrieved by contriever.} & 44.2 & - & 76.0 & - \\
\midrule
Codex + \model\  & 44.7 & - & 76.8 & - \\
Codex + \tunemodel\ & \textbf{45.5} & - & \textbf{77.3} & - \\
\bottomrule
\end{tabular}
\caption{Performance on NQ and TQA. We report results for both few-shot (64 shots for Chinchilla, PaLM, and Atlas; 16 shots for Codex-based models) and full training data settings. \tunemodel improves Codex by 12.0\% on NQ and 5.0\% on TQA, making it the best-performing model in the few-shot setting. Note that models with \dag~are finetuned using training examples, while other models use in-context learning. 
}
\label{tab:qa}
\end{table}
 

\paragraph{Datasets}
NQ and TriviaQA are two open-domain QA datasets consisting of questions, answers collected from Wikipedia and the Web. Following prior work~\cite{izacard-grave-2021-leveraging, si2022prompting}, we report results for the filtered set of TriviaQA. For evaluation, we consider the few-shot setting where the model is only given a few training examples and full data where the model is given all the training examples.



\paragraph{Baselines}
We compare our model with several state-of-the-art baselines, both in a few-shot setting and with full training data. The first group of models consists of powerful large language models, including Chinchilla~\cite{chinchilla}, PaLM~\cite{palm}, and Codex. These models are all evaluated using in-context learning under the few-shot setting, with Chinchilla and PaLM evaluated using 64 shots, and Codex using 16 shots. The second group of models for comparison includes retrieval-augmented language models such as RETRO~\cite{borgeaud2021improving}, R2-D2~\cite{fajcik-etal-2021-r2-d2}, and Atlas~\cite{izacard2022few}. All of these retrieval-augmented models are finetuned on the training data, either in a few-shot setting or with full training data. Specifically, Atlas is finetuned on 64 examples in the few-shot setting.


\paragraph{Our model}
We add \model and \tunemodel to Codex with Wikipedia (2018, December) as the retrieval corpus to evaluate the model in a 16-shot in context learning. Similar to the setting in language modeling and MMLU, we incorporate top-10 retrieved documents using our proposed ensemble method. 



\paragraph{Results}




As shown in Table~\ref{tab:qa}, \tunemodel significantly improves the performance of the original Codex by 12.0\% on NQ and 5.0\% on TQA. It outperforms the previous best model, Atlas, which was fine-tuned with 64 training examples, achieving a new state-of-the-art in the few-shot setting. However, this result still lags behind the performance of retrieval-augmented language models fine-tuned on the full training data. This is likely due to the presence of near-duplicate test questions in the training set (e.g., \citet{lewis2021question} found that 32.5\% of test questions overlap with the training sets in NQ). 














 \section{Analysis}

\begin{figure}[]
\centering 
\includegraphics[scale=0.34]{fig/ensemble.png}
\caption{
\textbf{Ensembling random documents does not result in improved performance.} BPB of Curie augmented with different methods (random, \model and \tunemodel) when varying the number of documents (i.e.; number of ensemble times.)}  
\label{fig:ensemble}
\end{figure}




\subsection{\model performance gain does not simply come from the ensembling effect
}


The core of our method design is the use of an ensemble method that combines output probabilities of different passes, in which each retrieved document is prepended separately to the input and fed into a language model. To study whether the gains come solely from the ensemble method, we compare our method to ensembling random documents. For this, we randomly sample several documents, concatenated each random document with the input, and ensemble the outputs of different runs (referred to as "random").
As shown in \autoref{fig:ensemble}, we evaluated the performance of GPT-3 Curie on Pile when augmented with random documents, documents retrieved by \model, and documents retrieved by \tunemodel. We observed that ensembling random documents leads to worse performance, indicating that the performance gains of \model do not solely come from the ensembling effect. Instead, ensembling the \textbf{relevant} documents is crucial for the success of \model. Additionally, as more documents were ensembled, the performance of \model and \tunemodel improved monotonically. However, a small number of documents (e.g., 10) was sufficient to achieve large performance gains.










\begin{figure*}[ht]
    \centering
    \subfigure[]{\includegraphics[width=0.3\textwidth]{fig/gpt2.pdf}} 
    \subfigure[]{\includegraphics[width=0.3\textwidth]{fig/bloom.pdf}} 
    \subfigure[]{\includegraphics[width=0.3\textwidth]{fig/opt.pdf}}
    \caption{
   \textbf{GPT-2, BLOOM and OPT models of varying sizes consistently benefit from \model.} 
 The x-axis indicates the size of the language model and the y-axis is its perplexity on Wikitext-103. 
    }
    \label{fig:scaling}
\end{figure*}



\subsection{\model is applicable to diverse language models}
Here we further study whether \model could enhance \textit{diverse} language model families that have been pre-trained using different data and methods.  Specifically, we focus on three groups of language models with varying sizes: GPT-2 (117M, 345M, 774M, 1.5B parameters)~\cite{NEURIPS2020_1457c0d6}, OPT (125M, 350M, 1.3B, 2.7B, 6.7B, 13B, 30B, 66B)~\cite{zhang2022opt} and BLOOM (560M, 1.1B, 1.7B, 3B and 7B)~\cite{scao2022bloom}. We evaluate each model on Wikitext-103~\cite{stephen2017pointer} test data and report its perplexity. For comparison, we augment each model with \model that adopts the ensemble method to incorporate top 10 retrieved documents. Following prior work~\cite{Khandelwal2020Generalization}, we use Wikitext-103 training data as the retrieval corpus. 

\autoref{fig:scaling} shows the performance of different-sized language models with and without \model. We observe that the performance gain brought by \model stays consistent with model size. For example, OPT with 125M parameters achieves 6.9\% perplexity improvement, while OPT with 66B parameters achieves 5.6\% perplexity improvement. Additionally, \model improves the perplexity of all the model families.
This indicates that \model is applicable to diverse language models with different sizes. 




\subsection{Qualitative Analysis: rare entities benefit from retrieval}
To understand why the \model improves language modeling performance, we conducted manual analysis of examples in which the \model results in a decrease in perplexity. We find that \model is more helpful when texts contain rare entities.
\autoref{fig:ensemble} shows a test context and its continuation from the Wikitext-103 test set. For \model, we use the test context as a query to retrieve a relevant document from Wikitext-103 training data. We then compute the perplexity of the continuation using the original GPT-2 1.5B and its \model enhanced version. After incorporating the retrieved document, the perplexity of the continuation improves by 11\%. 
Among all tokens in the continuation, we found that \model is most helpful for the rare entity name "Li Bai".
This is likely because the original LM does not have sufficient information about this rare entity name. However, by incorporating the retrieved document, \model was able to match the name with the relevant information in the retrieved document, resulting in better performance.




\begin{figure}[h]
\centering 
\includegraphics[scale=0.29]{fig/example.png}
\caption{\textbf{Rare entities benefit from
retrieval}. After incorporating the retrieved document during inference, the entity "\textit{Li Bai}" and the token "\textit{greatest}" in the continuation show the most improvement in perplexity (15\% for "\textit{Li Bai}" and 5\% for "\textit{greatest}"). Other tokens' perplexity changes are within 5\%.}
\label{fig:ensemble}
\end{figure}




%
 
\section{Conclusion}
We introduce \model, a retrieval-augmented language modeling paradigm that treats the language model as a black box and augments it with a tuneable retrieval model. Our evaluation shows that \model can be integrated with any existing language model to improve their performance on language modeling or downstream tasks. This work opens up new possibilities for integrating retrieval into large-scale black-box language models and demonstrates even the state-of-the-art large-scale LMs could benefit from retrieval. However, \model lacks interpretability as it is unclear when the model relies on retrieved knowledge or parametric knowledge. Future research could focus on developing more interpretable retrieval-augmented language models.




%
 




\bibliography{anthology,custom}
\bibliographystyle{icml2022}

\newpage


\appendix
\onecolumn

\begin{table*}
\small
\begin{tabularx}{\textwidth}{m{17cm}}
\toprule
\textbf{Knowledge}: Arctic Ocean. Although over half of Europe's original forests disappeared through the centuries of deforestation, Europe still has over one quarter of its land area as forest, such as the broadleaf and mixed forests, taiga of Scandinavia and Russia, mixed rainforests of the Caucasus and the Cork oak forests in the western Mediterranean. During recent times, deforestation has been slowed and many trees have been planted. However, in many cases monoculture plantations of conifers have replaced the original mixed natural forest, because these grow quicker. The plantations now cover vast areas of land, but offer poorer habitats for many European \\ 
\textbf{Question}: As of 2015, since 1990 forests have \xdash in Europe and have \xdash in Africa and the Americas. \\
A. "increased, increased"
B. "increased, decreased"
C. "decreased, increased"
D. "decreased, decreased" \\
\textbf{Answer}: B \\
\\
\textbf{Knowledge}: Over the past decades, the political outlook of Americans has become more progressive, with those below the age of thirty being considerably more liberal than the overall population. According to recent polls, 56\% of those age 18 to 29 favor gay marriage, 68\% state environmental protection to be as important as job creation, 52\% "think immigrants \'strengthen the country with their hard work and talents,\'" 62\% favor a "tax financed, government-administrated universal health care" program and 74\% "say \'people\'s will\' should have more influence on U.S. laws than the Bible, compared to 37\%, 49\%, 38\%, 47\% and 58\% among the \\ 
\textbf{Question}: As of 2019, about what percentage of Americans agree that the state is run for the benefit of all the people? \\
A. 31\%
B. 46\%
C. 61\%
D. 76\% \\
\textbf{Answer}: B \\
... \\
\textbf{Knowledge}: last week at a United Nations climate meeting in Germany, China and India should easily exceed the targets they set for themselves in the 2015 Paris Agreement... India is now expected to obtain 40 percent of its electricity from non-fossil fuel sources by 2022, eight years ahead of schedule." Solar power in Japan has been expanding since the late 1990s. By the end of 2017, cumulative installed PV capacity reached over 50 GW with nearly 8 GW installed in the year 2017. The country is a leading manufacturer of solar panels and is in the top 4 ranking for countries \\
\textbf{Question}: Which of the following countries generated the most total energy from solar sources in 2019? \\
A. China
B. United States
C. Germany
D. Japan
 \\
\bottomrule
\end{tabularx}
\caption{Prompt for MMLU}
\label{tab:full_instructions1}
\end{table*}

\begin{table*} []
\small
\begin{tabularx}{\textwidth}{m{17cm}}
\toprule
\textbf{Knowledge}: received 122,000 buys (excluding WWE Network views), down from the previous year\'s 199,000 buys. The event is named after the Money In The Bank ladder match, in which multiple wrestlers use ladders to retrieve a briefcase hanging above the ring. The winner is guaranteed a match for the WWE World Heavyweight Championship at a time of their choosing within the next year. On the June 2 episode of "Raw", Alberto Del Rio qualified for the match by defeating Dolph Ziggler. The following week, following Daniel Bryan being stripped of his WWE World Championship due to injury, Stephanie McMahon changed the \\ 
\textbf{Question}: Who won the mens money in the bank match? \\
\textbf{Answer}: Braun Strowman \\
\\
\textbf{Knowledge}: in 3D on March 17, 2017. The first official presentation of the film took place at Disney\'s three-day D23 Expo in August 2015. The world premiere of "Beauty and the Beast" took place at Spencer House in London, England on February 23, 2017; and the film later premiered at the El Capitan Theatre in Hollywood, California, on March 2, 2017. The stream was broadcast onto YouTube. A sing along version of the film released in over 1,200 US theaters nationwide on April 7, 2017. The United Kingdom received the same version on April 21, 2017. The film was re-released in \\ 
\textbf{Question}: When does beaty and the beast take place \\\
\textbf{Answer}: Rococo-era \\
... \\
\textbf{Knowledge}: Love Yourself "Love Yourself" is a song recorded by Canadian singer Justin Bieber for his fourth studio album "Purpose" (2015). The song was released first as a promotional single on November 8, 2015, and later was released as the album\'s third single. It was written by Ed Sheeran, Benny Blanco and Bieber, and produced by Blanco. An acoustic pop song, "Love Yourself" features an electric guitar and a brief flurry of trumpets as its main instrumentation. During the song, Bieber uses a husky tone in the lower registers. Lyrically, the song is a kiss-off to a narcissistic ex-lover who did \\
\textbf{Question}: love yourself by justin bieber is about who \\
 \\
\bottomrule
\end{tabularx}
\caption{Prompt for open-domain QA}
\label{tab:full_instructions1}
\end{table*} 
 






\end{document}
