
In this section, we start by the nature of 3D scene geometry. Then we give an overview of our GeoNet. It follows by its two components: rigid structure reconstructor and non-rigid motion localizer respectively. Finally, we raise the geometric consistency enforcement, which is the core of our GeoNet.

\begin{figure*}[t]
\begin{center}
   \includegraphics[clip, trim=0cm 0.6cm 0cm 2.8cm, width=1.0\textwidth]{pipeline.pdf}
\end{center}
\vspace{-1ex}
   \caption{Overview of GeoNet. It consists of rigid structure reconstructor for estimating static scene geometry and non-rigid motion localizer for capturing dynamic objects.
Consistency check within any pair of bidirectional flow predictions is adopted for taking care of occlusions and non-Lambertian surfaces.} \vspace{-2ex}
\label{fig::pipeline}
\end{figure*}
\subsection{Nature of 3D Scene Geometry}\label{sec::3dscene}

Videos or images are the screenshots of 3D space projected into certain dimensions. The 3D scene is naturally comprised of static background and moving objects. The movement of static parts in a video is solely caused by camera motion and depth structure. Whereas movement of dynamic objects is more complex, contributed by both homogeneous camera motion and specific object motion.

Understanding the homogeneous camera motion is relatively easier compared to complete scene understanding, since most of the region is bounded by its constraints. To decompose the problem of 3D scene understanding by its nature, we would like to learn the scene level consistent movement governed by camera motion, namely the \textit{rigid flow}, and the \textit{object motion} separately. 

Here we briefly introduce the notations and basic concepts used in our paper. To model the strictly restricted rigid flow, we define the static scene geometries by a collection of depth maps $D_i$ for frame $i$, and the relative camera motion $T_{t\to s}$ from target to source frame. The relative 2D rigid flow from target image $I_t$ to source image $I_s$ can be represented by\footnote{Similar to~\cite{zhou2017unsupervised}, we omit the necessary conversion to homogeneous coordinates here for notation brevity.}
\begin{equation}
    \label{equa::proj}
f_{t\to s}^{rig}(p_t) = KT_{t\to s}D_t(p_t)K^{-1}p_t - p_t,
\end{equation}
where $K$ denotes the camera intrinsic and $p_t$ denotes homogeneous coordinates of pixels in frame $I_t$. On the other hand, we model the unconstrained object motion as classical optical flow conception, \ie 2D displacement vectors. We learn the \textit{residual flow} $f^{res}_{t\to s}$ instead of the full representation for non-rigid cases, which we will explain later in Sec.~\ref{sec::resflow}. For brevity, we mainly illustrate the cases from target to source frames in the following, which one can easily generalize to the reversed cases. 
Guided by these positional constraints, we can apply \textit{differentiable inverse warping} \cite{JaderbergSZK15} between nearby frames, which later become the foundation of our fully unsupervised learning scheme. 

\subsection{Overview of GeoNet} \label{sec:overview}



Our proposed GeoNet perceives the 3D scene geometry by its nature in an unsupervised manner. In particular, we use separate components to learn the rigid flow and object motion by rigid structure reconstructor and non-rigid motion localizer respectively. The image appearance similarity is adopted to guide the unsupervised learning, which can be generalized to infinite number of video sequences without any labeling cost.

An overview of our GeoNet has been depicted in Fig.~\ref{fig::pipeline}. It contains two stages, the rigid structure reasoning stage and the non-rigid motion refinement stage. The first stage to infer scene layout is made up of two sub-networks, \ie the DepthNet and the PoseNet. Depth maps and camera poses are regressed respectively and fused to produce the rigid flow. Furthermore, the second stage is fulfilled by the ResFlowNet to handle dynamic objects. The residual non-rigid flow learned by ResFlowNet is combined with rigid flow, deriving our final flow prediction. Since each of our sub-networks targets at a specific sub-task, the complex scene geometry understanding goal is decomposed to some easier ones. View synthesis at different stage works as fundamental supervision for our unsupervised learning paradigm. 

Last but not the least, we conduct geometric consistency check during training, which significantly enhances the coherence of our predictions and achieves impressive performance.



\subsection{Rigid Structure Reconstructor}
\label{sec::static}

Our first stage aims to reconstruct the rigid scene structure with robustness towards non-rigidity and outliers. The training examples are temporal continuous frames $I_{i} (i=1\sim n)$ with known camera intrinsics. Typically, a target frame $I_t$ is specified as the reference view, and the other frames are source frames $I_s$. Our DepthNet takes single view as input and exploits accumulated scene priors for depth prediction. During training, 
the entire sequence is treated as a mini-batch of independent images and fed into the DepthNet.
In contrast, to better utilize the feature correspondences between different views, our PoseNet takes the entire sequence concated along channel dimension as input to regress all the relative 6DoF camera poses $T_{t\to s}$ at once. Building upon these elementary predictions, we are able to derive the global rigid flow according to Eq.~\eqref{equa::proj}. Immediately we can synthesize the other view between any pair of target and source frames. Let us denote $\tilde{I}_s^{rig}$ as the inverse warped image from $I_s$ to target image plane by $f_{t\to s}^{rig}$. 
Thereby the supervision signal for our current stage naturally comes in form of minimizing the dissimilarities between the synthesized view $\tilde{I}_s^{rig}$ and original frame $I_t$ (or inversely).

However, it should be pointed out that rigid flow only dominates the motion of non-occluded rigid region while becomes invalid in non-rigid region. Although such negative effect is slightly mitigated within the rather short sequence, we adopt a robust image similarity measurement~\cite{monodepth17} for the photometric loss, which maintains the balance between appropriate assessment of perceptual similarity and modest resilience for outliers, and is differentiable in nature as follows
\begin{equation}
    \label{equa::ssim}
\mathcal{L}_{rw}=\alpha\frac{1-SSIM(I_t,\tilde{I}_s^{rig})}{2}+(1-\alpha)\|I_t-\tilde{I}_s^{rig}\|_1,
\end{equation}
where SSIM denotes the structural similarity index~\cite{wang2004image} and $\alpha$ is taken to be $0.85$ by cross validation. Apart from the rigid warping loss $\mathcal{L}_{rw}$, to filter out erroneous predictions and preserve sharp details, we introduce an edge-aware depth smoothness loss $\mathcal{L}_{ds}$ weighted by image gradients
\begin{equation}
    \label{equa::smooth}
\mathcal{L}_{ds}=\sum_{p_t}|\nabla D(p_t)|\cdot (e^{-|\nabla I(p_t)|})^T,
\end{equation}
where $|\cdot|$ denotes elementwise absolute value, $\nabla$ is the vector differential operator, and T denotes the transpose of image gradient weighting.


\subsection{Non-rigid Motion Localizer}
\label{sec::resflow}
The first stage provides us with a stereoscopic perception of rigid scene layout, but ignores the common existence of dynamic objects. Therefore, we raise our second component, \ie the ResFlowNet to localize non-rigid motion. 

Intuitively, generic optical flow can directly model the unconstrained motion, which is commonly adopted in off-the-shelf deep models~\cite{FischerDIHHGSCB15,IMKDB17}. But they do not fully exploit the well-constrained property of rigid regions, which we have already done in the first stage actually. Instead, we formulate our ResFlowNet for learning the \textit{residual non-rigid flow}, the shift solely caused by relative object movement to the world plane. Specifically, we cascade the ResFlowNet after the first stage in a way recommended by \cite{IMKDB17}. For any given pair of frames, the ResFlowNet takes advantage of output from our rigid structure reconstructor, and predicts the corresponding residual signal $f^{res}_{t\to s}$. The final full flow prediction $f^{full}_{t\to s}$ is constituted by $f^{rig}_{t\to s}+f^{res}_{t\to s}$. 

As illustrated in Fig.~\ref{fig::residual}, our first stage, rigid structure reconstructor, produces high-quality reconstruction in most rigid scenes, which sets a good starting point for our second stage. Thereby, our ResFlowNet in motion localizer simply focuses on other non-rigid residues. Note that ResFlowNet can not only rectify wrong predictions in dynamic objects, but also refine imperfect results from first stage thanks to our end-to-end learning protocol, which may arise from high saturations and extreme lighting conditions. 


\begin{figure}[t]
\begin{center}
   \includegraphics[clip, trim=0cm 0cm 0cm 2.5cm, width=1.0\linewidth]{residual_illu.pdf}
\end{center}
\vspace{-1ex}
   \caption{Comparison between flow predictions at different stages. Rigid flow gives satisfactory result in most static regions, while residual flow module focuses on localizing non-rigid motion such as cars, and refining initial prediction in challenging cases such as dark illuminations and thin structures.}\vspace{-2ex}
\label{fig::residual}
\end{figure}

Likewise, we can extend the supervision in Sec.~\ref{sec::static} to current stage with slight modifications. In detail, following the full flow $f^{full}_{t\to s}$, we perform image warping between any pair of target and source frames again. Replacing the $\tilde{I}_s^{rig}$ with $\tilde{I}_s^{full}$ in Eq.~\eqref{equa::ssim}, we obtain the full flow warping loss $\mathcal{L}_{fw}$. Similarly, we extend the smoothness loss in Eq.~\eqref{equa::smooth} over 2D optical flow field, which we denote as $\mathcal{L}_{fs}$. \iffalse
Still without consistency enforcement, we introduce supervision at this level as follows
\begin{equation}
\mathcal{L}_{t\to s}^{fw}=\sum_{p_t\in I_t}\mathcal{C}(I_t, \tilde{I}_s^{fin})(p_t)
\end{equation}
\begin{equation}
\tilde{I}_s^{fin}(p_t):=I_s(p_t+f_{t\to s}^{fin}(p_t))
\end{equation}
\fi



\subsection{Geometric Consistency Enforcement}
\label{sec::geocst}
Our GeoNet takes rigid structure reconstructor for static scene, and non-rigid motion localizer as compensation for dynamic objects. Both stages utilize the view synthesis objective as supervision, with the implicit assumption of photometric consistency. Though we employ robust image similarity assessment such as Eq.~\eqref{equa::ssim}, occlusions and non-Lambertian surfaces still cannot be perfectly handled in practice.

To further mitigate these effects, we apply a forward-backward consistency check in our learning framework without changing the network architecture. The work by Godard \etal \cite{monodepth17} incorporated similar idea into their depth learning scheme with the left-right consistency loss. However, we argue that such consistency constraints, as well as the warping loss, should not be imposed at occluded regions (see Sec.~\ref{sec::exp_flow}). Instead we optimize an adaptive consistency loss across the final motion field.
 
Concretely, our geometric consistency enforcement is fulfilled by optimizing the following objective
\begin{equation}
    \label{equa::cst}
\mathcal{L}_{gc} = \sum_{p_t} [\delta(p_t)]\cdot\|\Delta f_{t\to s}^{full}(p_t)\|_1, 
\end{equation}
where $\Delta f_{t\to s}^{full}(p_t)$ is the full flow difference computed by forward-backward consistency check at pixel $p_t$ in $I_t$, $[\cdot]$ is the Iverson bracket, and $\delta(p_t)$ denotes the condition of  
\begin{equation}
\|\Delta f_{t\to s}^{full}(p_t)\|_2<max\{\alpha, \beta\|f_{t\to s}^{full}(p_t)\|_2\},
\end{equation}
in which $(\alpha,\beta)$ are set to be $(3.0,0.05)$ in our experiment. 
Pixels where the forward/backward flows contradict seriously are considered as possible outliers. Since these regions violate the photo consistency as well as geometric consistency assumptions, we handle them only with the smoothness loss $\mathcal{L}_{fs}$. 
Therefore both our full flow warping loss $\mathcal{L}_{fw}$ and geometric consistency loss $\mathcal{L}_{gc}$ are weighted by $[\delta(p_t)]$ pixelwise.

\iffalse
\begin{equation}
\mathcal{L}_{t\to s}^{fw}=\sum_{p_t\in I_t}P_{t\to s}^{noc}(p_t)\mathcal{C}(I_t,\tilde{I}_s^{fin})(p_t)
\end{equation}

\begin{equation}
\mathcal{L}_{t\to s}^{fc}=\sum_{p_t\in I_t}P_{t\to s}^{noc}(p_t)|\Delta f_{t\to s}(p_t)|
\end{equation}
\fi

\begin{figure*}[t]
\begin{center}
   \includegraphics[clip, trim=0cm 0.2cm 0cm 4.1cm, width=1.0\textwidth]{exp_depth.pdf}
\end{center}
\vspace{-1ex}
   \caption{Comparison of monocular depth estimation between Eigen~\etal~\cite{EigenPF14} (supervised by depth), Zhou~\etal~\cite{zhou2017unsupervised} (unsupervised) and ours (unsupervised). The groundtruth is interpolated for visualization purpose. Our method captures details in thin structures and preserves consistently high-quality predictions both in close and distant regions.}
   \vspace{-2ex}
\label{fig::exp_depth}
\end{figure*}



To summarize, our final loss through the entire pipeline becomes
\begin{equation}
\mathcal{L}=\sum_{l}\sum_{\langle t,s \rangle}\{\mathcal{L}_{rw}+\lambda_{ds}\mathcal{L}_{ds}+\mathcal{L}_{fw}+\lambda_{fs}\mathcal{L}_{fs}+\lambda_{gc}\mathcal{L}_{gc}\},
\end{equation}
where $\lambda$ denotes respective loss weight, $l$ indexes over pyramid image scales, and $\langle t,s \rangle$ indexes over all the target and source frame pairs and their inverse combinations. 


