\documentclass{article}





\usepackage[nonatbib,final]{neurips_2022}







\usepackage{pythonhighlight}
\definecolor{keywordcolour}{RGB}{207,34,46}
\definecolor{stringcolour}{RGB}{26,62,115}
\definecolor{literatecolour}{RGB}{20,90,179}
\definecolor{specmethodcolour}{RGB}{137,90,225}
\lstset{     emph={[8]with, as},
    emphstyle={[8]\color{keywordcolour}\bfseries}
}
\usepackage{inconsolata}    

\usepackage{dirtree}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{pifont}
\usepackage{enumitem}
\usepackage{wrapfig}
\usepackage{siunitx}

\usepackage[backend=biber,natbib=true,style=authoryear,doi=false,isbn=false,url=false,eprint=false,giveninits=true,maxbibnames=200,maxcitenames=2,mincitenames=1,uniquename=false,uniquelist=false,dashed=false]{biblatex}
\addbibresource{biblio.bib}

\usepackage[textwidth=3cm, textsize=scriptsize]{todonotes}





\renewcommand{\arraystretch}{1.1}


\definecolor{linkcolor}{RGB}{83,83,182}
\hypersetup{
    colorlinks=true,
    citecolor=linkcolor,
    linkcolor=linkcolor,
    urlcolor=linkcolor
}


\newcommand{\ie}{{\em i.e.,~}}
\newcommand{\wlg}{{\em w.l.o.g.,~}}
\newcommand{\eg}{{\em e.g.,~}}
\newcommand{\lcf}{{\em cf.~}}
\newcommand{\resp}{{\em resp.~}}
\newcommand{\etal}{{\em et al.~}}
\newcommand{\wrt}{{\em w.r.t.~}}

\newcommand{\repo}[1]{#1}
\newcommand{\rebuttal}[1]{#1}

\usepackage{amsmath}
\newcommand{\norm}[1]{\left \Vert #1 \right \Vert}
\newcommand{\normin}[1]{\Vert #1 \Vert}
\newcommand{\bbR}{\mathbb{R}}

\newcommand{\Benchopt}{{{\texttt{Benchopt}}}}
\newcommand{\Python}{{{\texttt{Python}}}}
\newcommand{\Julia}{{{\texttt{Julia}}}}
\newcommand{\PyTorch}{{{\texttt{PyTorch}}}}
\newcommand{\TensorFlow}{{{\texttt{TensorFlow}}}}
\newcommand{\Numba}{{{\texttt{numba}}}}
\newcommand{\Cython}{{{\texttt{Cython}}}}
\newcommand{\BLAS}{{{\texttt{BLAS}}}}
\newcommand{\Keras}{\texttt{Keras}}
\newcommand{\OpenML}{\texttt{OpenML}}
\newcommand{\MLPerf}{\texttt{MLPerf}}
\newcommand{\CSV}{\texttt{CSV}}
\newcommand{\HTML}{\texttt{HTML}}
\newcommand{\PDF}{\texttt{PDF}}
\newcommand{\skglm}{\texttt{skglm}}
\newcommand{\noncvxpro}{\texttt{noncvx-pro}}
\newcommand{\glmnet}{\texttt{glmnet}}
\newcommand{\cuML}{{{\texttt{cuML}}}}
\newcommand{\celer}{{{\texttt{celer}}}}
\newcommand{\blitz}{{{\texttt{blitz}}}}
\newcommand{\snapML}{{{\texttt{snapML}}}}
\newcommand{\lassojl}{{{\texttt{lasso.jl}}}}
\newcommand{\liblinear}{{{\texttt{liblinear}}}}

\newcommand{\argmin}{\mathop{\mathrm{arg\,min}}}
\DeclareMathOperator{\dist}{dist}


\usepackage[capitalize]{cleveref}

\newcommand{\myparagraph}[1]{\vspace{1mm}\noindent\textbf{#1} \,}


\usepackage{aliascnt}
\newaliascnt{problem}{equation}
\aliascntresetthe{problem}
\makeatletter
\def\problem{\@ignoretrue}
\makeatother


\def\equationautorefname~#1\null{(#1)\null}
\def\problemautorefname~#1\null{Problem (#1)\null}
\def\subsectionautorefname{Section}
\def\sectionautorefname{Section}

\title{Benchopt: Reproducible, efficient and collaborative optimization benchmarks}




\author{
  Thomas~Moreau,
  Mathurin~Massias,
  Alexandre~Gramfort,
  Pierre~Ablin,\\
  \textbf{
  Pierre-Antoine~Bannier, Benjamin~Charlier,
  Mathieu~Dagréou,
  Tom~Dupré~la~Tour,}\\
  \textbf{
  Ghislain~Durif,
  Cassio~F.~Dantas,
  Quentin~Klopfenstein,
  Johan~Larsson,
  En~Lai,}\\
  \textbf{
  Tanguy~Lefort,
  Benoit~Malézieux,
  Badr~Moufad,
  Binh~T.~Nguyen,
  Alain~Rakotomamonjy,}\\
  \textbf{
  Zaccharie~Ramzi,
  Joseph~Salmon,
  Samuel~Vaiter}\\label{eq:benchopt_pb}
    \theta^* \in \argmin_{\theta\in\Theta} f(\theta; \mathcal D, \Lambda)\enspace,

            \mathbb E[X_i] = 0~, \quad \mathbb E[X_i^2] = 1  \quad
        \text{and} \quad \mathbb E[X_iX_j] = \rho^{|i-j|} \ .

     y = X \theta^* + \varepsilon \ ,

|t|_\mu =
    \begin{cases}
    \frac{1}{2} t^2 &\textrm{if } |t| \leq \mu\\
    \mu |t|-\frac{\mu^2}{2} &\textrm{otherwise.}
    \end{cases}

\rho_{\lambda,\gamma}(t) =
\begin{cases}
     \lambda |t| - \frac{t^2}{2\gamma} \enspace, \quad &\text{if } |t| \leq \gamma  \lambda  \enspace,  \\
     \frac{\lambda^2 \gamma}{2} \enspace, \quad  &\text{if } |t| > \gamma  \lambda \enspace.
\end{cases}

    f(\mathbf {x} ) & = \sum _{i=1}^{N-1}[100(x_{i+1}-x_{i}^{2})^{2}+(1-x_{i})^{2}] \, &  \text{(Rosenbrock)} \\
    f(\mathbf {x} ) & = 10 \cdot N + \sum _{i=1}^{N}[(x_{i}^2-10\cdot \cos(2\pi x_i)]
    \, &\text{(Rastrigin)} \\
    f(\mathbf {x} ) & = -20 \cdot \exp \left[-0.2 {\sqrt {\frac{1}{d}\sum_{i=1}^N x_i^2}}\right] -\exp \left[  {\frac{1}{d}\sum_{i=1}^N \cos(2\pi x_i)}\right]+e+20\, & \text{(Ackley)}
    \enspace.

For each function, the domain is restricted to a box:  for Ackley,  for Rosenbrock and  for Rastrigin.
The algorithms considered in the benchmark are listed in
\autoref{table:algo-zero-order-benchmark}. As \texttt{BFGS} requires first-order information, gradients are approximated with finite-differences.}


\begin{table}[h]
  \centering
  \small
  \caption{List of solvers used in the zero-order benchmark}
\begin{tabular}{l p{3.5cm} l}
\toprule
  \textbf{Solver} & \textbf{References} & \textbf{Description} \\
  \midrule
   \texttt{Basin-hopping} &\citet{wales1997global,2020SciPy-NMeth} &  Two-phase method:  global step + local min. \\
\texttt{Nevergrad-RandomSearch} & \citet{nevergrad,Bergstra_Bengio12} & Sampler by random search\\
\texttt{Nevergrad-CMA} & \citet{nevergrad,hansen2001completely} & CMA evolutionary strategy \\
\texttt{Nevergrad-TwoPointsDE} & \citet{nevergrad} & Evolutionary strategy \\
\texttt{Nevergrad-NGOpt} & \citet{nevergrad} & Adaptive evolutionary algorithm \\
\texttt{Nelder-Mead} & \citet{gao2012implementing,2020SciPy-NMeth} & Direct search (downhill simplex)\\
\texttt{BFGS} & \citet{2020SciPy-NMeth} & BFGS with finite differences\\
\texttt{Powell} & \citet{powell1964efficient,2020SciPy-NMeth} & Conjugate direction method\\
\texttt{optuna-TPE} & \citet{optuna,bergstra13} & Sampler by Tree Parzen Estimation (TPE)\\
\texttt{optuna-CMA} & \citet{optuna,hansen2001completely} & CMA evolutionary strategy \\
\bottomrule
\end{tabular}
\vskip -0.1in
\label{table:algo-zero-order-benchmark}
\end{table}


\rebuttal{\repo{The code for the benchmark is available at \url{https://github.com/benchopt/benchmark_zero_order/}.}}


\subsection{\rebuttal{Results}}
\rebuttal{The results of the benchmark are presented in \autoref{fig:zero-order}.
The functions are non-convex and solvers are only guaranteed to converge to local minima; hence in \autoref{fig:zero-order} we monitor the value of the function. The functions are designed such that the global minimum of the function is always 0. One can observe that the CMA and TwoPointsDE implementations from \texttt{nevergrad} consistently reaches the global minimum.
In addition, the CMA implementation from \texttt{optuna} is a bit slower than the one from \texttt{nevergrad}. Also one can notice that random search offers reasonable results.
The TPE method seems to suffer from the curse of dimensionality, as most kernel methods in non-parametric estimation. Finally regarding the \texttt{scipy} solvers, \texttt{Powell} can be competitive, while \texttt{Nelder-Mead} and \texttt{BFGS} suffer a lot from local minima.}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\figwidth]{Figures/zero_order/zero_order_dim_20_legend.pdf}
    \includegraphics[width=0.9\figwidth]{Figures/zero_order/zero_order_dim_20.pdf}
    \caption{
        \rebuttal{
        Benchmark for the zero-order optimization on the Ackley, Rosenbrock
        and Rastrigin functions in dimension .
        }
    }
    \label{fig:zero-order}
\end{figure}


\clearpage{}


\end{document}
