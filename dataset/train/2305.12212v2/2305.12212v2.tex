\pdfoutput=1


\documentclass[11pt]{article}

\usepackage[]{EMNLP2023}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}
\usepackage{enumitem}
\usepackage{algorithm}

\usepackage{microtype}
\usepackage{boldline}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{hhline}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{enumitem}
\usepackage{colortbl}

\usepackage{mathtools}
\usepackage{pifont}

\usepackage{bbm}
\usepackage{amsfonts}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{xparse}
\usepackage{graphicx}
\usepackage{multicol}
\usetikzlibrary{shapes,arrows}
\usepackage{natbib}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tikz-dependency}
\usepackage{wrapfig}
\pgfplotsset{compat=1.17} 
\usepackage{subfiles}
\usepackage{readarray}
\usepackage{xcolor}
\usepackage{import}

\newcommand{\xvec}{\mathbf{x}}
\newcommand{\yvec}{\mathbf{y}}
\newcommand{\cvec}{\mathbf{c}}
\newcommand{\evec}{\mathbf{e}}
\newcommand{\zvec}{\mathbf{z}}
\newcommand{\mVec}{\mathbf{s}}
\newcommand{\tvec}{\mathbf{t}}
\newcommand{\rvec}{\mathbf{r}}
\newcommand{\dvec}{\mathbf{d}}
\newcommand{\mcL}{\mathcal{L}}
\newcommand{\mcR}{\mathcal{R}}
\newcommand{\mcT}{\mathcal{T}}
\newcommand{\mcY}{\mathcal{Y}}
\newcommand{\mcV}{\mathcal{V}}
\newcommand{\mcC}{\mathcal{C}}
\newcommand{\mcA}{\mathcal{A}}
\newcommand{\mcZ}{\mathcal{Z}}
\newcommand{\mcU}{\mathcal{U}}
\newcommand{\context}{\mathbf{y}_{\mathrm{c}}}
\newcommand{\embcontext}{\mathbf{\tilde{y}}_{\mathrm{c}}}
\newcommand{\inpcontext}{\mathbf{\tilde{x}}}
\newcommand{\start}{\mathbf{\tilde{y}}_{\mathrm{c0}}}
\newcommand{\End}{\mathrm{\texttt{</s>}}}
\newcommand{\Score}{\text{Score}}
\newcommand{\Uvec}{\mathbf{U}}
\newcommand{\Ivec}{\mathbf{I}}
\newcommand{\Evec}{\mathbf{E}}
\newcommand{\Gvec}{\mathbf{G}}
\newcommand{\Fvec}{\mathbf{F}}
\newcommand{\Pvec}{\mathbf{P}}
\newcommand{\mvec}{\mathbf{m}}
\newcommand{\pvec}{\mathbf{p}}
\newcommand{\Vvec}{\mathbf{V}}
\newcommand{\Wvec}{\mathbf{W}}
\newcommand{\hvec}{\mathbf{h}}
\newcommand{\wvec}{\mathbf{w}}
\newcommand{\uvec}{\mathbf{u}}
\newcommand{\vvec}{\mathbf{v}}
\newcommand{\bvec}{\mathbf{b}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\xmark}{\textcolor{red}{\ding{54}}}


\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{amsmath}

\setlength\titlebox{6.5cm}


\title{Prompting ChatGPT in MNER: Enhanced Multimodal Named Entity Recognition with Auxiliary Refined Knowledge}



\author{Jinyuan Li, Han Li, Zhuo Pan, Di Sun, Jiahao Wang, Wenkun Zhang, \textbf{Gang Pan\thanks{\hspace{1mm} corresponding authors. }}\\
 School of New Media and Communication, Tianjin University \\
 College of Mathematics, Taiyuan University of Technology \\
 College of Intelligence and Computing, Tianjin University \\
 Tianjin University of Science and Technology\\  
 University of Copenhagen \\
  {\tt \{jinyuanli, wjhwtt, pangang\}@tju.edu.cn,} 
  {\tt lihan0928@link.tyut.edu.com} \\
  {\tt panzhuotju@gmail.com,} 
  {\tt dsun@tust.edu.cn,}
  {\tt jls704@alumni.ku.dk}
}

\begin{document}
\maketitle
\begin{abstract}
Multimodal Named Entity Recognition (MNER) on social media aims to enhance textual entity prediction by incorporating image-based clues. Existing studies mainly focus on maximizing the utilization of pertinent image information or incorporating external knowledge from explicit knowledge bases. However, these methods either neglect the necessity of providing the model with external knowledge, or encounter issues of high redundancy in the retrieved knowledge. In this paper, we present PGIM --- a two-stage framework that aims to leverage ChatGPT as an implicit knowledge base and enable it to heuristically generate auxiliary knowledge for more efficient entity prediction. Specifically, PGIM contains a Multimodal Similar Example Awareness module that selects suitable examples from a small number of predefined artificial samples. These examples are then integrated into a formatted prompt template tailored to the MNER and guide ChatGPT to generate auxiliary refined knowledge. Finally, the acquired knowledge is integrated with the original text and fed into a downstream model for further processing. Extensive experiments show that PGIM outperforms state-of-the-art methods on two classic MNER datasets and exhibits a stronger robustness and generalization capability.\footnote{Our code is publicly available at \url{https://github.com/JinYuanLi0012/PGIM}} 
\end{abstract}

\section{Introduction}
\begin{figure}[h]
  \scriptsize
      \setlength{\belowcaptionskip}{-0.5cm}
    \centering  
    \includegraphics[height=1.8in,width=3.05in]{Head.pdf}
  \caption{The "CP3" in the text is a class of entities that are difficult to predict successfully by existing studies. PGIM demonstrates successful prediction of such entities with an approach more similar to human cognitive processes by endowing ChatGPT with reasonable heuristics.}
  \label{headphoto}
\end{figure}
Multimodal named entity recognition (MNER) has recently  garnered significant attention \citep{lu2018visual}. Users generate copious amounts of unstructured content primarily consisting of images and text on social media. The textual component in these posts possesses inherent characteristics associated with social media, including brevity and an informal style of writing. 
These unique characteristics pose challenges for traditional named entity recognition (NER) approaches \citep{chiu2016named,devlin2018bert}. 
To leverage the multimodal features and improve the NER performance, numerous previous works have attempted to align images and text implicitly using various attention mechanisms \citep{yu2020improving,sun2021rpbert}, but these  paradigm methods have several significant limitations. 
Limitation 1. The feature distribution of different modalities exhibits variations, which hinders the model to learn aligned representations across diverse modalities. Limitation 2. The image feature extractors used in these methods are trained on datasets like ImageNet \citep{deng2009imagenet} and COCO \citep{lin2014microsoft}, where the labels primarily consist of nouns rather than named entities. There are obvious deviations between the labels of these datasets and the named entities we aim to recognize. 
Given these limitations, these multimodal fusion methods may not be as effective as state-of-the-art language models that solely focus on text. 

While MNER is a multimodal task, the contributions of image and text modalities to this task are not equivalent. 
When the image cannot provide more interpretation information for the text, the image information can even be discarded and ignored. 
In addition, recent studies \citep{wang2021improving,zhang2022domain} has shown that introducing additional document-level context on the basis of the original text can significantly improve the performance of NER models. Therefore, recent studies \citep{wang2021ita,wang2022named} aim to solve the MNER task using the  paradigm.
In these approaches, images are reasonably  converted into textual representations through techniques such as image caption and optical character recognition (OCR). Apparently, the inter-text attention mechanism is more likely to outperform the cross-modal attention mechanism. 
However, existing second paradigm methods still exhibit certain potential deficiencies:
\begin{enumerate}[label=(\roman*)]
    \vspace{-3pt}
    \item
    For the methods that solely rely on in-sample information, 
    they often fall short in scenarios that demand additional external knowledge to enhance text comprehension. 
    \vspace{-3pt}
    \item
    For those existing methods that consider introducing external knowledge, the relevant knowledge retrieved from external explicit knowledge base (\emph{e.g.}, Wikipedia) is too redundant. These low-relevance extended knowledge may even mislead the model's understanding of the text in some cases.
\end{enumerate}
\vspace{-3pt}

Recently, the field of large language models (LLMs) is rapidly advancing with intriguing new findings and developments \citep{brown2020language,touvron2023llama}. On the one hand, recent research on LLMs \citep{qin2023chatgpt,wei2023zero,wang2023gpt} shows that the effect of the generative model in the sequence labeling task has obvious shortcomings. On the other hand, LLMs achieves promising results in various NLP \citep{vilar2022prompting,moslem2023adaptive} and multimodal tasks \citep{yang2022empirical,shao2023prompting}. 
These LLMs with in-context learning capability can be perceived as a comprehensive representation of internet-based knowledge and can offer high-quality auxiliary knowledge typically.
So we ask: \emph{Is it possible to activate the potential of ChatGPT in MNER task by endowing ChatGPT with reasonable heuristics?} 

In this paper, we present \textbf{PGIM} --- a conceptually simple framework that aims to boost the performance of model by \underline{P}rompting Chat\underline{G}PT \underline{I}n \underline{M}NER to generate auxiliary refined knowledge. 
As shown in Figure \ref{headphoto}, the additional auxiliary refined knowledge generated in this way overcomes the limitations of (i) and (ii). 
We begin by manually annotating a limited set of samples. Subsequently, PGIM utilizes the Multimodal Similar Example Awareness module to select relevant instances, and seamlessly integrates them into a meticulously crafted prompt template tailored for MNER task, thereby introducing pertinent knowledge. This approach effectively harnesses the in-context few-shot learning capability of ChatGPT. Finally, the auxiliary refined knowledge generated by heuristic approach of ChatGPT is subsequently combined with the original text and fed into a downstream text model for further processing. 

PGIM outperforms all state-of-the-art models based on the  and  paradigms on two classical MNER datasets and exhibits a stronger robustness and generalization capability. Moreover, compared with some previous methods, PGIM is friendly to most researchers, its implementation requires only a single GPU and a reasonable number of ChatGPT invocations.


\section{Related Work}
\subsection{Multimodal Named Entity Recognition}

Considering the inherent characteristics of social media text, previous approaches \citep{moon2018multimodal,zheng2020object,zhang2021multi,zhou2022span,zhao2022learning} have endeavored to incorporate visual information into NER. They employ diverse cross-modal attention mechanisms to facilitate the interaction between text and images. Recently, \citet{wang2021ita} points out that the performance limitations of such methods are largely attributed to the disparities in distribution between different modalities. Despite \citet{wang2022cat} try to mitigate the aforementioned issues by using further refining cross-modal attention, training this end-to-end cross-modal Transformer architectures imposes significant demands on computational resources. Due to the aforementioned limitations, ITA \citep{wang2021ita} and MoRe \citep{wang2022named} attempt to use a new paradigm to address MNER. ITA circumvents the challenge of multimodal alignment by forsaking the utilization of raw visual features and opting for OCR and image captioning techniques to convey image information. MoRe assists prediction by retrieving additional knowledge related to text and images from explicit knowledge base. 
However, none of these methods can adequately fulfill the requisite knowledge needed by the model to comprehend the text. The advancement of LLMs address the limitations identified in the aforementioned methods. While the direct prediction of named entities by LLMs in the full-shot case may not achieve comparable performance to task-specific models, we can utilize LLMs as an implicit knowledge base to heuristically generate further interpretations of text. This method is more aligned with the cognitive and reasoning processes of human. 


\subsection{In-context learning}
With the development of LLMs, empirical studies have shown that these models \citep{brown2020language} exhibit an interesting emerging behavior called In-Context Learning (ICL). Different from the paradigm of pre-training and then fine-tuning language models like BERT \citep{devlin2018bert}, LLMs represented by GPT have introduced a novel in-context few-shot learning paradigm. This paradigm requires no parameter updates and can achieve excellent results with just a few examples from downstream tasks. Since the effect of ICL is strongly related to the choice of demonstration examples, recent studies have explored several effective example selection methods, \emph{e.g.}, similarity-based retrieval method \citep{liu2021makes,rubin2021learning}, validation set scores based selection \citep{lee2021good}, gradient-based method \citep{wang2023large}. These results indicate that reasonable example selection can improve the performance of LLMs.

\begin{figure*}[t!]
	\centering
	\includegraphics[scale=0.48]{main.pdf}
	\caption{The architecture of PGIM.}
	\label{fig:architecture}
\end{figure*}
\section{Methodology}
PGIM is mainly divided into two stages. 
In the stage of generating auxiliary refined knowledge, PGIM leverages a limited set of predefined artificial samples and employs the Multimodal Similar Example Awareness (MSEA) module to carefully select relevant instances. These chosen examples are then incorporated into properly formatted prompts, thereby enhancing the heuristic guidance provided to ChatGPT for acquiring refined knowledge. (detailed in ยง\ref{sec:3.2}). 
In the stage of entity prediction based on auxiliary knowledge, PGIM combines the original text with the knowledge information generated by ChatGPT. This concatenated input is then fed into a transformer-based encoder to generate token representations.
Finally, PGIM feeds the representations into the linear-chain Conditional Random Field (CRF) \citep{lafferty2001conditional} layer to predict the probability distribution of the original text sequence (detailed in ยง\ref{sec:3.3}). An overview of the PGIM is depicted in Figure \ref{fig:architecture}.

\subsection{Preliminaries}
Before presenting the PGIM, we first formulate the MNER task, and briefly introduce the in-context learning paradigm originally developed by GPT-3 \citep{brown2020language} and its adaptation to MNER.

\paragraph{Task Formulation} 
Consider treating the MNER task as a sequence labeling task. Given a sentence  with  tokens and its corresponding image , the goal of MNER is to locate and classify named entities mentioned in the sentence as a label sequence , where  are predefined semantic categories with the BIO2 tagging schema \citep{sang1999representing}.

\paragraph{In-context learning in MNER}
GPT-3 and its successor ChatGPT (hereinafter referred to collectively as GPT) are autoregressive language models pretrained on a tremendous dataset. During inference, in-context few-shot learning accomplishes new downstream tasks in the manner of text sequence generation tasks on frozen GPT models. Concretely, given a test input , its target  is predicted based on the formatted prompt  as the condition, where  refers to a prompt head describing the task and in-context  contains  in-context examples. 
All the , , ,  are text sequences, and target  is a text sequence with the length of . At each decoding step , we have:

where LLM represents the weights of the pre-trained large language model, which are frozen for new tasks. Each in-context example  consists of an input-target pair of the task, and these examples is constructed manually or sampled from the training set.
 
Although the GPT-4\footnote{\url{https://openai.com/product/gpt-4}} can accept the input of multimodal information, this function is only in the internal testing stage and has not yet been opened for public use. In addition, compared with ChatGPT, GPT-4 has higher costs and slower API request speeds. In order to enhance the reproducibility of PGIM, we still choose ChatGPT as the main research object of our method. 
And this paradigm provided by PGIM can also be used in GPT-4. 
In order to enable ChatGPT to complete the image-text multimodal task, we use advanced multimodal pre-training model to convert images into image captions. Inspired by PICa \citep{yang2022empirical} and Prophet \citep{shao2023prompting} in knowledge-based VQA, PGIM formulates the testing input  as the following template:

\begin{table}[H]
    \fontsize{10}{10}\selectfont
    \centerline{
    \begin{tabular}{|l|}
    \hline
    \texttt{Text:} \textcolor{blue}{} \textbackslash\texttt{n} 
    \texttt{Image:} \textcolor{blue}{} \textbackslash\texttt{n}\enspace
    \texttt{Question:} \textcolor{blue}{} \textbackslash\texttt{n} 
    \texttt{Answer:}\enspace\enspace \\
    \hline
    \end{tabular} }
    
\end{table}
\noindent
where \textcolor{blue}{}, \textcolor{blue}{} and \textcolor{blue}{} represent specific test inputs. \textbackslash\texttt{n} stands for a carriage return in the template. Similarly, each in-context example  is defined with similar templates as follows:
\begin{table}[H]
    \fontsize{10}{10}\selectfont
    \centerline{
    \begin{tabular}{|l|}
    \hline
    \texttt{Text:} \textcolor{blue}{}\textbackslash\texttt{n} 
    \texttt{Image:} \textcolor{blue}{}\textbackslash\texttt{n}
    \texttt{Question:} \textcolor{blue}{} \textbackslash\texttt{n} 
    \texttt{Answer:} \textcolor{blue}{} \\
    \hline
    \end{tabular} {}}
\end{table}
\noindent
where \textcolor{blue}{}, \textcolor{blue}{}, \textcolor{blue}{} and \textcolor{blue}{} refer to an text-image-question-answer quadruple retrieved from predefined artificial samples. The complete prompt template of MNER consisting of a fixed prompt head, some in-context examples, and a test input is fed to ChatGPT for auxiliary knowledge generation.

\subsection{Stage-1. Auxiliary Refined Knowledge Heuristic Generation}\label{sec:3.2}
\paragraph{Predefined artificial samples} 
The key to making ChatGPT performs better in MNER is to choose suitable in-context examples. 
Acquiring accurately annotated in-context examples that precisely reflect the annotation style of the dataset and provide a means to expand auxiliary knowledge poses a significant challenge. And directly acquiring such examples from the original dataset is not feasible. 

To address this issue, we employ a random sampling approach to select a small subset of samples from the training set for manual annotation. Specifically, for Twitter-2017 dataset, we randomly sample 200 samples from training set for manual labeling, and for Twitter-2015 dataset, the number is 120. The annotation process comprises two main components. The first part involves identifying the named entities within the sentences, and the second part involves providing comprehensive justification by considering the image and text content, as well as relevant knowledge. For many possibilities encounter in the labeling process, what the annotator needs to do is to correctly judge and interpret the sample from the perspective of humans. For samples where images and text are related, we directly state which entities in the text are emphasized by the image. For samples where the image and text are unrelated, we directly declare that the image description is unrelated to the text. Through artificial annotation process, we emphasize the entities and their corresponding categories within the sentences. Furthermore, we incorporate relevant auxiliary knowledge to support these judgments. This meticulous annotation process serves as a guide for ChatGPT, enabling it to generate highly relevant and valuable responses.

\paragraph{Multimodel Similar Example Awareness Module}
Since the few-shot learning ability of GPT largely depends on the selection of in-context examples \citep{liu2021makes,yang2022empirical}, we design a Multimodel Similar Example Awareness (MSEA) module to select appropriate in-context examples. As a classic multimodal task, the prediction of MNER relies on the integration of both textual and visual information. Accordingly, PGIM leverages the fused features of text and image as the fundamental criterion for assessing similar examples.
And this multimodal fusion feature can be obtained from various previous vanilla MNER models. 

Denote the MNER dataset  and predefined artificial samples  as:
\vspace{-3pt}


where  refer to the text, image, and gold labels. The vanilla MNER model  trained on  mainly consists of a backbone encoder  and a CRF decoder . The input multimodal image-text pair is encoded by the encoder  to obtain multimodal fusion features :
\vspace{-3pt}


In previous studies, the fusion feature  after cross-attention projection into the high-dimensional latent space was directly input to the decoder layer for the prediction of the result. 
Unlike them, PGIM chooses  as the judgment basis for similar examples. 
Because examples approximated in high-dimensional latent space are more likely to have the same mapping method and entity type. 
PGIM calculates the cosine similarity of the fused feature  between the test input and each predefined artificial sample. And top- similar predefined artificial samples will be selected as in-context examples to enlighten ChatGPT generation auxiliary refined knowledge: 
\vspace{-3pt}

 is the index set of top- similar samples in . The in-context examples  are defined as follows: 
\vspace{-3pt}


In order to efficiently realize the awareness of similar examples, all the multimodal fusion features can be calculated and stored in advance.

\paragraph{Heuristics-enhanced Prompt Generation}
After obtaining the in-context example , PGIM builds a complete heuristics-enhanced prompt to exploit the few-shot learning ability of ChatGPT in MNER. 

A prompt head, a set of in-context examples, and a testing input together form a complete prompt. The prompt head describes the MNER task in natural language according to the requirements. 
Given that the input image and text may not always have a direct correlation, PGIM encourages ChatGPT to exercise its own discretion. 
The in-context examples are constructed from the results  of the MSEA module. For testing input, the answer slot is left blank for ChatGPT to generate.
The complete format of the prompt template is shown in Appendix \ref{templatephoto}. 

\subsection{Stage-2. Entity Prediction based on Auxiliary Refined Knowledge} \label{sec:3.3}
Define the auxiliary knowledge generated by ChatGPT after in-context learning as , where  is the length of . 
PGIM concatenates the original text  with the obtained auxiliary refining knowledge  as  and feeds it to the transformer-based encoder:
\vspace{-3pt}

Due to the attention mechanism employed by the transformer-based encoder, the token representation  obtained encompasses pertinent cues from the auxiliary knowledge . Similar to the previous studies, PGIM feeds  to a standard linear-chain CRF layer, which defines the probability of the label sequence  given the input sentence :

where  and  are potential functions. Finally, PGIM uses the negative log-likelihood (NLL) as the loss function for the input sequence with gold labels :
\vspace{-3pt}


\section{Experiments}
\subsection{Settings}

\begin{table*}[!]
\small
\setlength\tabcolsep{1.7pt}
\renewcommand{\arraystretch}{1.2}
\centering
\begin{tabular}{lcccccccccccccc}
\toprule
 \multirow{3}{*}{Methods} 
 & \multicolumn{7}{|c|}{\textbf{Twitter-2015}}  &\multicolumn{7}{c}{\textbf{Twitter-2017}}  \\\cline{2-15}
 & \multicolumn{4}{|c|}{\textbf{Single Type(F1)}} & \multicolumn{3}{c|}{\textbf{Overall}} & \multicolumn{4}{c|}{\textbf{Single Type(F1)}} & \multicolumn{3}{c}{\textbf{Overall}}\\
  &\multicolumn{1}{|c}{PER}   & LOC   & ORG  & \multicolumn{1}{c|}{OTH.}  & Pre.   & Rec.   & \multicolumn{1}{c|}{F1} & PER   & LOC   & ORG   & \multicolumn{1}{c|}{OTH.}  & Pre. &Rec. &F1   \\ 
\midrule 
 \multicolumn{15}{c}{ Text}\\
\midrule
BiLSTM-CRF &\multicolumn{1}{|c}{76.77} & 72.56 & 41.33 & \multicolumn{1}{c|}{26.80} & 68.14 & 61.09 & \multicolumn{1}{c|}{64.42} & 85.12 & 72.68 & 72.50 & \multicolumn{1}{c|}{52.56} & 79.42 & 73.43 & 76.31 \\
BERT-CRF & \multicolumn{1}{|c}{85.37} & 81.82 & 63.26 & \multicolumn{1}{c|}{44.13} & 75.56 & 73.88 & \multicolumn{1}{c|}{74.71} & 90.66 & 84.89 & 83.71 & \multicolumn{1}{c|}{66.86} & 86.10 & 83.85 & 84.96  \\
BERT-SPAN & \multicolumn{1}{|c}{85.35} & 81.88 & 62.06 & \multicolumn{1}{c|}{43.23} & 75.52 & 73.83 & \multicolumn{1}{c|}{74.76} & 90.84 & 85.55 & 81.99 & \multicolumn{1}{c|}{69.77} & 85.68 & 84.60 & 85.14 \\
RoBERTa-SPAN & \multicolumn{1}{|c}{87.20} & 83.58 & 66.33 & \multicolumn{1}{c|}{50.66} & 77.48 & 77.43 & \multicolumn{1}{c|}{77.45} & 94.27 & 86.23 & 87.22 & \multicolumn{1}{c|}{74.94} & 88.71 & 89.44 & 89.06 \\
\midrule 
\multicolumn{15}{c}{ Text+Image}\\
\midrule
UMT & \multicolumn{1}{|c}{85.24} & 81.58 &  63.03 &  \multicolumn{1}{c|}{39.45} &  71.67 &  75.23 &  \multicolumn{1}{c|}{73.41} &  91.56 &  84.73 & 82.24 &  \multicolumn{1}{c|}{70.10} &  85.28 &  85.34 & 85.31 \\
UMGF & \multicolumn{1}{|c}{84.26} & 83.17 & 62.45 &\multicolumn{1}{c|}{42.42} & 74.49 & 75.21 &  \multicolumn{1}{c|}{74.85} &  91.92 &  85.22 &  83.13 &
  \multicolumn{1}{c|}{69.83} &  86.54 &  84.50 & 85.51 \\
MNER-QG & \multicolumn{1}{|c}{85.68} &  81.42 &  63.62 &  \multicolumn{1}{c|}{41.53} &77.76 & 72.31 &  \multicolumn{1}{c|}{74.94} &  93.17 &  86.02 & 84.64 &  \multicolumn{1}{c|}{71.83} & 88.57 &  85.96 &  87.25 \\ 
R-GCN &  \multicolumn{1}{|c}{86.36} &  82.08 &  60.78 &\multicolumn{1}{c|}{41.56} &  73.95 &  76.18 &  \multicolumn{1}{c|}{75.00} &  92.86 &  86.10 &  84.05 &  \multicolumn{1}{c|}{72.38} &  86.72 &  87.53 &  87.11 \\
ITA &  \multicolumn{1}{|c}{-} &  - &  - &  \multicolumn{1}{c|}{-} &  - &  - &  \multicolumn{1}{c|}{78.03} &  - &  - &  - &  \multicolumn{1}{c|}{-} &  - &  - &  89.75 \\
PromptMNER &  \multicolumn{1}{|c}{-} &  - &  - &  \multicolumn{1}{c|}{-} &  78.03 &  79.17 &  \multicolumn{1}{c|}{78.60} &  - &  - &  - &\multicolumn{1}{c|}{-} &  89.93 &  90.60 &  90.27 \\
CAT-MNER &  \multicolumn{1}{|c}{88.04} &  \textbf{84.70} &  68.04 &  \multicolumn{1}{c|}{52.33} &  78.75 &  78.69 &  \multicolumn{1}{c|}{78.72} &  94.61 &  88.40 &  88.14 &  \multicolumn{1}{c|}{\textbf{80.50}} &  90.27 &  90.67 &  90.47\\
MoRe &  \multicolumn{1}{|c}{-} &  - &  - &  \multicolumn{1}{c|}{-} &  - &  - &
\multicolumn{1}{c|}{77.79} &  - &- &  - &  \multicolumn{1}{c|}{-} &  - &  - &  89.49 \\
MoRe &  \multicolumn{1}{|c}{-} &  - &  - &  \multicolumn{1}{c|}{-} &  - &  - &
\multicolumn{1}{c|}{77.57} &  - &- &  - &  \multicolumn{1}{c|}{-} &  - &  - &  90.28 \\
MoRe &  \multicolumn{1}{|c}{-} &  - &  - &  \multicolumn{1}{c|}{-} &  - &  - &
\multicolumn{1}{c|}{79.21} &  - &- &  - &  \multicolumn{1}{c|}{-} &  - &  - &  90.67 \\
\textbf{PGIM}(Ours) &  \multicolumn{1}{|c}{\textbf{88.34}} &  84.22 &  \textbf{70.15} &  \multicolumn{1}{c|}{\textbf{52.34}} &  \textbf{79.21} &  \textbf{79.45} &  \multicolumn{1}{c|}{\textbf{79.33}*} &  \textbf{96.46} &  \textbf{89.89} &  \textbf{89.03} &  \multicolumn{1}{c|}{79.62} &  \textbf{90.86} &  \textbf{92.01} & \textbf{91.43*} \\
      &  \multicolumn{1}{|c}{\textcolor{red}{0.02}} &  \textcolor{red}{0.12} &  \textcolor{red}{0.36} &  \multicolumn{1}{c|}{\textcolor{red}{0.98}} &  \textcolor{red}{0.63} &  \textcolor{red}{0.22} & 
\multicolumn{1}{c|}{\textcolor{red}{0.06}} &  \textcolor{red}{0.02} &\textcolor{red}{0.68} &  \textcolor{red}{0.53} &  \multicolumn{1}{c|}{\textcolor{red}{2.25}} &  \textcolor{red}{0.16} &  \textcolor{red}{0.07} &  \textcolor{red}{0.09} \\

\bottomrule
\end{tabular}
\caption{Performance comparison on the Twitter-15 and Twitter-17 datasets. For the baseline model, results of methods with  come from 
\citet{yu2020improving}, and results with  come from  \citet{wang2022cat}. The results of multimodal methods are all retrieved from the corresponding original paper. The marker * refers to significant test p-value < 0.05 when comparing with CAT-MNER and MoRe.}
\label{tab:maintabel}
\end{table*}

\paragraph{Datasets}
We conduct experiments on two public MNER datasets: Twitter-2015 \citep{zhang2018adaptive} and Twitter-2017 \citep{lu2018visual}.
These two classic MNER datasets contain 4000/1000/3257 and 3373/723/723 (train/development/test) image-text pairs posted by users on Twitter.

\paragraph{Model Configuration}
PGIM chooses the backbone of UMT \citep{yu2020improving} as the vanilla MNER model to extract multimodal fusion features. This backbone completes multimodal fusion without too much modification. 
BLIP-2 \citep{li2023blip} as an advanced multimodal pre-trained model, is used for conversion from image to image caption. 
The version of ChatGPT used in experiments is  and sampling temperature is set to 0. 
For a fair comparison, PGIM chooses to use the same text encoder XLM-RoBERTa \citep{conneau2019unsupervised} as ITA \citep{wang2021ita}, PromptMNER \citep{wang2022promptmner}, CAT-MNER \citep{wang2022cat} and MoRe \citep{wang2022named}.


\paragraph{Implementation Details}
PGIM is trained by Pytorch on single NVIDIA RTX 3090 GPU. During training, we use AdamW \citep{loshchilov2017decoupled} optimizer to minimize the loss function. We use grid search to find the learning rate for the embeddings within . 
Due to the different labeling styles of two datasets, the learning rates of Twitter-2015 and Twitter-2017 are finally set to  and . 
And we also use warmup linear scheduler to control the learning rate. 
The maximum length of the sentence input is set to 256, and the mini-batch size is set to 4. 
The model is trained for 25 epochs, and the model with the highest F1-score on the development set is selected to evaluate the performance on the test set. The number of in-context examples  in PGIM is set to 5. All of the results are averaged from 3 runs with different random seeds.

\subsection{Main Results}
\label{MainResults}
We compare PGIM with previous state-of-the-art approaches on MNER in Table \ref{tab:maintabel}. The first group of methods includes BiLSTM-CRF \citep{huang2015bidirectional},
BERT-CRF \citep{devlin2018bert} as well as the span-based NER models (\emph{e.g.}, BERT-span, RoBERTa-span \citep{yamada2020luke}), which only consider original text. The second group of methods includes several latest multimodal approaches for MNER task: UMT \citep{yu2020improving}, UMGF \citep{zhang2021multi}, MNER-QG \citep{jia2022mner}, R-GCN \citep{zhao2022learning}, ITA \citep{wang2021ita}, PromptMNER \citep{wang2022promptmner}, CAT-MNER \citep{wang2022cat} and MoRe \citep{wang2022named}, which consider both text and corresponding images. 

The experimental results demonstrate the superiority of PGIM over previous methods. PGIM surpasses the previous state-of-the-art method MoRe \citep{wang2022named} in terms of performance. This suggests that compared with the auxiliary knowledge retrieved by MoRe \citep{wang2022named} from Wikipedia, our refined auxiliary knowledge offers more substantial support. Furthermore, PGIM exhibits a more significant improvement in Twitter-2017 compared with Twitter-2015. This can be attributed to the more complete and standardized labeling approach adopted in Twitter-2017, in contrast to Twitter-2015. 
Apparently, the quality of dataset annotation has a certain influence on the accuracy of MNER model. 
In cases where the dataset annotation deviates from the ground truth, 
accurate and refined auxiliary knowledge leads the model to prioritize predicting the truly correct entities, since the process of ChatGPT heuristically generating auxiliary knowledge is not affected by mislabeling. 
This phenomenon coincidentally highlights the robustness of PGIM.  
The ultimate objective of the MNER is to support downstream tasks effectively. Obviously, downstream tasks of MNER expect to receive MNER model outputs that are unaffected by irregular labeling in the training dataset. 
We further demonstrate this argument through a case study, detailed in the Appendix \ref{misableled}.


\subsection{Detailed Analysis}

\begin{table}[t!]
\small
\setlength\tabcolsep{1.6pt}
\renewcommand{\arraystretch}{1.2}
\centering
\begin{tabular}{l|cccccc}
\toprule
& \multicolumn{3}{c|}{\textbf{Twitter-2015}} & \multicolumn{3}{c}{\textbf{Twitter-2017}}\\
& Pre. & Rec. & \multicolumn{1}{c|}{F1}  & Pre. & Rec. & \multicolumn{1}{c}{F1}    \\
\midrule
Baseline   & 75.56 &  73.88 & \multicolumn{1}{c|}{74.71}  & 86.10 & 83.85 & 84.96  \\
UMT    & 71.67 & 75.23 & \multicolumn{1}{c|}{73.41}  & 85.28 & 85.34 & 85.31  \\
UMGF    & 74.49  & 75.21 & \multicolumn{1}{c|}{74.85}  & 86.54  & 84.50 & 85.51  \\
R-GCN    & 73.95  & 76.18 & \multicolumn{1}{c|}{75.00}  & 86.72  & 87.53 & 87.11  \\
ITA & -  & - & \multicolumn{1}{c|}{75.60}  & -  & - & 85.72  \\
CAT  & \textbf{76.19} & 74.65 & \multicolumn{1}{c|}{75.41} & 87.04 & 84.97 & 85.99 \\
MoRe & 73.16 & 74.64 & \multicolumn{1}{c|}{73.89} & 85.49 & 86.38 & 85.94 \\
MoRe & 73.31 & 74.43 & \multicolumn{1}{c|}{73.86} & 85.92 & 86.75 & 86.34 \\
PGIM & 75.84 & \textbf{77.76} & \multicolumn{1}{c|}{\textbf{76.79}*} & \textbf{89.09} & \textbf{90.08} & \textbf{89.58*} \\
     & \textcolor{red}{0.30} & \textcolor{red}{0.22} & \multicolumn{1}{c|}{\textcolor{red}{0.19}} & \textcolor{red}{0.24} & \textcolor{red}{0.08} & \textcolor{red}{0.10}\\ 
\bottomrule
\end{tabular}
\caption{Results of methods with  are retrieved from the corresponding original paper. And results with  come from  \citet{wang2022cat}.}
\label{tab:PGIM BERT}
\end{table}



\paragraph{Impact of different text encoders on performance}
As shown in Table \ref{tab:PGIM BERT}, We perform experiments by replacing the encoders of all XLM-RoBERTa \citep{conneau2019unsupervised} MNER methods with BERT \citep{kenton2019bert}. Baseline represents inputting original samples into BERT-CRF. All of the results are averaged from 3 runs with different random seeds. The marker * refers to significant test p-value < 0.05 when comparing with ITA, CAT-MNER and MoRe. And  represents the results after we replace the text encoder in the MoRe official code with BERT.\footnote{\url{https://github.com/modelscope/AdaSeq/tree/master/examples/MoRe}} The experimental results show that PGIM achieves a greater performance improvement than the XLM-RoBERTa version experiment, especially on the Twitter-2017 dataset. We think the reasons for this phenomenon are as follows: XLM-RoBERTa conceals the defects of previous MNER methods through its strong encoding ability, and these defects are further amplified after using BERT. For example, the encoding ability of BERT on long text is weaker than XLM-RoBERTa, and the additional knowledge retrieved by MoRe is much longer than PGIM. Therefore, as shown in Table \ref{tab:PGIM BERT} and Table \ref{tab:PGIM&MORE}, the performance loss of MoRe is larger than the performance loss of PGIM after replacing BERT. 

  
\paragraph{Compared with direct prediction of ChatGPT}
\label{ChatGPT direct prediction}
Table \ref{tab:Prompt ChatGPT} presents the performance comparison between ChatGPT and PGIM in the few-shot scenario. VanillaGPT stands for no prompting, and PromptGPT denotes the selection of top-N similar samples for in-context learning.
As shown in Appendix \ref{templatephoto}, we employ a similar prompt template and utilize the same predefined artificial samples as in-context examples for the MSEA module to select from. But the labels of these samples are replaced by named entities in the text instead of auxiliary knowledge. 
The BIO annotation method is not considered in this experiment because it is a little difficult for ChatGPT.
Only the complete match will be considered, and only if the entity boundary and entity type are both accurately predicted, we judge it as a correct prediction. 

The results show that the performance of ChatGPT on MNER is far from satisfactory compared with PGIM in the full-shot case, which once again confirms the previous conclusion of ChatGPT on NER \citep{qin2023chatgpt}. 
In other words, when we have enough training samples, only relying on ChatGPT itself will not be able to achieve the desired effect.
The capability of ChatGPT shines in scenarios where sample data are scarce. 
Due to the in-context learning ability of ChatGPT, it can achieve significant performance improvement after learning a small number of carefully selected samples, and its performance increases linearly with the increase of the number of in-context samples. 
\begin{figure*}
	\centering
	\includegraphics[scale=0.475]{sample.pdf}
	\caption{Four case studies of how auxiliary refined knowledge can help model predictions.}
	\label{fig:cases}
\end{figure*}
We conduct experiments to evaluate the performance of PGIM in few-shot case. 
For each few-shot experiment, we randomly select 3 sets of training data and train 3 times on each set to obtain the average result. The results show that after 10 prompts, ChatGPT performs better than PGIM in the fs-100 scenario on both datasets. This suggests that ChatGPT exhibits superior performance when confronted with limited training samples.


\begin{table}[t!]
\small
\setlength\tabcolsep{2.3pt}
\renewcommand{\arraystretch}{1.2}
\centering
\begin{tabular}{l|cccccc}
\toprule
& \multicolumn{3}{c|}{\textbf{Twitter-2015}} & \multicolumn{3}{c}{\textbf{Twitter-2017}}\\
& Pre. & Rec. & \multicolumn{1}{c|}{F1}  & Pre. & Rec. & \multicolumn{1}{c}{F1}    \\
\midrule
fs-10      & 0.16  & 12.00 & \multicolumn{1}{c|}{0.32}  & 0.26  & 12.95 & 0.51  \\
fs-50      & 50.09 & 54.50 & \multicolumn{1}{c|}{52.20} & 49.40 & 51.96 & 50.65 \\
fs-100     & 57.33 & 66.26 & \multicolumn{1}{c|}{61.47} & 69.51 & 74.09 & 71.73 \\
fs-200     & 65.16 & 73.57 & \multicolumn{1}{c|}{69.11} & 80.97 & 85.05 & 82.96 \\
full-shot  & 79.21 & 79.45 & \multicolumn{1}{c|}{79.33} & 90.86 & 92.01 & 91.43 \\
\midrule
VanillaGPT & 42.96 & 75.37 & \multicolumn{1}{c|}{54.73} & 52.19 & 75.03 & 61.56\\
PromptGPT & 51.96 & 75.24 & \multicolumn{1}{c|}{61.47} & 56.99 & 74.77 & 64.68 \\
PromptGPT & 57.41 & 73.98 & \multicolumn{1}{c|}{64.65} & 72.03 & 75.50 & 73.73 \\
PromptGPT & 58.57 & 74.07 & \multicolumn{1}{c|}{65.41} & 72.90 & 77.65 & 75.20 \\
\bottomrule
\end{tabular}
\caption{Comparison of ChatGPT and PGIM in few-shot case. VanillaGPT and PromptGPT stand for direct prediction using ChatGPT.}
\label{tab:Prompt ChatGPT}
\end{table}

\begin{table}[t!]
\small
\setlength\tabcolsep{2.6pt}
\renewcommand{\arraystretch}{1.2}
\centering
\begin{tabular}{l|cccccc}
\toprule
& \multicolumn{3}{c|}{\textbf{Twitter-2015}} & \multicolumn{3}{c}{\textbf{Twitter-2017}}\\
& Pre. & Rec. & \multicolumn{1}{c|}{F1}  & Pre. & Rec. & \multicolumn{1}{c}{F1}    \\
\midrule
Baseline      & 76.45  & 78.22 & \multicolumn{1}{c|}{77.32}  & 88.46  & 90.23 & 89.34  \\\midrule
w/o MSEA     & 78.15  & 79.01 & \multicolumn{1}{c|}{78.58}  & 90.49  & 90.82 & 90.65  \\
w/o MSEA     & 78.11  & \textbf{79.82} & \multicolumn{1}{c|}{78.95}  & 90.62  & 91.49 & 91.05  \\
w/o MSEA     & 78.47  & 79.21 & \multicolumn{1}{c|}{78.84}  & 90.54  & 91.77 & 91.15 \\\midrule
PGIM     & 78.40 & 79.21 & \multicolumn{1}{c|}{78.76} & 89.90 & 91.63 & 90.76 \\
\textbf{PGIM}     & \textbf{79.21} & 79.45 & \multicolumn{1}{c|}{\textbf{79.33}} & \textbf{90.86} & 92.01 & \textbf{91.43} \\
PGIM      & 78.58 & 79.67 & \multicolumn{1}{c|}{79.12} & 90.54 & \textbf{92.08} & 91.30 \\
\bottomrule
\end{tabular}
\caption{Effect of the number of in-context examples on auxiliary refined knowledge.}
\label{tab:PGIM ChatGPT4}
\end{table}
\paragraph{Effectiveness of MSEA Module}

Table \ref{tab:PGIM ChatGPT4} demonstrates the effectiveness of the MSEA module. 
We use the auxiliary refined knowledge generated by ChatGPT after  in-context prompts to construct the datasets and train the model. 
The text encoder of the Baseline model is XLM-RoBERTa, and its input is the original text that does not contain any auxiliary knowledge. 
w/o MSEA represents a random choice of in-context examples. All results are averages of training results after three random initializations. 
Obviously, the addition of auxiliary refined knowledge can improve the effect of the model. And the addition of MSEA module can further improve the quality of the auxiliary knowledge generated by ChatGPT, which reflects the effectiveness of the MSEA module. An appropriate number of in-context examples can further improve the quality of auxiliary refined knowledge. 
But the number of examples is not the more the better. When ChatGPT is provided with an excessive number of examples, the quality of the auxiliary knowledge may deteriorate. One possible explanation for this phenomenon is that too many artificial examples introduce noise into the generation process of ChatGPT.
As a pre-trained large language model, ChatGPT lacks genuine comprehension of the underlying logical implications in the examples. Consequently, an excessive number of examples may disrupt its original reasoning process. 


\paragraph{Case Study}
Through some case studies in Figure \ref{fig:cases}, we show how auxiliary refined knowledge can help improve the predictive performance of the model. The Baseline model represents that no auxiliary knowledge is introduced. MoRe and MoRe denote the relevant knowledge of the input text and image retrieved using text retriever and image retriever, respectively. 
In PGIM, the auxiliary refined knowledge generated by ChatGPT is structured into two components: the first component provides a preliminary estimation of the named entity, and the second component offers a corresponding contextual explanation. 
In these examples, "Leadership Course", "Big B", "Maxim" and "Mumbai BJP" are all entities that were not accurately predicted by past methods. Because our auxiliary refined knowledge provides explicit explanations for such entities, PGIM makes the correct prediction.


\section{Conclusion}
In this paper, we propose a two-stage framework called PGIM and bring the potential of LLMs to MNER in a novel way. 
Extensive experiments show that PGIM outperforms state-of-the-art methods and considerably overcomes obvious problems in previous studies. 
Additionally, PGIM exhibits a strong robustness and generalization capability, and only necessitates a single GPU and a reasonable number of ChatGPT invocations. 
In our opinion, this is an ingenious way of introducing LLMs into MNER. 
We hope that PGIM will serve as a solid baseline to inspire future research on MNER and ultimately solve this task better.

\section*{Limitations}
In this paper, PGIM enables the integration of multimodal tasks into large language model by converting images into image captions. While PGIM achieves impressive results, we consider this Text-Text paradigm as a transitional phase in the development of MNER, rather than the ultimate solution. Because image captions are inherently limited in their ability to fully capture all the details of an image. This issue may potentially be further resolved in conjunction with the advancement of multimodal capabilities in language and vision models (\emph{e.g.}, GPT-4).


\section*{Ethics Statement}
In this paper, we use publicly available Twitter-2015, Twitter-2017 datasets for experiments. For the auxiliary refined knowledge, PGIM generates them using ChatGPT. Therefore, we trust that all data we use does not violate the privacy of any user.

\section*{Acknowledgments}
This work was supported by the Natural Science Foundation of Tianjin (No.21JCYBJC00640) and by the 2023 CCF-Baidu Songguo Foundation (Research on Scene Text Recognition Based on PaddlePaddle).

\bibliography{cite}
\bibliographystyle{acl_natbib}


\appendix

\section{Appendix}
\label{sec:appendix}


\begin{table*}[!]
\small
\setlength\tabcolsep{5pt}
\renewcommand{\arraystretch}{1.2}
\centering
\begin{tabular}{lcccc|ccc|c|c|c}
\toprule
 \multirow{2}{*}{\textbf{Methods}} 
 & \multicolumn{4}{|c|}{\textbf{Single Type(F1)}} & \multicolumn{3}{c|}{\textbf{Overall}} & \multirow{2}{*}{\textbf{Ave. length}}  &\multirow{2}{*}{\textbf{Memory(MB)}} &\multirow{2}{*}{\textbf{Ave. Improve}} \\
  &\multicolumn{1}{|c}{PER}   & LOC   & ORG  & \multicolumn{1}{c|}{OTH.}  & Pre.   & Rec.   & \multicolumn{1}{c|}{F1}  &&  \\ 
\midrule 
 \multicolumn{11}{c}{Twitter-2015}\\
\midrule
BaseLine &\multicolumn{1}{|c}{87.04} & 83.49 & 67.34 & \multicolumn{1}{c|}{50.16} & 76.45 & 78.22 & \multicolumn{1}{c|}{77.32} & - & 11865 & - \\\midrule
MoRe &\multicolumn{1}{|c}{86.92} & 83.08 & 68.20 & \multicolumn{1}{c|}{49.15} & 77.12 & 77.77 & \multicolumn{1}{c|}{77.45} & 227.41 & 16759 &  0.05  \\
MoRe &\multicolumn{1}{|c}{87.38} & 83.78 & 67.75 & \multicolumn{1}{c|}{49.38} & 77.44 & 78.06 & \multicolumn{1}{c|}{77.75} & 203.00 & 16711 &  0.28 \\
\textbf{PGIM}(Ours)  &\multicolumn{1}{|c}{\textbf{88.34}} & \textbf{84.22} & \textbf{70.15} & \multicolumn{1}{c|}{\textbf{52.34}} & \textbf{79.21} & \textbf{79.45} & \multicolumn{1}{c|}{\textbf{79.33}} & \textbf{104.56} & \textbf{13901} & \textbf{ 1.86} \\
\midrule 
\multicolumn{11}{c}{Twitter-2017}\\
\midrule
BaseLine &\multicolumn{1}{|c}{95.07} & 87.22 & 85.82 & \multicolumn{1}{c|}{78.66} & 88.46  & 90.23 &  \multicolumn{1}{c|}{89.34} & - & 11801 & -  \\\midrule
MoRe &\multicolumn{1}{|c}{95.16} & 88.77 & 87.00 & \multicolumn{1}{c|}{77.71} & 89.33 & 90.45 & \multicolumn{1}{c|}{89.89} & 241.47 & 16695 &  0.50 \\
MoRe &\multicolumn{1}{|c}{94.43} & 87.43 & 86.22 & \multicolumn{1}{c|}{74.77} & 88.06 & 89.49 & \multicolumn{1}{c|}{88.77} & 192.00 & 16447 &  0.80 \\
\textbf{PGIM}(Ours)  &\multicolumn{1}{|c}{\textbf{96.46}} & \textbf{89.89} & \textbf{89.03} & \multicolumn{1}{c|}{\textbf{79.62}} & \textbf{90.86} & \textbf{92.01} & \multicolumn{1}{c|}{\textbf{91.43}} & \textbf{94.52} & \textbf{13279}  & \textbf{ 2.07}\\

\bottomrule
\end{tabular}
\caption{Comparison of MoRe with PGIM. Since the original paper of MoRe \citep{wang2022named} did not report its Single Type (F1) on the Twitter-2015 and Twitter-2017 datasets, we run its released code and count the results. All of the results are averaged from 3 runs with different random seeds. }
\label{tab:PGIM&MORE}
\end{table*}


\begin{table}[t!]
\small
\setlength\tabcolsep{3pt}
\renewcommand{\arraystretch}{1.2}
\centering
\begin{tabular}{l|cccccc}
\toprule
& \multicolumn{3}{c|}{\textbf{Twitter-2015}\textbf{2017}}& \multicolumn{3}{c}{\textbf{Twitter-2017}\textbf{2015}}\\
& Pre. & Rec. & \multicolumn{1}{c|}{F1}  & Pre. & Rec. & \multicolumn{1}{c}{F1}    \\
\midrule
UMT     & 67.80  & 55.23 & \multicolumn{1}{c|}{60.87}  & 64.67  & 63.59 & 64.13  \\
UMGF     & 69.88 & 56.92 & \multicolumn{1}{c|}{62.74} & 67.00 & 62.81 & 66.21 \\
CAT-MNER  & 70.69 & 59.44 & \multicolumn{1}{c|}{64.58} & 74.86 & 63.01 & 68.43 \\
\textbf{PGIM}      & \textbf{72.66} & \textbf{65.51} & \multicolumn{1}{c|}{\textbf{68.90}} & \textbf{76.13} & \textbf{64.87} & \textbf{70.05} \\
\bottomrule
\end{tabular}
\caption{Comparison of the generalization ability. For the baseline model, results with  come from \citet{zhang2021multi}, and results with  come from \citet{wang2022cat}.  }
\label{tab:generalization ability}
\end{table}


\subsection{Generalization Analysis}
Due to the distinctive underlying logic of PGIM in incorporating auxiliary knowledge to enhance entity recognition, PGIM exhibits a stronger generalization capability that is not heavily reliant on specific datasets. Twitter-20152017 denotes the model is trained on Twitter-2015 and tested on Twitter-2017, vice versa. The results in Table \ref{tab:generalization ability} show that the generalization ability of PGIM is significantly improved compared with previous methods. This further validates the efficacy and superiority of our auxiliary refined knowledge in enhancing model performance.


\subsection{Comparison with MoRe}

As the previous state-of-the-art method, MoRe retrieves relevant knowledge from Wikipedia to assist entity prediction. We experimentally compare the quality of auxiliary knowledge of MoRe and PGIM. The results are shown in Table \ref{tab:PGIM&MORE}. The baseline method solely relies on the original text without any incorporation of auxiliary information. 
MoRe and MoRe denote the relevant knowledge of the input text and image retrieved using text retriever and image retriever, respectively. 
Ave.length represents the average length of the auxiliary knowledge in entire dataset. Memory indicates the GPU memory size required for training model. Ave.Improve represents the average result after summing the improvement of each indicator compared with the baseline method. All models use XLM-RoBERTa \citep{conneau2019unsupervised} as the text backbone with a fixed batch size of 4. 
The experimental results demonstrate that PGIM achieves performance improvement while requiring shorter average auxiliary knowledge length and consuming less memory. This observation highlights the lightweight nature of PGIM and further underscores the superiority of our auxiliary refined knowledge compared with auxiliary knowledge of MoRe sourced from Wikipedia. 

Additionally, we observe that in certain cases, the introduction of auxiliary knowledge by MoRe can even lead to a deterioration in model performance. One possible explanation for this phenomenon is that the information retrieved from Wikipedia often contains redundant or irrelevant content. 
The first case in Figure \ref{fig:PGIM vs MoRe} illustrates this phenomenon well. In this case, PGIM makes the correct prediction because the information retrieved by ChatGPT clearly states that "Mumbai BJP refers to the Bharatiya Janata Party". However, the information retrieved by MoRe from Wikipedia provides almost no assistance in recognition of named entities. 
MoRe alleviates this problem to some extent by introducing the Mixture of Experts (MoE) module in the post-processing stage.
They fixed the parameters of MoRe and MoRe, and trained the MoE module for 50 epochs on the basis of them. But as shown in Table \ref{tab:maintabel} before, compared with MoRe, PGIM still shows better results without any post-processing.

Furthermore, we also show an error prediction of PGIM in Figure \ref{fig:PGIM vs MoRe}. In this case, "Bush" is not a named entity that is hard to predict correctly. But since the additional knowledge retrieved by ChatGPT clearly states that "Bush 41" is a name of person, the prediction of PGIM is not in line with the gold label. This illustrates that the additional knowledge retrieved from ChatGPT can affect the final prediction of named entities to some extent. But the reason why MoRe can make correct prediction is obviously not related to the knowledge it retrieves from the Wikipedia, because "Bush" is not even mentioned in its knowledge. In fact, by using only the original text after masking the noise retrieved from the Wikipedia, the model can more easily predict correctly. 

In summary, considering the relevance and length of retrieved information, using ChatGPT is obviously more suitable for this additional knowledge-based NER method than using Wikipedia. 
The information retrieved from ChatGPT is generally unambiguous and directional, which causes it to significantly help predictions in most cases, and may also mislead predictions in rare cases. But the information retrieved from Wikipedia may mislead the original predictions in many cases.


\subsection{Predictions for mislabeled examples}
\label{misableled}
We observe that the annotation quality of the Twitter-2015 dataset is suboptimal. There have been a large number of errors and omissions in this dataset. This is the reason why the accuracy of Twitter-2015 has significantly decreased compared with Twitter-2017. However, as shown in Figure \ref{fig:error cases},  since the first stage of ChatGPT heuristically generating auxiliary knowledge is not affected by mislabeling, PGIM correctly predicts those unlabeled entities. This also demonstrates the robustness of PGIM. 
As a future direction, we intend to reannotate the dataset to facilitate better development of the MNER task. 

\begin{figure*}
	\centering
	\includegraphics[scale=0.475]{PGIMvsMoRe.pdf}
	\caption{Two case studies on how information retrieved from Wikipedia by MoRe and information retrieved by PGIM from ChatGPT affects model predictions.}
	\label{fig:PGIM vs MoRe}
\end{figure*}

\begin{figure*}
	\centering
	\includegraphics[scale=0.475]{errorsample.pdf}
	\caption{Some mislabeled examples of Twitter-2015 datasets. }
	\label{fig:error cases}
\end{figure*}

\subsection{Prompt template}
\label{templatephoto}
We present the template for prompting ChatGPT to generate answers. 
In Figure \ref{fig:template2}, PGIM guides ChatGPT for auxiliary refined knowledge generation. In-context examples and answers in the template are selected from predefined artificial samples by the MSEA module. 
In Figure \ref{fig:template}, we guide ChatGPT to make direct predictions. 
In-context examples are selected from the same predefined artificial samples by the MSEA module. Note that the answers here are no longer human answers, but named entities in text. 


\begin{figure*}
	\centering
	\includegraphics[scale=0.8]{template2.pdf}
	\caption{A prompt template for ChatGPT to make auxiliary explanation.}
	\label{fig:template2}
\end{figure*}

\begin{figure*}
	\centering
	\includegraphics[scale=0.8]{template.pdf}
	\caption{A prompt template for ChatGPT to direct predict.}
	\label{fig:template}
\end{figure*}














\end{document}
