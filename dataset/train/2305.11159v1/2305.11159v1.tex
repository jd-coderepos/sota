
\begin{figure}[!t]
 \centering
 \small
 \includegraphics[width=\columnwidth]{figures/qa_hook.pdf}
\caption{Main finding: Strong instruction-tuned LLMs underperform prior zero-shot RE methods using the standard (vanilla) RE formulation. Our QA4RE framework enables models in two sets of instruction-tuned LLMs (FLAN-T5 and GPT-3.5) to surpass the prior SoTA on 4 RE datasets by a large margin. Results are averaged over 4 RE datasets. We omit the word `davinci' from the GPT-3.5 model displayed for brevity.}
        \label{fig:hook-figure}
\vspace{-5pt}
\end{figure}

Large language models (LLMs)~\cite{Brown2020GPT3, Chowdhery2022PaLM, Zhang2022OPT} have been shown to achieve impressive performance on many NLP tasks. 
Using the in-context learning paradigm, without any parameter updating, LLMs are able to achieve comparable performance with small language models (LMs) fine-tuned on thousands of examples ~\cite{Liu2022KATE, Min2022Channel, Liang2022HolisticEO}.\footnote{We regard LMs with less than 1B params as small.}
More recently, fine-tuning LLMs on datasets containing thousands of downstream tasks transformed into an instruction following format (i.e., \textit{instruction-tuning}) has been shown to improve LLMs considerably across the board, especially in zero-shot setting~\cite{Iyer2022OPT-IML, Ouyang2022InstructGPT, ChungFlanT5}. 

We examine the capability of LLMs in identifying the relationship between entities in a sentence, i.e., relation extraction (RE), a fundamental task in information extraction.
Recent work~\cite{Gutierrez2022GPT3BioIE} has found that LLMs underperform fine-tuned small LMs for RE in the biomedical domain. 
Our results on general domain RE in Fig.~\ref{fig:hook-figure} reveal that even two of the most advanced instruction-tuned LLMs, FLAN-T5 XXL~\cite{ChungFlanT5} and text-davinci-003 \cite{Ouyang2022InstructGPT}, fail to outperform the state-of-the-art (SoTA) zero-shot RE method based on small LMs~\cite{Sainz2021NLI}.








We hypothesize that the limited relation extraction capability of instruction-tuned LLMs could be a byproduct of the low incidence of RE tasks in instruction-tuning datasets~\cite{Ouyang2022InstructGPT, Sanh2022T0, ChungFlanT5, Wang2022SuperInstructions}.\footnote{RE-like tasks are <0.5\% of the largest available instruction dataset~\cite{Wang2022SuperInstructions}; see Appendix~\ref{sed:appendix-instruction-dataset-portion} for details.}
To address the low incidence issue, we propose the QA4RE framework, which aligns RE with multiple-choice question answering (QA), a task that appears much more frequently in most instruction-tuning datasets---around $12$-$15$\% of all the tasks in both \citet{Wang2022SuperInstructions} and \citet{Ouyang2022InstructGPT}. 
Specifically, by casting the input sentence as a question and possible relation types as multiple-choice options (Fig.~\ref{fig:intro}), LLMs are able to perform RE by selecting the option representing the correct relation type.

Thorough evaluations on four real-world relation extraction datasets and six instruction-tuned models from two different series (OpenAI GPT-3.5 and FLAN-T5~\cite{ChungFlanT5}) show that QA4RE brings significant gains over the standard RE formulation on, validating its effectiveness and our hypothesis concerning the low incidence of RE.
More specifically, our framework enables text-davinci-003 and FLAN-T5-XXLarge to achieve an average of $8.2$\% and $8.6$\% absolute improvements in F1, respectively.
For the first time, an LLM is able to outperform prior small-LM-based SoTA in the zero-shot setting by a large margin.
In-depth analyses further demonstrate the robustness and few-shot effectiveness of QA4RE. 
More importantly, our framework has been proven to be effectively transferable on instruction-tuned models with various sizes, ranging from 80M to 175B.
Our contributions are summarized as follows:

\noindent
\textbf{(1)}
We systematically investigate instruction-tuned LLMs on four real-world relation extraction datasets and note that their limited performance on RE might stem from the low incidence of RE tasks in instruction-tuning datasets.

\noindent
\textbf{(2)}
We reformulate RE as multiple-choice QA in an effort to appropriately leverage QA's much higher prevalence in instruction-tuning datasets and achieve significant improvements on six recent instruction-tuned LLMs, significantly outperforming previous SoTA zero-shot RE methods based on small LM for the first time.

\noindent
\textbf{(3)}
In addition, we demonstrate our QA4RE method's robustness to diverse prompt designs as well as its promising results in the few-shot setting. 


\noindent
\textbf{(4)}
Finally, we show the effectiveness of QA4RE framework is transferable and consistent on various instruction-tuned models with different sizes from 80M to 175B.
Our study illustrates the potential of aligning infrequent and challenging tasks with frequent instruction-tuning tasks and can guide others in exploring this direction.






