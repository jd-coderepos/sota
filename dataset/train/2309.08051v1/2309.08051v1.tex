\documentclass{article}
\usepackage{csquotes}
\usepackage{spconf,amsmath,graphicx,siunitx,microtype,amsfonts}
\usepackage[numbers]{natbib}
\usepackage{array}
\usepackage{booktabs}
\usepackage{microtype}
\usepackage{url}
\usepackage{hyperref}
\usepackage[all]{hypcap}
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{color}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{amsmath,graphicx,url,times,booktabs,tabularx,amsfonts,siunitx}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{latexsym}
\usepackage{bbding}
\usepackage{footnote}
\ninept
\def\x{{\mathbf x}}
\def\L{{\cal L}}
\title{Retrieval-Augmented Text-to-Audio Generation}


\name{
      Yi Yuan,
      Haohe Liu,
      Xubo Liu,
      Qiushi Huang,
      Mark D. Plumbley,
      Wenwu Wang
      }
      
\address{School of Computer Science and Electronic Engineering, University of Surrey, UK\\
 }


\begin{document}

\maketitle

\begin{abstract}
Despite recent progress in text-to-audio (TTA) generation, we show that the state-of-the-art models, such as AudioLDM, trained on datasets with an imbalanced class distribution, such as AudioCaps, are biased in their generation performance. Specifically, they excel in generating common audio classes while underperforming in the rare ones, thus degrading the overall generation performance. We refer to this problem as long-tailed text-to-audio generation. To address this issue, we propose a simple retrieval-augmented approach for TTA models. Specifically, given an input text prompt, we first leverage a Contrastive Language Audio Pretraining (CLAP) model to retrieve relevant text-audio pairs. The features of the retrieved audio-text data are then used as additional conditions to guide the learning of TTA models. We enhance AudioLDM with our proposed approach and denote the resulting augmented system as Re-AudioLDM. On the AudioCaps dataset, Re-AudioLDM achieves a state-of-the-art Frechet Audio Distance (FAD) of $1.37$, outperforming the existing approaches by a large margin. Furthermore, we show that Re-AudioLDM can generate realistic audio for complex scenes, rare audio classes, and even unseen audio types, indicating its potential in TTA tasks.
\end{abstract}


\begin{keywords}
	Audio generation, retrieval-information, diffusion model, deep learning, long tail problem
\end{keywords}



\section{Introduction}
\label{sec:intro}
The landscape of text-to-audio (TTA) generation has been revolutionized by advancements in diffusion-based generative modelling~\cite{audioldm,makeaudio,wavjourney}. Leveraging powerful backbone models such as CLAP~\cite{audioldm} and large language model~(LLM)~\cite{tango}, these models are capable of extracting semantic information and enabling the creation of high-fidelity audio from textual descriptions. 

In this work, we show that due to the scarcity and diversity of audio training data, bias appears in these state-of-the-art models, leading to significant performance degradation. Figure~\ref{frequency} (top) draws a statistical analysis conducted on the $327$ labels of AudioCaps~\cite{audiocaps}, one of the largest audio-text datasets, indicating a notable imbalance in data distribution. The bottom-left graph of Figure~\ref{frequency} shows a sample result of the state-of-the-art model trained with AudioCaps, when giving the prompt ``\textit{A man is talking then pops the champagne and laughs}'', the model could only generate the content for ``\textit{man talking}'', but miss uncommon or complex events such as``\textit{champagne popped}'' then followed by ``\textit{laugh}''. Hence, an inherent limitation is seen due to the constrained scope and variability of the training dataset, where the quality of generated sounds seems largely correlated with their frequency of appearance during training. In this regard, these models can faithfully generate realistic audio clips for common sound events, but they may generate incorrect or irrelevant audio clips when encountering less frequent or unseen sound events. 

We denote this the \textit{long-tailed text-to-audio generation} problem, which influences the model performance in diversity and restricts the applicability of these models, especially in real-world scenarios. 
Our motivation is to develop a robust TTA framework that breaks the barrier of imbalanced data and achieves realistic generation on diverse acoustic entities.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{example_v2.png}
    \vspace{-6mm}
    \caption{The long-tailed problem in AudioCaps dataset (top). Example audio clips (bottom) generated with the baseline model~(left) and Re-AudioLDM~(right). }
    \label{frequency}
    \vspace{-1em}
\end{figure}

We propose a novel retrieval-augmented TTA framework to address the long-tailed generation issue. We enhance the state-of-the-art TTA model, AudioLDM~\cite{audioldm} with a retrieval module, dubbed Re-AudioLDM.
Specifically, we first use the input text prompt to retrieve relevant references (e.g., text-audio pairs) from dataset~(e.g., AudioCaps), and then use a pre-trained audio model and a language model to extract the acoustic and textual features, respectively. These extracted features are then further given to the cross-attention~\cite{transformer} module of the LDM to guide the generation process. The retrieved audio-text pairs serve as supplementary information that helps improve the modelling of low-frequency audio events in the training stage. In the inference stage, the retrieval-augmented strategy also provides references in relation to the text prompt, ensuring a more accurate and faithful audio generation result. 

We perform extensive experiments on events with different frequencies of occurrence in the dataset. We show that Re-AudioLDM provides a stable performance among a variety of audio entities. It significantly improves the performance for tail classes over the baseline models, demonstrating that it can provide effective alleviation for long-tail TTA issues. Furthermore, as compared with the baseline models, Re-AudioLDM is capable of generating more realistic and complex audio clips, including rare, complex, or even unseen audio events. As the example with the same prompt shown in Figure~\ref{frequency} (bottom), where Re-AudioLDM~(bottom-right) can generate both uncommon entities ``\textit{champagne poppe}'' with a complex structure followed with the sound of ``\textit{laugh}'', achieving a better result than the baseline models with all the required entities and semantic orders correctly. In addition, Re-AudioLDM achieves a FAD score of $1.37$, outperforming state-of-the-art TTA models by a large margin. 

  

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{overview_v7.pdf}
    \caption{The overview structure of Re-AudioLDM}
    \label{fig:overview}
\end{figure*}

The remainder of this paper is organized as follows. Section~\ref{sec:related_works} introduces the related works of audio generation and retrieval-based models, followed by the details of Re-AudioLDM in Section~\ref{sec:method}. Section \ref{sec:exp} presents the experimental setting and Section \ref{sec:result} shows the results and ablation studies. Conclusions are given in Section \ref{sec:conclusion}.

\section{Related work}
\label{sec:related_works}
Our work relates to two major works, diffusion-based text-to-audio models and retrieval-based generative models. These two fields are briefly discussed in the following subsections.

\subsection{Audio Generation}  
Recent works on audio generation follow an encoder-decoder framework~\cite{audioldm,leveraging}. The model first uses an encoder to encode the information into a latent representation, which can be decompressed into a mel-spectrogram feature. The decoder then uses a variational autoencoder~(VAE) and a generative adversarial network~(GAN) vocoder to turn such features into waveforms. Liu \textit{et al.}~\cite{Liu-tts} has used PixelSNAIL~\cite{pixelsnail} as the encoder to represent labels while Iashin and Rahtu ~\cite{specvqgan} applied a GPT2~\cite{gpt2} as the encoder to encode input images. Subsequently, diffusion-based models have been used for latent token generation. Yang \textit{et al.}~\cite{diffsound} replaces the transformer-based encoder with a diffusion-based encoder. Liu \textit{et al.}~\cite{audioldm} uses the CLAP model~\cite{clip} to obtain embeddings for the input data (audio or text), and uses the Latent Diffusion Model~(LDM) as the token generator. Ghosal \textit{et al.}~\cite{tango} then further improves this framework by replacing CLAP with LLM~\cite{t5}. 

\subsection{Retrieved Information Aggregation}
Several studies in the field of image generation have considered leveraging retrieved information. Li \textit{et al.}~\cite{Li2022} extract image features from a training set, and place them in a memory bank which is then used as a parallel input condition for audio generation. Blattmannet \textit{et al.}~\cite{blattmann2022} present a nearest-neighbours strategy to select related image samples from a neighbourhood area. The KNN-Diffusion~\cite{sheynin2022} uses image features obtained from large-scale retrieval databases during the inference stage to perform new-domain image generation. Chen \textit{et al.}~\cite{reimagen} extend the image-only retrieval into image-text pair retrieval, augmenting both high-level semantics and low-level visual information for a diffusion model. In contrast, no similar works have been done for audio generation, and Re-AudioLDM is the first attempt to introduce retrieved information from a dataset to improve the text-to-audio generation performance. 

\section{Proposed Method}
\label{sec:method}

Similar to previous audio generation works~\cite{audioldm,tango,tuning}, Re-AudioLDM is a cascaded model including three parts: input embedding, diffusion-based feature generator, and a pipeline to reconstruct the waveform from the latent feature. 
\subsection{Text and Retrieval Embedding Encoder}
\label{sec:encoder}
Re-AudioLDM takes two paralleled inputs: a text input $c_t$ as low-level semantic information, and a set of text-audio pairs as retrieval augmentation $c_r$ for high-level semantic-audio information. The text embedding $\boldsymbol{E}^{t}$ is obtained as: 
\begin{equation}
\label{eqa:1}
    \boldsymbol{E}^{t} = \textit{f}_{\text{clap}}(c_t)
\end{equation}
which $\textit{f}_{\text{clap}}(\cdot)$ is the CLAP model \cite{clap} used for text encoding, as in AudioLDM~\cite{audioldm}. The retrieved information $c_r =[<\text{text,audio}>_1,<\text{text,audio}>_2,...,<\text{text,audio}>_k] $ are the top-K neighbours selected through the similarity comparison between the embedding of the target caption and those of the retrieval dataset. Here for each pair, the multi-modal embedding is divided into two groups of concatenation, presented as audio retrieval $\boldsymbol{E}^{ra}$ and text retrieval $\boldsymbol{E}^{rt}$, encoded as:
\begin{align}
\label{eqa:2}
\boldsymbol{E}^{ra} = \text{CAT}[\textit{f}_{\text{mae}}(\text{audio}_1), ... , \textit{f}_{\text{mae}}(\text{audio}_k)],\\
\label{eqa:3}
\boldsymbol{E}^{rt} = \text{CAT}[\textit{f}_{\text{t5}}(\text{text}_1), ... ,\textit{f}_{\text{t5}}(\text{text}_k)]
\end{align}

\noindent
where $\textit{f}_{\text{t5}}(\cdot)$ is a pre-trained T5 model~\cite{t5} for obtaining the text embedding, and $\textit{f}_{\text{mae}}(\cdot)$ is a pre-trained AudioMAE model~\cite{mae} for obtaining the embedding of the paired audio. 
\subsection{Retrieval-Augmented Diffusion Generator}
Re-AudioLDM uses LDM as the generator to obtain the intermediate latent token of the target audio. The diffusion model involves two processes, a forward process to gradually add noise into the latent vectors and a reverse process to progressively predict the transition noise of the latent vector in each step. During the forward step, the latent representation $z_0$ is transformed into a standard Gaussian distribution $z_n$ with a continuous noise injection:
\begin{align}
q(\boldsymbol{z}_{n}|\boldsymbol{z}_{n-1})&=\mathcal{N}(\boldsymbol{z}_{n};\sqrt{1-\beta_{n}}\boldsymbol{z}_{n-1},\beta_{n}\boldsymbol{I}), \\
\label{forwardprocess}
q(\boldsymbol{z}_{n}|\boldsymbol{z}_{0})&=\mathcal  N(\boldsymbol{z}_{n};\sqrt{\bar{\alpha}_{n}}\boldsymbol{z}_{0},(1-\bar{\alpha}_{n})\boldsymbol{\epsilon})
\end{align} 
\noindent
where $\epsilon$ denotes the Gaussian noise with ${\alpha}_{n} = 1 -\beta_{n}$ controlling the noise level. 
In the reverse process, LDM learns to estimate the distribution of noise $\boldsymbol{\epsilon}_{\theta}$ in the latent space, given conditions from the text embedding $\boldsymbol{E}^{t}$, calculated with equation~(\ref{eqa:1}), and the retrieved embedding $\boldsymbol{E}^{ra}$ and $\boldsymbol{E}^{rt}$, calculated with equation~(\ref{eqa:2}) and ~(\ref{eqa:3}), respectively.
The LDM model applies UNet as the general structure, where the input layer takes the noisy latent vector $\boldsymbol{z}_{n}$, text embedding $\boldsymbol{E}^{t}$, and the time step $\textit{n}$ as the condition. Then the retrieved information of both text and audio is shared with all the cross-attention blocks within the remaining layers. Employing a re-weighted training objective ~\cite{DDPM}, LDM is trained by:
\begin{equation}
\label{trainingobjective}
L_{n}(\theta)=\mathbb{E}_{\boldsymbol{z}_{0},\boldsymbol{\epsilon},n}\left \| \boldsymbol{\epsilon} - \boldsymbol{\epsilon}_{\theta}(\boldsymbol{z}_{n},n,\boldsymbol{E}^{t},\text{Attn}(\boldsymbol{E}^{ra},\boldsymbol{E}^{rt})) \right\|^2_{2}
\end{equation}


\subsection{VAE Decoder \& Hifi-GAN Vocoder}
\label{sec:pipline}
Re-AudioLDM utilizes a combination of a VAE and a HiFi-GAN as the general pipeline for reconstructing waveform from the latent feature tokens. During the training stage, VAE learns to encode the mel-spectrogram into the intermediate representation and then decode it back to mel-spectrogram, while Hifi-GAN is trained to convert mel-spectrogram into waveform. For inference, Re-AudioLDM applies the VAE decoder for mel-spectrogram reconstruction and HiFi-GAN for waveform generation. 

\label{sec:method}

\begin{table*}[htbp]
\label{tab:result}
\centering
\small
\begin{tabular}{ccccccccc}
\toprule
 Model  & Dataset & Retrieval Info & Retrieval Number  & KL $\downarrow$ &  IS $\uparrow$& FAD $\downarrow$ & $\operatorname{CLAP}_{\operatorname{score}}$(\%)$\uparrow$ \\
\midrule
AudioGen~\cite{audiogen}
&AC+AS+8 others&\XSolidBrush&\XSolidBrush &    $1.69$    &  $5.13$   &  $2.15$   & $23.44$ \\
\midrule
AudioLDM~\cite{audioldm}
&AC+AS+2 others&\XSolidBrush&\XSolidBrush &    $1.66$    &  $6.51$   &  $2.08$   & $25.39$ \\
\midrule
Tango~\cite{tango}
&AudioCaps&\XSolidBrush&\XSolidBrush &   $1.32$     &  $6.45$   &  $1.68$  & $29.28$ \\
\midrule
\multirow{4}{*}{Re-AudioLDM-S}
&AudioCaps &\XSolidBrush&  \XSolidBrush   &  $1.63$   &  $6.48$   &   $2.31$    & $26.75$ \\
                       &AudioCaps &Audio&   $3$     &  $1.54$    &  $6.88$   &  $1.95$     & $31.05$ \\
                       &AudioCaps & Audio \& Text &  $3$     &  $1.27$   &  $7.31$   &   $1.48$    & $37.07$ \\
                       &AudioCaps & Audio \& Text&   $10$     &  $1.23$   &  $7.33$   &   $1.40$    & $\mathbf{37.15}$ \\
\midrule
Re-AudioLDM-L
&AudioCaps&Audio \& Text& $10$ &   $\mathbf{1.20}$     &  $\mathbf{7.39}$   &  $\mathbf{1.37}$  & $37.12$ \\
\bottomrule
\end{tabular}
\label{tab:pre-train}
\caption{The comparison between different frameworks, with and without retrieval information. AC and AS are short for AudioCaps~\cite{audiocaps} and AudioSet~\cite{audioset} respectively.}
\end{table*}


\section{Experiments}
\label{sec:exp}

\subsection{Datasets}
We use the AudioCaps dataset~\cite{tau2019} for the experiments, which comprises $46,000$ ten-second audio clips, each paired with a human-annotated caption. We follow the official training-testing split, where each training audio clip is assigned a single caption, while in the testing split, each audio clip is annotated with five captions. During the inference stage, we employ the first caption of each audio clip that appears in the test split as the text input. The remaining four captions are used only for the ablation study in Section~\ref{sec:result}.

\subsection{Experiment Setup}

\textbf{Data Preparation.}
For a retrieval-based AudioCaps dataset, we apply a CLAP-score based retrieval function to find the top-50 nearest neighbours of the target text embedding. The waveform and the text from each neighbour are stored as a text-audio pair. It is noted that for the training samples, the retrieval information is only collected from the training set, while Re-AudioLDM can access the full AudioCaps for evaluation, which can avoid any access to testing data during the training stage.

\noindent
\textbf{Implementation Detail.}
As a cascaded model, the encoder and decoder parts of Re-AudioLDM are trained separately with audio clips sampled at $16$ kHz. For the target, we use the short-time Fourier transform~(STFT) with a window of 1024 samples and a hop size of 160 samples, resulting in a mel-spectrogram with 64 mel-filterbanks. Then, a VAE model is applied to decompress the spectrogram with a ratio of $4$, resulting in a feature vector with a frequency dimension of 16. For the information provided by the retrieval strategy, the text feature is directly extracted by a pre-trained T5-medium model, presenting a fixed sequence length of 50. The audio feature, on the other hand, is first converted into filter banks with 128 mel-bins and then processed by a pre-trained AudioMAE model, leading to a vector of dimension 32. 

\noindent
\textbf{Training Detail.}
The LDM is optimized with a learning rate of $5.0\times10^{-5}$. Re-AudioLDM is trained for up to $80$ epochs with a batch size of $4$ and the evaluation is carried out every $100,000$ steps. Re-AudioLDM-S applies a UNet architecture consisting of $128$ channels, while we enlarge the model into Re-AudioLDM-L with $196$ channels for experiments on more complex models. 

\noindent
\textbf{Evaluation Metrics.}
Following Liu \textit{et al.}, we use the Inception Score~(IS), Fréchet Audio Distance~(FAD), and Kullback–Leibler~(KL) divergence to evaluate the performance of Re-AudioLDM. A higher IS score indicates a larger variety in the generated audio, while lower KL and FAD scores indicate better audio fidelity. For the semantic-level evaluation, we calculate the cosine similarity between the output audio embedding and the target text embedding calculated by the CLAP encoders, which demonstrates the correlation between audio and text.

\section{Results}
\label{sec:result}

\subsection{Evaluation Results}
The experiments are carried out on AudioCaps evaluation set. We compare the performance with several state-of-the-art frameworks, including AudioGen~\cite{audiogen}, AudioLDM~\cite{audioldm} and Tango~\cite{tango}. Selecting only the first caption of each audio clip as the text description, each framework infers $975$ 10-second audio clips with a sampling rate of $16$ kHz. Table~\ref{tab:result} compares the metrics achieved with different text-to-audio models, where Re-AudioLDM outperforms other methods by a large margin. It is noted that without the additional information provided by retrieval, Re-AudioLDM does not exhibit any advantage and is generally inferior to Tango, the current state-of-the-art model on AudioCaps. However, upon incorporating retrieval information, Re-AudioLDM successfully outperformed the baseline models in all four evaluation metrics. By enlarging the size of the hidden layer in LDM structure, the Re-AudioLDM-L using 10 retrieved pairs further decreases the FAD score to below $1.4$, which is a significant improvement over the baseline frameworks. 

\subsection{Ablation Study}
\label{ablation}
\textbf{Retrieval Type.}
Experiments in Table~\ref{tab:result} show the results on different retrieval information, e.g. audio, text, or neither. With the audio features extracted by AudioMAE, only a slight improvement is achieved by Re-AudioLDM, mainly because Re-AudioLDM misses the relationship between sound events, although it captures the features of related sound events. By adding the paired text information of each retrieved audio clip, Re-AudioLDM learns the relationship between audio features and high-level semantic information, contributing a significant improvement in capturing highly related semantic features for the sound events. 


\noindent
\textbf{Number of Retrieved Pairs.}
Several experiments are carried out to assess the impact of the number of retrieved audio-text pairs on audio generation performance. As depicted in Figure~\ref{fig:retriva-number}, the incorporation of retrieval information improves the performance, as the number of retrieved pairs increases, while such improvement slows down after the number reaches five and it becomes flattened at around ten. Therefore, in order to strike a balance between training costs and model performance, the number of retrieved pairs is chosen empirically to be in the range of $3$ to $5$ for this data.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{retrival_num.png}
    \caption{Performance comparison on numbers of retrieved information, where AC\_cap 1-5 refers to the caption groups of the testing set. }
    \label{fig:retriva-number}
\end{figure}

\noindent
\textbf{Long-Tailed Situations.}
Re-AudioLDM aims to tackle the long-tailed generation problem and generate more realistic audio clips on uncommon or unseen sound events. In order to evaluate the accuracy of each generated audio clip, we applied the CLAP score~\cite{clap} to show the relationship between the audio clip and text description. We first calculate the frequency of the occurrence of each sound event by counting the label of each audio clip and then illustrate the model performance by averaging the CLAP score of each sound class for the AudioCaps testing set. The bar chart on the left side of Figure~\ref{fig:retrival-freq} presents a statistical analysis of the quantities of all $327$ sound event classes in the AudioCaps training set. Similar to Figure~\ref{frequency} (top), tail classes constitute a significant portion, especially in the label group of 1 and 10. Figure~\ref{fig:retrival-freq} (right) shows the performance of each model on the events with different frequencies of event occurrence within the training set. Despite the initial gap in highly frequent audio events between Re-AudioLDM and baseline models, the baseline models perform worse when handling tailed entities. However, Re-AudioLDM has demonstrated more stable results, with a decrease of less than $3$ in the CLAP score as the frequency of event occurrence is reduced in training data. Hence, Re-AudioLDM can reduce the degradation of output quality when generating tailed sound events, enhancing the overall model performance. 

\noindent
\textbf{Zero-Shot Generation.}
For experiments on unseen entities, we evaluate several scenarios with events that are excluded during training. In Figure~\ref{fig:retrival-freq} (right), we found that baseline models show performance degradation on generating unseen audio~(zero frequency occurrence). This may be because the model has not learned the features of unseen entities, while Re-AudioLDM can still achieve realistic results by providing related audio and semantic information. Hence, with essential retrieval information, Re-AudioLDM has the potential to generate sounds which are excluded from training data. The retrieval-based generation may significantly enhance the robustness of zero-shot tasks, which is one of the directions we will explore in the future. 

\noindent
\textbf{Comparison with Mixup Strategy.}
Another way to address the class imbalance problem is to use a mixup strategy \cite{kong2020panns}. While mixup can increase the occurrence frequency for the tail entities, it also introduces more complex audio examples, as well as the synthetic audio data that may not align with real-world distributions. The results in~\cite{audioldm} have shown that the mixup strategy leads to degradation in overall performance. In contrast to the mixup method, our proposed retrieval augmentation strategy reduces the complexity of the training processes, resulting in an overall performance improvement.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{retrival_freq_v3.png}
    \caption{Performance on different frequency entities, where S and L indicate model size and r refers to the number of retrieved clips. }
    \label{fig:retrival-freq}


\end{figure}


\section{Conclusion}
\label{sec:conclusion}
In this paper, we have presented a retrieval-augmented model, Re-AudioLDM, to tackle the long-tailed problem in AudioCaps. The comparisons with current state-of-the-art models (i.e., AudioLDM and Tango) using several performance metrics (i.e., FAD, and CLAP-score) demonstrate that Re-AudioLDM can significantly enhance the performance of TTA models in generating high-fidelity audio clips. By integrating retrieved features, Re-AudioLDM not only achieves improvements in overall performance, but enables the generation of rare or unseen sound entities. In future work, we will investigate the model with external large datasets and explore the potential of the model in downstream tasks, such as zero-shot generation.  


\section{ACKNOWLEDGMENT}
\label{sec:ack}
This research was partly supported by a research scholarship from the China Scholarship Council~(CSC), funded by British Broadcasting Corporation Research and Development~(BBC R\&D), Engineering and Physical Sciences Research Council~(EPSRC) Grant EP/T019751/1 ``AI for Sound'', and a PhD scholarship from the Centre for Vision, Speech and Signal Processing~(CVSSP), University of Surrey. 
For the purpose of open access, the authors have applied a Creative Commons Attribution~(CC BY) license to any Author Accepted Manuscript version arising.
\bibliographystyle{IEEEtran}

\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}\typeout{** loaded for the language `#1'. Using the pattern for}\typeout{** the default language instead.}\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{audioldm}
H.~{Liu}, Z.~{Chen}, Y.~{Yuan}, X.~{Mei}, X.~{Liu}, D.~{Mandic}, W.~{Wang}, and M.~D. {Plumbley}, ``{AudioLDM: Text-to-Audio generation with latent diffusion models},'' in \emph{International Conference on Machine Learning}, 2023.

\bibitem{makeaudio}
R.~Huang, J.~Huang, D.~Yang, Y.~Ren, L.~Liu, M.~Li, Z.~Ye, J.~Liu, X.~Yin, and Z.~Zhao, ``Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models,'' in \emph{International Conference on Machine Learning}, 2023.

\bibitem{wavjourney}
X.~Liu, Z.~Zhu, H.~Liu, Y.~Yuan, M.~Cui, Q.~Huang, J.~Liang, Y.~Cao, Q.~Kong, M.~D. Plumbley, and W.~Wang, ``Wavjourney: Compositional audio creation with large language models,'' \emph{arXiv preprint arXiv:2307.14335}, 2023.

\bibitem{tango}
D.~Ghosal, N.~Majumder, A.~Mehrish, and S.~Poria, ``Text-to-audio generation using instruction tuned {LLM} and latent diffusion model,'' \emph{arXiv preprint arXiv:2304.13731}, 2023.

\bibitem{audiocaps}
C.~D. Kim, B.~Kim, H.~Lee, and G.~Kim, ``{AudioCaps}: Generating captions for audios in the wild,'' in \emph{Annual Conference of the North American Chapter of the Association for Computational Linguistics}, 2019.

\bibitem{transformer}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez, {\L}.~Kaiser, and I.~Polosukhin, ``Attention is all you need,'' \emph{Advances in Neural Information Processing Systems}, vol.~30, 2017.

\bibitem{leveraging}
Y.~Yuan, H.~Liu, J.~Liang, X.~Liu, M.~D. Plumbley, and W.~Wang, ``Leveraging pre-trained audioldm for sound generation: A benchmark study,'' in \emph{European Association for Signal Processing}, 2023.

\bibitem{Liu-tts}
X.~Liu, T.~Iqbal, J.~Zhao, Q.~Huang, M.~Plumbley, and W.~Wang, ``Conditional sound generation using neural discrete time-frequency representation learning,'' \emph{IEEE International Workshop on Machine Learning for Signal Processing}, 2021.

\bibitem{pixelsnail}
X.~Chen, N.~Mishra, M.~Rohaninejad, and P.~Abbeel, ``{PixelSNAIL}: An improved autoregressive generative model,'' in \emph{International Conference on Machine Learning}, 2018, pp. 864--872.

\bibitem{specvqgan}
V.~Iashin and E.~Rahtu, ``Taming visually guided sound generation,'' in \emph{British Machine Vision Conference}, 2021.

\bibitem{gpt2}
A.~Radford, J.~Wu, R.~Child, D.~Luan, D.~Amodei, and I.~Sutskever, ``Language models are unsupervised multitask learners,'' \emph{OpenAI blog}, vol.~1, no.~8, p.~9, 2019.

\bibitem{diffsound}
D.~Yang, J.~Yu, H.~Wang, W.~Wang, C.~Weng, Y.~Zou, and D.~Yu, ``Diffsound: Discrete diffusion model for text-to-sound generation,'' \emph{IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 2023.

\bibitem{clip}
A.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, G.~Goh, S.~Agarwal, G.~Sastry, A.~Askell, P.~Mishkin, J.~Clark \emph{et~al.}, ``Learning transferable visual models from natural language supervision,'' in \emph{International Conference on Machine Learning}, 2021, pp. 8748--8763.

\bibitem{t5}
C.~Raffel, N.~Shazeer, A.~Roberts, K.~Lee, S.~Narang, M.~Matena, Y.~Zhou, W.~Li, and P.~J. Liu, ``Exploring the limits of transfer learning with a unified text-to-text transformer,'' \emph{Journal of Machine Learning Research}, vol.~21, no.~1, pp. 5485--5551, 2020.

\bibitem{Li2022}
B.~Li, P.~H. Torr, and T.~Lukasiewicz, ``Memory-driven text-to-image generation,'' in \emph{British Machine Vision Conference}, 2022.

\bibitem{blattmann2022}
A.~Blattmann, R.~Rombach, K.~Oktay, J.~M{\"u}ller, and B.~Ommer, ``Retrieval-augmented diffusion models,'' \emph{Advances in Neural Information Processing Systems}, vol.~35, pp. 15\,309--15\,324, 2022.

\bibitem{sheynin2022}
S.~Sheynin, O.~Ashual, A.~Polyak, U.~Singer, O.~Gafni, E.~Nachmani, and Y.~Taigman, ``{KNN}-diffusion: Image generation via large-scale retrieval,'' in \emph{International Conference on Learning Representations}, 2023.

\bibitem{reimagen}
W.~Chen, H.~Hu, C.~Saharia, and W.~W. Cohen, ``{Re-Imagen}: Retrieval-augmented text-to-image generator,'' in \emph{International Conference on Learning Representations}, 2023.

\bibitem{tuning}
Y.~Yuan, H.~Liu, X.~Kang, P.~Wu, M.~D. Plumbley, and W.~Wang, ``Text-driven foley sound generation with latent diffusion model,'' in \emph{Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop}, 2023, pp. 231--235.

\bibitem{clap}
Y.~Wu, K.~Chen, T.~Zhang, Y.~Hui, T.~Berg-Kirkpatrick, and S.~Dubnov, ``Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation,'' in \emph{IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP}, 2023.

\bibitem{mae}
H.~Xu, J.~Li, A.~Baevski, M.~Auli, W.~Galuba, F.~Metze, C.~Feichtenhofer \emph{et~al.}, ``Masked autoencoders that listen,'' \emph{arXiv preprint:2207.06405}, 2022.

\bibitem{DDPM}
J.~Ho, A.~Jain, and P.~Abbeel, ``Denoising diffusion probabilistic models,'' in \emph{{Neural Information Processing Systems}}, 2020.

\bibitem{audiogen}
F.~{Kreuk}, G.~{Synnaeve}, A.~{Polyak}, U.~{Singer}, A.~{D{\'e}fossez}, J.~{Copet}, D.~{Parikh}, Y.~{Taigman}, and Y.~{Adi}, ``{AudioGen: textually guided audio generation},'' in \emph{International Conference on Learning Representations}, 2023.

\bibitem{audioset}
J.~F. Gemmeke, D.~P.~W. Ellis, D.~Freedman, A.~Jansen, W.~Lawrence, R.~C. Moore, M.~Plakal, and M.~Ritter, ``{AudioSet}: An ontology and human-labeled dataset for audio events,'' in \emph{IEEE International Conference on Acoustics, Speech and Signal Processing}, 2017, pp. 776--780.

\bibitem{tau2019}
\BIBentryALTinterwordspacing
T.~Heittola, A.~Mesaros, and T.~Virtanen, ``{TAU Urban Acoustic Scenes 2019, Development dataset},'' Mar. 2019. [Online]. Available: \url{https://doi.org/10.5281/zenodo.2589280}
\BIBentrySTDinterwordspacing

\bibitem{kong2020panns}
Q.~Kong, Y.~Cao, T.~Iqbal, Y.~Wang, W.~Wang, and M.~D. Plumbley, ``{PANNs}: Large-scale pretrained audio neural networks for audio pattern recognition,'' \emph{IEEE/ACM Transactions on Audio, Speech, and Language Processing}, vol.~28, pp. 2880--2894, 2020.

\end{thebibliography}



\end{document}