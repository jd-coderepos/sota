\documentclass{article}
\usepackage{fullpage}


\newcommand{\onlyarxiv}[1]{{#1}}

\usepackage{wrapfig}

\makeatletter
\newcommand\gobblepars{\@ifnextchar\par {\expandafter\gobblepars\@gobble}{}}
\makeatother
\renewcommand{\paragraph}[1]{\smallskip\noindent\textbf{#1}.\ \ \gobblepars}


\newcommand{\tightlinespace}{-1.3ex}

\newtheorem{definition}{Definition}\newtheorem{theorem}{Theorem}\newtheorem{lemma}{Lemma}\newtheorem{corollary}{Corollary}\newtheorem{proposition}{Proposition}\newtheorem{conjecture}{Conjecture}

\newtheorem{preexperiment}{Experiment}
\newenvironment{experiment}{\begin{preexperiment}\rm}{\hfill\qed\end{preexperiment}}



\usepackage{balance}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{ellipsis} \usepackage[hyphens]{url} \usepackage{verbatim} \usepackage{epsfig,color} \usepackage{stmaryrd} \usepackage{multirow} 








\newcommand{\drop}[1]{{\color{red}[drop: {#1}]}}

\newcommand{\ftnote}{\footnote}

\newcommand{\pt}{\mathsf{pt}}
\newcommand{\cnt}{\mathsf{count}}
\newcommand{\simm}{\mathsf{sim}}
\newcommand{\avg}{\mathsf{avg}}

\newcommand{\C}{\mathcal{C}}

\newcommand{\dist}[1]{\Delta({#1})}
\newcommand{\degen}{\delta}
\newcommand{\cross}{\times}
\newcommand{\pa}{\mathsf{par}}
\newcommand{\doo}{\mathsf{do}}
\newcommand{\msf}{\mathsf}

\newcommand*{\TitleParbox}[1]{\parbox[c]{9cm}{\raggedright #1}}\newcommand*{\TitleParboxSmall}[1]{\parbox[c]{8.3cm}{\raggedright #1}}



\newcommand*{\cons}{{\cdot}}
\newcommand*{\emptyseq}{[\,]}

\newcommand*{\dom}{\mathsf{level}}
\newcommand*{\flows}{\leadsto}
\newcommand*{\filter}[2]{\lfloor{#1}{\downarrow}{#2}\rfloor}
\newcommand*{\remove}[2]{\lceil{#1}{\uparrow}{#2}\rceil}

\newcommand{\nil}{\varnothing}

\newcommand{\posres}{\text{\tiny\texttt{+}}}
\newcommand{\negres}{\text{\tiny\texttt{-}}}

\newcommand{\notni}{\not\ni}

\newcommand*{\napprox}{\not\approx}
\newcommand*{\set}[2]{\{\,{#1}~|~{#2}\,\}}

\newcommand*{\given}{\mathop{{|}}}

\newcommand{\lnmap}{\ln^{\!*}}
\DeclareMathOperator{\coss}{coss}

\DeclareMathOperator{\st}{\,s.t.\,}
\newcommand{\mc}{\mathcal}
\newcommand{\bigtimes}{\times}

\newcommand*{\itmnum}[1]{\mbox{}\!\!\!\!{\small\S}\,{#1}\!\!\!\!\!}
\newcommand*{\egap}{1ex}

\usepackage{booktabs} \newenvironment{tab}[1]
{\let\oldarraystretch=\arraystretch
\renewcommand{\arraystretch}{1.2} \mbox{}\hfill
\begin{tabular}{@{}#1@{}}
\toprule
}
{\bottomrule
\end{tabular}
\mbox{}\hfill
\renewcommand{\arraystretch}{\oldarraystretch}
}

\newcommand{\midruleheaderbottom}{\hline}

\newenvironment{tablewide}{\begin{table}\footnotesize}{\end{table}}

\usepackage{array}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}




\newcommand{\email}{\url}

\author{Amit Datta\\
Carnegie Mellon University\\
\email{amitdatta@cmu.edu}
\and
Michael Carl Tschantz\\
International Computer Science Institute\\
\email{mct@icsi.berkeley.edu}
\and
Anupam Datta\\
Carnegie Mellon University\\
\email{danupam@cmu.edu}
}

\title{Automated Experiments on Ad Privacy Settings\\ A Tale of Opacity, Choice, and Discrimination\thanks{This report is an extended version of an article appearing in the Proceedings on Privacy Enhancing Technologies~\cite{datta15pets}.  Both of those works build upon an earlier version of this report~\cite{datta14arxiv}.}}





\begin{document}

\maketitle

\begin{abstract}
To partly address people's concerns over web tracking, Google has created the Ad Settings webpage to provide information about and some choice over the profiles Google creates on users.  We present AdFisher, an automated tool that explores how user behaviors, Google's ads, and Ad Settings interact. AdFisher can run browser-based experiments and analyze data using machine learning and significance tests. 
Our tool uses a rigorous experimental design and statistical analysis to ensure the statistical soundness of our results.  
We use AdFisher to find that the Ad Settings was opaque about some features of a user's profile, that it does provide some choice on ads, and that these choices can lead to seemingly discriminatory ads.  In particular, we found that visiting webpages associated with substance abuse changed the ads shown but not the settings page.  We also found that setting the gender to female resulted in getting fewer instances of an ad related to high paying jobs than setting it to male.  We cannot determine who caused these findings due to our limited visibility into the ad ecosystem, which includes Google, advertisers, websites, and users. Nevertheless, these results can form the starting point for deeper investigations by either the companies themselves or by regulatory bodies.
\end{abstract}






















\section{Introduction}

\paragraph{Problem and Overview}
With the advancement of tracking technologies and the growth of online data aggregators, data collection on the Internet has become a serious privacy concern. Colossal amounts of collected data are used, sold, and resold for serving targeted content, notably advertisements, on websites
(e.g.,~\cite{mayer12sp}).  Many websites providing content, such as news, outsource their advertising operations to large third-party ad networks, such as Google's DoubleClick.  These networks embed tracking code into webpages across many sites providing the network with a more global view of each user's behaviors. 

People are concerned about behavioral marketing on the web~(e.g.,~\cite{ur12soups}). To increase transparency and control, Google provides Ad Settings, which is ``a Google tool that helps you control the ads you see on Google services and on websites that partner with Google''~\cite{google-ad-settings-help}. It displays inferences Google has made about a user's demographics and interests based on his browsing behavior.
Users can view and edit these settings at\\
\centerline{\url{http://www.google.com/settings/ads}} Figure~\ref{fig:settings} provides a screenshot.
Yahoo~\cite{yahoo-help}
and Microsoft~\cite{microsoft-choice} also offer personalized ad settings.

\begin{wrapfigure}{r}{10.1cm}
\centering
\includegraphics[width=10cm]{settings.eps}
\caption{Screenshot of Google's Ad Settings webpage}
\label{fig:settings}
\end{wrapfigure}

However, they provide little information about how these pages operate, leaving open the question of
how completely these settings describe the profile they have about a user. In this study, we explore how a user's behaviors, either directly with the settings or with content providers, alter the ads and settings shown to the user and whether these changes are in harmony. 
In particular, we study the degree to which the settings provides transparency and choice as well as checking for the presence of discrimination.
Transparency is important for people to understand how the use of data about them affects the ads they see.
Choice allows users to control how this data gets used, enabling them to protect the information they find sensitive.
Discrimination is an increasing concern about machine learning systems and one reason people like to keep information private~\cite{bigdata14whitehouse, zemel13icml}.







To conduct these studies, we developed AdFisher, a tool for automating randomized, controlled experiments for studying online tracking.
Our tool offers a combination of automation, statistical rigor, scalability, and explanation for determining the use of information by web advertising algorithms and by personalized ad settings, such as Google Ad Settings.
The tool can simulate having a particular interest or attribute by visiting webpages associated with that interest or by altering the ad settings provided by Google.  It collects ads served by Google and also the settings that Google provides to the simulated users. It automatically analyzes the data to determine whether statistically significant differences between groups of agents exist.  AdFisher uses machine learning to automatically detect differences and then executes a test of significance specialized for the difference it found.




Someone using AdFisher to study behavioral targeting only has to provide the behaviors the two groups are to perform (e.g., visiting websites) and the measurements (e.g., which ads) to collect afterwards. 
AdFisher can easily run multiple experiments exploring the causal connections between users' browsing activities, and the ads and settings that Google shows.

The advertising ecosystem is a vast, distributed, and decentralized system with several players including the users consuming content, the advertisers, the publishers of web content, and ad networks.  With the exception of the user, we treat the entire ecosystem as a blackbox.  We measure simulated users' interactions with this blackbox including page views, ads, and ad settings.  
Without knowledge of the internal workings of the ecosystem, we cannot assign responsibility for our findings to any single player within it nor rule out that they are unintended consequences of interactions between players.
However, our results show the presence of concerning effects illustrating the existence of issues that could be investigated more deeply by either the players themselves or by regulatory bodies with the power to see the internal dynamics of the ecosystem.

\paragraph{Motivating Experiments}
In one experiment, we explored whether visiting websites related to substance abuse has an impact on Google's ads or settings.  We created an experimental group and a control group of agents.  The browser agents in the experimental group visited  websites on substance abuse while the agents in the control group simply waited.  Then, both groups of agents collected ads served by Google on a news website. 

Having run the experiment and collected the data, we had to determine whether any difference existed in the outputs shown to the agents.  One way would be to intuit what the difference could be (e.g. more ads containing the word ``alcohol'') and test for that difference.  However, developing this intuition can take considerable effort. Moreover, it does not help find unexpected differences. Thus, we instead used
machine learning to automatically find differentiating patterns in the data. Specifically, AdFisher finds a classifier that can predict which group an agent belonged to, from the ads shown to an agent. The classifier is trained on a subset of the data. A separate test subset is used to determine whether the classifier found a statistically significant difference between the ads shown to each group of agents.
In this experiment, AdFisher found a classifier that could distinguish between the two groups of agents by using the fact that only the agents that visited the substance abuse websites received ads for Watershed Rehab.

We also measured the settings that Google provided to each agent on its Ad Settings page after the experimental group of agents visited the webpages associated with substance abuse.  We found no differences (significant or otherwise) between the pages for the agents.  Thus, information about visits to these websites is indeed being used to serve ads, but the Ad Settings page does not reflect this use in this case. Rather than providing transparency, in this instance, the ad settings were \emph{opaque} as to the impact of this factor.



In another experiment, we examined whether the settings provide \emph{choice} to  users.  We found that removing interests from the Google Ad Settings page changes the ads that a user sees.  In particular, we had both groups of agents visit a site related to online dating.  Then, only one of the groups removed the interest related to online dating.  Thereafter, the top ads shown to the group that kept the interest were related to dating but not the top ads shown to the other group. Thus, the ad settings do offer the users a degree of choice over the ads they see.

We also found evidence suggestive of \emph{discrimination} from another experiment.
We set the agents' gender to female or male on Google's Ad Settings page.  We then had both the female and male groups of agents visit webpages associated with employment.  We established that Google used this gender information to select ads, as one might expect. The interesting result was how the ads differed between the groups: during this experiment, Google showed the simulated males ads from a certain career coaching agency that promised large salaries more frequently than the simulated females, a finding suggestive of discrimination. 
Ours is the first study that provides statistically significant evidence of an instance of discrimination in online advertising when demographic information is supplied via a transparency-control mechanism (i.e., the Ad Settings page).


While neither of our findings of opacity or discrimination are clear violations of Google's privacy policy~\cite{google-privacy} and we do not claim these findings to generalize or imply widespread issues, we find them concerning and warranting further investigation by those with visibility into the ad ecosystem.
Furthermore, while our finding of discrimination in the non-normative sense of the word is on firm statistical footing, we acknowledge that people may disagree about whether we found discrimination in the normative sense of the word.  We defer discussion of whether our findings suggest unjust discrimination until Section~\ref{sec:disc}.





\paragraph{Contributions}
In addition to the experimental findings highlighted above, we provide AdFisher, a tool for \emph{automating} such experiments.  AdFisher is structured as a Python API providing functions for setting up, running, and analyzing experiments.
We use Selenium to drive Firefox browsers and the scikit-learn library~\cite{scikit-learn} for implementations of classification algorithms. 
We use the SciPy library~\cite{scipy} for implementing the statistical analyses of the core methodology.




AdFisher offers \emph{rigor} by performing a carefully designed experiment. The statistical analyses techniques applied do not make questionable assumptions about the collected data. We base our design and analysis on a prior proposal that makes no assumptions about the data being independent or identically distributed~\cite{tschantz14arxiv}. Since advertisers update their behavior continuously in response to unobserved inputs (such as ad auctions) and the experimenters' own actions, such assumptions may not always hold. Indeed, in practice, the distribution of ads changes over time and simulated users, or \emph{agents}, interfere with one another~\cite{tschantz14arxiv}.

Our automation, experimental design, and statistical analyses allow us to \emph{scale} to handling large numbers of agents for finding subtle differences.  In particular, we modify the prior analysis of Tschantz et al.~\cite{tschantz14arxiv} to allow for experiments running over long periods of time.  We do so by using \emph{blocking} (e.g.,~\cite{good05book}), a nested statistical analysis not previously applied to understanding web advertising.  The blocking analysis ensures that agents are only compared to the agents that start out like it and then aggregates together the comparisons across blocks of agents.  Thus, AdFisher may run agents in batches spread out over time while only comparing those agents running simultaneously to one another.

AdFisher also provides \emph{explanations} as to how Google alters its behaviors in response to different user actions. 
It uses the trained classifier model to find which features were most useful for the classifier to make its predictions. It provides the top features from each group to provide the experimenter/analyst with a qualitative understanding of how the ads differed between the groups. 




To maintain statistical rigor, we carefully circumscribe our claims. We only claim statistical soundness of our results: if our techniques detect an effect of the browsing activities on the ads, then there is indeed one with high likelihood (made quantitative by a p-value). We do not claim that we will always find a difference if one exists, nor that the differences we find are typical of those experienced by users. Furthermore, while we can characterize the differences, we cannot assign blame for them since either Google or the advertisers working with Google could be responsible.






\paragraph{Contents}
After covering prior work next, we present, in Section~\ref{sec:prop}, privacy properties that our tool AdFisher can check: nondiscrimination, transparency, and choice.
Section~\ref{sec:meth} explains the methodology we use to ensure sound conclusions from using AdFisher.  Section~\ref{sec:tool} presents the design of AdFisher.  Section~\ref{sec:expr} discusses our use of AdFisher to study Google's ads and settings.
We end with conclusions and future work.

Raw data and additional details about AdFisher and our experiments can be found at\\
\centerline{\url{http://www.cs.cmu.edu/~mtschant/ife/}}
AdFisher is freely available at \\
\centerline{\url{https://github.com/tadatitam/info-flow-experiments/}}


\section{Prior Work}

We are not the first to study how Google uses information.
The work with the closest subject of study to ours is by Wills and Tatar~\cite{wills12wpes}.  They studied both the ads shown by Google and the behavior of Google's Ad Settings (then called the ``Ad Preferences'').  Like us, they find the presence of opacity: various interests impacted the ads and settings shown to the user and that ads could change without a corresponding change in Ad Settings.  
Unlike our study, theirs was mostly manual, small scale, lacked any statistical analysis, and did not follow a rigorous experimental design.  
Furthermore, we additionally study choice and discrimination.


Other related works differ from us in both goals and methods.  They all focus on how visiting webpages change the ads seen.  While we examine such changes in our work, we do so as part of a larger analysis of the interactions between ads and personalized ad settings, a topic they do not study.


Barford et al.\ come the closest in that their recent study looked at both ads and ad settings~\cite{barford14www}.
They do so in their study of the ``adscape'', an attempt to understand each ad on the Internet.
They study each ad individually and cast a wide net to analyze many ads from many websites while simulating many different interests.
They only examine the ad settings to determine whether they successfully induced an interest.
We rigorously study how the settings affects the ads shown (choice) and how behaviors can affect ads without affecting the settings (transparency).
Furthermore, we use focused collections of data and an analysis that considers all ads collectively to find subtle causal effects within Google's advertising ecosystem.
We also use a randomized experimental design and analysis to ensure that our results imply causation.




The usage study closest to ours in statistical methodology is that of Tschantz et al.~\cite{tschantz14arxiv}. 
They developed a rigorous methodology for determining whether a system like Google uses information.
Due to limitations of their methodology, they only ran small-scale studies.
While they observed that browsing behaviors could affect Ad Settings, they did not study how this related to the ads received.
Furthermore, while we build upon their methodology, we automate the selection of an appropriate test statistic by using machine learning whereas they manually selected test statistics.



The usage study closest to ours in terms of implementation is that of Liu et al.\ in that they also use machine learning~\cite{liu13hotnets}.
Their goal is to determine whether an ad was selected due to the content of a page, by using behavioral profiling, or from a previous webpage visit.
Thus, rather than using machine learning to select a statistical test for finding causal relations, they do so to detect whether an ad on a webpage matches the content on the page to make a case for the first possibility.  Thus, they have a separate classifier for each interest a webpage might cover.  Rather than perform a statistical analysis to determine whether treatment groups have a statistically significant difference, they use their classifiers to judge the ratio of ads on a page unrelated to the page's content, which they presume indicates that the ads were the result of behavioral targeting.



L\'ecuyer et al.\ present XRay, a tool that looks for correlations between the data that web services have about users and the ads shown to users~\cite{lecuyer14usenix}.  
While their tool may check many changes to a type of input to determine whether any of them has a correlation with the frequency of a single ad, it does not check for causation, as ours does.

Englehardt et al.\ study filter bubbles with an analysis that assumes independence between observations~\cite{englehardt14man}, an assumption we are uncomfortable making.  (See Section~\ref{sec:pitfalls}.)

Guha et al.\ compare ads seen by three agents to see whether Google treats differently the one that behaves differently from the other two~\cite{guha10imc}.  We adopt their suggestion of focusing on the title and URL displayed on ads when comparing ads to avoid noise from other less stable parts of the ad.  Our work differs by studying the ad settings in addition to the ads and by using larger numbers of agents.  Furthermore, we use rigorous statistical analyses. Balebako et al.\ run similar experiments 
to study the effectiveness of privacy tools~\cite{balebako12w2sp}.

Sweeney ran an experiment to determine that searching for names associated with African-Americans produced more search ads suggestive of an arrest record than names associated with European-Americans~\cite{sweeney13cacm}.  
Her study required considerable insight to determine that suggestions of an arrest was a key difference.
AdFisher can automate not just the collection of the ads, but also the identification of such key differences by using its machine learning capabilities.  Indeed, it found on its own that simulated males were more often shown ads encouraging the user to seek coaching for high paying jobs than simulated females.
  













\section{Privacy Properties}
\label{sec:prop}

Motivating our methodology for finding causal relationships, we present some properties of ad networks that we can check with such a methodology in place.
As a fundamental limitation of science, we can only prove the existence of a causal effect; we cannot prove that one does not exist (see Section~\ref{sec:scope}).  
Thus, experiments can only demonstrate violations of nondiscrimination and transparency, which require effects.
On the other hand, we can experimentally demonstrate that effectful choice and ad choice are complied with in the cases that we test since compliance follows from the existence of an effect.
Table~\ref{tbl:properties} summarizes these properties.

\begin{tablewide}
\newcommand*{\extrarowgap}{2.4ex}
\begin{tab}{lL{5cm}L{5cm}l}
Property Name & Requirement & Causal Test & Finding\\
\midrule
Nondiscrimination & Users differing only on protected attributes are treated similarly & Find that presence of protected attribute causes a change in ads & Violation\\extrarowgap]
Effectful choice & Changing a setting has an effect on ads & Find that changing a setting causes a change in ads & Compliance\\tightlinespace]\text{p-value}\end{array}\begin{array}{l}\text{Adj.}\\tightlinespace]\text{p-value}\end{array}\begin{array}{l}\text{Adj.}\\tightlinespace]\text{p-value}\end{array}\begin{array}{l}\text{Adj.}\\tightlinespace]\text{p-value}\end{array}\mbox{}\hspace{\antiarrayspace}\begin{array}{l}\text{Bonferroni}\\tightlinespace]\text{p-value}\end{array}\mbox{}\hspace{\antiarrayspace}\begin{array}{l}\text{Unadjusted}\\tightlinespace]\text{flipped p-value}\end{array}\mbox{}\hspace{\antiarrayspace}\begin{array}{l}\text{Holm-Bonferroni}\4~\cite{olejnik13ndss}.
In the billion dollar ad industry, its total effect was about \
\frac{|\set{a \in \mc{A}}{s(\vec{y}) \leq s(a(\vec{y}))}|}{|\mc{A}|}
= \frac{1}{|\mc{A}|} \sum_{a \in \mc{A}} I[s(\vec{y}) \leq s(a(\vec{y}))] \label{eqn:pt}

\frac{1}{(m!)^k} \sum_{a \in \bigtimes_{i = 1}^{k} \Pi(\mc{B}_i)} I[s(\vec{y}) \leq s(a(\vec{y}))] 
\label{eqn:pt-block}

\frac{1}{|\mc{A}'|} \sum_{a \in \mc{A}'} I[s(\vec{y}) \leq s(a(\vec{y}))] \label{eqn:pt-approx}

\Pr[ \hat{P} = \hat{p} \given n, p] = \binom{n}{\hat{p}n} p^{\hat{p}n} (1-p)^{(1-\hat{p})n}

s(\vec{y}) &= \sum_{i = 1}^{n/2} c(y_i) + \sum_{i = n/2 + 1}^{n} \neg c(y_i)

This is the number correctly classified.












\section{Holm-Bonferroni Correction}
\label{app:correction}

The Holm-Bonferroni Correction starts by ordering the hypotheses in a family from the hypothesis with the smallest (most significant) p-value  to the hypothesis with the largest (least significant) p-value ~\cite{holm79scandinavian}.
For a hypothesis , its unadjusted p-value  is compared to an adjusted level of significance  
where  is the unadjusted level of significance ( in our case),  is the total number of hypotheses in the family, and  is the index of hypothesis in the ordered list (counting from  to ).
Let  be the lowest index  such that .
The hypotheses  where  are accepted as having statistically significance evidence in favor of them (more technically, the corresponding null hypotheses are rejected).
The hypotheses  where  are not accepted as having significant evidence in favor of them (their null hypotheses are not rejected).

We report adjusted p-values to give an intuition about the strength of evidence for a hypothesis.
We let  be the adjusted p-value for  provided  since  iff .
Note that the adjusted p-value depends not just upon its unadjusted value but also upon its position in the list.
For the remaining hypotheses, we provide no adjusted p-value since their p-values are irrelevant to the correction beyond how they order the list of hypotheses.


\end{document}
