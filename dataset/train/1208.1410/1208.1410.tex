\documentclass[letterpaper,conference]{IEEEtran}
\hyphenation{op-tical net-works Ca-pe-ta-na-kis}


\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{property}{Property}
\newtheorem{corollary}{Corollary}
\newtheorem{opis}{}

\newcommand{\reals}{{\mbox{\bf R}}}
\newcommand{\eg}{{\it e.g.}}
\newcommand{\ie}{{\it i.e.}}

\pagestyle{empty}

\begin{document}

\title{Compressed Sensing based Protocol for Efficient Reconstruction of Sparse Superimposed Data in a Multi-Hop Wireless Sensor Network}
\author{\authorblockN{Megumi Kaneko and Khaldoun Al Agha}\\
\authorblockA{Graduate School of Informatics, Kyoto University,\\
Yoshida Honmachi Sakyo--ku, Kyoto, 606-8501, Japan\\
Email: meg@i.kyoto-u.ac.jp}
LRI, Universit\'{e} Paris-Sud, 91405 Orsay cedex, France\\
Email: alagha@lri.fr}

\maketitle

\begin{abstract}
We consider a multi-hop wireless sensor network that measures sparse events and propose a simple forwarding protocol based on Compressed Sensing (CS) which does not need any sophisticated Media Access Control (MAC) scheduling, neither a routing protocol, thereby making significant overhead and energy savings. By means of flooding, multiple packets with different superimposed measurements are received simultaneously at any node. Thanks to our protocol, each node is able to recover each measurement and forward it while avoiding cycles. Numerical results show that our protocol achieves close to zero reconstruction errors at the sink, while greatly reducing overhead. This initial research reveals a new and promising approach to protocol design through CS for wireless mesh and sensor networks.


\end{abstract}

\section{Introduction}

Wireless sensor networks are used in large sensing areas where multi-hop routing is needed to forward the information from the sensors to the sinks. When a sensor wakes up according to an event and then measures a physical value, it first needs to schedule its transmission in an interference-free slot using a Media Access Control (MAC) layer scheme and then to use a specific routing protocol in order to fix the next hop before reaching the final sink. Those steps include heavy operations for small sensors and consume a big quantity of energy. However, they are mandatory in order to avoid collisions and to determine the best routes to reach sinks.



Recently, the groundbreaking theory of Compressed Sensing (CS) was developed, which stipulates that a vector with correlated entries, i.e., that can be transformed into a sparse vector through a transformation basis, can be recovered with high accuracy from a few random projections onto another, incoherent basis~\cite{Can06feb}~\cite{Don06apr}~\cite{Can06dec}. CS, which has been widely used in the domains of digital signal and image processing, is envisioned as a highly promising tool for improving the performance of wireless communication networks.
In particular, based on the decentralized compression techniques presented in~\cite{Hau08mar}, there has been a number of works exploiting CS in wireless sensor networks dealing with space-time correlated data as in~\cite{Que09feb}\cite{Wan10oct}. In these works, the underlying assumption is that all sensor nodes have a data to send which are gathered at the sink and reconstructed with few observations using CS techniques. The transmission of each data follows some predetermined routes, such that packets are received without errors. In~\cite{Ngu10oct}, the CS technique is combined to the well-known Network Coding (NC), which provides an efficient method for data communication in wireless networks, as multiple packets aggregated in a single packet are decoded using prior information. Introducing CS enabled to mitigate the problem of exploding header size with NC where the header grew proportionally to the number of aggregated packets, since it became possible to decode the salient information from fewer number of packets. Note also that, as a consequence, CS enables to significantly decrease the delay required by NC for gathering packets, as at least  different combined packets were needed to decode  packets. However, in this work too, the transmissions follow predetermined routes in order to avoid any collision, i.e., packets are aggregated from node to node and not
``over-the-air".
By contrast,~\cite{Men09mar} considers a one-hop sensor network where multiple nodes may transmit their measurement simultaneously to the sinks. As the measured events are assumed to be sparse, i.e., the number of measurements simultaneously received at the sink is much smaller than the total number of sensors, the sink is able to recover each measurement from the superimposed signals from few observations, thanks to CS algorithms.
Such an approach is also taken for developing new multiple access schemes for random data traffic in~\cite{Mao10apr}, for channel reservation in~\cite{Qas09sep}, and for downlink scheduling in~\cite{Bha09jun}.

In this work, we consider a multi-hop wireless sensor network and take the approach of~\cite{Men09mar}--\cite{Bha09jun} where the events to be reported occur sparsely. Each sensor with a value to report combines it with its signature sequence giving its Identity (ID) and broadcasts the resulting vector to its neighbor nodes. We develop a protocol based on CS and flooding that enables the sink to obtain and reconstruct the sparse measurement data with high accuracy without any heavy routing nor MAC protocol. In a one-hop network, the simultaneously received measurements may be resolved by the CS-based algorithms in~\cite{Men09mar}--\cite{Bha09jun}, but they pose major problems in a multi-hop network where every node forwards all received packets, as the number of interfering measurement packets may be increased drastically, thereby causing the CS-based algorithms to perform poorly due to loss of sparsity. Moreover, if different measurements of a same source but generated at different times are contained in the same packet, they would be hardly resolvable.
To alleviate the aforementioned issues, in our protocol, the length of the signature sequences is further increased so that each sequence can identify the source node and time stamp, which allows to avoid cycles in the flooding process as follows. When neighboring nodes receive the superimposed packets, they first recover each measurement data from an under-determined system of equations, using a CS algorithm based on - optimization, commonly used in the case of noisy measurements~\cite{Elad10}. After decoding each data, they check in their local tables if it was already sent, using the recovered origin node ID and time stamp from its sequence; if it is the case, they discard the data. If not, they again superimpose each data into a unique packet which is broadcasted to their neighbors. This process is repeated until reaching the sink. In addition to avoiding cycles, this process allows to maintain the "sparsity" of the superimposed data by discarding superfluous packets, thereby guaranteeing a good performance of the CS algorithms. Still, the length of these sequences is far below the total number of possible combinations of IDs and time stamps, which would be required in a CDMA-based protocol.

Many advantages are offered by our new protocol:

\begin{itemize}
\item Sensors do not need any MAC scheduling that consumes resources, energy and increase the delivery delays.
\item Sensors do not need any routing protocols that need many control messages that waste the bandwidth and also increase the energy consumption.
\item Decoding in the sensors needs processing power and consumes energy. However, those operations represent a negligible consumption compared to the radio reception, listening, transmission used by the MAC scheduling.
\item The time stamp represents an additional overhead but is bounded by, e.g., the maximum number of hops in a network. We show that it represents a negligible part compared to the overhead induced by a routing protocol and even compared to a flooding-based protocol with CDMA.
\item While discarding packets that have already been forwarded for keeping sparsity of the superimposed data and avoiding cycles, our protocol still offers diversity for each measurement data as a packet may reach the sink by many routes. Even if a packet is lost, the sink or the intermediate nodes may receive it by other routes.
\end{itemize}

The paper is organized as follows: after giving some basic results in Compressed Sensing and presenting related works, we explain the system model and our proposed protocol. The simulation results show its effectiveness, while reducing the amount of overhead compared to conventional protocols.


\section{Results from Compressed Sensing}
\label{sec CS}

Compressed Sensing is a newly developed mathematical theory that enables to solve underdetermined systems of equations under the sparsity prior of the solution, a problem formulated as

where , ,  with , and , where sparsity of  is defined by .
However, this is a non-convex optimization problem that requires combinatorial search over all possible sparse combinations of columns of , which rapidly becomes intractable. Therefore, convex relaxation of this problem has been considered by the following -minimization,

where , for which various efficient algorithms have been developed, among which Pursuit algorithms and Iterative Shrinkage Algorithms~\cite{Elad10}.

Under the assumption of noisy observations, the optimization problem may be reformulated as

which can be equivalently expressed as an  minimization problem,

where the parameter  can be viewed as a Lagrange Multiplier making a trade-off between representation error and sparsity of the solution.
It was shown that random matrices  whose entries are drawn from Gaussian or Sub-Gaussian (e.g., Bernouilli) distributions, guarantee stable recovery in the noisy case, provided that the number of measurements obey

where  is a constant (this condition guarantees that  will guarantee the Restricted Isometry Property (RIP) with high probability, enabling stable recovery by -minimization, see~\cite{Elad10} and references therein).
In our proposed protocol, we will employ the Iterative Shrinkage-Thresholding Algorithm (ISTA)~\cite{Dau04nov} as it efficiently solves the
 minimization  with very low complexity.


\section{Related work}
\label{sec RelatedWork}

In the multiple access schemes of~\cite{Qas09sep}\cite{Mao10apr}\cite{Bha09jun} for a single-hop system, several Mobile Stations (MS) access the Base Station (BS) at the same time, on the same frequency.
Assuming  MSs in total, each MS  is assigned a pseudo-random signature sequence , a vector of size , which entries are generated by Bernoulli random variables of probability 1/2, with values . Thus, in the absence of noise, the received signal  at the BS may be expressed as

where  is the vector of size  with component  if MS  transmitted, and  if not. As the number of transmitting MSs is assumed to be much smaller than the total number of MSs ,  is a sparse vector. Hence,  is recovered at the BS using CS algorithms solving problems (\ref{eq P1}), or (\ref{eq P12}) in the noisy case.
Note that, the usage of these pseudo-random signature sequences to identify each MS may be regarded as similar to a Code Division Multiple Access (CDMA) system. However, CDMA systems typically require orthogonal codes to differentiate each MS, which would require each sequence  to be at least of size , whereas these multi-access systems with CS deal with non-orthogonal sequences since , which can be viewed as an overloaded CDMA system, in which case traditional CDMA receivers such as the linear minimum mean squared error filter perform very badly, as shown in~\cite{Bha09jun}.
Nevertheless, we will also compare our proposed protocol to a CDMA-based reference protocol that does not require any routing.

\section{System Model}
\label{sec SysMod}

We consider a multi-hop wireless sensor network with  nodes and one sink, forming a lattice as shown in Fig. \ref{fig:lattice1}. Each sensor node  can be either in transmit or receive mode at any one time. If a sensor detects an event, the network forwards the corresponding measurement to the sink based on the proposed protocol. As in~\cite{Men09mar} which considers a one-hop sensor network (but multiple sinks), we assume that there may be up to  events occurring simultaneously within the whole network, but these events are considered to be sparse compared to the number of nodes, namely .


\begin{figure}[h]
\begin{center}
   \includegraphics[width=.55\linewidth]{./Fig/Fev_2012/CS_Mesh_1.eps}
\caption{Multi-Hop Wireless Sensor Network Model, Step 1}
\label{fig:lattice1}
\end{center}
\end{figure}

We assume digital transmissions as in~\cite{Men09mar}, i.e., measurements , instead of directly transmitting scalar values by amplitude modulation as in~\cite{Hau08mar}, since interfering analog signals pose difficult problems in a multi-hop network. For example, a node could simultaneously receive two packets from different paths, but containing the same measurement , in which case it would erroneously decode . As the goal here is to present the benefits of our protocol, we consider the digital case for simplicity, but the protocol will be developed to accommodate analog transmissions in the next phase.

Given the lattice structure of the network, it is assumed that each node only communicates with its closest neighbors at distance , so there may be ,  or  neighbors depending on the node's location. For example, a packet sent by node  in Fig. \ref{fig:lattice1} will be received by nodes  and  only, while a packet sent by node  will be received by nodes , ,  and .
For simplicity, all channels are considered to be Additive White Gaussian Noise (AWGN), and all links between neighbors at distance  have the same gain. Channel fading and path loss effects will be considered in the future work\footnote{Note that, although fading and path loss effects are considered in~\cite{Men09mar}, the channel gains between each sensor and sink are assumed to be known perfectly as they form the columns of sensing matrix .}. Note that all nodes are synchronized as in~\cite{Hau08mar}\cite{Men09mar}, but there are interesting design issues in the asynchronous case, to which the protocol may be extended.



\begin{figure}[h]
\begin{center}
   \includegraphics[width=.55\linewidth]{./Fig/Fev_2012/CS_Mesh_2.eps}
\caption{Multi-Hop Wireless Sensor Network Model, Step 2}
\label{fig:lattice}
\end{center}
\end{figure}




\section{Proposed Protocol}
\label{sec Algo}

The proposed routing protocol consists in a simple flooding whereby upon packet reception, each node broadcasts this packet locally and then all its neighbors forward this information to their neighbors and so on, until it reaches the sink. In this setting, each packet will be forwarded through different paths, creating multiple copies that provide diversity at the sink. As in the works presented in Section \ref{sec RelatedWork}, we use in our system the Bernoulli sequences for identification, forming the independent projection vectors required for recovering the compressed signals.
However, if each source node is only assigned a unique signature sequence as in the previous works, it will not be possible to distinguish whether the multiple received packets are due to self-interference, i.e., they are copies of a same original packet, or whether they correspond to measurements from the same source but taken at different times.

To alleviate this problem, we associate each measurement with a unique sequence , vector of size , that identifies the pair (origin node ID , number of hops ), where  is the sensor that originally detected this measurement, and  is its time stamp parameter, defined as the number of hops elapsed by forwarding the packet bearing this measurement, counted from its origin node up to its current location in the network.
We have , given by a pseudo-random noise signature sequence generated by Bernoulli random variables of probability 1/2, with  values. Thus, fixing a maximum delay for a packet or time to live  in number of hops, which depends on the network size, there are  sequences  of size , where we set  for reducing overhead consumption. These sequences are assumed to be known at each node.
They are gathered into matrix  of size , whose column  contains .
For notational convenience, let  denote the -length sparse vector where  which groups all the measurements with possible hop counts, i.e.,  where the  first terms store the measurements of source node  for  and similarly, the  last terms for source node  for .

Then, the proposed algorithm works as follows:

\noindent \textbf{1.} If source node  in transmit mode detects a new event with measurement value  initialized by , he transmits the packet of  bits

which is received by all its neighbor nodes   .
For example, assuming  and  to be source nodes in Fig. \ref{fig:lattice1}, packets  and  are received by their respective neighbors, i.e.  for  and  for .

\noindent \textbf{2.} At a time , if sensor node  is in receive mode, he receives signal  containing all simultaneous transmissions from his neighbor nodes   . Thus, the received signal at a node  is written
  
where , the measurement with source node  is included in  if it was actually forwarded by node . Vector  of size , denotes the AWGN.
Using the matrix  defined above, this is equivalently reformulated as

where  is the vector of size  defined as

In the example of Fig. \ref{fig:lattice1}, note that , i.e., the source nodes and transmit nodes coincide since we describe Step 1, so the received signal at receiver node  is written , where
.




\noindent \textbf{3.} Receiver node  decodes each superimposed measurements contained in the received signal , using CS principles. In particular, this problem can be formulated by - optimization as in (\ref{eq P12}),
     
which we solve by ISTA with very good reconstruction abilities with low complexity. 
\noindent \textbf{4.} The reconstructed vector  is renormalized so that each component belongs to .

\noindent \textbf{5.} After decoding the received measurements at node , their respective (source ID, time stamp)-sequence  could be identified.
Node  then compares the sequence  of each newly decoded measurement to the sequences of previously received ones that are stocked in its local table .
      \begin{itemize}
        \item if there exists a sequence  in  with  and : the same data was already received before with high probability, since no fading is assumed, such that any new measurement should have a hop count , unless the packet that followed the shortest path was lost. However, this will rarely occur in this setting. Thus,  discards the decoded measurement , from the set of data to be forwarded next.
        \item if there exists a sequence  in  with  and : although both share the same origin node , this is a new data as it has a smaller hop count. Therefore,  will forward the decoded data .
        \item in all other cases,  will forward .
      \end{itemize}

\noindent \textbf{6.} For all data  to be forwarded, the hop count is incremented to .
All decoded measurements as well as sequences in  for which  are discarded.
Then,  superimposes all the remaining data to be forwarded into a packet  composed of  bits,

and forwards it. Thus, in Fig. \ref{fig:lattice}, node  sends .

\noindent \textbf{7.} The sink node runs the reconstruction algorithm for all incoming packets during the session of duration . Whenever multiple versions of the same data are received, more diversity gain may be achieved to improve the decision accuracy.



\section{Numerical Results}

We first evaluate our proposed protocol in a network of  nodes. In a session,  sources generate measurements that are forwarded to the sink as explained in Section \ref{sec Algo}. These sparse generation events occur at random times during each session, e.g., they may occur simultaneously. Each session ends after time out  in hop counts, at which point the measurements collected by the sink are evaluated. The Signal-to-Noise Ratio (SNR) between neighboring nodes is fixed to  dB. Each simulated point is averaged over  sessions.
We evaluate the reconstruction error as in~\cite{Que09feb}\cite{Wan10oct}, averaged over all sessions, defined for one session as

where  is the vector of size  containing the original measurements for all  sensors, i.e., its corresponding components are in  if sensor  is an origin node, and zero otherwise, while  denotes the reshaped vector of collected measurements at the sink at the end of a session.

\begin{figure}[h]
\begin{center}
   \includegraphics[width=1\linewidth]{./Fig/Fev_2012/N25_L5_M30_10dB.eps} \vspace{-0.6cm}
\caption{Proposed Protocol: Reconstruction Error performance for different values of , , , .}
\label{fig:REN25}
\end{center}
\end{figure}


\begin{figure}[h]
\begin{center}
   \includegraphics[width=1\linewidth]{./Fig/Fev_2012/N49_L7_40M50_T30_LambdaOPT.eps} \vspace{-0.6cm}
\caption{Proposed Protocol: Reconstruction Error performance for different values of ,  and , , .}
\label{fig:REN49}
\end{center}
\end{figure}


\begin{figure}[h]
\begin{center}
   \includegraphics[width=1\linewidth]{./Fig/Fev_2012/Lambda_6s8_N49_M40_L7_30dB.eps} \vspace{-0.6cm}
\caption{Proposed Protocol: Reconstruction Error performance for different values of  and ; , ,  and .}
\label{fig:REN49Lambda}
\end{center}
\end{figure}



Fig. \ref{fig:REN25} shows the average reconstruction error of our protocol for , for sequence length ,  and . We fix  in the decoding algorithm by ISTA. We observe that in both cases, excellent reconstruction performance is achieved for all numbers of event sources , even for larger values of  which imply lower sparsity in the number of superimposed measurement data. The error performance is improved by increasing , as more diversity gain is achieved at the expense of delay.

Next, we consider a network of  nodes for ,  and .
For fixed , the reconstruction error increases as  increases, due to lower sparsity: as the number of interfered packets increases, it becomes more difficult to correctly receive and recover all measurements at each node. However, by increasing  from  to , the error performance decreases significantly for , since it provides a larger number of independent observations that allow to correctly decode a higher number of superimposed data by improving the estimation accuracy of the CS algorithm. Note that, even for small values of , the number of interfering packets to be decoded at a certain node may be much more, since multiple copies of a given packet may be received through different paths, reducing the sparsity of the received signal .
However, increasing  creates a larger overhead, even though it is far smaller than what is required by conventional protocols, as shown later.
Instead of increasing , Fig. \ref{fig:REN49} shows that the reconstruction error can be made close to zero by adapting the parameter  to , given the number of event sources, namely the sparsity of the system. Thus,
Fig. \ref{fig:REN49Lambda} evaluates the reconstruction error performance for the number of event sources , and varying values of . Note that curves for the "sparser" cases  are not represented, as their reconstruction error stays close to zero for . We observe that, as the number of sources increases, and hence as the sparsity diminishes, the reconstruction performance by ISTA becomes more sensitive to an accurate optimization of . In this case, the optimized values of  could be chosen as  for , respectively. The tendency for  to increase with  can be interpreted by the fact that more weight should be put on the  term in (\ref{eq P12}) for finding a sparse solution, as more and more "denser" candidate solutions appear.
Of course, the excellent reconstruction performance achieved by  comes at the price of increased computational complexity at each node, even though it should remain at a reasonable level given the very low computational complexity of ISTA. On the other hand, the alternative to optimizing  is to sufficiently increase , which also provides very good performance as already shown in Fig. \ref{fig:REN49} with , . The trade-off is thus between algorithm complexity or increased overhead, which is evaluated next.

We compare the proposed protocol with two reference algorithms: a Conventional (Conv.) algorithm based on AODV routing~\cite{aodv} and MAC scheduling, and a flooding-based protocol similar to our proposed algorithm, but using CDMA as explained in Section \ref{sec RelatedWork}. The comparison is only made in terms of overhead, since both reference algorithms should ideally provide excellent reconstruction performances under this system model, given the absence of interference for the first one, and due to the orthogonal codes for the second.
As overhead and data (origin node ID and measurement) are not separable in the proposed protocol, we evaluate the total amount of packets generated by each method. For the Conv. algorithm, the bounds are roughly given by

where  bytes for Route Request,  bytes for Route Reply, and
 
where  bits and  bits for Cyclic Redundancy Check. The lower bound  is given by  with the origin node at 1-hop distance from the sink, and the upper bound  by  for  ( for ), with the origin node at a corner of the lattice network, and


for each source measurement.
For the proposed protocol, the number of bits obtained by simulations, may be expressed by , where  is the total number of packets occuring in a session, including all the forwarded ones due to flooding.
Similarly, for the reference scheme with CDMA, we have , as each sequence should have a minimal length of  to guarantee near-zero reconstruction error as explained in Section \ref{sec RelatedWork}.
Fig. \ref{fig:Bits} shows that the proposed protocol largely decreases the amount of packets per session in [kbytes] for networks with  and  nodes, corresponding to  to  reduction compared to the conventional algorithm with routing and MAC.
Compared to the reference scheme with CDMA, although the gain of our protocol is around  for , it becomes tremendously high for , going up to . Due to the large amount of bits required for having orthogonal sequences in the CDMA system, the total amount of packets drastically increases and even surpasses the conventional routing and MAC based protocol as the network enlarges, showing evident scalability issues that are alleviated by our proposed protocol.
Thus, the proposed protocol provides very low reconstruction errors, while achieving tremendous overhead savings that become even higher as the network enlarges.




\begin{figure}[h]
\begin{center}
   \includegraphics[width=1\linewidth]{./Fig/Fev_2012/Nb_Bytes_25N49_30M50_5S7_T30_CDMA.eps} \caption{Average amount of transmitted packets per session in [kbytes] for two cases: , , ,  and , , , .}
\label{fig:Bits}
\end{center}
\vspace{-0.6cm}
\end{figure}



\section{Conclusion}

We have proposed a novel protocol enabling non-scheduled, simultaneous transmissions for delivering measurement data to the sink in multi-hop wireless sensor networks.
Due to the sparse nature of the sensed events, the superimposed measurements resulting from interfered packet reception at each node can be recovered by CS algorithms. By making use of time stamps and signature sequences for each measurement, our protocol achieves low reconstruction errors, bringing significant savings in terms of overhead and energy, as compared to conventional routing and MAC scheduling strategies, as well as CDMA and flooding based protocol.

This fundamental work opens new vistas for further research. The protocol will be extended to more general systems, including path loss and channel fading, various network topologies, asynchronous nodes and, the more challenging case of analog transmissions. Another promising direction is the design of new algorithms based on the proposed ideas for general multi-hop mesh networks in the context of unicast/multicast transmissions.



\addcontentsline{toc}{chapter}{References}
\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,CS_Mesh_BIB}


\end{document}
