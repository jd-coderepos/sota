\documentclass{article}
\usepackage{pgfplots}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{thmtools}
\usepackage{thm-restate}
\usepackage{framed}
\usepackage{fullpage}


\pgfplotsset{width=7cm}

\newcommand{\ints}{\ensuremath{\mathbb{Z}}}
\newcommand{\reals}{\ensuremath{\mathbb{R}}}
\newcommand{\posreals}{\ensuremath{\reals_{\ge 0}}}
\newcommand{\nats}{\ensuremath{\mathbb{N}}}

\DeclareMathOperator*{\probOp}{\mathrm{Pr}}
\DeclareMathOperator*{\expectOp}{\mathbb{E}}
\DeclareMathOperator{\sdiff}{\Delta}
\DeclareMathOperator{\delete}{\setminus}
\DeclareMathOperator{\contract}{/}
\DeclareMathOperator{\restrict}{\mid}

\newcommand{\expect}[2][]{\expectOp_{#1}\left[#2\right]}
\newcommand{\prob}[2][]{\probOp_{#1}\left[#2\right]}

\newcommand{\todo}[1]{\begin{center}\fbox{{\noindent\textbf{COMMENT}:#1}}\end{center}}

\newtheorem{theorem}{Theorem}[section]
 \newtheorem{fact}{Fact}[section]
 \newtheorem{lemma}[theorem]{Lemma}
 \newtheorem{corollary}[theorem]{Corollary}
 \theoremstyle{definition}
 \newtheorem{definition}{Definition}[section]

\author{Maxim Sviridenko\thanks{Yahoo!\ Labs, New York, NY, USA.  \texttt{sviri@yahoo-inc.com}} \and Jan Vondr\'ak\thanks{IBM Almaden Research Center, San Jose, CA, USA.  \texttt{jvondrak@us.ibm.com}} \and Justin Ward\thanks{Department of Computer Science, University of Warwick, Coventry, United Kingdom. \texttt{J.D.Ward@warwick.ac.uk}\enspace Work supported by EPSRC grant EP/J021814/1.}}
\title{Optimal approximation for submodular and supermodular optimization with bounded curvature}

\begin{document}
\maketitle

\pagenumbering{arabic}
\setcounter{page}{1}

\begin{abstract}
We design new approximation algorithms for the problems of optimizing submodular and supermodular functions subject to a single matroid constraint.  Specifically, we consider the case in which we wish to maximize a nondecreasing submodular function or minimize a nonincreasing supermodular function in the setting of bounded total curvature . In the case of submodular maximization with curvature , we obtain a -approximation --- the first improvement over the greedy -approximation of Conforti and Cornuejols from 1984, which holds for a cardinality constraint, as well as recent approaches that hold for an arbitrary matroid constraint.  

Our approach is based on modifications of the continuous greedy algorithm and non-oblivious local search,  and allows us to approximately maximize the sum of a nonnegative, nondecreasing submodular function and a (possibly negative) linear function.  We show how to reduce both submodular maximization and supermodular minimization to this general problem when the objective function has bounded total curvature.
We prove that the approximation results we obtain are the best possible in the value oracle model, even in the case of a cardinality constraint. 

We define an extension of the notion of curvature to general monotone set functions and show -approximation for maximization and -approximation for minimization cases.   Finally, we give two concrete applications of our results in the settings of maximum entropy sampling, and the column-subset selection problem.
\end{abstract}


\newcommand{\submod}{g}
\newcommand{\lin}{\ell}
\newcommand{\obj}{f}
\newcommand{\submodExt}{G}
\newcommand{\linExt}{L}
\newcommand{\objExt}{F}
\newcommand{\ground}{X}
\newcommand{\I}{\mathcal{I}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\bases}[1]{\mathcal{B}(#1)}
\newcommand{\conv}{\mathrm{conv}}
\newcommand{\rank}[1]{r_{#1}}
\newcommand{\chr}[1]{\mathbf{1}_{#1}}
\newcommand{\cR}{R}

\newcommand{\linmax}{\hat{v}_\lin}
\newcommand{\submodmax}{\hat{v}_\submod}
\newcommand{\vmax}{\hat{v}}
\newcommand{\emax}{\hat{e}}
\newcommand{\elinmax}{\hat{e}_\lin}
\newcommand{\esubmodmax}{\hat{e}_\submod}

\newcommand{\NOsubmod}{h}
\newcommand{\NOsubmodEst}{\tilde{\NOsubmod}}
\newcommand{\pot}{\psi}
\newcommand{\potEst}{\tilde{\pot}}
\newcommand{\sinit}{S_{0}}

\newcommand{\comp}[1]{\overline{#1}}
\newcommand{\compf}{\comp{f}}
\newcommand{\Mdual}{\M^{*}}
\newcommand{\Bdual}{\B^{*}}
\newcommand{\Odual}{O^{*}}
\newcommand{\Idual}{\I^{*}}
\newcommand{\Sdual}{S^{*}}
\newcommand{\fdual}{f^{*}}
\newcommand{\cinv}{(1 - c)^{-1}}
\newcommand{\NOLS}{\textsc{NOLS}}
\newcommand{\SubmodMax}{\textsc{SubmodMax}}
\newcommand{\SupmodMin}{\textsc{SupmodMin}}

\newcommand{\be}{\mathbf e}
\newcommand{\bc}{\mathbf c}
\newcommand{\bx}{\mathbf x}
\newcommand{\by}{\mathbf y}
\newcommand{\bv}{\mathbf v}
\newcommand{\proj}{\mathsf{proj}}
\newcommand{\dist}{\mathsf{dist}}
\newcommand{\spn}{\mathsf{span}}

\section{Introduction}
\label{sec:introduction}

The problem of maximizing a submodular function subject to various constraints is a meta-problem that appears in various settings, from combinatorial auctions \cite{LLN06,DNS05,Vondrak2008} and viral marketing in social networks \cite{KKT03} to optimal sensor placement in machine learning \cite{KGGK06,KSG08,KRGG09,KG11}. A classic result by Nemhauser, Wolsey and Fisher \cite{Nemhauser1978a} is that the greedy algorithm provides a -approximation for maximizing a nondecreasing submodular function subject to a cardinality constraint. The factor of  cannot be improved, under the assumption that the algorithm queries the objective function a polynomial number of times \cite{Nemhauser1978}. 

The greedy algorithm has been applied in numerous settings in practice. Although it is useful to know that it never performs worse than  compared to the optimum, in practice its performance is often even better than this, in fact very close to the optimum. To get a quantitative handle on this phenomenon, various assumptions can be made about the input. One such assumption is the notion of {\em curvature}, introduced by Conforti and Cornu\'ejols \cite{Conforti1984}: A function  has curvature , if  does not change by a factor larger than  when varying . A function with  is linear, so the parameter measures in some sense how far  is from linear. It was shown in \cite{Conforti1984} that the greedy algorithm for nondecreasing submodular functions provides a -approximation, which tends to  as .

Recently, various applications have motivated the study of submodular optimization under various more general constraints. In particular, the -approximation under a cardinality constraint has been generalized to any matroid constraint in \cite{Calinescu2011}. This captures various applications such as welfare maximization in combinatorial auctions \cite{Vondrak2008}, generalized assignment problems \cite{Calinescu2007} and variants of sensor placement \cite{KRGG09}. Assuming curvature , \cite{Vondrak2010} generalized the -approximation of \cite{Conforti1984} to any matroid constraint, and hypothesized that this is the optimal approximation factor. It was proved in \cite{Vondrak2010} that this factor is indeed optimal for instances of curvature  {\em with respect to the optimum} (a technical variation of the definition, which depends on how values change when measured on top of the optimal solution). 
In the following, we use {\em total curvature} to refer to the original definition of \cite{Conforti1984}, to distinguish from curvature w.r.t.~the optimum \cite{Vondrak2010}.

\subsection{Our Contribution}
\label{sec:our-contribution}

Our main result is that given total curvature , the -approximation of Conforti and Cornu\'ejols for monotone submodular maximization subject to a cardinality constraint \cite{Conforti1984} is suboptimal and can be improved to a -approximation. We prove that this guarantee holds for the maximization of a nondecreasing submodular function subject to any matroid constraint, thus improving the result of \cite{Vondrak2010} as well. We give two techniques that achieve this result: a modification of the continuous greedy algorithm of \cite{Calinescu2011}, and a variant of the local search algorithm of \cite{Filmus2014}. 

Using the same techniques, we obtain an approximation factor of  for minimizing a nonincreasing supermodular function subject to a matroid constraint. Our approximation guarantees are strictly better than existing algorithms for every value of  except  and .  The relevant ratios are plotted in Figures \ref{fig:submod} and \ref{fig:supmod}.
In the case of minimization, we have also plotted the inverse approximation ratio to aid in comparison.
\begin{figure}
\begin{center}
\begin{tikzpicture}
  \begin{axis}[xmin=0,ymin=0,domain=0.001:1, samples=100, smooth, no markers,
    enlargelimits=false, xlabel=, ylabel=, legend pos=south west]
 \addplot gnuplot[black, dashed, id=jan]{(1 - exp(-x))/x};
\addlegendentry{Previous \cite{Vondrak2010}}
 \addplot gnuplot[black, id=us1]{1 - x*exp(-1)};
\addlegendentry{This Paper}
 \end{axis}
 \end{tikzpicture}
\end{center}
\caption{Comparison of Approximation Ratios for Submodular Maximization}
\label{fig:submod}
\end{figure}
\begin{figure*}
\begin{center}
\begin{tikzpicture}
  \begin{axis}[xmin=0.0,xmax=1.0,ymin=1.0,ymax=10, samples=100, smooth, no markers,
    enlargelimits=false, xlabel=, ylabel=,
    legend columns=1,
    legend entries={Previous \cite{Ilev2001}, This Paper},
    legend to name=named,
    title={Approximation Ratio}]
]
 \addplot gnuplot[black,dashed, domain=0.00:0.9,id=ilev]{(exp(x/(1-x)) - 1)/(x/(1-x))};
 \addplot gnuplot[black,domain=0.00:0.99,id=us2]{1 + exp(-1)*x/(1-x)};
 \end{axis}
\end{tikzpicture}
\hspace{1cm}
\begin{tikzpicture}
  \begin{axis}[xmin=0.0,xmax=1.0,ymin=0.0,ymax=1, samples=1000, smooth, no markers,
    enlargelimits=false, xlabel=, ylabel=, title={Inverse Approximation Ratio}]
 \addplot gnuplot[black,dashed, domain=0.001:1.0,id=ilevinv]{1/((exp(x/(1-x)) - 1)/(x/(1-x)))};
 \addplot gnuplot[black,domain=0.00:0.999,id=us2inv]{1/(1 + exp(-1)*x/(1-x))};
 \end{axis}
 \end{tikzpicture}
\ref{named}
\caption{Comparison of Approximation Ratios for Supermodular Minimization}
\label{fig:supmod}
\end{center}
\end{figure*}




We also derive complementary negative results, showing that no algorithm that evaluates  on only a polynomial number of sets can have approximation performance better than the algorithms we give.  Thus, we resolve the question of optimal approximation as a function of curvature in both the submodular and supermodular case.





Further, we show that the assumption of bounded curvature alone is sufficient to achieve certain approximations, even without assuming submodularity or supermodularity. Specifically, there is a (simple) algorithm that achieves a -approximation for the maximization of any nondecreasing function of total curvature at most , subject to a matroid constraint. (In contrast, we achieve a -approximation with the additional assumption of submodularity.) Also, there is a -approximation for the minimization of any  nonincreasing function of total curvature at most  subject to a matroid constraint, compared with a -approximation for supermodular functions.


\subsection{Applications}
We provide two applications of our results.  In the first application, we are given a positive semidefinite matrix . Let  be a principal minor defined by the columns and rows indexed by the set . In the maximum entropy sampling problem (or more precisely in the generalization of that problem) we would like to find a set  maximizing . It is well-known that this set function  is submodular \cite{Kelmans1983} (many earlier and alternative proofs of that fact are known). In addition, we know that solving this problem exactly is NP-hard \cite{Lee1996} (see also Lee \cite{Lee2002} for a survey on known optimization techniques for the problem).

We consider the maximum entropy sampling problem when the matrix  has eigenvalues . Since the determinant of any matrix is just a product of its eigenvalues, the Cauchy Interlacing Theorem implies that the  the submodular function  is nondecreasing. In addition we can easily derive a bound on its curvature  (see the formal definition of curvature in Section \ref{sec:related-work}).  This immediately implies that our new algorithms for submodular maximization have an approximation guarantee of  for the maximum entropy sampling problem.

Our second application is the Column-Subset Selection Problem  arising in various machine learning settings. The goal is, given a matrix , to select a subset of  columns such that the matrix is well-approximated (say in squared Frobenius norm) by a matrix whose columns are in the span of the selected  columns. This is a variant of {\em feature selection}, since the rows might correspond to examples and the columns to features. The problem is to select a subset of  features such that the remaining features can be approximated by linear combinations of the selected features. This is related but not identical to Principal Component Analysis (PCA) where we want to select a subspace of rank  (not necessarily generated by a subset of columns) such that the matrix is well approximated by its projection to this subspace. While PCA can be solved optimally by spectral methods, the Column-Subset Selection Problem is less well understood. Here we take the point of view of approximation algorithms: given a matrix , we want to find a subset of  columns such that the squared Frobenius distance of  from its projection on the span of these  columns is minimized.  To the best of our knowledge, this problem is not known to be NP-hard; on the other hand, the approximation factors of known algorithms are quite large.   The best known algorithm for the problem as stated is a -approximation algorithm given by Deshpande and Rademacher \cite{Deshpande2010}.  For the related problem in which we may select any set of  columns that form a rank  submatrix of , Deshpande and Vempala \cite{Deshpande2006} showed that there exist matrices for which  columns must be chosen to obtain a -approximation.  Boutsidis et al.\ \cite{Boutsidis2014} give a matching algorithm, which obtains a set of  columns that give a  approximation.  We refer the reader to \cite{Boutsidis2014} for further background on the history of this and related problems.  

Here, we return to the setting in which only  columns of  may be chosen and show that this is a special case of nonincreasing function minimization with bounded curvature.  We show a relationship between curvature and the condition number  of , which allows us to obtain approximation factor of . We define the problem and the related notions more precisely in Section \ref{application}.

\subsection{Related Work}
\label{sec:related-work}

The problem of maximizing a nondecreasing submodular function subject to a cardinality constraint (i.e., a uniform matroid) was studied by Nemhauser, Wolsey, and Fisher \cite{Nemhauser1978a}, who showed that the standard greedy algorithm gives a -approximation.  However, in \cite{Fisher1978}, they show that the greedy algorithm is only -approximation for maximizing a nondecreasing submodular function subject to an arbitrary matroid constraint.  More recently, Calinescu et al. \cite{Calinescu2011} obtained a  approximation for an arbitrary matroid constraint.  In their approach, the \emph{continuous greedy algorithm} first maximizes approximately a multilinear extension of the given submodular function and then applies a \emph{pipage rounding} technique inspired by \cite{Ageev2004} to obtain an integral solution.  The running time of this algorithm is dominated by the pipage rounding phase.  Chekuri, Vondr\'{a}k, and Zenklusen \cite{Chekuri2010} later showed that pipage rounding can be replaced by an alternative rounding procedure called \emph{swap rounding} based on the exchange properties of the underlying constraint.  In later work \cite{Chekuri2011,Chekuri2011a}, they developed the notion of a \emph{contention resolution scheme}, which gives a unified treatment for a variety of constraints, and allows rounding approaches for the continuous greedy algorithm to be composed in order to solve submodular maximization problems under combinations of constraints.  Later, Filmus and Ward \cite{Filmus2012} obtained a -approximation for submodular maximization in an arbitrary matroid by using a non-oblivious local search algorithm that does not require rounding.

On the negative side, Nemhauser and Wolsey \cite{Nemhauser1978} showed that it is impossible to improve upon the bound of  in the value oracle model, even under a single cardinality constraint.  In this model,  is given as a value oracle and an algorithm can evaluate  on only a polynomial number of sets.  Feige \cite{Feige1998} showeds that  is the best possible approximation even when the function is given explicitly, unless .  In later work, Vondr\'{a}k \cite{Vondrak2009} introduced the notion of the \emph{symmetry gap} of a submodular function , which unifies many inapproximability results in the value oracle model, and proved new inapproximability results for some specific constrained settings.  Later, Dobzinski and Vondr\'ak \cite{Dobzinski2012} showed how these inapproximability bounds may be converted to matching complexity-theoretic bounds, which hold when  is given explicitly, under the assumption that .

Conforti and Cornu\'{e}jols \cite{Conforti1984} defined the \emph{total curvature}  of non-decreasing submodular function  as

They showed that greedy algorithm has an approximation ratio of  for the problem of maximizing a nondecreasing submodular function with curvature at most  subject to a single matroid constraint.  In the special case of a uniform matroid, they were able to show that the greedy is a -approximation algorithm.  Later, Vondr\'{a}k \cite{Vondrak2010} considered the continuous greedy algorithm in the setting of bounded curvature.  He introduced the notion of \emph{curvature with respect to the optimum}, which is a weaker notion than total curvature, and showed that the continuous greedy algorithm is a -approximation for maximizing a nondecreasing submodular function  subject to an arbitrary matroid constraint whenever  has curvature at most  with respect to the optimum.  He also showed that it is impossible to obtain a -approximation in this setting when evaluating  on only a polynomial number of sets.  Unfortunately, unlike total curvature, it is in general not possible to compute the curvature of a function with respect to the optimum, as it requires knowledge of an optimal solution.

We shall also consider the problem of minimizing nonincreasing \emph{supermodular} functions .  By analogy with total curvature, Il'ev \cite{Ilev2001} defines the \emph{steepness}  of a nonincreasing supermodular function.  His definition, which is stated in terms of the marginal \emph{decreases} of the function, is equivalent to \eqref{eq:curvature} when reformulated in terms of marginal gains.  He showed that, in contrast to submodular maximization, the simple  greedy heuristic does not give a constant factor approximation algorithm in the general case.  However, when the supermodular function  has total curvature at most , he shows that the reverse greedy algorithm is an -approximation where .

\section{Preliminaries}
\label{sec:preliminaries}

We now fix some of our notation and give two lemmas pertaining to functions with bounded curvature.

\subsection{Set Functions}
A set function  is \emph{submodular} if  for all .  Submodularity can equivalently be characterized in terms of \emph{marginal values}, defined by  for  and .  Then,  is submodular if and only if  for all  and .    That is, submodular functions are characterized by decreasing marginal values.  

Intuitively,  is \emph{supermodular} if and only if  is submodular.  That is,  is supermodular if and only if  for all  and .

Finally, we say that a function is \emph{non-decreasing}, or \emph{monotone increasing}, if  for all  and , and \emph{non-increasing}, or \emph{monotone decreasing}, if  for all  and . 

\subsection{Matroids}
\label{sec:matroids}
We now present the definitions and notations that we shall require when dealing with matroids.  We refer the reader \cite{Schrijver2003} for a detailed introduction to basic matroid theory.  Let  be a matroid defined on ground set  with independent sets given by .  We denote by  the set of all bases (inclusion-wise maximal sets in ) of .  We denote by  the matroid polytope for , given by:

where  denotes the rank function associated with .
The second equality above is due to Edmonds \cite{Edmonds1971}.  Similarly, we denote by  the base polytope associated with :


For a matroid , we denote by  the dual system  whose independent sets  are defined as those subsets  that satisfy  for some  (i.e., those subsets that are disjoint from some base of .  Then, a standard result of matroid theory shows that  is a matroid whenever  is a matroid, and, moreover,   is precisely the set  of complements of bases of .

Finally, given a set of elements , we denote by  the matroid  obtained by restricting to  to .  The independent sets  of  are simply those independent sets of  that contain only elements from .  That is, .

\subsection{Lemmas for Functions with Bounded Curvature}
\label{sec:curvature}

We now give two general lemmas pertaining to functions of bounded curvature that will be useful in our analysis.  The proofs, which follow directly from \eqref{eq:curvature}, are given in the Appendix.
\begin{restatable}{lemma}{submodcurv}
\label{lem:submod-curv}
If  is a monotone increasing submodular function with total curvature at most , then 
 for all .
\end{restatable}
\begin{restatable}{lemma}{supmodcurv}
\label{lem:supmod-curv}
If  is a monotone decreasing supermodular function with total curvature at most , then 
 for all .
\end{restatable}

\section{Submodular + Linear Maximization}
\label{sec:overv-our-appr}

Our new results for both submodular maximization and supermodular minimization with bounded curvature make use of an algorithm for the following meta-problem: we are given a monotone increasing, nonnegative, submodular function , a linear function , and a matroid   and must find a base  maximizing .  Note that we do not require  to be nonnegative.  Indeed, in the case of supermodular minimization (discussed in Section \ref{sec:superm-minim}), our approach shall require that  be a negative, monotone decreasing function.  For , we shall write  and  as a shorthand for  and .  We note that because  is linear, we have  for all .

Let , , and .  Then, because  is submodular and  is linear, we have both  and  for every set .  Moreover, given  and , we can easily compute  in time .  Our main technical result is the following, which gives a joint approximation for  and .

\begin{theorem}
\label{thm:main}
For every , there is an algorithm that, given a monotone increasing submodular function , a linear function  and a matroid , produces a set  in polynomial time satisfying

for every , with high probability.
\end{theorem}
In the next two sections, we give two different algorithms satisfying the conditions of Theorem \ref{thm:main}.

\section{A Modified Continuous Greedy Algorithm}
\label{sec:cont-greedy}
The first algorithm we consider is a modification of the continuous greedy algorithm of \cite{Calinescu2011}
We first sketch the algorithm conceptually in the continuous setting, ignoring certain technicalities.  

\subsection{Overview of the Algorithm}
Consider .  For any function , the \emph{multilinear extension}  of  is a function  given by , where  is a random subset of  in which each element  appears independently with probability .  We let  denote the multilinear extension of the given, monotone increasing submodular function , and  denote the multilinear extension of the given linear function .
Note that due to the linearity of expectation, .  That is, the multilinear extension  corresponds to the natural, linear extension of .  Let  and  be the matroid polytope and matroid base polytope associated , and let  be the arbitrary base in  to which we shall compare our solution in Theorem \ref{thm:main}.
\begin{figure}
\begin{framed}
\noindent {\bf Modified Continuous Greedy}
\begin{itemize}[itemsep=0ex, topsep=0.5ex]
\item Guess the values of  and .
\item Initialize .
\item For time running from  to , update  according to ,
where  is a vector satisfying both:

Such a vector exists because  is one possible candidate (as in the analysis of \cite{Calinescu2011}).
\item Apply pipage rounding to the point  and return the resulting solution.
\end{itemize}
\end{framed}
\caption{The modified continuous greedy algorithm}
\label{fig:cont-greedy-alg}
\end{figure}
Our algorithm is shown in Figure \ref{fig:cont-greedy-alg}.
In contrast to the standard continuous greedy algorithm, in the third step we require a direction that is larger than \emph{both} the value of  and the residual value .  Applying the standard continuous greedy algorithm to  gives a direction that is larger than the \emph{sum} of these two values, but this is insufficient for our purposes.  

Our analysis proceeds separately for  and . First, because  is linear, we obtain

for every time , and hence . For the submodular component, we obtain

similar to the analysis in \cite{Calinescu2011}. This leads to a differential equation that gives


The final pipage rounding phase is oblivious to the value of the objective function and so is not affected by the potential negativity of .  We can view  as the multilinear extension of  and so, as in the standard continuous greedy analysis, pipage rounding produces an integral solution  satisfying .

\subsection{Implementation of the Modified Continuous Greedy Algorithm}

Now we discuss the technical details of how the continuous greedy algorithm can be implemented efficiently.
There are three main issues that we ignored in our previous discussion: (1) How do we ``guess" the values of  and ; 
(2) How do we find a suitable direction  in each step of the algorithm; and 
(3) How do we discretize time efficiently?  Let us now address them one by one.

\paragraph{Guessing the optimal values:}
In fact, it is enough to guess the value of ; we will optimize over  later.  
Recall that .  We discretize the interval\footnote{In the applications we consider  is either nonnegative or non-positive, and so we need only consider half of the given interval.  For simplicity, here we give a general approach that does not depend on the sign of .  In general, we have favored, whenever possible, simplicity in the analysis over obtaining the best runtime bounds.}  with  points of the form  for , filling the interval , together with  points of the form  and  for , filling the intervals  and , respectively.  We then run the following algorithm using each point as a guess for , and return the best solution found.  Then if , we must have  
 for some iteration (using one of the guesses in ).  Similarly, if , then for some iteration we have

(using one of the guesses in  or ).  For the remainder of our analysis we consider this particular iteration.

\paragraph{Finding a suitable direction:}
Given our guess of  and a current solution , our goal is to find a direction  such that
 and .  As in \cite{Calinescu2011}, we must estimate  by random sampling.   Then, given an estimate , we solve the linear program:

We can do this by the ellipsoid method, for example (or more efficiently using other methods).

Following the analysis of \cite{Calinescu2011}, we can obtain, in polynomial time, an estimate satisfying

with high probability.  Since , the base  is a feasible solution of \eqref{eq:lp}.  Because  is concave along any nonnegative direction, we then have:

just as in the analysis of \cite{Calinescu2011}.

\paragraph{Discretizing the algorithm:}
We discretize time into steps of length ; let us assume for simplicity that  is an integer.
In each step, we find a direction  as described above and we update

Clearly, after  steps we obtain a solution  which is a convex combination of points , and therefore a feasible solution. In each step, we had  and so

The analysis of the submodular component follows along the lines of \cite{Calinescu2011}.  In one time step, we gain

using the bound \eqref{eq:cg-submod-bound}. By induction (as in \cite{Calinescu2011}), we obtain

and so  as required.


\section{Non-Oblivious Local Search}
\label{sec:non-oblivious-local}

We now give another proof of Theorem \ref{thm:main}, using a modification of the local search algorithm of \cite{Filmus2014}.  In contrast to the modified continuous greedy algorithm, our modified local search algorithm does not need to guess the optimal value of , and also does not need to solve the associated continuous optimization problem given in \eqref{eq:lp}.  However, here the convergence time of the algorithm becomes an issue that must be dealt with.

\subsection{Overview of the Algorithm}
\label{sec:overv-non-obliv}

We begin by presenting a few necessary lemmas and definitions from the analysis of \cite{Filmus2014}.  We shall require the following general property of matroid bases, first proved by Brualdi \cite{Brualdi1969}, which can also be found in e.g. \cite[Corollary 39.12a]{Schrijver2003}.
\begin{lemma}
\label{lem:brualdi}
Let  be a matroid and  and  be two bases in .  Then, there exists a bijection  such that  for all .
\end{lemma}
We can restate Lemma \ref{lem:brualdi} as follows: let  and  be bases of a matroid  of .  Then we can index the elements of  so that , and then we have that  for all .  The resulting collection of sets  will define a set of feasible swaps between the bases  and  that we consider when analyzing our local search algorithm.

The local search algorithm of \cite{Filmus2014} maximizes a monotone submodular function  using a simple local search routine that evaluates the quality of the current solution using an auxiliary potential , derived from  as follows:


We defer a discussion of issues related to convergence and computing  until the next subsection, and first sketch the main idea of our modified algorithm.  We shall make use of the following fact, proved in \cite[Lemma 4.4, p.\ 524-5]{Filmus2014}: for all ,
 for some constant .

In order to jointly maximize , we employ a modified local search algorithm that is guided by the potential , given by:

Our final algorithm is shown in Figure \ref{fig:non-oblv-alg}.

\begin{figure}
\begin{framed}
\noindent {\bf Non-Oblivious Local Search}
\begin{itemize}[itemsep=0ex, topsep=0.5ex]
\item Let .
\item  an arbitrary base .
\item  While there exists  and 
such that  and  set .
\item Return 
\end{itemize}
\end{framed}
\caption{The non-oblivious local search algorithm}
\label{fig:non-oblv-alg}
\end{figure}

The following Lemma shows that if it is impossible to significantly improve  by exchanging a single element, then both  and  must have relatively high values.

\begin{lemma}
\label{lem:locality-gap}
Let  and  be any two bases of a matroid , and suppose that the elements of  are indexed according to Lemma \ref{lem:brualdi} so that  for all .  Then,

\end{lemma}
\begin{proof}
Filmus and Ward \cite[Theorem 5.1, p.\ 526]{Filmus2014} show that for any submodular function , the associated function  satisfies

We note that since  is linear, we have:

Adding  times \eqref{eq:filmus-locality-gap} to \eqref{eq:lin-locality-gap} then completes the proof.
\end{proof}

Suppose that  is locally optimal for  under single-element exchanges, and let  be an arbitrary base of .  Then, local optimality of  implies that  for all , where the elements  of  and  of  have been indexed according to Lemma \ref{lem:brualdi}.  Then, Lemma \ref{lem:locality-gap} gives , as required by Theorem \ref{thm:main}.

\subsection{Implementation of the Non-Oblivious Local Search Algorithm}
\label{sec:impl-non-obliv}

We now show how to obtain a polynomial-time algorithm from Lemma \ref{lem:locality-gap}.  We face two technical difficulties:
(1) how do we compute  efficiently in polynomial time; and (2) how do we ensure that the search for improvements converges to a local optimum in polynomial time?  As in the case of the continuous greedy algorithm, we can address these issues by using standard techniques, but we must be careful since  may take negative values.  As in that case, we have not attempted to obtain the most efficient possible running time analysis here, focusing instead on simplifying the arguments.

\paragraph{Estimating  efficiently:}

Although the definition of  requires evaluating  on a potentially exponential number of sets, Filmus and Ward show that  can be estimated efficiently using a sampling procedure:
\begin{lemma}[{\cite[Lemma 5.1, p.\ 525]{Filmus2014}}]
\label{lem:NOsubmodEst}
Let  be an estimate of  computed from  samples of .  Then, 

\end{lemma}
We let  be an estimate of .  Set .
We shall ensure that  differs from  by at most

Applying Lemma \ref{lem:NOsubmodEst}, we can then ensure that

by using  samples for each computation of .  By the union bound, we can ensure that  holds with high probability for all sets  considered by the algorithm, by setting  appropriately.  In particular, if we evaluate  on any polynomial number of distinct sets , it suffices to make  polynomially small, which requires only  a polynomial number of samples for each evaluation.

\paragraph{Bounding the convergence time of the algorithm:}

We initialize our search with an arbitrary base , and at each step of the algorithm, we restrict our search to those improvements that yield a significant increase in the value of .  Specifically, we require that each improvement increases the current value of  by at least an additive term .  We now bound the total number of improvements made by the algorithm.

We suppose that all values  computed by the algorithm satisfy
  From the previous discussion, we can ensure that this is indeed the case with high probability.

Let .  Then, the total number of improvements applied by the algorithm is at most:

Each improvement step requires  evaluations of .  From the discussion in the previous section, setting  sufficiently high will ensures that all of the estimates made for the first  iterations will satisfy our assumptions with high probability, and so the algorithm will converge in polynomial time.  

In order to obtain a deterministic bound on the running time of the algorithm we simply terminate our search if it has not converged in  steps and return the current solution.  Then when the resulting algorithm terminates, with high probability, we indeed have  for every  and so
 
From Lemma \ref{lem:locality-gap}, the set  produced by the algorithm then satisfies

as required by Theorem \ref{thm:main}.

\section{Submodular Maximization and Supermodular Minimization}
\label{sec:applications}
We now return to the problems of submodular maximization and supermodular minimization with bounded curvature.  We reduce both problems to the general setting introduced in Section \ref{sec:overv-our-appr}.  In both cases, we suppose that we are seeking optimize a function  over a given matroid  and we let  denote any optimal base of  (i.e., a base of  that either maximizes or minimizes , according to the setting).

\subsection{Submodular Maximization}
\label{sec:subm-maxim}

Suppose that  is a monotone increasing submodular function with curvature at most , and we seek to maximize  over a matroid . 

\begin{theorem}
\label{thm:submod-main}
For every  and , there is an algorithm that given a monotone increasing submodular function  of curvature  and a matroid , produces a set  in polynomial time satisfying

for every , with high probability.
\end{theorem}

\begin{proof}
Define the functions:

Then,  is linear and  is submodular, monotone increasing, and nonnegative (as verified in Lemma \ref{lem:submod-app} of the appendix).  Moreover, because  has curvature at most , Lemma \ref{lem:submod-curv} implies that for any set , 

In order to apply Theorem \ref{thm:main} we must bound the term .  By optimality of  and non-negativity of  and , we have .  From Theorem \ref{thm:main}, we can find a solution  satisfying:

\end{proof}


\subsection{Supermodular Minimization}
\label{sec:superm-minim}

Suppose that  is a monotone decreasing supermodular function with curvature at most  and we seek to minimize  over a matroid . 

\begin{theorem}
\label{thm:supermod-main}
For every  and , there is an algorithm that given a monotone decreasing supermodular function  of curvature  and a matroid , produces a set  in polynomial time satisfying

for every , with high probability.
\end{theorem}

\begin{proof}
Define the linear and submodular functions:

Because  is monotone decreasing, we have  and so  for all .  Thus,  is a non-positive, decreasing linear function.  However, as we verify in Lemma \ref{lem:supmod-app} of the appendix,  is submodular, monotone increasing, and nonnegative.

We shall consider the problem of \emph{maximizing}  in the \emph{dual} matroid , whose bases correspond to complements of bases of .  We compare our solution  to this problem to the base  of .  Again, in order to apply Theorem \ref{thm:main}, we must bound the term .  Here, because  is non-positive, we cannot bound  directly as in the previous section.  Rather, we proceed by partial enumeration.  Let .  We iterate through all possible guesses  for , and for each such  consider .  We set  to be the set , and consider the matroid , obtained by restricting  to the ground set .  For each  satisfying , we apply our algorithm to the problem , and return the best solution  obtained.  Note since , the set  is also a base of  and so  is a base of .  

Consider the iteration in which we correctly guess .  In the corresponding restricted instance we have  and  for all .  Additionally,  and so , as required by our analysis.   Finally, from the definition of  and , we have .  Since , and  is nonpositive while  is nonnegative, 

Therefore, by Theorem \ref{thm:main}, the base  of  returned by the algorithm satisfies:

Finally, since  is supermodular with curvature at most , Lemma \ref{lem:supmod-curv} implies that for all ,

Thus, we have

\end{proof}
We note that because the error term depends on , our result requires that  is bounded away from  by a constant.


\section{Inapproximability Results}
\label{sec:inappr-results}

We now show that our approximation guarantees are the best achievable using only a polynomial number of function evaluations, even in the special case that  is a uniform matroid (i.e., a cardinality constraint).  Nemhauser and Wolsey \cite{Nemhauser1978} considered the problem of finding a set  of cardinality at most  that maximizes a monotone submodular function. They give a class of functions for which obtaining a -approximation for any constant  requires a super-polynomial number of function evaluations. Our analysis uses the following additional property, satisfied by every function  in their class: let , and let  be a set of size  on which  takes its maximum value.  Then, .
\begin{theorem}
\label{lem:submod-inapprox}
For any constant  and , there is no -approximation algorithm for the problem , where  is a monotone increasing submodular function with curvature at most , that evaluates  on only a polynomial number of sets.
\end{theorem}
\begin{proof}
Let  be a function in the family given by \cite{Nemhauser1978} for the cardinality constraint , and let  be a set of size  on which  takes its maximum value.  Consider the function:    In Lemma \ref{lem:submod-inapprox-curv} of the appendix, we show that  is monotone increasing, submodular, and nonnegative with curvature at most .

We consider the problem .  Let , and suppose that some algorithm returns a solution  satisfying , evaluating  on only a polynomial number of sets.  Because  is monotone increasing, we assume without loss of generality that . Then,

and so

Because each evaluation of  requires only a single evaluation of , this contradicts the negative result of \cite{Nemhauser1978}.
\end{proof}

\begin{theorem}
\label{lem:supmod-inapprox}
For any constant  and , there is no -approximation algorithm for the problem , where  is a monotone decreasing supermodular function with curvature at most , that evaluates  on only a polynomial number of sets.
\end{theorem}
\begin{proof}
Again, let  be a function in the family given by \cite{Nemhauser1978} for the cardinality constraint .  Let  be a set of size  on which  takes its maximum value, and recall that , where .   Consider the function:   In Lemma \ref{lem:supmod-inapprox-curv} of the appendix we show that  is monotone decreasing, supermodular, and nonnegative with curvature at most .   

We consider the problem .  Let , and suppose that some algorithm returns a solution , satisfying , evaluating  on only a polynomial number of sets.  

Suppose we run the algorithm for minimizing  and return the set .  Because  is monotone decreasing, we assume without loss of generality that  and so .  Then,

and so

Again, since each evaluation of  requires only one evaluation of , this contradicts the negative result of \cite{Nemhauser1978}.
\end{proof}

\section{Optimizing Monotone Nonnegative Functions of Bounded Curvature}
Now we consider the problem of maximizing (respectively, minimizing) an arbitrary monotone increasing (respectively, monotone decreasing) nonnegative  function  of bounded curvature subject to a single matroid constraint.  We do not require that  be supermodular or submodular, but only that there is a bound on the following generalized notion of curvature.

Let  be an arbitrary monotone increasing or monotone decreasing function.  We define the curvature  of  as

Note that in the case that  is either monotone increasing and submodular or monotone decreasing and supermodular, the minimum of  over  and  is attained when  and .  Thus,  \eqref{eq:gen-curvature} agrees with the standard definition of curvature given in \eqref{eq:curvature}.  Moreover, if monotonically increasing  has curvature at most  for some , then for all  we have 

for any , and .    
Analogously,  for monotonically decreasing function  we have

for any , and .
As with the standard notion of curvature for submodular and supermodular functions, when , we find that \eqref{eq:gen-curvature-2} and  \eqref{eq:gen-curvature-3} require  to be a linear function, while when , they require only that  is monotone increasing or monotone decreasing, respectively.

First, we consider the case in which we wish to maximize a monotone increasing function  subject to a matroid constraint .  Suppose that we run the standard greedy algorithm, which at each step adds to the current solution  the element  yielding the largest marginal gain in , subject to the constraint .
\begin{theorem}
\label{thm:arbitrary-max}
Suppose that  is a monotone increasing function with curvature at most , and  is a matroid.  Let  be the base produced by the standard greedy maximization algorithm on  and , and let  be any base of .  Then, 

\end{theorem}

\begin{proof}
Let  be rank of .  Let  be the th element picked by the greedy algorithm, and let  be the set containing the first  elements picked by the greedy algorithm.  We use the bijection guaranteed by Lemma \ref{lem:brualdi} to order the elements of  of  so that  for all , and let  be the set containing the first  elements of  in this ordering.  Then, 

The first inequality follows from \eqref{eq:gen-curvature-2} and .  The last inequality is due to the fact that  but  was chosen by the greedy maximization algorithm in the th round.
\end{proof}

Similarly, we can consider the problem of finding a base of  that minimizes .  In this setting, we again employ a greedy algorithm, but at each step choose the element  yielding the \emph{smallest} marginal gain in , terminating only when no element can be added to the current solution.  We call this algorithm the standard greedy minimization algorithm.

\begin{theorem}
\label{thm:arbitrary-min}
Suppose that  is a monotone increasing function with curvature at most  and  is a matroid.  Let  be the base produced by the standard greedy minimization algorithm on  and , and let  be any base of .  Then,

\end{theorem}
\begin{proof}
Let  and  be defined as in the proof of Theorem \ref{thm:arbitrary-max}.  Then, 

As in the proof of Theorem \ref{thm:arbitrary-max}, the first inequality follows from \eqref{eq:gen-curvature-2} and .  The last inequality is due to the fact that  but  was chosen by the greedy minimization algorithm in the th round.
\end{proof}

Now, we consider the case in which  is a monotone decreasing function.  For any function , we define the function  by  for all .  Then, since  is monotone decreasing,  is monotone increasing.  Moreover, the next lemma shows that the curvature of  is the same as that of .

\begin{lemma}
\label{lem:increasing-decreasing-curvature}
Let  be a monotone decreasing function with curvature at most , and define  for all .  Then,  also has curvature at most .
\end{lemma}
\begin{proof}
From the definition of , we have:

for any  and .  Consider any  and .   Since  is monotone decreasing with curvature at most , \eqref{eq:gen-curvature-3} implies

Thus,  for all  and .
\end{proof}

Given a matroid , we consider the problem of finding a base of  minimizing .  This problem is equivalent to finding a base of the dual matroid  that minimizes .  Similarly, the problem of finding a base of  that maximizes  can be reduced to that of finding a base of  that maximizes .  Since  is monotone increasing with curvature no more than , we obtain the following corollaries, which show how to employ the standard greedy algorithm to optimize monotone decreasing functions.
\begin{corollary}
\label{thm:arbitrary-max-decreasing}
Suppose that  is a monotone decreasing function with curvature at most  and  is a matroid.  Let  be the base of  produced by running the standard greedy maximization algorithm on  and .  Let  be any base of , , and .  Then,

\end{corollary}
\begin{corollary}
\label{thm:arbitrary-min-decreasing}
Suppose that  is a monotone decreasing function with curvature at most  and  is a matroid.  Let  be the base of  produced by running the standard greedy minimization algorithm on  and .  Let  be any base of , , and .  Then,

\end{corollary}

The approximation factors of  and  for maximization and minimization respectively are best possible, given curvature .
The hardness result for minimization follows from \cite{IJB}, where it is shown that no algorithm using polynomially many value queries can achieve an approximation factor of  for the problem , where  is monotone increasing (even submodular) of curvature . This implies that no -approximation for a constant  is possible for this problem. Next, we prove the hardness result for maximization; this proof is based on known hardness constructions for maximization of XOS functions \cite{DobzinskiS06,MirrokniSV08}.

\begin{theorem}
\label{thm:curvature-max-hardness}
For any constant  and , there is no -approximation using polynomially many queries for the problem  where  is monotone increasing of curvature .
\end{theorem}


\begin{proof}
Fix ,  and let  be a random subset of size  (assume  is an integer). We define the following function:
\begin{itemize}
\item .
\end{itemize}
The marginal values of  are always between  and ; therefore, its curvature is .
Consider a query .
For , we have  in any case. For , we have , unless
. Since  is a random -fraction of the ground set, we have .
Therefore, by the Chernoff bound,  is exponentially small in  (with respect to the choice of ).
Furthermore, as long as ,  depends only on  and hence the algorithm does not learn anything about the identity of the optimal set . Unless the algorithm queries an exponential number of sets, it will never find a set  satisfying  and hence the value of the returned solution is  with high probability. On the other hand, the optimum is . Therefore, the algorithm cannot achieve a better than -approximation.
\end{proof}

Therefore, the approximation factors in Theorems~\ref{thm:arbitrary-max} and \ref{thm:arbitrary-min} are optimal.
Combining these inapproximability results with Lemma \ref{lem:increasing-decreasing-curvature} we derive similar inapproximability results showing the optimality of Corollary~\ref{thm:arbitrary-max-decreasing} and \ref{thm:arbitrary-min-decreasing}.


\section{Application: the Column-Subset Selection Problem}\label{application}


Let  be an  real matrix. We denote the columns of  by . I.e., for ,
. The (squared) Frobenius norm of  is defined as  where here, and throughout this section, we use  to denote the standard,  vector norm.

For a matrix  with independent columns, the {\em condition number} is defined as
  If the columns of  are dependent, then  (there is a nonzero vector  such that ).

Given a matrix  with columns , and a subset , we denote by  the projection of  onto the subspace spanned by the respective columns of .
Given , it is easy to see that the matrix  with columns spanned by  that is closest to  in squared Frobenius norm is . The distance between  two matrices is thus

We define  to be this quantity as a function of :

where the final equality follows from the fact that  and  are orthogonal.

\iffalse
In the following lemma, we show that the function  is a monotone increasing submodular function of .  It then follows that  is a monotone decreasing supermodular function.
\begin{lemma}
\label{lem:proj-submod}
Let  be a set of vectors in , and let  be an arbitrary vector in .  Then, the function  is monotone increasing and submodular.
\end{lemma}
\begin{proof}
Monotonicity follows from the standard properties of projection.  To prove submodularity, let , and let  and  be two values in .  It suffices to show that for any :

To simplify our notation, let  and .  Note that if , then both sides of the inequality are 0, and if  both sides are equal.  Suppose, then, that neither  nor  are in , and let .  Then,

Now, let  and .  Then, we have

Now, set the quantity  if  and  otherwise.  Then,

Hence,

\end{proof}
\fi

Given a matrix  and an integer , the \emph{column-subset selection problem} (CSSP) is to select a subset  of  columns of  so as to minimize .
It follows from standard properties of projection that  is non-increasing, and so CSSP is a special case of non-increasing minimization subject to a cardinality constraint.  We now show that the curvature of  is related to the condition number of .

\begin{lemma}
\label{lem:condition-curvature}
For any non-singular matrix , the curvature 
of the associated set function  satisfies 

\end{lemma}

\begin{proof}
We want to prove that for any  and , 

This implies that by varying the set , a marginal value can change by at most a factor of . 
Recall that the marginal values of  are negative, but only the ratio matters so we can consider the respective absolute values. 
The inequalities (\ref{conditionNumberInequality}) imply that 


We now prove the  inequalities (\ref{conditionNumberInequality}). Let  denote the component of  orthogonal to .
We have

because ,  and  are orthogonal.


Our first goal is to show that if   is large, then there is a unit vector  such that  is large. In particular, let us define  and . We have . Multiplying by matrix , we obtain . We can estimate  as follows:

By the Cauchy-Schwartz inequality, this implies that .

On the other hand, using the expression above, we have

since  is the component of  orthogonal to .
We claim that if  is small, then there is a unit vector  such that  is small. To this purpose, write  as a linear combination of the vectors : . Finally, we define , and normalize to obtain .
We get the following:

Since , and , we obtain .

In summary, we have given two unit vectors  with  and .
This proves that 
\end{proof}

By Corollary \ref{thm:arbitrary-min-decreasing}, the standard greedy minimization algorithm is then a -approximation for the column-subset selection problem.
The following lemma shows that Lemma~\ref{lem:condition-curvature} is asymptotically tight.

\begin{lemma}
There exists a matrix  with condition number  for which the associated function  has curvature .
\end{lemma}

Let us denote by  the distance from  to the subspace spanned by the columns corresponding to .




For some , consider  where  and  for .
(A similar example was used in \cite{Boutsidis2014} for a lower bound on column-subset approximation.) Here,  is the -th canonical basis vector in . We claim that the condition number of  is , while the curvature of  is .

To bound the condition number, consider a unit vector . We have

and

We need a lower bound and an upper bound on , assuming that . On the one hand, we have

On the other hand, to get a lower bound: if , then . If , then either , in which case  or  in which case by convexity we get  So, in all cases . This means that the condition number of  is .

To lower-bound the curvature of , consider the first column  and let us estimate  and . We have

On the other side,

We exhibit a linear combination of the columns  which is close to : Let . 
We obtain

Alternatively, we can also pick  which shows that . So we have

We conclude that the curvature of  is at least 

\paragraph{Acknowledgment}
We thank Christos Boutsidis for suggesting a connection between curvature and condition number.

\bibliographystyle{plain}
\bibliography{names,Curvature-full}

\appendix



\section{Proofs and Claims Omitted from the Main Body}
\label{sec:omitt-proofs-claims}

\submodcurv*
\begin{proof}
We order the elements of  arbitrarily, and let  be the set containing all those elements of  that precede the element .  Then, .  From \eqref{eq:curvature}, we have

which, since , is equivalent to

Because  is submodular, we have  for all ,
and so

\end{proof}

\supmodcurv*
\begin{proof}
Order  arbitrarily, and let  be the set of all elements in  that precede element , including  itself.  Then, 
.
From \eqref{eq:curvature}, we have

which, since , is equivalent to

Then, since  is supermodular, we have  for all ,  and so

\end{proof}

\begin{lemma}
\label{lem:submod-app}
Let  be a monotone-increasing submodular function and define  and .  Then,  is submodular,  monotone increasing, and nonnegative.
\end{lemma}
\begin{proof}
The function  is the sum of a submodular function  and a linear function , and so must be submodular.  For any set  and element ,

since  is submodular.
Thus,  is monotone increasing.  Finally, we note that  and so  must be nonnegative.
\end{proof}

\begin{lemma}
\label{lem:supmod-app}
Let  be a monotone-decreasing supermodular function and define  and .    Then,  is submodular, monotone increasing, and nonnegative.
\end{lemma}
\begin{proof}
We first show that  is monotone-increasing.  Consider an arbitrary  and .  Then,

where the last line holds because  is supermodular.  Moreover, note that , so  is nonnegative.  

Finally, we show that  is submodular.  Suppose  and .  Then,  and so, since  is supermodular, .  Thus,

\end{proof}

\begin{lemma}
\label{lem:submod-inapprox-curv}
Let  be a monotone increasing submodular function, satisfying  for all , and let .  Define   Then,  is submodular, monotone increasing, and nonnegative, and has curvature at most .
\end{lemma}
\begin{proof}
Because  is the sum of a monotone increasing, nonnegative submodular function and a nonnegative linear function, and hence must be monotone increasing, nonnegative, and submodular.  Furthermore, for any  and , we have .  Thus,

and so  has curvature at most .
\end{proof}

\begin{lemma}
\label{lem:supmod-inapprox-curv}
Let  be a monotone increasing submodular function, satisfying  for all , and let .  Define:
  Then,  is submodular, monotone increasing, and nonnegative, and has curvature at most .
\end{lemma}
\begin{proof}
Because  is submodular, so is , and hence  is supermodular.  Thus,  is the sum of a supermodular function and a linear function and so is supermodular.  In order to see that  is decreasing, we consider the marginal , which is equal to

Additionally, we note that , and so  must be nonnegative.  Finally, we show that  has curvature at most .  We have:

and therefore .
\end{proof}
\end{document}