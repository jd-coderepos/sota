In this appendix, we provide additional information on the PASTIS dataset and our exact model configuration. We also provide complementary qualitative experimental results.

\section{PASTIS Dataset}

\begin{figure}[ht!]
    \centering
    \begin{subfigure}{\linewidth}
    \centering
    \includegraphics[width=.78\linewidth,trim=0cm 3cm 0cm 3cm, clip, angle=90]{images/4tiles.pdf}
    \caption{Location of the four tiles.}
    \label{fig:data:global}
    \end{subfigure}
    \vfill
    \begin{subfigure}{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth, trim=0cm 0cm 9cm 0cm, clip]{images/t30uxv.pdf}
    \caption{Selected patches.}
    \label{fig:data:zoom1}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.485\linewidth}
    \centering
    \includegraphics[width=\textwidth, trim=0cm 0cm 6.4cm 0cm, clip]{images/singlepatchl.pdf}
    \caption{Single patch.}
    \label{fig:data:zoom3}
    \end{subfigure}
\caption{\textbf{Data Location.}~Spatial distribution of the four Sentinel tiles used in PASTIS \ref{fig:data:global}, and of the selected patches of tile T30UXV \ref{fig:data:zoom1}. We show an example of patch in \ref{fig:data:zoom3}, and highlight with red circles examples of parcels that are mostly outside of the patch's extent and thus annotated with the void label. The green circle \protect\tikz \protect\node[circle, thick, draw = green!90!black, fill = none, scale = 0.7] {}; highlight a parcel partially cut off by the patch borders, but with sufficient overlap to be kept as a valid parcel.
}
\end{figure}

\paragraph{Overview.} The PASTIS dataset is composed of $2433$ square $128\times128$ patches with $10$ spectral bands and at $10$m resolution, obtained from the open-access Sentinel-2 platform. \footnote{\url{https://scihub.copernicus.eu}} 
For each patch, we stack all available acquisitions between September $2018$ and November $2019$, forming our four dimensional multi-spectral SITS: $T \times C \times H \times W$. 
The publicly available French Land Parcel Identification System (FLPIS) allows us to retrieve the extent and content of all parcels within the tiles, as reported by the farmers.
Each patch pixel is annotated with a semantic label corresponding to either the parcels' crop type or the background class. The pixels of each unique parcel in the patch receive a corresponding instance label.

\paragraph{Dataset Extent.} 
The SITS of PASTIS are taken from 4 different Sentinel-2 tiles in different regions of the French metropolitan territory as depicted in Figure \ref{fig:data:global}. These regions cover a wide variety of climates and culture distributions.
Sentinel tiles span $100\times100$km and have a spatial resolution of $10$ meter per pixel. Each pixel is characterized by $13$ spectral bands. We select all bands except the atmospheric bands B01, B09, and B10.
Each of these tiles is subdivided in square patches of size $1.28\times1.28$km ($128\times128$ pixels at $10$m/pixel), for a total of around $24,000$ patches.
We then select $2,433$ patches ($~10\%$ of all available patches, see \figref{fig:data:zoom1}), favoring patches with rare crop types in order to decrease the otherwise extreme class imbalance of the dataset.



\paragraph{Nomenclature} 
The FLPIS uses a $73$ class breakdown for crop types. 
We select classes with at least $400$ parcels and with samples in at least $2$ of the $4$ Sentinel-2 tiles. This leads us to adopt a $18$ classes nomenclature, presented in Figure \ref{fig:nomenc}.
Parcels belonging to classes not in our $18$-classes nomenclature are annotated with the \emph{void} label, see below.

\paragraph{Patch Boundaries.}
The FLPIS allows us to retrieve the pixel-precise borders of each parcel. We also compute bounding boxes for each parcel.
The parcels' extents are cropped along the extent of their $128 \times 128$ patch, and the bounding boxes are modified accordingly.
Parcels whose surface is more than $50$\% outside of the patch are annotated with the \emph{void} label, see \figref{fig:data:zoom3}.


\paragraph{Void and Background Labels.} Pixels which are not within the extent of any declared parcel are annotated with the background ``stuff" label, corresponding to all non-agricultural land uses. For the semantic segmentation task, this label becomes the $20$-th class to predict. In the panoptic setting, this label is associated with pixels not within the extent of any predicted parcel. 
We do not compute the panoptic metrics for the background class, since our focus is on retrieving the parcels' extent rather than an extensive land-cover prediction. In other words, the reported panoptic metrics are the ``things" metrics, which already penalize parcels predicted for background pixels by counting them as false positives.

The void class is reserved for \emph{out-of-scope} parcels, either because their crop type is not in our nomenclature or because their overlap with the selected square patch is too small. We remove these parcels from all semantic or panoptic metrics and losses. Predicted parcels which overlap with an IoU superior to $0.5$ with a void parcel are not counted as false positive or true positive, but are simply ignored by the metric, as recommended in \cite{kirillov2019panoptic}.

\begin{figure}
    \centering
    \includegraphics[width=.85\linewidth, trim=0cm 5cm 38cm 0cm, clip]{images/Nomenclaturev2.pdf}
    \caption{Color code of our class nomenclature, and the number of parcel per class.}
    \label{fig:nomenc}
\end{figure}



\begin{figure}[ht!]
    \centering
    \includegraphics[width=\linewidth, trim=0cm 0cm 0cm 0cm, clip]{images/FoldDist.pdf}
    \caption{Class distribution for the five folds (in log-scale).}
    \label{fig:class_counts}
\end{figure}

\paragraph{Cross-Validation.} The $2,433$ selected patches are randomly subdivided into $5$ splits, allowing us to perform cross-validation. The official $5$-fold cross-validation scheme used for benchmarking is given in Table \ref{tab:cv}.
In order to avoid heterogeneous folds, 
each fold is constituted of patches taken from all four Sentinel tiles.
We also chose folds with comparable class distributions, as measured by their pairwise Kullback-Leiber divergence. We show the resulting class distribution for each fold in Figure \ref{fig:class_counts}. 
Finally, we prevent adjacent patches from being in different folds to avoid data contamination.
Geo-referencing metadata of the patches and parcels is included in PASTIS, allowing for the constitution of geographically consistent folds to evaluate spatial generalization. However, this is out of the scope of this paper.
\begin{table}[h]
    \centering
    \begin{tabular}{c|ccc}
       Fold  & Train & Val & Test  \\\midrule
        I &1-2-3 & 4 & 5\\
        II &2-3-4 & 5 & 1\\
        III &3-4-5 & 1 & 2\\
        IV &4-5-1 & 2 & 3\\
        V &5-1-2 & 3 & 4\\    \end{tabular}
    \caption{Official $5$-fold cross validation scheme. Each line gives the repartition of the splits into train, validation and test set for each fold. }
    \label{tab:cv}
\end{table}


\paragraph{Temporal Sampling.} The temporal sampling of the sequences in PASTIS is irregular: depending on their location, patches are observed a different number of times and at different intervals.
This is a result of both the orbit schedule of Sentinel-2 and the policy of Sentinel data providers not to process tile observations identified as covered by clouds for more than $90\%$ of the tile's surface.
As this corresponds to the \emph{real world} setting, we decided to leave the SITS as is, and thus to encourage methods that can favourably address this technical challenge. As a result, the proposed SITS are constituted of $33$ to $61$ acquisitions.
In order to assess how our model handles lower sampling frequencies, we limited the number of available acquisitions at inference time\footnote{This can be interpreted as the test set having an increased cloud cover.}, and observed a  drop of performance of $-0.7$, $-2.0$, $-5.5$, and $-14.6$ points of mIoU with $32$, $24$, $16$, and $8$ available dates, respectively.

\paragraph{Clouds Cover. } Even after the automatic filtering of predominantly cloudy acquisitions, some patches are still partially or completely obstructed by cloud cover. We opt to not apply further pre-processing or cloud detection, and produce the raw data in PASTIS. Our reasoning is that an adequate algorithm should be able to learn to deal with such acquisitions. Indeed, robustness to cloud-cover has been experimentally demonstrated for deep learning methods by Ru{\ss}wurm and K\"orner \cite{russwurm2018convolutional, russwurm2020self}.
\section{Implementation Details}
In this section, we detail the exact configuration of our method as well as the competing algorithms evaluated.

\paragraph{Training Details.} 
Across our experiments, we use Adam \cite{kingma2014adam} optimizer with default parameters and a batch size of $4$ sequences. The semantic segmentation experiments use a fixed learning rate of $0.001$ for $100$ epochs. For the panoptic segmentation experiments, we start with a higher learning rate of $0.01$ for $50$ epochs, and decrease it to $0.001$ for the last $50$ epochs.

\paragraph{U-TAE.} 
In \tabref{tab:unet_conf}, we report the width of the feature maps outputted by each level of the U-TAE's encoder and decoder.
In both networks, we use the the same convolutional block shown in \figref{fig:convlblock} and constituted of one $3\times3$ convolution from the input to the output's width, and one residual $3\times3$ convolution. In the encoding branch, we use Group Normalisation with $4$ groups and Batch Normalisation in the decoding branch .

For the temporal encoding, we chose a L-TAE with $16$ heads, and a key-query space of dimension $d_k=4$. We use Group Normalisation with $16$ groups at the input and output of the L-TAE, meaning that that the inputs of each head are layer-normalized.


\begin{figure}[h!]
    \centering
    \includegraphics[width=.7\linewidth, trim=0cm 30cm 42cm 0cm, clip]{images/COnvBlock.pdf}
    \caption{Structure of the convolutional block used in the spatial encoder-decoder network. This block maps a feature map with $D_{in}$ channels to a feature map with $D_{out}$ channels. }
    \label{fig:convlblock}
\end{figure}


\begin{table}[]
    \centering
    \caption{Width of the feature maps outputted at each level of the encoding and decoding branches of the spatial module.}
    \begin{tabular}{ccccc}
      \multicolumn{2}{c}{Encoder}&\phantom{a}&\multicolumn{2}{c}{Decoder}\\\cmidrule{1-2}\cmidrule{4-5}
       $e_1$ & 64 && $d_1$ & 32 \\
        $e_2$ & 64 && $d_2$ & 32 \\
       $e_3$ & 64 && $d_3$ & 64 \\
       $e_4$ & 128 && $d_4$ & 128 \\
    \end{tabular}
    \label{tab:unet_conf}
\end{table}

\paragraph{Recurrent Models.} 
We use the same U-Net architecture for our models and \emph{U-BiConvLSTM} and \emph{U-ConvLSTM}, but simply replace the L-TAE by a ConvLSTM or BiConvLSTM respectively. The hidden state's size of the biConvLSTM is chosen as $32$ in both directions, and $64$ for the convLSTM.
For the recurrent-convolutional methods \emph{ConvLSTM} and \emph{ConvGRU} not using a U-Net, we set hidden sizes of $160$ and $188$ respectively. 


\paragraph{3D-Unet.} 
For this network, we use the official PyTorch implementation of Rustowicz \etal \cite{m2019semantic}. This network is constituted of 
five successive 3D-convolution blocks with spatial down-sampling after the $2$nd and $4$th blocks. Each convolutional block doubles the number of channels of the processed feature maps, and the innermost feature maps have a channel dimension set to $128$. Leaky ReLu and 3D Batch Normalisation are used across the convolutional blocks of this architecture. The sequence of feature maps is averaged along the temporal dimension to produce the final embedding of the input image sequence. In their implementation, the authors used a linear layer to collapse the temporal dimension, yet this was not a valid option for PASTIS as the sequences have highly variable lengths and
the sequence indices do not correspond to the same acquisition date from one sequence to another.


\paragraph{FPN-ConvLSTM.} 
For this architecture, the input sequence of images is first mapped to feature maps of channel dimension $64$ with two consecutive $3\times3$ convolution layers, followed by Group Normalization and ReLu. A $5$-level feature pyramid is then constructed for each date of the sequence
by applying to the feature maps $4$ different $3\times3$ convolution of respective dilation rates $1$, $2$, $4$ and $8$, and computing the spatial average of the feature map.
These $5$ maps are concatenated along the channel dimension, and processed by a ConvLSTM with a hidden state size of $88$. We found it beneficial to use a supplementary convolution before the ConvLSTM to reduce the number of channels of the feature pyramid by a factor $2$.



\paragraph{PaPs module.} In the PaPs module, the saliency and heatmap predictions are obtained with two separate convolutional blocks operating on the high resolution feature map $d_1$ with $32$ channels. These blocks are composed of two convolutional layers
of width $32$ and $1$  respectively. We use Batch Normalisation and ReLu after the first convolution, and a sigmoid after the second.


The $256$-dimensional multi-scale feature vector ($128+64+32+32)$ is mapped to the shape, class and size predictions by three different MLPs described in Table \ref{tab:mlp_config}. The inner layers use Batch Normalisation and ReLu activation.

{The residual CNN used for shape refinement is composed of three convolutional layers : $1 \mapsto 16 \mapsto 16 \mapsto 1$, with ReLu activation and instance normalisation on the first layer only.}
\begin{table}[]
    \caption{Configuration of the four MLPs of PaPs}

    \centering
    \begin{tabular}{l|lc}
        MLP & Layers & Final Layer\\\midrule
        Shape & $256 \mapsto 128 \mapsto S^2$ & - \\
        Size & $256 \mapsto 128 \mapsto 2$ & Softplus\\
        Class & $256 \mapsto 128 \mapsto 64 \mapsto K$ & Softmax\\
    \end{tabular}
    \label{tab:mlp_config}
\end{table}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth, trim=0cm .9cm 0cm 0cm, clip]{images/SemSeg_PerClass.pdf}
    \caption{Per class IoU of the three best performing semantic segmentation models. Our U-TAE outperforms the other two approaches on every classes, and brings noticeable improvement on hard classes such as Mixed cereal and Sorghum. }
    \label{fig:per_class}
\end{figure}


\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{images/ConfMat_UTAE.pdf}
    \caption{Confusion matrix of U-TAE for semantic segmentation on PASTIS. The color of each pixel at line $i$ and column $j$ corresponds to the proportion of samples of the class $i$  that were attributed to the class $j$.}
    \label{fig:confmat}
\end{figure}


\paragraph{Handling Sequences of Variable Lengths.} All models are trained on batches of sequences of variable length.
To facilitate the handling of batches by the GPU, we append all-zeroes images at the end of shorter sequences to match the length of the longer sequence in the batch.
We retain a padding mask to prevent the spatial and temporal encoding of padded values, and to exclude these padded values from temporal averages.


\section{Additional Results}
{In \figref{fig:per_class}, we show the class-wise performance of the three best performing semantic segmentation models, displaying an improvement of U-TAE compared to the other methods across all crop types. We also show on \figref{fig:confmat} the confusion matrix of U-TAE. Unsurprisingly, confusions seem to occur between semantically close classes such as different cereal types, or \textit{Sunflower} and \textit{Fruits, Vegetable, Flower}.  }

In \figref{fig:qualisup}, we present qualitative results illustrating the predicted panoptic and semantic segmentations compared to the ground truth. In particular, we show some failure cases in which thin or visually fragmented parcels are not recovered correctly.

In \figref{fig:sem}, we illustrate the results of the semantic segmentation for our method and three other competing approaches: \emph{3D-Unet}, \emph{U-BiConvLSTM}, and \emph{convGRU}. We show how our multi-scale temporal attention masks allow our predictions to be both pixel-precise and consistent for large parcels.

Finally, we present in \figref{fig:mono} an example of inference using a single image from the sequence. As expected for mono-temporal segmentation, the parcel classification is poor. Furthermore, we show a case of a border that is essentially invisible on a single image, but that our full model is able to detect using the entire sequence of satellite images. 


\section*{Acknowledgments}
The satellite images used in PASTIS were gathered from  \href{www.theia.land.fr}{THEIA}: 
``\emph{Value-added data processed by the CNES for the Theia data cluster using Copernicus data.
The treatments use algorithms developed by Theiaâ€™s Scientific Expertise Centres.}''
The annotations of PASTIS were taken from the French \href{https://www.data.gouv.fr/en/datasets/registre-parcellaire-graphique-rpg-contours-des-parcelles-et-ilots-culturaux-et-leur-groupe-de-cultures-majoritaire/}{LPIS} produced
 by IGN, the French mapping agency.
This work was partly supported by \href{https://www.asp-public.fr}{ASP}, the French Payment Agency.


\begin{figure*}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/Fig10_v2/Fig4_1_9.pdf}
\label{fig:qualisup:rgb}
        \end{subfigure}
    \hfill
        \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{images/Fig10_v2/Fig4_2_9.pdf}
\label{fig:qualisup:gt}
        \end{subfigure}
    \hfill
        \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{images/Fig10_v2/Fig4_3_9.pdf}
\label{fig:qualisup:pan}
        \end{subfigure}
    \hfill
        \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{images/Fig10_v2/Fig4_4_9.pdf}
\label{fig:qualisup:seg}
        \end{subfigure}
    \vfill
    \vspace{-.4cm}
        \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/fig5_v2/Fig4_1_10.pdf}
\label{fig:qualisup2:rgb}
        \end{subfigure}
    \hfill
        \begin{subfigure}{0.24\textwidth}
        \centering
        \begin{tikzpicture}
         \node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=\textwidth]{images/fig5_v2/Fig4_2_10.pdf}};
        \begin{scope}[x={(image.south east)},y={(image.north west)}]
        \draw[green,ultra thick] (0.6,0.45) circle (0.18);
        \end{scope}
        \end{tikzpicture}   
\label{fig:qualisup2:gt}
        \end{subfigure}
    \hfill
        \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{images/fig5_v2/Fig4_3_10.pdf}
        \label{fig:qualisup2:pan}
        \end{subfigure}
    \hfill
        \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{images/fig5_v2/Fig4_4_10.pdf}
        \label{fig:qualisup2:seg}
        \end{subfigure}
        
    \vfill
    \vspace{-.4cm}
        \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/Fig10_v2/Fig4_1_1.pdf}
\label{fig:qualisup3:rgb}
        \end{subfigure}
    \hfill
        \begin{subfigure}{0.24\textwidth}
        \centering
        \begin{tikzpicture}
         \node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=\textwidth]{images/Fig10_v2/Fig4_2_1.pdf}};
        \begin{scope}[x={(image.south east)},y={(image.north west)}]
        \draw[cyan ,ultra thick] (0.55,0.3) circle (0.12);
        \end{scope}
        \end{tikzpicture}
\label{fig:qualisup3:gt}
        \end{subfigure}
    \hfill
        \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{images/Fig10_v2/Fig4_3_1.pdf}
        \label{fig:qualisup3:pan}
        \end{subfigure}
    \hfill
        \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{images/Fig10_v2/Fig4_4_1.pdf}
        \label{fig:qualisup3:seg}
        \end{subfigure}
    
    \vfill 
    \vspace{-.4cm}

        \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/Fig10_v2/Fig4_1_7.pdf}
        \caption{Image from the sequence.}
        \label{fig:qualisup4:rgb}
        \end{subfigure}
    \hfill
        \begin{subfigure}{0.24\textwidth}
        \begin{tikzpicture}
         \node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=\textwidth]{images/Fig10_v2/Fig4_2_7.pdf}};
        \begin{scope}[x={(image.south east)},y={(image.north west)}]
        \draw[red,ultra thick] (0.7,0.8) circle (0.18);
        \end{scope}
        \end{tikzpicture}
        
\caption{Panoptic annotation.}
        \label{fig:qualisup4:gt}
        \end{subfigure}
    \hfill
        \begin{subfigure}{0.24\textwidth}
        \begin{tikzpicture}
         \node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=\textwidth]{images/Fig10_v2/Fig4_3_7.pdf}};
        \begin{scope}[x={(image.south east)},y={(image.north west)}]
        \draw[blue ,ultra thick] (0.4,0.4) circle (0.1);
        \end{scope}
        \end{tikzpicture}
        
\caption{Panoptic segmentation.}
        \label{fig:qualisup4:pan}
        \end{subfigure}
    \hfill
        \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=\textwidth]{images/Fig10_v2/Fig4_4_7.pdf}
        \caption{Semantic segmentation.}
        \label{fig:qualisup4:seg}
        \end{subfigure}
\caption{\textbf{Qualitative  Panoptic Segmentation Results.} We represent a single image from the sequence using the RGB channels (\subref{fig:qualisup4:rgb}), and whose ground truth parcel's limit and types are known (\subref{fig:qualisup4:gt}). We then represent the parcels predicted by our panoptic segmentation module (\subref{fig:qualisup4:pan}), and the pixelwise prediction of our semantic segmentation module (\subref{fig:qualisup4:seg}). See \figref{fig:nomenc} for the color to crop type correspondence. 
We highlight with a green circle \protect\tikz \protect\node[circle, thick, draw = green!90!black, fill = none, scale = 0.7] {}; a large, fragmented parcel declared as one single field. This leads to predictions with low confidence and a low panoptic quality.
Conversely, the cyan circle \protect\tikz \protect\node[circle, thick, draw = cyan, fill = none, scale = 0.7] {}; highlights such fragmented parcel which is correctly predicted as a single instance. This suggests that our network is able to use the temporal dynamics to recover ambiguous borders.
We highlight a failure case with the red circle \protect\tikz \protect\node[circle, thick, draw = red, fill = none, scale = 0.7] {};, for which many thin parcels are not properly detected, resulting in a low panoptic quality. {We observe that the semantic segmentation model struggles as well for such thin parcels.}  Finally, we 
highlight with a blue circle \protect\tikz \protect\node[circle, thick, draw = blue, fill = none, scale = 0.7] {}; an example in which the panoptic prediction is superior to the semantic segmentation, indicating that detecting parcels' boundaries and extent can be informative for their classification.
}
\label{fig:qualisup}
\end{figure*}

\begin{figure*}

\begin{subfigure}{0.16\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/quali_sem/SV_item1_RGB.pdf}
        \end{subfigure}
    \hfill
        \begin{subfigure}{0.16\textwidth}
        \includegraphics[width=\textwidth]{images/quali_sem/SV_item1_GT.pdf}
        \end{subfigure}
    \hfill
        \begin{subfigure}{0.16\textwidth}
        \includegraphics[width=\textwidth]{images/quali_sem/SV_item1_U-TAE.pdf}
        \end{subfigure}
    \hfill
        \begin{subfigure}{0.16\textwidth}
        \includegraphics[width=\textwidth]{images/quali_sem/SV_item1_3d-Unet.pdf}
        \end{subfigure}
    \hfill
        \begin{subfigure}{0.16\textwidth}
        \includegraphics[width=\textwidth]{images/quali_sem/SV_item1_U-BiConv.pdf}
        \end{subfigure}
\hfill
        \begin{subfigure}{0.16\textwidth}
        \includegraphics[width=\textwidth]{images/quali_sem/SV_item1_ConvGRU.pdf}
        \end{subfigure}
\vfill
\begin{subfigure}{0.16\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/quali_sem/SV_item2_RGB.pdf}
        \end{subfigure}
    \hfill
        \begin{subfigure}{0.16\textwidth}
        \includegraphics[width=\textwidth]{images/quali_sem/SV_item2_GT.pdf}
        \end{subfigure}
    \hfill
        \begin{subfigure}{0.16\textwidth}
        \includegraphics[width=\textwidth, trim=0cm 4cm 0cm 4cm, clip]{images/quali_sem/SV_item2_U-TAE.pdf}
        \end{subfigure}
    \hfill
        \begin{subfigure}{0.16\textwidth}
        \includegraphics[width=\textwidth]{images/quali_sem/SV_item2_3d-Unet.pdf}
        \end{subfigure}
    \hfill
        \begin{subfigure}{0.16\textwidth}
        \includegraphics[width=\textwidth]{images/quali_sem/SV_item2_U-BiConv.pdf}
        \end{subfigure}
\hfill
        \begin{subfigure}{0.16\textwidth}
        \includegraphics[width=\textwidth]{images/quali_sem/SV_item2_ConvGRU.pdf}
        \end{subfigure}
\vfill
\begin{subfigure}{0.16\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/quali_sem/SV_item3_RGB.pdf}
        \end{subfigure}
    \hfill
        \begin{subfigure}{0.16\textwidth}
        \includegraphics[width=\textwidth]{images/quali_sem/SV_item3_GT.pdf}
        \end{subfigure}
    \hfill
        \begin{subfigure}{0.16\textwidth}
        \includegraphics[width=\textwidth, trim=0cm 4cm 0cm 4cm, clip]{images/quali_sem/SV_item3_U-TAE.pdf}
        \end{subfigure}
    \hfill
        \begin{subfigure}{0.16\textwidth}
        \includegraphics[width=\textwidth]{images/quali_sem/SV_item3_3d-Unet.pdf}
        \end{subfigure}
    \hfill
        \begin{subfigure}{0.16\textwidth}
        \includegraphics[width=\textwidth]{images/quali_sem/SV_item3_U-BiConv.pdf}
        \end{subfigure}
\hfill
        \begin{subfigure}{0.16\textwidth}
        \includegraphics[width=\textwidth, trim=0cm 4cm 0cm 4cm, clip]{images/quali_sem/SV_item3_ConvGRU2.pdf}
        \end{subfigure}
\vfill
\begin{subfigure}{0.16\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/quali_sem/SV_item4_RGB.pdf}
        \caption{Single image.}
        \label{fig:sem:rgb}
        \end{subfigure}
    \hfill
        \begin{subfigure}{0.16\textwidth}
        \includegraphics[width=\textwidth]{images/quali_sem/SV_item4_GT.pdf}
        \caption{Annotation.}
        \label{fig:sem:gt}
        \end{subfigure}
    \hfill
        \begin{subfigure}{0.16\textwidth}
        \includegraphics[width=\textwidth, trim=0cm 4cm 0cm 4cm, clip]{images/quali_sem/SV_item4_U-TAE.pdf}
        \caption{U-TAE.}
        \label{fig:sem:ours}
        \end{subfigure}
    \hfill
        \begin{subfigure}{0.16\textwidth}
        \includegraphics[width=\textwidth]{images/quali_sem/SV_item4_3d-Unet.pdf}
        \caption{3D-Unet.}
        \label{fig:sem:3d}
        \end{subfigure}
    \hfill
        \begin{subfigure}{0.16\textwidth}
        \includegraphics[width=\textwidth]{images/quali_sem/SV_item4_U-BiConv.pdf}
        \caption{U-BiConvLSTM.}
        \label{fig:sem:ubc}

        \end{subfigure}
\hfill
        \begin{subfigure}{0.16\textwidth}
        \includegraphics[width=\textwidth, trim=0cm 4cm 0cm 4cm, clip]{images/quali_sem/SV_item4_ConvGRU2.pdf}
        \caption{ConvGRU.}
        \label{fig:sem:cgru}
        \end{subfigure}

\caption{\textbf{Qualitative Semantic Segmentation Results.}
We represent a single image from the sequence using the RGB channels (\subref{fig:sem:rgb}), and whose ground truth parcel's limit and crop type are known (\subref{fig:sem:gt}). We then represent the pixelwise prediction from our approach (\subref{fig:sem:ours}), and for three other competing algorithms (\subref{fig:sem:3d}-\subref{fig:sem:cgru}). 
The different predictions shown on this figure illustrate the importance of the resolution at which temporal encoding is performed. ConvGRU applies a recurrent-convolutional network at the highest resolution, which results in predictions with high spatial variability. As a consequence, the prediction over large parcels are inconsistent (blue circles \protect\tikz \protect\node[circle, thick, draw = blue, fill = none, scale = 0.7] {};). Conversely, U-BiConvLSTM applies temporal encoding to feature maps with a larger receptive field, resulting in more spatially consistent predictions. Yet, this architecture often fails to retrieve small or thin parcels. In contrast, our U-TAE produces spatially consistent predictions on large parcels, while being able to retrieve such small parcels (green circles \protect\tikz \protect\node[circle, thick, draw = green!90!black, fill = none, scale = 0.7] {};). 3D-Unet also uses temporal encoding at different resolution levels, yet fails to recover these small parcels.
}
\label{fig:sem}
\end{figure*}

\begin{figure*}[ht!]
\begin{subfigure}{.24\textwidth}
\includegraphics[width=\textwidth]{images/Fig12_v2/Fig12_1_1.pdf}
\caption{Single observation.}
\label{fig:mono:rgb}
\end{subfigure}
\hfill
\begin{subfigure}{.24\textwidth}
\includegraphics[width=\textwidth]{images/Fig12_v2/Fig12_2_1.pdf}
\caption{Panoptic annotation.}
\label{fig:mono:gt}

\end{subfigure}
\hfill
\begin{subfigure}{.24\textwidth}
\includegraphics[width=\textwidth]{images/Fig12_v2/Fig12_4_1.pdf}
\caption{Mono-temporal prediction.}
\label{fig:mono:pred}
\end{subfigure}
\hfill
\begin{subfigure}{.24\textwidth}

\begin{tikzpicture}
 \node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=\textwidth]{images/Fig12_v2/Fig12_3_1.pdf}};
\begin{scope}[x={(image.south east)},y={(image.north west)}]
\draw[cyan ,ultra thick] (0.15,0.75) circle (0.1);
\end{scope}
\end{tikzpicture}

\caption{Multi-temporal prediction.}
\label{fig:mono:multi}
\end{subfigure}
\caption{\textbf{Mono-temporal Panoptic Segmentation.} 
We train our mono-temporal model on a single image (\subref{fig:mono:rgb}), with panoptic annotation (\subref{fig:mono:gt}).
We then compare the results of the mono-temporal model in (\subref{fig:mono:pred}) with the results our full model when performing inference on the full length sequence (\subref{fig:mono:multi}) from which the single patch (\subref{fig:mono:rgb}) is drawn.
First, we observe that many parcels are not detected by the mono-temporal model, indicating an overall low predicted {quality}.
Second, we can see that most detected parcels are misclassified by the mono-temporal model. This is in accordance with the low semantic segmentation score of the mono-temporal model: crop types are hard to distinguish from a single observation.
Last, adjacent parcels with no clear borders are predicted as a single parcel, when the multi-temporal model is able to differentiate between the two parcels (cyan circle \protect\tikz \protect\node[circle, thick, draw = cyan, fill = none, scale = 0.7] {};). 
This illustrates how using SITS instead of single images can help resolve ambiguous parcels delineation.
}
\label{fig:mono}
\end{figure*}

