\pdfoutput=1


\documentclass[11pt]{article}

\usepackage{acl}

\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{graphicx}
\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\usepackage{microtype}
\usepackage{arydshln}
\usepackage{mathabx}
\usepackage{fontawesome}

\newcommand\scalemath[2]{\scalebox{#1}{\mbox{\ensuremath{\displaystyle #2}}}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\myparagraph}[1]{\vspace{-5pt} \paragraph{#1}}


\title{Modeling Label Correlations for Ultra-Fine Entity Typing with \\ Neural Pairwise Conditional Random Field}




\makeatletter
\def\@fnsymbol#1{\ensuremath{\ifcase#1\or \dagger\or *\or \ddagger\or
   \mathsection\or \mathparagraph\or \|\or **\or \dagger\dagger
   \or \ddagger\ddagger \else\@ctrerr\fi}}
\makeatother

\newcommand{\shtu}{\textsuperscript{\faSunO}}
\newcommand{\damo}{\textsuperscript{\faStarO}}
\newcommand{\github}{\faGithub}
\newcommand{\code}{\url{http://github.com/modelscope/adaseq/tree/master/examples/NPCRF}}

\author{
    \textbf{Chengyue Jiang}\thanks{This work was done during Chengyue Jiang's internship at DAMO Academy, Alibaba Group. } ,
    \textbf{Yong Jiang}\damo,
    \textbf{Weiqi Wu},
    \textbf{Pengjun Xie}\damo,
    \textbf{Kewei Tu}\thanks{Yong Jiang and Kewei Tu are corresponding authors.} \\
\damo DAMO Academy, Alibaba Group, China \\
    \texttt{\{jiangchy1997,kewei.tu.nlp\}@gmail.com;}
    \texttt{wuwq1022@foxmail.com;}\\
    \texttt{\{yongjiang.jy,chengchen.xpj\}@alibaba-inc.com }
}


\begin{document}
\maketitle

\begin{abstract}
    Ultra-fine entity typing (UFET) aims to predict a wide range of type phrases that correctly describe the categories of a given entity mention in a sentence. 
    Most recent works infer each entity type independently, ignoring the correlations between types, e.g., when an entity is inferred as a {\it president}, it should also be a {\it politician} and a {\it leader}.
    To this end, we use an undirected graphical model called pairwise conditional random field (PCRF) to formulate the UFET problem, in which the type variables are not only unarily influenced by the input but also pairwisely relate to all the other type variables.
    We use various modern backbones for entity typing to compute unary potentials, and derive pairwise potentials from type phrase representations that both capture prior semantic information and facilitate accelerated inference. We use mean-field variational inference for efficient type inference on very large type sets and unfold it as a neural network module to enable end-to-end training. 
    Experiments on UFET show that the Neural-PCRF consistently outperforms its backbones with little cost and results in a competitive performance against cross-encoder based SOTA while being \emph{thousands of times} faster. We also find Neural-PCRF effective on a widely used fine-grained entity typing dataset with a smaller type set. We pack Neural-PCRF as a network module that can be plugged onto multi-label type classifiers with ease and release it in \code.
\end{abstract}

  \section{Introduction}
Entity typing assigns semantic types to entities mentioned in the text. The extracted type information has a wide range of applications. It acts as a primitive for information extraction \cite{2phaseNER} and spoken language understanding \cite{coucke2018snips}, and assists in more complicated tasks such as machine reading comprehension \cite{joshi2017triviaqa} and semantic parsing \cite{yavuz-etal-2016-improving}.
During its long history of development, the granularity of the type set for entity typing and recognition changes from coarse (sized less than 20) \cite{conll03,06ontonotes}, to fine (sized around 100) \cite{bbn,figer,ontonotes,ding-etal-2021-nerd}, to ultra-fine and free-form (sized 10k) \cite{ufet}. The expansion of the type set reveals the diversity of real-world entity categories and the importance of a finer-grained understanding of entities in applications \cite{ufet}.

The increasing number of types results in difficulties in predicting correct types, so a better understanding of the entity types and their correlations is needed. Most previous works solve an -type entity typing problem as  independent binary classifications \cite{figer,ontonotes,ufet,onoe-durrett-2019-learning,ding2021prompt,dfet,lite}. However, types are highly correlated and hence they should be predicted in a joint manner. As an example, when `{\em Joe Biden}' is inferred as a {\it president}, it should also be inferred as a {\it politician}, but not a {\it science fiction}. Type correlation is partially specified in fine-grained entity typing (FET) datasets by a two or three-level type hierarchy, and is commonly utilized by a hierarchy-aware objective function \cite{ren2016label, jin-etal-2019-fine, xu-barbosa-2018-neural}. However, type hierarchies cannot encode type relationships beyond strict containment, such as similarity and mutual exclusion \cite{onoe-etal-2021-modeling}, and could be noisily defined \cite{wu2019modeling} or even unavailable (in ultra-fine entity typing (UFET)). Many recent works handle these problems by embedding types and mentions into special spaces such as the hyperbolic space \cite{lopez-strube-2020-fully} or box space \cite{onoe-etal-2021-modeling} that can be trained to latently encode type correlations without a hierarchy. Although these methods are expressive for modeling type correlations, they are constrained by these special spaces and thus incapable of being combined with modern entity typing backbones such as prompt learning \cite{ding2021prompt} and cross-encoder \cite{lite}, and cannot integrate prior type semantics.

In this paper, we present an efficient method that expressively models type correlations while being backbone-agnostic. We formulate the UFET and FET problems under a classical undirected graphical model (UGM) \cite{pgm} called pairwise conditional random field (PCRF) \cite{pcrf2005}. In PCRF, types are binary variables that are not only unarily influenced by the input but also pairwisely relate to all the other type variables. We formulate the unary potentials using the type logits provided by any modern backbone such as prompt learning. To compose the pairwise potentials sized  ( is the number of types, 10k for UFET), we use matrix decomposition, which is efficient and able to utilize prior type semantic knowledge from word embeddings \cite{glove}. Exact inference on such a large and dense UGM is intractable, and therefore we apply mean-field variational inference (MFVI) for approximate inference. Inspired by \citet{crfrnn}, we unfold the MFVI as a recurrent neural network module and connect it with the unary potential backbone to enable end-to-end training and inference. We call our method Neural-PCRF (NPCRF). Experiments on UFET show that NPCRF consistently outperforms the backbones with negligible additional cost, and results in a strong performance against cross-encoder based SOTA models while being \emph{thousands of times} faster. We also found NPCRF effective on a widely used fine-grained entity typing dataset with a smaller type set. We pack NPCRF as a network module that can be plugged onto multi-label type classifiers with ease and release it in \code.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.98\textwidth]{img/all.pdf}
    \caption{(a) Our pairwise CRF (PCRF) of ultra-fine entity typing.  is the entity mention and its context. (b) The neural network architecture corresponding to the Neural-PCRF.}
    \label{fig:all}
\end{figure*} \section{Related Work}
\subsection{Fine-grained and Ultra-Fine Entity Typing}
Fine-grained \cite{ontonotes,figer,bbn} (FET) and ultra-fine \cite{ufet} entity typing (UFET) share common challenges. Researchers put lots of effort into better utilizing the distantly and manually labeled training data, including rule-based denoising \cite{ontonotes}, multi-round automatic denoising \cite{onoe-durrett-2019-learning,dfet,mlmet}, and partial label embedding \cite{ren2016label}. Some recent works introduce better entity typing backbones compared with commonly used multi-label classifiers \cite{ontonotes,ufet,onoe-durrett-2019-learning}, such as prompt learning \cite{ding2021prompt}, cross-encoders with indirect supervision from natural language inference \cite{lite} and using a graph neural network to enhance mention representations by sibling mentions for FET \cite{chen-etal-2022-learning-sibling}. Another line of work concentrates on modeling the given or latent type correlations in entity typing, which is more relevant to our work. \citet{ren2016label, jin-etal-2019-fine, xu-barbosa-2018-neural} leverage a given type hierarchy through a hierarchy-aware loss for FET. However, in UFET, the type hierarchy is not given. Therefore various special type embeddings are designed to learn the latent type correlation during training.
\citet{xiong-etal-2019-imposing} utilize graph embedding of a label co-occurrence graph counted from training data, \citet{lopez-strube-2020-fully} embeds entity mentions, contexts and labels into the hyperbolic space that can latently encodes tree structures, and \citet{onoe-etal-2021-modeling} embeds those into the box space that encodes label dependency through the topology of hyperrectangles. Unlike them, we treat types as random variables under an undirected probabilistic graphical model, and only require the type logits, therefore our neural PCRF is backbone-agnostic. \citet{liu-etal-2021-fine} propose a label reasoning network (LRN) that generates types in an auto-regressive manner, while our method decodes types in parallel and can be combined with prompt-based backbones which LRN cannot do.

\subsection{Pairwise Conditional Random Field}
Pairwise conditional random field \cite{pcrf2005} is a classical undirected graphical model proposed for modeling label correlations. In the deep learning era, PCRF was first found effective when combined with a convolutional neural network (CNN) \cite{cnn} for semantic segmentation in computer vision \cite{crfrnn,pcrf1,pcrf2,pcrf3,fw2021}, in which it was used to encourage adjacent pixels being segmented together. In contrast to its popularity in computer vision, PCRF is much less explored in natural language processing.
\citet{pcrf2005} and \citet{pcrfT} apply PCRF on an n-gram feature based non-neural sentence classifier with up to 203 classes. Besides the difference in tasks, numbers of classes and backbones, our method is different from theirs in two main aspects: (1) We unfold mean-field approximate inference of PCRF as a recurrent neural network module for efficient end-to-end training and inference, while they use the `supported inference method' \cite{pcrf2005} which is intractable for large type sets and incompatible with neural backbones (2) We design the parameterization of the pairwise potential function based on the technique of matrix decomposition to accelerate training and inference, and encode type semantics which is important for entity typing \cite{lite} and sentence classification \cite{mueller2022label}. Our parameterization also conforms to intrinsic properties of pairwise potentials (explained in Sec. \ref{sec:pairwise}). \citet{hu-etal-2020-investigation} investigates different potential function variants for linear-chain CRFs for sequence labeling, while we design pairwise potential functions for PCRF. \section{Method}
\subsection{Problem Definition} 
Entity typing datasets consist of  entity mentions  with their corresponding context sentences : , and a type set  of size . 
The task of entity typing is to predict the types  of the entity mention  in the given context , where  is a subset of the type set. The number of gold types  could be larger than one in most entity typing datasets. The average number of gold types per instance  and the size of the type set  vary in different datasets.

\subsection{PCRF for Entity Typing}
We first describe our pairwise conditional random field for the entity typing \cite{pcrf2005}, as shown in Fig. \ref{fig:all}(a).  denotes a data point .  denotes the binary random variable for the -th type in type set . The type variables  are unarily connected to the input, model how likely a type is given the input, and pairwisely connect a type variable with all other type variables to model type correlations.
Let  be an assignment of all the type variables. The probability of  given a data point  under a conditional random field can be factorized as:

where  are scoring functions for unary and pairwise edges, and  is the partition function for normalization.

\subsection{Unary Potential Function}
\label{sec:backbone}
As illustrated in Figure \ref{fig:all}(b), we parameterize the log unary potential function  using modern entity typing backbones based on pretrained language models (PLM)\footnote{We use PLMs from \url{https://huggingface.co/}}.We introduce two of the backbones below. 
\paragraph{Multi-label Classifier (MLC)} Given a data point , we formulate the input as:

and feed it into RoBERTa-large \cite{liu2019roberta} to obtain the  embedding . Then we obtain the unary type scores as follows.

\paragraph{Prompt Learning (PL)} We adopt prompt learning for entity typing \cite{ding2021prompt,dfet} as our second backbone. We feed:

into Bert-cased \cite{devlin2018bert}, where  are trainable soft prompt tokens. We use a verbalizer  to map types to subwords,  denotes the subwords corresponding to type , e.g., . We obtain the unary score of choosing type  using the averaged masked language model logits of :

Similarly, we set .

\subsection{Pairwise Potential Function}
\label{sec:pairwise}
\begin{table}[t]
\renewcommand\arraystretch{1.15}
\centering
\scalebox{0.75}{
\begin{tabular}{ccc} \toprule
   potential               &       encode          &    property \\ \midrule
& co-occurrence &  \\
 & co-absence   & \\
 & co-exclusion &  \\
 & co-exclusion &  \\ \bottomrule
\end{tabular}}
\caption{Properties of the four log potential matrices.}
\label{tab:pairwise}
\end{table}

Pairwise log potential  encoding type correlations should naturally satisfy the properties shown in Table~\ref{tab:pairwise}.
Specifically,  and  are symmetric matrices and encode co-occurrence and co-absence respectively, while  and are asymmetric ( e.g., ``not a person but a president''  ``not a president but a person''). Directly parameterizing these 4 potential matrices \cite{pcrf2005,pcrfT} ignores these intrinsic properties and results in an unbearable number of model parameters for datasets with a large type set (e.g., 400M parameters for 10331-type UFET, which is more than Bert-large).

To tackle these problems, we parameterize the pairwise potential based on matrix rank decomposition, i.e., we represent each  matrix as the product of two  matrices, where  is the number of ranks.
Crucially, we use pretrained word embedding to derive the two  matrices, so that they can encode type semantics.
Specifically, we obtain a type embedding matrix  based on 300-dimension GloVe embedding \cite{glove}. For a type phrase consisting of multiple words (e.g., \emph{``living\_thing''}), we take the average of the word embeddings. We then transform  into two embedding spaces encoding \emph{``occurrence''} and \emph{``absence''} respectively using two feed-forward networks.


To enforce the intrinsic properties of the four matrices, we parameterize them as follows:

The negative signs in defining  ensure that they encode co-exclusion, not similarity. As we will show in the next subsection, we do not need to actually recover these large matrices during inference, leading to lower computational complexity.

\subsection{Mean-field Variational Inference}
\label{sec:mfvi}
We aim to infer the best type set given the input.

The MAP (maximum-a-posteriori) inference over this large and dense graph is NP-hard. We instead use mean-field variational inference (MFVI) to approximately infer the best assignment . MFVI approximates the true posterior distribution  by a variational distribution  that can be factorized into independent marginals \cite{wainwright2008graphical}. It iteratively minimizes the KL-divergence between  and . We initialize the  distribution by unary scores produced by backbones:

 and derive the MFVI iteration as follows:

We rewrite the formulas in the vector form with our parametrization.  denotes the unary score vector of  for all , and we define  similarly.
 are vectors of the  distributions at the -th iteration.


Note that by following these update formulas, we do not need to recover the  matrices, and hence the time complexity of each iteration is . Since the iteration number  is typically a small constant (), the computational complexity of the whole MFVI is . The predicted type set of  is obtained by:

We follow the treatment of MFVI as entropy-regularized Frank-Wolfe \cite{le2021regularized} and introduce another hyper-parameter  to control the step size of each update. Let ,

\subsection{Unfolding Mean-field Variational Inference as a Recurrent Neural Network}
We follow \citet{crfrnn} and treat the mean-field variational inference procedure with a fixed number of iterations
 as a recurrent neural network (RNN) parameterized by the pairwise potential functions. As shown in the top part of Figure \ref{fig:all}(b), the initial hidden state of the RNN is the type distribution  produced by the unary logits, and the final hidden states  after  iterations are used for end-to-end training and inference.
 
 \subsection{Training Objective and Optimization} 
We use the Binary Cross Entropy loss for training:

 is the loss of each instance ,  is the gold annotation of  for type . We follow previous works \cite{ufet,mlmet} and use  as a weight for the loss of positive types. 
We train the pretrained language model, label embedding matrix  and FFNs.
We use AdamW \cite{loshchilov2018fixing} for optimization. \section{Experiment}
\subsection{Datasets and Experimental Settings}
We mainly evaluate our NPCRF method on the ultra-fine entity typing dataset: UFET \cite{ufet}. We also conduct experiments on the augmented version \cite{ufet} of OntoNotes \cite{ontonotes} to examine if our method also works for datasets with smaller type sets and fewer gold types per instance. We show the dataset statistics in Table \ref{tab:stat}. Note that UFET also provides 25M distantly labeled training data extracted by linking to KB and parsing. We follow recent works \cite{dfet,liu-etal-2021-fine} and only use the manually annotated 2k data for training. We use standard metrics for evaluation: for UFET, we report macro-averaged precision (P), recall (R), and F1; for OntoNotes, we report macro-averaged and micro-averaged F1. We run experiments three times and report the average results.

\begin{table}[h]
\scalebox{0.8}{
\centering
\begin{tabular}{ccccc} 
\toprule
dataset &       &  & train/dev/test & hierarchy \\ \midrule
UFET    & 10331       & 5.4   & 2k/2k/2k       & No        \\
OntoNotes  & 89          & 1.5   &   0.8M/2k/9k             & 3-layer       \\ \bottomrule
\end{tabular}}
\caption{ denotes the size of the type set,  denotes the average number of gold types per instance.}
\label{tab:stat}
\end{table}

\subsection{Baseline Methods}
\begin{table}[t]
\centering
\scalebox{0.8}{
\begin{tabular}{llll} \toprule
\bf \textsc{Model}                & \bf \textsc{P}    & \bf \textsc{R}    &  \bf \textsc{F1} \\ \midrule
\multicolumn{4}{l}{\emph{non-PLM model for reference}}        \\
{\bf \textsc{UFET-BiLSTM}}  \cite{ufet}     & 48.1 & 23.2 & 31.3 \\
{\bf \textsc{LabelGCN}} \cite{xiong-etal-2019-imposing}        & 50.3 & 29.2 & 36.9 \\ \hline \midrule
\multicolumn{4}{l}{\emph{PLM-based models}}        \\
{\bf \textsc{LDET}} \cite{onoe-durrett-2019-learning}          & 51.5 & 33.0 & 40.1 \\
{\bf \textsc{Box4Types}} \cite{onoe-etal-2021-modeling}  & 52.8 & 38.8 & 44.8 \\
{\bf \textsc{LRN} }  {\cite{liu-etal-2021-fine}}                & 54.5 & 38.9 & 45.4 \\
{\bf \textsc{MLMET}}  {\cite{mlmet}}    & 53.6 & 45.3 & 49.1 \\
{\bf \textsc{LITE+L}}  \cite{lite}             & 48.7 & 45.8 & 47.2 \\
{\bf \textsc{DFET}}    \cite{dfet}             & 55.6 & 44.7 & 49.5 \\
{\bf \textsc{LITE+NLI+L}} \cite{lite}  & 52.4 & 48.9 & {\bf 50.6} \\ \hline \midrule
\multicolumn{4}{l}{\emph{PLM-based models w/ or w/o NPCRF}}        \\
\bf \textsc{MLC-RoBerta}                  & 47.8 & 40.4 & 43.8 \\
\bf \textsc{MLC-RoBerta w/ NPCRF}             & 48.7 & 45.5 & 47.0  \\ [0.5ex] \hdashline \-2ex]
{\bf \textsc{PL-Bert-base}}  \cite{ding2021prompt}       & 57.8 & 40.7 & 47.7 \\
\bf \textsc{PL-Bert-base w/ NPCRF}    & 52.1 & 47.5 & 49.7  \\ [0.5ex] \hdashline \-2ex]
\bf \textsc{LabelGCN-RoBerta}       & 85.5 & 80.2 &  \\
\bf \textsc{LabelGCN-RoBerta w/ NPCRF}    & 86.2 & 81.3 &   \\ [0.5ex] \hdashline \0.5ex] \hdashline \-1.5ex]
\bf \textsc{LITE} & 20.41 & 0.03 \\ \bottomrule
\end{tabular}}
\caption{Comparison of training and inference speed of different methods in Table \ref{tab:ufet}.}
\label{tab:speed}
\end{table}
 \section{Conclusion and Future Work}
We propose {\bf \textsc{NPCRF}}, a method that efficiently models type correlation for ultra-fine and fine-grained entity typing, and is applicable to various entity typing backbones. In {\bf \textsc{NPCRF}}, the unary potential is formulated as the type logits of modern UFET backbones, the pairwise potentials are derived from type phrase representations that both capture prior semantic information and facilitate accelerated inference. We unfold mean-field variational inference of {\bf \textsc{NPCRF}} as a neural network for end-to-end training and inference. We find our method consistently outperforms its backbone, and reach competitive performance against very recent baselines on UFET and FET.  {\bf \textsc{NPCRF}} is efficient and require low additional computation costs. For future work, modeling higher-order label correlations, injecting prior knowledge into pairwise potentials, and extending {\bf \textsc{NPCRF}} to other tasks are worth exploring. \section*{Limitations}
As shown in the experiments, the main limitation of {\bf \textsc{NPCRF}} is that it has less positive effect on tasks that do not require understanding type correlation (e.g., tasks with small label sets and a low number of gold labels per instance), and on models that already model label semantics quite well (e.g., prompt-based methods). Another limitation of {\bf \textsc{NPCRF}} is that, although it can be combined with many backbones, there still exist some backbones that cannot directly use {\bf \textsc{NPCRF}}, such as models that generate types one by one in an auto-regressive way (e.g., LRN) and models that cannot efficiently compute label logits (e.g., LITE). \section{Acknowledgement}
This work was supported by the National Natural Science Foundation of China (61976139) and by Alibaba Group through Alibaba Innovative Research Program. 
\bibliography{main}
\bibliographystyle{acl_natbib}

\appendix


\section{Example Appendix}
\label{sec:appendix}
 
 \subsection{Pairwise Potentials}
 \label{app:potential}
We recover the learned log potentials  using Eq. \ref{eq:2}. For visualization, we pick three sets of types, types in the same set are relevant, and types between different set are irrelevant, and draw the heatmap of log potentials of them in Fig. \ref{fig:pairwise}. The figure shows that: (1) the learned potentials obey the intrinsic properties,  are symmetric and different from each other,  and they are asymmetric. (2)  do encode different levels of similarity, the scores representing the co-absence and co-occurrence of types in the same set are higher than types in different set. (3) The  and  obey the intrinsic properties and cells with high scores usually represent the co-exclusion of types from different sets. In general,  and  are less interpretable compared to  and .

 \begin{figure*}[h]
     \centering
     \scalebox{0.4}{
     \includegraphics{img/potential1.pdf}}
     \caption{Visualization of pairwise potentials. The horizontal axis is for , and the vertical axis is for , for example, .}
     \label{fig:pairwise}
 \end{figure*}
  

\end{document}