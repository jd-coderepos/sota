\documentclass{llncs}

\sloppy

\newcommand\new{\color{red}}


\newcommand\remove[1]{}


\newcommand\ttsays[1]{{\bf\color{red} TT: #1}}





\usepackage{amsmath, amssymb, amsfonts}
\usepackage{paralist}
\usepackage{alltt}
\usepackage{wrapfig}
\usepackage{stmaryrd}

\usepackage{caption}
\usepackage{subcaption}
\captionsetup{compatibility=false}
\usepackage{floatrow}
\usepackage{calc}

\usepackage{algorithm}
\usepackage[noend]{algorithmic}

\usepackage{tikz}
\usetikzlibrary{matrix}
\usetikzlibrary{fit}
\usetikzlibrary{automata}
\usetikzlibrary{shapes}

\usepackage{pbox}
\usepackage{mathtools}
\usepackage{footmisc}
\usepackage{xspace}
\usepackage{color}


\newcommand\comment[1]{}

\setlength{\intextsep}{1pt}
\setlength{\textfloatsep}{1pt}
\setlength{\floatsep}{1pt}
\abovedisplayskip=6pt plus 3pt minus 9pt
\abovedisplayshortskip=0pt plus 3pt
\belowdisplayskip=6pt plus 3pt minus 9pt
\belowdisplayshortskip=7pt plus 3pt minus 4pt
\setlength\abovecaptionskip{1pt}
\setlength\belowcaptionskip{1pt}

\newcommand\Locs{\ensuremath{\mathcal{L}}}
\newcommand\Trans{\ensuremath{\Delta}}
\newcommand\Vars{\ensuremath{\mathcal{V}}}
\newcommand\Stmts{\ensuremath{\mathit{St}}}
\newcommand\Prog{\ensuremath{\mathit{P}}}
\newcommand\PProg{\ensuremath{\mathcal{P}}}
\newcommand\SemPreGen[1]{\ensuremath{\mathit{SemPreGen}(#1)}}
\newcommand\progs[1]{\ensuremath{\llbracket #1 \rrbracket}}
\newcommand\ProgCons{\ensuremath{\Phi}}
\newcommand\ProgState{\ensuremath{S}}
\newcommand\CFG{\ensuremath{\mathcal{C}}}
\newcommand\Valuation{\ensuremath{\mathcal{D}}}
\newcommand\errVar{\ensuremath{\mathit{err}}}
\newcommand\atomicVar{\ensuremath{\mathit{in\_atomic}}}
\newcommand\PreFreeSem[1]{\llparenthesis #1 \rrparenthesis}
\newcommand\trace{\pi}
\newcommand\Restrict{\mathit{Restrict}}
\newcommand\LearnGood{\mathit{LearnGood}}
\newcommand\LearnGoodUnder{\mathit{LearnGoodUnder}}
\newcommand\FixBad{\mathit{FixBad}}
\newcommand\LearnGoodGen{\mathit{LearnGoodGeneral}}
\newcommand{\redn}{\mathit{DFAsserts}}
\newcommand{\blackn}{\mathit{IntraThreadOrder}}
\newcommand{\bluen}{\mathit{DFConds}}
\newcommand{\pinkn}{\mathit{NonFreeOrder}}
\newcommand{\readn}{\mathit{Read}}
\newcommand{\writen}{\mathit{write}}
\newcommand{\written}{\mathit{written}}
\newcommand{\last}{\ensuremath{\mathit{last}}\xspace}
\newcommand{\depends}{\ensuremath{\mathit{depends}}\xspace}
\newcommand{\prev}{\mathit{prev}}
\newcommand{\before}{\mathit{before}}
\newcommand{\after}{\mathit{after}}
\newcommand{\interfere}{\mathit{interfere}}
\newcommand{\cover}{\mathit{cover}}
\newcommand{\allcover}{\mathit{allcover}}
\newcommand\ToBeProtected{\mathit{ToBeProtected}}
\newcommand{\thread}{\mathit{thread}}


\newcommand\switch[2]{#1 \ensuremath{\leftrightsquigarrow} #2}
\newcommand\reorder{\theta}

\newcommand\arsays[1]{{\bf AR: #1}}
\newcommand\lrsays[1]{{\bf LR: #1}}
\newcommand\pcsays[1]{{\bf PC: #1}}


\newcommand\mypara[1]{\noindent{\bf #1}}
\newcommand\myparait[1]{\noindent{\it #1}}

\begin{document}
\title{Regression-free Synthesis for Concurrency\thanks{This 
    research was funded in part by the European Research Council (ERC)
    under grant agreement 267989 (QUAREM), by the Austrian Science
    Fund (FWF) project S11402-N23 (RiSE), and by a gift from Intel
    Corporation. NICTA is funded by the Australian Government through
    the Department of Communications and the Australian Research Council
through the ICT Centre of Excellence Program.}} 
\author{Pavol {\v C}ern{\'y}\inst{1} \and Thomas A. Henzinger\inst{2}
\and Arjun Radhakrishna\inst{2} \and Leonid
Ryzhyk\inst{3}\inst{4}\and Thorsten Tarrach\inst{2}}
\institute{University of Colorado Boulder \and IST Austria 
\and University of Toronto \and NICTA, Sydney, Australia}
\maketitle

\begin{abstract}
While fixing concurrency bugs, program repair algorithms may introduce new
concurrency bugs.
We present an algorithm that avoids such regressions.
The solution space is given by a set of program transformations we consider
in the repair process. 
These include   
reordering of instructions within a thread and inserting atomic sections. 
The new algorithm learns a constraint on the space of candidate solutions, 
from both positive examples (error-free traces) and counterexamples (error
traces).    
From each counterexample, the algorithm learns a constraint necessary to
remove the errors.
From each positive examples, it learns a constraint that is
necessary in order to prevent the repair from turning the trace into an
error trace. 
We implemented the algorithm and evaluated it on simplified Linux
device drivers with known bugs.  
\end{abstract}



\section{Introduction}
\label{sec:intro}

The goal of program synthesis is to simplify the programming task by 
letting the programmer specify (parts of) her intent declaratively.
{\em Program repair} is the instance of synthesis where we are given both a 
program and a specification. 
The specification classifies the execution of the program into {\em good
traces} and {\em bad traces}. 
The synthesis task is to automatically modify the program so that the 
bad traces are removed, while (many of) the good traces are preserved. 

In {\em program repair for concurrency}, we assume that all errors 
are caused by concurrent execution.
We formalize this assumption into a requirement that all preemption-free
traces are good. 
The program may contain concurrency errors that are triggered by more
aggressive, preemptive scheduling. 
Such errors are notoriously difficult to detect 
and, in extreme cases, may only show up after years of operation 
of the system. 
Program repair for concurrency allows the programmer to focus on the
preemption-free correctness, while putting 
the intricate task of proofing the code for concurrency to the
synthesis tool.

\noindent
{\bf Program repair for concurrency.} 
The specification is provided by assertions placed by the programmer 
in the code. 
A trace, which runs without any assertion failure, is called ``good'',
and conversely a trace with an assertion failure is ``bad''.
We assume that the good traces specify the intent of the programmer.
A trace is complete if every thread finishes its execution. 
A trace of a multi-threaded program is preemption-free if a thread is 
de-scheduled only at preemption-points, i.e., when a thread tries to
execute a blocking operation, such as obtaining a lock. 

Given a multithreaded program in which all complete preemption-free
traces are good, the  program repair for concurrency problem is to find
a program for which the following two conditions hold: 
(a)~all bad traces of the original program are removed; and 
(b)~all the complete preemption-free traces are preserved.
We further extend this problem statement by saying that if not all
preemption-free traces are good, but all complete sequential traces are
good, then we need to find a program such that (a)~holds, and all
complete sequential traces are preserved.

\noindent
{\bf Regression-free algorithms.} Let us consider a trace-based algorithm
for program repair, that is, an iterative algorithm that in each
iteration is given a trace (good or bad) of the program-under-repair,
and produces a new program based on the traces seen. 
We say that such an algorithm is {\em regression-free} if after every
iteration, we have that: first, all bad traces examined so far are
removed, and second, all good traces examined so far are not turned
into bad traces of the new program. (Of course, to make this
definition precise, we will need to define a correspondence between
traces of the original program and the new program.)

\noindent
{\bf Program transformations.}
In order to remove bad traces, we apply the following program
transformations: (1)~reordering of adjacent instructions  within 
a thread if the instructions are sequentially independent (i.e., if 
 is sequentially equivalent to ), and (2)~inserting
atomic sections. 
The reordering of instructions is given priority as it may result in a 
better performance than the insertion of atomic sections. Furthermore, 
the reordering of instructions removes a surprisingly large number of
concurrency bugs that occur in practice; according to a study of how
programmers fix concurrency bugs in Linux device drivers~\cite{cav2013},
reordering of instructions is the most commonly used.  


\noindent
{\bf Our algorithm.}
Our algorithm learns constraints on the space of candidate solutions
from both good traces and bad traces.
We explain the constraint learning using as an example the program 
transformation~(1), which reorders instructions within threads. 
From a bad trace, we learn reordering constraints that eliminate 
the counterexample using the algorithm of~\cite{cav2013}.  
While eliminating the counterexample, such reorderings may transform a
(not necessarily preemption-free) good trace 
into a bad trace
--- this would constitute a regression.
In order to avoid regressions, our algorithm learns also from 
good traces. 
Intuitively, from a good trace , we want to learn all the ways 
in which  can be transformed by reordering without turning it into
an error trace--- this is expressed as a program constraint.
The program constraint is
\begin{inparaenum}[(a)]
\item~sound, if all programs satisfying the constraint are
  regression-free; and
\item~complete, if all programs violating the constraint have
  regressions.
\end{inparaenum}
However, as learning a sound and complete constraint is
computationally expensive, given a good trace  we learn a sound
constraint that only guarantees that  is not transformed into a
bad trace. We generate the constraint using data-flow analysis
on the instructions in . The main idea of the 
analysis is that in good traces, the data-flow into passing assertions
is protected by synchronization mechanisms (such as locks) and data-flow
into conditionals along the trace. 
This protection may fail if we reorder instructions. We thus find
a constraint that prevents such bad reorderings. 
 
Summarizing, as the algorithm progresses and sees a set of bad
traces and a set of good traces, it learns constraints that encode the ways
in which the program can be transformed in order to eliminate the 
bad traces without turning the good traces into bad traces of the
resulting program. 

\noindent
{\bf CEGIS vs PACES.}
A popular recent approach to synthesis is counterexample-guided 
inductive synthesis (CEGIS)~\cite{asplos06}.
Our algorithm can be viewed as an instance of CEGIS with the important
feature that we learn from positive examples.
We dub this approach PACES, for {\em Positive- and Counter-Examples in
Synthesis}.
The input to the CEGIS algorithm is a specification~ 
(possibly in multiple pieces -- say, as a temporal formula and a
language of possible solutions~\cite{sygus}).  
In the basic CEGIS loop, the synthesizer proposes a 
candidate solution~, which is then checked against~.  
If it is correct, the CEGIS loop terminates; if not, a counterexample is 
provided and the synthesizer uses it to improve~.
In practice, the CEGIS loop often faces performance issues, in particular, 
it can suffer from regressions: new candidate solutions may introduce 
errors that were not present in previous candidate solutions.
We address this issue by {\em making use of positive examples} (good 
traces) in addition to counterexamples (bad traces). The good traces
are used to learn constraints that ensure that these good traces are
preserved in the candidate solution programs proposed by the CEGIS loop. 
The PACES approach applies in many program synthesis contexts, but in
this paper, we focus on program repair for concurrency.

\remove{
\noindent
{\bf Experimental evaluation.}
To evaluate our approach, we implemented a program repair tool and 
applied it to a collection of (simplified) open-source Linux device 
drivers. 
We looked at concurrency bugs that were reported and fixed in the 
Linux kernel repository. 
We used five examples, where we modelled the concurrency skeleton in 
sufficient detail to reproduce the bug (between 35 and 80 lines of 
code per example). 
In addition, we did two larger case studies, of the {\tt rtl8169} and 
{\tt usb-serial} drivers, modelled in more detail, with about 400 lines 
of code each. 
As explained above, our tool tries to fix a bug by reordering 
instructions within threads, which is the preferred solution and/or 
by inserting atomic sections.
In each case, our tool found a solution that fixed the 
problem (or, in the case of {\tt rtl8169}, multiple problems).  
To evaluate the impact of using positive examples, we compared our
tool with two earlier versions (based on~\cite{cav2013}), which do not use 
positive examples. 
The first version ({\tt ce1}) prefers to exhaust all possible
instruction reorderings before using atomic sections; the second 
version ({\tt ce2}) heuristically decides to insert atomic sections
earlier.  
We found that (a)~the new tool converges to a solution in a significantly 
smaller number of iterations than ({\tt ce1}), and (b)~the new tool finds 
solutions with fewer atomic sections than ({\tt ce2}) in a comparable 
number of iterations. 
We thus conclude that the use of positive examples can substantially 
improve the performance and quality of counterexample-guided inductive 
synthesis algorithms.
}

\noindent
{\bf Related work.}
The closest related work is by von Essen and Jobstmann~\cite{barbara},
which continues the work on program
repair~\cite{JGB05,GBC06,SDE08}. In~\cite{barbara}, 
the goal is to repair reactive systems (given as automata) according to an
LTL specification, with a guarantee that good traces do not disappear
as a result of the repair. Their algorithm is based on the
classic synthesis algorithm which translates the LTL specification to
an automaton. In contrast, we focus on the repair of concurrent
programs, and our algorithm uses positive examples and
counterexamples. 


There are several recent algorithms for inserting synchronization by locks,
fences, atomic
sections, and other synchronization primitives~(\cite{Vechev:2010:ASS:1706299.1706338,Cherem:2008:ILA:1375581.1375619,ramalingam,SLJB08}).
Deshmukh et al.\ \cite{ramalingam} is the only one of these which
uses information about the correct parts of the program in bug fixing -- 
a proof of sequential correctness is used to identify positions for
locks in a concurrent library that is sequentially correct. 
CFix (Jin et al.\ \cite{shanlu}) can detect and fix
concurrency bugs using specific bug detection patterns and a fixing
strategy for each pattern of bug. 
Our approach relies on a general-purpose model checker and does not use
any patterns.

Our algorithm for fixing bad traces starts by generalizing
counterexample traces.  
In verification (as opposed to synthesis), 
concurrent trace generalization was used by
Sinha et
al.\ \cite{Sinha:2011:IA:1926385.1926433,DBLP:conf/sigsoft/SinhaW10};
and by Alglave et
al.\ \cite{DBLP:conf/cav/AlglaveKT13} for detecting errors due to weak
memory models.
Generalizations of good traces was previously used by Farzan et
al.\ \cite{Farzan:2013:IDF:2480359.2429086}, who create an inductive
data-flow graph (iDFG) to represent a proof of program correctness. 
They do not attempt to use iDFGs in synthesis. 

We use the model checker CBMC~\cite{cbmc} to generate both good and bad
traces.
Sen introduced concurrent directed random testing
\cite{Sen:2008:RDR:1375581.1375584}, which can be used to obtain good
or bad traces much faster than a model checker.
For a 30k LOC program their tool needs only about 2
seconds. 
We could use this tool to initially obtain good and bad traces
faster, thus increasing the scalability of our tool. 





\noindent
{\bf Illustrative example.}
We motivate our approach on the program  in
Figure~\ref{fig:example}.    
There is a bug witnessed by the following trace:
 (the assertion at line  fails). 
Let us attempt to fix the bug using the algorithm from~\cite{cav2013}.
The algorithm discovers possible fixes by first generalizing the trace
into a partial order (Figure~\ref{fig:partialorder}, without the dotted 
edges) representing the happens-before relations necessary for the bug
to occur, and second, trying to create a cycle in the partial order to
eliminate the generalized counterexample.
It finds three possible ways to do this: swapping  and
, or moving  before , or moving  after , indicated by
the dotted edges in Figure~\ref{fig:partialorder}. 
Assume that we continue with swapping  and  to obtain program
 where the first thread is . 
Program  contains an error trace  (the assertion at line  fails). 
This bug was not in the original program, but was introduced
by our fix. 
We refer to this type of bug as a regression. 

In order to prevent regressions, the algorithm learns from good
traces. 
Consider the following good trace .
The algorithm analyses the trace, and produces the graph in 
Figure~\ref{fig:colour1}. 
Here, the thick red edges indicate the reads-from
relation for {\tt assert} commands, and the dashed blue edges indicate
the reads-from relation for {\tt await} commands. 
Intuitively, the algorithm now analyses why the assertion at line 
holds in the given trace. 
This assertion reads the value written in line  (indicated by the
thick red edge).
The algorithm finds a path from  to  composed entirely from
intra-thread sequential edges ( and ) and dashed blue
edges ().
This path guarantees that this trace cannot be changed by different
scheduler choices into a path where  reads from elsewhere and fails. 
From the good trace  we thus find that there could be a
regression unless  precedes  and  precedes . 
Having learned this constraint, the synthesizer can find a better way
to fix . 
Of the three options described above, it chooses the only way which does not
reorder  and , i.e., it moves  after . 
This fixes the program without regressions.

\begin{figure}[tb]
\ffigbox
{\begin{subfloatrow}[3]
    \ffigbox[\FBwidth]{\begin{minipage}{6cm}
      \small{ \tt \setlength{\tabcolsep}{2pt}
    init: x = 0; y = 0; z = 0\\
    \begin{tabular}{l l l}
    \underline{thread1}&\underline{thread2}&\underline{thread3}\\
      1: await(x==1)  & A: x:=1 & n: await(z==1)\\
      2: await(y==1)  & B: y:=1 & p: assert(y==1)\\
      3: assert(z==1)  & C: z:=1 \\
      \end{tabular}
      }
      \end{minipage}
    }
    {\caption{Program }\label{fig:example}}
    \ffigbox[\FBwidth+0.3cm]{\begin{tikzpicture}
      \tikzstyle{every state}=[draw=black, minimum size=12]
      \node[state]             (1) {{\tt 1}};
      \node[state,below of=1,yshift=0.25cm]  (2) {{\tt 2}};
      \node[state,below of=2,yshift=0.25cm]  (3) {{\tt 3}};
      \node[state,left of=1]  (a) {{\tt A}};
      \node[state,below of=a,yshift=0.25cm]  (b) {{\tt B}};
      \node[state,below of=b,yshift=0.25cm]  (c) {{\tt C}};

      \path[->]
            (b) edge (2)
            (1) edge (2)
            (2) edge (3)
            (3) edge (c)
            (a) edge (1)
            (c) edge[dotted] (b)
            (c) edge[in=200,out=160,dotted] (a)
      ;
      

    \end{tikzpicture}
    }
    {\subcaption{Reorderings from bad traces}\label{fig:partialorder}}
    
    \ffigbox[\FBwidth]{\begin{tikzpicture}
      \tikzstyle{every state}=[draw=black, minimum size=12]
			\node[state]             (1) {{\tt 1}};
			\node[state,below of=1,yshift=0.25cm]  (2) {{\tt 2}};
			\node[state,below of=2,yshift=0.25cm]  (3) {{\tt 3}};
			\node[state,right of=1,xshift=0.15cm]  (a) {{\tt A}};
			\node[state,below of=a,yshift=0.25cm]  (b) {{\tt B}};
			\node[state,below of=b,yshift=0.25cm]  (c) {{\tt C}};
			\node[state,right of=a,xshift=0.15cm]  (n) {{\tt n}};
			\node[state,below of=n,yshift=0.25cm]  (p) {{\tt p}};

			\path[->]
(n) edge  (p)
						(c) edge[red,thick] (3)
						(b) edge[red,thick] (p)
						(a) edge[blue,dashed] (1)
						(b) edge[blue,dashed] (2)
						(c) edge[blue,dashed] (n)
						(b) edge (c)
;
    \end{tikzpicture}
    }
    {\subcaption{Learning from a good trace}\label{fig:colour1}}
    
  \end{subfloatrow}

}
{
  \vspace{-2ex}
  \caption{Program analysis with good and bad
traces}\label{fig:simple_example}}
\end{figure}



\section{Programming Model and the Problem Statement}
\label{sec:defs}



Our programs are composed of a fixed number (say ) threads
written in the {\sc Cwhile} language (Figure~\ref{fig:syntax}).
Each statement has a unique program location and each thread has unique
initial and final program locations.
Further, we assume that execution does not stop on assertion
failure, but instead, a variable  is set to .
The {\tt await} construct is a blocking assume, i.e., execution of
{\tt await(cond)} stops till {\tt cond} holds.
For example, a lock construct can be modelled as {\tt
atomic \{  await(lock\_var == 0); lock\_var := 1 \}}.
Note that {\tt await} is the only blocking operation in {\sc Cwhile} --
hence, we call the {\tt await} operations {\em preemption-points}.
\begin{figure} 
  \vspace{1ex}
\begin{alltt}
iexp ::= iexp + iexp | iexp / iexp | iexp * iexp | var | constant
bexp ::= iexp >= iexp | iexp == iexp | bexp && bexp | !bexp
stmt ::= variable := iexp | variable := bexp | stmt; stmt | assume(bexp) 
         | if (*) stmt else stmt | while (*) stmt | atomic \{ stmt \}
         | assert(bexp) | await(bexp)
thrd ::= stmt                               prog  ::= thrd | progthrd
\end{alltt}
\vspace{-4ex}
\caption{Syntax of programming language}
\label{fig:syntax}
\end{figure}

\!\!\!\!\!\!\!\!
\paragraph{Semantics.}
The {\em program-state}  of a program  is given by
 where  is a
valuation of variables, and each  is a thread~~program location.
Execution of the thread~ statement at location  is
represented as  where
 and
,
and  and  are the program location and
variable valuation after executing the statement from . 
A {\em trace}  of  is a sequence  where 
\begin{inparaenum}[(a)]
\item  where each  is the initial location of
  thread ; and
\item each  is a thread~
  transition for some .
\end{inparaenum}
Trace  is {\em complete} if , where each  is the
final location of thread~.
We say  is {\em equal modulo
error-flag} to  if each
 and  differ only in the valuation of the
variable .




Trace  is {\em preemption-free} if every context-switch occurs
either at a preemption-point ({\tt await} statement) or at the end of a
thread's execution, i.e., if where  and  are
transitions of different threads (say threads  and ), either the
next thread~ instruction after  is an {\tt await}, or the
thread~ is in the final location in .
Similarly, we call a trace {\em sequential} if every context-switch
happens at the end of a thread's execution.

A trace 
is {\em bad} if the error variable  has values  and  in
 and , respectively; otherwise,  is
{\em good} trace.
We assume that the bugs present in the input
programs are {\em data-independent} -- if  is bad, so is every trace  where
 for all .

\comment{
\arsays{Why is this even here? Can't this be done using program and free
transformations acting on the trace. 

The below definition is simply saying transform  to 
using program transformations, but not free transformations
}
{\new Let there be two traces  and  that may originate from different programs and
\begin{inparaenum}[(a)]
 \item there exists a bijection  that maps indices in  to  such that ; and
 \item the result stays the same: .
\end{inparaenum}
We call these traces  {\em equal modulo reordering} if for any two preemption points
, .
We call them {\em sequentially equivalent} if for any two final thread locations
, .
}
}
 


\comment{
Partial-programs are partially specified programs containing
non-deterministic constructs.
Here, we extend {\sc Cwhile} with three non-deterministic constructs:
\begin{compactitem}
\item {\tt reorder \{.\} }:  Intuitively, the statements
  inside a {\tt reorder \{.\}} block are to be executed in some order.
For example, {\tt reorder \{ x := 1; y := 1 \}} is a choice between
  the fragments {\tt x := 1; y:= 1} and {\tt y := 1; x := 1}.
\item {\tt atomize \{.\} }: An {\tt atomize} block is a
  non-deterministic choice on which (if any) sub-sequence of statements
  inside are to be executed atomically.
  Colloquially, this is equivalent to choosing which atomic section to
  insert.
\end{compactitem}
A program  is a {\em specialization} of a partial-program
 (written ) if every non-deterministic block is replaced by a particular
determinization (e.g., {\tt reorder} blocks by a particular
reordering).
}



\comment{
\paragraph{Generalizing a program to a partial-program.}
Given a program , we define the preemption-free
semantics-preserving partial-program .
The procedure to compute  is a generalization of the
procedure described in~\cite{cav2013}.
Intuitively, the procedure works as follows: 
\begin{compactitem}
\item The procedure iteratively picks neighbouring statements (say
   and ) and checks for a variable valuation such
  that executing  results in a different result from executing .
  If not,  and  are added to a {\tt reorder} block.
\item The procedure then inserts the largest {\tt atomize} blocks which
  do not contain any {\new {\tt await} statements}.
\end{compactitem}
}

\noindent{\bf Program transformations and Program constraints.}
We consider two kinds of transformations for fixing bugs:
\begin{compactitem}
\item~A {\em reordering transformation} 
  transforms  to  if location  immediately precedes
   in  and  immediately precedes  in
  .
  We only consider cases where the sequential semantics
  are preserved, i.e., if
\begin{inparaenum}[(a)]
  \item~ and  are from the same basic block; and
  \item~ is equivalent to .
  \end{inparaenum}
\item~An {\em atomic section transformation} 
  transforms  to  if 
neighbouring locations  and  are in an atomic
  section in , but not in .
\end{compactitem}
We write
 if applying each
of  in order transforms  to .
We say transformation  {\em
acts across preemption-points} if either 
and one of  or  is a preemption-point; or if  and  is a preemption-point.


Given a program , we define {\em program constraints} to
represent sets of programs that can be obtained through
applying program transformations on .
\begin{compactitem}
\item~{\em Atomicity constraint}: Program  if
   and  are in an atomic block.
\item~{\em Ordering constraint}: Program  if  and  are from the same basic block and either
   occurs before , or  satisfies .
\end{compactitem}
If , we say that  {\em satisfies}
.
Further, we define conjunction of  and
 by letting .

\noindent{\bf Trace Transformations and Regressions.}
A trace 
{\em transforms} into a trace  by {\em switching} if:
\begin{inparaenum}[(a)]
\item  and the suffixes
   and  are equal modulo error-flag; and
\item .
\end{inparaenum}
We label switching transformations as a:
\begin{compactitem}
\item {\em Free transformation} if  and  are from
  different threads. 
  We write  if a sequence of free transformations
  takes  to .
\item {\em Reordering transformation  acting on } if 
    and .
We have  if repeated applications of
   transformations acting on  give . 
  Similarly,  if repeated applications of
   and free transformations acting on 
  give .
\end{compactitem}
Similarly,  is obtained by {\em atomicity
transformation  acting on a trace} 
if , and there are no context-switches between
 and  in .

\paragraph{Trace analysis graphs.}
We use trace analysis graphs to characterize data-flow and
scheduling in a trace.
First, given a trace , we define
the function  to recursively find the data-flow
edges into the .
Formally,  where  ranges over variables read by , and
 returns  if  reads the value of  written by
 and  if no such  exists.
As the base case, we define .


Now, a {\em trace analysis graph} for trace  is a
multi-graph  , where  are the positions in the trace along
with  (representing the initial state) and  contains the
following types of edges.
\begin{compactenum}
\item~{\em Intra-thread order} (): We have  if either
  , and  and  are from the same thread, or if .
\item~{\em Data-flow into conditionals} (): We have
   where  iff  is an assume or an await statement.
\item~{\em Data-flow into assertions} (): We have
   where
   iff  is an assert statement.
\item {\em Non-free order} (): We have  if  and
   write two different values to the same variable. 
  Intuitively, the non-free orders prevent switching transformations
  that switch  and .
\end{compactenum}


\paragraph{Regressions.}
Suppose .
We say  introduces a {\em
regression} with respect to a good trace  of  if there exists a trace  such that:
\begin{inparaenum}[(a)]
\item  is a bad trace of ;
\item  does not freely transform into any bad trace of ; and
\item for every data-flow into conditionals edge  (say 
  reads the variables  from ) in , the edge  is a data-flow into conditionals edge in  (where
   reads the same variables  from ).
  Here,  is the position in  of instruction at position
   in  after the sequence of switching transformations that
  take  to .
\end{inparaenum}
We say  introduces a regression with respect
to a set  of good traces if it introduces a
regression with respect to at least one trace .

Intuitively, a program-transformation induces a regression if it allows
a good trace  to become a bad trace  due to the program
transformations.
Further, we require that  and  have the conditionals
enabled in the same way, i.e., the {\tt assume} and {\tt await}
statements read from the same locations.
\begin{remark}
  \label{rem:regressions}
  The above definition of regression attempts to capture the intuition
  that a good trace transforms into a ``similar'' bad trace.
  The notion of similar asks that the traces have the same data-flow
  into conditionals -- this condition can be relaxed to obtain more
  general notions of regression.
  However, this makes trace analysis and finding regression-free fixes
  much harder (See Example~\ref{ex:non_locality}).
\end{remark}

\begin{example}
  In Figure~\ref{fig:simple_example}, the trace  transforms under  to , which freely transforms to .
  Hence,  introduces a regression with respect to 
  as  does not freely transform into a bad trace,
  and  is bad while the {\tt await} in  still reads from .
\end{example}

\noindent{\bf The Regression-free Program-Repair Problem.}
Intuitively, the program-repair problem asks for a correct program
 that is a transformation of .
Further,  should preserve all sequential behaviour
of ; and if all preemption-free behaviour of  is good, we
require that  preserves it.

\paragraph{Program repair problem.}
The input is a program  where all complete sequential traces are
good.
The result is a sequence of program transformations  and , such that
\begin{inparaenum}[(a)]
\item~;
\item~ has no bad traces;
\item~for each complete sequential trace  of , there
  exists a complete sequential trace  of  such that
  ; and
\item~if all complete preemption-free traces of  are good, then
  for each such trace , there exists a complete preemption-free
  trace  of  such that .
\end{inparaenum}
We call the conditions (c)~and (d)~the {\em preservation of sequential and
correct preemption-free behaviour}.



\paragraph{Regression-free error fix.}
Our approach to the above problem is through repeated
regression-free error fixing.
Formally, the regression-free error fix problem takes a set of good
traces , a program  and a bad trace  as input,
and produces transformations  and 
such that ,
 is a trace
in , and  does not introduce a {\em
regression} with respect to .


\section{Good and Bad Traces}

Our approach to program-repair is through
learning regression preventing constraints from good traces 
and error eliminating constraints from bad traces.


\subsection{Learning from Good Traces}
\label{sec:learn_good}

Given a trace  of , a program constraint  is a
{\em sound regression preventing constraint} for  if every
sequence of program transformations ,
such that  and , does not introduce a
regression with respect to .
Further, if every , such that
 and
, introduces a regression with respect to
, then  is a {\em complete regression preventing
constraint}.

\begin{example}
  \label{ex:sound_complete_cons}
  Let the program  be  . 
  In Figure~\ref{fig:orig_trace}, the constraint  is a sound and complete regression-preventing
  constraint for the trace .
\end{example}

\begin{figure}[b]
\ffigbox
{\scriptsize \begin{subfloatrow}[3]
    \ffigbox[\FBwidth]{\begin{tikzpicture}
        \node[rounded rectangle] (c) {{\tt 1: x:=1}};
        \node[rounded rectangle, below of=c, yshift=10] (d) {{\tt 2: y:=1}};
        \node[rounded rectangle, left of=c, xshift=-20, yshift=-40] (a) {{\tt A: await(y==1)}};
        \node[rounded rectangle, below of=a, yshift=10] (b) {{\tt B: assert(x==1)}};

        \draw[->] (a) to (b);
        \draw[->] (c) to (d);
        \draw[->, blue, dashed] (d) to (a);
        \draw[->, red,thick] (c.south west) to (b.north east);
      \end{tikzpicture}
    }
    {\vspace{-2ex}\subcaption{}\label{fig:orig_trace}}
    \ffigbox[\FBwidth]{\begin{tikzpicture}
        \node[rounded rectangle] (c1) {{\tt 1: x:=1}};
        \node[rounded rectangle, below of=c1, yshift=10] (d1) {{\tt 2: y:=1}};
        \node[rounded rectangle, left of=c1, xshift=-30, yshift=-20] (a1) {{\tt A: await(y==1)}};
        \node[rounded rectangle, below of=a1, yshift=10] (b1) {{\tt B: assert(x==1)}};
        \node[rounded rectangle, below of=b1, yshift=10] (b2) {{\tt C: a:=1}};
        \node[rounded rectangle, below of=d1, yshift=-15] (e2) {{\tt 3: assume(a==1)}};
        \node[rounded rectangle, below of=e2, yshift=10] (e1) {{\tt 4: x:=0}};

        \draw[->] (a1) to (b1);
        \draw[->] (b1) to (b2);
        \draw[->] (c1) to (d1);
        \draw[->] (e2) to (e1);
        \draw[->, blue, dashed] (d1) to (a1);
        \draw[->, red,thick] (c1.south west) to (b1.north east);
        \draw[->, green, thin] (b1.340) to (e1.west);
        \draw[->, blue, dashed] (b2.east) to (e2.north west);
      \end{tikzpicture}
    }
    {\vspace{-2ex}\subcaption{} \label{fig:from_read_trace}}
    \ffigbox[\FBwidth-0.5cm]{\begin{tikzpicture}
        \node[rounded rectangle] (c1) {{\tt 1: x:=1}};
        \node[rounded rectangle, below of=c1, yshift=10] (e1) {{\tt 2': y:=2}};
        \node[rounded rectangle, below of=e1, yshift=10] (d1) {{\tt 2: y:=1}};
        \node[rounded rectangle, left of=e1, xshift=-30, yshift=-30] (a1) {{\tt A: await(y>=1)}};
        \node[rounded rectangle, below of=a1, yshift=10] (b1) {{\tt B: assert(x==1)}};

        \draw[->] (a1) to (b1);
        \draw[->, blue, dashed] (d1) to (a1);
        \draw[->, red,thick] (c1.west) to (b1.east);
        \draw[->] (c1.south east) to[bend left=60] (d1.north east);
      \end{tikzpicture}
    }
    {\vspace{-2ex}\subcaption{} \label{fig:write_order_trace}}
    \end{subfloatrow}
}
{
  \vspace{-2ex}
\caption{Sample Good Traces for Regression-preventing constraints}\label{fig:traces}
}
\end{figure}

\begin{lemma}
  \label{lem:learning_completeness}
  For a program  and a good trace , the sound and
  complete regression-preventing constraint  is computable
  in exponential time in .
\end{lemma}
Intuitively, the proof relies on an algorithm 
that iteratively applies all possible free
and program transformations in different combinations (there are a
finite, though exponential, number of these) to .
It then records the constraints satisfied by programs obtained by
transformations that do not introduce regressions. 

\comment{
While the complexity is exponential, we can
show that this cost is unavoidable.
We do not present the proof here, but only state that
it is non-constructive and is based on Shannon's lower bounds on
circuit complexity for boolean functions.
\arsays{This is a bit shady: is there a family of boolean functions that
has poly size circuits/poly size turing machines, but a exponential size
formulae}
\begin{lemma}
  There exist a class of programs , and traces  of
  length  such that the most-general regression-preventing
  constraint is of size .
\end{lemma}
}



The sound and complete constraints are usually large and
impractical to compute.
Instead, we present an algorithm to compute sound
regression-preventing constraints.
The main issue here is non-locality, i.e., statements that are not close
to the assertion may influence the regression-preventing constraint.



\begin{example}
  \label{ex:non_locality}
  The trace in Figures~\ref{fig:from_read_trace} is a simple extension
  of Figure~\ref{fig:orig_trace}.
  However, the constraint 
  (from Example~\ref{ex:sound_complete_cons}) does not prevent
  regressions for Figure~\ref{fig:from_read_trace}.
  An additional constraint  is needed as
  reordering these statements can lead to the assertion failing by
  reading the value of {\tt x} ``too late'', i.e., from the statement
  {\tt 4} (trace: ).
  
  Figure~\ref{fig:write_order_trace} clarifies our definition of
  regression, which requires that the data-flow edges into
  assumptions and awaits need to be preserved.
  The await can be activated by both {\tt 2} and {\tt 2'}; in the trace
  we analyse it is activated by {\tt 2}.
  Moving {\tt 2'} before {\tt 1} could activate the await ``too early''
  and the assertion would fail (trace: ).
  However, it is not possible to learn this purely with data-flow
  analysis -- for example, if statement {\tt 2'} was {\tt y := -1}, then
  this would not lead to a bad trace.
  Hence, we exclude such cases from our definition of regressions by
  requiring that the await reads {\tt A} reads from the same location.
\end{example}



\comment{
\begin{example}[Problems for over-approximation]
  Consider the modified version of the trace in
  Figure~\ref{fig:over_approx_trace}.
  Here, the constraint  is too strong, i.e., there are programs
  that violate the constraint and yet do not introduce regressions.
  The problem here is that the assertion may succeed even if the current
  version of the data-flow into the assertion is not respected -- even
  if the assertion reads the value of {\tt x} too early, i.e., from {\tt
  0'}, the assertion still succeeds.
\end{example}
}

\paragraph{Learning Sound Regression-Preventing Constraints.}
The sound regression-preventing constraint learned by our algorithm for
a trace ensures that the data-flow into an assertion is preserved.
This is achieved through two steps: suppose an assertion at location
 reads from a write at location . 
First, the constraint ensures that  always happens before
.
Second, the constraint ensures that no other writes 
interfere with the above read-write relationship.


For ensuring happens-before relationships, we use the notion of a {\em
cover}.
Intuitively, given a trace  of  where location 
happens before location ,
we learn a  that ensures that if ,
then each trace  of  obtained as free and program
transformations acting on  satisfies the happens-before
relationship between  and .
Formally, given a trace  of program , we call a path  in the trace analysis graph a {\em cover} of
edge  if  and each of 
is either a intra-thread order edge, or a  data-flow into conditionals
edge, or a non-free order edge.

Given a trace , where statement at position  (i.e., ) reads a set
of variables (say ) written by a statement at position  (i.e.,
), the the non-interference edges define a sufficient set of
happens-before relations to ensure that no other statements can
interfere with the read-write pair, i.e., that every other write to

either happens before  or after .
Formally, we have that  where  and  are the 
variables read and written at location .
If , we have .
\begin{algorithm}
  \begin{algorithmic}[1]
    \REQUIRE A good trace 
    \ENSURE Regression-preventing constraint 
    \STATE 
    \FORALL {}
    \STATE \textbf{if}~ is not
    covered~\textbf{then}~\textbf{return}~~\label{line:ret_fallback}
      \STATE 
      \FORALL {  cover of }  \label{line:pick_cover}
      \STATE x_i \neq \bot
      \item[]  and  are from the same
  execution of a basic block in  \}
\ENDFOR
      \STATE 
    \ENDFOR
    \RETURN {} \label{line:ret_normal}
  \end{algorithmic}
  \caption{Algorithm } 
  \label{algo:learn_good}
\end{algorithm}

Algorithm~\ref{algo:learn_good} works by ensuring that for each data-flow
into assertions edge , the edge itself is covered and that the
interference edges are covered.
For each such cover, the set of intra-thread order edges needed
for the covering are conjuncted to obtain a constraint.
We take the disjunction  of the constraints produced by all
covers of one edge and add it to a constraint  to
be returned.
If an edge cannot be covered, the algorithm falls back by returning a
constraint that fixes all current intra-thread orders.
The algorithm can be made to run in
polynomial time in  using standard dynamic
programming techniques.
\begin{theorem}
  \label{lem:learning_soundness}
  Given a trace , Algorithm~\ref{algo:learn_good} returns a
  constraint  that is a sound regression-preventing
  constraint for  and runs in polynomial time in .
\end{theorem}
\begin{proof}[Outline]
The fallback case (line~\ref{line:ret_fallback}) is trivially sound.
  Let us assume towards contradiction that there is a bad trace
   of
  , that is obtained by transformation of 
  .
  For each , let  be such that the instruction at
  position  in  is at position  in  after the
  sequence of switching transformations taking  to .

  If for every data-flow into assertion edge in  in
  , we have that  is a corresponding data-flow
  into assertion edge in , then it can be easily shown that
   is also good (each corresponding edge in  reads the
  same values as in ).
  Now, suppose  is the first (with minimal ) such edge in
   that does not hold in .
  We will show in two steps that  happens before  in ,
  and that  reads from  which will lead to a contradiction.

  For the first step, we know that there exists a cover of 
  in .
  For now, assume there is exactly one cover -- the other case is
  similar.
  For each edge  in this cover, no switching transformation
  can switch the order of  and :
  \begin{compactitem}
  \item~If  is a data-flow into conditionals edge,
    as  has to preserve all  edges (definition of
    regression), 
     happens before  in .
  \item~If  is a non-free order edge, no switching
    transformation can reorder  and  as that would change
    variables values (by definition of non-free edges).
  \item~If  is a intra-thread order edge, we have that  and , and hence, no
    switching transformation would change the order of  and .
  \end{compactitem}
  Hence, we have that all the happens before relations given by the
  cover are all preserved by  and hence,  happens before
   in .
The fact that  reads from  follows from a similar
  argument with the  edges
  showing that every interfering write either happens before  or
  after . 
\qed
\end{proof}


\comment{
\paragraph{Learning Over-approximations.} 
We describe an over-approximation algorithm for computing
complete regression-preventing constraints.
The algorithm splits the trace into two parts -- the prefix
 and the suffix  (line~\ref{line:...}). 
The assertion statement under consideration is assumed to be in the
suffix.

The aim is to find a program constraint that ``enables the prefix''
and ``protects the suffix''.
To enable the prefix, we want to learn a constraint  such
that every program that respects  contains some free
transformation of  (line~\ref{line:...}).
The intention is that  allows the execution to get to the same
situation that exists at the beginning of .
For example, the program  itself is a candidate for .
However, in the algorithm, we find a more general constraint. 

In protecting the suffix, we find a constraint  such that
every program that violates  induces regressions into the
suffix of the trace.
However, the major issue was illustrated in Example~\ref{ex:...}. 
To overcome this, we generalize the flow-graph of the suffix
 so that any violation of the flow-graph leads to an
assertion failure.

\begin{algorithm}
  \begin{algorithmic}[1]
    \REQUIRE Partial-program , program , and trace 
    \ENSURE Regression-preventing constraint 
    \STATE Pick ,  such that .
\STATE 
    \STATE   and  are from
        the same thread and there is a path from  to  through a 
    node from another thread .
\STATE Generalize the suffix
    \STATE  protect red edges in generalized suffix
    \RETURN 
  \end{algorithmic}
  \caption{Algorithm } 
  \label{algo:learn_good}
\end{algorithm}


\begin{theorem}
  \label{lem:learning_soundness}
  Given a trace  of a program ,
  Algorithm~\ref{algo:learn_good} returns a constraint  that
  is a complete regression-preventing constraint for .
\end{theorem}

The advantage of this approach is that by changing the length of the
prefix vs. the length of the suffix, we can vary the cost of computing
constraint and the accuracy.
}




\subsection{Eliminating Bad Traces}
\label{sec:fix_bad}



Given a bad trace  of , a program constraint 
is a {\em error eliminating constraint} if for all transformations
 and  such that 
 and , each bad trace
 in  is not a
trace of .
In~\cite{cav2013}, we presented an algorithm to fix bad traces using
reordering and atomic sections.
The main idea behind the algorithm is as follows.
Given a bad trace , we 
\begin{inparaenum}[(a)]
\item first, generalize the trace into a partial order trace; and
\item then, compute a program constraint that violates some essential
  part of the ordering necessary for the bug. 
\end{inparaenum}

More precisely, the procedure builds a trace elimination graph which
contain edges corresponding to the orderings necessary for the
bug to occur, as well as the edges corresponding program constraints.
Fixes are found by finding cycles in this graph -- the conjunction of
the program constraints in a cycle form an error elimination constraint.
Intuitively, the program constraints in the cycle will enforce a
happens-before conflicting with the orderings necessary for the bug.


\begin{figure}[tb]
\ffigbox
{\scriptsize \begin{subfloatrow}[3]
    \ffigbox[\FBwidth]{\begin{tikzpicture}
        \node[rounded rectangle] (c) {{\tt A: x:=1}};
        \node[rounded rectangle, below of=c, yshift=5] (e) {{\tt B: z:=1}};
        \node[rounded rectangle, below of=e, yshift=5] (d) {{\tt C: y:=1}};
        \node[rounded rectangle, right of=c, xshift=30, yshift=-10] (a) {{\tt 1: await(x=1)}};
        \node[rounded rectangle, below of=a, yshift=5] (b) {{\tt 2: assert(y=1)}};

        \draw[->] (c) edge (a);
        \draw[->] (b) edge (d);
	\draw[->, dotted] (a) edge node[right] {\color{red} } (b);
	\draw[->, dotted] (d) edge[bend left,in=90,out=90] node[left] {\color{red} } (c);
      \end{tikzpicture}
    }{\vspace{-3ex}\subcaption*{}\label{fig:bad_trace_ex1}}
    \ffigbox[\FBwidth-0.5cm]{\begin{tikzpicture}
        \node[rounded rectangle] (c) {{\tt A: x:=0}};
        \node[rounded rectangle, below of=c, yshift=-20] (e) {{\tt B: x:=1}};
        \node[rounded rectangle, right of=c, xshift=30, yshift=-15] (a) {{\tt 1: assert(x=1)}};

        \draw[->] (c) to (a);
	\draw[->, draw, dotted] (e) edge[bend left] node[right] {\color{red} }  (c);
        \draw[->, draw] (a) edge (e);
      \end{tikzpicture}
    } {\vspace{-3ex}\subcaption*{}\label{fig:bad_trace_ex2}}
    \ffigbox[\FBwidth-0.25cm]{\begin{tikzpicture}
        \node[rounded rectangle] (c) {{\tt A: x:=1}};
        \node[rounded rectangle, below of=c, yshift=0] (e) {{\tt B: y:=1}};
        \node[rounded rectangle, right of=c, xshift=30, yshift=15] (a) {{\tt 1: assert(y=1)}};

\draw[->, draw, dotted] (e) edge[bend right] node[right,xshift=2] {\color{red} }  (a);
        \draw[->, draw] (a) edge (e);
        \draw[->, draw] (c) edge (e);
      \end{tikzpicture}
    } {\vspace{-3ex}\subcaption*{}\label{fig:bad_trace_ex3}}
    \end{subfloatrow}
  }
  {\vspace{-7ex}\caption{Eliminating bad traces}\label{fig:bad_trace}}
\end{figure}
\begin{example}
Consider the program in Figure~\ref{fig:bad_trace}(left) and the trace
  elimination graph for the trace .
  The orderings  happens-before  and 
  happens-before  are necessary for the error to happen.
  The cycle  is the elimination cycle.
  The corresponding error eliminating constraint is , and one possible fix is to move  ahead of .
For the bad trace  in Figure~\ref{fig:bad_trace}(center), the
  elimination cycle is  giving us the 
  constraint  and an atomic section around  as the
  fix.
\end{example}

\paragraph{The  algorithm.}
The  algorithm takes as input a program , a constraint
 and a bad trace .
It outputs a program constraint ,
sequence of program transformations , and
a new program , such that
.
The algorithm guarantees that
\begin{inparaenum}[(a)]
 \item~ is an error eliminating constraint;
 \item ; and
 \item if there is no preemption-free trace  of  such
   that  freely transforms to  (i.e., ), then none of the transformations  acts across preemption-points.
\end{inparaenum}
The fact that  and  can be chosen to
satisfy (c)~is a consequence of the algorithm described
in~\cite{cav2013}.

\noindent{\bf Fixes using wait/notify statements.}
Some programs cannot be fixed by statement reordering or atomic section
insertion.
These programs are in general outside our definition of the program 
repair problem as they have bad sequential traces.
However, they can be fixed by the insertion of wait/notify
statements.
One such example is depicted in Figure~\ref{fig:bad_trace}(right) where the
trace  causes an assertion failure.
A possible fix is to add a {\tt wait} statement before  and a
corresponding {\tt notify} statement after .
The algorithm  can be modified to insert such wait-notify
statements by also considering constraints of the
form  to represent that  is scheduled before  -- the
corresponding program transfomation is to add a wait statement before
 and a notify statement after .
In Figure~\ref{fig:bad_trace}(right), the edge  represents such a
constraint  -- the elimination cycle  corresponds to the above described fix.

\section{The Program-Repair Algorithm}
\label{sec:algo}


\begin{algorithm}[tbh]
  \begin{algorithmic}[1]
    \REQUIRE A concurrent program , all sequential traces are good
    \ENSURE Program  such that  has no bad traces
    \STATE 
    \WHILE{\TRUE}
    \STATE{} \label{line:verify_prog}
    \STATE Choose  from  \hfill (non-deterministic) \label{line:pick_trace}
    \IF{ is non-erroneous}
    \STATE 
    \ELSE
    \STATE \label{line:fix_bad}
    \STATE
     \label{line:adjust}
    \ENDIF
    \ENDWHILE
  \end{algorithmic}
  \caption{Program-Repair Algorithm for Concurrency}
  \label{algo:main}
\end{algorithm}

Algorithm~\ref{algo:main} is a program-repair procedure to fix
concurrency bugs while avoiding regressions.
The algorithm maintains the current program ,
and a constraint  that restricts possible reorderings.
In each iteration, the algorithm tests if  is correct and if so
returns .
If not it picks a trace  in 
(line~\ref{line:pick_trace}).
If the trace is good it learns the regression-preventing constraint
 for  and the trace  is added to the set of
good traces  ( is required only for the correctness proof).
If  is bad it calls  to generate a new program that
excludes  while respecting , and  is
strengthened by conjunction with the error elimination constraint
 produced by .
The algorithm terminates with a valid solution for all choices
of  in line~\ref{line:fix_bad} as the constraint  is
strengthened in each  iteration. 
Eventually, the strongest program-constraint will restrict the possible
program  to one with large enough atomic sections such that it
will have only preemption-free or sequential traces.


\begin{theorem}[Soundness]
  \label{lem:algo_soundness}
  Given a program , Algorithm~\ref{algo:main} returns a program
   with no bad traces that preserves the sequential and correct
  preemption-free behaviour of .
  Further, each iteration of the {\bf while} loop where a bad trace
   is chosen performs a regression-free error fix with respect
  to the good traces .
\end{theorem}
\remove{
\begin{proof}[outline]
  We need to show there are no bad traces in  and all sequential traces remain.
  The first follows because the algorithm only terminates if  is correct and
  the second follows because we do not insert wait-notify statements. Additionally we need to show that 
  if all preemption-free traces are good they are preserved. This is guaranteed by 
  as it does not do transformations across preemption-points if the trace is not preemption-free.
  
  We further need to show that no regression is introduced in each iteration and that the
  bad trace disappears. These follow directly from the properties of .
\end{proof}
}
The extension of the  algorithm to wait/notify
fixes in Algorithm~\ref{algo:main} may lead to  not preserving
the good preemption-free and sequential behaviours of .
However, in this case, the input  violates the pre-conditions 
of the algorithm.


\begin{theorem}[Fair Termination]
  Assuming that a bad trace will eventually be chosen in 
  line~\ref{line:pick_trace} if one exists in , Algorithm~\ref{algo:main}
  terminates for any instantiation of .
\end{theorem}
\remove{
\begin{proof}[outline]
 {\new Fairness guarantees that eventually a bad trace will be chosen if there exists one.
 Every call to  either increases an atomic section or adds a constraint. The number of both is finite. The correct program always exists because every thread can be wrapped into an atomic section and all sequential traces are good.
}\end{proof}
}










\newcommand\choices{\mathcal{C}}
\newcommand\choice{\mathbf{c}}
\newcommand\expts{\mathcal{E}}
\newcommand\inp{\mathbf{i}}
\mypara{A Generic Program-Repair Algorithm.}
We now explain how our program-repair algorithm relates to generic
synthesis procedures based on {\em counter-example guided inductive
synthesis} (CEGIS)~\cite{asplos06}.
In the CEGIS approach, the input is a {\em partial-program} , i.e., a
non-deterministic program and the goal is to specialize  to a
program  so that all behaviours of  satisfy a
specification.
In our case, the partial-program would non-deterministically choose
between various reorderings and atomics sections.
Let  be the set of choices (e.g., statement
orderings) available in .
For a given , let  be the predicate that program obtained by
specializing  with  behaves correctly on the input
.

The CEGIS algorithm maintains a set  of inputs 
called experiments.
In each iteration, it finds  such
that the .
Then, it attempts to find an input  such that
 does not hold.
If there is no such input, then  is the correct
specialization.
Otherwise,  is added to .
This procedure is illustrated in Figure~\ref{fig:cegis}(left).
Alternatively, CEGIS can be rewritten in terms of
constraints on .
For each input , we associate the constraint 
where .
Now, instead of , the algorithm maintains the constraint .
Every iteration, the algorithm picks a  such that ; tries to find an input  such that
 holds, and then strengthens
 by .

\begin{figure}
  \begin{minipage}{0.3\textwidth}
    \begin{tikzpicture}
      \node [draw, rectangle] (a) {
	\tabular{c}  \\
	
	\endtabular
      };
      \node [draw, below of=a, yshift=-5, rectangle] (b) {
	\tabular{c}
	 s.t.   \\
	
	\endtabular
      };

      \draw[->] (a) edge[bend left, out=90, in=90] (b);
      \draw[->] (b) edge[bend left, out=90, in=90] node[left] {
	\tabular{c} 
	\\
	
	\endtabular
      } (a);
    \end{tikzpicture}
  \end{minipage}
  \hfill
  \begin{minipage}{0.6\textwidth}
    \begin{tikzpicture}
      \node [draw, rectangle] (a) {
	\tabular{c}  
\endtabular
      };
      \node [draw, below of=a, yshift=-5, xshift=-37, rectangle] (b) {
	\tabular{c}
	 s.t.   \\
	
	\endtabular
      };
      \node [draw, below of=a, yshift=-5, xshift=37, rectangle] (c) {
	\tabular{c}
	 s.t.   \\
	
	\endtabular
      };

      \draw[->] (a) edge (b);
      \draw[->] (a) edge (c);
      \draw[->] (b) edge[out=135,in=270,bend left] node[left] {
	\tabular{c}
	 \\
	 
	\endtabular
      } (a.west);
      \draw[->] (c) edge[out=135,in=180,bend right] node[right] {
	\tabular{c}
	 \\
	 
	\endtabular
      } (a.east);
    \end{tikzpicture}
  \end{minipage}
  \vspace{-2ex}
  \caption{The CEGIS and PACES spectrum}
  \label{fig:cegis}
\end{figure}

This procedure is exactly the else branch (i.e.,  procedure) of
an iteration in Algorithm~\ref{algo:main} where 
and  correspond to  and .
Intuitively, the initial variable values in  and the scheduler
choices are the inputs to our concurrent programs.
This suggests that the then branch in Algorithm~\ref{algo:main} could
also be incorporated into the standard CEGIS approach.
This extension (dubbed PACES for {\em Positive and Counter-Examples 
in Synthesis}) to the CEGIS approach is shown in
Figure~\ref{fig:cegis}(right).
Here, the algorithm in each iteration may choose to find an input for
which the program is correct and use the constraints arising
from it.
We discuss the advantages and disadvantages of this approach below.


\vspace{-1ex}
\paragraph{Constraints vs. Inputs.}
A major advantage of using constraints instead of sample inputs is the
possibility of using over- and under-approximations.
As seen in Section~\ref{sec:learn_good}, it is sometimes easier
to work with approximations of constraints due to simplicity
of representation at the cost of potentially missing good solutions.
Another advantage is that the sample inputs may have no simple
representations in some domains. 
The scheduler decisions are one such example -- the
scheduler choices for one program are hard to translate into the
scheduler choices for another.
For example, the original CEGIS for concurrency work~\cite{SLJB08} uses 
ad-hoc trace projection to translate the scheduler choices
between programs.


\vspace{-1ex}
\paragraph{Positive-examples and Counter-examples vs. Counter-examples.}
In standard program-repair tasks, although the faulty
program and the search space  may be large, the
solution program is usually ``near'' the original program, i.e., the fix
is small.
Further, we do not want to change the given program unnecessarily. 
In this case, the use of positive examples and over-approximations of
learned constraints can be used to narrow down the search space quickly.
Another possible advantage comes in the case where the search space
for synthesis is structured (for example, in modular synthesis).
In this case, we can use the correct behaviour displayed by a candidate
solution to fix parts of the search space.


\section{Implementation and Experiments}
\label{sec:impl}

We implemented Algorithm~\ref{algo:main} in our tool ConRepair.
The tool consists of 3300 lines of Scala code and is available at
https://github.com/thorstent/ConRepair.
Model checker CBMC~\cite{cbmc} is used for generating both good and bad
traces, and on an average more than 95\% of the total execution time is
spent in CBMC.
Model checking is far from optimal to obtain good traces, and we expect
that techniques from~\cite{Sen:2008:RDR:1375581.1375584} can be used to
generate good traces much faster.
Our tool can operate in two modes: In ``mixed'' mode it first
analyses good traces and then proceeds to fixing the program. 
The baseline ``badOnly'' mode skips the analysis of good traces
(corresponds to the algorithm in~\cite{cav2013}).

In practice the analysis of bad traces usually generates a large number
of potential reorderings that could fix the bug.
Our original algorithm from~\cite{cav2013} (badOnly {\tt ce1}) prefers
reorderings over atomic sections, but in examples where an atomic
section is the only fix, this algorithm has poor performance.
To address this we implemented a heuristic ({\tt ce2}) that places
atomic sections before having tried all possible reorderings, but this
can result in solutions having unnecessary atomic sections.

The fall back case in Algorithm~\ref{algo:learn_good} severely limits
further fixes -- it forces further fixes involving the same instructions
to be atomic sections.
Hence, in our implementation, we omit this step and prefer an unsound
algorithm (i.e., not necessarily regression-free) that can fix more
programs with reorderings.
While the implemented algorithm is unsound, our experiments show that
even without the fallback, in our examples, there is no regression
except for one artificial example ({\tt ex-regr.c}) constructed
precisely for that purpose.

\vspace{-1ex}
\paragraph{Benchmarks.}
We evaluate our tool on a set of examples that model real bugs found and 
fixed in Linux device drivers by their developers.  
To this end, we explored a history of bug fixes in the drivers subtree
of the Linux kernel and identified concurrency bugs.
We further focused our attention on a subset of 
particularly subtle bugs involving more than two racing threads 
and/or a mix of different synchronization mechanisms, e.g., lock-based 
and lock-free synchronization.  
Approximately 20\% of concurrency bugs that we considered satisfy this
criterion.
Such bugs are particularly tricky to fix either manually or
automatically, as new races or deadlocks can be easily introduced while
eliminating them.
Hence, these bugs are most likely to benefit from good trace analysis.


\begin{wraptable}{r}{0.65\textwidth}
  \small
  \begin{tabular}{l|l|l|l|l}
File & LOC & mixed & {\scriptsize badOnly ce1} & {\scriptsize badOnly ce2} \\
    \hline
    {\tt ex1.c} & 60 & 1 & 2 & 2  \\
    {\tt ex2.c} & 37 & 2 & 5 & 6 \\
    {\tt ex3.c} & 35 & 1 & 2 & 2 \\
    {\tt ex4.c} & 60 & 1 & 2 & 2 \\
    {\tt ex5.c} & 43 & 1 & 8 & 3 \\
    {\tt ex-regr.c} & 30 & 2 & 2 & 2\\
    {\tt paper1.c} & 28 & 1 & 3 & 3\footnote{{\tt ce2} heuristic placed 
  unnecessary atomic section\vspace{-3ex}} \\
    {\tt dv1394.c} & 81 & 1 (13+4s) & 51 (60s) & 5\mpfootnotemark[1] (9s) \\
    {\tt iwl3945.c} & 66 & 1(3+2s) & 2(2s) & 2(2s) \\
    {\tt lc-rc.c} & 40 & 10 (2+7s) & 179 (122s) & 203 (134s) \\
{\tt rtl8169.c} & 405 & 7 (10+45m) & \textgreater100 (\textgreater6h) & 8 (54m) \\
    {\tt usb-serial.c} & 410 & 4 (56+20m) & 6 (38m) & 6 (38m)
  \end{tabular}
  \vspace{-2ex}
  \caption{Results in iterations and time needed.}
  \label{tbl:results}
\end{wraptable}
Table~\ref{tbl:results} shows our experimental results: the iterations
and the wall-clock time needed to find a valid fix for our mixed
algorithm and the two heuristics of the badOnly algorithm. For the mixed
algorithm the time is split into the time needed to generate and
analyse good traces (first number) and the time needed for the fixing
afterwards.

\vspace{-1ex}
\paragraph{Detailed analysis.}
The artificial examples {\tt ex1.c} to {\tt ex5.c} are used for testing
and take only a few seconds;
example {\tt paper1.c} is the one in Figure~\ref{fig:example}.
Example {\tt ex-regr.c} was constructed to show unsoundness of the
implementation.
Example {\tt usb-serial.c} models the USB-to-serial adapter driver.
Here, from the good traces the tool learns that two statements should
not be reordered as it will trigger another bug.
This prompts them to be reordered above a third statement together,
while the badOnly analysis would first move one, find a new bug, and
then fix that by moving the other statement.
Thus, the good trace analysis saves us two rounds of bug fixing and
reduces bug fixing time by 18 minutes.


The  {\tt rtl8169.c} example
models the Realtek 8169 driver containing 5 concurrency bugs.  One of
the reorderings that the tool considers introduces a new bug; further, after
doing the reordering, the atomic section is the only valid fix. 
The good trace analysis discover that the reordering would lead to a
new bug, and thus does the algorithm does not use it.
But, without good traces, the tool uses the faultly reordering and then
{\tt ce1} takes a very long time to search through all possible reorderings
and then discover that an atomic section is required.
The situation is improved when using heuristic {\tt ce2} as it
interrupts the search early.  However, the same heuristic has an adverse 
effect in the {\tt dv1394.c} example: by interrupting  
the search early, it prevents the algorithm from finding a correct 
reordering and inserts an unnecessary atomic section.
The {\tt dv1394.c} example also benefits from good traces in a different
way than the other examples.
Instead of preventing regressions, they are used
to obtain {\em hints} as to what reorderings would provide coverage
for a specific data-flow into assertion edge. 
Then, if a bad trace is encountered and can be fixed by the hinted
reordering, the hinted reordering is preferred over all other possible
ones.
Without hints the {\tt dv1394.c} example
would require 5 iterations. Though hints are not part of our theory they
are a simple and logical extension.

Example {\tt lc-rc.c} models a bug in an ultra-wide 
band driver that requires two reorderings to fix. Though there is initially 
no deadlock, one may easily be introduced when reordering statements.
Here, the good-trace analysis identifies a dependency 
between two {\tt await} statements and learns not to reorder statements
to prevent a deadlock. 
Without good traces, a large number of candidate solutions that cause
a regression are generated.  

\vspace{-1ex}
\section{Conclusion}
We have developed a regression-free algorithm for fixing errors that
are due to concurrent execution of the program. The contributions
include the problem setup (the definitions of program repair for
concurrency, and the regression-free algorithm), the PACES approach
that extends the CEGIS loop with learning from positive examples, and
the analysis of positive examples using data flow to assertions and to
synchronization constructs.   

There are several possible directions for future work. 
One interesting direction is to examine the possibility of
extending the definition of regressions (see
Remark~\ref{rem:regressions} and Example~\ref{ex:non_locality}) -- this
requires going beyond data-flow analysis for learning
regression-preventing constraints.
Another possible extension is to remove the assumption that the
errors are data-independent.
A more pragmatic goal would be to develop a practical version of the
tool for device-driver synthesis starting from the current prototype.


\noindent {\bf Acknowledgements.} We would like to thank Daniel
Kroening and Michael Tautschnig for their prompt help with all our
questions about CBMC. 
We would also like to thank Roderick Bloem, Bettina K\"onighofer and Roopsha Samanta for
fruitful discussions 
regarding repair of concurrent programs.

\bibliographystyle{splncs03}
\bibliography{references}

\newpage

\appendix

\section{The iwl3945 driver.}\label{app:driver}

\begin{figure}[tb]
\ffigbox
{\begin{subfloatrow}[1]
    \ffigbox[\FBwidth+0.5cm]{\begin{minipage}{0.35\textwidth}
      \small{ \tt
config\_thread () \{\\
\phantom{~~}A: lock(rtnl\_lock);\\
\phantom{~~}B: lock(lock);\\
\phantom{~~}C: unlock(lock);\\
\phantom{~~}D: unlock(rtnl\_lock);\\
\}
      }
      \end{minipage}
\quad
      \begin{minipage}{0.4\textwidth}
      \small{ \tt
iwl3945\_bg\_alive\_start\_thread() \{\\
\phantom{~~}1: lock(lock);\\
\phantom{~~}2: restart = 1;\\
\phantom{~~}\\
\phantom{~~}6: unlock(lock);\\
\}
      }
      \end{minipage}
\quad
      \begin{minipage}{0.4\textwidth}
      \small{ \tt
reassoc\_thread () \{\\
\phantom{~~}n: await(notify==1);\\
\phantom{~~}p: assert(restart==1);\\
\}
      }
      \end{minipage}
    }
    {\subcaption{Simplified threads of the iwl3945 driver}\label{fig:iwl3945}}
  \end{subfloatrow}\\
  \begin{subfloatrow}[2]
    
    
    \ffigbox[\FBwidth+0.5cm]{\begin{tikzpicture}[hhilit/.style={draw=black, thick, densely dotted,
    inner xsep=.1em,
    inner ysep=.1em},
    ]
      \tikzstyle{every state}=[draw=black, minimum size=12]


      \node[state]  (1) {{\tt 1}};
      \node[state,below of=1,yshift=0.25cm]  (2) {{\tt 2}};
      \node[state,below of=2,yshift=0.25cm]  (3) {{\tt 3}};
      \node[state,below of=3,yshift=0.25cm]  (4) {{\tt 4}};
      \node[state,below of=4,yshift=0.25cm]  (5) {{\tt 5}};
      \node[state,below of=5,yshift=0.25cm]  (6) {{\tt 6}};
      
      \node[state,left of=2]  (a) {{\tt A}};
      \node[state,below of=a,yshift=0.25cm]  (b) {{\tt B}};
      \node[state,below of=b,yshift=0.25cm]  (c) {{\tt C}};
      \node[state,below of=c,yshift=0.25cm]  (d) {{\tt D}};

      \node[hhilit, fit=(3) (5)] (bo) {};
      
      \path[->]
            (1) edge (a)
            (a) edge (2)
            (2) edge (b)
            (b) edge (3)
            (5) edge (c)
            (c) edge (6)
            (6) edge (d)
            (bo) edge[red,densely dashed,in=330,out=60] node[xshift=0.2cm] {} (2)
            (6) edge[red,densely dashed,in=300,out=40] node[xshift=0.2cm] {} (bo)
      ;
      
    \end{tikzpicture}
    }
    {\subcaption{Possible fixes for this trace}\label{fig:bad}}
    
    \ffigbox[\FBwidth+0.5cm]{\begin{tikzpicture}[hhilit/.style={draw=black, thick, densely dotted,
    inner xsep=.1em,
    inner ysep=.1em},
    ]
      \tikzstyle{every state}=[draw=black, minimum size=12]
      \node[state]  (1) {{\tt 1}};
      \node[state,below of=1,yshift=0.25cm]  (2) {{\tt 2}};
      \node[state,below of=2,yshift=0.25cm]  (3) {{\tt 3}};
      \node[state,below of=3,yshift=0.25cm]  (4) {{\tt 4}};
      \node[state,below of=4,yshift=0.25cm]  (5) {{\tt 5}};
      \node[state,below of=5,yshift=0.25cm]  (6) {{\tt 6}};

      \node[state,right of=3]  (n) {{\tt n}};
      \node[state,below of=n,yshift=0.25cm]  (p) {{\tt p}};
      
      \node[hhilit, fit=(3) (5)] (bo) {};
      
      \path[->]
            (4) edge[blue]  (n)
            (2) edge[red] (p)
            (n) edge (p)
            (2) edge (bo)
      ;
      
      
    \end{tikzpicture}
    }
    {\subcaption{Learning from a good trace}\label{fig:good}}
  \end{subfloatrow}
}
{\caption{Model of deadlock bug in the iwl3945 driver}}
\end{figure}

This example models a real concurrency bug found in the 
Linux driver for the Intel wireless adapter 3945.  The model involves three 
kernel threads calling driver entry points shown in Figure~\ref{fig:iwl3945}.

The driver suffers from a classical ABBA deadlock; if the {\tt config\_thread}
locks {\tt rtnl\_lock} then the {\tt iwl3945\_bg\_alive\_start\_thread} locks
{\tt lock} none of the threads can proceed.

{\tt config\_thread} is located outside the driver and cannot be changed
in order to fix the bug. Furthermore instructions {\tt 3} to {\tt 5} need
to stay together because they are located in a separate function in the
original code.
The {\tt iwl3945\_bg\_alive\_start\_thread} is used to restart the device
if it no longer responds properly. After a restart (line {\tt 2}) the {\tt
reassoc\_thread} is notified (line {\tt 4}), which depends on the device
having completed the restart.

In the actual model the deadlock is modelled with an assertion
between lines {\tt A} and {\tt B} that
tests which thread owns which locks and which thread is waiting for
which locks. The assertion fails if there is a deadlock. This construction
is needed because CBMC does not support deadlock detection.

Our old algorithm {\tt ce1} without a learning from good traces phase now
proceeds to finding a bad trace, such as 
 displayed
in Fig.~\ref{fig:bad}. Note that after an assertion failure the
trace is completed as if the assertion had succeeded in order to find all
reordering options. This leaves a number of bug-fixes to the algorithm: 
We denote only the two important for the description of our algorithms.
The first is to  move
the block {\tt3-5} in front of {\tt 1} (denoted as )
and the second is to move {\tt6} in front of {\tt3-5} (denoted as ).
Without further knowledge the algorithm will choose  for no
particular reason.
This does not fix the deadlock, but it furthermore introduces a regression because
now assertion {\tt p} may fail because the notify signal is sent before the restart
is completed.

The good trace analysis prevents a regression with respect to good traces
such as the trace where all threads are run sequentially. The relevant parts of the
result of our analysis are depicted in Fig.~\ref{fig:good}.
The red and blue edge indicate the variable assignments the assertion
and the await read from respectively. In order to protect the red edge
two black edges are needed, from {\tt2} to the instruction block
{\tt3-5} and from {\tt n} to {\tt p}. When the algorithm comes to the phase of fixing 
bugs it will discover the same two possibilities, but  is blocked
by the black edge from {\tt2} to {\tt3-5}. The algorithm will then
choose the correct fix .

This is also the fix the developers took in the actual driver.
By taking the notify with its needed {\tt rtnl\_lock} out of the {\tt lock} environment
the deadlock is avoided.


\end{document}
