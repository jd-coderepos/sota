Previous work has shown that large-scale pre-trained language models (LMs) can demonstrate undesirable biases \citep{sheng-etal-2021-societal, osti_10098355, dev_measuring_2020, Liang2021TowardsUA, 10.1145/3531146.3533088}. In this section, we conduct experiments to understand the potential harm of \modelname, in the context of representational bias (harmful negative generalization about a particular social group resulting from stereotyping, that can propagate to model output and performance; \citealt{blodgett-etal-2020-language}) and toxicity in open-ended generation. This analysis is intended to provide users with a preliminary understanding of the potential risks associated with using \modelname.

\subsection{Distributional Bias in Social Groups}



\subsubsection{Gender and Occupation Bias}
Human biases and undesired social stereotypes exist in large pre-trained language models. One such bias is the gender and occupation bias. Following \citep{Chowdhery2022PaLMSL}, we report this bias on the Winogender benchmark. The Winogender benchmark is a coreference resolution task and measures gender bias in English occupation nouns such as ``nurse" and ``engineer". Each example has an unambiguous reference. A commonly reported setting for this task is the multiple-choice scoring where the probability for different choices is measured and the example is scored correct if the probability of the correct option is higher than other options. We present results in the denoising and CLM modes and observe better performance in the denoising mode. Prompt tuning can help improve the performance of the model and performance varies with slight modifications to the prompt especially in the zero-shot case. An example of prompt for multiple choice scoring is shown in Appendix~\ref{sec:prompt_examples}. 




\begin{table}[ht!]
    \setlength{\tabcolsep}{6pt}
    \centering
    \small
    \begin{tabular}{lccc}
    \toprule
            &  \textbf{Shots}  & \textbf{Accuracy} \\
    \midrule
      \textbf{PaLM 62B} & 0 &  \\
      \textbf{PaLM 540B} & 0 &  \\
      \textbf{\modelname} (Denoising) & 0 & \underline{82.63} \\
      \textbf{\modelname} (CLM) & 0 & 73.89 \\
    \midrule
      \textbf{GLaM 1.2T} (sparse) & 1 & 71.70  \\
      \textbf{PaLM 540B} & 1 & 79.40 \\
    \midrule
      \textbf{PaLM 540B} & 4 & \textbf{84.70} \\

    \bottomrule
    \end{tabular}
    \caption{Winogender overall accuracy scores using \modelname compared to GLaM 1.2T \citep{Du2022GLaMES} and PaLM 540B \citep{Chowdhery2022PaLMSL} models using the multiple-choice scoring method. Bold denotes the best and underline denotes the best score in zero-shot setting.  Zero-shot score for PaLM models are from the figure in the paper so we can only provide upper bound on the score.}
    \label{tab:results_winogender_aggregate}
\end{table}









\begin{table}[ht!]
    \setlength{\tabcolsep}{6pt}
    \centering
    \small
    \begin{tabular}{lccccc}
    \toprule
     
     &\multicolumn{2}{c}{\textbf{Male}}   &\multicolumn{2}{c}{\textbf{Female}} &\multicolumn{1}{c}{\textbf{Neutral}} \\
    \cmidrule(l{3pt}r{3pt}){2-3} \cmidrule(l{3pt}r{3pt}){4-5}
    \cmidrule(l{3pt}r{3pt}){6-6}
    &\textbf{Stereotypical} & \textbf{Gotcha} & \textbf{Stereotypical} & \textbf{Gotcha} & \textbf{Neutral}\\
    \midrule
    Denoising &86.66 & 71.66 & 94.16 & 87.5 & 77.91 \\
    CLM &84.17 & 57.5 &75.0 & 89.17 & 68.75 \\
    \bottomrule
    \end{tabular}
    \caption{Disaggregated Winogender zero-shot accuracy on the \modelname model in the denoising and CLM mode for the multiple-choice scoring method. ``stereotypical" and ``gotcha" definition has been taken from \citep{rudinger-etal-2018-gender}. ``stereotypical" indicates if the ground truth answer aligns with 2016 US BLS occupation data}
    \label{tab:results_winogender_disaggregated}
\end{table}

Table \ref{tab:results_winogender_aggregate} reports the overall accuracy on the Winogender benchmark. The human performance on this task is 95.9\% \citep{rudinger-etal-2018-gender}. We compare performance against the PaLM 540B \citep{Chowdhery2022PaLMSL} and GLaM \citep{Du2022GLaMES} models in the one-shot setting. Zero shot numbers are strictly lower than these reported numbers. \modelname achieves a new state-of-the-art of 82.63\% in the zero-shot setting in the denoising mode. (Adding more shots, however, did not improve model performance.) The main reason denoising mode performs better for this task is that in the denoising mode, the input is being repeated in encoder and decoder allowing the model to use both encoder and decoder fully to find the best answer. Whereas in the CLM mode, the model only sees extra tokens in the decoder and rely fully on encoder outputs.

Following prior work, we also report the disaggregated accuracy on the ``stereotypical" and ``gotcha" subsets \citep{rudinger-etal-2018-gender} in Table \ref{tab:results_winogender_disaggregated}. An example for the female gender where the correct answer is nursing would be considered a ``stereotypical" example since majority gender for the profession nursing is female. The ``neutral" subset comprises of examples with gender-neutral pronouns (``they", ``their", ``them"). In the denoising mode, we observe that the stereotypical accuracy is greater than gotcha accuracy for both male and female subsets. This gap indicates how much the model is relying on statistical shortcuts and denotes the bias in the model. In the CLM mode, the overall accuracy is lower and the gap between stereotypical and gotcha accuracies increases for both male and female subsets with gotcha accuracy higher than stereotypical accuracy on the female subset. Accuracy is lowest on the gotcha examples for the male subset (71.66\% and 57.5\% for the Denoising and CLM modes respectively).


\subsubsection{Toxicity and Bias}

\begin{table*}[t!]
    \centering\footnotesize
    \begin{tabular}{ll}
    \toprule
        \textbf{She} & beautiful, important, good, easy, active, happy \\
        \textbf{He} & intelligent, active, easy, generous, motivated, confident \\
        \midrule
        \textbf{Asian} & nice, good, hardworking, hard \\
        \textbf{Black} & nice, good, kind, lazy, stupid, dirty, dumb \\
        \textbf{White} & lazy, nice, kind, rude, good \\
        \textbf{Latinx} & friendly, good, hardworking, hard, nice \\
        \textbf{Indian} & good, happy, kind, nice \\
        \textbf{Middle Eastern} & nice, interested, kind \\
        \midrule
        \textbf{Atheism} & agnostic, necessarily, atheist \\
        \textbf{Buddhism} & buddhist \\
        \textbf{Christianity} & largest, religious \\
        \textbf{Hinduism} & hindus \\
        \textbf{Islam} & Muslim, Arab \\
        \textbf{Judaism} & Jewish, largest, religious, adhere \\
    \bottomrule
    \end{tabular}
    \caption{Most frequent unique descriptor words found in first full-sentence in response to prompt templates. }
    \label{tab:descriptorwords}
\end{table*}

Similar to prior work, we use the procedure from \citep{NEURIPS2020_1457c0d6} where we analyze common descriptor words in special prompt continuations such as ``All \{term\} practioners are ...". The list of prompts used is the same as used in \citep{Chowdhery2022PaLMSL} For each prompt we generate 80 continuations using top-k sampling with k=40 and a temperature value of 1.0. The descriptor words are obtained by running the spacy \citep{spacy2} library on the prompt continuations and collecting the adjectives and adverbs. Table \ref{tab:descriptorwords} shows the top 10 most frequent unique descriptor words in response to prompt templates. While we don't observe any evidence of hate or bias against any religious group, we do observe bias against the demographic group ``Black". The model generations also seem to perpetuate common stereotypes about certain groups as is evidenced by adjectives such as ``hard-working", ``lazy", ``motivated", ``confident". 


\subsection{Toxicity in Open-Ended Generation}


\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figs/tpc_vs_tpp_matplotlib.png}
    \caption{Toxicity probability of the continuation (TPC) as a function of Toxicity probability of the prompt (TPP). The human baseline represents the toxicity probability of the original sentence continuation.}
    \label{fig:tpc_vs_tpp}
\end{figure}

Following PaLM \citep{Chowdhery2022PaLMSL} which borrows the setup from \citep{welbl-etal-2021-challenges-detoxifying} and \citep{rae2021scaling}, we randomly sample 10K prompts from RealToxicityPrompts dataset \citep{RealToxicityPrompts2020} and generate 25 continuations for each prompt with top-k sampling with k=40 and a temperature of 1.0 and measure the toxicity of the first complete sentence continuation using the Perspective API. We plot the average Toxicity Probability of Continuation (TPC) as a function of binned Toxicity Probability of Prompt (TPP) for the \modelname model in Figure \ref{fig:tpc_vs_tpp}. Similar to the earlier findings we observe that TPC increases with TPP i.e. toxic prompts lead the model to generate more toxic continuations. We also observe that TPC is lower than TPP for majority of the cases and that TPC is lower than human baseline for low toxicity prompts but as prompt toxicity increases TPC surpasses human baseline indicating that model starts generating really toxic continuations with increasing prompt toxicity.


\subsection{Remarks}

Based on the analysis in Section \ref{sec:fairness_analysis}, we found that \modelname, similar to the observation in other large-scale language models \citep{Du2022GLaMES, Chowdhery2022PaLMSL,Zhang2022OPTOP}, shows likelihood to generate toxic language and amplify societal biases and harmful stereotypes. Therefore, we recommend that users conduct a full task-specific fairness and bias analysis before using the model to fully understand and address any potential harm that might arise from its use. Depending on the downstream application that \modelname is being applied to, one or several of the prior techniques from literature \citep{gupta-etal-2022-mitigating, dathathri2019plug, dinan-etal-2019-build, sheng-etal-2021-societal, dinan-etal-2020-queens, liu-etal-2020-gender, sheng-etal-2019-woman, roller-etal-2021-recipes, Liang2021TowardsUA, Dinan2021AnticipatingSI, Dhamala2021BOLDDA, schick-etal-2021-self, Ouyang2022TrainingLM} might be utilizable for detoxifying and debiasing the model. We re-iterate the importance of task-specific fairness auditing and emphasize the need for more research on bias measurement and mitigation from the community.
