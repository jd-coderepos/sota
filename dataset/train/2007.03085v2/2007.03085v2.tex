\documentclass{article}

\PassOptionsToPackage{numbers, compress,sort}{natbib}

\usepackage[final]{style_2020}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}




\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      

\usepackage{wrapfig}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption} \usepackage{xcolor}
\usepackage[ruled, vlined]{algorithm2e}

\usepackage{array}
\usepackage{xcolor}
\definecolor{lightpurple}{RGB}{178,145,226}
\definecolor{lightblue}{RGB}{0,128,255}
\definecolor{yellow}{rgb}{1,1,0}
\definecolor{dandelion}{rgb}{0.94, 0.88, 0.19}
\definecolor{carnationpink}{rgb}{1.0, 0.65, 0.79}

\newcommand*{\rowstyle}[1]{\gdef\@rowstyle{#1}\@rowstyle\ignorespaces }
\newcolumntype{=}{>{\gdef\@rowstyle{}}}
\newcolumntype{+}{>{\@rowstyle}}

\usepackage{amssymb}
\usepackage{amsmath,amsfonts}
\usepackage{amsopn}
\usepackage{bm} \usepackage{multirow}

\newcommand{\vct}[1]{\boldsymbol{#1}} \newcommand{\mat}[1]{\boldsymbol{#1}} \newcommand{\cst}[1]{\mathsf{#1}}  

\newcommand{\field}[1]{\mathbb{#1}}
\newcommand{\R}{\field{R}} \newcommand{\C}{\field{C}} \newcommand{\F}{\field{F}} \newcommand{\I}{\field{I}} \newcommand{\T}{^{\textrm T}} 


\newcommand{\inner}[2]{#1\cdot #2}
\newcommand{\norm}[1]{\|#1\|}
\newcommand{\twonorm}[1]{\left\|#1\right\|_2^2}
\newcommand{\onenorm}[1]{\|#1\|_1}
\newcommand{\Map}[1]{\mathcal{#1}}  

\newcommand{\ProbOpr}[1]{\mathbb{#1}}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{}\mkern2mu{#1#2}}}
\newcommand{\ind}[2]{{#1} \independent{#2}}
\newcommand{\cind}[3]{{#1} \independent{#2}\,|\,#3} \newcommand{\expect}[2]{\ifthenelse{\equal{#2}{}}{\ProbOpr{E}_{#1}}
{\ifthenelse{\equal{#1}{}}{\ProbOpr{E}\left[#2\right]}{\ProbOpr{E}_{#1}\left[#2\right]}}} \newcommand{\var}[2]{\ifthenelse{\equal{#2}{}}{\ProbOpr{VAR}_{#1}}
{\ifthenelse{\equal{#1}{}}{\ProbOpr{VAR}\left[#2\right]}{\ProbOpr{VAR}_{#1}\left[#2\right]}}} \newcommand{\cndexp}[2]{\ProbOpr{E}\,[ #1\,|\,#2\,]}  

\DeclareMathOperator*{\argmax}{\mathrm{argmax}}
\DeclareMathOperator*{\argmin}{\mathrm{argmin}}
\DeclareMathOperator*{\softmax}{softmax}

\newcommand{\trace}[1]{\operatornamewithlimits{tr}\left\{#1\right\}}
\newcommand{\diag}{\operatornamewithlimits{diag}}
\newcommand{\sign}{\operatornamewithlimits{sign}}
\newcommand{\const}{\operatornamewithlimits{const}}

\newcommand{\parde}[2]{\frac{\partial #1}{\partial  #2}}



\newcommand{\vtheta}{\vct{\theta}}
\newcommand{\vmu}{\vct{\mu}}
\newcommand{\vc}{\vct{c}}
\newcommand{\vp}{\vct{p}}
\newcommand{\vq}{\vct{q}}
\newcommand{\vx}{{\vct{x}}}
\newcommand{\vy}{\vct{y}}
\newcommand{\vz}{{\vct{z}}}
\newcommand{\vu}{\vct{u}}
\newcommand{\vo}{{\vct{o}}}
\newcommand{\va}{\vct{a}}
\newcommand{\vb}{\vct{b}}
\newcommand{\vr}{\vct{r}}
\newcommand{\vt}{\vct{t}}
\newcommand{\vv}{\vct{v}}
\newcommand{\vw}{\vct{w}}
\newcommand{\vzero}{\vct{0}}
\newcommand{\vf}{\vct{f}}
\newcommand{\vh}{\vct{h}}
\newcommand{\vg}{\vct{g}}
\newcommand{\vphi}{\vct{\phi}}
\newcommand{\vpsi}{\vct{\psi}}
\newcommand{\ones}{\vct{1}}
\newcommand{\mU}{\mat{U}}
\newcommand{\mA}{\mat{A}}
\newcommand{\mB}{\mat{B}}
\newcommand{\mD}{\mat{D}}
\newcommand{\mW}{\mat{W}}
\newcommand{\mH}{\mat{H}}
\newcommand{\mS}{\mat{S}}
\newcommand{\mJ}{\mat{J}}
\newcommand{\mM}{\mat{M}}
\newcommand{\mT}{\mat{T}}
\newcommand{\mZ}{\mat{Z}}
\newcommand{\mO}{\mat{O}}
\newcommand{\mY}{\mat{Y}}
\newcommand{\mL}{\mat{L}}
\newcommand{\mI}{\mat{I}}
\newcommand{\mK}{\mat{K}}
\newcommand{\mSigma}{\mat{\Sigma}}
\newcommand{\mOmega}{\mat{\Omega}}
\newcommand{\cC}{\cst{C}}
\newcommand{\cM}{\cst{M}}
\newcommand{\cN}{\cst{N}}
\newcommand{\cQ}{\cst{Q}}
\newcommand{\cD}{\cst{D}}
\newcommand{\cL}{\cst{L}}
\newcommand{\cK}{\cst{K}}
\newcommand{\cH}{\cst{H}}
\newcommand{\cR}{\cst{R}}
\newcommand{\cU}{\cst{U}}
\newcommand{\cS}{\cst{S}}
\newcommand{\sQ}{\mathcal{Q}}
\newcommand{\sS}{\mathcal{S}}
\newcommand{\sF}{\mathcal{F}}
\newcommand{\sC}{\mathcal{C}}


\newcommand{\nx}{\tilde{x}}
\newcommand{\vnx}{{\tilde{\vx}}}
\newcommand{\vnz}{{\tilde{\vz}}}
\newcommand{\deltavx}{\delta_\vx}
\newcommand{\vmx}{\bar{\vx}}
\newcommand{\vmz}{\bar{\vz}}
\newcommand{\sigmax}{\mSigma_{\vx}}
\newcommand{\sigmaz}{\mSigma_{\vz}}
\newcommand{\no}{\tilde{o}}
\newcommand{\vno}{{\tilde{\vo}}}
\newcommand{\nell}{\tilde{\ell}}
\newcommand{\jacob}{\mat{J}}
\newcommand{\hess}{\mat{H}}
\newcommand{\mloss}{\hat{\ell}}

\newcommand{\eat}[1]{}
\newcommand{\method}[1]{\textsc{#1}}
\newcommand{\task}[1]{\textbf{#1}}
\newcommand{\GDC}{\method{GDC}\xspace}
\newcommand{\SDN}{\method{SDN}\xspace}
\newcommand{\GSTD}{\method{GS3D}\xspace}
\newcommand{\vPIXOR}{\method{PIXOR}\xspace}
\newcommand{\PIXOR}{\method{PIXOR}\xspace}
\newcommand{\PRCNN}{\method{P-RCNN}\xspace}
\newcommand{\SRCNN}{\method{S-RCNN}\xspace}
\newcommand{\PL}{\method{pseudo-LiDAR}\xspace}
\newcommand{\AVOD}{\method{AVOD}\xspace}
\newcommand{\Frustum}{\method{F-PointNet}\xspace}
\newcommand{\Mono}{\method{Mono3D}\xspace}
\newcommand{\DOP}{\method{3DOP}\xspace}
\newcommand{\DORN}{\method{DORN}\xspace}
\newcommand{\MLF}{\method{MLF}\xspace}
\newcommand{\MLFmono}{\method{MLF-mono}\xspace}
\newcommand{\MLFstereo}{\method{MLF-stereo}\xspace}
\newcommand{\DispRCNN}{\method{Disp R-CNN}\xspace}

\newcommand{\APBEV}{AP\xspace}
\newcommand{\AP}{AP\xspace}

\newcommand{\PSMNet}{\method{PSMNet}\xspace}
\newcommand{\PSMNetpd}{\method{PSMNet}\xspace}
\newcommand{\DispNet}{\method{DispNet}\xspace}
\newcommand{\DispNetS}{\method{DispNet-S}\xspace}
\newcommand{\DispNetC}{\method{DispNet-C}\xspace}
\newcommand{\SPSstereo}{\method{SPS-stereo}\xspace}
\newcommand{\PnP}{\method{PnP}\xspace}

\newcommand{\RTD}{\method{RT3DStereo}\xspace}
\newcommand{\OCS}{\method{OC-Stereo}\xspace}
\newcommand{\ETE}{\method{E2E-PL}\xspace}
\newcommand{\DSGN}{\method{DSGN}\xspace}
\newcommand{\GANet}{\method{GANet}\xspace}
\newcommand{\CDN}{\textbf{\method{CDN}}\xspace}
 
\renewcommand{\paragraph}[1]{\vspace{-0.5ex}\textbf{#1}}
\newcommand{\ie}{i.e.\xspace}
\newcommand{\eg}{e.g.\xspace}


\title{Wasserstein Distances for Stereo Disparity Estimation}


\author{Divyansh Garg \hspace{10pt}Yan Wang
	\hspace{10pt}Bharath Hariharan \\ 
	\textbf{Mark Campbell \hspace{10pt}Kilian Q. Weinberger \hspace{10pt}Wei-Lun Chao}\\
	Cornell University, Ithaca, NY \hspace{20pt}The Ohio State University, Columbus, OH\\
	{\tt\small \{dg595, yw763, bh497, mc288, kqw4\}@cornell.edu}
	\hspace{20pt}{\tt\small chao.209@osu.edu}
}



\begin{document}

\maketitle

\begin{abstract}
Existing approaches to depth or disparity estimation output a distribution over a set of pre-defined discrete values. 
This leads to inaccurate results when the true depth or disparity does not match any of these values.
The fact that this distribution is usually learned indirectly through a regression loss causes further problems in ambiguous regions around object boundaries.
We address these issues using a new neural network architecture that is capable of outputting arbitrary depth values, and a new loss function that is derived from the Wasserstein distance between the true and the predicted distributions.
We validate our approach on a variety of tasks, including stereo disparity and depth estimation, and the downstream 3D object detection. Our approach drastically reduces the error in ambiguous regions, especially around object boundaries that greatly affect the localization of objects in 3D, achieving the state-of-the-art in 3D object detection for autonomous driving.
Our code will be available at \url{https://github.com/Div99/W-Stereo-Disp}.
\end{abstract}
 \section{Introduction}
\label{sec: intro}

\begin{wrapfigure}{r}{0.5\textwidth}
		\vspace{-32pt}
		\includegraphics[width=\linewidth]{figures/teaser_new.pdf}
		\vspace{-10pt}
		\caption{\textbf{The effect of our continuous disparity network (CDN).} We show a person (green box) in front of a wall. The blue 3D points are obtained using PSMNet~\cite{chang2018pyramid}. The red points from our \textbf{CDN} model are much better aligned with the shape of the objects: they do not suffer the streaking artifacts near edges. Yellow points are from the ground truth LiDAR. (One floor square is 1m1m.) \label{fig:teaser}}
		\vskip -10pt
\end{wrapfigure}

Depth estimation from stereo images is a long-standing task in computer vision~\cite{scharstein2003high,papandreou2017towards}.
It is a key component of many downstream problems, ranging from 3D object detection in autonomous vehicles~\cite{pseudoLiDAR,you2019pseudo,chen2020dsgn,li2019stereo,qian2020end} to graphics applications such as novel view generation~\cite{yu2019depth,liu2018geometry}.
The importance of this task in practical applications has led to a flurry of recent research.
Convolutional networks have now superseded more classical techniques and led to significant improvements in accuracy~\cite{wang2018anytime,chang2018pyramid,mayer2016large,zhang2019ga}.

These techniques estimate depth by finding accurate pixel correspondences and estimating the \emph{disparity} between their -coordinates, which is inversely proportional to depth.
Because pixels have integral coordinates, so does the estimated disparity --- causing even the resulting depth estimates to be discrete. 
This introduces inaccuracy, as the ground truth disparity and depth are naturally real-valued. This discrepancy is typically addressed by predicting a \emph{categorical distribution} over a fixed set of discrete values, and then computing the \emph{expected} depth from this distribution, which can in theory be any arbitrary real value (within the range of the set)~\cite{wang2018anytime,chang2018pyramid,zhang2019ga,kendall2017end,you2019pseudo}.


In this paper, we argue that such a design choice may lead to inaccurate depth estimates, especially around object boundaries.
For example, in \autoref{fig:teaser} we show the pixels (back-projected into 3D using the depth estimates) along the boundary between a person in the foreground at m depth and a wall in the background at m depth. The predicted depth distribution of these border pixels is likely to be multi-modal, having two peaks around  and  meters. Simply taking the mean outputs a low probability value in between the two modes (\eg, m). 
Such ``smoothed'' depth estimates can have a strong negative impact on subsequent 3D object detection, as they  ``smear'' the pedestrian around the edges towards the background (note the many blue points between the wall and the pedestrian). A bounding box including all these trailing points, far from the actual person, would strongly misrepresent the scene's geometry. What may further aggravate the problem is how the distribution is usually learned. 
Existing approaches mostly learn the distribution via a regression loss: minimizing the distance between the mean value and the ground truth~\cite{kendall2017end,you2019pseudo}. 
In other words, there is no direct supervision to teach the model to assign higher probabilities around the truth depth.

To address these issues, we propose a novel neural network architecture for stereo disparity estimation that is capable of outputting a distribution over \emph{arbitrary} disparity values, from which we can directly take the mode and bypass the mean. 
As with existing work, our model predicts a probability for each disparity value in a pre-defined, discrete set.
Additionally, it predicts a real-valued \emph{offset} for each discrete value.
This is a simple architectural modification, but it has a profound impact.
With these offsets, the output is converted from a discrete categorical distribution to a \emph{continuous} distribution 
 over disparity values: a  mixture of Dirac delta functions, centered at the pre-defined discrete values shifted by predicted offsets\footnote{Our work is reminiscent of G-RMI pose estimator~\cite{papandreou2017towards}, which predicts the heatmaps (at fixed locations) and offsets for each keypoint. Our work is also related to one-stage object detectors~\cite{redmon2016you,liu2016ssd,lin2017focal} that predict the class probabilities and box offsets for each anchor box.}.
This simple addition of predicted offsets allows us to use the mode as the prediction during inference, instead of the mean, guaranteeing that the predicted depth has a high estimated probability. \autoref{fig:illustration} illustrates our model, \textbf{continuous disparity network (CDN)}.


Next, we propose a novel loss function that provides a more informative objective during training. Concretely, we allow uni- or multi-modal ground truth depth distributions (obtained from nearby pixels) and represent them as (mixtures of) Dirac delta functions. The learning objective is then to minimize the divergence between the predicted and the ground truth distributions.
Noting that the two distributions might not have a common support, we apply the Wasserstein distance~\cite{villani2008optimal} to measure the divergence. While computing the exact Wasserstein distance of arbitrary distributions can be time-consuming, computing it for one-dimensional distributions (\eg, distributions of one-dimensional disparity) enjoys efficient solutions, creating negligible training overhead. 

Our proposed approach is both mathematically well-founded and practically extremely simple.
It is compatible with most existing stereo depth or disparity estimation approaches --- we only need to add an additional offset branch and replace the commonly used regression loss by the Wasserstein distance.
We validate our approach using multiple existing stereo networks~\cite{chang2018pyramid,you2019pseudo,zhang2019ga} on three tasks: stereo disparity estimation~\cite{mayer2016large}, stereo depth estimation~\cite{geiger2012we}, and 3D object detection~\cite{geiger2012we}. The last is a downstream task using stereo depth as the input to detect objects in 3D. We conduct comprehensive experiments and show that our algorithm leads to significant improvement in all three tasks.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{figures/pipelinefig6.pdf}
\vskip-5pt
\caption{\small\textbf{Continuous disparity network (CDN).} We propose to predict a \emph{real-value} offset ({\color{dandelion}yellow arrows}) for each pre-defined discrete disparity value (\eg, ), turning a categorical distribution ({\color{carnationpink}magenta bars}) to a continuous distribution ({\color{red}red bars}), from which we can output the mode disparity for accurate estimation.}
\label{fig:illustration}
\vskip-10pt
\end{figure}


 \section{Background}
\label{sec: related}



Stereo techniques rely on two cameras oriented parallel and translated horizontally relative to each other~\cite{yamaguchi2014efficient,zbontar2016stereo}.
In this setting, for a pixel  in one image, the corresponding pixel in the second image is constrained to be at , where  is called the \emph{disparity} of the pixel.
The disparity is inversely proportional to the \emph{depth}  : , where  is the translation between the cameras (called the \emph{baseline}) and  is the focal length of the cameras. Stereo depth estimation techniques typically first estimate disparity in units of pixels and then exploit the reciprocal relationship to approximate depth. 
The basic approach is to compare pixels  in the left image  with pixels  in the right image  for different values of , and find the best match.
Since pixel coordinates are constrained to be integers,  is constrained to be an integer as well.
The estimated disparity is thus an integer, forcing the estimated depth to be one of a few discrete values.

Instead of producing a single integer-valued disparity value, modern pipelines produce a \emph{distribution} over these possible disparities~\cite{kendall2017end,chang2018pyramid}.
They do this by constructing a 4D \emph{disparity feature volume}, , in which  is a feature vector that captures the difference in appearance between  and .
This feature vector can be, for instance, the concatenation of the feature vectors of the two pixels, in turn obtained by running a convolutional network on each image.
The disparity feature volume is then passed through a series of 3D convolutional layers, culminating in a cost for each disparity value  for each pixel, ~\cite{chang2018pyramid}.
By taking  along the disparity dimension, one can turn  into a probability distribution~\cite{luo2016efficient}.
Because we only consider integral disparity values, this distribution is a categorical distribution over the  possible disparity values (\eg, ).
One can then obtain the disparity , for example, by . However, in order to obtain continuous disparity estimates beyond integer-valued disparities, \cite{chang2018pyramid,wang2018anytime,kendall2017end,zhang2019ga} apply the following weighted combination (\ie, mean),

The whole neural network can be learned end-to-end, including the image feature extractor and 3D convolution kernels, to minimize the disparity error (on one image)

where  is the smooth L1 loss,  is the ground truth map, and  contains pixels with ground truths. 





Recently, \cite{you2019pseudo} argue that learning with \autoref{eq_disp_loss} may over-emphasize nearby depths, and accordingly propose to learn the network directly to minimize the depth loss. Specifically, they constructed depth cost volume  , rather than , and predicted the continuous depth by 
The entire network is learned to minimize the distance to the ground truth depth map 

In this paper, we argue that the design choices to output continuous values (\autoref{eq_disp_pred} and \autoref{eq_depth_pred}) can be harmful to pixels in ambiguous regions, and the objective functions for learning the networks (\autoref{eq_disp_loss} and \autoref{eq_depth_loss}) do not directly match the predicted distribution to the true one. The most similar work to ours is~\cite{zhang2020adaptive}, which learns the network with a distribution matching loss on ; however, they still need to apply~\autoref{eq_disp_pred} to obtain continuous estimates. \citet{luo2016efficient} also learned the network by distribution matching, but applied post-processing (\eg, semi global block matching) to obtain continuous estimates.

\paragraph{Stereo-based 3D object detection.} 3D object detection has attracted significant attention recently, especially for the application of self-driving cars~\cite{geiger2012we,geiger2013vision,argoverse,nuscenes2019,waymo_open_dataset,lyft2019}. While many algorithms rely on the expensive LiDAR sensor as input~\cite{shi2019pointrcnn,lang2019pointpillars,yang2018pixor,qi2018frustum,ku2018joint}, several recent papers have shown promising accuracy using the much cheaper stereo images~\cite{li2019gs3d,xu2018multi,chen2020dsgn,li2019stereo,konigshofrealtime,xu2020zoomnet,pon2019object}. One particular framework is Pseudo-LiDAR~\cite{pseudoLiDAR,you2019pseudo,qian2020end}, which converts stereo depth estimates into a 3D point cloud that can be inputted to any existing LiDAR-based detector, achieving the state-of-the-art results.



 \section{Disparity Estimation}
\label{sec: approach}

\textit{For brevity, in the following we mainly discuss disparity estimation. The same technique can easily be applied to depth estimation, which is usually adapted from their disparity estimation counterparts.}


As reviewed in~\autoref{sec: related}, many existing stereo networks output a distribution of disparities at each pixel. 
This distribution is a categorical distribution over discrete disparity values: discrete because they are estimated as the difference in -coordinates of corresponding pixels, and as such are integers.
Stereo techniques then compute the mean of the distribution to obtain a continuous estimate that is not limited to integral values.

\begin{wrapfigure}{r}{0.7\textwidth}
\vskip -17pt
\includegraphics[width=\linewidth]{figures/distribution.pdf}
\vskip -5pt
\caption{\small \textbf{The predicted disparity posterior for a pixel on object boundaries.} The uni-modal assumption can break down, leading to a  \emph{mean} estimate that is in a low probability region. Learning offsets allow us to predict the continuous \emph{mode}. (Offsets are in  here.) 
} \label{fig:depth_pred}
\vskip -5pt
\end{wrapfigure}



We point out two disadvantages of taking the mean estimate. First, the mean value can deviate from the mode and may wrongly predict values of low probability when the predicted distribution is multi-modal (see~\autoref{fig:depth_pred}). Such multi-modal distributions appear frequently at pixels around the object boundaries. While they collectively occupy only a tiny portion of image pixels, recent studies have shown their particular importance in the downstream tasks like 3D object detection~\cite{li2019stereo,qian2020end,li2018stereo}.
For instance, let us consider a street scene where a car m away (a disparity of, say, 10 pixels) is driving on the road towards the camera, with the sky as the background. The pixels on the car boundary can either take a disparity of around 10 pixels (for the car) or a disparity of 0 pixels (for the sky). Simply taking the mean likely produces arbitrary disparity estimates between these values, producing depth estimates that are neither on the car nor on the background. The downstream 3D object detector can, therefore, wrongly predict the car orientation and size, potentially leading to accidents. Second, 
the physical meaning of the mean value is by no means aligned with the true disparity: uncertainty in correspondence might yield a  chance of a disparity of 10 pixels and a  chance for a disparity of 20 pixels,  but this does not mean that the disparity should be 16 pixels. 

Instead, a more straightforward way to simultaneously model the uncertainty and output continuous disparity estimates is to extend the support of the output distribution beyond integers.

\subsection{Continuous disparity network (CDN)}
\label{ssec:CDN}
To this end, we propose a new neural network architecture and output representation for disparity estimation.
The output of our network will still be a set of discrete values with corresponding probabilities, but the discrete values \emph{will not be restricted to integers}.
The key idea is to start with integral disparity values, and predict \emph{offsets} in addition to probabilities.



Denote by  the set of integral disparity values.
As above, disparity estimation techniques produce a cost  for every .
A softmax converts this cost into a probability distribution:


We propose to add a sub-network  that predicts an offset disparity value for each integral disparity value  at each pixel .
We use this to displace the probability mass at  to .
This results in the following probability distribution:
\iffalse

\fi

which is a mixture of Dirac delta functions over arbitrary disparity values .
In other words,  has  supports, each located at  with a weight . The resulting continuous disparity estimate  at  is the \textbf{mode} of .

Our network design with a sub-network for offset prediction is reminiscent of G-RMI pose estimator~\cite{papandreou2017towards} and one-stage 2D object detectors~\cite{liu2016ssd,redmon2016you,lin2017focal}. The former 
predicts the heatmaps (at fixed locations) and offsets for each keypoint; the latter parameterizes the predicted bounding box coordinates by the anchor box location plus the predicted offset. One may also interpret our approach as a coarse-to-fine depth prediction, first picking the bin centered around  and then locally adjusting it by an offset.

In our implementation, the sub-network  shares its feature and computation with  except for the last block of  fully-connected or convolutional layers. 


\subsection{Learning with Wasserstein distances}
\label{ssec:LWWD}
We propose to train our disparity network such that the mixture of Dirac delta functions (\autoref{e_mix_Dirac}) is directly learned to match the ground truth distribution. Concretely,
we represent the distribution of ground truth disparity at a pixel , , as a Dirac delta function centered at the ground truth disparity  : . We then employ a learning objective to minimize the divergence (distance) between  and . There are many popular divergence measures between distributions, such as Kullback-Leibler divergence, Jensen-Shannon divergence, total Variation, the Wasserstein distance, etc. In this paper, we choose the Wasserstein distance for one particular reason:  and  may not have any common supports.

The Wasserstein- distance between two distributions ,  over a metric space  is defined as

where  denotes the set of all the joint distributions  whose marginal distributions   and  are exactly  and , respectively. Intuitively,  indicates how much ``mass'' to be transported from  to  in order to transform the distribution  to .

Estimating the Wasserstein distance is usually non-trivial and requires solving a linear programming problem. One particular exception is when  and  are both distributions of one-dimensional variables, which is the case for our distribution over disparity values\footnote{For dealing with disparity or depth values at a pixel, our metric space naturally becomes .}. 
Specifically, when  is a Dirac delta function whose support is located at , the Wasserstein- distance can be simplified as

By plugging  and  into  and  respectively, we obtain

based on which we can learn the conventional disparity network ({\color{red}red}) and the additional offset sub-network ({\color{blue}blue}) jointly (\ie, by minimizing~\autoref{e_loss_WD}). We focus on  and  distances.


\subsection{Extension: learning with multi-modal ground truths}
\label{sec:mm-ext}

One particular advantage of learning to match the distributions is the capability of allowing multiple ground truth values (\ie, a multi-modal ground truth distribution) at a single pixel location.
Denote  as the set of ground truth disparity values at a pixel , the ground truth distribution becomes

Since  is not a Dirac delta function, we can no longer apply \autoref{e_WD_Dirac} but the following equation for comparing two one-dimensional distributions \cite{panaretos2020invitation,Wasserman20,ramdas2017wasserstein}

where  and  are the cumulative distribution functions (CDFs) of  and , respectively. For the case , we can rewrite \autoref{e_WD_one} as \cite{thorpeintroduction}

We note that, both \autoref{e_WD_one} and \autoref{e_WD_one_one} can be computed efficiently.

While existing datasets do not provide multi-modal ground truths directly, we investigate the following procedure to construct them. For each pixel, we consider a  neighborhood and create a multi-modal distribution by setting the center-pixel disparity with a weight  and the remaining ones each with . We set  and  in the experiment. Our empirical study shows that using a multi-modal ground truth leads to a much faster model convergence.

\subsection{Comparisons to related work}
\citet{kendall2017end} discussed the use of means or modes. They employed pre-scaling to sharpen the predicted probability, which might resolve the multi-modal issue but makes the prediction concentrate on discrete disparity values. In contrast, we do not prevent predicting a multi-modal distribution, especially for pixels whose disparities are inherently multi-modal. We output the mode (after an offset), which is what \citet{kendall2017end} hoped to achieve.
We note that 3D convolutions can smooth the estimation but cannot guarantee uni-modal distributions. 

Compared to G-RMI pose estimator and one-stage 2D object detectors mentioned in~\autoref{ssec:CDN}, our work learns the two (sub-)networks jointly using a single objective function rather than a combination of two separated ones. See the supplementary material for more comparisons. \citet{liu2019conservative} propose to use the Wasserstein loss for pose estimation to characterize the inter-class correlations; however, they do not predict offsets for pre-defined discrete pose labels. Our work is also related to~\cite{bellemare2017distributional}, in which the authors propose to learn the value distribution, instead of the expected value, using the Wasserstein loss for reinforcement learning.

 \section{Experiments}
\label{sec: exp}

 


\subsection{Datasets and metrics}
\paragraph{Datasets.} We evaluate our method on two challenging stereo benchmark datasets, i.e., Scene Flow~\cite{mayer2016large} and KITTI 2015~\cite{menze2015object}, and on a 3D object detection
benchmark KITTI 3D~\cite{geiger2013vision,geiger2012we}.

\paragraph{1) Scene Flow~\cite{mayer2016large}.} 
Scene Flow is a large synthetic dataset containing 35,454 training image pairs and 4,370 testing image pairs, where the ground truth disparity maps are densely provided, which is large enough for directly training deep neural networks.

\paragraph{2) KITTI 2015~\cite{menze2015object}.} 
KITTI 2015 is a  real-world dataset with street scenes captured from a driving car. KITTI 2015 contains 200 training stereo image pairs with sparse ground truth disparities obtained using LiDAR, and 200 testing image pairs with ground truth disparities held by evaluation server for submission evaluation only. Its small size makes it a challenging dataset. 

\paragraph{3) KITTI 3D~\cite{geiger2013vision,geiger2012we}.}
KITTI 3D contains 7,481 (pairs of) images for training and 7,518 (pairs of) images for testing. We follow the
same training and validation splits as suggested by~\citet{chen20153d}, containing 3,712 and 3,769 images, respectively. For each image, KITTI provides the corresponding Velodyne LiDAR point cloud (for sparse depth ground truths), camera calibration matrices, and 3D bounding box annotations. We evaluate our approach by plugging it into existing stereo-based 3D object detectors~\cite{you2019pseudo,pseudoLiDAR,chen2020dsgn}, which all require stereo depth estimation as a key component.

\paragraph{Metrics.} We evaluate our methods on three tasks: stereo disparity estimation, stereo depth estimation, and 3D object detection. We apply the corresponding standard metrics listed as follows.

\textbf{1) stereo disparity}, we use two standard metrics: End-Point-Error (EPE), \ie, the average difference of the predicted disparities and their true
ones, and -Pixel Threshold Error (PE), \ie, the percentage of pixels for which the predicted disparity is off the ground truth by more than  pixels. We use the 1-pixel and 3-pixel threshold errors, denoted as 1PE and 3PE. PE is robust to outliers with large disparity errors, while EPE measures errors to sub-pixel level. 

\textbf{2) stereo depth.} We use the Root Mean Square Error (RMSE)  and Absolute Relative Error (ABSR) , where  denotes all the pixels having ground truths, and  and  are estimated depth and ground truth depth respectively.

\textbf{3) 3D object detection.} We focus on 3D and bird’s-eye-view (BEV) localization and report the results on the official leader board and the validation set. Specifically, we focus on the “car” category, following~\cite{chen2017multi, xu2018pointfusion}. We report the average precision (AP) at IoU thresholds 0.5 and 0.7. We denote AP for the 3D and BEV tasks by \AP and \APBEV, respectively. The benchmark defines for each category three cases --- easy, moderate, and
hard -- according to the bounding box height and occlusion and truncation. In general, the easy cases correspond to cars within 30 meters of the ego-car distance. 

\subsection{Implementation details}
\label{ssec:ID}
We mainly use the Wasserstein-1 distance (\ie,  loss) for training our \CDN model. We compare  and  losses in the supplementary material.

\paragraph{Stereo disparity.}
We apply our \textbf{continuous disparity network (\CDN)} architecture to PSMNet~\cite{chang2018pyramid} and GANet~\cite{zhang2019ga}, namely \CDN-\PSMNet and \CDN-\GANet.
To keep a fair comparison, we train the models with their default settings.
For Scene Flow, the models are trained from
scratch with a constant learning rate of 0.001 for 10 epochs. For KITTI 2015, the models pre-trained on Scene Flow are fine-tuned following the default strategy of the vanilla models.
We consider disparities in the range of  for both datasets.  We use a uniform grid of bin size  pixels to create the categorical distribution (cf.~\autoref{eq_cat}).
We show the effect of bin sizes in the supplementary material.

\paragraph{Stereo depth.}
We apply \CDN to the \SDN architecture \cite{you2019pseudo}, namely \CDN-\SDN.
We follow the training procedure in \cite{you2019pseudo}. We consider depths in the range of .  We use a uniform grid of bin size m to create the categorical distribution.

\paragraph{The offset sub-network.}
We implement  with a Conv3D-Relu-Conv3D block. It takes the 4D cost volume, before the last fully-connected or convolutional block of , as the input. We predict a single offset  for each integral disparity value , where  is the bin size. We achieve this by clipping. The sub-network has 30K parameters, only  w.r.t. \PSMNet~\cite{chang2018pyramid}. For stereo depth, we implement  in the same way for each integral depth value . 

\paragraph{Stereo 3D object detection.}
We apply \CDN-\SDN to \PL++ \cite{you2019pseudo}, which uses \SDN to estimate depth. We fine-tune the \CDN-\SDN model pre-trained on Scene Flow on KITTI 3D dataset, followed by using an 3D object detector, here \PRCNN~\cite{shi2019pointrcnn}, to detect 3D bounding boxes of cars. We also apply \CDN to \DSGN~\cite{chen2020dsgn}, the state-of-the-art stereo-based 3D object detector. \DSGN uses as a backbone depth estimator based on \PSMNet and we replace it with our \CDN version.



\paragraph{Multi-modal ground truths.}
As mentioned in \autoref{sec:mm-ext}, we create multi-modal ground truths for a pixel by considering a patch in its  neighborhood. We give the center-pixel disparity a weight , and the remaining ones an equal weight such that the total sums to 1. In this case, we use \autoref{e_WD_one_one} as the loss function. We implement a differentiable loss module in Pytorch that can be applied to a batch of image tensors. Please see the supplementary material for more details. 





\begin{table}[]
	\centering
	\small
	\caption{\small \textbf{Disparity results}. We report results on Scene Flow and KITTI 2015. For Scene Flow, end point errors (EPE) and the 1-pixel and 3-pixel threshold error rates (1PE, 3PE) are reported. For KITTI 2015 we report the standard metrics (using 3PE) for both Non-occluded and All pixels regions. Methods based on \CDN are highlighted in {\color{blue} blue}. Lower is better. The best result per column in in bold. \emph{Since the baselines are mostly trained with uni-modal ground truths, we only show \CDN with the same ground truths here for a fair comparison.}} \label{tbMain}
	\begin{tabular}{=l|+c|+c|+c|+c|+c|+c|+c}  & \multicolumn{3}{c|}{Scene Flow} & \multicolumn{4}{c}{KITTI 2015} \\ \cline{2-8}
	 &  & & & \multicolumn{2}{c|}{Non Occlusion 3PE} & \multicolumn{2}{c}{All Areas 3PE}\\  \cline{5-8}
		\multicolumn{1}{c|}{Method} & EPE & 1PE & 3PE & Foreground & All & Foreground & All \\ \hline
		MC-CNN~\cite{zbontar2016stereo} &  3.79 & - & - & 7.64 & 3.33 & 8.88 & 3.89 \\ 
		GC-Net~\cite{kendall2017end} & 2.51 & 16.9 & 9.34 & 5.58 & 2.61 & 6.16 & 2.87 \\
        PSMNet~\cite{chang2018pyramid} & 1.09 & 12.1 & 4.56 & 4.31 & 2.14 & 4.62 & 2.32 \\
        SegStereo~\cite{yang2018segstereo} & 1.45 & - & - & 3.70 & 2.08 & 4.07 & 2.25\\
        GwcNet-g~\cite{guo2019group} & 0.77 & 8.0 & 3.30 & 3.49 & 1.92 & 3.93 & 2.11\\
        HD\textsuperscript{3}-Stereo~\cite{Yin_2019_CVPR} & 1.08 & - & - & 3.43 & 1.87 & 3.63 & 2.02\\
        GANet~\cite{zhang2019ga} & 0.84 & 9.9  & - & 3.37 & 1.73 & 3.82 & 1.93 \\
        AcfNet~\cite{zhang2020adaptive} & 0.87 & - & 4.31 & 3.49 & 1.72 & 3.80 & 1.89 \\
        Stereo Expansion~\cite{yangupgrading} & - & -&- & 3.11 & 1.63 & 3.46 & 1.81 \\
        GANet Deep~\cite{zhang2019ga} & 0.78 & 8.7 & - & 3.11 & \textbf{1.63} & 3.46 & \textbf{1.81} \\ \hline
        \rowstyle{\color{blue}}
		\CDN-PSMNet    & 0.98 & 9.1 & 3.99 & 4.01 & 2.12 & 4.34 & 2.29  \\
		\rowstyle{\color{blue}}
		\rowstyle{\color{blue}} \CDN-GANet Deep & \textbf{0.70} & \textbf{7.7} & \textbf{2.98} & \textbf{2.79} & 1.72 & \textbf{3.20} & 1.92   \\
		\hline
	\end{tabular}
	\label{tbl:disp}
	\vskip-10pt
\end{table}

\subsection{Main results}
\label{ssec:ER}

\paragraph{Disparity estimation.} \autoref{tbl:disp} summarizes the results on disparity estimation. \CDN-\GANet Deep\footnote{We apply the \GANet Deep model introduced in the released code of~\cite{zhang2019ga}, available at~\url{https://github.com/feihuzhang/GANet}. The main architectures of \GANet Deep and \GANet are the same, while the former has some more 2D and 3D convolutional layers.} achieves the lowest error at all three metrics on Scene Flow. It reduces the error for \GANet Deep by  1PE and  EPE, both are significant. We see a similar gain for \PSMNet: \CDN-\PSMNet reduces EPE by , demonstrating the general applicability of our approach to existing networks.


On KITTI 2015, \CDN-\GANet Deep obtains the lowest error on the \emph{foreground} pixels and performs comparably to other methods on all the pixels\footnote{There are two possible reasons that \CDN-\GANet Deep does not outperform \GANet Deep on all the pixels. First, \CDN overly focuses on foreground pixels. Second, we used the same hyper-parameters as the original \GANet without specific tuning for \CDN. We note that the ratio of foreground/background pixels is ; the degradation by \CDN on the background is  3PE, smaller than the gain on foreground.}. We see a similar gain by \CDN-\PSMNet over \PSMNet on the foreground, which is quite surprising, as we do not specifically \emph{re-weight} the loss function towards foreground pixels. Since \CDN has advantages on pixels whose disparity is ambiguous and hard to estimate correctly (\eg, due to multi-modal distributions), the fact that foreground pixels have a higher error and \CDN can effectively reduce it suggests that those challenging pixels are mostly in the foreground. 
As will be seen in 3D object detection, the improvement by \CDN on foreground pixels translates to a higher accuracy on localizing objects.


\begin{table}[]
\small
	\centering
	\caption{\small \textbf{3D object detection results on the KITTI leader board.} We report \APBEV and \AP (in \%) of the \textbf{car} category at IoU. Methods with \CDN are in {\color{blue} blue}. The best result of each column is in bold font.}  \label{tbl:3d_object_detection}
	\begin{tabular}{=l|+c|+c|+c|+c|+c|+c}
		 & \multicolumn{3}{c|}{BEV Detection AP (\APBEV)} & \multicolumn{3}{c}{3D Detection AP (\AP)} \\ \cline{2-7}
		\multicolumn{1}{c|}{Method} & Easy & Moderate & Hard & Easy & Moderate & Hard \\ \hline
		\SRCNN~\cite{li2019stereo} & 61.9 & 41.3 & 33.4 & 47.6 & 30.2 & 23.7 \\
		\OCS~\cite{pon2019object} & 68.9 & 51.5 & 43.0 & 55.2 & 37.6 & 30.3\\
		\DispRCNN~\cite{sun2020disprcnn} & 74.1 & 52.4 & 43.8 & 59.6 & 39.4 & 32.0\\ \hline
		\PL~\cite{pseudoLiDAR}    & 67.3  & 45.0 & 38.4 & 54.5 & 34.1 & 28.3  \\
		\PL++~\cite{you2019pseudo}    & 78.3 & 58.0  & 51.3 & 61.1 & 42.4 & 37.0\\
		\PL E2E~\cite{qian2020end}   & 79.6 & 58.8 & 52.1 & 64.8 & 43.9 & 38.1 \\
		\rowstyle{\color{blue}}
		\CDN-\PL++    & 81.3 & 61.0 & 52.8 & 64.3 & 44.9 & 38.1 \\ \hline
 		\DSGN~\cite{chen2020dsgn} & 82.9 & 65.0 & 56.6 & 73.5 & 52.2 & 45.1\\
 		\rowstyle{\color{blue}}
        \CDN-\DSGN & \textbf{83.3} & \textbf{66.2} & \textbf{57.7} & \textbf{74.5} & \textbf{54.2} & \textbf{46.4}\\ \hline
	\end{tabular}
	\vskip-10pt
\end{table}

\begin{wrapfigure}{R}{0.35\textwidth}
	\vspace{-13pt}
	\includegraphics[width=\linewidth]{figures/mm.pdf}
	\vskip-5pt
	\caption{\small \textbf{MM training.} We show the EPE and 3PE disparity errors on Scene Flow test set using \CDN-\PSMNet, w/ or w/o MM training. MM training leads to faster convergence.} \label{fig:converge}
	\vskip -5pt
\end{wrapfigure}

\paragraph{3D object detection.}
\autoref{tbl:3d_object_detection} summarizes the results on the test set of KITTI 3D. Our \CDN consistently improves the two mainstream approaches, namely, \DSGN and \PL. For \PL, we achieve a / gain on \AP/\APBEV Moderate (the standard metric on the leader board) against \PL++: the only difference is that we replace \SDN by our \CDN-\SDN to have better depth estimates. Our approach even outperforms \PL E2E, which fine-tunes the depth network specifically for object detection. We argue that our approach, which can \emph{automatically} focus on the foregrounds, may have a similar effect as end-to-end training with object detection losses.
For \DSGN, plugging our \CDN-\SDN leads to a notable  gain at \AP, attaining the highest entry of stereo-based 3D detection accuracy on the KITTI leader board.




\iffalse
\begin{table}[t]
\small
	\begin{minipage}{.46\textwidth}
		\small 
		\caption{\small \textbf{Disparity multi-modal results.} We report the EPE, 1PE and 3PE on Scene Flow. Methods with \CDN are highlighted in {\color{blue} blue}. The best result of each column is in bold font.}  \label{tbl:disp_mm}
	\begin{tabular}{=l|+c|+c|+c}
		Method & EPE & 1PE & 3PE \\ \hline
        \PSMNet & 1.09 & 12.1 & 4.56 \\
		\rowstyle{\color{blue}}
		CDN-\PSMNet    & 0.98 & 9.1 & 3.99   \\
		\rowstyle{\color{blue}}
		CDN-\PSMNet MM & \textbf{0.96} & \textbf{9.0} & \textbf{3.96}  \\ \hline
	\end{tabular}
    \end{minipage}
\hfill
    \begin{minipage}{.46\textwidth}
		\small 
		\caption{\small \textbf{Depth multi-modal results.} We report the RMSE and ABSR errors on Scene Flow. The best result of each column is in bold font.}  \label{tbl:depth}
	\begin{tabular}{=l|+c|+c}
		Method & RMSE (m) & ABSR \\ \hline
		\SDN  & 2.05 & 0.039  \\
		\rowstyle{\color{blue}}
	    \CDN-\SDN    & 1.81 & 0.030   \\
		\rowstyle{\color{blue}}
		\CDN-\SDN MM & \textbf{1.80} & \textbf{0.028}     \\ \hline
	\end{tabular}
    \end{minipage}
    \vskip -10pt
\end{table}    
\fi


\begin{table}[t]
\small
\caption{\small \textbf{Disparity multi-modal results.} We report the EPE, 1PE and 3PE on Scene Flow. Methods with \CDN are highlighted in {\color{blue} blue}. The best result of each column is in bold font.}
\label{tbl:disp_mm}

\begin{minipage}{.48\textwidth}
		\small 
	\begin{tabular}{=l|+c|+c|+c}
		Method & EPE & 1PE & 3PE \\ \hline
        \PSMNet~\cite{chang2018pyramid} & 1.09 & 12.1 & 4.56 \\
		\rowstyle{\color{blue}}
		\CDN-\PSMNet    & 0.98 & 9.1 & 3.99   \\
		\rowstyle{\color{blue}}
		\CDN-\PSMNet MM & \textbf{0.96} & \textbf{9.0} & \textbf{3.96}  \\ \hline
	\end{tabular}
    \end{minipage}
\hfill
    \begin{minipage}{.48\textwidth}
		\small  \begin{tabular}{=l|+c|+c|+c}
		Method & EPE & 1PE & 3PE \\ \hline
        \GANet Deep~\cite{zhang2019ga} & 0.78 & 8.7 & - \\
		\rowstyle{\color{blue}}
		\CDN-\GANet Deep   & 0.70 & 7.7 & 2.98   \\
		\rowstyle{\color{blue}}
		\CDN-\GANet Deep MM & \textbf{0.68} & \textbf{7.7} & \textbf{2.97}  \\ \hline
	\end{tabular}
    \end{minipage}
    \vskip -10pt
\end{table}

\begin{table}[t]
\small
\begin{minipage}{.48\textwidth}
	\small 
	\caption{\small \textbf{Depth multi-modal results.} We report the RMSE and ABSR errors on Scene Flow. 
	The best result of each column is in bold font.}  
	\label{tbl:depth}
	\begin{tabular}{=l|+c|+c}
		Method & RMSE (m) & ABSR \\ \hline
		\SDN~\cite{you2019pseudo}  & 2.05 & 0.039  \\
		\rowstyle{\color{blue}}
	    \CDN-\SDN    & 1.81 & 0.030   \\
		\rowstyle{\color{blue}}
		\CDN-\SDN MM & \textbf{1.80} & \textbf{0.028}     \\ \hline
\end{tabular}
    \end{minipage}
\hfill
    \begin{minipage}{.48\textwidth}
		\small  
	\caption{\small \textbf{Ambiguous regions (object boundaries).}  We report the disparity error on Scene Flow. The best result of each column is in bold font.}
	\label{tbl:eval_ambigious}
	\begin{tabular}{=l|+c|+c|+c}
		Method & EPE & 1PE & 3PE \\ \hline
		PSMNet~\cite{chang2018pyramid} & 3.10 & 20.1 & 11.33 \\\rowstyle{\color{blue}}
		CDN-PSMNet & 2.10 & 15.3 & 8.92 \\
		\rowstyle{\color{blue}}
		CDN-PSMNet MM & \textbf{2.08} & \textbf{13.2} & \textbf{8.65} \\
		\hline	
	\end{tabular}
    \end{minipage}
    \vskip -10pt
\end{table} 



\subsection{Analysis}
\label{ssec:ER}


\paragraph{Multi-modal (MM) ground truth.}
We investigate creating the multi-modal (MM) ground truths for training our models.  \autoref{tbl:disp_mm} and \autoref{tbl:depth} summarize the results on Scene Flow for disparity and depth estimation, respectively. MM training slightly reduces the errors. To better understand how MM ground truths affect network training, we plot the test accuracy along the training epochs in~\autoref{fig:converge}: \CDN-\PSMNet trained with MM ground truths converges much faster. We attribute this to the observations in~\cite{arpit2017closer}: a neural network tends to learn simple and \emph{clean} patterns first. We note that, for boundary pixels whose disparities are inherently multi-modal, uni-modal ground truths are indeed \emph{noisy} labels. A network thus tends to ignore these pixels in the early epochs. In contrast, MM ground truths provide \emph{clean} supervisions for these boundary pixels; the network thus can learn the patterns much faster. See the supplementary material for a visualization and further discussions.

\begin{wraptable}{R}{0.45\linewidth}
	\vskip -8pt
	\centering
	\small
	\tabcolsep 3pt
	\vskip -5pt
	\caption{\small \textbf{Ablation studies.} We report disparity error for \CDN-\PSMNet on Scene Flow. Methods without  loss are learned with \emph{mean} regression.}  \label{tbl:depth_abl}
	\begin{tabular}{=l|+c|+c|+c|+c|+c}
		Offsets &  Loss & Output & EPE& 1PE & 3PE \\ \hline
		& & Mean & 1.09 & 12.1 & 4.56 \\
		\hspace{5pt} \checkmark & & Mean & 1.04 & 12.0 & 4.55\\
		& \checkmark & Mode & 1.20 & 10.5 & 4.21 \\
		\hspace{5pt} \checkmark & \checkmark & Mode &\textbf{0.98} & \textbf{9.1} & \textbf{3.99} \\ \hline	
	\end{tabular}
\vskip -5pt
\end{wraptable}


\paragraph{Ablation studies.} We study different components of our approach in~\autoref{tbl:depth_abl}. Methods without  loss use the regression loss for optimization (cf. \autoref{eq_disp_loss}) and output the mean. Methods with  loss output the mode. 
We see that, the offset sub-network alone can hardly improve the performance. Using  distance alone reduces 1PE and 3PE errors, but not EPE, suggesting that it cannot produce sub-pixel disparity estimates\footnote{Using a bin size  without offsets, the mode is restricted to integral values and EPE suffers.}. Only combining the offset sub-network and the  loss produces consistent improvement over all three metrics.

\iffalse
\begin{wraptable}{R}{0.35\linewidth}
	\centering
	\small
	\vskip-12pt
	\tabcolsep 2pt
	\caption{\small \textbf{Ambiguous regions.}  We report the disparity error on Scene Flow.}
	\vskip-5pt
	\label{tbl:eval_ambigious}
	\begin{tabular}{=l|+c|+c|+c}
		Method & EPE & 1PE & 3PE \\ \hline
		PSMNet~\cite{chang2018pyramid} & 3.10 & 20.1 & 11.33 \\\rowstyle{\color{blue}}
		CDN-PSMNet & 2.10 & 15.3 & 8.92 \\
		\rowstyle{\color{blue}}
		CDN-PSMNet MM & 2.08 & 13.2 & 8.65 \\
		\hline	
	\end{tabular}
	\vskip-5pt
\end{wraptable}
\fi

\paragraph{Disparity on boundaries.}
\autoref{tbl:eval_ambigious} shows the results: we obtain pixels on object boundaries using the OpenCV Canny edge detector with minVal/maxVal=100/200.
Both \CDN and training with multi-modal ground truths reduce the error significantly.

\paragraph{Qualitative disparity results on KITTI.}
As shown in~\autoref{fig:qualitative}, our approach is able to estimate disparity accurately, especially along the object boundaries. Specifically, \CDN-\GANet Deep maintains the straight bar shape (on the right), while \GANet Deep blends it with the background sky due to the mean estimates.
 \section{Conclusion}
\label{sec: disc}

\begin{wrapfigure}{R}{0.45\textwidth}
    \vskip -50pt
    \centering
	\tabcolsep 1.5pt
	\includegraphics[width=\linewidth]{figures/pip7_2.pdf}
	\vskip-5pt
	\caption{\small \textbf{Qualitative results on disparity.} The top, middle, and bottom images are the left image, the result of \GANet Deep, and the result of \CDN-\GANet Deep, together with the foreground 3PE.
	} \label{fig:qualitative}
	\vskip-20pt
\end{wrapfigure}

In this paper we have introduced a new output representation, model architecture and loss function for depth/disparity estimation that can faithfully produce real-valued estimates of depth/disparity.
We have shown that results not just in more accurate depth estimates, but also significant improvement in downstream tasks like object detection.
Finally, because we explicitly output and optimize a distribution over depths, our approach can naturally take into account \emph{uncertainty and multimodality} in the ground truth.
More generally, our results suggest that removing suboptimalities in how we represent and optimize 3D information can have a large impact on a multitude of vision tasks.
 
\clearpage
\section*{Broader Impact}
The end results of this paper are improved depth and disparity estimation, particularly on foreground objects.
This is of use to self-driving cars, 3D reconstruction, and other robotics applications.  
In particular, it has the potential to improve the \emph{safety} of these systems, as indicated by the increased 3D object detection performance.
Our approach can also easily be incorporated into other depth or disparity estimation algorithms for further improvement.

While our depth predictions are significantly better, any failure has important safety considerations, such as collisions and accidents.
Before deployment, appropriate safety thresholds must be cleared.

Our approach does not specifically leverage dataset biases, although being a machine learning approach, it is impacted as much as other machine learning techniques.

\section*{Acknowledgments}
This research is supported by grants from the National Science Foundation NSF (III-1618134, III-1526012, IIS-1149882, IIS-1724282, and TRIPODS-1740822, OAC-1934714), the Office of Naval Research DOD (N00014-17-1-2175), the Bill and Melinda Gates Foundation, and the Cornell Center for Materials Research with funding from the NSF MRSEC program (DMR-1719875). We are thankful for generous support by Zillow, SAP America Inc, AWS Cloud Credits for Research, Ohio Supercomputer Center, and Facebook.

\bibliography{main}
\bibliographystyle{plainnat}

\newpage
\appendix
\begin{center}
	\textbf{\Large Supplementary Material}
\end{center}





We provide in this material the contents omitted in the main paper:
\begin{itemize}
    \item \autoref{supsec:detail}: additional implementation details (cf. subsection 3.2 and subsection 4.2 of the main paper).
    \item \autoref{supsec:disc}: additional discussions  (cf. section 3 and subsection 4.4 of the main paper).
    \item \autoref{supsec:result_analysis}: additional experimental results and analysis (cf. subsection 4.2, subsection 4.3, and subsection 4.4 of the main paper).
\end{itemize}



\section{Implementation Details}
\label{supsec:detail}

\subsection{Learning with multi-modal ground truths}

For multi-modal ground truths, we cannot use Equation 8 of the main paper for optimization. Instead, we apply the loss in Equation 12, for  distance.
This loss essentially computes the difference in areas between the CDFs of the two distributions. For mixtures of Dirac delta functions, it can be efficiently implemented by computing the accumulated difference between CDF histograms. It takes  for each pixel using sorting, where  is the total number of supports of both distributions. 
Our implementation is adapted from \emph{scipy.stats.wasserstein\_distance} and we modify it to be compatible with Pytorch tensors and use CUDA to parallelize the computation over all the pixels.


\subsection{Learning with the (approximated) KL divergence}
\label{supssec:kl_divergence}

The \emph{Kullback–Leibler} (KL) divergence 

between two distributions  and  requires them to have the same supports: \ie,  if , for  to be finite.

For our case,  and .
These two measures may have different supports. To make the KL divergence applicable, we can smooth  to form a mixture of Laplace or Gaussian distributions.

For example, smoothing  with a Laplace distribution, , we get

With , the KL divergence reduces to the following loss

where  is the grid disparity value of the bin the true disparity  belongs to.

Similarly, smoothing  with a Gaussian distribution , we get

With , the KL divergence reduces to the following loss

where  is the grid disparity value of the bin the true disparity  belongs to.

These formulations reduce to the conventional classification loss plus offset regression loss, commonly used for keypoint estimation~\cite{zhou2019objects,papandreou2017towards} and one-stage 2D object detection~\cite{zhou2019objects,redmon2016you,liu2016ssd,lin2017focal}.

\section{Additional Discussions}
\label{supsec:disc}

\subsection{Multi-modal ground truths} There are three reasons why multi-modal ground truths would benefit disparity or depth estimation. First, pixels are discrete: a single pixel may capture different depths. Second, real datasets need to project signals from a depth sensor (\eg, LiDAR) to a depth map. As pixels are discrete and the cameras and LiDAR might be placed differently, multiple LiDAR points of different depths may be projected to the same pixel. Third, for stereo estimation, pixels along boundaries or occluded regions cause ambiguity to the model; multi-modal ground truths offer better supervision for training, especially in early training epochs.

Conceptually, learning with multi-modal ground truths should notably improve results in Table 3 and Table 4 of the main paper. However, in evaluation, a majority of pixels are not on the object boundaries. Besides, we still evaluate using the (likely noisy) uni-modal ground truths. To further analyze these, we show in Table 6 of the main paper the disparity error calculated on object boundaries: learning with multi-modal ground truths leads to a significant improvement.

\subsection{Multi-task learning}
One way to mitigate stereo predictions at depth discontinuities is to jointly perform stereo estimation and other tasks such as semantic segmentation~\cite{jiao2018look,xu2018pad,jafari2017analyzing}, which can reason about object boundaries. The core idea is to leverage additional semantic labels to guide the model to resolve depth discontinuities (\ie, predict uni-modal distributions). Our method, in contrast, does not prevent predicting multi-modal distributions along depth discontinuities, but changes the outputting rule (\ie,  with a predicted offset). 
Our method can also capture depth discontinuities within an object or an object class. In contrast, semantic segmentation labels overlapped objects of the same class by the same label and does not directly tell their boundaries.

\subsection{Offsets and distributions without common supports} While the \emph{learned} offsets may lead to common supports between the predicted and ground truth distributions, we have to first come up with a loss to \emph{learn} such offsets before common supports become possible.
Concretely, to learn  in Equation 9, we need a loss that can measure the divergence between  and , which may not have common supports. We note that, this may occur even if the target distribution  is a Dirac delta function. 
While the KL divergence or a regression loss may be applied to learn the offsets, they need to either smooth the distributions or carefully design the loss to learn both the distribution and the offset networks.
The Wasserstein distance offers a principled loss to learn the two networks jointly.


\section{Additional Results and Analysis}

\label{supsec:result_analysis}


\subsection{Ablation studies on different divergences}
\label{supsec:loss}
We show the ablation study on using different divergences between distributions in \autoref{tbl:loss_ablation}. For the KL divergence (\autoref{supssec:kl_divergence}), we use Laplace smoothing with  (\autoref{eq_Lap_loss}). Our results show that the Wasserstein distance is a better choice than the KL divergence for comparing the predicted and the ground truth disparity (or depth) distributions. We also see that  distance performs worse than . We attribute this to outliers (\ie, noisy disparity labels) in a dataset.

\begin{table}[]
		\small
		\centering
		\caption{\small \textbf{Comparison of different divergences (distances).} We report the RMSE and the ABSR error for depth estimation on Scene Flow. The best result of each column is in bold font.}  \label{tbl:loss_ablation}
		\begin{tabular}{l|c|c|c}
		Method & Divergence &  RMSE (m) & ABSR \\ \hline
		\SDN  & - & 2.05 & 0.04  \\
		\CDN-\SDN & KL    & 2.57 & 0.04   \\ 
		\CDN-\SDN &     & \textbf{1.81} & \textbf{0.03}   \\
		\CDN-\SDN &   & 1.91 & 0.05   \\ \hline
	\end{tabular}
\end{table}


\subsection{Effect of bin sizes}
\CDN outputs modes and needs (a) the bin containing the truth disparity or depth to have the highest probability and (b) the offset to be accurate. 
The bin size balances the difficulty of (a) and (b).
A smaller bin size makes (a) harder. A larger bin size makes (a) easier but makes (b) harder as the range of offsets gets larger. It is the only hyper-parameter to tune and only integral values are considered.

We show these effects of bin sizes on uniform grids, with disparities in the range of  for disparity estimation in \autoref{tbl:abl_bins}. For a bin size , predicting the correct bin is harder. 
For a bin size , predicting the correct bin is easier, whereas predicting the correct offset becomes harder. We found  to perform well in general. 



\begin{table}[]
	\centering
	\small
	\caption{\small \textbf{Comparison of bin sizes.}  We report the disparity error on Scene Flow using \CDN-\PSMNet model.}
	\label{tbl:abl_bins}
	\begin{tabular}{=c|+c|+c|+c}
		Bin size & EPE & 1PE & 3PE \\ \hline
		1 & 1.22 & 13.9 & 4.33 \\
		2 & \textbf{0.98} & \textbf{9.1} & \textbf{3.99} \\
		4 & 1.52 & 26.1 & 4.17 \\
     \hline	
	\end{tabular}
\end{table}

\begin{table}[t]
    \centering
	\caption{ \small \textbf{Ablation studies on the MM ground truths.} We conduct experiments using \CDN-\SDN MM on Scene Flow (cf. Table 4 of the main paper).}
    \renewcommand{\arraystretch}{0.3}
	\begin{tabular}{c|c|c|c}
		 & k & RMSE & ABSR \\ \hline
		0.8 & 3  & \textbf{1.80} & \textbf{0.028}  \\
		0.8 & 5  & 1.82 & 0.029  \\
		0.8 & 7  & 1.88 & 0.035  \\
		0.5 & 3  & 1.81 & 0.029  \\
		0.2 & 3  & 2.20 & 0.062   \\
		\hline
	\end{tabular}
	\label{tbl:depth-suppl}
\end{table} 

\subsection{Ablation studies on  and  in multi-modal (MM) ground truths} \autoref{tbl:depth-suppl} shows the depth estimation error on Scene Flow using \CDN-\SDN-MM, with different  and  in preparing the MM ground truths (cf. Table 4 of the main paper). A smaller  leads to a larger error, which makes sense as it relies less on the ground truths.

\subsection{Learning with multi-modal (MM) ground truths}

Following subsection 4.4 and Figure 4 of the main paper, we train \CDN-\PSMNet and \CDN-\PSMNet MM for only two epochs and compare their disparity estimation performance. \autoref{fig:MM-analysis} shows the results on KITTI images. While both methods have similar predictions at smooth regions, \CDN-\PSMNet MM leads to much sharper and clearer object boundaries, suggesting that the multi-modal ground truths are better supervisions for learning around the boundaries in early epochs. 

\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{figures/early_image.png}
\includegraphics[width=0.6\linewidth]{figures/early.png}
\includegraphics[width=0.6\linewidth]{figures/early_mm.png}
\caption{\small \textbf{Qualitative results on learning with multi-modal ground truths at early epochs}. The top, middle, and bottom images are the left image, the result of \CDN-\PSMNet, and the result of \CDN-\PSMNet MM.}
\label{fig:MM-analysis}
\end{figure}


\subsection{Learned offsets} The offset network learns to produce the sub-grid disparity at each integral disparity values. \autoref{fig:off_visual} shows an example, in which we back-project pixels into 3D points using the estimated disparity or depth at each pixel by the \emph{mode}, with or without the offset prediction. Without the offset, the 3D points can only occupy discrete depths, leading to a discontinuous, non-smooth point cloud.


\begin{figure}
\includegraphics[width=\linewidth]{figures/offset1.pdf}
\caption{\small {A visualization of the 3D point cloud (from the bird's-eye view) derived from the estimated disparities or depths (by modes), with (right) and without (left) offset prediction.}
} \label{fig:off_visual}
\end{figure}



\subsection{Point cloud visualization}
\autoref{sfig:illustration} shows the BEV point cloud visualization. We show the 3D points generated by \SDN and \CDN-\SDN as well as the ground truth LiDAR points and car/pedestrian boxes. We see that, \CDN-\SDN generates sharper points than \SDN. Specifically for pixels on the foreground objects, \SDN usually predicts the depths beyond the boxes due to the \emph{mean} estimates from multi-modal distributions on the boundary pixels, whereas \CDN-\SDN significantly alleviates the problem. We also see some failure cases of \CDN-\SDN: on the right image, \CDN-\SDN has a larger error on the background compared to \SDN.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{figures/pointcloud.pdf}
\caption{\small\textbf{BEV Point cloud visualization.} The blue points are obtained using \SDN. The red points are from our \CDN-\SDN model. The yellow points are from the ground truth LiDAR. The green boxes are ground truth car / pedestrian locations. The observer is at the left-hand side of the point cloud looking to the right.}
\label{sfig:illustration}
\end{figure}


\subsection{Depth estimation}
Besides the Scene Flow dataset, we show the depth estimation error on KITTI Val: the 3,769 validation images for 3D object detection. We follow~\cite{you2019pseudo} to train the depth estimation model and compute the depth estimation error on pixels associated with ground truth LiDAR points. \autoref{tbl:abl_DSGN} and \autoref{sfig:converge} show the results, \CDN-\SDN achieves lower error than \SDN, which explains why \CDN-\SDN (and \CDN-\DSGN) can lead to better 3D object detection accuracy. 

\begin{table}[]
	\centering
	\small
	\caption{\small \textbf{Depth error on KITTI Val.}  We compare \SDN and \CDN-\SDN models.}
	\label{tbl:abl_DSGN}
	\begin{tabular}{=l|+c|+c|+c|+c}
	 & \multicolumn{2}{c}{Depth errors (m)} \\ \cline{2-5}
		  \multicolumn{1}{c|}{Method} & Mean & Median & RMSE & ABSR \\ \hline
		\SDN & 0.589 & 0.128 & 3.08 & 0.044\\
		\CDN-\SDN & \textbf{0.524} & \textbf{0.093} & \textbf{3.00} & \textbf{0.042}\\
     \hline	
	\end{tabular}
\end{table}

\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{figures/med_err.pdf}
\caption{\small  \textbf{Depth error on KITTI Val.} We compute the median absolute depth error for different depth ranges on KITTI Val images using \SDN and \CDN-\SDN.} \label{sfig:converge}
\end{figure}

\subsection{3D object detection}

We show in~\autoref{fig:AP} the object detection precision-recall curves of \DSGN vs. \CDN-\DSGN. \CDN-\DSGN has higher precision (vertical) values than \DSGN at different recall (horizontal) values.

\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{figures/cdn_iou_dsgn.pdf}
\caption{\small We show the object detection precision-recall curves for \AP at moderate cases on cars. We compare \DSGN (stereo images) and \CDN-\DSGN (stereo images).}
\label{fig:AP}
\end{figure}


\subsection{Qualitative disparity results}

We show in~\autoref{fig:depth2} and~\autoref{fig:depth3} the predicted disparity maps and the foreground errors of both \GANet Deep and \CDN-\GANet Deep on KITTI and Scene Flow. \CDN generally leads to sharper and clearer object boundaries.

\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{figures/pip7_1.pdf}
\caption{\small \textbf{Qualitative results on KITTI.} The top, middle, and bottom images are the left image, the result of \GANet Deep, and the result of \CDN-\GANet Deep, together with the foreground 3PE.}
\label{fig:depth2}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{figures/pip7_3.pdf}
\caption{\small \textbf{Qualitative results on Scene Flow validation set.} The first row shows the left image and the ground truth map. The second row shows the result of \GANet Deep and the result of \CDN-\GANet Deep, together with the foreground end point errors (EPE).}
\label{fig:depth3}
\end{figure}






 
\end{document}